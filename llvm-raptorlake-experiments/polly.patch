--- a/polly/lib/Pass/PhaseManager.cpp	2025-11-16 16:34:09.155390732 +0100
+++ b/polly/lib/Pass/PhaseManager.cpp	2025-11-16 16:35:44.485198412 +0100
@@ -68,9 +68,9 @@ public:
     // These must be preserved during all phases so that if processing one SCoP
     // has finished, the next SCoP can still use them. Recomputing is not an
     // option because ScopDetection stores references to the old results.
-    // TODO: CodePreparation doesn't actually need these analysis, it just keeps
-    // them up-to-date. If they are not computed yet, can also compute after the
-    // prepare phase.
+    // TODO: CodePreparation doesn't actually need these analyses, it just keeps
+    // them up-to-date. If they are not computed yet, they can also be computed
+    // after the prepare phase.
     LoopInfo &LI = FAM.getResult<LoopAnalysis>(F);
     DominatorTree &DT = FAM.getResult<DominatorTreeAnalysis>(F);
     bool ModifiedIR = false;
@@ -88,9 +88,10 @@ public:
       }
     }
 
-    // Can't do anything without detection
+    // Can't do anything SCoP-related without detection, but we may already have
+    // modified the IR in the prepare phase.
     if (!Opts.isPhaseEnabled(PassPhase::Detection))
-      return false;
+      return ModifiedIR;
 
     AAResults &AA = FAM.getResult<AAManager>(F);
     ScalarEvolution &SE = FAM.getResult<ScalarEvolutionAnalysis>(F);
@@ -104,6 +105,7 @@ public:
     // Phase: detection
     ScopDetection SD(DT, SE, LI, RI, AA, ORE);
     SD.detect(F);
+
     if (Opts.isPhaseEnabled(PassPhase::PrintDetect)) {
       outs() << "Detected Scops in Function " << F.getName() << "\n";
       for (const Region *R : SD.ValidRegions)
@@ -116,28 +118,48 @@ public:
     if (Opts.isPhaseEnabled(PassPhase::DotScopsOnly))
       printGraphForFunction(F, &SD, "scopsonly", true);
 
-    auto ViewScops = [&](const char *Name, bool IsSimply) {
-      if (Opts.ViewFilter.empty() && !F.getName().count(Opts.ViewFilter))
+    auto ViewScops = [&](const char *Name, bool IsSimple) {
+      // If a view filter is specified, only visualize functions whose name
+      // contains the filter substring.
+      if (!Opts.ViewFilter.empty() &&
+          !F.getName().contains(Opts.ViewFilter))
         return;
 
-      if (Opts.ViewAll || std::distance(SD.begin(), SD.end()) > 0)
-        viewGraphForFunction(F, &SD, Name, IsSimply);
+      bool HasScops = !SD.ValidRegions.empty();
+      if (Opts.ViewAll || HasScops)
+        viewGraphForFunction(F, &SD, Name, IsSimple);
     };
+
     if (Opts.isPhaseEnabled(PassPhase::ViewScops))
       ViewScops("scops", false);
     if (Opts.isPhaseEnabled(PassPhase::ViewScopsOnly))
       ViewScops("scopsonly", true);
 
+    // If no phase at or beyond ScopInfo is enabled, we do not need to build
+    // ScopInfo or execute any SCoP-level pipeline.
+    bool NeedScops = false;
+    for (PassPhase P :
+         enum_seq_inclusive(PassPhase::ScopInfo, PassPhase::PassPhaseLast)) {
+      if (Opts.isPhaseEnabled(P)) {
+        NeedScops = true;
+        break;
+      }
+    }
+
+    if (!NeedScops)
+      return ModifiedIR;
+
     // Phase: scops
     AssumptionCache &AC = FAM.getResult<AssumptionAnalysis>(F);
     const DataLayout &DL = F.getParent()->getDataLayout();
     ScopInfo Info(DL, SD, SE, LI, AA, DT, AC, ORE);
+
     if (Opts.isPhaseEnabled(PassPhase::PrintScopInfo)) {
       if (Region *TLR = RI.getTopLevelRegion()) {
         SmallVector<Region *> Regions;
         addRegionIntoQueue(*TLR, Regions);
 
-        // reverse iteration because the regression tests expect it.
+        // Reverse iteration because the regression tests expect it.
         for (Region *R : reverse(Regions)) {
           Scop *S = Info.getScop(R);
           outs() << "Printing analysis 'Polly - Create polyhedral "
@@ -158,11 +180,12 @@ public:
         Worklist.insert(R);
 
     TargetTransformInfo &TTI = FAM.getResult<TargetIRAnalysis>(F);
+
     while (!Worklist.empty()) {
       Region *R = Worklist.pop_back_val();
       Scop *S = Info.getScop(R);
       if (!S) {
-        // This can happen if codegenning of a previous SCoP made this region
+        // This can happen if codegen of a previous SCoP made this region
         // not-a-SCoP anymore.
         POLLY_DEBUG(dbgs() << "SCoP in Region '" << *R << "' disappeared");
         continue;
@@ -211,7 +234,8 @@ public:
       // Phase: simplify-1
       // If we have already run simplify-0, do not re-run it if the SCoP has not
       // changed since then.
-      if (ModifiedSinceSimplify && Opts.isPhaseEnabled(PassPhase::Simplify1)) {
+      if (ModifiedSinceSimplify &&
+          Opts.isPhaseEnabled(PassPhase::Simplify1)) {
         runSimplify(*S, 1);
         ModifiedSinceSimplify = false;
       }
@@ -232,12 +256,12 @@ public:
       if (Opts.isPhaseEnabled(PassPhase::Optimization))
         runIslScheduleOptimizer(*S, &TTI, DA);
 
-      // Phase: import-jscop
+      // Phase: export-jscop
       if (Opts.isPhaseEnabled(PassPhase::ExportJScop))
         runExportJSON(*S);
 
       // Phase: ast
-      // Cannot run codegen unless ast is enabled
+      // Cannot run codegen unless ast is enabled.
       if (!Opts.isPhaseEnabled(PassPhase::AstGen))
         continue;
       std::unique_ptr<IslAstInfo> IslAst = runIslAstGen(*S, DA);
@@ -250,8 +274,8 @@ public:
         ModifiedIR = true;
 
         // For all regions, create new polly::Scop objects because the old ones
-        // refere to invalidated LLVM-IR.
-        // FIXME: Adds all SCoPs again to statistics
+        // refer to invalidated LLVM-IR.
+        // FIXME: Adds all SCoPs again to statistics.
         Info.recompute();
       }
     }


--- a/polly/lib/Analysis/ScopDetection.cpp	2025-09-21 15:28:29.292203509 +0200
+++ b/polly/lib/Analysis/ScopDetection.cpp	2025-11-16 15:35:24.476840546 +0200
@@ -657,30 +657,49 @@ bool ScopDetection::isValidCFG(BasicBloc
   Region &CurRegion = Context.CurRegion;
 
   Instruction *TI = BB.getTerminator();
+  assert(TI && "BasicBlock must have a terminator");
 
-  if (AllowUnreachable && isa<UnreachableInst>(TI))
+  // If unreachable blocks are explicitly allowed, accept an unreachable
+  // terminator without further checks.
+  if (AllowUnreachable && isa<UnreachableInst>(TI)) {
     return true;
+  }
 
-  // Return instructions are only valid if the region is the top level region.
-  if (isa<ReturnInst>(TI) && CurRegion.isTopLevelRegion())
+  // Return instructions are only valid if the region is the top-level region.
+  if (isa<ReturnInst>(TI) && CurRegion.isTopLevelRegion()) {
     return true;
+  }
 
+  // Extract the condition from the terminator, if any.
+  // For unconditional branches and other terminators this may be null.
   Value *Condition = getConditionFromTerminator(TI);
 
-  if (!Condition)
-    return invalid<ReportInvalidTerminator>(Context, /*Assert=*/true, &BB);
+  if (!Condition) {
+    // Any terminator we cannot model via a condition is considered invalid
+    // for SCoP formation.
+    return invalid<ReportInvalidTerminator>(Context,
+                                            /*Assert=*/true, &BB);
+  }
 
-  // UndefValue is not allowed as condition.
-  if (isa<UndefValue>(Condition))
-    return invalid<ReportUndefCond>(Context, /*Assert=*/true, TI, &BB);
+  // UndefValue is not allowed as a condition.
+  if (isa<UndefValue>(Condition)) {
+    return invalid<ReportUndefCond>(Context,
+                                    /*Assert=*/true, TI, &BB);
+  }
 
-  if (BranchInst *BI = dyn_cast<BranchInst>(TI))
+  if (auto *BI = dyn_cast<BranchInst>(TI)) {
     return isValidBranch(BB, BI, Condition, IsLoopBranch, Context);
+  }
 
-  SwitchInst *SI = dyn_cast<SwitchInst>(TI);
-  assert(SI && "Terminator was neither branch nor switch");
+  if (auto *SI = dyn_cast<SwitchInst>(TI)) {
+    return isValidSwitch(BB, SI, Condition, IsLoopBranch, Context);
+  }
 
-  return isValidSwitch(BB, SI, Condition, IsLoopBranch, Context);
+  // Any other terminator (invoke, indirectbr, callbr, etc.) is not supported
+  // by the current SCoP detection logic. Instead of asserting, we gracefully
+  // reject the CFG at this point.
+  return invalid<ReportInvalidTerminator>(Context,
+                                          /*Assert=*/true, &BB);
 }
 
 bool ScopDetection::isValidCallInst(CallInst &CI,
@@ -1206,19 +1225,22 @@ bool ScopDetection::isValidMemoryAccess(
 
 bool ScopDetection::isValidInstruction(Instruction &Inst,
                                        DetectionContext &Context) {
-  for (auto &Op : Inst.operands()) {
-    auto *OpInst = dyn_cast<Instruction>(&Op);
-
-    if (!OpInst)
+  // Reject uses of values produced in error blocks, except in very constrained
+  // situations (PHI nodes whose users are all terminators). This maintains a
+  // clean separation between the "main" SCoP region and error-handling paths.
+  for (Use &Op : Inst.operands()) {
+    auto *OpInst = dyn_cast<Instruction>(Op.get());
+    if (!OpInst) {
       continue;
+    }
 
     if (isErrorBlock(*OpInst->getParent(), Context.CurRegion)) {
-      auto *PHI = dyn_cast<PHINode>(OpInst);
-      if (PHI) {
+      if (auto *PHI = dyn_cast<PHINode>(OpInst)) {
         for (User *U : PHI->users()) {
           auto *UI = dyn_cast<Instruction>(U);
-          if (!UI || !UI->isTerminator())
+          if (!UI || !UI->isTerminator()) {
             return false;
+          }
         }
       } else {
         return false;
@@ -1226,37 +1248,50 @@ bool ScopDetection::isValidInstruction(I
     }
   }
 
-  if (isa<LandingPadInst>(&Inst) || isa<ResumeInst>(&Inst))
+  // LandingPad and Resume are not supported in SCoPs.
+  if (isa<LandingPadInst>(&Inst) || isa<ResumeInst>(&Inst)) {
     return false;
+  }
 
-  // We only check the call instruction but not invoke instruction.
-  if (CallInst *CI = dyn_cast<CallInst>(&Inst)) {
-    if (isValidCallInst(*CI, Context))
+  // We only check the call instruction but not invoke instruction here.
+  if (auto *CI = dyn_cast<CallInst>(&Inst)) {
+    if (isValidCallInst(*CI, Context)) {
       return true;
+    }
 
-    return invalid<ReportFuncCall>(Context, /*Assert=*/true, &Inst);
+    return invalid<ReportFuncCall>(Context,
+                                   /*Assert=*/true, &Inst);
   }
 
+  // Instructions which do not read or write memory and are not allocas
+  // are always fine.
   if (!Inst.mayReadOrWriteMemory()) {
-    if (!isa<AllocaInst>(Inst))
+    if (!isa<AllocaInst>(Inst)) {
       return true;
+    }
 
-    return invalid<ReportAlloca>(Context, /*Assert=*/true, &Inst);
+    // Allocas are currently not supported inside SCoPs.
+    return invalid<ReportAlloca>(Context,
+                                 /*Assert=*/true, &Inst);
   }
 
-  // Check the access function.
+  // Check the access function for memory instructions.
   if (auto MemInst = MemAccInst::dyn_cast(Inst)) {
     Context.hasStores |= isa<StoreInst>(MemInst);
-    Context.hasLoads |= isa<LoadInst>(MemInst);
-    if (!MemInst.isSimple())
-      return invalid<ReportNonSimpleMemoryAccess>(Context, /*Assert=*/true,
-                                                  &Inst);
+    Context.hasLoads  |= isa<LoadInst>(MemInst);
+
+    if (!MemInst.isSimple()) {
+      return invalid<ReportNonSimpleMemoryAccess>(Context,
+                                                  /*Assert=*/true, &Inst);
+    }
 
     return isValidMemoryAccess(MemInst, Context);
   }
 
-  // We do not know this instruction, therefore we assume it is invalid.
-  return invalid<ReportUnknownInst>(Context, /*Assert=*/true, &Inst);
+  // Any other memory-reading/writing instruction we do not explicitly
+  // understand is conservatively treated as invalid for SCoPs.
+  return invalid<ReportUnknownInst>(Context,
+                                    /*Assert=*/true, &Inst);
 }
 
 /// Check whether @p L has exiting blocks.
@@ -1629,48 +1664,94 @@ void ScopDetection::findScops(Region &R)
 bool ScopDetection::allBlocksValid(DetectionContext &Context) {
   Region &CurRegion = Context.CurRegion;
 
+  // Phase 1: Validate loops that are (partially) contained in the region.
+  //
+  // We ensure that loops either:
+  //  - Are fully contained and structurally valid for SCoP detection, or
+  //  - Do not partially overlap the region.
   for (const BasicBlock *BB : CurRegion.blocks()) {
     Loop *L = LI.getLoopFor(BB);
-    if (L && L->getHeader() == BB) {
-      if (CurRegion.contains(L)) {
-        if (!isValidLoop(L, Context)) {
-          Context.IsInvalid = true;
-          if (!KeepGoing)
-            return false;
+    if (!L) {
+      continue;
+    }
+
+    // We only act when this basic block is the header of the loop.
+    if (L->getHeader() != BB) {
+      continue;
+    }
+
+    if (CurRegion.contains(L)) {
+      // The loop is fully contained in the region.
+      if (!isValidLoop(L, Context)) {
+        Context.IsInvalid = true;
+        if (!KeepGoing) {
+          return false;
+        }
+      }
+    } else {
+      // The loop is not fully contained. Ensure that none of its latches
+      // are inside the region. If they are, the loop partially overlaps the
+      // region, which we cannot handle.
+      SmallVector<BasicBlock *, 1> Latches;
+      L->getLoopLatches(Latches);
+
+      for (BasicBlock *Latch : Latches) {
+        if (CurRegion.contains(Latch)) {
+          return invalid<ReportLoopOnlySomeLatches>(Context,
+                                                    /*Assert=*/true, L);
         }
-      } else {
-        SmallVector<BasicBlock *, 1> Latches;
-        L->getLoopLatches(Latches);
-        for (BasicBlock *Latch : Latches)
-          if (CurRegion.contains(Latch))
-            return invalid<ReportLoopOnlySomeLatches>(Context, /*Assert=*/true,
-                                                      L);
       }
     }
   }
 
+  // Phase 2: Validate the CFG and instructions of each basic block.
   for (BasicBlock *BB : CurRegion.blocks()) {
-    bool IsErrorBlock = isErrorBlock(*BB, CurRegion);
+    const bool IsErrorBlock = isErrorBlock(*BB, CurRegion);
 
     // Also check exception blocks (and possibly register them as non-affine
     // regions). Even though exception blocks are not modeled, we use them
     // to forward-propagate domain constraints during ScopInfo construction.
-    if (!isValidCFG(*BB, false, IsErrorBlock, Context) && !KeepGoing)
+    if (!isValidCFG(*BB, /*IsLoopBranch=*/false, IsErrorBlock, Context) &&
+        !KeepGoing) {
       return false;
+    }
 
-    if (IsErrorBlock)
+    if (IsErrorBlock) {
+      // We treat error blocks specially; their internal instructions are not
+      // considered part of the SCoP proper.
       continue;
+    }
+
+    // Iterate all instructions except the terminator, which has already been
+    // validated by isValidCFG().
+    auto I = BB->begin();
+    auto E = BB->end();
+
+    // Defensive check: normally a basic block always has at least a terminator,
+    // so begin() != end(). However, if the IR is malformed, we avoid
+    // dereferencing end().
+    if (I == E) {
+      continue;
+    }
+
+    --E; // Exclude the terminator.
 
-    for (BasicBlock::iterator I = BB->begin(), E = --BB->end(); I != E; ++I)
+    for (; I != E; ++I) {
       if (!isValidInstruction(*I, Context)) {
         Context.IsInvalid = true;
-        if (!KeepGoing)
+        if (!KeepGoing) {
           return false;
+        }
       }
+    }
   }
 
-  if (!hasAffineMemoryAccesses(Context))
+  // Finally, check that all memory accesses in the region can either be
+  // represented as affine accesses or be handled according to our
+  // configuration (e.g., non-affine handling when allowed).
+  if (!hasAffineMemoryAccesses(Context)) {
     return false;
+  }
 
   return true;
 }

--- a/polly/lib/Analysis/DependenceInfo.cpp	2025-11-16 14:59:12.228534011 +0200
+++ b/polly/lib/Analysis/DependenceInfo.cpp	2025-11-16 15:16:14.364885868 +0200
@@ -313,9 +313,12 @@ static __isl_give isl_union_flow *buildF
 }
 
 void Dependences::calculateDependences(Scop &S) {
-  isl_union_map *Read, *MustWrite, *MayWrite, *ReductionTagMap;
-  isl_schedule *Schedule;
-  isl_union_set *TaggedStmtDomain;
+  isl_union_map *Read = nullptr;
+  isl_union_map *MustWrite = nullptr;
+  isl_union_map *MayWrite = nullptr;
+  isl_union_map *ReductionTagMap = nullptr;
+  isl_schedule *Schedule = nullptr;
+  isl_union_set *TaggedStmtDomain = nullptr;
 
   POLLY_DEBUG(dbgs() << "Scop: \n" << S << "\n");
 
@@ -334,7 +337,8 @@ void Dependences::calculateDependences(S
 
   if (!HasReductions) {
     isl_union_map_free(ReductionTagMap);
-    // Tag the schedule tree if we want fine-grain dependence info
+
+    // Tag the schedule tree if we want fine-grain dependence info.
     if (Level > AL_Statement) {
       auto TaggedMap =
           isl_union_set_unwrap(isl_union_set_copy(TaggedStmtDomain));
@@ -342,25 +346,28 @@ void Dependences::calculateDependences(S
       Schedule = isl_schedule_pullback_union_pw_multi_aff(Schedule, Tags);
     }
   } else {
-    isl_union_map *IdentityMap;
-    isl_union_pw_multi_aff *ReductionTags, *IdentityTags, *Tags;
+    isl_union_map *IdentityMap = nullptr;
+    isl_union_pw_multi_aff *ReductionTags = nullptr;
+    isl_union_pw_multi_aff *IdentityTags = nullptr;
+    isl_union_pw_multi_aff *Tags = nullptr;
 
-    // Extract Reduction tags from the combined access domains in the given
+    // Extract reduction tags from the combined access domains in the given
     // SCoP. The result is a map that maps each tagged element in the domain to
-    // the memory location it accesses. ReductionTags = {[Stmt[i] ->
-    // Array[f(i)]] -> Stmt[i] }
+    // the memory location it accesses:
+    //   ReductionTags = { [Stmt[i] -> Array[f(i)]] -> Stmt[i] }
     ReductionTags =
         isl_union_map_domain_map_union_pw_multi_aff(ReductionTagMap);
 
-    // Compute an identity map from each statement in domain to itself.
-    // IdentityTags = { [Stmt[i] -> Stmt[i] }
-    IdentityMap = isl_union_set_identity(isl_union_set_copy(TaggedStmtDomain));
+    // Compute an identity map from each statement in the domain to itself:
+    //   IdentityTags = { [Stmt[i] -> Stmt[i]] }
+    IdentityMap =
+        isl_union_set_identity(isl_union_set_copy(TaggedStmtDomain));
     IdentityTags = isl_union_pw_multi_aff_from_union_map(IdentityMap);
 
     Tags = isl_union_pw_multi_aff_union_add(ReductionTags, IdentityTags);
 
     // By pulling back Tags from Schedule, we have a schedule tree that can
-    // be used to compute normal dependences, as well as 'tagged' reduction
+    // be used to compute normal dependences as well as 'tagged' reduction
     // dependences.
     Schedule = isl_schedule_pullback_union_pw_multi_aff(Schedule, Tags);
   }
@@ -371,77 +378,25 @@ void Dependences::calculateDependences(S
               dbgs() << "Schedule: " << Schedule << "\n");
 
   isl_union_map *StrictWAW = nullptr;
+
   {
+    // Bound the number of ISL operations in this block.
     IslMaxOperationsGuard MaxOpGuard(IslCtx.get(), OptComputeOut);
 
     RAW = WAW = WAR = RED = nullptr;
-    isl_union_map *Write = isl_union_map_union(isl_union_map_copy(MustWrite),
-                                               isl_union_map_copy(MayWrite));
 
-    // We are interested in detecting reductions that do not have intermediate
-    // computations that are captured by other statements.
-    //
-    // Example:
-    // void f(int *A, int *B) {
-    //     for(int i = 0; i <= 100; i++) {
-    //
-    //            *-WAR (S0[i] -> S0[i + 1] 0 <= i <= 100)------------*
-    //            |                                                   |
-    //            *-WAW (S0[i] -> S0[i + 1] 0 <= i <= 100)------------*
-    //            |                                                   |
-    //            v                                                   |
-    //     S0:    *A += i; >------------------*-----------------------*
-    //                                        |
-    //         if (i >= 98) {          WAR (S0[i] -> S1[i]) 98 <= i <= 100
-    //                                        |
-    //     S1:        *B = *A; <--------------*
-    //         }
-    //     }
-    // }
-    //
-    // S0[0 <= i <= 100] has a reduction. However, the values in
-    // S0[98 <= i <= 100] is captured in S1[98 <= i <= 100].
-    // Since we allow free reordering on our reduction dependences, we need to
-    // remove all instances of a reduction statement that have data dependences
-    // originating from them.
-    // In the case of the example, we need to remove S0[98 <= i <= 100] from
-    // our reduction dependences.
-    //
-    // When we build up the WAW dependences that are used to detect reductions,
-    // we consider only **Writes that have no intermediate Reads**.
-    //
-    // `isl_union_flow_get_must_dependence` gives us dependences of the form:
-    // (sink <- must_source).
-    //
-    // It *will not give* dependences of the form:
-    // 1. (sink <- ... <- may_source <- ... <- must_source)
-    // 2. (sink <- ... <- must_source <- ... <- must_source)
-    //
-    // For a detailed reference on ISL's flow analysis, see:
-    // "Presburger Formulas and Polyhedral Compilation" - Approximate Dataflow
-    //  Analysis.
-    //
-    // Since we set "Write" as a must-source, "Read" as a may-source, and ask
-    // for must dependences, we get all Writes to Writes that **do not flow
-    // through a Read**.
-    //
-    // ScopInfo::checkForReductions makes sure that if something captures
-    // the reduction variable in the same basic block, then it is rejected
-    // before it is even handed here. This makes sure that there is exactly
-    // one read and one write to a reduction variable in a Statement.
-    // Example:
-    //     void f(int *sum, int A[N], int B[N]) {
-    //       for (int i = 0; i < N; i++) {
-    //         *sum += A[i]; < the store and the load is not tagged as a
-    //         B[i] = *sum;  < reduction-like access due to the overlap.
-    //       }
-    //     }
-
-    isl_union_flow *Flow = buildFlow(Write, Write, Read, nullptr, Schedule);
+    // Union of all write accesses.
+    isl_union_map *Write = isl_union_map_union(
+        isl_union_map_copy(MustWrite), isl_union_map_copy(MayWrite));
+
+    // StrictWAW: WAW dependences that do not flow through a Read.
+    isl_union_flow *Flow =
+        buildFlow(Write, Write, Read, nullptr, Schedule);
     StrictWAW = isl_union_flow_get_must_dependence(Flow);
     isl_union_flow_free(Flow);
 
     if (OptAnalysisType == VALUE_BASED_ANALYSIS) {
+      // RAW, WAW, WAR for value-based analysis.
       Flow = buildFlow(Read, MustWrite, MayWrite, nullptr, Schedule);
       RAW = isl_union_flow_get_may_dependence(Flow);
       isl_union_flow_free(Flow);
@@ -450,12 +405,12 @@ void Dependences::calculateDependences(S
       WAW = isl_union_flow_get_may_dependence(Flow);
       isl_union_flow_free(Flow);
 
-      // ISL now supports "kills" in approximate dataflow analysis, we can
-      // specify the MustWrite as kills, Read as source and Write as sink.
+      // WAR: using MustWrite as "kills".
       Flow = buildFlow(Write, nullptr, Read, MustWrite, Schedule);
       WAR = isl_union_flow_get_may_dependence(Flow);
       isl_union_flow_free(Flow);
     } else {
+      // RAW, WAR, WAW for memory-based over-approximation.
       Flow = buildFlow(Read, nullptr, Write, nullptr, Schedule);
       RAW = isl_union_flow_get_may_dependence(Flow);
       isl_union_flow_free(Flow);
@@ -475,40 +430,71 @@ void Dependences::calculateDependences(S
     isl_union_map_free(Read);
     isl_schedule_free(Schedule);
 
-    RAW = isl_union_map_coalesce(RAW);
-    WAW = isl_union_map_coalesce(WAW);
-    WAR = isl_union_map_coalesce(WAR);
-
+    // Coalesce only if the maps are non-null; if any operation failed,
+    // we will detect that after this guarded section.
+    if (RAW)
+      RAW = isl_union_map_coalesce(RAW);
+    if (WAW)
+      WAW = isl_union_map_coalesce(WAW);
+    if (WAR)
+      WAR = isl_union_map_coalesce(WAR);
     // End of max_operations scope.
   }
 
-  if (isl_ctx_last_error(IslCtx.get()) == isl_error_quota) {
+  isl_ctx *Ctx = IslCtx.get();
+
+  // Handle quota exhaustion explicitly: analysis did not complete.
+  if (isl_ctx_last_error(Ctx) == isl_error_quota) {
+    isl_union_map_free(RAW);
+    isl_union_map_free(WAW);
+    isl_union_map_free(WAR);
+    isl_union_map_free(StrictWAW);
+    RAW = WAW = WAR = StrictWAW = nullptr;
+
+    isl_union_set_free(TaggedStmtDomain);
+    TaggedStmtDomain = nullptr;
+
+    isl_ctx_reset_error(Ctx);
+    // Leave RED/TC_RED as nullptr; hasValidDependences() will report false.
+    return;
+  }
+
+  // If any of the core dependence maps failed to compute, treat the analysis
+  // as failed and stop here.
+  if (!RAW || !WAW || !WAR || !StrictWAW) {
     isl_union_map_free(RAW);
     isl_union_map_free(WAW);
     isl_union_map_free(WAR);
     isl_union_map_free(StrictWAW);
     RAW = WAW = WAR = StrictWAW = nullptr;
-    isl_ctx_reset_error(IslCtx.get());
+
+    isl_union_set_free(TaggedStmtDomain);
+    TaggedStmtDomain = nullptr;
+
+    isl_ctx_reset_error(Ctx);
+    return;
   }
 
-  // Drop out early, as the remaining computations are only needed for
-  // reduction dependences or dependences that are finer than statement
-  // level dependences.
+  // Drop out early if there are no reductions and we only need statement-level
+  // dependences. In this case reduction-specific information is not required.
   if (!HasReductions && Level == AL_Statement) {
     RED = isl_union_map_empty(isl_union_map_get_space(RAW));
-    TC_RED = isl_union_map_empty(isl_union_set_get_space(TaggedStmtDomain));
+    TC_RED =
+        isl_union_map_empty(isl_union_set_get_space(TaggedStmtDomain));
     isl_union_set_free(TaggedStmtDomain);
     isl_union_map_free(StrictWAW);
     return;
   }
 
-  isl_union_map *STMT_RAW, *STMT_WAW, *STMT_WAR;
-  STMT_RAW = isl_union_map_intersect_domain(
+  // Restrict dependences to the tagged statement domain for statement-level
+  // views (STMT_*).
+  isl_union_map *STMT_RAW = isl_union_map_intersect_domain(
       isl_union_map_copy(RAW), isl_union_set_copy(TaggedStmtDomain));
-  STMT_WAW = isl_union_map_intersect_domain(
+  isl_union_map *STMT_WAW = isl_union_map_intersect_domain(
       isl_union_map_copy(WAW), isl_union_set_copy(TaggedStmtDomain));
-  STMT_WAR =
-      isl_union_map_intersect_domain(isl_union_map_copy(WAR), TaggedStmtDomain);
+  isl_union_map *STMT_WAR = isl_union_map_intersect_domain(
+      isl_union_map_copy(WAR), TaggedStmtDomain);
+
   POLLY_DEBUG({
     dbgs() << "Wrapped Dependences:\n";
     dump();
@@ -516,47 +502,43 @@ void Dependences::calculateDependences(S
   });
 
   // To handle reduction dependences we proceed as follows:
-  // 1) Aggregate all possible reduction dependences, namely all self
-  //    dependences on reduction like statements.
-  // 2) Intersect them with the actual RAW & WAW dependences to the get the
-  //    actual reduction dependences. This will ensure the load/store memory
-  //    addresses were __identical__ in the two iterations of the statement.
-  // 3) Relax the original RAW, WAW and WAR dependences by subtracting the
-  //    actual reduction dependences. Binary reductions (sum += A[i]) cause
-  //    the same, RAW, WAW and WAR dependences.
-  // 4) Add the privatization dependences which are widened versions of
-  //    already present dependences. They model the effect of manual
-  //    privatization at the outermost possible place (namely after the last
-  //    write and before the first access to a reduction location).
+  // 1) Aggregate all possible reduction dependences: self-dependences on
+  //    reduction-like statements.
+  // 2) Intersect them with actual RAW & WAW dependences to get true reduction
+  //    dependences (identical memory addresses in the two iterations).
+  // 3) Relax RAW, WAW, and WAR dependences by subtracting the reduction deps.
+  // 4) Add privatization dependences, widened versions of reduction deps.
 
-  // Step 1)
+  // Step 1) Build candidate reduction dependences in wrapped access space.
   RED = isl_union_map_empty(isl_union_map_get_space(RAW));
   for (ScopStmt &Stmt : S) {
     for (MemoryAccess *MA : Stmt) {
       if (!MA->isReductionLike())
         continue;
-      isl_set *AccDomW = isl_map_wrap(MA->getAccessRelation().release());
-      isl_map *Identity =
-          isl_map_from_domain_and_range(isl_set_copy(AccDomW), AccDomW);
+
+      isl_set *AccDomW =
+          isl_map_wrap(MA->getAccessRelation().release());
+      isl_map *Identity = isl_map_from_domain_and_range(
+          isl_set_copy(AccDomW), AccDomW);
       RED = isl_union_map_add_map(RED, Identity);
     }
   }
 
-  // Step 2)
+  // Step 2) Intersect with actual RAW and StrictWAW to get true reductions.
   RED = isl_union_map_intersect(RED, isl_union_map_copy(RAW));
   RED = isl_union_map_intersect(RED, StrictWAW);
 
   if (!isl_union_map_is_empty(RED)) {
-
-    // Step 3)
+    // Step 3) Relax original dependences by subtracting reduction deps.
     RAW = isl_union_map_subtract(RAW, isl_union_map_copy(RED));
     WAW = isl_union_map_subtract(WAW, isl_union_map_copy(RED));
     WAR = isl_union_map_subtract(WAR, isl_union_map_copy(RED));
 
-    // Step 4)
+    // Step 4) Add privatization dependences (widened reduction deps).
     addPrivatizationDependences();
-  } else
+  } else {
     TC_RED = isl_union_map_empty(isl_union_map_get_space(RED));
+  }
 
   POLLY_DEBUG({
     dbgs() << "Final Wrapped Dependences:\n";
@@ -564,33 +546,35 @@ void Dependences::calculateDependences(S
     dbgs() << "\n";
   });
 
-  // RED_SIN is used to collect all reduction dependences again after we
-  // split them according to the causing memory accesses. The current assumption
-  // is that our method of splitting will not have any leftovers. In the end
-  // we validate this assumption until we have more confidence in this method.
-  isl_union_map *RED_SIN = isl_union_map_empty(isl_union_map_get_space(RAW));
-
-  // For each reduction like memory access, check if there are reduction
-  // dependences with the access relation of the memory access as a domain
-  // (wrapped space!). If so these dependences are caused by this memory access.
-  // We then move this portion of reduction dependences back to the statement ->
-  // statement space and add a mapping from the memory access to these
-  // dependences.
+  // RED_SIN is used to collect all reduction dependences again after we split
+  // them according to the causing memory accesses. The assumption is that our
+  // method of splitting will not have any leftovers. We validate this
+  // assumption here.
+  isl_union_map *RED_SIN =
+      isl_union_map_empty(isl_union_map_get_space(RAW));
+
+  // For each reduction-like memory access, check if there are reduction
+  // dependences with the access relation as domain (wrapped space!). If so,
+  // these are caused by this memory access. We then move this portion of
+  // reduction dependences back to statement->statement space and record it.
   for (ScopStmt &Stmt : S) {
     for (MemoryAccess *MA : Stmt) {
       if (!MA->isReductionLike())
         continue;
 
-      isl_set *AccDomW = isl_map_wrap(MA->getAccessRelation().release());
+      isl_set *AccDomW =
+          isl_map_wrap(MA->getAccessRelation().release());
       isl_union_map *AccRedDepU = isl_union_map_intersect_domain(
-          isl_union_map_copy(TC_RED), isl_union_set_from_set(AccDomW));
+          isl_union_map_copy(TC_RED),
+          isl_union_set_from_set(AccDomW));
       if (isl_union_map_is_empty(AccRedDepU)) {
         isl_union_map_free(AccRedDepU);
         continue;
       }
 
       isl_map *AccRedDep = isl_map_from_union_map(AccRedDepU);
-      RED_SIN = isl_union_map_add_map(RED_SIN, isl_map_copy(AccRedDep));
+      RED_SIN = isl_union_map_add_map(RED_SIN,
+                                      isl_map_copy(AccRedDep));
       AccRedDep = isl_map_zip(AccRedDep);
       AccRedDep = isl_set_unwrap(isl_map_domain(AccRedDep));
       setReductionDependences(MA, AccRedDep);
@@ -598,10 +582,10 @@ void Dependences::calculateDependences(S
   }
 
   assert(isl_union_map_is_equal(RED_SIN, TC_RED) &&
-         "Intersecting the reduction dependence domain with the wrapped access "
-         "relation is not enough, we need to loosen the access relation also");
+         "Splitting reduction dependences by access left unmatched parts");
   isl_union_map_free(RED_SIN);
 
+  // Zip dependences into [Domain -> Range] pairs.
   RAW = isl_union_map_zip(RAW);
   WAW = isl_union_map_zip(WAW);
   WAR = isl_union_map_zip(WAR);
@@ -614,6 +598,7 @@ void Dependences::calculateDependences(S
     dbgs() << "\n";
   });
 
+  // Unwrap to statement/access-level domains.
   RAW = isl_union_set_unwrap(isl_union_map_domain(RAW));
   WAW = isl_union_set_unwrap(isl_union_map_domain(WAW));
   WAR = isl_union_set_unwrap(isl_union_map_domain(WAR));
@@ -626,10 +611,12 @@ void Dependences::calculateDependences(S
     dbgs() << "\n";
   });
 
+  // Merge statement-only dependences back in.
   RAW = isl_union_map_union(RAW, STMT_RAW);
   WAW = isl_union_map_union(WAW, STMT_WAW);
   WAR = isl_union_map_union(WAR, STMT_WAR);
 
+  // Final coalesce.
   RAW = isl_union_map_coalesce(RAW);
   WAW = isl_union_map_coalesce(WAW);
   WAR = isl_union_map_coalesce(WAR);
@@ -799,8 +786,10 @@ void Dependences::releaseMemory() {
 
 isl::union_map Dependences::getDependences(int Kinds) const {
   assert(hasValidDependences() && "No valid dependences available");
+
+  // Use RAW's space as a representative for the overall dependence space.
   isl::space Space = isl::manage_copy(RAW).get_space();
-  isl::union_map Deps = Deps.empty(Space.ctx());
+  isl::union_map Deps = isl::union_map::empty(Space.ctx());
 
   if (Kinds & TYPE_RAW)
     Deps = Deps.unite(isl::manage_copy(RAW));

--- a/polly/lib/Transform/DeadCodeElimination.cpp	2025-09-21 17:32:16.072456710 +0200
+++ b/polly/lib/Transform/DeadCodeElimination.cpp	2025-09-21 17:32:47.327399217 +0200

--- a/polly/lib/Transform/DeLICM.cpp	2025-09-21 16:25:17.318367274 +0200
+++ b/polly/lib/Transform/DeLICM.cpp	2025-09-21 16:26:08.074761600 +0200

--- a/polly/lib/Transform/ForwardOpTree.cpp	2025-09-21 16:10:05.509588643 +0200
+++ b/polly/lib/Transform/ForwardOpTree.cpp	2025-11-16 16:14:13.166720281 +0200
@@ -359,20 +359,34 @@ public:
       IslQuotaScope QuotaScope = MaxOpGuard.enter();
 
       computeCommon();
-      if (NormalizePHIs)
+      if (NormalizePHIs) {
         computeNormalizedPHIs();
-      Known = computeKnown(true, true);
+      }
+
+      // Compute the map of known array contents.
+      Known = computeKnown(/*IncludeMustWrites=*/true,
+                          /*IncludeMayWrites=*/true);
 
       // Preexisting ValInsts use the known content analysis of themselves.
-      Translator = makeIdentityMap(Known.range(), false);
+      // Only build the identity translator if Known was successfully computed.
+      if (!Known.is_null()) {
+        Translator = makeIdentityMap(Known.range(), /*AlignParams=*/false);
+      }
     }
 
+    // If any of the core analysis maps are unusable, treat the analysis as failed.
     if (Known.is_null() || Translator.is_null() || NormalizeMap.is_null()) {
-      assert(isl_ctx_last_error(IslCtx.get()) == isl_error_quota);
+      // In practice this is expected to be a quota error, but in release builds
+      // we handle it gracefully regardless of the exact isl error code.
+      assert(isl_ctx_last_error(IslCtx.get()) == isl_error_quota &&
+            "Known analysis failed for a reason other than quota exhaustion");
+
       Known = {};
       Translator = {};
       NormalizeMap = {};
-      POLLY_DEBUG(dbgs() << "Known analysis exceeded max_operations\n");
+
+      POLLY_DEBUG(
+          dbgs() << "Known analysis exceeded max_operations or failed\n");
       return false;
     }
 
@@ -418,27 +432,37 @@ public:
   /// @param AccessRelation The array element that each statement instance
   ///                       accesses.
   ///
-  /// @param The newly created access.
+  /// @return The newly created access.
   MemoryAccess *makeReadArrayAccess(ScopStmt *Stmt, LoadInst *LI,
                                     isl::map AccessRelation) {
+    // The output tuple id of the access relation is expected to carry the
+    // ScopArrayInfo object as its user pointer.
     isl::id ArrayId = AccessRelation.get_tuple_id(isl::dim::out);
-    ScopArrayInfo *SAI = reinterpret_cast<ScopArrayInfo *>(ArrayId.get_user());
+    auto *SAI =
+        static_cast<ScopArrayInfo *>(ArrayId.get_user());
 
-    // Create a dummy SCEV access, to be replaced anyway.
+    assert(SAI && "AccessRelation output id must reference a ScopArrayInfo");
+
+    // Create a dummy SCEV access, to be replaced by the finalized isl access
+    // relation. We initialize the Sizes with the array's dimension sizes as
+    // expected by MemoryAccess.
     SmallVector<const SCEV *, 4> Sizes;
     Sizes.reserve(SAI->getNumberOfDimensions());
-    SmallVector<const SCEV *, 4> Subscripts;
-    Subscripts.reserve(SAI->getNumberOfDimensions());
-    for (unsigned i = 0; i < SAI->getNumberOfDimensions(); i += 1) {
+
+    for (unsigned i = 0, e = SAI->getNumberOfDimensions(); i != e; ++i) {
       Sizes.push_back(SAI->getDimensionSize(i));
-      Subscripts.push_back(nullptr);
     }
 
-    MemoryAccess *Access =
-        new MemoryAccess(Stmt, LI, MemoryAccess::READ, SAI->getBasePtr(),
-                         LI->getType(), true, {}, Sizes, LI, MemoryKind::Array);
+    // Create a new array read MemoryAccess. We pass an empty access function
+    // (to be defined by the isl relation) and the dimension Sizes. The LI is
+    // used as the access instruction and debug reference.
+    MemoryAccess *Access = new MemoryAccess(
+        Stmt, LI, MemoryAccess::READ, SAI->getBasePtr(), LI->getType(),
+        /*IsAffine=*/true,
+        /*AccessFunction=*/{}, Sizes, LI, MemoryKind::Array);
+
     S->addAccessFunction(Access);
-    Stmt->addAccess(Access, true);
+    Stmt->addAccess(Access, /*IsNew=*/true);
 
     Access->setNewAccessRelation(AccessRelation);
 
--- a/polly/lib/Transform/ScheduleOptimizer.cpp	2025-07-13 23:25:57.922600146 +0200
+++ b/polly/lib/Transform/ScheduleOptimizer.cpp	2025-07-13 23:27:47.918372394 +0200
