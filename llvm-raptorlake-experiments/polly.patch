--- a/polly/lib/Analysis/ScopDetection.cpp	2025-09-21 15:28:29.292203509 +0200
+++ b/polly/lib/Analysis/ScopDetection.cpp	2025-09-21 15:35:24.476840546 +0200
@@ -1,46 +1,10 @@
 //===- ScopDetection.cpp - Detect Scops -----------------------------------===//
 //
-// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
-// See https://llvm.org/LICENSE.txt for license information.
+// Part of the the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
 // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 //
-//===----------------------------------------------------------------------===//
-//
 // Detect the maximal Scops of a function.
 //
-// A static control part (Scop) is a subgraph of the control flow graph (CFG)
-// that only has statically known control flow and can therefore be described
-// within the polyhedral model.
-//
-// Every Scop fulfills these restrictions:
-//
-// * It is a single entry single exit region
-//
-// * Only affine linear bounds in the loops
-//
-// Every natural loop in a Scop must have a number of loop iterations that can
-// be described as an affine linear function in surrounding loop iterators or
-// parameters. (A parameter is a scalar that does not change its value during
-// execution of the Scop).
-//
-// * Only comparisons of affine linear expressions in conditions
-//
-// * All loops and conditions perfectly nested
-//
-// The control flow needs to be structured such that it could be written using
-// just 'for' and 'if' statements, without the need for any 'goto', 'break' or
-// 'continue'.
-//
-// * Side effect free functions call
-//
-// Function calls and intrinsics that do not have side effects (readnone)
-// or memory intrinsics (memset, memcpy, memmove) are allowed.
-//
-// The Scop detection finds the largest Scops by checking if the largest
-// region is a Scop. If this is not the case, its canonical subregions are
-// checked until a region is a Scop. It is now tried to extend this Scop by
-// creating a larger non canonical region.
-//
 //===----------------------------------------------------------------------===//
 
 #include "polly/ScopDetection.h"
@@ -50,7 +14,10 @@
 #include "polly/Support/SCEVValidator.h"
 #include "polly/Support/ScopHelper.h"
 #include "polly/Support/ScopLocation.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/SetVector.h"
 #include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/ADT/SmallVector.h"
 #include "llvm/ADT/Statistic.h"
 #include "llvm/Analysis/AliasAnalysis.h"
 #include "llvm/Analysis/Delinearization.h"
@@ -60,6 +27,7 @@
 #include "llvm/Analysis/RegionInfo.h"
 #include "llvm/Analysis/ScalarEvolution.h"
 #include "llvm/Analysis/ScalarEvolutionExpressions.h"
+#include "llvm/IR/Attributes.h"
 #include "llvm/IR/BasicBlock.h"
 #include "llvm/IR/DebugLoc.h"
 #include "llvm/IR/DerivedTypes.h"
@@ -83,7 +51,6 @@
 #include <algorithm>
 #include <cassert>
 #include <memory>
-#include <stack>
 #include <string>
 #include <utility>
 #include <vector>
@@ -94,38 +61,47 @@ using namespace polly;
 #include "polly/Support/PollyDebug.h"
 #define DEBUG_TYPE "polly-detect"
 
-// This option is set to a very high value, as analyzing such loops increases
-// compile time on several cases. For experiments that enable this option,
-// a value of around 40 has been working to avoid run-time regressions with
-// Polly while still exposing interesting optimization opportunities.
 static cl::opt<int> ProfitabilityMinPerLoopInstructions(
     "polly-detect-profitability-min-per-loop-insts",
     cl::desc("The minimal number of per-loop instructions before a single loop "
              "region is considered profitable"),
     cl::Hidden, cl::ValueRequired, cl::init(100000000), cl::cat(PollyCategory));
 
+static cl::opt<unsigned> PollyScopsMaxBlocks(
+    "polly-scops-max-blocks",
+    cl::desc("Maximum number of basic blocks in a function for Scop detection"),
+    cl::Hidden, cl::init(20000), cl::cat(PollyCategory));
+
+static cl::opt<unsigned> PollyMaxSubRegions(
+    "polly-max-subregions",
+    cl::desc("Maximum number of sub-regions considered recursively"),
+    cl::Hidden, cl::init(512), cl::cat(PollyCategory));
+
+static cl::opt<unsigned> PollyMinMemops(
+    "polly-min-memops",
+    cl::desc("Minimal count of memory operations in a single-loop Scop"),
+    cl::Hidden, cl::ValueRequired, cl::init(8), cl::cat(PollyCategory));
+
+static cl::opt<unsigned> PollyMaxSwitchCases(
+    "polly-max-switch-cases",
+    cl::desc("Maximum number of cases in a switch to be considered"),
+    cl::Hidden, cl::init(32), cl::cat(PollyCategory));
+
 bool polly::PollyProcessUnprofitable;
 
 static cl::opt<bool, true> XPollyProcessUnprofitable(
     "polly-process-unprofitable",
-    cl::desc(
-        "Process scops that are unlikely to benefit from Polly optimizations."),
+    cl::desc("Process scops that are unlikely to benefit from Polly optimizations."),
     cl::location(PollyProcessUnprofitable), cl::cat(PollyCategory));
 
 static cl::list<std::string> OnlyFunctions(
     "polly-only-func",
-    cl::desc("Only run on functions that match a regex. "
-             "Multiple regexes can be comma separated. "
-             "Scop detection will run on all functions that match "
-             "ANY of the regexes provided."),
+    cl::desc("Only run on functions that match a regex. Multiple regexes can be comma separated. Scop detection will run on all functions that match ANY of the regexes provided."),
     cl::CommaSeparated, cl::cat(PollyCategory));
 
 static cl::list<std::string> IgnoredFunctions(
     "polly-ignore-func",
-    cl::desc("Ignore functions that match a regex. "
-             "Multiple regexes can be comma separated. "
-             "Scop detection will ignore all functions that match "
-             "ANY of the regexes provided."),
+    cl::desc("Ignore functions that match a regex. Multiple regexes can be comma separated. Scop detection will ignore all functions that match ANY of the regexes provided."),
     cl::CommaSeparated, cl::cat(PollyCategory));
 
 bool polly::PollyAllowFullFunction;
@@ -138,8 +114,7 @@ static cl::opt<bool, true>
 
 static cl::opt<std::string> OnlyRegion(
     "polly-only-region",
-    cl::desc("Only run on certain regions (The provided identifier must "
-             "appear in the name of the region's entry block"),
+    cl::desc("Only run on certain regions (identifier must appear in the region entry block name)"),
     cl::value_desc("identifier"), cl::ValueRequired, cl::init(""),
     cl::cat(PollyCategory));
 
@@ -171,8 +146,8 @@ static cl::opt<bool>
 
 static cl::opt<bool> AllowDifferentTypes(
     "polly-allow-differing-element-types",
-    cl::desc("Allow different element types for array accesses"), cl::Hidden,
-    cl::init(true), cl::cat(PollyCategory));
+    cl::desc("Allow different element types for array accesses"),
+    cl::Hidden, cl::init(true), cl::cat(PollyCategory));
 
 static cl::opt<bool>
     AllowNonAffine("polly-allow-nonaffine",
@@ -186,8 +161,8 @@ static cl::opt<bool>
 
 static cl::opt<bool> AllowNonAffineSubRegions(
     "polly-allow-nonaffine-branches",
-    cl::desc("Allow non affine conditions for branches"), cl::Hidden,
-    cl::init(true), cl::cat(PollyCategory));
+    cl::desc("Allow non affine conditions for branches"),
+    cl::Hidden, cl::init(true), cl::cat(PollyCategory));
 
 static cl::opt<bool>
     AllowNonAffineSubLoops("polly-allow-nonaffine-loops",
@@ -228,16 +203,12 @@ static cl::opt<bool> PollyAllowErrorBloc
     cl::desc("Allow to speculate on the execution of 'error blocks'."),
     cl::Hidden, cl::init(true), cl::cat(PollyCategory));
 
-/// The minimal trip count under which loops are considered unprofitable.
 static const unsigned MIN_LOOP_TRIP_COUNT = 8;
 
 bool polly::PollyTrackFailures = false;
 bool polly::PollyDelinearize = false;
 StringRef polly::PollySkipFnAttr = "polly.skip.fn";
 
-//===----------------------------------------------------------------------===//
-// Statistics.
-
 STATISTIC(NumScopRegions, "Number of scops");
 STATISTIC(NumLoopsInScop, "Number of loops in scops");
 STATISTIC(NumScopsDepthZero, "Number of scops with maximal loop depth 0");
@@ -246,33 +217,21 @@ STATISTIC(NumScopsDepthTwo, "Number of s
 STATISTIC(NumScopsDepthThree, "Number of scops with maximal loop depth 3");
 STATISTIC(NumScopsDepthFour, "Number of scops with maximal loop depth 4");
 STATISTIC(NumScopsDepthFive, "Number of scops with maximal loop depth 5");
-STATISTIC(NumScopsDepthLarger,
-          "Number of scops with maximal loop depth 6 and larger");
+STATISTIC(NumScopsDepthLarger, "Number of scops with maximal loop depth 6 and larger");
 STATISTIC(NumProfScopRegions, "Number of scops (profitable scops only)");
-STATISTIC(NumLoopsInProfScop,
-          "Number of loops in scops (profitable scops only)");
+STATISTIC(NumLoopsInProfScop, "Number of loops in scops (profitable scops only)");
 STATISTIC(NumLoopsOverall, "Number of total loops");
-STATISTIC(NumProfScopsDepthZero,
-          "Number of scops with maximal loop depth 0 (profitable scops only)");
-STATISTIC(NumProfScopsDepthOne,
-          "Number of scops with maximal loop depth 1 (profitable scops only)");
-STATISTIC(NumProfScopsDepthTwo,
-          "Number of scops with maximal loop depth 2 (profitable scops only)");
-STATISTIC(NumProfScopsDepthThree,
-          "Number of scops with maximal loop depth 3 (profitable scops only)");
-STATISTIC(NumProfScopsDepthFour,
-          "Number of scops with maximal loop depth 4 (profitable scops only)");
-STATISTIC(NumProfScopsDepthFive,
-          "Number of scops with maximal loop depth 5 (profitable scops only)");
-STATISTIC(NumProfScopsDepthLarger,
-          "Number of scops with maximal loop depth 6 and larger "
-          "(profitable scops only)");
+STATISTIC(NumProfScopsDepthZero, "Number of scops with maximal loop depth 0 (profitable scops only)");
+STATISTIC(NumProfScopsDepthOne, "Number of scops with maximal loop depth 1 (profitable scops only)");
+STATISTIC(NumProfScopsDepthTwo, "Number of scops with maximal loop depth 2 (profitable scops only)");
+STATISTIC(NumProfScopsDepthThree, "Number of scops with maximal loop depth 3 (profitable scops only)");
+STATISTIC(NumProfScopsDepthFour, "Number of scops with maximal loop depth 4 (profitable scops only)");
+STATISTIC(NumProfScopsDepthFive, "Number of scops with maximal loop depth 5 (profitable scops only)");
+STATISTIC(NumProfScopsDepthLarger, "Number of scops with maximal loop depth 6 and larger (profitable scops only)");
 STATISTIC(MaxNumLoopsInScop, "Maximal number of loops in scops");
-STATISTIC(MaxNumLoopsInProfScop,
-          "Maximal number of loops in scops (profitable scops only)");
+STATISTIC(MaxNumLoopsInProfScop, "Maximal number of loops in scops (profitable scops only)");
 
-static void updateLoopCountStatistic(ScopDetection::LoopStats Stats,
-                                     bool OnlyProfitable);
+static void updateLoopCountStatistic(ScopDetection::LoopStats Stats, bool OnlyProfitable);
 
 namespace {
 
@@ -285,304 +244,286 @@ private:
   unsigned EntryLine, ExitLine;
 
 public:
-  DiagnosticScopFound(Function &F, std::string FileName, unsigned EntryLine,
-                      unsigned ExitLine)
-      : DiagnosticInfo(PluginDiagnosticKind, DS_Note), F(F), FileName(FileName),
-        EntryLine(EntryLine), ExitLine(ExitLine) {}
+  DiagnosticScopFound(Function &F, std::string FileName, unsigned EntryLine, unsigned ExitLine)
+      : DiagnosticInfo(PluginDiagnosticKind, DS_Note), F(F), FileName(FileName), EntryLine(EntryLine),
+        ExitLine(ExitLine) {}
 
   void print(DiagnosticPrinter &DP) const override;
 
-  static bool classof(const DiagnosticInfo *DI) {
-    return DI->getKind() == PluginDiagnosticKind;
-  }
+  static bool classof(const DiagnosticInfo *DI) { return DI->getKind() == PluginDiagnosticKind; }
 };
+
 } // namespace
 
-int DiagnosticScopFound::PluginDiagnosticKind =
-    getNextAvailablePluginDiagnosticKind();
+int DiagnosticScopFound::PluginDiagnosticKind = getNextAvailablePluginDiagnosticKind();
 
 void DiagnosticScopFound::print(DiagnosticPrinter &DP) const {
-  DP << "Polly detected an optimizable loop region (scop) in function '" << F
-     << "'\n";
-
+  DP << "Polly detected an optimizable loop region (scop) in function '" << F << "'\n";
   if (FileName.empty()) {
-    DP << "Scop location is unknown. Compile with debug info "
-          "(-g) to get more precise information. ";
+    DP << "Scop location is unknown. Compile with debug info (-g).";
     return;
   }
-
   DP << FileName << ":" << EntryLine << ": Start of scop\n";
   DP << FileName << ":" << ExitLine << ": End of scop";
 }
 
-/// Check if a string matches any regex in a list of regexes.
-/// @param Str the input string to match against.
-/// @param RegexList a list of strings that are regular expressions.
-static bool doesStringMatchAnyRegex(StringRef Str,
-                                    const cl::list<std::string> &RegexList) {
-  for (auto RegexStr : RegexList) {
-    Regex R(RegexStr);
-
-    std::string Err;
-    if (!R.isValid(Err))
-      report_fatal_error(Twine("invalid regex given as input to polly: ") + Err,
-                         true);
+static bool doesStringMatchAnyRegex(StringRef Str, const cl::list<std::string> &RegexList) {
+  using RegexVec = std::vector<Regex>;
+  static thread_local DenseMap<const void *, RegexVec> Cache;
+
+  auto It = Cache.find(&RegexList);
+  if (It == Cache.end()) {
+    RegexVec RV;
+    RV.reserve(RegexList.size());
+    for (const auto &RegexStr : RegexList) {
+      Regex R(RegexStr);
+      std::string Err;
+      if (!R.isValid(Err)) {
+        report_fatal_error(Twine("invalid regex given as input to polly: ") + Err, true);
+      }
+      RV.push_back(std::move(R));
+    }
+    It = Cache.insert({&RegexList, std::move(RV)}).first;
+  }
 
-    if (R.match(Str))
+  for (const Regex &R : It->second) {
+    if (R.match(Str)) {
       return true;
+    }
   }
   return false;
 }
 
-//===----------------------------------------------------------------------===//
-// ScopDetection.
-
-ScopDetection::ScopDetection(const DominatorTree &DT, ScalarEvolution &SE,
-                             LoopInfo &LI, RegionInfo &RI, AAResults &AA,
-                             OptimizationRemarkEmitter &ORE)
+ScopDetection::ScopDetection(const DominatorTree &DT, ScalarEvolution &SE, LoopInfo &LI, RegionInfo &RI,
+                             AAResults &AA, OptimizationRemarkEmitter &ORE)
     : DT(DT), SE(SE), LI(LI), RI(RI), AA(AA), ORE(ORE) {}
 
 void ScopDetection::detect(Function &F) {
   assert(ValidRegions.empty() && "Detection must run only once");
 
-  if (!PollyProcessUnprofitable && LI.empty())
+  if (!PollyProcessUnprofitable && LI.empty()) {
     return;
+  }
 
-  Region *TopRegion = RI.getTopLevelRegion();
-
-  if (!OnlyFunctions.empty() &&
-      !doesStringMatchAnyRegex(F.getName(), OnlyFunctions))
+  if (!OnlyFunctions.empty() && !doesStringMatchAnyRegex(F.getName(), OnlyFunctions)) {
     return;
+  }
 
-  if (doesStringMatchAnyRegex(F.getName(), IgnoredFunctions))
+  if (doesStringMatchAnyRegex(F.getName(), IgnoredFunctions)) {
     return;
+  }
 
-  if (!isValidFunction(F))
+  if (!isValidFunction(F)) {
     return;
+  }
 
+  Region *TopRegion = RI.getTopLevelRegion();
   findScops(*TopRegion);
 
   NumScopRegions += ValidRegions.size();
 
-  // Prune non-profitable regions.
   for (auto &DIt : DetectionContextMap) {
     DetectionContext &DC = *DIt.getSecond();
-    if (DC.Log.hasErrors())
+    if (DC.Log.hasErrors()) {
       continue;
-    if (!ValidRegions.count(&DC.CurRegion))
+    }
+    if (!ValidRegions.count(&DC.CurRegion)) {
       continue;
+    }
     LoopStats Stats = countBeneficialLoops(&DC.CurRegion, SE, LI, 0);
-    updateLoopCountStatistic(Stats, false /* OnlyProfitable */);
+    updateLoopCountStatistic(Stats, false);
     if (isProfitableRegion(DC)) {
-      updateLoopCountStatistic(Stats, true /* OnlyProfitable */);
+      updateLoopCountStatistic(Stats, true);
       continue;
     }
-
     ValidRegions.remove(&DC.CurRegion);
   }
 
   NumProfScopRegions += ValidRegions.size();
   NumLoopsOverall += countBeneficialLoops(TopRegion, SE, LI, 0).NumLoops;
 
-  // Only makes sense when we tracked errors.
-  if (PollyTrackFailures)
+  if (PollyTrackFailures) {
     emitMissedRemarks(F);
-
-  if (ReportLevel)
+  }
+  if (ReportLevel) {
     printLocations(F);
+  }
 
-  assert(ValidRegions.size() <= DetectionContextMap.size() &&
-         "Cached more results than valid regions");
+  assert(ValidRegions.size() <= DetectionContextMap.size() && "Cached more results than valid regions");
 }
 
 template <class RR, typename... Args>
-inline bool ScopDetection::invalid(DetectionContext &Context, bool Assert,
-                                   Args &&...Arguments) const {
+inline bool ScopDetection::invalid(DetectionContext &Context, bool Assert, Args &&...Arguments) const {
   if (!Context.Verifying) {
     RejectLog &Log = Context.Log;
     std::shared_ptr<RR> RejectReason = std::make_shared<RR>(Arguments...);
     Context.IsInvalid = true;
-
-    // Log even if PollyTrackFailures is false, the log entries are also used in
-    // canUseISLTripCount().
     Log.report(RejectReason);
-
-    POLLY_DEBUG(dbgs() << RejectReason->getMessage());
-    POLLY_DEBUG(dbgs() << "\n");
+    POLLY_DEBUG(dbgs() << RejectReason->getMessage() << "\n");
   } else {
     assert(!Assert && "Verification of detected scop failed");
   }
-
   return false;
 }
 
 bool ScopDetection::isMaxRegionInScop(const Region &R, bool Verify) {
-  if (!ValidRegions.count(&R))
+  if (!ValidRegions.count(&R)) {
     return false;
-
+  }
   if (Verify) {
     BBPair P = getBBPairForRegion(&R);
     std::unique_ptr<DetectionContext> &Entry = DetectionContextMap[P];
-
-    // Free previous DetectionContext for the region and create and verify a new
-    // one. Be sure that the DetectionContext is not still used by a ScopInfop.
-    // Due to changes but CodeGeneration of another Scop, the Region object and
-    // the BBPair might not match anymore.
-    Entry = std::make_unique<DetectionContext>(const_cast<Region &>(R), AA,
-                                               /*Verifying=*/false);
-
+    Entry = std::make_unique<DetectionContext>(const_cast<Region &>(R), AA, false);
     return isValidRegion(*Entry);
   }
-
   return true;
 }
 
 std::string ScopDetection::regionIsInvalidBecause(const Region *R) const {
-  // Get the first error we found. Even in keep-going mode, this is the first
-  // reason that caused the candidate to be rejected.
-  auto *Log = lookupRejectionLog(R);
-
-  // This can happen when we marked a region invalid, but didn't track
-  // an error for it.
-  if (!Log || !Log->hasErrors())
+  const RejectLog *Log = lookupRejectionLog(R);
+  if (!Log || !Log->hasErrors()) {
     return "";
-
+  }
   RejectReasonPtr RR = *Log->begin();
   return RR->getMessage();
 }
 
-bool ScopDetection::addOverApproximatedRegion(Region *AR,
-                                              DetectionContext &Context) const {
-  // If we already know about Ar we can exit.
-  if (!Context.NonAffineSubRegionSet.insert(AR))
+bool ScopDetection::addOverApproximatedRegion(Region *AR, DetectionContext &Context) const {
+  if (!Context.NonAffineSubRegionSet.insert(AR)) {
     return true;
-
-  // All loops in the region have to be overapproximated too if there
-  // are accesses that depend on the iteration count.
+  }
 
   for (BasicBlock *BB : AR->blocks()) {
     Loop *L = LI.getLoopFor(BB);
-    if (AR->contains(L))
+    if (AR->contains(L)) {
       Context.BoxedLoopsSet.insert(L);
+    }
   }
 
   return (AllowNonAffineSubLoops || Context.BoxedLoopsSet.empty());
 }
 
-bool ScopDetection::onlyValidRequiredInvariantLoads(
-    InvariantLoadsSetTy &RequiredILS, DetectionContext &Context) const {
+bool ScopDetection::onlyValidRequiredInvariantLoads(InvariantLoadsSetTy &RequiredILS,
+                                                    DetectionContext &Context) const {
   Region &CurRegion = Context.CurRegion;
   const DataLayout &DL = CurRegion.getEntry()->getModule()->getDataLayout();
 
-  if (!PollyInvariantLoadHoisting && !RequiredILS.empty())
+  if (!PollyInvariantLoadHoisting && !RequiredILS.empty()) {
     return false;
+  }
 
   for (LoadInst *Load : RequiredILS) {
-    // If we already know a load has been accepted as required invariant, we
-    // already run the validation below once and consequently don't need to
-    // run it again. Hence, we return early. For certain test cases (e.g.,
-    // COSMO this avoids us spending 50% of scop-detection time in this
-    // very function (and its children).
-    if (Context.RequiredILS.count(Load))
+    if (Context.RequiredILS.count(Load)) {
       continue;
-    if (!isHoistableLoad(Load, CurRegion, LI, SE, DT, Context.RequiredILS))
+    }
+    if (!isHoistableLoad(Load, CurRegion, LI, SE, DT, Context.RequiredILS)) {
       return false;
+    }
 
     for (auto NonAffineRegion : Context.NonAffineSubRegionSet) {
-      if (isSafeToLoadUnconditionally(Load->getPointerOperand(),
-                                      Load->getType(), Load->getAlign(), DL,
-                                      nullptr))
+      if (isSafeToLoadUnconditionally(Load->getPointerOperand(), Load->getType(), Load->getAlign(), DL, nullptr)) {
         continue;
-
-      if (NonAffineRegion->contains(Load) &&
-          Load->getParent() != NonAffineRegion->getEntry())
+      }
+      if (NonAffineRegion->contains(Load) && Load->getParent() != NonAffineRegion->getEntry()) {
         return false;
+      }
     }
   }
 
   Context.RequiredILS.insert_range(RequiredILS);
-
   return true;
 }
 
-bool ScopDetection::involvesMultiplePtrs(const SCEV *S0, const SCEV *S1,
-                                         Loop *Scope) const {
+bool ScopDetection::involvesMultiplePtrs(const SCEV *S0, const SCEV *S1, Loop *Scope) const {
   SetVector<Value *> Values;
   findValues(S0, SE, Values);
-  if (S1)
+  if (S1) {
     findValues(S1, SE, Values);
+  }
 
   SmallPtrSet<Value *, 8> PtrVals;
   for (auto *V : Values) {
-    if (auto *P2I = dyn_cast<PtrToIntInst>(V))
+    if (auto *P2I = dyn_cast<PtrToIntInst>(V)) {
       V = P2I->getOperand(0);
-
-    if (!V->getType()->isPointerTy())
+    }
+    if (!V->getType()->isPointerTy()) {
       continue;
+    }
 
     const SCEV *PtrSCEV = SE.getSCEVAtScope(V, Scope);
-    if (isa<SCEVConstant>(PtrSCEV))
+    if (isa<SCEVConstant>(PtrSCEV)) {
       continue;
+    }
 
     auto *BasePtr = dyn_cast<SCEVUnknown>(SE.getPointerBase(PtrSCEV));
-    if (!BasePtr)
+    if (!BasePtr) {
       return true;
+    }
 
     Value *BasePtrVal = BasePtr->getValue();
     if (PtrVals.insert(BasePtrVal).second) {
-      for (auto *PtrVal : PtrVals)
-        if (PtrVal != BasePtrVal && !AA.isNoAlias(PtrVal, BasePtrVal))
+      for (auto *PtrVal : PtrVals) {
+        if (PtrVal != BasePtrVal && !AA.isNoAlias(PtrVal, BasePtrVal)) {
           return true;
+        }
+      }
     }
   }
 
   return false;
 }
 
-bool ScopDetection::isAffine(const SCEV *S, Loop *Scope,
-                             DetectionContext &Context) const {
+bool ScopDetection::isAffine(const SCEV *S, Loop *Scope, DetectionContext &Context) const {
   InvariantLoadsSetTy AccessILS;
-  if (!isAffineExpr(&Context.CurRegion, Scope, S, SE, &AccessILS))
+  if (!isAffineExpr(&Context.CurRegion, Scope, S, SE, &AccessILS)) {
     return false;
-
-  if (!onlyValidRequiredInvariantLoads(AccessILS, Context))
+  }
+  if (!onlyValidRequiredInvariantLoads(AccessILS, Context)) {
     return false;
-
+  }
   return true;
 }
 
-bool ScopDetection::isValidSwitch(BasicBlock &BB, SwitchInst *SI,
-                                  Value *Condition, bool IsLoopBranch,
+bool ScopDetection::isValidSwitch(BasicBlock &BB, SwitchInst *SI, Value *Condition, bool IsLoopBranch,
                                   DetectionContext &Context) const {
+  if (SI->getNumCases() > PollyMaxSwitchCases) {
+    if (AllowNonAffineSubRegions && addOverApproximatedRegion(RI.getRegionFor(&BB), Context)) {
+      return true;
+    }
+    Loop *L = LI.getLoopFor(&BB);
+    const SCEV *CondS = SE.getSCEVAtScope(Condition, L);
+    return invalid<ReportNonAffBranch>(Context, true, &BB, CondS, CondS, SI);
+  }
+
   Loop *L = LI.getLoopFor(&BB);
   const SCEV *ConditionSCEV = SE.getSCEVAtScope(Condition, L);
 
-  if (IsLoopBranch && L->isLoopLatch(&BB))
+  if (IsLoopBranch && L->isLoopLatch(&BB)) {
     return false;
+  }
 
-  // Check for invalid usage of different pointers in one expression.
-  if (involvesMultiplePtrs(ConditionSCEV, nullptr, L))
+  if (involvesMultiplePtrs(ConditionSCEV, nullptr, L)) {
     return false;
+  }
 
-  if (isAffine(ConditionSCEV, L, Context))
+  if (isAffine(ConditionSCEV, L, Context)) {
     return true;
+  }
 
-  if (AllowNonAffineSubRegions &&
-      addOverApproximatedRegion(RI.getRegionFor(&BB), Context))
+  if (AllowNonAffineSubRegions && addOverApproximatedRegion(RI.getRegionFor(&BB), Context)) {
     return true;
+  }
 
-  return invalid<ReportNonAffBranch>(Context, /*Assert=*/true, &BB,
-                                     ConditionSCEV, ConditionSCEV, SI);
+  return invalid<ReportNonAffBranch>(Context, true, &BB, ConditionSCEV, ConditionSCEV, SI);
 }
 
-bool ScopDetection::isValidBranch(BasicBlock &BB, BranchInst *BI,
-                                  Value *Condition, bool IsLoopBranch,
+bool ScopDetection::isValidBranch(BasicBlock &BB, BranchInst *BI, Value *Condition, bool IsLoopBranch,
                                   DetectionContext &Context) {
-  // Constant integer conditions are always affine.
-  if (isa<ConstantInt>(Condition))
+  if (isa<ConstantInt>(Condition)) {
     return true;
+  }
 
-  if (BinaryOperator *BinOp = dyn_cast<BinaryOperator>(Condition)) {
+  if (auto *BinOp = dyn_cast<BinaryOperator>(Condition)) {
     auto Opcode = BinOp->getOpcode();
     if (Opcode == Instruction::And || Opcode == Instruction::Or) {
       Value *Op0 = BinOp->getOperand(0);
@@ -592,33 +533,32 @@ bool ScopDetection::isValidBranch(BasicB
     }
   }
 
-  if (auto PHI = dyn_cast<PHINode>(Condition)) {
-    auto *Unique = dyn_cast_or_null<ConstantInt>(
-        getUniqueNonErrorValue(PHI, &Context.CurRegion, this));
-    if (Unique && (Unique->isZero() || Unique->isOne()))
+  if (auto *PHI = dyn_cast<PHINode>(Condition)) {
+    auto *Unique = dyn_cast_or_null<ConstantInt>(getUniqueNonErrorValue(PHI, &Context.CurRegion, this));
+    if (Unique && (Unique->isZero() || Unique->isOne())) {
       return true;
+    }
   }
 
-  if (auto Load = dyn_cast<LoadInst>(Condition))
+  if (auto *Load = dyn_cast<LoadInst>(Condition)) {
     if (!IsLoopBranch && Context.CurRegion.contains(Load)) {
       Context.RequiredILS.insert(Load);
       return true;
     }
+  }
 
-  // Non constant conditions of branches need to be ICmpInst.
   if (!isa<ICmpInst>(Condition)) {
-    if (!IsLoopBranch && AllowNonAffineSubRegions &&
-        addOverApproximatedRegion(RI.getRegionFor(&BB), Context))
+    if (!IsLoopBranch && AllowNonAffineSubRegions && addOverApproximatedRegion(RI.getRegionFor(&BB), Context)) {
       return true;
-    return invalid<ReportInvalidCond>(Context, /*Assert=*/true, BI, &BB);
+    }
+    return invalid<ReportInvalidCond>(Context, true, BI, &BB);
   }
 
-  ICmpInst *ICmp = cast<ICmpInst>(Condition);
+  auto *ICmp = cast<ICmpInst>(Condition);
 
-  // Are both operands of the ICmp affine?
-  if (isa<UndefValue>(ICmp->getOperand(0)) ||
-      isa<UndefValue>(ICmp->getOperand(1)))
-    return invalid<ReportUndefOperand>(Context, /*Assert=*/true, &BB, ICmp);
+  if (isa<UndefValue>(ICmp->getOperand(0)) || isa<UndefValue>(ICmp->getOperand(1))) {
+    return invalid<ReportUndefOperand>(Context, true, &BB, ICmp);
+  }
 
   Loop *L = LI.getLoopFor(&BB);
   const SCEV *LHS = SE.getSCEVAtScope(ICmp->getOperand(0), L);
@@ -627,87 +567,86 @@ bool ScopDetection::isValidBranch(BasicB
   LHS = tryForwardThroughPHI(LHS, Context.CurRegion, SE, this);
   RHS = tryForwardThroughPHI(RHS, Context.CurRegion, SE, this);
 
-  // If unsigned operations are not allowed try to approximate the region.
-  if (ICmp->isUnsigned() && !PollyAllowUnsignedOperations)
-    return !IsLoopBranch && AllowNonAffineSubRegions &&
-           addOverApproximatedRegion(RI.getRegionFor(&BB), Context);
+  if (ICmp->isUnsigned() && !PollyAllowUnsignedOperations) {
+    return !IsLoopBranch && AllowNonAffineSubRegions && addOverApproximatedRegion(RI.getRegionFor(&BB), Context);
+  }
 
-  // Check for invalid usage of different pointers in one expression.
-  if (ICmp->isEquality() && involvesMultiplePtrs(LHS, nullptr, L) &&
-      involvesMultiplePtrs(RHS, nullptr, L))
+  if (ICmp->isEquality() && involvesMultiplePtrs(LHS, nullptr, L) && involvesMultiplePtrs(RHS, nullptr, L)) {
     return false;
+  }
 
-  // Check for invalid usage of different pointers in a relational comparison.
-  if (ICmp->isRelational() && involvesMultiplePtrs(LHS, RHS, L))
+  if (ICmp->isRelational() && involvesMultiplePtrs(LHS, RHS, L)) {
     return false;
+  }
 
-  if (isAffine(LHS, L, Context) && isAffine(RHS, L, Context))
+  if (isAffine(LHS, L, Context) && isAffine(RHS, L, Context)) {
     return true;
+  }
 
-  if (!IsLoopBranch && AllowNonAffineSubRegions &&
-      addOverApproximatedRegion(RI.getRegionFor(&BB), Context))
+  if (!IsLoopBranch && AllowNonAffineSubRegions && addOverApproximatedRegion(RI.getRegionFor(&BB), Context)) {
     return true;
+  }
 
-  if (IsLoopBranch)
+  if (IsLoopBranch) {
     return false;
+  }
 
-  return invalid<ReportNonAffBranch>(Context, /*Assert=*/true, &BB, LHS, RHS,
-                                     ICmp);
+  return invalid<ReportNonAffBranch>(Context, true, &BB, LHS, RHS, ICmp);
 }
 
-bool ScopDetection::isValidCFG(BasicBlock &BB, bool IsLoopBranch,
-                               bool AllowUnreachable,
+bool ScopDetection::isValidCFG(BasicBlock &BB, bool IsLoopBranch, bool AllowUnreachable,
                                DetectionContext &Context) {
   Region &CurRegion = Context.CurRegion;
-
   Instruction *TI = BB.getTerminator();
 
-  if (AllowUnreachable && isa<UnreachableInst>(TI))
+  if (AllowUnreachable && isa<UnreachableInst>(TI)) {
     return true;
+  }
 
-  // Return instructions are only valid if the region is the top level region.
-  if (isa<ReturnInst>(TI) && CurRegion.isTopLevelRegion())
+  if (isa<ReturnInst>(TI) && CurRegion.isTopLevelRegion()) {
     return true;
+  }
 
   Value *Condition = getConditionFromTerminator(TI);
+  if (!Condition) {
+    return invalid<ReportInvalidTerminator>(Context, true, &BB);
+  }
 
-  if (!Condition)
-    return invalid<ReportInvalidTerminator>(Context, /*Assert=*/true, &BB);
-
-  // UndefValue is not allowed as condition.
-  if (isa<UndefValue>(Condition))
-    return invalid<ReportUndefCond>(Context, /*Assert=*/true, TI, &BB);
+  if (isa<UndefValue>(Condition)) {
+    return invalid<ReportUndefCond>(Context, true, TI, &BB);
+  }
 
-  if (BranchInst *BI = dyn_cast<BranchInst>(TI))
+  if (auto *BI = dyn_cast<BranchInst>(TI)) {
     return isValidBranch(BB, BI, Condition, IsLoopBranch, Context);
+  }
 
-  SwitchInst *SI = dyn_cast<SwitchInst>(TI);
+  auto *SI = dyn_cast<SwitchInst>(TI);
   assert(SI && "Terminator was neither branch nor switch");
-
   return isValidSwitch(BB, SI, Condition, IsLoopBranch, Context);
 }
 
-bool ScopDetection::isValidCallInst(CallInst &CI,
-                                    DetectionContext &Context) const {
-  if (CI.doesNotReturn())
+bool ScopDetection::isValidCallInst(CallInst &CI, DetectionContext &Context) const {
+  if (CI.doesNotReturn()) {
     return false;
+  }
 
-  if (CI.doesNotAccessMemory())
+  if (CI.doesNotAccessMemory()) {
     return true;
+  }
 
-  if (auto *II = dyn_cast<IntrinsicInst>(&CI))
-    if (isValidIntrinsicInst(*II, Context))
+  if (auto *II = dyn_cast<IntrinsicInst>(&CI)) {
+    if (isValidIntrinsicInst(*II, Context)) {
       return true;
+    }
+  }
 
   Function *CalledFunction = CI.getCalledFunction();
-
-  // Indirect calls are not supported.
-  if (CalledFunction == nullptr)
+  if (CalledFunction == nullptr) {
     return false;
+  }
 
   if (isDebugCall(&CI)) {
-    POLLY_DEBUG(dbgs() << "Allow call to debug function: "
-                       << CalledFunction->getName() << '\n');
+    POLLY_DEBUG(dbgs() << "Allow call to debug function: " << CalledFunction->getName() << '\n');
     return true;
   }
 
@@ -715,37 +654,25 @@ bool ScopDetection::isValidCallInst(Call
     MemoryEffects ME = AA.getMemoryEffects(CalledFunction);
     if (ME.onlyAccessesArgPointees()) {
       for (const auto &Arg : CI.args()) {
-        if (!Arg->getType()->isPointerTy())
+        if (!Arg->getType()->isPointerTy()) {
           continue;
-
-        // Bail if a pointer argument has a base address not known to
-        // ScalarEvolution. Note that a zero pointer is acceptable.
-        const SCEV *ArgSCEV =
-            SE.getSCEVAtScope(Arg, LI.getLoopFor(CI.getParent()));
-        if (ArgSCEV->isZero())
+        }
+        const SCEV *ArgSCEV = SE.getSCEVAtScope(Arg, LI.getLoopFor(CI.getParent()));
+        if (ArgSCEV->isZero()) {
           continue;
-
+        }
         auto *BP = dyn_cast<SCEVUnknown>(SE.getPointerBase(ArgSCEV));
-        if (!BP)
+        if (!BP) {
           return false;
-
-        // Implicitly disable delinearization since we have an unknown
-        // accesses with an unknown access function.
+        }
         Context.HasUnknownAccess = true;
       }
-
-      // Explicitly use addUnknown so we don't put a loop-variant
-      // pointer into the alias set.
       Context.AST.addUnknown(&CI);
       return true;
     }
 
     if (ME.onlyReadsMemory()) {
-      // Implicitly disable delinearization since we have an unknown
-      // accesses with an unknown access function.
       Context.HasUnknownAccess = true;
-      // Explicitly use addUnknown so we don't put a loop-variant
-      // pointer into the alias set.
       Context.AST.addUnknown(&CI);
       return true;
     }
@@ -755,44 +682,37 @@ bool ScopDetection::isValidCallInst(Call
   return false;
 }
 
-bool ScopDetection::isValidIntrinsicInst(IntrinsicInst &II,
-                                         DetectionContext &Context) const {
-  if (isIgnoredIntrinsic(&II))
+bool ScopDetection::isValidIntrinsicInst(IntrinsicInst &II, DetectionContext &Context) const {
+  if (isIgnoredIntrinsic(&II)) {
     return true;
+  }
 
-  // The closest loop surrounding the call instruction.
   Loop *L = LI.getLoopFor(II.getParent());
-
-  // The access function and base pointer for memory intrinsics.
   const SCEV *AF;
   const SCEVUnknown *BP;
 
   switch (II.getIntrinsicID()) {
-  // Memory intrinsics that can be represented are supported.
   case Intrinsic::memmove:
   case Intrinsic::memcpy:
     AF = SE.getSCEVAtScope(cast<MemTransferInst>(II).getSource(), L);
     if (!AF->isZero()) {
       BP = dyn_cast<SCEVUnknown>(SE.getPointerBase(AF));
-      // Bail if the source pointer is not valid.
-      if (!isValidAccess(&II, AF, BP, Context))
+      if (!isValidAccess(&II, AF, BP, Context)) {
         return false;
+      }
     }
     [[fallthrough]];
   case Intrinsic::memset:
     AF = SE.getSCEVAtScope(cast<MemIntrinsic>(II).getDest(), L);
     if (!AF->isZero()) {
       BP = dyn_cast<SCEVUnknown>(SE.getPointerBase(AF));
-      // Bail if the destination pointer is not valid.
-      if (!isValidAccess(&II, AF, BP, Context))
+      if (!isValidAccess(&II, AF, BP, Context)) {
         return false;
+      }
     }
-
-    // Bail if the length is not affine.
-    if (!isAffine(SE.getSCEVAtScope(cast<MemIntrinsic>(II).getLength(), L), L,
-                  Context))
+    if (!isAffine(SE.getSCEVAtScope(cast<MemIntrinsic>(II).getLength(), L), L, Context)) {
       return false;
-
+    }
     return true;
   default:
     break;
@@ -801,24 +721,22 @@ bool ScopDetection::isValidIntrinsicInst
   return false;
 }
 
-bool ScopDetection::isInvariant(Value &Val, const Region &Reg,
-                                DetectionContext &Ctx) const {
-  // A reference to function argument or constant value is invariant.
-  if (isa<Argument>(Val) || isa<Constant>(Val))
+bool ScopDetection::isInvariant(Value &Val, const Region &Reg, DetectionContext &Ctx) const {
+  if (isa<Argument>(Val) || isa<Constant>(Val)) {
     return true;
+  }
 
-  Instruction *I = dyn_cast<Instruction>(&Val);
-  if (!I)
+  auto *I = dyn_cast<Instruction>(&Val);
+  if (!I) {
     return false;
+  }
 
-  if (!Reg.contains(I))
+  if (!Reg.contains(I)) {
     return true;
+  }
 
-  // Loads within the SCoP may read arbitrary values, need to hoist them. If it
-  // is not hoistable, it will be rejected later, but here we assume it is and
-  // that makes the value invariant.
-  if (auto LI = dyn_cast<LoadInst>(I)) {
-    Ctx.RequiredILS.insert(LI);
+  if (auto *LI_ = dyn_cast<LoadInst>(I)) {
+    Ctx.RequiredILS.insert(LI_);
     return true;
   }
 
@@ -827,25 +745,11 @@ bool ScopDetection::isInvariant(Value &V
 
 namespace {
 
-/// Remove smax of smax(0, size) expressions from a SCEV expression and
-/// register the '...' components.
-///
-/// Array access expressions as they are generated by GFortran contain smax(0,
-/// size) expressions that confuse the 'normal' delinearization algorithm.
-/// However, if we extract such expressions before the normal delinearization
-/// takes place they can actually help to identify array size expressions in
-/// Fortran accesses. For the subsequently following delinearization the smax(0,
-/// size) component can be replaced by just 'size'. This is correct as we will
-/// always add and verify the assumption that for all subscript expressions
-/// 'exp' the inequality 0 <= exp < size holds. Hence, we will also verify
-/// that 0 <= size, which means smax(0, size) == size.
 class SCEVRemoveMax final : public SCEVRewriteVisitor<SCEVRemoveMax> {
 public:
-  SCEVRemoveMax(ScalarEvolution &SE, std::vector<const SCEV *> *Terms)
-      : SCEVRewriteVisitor(SE), Terms(Terms) {}
+  SCEVRemoveMax(ScalarEvolution &SE, std::vector<const SCEV *> *Terms) : SCEVRewriteVisitor(SE), Terms(Terms) {}
 
-  static const SCEV *rewrite(const SCEV *Scev, ScalarEvolution &SE,
-                             std::vector<const SCEV *> *Terms = nullptr) {
+  static const SCEV *rewrite(const SCEV *Scev, ScalarEvolution &SE, std::vector<const SCEV *> *Terms = nullptr) {
     SCEVRemoveMax Rewriter(SE, Terms);
     return Rewriter.visit(Scev);
   }
@@ -853,22 +757,22 @@ public:
   const SCEV *visitSMaxExpr(const SCEVSMaxExpr *Expr) {
     if ((Expr->getNumOperands() == 2) && Expr->getOperand(0)->isZero()) {
       auto Res = visit(Expr->getOperand(1));
-      if (Terms)
+      if (Terms) {
         (*Terms).push_back(Res);
+      }
       return Res;
     }
-
     return Expr;
   }
 
 private:
   std::vector<const SCEV *> *Terms;
 };
+
 } // namespace
 
-SmallVector<const SCEV *, 4>
-ScopDetection::getDelinearizationTerms(DetectionContext &Context,
-                                       const SCEVUnknown *BasePointer) const {
+SmallVector<const SCEV *, 4> ScopDetection::getDelinearizationTerms(DetectionContext &Context,
+                                                                    const SCEVUnknown *BasePointer) const {
   SmallVector<const SCEV *, 4> Terms;
   for (const auto &Pair : Context.Accesses[BasePointer]) {
     std::vector<const SCEV *> MaxTerms;
@@ -877,63 +781,49 @@ ScopDetection::getDelinearizationTerms(D
       Terms.insert(Terms.begin(), MaxTerms.begin(), MaxTerms.end());
       continue;
     }
-    // In case the outermost expression is a plain add, we check if any of its
-    // terms has the form 4 * %inst * %param * %param ..., aka a term that
-    // contains a product between a parameter and an instruction that is
-    // inside the scop. Such instructions, if allowed at all, are instructions
-    // SCEV can not represent, but Polly is still looking through. As a
-    // result, these instructions can depend on induction variables and are
-    // most likely no array sizes. However, terms that are multiplied with
-    // them are likely candidates for array sizes.
     if (auto *AF = dyn_cast<SCEVAddExpr>(Pair.second)) {
       for (auto Op : AF->operands()) {
-        if (auto *AF2 = dyn_cast<SCEVAddRecExpr>(Op))
+        if (auto *AF2 = dyn_cast<SCEVAddRecExpr>(Op)) {
           collectParametricTerms(SE, AF2, Terms);
+        }
         if (auto *AF2 = dyn_cast<SCEVMulExpr>(Op)) {
           SmallVector<const SCEV *, 0> Operands;
-
           for (const SCEV *MulOp : AF2->operands()) {
-            if (auto *Const = dyn_cast<SCEVConstant>(MulOp))
+            if (auto *Const = dyn_cast<SCEVConstant>(MulOp)) {
               Operands.push_back(Const);
+            }
             if (auto *Unknown = dyn_cast<SCEVUnknown>(MulOp)) {
               if (auto *Inst = dyn_cast<Instruction>(Unknown->getValue())) {
-                if (!Context.CurRegion.contains(Inst))
+                if (!Context.CurRegion.contains(Inst)) {
                   Operands.push_back(MulOp);
-
+                }
               } else {
                 Operands.push_back(MulOp);
               }
             }
           }
-          if (Operands.size())
+          if (!Operands.empty()) {
             Terms.push_back(SE.getMulExpr(Operands));
+          }
         }
       }
     }
-    if (Terms.empty())
+    if (Terms.empty()) {
       collectParametricTerms(SE, Pair.second, Terms);
+    }
   }
   return Terms;
 }
 
-bool ScopDetection::hasValidArraySizes(DetectionContext &Context,
-                                       SmallVectorImpl<const SCEV *> &Sizes,
-                                       const SCEVUnknown *BasePointer,
-                                       Loop *Scope) const {
-  // If no sizes were found, all sizes are trivially valid. We allow this case
-  // to make it possible to pass known-affine accesses to the delinearization to
-  // try to recover some interesting multi-dimensional accesses, but to still
-  // allow the already known to be affine access in case the delinearization
-  // fails. In such situations, the delinearization will just return a Sizes
-  // array of size zero.
-  if (Sizes.size() == 0)
+bool ScopDetection::hasValidArraySizes(DetectionContext &Context, SmallVectorImpl<const SCEV *> &Sizes,
+                                       const SCEVUnknown *BasePointer, Loop *Scope) const {
+  if (Sizes.empty()) {
     return true;
+  }
 
   Value *BaseValue = BasePointer->getValue();
   Region &CurRegion = Context.CurRegion;
   for (const SCEV *DelinearizedSize : Sizes) {
-    // Don't pass down the scope to isAfffine; array dimensions must be
-    // invariant across the entire scop.
     if (!isAffine(DelinearizedSize, nullptr, Context)) {
       Sizes.clear();
       break;
@@ -941,33 +831,31 @@ bool ScopDetection::hasValidArraySizes(D
     if (auto *Unknown = dyn_cast<SCEVUnknown>(DelinearizedSize)) {
       auto *V = dyn_cast<Value>(Unknown->getValue());
       if (auto *Load = dyn_cast<LoadInst>(V)) {
-        if (Context.CurRegion.contains(Load) &&
-            isHoistableLoad(Load, CurRegion, LI, SE, DT, Context.RequiredILS))
+        if (Context.CurRegion.contains(Load) && isHoistableLoad(Load, CurRegion, LI, SE, DT, Context.RequiredILS)) {
           Context.RequiredILS.insert(Load);
+        }
         continue;
       }
     }
-    if (hasScalarDepsInsideRegion(DelinearizedSize, &CurRegion, Scope, false,
-                                  Context.RequiredILS))
-      return invalid<ReportNonAffineAccess>(
-          Context, /*Assert=*/true, DelinearizedSize,
-          Context.Accesses[BasePointer].front().first, BaseValue);
+    if (hasScalarDepsInsideRegion(DelinearizedSize, &CurRegion, Scope, false, Context.RequiredILS)) {
+      return invalid<ReportNonAffineAccess>(Context, true, DelinearizedSize,
+                                            Context.Accesses[BasePointer].front().first, BaseValue);
+    }
   }
 
-  // No array shape derived.
   if (Sizes.empty()) {
-    if (AllowNonAffine)
+    if (AllowNonAffine) {
       return true;
+    }
 
     for (const auto &Pair : Context.Accesses[BasePointer]) {
       const Instruction *Insn = Pair.first;
       const SCEV *AF = Pair.second;
-
       if (!isAffine(AF, Scope, Context)) {
-        invalid<ReportNonAffineAccess>(Context, /*Assert=*/true, AF, Insn,
-                                       BaseValue);
-        if (!KeepGoing)
+        invalid<ReportNonAffineAccess>(Context, true, AF, Insn, BaseValue);
+        if (!KeepGoing) {
           return false;
+        }
       }
     }
     return false;
@@ -975,18 +863,12 @@ bool ScopDetection::hasValidArraySizes(D
   return true;
 }
 
-// We first store the resulting memory accesses in TempMemoryAccesses. Only
-// if the access functions for all memory accesses have been successfully
-// delinearized we continue. Otherwise, we either report a failure or, if
-// non-affine accesses are allowed, we drop the information. In case the
-// information is dropped the memory accesses need to be overapproximated
-// when translated to a polyhedral representation.
-bool ScopDetection::computeAccessFunctions(
-    DetectionContext &Context, const SCEVUnknown *BasePointer,
-    std::shared_ptr<ArrayShape> Shape) const {
+bool ScopDetection::computeAccessFunctions(DetectionContext &Context, const SCEVUnknown *BasePointer,
+                                           std::shared_ptr<ArrayShape> Shape) const {
   Value *BaseValue = BasePointer->getValue();
   bool BasePtrHasNonAffine = false;
   MapInsnToMemAcc TempMemoryAccesses;
+
   for (const auto &Pair : Context.Accesses[BasePointer]) {
     const Instruction *Insn = Pair.first;
     auto *AF = Pair.second;
@@ -997,97 +879,95 @@ bool ScopDetection::computeAccessFunctio
     auto *Scope = LI.getLoopFor(Insn->getParent());
 
     if (!AF) {
-      if (isAffine(Pair.second, Scope, Context))
+      if (isAffine(Pair.second, Scope, Context)) {
         Acc->DelinearizedSubscripts.push_back(Pair.second);
-      else
+      } else {
         IsNonAffine = true;
+      }
     } else {
-      if (Shape->DelinearizedSizes.size() == 0) {
+      if (Shape->DelinearizedSizes.empty()) {
         Acc->DelinearizedSubscripts.push_back(AF);
       } else {
-        llvm::computeAccessFunctions(SE, AF, Acc->DelinearizedSubscripts,
-                                     Shape->DelinearizedSizes);
-        if (Acc->DelinearizedSubscripts.size() == 0)
+        llvm::computeAccessFunctions(SE, AF, Acc->DelinearizedSubscripts, Shape->DelinearizedSizes);
+        if (Acc->DelinearizedSubscripts.empty()) {
           IsNonAffine = true;
+        }
       }
-      for (const SCEV *S : Acc->DelinearizedSubscripts)
-        if (!isAffine(S, Scope, Context))
+      for (const SCEV *S : Acc->DelinearizedSubscripts) {
+        if (!isAffine(S, Scope, Context)) {
           IsNonAffine = true;
+        }
+      }
     }
 
-    // (Possibly) report non affine access
     if (IsNonAffine) {
       BasePtrHasNonAffine = true;
       if (!AllowNonAffine) {
-        invalid<ReportNonAffineAccess>(Context, /*Assert=*/true, Pair.second,
-                                       Insn, BaseValue);
-        if (!KeepGoing)
+        invalid<ReportNonAffineAccess>(Context, true, Pair.second, Insn, BaseValue);
+        if (!KeepGoing) {
           return false;
+        }
       }
     }
   }
 
-  if (!BasePtrHasNonAffine)
-    Context.InsnToMemAcc.insert(TempMemoryAccesses.begin(),
-                                TempMemoryAccesses.end());
+  if (!BasePtrHasNonAffine) {
+    Context.InsnToMemAcc.insert(TempMemoryAccesses.begin(), TempMemoryAccesses.end());
+  }
 
   return true;
 }
 
-bool ScopDetection::hasBaseAffineAccesses(DetectionContext &Context,
-                                          const SCEVUnknown *BasePointer,
+bool ScopDetection::hasBaseAffineAccesses(DetectionContext &Context, const SCEVUnknown *BasePointer,
                                           Loop *Scope) const {
   auto Shape = std::shared_ptr<ArrayShape>(new ArrayShape(BasePointer));
-
   auto Terms = getDelinearizationTerms(Context, BasePointer);
 
-  findArrayDimensions(SE, Terms, Shape->DelinearizedSizes,
-                      Context.ElementSize[BasePointer]);
+  findArrayDimensions(SE, Terms, Shape->DelinearizedSizes, Context.ElementSize[BasePointer]);
 
-  if (!hasValidArraySizes(Context, Shape->DelinearizedSizes, BasePointer,
-                          Scope))
+  if (!hasValidArraySizes(Context, Shape->DelinearizedSizes, BasePointer, Scope)) {
     return false;
+  }
 
   return computeAccessFunctions(Context, BasePointer, Shape);
 }
 
 bool ScopDetection::hasAffineMemoryAccesses(DetectionContext &Context) const {
-  // TODO: If we have an unknown access and other non-affine accesses we do
-  //       not try to delinearize them for now.
-  if (Context.HasUnknownAccess && !Context.NonAffineAccesses.empty())
+  if (Context.HasUnknownAccess && !Context.NonAffineAccesses.empty()) {
     return AllowNonAffine;
+  }
 
   for (auto &Pair : Context.NonAffineAccesses) {
     auto *BasePointer = Pair.first;
     auto *Scope = Pair.second;
     if (!hasBaseAffineAccesses(Context, BasePointer, Scope)) {
       Context.IsInvalid = true;
-      if (!KeepGoing)
+      if (!KeepGoing) {
         return false;
+      }
     }
   }
   return true;
 }
 
-bool ScopDetection::isValidAccess(Instruction *Inst, const SCEV *AF,
-                                  const SCEVUnknown *BP,
+bool ScopDetection::isValidAccess(Instruction *Inst, const SCEV *AF, const SCEVUnknown *BP,
                                   DetectionContext &Context) const {
-
-  if (!BP)
-    return invalid<ReportNoBasePtr>(Context, /*Assert=*/true, Inst);
+  if (!BP) {
+    return invalid<ReportNoBasePtr>(Context, true, Inst);
+  }
 
   auto *BV = BP->getValue();
-  if (isa<UndefValue>(BV))
-    return invalid<ReportUndefBasePtr>(Context, /*Assert=*/true, Inst);
+  if (isa<UndefValue>(BV)) {
+    return invalid<ReportUndefBasePtr>(Context, true, Inst);
+  }
+
+  if (auto *ITP = dyn_cast<IntToPtrInst>(BV)) {
+    return invalid<ReportIntToPtr>(Context, true, ITP);
+  }
 
-  // FIXME: Think about allowing IntToPtrInst
-  if (IntToPtrInst *Inst = dyn_cast<IntToPtrInst>(BV))
-    return invalid<ReportIntToPtr>(Context, /*Assert=*/true, Inst);
-
-  // Check that the base address of the access is invariant in the current
-  // region.
-  if (!isInvariant(*BV, Context.CurRegion, Context))
-    return invalid<ReportVariantBasePtr>(Context, /*Assert=*/true, BV, Inst);
+  if (!isInvariant(*BV, Context.CurRegion, Context)) {
+    return invalid<ReportVariantBasePtr>(Context, true, BV, Inst);
+  }
 
   AF = SE.getMinusSCEV(AF, BP);
 
@@ -1095,16 +975,14 @@ bool ScopDetection::isValidAccess(Instru
   if (!isa<MemIntrinsic>(Inst)) {
     Size = SE.getElementSize(Inst);
   } else {
-    auto *SizeTy =
-        SE.getEffectiveSCEVType(PointerType::getUnqual(SE.getContext()));
+    auto *SizeTy = SE.getEffectiveSCEVType(PointerType::getUnqual(SE.getContext()));
     Size = SE.getConstant(SizeTy, 8);
   }
 
   if (Context.ElementSize[BP]) {
-    if (!AllowDifferentTypes && Context.ElementSize[BP] != Size)
-      return invalid<ReportDifferentArrayElementSize>(Context, /*Assert=*/true,
-                                                      Inst, BV);
-
+    if (!AllowDifferentTypes && Context.ElementSize[BP] != Size) {
+      return invalid<ReportDifferentArrayElementSize>(Context, true, Inst, BV);
+    }
     Context.ElementSize[BP] = SE.getSMinExpr(Size, Context.ElementSize[BP]);
   } else {
     Context.ElementSize[BP] = Size;
@@ -1113,116 +991,102 @@ bool ScopDetection::isValidAccess(Instru
   bool IsVariantInNonAffineLoop = false;
   SetVector<const Loop *> Loops;
   findLoops(AF, Loops);
-  for (const Loop *L : Loops)
-    if (Context.BoxedLoopsSet.count(L))
+  for (const Loop *L : Loops) {
+    if (Context.BoxedLoopsSet.count(L)) {
       IsVariantInNonAffineLoop = true;
+    }
+  }
 
   auto *Scope = LI.getLoopFor(Inst->getParent());
   bool IsAffine = !IsVariantInNonAffineLoop && isAffine(AF, Scope, Context);
-  // Do not try to delinearize memory intrinsics and force them to be affine.
+
   if (isa<MemIntrinsic>(Inst) && !IsAffine) {
-    return invalid<ReportNonAffineAccess>(Context, /*Assert=*/true, AF, Inst,
-                                          BV);
+    return invalid<ReportNonAffineAccess>(Context, true, AF, Inst, BV);
   } else if (PollyDelinearize && !IsVariantInNonAffineLoop) {
     Context.Accesses[BP].push_back({Inst, AF});
-
-    if (!IsAffine)
-      Context.NonAffineAccesses.insert(
-          std::make_pair(BP, LI.getLoopFor(Inst->getParent())));
+    if (!IsAffine) {
+      Context.NonAffineAccesses.insert(std::make_pair(BP, LI.getLoopFor(Inst->getParent())));
+    }
   } else if (!AllowNonAffine && !IsAffine) {
-    return invalid<ReportNonAffineAccess>(Context, /*Assert=*/true, AF, Inst,
-                                          BV);
+    return invalid<ReportNonAffineAccess>(Context, true, AF, Inst, BV);
   }
 
-  if (IgnoreAliasing)
+  if (IgnoreAliasing) {
     return true;
+  }
 
-  // Check if the base pointer of the memory access does alias with
-  // any other pointer. This cannot be handled at the moment.
   AAMDNodes AATags = Inst->getAAMetadata();
-  AliasSet &AS = Context.AST.getAliasSetFor(
-      MemoryLocation::getBeforeOrAfter(BP->getValue(), AATags));
+  AliasSet &AS =
+      Context.AST.getAliasSetFor(MemoryLocation::getBeforeOrAfter(BP->getValue(), AATags));
 
   if (!AS.isMustAlias()) {
     if (PollyUseRuntimeAliasChecks) {
       bool CanBuildRunTimeCheck = true;
-      // The run-time alias check places code that involves the base pointer at
-      // the beginning of the SCoP. This breaks if the base pointer is defined
-      // inside the scop. Hence, we can only create a run-time check if we are
-      // sure the base pointer is not an instruction defined inside the scop.
-      // However, we can ignore loads that will be hoisted.
-
       auto ASPointers = AS.getPointers();
 
       InvariantLoadsSetTy VariantLS, InvariantLS;
-      // In order to detect loads which are dependent on other invariant loads
-      // as invariant, we use fixed-point iteration method here i.e we iterate
-      // over the alias set for arbitrary number of times until it is safe to
-      // assume that all the invariant loads have been detected
       while (true) {
-        const unsigned int VariantSize = VariantLS.size(),
-                           InvariantSize = InvariantLS.size();
+        const unsigned int VariantSize = VariantLS.size(), InvariantSize = InvariantLS.size();
 
         for (const Value *Ptr : ASPointers) {
-          Instruction *Inst = dyn_cast<Instruction>(const_cast<Value *>(Ptr));
-          if (Inst && Context.CurRegion.contains(Inst)) {
-            auto *Load = dyn_cast<LoadInst>(Inst);
-            if (Load && InvariantLS.count(Load))
+          Instruction *PtrInst = dyn_cast<Instruction>(const_cast<Value *>(Ptr));
+          if (PtrInst && Context.CurRegion.contains(PtrInst)) {
+            auto *Load = dyn_cast<LoadInst>(PtrInst);
+            if (Load && InvariantLS.count(Load)) {
               continue;
-            if (Load && isHoistableLoad(Load, Context.CurRegion, LI, SE, DT,
-                                        InvariantLS)) {
-              if (VariantLS.count(Load))
+            }
+            if (Load && isHoistableLoad(Load, Context.CurRegion, LI, SE, DT, InvariantLS)) {
+              if (VariantLS.count(Load)) {
                 VariantLS.remove(Load);
+              }
               Context.RequiredILS.insert(Load);
               InvariantLS.insert(Load);
             } else {
               CanBuildRunTimeCheck = false;
-              VariantLS.insert(Load);
+              if (Load) {
+                VariantLS.insert(Load);
+              }
             }
           }
         }
 
-        if (InvariantSize == InvariantLS.size() &&
-            VariantSize == VariantLS.size())
+        if (InvariantSize == InvariantLS.size() && VariantSize == VariantLS.size()) {
           break;
+        }
       }
 
-      if (CanBuildRunTimeCheck)
+      if (CanBuildRunTimeCheck) {
         return true;
+      }
     }
-    return invalid<ReportAlias>(Context, /*Assert=*/true, Inst, AS);
+    return invalid<ReportAlias>(Context, true, Inst, AS);
   }
 
   return true;
 }
 
-bool ScopDetection::isValidMemoryAccess(MemAccInst Inst,
-                                        DetectionContext &Context) const {
+bool ScopDetection::isValidMemoryAccess(MemAccInst Inst, DetectionContext &Context) const {
   Value *Ptr = Inst.getPointerOperand();
   Loop *L = LI.getLoopFor(Inst->getParent());
   const SCEV *AccessFunction = SE.getSCEVAtScope(Ptr, L);
-  const SCEVUnknown *BasePointer;
-
-  BasePointer = dyn_cast<SCEVUnknown>(SE.getPointerBase(AccessFunction));
-
+  const SCEVUnknown *BasePointer = dyn_cast<SCEVUnknown>(SE.getPointerBase(AccessFunction));
   return isValidAccess(Inst, AccessFunction, BasePointer, Context);
 }
 
-bool ScopDetection::isValidInstruction(Instruction &Inst,
-                                       DetectionContext &Context) {
+bool ScopDetection::isValidInstruction(Instruction &Inst, DetectionContext &Context) {
   for (auto &Op : Inst.operands()) {
-    auto *OpInst = dyn_cast<Instruction>(&Op);
-
-    if (!OpInst)
+    Value *OpV = Op.get();
+    auto *OpInst = dyn_cast<Instruction>(OpV);
+    if (!OpInst) {
       continue;
-
+    }
     if (isErrorBlock(*OpInst->getParent(), Context.CurRegion)) {
-      auto *PHI = dyn_cast<PHINode>(OpInst);
-      if (PHI) {
+      if (auto *PHI = dyn_cast<PHINode>(OpInst)) {
         for (User *U : PHI->users()) {
           auto *UI = dyn_cast<Instruction>(U);
-          if (!UI || !UI->isTerminator())
+          if (!UI || !UI->isTerminator()) {
             return false;
+          }
         }
       } else {
         return false;
@@ -1230,44 +1094,36 @@ bool ScopDetection::isValidInstruction(I
     }
   }
 
-  if (isa<LandingPadInst>(&Inst) || isa<ResumeInst>(&Inst))
+  if (isa<LandingPadInst>(&Inst) || isa<ResumeInst>(&Inst)) {
     return false;
+  }
 
-  // We only check the call instruction but not invoke instruction.
-  if (CallInst *CI = dyn_cast<CallInst>(&Inst)) {
-    if (isValidCallInst(*CI, Context))
+  if (auto *CI = dyn_cast<CallInst>(&Inst)) {
+    if (isValidCallInst(*CI, Context)) {
       return true;
-
-    return invalid<ReportFuncCall>(Context, /*Assert=*/true, &Inst);
+    }
+    return invalid<ReportFuncCall>(Context, true, &Inst);
   }
 
   if (!Inst.mayReadOrWriteMemory()) {
-    if (!isa<AllocaInst>(Inst))
+    if (!isa<AllocaInst>(Inst)) {
       return true;
-
-    return invalid<ReportAlloca>(Context, /*Assert=*/true, &Inst);
+    }
+    return invalid<ReportAlloca>(Context, true, &Inst);
   }
 
-  // Check the access function.
   if (auto MemInst = MemAccInst::dyn_cast(Inst)) {
     Context.hasStores |= isa<StoreInst>(MemInst);
     Context.hasLoads |= isa<LoadInst>(MemInst);
-    if (!MemInst.isSimple())
-      return invalid<ReportNonSimpleMemoryAccess>(Context, /*Assert=*/true,
-                                                  &Inst);
-
+    if (!MemInst.isSimple()) {
+      return invalid<ReportNonSimpleMemoryAccess>(Context, true, &Inst);
+    }
     return isValidMemoryAccess(MemInst, Context);
   }
 
-  // We do not know this instruction, therefore we assume it is invalid.
-  return invalid<ReportUnknownInst>(Context, /*Assert=*/true, &Inst);
+  return invalid<ReportUnknownInst>(Context, true, &Inst);
 }
 
-/// Check whether @p L has exiting blocks.
-///
-/// @param L The loop of interest
-///
-/// @return True if the loop has exiting blocks, false otherwise.
 static bool hasExitingBlocks(Loop *L) {
   SmallVector<BasicBlock *, 4> ExitingBlocks;
   L->getExitingBlocks(ExitingBlocks);
@@ -1275,17 +1131,8 @@ static bool hasExitingBlocks(Loop *L) {
 }
 
 bool ScopDetection::canUseISLTripCount(Loop *L, DetectionContext &Context) {
-  // FIXME: Yes, this is bad. isValidCFG() may call invalid<Reason>() which
-  // causes the SCoP to be rejected regardless on whether non-ISL trip counts
-  // could be used. We currently preserve the legacy behaviour of rejecting
-  // based on Context.Log.size() added by isValidCFG() or before, regardless on
-  // whether the ISL trip count can be used or can be used as a non-affine
-  // region. However, we allow rejections by isValidCFG() that do not result in
-  // an error log entry.
   bool OldIsInvalid = Context.IsInvalid;
 
-  // Ensure the loop has valid exiting blocks as well as latches, otherwise we
-  // need to overapproximate it as a boxed loop.
   SmallVector<BasicBlock *, 4> LoopControlBlocks;
   L->getExitingBlocks(LoopControlBlocks);
   L->getLoopLatches(LoopControlBlocks);
@@ -1296,86 +1143,69 @@ bool ScopDetection::canUseISLTripCount(L
     }
   }
 
-  // We can use ISL to compute the trip count of L.
   Context.IsInvalid = OldIsInvalid || Context.Log.size();
   return true;
 }
 
 bool ScopDetection::isValidLoop(Loop *L, DetectionContext &Context) {
-  // Loops that contain part but not all of the blocks of a region cannot be
-  // handled by the schedule generation. Such loop constructs can happen
-  // because a region can contain BBs that have no path to the exit block
-  // (Infinite loops, UnreachableInst), but such blocks are never part of a
-  // loop.
-  //
-  // _______________
-  // | Loop Header | <-----------.
-  // ---------------             |
-  //        |                    |
-  // _______________       ______________
-  // | RegionEntry |-----> | RegionExit |----->
-  // ---------------       --------------
-  //        |
-  // _______________
-  // | EndlessLoop | <--.
-  // ---------------    |
-  //       |            |
-  //       \------------/
-  //
-  // In the example above, the loop (LoopHeader,RegionEntry,RegionExit) is
-  // neither entirely contained in the region RegionEntry->RegionExit
-  // (containing RegionEntry,EndlessLoop) nor is the region entirely contained
-  // in the loop.
-  // The block EndlessLoop is contained in the region because Region::contains
-  // tests whether it is not dominated by RegionExit. This is probably to not
-  // having to query the PostdominatorTree. Instead of an endless loop, a dead
-  // end can also be formed by an UnreachableInst. This case is already caught
-  // by isErrorBlock(). We hence only have to reject endless loops here.
-  if (!hasExitingBlocks(L))
-    return invalid<ReportLoopHasNoExit>(Context, /*Assert=*/true, L);
-
-  // The algorithm for domain construction assumes that loops has only a single
-  // exit block (and hence corresponds to a subregion). Note that we cannot use
-  // L->getExitBlock() because it does not check whether all exiting edges point
-  // to the same BB.
+  if (!hasExitingBlocks(L)) {
+    return invalid<ReportLoopHasNoExit>(Context, true, L);
+  }
+
   SmallVector<BasicBlock *, 4> ExitBlocks;
   L->getExitBlocks(ExitBlocks);
-  BasicBlock *TheExitBlock = ExitBlocks[0];
+  BasicBlock *HotExit = nullptr;
+
   for (BasicBlock *ExitBB : ExitBlocks) {
-    if (TheExitBlock != ExitBB)
-      return invalid<ReportLoopHasMultipleExits>(Context, /*Assert=*/true, L);
+    if (PollyAllowErrorBlocks && isErrorBlock(*ExitBB, Context.CurRegion)) {
+      continue;
+    }
+    if (!HotExit) {
+      HotExit = ExitBB;
+      continue;
+    }
+    if (HotExit != ExitBB) {
+      return invalid<ReportLoopHasMultipleExits>(Context, true, L);
+    }
   }
 
-  if (canUseISLTripCount(L, Context))
+  if (!HotExit) {
+    return invalid<ReportLoopHasMultipleExits>(Context, true, L);
+  }
+
+  if (canUseISLTripCount(L, Context)) {
     return true;
+  }
 
   if (AllowNonAffineSubLoops && AllowNonAffineSubRegions) {
     Region *R = RI.getRegionFor(L->getHeader());
-    while (R != &Context.CurRegion && !R->contains(L))
+    while (R != &Context.CurRegion && !R->contains(L)) {
       R = R->getParent();
-
-    if (addOverApproximatedRegion(R, Context))
+    }
+    if (addOverApproximatedRegion(R, Context)) {
       return true;
+    }
   }
 
   const SCEV *LoopCount = SE.getBackedgeTakenCount(L);
-  return invalid<ReportLoopBound>(Context, /*Assert=*/true, L, LoopCount);
+  return invalid<ReportLoopBound>(Context, true, L, LoopCount);
 }
 
-/// Return the number of loops in @p L (incl. @p L) that have a trip
-///        count that is not known to be less than @MinProfitableTrips.
-ScopDetection::LoopStats
-ScopDetection::countBeneficialSubLoops(Loop *L, ScalarEvolution &SE,
-                                       unsigned MinProfitableTrips) {
+ScopDetection::LoopStats ScopDetection::countBeneficialSubLoops(Loop *L, ScalarEvolution &SE,
+                                                                unsigned MinProfitableTrips) {
   const SCEV *TripCount = SE.getBackedgeTakenCount(L);
 
   int NumLoops = 1;
   int MaxLoopDepth = 1;
-  if (MinProfitableTrips > 0)
-    if (auto *TripCountC = dyn_cast<SCEVConstant>(TripCount))
-      if (TripCountC->getType()->getScalarSizeInBits() <= 64)
-        if (TripCountC->getValue()->getZExtValue() <= MinProfitableTrips)
+  if (MinProfitableTrips > 0) {
+    if (auto *TripCountC = dyn_cast<SCEVConstant>(TripCount)) {
+      if (TripCountC->getType()->getScalarSizeInBits() <= 64) {
+        if (TripCountC->getValue()->getZExtValue() <= MinProfitableTrips) {
           NumLoops -= 1;
+        }
+      }
+    }
+  }
 
   for (auto &SubLoop : *L) {
     LoopStats Stats = countBeneficialSubLoops(SubLoop, SE, MinProfitableTrips);
@@ -1386,50 +1216,43 @@ ScopDetection::countBeneficialSubLoops(L
   return {NumLoops, MaxLoopDepth};
 }
 
-ScopDetection::LoopStats
-ScopDetection::countBeneficialLoops(Region *R, ScalarEvolution &SE,
-                                    LoopInfo &LI, unsigned MinProfitableTrips) {
+ScopDetection::LoopStats ScopDetection::countBeneficialLoops(Region *R, ScalarEvolution &SE, LoopInfo &LI,
+                                                             unsigned MinProfitableTrips) {
   int LoopNum = 0;
   int MaxLoopDepth = 0;
 
-  auto L = LI.getLoopFor(R->getEntry());
-
-  // If L is fully contained in R, move to first loop surrounding R. Otherwise,
-  // L is either nullptr or already surrounding R.
+  auto *L = LI.getLoopFor(R->getEntry());
   if (L && R->contains(L)) {
     L = R->outermostLoopInRegion(L);
     L = L->getParentLoop();
   }
 
-  auto SubLoops =
-      L ? L->getSubLoopsVector() : std::vector<Loop *>(LI.begin(), LI.end());
+  auto SubLoops = L ? L->getSubLoopsVector() : std::vector<Loop *>(LI.begin(), LI.end());
 
-  for (auto &SubLoop : SubLoops)
+  for (auto &SubLoop : SubLoops) {
     if (R->contains(SubLoop)) {
-      LoopStats Stats =
-          countBeneficialSubLoops(SubLoop, SE, MinProfitableTrips);
+      LoopStats Stats = countBeneficialSubLoops(SubLoop, SE, MinProfitableTrips);
       LoopNum += Stats.NumLoops;
       MaxLoopDepth = std::max(MaxLoopDepth, Stats.MaxDepth);
     }
+  }
 
   return {LoopNum, MaxLoopDepth};
 }
 
-static bool isErrorBlockImpl(BasicBlock &BB, const Region &R, LoopInfo &LI,
-                             const DominatorTree &DT) {
-  if (isa<UnreachableInst>(BB.getTerminator()))
+static bool isErrorBlockImpl(BasicBlock &BB, const Region &R, LoopInfo &LI, const DominatorTree &DT) {
+  if (isa<UnreachableInst>(BB.getTerminator())) {
     return true;
+  }
 
-  if (LI.isLoopHeader(&BB))
+  if (LI.isLoopHeader(&BB)) {
     return false;
+  }
 
-  // Don't consider something outside the SCoP as error block. It will precede
-  // the code versioning runtime check.
-  if (!R.contains(&BB))
+  if (!R.contains(&BB)) {
     return false;
+  }
 
-  // Basic blocks that are always executed are not considered error blocks,
-  // as their execution can not be a rare event.
   bool DominatesAllPredecessors = true;
   if (R.isTopLevelRegion()) {
     for (BasicBlock &I : *R.getEntry()->getParent()) {
@@ -1439,7 +1262,7 @@ static bool isErrorBlockImpl(BasicBlock
       }
     }
   } else {
-    for (auto Pred : predecessors(R.getExit())) {
+    for (auto *Pred : predecessors(R.getExit())) {
       if (R.contains(Pred) && !DT.dominates(&BB, Pred)) {
         DominatesAllPredecessors = false;
         break;
@@ -1447,45 +1270,49 @@ static bool isErrorBlockImpl(BasicBlock
     }
   }
 
-  if (DominatesAllPredecessors)
+  if (DominatesAllPredecessors) {
     return false;
+  }
 
-  for (Instruction &Inst : BB)
-    if (CallInst *CI = dyn_cast<CallInst>(&Inst)) {
-      if (isDebugCall(CI))
+  for (Instruction &Inst : BB) {
+    if (auto *CI = dyn_cast<CallInst>(&Inst)) {
+      if (isDebugCall(CI)) {
         continue;
-
-      if (isIgnoredIntrinsic(CI))
+      }
+      if (isIgnoredIntrinsic(CI)) {
         continue;
-
-      // memset, memcpy and memmove are modeled intrinsics.
-      if (isa<MemSetInst>(CI) || isa<MemTransferInst>(CI))
+      }
+      if (isa<MemSetInst>(CI) || isa<MemTransferInst>(CI)) {
         continue;
-
-      if (!CI->doesNotAccessMemory())
+      }
+      if (!CI->doesNotAccessMemory()) {
         return true;
-      if (CI->doesNotReturn())
+      }
+      if (CI->doesNotReturn()) {
         return true;
+      }
     }
+  }
 
   return false;
 }
 
 bool ScopDetection::isErrorBlock(llvm::BasicBlock &BB, const llvm::Region &R) {
-  if (!PollyAllowErrorBlocks)
+  if (!PollyAllowErrorBlocks) {
     return false;
+  }
 
-  auto It = ErrorBlockCache.insert({std::make_pair(&BB, &R), false});
-  if (!It.second)
-    return It.first->getSecond();
+  auto InsertResult = ErrorBlockCache.insert({std::make_pair(&BB, &R), false});
+  if (!InsertResult.second) {
+    return InsertResult.first->second;
+  }
 
   bool Result = isErrorBlockImpl(BB, R, LI, DT);
-  It.first->second = Result;
+  InsertResult.first->second = Result;
   return Result;
 }
 
 Region *ScopDetection::expandRegion(Region &R) {
-  // Initial no valid region was found (greater than R)
   std::unique_ptr<Region> LastValidRegion;
   auto ExpandedRegion = std::unique_ptr<Region>(R.getExpandedRegion());
 
@@ -1494,60 +1321,49 @@ Region *ScopDetection::expandRegion(Regi
   while (ExpandedRegion) {
     BBPair P = getBBPairForRegion(ExpandedRegion.get());
     std::unique_ptr<DetectionContext> &Entry = DetectionContextMap[P];
-    Entry = std::make_unique<DetectionContext>(*ExpandedRegion, AA,
-                                               /*Verifying=*/false);
+    Entry = std::make_unique<DetectionContext>(*ExpandedRegion, AA, false);
     DetectionContext &Context = *Entry;
 
-    POLLY_DEBUG(dbgs() << "\t\tTrying " << ExpandedRegion->getNameStr()
-                       << "\n");
-    // Only expand when we did not collect errors.
+    POLLY_DEBUG(dbgs() << "\t\tTrying " << ExpandedRegion->getNameStr() << "\n");
 
     if (!Context.Log.hasErrors()) {
-      // If the exit is valid check all blocks
-      //  - if true, a valid region was found => store it + keep expanding
-      //  - if false, .tbd. => stop  (should this really end the loop?)
       if (!allBlocksValid(Context) || Context.Log.hasErrors()) {
         removeCachedResults(*ExpandedRegion);
         DetectionContextMap.erase(P);
         break;
       }
 
-      // Store this region, because it is the greatest valid (encountered so
-      // far).
       if (LastValidRegion) {
         removeCachedResults(*LastValidRegion);
         DetectionContextMap.erase(P);
       }
       LastValidRegion = std::move(ExpandedRegion);
-
-      // Create and test the next greater region (if any)
-      ExpandedRegion =
-          std::unique_ptr<Region>(LastValidRegion->getExpandedRegion());
+      ExpandedRegion = std::unique_ptr<Region>(LastValidRegion->getExpandedRegion());
 
     } else {
-      // Create and test the next greater region (if any)
       removeCachedResults(*ExpandedRegion);
       DetectionContextMap.erase(P);
-      ExpandedRegion =
-          std::unique_ptr<Region>(ExpandedRegion->getExpandedRegion());
+      ExpandedRegion = std::unique_ptr<Region>(ExpandedRegion->getExpandedRegion());
     }
   }
 
   POLLY_DEBUG({
-    if (LastValidRegion)
+    if (LastValidRegion) {
       dbgs() << "\tto " << LastValidRegion->getNameStr() << "\n";
-    else
+    } else {
       dbgs() << "\tExpanding " << R.getNameStr() << " failed\n";
+    }
   });
 
   return LastValidRegion.release();
 }
 
 static bool regionWithoutLoops(Region &R, LoopInfo &LI) {
-  for (const BasicBlock *BB : R.blocks())
-    if (R.contains(LI.getLoopFor(BB)))
+  for (const BasicBlock *BB : R.blocks()) {
+    if (R.contains(LI.getLoopFor(BB))) {
       return false;
-
+    }
+  }
   return true;
 }
 
@@ -1555,73 +1371,66 @@ void ScopDetection::removeCachedResultsR
   for (auto &SubRegion : R) {
     if (ValidRegions.count(SubRegion.get())) {
       removeCachedResults(*SubRegion);
-    } else
+    } else {
       removeCachedResultsRecursively(*SubRegion);
+    }
   }
 }
 
-void ScopDetection::removeCachedResults(const Region &R) {
-  ValidRegions.remove(&R);
-}
+void ScopDetection::removeCachedResults(const Region &R) { ValidRegions.remove(&R); }
 
 void ScopDetection::findScops(Region &R) {
-  std::unique_ptr<DetectionContext> &Entry =
-      DetectionContextMap[getBBPairForRegion(&R)];
-  Entry = std::make_unique<DetectionContext>(R, AA, /*Verifying=*/false);
+  std::unique_ptr<DetectionContext> &Entry = DetectionContextMap[getBBPairForRegion(&R)];
+  Entry = std::make_unique<DetectionContext>(R, AA, false);
   DetectionContext &Context = *Entry;
 
   bool DidBailout = true;
-  if (!PollyProcessUnprofitable && regionWithoutLoops(R, LI))
-    invalid<ReportUnprofitable>(Context, /*Assert=*/true, &R);
-  else
+  if (!PollyProcessUnprofitable && regionWithoutLoops(R, LI)) {
+    invalid<ReportUnprofitable>(Context, true, &R);
+  } else {
     DidBailout = !isValidRegion(Context);
+  }
 
   (void)DidBailout;
   if (KeepGoing) {
     assert((!DidBailout || Context.IsInvalid) &&
-           "With -polly-detect-keep-going, it is sufficient that if "
-           "isValidRegion short-circuited, that SCoP is invalid");
+           "With -polly-detect-keep-going, it is sufficient that if isValidRegion short-circuited, that SCoP is invalid");
   } else {
-    assert(DidBailout == Context.IsInvalid &&
-           "isValidRegion must short-circuit iff the ScoP is invalid");
+    assert(DidBailout == Context.IsInvalid && "isValidRegion must short-circuit iff the ScoP is invalid");
   }
 
-  if (Context.IsInvalid) {
-    removeCachedResults(R);
-  } else {
+  if (!Context.IsInvalid) {
     ValidRegions.insert(&R);
     return;
   }
 
-  for (auto &SubRegion : R)
+  removeCachedResults(R);
+
+  std::vector<Region *> RegionsToExpand;
+  unsigned SubRegionCounter = 0;
+
+  for (auto &SubRegion : R) {
+    if (++SubRegionCounter > PollyMaxSubRegions) {
+      POLLY_DEBUG(dbgs() << "Polly: Region '" << R.getNameStr() << "' has too many sub-regions (>" << PollyMaxSubRegions
+                         << "). Skipping recursion.\n");
+      return;
+    }
     findScops(*SubRegion);
+    RegionsToExpand.push_back(SubRegion.get());
+  }
 
-  // Try to expand regions.
-  //
-  // As the region tree normally only contains canonical regions, non canonical
-  // regions that form a Scop are not found. Therefore, those non canonical
-  // regions are checked by expanding the canonical ones.
-
-  std::vector<Region *> ToExpand;
-
-  for (auto &SubRegion : R)
-    ToExpand.push_back(SubRegion.get());
-
-  for (Region *CurrentRegion : ToExpand) {
-    // Skip invalid regions. Regions may become invalid, if they are element of
-    // an already expanded region.
-    if (!ValidRegions.count(CurrentRegion))
+  for (Region *CurrentRegion : RegionsToExpand) {
+    if (!ValidRegions.count(CurrentRegion)) {
       continue;
-
-    // Skip regions that had errors.
-    bool HadErrors = lookupRejectionLog(CurrentRegion)->hasErrors();
-    if (HadErrors)
+    }
+    if (lookupRejectionLog(CurrentRegion)->hasErrors()) {
       continue;
+    }
 
     Region *ExpandedR = expandRegion(*CurrentRegion);
-
-    if (!ExpandedR)
+    if (!ExpandedR) {
       continue;
+    }
 
     R.addSubRegion(ExpandedR, true);
     ValidRegions.insert(ExpandedR);
@@ -1639,16 +1448,18 @@ bool ScopDetection::allBlocksValid(Detec
       if (CurRegion.contains(L)) {
         if (!isValidLoop(L, Context)) {
           Context.IsInvalid = true;
-          if (!KeepGoing)
+          if (!KeepGoing) {
             return false;
+          }
         }
       } else {
         SmallVector<BasicBlock *, 1> Latches;
         L->getLoopLatches(Latches);
-        for (BasicBlock *Latch : Latches)
-          if (CurRegion.contains(Latch))
-            return invalid<ReportLoopOnlySomeLatches>(Context, /*Assert=*/true,
-                                                      L);
+        for (BasicBlock *Latch : Latches) {
+          if (CurRegion.contains(Latch)) {
+            return invalid<ReportLoopOnlySomeLatches>(Context, true, L);
+          }
+        }
       }
     }
   }
@@ -1656,60 +1467,65 @@ bool ScopDetection::allBlocksValid(Detec
   for (BasicBlock *BB : CurRegion.blocks()) {
     bool IsErrorBlock = isErrorBlock(*BB, CurRegion);
 
-    // Also check exception blocks (and possibly register them as non-affine
-    // regions). Even though exception blocks are not modeled, we use them
-    // to forward-propagate domain constraints during ScopInfo construction.
-    if (!isValidCFG(*BB, false, IsErrorBlock, Context) && !KeepGoing)
+    if (!isValidCFG(*BB, false, IsErrorBlock, Context) && !KeepGoing) {
       return false;
+    }
 
-    if (IsErrorBlock)
+    if (IsErrorBlock) {
       continue;
+    }
 
-    for (BasicBlock::iterator I = BB->begin(), E = --BB->end(); I != E; ++I)
+    for (BasicBlock::iterator I = BB->begin(), E = --BB->end(); I != E; ++I) {
       if (!isValidInstruction(*I, Context)) {
         Context.IsInvalid = true;
-        if (!KeepGoing)
+        if (!KeepGoing) {
           return false;
+        }
       }
+    }
   }
 
-  if (!hasAffineMemoryAccesses(Context))
+  if (!hasAffineMemoryAccesses(Context)) {
     return false;
+  }
 
   return true;
 }
 
-bool ScopDetection::hasSufficientCompute(DetectionContext &Context,
-                                         int NumLoops) const {
+bool ScopDetection::hasSufficientCompute(DetectionContext &Context, int NumLoops) const {
   int InstCount = 0;
-
-  if (NumLoops == 0)
+  if (NumLoops == 0) {
     return false;
+  }
 
-  for (auto *BB : Context.CurRegion.blocks())
-    if (Context.CurRegion.contains(LI.getLoopFor(BB)))
+  for (auto *BB : Context.CurRegion.blocks()) {
+    if (Context.CurRegion.contains(LI.getLoopFor(BB))) {
       InstCount += BB->size();
+    }
+  }
 
   InstCount = InstCount / NumLoops;
-
   return InstCount >= ProfitabilityMinPerLoopInstructions;
 }
 
-bool ScopDetection::hasPossiblyDistributableLoop(
-    DetectionContext &Context) const {
+bool ScopDetection::hasPossiblyDistributableLoop(DetectionContext &Context) const {
   for (auto *BB : Context.CurRegion.blocks()) {
     auto *L = LI.getLoopFor(BB);
-    if (!L)
+    if (!L) {
       continue;
-    if (!Context.CurRegion.contains(L))
+    }
+    if (!Context.CurRegion.contains(L)) {
       continue;
-    if (Context.BoxedLoopsSet.count(L))
+    }
+    if (Context.BoxedLoopsSet.count(L)) {
       continue;
+    }
     unsigned StmtsWithStoresInLoops = 0;
     for (auto *LBB : L->blocks()) {
       bool MemStore = false;
-      for (auto &I : *LBB)
+      for (auto &I : *LBB) {
         MemStore |= isa<StoreInst>(&I);
+      }
       StmtsWithStoresInLoops += MemStore;
     }
     return (StmtsWithStoresInLoops > 1);
@@ -1720,44 +1536,55 @@ bool ScopDetection::hasPossiblyDistribut
 bool ScopDetection::isProfitableRegion(DetectionContext &Context) const {
   Region &CurRegion = Context.CurRegion;
 
-  if (PollyProcessUnprofitable)
+  if (PollyProcessUnprofitable) {
     return true;
+  }
 
-  // We can probably not do a lot on scops that only write or only read
-  // data.
-  if (!Context.hasStores || !Context.hasLoads)
-    return invalid<ReportUnprofitable>(Context, /*Assert=*/true, &CurRegion);
+  if (!Context.hasStores && !Context.hasLoads) {
+    return invalid<ReportUnprofitable>(Context, true, &CurRegion);
+  }
 
-  int NumLoops =
-      countBeneficialLoops(&CurRegion, SE, LI, MIN_LOOP_TRIP_COUNT).NumLoops;
+  int NumLoops = countBeneficialLoops(&CurRegion, SE, LI, MIN_LOOP_TRIP_COUNT).NumLoops;
   int NumAffineLoops = NumLoops - Context.BoxedLoopsSet.size();
 
-  // Scops with at least two loops may allow either loop fusion or tiling and
-  // are consequently interesting to look at.
-  if (NumAffineLoops >= 2)
+  if (NumAffineLoops >= 2) {
     return true;
+  }
 
-  // A loop with multiple non-trivial blocks might be amendable to distribution.
-  if (NumAffineLoops == 1 && hasPossiblyDistributableLoop(Context))
-    return true;
+  if (NumAffineLoops == 1) {
+    if (hasPossiblyDistributableLoop(Context)) {
+      return true;
+    }
+    if (!Context.hasStores) {
+      return invalid<ReportUnprofitable>(Context, true, &CurRegion);
+    }
 
-  // Scops that contain a loop with a non-trivial amount of computation per
-  // loop-iteration are interesting as we may be able to parallelize such
-  // loops. Individual loops that have only a small amount of computation
-  // per-iteration are performance-wise very fragile as any change to the
-  // loop induction variables may affect performance. To not cause spurious
-  // performance regressions, we do not consider such loops.
-  if (NumAffineLoops == 1 && hasSufficientCompute(Context, NumLoops))
-    return true;
+    unsigned MemOpCount = 0;
+    for (BasicBlock *BB : CurRegion.blocks()) {
+      const Loop *L = LI.getLoopFor(BB);
+      if (L && CurRegion.contains(L) && !Context.BoxedLoopsSet.count(L)) {
+        for (Instruction &Inst : *BB) {
+          if (Inst.mayReadOrWriteMemory()) {
+            if (isa<AllocaInst>(Inst) || isDebugCall(&Inst)) {
+              continue;
+            }
+            MemOpCount++;
+          }
+        }
+      }
+    }
+    if (MemOpCount >= PollyMinMemops) {
+      return true;
+    }
+  }
 
-  return invalid<ReportUnprofitable>(Context, /*Assert=*/true, &CurRegion);
+  return invalid<ReportUnprofitable>(Context, true, &CurRegion);
 }
 
 bool ScopDetection::isValidRegion(DetectionContext &Context) {
   Region &CurRegion = Context.CurRegion;
 
-  POLLY_DEBUG(dbgs() << "Checking region: " << CurRegion.getNameStr()
-                     << "\n\t");
+  POLLY_DEBUG(dbgs() << "Checking region: " << CurRegion.getNameStr() << "\n\t");
 
   if (!PollyAllowFullFunction && CurRegion.isTopLevelRegion()) {
     POLLY_DEBUG(dbgs() << "Top level region is invalid\n");
@@ -1766,15 +1593,12 @@ bool ScopDetection::isValidRegion(Detect
   }
 
   DebugLoc DbgLoc;
-  if (CurRegion.getExit() &&
-      isa<UnreachableInst>(CurRegion.getExit()->getTerminator())) {
+  if (CurRegion.getExit() && isa<UnreachableInst>(CurRegion.getExit()->getTerminator())) {
     POLLY_DEBUG(dbgs() << "Unreachable in exit\n");
-    return invalid<ReportUnreachableInExit>(Context, /*Assert=*/true,
-                                            CurRegion.getExit(), DbgLoc);
+    return invalid<ReportUnreachableInExit>(Context, true, CurRegion.getExit(), DbgLoc);
   }
 
-  if (!OnlyRegion.empty() &&
-      !CurRegion.getEntry()->getName().count(OnlyRegion)) {
+  if (!OnlyRegion.empty() && !CurRegion.getEntry()->getName().count(OnlyRegion)) {
     POLLY_DEBUG({
       dbgs() << "Region entry does not match -polly-only-region";
       dbgs() << "\n";
@@ -1785,47 +1609,56 @@ bool ScopDetection::isValidRegion(Detect
 
   for (BasicBlock *Pred : predecessors(CurRegion.getEntry())) {
     Instruction *PredTerm = Pred->getTerminator();
-    if (isa<IndirectBrInst>(PredTerm) || isa<CallBrInst>(PredTerm))
-      return invalid<ReportIndirectPredecessor>(
-          Context, /*Assert=*/true, PredTerm, PredTerm->getDebugLoc());
+    if (isa<IndirectBrInst>(PredTerm) || isa<CallBrInst>(PredTerm)) {
+      return invalid<ReportIndirectPredecessor>(Context, true, PredTerm, PredTerm->getDebugLoc());
+    }
   }
 
-  // SCoP cannot contain the entry block of the function, because we need
-  // to insert alloca instruction there when translate scalar to array.
   if (!PollyAllowFullFunction &&
-      CurRegion.getEntry() ==
-          &(CurRegion.getEntry()->getParent()->getEntryBlock()))
-    return invalid<ReportEntry>(Context, /*Assert=*/true, CurRegion.getEntry());
+      CurRegion.getEntry() == &(CurRegion.getEntry()->getParent()->getEntryBlock())) {
+    return invalid<ReportEntry>(Context, true, CurRegion.getEntry());
+  }
 
   if (!allBlocksValid(Context)) {
-    // TODO: Every failure condition within allBlocksValid should call
-    // invalid<Reason>(). Otherwise we reject SCoPs without giving feedback to
-    // the user.
     Context.IsInvalid = true;
     return false;
   }
 
-  if (!isReducibleRegion(CurRegion, DbgLoc))
-    return invalid<ReportIrreducibleRegion>(Context, /*Assert=*/true,
-                                            &CurRegion, DbgLoc);
+  if (!isReducibleRegion(CurRegion, DbgLoc)) {
+    return invalid<ReportIrreducibleRegion>(Context, true, &CurRegion, DbgLoc);
+  }
 
   POLLY_DEBUG(dbgs() << "OK\n");
   return true;
 }
 
-void ScopDetection::markFunctionAsInvalid(Function *F) {
-  F->addFnAttr(PollySkipFnAttr);
-}
+void ScopDetection::markFunctionAsInvalid(Function *F) { F->addFnAttr(PollySkipFnAttr); }
 
 bool ScopDetection::isValidFunction(Function &F) {
-  return !F.hasFnAttribute(PollySkipFnAttr);
+  if (F.hasFnAttribute(PollySkipFnAttr) || F.hasFnAttribute(Attribute::OptimizeNone)) {
+    return false;
+  }
+
+  for (BasicBlock &BB : F) {
+    const Instruction *Terminator = BB.getTerminator();
+    if (isa<InvokeInst>(Terminator) || isa<CatchSwitchInst>(Terminator) || isa<CatchReturnInst>(Terminator) ||
+        isa<CleanupReturnInst>(Terminator)) {
+      return false;
+    }
+    if (const auto *SI = dyn_cast<SwitchInst>(Terminator)) {
+      if (SI->getNumCases() > PollyMaxSwitchCases) {
+        return false;
+      }
+    }
+  }
+
+  return true;
 }
 
 void ScopDetection::printLocations(Function &F) {
   for (const Region *R : *this) {
     unsigned LineEntry, LineExit;
     std::string FileName;
-
     getDebugLocation(R, LineEntry, LineExit, FileName);
     DiagnosticScopFound Diagnostic(F, FileName, LineEntry, LineExit);
     F.getContext().diagnose(Diagnostic);
@@ -1835,131 +1668,112 @@ void ScopDetection::printLocations(Funct
 void ScopDetection::emitMissedRemarks(const Function &F) {
   for (auto &DIt : DetectionContextMap) {
     DetectionContext &DC = *DIt.getSecond();
-    if (DC.Log.hasErrors())
+    if (DC.Log.hasErrors()) {
       emitRejectionRemarks(DIt.getFirst(), DC.Log, ORE);
+    }
   }
 }
 
 bool ScopDetection::isReducibleRegion(Region &R, DebugLoc &DbgLoc) const {
-  /// Enum for coloring BBs in Region.
-  ///
-  /// WHITE - Unvisited BB in DFS walk.
-  /// GREY - BBs which are currently on the DFS stack for processing.
-  /// BLACK - Visited and completely processed BB.
   enum Color { WHITE, GREY, BLACK };
 
   BasicBlock *REntry = R.getEntry();
   BasicBlock *RExit = R.getExit();
-  // Map to match the color of a BasicBlock during the DFS walk.
-  DenseMap<const BasicBlock *, Color> BBColorMap;
-  // Stack keeping track of current BB and index of next child to be processed.
-  std::stack<std::pair<BasicBlock *, unsigned>> DFSStack;
-
-  unsigned AdjacentBlockIndex = 0;
-  BasicBlock *CurrBB, *SuccBB;
-  CurrBB = REntry;
 
-  // Initialize the map for all BB with WHITE color.
-  for (auto *BB : R.blocks())
+  DenseMap<const BasicBlock *, Color> BBColorMap;
+  for (auto *BB : R.blocks()) {
     BBColorMap[BB] = WHITE;
+  }
+
+  using StackElem = std::pair<BasicBlock *, unsigned>;
+  SmallVector<StackElem, 32> DFSStack;
 
-  // Process the entry block of the Region.
+  BasicBlock *CurrBB = REntry;
   BBColorMap[CurrBB] = GREY;
-  DFSStack.push(std::make_pair(CurrBB, 0));
+  DFSStack.push_back({CurrBB, 0});
 
   while (!DFSStack.empty()) {
-    // Get next BB on stack to be processed.
-    CurrBB = DFSStack.top().first;
-    AdjacentBlockIndex = DFSStack.top().second;
-    DFSStack.pop();
+    auto [BB, NextIdx] = DFSStack.pop_back_val();
+    CurrBB = BB;
+    unsigned AdjacentBlockIndex = NextIdx;
 
-    // Loop to iterate over the successors of current BB.
     const Instruction *TInst = CurrBB->getTerminator();
     unsigned NSucc = TInst->getNumSuccessors();
-    for (unsigned I = AdjacentBlockIndex; I < NSucc;
-         ++I, ++AdjacentBlockIndex) {
-      SuccBB = TInst->getSuccessor(I);
 
-      // Checks for region exit block and self-loops in BB.
-      if (SuccBB == RExit || SuccBB == CurrBB)
+    for (unsigned I = AdjacentBlockIndex; I < NSucc; ++I, ++AdjacentBlockIndex) {
+      BasicBlock *SuccBB = TInst->getSuccessor(I);
+
+      if (SuccBB == RExit || SuccBB == CurrBB) {
         continue;
+      }
 
-      // WHITE indicates an unvisited BB in DFS walk.
-      if (BBColorMap[SuccBB] == WHITE) {
-        // Push the current BB and the index of the next child to be visited.
-        DFSStack.push(std::make_pair(CurrBB, I + 1));
-        // Push the next BB to be processed.
-        DFSStack.push(std::make_pair(SuccBB, 0));
-        // First time the BB is being processed.
-        BBColorMap[SuccBB] = GREY;
-        break;
-      } else if (BBColorMap[SuccBB] == GREY) {
-        // GREY indicates a loop in the control flow.
-        // If the destination dominates the source, it is a natural loop
-        // else, an irreducible control flow in the region is detected.
+      Color &SuccColor = BBColorMap[SuccBB];
+      if (SuccColor == WHITE) {
+        DFSStack.push_back({CurrBB, I + 1});
+        DFSStack.push_back({SuccBB, 0});
+        SuccColor = GREY;
+        goto NextIteration;
+      } else if (SuccColor == GREY) {
         if (!DT.dominates(SuccBB, CurrBB)) {
-          // Get debug info of instruction which causes irregular control flow.
           DbgLoc = TInst->getDebugLoc();
           return false;
         }
       }
     }
 
-    // If all children of current BB have been processed,
-    // then mark that BB as fully processed.
-    if (AdjacentBlockIndex == NSucc)
-      BBColorMap[CurrBB] = BLACK;
+    BBColorMap[CurrBB] = BLACK;
+  NextIteration:
+    (void)0;
   }
 
   return true;
 }
 
-static void updateLoopCountStatistic(ScopDetection::LoopStats Stats,
-                                     bool OnlyProfitable) {
+static void updateLoopCountStatistic(ScopDetection::LoopStats Stats, bool OnlyProfitable) {
   if (!OnlyProfitable) {
     NumLoopsInScop += Stats.NumLoops;
-    MaxNumLoopsInScop =
-        std::max(MaxNumLoopsInScop.getValue(), (uint64_t)Stats.NumLoops);
-    if (Stats.MaxDepth == 0)
+    MaxNumLoopsInScop = std::max(MaxNumLoopsInScop.getValue(), (uint64_t)Stats.NumLoops);
+    if (Stats.MaxDepth == 0) {
       NumScopsDepthZero++;
-    else if (Stats.MaxDepth == 1)
+    } else if (Stats.MaxDepth == 1) {
       NumScopsDepthOne++;
-    else if (Stats.MaxDepth == 2)
+    } else if (Stats.MaxDepth == 2) {
       NumScopsDepthTwo++;
-    else if (Stats.MaxDepth == 3)
+    } else if (Stats.MaxDepth == 3) {
       NumScopsDepthThree++;
-    else if (Stats.MaxDepth == 4)
+    } else if (Stats.MaxDepth == 4) {
       NumScopsDepthFour++;
-    else if (Stats.MaxDepth == 5)
+    } else if (Stats.MaxDepth == 5) {
       NumScopsDepthFive++;
-    else
+    } else {
       NumScopsDepthLarger++;
+    }
   } else {
     NumLoopsInProfScop += Stats.NumLoops;
-    MaxNumLoopsInProfScop =
-        std::max(MaxNumLoopsInProfScop.getValue(), (uint64_t)Stats.NumLoops);
-    if (Stats.MaxDepth == 0)
+    MaxNumLoopsInProfScop = std::max(MaxNumLoopsInProfScop.getValue(), (uint64_t)Stats.NumLoops);
+    if (Stats.MaxDepth == 0) {
       NumProfScopsDepthZero++;
-    else if (Stats.MaxDepth == 1)
+    } else if (Stats.MaxDepth == 1) {
       NumProfScopsDepthOne++;
-    else if (Stats.MaxDepth == 2)
+    } else if (Stats.MaxDepth == 2) {
       NumProfScopsDepthTwo++;
-    else if (Stats.MaxDepth == 3)
+    } else if (Stats.MaxDepth == 3) {
       NumProfScopsDepthThree++;
-    else if (Stats.MaxDepth == 4)
+    } else if (Stats.MaxDepth == 4) {
       NumProfScopsDepthFour++;
-    else if (Stats.MaxDepth == 5)
+    } else if (Stats.MaxDepth == 5) {
       NumProfScopsDepthFive++;
-    else
+    } else {
       NumProfScopsDepthLarger++;
+    }
   }
 }
 
-ScopDetection::DetectionContext *
-ScopDetection::getDetectionContext(const Region *R) const {
+ScopDetection::DetectionContext *ScopDetection::getDetectionContext(const Region *R) const {
   auto DCMIt = DetectionContextMap.find(getBBPairForRegion(R));
-  if (DCMIt == DetectionContextMap.end())
+  if (DCMIt == DetectionContextMap.end()) {
     return nullptr;
+  }
   return DCMIt->second.get();
 }
 
@@ -1970,17 +1784,17 @@ const RejectLog *ScopDetection::lookupRe
 
 void ScopDetection::verifyRegion(const Region &R) {
   assert(isMaxRegionInScop(R) && "Expect R is a valid region.");
-
-  DetectionContext Context(const_cast<Region &>(R), AA, true /*verifying*/);
+  DetectionContext Context(const_cast<Region &>(R), AA, true);
   isValidRegion(Context);
 }
 
 void ScopDetection::verifyAnalysis() {
-  if (!VerifyScops)
+  if (!VerifyScops) {
     return;
-
-  for (const Region *R : ValidRegions)
+  }
+  for (const Region *R : ValidRegions) {
     verifyRegion(*R);
+  }
 }
 
 bool ScopDetectionWrapperPass::runOnFunction(Function &F) {
@@ -1991,6 +1805,18 @@ bool ScopDetectionWrapperPass::runOnFunc
   auto &DT = getAnalysis<DominatorTreeWrapperPass>().getDomTree();
   auto &ORE = getAnalysis<OptimizationRemarkEmitterWrapperPass>().getORE();
 
+  if (F.size() > PollyScopsMaxBlocks) {
+    LLVM_DEBUG(dbgs() << "Polly: Function '" << F.getName() << "' is too large (" << F.size() << " basic blocks > "
+                      << PollyScopsMaxBlocks << ") for Scop analysis. Skipping.\n");
+    ORE.emit([&]() {
+      return OptimizationRemarkMissed("polly-detect", "FunctionTooLarge", &F)
+             << "Polly analysis skipped: function is too large ("
+             << ore::NV("NumBlocks", F.size()) << " basic blocks, limit is "
+             << ore::NV("BlockLimit", PollyScopsMaxBlocks) << ").";
+    });
+    return false;
+  }
+
   Result = std::make_unique<ScopDetection>(DT, SE, LI, RI, AA, ORE);
   Result->detect(F);
   return false;
@@ -2001,29 +1827,28 @@ void ScopDetectionWrapperPass::getAnalys
   AU.addRequiredTransitive<ScalarEvolutionWrapperPass>();
   AU.addRequired<DominatorTreeWrapperPass>();
   AU.addRequired<OptimizationRemarkEmitterWrapperPass>();
-  // We also need AA and RegionInfo when we are verifying analysis.
   AU.addRequiredTransitive<AAResultsWrapperPass>();
   AU.addRequiredTransitive<RegionInfoPass>();
   AU.setPreservesAll();
 }
 
 void ScopDetectionWrapperPass::print(raw_ostream &OS, const Module *) const {
-  for (const Region *R : Result->ValidRegions)
+  for (const Region *R : Result->ValidRegions) {
     OS << "Valid Region for Scop: " << R->getNameStr() << '\n';
-
+  }
   OS << "\n";
 }
 
 ScopDetectionWrapperPass::ScopDetectionWrapperPass() : FunctionPass(ID) {
-  // Disable runtime alias checks if we ignore aliasing all together.
-  if (IgnoreAliasing)
+  if (IgnoreAliasing) {
     PollyUseRuntimeAliasChecks = false;
+  }
 }
 
 ScopAnalysis::ScopAnalysis() {
-  // Disable runtime alias checks if we ignore aliasing all together.
-  if (IgnoreAliasing)
+  if (IgnoreAliasing) {
     PollyUseRuntimeAliasChecks = false;
+  }
 }
 
 void ScopDetectionWrapperPass::releaseMemory() { Result.reset(); }
@@ -2040,29 +1865,37 @@ ScopDetection ScopAnalysis::run(Function
   auto &DT = FAM.getResult<DominatorTreeAnalysis>(F);
   auto &ORE = FAM.getResult<OptimizationRemarkEmitterAnalysis>(F);
 
+  if (F.size() > PollyScopsMaxBlocks) {
+    LLVM_DEBUG(dbgs() << "Polly: Function '" << F.getName() << "' is too large (" << F.size() << " basic blocks > "
+                      << PollyScopsMaxBlocks << ") for Scop analysis. Skipping.\n");
+    ORE.emit([&]() {
+      return OptimizationRemarkMissed("polly-detect", "FunctionTooLarge", &F)
+             << "Polly analysis skipped: function is too large ("
+             << ore::NV("NumBlocks", F.size()) << " basic blocks, limit is "
+             << ore::NV("BlockLimit", PollyScopsMaxBlocks) << ").";
+    });
+    return ScopDetection(DT, SE, LI, RI, AA, ORE);
+  }
+
   ScopDetection Result(DT, SE, LI, RI, AA, ORE);
   Result.detect(F);
   return Result;
 }
 
-PreservedAnalyses ScopAnalysisPrinterPass::run(Function &F,
-                                               FunctionAnalysisManager &FAM) {
+PreservedAnalyses ScopAnalysisPrinterPass::run(Function &F, FunctionAnalysisManager &FAM) {
   OS << "Detected Scops in Function " << F.getName() << "\n";
   auto &SD = FAM.getResult<ScopAnalysis>(F);
-  for (const Region *R : SD.ValidRegions)
+  for (const Region *R : SD.ValidRegions) {
     OS << "Valid Region for Scop: " << R->getNameStr() << '\n';
-
+  }
   OS << "\n";
   return PreservedAnalyses::all();
 }
 
-Pass *polly::createScopDetectionWrapperPassPass() {
-  return new ScopDetectionWrapperPass();
-}
+Pass *polly::createScopDetectionWrapperPassPass() { return new ScopDetectionWrapperPass(); }
 
 INITIALIZE_PASS_BEGIN(ScopDetectionWrapperPass, "polly-detect",
-                      "Polly - Detect static control parts (SCoPs)", false,
-                      false);
+                      "Polly - Detect static control parts (SCoPs)", false, false);
 INITIALIZE_PASS_DEPENDENCY(AAResultsWrapperPass);
 INITIALIZE_PASS_DEPENDENCY(LoopInfoWrapperPass);
 INITIALIZE_PASS_DEPENDENCY(RegionInfoPass);
@@ -2072,26 +1905,20 @@ INITIALIZE_PASS_DEPENDENCY(OptimizationR
 INITIALIZE_PASS_END(ScopDetectionWrapperPass, "polly-detect",
                     "Polly - Detect static control parts (SCoPs)", false, false)
 
-//===----------------------------------------------------------------------===//
-
 namespace {
-/// Print result from ScopDetectionWrapperPass.
+
 class ScopDetectionPrinterLegacyPass final : public FunctionPass {
 public:
   static char ID;
 
   ScopDetectionPrinterLegacyPass() : ScopDetectionPrinterLegacyPass(outs()) {}
-
-  explicit ScopDetectionPrinterLegacyPass(llvm::raw_ostream &OS)
-      : FunctionPass(ID), OS(OS) {}
+  explicit ScopDetectionPrinterLegacyPass(llvm::raw_ostream &OS) : FunctionPass(ID), OS(OS) {}
 
   bool runOnFunction(Function &F) override {
     ScopDetectionWrapperPass &P = getAnalysis<ScopDetectionWrapperPass>();
 
-    OS << "Printing analysis '" << P.getPassName() << "' for function '"
-       << F.getName() << "':\n";
+    OS << "Printing analysis '" << P.getPassName() << "' for function '" << F.getName() << "':\n";
     P.print(OS);
-
     return false;
   }
 
@@ -2106,6 +1933,7 @@ private:
 };
 
 char ScopDetectionPrinterLegacyPass::ID = 0;
+
 } // namespace
 
 Pass *polly::createScopDetectionPrinterLegacyPass(raw_ostream &OS) {
@@ -2113,8 +1941,7 @@ Pass *polly::createScopDetectionPrinterL
 }
 
 INITIALIZE_PASS_BEGIN(ScopDetectionPrinterLegacyPass, "polly-print-detect",
-                      "Polly - Print static control parts (SCoPs)", false,
-                      false);
+                      "Polly - Print static control parts (SCoPs)", false, false);
 INITIALIZE_PASS_DEPENDENCY(ScopDetectionWrapperPass);
 INITIALIZE_PASS_END(ScopDetectionPrinterLegacyPass, "polly-print-detect",
                     "Polly - Print static control parts (SCoPs)", false, false)

--- a/polly/lib/Analysis/PolyhedralInfo.cpp	2025-09-21 15:19:47.774089816 +0200
+++ b/polly/lib/Analysis/PolyhedralInfo.cpp	2025-09-21 15:22:42.837583285 +0200
@@ -1,20 +1,9 @@
 //===--------- PolyhedralInfo.cpp  - Create Scops from LLVM IR-------------===//
 //
 // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
-// See https://llvm.org/LICENSE.txt for license information.
 // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 //
-//===----------------------------------------------------------------------===//
-//
-// An interface to the Polyhedral analysis engine(Polly) of LLVM.
-//
-// This pass provides an interface to the polyhedral analysis performed by
-// Polly.
-//
-// This interface provides basic interface like isParallel, isVectorizable
-// that can be used in LLVM transformation passes.
-//
-// Work in progress, this file is subject to change.
+// Interface to Polly's polyhedral analysis.
 //
 //===----------------------------------------------------------------------===//
 
@@ -27,6 +16,7 @@
 #include "llvm/Analysis/LoopInfo.h"
 #include "llvm/InitializePasses.h"
 #include "llvm/Support/Debug.h"
+#include "isl/map.h"
 #include "isl/union_map.h"
 
 using namespace llvm;
@@ -61,23 +51,27 @@ void PolyhedralInfo::print(raw_ostream &
   for (auto *TopLevelLoop : LI) {
     for (auto *L : depth_first(TopLevelLoop)) {
       OS.indent(2) << L->getHeader()->getName() << ":\t";
-      if (CheckParallel && isParallel(L))
+      if (CheckParallel && isParallel(L)) {
         OS << "Loop is parallel.\n";
-      else if (CheckParallel)
+      } else if (CheckParallel) {
         OS << "Loop is not parallel.\n";
+      }
     }
   }
 }
 
 bool PolyhedralInfo::checkParallel(Loop *L, isl_pw_aff **MinDepDistPtr) const {
-  bool IsParallel;
   const Scop *S = getScopContainingLoop(L);
-  if (!S)
+  if (!S) {
     return false;
+  }
+
   const Dependences &D =
       DI->getDependences(const_cast<Scop *>(S), Dependences::AL_Access);
-  if (!D.hasValidDependences())
+  if (!D.hasValidDependences()) {
     return false;
+  }
+
   POLLY_DEBUG(dbgs() << "Loop :\t" << L->getHeader()->getName() << ":\n");
 
   isl_union_map *Deps =
@@ -92,36 +86,27 @@ bool PolyhedralInfo::checkParallel(Loop
   POLLY_DEBUG(dbgs() << "Schedule: \t" << stringFromIslObj(Schedule, "null")
                      << "\n");
 
-  IsParallel = D.isParallel(Schedule, Deps, MinDepDistPtr);
+  bool IsParallel = D.isParallel(Schedule, Deps, MinDepDistPtr);
   isl_union_map_free(Schedule);
   return IsParallel;
 }
 
-bool PolyhedralInfo::isParallel(Loop *L) const { return checkParallel(L); }
+bool PolyhedralInfo::isParallel(Loop *L) const {
+  return checkParallel(L, nullptr);
+}
 
 const Scop *PolyhedralInfo::getScopContainingLoop(Loop *L) const {
-  assert((SI) && "ScopInfoWrapperPass is required by PolyhedralInfo pass!\n");
+  assert(SI && "ScopInfoWrapperPass is required by PolyhedralInfo pass!");
   for (auto &It : *SI) {
     Region *R = It.first;
-    if (R->contains(L))
+    if (R->contains(L)) {
       return It.second.get();
+    }
   }
   return nullptr;
 }
 
-//  Given a Loop and the containing SCoP, we compute the partial schedule
-//  by taking union of individual schedules of each ScopStmt within the loop
-//  and projecting out the inner dimensions from the range of the schedule.
-//   for (i = 0; i < n; i++)
-//      for (j = 0; j < n; j++)
-//        A[j] = 1;  //Stmt
-//
-//  The original schedule will be
-//    Stmt[i0, i1] -> [i0, i1]
-//  The schedule for the outer loop will be
-//    Stmt[i0, i1] -> [i0]
-//  The schedule for the inner loop will be
-//    Stmt[i0, i1] -> [i0, i1]
+// Union of statement schedules at loop depth CurrDim, inner dims projected out.
 __isl_give isl_union_map *PolyhedralInfo::getScheduleForLoop(const Scop *S,
                                                              Loop *L) const {
   isl_union_map *Schedule = isl_union_map_empty(S->getParamSpace().release());
@@ -130,23 +115,29 @@ __isl_give isl_union_map *PolyhedralInfo
   assert(CurrDim >= 0 && "Loop in region should have at least depth one");
 
   for (auto &SS : *S) {
-    if (L->contains(SS.getSurroundingLoop())) {
+    if (!L->contains(SS.getSurroundingLoop())) {
+      continue;
+    }
+
+    unsigned MaxDim = SS.getNumIterators();
+    POLLY_DEBUG(dbgs() << "Maximum depth of Stmt:\t" << MaxDim << "\n");
 
-      unsigned int MaxDim = SS.getNumIterators();
-      POLLY_DEBUG(dbgs() << "Maximum depth of Stmt:\t" << MaxDim << "\n");
-      isl_map *ScheduleMap = SS.getSchedule().release();
-      assert(
-          ScheduleMap &&
-          "Schedules that contain extension nodes require special handling.");
-
-      ScheduleMap = isl_map_project_out(ScheduleMap, isl_dim_out, CurrDim + 1,
-                                        MaxDim - CurrDim - 1);
-      ScheduleMap = isl_map_set_tuple_id(ScheduleMap, isl_dim_in,
-                                         SS.getDomainId().release());
-      Schedule =
-          isl_union_map_union(Schedule, isl_union_map_from_map(ScheduleMap));
+    isl_map *ScheduleMap = SS.getSchedule().release();
+    assert(ScheduleMap &&
+           "Schedules that contain extension nodes require special handling.");
+
+    unsigned First = static_cast<unsigned>(CurrDim + 1);
+    unsigned N = (MaxDim > First) ? (MaxDim - First) : 0;
+    if (N > 0) {
+      ScheduleMap = isl_map_project_out(ScheduleMap, isl_dim_out, First, N);
     }
+
+    ScheduleMap = isl_map_set_tuple_id(ScheduleMap, isl_dim_in,
+                                       SS.getDomainId().release());
+    Schedule =
+        isl_union_map_union(Schedule, isl_union_map_from_map(ScheduleMap));
   }
+
   Schedule = isl_union_map_coalesce(Schedule);
   return Schedule;
 }
@@ -168,7 +159,6 @@ INITIALIZE_PASS_END(PolyhedralInfo, "pol
 //===----------------------------------------------------------------------===//
 
 namespace {
-/// Print result from PolyhedralInfo.
 class PolyhedralInfoPrinterLegacyPass final : public FunctionPass {
 public:
   static char ID;


--- a/polly/lib/Analysis/DependenceInfo.cpp	2025-09-21 14:59:12.228534011 +0200
+++ b/polly/lib/Analysis/DependenceInfo.cpp	2025-09-21 15:16:14.364885868 +0200

--- a/polly/lib/Transform/DeadCodeElimination.cpp	2025-09-21 17:32:16.072456710 +0200
+++ b/polly/lib/Transform/DeadCodeElimination.cpp	2025-09-21 17:32:47.327399217 +0200

--- a/polly/lib/Transform/DeLICM.cpp	2025-09-21 16:25:17.318367274 +0200
+++ b/polly/lib/Transform/DeLICM.cpp	2025-09-21 16:26:08.074761600 +0200

--- a/polly/lib/Transform/ForwardOpTree.cpp	2025-09-21 16:10:05.509588643 +0200
+++ b/polly/lib/Transform/ForwardOpTree.cpp	2025-09-21 16:14:13.166720281 +0200
@@ -1,11 +1,8 @@
 //===- ForwardOpTree.h ------------------------------------------*- C++ -*-===//
 //
 // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
-// See https://llvm.org/LICENSE.txt for license information.
 // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 //
-//===----------------------------------------------------------------------===//
-//
 // Move instructions between statements.
 //
 //===----------------------------------------------------------------------===//
@@ -20,6 +17,9 @@
 #include "polly/Support/ISLTools.h"
 #include "polly/Support/VirtualInstruction.h"
 #include "polly/ZoneAlgo.h"
+#include "llvm/ADT/ArrayRef.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/DenseSet.h"
 #include "llvm/ADT/STLExtras.h"
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/ADT/Statistic.h"
@@ -38,6 +38,7 @@
 #include "isl/ctx.h"
 #include "isl/isl-noexceptions.h"
 #include <cassert>
+#include <functional>
 #include <memory>
 
 #include "polly/Support/PollyDebug.h"
@@ -89,84 +90,37 @@ STATISTIC(NumSingletonWritesInLoops,
 
 namespace {
 
-/// The state of whether an operand tree was/can be forwarded.
-///
-/// The items apply to an instructions and its operand tree with the instruction
-/// as the root element. If the value in question is not an instruction in the
-/// SCoP, it can be a leaf of an instruction's operand tree.
 enum ForwardingDecision {
-  /// An uninitialized value.
   FD_Unknown,
-
-  /// The root instruction or value cannot be forwarded at all.
   FD_CannotForward,
-
-  /// The root instruction or value can be forwarded as a leaf of a larger
-  /// operand tree.
-  /// It does not make sense to move the value itself, it would just replace it
-  /// by a use of itself. For instance, a constant "5" used in a statement can
-  /// be forwarded, but it would just replace it by the same constant "5".
-  /// However, it makes sense to move as an operand of
-  ///
-  ///   %add = add 5, 5
-  ///
-  /// where "5" is moved as part of a larger operand tree. "5" would be placed
-  /// (disregarding for a moment that literal constants don't have a location
-  /// and can be used anywhere) into the same statement as %add would.
   FD_CanForwardLeaf,
-
-  /// The root instruction can be forwarded and doing so avoids a scalar
-  /// dependency.
-  ///
-  /// This can be either because the operand tree can be moved to the target
-  /// statement, or a memory access is redirected to read from a different
-  /// location.
   FD_CanForwardProfitably,
-
-  /// A forwarding method cannot be applied to the operand tree.
-  /// The difference to FD_CannotForward is that there might be other methods
-  /// that can handle it.
   FD_NotApplicable
 };
 
-/// Represents the evaluation of and action to taken when forwarding a value
-/// from an operand tree.
 struct ForwardingAction {
   using KeyTy = std::pair<Value *, ScopStmt *>;
 
-  /// Evaluation of forwarding a value.
   ForwardingDecision Decision = FD_Unknown;
 
-  /// Callback to execute the forwarding.
-  /// Returning true allows deleting the polly::MemoryAccess if the value is the
-  /// root of the operand tree (and its elimination the reason why the
-  /// forwarding is done). Return false if the MemoryAccess is reused or there
-  /// might be other users of the read accesses. In the letter case the
-  /// polly::SimplifyPass can remove dead MemoryAccesses.
   std::function<bool()> Execute = []() -> bool {
     llvm_unreachable("unspecified how to forward");
   };
 
-  /// Other values that need to be forwarded if this action is executed. Their
-  /// actions are executed after this one.
   SmallVector<KeyTy, 4> Depends;
 
-  /// Named ctor: The method creating this object does not apply to the kind of
-  /// value, but other methods may.
   static ForwardingAction notApplicable() {
     ForwardingAction Result;
     Result.Decision = FD_NotApplicable;
     return Result;
   }
 
-  /// Named ctor: The value cannot be forwarded.
   static ForwardingAction cannotForward() {
     ForwardingAction Result;
     Result.Decision = FD_CannotForward;
     return Result;
   }
 
-  /// Named ctor: The value can just be used without any preparation.
   static ForwardingAction triviallyForwardable(bool IsProfitable, Value *Val) {
     ForwardingAction Result;
     Result.Decision =
@@ -178,7 +132,6 @@ struct ForwardingAction {
     return Result;
   }
 
-  /// Name ctor: The value can be forwarded by executing an action.
   static ForwardingAction canForward(std::function<bool()> Execute,
                                      ArrayRef<KeyTy> Depends,
                                      bool IsProfitable) {
@@ -191,147 +144,62 @@ struct ForwardingAction {
   }
 };
 
-/// Implementation of operand tree forwarding for a specific SCoP.
-///
-/// For a statement that requires a scalar value (through a value read
-/// MemoryAccess), see if its operand can be moved into the statement. If so,
-/// the MemoryAccess is removed and the all the operand tree instructions are
-/// moved into the statement. All original instructions are left in the source
-/// statements. The simplification pass can clean these up.
 class ForwardOpTreeImpl final : ZoneAlgorithm {
 private:
   using MemoizationTy = DenseMap<ForwardingAction::KeyTy, ForwardingAction>;
 
-  /// Scope guard to limit the number of isl operations for this pass.
   IslMaxOperationsGuard &MaxOpGuard;
 
-  /// How many instructions have been copied to other statements.
   int NumInstructionsCopied = 0;
-
-  /// Number of loads forwarded because their value was known.
   int NumKnownLoadsForwarded = 0;
-
-  /// Number of values reloaded from known array elements.
   int NumReloads = 0;
-
-  /// How many read-only accesses have been copied.
   int NumReadOnlyCopied = 0;
-
-  /// How many operand trees have been forwarded.
   int NumForwardedTrees = 0;
-
-  /// Number of statements with at least one forwarded operand tree.
   int NumModifiedStmts = 0;
-
-  /// Whether we carried out at least one change to the SCoP.
   bool Modified = false;
 
-  /// Cache of how to forward values.
-  /// The key of this map is the llvm::Value to be forwarded and the
-  /// polly::ScopStmt it is forwarded from. This is because the same llvm::Value
-  /// can evaluate differently depending on where it is evaluate. For instance,
-  /// a synthesizable Scev represents a recurrence with an loop but the loop's
-  /// exit value if evaluated after the loop.
-  /// The cached results are only valid for the current TargetStmt.
-  /// CHECKME: ScalarEvolution::getScevAtScope should take care for getting the
-  /// exit value when instantiated outside of the loop. The primary concern is
-  /// ambiguity when crossing PHI nodes, which currently is not supported.
   MemoizationTy ForwardingActions;
 
-  /// Contains the zones where array elements are known to contain a specific
-  /// value.
-  /// { [Element[] -> Zone[]] -> ValInst[] }
-  /// @see computeKnown()
   isl::union_map Known;
-
-  /// Translator for newly introduced ValInsts to already existing ValInsts such
-  /// that new introduced load instructions can reuse the Known analysis of its
-  /// original load. { ValInst[] -> ValInst[] }
   isl::union_map Translator;
 
-  /// Get list of array elements that do contain the same ValInst[] at Domain[].
-  ///
-  /// @param ValInst { Domain[] -> ValInst[] }
-  ///                The values for which we search for alternative locations,
-  ///                per statement instance.
-  ///
-  /// @return { Domain[] -> Element[] }
-  ///         For each statement instance, the array elements that contain the
-  ///         same ValInst.
   isl::union_map findSameContentElements(isl::union_map ValInst) {
     assert(!ValInst.is_single_valued().is_false());
 
-    // { Domain[] }
     isl::union_set Domain = ValInst.domain();
-
-    // { Domain[] -> Scatter[] }
     isl::union_map Schedule = getScatterFor(Domain);
 
-    // { Element[] -> [Scatter[] -> ValInst[]] }
     isl::union_map MustKnownCurried =
         convertZoneToTimepoints(Known, isl::dim::in, false, true).curry();
 
-    // { [Domain[] -> ValInst[]] -> Scatter[] }
     isl::union_map DomValSched = ValInst.domain_map().apply_range(Schedule);
 
-    // { [Scatter[] -> ValInst[]] -> [Domain[] -> ValInst[]] }
     isl::union_map SchedValDomVal =
         DomValSched.range_product(ValInst.range_map()).reverse();
 
-    // { Element[] -> [Domain[] -> ValInst[]] }
     isl::union_map MustKnownInst = MustKnownCurried.apply_range(SchedValDomVal);
 
-    // { Domain[] -> Element[] }
     isl::union_map MustKnownMap =
         MustKnownInst.uncurry().domain().unwrap().reverse();
     simplify(MustKnownMap);
-
     return MustKnownMap;
   }
 
-  /// Find a single array element for each statement instance, within a single
-  /// array.
-  ///
-  /// @param MustKnown { Domain[] -> Element[] }
-  ///                  Set of candidate array elements.
-  /// @param Domain    { Domain[] }
-  ///                  The statement instance for which we need elements for.
-  ///
-  /// @return { Domain[] -> Element[] }
-  ///         For each statement instance, an array element out of @p MustKnown.
-  ///         All array elements must be in the same array (Polly does not yet
-  ///         support reading from different accesses using the same
-  ///         MemoryAccess). If no mapping for all of @p Domain exists, returns
-  ///         null.
   isl::map singleLocation(isl::union_map MustKnown, isl::set Domain) {
-    // { Domain[] -> Element[] }
     isl::map Result;
-
-    // Make irrelevant elements not interfere.
     Domain = Domain.intersect_params(S->getContext());
 
-    // MemoryAccesses can read only elements from a single array
-    // (i.e. not: { Dom[0] -> A[0]; Dom[1] -> B[1] }).
-    // Look through all spaces until we find one that contains at least the
-    // wanted statement instance.s
     for (isl::map Map : MustKnown.get_map_list()) {
-      // Get the array this is accessing.
       isl::id ArrayId = Map.get_tuple_id(isl::dim::out);
       ScopArrayInfo *SAI = static_cast<ScopArrayInfo *>(ArrayId.get_user());
 
-      // No support for generation of indirect array accesses.
       if (SAI->getBasePtrOriginSAI())
         continue;
 
-      // Determine whether this map contains all wanted values.
       isl::set MapDom = Map.domain();
       if (!Domain.is_subset(MapDom).is_true())
         continue;
 
-      // There might be multiple array elements that contain the same value, but
-      // choose only one of them. lexmin is used because it returns a one-value
-      // mapping, we do not care about which one.
-      // TODO: Get the simplest access function.
       Result = Map.lexmin();
       break;
     }
@@ -343,13 +211,7 @@ public:
   ForwardOpTreeImpl(Scop *S, LoopInfo *LI, IslMaxOperationsGuard &MaxOpGuard)
       : ZoneAlgorithm("polly-optree", S, LI), MaxOpGuard(MaxOpGuard) {}
 
-  /// Compute the zones of known array element contents.
-  ///
-  /// @return True if the computed #Known is usable.
   bool computeKnownValues() {
-    isl::union_map MustKnown, KnownFromLoad, KnownFromInit;
-
-    // Check that nothing strange occurs.
     collectCompatibleElts();
 
     {
@@ -360,7 +222,6 @@ public:
         computeNormalizedPHIs();
       Known = computeKnown(true, true);
 
-      // Preexisting ValInsts use the known content analysis of themselves.
       Translator = makeIdentityMap(Known.range(), false);
     }
 
@@ -375,7 +236,6 @@ public:
 
     KnownAnalyzed++;
     POLLY_DEBUG(dbgs() << "All known: " << Known << "\n");
-
     return true;
   }
 
@@ -408,20 +268,11 @@ public:
     OS.indent(Indent) << "}\n";
   }
 
-  /// Create a new MemoryAccess of type read and MemoryKind::Array.
-  ///
-  /// @param Stmt           The statement in which the access occurs.
-  /// @param LI             The instruction that does the access.
-  /// @param AccessRelation The array element that each statement instance
-  ///                       accesses.
-  ///
-  /// @param The newly created access.
   MemoryAccess *makeReadArrayAccess(ScopStmt *Stmt, LoadInst *LI,
                                     isl::map AccessRelation) {
     isl::id ArrayId = AccessRelation.get_tuple_id(isl::dim::out);
     ScopArrayInfo *SAI = reinterpret_cast<ScopArrayInfo *>(ArrayId.get_user());
 
-    // Create a dummy SCEV access, to be replaced anyway.
     SmallVector<const SCEV *, 4> Sizes;
     Sizes.reserve(SAI->getNumberOfDimensions());
     SmallVector<const SCEV *, 4> Subscripts;
@@ -438,29 +289,13 @@ public:
     Stmt->addAccess(Access, true);
 
     Access->setNewAccessRelation(AccessRelation);
-
     return Access;
   }
 
-  /// Forward a load by reading from an array element that contains the same
-  /// value. Typically the location it was loaded from.
-  ///
-  /// @param TargetStmt  The statement the operand tree will be copied to.
-  /// @param Inst        The (possibly speculatable) instruction to forward.
-  /// @param UseStmt     The statement that uses @p Inst.
-  /// @param UseLoop     The loop @p Inst is used in.
-  /// @param DefStmt     The statement @p Inst is defined in.
-  /// @param DefLoop     The loop which contains @p Inst.
-  ///
-  /// @return A ForwardingAction object describing the feasibility and
-  ///         profitability evaluation and the callback carrying-out the value
-  ///         forwarding.
   ForwardingAction forwardKnownLoad(ScopStmt *TargetStmt, Instruction *Inst,
                                     ScopStmt *UseStmt, Loop *UseLoop,
                                     ScopStmt *DefStmt, Loop *DefLoop) {
-    // Cannot do anything without successful known analysis.
-    if (Known.is_null() || Translator.is_null() ||
-        MaxOpGuard.hasQuotaExceeded())
+    if (Known.is_null() || Translator.is_null() || MaxOpGuard.hasQuotaExceeded())
       return ForwardingAction::notApplicable();
 
     LoadInst *LI = dyn_cast<LoadInst>(Inst);
@@ -481,21 +316,10 @@ public:
 
     MemoryAccess *Access = TargetStmt->getArrayAccessOrNULLFor(LI);
     if (Access) {
-      // If the load is already in the statement, no forwarding is necessary.
-      // However, it might happen that the LoadInst is already present in the
-      // statement's instruction list. In that case we do as follows:
-      // - For the evaluation, we can trivially forward it as it is
-      //   benefit of forwarding an already present instruction.
-      // - For the execution, prepend the instruction (to make it
-      //   available to all instructions following in the instruction list), but
-      //   do not add another MemoryAccess.
       auto ExecAction = [this, TargetStmt, LI, Access]() -> bool {
         TargetStmt->prependInstruction(LI);
-        POLLY_DEBUG(
-            dbgs() << "    forwarded known load with preexisting MemoryAccess"
-                   << Access << "\n");
+        POLLY_DEBUG(dbgs() << "    forwarded known load with preexisting MemoryAccess" << Access << "\n");
         (void)Access;
-
         NumKnownLoadsForwarded++;
         TotalKnownLoadsForwarded++;
         return true;
@@ -504,84 +328,48 @@ public:
           ExecAction, {{LI->getPointerOperand(), DefStmt}}, true);
     }
 
-    // Allow the following Isl calculations (until we return the
-    // ForwardingAction, excluding the code inside the lambda that will be
-    // executed later) to fail.
     IslQuotaScope QuotaScope = MaxOpGuard.enter();
 
-    // { DomainDef[] -> ValInst[] }
     isl::map ExpectedVal = makeValInst(Inst, UseStmt, UseLoop);
-    assert(!isNormalized(ExpectedVal).is_false() &&
-           "LoadInsts are always normalized");
+    assert(!isNormalized(ExpectedVal).is_false() && "LoadInsts are always normalized");
 
-    // { DomainUse[] -> DomainTarget[] }
     isl::map UseToTarget = getDefToTarget(UseStmt, TargetStmt);
-
-    // { DomainTarget[] -> ValInst[] }
     isl::map TargetExpectedVal = ExpectedVal.apply_domain(UseToTarget);
     isl::union_map TranslatedExpectedVal =
         isl::union_map(TargetExpectedVal).apply_range(Translator);
 
-    // { DomainTarget[] -> Element[] }
     isl::union_map Candidates = findSameContentElements(TranslatedExpectedVal);
-
     isl::map SameVal = singleLocation(Candidates, getDomainFor(TargetStmt));
     if (SameVal.is_null())
       return ForwardingAction::notApplicable();
 
-    POLLY_DEBUG(dbgs() << "      expected values where " << TargetExpectedVal
-                       << "\n");
-    POLLY_DEBUG(dbgs() << "      candidate elements where " << Candidates
-                       << "\n");
+    POLLY_DEBUG(dbgs() << "      expected values where " << TargetExpectedVal << "\n");
+    POLLY_DEBUG(dbgs() << "      candidate elements where " << Candidates << "\n");
 
-    // { ValInst[] }
     isl::space ValInstSpace = ExpectedVal.get_space().range();
 
-    // After adding a new load to the SCoP, also update the Known content
-    // about it. The new load will have a known ValInst of
-    // { [DomainTarget[] -> Value[]] }
-    // but which -- because it is a copy of it -- has same value as the
-    // { [DomainDef[] -> Value[]] }
-    // that it replicates. Instead of  cloning the known content of
-    // [DomainDef[] -> Value[]]
-    // for DomainTarget[], we add a 'translator' that maps
-    // [DomainTarget[] -> Value[]] to [DomainDef[] -> Value[]]
-    // before comparing to the known content.
-    // TODO: 'Translator' could also be used to map PHINodes to their incoming
-    // ValInsts.
     isl::map LocalTranslator;
     if (!ValInstSpace.is_wrapping().is_false()) {
-      // { DefDomain[] -> Value[] }
       isl::map ValInsts = ExpectedVal.range().unwrap();
-
-      // { DefDomain[] }
       isl::set DefDomain = ValInsts.domain();
+      (void)DefDomain;
 
-      // { Value[] }
       isl::space ValSpace = ValInstSpace.unwrap().range();
-
-      // { Value[] -> Value[] }
       isl::map ValToVal =
           isl::map::identity(ValSpace.map_from_domain_and_range(ValSpace));
 
-      // { DomainDef[] -> DomainTarget[] }
       isl::map DefToTarget = getDefToTarget(DefStmt, TargetStmt);
-
-      // { [TargetDomain[] -> Value[]] -> [DefDomain[] -> Value] }
       LocalTranslator = DefToTarget.reverse().product(ValToVal);
-      POLLY_DEBUG(dbgs() << "      local translator is " << LocalTranslator
-                         << "\n");
+      POLLY_DEBUG(dbgs() << "      local translator is " << LocalTranslator << "\n");
 
       if (LocalTranslator.is_null())
         return ForwardingAction::notApplicable();
     }
 
-    auto ExecAction = [this, TargetStmt, LI, SameVal,
-                       LocalTranslator]() -> bool {
+    auto ExecAction = [this, TargetStmt, LI, SameVal, LocalTranslator]() -> bool {
       TargetStmt->prependInstruction(LI);
       MemoryAccess *Access = makeReadArrayAccess(TargetStmt, LI, SameVal);
-      POLLY_DEBUG(dbgs() << "    forwarded known load with new MemoryAccess"
-                         << Access << "\n");
+      POLLY_DEBUG(dbgs() << "    forwarded known load with new MemoryAccess" << Access << "\n");
       (void)Access;
 
       if (!LocalTranslator.is_null())
@@ -595,42 +383,21 @@ public:
         ExecAction, {{LI->getPointerOperand(), DefStmt}}, true);
   }
 
-  /// Forward a scalar by redirecting the access to an array element that stores
-  /// the same value.
-  ///
-  /// @param TargetStmt  The statement the operand tree will be copied to.
-  /// @param Inst        The scalar to forward.
-  /// @param UseStmt     The statement that uses @p Inst.
-  /// @param UseLoop     The loop @p Inst is used in.
-  /// @param DefStmt     The statement @p Inst is defined in.
-  /// @param DefLoop     The loop which contains @p Inst.
-  ///
-  /// @return A ForwardingAction object describing the feasibility and
-  ///         profitability evaluation and the callback carrying-out the value
-  ///         forwarding.
   ForwardingAction reloadKnownContent(ScopStmt *TargetStmt, Instruction *Inst,
                                       ScopStmt *UseStmt, Loop *UseLoop,
                                       ScopStmt *DefStmt, Loop *DefLoop) {
-    // Cannot do anything without successful known analysis.
-    if (Known.is_null() || Translator.is_null() ||
-        MaxOpGuard.hasQuotaExceeded())
+    if (Known.is_null() || Translator.is_null() || MaxOpGuard.hasQuotaExceeded())
       return ForwardingAction::notApplicable();
 
-    // Don't spend too much time analyzing whether it can be reloaded.
     IslQuotaScope QuotaScope = MaxOpGuard.enter();
 
-    // { DomainDef[] -> ValInst[] }
     isl::union_map ExpectedVal = makeNormalizedValInst(Inst, UseStmt, UseLoop);
-
-    // { DomainUse[] -> DomainTarget[] }
     isl::map UseToTarget = getDefToTarget(UseStmt, TargetStmt);
 
-    // { DomainTarget[] -> ValInst[] }
     isl::union_map TargetExpectedVal = ExpectedVal.apply_domain(UseToTarget);
     isl::union_map TranslatedExpectedVal =
         TargetExpectedVal.apply_range(Translator);
 
-    // { DomainTarget[] -> Element[] }
     isl::union_map Candidates = findSameContentElements(TranslatedExpectedVal);
 
     isl::map SameVal = singleLocation(Candidates, getDomainFor(TargetStmt));
@@ -644,8 +411,7 @@ public:
         Access = TargetStmt->ensureValueRead(Inst);
       Access->setNewAccessRelation(SameVal);
 
-      POLLY_DEBUG(dbgs() << "    forwarded known content of " << *Inst
-                         << " which is " << SameVal << "\n");
+      POLLY_DEBUG(dbgs() << "    forwarded known content of " << *Inst << " which is " << SameVal << "\n");
       TotalReloads++;
       NumReloads++;
       return false;
@@ -654,67 +420,34 @@ public:
     return ForwardingAction::canForward(ExecAction, {}, true);
   }
 
-  /// Forwards a speculatively executable instruction.
-  ///
-  /// @param TargetStmt  The statement the operand tree will be copied to.
-  /// @param UseInst     The (possibly speculatable) instruction to forward.
-  /// @param DefStmt     The statement @p UseInst is defined in.
-  /// @param DefLoop     The loop which contains @p UseInst.
-  ///
-  /// @return A ForwardingAction object describing the feasibility and
-  ///         profitability evaluation and the callback carrying-out the value
-  ///         forwarding.
-  ForwardingAction forwardSpeculatable(ScopStmt *TargetStmt,
-                                       Instruction *UseInst, ScopStmt *DefStmt,
+  ForwardingAction forwardSpeculatable(ScopStmt *TargetStmt, Instruction *UseInst, ScopStmt *DefStmt,
                                        Loop *DefLoop) {
-    // PHIs, unless synthesizable, are not yet supported.
     if (isa<PHINode>(UseInst))
       return ForwardingAction::notApplicable();
 
-    // Compatible instructions must satisfy the following conditions:
-    // 1. Idempotent (instruction will be copied, not moved; although its
-    //    original instance might be removed by simplification)
-    // 2. Not access memory (There might be memory writes between)
-    // 3. Not cause undefined behaviour (we might copy to a location when the
-    //    original instruction was no executed; this is currently not possible
-    //    because we do not forward PHINodes)
-    // 4. Not leak memory if executed multiple times (i.e. malloc)
-    //
-    // Instruction::mayHaveSideEffects is not sufficient because it considers
-    // malloc to not have side-effects. llvm::isSafeToSpeculativelyExecute is
-    // not sufficient because it allows memory accesses.
     if (mayHaveNonDefUseDependency(*UseInst))
       return ForwardingAction::notApplicable();
 
     SmallVector<ForwardingAction::KeyTy, 4> Depends;
     Depends.reserve(UseInst->getNumOperands());
     for (Value *OpVal : UseInst->operand_values()) {
-      ForwardingDecision OpDecision =
-          forwardTree(TargetStmt, OpVal, DefStmt, DefLoop);
+      ForwardingDecision OpDecision = forwardTree(TargetStmt, OpVal, DefStmt, DefLoop);
       switch (OpDecision) {
       case FD_CannotForward:
         return ForwardingAction::cannotForward();
-
       case FD_CanForwardLeaf:
       case FD_CanForwardProfitably:
         Depends.emplace_back(OpVal, DefStmt);
         break;
-
       case FD_NotApplicable:
       case FD_Unknown:
-        llvm_unreachable(
-            "forwardTree should never return FD_NotApplicable/FD_Unknown");
+        llvm_unreachable("forwardTree should never return FD_NotApplicable/FD_Unknown");
       }
     }
 
     auto ExecAction = [this, TargetStmt, UseInst]() {
-      // To ensure the right order, prepend this instruction before its
-      // operands. This ensures that its operands are inserted before the
-      // instruction using them.
       TargetStmt->prependInstruction(UseInst);
-
-      POLLY_DEBUG(dbgs() << "    forwarded speculable instruction: " << *UseInst
-                         << "\n");
+      POLLY_DEBUG(dbgs() << "    forwarded speculable instruction: " << *UseInst << "\n");
       NumInstructionsCopied++;
       TotalInstructionsCopied++;
       return true;
@@ -722,24 +455,10 @@ public:
     return ForwardingAction::canForward(ExecAction, Depends, true);
   }
 
-  /// Determines whether an operand tree can be forwarded and returns
-  /// instructions how to do so in the form of a ForwardingAction object.
-  ///
-  /// @param TargetStmt  The statement the operand tree will be copied to.
-  /// @param UseVal      The value (usually an instruction) which is root of an
-  ///                    operand tree.
-  /// @param UseStmt     The statement that uses @p UseVal.
-  /// @param UseLoop     The loop @p UseVal is used in.
-  ///
-  /// @return A ForwardingAction object describing the feasibility and
-  ///         profitability evaluation and the callback carrying-out the value
-  ///         forwarding.
-  ForwardingAction forwardTreeImpl(ScopStmt *TargetStmt, Value *UseVal,
-                                   ScopStmt *UseStmt, Loop *UseLoop) {
+  ForwardingAction forwardTreeImpl(ScopStmt *TargetStmt, Value *UseVal, ScopStmt *UseStmt, Loop *UseLoop) {
     ScopStmt *DefStmt = nullptr;
     Loop *DefLoop = nullptr;
 
-    // { DefDomain[] -> TargetDomain[] }
     isl::map DefToTarget;
 
     VirtualUse VUse = VirtualUse::create(UseStmt, UseLoop, UseVal, true);
@@ -747,28 +466,15 @@ public:
     case VirtualUse::Constant:
     case VirtualUse::Block:
     case VirtualUse::Hoisted:
-      // These can be used anywhere without special considerations.
       return ForwardingAction::triviallyForwardable(false, UseVal);
 
     case VirtualUse::Synthesizable: {
-      // Check if the value is synthesizable at the new location as well. This
-      // might be possible when leaving a loop for which ScalarEvolution is
-      // unable to derive the exit value for.
-      // TODO: If there is a LCSSA PHI at the loop exit, use that one.
-      // If the SCEV contains a SCEVAddRecExpr, we currently depend on that we
-      // do not forward past its loop header. This would require us to use a
-      // previous loop induction variable instead the current one. We currently
-      // do not allow forwarding PHI nodes, thus this should never occur (the
-      // only exception where no phi is necessary being an unreachable loop
-      // without edge from the outside).
-      VirtualUse TargetUse = VirtualUse::create(
-          S, TargetStmt, TargetStmt->getSurroundingLoop(), UseVal, true);
+      VirtualUse TargetUse =
+          VirtualUse::create(S, TargetStmt, TargetStmt->getSurroundingLoop(), UseVal, true);
       if (TargetUse.getKind() == VirtualUse::Synthesizable)
         return ForwardingAction::triviallyForwardable(false, UseVal);
 
-      POLLY_DEBUG(
-          dbgs() << "    Synthesizable would not be synthesizable anymore: "
-                 << *UseVal << "\n");
+      POLLY_DEBUG(dbgs() << "    Synthesizable would not be synthesizable anymore: " << *UseVal << "\n");
       return ForwardingAction::cannotForward();
     }
 
@@ -776,34 +482,21 @@ public:
       if (!ModelReadOnlyScalars)
         return ForwardingAction::triviallyForwardable(false, UseVal);
 
-      // If we model read-only scalars, we need to create a MemoryAccess for it.
       auto ExecAction = [this, TargetStmt, UseVal]() {
         TargetStmt->ensureValueRead(UseVal);
-
-        POLLY_DEBUG(dbgs() << "    forwarded read-only value " << *UseVal
-                           << "\n");
+        POLLY_DEBUG(dbgs() << "    forwarded read-only value " << *UseVal << "\n");
         NumReadOnlyCopied++;
         TotalReadOnlyCopied++;
-
-        // Note that we cannot return true here. With a operand tree
-        // depth of 0, UseVal is the use in TargetStmt that we try to replace.
-        // With -polly-analyze-read-only-scalars=true we would ensure the
-        // existence of a MemoryAccess (which already exists for a leaf) and be
-        // removed again by tryForwardTree because it's goal is to remove this
-        // scalar MemoryAccess. It interprets FD_CanForwardTree as the
-        // permission to do so.
         return false;
       };
       return ForwardingAction::canForward(ExecAction, {}, false);
     }
 
     case VirtualUse::Intra:
-      // Knowing that UseStmt and DefStmt are the same statement instance, just
-      // reuse the information about UseStmt for DefStmt
       DefStmt = UseStmt;
-
       [[fallthrough]];
-    case VirtualUse::Inter:
+
+    case VirtualUse::Inter: {
       Instruction *Inst = cast<Instruction>(UseVal);
 
       if (!DefStmt) {
@@ -814,155 +507,103 @@ public:
 
       DefLoop = LI->getLoopFor(Inst->getParent());
 
-      ForwardingAction SpeculativeResult =
-          forwardSpeculatable(TargetStmt, Inst, DefStmt, DefLoop);
+      ForwardingAction SpeculativeResult = forwardSpeculatable(TargetStmt, Inst, DefStmt, DefLoop);
       if (SpeculativeResult.Decision != FD_NotApplicable)
         return SpeculativeResult;
 
-      ForwardingAction KnownResult = forwardKnownLoad(
-          TargetStmt, Inst, UseStmt, UseLoop, DefStmt, DefLoop);
+      ForwardingAction KnownResult =
+          forwardKnownLoad(TargetStmt, Inst, UseStmt, UseLoop, DefStmt, DefLoop);
       if (KnownResult.Decision != FD_NotApplicable)
         return KnownResult;
 
-      ForwardingAction ReloadResult = reloadKnownContent(
-          TargetStmt, Inst, UseStmt, UseLoop, DefStmt, DefLoop);
+      ForwardingAction ReloadResult =
+          reloadKnownContent(TargetStmt, Inst, UseStmt, UseLoop, DefStmt, DefLoop);
       if (ReloadResult.Decision != FD_NotApplicable)
         return ReloadResult;
 
-      // When no method is found to forward the operand tree, we effectively
-      // cannot handle it.
-      POLLY_DEBUG(dbgs() << "    Cannot forward instruction: " << *Inst
-                         << "\n");
+      POLLY_DEBUG(dbgs() << "    Cannot forward instruction: " << *Inst << "\n");
       return ForwardingAction::cannotForward();
     }
+    }
 
     llvm_unreachable("Case unhandled");
   }
 
-  /// Determines whether an operand tree can be forwarded. Previous evaluations
-  /// are cached.
-  ///
-  /// @param TargetStmt  The statement the operand tree will be copied to.
-  /// @param UseVal      The value (usually an instruction) which is root of an
-  ///                    operand tree.
-  /// @param UseStmt     The statement that uses @p UseVal.
-  /// @param UseLoop     The loop @p UseVal is used in.
-  ///
-  /// @return FD_CannotForward        if @p UseVal cannot be forwarded.
-  ///         FD_CanForwardLeaf       if @p UseVal is forwardable, but not
-  ///                                 profitable.
-  ///         FD_CanForwardProfitably if @p UseVal is forwardable and useful to
-  ///                                 do.
-  ForwardingDecision forwardTree(ScopStmt *TargetStmt, Value *UseVal,
-                                 ScopStmt *UseStmt, Loop *UseLoop) {
-    // Lookup any cached evaluation.
+  ForwardingDecision forwardTree(ScopStmt *TargetStmt, Value *UseVal, ScopStmt *UseStmt, Loop *UseLoop) {
     auto It = ForwardingActions.find({UseVal, UseStmt});
     if (It != ForwardingActions.end())
       return It->second.Decision;
 
-    // Make a new evaluation.
-    ForwardingAction Action =
-        forwardTreeImpl(TargetStmt, UseVal, UseStmt, UseLoop);
+    ForwardingAction Action = forwardTreeImpl(TargetStmt, UseVal, UseStmt, UseLoop);
     ForwardingDecision Result = Action.Decision;
 
-    // Remember for the next time.
-    assert(!ForwardingActions.count({UseVal, UseStmt}) &&
-           "circular dependency?");
+    assert(!ForwardingActions.count({UseVal, UseStmt}) && "circular dependency?");
     ForwardingActions.insert({{UseVal, UseStmt}, std::move(Action)});
-
     return Result;
   }
 
-  /// Forward an operand tree using cached actions.
-  ///
-  /// @param Stmt   Statement the operand tree is moved into.
-  /// @param UseVal Root of the operand tree within @p Stmt.
-  /// @param RA     The MemoryAccess for @p UseVal that the forwarding intends
-  ///               to remove.
   void applyForwardingActions(ScopStmt *Stmt, Value *UseVal, MemoryAccess *RA) {
-    using ChildItTy =
-        decltype(std::declval<ForwardingAction>().Depends.begin());
+    using ChildItTy = decltype(std::declval<ForwardingAction>().Depends.begin());
     using EdgeTy = std::pair<ForwardingAction *, ChildItTy>;
 
     DenseSet<ForwardingAction::KeyTy> Visited;
     SmallVector<EdgeTy, 32> Stack;
     SmallVector<ForwardingAction *, 32> Ordered;
 
-    // Seed the tree search using the root value.
     assert(ForwardingActions.count({UseVal, Stmt}));
     ForwardingAction *RootAction = &ForwardingActions[{UseVal, Stmt}];
     Stack.emplace_back(RootAction, RootAction->Depends.begin());
 
-    // Compute the postorder of the operand tree: all operands of an instruction
-    // must be visited before the instruction itself. As an additional
-    // requirement, the topological ordering must be 'compact': Any subtree node
-    // must not be interleaved with nodes from a non-shared subtree. This is
-    // because the same llvm::Instruction can be materialized multiple times as
-    // used at different ScopStmts which might be different values. Intersecting
-    // these lifetimes may result in miscompilations.
-    // FIXME: Intersecting lifetimes might still be possible for the roots
-    // themselves, since instructions are just prepended to a ScopStmt's
-    // instruction list.
     while (!Stack.empty()) {
       EdgeTy &Top = Stack.back();
       ForwardingAction *TopAction = Top.first;
       ChildItTy &TopEdge = Top.second;
 
       if (TopEdge == TopAction->Depends.end()) {
-        // Postorder sorting
         Ordered.push_back(TopAction);
         Stack.pop_back();
         continue;
       }
-      ForwardingAction::KeyTy Key = *TopEdge;
 
-      // Next edge for this level
+      ForwardingAction::KeyTy Key = *TopEdge;
       ++TopEdge;
 
       auto VisitIt = Visited.insert(Key);
       if (!VisitIt.second)
         continue;
 
-      assert(ForwardingActions.count(Key) &&
-             "Must not insert new actions during execution phase");
+      assert(ForwardingActions.count(Key) && "Must not insert new actions during execution phase");
       ForwardingAction *ChildAction = &ForwardingActions[Key];
       Stack.emplace_back(ChildAction, ChildAction->Depends.begin());
     }
 
-    // Actually, we need the reverse postorder because actions prepend new
-    // instructions. Therefore, the first one will always be the action for the
-    // operand tree's root.
     assert(Ordered.back() == RootAction);
     if (RootAction->Execute())
       Stmt->removeSingleMemoryAccess(RA);
     Ordered.pop_back();
     for (auto DepAction : reverse(Ordered)) {
-      assert(DepAction->Decision != FD_Unknown &&
-             DepAction->Decision != FD_CannotForward);
+      assert(DepAction->Decision != FD_Unknown && DepAction->Decision != FD_CannotForward);
       assert(DepAction != RootAction);
       DepAction->Execute();
     }
   }
 
-  /// Try to forward an operand tree rooted in @p RA.
   bool tryForwardTree(MemoryAccess *RA) {
     assert(RA->isLatestScalarKind());
     POLLY_DEBUG(dbgs() << "Trying to forward operand tree " << RA << "...\n");
 
     ScopStmt *Stmt = RA->getStatement();
     Loop *InLoop = Stmt->getSurroundingLoop();
+    (void)InLoop;
 
     isl::map TargetToUse;
     if (!Known.is_null()) {
       isl::space DomSpace = Stmt->getDomainSpace();
-      TargetToUse =
-          isl::map::identity(DomSpace.map_from_domain_and_range(DomSpace));
+      TargetToUse = isl::map::identity(DomSpace.map_from_domain_and_range(DomSpace));
     }
 
-    ForwardingDecision Assessment =
-        forwardTree(Stmt, RA->getAccessValue(), Stmt, InLoop);
+    ForwardingDecision Assessment = forwardTree(Stmt, RA->getAccessValue(), Stmt, InLoop);
 
-    // If considered feasible and profitable, forward it.
     bool Changed = false;
     if (Assessment == FD_CanForwardProfitably) {
       applyForwardingActions(Stmt, RA->getAccessValue(), RA);
@@ -973,17 +614,12 @@ public:
     return Changed;
   }
 
-  /// Return which SCoP this instance is processing.
   Scop *getScop() const { return S; }
 
-  /// Run the algorithm: Use value read accesses as operand tree roots and try
-  /// to forward them into the statement.
   bool forwardOperandTrees() {
     for (ScopStmt &Stmt : *S) {
       bool StmtModified = false;
 
-      // Because we are modifying the MemoryAccess list, collect them first to
-      // avoid iterator invalidation.
       SmallVector<MemoryAccess *, 16> Accs(Stmt.begin(), Stmt.end());
 
       for (MemoryAccess *RA : Accs) {
@@ -1013,13 +649,10 @@ public:
     return Modified;
   }
 
-  /// Print the pass result, performed transformations and the SCoP after the
-  /// transformation.
   void print(raw_ostream &OS, int Indent = 0) {
     printStatistics(OS, Indent);
 
     if (!Modified) {
-      // This line can easily be checked in regression tests.
       OS << "ForwardOpTree executed, but did not modify anything\n";
       return;
     }
@@ -1030,8 +663,7 @@ public:
   bool isModified() const { return Modified; }
 };
 
-static std::unique_ptr<ForwardOpTreeImpl> runForwardOpTree(Scop &S,
-                                                           LoopInfo &LI) {
+static std::unique_ptr<ForwardOpTreeImpl> runForwardOpTree(Scop &S, LoopInfo &LI) {
   std::unique_ptr<ForwardOpTreeImpl> Impl;
   {
     IslMaxOperationsGuard MaxOpGuard(S.getIslCtx().get(), MaxOps, false);
@@ -1046,8 +678,7 @@ static std::unique_ptr<ForwardOpTreeImpl
     Impl->forwardOperandTrees();
 
     if (MaxOpGuard.hasQuotaExceeded()) {
-      POLLY_DEBUG(dbgs() << "Not all operations completed because of "
-                            "max_operations exceeded\n");
+      POLLY_DEBUG(dbgs() << "Not all operations completed because of max_operations exceeded\n");
       KnownOutOfQuota++;
     }
   }
@@ -1055,7 +686,6 @@ static std::unique_ptr<ForwardOpTreeImpl
   POLLY_DEBUG(dbgs() << "\nFinal Scop:\n");
   POLLY_DEBUG(dbgs() << S);
 
-  // Update statistics
   Scop::ScopStatistics ScopStats = S.getStatistics();
   NumValueWrites += ScopStats.NumValueWrites;
   NumValueWritesInLoops += ScopStats.NumValueWritesInLoops;
@@ -1067,20 +697,17 @@ static std::unique_ptr<ForwardOpTreeImpl
   return Impl;
 }
 
-static PreservedAnalyses
-runForwardOpTreeUsingNPM(Scop &S, ScopAnalysisManager &SAM,
-                         ScopStandardAnalysisResults &SAR, SPMUpdater &U,
-                         raw_ostream *OS) {
+static PreservedAnalyses runForwardOpTreeUsingNPM(Scop &S, ScopAnalysisManager &SAM,
+                                                  ScopStandardAnalysisResults &SAR, SPMUpdater &U,
+                                                  raw_ostream *OS) {
   LoopInfo &LI = SAR.LI;
 
   std::unique_ptr<ForwardOpTreeImpl> Impl = runForwardOpTree(S, LI);
   if (OS) {
-    *OS << "Printing analysis 'Polly - Forward operand tree' for region: '"
-        << S.getName() << "' in function '" << S.getFunction().getName()
-        << "':\n";
+    *OS << "Printing analysis 'Polly - Forward operand tree' for region: '" << S.getName() << "' in function '"
+        << S.getFunction().getName() << "':\n";
     if (Impl) {
       assert(Impl->getScop() == &S);
-
       Impl->print(*OS);
     }
   }
@@ -1095,17 +722,8 @@ runForwardOpTreeUsingNPM(Scop &S, ScopAn
   return PA;
 }
 
-/// Pass that redirects scalar reads to array elements that are known to contain
-/// the same value.
-///
-/// This reduces the number of scalar accesses and therefore potentially
-/// increases the freedom of the scheduler. In the ideal case, all reads of a
-/// scalar definition are redirected (We currently do not care about removing
-/// the write in this case).  This is also useful for the main DeLICM pass as
-/// there are less scalars to be mapped.
 class ForwardOpTreeWrapperPass final : public ScopPass {
 private:
-  /// The pass implementation, also holding per-scop data.
   std::unique_ptr<ForwardOpTreeImpl> Impl;
 
 public:
@@ -1113,8 +731,7 @@ public:
 
   explicit ForwardOpTreeWrapperPass() : ScopPass(ID) {}
   ForwardOpTreeWrapperPass(const ForwardOpTreeWrapperPass &) = delete;
-  ForwardOpTreeWrapperPass &
-  operator=(const ForwardOpTreeWrapperPass &) = delete;
+  ForwardOpTreeWrapperPass &operator=(const ForwardOpTreeWrapperPass &) = delete;
 
   void getAnalysisUsage(AnalysisUsage &AU) const override {
     AU.addRequiredTransitive<ScopInfoRegionPass>();
@@ -1123,46 +740,36 @@ public:
   }
 
   bool runOnScop(Scop &S) override {
-    // Free resources for previous SCoP's computation, if not yet done.
     releaseMemory();
-
     LoopInfo &LI = getAnalysis<LoopInfoWrapperPass>().getLoopInfo();
-
     Impl = runForwardOpTree(S, LI);
-
     return false;
   }
 
   void printScop(raw_ostream &OS, Scop &S) const override {
     if (!Impl)
       return;
-
     assert(Impl->getScop() == &S);
     Impl->print(OS);
   }
 
   void releaseMemory() override { Impl.reset(); }
-}; // class ForwardOpTree
+};
 
 char ForwardOpTreeWrapperPass::ID;
 
-/// Print result from ForwardOpTreeWrapperPass.
 class ForwardOpTreePrinterLegacyPass final : public ScopPass {
 public:
   static char ID;
 
   ForwardOpTreePrinterLegacyPass() : ForwardOpTreePrinterLegacyPass(outs()) {}
-  explicit ForwardOpTreePrinterLegacyPass(llvm::raw_ostream &OS)
-      : ScopPass(ID), OS(OS) {}
+  explicit ForwardOpTreePrinterLegacyPass(llvm::raw_ostream &OS) : ScopPass(ID), OS(OS) {}
 
   bool runOnScop(Scop &S) override {
     ForwardOpTreeWrapperPass &P = getAnalysis<ForwardOpTreeWrapperPass>();
-
-    OS << "Printing analysis '" << P.getPassName() << "' for region: '"
-       << S.getRegion().getNameStr() << "' in function '"
-       << S.getFunction().getName() << "':\n";
+    OS << "Printing analysis '" << P.getPassName() << "' for region: '" << S.getRegion().getNameStr()
+       << "' in function '" << S.getFunction().getName() << "':\n";
     P.printScop(OS, S);
-
     return false;
   }
 
@@ -1179,32 +786,25 @@ private:
 char ForwardOpTreePrinterLegacyPass::ID = 0;
 } // namespace
 
-Pass *polly::createForwardOpTreeWrapperPass() {
-  return new ForwardOpTreeWrapperPass();
-}
+Pass *polly::createForwardOpTreeWrapperPass() { return new ForwardOpTreeWrapperPass(); }
 
 Pass *polly::createForwardOpTreePrinterLegacyPass(llvm::raw_ostream &OS) {
   return new ForwardOpTreePrinterLegacyPass(OS);
 }
 
-llvm::PreservedAnalyses ForwardOpTreePass::run(Scop &S,
-                                               ScopAnalysisManager &SAM,
-                                               ScopStandardAnalysisResults &SAR,
-                                               SPMUpdater &U) {
+llvm::PreservedAnalyses ForwardOpTreePass::run(Scop &S, ScopAnalysisManager &SAM,
+                                               ScopStandardAnalysisResults &SAR, SPMUpdater &U) {
   return runForwardOpTreeUsingNPM(S, SAM, SAR, U, nullptr);
 }
 
-llvm::PreservedAnalyses
-ForwardOpTreePrinterPass::run(Scop &S, ScopAnalysisManager &SAM,
-                              ScopStandardAnalysisResults &SAR, SPMUpdater &U) {
+llvm::PreservedAnalyses ForwardOpTreePrinterPass::run(Scop &S, ScopAnalysisManager &SAM,
+                                                      ScopStandardAnalysisResults &SAR, SPMUpdater &U) {
   return runForwardOpTreeUsingNPM(S, SAM, SAR, U, &OS);
 }
 
-INITIALIZE_PASS_BEGIN(ForwardOpTreeWrapperPass, "polly-optree",
-                      "Polly - Forward operand tree", false, false)
+INITIALIZE_PASS_BEGIN(ForwardOpTreeWrapperPass, "polly-optree", "Polly - Forward operand tree", false, false)
 INITIALIZE_PASS_DEPENDENCY(LoopInfoWrapperPass)
-INITIALIZE_PASS_END(ForwardOpTreeWrapperPass, "polly-optree",
-                    "Polly - Forward operand tree", false, false)
+INITIALIZE_PASS_END(ForwardOpTreeWrapperPass, "polly-optree", "Polly - Forward operand tree", false, false)
 
 INITIALIZE_PASS_BEGIN(ForwardOpTreePrinterLegacyPass, "polly-print-optree",
                       "Polly - Print forward operand tree result", false, false)

--- a/polly/lib/Transform/ScheduleOptimizer.cpp	2025-07-13 23:25:57.922600146 +0200
+++ b/polly/lib/Transform/ScheduleOptimizer.cpp	2025-07-13 23:27:47.918372394 +0200
@@ -1,49 +1,9 @@
 //===- ScheduleOptimizer.cpp - Calculate an optimized schedule ------------===//
 //
 // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
-// See https://llvm.org/LICENSE.txt for license information.
 // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 //
 //===----------------------------------------------------------------------===//
-//
-// This pass generates an entirely new schedule tree from the data dependences
-// and iteration domains. The new schedule tree is computed in two steps:
-//
-// 1) The isl scheduling optimizer is run
-//
-// The isl scheduling optimizer creates a new schedule tree that maximizes
-// parallelism and tileability and minimizes data-dependence distances. The
-// algorithm used is a modified version of the ``Pluto'' algorithm:
-//
-//   U. Bondhugula, A. Hartono, J. Ramanujam, and P. Sadayappan.
-//   A Practical Automatic Polyhedral Parallelizer and Locality Optimizer.
-//   In Proceedings of the 2008 ACM SIGPLAN Conference On Programming Language
-//   Design and Implementation, PLDI ’08, pages 101–113. ACM, 2008.
-//
-// 2) A set of post-scheduling transformations is applied on the schedule tree.
-//
-// These optimizations include:
-//
-//  - Tiling of the innermost tilable bands
-//  - Prevectorization - The choice of a possible outer loop that is strip-mined
-//                       to the innermost level to enable inner-loop
-//                       vectorization.
-//  - Some optimizations for spatial locality are also planned.
-//
-// For a detailed description of the schedule tree itself please see section 6
-// of:
-//
-// Polyhedral AST generation is more than scanning polyhedra
-// Tobias Grosser, Sven Verdoolaege, Albert Cohen
-// ACM Transactions on Programming Languages and Systems (TOPLAS),
-// 37(4), July 2015
-// http://www.grosser.es/#pub-polyhedral-AST-generation
-//
-// This publication also contains a detailed discussion of the different options
-// for polyhedral loop unrolling, full/partial tile separation and other uses
-// of the schedule tree.
-//
-//===----------------------------------------------------------------------===//
 
 #include "polly/ScheduleOptimizer.h"
 #include "polly/CodeGen/CodeGeneration.h"
@@ -52,14 +12,24 @@
 #include "polly/MatmulOptimizer.h"
 #include "polly/Options.h"
 #include "polly/ScheduleTreeTransform.h"
+#include "polly/ScopInfo.h"
 #include "polly/Support/ISLOStream.h"
 #include "polly/Support/ISLTools.h"
+#include "llvm/ADT/DenseMap.h"
 #include "llvm/ADT/Sequence.h"
+#include "llvm/ADT/SmallPtrSet.h"
 #include "llvm/ADT/Statistic.h"
 #include "llvm/Analysis/OptimizationRemarkEmitter.h"
+#include "llvm/Analysis/ScalarEvolution.h"
+#include "llvm/Analysis/TargetTransformInfo.h"
 #include "llvm/InitializePasses.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/IR/Module.h"
 #include "llvm/Support/CommandLine.h"
+#include "llvm/Support/MathExtras.h"
 #include "isl/options.h"
+#include <algorithm>
+#include <cmath>
 
 using namespace llvm;
 using namespace polly;
@@ -72,6 +42,10 @@ class Module;
 #include "polly/Support/PollyDebug.h"
 #define DEBUG_TYPE "polly-opt-isl"
 
+//===----------------------------------------------------------------------===//
+// Command-line options
+//===----------------------------------------------------------------------===//
+
 static cl::opt<std::string>
     OptimizeDeps("polly-opt-optimize-only",
                  cl::desc("Only a certain kind of dependences (all/raw)"),
@@ -99,8 +73,8 @@ static cl::opt<std::string>
 
 static cl::opt<int>
     ScheduleComputeOut("polly-schedule-computeout",
-                       cl::desc("Bound the scheduler by maximal amount"
-                                "of computational steps. "),
+                       cl::desc("Bound the scheduler by maximal amount "
+                                "of computational steps."),
                        cl::Hidden, cl::init(300000), cl::ZeroOrMore,
                        cl::cat(PollyCategory));
 
@@ -127,8 +101,8 @@ static cl::opt<bool> FirstLevelTiling("p
 
 static cl::opt<int> FirstLevelDefaultTileSize(
     "polly-default-tile-size",
-    cl::desc("The default tile size (if not enough were provided by"
-             " --polly-tile-sizes)"),
+    cl::desc("The default tile size (if not enough were provided by "
+             "--polly-tile-sizes)"),
     cl::Hidden, cl::init(32), cl::cat(PollyCategory));
 
 static cl::list<int>
@@ -144,8 +118,8 @@ static cl::opt<bool>
 
 static cl::opt<int> SecondLevelDefaultTileSize(
     "polly-2nd-level-default-tile-size",
-    cl::desc("The default 2nd-level tile size (if not enough were provided by"
-             " --polly-2nd-level-tile-sizes)"),
+    cl::desc("The default 2nd-level tile size (if not enough were provided by "
+             "--polly-2nd-level-tile-sizes)"),
     cl::Hidden, cl::init(16), cl::cat(PollyCategory));
 
 static cl::list<int>
@@ -161,9 +135,9 @@ static cl::opt<bool> RegisterTiling("pol
 
 static cl::opt<int> RegisterDefaultTileSize(
     "polly-register-tiling-default-tile-size",
-    cl::desc("The default register tile size (if not enough were provided by"
-             " --polly-register-tile-sizes)"),
-    cl::Hidden, cl::init(2), cl::cat(PollyCategory));
+    cl::desc("The default register tile size (if not enough were provided by "
+             "--polly-register-tile-sizes)"),
+    cl::Hidden, cl::init(4), cl::cat(PollyCategory));
 
 static cl::list<int>
     RegisterTileSizes("polly-register-tile-sizes",
@@ -198,39 +172,39 @@ static cl::opt<bool> OptimizedScops(
              "transformations is applied on the schedule tree"),
     cl::cat(PollyCategory));
 
+//===----------------------------------------------------------------------===//
+// Statistics
+//===----------------------------------------------------------------------===//
+
 STATISTIC(ScopsProcessed, "Number of scops processed");
+STATISTIC(ScopsRejected, "Number of scops rejected by profitability model");
 STATISTIC(ScopsRescheduled, "Number of scops rescheduled");
 STATISTIC(ScopsOptimized, "Number of scops optimized");
-
 STATISTIC(NumAffineLoopsOptimized, "Number of affine loops optimized");
 STATISTIC(NumBoxedLoopsOptimized, "Number of boxed loops optimized");
-
 #define THREE_STATISTICS(VARNAME, DESC)                                        \
   static Statistic VARNAME[3] = {                                              \
       {DEBUG_TYPE, #VARNAME "0", DESC " (original)"},                          \
       {DEBUG_TYPE, #VARNAME "1", DESC " (after scheduler)"},                   \
       {DEBUG_TYPE, #VARNAME "2", DESC " (after optimizer)"}}
-
 THREE_STATISTICS(NumBands, "Number of bands");
 THREE_STATISTICS(NumBandMembers, "Number of band members");
 THREE_STATISTICS(NumCoincident, "Number of coincident band members");
 THREE_STATISTICS(NumPermutable, "Number of permutable bands");
 THREE_STATISTICS(NumFilters, "Number of filter nodes");
 THREE_STATISTICS(NumExtension, "Number of extension nodes");
-
 STATISTIC(FirstLevelTileOpts, "Number of first level tiling applied");
 STATISTIC(SecondLevelTileOpts, "Number of second level tiling applied");
 STATISTIC(RegisterTileOpts, "Number of register tiling applied");
 STATISTIC(PrevectOpts, "Number of strip-mining for prevectorization applied");
 STATISTIC(MatMulOpts,
           "Number of matrix multiplication patterns detected and optimized");
+STATISTIC(NumVectorizedBands, "Number of bands marked for vectorization");
 
 namespace {
-/// Additional parameters of the schedule optimizer.
-///
-/// Target Transform Info and the SCoP dependencies used by the schedule
-/// optimizer.
+
 struct OptimizerAdditionalInfoTy {
+  Scop *S;
   const llvm::TargetTransformInfo *TTI;
   const Dependences *D;
   bool PatternOpts;
@@ -239,149 +213,57 @@ struct OptimizerAdditionalInfoTy {
   bool &DepsChanged;
 };
 
+static bool isSimpleInnermostBand(const isl::schedule_node &Node);
+
 class ScheduleTreeOptimizer final {
 public:
-  /// Apply schedule tree transformations.
-  ///
-  /// This function takes an (possibly already optimized) schedule tree and
-  /// applies a set of additional optimizations on the schedule tree. The
-  /// transformations applied include:
-  ///
-  ///   - Pattern-based optimizations
-  ///   - Tiling
-  ///   - Prevectorization
-  ///
-  /// @param Schedule The schedule object the transformations will be applied
-  ///                 to.
-  /// @param OAI      Target Transform Info and the SCoP dependencies.
-  /// @returns        The transformed schedule.
   static isl::schedule
-  optimizeSchedule(isl::schedule Schedule,
-                   const OptimizerAdditionalInfoTy *OAI = nullptr);
-
-  /// Apply schedule tree transformations.
-  ///
-  /// This function takes a node in an (possibly already optimized) schedule
-  /// tree and applies a set of additional optimizations on this schedule tree
-  /// node and its descendants. The transformations applied include:
-  ///
-  ///   - Pattern-based optimizations
-  ///   - Tiling
-  ///   - Prevectorization
-  ///
-  /// @param Node The schedule object post-transformations will be applied to.
-  /// @param OAI  Target Transform Info and the SCoP dependencies.
-  /// @returns    The transformed schedule.
+  optimizeSchedule(isl::schedule Schedule, const OptimizerAdditionalInfoTy *OAI);
   static isl::schedule_node
-  optimizeScheduleNode(isl::schedule_node Node,
-                       const OptimizerAdditionalInfoTy *OAI = nullptr);
-
-  /// Decide if the @p NewSchedule is profitable for @p S.
-  ///
-  /// @param S           The SCoP we optimize.
-  /// @param NewSchedule The new schedule we computed.
-  ///
-  /// @return True, if we believe @p NewSchedule is an improvement for @p S.
-  static bool isProfitableSchedule(polly::Scop &S, isl::schedule NewSchedule);
-
-  /// Isolate a set of partial tile prefixes.
-  ///
-  /// This set should ensure that it contains only partial tile prefixes that
-  /// have exactly VectorWidth iterations.
-  ///
-  /// @param Node A schedule node band, which is a parent of a band node,
-  ///             that contains a vector loop.
-  /// @return Modified isl_schedule_node.
-  static isl::schedule_node isolateFullPartialTiles(isl::schedule_node Node,
-                                                    int VectorWidth);
+  optimizeScheduleNode(isl::schedule_node Node, const OptimizerAdditionalInfoTy *OAI);
+  static bool isProfitableSchedule(Scop &S, isl::schedule NewSchedule);
+  static isl::schedule_node isolateFullPartialTiles(isl::schedule_node Node, int VectorWidth);
 
 private:
-  /// Check if this node is a band node we want to tile.
-  ///
-  /// We look for innermost band nodes where individual dimensions are marked as
-  /// permutable.
-  ///
-  /// @param Node The node to check.
   static bool isTileableBandNode(isl::schedule_node Node);
-
-  /// Check if this node is a band node we want to transform using pattern
-  /// matching.
-  ///
-  /// We look for innermost band nodes where individual dimensions are marked as
-  /// permutable. There is no restriction on the number of individual
-  /// dimensions.
-  ///
-  /// @param Node The node to check.
   static bool isPMOptimizableBandNode(isl::schedule_node Node);
-
-  /// Pre-vectorizes one scheduling dimension of a schedule band.
-  ///
-  /// prevectSchedBand splits out the dimension DimToVectorize, tiles it and
-  /// sinks the resulting point loop.
-  ///
-  /// Example (DimToVectorize=0, VectorWidth=4):
-  ///
-  /// | Before transformation:
-  /// |
-  /// | A[i,j] -> [i,j]
-  /// |
-  /// | for (i = 0; i < 128; i++)
-  /// |    for (j = 0; j < 128; j++)
-  /// |      A(i,j);
-  ///
-  /// | After transformation:
-  /// |
-  /// | for (it = 0; it < 32; it+=1)
-  /// |    for (j = 0; j < 128; j++)
-  /// |      for (ip = 0; ip <= 3; ip++)
-  /// |        A(4 * it + ip,j);
-  ///
-  /// The goal of this transformation is to create a trivially vectorizable
-  /// loop.  This means a parallel loop at the innermost level that has a
-  /// constant number of iterations corresponding to the target vector width.
-  ///
-  /// This transformation creates a loop at the innermost level. The loop has
-  /// a constant number of iterations, if the number of loop iterations at
-  /// DimToVectorize can be divided by VectorWidth. The default VectorWidth is
-  /// currently constant and not yet target specific. This function does not
-  /// reason about parallelism.
   static isl::schedule_node prevectSchedBand(isl::schedule_node Node,
                                              unsigned DimToVectorize,
-                                             int VectorWidth);
+                                             int VectorWidth,
+                                             const TargetTransformInfo *TTI);
+  static isl_schedule_node *optimizeBand(__isl_take isl_schedule_node *Node, void *User);
+  static isl::schedule_node applyTileBandOpt(isl::schedule_node Node, const OptimizerAdditionalInfoTy *OAI);
+  static isl::schedule_node applyPrevectBandOpt(isl::schedule_node Node, const OptimizerAdditionalInfoTy *OAI);
+};
 
-  /// Apply additional optimizations on the bands in the schedule tree.
-  ///
-  /// We are looking for an innermost band node and apply the following
-  /// transformations:
-  ///
-  ///  - Tile the band
-  ///      - if the band is tileable
-  ///      - if the band has more than one loop dimension
-  ///
-  ///  - Prevectorize the schedule of the band (or the point loop in case of
-  ///    tiling).
-  ///      - if vectorization is enabled
-  ///
-  /// @param Node The schedule node to (possibly) optimize.
-  /// @param User A pointer to forward some use information
-  ///        (currently unused).
-  static isl_schedule_node *optimizeBand(isl_schedule_node *Node, void *User);
-
-  /// Apply tiling optimizations on the bands in the schedule tree.
-  ///
-  /// @param Node The schedule node to (possibly) optimize.
-  static isl::schedule_node applyTileBandOpt(isl::schedule_node Node);
-
-  /// Apply prevectorization on the bands in the schedule tree.
-  ///
-  /// @param Node The schedule node to (possibly) prevectorize.
-  static isl::schedule_node applyPrevectBandOpt(isl::schedule_node Node);
+// SIMD marks for innermost simple bands.
+struct InsertSimdMarkers final : ScheduleNodeRewriter<InsertSimdMarkers> {
+  const TargetTransformInfo *TTI;
+  InsertSimdMarkers(const TargetTransformInfo *TTI) : TTI(TTI) {}
+
+  isl::schedule_node visitBand(isl::schedule_node_band Band) {
+    isl::schedule_node Node = visitChildren(Band);
+    if (!Node.isa<isl::schedule_node_band>()) {
+      return Node;
+    }
+    isl::schedule_node_band BandNode = Node.as<isl::schedule_node_band>();
+    if (!isSimpleInnermostBand(BandNode)) {
+      return BandNode;
+    }
+    if (TTI) {
+      unsigned RegBits = TTI->getLoadStoreVecRegBitWidth(0);
+      if (RegBits == 0) {
+        return BandNode;
+      }
+    }
+    NumVectorizedBands++;
+    return BandNode.insert_mark(isl::id::alloc(Band.ctx(), "SIMD", nullptr));
+  }
 };
 
 isl::schedule_node
-ScheduleTreeOptimizer::isolateFullPartialTiles(isl::schedule_node Node,
-                                               int VectorWidth) {
-  assert(isl_schedule_node_get_type(Node.get()) == isl_schedule_node_band);
+ScheduleTreeOptimizer::isolateFullPartialTiles(isl::schedule_node Node, int VectorWidth) {
+  assert(isl_schedule_node_get_type(Node.get()) == isl_schedule_node_band && "Expected a band node");
   Node = Node.child(0).child(0);
   isl::union_map SchedRelUMap = Node.get_prefix_schedule_relation();
   isl::union_set ScheduleRangeUSet = SchedRelUMap.range();
@@ -396,53 +278,31 @@ ScheduleTreeOptimizer::isolateFullPartia
   return Result;
 }
 
-struct InsertSimdMarkers final : ScheduleNodeRewriter<InsertSimdMarkers> {
-  isl::schedule_node visitBand(isl::schedule_node_band Band) {
-    isl::schedule_node Node = visitChildren(Band);
-
-    // Only add SIMD markers to innermost bands.
-    if (!Node.first_child().isa<isl::schedule_node_leaf>())
-      return Node;
-
-    isl::id LoopMarker = isl::id::alloc(Band.ctx(), "SIMD", nullptr);
-    return Band.insert_mark(LoopMarker);
-  }
-};
-
 isl::schedule_node ScheduleTreeOptimizer::prevectSchedBand(
-    isl::schedule_node Node, unsigned DimToVectorize, int VectorWidth) {
+    isl::schedule_node Node, unsigned DimToVectorize, int VectorWidth,
+    const TargetTransformInfo *TTI) {
   assert(isl_schedule_node_get_type(Node.get()) == isl_schedule_node_band);
 
   auto Space = isl::manage(isl_schedule_node_band_get_space(Node.get()));
   unsigned ScheduleDimensions = unsignedFromIslSize(Space.dim(isl::dim::set));
-  assert(DimToVectorize < ScheduleDimensions);
+  assert(DimToVectorize < ScheduleDimensions && "DimToVectorize out of range");
 
   if (DimToVectorize > 0) {
-    Node = isl::manage(
-        isl_schedule_node_band_split(Node.release(), DimToVectorize));
+    Node = isl::manage(isl_schedule_node_band_split(Node.release(), DimToVectorize));
     Node = Node.child(0);
   }
-  if (DimToVectorize < ScheduleDimensions - 1)
+  if (DimToVectorize < ScheduleDimensions - 1) {
     Node = isl::manage(isl_schedule_node_band_split(Node.release(), 1));
+  }
+
   Space = isl::manage(isl_schedule_node_band_get_space(Node.get()));
   auto Sizes = isl::multi_val::zero(Space);
   Sizes = Sizes.set_val(0, isl::val(Node.ctx(), VectorWidth));
-  Node =
-      isl::manage(isl_schedule_node_band_tile(Node.release(), Sizes.release()));
+  Node = isl::manage(isl_schedule_node_band_tile(Node.release(), Sizes.release()));
   Node = isolateFullPartialTiles(Node, VectorWidth);
   Node = Node.child(0);
-  // Make sure the "trivially vectorizable loop" is not unrolled. Otherwise,
-  // we will have troubles to match it in the backend.
-  Node = Node.as<isl::schedule_node_band>().set_ast_build_options(
-      isl::union_set(Node.ctx(), "{ unroll[x]: 1 = 0 }"));
-
-  // Sink the inner loop into the smallest possible statements to make them
-  // represent a single vector instruction if possible.
   Node = isl::manage(isl_schedule_node_band_sink(Node.release()));
-
-  // Add SIMD markers to those vector statements.
-  InsertSimdMarkers SimdMarkerInserter;
-  Node = SimdMarkerInserter.visit(Node);
+  Node = InsertSimdMarkers(TTI).visit(Node);
 
   PrevectOpts++;
   return Node.parent();
@@ -453,96 +313,254 @@ static bool isSimpleInnermostBand(const
   assert(isl_schedule_node_n_children(Node.get()) == 1);
 
   auto ChildType = isl_schedule_node_get_type(Node.child(0).get());
-
-  if (ChildType == isl_schedule_node_leaf)
+  if (ChildType == isl_schedule_node_leaf) {
     return true;
-
-  if (ChildType != isl_schedule_node_sequence)
+  }
+  if (ChildType != isl_schedule_node_sequence) {
     return false;
+  }
 
   auto Sequence = Node.child(0);
-
-  for (int c = 0, nc = isl_schedule_node_n_children(Sequence.get()); c < nc;
-       ++c) {
+  for (int c = 0, nc = isl_schedule_node_n_children(Sequence.get()); c < nc; ++c) {
     auto Child = Sequence.child(c);
-    if (isl_schedule_node_get_type(Child.get()) != isl_schedule_node_filter)
+    if (isl_schedule_node_get_type(Child.get()) != isl_schedule_node_filter) {
       return false;
-    if (isl_schedule_node_get_type(Child.child(0).get()) !=
-        isl_schedule_node_leaf)
+    }
+    if (isl_schedule_node_get_type(Child.child(0).get()) != isl_schedule_node_leaf) {
       return false;
+    }
   }
   return true;
 }
 
-/// Check if this node is a band node, which has only one child.
-///
-/// @param Node The node to check.
 static bool isOneTimeParentBandNode(isl::schedule_node Node) {
-  if (isl_schedule_node_get_type(Node.get()) != isl_schedule_node_band)
+  if (isl_schedule_node_get_type(Node.get()) != isl_schedule_node_band) {
     return false;
-
-  if (isl_schedule_node_n_children(Node.get()) != 1)
+  }
+  if (isl_schedule_node_n_children(Node.get()) != 1) {
     return false;
-
+  }
   return true;
 }
 
 bool ScheduleTreeOptimizer::isTileableBandNode(isl::schedule_node Node) {
-  if (!isOneTimeParentBandNode(Node))
+  if (!isOneTimeParentBandNode(Node)) {
     return false;
-
-  if (!isl_schedule_node_band_get_permutable(Node.get()))
+  }
+  if (!isl_schedule_node_band_get_permutable(Node.get())) {
     return false;
-
+  }
   auto Space = isl::manage(isl_schedule_node_band_get_space(Node.get()));
-
-  if (unsignedFromIslSize(Space.dim(isl::dim::set)) <= 1u)
+  if (unsignedFromIslSize(Space.dim(isl::dim::set)) <= 1u) {
     return false;
-
+  }
   return isSimpleInnermostBand(Node);
 }
 
 bool ScheduleTreeOptimizer::isPMOptimizableBandNode(isl::schedule_node Node) {
-  if (!isOneTimeParentBandNode(Node))
+  if (!isOneTimeParentBandNode(Node)) {
     return false;
-
+  }
   return Node.child(0).isa<isl::schedule_node_leaf>();
 }
 
+// Distinct arrays touched.
+static unsigned countDistinctArrays(const Scop &S) {
+  SmallPtrSet<const ScopArrayInfo *, 16> Arrays;
+  for (const ScopStmt &Stmt : S) {
+    for (const MemoryAccess *MA : Stmt) {
+      Arrays.insert(MA->getScopArrayInfo());
+    }
+  }
+  return Arrays.size();
+}
+
+// Dominant element type size, cached per Scop.
+static unsigned getDominantElementTypeBytes(Scop &S) {
+  static thread_local DenseMap<const Scop *, unsigned> Cache;
+  if (auto It = Cache.find(&S); It != Cache.end()) {
+    return It->second;
+  }
+
+  const DataLayout &DL = S.getFunction().getParent()->getDataLayout();
+  DenseMap<unsigned, unsigned> Freq;
+
+  for (const ScopStmt &Stmt : S) {
+    for (const MemoryAccess *MA : Stmt) {
+      Type *Ty = MA->getElementType();
+      if (!Ty || !Ty->isSized()) {
+        continue;
+      }
+      unsigned Bytes = static_cast<unsigned>(DL.getTypeStoreSize(Ty));
+      if (!Bytes) {
+        continue;
+      }
+      Freq[Bytes] = Freq.lookup(Bytes) + 1;
+    }
+  }
+
+  unsigned BestBytes = 4, BestCount = 0;
+  for (const auto &KV : Freq) {
+    unsigned B = KV.first, C = KV.second;
+    if (C > BestCount || (C == BestCount && B < BestBytes)) {
+      BestCount = C;
+      BestBytes = B;
+    }
+  }
+  if (BestBytes == 0) {
+    BestBytes = 4;
+  }
+  Cache[&S] = BestBytes;
+  return BestBytes;
+}
+
+// Compute lanes from vector register bitwidth.
+static int getVectorLanes(const TargetTransformInfo *TTI, unsigned ElemBytes) {
+  if (!TTI || ElemBytes == 0) {
+    return 1;
+  }
+  unsigned RegBits = TTI->getLoadStoreVecRegBitWidth(0);
+  if (RegBits == 0) {
+    return 1;
+  }
+  unsigned ElemBits = ElemBytes * 8U;
+  if (ElemBits == 0) {
+    return 1;
+  }
+  return std::max(1, static_cast<int>(RegBits / ElemBits));
+}
+
+// Baseline cache-aware default tile size.
+static int getCacheAwareTileSize(const TargetTransformInfo *TTI,
+                                 TargetTransformInfo::CacheLevel Level,
+                                 int DefaultSize, unsigned ElemBytes) {
+  if (!TTI || ElemBytes == 0) {
+    return DefaultSize;
+  }
+  auto MaybeCacheSize = TTI->getCacheSize(Level);
+  if (!MaybeCacheSize || *MaybeCacheSize == 0) {
+    return DefaultSize;
+  }
+  double Effective = 0.8 * static_cast<double>(*MaybeCacheSize) / static_cast<double>(ElemBytes);
+  double TileDouble = std::sqrt(std::max(1.0, Effective));
+  int Tile = std::clamp(static_cast<int>(TileDouble), 16, 256);
+  return static_cast<int>(llvm::PowerOf2Ceil(static_cast<unsigned>(Tile)));
+}
+
+// Smarter tile size factoring in arrays and vector lanes.
+static int getSmartTileSize(const TargetTransformInfo *TTI,
+                            TargetTransformInfo::CacheLevel Level,
+                            int DefaultSize, unsigned ElemBytes,
+                            unsigned NumArrays, int VecLanes) {
+  if (!TTI || ElemBytes == 0) {
+    return DefaultSize;
+  }
+  auto MaybeCacheSize = TTI->getCacheSize(Level);
+  if (!MaybeCacheSize || *MaybeCacheSize == 0) {
+    return DefaultSize;
+  }
+  unsigned Arrays = std::max(1u, NumArrays);
+  double Effective = 0.7 * static_cast<double>(*MaybeCacheSize) /
+                     (static_cast<double>(ElemBytes) * static_cast<double>(Arrays));
+  double TileDouble = std::sqrt(std::max(1.0, Effective));
+  int Tile = std::clamp(static_cast<int>(TileDouble), 16, 256);
+  int Pow2 = static_cast<int>(llvm::PowerOf2Ceil(static_cast<unsigned>(Tile)));
+  int Lanes = std::max(1, VecLanes);
+  int Aligned = std::max(Lanes, (Pow2 / Lanes) * Lanes);
+  return Aligned;
+}
+
+// Scoped guard for ISL scheduler options.
+struct ScopedIslScheduleOptionsGuard {
+  isl_ctx *Ctx;
+  int OldMaxConst = 0;
+  int OldMaxCoeff = 0;
+  bool Enabled = false;
+
+  ScopedIslScheduleOptionsGuard(isl_ctx *Ctx, int NewMaxConst, int NewMaxCoeff)
+      : Ctx(Ctx) {
+    if (!Ctx) {
+      return;
+    }
+    OldMaxConst = isl_options_get_schedule_max_constant_term(Ctx);
+    OldMaxCoeff = isl_options_get_schedule_max_coefficient(Ctx);
+    bool Change = false;
+    if (NewMaxConst >= 0 && (OldMaxConst < 0 || NewMaxConst < OldMaxConst)) {
+      isl_options_set_schedule_max_constant_term(Ctx, NewMaxConst);
+      Change = true;
+    }
+    if (NewMaxCoeff >= 0 && (OldMaxCoeff < 0 || NewMaxCoeff < OldMaxCoeff)) {
+      isl_options_set_schedule_max_coefficient(Ctx, NewMaxCoeff);
+      Change = true;
+    }
+    Enabled = Change;
+  }
+
+  ~ScopedIslScheduleOptionsGuard() {
+    if (!Ctx || !Enabled) {
+      return;
+    }
+    isl_options_set_schedule_max_constant_term(Ctx, OldMaxConst);
+    isl_options_set_schedule_max_coefficient(Ctx, OldMaxCoeff);
+  }
+};
+
 __isl_give isl::schedule_node
-ScheduleTreeOptimizer::applyTileBandOpt(isl::schedule_node Node) {
-  if (FirstLevelTiling) {
-    Node = tileNode(Node, "1st level tiling", FirstLevelTileSizes,
-                    FirstLevelDefaultTileSize);
+ScheduleTreeOptimizer::applyTileBandOpt(isl::schedule_node Node, const OptimizerAdditionalInfoTy *OAI) {
+  unsigned ElemBytes = getDominantElementTypeBytes(*OAI->S);
+  unsigned NumArrays = countDistinctArrays(*OAI->S);
+  int VecLanes = getVectorLanes(OAI->TTI, ElemBytes);
+
+  if (FirstLevelTiling.getValue()) {
+    int DefaultT = FirstLevelDefaultTileSize.getValue();
+    int TileSize = getSmartTileSize(OAI->TTI, TargetTransformInfo::CacheLevel::L1D,
+                                    DefaultT, ElemBytes, NumArrays, VecLanes);
+    Node = tileNode(Node, "1st level tiling", FirstLevelTileSizes, TileSize);
     FirstLevelTileOpts++;
   }
 
-  if (SecondLevelTiling) {
-    Node = tileNode(Node, "2nd level tiling", SecondLevelTileSizes,
-                    SecondLevelDefaultTileSize);
+  if (SecondLevelTiling.getValue()) {
+    int DefaultT2 = SecondLevelDefaultTileSize.getValue();
+    int TileSize2 = getSmartTileSize(OAI->TTI, TargetTransformInfo::CacheLevel::L2D,
+                                     DefaultT2, ElemBytes, NumArrays, VecLanes);
+    Node = tileNode(Node, "2nd level tiling", SecondLevelTileSizes, TileSize2);
     SecondLevelTileOpts++;
   }
 
-  if (RegisterTiling) {
-    Node =
-        applyRegisterTiling(Node, RegisterTileSizes, RegisterDefaultTileSize);
+  if (RegisterTiling.getValue()) {
+    int RegTileSize = RegisterDefaultTileSize.getValue();
+    if (OAI->TTI && ElemBytes > 0) {
+      unsigned RegBitWidth = OAI->TTI->getLoadStoreVecRegBitWidth(0);
+      if (RegBitWidth > 0) {
+        int Lanes = std::max(1, static_cast<int>(RegBitWidth / (ElemBytes * 8U)));
+        RegTileSize = std::max(RegTileSize, Lanes);
+      }
+    }
+    Node = applyRegisterTiling(Node, RegisterTileSizes, RegTileSize);
     RegisterTileOpts++;
   }
-
   return Node;
 }
 
 isl::schedule_node
-ScheduleTreeOptimizer::applyPrevectBandOpt(isl::schedule_node Node) {
+ScheduleTreeOptimizer::applyPrevectBandOpt(isl::schedule_node Node, const OptimizerAdditionalInfoTy *OAI) {
   auto Space = isl::manage(isl_schedule_node_band_get_space(Node.get()));
-  int Dims = unsignedFromIslSize(Space.dim(isl::dim::set));
+  int Dims = static_cast<int>(unsignedFromIslSize(Space.dim(isl::dim::set)));
+  unsigned ElemBytes = getDominantElementTypeBytes(*OAI->S);
 
-  for (int i = Dims - 1; i >= 0; i--)
+  for (int i = Dims - 1; i >= 0; i--) {
     if (Node.as<isl::schedule_node_band>().member_get_coincident(i)) {
-      Node = prevectSchedBand(Node, i, PrevectorWidth);
+      int VecWidth = std::max(1, PrevectorWidth.getValue());
+      if (OAI->TTI && ElemBytes > 0) {
+        unsigned RegBitWidth = OAI->TTI->getLoadStoreVecRegBitWidth(0);
+        if (RegBitWidth > 0) {
+          VecWidth = std::max(1, static_cast<int>(RegBitWidth / (ElemBytes * 8U)));
+        }
+      }
+      Node = prevectSchedBand(Node, static_cast<unsigned>(i), VecWidth, OAI->TTI);
       break;
     }
-
+  }
   return Node;
 }
 
@@ -565,16 +583,16 @@ ScheduleTreeOptimizer::optimizeBand(__is
     }
   }
 
-  if (!isTileableBandNode(Node))
+  if (!isTileableBandNode(Node)) {
     return Node.release();
+  }
 
-  if (OAI->Postopts)
-    Node = applyTileBandOpt(Node);
+  if (OAI->Postopts) {
+    Node = applyTileBandOpt(Node, OAI);
+  }
 
   if (OAI->Prevect) {
-    // FIXME: Prevectorization requirements are different from those checked by
-    // isTileableBandNode.
-    Node = applyPrevectBandOpt(Node);
+    Node = applyPrevectBandOpt(Node, OAI);
   }
 
   return Node.release();
@@ -596,27 +614,105 @@ isl::schedule_node ScheduleTreeOptimizer
   return Node;
 }
 
+static unsigned countParallelBands(const isl::schedule &Schedule) {
+  unsigned Count = 0;
+  if (isl::schedule_node Root = Schedule.get_root(); !Root.is_null()) {
+    Root.foreach_descendant_top_down(
+        [&](const isl::schedule_node &node) -> isl::boolean {
+          if (node.isa<isl::schedule_node_band>()) {
+            if (node.as<isl::schedule_node_band>().get_permutable()) {
+              Count++;
+            }
+          }
+          return isl::boolean(true);
+        });
+  }
+  return Count;
+}
+
 bool ScheduleTreeOptimizer::isProfitableSchedule(Scop &S,
                                                  isl::schedule NewSchedule) {
-  // To understand if the schedule has been optimized we check if the schedule
-  // has changed at all.
-  // TODO: We can improve this by tracking if any necessarily beneficial
-  // transformations have been performed. This can e.g. be tiling, loop
-  // interchange, or ...) We can track this either at the place where the
-  // transformation has been performed or, in case of automatic ILP based
-  // optimizations, by comparing (yet to be defined) performance metrics
-  // before/after the scheduling optimizer
-  // (e.g., #stride-one accesses)
-  // FIXME: A schedule tree whose union_map-conversion is identical to the
-  // original schedule map may still allow for parallelization, i.e. can still
-  // be profitable.
-  auto NewScheduleMap = NewSchedule.get_map();
-  auto OldSchedule = S.getSchedule();
-  assert(!OldSchedule.is_null() &&
-         "Only IslScheduleOptimizer can insert extension nodes "
-         "that make Scop::getSchedule() return nullptr.");
-  bool changed = !OldSchedule.is_equal(NewScheduleMap);
-  return changed;
+  auto OldSchedule = S.getScheduleTree();
+  assert(!OldSchedule.is_null() && "Original schedule should be valid");
+
+  unsigned NewParallelBands = countParallelBands(NewSchedule);
+  unsigned OldParallelBands = countParallelBands(OldSchedule);
+  if (NewParallelBands > OldParallelBands) {
+    return true;
+  }
+  return !OldSchedule.get_map().is_equal(NewSchedule.get_map());
+}
+
+// Profitability gate with compute/memory balance.
+static bool isProfitableToOptimize(Scop &S) {
+  const unsigned MinProfitableLoopDepth = 2;
+  const unsigned MaxIrregularAccesses = 2;
+
+  if (S.getMaxLoopDepth() < MinProfitableLoopDepth) {
+    POLLY_DEBUG(dbgs() << "Skipping SCoP: Not enough loop depth\n");
+    ScopsRejected++;
+    return false;
+  }
+
+  unsigned IrregularAccessCount = 0;
+  unsigned ComputeOps = 0;
+  unsigned MemOps = 0;
+
+  for (const ScopStmt &Stmt : S) {
+    for (const MemoryAccess *MA : Stmt) {
+      if (!MA->isAffine()) {
+        IrregularAccessCount++;
+      }
+      MemOps++;
+    }
+    for (const Instruction *I : Stmt.getInstructions()) {
+      if (const auto *BO = dyn_cast<BinaryOperator>(I)) {
+        switch (BO->getOpcode()) {
+        case Instruction::Add:
+        case Instruction::Sub:
+        case Instruction::Mul:
+        case Instruction::Shl:
+        case Instruction::AShr:
+        case Instruction::LShr:
+        case Instruction::And:
+        case Instruction::Or:
+        case Instruction::Xor:
+        case Instruction::FAdd:
+        case Instruction::FSub:
+        case Instruction::FMul:
+        case Instruction::FDiv:
+        case Instruction::FRem:
+          ComputeOps++;
+          break;
+        default:
+          break;
+        }
+      }
+    }
+  }
+
+  if (IrregularAccessCount > MaxIrregularAccesses) {
+    POLLY_DEBUG(dbgs() << "Skipping SCoP: Too many irregular memory accesses\n");
+    ScopsRejected++;
+    return false;
+  }
+
+  if (ComputeOps == 0) {
+    POLLY_DEBUG(dbgs() << "Skipping SCoP: Not enough compute\n");
+    ScopsRejected++;
+    return false;
+  }
+
+  if (MemOps > 0) {
+    double Ratio = static_cast<double>(ComputeOps) / static_cast<double>(MemOps);
+    if (Ratio < 0.25) {
+      POLLY_DEBUG(dbgs() << "Skipping SCoP: Low compute per memory access\n");
+      ScopsRejected++;
+      return false;
+    }
+  }
+
+  return true;
 }
 
 class IslScheduleOptimizerWrapperPass final : public ScopPass {
@@ -625,16 +721,9 @@ public:
 
   explicit IslScheduleOptimizerWrapperPass() : ScopPass(ID) {}
 
-  /// Optimize the schedule of the SCoP @p S.
   bool runOnScop(Scop &S) override;
-
-  /// Print the new schedule for the SCoP @p S.
   void printScop(raw_ostream &OS, Scop &S) const override;
-
-  /// Register all analyses and transformation required.
   void getAnalysisUsage(AnalysisUsage &AU) const override;
-
-  /// Release the internal memory.
   void releaseMemory() override {
     LastSchedule = {};
     IslCtx.reset();
@@ -661,251 +750,202 @@ static void printSchedule(llvm::raw_ostr
 }
 #endif
 
-/// Collect statistics for the schedule tree.
-///
-/// @param Schedule The schedule tree to analyze. If not a schedule tree it is
-/// ignored.
-/// @param Version  The version of the schedule tree that is analyzed.
-///                 0 for the original schedule tree before any transformation.
-///                 1 for the schedule tree after isl's rescheduling.
-///                 2 for the schedule tree after optimizations are applied
-///                 (tiling, pattern matching)
 static void walkScheduleTreeForStatistics(isl::schedule Schedule, int Version) {
   auto Root = Schedule.get_root();
-  if (Root.is_null())
+  if (Root.is_null()) {
     return;
+  }
 
   isl_schedule_node_foreach_descendant_top_down(
       Root.get(),
       [](__isl_keep isl_schedule_node *nodeptr, void *user) -> isl_bool {
         isl::schedule_node Node = isl::manage_copy(nodeptr);
-        int Version = *static_cast<int *>(user);
+        int Ver = *static_cast<int *>(user);
 
         switch (isl_schedule_node_get_type(Node.get())) {
         case isl_schedule_node_band: {
-          NumBands[Version]++;
-          if (isl_schedule_node_band_get_permutable(Node.get()) ==
-              isl_bool_true)
-            NumPermutable[Version]++;
-
+          NumBands[Ver]++;
+          if (isl_schedule_node_band_get_permutable(Node.get()) == isl_bool_true) {
+            NumPermutable[Ver]++;
+          }
           int CountMembers = isl_schedule_node_band_n_member(Node.get());
-          NumBandMembers[Version] += CountMembers;
+          NumBandMembers[Ver] += CountMembers;
           for (int i = 0; i < CountMembers; i += 1) {
-            if (Node.as<isl::schedule_node_band>().member_get_coincident(i))
-              NumCoincident[Version]++;
+            if (Node.as<isl::schedule_node_band>().member_get_coincident(i)) {
+              NumCoincident[Ver]++;
+            }
           }
           break;
         }
-
         case isl_schedule_node_filter:
-          NumFilters[Version]++;
+          NumFilters[Ver]++;
           break;
-
         case isl_schedule_node_extension:
-          NumExtension[Version]++;
+          NumExtension[Ver]++;
           break;
-
         default:
           break;
         }
-
         return isl_bool_true;
       },
       &Version);
 }
 
-static void runIslScheduleOptimizer(
+void prepareIslOptions(isl_ctx *Ctx) {
+  int IslMaximizeBands = (MaximizeBandDepth == "yes") ? 1 : 0;
+  if (MaximizeBandDepth != "yes" && MaximizeBandDepth != "no") {
+    errs() << "warning: Option -polly-opt-maximize-bands should be 'yes'/'no'\n";
+  }
+
+  int IslOuterCoincidence = (OuterCoincidence == "yes") ? 1 : 0;
+  if (OuterCoincidence != "yes" && OuterCoincidence != "no") {
+    errs() << "warning: Option -polly-opt-outer-coincidence should be 'yes'/'no'\n";
+  }
+
+  isl_options_set_schedule_outer_coincidence(Ctx, IslOuterCoincidence);
+  isl_options_set_schedule_maximize_band_depth(Ctx, IslMaximizeBands);
+  isl_options_set_schedule_max_constant_term(Ctx, MaxConstantTerm);
+  isl_options_set_schedule_max_coefficient(Ctx, MaxCoefficient);
+  isl_options_set_tile_scale_tile_loops(Ctx, 0);
+}
+
+isl::schedule computeSchedule(Scop &S, const Dependences &D) {
+  int ValidityKinds = Dependences::TYPE_RAW | Dependences::TYPE_WAR | Dependences::TYPE_WAW;
+  int ProximityKinds = (OptimizeDeps == "raw") ? Dependences::TYPE_RAW
+                       : Dependences::TYPE_RAW | Dependences::TYPE_WAR | Dependences::TYPE_WAW;
+
+  isl::union_set Domain = S.getDomains();
+  if (Domain.is_null()) {
+    return {};
+  }
+
+  isl::union_map Validity = D.getDependences(ValidityKinds);
+  isl::union_map Proximity = D.getDependences(ProximityKinds);
+
+  if (SimplifyDeps == "yes") {
+    Validity = Validity.gist_domain(Domain).gist_range(Domain);
+    Proximity = Proximity.gist_domain(Domain).gist_range(Domain);
+  } else if (SimplifyDeps != "no") {
+    errs() << "warning: -polly-opt-simplify-deps should be 'yes' or 'no'\n";
+  }
+
+  POLLY_DEBUG(dbgs() << "\n\nCompute schedule from:\n";
+              dbgs() << "Domain := " << Domain << ";\n";
+              dbgs() << "Proximity := " << Proximity << ";\n";
+              dbgs() << "Validity := " << Validity << ";\n");
+
+  auto SC = isl::schedule_constraints::on_domain(Domain)
+                .set_proximity(Proximity)
+                .set_validity(Validity)
+                .set_coincidence(Validity);
+
+  isl::schedule Result;
+  isl_ctx *Ctx = S.getIslCtx().get();
+  auto OnErrorStatus = isl_options_get_on_error(Ctx);
+  isl_options_set_on_error(Ctx, ISL_ON_ERROR_CONTINUE);
+
+  int NewMaxConst = MaxConstantTerm;
+  int NewMaxCoeff = MaxCoefficient;
+  if (S.getSize() > 50 || S.getMaxLoopDepth() > 3) {
+    if (NewMaxConst > 0) {
+      NewMaxConst = std::max(10, NewMaxConst / 2);
+    }
+    if (NewMaxCoeff > 0) {
+      NewMaxCoeff = std::max(10, NewMaxCoeff / 2);
+    }
+  }
+
+  {
+    ScopedIslScheduleOptionsGuard OptGuard(Ctx, NewMaxConst, NewMaxCoeff);
+    IslMaxOperationsGuard MaxOpGuard(Ctx, ScheduleComputeOut.getValue());
+    Result = SC.compute_schedule();
+    if (MaxOpGuard.hasQuotaExceeded()) {
+      POLLY_DEBUG(dbgs() << "Scheduler calculation exceeds ISL quota\n");
+    }
+  }
+
+  isl_options_set_on_error(Ctx, OnErrorStatus);
+  return Result;
+}
+
+void runIslScheduleOptimizer(
     Scop &S,
     function_ref<const Dependences &(Dependences::AnalysisLevel)> GetDeps,
     TargetTransformInfo *TTI, OptimizationRemarkEmitter *ORE,
     isl::schedule &LastSchedule, bool &DepsChanged) {
-  // Skip empty SCoPs but still allow code generation as it will delete the
-  // loops present but not needed.
   if (S.getSize() == 0) {
     S.markAsOptimized();
     return;
   }
-
   ScopsProcessed++;
 
-  // Schedule without optimizations.
+  if (!isProfitableToOptimize(S)) {
+    return;
+  }
+
   isl::schedule Schedule = S.getScheduleTree();
-  walkScheduleTreeForStatistics(S.getScheduleTree(), 0);
+  walkScheduleTreeForStatistics(Schedule, 0);
   POLLY_DEBUG(printSchedule(dbgs(), Schedule, "Original schedule tree"));
 
   bool HasUserTransformation = false;
-  if (PragmaBasedOpts) {
-    isl::schedule ManuallyTransformed = applyManualTransformations(
-        &S, Schedule, GetDeps(Dependences::AL_Statement), ORE);
+  if (PragmaBasedOpts.getValue()) {
+    isl::schedule ManuallyTransformed =
+        applyManualTransformations(&S, Schedule, GetDeps(Dependences::AL_Statement), ORE);
     if (ManuallyTransformed.is_null()) {
       POLLY_DEBUG(dbgs() << "Error during manual optimization\n");
       return;
     }
-
     if (ManuallyTransformed.get() != Schedule.get()) {
-      // User transformations have precedence over other transformations.
       HasUserTransformation = true;
       Schedule = std::move(ManuallyTransformed);
-      POLLY_DEBUG(
-          printSchedule(dbgs(), Schedule, "After manual transformations"));
+      POLLY_DEBUG(printSchedule(dbgs(), Schedule, "After manual transformations"));
     }
   }
 
-  // Only continue if either manual transformations have been applied or we are
-  // allowed to apply heuristics.
-  // TODO: Detect disabled heuristics and no user-directed transformation
-  // metadata earlier in ScopDetection.
   if (!HasUserTransformation && S.hasDisableHeuristicsHint()) {
     POLLY_DEBUG(dbgs() << "Heuristic optimizations disabled by metadata\n");
     return;
   }
 
-  // Get dependency analysis.
   const Dependences &D = GetDeps(Dependences::AL_Statement);
-  if (D.getSharedIslCtx() != S.getSharedIslCtx()) {
-    POLLY_DEBUG(dbgs() << "DependenceInfo for another SCoP/isl_ctx\n");
-    return;
-  }
-  if (!D.hasValidDependences()) {
-    POLLY_DEBUG(dbgs() << "Dependency information not available\n");
+  if (D.getSharedIslCtx() != S.getSharedIslCtx() || !D.hasValidDependences()) {
+    POLLY_DEBUG(dbgs() << "Dependency information not available or invalid\n");
     return;
   }
 
-  // Apply ISL's algorithm only if not overridden by the user. Note that
-  // post-rescheduling optimizations (tiling, pattern-based, prevectorization)
-  // rely on the coincidence/permutable annotations on schedule tree bands that
-  // are added by the rescheduling analyzer. Therefore, disabling the
-  // rescheduler implicitly also disables these optimizations.
-  if (!EnableReschedule) {
+  if (!EnableReschedule.getValue()) {
     POLLY_DEBUG(dbgs() << "Skipping rescheduling due to command line option\n");
   } else if (HasUserTransformation) {
-    POLLY_DEBUG(
-        dbgs() << "Skipping rescheduling due to manual transformation\n");
+    POLLY_DEBUG(dbgs() << "Skipping rescheduling due to manual transformation\n");
   } else {
-    // Build input data.
-    int ValidityKinds =
-        Dependences::TYPE_RAW | Dependences::TYPE_WAR | Dependences::TYPE_WAW;
-    int ProximityKinds;
-
-    if (OptimizeDeps == "all")
-      ProximityKinds =
-          Dependences::TYPE_RAW | Dependences::TYPE_WAR | Dependences::TYPE_WAW;
-    else if (OptimizeDeps == "raw")
-      ProximityKinds = Dependences::TYPE_RAW;
-    else {
-      errs() << "Do not know how to optimize for '" << OptimizeDeps << "'"
-             << " Falling back to optimizing all dependences.\n";
-      ProximityKinds =
-          Dependences::TYPE_RAW | Dependences::TYPE_WAR | Dependences::TYPE_WAW;
-    }
-
-    isl::union_set Domain = S.getDomains();
-
-    if (Domain.is_null())
+    prepareIslOptions(S.getIslCtx().get());
+    Schedule = computeSchedule(S, D);
+    if (Schedule.is_null()) {
+      POLLY_DEBUG(dbgs() << "ISL scheduler failed to find a schedule\n");
       return;
-
-    isl::union_map Validity = D.getDependences(ValidityKinds);
-    isl::union_map Proximity = D.getDependences(ProximityKinds);
-
-    // Simplify the dependences by removing the constraints introduced by the
-    // domains. This can speed up the scheduling time significantly, as large
-    // constant coefficients will be removed from the dependences. The
-    // introduction of some additional dependences reduces the possible
-    // transformations, but in most cases, such transformation do not seem to be
-    // interesting anyway. In some cases this option may stop the scheduler to
-    // find any schedule.
-    if (SimplifyDeps == "yes") {
-      Validity = Validity.gist_domain(Domain);
-      Validity = Validity.gist_range(Domain);
-      Proximity = Proximity.gist_domain(Domain);
-      Proximity = Proximity.gist_range(Domain);
-    } else if (SimplifyDeps != "no") {
-      errs()
-          << "warning: Option -polly-opt-simplify-deps should either be 'yes' "
-             "or 'no'. Falling back to default: 'yes'\n";
-    }
-
-    POLLY_DEBUG(dbgs() << "\n\nCompute schedule from: ");
-    POLLY_DEBUG(dbgs() << "Domain := " << Domain << ";\n");
-    POLLY_DEBUG(dbgs() << "Proximity := " << Proximity << ";\n");
-    POLLY_DEBUG(dbgs() << "Validity := " << Validity << ";\n");
-
-    int IslMaximizeBands;
-    if (MaximizeBandDepth == "yes") {
-      IslMaximizeBands = 1;
-    } else if (MaximizeBandDepth == "no") {
-      IslMaximizeBands = 0;
-    } else {
-      errs()
-          << "warning: Option -polly-opt-maximize-bands should either be 'yes'"
-             " or 'no'. Falling back to default: 'yes'\n";
-      IslMaximizeBands = 1;
-    }
-
-    int IslOuterCoincidence;
-    if (OuterCoincidence == "yes") {
-      IslOuterCoincidence = 1;
-    } else if (OuterCoincidence == "no") {
-      IslOuterCoincidence = 0;
-    } else {
-      errs() << "warning: Option -polly-opt-outer-coincidence should either be "
-                "'yes' or 'no'. Falling back to default: 'no'\n";
-      IslOuterCoincidence = 0;
-    }
-
-    isl_ctx *Ctx = S.getIslCtx().get();
-
-    isl_options_set_schedule_outer_coincidence(Ctx, IslOuterCoincidence);
-    isl_options_set_schedule_maximize_band_depth(Ctx, IslMaximizeBands);
-    isl_options_set_schedule_max_constant_term(Ctx, MaxConstantTerm);
-    isl_options_set_schedule_max_coefficient(Ctx, MaxCoefficient);
-    isl_options_set_tile_scale_tile_loops(Ctx, 0);
-
-    auto OnErrorStatus = isl_options_get_on_error(Ctx);
-    isl_options_set_on_error(Ctx, ISL_ON_ERROR_CONTINUE);
-
-    auto SC = isl::schedule_constraints::on_domain(Domain);
-    SC = SC.set_proximity(Proximity);
-    SC = SC.set_validity(Validity);
-    SC = SC.set_coincidence(Validity);
-
-    {
-      IslMaxOperationsGuard MaxOpGuard(Ctx, ScheduleComputeOut);
-      Schedule = SC.compute_schedule();
-
-      if (MaxOpGuard.hasQuotaExceeded())
-        POLLY_DEBUG(
-            dbgs() << "Schedule optimizer calculation exceeds ISL quota\n");
     }
-
-    isl_options_set_on_error(Ctx, OnErrorStatus);
-
     ScopsRescheduled++;
     POLLY_DEBUG(printSchedule(dbgs(), Schedule, "After rescheduling"));
   }
-
   walkScheduleTreeForStatistics(Schedule, 1);
 
-  // In cases the scheduler is not able to optimize the code, we just do not
-  // touch the schedule.
-  if (Schedule.is_null())
-    return;
-
-  if (GreedyFusion) {
+  if (GreedyFusion.getValue()) {
+    isl::schedule BeforeGF = Schedule;
     isl::union_map Validity = D.getDependences(
         Dependences::TYPE_RAW | Dependences::TYPE_WAR | Dependences::TYPE_WAW);
     Schedule = applyGreedyFusion(Schedule, Validity);
-    assert(!Schedule.is_null());
+    assert(!Schedule.is_null() && "Greedy fusion should not fail");
+    unsigned BeforeBands = countParallelBands(BeforeGF);
+    unsigned AfterBands = countParallelBands(Schedule);
+    if (AfterBands < BeforeBands && Schedule.get_map().is_equal(BeforeGF.get_map())) {
+      Schedule = BeforeGF;
+    }
   }
 
-  // Apply post-rescheduling optimizations (if enabled) and/or prevectorization.
   const OptimizerAdditionalInfoTy OAI = {
-      TTI,
-      const_cast<Dependences *>(&D),
-      /*PatternOpts=*/!HasUserTransformation && PMBasedOpts,
-      /*Postopts=*/!HasUserTransformation && EnablePostopts,
-      /*Prevect=*/PollyVectorizerChoice != VECTORIZER_NONE,
-      DepsChanged};
+      &S, TTI, &D, !HasUserTransformation && PMBasedOpts.getValue(),
+      !HasUserTransformation && EnablePostopts.getValue(),
+      PollyVectorizerChoice != VECTORIZER_NONE, DepsChanged};
   if (OAI.PatternOpts || OAI.Postopts || OAI.Prevect) {
     Schedule = ScheduleTreeOptimizer::optimizeSchedule(Schedule, &OAI);
     Schedule = hoistExtensionNodes(Schedule);
@@ -913,82 +953,58 @@ static void runIslScheduleOptimizer(
     walkScheduleTreeForStatistics(Schedule, 2);
   }
 
-  // Skip profitability check if user transformation(s) have been applied.
-  if (!HasUserTransformation &&
-      !ScheduleTreeOptimizer::isProfitableSchedule(S, Schedule))
+  if (!HasUserTransformation && !ScheduleTreeOptimizer::isProfitableSchedule(S, Schedule)) {
     return;
+  }
 
   auto ScopStats = S.getStatistics();
   ScopsOptimized++;
   NumAffineLoopsOptimized += ScopStats.NumAffineLoops;
   NumBoxedLoopsOptimized += ScopStats.NumBoxedLoops;
   LastSchedule = Schedule;
-
   S.setScheduleTree(Schedule);
   S.markAsOptimized();
 
-  if (OptimizedScops)
+  if (OptimizedScops.getValue()) {
     errs() << S;
+  }
 }
 
 bool IslScheduleOptimizerWrapperPass::runOnScop(Scop &S) {
   releaseMemory();
-
-  Function &F = S.getFunction();
   IslCtx = S.getSharedIslCtx();
-
-  auto getDependences =
-      [this](Dependences::AnalysisLevel) -> const Dependences & {
-    return getAnalysis<DependenceInfo>().getDependences(
-        Dependences::AL_Statement);
+  auto GetDeps = [this](Dependences::AnalysisLevel) -> const Dependences & {
+    return getAnalysis<DependenceInfo>().getDependences(Dependences::AL_Statement);
   };
-  OptimizationRemarkEmitter &ORE =
-      getAnalysis<OptimizationRemarkEmitterWrapperPass>().getORE();
-  TargetTransformInfo *TTI =
-      &getAnalysis<TargetTransformInfoWrapperPass>().getTTI(F);
-
+  OptimizationRemarkEmitter &ORE = getAnalysis<OptimizationRemarkEmitterWrapperPass>().getORE();
+  TargetTransformInfo &TTI = getAnalysis<TargetTransformInfoWrapperPass>().getTTI(S.getFunction());
   bool DepsChanged = false;
-  runIslScheduleOptimizer(S, getDependences, TTI, &ORE, LastSchedule,
-                          DepsChanged);
-  if (DepsChanged)
+  runIslScheduleOptimizer(S, GetDeps, &TTI, &ORE, LastSchedule, DepsChanged);
+  if (DepsChanged) {
     getAnalysis<DependenceInfo>().abandonDependences();
+  }
   return false;
 }
 
 static void runScheduleOptimizerPrinter(raw_ostream &OS,
                                         isl::schedule LastSchedule) {
-  isl_printer *p;
-  char *ScheduleStr;
-
-  OS << "Calculated schedule:\n";
-
   if (LastSchedule.is_null()) {
     OS << "n/a\n";
     return;
   }
-
-  p = isl_printer_to_str(LastSchedule.ctx().get());
-  p = isl_printer_set_yaml_style(p, ISL_YAML_STYLE_BLOCK);
-  p = isl_printer_print_schedule(p, LastSchedule.get());
-  ScheduleStr = isl_printer_get_str(p);
-  isl_printer_free(p);
-
-  OS << ScheduleStr << "\n";
-
-  free(ScheduleStr);
+  OS << LastSchedule << "\n";
 }
 
 void IslScheduleOptimizerWrapperPass::printScop(raw_ostream &OS, Scop &) const {
+  OS << "Calculated schedule:\n";
   runScheduleOptimizerPrinter(OS, LastSchedule);
 }
 
-void IslScheduleOptimizerWrapperPass::getAnalysisUsage(
-    AnalysisUsage &AU) const {
+void IslScheduleOptimizerWrapperPass::getAnalysisUsage(AnalysisUsage &AU) const {
   ScopPass::getAnalysisUsage(AU);
   AU.addRequired<DependenceInfo>();
   AU.addRequired<TargetTransformInfoWrapperPass>();
   AU.addRequired<OptimizationRemarkEmitterWrapperPass>();
-
   AU.addPreserved<DependenceInfo>();
   AU.addPreserved<OptimizationRemarkEmitterWrapperPass>();
 }
@@ -1008,7 +1024,11 @@ INITIALIZE_PASS_DEPENDENCY(OptimizationR
 INITIALIZE_PASS_END(IslScheduleOptimizerWrapperPass, "polly-opt-isl",
                     "Polly - Optimize schedule of SCoP", false, false)
 
-static llvm::PreservedAnalyses
+//===----------------------------------------------------------------------===//
+// New Pass Manager Implementation
+//===----------------------------------------------------------------------===//
+
+static PreservedAnalyses
 runIslScheduleOptimizerUsingNPM(Scop &S, ScopAnalysisManager &SAM,
                                 ScopStandardAnalysisResults &SAR, SPMUpdater &U,
                                 raw_ostream *OS) {
@@ -1017,12 +1037,12 @@ runIslScheduleOptimizerUsingNPM(Scop &S,
     return Deps.getDependences(Dependences::AL_Statement);
   };
   OptimizationRemarkEmitter ORE(&S.getFunction());
-  TargetTransformInfo *TTI = &SAR.TTI;
   isl::schedule LastSchedule;
   bool DepsChanged = false;
-  runIslScheduleOptimizer(S, GetDeps, TTI, &ORE, LastSchedule, DepsChanged);
-  if (DepsChanged)
+  runIslScheduleOptimizer(S, GetDeps, &SAR.TTI, &ORE, LastSchedule, DepsChanged);
+  if (DepsChanged) {
     Deps.abandonDependences();
+  }
 
   if (OS) {
     *OS << "Printing analysis 'Polly - Optimize schedule of SCoP' for region: '"
@@ -1033,13 +1053,13 @@ runIslScheduleOptimizerUsingNPM(Scop &S,
   return PreservedAnalyses::all();
 }
 
-llvm::PreservedAnalyses
+PreservedAnalyses
 IslScheduleOptimizerPass::run(Scop &S, ScopAnalysisManager &SAM,
                               ScopStandardAnalysisResults &SAR, SPMUpdater &U) {
   return runIslScheduleOptimizerUsingNPM(S, SAM, SAR, U, nullptr);
 }
 
-llvm::PreservedAnalyses
+PreservedAnalyses
 IslScheduleOptimizerPrinterPass::run(Scop &S, ScopAnalysisManager &SAM,
                                      ScopStandardAnalysisResults &SAR,
                                      SPMUpdater &U) {
@@ -1047,13 +1067,14 @@ IslScheduleOptimizerPrinterPass::run(Sco
 }
 
 //===----------------------------------------------------------------------===//
+// Legacy Printer Pass
+//===----------------------------------------------------------------------===//
 
 namespace {
-/// Print result from IslScheduleOptimizerWrapperPass.
+
 class IslScheduleOptimizerPrinterLegacyPass final : public ScopPass {
 public:
   static char ID;
-
   IslScheduleOptimizerPrinterLegacyPass()
       : IslScheduleOptimizerPrinterLegacyPass(outs()) {}
   explicit IslScheduleOptimizerPrinterLegacyPass(llvm::raw_ostream &OS)
@@ -1062,12 +1083,10 @@ public:
   bool runOnScop(Scop &S) override {
     IslScheduleOptimizerWrapperPass &P =
         getAnalysis<IslScheduleOptimizerWrapperPass>();
-
     OS << "Printing analysis '" << P.getPassName() << "' for region: '"
        << S.getRegion().getNameStr() << "' in function '"
        << S.getFunction().getName() << "':\n";
     P.printScop(OS, S);
-
     return false;
   }
 
@@ -1082,6 +1101,7 @@ private:
 };
 
 char IslScheduleOptimizerPrinterLegacyPass::ID = 0;
+
 } // namespace
 
 Pass *polly::createIslScheduleOptimizerPrinterLegacyPass(raw_ostream &OS) {
