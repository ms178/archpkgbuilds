--- a/polly/lib/Analysis/ScopBuilder.cpp	2025-09-21 15:28:29.292203509 +0200
+++ b/polly/lib/Analysis/ScopBuilder.cpp	2026-01-04 15:35:24.476840546 +0200
@@ -214,50 +214,92 @@ static isl::map createNextIterationMap(i
 
 /// Add @p BSet to set @p BoundedParts if @p BSet is bounded.
 static isl::set collectBoundedParts(isl::set S) {
+  if (S.is_null())
+    return isl::set();
+
   isl::set BoundedParts = isl::set::empty(S.get_space());
-  for (isl::basic_set BSet : S.get_basic_set_list())
+  if (BoundedParts.is_null())
+    return isl::set();
+
+  for (isl::basic_set BSet : S.get_basic_set_list()) {
+    if (BSet.is_null())
+      continue;
     if (BSet.is_bounded())
       BoundedParts = BoundedParts.unite(isl::set(BSet));
+  }
   return BoundedParts;
 }
 
 /// Compute the (un)bounded parts of @p S wrt. to dimension @p Dim.
 ///
 /// @returns A separation of @p S into first an unbounded then a bounded subset,
-///          both with regards to the dimension @p Dim.
+///          both with regards to the dimension @p Dim. Returns a pair of null
+///          sets if computation fails.
 static std::pair<isl::set, isl::set> partitionSetParts(isl::set S,
                                                        unsigned Dim) {
-  for (unsigned u : rangeIslSize(0, S.tuple_dim()))
+  if (S.is_null())
+    return std::make_pair(isl::set(), isl::set());
+
+  for (unsigned u : rangeIslSize(0, S.tuple_dim())) {
     S = S.lower_bound_si(isl::dim::set, u, 0);
+    if (S.is_null())
+      return std::make_pair(isl::set(), isl::set());
+  }
 
-  unsigned NumDimsS = unsignedFromIslSize(S.tuple_dim());
+  isl::size TupleDimSize = S.tuple_dim();
+  if (TupleDimSize.is_error())
+    return std::make_pair(isl::set(), isl::set());
+
+  unsigned NumDimsS = unsignedFromIslSize(TupleDimSize);
   isl::set OnlyDimS = S;
 
   // Remove dimensions that are greater than Dim as they are not interesting.
   assert(NumDimsS >= Dim + 1);
   OnlyDimS = OnlyDimS.project_out(isl::dim::set, Dim + 1, NumDimsS - Dim - 1);
+  if (OnlyDimS.is_null())
+    return std::make_pair(isl::set(), isl::set());
 
   // Create artificial parametric upper bounds for dimensions smaller than Dim
   // as we are not interested in them.
   OnlyDimS = OnlyDimS.insert_dims(isl::dim::param, 0, Dim);
+  if (OnlyDimS.is_null())
+    return std::make_pair(isl::set(), isl::set());
 
   for (unsigned u = 0; u < Dim; u++) {
-    isl::constraint C = isl::constraint::alloc_inequality(
-        isl::local_space(OnlyDimS.get_space()));
+    isl::space Space = OnlyDimS.get_space();
+    if (Space.is_null())
+      return std::make_pair(isl::set(), isl::set());
+
+    isl::local_space LS(Space);
+    if (LS.is_null())
+      return std::make_pair(isl::set(), isl::set());
+
+    isl::constraint C = isl::constraint::alloc_inequality(LS);
+    if (C.is_null())
+      return std::make_pair(isl::set(), isl::set());
+
     C = C.set_coefficient_si(isl::dim::param, u, 1);
     C = C.set_coefficient_si(isl::dim::set, u, -1);
     OnlyDimS = OnlyDimS.add_constraint(C);
+    if (OnlyDimS.is_null())
+      return std::make_pair(isl::set(), isl::set());
   }
 
   // Collect all bounded parts of OnlyDimS.
   isl::set BoundedParts = collectBoundedParts(OnlyDimS);
+  if (BoundedParts.is_null())
+    return std::make_pair(isl::set(), isl::set());
 
   // Create the dimensions greater than Dim again.
   BoundedParts =
       BoundedParts.insert_dims(isl::dim::set, Dim + 1, NumDimsS - Dim - 1);
+  if (BoundedParts.is_null())
+    return std::make_pair(isl::set(), isl::set());
 
   // Remove the artificial upper bound parameters again.
   BoundedParts = BoundedParts.remove_dims(isl::dim::param, 0, Dim);
+  if (BoundedParts.is_null())
+    return std::make_pair(isl::set(), isl::set());
 
   isl::set UnboundedParts = S.subtract(BoundedParts);
   return std::make_pair(UnboundedParts, BoundedParts);
@@ -632,7 +674,7 @@ void ScopBuilder::propagateDomainConstra
     BasicBlock *BB, Loop *BBLoop,
     SmallPtrSetImpl<BasicBlock *> &FinishedExitBlocks,
     DenseMap<BasicBlock *, isl::set> &InvalidDomainMap) {
-  // Check if the block @p BB is the entry of a region. If so we propagate it's
+  // Check if the block @p BB is the entry of a region. If so we propagate its
   // domain to the exit block of the region. Otherwise we are done.
   auto *RI = scop->getRegion().getRegionInfo();
   auto *BBReg = RI ? RI->getRegionFor(BB) : nullptr;
@@ -642,10 +684,10 @@ void ScopBuilder::propagateDomainConstra
 
   // Do not propagate the domain if there is a loop backedge inside the region
   // that would prevent the exit block from being executed.
-  auto *L = BBLoop;
+  Loop *L = BBLoop;
   while (L && scop->contains(L)) {
     SmallVector<BasicBlock *, 4> LatchBBs;
-    BBLoop->getLoopLatches(LatchBBs);
+    L->getLoopLatches(LatchBBs);
     for (auto *LatchBB : LatchBBs)
       if (BB != LatchBB && BBReg->contains(LatchBB))
         return;
@@ -737,10 +779,21 @@ bool ScopBuilder::addLoopBoundsToHeaderD
   assert(scop->isDomainDefined(HeaderBB));
   isl::set &HeaderBBDom = scop->getOrInitEmptyDomain(HeaderBB);
 
+  if (HeaderBBDom.is_null())
+    return false;
+
+  isl::space DomSpace = HeaderBBDom.get_space();
+  if (DomSpace.is_null())
+    return false;
+
   isl::map NextIterationMap =
-      createNextIterationMap(HeaderBBDom.get_space(), LoopDepth);
+      createNextIterationMap(DomSpace, static_cast<unsigned>(LoopDepth));
+  if (NextIterationMap.is_null())
+    return false;
 
-  isl::set UnionBackedgeCondition = HeaderBBDom.empty(HeaderBBDom.get_space());
+  isl::set UnionBackedgeCondition = HeaderBBDom.empty(DomSpace);
+  if (UnionBackedgeCondition.is_null())
+    return false;
 
   SmallVector<BasicBlock *, 4> LatchBlocks;
   L->getLoopLatches(LatchBlocks);
@@ -751,6 +804,8 @@ bool ScopBuilder::addLoopBoundsToHeaderD
       continue;
 
     isl::set LatchBBDom = scop->getDomainConditions(LatchBB);
+    if (LatchBBDom.is_null())
+      continue;
 
     isl::set BackedgeCondition;
 
@@ -758,9 +813,9 @@ bool ScopBuilder::addLoopBoundsToHeaderD
     BranchInst *BI = dyn_cast<BranchInst>(TI);
     assert(BI && "Only branch instructions allowed in loop latches");
 
-    if (BI->isUnconditional())
+    if (BI->isUnconditional()) {
       BackedgeCondition = LatchBBDom;
-    else {
+    } else {
       SmallVector<isl_set *, 8> ConditionSets;
       int idx = BI->getSuccessor(0) != HeaderBB;
       if (!buildConditionSets(LatchBB, TI, L, LatchBBDom.get(),
@@ -773,28 +828,59 @@ bool ScopBuilder::addLoopBoundsToHeaderD
       BackedgeCondition = isl::manage(ConditionSets[idx]);
     }
 
+    if (BackedgeCondition.is_null())
+      continue;
+
     int LatchLoopDepth = scop->getRelativeLoopDepth(LI.getLoopFor(LatchBB));
     assert(LatchLoopDepth >= LoopDepth);
     BackedgeCondition = BackedgeCondition.project_out(
         isl::dim::set, LoopDepth + 1, LatchLoopDepth - LoopDepth);
+    if (BackedgeCondition.is_null())
+      continue;
+
     UnionBackedgeCondition = UnionBackedgeCondition.unite(BackedgeCondition);
+    if (UnionBackedgeCondition.is_null())
+      return false;
   }
 
-  isl::map ForwardMap = ForwardMap.lex_le(HeaderBBDom.get_space());
-  for (int i = 0; i < LoopDepth; i++)
+  isl::map ForwardMap = isl::map::lex_le(DomSpace);
+  if (ForwardMap.is_null())
+    return false;
+
+  for (int i = 0; i < LoopDepth; i++) {
     ForwardMap = ForwardMap.equate(isl::dim::in, i, isl::dim::out, i);
+    if (ForwardMap.is_null())
+      return false;
+  }
 
   isl::set UnionBackedgeConditionComplement =
       UnionBackedgeCondition.complement();
+  if (UnionBackedgeConditionComplement.is_null())
+    return false;
+
   UnionBackedgeConditionComplement =
       UnionBackedgeConditionComplement.lower_bound_si(isl::dim::set, LoopDepth,
                                                       0);
+  if (UnionBackedgeConditionComplement.is_null())
+    return false;
+
   UnionBackedgeConditionComplement =
       UnionBackedgeConditionComplement.apply(ForwardMap);
+  if (UnionBackedgeConditionComplement.is_null())
+    return false;
+
   HeaderBBDom = HeaderBBDom.subtract(UnionBackedgeConditionComplement);
+  if (HeaderBBDom.is_null())
+    return false;
+
   HeaderBBDom = HeaderBBDom.apply(NextIterationMap);
+  if (HeaderBBDom.is_null())
+    return false;
+
+  auto Parts = partitionSetParts(HeaderBBDom, static_cast<unsigned>(LoopDepth));
+  if (Parts.first.is_null() || Parts.second.is_null())
+    return false;
 
-  auto Parts = partitionSetParts(HeaderBBDom, LoopDepth);
   HeaderBBDom = Parts.second;
 
   // Check if there is a <nsw> tagged AddRec for this loop and if so do not
@@ -803,9 +889,11 @@ bool ScopBuilder::addLoopBoundsToHeaderD
   bool RequiresRTC = !scop->hasNSWAddRecForLoop(L);
 
   isl::set UnboundedCtx = Parts.first.params();
-  recordAssumption(&RecordedAssumptions, INFINITELOOP, UnboundedCtx,
-                   HeaderBB->getTerminator()->getDebugLoc(), AS_RESTRICTION,
-                   nullptr, RequiresRTC);
+  if (!UnboundedCtx.is_null()) {
+    recordAssumption(&RecordedAssumptions, INFINITELOOP, UnboundedCtx,
+                     HeaderBB->getTerminator()->getDebugLoc(), AS_RESTRICTION,
+                     nullptr, RequiresRTC);
+  }
   return true;
 }
 
@@ -897,8 +985,7 @@ bool ScopBuilder::buildDomainsWithBranch
     }
 
     if (containsErrorBlock(RN, scop->getRegion(), &SD))
-      scop->notifyErrorBlock();
-    ;
+      scop->notifyErrorBlock();  // FIX: Removed extraneous semicolon
 
     BasicBlock *BB = getRegionNodeBasicBlock(RN);
     Instruction *TI = BB->getTerminator();
@@ -945,7 +1032,8 @@ bool ScopBuilder::buildDomainsWithBranch
     // we leave a loop not to keep constraints over a dimension that doesn't
     // exist anymore.
     assert(RN->isSubRegion() || TI->getNumSuccessors() == ConditionSets.size());
-    for (unsigned u = 0, e = ConditionSets.size(); u < e; u++) {
+    unsigned NumCondSets = static_cast<unsigned>(ConditionSets.size());
+    for (unsigned u = 0; u < NumCondSets; u++) {
       isl::set CondSet = isl::manage(ConditionSets[u]);
       BasicBlock *SuccBB = getRegionNodeSuccessor(RN, TI, u);
 
@@ -988,7 +1076,7 @@ bool ScopBuilder::buildDomainsWithBranch
         continue;
 
       scop->invalidate(COMPLEXITY, DebugLoc());
-      while (++u < ConditionSets.size())
+      while (++u < NumCondSets)
         isl_set_free(ConditionSets[u]);
       return false;
     }
@@ -1419,16 +1507,26 @@ void ScopBuilder::addUserAssumptions(
 
     // Project out newly introduced parameters as they are not otherwise useful.
     if (!NewParams.empty()) {
-      for (isl_size u = 0; u < isl_set_n_param(AssumptionCtx); u++) {
-        auto *Id = isl_set_get_dim_id(AssumptionCtx, isl_dim_param, u);
-        auto *Param = static_cast<const SCEV *>(isl_id_get_user(Id));
+      isl_size CtxNumParams = isl_set_n_param(AssumptionCtx);
+      // Handle ISL error case
+      if (CtxNumParams < 0) {
+        isl_set_free(AssumptionCtx);
+        continue;
+      }
+
+      // Iterate backwards to avoid index shifting issues when projecting out
+      for (isl_size u = CtxNumParams - 1; u >= 0; u--) {
+        isl_id *Id = isl_set_get_dim_id(AssumptionCtx, isl_dim_param, u);
+        if (!Id)
+          continue;
+        const SCEV *Param = static_cast<const SCEV *>(isl_id_get_user(Id));
         isl_id_free(Id);
 
         if (!NewParams.count(Param))
           continue;
 
         AssumptionCtx =
-            isl_set_project_out(AssumptionCtx, isl_dim_param, u--, 1);
+            isl_set_project_out(AssumptionCtx, isl_dim_param, u, 1);
       }
     }
     ORE.emit(OptimizationRemarkAnalysis(DEBUG_TYPE, "UserAssumption", CI)
@@ -2159,7 +2257,10 @@ void ScopBuilder::addArrayAccess(ScopStm
 
 /// Check if @p Expr is divisible by @p Size.
 static bool isDivisible(const SCEV *Expr, unsigned Size, ScalarEvolution &SE) {
-  assert(Size != 0);
+  // Defensive check: Size of 0 would cause issues.
+  // In release builds, the assert is removed, so guard explicitly.
+  if (Size == 0)
+    return false;
   if (Size == 1)
     return true;
 
@@ -2188,74 +2289,183 @@ static bool isDivisible(const SCEV *Expr
 
 void ScopBuilder::foldSizeConstantsToRight() {
   isl::union_set Accessed = scop->getAccesses().range();
+  if (Accessed.is_null())
+    return;
 
   for (auto Array : scop->arrays()) {
     if (Array->getNumberOfDimensions() <= 1)
       continue;
 
     isl::space Space = Array->getSpace();
-    Space = Space.align_params(Accessed.get_space());
+    if (Space.is_null())
+      continue;
+
+    isl::space AccessedSpace = Accessed.get_space();
+    if (AccessedSpace.is_null())
+      continue;
+
+    Space = Space.align_params(AccessedSpace);
+    if (Space.is_null())
+      continue;
 
     if (!Accessed.contains(Space))
       continue;
 
     isl::set Elements = Accessed.extract_set(Space);
-    isl::map Transform = isl::map::universe(Array->getSpace().map_from_set());
+    if (Elements.is_null())
+      continue;
+
+    isl::space MapSpace = Array->getSpace().map_from_set();
+    if (MapSpace.is_null())
+      continue;
+
+    isl::map Transform = isl::map::universe(MapSpace);
+    if (Transform.is_null())
+      continue;
 
     std::vector<int> Int;
-    unsigned Dims = unsignedFromIslSize(Elements.tuple_dim());
-    for (unsigned i = 0; i < Dims; i++) {
-      isl::set DimOnly = isl::set(Elements).project_out(isl::dim::set, 0, i);
+    isl::size TupleDimSize = Elements.tuple_dim();
+    if (TupleDimSize.is_error())
+      continue;
+
+    unsigned Dims = unsignedFromIslSize(TupleDimSize);
+
+    bool Failed = false;
+    for (unsigned i = 0; i < Dims && !Failed; i++) {
+      isl::set DimOnly = isl::set(Elements);
+      if (DimOnly.is_null()) {
+        Failed = true;
+        break;
+      }
+
+      DimOnly = DimOnly.project_out(isl::dim::set, 0, i);
+      if (DimOnly.is_null()) {
+        Failed = true;
+        break;
+      }
+
       DimOnly = DimOnly.project_out(isl::dim::set, 1, Dims - i - 1);
+      if (DimOnly.is_null()) {
+        Failed = true;
+        break;
+      }
+
       DimOnly = DimOnly.lower_bound_si(isl::dim::set, 0, 0);
+      if (DimOnly.is_null()) {
+        Failed = true;
+        break;
+      }
 
       isl::basic_set DimHull = DimOnly.affine_hull();
+      if (DimHull.is_null()) {
+        Failed = true;
+        break;
+      }
 
       if (i == Dims - 1) {
         Int.push_back(1);
         Transform = Transform.equate(isl::dim::in, i, isl::dim::out, i);
+        if (Transform.is_null()) {
+          Failed = true;
+          break;
+        }
         continue;
       }
 
-      if (unsignedFromIslSize(DimHull.dim(isl::dim::div)) == 1) {
+      isl::size NumDivsSize = DimHull.dim(isl::dim::div);
+      if (NumDivsSize.is_error()) {
+        Failed = true;
+        break;
+      }
+
+      unsigned NumDivs = unsignedFromIslSize(NumDivsSize);
+
+      if (NumDivs == 1) {
         isl::aff Diff = DimHull.get_div(0);
+        if (Diff.is_null()) {
+          Failed = true;
+          break;
+        }
+
         isl::val Val = Diff.get_denominator_val();
+        if (Val.is_null()) {
+          Failed = true;
+          break;
+        }
 
         int ValInt = 1;
         if (Val.is_int()) {
           auto ValAPInt = APIntFromVal(Val);
           if (ValAPInt.isSignedIntN(32))
             ValInt = ValAPInt.getSExtValue();
-        } else {
         }
 
         Int.push_back(ValInt);
-        isl::constraint C = isl::constraint::alloc_equality(
-            isl::local_space(Transform.get_space()));
+
+        isl::space TransformSpace = Transform.get_space();
+        if (TransformSpace.is_null()) {
+          Failed = true;
+          break;
+        }
+
+        isl::local_space LS(TransformSpace);
+        if (LS.is_null()) {
+          Failed = true;
+          break;
+        }
+
+        isl::constraint C = isl::constraint::alloc_equality(LS);
+        if (C.is_null()) {
+          Failed = true;
+          break;
+        }
+
         C = C.set_coefficient_si(isl::dim::out, i, ValInt);
         C = C.set_coefficient_si(isl::dim::in, i, -1);
         Transform = Transform.add_constraint(C);
+        if (Transform.is_null()) {
+          Failed = true;
+          break;
+        }
         continue;
       }
 
       isl::basic_set ZeroSet = isl::basic_set(DimHull);
+      if (ZeroSet.is_null()) {
+        Failed = true;
+        break;
+      }
+
       ZeroSet = ZeroSet.fix_si(isl::dim::set, 0, 0);
+      if (ZeroSet.is_null()) {
+        Failed = true;
+        break;
+      }
 
       int ValInt = 1;
-      if (ZeroSet.is_equal(DimHull)) {
+      if (ZeroSet.is_equal(DimHull))
         ValInt = 0;
-      }
 
       Int.push_back(ValInt);
       Transform = Transform.equate(isl::dim::in, i, isl::dim::out, i);
+      if (Transform.is_null()) {
+        Failed = true;
+        break;
+      }
     }
 
+    if (Failed)
+      continue;
+
     isl::set MappedElements = isl::map(Transform).domain();
+    if (MappedElements.is_null())
+      continue;
+
     if (!Elements.is_subset(MappedElements))
       continue;
 
     bool CanFold = true;
-    if (Int[0] <= 1)
+    if (Int.empty() || Int[0] <= 1)
       CanFold = false;
 
     unsigned NumDims = Array->getNumberOfDimensions();
@@ -2266,10 +2476,18 @@ void ScopBuilder::foldSizeConstantsToRig
     if (!CanFold)
       continue;
 
-    for (auto &Access : scop->access_functions())
-      if (Access->getScopArrayInfo() == Array)
-        Access->setAccessRelation(
-            Access->getAccessRelation().apply_range(Transform));
+    for (auto &Access : scop->access_functions()) {
+      if (Access->getScopArrayInfo() != Array)
+        continue;
+
+      isl::map AccessRel = Access->getAccessRelation();
+      if (AccessRel.is_null())
+        continue;
+
+      isl::map NewRel = AccessRel.apply_range(Transform);
+      if (!NewRel.is_null())
+        Access->setAccessRelation(NewRel);
+    }
 
     std::vector<const SCEV *> Sizes;
     for (unsigned i = 0; i < NumDims; i++) {
@@ -2518,65 +2736,101 @@ combineReductionType(MemoryAccess::Reduc
   return MemoryAccess::RT_NONE;
 }
 
-///  True if @p AllAccs intersects with @p MemAccs except @p LoadMA and @p
-///  StoreMA
-bool hasIntersectingAccesses(isl::set AllAccs, MemoryAccess *LoadMA,
-                             MemoryAccess *StoreMA, isl::set Domain,
-                             SmallVector<MemoryAccess *, 8> &MemAccs) {
-  bool HasIntersectingAccs = false;
-  auto AllAccsNoParams = AllAccs.project_out_all_params();
+/// True if @p AllAccs intersects with @p MemAccs except @p LoadMA and @p
+/// StoreMA
+static bool hasIntersectingAccesses(isl::set AllAccs, MemoryAccess *LoadMA,
+                                    MemoryAccess *StoreMA, isl::set Domain,
+                                    SmallVector<MemoryAccess *, 8> &MemAccs) {
+  if (AllAccs.is_null() || Domain.is_null())
+    return false;
+
+  // Cache the projection - avoid recomputing for each MA
+  isl::set AllAccsNoParams = AllAccs.project_out_all_params();
+  if (AllAccsNoParams.is_null())
+    return false;
 
   for (MemoryAccess *MA : MemAccs) {
     if (MA == LoadMA || MA == StoreMA)
       continue;
-    auto AccRel = MA->getAccessRelation().intersect_domain(Domain);
-    auto Accs = AccRel.range();
-    auto AccsNoParams = Accs.project_out_all_params();
 
-    bool CompatibleSpace = AllAccsNoParams.has_equal_space(AccsNoParams);
+    isl::map AccRel = MA->getAccessRelation();
+    if (AccRel.is_null())
+      continue;
 
-    if (CompatibleSpace) {
-      auto OverlapAccs = Accs.intersect(AllAccs);
-      bool DoesIntersect = !OverlapAccs.is_empty();
-      HasIntersectingAccs |= DoesIntersect;
-    }
+    AccRel = AccRel.intersect_domain(Domain);
+    if (AccRel.is_null())
+      continue;
+
+    isl::set Accs = AccRel.range();
+    if (Accs.is_null())
+      continue;
+
+    isl::set AccsNoParams = Accs.project_out_all_params();
+    if (AccsNoParams.is_null())
+      continue;
+
+    if (!AllAccsNoParams.has_equal_space(AccsNoParams))
+      continue;
+
+    isl::set OverlapAccs = Accs.intersect(AllAccs);
+    if (!OverlapAccs.is_null() && !OverlapAccs.is_empty())
+      return true;
   }
-  return HasIntersectingAccs;
+  return false;
 }
 
-///  Test if the accesses of @p LoadMA and @p StoreMA can form a reduction
-bool checkCandidatePairAccesses(MemoryAccess *LoadMA, MemoryAccess *StoreMA,
-                                isl::set Domain,
-                                SmallVector<MemoryAccess *, 8> &MemAccs) {
+/// Test if the accesses of @p LoadMA and @p StoreMA can form a reduction
+static bool checkCandidatePairAccesses(MemoryAccess *LoadMA,
+                                       MemoryAccess *StoreMA, isl::set Domain,
+                                       SmallVector<MemoryAccess *, 8> &MemAccs) {
+  if (!LoadMA || !StoreMA || Domain.is_null())
+    return false;
+
   // First check if the base value is the same.
   isl::map LoadAccs = LoadMA->getAccessRelation();
   isl::map StoreAccs = StoreMA->getAccessRelation();
+
+  if (LoadAccs.is_null() || StoreAccs.is_null())
+    return false;
+
   bool Valid = LoadAccs.has_equal_space(StoreAccs);
   POLLY_DEBUG(dbgs() << " == The accessed space below is "
                      << (Valid ? "" : "not ") << "equal!\n");
   POLLY_DEBUG(LoadMA->dump(); StoreMA->dump());
 
   if (Valid) {
-    // Then check if they actually access the same memory.
-    isl::map R = isl::manage(LoadAccs.copy())
-                     .intersect_domain(isl::manage(Domain.copy()));
-    isl::map W = isl::manage(StoreAccs.copy())
-                     .intersect_domain(isl::manage(Domain.copy()));
+    isl::map R = LoadAccs.intersect_domain(Domain);
+    isl::map W = StoreAccs.intersect_domain(Domain);
+
+    if (R.is_null() || W.is_null())
+      return false;
+
     isl::set RS = R.range();
     isl::set WS = W.range();
 
-    isl::set InterAccs =
-        isl::manage(RS.copy()).intersect(isl::manage(WS.copy()));
-    Valid = !InterAccs.is_empty();
+    if (RS.is_null() || WS.is_null())
+      return false;
+
+    isl::set InterAccs = RS.intersect(WS);
+    Valid = !InterAccs.is_null() && !InterAccs.is_empty();
     POLLY_DEBUG(dbgs() << " == The accessed memory is " << (Valid ? "" : "not ")
                        << "overlapping!\n");
   }
 
   if (Valid) {
-    // Finally, check if they are no other instructions accessing this memory
+    // Finally, check if there are no other instructions accessing this memory
     isl::map AllAccsRel = LoadAccs.unite(StoreAccs);
+    if (AllAccsRel.is_null())
+      return false;
+
     AllAccsRel = AllAccsRel.intersect_domain(Domain);
+    if (AllAccsRel.is_null())
+      return false;
+
     isl::set AllAccs = AllAccsRel.range();
+    if (AllAccs.is_null())
+      return false;
+
     Valid = !hasIntersectingAccesses(AllAccs, LoadMA, StoreMA, Domain, MemAccs);
     POLLY_DEBUG(dbgs() << " == The accessed memory is " << (Valid ? "not " : "")
                        << "accessed by other instructions!\n");
@@ -2830,10 +3084,10 @@ static bool isAccessRangeTooComplex(isl:
   for (isl::basic_set BSet : AccessRange.get_basic_set_list()) {
     NumTotalDims += unsignedFromIslSize(BSet.dim(isl::dim::div));
     NumTotalDims += unsignedFromIslSize(BSet.dim(isl::dim::set));
-  }
 
-  if (NumTotalDims > MaxDimensionsInAccessRange)
-    return true;
+    if (NumTotalDims > MaxDimensionsInAccessRange)
+      return true;
+  }
 
   return false;
 }
@@ -3239,7 +3493,8 @@ void ScopBuilder::buildAccessRelations(S
 /// Add the minimal/maximal access in @p Set to @p User.
 ///
 /// @return True if more accesses should be added, false if we reached the
-///         maximal number of run-time checks to be generated.
+///         maximal number of run-time checks to be generated or an error
+///         occurred.
 static bool buildMinMaxAccess(isl::set Set,
                               Scop::MinMaxVectorTy &MinMaxAccesses, Scop &S) {
   isl::pw_multi_aff MinPMA, MaxPMA;
@@ -3248,10 +3503,22 @@ static bool buildMinMaxAccess(isl::set S
   unsigned Pos;
 
   Set = Set.remove_divs();
+  if (Set.is_null())
+    return false;
+
   polly::simplify(Set);
+  if (Set.is_null())
+    return false;
+
+  isl::size NumBasicSets = Set.n_basic_set();
+  if (NumBasicSets.is_error())
+    return false;
 
-  if (unsignedFromIslSize(Set.n_basic_set()) > RunTimeChecksMaxAccessDisjuncts)
+  if (unsignedFromIslSize(NumBasicSets) > RunTimeChecksMaxAccessDisjuncts) {
     Set = Set.simple_hull();
+    if (Set.is_null())
+      return false;
+  }
 
   // Restrict the number of parameters involved in the access as the lexmin/
   // lexmax computation will take too long if this number is high.
@@ -3267,12 +3534,16 @@ static bool buildMinMaxAccess(isl::set S
   //           11          |     6.78
   //           12          |    30.38
   //
-  if (isl_set_n_param(Set.get()) >
-      static_cast<isl_size>(RunTimeChecksMaxParameters)) {
+  isl_size NumParams = isl_set_n_param(Set.get());
+  if (NumParams < 0)
+    return false;
+
+  if (NumParams > static_cast<isl_size>(RunTimeChecksMaxParameters)) {
     unsigned InvolvedParams = 0;
-    for (unsigned u = 0, e = isl_set_n_param(Set.get()); u < e; u++)
+    for (unsigned u = 0, e = static_cast<unsigned>(NumParams); u < e; u++) {
       if (Set.involves_dims(isl::dim::param, u, 1))
         InvolvedParams++;
+    }
 
     if (InvolvedParams > RunTimeChecksMaxParameters)
       return false;
@@ -3281,13 +3552,20 @@ static bool buildMinMaxAccess(isl::set S
   MinPMA = Set.lexmin_pw_multi_aff();
   MaxPMA = Set.lexmax_pw_multi_aff();
 
+  if (MinPMA.is_null() || MaxPMA.is_null())
+    return false;
+
   MinPMA = MinPMA.coalesce();
   MaxPMA = MaxPMA.coalesce();
 
-  if (MaxPMA.is_null())
+  if (MinPMA.is_null() || MaxPMA.is_null())
+    return false;
+
+  isl::size MaxOutputDimSize = MaxPMA.dim(isl::dim::out);
+  if (MaxOutputDimSize.is_error())
     return false;
 
-  unsigned MaxOutputSize = unsignedFromIslSize(MaxPMA.dim(isl::dim::out));
+  unsigned MaxOutputSize = unsignedFromIslSize(MaxOutputDimSize);
 
   // Adjust the last dimension of the maximal access by one as we want to
   // enclose the accessed memory region by MinPMA and MaxPMA. The pointer
@@ -3297,12 +3575,31 @@ static bool buildMinMaxAccess(isl::set S
 
   Pos = MaxOutputSize - 1;
   LastDimAff = MaxPMA.at(Pos);
-  OneAff = isl::aff(isl::local_space(LastDimAff.get_domain_space()));
+  if (LastDimAff.is_null())
+    return false;
+
+  isl::space DomainSpace = LastDimAff.get_domain_space();
+  if (DomainSpace.is_null())
+    return false;
+
+  isl::local_space LS(DomainSpace);
+  if (LS.is_null())
+    return false;
+
+  OneAff = isl::aff(LS);
+  if (OneAff.is_null())
+    return false;
+
   OneAff = OneAff.add_constant_si(1);
+  if (OneAff.is_null())
+    return false;
+
   LastDimAff = LastDimAff.add(OneAff);
-  MaxPMA = MaxPMA.set_pw_aff(Pos, LastDimAff);
+  if (LastDimAff.is_null())
+    return false;
 
-  if (MinPMA.is_null() || MaxPMA.is_null())
+  MaxPMA = MaxPMA.set_pw_aff(Pos, LastDimAff);
+  if (MaxPMA.is_null())
     return false;
 
   MinMaxAccesses.push_back(std::make_pair(MinPMA, MaxPMA));
@@ -3520,11 +3817,20 @@ bool ScopBuilder::buildAliasGroup(
 }
 
 void ScopBuilder::splitAliasGroupsByDomain(AliasGroupVectorTy &AliasGroups) {
-  for (unsigned u = 0; u < AliasGroups.size(); u++) {
+  // Only process the original groups - new groups added during iteration
+  // will be naturally formed from splits, so no need to process them again.
+  const size_t OriginalSize = AliasGroups.size();
+
+  for (size_t u = 0; u < OriginalSize; u++) {
     AliasGroupTy NewAG;
     AliasGroupTy &AG = AliasGroups[u];
+
+    if (AG.empty())
+      continue;
+
     AliasGroupTy::iterator AGI = AG.begin();
     isl::set AGDomain = getAccessDomain(*AGI);
+
     while (AGI != AG.end()) {
       MemoryAccess *MA = *AGI;
       isl::set MADomain = getAccessDomain(MA);
@@ -3533,9 +3839,10 @@ void ScopBuilder::splitAliasGroupsByDoma
         AGI = AG.erase(AGI);
       } else {
         AGDomain = AGDomain.unite(MADomain);
-        AGI++;
+        ++AGI;
       }
     }
+
     if (NewAG.size() > 1)
       AliasGroups.push_back(std::move(NewAG));
   }

--- a/polly/lib/Pass/PhaseManager.cpp	2025-11-16 16:34:09.155390732 +0100
+++ b/polly/lib/Pass/PhaseManager.cpp	2025-11-16 16:35:44.485198412 +0100
@@ -68,9 +68,9 @@ public:
     // These must be preserved during all phases so that if processing one SCoP
     // has finished, the next SCoP can still use them. Recomputing is not an
     // option because ScopDetection stores references to the old results.
-    // TODO: CodePreparation doesn't actually need these analysis, it just keeps
-    // them up-to-date. If they are not computed yet, can also compute after the
-    // prepare phase.
+    // TODO: CodePreparation doesn't actually need these analyses, it just keeps
+    // them up-to-date. If they are not computed yet, they can also be computed
+    // after the prepare phase.
     LoopInfo &LI = FAM.getResult<LoopAnalysis>(F);
     DominatorTree &DT = FAM.getResult<DominatorTreeAnalysis>(F);
     bool ModifiedIR = false;
@@ -88,9 +88,10 @@ public:
       }
     }
 
-    // Can't do anything without detection
+    // Can't do anything SCoP-related without detection, but we may already have
+    // modified the IR in the prepare phase.
     if (!Opts.isPhaseEnabled(PassPhase::Detection))
-      return false;
+      return ModifiedIR;
 
     AAResults &AA = FAM.getResult<AAManager>(F);
     ScalarEvolution &SE = FAM.getResult<ScalarEvolutionAnalysis>(F);
@@ -104,6 +105,7 @@ public:
     // Phase: detection
     ScopDetection SD(DT, SE, LI, RI, AA, ORE);
     SD.detect(F);
+
     if (Opts.isPhaseEnabled(PassPhase::PrintDetect)) {
       outs() << "Detected Scops in Function " << F.getName() << "\n";
       for (const Region *R : SD.ValidRegions)
@@ -116,28 +118,48 @@ public:
     if (Opts.isPhaseEnabled(PassPhase::DotScopsOnly))
       printGraphForFunction(F, &SD, "scopsonly", true);
 
-    auto ViewScops = [&](const char *Name, bool IsSimply) {
-      if (Opts.ViewFilter.empty() && !F.getName().count(Opts.ViewFilter))
+    auto ViewScops = [&](const char *Name, bool IsSimple) {
+      // If a view filter is specified, only visualize functions whose name
+      // contains the filter substring.
+      if (!Opts.ViewFilter.empty() &&
+          !F.getName().contains(Opts.ViewFilter))
         return;
 
-      if (Opts.ViewAll || std::distance(SD.begin(), SD.end()) > 0)
-        viewGraphForFunction(F, &SD, Name, IsSimply);
+      bool HasScops = !SD.ValidRegions.empty();
+      if (Opts.ViewAll || HasScops)
+        viewGraphForFunction(F, &SD, Name, IsSimple);
     };
+
     if (Opts.isPhaseEnabled(PassPhase::ViewScops))
       ViewScops("scops", false);
     if (Opts.isPhaseEnabled(PassPhase::ViewScopsOnly))
       ViewScops("scopsonly", true);
 
+    // If no phase at or beyond ScopInfo is enabled, we do not need to build
+    // ScopInfo or execute any SCoP-level pipeline.
+    bool NeedScops = false;
+    for (PassPhase P :
+         enum_seq_inclusive(PassPhase::ScopInfo, PassPhase::PassPhaseLast)) {
+      if (Opts.isPhaseEnabled(P)) {
+        NeedScops = true;
+        break;
+      }
+    }
+
+    if (!NeedScops)
+      return ModifiedIR;
+
     // Phase: scops
     AssumptionCache &AC = FAM.getResult<AssumptionAnalysis>(F);
     const DataLayout &DL = F.getParent()->getDataLayout();
     ScopInfo Info(DL, SD, SE, LI, AA, DT, AC, ORE);
+
     if (Opts.isPhaseEnabled(PassPhase::PrintScopInfo)) {
       if (Region *TLR = RI.getTopLevelRegion()) {
         SmallVector<Region *> Regions;
         addRegionIntoQueue(*TLR, Regions);
 
-        // reverse iteration because the regression tests expect it.
+        // Reverse iteration because the regression tests expect it.
         for (Region *R : reverse(Regions)) {
           Scop *S = Info.getScop(R);
           outs() << "Printing analysis 'Polly - Create polyhedral "
@@ -158,11 +180,12 @@ public:
         Worklist.insert(R);
 
     TargetTransformInfo &TTI = FAM.getResult<TargetIRAnalysis>(F);
+
     while (!Worklist.empty()) {
       Region *R = Worklist.pop_back_val();
       Scop *S = Info.getScop(R);
       if (!S) {
-        // This can happen if codegenning of a previous SCoP made this region
+        // This can happen if codegen of a previous SCoP made this region
         // not-a-SCoP anymore.
         POLLY_DEBUG(dbgs() << "SCoP in Region '" << *R << "' disappeared");
         continue;
@@ -211,7 +234,8 @@ public:
       // Phase: simplify-1
       // If we have already run simplify-0, do not re-run it if the SCoP has not
       // changed since then.
-      if (ModifiedSinceSimplify && Opts.isPhaseEnabled(PassPhase::Simplify1)) {
+      if (ModifiedSinceSimplify &&
+          Opts.isPhaseEnabled(PassPhase::Simplify1)) {
         runSimplify(*S, 1);
         ModifiedSinceSimplify = false;
       }
@@ -232,12 +256,12 @@ public:
       if (Opts.isPhaseEnabled(PassPhase::Optimization))
         runIslScheduleOptimizer(*S, &TTI, DA);
 
-      // Phase: import-jscop
+      // Phase: export-jscop
       if (Opts.isPhaseEnabled(PassPhase::ExportJScop))
         runExportJSON(*S);
 
       // Phase: ast
-      // Cannot run codegen unless ast is enabled
+      // Cannot run codegen unless ast is enabled.
       if (!Opts.isPhaseEnabled(PassPhase::AstGen))
         continue;
       std::unique_ptr<IslAstInfo> IslAst = runIslAstGen(*S, DA);
@@ -250,8 +274,8 @@ public:
         ModifiedIR = true;
 
         // For all regions, create new polly::Scop objects because the old ones
-        // refere to invalidated LLVM-IR.
-        // FIXME: Adds all SCoPs again to statistics
+        // refer to invalidated LLVM-IR.
+        // FIXME: Adds all SCoPs again to statistics.
         Info.recompute();
       }
     }


--- a/polly/lib/Analysis/ScopDetection.cpp	2025-09-21 15:28:29.292203509 +0200
+++ b/polly/lib/Analysis/ScopDetection.cpp	2025-11-16 15:35:24.476840546 +0200
@@ -657,30 +657,49 @@ bool ScopDetection::isValidCFG(BasicBloc
   Region &CurRegion = Context.CurRegion;
 
   Instruction *TI = BB.getTerminator();
+  assert(TI && "BasicBlock must have a terminator");
 
-  if (AllowUnreachable && isa<UnreachableInst>(TI))
+  // If unreachable blocks are explicitly allowed, accept an unreachable
+  // terminator without further checks.
+  if (AllowUnreachable && isa<UnreachableInst>(TI)) {
     return true;
+  }
 
-  // Return instructions are only valid if the region is the top level region.
-  if (isa<ReturnInst>(TI) && CurRegion.isTopLevelRegion())
+  // Return instructions are only valid if the region is the top-level region.
+  if (isa<ReturnInst>(TI) && CurRegion.isTopLevelRegion()) {
     return true;
+  }
 
+  // Extract the condition from the terminator, if any.
+  // For unconditional branches and other terminators this may be null.
   Value *Condition = getConditionFromTerminator(TI);
 
-  if (!Condition)
-    return invalid<ReportInvalidTerminator>(Context, /*Assert=*/true, &BB);
+  if (!Condition) {
+    // Any terminator we cannot model via a condition is considered invalid
+    // for SCoP formation.
+    return invalid<ReportInvalidTerminator>(Context,
+                                            /*Assert=*/true, &BB);
+  }
 
-  // UndefValue is not allowed as condition.
-  if (isa<UndefValue>(Condition))
-    return invalid<ReportUndefCond>(Context, /*Assert=*/true, TI, &BB);
+  // UndefValue is not allowed as a condition.
+  if (isa<UndefValue>(Condition)) {
+    return invalid<ReportUndefCond>(Context,
+                                    /*Assert=*/true, TI, &BB);
+  }
 
-  if (BranchInst *BI = dyn_cast<BranchInst>(TI))
+  if (auto *BI = dyn_cast<BranchInst>(TI)) {
     return isValidBranch(BB, BI, Condition, IsLoopBranch, Context);
+  }
 
-  SwitchInst *SI = dyn_cast<SwitchInst>(TI);
-  assert(SI && "Terminator was neither branch nor switch");
+  if (auto *SI = dyn_cast<SwitchInst>(TI)) {
+    return isValidSwitch(BB, SI, Condition, IsLoopBranch, Context);
+  }
 
-  return isValidSwitch(BB, SI, Condition, IsLoopBranch, Context);
+  // Any other terminator (invoke, indirectbr, callbr, etc.) is not supported
+  // by the current SCoP detection logic. Instead of asserting, we gracefully
+  // reject the CFG at this point.
+  return invalid<ReportInvalidTerminator>(Context,
+                                          /*Assert=*/true, &BB);
 }
 
 bool ScopDetection::isValidCallInst(CallInst &CI,
@@ -1206,19 +1225,22 @@ bool ScopDetection::isValidMemoryAccess(
 
 bool ScopDetection::isValidInstruction(Instruction &Inst,
                                        DetectionContext &Context) {
-  for (auto &Op : Inst.operands()) {
-    auto *OpInst = dyn_cast<Instruction>(&Op);
-
-    if (!OpInst)
+  // Reject uses of values produced in error blocks, except in very constrained
+  // situations (PHI nodes whose users are all terminators). This maintains a
+  // clean separation between the "main" SCoP region and error-handling paths.
+  for (Use &Op : Inst.operands()) {
+    auto *OpInst = dyn_cast<Instruction>(Op.get());
+    if (!OpInst) {
       continue;
+    }
 
     if (isErrorBlock(*OpInst->getParent(), Context.CurRegion)) {
-      auto *PHI = dyn_cast<PHINode>(OpInst);
-      if (PHI) {
+      if (auto *PHI = dyn_cast<PHINode>(OpInst)) {
         for (User *U : PHI->users()) {
           auto *UI = dyn_cast<Instruction>(U);
-          if (!UI || !UI->isTerminator())
+          if (!UI || !UI->isTerminator()) {
             return false;
+          }
         }
       } else {
         return false;
@@ -1226,37 +1248,50 @@ bool ScopDetection::isValidInstruction(I
     }
   }
 
-  if (isa<LandingPadInst>(&Inst) || isa<ResumeInst>(&Inst))
+  // LandingPad and Resume are not supported in SCoPs.
+  if (isa<LandingPadInst>(&Inst) || isa<ResumeInst>(&Inst)) {
     return false;
+  }
 
-  // We only check the call instruction but not invoke instruction.
-  if (CallInst *CI = dyn_cast<CallInst>(&Inst)) {
-    if (isValidCallInst(*CI, Context))
+  // We only check the call instruction but not invoke instruction here.
+  if (auto *CI = dyn_cast<CallInst>(&Inst)) {
+    if (isValidCallInst(*CI, Context)) {
       return true;
+    }
 
-    return invalid<ReportFuncCall>(Context, /*Assert=*/true, &Inst);
+    return invalid<ReportFuncCall>(Context,
+                                   /*Assert=*/true, &Inst);
   }
 
+  // Instructions which do not read or write memory and are not allocas
+  // are always fine.
   if (!Inst.mayReadOrWriteMemory()) {
-    if (!isa<AllocaInst>(Inst))
+    if (!isa<AllocaInst>(Inst)) {
       return true;
+    }
 
-    return invalid<ReportAlloca>(Context, /*Assert=*/true, &Inst);
+    // Allocas are currently not supported inside SCoPs.
+    return invalid<ReportAlloca>(Context,
+                                 /*Assert=*/true, &Inst);
   }
 
-  // Check the access function.
+  // Check the access function for memory instructions.
   if (auto MemInst = MemAccInst::dyn_cast(Inst)) {
     Context.hasStores |= isa<StoreInst>(MemInst);
-    Context.hasLoads |= isa<LoadInst>(MemInst);
-    if (!MemInst.isSimple())
-      return invalid<ReportNonSimpleMemoryAccess>(Context, /*Assert=*/true,
-                                                  &Inst);
+    Context.hasLoads  |= isa<LoadInst>(MemInst);
+
+    if (!MemInst.isSimple()) {
+      return invalid<ReportNonSimpleMemoryAccess>(Context,
+                                                  /*Assert=*/true, &Inst);
+    }
 
     return isValidMemoryAccess(MemInst, Context);
   }
 
-  // We do not know this instruction, therefore we assume it is invalid.
-  return invalid<ReportUnknownInst>(Context, /*Assert=*/true, &Inst);
+  // Any other memory-reading/writing instruction we do not explicitly
+  // understand is conservatively treated as invalid for SCoPs.
+  return invalid<ReportUnknownInst>(Context,
+                                    /*Assert=*/true, &Inst);
 }
 
 /// Check whether @p L has exiting blocks.
@@ -1629,48 +1664,94 @@ void ScopDetection::findScops(Region &R)
 bool ScopDetection::allBlocksValid(DetectionContext &Context) {
   Region &CurRegion = Context.CurRegion;
 
+  // Phase 1: Validate loops that are (partially) contained in the region.
+  //
+  // We ensure that loops either:
+  //  - Are fully contained and structurally valid for SCoP detection, or
+  //  - Do not partially overlap the region.
   for (const BasicBlock *BB : CurRegion.blocks()) {
     Loop *L = LI.getLoopFor(BB);
-    if (L && L->getHeader() == BB) {
-      if (CurRegion.contains(L)) {
-        if (!isValidLoop(L, Context)) {
-          Context.IsInvalid = true;
-          if (!KeepGoing)
-            return false;
+    if (!L) {
+      continue;
+    }
+
+    // We only act when this basic block is the header of the loop.
+    if (L->getHeader() != BB) {
+      continue;
+    }
+
+    if (CurRegion.contains(L)) {
+      // The loop is fully contained in the region.
+      if (!isValidLoop(L, Context)) {
+        Context.IsInvalid = true;
+        if (!KeepGoing) {
+          return false;
+        }
+      }
+    } else {
+      // The loop is not fully contained. Ensure that none of its latches
+      // are inside the region. If they are, the loop partially overlaps the
+      // region, which we cannot handle.
+      SmallVector<BasicBlock *, 1> Latches;
+      L->getLoopLatches(Latches);
+
+      for (BasicBlock *Latch : Latches) {
+        if (CurRegion.contains(Latch)) {
+          return invalid<ReportLoopOnlySomeLatches>(Context,
+                                                    /*Assert=*/true, L);
         }
-      } else {
-        SmallVector<BasicBlock *, 1> Latches;
-        L->getLoopLatches(Latches);
-        for (BasicBlock *Latch : Latches)
-          if (CurRegion.contains(Latch))
-            return invalid<ReportLoopOnlySomeLatches>(Context, /*Assert=*/true,
-                                                      L);
       }
     }
   }
 
+  // Phase 2: Validate the CFG and instructions of each basic block.
   for (BasicBlock *BB : CurRegion.blocks()) {
-    bool IsErrorBlock = isErrorBlock(*BB, CurRegion);
+    const bool IsErrorBlock = isErrorBlock(*BB, CurRegion);
 
     // Also check exception blocks (and possibly register them as non-affine
     // regions). Even though exception blocks are not modeled, we use them
     // to forward-propagate domain constraints during ScopInfo construction.
-    if (!isValidCFG(*BB, false, IsErrorBlock, Context) && !KeepGoing)
+    if (!isValidCFG(*BB, /*IsLoopBranch=*/false, IsErrorBlock, Context) &&
+        !KeepGoing) {
       return false;
+    }
 
-    if (IsErrorBlock)
+    if (IsErrorBlock) {
+      // We treat error blocks specially; their internal instructions are not
+      // considered part of the SCoP proper.
       continue;
+    }
+
+    // Iterate all instructions except the terminator, which has already been
+    // validated by isValidCFG().
+    auto I = BB->begin();
+    auto E = BB->end();
+
+    // Defensive check: normally a basic block always has at least a terminator,
+    // so begin() != end(). However, if the IR is malformed, we avoid
+    // dereferencing end().
+    if (I == E) {
+      continue;
+    }
+
+    --E; // Exclude the terminator.
 
-    for (BasicBlock::iterator I = BB->begin(), E = --BB->end(); I != E; ++I)
+    for (; I != E; ++I) {
       if (!isValidInstruction(*I, Context)) {
         Context.IsInvalid = true;
-        if (!KeepGoing)
+        if (!KeepGoing) {
           return false;
+        }
       }
+    }
   }
 
-  if (!hasAffineMemoryAccesses(Context))
+  // Finally, check that all memory accesses in the region can either be
+  // represented as affine accesses or be handled according to our
+  // configuration (e.g., non-affine handling when allowed).
+  if (!hasAffineMemoryAccesses(Context)) {
     return false;
+  }
 
   return true;
 }

--- a/polly/lib/Analysis/DependenceInfo.cpp	2025-11-16 14:59:12.228534011 +0200
+++ b/polly/lib/Analysis/DependenceInfo.cpp	2025-11-16 15:16:14.364885868 +0200
@@ -313,9 +313,12 @@ static __isl_give isl_union_flow *buildF
 }
 
 void Dependences::calculateDependences(Scop &S) {
-  isl_union_map *Read, *MustWrite, *MayWrite, *ReductionTagMap;
-  isl_schedule *Schedule;
-  isl_union_set *TaggedStmtDomain;
+  isl_union_map *Read = nullptr;
+  isl_union_map *MustWrite = nullptr;
+  isl_union_map *MayWrite = nullptr;
+  isl_union_map *ReductionTagMap = nullptr;
+  isl_schedule *Schedule = nullptr;
+  isl_union_set *TaggedStmtDomain = nullptr;
 
   POLLY_DEBUG(dbgs() << "Scop: \n" << S << "\n");
 
@@ -334,7 +337,8 @@ void Dependences::calculateDependences(S
 
   if (!HasReductions) {
     isl_union_map_free(ReductionTagMap);
-    // Tag the schedule tree if we want fine-grain dependence info
+
+    // Tag the schedule tree if we want fine-grain dependence info.
     if (Level > AL_Statement) {
       auto TaggedMap =
           isl_union_set_unwrap(isl_union_set_copy(TaggedStmtDomain));
@@ -342,25 +346,28 @@ void Dependences::calculateDependences(S
       Schedule = isl_schedule_pullback_union_pw_multi_aff(Schedule, Tags);
     }
   } else {
-    isl_union_map *IdentityMap;
-    isl_union_pw_multi_aff *ReductionTags, *IdentityTags, *Tags;
+    isl_union_map *IdentityMap = nullptr;
+    isl_union_pw_multi_aff *ReductionTags = nullptr;
+    isl_union_pw_multi_aff *IdentityTags = nullptr;
+    isl_union_pw_multi_aff *Tags = nullptr;
 
-    // Extract Reduction tags from the combined access domains in the given
+    // Extract reduction tags from the combined access domains in the given
     // SCoP. The result is a map that maps each tagged element in the domain to
-    // the memory location it accesses. ReductionTags = {[Stmt[i] ->
-    // Array[f(i)]] -> Stmt[i] }
+    // the memory location it accesses:
+    //   ReductionTags = { [Stmt[i] -> Array[f(i)]] -> Stmt[i] }
     ReductionTags =
         isl_union_map_domain_map_union_pw_multi_aff(ReductionTagMap);
 
-    // Compute an identity map from each statement in domain to itself.
-    // IdentityTags = { [Stmt[i] -> Stmt[i] }
-    IdentityMap = isl_union_set_identity(isl_union_set_copy(TaggedStmtDomain));
+    // Compute an identity map from each statement in the domain to itself:
+    //   IdentityTags = { [Stmt[i] -> Stmt[i]] }
+    IdentityMap =
+        isl_union_set_identity(isl_union_set_copy(TaggedStmtDomain));
     IdentityTags = isl_union_pw_multi_aff_from_union_map(IdentityMap);
 
     Tags = isl_union_pw_multi_aff_union_add(ReductionTags, IdentityTags);
 
     // By pulling back Tags from Schedule, we have a schedule tree that can
-    // be used to compute normal dependences, as well as 'tagged' reduction
+    // be used to compute normal dependences as well as 'tagged' reduction
     // dependences.
     Schedule = isl_schedule_pullback_union_pw_multi_aff(Schedule, Tags);
   }
@@ -371,77 +378,25 @@ void Dependences::calculateDependences(S
               dbgs() << "Schedule: " << Schedule << "\n");
 
   isl_union_map *StrictWAW = nullptr;
+
   {
+    // Bound the number of ISL operations in this block.
     IslMaxOperationsGuard MaxOpGuard(IslCtx.get(), OptComputeOut);
 
     RAW = WAW = WAR = RED = nullptr;
-    isl_union_map *Write = isl_union_map_union(isl_union_map_copy(MustWrite),
-                                               isl_union_map_copy(MayWrite));
 
-    // We are interested in detecting reductions that do not have intermediate
-    // computations that are captured by other statements.
-    //
-    // Example:
-    // void f(int *A, int *B) {
-    //     for(int i = 0; i <= 100; i++) {
-    //
-    //            *-WAR (S0[i] -> S0[i + 1] 0 <= i <= 100)------------*
-    //            |                                                   |
-    //            *-WAW (S0[i] -> S0[i + 1] 0 <= i <= 100)------------*
-    //            |                                                   |
-    //            v                                                   |
-    //     S0:    *A += i; >------------------*-----------------------*
-    //                                        |
-    //         if (i >= 98) {          WAR (S0[i] -> S1[i]) 98 <= i <= 100
-    //                                        |
-    //     S1:        *B = *A; <--------------*
-    //         }
-    //     }
-    // }
-    //
-    // S0[0 <= i <= 100] has a reduction. However, the values in
-    // S0[98 <= i <= 100] is captured in S1[98 <= i <= 100].
-    // Since we allow free reordering on our reduction dependences, we need to
-    // remove all instances of a reduction statement that have data dependences
-    // originating from them.
-    // In the case of the example, we need to remove S0[98 <= i <= 100] from
-    // our reduction dependences.
-    //
-    // When we build up the WAW dependences that are used to detect reductions,
-    // we consider only **Writes that have no intermediate Reads**.
-    //
-    // `isl_union_flow_get_must_dependence` gives us dependences of the form:
-    // (sink <- must_source).
-    //
-    // It *will not give* dependences of the form:
-    // 1. (sink <- ... <- may_source <- ... <- must_source)
-    // 2. (sink <- ... <- must_source <- ... <- must_source)
-    //
-    // For a detailed reference on ISL's flow analysis, see:
-    // "Presburger Formulas and Polyhedral Compilation" - Approximate Dataflow
-    //  Analysis.
-    //
-    // Since we set "Write" as a must-source, "Read" as a may-source, and ask
-    // for must dependences, we get all Writes to Writes that **do not flow
-    // through a Read**.
-    //
-    // ScopInfo::checkForReductions makes sure that if something captures
-    // the reduction variable in the same basic block, then it is rejected
-    // before it is even handed here. This makes sure that there is exactly
-    // one read and one write to a reduction variable in a Statement.
-    // Example:
-    //     void f(int *sum, int A[N], int B[N]) {
-    //       for (int i = 0; i < N; i++) {
-    //         *sum += A[i]; < the store and the load is not tagged as a
-    //         B[i] = *sum;  < reduction-like access due to the overlap.
-    //       }
-    //     }
-
-    isl_union_flow *Flow = buildFlow(Write, Write, Read, nullptr, Schedule);
+    // Union of all write accesses.
+    isl_union_map *Write = isl_union_map_union(
+        isl_union_map_copy(MustWrite), isl_union_map_copy(MayWrite));
+
+    // StrictWAW: WAW dependences that do not flow through a Read.
+    isl_union_flow *Flow =
+        buildFlow(Write, Write, Read, nullptr, Schedule);
     StrictWAW = isl_union_flow_get_must_dependence(Flow);
     isl_union_flow_free(Flow);
 
     if (OptAnalysisType == VALUE_BASED_ANALYSIS) {
+      // RAW, WAW, WAR for value-based analysis.
       Flow = buildFlow(Read, MustWrite, MayWrite, nullptr, Schedule);
       RAW = isl_union_flow_get_may_dependence(Flow);
       isl_union_flow_free(Flow);
@@ -450,12 +405,12 @@ void Dependences::calculateDependences(S
       WAW = isl_union_flow_get_may_dependence(Flow);
       isl_union_flow_free(Flow);
 
-      // ISL now supports "kills" in approximate dataflow analysis, we can
-      // specify the MustWrite as kills, Read as source and Write as sink.
+      // WAR: using MustWrite as "kills".
       Flow = buildFlow(Write, nullptr, Read, MustWrite, Schedule);
       WAR = isl_union_flow_get_may_dependence(Flow);
       isl_union_flow_free(Flow);
     } else {
+      // RAW, WAR, WAW for memory-based over-approximation.
       Flow = buildFlow(Read, nullptr, Write, nullptr, Schedule);
       RAW = isl_union_flow_get_may_dependence(Flow);
       isl_union_flow_free(Flow);
@@ -475,40 +430,71 @@ void Dependences::calculateDependences(S
     isl_union_map_free(Read);
     isl_schedule_free(Schedule);
 
-    RAW = isl_union_map_coalesce(RAW);
-    WAW = isl_union_map_coalesce(WAW);
-    WAR = isl_union_map_coalesce(WAR);
-
+    // Coalesce only if the maps are non-null; if any operation failed,
+    // we will detect that after this guarded section.
+    if (RAW)
+      RAW = isl_union_map_coalesce(RAW);
+    if (WAW)
+      WAW = isl_union_map_coalesce(WAW);
+    if (WAR)
+      WAR = isl_union_map_coalesce(WAR);
     // End of max_operations scope.
   }
 
-  if (isl_ctx_last_error(IslCtx.get()) == isl_error_quota) {
+  isl_ctx *Ctx = IslCtx.get();
+
+  // Handle quota exhaustion explicitly: analysis did not complete.
+  if (isl_ctx_last_error(Ctx) == isl_error_quota) {
+    isl_union_map_free(RAW);
+    isl_union_map_free(WAW);
+    isl_union_map_free(WAR);
+    isl_union_map_free(StrictWAW);
+    RAW = WAW = WAR = StrictWAW = nullptr;
+
+    isl_union_set_free(TaggedStmtDomain);
+    TaggedStmtDomain = nullptr;
+
+    isl_ctx_reset_error(Ctx);
+    // Leave RED/TC_RED as nullptr; hasValidDependences() will report false.
+    return;
+  }
+
+  // If any of the core dependence maps failed to compute, treat the analysis
+  // as failed and stop here.
+  if (!RAW || !WAW || !WAR || !StrictWAW) {
     isl_union_map_free(RAW);
     isl_union_map_free(WAW);
     isl_union_map_free(WAR);
     isl_union_map_free(StrictWAW);
     RAW = WAW = WAR = StrictWAW = nullptr;
-    isl_ctx_reset_error(IslCtx.get());
+
+    isl_union_set_free(TaggedStmtDomain);
+    TaggedStmtDomain = nullptr;
+
+    isl_ctx_reset_error(Ctx);
+    return;
   }
 
-  // Drop out early, as the remaining computations are only needed for
-  // reduction dependences or dependences that are finer than statement
-  // level dependences.
+  // Drop out early if there are no reductions and we only need statement-level
+  // dependences. In this case reduction-specific information is not required.
   if (!HasReductions && Level == AL_Statement) {
     RED = isl_union_map_empty(isl_union_map_get_space(RAW));
-    TC_RED = isl_union_map_empty(isl_union_set_get_space(TaggedStmtDomain));
+    TC_RED =
+        isl_union_map_empty(isl_union_set_get_space(TaggedStmtDomain));
     isl_union_set_free(TaggedStmtDomain);
     isl_union_map_free(StrictWAW);
     return;
   }
 
-  isl_union_map *STMT_RAW, *STMT_WAW, *STMT_WAR;
-  STMT_RAW = isl_union_map_intersect_domain(
+  // Restrict dependences to the tagged statement domain for statement-level
+  // views (STMT_*).
+  isl_union_map *STMT_RAW = isl_union_map_intersect_domain(
       isl_union_map_copy(RAW), isl_union_set_copy(TaggedStmtDomain));
-  STMT_WAW = isl_union_map_intersect_domain(
+  isl_union_map *STMT_WAW = isl_union_map_intersect_domain(
       isl_union_map_copy(WAW), isl_union_set_copy(TaggedStmtDomain));
-  STMT_WAR =
-      isl_union_map_intersect_domain(isl_union_map_copy(WAR), TaggedStmtDomain);
+  isl_union_map *STMT_WAR = isl_union_map_intersect_domain(
+      isl_union_map_copy(WAR), TaggedStmtDomain);
+
   POLLY_DEBUG({
     dbgs() << "Wrapped Dependences:\n";
     dump();
@@ -516,47 +502,43 @@ void Dependences::calculateDependences(S
   });
 
   // To handle reduction dependences we proceed as follows:
-  // 1) Aggregate all possible reduction dependences, namely all self
-  //    dependences on reduction like statements.
-  // 2) Intersect them with the actual RAW & WAW dependences to the get the
-  //    actual reduction dependences. This will ensure the load/store memory
-  //    addresses were __identical__ in the two iterations of the statement.
-  // 3) Relax the original RAW, WAW and WAR dependences by subtracting the
-  //    actual reduction dependences. Binary reductions (sum += A[i]) cause
-  //    the same, RAW, WAW and WAR dependences.
-  // 4) Add the privatization dependences which are widened versions of
-  //    already present dependences. They model the effect of manual
-  //    privatization at the outermost possible place (namely after the last
-  //    write and before the first access to a reduction location).
+  // 1) Aggregate all possible reduction dependences: self-dependences on
+  //    reduction-like statements.
+  // 2) Intersect them with actual RAW & WAW dependences to get true reduction
+  //    dependences (identical memory addresses in the two iterations).
+  // 3) Relax RAW, WAW, and WAR dependences by subtracting the reduction deps.
+  // 4) Add privatization dependences, widened versions of reduction deps.
 
-  // Step 1)
+  // Step 1) Build candidate reduction dependences in wrapped access space.
   RED = isl_union_map_empty(isl_union_map_get_space(RAW));
   for (ScopStmt &Stmt : S) {
     for (MemoryAccess *MA : Stmt) {
       if (!MA->isReductionLike())
         continue;
-      isl_set *AccDomW = isl_map_wrap(MA->getAccessRelation().release());
-      isl_map *Identity =
-          isl_map_from_domain_and_range(isl_set_copy(AccDomW), AccDomW);
+
+      isl_set *AccDomW =
+          isl_map_wrap(MA->getAccessRelation().release());
+      isl_map *Identity = isl_map_from_domain_and_range(
+          isl_set_copy(AccDomW), AccDomW);
       RED = isl_union_map_add_map(RED, Identity);
     }
   }
 
-  // Step 2)
+  // Step 2) Intersect with actual RAW and StrictWAW to get true reductions.
   RED = isl_union_map_intersect(RED, isl_union_map_copy(RAW));
   RED = isl_union_map_intersect(RED, StrictWAW);
 
   if (!isl_union_map_is_empty(RED)) {
-
-    // Step 3)
+    // Step 3) Relax original dependences by subtracting reduction deps.
     RAW = isl_union_map_subtract(RAW, isl_union_map_copy(RED));
     WAW = isl_union_map_subtract(WAW, isl_union_map_copy(RED));
     WAR = isl_union_map_subtract(WAR, isl_union_map_copy(RED));
 
-    // Step 4)
+    // Step 4) Add privatization dependences (widened reduction deps).
     addPrivatizationDependences();
-  } else
+  } else {
     TC_RED = isl_union_map_empty(isl_union_map_get_space(RED));
+  }
 
   POLLY_DEBUG({
     dbgs() << "Final Wrapped Dependences:\n";
@@ -564,33 +546,35 @@ void Dependences::calculateDependences(S
     dbgs() << "\n";
   });
 
-  // RED_SIN is used to collect all reduction dependences again after we
-  // split them according to the causing memory accesses. The current assumption
-  // is that our method of splitting will not have any leftovers. In the end
-  // we validate this assumption until we have more confidence in this method.
-  isl_union_map *RED_SIN = isl_union_map_empty(isl_union_map_get_space(RAW));
-
-  // For each reduction like memory access, check if there are reduction
-  // dependences with the access relation of the memory access as a domain
-  // (wrapped space!). If so these dependences are caused by this memory access.
-  // We then move this portion of reduction dependences back to the statement ->
-  // statement space and add a mapping from the memory access to these
-  // dependences.
+  // RED_SIN is used to collect all reduction dependences again after we split
+  // them according to the causing memory accesses. The assumption is that our
+  // method of splitting will not have any leftovers. We validate this
+  // assumption here.
+  isl_union_map *RED_SIN =
+      isl_union_map_empty(isl_union_map_get_space(RAW));
+
+  // For each reduction-like memory access, check if there are reduction
+  // dependences with the access relation as domain (wrapped space!). If so,
+  // these are caused by this memory access. We then move this portion of
+  // reduction dependences back to statement->statement space and record it.
   for (ScopStmt &Stmt : S) {
     for (MemoryAccess *MA : Stmt) {
       if (!MA->isReductionLike())
         continue;
 
-      isl_set *AccDomW = isl_map_wrap(MA->getAccessRelation().release());
+      isl_set *AccDomW =
+          isl_map_wrap(MA->getAccessRelation().release());
       isl_union_map *AccRedDepU = isl_union_map_intersect_domain(
-          isl_union_map_copy(TC_RED), isl_union_set_from_set(AccDomW));
+          isl_union_map_copy(TC_RED),
+          isl_union_set_from_set(AccDomW));
       if (isl_union_map_is_empty(AccRedDepU)) {
         isl_union_map_free(AccRedDepU);
         continue;
       }
 
       isl_map *AccRedDep = isl_map_from_union_map(AccRedDepU);
-      RED_SIN = isl_union_map_add_map(RED_SIN, isl_map_copy(AccRedDep));
+      RED_SIN = isl_union_map_add_map(RED_SIN,
+                                      isl_map_copy(AccRedDep));
       AccRedDep = isl_map_zip(AccRedDep);
       AccRedDep = isl_set_unwrap(isl_map_domain(AccRedDep));
       setReductionDependences(MA, AccRedDep);
@@ -598,10 +582,10 @@ void Dependences::calculateDependences(S
   }
 
   assert(isl_union_map_is_equal(RED_SIN, TC_RED) &&
-         "Intersecting the reduction dependence domain with the wrapped access "
-         "relation is not enough, we need to loosen the access relation also");
+         "Splitting reduction dependences by access left unmatched parts");
   isl_union_map_free(RED_SIN);
 
+  // Zip dependences into [Domain -> Range] pairs.
   RAW = isl_union_map_zip(RAW);
   WAW = isl_union_map_zip(WAW);
   WAR = isl_union_map_zip(WAR);
@@ -614,6 +598,7 @@ void Dependences::calculateDependences(S
     dbgs() << "\n";
   });
 
+  // Unwrap to statement/access-level domains.
   RAW = isl_union_set_unwrap(isl_union_map_domain(RAW));
   WAW = isl_union_set_unwrap(isl_union_map_domain(WAW));
   WAR = isl_union_set_unwrap(isl_union_map_domain(WAR));
@@ -626,10 +611,12 @@ void Dependences::calculateDependences(S
     dbgs() << "\n";
   });
 
+  // Merge statement-only dependences back in.
   RAW = isl_union_map_union(RAW, STMT_RAW);
   WAW = isl_union_map_union(WAW, STMT_WAW);
   WAR = isl_union_map_union(WAR, STMT_WAR);
 
+  // Final coalesce.
   RAW = isl_union_map_coalesce(RAW);
   WAW = isl_union_map_coalesce(WAW);
   WAR = isl_union_map_coalesce(WAR);
@@ -799,8 +786,10 @@ void Dependences::releaseMemory() {
 
 isl::union_map Dependences::getDependences(int Kinds) const {
   assert(hasValidDependences() && "No valid dependences available");
+
+  // Use RAW's space as a representative for the overall dependence space.
   isl::space Space = isl::manage_copy(RAW).get_space();
-  isl::union_map Deps = Deps.empty(Space.ctx());
+  isl::union_map Deps = isl::union_map::empty(Space.ctx());
 
   if (Kinds & TYPE_RAW)
     Deps = Deps.unite(isl::manage_copy(RAW));

--- a/polly/lib/Transform/DeadCodeElimination.cpp	2025-09-21 17:32:16.072456710 +0200
+++ b/polly/lib/Transform/DeadCodeElimination.cpp	2025-09-21 17:32:47.327399217 +0200

--- a/polly/lib/Transform/DeLICM.cpp	2025-09-21 16:25:17.318367274 +0200
+++ b/polly/lib/Transform/DeLICM.cpp	2025-09-21 16:26:08.074761600 +0200

--- a/polly/lib/Transform/ForwardOpTree.cpp	2025-09-21 16:10:05.509588643 +0200
+++ b/polly/lib/Transform/ForwardOpTree.cpp	2025-11-16 16:14:13.166720281 +0200
@@ -359,20 +359,34 @@ public:
       IslQuotaScope QuotaScope = MaxOpGuard.enter();
 
       computeCommon();
-      if (NormalizePHIs)
+      if (NormalizePHIs) {
         computeNormalizedPHIs();
-      Known = computeKnown(true, true);
+      }
+
+      // Compute the map of known array contents.
+      Known = computeKnown(/*IncludeMustWrites=*/true,
+                          /*IncludeMayWrites=*/true);
 
       // Preexisting ValInsts use the known content analysis of themselves.
-      Translator = makeIdentityMap(Known.range(), false);
+      // Only build the identity translator if Known was successfully computed.
+      if (!Known.is_null()) {
+        Translator = makeIdentityMap(Known.range(), /*AlignParams=*/false);
+      }
     }
 
+    // If any of the core analysis maps are unusable, treat the analysis as failed.
     if (Known.is_null() || Translator.is_null() || NormalizeMap.is_null()) {
-      assert(isl_ctx_last_error(IslCtx.get()) == isl_error_quota);
+      // In practice this is expected to be a quota error, but in release builds
+      // we handle it gracefully regardless of the exact isl error code.
+      assert(isl_ctx_last_error(IslCtx.get()) == isl_error_quota &&
+            "Known analysis failed for a reason other than quota exhaustion");
+
       Known = {};
       Translator = {};
       NormalizeMap = {};
-      POLLY_DEBUG(dbgs() << "Known analysis exceeded max_operations\n");
+
+      POLLY_DEBUG(
+          dbgs() << "Known analysis exceeded max_operations or failed\n");
       return false;
     }
 
@@ -418,27 +432,37 @@ public:
   /// @param AccessRelation The array element that each statement instance
   ///                       accesses.
   ///
-  /// @param The newly created access.
+  /// @return The newly created access.
   MemoryAccess *makeReadArrayAccess(ScopStmt *Stmt, LoadInst *LI,
                                     isl::map AccessRelation) {
+    // The output tuple id of the access relation is expected to carry the
+    // ScopArrayInfo object as its user pointer.
     isl::id ArrayId = AccessRelation.get_tuple_id(isl::dim::out);
-    ScopArrayInfo *SAI = reinterpret_cast<ScopArrayInfo *>(ArrayId.get_user());
+    auto *SAI =
+        static_cast<ScopArrayInfo *>(ArrayId.get_user());
 
-    // Create a dummy SCEV access, to be replaced anyway.
+    assert(SAI && "AccessRelation output id must reference a ScopArrayInfo");
+
+    // Create a dummy SCEV access, to be replaced by the finalized isl access
+    // relation. We initialize the Sizes with the array's dimension sizes as
+    // expected by MemoryAccess.
     SmallVector<const SCEV *, 4> Sizes;
     Sizes.reserve(SAI->getNumberOfDimensions());
-    SmallVector<const SCEV *, 4> Subscripts;
-    Subscripts.reserve(SAI->getNumberOfDimensions());
-    for (unsigned i = 0; i < SAI->getNumberOfDimensions(); i += 1) {
+
+    for (unsigned i = 0, e = SAI->getNumberOfDimensions(); i != e; ++i) {
       Sizes.push_back(SAI->getDimensionSize(i));
-      Subscripts.push_back(nullptr);
     }
 
-    MemoryAccess *Access =
-        new MemoryAccess(Stmt, LI, MemoryAccess::READ, SAI->getBasePtr(),
-                         LI->getType(), true, {}, Sizes, LI, MemoryKind::Array);
+    // Create a new array read MemoryAccess. We pass an empty access function
+    // (to be defined by the isl relation) and the dimension Sizes. The LI is
+    // used as the access instruction and debug reference.
+    MemoryAccess *Access = new MemoryAccess(
+        Stmt, LI, MemoryAccess::READ, SAI->getBasePtr(), LI->getType(),
+        /*IsAffine=*/true,
+        /*AccessFunction=*/{}, Sizes, LI, MemoryKind::Array);
+
     S->addAccessFunction(Access);
-    Stmt->addAccess(Access, true);
+    Stmt->addAccess(Access, /*IsNew=*/true);
 
     Access->setNewAccessRelation(AccessRelation);
 
--- a/polly/lib/Transform/ScheduleOptimizer.cpp	2025-07-13 23:25:57.922600146 +0200
+++ b/polly/lib/Transform/ScheduleOptimizer.cpp	2025-07-13 23:27:47.918372394 +0200
