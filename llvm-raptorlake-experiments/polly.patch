--- a/polly/lib/Analysis/ScopDetection.cpp	2025-07-27 20:25:04.400923963 +0200
+++ b/polly/lib/Analysis/ScopDetection.cpp	2025-07-27 20:28:47.092854938 +0200
@@ -104,6 +104,34 @@ static cl::opt<int> ProfitabilityMinPerL
              "region is considered profitable"),
     cl::Hidden, cl::ValueRequired, cl::init(100000000), cl::cat(PollyCategory));
 
+// PERFECTED CODE INTEGRATION: Add new cl::opt definitions for perfected heuristics.
+// These are unsigned to match the semantics of the values they are compared
+// against, preventing signedness warnings and ensuring correctness.
+
+static cl::opt<unsigned> PollyScopsMaxBlocks(
+    "polly-scops-max-blocks",
+    cl::desc("The maximum number of basic blocks in a function to be "
+             "considered for Scop detection by Polly"),
+    cl::Hidden, cl::init(20000), cl::cat(PollyCategory));
+
+static cl::opt<unsigned> PollyMaxSubRegions(
+    "polly-max-subregions",
+    cl::desc("The maximum number of sub-regions in a region to be "
+             "considered for Scop detection by Polly"),
+    cl::Hidden, cl::init(512), cl::cat(PollyCategory));
+
+static cl::opt<unsigned> PollyMinMemops(
+    "polly-min-memops",
+    cl::desc("The minimal number of memory operations in a single-loop Scop "
+             "to be considered profitable"),
+    cl::Hidden, cl::ValueRequired, cl::init(8), cl::cat(PollyCategory));
+
+static cl::opt<unsigned> PollyMaxSwitchCases(
+    "polly-max-switch-cases",
+    cl::desc("The maximal number of cases in a switch statement to be "
+             "considered for Scop detection by Polly"),
+    cl::Hidden, cl::init(32), cl::cat(PollyCategory));
+
 bool polly::PollyProcessUnprofitable;
 
 static cl::opt<bool, true> XPollyProcessUnprofitable(
@@ -348,8 +376,6 @@ void ScopDetection::detect(Function &F)
   if (!PollyProcessUnprofitable && LI.empty())
     return;
 
-  Region *TopRegion = RI.getTopLevelRegion();
-
   if (!OnlyFunctions.empty() &&
       !doesStringMatchAnyRegex(F.getName(), OnlyFunctions))
     return;
@@ -360,6 +386,7 @@ void ScopDetection::detect(Function &F)
   if (!isValidFunction(F))
     return;
 
+  Region *TopRegion = RI.getTopLevelRegion();
   findScops(*TopRegion);
 
   NumScopRegions += ValidRegions.size();
@@ -1301,61 +1328,62 @@ bool ScopDetection::canUseISLTripCount(L
   return true;
 }
 
+// PERFECTED CODE INTEGRATION: Replace isValidLoop with the perfected version.
 bool ScopDetection::isValidLoop(Loop *L, DetectionContext &Context) {
-  // Loops that contain part but not all of the blocks of a region cannot be
-  // handled by the schedule generation. Such loop constructs can happen
-  // because a region can contain BBs that have no path to the exit block
-  // (Infinite loops, UnreachableInst), but such blocks are never part of a
-  // loop.
-  //
-  // _______________
-  // | Loop Header | <-----------.
-  // ---------------             |
-  //        |                    |
-  // _______________       ______________
-  // | RegionEntry |-----> | RegionExit |----->
-  // ---------------       --------------
-  //        |
-  // _______________
-  // | EndlessLoop | <--.
-  // ---------------    |
-  //       |            |
-  //       \------------/
-  //
-  // In the example above, the loop (LoopHeader,RegionEntry,RegionExit) is
-  // neither entirely contained in the region RegionEntry->RegionExit
-  // (containing RegionEntry,EndlessLoop) nor is the region entirely contained
-  // in the loop.
-  // The block EndlessLoop is contained in the region because Region::contains
-  // tests whether it is not dominated by RegionExit. This is probably to not
-  // having to query the PostdominatorTree. Instead of an endless loop, a dead
-  // end can also be formed by an UnreachableInst. This case is already caught
-  // by isErrorBlock(). We hence only have to reject endless loops here.
-  if (!hasExitingBlocks(L))
+  // Loops must have at least one reachable exit to be analyzable.
+  if (!hasExitingBlocks(L)) {
     return invalid<ReportLoopHasNoExit>(Context, /*Assert=*/true, L);
+  }
 
-  // The algorithm for domain construction assumes that loops has only a single
-  // exit block (and hence corresponds to a subregion). Note that we cannot use
-  // L->getExitBlock() because it does not check whether all exiting edges point
-  // to the same BB.
+  // Perfected Initiative 3: Speculative Analysis of Loops with Error-Path Exits.
+  // The polyhedral model requires a single "hot-path" exit to define loop
+  // bounds cleanly. However, production code (especially in drivers) is full of
+  // loops with a main hot-path exit and several cold-path error-checking exits.
+  // We relax the single-exit constraint by speculatively ignoring any exit that
+  // leads to a well-defined "error block".
   SmallVector<BasicBlock *, 4> ExitBlocks;
   L->getExitBlocks(ExitBlocks);
-  BasicBlock *TheExitBlock = ExitBlocks[0];
+  BasicBlock *TheHotPathExitBlock = nullptr;
+
   for (BasicBlock *ExitBB : ExitBlocks) {
-    if (TheExitBlock != ExitBB)
-      return invalid<ReportLoopHasMultipleExits>(Context, /*Assert=*/true, L);
+    // An error block is considered a cold path and is ignored for the purpose
+    // of defining the loop's primary control flow.
+    if (isErrorBlock(*ExitBB, Context.CurRegion)) {
+      continue;
+    }
+
+    // If we have already found one hot-path exit, finding another means this
+    // loop has ambiguous control flow on its hot path and cannot be modeled.
+    if (TheHotPathExitBlock) {
+      if (TheHotPathExitBlock != ExitBB) {
+        return invalid<ReportLoopHasMultipleExits>(Context, /*Assert=*/true, L);
+      }
+    } else {
+      TheHotPathExitBlock = ExitBB;
+    }
+  }
+
+  // If we found no hot-path exits, the loop's only exits lead to
+  // error conditions. On the hot path, it is effectively an infinite loop, which
+  // we cannot model.
+  if (!TheHotPathExitBlock) {
+    return invalid<ReportLoopHasMultipleExits>(Context, /*Assert=*/true, L);
   }
 
-  if (canUseISLTripCount(L, Context))
+  // With a single, well-defined hot-path exit, we can now proceed with the
+  // standard affine analysis of the loop's trip count.
+  if (canUseISLTripCount(L, Context)) {
     return true;
+  }
 
   if (AllowNonAffineSubLoops && AllowNonAffineSubRegions) {
     Region *R = RI.getRegionFor(L->getHeader());
-    while (R != &Context.CurRegion && !R->contains(L))
+    while (R != &Context.CurRegion && !R->contains(L)) {
       R = R->getParent();
-
-    if (addOverApproximatedRegion(R, Context))
+    }
+    if (addOverApproximatedRegion(R, Context)) {
       return true;
+    }
   }
 
   const SCEV *LoopCount = SE.getBackedgeTakenCount(L);
@@ -1571,10 +1599,11 @@ void ScopDetection::findScops(Region &R)
   DetectionContext &Context = *Entry;
 
   bool DidBailout = true;
-  if (!PollyProcessUnprofitable && regionWithoutLoops(R, LI))
+  if (!PollyProcessUnprofitable && regionWithoutLoops(R, LI)) {
     invalid<ReportUnprofitable>(Context, /*Assert=*/true, &R);
-  else
+  } else {
     DidBailout = !isValidRegion(Context);
+  }
 
   (void)DidBailout;
   if (KeepGoing) {
@@ -1586,42 +1615,59 @@ void ScopDetection::findScops(Region &R)
            "isValidRegion must short-circuit iff the ScoP is invalid");
   }
 
-  if (Context.IsInvalid) {
-    removeCachedResults(R);
-  } else {
+  // If the current region is a valid Scop, we mark it and do not need to
+  // check its sub-regions. We will try to expand it later.
+  if (!Context.IsInvalid) {
     ValidRegions.insert(&R);
     return;
   }
 
-  for (auto &SubRegion : R)
+  removeCachedResults(R);
+
+  // If the region is invalid, we now check its sub-regions.
+  // This is the location of the potential recursive explosion.
+
+  std::vector<Region *> RegionsToExpand;
+  unsigned SubRegionCounter = 0;
+
+  for (auto &SubRegion : R) {
+    // 1. Count: Check if we have exceeded the sub-region limit.
+    if (++SubRegionCounter > PollyMaxSubRegions) {
+      POLLY_DEBUG(dbgs() << "Polly: Region '" << R.getNameStr()
+                        << "' has too many sub-regions (>" << PollyMaxSubRegions
+                        << "). Skipping recursion to prevent stack overflow.\n");
+      // If the limit is exceeded, we must not continue. We cannot recurse
+      // further, and we cannot try to expand, as we haven't processed all
+      // sub-regions. Bail out of all processing for this region.
+      return;
+    }
+
+    // 2. Recurse: If the limit is not hit, perform the recursive analysis.
     findScops(*SubRegion);
 
-  // Try to expand regions.
-  //
-  // As the region tree normally only contains canonical regions, non canonical
-  // regions that form a Scop are not found. Therefore, those non canonical
-  // regions are checked by expanding the canonical ones.
-
-  std::vector<Region *> ToExpand;
-
-  for (auto &SubRegion : R)
-    ToExpand.push_back(SubRegion.get());
-
-  for (Region *CurrentRegion : ToExpand) {
-    // Skip invalid regions. Regions may become invalid, if they are element of
-    // an already expanded region.
-    if (!ValidRegions.count(CurrentRegion))
+    // 3. Collect: Add the sub-region to the list for a potential expansion
+    //    attempt later. This avoids a second or third loop over the children.
+    RegionsToExpand.push_back(SubRegion.get());
+  }
+
+  // After successfully analyzing all sub-regions (without hitting the limit),
+  // we can now try to expand any valid Scops we found within them.
+  for (Region *CurrentRegion : RegionsToExpand) {
+    // Skip sub-regions that were themselves (or contained) no valid Scops.
+    if (!ValidRegions.count(CurrentRegion)) {
       continue;
+    }
 
-    // Skip regions that had errors.
-    bool HadErrors = lookupRejectionLog(CurrentRegion)->hasErrors();
-    if (HadErrors)
+    // Skip regions that had analysis errors.
+    if (lookupRejectionLog(CurrentRegion)->hasErrors()) {
       continue;
+    }
 
     Region *ExpandedR = expandRegion(*CurrentRegion);
 
-    if (!ExpandedR)
+    if (!ExpandedR) {
       continue;
+    }
 
     R.addSubRegion(ExpandedR, true);
     ValidRegions.insert(ExpandedR);
@@ -1717,38 +1763,78 @@ bool ScopDetection::hasPossiblyDistribut
   return false;
 }
 
+// PERFECTED CODE INTEGRATION: Replace isProfitableRegion with the perfected version.
 bool ScopDetection::isProfitableRegion(DetectionContext &Context) const {
   Region &CurRegion = Context.CurRegion;
 
-  if (PollyProcessUnprofitable)
+  if (PollyProcessUnprofitable) {
     return true;
+  }
 
-  // We can probably not do a lot on scops that only write or only read
-  // data.
-  if (!Context.hasStores || !Context.hasLoads)
+  // A Scop without any main memory operations is purely computational. Polly's
+  // primary strengths are in transforming memory access patterns. For pure
+  // arithmetic, it is unlikely to outperform standard LLVM loop optimizations.
+  // This is the first and cheapest profitability check.
+  if (!Context.hasStores && !Context.hasLoads) {
     return invalid<ReportUnprofitable>(Context, /*Assert=*/true, &CurRegion);
+  }
 
   int NumLoops =
       countBeneficialLoops(&CurRegion, SE, LI, MIN_LOOP_TRIP_COUNT).NumLoops;
   int NumAffineLoops = NumLoops - Context.BoxedLoopsSet.size();
 
-  // Scops with at least two loops may allow either loop fusion or tiling and
-  // are consequently interesting to look at.
-  if (NumAffineLoops >= 2)
-    return true;
-
-  // A loop with multiple non-trivial blocks might be amendable to distribution.
-  if (NumAffineLoops == 1 && hasPossiblyDistributableLoop(Context))
-    return true;
-
-  // Scops that contain a loop with a non-trivial amount of computation per
-  // loop-iteration are interesting as we may be able to parallelize such
-  // loops. Individual loops that have only a small amount of computation
-  // per-iteration are performance-wise very fragile as any change to the
-  // loop induction variables may affect performance. To not cause spurious
-  // performance regressions, we do not consider such loops.
-  if (NumAffineLoops == 1 && hasSufficientCompute(Context, NumLoops))
+  // Perfected Initiative 4: Aggressively target nested loops.
+  // Scops with at least two affine loops are the highest-value targets. The
+  // potential for loop fusion or tiling for cache locality is immense, which is
+  // critical for modern deep cache hierarchies (like on Raptor Lake). This holds
+  // true even for read-only Scops, which benefit greatly from improved data locality.
+  if (NumAffineLoops >= 2) {
     return true;
+  }
+
+  // For single-loop Scops, the profitability calculation is more nuanced.
+  if (NumAffineLoops == 1) {
+    // A single loop with multiple non-trivial blocks might be amendable to loop
+    // distribution. This is a strong positive heuristic as it can enable
+    // better vectorization on the separated loops.
+    if (hasPossiblyDistributableLoop(Context)) {
+      return true;
+    }
+
+    // Perfected Initiative 2: Memory-centric heuristic for single loops.
+    // Single loops that only read data are less likely to be profitable as tiling
+    // is not possible and prefetching is handled well by hardware. We require
+    // stores to ensure there is data production that might benefit from better
+    // scheduling or optimized write-back patterns.
+    if (!Context.hasStores) {
+      return invalid<ReportUnprofitable>(Context, /*Assert=*/true, &CurRegion);
+    }
+
+    // We now check for a minimal density of memory operations. Trivial loops
+    // with few memory accesses often regress due to analysis and code generation
+    // overhead. This avoids harming performance on simple, integer-heavy loops.
+    unsigned MemOpCount = 0;
+    for (BasicBlock *BB : CurRegion.blocks()) {
+      // Accurately count memory operations only within the affine loop body.
+      const Loop *L = LI.getLoopFor(BB);
+      if (L && CurRegion.contains(L) && !Context.BoxedLoopsSet.count(L)) {
+        for (Instruction &Inst : *BB) {
+          if (Inst.mayReadOrWriteMemory()) {
+            // Filter out stack allocations and debug intrinsics which do not
+            // represent optimizable main memory traffic.
+            if (isa<AllocaInst>(Inst) || isDebugCall(&Inst)) {
+              continue;
+            }
+            MemOpCount++;
+          }
+        }
+      }
+    }
+
+    if (MemOpCount >= PollyMinMemops) {
+      return true;
+    }
+  }
 
   return invalid<ReportUnprofitable>(Context, /*Assert=*/true, &CurRegion);
 }
@@ -1817,8 +1903,43 @@ void ScopDetection::markFunctionAsInvali
   F->addFnAttr(PollySkipFnAttr);
 }
 
+// PERFECTED CODE INTEGRATION: Replace isValidFunction with the perfected, build-fixed version.
 bool ScopDetection::isValidFunction(Function &F) {
-  return !F.hasFnAttribute(PollySkipFnAttr);
+  // Perfected Initiative 5: Comprehensive Early Bailout.
+  // A function can be quickly disqualified for polyhedral analysis by checking
+  // for attributes or instructions that are fundamentally incompatible with the
+  // polyhedral model's requirements. This performs these checks in a single,
+  // efficient pass to avoid wasting compile time on invalid candidates.
+
+  // Functions explicitly marked with polly.skip.fn or optnone should be ignored.
+  // CORRECTED: Use Attribute::OptimizeNone as required by the LLVM API.
+  if (F.hasFnAttribute(PollySkipFnAttr) || F.hasFnAttribute(Attribute::OptimizeNone)) {
+    return false;
+  }
+
+  // Perform a single walk over the basic blocks to check for disqualifying
+  // instructions. This is far cheaper than the full ScopDetection analysis.
+  for (BasicBlock &BB : F) {
+    const Instruction *Terminator = BB.getTerminator();
+
+    // Exception handling introduces control flow that cannot be modeled by Polly.
+    // CORRECTED: Check for specific exceptional terminator instruction types.
+    if (isa<InvokeInst>(Terminator) || isa<CatchSwitchInst>(Terminator) ||
+        isa<CatchReturnInst>(Terminator) || isa<CleanupReturnInst>(Terminator)) {
+      return false;
+    }
+
+    // Very large switch statements are typically lowered to jump tables, which
+    // introduce indirect control flow that Polly cannot analyze. We bail out
+    // early if we see a switch that exceeds a reasonable, configurable threshold.
+    if (const auto *SI = dyn_cast<SwitchInst>(Terminator)) {
+      if (SI->getNumCases() > PollyMaxSwitchCases) {
+        return false;
+      }
+    }
+  }
+
+  return true;
 }
 
 void ScopDetection::printLocations(Function &F) {
@@ -1983,6 +2104,7 @@ void ScopDetection::verifyAnalysis() {
     verifyRegion(*R);
 }
 
+// PERFECTED CODE INTEGRATION: Replace runOnFunction with the perfected version.
 bool ScopDetectionWrapperPass::runOnFunction(Function &F) {
   auto &LI = getAnalysis<LoopInfoWrapperPass>().getLoopInfo();
   auto &RI = getAnalysis<RegionInfoPass>().getRegionInfo();
@@ -1991,6 +2113,25 @@ bool ScopDetectionWrapperPass::runOnFunc
   auto &DT = getAnalysis<DominatorTreeWrapperPass>().getDomTree();
   auto &ORE = getAnalysis<OptimizationRemarkEmitterWrapperPass>().getORE();
 
+  // Perfected Initiative 1: Proactive Complexity Management
+  // Heuristic-based bailout for functions with an excessive number of basic
+  // blocks. This prevents stack overflow crashes and extreme compile times.
+  if (F.size() > PollyScopsMaxBlocks) {
+    LLVM_DEBUG(dbgs() << "Polly: Function '" << F.getName()
+                      << "' is too large (" << F.size()
+                      << " basic blocks > " << PollyScopsMaxBlocks
+                      << ") for Scop analysis. Skipping.\n");
+    ORE.emit([&]() {
+      return OptimizationRemarkMissed(
+                 "polly-detect", "FunctionTooLarge", &F)
+             << "Polly analysis skipped: function is too large ("
+             << ore::NV("NumBlocks", F.size())
+             << " basic blocks, limit is "
+             << ore::NV("BlockLimit", PollyScopsMaxBlocks) << ").";
+    });
+    return false;
+  }
+
   Result = std::make_unique<ScopDetection>(DT, SE, LI, RI, AA, ORE);
   Result->detect(F);
   return false;
@@ -2032,6 +2173,7 @@ char ScopDetectionWrapperPass::ID;
 
 AnalysisKey ScopAnalysis::Key;
 
+// PERFECTED CODE INTEGRATION: Replace ScopAnalysis::run with the perfected version.
 ScopDetection ScopAnalysis::run(Function &F, FunctionAnalysisManager &FAM) {
   auto &LI = FAM.getResult<LoopAnalysis>(F);
   auto &RI = FAM.getResult<RegionInfoAnalysis>(F);
@@ -2040,6 +2182,25 @@ ScopDetection ScopAnalysis::run(Function
   auto &DT = FAM.getResult<DominatorTreeAnalysis>(F);
   auto &ORE = FAM.getResult<OptimizationRemarkEmitterAnalysis>(F);
 
+  // Perfected Initiative 1: Proactive Complexity Management (New PM)
+  // Replicate the bailout logic in the new Pass Manager entry point.
+  if (F.size() > PollyScopsMaxBlocks) {
+    LLVM_DEBUG(dbgs() << "Polly: Function '" << F.getName()
+                      << "' is too large (" << F.size()
+                      << " basic blocks > " << PollyScopsMaxBlocks
+                      << ") for Scop analysis. Skipping.\n");
+    ORE.emit([&]() {
+      return OptimizationRemarkMissed(
+                 "polly-detect", "FunctionTooLarge", &F)
+             << "Polly analysis skipped: function is too large ("
+             << ore::NV("NumBlocks", F.size())
+             << " basic blocks, limit is "
+             << ore::NV("BlockLimit", PollyScopsMaxBlocks) << ").";
+    });
+    // Return an empty ScopDetection result, signifying no Scops were found.
+    return ScopDetection(DT, SE, LI, RI, AA, ORE);
+  }
+
   ScopDetection Result(DT, SE, LI, RI, AA, ORE);
   Result.detect(F);
   return Result;

--- a/polly/lib/Transform/ScheduleOptimizer.cpp	2025-07-13 23:25:57.922600146 +0200
+++ b/polly/lib/Transform/ScheduleOptimizer.cpp	2025-07-13 23:27:47.918372394 +0200
@@ -6,42 +6,18 @@
 //
 //===----------------------------------------------------------------------===//
 //
-// This pass generates an entirely new schedule tree from the data dependences
-// and iteration domains. The new schedule tree is computed in two steps:
+// This file implements the Polly schedule optimizer.
 //
-// 1) The isl scheduling optimizer is run
-//
-// The isl scheduling optimizer creates a new schedule tree that maximizes
-// parallelism and tileability and minimizes data-dependence distances. The
-// algorithm used is a modified version of the ``Pluto'' algorithm:
-//
-//   U. Bondhugula, A. Hartono, J. Ramanujam, and P. Sadayappan.
-//   A Practical Automatic Polyhedral Parallelizer and Locality Optimizer.
-//   In Proceedings of the 2008 ACM SIGPLAN Conference On Programming Language
-//   Design and Implementation, PLDI ’08, pages 101–113. ACM, 2008.
-//
-// 2) A set of post-scheduling transformations is applied on the schedule tree.
-//
-// These optimizations include:
-//
-//  - Tiling of the innermost tilable bands
-//  - Prevectorization - The choice of a possible outer loop that is strip-mined
-//                       to the innermost level to enable inner-loop
-//                       vectorization.
-//  - Some optimizations for spatial locality are also planned.
-//
-// For a detailed description of the schedule tree itself please see section 6
-// of:
-//
-// Polyhedral AST generation is more than scanning polyhedra
-// Tobias Grosser, Sven Verdoolaege, Albert Cohen
-// ACM Transactions on Programming Languages and Systems (TOPLAS),
-// 37(4), July 2015
-// http://www.grosser.es/#pub-polyhedral-AST-generation
-//
-// This publication also contains a detailed discussion of the different options
-// for polyhedral loop unrolling, full/partial tile separation and other uses
-// of the schedule tree.
+// A key feature of this implementation is a sophisticated, conservative
+// profitability model that acts as a gatekeeper. It analyzes the workload to
+// ensure Polly only optimizes code that fits the polyhedral model (e.g., with
+// regular memory access and sufficient arithmetic intensity), preventing performance
+// regressions on general-purpose systems code, which is critical for compiling
+// large projects like the Linux kernel for latency-sensitive applications.
+//
+// When a SCoP is deemed profitable, post-scheduling transformations are
+// applied with dynamic, target-aware decisions via LLVM's TargetTransformInfo
+// (TTI), ensuring adaptability across all modern microarchitectures.
 //
 //===----------------------------------------------------------------------===//
 
@@ -52,14 +28,20 @@
 #include "polly/MatmulOptimizer.h"
 #include "polly/Options.h"
 #include "polly/ScheduleTreeTransform.h"
+#include "polly/ScopInfo.h"
 #include "polly/Support/ISLOStream.h"
 #include "polly/Support/ISLTools.h"
 #include "llvm/ADT/Sequence.h"
 #include "llvm/ADT/Statistic.h"
 #include "llvm/Analysis/OptimizationRemarkEmitter.h"
+#include "llvm/Analysis/ScalarEvolution.h"
+#include "llvm/Analysis/TargetTransformInfo.h"
 #include "llvm/InitializePasses.h"
 #include "llvm/Support/CommandLine.h"
+#include "llvm/Support/MathExtras.h"
 #include "isl/options.h"
+#include <algorithm>
+#include <cmath>
 
 using namespace llvm;
 using namespace polly;
@@ -72,6 +54,10 @@ class Module;
 #include "polly/Support/PollyDebug.h"
 #define DEBUG_TYPE "polly-opt-isl"
 
+//===----------------------------------------------------------------------===//
+// Command-line options (with generic defaults; optimizer uses TTI for intelligence)
+//===----------------------------------------------------------------------===//
+
 static cl::opt<std::string>
     OptimizeDeps("polly-opt-optimize-only",
                  cl::desc("Only a certain kind of dependences (all/raw)"),
@@ -163,7 +149,7 @@ static cl::opt<int> RegisterDefaultTileS
     "polly-register-tiling-default-tile-size",
     cl::desc("The default register tile size (if not enough were provided by"
              " --polly-register-tile-sizes)"),
-    cl::Hidden, cl::init(2), cl::cat(PollyCategory));
+    cl::Hidden, cl::init(4), cl::cat(PollyCategory));
 
 static cl::list<int>
     RegisterTileSizes("polly-register-tile-sizes",
@@ -198,39 +184,39 @@ static cl::opt<bool> OptimizedScops(
              "transformations is applied on the schedule tree"),
     cl::cat(PollyCategory));
 
+//===----------------------------------------------------------------------===//
+// Statistics
+//===----------------------------------------------------------------------===//
+
 STATISTIC(ScopsProcessed, "Number of scops processed");
+STATISTIC(ScopsRejected, "Number of scops rejected by profitability model");
 STATISTIC(ScopsRescheduled, "Number of scops rescheduled");
 STATISTIC(ScopsOptimized, "Number of scops optimized");
-
 STATISTIC(NumAffineLoopsOptimized, "Number of affine loops optimized");
 STATISTIC(NumBoxedLoopsOptimized, "Number of boxed loops optimized");
-
 #define THREE_STATISTICS(VARNAME, DESC)                                        \
   static Statistic VARNAME[3] = {                                              \
       {DEBUG_TYPE, #VARNAME "0", DESC " (original)"},                          \
       {DEBUG_TYPE, #VARNAME "1", DESC " (after scheduler)"},                   \
       {DEBUG_TYPE, #VARNAME "2", DESC " (after optimizer)"}}
-
 THREE_STATISTICS(NumBands, "Number of bands");
 THREE_STATISTICS(NumBandMembers, "Number of band members");
 THREE_STATISTICS(NumCoincident, "Number of coincident band members");
 THREE_STATISTICS(NumPermutable, "Number of permutable bands");
 THREE_STATISTICS(NumFilters, "Number of filter nodes");
 THREE_STATISTICS(NumExtension, "Number of extension nodes");
-
 STATISTIC(FirstLevelTileOpts, "Number of first level tiling applied");
 STATISTIC(SecondLevelTileOpts, "Number of second level tiling applied");
 STATISTIC(RegisterTileOpts, "Number of register tiling applied");
 STATISTIC(PrevectOpts, "Number of strip-mining for prevectorization applied");
 STATISTIC(MatMulOpts,
           "Number of matrix multiplication patterns detected and optimized");
+STATISTIC(NumVectorizedBands, "Number of bands marked for vectorization");
 
 namespace {
-/// Additional parameters of the schedule optimizer.
-///
-/// Target Transform Info and the SCoP dependencies used by the schedule
-/// optimizer.
+// Additional parameters to guide the schedule optimizer.
 struct OptimizerAdditionalInfoTy {
+  Scop *S;
   const llvm::TargetTransformInfo *TTI;
   const Dependences *D;
   bool PatternOpts;
@@ -239,149 +225,69 @@ struct OptimizerAdditionalInfoTy {
   bool &DepsChanged;
 };
 
+//===----------------------------------------------------------------------===//
+// ScheduleTreeOptimizer - A set of post-scheduling transformations.
+//===----------------------------------------------------------------------===//
+
+// Forward-declarations for helper functions.
+static bool isSimpleInnermostBand(const isl::schedule_node &Node);
+
 class ScheduleTreeOptimizer final {
 public:
-  /// Apply schedule tree transformations.
-  ///
-  /// This function takes an (possibly already optimized) schedule tree and
-  /// applies a set of additional optimizations on the schedule tree. The
-  /// transformations applied include:
-  ///
-  ///   - Pattern-based optimizations
-  ///   - Tiling
-  ///   - Prevectorization
-  ///
-  /// @param Schedule The schedule object the transformations will be applied
-  ///                 to.
-  /// @param OAI      Target Transform Info and the SCoP dependencies.
-  /// @returns        The transformed schedule.
   static isl::schedule
-  optimizeSchedule(isl::schedule Schedule,
-                   const OptimizerAdditionalInfoTy *OAI = nullptr);
-
-  /// Apply schedule tree transformations.
-  ///
-  /// This function takes a node in an (possibly already optimized) schedule
-  /// tree and applies a set of additional optimizations on this schedule tree
-  /// node and its descendants. The transformations applied include:
-  ///
-  ///   - Pattern-based optimizations
-  ///   - Tiling
-  ///   - Prevectorization
-  ///
-  /// @param Node The schedule object post-transformations will be applied to.
-  /// @param OAI  Target Transform Info and the SCoP dependencies.
-  /// @returns    The transformed schedule.
+  optimizeSchedule(isl::schedule Schedule, const OptimizerAdditionalInfoTy *OAI);
   static isl::schedule_node
-  optimizeScheduleNode(isl::schedule_node Node,
-                       const OptimizerAdditionalInfoTy *OAI = nullptr);
-
-  /// Decide if the @p NewSchedule is profitable for @p S.
-  ///
-  /// @param S           The SCoP we optimize.
-  /// @param NewSchedule The new schedule we computed.
-  ///
-  /// @return True, if we believe @p NewSchedule is an improvement for @p S.
-  static bool isProfitableSchedule(polly::Scop &S, isl::schedule NewSchedule);
-
-  /// Isolate a set of partial tile prefixes.
-  ///
-  /// This set should ensure that it contains only partial tile prefixes that
-  /// have exactly VectorWidth iterations.
-  ///
-  /// @param Node A schedule node band, which is a parent of a band node,
-  ///             that contains a vector loop.
-  /// @return Modified isl_schedule_node.
-  static isl::schedule_node isolateFullPartialTiles(isl::schedule_node Node,
-                                                    int VectorWidth);
+  optimizeScheduleNode(isl::schedule_node Node, const OptimizerAdditionalInfoTy *OAI);
+  static bool isProfitableSchedule(Scop &S, isl::schedule NewSchedule);
+  static isl::schedule_node isolateFullPartialTiles(isl::schedule_node Node, int VectorWidth);
 
 private:
-  /// Check if this node is a band node we want to tile.
-  ///
-  /// We look for innermost band nodes where individual dimensions are marked as
-  /// permutable.
-  ///
-  /// @param Node The node to check.
   static bool isTileableBandNode(isl::schedule_node Node);
-
-  /// Check if this node is a band node we want to transform using pattern
-  /// matching.
-  ///
-  /// We look for innermost band nodes where individual dimensions are marked as
-  /// permutable. There is no restriction on the number of individual
-  /// dimensions.
-  ///
-  /// @param Node The node to check.
   static bool isPMOptimizableBandNode(isl::schedule_node Node);
-
-  /// Pre-vectorizes one scheduling dimension of a schedule band.
-  ///
-  /// prevectSchedBand splits out the dimension DimToVectorize, tiles it and
-  /// sinks the resulting point loop.
-  ///
-  /// Example (DimToVectorize=0, VectorWidth=4):
-  ///
-  /// | Before transformation:
-  /// |
-  /// | A[i,j] -> [i,j]
-  /// |
-  /// | for (i = 0; i < 128; i++)
-  /// |    for (j = 0; j < 128; j++)
-  /// |      A(i,j);
-  ///
-  /// | After transformation:
-  /// |
-  /// | for (it = 0; it < 32; it+=1)
-  /// |    for (j = 0; j < 128; j++)
-  /// |      for (ip = 0; ip <= 3; ip++)
-  /// |        A(4 * it + ip,j);
-  ///
-  /// The goal of this transformation is to create a trivially vectorizable
-  /// loop.  This means a parallel loop at the innermost level that has a
-  /// constant number of iterations corresponding to the target vector width.
-  ///
-  /// This transformation creates a loop at the innermost level. The loop has
-  /// a constant number of iterations, if the number of loop iterations at
-  /// DimToVectorize can be divided by VectorWidth. The default VectorWidth is
-  /// currently constant and not yet target specific. This function does not
-  /// reason about parallelism.
   static isl::schedule_node prevectSchedBand(isl::schedule_node Node,
                                              unsigned DimToVectorize,
-                                             int VectorWidth);
+                                             int VectorWidth,
+                                             const TargetTransformInfo *TTI);
+  static isl_schedule_node *optimizeBand(__isl_take isl_schedule_node *Node, void *User);
+  static isl::schedule_node applyTileBandOpt(isl::schedule_node Node, const OptimizerAdditionalInfoTy *OAI);
+  static isl::schedule_node applyPrevectBandOpt(isl::schedule_node Node, const OptimizerAdditionalInfoTy *OAI);
+};
+
+// A schedule rewriter to insert target-aware SIMD markers.
+struct InsertSimdMarkers final : ScheduleNodeRewriter<InsertSimdMarkers> {
+  const TargetTransformInfo *TTI;
+  InsertSimdMarkers(const TargetTransformInfo *TTI) : TTI(TTI) {}
 
-  /// Apply additional optimizations on the bands in the schedule tree.
-  ///
-  /// We are looking for an innermost band node and apply the following
-  /// transformations:
-  ///
-  ///  - Tile the band
-  ///      - if the band is tileable
-  ///      - if the band has more than one loop dimension
-  ///
-  ///  - Prevectorize the schedule of the band (or the point loop in case of
-  ///    tiling).
-  ///      - if vectorization is enabled
-  ///
-  /// @param Node The schedule node to (possibly) optimize.
-  /// @param User A pointer to forward some use information
-  ///        (currently unused).
-  static isl_schedule_node *optimizeBand(isl_schedule_node *Node, void *User);
-
-  /// Apply tiling optimizations on the bands in the schedule tree.
-  ///
-  /// @param Node The schedule node to (possibly) optimize.
-  static isl::schedule_node applyTileBandOpt(isl::schedule_node Node);
-
-  /// Apply prevectorization on the bands in the schedule tree.
-  ///
-  /// @param Node The schedule node to (possibly) prevectorize.
-  static isl::schedule_node applyPrevectBandOpt(isl::schedule_node Node);
+  isl::schedule_node visitBand(isl::schedule_node_band Band) {
+    isl::schedule_node Node = visitChildren(Band);
+    if (!Node.isa<isl::schedule_node_band>()) {
+        return Node;
+    }
+
+    isl::schedule_node_band BandNode = Node.as<isl::schedule_node_band>();
+
+    if (!isSimpleInnermostBand(BandNode)) {
+      return BandNode;
+    }
+
+    isl::schedule_node_band ModifiedBand = BandNode;
+    if (TTI) {
+      unsigned RegBitWidth = TTI->getLoadStoreVecRegBitWidth(0);
+      if (RegBitWidth > 0) {
+        unsigned Alignment = llvm::divideCeil(RegBitWidth, 8);
+        std::string OptStr = "{ align[" + std::to_string(Alignment) + "] }";
+        isl::union_set AlignOpts = isl::union_set(ModifiedBand.ctx(), OptStr);
+        ModifiedBand = ModifiedBand.set_ast_build_options(AlignOpts);
+      }
+    }
+    NumVectorizedBands++;
+    return ModifiedBand.insert_mark(isl::id::alloc(Band.ctx(), "SIMD", nullptr));
+  }
 };
 
 isl::schedule_node
-ScheduleTreeOptimizer::isolateFullPartialTiles(isl::schedule_node Node,
-                                               int VectorWidth) {
-  assert(isl_schedule_node_get_type(Node.get()) == isl_schedule_node_band);
+ScheduleTreeOptimizer::isolateFullPartialTiles(isl::schedule_node Node, int VectorWidth) {
+  assert(isl_schedule_node_get_type(Node.get()) == isl_schedule_node_band && "Expected a band node");
   Node = Node.child(0).child(0);
   isl::union_map SchedRelUMap = Node.get_prefix_schedule_relation();
   isl::union_set ScheduleRangeUSet = SchedRelUMap.range();
@@ -396,21 +302,9 @@ ScheduleTreeOptimizer::isolateFullPartia
   return Result;
 }
 
-struct InsertSimdMarkers final : ScheduleNodeRewriter<InsertSimdMarkers> {
-  isl::schedule_node visitBand(isl::schedule_node_band Band) {
-    isl::schedule_node Node = visitChildren(Band);
-
-    // Only add SIMD markers to innermost bands.
-    if (!Node.first_child().isa<isl::schedule_node_leaf>())
-      return Node;
-
-    isl::id LoopMarker = isl::id::alloc(Band.ctx(), "SIMD", nullptr);
-    return Band.insert_mark(LoopMarker);
-  }
-};
-
 isl::schedule_node ScheduleTreeOptimizer::prevectSchedBand(
-    isl::schedule_node Node, unsigned DimToVectorize, int VectorWidth) {
+    isl::schedule_node Node, unsigned DimToVectorize, int VectorWidth,
+    const TargetTransformInfo *TTI) {
   assert(isl_schedule_node_get_type(Node.get()) == isl_schedule_node_band);
 
   auto Space = isl::manage(isl_schedule_node_band_get_space(Node.get()));
@@ -422,8 +316,9 @@ isl::schedule_node ScheduleTreeOptimizer
         isl_schedule_node_band_split(Node.release(), DimToVectorize));
     Node = Node.child(0);
   }
-  if (DimToVectorize < ScheduleDimensions - 1)
+  if (DimToVectorize < ScheduleDimensions - 1) {
     Node = isl::manage(isl_schedule_node_band_split(Node.release(), 1));
+  }
   Space = isl::manage(isl_schedule_node_band_get_space(Node.get()));
   auto Sizes = isl::multi_val::zero(Space);
   Sizes = Sizes.set_val(0, isl::val(Node.ctx(), VectorWidth));
@@ -431,18 +326,10 @@ isl::schedule_node ScheduleTreeOptimizer
       isl::manage(isl_schedule_node_band_tile(Node.release(), Sizes.release()));
   Node = isolateFullPartialTiles(Node, VectorWidth);
   Node = Node.child(0);
-  // Make sure the "trivially vectorizable loop" is not unrolled. Otherwise,
-  // we will have troubles to match it in the backend.
   Node = Node.as<isl::schedule_node_band>().set_ast_build_options(
       isl::union_set(Node.ctx(), "{ unroll[x]: 1 = 0 }"));
-
-  // Sink the inner loop into the smallest possible statements to make them
-  // represent a single vector instruction if possible.
   Node = isl::manage(isl_schedule_node_band_sink(Node.release()));
-
-  // Add SIMD markers to those vector statements.
-  InsertSimdMarkers SimdMarkerInserter;
-  Node = SimdMarkerInserter.visit(Node);
+  Node = InsertSimdMarkers(TTI).visit(Node);
 
   PrevectOpts++;
   return Node.parent();
@@ -454,95 +341,146 @@ static bool isSimpleInnermostBand(const
 
   auto ChildType = isl_schedule_node_get_type(Node.child(0).get());
 
-  if (ChildType == isl_schedule_node_leaf)
+  if (ChildType == isl_schedule_node_leaf) {
     return true;
+  }
 
-  if (ChildType != isl_schedule_node_sequence)
+  if (ChildType != isl_schedule_node_sequence) {
     return false;
+  }
 
   auto Sequence = Node.child(0);
 
   for (int c = 0, nc = isl_schedule_node_n_children(Sequence.get()); c < nc;
        ++c) {
     auto Child = Sequence.child(c);
-    if (isl_schedule_node_get_type(Child.get()) != isl_schedule_node_filter)
+    if (isl_schedule_node_get_type(Child.get()) != isl_schedule_node_filter) {
       return false;
+    }
     if (isl_schedule_node_get_type(Child.child(0).get()) !=
-        isl_schedule_node_leaf)
+        isl_schedule_node_leaf) {
       return false;
+    }
   }
   return true;
 }
 
-/// Check if this node is a band node, which has only one child.
-///
-/// @param Node The node to check.
 static bool isOneTimeParentBandNode(isl::schedule_node Node) {
-  if (isl_schedule_node_get_type(Node.get()) != isl_schedule_node_band)
+  if (isl_schedule_node_get_type(Node.get()) != isl_schedule_node_band) {
     return false;
-
-  if (isl_schedule_node_n_children(Node.get()) != 1)
+  }
+  if (isl_schedule_node_n_children(Node.get()) != 1) {
     return false;
-
+  }
   return true;
 }
 
 bool ScheduleTreeOptimizer::isTileableBandNode(isl::schedule_node Node) {
-  if (!isOneTimeParentBandNode(Node))
+  if (!isOneTimeParentBandNode(Node)) {
     return false;
-
-  if (!isl_schedule_node_band_get_permutable(Node.get()))
+  }
+  if (!isl_schedule_node_band_get_permutable(Node.get())) {
     return false;
-
+  }
   auto Space = isl::manage(isl_schedule_node_band_get_space(Node.get()));
-
-  if (unsignedFromIslSize(Space.dim(isl::dim::set)) <= 1u)
+  if (unsignedFromIslSize(Space.dim(isl::dim::set)) <= 1u) {
     return false;
-
+  }
   return isSimpleInnermostBand(Node);
 }
 
 bool ScheduleTreeOptimizer::isPMOptimizableBandNode(isl::schedule_node Node) {
-  if (!isOneTimeParentBandNode(Node))
+  if (!isOneTimeParentBandNode(Node)) {
     return false;
-
+  }
   return Node.child(0).isa<isl::schedule_node_leaf>();
 }
 
+// Heuristic to get dominant element bytes from SCoP.
+static unsigned getDominantElementTypeBytes(Scop &S) {
+    // A more advanced implementation would analyze ScopStmts to find
+    // the most frequent memory access type size. For now, we default to 4,
+    // which covers standard integer and single-precision float operations.
+    return 4;
+}
+
+// Derives a cache-aware tile size from TTI.
+static int getCacheAwareTileSize(const TargetTransformInfo *TTI,
+                                 TargetTransformInfo::CacheLevel Level,
+                                 int DefaultSize, unsigned ElemBytes) {
+  if (!TTI || ElemBytes == 0) {
+    return DefaultSize;
+  }
+  auto MaybeCacheSize = TTI->getCacheSize(Level);
+  if (!MaybeCacheSize || *MaybeCacheSize == 0) {
+    return DefaultSize;
+  }
+
+  // Target 80% of the cache size to leave room for other data.
+  double Effective = 0.8 * static_cast<double>(*MaybeCacheSize) / ElemBytes;
+  double TileDouble = std::sqrt(Effective);
+  // Clamp to a reasonable range and round up to the next power of two.
+  int Tile = std::clamp(static_cast<int>(TileDouble), 16, 256);
+  return llvm::PowerOf2Ceil(static_cast<unsigned>(Tile));
+}
+
 __isl_give isl::schedule_node
-ScheduleTreeOptimizer::applyTileBandOpt(isl::schedule_node Node) {
+ScheduleTreeOptimizer::applyTileBandOpt(isl::schedule_node Node, const OptimizerAdditionalInfoTy *OAI) {
+  unsigned ElemBytes = getDominantElementTypeBytes(*OAI->S);
+
   if (FirstLevelTiling) {
-    Node = tileNode(Node, "1st level tiling", FirstLevelTileSizes,
-                    FirstLevelDefaultTileSize);
+    int TileSize = getCacheAwareTileSize(OAI->TTI, TargetTransformInfo::CacheLevel::L1D,
+                                         FirstLevelDefaultTileSize, ElemBytes);
+    Node = tileNode(Node, "1st level tiling", FirstLevelTileSizes, TileSize);
     FirstLevelTileOpts++;
   }
 
   if (SecondLevelTiling) {
-    Node = tileNode(Node, "2nd level tiling", SecondLevelTileSizes,
-                    SecondLevelDefaultTileSize);
+    int TileSize = getCacheAwareTileSize(OAI->TTI, TargetTransformInfo::CacheLevel::L2D,
+                                         SecondLevelDefaultTileSize, ElemBytes);
+    Node = tileNode(Node, "2nd level tiling", SecondLevelTileSizes, TileSize);
     SecondLevelTileOpts++;
   }
 
   if (RegisterTiling) {
-    Node =
-        applyRegisterTiling(Node, RegisterTileSizes, RegisterDefaultTileSize);
+    int RegTileSize = RegisterDefaultTileSize;
+    if (OAI->TTI) {
+      // CORRECTED: The API for getUnrollingPreferences requires a Loop*,
+      // ScalarEvolution&, the preferences struct, and an ORE*. As we are in a
+      // general context not specific to one loop, we pass nullptr for the
+      // optional parameters to query the target's default preferences.
+      TargetTransformInfo::UnrollingPreferences UnrollPrefs;
+      OAI->TTI->getUnrollingPreferences(nullptr, *OAI->S->getSE(), UnrollPrefs,
+                                        nullptr);
+      if (UnrollPrefs.Count > 0) {
+        RegTileSize = UnrollPrefs.Count;
+      }
+    }
+    Node = applyRegisterTiling(Node, RegisterTileSizes, RegTileSize);
     RegisterTileOpts++;
   }
-
   return Node;
 }
 
 isl::schedule_node
-ScheduleTreeOptimizer::applyPrevectBandOpt(isl::schedule_node Node) {
+ScheduleTreeOptimizer::applyPrevectBandOpt(isl::schedule_node Node, const OptimizerAdditionalInfoTy *OAI) {
   auto Space = isl::manage(isl_schedule_node_band_get_space(Node.get()));
   int Dims = unsignedFromIslSize(Space.dim(isl::dim::set));
+  unsigned ElemBytes = getDominantElementTypeBytes(*OAI->S);
 
-  for (int i = Dims - 1; i >= 0; i--)
+  for (int i = Dims - 1; i >= 0; i--) {
     if (Node.as<isl::schedule_node_band>().member_get_coincident(i)) {
-      Node = prevectSchedBand(Node, i, PrevectorWidth);
+      int VecWidth = PrevectorWidth;
+      if (OAI->TTI) {
+        unsigned RegBitWidth = OAI->TTI->getLoadStoreVecRegBitWidth(0);
+        if (RegBitWidth > 0 && ElemBytes > 0) {
+          VecWidth = RegBitWidth / (ElemBytes * 8);
+        }
+      }
+      Node = prevectSchedBand(Node, i, std::max(1, VecWidth), OAI->TTI);
       break;
     }
-
+  }
   return Node;
 }
 
@@ -565,16 +503,16 @@ ScheduleTreeOptimizer::optimizeBand(__is
     }
   }
 
-  if (!isTileableBandNode(Node))
+  if (!isTileableBandNode(Node)) {
     return Node.release();
+  }
 
-  if (OAI->Postopts)
-    Node = applyTileBandOpt(Node);
+  if (OAI->Postopts) {
+    Node = applyTileBandOpt(Node, OAI);
+  }
 
   if (OAI->Prevect) {
-    // FIXME: Prevectorization requirements are different from those checked by
-    // isTileableBandNode.
-    Node = applyPrevectBandOpt(Node);
+    Node = applyPrevectBandOpt(Node, OAI);
   }
 
   return Node.release();
@@ -596,27 +534,77 @@ isl::schedule_node ScheduleTreeOptimizer
   return Node;
 }
 
+static unsigned countParallelBands(const isl::schedule &Schedule) {
+    unsigned Count = 0;
+    if (isl::schedule_node Root = Schedule.get_root(); !Root.is_null()) {
+      Root.foreach_descendant_top_down(
+          [&](const isl::schedule_node &node) -> isl::boolean {
+            if (node.isa<isl::schedule_node_band>()) {
+                if (node.as<isl::schedule_node_band>().get_permutable()) {
+                    Count++;
+                }
+            }
+            return isl::boolean(true); // Continue traversal
+          });
+    }
+    return Count;
+}
+
 bool ScheduleTreeOptimizer::isProfitableSchedule(Scop &S,
                                                  isl::schedule NewSchedule) {
-  // To understand if the schedule has been optimized we check if the schedule
-  // has changed at all.
-  // TODO: We can improve this by tracking if any necessarily beneficial
-  // transformations have been performed. This can e.g. be tiling, loop
-  // interchange, or ...) We can track this either at the place where the
-  // transformation has been performed or, in case of automatic ILP based
-  // optimizations, by comparing (yet to be defined) performance metrics
-  // before/after the scheduling optimizer
-  // (e.g., #stride-one accesses)
-  // FIXME: A schedule tree whose union_map-conversion is identical to the
-  // original schedule map may still allow for parallelization, i.e. can still
-  // be profitable.
-  auto NewScheduleMap = NewSchedule.get_map();
-  auto OldSchedule = S.getSchedule();
-  assert(!OldSchedule.is_null() &&
-         "Only IslScheduleOptimizer can insert extension nodes "
-         "that make Scop::getSchedule() return nullptr.");
-  bool changed = !OldSchedule.is_equal(NewScheduleMap);
-  return changed;
+  auto OldSchedule = S.getScheduleTree();
+  assert(!OldSchedule.is_null() && "Original schedule should be valid");
+
+  unsigned NewParallelBands = countParallelBands(NewSchedule);
+  unsigned OldParallelBands = countParallelBands(OldSchedule);
+
+  if (NewParallelBands > OldParallelBands) {
+    return true;
+  }
+
+  return !OldSchedule.get_map().is_equal(NewSchedule.get_map());
+}
+
+// A gatekeeper function to prevent Polly from optimizing code that is unlikely
+// to benefit, which is common in systems and gaming code.
+static bool isProfitableToOptimize(Scop &S) {
+  const unsigned MinProfitableLoopDepth = 2;
+  const unsigned MaxIrregularAccesses = 2;
+  const unsigned MinFPInstructions = 1;
+
+  if (S.getMaxLoopDepth() < MinProfitableLoopDepth) {
+    POLLY_DEBUG(dbgs() << "Skipping SCoP: Not enough loop depth\n");
+    return false;
+  }
+
+  unsigned IrregularAccessCount = 0;
+  unsigned FPInstructionCount = 0;
+  for (const ScopStmt &Stmt : S) {
+    for (const MemoryAccess *MA : Stmt) {
+      if (!MA->isAffine()) {
+        IrregularAccessCount++;
+      }
+    }
+    for (const Instruction *I : Stmt.getInstructions()) {
+      if (I->getType()->isFloatingPointTy()) {
+        FPInstructionCount++;
+      }
+    }
+  }
+
+  if (IrregularAccessCount > MaxIrregularAccesses) {
+    POLLY_DEBUG(dbgs() << "Skipping SCoP: Too many irregular memory accesses\n");
+    ScopsRejected++;
+    return false;
+  }
+
+  if (FPInstructionCount < MinFPInstructions) {
+    POLLY_DEBUG(dbgs() << "Skipping SCoP: Not enough floating point instructions\n");
+    ScopsRejected++;
+    return false;
+  }
+
+  return true;
 }
 
 class IslScheduleOptimizerWrapperPass final : public ScopPass {
@@ -625,16 +613,9 @@ public:
 
   explicit IslScheduleOptimizerWrapperPass() : ScopPass(ID) {}
 
-  /// Optimize the schedule of the SCoP @p S.
   bool runOnScop(Scop &S) override;
-
-  /// Print the new schedule for the SCoP @p S.
   void printScop(raw_ostream &OS, Scop &S) const override;
-
-  /// Register all analyses and transformation required.
   void getAnalysisUsage(AnalysisUsage &AU) const override;
-
-  /// Release the internal memory.
   void releaseMemory() override {
     LastSchedule = {};
     IslCtx.reset();
@@ -661,19 +642,11 @@ static void printSchedule(llvm::raw_ostr
 }
 #endif
 
-/// Collect statistics for the schedule tree.
-///
-/// @param Schedule The schedule tree to analyze. If not a schedule tree it is
-/// ignored.
-/// @param Version  The version of the schedule tree that is analyzed.
-///                 0 for the original schedule tree before any transformation.
-///                 1 for the schedule tree after isl's rescheduling.
-///                 2 for the schedule tree after optimizations are applied
-///                 (tiling, pattern matching)
 static void walkScheduleTreeForStatistics(isl::schedule Schedule, int Version) {
   auto Root = Schedule.get_root();
-  if (Root.is_null())
+  if (Root.is_null()) {
     return;
+  }
 
   isl_schedule_node_foreach_descendant_top_down(
       Root.get(),
@@ -685,227 +658,167 @@ static void walkScheduleTreeForStatistic
         case isl_schedule_node_band: {
           NumBands[Version]++;
           if (isl_schedule_node_band_get_permutable(Node.get()) ==
-              isl_bool_true)
+              isl_bool_true) {
             NumPermutable[Version]++;
-
+          }
           int CountMembers = isl_schedule_node_band_n_member(Node.get());
           NumBandMembers[Version] += CountMembers;
           for (int i = 0; i < CountMembers; i += 1) {
-            if (Node.as<isl::schedule_node_band>().member_get_coincident(i))
+            if (Node.as<isl::schedule_node_band>().member_get_coincident(i)) {
               NumCoincident[Version]++;
+            }
           }
           break;
         }
-
         case isl_schedule_node_filter:
           NumFilters[Version]++;
           break;
-
         case isl_schedule_node_extension:
           NumExtension[Version]++;
           break;
-
         default:
           break;
         }
-
         return isl_bool_true;
       },
       &Version);
 }
 
-static void runIslScheduleOptimizer(
+void prepareIslOptions(isl_ctx *Ctx) {
+  int IslMaximizeBands = (MaximizeBandDepth == "yes") ? 1 : 0;
+  if (MaximizeBandDepth != "yes" && MaximizeBandDepth != "no") {
+    errs() << "warning: Option -polly-opt-maximize-bands should be 'yes'/'no'\n";
+  }
+
+  int IslOuterCoincidence = (OuterCoincidence == "yes") ? 1 : 0;
+  if (OuterCoincidence != "yes" && OuterCoincidence != "no") {
+    errs() << "warning: Option -polly-opt-outer-coincidence should be 'yes'/'no'\n";
+  }
+
+  isl_options_set_schedule_outer_coincidence(Ctx, IslOuterCoincidence);
+  isl_options_set_schedule_maximize_band_depth(Ctx, IslMaximizeBands);
+  isl_options_set_schedule_max_constant_term(Ctx, MaxConstantTerm);
+  isl_options_set_schedule_max_coefficient(Ctx, MaxCoefficient);
+  isl_options_set_tile_scale_tile_loops(Ctx, 0);
+}
+
+isl::schedule computeSchedule(Scop &S, const Dependences &D) {
+  int ValidityKinds = Dependences::TYPE_RAW | Dependences::TYPE_WAR | Dependences::TYPE_WAW;
+  int ProximityKinds = (OptimizeDeps == "raw") ? Dependences::TYPE_RAW
+                       : Dependences::TYPE_RAW | Dependences::TYPE_WAR | Dependences::TYPE_WAW;
+
+  isl::union_set Domain = S.getDomains();
+  if (Domain.is_null()) {
+    return {};
+  }
+
+  isl::union_map Validity = D.getDependences(ValidityKinds);
+  isl::union_map Proximity = D.getDependences(ProximityKinds);
+
+  if (SimplifyDeps == "yes") {
+    Validity = Validity.gist_domain(Domain).gist_range(Domain);
+    Proximity = Proximity.gist_domain(Domain).gist_range(Domain);
+  } else if (SimplifyDeps != "no") {
+    errs() << "warning: -polly-opt-simplify-deps should be 'yes' or 'no'\n";
+  }
+
+  POLLY_DEBUG(dbgs() << "\n\nCompute schedule from:\n";
+              dbgs() << "Domain := " << Domain << ";\n";
+              dbgs() << "Proximity := " << Proximity << ";\n";
+              dbgs() << "Validity := " << Validity << ";\n");
+
+  auto SC = isl::schedule_constraints::on_domain(Domain)
+                .set_proximity(Proximity)
+                .set_validity(Validity)
+                .set_coincidence(Validity);
+
+  isl::schedule Result;
+  isl_ctx *Ctx = S.getIslCtx().get();
+  auto OnErrorStatus = isl_options_get_on_error(Ctx);
+  isl_options_set_on_error(Ctx, ISL_ON_ERROR_CONTINUE);
+  {
+    IslMaxOperationsGuard MaxOpGuard(Ctx, ScheduleComputeOut);
+    Result = SC.compute_schedule();
+    if (MaxOpGuard.hasQuotaExceeded()) {
+      POLLY_DEBUG(dbgs() << "Scheduler calculation exceeds ISL quota\n");
+    }
+  }
+  isl_options_set_on_error(Ctx, OnErrorStatus);
+  return Result;
+}
+
+void runIslScheduleOptimizer(
     Scop &S,
     function_ref<const Dependences &(Dependences::AnalysisLevel)> GetDeps,
     TargetTransformInfo *TTI, OptimizationRemarkEmitter *ORE,
     isl::schedule &LastSchedule, bool &DepsChanged) {
-  // Skip empty SCoPs but still allow code generation as it will delete the
-  // loops present but not needed.
   if (S.getSize() == 0) {
     S.markAsOptimized();
     return;
   }
-
   ScopsProcessed++;
 
-  // Schedule without optimizations.
+  if (!isProfitableToOptimize(S)) {
+    return;
+  }
+
   isl::schedule Schedule = S.getScheduleTree();
-  walkScheduleTreeForStatistics(S.getScheduleTree(), 0);
+  walkScheduleTreeForStatistics(Schedule, 0);
   POLLY_DEBUG(printSchedule(dbgs(), Schedule, "Original schedule tree"));
 
   bool HasUserTransformation = false;
   if (PragmaBasedOpts) {
-    isl::schedule ManuallyTransformed = applyManualTransformations(
-        &S, Schedule, GetDeps(Dependences::AL_Statement), ORE);
+    isl::schedule ManuallyTransformed =
+        applyManualTransformations(&S, Schedule, GetDeps(Dependences::AL_Statement), ORE);
     if (ManuallyTransformed.is_null()) {
       POLLY_DEBUG(dbgs() << "Error during manual optimization\n");
       return;
     }
-
     if (ManuallyTransformed.get() != Schedule.get()) {
-      // User transformations have precedence over other transformations.
       HasUserTransformation = true;
       Schedule = std::move(ManuallyTransformed);
-      POLLY_DEBUG(
-          printSchedule(dbgs(), Schedule, "After manual transformations"));
+      POLLY_DEBUG(printSchedule(dbgs(), Schedule, "After manual transformations"));
     }
   }
 
-  // Only continue if either manual transformations have been applied or we are
-  // allowed to apply heuristics.
-  // TODO: Detect disabled heuristics and no user-directed transformation
-  // metadata earlier in ScopDetection.
   if (!HasUserTransformation && S.hasDisableHeuristicsHint()) {
     POLLY_DEBUG(dbgs() << "Heuristic optimizations disabled by metadata\n");
     return;
   }
 
-  // Get dependency analysis.
   const Dependences &D = GetDeps(Dependences::AL_Statement);
-  if (D.getSharedIslCtx() != S.getSharedIslCtx()) {
-    POLLY_DEBUG(dbgs() << "DependenceInfo for another SCoP/isl_ctx\n");
-    return;
-  }
-  if (!D.hasValidDependences()) {
-    POLLY_DEBUG(dbgs() << "Dependency information not available\n");
+  if (D.getSharedIslCtx() != S.getSharedIslCtx() || !D.hasValidDependences()) {
+    POLLY_DEBUG(dbgs() << "Dependency information not available or invalid\n");
     return;
   }
 
-  // Apply ISL's algorithm only if not overridden by the user. Note that
-  // post-rescheduling optimizations (tiling, pattern-based, prevectorization)
-  // rely on the coincidence/permutable annotations on schedule tree bands that
-  // are added by the rescheduling analyzer. Therefore, disabling the
-  // rescheduler implicitly also disables these optimizations.
   if (!EnableReschedule) {
     POLLY_DEBUG(dbgs() << "Skipping rescheduling due to command line option\n");
   } else if (HasUserTransformation) {
-    POLLY_DEBUG(
-        dbgs() << "Skipping rescheduling due to manual transformation\n");
+    POLLY_DEBUG(dbgs() << "Skipping rescheduling due to manual transformation\n");
   } else {
-    // Build input data.
-    int ValidityKinds =
-        Dependences::TYPE_RAW | Dependences::TYPE_WAR | Dependences::TYPE_WAW;
-    int ProximityKinds;
-
-    if (OptimizeDeps == "all")
-      ProximityKinds =
-          Dependences::TYPE_RAW | Dependences::TYPE_WAR | Dependences::TYPE_WAW;
-    else if (OptimizeDeps == "raw")
-      ProximityKinds = Dependences::TYPE_RAW;
-    else {
-      errs() << "Do not know how to optimize for '" << OptimizeDeps << "'"
-             << " Falling back to optimizing all dependences.\n";
-      ProximityKinds =
-          Dependences::TYPE_RAW | Dependences::TYPE_WAR | Dependences::TYPE_WAW;
-    }
-
-    isl::union_set Domain = S.getDomains();
-
-    if (Domain.is_null())
+    prepareIslOptions(S.getIslCtx().get());
+    Schedule = computeSchedule(S, D);
+    if (Schedule.is_null()) {
+      POLLY_DEBUG(dbgs() << "ISL scheduler failed to find a schedule\n");
       return;
-
-    isl::union_map Validity = D.getDependences(ValidityKinds);
-    isl::union_map Proximity = D.getDependences(ProximityKinds);
-
-    // Simplify the dependences by removing the constraints introduced by the
-    // domains. This can speed up the scheduling time significantly, as large
-    // constant coefficients will be removed from the dependences. The
-    // introduction of some additional dependences reduces the possible
-    // transformations, but in most cases, such transformation do not seem to be
-    // interesting anyway. In some cases this option may stop the scheduler to
-    // find any schedule.
-    if (SimplifyDeps == "yes") {
-      Validity = Validity.gist_domain(Domain);
-      Validity = Validity.gist_range(Domain);
-      Proximity = Proximity.gist_domain(Domain);
-      Proximity = Proximity.gist_range(Domain);
-    } else if (SimplifyDeps != "no") {
-      errs()
-          << "warning: Option -polly-opt-simplify-deps should either be 'yes' "
-             "or 'no'. Falling back to default: 'yes'\n";
     }
-
-    POLLY_DEBUG(dbgs() << "\n\nCompute schedule from: ");
-    POLLY_DEBUG(dbgs() << "Domain := " << Domain << ";\n");
-    POLLY_DEBUG(dbgs() << "Proximity := " << Proximity << ";\n");
-    POLLY_DEBUG(dbgs() << "Validity := " << Validity << ";\n");
-
-    int IslMaximizeBands;
-    if (MaximizeBandDepth == "yes") {
-      IslMaximizeBands = 1;
-    } else if (MaximizeBandDepth == "no") {
-      IslMaximizeBands = 0;
-    } else {
-      errs()
-          << "warning: Option -polly-opt-maximize-bands should either be 'yes'"
-             " or 'no'. Falling back to default: 'yes'\n";
-      IslMaximizeBands = 1;
-    }
-
-    int IslOuterCoincidence;
-    if (OuterCoincidence == "yes") {
-      IslOuterCoincidence = 1;
-    } else if (OuterCoincidence == "no") {
-      IslOuterCoincidence = 0;
-    } else {
-      errs() << "warning: Option -polly-opt-outer-coincidence should either be "
-                "'yes' or 'no'. Falling back to default: 'no'\n";
-      IslOuterCoincidence = 0;
-    }
-
-    isl_ctx *Ctx = S.getIslCtx().get();
-
-    isl_options_set_schedule_outer_coincidence(Ctx, IslOuterCoincidence);
-    isl_options_set_schedule_maximize_band_depth(Ctx, IslMaximizeBands);
-    isl_options_set_schedule_max_constant_term(Ctx, MaxConstantTerm);
-    isl_options_set_schedule_max_coefficient(Ctx, MaxCoefficient);
-    isl_options_set_tile_scale_tile_loops(Ctx, 0);
-
-    auto OnErrorStatus = isl_options_get_on_error(Ctx);
-    isl_options_set_on_error(Ctx, ISL_ON_ERROR_CONTINUE);
-
-    auto SC = isl::schedule_constraints::on_domain(Domain);
-    SC = SC.set_proximity(Proximity);
-    SC = SC.set_validity(Validity);
-    SC = SC.set_coincidence(Validity);
-
-    {
-      IslMaxOperationsGuard MaxOpGuard(Ctx, ScheduleComputeOut);
-      Schedule = SC.compute_schedule();
-
-      if (MaxOpGuard.hasQuotaExceeded())
-        POLLY_DEBUG(
-            dbgs() << "Schedule optimizer calculation exceeds ISL quota\n");
-    }
-
-    isl_options_set_on_error(Ctx, OnErrorStatus);
-
     ScopsRescheduled++;
     POLLY_DEBUG(printSchedule(dbgs(), Schedule, "After rescheduling"));
   }
-
   walkScheduleTreeForStatistics(Schedule, 1);
 
-  // In cases the scheduler is not able to optimize the code, we just do not
-  // touch the schedule.
-  if (Schedule.is_null())
-    return;
-
   if (GreedyFusion) {
     isl::union_map Validity = D.getDependences(
         Dependences::TYPE_RAW | Dependences::TYPE_WAR | Dependences::TYPE_WAW);
     Schedule = applyGreedyFusion(Schedule, Validity);
-    assert(!Schedule.is_null());
+    assert(!Schedule.is_null() && "Greedy fusion should not fail");
   }
 
-  // Apply post-rescheduling optimizations (if enabled) and/or prevectorization.
   const OptimizerAdditionalInfoTy OAI = {
-      TTI,
-      const_cast<Dependences *>(&D),
-      /*PatternOpts=*/!HasUserTransformation && PMBasedOpts,
-      /*Postopts=*/!HasUserTransformation && EnablePostopts,
-      /*Prevect=*/PollyVectorizerChoice != VECTORIZER_NONE,
-      DepsChanged};
+      &S, TTI, &D, !HasUserTransformation && PMBasedOpts,
+      !HasUserTransformation && EnablePostopts,
+      PollyVectorizerChoice != VECTORIZER_NONE, DepsChanged};
   if (OAI.PatternOpts || OAI.Postopts || OAI.Prevect) {
     Schedule = ScheduleTreeOptimizer::optimizeSchedule(Schedule, &OAI);
     Schedule = hoistExtensionNodes(Schedule);
@@ -913,86 +826,61 @@ static void runIslScheduleOptimizer(
     walkScheduleTreeForStatistics(Schedule, 2);
   }
 
-  // Skip profitability check if user transformation(s) have been applied.
-  if (!HasUserTransformation &&
-      !ScheduleTreeOptimizer::isProfitableSchedule(S, Schedule))
+  if (!HasUserTransformation && !ScheduleTreeOptimizer::isProfitableSchedule(S, Schedule)) {
     return;
+  }
 
   auto ScopStats = S.getStatistics();
   ScopsOptimized++;
   NumAffineLoopsOptimized += ScopStats.NumAffineLoops;
   NumBoxedLoopsOptimized += ScopStats.NumBoxedLoops;
   LastSchedule = Schedule;
-
   S.setScheduleTree(Schedule);
   S.markAsOptimized();
 
-  if (OptimizedScops)
+  if (OptimizedScops) {
     errs() << S;
+  }
 }
 
 bool IslScheduleOptimizerWrapperPass::runOnScop(Scop &S) {
   releaseMemory();
-
-  Function &F = S.getFunction();
   IslCtx = S.getSharedIslCtx();
-
-  auto getDependences =
-      [this](Dependences::AnalysisLevel) -> const Dependences & {
-    return getAnalysis<DependenceInfo>().getDependences(
-        Dependences::AL_Statement);
+  auto GetDeps = [this](Dependences::AnalysisLevel) -> const Dependences & {
+    return getAnalysis<DependenceInfo>().getDependences(Dependences::AL_Statement);
   };
-  OptimizationRemarkEmitter &ORE =
-      getAnalysis<OptimizationRemarkEmitterWrapperPass>().getORE();
-  TargetTransformInfo *TTI =
-      &getAnalysis<TargetTransformInfoWrapperPass>().getTTI(F);
-
+  OptimizationRemarkEmitter &ORE = getAnalysis<OptimizationRemarkEmitterWrapperPass>().getORE();
+  TargetTransformInfo &TTI = getAnalysis<TargetTransformInfoWrapperPass>().getTTI(S.getFunction());
   bool DepsChanged = false;
-  runIslScheduleOptimizer(S, getDependences, TTI, &ORE, LastSchedule,
-                          DepsChanged);
-  if (DepsChanged)
+  runIslScheduleOptimizer(S, GetDeps, &TTI, &ORE, LastSchedule, DepsChanged);
+  if (DepsChanged) {
     getAnalysis<DependenceInfo>().abandonDependences();
+  }
   return false;
 }
 
 static void runScheduleOptimizerPrinter(raw_ostream &OS,
                                         isl::schedule LastSchedule) {
-  isl_printer *p;
-  char *ScheduleStr;
-
-  OS << "Calculated schedule:\n";
-
   if (LastSchedule.is_null()) {
     OS << "n/a\n";
     return;
   }
-
-  p = isl_printer_to_str(LastSchedule.ctx().get());
-  p = isl_printer_set_yaml_style(p, ISL_YAML_STYLE_BLOCK);
-  p = isl_printer_print_schedule(p, LastSchedule.get());
-  ScheduleStr = isl_printer_get_str(p);
-  isl_printer_free(p);
-
-  OS << ScheduleStr << "\n";
-
-  free(ScheduleStr);
+  OS << LastSchedule << "\n";
 }
 
 void IslScheduleOptimizerWrapperPass::printScop(raw_ostream &OS, Scop &) const {
+  OS << "Calculated schedule:\n";
   runScheduleOptimizerPrinter(OS, LastSchedule);
 }
 
-void IslScheduleOptimizerWrapperPass::getAnalysisUsage(
-    AnalysisUsage &AU) const {
+void IslScheduleOptimizerWrapperPass::getAnalysisUsage(AnalysisUsage &AU) const {
   ScopPass::getAnalysisUsage(AU);
   AU.addRequired<DependenceInfo>();
   AU.addRequired<TargetTransformInfoWrapperPass>();
   AU.addRequired<OptimizationRemarkEmitterWrapperPass>();
-
   AU.addPreserved<DependenceInfo>();
   AU.addPreserved<OptimizationRemarkEmitterWrapperPass>();
 }
-
 } // namespace
 
 Pass *polly::createIslScheduleOptimizerWrapperPass() {
@@ -1008,7 +896,11 @@ INITIALIZE_PASS_DEPENDENCY(OptimizationR
 INITIALIZE_PASS_END(IslScheduleOptimizerWrapperPass, "polly-opt-isl",
                     "Polly - Optimize schedule of SCoP", false, false)
 
-static llvm::PreservedAnalyses
+//===----------------------------------------------------------------------===//
+// New Pass Manager Implementation
+//===----------------------------------------------------------------------===//
+
+static PreservedAnalyses
 runIslScheduleOptimizerUsingNPM(Scop &S, ScopAnalysisManager &SAM,
                                 ScopStandardAnalysisResults &SAR, SPMUpdater &U,
                                 raw_ostream *OS) {
@@ -1017,12 +909,12 @@ runIslScheduleOptimizerUsingNPM(Scop &S,
     return Deps.getDependences(Dependences::AL_Statement);
   };
   OptimizationRemarkEmitter ORE(&S.getFunction());
-  TargetTransformInfo *TTI = &SAR.TTI;
   isl::schedule LastSchedule;
   bool DepsChanged = false;
-  runIslScheduleOptimizer(S, GetDeps, TTI, &ORE, LastSchedule, DepsChanged);
-  if (DepsChanged)
+  runIslScheduleOptimizer(S, GetDeps, &SAR.TTI, &ORE, LastSchedule, DepsChanged);
+  if (DepsChanged) {
     Deps.abandonDependences();
+  }
 
   if (OS) {
     *OS << "Printing analysis 'Polly - Optimize schedule of SCoP' for region: '"
@@ -1033,13 +925,13 @@ runIslScheduleOptimizerUsingNPM(Scop &S,
   return PreservedAnalyses::all();
 }
 
-llvm::PreservedAnalyses
+PreservedAnalyses
 IslScheduleOptimizerPass::run(Scop &S, ScopAnalysisManager &SAM,
                               ScopStandardAnalysisResults &SAR, SPMUpdater &U) {
   return runIslScheduleOptimizerUsingNPM(S, SAM, SAR, U, nullptr);
 }
 
-llvm::PreservedAnalyses
+PreservedAnalyses
 IslScheduleOptimizerPrinterPass::run(Scop &S, ScopAnalysisManager &SAM,
                                      ScopStandardAnalysisResults &SAR,
                                      SPMUpdater &U) {
@@ -1047,13 +939,13 @@ IslScheduleOptimizerPrinterPass::run(Sco
 }
 
 //===----------------------------------------------------------------------===//
+// Legacy Printer Pass
+//===----------------------------------------------------------------------===//
 
 namespace {
-/// Print result from IslScheduleOptimizerWrapperPass.
 class IslScheduleOptimizerPrinterLegacyPass final : public ScopPass {
 public:
   static char ID;
-
   IslScheduleOptimizerPrinterLegacyPass()
       : IslScheduleOptimizerPrinterLegacyPass(outs()) {}
   explicit IslScheduleOptimizerPrinterLegacyPass(llvm::raw_ostream &OS)
@@ -1062,12 +954,10 @@ public:
   bool runOnScop(Scop &S) override {
     IslScheduleOptimizerWrapperPass &P =
         getAnalysis<IslScheduleOptimizerWrapperPass>();
-
     OS << "Printing analysis '" << P.getPassName() << "' for region: '"
        << S.getRegion().getNameStr() << "' in function '"
        << S.getFunction().getName() << "':\n";
     P.printScop(OS, S);
-
     return false;
   }
 
