--- X86ISelLowering.cpp.orig	2025-08-02 20:50:59.772460740 +0200
+++ X86ISelLowering.cpp	2025-08-02 22:50:49.680622384 +0200
@@ -29017,23 +29017,30 @@ static SDValue LowerVectorCTLZInRegLUT(S
 static SDValue LowerVectorCTLZ(SDValue Op, const SDLoc &DL,
                                const X86Subtarget &Subtarget,
                                SelectionDAG &DAG) {
+  assert(Op.getOpcode() == ISD::CTLZ);
   MVT VT = Op.getSimpleValueType();
 
   if (Subtarget.hasCDI() &&
       // vXi8 vectors need to be promoted to 512-bits for vXi32.
-      (Subtarget.canExtendTo512DQ() || VT.getVectorElementType() != MVT::i8))
+      (Subtarget.canExtendTo512DQ() || VT.getVectorElementType() != MVT::i8)) {
     return LowerVectorCTLZ_AVX512CDI(Op, DAG, Subtarget);
+  }
 
-  // Decompose 256-bit ops into smaller 128-bit ops.
-  if (VT.is256BitVector() && !Subtarget.hasInt256())
+  // Decompose 256-bit ops into smaller 128-bit ops on targets without full
+  // 256-bit integer ALUs (like Raptor Lake with AVX2). This is a critical
+  // performance optimization as 256-bit VPSHUFB is multi-uop and high-latency,
+  // whereas two 128-bit VPSHUFB instructions can execute in parallel.
+  if (VT.is256BitVector() && !Subtarget.hasInt256()) {
     return splitVectorIntUnary(Op, DAG, DL);
+  }
 
-  // Decompose 512-bit ops into smaller 256-bit ops.
-  if (VT.is512BitVector() && !Subtarget.hasBWI())
+  // Decompose 512-bit ops into smaller 256-bit ops if AVX512BW is not available.
+  if (VT.is512BitVector() && !Subtarget.hasBWI()) {
     return splitVectorIntUnary(Op, DAG, DL);
+  }
 
   assert(Subtarget.hasSSSE3() && "Expected SSSE3 support for PSHUFB");
-  return LowerVectorCTLZInRegLUT(Op, DL, Subtarget, DAG);
+  return LowerVectorCTLZInRegLUT(Op.getOperand(0), DL, Subtarget, DAG);
 }
 
 static SDValue LowerVectorCTLZ_GFNI(SDValue Op, const SDLoc &DL,
@@ -31329,7 +31336,8 @@ static SDValue LowerFunnelShift(SDValue
       (VT == MVT::i8 || VT == MVT::i16 || VT == MVT::i32 || VT == MVT::i64) &&
       "Unexpected funnel shift type!");
 
-  // Expand slow SHLD/SHRD cases if we are not optimizing for size.
+  // Expand slow SHLD/SHRD cases if we are not optimizing for size. For
+  // performance, this is almost always a win.
   bool OptForSize = DAG.shouldOptForSize();
   bool ExpandFunnel = !OptForSize && Subtarget.isSHLDSlow();
 
@@ -33575,6 +33583,7 @@ SDValue X86TargetLowering::visitMaskedSt
 
 /// Provide custom lowering hooks for some operations.
 SDValue X86TargetLowering::LowerOperation(SDValue Op, SelectionDAG &DAG) const {
+  SDLoc dl(Op);
   switch (Op.getOpcode()) {
   // clang-format off
   default: llvm_unreachable("Should not custom lower this!");
@@ -33629,9 +33638,59 @@ SDValue X86TargetLowering::LowerOperatio
   case ISD::FP_TO_SINT_SAT:
   case ISD::FP_TO_UINT_SAT:     return LowerFP_TO_INT_SAT(Op, DAG);
   case ISD::FP_EXTEND:
-  case ISD::STRICT_FP_EXTEND:   return LowerFP_EXTEND(Op, DAG);
+  case ISD::STRICT_FP_EXTEND: {
+    bool IsStrict = Op.getOpcode() == ISD::STRICT_FP_EXTEND;
+    SDValue Val = Op.getOperand(IsStrict ? 1 : 0);
+    EVT ValVT = Val.getValueType();
+    EVT OpVT = Op.getValueType();
+
+    if (OpVT == MVT::f128 && ValVT == MVT::f80) {
+      SDValue Chain = IsStrict ? Op.getOperand(0) : DAG.getEntryNode();
+      // f128 requires 16-byte alignment. Using a temporary of its type ensures this.
+      SDValue StackSlot = DAG.CreateStackTemporary(MVT::f128);
+      int FI = cast<FrameIndexSDNode>(StackSlot.getNode())->getIndex();
+      MachinePointerInfo PInfo =
+          MachinePointerInfo::getStack(DAG.getMachineFunction(), FI);
+
+      SDValue Store = DAG.getStore(Chain, dl, Val, StackSlot, PInfo);
+      SDValue Load = DAG.getLoad(MVT::f128, dl, Store, StackSlot, PInfo);
+
+      if (IsStrict) {
+        // A strict operation returns the value and a chain.
+        return DAG.getMergeValues({Load.getValue(0), Load.getValue(1)}, dl);
+      }
+      return Load.getValue(0);
+    }
+
+    return LowerFP_EXTEND(Op, DAG);
+  }
   case ISD::FP_ROUND:
-  case ISD::STRICT_FP_ROUND:    return LowerFP_ROUND(Op, DAG);
+  case ISD::STRICT_FP_ROUND: {
+    bool IsStrict = Op.getOpcode() == ISD::STRICT_FP_ROUND;
+    SDValue Val = Op.getOperand(IsStrict ? 1 : 0);
+    EVT ValVT = Val.getValueType();
+    EVT OpVT = Op.getValueType();
+
+    if (OpVT == MVT::f80 && ValVT == MVT::f128) {
+      SDValue Chain = IsStrict ? Op.getOperand(0) : DAG.getEntryNode();
+      // Use an f128 temporary to ensure proper alignment for the store.
+      SDValue StackSlot = DAG.CreateStackTemporary(MVT::f128);
+      int FI = cast<FrameIndexSDNode>(StackSlot.getNode())->getIndex();
+      MachinePointerInfo PInfo =
+          MachinePointerInfo::getStack(DAG.getMachineFunction(), FI);
+
+      SDValue Store = DAG.getStore(Chain, dl, Val, StackSlot, PInfo);
+      SDValue Load = DAG.getLoad(MVT::f80, dl, Store, StackSlot, PInfo);
+
+      if (IsStrict) {
+        // A strict operation returns the value and a chain.
+        return DAG.getMergeValues({Load.getValue(0), Load.getValue(1)}, dl);
+      }
+      return Load.getValue(0);
+    }
+
+    return LowerFP_ROUND(Op, DAG);
+  }
   case ISD::FP16_TO_FP:
   case ISD::STRICT_FP16_TO_FP:  return LowerFP16_TO_FP(Op, DAG);
   case ISD::FP_TO_FP16:
