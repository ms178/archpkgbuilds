--- a/llvm/lib/Target/X86/X86ISelLowering.cpp	2025-08-02 20:50:59.772460740 +0200
+++ b/llvm/lib/Target/X86/X86ISelLowering.cpp	2025-08-14 15:49:20.796623328 +0200
@@ -20202,11 +20202,22 @@ std::pair<SDValue, SDValue> X86TargetLow
 /// Horizontal vector math instructions may be slower than normal math with
 /// shuffles. Limit horizontal op codegen based on size/speed trade-offs, uarch
 /// implementation, and likely shuffle complexity of the alternate sequence.
+
 static bool shouldUseHorizontalOp(bool IsSingleSource, SelectionDAG &DAG,
                                   const X86Subtarget &Subtarget) {
-  bool IsOptimizingSize = DAG.shouldOptForSize();
-  bool HasFastHOps = Subtarget.hasFastHorizontalOps();
-  return !IsSingleSource || IsOptimizingSize || HasFastHOps;
+  // On high-performance cores like Raptor Lake, horizontal operations
+  // (e.g., HADDPS) are almost always slower than a shuffle-and-add/sub
+  // sequence due to high latency and low throughput from cross-lane data
+  // movement. For maximum performance, we should only use them when
+  // optimizing for code size.
+  if (DAG.shouldOptForSize())
+    return true;
+
+  // For Raptor Lake (and most high-performance Intel/AMD cores),
+  // HasFastHorizontalOps should be false. We will rely on that, but could
+  // add an explicit check for CPU models if needed. If a future CPU has
+  // truly fast horizontal ops, its Subtarget feature flag will enable them.
+  return Subtarget.hasFastHorizontalOps();
 }
 
 /// 64-bit unsigned integer to double expansion.
@@ -29029,20 +29040,27 @@ static SDValue LowerVectorCTLZInRegLUT(S
 static SDValue LowerVectorCTLZ(SDValue Op, const SDLoc &DL,
                                const X86Subtarget &Subtarget,
                                SelectionDAG &DAG) {
+  assert(Op.getOpcode() == ISD::CTLZ);
   MVT VT = Op.getSimpleValueType();
 
   if (Subtarget.hasCDI() &&
       // vXi8 vectors need to be promoted to 512-bits for vXi32.
-      (Subtarget.canExtendTo512DQ() || VT.getVectorElementType() != MVT::i8))
+      (Subtarget.canExtendTo512DQ() || VT.getVectorElementType() != MVT::i8)) {
     return LowerVectorCTLZ_AVX512CDI(Op, DAG, Subtarget);
+  }
 
-  // Decompose 256-bit ops into smaller 128-bit ops.
-  if (VT.is256BitVector() && !Subtarget.hasInt256())
+  // Decompose 256-bit ops into smaller 128-bit ops on targets without full
+  // 256-bit integer ALUs (like Raptor Lake with AVX2). This is a critical
+  // performance optimization as 256-bit VPSHUFB is multi-uop and high-latency,
+  // whereas two 128-bit VPSHUFB instructions can execute in parallel.
+  if (VT.is256BitVector() && !Subtarget.hasInt256()) {
     return splitVectorIntUnary(Op, DAG, DL);
+  }
 
-  // Decompose 512-bit ops into smaller 256-bit ops.
-  if (VT.is512BitVector() && !Subtarget.hasBWI())
+  // Decompose 512-bit ops into smaller 256-bit ops if AVX512BW is not available.
+  if (VT.is512BitVector() && !Subtarget.hasBWI()) {
     return splitVectorIntUnary(Op, DAG, DL);
+  }
 
   assert(Subtarget.hasSSSE3() && "Expected SSSE3 support for PSHUFB");
   return LowerVectorCTLZInRegLUT(Op, DL, Subtarget, DAG);
@@ -31341,7 +31359,8 @@ static SDValue LowerFunnelShift(SDValue
       (VT == MVT::i8 || VT == MVT::i16 || VT == MVT::i32 || VT == MVT::i64) &&
       "Unexpected funnel shift type!");
 
-  // Expand slow SHLD/SHRD cases if we are not optimizing for size.
+  // Expand slow SHLD/SHRD cases if we are not optimizing for size. For
+  // performance, this is almost always a win.
   bool OptForSize = DAG.shouldOptForSize();
   bool ExpandFunnel = !OptForSize && Subtarget.isSHLDSlow();
 
@@ -62046,15 +62065,17 @@ X86TargetLowering::getRegForInlineAsmCon
 }
 
 bool X86TargetLowering::isIntDivCheap(EVT VT, AttributeList Attr) const {
-  // Integer division on x86 is expensive. However, when aggressively optimizing
-  // for code size, we prefer to use a div instruction, as it is usually smaller
-  // than the alternative sequence.
-  // The exception to this is vector division. Since x86 doesn't have vector
-  // integer division, leaving the division as-is is a loss even in terms of
-  // size, because it will have to be scalarized, while the alternative code
-  // sequence can be performed in vector form.
-  bool OptSize = Attr.hasFnAttr(Attribute::MinSize);
-  return OptSize && !VT.isVector();
+  // On modern CPUs like Raptor Lake, the 'idiv' instruction has a very high,
+  // data-dependent latency (10-26 cycles) and is not well-pipelined, making it
+  // a performance catastrophe. For performance-critical code like games, it is
+  // almost never "cheap". The alternative magic-multiply sequence is much faster.
+  // We only consider 'idiv' cheap if optimizing for code size, as it is a
+  // single instruction. Vector division is never cheap as it must be scalarized.
+  if (VT.isVector())
+    return false;
+
+  bool OptSize = Attr.hasFnAttr(Attribute::MinSize) || Attr.hasFnAttr(Attribute::OptimizeForSize);
+  return OptSize;
 }
 
 void X86TargetLowering::initializeSplitCSR(MachineBasicBlock *Entry) const {
