--- a/llvm/lib/Target/X86/X86ISelLowering.cpp	2025-08-02 20:50:59.772460740 +0200
+++ b/llvm/lib/Target/X86/X86ISelLowering.cpp	2025-08-14 15:49:20.796623328 +0200
@@ -71,7 +71,7 @@ using namespace llvm;
 #define DEBUG_TYPE "x86-isel"
 
 static cl::opt<int> ExperimentalPrefInnermostLoopAlignment(
-    "x86-experimental-pref-innermost-loop-alignment", cl::init(4),
+    "x86-experimental-pref-innermost-loop-alignment", cl::init(5),
     cl::desc(
         "Sets the preferable loop alignment for experiments (as log2 bytes) "
         "for innermost loops only. If specified, this option overrides "
@@ -79,7 +79,7 @@ static cl::opt<int> ExperimentalPrefInne
     cl::Hidden);
 
 static cl::opt<int> BrMergingBaseCostThresh(
-    "x86-br-merging-base-cost", cl::init(2),
+    "x86-br-merging-base-cost", cl::init(4),
     cl::desc(
         "Sets the cost threshold for when multiple conditionals will be merged "
         "into one branch versus be split in multiple branches. Merging "
@@ -90,7 +90,7 @@ static cl::opt<int> BrMergingBaseCostThr
     cl::Hidden);
 
 static cl::opt<int> BrMergingCcmpBias(
-    "x86-br-merging-ccmp-bias", cl::init(6),
+    "x86-br-merging-ccmp-bias", cl::init(8),
     cl::desc("Increases 'x86-br-merging-base-cost' in cases that the target "
              "supports conditional compare instructions."),
     cl::Hidden);
@@ -20227,9 +20227,19 @@ std::pair<SDValue, SDValue> X86TargetLow
 /// implementation, and likely shuffle complexity of the alternate sequence.
 static bool shouldUseHorizontalOp(bool IsSingleSource, SelectionDAG &DAG,
                                   const X86Subtarget &Subtarget) {
-  bool IsOptimizingSize = DAG.shouldOptForSize();
-  bool HasFastHOps = Subtarget.hasFastHorizontalOps();
-  return !IsSingleSource || IsOptimizingSize || HasFastHOps;
+  // On high-performance cores like Raptor Lake, horizontal operations
+  // (e.g., HADDPS) are almost always slower than a shuffle-and-add/sub
+  // sequence due to high latency and low throughput from cross-lane data
+  // movement. For maximum performance, we should only use them when
+  // optimizing for code size.
+  if (DAG.shouldOptForSize())
+    return true;
+
+  // For Raptor Lake (and most high-performance Intel/AMD cores),
+  // HasFastHorizontalOps should be false. We will rely on that, but could
+  // add an explicit check for CPU models if needed. If a future CPU has
+  // truly fast horizontal ops, its Subtarget feature flag will enable them.
+  return Subtarget.hasFastHorizontalOps();
 }
 
 /// 64-bit unsigned integer to double expansion.
@@ -62150,15 +62160,17 @@ X86TargetLowering::getRegForInlineAsmCon
 }
 
 bool X86TargetLowering::isIntDivCheap(EVT VT, AttributeList Attr) const {
-  // Integer division on x86 is expensive. However, when aggressively optimizing
-  // for code size, we prefer to use a div instruction, as it is usually smaller
-  // than the alternative sequence.
-  // The exception to this is vector division. Since x86 doesn't have vector
-  // integer division, leaving the division as-is is a loss even in terms of
-  // size, because it will have to be scalarized, while the alternative code
-  // sequence can be performed in vector form.
+  // On modern CPUs like Raptor Lake, the 'idiv' instruction has a very high,
+  // data-dependent latency (10-26 cycles) and is not well-pipelined, making it
+  // a performance catastrophe. For performance-critical code, it is
+  // almost never "cheap". The alternative magic-multiply sequence is much faster.
+  // We only consider 'idiv' cheap if optimizing for code size, as it is a
+  // single instruction. Vector division is never cheap as it must be scalarized.
+  if (VT.isVector())
+    return false;
+
   bool OptSize = Attr.hasFnAttr(Attribute::MinSize);
-  return OptSize && !VT.isVector();
+  return OptSize;
 }
 
 void X86TargetLowering::initializeSplitCSR(MachineBasicBlock *Entry) const {
