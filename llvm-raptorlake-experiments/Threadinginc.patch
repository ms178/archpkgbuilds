--- Threading.inc.orig	2025-08-17 15:38:45.541940793 +0200
+++ Threading.inc	2025-08-17 15:47:53.023375379 +0200
@@ -1,7 +1,7 @@
 //===- Unix/Threading.inc - Unix Threading Implementation ----- -*- C++ -*-===//
 //
-// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
-// See https://llvm.org/LICENSE.txt for license information.
+// Part of the the LLVM Project, under the Apache License v2.0 with LLVM
+// Exceptions. See https://llvm.org/LICENSE.txt for license information.
 // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 //
 //===----------------------------------------------------------------------===//
@@ -11,6 +11,7 @@
 //===----------------------------------------------------------------------===//
 
 #include "Unix.h"
+#include "llvm/ADT/BitVector.h"
 #include "llvm/ADT/ScopeExit.h"
 #include "llvm/ADT/SmallString.h"
 #include "llvm/ADT/SmallVector.h"
@@ -28,6 +29,7 @@
 #endif
 
 #include <pthread.h>
+#include <cstring> // for strlen
 
 #if defined(__FreeBSD__) || defined(__OpenBSD__) || defined(__DragonFly__)
 #include <pthread_np.h> // For pthread_getthreadid_np() / pthread_set_name_np()
@@ -59,7 +61,7 @@
 #if defined(__linux__)
 #include <sched.h>       // For sched_getaffinity
 #include <sys/syscall.h> // For syscall codes
-#include <unistd.h>      // For syscall()
+#include <unistd.h>      // For syscall(), sysconf()
 #endif
 
 #if defined(__CYGWIN__)
@@ -128,11 +130,13 @@ pthread_t llvm_thread_get_current_id_imp
 uint64_t llvm::get_threadid() {
 #if defined(__APPLE__)
   // Calling "mach_thread_self()" bumps the reference count on the thread
-  // port, so we need to deallocate it. mach_task_self() doesn't bump the ref
-  // count.
+  // port, so we need to deallocate it when no longer needed. Here we cache
+  // the port for the lifetime of the thread.
   static thread_local thread_port_t Self = [] {
     thread_port_t InitSelf = mach_thread_self();
-    mach_port_deallocate(mach_task_self(), Self);
+    // Note: we intentionally do not deallocate InitSelf here, as we keep it
+    // cached for the thread lifetime. The OS will reclaim it when the thread
+    // exits.
     return InitSelf;
   }();
   return Self;
@@ -186,7 +190,7 @@ void llvm::set_thread_name(const Twine &
   StringRef NameStr = Name.toNullTerminatedStringRef(Storage);
 
   // Truncate from the beginning, not the end, if the specified name is too
-  // long.  For one, this ensures that the resulting string is still null
+  // long. For one, this ensures that the resulting string is still null
   // terminated, but additionally the end of a long thread name will usually
   // be more unique than the beginning, since a common pattern is for similar
   // threads to share a common prefix.
@@ -270,32 +274,36 @@ void llvm::get_thread_name(SmallVectorIm
 
 SetThreadPriorityResult llvm::set_thread_priority(ThreadPriority Priority) {
 #if (defined(__linux__) || defined(__CYGWIN__)) && defined(SCHED_IDLE)
-  // Some *really* old glibcs are missing SCHED_IDLE.
-  // http://man7.org/linux/man-pages/man3/pthread_setschedparam.3.html
-  // http://man7.org/linux/man-pages/man2/sched_setscheduler.2.html
-  sched_param priority;
-  // For each of the above policies, param->sched_priority must be 0.
-  priority.sched_priority = 0;
-  // SCHED_IDLE    for running very low priority background jobs.
-  // SCHED_OTHER   the standard round-robin time-sharing policy;
-  return !pthread_setschedparam(
-             pthread_self(),
-             // FIXME: consider SCHED_BATCH for Low
-             Priority == ThreadPriority::Default ? SCHED_OTHER : SCHED_IDLE,
-             &priority)
+  // Some very old glibcs may be missing SCHED_IDLE.
+  // https://man7.org/linux/man-pages/man3/pthread_setschedparam.3.html
+  // https://man7.org/linux/man-pages/man2/sched_setscheduler.2.html
+  sched_param Param;
+  Param.sched_priority = 0;
+
+  int Policy = SCHED_OTHER;
+  switch (Priority) {
+  case ThreadPriority::Background:
+    // True background work. Lowest CPU pressure; won't interfere with foreground.
+    Policy = SCHED_IDLE;
+    break;
+  case ThreadPriority::Low:
+#if defined(SCHED_BATCH)
+    // Deprioritized but does not starve; ideal for auxiliary work.
+    Policy = SCHED_BATCH;
+#else
+    Policy = SCHED_OTHER;
+#endif
+    break;
+  case ThreadPriority::Default:
+    Policy = SCHED_OTHER;
+    break;
+  }
+
+  return !pthread_setschedparam(pthread_self(), Policy, &Param)
              ? SetThreadPriorityResult::SUCCESS
              : SetThreadPriorityResult::FAILURE;
 #elif defined(__APPLE__)
   // https://developer.apple.com/documentation/apple-silicon/tuning-your-code-s-performance-for-apple-silicon
-  //
-  // Background - Applies to work that isnâ€™t visible to the user and may take
-  // significant time to complete. Examples include indexing, backing up, or
-  // synchronizing data. This class emphasizes energy efficiency.
-  //
-  // Utility - Applies to work that takes anywhere from a few seconds to a few
-  // minutes to complete. Examples include downloading a document or importing
-  // data. This class offers a balance between responsiveness, performance, and
-  // energy efficiency.
   const auto qosClass = [&]() {
     switch (Priority) {
     case ThreadPriority::Background:
@@ -323,75 +331,193 @@ static int computeHostNumHardwareThreads
                          &mask) == 0)
     return CPU_COUNT(&mask);
 #elif (defined(__linux__) || defined(__CYGWIN__))
+  // Try fixed-size affinity mask first.
   cpu_set_t Set;
   CPU_ZERO(&Set);
   if (sched_getaffinity(0, sizeof(Set), &Set) == 0)
     return CPU_COUNT(&Set);
+
+  // Fallback: dynamic mask for systems with many CPUs or where fixed-size fails.
+  long N = sysconf(_SC_NPROCESSORS_CONF);
+  if (N > 0) {
+    size_t Count = static_cast<size_t>(N);
+    size_t Bytes = CPU_ALLOC_SIZE(Count);
+    cpu_set_t *DynSet = CPU_ALLOC(Count);
+    if (DynSet) {
+      CPU_ZERO_S(Bytes, DynSet);
+      if (sched_getaffinity(0, Bytes, DynSet) == 0) {
+        int Num = CPU_COUNT_S(Bytes, DynSet);
+        CPU_FREE(DynSet);
+        return Num;
+      }
+      CPU_FREE(DynSet);
+    }
+  }
 #endif
   // Guard against std::thread::hardware_concurrency() returning 0.
   if (unsigned Val = std::thread::hardware_concurrency())
-    return Val;
+    return static_cast<int>(Val);
   return 1;
 }
 
 void llvm::ThreadPoolStrategy::apply_thread_strategy(
-    unsigned ThreadPoolNum) const {}
+    unsigned ThreadPoolNum) const {
+  (void)ThreadPoolNum;
+}
 
 llvm::BitVector llvm::get_thread_affinity_mask() {
-  // FIXME: Implement
-  llvm_unreachable("Not implemented!");
+#if defined(__linux__) || defined(__CYGWIN__)
+  // Try fixed-size affinity mask first.
+  cpu_set_t Set;
+  CPU_ZERO(&Set);
+  if (sched_getaffinity(0, sizeof(Set), &Set) == 0) {
+    llvm::BitVector BV(CPU_SETSIZE, false);
+    for (int i = 0; i < CPU_SETSIZE; ++i) {
+      if (CPU_ISSET(i, &Set))
+        BV.set(static_cast<unsigned>(i));
+    }
+    int Last = BV.find_last();
+    if (Last >= 0)
+      BV.resize(static_cast<unsigned>(Last + 1));
+    else
+      BV.clear();
+    return BV;
+  }
+
+  // Fallback: dynamic mask sized to configured processors.
+  long N = sysconf(_SC_NPROCESSORS_CONF);
+  if (N > 0) {
+    size_t Count = static_cast<size_t>(N);
+    size_t Bytes = CPU_ALLOC_SIZE(Count);
+    cpu_set_t *DynSet = CPU_ALLOC(Count);
+    if (DynSet) {
+      CPU_ZERO_S(Bytes, DynSet);
+      if (sched_getaffinity(0, Bytes, DynSet) == 0) {
+        llvm::BitVector BV(static_cast<unsigned>(Count), false);
+        for (size_t i = 0; i < Count; ++i) {
+          if (CPU_ISSET_S(i, Bytes, DynSet))
+            BV.set(static_cast<unsigned>(i));
+        }
+        CPU_FREE(DynSet);
+        int Last = BV.find_last();
+        if (Last >= 0)
+          BV.resize(static_cast<unsigned>(Last + 1));
+        else
+          BV.clear();
+        return BV;
+      }
+      CPU_FREE(DynSet);
+    }
+  }
+#endif
+  // Unknown/unsupported: return an empty mask.
+  return llvm::BitVector();
 }
 
-unsigned llvm::get_cpus() { return 1; }
+unsigned llvm::get_cpus() {
+  // Prefer physical cores: better throughput/cache locality on Raptor Lake.
+  int Physical = llvm::get_physical_cores();
+  if (Physical > 0)
+    return static_cast<unsigned>(Physical);
+
+  // Fall back to available hardware threads (respecting affinity/cgroups).
+  int HW = computeHostNumHardwareThreads();
+  if (HW > 0)
+    return static_cast<unsigned>(HW);
+
+  return 1;
+}
 
 #if (defined(__linux__) || defined(__CYGWIN__)) &&                             \
     (defined(__i386__) || defined(__x86_64__))
 // On Linux, the number of physical cores can be computed from /proc/cpuinfo,
-// using the number of unique physical/core id pairs. The following
-// implementation reads the /proc/cpuinfo format on an x86_64 system.
+// using the number of unique (physical id, core id) pairs among the CPUs
+// enabled by the process's affinity mask.
 static int computeHostNumPhysicalCores() {
-  // Enabled represents the number of physical id/core id pairs with at least
-  // one processor id enabled by the CPU affinity mask.
-  cpu_set_t Affinity, Enabled;
+  // Query current affinity (fixed-size).
+  cpu_set_t Affinity;
+  CPU_ZERO(&Affinity);
   if (sched_getaffinity(0, sizeof(Affinity), &Affinity) != 0)
     return -1;
-  CPU_ZERO(&Enabled);
 
-  // Read /proc/cpuinfo as a stream (until EOF reached). It cannot be
-  // mmapped because it appears to have 0 size.
+  // Read /proc/cpuinfo as a stream (size appears as 0; mmapping is not appropriate).
   llvm::ErrorOr<std::unique_ptr<llvm::MemoryBuffer>> Text =
       llvm::MemoryBuffer::getFileAsStream("/proc/cpuinfo");
   if (std::error_code EC = Text.getError()) {
-    llvm::errs() << "Can't read "
-                 << "/proc/cpuinfo: " << EC.message() << "\n";
-    return -1;
+    // Fallback: available hardware threads (respecting affinity).
+    cpu_set_t Set;
+    CPU_ZERO(&Set);
+    if (sched_getaffinity(0, sizeof(Set), &Set) == 0)
+      return CPU_COUNT(&Set);
+    return 1; // Last-resort default.
   }
-  SmallVector<StringRef, 8> strs;
-  (*Text)->getBuffer().split(strs, "\n", /*MaxSplit=*/-1,
-                             /*KeepEmpty=*/false);
+
+  SmallVector<StringRef, 8> Lines;
+  (*Text)->getBuffer().split(Lines, "\n", /*MaxSplit=*/-1, /*KeepEmpty=*/false);
+
+  // Track current logical CPU record state while parsing.
   int CurProcessor = -1;
   int CurPhysicalId = -1;
   int CurSiblings = -1;
   int CurCoreId = -1;
-  for (StringRef Line : strs) {
-    std::pair<StringRef, StringRef> Data = Line.split(':');
-    auto Name = Data.first.trim();
-    auto Val = Data.second.trim();
-    // These fields are available if the kernel is configured with CONFIG_SMP.
-    if (Name == "processor")
+
+  // Enabled bitset for unique (package, core) indices; maps (physical_id, core_id)
+  // to an integer index computed as (physical_id * siblings + core_id).
+  cpu_set_t Enabled;
+  CPU_ZERO(&Enabled);
+
+  for (StringRef Line : Lines) {
+    auto KV = Line.split(':');
+    StringRef Name = KV.first.trim();
+    StringRef Val = KV.second.trim();
+
+    if (Name == "processor") {
       Val.getAsInteger(10, CurProcessor);
-    else if (Name == "physical id")
+      // Reset per-CPU fields for the new logical CPU.
+      CurPhysicalId = -1;
+      CurSiblings = -1;
+      CurCoreId = -1;
+      continue;
+    }
+
+    if (Name == "physical id") {
       Val.getAsInteger(10, CurPhysicalId);
-    else if (Name == "siblings")
+      continue;
+    }
+
+    if (Name == "siblings") {
       Val.getAsInteger(10, CurSiblings);
-    else if (Name == "core id") {
+      continue;
+    }
+
+    if (Name == "core id") {
       Val.getAsInteger(10, CurCoreId);
-      // The processor id corresponds to an index into cpu_set_t.
-      if (CPU_ISSET(CurProcessor, &Affinity))
-        CPU_SET(CurPhysicalId * CurSiblings + CurCoreId, &Enabled);
+
+      // Only record when all fields are valid and this logical CPU is enabled
+      // by the current process affinity mask.
+      if (CurProcessor >= 0 && CurPhysicalId >= 0 && CurCoreId >= 0 &&
+          CurSiblings > 0 && CPU_ISSET(CurProcessor, &Affinity)) {
+        // Build a unique index per (physical_id, core_id) within the package.
+        // Using 'siblings' as the package stride is robust across typical x86 cpuinfo layouts.
+        int Index = CurPhysicalId * CurSiblings + CurCoreId;
+        if (Index >= 0 && Index < CPU_SETSIZE)
+          CPU_SET(Index, &Enabled);
+      }
+      continue;
     }
   }
-  return CPU_COUNT(&Enabled);
+
+  int Count = CPU_COUNT(&Enabled);
+  if (Count > 0)
+    return Count;
+
+  // Fallback if parsing produced no result: return available hardware threads.
+  cpu_set_t Set;
+  CPU_ZERO(&Set);
+  if (sched_getaffinity(0, sizeof(Set), &Set) == 0)
+    return CPU_COUNT(&Set);
+
+  return 1;
 }
 #elif (defined(__linux__) && defined(__s390x__)) || defined(_AIX)
 static int computeHostNumPhysicalCores() {
