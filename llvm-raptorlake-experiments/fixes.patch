--- a/llvm/lib/Transforms/IPO/WholeProgramDevirt.cpp	2025-10-19 11:42:19.902348610 +0200
+++ b/llvm/lib/Transforms/IPO/WholeProgramDevirt.cpp	2025-10-19 11:45:48.513795496 +0200
@@ -227,18 +227,22 @@ namespace {
 struct PatternList {
   std::vector<GlobPattern> Patterns;
   template <class T> void init(const T &StringList) {
-    for (const auto &S : StringList)
-      if (Expected<GlobPattern> Pat = GlobPattern::create(S))
+    for (const auto &S : StringList) {
+      if (Expected<GlobPattern> Pat = GlobPattern::create(S)) {
         Patterns.push_back(std::move(*Pat));
+      }
+    }
   }
   bool match(StringRef S) {
-    for (const GlobPattern &P : Patterns)
-      if (P.match(S))
+    for (const GlobPattern &P : Patterns) {
+      if (P.match(S)) {
         return true;
+      }
+    }
     return false;
   }
 };
-} // namespace
+} // end anonymous namespace
 
 // Find the minimum offset that we may store a value of size Size bits at. If
 // IsAfter is set, look for an offset before the object, otherwise look for an
@@ -249,10 +253,11 @@ wholeprogramdevirt::findLowestOffset(Arr
   // Find a minimum offset taking into account only vtable sizes.
   uint64_t MinByte = 0;
   for (const VirtualCallTarget &Target : Targets) {
-    if (IsAfter)
+    if (IsAfter) {
       MinByte = std::max(MinByte, Target.minAfterBytes());
-    else
+    } else {
       MinByte = std::max(MinByte, Target.minBeforeBytes());
+    }
   }
 
   // Build a vector of arrays of bytes covering, for each target, a slice of the
@@ -275,7 +280,9 @@ wholeprogramdevirt::findLowestOffset(Arr
   //
   // This code produces the slices of A, B and C that appear after the divider
   // at MinByte.
-  std::vector<ArrayRef<uint8_t>> Used;
+  SmallVector<ArrayRef<uint8_t>, 8> Used;
+  Used.reserve(Targets.size());
+
   for (const VirtualCallTarget &Target : Targets) {
     ArrayRef<uint8_t> VTUsed = IsAfter ? Target.TM->Bits->After.BytesUsed
                                        : Target.TM->Bits->Before.BytesUsed;
@@ -284,36 +291,48 @@ wholeprogramdevirt::findLowestOffset(Arr
 
     // Disregard used regions that are smaller than Offset. These are
     // effectively all-free regions that do not need to be checked.
-    if (VTUsed.size() > Offset)
+    if (VTUsed.size() > Offset) {
       Used.push_back(VTUsed.slice(Offset));
+    }
   }
 
   if (Size == 1) {
     // Find a free bit in each member of Used.
     for (unsigned I = 0;; ++I) {
       uint8_t BitsUsed = 0;
-      for (auto &&B : Used)
-        if (I < B.size())
+      for (auto &&B : Used) {
+        if (I < B.size()) {
           BitsUsed |= B[I];
-      if (BitsUsed != 0xff)
+          if (BitsUsed == 0xff) {
+            break; // Early termination - already all bits used
+          }
+        }
+      }
+      if (BitsUsed != 0xff) {
         return (MinByte + I) * 8 + llvm::countr_zero(uint8_t(~BitsUsed));
+      }
     }
   } else {
     // Find a free (Size/8) byte region in each member of Used.
-    // FIXME: see if alignment helps.
+    const uint64_t BytesNeeded = (Size + 7) / 8;
     for (unsigned I = 0;; ++I) {
+      bool FoundFreeRegion = true;
       for (auto &&B : Used) {
-        unsigned Byte = 0;
-        while ((I + Byte) < B.size() && Byte < (Size / 8)) {
-          if (B[I + Byte])
-            goto NextI;
-          ++Byte;
+        for (unsigned Byte = 0; Byte < BytesNeeded; ++Byte) {
+          if ((I + Byte) < B.size() && B[I + Byte]) {
+            FoundFreeRegion = false;
+            break;
+          }
         }
+        if (!FoundFreeRegion) {
+          break;
+        }
+      }
+      if (FoundFreeRegion) {
+        // Rounding up ensures the constant is always stored at address we
+        // can directly load from without misalignment.
+        return alignTo((MinByte + I) * 8, Size);
       }
-      // Rounding up ensures the constant is always stored at address we
-      // can directly load from without misalignment.
-      return alignTo((MinByte + I) * 8, Size);
-    NextI:;
     }
   }
 }
@@ -321,34 +340,39 @@ wholeprogramdevirt::findLowestOffset(Arr
 void wholeprogramdevirt::setBeforeReturnValues(
     MutableArrayRef<VirtualCallTarget> Targets, uint64_t AllocBefore,
     unsigned BitWidth, int64_t &OffsetByte, uint64_t &OffsetBit) {
-  if (BitWidth == 1)
-    OffsetByte = -(AllocBefore / 8 + 1);
-  else
-    OffsetByte = -((AllocBefore + 7) / 8 + (BitWidth + 7) / 8);
+  if (BitWidth == 1) {
+    OffsetByte = -(static_cast<int64_t>(AllocBefore / 8) + 1);
+  } else {
+    OffsetByte = -(static_cast<int64_t>((AllocBefore + 7) / 8) +
+                   static_cast<int64_t>((BitWidth + 7) / 8));
+  }
   OffsetBit = AllocBefore % 8;
 
   for (VirtualCallTarget &Target : Targets) {
-    if (BitWidth == 1)
+    if (BitWidth == 1) {
       Target.setBeforeBit(AllocBefore);
-    else
+    } else {
       Target.setBeforeBytes(AllocBefore, (BitWidth + 7) / 8);
+    }
   }
 }
 
 void wholeprogramdevirt::setAfterReturnValues(
     MutableArrayRef<VirtualCallTarget> Targets, uint64_t AllocAfter,
     unsigned BitWidth, int64_t &OffsetByte, uint64_t &OffsetBit) {
-  if (BitWidth == 1)
-    OffsetByte = AllocAfter / 8;
-  else
-    OffsetByte = (AllocAfter + 7) / 8;
+  if (BitWidth == 1) {
+    OffsetByte = static_cast<int64_t>(AllocAfter / 8);
+  } else {
+    OffsetByte = static_cast<int64_t>((AllocAfter + 7) / 8);
+  }
   OffsetBit = AllocAfter % 8;
 
   for (VirtualCallTarget &Target : Targets) {
-    if (BitWidth == 1)
+    if (BitWidth == 1) {
       Target.setAfterBit(AllocAfter);
-    else
+    } else {
       Target.setAfterBytes(AllocAfter, (BitWidth + 7) / 8);
+    }
   }
 }
 
@@ -418,8 +442,9 @@ template <> struct llvm::DenseMapInfo<VT
 //   2) All function summaries indicate it's unreachable
 //   3) There is no non-function with the same GUID (which is rare)
 static bool mustBeUnreachableFunction(ValueInfo TheFnVI) {
-  if (WholeProgramDevirtKeepUnreachableFunction)
+  if (WholeProgramDevirtKeepUnreachableFunction) {
     return false;
+  }
 
   if ((!TheFnVI) || TheFnVI.getSummaryList().empty()) {
     // Returns false if ValueInfo is absent, or the summary list is empty
@@ -430,15 +455,18 @@ static bool mustBeUnreachableFunction(Va
   for (const auto &Summary : TheFnVI.getSummaryList()) {
     // Conservatively returns false if any non-live functions are seen.
     // In general either all summaries should be live or all should be dead.
-    if (!Summary->isLive())
+    if (!Summary->isLive()) {
       return false;
+    }
     if (auto *FS = dyn_cast<FunctionSummary>(Summary->getBaseObject())) {
-      if (!FS->fflags().MustBeUnreachable)
+      if (!FS->fflags().MustBeUnreachable) {
         return false;
+      }
     }
     // Be conservative if a non-function has the same GUID (which is rare).
-    else
+    else {
       return false;
+    }
   }
   // All function summaries are live and all of them agree that the function is
   // unreachble.
@@ -450,7 +478,7 @@ namespace {
 // the indirect virtual call.
 struct VirtualCallSite {
   Value *VTable = nullptr;
-  CallBase &CB;
+  CallBase *CB;  // Changed from reference to pointer
 
   // If non-null, this field points to the associated unsafe use count stored in
   // the DevirtModule::NumUnsafeUsesForTypeTest map below. See the description
@@ -460,9 +488,9 @@ struct VirtualCallSite {
   void
   emitRemark(const StringRef OptName, const StringRef TargetName,
              function_ref<OptimizationRemarkEmitter &(Function &)> OREGetter) {
-    Function *F = CB.getCaller();
-    DebugLoc DLoc = CB.getDebugLoc();
-    BasicBlock *Block = CB.getParent();
+    Function *F = CB->getCaller();
+    DebugLoc DLoc = CB->getDebugLoc();
+    BasicBlock *Block = CB->getParent();
 
     using namespace ore;
     OREGetter(*F).emit(OptimizationRemark(DEBUG_TYPE, OptName, DLoc, Block)
@@ -475,17 +503,19 @@ struct VirtualCallSite {
       const StringRef OptName, const StringRef TargetName, bool RemarksEnabled,
       function_ref<OptimizationRemarkEmitter &(Function &)> OREGetter,
       Value *New) {
-    if (RemarksEnabled)
+    if (RemarksEnabled) {
       emitRemark(OptName, TargetName, OREGetter);
-    CB.replaceAllUsesWith(New);
-    if (auto *II = dyn_cast<InvokeInst>(&CB)) {
-      BranchInst::Create(II->getNormalDest(), CB.getIterator());
+    }
+    CB->replaceAllUsesWith(New);
+    if (auto *II = dyn_cast<InvokeInst>(CB)) {
+      BranchInst::Create(II->getNormalDest(), CB->getIterator());
       II->getUnwindDest()->removePredecessor(II->getParent());
     }
-    CB.eraseFromParent();
+    CB->eraseFromParent();
     // This use is no longer unsafe.
-    if (NumUnsafeUses)
+    if (NumUnsafeUses) {
       --*NumUnsafeUses;
+    }
   }
 };
 
@@ -497,7 +527,7 @@ struct CallSiteInfo {
   /// import phase of ThinLTO (as well as the export phase of ThinLTO for any
   /// call sites that appear in the merged module itself); in each of these
   /// cases we are directly operating on the call sites at the IR level.
-  std::vector<VirtualCallSite> CallSites;
+  SmallVector<VirtualCallSite, 4> CallSites;
 
   /// Whether all call sites represented by this CallSiteInfo, including those
   /// in summaries, have been devirtualized. This starts off as true because a
@@ -558,12 +588,14 @@ private:
 CallSiteInfo &VTableSlotInfo::findCallSiteInfo(CallBase &CB) {
   std::vector<uint64_t> Args;
   auto *CBType = dyn_cast<IntegerType>(CB.getType());
-  if (!CBType || CBType->getBitWidth() > 64 || CB.arg_empty())
+  if (!CBType || CBType->getBitWidth() > 64 || CB.arg_empty()) {
     return CSInfo;
+  }
   for (auto &&Arg : drop_begin(CB.args())) {
     auto *CI = dyn_cast<ConstantInt>(Arg);
-    if (!CI || CI->getBitWidth() > 64)
+    if (!CI || CI->getBitWidth() > 64) {
       return CSInfo;
+    }
     Args.push_back(CI->getZExtValue());
   }
   return ConstCSInfo[Args];
@@ -573,7 +605,7 @@ void VTableSlotInfo::addCallSite(Value *
                                  unsigned *NumUnsafeUses) {
   auto &CSI = findCallSiteInfo(CB);
   CSI.AllCallSitesDevirted = false;
-  CSI.CallSites.push_back({VTable, CB, NumUnsafeUses});
+  CSI.CallSites.push_back({VTable, &CB, NumUnsafeUses});  // Pass address
 }
 
 struct DevirtModule {
@@ -790,12 +822,14 @@ struct DevirtIndex {
 PreservedAnalyses WholeProgramDevirtPass::run(Module &M,
                                               ModuleAnalysisManager &MAM) {
   if (UseCommandLine) {
-    if (!DevirtModule::runForTesting(M, MAM))
+    if (!DevirtModule::runForTesting(M, MAM)) {
       return PreservedAnalyses::all();
+    }
     return PreservedAnalyses::none();
   }
-  if (!DevirtModule(M, MAM, ExportSummary, ImportSummary).run())
+  if (!DevirtModule(M, MAM, ExportSummary, ImportSummary).run()) {
     return PreservedAnalyses::all();
+  }
   return PreservedAnalyses::none();
 }
 
@@ -812,14 +846,16 @@ typeIDVisibleToRegularObj(StringRef Type
   // TypeID for member function pointer type is an internal construct
   // and won't exist in IsVisibleToRegularObj. The full TypeID
   // will be present and participate in invalidation.
-  if (TypeID.ends_with(".virtual"))
+  if (TypeID.ends_with(".virtual")) {
     return false;
+  }
 
   // TypeID that doesn't start with Itanium mangling (_ZTS) will be
   // non-externally visible types which cannot interact with
   // external native files. See CodeGenModule::CreateMetadataIdentifierImpl.
-  if (!TypeID.consume_front("_ZTS"))
+  if (!TypeID.consume_front("_ZTS")) {
     return false;
+  }
 
   // TypeID is keyed off the type name symbol (_ZTS). However, the native
   // object may not contain this symbol if it does not contain a key
@@ -836,10 +872,14 @@ skipUpdateDueToValidation(GlobalVariable
   SmallVector<MDNode *, 2> Types;
   GV.getMetadata(LLVMContext::MD_type, Types);
 
-  for (auto *Type : Types)
-    if (auto *TypeID = dyn_cast<MDString>(Type->getOperand(1).get()))
-      return typeIDVisibleToRegularObj(TypeID->getString(),
-                                       IsVisibleToRegularObj);
+  for (auto *Type : Types) {
+    if (auto *TypeID = dyn_cast<MDString>(Type->getOperand(1).get())) {
+      if (typeIDVisibleToRegularObj(TypeID->getString(),
+                                    IsVisibleToRegularObj)) {
+        return true;
+      }
+    }
+  }
 
   return false;
 }
@@ -852,8 +892,9 @@ void llvm::updateVCallVisibilityInModule
     const DenseSet<GlobalValue::GUID> &DynamicExportSymbols,
     bool ValidateAllVtablesHaveTypeInfos,
     function_ref<bool(StringRef)> IsVisibleToRegularObj) {
-  if (!hasWholeProgramVisibility(WholeProgramVisibilityEnabledInLTO))
+  if (!hasWholeProgramVisibility(WholeProgramVisibilityEnabledInLTO)) {
     return;
+  }
   for (GlobalVariable &GV : M.globals()) {
     // Add linkage unit visibility to any variable with type metadata, which are
     // the vtable definitions. We won't have an existing vcall_visibility
@@ -868,8 +909,9 @@ void llvm::updateVCallVisibilityInModule
         // current implementation but those with VCallVisibilityTranslationUnit
         // will have already been marked in clang so are unaffected.
         !(ValidateAllVtablesHaveTypeInfos &&
-          skipUpdateDueToValidation(GV, IsVisibleToRegularObj)))
+          skipUpdateDueToValidation(GV, IsVisibleToRegularObj))) {
       GV.setVCallVisibilityMetadata(GlobalObject::VCallVisibilityLinkageUnit);
+    }
   }
 }
 
@@ -878,8 +920,9 @@ void llvm::updatePublicTypeTestCalls(Mod
   llvm::TimeTraceScope timeScope("Update public type test calls");
   Function *PublicTypeTestFunc =
       Intrinsic::getDeclarationIfExists(&M, Intrinsic::public_type_test);
-  if (!PublicTypeTestFunc)
+  if (!PublicTypeTestFunc) {
     return;
+  }
   if (hasWholeProgramVisibility(WholeProgramVisibilityEnabledInLTO)) {
     Function *TypeTestFunc =
         Intrinsic::getOrInsertDeclaration(&M, Intrinsic::type_test);
@@ -908,9 +951,11 @@ void llvm::getVisibleToRegularObjVtableG
     DenseSet<GlobalValue::GUID> &VisibleToRegularObjSymbols,
     function_ref<bool(StringRef)> IsVisibleToRegularObj) {
   for (const auto &TypeID : Index.typeIdCompatibleVtableMap()) {
-    if (typeIDVisibleToRegularObj(TypeID.first, IsVisibleToRegularObj))
-      for (const TypeIdOffsetVtableInfo &P : TypeID.second)
+    if (typeIDVisibleToRegularObj(TypeID.first, IsVisibleToRegularObj)) {
+      for (const TypeIdOffsetVtableInfo &P : TypeID.second) {
         VisibleToRegularObjSymbols.insert(P.VTableVI.getGUID());
+      }
+    }
   }
 }
 
@@ -921,24 +966,28 @@ void llvm::updateVCallVisibilityInIndex(
     ModuleSummaryIndex &Index, bool WholeProgramVisibilityEnabledInLTO,
     const DenseSet<GlobalValue::GUID> &DynamicExportSymbols,
     const DenseSet<GlobalValue::GUID> &VisibleToRegularObjSymbols) {
-  if (!hasWholeProgramVisibility(WholeProgramVisibilityEnabledInLTO))
+  if (!hasWholeProgramVisibility(WholeProgramVisibilityEnabledInLTO)) {
     return;
+  }
   for (auto &P : Index) {
     // Don't upgrade the visibility for symbols exported to the dynamic
     // linker, as we have no information on their eventual use.
-    if (DynamicExportSymbols.count(P.first))
+    if (DynamicExportSymbols.count(P.first)) {
       continue;
+    }
     for (auto &S : P.second.SummaryList) {
       auto *GVar = dyn_cast<GlobalVarSummary>(S.get());
       if (!GVar ||
-          GVar->getVCallVisibility() != GlobalObject::VCallVisibilityPublic)
+          GVar->getVCallVisibility() != GlobalObject::VCallVisibilityPublic) {
         continue;
+      }
       // With validation enabled, we want to exclude symbols visible to regular
       // objects. Local symbols will be in this group due to the current
       // implementation but those with VCallVisibilityTranslationUnit will have
       // already been marked in clang so are unaffected.
-      if (VisibleToRegularObjSymbols.count(P.first))
+      if (VisibleToRegularObjSymbols.count(P.first)) {
         continue;
+      }
       GVar->setVCallVisibility(GlobalObject::VCallVisibilityLinkageUnit);
     }
   }
@@ -960,8 +1009,9 @@ void llvm::updateIndexWPDForExports(
     assert(VI.getSummaryList().size() == 1 &&
            "Devirt of local target has more than one copy");
     auto &S = VI.getSummaryList()[0];
-    if (!IsExported(S->modulePath(), VI))
+    if (!IsExported(S->modulePath(), VI)) {
       continue;
+    }
 
     // It's been exported by a cross module import.
     for (auto &SlotSummary : T.second) {
@@ -983,10 +1033,11 @@ static Error checkCombinedSummaryForTest
   // DevirtIndex::run, not to DevirtModule::run used by opt/runForTesting.
   const auto &ModPaths = Summary->modulePaths();
   if (ClSummaryAction != PassSummaryAction::Import &&
-      !ModPaths.contains(ModuleSummaryIndex::getRegularLTOModuleName()))
+      !ModPaths.contains(ModuleSummaryIndex::getRegularLTOModuleName())) {
     return createStringError(
         errc::invalid_argument,
         "combined summary should contain Regular LTO module");
+  }
   return ErrorSuccess();
 }
 
@@ -1045,13 +1096,16 @@ void DevirtModule::buildTypeIdentifierMa
     std::vector<VTableBits> &Bits,
     DenseMap<Metadata *, std::set<TypeMemberInfo>> &TypeIdMap) {
   DenseMap<GlobalVariable *, VTableBits *> GVToBits;
-  Bits.reserve(M.global_size());
+  const size_t GlobalCount = M.global_size();
+  Bits.reserve(GlobalCount);
+  GVToBits.reserve(GlobalCount);
   SmallVector<MDNode *, 2> Types;
   for (GlobalVariable &GV : M.globals()) {
     Types.clear();
     GV.getMetadata(LLVMContext::MD_type, Types);
-    if (GV.isDeclaration() || Types.empty())
+    if (GV.isDeclaration() || Types.empty()) {
       continue;
+    }
 
     VTableBits *&BitsPtr = GVToBits[&GV];
     if (!BitsPtr) {
@@ -1080,35 +1134,41 @@ bool DevirtModule::tryFindVirtualCallTar
     const std::set<TypeMemberInfo> &TypeMemberInfos, uint64_t ByteOffset,
     ModuleSummaryIndex *ExportSummary) {
   for (const TypeMemberInfo &TM : TypeMemberInfos) {
-    if (!TM.Bits->GV->isConstant())
+    if (__builtin_expect(!TM.Bits->GV->isConstant(), 0)) {
       return false;
+    }
 
     // We cannot perform whole program devirtualization analysis on a vtable
     // with public LTO visibility.
-    if (TM.Bits->GV->getVCallVisibility() ==
-        GlobalObject::VCallVisibilityPublic)
+    if (__builtin_expect(TM.Bits->GV->getVCallVisibility() ==
+                         GlobalObject::VCallVisibilityPublic, 0)) {
       return false;
+    }
 
     Function *Fn = nullptr;
     Constant *C = nullptr;
     std::tie(Fn, C) =
         getFunctionAtVTableOffset(TM.Bits->GV, TM.Offset + ByteOffset, M);
 
-    if (!Fn)
+    if (__builtin_expect(!Fn, 0)) {
       return false;
+    }
 
-    if (FunctionsToSkip.match(Fn->getName()))
+    if (__builtin_expect(FunctionsToSkip.match(Fn->getName()), 0)) {
       return false;
+    }
 
     // We can disregard __cxa_pure_virtual as a possible call target, as
     // calls to pure virtuals are UB.
-    if (Fn->getName() == "__cxa_pure_virtual")
+    if (Fn->getName() == "__cxa_pure_virtual") {
       continue;
+    }
 
     // We can disregard unreachable functions as possible call targets, as
     // unreachable functions shouldn't be called.
-    if (mustBeUnreachableFunction(Fn, ExportSummary))
+    if (mustBeUnreachableFunction(Fn, ExportSummary)) {
       continue;
+    }
 
     // Save the symbol used in the vtable to use as the devirtualization
     // target.
@@ -1118,7 +1178,7 @@ bool DevirtModule::tryFindVirtualCallTar
   }
 
   // Give up if we couldn't find any targets.
-  return !TargetsForSlot.empty();
+  return __builtin_expect(!TargetsForSlot.empty(), 1);
 }
 
 bool DevirtIndex::tryFindVirtualCallTargets(
@@ -1139,9 +1199,15 @@ bool DevirtIndex::tryFindVirtualCallTarg
     bool LocalFound = false;
     for (const auto &S : P.VTableVI.getSummaryList()) {
       if (GlobalValue::isLocalLinkage(S->linkage())) {
-        if (LocalFound)
+        if (LocalFound) {
           return false;
+        }
         LocalFound = true;
+      } else {
+        // Don't expect to find a mix of locals and non-locals (due to path
+        // prefix for locals one should never have the same GUID as a
+        // non-local).
+        assert(!LocalFound);
       }
       auto *CurVS = cast<GlobalVarSummary>(S->getBaseObject());
       if (!CurVS->vTableFuncs().empty() ||
@@ -1156,22 +1222,32 @@ bool DevirtIndex::tryFindVirtualCallTarg
         VS = CurVS;
         // We cannot perform whole program devirtualization analysis on a vtable
         // with public LTO visibility.
-        if (VS->getVCallVisibility() == GlobalObject::VCallVisibilityPublic)
+        if (VS->getVCallVisibility() == GlobalObject::VCallVisibilityPublic) {
           return false;
+        }
+        // Unless VS is a local, we don't need to keep looking through the rest
+        // of the summaries.
+        if (!LocalFound) {
+          break;
+        }
       }
     }
     // There will be no VS if all copies are available_externally having no
     // type metadata. In that case we can't safely perform WPD.
-    if (!VS)
+    if (!VS) {
       return false;
-    if (!VS->isLive())
+    }
+    if (!VS->isLive()) {
       continue;
+    }
     for (auto VTP : VS->vTableFuncs()) {
-      if (VTP.VTableOffset != P.AddressPointOffset + ByteOffset)
+      if (VTP.VTableOffset != P.AddressPointOffset + ByteOffset) {
         continue;
+      }
 
-      if (mustBeUnreachableFunction(VTP.FuncVI))
+      if (mustBeUnreachableFunction(VTP.FuncVI)) {
         continue;
+      }
 
       TargetsForSlot.push_back(VTP.FuncVI);
     }
@@ -1185,24 +1261,28 @@ void DevirtModule::applySingleImplDevirt
                                          Constant *TheFn, bool &IsExported) {
   // Don't devirtualize function if we're told to skip it
   // in -wholeprogramdevirt-skip.
-  if (FunctionsToSkip.match(TheFn->stripPointerCasts()->getName()))
+  if (FunctionsToSkip.match(TheFn->stripPointerCasts()->getName())) {
     return;
+  }
   auto Apply = [&](CallSiteInfo &CSInfo) {
     for (auto &&VCallSite : CSInfo.CallSites) {
-      if (!OptimizedCalls.insert(&VCallSite.CB).second)
+      if (!OptimizedCalls.insert(VCallSite.CB).second) {
         continue;
+      }
 
       // Stop when the number of devirted calls reaches the cutoff.
       if (WholeProgramDevirtCutoff.getNumOccurrences() > 0 &&
-          NumDevirtCalls >= WholeProgramDevirtCutoff)
+          NumDevirtCalls >= WholeProgramDevirtCutoff) {
         return;
+      }
 
-      if (RemarksEnabled)
+      if (RemarksEnabled) {
         VCallSite.emitRemark("single-impl",
                              TheFn->stripPointerCasts()->getName(), OREGetter);
+      }
       NumSingleImpl++;
       NumDevirtCalls++;
-      auto &CB = VCallSite.CB;
+      CallBase &CB = *VCallSite.CB;
       assert(!CB.getCalledFunction() && "devirtualizing direct call?");
       IRBuilder<> Builder(&CB);
       Value *Callee =
@@ -1226,7 +1306,7 @@ void DevirtModule::applySingleImplDevirt
       // If fallback checking is enabled, add support to compare the virtual
       // function pointer to the devirtualized target. In case of a mismatch,
       // fall back to indirect call.
-      if (DevirtCheckMode == WPDCheckMode::Fallback) {
+      else if (DevirtCheckMode == WPDCheckMode::Fallback) {
         MDNode *Weights = MDBuilder(M.getContext()).createLikelyBranchWeights();
         // Version the indirect call site. If the called value is equal to the
         // given callee, 'NewInst' will be executed, otherwise the original call
@@ -1264,22 +1344,26 @@ void DevirtModule::applySingleImplDevirt
       }
 
       // This use is no longer unsafe.
-      if (VCallSite.NumUnsafeUses)
+      if (VCallSite.NumUnsafeUses) {
         --*VCallSite.NumUnsafeUses;
+      }
     }
-    if (CSInfo.isExported())
+    if (CSInfo.isExported()) {
       IsExported = true;
+    }
     CSInfo.markDevirt();
   };
   Apply(SlotInfo.CSInfo);
-  for (auto &P : SlotInfo.ConstCSInfo)
+  for (auto &P : SlotInfo.ConstCSInfo) {
     Apply(P.second);
+  }
 }
 
 static bool addCalls(VTableSlotInfo &SlotInfo, const ValueInfo &Callee) {
   // We can't add calls if we haven't seen a definition
-  if (Callee.getSummaryList().empty())
+  if (Callee.getSummaryList().empty()) {
     return false;
+  }
 
   // Insert calls into the summary index so that the devirtualized targets
   // are eligible for import.
@@ -1300,8 +1384,9 @@ static bool addCalls(VTableSlotInfo &Slo
     }
   };
   AddCalls(SlotInfo.CSInfo);
-  for (auto &P : SlotInfo.ConstCSInfo)
+  for (auto &P : SlotInfo.ConstCSInfo) {
     AddCalls(P.second);
+  }
   return IsExported;
 }
 
@@ -1312,18 +1397,22 @@ bool DevirtModule::trySingleImplDevirt(
   // See if the program contains a single implementation of this virtual
   // function.
   auto *TheFn = TargetsForSlot[0].Fn;
-  for (auto &&Target : TargetsForSlot)
-    if (TheFn != Target.Fn)
+  for (auto &&Target : TargetsForSlot) {
+    if (TheFn != Target.Fn) {
       return false;
+    }
+  }
 
   // If so, update each call site to call that implementation directly.
-  if (RemarksEnabled || AreStatisticsEnabled())
+  if (RemarksEnabled || AreStatisticsEnabled()) {
     TargetsForSlot[0].WasDevirt = true;
+  }
 
   bool IsExported = false;
   applySingleImplDevirt(SlotInfo, TheFn, IsExported);
-  if (!IsExported)
+  if (!IsExported) {
     return false;
+  }
 
   // If the only implementation has local linkage, we must promote to external
   // to make it visible to thin LTO objects. We can only get here during the
@@ -1338,9 +1427,11 @@ bool DevirtModule::trySingleImplDevirt(
       if (C->getName() == TheFn->getName()) {
         Comdat *NewC = M.getOrInsertComdat(NewName);
         NewC->setSelectionKind(C->getSelectionKind());
-        for (GlobalObject &GO : M.global_objects())
-          if (GO.getComdat() == C)
+        for (GlobalObject &GO : M.global_objects()) {
+          if (GO.getComdat() == C) {
             GO.setComdat(NewC);
+          }
+        }
       }
     }
 
@@ -1348,10 +1439,11 @@ bool DevirtModule::trySingleImplDevirt(
     TheFn->setVisibility(GlobalValue::HiddenVisibility);
     TheFn->setName(NewName);
   }
-  if (ValueInfo TheFnVI = ExportSummary->getValueInfo(TheFn->getGUID()))
+  if (ValueInfo TheFnVI = ExportSummary->getValueInfo(TheFn->getGUID())) {
     // Any needed promotion of 'TheFn' has already been done during
     // LTO unit split, so we can ignore return value of AddCalls.
     addCalls(SlotInfo, TheFnVI);
+  }
 
   Res->TheKind = WholeProgramDevirtResolution::SingleImpl;
   Res->SingleImplName = std::string(TheFn->getName());
@@ -1367,51 +1459,60 @@ bool DevirtIndex::trySingleImplDevirt(Mu
   // See if the program contains a single implementation of this virtual
   // function.
   auto TheFn = TargetsForSlot[0];
-  for (auto &&Target : TargetsForSlot)
-    if (TheFn != Target)
+  for (auto &&Target : TargetsForSlot) {
+    if (TheFn != Target) {
       return false;
+    }
+  }
 
   // Don't devirtualize if we don't have target definition.
   auto Size = TheFn.getSummaryList().size();
-  if (!Size)
+  if (!Size) {
     return false;
+  }
 
   // Don't devirtualize function if we're told to skip it
   // in -wholeprogramdevirt-skip.
-  if (FunctionsToSkip.match(TheFn.name()))
+  if (FunctionsToSkip.match(TheFn.name())) {
     return false;
+  }
 
   // If the summary list contains multiple summaries where at least one is
   // a local, give up, as we won't know which (possibly promoted) name to use.
-  for (const auto &S : TheFn.getSummaryList())
-    if (GlobalValue::isLocalLinkage(S->linkage()) && Size > 1)
+  for (const auto &S : TheFn.getSummaryList()) {
+    if (GlobalValue::isLocalLinkage(S->linkage()) && Size > 1) {
       return false;
+    }
+  }
 
   // Collect functions devirtualized at least for one call site for stats.
-  if (PrintSummaryDevirt || AreStatisticsEnabled())
+  if (PrintSummaryDevirt || AreStatisticsEnabled()) {
     DevirtTargets.insert(TheFn);
+  }
 
   auto &S = TheFn.getSummaryList()[0];
   bool IsExported = addCalls(SlotInfo, TheFn);
-  if (IsExported)
+  if (IsExported) {
     ExportedGUIDs.insert(TheFn.getGUID());
+  }
 
   // Record in summary for use in devirtualization during the ThinLTO import
   // step.
   Res->TheKind = WholeProgramDevirtResolution::SingleImpl;
   if (GlobalValue::isLocalLinkage(S->linkage())) {
-    if (IsExported)
+    if (IsExported) {
       // If target is a local function and we are exporting it by
       // devirtualizing a call in another module, we need to record the
       // promoted name.
       Res->SingleImplName = ModuleSummaryIndex::getGlobalNameForLocal(
           TheFn.name(), ExportSummary.getModuleHash(S->modulePath()));
-    else {
+    } else {
       LocalWPDTargetsMap[TheFn].push_back(SlotSummary);
       Res->SingleImplName = std::string(TheFn.name());
     }
-  } else
+  } else {
     Res->SingleImplName = std::string(TheFn.name());
+  }
 
   // Name will be empty if this thin link driven off of serialized combined
   // index (e.g. llvm-lto). However, WPD is not supported/invoked for the
@@ -1425,22 +1526,27 @@ void DevirtModule::tryICallBranchFunnel(
     MutableArrayRef<VirtualCallTarget> TargetsForSlot, VTableSlotInfo &SlotInfo,
     WholeProgramDevirtResolution *Res, VTableSlot Slot) {
   Triple T(M.getTargetTriple());
-  if (T.getArch() != Triple::x86_64)
+  if (T.getArch() != Triple::x86_64) {
     return;
+  }
 
-  if (TargetsForSlot.size() > ClThreshold)
+  if (TargetsForSlot.size() > ClThreshold) {
     return;
+  }
 
   bool HasNonDevirt = !SlotInfo.CSInfo.AllCallSitesDevirted;
-  if (!HasNonDevirt)
-    for (auto &P : SlotInfo.ConstCSInfo)
+  if (!HasNonDevirt) {
+    for (auto &P : SlotInfo.ConstCSInfo) {
       if (!P.second.AllCallSitesDevirted) {
         HasNonDevirt = true;
         break;
       }
+    }
+  }
 
-  if (!HasNonDevirt)
+  if (!HasNonDevirt) {
     return;
+  }
 
   // If any GV is AvailableExternally, not to generate branch.funnel.
   // NOTE: It is to avoid crash in LowerTypeTest.
@@ -1454,8 +1560,9 @@ void DevirtModule::tryICallBranchFunnel(
   // or SelectionDAGBuilder later, because operands linkage type consistency
   // check of icall.branch.funnel can not pass.
   for (auto &T : TargetsForSlot) {
-    if (T.TM->Bits->GV->hasAvailableExternallyLinkage())
+    if (T.TM->Bits->GV->hasAvailableExternallyLinkage()) {
       return;
+    }
   }
 
   FunctionType *FT =
@@ -1490,8 +1597,9 @@ void DevirtModule::tryICallBranchFunnel(
 
   bool IsExported = false;
   applyICallBranchFunnel(SlotInfo, *JT, IsExported);
-  if (IsExported)
+  if (IsExported) {
     Res->TheKind = WholeProgramDevirtResolution::BranchFunnel;
+  }
 
   if (!JT->getEntryCount().has_value()) {
     // FIXME: we could pass through thinlto the necessary information.
@@ -1503,16 +1611,18 @@ void DevirtModule::applyICallBranchFunne
                                           Function &JT, bool &IsExported) {
   DenseMap<Function *, double> FunctionEntryCounts;
   auto Apply = [&](CallSiteInfo &CSInfo) {
-    if (CSInfo.isExported())
+    if (CSInfo.isExported()) {
       IsExported = true;
-    if (CSInfo.AllCallSitesDevirted)
+    }
+    if (CSInfo.AllCallSitesDevirted) {
       return;
+    }
 
     std::map<CallBase *, CallBase *> CallBases;
     for (auto &&VCallSite : CSInfo.CallSites) {
-      CallBase &CB = VCallSite.CB;
+      CallBase *CB = VCallSite.CB;
 
-      if (CallBases.find(&CB) != CallBases.end()) {
+      if (CallBases.find(CB) != CallBases.end()) {
         // When finding devirtualizable calls, it's possible to find the same
         // vtable passed to multiple llvm.type.test or llvm.type.checked.load
         // calls, which can cause duplicate call sites to be recorded in
@@ -1522,33 +1632,35 @@ void DevirtModule::applyICallBranchFunne
       }
 
       // Jump tables are only profitable if the retpoline mitigation is enabled.
-      Attribute FSAttr = CB.getCaller()->getFnAttribute("target-features");
+      Attribute FSAttr = CB->getCaller()->getFnAttribute("target-features");
       if (!FSAttr.isValid() ||
-          !FSAttr.getValueAsString().contains("+retpoline"))
+          !FSAttr.getValueAsString().contains("+retpoline")) {
         continue;
+      }
 
       NumBranchFunnel++;
-      if (RemarksEnabled)
+      if (RemarksEnabled) {
         VCallSite.emitRemark("branch-funnel", JT.getName(), OREGetter);
+      }
 
       // Pass the address of the vtable in the nest register, which is r10 on
       // x86_64.
       std::vector<Type *> NewArgs;
       NewArgs.push_back(Int8PtrTy);
-      append_range(NewArgs, CB.getFunctionType()->params());
+      append_range(NewArgs, CB->getFunctionType()->params());
       FunctionType *NewFT =
-          FunctionType::get(CB.getFunctionType()->getReturnType(), NewArgs,
-                            CB.getFunctionType()->isVarArg());
-      IRBuilder<> IRB(&CB);
+          FunctionType::get(CB->getFunctionType()->getReturnType(), NewArgs,
+                            CB->getFunctionType()->isVarArg());
+      IRBuilder<> IRB(CB);
       std::vector<Value *> Args;
       Args.push_back(VCallSite.VTable);
-      llvm::append_range(Args, CB.args());
+      llvm::append_range(Args, CB->args());
 
       CallBase *NewCS = nullptr;
       if (!JT.isDeclaration() && !ProfcheckDisableMetadataFixes) {
         // Accumulate the call frequencies of the original call site, and use
         // that as total entry count for the funnel function.
-        auto &F = *CB.getCaller();
+        auto &F = *CB->getCaller();
         auto &BFI = FAM.getResult<BlockFrequencyAnalysis>(F);
         auto EC = BFI.getBlockFreq(&F.getEntryBlock());
         auto CC = F.getEntryCount(/*AllowSynthetic=*/true);
@@ -1556,36 +1668,39 @@ void DevirtModule::applyICallBranchFunne
         if (EC.getFrequency() != 0 && CC && CC->getCount() != 0) {
           double CallFreq =
               static_cast<double>(
-                  BFI.getBlockFreq(CB.getParent()).getFrequency()) /
+                  BFI.getBlockFreq(CB->getParent()).getFrequency()) /
               EC.getFrequency();
           CallCount = CallFreq * CC->getCount();
         }
         FunctionEntryCounts[&JT] += CallCount;
       }
-      if (isa<CallInst>(CB))
+      if (isa<CallInst>(CB)) {
         NewCS = IRB.CreateCall(NewFT, &JT, Args);
-      else
+      } else {
         NewCS =
-            IRB.CreateInvoke(NewFT, &JT, cast<InvokeInst>(CB).getNormalDest(),
-                             cast<InvokeInst>(CB).getUnwindDest(), Args);
-      NewCS->setCallingConv(CB.getCallingConv());
+            IRB.CreateInvoke(NewFT, &JT, cast<InvokeInst>(CB)->getNormalDest(),
+                             cast<InvokeInst>(CB)->getUnwindDest(), Args);
+      }
+      NewCS->setCallingConv(CB->getCallingConv());
 
-      AttributeList Attrs = CB.getAttributes();
+      AttributeList Attrs = CB->getAttributes();
       std::vector<AttributeSet> NewArgAttrs;
       NewArgAttrs.push_back(AttributeSet::get(
           M.getContext(), ArrayRef<Attribute>{Attribute::get(
                               M.getContext(), Attribute::Nest)}));
-      for (unsigned I = 0; I + 2 <  Attrs.getNumAttrSets(); ++I)
+      for (unsigned I = 0; I + 2 < Attrs.getNumAttrSets(); ++I) {
         NewArgAttrs.push_back(Attrs.getParamAttrs(I));
+      }
       NewCS->setAttributes(
           AttributeList::get(M.getContext(), Attrs.getFnAttrs(),
                              Attrs.getRetAttrs(), NewArgAttrs));
 
-      CallBases[&CB] = NewCS;
+      CallBases[CB] = NewCS;
 
       // This use is no longer unsafe.
-      if (VCallSite.NumUnsafeUses)
+      if (VCallSite.NumUnsafeUses) {
         --*VCallSite.NumUnsafeUses;
+      }
     }
     // Don't mark as devirtualized because there may be callers compiled without
     // retpoline mitigation, which would mean that they are lowered to
@@ -1598,8 +1713,9 @@ void DevirtModule::applyICallBranchFunne
     }
   };
   Apply(SlotInfo.CSInfo);
-  for (auto &P : SlotInfo.ConstCSInfo)
+  for (auto &P : SlotInfo.ConstCSInfo) {
     Apply(P.second);
+  }
   for (auto &[F, C] : FunctionEntryCounts) {
     assert(!F->getEntryCount(/*AllowSynthetic=*/true) &&
            "Unexpected entry count for funnel that was freshly synthesized");
@@ -1617,11 +1733,13 @@ bool DevirtModule::tryEvaluateFunctionsW
     // need to evaluate whether it would be correct to analyze the aliasee
     // function for this optimization.
     auto *Fn = dyn_cast<Function>(Target.Fn);
-    if (!Fn)
+    if (!Fn) {
       return false;
+    }
 
-    if (Fn->arg_size() != Args.size() + 1)
+    if (Fn->arg_size() != Args.size() + 1) {
       return false;
+    }
 
     Evaluator Eval(M.getDataLayout(), nullptr);
     SmallVector<Constant *, 2> EvalArgs;
@@ -1630,15 +1748,17 @@ bool DevirtModule::tryEvaluateFunctionsW
     for (unsigned I = 0; I != Args.size(); ++I) {
       auto *ArgTy =
           dyn_cast<IntegerType>(Fn->getFunctionType()->getParamType(I + 1));
-      if (!ArgTy)
+      if (!ArgTy) {
         return false;
+      }
       EvalArgs.push_back(ConstantInt::get(ArgTy, Args[I]));
     }
 
     Constant *RetVal;
     if (!Eval.EvaluateFunction(Fn, RetVal, EvalArgs) ||
-        !isa<ConstantInt>(RetVal))
+        !isa<ConstantInt>(RetVal)) {
       return false;
+    }
     Target.RetVal = cast<ConstantInt>(RetVal)->getZExtValue();
   }
   return true;
@@ -1647,12 +1767,13 @@ bool DevirtModule::tryEvaluateFunctionsW
 void DevirtModule::applyUniformRetValOpt(CallSiteInfo &CSInfo, StringRef FnName,
                                          uint64_t TheRetVal) {
   for (auto Call : CSInfo.CallSites) {
-    if (!OptimizedCalls.insert(&Call.CB).second)
+    if (!OptimizedCalls.insert(Call.CB).second) {
       continue;
+    }
     NumUniformRetVal++;
     Call.replaceAndErase(
         "uniform-ret-val", FnName, RemarksEnabled, OREGetter,
-        ConstantInt::get(cast<IntegerType>(Call.CB.getType()), TheRetVal));
+        ConstantInt::get(cast<IntegerType>(Call.CB->getType()), TheRetVal));
   }
   CSInfo.markDevirt();
 }
@@ -1663,9 +1784,11 @@ bool DevirtModule::tryUniformRetValOpt(
   // Uniform return value optimization. If all functions return the same
   // constant, replace all calls with that constant.
   uint64_t TheRetVal = TargetsForSlot[0].RetVal;
-  for (const VirtualCallTarget &Target : TargetsForSlot)
-    if (Target.RetVal != TheRetVal)
+  for (const VirtualCallTarget &Target : TargetsForSlot) {
+    if (Target.RetVal != TheRetVal) {
       return false;
+    }
+  }
 
   if (CSInfo.isExported()) {
     Res->TheKind = WholeProgramDevirtResolution::ByArg::UniformRetVal;
@@ -1673,22 +1796,26 @@ bool DevirtModule::tryUniformRetValOpt(
   }
 
   applyUniformRetValOpt(CSInfo, TargetsForSlot[0].Fn->getName(), TheRetVal);
-  if (RemarksEnabled || AreStatisticsEnabled())
-    for (auto &&Target : TargetsForSlot)
+  if (RemarksEnabled || AreStatisticsEnabled()) {
+    for (auto &&Target : TargetsForSlot) {
       Target.WasDevirt = true;
+    }
+  }
   return true;
 }
 
 std::string DevirtModule::getGlobalName(VTableSlot Slot,
                                         ArrayRef<uint64_t> Args,
                                         StringRef Name) {
-  std::string FullName = "__typeid_";
-  raw_string_ostream OS(FullName);
+  SmallString<128> FullName;
+  FullName += "__typeid_";
+  raw_svector_ostream OS(FullName);
   OS << cast<MDString>(Slot.TypeID)->getString() << '_' << Slot.ByteOffset;
-  for (uint64_t Arg : Args)
+  for (uint64_t Arg : Args) {
     OS << '_' << Arg;
+  }
   OS << '_' << Name;
-  return FullName;
+  return std::string(FullName);
 }
 
 bool DevirtModule::shouldExportConstantsAsAbsoluteSymbols() {
@@ -1727,8 +1854,9 @@ Constant *DevirtModule::importGlobal(VTa
 Constant *DevirtModule::importConstant(VTableSlot Slot, ArrayRef<uint64_t> Args,
                                        StringRef Name, IntegerType *IntTy,
                                        uint32_t Storage) {
-  if (!shouldExportConstantsAsAbsoluteSymbols())
+  if (!shouldExportConstantsAsAbsoluteSymbols()) {
     return ConstantInt::get(IntTy, Storage);
+  }
 
   Constant *C = importGlobal(Slot, Args, Name);
   auto *GV = cast<GlobalVariable>(C->stripPointerCasts());
@@ -1736,8 +1864,9 @@ Constant *DevirtModule::importConstant(V
 
   // We only need to set metadata if the global is newly created, in which
   // case it would not have hidden visibility.
-  if (GV->hasMetadata(LLVMContext::MD_absolute_symbol))
+  if (GV->hasMetadata(LLVMContext::MD_absolute_symbol)) {
     return C;
+  }
 
   auto SetAbsRange = [&](uint64_t Min, uint64_t Max) {
     auto *MinC = ConstantAsMetadata::get(ConstantInt::get(IntPtrTy, Min));
@@ -1746,10 +1875,11 @@ Constant *DevirtModule::importConstant(V
                     MDNode::get(M.getContext(), {MinC, MaxC}));
   };
   unsigned AbsWidth = IntTy->getBitWidth();
-  if (AbsWidth == IntPtrTy->getBitWidth())
+  if (AbsWidth == IntPtrTy->getBitWidth()) {
     SetAbsRange(~0ull, ~0ull); // Full set.
-  else
+  } else {
     SetAbsRange(0, 1ull << AbsWidth);
+  }
   return C;
 }
 
@@ -1757,13 +1887,14 @@ void DevirtModule::applyUniqueRetValOpt(
                                         bool IsOne,
                                         Constant *UniqueMemberAddr) {
   for (auto &&Call : CSInfo.CallSites) {
-    if (!OptimizedCalls.insert(&Call.CB).second)
+    if (!OptimizedCalls.insert(Call.CB).second) {
       continue;
-    IRBuilder<> B(&Call.CB);
+    }
+    IRBuilder<> B(Call.CB);
     Value *Cmp =
         B.CreateICmp(IsOne ? ICmpInst::ICMP_EQ : ICmpInst::ICMP_NE, Call.VTable,
                      B.CreateBitCast(UniqueMemberAddr, Call.VTable->getType()));
-    Cmp = B.CreateZExt(Cmp, Call.CB.getType());
+    Cmp = B.CreateZExt(Cmp, Call.CB->getType());
     NumUniqueRetVal++;
     Call.replaceAndErase("unique-ret-val", FnName, RemarksEnabled, OREGetter,
                          Cmp);
@@ -1785,8 +1916,9 @@ bool DevirtModule::tryUniqueRetValOpt(
     const TypeMemberInfo *UniqueMember = nullptr;
     for (const VirtualCallTarget &Target : TargetsForSlot) {
       if (Target.RetVal == (IsOne ? 1 : 0)) {
-        if (UniqueMember)
+        if (UniqueMember) {
           return false;
+        }
         UniqueMember = Target.TM;
       }
     }
@@ -1808,18 +1940,22 @@ bool DevirtModule::tryUniqueRetValOpt(
                          UniqueMemberAddr);
 
     // Update devirtualization statistics for targets.
-    if (RemarksEnabled || AreStatisticsEnabled())
-      for (auto &&Target : TargetsForSlot)
+    if (RemarksEnabled || AreStatisticsEnabled()) {
+      for (auto &&Target : TargetsForSlot) {
         Target.WasDevirt = true;
+      }
+    }
 
     return true;
   };
 
   if (BitWidth == 1) {
-    if (tryUniqueRetValOptFor(true))
+    if (tryUniqueRetValOptFor(true)) {
       return true;
-    if (tryUniqueRetValOptFor(false))
+    }
+    if (tryUniqueRetValOptFor(false)) {
       return true;
+    }
   }
   return false;
 }
@@ -1827,10 +1963,11 @@ bool DevirtModule::tryUniqueRetValOpt(
 void DevirtModule::applyVirtualConstProp(CallSiteInfo &CSInfo, StringRef FnName,
                                          Constant *Byte, Constant *Bit) {
   for (auto Call : CSInfo.CallSites) {
-    if (!OptimizedCalls.insert(&Call.CB).second)
+    if (!OptimizedCalls.insert(Call.CB).second) {
       continue;
-    auto *RetType = cast<IntegerType>(Call.CB.getType());
-    IRBuilder<> B(&Call.CB);
+    }
+    auto *RetType = cast<IntegerType>(Call.CB->getType());
+    IRBuilder<> B(Call.CB);
     Value *Addr = B.CreatePtrAdd(Call.VTable, Byte);
     if (RetType->getBitWidth() == 1) {
       Value *Bits = B.CreateLoad(Int8Ty, Addr);
@@ -1856,12 +1993,14 @@ bool DevirtModule::tryVirtualConstProp(
   // need to evaluate whether it would be correct to analyze the aliasee
   // function for this optimization.
   auto *Fn = dyn_cast<Function>(TargetsForSlot[0].Fn);
-  if (!Fn)
+  if (!Fn) {
     return false;
+  }
   // This only works if the function returns an integer.
   auto *RetType = dyn_cast<IntegerType>(Fn->getReturnType());
-  if (!RetType)
+  if (!RetType) {
     return false;
+  }
   unsigned BitWidth = RetType->getBitWidth();
 
   // TODO: Since we can evaluated these constants at compile-time, we can save
@@ -1871,8 +2010,9 @@ bool DevirtModule::tryVirtualConstProp(
   // extension. For example, if we would store an i64, but we can see that all
   // the values fit into an i16, then we can store an i16 before/after the
   // vtable and at each callsite do a s/zext.
-  if (BitWidth > 64)
+  if (BitWidth > 64) {
     return false;
+  }
 
   Align TypeAlignment = M.getDataLayout().getABIIntegerTypeAlignment(BitWidth);
 
@@ -1891,15 +2031,17 @@ bool DevirtModule::tryVirtualConstProp(
     // need to evaluate whether it would be correct to analyze the aliasee
     // function for this optimization.
     auto *Fn = dyn_cast<Function>(Target.Fn);
-    if (!Fn)
+    if (!Fn) {
       return false;
+    }
 
     if (Fn->isDeclaration() ||
         !computeFunctionBodyMemoryAccess(*Fn, FAM.getResult<AAManager>(*Fn))
              .doesNotAccessMemory() ||
         Fn->arg_empty() || !Fn->arg_begin()->use_empty() ||
-        Fn->getReturnType() != RetType)
+        Fn->getReturnType() != RetType) {
       return false;
+    }
 
     // This only works if the integer size is at most the alignment of the
     // vtable. If the table is underaligned, then we can't guarantee that the
@@ -1910,24 +2052,29 @@ bool DevirtModule::tryVirtualConstProp(
     GlobalVariable *GV = Target.TM->Bits->GV;
     Align TableAlignment = M.getDataLayout().getValueOrABITypeAlignment(
         GV->getAlign(), GV->getValueType());
-    if (TypeAlignment > TableAlignment)
+    if (TypeAlignment > TableAlignment) {
       return false;
+    }
   }
 
   for (auto &&CSByConstantArg : SlotInfo.ConstCSInfo) {
-    if (!tryEvaluateFunctionsWithArgs(TargetsForSlot, CSByConstantArg.first))
+    if (!tryEvaluateFunctionsWithArgs(TargetsForSlot, CSByConstantArg.first)) {
       continue;
+    }
 
     WholeProgramDevirtResolution::ByArg *ResByArg = nullptr;
-    if (Res)
+    if (Res) {
       ResByArg = &Res->ResByArg[CSByConstantArg.first];
+    }
 
-    if (tryUniformRetValOpt(TargetsForSlot, CSByConstantArg.second, ResByArg))
+    if (tryUniformRetValOpt(TargetsForSlot, CSByConstantArg.second, ResByArg)) {
       continue;
+    }
 
     if (tryUniqueRetValOpt(BitWidth, TargetsForSlot, CSByConstantArg.second,
-                           ResByArg, Slot, CSByConstantArg.first))
+                           ResByArg, Slot, CSByConstantArg.first)) {
       continue;
+    }
 
     // Find an allocation offset in bits in all vtables associated with the
     // type.
@@ -1951,19 +2098,21 @@ bool DevirtModule::tryVirtualConstProp(
 
     // If the amount of padding is too large, give up.
     // FIXME: do something smarter here.
-    if (std::min(TotalPaddingBefore, TotalPaddingAfter) > 128)
+    if (std::min(TotalPaddingBefore, TotalPaddingAfter) > 128) {
       continue;
+    }
 
     // Calculate the offset to the value as a (possibly negative) byte offset
     // and (if applicable) a bit offset, and store the values in the targets.
     int64_t OffsetByte;
     uint64_t OffsetBit;
-    if (TotalPaddingBefore <= TotalPaddingAfter)
+    if (TotalPaddingBefore <= TotalPaddingAfter) {
       setBeforeReturnValues(TargetsForSlot, AllocBefore, BitWidth, OffsetByte,
                             OffsetBit);
-    else
+    } else {
       setAfterReturnValues(TargetsForSlot, AllocAfter, BitWidth, OffsetByte,
                            OffsetBit);
+    }
 
     // In an earlier check we forbade constant propagation from operating on
     // tables whose alignment is less than the alignment needed for loading
@@ -1973,10 +2122,11 @@ bool DevirtModule::tryVirtualConstProp(
     // have an aligned load.
     assert(OffsetByte % TypeAlignment.value() == 0);
 
-    if (RemarksEnabled || AreStatisticsEnabled())
-      for (auto &&Target : TargetsForSlot)
+    if (RemarksEnabled || AreStatisticsEnabled()) {
+      for (auto &&Target : TargetsForSlot) {
         Target.WasDevirt = true;
-
+      }
+    }
 
     if (CSByConstantArg.second.isExported()) {
       ResByArg->TheKind = WholeProgramDevirtResolution::ByArg::VirtualConstProp;
@@ -1996,8 +2146,9 @@ bool DevirtModule::tryVirtualConstProp(
 }
 
 void DevirtModule::rebuildGlobal(VTableBits &B) {
-  if (B.Before.Bytes.empty() && B.After.Bytes.empty())
+  if (B.Before.Bytes.empty() && B.After.Bytes.empty()) {
     return;
+  }
 
   // Align the before byte array to the global's minimum alignment so that we
   // don't break any alignment requirements on the global.
@@ -2006,8 +2157,9 @@ void DevirtModule::rebuildGlobal(VTableB
   B.Before.Bytes.resize(alignTo(B.Before.Bytes.size(), Alignment));
 
   // Before was stored in reverse order; flip it now.
-  for (size_t I = 0, Size = B.Before.Bytes.size(); I != Size / 2; ++I)
+  for (size_t I = 0, Size = B.Before.Bytes.size(); I != Size / 2; ++I) {
     std::swap(B.Before.Bytes[I], B.Before.Bytes[Size - 1 - I]);
+  }
 
   // Build an anonymous global containing the before bytes, followed by the
   // original initializer, followed by the after bytes.
@@ -2045,8 +2197,9 @@ void DevirtModule::rebuildGlobal(VTableB
 bool DevirtModule::areRemarksEnabled() {
   const auto &FL = M.getFunctionList();
   for (const Function &Fn : FL) {
-    if (Fn.empty())
+    if (Fn.empty()) {
       continue;
+    }
     auto DI = OptimizationRemark(DEBUG_TYPE, "", DebugLoc(), &Fn.front());
     return DI.isEnabled();
   }
@@ -2063,8 +2216,9 @@ void DevirtModule::scanTypeTestUsers(
   // to CallSlots.
   for (Use &U : llvm::make_early_inc_range(TypeTestFunc->uses())) {
     auto *CI = dyn_cast<CallInst>(U.getUser());
-    if (!CI)
+    if (!CI) {
       continue;
+    }
 
     // Search for virtual calls based on %p and add them to DevirtCalls.
     SmallVector<DevirtCallSite, 1> DevirtCalls;
@@ -2077,18 +2231,21 @@ void DevirtModule::scanTypeTestUsers(
     // If we found any, add them to CallSlots.
     if (!Assumes.empty()) {
       Value *Ptr = CI->getArgOperand(0)->stripPointerCasts();
-      for (DevirtCallSite Call : DevirtCalls)
+      for (DevirtCallSite Call : DevirtCalls) {
         CallSlots[{TypeId, Call.Offset}].addCallSite(Ptr, Call.CB, nullptr);
+      }
     }
 
     auto RemoveTypeTestAssumes = [&]() {
       // We no longer need the assumes or the type test.
-      for (auto *Assume : Assumes)
+      for (auto *Assume : Assumes) {
         Assume->eraseFromParent();
+      }
       // We can't use RecursivelyDeleteTriviallyDeadInstructions here because we
       // may use the vtable argument later.
-      if (CI->use_empty())
+      if (CI->use_empty()) {
         CI->eraseFromParent();
+      }
     };
 
     // At this point we could remove all type test assume sequences, as they
@@ -2103,8 +2260,9 @@ void DevirtModule::scanTypeTestUsers(
 
     // The type test assumes will be treated by LTT as Unsat if the type id is
     // not used on a global (in which case it has no entry in the TypeIdMap).
-    if (!TypeIdMap.count(TypeId))
+    if (!TypeIdMap.count(TypeId)) {
       RemoveTypeTestAssumes();
+    }
 
     // For ThinLTO importing, we need to remove the type test assumes if this is
     // an MDString type id without a corresponding TypeIdSummary. Any
@@ -2118,12 +2276,13 @@ void DevirtModule::scanTypeTestUsers(
     else if (ImportSummary && isa<MDString>(TypeId)) {
       const TypeIdSummary *TidSummary =
           ImportSummary->getTypeIdSummary(cast<MDString>(TypeId)->getString());
-      if (!TidSummary)
+      if (!TidSummary) {
         RemoveTypeTestAssumes();
-      else
+      } else {
         // If one was created it should not be Unsat, because if we reached here
         // the type id was used on a global.
         assert(TidSummary->TTRes.TheKind != TypeTestResolution::Unsat);
+      }
     }
   }
 }
@@ -2134,8 +2293,9 @@ void DevirtModule::scanTypeCheckedLoadUs
 
   for (Use &U : llvm::make_early_inc_range(TypeCheckedLoadFunc->uses())) {
     auto *CI = dyn_cast<CallInst>(U.getUser());
-    if (!CI)
+    if (!CI) {
       continue;
+    }
 
     Value *Ptr = CI->getArgOperand(0);
     Value *Offset = CI->getArgOperand(1);
@@ -2203,8 +2363,9 @@ void DevirtModule::scanTypeCheckedLoadUs
     // If the function pointer has a non-call user, we cannot eliminate the type
     // check, as one of those users may eventually call the pointer. Increment
     // the unsafe use count to make sure it cannot reach zero.
-    if (HasNonCallUses)
+    if (HasNonCallUses) {
       ++NumUnsafeUses;
+    }
     for (DevirtCallSite Call : DevirtCalls) {
       CallSlots[{TypeId, Call.Offset}].addCallSite(Ptr, Call.CB,
                                                    &NumUnsafeUses);
@@ -2216,15 +2377,18 @@ void DevirtModule::scanTypeCheckedLoadUs
 
 void DevirtModule::importResolution(VTableSlot Slot, VTableSlotInfo &SlotInfo) {
   auto *TypeId = dyn_cast<MDString>(Slot.TypeID);
-  if (!TypeId)
+  if (!TypeId) {
     return;
+  }
   const TypeIdSummary *TidSummary =
       ImportSummary->getTypeIdSummary(TypeId->getString());
-  if (!TidSummary)
+  if (!TidSummary) {
     return;
+  }
   auto ResI = TidSummary->WPDRes.find(Slot.ByteOffset);
-  if (ResI == TidSummary->WPDRes.end())
+  if (ResI == TidSummary->WPDRes.end()) {
     return;
+  }
   const WholeProgramDevirtResolution &Res = ResI->second;
 
   if (Res.TheKind == WholeProgramDevirtResolution::SingleImpl) {
@@ -2244,8 +2408,9 @@ void DevirtModule::importResolution(VTab
 
   for (auto &CSByConstantArg : SlotInfo.ConstCSInfo) {
     auto I = Res.ResByArg.find(CSByConstantArg.first);
-    if (I == Res.ResByArg.end())
+    if (I == Res.ResByArg.end()) {
       continue;
+    }
     auto &ResByArg = I->second;
     // FIXME: We should figure out what to do about the "function name" argument
     // to the apply* functions, as the function names are unavailable during the
@@ -2290,12 +2455,20 @@ void DevirtModule::importResolution(VTab
 
 void DevirtModule::removeRedundantTypeTests() {
   auto *True = ConstantInt::getTrue(M.getContext());
+  SmallVector<CallInst *, 16> ToErase;
+  ToErase.reserve(NumUnsafeUsesForTypeTest.size());
+
   for (auto &&U : NumUnsafeUsesForTypeTest) {
     if (U.second == 0) {
       U.first->replaceAllUsesWith(True);
-      U.first->eraseFromParent();
+      ToErase.push_back(U.first);
     }
   }
+
+  // Batch erase to reduce IR manipulation overhead
+  for (CallInst *CI : ToErase) {
+    CI->eraseFromParent();
+  }
 }
 
 ValueInfo
@@ -2325,8 +2498,9 @@ DevirtModule::lookUpFunctionValueInfo(Fu
 
 bool DevirtModule::mustBeUnreachableFunction(
     Function *const F, ModuleSummaryIndex *ExportSummary) {
-  if (WholeProgramDevirtKeepUnreachableFunction)
+  if (WholeProgramDevirtKeepUnreachableFunction) {
     return false;
+  }
   // First, learn unreachability by analyzing function IR.
   if (!F->isDeclaration()) {
     // A function must be unreachable if its entry block ends with an
@@ -2345,8 +2519,9 @@ bool DevirtModule::run() {
   // with partially split modules during the thin link, and would have emitted
   // an error if any were found, so here we can simply return.
   if ((ExportSummary && ExportSummary->partiallySplitLTOUnits()) ||
-      (ImportSummary && ImportSummary->partiallySplitLTOUnits()))
+      (ImportSummary && ImportSummary->partiallySplitLTOUnits())) {
     return false;
+  }
 
   Function *TypeTestFunc =
       Intrinsic::getDeclarationIfExists(&M, Intrinsic::type_test);
@@ -2365,58 +2540,67 @@ bool DevirtModule::run() {
        AssumeFunc->use_empty()) &&
       (!TypeCheckedLoadFunc || TypeCheckedLoadFunc->use_empty()) &&
       (!TypeCheckedLoadRelativeFunc ||
-       TypeCheckedLoadRelativeFunc->use_empty()))
+       TypeCheckedLoadRelativeFunc->use_empty())) {
     return false;
+  }
 
   // Rebuild type metadata into a map for easy lookup.
   std::vector<VTableBits> Bits;
   DenseMap<Metadata *, std::set<TypeMemberInfo>> TypeIdMap;
   buildTypeIdentifierMap(Bits, TypeIdMap);
 
-  if (TypeTestFunc && AssumeFunc)
+  if (TypeTestFunc && AssumeFunc) {
     scanTypeTestUsers(TypeTestFunc, TypeIdMap);
+  }
 
-  if (TypeCheckedLoadFunc)
+  if (TypeCheckedLoadFunc) {
     scanTypeCheckedLoadUsers(TypeCheckedLoadFunc);
+  }
 
-  if (TypeCheckedLoadRelativeFunc)
+  if (TypeCheckedLoadRelativeFunc) {
     scanTypeCheckedLoadUsers(TypeCheckedLoadRelativeFunc);
+  }
 
   if (ImportSummary) {
-    for (auto &S : CallSlots)
+    for (auto &S : CallSlots) {
       importResolution(S.first, S.second);
+    }
 
     removeRedundantTypeTests();
 
     // We have lowered or deleted the type intrinsics, so we will no longer have
     // enough information to reason about the liveness of virtual function
     // pointers in GlobalDCE.
-    for (GlobalVariable &GV : M.globals())
+    for (GlobalVariable &GV : M.globals()) {
       GV.eraseMetadata(LLVMContext::MD_vcall_visibility);
+    }
 
     // The rest of the code is only necessary when exporting or during regular
     // LTO, so we are done.
     return true;
   }
 
-  if (TypeIdMap.empty())
+  if (TypeIdMap.empty()) {
     return true;
+  }
 
   // Collect information from summary about which calls to try to devirtualize.
   if (ExportSummary) {
     DenseMap<GlobalValue::GUID, TinyPtrVector<Metadata *>> MetadataByGUID;
     for (auto &P : TypeIdMap) {
-      if (auto *TypeId = dyn_cast<MDString>(P.first))
+      if (auto *TypeId = dyn_cast<MDString>(P.first)) {
         MetadataByGUID[GlobalValue::getGUIDAssumingExternalLinkage(
                            TypeId->getString())]
             .push_back(TypeId);
+      }
     }
 
     for (auto &P : *ExportSummary) {
       for (auto &S : P.second.SummaryList) {
         auto *FS = dyn_cast<FunctionSummary>(S.get());
-        if (!FS)
+        if (!FS) {
           continue;
+        }
         // FIXME: Only add live functions.
         for (FunctionSummary::VFuncId VF : FS->type_test_assume_vcalls()) {
           for (Metadata *MD : MetadataByGUID[VF.GUID]) {
@@ -2452,14 +2636,27 @@ bool DevirtModule::run() {
   bool DidVirtualConstProp = false;
   std::map<std::string, GlobalValue *> DevirtTargets;
   for (auto &S : CallSlots) {
+    // Cache the TypeMemberInfos lookup (hot path optimization)
+    auto TypeIdMapIt = TypeIdMap.find(S.first.TypeID);
+    if (TypeIdMapIt == TypeIdMap.end()) {
+      // TypeID not in map, skip processing
+      if (ExportSummary && isa<MDString>(S.first.TypeID)) {
+        // Still create summary entry for Unsat detection
+        ExportSummary->getOrInsertTypeIdSummary(
+            cast<MDString>(S.first.TypeID)->getString())
+            .WPDRes[S.first.ByteOffset];
+      }
+      continue;
+    }
+    const std::set<TypeMemberInfo> &TypeMemberInfos = TypeIdMapIt->second;
+
     // Search each of the members of the type identifier for the virtual
     // function implementation at offset S.first.ByteOffset, and add to
     // TargetsForSlot.
     std::vector<VirtualCallTarget> TargetsForSlot;
     WholeProgramDevirtResolution *Res = nullptr;
-    const std::set<TypeMemberInfo> &TypeMemberInfos = TypeIdMap[S.first.TypeID];
     if (ExportSummary && isa<MDString>(S.first.TypeID) &&
-        TypeMemberInfos.size())
+        TypeMemberInfos.size()) {
       // For any type id used on a global's type metadata, create the type id
       // summary resolution regardless of whether we can devirtualize, so that
       // lower type tests knows the type id is not Unsat. If it was not used on
@@ -2470,6 +2667,7 @@ bool DevirtModule::run() {
                  ->getOrInsertTypeIdSummary(
                      cast<MDString>(S.first.TypeID)->getString())
                  .WPDRes[S.first.ByteOffset];
+    }
     if (tryFindVirtualCallTargets(TargetsForSlot, TypeMemberInfos,
                                   S.first.ByteOffset, ExportSummary)) {
 
@@ -2481,10 +2679,13 @@ bool DevirtModule::run() {
       }
 
       // Collect functions devirtualized at least for one call site for stats.
-      if (RemarksEnabled || AreStatisticsEnabled())
-        for (const auto &T : TargetsForSlot)
-          if (T.WasDevirt)
+      if (RemarksEnabled || AreStatisticsEnabled()) {
+        for (const auto &T : TargetsForSlot) {
+          if (T.WasDevirt) {
             DevirtTargets[std::string(T.Fn->getName())] = T.Fn;
+          }
+        }
+      }
     }
 
     // CFI-specific: if we are exporting and any llvm.type.checked.load
@@ -2495,13 +2696,16 @@ bool DevirtModule::run() {
       auto GUID = GlobalValue::getGUIDAssumingExternalLinkage(
           cast<MDString>(S.first.TypeID)->getString());
       auto AddTypeTestsForTypeCheckedLoads = [&](CallSiteInfo &CSI) {
-        if (!CSI.AllCallSitesDevirted)
-          for (auto *FS : CSI.SummaryTypeCheckedLoadUsers)
+        if (!CSI.AllCallSitesDevirted) {
+          for (auto *FS : CSI.SummaryTypeCheckedLoadUsers) {
             FS->addTypeTest(GUID);
+          }
+        }
       };
       AddTypeTestsForTypeCheckedLoads(S.second.CSInfo);
-      for (auto &CCS : S.second.ConstCSInfo)
+      for (auto &CCS : S.second.ConstCSInfo) {
         AddTypeTestsForTypeCheckedLoads(CCS.second);
+      }
     }
   }
 
@@ -2529,25 +2733,30 @@ bool DevirtModule::run() {
 
   // Rebuild each global we touched as part of virtual constant propagation to
   // include the before and after bytes.
-  if (DidVirtualConstProp)
-    for (VTableBits &B : Bits)
+  if (DidVirtualConstProp) {
+    for (VTableBits &B : Bits) {
       rebuildGlobal(B);
+    }
+  }
 
   // We have lowered or deleted the type intrinsics, so we will no longer have
   // enough information to reason about the liveness of virtual function
   // pointers in GlobalDCE.
-  for (GlobalVariable &GV : M.globals())
+  for (GlobalVariable &GV : M.globals()) {
     GV.eraseMetadata(LLVMContext::MD_vcall_visibility);
+  }
 
-  for (auto *CI : CallsWithPtrAuthBundleRemoved)
+  for (auto *CI : CallsWithPtrAuthBundleRemoved) {
     CI->eraseFromParent();
+  }
 
   return true;
 }
 
 void DevirtIndex::run() {
-  if (ExportSummary.typeIdCompatibleVtableMap().empty())
+  if (ExportSummary.typeIdCompatibleVtableMap().empty()) {
     return;
+  }
 
   DenseMap<GlobalValue::GUID, std::vector<StringRef>> NameByGUID;
   for (const auto &P : ExportSummary.typeIdCompatibleVtableMap()) {
@@ -2566,8 +2775,9 @@ void DevirtIndex::run() {
   for (auto &P : ExportSummary) {
     for (auto &S : P.second.SummaryList) {
       auto *FS = dyn_cast<FunctionSummary>(S.get());
-      if (!FS)
+      if (!FS) {
         continue;
+      }
       // FIXME: Only add live functions.
       for (FunctionSummary::VFuncId VF : FS->type_test_assume_vcalls()) {
         for (StringRef Name : NameByGUID[VF.GUID]) {
@@ -2616,16 +2826,19 @@ void DevirtIndex::run() {
                                   S.first.ByteOffset)) {
 
       if (!trySingleImplDevirt(TargetsForSlot, S.first, S.second, Res,
-                               DevirtTargets))
+                               DevirtTargets)) {
         continue;
+      }
     }
   }
 
   // Optionally have the thin link print message for each devirtualized
   // function.
-  if (PrintSummaryDevirt)
-    for (const auto &DT : DevirtTargets)
+  if (PrintSummaryDevirt) {
+    for (const auto &DT : DevirtTargets) {
       errs() << "Devirtualized call to " << DT << "\n";
+    }
+  }
 
   NumDevirtTargets += DevirtTargets.size();
 }

From 8eb98a93cc912887c302def9b8259688df64cfdc Mon Sep 17 00:00:00 2001
From: Osama Abdelkader <osama.abdelkader@gmail.com>
Date: Mon, 22 Sep 2025 00:59:48 +0300
Subject: [PATCH] [clang][bytecode] Fix unknown size arrays crash in clang
 bytecode

This fixes issue #153948 where clang crashes with assertion failure
'Array of unknown size' when evaluating strlen() on external const char[]
declarations.

The issue was in evaluateStrlen() which called getNumElems() on unknown
size arrays, leading to an assertion in Descriptor::getSize().

Fix: Add check for isUnknownSizeArray() before calling getNumElems() to
gracefully handle unknown size arrays by returning false (indicating
strlen cannot be evaluated at compile time).

Tested with the reproducer from the GitHub issue.
---
 clang/lib/AST/ByteCode/Context.cpp | 5 +++++
 1 file changed, 5 insertions(+)

diff --git a/clang/lib/AST/ByteCode/Context.cpp b/clang/lib/AST/ByteCode/Context.cpp
index cfda6e8ded760..f9bc3906beec1 100644
--- a/clang/lib/AST/ByteCode/Context.cpp
+++ b/clang/lib/AST/ByteCode/Context.cpp
@@ -245,6 +245,11 @@ bool Context::evaluateStrlen(State &Parent, const Expr *E, uint64_t &Result) {
     if (!FieldDesc->isPrimitiveArray())
       return false;
 
+    // Handle unknown size arrays - we can't determine the length at compile time
+    if (Ptr.isUnknownSizeArray()) {
+      return false;
+    }
+
     unsigned N = Ptr.getNumElems();
     if (Ptr.elemSize() == 1) {
       Result = strnlen(reinterpret_cast<const char *>(Ptr.getRawAddress()), N);

From: patphzhang <patphzhang@tencent.com>
Date: Thu, 7 Nov 2024 11:13:49 +0800
Subject: [PATCH 1/7] [BOLT] support mold linker generated PLT in disassembling

---
 bolt/include/bolt/Utils/CommandLineOpts.h |   1 +
 bolt/lib/Rewrite/RewriteInstance.cpp      |  30 +-
 bolt/lib/Utils/CommandLineOpts.cpp        |   6 +
 bolt/test/X86/Inputs/plt-mold-header.yaml | 399 ++++++++++++++++++++++
 bolt/test/X86/plt-mold-header.test        |   7 +
 5 files changed, 442 insertions(+), 1 deletion(-)
 create mode 100644 bolt/test/X86/Inputs/plt-mold-header.yaml
 create mode 100644 bolt/test/X86/plt-mold-header.test

diff --git a/bolt/include/bolt/Utils/CommandLineOpts.h b/bolt/include/bolt/Utils/CommandLineOpts.h
index 04bf7db5de9527..3b0c0db1bd089e 100644
--- a/bolt/include/bolt/Utils/CommandLineOpts.h
+++ b/bolt/include/bolt/Utils/CommandLineOpts.h
@@ -34,6 +34,7 @@ extern llvm::cl::opt<bool> AggregateOnly;
 extern llvm::cl::opt<unsigned> BucketsPerLine;
 extern llvm::cl::opt<bool> DiffOnly;
 extern llvm::cl::opt<bool> EnableBAT;
+extern llvm::cl::opt<bool> UseMold;
 extern llvm::cl::opt<bool> EqualizeBBCounts;
 extern llvm::cl::opt<bool> RemoveSymtab;
 extern llvm::cl::opt<unsigned> ExecutionCountThreshold;
diff --git a/bolt/lib/Rewrite/RewriteInstance.cpp b/bolt/lib/Rewrite/RewriteInstance.cpp
index 32ec7abe8b666a..a7118be5dc263a 100644
--- a/bolt/lib/Rewrite/RewriteInstance.cpp
+++ b/bolt/lib/Rewrite/RewriteInstance.cpp
@@ -1672,7 +1672,35 @@ void RewriteInstance::disassemblePLTSectionX86(BinarySection &Section,
   const uint64_t SectionAddress = Section.getAddress();
   const uint64_t SectionSize = Section.getSize();
 
-  for (uint64_t EntryOffset = 0; EntryOffset + EntrySize <= SectionSize;
+  uint64_t EntryStartOffset = 0;
+  if (opts::UseMold) {
+    // The mold linker (https://github.com/rui314/mold/blob/v2.34.1/src/arch-x86-64.cc#L50)
+    // generates a unique format for the PLT.
+    // The first entry of the mold-style PLT is 32 bytes long, while the remaining entries
+    // are 16 bytes long. We need to parse the first entry with a special offset limit setting.
+    uint64_t HeaderSize = 32;
+    outs() << "BOLT-INFO: parsing PLT header for mold\n";
+    MCInst Instruction;
+    uint64_t InstrSize, InstrOffset = EntryStartOffset;
+    while (InstrOffset < HeaderSize) {
+      disassemblePLTInstruction(Section, InstrOffset, Instruction, InstrSize);
+      if (BC->MIB->isIndirectBranch(Instruction))
+        break;
+      InstrOffset += InstrSize;
+    }
+    uint64_t TargetAddress;
+    if (!BC->MIB->evaluateMemOperandTarget(Instruction, TargetAddress,
+                                            SectionAddress + InstrOffset,
+                                            InstrSize)) {
+      errs() << "BOLT-ERROR: error evaluating PLT instruction for the mold header at offset 0x"
+                  << Twine::utohexstr(SectionAddress + InstrOffset) << '\n';
+      exit(1);
+    }
+    createPLTBinaryFunction(TargetAddress, SectionAddress, HeaderSize);
+    EntryStartOffset += HeaderSize;
+  }
+
+  for (uint64_t EntryOffset = EntryStartOffset; EntryOffset + EntrySize <= SectionSize;
        EntryOffset += EntrySize) {
     MCInst Instruction;
     uint64_t InstrSize, InstrOffset = EntryOffset;
diff --git a/bolt/lib/Utils/CommandLineOpts.cpp b/bolt/lib/Utils/CommandLineOpts.cpp
index de82420a167131..356e530c9ca361 100644
--- a/bolt/lib/Utils/CommandLineOpts.cpp
+++ b/bolt/lib/Utils/CommandLineOpts.cpp
@@ -72,6 +72,12 @@ EnableBAT("enable-bat",
   cl::ZeroOrMore,
   cl::cat(BoltCategory));
 
+cl::opt<bool> UseMold("use-mold",
+  cl::desc("the binary is generated by the mold linker"),
+  cl::init(false),
+  cl::ZeroOrMore,
+  cl::cat(BoltCategory));
+
 cl::opt<bool> EqualizeBBCounts(
     "equalize-bb-counts",
     cl::desc("use same count for BBs that should have equivalent count (used "
diff --git a/bolt/test/X86/Inputs/plt-mold-header.yaml b/bolt/test/X86/Inputs/plt-mold-header.yaml
new file mode 100644
index 00000000000000..be6eabeccbba8f
--- /dev/null
+++ b/bolt/test/X86/Inputs/plt-mold-header.yaml
@@ -0,0 +1,399 @@
+--- !ELF
+FileHeader:
+  Class:           ELFCLASS64
+  Data:            ELFDATA2LSB
+  Type:            ET_DYN
+  Machine:         EM_X86_64
+  Entry:           0x13D0
+ProgramHeaders:
+  - Type:            PT_PHDR
+    Flags:           [ PF_R ]
+    VAddr:           0x40
+    Align:           0x8
+  - Type:            PT_INTERP
+    Flags:           [ PF_R ]
+    FirstSec:        .interp
+    LastSec:         .interp
+    VAddr:           0x270
+  - Type:            PT_LOAD
+    Flags:           [ PF_R ]
+    FirstSec:        .interp
+    LastSec:         .rodata.str
+    Align:           0x1000
+  - Type:            PT_LOAD
+    Flags:           [ PF_X, PF_R ]
+    FirstSec:        .plt
+    LastSec:         .text
+    VAddr:           0x13A0
+    Align:           0x1000
+  - Type:            PT_LOAD
+    Flags:           [ PF_W, PF_R ]
+    FirstSec:        .dynamic
+    LastSec:         .relro_padding
+    VAddr:           0x23F8
+    Align:           0x1000
+  - Type:            PT_LOAD
+    Flags:           [ PF_W, PF_R ]
+    FirstSec:        .got.plt
+    LastSec:         .got.plt
+    VAddr:           0x3550
+    Align:           0x1000
+  - Type:            PT_DYNAMIC
+    Flags:           [ PF_W, PF_R ]
+    FirstSec:        .dynamic
+    LastSec:         .dynamic
+    VAddr:           0x23F8
+    Align:           0x8
+  - Type:            PT_GNU_EH_FRAME
+    Flags:           [ PF_R ]
+    FirstSec:        .eh_frame_hdr
+    LastSec:         .eh_frame_hdr
+    VAddr:           0x37C
+    Align:           0x4
+  - Type:            PT_GNU_STACK
+    Flags:           [ PF_W, PF_R ]
+  - Type:            PT_GNU_RELRO
+    Flags:           [ PF_R ]
+    FirstSec:        .dynamic
+    LastSec:         .relro_padding
+    VAddr:           0x23F8
+Sections:
+  - Name:            .interp
+    Type:            SHT_PROGBITS
+    Flags:           [ SHF_ALLOC ]
+    Address:         0x270
+    AddressAlign:    0x1
+    Content:         2F6C696236342F6C642D6C696E75782D7838362D36342E736F2E3200
+  - Name:            .gnu.hash
+    Type:            SHT_GNU_HASH
+    Flags:           [ SHF_ALLOC ]
+    Address:         0x290
+    Link:            .dynsym
+    AddressAlign:    0x8
+    Header:
+      SymNdx:          0x2
+      Shift2:          0x1A
+    BloomFilter:     [ 0x0 ]
+    HashBuckets:     [ 0x0 ]
+    HashValues:      [  ]
+  - Name:            .dynsym
+    Type:            SHT_DYNSYM
+    Flags:           [ SHF_ALLOC ]
+    Address:         0x2B0
+    Link:            .dynstr
+    AddressAlign:    0x8
+  - Name:            .dynstr
+    Type:            SHT_STRTAB
+    Flags:           [ SHF_ALLOC ]
+    Address:         0x2E0
+    AddressAlign:    0x1
+  - Name:            .gnu.version
+    Type:            SHT_GNU_versym
+    Flags:           [ SHF_ALLOC ]
+    Address:         0x2FE
+    Link:            .dynsym
+    AddressAlign:    0x2
+    Entries:         [ 0, 2 ]
+  - Name:            .gnu.version_r
+    Type:            SHT_GNU_verneed
+    Flags:           [ SHF_ALLOC ]
+    Address:         0x308
+    Link:            .dynstr
+    AddressAlign:    0x8
+    Dependencies:
+      - Version:         1
+        File:            libc.so.6
+        Entries:
+          - Name:            GLIBC_2.2.5
+            Hash:            157882997
+            Flags:           0
+            Other:           2
+  - Name:            .rela.plt
+    Type:            SHT_RELA
+    Flags:           [ SHF_ALLOC, SHF_INFO_LINK ]
+    Address:         0x328
+    Link:            .dynsym
+    AddressAlign:    0x8
+    Info:            .got.plt
+    Relocations:
+      - Offset:          0x3568
+        Symbol:          printf
+        Type:            R_X86_64_JUMP_SLOT
+  - Name:            .eh_frame
+    Type:            SHT_PROGBITS
+    Flags:           [ SHF_ALLOC ]
+    Address:         0x340
+    AddressAlign:    0x8
+    Content:         1400000000000000017A5200017810011B0C0708900100001C0000001C000000701000002500000000410E108602430D06600C070800000000000000
+  - Name:            .eh_frame_hdr
+    Type:            SHT_PROGBITS
+    Flags:           [ SHF_ALLOC ]
+    Address:         0x37C
+    AddressAlign:    0x4
+    Content:         011B033BC0FFFFFF0100000054100000DCFFFFFF
+  - Name:            .rodata.str
+    Type:            SHT_PROGBITS
+    Flags:           [ SHF_ALLOC ]
+    Address:         0x390
+    AddressAlign:    0x1
+    Content:         48656C6C6F20776F726C64210A00
+  - Name:            .plt
+    Type:            SHT_PROGBITS
+    Flags:           [ SHF_ALLOC, SHF_EXECINSTR ]
+    Address:         0x13A0
+    AddressAlign:    0x10
+    Content:         F30F1EFA4153FF35CC3C1602FF25CE3C1602CCCCCCCCCCCCCCCCCCCCCCCCCCCCF30F1EFA41BB00000000FF2598210000
+  - Name:            .text
+    Type:            SHT_PROGBITS
+    Flags:           [ SHF_ALLOC, SHF_EXECINSTR ]
+    Address:         0x13D0
+    AddressAlign:    0x10
+    Content:         554889E54883EC10C745FC00000000488D3DAAEFFFFFB000E8D3FFFFFF31C04883C4105DC3
+  - Name:            .dynamic
+    Type:            SHT_DYNAMIC
+    Flags:           [ SHF_WRITE, SHF_ALLOC ]
+    Address:         0x23F8
+    Link:            .dynstr
+    AddressAlign:    0x8
+    Entries:
+      - Tag:             DT_NEEDED
+        Value:           0x1
+      - Tag:             DT_JMPREL
+        Value:           0x328
+      - Tag:             DT_PLTRELSZ
+        Value:           0x18
+      - Tag:             DT_PLTREL
+        Value:           0x7
+      - Tag:             DT_PLTGOT
+        Value:           0x3550
+      - Tag:             DT_SYMTAB
+        Value:           0x2B0
+      - Tag:             DT_SYMENT
+        Value:           0x18
+      - Tag:             DT_STRTAB
+        Value:           0x2E0
+      - Tag:             DT_STRSZ
+        Value:           0x1E
+      - Tag:             DT_VERSYM
+        Value:           0x2FE
+      - Tag:             DT_VERNEED
+        Value:           0x308
+      - Tag:             DT_VERNEEDNUM
+        Value:           0x1
+      - Tag:             DT_GNU_HASH
+        Value:           0x290
+      - Tag:             DT_FLAGS_1
+        Value:           0x8000000
+      - Tag:             DT_DEBUG
+        Value:           0x0
+      - Tag:             DT_NULL
+        Value:           0x0
+      - Tag:             DT_NULL
+        Value:           0x0
+      - Tag:             DT_NULL
+        Value:           0x0
+      - Tag:             DT_NULL
+        Value:           0x0
+      - Tag:             DT_NULL
+        Value:           0x0
+      - Tag:             DT_NULL
+        Value:           0x0
+  - Name:            .got
+    Type:            SHT_PROGBITS
+    Flags:           [ SHF_WRITE, SHF_ALLOC ]
+    Address:         0x2548
+    AddressAlign:    0x8
+    Content:         '0000000000000000'
+  - Name:            .relro_padding
+    Type:            SHT_NOBITS
+    Flags:           [ SHF_WRITE, SHF_ALLOC ]
+    Address:         0x2550
+    AddressAlign:    0x1
+    Size:            0xAB0
+  - Name:            .got.plt
+    Type:            SHT_PROGBITS
+    Flags:           [ SHF_WRITE, SHF_ALLOC ]
+    Address:         0x3550
+    AddressAlign:    0x8
+    Content:         F82300000000000000000000000000000000000000000000A013000000000000
+  - Name:            .rela.text
+    Type:            SHT_RELA
+    Flags:           [ SHF_INFO_LINK ]
+    Link:            .symtab
+    AddressAlign:    0x8
+    Info:            .text
+    Relocations:
+      - Offset:          0x13E2
+        Symbol:          .L.str
+        Type:            R_X86_64_PC32
+        Addend:          -4
+      - Offset:          0x13E9
+        Symbol:          printf
+        Type:            R_X86_64_PLT32
+        Addend:          -4
+  - Type:            SectionHeaderTable
+    Sections:
+      - Name:            .interp
+      - Name:            .gnu.hash
+      - Name:            .dynsym
+      - Name:            .dynstr
+      - Name:            .gnu.version
+      - Name:            .gnu.version_r
+      - Name:            .rela.plt
+      - Name:            .eh_frame
+      - Name:            .eh_frame_hdr
+      - Name:            .rodata.str
+      - Name:            .plt
+      - Name:            .text
+      - Name:            .rela.text
+      - Name:            .dynamic
+      - Name:            .got
+      - Name:            .relro_padding
+      - Name:            .got.plt
+      - Name:            .symtab
+      - Name:            .strtab
+      - Name:            .shstrtab
+Symbols:
+  - Name:            .interp
+    Type:            STT_SECTION
+    Section:         .interp
+    Value:           0x270
+  - Name:            .gnu.hash
+    Type:            STT_SECTION
+    Section:         .gnu.hash
+    Value:           0x290
+  - Name:            .dynsym
+    Type:            STT_SECTION
+    Section:         .dynsym
+    Value:           0x2B0
+  - Name:            .dynstr
+    Type:            STT_SECTION
+    Section:         .dynstr
+    Value:           0x2E0
+  - Name:            .gnu.version
+    Type:            STT_SECTION
+    Section:         .gnu.version
+    Value:           0x2FE
+  - Name:            .gnu.version_r
+    Type:            STT_SECTION
+    Section:         .gnu.version_r
+    Value:           0x308
+  - Name:            .rela.plt
+    Type:            STT_SECTION
+    Section:         .rela.plt
+    Value:           0x328
+  - Name:            .eh_frame
+    Type:            STT_SECTION
+    Section:         .eh_frame
+    Value:           0x340
+  - Name:            .eh_frame_hdr
+    Type:            STT_SECTION
+    Section:         .eh_frame_hdr
+    Value:           0x37C
+  - Name:            .rodata.str
+    Type:            STT_SECTION
+    Section:         .rodata.str
+    Value:           0x390
+  - Name:            .plt
+    Type:            STT_SECTION
+    Section:         .plt
+    Value:           0x13A0
+  - Name:            .text
+    Type:            STT_SECTION
+    Section:         .text
+    Value:           0x13D0
+  - Name:            .dynamic
+    Type:            STT_SECTION
+    Section:         .dynamic
+    Value:           0x23F8
+  - Name:            .got
+    Type:            STT_SECTION
+    Section:         .got
+    Value:           0x2548
+  - Name:            .relro_padding
+    Type:            STT_SECTION
+    Section:         .relro_padding
+    Value:           0x2550
+  - Name:            .got.plt
+    Type:            STT_SECTION
+    Section:         .got.plt
+    Value:           0x3550
+  - Name:            'printf$plt'
+    Type:            STT_FUNC
+    Section:         .plt
+    Value:           0x13C0
+  - Name:            hello.c
+    Type:            STT_FILE
+    Index:           SHN_ABS
+  - Name:            .L.str
+    Type:            STT_OBJECT
+    Section:         .rodata.str
+    Value:           0x390
+  - Name:            main
+    Type:            STT_FUNC
+    Section:         .text
+    Value:           0x13D0
+    Size:            0x25
+  - Name:            __ehdr_start
+    Section:         .interp
+  - Name:            __init_array_start
+    Index:           SHN_ABS
+  - Name:            __init_array_end
+    Index:           SHN_ABS
+  - Name:            __fini_array_start
+    Index:           SHN_ABS
+  - Name:            __fini_array_end
+    Index:           SHN_ABS
+  - Name:            __preinit_array_start
+    Index:           SHN_ABS
+  - Name:            __preinit_array_end
+    Index:           SHN_ABS
+  - Name:            _DYNAMIC
+    Section:         .dynamic
+    Value:           0x23F8
+  - Name:            _GLOBAL_OFFSET_TABLE_
+    Section:         .got.plt
+    Value:           0x3550
+  - Name:            _PROCEDURE_LINKAGE_TABLE_
+    Section:         .plt
+    Value:           0x13A0
+  - Name:            __bss_start
+    Index:           SHN_ABS
+  - Name:            _end
+    Section:         .got.plt
+    Value:           0x3570
+  - Name:            _etext
+    Section:         .text
+    Value:           0x13F5
+  - Name:            _edata
+    Section:         .got.plt
+    Value:           0x3570
+  - Name:            __executable_start
+    Section:         .interp
+  - Name:            __rela_iplt_start
+    Index:           SHN_ABS
+  - Name:            __rela_iplt_end
+    Index:           SHN_ABS
+  - Name:            __GNU_EH_FRAME_HDR
+    Section:         .eh_frame_hdr
+    Value:           0x37C
+  - Name:            end
+    Section:         .got.plt
+    Value:           0x3570
+  - Name:            etext
+    Section:         .text
+    Value:           0x13F5
+  - Name:            edata
+    Section:         .got.plt
+    Value:           0x3570
+  - Name:            __dso_handle
+    Section:         .interp
+  - Name:            _TLS_MODULE_BASE_
+    Section:         .interp
+  - Name:            printf
+    Binding:         STB_GLOBAL
+DynamicSymbols:
+  - Name:            printf
+    Type:            STT_FUNC
+    Binding:         STB_GLOBAL
+...
diff --git a/bolt/test/X86/plt-mold-header.test b/bolt/test/X86/plt-mold-header.test
new file mode 100644
index 00000000000000..8cbbed8711cbce
--- /dev/null
+++ b/bolt/test/X86/plt-mold-header.test
@@ -0,0 +1,7 @@
+# RUN: yaml2obj %p/Inputs/plt-mold-header.yaml &> %t.exe
+# RUN: llvm-bolt -use-mold %t.exe --print-cfg --print-only=main.* -o %t.out | FileCheck %s
+
+## Check that llvm-bolt correctly parses PLT header created by mold linker.
+## Without the '-use-mold' option, "BOLT-ERROR: unable to disassemble instruction in PLT section .plt at offset 0x10" will be reported.
+## The only call instruction in main() should be a call to printf() in PLT.
+CHECK:  callq "printf$plt

From fd9dc20eb9e5da903ea593df7c8a774dc481414a Mon Sep 17 00:00:00 2001
From: patphzhang <patphzhang@tencent.com>
Date: Fri, 8 Nov 2024 16:16:02 +0800
Subject: [PATCH 2/7] [BOLT] support mold linker generated PLT in disassembling
 without the new option

---
 bolt/include/bolt/Core/MCPlusBuilder.h    |  5 +++
 bolt/include/bolt/Utils/CommandLineOpts.h |  1 -
 bolt/lib/Rewrite/RewriteInstance.cpp      | 54 ++++++++++-------------
 bolt/lib/Target/X86/X86MCPlusBuilder.cpp  | 23 ++++++++++
 bolt/lib/Utils/CommandLineOpts.cpp        |  6 ---
 5 files changed, 52 insertions(+), 37 deletions(-)

diff --git a/bolt/include/bolt/Core/MCPlusBuilder.h b/bolt/include/bolt/Core/MCPlusBuilder.h
index 32eda0b283b883..2cc94c52f802de 100644
--- a/bolt/include/bolt/Core/MCPlusBuilder.h
+++ b/bolt/include/bolt/Core/MCPlusBuilder.h
@@ -1495,6 +1495,11 @@ class MCPlusBuilder {
     return 0;
   }
 
+  virtual bool isMoldPLTHeader(std::vector<MCInst *> &Insns) const {
+    llvm_unreachable("not implemented");
+    return false;
+  }
+
   virtual bool analyzeVirtualMethodCall(InstructionIterator Begin,
                                         InstructionIterator End,
                                         std::vector<MCInst *> &MethodFetchInsns,
diff --git a/bolt/include/bolt/Utils/CommandLineOpts.h b/bolt/include/bolt/Utils/CommandLineOpts.h
index 3b0c0db1bd089e..04bf7db5de9527 100644
--- a/bolt/include/bolt/Utils/CommandLineOpts.h
+++ b/bolt/include/bolt/Utils/CommandLineOpts.h
@@ -34,7 +34,6 @@ extern llvm::cl::opt<bool> AggregateOnly;
 extern llvm::cl::opt<unsigned> BucketsPerLine;
 extern llvm::cl::opt<bool> DiffOnly;
 extern llvm::cl::opt<bool> EnableBAT;
-extern llvm::cl::opt<bool> UseMold;
 extern llvm::cl::opt<bool> EqualizeBBCounts;
 extern llvm::cl::opt<bool> RemoveSymtab;
 extern llvm::cl::opt<unsigned> ExecutionCountThreshold;
diff --git a/bolt/lib/Rewrite/RewriteInstance.cpp b/bolt/lib/Rewrite/RewriteInstance.cpp
index a7118be5dc263a..831880233b3acf 100644
--- a/bolt/lib/Rewrite/RewriteInstance.cpp
+++ b/bolt/lib/Rewrite/RewriteInstance.cpp
@@ -1672,35 +1672,7 @@ void RewriteInstance::disassemblePLTSectionX86(BinarySection &Section,
   const uint64_t SectionAddress = Section.getAddress();
   const uint64_t SectionSize = Section.getSize();
 
-  uint64_t EntryStartOffset = 0;
-  if (opts::UseMold) {
-    // The mold linker (https://github.com/rui314/mold/blob/v2.34.1/src/arch-x86-64.cc#L50)
-    // generates a unique format for the PLT.
-    // The first entry of the mold-style PLT is 32 bytes long, while the remaining entries
-    // are 16 bytes long. We need to parse the first entry with a special offset limit setting.
-    uint64_t HeaderSize = 32;
-    outs() << "BOLT-INFO: parsing PLT header for mold\n";
-    MCInst Instruction;
-    uint64_t InstrSize, InstrOffset = EntryStartOffset;
-    while (InstrOffset < HeaderSize) {
-      disassemblePLTInstruction(Section, InstrOffset, Instruction, InstrSize);
-      if (BC->MIB->isIndirectBranch(Instruction))
-        break;
-      InstrOffset += InstrSize;
-    }
-    uint64_t TargetAddress;
-    if (!BC->MIB->evaluateMemOperandTarget(Instruction, TargetAddress,
-                                            SectionAddress + InstrOffset,
-                                            InstrSize)) {
-      errs() << "BOLT-ERROR: error evaluating PLT instruction for the mold header at offset 0x"
-                  << Twine::utohexstr(SectionAddress + InstrOffset) << '\n';
-      exit(1);
-    }
-    createPLTBinaryFunction(TargetAddress, SectionAddress, HeaderSize);
-    EntryStartOffset += HeaderSize;
-  }
-
-  for (uint64_t EntryOffset = EntryStartOffset; EntryOffset + EntrySize <= SectionSize;
+  for (uint64_t EntryOffset = 0; EntryOffset + EntrySize <= SectionSize;
        EntryOffset += EntrySize) {
     MCInst Instruction;
     uint64_t InstrSize, InstrOffset = EntryOffset;
@@ -1717,8 +1689,30 @@ void RewriteInstance::disassemblePLTSectionX86(BinarySection &Section,
       InstrOffset += InstrSize;
     }
 
-    if (InstrOffset + InstrSize > EntryOffset + EntrySize)
+    if (InstrOffset + InstrSize > EntryOffset + EntrySize) {
+      // Check if it is a mold header before rolling back because the mold linker generates
+      // a unique format. The header entry of the mold-style PLT is 32 bytes long, while the
+      // remaining entries are 16 bytes long. We need to skip the header entry.
+      uint64_t HeaderOffset = 0, MoldHeaderSize = 32;
+      if (EntryOffset == HeaderOffset && SectionSize >= MoldHeaderSize) {
+        std::vector<MCInst *> Insns;
+        MCInst Instructions[32]; // 32 insns at most
+        uint32_t Index = 0;
+        while (HeaderOffset < MoldHeaderSize) {
+          disassemblePLTInstruction(Section, HeaderOffset, Instructions[Index], InstrSize);
+          Insns.push_back(&Instructions[Index]);
+          HeaderOffset += InstrSize;
+          Index++;
+        }
+        // if it is a mold header, skip it
+        if (BC->MIB->isMoldPLTHeader(Insns)) {
+          BC->outs() << "BOLT-INFO: parsing the PLT of the mold linker\n";
+          EntryOffset += EntrySize;
+        }
+          
+      }
       continue;
+    }
 
     uint64_t TargetAddress;
     if (!BC->MIB->evaluateMemOperandTarget(Instruction, TargetAddress,
diff --git a/bolt/lib/Target/X86/X86MCPlusBuilder.cpp b/bolt/lib/Target/X86/X86MCPlusBuilder.cpp
index 63086c06d74fd9..215380085deb01 100644
--- a/bolt/lib/Target/X86/X86MCPlusBuilder.cpp
+++ b/bolt/lib/Target/X86/X86MCPlusBuilder.cpp
@@ -2127,6 +2127,29 @@ class X86MCPlusBuilder : public MCPlusBuilder {
     return Type;
   }
 
+  /// Analyze a series of insns that match the PLT header of the mold linker
+  /// (https://github.com/rui314/mold/blob/v2.34.1/src/arch-x86-64.cc#L50).
+  /// The size of the header is 32 bytes and the format is as follows:
+  ///   endbr64
+  ///   push %r11
+  ///   push GOTPLT+8(%rip)
+  ///   jmp *GOTPLT+16(%rip)
+  ///   padding （14 bytes）
+  ///
+  bool isMoldPLTHeader(std::vector<MCInst *> &Insns) const override {
+    if (Insns.size() != 18)
+      return false;
+      
+    if (!isTerminateBranch(*Insns[0]) || !isPush(*Insns[1])
+        || !isPush(*Insns[2]) || !isIndirectBranch(*Insns[3]))
+      return false;
+      
+    for (unsigned int i = 4; i < 18; ++i)
+      if (Insns[i]->getOpcode() != X86::INT3)
+        return false;
+    return true;
+  }
+
   /// Analyze a callsite to see if it could be a virtual method call.  This only
   /// checks to see if the overall pattern is satisfied, it does not guarantee
   /// that the callsite is a true virtual method call.
diff --git a/bolt/lib/Utils/CommandLineOpts.cpp b/bolt/lib/Utils/CommandLineOpts.cpp
index 356e530c9ca361..de82420a167131 100644
--- a/bolt/lib/Utils/CommandLineOpts.cpp
+++ b/bolt/lib/Utils/CommandLineOpts.cpp
@@ -72,12 +72,6 @@ EnableBAT("enable-bat",
   cl::ZeroOrMore,
   cl::cat(BoltCategory));
 
-cl::opt<bool> UseMold("use-mold",
-  cl::desc("the binary is generated by the mold linker"),
-  cl::init(false),
-  cl::ZeroOrMore,
-  cl::cat(BoltCategory));
-
 cl::opt<bool> EqualizeBBCounts(
     "equalize-bb-counts",
     cl::desc("use same count for BBs that should have equivalent count (used "

From 8c51da24960b673d242247a3af8e486bac8a2b78 Mon Sep 17 00:00:00 2001
From: patphzhang <patphzhang@tencent.com>
Date: Fri, 8 Nov 2024 16:33:15 +0800
Subject: [PATCH 3/7] [BOLT] removed the mold option in the test case

---
 bolt/test/X86/plt-mold-header.test | 3 +--
 1 file changed, 1 insertion(+), 2 deletions(-)

diff --git a/bolt/test/X86/plt-mold-header.test b/bolt/test/X86/plt-mold-header.test
index 8cbbed8711cbce..563e15e22d6642 100644
--- a/bolt/test/X86/plt-mold-header.test
+++ b/bolt/test/X86/plt-mold-header.test
@@ -1,7 +1,6 @@
 # RUN: yaml2obj %p/Inputs/plt-mold-header.yaml &> %t.exe
-# RUN: llvm-bolt -use-mold %t.exe --print-cfg --print-only=main.* -o %t.out | FileCheck %s
+# RUN: llvm-bolt %t.exe --print-cfg --print-only=main.* -o %t.out | FileCheck %s
 
 ## Check that llvm-bolt correctly parses PLT header created by mold linker.
-## Without the '-use-mold' option, "BOLT-ERROR: unable to disassemble instruction in PLT section .plt at offset 0x10" will be reported.
 ## The only call instruction in main() should be a call to printf() in PLT.
 CHECK:  callq "printf$plt

From beaa547d8e65baa6d81f1bbc9916b766b713f99e Mon Sep 17 00:00:00 2001
From: patphzhang <patphzhang@tencent.com>
Date: Mon, 11 Nov 2024 14:05:47 +0800
Subject: [PATCH 4/7] [BOLT] parse the PLT in two parts: header detection and
 entry detection

---
 bolt/include/bolt/Core/MCPlusBuilder.h   |  5 ---
 bolt/lib/Rewrite/RewriteInstance.cpp     | 40 ++++++++++--------------
 bolt/lib/Target/X86/X86MCPlusBuilder.cpp | 23 --------------
 3 files changed, 16 insertions(+), 52 deletions(-)

diff --git a/bolt/include/bolt/Core/MCPlusBuilder.h b/bolt/include/bolt/Core/MCPlusBuilder.h
index 2cc94c52f802de..32eda0b283b883 100644
--- a/bolt/include/bolt/Core/MCPlusBuilder.h
+++ b/bolt/include/bolt/Core/MCPlusBuilder.h
@@ -1495,11 +1495,6 @@ class MCPlusBuilder {
     return 0;
   }
 
-  virtual bool isMoldPLTHeader(std::vector<MCInst *> &Insns) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
   virtual bool analyzeVirtualMethodCall(InstructionIterator Begin,
                                         InstructionIterator End,
                                         std::vector<MCInst *> &MethodFetchInsns,
diff --git a/bolt/lib/Rewrite/RewriteInstance.cpp b/bolt/lib/Rewrite/RewriteInstance.cpp
index 5d4118e51d2f1c..806edb970dab0f 100644
--- a/bolt/lib/Rewrite/RewriteInstance.cpp
+++ b/bolt/lib/Rewrite/RewriteInstance.cpp
@@ -1707,7 +1707,21 @@ void RewriteInstance::disassemblePLTSectionX86(BinarySection &Section,
   const uint64_t SectionAddress = Section.getAddress();
   const uint64_t SectionSize = Section.getSize();
 
-  for (uint64_t EntryOffset = 0; EntryOffset + EntrySize <= SectionSize;
+  // Parse the PLT header
+  uint64_t HeaderSize = 16;
+  MCInst FirstInstr;
+  uint64_t FirstInstrSize;
+  disassemblePLTInstruction(Section, 0, FirstInstr, FirstInstrSize);
+  if (BC->MIB->isTerminateBranch(FirstInstr)) {
+    // The mold linker (https://github.com/rui314/mold/blob/v2.34.1/src/arch-x86-64.cc#L50)
+    // generates a unique format for the PLT. The header entry is 32 bytes long, while the 
+    // remaining entries are 16 bytes long.
+    BC->outs() << "BOLT-INFO: parsing PLT header for mold\n";
+    HeaderSize = 32;
+  }
+
+  // Parse the PLT entries
+  for (uint64_t EntryOffset = HeaderSize; EntryOffset + EntrySize <= SectionSize;
        EntryOffset += EntrySize) {
     MCInst Instruction;
     uint64_t InstrSize, InstrOffset = EntryOffset;
@@ -1724,30 +1738,8 @@ void RewriteInstance::disassemblePLTSectionX86(BinarySection &Section,
       InstrOffset += InstrSize;
     }
 
-    if (InstrOffset + InstrSize > EntryOffset + EntrySize) {
-      // Check if it is a mold header before rolling back because the mold linker generates
-      // a unique format. The header entry of the mold-style PLT is 32 bytes long, while the
-      // remaining entries are 16 bytes long. We need to skip the header entry.
-      uint64_t HeaderOffset = 0, MoldHeaderSize = 32;
-      if (EntryOffset == HeaderOffset && SectionSize >= MoldHeaderSize) {
-        std::vector<MCInst *> Insns;
-        MCInst Instructions[32]; // 32 insns at most
-        uint32_t Index = 0;
-        while (HeaderOffset < MoldHeaderSize) {
-          disassemblePLTInstruction(Section, HeaderOffset, Instructions[Index], InstrSize);
-          Insns.push_back(&Instructions[Index]);
-          HeaderOffset += InstrSize;
-          Index++;
-        }
-        // if it is a mold header, skip it
-        if (BC->MIB->isMoldPLTHeader(Insns)) {
-          BC->outs() << "BOLT-INFO: parsing the PLT of the mold linker\n";
-          EntryOffset += EntrySize;
-        }
-          
-      }
+    if (InstrOffset + InstrSize > EntryOffset + EntrySize)
       continue;
-    }
 
     uint64_t TargetAddress;
     if (!BC->MIB->evaluateMemOperandTarget(Instruction, TargetAddress,
diff --git a/bolt/lib/Target/X86/X86MCPlusBuilder.cpp b/bolt/lib/Target/X86/X86MCPlusBuilder.cpp
index 215380085deb01..63086c06d74fd9 100644
--- a/bolt/lib/Target/X86/X86MCPlusBuilder.cpp
+++ b/bolt/lib/Target/X86/X86MCPlusBuilder.cpp
@@ -2127,29 +2127,6 @@ class X86MCPlusBuilder : public MCPlusBuilder {
     return Type;
   }
 
-  /// Analyze a series of insns that match the PLT header of the mold linker
-  /// (https://github.com/rui314/mold/blob/v2.34.1/src/arch-x86-64.cc#L50).
-  /// The size of the header is 32 bytes and the format is as follows:
-  ///   endbr64
-  ///   push %r11
-  ///   push GOTPLT+8(%rip)
-  ///   jmp *GOTPLT+16(%rip)
-  ///   padding （14 bytes）
-  ///
-  bool isMoldPLTHeader(std::vector<MCInst *> &Insns) const override {
-    if (Insns.size() != 18)
-      return false;
-      
-    if (!isTerminateBranch(*Insns[0]) || !isPush(*Insns[1])
-        || !isPush(*Insns[2]) || !isIndirectBranch(*Insns[3]))
-      return false;
-      
-    for (unsigned int i = 4; i < 18; ++i)
-      if (Insns[i]->getOpcode() != X86::INT3)
-        return false;
-    return true;
-  }
-
   /// Analyze a callsite to see if it could be a virtual method call.  This only
   /// checks to see if the overall pattern is satisfied, it does not guarantee
   /// that the callsite is a true virtual method call.

From e42256e0cf011d528e15e4d51332d1b269a8f05b Mon Sep 17 00:00:00 2001
From: patphzhang <patphzhang@tencent.com>
Date: Mon, 11 Nov 2024 15:01:44 +0800
Subject: [PATCH 5/7] [BOLT] skip the '.plt.sec' section when parsing mold
 header

---
 bolt/lib/Rewrite/RewriteInstance.cpp | 24 +++++++++++++-----------
 1 file changed, 13 insertions(+), 11 deletions(-)

diff --git a/bolt/lib/Rewrite/RewriteInstance.cpp b/bolt/lib/Rewrite/RewriteInstance.cpp
index 806edb970dab0f..3c4631f0c1451e 100644
--- a/bolt/lib/Rewrite/RewriteInstance.cpp
+++ b/bolt/lib/Rewrite/RewriteInstance.cpp
@@ -1708,17 +1708,19 @@ void RewriteInstance::disassemblePLTSectionX86(BinarySection &Section,
   const uint64_t SectionSize = Section.getSize();
 
   // Parse the PLT header
-  uint64_t HeaderSize = 16;
-  MCInst FirstInstr;
-  uint64_t FirstInstrSize;
-  disassemblePLTInstruction(Section, 0, FirstInstr, FirstInstrSize);
-  if (BC->MIB->isTerminateBranch(FirstInstr)) {
-    // The mold linker (https://github.com/rui314/mold/blob/v2.34.1/src/arch-x86-64.cc#L50)
-    // generates a unique format for the PLT. The header entry is 32 bytes long, while the 
-    // remaining entries are 16 bytes long.
-    BC->outs() << "BOLT-INFO: parsing PLT header for mold\n";
-    HeaderSize = 32;
-  }
+  uint64_t HeaderSize = 0;
+  if (Section.getName() != ".plt.sec") {
+    MCInst FirstInstr;
+    uint64_t FirstInstrSize;
+    disassemblePLTInstruction(Section, 0, FirstInstr, FirstInstrSize);
+    if (BC->MIB->isTerminateBranch(FirstInstr)) {
+      // The mold linker (https://github.com/rui314/mold/blob/v2.34.1/src/arch-x86-64.cc#L50)
+      // generates a unique format for the PLT. The header entry is 32 bytes long, while the 
+      // remaining entries are 16 bytes long.
+      BC->outs() << "BOLT-INFO: parsing PLT header for mold\n";
+      HeaderSize = 32;
+    }
+  }  
 
   // Parse the PLT entries
   for (uint64_t EntryOffset = HeaderSize; EntryOffset + EntrySize <= SectionSize;

From 7133d21a5c4f7d32002a3d0a02f626ba6296fbcf Mon Sep 17 00:00:00 2001
From: patphzhang <patphzhang@tencent.com>
Date: Mon, 11 Nov 2024 15:14:51 +0800
Subject: [PATCH 6/7] [BOLT] choose the '.plt' section when parsing mold header

---
 bolt/lib/Rewrite/RewriteInstance.cpp | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/bolt/lib/Rewrite/RewriteInstance.cpp b/bolt/lib/Rewrite/RewriteInstance.cpp
index 3c4631f0c1451e..1333851fa21945 100644
--- a/bolt/lib/Rewrite/RewriteInstance.cpp
+++ b/bolt/lib/Rewrite/RewriteInstance.cpp
@@ -1709,7 +1709,7 @@ void RewriteInstance::disassemblePLTSectionX86(BinarySection &Section,
 
   // Parse the PLT header
   uint64_t HeaderSize = 0;
-  if (Section.getName() != ".plt.sec") {
+  if (Section.getName() == ".plt") {
     MCInst FirstInstr;
     uint64_t FirstInstrSize;
     disassemblePLTInstruction(Section, 0, FirstInstr, FirstInstrSize);

From f2f238e128bcd7ce1f4b2f5c19dc3967a0090a55 Mon Sep 17 00:00:00 2001
From: patphzhang <patphzhang@tencent.com>
Date: Fri, 15 Nov 2024 14:50:43 +0800
Subject: [PATCH 7/7] [BOLT] support different PLT header type in disassembling

---
 bolt/include/bolt/Core/MCPlusBuilder.h      |  7 +++++
 bolt/include/bolt/Rewrite/RewriteInstance.h |  3 +++
 bolt/lib/Rewrite/RewriteInstance.cpp        | 29 ++++++++++++---------
 bolt/lib/Target/X86/X86MCPlusBuilder.cpp    | 28 ++++++++++++++++++++
 4 files changed, 54 insertions(+), 13 deletions(-)

diff --git a/bolt/include/bolt/Core/MCPlusBuilder.h b/bolt/include/bolt/Core/MCPlusBuilder.h
index 32eda0b283b883..6a619b33aaf48c 100644
--- a/bolt/include/bolt/Core/MCPlusBuilder.h
+++ b/bolt/include/bolt/Core/MCPlusBuilder.h
@@ -1495,6 +1495,13 @@ class MCPlusBuilder {
     return 0;
   }
 
+  /// Analyze preamble instrucions in PLT section and try to determine
+  /// the size of the header.
+  virtual uint32_t analyzePLTHeader(std::vector<MCInst *> &Insns) const {
+    llvm_unreachable("not implemented");
+    return 0;
+  }
+
   virtual bool analyzeVirtualMethodCall(InstructionIterator Begin,
                                         InstructionIterator End,
                                         std::vector<MCInst *> &MethodFetchInsns,
diff --git a/bolt/include/bolt/Rewrite/RewriteInstance.h b/bolt/include/bolt/Rewrite/RewriteInstance.h
index e5b7ad63007cab..54708da2bdf41a 100644
--- a/bolt/include/bolt/Rewrite/RewriteInstance.h
+++ b/bolt/include/bolt/Rewrite/RewriteInstance.h
@@ -277,6 +277,9 @@ class RewriteInstance {
   /// is the expected .plt \p Section entry function size.
   void disassemblePLTSectionX86(BinarySection &Section, uint64_t EntrySize);
 
+  /// Disassemble the X86-specific .plt \p Section header and get header size.
+  uint32_t disassemblePLTHeaderX86(BinarySection &Section, uint64_t EntrySize);
+
   /// Disassemble riscv-specific .plt \p Section auxiliary function
   void disassemblePLTSectionRISCV(BinarySection &Section);
 
diff --git a/bolt/lib/Rewrite/RewriteInstance.cpp b/bolt/lib/Rewrite/RewriteInstance.cpp
index 1333851fa21945..ca3f86f72d1ddf 100644
--- a/bolt/lib/Rewrite/RewriteInstance.cpp
+++ b/bolt/lib/Rewrite/RewriteInstance.cpp
@@ -1708,19 +1708,7 @@ void RewriteInstance::disassemblePLTSectionX86(BinarySection &Section,
   const uint64_t SectionSize = Section.getSize();
 
   // Parse the PLT header
-  uint64_t HeaderSize = 0;
-  if (Section.getName() == ".plt") {
-    MCInst FirstInstr;
-    uint64_t FirstInstrSize;
-    disassemblePLTInstruction(Section, 0, FirstInstr, FirstInstrSize);
-    if (BC->MIB->isTerminateBranch(FirstInstr)) {
-      // The mold linker (https://github.com/rui314/mold/blob/v2.34.1/src/arch-x86-64.cc#L50)
-      // generates a unique format for the PLT. The header entry is 32 bytes long, while the 
-      // remaining entries are 16 bytes long.
-      BC->outs() << "BOLT-INFO: parsing PLT header for mold\n";
-      HeaderSize = 32;
-    }
-  }  
+  uint64_t HeaderSize = disassemblePLTHeaderX86(Section, EntrySize);
 
   // Parse the PLT entries
   for (uint64_t EntryOffset = HeaderSize; EntryOffset + EntrySize <= SectionSize;
@@ -1757,6 +1745,21 @@ void RewriteInstance::disassemblePLTSectionX86(BinarySection &Section,
   }
 }
 
+uint32_t RewriteInstance::disassemblePLTHeaderX86(BinarySection &Section,
+                                               uint64_t EntrySize) {
+  uint64_t InstrSize, InstrOffset = 0;
+  std::vector<MCInst *> Insns;
+  MCInst Instructions[32]; // 32 insns (bytes) at most
+  uint32_t Index = 0;
+  while (InstrOffset < EntrySize) {
+    disassemblePLTInstruction(Section, InstrOffset, Instructions[Index], InstrSize);
+    Insns.push_back(&Instructions[Index]);
+    InstrOffset += InstrSize;
+    Index++;
+  }
+  return BC->MIB->analyzePLTHeader(Insns);
+}
+
 void RewriteInstance::disassemblePLT() {
   auto analyzeOnePLTSection = [&](BinarySection &Section, uint64_t EntrySize) {
     if (BC->isAArch64())
diff --git a/bolt/lib/Target/X86/X86MCPlusBuilder.cpp b/bolt/lib/Target/X86/X86MCPlusBuilder.cpp
index 63086c06d74fd9..2acff1c018c0c7 100644
--- a/bolt/lib/Target/X86/X86MCPlusBuilder.cpp
+++ b/bolt/lib/Target/X86/X86MCPlusBuilder.cpp
@@ -2127,6 +2127,34 @@ class X86MCPlusBuilder : public MCPlusBuilder {
     return Type;
   }
 
+  uint32_t analyzePLTHeader(std::vector<MCInst *> &Insns) const override {
+    uint32_t HeaderSize = 0;
+    if (Insns.size() == 0) // empty header
+      return HeaderSize;
+    if (isTerminateBranch(*Insns[0])) {
+      // starting with an endbr, possible headers: mold
+      if (Insns.size() >= 4 && isPush(*Insns[1]) && isPush(*Insns[2]) &&
+          isIndirectBranch(*Insns[3])) {
+        // The mold linker (https://github.com/rui314/mold/blob/v2.34.1/src/arch-x86-64.cc#L50)
+        // generates a unique format for the PLT. The size of the header is 32 bytes and the 
+        // format is as follows:
+        ///   endbr64
+        ///   push %r11
+        ///   push GOTPLT+8(%rip)
+        ///   jmp *GOTPLT+16(%rip)
+        ///   padding （14 bytes）
+        HeaderSize = 32; // mold with CET support
+      } else {
+        // In case other linkers have new proposals.
+      }
+    } else {
+      // TODO: headers with endbr in the midddle, including the lld version plt,
+      // or headers without CET support, including R_386_PLT32, R_X86_64_PLT32,
+      // retpolineplt of lld (for Spectre v2 mitigation), and etc.
+    }
+    return HeaderSize;
+  }
+
   /// Analyze a callsite to see if it could be a virtual method call.  This only
   /// checks to see if the overall pattern is satisfied, it does not guarantee
   /// that the callsite is a true virtual method call.
