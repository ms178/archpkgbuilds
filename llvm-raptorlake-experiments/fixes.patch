From 4c4a9b6cfc5b883a99d111cda3897538cda125e1 Mon Sep 17 00:00:00 2001
From: raventid <juliankul@gmail.com>
Date: Sat, 10 Jan 2026 16:44:58 +0800
Subject: [PATCH] [VectorCombine] foldPermuteOfIntrinsic - support multiple
 uses of shuffled ops

Fixes https://github.com/llvm/llvm-project/issues/173039
---
 .../Transforms/Vectorize/VectorCombine.cpp    | 29 ++++++++++-------
 .../VectorCombine/X86/shuffle-of-fma-const.ll | 32 +++++++++++++++++++
 2 files changed, 50 insertions(+), 11 deletions(-)

diff --git a/llvm/lib/Transforms/Vectorize/VectorCombine.cpp b/llvm/lib/Transforms/Vectorize/VectorCombine.cpp
index c9e45a8d05d78..fdab1813e9f23 100644
--- a/llvm/lib/Transforms/Vectorize/VectorCombine.cpp
+++ b/llvm/lib/Transforms/Vectorize/VectorCombine.cpp
@@ -55,9 +55,9 @@ STATISTIC(NumScalarOps, "Number of scalar unary + binary ops formed");
 STATISTIC(NumScalarCmp, "Number of scalar compares formed");
 STATISTIC(NumScalarIntrinsic, "Number of scalar intrinsic calls formed");
 
-static cl::opt<bool> DisableVectorCombine(
-    "disable-vector-combine", cl::init(false), cl::Hidden,
-    cl::desc("Disable all vector combine transforms"));
+static cl::opt<bool>
+    DisableVectorCombine("disable-vector-combine", cl::init(false), cl::Hidden,
+                         cl::desc("Disable all vector combine transforms"));
 
 static cl::opt<bool> DisableBinopExtractShuffle(
     "disable-binop-extract-shuffle", cl::init(false), cl::Hidden,
@@ -1211,8 +1211,7 @@ bool VectorCombine::scalarizeVPIntrinsic(Instruction &I) {
   InstructionCost OldCost = 2 * SplatCost + VectorOpCost;
 
   // Determine scalar opcode
-  std::optional<unsigned> FunctionalOpcode =
-      VPI.getFunctionalOpcode();
+  std::optional<unsigned> FunctionalOpcode = VPI.getFunctionalOpcode();
   std::optional<Intrinsic::ID> ScalarIntrID = std::nullopt;
   if (!FunctionalOpcode) {
     ScalarIntrID = VPI.getFunctionalIntrinsicID();
@@ -1235,8 +1234,7 @@ bool VectorCombine::scalarizeVPIntrinsic(Instruction &I) {
       (SplatCost * !Op0->hasOneUse()) + (SplatCost * !Op1->hasOneUse());
   InstructionCost NewCost = ScalarOpCost + SplatCost + CostToKeepSplats;
 
-  LLVM_DEBUG(dbgs() << "Found a VP Intrinsic to scalarize: " << VPI
-                    << "\n");
+  LLVM_DEBUG(dbgs() << "Found a VP Intrinsic to scalarize: " << VPI << "\n");
   LLVM_DEBUG(dbgs() << "Cost of Intrinsic: " << OldCost
                     << ", Cost of scalarizing:" << NewCost << "\n");
 
@@ -2330,10 +2328,12 @@ bool VectorCombine::foldPermuteOfBinops(Instruction &I) {
   }
 
   unsigned NumOpElts = Op0Ty->getNumElements();
-  bool IsIdentity0 = ShuffleDstTy == Op0Ty &&
+  bool IsIdentity0 =
+      ShuffleDstTy == Op0Ty &&
       all_of(NewMask0, [NumOpElts](int M) { return M < (int)NumOpElts; }) &&
       ShuffleVectorInst::isIdentityMask(NewMask0, NumOpElts);
-  bool IsIdentity1 = ShuffleDstTy == Op1Ty &&
+  bool IsIdentity1 =
+      ShuffleDstTy == Op1Ty &&
       all_of(NewMask1, [NumOpElts](int M) { return M < (int)NumOpElts; }) &&
       ShuffleVectorInst::isIdentityMask(NewMask1, NumOpElts);
 
@@ -3204,7 +3204,7 @@ bool VectorCombine::foldShuffleOfIntrinsics(Instruction &I) {
 bool VectorCombine::foldPermuteOfIntrinsic(Instruction &I) {
   Value *V0;
   ArrayRef<int> Mask;
-  if (!match(&I, m_Shuffle(m_OneUse(m_Value(V0)), m_Undef(), m_Mask(Mask))))
+  if (!match(&I, m_Shuffle(m_Value(V0), m_Undef(), m_Mask(Mask))))
     return false;
 
   auto *II0 = dyn_cast<IntrinsicInst>(V0);
@@ -3226,8 +3226,10 @@ bool VectorCombine::foldPermuteOfIntrinsic(Instruction &I) {
     return false;
 
   // Cost analysis
+  InstructionCost IntrinsicCost =
+      TTI.getIntrinsicInstrCost(IntrinsicCostAttributes(IID, *II0), CostKind);
   InstructionCost OldCost =
-      TTI.getIntrinsicInstrCost(IntrinsicCostAttributes(IID, *II0), CostKind) +
+      IntrinsicCost +
       TTI.getShuffleCost(TargetTransformInfo::SK_PermuteSingleSrc, ShuffleDstTy,
                          IntrinsicSrcTy, Mask, CostKind, 0, nullptr, {V0}, &I);
 
@@ -3249,6 +3251,11 @@ bool VectorCombine::foldPermuteOfIntrinsic(Instruction &I) {
   IntrinsicCostAttributes NewAttr(IID, ShuffleDstTy, NewArgsTy);
   NewCost += TTI.getIntrinsicInstrCost(NewAttr, CostKind);
 
+  // If the intrinsic has multiple uses, we need to account for the cost of
+  // keeping the original intrinsic around.
+  if (!II0->hasOneUse())
+    NewCost += IntrinsicCost;
+
   LLVM_DEBUG(dbgs() << "Found a permute of intrinsic: " << I << "\n  OldCost: "
                     << OldCost << " vs NewCost: " << NewCost << "\n");
 
diff --git a/llvm/test/Transforms/VectorCombine/X86/shuffle-of-fma-const.ll b/llvm/test/Transforms/VectorCombine/X86/shuffle-of-fma-const.ll
index ff810b615bac9..78879d4903a97 100644
--- a/llvm/test/Transforms/VectorCombine/X86/shuffle-of-fma-const.ll
+++ b/llvm/test/Transforms/VectorCombine/X86/shuffle-of-fma-const.ll
@@ -52,3 +52,35 @@ define <8 x float> @interleave_fma_const_chain(<4 x float> %a0, <4 x float> %a1)
   %res = shufflevector <4 x float> %l, <4 x float> %h, <8 x i32> <i32 0, i32 4, i32 1, i32 5, i32 2, i32 6, i32 3, i32 7>
   ret <8 x float> %res
 }
+
+; Negative test - multiple uses make the transformation unprofitable
+define <4 x float> @shuffle_fma_const_chain_multiuse(<4 x float> %a0, ptr %p) {
+; CHECK-LABEL: define <4 x float> @shuffle_fma_const_chain_multiuse(
+; CHECK-SAME: <4 x float> [[A0:%.*]], ptr [[P:%.*]]) #[[ATTR0]] {
+; CHECK-NEXT:    [[F:%.*]] = tail call noundef <4 x float> @llvm.fma.v4f32(<4 x float> [[A0]], <4 x float> splat (float 0x3F8DE8D040000000), <4 x float> splat (float 0xBFB3715EE0000000))
+; CHECK-NEXT:    [[RES:%.*]] = shufflevector <4 x float> [[F]], <4 x float> poison, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
+; CHECK-NEXT:    store <4 x float> [[F]], ptr [[P]], align 16
+; CHECK-NEXT:    ret <4 x float> [[RES]]
+;
+  %f = tail call noundef <4 x float> @llvm.fma.v4f32(<4 x float> %a0, <4 x float> splat (float 0x3F8DE8D040000000), <4 x float> splat (float 0xBFB3715EE0000000))
+  %res = shufflevector <4 x float> %f, <4 x float> poison, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
+  store <4 x float> %f, ptr %p, align 16
+  ret <4 x float> %res
+}
+
+; Negative test - intrinsic used by shuffle and arithmetic
+define <4 x float> @shuffle_fma_multiuse_with_arith(<4 x float> %a0, <4 x float> %b) {
+; CHECK-LABEL: define <4 x float> @shuffle_fma_multiuse_with_arith(
+; CHECK-SAME: <4 x float> [[A0:%.*]], <4 x float> [[B:%.*]]) #[[ATTR0]] {
+; CHECK-NEXT:    [[F:%.*]] = tail call noundef <4 x float> @llvm.fma.v4f32(<4 x float> [[A0]], <4 x float> splat (float 0x3F8DE8D040000000), <4 x float> splat (float 0xBFB3715EE0000000))
+; CHECK-NEXT:    [[SHUF:%.*]] = shufflevector <4 x float> [[F]], <4 x float> poison, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
+; CHECK-NEXT:    [[ADD:%.*]] = fadd <4 x float> [[F]], [[B]]
+; CHECK-NEXT:    [[RES:%.*]] = fadd <4 x float> [[SHUF]], [[ADD]]
+; CHECK-NEXT:    ret <4 x float> [[RES]]
+;
+  %f = tail call noundef <4 x float> @llvm.fma.v4f32(<4 x float> %a0, <4 x float> splat (float 0x3F8DE8D040000000), <4 x float> splat (float 0xBFB3715EE0000000))
+  %shuf = shufflevector <4 x float> %f, <4 x float> poison, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
+  %add = fadd <4 x float> %f, %b
+  %res = fadd <4 x float> %shuf, %add
+  ret <4 x float> %res
+}

From 937c71377b4a6858c107ef351cd3bb968939dc4b Mon Sep 17 00:00:00 2001
From: Natalia Kokoromyti <nataliakokoromyti@gmail.com>
Date: Sat, 10 Jan 2026 16:29:17 -0800
Subject: [PATCH] [clang][bytecode] Fix crash on arrays with excessive size

The bytecode interpreter was crashing when encountering arrays with
sizes that exceed Descriptor::MaxArrayElemBytes. The bounds check in
Program::createDescriptor was using std::numeric_limits<unsigned>::max()
instead of the correct limit Descriptor::MaxArrayElemBytes.

This caused the check to pass for sizes that would later fail the
assertion in the Descriptor constructor.

Fixes #175293
---
 clang/lib/AST/ByteCode/Program.cpp          |  4 ++--
 clang/test/AST/ByteCode/huge-array-size.cpp | 10 ++++++++++
 2 files changed, 12 insertions(+), 2 deletions(-)
 create mode 100644 clang/test/AST/ByteCode/huge-array-size.cpp

diff --git a/clang/lib/AST/ByteCode/Program.cpp b/clang/lib/AST/ByteCode/Program.cpp
index d96934071cb60..a9ed47df89a86 100644
--- a/clang/lib/AST/ByteCode/Program.cpp
+++ b/clang/lib/AST/ByteCode/Program.cpp
@@ -411,7 +411,7 @@ Descriptor *Program::createDescriptor(const DeclTy &D, const Type *Ty,
       if (OptPrimType T = Ctx.classify(ElemTy)) {
         // Arrays of primitives.
         unsigned ElemSize = primSize(*T);
-        if (std::numeric_limits<unsigned>::max() / ElemSize <= NumElems) {
+        if (Descriptor::MaxArrayElemBytes / ElemSize < NumElems) {
           return {};
         }
         return allocateDescriptor(D, *T, MDSize, NumElems, IsConst, IsTemporary,
@@ -424,7 +424,7 @@ Descriptor *Program::createDescriptor(const DeclTy &D, const Type *Ty,
         if (!ElemDesc)
           return nullptr;
         unsigned ElemSize = ElemDesc->getAllocSize() + sizeof(InlineDescriptor);
-        if (std::numeric_limits<unsigned>::max() / ElemSize <= NumElems)
+        if (Descriptor::MaxArrayElemBytes / ElemSize < NumElems)
           return {};
         return allocateDescriptor(D, Ty, ElemDesc, MDSize, NumElems, IsConst,
                                   IsTemporary, IsMutable);

From 9af9ed3356707a501b658308f4cd695746874f49 Mon Sep 17 00:00:00 2001
From: Natalia Kokoromyti <nataliakokoromyti@gmail.com>
Date: Sat, 10 Jan 2026 21:54:11 -0800
Subject: [PATCH] [Sema] Fix crash in asm goto with undeclared label

When an asm goto statement references an undeclared label and there's
a variable with __attribute__((cleanup)) in scope, clang would crash
with a segmentation fault.

The issue was that DiagnoseIndirectOrAsmJumpStmt() called
Target->getStmt()->getIdentLoc() without checking if getStmt() returns
null. For undeclared labels, the LabelDecl exists but has no associated
LabelStmt.

This patch adds a null check and falls back to Target->getLocation()
when the statement is null.

Fixes #175314
---
 clang/lib/Sema/JumpDiagnostics.cpp                |  7 +++++--
 clang/test/Sema/asm-goto-undeclared-label-crash.c | 12 ++++++++++++
 2 files changed, 17 insertions(+), 2 deletions(-)
 create mode 100644 clang/test/Sema/asm-goto-undeclared-label-crash.c

diff --git a/clang/lib/Sema/JumpDiagnostics.cpp b/clang/lib/Sema/JumpDiagnostics.cpp
index 36c9d9afb37f1..b630559c2db2c 100644
--- a/clang/lib/Sema/JumpDiagnostics.cpp
+++ b/clang/lib/Sema/JumpDiagnostics.cpp
@@ -914,8 +914,11 @@ static void DiagnoseIndirectOrAsmJumpStmt(Sema &S, Stmt *Jump,
   bool IsAsmGoto = isa<GCCAsmStmt>(Jump);
   S.Diag(Jump->getBeginLoc(), diag::err_indirect_goto_in_protected_scope)
       << IsAsmGoto;
-  S.Diag(Target->getStmt()->getIdentLoc(), diag::note_indirect_goto_target)
-      << IsAsmGoto;
+  // Target->getStmt() can be null for undeclared labels.
+  SourceLocation TargetLoc = Target->getStmt()
+                                 ? Target->getStmt()->getIdentLoc()
+                                 : Target->getLocation();
+  S.Diag(TargetLoc, diag::note_indirect_goto_target) << IsAsmGoto;
   Diagnosed = true;
 }
 
From b6263bf2b053ebef7662a430aa55301b1c21e25e Mon Sep 17 00:00:00 2001
From: Justin Stitt <justinstitt@google.com>
Date: Mon, 5 Jan 2026 11:39:46 -0800
Subject: [PATCH 1/3] [CodeGen] Check BlockAddress users before marking block
 as taken

After commit 4109bac3301e ("[IR] Do not store Function inside BlockAddress"),
the `hasAddressTaken` flag on BasicBlock can become stale. When a BlockAddress
constant is created, the flag is set to true. However, if optimizations
remove all users of that BlockAddress (e.g., simplifying an `indirectbr` to
a direct branch), the BlockAddress may remain in the uniquing table with no
users, but the flag stays true.

Check that BlockAddress actually has users before marking the MBB. If
the BlockAddress exists but has no users (probably because they were
optimized away), we can skip marking the block.

The performance impact should be negligible as we still only check for
users after verifying that the block address has its address taken
somewhere, which should be false pretty often (I think?!).

Signed-off-by: Justin Stitt <justinstitt@google.com>
---
 llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp          | 10 ++++++++--
 .../lib/CodeGen/SelectionDAG/FunctionLoweringInfo.cpp | 11 +++++++++--
 .../X86/speculative-load-hardening-indirect.ll        |  8 ++++----
 3 files changed, 21 insertions(+), 8 deletions(-)

diff --git a/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp b/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp
index 12552bce3caaa..18a6dd2c15365 100644
--- a/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp
+++ b/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp
@@ -4217,8 +4217,14 @@ bool IRTranslator::runOnMachineFunction(MachineFunction &CurMF) {
     MBB = MF->CreateMachineBasicBlock(&BB);
     MF->push_back(MBB);
 
-    if (BB.hasAddressTaken())
-      MBB->setAddressTakenIRBlock(const_cast<BasicBlock *>(&BB));
+    // Only mark the block if the BlockAddress actually has users. The
+    // hasAddressTaken flag may be stale if the BlockAddress was optimized away
+    // but the constant still exists in the uniquing table.
+    if (BB.hasAddressTaken()) {
+      if (BlockAddress *BA = BlockAddress::lookup(&BB))
+        if (!BA->use_empty())
+          MBB->setAddressTakenIRBlock(const_cast<BasicBlock *>(&BB));
+    }
 
     if (!HasMustTailInVarArgFn)
       HasMustTailInVarArgFn = checkForMustTailInVarArgFn(IsVarArg, BB);
diff --git a/llvm/lib/CodeGen/SelectionDAG/FunctionLoweringInfo.cpp b/llvm/lib/CodeGen/SelectionDAG/FunctionLoweringInfo.cpp
index e73743ecbc9fa..74100eeeefcdd 100644
--- a/llvm/lib/CodeGen/SelectionDAG/FunctionLoweringInfo.cpp
+++ b/llvm/lib/CodeGen/SelectionDAG/FunctionLoweringInfo.cpp
@@ -26,6 +26,7 @@
 #include "llvm/CodeGen/TargetSubtargetInfo.h"
 #include "llvm/CodeGen/WasmEHFuncInfo.h"
 #include "llvm/CodeGen/WinEHFuncInfo.h"
+#include "llvm/IR/Constants.h"
 #include "llvm/IR/DataLayout.h"
 #include "llvm/IR/DerivedTypes.h"
 #include "llvm/IR/Function.h"
@@ -277,8 +278,14 @@ void FunctionLoweringInfo::set(const Function &fn, MachineFunction &mf,
     // Transfer the address-taken flag. This is necessary because there could
     // be multiple MachineBasicBlocks corresponding to one BasicBlock, and only
     // the first one should be marked.
-    if (BB.hasAddressTaken())
-      MBB->setAddressTakenIRBlock(const_cast<BasicBlock *>(&BB));
+    // Only mark the block if the BlockAddress actually has users. The
+    // hasAddressTaken flag may be stale if the BlockAddress was optimized away
+    // but the constant still exists in the uniquing table.
+    if (BB.hasAddressTaken()) {
+      if (BlockAddress *BA = BlockAddress::lookup(&BB))
+        if (!BA->use_empty())
+          MBB->setAddressTakenIRBlock(const_cast<BasicBlock *>(&BB));
+    }
 
     // Mark landing pad blocks.
     if (BB.isEHPad())
diff --git a/llvm/test/CodeGen/X86/speculative-load-hardening-indirect.ll b/llvm/test/CodeGen/X86/speculative-load-hardening-indirect.ll
index fd5085c8c2ac9..2092fc9cd6e00 100644
--- a/llvm/test/CodeGen/X86/speculative-load-hardening-indirect.ll
+++ b/llvm/test/CodeGen/X86/speculative-load-hardening-indirect.ll
@@ -464,34 +464,34 @@ define dso_local i32 @test_indirectbr_global(i32 %idx) nounwind {
 ; X64-RETPOLINE-NEXT:    cmoveq %rax, %rcx
 ; X64-RETPOLINE-NEXT:    cmpq $4, %rdx
 ; X64-RETPOLINE-NEXT:    jne .LBB6_3
-; X64-RETPOLINE-NEXT:  .Ltmp0: # Block address taken
 ; X64-RETPOLINE-NEXT:  # %bb.6: # %bb3
 ; X64-RETPOLINE-NEXT:    cmovneq %rax, %rcx
 ; X64-RETPOLINE-NEXT:    shlq $47, %rcx
 ; X64-RETPOLINE-NEXT:    movl $42, %eax
 ; X64-RETPOLINE-NEXT:    orq %rcx, %rsp
 ; X64-RETPOLINE-NEXT:    retq
-; X64-RETPOLINE-NEXT:  .Ltmp1: # Block address taken
 ; X64-RETPOLINE-NEXT:  .LBB6_5: # %bb2
 ; X64-RETPOLINE-NEXT:    cmovneq %rax, %rcx
 ; X64-RETPOLINE-NEXT:    shlq $47, %rcx
 ; X64-RETPOLINE-NEXT:    movl $13, %eax
 ; X64-RETPOLINE-NEXT:    orq %rcx, %rsp
 ; X64-RETPOLINE-NEXT:    retq
-; X64-RETPOLINE-NEXT:  .Ltmp2: # Block address taken
 ; X64-RETPOLINE-NEXT:  .LBB6_4: # %bb1
 ; X64-RETPOLINE-NEXT:    cmovneq %rax, %rcx
 ; X64-RETPOLINE-NEXT:    shlq $47, %rcx
 ; X64-RETPOLINE-NEXT:    movl $7, %eax
 ; X64-RETPOLINE-NEXT:    orq %rcx, %rsp
 ; X64-RETPOLINE-NEXT:    retq
-; X64-RETPOLINE-NEXT:  .Ltmp3: # Block address taken
 ; X64-RETPOLINE-NEXT:  .LBB6_3: # %bb0
 ; X64-RETPOLINE-NEXT:    cmoveq %rax, %rcx
 ; X64-RETPOLINE-NEXT:    shlq $47, %rcx
 ; X64-RETPOLINE-NEXT:    movl $2, %eax
 ; X64-RETPOLINE-NEXT:    orq %rcx, %rsp
 ; X64-RETPOLINE-NEXT:    retq
+; X64-RETPOLINE-NEXT:  .Ltmp0: # Address of block that was removed by CodeGen
+; X64-RETPOLINE-NEXT:  .Ltmp1: # Address of block that was removed by CodeGen
+; X64-RETPOLINE-NEXT:  .Ltmp2: # Address of block that was removed by CodeGen
+; X64-RETPOLINE-NEXT:  .Ltmp3: # Address of block that was removed by CodeGen
 entry:
   %ptr = getelementptr [4 x ptr], ptr @global_blockaddrs, i32 0, i32 %idx
   %a = load ptr, ptr %ptr

From 6bb4ffbd9800534b17239c35bf14f980446196f0 Mon Sep 17 00:00:00 2001
From: Justin Stitt <justinstitt@google.com>
Date: Mon, 5 Jan 2026 15:17:36 -0800
Subject: [PATCH 2/3] add more tests

Signed-off-by: Justin Stitt <justinstitt@google.com>
---
 .../blockaddress-stale-addresstaken.ll        | 48 +++++++++++++++++++
 .../X86/blockaddress-stale-addresstaken.ll    | 39 +++++++++++++++
 2 files changed, 87 insertions(+)
 create mode 100644 llvm/test/CodeGen/AArch64/GlobalISel/blockaddress-stale-addresstaken.ll
 create mode 100644 llvm/test/CodeGen/X86/blockaddress-stale-addresstaken.ll

diff --git a/llvm/test/CodeGen/AArch64/GlobalISel/blockaddress-stale-addresstaken.ll b/llvm/test/CodeGen/AArch64/GlobalISel/blockaddress-stale-addresstaken.ll
new file mode 100644
index 0000000000000..601645e0944f5
--- /dev/null
+++ b/llvm/test/CodeGen/AArch64/GlobalISel/blockaddress-stale-addresstaken.ll
@@ -0,0 +1,48 @@
+; RUN: llc -O0 -mtriple=aarch64-linux-gnu -global-isel -stop-after=irtranslator %s -o - | FileCheck %s
+
+; Test that the GlobalISel IRTranslator correctly marks blocks as address-taken
+; based on whether the BlockAddress actually has users.
+
+; CHECK-LABEL: name: test_indirectbr_blockaddress
+; CHECK: G_BLOCK_ADDR blockaddress(@test_indirectbr_blockaddress, %ir-block.target)
+; CHECK: G_BLOCK_ADDR blockaddress(@test_indirectbr_blockaddress, %ir-block.other)
+; CHECK: G_BRINDIRECT
+; CHECK: bb.{{[0-9]+}}.target (ir-block-address-taken %ir-block.target):
+; CHECK: bb.{{[0-9]+}}.other (ir-block-address-taken %ir-block.other):
+define i32 @test_indirectbr_blockaddress(i32 %idx) {
+entry:
+  %targets = alloca [2 x ptr], align 8
+  %ptr0 = getelementptr [2 x ptr], ptr %targets, i64 0, i64 0
+  store ptr blockaddress(@test_indirectbr_blockaddress, %target), ptr %ptr0, align 8
+  %ptr1 = getelementptr [2 x ptr], ptr %targets, i64 0, i64 1
+  store ptr blockaddress(@test_indirectbr_blockaddress, %other), ptr %ptr1, align 8
+  %idx64 = zext i32 %idx to i64
+  %selected = getelementptr [2 x ptr], ptr %targets, i64 0, i64 %idx64
+  %dest = load ptr, ptr %selected, align 8
+  indirectbr ptr %dest, [label %target, label %other]
+
+target:
+  ret i32 42
+
+other:
+  ret i32 -1
+}
+
+; normal conditional branch (no blockaddress).
+; blocks should NOT be marked as address-taken.
+
+; CHECK-LABEL: name: test_normal_branch
+; CHECK: bb.{{[0-9]+}}.target:
+; CHECK-NOT: ir-block-address-taken
+; CHECK: bb.{{[0-9]+}}.other:
+; CHECK-NOT: ir-block-address-taken
+define i32 @test_normal_branch(i1 %cond) {
+entry:
+  br i1 %cond, label %target, label %other
+
+target:
+  ret i32 42
+
+other:
+  ret i32 -1
+}
diff --git a/llvm/test/CodeGen/X86/blockaddress-stale-addresstaken.ll b/llvm/test/CodeGen/X86/blockaddress-stale-addresstaken.ll
new file mode 100644
index 0000000000000..52adca08cb259
--- /dev/null
+++ b/llvm/test/CodeGen/X86/blockaddress-stale-addresstaken.ll
@@ -0,0 +1,39 @@
+; RUN: llc < %s -mtriple=x86_64-unknown-linux-gnu -mattr=+retpoline | FileCheck %s
+;
+; verify that blocks are NOT marked as "Block address taken" when the
+; BlockAddress constant has no users (was optimized away).
+;
+; With retpoline enabled, the indirectbr is replaced with direct comparisons
+; against constant integer values. The BlockAddress constants in the global
+; array become unused (constant-folded to integers), so the blocks should NOT
+; be marked as address-taken.
+
+@targets = internal constant [4 x ptr] [
+  ptr blockaddress(@test_stale_addresstaken, %bb0),
+  ptr blockaddress(@test_stale_addresstaken, %bb1),
+  ptr blockaddress(@test_stale_addresstaken, %bb2),
+  ptr blockaddress(@test_stale_addresstaken, %bb3)
+]
+
+define i32 @test_stale_addresstaken(i32 %idx) {
+entry:
+  %ptr = getelementptr [4 x ptr], ptr @targets, i32 0, i32 %idx
+  %dest = load ptr, ptr %ptr
+  indirectbr ptr %dest, [label %bb0, label %bb1, label %bb2, label %bb3]
+
+; CHECK-LABEL: test_stale_addresstaken:
+; CHECK-NOT: Block address taken
+; CHECK-LABEL: .Lfunc_end0:
+
+bb0:
+  ret i32 0
+
+bb1:
+  ret i32 1
+
+bb2:
+  ret i32 2
+
+bb3:
+  ret i32 3
+}

From dd814bcea4d6b083f023074b6cc812334b180a33 Mon Sep 17 00:00:00 2001
From: Justin Stitt <justinstitt@google.com>
Date: Mon, 5 Jan 2026 15:34:29 -0800
Subject: [PATCH 3/3] use hasZeroLiveUses over use_empty

Signed-off-by: Justin Stitt <justinstitt@google.com>
---
 llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp           | 2 +-
 llvm/lib/CodeGen/SelectionDAG/FunctionLoweringInfo.cpp | 2 +-
 2 files changed, 2 insertions(+), 2 deletions(-)

diff --git a/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp b/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp
index 18a6dd2c15365..e1b7e192939c9 100644
--- a/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp
+++ b/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp
@@ -4222,7 +4222,7 @@ bool IRTranslator::runOnMachineFunction(MachineFunction &CurMF) {
     // but the constant still exists in the uniquing table.
     if (BB.hasAddressTaken()) {
       if (BlockAddress *BA = BlockAddress::lookup(&BB))
-        if (!BA->use_empty())
+        if (!BA->hasZeroLiveUses())
           MBB->setAddressTakenIRBlock(const_cast<BasicBlock *>(&BB));
     }
 
diff --git a/llvm/lib/CodeGen/SelectionDAG/FunctionLoweringInfo.cpp b/llvm/lib/CodeGen/SelectionDAG/FunctionLoweringInfo.cpp
index 74100eeeefcdd..dfaabae6e1f97 100644
--- a/llvm/lib/CodeGen/SelectionDAG/FunctionLoweringInfo.cpp
+++ b/llvm/lib/CodeGen/SelectionDAG/FunctionLoweringInfo.cpp
@@ -283,7 +283,7 @@ void FunctionLoweringInfo::set(const Function &fn, MachineFunction &mf,
     // but the constant still exists in the uniquing table.
     if (BB.hasAddressTaken()) {
       if (BlockAddress *BA = BlockAddress::lookup(&BB))
-        if (!BA->use_empty())
+        if (!BA->hasZeroLiveUses())
           MBB->setAddressTakenIRBlock(const_cast<BasicBlock *>(&BB));
     }
 

From 96583b36b24c9436a05ca8748ee99a490ed30038 Mon Sep 17 00:00:00 2001
From: Marius Kamp <msk@posteo.org>
Date: Fri, 9 Jan 2026 17:46:52 +0100
Subject: [PATCH 1/2] [X86] Handle Oversized Immediates in Asm Expression

For asm expressions with an 'i' input constraint and a constant with
more than 64 significant bits, we used to violate the assertion that
`getSignificantBits() <= 64` in `APInt::getSExtValue()`.

GCC, however, handles constants that fit into 64 bits by truncating them
appropriately. For constants that do not fit into 64 bits, GCC 15
generates invalid assembler, which causes the compilation to fail.
As inline assembler expressions are modeled after the way GCC handles
inline assembly, LLVM should behave similarly here.

This change avoids violating the mentioned assertion by handling
constant types with bit widths exceeding 64 bits in a way similar to
GCC: If the constant fits into 64 bits, the constant is truncated
appropriately. If not, the backend outputs an error (instead of
generating invalid assembly).

Closes #173841.
---
 llvm/lib/Target/X86/X86ISelLowering.cpp       | 21 ++++++-
 .../inline-asm-i-constraint-i128-invalid.ll   | 17 ++++++
 .../X86/inline-asm-i-constraint-i128.ll       | 58 +++++++++++++++++++
 3 files changed, 93 insertions(+), 3 deletions(-)
 create mode 100644 llvm/test/CodeGen/X86/inline-asm-i-constraint-i128-invalid.ll
 create mode 100644 llvm/test/CodeGen/X86/inline-asm-i-constraint-i128.ll

diff --git a/llvm/lib/Target/X86/X86ISelLowering.cpp b/llvm/lib/Target/X86/X86ISelLowering.cpp
index 40ea3cb76bae4..8c7e688a587e8 100644
--- a/llvm/lib/Target/X86/X86ISelLowering.cpp
+++ b/llvm/lib/Target/X86/X86ISelLowering.cpp
@@ -62364,13 +62364,28 @@ void X86TargetLowering::LowerAsmOperandForConstraint(SDValue Op,
     return;
   }
   case 'i': {
-    // Literal immediates are always ok.
+    // Literal immediates are ok if they fit into a 64-bit register.
     if (auto *CST = dyn_cast<ConstantSDNode>(Op)) {
-      bool IsBool = CST->getConstantIntValue()->getBitWidth() == 1;
+      SDLoc DL(Op);
+      unsigned BitWidth = CST->getConstantIntValue()->getBitWidth();
+      if (BitWidth > 64) {
+        // Check if the value would fit into 64 bit by either treating the
+        // value as an unsigned integer (active bits <= 64) or a signed
+        // integer (significant bits <= 64).
+        const APInt &CSTAPInt = CST->getAPIntValue();
+        if (CSTAPInt.getActiveBits() > 64 && CSTAPInt.getSignificantBits() > 64)
+          DAG.getContext()->diagnose(DiagnosticInfoUnsupported(
+              DAG.getMachineFunction().getFunction(),
+              "unsupported size for integer operand",
+              DiagnosticLocation(DL.getDebugLoc()), DS_Error));
+        Result = DAG.getSignedTargetConstant(
+            CSTAPInt.trunc(64).getLimitedValue(), DL, MVT::i64);
+        break;
+      }
+      bool IsBool = BitWidth == 1;
       BooleanContent BCont = getBooleanContents(MVT::i64);
       ISD::NodeType ExtOpc = IsBool ? getExtendForContent(BCont)
                                     : ISD::SIGN_EXTEND;
-      SDLoc DL(Op);
       Result =
           ExtOpc == ISD::ZERO_EXTEND
               ? DAG.getTargetConstant(CST->getZExtValue(), DL, MVT::i64)
diff --git a/llvm/test/CodeGen/X86/inline-asm-i-constraint-i128-invalid.ll b/llvm/test/CodeGen/X86/inline-asm-i-constraint-i128-invalid.ll
new file mode 100644
index 0000000000000..6159190a0cb31
--- /dev/null
+++ b/llvm/test/CodeGen/X86/inline-asm-i-constraint-i128-invalid.ll
@@ -0,0 +1,17 @@
+; RUN: not llc -mtriple=x86_64-unknown-linux-gnu -filetype=null < %s 2>&1 | FileCheck %s
+
+; PR173841
+define i32 @test_constant_does_not_fit_into_64_bits() {
+; CHECK: error: <unknown>:0:0: in function test_constant_does_not_fit_into_64_bits i32 (): unsupported size for integer operand
+entry:
+  tail call void asm sideeffect "movq $0, %rax", "i,~{dirflag},~{fpsr},~{flags}"(i128 18446744073709551616)
+  ret i32 0
+}
+
+; PR173841
+define i32 @test_negative_constant_does_not_fit_into_64_bits() {
+; CHECK: error: <unknown>:0:0: in function test_negative_constant_does_not_fit_into_64_bits i32 (): unsupported size for integer operand
+entry:
+  tail call void asm sideeffect "movq $0, %rax", "i,~{dirflag},~{fpsr},~{flags}"(i128 -9223372036854775809)
+  ret i32 0
+}
diff --git a/llvm/test/CodeGen/X86/inline-asm-i-constraint-i128.ll b/llvm/test/CodeGen/X86/inline-asm-i-constraint-i128.ll
new file mode 100644
index 0000000000000..6c9214354278b
--- /dev/null
+++ b/llvm/test/CodeGen/X86/inline-asm-i-constraint-i128.ll
@@ -0,0 +1,58 @@
+; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 6
+; RUN: llc -mtriple=x86_64-unknown-linux-gnu < %s | FileCheck %s
+
+; PR173841
+define i32 @test_constant_fits_into_64_bits() {
+; CHECK-LABEL: test_constant_fits_into_64_bits:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    #APP
+; CHECK-NEXT:    movabsq $-6066929602142479174, %rax # imm = 0xABCDEFABBAFEDCBA
+; CHECK-NEXT:    #NO_APP
+; CHECK-NEXT:    xorl %eax, %eax
+; CHECK-NEXT:    retq
+entry:
+  tail call void asm sideeffect "movq $0, %rax", "imr,~{dirflag},~{fpsr},~{flags}"(i128 12379814471567072442)
+  ret i32 0
+}
+
+; PR173841
+define i32 @test_negative_constant_fits_into_64_bits() {
+; CHECK-LABEL: test_negative_constant_fits_into_64_bits:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    #APP
+; CHECK-NEXT:    movq $-1234, %rax # imm = 0xFB2E
+; CHECK-NEXT:    #NO_APP
+; CHECK-NEXT:    xorl %eax, %eax
+; CHECK-NEXT:    retq
+entry:
+  tail call void asm sideeffect "movq $0, %rax", "i,~{dirflag},~{fpsr},~{flags}"(i128 -1234)
+  ret i32 0
+}
+
+; PR173841
+define i32 @test_max_64_bit_unsigned_int() {
+; CHECK-LABEL: test_max_64_bit_unsigned_int:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    #APP
+; CHECK-NEXT:    movq $-1, %rax
+; CHECK-NEXT:    #NO_APP
+; CHECK-NEXT:    xorl %eax, %eax
+; CHECK-NEXT:    retq
+entry:
+  tail call void asm sideeffect "movq $0, %rax", "i,~{dirflag},~{fpsr},~{flags}"(i128 18446744073709551615)
+  ret i32 0
+}
+
+; PR173841
+define i32 @test_min_64_bit_signed_int() {
+; CHECK-LABEL: test_min_64_bit_signed_int:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    #APP
+; CHECK-NEXT:    movabsq $-9223372036854775808, %rax # imm = 0x8000000000000000
+; CHECK-NEXT:    #NO_APP
+; CHECK-NEXT:    xorl %eax, %eax
+; CHECK-NEXT:    retq
+entry:
+  tail call void asm sideeffect "movq $0, %rax", "i,~{dirflag},~{fpsr},~{flags}"(i128 -9223372036854775808)
+  ret i32 0
+}

From b31b7c04b0f20a438f859a9d78ae75b0075b4d09 Mon Sep 17 00:00:00 2001
From: Marius Kamp <msk@posteo.org>
Date: Sat, 10 Jan 2026 07:39:12 +0100
Subject: [PATCH 2/2] Fix Formatting

---
 llvm/lib/Target/X86/X86ISelLowering.cpp | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/llvm/lib/Target/X86/X86ISelLowering.cpp b/llvm/lib/Target/X86/X86ISelLowering.cpp
index 8c7e688a587e8..e56b23ee1f9bf 100644
--- a/llvm/lib/Target/X86/X86ISelLowering.cpp
+++ b/llvm/lib/Target/X86/X86ISelLowering.cpp
@@ -62384,8 +62384,8 @@ void X86TargetLowering::LowerAsmOperandForConstraint(SDValue Op,
       }
       bool IsBool = BitWidth == 1;
       BooleanContent BCont = getBooleanContents(MVT::i64);
-      ISD::NodeType ExtOpc = IsBool ? getExtendForContent(BCont)
-                                    : ISD::SIGN_EXTEND;
+      ISD::NodeType ExtOpc =
+          IsBool ? getExtendForContent(BCont) : ISD::SIGN_EXTEND;
       Result =
           ExtOpc == ISD::ZERO_EXTEND
               ? DAG.getTargetConstant(CST->getZExtValue(), DL, MVT::i64)

--- a/llvm/lib/Transforms/IPO/WholeProgramDevirt.cpp	2025-12-25 11:42:19.902348610 +0200
+++ b/llvm/lib/Transforms/IPO/WholeProgramDevirt.cpp	2025-12-25 11:45:48.513795496 +0200
@@ -239,19 +239,32 @@ static cl::opt<WPDCheckMode> DevirtCheck
 namespace {
 struct PatternList {
   std::vector<GlobPattern> Patterns;
-  template <class T> void init(const T &StringList) {
-    for (const auto &S : StringList)
-      if (Expected<GlobPattern> Pat = GlobPattern::create(S))
+
+  template <class T>
+  void init(const T &StringList) {
+    Patterns.clear();
+    Patterns.reserve(StringList.size());
+    for (const auto &S : StringList) {
+      if (Expected<GlobPattern> Pat = GlobPattern::create(S)) {
         Patterns.push_back(std::move(*Pat));
+      }
+    }
   }
-  bool match(StringRef S) {
-    for (const GlobPattern &P : Patterns)
-      if (P.match(S))
+
+  bool match(StringRef S) const {
+    // Fast path: empty pattern list is common case (no skip list configured)
+    if (Patterns.empty()) {
+      return false;
+    }
+    for (const GlobPattern &P : Patterns) {
+      if (P.match(S)) {
         return true;
+      }
+    }
     return false;
   }
 };
-} // namespace
+} // end anonymous namespace
 
 // Find the minimum offset that we may store a value of size Size bits at. If
 // IsAfter is set, look for an offset before the object, otherwise look for an
@@ -262,33 +275,19 @@ wholeprogramdevirt::findLowestOffset(Arr
   // Find a minimum offset taking into account only vtable sizes.
   uint64_t MinByte = 0;
   for (const VirtualCallTarget &Target : Targets) {
-    if (IsAfter)
+    if (IsAfter) {
       MinByte = std::max(MinByte, Target.minAfterBytes());
-    else
+    } else {
       MinByte = std::max(MinByte, Target.minBeforeBytes());
+    }
   }
 
   // Build a vector of arrays of bytes covering, for each target, a slice of the
-  // used region (see AccumBitVector::BytesUsed in
-  // llvm/Transforms/IPO/WholeProgramDevirt.h) starting at MinByte. Effectively,
-  // this aligns the used regions to start at MinByte.
-  //
-  // In this example, A, B and C are vtables, # is a byte already allocated for
-  // a virtual function pointer, AAAA... (etc.) are the used regions for the
-  // vtables and Offset(X) is the value computed for the Offset variable below
-  // for X.
-  //
-  //                    Offset(A)
-  //                    |       |
-  //                            |MinByte
-  // A: ################AAAAAAAA|AAAAAAAA
-  // B: ########BBBBBBBBBBBBBBBB|BBBB
-  // C: ########################|CCCCCCCCCCCCCCCC
-  //            |   Offset(B)   |
-  //
-  // This code produces the slices of A, B and C that appear after the divider
-  // at MinByte.
-  std::vector<ArrayRef<uint8_t>> Used;
+  // used region starting at MinByte. Use SmallVector to avoid heap allocation
+  // for typical vtable counts (< 8 implementations per virtual function).
+  SmallVector<ArrayRef<uint8_t>, 8> Used;
+  Used.reserve(Targets.size());
+
   for (const VirtualCallTarget &Target : Targets) {
     ArrayRef<uint8_t> VTUsed = IsAfter ? Target.TM->Bits->After.BytesUsed
                                        : Target.TM->Bits->Before.BytesUsed;
@@ -297,36 +296,53 @@ wholeprogramdevirt::findLowestOffset(Arr
 
     // Disregard used regions that are smaller than Offset. These are
     // effectively all-free regions that do not need to be checked.
-    if (VTUsed.size() > Offset)
+    if (VTUsed.size() > Offset) {
       Used.push_back(VTUsed.slice(Offset));
+    }
   }
 
   if (Size == 1) {
     // Find a free bit in each member of Used.
     for (unsigned I = 0;; ++I) {
       uint8_t BitsUsed = 0;
-      for (auto &&B : Used)
-        if (I < B.size())
+      for (const auto &B : Used) {
+        if (I < B.size()) {
           BitsUsed |= B[I];
-      if (BitsUsed != 0xff)
-        return (MinByte + I) * 8 + llvm::countr_zero(uint8_t(~BitsUsed));
+          // Early termination: if all bits are used, no point checking more
+          if (BitsUsed == 0xff) {
+            break;
+          }
+        }
+      }
+      if (BitsUsed != 0xff) {
+        // Found a byte with at least one free bit
+        // countr_zero returns the index of the least significant set bit in ~BitsUsed
+        return (MinByte + I) * 8 +
+               static_cast<uint64_t>(llvm::countr_zero(static_cast<uint8_t>(~BitsUsed)));
+      }
     }
   } else {
     // Find a free (Size/8) byte region in each member of Used.
-    // FIXME: see if alignment helps.
+    const unsigned BytesNeeded = Size / 8;
     for (unsigned I = 0;; ++I) {
-      for (auto &&B : Used) {
-        unsigned Byte = 0;
-        while ((I + Byte) < B.size() && Byte < (Size / 8)) {
-          if (B[I + Byte])
-            goto NextI;
-          ++Byte;
+      bool FoundFreeRegion = true;
+      for (const auto &B : Used) {
+        for (unsigned Byte = 0; Byte < BytesNeeded; ++Byte) {
+          const unsigned Idx = I + Byte;
+          if (Idx < B.size() && B[Idx] != 0) {
+            FoundFreeRegion = false;
+            break;
+          }
+        }
+        if (!FoundFreeRegion) {
+          break;
         }
       }
-      // Rounding up ensures the constant is always stored at address we
-      // can directly load from without misalignment.
-      return alignTo((MinByte + I) * 8, Size);
-    NextI:;
+      if (FoundFreeRegion) {
+        // Rounding up ensures the constant is always stored at address we
+        // can directly load from without misalignment.
+        return alignTo((MinByte + I) * 8, Size);
+      }
     }
   }
 }
@@ -334,34 +350,42 @@ wholeprogramdevirt::findLowestOffset(Arr
 void wholeprogramdevirt::setBeforeReturnValues(
     MutableArrayRef<VirtualCallTarget> Targets, uint64_t AllocBefore,
     unsigned BitWidth, int64_t &OffsetByte, uint64_t &OffsetBit) {
-  if (BitWidth == 1)
-    OffsetByte = -(AllocBefore / 8 + 1);
-  else
-    OffsetByte = -((AllocBefore + 7) / 8 + (BitWidth + 7) / 8);
+  if (BitWidth == 1) {
+    // Cast before arithmetic to ensure signed computation
+    OffsetByte = -(static_cast<int64_t>(AllocBefore / 8) + 1);
+  } else {
+    // Explicit casts prevent implicit conversion warnings and ensure
+    // correct signed arithmetic for negative offset computation
+    OffsetByte = -(static_cast<int64_t>((AllocBefore + 7) / 8) +
+                   static_cast<int64_t>((BitWidth + 7) / 8));
+  }
   OffsetBit = AllocBefore % 8;
 
   for (VirtualCallTarget &Target : Targets) {
-    if (BitWidth == 1)
+    if (BitWidth == 1) {
       Target.setBeforeBit(AllocBefore);
-    else
+    } else {
       Target.setBeforeBytes(AllocBefore, (BitWidth + 7) / 8);
+    }
   }
 }
 
 void wholeprogramdevirt::setAfterReturnValues(
     MutableArrayRef<VirtualCallTarget> Targets, uint64_t AllocAfter,
     unsigned BitWidth, int64_t &OffsetByte, uint64_t &OffsetBit) {
-  if (BitWidth == 1)
-    OffsetByte = AllocAfter / 8;
-  else
-    OffsetByte = (AllocAfter + 7) / 8;
+  if (BitWidth == 1) {
+    OffsetByte = static_cast<int64_t>(AllocAfter / 8);
+  } else {
+    OffsetByte = static_cast<int64_t>((AllocAfter + 7) / 8);
+  }
   OffsetBit = AllocAfter % 8;
 
   for (VirtualCallTarget &Target : Targets) {
-    if (BitWidth == 1)
+    if (BitWidth == 1) {
       Target.setAfterBit(AllocAfter);
-    else
+    } else {
       Target.setAfterBytes(AllocAfter, (BitWidth + 7) / 8);
+    }
   }
 }
 
@@ -423,15 +447,16 @@ template <> struct llvm::DenseMapInfo<VT
 // Returns true if the function must be unreachable based on ValueInfo.
 //
 // In particular, identifies a function as unreachable in the following
-// conditions
+// conditions:
 //   1) All summaries are live.
 //   2) All function summaries indicate it's unreachable
 //   3) There is no non-function with the same GUID (which is rare)
 static bool mustBeUnreachableFunction(ValueInfo TheFnVI) {
-  if (WholeProgramDevirtKeepUnreachableFunction)
+  if (WholeProgramDevirtKeepUnreachableFunction) {
     return false;
+  }
 
-  if ((!TheFnVI) || TheFnVI.getSummaryList().empty()) {
+  if (!TheFnVI || TheFnVI.getSummaryList().empty()) {
     // Returns false if ValueInfo is absent, or the summary list is empty
     // (e.g., function declarations).
     return false;
@@ -440,18 +465,20 @@ static bool mustBeUnreachableFunction(Va
   for (const auto &Summary : TheFnVI.getSummaryList()) {
     // Conservatively returns false if any non-live functions are seen.
     // In general either all summaries should be live or all should be dead.
-    if (!Summary->isLive())
+    if (!Summary->isLive()) {
       return false;
+    }
     if (auto *FS = dyn_cast<FunctionSummary>(Summary->getBaseObject())) {
-      if (!FS->fflags().MustBeUnreachable)
+      if (!FS->fflags().MustBeUnreachable) {
         return false;
-    }
-    // Be conservative if a non-function has the same GUID (which is rare).
-    else
+      }
+    } else {
+      // Be conservative if a non-function has the same GUID (which is rare).
       return false;
+    }
   }
   // All function summaries are live and all of them agree that the function is
-  // unreachble.
+  // unreachable.
   return true;
 }
 
@@ -460,7 +487,7 @@ namespace {
 // the indirect virtual call.
 struct VirtualCallSite {
   Value *VTable = nullptr;
-  CallBase &CB;
+  CallBase *CB = nullptr;
 
   // If non-null, this field points to the associated unsafe use count stored in
   // the DevirtModule::NumUnsafeUsesForTypeTest map below. See the description
@@ -470,9 +497,10 @@ struct VirtualCallSite {
   void
   emitRemark(const StringRef OptName, const StringRef TargetName,
              function_ref<OptimizationRemarkEmitter &(Function &)> OREGetter) {
-    Function *F = CB.getCaller();
-    DebugLoc DLoc = CB.getDebugLoc();
-    BasicBlock *Block = CB.getParent();
+    assert(CB && "Cannot emit remark for null CallBase");
+    Function *F = CB->getCaller();
+    DebugLoc DLoc = CB->getDebugLoc();
+    BasicBlock *Block = CB->getParent();
 
     using namespace ore;
     OREGetter(*F).emit(OptimizationRemark(DEBUG_TYPE, OptName, DLoc, Block)
@@ -485,17 +513,20 @@ struct VirtualCallSite {
       const StringRef OptName, const StringRef TargetName, bool RemarksEnabled,
       function_ref<OptimizationRemarkEmitter &(Function &)> OREGetter,
       Value *New) {
-    if (RemarksEnabled)
+    assert(CB && "Cannot replace null CallBase");
+    if (RemarksEnabled) {
       emitRemark(OptName, TargetName, OREGetter);
-    CB.replaceAllUsesWith(New);
-    if (auto *II = dyn_cast<InvokeInst>(&CB)) {
-      BranchInst::Create(II->getNormalDest(), CB.getIterator());
+    }
+    CB->replaceAllUsesWith(New);
+    if (auto *II = dyn_cast<InvokeInst>(CB)) {
+      BranchInst::Create(II->getNormalDest(), CB->getIterator());
       II->getUnwindDest()->removePredecessor(II->getParent());
     }
-    CB.eraseFromParent();
+    CB->eraseFromParent();
     // This use is no longer unsafe.
-    if (NumUnsafeUses)
+    if (NumUnsafeUses) {
       --*NumUnsafeUses;
+    }
   }
 };
 
@@ -507,7 +538,7 @@ struct CallSiteInfo {
   /// import phase of ThinLTO (as well as the export phase of ThinLTO for any
   /// call sites that appear in the merged module itself); in each of these
   /// cases we are directly operating on the call sites at the IR level.
-  std::vector<VirtualCallSite> CallSites;
+  SmallVector<VirtualCallSite, 4> CallSites;
 
   /// Whether all call sites represented by this CallSiteInfo, including those
   /// in summaries, have been devirtualized. This starts off as true because a
@@ -568,12 +599,14 @@ private:
 CallSiteInfo &VTableSlotInfo::findCallSiteInfo(CallBase &CB) {
   std::vector<uint64_t> Args;
   auto *CBType = dyn_cast<IntegerType>(CB.getType());
-  if (!CBType || CBType->getBitWidth() > 64 || CB.arg_empty())
+  if (!CBType || CBType->getBitWidth() > 64 || CB.arg_empty()) {
     return CSInfo;
+  }
   for (auto &&Arg : drop_begin(CB.args())) {
     auto *CI = dyn_cast<ConstantInt>(Arg);
-    if (!CI || CI->getBitWidth() > 64)
+    if (!CI || CI->getBitWidth() > 64) {
       return CSInfo;
+    }
     Args.push_back(CI->getZExtValue());
   }
   return ConstCSInfo[Args];
@@ -583,7 +616,7 @@ void VTableSlotInfo::addCallSite(Value *
                                  unsigned *NumUnsafeUses) {
   auto &CSI = findCallSiteInfo(CB);
   CSI.AllCallSitesDevirted = false;
-  CSI.CallSites.push_back({VTable, CB, NumUnsafeUses});
+  CSI.CallSites.push_back({VTable, &CB, NumUnsafeUses});
 }
 
 struct DevirtModule {
@@ -861,10 +894,14 @@ skipUpdateDueToValidation(GlobalVariable
   SmallVector<MDNode *, 2> Types;
   GV.getMetadata(LLVMContext::MD_type, Types);
 
-  for (auto *Type : Types)
-    if (auto *TypeID = dyn_cast<MDString>(Type->getOperand(1).get()))
-      return typeIDVisibleToRegularObj(TypeID->getString(),
-                                       IsVisibleToRegularObj);
+  for (auto *Type : Types) {
+    if (auto *TypeID = dyn_cast<MDString>(Type->getOperand(1).get())) {
+      if (typeIDVisibleToRegularObj(TypeID->getString(),
+                                    IsVisibleToRegularObj)) {
+        return true;
+      }
+    }
+  }
 
   return false;
 }
@@ -1074,13 +1111,17 @@ void DevirtModule::buildTypeIdentifierMa
     std::vector<VTableBits> &Bits,
     DenseMap<Metadata *, std::set<TypeMemberInfo>> &TypeIdMap) {
   DenseMap<GlobalVariable *, VTableBits *> GVToBits;
-  Bits.reserve(M.global_size());
+  const size_t GlobalCount = M.global_size();
+  Bits.reserve(GlobalCount);
+  GVToBits.reserve(GlobalCount);
   SmallVector<MDNode *, 2> Types;
+
   for (GlobalVariable &GV : M.globals()) {
     Types.clear();
     GV.getMetadata(LLVMContext::MD_type, Types);
-    if (GV.isDeclaration() || Types.empty())
+    if (GV.isDeclaration() || Types.empty()) {
       continue;
+    }
 
     VTableBits *&BitsPtr = GVToBits[&GV];
     if (!BitsPtr) {
@@ -1108,42 +1149,51 @@ bool DevirtModule::tryFindVirtualCallTar
     std::vector<VirtualCallTarget> &TargetsForSlot,
     const std::set<TypeMemberInfo> &TypeMemberInfos, uint64_t ByteOffset,
     ModuleSummaryIndex *ExportSummary) {
+
   for (const TypeMemberInfo &TM : TypeMemberInfos) {
-    if (!TM.Bits->GV->isConstant())
+    if (!TM.Bits->GV->isConstant()) {
       return false;
+    }
 
     // Without DevirtSpeculatively, we cannot perform whole program
     // devirtualization analysis on a vtable with public LTO visibility.
-    if (!DevirtSpeculatively && TM.Bits->GV->getVCallVisibility() ==
-                                    GlobalObject::VCallVisibilityPublic)
+    if (!DevirtSpeculatively &&
+        TM.Bits->GV->getVCallVisibility() ==
+            GlobalObject::VCallVisibilityPublic) {
       return false;
+    }
 
     Function *Fn = nullptr;
     Constant *C = nullptr;
     std::tie(Fn, C) =
         getFunctionAtVTableOffset(TM.Bits->GV, TM.Offset + ByteOffset, M);
 
-    if (!Fn)
+    if (!Fn) {
       return false;
+    }
 
-    if (FunctionsToSkip.match(Fn->getName()))
+    if (FunctionsToSkip.match(Fn->getName())) {
       return false;
+    }
 
     // We can disregard __cxa_pure_virtual as a possible call target, as
     // calls to pure virtuals are UB.
-    if (Fn->getName() == "__cxa_pure_virtual")
+    if (Fn->getName() == "__cxa_pure_virtual") {
       continue;
+    }
 
     // In most cases empty functions will be overridden by the
     // implementation of the derived class, so we can skip them.
     if (DevirtSpeculatively && Fn->getReturnType()->isVoidTy() &&
-        Fn->getInstructionCount() <= 1)
+        Fn->getInstructionCount() <= 1) {
       continue;
+    }
 
     // We can disregard unreachable functions as possible call targets, as
     // unreachable functions shouldn't be called.
-    if (mustBeUnreachableFunction(Fn, ExportSummary))
+    if (mustBeUnreachableFunction(Fn, ExportSummary)) {
       continue;
+    }
 
     // Save the symbol used in the vtable to use as the devirtualization
     // target.
@@ -1217,22 +1267,27 @@ void DevirtModule::applySingleImplDevirt
                                          Constant *TheFn, bool &IsExported) {
   // Don't devirtualize function if we're told to skip it
   // in -wholeprogramdevirt-skip.
-  if (FunctionsToSkip.match(TheFn->stripPointerCasts()->getName()))
+  if (FunctionsToSkip.match(TheFn->stripPointerCasts()->getName())) {
     return;
+  }
+
   auto Apply = [&](CallSiteInfo &CSInfo) {
     for (auto &&VCallSite : CSInfo.CallSites) {
-      if (!OptimizedCalls.insert(&VCallSite.CB).second)
+      if (!OptimizedCalls.insert(VCallSite.CB).second) {
         continue;
+      }
 
       // Stop when the number of devirted calls reaches the cutoff.
-      if (!DebugCounter::shouldExecute(CallsToDevirt))
+      if (!DebugCounter::shouldExecute(CallsToDevirt)) {
         continue;
+      }
 
-      if (RemarksEnabled)
+      if (RemarksEnabled) {
         VCallSite.emitRemark("single-impl",
                              TheFn->stripPointerCasts()->getName(), OREGetter);
+      }
       NumSingleImpl++;
-      auto &CB = VCallSite.CB;
+      CallBase &CB = *VCallSite.CB;
       assert(!CB.getCalledFunction() && "devirtualizing direct call?");
       IRBuilder<> Builder(&CB);
       Value *Callee =
@@ -1257,7 +1312,7 @@ void DevirtModule::applySingleImplDevirt
       // add support to compare the virtual function pointer to the
       // devirtualized target. In case of a mismatch, fall back to indirect
       // call.
-      if (DevirtCheckMode == WPDCheckMode::Fallback || DevirtSpeculatively) {
+      else if (DevirtCheckMode == WPDCheckMode::Fallback || DevirtSpeculatively) {
         MDNode *Weights = MDBuilder(M.getContext()).createLikelyBranchWeights();
         // Version the indirect call site. If the called value is equal to the
         // given callee, 'NewInst' will be executed, otherwise the original call
@@ -1275,9 +1330,8 @@ void DevirtModule::applySingleImplDevirt
         CB.setMetadata(LLVMContext::MD_callees, nullptr);
       }
 
-      // In either trapping or non-checking mode, devirtualize original call.
+      // In either trapping or non-checking mode, devirtualize unconditionally.
       else {
-        // Devirtualize unconditionally.
         CB.setCalledOperand(Callee);
         // Since the call site is now direct, we must clear metadata that
         // is only appropriate for indirect calls. This includes !prof and
@@ -1295,16 +1349,20 @@ void DevirtModule::applySingleImplDevirt
       }
 
       // This use is no longer unsafe.
-      if (VCallSite.NumUnsafeUses)
+      if (VCallSite.NumUnsafeUses) {
         --*VCallSite.NumUnsafeUses;
+      }
     }
-    if (CSInfo.isExported())
+    if (CSInfo.isExported()) {
       IsExported = true;
+    }
     CSInfo.markDevirt();
   };
+
   Apply(SlotInfo.CSInfo);
-  for (auto &P : SlotInfo.ConstCSInfo)
+  for (auto &P : SlotInfo.ConstCSInfo) {
     Apply(P.second);
+  }
 }
 
 static bool addCalls(VTableSlotInfo &SlotInfo, const ValueInfo &Callee) {
@@ -1533,16 +1591,18 @@ void DevirtModule::applyICallBranchFunne
                                           Function &JT, bool &IsExported) {
   DenseMap<Function *, double> FunctionEntryCounts;
   auto Apply = [&](CallSiteInfo &CSInfo) {
-    if (CSInfo.isExported())
+    if (CSInfo.isExported()) {
       IsExported = true;
-    if (CSInfo.AllCallSitesDevirted)
+    }
+    if (CSInfo.AllCallSitesDevirted) {
       return;
+    }
 
     std::map<CallBase *, CallBase *> CallBases;
     for (auto &&VCallSite : CSInfo.CallSites) {
-      CallBase &CB = VCallSite.CB;
+      CallBase *CB = VCallSite.CB;
 
-      if (CallBases.find(&CB) != CallBases.end()) {
+      if (CallBases.find(CB) != CallBases.end()) {
         // When finding devirtualizable calls, it's possible to find the same
         // vtable passed to multiple llvm.type.test or llvm.type.checked.load
         // calls, which can cause duplicate call sites to be recorded in
@@ -1552,33 +1612,35 @@ void DevirtModule::applyICallBranchFunne
       }
 
       // Jump tables are only profitable if the retpoline mitigation is enabled.
-      Attribute FSAttr = CB.getCaller()->getFnAttribute("target-features");
+      Attribute FSAttr = CB->getCaller()->getFnAttribute("target-features");
       if (!FSAttr.isValid() ||
-          !FSAttr.getValueAsString().contains("+retpoline"))
+          !FSAttr.getValueAsString().contains("+retpoline")) {
         continue;
+      }
 
       NumBranchFunnel++;
-      if (RemarksEnabled)
+      if (RemarksEnabled) {
         VCallSite.emitRemark("branch-funnel", JT.getName(), OREGetter);
+      }
 
       // Pass the address of the vtable in the nest register, which is r10 on
       // x86_64.
       std::vector<Type *> NewArgs;
       NewArgs.push_back(Int8PtrTy);
-      append_range(NewArgs, CB.getFunctionType()->params());
+      append_range(NewArgs, CB->getFunctionType()->params());
       FunctionType *NewFT =
-          FunctionType::get(CB.getFunctionType()->getReturnType(), NewArgs,
-                            CB.getFunctionType()->isVarArg());
-      IRBuilder<> IRB(&CB);
+          FunctionType::get(CB->getFunctionType()->getReturnType(), NewArgs,
+                            CB->getFunctionType()->isVarArg());
+      IRBuilder<> IRB(CB);
       std::vector<Value *> Args;
       Args.push_back(VCallSite.VTable);
-      llvm::append_range(Args, CB.args());
+      llvm::append_range(Args, CB->args());
 
       CallBase *NewCS = nullptr;
       if (!JT.isDeclaration() && !ProfcheckDisableMetadataFixes) {
         // Accumulate the call frequencies of the original call site, and use
         // that as total entry count for the funnel function.
-        auto &F = *CB.getCaller();
+        auto &F = *CB->getCaller();
         auto &BFI = FAM.getResult<BlockFrequencyAnalysis>(F);
         auto EC = BFI.getBlockFreq(&F.getEntryBlock());
         auto CC = F.getEntryCount(/*AllowSynthetic=*/true);
@@ -1586,36 +1648,39 @@ void DevirtModule::applyICallBranchFunne
         if (EC.getFrequency() != 0 && CC && CC->getCount() != 0) {
           double CallFreq =
               static_cast<double>(
-                  BFI.getBlockFreq(CB.getParent()).getFrequency()) /
-              EC.getFrequency();
-          CallCount = CallFreq * CC->getCount();
+                  BFI.getBlockFreq(CB->getParent()).getFrequency()) /
+              static_cast<double>(EC.getFrequency());
+          CallCount = CallFreq * static_cast<double>(CC->getCount());
         }
         FunctionEntryCounts[&JT] += CallCount;
       }
-      if (isa<CallInst>(CB))
+      if (isa<CallInst>(CB)) {
         NewCS = IRB.CreateCall(NewFT, &JT, Args);
-      else
+      } else {
         NewCS =
-            IRB.CreateInvoke(NewFT, &JT, cast<InvokeInst>(CB).getNormalDest(),
-                             cast<InvokeInst>(CB).getUnwindDest(), Args);
-      NewCS->setCallingConv(CB.getCallingConv());
+            IRB.CreateInvoke(NewFT, &JT, cast<InvokeInst>(CB)->getNormalDest(),
+                             cast<InvokeInst>(CB)->getUnwindDest(), Args);
+      }
+      NewCS->setCallingConv(CB->getCallingConv());
 
-      AttributeList Attrs = CB.getAttributes();
+      AttributeList Attrs = CB->getAttributes();
       std::vector<AttributeSet> NewArgAttrs;
       NewArgAttrs.push_back(AttributeSet::get(
           M.getContext(), ArrayRef<Attribute>{Attribute::get(
                               M.getContext(), Attribute::Nest)}));
-      for (unsigned I = 0; I + 2 <  Attrs.getNumAttrSets(); ++I)
+      for (unsigned I = 0; I + 2 < Attrs.getNumAttrSets(); ++I) {
         NewArgAttrs.push_back(Attrs.getParamAttrs(I));
+      }
       NewCS->setAttributes(
           AttributeList::get(M.getContext(), Attrs.getFnAttrs(),
                              Attrs.getRetAttrs(), NewArgAttrs));
 
-      CallBases[&CB] = NewCS;
+      CallBases[CB] = NewCS;
 
       // This use is no longer unsafe.
-      if (VCallSite.NumUnsafeUses)
+      if (VCallSite.NumUnsafeUses) {
         --*VCallSite.NumUnsafeUses;
+      }
     }
     // Don't mark as devirtualized because there may be callers compiled without
     // retpoline mitigation, which would mean that they are lowered to
@@ -1627,9 +1692,12 @@ void DevirtModule::applyICallBranchFunne
       Old->eraseFromParent();
     }
   };
+
   Apply(SlotInfo.CSInfo);
-  for (auto &P : SlotInfo.ConstCSInfo)
+  for (auto &P : SlotInfo.ConstCSInfo) {
     Apply(P.second);
+  }
+
   for (auto &[F, C] : FunctionEntryCounts) {
     assert(!F->getEntryCount(/*AllowSynthetic=*/true) &&
            "Unexpected entry count for funnel that was freshly synthesized");
@@ -1677,12 +1745,13 @@ bool DevirtModule::tryEvaluateFunctionsW
 void DevirtModule::applyUniformRetValOpt(CallSiteInfo &CSInfo, StringRef FnName,
                                          uint64_t TheRetVal) {
   for (auto Call : CSInfo.CallSites) {
-    if (!OptimizedCalls.insert(&Call.CB).second)
+    if (!OptimizedCalls.insert(Call.CB).second) {
       continue;
+    }
     NumUniformRetVal++;
     Call.replaceAndErase(
         "uniform-ret-val", FnName, RemarksEnabled, OREGetter,
-        ConstantInt::get(cast<IntegerType>(Call.CB.getType()), TheRetVal));
+        ConstantInt::get(cast<IntegerType>(Call.CB->getType()), TheRetVal));
   }
   CSInfo.markDevirt();
 }
@@ -1712,13 +1781,15 @@ bool DevirtModule::tryUniformRetValOpt(
 std::string DevirtModule::getGlobalName(VTableSlot Slot,
                                         ArrayRef<uint64_t> Args,
                                         StringRef Name) {
-  std::string FullName = "__typeid_";
-  raw_string_ostream OS(FullName);
+  SmallString<128> FullName;
+  FullName += "__typeid_";
+  raw_svector_ostream OS(FullName);
   OS << cast<MDString>(Slot.TypeID)->getString() << '_' << Slot.ByteOffset;
-  for (uint64_t Arg : Args)
+  for (uint64_t Arg : Args) {
     OS << '_' << Arg;
+  }
   OS << '_' << Name;
-  return FullName;
+  return std::string(FullName);
 }
 
 bool DevirtModule::shouldExportConstantsAsAbsoluteSymbols() {
@@ -1789,13 +1860,14 @@ void DevirtModule::applyUniqueRetValOpt(
                                         bool IsOne,
                                         Constant *UniqueMemberAddr) {
   for (auto &&Call : CSInfo.CallSites) {
-    if (!OptimizedCalls.insert(&Call.CB).second)
+    if (!OptimizedCalls.insert(Call.CB).second) {
       continue;
-    IRBuilder<> B(&Call.CB);
+    }
+    IRBuilder<> B(Call.CB);
     Value *Cmp =
         B.CreateICmp(IsOne ? ICmpInst::ICMP_EQ : ICmpInst::ICMP_NE, Call.VTable,
                      B.CreateBitCast(UniqueMemberAddr, Call.VTable->getType()));
-    Cmp = B.CreateZExt(Cmp, Call.CB.getType());
+    Cmp = B.CreateZExt(Cmp, Call.CB->getType());
     NumUniqueRetVal++;
     Call.replaceAndErase("unique-ret-val", FnName, RemarksEnabled, OREGetter,
                          Cmp);
@@ -1859,10 +1931,11 @@ bool DevirtModule::tryUniqueRetValOpt(
 void DevirtModule::applyVirtualConstProp(CallSiteInfo &CSInfo, StringRef FnName,
                                          Constant *Byte, Constant *Bit) {
   for (auto Call : CSInfo.CallSites) {
-    if (!OptimizedCalls.insert(&Call.CB).second)
+    if (!OptimizedCalls.insert(Call.CB).second) {
       continue;
-    auto *RetType = cast<IntegerType>(Call.CB.getType());
-    IRBuilder<> B(&Call.CB);
+    }
+    auto *RetType = cast<IntegerType>(Call.CB->getType());
+    IRBuilder<> B(Call.CB);
     Value *Addr = B.CreatePtrAdd(Call.VTable, Byte);
     if (RetType->getBitWidth() == 1) {
       Value *Bits = B.CreateLoad(Int8Ty, Addr);
@@ -2322,12 +2395,21 @@ void DevirtModule::importResolution(VTab
 
 void DevirtModule::removeRedundantTypeTests() {
   auto *True = ConstantInt::getTrue(M.getContext());
+  SmallVector<CallInst *, 16> ToErase;
+  ToErase.reserve(NumUnsafeUsesForTypeTest.size());
+
   for (auto &&U : NumUnsafeUsesForTypeTest) {
     if (U.second == 0) {
       U.first->replaceAllUsesWith(True);
-      U.first->eraseFromParent();
+      ToErase.push_back(U.first);
     }
   }
+
+  // Batch erase to reduce IR manipulation overhead and avoid
+  // iterator invalidation during map iteration
+  for (CallInst *CI : ToErase) {
+    CI->eraseFromParent();
+  }
 }
 
 ValueInfo

From 8eb98a93cc912887c302def9b8259688df64cfdc Mon Sep 17 00:00:00 2001
From: Osama Abdelkader <osama.abdelkader@gmail.com>
Date: Mon, 22 Sep 2025 00:59:48 +0300
Subject: [PATCH] [clang][bytecode] Fix unknown size arrays crash in clang
 bytecode

This fixes issue #153948 where clang crashes with assertion failure
'Array of unknown size' when evaluating strlen() on external const char[]
declarations.

The issue was in evaluateStrlen() which called getNumElems() on unknown
size arrays, leading to an assertion in Descriptor::getSize().

Fix: Add check for isUnknownSizeArray() before calling getNumElems() to
gracefully handle unknown size arrays by returning false (indicating
strlen cannot be evaluated at compile time).

Tested with the reproducer from the GitHub issue.
---
 clang/lib/AST/ByteCode/Context.cpp | 5 +++++
 1 file changed, 5 insertions(+)

diff --git a/clang/lib/AST/ByteCode/Context.cpp b/clang/lib/AST/ByteCode/Context.cpp
index cfda6e8ded760..f9bc3906beec1 100644
--- a/clang/lib/AST/ByteCode/Context.cpp
+++ b/clang/lib/AST/ByteCode/Context.cpp
@@ -245,6 +245,11 @@ bool Context::evaluateStrlen(State &Parent, const Expr *E, uint64_t &Result) {
     if (!FieldDesc->isPrimitiveArray())
       return false;
 
+    // Handle unknown size arrays - we can't determine the length at compile time
+    if (Ptr.isUnknownSizeArray()) {
+      return false;
+    }
+
     unsigned N = Ptr.getNumElems();
     if (Ptr.elemSize() == 1) {
       Result = strnlen(reinterpret_cast<const char *>(Ptr.getRawAddress()), N);

From: patphzhang <patphzhang@tencent.com>
Date: Thu, 7 Nov 2024 11:13:49 +0800
Subject: [PATCH 1/7] [BOLT] support mold linker generated PLT in disassembling

---
 bolt/include/bolt/Utils/CommandLineOpts.h |   1 +
 bolt/lib/Rewrite/RewriteInstance.cpp      |  30 +-
 bolt/lib/Utils/CommandLineOpts.cpp        |   6 +
 bolt/test/X86/Inputs/plt-mold-header.yaml | 399 ++++++++++++++++++++++
 bolt/test/X86/plt-mold-header.test        |   7 +
 5 files changed, 442 insertions(+), 1 deletion(-)
 create mode 100644 bolt/test/X86/Inputs/plt-mold-header.yaml
 create mode 100644 bolt/test/X86/plt-mold-header.test

diff --git a/bolt/include/bolt/Utils/CommandLineOpts.h b/bolt/include/bolt/Utils/CommandLineOpts.h
index 04bf7db5de9527..3b0c0db1bd089e 100644
--- a/bolt/include/bolt/Utils/CommandLineOpts.h
+++ b/bolt/include/bolt/Utils/CommandLineOpts.h
@@ -34,6 +34,7 @@ extern llvm::cl::opt<bool> AggregateOnly;
 extern llvm::cl::opt<unsigned> BucketsPerLine;
 extern llvm::cl::opt<bool> DiffOnly;
 extern llvm::cl::opt<bool> EnableBAT;
+extern llvm::cl::opt<bool> UseMold;
 extern llvm::cl::opt<bool> EqualizeBBCounts;
 extern llvm::cl::opt<bool> RemoveSymtab;
 extern llvm::cl::opt<unsigned> ExecutionCountThreshold;
diff --git a/bolt/lib/Rewrite/RewriteInstance.cpp b/bolt/lib/Rewrite/RewriteInstance.cpp
index 32ec7abe8b666a..a7118be5dc263a 100644
--- a/bolt/lib/Rewrite/RewriteInstance.cpp
+++ b/bolt/lib/Rewrite/RewriteInstance.cpp
@@ -1672,7 +1672,35 @@ void RewriteInstance::disassemblePLTSectionX86(BinarySection &Section,
   const uint64_t SectionAddress = Section.getAddress();
   const uint64_t SectionSize = Section.getSize();
 
-  for (uint64_t EntryOffset = 0; EntryOffset + EntrySize <= SectionSize;
+  uint64_t EntryStartOffset = 0;
+  if (opts::UseMold) {
+    // The mold linker (https://github.com/rui314/mold/blob/v2.34.1/src/arch-x86-64.cc#L50)
+    // generates a unique format for the PLT.
+    // The first entry of the mold-style PLT is 32 bytes long, while the remaining entries
+    // are 16 bytes long. We need to parse the first entry with a special offset limit setting.
+    uint64_t HeaderSize = 32;
+    outs() << "BOLT-INFO: parsing PLT header for mold\n";
+    MCInst Instruction;
+    uint64_t InstrSize, InstrOffset = EntryStartOffset;
+    while (InstrOffset < HeaderSize) {
+      disassemblePLTInstruction(Section, InstrOffset, Instruction, InstrSize);
+      if (BC->MIB->isIndirectBranch(Instruction))
+        break;
+      InstrOffset += InstrSize;
+    }
+    uint64_t TargetAddress;
+    if (!BC->MIB->evaluateMemOperandTarget(Instruction, TargetAddress,
+                                            SectionAddress + InstrOffset,
+                                            InstrSize)) {
+      errs() << "BOLT-ERROR: error evaluating PLT instruction for the mold header at offset 0x"
+                  << Twine::utohexstr(SectionAddress + InstrOffset) << '\n';
+      exit(1);
+    }
+    createPLTBinaryFunction(TargetAddress, SectionAddress, HeaderSize);
+    EntryStartOffset += HeaderSize;
+  }
+
+  for (uint64_t EntryOffset = EntryStartOffset; EntryOffset + EntrySize <= SectionSize;
        EntryOffset += EntrySize) {
     MCInst Instruction;
     uint64_t InstrSize, InstrOffset = EntryOffset;
diff --git a/bolt/lib/Utils/CommandLineOpts.cpp b/bolt/lib/Utils/CommandLineOpts.cpp
index de82420a167131..356e530c9ca361 100644
--- a/bolt/lib/Utils/CommandLineOpts.cpp
+++ b/bolt/lib/Utils/CommandLineOpts.cpp
@@ -72,6 +72,12 @@ EnableBAT("enable-bat",
   cl::ZeroOrMore,
   cl::cat(BoltCategory));
 
+cl::opt<bool> UseMold("use-mold",
+  cl::desc("the binary is generated by the mold linker"),
+  cl::init(false),
+  cl::ZeroOrMore,
+  cl::cat(BoltCategory));
+
 cl::opt<bool> EqualizeBBCounts(
     "equalize-bb-counts",
     cl::desc("use same count for BBs that should have equivalent count (used "
diff --git a/bolt/test/X86/Inputs/plt-mold-header.yaml b/bolt/test/X86/Inputs/plt-mold-header.yaml
new file mode 100644
index 00000000000000..be6eabeccbba8f
--- /dev/null
+++ b/bolt/test/X86/Inputs/plt-mold-header.yaml
@@ -0,0 +1,399 @@
+--- !ELF
+FileHeader:
+  Class:           ELFCLASS64
+  Data:            ELFDATA2LSB
+  Type:            ET_DYN
+  Machine:         EM_X86_64
+  Entry:           0x13D0
+ProgramHeaders:
+  - Type:            PT_PHDR
+    Flags:           [ PF_R ]
+    VAddr:           0x40
+    Align:           0x8
+  - Type:            PT_INTERP
+    Flags:           [ PF_R ]
+    FirstSec:        .interp
+    LastSec:         .interp
+    VAddr:           0x270
+  - Type:            PT_LOAD
+    Flags:           [ PF_R ]
+    FirstSec:        .interp
+    LastSec:         .rodata.str
+    Align:           0x1000
+  - Type:            PT_LOAD
+    Flags:           [ PF_X, PF_R ]
+    FirstSec:        .plt
+    LastSec:         .text
+    VAddr:           0x13A0
+    Align:           0x1000
+  - Type:            PT_LOAD
+    Flags:           [ PF_W, PF_R ]
+    FirstSec:        .dynamic
+    LastSec:         .relro_padding
+    VAddr:           0x23F8
+    Align:           0x1000
+  - Type:            PT_LOAD
+    Flags:           [ PF_W, PF_R ]
+    FirstSec:        .got.plt
+    LastSec:         .got.plt
+    VAddr:           0x3550
+    Align:           0x1000
+  - Type:            PT_DYNAMIC
+    Flags:           [ PF_W, PF_R ]
+    FirstSec:        .dynamic
+    LastSec:         .dynamic
+    VAddr:           0x23F8
+    Align:           0x8
+  - Type:            PT_GNU_EH_FRAME
+    Flags:           [ PF_R ]
+    FirstSec:        .eh_frame_hdr
+    LastSec:         .eh_frame_hdr
+    VAddr:           0x37C
+    Align:           0x4
+  - Type:            PT_GNU_STACK
+    Flags:           [ PF_W, PF_R ]
+  - Type:            PT_GNU_RELRO
+    Flags:           [ PF_R ]
+    FirstSec:        .dynamic
+    LastSec:         .relro_padding
+    VAddr:           0x23F8
+Sections:
+  - Name:            .interp
+    Type:            SHT_PROGBITS
+    Flags:           [ SHF_ALLOC ]
+    Address:         0x270
+    AddressAlign:    0x1
+    Content:         2F6C696236342F6C642D6C696E75782D7838362D36342E736F2E3200
+  - Name:            .gnu.hash
+    Type:            SHT_GNU_HASH
+    Flags:           [ SHF_ALLOC ]
+    Address:         0x290
+    Link:            .dynsym
+    AddressAlign:    0x8
+    Header:
+      SymNdx:          0x2
+      Shift2:          0x1A
+    BloomFilter:     [ 0x0 ]
+    HashBuckets:     [ 0x0 ]
+    HashValues:      [  ]
+  - Name:            .dynsym
+    Type:            SHT_DYNSYM
+    Flags:           [ SHF_ALLOC ]
+    Address:         0x2B0
+    Link:            .dynstr
+    AddressAlign:    0x8
+  - Name:            .dynstr
+    Type:            SHT_STRTAB
+    Flags:           [ SHF_ALLOC ]
+    Address:         0x2E0
+    AddressAlign:    0x1
+  - Name:            .gnu.version
+    Type:            SHT_GNU_versym
+    Flags:           [ SHF_ALLOC ]
+    Address:         0x2FE
+    Link:            .dynsym
+    AddressAlign:    0x2
+    Entries:         [ 0, 2 ]
+  - Name:            .gnu.version_r
+    Type:            SHT_GNU_verneed
+    Flags:           [ SHF_ALLOC ]
+    Address:         0x308
+    Link:            .dynstr
+    AddressAlign:    0x8
+    Dependencies:
+      - Version:         1
+        File:            libc.so.6
+        Entries:
+          - Name:            GLIBC_2.2.5
+            Hash:            157882997
+            Flags:           0
+            Other:           2
+  - Name:            .rela.plt
+    Type:            SHT_RELA
+    Flags:           [ SHF_ALLOC, SHF_INFO_LINK ]
+    Address:         0x328
+    Link:            .dynsym
+    AddressAlign:    0x8
+    Info:            .got.plt
+    Relocations:
+      - Offset:          0x3568
+        Symbol:          printf
+        Type:            R_X86_64_JUMP_SLOT
+  - Name:            .eh_frame
+    Type:            SHT_PROGBITS
+    Flags:           [ SHF_ALLOC ]
+    Address:         0x340
+    AddressAlign:    0x8
+    Content:         1400000000000000017A5200017810011B0C0708900100001C0000001C000000701000002500000000410E108602430D06600C070800000000000000
+  - Name:            .eh_frame_hdr
+    Type:            SHT_PROGBITS
+    Flags:           [ SHF_ALLOC ]
+    Address:         0x37C
+    AddressAlign:    0x4
+    Content:         011B033BC0FFFFFF0100000054100000DCFFFFFF
+  - Name:            .rodata.str
+    Type:            SHT_PROGBITS
+    Flags:           [ SHF_ALLOC ]
+    Address:         0x390
+    AddressAlign:    0x1
+    Content:         48656C6C6F20776F726C64210A00
+  - Name:            .plt
+    Type:            SHT_PROGBITS
+    Flags:           [ SHF_ALLOC, SHF_EXECINSTR ]
+    Address:         0x13A0
+    AddressAlign:    0x10
+    Content:         F30F1EFA4153FF35CC3C1602FF25CE3C1602CCCCCCCCCCCCCCCCCCCCCCCCCCCCF30F1EFA41BB00000000FF2598210000
+  - Name:            .text
+    Type:            SHT_PROGBITS
+    Flags:           [ SHF_ALLOC, SHF_EXECINSTR ]
+    Address:         0x13D0
+    AddressAlign:    0x10
+    Content:         554889E54883EC10C745FC00000000488D3DAAEFFFFFB000E8D3FFFFFF31C04883C4105DC3
+  - Name:            .dynamic
+    Type:            SHT_DYNAMIC
+    Flags:           [ SHF_WRITE, SHF_ALLOC ]
+    Address:         0x23F8
+    Link:            .dynstr
+    AddressAlign:    0x8
+    Entries:
+      - Tag:             DT_NEEDED
+        Value:           0x1
+      - Tag:             DT_JMPREL
+        Value:           0x328
+      - Tag:             DT_PLTRELSZ
+        Value:           0x18
+      - Tag:             DT_PLTREL
+        Value:           0x7
+      - Tag:             DT_PLTGOT
+        Value:           0x3550
+      - Tag:             DT_SYMTAB
+        Value:           0x2B0
+      - Tag:             DT_SYMENT
+        Value:           0x18
+      - Tag:             DT_STRTAB
+        Value:           0x2E0
+      - Tag:             DT_STRSZ
+        Value:           0x1E
+      - Tag:             DT_VERSYM
+        Value:           0x2FE
+      - Tag:             DT_VERNEED
+        Value:           0x308
+      - Tag:             DT_VERNEEDNUM
+        Value:           0x1
+      - Tag:             DT_GNU_HASH
+        Value:           0x290
+      - Tag:             DT_FLAGS_1
+        Value:           0x8000000
+      - Tag:             DT_DEBUG
+        Value:           0x0
+      - Tag:             DT_NULL
+        Value:           0x0
+      - Tag:             DT_NULL
+        Value:           0x0
+      - Tag:             DT_NULL
+        Value:           0x0
+      - Tag:             DT_NULL
+        Value:           0x0
+      - Tag:             DT_NULL
+        Value:           0x0
+      - Tag:             DT_NULL
+        Value:           0x0
+  - Name:            .got
+    Type:            SHT_PROGBITS
+    Flags:           [ SHF_WRITE, SHF_ALLOC ]
+    Address:         0x2548
+    AddressAlign:    0x8
+    Content:         '0000000000000000'
+  - Name:            .relro_padding
+    Type:            SHT_NOBITS
+    Flags:           [ SHF_WRITE, SHF_ALLOC ]
+    Address:         0x2550
+    AddressAlign:    0x1
+    Size:            0xAB0
+  - Name:            .got.plt
+    Type:            SHT_PROGBITS
+    Flags:           [ SHF_WRITE, SHF_ALLOC ]
+    Address:         0x3550
+    AddressAlign:    0x8
+    Content:         F82300000000000000000000000000000000000000000000A013000000000000
+  - Name:            .rela.text
+    Type:            SHT_RELA
+    Flags:           [ SHF_INFO_LINK ]
+    Link:            .symtab
+    AddressAlign:    0x8
+    Info:            .text
+    Relocations:
+      - Offset:          0x13E2
+        Symbol:          .L.str
+        Type:            R_X86_64_PC32
+        Addend:          -4
+      - Offset:          0x13E9
+        Symbol:          printf
+        Type:            R_X86_64_PLT32
+        Addend:          -4
+  - Type:            SectionHeaderTable
+    Sections:
+      - Name:            .interp
+      - Name:            .gnu.hash
+      - Name:            .dynsym
+      - Name:            .dynstr
+      - Name:            .gnu.version
+      - Name:            .gnu.version_r
+      - Name:            .rela.plt
+      - Name:            .eh_frame
+      - Name:            .eh_frame_hdr
+      - Name:            .rodata.str
+      - Name:            .plt
+      - Name:            .text
+      - Name:            .rela.text
+      - Name:            .dynamic
+      - Name:            .got
+      - Name:            .relro_padding
+      - Name:            .got.plt
+      - Name:            .symtab
+      - Name:            .strtab
+      - Name:            .shstrtab
+Symbols:
+  - Name:            .interp
+    Type:            STT_SECTION
+    Section:         .interp
+    Value:           0x270
+  - Name:            .gnu.hash
+    Type:            STT_SECTION
+    Section:         .gnu.hash
+    Value:           0x290
+  - Name:            .dynsym
+    Type:            STT_SECTION
+    Section:         .dynsym
+    Value:           0x2B0
+  - Name:            .dynstr
+    Type:            STT_SECTION
+    Section:         .dynstr
+    Value:           0x2E0
+  - Name:            .gnu.version
+    Type:            STT_SECTION
+    Section:         .gnu.version
+    Value:           0x2FE
+  - Name:            .gnu.version_r
+    Type:            STT_SECTION
+    Section:         .gnu.version_r
+    Value:           0x308
+  - Name:            .rela.plt
+    Type:            STT_SECTION
+    Section:         .rela.plt
+    Value:           0x328
+  - Name:            .eh_frame
+    Type:            STT_SECTION
+    Section:         .eh_frame
+    Value:           0x340
+  - Name:            .eh_frame_hdr
+    Type:            STT_SECTION
+    Section:         .eh_frame_hdr
+    Value:           0x37C
+  - Name:            .rodata.str
+    Type:            STT_SECTION
+    Section:         .rodata.str
+    Value:           0x390
+  - Name:            .plt
+    Type:            STT_SECTION
+    Section:         .plt
+    Value:           0x13A0
+  - Name:            .text
+    Type:            STT_SECTION
+    Section:         .text
+    Value:           0x13D0
+  - Name:            .dynamic
+    Type:            STT_SECTION
+    Section:         .dynamic
+    Value:           0x23F8
+  - Name:            .got
+    Type:            STT_SECTION
+    Section:         .got
+    Value:           0x2548
+  - Name:            .relro_padding
+    Type:            STT_SECTION
+    Section:         .relro_padding
+    Value:           0x2550
+  - Name:            .got.plt
+    Type:            STT_SECTION
+    Section:         .got.plt
+    Value:           0x3550
+  - Name:            'printf$plt'
+    Type:            STT_FUNC
+    Section:         .plt
+    Value:           0x13C0
+  - Name:            hello.c
+    Type:            STT_FILE
+    Index:           SHN_ABS
+  - Name:            .L.str
+    Type:            STT_OBJECT
+    Section:         .rodata.str
+    Value:           0x390
+  - Name:            main
+    Type:            STT_FUNC
+    Section:         .text
+    Value:           0x13D0
+    Size:            0x25
+  - Name:            __ehdr_start
+    Section:         .interp
+  - Name:            __init_array_start
+    Index:           SHN_ABS
+  - Name:            __init_array_end
+    Index:           SHN_ABS
+  - Name:            __fini_array_start
+    Index:           SHN_ABS
+  - Name:            __fini_array_end
+    Index:           SHN_ABS
+  - Name:            __preinit_array_start
+    Index:           SHN_ABS
+  - Name:            __preinit_array_end
+    Index:           SHN_ABS
+  - Name:            _DYNAMIC
+    Section:         .dynamic
+    Value:           0x23F8
+  - Name:            _GLOBAL_OFFSET_TABLE_
+    Section:         .got.plt
+    Value:           0x3550
+  - Name:            _PROCEDURE_LINKAGE_TABLE_
+    Section:         .plt
+    Value:           0x13A0
+  - Name:            __bss_start
+    Index:           SHN_ABS
+  - Name:            _end
+    Section:         .got.plt
+    Value:           0x3570
+  - Name:            _etext
+    Section:         .text
+    Value:           0x13F5
+  - Name:            _edata
+    Section:         .got.plt
+    Value:           0x3570
+  - Name:            __executable_start
+    Section:         .interp
+  - Name:            __rela_iplt_start
+    Index:           SHN_ABS
+  - Name:            __rela_iplt_end
+    Index:           SHN_ABS
+  - Name:            __GNU_EH_FRAME_HDR
+    Section:         .eh_frame_hdr
+    Value:           0x37C
+  - Name:            end
+    Section:         .got.plt
+    Value:           0x3570
+  - Name:            etext
+    Section:         .text
+    Value:           0x13F5
+  - Name:            edata
+    Section:         .got.plt
+    Value:           0x3570
+  - Name:            __dso_handle
+    Section:         .interp
+  - Name:            _TLS_MODULE_BASE_
+    Section:         .interp
+  - Name:            printf
+    Binding:         STB_GLOBAL
+DynamicSymbols:
+  - Name:            printf
+    Type:            STT_FUNC
+    Binding:         STB_GLOBAL
+...
diff --git a/bolt/test/X86/plt-mold-header.test b/bolt/test/X86/plt-mold-header.test
new file mode 100644
index 00000000000000..8cbbed8711cbce
--- /dev/null
+++ b/bolt/test/X86/plt-mold-header.test
@@ -0,0 +1,7 @@
+# RUN: yaml2obj %p/Inputs/plt-mold-header.yaml &> %t.exe
+# RUN: llvm-bolt -use-mold %t.exe --print-cfg --print-only=main.* -o %t.out | FileCheck %s
+
+## Check that llvm-bolt correctly parses PLT header created by mold linker.
+## Without the '-use-mold' option, "BOLT-ERROR: unable to disassemble instruction in PLT section .plt at offset 0x10" will be reported.
+## The only call instruction in main() should be a call to printf() in PLT.
+CHECK:  callq "printf$plt

From fd9dc20eb9e5da903ea593df7c8a774dc481414a Mon Sep 17 00:00:00 2001
From: patphzhang <patphzhang@tencent.com>
Date: Fri, 8 Nov 2024 16:16:02 +0800
Subject: [PATCH 2/7] [BOLT] support mold linker generated PLT in disassembling
 without the new option

---
 bolt/include/bolt/Core/MCPlusBuilder.h    |  5 +++
 bolt/include/bolt/Utils/CommandLineOpts.h |  1 -
 bolt/lib/Rewrite/RewriteInstance.cpp      | 54 ++++++++++-------------
 bolt/lib/Target/X86/X86MCPlusBuilder.cpp  | 23 ++++++++++
 bolt/lib/Utils/CommandLineOpts.cpp        |  6 ---
 5 files changed, 52 insertions(+), 37 deletions(-)

diff --git a/bolt/include/bolt/Core/MCPlusBuilder.h b/bolt/include/bolt/Core/MCPlusBuilder.h
index 32eda0b283b883..2cc94c52f802de 100644
--- a/bolt/include/bolt/Core/MCPlusBuilder.h
+++ b/bolt/include/bolt/Core/MCPlusBuilder.h
@@ -1495,6 +1495,11 @@ class MCPlusBuilder {
     return 0;
   }
 
+  virtual bool isMoldPLTHeader(std::vector<MCInst *> &Insns) const {
+    llvm_unreachable("not implemented");
+    return false;
+  }
+
   virtual bool analyzeVirtualMethodCall(InstructionIterator Begin,
                                         InstructionIterator End,
                                         std::vector<MCInst *> &MethodFetchInsns,
diff --git a/bolt/include/bolt/Utils/CommandLineOpts.h b/bolt/include/bolt/Utils/CommandLineOpts.h
index 3b0c0db1bd089e..04bf7db5de9527 100644
--- a/bolt/include/bolt/Utils/CommandLineOpts.h
+++ b/bolt/include/bolt/Utils/CommandLineOpts.h
@@ -34,7 +34,6 @@ extern llvm::cl::opt<bool> AggregateOnly;
 extern llvm::cl::opt<unsigned> BucketsPerLine;
 extern llvm::cl::opt<bool> DiffOnly;
 extern llvm::cl::opt<bool> EnableBAT;
-extern llvm::cl::opt<bool> UseMold;
 extern llvm::cl::opt<bool> EqualizeBBCounts;
 extern llvm::cl::opt<bool> RemoveSymtab;
 extern llvm::cl::opt<unsigned> ExecutionCountThreshold;
diff --git a/bolt/lib/Rewrite/RewriteInstance.cpp b/bolt/lib/Rewrite/RewriteInstance.cpp
index a7118be5dc263a..831880233b3acf 100644
--- a/bolt/lib/Rewrite/RewriteInstance.cpp
+++ b/bolt/lib/Rewrite/RewriteInstance.cpp
@@ -1672,35 +1672,7 @@ void RewriteInstance::disassemblePLTSectionX86(BinarySection &Section,
   const uint64_t SectionAddress = Section.getAddress();
   const uint64_t SectionSize = Section.getSize();
 
-  uint64_t EntryStartOffset = 0;
-  if (opts::UseMold) {
-    // The mold linker (https://github.com/rui314/mold/blob/v2.34.1/src/arch-x86-64.cc#L50)
-    // generates a unique format for the PLT.
-    // The first entry of the mold-style PLT is 32 bytes long, while the remaining entries
-    // are 16 bytes long. We need to parse the first entry with a special offset limit setting.
-    uint64_t HeaderSize = 32;
-    outs() << "BOLT-INFO: parsing PLT header for mold\n";
-    MCInst Instruction;
-    uint64_t InstrSize, InstrOffset = EntryStartOffset;
-    while (InstrOffset < HeaderSize) {
-      disassemblePLTInstruction(Section, InstrOffset, Instruction, InstrSize);
-      if (BC->MIB->isIndirectBranch(Instruction))
-        break;
-      InstrOffset += InstrSize;
-    }
-    uint64_t TargetAddress;
-    if (!BC->MIB->evaluateMemOperandTarget(Instruction, TargetAddress,
-                                            SectionAddress + InstrOffset,
-                                            InstrSize)) {
-      errs() << "BOLT-ERROR: error evaluating PLT instruction for the mold header at offset 0x"
-                  << Twine::utohexstr(SectionAddress + InstrOffset) << '\n';
-      exit(1);
-    }
-    createPLTBinaryFunction(TargetAddress, SectionAddress, HeaderSize);
-    EntryStartOffset += HeaderSize;
-  }
-
-  for (uint64_t EntryOffset = EntryStartOffset; EntryOffset + EntrySize <= SectionSize;
+  for (uint64_t EntryOffset = 0; EntryOffset + EntrySize <= SectionSize;
        EntryOffset += EntrySize) {
     MCInst Instruction;
     uint64_t InstrSize, InstrOffset = EntryOffset;
@@ -1717,8 +1689,30 @@ void RewriteInstance::disassemblePLTSectionX86(BinarySection &Section,
       InstrOffset += InstrSize;
     }
 
-    if (InstrOffset + InstrSize > EntryOffset + EntrySize)
+    if (InstrOffset + InstrSize > EntryOffset + EntrySize) {
+      // Check if it is a mold header before rolling back because the mold linker generates
+      // a unique format. The header entry of the mold-style PLT is 32 bytes long, while the
+      // remaining entries are 16 bytes long. We need to skip the header entry.
+      uint64_t HeaderOffset = 0, MoldHeaderSize = 32;
+      if (EntryOffset == HeaderOffset && SectionSize >= MoldHeaderSize) {
+        std::vector<MCInst *> Insns;
+        MCInst Instructions[32]; // 32 insns at most
+        uint32_t Index = 0;
+        while (HeaderOffset < MoldHeaderSize) {
+          disassemblePLTInstruction(Section, HeaderOffset, Instructions[Index], InstrSize);
+          Insns.push_back(&Instructions[Index]);
+          HeaderOffset += InstrSize;
+          Index++;
+        }
+        // if it is a mold header, skip it
+        if (BC->MIB->isMoldPLTHeader(Insns)) {
+          BC->outs() << "BOLT-INFO: parsing the PLT of the mold linker\n";
+          EntryOffset += EntrySize;
+        }
+          
+      }
       continue;
+    }
 
     uint64_t TargetAddress;
     if (!BC->MIB->evaluateMemOperandTarget(Instruction, TargetAddress,
diff --git a/bolt/lib/Target/X86/X86MCPlusBuilder.cpp b/bolt/lib/Target/X86/X86MCPlusBuilder.cpp
index 63086c06d74fd9..215380085deb01 100644
--- a/bolt/lib/Target/X86/X86MCPlusBuilder.cpp
+++ b/bolt/lib/Target/X86/X86MCPlusBuilder.cpp
@@ -2127,6 +2127,29 @@ class X86MCPlusBuilder : public MCPlusBuilder {
     return Type;
   }
 
+  /// Analyze a series of insns that match the PLT header of the mold linker
+  /// (https://github.com/rui314/mold/blob/v2.34.1/src/arch-x86-64.cc#L50).
+  /// The size of the header is 32 bytes and the format is as follows:
+  ///   endbr64
+  ///   push %r11
+  ///   push GOTPLT+8(%rip)
+  ///   jmp *GOTPLT+16(%rip)
+  ///   padding （14 bytes）
+  ///
+  bool isMoldPLTHeader(std::vector<MCInst *> &Insns) const override {
+    if (Insns.size() != 18)
+      return false;
+      
+    if (!isTerminateBranch(*Insns[0]) || !isPush(*Insns[1])
+        || !isPush(*Insns[2]) || !isIndirectBranch(*Insns[3]))
+      return false;
+      
+    for (unsigned int i = 4; i < 18; ++i)
+      if (Insns[i]->getOpcode() != X86::INT3)
+        return false;
+    return true;
+  }
+
   /// Analyze a callsite to see if it could be a virtual method call.  This only
   /// checks to see if the overall pattern is satisfied, it does not guarantee
   /// that the callsite is a true virtual method call.
diff --git a/bolt/lib/Utils/CommandLineOpts.cpp b/bolt/lib/Utils/CommandLineOpts.cpp
index 356e530c9ca361..de82420a167131 100644
--- a/bolt/lib/Utils/CommandLineOpts.cpp
+++ b/bolt/lib/Utils/CommandLineOpts.cpp
@@ -72,12 +72,6 @@ EnableBAT("enable-bat",
   cl::ZeroOrMore,
   cl::cat(BoltCategory));
 
-cl::opt<bool> UseMold("use-mold",
-  cl::desc("the binary is generated by the mold linker"),
-  cl::init(false),
-  cl::ZeroOrMore,
-  cl::cat(BoltCategory));
-
 cl::opt<bool> EqualizeBBCounts(
     "equalize-bb-counts",
     cl::desc("use same count for BBs that should have equivalent count (used "

From 8c51da24960b673d242247a3af8e486bac8a2b78 Mon Sep 17 00:00:00 2001
From: patphzhang <patphzhang@tencent.com>
Date: Fri, 8 Nov 2024 16:33:15 +0800
Subject: [PATCH 3/7] [BOLT] removed the mold option in the test case

---
 bolt/test/X86/plt-mold-header.test | 3 +--
 1 file changed, 1 insertion(+), 2 deletions(-)

diff --git a/bolt/test/X86/plt-mold-header.test b/bolt/test/X86/plt-mold-header.test
index 8cbbed8711cbce..563e15e22d6642 100644
--- a/bolt/test/X86/plt-mold-header.test
+++ b/bolt/test/X86/plt-mold-header.test
@@ -1,7 +1,6 @@
 # RUN: yaml2obj %p/Inputs/plt-mold-header.yaml &> %t.exe
-# RUN: llvm-bolt -use-mold %t.exe --print-cfg --print-only=main.* -o %t.out | FileCheck %s
+# RUN: llvm-bolt %t.exe --print-cfg --print-only=main.* -o %t.out | FileCheck %s
 
 ## Check that llvm-bolt correctly parses PLT header created by mold linker.
-## Without the '-use-mold' option, "BOLT-ERROR: unable to disassemble instruction in PLT section .plt at offset 0x10" will be reported.
 ## The only call instruction in main() should be a call to printf() in PLT.
 CHECK:  callq "printf$plt

From beaa547d8e65baa6d81f1bbc9916b766b713f99e Mon Sep 17 00:00:00 2001
From: patphzhang <patphzhang@tencent.com>
Date: Mon, 11 Nov 2024 14:05:47 +0800
Subject: [PATCH 4/7] [BOLT] parse the PLT in two parts: header detection and
 entry detection

---
 bolt/include/bolt/Core/MCPlusBuilder.h   |  5 ---
 bolt/lib/Rewrite/RewriteInstance.cpp     | 40 ++++++++++--------------
 bolt/lib/Target/X86/X86MCPlusBuilder.cpp | 23 --------------
 3 files changed, 16 insertions(+), 52 deletions(-)

diff --git a/bolt/include/bolt/Core/MCPlusBuilder.h b/bolt/include/bolt/Core/MCPlusBuilder.h
index 2cc94c52f802de..32eda0b283b883 100644
--- a/bolt/include/bolt/Core/MCPlusBuilder.h
+++ b/bolt/include/bolt/Core/MCPlusBuilder.h
@@ -1495,11 +1495,6 @@ class MCPlusBuilder {
     return 0;
   }
 
-  virtual bool isMoldPLTHeader(std::vector<MCInst *> &Insns) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
   virtual bool analyzeVirtualMethodCall(InstructionIterator Begin,
                                         InstructionIterator End,
                                         std::vector<MCInst *> &MethodFetchInsns,
diff --git a/bolt/lib/Rewrite/RewriteInstance.cpp b/bolt/lib/Rewrite/RewriteInstance.cpp
index 5d4118e51d2f1c..806edb970dab0f 100644
--- a/bolt/lib/Rewrite/RewriteInstance.cpp
+++ b/bolt/lib/Rewrite/RewriteInstance.cpp
@@ -1707,7 +1707,21 @@ void RewriteInstance::disassemblePLTSectionX86(BinarySection &Section,
   const uint64_t SectionAddress = Section.getAddress();
   const uint64_t SectionSize = Section.getSize();
 
-  for (uint64_t EntryOffset = 0; EntryOffset + EntrySize <= SectionSize;
+  // Parse the PLT header
+  uint64_t HeaderSize = 16;
+  MCInst FirstInstr;
+  uint64_t FirstInstrSize;
+  disassemblePLTInstruction(Section, 0, FirstInstr, FirstInstrSize);
+  if (BC->MIB->isTerminateBranch(FirstInstr)) {
+    // The mold linker (https://github.com/rui314/mold/blob/v2.34.1/src/arch-x86-64.cc#L50)
+    // generates a unique format for the PLT. The header entry is 32 bytes long, while the 
+    // remaining entries are 16 bytes long.
+    BC->outs() << "BOLT-INFO: parsing PLT header for mold\n";
+    HeaderSize = 32;
+  }
+
+  // Parse the PLT entries
+  for (uint64_t EntryOffset = HeaderSize; EntryOffset + EntrySize <= SectionSize;
        EntryOffset += EntrySize) {
     MCInst Instruction;
     uint64_t InstrSize, InstrOffset = EntryOffset;
@@ -1724,30 +1738,8 @@ void RewriteInstance::disassemblePLTSectionX86(BinarySection &Section,
       InstrOffset += InstrSize;
     }
 
-    if (InstrOffset + InstrSize > EntryOffset + EntrySize) {
-      // Check if it is a mold header before rolling back because the mold linker generates
-      // a unique format. The header entry of the mold-style PLT is 32 bytes long, while the
-      // remaining entries are 16 bytes long. We need to skip the header entry.
-      uint64_t HeaderOffset = 0, MoldHeaderSize = 32;
-      if (EntryOffset == HeaderOffset && SectionSize >= MoldHeaderSize) {
-        std::vector<MCInst *> Insns;
-        MCInst Instructions[32]; // 32 insns at most
-        uint32_t Index = 0;
-        while (HeaderOffset < MoldHeaderSize) {
-          disassemblePLTInstruction(Section, HeaderOffset, Instructions[Index], InstrSize);
-          Insns.push_back(&Instructions[Index]);
-          HeaderOffset += InstrSize;
-          Index++;
-        }
-        // if it is a mold header, skip it
-        if (BC->MIB->isMoldPLTHeader(Insns)) {
-          BC->outs() << "BOLT-INFO: parsing the PLT of the mold linker\n";
-          EntryOffset += EntrySize;
-        }
-          
-      }
+    if (InstrOffset + InstrSize > EntryOffset + EntrySize)
       continue;
-    }
 
     uint64_t TargetAddress;
     if (!BC->MIB->evaluateMemOperandTarget(Instruction, TargetAddress,
diff --git a/bolt/lib/Target/X86/X86MCPlusBuilder.cpp b/bolt/lib/Target/X86/X86MCPlusBuilder.cpp
index 215380085deb01..63086c06d74fd9 100644
--- a/bolt/lib/Target/X86/X86MCPlusBuilder.cpp
+++ b/bolt/lib/Target/X86/X86MCPlusBuilder.cpp
@@ -2127,29 +2127,6 @@ class X86MCPlusBuilder : public MCPlusBuilder {
     return Type;
   }
 
-  /// Analyze a series of insns that match the PLT header of the mold linker
-  /// (https://github.com/rui314/mold/blob/v2.34.1/src/arch-x86-64.cc#L50).
-  /// The size of the header is 32 bytes and the format is as follows:
-  ///   endbr64
-  ///   push %r11
-  ///   push GOTPLT+8(%rip)
-  ///   jmp *GOTPLT+16(%rip)
-  ///   padding （14 bytes）
-  ///
-  bool isMoldPLTHeader(std::vector<MCInst *> &Insns) const override {
-    if (Insns.size() != 18)
-      return false;
-      
-    if (!isTerminateBranch(*Insns[0]) || !isPush(*Insns[1])
-        || !isPush(*Insns[2]) || !isIndirectBranch(*Insns[3]))
-      return false;
-      
-    for (unsigned int i = 4; i < 18; ++i)
-      if (Insns[i]->getOpcode() != X86::INT3)
-        return false;
-    return true;
-  }
-
   /// Analyze a callsite to see if it could be a virtual method call.  This only
   /// checks to see if the overall pattern is satisfied, it does not guarantee
   /// that the callsite is a true virtual method call.

From e42256e0cf011d528e15e4d51332d1b269a8f05b Mon Sep 17 00:00:00 2001
From: patphzhang <patphzhang@tencent.com>
Date: Mon, 11 Nov 2024 15:01:44 +0800
Subject: [PATCH 5/7] [BOLT] skip the '.plt.sec' section when parsing mold
 header

---
 bolt/lib/Rewrite/RewriteInstance.cpp | 24 +++++++++++++-----------
 1 file changed, 13 insertions(+), 11 deletions(-)

diff --git a/bolt/lib/Rewrite/RewriteInstance.cpp b/bolt/lib/Rewrite/RewriteInstance.cpp
index 806edb970dab0f..3c4631f0c1451e 100644
--- a/bolt/lib/Rewrite/RewriteInstance.cpp
+++ b/bolt/lib/Rewrite/RewriteInstance.cpp
@@ -1708,17 +1708,19 @@ void RewriteInstance::disassemblePLTSectionX86(BinarySection &Section,
   const uint64_t SectionSize = Section.getSize();
 
   // Parse the PLT header
-  uint64_t HeaderSize = 16;
-  MCInst FirstInstr;
-  uint64_t FirstInstrSize;
-  disassemblePLTInstruction(Section, 0, FirstInstr, FirstInstrSize);
-  if (BC->MIB->isTerminateBranch(FirstInstr)) {
-    // The mold linker (https://github.com/rui314/mold/blob/v2.34.1/src/arch-x86-64.cc#L50)
-    // generates a unique format for the PLT. The header entry is 32 bytes long, while the 
-    // remaining entries are 16 bytes long.
-    BC->outs() << "BOLT-INFO: parsing PLT header for mold\n";
-    HeaderSize = 32;
-  }
+  uint64_t HeaderSize = 0;
+  if (Section.getName() != ".plt.sec") {
+    MCInst FirstInstr;
+    uint64_t FirstInstrSize;
+    disassemblePLTInstruction(Section, 0, FirstInstr, FirstInstrSize);
+    if (BC->MIB->isTerminateBranch(FirstInstr)) {
+      // The mold linker (https://github.com/rui314/mold/blob/v2.34.1/src/arch-x86-64.cc#L50)
+      // generates a unique format for the PLT. The header entry is 32 bytes long, while the 
+      // remaining entries are 16 bytes long.
+      BC->outs() << "BOLT-INFO: parsing PLT header for mold\n";
+      HeaderSize = 32;
+    }
+  }  
 
   // Parse the PLT entries
   for (uint64_t EntryOffset = HeaderSize; EntryOffset + EntrySize <= SectionSize;

From 7133d21a5c4f7d32002a3d0a02f626ba6296fbcf Mon Sep 17 00:00:00 2001
From: patphzhang <patphzhang@tencent.com>
Date: Mon, 11 Nov 2024 15:14:51 +0800
Subject: [PATCH 6/7] [BOLT] choose the '.plt' section when parsing mold header

---
 bolt/lib/Rewrite/RewriteInstance.cpp | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/bolt/lib/Rewrite/RewriteInstance.cpp b/bolt/lib/Rewrite/RewriteInstance.cpp
index 3c4631f0c1451e..1333851fa21945 100644
--- a/bolt/lib/Rewrite/RewriteInstance.cpp
+++ b/bolt/lib/Rewrite/RewriteInstance.cpp
@@ -1709,7 +1709,7 @@ void RewriteInstance::disassemblePLTSectionX86(BinarySection &Section,
 
   // Parse the PLT header
   uint64_t HeaderSize = 0;
-  if (Section.getName() != ".plt.sec") {
+  if (Section.getName() == ".plt") {
     MCInst FirstInstr;
     uint64_t FirstInstrSize;
     disassemblePLTInstruction(Section, 0, FirstInstr, FirstInstrSize);

From f2f238e128bcd7ce1f4b2f5c19dc3967a0090a55 Mon Sep 17 00:00:00 2001
From: patphzhang <patphzhang@tencent.com>
Date: Fri, 15 Nov 2024 14:50:43 +0800
Subject: [PATCH 7/7] [BOLT] support different PLT header type in disassembling

---
 bolt/include/bolt/Core/MCPlusBuilder.h      |  7 +++++
 bolt/include/bolt/Rewrite/RewriteInstance.h |  3 +++
 bolt/lib/Rewrite/RewriteInstance.cpp        | 29 ++++++++++++---------
 bolt/lib/Target/X86/X86MCPlusBuilder.cpp    | 28 ++++++++++++++++++++
 4 files changed, 54 insertions(+), 13 deletions(-)

diff --git a/bolt/include/bolt/Core/MCPlusBuilder.h b/bolt/include/bolt/Core/MCPlusBuilder.h
index 32eda0b283b883..6a619b33aaf48c 100644
--- a/bolt/include/bolt/Core/MCPlusBuilder.h
+++ b/bolt/include/bolt/Core/MCPlusBuilder.h
@@ -1495,6 +1495,13 @@ class MCPlusBuilder {
     return 0;
   }
 
+  /// Analyze preamble instrucions in PLT section and try to determine
+  /// the size of the header.
+  virtual uint32_t analyzePLTHeader(std::vector<MCInst *> &Insns) const {
+    llvm_unreachable("not implemented");
+    return 0;
+  }
+
   virtual bool analyzeVirtualMethodCall(InstructionIterator Begin,
                                         InstructionIterator End,
                                         std::vector<MCInst *> &MethodFetchInsns,
diff --git a/bolt/include/bolt/Rewrite/RewriteInstance.h b/bolt/include/bolt/Rewrite/RewriteInstance.h
index e5b7ad63007cab..54708da2bdf41a 100644
--- a/bolt/include/bolt/Rewrite/RewriteInstance.h
+++ b/bolt/include/bolt/Rewrite/RewriteInstance.h
@@ -277,6 +277,9 @@ class RewriteInstance {
   /// is the expected .plt \p Section entry function size.
   void disassemblePLTSectionX86(BinarySection &Section, uint64_t EntrySize);
 
+  /// Disassemble the X86-specific .plt \p Section header and get header size.
+  uint32_t disassemblePLTHeaderX86(BinarySection &Section, uint64_t EntrySize);
+
   /// Disassemble riscv-specific .plt \p Section auxiliary function
   void disassemblePLTSectionRISCV(BinarySection &Section);
 
diff --git a/bolt/lib/Rewrite/RewriteInstance.cpp b/bolt/lib/Rewrite/RewriteInstance.cpp
index 1333851fa21945..ca3f86f72d1ddf 100644
--- a/bolt/lib/Rewrite/RewriteInstance.cpp
+++ b/bolt/lib/Rewrite/RewriteInstance.cpp
@@ -1708,19 +1708,7 @@ void RewriteInstance::disassemblePLTSectionX86(BinarySection &Section,
   const uint64_t SectionSize = Section.getSize();
 
   // Parse the PLT header
-  uint64_t HeaderSize = 0;
-  if (Section.getName() == ".plt") {
-    MCInst FirstInstr;
-    uint64_t FirstInstrSize;
-    disassemblePLTInstruction(Section, 0, FirstInstr, FirstInstrSize);
-    if (BC->MIB->isTerminateBranch(FirstInstr)) {
-      // The mold linker (https://github.com/rui314/mold/blob/v2.34.1/src/arch-x86-64.cc#L50)
-      // generates a unique format for the PLT. The header entry is 32 bytes long, while the 
-      // remaining entries are 16 bytes long.
-      BC->outs() << "BOLT-INFO: parsing PLT header for mold\n";
-      HeaderSize = 32;
-    }
-  }  
+  uint64_t HeaderSize = disassemblePLTHeaderX86(Section, EntrySize);
 
   // Parse the PLT entries
   for (uint64_t EntryOffset = HeaderSize; EntryOffset + EntrySize <= SectionSize;
@@ -1757,6 +1745,21 @@ void RewriteInstance::disassemblePLTSectionX86(BinarySection &Section,
   }
 }
 
+uint32_t RewriteInstance::disassemblePLTHeaderX86(BinarySection &Section,
+                                               uint64_t EntrySize) {
+  uint64_t InstrSize, InstrOffset = 0;
+  std::vector<MCInst *> Insns;
+  MCInst Instructions[32]; // 32 insns (bytes) at most
+  uint32_t Index = 0;
+  while (InstrOffset < EntrySize) {
+    disassemblePLTInstruction(Section, InstrOffset, Instructions[Index], InstrSize);
+    Insns.push_back(&Instructions[Index]);
+    InstrOffset += InstrSize;
+    Index++;
+  }
+  return BC->MIB->analyzePLTHeader(Insns);
+}
+
 void RewriteInstance::disassemblePLT() {
   auto analyzeOnePLTSection = [&](BinarySection &Section, uint64_t EntrySize) {
     if (BC->isAArch64())
diff --git a/bolt/lib/Target/X86/X86MCPlusBuilder.cpp b/bolt/lib/Target/X86/X86MCPlusBuilder.cpp
index 63086c06d74fd9..2acff1c018c0c7 100644
--- a/bolt/lib/Target/X86/X86MCPlusBuilder.cpp
+++ b/bolt/lib/Target/X86/X86MCPlusBuilder.cpp
@@ -2127,6 +2127,34 @@ class X86MCPlusBuilder : public MCPlusBuilder {
     return Type;
   }
 
+  uint32_t analyzePLTHeader(std::vector<MCInst *> &Insns) const override {
+    uint32_t HeaderSize = 0;
+    if (Insns.size() == 0) // empty header
+      return HeaderSize;
+    if (isTerminateBranch(*Insns[0])) {
+      // starting with an endbr, possible headers: mold
+      if (Insns.size() >= 4 && isPush(*Insns[1]) && isPush(*Insns[2]) &&
+          isIndirectBranch(*Insns[3])) {
+        // The mold linker (https://github.com/rui314/mold/blob/v2.34.1/src/arch-x86-64.cc#L50)
+        // generates a unique format for the PLT. The size of the header is 32 bytes and the 
+        // format is as follows:
+        ///   endbr64
+        ///   push %r11
+        ///   push GOTPLT+8(%rip)
+        ///   jmp *GOTPLT+16(%rip)
+        ///   padding （14 bytes）
+        HeaderSize = 32; // mold with CET support
+      } else {
+        // In case other linkers have new proposals.
+      }
+    } else {
+      // TODO: headers with endbr in the midddle, including the lld version plt,
+      // or headers without CET support, including R_386_PLT32, R_X86_64_PLT32,
+      // retpolineplt of lld (for Spectre v2 mitigation), and etc.
+    }
+    return HeaderSize;
+  }
+
   /// Analyze a callsite to see if it could be a virtual method call.  This only
   /// checks to see if the overall pattern is satisfied, it does not guarantee
   /// that the callsite is a true virtual method call.
