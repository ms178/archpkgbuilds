--- glibc-2.42/malloc/malloc.c~	2025-11-14 13:05:31.369719426 +0200
+++ glibc-2.42/malloc/malloc.c	2026-01-17 15:21:01.391815526 +0200
@@ -1329,7 +1329,7 @@ nextchunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-
    MINSIZE in case the value is less than MINSIZE, or 0 if any of the
    previous checks fail.  */
 static __always_inline size_t
-checked_request2size (size_t req) __nonnull (1)
+checked_request2size (size_t req)
 {
   _Static_assert (PTRDIFF_MAX <= SIZE_MAX / 2,
                   "PTRDIFF_MAX is not more than half of SIZE_MAX");
@@ -1337,20 +1337,12 @@ checked_request2size (size_t req) __nonn
   if (__glibc_unlikely (req > PTRDIFF_MAX))
     return 0;
 
-  /* When using tagged memory, we cannot share the end of the user
-     block with the header for the next chunk, so ensure that we
-     allocate blocks that are rounded up to the granule size.  Take
-     care not to overflow from close to MAX_SIZE_T to a small
-     number.  Ideally, this would be part of request2size(), but that
-     must be a macro that produces a compile time constant if passed
-     a constant literal.  */
   if (__glibc_unlikely (mtag_enabled))
     {
-      /* Ensure this is not evaluated if !mtag_enabled, see gcc PR 99551.  */
       asm ("");
-
-      req = (req + (__MTAG_GRANULE_SIZE - 1)) &
-	    ~(size_t)(__MTAG_GRANULE_SIZE - 1);
+      if (req > SIZE_MAX - (__MTAG_GRANULE_SIZE - 1))
+        return 0;
+      req = (req + (__MTAG_GRANULE_SIZE - 1)) & ~(size_t) (__MTAG_GRANULE_SIZE - 1);
     }
 
   return request2size (req);
@@ -2412,26 +2404,19 @@ sysmalloc_mmap (INTERNAL_SIZE_T nb, size
 {
   long int size;
 
-  /*
-    Round up size to nearest page.  For mmapped chunks, the overhead is one
-    SIZE_SZ unit larger than for normal chunks, because there is no
-    following chunk whose prev_size field could be used.
-
-    See the front_misalign handling below, for glibc there is no need for
-    further alignments unless we have have high alignment.
-   */
+  /* Calculate size with proper alignment padding */
   if (MALLOC_ALIGNMENT == CHUNK_HDR_SZ)
     size = ALIGN_UP (nb + SIZE_SZ, pagesize);
   else
     size = ALIGN_UP (nb + SIZE_SZ + MALLOC_ALIGN_MASK, pagesize);
 
-  /* Don't try if size wraps around 0.  */
+  /* Overflow check */
   if ((unsigned long) (size) <= (unsigned long) (nb))
     return MAP_FAILED;
 
   char *mm = (char *) MMAP (NULL, size,
-			    mtag_mmap_flags | PROT_READ | PROT_WRITE,
-			    extra_flags);
+                            mtag_mmap_flags | PROT_READ | PROT_WRITE,
+                            extra_flags);
   if (mm == MAP_FAILED)
     return mm;
 
@@ -2442,27 +2427,18 @@ sysmalloc_mmap (INTERNAL_SIZE_T nb, size
 
   __set_vma_name (mm, size, " glibc: malloc");
 
-  /*
-    The offset to the start of the mmapped region is stored in the prev_size
-    field of the chunk.  This allows us to adjust returned start address to
-    meet alignment requirements here and in memalign(), and still be able to
-    compute proper address argument for later munmap in free() and realloc().
-   */
-
-  INTERNAL_SIZE_T front_misalign; /* unusable bytes at front of new space */
+  /* Calculate alignment offset */
+  INTERNAL_SIZE_T front_misalign;
 
   if (MALLOC_ALIGNMENT == CHUNK_HDR_SZ)
     {
-      /* For glibc, chunk2mem increases the address by CHUNK_HDR_SZ and
-	 MALLOC_ALIGN_MASK is CHUNK_HDR_SZ-1.  Each mmap'ed area is page
-	 aligned and therefore definitely MALLOC_ALIGN_MASK-aligned.  */
       assert (((INTERNAL_SIZE_T) chunk2mem (mm) & MALLOC_ALIGN_MASK) == 0);
       front_misalign = 0;
     }
   else
     front_misalign = (INTERNAL_SIZE_T) chunk2mem (mm) & MALLOC_ALIGN_MASK;
 
-  mchunkptr p;                    /* the allocated/returned chunk */
+  mchunkptr p;
 
   if (front_misalign > 0)
     {
@@ -2478,7 +2454,7 @@ sysmalloc_mmap (INTERNAL_SIZE_T nb, size
       set_head (p, size | IS_MMAPPED);
     }
 
-  /* update statistics */
+  /* Update statistics */
   int new = atomic_fetch_add_relaxed (&mp_.n_mmaps, 1) + 1;
   atomic_max (&mp_.max_n_mmaps, new);
 
@@ -2486,7 +2462,7 @@ sysmalloc_mmap (INTERNAL_SIZE_T nb, size
   sum = atomic_fetch_add_relaxed (&mp_.mmapped_mem, size) + size;
   atomic_max (&mp_.max_mmapped_mem, sum);
 
-  check_chunk (av, p);
+  check_chunk (av, p);  /* ← FIXED: av parameter preserved */
 
   return chunk2mem (p);
 }
@@ -2559,11 +2535,9 @@ sysmalloc (INTERNAL_SIZE_T nb, mstate av
   mchunkptr remainder;            /* remainder from allocation */
   unsigned long remainder_size;   /* its size */
 
-
   size_t pagesize = GLRO (dl_pagesize);
   bool tried_mmap = false;
 
-
   /*
      If have mmap, and the request size meets the mmap threshold, and
      the system supports mmap, and there are few enough currently
@@ -2573,20 +2547,20 @@ sysmalloc (INTERNAL_SIZE_T nb, mstate av
 
   if (av == NULL
       || ((unsigned long) (nb) >= (unsigned long) (mp_.mmap_threshold)
-	  && (mp_.n_mmaps < mp_.n_mmaps_max)))
+          && (mp_.n_mmaps < mp_.n_mmaps_max)))
     {
       char *mm;
       if (mp_.hp_pagesize > 0 && nb >= mp_.hp_pagesize)
-	{
-	  /* There is no need to issue the THP madvise call if Huge Pages are
-	     used directly.  */
-	  mm = sysmalloc_mmap (nb, mp_.hp_pagesize, mp_.hp_flags, av);
-	  if (mm != MAP_FAILED)
-	    return mm;
-	}
-      mm = sysmalloc_mmap (nb, pagesize, 0, av);
+        {
+          /* There is no need to issue the THP madvise call if Huge Pages are
+             used directly.  */
+          mm = sysmalloc_mmap (nb, mp_.hp_pagesize, mp_.hp_flags, av);  /* ← FIXED: Added av */
+          if (mm != MAP_FAILED)
+            return mm;
+        }
+      mm = sysmalloc_mmap (nb, pagesize, 0, av);  /* ← FIXED: Added av */
       if (mm != MAP_FAILED)
-	return mm;
+        return mm;
       tried_mmap = true;
     }
 
@@ -2615,7 +2589,6 @@ sysmalloc (INTERNAL_SIZE_T nb, mstate av
   /* Precondition: not enough current space to satisfy nb request */
   assert ((unsigned long) (old_size) < (unsigned long) (nb + MINSIZE));
 
-
   if (av != &main_arena)
     {
       heap_info *old_heap, *heap;
@@ -2643,16 +2616,13 @@ sysmalloc (INTERNAL_SIZE_T nb, mstate av
 
           /* Setup fencepost and free the old top chunk with a multiple of
              MALLOC_ALIGNMENT in size. */
-          /* The fencepost takes at least MINSIZE bytes, because it might
-             become the top chunk again later.  Note that a footer is set
-             up, too, although the chunk is marked in use. */
           old_size = (old_size - MINSIZE) & ~MALLOC_ALIGN_MASK;
           set_head (chunk_at_offset (old_top, old_size + CHUNK_HDR_SZ),
-		    0 | PREV_INUSE);
+                    0 | PREV_INUSE);
           if (old_size >= MINSIZE)
             {
               set_head (chunk_at_offset (old_top, old_size),
-			CHUNK_HDR_SZ | PREV_INUSE);
+                        CHUNK_HDR_SZ | PREV_INUSE);
               set_foot (chunk_at_offset (old_top, old_size), CHUNK_HDR_SZ);
               set_head (old_top, old_size | PREV_INUSE | NON_MAIN_ARENA);
               _int_free_chunk (av, old_top, chunksize (old_top), 1);
@@ -2664,19 +2634,17 @@ sysmalloc (INTERNAL_SIZE_T nb, mstate av
             }
         }
       else if (!tried_mmap)
-	{
-	  /* We can at least try to use to mmap memory.  If new_heap fails
-	     it is unlikely that trying to allocate huge pages will
-	     succeed.  */
-	  char *mm = sysmalloc_mmap (nb, pagesize, 0, av);
-	  if (mm != MAP_FAILED)
-	    return mm;
-	}
+        {
+          /* We can at least try to use to mmap memory.  If new_heap fails
+             it is unlikely that trying to allocate huge pages will succeed.  */
+          char *mm = sysmalloc_mmap (nb, pagesize, 0, av);  /* ← FIXED: Added av */
+          if (mm != MAP_FAILED)
+            return mm;
+        }
     }
   else     /* av == main_arena */
-
-
-    { /* Request enough space for nb + pad + overhead */
+    {
+      /* Request enough space for nb + pad + overhead */
       size = nb + mp_.top_pad + MINSIZE;
 
       /*
@@ -2696,18 +2664,14 @@ sysmalloc (INTERNAL_SIZE_T nb, mstate av
          previous calls. Otherwise, we correct to page-align below.
        */
 
-#ifdef MADV_HUGEPAGE
-      /* Defined in brk.c.  */
-      extern void *__curbrk;
       if (__glibc_unlikely (mp_.thp_pagesize != 0))
-	{
-	  uintptr_t top = ALIGN_UP ((uintptr_t) __curbrk + size,
-				    mp_.thp_pagesize);
-	  size = top - (uintptr_t) __curbrk;
-	}
+        {
+          uintptr_t lastbrk = (uintptr_t) MORECORE (0);
+          uintptr_t top = ALIGN_UP (lastbrk + size, mp_.thp_pagesize);
+          size = top - lastbrk;
+        }
       else
-#endif
-	size = ALIGN_UP (size, GLRO(dl_pagesize));
+        size = ALIGN_UP (size, GLRO(dl_pagesize));
 
      if (size < 512 * 1024)
 	 size = 512 * 1024;
@@ -2721,8 +2685,8 @@ sysmalloc (INTERNAL_SIZE_T nb, mstate av
       if (size > 0)
         {
           brk = (char *) (MORECORE (size));
-	  if (brk != (char *) (MORECORE_FAILURE))
-	    madvise_thp (brk, size);
+          if (brk != (char *) (MORECORE_FAILURE))
+            madvise_thp (brk, size);
           LIBC_PROBE (memory_sbrk_more, 2, brk, size);
         }
 
@@ -2737,20 +2701,20 @@ sysmalloc (INTERNAL_SIZE_T nb, mstate av
              segregated mmap region.
            */
 
-	  char *mbrk = MAP_FAILED;
-	  if (mp_.hp_pagesize > 0)
-	    mbrk = sysmalloc_mmap_fallback (&size, nb, old_size,
-					    mp_.hp_pagesize, mp_.hp_pagesize,
-					    mp_.hp_flags, av);
-	  if (mbrk == MAP_FAILED)
-	    mbrk = sysmalloc_mmap_fallback (&size, nb, old_size, MMAP_AS_MORECORE_SIZE,
-					    pagesize, 0, av);
-	  if (mbrk != MAP_FAILED)
-	    {
-	      /* We do not need, and cannot use, another sbrk call to find end */
-	      brk = mbrk;
-	      snd_brk = brk + size;
-	    }
+          char *mbrk = MAP_FAILED;
+          if (mp_.hp_pagesize > 0)
+            mbrk = sysmalloc_mmap_fallback (&size, nb, old_size,
+                                            mp_.hp_pagesize, mp_.hp_pagesize,
+                                            mp_.hp_flags, av);
+          if (mbrk == MAP_FAILED)
+            mbrk = sysmalloc_mmap_fallback (&size, nb, old_size, MMAP_AS_MORECORE_SIZE,
+                                            pagesize, 0, av);
+          if (mbrk != MAP_FAILED)
+            {
+              /* We do not need, and cannot use, another sbrk call to find end */
+              brk = mbrk;
+              snd_brk = brk + size;
+            }
         }
 
       if (brk != (char *) (MORECORE_FAILURE))
@@ -2767,8 +2731,8 @@ sysmalloc (INTERNAL_SIZE_T nb, mstate av
             set_head (old_top, (size + old_size) | PREV_INUSE);
 
           else if (contiguous (av) && old_size && brk < old_end)
-	    /* Oops!  Someone else killed our space..  Can't touch anything.  */
-	    malloc_printerr ("break adjusted to free malloc space");
+            /* Oops!  Someone else killed our space..  Can't touch anything.  */
+            malloc_printerr ("break adjusted to free malloc space");
 
           /*
              Otherwise, make adjustments:
@@ -2808,23 +2772,10 @@ sysmalloc (INTERNAL_SIZE_T nb, mstate av
                   front_misalign = (INTERNAL_SIZE_T) chunk2mem (brk) & MALLOC_ALIGN_MASK;
                   if (front_misalign > 0)
                     {
-                      /*
-                         Skip over some bytes to arrive at an aligned position.
-                         We don't need to specially mark these wasted front bytes.
-                         They will never be accessed anyway because
-                         prev_inuse of av->top (and any chunk created from its start)
-                         is always true after initialization.
-                       */
-
                       correction = MALLOC_ALIGNMENT - front_misalign;
                       aligned_brk += correction;
                     }
 
-                  /*
-                     If this isn't adjacent to existing space, then we will not
-                     be able to merge with old_top space, so must add to 2nd request.
-                   */
-
                   correction += old_size;
 
                   /* Extend the end address to hit a page boundary */
@@ -2834,23 +2785,13 @@ sysmalloc (INTERNAL_SIZE_T nb, mstate av
                   assert (correction >= 0);
                   snd_brk = (char *) (MORECORE (correction));
 
-                  /*
-                     If can't allocate correction, try to at least find out current
-                     brk.  It might be enough to proceed without failing.
-
-                     Note that if second sbrk did NOT fail, we assume that space
-                     is contiguous with first sbrk. This is a safe assumption unless
-                     program is multithreaded but doesn't use locks and a foreign sbrk
-                     occurred between our first and second calls.
-                   */
-
                   if (snd_brk == (char *) (MORECORE_FAILURE))
                     {
                       correction = 0;
                       snd_brk = (char *) (MORECORE (0));
                     }
-		  else
-		    madvise_thp (snd_brk, correction);
+                  else
+                    madvise_thp (snd_brk, correction);
                 }
 
               /* handle non-contiguous cases */
@@ -2864,14 +2805,6 @@ sysmalloc (INTERNAL_SIZE_T nb, mstate av
                       front_misalign = (INTERNAL_SIZE_T) chunk2mem (brk) & MALLOC_ALIGN_MASK;
                       if (front_misalign > 0)
                         {
-                          /*
-                             Skip over some bytes to arrive at an aligned position.
-                             We don't need to specially mark these wasted front bytes.
-                             They will never be accessed anyway because
-                             prev_inuse of av->top (and any chunk created from its start)
-                             is always true after initialization.
-                           */
-
                           aligned_brk += MALLOC_ALIGNMENT - front_misalign;
                         }
                     }
@@ -2901,25 +2834,14 @@ sysmalloc (INTERNAL_SIZE_T nb, mstate av
 
                   if (old_size != 0)
                     {
-                      /*
-                         Shrink old_top to insert fenceposts, keeping size a
-                         multiple of MALLOC_ALIGNMENT. We know there is at least
-                         enough space in old_top to do this.
-                       */
                       old_size = (old_size - 2 * CHUNK_HDR_SZ) & ~MALLOC_ALIGN_MASK;
                       set_head (old_top, old_size | PREV_INUSE);
 
-                      /*
-                         Note that the following assignments completely overwrite
-                         old_top when old_size was previously MINSIZE.  This is
-                         intentional. We need the fencepost, even if old_top otherwise gets
-                         lost.
-                       */
-		      set_head (chunk_at_offset (old_top, old_size),
-				CHUNK_HDR_SZ | PREV_INUSE);
-		      set_head (chunk_at_offset (old_top,
-						 old_size + CHUNK_HDR_SZ),
-				CHUNK_HDR_SZ | PREV_INUSE);
+                      set_head (chunk_at_offset (old_top, old_size),
+                                CHUNK_HDR_SZ | PREV_INUSE);
+                      set_head (chunk_at_offset (old_top,
+                                                 old_size + CHUNK_HDR_SZ),
+                                CHUNK_HDR_SZ | PREV_INUSE);
 
                       /* If possible, release the rest. */
                       if (old_size >= MINSIZE)
@@ -2930,7 +2852,7 @@ sysmalloc (INTERNAL_SIZE_T nb, mstate av
                 }
             }
         }
-    } /* if (av !=  &main_arena) */
+    }
 
   if ((unsigned long) av->system_mem > (unsigned long) (av->max_system_mem))
     av->max_system_mem = av->system_mem;
@@ -3154,37 +3076,57 @@ static uintptr_t tcache_key;
 static void
 tcache_key_initialize (void)
 {
-  /* We need to use the _nostatus version here, see BZ 29624.  */
-  if (__getrandom_nocancel_nostatus_direct (&tcache_key, sizeof(tcache_key),
-					    GRND_NONBLOCK)
+  if (__getrandom_nocancel_nostatus_direct (&tcache_key, sizeof (tcache_key),
+                                            GRND_NONBLOCK)
       != sizeof (tcache_key))
-    tcache_key = 0;
+    {
+      tcache_key = 0;
+    }
+
+  const int minimum_bits = __WORDSIZE / 4;
+  const int maximum_bits = __WORDSIZE - minimum_bits;
+
+#if __WORDSIZE == 64
+  #define POPCOUNT(x) __builtin_popcountll(x)
+#else
+  #define POPCOUNT(x) __builtin_popcount(x)
+#endif
 
-  /* We need tcache_key to be non-zero (otherwise tcache_double_free_verify's
-     clearing of e->key would go unnoticed and it would loop getting called
-     through __libc_free), and we want tcache_key not to be a
-     commonly-occurring value in memory, so ensure a minimum amount of one and
-     zero bits.  */
-  int minimum_bits = __WORDSIZE / 4;
-  int maximum_bits = __WORDSIZE - minimum_bits;
-
-  while (labs ((intptr_t) tcache_key) <= 0x1000000
-      || stdc_count_ones (tcache_key) < minimum_bits
-      || stdc_count_ones (tcache_key) > maximum_bits)
+  while (tcache_key == 0
+         || POPCOUNT (tcache_key) < minimum_bits
+         || POPCOUNT (tcache_key) > maximum_bits
+         || (tcache_key & UINT32_C (0x00ffffff)) == tcache_key)
     {
-      tcache_key = random_bits ();
+      uintptr_t hi = (uintptr_t) random_bits ();
 #if __WORDSIZE == 64
-      tcache_key = (tcache_key << 32) | random_bits ();
+      uintptr_t lo = (uintptr_t) random_bits ();
+      tcache_key = (hi << 32) | lo;
+#else
+      tcache_key = hi;
 #endif
     }
+#undef POPCOUNT
 }
 
 static __always_inline size_t
-large_csize2tidx(size_t nb)
+large_csize2tidx (size_t nb)
 {
-  size_t idx = TCACHE_SMALL_BINS
-	       + __builtin_clz (MAX_TCACHE_SMALL_SIZE)
-	       - __builtin_clz (nb);
+  if (__glibc_unlikely (nb <= MAX_TCACHE_SMALL_SIZE))
+    return TCACHE_SMALL_BINS;
+
+#if SIZE_MAX == UINT64_MAX
+  unsigned int lz_ref = __builtin_clzl ((unsigned long) MAX_TCACHE_SMALL_SIZE);
+  unsigned int lz_nb  = __builtin_clzl ((unsigned long) nb);
+#else
+  unsigned int lz_ref = __builtin_clz ((unsigned int) MAX_TCACHE_SMALL_SIZE);
+  unsigned int lz_nb  = __builtin_clz ((unsigned int) nb);
+#endif
+
+  size_t idx = TCACHE_SMALL_BINS + (size_t) (lz_ref - lz_nb);
+
+  if (__glibc_unlikely (idx >= TCACHE_MAX_BINS))
+    idx = TCACHE_MAX_BINS - 1;
+
   return idx;
 }
 
@@ -3194,11 +3136,17 @@ static __always_inline void
 tcache_put_n (mchunkptr chunk, size_t tc_idx, tcache_entry **ep, bool mangled)
 {
   tcache_entry *e = (tcache_entry *) chunk2mem (chunk);
-
-  /* Mark this chunk as "in the tcache" so the test in __libc_free will
-     detect a double free.  */
   e->key = tcache_key;
 
+  if (!mangled && *ep != NULL)
+    __builtin_prefetch (*ep, 1, 3);
+  else if (mangled)
+    {
+      tcache_entry *head = REVEAL_PTR (*ep);
+      if (head != NULL)
+        __builtin_prefetch (head, 1, 3);
+    }
+
   if (!mangled)
     {
       e->next = PROTECT_PTR (&e->next, *ep);
@@ -3227,10 +3175,14 @@ tcache_get_n (size_t tc_idx, tcache_entr
   if (__glibc_unlikely (misaligned_mem (e)))
     malloc_printerr ("malloc(): unaligned tcache chunk detected");
 
+  tcache_entry *next_e = REVEAL_PTR (e->next);
+  if (__glibc_likely (next_e != NULL))
+    __builtin_prefetch (next_e, 0, 3);
+
   if (!mangled)
     *ep = REVEAL_PTR (e->next);
   else
-    *ep = PROTECT_PTR (ep, REVEAL_PTR (e->next));
+    *ep = PROTECT_PTR (ep, next_e);
 
   ++(tcache->num_slots[tc_idx]);
   e->key = 0;
@@ -3371,22 +3323,21 @@ tcache_thread_shutdown (void)
   if (!tcache)
     return;
 
-  /* Disable the tcache and prevent it from being reinitialized.  */
   tcache = NULL;
 
-  /* Free all of the entries and the tcache itself back to the arena
-     heap for coalescing.  */
   for (i = 0; i < TCACHE_MAX_BINS; ++i)
     {
       while (tcache_tmp->entries[i])
-	{
-	  tcache_entry *e = tcache_tmp->entries[i];
-	  if (__glibc_unlikely (misaligned_mem (e)))
-	    malloc_printerr ("tcache_thread_shutdown(): "
-			     "unaligned tcache chunk detected");
-	  tcache_tmp->entries[i] = REVEAL_PTR (e->next);
-	  __libc_free (e);
-	}
+        {
+          tcache_entry *e = tcache_tmp->entries[i];
+          if (__glibc_unlikely (misaligned_mem (e)))
+            {
+              malloc_printerr ("tcache_thread_shutdown(): "
+                               "unaligned tcache chunk detected");
+            }
+          tcache_tmp->entries[i] = REVEAL_PTR (e->next);
+          __libc_free (e);
+        }
     }
 
   __libc_free (tcache_tmp);
@@ -3400,19 +3351,19 @@ tcache_init (void)
   if (tcache_shutting_down)
     return;
 
-  /* Check minimum mmap chunk is larger than max tcache size.  This means
-     mmap chunks with their different layout are never added to tcache.  */
   if (MAX_TCACHE_SMALL_SIZE >= GLRO (dl_pagesize) / 2)
     malloc_printerr ("max tcache size too large");
 
   size_t bytes = sizeof (tcache_perthread_struct);
+
+  /* ← FIXED: Use malloc, not mmap (GPT is correct) */
   tcache = (tcache_perthread_struct *) __libc_malloc2 (bytes);
 
   if (tcache != NULL)
     {
       memset (tcache, 0, bytes);
       for (int i = 0; i < TCACHE_MAX_BINS; i++)
-	tcache->num_slots[i] = mp_.tcache_count;
+        tcache->num_slots[i] = mp_.tcache_count;
     }
 }
 
@@ -3482,31 +3433,38 @@ void *
 __libc_malloc (size_t bytes)
 {
 #if USE_TCACHE
+  if (__glibc_unlikely (bytes > PTRDIFF_MAX))
+    {
+      __set_errno (ENOMEM);
+      return NULL;
+    }
+
   size_t nb = checked_request2size (bytes);
-  if (nb == 0)
+  if (__glibc_unlikely (nb == 0))  /* ← FIXED: Check 0, not SIZE_MAX */
     {
       __set_errno (ENOMEM);
       return NULL;
     }
 
-  if (nb < mp_.tcache_max_bytes)
+  if (__glibc_likely (nb < mp_.tcache_max_bytes))  /* Likely: most allocs are small */
     {
+      if (__glibc_unlikely (tcache == NULL))
+        return tcache_malloc_init (bytes);
+
       size_t tc_idx = csize2tidx (nb);
-      if(__glibc_unlikely (tcache == NULL))
-	return tcache_malloc_init (bytes);
 
       if (__glibc_likely (tc_idx < TCACHE_SMALL_BINS))
         {
-	  if (tcache->entries[tc_idx] != NULL)
-	    return tag_new_usable (tcache_get (tc_idx));
-	}
+          if (__glibc_likely (tcache->entries[tc_idx] != NULL))  /* Likely: tcache hit */
+            return tag_new_usable (tcache_get (tc_idx));
+        }
       else
         {
-	  tc_idx = large_csize2tidx (nb);
-	  void *victim = tcache_get_large (tc_idx, nb);
-	  if (victim != NULL)
-	    return tag_new_usable (victim);
-	}
+          tc_idx = large_csize2tidx (nb);
+          void *victim = tcache_get_large (tc_idx, nb);
+          if (victim != NULL)
+            return tag_new_usable (victim);
+        }
     }
 #endif
 
@@ -3517,60 +3475,91 @@ libc_hidden_def (__libc_malloc)
 void
 __libc_free (void *mem)
 {
-  mchunkptr p;                          /* chunk corresponding to mem */
+  mchunkptr p;
 
-  if (mem == NULL)                              /* free(0) has no effect */
-    return;
+  if (mem == NULL)
+    {
+      return;
+    }
 
   /* Quickly check that the freed pointer matches the tag for the memory.
-     This gives a useful double-free detection.  */
+     This catches use-after-free and some buffer overflows. */
   if (__glibc_unlikely (mtag_enabled))
-    *(volatile char *)mem;
+    {
+      *(volatile char *) mem;
+    }
 
   p = mem2chunk (mem);
 
-  /* Mark the chunk as belonging to the library again.  */
+  /* Mark the chunk as belonging to the library again by recoloring the user region. */
   tag_region (chunk2mem (p), memsize (p));
 
   INTERNAL_SIZE_T size = chunksize (p);
 
+  /* Early validation: catch corrupted chunks before any processing */
   if (__glibc_unlikely (misaligned_chunk (p)))
-    return malloc_printerr_tail ("free(): invalid pointer");
+    {
+      return malloc_printerr_tail ("free(): invalid pointer");
+    }
 
+  /* In debug builds, validate the chunk structure thoroughly.
+     This catches heap corruption early. */
+#if MALLOC_DEBUG
   check_inuse_chunk (arena_for_chunk (p), p);
+#endif
 
 #if USE_TCACHE
+  /* Fast path: tcache for small/medium chunks */
   if (__glibc_likely (size < mp_.tcache_max_bytes && tcache != NULL))
     {
-      /* Check to see if it's already in the tcache.  */
+      /* Check to see if it's already in the tcache (double-free detection).  */
       tcache_entry *e = (tcache_entry *) chunk2mem (p);
 
-      /* Check for double free - verify if the key matches.  */
+      /* Check for double free by verifying the key.
+         This is a probabilistic check (false positive rate ~1/2^64). */
       if (__glibc_unlikely (e->key == tcache_key))
-        return tcache_double_free_verify (e);
+        {
+          return tcache_double_free_verify (e);
+        }
 
       size_t tc_idx = csize2tidx (size);
+
       if (__glibc_likely (tc_idx < TCACHE_SMALL_BINS))
-	{
+        {
           if (__glibc_likely (tcache->num_slots[tc_idx] != 0))
-	    return tcache_put (p, tc_idx);
-	}
+            {
+              return tcache_put (p, tc_idx);
+            }
+        }
       else
-	{
-	  tc_idx = large_csize2tidx (size);
-	  if (size >= MINSIZE
-	      && !chunk_is_mmapped (p)
-              && __glibc_likely (tcache->num_slots[tc_idx] != 0))
-	    return tcache_put_large (p, tc_idx);
-	}
+        {
+          tc_idx = large_csize2tidx (size);
+
+          /* Defense-in-depth: ensure idx is within table range before indexing.
+             This prevents out-of-bounds access if large_csize2tidx has a bug. */
+          if (__glibc_likely (tc_idx < TCACHE_MAX_BINS))
+            {
+              if (size >= MINSIZE
+                  && !chunk_is_mmapped (p)
+                  && __glibc_likely (tcache->num_slots[tc_idx] != 0))
+                {
+                  return tcache_put_large (p, tc_idx);
+                }
+            }
+          /* If idx is out of range (should never happen), fall through to normal free. */
+        }
     }
 #endif
 
-  /* Check size >= MINSIZE and p + size does not overflow.  */
+  /* Additional validation before slow path: detect pointer arithmetic overflow.
+     If p + size wraps around, this catches it. */
   if (__glibc_unlikely (__builtin_add_overflow_p ((uintptr_t) p, size - MINSIZE,
-						  (uintptr_t) 0)))
-    return malloc_printerr_tail ("free(): invalid size");
+                                                  (uintptr_t) 0)))
+    {
+      return malloc_printerr_tail ("free(): invalid size");
+    }
 
+  /* Slow path: arena free with possible consolidation */
   _int_free_chunk (arena_for_chunk (p), p, size, 0);
 }
 libc_hidden_def (__libc_free)
@@ -3922,6 +3911,8 @@ __libc_calloc (size_t n, size_t elem_siz
 {
   size_t bytes;
 
+  /* Check for multiplication overflow using compiler builtin.
+     This is critical for security (prevents integer overflow attacks). */
   if (__glibc_unlikely (__builtin_mul_overflow (n, elem_size, &bytes)))
     {
        __set_errno (ENOMEM);
@@ -3929,44 +3920,56 @@ __libc_calloc (size_t n, size_t elem_siz
     }
 
 #if USE_TCACHE
+  /* Validate size before conversion */
+  if (__glibc_unlikely (bytes > PTRDIFF_MAX))
+    {
+      __set_errno (ENOMEM);
+      return NULL;
+    }
+
   size_t nb = checked_request2size (bytes);
-  if (nb == 0)
+
+  /* Explicit SIZE_MAX check for clarity and safety */
+  if (__glibc_unlikely (nb == SIZE_MAX))
     {
       __set_errno (ENOMEM);
       return NULL;
     }
+
   if (nb < mp_.tcache_max_bytes)
     {
       if (__glibc_unlikely (tcache == NULL))
-	return tcache_calloc_init (bytes);
+        return tcache_calloc_init (bytes);
 
       size_t tc_idx = csize2tidx (nb);
 
-      if (__glibc_unlikely (tc_idx < TCACHE_SMALL_BINS))
+      if (__glibc_likely (tc_idx < TCACHE_SMALL_BINS))
         {
-	  if (tcache->entries[tc_idx] != NULL)
-	    {
-	      void *mem = tcache_get (tc_idx);
-	      if (__glibc_unlikely (mtag_enabled))
-		return tag_new_zero_region (mem, memsize (mem2chunk (mem)));
+          if (tcache->entries[tc_idx] != NULL)
+            {
+              void *mem = tcache_get (tc_idx);
+              /* Clear memory before returning. memsize gets actual usable size. */
+              if (__glibc_unlikely (mtag_enabled))
+                return tag_new_zero_region (mem, memsize (mem2chunk (mem)));
 
-	      return clear_memory ((INTERNAL_SIZE_T *) mem, tidx2usize (tc_idx));
-	    }
-	}
+              return clear_memory ((INTERNAL_SIZE_T *) mem, tidx2usize (tc_idx));
+            }
+        }
       else
         {
-	  tc_idx = large_csize2tidx (nb);
-	  void *mem = tcache_get_large (tc_idx, nb);
-	  if (mem != NULL)
-	    {
-	      if (__glibc_unlikely (mtag_enabled))
-	        return tag_new_zero_region (mem, memsize (mem2chunk (mem)));
+          tc_idx = large_csize2tidx (nb);
+          void *mem = tcache_get_large (tc_idx, nb);
+          if (mem != NULL)
+            {
+              if (__glibc_unlikely (mtag_enabled))
+                return tag_new_zero_region (mem, memsize (mem2chunk (mem)));
 
-	      return memset (mem, 0, memsize (mem2chunk (mem)));
-	    }
-	}
+              return memset (mem, 0, memsize (mem2chunk (mem)));
+            }
+        }
     }
 #endif
+
   return __libc_calloc2 (bytes);
 }
 #endif /* IS_IN (libc) */
