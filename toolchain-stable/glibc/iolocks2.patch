On Neoverse V1, a loop using rand() improved 3.6 times. Multithreaded
performance is unchanged.

Passes regress on AArch64, OK for commit?

---
diff --git a/sysdeps/nptl/libc-lockP.h b/sysdeps/nptl/libc-lockP.h
index d3a6837fd212f3f5dfd80f46d0e9ce365042ae0c..ccdb11fee6f14069d0b936be93d0f0fa6d8bc30b 100644
--- a/sysdeps/nptl/libc-lockP.h
+++ b/sysdeps/nptl/libc-lockP.h
@@ -108,7 +108,14 @@ _Static_assert (LLL_LOCK_INITIALIZER == 0, "LLL_LOCK_INITIALIZER != 0");
 #define __libc_rwlock_fini(NAME) ((void) 0)

 /* Lock the named lock variable.  */
-#define __libc_lock_lock(NAME) ({ lll_lock (NAME, LLL_PRIVATE); 0; })
+#define __libc_lock_lock(NAME)						\
+ ({									\
+    if (SINGLE_THREAD_P)						\
+      (NAME) = LLL_LOCK_INITIALIZER_LOCKED;				\
+    else								\
+      lll_lock (NAME, LLL_PRIVATE);					\
+    0;									\
+  })
 #define __libc_rwlock_rdlock(NAME) __pthread_rwlock_rdlock (&(NAME))
 #define __libc_rwlock_wrlock(NAME) __pthread_rwlock_wrlock (&(NAME))

@@ -116,7 +123,14 @@ _Static_assert (LLL_LOCK_INITIALIZER == 0, "LLL_LOCK_INITIALIZER != 0");
 #define __libc_lock_trylock(NAME) lll_trylock (NAME)

 /* Unlock the named lock variable.  */
-#define __libc_lock_unlock(NAME) lll_unlock (NAME, LLL_PRIVATE)
+#define __libc_lock_unlock(NAME)					\
+ ({									\
+    if (SINGLE_THREAD_P)						\
+      (NAME) = LLL_LOCK_INITIALIZER;					\
+    else								\
+      lll_unlock (NAME, LLL_PRIVATE);					\
+    0;									\
+ })
 #define __libc_rwlock_unlock(NAME) __pthread_rwlock_unlock (&(NAME))

 #if IS_IN (rtld)

 diff --git a/nptl/nptl_free_tcb.c b/nptl/nptl_free_tcb.c
index 0d31143ac849de6398e06c399b94813ae57dcff3..86a89048984da79476095dc64c4d3d2edf5eca74 100644
--- a/nptl/nptl_free_tcb.c
+++ b/nptl/nptl_free_tcb.c
@@ -24,7 +24,8 @@ void
 __nptl_free_tcb (struct pthread *pd)
 {
   /* The thread is exiting now.  */
-  if (atomic_bit_test_set (&pd->cancelhandling, TERMINATED_BIT) == 0)
+  if ((atomic_fetch_or_relaxed (&pd->cancelhandling, TERMINATED_BITMASK)
+      & TERMINATED_BITMASK) == 0)
     {
       /* Free TPP data.  */
       if (pd->tpp != NULL)
diff --git a/nptl/pthread_create.c b/nptl/pthread_create.c
index 870a8fcb34eb43b58c2260fee6a4624f0fbbd469..77048bd4c148b0b2fe4ef8b4a2bf82593fef2d57 100644
--- a/nptl/pthread_create.c
+++ b/nptl/pthread_create.c
@@ -487,7 +487,7 @@ start_thread (void *arg)
   /* The thread is exiting now.  Don't set this bit until after we've hit
      the event-reporting breakpoint, so that td_thr_get_info on us while at
      the breakpoint reports TD_THR_RUN state rather than TD_THR_ZOMBIE.  */
-  atomic_bit_set (&pd->cancelhandling, EXITING_BIT);
+  atomic_fetch_or_relaxed (&pd->cancelhandling, EXITING_BITMASK);

   if (__glibc_unlikely (atomic_decrement_and_test (&__nptl_nthreads)))
     /* This was the last thread.  */
diff --git a/sysdeps/nptl/pthreadP.h b/sysdeps/nptl/pthreadP.h
index 39af275c254ef3e737736bd5c38099bada8746d6..c8521b0b232d9f9eeb09a7f62e81e8068411b9f3 100644
--- a/sysdeps/nptl/pthreadP.h
+++ b/sysdeps/nptl/pthreadP.h
@@ -43,12 +43,6 @@
   atomic_compare_and_exchange_val_acq (&(descr)->member, new, old)
 #endif

-#ifndef THREAD_ATOMIC_BIT_SET
-# define THREAD_ATOMIC_BIT_SET(descr, member, bit) \
-  atomic_bit_set (&(descr)->member, bit)
-#endif
-
-
 static inline short max_adaptive_count (void)
 {
 #if HAVE_TUNABLES
@@ -276,7 +270,7 @@ __do_cancel (void)
   struct pthread *self = THREAD_SELF;

   /* Make sure we get no more cancellations.  */
-  atomic_bit_set (&self->cancelhandling, EXITING_BIT);
+  atomic_fetch_or_relaxed (&self->cancelhandling, EXITING_BITMASK);

   __pthread_unwind ((__pthread_unwind_buf_t *)
 		    THREAD_GETMEM (self, cleanup_jmp_buf));

diff --git a/malloc/malloc.c b/malloc/malloc.c
index 6402cf94ea8ae724d86d2a04fc7d42e3af174564..adafef9d5a7b3c323bd6307c8d15f7f1921f0192 100644
--- a/malloc/malloc.c
+++ b/malloc/malloc.c
@@ -3034,7 +3034,7 @@ munmap_chunk (mchunkptr p)
     malloc_printerr ("munmap_chunk(): invalid pointer");

   atomic_decrement (&mp_.n_mmaps);
-  atomic_add (&mp_.mmapped_mem, -total_size);
+  atomic_fetch_add_relaxed (&mp_.mmapped_mem, -total_size);

   /* If munmap failed the process virtual memory address space is in a
      bad shape.  Just leave the block hanging around, the process will
diff --git a/sysdeps/unix/sysv/linux/check_pf.c b/sysdeps/unix/sysv/linux/check_pf.c
index fe73fe3ba84db02b1d7188de8f29d55019a69ff3..4d486ca9b5026c7a7950bd7c7155212966df3c44 100644
--- a/sysdeps/unix/sysv/linux/check_pf.c
+++ b/sysdeps/unix/sysv/linux/check_pf.c
@@ -278,7 +278,7 @@ make_request (int fd, pid_t pid)
     {
       free (result);

-      atomic_add (&noai6ai_cached.usecnt, 2);
+      atomic_fetch_add_relaxed (&noai6ai_cached.usecnt, 2);
       noai6ai_cached.seen_ipv4 = seen_ipv4;
       noai6ai_cached.seen_ipv6 = seen_ipv6;
       result = &noai6ai_cached;
@@ -349,7 +349,7 @@ __check_pf (bool *seen_ipv4, bool *seen_ipv6,
       *in6ai = data->in6ai;

       if (olddata != NULL && olddata->usecnt > 0
-	  && atomic_add_zero (&olddata->usecnt, -1))
+	  && atomic_fetch_add_relaxed (&olddata->usecnt, -1) == 1)
 	free (olddata);

       return;
@@ -377,7 +377,7 @@ __free_in6ai (struct in6addrinfo *ai)
 	(struct cached_data *) ((char *) ai
 				- offsetof (struct cached_data, in6ai));

-      if (atomic_add_zero (&data->usecnt, -1))
+      if (atomic_fetch_add_relaxed (&data->usecnt, -1) == 1)
 	{
 	  __libc_lock_lock (lock);

---
diff --git a/nptl/pthread_create.c b/nptl/pthread_create.c
index d206ed7bf4c2305c0d65bc2a47baefe02969e3d2..d802c67b059af390e122e82f09e886d0c8950fd7 100644
--- a/nptl/pthread_create.c
+++ b/nptl/pthread_create.c
@@ -539,7 +539,7 @@ start_thread (void *arg)
 # endif
 	  this->__list.__next = NULL;

-	  atomic_or (&this->__lock, FUTEX_OWNER_DIED);
+	  atomic_fetch_or_acquire (&this->__lock, FUTEX_OWNER_DIED);
 	  futex_wake ((unsigned int *) &this->__lock, 1,
 		      /* XYZ */ FUTEX_SHARED);
 	}
diff --git a/nptl/pthread_mutex_lock.c b/nptl/pthread_mutex_lock.c
index 6e767a87247063c0ac84242ef13e72af79021104..439b1e6391c50d5922dec6c48e7f2a2a632a89d9 100644
--- a/nptl/pthread_mutex_lock.c
+++ b/nptl/pthread_mutex_lock.c
@@ -462,7 +462,7 @@ __pthread_mutex_lock_full (pthread_mutex_t *mutex)

 	if (__glibc_unlikely (oldval & FUTEX_OWNER_DIED))
 	  {
-	    atomic_and (&mutex->__data.__lock, ~FUTEX_OWNER_DIED);
+	    atomic_fetch_and_acquire (&mutex->__data.__lock, ~FUTEX_OWNER_DIED);

 	    /* We got the mutex.  */
 	    mutex->__data.__count = 1;
diff --git a/nptl/pthread_mutex_timedlock.c b/nptl/pthread_mutex_timedlock.c
index 0fcaabfb482546fd6f1f9cc4b13edc82f6e6796c..af70a60528cb101c8e52d4165950ee0d11f6f895 100644
--- a/nptl/pthread_mutex_timedlock.c
+++ b/nptl/pthread_mutex_timedlock.c
@@ -392,7 +392,7 @@ __pthread_mutex_clocklock_common (pthread_mutex_t *mutex,

 	if (__glibc_unlikely (oldval & FUTEX_OWNER_DIED))
 	  {
-	    atomic_and (&mutex->__data.__lock, ~FUTEX_OWNER_DIED);
+	    atomic_fetch_and_acquire (&mutex->__data.__lock, ~FUTEX_OWNER_DIED);

 	    /* We got the mutex.  */
 	    mutex->__data.__count = 1;
diff --git a/nptl/pthread_mutex_trylock.c b/nptl/pthread_mutex_trylock.c
index 8a7de8e598803f606899fe1c9b8775bc24dd14ec..50524942a76c753ce4add20c35dfe7f659a1908b 100644
--- a/nptl/pthread_mutex_trylock.c
+++ b/nptl/pthread_mutex_trylock.c
@@ -308,7 +308,7 @@ ___pthread_mutex_trylock (pthread_mutex_t *mutex)

 	if (__glibc_unlikely (oldval & FUTEX_OWNER_DIED))
 	  {
-	    atomic_and (&mutex->__data.__lock, ~FUTEX_OWNER_DIED);
+	    atomic_fetch_and_acquire (&mutex->__data.__lock, ~FUTEX_OWNER_DIED);

 	    /* We got the mutex.  */
 	    mutex->__data.__count = 1;

Use a simpler way to count the number of bits set in the masked
value in _dl_important_hwcaps.

Signed-off-by: Javier Pello <devel@otheo.eu>
---
 elf/dl-hwcaps.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/elf/dl-hwcaps.c b/elf/dl-hwcaps.c
index 92eb5379..8f11a18f 100644
--- a/elf/dl-hwcaps.c
+++ b/elf/dl-hwcaps.c
@@ -197,8 +197,8 @@ _dl_important_hwcaps (const char *glibc_hwcaps_prepend,
 		  + hwcaps_counts.total_length);

   /* Count the number of bits set in the masked value.  */
-  for (n = 0; (~((1ULL << n) - 1) & masked) != 0; ++n)
-    if ((masked & (1ULL << n)) != 0)
+  for (uint64_t mm = masked; mm != 0; mm >>= 1)
+    if ((mm & 1ULL) != 0)
       ++cnt;

   /* For TLS enabled builds always add 'tls'.  */
--
2.36.0

_dl_important_hwcaps had a special case for building the power set
strings when cnt == 2, but the generic case does exactly the same.

Signed-off-by: Javier Pello <devel@otheo.eu>
---
 elf/dl-hwcaps.c | 32 ++++++++++++--------------------
 1 file changed, 12 insertions(+), 20 deletions(-)

diff --git a/elf/dl-hwcaps.c b/elf/dl-hwcaps.c
index 8f11a18f..b248f74b 100644
--- a/elf/dl-hwcaps.c
+++ b/elf/dl-hwcaps.c
@@ -307,31 +307,23 @@ _dl_important_hwcaps (const char *glibc_hwcaps_prepend,
   result[1].str = result[0].str = cp;
 #define add(idx) \
       cp = __mempcpy (__mempcpy (cp, temp[idx].str, temp[idx].len), "/", 1);
-  if (cnt == 2)
-    {
-      add (1);
-      add (0);
-    }
-  else
+  n = 1 << (cnt - 1);
+  do
     {
-      n = 1 << (cnt - 1);
-      do
-	{
-	  n -= 2;
+      n -= 2;

-	  /* We always add the last string.  */
-	  add (cnt - 1);
+      /* We always add the last string.  */
+      add (cnt - 1);

-	  /* Add the strings which have the bit set in N.  */
-	  for (m = cnt - 2; m > 0; --m)
-	    if ((n & (1 << m)) != 0)
-	      add (m);
+      /* Add the strings which have the bit set in N.  */
+      for (m = cnt - 2; m > 0; --m)
+	if ((n & (1 << m)) != 0)
+	  add (m);

-	  /* Always add the first string.  */
-	  add (0);
-	}
-      while (n != 0);
+      /* Always add the first string.  */
+      add (0);
     }
+  while (n != 0);
 #undef add

   /* Now we are ready to install the string pointers and length.  */
--
2.36.0

The power set strings in _dl_important_hwcaps were being computed
in two passes, one to generate the strings and another one to store
the pointers in the result array. Do it in a single pass instead,
as this is simpler.

Signed-off-by: Javier Pello <devel@otheo.eu>
---
 elf/dl-hwcaps.c | 58 ++++++++++++++++++-------------------------------
 1 file changed, 21 insertions(+), 37 deletions(-)

diff --git a/elf/dl-hwcaps.c b/elf/dl-hwcaps.c
index b248f74b..93a9c56d 100644
--- a/elf/dl-hwcaps.c
+++ b/elf/dl-hwcaps.c
@@ -176,7 +176,6 @@ _dl_important_hwcaps (const char *glibc_hwcaps_prepend,
   size_t cnt = GLRO (dl_platform) != NULL;
   size_t n, m;
   struct r_strlenpair *result;
-  struct r_strlenpair *rp;
   char *cp;

   /* glibc-hwcaps subdirectories.  These are exempted from the power
@@ -304,63 +303,48 @@ _dl_important_hwcaps (const char *glibc_hwcaps_prepend,
 	      #3: 0, 3			1001
      This allows the representation of all possible combinations of
      capability names in the string.  First generate the strings.  */
-  result[1].str = result[0].str = cp;
+  struct r_strlenpair *rp = result;
+  n = 1 << (cnt - 1);
+  struct r_strlenpair *rq = rp + n;
 #define add(idx) \
       cp = __mempcpy (__mempcpy (cp, temp[idx].str, temp[idx].len), "/", 1);
-  n = 1 << (cnt - 1);
   do
     {
       n -= 2;

+      /* Strings in the first half include the first component. */
+      rp[1].str = rp[0].str = cp;
+
       /* We always add the last string.  */
       add (cnt - 1);

+      /* Strings in the second half start after the first component. */
+      rq[1].str = rq[0].str = cp;
+
       /* Add the strings which have the bit set in N.  */
       for (m = cnt - 2; m > 0; --m)
 	if ((n & (1 << m)) != 0)
 	  add (m);

+      /* Odd-numbered strings end before before the last component. */
+      rp[1].len = cp - rp[1].str;
+      rq[1].len = cp - rq[1].str;
+
       /* Always add the first string.  */
       add (0);
-    }
-  while (n != 0);
-#undef add

-  /* Now we are ready to install the string pointers and length.  */
-  for (n = 0; n < (1UL << cnt); ++n)
-    result[n].len = 0;
-  n = cnt;
-  do
-    {
-      size_t mask = 1 << --n;
+      /* Even-numbered strings include the last component. */
+      rp[0].len = cp - rp[0].str;
+      rq[0].len = cp - rq[0].str;

-      rp = result;
-      for (m = 1 << cnt; m > 0; ++rp)
-	if ((--m & mask) != 0)
-	  rp->len += temp[n].len + 1;
+      rp += 2;
+      rq += 2;
     }
   while (n != 0);
+#undef add

-  /* The first half of the strings all include the first string.  */
-  n = (1 << cnt) - 2;
-  rp = &result[2];
-  while (n != (1UL << (cnt - 1)))
-    {
-      if ((--n & 1) != 0)
-	rp[0].str = rp[-2].str + rp[-2].len;
-      else
-	rp[0].str = rp[-1].str;
-      ++rp;
-    }
-
-  /* The second half starts right after the first part of the string of
-     the corresponding entry in the first half.  */
-  do
-    {
-      rp[0].str = rp[-(1 << (cnt - 1))].str + temp[cnt - 1].len + 1;
-      ++rp;
-    }
-  while (--n != 0);
+  /* Assert that we got memory allocation right. */
+  assert (cp == (char *) (overall_result + *sz) + total);

   /* The maximum string length.  */
   if (result[0].len > hwcaps_counts.maximum_length)
--
2.36.0

From dfa3771cc21ef1e082215afbc49d72d3fff61f6d Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?=D0=9B=D0=B5=D0=BE=D0=BD=D0=B8=D0=B4=20=D0=AE=D1=80=D1=8C?=
 =?UTF-8?q?=D0=B5=D0=B2=20=28Leonid=20Yuriev=29?= <leo@yuriev.ru>
Date: Wed, 3 Aug 2022 20:31:13 +0300
Subject: [PATCH] gmon: fix memory corruption due wrong calculation of required
 buffer size.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

The `__monstartup()` allocates a buffer used to store all the data
accumulated by the monitor.

The size of this buffer depends on the size of the internal structures
used and the address range for which the monitor is activated, as well
as on the maximum density of call instuctions and/or callable functions
that could be potentially on a segment of executable code.

In particular a hash table of arcs is placed at the end of this buffer.
The size of this hash table is calculated in bytes as:
  p->fromssize = p->textsize / HASHFRACTION;
but actually should be:
  p->fromssize = ROUNDUP(p->textsize / HASHFRACTION, sizeof(*p->froms));

This results in writing beyond the end of the allocated buffer when an
added arc corresponds to a call near from the end of the monitored
address range, since `_mcount()` check the incoming caller address for
monitored range but not the intermediate result hash-like index that
uses to write into the table.

It should be noted that when the results are output to `gmon.out`, the
table is read to the last element calculated from the allocated size in
bytes, so the arcs stored outside the buffer boundary did not fall into
`gprof` for analysis. Thus this "feature" help me to found this bug
during working with Bug 29438.

Another minor error seems a related typo in the calculation of `kcountsize`.

https://sourceware.org/bugzilla/show_bug.cgi?id=29444

Signed-off-by: Ð›ÐµÐ¾Ð½Ð¸Ð´ Ð®Ñ€ÑŒÐµÐ² (Leonid Yuriev) <leo@yuriev.ru>
---
 gmon/gmon.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/gmon/gmon.c b/gmon/gmon.c
index dee64803ad..4712d9f66b 100644
--- a/gmon/gmon.c
+++ b/gmon/gmon.c
@@ -132,7 +132,7 @@ __monstartup (u_long lowpc, u_long highpc)
   p->lowpc = ROUNDDOWN(lowpc, HISTFRACTION * sizeof(HISTCOUNTER));
   p->highpc = ROUNDUP(highpc, HISTFRACTION * sizeof(HISTCOUNTER));
   p->textsize = p->highpc - p->lowpc;
-  p->kcountsize = ROUNDUP(p->textsize / HISTFRACTION, sizeof(*p->froms));
+  p->kcountsize = ROUNDUP(p->textsize / HISTFRACTION, sizeof(*p->kcount));
   p->hashfraction = HASHFRACTION;
   p->log_hashfraction = -1;
   /* The following test must be kept in sync with the corresponding
@@ -142,7 +142,7 @@ __monstartup (u_long lowpc, u_long highpc)
 	 instead of integer division.  Precompute shift amount. */
       p->log_hashfraction = ffs(p->hashfraction * sizeof(*p->froms)) - 1;
   }
-  p->fromssize = p->textsize / HASHFRACTION;
+  p->fromssize = ROUNDUP(p->textsize / HASHFRACTION, sizeof(*p->froms));
   p->tolimit = p->textsize * ARCDENSITY / 100;
   if (p->tolimit < MINARCS)
     p->tolimit = MINARCS;
--
2.37.1

From 0c62c38deb339d973ac5eaf3476441f304473938 Mon Sep 17 00:00:00 2001
From: Guobing Chen <guobing.chen@intel.com>
Date: Tue, 21 May 2019 15:53:18 +0800
Subject: [PATCH] Set vector-width and alignment to fix GCC AVX issue

GCC does not always provide optimized binary code when compile under
arch=haswell or arch=skylake-avx512. Some generated functions like
libm's sin/cos/sincos/sqrt are even performing worse under AVX2/AVX512
compiling options. This patch restrict the vector-width case-by-case for
some senstive libm functions, and also add alignment (align(64)) to make
related functions perform good.

With this patch, on AVX2/AVX512 platforms, sqrt with 11~13% better
performance, exp with 24~26% better performance, exp2 with 7~9% better
performance. And sincos can performs ~97% better on AVX512 platform.

Signed-off-by: Guobing Chen <guobing.chen@intel.com>
---
 math/w_sqrt_compat.c            | 1 +
 sysdeps/ieee754/dbl-64/e_exp.c  | 1 +
 sysdeps/ieee754/dbl-64/e_exp2.c | 1 +
 sysdeps/x86_64/fpu/Makefile     | 3 +++
 4 files changed, 6 insertions(+)

diff --git a/math/w_sqrt_compat.c b/math/w_sqrt_compat.c
index cc5ba4b7..d977ec4c 100644
--- a/math/w_sqrt_compat.c
+++ b/math/w_sqrt_compat.c
@@ -26,6 +26,7 @@
 #if LIBM_SVID_COMPAT
 /* wrapper sqrt */
 double
+__attribute__((aligned(64)))
 __sqrt (double x)
 {
   if (__builtin_expect (isless (x, 0.0), 0) && _LIB_VERSION != _IEEE_)
diff --git a/sysdeps/ieee754/dbl-64/e_exp.c b/sysdeps/ieee754/dbl-64/e_exp.c
index 853d6ca7..7290caab 100644
--- a/sysdeps/ieee754/dbl-64/e_exp.c
+++ b/sysdeps/ieee754/dbl-64/e_exp.c
@@ -94,6 +94,7 @@ top12 (double x)

 double
 SECTION
+__attribute__((aligned(64)))
 __exp (double x)
 {
   uint32_t abstop;
diff --git a/sysdeps/ieee754/dbl-64/e_exp2.c b/sysdeps/ieee754/dbl-64/e_exp2.c
index 22cade8b..fe20e84c 100644
--- a/sysdeps/ieee754/dbl-64/e_exp2.c
+++ b/sysdeps/ieee754/dbl-64/e_exp2.c
@@ -87,6 +87,7 @@ top12 (double x)
 }

 double
+__attribute__((aligned(64)))
 __exp2 (double x)
 {
   uint32_t abstop;
diff --git a/sysdeps/x86_64/fpu/Makefile b/sysdeps/x86_64/fpu/Makefile
index b5f95890..30c71c84 100644
--- a/sysdeps/x86_64/fpu/Makefile
+++ b/sysdeps/x86_64/fpu/Makefile
@@ -240,4 +240,7 @@ endif
 # Limit vector width to 128 bits to work around this issue.  It improves
 # performance of sin and cos by more than 40% on Skylake.
 CFLAGS-branred.c = -mprefer-vector-width=128
+CFLAGS-s_sincos.c = -mprefer-vector-width=256
+CFLAGS-e_exp.c = -mprefer-vector-width=128
+CFLAGS-e_exp2.c = -mprefer-vector-width=128
 endif
--
2.21.0

diff --git a/sysdeps/x86_64/fpu/Makefile b/sysdeps/x86_64/fpu/Makefile
index b82cd126..56e68f04 100644
--- a/sysdeps/x86_64/fpu/Makefile
+++ b/sysdeps/x86_64/fpu/Makefile
@@ -136,8 +136,8 @@ ifeq ($(subdir)$(config-cflags-mprefer-vector-width),mathyes)
 #
 # Limit vector width to 128 bits to work around this issue.  It improves
 # performance of sin and cos by more than 40% on Skylake.
-CFLAGS-branred.c = -mprefer-vector-width=128
-CFLAGS-s_sincos.c = -mprefer-vector-width=256
-CFLAGS-e_exp.c = -mprefer-vector-width=128
-CFLAGS-e_exp2.c = -mprefer-vector-width=128
+CFLAGS-branred.c = -mprefer-vector-width=128 -fno-tree-vectorize
+CFLAGS-s_sincos.c = -mprefer-vector-width=256 -fno-tree-vectorize
+CFLAGS-e_exp.c = -mprefer-vector-width=128 -fno-tree-vectorize
+CFLAGS-e_exp2.c = -mprefer-vector-width=128 -fno-tree-vectorize
 endif
