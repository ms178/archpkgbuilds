From 5bf0ab80573d66e4ae5d94b094659094336da90f Mon Sep 17 00:00:00 2001
From: Noah Goldstein <goldstein.w.n@gmail.com>
Date: Fri, 24 May 2024 12:38:50 -0500
Subject: [PATCH] x86: Improve large memset perf with non-temporal stores
 [RHEL-29312]

Previously we use `rep stosb` for all medium/large memsets. This is
notably worse than non-temporal stores for large (above a
few MBs) memsets.
See:
https://docs.google.com/spreadsheets/d/1opzukzvum4n6-RUVHTGddV6RjAEil4P2uMjjQGLbLcU/edit?usp=sharing
For data using different stategies for large memset on ICX and SKX.

Using non-temporal stores can be up to 3x faster on ICX and 2x faster
on SKX. Historically, these numbers would not have been so good
because of the zero-over-zero writeback optimization that `rep stosb`
is able to do. But, the zero-over-zero writeback optimization has been
removed as a potential side-channel attack, so there is no longer any
good reason to only rely on `rep stosb` for large memsets. On the flip
size, non-temporal writes can avoid data in their RFO requests saving
memory bandwidth.

All of the other changes to the file are to re-organize the
code-blocks to maintain "good" alignment given the new code added in
the `L(stosb_local)` case.

The results from running the GLIBC memset benchmarks on TGL-client for
N=20 runs:

Geometric Mean across the suite New / Old EXEX256: 0.979
Geometric Mean across the suite New / Old EXEX512: 0.979
Geometric Mean across the suite New / Old AVX2   : 0.986
Geometric Mean across the suite New / Old SSE2   : 0.979

Most of the cases are essentially unchanged, this is mostly to show
that adding the non-temporal case didn't add any regressions to the
other cases.

The results on the memset-large benchmark suite on TGL-client for N=20
runs:

Geometric Mean across the suite New / Old EXEX256: 0.926
Geometric Mean across the suite New / Old EXEX512: 0.925
Geometric Mean across the suite New / Old AVX2   : 0.928
Geometric Mean across the suite New / Old SSE2   : 0.924

So roughly a 7.5% speedup. This is lower than what we see on servers
(likely because clients typically have faster single-core bandwidth so
saving bandwidth on RFOs is less impactful), but still advantageous.

Full test-suite passes on x86_64 w/ and w/o multiarch.
Reviewed-by: H.J. Lu <hjl.tools@gmail.com>
---
 .../multiarch/memset-vec-unaligned-erms.S     | 149 +++++++++++-------
 1 file changed, 91 insertions(+), 58 deletions(-)

diff --git a/sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S b/sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S
index 97839a2248..637caadb40 100644
--- a/sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S
+++ b/sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S
@@ -21,10 +21,13 @@
    2. If size is less than VEC, use integer register stores.
    3. If size is from VEC_SIZE to 2 * VEC_SIZE, use 2 VEC stores.
    4. If size is from 2 * VEC_SIZE to 4 * VEC_SIZE, use 4 VEC stores.
-   5. On machines ERMS feature, if size is greater or equal than
-      __x86_rep_stosb_threshold then REP STOSB will be used.
-   6. If size is more to 4 * VEC_SIZE, align to 4 * VEC_SIZE with
-      4 VEC stores and store 4 * VEC at a time until done.  */
+   5. If size is more to 4 * VEC_SIZE, align to 1 * VEC_SIZE with
+      4 VEC stores and store 4 * VEC at a time until done.
+   6. On machines ERMS feature, if size is range
+	  [__x86_rep_stosb_threshold, __x86_shared_non_temporal_threshold)
+	  then REP STOSB will be used.
+   7. If size >= __x86_shared_non_temporal_threshold, use a
+	  non-temporal stores.  */

 #include <sysdep.h>

@@ -147,6 +150,41 @@ L(entry_from_wmemset):
 	VMOVU	%VMM(0), -VEC_SIZE(%rdi,%rdx)
 	VMOVU	%VMM(0), (%rdi)
 	VZEROUPPER_RETURN
+
+	/* If have AVX512 mask instructions put L(less_vec) close to
+	   entry as it doesn't take much space and is likely a hot target.  */
+#ifdef USE_LESS_VEC_MASK_STORE
+    /* Align to ensure the L(less_vec) logic all fits in 1x cache lines.  */
+	.p2align 6,, 47
+	.p2align 4
+L(less_vec):
+L(less_vec_from_wmemset):
+	/* Less than 1 VEC.  */
+# if VEC_SIZE != 16 && VEC_SIZE != 32 && VEC_SIZE != 64
+#  error Unsupported VEC_SIZE!
+# endif
+	/* Clear high bits from edi. Only keeping bits relevant to page
+	   cross check. Note that we are using rax which is set in
+	   MEMSET_VDUP_TO_VEC0_AND_SET_RETURN as ptr from here on out.  */
+	andl	$(PAGE_SIZE - 1), %edi
+	/* Check if VEC_SIZE store cross page. Mask stores suffer
+	   serious performance degradation when it has to fault suppress.  */
+	cmpl	$(PAGE_SIZE - VEC_SIZE), %edi
+	/* This is generally considered a cold target.  */
+	ja	L(cross_page)
+# if VEC_SIZE > 32
+	movq	$-1, %rcx
+	bzhiq	%rdx, %rcx, %rcx
+	kmovq	%rcx, %k1
+# else
+	movl	$-1, %ecx
+	bzhil	%edx, %ecx, %ecx
+	kmovd	%ecx, %k1
+# endif
+	vmovdqu8 %VMM(0), (%rax){%k1}
+	VZEROUPPER_RETURN
+#endif
+
 #if defined USE_MULTIARCH && IS_IN (libc)
 END (MEMSET_SYMBOL (__memset, unaligned))

@@ -185,54 +223,6 @@ L(last_2x_vec):
 #endif
 	VZEROUPPER_RETURN

-	/* If have AVX512 mask instructions put L(less_vec) close to
-	   entry as it doesn't take much space and is likely a hot target.
-	 */
-#ifdef USE_LESS_VEC_MASK_STORE
-	.p2align 4,, 10
-L(less_vec):
-L(less_vec_from_wmemset):
-	/* Less than 1 VEC.  */
-# if VEC_SIZE != 16 && VEC_SIZE != 32 && VEC_SIZE != 64
-#  error Unsupported VEC_SIZE!
-# endif
-	/* Clear high bits from edi. Only keeping bits relevant to page
-	   cross check. Note that we are using rax which is set in
-	   MEMSET_VDUP_TO_VEC0_AND_SET_RETURN as ptr from here on out.  */
-	andl	$(PAGE_SIZE - 1), %edi
-	/* Check if VEC_SIZE store cross page. Mask stores suffer
-	   serious performance degradation when it has to fault suppress.
-	 */
-	cmpl	$(PAGE_SIZE - VEC_SIZE), %edi
-	/* This is generally considered a cold target.  */
-	ja	L(cross_page)
-# if VEC_SIZE > 32
-	movq	$-1, %rcx
-	bzhiq	%rdx, %rcx, %rcx
-	kmovq	%rcx, %k1
-# else
-	movl	$-1, %ecx
-	bzhil	%edx, %ecx, %ecx
-	kmovd	%ecx, %k1
-# endif
-	vmovdqu8 %VMM(0), (%rax){%k1}
-	VZEROUPPER_RETURN
-
-# if defined USE_MULTIARCH && IS_IN (libc)
-	/* Include L(stosb_local) here if including L(less_vec) between
-	   L(stosb_more_2x_vec) and ENTRY. This is to cache align the
-	   L(stosb_more_2x_vec) target.  */
-	.p2align 4,, 10
-L(stosb_local):
-	movzbl	%sil, %eax
-	mov	%RDX_LP, %RCX_LP
-	mov	%RDI_LP, %RDX_LP
-	rep	stosb
-	mov	%RDX_LP, %RAX_LP
-	VZEROUPPER_RETURN
-# endif
-#endif
-
 #if defined USE_MULTIARCH && IS_IN (libc)
 	.p2align 4
 L(stosb_more_2x_vec):
@@ -318,21 +308,33 @@ L(return_vzeroupper):
 	ret
 #endif

-	.p2align 4,, 10
-#ifndef USE_LESS_VEC_MASK_STORE
-# if defined USE_MULTIARCH && IS_IN (libc)
+#ifdef USE_WITH_AVX2
+	.p2align 4
+#else
+	.p2align 4,, 4
+#endif
+
+#if defined USE_MULTIARCH && IS_IN (libc)
 	/* If no USE_LESS_VEC_MASK put L(stosb_local) here. Will be in
 	   range for 2-byte jump encoding.  */
 L(stosb_local):
+	cmp	__x86_shared_non_temporal_threshold(%rip), %RDX_LP
+	jae	L(nt_memset)
 	movzbl	%sil, %eax
 	mov	%RDX_LP, %RCX_LP
 	mov	%RDI_LP, %RDX_LP
 	rep	stosb
+# if (defined USE_WITH_SSE2) || (defined USE_WITH_AVX512)
+	/* Use xchg to save 1-byte (this helps align targets below).  */
+	xchg	%RDX_LP, %RAX_LP
+# else
 	mov	%RDX_LP, %RAX_LP
-	VZEROUPPER_RETURN
 # endif
+	VZEROUPPER_RETURN
+#endif
+#ifndef USE_LESS_VEC_MASK_STORE
 	/* Define L(less_vec) only if not otherwise defined.  */
-	.p2align 4
+	.p2align 4,, 12
 L(less_vec):
 	/* Broadcast esi to partial register (i.e VEC_SIZE == 32 broadcast to
 	   xmm). This is only does anything for AVX2.  */
@@ -423,4 +425,35 @@ L(between_2_3):
 	movb	%SET_REG8, -1(%LESS_VEC_REG, %rdx)
 #endif
 	ret
-END (MEMSET_SYMBOL (__memset, unaligned_erms))
+
+#if defined USE_MULTIARCH && IS_IN (libc)
+# ifdef USE_WITH_AVX512
+	/* Force align so the loop doesn't cross a cache-line.  */
+	.p2align 4
+# endif
+	.p2align 4,, 7
+    /* Memset using non-temporal stores.  */
+L(nt_memset):
+	VMOVU	%VMM(0), (VEC_SIZE * 0)(%rdi)
+	leaq	(VEC_SIZE * -4)(%rdi, %rdx), %rdx
+    /* Align DST.  */
+	orq	$(VEC_SIZE * 1 - 1), %rdi
+	incq	%rdi
+	.p2align 4,, 7
+L(nt_loop):
+	VMOVNT	%VMM(0), (VEC_SIZE * 0)(%rdi)
+	VMOVNT	%VMM(0), (VEC_SIZE * 1)(%rdi)
+	VMOVNT	%VMM(0), (VEC_SIZE * 2)(%rdi)
+	VMOVNT	%VMM(0), (VEC_SIZE * 3)(%rdi)
+	subq	$(VEC_SIZE * -4), %rdi
+	cmpq	%rdx, %rdi
+	jb	L(nt_loop)
+	sfence
+	VMOVU	%VMM(0), (VEC_SIZE * 0)(%rdx)
+	VMOVU	%VMM(0), (VEC_SIZE * 1)(%rdx)
+	VMOVU	%VMM(0), (VEC_SIZE * 2)(%rdx)
+	VMOVU	%VMM(0), (VEC_SIZE * 3)(%rdx)
+	VZEROUPPER_RETURN
+#endif
+
+END(MEMSET_SYMBOL(__memset, unaligned_erms))
--
2.43.0

--- glibc-2.25/math/Makefile~	2017-02-05 15:28:43.000000000 +0000
+++ glibc-2.25/math/Makefile	2017-07-29 13:52:56.181213874 +0000
@@ -21,6 +21,8 @@

 include ../Makeconfig

+CFLAGS-.o += -fno-stack-protector -fno-math-errno -falign-functions=32
+
 # Installed header files.
 headers		:= math.h bits/mathcalls.h \
 		   fpu_control.h complex.h bits/cmathcalls.h fenv.h \
--- glibc-2.37/sysdeps/generic/adaptive_spin_count.h~	2023-02-01 03:27:45.000000000 +0000
+++ glibc-2.37/sysdeps/generic/adaptive_spin_count.h	2023-04-20 16:24:07.132939021 +0000
@@ -19,4 +19,4 @@
 /* The choice of 100 spins for the default spin count for an adaptive spin
    is a completely arbitrary choice that has not been evaluated thoroughly
    using modern hardware.  */
-#define DEFAULT_ADAPTIVE_COUNT 100
+#define DEFAULT_ADAPTIVE_COUNT 5000
--- glibc-2.24/malloc/malloc.c~	2016-11-13 22:53:14.000000000 +0000
+++ glibc-2.24/malloc/malloc.c	2016-11-13 23:01:29.750884186 +0000
@@ -858,7 +858,7 @@
 #define M_TRIM_THRESHOLD       -1

 #ifndef DEFAULT_TRIM_THRESHOLD
-#define DEFAULT_TRIM_THRESHOLD (128 * 1024)
+#define DEFAULT_TRIM_THRESHOLD (512 * 1024)
 #endif

 /*
@@ -891,7 +891,7 @@
 #define M_TOP_PAD              -2

 #ifndef DEFAULT_TOP_PAD
-#define DEFAULT_TOP_PAD        (0)
+#define DEFAULT_TOP_PAD        (256 * 1024)
 #endif

 /*
--- glibc-2.36/malloc/malloc.c~	2022-07-29 22:03:09.000000000 +0000
+++ glibc-2.36/malloc/malloc.c	2022-09-02 18:38:17.273948072 +0000
@@ -958,7 +958,7 @@
 */

 #ifndef DEFAULT_MMAP_THRESHOLD_MIN
-#define DEFAULT_MMAP_THRESHOLD_MIN (128 * 1024)
+#define DEFAULT_MMAP_THRESHOLD_MIN (512 * 1024)
 #endif

 #ifndef DEFAULT_MMAP_THRESHOLD_MAX
--- glibc-2.36/malloc/arena.c~	2022-07-29 22:03:09.000000000 +0000
+++ glibc-2.36/malloc/arena.c	2022-11-08 17:43:39.951807284 +0000
@@ -940,11 +940,11 @@
               int n = __get_nprocs_sched ();

               if (n >= 1)
-                narenas_limit = NARENAS_FROM_NCORES (n);
+                narenas_limit = NARENAS_FROM_NCORES (n + 32);
               else
                 /* We have no information about the system.  Assume two
                    cores.  */
-                narenas_limit = NARENAS_FROM_NCORES (2);
+                narenas_limit = NARENAS_FROM_NCORES (2 + 32);
             }
         }
     repeat:;
--- glibc-2.36/malloc/malloc.c~	2022-09-02 18:38:17.000000000 +0000
+++ glibc-2.36/malloc/malloc.c	2022-11-08 20:32:05.797225851 +0000
@@ -2710,6 +2710,9 @@
 #endif
 	size = ALIGN_UP (size, GLRO(dl_pagesize));

+     if (size < 512 * 1024)
+	 size = 512 * 1024;
+
       /*
          Don't try to call MORECORE if argument is so big as to appear
          negative. Note that since mmap takes size_t arg, it may succeed
From 9c308b6c4800e39bed92becde22c4a5365510b35 Mon Sep 17 00:00:00 2001
From: Ikey Doherty <michael.i.doherty@intel.com>
Date: Thu, 9 Apr 2015 16:24:28 +0100
Subject: [PATCH] locale: Use cache location

Signed-off-by: Ikey Doherty <michael.i.doherty@intel.com>
---
 locale/loadarchive.c         | 2 +-
 locale/programs/locale.c     | 2 +-
 locale/programs/locarchive.c | 2 +-
 3 files changed, 3 insertions(+), 3 deletions(-)

diff --git a/locale/loadarchive.c b/locale/loadarchive.c
index 0ac11af..4767db2 100644
--- a/locale/loadarchive.c
+++ b/locale/loadarchive.c
@@ -42,7 +42,7 @@


 /* Name of the locale archive file.  */
-static const char archfname[] = COMPLOCALEDIR "/locale-archive";
+static const char archfname[] = "/var/cache/locale/locale-archive";

 /* Size of initial mapping window, optimal if large enough to
    cover the header plus the initial locale.  */
diff --git a/locale/programs/locale.c b/locale/programs/locale.c
index 6cb3d5e..6fd43c6 100644
--- a/locale/programs/locale.c
+++ b/locale/programs/locale.c
@@ -45,7 +45,7 @@
 #include "../locarchive.h"
 #include <programs/xmalloc.h>

-#define ARCHIVE_NAME COMPLOCALEDIR "/locale-archive"
+#define ARCHIVE_NAME "/var/cache/locale/locale-archive"

 /* If set print the name of the category.  */
 static int show_category_name;
diff --git a/locale/programs/locarchive.c b/locale/programs/locarchive.c
index fadc3bf..9752b88 100644
--- a/locale/programs/locarchive.c
+++ b/locale/programs/locarchive.c
@@ -57,7 +57,7 @@

 extern const char *output_prefix;

-#define ARCHIVE_NAME COMPLOCALEDIR "/locale-archive"
+#define ARCHIVE_NAME "/var/cache/locale/locale-archive"

 static const char *locnames[] =
   {
--
2.7.0

From fd8daa179a05d1ba4adde64cc6fa804302b61500 Mon Sep 17 00:00:00 2001
From: Shan Kang <shan.kang@intel.com>
Date: Tue, 4 Jun 2019 07:38:21 +0000
Subject: [PATCH] Force ffsll to be 64 bytes aligned

Signed-off-by: Shan Kang <shan.kang@intel.com>
---
 sysdeps/x86_64/ffsll.c | 1 +
 1 file changed, 1 insertion(+)

diff --git a/sysdeps/x86_64/ffsll.c b/sysdeps/x86_64/ffsll.c
index 1caf6ac1..72a04a8f 100644
--- a/sysdeps/x86_64/ffsll.c
+++ b/sysdeps/x86_64/ffsll.c
@@ -24,6 +24,7 @@
 #undef	ffsll

 int
+__attribute__((aligned(64)))
 ffsll (long long int x)
 {
   long long int cnt;
--
2.21.0

--- glibc-2.30/sysdeps/x86_64/ffs.c~	2019-08-01 04:29:31.000000000 +0000
+++ glibc-2.30/sysdeps/x86_64/ffs.c	2020-02-27 02:05:49.505629357 +0000
@@ -23,6 +23,7 @@
 #undef	ffs

 int
+__attribute__((aligned(64)))
 __ffs (int x)
 {
   int cnt;
--- glibc-2.21/nscd/nscd_helper.c~	2015-02-06 01:40:18.000000000 -0500
+++ glibc-2.21/nscd/nscd_helper.c	2015-04-24 09:04:49.404527044 -0400
@@ -560,6 +560,7 @@
 __nscd_open_socket (const char *key, size_t keylen, request_type type,
 		    void *response, size_t responselen)
 {
+  return -1;
   /* This should never happen and it is something the nscd daemon
      enforces, too.  He it helps to limit the amount of stack
      used.  */

--- glibc-2.29/sysdeps/x86/atomic-machine.h~	2019-01-31 16:45:36.000000000 +0000
+++ glibc-2.29/sysdeps/x86/atomic-machine.h	2019-01-31 22:21:32.316611237 +0000
@@ -566,6 +566,6 @@
 #define atomic_read_barrier() __asm ("" ::: "memory")
 #define atomic_write_barrier() __asm ("" ::: "memory")

-#define atomic_spin_nop() __asm ("pause")
+#define atomic_spin_nop() __asm ("lfence")

 #endif /* atomic-machine.h */

From: Guobing Chen <guobing.chen@intel.com>
Date: Tue, 21 May 2019 15:53:18 +0800
Subject: [PATCH] Set vector-width and alignment to fix GCC AVX issue

GCC does not always provide optimized binary code when compile under
arch=haswell or arch=skylake-avx512. Some generated functions like
libm's sin/cos/sincos/sqrt are even performing worse under AVX2/AVX512
compiling options. This patch restrict the vector-width case-by-case for
some senstive libm functions, and also add alignment (align(64)) to make
related functions perform good.

With this patch, on AVX2/AVX512 platforms, sqrt with 11~13% better
performance, exp with 24~26% better performance, exp2 with 7~9% better
performance. And sincos can performs ~97% better on AVX512 platform.

Signed-off-by: Guobing Chen <guobing.chen@intel.com>
---
 math/w_sqrt_compat.c            | 1 +
 sysdeps/ieee754/dbl-64/e_exp.c  | 1 +
 sysdeps/ieee754/dbl-64/e_exp2.c | 1 +
 sysdeps/x86_64/fpu/Makefile     | 3 +++
 4 files changed, 6 insertions(+)

diff --git a/math/w_sqrt_compat.c b/math/w_sqrt_compat.c
index cc5ba4b7..d977ec4c 100644
--- a/math/w_sqrt_compat.c
+++ b/math/w_sqrt_compat.c
@@ -26,6 +26,7 @@
 #if LIBM_SVID_COMPAT
 /* wrapper sqrt */
 double
+__attribute__((aligned(64)))
 __sqrt (double x)
 {
   if (__builtin_expect (isless (x, 0.0), 0) && _LIB_VERSION != _IEEE_)
diff --git a/sysdeps/ieee754/dbl-64/e_exp.c b/sysdeps/ieee754/dbl-64/e_exp.c
index 853d6ca7..7290caab 100644
--- a/sysdeps/ieee754/dbl-64/e_exp.c
+++ b/sysdeps/ieee754/dbl-64/e_exp.c
@@ -94,6 +94,7 @@ top12 (double x)

 double
 SECTION
+__attribute__((aligned(64)))
 __exp (double x)
 {
   uint32_t abstop;
diff --git a/sysdeps/ieee754/dbl-64/e_exp2.c b/sysdeps/ieee754/dbl-64/e_exp2.c
index 22cade8b..fe20e84c 100644
--- a/sysdeps/ieee754/dbl-64/e_exp2.c
+++ b/sysdeps/ieee754/dbl-64/e_exp2.c
@@ -87,6 +87,7 @@ top12 (double x)
 }

 double
+__attribute__((aligned(64)))
 __exp2 (double x)
 {
   uint32_t abstop;
diff --git a/sysdeps/x86_64/fpu/Makefile b/sysdeps/x86_64/fpu/Makefile
index b5f95890..30c71c84 100644
--- a/sysdeps/x86_64/fpu/Makefile
+++ b/sysdeps/x86_64/fpu/Makefile
@@ -240,4 +240,7 @@ endif
 # Limit vector width to 128 bits to work around this issue.  It improves
 # performance of sin and cos by more than 40% on Skylake.
 CFLAGS-branred.c = -mprefer-vector-width=128
+CFLAGS-s_sincos.c = -mprefer-vector-width=256
+CFLAGS-e_exp.c = -mprefer-vector-width=128
+CFLAGS-e_exp2.c = -mprefer-vector-width=128
 endif
--
2.21.0

diff --git a/sysdeps/x86_64/fpu/Makefile b/sysdeps/x86_64/fpu/Makefile
index b82cd126..56e68f04 100644
--- a/sysdeps/x86_64/fpu/Makefile
+++ b/sysdeps/x86_64/fpu/Makefile
@@ -136,8 +136,8 @@ ifeq ($(subdir)$(config-cflags-mprefer-vector-width),mathyes)
 #
 # Limit vector width to 128 bits to work around this issue.  It improves
 # performance of sin and cos by more than 40% on Skylake.
-CFLAGS-branred.c = -mprefer-vector-width=128
-CFLAGS-s_sincos.c = -mprefer-vector-width=256
-CFLAGS-e_exp.c = -mprefer-vector-width=128
-CFLAGS-e_exp2.c = -mprefer-vector-width=128
+CFLAGS-branred.c = -mprefer-vector-width=128 -fno-tree-vectorize
+CFLAGS-s_sincos.c = -mprefer-vector-width=256 -fno-tree-vectorize
+CFLAGS-e_exp.c = -mprefer-vector-width=128 -fno-tree-vectorize
+CFLAGS-e_exp2.c = -mprefer-vector-width=128 -fno-tree-vectorize
 endif
Improve performance of rand() and __random() by adding a single-threaded fast
path.  Bench-random-lock shows about 5x speedup on Neoverse V1.

---

diff --git a/stdlib/random.c b/stdlib/random.c
index 62f22fac8d58c7977f09c134bf80a797750da645..a22de60a0f96031c74dd5a949b6717c2b0fc321a 100644
--- a/stdlib/random.c
+++ b/stdlib/random.c
@@ -51,6 +51,7 @@
    SUCH DAMAGE.*/

 #include <libc-lock.h>
+#include <single-thread.h>
 #include <limits.h>
 #include <stddef.h>
 #include <stdlib.h>
@@ -288,6 +289,12 @@ __random (void)
 {
   int32_t retval;

+  if (SINGLE_THREAD_P)
+    {
+      (void) __random_r (&unsafe_state, &retval);
+      return retval;
+    }
+
   __libc_lock_lock (lock);

   (void) __random_r (&unsafe_state, &retval);

This patch align memmove unaligned routines to 64 byte.  Default 16 byte
alignment may cause upto 15% random perf regression for less than vector
size memmove.
---
 sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
index 838f8f8bff..85c0efd9e3 100644
--- a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
+++ b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
@@ -207,7 +207,7 @@ ENTRY (MEMMOVE_CHK_SYMBOL (__memmove_chk, unaligned))
 END (MEMMOVE_CHK_SYMBOL (__memmove_chk, unaligned))
 #endif

-ENTRY (MEMMOVE_SYMBOL (__memmove, unaligned))
+ENTRY_P2ALIGN (MEMMOVE_SYMBOL (__memmove, unaligned), 6)
 	movq	%rdi, %rax
 L(start):
 # ifdef __ILP32__
--
2.44.0

Left shift of ki is undefined when ki<0, copy the logic from exp,
which uses unsigned arithmetics, to fix it.
---
 sysdeps/ieee754/dbl-64/e_exp10.c | 6 +++---
 1 file changed, 3 insertions(+), 3 deletions(-)

diff --git a/sysdeps/ieee754/dbl-64/e_exp10.c b/sysdeps/ieee754/dbl-64/e_exp10.c
index 225fc74c4c..7ea8270063 100644
--- a/sysdeps/ieee754/dbl-64/e_exp10.c
+++ b/sysdeps/ieee754/dbl-64/e_exp10.c
@@ -38,7 +38,7 @@ special_case (uint64_t sbits, double_t tmp, uint64_t ki)
 {
   double_t scale, y;

-  if (ki - (1ull << 16) < 0x80000000)
+  if ((ki & 0x80000000) == 0)
     {
       /* The exponent of scale might have overflowed by 1.  */
       sbits -= 1ull << 52;
@@ -100,14 +100,14 @@ __exp10 (double x)
   /* Reduce x: z = x * N / log10(2), k = round(z).  */
   double_t z = __exp_data.invlog10_2N * x;
   double_t kd;
-  int64_t ki;
+  uint64_t ki;
 #if TOINT_INTRINSICS
   kd = roundtoint (z);
   ki = converttoint (z);
 #else
   kd = math_narrow_eval (z + Shift);
+  ki = asuint64 (kd);
   kd -= Shift;
-  ki = kd;
 #endif

   /* r = x - k * log10(2), r in [-0.5, 0.5].  */
--
2.25.1
