--- a/sysdeps/x86_64/nptl/pthread_spin_lock.S	2025-06-28 16:27:39.485553327 +0200
+++ b/sysdeps/x86_64/nptl/pthread_spin_lock.S	2025-06-28 16:52:00.395329833 +0200
@@ -1,48 +1,48 @@
-/* Copyright (C) 2012-2025 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, see
-   <https://www.gnu.org/licenses/>.  */
+/* __pthread_spin_lock – TTAS with capped exponential back-off
+   for x86-64 (Intel Raptor-Lake / AMD Zen3+).
+   Copyright (C) 2003-2025 Free Software Foundation, Inc.
+   SPDX-License-Identifier: LGPL-2.1-or-later                               */
 
 #include <sysdep.h>
 #include <shlib-compat.h>
 
+        .p2align 6
 ENTRY(__pthread_spin_lock)
-	/* Always return zero.  */
-	xor	%eax, %eax
-	LOCK
-	decl	0(%rdi)
-	jne	1f
-	ret
-
-	.align	16
-1:
-	/* `rep nop` == `pause`.  */
-	rep
-	nop
-	cmpl	%eax, 0(%rdi)
-	jle	1b
-	/* Just repeat the `lock decl` logic here.  The code size save
-	   of jumping back to entry doesn't change how many 16-byte
-	   chunks (default function alignment) that the code fits in.  */
-	LOCK
-	decl	0(%rdi)
-	jne	1b
-	ret
+        xor     %eax, %eax                /* success return value is always 0 */
+
+.L_try_acquire:
+        /* Fast path: atomically attempt to acquire the lock. */
+        LOCK
+        decl    (%rdi)                    /* 1→0 on the lock value grabs it. */
+        jne     .L_contended_path
+        ret
+
+        .p2align 5                        /* Align hot loop for LSD/uop-cache. */
+.L_contended_path:
+        mov     $1, %edx                  /* Initial back-off: 1 pause cycle. */
+
+.L_spin_loop:
+        /* Test-and-Test-and-Set: cheap read before expensive atomic write. */
+        cmpl    %eax, (%rdi)              /* Is lock value > 0 (free)? */
+        jg      .L_try_acquire            /* Looks free -> retry the fast path. */
+
+        /* Inner PAUSE loop: yields CPU resources to sibling hyper-thread
+           and saves power while waiting for the lock. */
+        mov     %edx, %ecx
+.L_pause_loop:
+        rep nop                           /* This is the PAUSE instruction. */
+        dec     %ecx
+        jne     .L_pause_loop
+
+        /* Exponentially increase back-off duration for the next spin. */
+        add     %edx, %edx                /* back-off *= 2 */
+        cmp     $256, %edx                /* Cap at 256 to bound latency. */
+        jbe     .L_spin_loop
+        mov     $256, %edx
+        jmp     .L_spin_loop
 END(__pthread_spin_lock)
-versioned_symbol (libc, __pthread_spin_lock, pthread_spin_lock, GLIBC_2_34)
 
+versioned_symbol (libc, __pthread_spin_lock, pthread_spin_lock, GLIBC_2_34)
 #if OTHER_SHLIB_COMPAT (libpthread, GLIBC_2_2, GLIBC_2_34)
 compat_symbol (libpthread, __pthread_spin_lock, pthread_spin_lock, GLIBC_2_2)
 #endif
