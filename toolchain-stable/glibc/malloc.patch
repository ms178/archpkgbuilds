--- glibc-2.41/malloc/malloc.c~	2025-05-03 13:05:31.369719426 +0200
+++ glibc-2.41/malloc/malloc.c	2025-06-05 15:21:01.391815526 +0200
@@ -294,9 +294,9 @@
 # define TCACHE_SMALL_BINS		64
 # define TCACHE_LARGE_BINS		12 /* Up to 4M chunks */
 # define TCACHE_MAX_BINS	(TCACHE_SMALL_BINS + TCACHE_LARGE_BINS)
-# define MAX_TCACHE_SMALL_SIZE	tidx2usize (TCACHE_SMALL_BINS-1)
+# define MAX_TCACHE_SMALL_SIZE	tidx2csize (TCACHE_SMALL_BINS-1)
 
-/* Only used to pre-fill the tunables.  */
+# define tidx2csize(idx)	(((size_t) idx) * MALLOC_ALIGNMENT + MINSIZE)
 # define tidx2usize(idx)	(((size_t) idx) * MALLOC_ALIGNMENT + MINSIZE - SIZE_SZ)
 
 /* When "x" is from chunksize().  */
@@ -1616,40 +1616,51 @@ static void
 unlink_chunk (mstate av, mchunkptr p)
 {
   if (chunksize (p) != prev_size (next_chunk (p)))
+  {
     malloc_printerr ("corrupted size vs. prev_size");
+  }
 
   mchunkptr fd = p->fd;
   mchunkptr bk = p->bk;
 
   if (__glibc_unlikely (fd->bk != p || bk->fd != p))
+  {
     malloc_printerr ("corrupted double-linked list");
+  }
 
   fd->bk = bk;
   bk->fd = fd;
-  if (!in_smallbin_range (chunksize_nomask (p)) && p->fd_nextsize != NULL)
+
+  /* Use chunksize() here, not chunksize_nomask(), which could contain
+   *    status bits and incorrectly classify a large chunk as small. */
+  if (!in_smallbin_range (chunksize (p)) && p->fd_nextsize != NULL)
+  {
+    if (p->fd_nextsize->bk_nextsize != p
+      || p->bk_nextsize->fd_nextsize != p)
     {
-      if (p->fd_nextsize->bk_nextsize != p
-	  || p->bk_nextsize->fd_nextsize != p)
-	malloc_printerr ("corrupted double-linked list (not small)");
+      malloc_printerr ("corrupted double-linked list (not small)");
+    }
 
-      if (fd->fd_nextsize == NULL)
-	{
-	  if (p->fd_nextsize == p)
-	    fd->fd_nextsize = fd->bk_nextsize = fd;
-	  else
-	    {
-	      fd->fd_nextsize = p->fd_nextsize;
-	      fd->bk_nextsize = p->bk_nextsize;
-	      p->fd_nextsize->bk_nextsize = fd;
-	      p->bk_nextsize->fd_nextsize = fd;
-	    }
-	}
+    if (fd->fd_nextsize == NULL)
+    {
+      if (p->fd_nextsize == p)
+      {
+        fd->fd_nextsize = fd->bk_nextsize = fd;
+      }
       else
-	{
-	  p->fd_nextsize->bk_nextsize = p->bk_nextsize;
-	  p->bk_nextsize->fd_nextsize = p->fd_nextsize;
-	}
+      {
+        fd->fd_nextsize = p->fd_nextsize;
+        fd->bk_nextsize = p->bk_nextsize;
+        p->fd_nextsize->bk_nextsize = fd;
+        p->bk_nextsize->fd_nextsize = fd;
+      }
+    }
+    else
+    {
+      p->fd_nextsize->bk_nextsize = p->bk_nextsize;
+      p->bk_nextsize->fd_nextsize = p->fd_nextsize;
     }
+  }
 }
 
 /*
@@ -1825,9 +1836,10 @@ struct malloc_state
   /* Flags (formerly in max_fast).  */
   int flags;
 
-  /* Set if the fastbin chunks contain recently inserted free blocks.  */
-  /* Note this is a bool but not all targets support atomics on booleans.  */
-  int have_fastchunks;
+  /* Set if the fastbin chunks contain recently inserted free blocks.
+   *   Note this is a bool but not all targets support atomics on booleans.
+   *   Align to a cache line to prevent false sharing with other members.  */
+  __attribute__((aligned(64))) int have_fastchunks;
 
   /* Fastbins */
   mfastbinptr fastbinsY[NFASTBINS];
@@ -1848,12 +1860,12 @@ struct malloc_state
   struct malloc_state *next;
 
   /* Linked list for free arenas.  Access to this field is serialized
-     by free_list_lock in arena.c.  */
+   *   by free_list_lock in arena.c.  */
   struct malloc_state *next_free;
 
   /* Number of threads attached to this arena.  0 if the arena is on
-     the free list.  Access to this field is serialized by
-     free_list_lock in arena.c.  */
+   *   the free list.  Access to this field is serialized by
+   *   free_list_lock in arena.c.  */
   INTERNAL_SIZE_T attached_threads;
 
   /* Memory allocated from the system in this arena.  */
@@ -1926,15 +1938,23 @@ static struct malloc_par mp_ =
   .n_mmaps_max = DEFAULT_MMAP_MAX,
   .mmap_threshold = DEFAULT_MMAP_THRESHOLD,
   .trim_threshold = DEFAULT_TRIM_THRESHOLD,
-#define NARENAS_FROM_NCORES(n) ((n) * (sizeof (long) == 4 ? 2 : 8))
+  /* On 64-bit, scale arenas to 16x cores, and 4x on 32-bit.
+   *   This provides better scaling for high-core-count CPUs like Raptor Lake,
+   *   reducing lock contention. */
+  #undef NARENAS_FROM_NCORES
+  #define NARENAS_FROM_NCORES(n) ((n) * (sizeof (long) == 4 ? 4 : 16))
   .arena_test = NARENAS_FROM_NCORES (1)
-#if USE_TCACHE
+  #if USE_TCACHE
   ,
-  .tcache_count = TCACHE_FILL_COUNT,
+  /* Increase tcache capacity to improve hit rate for small, frequent
+   *   allocations common in games. This avoids expensive arena locking. */
+  .tcache_count = 32,
   .tcache_small_bins = TCACHE_SMALL_BINS,
-  .tcache_max_bytes = MAX_TCACHE_SMALL_SIZE,
-  .tcache_unsorted_limit = 0 /* No limit.  */
-#endif
+  .tcache_max_bytes = MAX_TCACHE_SMALL_SIZE + 1,
+  /* Throttle the unsorted bin scan when filling tcache to prevent one
+   *   thread from holding the arena lock for too long, improving fairness. */
+  .tcache_unsorted_limit = 24
+  #endif
 };
 
 /*
@@ -3127,7 +3147,7 @@ typedef struct tcache_entry
    free blocks, while 'num_slots' contains the number of free blocks that can
    be added.  Each bin may allow a different maximum number of free blocks,
    and can be disabled by initializing 'num_slots' to zero.  */
-typedef struct tcache_perthread_struct
+typedef struct __attribute__((aligned(64))) tcache_perthread_struct
 {
   uint16_t num_slots[TCACHE_MAX_BINS];
   tcache_entry *entries[TCACHE_MAX_BINS];
@@ -3354,27 +3374,31 @@ tcache_thread_shutdown (void)
   tcache_shutting_down = true;
 
   if (!tcache)
+  {
     return;
+  }
 
   /* Disable the tcache and prevent it from being reinitialized.  */
   tcache = NULL;
 
-  /* Free all of the entries and the tcache itself back to the arena
-     heap for coalescing.  */
+  /* Free all of the chunk entries back to the arena heap for coalescing.  */
   for (i = 0; i < TCACHE_MAX_BINS; ++i)
+  {
+    while (tcache_tmp->entries[i])
     {
-      while (tcache_tmp->entries[i])
-	{
-	  tcache_entry *e = tcache_tmp->entries[i];
-	  if (__glibc_unlikely (misaligned_mem (e)))
-	    malloc_printerr ("tcache_thread_shutdown(): "
-			     "unaligned tcache chunk detected");
-	  tcache_tmp->entries[i] = REVEAL_PTR (e->next);
-	  __libc_free (e);
-	}
+      tcache_entry *e = tcache_tmp->entries[i];
+      if (__glibc_unlikely (misaligned_mem (e)))
+      {
+        malloc_printerr ("tcache_thread_shutdown(): "
+        "unaligned tcache chunk detected");
+      }
+      tcache_tmp->entries[i] = REVEAL_PTR (e->next);
+      __libc_free (e);
     }
+  }
 
-  __libc_free (tcache_tmp);
+  /* Deallocate the tcache struct itself, which was allocated via mmap.  */
+  __munmap (tcache_tmp, sizeof (tcache_perthread_struct));
 }
 
 /* Initialize tcache.  In the rare case there isn't any memory available,
@@ -3383,21 +3407,32 @@ static void
 tcache_init (void)
 {
   if (tcache_shutting_down)
-    return;
+    {
+      return;
+    }
 
   /* Check minimum mmap chunk is larger than max tcache size.  This means
      mmap chunks with their different layout are never added to tcache.  */
   if (MAX_TCACHE_SMALL_SIZE >= GLRO (dl_pagesize) / 2)
-    malloc_printerr ("max tcache size too large");
+    {
+      malloc_printerr ("max tcache size too large");
+    }
 
   size_t bytes = sizeof (tcache_perthread_struct);
-  tcache = (tcache_perthread_struct *) __libc_malloc2 (bytes);
+  /* Use a direct mmap call to acquire page-aligned memory for the tcache.
+     This is guaranteed to be 64-byte aligned and, crucially, avoids
+     calling back into the main malloc pathways, preventing infinite
+     recursion during initialization. */
+  void *mem = MMAP (NULL, bytes, PROT_READ | PROT_WRITE, 0);
 
-  if (tcache != NULL)
+  if (mem != MAP_FAILED)
     {
-      memset (tcache, 0, bytes);
+      tcache = (tcache_perthread_struct *) mem;
+      /* The allocated memory from mmap is already zeroed by the kernel. */
       for (int i = 0; i < TCACHE_MAX_BINS; i++)
-	tcache->num_slots[i] = mp_.tcache_count;
+	{
+	  tcache->num_slots[i] = mp_.tcache_count;
+	}
     }
 }
 
@@ -4007,7 +4042,9 @@ _int_malloc (mstate av, size_t bytes)
     {
       void *p = sysmalloc (nb, av);
       if (p != NULL)
-	alloc_perturb (p, bytes);
+	{
+	  alloc_perturb (p, bytes);
+	}
       return p;
     }
 
@@ -4040,17 +4077,25 @@ _int_malloc (mstate av, size_t bytes)
       if (victim != NULL)
 	{
 	  if (__glibc_unlikely (misaligned_chunk (victim)))
-	    malloc_printerr ("malloc(): unaligned fastbin chunk detected 2");
+	    {
+	      malloc_printerr ("malloc(): unaligned fastbin chunk detected 2");
+	    }
 
 	  if (SINGLE_THREAD_P)
-	    *fb = REVEAL_PTR (victim->fd);
+	    {
+	      *fb = REVEAL_PTR (victim->fd);
+	    }
 	  else
-	    REMOVE_FB (fb, pp, victim);
+	    {
+	      REMOVE_FB (fb, pp, victim);
+	    }
 	  if (__glibc_likely (victim != NULL))
 	    {
 	      size_t victim_idx = fastbin_index (chunksize (victim));
 	      if (__glibc_unlikely (victim_idx != idx))
-		malloc_printerr ("malloc(): memory corruption (fast)");
+		{
+		  malloc_printerr ("malloc(): memory corruption (fast)");
+		}
 	      check_remalloced_chunk (av, victim, nb);
 #if USE_TCACHE
 	      /* While we're here, if we see other chunks of the same size,
@@ -4064,17 +4109,25 @@ _int_malloc (mstate av, size_t bytes)
 		  while (tcache->num_slots[tc_idx] != 0 && (tc_victim = *fb) != NULL)
 		    {
 		      if (__glibc_unlikely (misaligned_chunk (tc_victim)))
-			malloc_printerr ("malloc(): unaligned fastbin chunk detected 3");
+			{
+			  malloc_printerr ("malloc(): unaligned fastbin chunk detected 3");
+			}
 		      size_t victim_tc_idx = csize2tidx (chunksize (tc_victim));
 		      if (__glibc_unlikely (tc_idx != victim_tc_idx))
-			malloc_printerr ("malloc(): chunk size mismatch in fastbin");
+			{
+			  malloc_printerr ("malloc(): chunk size mismatch in fastbin");
+			}
 		      if (SINGLE_THREAD_P)
-			*fb = REVEAL_PTR (tc_victim->fd);
+			{
+			  *fb = REVEAL_PTR (tc_victim->fd);
+			}
 		      else
 			{
 			  REMOVE_FB (fb, pp, tc_victim);
 			  if (__glibc_unlikely (tc_victim == NULL))
-			    break;
+			    {
+			      break;
+			    }
 			}
 		      tcache_put (tc_victim, tc_idx);
 		    }
@@ -4104,13 +4157,17 @@ _int_malloc (mstate av, size_t bytes)
         {
           bck = victim->bk;
 	  if (__glibc_unlikely (bck->fd != victim))
-	    malloc_printerr ("malloc(): smallbin double linked list corrupted");
+	    {
+	      malloc_printerr ("malloc(): smallbin double linked list corrupted");
+	    }
           set_inuse_bit_at_offset (victim, nb);
           bin->bk = bck;
           bck->fd = bin;
 
           if (av != &main_arena)
-	    set_non_main_arena (victim);
+	    {
+	      set_non_main_arena (victim);
+	    }
           check_malloced_chunk (av, victim, nb);
 #if USE_TCACHE
 	  /* While we're here, if we see other chunks of the same size,
@@ -4129,7 +4186,9 @@ _int_malloc (mstate av, size_t bytes)
 		      bck = tc_victim->bk;
 		      set_inuse_bit_at_offset (tc_victim, nb);
 		      if (av != &main_arena)
-			set_non_main_arena (tc_victim);
+			{
+			  set_non_main_arena (tc_victim);
+			}
 		      bin->bk = bck;
 		      bck->fd = bin;
 
@@ -4159,7 +4218,9 @@ _int_malloc (mstate av, size_t bytes)
     {
       idx = largebin_index (nb);
       if (atomic_load_relaxed (&av->have_fastchunks))
-        malloc_consolidate (av);
+        {
+	  malloc_consolidate (av);
+	}
     }
 
   /*
@@ -4179,7 +4240,9 @@ _int_malloc (mstate av, size_t bytes)
   INTERNAL_SIZE_T tcache_nb = 0;
   size_t tc_idx = csize2tidx (nb);
   if (tcache != NULL && tc_idx < mp_.tcache_small_bins)
-    tcache_nb = nb;
+    {
+      tcache_nb = nb;
+    }
   int return_cached = 0;
 
   tcache_unsorted_count = 0;
@@ -4190,23 +4253,39 @@ _int_malloc (mstate av, size_t bytes)
       int iters = 0;
       while ((victim = unsorted_chunks (av)->bk) != unsorted_chunks (av))
         {
+	  /* Prefetch the next chunk to hide memory latency. */
+	  if (victim->bk != unsorted_chunks (av))
+	    {
+	      __builtin_prefetch (victim->bk, 0, 1);
+	    }
+
           bck = victim->bk;
           size = chunksize (victim);
           mchunkptr next = chunk_at_offset (victim, size);
 
           if (__glibc_unlikely (size <= CHUNK_HDR_SZ)
               || __glibc_unlikely (size > av->system_mem))
-            malloc_printerr ("malloc(): invalid size (unsorted)");
+            {
+	      malloc_printerr ("malloc(): invalid size (unsorted)");
+	    }
           if (__glibc_unlikely (chunksize_nomask (next) < CHUNK_HDR_SZ)
               || __glibc_unlikely (chunksize_nomask (next) > av->system_mem))
-            malloc_printerr ("malloc(): invalid next size (unsorted)");
+            {
+	      malloc_printerr ("malloc(): invalid next size (unsorted)");
+	    }
           if (__glibc_unlikely ((prev_size (next) & ~(SIZE_BITS)) != size))
-            malloc_printerr ("malloc(): mismatching next->prev_size (unsorted)");
+            {
+	      malloc_printerr ("malloc(): mismatching next->prev_size (unsorted)");
+	    }
           if (__glibc_unlikely (bck->fd != victim)
               || __glibc_unlikely (victim->fd != unsorted_chunks (av)))
-            malloc_printerr ("malloc(): unsorted double linked list corrupted");
+            {
+	      malloc_printerr ("malloc(): unsorted double linked list corrupted");
+	    }
           if (__glibc_unlikely (prev_inuse (next)))
-            malloc_printerr ("malloc(): invalid next->prev_inuse (unsorted)");
+            {
+	      malloc_printerr ("malloc(): invalid next->prev_inuse (unsorted)");
+	    }
 
           /*
              If a small request, try to use last remainder if it is the
@@ -4254,7 +4333,9 @@ _int_malloc (mstate av, size_t bytes)
             {
               set_inuse_bit_at_offset (victim, size);
               if (av != &main_arena)
-		set_non_main_arena (victim);
+		{
+		  set_non_main_arena (victim);
+		}
 #if USE_TCACHE
 	      /* Fill cache first, return to user only if cache fills.
 		 We may return one of these chunks later.  */
@@ -4295,17 +4376,19 @@ _int_malloc (mstate av, size_t bytes)
               if (fwd != bck)
                 {
                   /* Or with inuse bit to speed comparisons */
-                  size |= PREV_INUSE;
+                  INTERNAL_SIZE_T cmp_size = size | PREV_INUSE;
                   /* if smaller than smallest, bypass loop below */
                   assert (chunk_main_arena (bck->bk));
-                  if ((unsigned long) (size)
+                  if ((unsigned long) (cmp_size)
 		      < (unsigned long) chunksize_nomask (bck->bk))
                     {
                       fwd = bck;
                       bck = bck->bk;
 
                       if (__glibc_unlikely (fwd->fd->bk_nextsize->fd_nextsize != fwd->fd))
-                        malloc_printerr ("malloc(): largebin double linked list corrupted (nextsize)");
+                        {
+			  malloc_printerr ("malloc(): largebin double linked list corrupted (nextsize)");
+			}
 
                       victim->fd_nextsize = fwd->fd;
                       victim->bk_nextsize = fwd->fd->bk_nextsize;
@@ -4314,32 +4397,40 @@ _int_malloc (mstate av, size_t bytes)
                   else
                     {
                       assert (chunk_main_arena (fwd));
-                      while ((unsigned long) size < chunksize_nomask (fwd))
+                      while ((unsigned long) cmp_size < chunksize_nomask (fwd))
                         {
                           fwd = fwd->fd_nextsize;
 			  assert (chunk_main_arena (fwd));
                         }
 
-                      if ((unsigned long) size
+                      if ((unsigned long) cmp_size
 			  == (unsigned long) chunksize_nomask (fwd))
                         /* Always insert in the second position.  */
-                        fwd = fwd->fd;
+                        {
+			  fwd = fwd->fd;
+			}
                       else
                         {
                           victim->fd_nextsize = fwd;
                           victim->bk_nextsize = fwd->bk_nextsize;
                           if (__glibc_unlikely (fwd->bk_nextsize->fd_nextsize != fwd))
-                            malloc_printerr ("malloc(): largebin double linked list corrupted (nextsize)");
+                            {
+			      malloc_printerr ("malloc(): largebin double linked list corrupted (nextsize)");
+			    }
                           fwd->bk_nextsize = victim;
                           victim->bk_nextsize->fd_nextsize = victim;
                         }
                       bck = fwd->bk;
                       if (bck->fd != fwd)
-                        malloc_printerr ("malloc(): largebin double linked list corrupted (bk)");
+                        {
+			  malloc_printerr ("malloc(): largebin double linked list corrupted (bk)");
+			}
                     }
                 }
               else
-                victim->fd_nextsize = victim->bk_nextsize = victim;
+                {
+		  victim->fd_nextsize = victim->bk_nextsize = victim;
+		}
             }
 
           mark_bin (av, victim_index);
@@ -4362,7 +4453,9 @@ _int_malloc (mstate av, size_t bytes)
 
 #define MAX_ITERS       10000
           if (++iters >= MAX_ITERS)
-            break;
+            {
+	      break;
+	    }
         }
 
 #if USE_TCACHE
@@ -4390,14 +4483,18 @@ _int_malloc (mstate av, size_t bytes)
               victim = victim->bk_nextsize;
               while (((unsigned long) (size = chunksize (victim)) <
                       (unsigned long) (nb)))
-                victim = victim->bk_nextsize;
+                {
+		  victim = victim->bk_nextsize;
+		}
 
               /* Avoid removing the first entry for a size so that the skip
                  list does not have to be rerouted.  */
               if (victim != last (bin)
 		  && chunksize_nomask (victim)
 		    == chunksize_nomask (victim->fd))
-                victim = victim->fd;
+                {
+		  victim = victim->fd;
+		}
 
               remainder_size = size - nb;
               unlink_chunk (av, victim);
@@ -4407,7 +4504,9 @@ _int_malloc (mstate av, size_t bytes)
                 {
                   set_inuse_bit_at_offset (victim, size);
                   if (av != &main_arena)
-		    set_non_main_arena (victim);
+		    {
+		      set_non_main_arena (victim);
+		    }
                 }
               /* Split */
               else
@@ -4418,7 +4517,9 @@ _int_malloc (mstate av, size_t bytes)
                   bck = unsorted_chunks (av);
                   fwd = bck->fd;
 		  if (__glibc_unlikely (fwd->bk != bck))
-		    malloc_printerr ("malloc(): corrupted unsorted chunks");
+		    {
+		      malloc_printerr ("malloc(): corrupted unsorted chunks");
+		    }
                   remainder->bk = bck;
                   remainder->fd = fwd;
                   bck->fd = remainder;
@@ -4465,7 +4566,9 @@ _int_malloc (mstate av, size_t bytes)
               do
                 {
                   if (++block >= BINMAPSIZE) /* out of bins */
-                    goto use_top;
+                    {
+		      goto use_top;
+		    }
                 }
               while ((map = av->binmap[block]) == 0);
 
@@ -4509,7 +4612,9 @@ _int_malloc (mstate av, size_t bytes)
                 {
                   set_inuse_bit_at_offset (victim, size);
                   if (av != &main_arena)
-		    set_non_main_arena (victim);
+		    {
+		      set_non_main_arena (victim);
+		    }
                 }
 
               /* Split */
@@ -4522,7 +4627,9 @@ _int_malloc (mstate av, size_t bytes)
                   bck = unsorted_chunks (av);
                   fwd = bck->fd;
 		  if (__glibc_unlikely (fwd->bk != bck))
-		    malloc_printerr ("malloc(): corrupted unsorted chunks 2");
+		    {
+		      malloc_printerr ("malloc(): corrupted unsorted chunks 2");
+		    }
                   remainder->bk = bck;
                   remainder->fd = fwd;
                   bck->fd = remainder;
@@ -4530,7 +4637,9 @@ _int_malloc (mstate av, size_t bytes)
 
                   /* advertise as last remainder */
                   if (in_smallbin_range (nb))
-                    av->last_remainder = remainder;
+                    {
+		      av->last_remainder = remainder;
+		    }
                   if (!in_smallbin_range (remainder_size))
                     {
                       remainder->fd_nextsize = NULL;
@@ -4568,7 +4677,9 @@ _int_malloc (mstate av, size_t bytes)
       size = chunksize (victim);
 
       if (__glibc_unlikely (size > av->system_mem))
-        malloc_printerr ("malloc(): corrupted top size");
+        {
+	  malloc_printerr ("malloc(): corrupted top size");
+	}
 
       if ((unsigned long) (size) >= (unsigned long) (nb + MINSIZE))
         {
@@ -4592,9 +4703,13 @@ _int_malloc (mstate av, size_t bytes)
           malloc_consolidate (av);
           /* restore original bin index */
           if (in_smallbin_range (nb))
-            idx = smallbin_index (nb);
+            {
+	      idx = smallbin_index (nb);
+	    }
           else
-            idx = largebin_index (nb);
+            {
+	      idx = largebin_index (nb);
+	    }
         }
 
       /*
@@ -4604,7 +4719,9 @@ _int_malloc (mstate av, size_t bytes)
         {
           void *p = sysmalloc (nb, av);
           if (p != NULL)
-            alloc_perturb (p, bytes);
+            {
+	      alloc_perturb (p, bytes);
+	    }
           return p;
         }
     }
@@ -5590,15 +5707,13 @@ do_set_arena_max (size_t value)
 static __always_inline int
 do_set_tcache_max (size_t value)
 {
+  if (value > PTRDIFF_MAX)
+    return 0;
+
   size_t nb = request2size (value);
   size_t tc_idx = csize2tidx (nb);
 
-  /* To check that value is not too big and request2size does not return an
-     overflown value.  */
-  if (value > nb)
-    return 0;
-
-  if (nb > MAX_TCACHE_SMALL_SIZE)
+  if (tc_idx >= TCACHE_SMALL_BINS)
     tc_idx = large_csize2tidx (nb);
 
   LIBC_PROBE (memory_tunable_tcache_max_bytes, 2, value, mp_.tcache_max_bytes);
@@ -5607,7 +5722,7 @@ do_set_tcache_max (size_t value)
     {
       if (tc_idx < TCACHE_SMALL_BINS)
 	mp_.tcache_small_bins = tc_idx + 1;
-      mp_.tcache_max_bytes = nb;
+      mp_.tcache_max_bytes = nb + 1;
       return 1;
     }

Upstream
Change checked_request2size to return SIZE_MAX for huge inputs.  This
ensures large allocation request stay large and can't be confused with a
small allocation.  As a result several existing checks against PTRDIFF_MAX
become redundant.

Passes regress, OK for commit?

---

diff --git a/malloc/malloc-check.c b/malloc/malloc-check.c
index f5ca5fb41ca03dddc21bf92c78d8a34d1cb496d5..9532316a298c4b6ef779f3ac8ee7692d78d5e1db 100644
--- a/malloc/malloc-check.c
+++ b/malloc/malloc-check.c
@@ -275,12 +275,12 @@ realloc_check (void *oldmem, size_t bytes)
     malloc_printerr ("realloc(): invalid pointer");
   const INTERNAL_SIZE_T oldsize = chunksize (oldp);
 
-  chnb = checked_request2size (rb);
-  if (chnb == 0)
+  if (rb > PTRDIFF_MAX)
     {
       __set_errno (ENOMEM);
       goto invert;
     }
+  chnb = checked_request2size (rb);
 
   __libc_lock_lock (main_arena.mutex);
 
diff --git a/malloc/malloc.c b/malloc/malloc.c
index ee4ea71d7323bf54da708c9af5fb7d2bcc2a33b5..330f4f11e5981c0656d2e94da5e7c9fa6d1f821b 100644
--- a/malloc/malloc.c
+++ b/malloc/malloc.c
@@ -1323,8 +1323,8 @@ nextchunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
 
 /* Check if REQ overflows when padded and aligned and if the resulting
    value is less than PTRDIFF_T.  Returns the requested size or
-   MINSIZE in case the value is less than MINSIZE, or 0 if any of the
-   previous checks fail.  */
+   MINSIZE in case the value is less than MINSIZE, or SIZE_MAX if any
+   of the previous checks fail.  */
 static __always_inline size_t
 checked_request2size (size_t req) __nonnull (1)
 {
@@ -1332,7 +1332,7 @@ checked_request2size (size_t req) __nonnull (1)
                   "PTRDIFF_MAX is not more than half of SIZE_MAX");
 
   if (__glibc_unlikely (req > PTRDIFF_MAX))
-    return 0;
+    return SIZE_MAX;
 
   /* When using tagged memory, we cannot share the end of the user
      block with the header for the next chunk, so ensure that we
@@ -3471,11 +3471,6 @@ __libc_malloc (size_t bytes)
 {
 #if USE_TCACHE
   size_t nb = checked_request2size (bytes);
-  if (nb == 0)
-    {
-      __set_errno (ENOMEM);
-      return NULL;
-    }
 
   if (nb < mp_.tcache_max_bytes)
     {
@@ -3620,12 +3615,12 @@ __libc_realloc (void *oldmem, size_t bytes)
                         || misaligned_chunk (oldp)))
       malloc_printerr ("realloc(): invalid pointer");
 
-  nb = checked_request2size (bytes);
-  if (nb == 0)
+  if (bytes > PTRDIFF_MAX)
     {
       __set_errno (ENOMEM);
       return NULL;
     }
+  nb = checked_request2size (bytes);
 
   if (chunk_is_mmapped (oldp))
     {
@@ -3751,13 +3746,7 @@ _mid_memalign (size_t alignment, size_t bytes)
     }
 
 #if USE_TCACHE
-  size_t nb = checked_request2size (bytes);
-  if (nb == 0)
-    {
-      __set_errno (ENOMEM);
-      return NULL;
-    }
-  void *victim = tcache_get_align (nb, alignment);
+  void *victim = tcache_get_align (checked_request2size (bytes), alignment);
   if (victim != NULL)
     return tag_new_usable (victim);
 #endif
@@ -3918,11 +3907,7 @@ __libc_calloc (size_t n, size_t elem_size)
 
 #if USE_TCACHE
   size_t nb = checked_request2size (bytes);
-  if (nb == 0)
-    {
-      __set_errno (ENOMEM);
-      return NULL;
-    }
+
   if (nb < mp_.tcache_max_bytes)
     {
       if (__glibc_unlikely (tcache == NULL))
@@ -3997,12 +3982,12 @@ _int_malloc (mstate av, size_t bytes)
      aligned.
    */
 
-  nb = checked_request2size (bytes);
-  if (nb == 0)
+  if (bytes > PTRDIFF_MAX)
     {
       __set_errno (ENOMEM);
       return NULL;
     }
+  nb = checked_request2size (bytes);
 
   /* There are no usable arenas.  Fall back to sysmalloc to get a chunk from
      mmap.  */
@@ -5157,12 +5142,12 @@ _int_memalign (mstate av, size_t alignment, size_t bytes)
   unsigned long remainder_size;   /* its size */
   INTERNAL_SIZE_T size;
 
-  nb = checked_request2size (bytes);
-  if (nb == 0)
+  if (bytes > PTRDIFF_MAX)
     {
       __set_errno (ENOMEM);
       return NULL;
     }
+  nb = checked_request2size (bytes);
 
   /* We can't check tcache here because we hold the arena lock, which
      tcache doesn't expect.  We expect it has been checked

Othersize even if tcache_double_free_verify sets e->key to 0 before
calling __libc_free, it gets called again by __libc_free, thus looping
indefinitely.

Fixes: c968fe50628db74b52124d863cd828225a1d305c ("malloc: Use tailcalls in __libc_free")
---
 malloc/malloc.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/malloc/malloc.c b/malloc/malloc.c
index 5ca390cc22..970717eb28 100644
--- a/malloc/malloc.c
+++ b/malloc/malloc.c
@@ -3152,6 +3152,9 @@ tcache_key_initialize (void)
   if (__getrandom_nocancel_nostatus_direct (&tcache_key, sizeof(tcache_key),
 					    GRND_NONBLOCK)
       != sizeof (tcache_key))
+    tcache_key = 0;
+
+  while (tcache_key == 0)
     {
       tcache_key = random_bits ();
 #if __WORDSIZE == 64
-- 
2.47.2

Remove support for obsolete dumped heaps.  Dumping heaps was discontinued
8 years ago, however loading a dumped heap is still supported. This blocks
changes and improvements of the malloc data structures - hence it is time
to remove this.  Ancient binaries that still call malloc_set_state will now
get the -1 error code.  Update tst-mallocstate.c to just check for this.

Passes regress, OK for commit?

---

diff --git a/malloc/malloc-debug.c b/malloc/malloc-debug.c
index 8bcb5652e0d40cab49120dd8b98f4657bcd01f82..0bb57841eebb74cb8097577b2577f18d1be6442a 100644
--- a/malloc/malloc-debug.c
+++ b/malloc/malloc-debug.c
@@ -150,19 +150,6 @@ memalign_hook_ini (size_t alignment, size_t sz, const void *caller)
 
 static size_t pagesize;
 
-/* These variables are used for undumping support.  Chunked are marked
-   as using mmap, but we leave them alone if they fall into this
-   range.  NB: The chunk size for these chunks only includes the
-   initial size field (of SIZE_SZ bytes), there is no trailing size
-   field (unlike with regular mmapped chunks).  */
-static mchunkptr dumped_main_arena_start; /* Inclusive.  */
-static mchunkptr dumped_main_arena_end;   /* Exclusive.  */
-
-/* True if the pointer falls into the dumped arena.  Use this after
-   chunk_is_mmapped indicates a chunk is mmapped.  */
-#define DUMPED_MAIN_ARENA_CHUNK(p) \
-  ((p) >= dumped_main_arena_start && (p) < dumped_main_arena_end)
-
 /* The allocator functions.  */
 
 static void *
@@ -202,9 +189,7 @@ __debug_free (void *mem)
   if (__is_malloc_debug_enabled (MALLOC_MCHECK_HOOK))
     mem = free_mcheck (mem);
 
-  if (DUMPED_MAIN_ARENA_CHUNK (mem2chunk (mem)))
-    /* Do nothing.  */;
-  else if (__is_malloc_debug_enabled (MALLOC_CHECK_HOOK))
+  if (__is_malloc_debug_enabled (MALLOC_CHECK_HOOK))
     free_check (mem);
   else
     __libc_free (mem);
@@ -227,32 +212,7 @@ __debug_realloc (void *oldmem, size_t bytes)
   if ((!__is_malloc_debug_enabled (MALLOC_MCHECK_HOOK)
        || !realloc_mcheck_before (&oldmem, &bytes, &oldsize, &victim)))
     {
-      mchunkptr oldp = mem2chunk (oldmem);
-
-      /* If this is a faked mmapped chunk from the dumped main arena,
-	 always make a copy (and do not free the old chunk).  */
-      if (DUMPED_MAIN_ARENA_CHUNK (oldp))
-	{
-	  if (bytes == 0 && oldmem != NULL)
-	    victim = NULL;
-	  else
-	    {
-	      const INTERNAL_SIZE_T osize = chunksize (oldp);
-	      /* Must alloc, copy, free. */
-	      victim = __debug_malloc (bytes);
-	      /* Copy as many bytes as are available from the old chunk
-		 and fit into the new size.  NB: The overhead for faked
-		 mmapped chunks is only SIZE_SZ, not CHUNK_HDR_SZ as for
-		 regular mmapped chunks.  */
-	      if (victim != NULL)
-		{
-		  if (bytes > osize - SIZE_SZ)
-		    bytes = osize - SIZE_SZ;
-		  memcpy (victim, oldmem, bytes);
-		}
-	    }
-	}
-      else if (__is_malloc_debug_enabled (MALLOC_CHECK_HOOK))
+      if (__is_malloc_debug_enabled (MALLOC_CHECK_HOOK))
 	victim =  realloc_check (oldmem, bytes);
       else
 	victim = __libc_realloc (oldmem, bytes);
@@ -414,10 +374,6 @@ malloc_usable_size (void *mem)
   if (__is_malloc_debug_enabled (MALLOC_CHECK_HOOK))
     return malloc_check_get_size (mem);
 
-  mchunkptr p = mem2chunk (mem);
-  if (DUMPED_MAIN_ARENA_CHUNK (p))
-    return chunksize (p) - SIZE_SZ;
-
   return musable (mem);
 }
 
@@ -517,43 +473,10 @@ malloc_trim (size_t s)
 
 #if SHLIB_COMPAT (libc_malloc_debug, GLIBC_2_0, GLIBC_2_25)
 
-/* Support for restoring dumped heaps contained in historic Emacs
-   executables.  The heap saving feature (malloc_get_state) is no
-   longer implemented in this version of glibc, but we have a heap
-   rewriter in malloc_set_state which transforms the heap into a
-   version compatible with current malloc.  */
-
-#define MALLOC_STATE_MAGIC   0x444c4541l
-#define MALLOC_STATE_VERSION (0 * 0x100l + 5l) /* major*0x100 + minor */
-
-struct malloc_save_state
-{
-  long magic;
-  long version;
-  mbinptr av[NBINS * 2 + 2];
-  char *sbrk_base;
-  int sbrked_mem_bytes;
-  unsigned long trim_threshold;
-  unsigned long top_pad;
-  unsigned int n_mmaps_max;
-  unsigned long mmap_threshold;
-  int check_action;
-  unsigned long max_sbrked_mem;
-  unsigned long max_total_mem;	/* Always 0, for backwards compatibility.  */
-  unsigned int n_mmaps;
-  unsigned int max_n_mmaps;
-  unsigned long mmapped_mem;
-  unsigned long max_mmapped_mem;
-  int using_malloc_checking;
-  unsigned long max_fast;
-  unsigned long arena_test;
-  unsigned long arena_max;
-  unsigned long narenas;
-};
-
-/* Dummy implementation which always fails.  We need to provide this
-   symbol so that existing Emacs binaries continue to work with
-   BIND_NOW.  */
+/* Support for saving/restoring dumped heaps in old GLIBCs is no
+   longer implemented - instead we provide dummy implementations
+   which always fail.  We need to provide these symbol so that
+   existing Emacs binaries continue to work with BIND_NOW.  */
 void *
 malloc_get_state (void)
 {
@@ -566,81 +489,7 @@ compat_symbol (libc_malloc_debug, malloc_get_state, malloc_get_state,
 int
 malloc_set_state (void *msptr)
 {
-  struct malloc_save_state *ms = (struct malloc_save_state *) msptr;
-
-  if (ms->magic != MALLOC_STATE_MAGIC)
-    return -1;
-
-  /* Must fail if the major version is too high. */
-  if ((ms->version & ~0xffl) > (MALLOC_STATE_VERSION & ~0xffl))
-    return -2;
-
-  if (debug_initialized == 1)
-    return -1;
-
-  bool check_was_enabled = __is_malloc_debug_enabled (MALLOC_CHECK_HOOK);
-
-  /* It's not too late, so disable MALLOC_CHECK_ and all of the hooks.  */
-  __malloc_hook = NULL;
-  __realloc_hook = NULL;
-  __free_hook = NULL;
-  __memalign_hook = NULL;
-  __malloc_debug_disable (MALLOC_CHECK_HOOK);
-
-  /* We do not need to perform locking here because malloc_set_state
-     must be called before the first call into the malloc subsystem (usually via
-     __malloc_initialize_hook).  pthread_create always calls calloc and thus
-     must be called only afterwards, so there cannot be more than one thread
-     when we reach this point.  Also handle initialization if either we ended
-     up being called before the first malloc or through the hook when
-     malloc-check was enabled.  */
-  if (debug_initialized < 0)
-    generic_hook_ini ();
-  else if (check_was_enabled)
-    __libc_free (__libc_malloc (0));
-
-  /* Patch the dumped heap.  We no longer try to integrate into the
-     existing heap.  Instead, we mark the existing chunks as mmapped.
-     Together with the update to dumped_main_arena_start and
-     dumped_main_arena_end, realloc and free will recognize these
-     chunks as dumped fake mmapped chunks and never free them.  */
-
-  /* Find the chunk with the lowest address with the heap.  */
-  mchunkptr chunk = NULL;
-  {
-    size_t *candidate = (size_t *) ms->sbrk_base;
-    size_t *end = (size_t *) (ms->sbrk_base + ms->sbrked_mem_bytes);
-    while (candidate < end)
-      if (*candidate != 0)
-	{
-	  chunk = mem2chunk ((void *) (candidate + 1));
-	  break;
-	}
-      else
-	++candidate;
-  }
-  if (chunk == NULL)
-    return 0;
-
-  /* Iterate over the dumped heap and patch the chunks so that they
-     are treated as fake mmapped chunks.  */
-  mchunkptr top = ms->av[2];
-  while (chunk < top)
-    {
-      if (inuse (chunk))
-	{
-	  /* Mark chunk as mmapped, to trigger the fallback path.  */
-	  size_t size = chunksize (chunk);
-	  set_head (chunk, size | IS_MMAPPED);
-	}
-      chunk = next_chunk (chunk);
-    }
-
-  /* The dumped fake mmapped chunks all lie in this address range.  */
-  dumped_main_arena_start = (mchunkptr) ms->sbrk_base;
-  dumped_main_arena_end = top;
-
-  return 0;
+  return -1;
 }
 compat_symbol (libc_malloc_debug, malloc_set_state, malloc_set_state,
 	       GLIBC_2_0);
diff --git a/malloc/tst-mallocstate.c b/malloc/tst-mallocstate.c
index ccfa055ab0fdf5d9e0d2b1085fcf93f71b53c599..5428d20db2bbc9453a497f6ca7d04a9709f23f19 100644
--- a/malloc/tst-mallocstate.c
+++ b/malloc/tst-mallocstate.c
@@ -17,7 +17,6 @@
    <https://www.gnu.org/licenses/>.  */
 
 #include <errno.h>
-#include <stdbool.h>
 #include <stdio.h>
 #include <string.h>
 #include <libc-symbols.h>
@@ -34,36 +33,8 @@ compat_symbol_reference (libc, malloc_get_state, malloc_get_state, GLIBC_2_0);
 int malloc_set_state (void *);
 compat_symbol_reference (libc, malloc_set_state, malloc_set_state, GLIBC_2_0);
 
-/* Maximum object size in the fake heap.  */
-enum { max_size = 64 };
+#define NBINS 128
 
-/* Allocation actions.  These are randomized actions executed on the
-   dumped heap (see allocation_tasks below).  They are interspersed
-   with operations on the new heap (see heap_activity).  */
-enum allocation_action
-  {
-    action_free,                /* Dumped and freed.  */
-    action_realloc,             /* Dumped and realloc'ed.  */
-    action_realloc_same,        /* Dumped and realloc'ed, same size.  */
-    action_realloc_smaller,     /* Dumped and realloc'ed, shrunk.  */
-    action_count
-  };
-
-/* Dumped heap.  Initialize it, so that the object is placed into the
-   .data section, for increased realism.  The size is an upper bound;
-   we use about half of the space.  */
-static size_t dumped_heap[action_count * max_size * max_size
-                          / sizeof (size_t)] = {1};
-
-/* Next free space in the dumped heap.  Also top of the heap at the
-   end of the initialization procedure.  */
-static size_t *next_heap_chunk;
-
-/* Copied from malloc.c and hooks.c.  The version is deliberately
-   lower than the final version of malloc_set_state.  */
-# define NBINS 128
-# define MALLOC_STATE_MAGIC   0x444c4541l
-# define MALLOC_STATE_VERSION (0 * 0x100l + 4l)
 static struct
 {
   long magic;
@@ -87,407 +58,20 @@ static struct
   unsigned long arena_test;
   unsigned long arena_max;
   unsigned long narenas;
-} save_state =
-  {
-    .magic = MALLOC_STATE_MAGIC,
-    .version = MALLOC_STATE_VERSION,
-  };
-
-/* Allocate a blob in the fake heap.  */
-static void *
-dumped_heap_alloc (size_t length)
-{
-  /* malloc needs three state bits in the size field, so the minimum
-     alignment is 8 even on 32-bit architectures.  malloc_set_state
-     should be compatible with such heaps even if it currently
-     provides more alignment to applications.  */
-  enum
-  {
-    heap_alignment = 8,
-    heap_alignment_mask = heap_alignment - 1
-  };
-  _Static_assert (sizeof (size_t) <= heap_alignment,
-                  "size_t compatible with heap alignment");
-
-  /* Need at least this many bytes for metadata and application
-     data. */
-  size_t chunk_size = sizeof (size_t) + length;
-  /* Round up the allocation size to the heap alignment.  */
-  chunk_size += heap_alignment_mask;
-  chunk_size &= ~heap_alignment_mask;
-  TEST_VERIFY_EXIT ((chunk_size & 3) == 0);
-  if (next_heap_chunk == NULL)
-    /* Initialize the top of the heap.  Add one word of zero padding,
-       to match existing practice.  */
-    {
-      dumped_heap[0] = 0;
-      next_heap_chunk = dumped_heap + 1;
-    }
-  else
-    /* The previous chunk is allocated. */
-    chunk_size |= 1;
-  *next_heap_chunk = chunk_size;
-
-  /* User data starts after the chunk header.  */
-  void *result = next_heap_chunk + 1;
-  next_heap_chunk += chunk_size / sizeof (size_t);
-
-  /* Mark the previous chunk as used.   */
-  *next_heap_chunk = 1;
-  return result;
-}
-
-/* Global seed variable for the random number generator.  */
-static unsigned long long global_seed;
-
-/* Simple random number generator.  The numbers are in the range from
-   0 to UINT_MAX (inclusive).  */
-static unsigned int
-rand_next (unsigned long long *seed)
-{
-  /* Linear congruential generated as used for MMIX.  */
-  *seed = *seed * 6364136223846793005ULL + 1442695040888963407ULL;
-  return *seed >> 32;
-}
-
-/* Fill LENGTH bytes at BUFFER with random contents, as determined by
-   SEED.  */
-static void
-randomize_buffer (unsigned char *buffer, size_t length,
-                  unsigned long long seed)
-{
-  for (size_t i = 0; i < length; ++i)
-    buffer[i] = rand_next (&seed);
-}
-
-/* Dumps the buffer to standard output,  in hexadecimal.  */
-static void
-dump_hex (unsigned char *buffer, size_t length)
-{
-  for (int i = 0; i < length; ++i)
-    printf (" %02X", buffer[i]);
-}
-
-/* Set to true if an error is encountered.  */
-static bool errors = false;
-
-/* Keep track of object allocations.  */
-struct allocation
-{
-  unsigned char *data;
-  unsigned int size;
-  unsigned int seed;
-};
-
-/* Check that the allocation task allocation has the expected
-   contents.  */
-static void
-check_allocation (const struct allocation *alloc, int index)
-{
-  size_t size = alloc->size;
-  if (alloc->data == NULL)
-    {
-      printf ("error: NULL pointer for allocation of size %zu at %d, seed %u\n",
-              size, index, alloc->seed);
-      errors = true;
-      return;
-    }
-
-  unsigned char expected[4096];
-  if (size > sizeof (expected))
-    {
-      printf ("error: invalid allocation size %zu at %d, seed %u\n",
-              size, index, alloc->seed);
-      errors = true;
-      return;
-    }
-  randomize_buffer (expected, size, alloc->seed);
-  if (memcmp (alloc->data, expected, size) != 0)
-    {
-      printf ("error: allocation %d data mismatch, size %zu, seed %u\n",
-              index, size, alloc->seed);
-      printf ("  expected:");
-      dump_hex (expected, size);
-      putc ('\n', stdout);
-      printf ("    actual:");
-      dump_hex (alloc->data, size);
-      putc ('\n', stdout);
-      errors = true;
-    }
-}
-
-/* A heap allocation combined with pending actions on it.  */
-struct allocation_task
-{
-  struct allocation allocation;
-  enum allocation_action action;
-};
-
-/* Allocation tasks.  Initialized by init_allocation_tasks and used by
-   perform_allocations.  */
-enum { allocation_task_count = action_count * max_size };
-static struct allocation_task allocation_tasks[allocation_task_count];
-
-/* Fisher-Yates shuffle of allocation_tasks.  */
-static void
-shuffle_allocation_tasks (void)
-{
-  for (int i = 0; i < allocation_task_count - 1; ++i)
-    {
-      /* Pick pair in the tail of the array.  */
-      int j = i + (rand_next (&global_seed)
-                   % ((unsigned) (allocation_task_count - i)));
-      TEST_VERIFY_EXIT (j >= 0 && j < allocation_task_count);
-      /* Exchange. */
-      struct allocation_task tmp = allocation_tasks[i];
-      allocation_tasks[i] = allocation_tasks[j];
-      allocation_tasks[j] = tmp;
-    }
-}
-
-/* Set up the allocation tasks and the dumped heap.  */
-static void
-initial_allocations (void)
-{
-  /* Initialize in a position-dependent way.  */
-  for (int i = 0; i < allocation_task_count; ++i)
-    allocation_tasks[i] = (struct allocation_task)
-      {
-        .allocation =
-          {
-            .size = 1 + (i / action_count),
-            .seed = i,
-          },
-        .action = i % action_count
-      };
-
-  /* Execute the tasks in a random order.  */
-  shuffle_allocation_tasks ();
-
-  /* Initialize the contents of the dumped heap.   */
-  for (int i = 0; i < allocation_task_count; ++i)
-    {
-      struct allocation_task *task = allocation_tasks + i;
-      task->allocation.data = dumped_heap_alloc (task->allocation.size);
-      randomize_buffer (task->allocation.data, task->allocation.size,
-                        task->allocation.seed);
-    }
-
-  for (int i = 0; i < allocation_task_count; ++i)
-    check_allocation (&allocation_tasks[i].allocation, i);
-}
-
-/* Indicates whether init_heap has run.  This variable needs to be
-   volatile because malloc is declared __THROW, which implies it is a
-   leaf function, but we expect it to run our hooks.  */
-static volatile bool heap_initialized;
-
-/* Executed by glibc malloc, through __malloc_initialize_hook
-   below.  */
-static void
-init_heap (void)
-{
-  if (test_verbose)
-    printf ("info: performing heap initialization\n");
-  heap_initialized = true;
-
-  /* Populate the dumped heap.  */
-  initial_allocations ();
-
-  /* Complete initialization of the saved heap data structure.  */
-  save_state.sbrk_base = (void *) dumped_heap;
-  save_state.sbrked_mem_bytes = sizeof (dumped_heap);
-  /* Top pointer.  Adjust so that it points to the start of struct
-     malloc_chunk.  */
-  save_state.av[2] = (void *) (next_heap_chunk - 1);
-
-  /* Integrate the dumped heap into the process heap.  */
-  TEST_VERIFY_EXIT (malloc_set_state (&save_state) == 0);
-}
-
-/* Interpose the initialization callback.  */
-void (*volatile __malloc_initialize_hook) (void) = init_heap;
-compat_symbol_reference (libc, __malloc_initialize_hook,
-                         __malloc_initialize_hook, GLIBC_2_0);
-
-/* Simulate occasional unrelated heap activity in the non-dumped
-   heap.  */
-enum { heap_activity_allocations_count = 32 };
-static struct allocation heap_activity_allocations
-  [heap_activity_allocations_count] = {};
-static int heap_activity_seed_counter = 1000 * 1000;
-
-static void
-heap_activity (void)
-{
-  /* Only do this from time to time.  */
-  if ((rand_next (&global_seed) % 4) == 0)
-    {
-      int slot = rand_next (&global_seed) % heap_activity_allocations_count;
-      struct allocation *alloc = heap_activity_allocations + slot;
-      if (alloc->data == NULL)
-        {
-          alloc->size = rand_next (&global_seed) % (4096U + 1);
-          alloc->data = xmalloc (alloc->size);
-          alloc->seed = heap_activity_seed_counter++;
-          randomize_buffer (alloc->data, alloc->size, alloc->seed);
-          check_allocation (alloc, 1000 + slot);
-        }
-      else
-        {
-          check_allocation (alloc, 1000 + slot);
-          free (alloc->data);
-          alloc->data = NULL;
-        }
-    }
-}
-
-static void
-heap_activity_deallocate (void)
-{
-  for (int i = 0; i < heap_activity_allocations_count; ++i)
-    free (heap_activity_allocations[i].data);
-}
-
-/* Perform a full heap check across the dumped heap allocation tasks,
-   and the simulated heap activity directly above.  */
-static void
-full_heap_check (void)
-{
-  /* Dumped heap.  */
-  for (int i = 0; i < allocation_task_count; ++i)
-    if (allocation_tasks[i].allocation.data != NULL)
-      check_allocation (&allocation_tasks[i].allocation, i);
-
-  /* Heap activity allocations.  */
-  for (int i = 0; i < heap_activity_allocations_count; ++i)
-    if (heap_activity_allocations[i].data != NULL)
-      check_allocation (heap_activity_allocations + i, i);
-}
-
-/* Used as an optimization barrier to force a heap allocation.  */
-__attribute_optimization_barrier__
-static void
-my_free (void *ptr)
-{
-  free (ptr);
-}
+} save_state;
 
 static int
 do_test (void)
 {
-  my_free (malloc (1));
-  TEST_VERIFY_EXIT (heap_initialized);
-
-  /* The first pass performs the randomly generated allocation
-     tasks.  */
-  if (test_verbose)
-    printf ("info: first pass through allocation tasks\n");
-  full_heap_check ();
-
-  /* Execute the post-undump tasks in a random order.  */
-  shuffle_allocation_tasks ();
-
-  for (int i = 0; i < allocation_task_count; ++i)
-    {
-      heap_activity ();
-      struct allocation_task *task = allocation_tasks + i;
-      switch (task->action)
-        {
-        case action_free:
-          check_allocation (&task->allocation, i);
-          free (task->allocation.data);
-          task->allocation.data = NULL;
-          break;
-
-        case action_realloc:
-          check_allocation (&task->allocation, i);
-          task->allocation.data = xrealloc
-            (task->allocation.data, task->allocation.size + max_size);
-          check_allocation (&task->allocation, i);
-          break;
+  /* Check the dummy implementations always fail.  */
+  TEST_VERIFY_EXIT (malloc_set_state (&save_state) == -1);
 
-        case action_realloc_same:
-          check_allocation (&task->allocation, i);
-          task->allocation.data = xrealloc
-            (task->allocation.data, task->allocation.size);
-          check_allocation (&task->allocation, i);
-          break;
-
-        case action_realloc_smaller:
-          check_allocation (&task->allocation, i);
-          size_t new_size = task->allocation.size - 1;
-          task->allocation.data = xrealloc (task->allocation.data, new_size);
-          if (new_size == 0)
-            {
-              if (task->allocation.data != NULL)
-                {
-                  printf ("error: realloc with size zero did not deallocate\n");
-                  errors = true;
-                }
-              /* No further action on this task.  */
-              task->action = action_free;
-            }
-          else
-            {
-              task->allocation.size = new_size;
-              check_allocation (&task->allocation, i);
-            }
-          break;
-
-        case action_count:
-          FAIL_EXIT1 ("task->action should never be action_count");
-        }
-      full_heap_check ();
-    }
-
-  /* The second pass frees the objects which were allocated during the
-     first pass.  */
-  if (test_verbose)
-    printf ("info: second pass through allocation tasks\n");
-
-  shuffle_allocation_tasks ();
-  for (int i = 0; i < allocation_task_count; ++i)
-    {
-      heap_activity ();
-      struct allocation_task *task = allocation_tasks + i;
-      switch (task->action)
-        {
-        case action_free:
-          /* Already freed, nothing to do.  */
-          break;
-
-        case action_realloc:
-        case action_realloc_same:
-        case action_realloc_smaller:
-          check_allocation (&task->allocation, i);
-          free (task->allocation.data);
-          task->allocation.data = NULL;
-          break;
-
-        case action_count:
-          FAIL_EXIT1 ("task->action should never be action_count");
-        }
-      full_heap_check ();
-    }
-
-  heap_activity_deallocate ();
-
-  /* Check that the malloc_get_state stub behaves in the intended
-     way.  */
   errno = 0;
-  if (malloc_get_state () != NULL)
-    {
-      printf ("error: malloc_get_state succeeded\n");
-      errors = true;
-    }
-  if (errno != ENOSYS)
-    {
-      printf ("error: malloc_get_state: %m\n");
-      errors = true;
-    }
+  TEST_VERIFY_EXIT (malloc_get_state () == NULL);
+
+  TEST_VERIFY_EXIT (errno == ENOSYS);
 
-  return errors;
+  return 0;
 }
 
 #include <support/test-driver.c>
