--- glibc-2.41/malloc/malloc.c~	2025-05-03 13:05:31.369719426 +0200
+++ glibc-2.41/malloc/malloc.c	2025-06-05 15:21:01.391815526 +0200
@@ -1620,7 +1620,7 @@ unlink_chunk (mstate av, mchunkptr p)
 
   fd->bk = bk;
   bk->fd = fd;
-  if (!in_smallbin_range (chunksize_nomask (p)) && p->fd_nextsize != NULL)
+  if (!in_smallbin_range (chunksize (p)) && p->fd_nextsize != NULL)
     {
       if (p->fd_nextsize->bk_nextsize != p
 	  || p->bk_nextsize->fd_nextsize != p)
@@ -1813,15 +1813,11 @@ get_max_fast (void)
 
 struct malloc_state
 {
-  /* Serialize access.  */
   __libc_lock_define (, mutex);
-
-  /* Flags (formerly in max_fast).  */
   int flags;
 
-  /* Set if the fastbin chunks contain recently inserted free blocks.  */
-  /* Note this is a bool but not all targets support atomics on booleans.  */
-  int have_fastchunks;
+  /* hot, frequently written â€“ isolate to stop MESI ping-pong */
+  __attribute__((aligned(64))) int have_fastchunks;
 
   /* Fastbins */
   mfastbinptr fastbinsY[NFASTBINS];
@@ -1920,15 +1916,16 @@ static struct malloc_par mp_ =
   .n_mmaps_max = DEFAULT_MMAP_MAX,
   .mmap_threshold = DEFAULT_MMAP_THRESHOLD,
   .trim_threshold = DEFAULT_TRIM_THRESHOLD,
-#define NARENAS_FROM_NCORES(n) ((n) * (sizeof (long) == 4 ? 2 : 8))
+#undef  NARENAS_FROM_NCORES
+#define NARENAS_FROM_NCORES(n)  ((n) * (sizeof (long) == 4 ? 4 : 16))
   .arena_test = NARENAS_FROM_NCORES (1)
-#if USE_TCACHE
+  #if USE_TCACHE
   ,
-  .tcache_count = TCACHE_FILL_COUNT,
-  .tcache_bins = TCACHE_MAX_BINS,
-  .tcache_max_bytes = tidx2usize (TCACHE_MAX_BINS-1),
-  .tcache_unsorted_limit = 0 /* No limit.  */
-#endif
+  .tcache_count          = 32,                  /* up from 7   */
+  .tcache_bins           = TCACHE_MAX_BINS,
+  .tcache_max_bytes      = tidx2usize (TCACHE_MAX_BINS-1),
+  .tcache_unsorted_limit = 24                   /* throttle */
+  #endif
 };
 
 /*
@@ -3120,9 +3117,9 @@ typedef struct tcache_entry
    overall size low is mildly important.  Note that COUNTS and ENTRIES
    are redundant (we could have just counted the linked list each
    time), this is for performance reasons.  */
-typedef struct tcache_perthread_struct
+typedef struct __attribute__((aligned(64))) tcache_perthread_struct
 {
-  uint16_t counts[TCACHE_MAX_BINS];
+  uint16_t      counts[TCACHE_MAX_BINS];
   tcache_entry *entries[TCACHE_MAX_BINS];
 } tcache_perthread_struct;
 
@@ -3902,6 +3899,8 @@ __libc_calloc (size_t n, size_t elem_siz
    ------------------------------ malloc ------------------------------
  */
 
+#define PREFETCH_BIN(ptr)  __builtin_prefetch ((ptr), 0, 1)
+
 static void *
 _int_malloc (mstate av, size_t bytes)
 {
@@ -4129,8 +4128,10 @@ _int_malloc (mstate av, size_t bytes)
     {
       int iters = 0;
       while ((victim = unsorted_chunks (av)->bk) != unsorted_chunks (av))
-        {
-          bck = victim->bk;
+      {
+        if (victim->bk != unsorted_chunks (av))
+          PREFETCH_BIN (victim->bk);
+        bck = victim->bk;
           size = chunksize (victim);
           mchunkptr next = chunk_at_offset (victim, size);
 
