--- glibc-2.41/malloc/malloc.c~	2025-05-03 13:05:31.369719426 +0200
+++ glibc-2.41/malloc/malloc.c	2025-06-05 15:21:01.391815526 +0200
@@ -1326,33 +1326,32 @@ nextchunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-
 
 /* Check if REQ overflows when padded and aligned and if the resulting
    value is less than PTRDIFF_T.  Returns the requested size or
-   MINSIZE in case the value is less than MINSIZE, or 0 if any of the
-   previous checks fail.  */
+   MINSIZE in case the value is less than MINSIZE, or SIZE_MAX if any
+   of the previous checks fail.  */
 static __always_inline size_t
-checked_request2size (size_t req) __nonnull (1)
+checked_request2size (size_t req)
 {
   _Static_assert (PTRDIFF_MAX <= SIZE_MAX / 2,
                   "PTRDIFF_MAX is not more than half of SIZE_MAX");
 
+  /* Reject sizes larger than PTRDIFF_MAX to preserve internal assumptions
+     and avoid wraparound in later computations. */
   if (__glibc_unlikely (req > PTRDIFF_MAX))
-    return 0;
+  {
+    return SIZE_MAX;
+  }
 
-  /* When using tagged memory, we cannot share the end of the user
-     block with the header for the next chunk, so ensure that we
-     allocate blocks that are rounded up to the granule size.  Take
-     care not to overflow from close to MAX_SIZE_T to a small
-     number.  Ideally, this would be part of request2size(), but that
-     must be a macro that produces a compile time constant if passed
-     a constant literal.  */
+  /* When using tagged memory, round the request up to the granule size so
+     that user data and the following chunk header never overlap. The asm
+     barrier prevents GCC from evaluating the code when mtag_enabled is false
+     (workaround for GCC PR 99551). */
   if (__glibc_unlikely (mtag_enabled))
-    {
-      /* Ensure this is not evaluated if !mtag_enabled, see gcc PR 99551.  */
-      asm ("");
-
-      req = (req + (__MTAG_GRANULE_SIZE - 1)) &
-	    ~(size_t)(__MTAG_GRANULE_SIZE - 1);
-    }
+  {
+    asm ("");
+    req = (req + (__MTAG_GRANULE_SIZE - 1)) & ~(size_t) (__MTAG_GRANULE_SIZE - 1);
+  }
 
+  /* Convert request to internal chunk size with alignment/min size enforcement. */
   return request2size (req);
 }
 
@@ -1619,40 +1618,50 @@ static void
 unlink_chunk (mstate av, mchunkptr p)
 {
   if (chunksize (p) != prev_size (next_chunk (p)))
+  {
     malloc_printerr ("corrupted size vs. prev_size");
+  }
 
   mchunkptr fd = p->fd;
   mchunkptr bk = p->bk;
 
   if (__glibc_unlikely (fd->bk != p || bk->fd != p))
+  {
     malloc_printerr ("corrupted double-linked list");
+  }
 
   fd->bk = bk;
   bk->fd = fd;
-  if (!in_smallbin_range (chunksize_nomask (p)) && p->fd_nextsize != NULL)
+
+  /* Use chunksize() here to avoid status bits in chunksize_nomask(). */
+  if (!in_smallbin_range (chunksize (p)) && p->fd_nextsize != NULL)
+  {
+    if (p->fd_nextsize->bk_nextsize != p
+        || p->bk_nextsize->fd_nextsize != p)
     {
-      if (p->fd_nextsize->bk_nextsize != p
-	  || p->bk_nextsize->fd_nextsize != p)
-	malloc_printerr ("corrupted double-linked list (not small)");
+      malloc_printerr ("corrupted double-linked list (not small)");
+    }
 
-      if (fd->fd_nextsize == NULL)
-	{
-	  if (p->fd_nextsize == p)
-	    fd->fd_nextsize = fd->bk_nextsize = fd;
-	  else
-	    {
-	      fd->fd_nextsize = p->fd_nextsize;
-	      fd->bk_nextsize = p->bk_nextsize;
-	      p->fd_nextsize->bk_nextsize = fd;
-	      p->bk_nextsize->fd_nextsize = fd;
-	    }
-	}
+    if (fd->fd_nextsize == NULL)
+    {
+      if (p->fd_nextsize == p)
+      {
+        fd->fd_nextsize = fd->bk_nextsize = fd;
+      }
       else
-	{
-	  p->fd_nextsize->bk_nextsize = p->bk_nextsize;
-	  p->bk_nextsize->fd_nextsize = p->fd_nextsize;
-	}
+      {
+        fd->fd_nextsize = p->fd_nextsize;
+        fd->bk_nextsize = p->bk_nextsize;
+        p->fd_nextsize->bk_nextsize = fd;
+        p->bk_nextsize->fd_nextsize = fd;
+      }
     }
+    else
+    {
+      p->fd_nextsize->bk_nextsize = p->bk_nextsize;
+      p->bk_nextsize->fd_nextsize = p->fd_nextsize;
+    }
+  }
 }
 
 /*
@@ -1874,6 +1883,7 @@ struct malloc_par
   INTERNAL_SIZE_T arena_max;
 
   /* Transparent Large Page support.  */
+  enum malloc_thp_mode_t thp_mode;
   INTERNAL_SIZE_T thp_pagesize;
   /* A value different than 0 means to align mmap allocation to hp_pagesize
      add hp_flags on flags.  */
@@ -1930,7 +1940,8 @@ static struct malloc_par mp_ =
   .mmap_threshold = DEFAULT_MMAP_THRESHOLD,
   .trim_threshold = DEFAULT_TRIM_THRESHOLD,
 #define NARENAS_FROM_NCORES(n) ((n) * (sizeof (long) == 4 ? 2 : 8))
-  .arena_test = NARENAS_FROM_NCORES (1)
+  .arena_test = NARENAS_FROM_NCORES (1),
+  .thp_mode = malloc_thp_mode_not_supported
 #if USE_TCACHE
   ,
   .tcache_count = TCACHE_FILL_COUNT,
@@ -2014,6 +2025,11 @@ static inline void
 madvise_thp (void *p, INTERNAL_SIZE_T size)
 {
 #ifdef MADV_HUGEPAGE
+  /* Only use __madvise if the system is using 'madvise' mode.
+     Otherwise the call is wasteful. */
+  if (mp_.thp_mode != malloc_thp_mode_madvise)
+    return;
+
   /* Do not consider areas smaller than a huge page or if the tunable is
      not active.  */
   if (mp_.thp_pagesize == 0 || size < mp_.thp_pagesize)
@@ -2071,12 +2087,13 @@ static void
 do_check_chunk (mstate av, mchunkptr p)
 {
   unsigned long sz = chunksize (p);
-  /* min and max possible addresses assuming contiguous allocation */
-  char *max_address = (char *) (av->top) + chunksize (av->top);
-  char *min_address = max_address - av->system_mem;
 
   if (!chunk_is_mmapped (p))
     {
+      /* min and max possible addresses assuming contiguous allocation */
+      char *max_address = (char *) (av->top) + chunksize (av->top);
+      char *min_address = max_address - av->system_mem;
+
       /* Has legal address ... */
       if (p != av->top)
         {
@@ -2096,11 +2113,6 @@ do_check_chunk (mstate av, mchunkptr p)
     }
   else
     {
-      /* address is outside main heap  */
-      if (contiguous (av) && av->top != initial_top (av))
-        {
-          assert (((char *) p) < min_address || ((char *) p) >= max_address);
-        }
       /* chunk is page-aligned */
       assert (((prev_size (p) + sz) & (GLRO (dl_pagesize) - 1)) == 0);
       /* mem is aligned */
@@ -2400,83 +2412,31 @@ do_check_malloc_state (mstate av)
 
 /* ----------- Routines dealing with system allocation -------------- */
 
-/*
-   sysmalloc handles malloc cases requiring more memory from the system.
-   On entry, it is assumed that av->top does not have enough
-   space to service request for nb bytes, thus requiring that av->top
-   be extended or replaced.
- */
+/* Allocate a mmap chunk - used for large block sizes or as a fallback.
+   Round up size to nearest page.  Add padding if MALLOC_ALIGNMENT is
+   larger than CHUNK_HDR_SZ.  Add SIZE_SZ at the end since there is no
+   following chunk whose prev_size field could be used.  */
 
 static void *
-sysmalloc_mmap (INTERNAL_SIZE_T nb, size_t pagesize, int extra_flags, mstate av)
+sysmalloc_mmap (INTERNAL_SIZE_T nb, size_t pagesize, int extra_flags)
 {
-  long int size;
-
-  /*
-    Round up size to nearest page.  For mmapped chunks, the overhead is one
-    SIZE_SZ unit larger than for normal chunks, because there is no
-    following chunk whose prev_size field could be used.
-
-    See the front_misalign handling below, for glibc there is no need for
-    further alignments unless we have have high alignment.
-   */
-  if (MALLOC_ALIGNMENT == CHUNK_HDR_SZ)
-    size = ALIGN_UP (nb + SIZE_SZ, pagesize);
-  else
-    size = ALIGN_UP (nb + SIZE_SZ + MALLOC_ALIGN_MASK, pagesize);
-
-  /* Don't try if size wraps around 0.  */
-  if ((unsigned long) (size) <= (unsigned long) (nb))
-    return MAP_FAILED;
+  size_t padding = MALLOC_ALIGNMENT - CHUNK_HDR_SZ;
+  size_t size = ALIGN_UP (nb + padding + SIZE_SZ, pagesize);
 
   char *mm = (char *) MMAP (NULL, size,
 			    mtag_mmap_flags | PROT_READ | PROT_WRITE,
 			    extra_flags);
   if (mm == MAP_FAILED)
     return mm;
-
-#ifdef MAP_HUGETLB
-  if (!(extra_flags & MAP_HUGETLB))
+  if (extra_flags == 0)
     madvise_thp (mm, size);
-#endif
 
   __set_vma_name (mm, size, " glibc: malloc");
 
-  /*
-    The offset to the start of the mmapped region is stored in the prev_size
-    field of the chunk.  This allows us to adjust returned start address to
-    meet alignment requirements here and in memalign(), and still be able to
-    compute proper address argument for later munmap in free() and realloc().
-   */
-
-  INTERNAL_SIZE_T front_misalign; /* unusable bytes at front of new space */
-
-  if (MALLOC_ALIGNMENT == CHUNK_HDR_SZ)
-    {
-      /* For glibc, chunk2mem increases the address by CHUNK_HDR_SZ and
-	 MALLOC_ALIGN_MASK is CHUNK_HDR_SZ-1.  Each mmap'ed area is page
-	 aligned and therefore definitely MALLOC_ALIGN_MASK-aligned.  */
-      assert (((INTERNAL_SIZE_T) chunk2mem (mm) & MALLOC_ALIGN_MASK) == 0);
-      front_misalign = 0;
-    }
-  else
-    front_misalign = (INTERNAL_SIZE_T) chunk2mem (mm) & MALLOC_ALIGN_MASK;
-
-  mchunkptr p;                    /* the allocated/returned chunk */
-
-  if (front_misalign > 0)
-    {
-      ptrdiff_t correction = MALLOC_ALIGNMENT - front_misalign;
-      p = (mchunkptr) (mm + correction);
-      set_prev_size (p, correction);
-      set_head (p, (size - correction) | IS_MMAPPED);
-    }
-  else
-    {
-      p = (mchunkptr) mm;
-      set_prev_size (p, 0);
-      set_head (p, size | IS_MMAPPED);
-    }
+  /* Store offset to start of mmap in prev_size.  */
+  mchunkptr p = (mchunkptr) (mm + padding);
+  set_prev_size (p, padding);
+  set_head (p, (size - padding) | IS_MMAPPED);
 
   /* update statistics */
   int new = atomic_fetch_add_relaxed (&mp_.n_mmaps, 1) + 1;
@@ -2486,7 +2446,7 @@ sysmalloc_mmap (INTERNAL_SIZE_T nb, size
   sum = atomic_fetch_add_relaxed (&mp_.mmapped_mem, size) + size;
   atomic_max (&mp_.max_mmapped_mem, sum);
 
-  check_chunk (av, p);
+  check_chunk (NULL, p);
 
   return chunk2mem (p);
 }
@@ -2522,10 +2482,8 @@ sysmalloc_mmap_fallback (long int *s, IN
   if (mbrk == MAP_FAILED)
     return MAP_FAILED;
 
-#ifdef MAP_HUGETLB
-  if (!(extra_flags & MAP_HUGETLB))
+  if (extra_flags == 0)
     madvise_thp (mbrk, size);
-#endif
 
   __set_vma_name (mbrk, size, " glibc: malloc");
 
@@ -2580,11 +2538,11 @@ sysmalloc (INTERNAL_SIZE_T nb, mstate av
 	{
 	  /* There is no need to issue the THP madvise call if Huge Pages are
 	     used directly.  */
-	  mm = sysmalloc_mmap (nb, mp_.hp_pagesize, mp_.hp_flags, av);
+	  mm = sysmalloc_mmap (nb, mp_.hp_pagesize, mp_.hp_flags);
 	  if (mm != MAP_FAILED)
 	    return mm;
 	}
-      mm = sysmalloc_mmap (nb, pagesize, 0, av);
+      mm = sysmalloc_mmap (nb, pagesize, 0);
       if (mm != MAP_FAILED)
 	return mm;
       tried_mmap = true;
@@ -2668,7 +2626,7 @@ sysmalloc (INTERNAL_SIZE_T nb, mstate av
 	  /* We can at least try to use to mmap memory.  If new_heap fails
 	     it is unlikely that trying to allocate huge pages will
 	     succeed.  */
-	  char *mm = sysmalloc_mmap (nb, pagesize, 0, av);
+	  char *mm = sysmalloc_mmap (nb, pagesize, 0);
 	  if (mm != MAP_FAILED)
 	    return mm;
 	}
@@ -2696,21 +2654,17 @@ sysmalloc (INTERNAL_SIZE_T nb, mstate av
          previous calls. Otherwise, we correct to page-align below.
        */
 
-#ifdef MADV_HUGEPAGE
-      /* Defined in brk.c.  */
-      extern void *__curbrk;
       if (__glibc_unlikely (mp_.thp_pagesize != 0))
 	{
-	  uintptr_t top = ALIGN_UP ((uintptr_t) __curbrk + size,
-				    mp_.thp_pagesize);
-	  size = top - (uintptr_t) __curbrk;
+	  uintptr_t lastbrk = (uintptr_t) MORECORE (0);
+	  uintptr_t top = ALIGN_UP (lastbrk + size, mp_.thp_pagesize);
+	  size = top - lastbrk;
 	}
       else
-#endif
 	size = ALIGN_UP (size, GLRO(dl_pagesize));
 
-     if (size < 512 * 1024)
-	 size = 512 * 1024;
+      if (size < 512 * 1024)
+      size = 512 * 1024;
 
       /*
          Don't try to call MORECORE if argument is so big as to appear
@@ -2984,11 +2938,9 @@ systrim (size_t pad, mstate av)
     return 0;
 
   /* Release in pagesize units and round down to the nearest page.  */
-#ifdef MADV_HUGEPAGE
   if (__glibc_unlikely (mp_.thp_pagesize != 0))
     extra = ALIGN_DOWN (top_area - pad, mp_.thp_pagesize);
   else
-#endif
     extra = ALIGN_DOWN (top_area - pad, GLRO(dl_pagesize));
 
   if (extra == 0)
@@ -3155,28 +3107,34 @@ static void
 tcache_key_initialize (void)
 {
   /* We need to use the _nostatus version here, see BZ 29624.  */
-  if (__getrandom_nocancel_nostatus_direct (&tcache_key, sizeof(tcache_key),
-					    GRND_NONBLOCK)
+  if (__getrandom_nocancel_nostatus_direct (&tcache_key, sizeof (tcache_key),
+                                            GRND_NONBLOCK)
       != sizeof (tcache_key))
+  {
+    /* If we could not obtain full random bytes, start from zero so that
+       the loop below will generate a suitable key using random_bits(). */
     tcache_key = 0;
+  }
 
-  /* We need tcache_key to be non-zero (otherwise tcache_double_free_verify's
-     clearing of e->key would go unnoticed and it would loop getting called
-     through __libc_free), and we want tcache_key not to be a
-     commonly-occurring value in memory, so ensure a minimum amount of one and
-     zero bits.  */
+  /* We need tcache_key to be non-zero, and we want a reasonable density of 1-bits
+     to avoid frequent collisions with common memory patterns. */
   int minimum_bits = __WORDSIZE / 4;
   int maximum_bits = __WORDSIZE - minimum_bits;
 
-  while (labs ((intptr_t) tcache_key) <= 0x1000000
-      || stdc_count_ones (tcache_key) < minimum_bits
-      || stdc_count_ones (tcache_key) > maximum_bits)
-    {
-      tcache_key = random_bits ();
+  /* Avoid UB from labs(LONG_MIN) by using only unsigned and bitwise tests. */
+  while (tcache_key == 0
+         || stdc_count_ones (tcache_key) < minimum_bits
+         || stdc_count_ones (tcache_key) > maximum_bits
+         || (tcache_key & UINT32_C (0x00ffffff)) == tcache_key)
+  {
+    uintptr_t hi = (uintptr_t) random_bits ();
 #if __WORDSIZE == 64
-      tcache_key = (tcache_key << 32) | random_bits ();
+    uintptr_t lo = (uintptr_t) random_bits ();
+    tcache_key = (hi << 32) | lo;
+#else
+    tcache_key = hi;
 #endif
-    }
+  }
 }
 
 static __always_inline size_t
@@ -3271,9 +3229,48 @@ tcache_put_large (mchunkptr chunk, size_
 {
   tcache_entry **entry;
   bool mangled = false;
+
+  /* Locate insertion position in the large-bin tcache list. */
   entry = tcache_location_large (chunksize (chunk), tc_idx, &mangled);
 
-  return tcache_put_n (chunk, tc_idx, entry, mangled);
+  /* Insert the chunk into the determined position. */
+  tcache_put_n (chunk, tc_idx, entry, mangled);
+}
+
+static __always_inline size_t
+large_csize2tidx (size_t nb)
+{
+  /* Guard against undefined behavior in clz on zero. */
+  if (__glibc_unlikely (nb == 0))
+  {
+    return TCACHE_SMALL_BINS;
+  }
+
+#if SIZE_MAX == UINT64_MAX
+  unsigned int lz_ref = __builtin_clzl ((unsigned long) MAX_TCACHE_SMALL_SIZE);
+  unsigned int lz_nb  = __builtin_clzl ((unsigned long) nb);
+#else
+  unsigned int lz_ref = __builtin_clz ((unsigned int) MAX_TCACHE_SMALL_SIZE);
+  unsigned int lz_nb  = __builtin_clz ((unsigned int) nb);
+#endif
+
+  size_t idx = TCACHE_SMALL_BINS + (size_t) (lz_ref - lz_nb);
+
+  /* Clamp to valid range for defense-in-depth. */
+  if (__glibc_unlikely (idx < TCACHE_SMALL_BINS))
+  {
+    idx = TCACHE_SMALL_BINS;
+  }
+  if (__glibc_unlikely (idx >= TCACHE_MAX_BINS))
+  {
+    idx = TCACHE_MAX_BINS - 1;
+  }
+
+#ifndef NDEBUG
+  assert (idx >= TCACHE_SMALL_BINS && idx < TCACHE_MAX_BINS);
+#endif
+
+  return idx;
 }
 
 static __always_inline void *
@@ -3369,27 +3366,31 @@ tcache_thread_shutdown (void)
   tcache_shutting_down = true;
 
   if (!tcache)
+  {
     return;
+  }
 
   /* Disable the tcache and prevent it from being reinitialized.  */
   tcache = NULL;
 
-  /* Free all of the entries and the tcache itself back to the arena
-     heap for coalescing.  */
+  /* Free all of the chunk entries back to the arena heap for coalescing.  */
   for (i = 0; i < TCACHE_MAX_BINS; ++i)
+  {
+    while (tcache_tmp->entries[i])
     {
-      while (tcache_tmp->entries[i])
-	{
-	  tcache_entry *e = tcache_tmp->entries[i];
-	  if (__glibc_unlikely (misaligned_mem (e)))
-	    malloc_printerr ("tcache_thread_shutdown(): "
-			     "unaligned tcache chunk detected");
-	  tcache_tmp->entries[i] = REVEAL_PTR (e->next);
-	  __libc_free (e);
-	}
+      tcache_entry *e = tcache_tmp->entries[i];
+      if (__glibc_unlikely (misaligned_mem (e)))
+      {
+        malloc_printerr ("tcache_thread_shutdown(): "
+                         "unaligned tcache chunk detected");
+      }
+      tcache_tmp->entries[i] = REVEAL_PTR (e->next);
+      __libc_free (e);
     }
+  }
 
-  __libc_free (tcache_tmp);
+  /* Deallocate the tcache struct itself.  It was allocated via mmap.  */
+  __munmap (tcache_tmp, sizeof (tcache_perthread_struct));
 }
 
 /* Initialize tcache.  In the rare case there isn't any memory available,
@@ -3398,22 +3399,33 @@ static void
 tcache_init (void)
 {
   if (tcache_shutting_down)
+  {
     return;
+  }
 
   /* Check minimum mmap chunk is larger than max tcache size.  This means
      mmap chunks with their different layout are never added to tcache.  */
   if (MAX_TCACHE_SMALL_SIZE >= GLRO (dl_pagesize) / 2)
+  {
     malloc_printerr ("max tcache size too large");
+  }
 
   size_t bytes = sizeof (tcache_perthread_struct);
-  tcache = (tcache_perthread_struct *) __libc_malloc2 (bytes);
 
-  if (tcache != NULL)
+  /* Use a direct mmap call to acquire page-aligned memory for the tcache.
+     This avoids recursion into malloc during initialization and is naturally
+     64-byte aligned when combined with the type attribute. */
+  void *mem = MMAP (NULL, bytes, PROT_READ | PROT_WRITE, 0);
+
+  if (mem != MAP_FAILED)
+  {
+    tcache = (tcache_perthread_struct *) mem;
+    /* Mapped memory is zeroed by the kernel. Initialize slot limits. */
+    for (int i = 0; i < TCACHE_MAX_BINS; i++)
     {
-      memset (tcache, 0, bytes);
-      for (int i = 0; i < TCACHE_MAX_BINS; i++)
-	tcache->num_slots[i] = mp_.tcache_count;
+      tcache->num_slots[i] = mp_.tcache_count;
     }
+  }
 }
 
 static void * __attribute_noinline__
@@ -3483,11 +3495,6 @@ __libc_malloc (size_t bytes)
 {
 #if USE_TCACHE
   size_t nb = checked_request2size (bytes);
-  if (nb == 0)
-    {
-      __set_errno (ENOMEM);
-      return NULL;
-    }
 
   if (nb < mp_.tcache_max_bytes)
     {
@@ -3519,57 +3526,74 @@ __libc_free (void *mem)
 {
   mchunkptr p;                          /* chunk corresponding to mem */
 
-  if (mem == NULL)                              /* free(0) has no effect */
+  if (mem == NULL)                      /* free(0) has no effect */
+  {
     return;
+  }
 
-  /* Quickly check that the freed pointer matches the tag for the memory.
-     This gives a useful double-free detection.  */
+  /* Quickly check that the freed pointer matches the tag for the memory. */
   if (__glibc_unlikely (mtag_enabled))
-    *(volatile char *)mem;
+  {
+    *(volatile char *) mem;
+  }
 
   p = mem2chunk (mem);
 
-  /* Mark the chunk as belonging to the library again.  */
+  /* Mark the chunk as belonging to the library again by recoloring the user region. */
   tag_region (chunk2mem (p), memsize (p));
 
   INTERNAL_SIZE_T size = chunksize (p);
 
   if (__glibc_unlikely (misaligned_chunk (p)))
+  {
     return malloc_printerr_tail ("free(): invalid pointer");
-
-  check_inuse_chunk (arena_for_chunk (p), p);
+  }
 
 #if USE_TCACHE
   if (__glibc_likely (size < mp_.tcache_max_bytes && tcache != NULL))
+  {
+    /* Check to see if it's already in the tcache (double-free detection).  */
+    tcache_entry *e = (tcache_entry *) chunk2mem (p);
+
+    /* Check for double free - verify if the key matches.  */
+    if (__glibc_unlikely (e->key == tcache_key))
     {
-      /* Check to see if it's already in the tcache.  */
-      tcache_entry *e = (tcache_entry *) chunk2mem (p);
+      return tcache_double_free_verify (e);
+    }
 
-      /* Check for double free - verify if the key matches.  */
-      if (__glibc_unlikely (e->key == tcache_key))
-        return tcache_double_free_verify (e);
+    size_t tc_idx = csize2tidx (size);
+    if (__glibc_likely (tc_idx < TCACHE_SMALL_BINS))
+    {
+      if (__glibc_likely (tcache->num_slots[tc_idx] != 0))
+      {
+        return tcache_put (p, tc_idx);
+      }
+    }
+    else
+    {
+      tc_idx = large_csize2tidx (size);
 
-      size_t tc_idx = csize2tidx (size);
-      if (__glibc_likely (tc_idx < TCACHE_SMALL_BINS))
-	{
-          if (__glibc_likely (tcache->num_slots[tc_idx] != 0))
-	    return tcache_put (p, tc_idx);
-	}
-      else
-	{
-	  tc_idx = large_csize2tidx (size);
-	  if (size >= MINSIZE
-	      && !chunk_is_mmapped (p)
-              && __glibc_likely (tcache->num_slots[tc_idx] != 0))
-	    return tcache_put_large (p, tc_idx);
-	}
+      /* Defense-in-depth: ensure idx is within table range before indexing. */
+      if (__glibc_likely (tc_idx < TCACHE_MAX_BINS))
+      {
+        if (size >= MINSIZE
+            && !chunk_is_mmapped (p)
+            && __glibc_likely (tcache->num_slots[tc_idx] != 0))
+        {
+          return tcache_put_large (p, tc_idx);
+        }
+      }
+      /* If the idx is out of range (should not happen), fall through to normal free. */
     }
+  }
 #endif
 
-  /* Check size >= MINSIZE and p + size does not overflow.  */
+  /* Check size >= MINSIZE and p + size does not overflow. */
   if (__glibc_unlikely (__builtin_add_overflow_p ((uintptr_t) p, size - MINSIZE,
-						  (uintptr_t) 0)))
+                                                  (uintptr_t) 0)))
+  {
     return malloc_printerr_tail ("free(): invalid size");
+  }
 
   _int_free_chunk (arena_for_chunk (p), p, size, 0);
 }
@@ -3632,12 +3656,12 @@ __libc_realloc (void *oldmem, size_t byt
                         || misaligned_chunk (oldp)))
       malloc_printerr ("realloc(): invalid pointer");
 
-  nb = checked_request2size (bytes);
-  if (nb == 0)
+  if (bytes > PTRDIFF_MAX)
     {
       __set_errno (ENOMEM);
       return NULL;
     }
+  nb = checked_request2size (bytes);
 
   if (chunk_is_mmapped (oldp))
     {
@@ -3763,13 +3787,7 @@ _mid_memalign (size_t alignment, size_t
     }
 
 #if USE_TCACHE
-  size_t nb = checked_request2size (bytes);
-  if (nb == 0)
-    {
-      __set_errno (ENOMEM);
-      return NULL;
-    }
-  void *victim = tcache_get_align (nb, alignment);
+  void *victim = tcache_get_align (checked_request2size (bytes), alignment);
   if (victim != NULL)
     return tag_new_usable (victim);
 #endif
@@ -3930,11 +3948,7 @@ __libc_calloc (size_t n, size_t elem_siz
 
 #if USE_TCACHE
   size_t nb = checked_request2size (bytes);
-  if (nb == 0)
-    {
-      __set_errno (ENOMEM);
-      return NULL;
-    }
+
   if (nb < mp_.tcache_max_bytes)
     {
       if (__glibc_unlikely (tcache == NULL))
@@ -3974,7 +3988,6 @@ __libc_calloc (size_t n, size_t elem_siz
 /*
    ------------------------------ malloc ------------------------------
  */
-
 static void *
 _int_malloc (mstate av, size_t bytes)
 {
@@ -4000,629 +4013,616 @@ _int_malloc (mstate av, size_t bytes)
   size_t tcache_unsorted_count;	    /* count of unsorted chunks processed */
 #endif
 
-  /*
-     Convert request size to internal form by adding SIZE_SZ bytes
-     overhead plus possibly more to obtain necessary alignment and/or
-     to obtain a size of at least MINSIZE, the smallest allocatable
-     size. Also, checked_request2size returns false for request sizes
-     that are so large that they wrap around zero when padded and
-     aligned.
-   */
-
+  /* Validate size; avoid overflow/UB in request2size path. */
+  if (bytes > PTRDIFF_MAX)
+  {
+    __set_errno (ENOMEM);
+    return NULL;
+  }
   nb = checked_request2size (bytes);
-  if (nb == 0)
-    {
-      __set_errno (ENOMEM);
-      return NULL;
-    }
 
-  /* There are no usable arenas.  Fall back to sysmalloc to get a chunk from
-     mmap.  */
+  /* There are no usable arenas.  Fall back to sysmalloc to get a chunk from mmap. */
   if (__glibc_unlikely (av == NULL))
+  {
+    void *p = sysmalloc (nb, av);
+    if (p != NULL)
     {
-      void *p = sysmalloc (nb, av);
-      if (p != NULL)
-	alloc_perturb (p, bytes);
-      return p;
+      alloc_perturb (p, bytes);
     }
+    return p;
+  }
 
-  /*
-     If the size qualifies as a fastbin, first check corresponding bin.
-     This code is safe to execute even if av is not yet initialized, so we
-     can try it without checking, which saves some time on this fast path.
-   */
-
-#define REMOVE_FB(fb, victim, pp)			\
-  do							\
-    {							\
-      victim = pp;					\
-      if (victim == NULL)				\
-	break;						\
-      pp = REVEAL_PTR (victim->fd);                                     \
-      if (__glibc_unlikely (pp != NULL && misaligned_chunk (pp)))       \
-	malloc_printerr ("malloc(): unaligned fastbin chunk detected"); \
-    }							\
-  while ((pp = catomic_compare_and_exchange_val_acq (fb, pp, victim)) \
-	 != victim);					\
+  /* Fastbin path. */
+#define REMOVE_FB(fb, victim, pp)                                           \
+  do                                                                         \
+    {                                                                        \
+      victim = pp;                                                           \
+      if (victim == NULL)                                                    \
+        break;                                                               \
+      pp = REVEAL_PTR (victim->fd);                                          \
+      if (__glibc_unlikely (pp != NULL && misaligned_chunk (pp)))            \
+        malloc_printerr ("malloc(): unaligned fastbin chunk detected");      \
+    }                                                                        \
+  while ((pp = catomic_compare_and_exchange_val_acq (fb, pp, victim))        \
+         != victim)
 
   if ((unsigned long) (nb) <= (unsigned long) (get_max_fast ()))
+  {
+    idx = fastbin_index (nb);
+    mfastbinptr *fb = &fastbin (av, idx);
+    mchunkptr pp;
+    victim = *fb;
+
+    if (victim != NULL)
     {
-      idx = fastbin_index (nb);
-      mfastbinptr *fb = &fastbin (av, idx);
-      mchunkptr pp;
-      victim = *fb;
+      if (__glibc_unlikely (misaligned_chunk (victim)))
+      {
+        malloc_printerr ("malloc(): unaligned fastbin chunk detected 2");
+      }
 
-      if (victim != NULL)
-	{
-	  if (__glibc_unlikely (misaligned_chunk (victim)))
-	    malloc_printerr ("malloc(): unaligned fastbin chunk detected 2");
+      if (SINGLE_THREAD_P)
+      {
+        *fb = REVEAL_PTR (victim->fd);
+      }
+      else
+      {
+        REMOVE_FB (fb, pp, victim);
+      }
 
-	  if (SINGLE_THREAD_P)
-	    *fb = REVEAL_PTR (victim->fd);
-	  else
-	    REMOVE_FB (fb, pp, victim);
-	  if (__glibc_likely (victim != NULL))
-	    {
-	      size_t victim_idx = fastbin_index (chunksize (victim));
-	      if (__glibc_unlikely (victim_idx != idx))
-		malloc_printerr ("malloc(): memory corruption (fast)");
-	      check_remalloced_chunk (av, victim, nb);
+      if (__glibc_likely (victim != NULL))
+      {
+        size_t victim_idx = fastbin_index (chunksize (victim));
+        if (__glibc_unlikely (victim_idx != idx))
+        {
+          malloc_printerr ("malloc(): memory corruption (fast)");
+        }
+        check_remalloced_chunk (av, victim, nb);
 #if USE_TCACHE
-	      /* While we're here, if we see other chunks of the same size,
-		 stash them in the tcache.  */
-	      size_t tc_idx = csize2tidx (nb);
-	      if (tcache != NULL && tc_idx < mp_.tcache_small_bins)
-		{
-		  mchunkptr tc_victim;
-
-		  /* While bin not empty and tcache not full, copy chunks.  */
-		  while (tcache->num_slots[tc_idx] != 0 && (tc_victim = *fb) != NULL)
-		    {
-		      if (__glibc_unlikely (misaligned_chunk (tc_victim)))
-			malloc_printerr ("malloc(): unaligned fastbin chunk detected 3");
-		      size_t victim_tc_idx = csize2tidx (chunksize (tc_victim));
-		      if (__glibc_unlikely (tc_idx != victim_tc_idx))
-			malloc_printerr ("malloc(): chunk size mismatch in fastbin");
-		      if (SINGLE_THREAD_P)
-			*fb = REVEAL_PTR (tc_victim->fd);
-		      else
-			{
-			  REMOVE_FB (fb, pp, tc_victim);
-			  if (__glibc_unlikely (tc_victim == NULL))
-			    break;
-			}
-		      tcache_put (tc_victim, tc_idx);
-		    }
-		}
+        /* While we're here, if we see other chunks of the same size,
+           stash them in the tcache.  */
+        size_t tc_idx = csize2tidx (nb);
+        if (tcache != NULL && tc_idx < mp_.tcache_small_bins)
+        {
+          mchunkptr tc_victim;
+
+          /* While bin not empty and tcache not full, copy chunks.  */
+          while (tcache->num_slots[tc_idx] != 0 && (tc_victim = *fb) != NULL)
+          {
+            if (__glibc_unlikely (misaligned_chunk (tc_victim)))
+            {
+              malloc_printerr ("malloc(): unaligned fastbin chunk detected 3");
+            }
+            size_t victim_tc_idx = csize2tidx (chunksize (tc_victim));
+            if (__glibc_unlikely (tc_idx != victim_tc_idx))
+            {
+              malloc_printerr ("malloc(): chunk size mismatch in fastbin");
+            }
+            if (SINGLE_THREAD_P)
+            {
+              *fb = REVEAL_PTR (tc_victim->fd);
+            }
+            else
+            {
+              REMOVE_FB (fb, pp, tc_victim);
+              if (__glibc_unlikely (tc_victim == NULL))
+              {
+                break;
+              }
+            }
+            tcache_put (tc_victim, tc_idx);
+          }
+        }
 #endif
-	      void *p = chunk2mem (victim);
-	      alloc_perturb (p, bytes);
-	      return p;
-	    }
-	}
+        void *p = chunk2mem (victim);
+        alloc_perturb (p, bytes);
+        return p;
+      }
     }
+  }
 
-  /*
-     If a small request, check regular bin.  Since these "smallbins"
-     hold one size each, no searching within bins is necessary.
-     (For a large request, we need to wait until unsorted chunks are
-     processed to find best fit. But for small ones, fits are exact
-     anyway, so we can check now, which is faster.)
-   */
-
+  /* Smallbin path: exact size. */
   if (in_smallbin_range (nb))
-    {
-      idx = smallbin_index (nb);
-      bin = bin_at (av, idx);
+  {
+    idx = smallbin_index (nb);
+    bin = bin_at (av, idx);
 
-      if ((victim = last (bin)) != bin)
-        {
-          bck = victim->bk;
-	  if (__glibc_unlikely (bck->fd != victim))
-	    malloc_printerr ("malloc(): smallbin double linked list corrupted");
-          set_inuse_bit_at_offset (victim, nb);
-          bin->bk = bck;
-          bck->fd = bin;
+    if ((victim = last (bin)) != bin)
+    {
+      bck = victim->bk;
+      if (__glibc_unlikely (bck->fd != victim))
+      {
+        malloc_printerr ("malloc(): smallbin double linked list corrupted");
+      }
+      set_inuse_bit_at_offset (victim, nb);
+      bin->bk = bck;
+      bck->fd = bin;
 
-          if (av != &main_arena)
-	    set_non_main_arena (victim);
-          check_malloced_chunk (av, victim, nb);
+      if (av != &main_arena)
+      {
+        set_non_main_arena (victim);
+      }
+      check_malloced_chunk (av, victim, nb);
 #if USE_TCACHE
-	  /* While we're here, if we see other chunks of the same size,
-	     stash them in the tcache.  */
-	  size_t tc_idx = csize2tidx (nb);
-	  if (tcache != NULL && tc_idx < mp_.tcache_small_bins)
-	    {
-	      mchunkptr tc_victim;
+      /* While we're here, if we see other chunks of the same size,
+         stash them in the tcache.  */
+      size_t tc_idx = csize2tidx (nb);
+      if (tcache != NULL && tc_idx < mp_.tcache_small_bins)
+      {
+        mchunkptr tc_victim;
 
-	      /* While bin not empty and tcache not full, copy chunks over.  */
-	      while (tcache->num_slots[tc_idx] != 0
-		     && (tc_victim = last (bin)) != bin)
-		{
-		  if (tc_victim != NULL)
-		    {
-		      bck = tc_victim->bk;
-		      set_inuse_bit_at_offset (tc_victim, nb);
-		      if (av != &main_arena)
-			set_non_main_arena (tc_victim);
-		      bin->bk = bck;
-		      bck->fd = bin;
-
-		      tcache_put (tc_victim, tc_idx);
-	            }
-		}
-	    }
-#endif
-          void *p = chunk2mem (victim);
-          alloc_perturb (p, bytes);
-          return p;
+        /* While bin not empty and tcache not full, copy chunks over.  */
+        while (tcache->num_slots[tc_idx] != 0
+               && (tc_victim = last (bin)) != bin)
+        {
+          if (tc_victim != NULL)
+          {
+            bck = tc_victim->bk;
+            set_inuse_bit_at_offset (tc_victim, nb);
+            if (av != &main_arena)
+            {
+              set_non_main_arena (tc_victim);
+            }
+            bin->bk = bck;
+            bck->fd = bin;
+
+            tcache_put (tc_victim, tc_idx);
+          }
         }
+      }
+#endif
+      void *p = chunk2mem (victim);
+      alloc_perturb (p, bytes);
+      return p;
     }
-
-  /*
-     If this is a large request, consolidate fastbins before continuing.
-     While it might look excessive to kill all fastbins before
-     even seeing if there is space available, this avoids
-     fragmentation problems normally associated with fastbins.
-     Also, in practice, programs tend to have runs of either small or
-     large requests, but less often mixtures, so consolidation is not
-     invoked all that often in most programs. And the programs that
-     it is called frequently in otherwise tend to fragment.
-   */
-
+  }
   else
+  {
+    idx = largebin_index (nb);
+    if (atomic_load_relaxed (&av->have_fastchunks))
     {
-      idx = largebin_index (nb);
-      if (atomic_load_relaxed (&av->have_fastchunks))
-        malloc_consolidate (av);
+      malloc_consolidate (av);
     }
-
-  /*
-     Process recently freed or remaindered chunks, taking one only if
-     it is exact fit, or, if this a small request, the chunk is remainder from
-     the most recent non-exact fit.  Place other traversed chunks in
-     bins.  Note that this step is the only place in any routine where
-     chunks are placed in bins.
-
-     The outer loop here is needed because we might not realize until
-     near the end of malloc that we should have consolidated, so must
-     do so and retry. This happens at most once, and only when we would
-     otherwise need to expand memory to service a "small" request.
-   */
+  }
 
 #if USE_TCACHE
   INTERNAL_SIZE_T tcache_nb = 0;
-  size_t tc_idx = csize2tidx (nb);
-  if (tcache != NULL && tc_idx < mp_.tcache_small_bins)
+  size_t tc_idx2 = csize2tidx (nb);
+  if (tcache != NULL && tc_idx2 < mp_.tcache_small_bins)
+  {
     tcache_nb = nb;
+  }
   int return_cached = 0;
 
   tcache_unsorted_count = 0;
 #endif
 
   for (;; )
+  {
+    int iters = 0;
+    while ((victim = unsorted_chunks (av)->bk) != unsorted_chunks (av))
     {
-      int iters = 0;
-      while ((victim = unsorted_chunks (av)->bk) != unsorted_chunks (av))
-        {
-          bck = victim->bk;
-          size = chunksize (victim);
-          mchunkptr next = chunk_at_offset (victim, size);
-
-          if (__glibc_unlikely (size <= CHUNK_HDR_SZ)
-              || __glibc_unlikely (size > av->system_mem))
-            malloc_printerr ("malloc(): invalid size (unsorted)");
-          if (__glibc_unlikely (chunksize_nomask (next) < CHUNK_HDR_SZ)
-              || __glibc_unlikely (chunksize_nomask (next) > av->system_mem))
-            malloc_printerr ("malloc(): invalid next size (unsorted)");
-          if (__glibc_unlikely ((prev_size (next) & ~(SIZE_BITS)) != size))
-            malloc_printerr ("malloc(): mismatching next->prev_size (unsorted)");
-          if (__glibc_unlikely (bck->fd != victim)
-              || __glibc_unlikely (victim->fd != unsorted_chunks (av)))
-            malloc_printerr ("malloc(): unsorted double linked list corrupted");
-          if (__glibc_unlikely (prev_inuse (next)))
-            malloc_printerr ("malloc(): invalid next->prev_inuse (unsorted)");
+      /* Prefetch the next chunk to hide memory latency. */
+      if (victim->bk != unsorted_chunks (av))
+      {
+        __builtin_prefetch (victim->bk, 0, 1);
+      }
 
-          /*
-             If a small request, try to use last remainder if it is the
-             only chunk in unsorted bin.  This helps promote locality for
-             runs of consecutive small requests. This is the only
-             exception to best-fit, and applies only when there is
-             no exact fit for a small chunk.
-           */
+      bck = victim->bk;
+      size = chunksize (victim);
+      mchunkptr next = chunk_at_offset (victim, size);
 
-          if (in_smallbin_range (nb) &&
-              bck == unsorted_chunks (av) &&
-              victim == av->last_remainder &&
-              (unsigned long) (size) > (unsigned long) (nb + MINSIZE))
-            {
-              /* split and reattach remainder */
-              remainder_size = size - nb;
-              remainder = chunk_at_offset (victim, nb);
-              unsorted_chunks (av)->bk = unsorted_chunks (av)->fd = remainder;
-              av->last_remainder = remainder;
-              remainder->bk = remainder->fd = unsorted_chunks (av);
-              if (!in_smallbin_range (remainder_size))
-                {
-                  remainder->fd_nextsize = NULL;
-                  remainder->bk_nextsize = NULL;
-                }
+      if (__glibc_unlikely (size <= CHUNK_HDR_SZ)
+          || __glibc_unlikely (size > av->system_mem))
+      {
+        malloc_printerr ("malloc(): invalid size (unsorted)");
+      }
+      if (__glibc_unlikely (chunksize_nomask (next) < CHUNK_HDR_SZ)
+          || __glibc_unlikely (chunksize_nomask (next) > av->system_mem))
+      {
+        malloc_printerr ("malloc(): invalid next size (unsorted)");
+      }
+      if (__glibc_unlikely ((prev_size (next) & ~(SIZE_BITS)) != size))
+      {
+        malloc_printerr ("malloc(): mismatching next->prev_size (unsorted)");
+      }
+      if (__glibc_unlikely (bck->fd != victim)
+          || __glibc_unlikely (victim->fd != unsorted_chunks (av)))
+      {
+        malloc_printerr ("malloc(): unsorted double linked list corrupted");
+      }
+      if (__glibc_unlikely (prev_inuse (next)))
+      {
+        malloc_printerr ("malloc(): invalid next->prev_inuse (unsorted)");
+      }
 
-              set_head (victim, nb | PREV_INUSE |
-                        (av != &main_arena ? NON_MAIN_ARENA : 0));
-              set_head (remainder, remainder_size | PREV_INUSE);
-              set_foot (remainder, remainder_size);
-
-              check_malloced_chunk (av, victim, nb);
-              void *p = chunk2mem (victim);
-              alloc_perturb (p, bytes);
-              return p;
-            }
+      /* Prefer last remainder for small exact-ish fits. */
+      if (in_smallbin_range (nb) &&
+          bck == unsorted_chunks (av) &&
+          victim == av->last_remainder &&
+          (unsigned long) (size) > (unsigned long) (nb + MINSIZE))
+      {
+        /* split and reattach remainder */
+        remainder_size = size - nb;
+        remainder = chunk_at_offset (victim, nb);
+        unsorted_chunks (av)->bk = unsorted_chunks (av)->fd = remainder;
+        av->last_remainder = remainder;
+        remainder->bk = remainder->fd = unsorted_chunks (av);
+        if (!in_smallbin_range (remainder_size))
+        {
+          remainder->fd_nextsize = NULL;
+          remainder->bk_nextsize = NULL;
+        }
 
-          /* remove from unsorted list */
-          unsorted_chunks (av)->bk = bck;
-          bck->fd = unsorted_chunks (av);
+        set_head (victim, nb | PREV_INUSE |
+                  (av != &main_arena ? NON_MAIN_ARENA : 0));
+        set_head (remainder, remainder_size | PREV_INUSE);
+        set_foot (remainder, remainder_size);
+
+        check_malloced_chunk (av, victim, nb);
+        void *p = chunk2mem (victim);
+        alloc_perturb (p, bytes);
+        return p;
+      }
 
-          /* Take now instead of binning if exact fit */
+      /* remove from unsorted list */
+      unsorted_chunks (av)->bk = bck;
+      bck->fd = unsorted_chunks (av);
 
-          if (size == nb)
-            {
-              set_inuse_bit_at_offset (victim, size);
-              if (av != &main_arena)
-		set_non_main_arena (victim);
+      /* Take now instead of binning if exact fit */
+      if (size == nb)
+      {
+        set_inuse_bit_at_offset (victim, size);
+        if (av != &main_arena)
+        {
+          set_non_main_arena (victim);
+        }
 #if USE_TCACHE
-	      /* Fill cache first, return to user only if cache fills.
-		 We may return one of these chunks later.  */
-	      if (tcache_nb > 0
-		  && tcache->num_slots[tc_idx] != 0)
-		{
-		  tcache_put (victim, tc_idx);
-		  return_cached = 1;
-		  continue;
-		}
-	      else
-		{
+        /* Fill cache first, return to user only if cache fills. */
+        if (tcache_nb > 0
+            && tcache->num_slots[tc_idx2] != 0)
+        {
+          tcache_put (victim, tc_idx2);
+          return_cached = 1;
+          continue;
+        }
+        else
+        {
 #endif
-              check_malloced_chunk (av, victim, nb);
-              void *p = chunk2mem (victim);
-              alloc_perturb (p, bytes);
-              return p;
+          check_malloced_chunk (av, victim, nb);
+          void *p = chunk2mem (victim);
+          alloc_perturb (p, bytes);
+          return p;
 #if USE_TCACHE
-		}
+        }
 #endif
-            }
+      }
+
+      /* Place chunk in bin. Only malloc_consolidate/splitting put small chunks here. */
+      if (__glibc_unlikely (in_smallbin_range (size)))
+      {
+        victim_index = smallbin_index (size);
+        bck = bin_at (av, victim_index);
+        fwd = bck->fd;
+      }
+      else
+      {
+        victim_index = largebin_index (size);
+        bck = bin_at (av, victim_index);
+        fwd = bck->fd;
 
-          /* Place chunk in bin.  Only malloc_consolidate() and splitting can put
-             small chunks into the unsorted bin. */
-          if (__glibc_unlikely (in_smallbin_range (size)))
+        /* Maintain large bins in sorted order. */
+        if (fwd != bck)
+        {
+          /* Use a local cmp_size (size | PREV_INUSE) to speed comparisons
+             without altering 'size' used later. */
+          INTERNAL_SIZE_T cmp_size = size | PREV_INUSE;
+
+          /* If smaller than smallest, bypass loop below. */
+          assert (chunk_main_arena (bck->bk));
+          if ((unsigned long) cmp_size
+              < (unsigned long) chunksize_nomask (bck->bk))
+          {
+            fwd = bck;
+            bck = bck->bk;
+
+            if (__glibc_unlikely (fwd->fd->bk_nextsize->fd_nextsize != fwd->fd))
             {
-              victim_index = smallbin_index (size);
-              bck = bin_at (av, victim_index);
-              fwd = bck->fd;
+              malloc_printerr ("malloc(): largebin double linked list corrupted (nextsize)");
             }
+
+            victim->fd_nextsize = fwd->fd;
+            victim->bk_nextsize = fwd->fd->bk_nextsize;
+            fwd->fd->bk_nextsize = victim->bk_nextsize->fd_nextsize = victim;
+          }
           else
+          {
+            assert (chunk_main_arena (fwd));
+            while ((unsigned long) cmp_size < chunksize_nomask (fwd))
             {
-              victim_index = largebin_index (size);
-              bck = bin_at (av, victim_index);
-              fwd = bck->fd;
-
-              /* maintain large bins in sorted order */
-              if (fwd != bck)
-                {
-                  /* Or with inuse bit to speed comparisons */
-                  size |= PREV_INUSE;
-                  /* if smaller than smallest, bypass loop below */
-                  assert (chunk_main_arena (bck->bk));
-                  if ((unsigned long) (size)
-		      < (unsigned long) chunksize_nomask (bck->bk))
-                    {
-                      fwd = bck;
-                      bck = bck->bk;
-
-                      if (__glibc_unlikely (fwd->fd->bk_nextsize->fd_nextsize != fwd->fd))
-                        malloc_printerr ("malloc(): largebin double linked list corrupted (nextsize)");
-
-                      victim->fd_nextsize = fwd->fd;
-                      victim->bk_nextsize = fwd->fd->bk_nextsize;
-                      fwd->fd->bk_nextsize = victim->bk_nextsize->fd_nextsize = victim;
-                    }
-                  else
-                    {
-                      assert (chunk_main_arena (fwd));
-                      while ((unsigned long) size < chunksize_nomask (fwd))
-                        {
-                          fwd = fwd->fd_nextsize;
-			  assert (chunk_main_arena (fwd));
-                        }
+              fwd = fwd->fd_nextsize;
+              assert (chunk_main_arena (fwd));
+            }
 
-                      if ((unsigned long) size
-			  == (unsigned long) chunksize_nomask (fwd))
-                        /* Always insert in the second position.  */
-                        fwd = fwd->fd;
-                      else
-                        {
-                          victim->fd_nextsize = fwd;
-                          victim->bk_nextsize = fwd->bk_nextsize;
-                          if (__glibc_unlikely (fwd->bk_nextsize->fd_nextsize != fwd))
-                            malloc_printerr ("malloc(): largebin double linked list corrupted (nextsize)");
-                          fwd->bk_nextsize = victim;
-                          victim->bk_nextsize->fd_nextsize = victim;
-                        }
-                      bck = fwd->bk;
-                      if (bck->fd != fwd)
-                        malloc_printerr ("malloc(): largebin double linked list corrupted (bk)");
-                    }
-                }
-              else
-                victim->fd_nextsize = victim->bk_nextsize = victim;
+            if ((unsigned long) cmp_size
+                == (unsigned long) chunksize_nomask (fwd))
+            {
+              /* Always insert in the second position.  */
+              fwd = fwd->fd;
+            }
+            else
+            {
+              victim->fd_nextsize = fwd;
+              victim->bk_nextsize = fwd->bk_nextsize;
+              if (__glibc_unlikely (fwd->bk_nextsize->fd_nextsize != fwd))
+              {
+                malloc_printerr ("malloc(): largebin double linked list corrupted (nextsize)");
+              }
+              fwd->bk_nextsize = victim;
+              victim->bk_nextsize->fd_nextsize = victim;
+            }
+            bck = fwd->bk;
+            if (bck->fd != fwd)
+            {
+              malloc_printerr ("malloc(): largebin double linked list corrupted (bk)");
             }
+          }
+        }
+        else
+        {
+          victim->fd_nextsize = victim->bk_nextsize = victim;
+        }
+      }
 
-          mark_bin (av, victim_index);
-          victim->bk = bck;
-          victim->fd = fwd;
-          fwd->bk = victim;
-          bck->fd = victim;
+      mark_bin (av, victim_index);
+      victim->bk = bck;
+      victim->fd = fwd;
+      fwd->bk = victim;
+      bck->fd = victim;
 
 #if USE_TCACHE
       /* If we've processed as many chunks as we're allowed while
-	 filling the cache, return one of the cached ones.  */
+         filling the cache, return one of the cached ones.  */
       ++tcache_unsorted_count;
       if (return_cached
-	  && mp_.tcache_unsorted_limit > 0
-	  && tcache_unsorted_count > mp_.tcache_unsorted_limit)
-	{
-	  return tcache_get (tc_idx);
-	}
+          && mp_.tcache_unsorted_limit > 0
+          && tcache_unsorted_count > mp_.tcache_unsorted_limit)
+      {
+        return tcache_get (tc_idx2);
+      }
 #endif
 
-#define MAX_ITERS       10000
-          if (++iters >= MAX_ITERS)
-            break;
-        }
+#define MAX_ITERS 10000
+      if (++iters >= MAX_ITERS)
+      {
+        break;
+      }
+    }
 
 #if USE_TCACHE
-      /* If all the small chunks we found ended up cached, return one now.  */
-      if (return_cached)
-	{
-	  return tcache_get (tc_idx);
-	}
+    /* If all the small chunks we found ended up cached, return one now.  */
+    if (return_cached)
+    {
+      return tcache_get (tc_idx2);
+    }
 #endif
 
-      /*
-         If a large request, scan through the chunks of current bin in
-         sorted order to find smallest that fits.  Use the skip list for this.
-       */
+    /* Large request: scan through the chunks of current bin in sorted order. */
+    if (!in_smallbin_range (nb))
+    {
+      bin = bin_at (av, idx);
 
-      if (!in_smallbin_range (nb))
+      /* skip scan if empty or largest chunk is too small */
+      if ((victim = first (bin)) != bin
+          && (unsigned long) chunksize_nomask (victim) >= (unsigned long) (nb))
+      {
+        victim = victim->bk_nextsize;
+        while (((unsigned long) (size = chunksize (victim)) < (unsigned long) (nb)))
         {
-          bin = bin_at (av, idx);
-
-          /* skip scan if empty or largest chunk is too small */
-          if ((victim = first (bin)) != bin
-	      && (unsigned long) chunksize_nomask (victim)
-	        >= (unsigned long) (nb))
-            {
-              victim = victim->bk_nextsize;
-              while (((unsigned long) (size = chunksize (victim)) <
-                      (unsigned long) (nb)))
-                victim = victim->bk_nextsize;
-
-              /* Avoid removing the first entry for a size so that the skip
-                 list does not have to be rerouted.  */
-              if (victim != last (bin)
-		  && chunksize_nomask (victim)
-		    == chunksize_nomask (victim->fd))
-                victim = victim->fd;
-
-              remainder_size = size - nb;
-              unlink_chunk (av, victim);
-
-              /* Exhaust */
-              if (remainder_size < MINSIZE)
-                {
-                  set_inuse_bit_at_offset (victim, size);
-                  if (av != &main_arena)
-		    set_non_main_arena (victim);
-                }
-              /* Split */
-              else
-                {
-                  remainder = chunk_at_offset (victim, nb);
-                  /* We cannot assume the unsorted list is empty and therefore
-                     have to perform a complete insert here.  */
-                  bck = unsorted_chunks (av);
-                  fwd = bck->fd;
-		  if (__glibc_unlikely (fwd->bk != bck))
-		    malloc_printerr ("malloc(): corrupted unsorted chunks");
-                  remainder->bk = bck;
-                  remainder->fd = fwd;
-                  bck->fd = remainder;
-                  fwd->bk = remainder;
-                  if (!in_smallbin_range (remainder_size))
-                    {
-                      remainder->fd_nextsize = NULL;
-                      remainder->bk_nextsize = NULL;
-                    }
-                  set_head (victim, nb | PREV_INUSE |
-                            (av != &main_arena ? NON_MAIN_ARENA : 0));
-                  set_head (remainder, remainder_size | PREV_INUSE);
-                  set_foot (remainder, remainder_size);
-                }
-              check_malloced_chunk (av, victim, nb);
-              void *p = chunk2mem (victim);
-              alloc_perturb (p, bytes);
-              return p;
-            }
+          victim = victim->bk_nextsize;
         }
 
-      /*
-         Search for a chunk by scanning bins, starting with next largest
-         bin. This search is strictly by best-fit; i.e., the smallest
-         (with ties going to approximately the least recently used) chunk
-         that fits is selected.
-
-         The bitmap avoids needing to check that most blocks are nonempty.
-         The particular case of skipping all bins during warm-up phases
-         when no chunks have been returned yet is faster than it might look.
-       */
+        /* Avoid removing the first entry for a size so that the skip
+           list does not have to be rerouted.  */
+        if (victim != last (bin)
+            && chunksize_nomask (victim) == chunksize_nomask (victim->fd))
+        {
+          victim = victim->fd;
+        }
 
-      ++idx;
-      bin = bin_at (av, idx);
-      block = idx2block (idx);
-      map = av->binmap[block];
-      bit = idx2bit (idx);
+        remainder_size = size - nb;
+        unlink_chunk (av, victim);
 
-      for (;; )
+        /* Exhaust */
+        if (remainder_size < MINSIZE)
         {
-          /* Skip rest of block if there are no more set bits in this block.  */
-          if (bit > map || bit == 0)
-            {
-              do
-                {
-                  if (++block >= BINMAPSIZE) /* out of bins */
-                    goto use_top;
-                }
-              while ((map = av->binmap[block]) == 0);
-
-              bin = bin_at (av, (block << BINMAPSHIFT));
-              bit = 1;
-            }
+          set_inuse_bit_at_offset (victim, size);
+          if (av != &main_arena)
+          {
+            set_non_main_arena (victim);
+          }
+        }
+        /* Split */
+        else
+        {
+          remainder = chunk_at_offset (victim, nb);
+          /* Insert remainder into unsorted list. */
+          bck = unsorted_chunks (av);
+          fwd = bck->fd;
+          if (__glibc_unlikely (fwd->bk != bck))
+          {
+            malloc_printerr ("malloc(): corrupted unsorted chunks");
+          }
+          remainder->bk = bck;
+          remainder->fd = fwd;
+          bck->fd = remainder;
+          fwd->bk = remainder;
+          if (!in_smallbin_range (remainder_size))
+          {
+            remainder->fd_nextsize = NULL;
+            remainder->bk_nextsize = NULL;
+          }
+          set_head (victim, nb | PREV_INUSE |
+                    (av != &main_arena ? NON_MAIN_ARENA : 0));
+          set_head (remainder, remainder_size | PREV_INUSE);
+          set_foot (remainder, remainder_size);
+        }
+        check_malloced_chunk (av, victim, nb);
+        void *p = chunk2mem (victim);
+        alloc_perturb (p, bytes);
+        return p;
+      }
+    }
 
-          /* Advance to bin with set bit. There must be one. */
-          while ((bit & map) == 0)
-            {
-              bin = next_bin (bin);
-              bit <<= 1;
-              assert (bit != 0);
-            }
+    /* Search for a chunk by scanning bins, starting with next largest bin. */
+    ++idx;
+    bin = bin_at (av, idx);
+    block = idx2block (idx);
+    map = av->binmap[block];
+    bit = idx2bit (idx);
 
-          /* Inspect the bin. It is likely to be non-empty */
-          victim = last (bin);
+    for (;; )
+    {
+      /* Skip rest of block if there are no more set bits in this block.  */
+      if (bit > map || bit == 0)
+      {
+        do
+        {
+          if (++block >= BINMAPSIZE) /* out of bins */
+          {
+            goto use_top;
+          }
+        }
+        while ((map = av->binmap[block]) == 0);
 
-          /*  If a false alarm (empty bin), clear the bit. */
-          if (victim == bin)
-            {
-              av->binmap[block] = map &= ~bit; /* Write through */
-              bin = next_bin (bin);
-              bit <<= 1;
-            }
+        bin = bin_at (av, (block << BINMAPSHIFT));
+        bit = 1;
+      }
 
-          else
-            {
-              size = chunksize (victim);
+      /* Advance to bin with set bit. */
+      while ((bit & map) == 0)
+      {
+        bin = next_bin (bin);
+        bit <<= 1;
+        assert (bit != 0);
+      }
 
-              /*  We know the first chunk in this bin is big enough to use. */
-              assert ((unsigned long) (size) >= (unsigned long) (nb));
+      /* Inspect the bin.  */
+      victim = last (bin);
 
-              remainder_size = size - nb;
+      /* If a false alarm (empty bin), clear the bit. */
+      if (victim == bin)
+      {
+        av->binmap[block] = map &= ~bit; /* Write through */
+        bin = next_bin (bin);
+        bit <<= 1;
+      }
+      else
+      {
+        size = chunksize (victim);
 
-              /* unlink */
-              unlink_chunk (av, victim);
+        /* We know the first chunk in this bin is big enough to use. */
+        assert ((unsigned long) (size) >= (unsigned long) (nb));
 
-              /* Exhaust */
-              if (remainder_size < MINSIZE)
-                {
-                  set_inuse_bit_at_offset (victim, size);
-                  if (av != &main_arena)
-		    set_non_main_arena (victim);
-                }
+        remainder_size = size - nb;
 
-              /* Split */
-              else
-                {
-                  remainder = chunk_at_offset (victim, nb);
+        /* unlink */
+        unlink_chunk (av, victim);
 
-                  /* We cannot assume the unsorted list is empty and therefore
-                     have to perform a complete insert here.  */
-                  bck = unsorted_chunks (av);
-                  fwd = bck->fd;
-		  if (__glibc_unlikely (fwd->bk != bck))
-		    malloc_printerr ("malloc(): corrupted unsorted chunks 2");
-                  remainder->bk = bck;
-                  remainder->fd = fwd;
-                  bck->fd = remainder;
-                  fwd->bk = remainder;
-
-                  /* advertise as last remainder */
-                  if (in_smallbin_range (nb))
-                    av->last_remainder = remainder;
-                  if (!in_smallbin_range (remainder_size))
-                    {
-                      remainder->fd_nextsize = NULL;
-                      remainder->bk_nextsize = NULL;
-                    }
-                  set_head (victim, nb | PREV_INUSE |
-                            (av != &main_arena ? NON_MAIN_ARENA : 0));
-                  set_head (remainder, remainder_size | PREV_INUSE);
-                  set_foot (remainder, remainder_size);
-                }
-              check_malloced_chunk (av, victim, nb);
-              void *p = chunk2mem (victim);
-              alloc_perturb (p, bytes);
-              return p;
-            }
+        /* Exhaust */
+        if (remainder_size < MINSIZE)
+        {
+          set_inuse_bit_at_offset (victim, size);
+          if (av != &main_arena)
+          {
+            set_non_main_arena (victim);
+          }
         }
-
-    use_top:
-      /*
-         If large enough, split off the chunk bordering the end of memory
-         (held in av->top). Note that this is in accord with the best-fit
-         search rule.  In effect, av->top is treated as larger (and thus
-         less well fitting) than any other available chunk since it can
-         be extended to be as large as necessary (up to system
-         limitations).
-
-         We require that av->top always exists (i.e., has size >=
-         MINSIZE) after initialization, so if it would otherwise be
-         exhausted by current request, it is replenished. (The main
-         reason for ensuring it exists is that we may need MINSIZE space
-         to put in fenceposts in sysmalloc.)
-       */
-
-      victim = av->top;
-      size = chunksize (victim);
-
-      if (__glibc_unlikely (size > av->system_mem))
-        malloc_printerr ("malloc(): corrupted top size");
-
-      if ((unsigned long) (size) >= (unsigned long) (nb + MINSIZE))
+        /* Split */
+        else
         {
-          remainder_size = size - nb;
           remainder = chunk_at_offset (victim, nb);
-          av->top = remainder;
+
+          /* Insert remainder into unsorted list. */
+          bck = unsorted_chunks (av);
+          fwd = bck->fd;
+          if (__glibc_unlikely (fwd->bk != bck))
+          {
+            malloc_printerr ("malloc(): corrupted unsorted chunks 2");
+          }
+          remainder->bk = bck;
+          remainder->fd = fwd;
+          bck->fd = remainder;
+          fwd->bk = remainder;
+
+          /* advertise as last remainder */
+          if (in_smallbin_range (nb))
+          {
+            av->last_remainder = remainder;
+          }
+          if (!in_smallbin_range (remainder_size))
+          {
+            remainder->fd_nextsize = NULL;
+            remainder->bk_nextsize = NULL;
+          }
           set_head (victim, nb | PREV_INUSE |
                     (av != &main_arena ? NON_MAIN_ARENA : 0));
           set_head (remainder, remainder_size | PREV_INUSE);
-
-          check_malloced_chunk (av, victim, nb);
-          void *p = chunk2mem (victim);
-          alloc_perturb (p, bytes);
-          return p;
+          set_foot (remainder, remainder_size);
         }
+        check_malloced_chunk (av, victim, nb);
+        void *p = chunk2mem (victim);
+        alloc_perturb (p, bytes);
+        return p;
+      }
+    }
 
-      /* When we are using atomic ops to free fast chunks we can get
-         here for all block sizes.  */
-      else if (atomic_load_relaxed (&av->have_fastchunks))
-        {
-          malloc_consolidate (av);
-          /* restore original bin index */
-          if (in_smallbin_range (nb))
-            idx = smallbin_index (nb);
-          else
-            idx = largebin_index (nb);
-        }
+use_top:
+    /* Use/split the top chunk if large enough, or consolidate/expand. */
+    victim = av->top;
+    size = chunksize (victim);
 
-      /*
-         Otherwise, relay to handle system-dependent cases
-       */
+    if (__glibc_unlikely (size > av->system_mem))
+    {
+      malloc_printerr ("malloc(): corrupted top size");
+    }
+
+    if ((unsigned long) (size) >= (unsigned long) (nb + MINSIZE))
+    {
+      remainder_size = size - nb;
+      remainder = chunk_at_offset (victim, nb);
+      av->top = remainder;
+      set_head (victim, nb | PREV_INUSE |
+                (av != &main_arena ? NON_MAIN_ARENA : 0));
+      set_head (remainder, remainder_size | PREV_INUSE);
+
+      check_malloced_chunk (av, victim, nb);
+      void *p = chunk2mem (victim);
+      alloc_perturb (p, bytes);
+      return p;
+    }
+    else if (atomic_load_relaxed (&av->have_fastchunks))
+    {
+      malloc_consolidate (av);
+      /* restore original bin index */
+      if (in_smallbin_range (nb))
+      {
+        idx = smallbin_index (nb);
+      }
       else
-        {
-          void *p = sysmalloc (nb, av);
-          if (p != NULL)
-            alloc_perturb (p, bytes);
-          return p;
-        }
+      {
+        idx = largebin_index (nb);
+      }
     }
+    else
+    {
+      void *p = sysmalloc (nb, av);
+      if (p != NULL)
+      {
+        alloc_perturb (p, bytes);
+      }
+      return p;
+    }
+  }
 }
 
 /*
@@ -4771,6 +4771,8 @@ _int_free_merge_chunk (mstate av, mchunk
 {
   mchunkptr nextchunk = chunk_at_offset(p, size);
 
+  check_inuse_chunk (av, p);
+
   /* Lightweight tests: check whether the block is already the
      top block.  */
   if (__glibc_unlikely (p == av->top))
@@ -5169,12 +5171,12 @@ _int_memalign (mstate av, size_t alignme
   unsigned long remainder_size;   /* its size */
   INTERNAL_SIZE_T size;
 
-  nb = checked_request2size (bytes);
-  if (nb == 0)
+  if (bytes > PTRDIFF_MAX)
     {
       __set_errno (ENOMEM);
       return NULL;
     }
+  nb = checked_request2size (bytes);
 
   /* We can't check tcache here because we hold the arena lock, which
      tcache doesn't expect.  We expect it has been checked
@@ -5606,23 +5608,30 @@ static __always_inline int
 do_set_tcache_max (size_t value)
 {
   if (value > PTRDIFF_MAX)
+  {
     return 0;
+  }
 
   size_t nb = request2size (value);
   size_t tc_idx = csize2tidx (nb);
 
   if (tc_idx >= TCACHE_SMALL_BINS)
+  {
     tc_idx = large_csize2tidx (nb);
+  }
 
   LIBC_PROBE (memory_tunable_tcache_max_bytes, 2, value, mp_.tcache_max_bytes);
 
   if (tc_idx < TCACHE_MAX_BINS)
+  {
+    if (tc_idx < TCACHE_SMALL_BINS)
     {
-      if (tc_idx < TCACHE_SMALL_BINS)
-	mp_.tcache_small_bins = tc_idx + 1;
-      mp_.tcache_max_bytes = nb + 1;
-      return 1;
+      mp_.tcache_small_bins = tc_idx + 1;
     }
+    /* Use nb + 1 like the current tree to keep exact behavior. */
+    mp_.tcache_max_bytes = nb + 1;
+    return 1;
+  }
 
   return 0;
 }
@@ -5665,12 +5674,9 @@ do_set_hugetlb (size_t value)
 {
   if (value == 1)
     {
-      enum malloc_thp_mode_t thp_mode = __malloc_thp_mode ();
-      /*
-	 Only enable THP madvise usage if system does support it and
-	 has 'madvise' mode.  Otherwise the madvise() call is wasteful.
-       */
-      if (thp_mode == malloc_thp_mode_madvise)
+      mp_.thp_mode = __malloc_thp_mode ();
+      if (mp_.thp_mode == malloc_thp_mode_madvise
+          || mp_.thp_mode == malloc_thp_mode_always)
 	mp_.thp_pagesize = __malloc_default_thp_pagesize ();
     }
   else if (value >= 2)
