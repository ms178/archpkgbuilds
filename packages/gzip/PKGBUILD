pkgname=gzip
pkgver=1.13
pkgrel=5.3 # Incremented pkgrel for new PGO workload
pkgdesc='GNU compression utility'
arch=('x86_64')
url='https://www.gnu.org/software/gzip/'
license=('GPL-3.0-or-later')
depends=('glibc' 'bash' 'coreutils' 'sed' 'grep')
# Added PGO toolchain depends + helpers for workload
makedepends=('clang' 'llvm' 'lld' 'coreutils' 'tar')
optdepends=('less: zless support'
            'util-linux: zmore support'
            'diffutils: zdiff/zcmp support'
           )
validpgpkeys=('155D3FC500C834486D1EEA677FD9FCCB000BEEEE') # Jim Meyering
source=("https://ftp.gnu.org/pub/gnu/gzip/gzip-$pkgver.tar.xz"{,.sig})
sha256sums=('7454eb6935db17c6655576c2e1b0fabefd38b4d0936e0f87f48cd062ce91a057'
            'SKIP')
options=(strip)

# --- PGO Workload Functions ---

# Function to generate diverse training files reflecting target workloads
generate_training_data() {
  local training_base_dir="$1"
  local training_dir="${training_base_dir}/training" # Actual files go here
  local MB_SCALE=1048576 # 1 MiB

  # Only generate if the directory doesn't exist (to reuse between stages)
  if [ -d "${training_dir}" ]; then
    echo "Training data already exists in ${training_dir}, skipping generation."
    return 0
  fi

  echo "Generating comprehensive training data in ${training_dir}..."
  mkdir -p "${training_dir}"

  # --- Compilation Workload Data ---
  echo "--> Generating Compilation Data..."
  mkdir -p "${training_dir}/compilation/projectA/src" "${training_dir}/compilation/projectA/include" "${training_dir}/compilation/projectB"
  # Simple C/C++ files
  for i in {1..50}; do
    cat <<EOF > "${training_dir}/compilation/projectA/src/module_${i}.c"
#include "header_${i}.h"
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

// Function simulating some work
int process_data_${i}(const char* input) {
    if (!input) return -1;
    // printf("Processing module ${i}: %s\\n", input); // Reduce verbosity
    // Simulate allocation and string operations
    char* buffer = malloc(256);
    if (!buffer) return -1;
    strncpy(buffer, input, 255);
    buffer[255] = '\\0';
    // Some dummy calculations
    int result = 0;
    for(int j=0; j<strlen(buffer); ++j) result += buffer[j];
    free(buffer);
    return result % 100;
}

#ifndef HEADER_${i}_H
#define HEADER_${i}_H
struct DataStruct_${i} { int id; char name[64]; };
int process_data_${i}(const char* input);
#endif
EOF
    cp "${training_dir}/compilation/projectA/src/module_${i}.c" "${training_dir}/compilation/projectA/include/header_${i}.h" # Use same content for header for simplicity
  done
  # Larger source file
  cat $(find "${training_dir}/compilation/projectA/" -type f) > "${training_dir}/compilation/projectB/large_source_file.c"
  # Build Logs (simulated)
  for i in {1..5}; do
    { for j in {1..5000}; do echo "[$(date +%T.%N)] INFO: Compiling object $j for target $i..."; done;
      echo "WARNING: Linker symbol collision detected in build $i";
      for j in {1..1000}; do echo "DEBUG: Variable value $j = $((RANDOM % 1000))"; done;
      echo "ERROR: Build $i failed with exit code 1";
    } > "${training_dir}/compilation/build_log_${i}.log"
  done

  # --- Gaming Workload Data (Simulated) ---
  echo "--> Generating Gaming Data..."
  mkdir -p "${training_dir}/gaming/assets/textures" "${training_dir}/gaming/assets/models" "${training_dir}/gaming/config" "${training_dir}/gaming/scripts"
  # Textures (binary, some random, some slightly patterned)
  dd if=/dev/urandom of="${training_dir}/gaming/assets/textures/noise_1.bin" bs=1M count=10 status=none
  dd if=/dev/zero bs=1k count=1 | tr '\0' '\123\456\701' | dd bs=1M count=5 of="${training_dir}/gaming/assets/textures/pattern_1.bin" status=none
  # Models (binary)
  dd if=/dev/urandom of="${training_dir}/gaming/assets/models/character_mesh.bin" bs=1M count=8 status=none
  # Config files (text, xml/json like)
  for i in {1..20}; do
    cat <<EOF > "${training_dir}/gaming/config/settings_${i}.xml"
<settings game="Game ${i}">
  <resolution width="1920" height="1080"/>
  <graphics quality="high" vsync="true"/>
  <audio volume="0.8" device="default"/>
  <controls sensitivity="0.5">
    <keybind action="jump" key="SPACE"/>
    <keybind action="move_forward" key="W"/>
  </controls>
  <!-- Repetitive section -->
  $(yes '<data value="placeholder"/>' | head -n 500)
</settings>
EOF
  done
  # Scripts (text, lua-like)
  for i in {1..30}; do
    cat <<EOF > "${training_dir}/gaming/scripts/ai_behavior_${i}.lua"
function update_ai_${i}(entity, delta_time)
  local state = entity:get_state()
  if state == "idle" then
    entity:wander(delta_time * 5.0)
  elseif state == "attacking" then
    entity:attack_target(entity:get_target())
  end
  -- Log debugging info periodically
  -- if math.random() < 0.01 then log_debug("AI ${i} state: " .. state) end -- Reduce verbosity
end
-- Constant data table
local ai_params_${i} = { speed = 10.5, health = 100, damage = 15 }
EOF
  done

  # --- General Desktop Data ---
  echo "--> Generating Desktop Data..."
  mkdir -p "${training_dir}/desktop/documents" "${training_dir}/desktop/misc"
  # Documents (text, some repetitive)
  yes "This is a line in a long document that repeats itself for testing compression. " | head -c 5M > "${training_dir}/desktop/documents/report.txt"
  seq 1 500000 > "${training_dir}/desktop/documents/numbers.csv"
  # Misc binary data
  dd if=/dev/urandom of="${training_dir}/desktop/misc/archive_data.bin" bs=1M count=15 status=none
  # Highly compressible
  printf "%0.sX" $(seq 1 10000000) > "${training_dir}/desktop/misc/zeros_x.txt"

  # --- Pre-compressed Data ---
  echo "--> Generating Pre-compressed Data..."
  gzip -f -9 "${training_dir}/desktop/documents/report.txt" -c > "${training_dir}/desktop/documents/report.txt.gz" 2>/dev/null
  gzip -f -1 "${training_dir}/compilation/build_log_1.log" -c > "${training_dir}/compilation/build_log_1.log.gz" 2>/dev/null

  echo "Training data generation complete in ${training_dir}."
}
export -f generate_training_data

# Function to run comprehensive gzip workload
run_gzip_workload() {
  local gzip_binary="$1"
  local training_data_dir="$2" # Root of the generated /training dir
  local tmp_work_dir="$3" # Base temporary directory for this run

  echo "Running comprehensive gzip workload using: ${gzip_binary}"
  echo "Training data source: ${training_data_dir}"
  echo "Temporary work dir: ${tmp_work_dir}"

  chmod +x "${gzip_binary}" || { echo "Error: Cannot make ${gzip_binary} executable."; return 1; }
  mkdir -p "${tmp_work_dir}"

  local tmp_compress_dir="${tmp_work_dir}/compressed"
  local tmp_decompress_dir="${tmp_work_dir}/decompressed"
  mkdir -p "${tmp_compress_dir}" "${tmp_decompress_dir}"

  # --- Compression Workloads ---
  msg "--> Workload: Compressing individual files (various levels)..."
  # Level 1 (Fast) - Focus on different data types
  find "${training_data_dir}" \( -path "*/compilation/*" -o -path "*/gaming/scripts/*" -o -path "*/gaming/config/*" \) -type f -print0 | \
    xargs -0 -P$(nproc) -I{} sh -c \
    '"$1" -k -f -1 "$2" -c > "$3/$(basename "$2").l1.gz" 2>/dev/null' _ "$gzip_binary" "{}" "$tmp_compress_dir"
  # Level 6 (Default) - Wider range of files
  find "${training_data_dir}" \( -path "*/desktop/*" -o -path "*/gaming/assets/*" \) -type f -print0 | \
    xargs -0 -P$(nproc) -I{} sh -c \
    '"$1" -k -f -6 "$2" -c > "$3/$(basename "$2").l6.gz" 2>/dev/null' _ "$gzip_binary" "{}" "$tmp_compress_dir"
  # Level 9 (Best) - Focus on compressible & binary data
  find "${training_data_dir}" \( -path "*/desktop/misc/*" -o -path "*/compilation/build_log_*.log" -o -path "*/gaming/assets/textures/*" \) -type f -print0 | \
    xargs -0 -P$(nproc) -I{} sh -c \
    '"$1" -k -f -9 "$2" -c > "$3/$(basename "$2").l9.gz" 2>/dev/null' _ "$gzip_binary" "{}" "$tmp_compress_dir"

  msg "--> Workload: Simulating tar | gzip..."
  # Compress compilation project structure
  tar cf - -C "${training_data_dir}/compilation" projectA projectB | "$gzip_binary" -1 > "${tmp_compress_dir}/compilation_archive.tar.gz" 2>/dev/null
  # Compress gaming assets
  tar cf - -C "${training_data_dir}/gaming" assets | "$gzip_binary" -6 > "${tmp_compress_dir}/gaming_assets.tar.gz" 2>/dev/null

  msg "--> Workload: Simulating cat | gzip..."
  cat "${training_data_dir}/desktop/documents/report.txt" | "$gzip_binary" > "${tmp_compress_dir}/report_piped.txt.gz" 2>/dev/null
  cat "${training_data_dir}/desktop/misc/archive_data.bin" | "$gzip_binary" > "${tmp_compress_dir}/archive_data_piped.bin.gz" 2>/dev/null


  # --- Decompression Workloads ---
  msg "--> Workload: Decompressing individual files..."
  find "${tmp_compress_dir}" -maxdepth 1 -name '*.gz' -type f -print0 | \
    xargs -0 -P$(nproc) -I{} sh -c \
    '"$1" -d -f "$2" -c > "$3/$(basename "$2" .gz).decomp" 2>/dev/null' _ "$gzip_binary" "{}" "$tmp_decompress_dir" || true # Allow errors if some files are corrupted/empty

  msg "--> Workload: Simulating gzip -dc | tar xf..."
  "$gzip_binary" -dc "${tmp_compress_dir}/compilation_archive.tar.gz" 2>/dev/null | tar xf - -C "$tmp_decompress_dir" || true
  "$gzip_binary" -dc "${tmp_compress_dir}/gaming_assets.tar.gz" 2>/dev/null | tar xf - -C "$tmp_decompress_dir" || true

  msg "--> Workload: Simulating gzip -dc > file..."
  "$gzip_binary" -dc "${tmp_compress_dir}/report_piped.txt.gz" > "${tmp_decompress_dir}/report_piped.txt" 2>/dev/null || true
  "$gzip_binary" -dc "${tmp_compress_dir}/archive_data_piped.bin.gz" > "${tmp_decompress_dir}/archive_data_piped.bin" 2>/dev/null || true


  # --- Testing & Listing Workloads ---
  msg "--> Workload: Testing (-t) and Listing (-l) files..."
  find "${tmp_compress_dir}" -maxdepth 1 -name '*.gz' -type f -print0 | \
    xargs -0 -P$(nproc) -I{} sh -c \
    ' ("$1" -t "$2" >/dev/null 2>&1 && "$1" -l "$2" >/dev/null 2>&1) || true ' _ "$gzip_binary" "{}" # Allow test/list to fail without stopping workload

  echo "Comprehensive gzip workload finished."
  # Cleanup the temporary directories for this run
  rm -rf "${tmp_compress_dir}" "${tmp_decompress_dir}"
}
export -f run_gzip_workload

# --- Standard PKGBUILD Functions ---

prepare() {
  cd "$srcdir/$pkgname-$pkgver"

  # --- PGO/CS-PGO Toolchain Setup ---
  export CC=clang
  export CXX=clang++
  export CC_LD=lld
  export CXX_LD=lld
  export LDFLAGS+=" -fuse-ld=lld"
  export AR=llvm-ar
  export NM=llvm-nm
  export STRIP=llvm-strip
  export OBJCOPY=llvm-objcopy
  export OBJDUMP=llvm-objdump
  export READELF=llvm-readelf
  export RANLIB=llvm-ranlib

  # Run autoreconf once before any configure stage
  autoreconf -vfi
}

build() {
  # Setup cleanup trap for temporary directories
  trap 'rm -rf "$srcdir/pgo" "$srcdir/cspgo" "$srcdir/pgo_training_data" "$srcdir/pgo_workload_tmp_stage1" "$srcdir/cspgo_workload_tmp_stage2"' EXIT

  # Create directory for PGO profiles
  mkdir -p "$srcdir/pgo"

  # Create directory for CS-PGO profiles
  mkdir -p "$srcdir/cspgo"

  # Directory for training data (generated once if possible)
  local _pgo_training_data_dir="$srcdir/pgo_training_data"

  # Common additional flags for both instrumentation phases
  local _common_instrument_flags=" -g3 -fno-omit-frame-pointer -Xclang -mllvm -Xclang -vp-counters-per-site=50 -Xclang -mllvm -Xclang -runtime-counter-relocation -Xclang -mllvm -Xclang -enable-value-profiling"

  # Save original flags
  local _original_cflags="$CFLAGS"
  local _original_cxxflags="$CXXFLAGS"
  local _original_ldflags="$LDFLAGS"

  # Base configure options
  local _configure_options=(
    --prefix=/usr
  )

  # Change directory once
  cd "$srcdir/$pkgname-$pkgver"

  # Generate training data (will only run fully the first time)
  generate_training_data "$_pgo_training_data_dir"

  # --- Stage 1: Build with PGO instrumentation ---
  msg "Starting PGO Instrumentation Build (Stage 1/3)..."
  export CFLAGS="$_original_cflags -fprofile-generate=$srcdir/pgo $_common_instrument_flags"
  export CXXFLAGS="$_original_cxxflags -fprofile-generate=$srcdir/pgo $_common_instrument_flags"
  export LDFLAGS="$_original_ldflags"

  ./configure "${_configure_options[@]}"
  make -j$(nproc)

  # Run workload to generate PGO profile data
  msg "Running comprehensive workload to generate PGO profile data (Stage 1/3)..."
  local pgo_workload_tmp_stage1="$srcdir/pgo_workload_tmp_stage1" # Unique temp dir name
  mkdir -p "$pgo_workload_tmp_stage1"
  export LLVM_PROFILE_FILE="$srcdir/pgo/%p-%m.profraw"
  # Run workload using the instrumented binary in the build dir
  run_gzip_workload "./gzip" "${_pgo_training_data_dir}/training" "$pgo_workload_tmp_stage1"
  unset LLVM_PROFILE_FILE
  # rm -rf "$pgo_workload_tmp_stage1" # Cleanup handled by trap or at end

  # Merge PGO profiles
  msg "Merging PGO profiles (Stage 1/3)..."
  llvm-profdata merge -output="$srcdir/pgo/default.profdata" "$srcdir"/pgo/*.profraw

  # Clean up build directory for the next stage
  make clean

  # --- Stage 2: Build with CS-PGO instrumentation ---
  msg "Starting CS-PGO Instrumentation Build (Stage 2/3)..."
  export CFLAGS="$_original_cflags -fprofile-use=$srcdir/pgo/default.profdata -fcs-profile-generate=$srcdir/cspgo $_common_instrument_flags"
  export CXXFLAGS="$_original_cxxflags -fprofile-use=$srcdir/pgo/default.profdata -fcs-profile-generate=$srcdir/cspgo $_common_instrument_flags"
  export LDFLAGS="$_original_ldflags"

  ./configure "${_configure_options[@]}"
  make -j$(nproc)

  # Run workload to generate CS-PGO profile data
  msg "Running comprehensive workload to generate CS-PGO profile data (Stage 2/3)..."
  local cspgo_workload_tmp_stage2="$srcdir/cspgo_workload_tmp_stage2" # Unique temp dir name
  mkdir -p "$cspgo_workload_tmp_stage2"
  export LLVM_PROFILE_FILE="$srcdir/cspgo/%p-%m.profraw"
  # Run workload using the instrumented binary
  run_gzip_workload "./gzip" "${_pgo_training_data_dir}/training" "$cspgo_workload_tmp_stage2"
  unset LLVM_PROFILE_FILE
  # rm -rf "$cspgo_workload_tmp_stage2" # Cleanup handled by trap or at end

  # Merge CS-PGO profiles with PGO profiles
  msg "Merging CS-PGO profiles with PGO profiles (Stage 2/3)..."
  llvm-profdata merge -output="$srcdir/cspgo/cs.profdata" "$srcdir/cspgo"/*.profraw "$srcdir/pgo/default.profdata"

  # Clean up build directory for the final stage
  make clean

  # --- Stage 3: Build with merged CS-PGO data ---
  msg "Starting Final Optimized Build (Stage 3/3)..."
  export CFLAGS="$_original_cflags -fprofile-use=$srcdir/cspgo/cs.profdata"
  export CXXFLAGS="$_original_cxxflags -fprofile-use=$srcdir/cspgo/cs.profdata"
  export LDFLAGS="$_original_ldflags"

  ./configure "${_configure_options[@]}"
  make -j$(nproc)

  # Unset exported flags after build is done (good practice)
  unset CFLAGS CXXFLAGS LDFLAGS

  msg "PGO/CS-PGO Build Process Complete."
  # Clean up intermediate profile directories and data now that build is successful
  rm -rf "$srcdir/pgo" "$srcdir/cspgo" "$srcdir/pgo_training_data" "$srcdir/pgo_workload_tmp_stage1" "$srcdir/cspgo_workload_tmp_stage2"
}

check() {
  cd "$srcdir/$pkgname-$pkgver"
  msg "Running tests (make check) on the final optimized build..."
  # Ensure check runs against final build binaries
  make check
}

package() {
  cd "$srcdir/$pkgname-$pkgver"
  # Uses the final build from Stage 3
  make prefix="$pkgdir/usr" install
}

# vim:set ts=2 sw=2 et:
