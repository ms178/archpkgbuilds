--- a/arch/x86/adler32_avx2_p.h	2025-11-27 21:41:21.162188845 +0100
+++ b/arch/x86/adler32_avx2_p.h	2026-01-23 21:41:52.969024948 +0100
@@ -1,4 +1,4 @@
-/* adler32_avx2_p.h -- adler32 avx2 utility functions
+/* adler32_avx2_p.h -- Adler-32 AVX2 utility functions
  * Copyright (C) 2022 Adam Stylinski
  * For conditions of distribution and use, see copyright notice in zlib.h
  */
@@ -8,6 +8,9 @@
 
 #if defined(X86_AVX2) || defined(X86_AVX512VNNI)
 
+#include <immintrin.h>
+#include <stdint.h>
+
 /* 32 bit horizontal sum, adapted from Agner Fog's vector library. */
 static inline uint32_t hsum256(__m256i x) {
     __m128i sum1  = _mm_add_epi32(_mm256_extracti128_si256(x, 1),
@@ -17,16 +20,17 @@ static inline uint32_t hsum256(__m256i x
     return (uint32_t)_mm_cvtsi128_si32(sum3);
 }
 
+/* Horizontal sum of SAD results in YMM register. */
 static inline uint32_t partial_hsum256(__m256i x) {
-    /* We need a permutation vector to extract every other integer. The
-     * rest are going to be zeros */
     const __m256i perm_vec = _mm256_setr_epi32(0, 2, 4, 6, 1, 1, 1, 1);
     __m256i non_zero = _mm256_permutevar8x32_epi32(x, perm_vec);
     __m128i non_zero_sse = _mm256_castsi256_si128(non_zero);
-    __m128i sum2  = _mm_add_epi32(non_zero_sse,_mm_unpackhi_epi64(non_zero_sse, non_zero_sse));
+    __m128i sum2  = _mm_add_epi32(non_zero_sse,
+                                  _mm_unpackhi_epi64(non_zero_sse, non_zero_sse));
     __m128i sum3  = _mm_add_epi32(sum2, _mm_shuffle_epi32(sum2, 1));
     return (uint32_t)_mm_cvtsi128_si32(sum3);
 }
+
 #endif
 
 #endif

--- a/arch/x86/adler32_avx2.c	2025-11-27 20:19:53.224919411 +0100
+++ a/arch/x86/adler32_avx2.c	2025-11-27 21:43:06.966053929 +0100
@@ -11,169 +11,144 @@
 
 #include "zbuild.h"
 #include <immintrin.h>
+#include <stdint.h>
 #include "adler32_p.h"
 #include "adler32_avx2_p.h"
 #include "x86_intrins.h"
 
-extern uint32_t adler32_fold_copy_sse42(uint32_t adler, uint8_t *dst, const uint8_t *src, size_t len);
-extern uint32_t adler32_ssse3(uint32_t adler, const uint8_t *src, size_t len);
+#if defined(__STDC_VERSION__) && __STDC_VERSION__ >= 201112L
+_Static_assert(NMAX >= 64, "NMAX must accommodate at least one AVX2 block");
+_Static_assert(NMAX < 5553, "NMAX must prevent 32-bit overflow in s2");
+_Static_assert(BASE == 65521, "BASE must be largest prime below 2^16");
+#endif
 
-static inline uint32_t adler32_fold_copy_impl(uint32_t adler, uint8_t *dst, const uint8_t *src, size_t len, const int COPY) {
-    if (src == NULL) return 1L;
-    if (len == 0) return adler;
-
-    uint32_t adler0, adler1;
-    adler1 = (adler >> 16) & 0xffff;
-    adler0 = adler & 0xffff;
-
-rem_peel:
-    if (len < 16) {
-        if (COPY) {
-            return adler32_copy_len_16(adler0, src, dst, len, adler1);
-        } else {
-            return adler32_len_16(adler0, src, len, adler1);
-        }
-    } else if (len < 32) {
-        if (COPY) {
-            return adler32_fold_copy_sse42(adler, dst, src, len);
-        } else {
-            return adler32_ssse3(adler, src, len);
-        }
-    }
-
-    __m256i vs1, vs2, vs2_0;
+extern uint32_t adler32_fold_copy_sse42(uint32_t adler, uint8_t *dst,
+                                        const uint8_t *src, size_t len);
+extern uint32_t adler32_ssse3(uint32_t adler, const uint8_t *src, size_t len);
 
-    const __m256i dot2v = _mm256_setr_epi8(64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47,
-                                           46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33);
-    const __m256i dot2v_0 = _mm256_setr_epi8(32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15,
-                                             14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1);
+static inline uint32_t adler32_fold_copy_impl(uint32_t adler, uint8_t *dst,
+                                              const uint8_t *src, size_t len,
+                                              const int COPY) {
+    if (src == NULL)
+        return 1U;
+    if (len == 0)
+        return adler;
+    if (COPY && dst == NULL)
+        return 1U;
+
+    uint32_t adler0 = adler & 0xFFFFU;
+    uint32_t adler1 = (adler >> 16) & 0xFFFFU;
+
+    const __m256i dot2v = _mm256_setr_epi8(
+        64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49,
+        48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33);
+    const __m256i dot2v_0 = _mm256_setr_epi8(
+        32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17,
+        16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1);
     const __m256i dot3v = _mm256_set1_epi16(1);
     const __m256i zero = _mm256_setzero_si256();
 
     while (len >= 32) {
-        vs1 = _mm256_zextsi128_si256(_mm_cvtsi32_si128(adler0));
-        vs2 = _mm256_zextsi128_si256(_mm_cvtsi32_si128(adler1));
-        __m256i vs1_0 = vs1;
-        __m256i vs3 = _mm256_setzero_si256();
-        vs2_0 = vs3;
+        __m256i vs1, vs2, vs1_0, vs2_0, vs3;
+
+        vs1 = _mm256_zextsi128_si256(_mm_cvtsi32_si128((int)adler0));
+        vs2 = _mm256_zextsi128_si256(_mm_cvtsi32_si128((int)adler1));
+        vs1_0 = vs1;
+        vs3 = _mm256_setzero_si256();
+        vs2_0 = _mm256_setzero_si256();
 
-        size_t k = MIN(len, NMAX);
-        k -= k % 32;
+        size_t k = (len < NMAX) ? len : NMAX;
+        k &= ~(size_t)31;
         len -= k;
 
         while (k >= 64) {
-            __m256i vbuf = _mm256_loadu_si256((__m256i*)src);
-            __m256i vbuf_0 = _mm256_loadu_si256((__m256i*)(src + 32));
-            src += 64;
-            k -= 64;
-
+            __m256i vbuf = _mm256_loadu_si256((const __m256i *)src);
+            __m256i vbuf_0 = _mm256_loadu_si256((const __m256i *)(src + 32));
             __m256i vs1_sad = _mm256_sad_epu8(vbuf, zero);
             __m256i vs1_sad2 = _mm256_sad_epu8(vbuf_0, zero);
 
             if (COPY) {
-                _mm256_storeu_si256((__m256i*)dst, vbuf);
-                _mm256_storeu_si256((__m256i*)(dst + 32), vbuf_0);
+                _mm256_storeu_si256((__m256i *)dst, vbuf);
+                _mm256_storeu_si256((__m256i *)(dst + 32), vbuf_0);
                 dst += 64;
             }
 
-            vs1 = _mm256_add_epi32(vs1, vs1_sad);
             vs3 = _mm256_add_epi32(vs3, vs1_0);
-            __m256i v_short_sum2 = _mm256_maddubs_epi16(vbuf, dot2v); // sum 32 uint8s to 16 shorts
-            __m256i v_short_sum2_0 = _mm256_maddubs_epi16(vbuf_0, dot2v_0); // sum 32 uint8s to 16 shorts
-            __m256i vsum2 = _mm256_madd_epi16(v_short_sum2, dot3v); // sum 16 shorts to 8 uint32s
-            __m256i vsum2_0 = _mm256_madd_epi16(v_short_sum2_0, dot3v); // sum 16 shorts to 8 uint32s
-            vs1 = _mm256_add_epi32(vs1_sad2, vs1);
-            vs2 = _mm256_add_epi32(vsum2, vs2);
-            vs2_0 = _mm256_add_epi32(vsum2_0, vs2_0);
+            vs1 = _mm256_add_epi32(vs1, vs1_sad);
+
+            __m256i v_short_sum2 = _mm256_maddubs_epi16(vbuf, dot2v);
+            __m256i v_short_sum2_0 = _mm256_maddubs_epi16(vbuf_0, dot2v_0);
+            __m256i vsum2 = _mm256_madd_epi16(v_short_sum2, dot3v);
+            __m256i vsum2_0 = _mm256_madd_epi16(v_short_sum2_0, dot3v);
+
+            vs1 = _mm256_add_epi32(vs1, vs1_sad2);
+            vs2 = _mm256_add_epi32(vs2, vsum2);
+            vs2_0 = _mm256_add_epi32(vs2_0, vsum2_0);
+
             vs1_0 = vs1;
+            src += 64;
+            k -= 64;
         }
 
-        vs2 = _mm256_add_epi32(vs2_0, vs2);
+        vs2 = _mm256_add_epi32(vs2, vs2_0);
         vs3 = _mm256_slli_epi32(vs3, 6);
-        vs2 = _mm256_add_epi32(vs3, vs2);
+        vs2 = _mm256_add_epi32(vs2, vs3);
+
         vs3 = _mm256_setzero_si256();
 
         while (k >= 32) {
-            /*
-               vs1 = adler + sum(c[i])
-               vs2 = sum2 + 32 vs1 + sum( (32-i+1) c[i] )
-            */
-            __m256i vbuf = _mm256_loadu_si256((__m256i*)src);
-            src += 32;
-            k -= 32;
-
-            __m256i vs1_sad = _mm256_sad_epu8(vbuf, zero); // Sum of abs diff, resulting in 2 x int32's
+            __m256i vbuf = _mm256_loadu_si256((const __m256i *)src);
+            __m256i vs1_sad = _mm256_sad_epu8(vbuf, zero);
 
             if (COPY) {
-                _mm256_storeu_si256((__m256i*)dst, vbuf);
+                _mm256_storeu_si256((__m256i *)dst, vbuf);
                 dst += 32;
             }
- 
-            vs1 = _mm256_add_epi32(vs1, vs1_sad);
+
             vs3 = _mm256_add_epi32(vs3, vs1_0);
-            __m256i v_short_sum2 = _mm256_maddubs_epi16(vbuf, dot2v_0); // sum 32 uint8s to 16 shorts
-            __m256i vsum2 = _mm256_madd_epi16(v_short_sum2, dot3v); // sum 16 shorts to 8 uint32s
-            vs2 = _mm256_add_epi32(vsum2, vs2);
+            vs1 = _mm256_add_epi32(vs1, vs1_sad);
+
+            __m256i v_short_sum2 = _mm256_maddubs_epi16(vbuf, dot2v_0);
+            __m256i vsum2 = _mm256_madd_epi16(v_short_sum2, dot3v);
+            vs2 = _mm256_add_epi32(vs2, vsum2);
+
             vs1_0 = vs1;
+            src += 32;
+            k -= 32;
         }
 
-        /* Defer the multiplication with 32 to outside of the loop */
         vs3 = _mm256_slli_epi32(vs3, 5);
         vs2 = _mm256_add_epi32(vs2, vs3);
 
-        /* The compiler is generating the following sequence for this integer modulus
-         * when done the scalar way, in GPRs:
-
-         adler = (s1_unpack[0] % BASE) + (s1_unpack[1] % BASE) + (s1_unpack[2] % BASE) + (s1_unpack[3] % BASE) +
-                 (s1_unpack[4] % BASE) + (s1_unpack[5] % BASE) + (s1_unpack[6] % BASE) + (s1_unpack[7] % BASE);
-
-         mov    $0x80078071,%edi // move magic constant into 32 bit register %edi
-         ...
-         vmovd  %xmm1,%esi // move vector lane 0 to 32 bit register %esi
-         mov    %rsi,%rax  // zero-extend this value to 64 bit precision in %rax
-         imul   %rdi,%rsi // do a signed multiplication with magic constant and vector element
-         shr    $0x2f,%rsi // shift right by 47
-         imul   $0xfff1,%esi,%esi // do a signed multiplication with value truncated to 32 bits with 0xfff1
-         sub    %esi,%eax // subtract lower 32 bits of original vector value from modified one above
-         ...
-         // repeats for each element with vpextract instructions
-
-         This is tricky with AVX2 for a number of reasons:
-             1.) There's no 64 bit multiplication instruction, but there is a sequence to get there
-             2.) There's ways to extend vectors to 64 bit precision, but no simple way to truncate
-                 back down to 32 bit precision later (there is in AVX512)
-             3.) Full width integer multiplications aren't cheap
-
-         We can, however, do a relatively cheap sequence for horizontal sums.
-         Then, we simply do the integer modulus on the resulting 64 bit GPR, on a scalar value. It was
-         previously thought that casting to 64 bit precision was needed prior to the horizontal sum, but
-         that is simply not the case, as NMAX is defined as the maximum number of scalar sums that can be
-         performed on the maximum possible inputs before overflow
-         */
-
-
-         /* In AVX2-land, this trip through GPRs will probably be unavoidable, as there's no cheap and easy
-          * conversion from 64 bit integer to 32 bit (needed for the inexpensive modulus with a constant).
-          * This casting to 32 bit is cheap through GPRs (just register aliasing). See above for exactly
-          * what the compiler is doing to avoid integer divisions. */
-         adler0 = partial_hsum256(vs1) % BASE;
-         adler1 = hsum256(vs2) % BASE;
+        adler0 = partial_hsum256(vs1) % BASE;
+        adler1 = hsum256(vs2) % BASE;
     }
 
-    adler = adler0 | (adler1 << 16);
+    if (len > 0) {
+        uint32_t adler_combined = adler0 | (adler1 << 16);
+        _mm256_zeroupper();
+
+        if (len < 16) {
+            if (COPY)
+                return adler32_copy_len_16(adler0, src, dst, len, adler1);
+            return adler32_len_16(adler0, src, len, adler1);
+        }
 
-    if (len) {
-        goto rem_peel;
+        if (COPY)
+            return adler32_fold_copy_sse42(adler_combined, dst, src, len);
+        return adler32_ssse3(adler_combined, src, len);
     }
 
-    return adler;
+    _mm256_zeroupper();
+    return adler0 | (adler1 << 16);
 }
 
 Z_INTERNAL uint32_t adler32_avx2(uint32_t adler, const uint8_t *src, size_t len) {
     return adler32_fold_copy_impl(adler, NULL, src, len, 0);
 }
 
-Z_INTERNAL uint32_t adler32_fold_copy_avx2(uint32_t adler, uint8_t *dst, const uint8_t *src, size_t len) {
+Z_INTERNAL uint32_t adler32_fold_copy_avx2(uint32_t adler, uint8_t *dst,
+                                           const uint8_t *src, size_t len) {
     return adler32_fold_copy_impl(adler, dst, src, len, 1);
 }
 
