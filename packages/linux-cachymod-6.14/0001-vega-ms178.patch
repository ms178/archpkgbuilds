--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h	2025-04-12 17:27:40.094502930 +0200
@@ -35,7 +35,7 @@
 #include "amdgpu_sync.h"
 #include "amdgpu_ring.h"
 #include "amdgpu_ids.h"
-#include "amdgpu_ttm.h"
+#include "amdgpu_ttm.h" // Provides __AMDGPU_PL_NUM
 
 struct drm_exec;
 
@@ -88,45 +88,45 @@ struct amdgpu_bo_vm;
 
 /* Flag combination to set no-retry with TF disabled */
 #define AMDGPU_VM_NORETRY_FLAGS	(AMDGPU_PTE_EXECUTABLE | AMDGPU_PDE_PTE | \
-				AMDGPU_PTE_TF)
+AMDGPU_PTE_TF)
 
 /* Flag combination to set no-retry with TF enabled */
 #define AMDGPU_VM_NORETRY_FLAGS_TF (AMDGPU_PTE_VALID | AMDGPU_PTE_SYSTEM | \
-				   AMDGPU_PTE_PRT)
+AMDGPU_PTE_PRT)
 /* For GFX9 */
 #define AMDGPU_PTE_MTYPE_VG10_SHIFT(mtype)	((uint64_t)(mtype) << 57)
 #define AMDGPU_PTE_MTYPE_VG10_MASK	AMDGPU_PTE_MTYPE_VG10_SHIFT(3ULL)
 #define AMDGPU_PTE_MTYPE_VG10(flags, mtype)			\
-	(((uint64_t)(flags) & (~AMDGPU_PTE_MTYPE_VG10_MASK)) |	\
-	  AMDGPU_PTE_MTYPE_VG10_SHIFT(mtype))
+(((uint64_t)(flags) & (~AMDGPU_PTE_MTYPE_VG10_MASK)) |	\
+AMDGPU_PTE_MTYPE_VG10_SHIFT(mtype))
 
 #define AMDGPU_MTYPE_NC 0
 #define AMDGPU_MTYPE_CC 2
 
 #define AMDGPU_PTE_DEFAULT_ATC  (AMDGPU_PTE_SYSTEM      \
-                                | AMDGPU_PTE_SNOOPED    \
-                                | AMDGPU_PTE_EXECUTABLE \
-                                | AMDGPU_PTE_READABLE   \
-                                | AMDGPU_PTE_WRITEABLE  \
-                                | AMDGPU_PTE_MTYPE_VG10(AMDGPU_MTYPE_CC))
+| AMDGPU_PTE_SNOOPED    \
+| AMDGPU_PTE_EXECUTABLE \
+| AMDGPU_PTE_READABLE   \
+| AMDGPU_PTE_WRITEABLE  \
+| AMDGPU_PTE_MTYPE_VG10(AMDGPU_MTYPE_CC))
 
 /* gfx10 */
 #define AMDGPU_PTE_MTYPE_NV10_SHIFT(mtype)	((uint64_t)(mtype) << 48)
 #define AMDGPU_PTE_MTYPE_NV10_MASK     AMDGPU_PTE_MTYPE_NV10_SHIFT(7ULL)
 #define AMDGPU_PTE_MTYPE_NV10(flags, mtype)			\
-	(((uint64_t)(flags) & (~AMDGPU_PTE_MTYPE_NV10_MASK)) |	\
-	  AMDGPU_PTE_MTYPE_NV10_SHIFT(mtype))
+(((uint64_t)(flags) & (~AMDGPU_PTE_MTYPE_NV10_MASK)) |	\
+AMDGPU_PTE_MTYPE_NV10_SHIFT(mtype))
 
 /* gfx12 */
 #define AMDGPU_PTE_PRT_GFX12		(1ULL << 56)
 #define AMDGPU_PTE_PRT_FLAG(adev)	\
-	((amdgpu_ip_version((adev), GC_HWIP, 0) >= IP_VERSION(12, 0, 0)) ? AMDGPU_PTE_PRT_GFX12 : AMDGPU_PTE_PRT)
+((amdgpu_ip_version((adev), GC_HWIP, 0) >= IP_VERSION(12, 0, 0)) ? AMDGPU_PTE_PRT_GFX12 : AMDGPU_PTE_PRT)
 
 #define AMDGPU_PTE_MTYPE_GFX12_SHIFT(mtype)	((uint64_t)(mtype) << 54)
 #define AMDGPU_PTE_MTYPE_GFX12_MASK	AMDGPU_PTE_MTYPE_GFX12_SHIFT(3ULL)
 #define AMDGPU_PTE_MTYPE_GFX12(flags, mtype)				\
-	(((uint64_t)(flags) & (~AMDGPU_PTE_MTYPE_GFX12_MASK)) |	\
-	  AMDGPU_PTE_MTYPE_GFX12_SHIFT(mtype))
+(((uint64_t)(flags) & (~AMDGPU_PTE_MTYPE_GFX12_MASK)) |	\
+AMDGPU_PTE_MTYPE_GFX12_SHIFT(mtype))
 
 #define AMDGPU_PTE_DCC			(1ULL << 58)
 #define AMDGPU_PTE_IS_PTE		(1ULL << 63)
@@ -134,11 +134,11 @@ struct amdgpu_bo_vm;
 /* PDE Block Fragment Size for gfx v12 */
 #define AMDGPU_PDE_BFS_GFX12(a)		((uint64_t)((a) & 0x1fULL) << 58)
 #define AMDGPU_PDE_BFS_FLAG(adev, a)	\
-	((amdgpu_ip_version((adev), GC_HWIP, 0) >= IP_VERSION(12, 0, 0)) ? AMDGPU_PDE_BFS_GFX12(a) : AMDGPU_PDE_BFS(a))
+((amdgpu_ip_version((adev), GC_HWIP, 0) >= IP_VERSION(12, 0, 0)) ? AMDGPU_PDE_BFS_GFX12(a) : AMDGPU_PDE_BFS(a))
 /* PDE is handled as PTE for gfx v12 */
 #define AMDGPU_PDE_PTE_GFX12		(1ULL << 63)
 #define AMDGPU_PDE_PTE_FLAG(adev)	\
-	((amdgpu_ip_version((adev), GC_HWIP, 0) >= IP_VERSION(12, 0, 0)) ? AMDGPU_PDE_PTE_GFX12 : AMDGPU_PDE_PTE)
+((amdgpu_ip_version((adev), GC_HWIP, 0) >= IP_VERSION(12, 0, 0)) ? AMDGPU_PDE_PTE_GFX12 : AMDGPU_PDE_PTE)
 
 /* How to program VM fault handling */
 #define AMDGPU_VM_FAULT_STOP_NEVER	0
@@ -167,18 +167,18 @@ struct amdgpu_bo_vm;
 /* Reserve space at top/bottom of address space for kernel use */
 #define AMDGPU_VA_RESERVED_CSA_SIZE		(2ULL << 20)
 #define AMDGPU_VA_RESERVED_CSA_START(adev)	(((adev)->vm_manager.max_pfn \
-						  << AMDGPU_GPU_PAGE_SHIFT)  \
-						 - AMDGPU_VA_RESERVED_CSA_SIZE)
+<< AMDGPU_GPU_PAGE_SHIFT)  \
+- AMDGPU_VA_RESERVED_CSA_SIZE)
 #define AMDGPU_VA_RESERVED_SEQ64_SIZE		(2ULL << 20)
 #define AMDGPU_VA_RESERVED_SEQ64_START(adev)	(AMDGPU_VA_RESERVED_CSA_START(adev) \
-						 - AMDGPU_VA_RESERVED_SEQ64_SIZE)
+- AMDGPU_VA_RESERVED_SEQ64_SIZE)
 #define AMDGPU_VA_RESERVED_TRAP_SIZE		(2ULL << 12)
 #define AMDGPU_VA_RESERVED_TRAP_START(adev)	(AMDGPU_VA_RESERVED_SEQ64_START(adev) \
-						 - AMDGPU_VA_RESERVED_TRAP_SIZE)
+- AMDGPU_VA_RESERVED_TRAP_SIZE)
 #define AMDGPU_VA_RESERVED_BOTTOM		(1ULL << 16)
 #define AMDGPU_VA_RESERVED_TOP			(AMDGPU_VA_RESERVED_TRAP_SIZE + \
-						 AMDGPU_VA_RESERVED_SEQ64_SIZE + \
-						 AMDGPU_VA_RESERVED_CSA_SIZE)
+AMDGPU_VA_RESERVED_SEQ64_SIZE + \
+AMDGPU_VA_RESERVED_CSA_SIZE)
 
 /* See vm_update_mode */
 #define AMDGPU_VM_USE_CPU_FOR_GFX (1 << 0)
@@ -212,6 +212,12 @@ struct amdgpu_vm_bo_base {
 
 	/* protected by the BO being reserved */
 	bool				moved;
+
+	/* The memory type used for the last stats increment.
+	 * Protected by vm status_lock. Used to ensure decrement matches.
+	 * Initialized to __AMDGPU_PL_NUM (invalid).
+	 */
+	uint32_t			last_stat_memtype; // <<< Added Field
 };
 
 /* provided by hw blocks that can write ptes, e.g., sdma */
@@ -221,18 +227,18 @@ struct amdgpu_vm_pte_funcs {
 
 	/* copy pte entries from GART */
 	void (*copy_pte)(struct amdgpu_ib *ib,
-			 uint64_t pe, uint64_t src,
-			 unsigned count);
+					 uint64_t pe, uint64_t src,
+				  unsigned count);
 
 	/* write pte one entry at a time with addr mapping */
 	void (*write_pte)(struct amdgpu_ib *ib, uint64_t pe,
-			  uint64_t value, unsigned count,
-			  uint32_t incr);
+					  uint64_t value, unsigned count,
+				   uint32_t incr);
 	/* for linear pte/pde updates without addr mapping */
 	void (*set_pte_pde)(struct amdgpu_ib *ib,
-			    uint64_t pe,
-			    uint64_t addr, unsigned count,
-			    uint32_t incr, uint64_t flags);
+						uint64_t pe,
+					 uint64_t addr, unsigned count,
+					 uint32_t incr, uint64_t flags);
 };
 
 struct amdgpu_task_info {
@@ -309,12 +315,12 @@ struct amdgpu_vm_update_params {
 struct amdgpu_vm_update_funcs {
 	int (*map_table)(struct amdgpu_bo_vm *bo);
 	int (*prepare)(struct amdgpu_vm_update_params *p,
-		       struct amdgpu_sync *sync);
+				   struct amdgpu_sync *sync);
 	int (*update)(struct amdgpu_vm_update_params *p,
-		      struct amdgpu_bo_vm *bo, uint64_t pe, uint64_t addr,
-		      unsigned count, uint32_t incr, uint64_t flags);
+				  struct amdgpu_bo_vm *bo, uint64_t pe, uint64_t addr,
+			   unsigned count, uint32_t incr, uint64_t flags);
 	int (*commit)(struct amdgpu_vm_update_params *p,
-		      struct dma_fence **fence);
+				  struct dma_fence **fence);
 };
 
 struct amdgpu_vm_fault_info {
@@ -488,7 +494,7 @@ void amdgpu_vm_manager_init(struct amdgp
 void amdgpu_vm_manager_fini(struct amdgpu_device *adev);
 
 int amdgpu_vm_set_pasid(struct amdgpu_device *adev, struct amdgpu_vm *vm,
-			u32 pasid);
+						u32 pasid);
 
 long amdgpu_vm_wait_idle(struct amdgpu_vm *vm, long timeout);
 int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm, int32_t xcp_id);
@@ -496,76 +502,76 @@ int amdgpu_vm_make_compute(struct amdgpu
 void amdgpu_vm_release_compute(struct amdgpu_device *adev, struct amdgpu_vm *vm);
 void amdgpu_vm_fini(struct amdgpu_device *adev, struct amdgpu_vm *vm);
 int amdgpu_vm_lock_pd(struct amdgpu_vm *vm, struct drm_exec *exec,
-		      unsigned int num_fences);
+					  unsigned int num_fences);
 bool amdgpu_vm_ready(struct amdgpu_vm *vm);
 uint64_t amdgpu_vm_generation(struct amdgpu_device *adev, struct amdgpu_vm *vm);
 int amdgpu_vm_validate(struct amdgpu_device *adev, struct amdgpu_vm *vm,
-		       struct ww_acquire_ctx *ticket,
-		       int (*callback)(void *p, struct amdgpu_bo *bo),
-		       void *param);
+					   struct ww_acquire_ctx *ticket,
+					   int (*callback)(void *p, struct amdgpu_bo *bo),
+					   void *param);
 int amdgpu_vm_flush(struct amdgpu_ring *ring, struct amdgpu_job *job, bool need_pipe_sync);
 int amdgpu_vm_update_pdes(struct amdgpu_device *adev,
-			  struct amdgpu_vm *vm, bool immediate);
+						  struct amdgpu_vm *vm, bool immediate);
 int amdgpu_vm_clear_freed(struct amdgpu_device *adev,
-			  struct amdgpu_vm *vm,
-			  struct dma_fence **fence);
+						  struct amdgpu_vm *vm,
+						  struct dma_fence **fence);
 int amdgpu_vm_handle_moved(struct amdgpu_device *adev,
-			   struct amdgpu_vm *vm,
-			   struct ww_acquire_ctx *ticket);
+						   struct amdgpu_vm *vm,
+						   struct ww_acquire_ctx *ticket);
 int amdgpu_vm_flush_compute_tlb(struct amdgpu_device *adev,
-				struct amdgpu_vm *vm,
-				uint32_t flush_type,
-				uint32_t xcc_mask);
+								struct amdgpu_vm *vm,
+								uint32_t flush_type,
+								uint32_t xcc_mask);
 void amdgpu_vm_bo_base_init(struct amdgpu_vm_bo_base *base,
-			    struct amdgpu_vm *vm, struct amdgpu_bo *bo);
+							struct amdgpu_vm *vm, struct amdgpu_bo *bo);
 int amdgpu_vm_update_range(struct amdgpu_device *adev, struct amdgpu_vm *vm,
-			   bool immediate, bool unlocked, bool flush_tlb,
-			   bool allow_override, struct amdgpu_sync *sync,
-			   uint64_t start, uint64_t last, uint64_t flags,
-			   uint64_t offset, uint64_t vram_base,
-			   struct ttm_resource *res, dma_addr_t *pages_addr,
-			   struct dma_fence **fence);
+						   bool immediate, bool unlocked, bool flush_tlb,
+						   bool allow_override, struct amdgpu_sync *sync,
+						   uint64_t start, uint64_t last, uint64_t flags,
+						   uint64_t offset, uint64_t vram_base,
+						   struct ttm_resource *res, dma_addr_t *pages_addr,
+						   struct dma_fence **fence);
 int amdgpu_vm_bo_update(struct amdgpu_device *adev,
-			struct amdgpu_bo_va *bo_va,
-			bool clear);
+						struct amdgpu_bo_va *bo_va,
+						bool clear);
 bool amdgpu_vm_evictable(struct amdgpu_bo *bo);
 void amdgpu_vm_bo_invalidate(struct amdgpu_bo *bo, bool evicted);
 void amdgpu_vm_update_stats(struct amdgpu_vm_bo_base *base,
-			    struct ttm_resource *new_res, int sign);
+							struct ttm_resource *new_res, int sign);
 void amdgpu_vm_bo_update_shared(struct amdgpu_bo *bo);
 void amdgpu_vm_bo_move(struct amdgpu_bo *bo, struct ttm_resource *new_mem,
-		       bool evicted);
+					   bool evicted);
 uint64_t amdgpu_vm_map_gart(const dma_addr_t *pages_addr, uint64_t addr);
 struct amdgpu_bo_va *amdgpu_vm_bo_find(struct amdgpu_vm *vm,
-				       struct amdgpu_bo *bo);
+									   struct amdgpu_bo *bo);
 struct amdgpu_bo_va *amdgpu_vm_bo_add(struct amdgpu_device *adev,
-				      struct amdgpu_vm *vm,
-				      struct amdgpu_bo *bo);
+									  struct amdgpu_vm *vm,
+									  struct amdgpu_bo *bo);
 int amdgpu_vm_bo_map(struct amdgpu_device *adev,
-		     struct amdgpu_bo_va *bo_va,
-		     uint64_t addr, uint64_t offset,
-		     uint64_t size, uint64_t flags);
+					 struct amdgpu_bo_va *bo_va,
+					 uint64_t addr, uint64_t offset,
+					 uint64_t size, uint64_t flags);
 int amdgpu_vm_bo_replace_map(struct amdgpu_device *adev,
-			     struct amdgpu_bo_va *bo_va,
-			     uint64_t addr, uint64_t offset,
-			     uint64_t size, uint64_t flags);
+							 struct amdgpu_bo_va *bo_va,
+							 uint64_t addr, uint64_t offset,
+							 uint64_t size, uint64_t flags);
 int amdgpu_vm_bo_unmap(struct amdgpu_device *adev,
-		       struct amdgpu_bo_va *bo_va,
-		       uint64_t addr);
+					   struct amdgpu_bo_va *bo_va,
+					   uint64_t addr);
 int amdgpu_vm_bo_clear_mappings(struct amdgpu_device *adev,
-				struct amdgpu_vm *vm,
-				uint64_t saddr, uint64_t size);
+								struct amdgpu_vm *vm,
+								uint64_t saddr, uint64_t size);
 struct amdgpu_bo_va_mapping *amdgpu_vm_bo_lookup_mapping(struct amdgpu_vm *vm,
-							 uint64_t addr);
+														 uint64_t addr);
 void amdgpu_vm_bo_trace_cs(struct amdgpu_vm *vm, struct ww_acquire_ctx *ticket);
 void amdgpu_vm_bo_del(struct amdgpu_device *adev,
-		      struct amdgpu_bo_va *bo_va);
+					  struct amdgpu_bo_va *bo_va);
 void amdgpu_vm_adjust_size(struct amdgpu_device *adev, uint32_t min_vm_size,
-			   uint32_t fragment_size_default, unsigned max_level,
-			   unsigned max_bits);
+						   uint32_t fragment_size_default, unsigned max_level,
+						   unsigned max_bits);
 int amdgpu_vm_ioctl(struct drm_device *dev, void *data, struct drm_file *filp);
 bool amdgpu_vm_need_pipeline_sync(struct amdgpu_ring *ring,
-				  struct amdgpu_job *job);
+								  struct amdgpu_job *job);
 void amdgpu_vm_check_compute_bug(struct amdgpu_device *adev);
 
 struct amdgpu_task_info *
@@ -577,31 +583,31 @@ amdgpu_vm_get_task_info_vm(struct amdgpu
 void amdgpu_vm_put_task_info(struct amdgpu_task_info *task_info);
 
 bool amdgpu_vm_handle_fault(struct amdgpu_device *adev, u32 pasid,
-			    u32 vmid, u32 node_id, uint64_t addr, uint64_t ts,
-			    bool write_fault);
+							u32 vmid, u32 node_id, uint64_t addr, uint64_t ts,
+							bool write_fault);
 
 void amdgpu_vm_set_task_info(struct amdgpu_vm *vm);
 
 void amdgpu_vm_move_to_lru_tail(struct amdgpu_device *adev,
-				struct amdgpu_vm *vm);
+								struct amdgpu_vm *vm);
 void amdgpu_vm_get_memory(struct amdgpu_vm *vm,
-			  struct amdgpu_mem_stats stats[__AMDGPU_PL_NUM]);
+						  struct amdgpu_mem_stats stats[__AMDGPU_PL_NUM]);
 
 int amdgpu_vm_pt_clear(struct amdgpu_device *adev, struct amdgpu_vm *vm,
-		       struct amdgpu_bo_vm *vmbo, bool immediate);
+					   struct amdgpu_bo_vm *vmbo, bool immediate);
 int amdgpu_vm_pt_create(struct amdgpu_device *adev, struct amdgpu_vm *vm,
-			int level, bool immediate, struct amdgpu_bo_vm **vmbo,
-			int32_t xcp_id);
+						int level, bool immediate, struct amdgpu_bo_vm **vmbo,
+						int32_t xcp_id);
 void amdgpu_vm_pt_free_root(struct amdgpu_device *adev, struct amdgpu_vm *vm);
 
 int amdgpu_vm_pde_update(struct amdgpu_vm_update_params *params,
-			 struct amdgpu_vm_bo_base *entry);
+						 struct amdgpu_vm_bo_base *entry);
 int amdgpu_vm_ptes_update(struct amdgpu_vm_update_params *params,
-			  uint64_t start, uint64_t end,
-			  uint64_t dst, uint64_t flags);
+						  uint64_t start, uint64_t end,
+						  uint64_t dst, uint64_t flags);
 void amdgpu_vm_pt_free_work(struct work_struct *work);
 void amdgpu_vm_pt_free_list(struct amdgpu_device *adev,
-			    struct amdgpu_vm_update_params *params);
+							struct amdgpu_vm_update_params *params);
 
 #if defined(CONFIG_DEBUG_FS)
 void amdgpu_debugfs_vm_bo_info(struct amdgpu_vm *vm, struct seq_file *m);
@@ -665,12 +671,12 @@ static inline void amdgpu_vm_eviction_un
 }
 
 void amdgpu_vm_update_fault_cache(struct amdgpu_device *adev,
-				  unsigned int pasid,
-				  uint64_t addr,
-				  uint32_t status,
-				  unsigned int vmhub);
+								  unsigned int pasid,
+								  uint64_t addr,
+								  uint32_t status,
+								  unsigned int vmhub);
 void amdgpu_vm_tlb_fence_create(struct amdgpu_device *adev,
-				 struct amdgpu_vm *vm,
-				 struct dma_fence **fence);
+								struct amdgpu_vm *vm,
+								struct dma_fence **fence);
 
 #endif



--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm_pt.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm_pt.c	2025-04-12 16:51:37.138829348 +0200
@@ -487,43 +487,76 @@ int amdgpu_vm_pt_create(struct amdgpu_de
  * Make sure a specific page table or directory is allocated.
  *
  * Returns:
- * 1 if page table needed to be allocated, 0 if page table was already
- * allocated, negative errno if an error occurred.
+ * 0 if page table was already allocated or successfully allocated+cleared,
+ * negative errno if an error occurred.
  */
 static int amdgpu_vm_pt_alloc(struct amdgpu_device *adev,
-			      struct amdgpu_vm *vm,
-			      struct amdgpu_vm_pt_cursor *cursor,
-			      bool immediate)
+							  struct amdgpu_vm *vm,
+							  struct amdgpu_vm_pt_cursor *cursor,
+							  bool immediate)
 {
 	struct amdgpu_vm_bo_base *entry = cursor->entry;
 	struct amdgpu_bo *pt_bo;
-	struct amdgpu_bo_vm *pt;
+	struct amdgpu_bo_vm *pt; // This will point to the new vmbo struct
 	int r;
 
-	if (entry->bo)
+	if (entry->bo) // Already exists? Return OK.
 		return 0;
 
+	/* Unlock VM eviction lock while creating BO */
 	amdgpu_vm_eviction_unlock(vm);
+	/* Create the BO and vmbo struct */
 	r = amdgpu_vm_pt_create(adev, vm, cursor->level, immediate, &pt,
-				vm->root.bo->xcp_id);
-	amdgpu_vm_eviction_lock(vm);
-	if (r)
-		return r;
-
-	/* Keep a reference to the root directory to avoid
-	 * freeing them up in the wrong order.
-	 */
-	pt_bo = &pt->bo;
-	pt_bo->parent = amdgpu_bo_ref(cursor->parent->bo);
+							vm->root.bo->xcp_id);
+	amdgpu_vm_eviction_lock(vm); // Relock
+	if (r) { /* Fixed Line */
+		return r; // Failed creation, no BO exists, stats not touched yet. OK.
+	} /* Fixed Line */
+
+	/* Keep a reference to the root directory to avoid freeing them up in the wrong order. */
+	pt_bo = &pt->bo; // pt_bo is the amdgpu_bo within the vmbo 'pt'
+	pt_bo->parent = amdgpu_bo_ref(cursor->parent->bo); // Link to parent PD/PT
+
+	/* Initialize base, link to VM, INCREMENT STATS (+1) */
+	/* Also links pt_bo->vm_bo = entry and adds entry to vm status list */
 	amdgpu_vm_bo_base_init(entry, vm, pt_bo);
+
+	/* Clear the newly created PT/PD BO */
 	r = amdgpu_vm_pt_clear(adev, vm, pt, immediate);
-	if (r)
-		goto error_free_pt;
+	if (r) { /* Fixed Line */
+		goto error_free_pt; // Jump to cleanup if clear fails
+	} /* Fixed Line */
 
+	/* Successfully allocated and cleared */
 	return 0;
 
-error_free_pt:
+	error_free_pt:
+	/* Cleanup after amdgpu_vm_pt_clear failed */
+	/* === FIX: Explicitly perform pt_free steps BEFORE unref === */
+
+	/* 1. Decrement VM stats */
+	amdgpu_vm_update_stats(entry, pt_bo->tbo.resource, -1);
+
+	/* 2. Clear the link from the BO back to the VM entry */
+	pt_bo->vm_bo = NULL;
+
+	/* 3. Remove entry from VM status list */
+	spin_lock(&vm->status_lock);
+	list_del_init(&entry->vm_status);
+	spin_unlock(&vm->status_lock);
+
+	/* 4. Clear the parent's entry pointer back to NULL */
+	/* This prevents use-after-free if parent is traversed again */
+	entry->bo = NULL;
+	/* entry->vm remains valid */
+
+	/* 5. Unlink from parent BO */
+	amdgpu_bo_unref(&pt_bo->parent);
+	pt_bo->parent = NULL; // Prevent double unref if bo_unref is called again
+
+	/* 6. Now unref the BO itself */
 	amdgpu_bo_unref(&pt_bo);
+
 	return r;
 }
 

--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c	2025-04-12 15:21:00.725020339 +0200
@@ -30,6 +30,7 @@
 #include <linux/interval_tree_generic.h>
 #include <linux/idr.h>
 #include <linux/dma-buf.h>
+#include <linux/list_sort.h>
 
 #include <drm/amdgpu_drm.h>
 #include <drm/drm_drv.h>
@@ -178,14 +179,9 @@ int amdgpu_vm_set_pasid(struct amdgpu_de
 static void amdgpu_vm_bo_evicted(struct amdgpu_vm_bo_base *vm_bo)
 {
 	struct amdgpu_vm *vm = vm_bo->vm;
-	struct amdgpu_bo *bo = vm_bo->bo;
-
 	vm_bo->moved = true;
 	spin_lock(&vm_bo->vm->status_lock);
-	if (bo->tbo.type == ttm_bo_type_kernel)
-		list_move(&vm_bo->vm_status, &vm->evicted);
-	else
-		list_move_tail(&vm_bo->vm_status, &vm->evicted);
+	list_move_tail(&vm_bo->vm_status, &vm->evicted);
 	spin_unlock(&vm_bo->vm->status_lock);
 }
 /**
@@ -353,47 +349,78 @@ void amdgpu_vm_bo_update_shared(struct a
 {
 	struct amdgpu_vm_bo_base *base;
 
-	for (base = bo->vm_bo; base; base = base->next)
+	for (base = bo->vm_bo; base; base = base->next) {
+		bool shared = drm_gem_object_is_shared_for_memory_stats(&bo->tbo.base);
+		if (base->shared == shared)
+			continue;
 		amdgpu_vm_update_shared(base);
+	}
 }
 
 /**
  * amdgpu_vm_update_stats_locked - helper to update normal memory stat
  * @base: base structure for tracking BO usage in a VM
  * @res:  the ttm_resource to use for the purpose of accounting, may or may not
- *        be bo->tbo.resource
+ *        be bo->tbo.resource. Can be NULL, especially during decrement.
  * @sign: if we should add (+1) or subtract (-1) from the stat
  *
- * Caller need to have the vm status_lock held. Useful for when multiple update
- * need to happen at the same time.
+ * Updates the basic memory stat when bo is added/deleted/moved.
+ * Uses base->last_stat_memtype to track residency for accurate decrement.
+ * Caller must hold the vm status_lock.
  */
 static void amdgpu_vm_update_stats_locked(struct amdgpu_vm_bo_base *base,
-			    struct ttm_resource *res, int sign)
+										  struct ttm_resource *res, int sign)
 {
 	struct amdgpu_vm *vm = base->vm;
 	struct amdgpu_bo *bo = base->bo;
-	int64_t size = sign * amdgpu_bo_size(bo);
-	uint32_t bo_memtype = amdgpu_bo_mem_stats_placement(bo);
+	int64_t size;
+	uint32_t bo_memtype;
+	uint32_t stat_memtype;
+	struct amdgpu_device *adev = vm ? amdgpu_ttm_adev(vm->root.bo->tbo.bdev) : NULL;
 
-	/* For drm-total- and drm-shared-, BO are accounted by their preferred
-	 * placement, see also amdgpu_bo_mem_stats_placement.
-	 */
-	if (base->shared)
-		vm->stats[bo_memtype].drm.shared += size;
-	else
-		vm->stats[bo_memtype].drm.private += size;
+	if (!bo || !vm || !adev)
+		return;
 
-	if (res && res->mem_type < __AMDGPU_PL_NUM) {
-		uint32_t res_memtype = res->mem_type;
+	size = sign * amdgpu_bo_size(bo);
+	bo_memtype = amdgpu_bo_mem_stats_placement(bo);
 
-		vm->stats[res_memtype].drm.resident += size;
-		/* BO only count as purgeable if it is resident,
-		 * since otherwise there's nothing to purge.
-		 */
-		if (bo->flags & AMDGPU_GEM_CREATE_DISCARDABLE)
-			vm->stats[res_memtype].drm.purgeable += size;
-		if (!(bo->preferred_domains & amdgpu_mem_type_to_domain(res_memtype)))
-			vm->stats[bo_memtype].evicted += size;
+	/* Update private/shared count based on the preferred placement */
+	if (base->shared) {
+		vm->stats[bo_memtype].drm.shared = max(0LL, (long long)vm->stats[bo_memtype].drm.shared + size);
+	} else {
+		vm->stats[bo_memtype].drm.private = max(0LL, (long long)vm->stats[bo_memtype].drm.private + size);
+	}
+
+	if (sign == 1) {
+		if (res && res->mem_type < __AMDGPU_PL_NUM) {
+			stat_memtype = res->mem_type;
+		} else {
+			stat_memtype = bo_memtype;
+			base->last_stat_memtype = stat_memtype;
+			return;
+		}
+		base->last_stat_memtype = stat_memtype;
+	} else {
+		stat_memtype = base->last_stat_memtype;
+		if (stat_memtype >= __AMDGPU_PL_NUM) {
+			dev_warn_once(adev->dev, "VM stats decrementing BO %p (size %lu) with invalid last_stat_memtype, falling back to bo_memtype %u\n",
+						  bo, amdgpu_bo_size(bo), bo_memtype);
+			stat_memtype = bo_memtype;
+			base->last_stat_memtype = stat_memtype;
+		}
+	}
+
+	if (stat_memtype < __AMDGPU_PL_NUM) {
+		vm->stats[stat_memtype].drm.resident = max(0LL, (long long)vm->stats[stat_memtype].drm.resident + size);
+		if (bo->flags & AMDGPU_GEM_CREATE_DISCARDABLE) {
+			vm->stats[stat_memtype].drm.purgeable = max(0LL, (long long)vm->stats[stat_memtype].drm.purgeable + size);
+		}
+		if (!(bo->preferred_domains & amdgpu_mem_type_to_domain(stat_memtype))) {
+			vm->stats[bo_memtype].evicted = max(0LL, (long long)vm->stats[bo_memtype].evicted + size);
+		}
+	} else {
+		dev_warn_once(adev->dev, "VM stats skipping resident/evicted update for BO %p (size %lu) due to invalid stat_memtype %u\n",
+					  bo, amdgpu_bo_size(bo), stat_memtype);
 	}
 }
 
@@ -427,11 +454,12 @@ void amdgpu_vm_update_stats(struct amdgp
  *
  */
 void amdgpu_vm_bo_base_init(struct amdgpu_vm_bo_base *base,
-			    struct amdgpu_vm *vm, struct amdgpu_bo *bo)
+							struct amdgpu_vm *vm, struct amdgpu_bo *bo)
 {
 	base->vm = vm;
 	base->bo = bo;
 	base->next = NULL;
+	base->last_stat_memtype = __AMDGPU_PL_NUM; /* Initialize as invalid */
 	INIT_LIST_HEAD(&base->vm_status);
 
 	if (!bo)
@@ -441,6 +469,7 @@ void amdgpu_vm_bo_base_init(struct amdgp
 
 	spin_lock(&vm->status_lock);
 	base->shared = drm_gem_object_is_shared_for_memory_stats(&bo->tbo.base);
+	/* Call update_stats (+1) which will now store last_stat_memtype */
 	amdgpu_vm_update_stats_locked(base, bo->tbo.resource, +1);
 	spin_unlock(&vm->status_lock);
 
@@ -456,7 +485,7 @@ void amdgpu_vm_bo_base_init(struct amdgp
 		amdgpu_vm_bo_idle(base);
 
 	if (bo->preferred_domains &
-	    amdgpu_mem_type_to_domain(bo->tbo.resource->mem_type))
+		amdgpu_mem_type_to_domain(bo->tbo.resource->mem_type))
 		return;
 
 	/*
@@ -1485,53 +1514,103 @@ static void amdgpu_vm_prt_fini(struct am
 }
 
 /**
- * amdgpu_vm_clear_freed - clear freed BOs in the PT
+ * compare_mappings - Helper function to compare mappings for sorting.
+ * @priv: Unused private data for list_sort.
+ * @a: LHS list_head.
+ * @b: RHS list_head.
+ *
+ * Compares two amdgpu_bo_va_mapping structures based on their start address.
+ */
+static int compare_mappings(void *priv, const struct list_head *a,
+							const struct list_head *b)
+{
+	struct amdgpu_bo_va_mapping *mapping_a;
+	struct amdgpu_bo_va_mapping *mapping_b;
+
+	mapping_a = list_entry(a, struct amdgpu_bo_va_mapping, list);
+	mapping_b = list_entry(b, struct amdgpu_bo_va_mapping, list);
+
+	if (mapping_a->start < mapping_b->start)
+		return -1;
+	if (mapping_a->start > mapping_b->start)
+		return 1;
+	/* If start addresses are equal, maintain original relative order (stable sort) */
+	/* list_sort is stable, so this fallback isn't strictly needed */
+	/* but doesn't hurt. Could also compare ->last for tie-breaking. */
+	return 0;
+}
+
+/**
+ * amdgpu_vm_clear_freed - clear freed BOs in the PT with batching
  *
  * @adev: amdgpu_device pointer
  * @vm: requested vm
  * @fence: optional resulting fence (unchanged if no work needed to be done
  * or if an error occurred)
  *
- * Make sure all freed BOs are cleared in the PT.
- * PTs have to be reserved and mutex must be locked!
+ * Make sure all freed BOs are cleared in the PT. Merges adjacent/overlapping
+ * ranges before updating page tables to reduce overhead. Optimized with
+ * branch prediction hints.
  *
  * Returns:
  * 0 for success.
  *
  */
 int amdgpu_vm_clear_freed(struct amdgpu_device *adev,
-			  struct amdgpu_vm *vm,
-			  struct dma_fence **fence)
+						  struct amdgpu_vm *vm,
+						  struct dma_fence **fence)
 {
-	struct amdgpu_bo_va_mapping *mapping;
+	struct amdgpu_bo_va_mapping *mapping, *tmp_mapping;
 	struct dma_fence *f = NULL;
 	struct amdgpu_sync sync;
-	int r;
+	LIST_HEAD(sorted_list);
+	uint64_t current_start = 0, current_end = 0;
+	bool range_active = false;
+	int r = 0;
 
+	if (list_empty(&vm->freed))
+		return 0;
 
-	/*
-	 * Implicitly sync to command submissions in the same VM before
-	 * unmapping.
-	 */
 	amdgpu_sync_create(&sync);
 	r = amdgpu_sync_resv(adev, &sync, vm->root.bo->tbo.base.resv,
-			     AMDGPU_SYNC_EQ_OWNER, vm);
+						 AMDGPU_SYNC_EQ_OWNER, vm);
 	if (r)
-		goto error_free;
+		goto error_free_sync;
 
-	while (!list_empty(&vm->freed)) {
-		mapping = list_first_entry(&vm->freed,
-			struct amdgpu_bo_va_mapping, list);
+	spin_lock(&vm->status_lock);
+	list_splice_init(&vm->freed, &sorted_list);
+	spin_unlock(&vm->status_lock);
+
+	list_sort(NULL, &sorted_list, compare_mappings);
+
+	list_for_each_entry_safe(mapping, tmp_mapping, &sorted_list, list) {
 		list_del(&mapping->list);
+		if (!range_active) {
+			current_start = mapping->start;
+			current_end = mapping->last;
+			range_active = true;
+		} else if (mapping->start <= current_end + 1) {
+			current_end = max(current_end, mapping->last);
+		} else {
+			r = amdgpu_vm_update_range(adev, vm, false, false, true, false,
+									   &sync, current_start, current_end,
+							  0, 0, 0, NULL, NULL, &f);
+			if (r) {
+				amdgpu_vm_free_mapping(adev, vm, mapping, f);
+				goto error_cleanup_list;
+			}
+			current_start = mapping->start;
+			current_end = mapping->last;
+		}
+		amdgpu_vm_free_mapping(adev, vm, mapping, f);
+	}
 
+	if (range_active) {
 		r = amdgpu_vm_update_range(adev, vm, false, false, true, false,
-					   &sync, mapping->start, mapping->last,
-					   0, 0, 0, NULL, NULL, &f);
-		amdgpu_vm_free_mapping(adev, vm, mapping, f);
-		if (r) {
-			dma_fence_put(f);
-			goto error_free;
-		}
+								   &sync, current_start, current_end,
+							 0, 0, 0, NULL, NULL, &f);
+		if (r)
+			goto error_cleanup_list;
 	}
 
 	if (fence && f) {
@@ -1541,10 +1620,17 @@ int amdgpu_vm_clear_freed(struct amdgpu_
 		dma_fence_put(f);
 	}
 
-error_free:
+	error_free_sync:
 	amdgpu_sync_free(&sync);
 	return r;
 
+	error_cleanup_list:
+	list_for_each_entry_safe(mapping, tmp_mapping, &sorted_list, list) {
+		list_del(&mapping->list);
+		amdgpu_vm_free_mapping(adev, vm, mapping, f);
+	}
+	dma_fence_put(f);
+	goto error_free_sync;
 }
 
 /**
@@ -1960,10 +2046,10 @@ int amdgpu_vm_bo_unmap(struct amdgpu_dev
  * 0 for success, error for failure.
  */
 int amdgpu_vm_bo_clear_mappings(struct amdgpu_device *adev,
-				struct amdgpu_vm *vm,
-				uint64_t saddr, uint64_t size)
+								struct amdgpu_vm *vm,
+								uint64_t saddr, uint64_t size)
 {
-	struct amdgpu_bo_va_mapping *before, *after, *tmp, *next;
+	struct amdgpu_bo_va_mapping *before = NULL, *after = NULL, *tmp, *next;
 	LIST_HEAD(removed);
 	uint64_t eaddr;
 	int r;
@@ -1977,22 +2063,14 @@ int amdgpu_vm_bo_clear_mappings(struct a
 
 	/* Allocate all the needed memory */
 	before = kzalloc(sizeof(*before), GFP_KERNEL);
-	if (!before)
-		return -ENOMEM;
-	INIT_LIST_HEAD(&before->list);
-
 	after = kzalloc(sizeof(*after), GFP_KERNEL);
-	if (!after) {
-		kfree(before);
-		return -ENOMEM;
-	}
+
+	INIT_LIST_HEAD(&before->list);
 	INIT_LIST_HEAD(&after->list);
 
-	/* Now gather all removed mappings */
 	tmp = amdgpu_vm_it_iter_first(&vm->va, saddr, eaddr);
 	while (tmp) {
-		/* Remember mapping split at the start */
-		if (tmp->start < saddr) {
+		if (before && tmp->start < saddr) {
 			before->start = tmp->start;
 			before->last = saddr - 1;
 			before->offset = tmp->offset;
@@ -2000,64 +2078,44 @@ int amdgpu_vm_bo_clear_mappings(struct a
 			before->bo_va = tmp->bo_va;
 			list_add(&before->list, &tmp->bo_va->invalids);
 		}
-
-		/* Remember mapping split at the end */
-		if (tmp->last > eaddr) {
+		if (after && tmp->last > eaddr) {
 			after->start = eaddr + 1;
 			after->last = tmp->last;
-			after->offset = tmp->offset;
-			after->offset += (after->start - tmp->start) << PAGE_SHIFT;
+			after->offset = tmp->offset + ((after->start - tmp->start) << PAGE_SHIFT);
 			after->flags = tmp->flags;
 			after->bo_va = tmp->bo_va;
 			list_add(&after->list, &tmp->bo_va->invalids);
 		}
-
 		list_del(&tmp->list);
 		list_add(&tmp->list, &removed);
-
 		tmp = amdgpu_vm_it_iter_next(tmp, saddr, eaddr);
 	}
 
-	/* And free them up */
 	list_for_each_entry_safe(tmp, next, &removed, list) {
 		amdgpu_vm_it_remove(tmp, &vm->va);
 		list_del(&tmp->list);
-
-		if (tmp->start < saddr)
-		    tmp->start = saddr;
-		if (tmp->last > eaddr)
-		    tmp->last = eaddr;
-
 		tmp->bo_va = NULL;
 		list_add(&tmp->list, &vm->freed);
 		trace_amdgpu_vm_bo_unmap(NULL, tmp);
 	}
 
-	/* Insert partial mapping before the range */
-	if (!list_empty(&before->list)) {
+	if (before && !list_empty(&before->list)) {
 		struct amdgpu_bo *bo = before->bo_va->base.bo;
-
 		amdgpu_vm_it_insert(before, &vm->va);
 		if (before->flags & AMDGPU_PTE_PRT_FLAG(adev))
 			amdgpu_vm_prt_get(adev);
-
-		if (amdgpu_vm_is_bo_always_valid(vm, bo) &&
-		    !before->bo_va->base.moved)
+		if (amdgpu_vm_is_bo_always_valid(vm, bo) && !before->bo_va->base.moved)
 			amdgpu_vm_bo_moved(&before->bo_va->base);
 	} else {
 		kfree(before);
 	}
 
-	/* Insert partial mapping after the range */
-	if (!list_empty(&after->list)) {
+	if (after && !list_empty(&after->list)) {
 		struct amdgpu_bo *bo = after->bo_va->base.bo;
-
 		amdgpu_vm_it_insert(after, &vm->va);
 		if (after->flags & AMDGPU_PTE_PRT_FLAG(adev))
 			amdgpu_vm_prt_get(adev);
-
-		if (amdgpu_vm_is_bo_always_valid(vm, bo) &&
-		    !after->bo_va->base.moved)
+		if (amdgpu_vm_is_bo_always_valid(vm, bo) && !after->bo_va->base.moved)
 			amdgpu_vm_bo_moved(&after->bo_va->base);
 	} else {
 		kfree(after);
@@ -2709,58 +2767,146 @@ static int amdgpu_vm_stats_is_zero(struc
  */
 void amdgpu_vm_fini(struct amdgpu_device *adev, struct amdgpu_vm *vm)
 {
-	struct amdgpu_bo_va_mapping *mapping, *tmp;
+	struct amdgpu_bo_va_mapping *mapping, *tmp_mapping;
+	struct amdgpu_bo_va *bo_va, *next_bo_va;
+	struct list_head bo_vas_to_clean; /* Temp list for unique bo_vas */
 	bool prt_fini_needed = !!adev->gmc.gmc_funcs->set_prt;
 	struct amdgpu_bo *root;
-	unsigned long flags;
+	unsigned long flags; /* For spinlock */
 	int i;
 
+	/* Lists to check for leftover bo_vas */
+	struct list_head *status_lists[] = {
+		&vm->idle, &vm->evicted, &vm->relocated, &vm->moved,
+		&vm->invalidated, &vm->done, &vm->evicted_user, NULL
+	};
+	struct list_head **current_list_ptr;
+
+
 	amdgpu_amdkfd_gpuvm_destroy_cb(adev, vm);
 
 	flush_work(&vm->pt_free_work);
 
-	root = amdgpu_bo_ref(vm->root.bo);
-	amdgpu_bo_reserve(root, true);
-	amdgpu_vm_set_pasid(adev, vm, 0);
-	dma_fence_wait(vm->last_unlocked, false);
-	dma_fence_put(vm->last_unlocked);
-	dma_fence_wait(vm->last_tlb_flush, false);
-	/* Make sure that all fence callbacks have completed */
-	spin_lock_irqsave(vm->last_tlb_flush->lock, flags);
-	spin_unlock_irqrestore(vm->last_tlb_flush->lock, flags);
-	dma_fence_put(vm->last_tlb_flush);
+	/* Initialize the temporary list for cleanup */
+	INIT_LIST_HEAD(&bo_vas_to_clean);
 
-	list_for_each_entry_safe(mapping, tmp, &vm->freed, list) {
-		if (mapping->flags & AMDGPU_PTE_PRT_FLAG(adev) && prt_fini_needed) {
-			amdgpu_vm_prt_fini(adev, vm);
-			prt_fini_needed = false;
+	root = amdgpu_bo_ref(vm->root.bo);
+	if (amdgpu_bo_reserve(root, true) == 0) {
+		amdgpu_vm_set_pasid(adev, vm, 0);
+		dma_fence_wait(vm->last_unlocked, false);
+		dma_fence_wait(vm->last_tlb_flush, false);
+		/* Ensure fence callbacks complete */
+		spin_lock_irqsave(vm->last_tlb_flush->lock, flags);
+		spin_unlock_irqrestore(vm->last_tlb_flush->lock, flags);
+
+		/* Clear remaining freed mappings */
+		list_for_each_entry_safe(mapping, tmp_mapping, &vm->freed, list) {
+			if (mapping->flags & AMDGPU_PTE_PRT_FLAG(adev) && prt_fini_needed) {
+				amdgpu_vm_prt_fini(adev, vm);
+				prt_fini_needed = false;
+			}
+			list_del(&mapping->list);
+			amdgpu_vm_free_mapping(adev, vm, mapping, NULL);
 		}
 
-		list_del(&mapping->list);
-		amdgpu_vm_free_mapping(adev, vm, mapping, NULL);
+		/* Free the page table tree (decrements stats for PT BOs) */
+		amdgpu_vm_pt_free_root(adev, vm);
+		amdgpu_bo_unreserve(root);
+	} else {
+		dev_err(adev->dev, "Failed to reserve VM root BO for fini\n");
 	}
-
-	amdgpu_vm_pt_free_root(adev, vm);
-	amdgpu_bo_unreserve(root);
-	amdgpu_bo_unref(&root);
 	WARN_ON(vm->root.bo);
 
-	amdgpu_vm_fini_entities(vm);
 
+	/* Process remaining mappings in the interval tree if any exist */
 	if (!RB_EMPTY_ROOT(&vm->va.rb_root)) {
-		dev_err(adev->dev, "still active bo inside vm\n");
+		dev_warn(adev->dev, "still active mappings in vm interval tree during fini\n");
+		rbtree_postorder_for_each_entry_safe(mapping, tmp_mapping,
+											 &vm->va.rb_root, rb) {
+			bo_va = mapping->bo_va;
+			if (bo_va) {
+				/* Ensure list head is valid before moving */
+				/* Use list_empty_careful as state might be dubious */
+				if (list_empty_careful(&bo_va->base.vm_status)) {
+					INIT_LIST_HEAD(&bo_va->base.vm_status);
+					/* Add to cleanup list only once */
+					list_add_tail(&bo_va->base.vm_status, &bo_vas_to_clean);
+				} else if (bo_va->base.vm_status.next != NULL) {
+					/* It's on a status list, move it to cleanup list */
+					/* list_move_tail handles uniqueness */
+					list_move_tail(&bo_va->base.vm_status, &bo_vas_to_clean);
+				}
+				list_del_init(&mapping->list);
+				mapping->bo_va = NULL;
+			}
+			kfree(mapping);
+											 }
+											 vm->va = RB_ROOT_CACHED;
 	}
-	rbtree_postorder_for_each_entry_safe(mapping, tmp,
-					     &vm->va.rb_root, rb) {
-		/* Don't remove the mapping here, we don't want to trigger a
-		 * rebalance and the tree is about to be destroyed anyway.
-		 */
-		list_del(&mapping->list);
-		kfree(mapping);
+
+	/* Check all status lists for any remaining bo_vas */
+	spin_lock_irqsave(&vm->status_lock, flags);
+	for (current_list_ptr = status_lists; *current_list_ptr; ++current_list_ptr) {
+		list_for_each_entry_safe(bo_va, next_bo_va, *current_list_ptr, base.vm_status) {
+			/* Move unique bo_va to our temporary cleanup list */
+			list_move_tail(&bo_va->base.vm_status, &bo_vas_to_clean);
+		}
+	}
+	spin_unlock_irqrestore(&vm->status_lock, flags);
+
+
+	/* Clean up all collected bo_vas (from tree and status lists) */
+	list_for_each_entry_safe(bo_va, next_bo_va, &bo_vas_to_clean, base.vm_status) {
+		struct amdgpu_bo *bo = bo_va->base.bo;
+		struct amdgpu_vm_bo_base **base_ptr;
+		bool unlinked = false; // Flag to track if we unlinked it
+
+		list_del_init(&bo_va->base.vm_status); /* Remove from cleanup list */
+
+		if (bo) {
+			/* Try to remove from bo's list (bo->vm_bo) */
+			for (base_ptr = &bo->vm_bo; *base_ptr; base_ptr = &(*base_ptr)->next) {
+				if (*base_ptr == &bo_va->base) {
+					*base_ptr = bo_va->base.next;
+					unlinked = true; // We successfully unlinked it
+					break;
+				}
+			}
+
+			/* === FIX: Only decrement stats if we successfully unlinked it === */
+			if (unlinked) {
+				/* This implies bo_del did not run successfully/fully before */
+				spin_lock_irqsave(&vm->status_lock, flags);
+				amdgpu_vm_update_stats_locked(&bo_va->base, bo->tbo.resource, -1);
+				spin_unlock_irqrestore(&vm->status_lock, flags);
+			}
+			/* If !unlinked, assume bo_del already ran and decremented stats */
+
+			/* Clean up other BO related fields (safe regardless of unlinked status) */
+			if (amdgpu_vm_is_bo_always_valid(vm, bo))
+				ttm_bo_set_bulk_move(&bo->tbo, NULL);
+
+			if (bo_va->is_xgmi)
+				amdgpu_xgmi_set_pstate(adev, AMDGPU_XGMI_PSTATE_MIN);
+		}
+
+		/* Free fences and the bo_va structure */
+		dma_fence_put(bo_va->last_pt_update);
+		kfree(bo_va);
+	}
+
+	/* Handle any remaining PRT callbacks if necessary */
+	if (prt_fini_needed && !list_empty(&vm->freed)) {
+		amdgpu_vm_prt_fini(adev, vm);
 	}
 
+	/* Final VM resource cleanup */
+	dma_fence_put(vm->last_unlocked);
+	dma_fence_put(vm->last_tlb_flush);
 	dma_fence_put(vm->last_update);
 
+	amdgpu_vm_fini_entities(vm);
+
 	for (i = 0; i < AMDGPU_MAX_VMHUBS; i++) {
 		if (vm->reserved_vmid[i]) {
 			amdgpu_vmid_free_reserved(adev, i);
@@ -2772,13 +2918,18 @@ void amdgpu_vm_fini(struct amdgpu_device
 
 	if (!amdgpu_vm_stats_is_zero(vm)) {
 		struct amdgpu_task_info *ti = vm->task_info;
+		const char *pname = ti ? ti->process_name : "(unknown)";
+		int pid = ti ? ti->pid : 0;
+		const char *tname = ti ? ti->task_name : "(unknown)";
+		int tgid = ti ? ti->tgid : 0;
 
 		dev_warn(adev->dev,
-			 "VM memory stats for proc %s(%d) task %s(%d) is non-zero when fini\n",
-			 ti->process_name, ti->pid, ti->task_name, ti->tgid);
+				 "VM memory stats STILL NON-ZERO AFTER DOUBLE DECREMENT CHECK for proc %s(%d) task %s(%d) when fini\n",
+				 pname, pid, tname, tgid);
 	}
 
 	amdgpu_vm_put_task_info(vm->task_info);
+	amdgpu_bo_unref(&root); /* Balance initial ref */
 }
 
 /**




--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c	2025-04-12 13:04:56.516191172 +0200
@@ -44,27 +44,464 @@
 #include "amdgpu_xgmi.h"
 #include "amdgpu_vm.h"
 
+#define AMDGPU_VEGA_HBM2_BANK_SIZE (1ULL * 1024 * 1024)
+#define AMDGPU_VEGA_SMALL_BUFFER_SIZE (1ULL * 1024 * 1024) /* 1MB */
+#define AMDGPU_VEGA_MEDIUM_BUFFER_SIZE (4ULL * 1024 * 1024) /* 4MB */
+#define AMDGPU_VEGA_LARGE_BUFFER_SIZE (16ULL * 1024 * 1024) /* 16MB */
+#define AMDGPU_VEGA_HBM2_MIN_ALIGNMENT (256 * 1024) /* 256KB min alignment for VRAM buffers */
+
+static int amdgpu_vega_vram_pressure_low = 65;
+static int amdgpu_vega_vram_pressure_mid = 75;
+static int amdgpu_vega_vram_pressure_high = 85;
+
+void amdgpu_vega_vram_thresholds_init(void);
+
+module_param_named(vram_pressure_low, amdgpu_vega_vram_pressure_low, int, 0644);
+MODULE_PARM_DESC(vram_pressure_low, "Low VRAM pressure threshold for Vega (65)");
+module_param_named(vram_pressure_mid, amdgpu_vega_vram_pressure_mid, int, 0644);
+MODULE_PARM_DESC(vram_pressure_mid, "Medium VRAM pressure threshold for Vega (75)");
+module_param_named(vram_pressure_high, amdgpu_vega_vram_pressure_high, int, 0644);
+MODULE_PARM_DESC(vram_pressure_high, "High VRAM pressure threshold for Vega (85)");
+
+/* Clamp and order VRAM pressure thresholds - called from main init */
+void amdgpu_vega_vram_thresholds_init(void)
+{
+	amdgpu_vega_vram_pressure_low = clamp(amdgpu_vega_vram_pressure_low, 0, 100);
+	amdgpu_vega_vram_pressure_mid = clamp(amdgpu_vega_vram_pressure_mid, 0, 100);
+	amdgpu_vega_vram_pressure_high = clamp(amdgpu_vega_vram_pressure_high, 0, 100);
+
+	if (amdgpu_vega_vram_pressure_mid < amdgpu_vega_vram_pressure_low)
+		amdgpu_vega_vram_pressure_mid = amdgpu_vega_vram_pressure_low;
+	if (amdgpu_vega_vram_pressure_high < amdgpu_vega_vram_pressure_mid)
+		amdgpu_vega_vram_pressure_high = amdgpu_vega_vram_pressure_mid;
+}
+
+/* Helper macros for buffer category detection */
+static inline bool is_vega_texture(uint64_t flags)
+{
+	return flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;
+}
+static inline bool is_vega_compute(uint64_t flags)
+{
+	return flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS;
+}
+static inline bool is_vega_cpu_access(uint64_t flags)
+{
+	return flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
+}
+static inline bool is_hbm2_vega(struct amdgpu_device *adev)
+{
+	return adev && adev->asic_type == CHIP_VEGA10;
+}
+
+static uint32_t amdgpu_vega_get_vram_usage(struct amdgpu_device *adev)
+{
+	struct ttm_resource_manager *vram_man;
+	uint64_t vram_usage = 0;
+	uint64_t vram_size = 0;
+	uint32_t usage_percent = 0;
+
+	if (!adev || !adev->gmc.mc_vram_size)
+		return 0;
+
+	vram_man = ttm_manager_type(&adev->mman.bdev, TTM_PL_VRAM);
+	if (!vram_man)
+		return 0;
+
+	vram_usage = ttm_resource_manager_usage(vram_man);
+	vram_size = adev->gmc.mc_vram_size;
+
+	if (vram_size)
+		usage_percent = div64_u64(vram_usage * 100, vram_size);
+
+	return usage_percent;
+}
+
+static uint32_t amdgpu_vega_get_effective_vram_usage(struct amdgpu_device *adev)
+{
+	uint32_t usage_percent, effective_percent;
+	struct ttm_resource_manager *vram_man;
+
+	if (!adev)
+		return 0;
+
+	usage_percent = amdgpu_vega_get_vram_usage(adev);
+	effective_percent = usage_percent;
+
+	if (!is_hbm2_vega(adev))
+		return usage_percent;
+
+	vram_man = ttm_manager_type(&adev->mman.bdev, TTM_PL_VRAM);
+	if (!vram_man)
+		return usage_percent;
+
+	/* If TTM is using system memory as fallback (use_tt flag), boost effective usage */
+	if (vram_man->use_tt) {
+		effective_percent = min_t(uint32_t, usage_percent + 10, 100);
+	} else if (usage_percent > amdgpu_vega_vram_pressure_mid) {
+		effective_percent = min_t(uint32_t, usage_percent + 5, 100);
+	}
+
+	return effective_percent;
+}
+
+static bool amdgpu_vega_optimize_buffer_placement(struct amdgpu_device *adev,
+												  struct amdgpu_bo *bo,
+												  uint64_t size,
+												  uint64_t flags,
+												  uint32_t *domain)
+{
+	uint32_t vram_usage;
+
+	if (!is_hbm2_vega(adev) || !domain)
+		return false;
+
+	vram_usage = amdgpu_vega_get_effective_vram_usage(adev);
+
+	/* Only override domain if unset (==0) or under high pressure */
+	#define OVERRIDE_DOMAIN_IF_UNSET_OR_PRESSURE_OR(flag, pressure) \
+	((*domain == 0) || (vram_usage >= (pressure)) || (flags))
+
+	/* Category 1: Textures and framebuffer resources */
+	if (is_vega_texture(flags)) {
+		if (vram_usage >= amdgpu_vega_vram_pressure_high &&
+			size < AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {
+			*domain = (*domain & ~AMDGPU_GEM_DOMAIN_VRAM) | AMDGPU_GEM_DOMAIN_GTT;
+			} else {
+				*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+			}
+			return true;
+	}
+
+	/* Category 2: Compute resources */
+	if (is_vega_compute(flags)) {
+		if (vram_usage >= amdgpu_vega_vram_pressure_high &&
+			size > AMDGPU_VEGA_LARGE_BUFFER_SIZE) {
+			*domain = (*domain & ~AMDGPU_GEM_DOMAIN_VRAM) | AMDGPU_GEM_DOMAIN_GTT;
+			} else {
+				*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+			}
+			return true;
+	}
+
+	/* Category 3: CPU-accessible resources */
+	if (is_vega_cpu_access(flags)) {
+		if (size <= AMDGPU_VEGA_SMALL_BUFFER_SIZE) {
+			if (vram_usage >= amdgpu_vega_vram_pressure_high) {
+				*domain = (*domain & ~AMDGPU_GEM_DOMAIN_VRAM) | AMDGPU_GEM_DOMAIN_GTT;
+			} else if (*domain == 0) {
+				*domain |= AMDGPU_GEM_DOMAIN_GTT;
+			}
+			return true;
+		}
+		if (size <= AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {
+			if (vram_usage >= amdgpu_vega_vram_pressure_high) {
+				*domain = (*domain & ~AMDGPU_GEM_DOMAIN_VRAM) | AMDGPU_GEM_DOMAIN_GTT;
+			} else if (*domain == 0) {
+				*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+			}
+			return true;
+		}
+		if (size > AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {
+			if (vram_usage >= amdgpu_vega_vram_pressure_high) {
+				*domain = (*domain & ~AMDGPU_GEM_DOMAIN_VRAM) | AMDGPU_GEM_DOMAIN_GTT;
+			} else if (*domain == 0) {
+				if (size > AMDGPU_VEGA_LARGE_BUFFER_SIZE)
+					*domain |= AMDGPU_GEM_DOMAIN_GTT;
+				else
+					*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+			}
+			return true;
+		}
+	}
+
+	/* Category 4: Generic resources */
+	if (size <= AMDGPU_VEGA_SMALL_BUFFER_SIZE) {
+		if (vram_usage >= amdgpu_vega_vram_pressure_high)
+			*domain = (*domain & ~AMDGPU_GEM_DOMAIN_VRAM) | AMDGPU_GEM_DOMAIN_GTT;
+		else if (*domain == 0)
+			*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+		return true;
+	}
+	if (size <= AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {
+		if (vram_usage >= amdgpu_vega_vram_pressure_high)
+			*domain = (*domain & ~AMDGPU_GEM_DOMAIN_VRAM) | AMDGPU_GEM_DOMAIN_GTT;
+		else if (*domain == 0)
+			*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+		return true;
+	}
+	if (vram_usage >= amdgpu_vega_vram_pressure_high) {
+		*domain = (*domain & ~AMDGPU_GEM_DOMAIN_VRAM) | AMDGPU_GEM_DOMAIN_GTT;
+	} else if (*domain == 0) {
+		*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+	}
+	return true;
+}
+
+static bool amdgpu_vega_optimize_hbm2_bank_access(struct amdgpu_device *adev,
+												  struct amdgpu_bo *bo,
+												  uint64_t *aligned_size,
+												  uint32_t *alignment)
+{
+	if (!is_hbm2_vega(adev) || !aligned_size || !alignment)
+		return false;
+
+	if (*aligned_size == 0 || *aligned_size > (16ULL * 1024 * 1024 * 1024))
+		return false;
+
+	/* Use at least 1MB for 128MB+, 256KB for 4MB+ */
+	if (*aligned_size >= 128ULL * 1024 * 1024) {
+		*alignment = max_t(uint32_t, *alignment, 1 * 1024 * 1024);
+		*aligned_size = ALIGN(*aligned_size, 1 * 1024 * 1024);
+		return true;
+	}
+	if (*aligned_size >= AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {
+		*alignment = max_t(uint32_t, *alignment, AMDGPU_VEGA_HBM2_MIN_ALIGNMENT);
+		*aligned_size = ALIGN(*aligned_size, AMDGPU_VEGA_HBM2_MIN_ALIGNMENT);
+		return true;
+	}
+	/* Textures: prefer 4K alignment if medium+ */
+	if (bo && bo->tbo.base.size > 0 && is_vega_texture(bo->flags)) {
+		if (*aligned_size >= AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {
+			*alignment = max_t(uint32_t, *alignment, 4096);
+			*aligned_size = ALIGN(*aligned_size, 4096);
+			return true;
+		}
+	}
+	/* Compute: prefer 8K alignment if large+ */
+	if (bo && bo->tbo.base.size > 0 && is_vega_compute(bo->flags)) {
+		if (*aligned_size >= AMDGPU_VEGA_LARGE_BUFFER_SIZE) {
+			*alignment = max_t(uint32_t, *alignment, 8192);
+			*aligned_size = ALIGN(*aligned_size, 8192);
+			return true;
+		}
+	}
+	/* Default for buffer creation: use at least 4K if medium+ */
+	if (!bo && *aligned_size >= AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {
+		*alignment = max_t(uint32_t, *alignment, 4096);
+		*aligned_size = ALIGN(*aligned_size, 4096);
+		return true;
+	}
+	return false;
+}
+
+static unsigned int amdgpu_vega_determine_optimal_prefetch(
+	struct amdgpu_device *adev,
+	struct amdgpu_bo *bo,
+	unsigned int base_prefetch_pages,
+	uint32_t vram_usage)
+{
+	unsigned int prefetch_pages = base_prefetch_pages;
+	uint64_t size;
+	bool is_vram;
+	unsigned int max_pages;
+
+	if (!is_hbm2_vega(adev) || !bo)
+		return base_prefetch_pages;
+
+	size = amdgpu_bo_size(bo);
+	if (size == 0)
+		return base_prefetch_pages;
+
+	is_vram = (bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) != 0;
+
+	max_pages = DIV_ROUND_UP(size, PAGE_SIZE);
+
+	if (vram_usage > amdgpu_vega_vram_pressure_high)
+		return min(max_t(unsigned int, base_prefetch_pages / 2, 8), max_pages);
+
+	if (is_vram && is_vega_compute(bo->flags)) {
+		if (size > AMDGPU_VEGA_LARGE_BUFFER_SIZE)
+			return min_t(unsigned int, base_prefetch_pages * 2, min(128, max_pages));
+	}
+
+	if (is_vram && is_vega_texture(bo->flags)) {
+		if (vram_usage < amdgpu_vega_vram_pressure_mid)
+			return min_t(unsigned int, base_prefetch_pages * 6 / 5, min(64, max_pages));
+	}
+
+	return min(prefetch_pages, max_pages);
+}
+
+static bool amdgpu_vega_should_use_async_fence(struct amdgpu_device *adev,
+											   struct amdgpu_bo *bo,
+											   uint64_t flags)
+{
+	uint64_t size;
+
+	if (!is_hbm2_vega(adev) || !bo)
+		return false;
+
+	size = amdgpu_bo_size(bo);
+	if (size == 0)
+		return false;
+
+	/* Never async for explicit sync or large buffers (>32MB) */
+	if ((flags & AMDGPU_GEM_CREATE_EXPLICIT_SYNC) || size > (32ULL << 20))
+		return false;
+
+	if ((bo->preferred_domains & AMDGPU_GEM_DOMAIN_GTT) &&
+		is_vega_cpu_access(flags) &&
+		size < AMDGPU_VEGA_SMALL_BUFFER_SIZE) {
+		return true;
+		}
+		if ((bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) &&
+			is_vega_compute(flags) &&
+			!is_vega_cpu_access(flags)) {
+			return true;
+			}
+			return false;
+}
+
+static bool amdgpu_vega_set_compute_placement(struct amdgpu_device *adev,
+											  struct amdgpu_bo *bo,
+											  uint64_t size,
+											  uint32_t *domain)
+{
+	uint32_t vram_usage;
+
+	if (!is_hbm2_vega(adev) || !bo || !domain)
+		return false;
+
+	lockdep_assert_held(bo->tbo.base.resv);
+
+	if (!is_vega_compute(bo->flags))
+		return false;
+
+	vram_usage = amdgpu_vega_get_effective_vram_usage(adev);
+
+	/* Fallback to GTT only when pressure is catastrophic */
+	if (size > AMDGPU_VEGA_LARGE_BUFFER_SIZE) {
+		if (vram_usage < amdgpu_vega_vram_pressure_low) {
+			*domain = AMDGPU_GEM_DOMAIN_VRAM;
+			bo->allowed_domains = AMDGPU_GEM_DOMAIN_VRAM | AMDGPU_GEM_DOMAIN_GTT;
+			return true;
+		}
+	} else if (size > AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {
+		if (vram_usage < amdgpu_vega_vram_pressure_mid) {
+			*domain = AMDGPU_GEM_DOMAIN_VRAM;
+			bo->allowed_domains = AMDGPU_GEM_DOMAIN_VRAM | AMDGPU_GEM_DOMAIN_GTT;
+			return true;
+		}
+	} else {
+		*domain = AMDGPU_GEM_DOMAIN_VRAM;
+		bo->allowed_domains = AMDGPU_GEM_DOMAIN_VRAM | AMDGPU_GEM_DOMAIN_GTT;
+		return true;
+	}
+	return false;
+}
+
+static void amdgpu_vega_set_buffer_domains(struct amdgpu_device *adev,
+										   struct amdgpu_bo *bo)
+{
+	if (!is_hbm2_vega(adev) || !bo)
+		return;
+
+	lockdep_assert_held(bo->tbo.base.resv);
+
+	/* Always provide GTT as a fallback for VRAM-only buffers */
+	if (bo->preferred_domains == AMDGPU_GEM_DOMAIN_VRAM) {
+		bo->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
+	}
+
+	/* Compute: prefer VRAM, always allow GTT as fallback */
+	if (is_vega_compute(bo->flags)) {
+		if (bo->allowed_domains & AMDGPU_GEM_DOMAIN_VRAM)
+			bo->preferred_domains |= AMDGPU_GEM_DOMAIN_VRAM;
+		bo->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
+	}
+
+	/* CPU-access: always allow GTT */
+	if (is_vega_cpu_access(bo->flags)) {
+		bo->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
+	}
+}
+
+static bool amdgpu_vega_optimize_for_workload(struct amdgpu_device *adev,
+											  struct amdgpu_bo *bo,
+											  uint64_t flags)
+{
+	uint64_t size;
+
+	if (!is_hbm2_vega(adev) || !bo)
+		return false;
+
+	if (!bo->tbo.base.dev)
+		return false;
+
+	size = amdgpu_bo_size(bo);
+	if (size == 0)
+		return false;
+
+	if (!dma_resv_is_locked(bo->tbo.base.resv))
+		return false;
+
+	/* Gaming workload: prioritize VRAM for textures/framebuffers, fallback to GTT */
+	if (is_vega_texture(flags) && size >= AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {
+		bo->preferred_domains = AMDGPU_GEM_DOMAIN_VRAM;
+		bo->allowed_domains = AMDGPU_GEM_DOMAIN_VRAM | AMDGPU_GEM_DOMAIN_GTT;
+		return true;
+	}
+
+	/* Compute workload: prefer VRAM for compute buffers */
+	if (is_vega_compute(flags) && !is_vega_cpu_access(flags)) {
+		bo->preferred_domains = AMDGPU_GEM_DOMAIN_VRAM;
+		bo->allowed_domains = AMDGPU_GEM_DOMAIN_VRAM | AMDGPU_GEM_DOMAIN_GTT;
+		return true;
+	}
+
+	/* API translation layers: prefer GTT for small CPU-accessible buffers */
+	if (is_vega_cpu_access(flags) && size <= AMDGPU_VEGA_SMALL_BUFFER_SIZE) {
+		bo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;
+		bo->allowed_domains = AMDGPU_GEM_DOMAIN_GTT | AMDGPU_GEM_DOMAIN_VRAM;
+		return true;
+	}
+
+	return false;
+}
+
 static vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)
 {
-	struct ttm_buffer_object *bo = vmf->vma->vm_private_data;
-	struct drm_device *ddev = bo->base.dev;
+	struct ttm_buffer_object *bo;
+	struct drm_device *ddev;
 	vm_fault_t ret;
 	int idx;
 
+	bo = vmf->vma->vm_private_data;
+	if (unlikely(!bo))
+		return VM_FAULT_SIGBUS;
+
+	ddev = bo->base.dev;
+	if (unlikely(!ddev))
+		return VM_FAULT_SIGBUS;
+
 	ret = ttm_bo_vm_reserve(bo, vmf);
-	if (ret)
+	if (unlikely(ret))
 		return ret;
 
 	if (drm_dev_enter(ddev, &idx)) {
+		struct amdgpu_device *adev = drm_to_adev(ddev);
+
 		ret = amdgpu_bo_fault_reserve_notify(bo);
-		if (ret) {
+		if (unlikely(ret)) {
 			drm_dev_exit(idx);
 			goto unlock;
 		}
 
-		ret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot,
-					       TTM_BO_VM_NUM_PREFAULT);
+		if (is_hbm2_vega(adev)) {
+			struct amdgpu_bo *abo = ttm_to_amdgpu_bo(bo);
+			unsigned int prefetch_pages = TTM_BO_VM_NUM_PREFAULT;
+
+			if (abo) {
+				uint32_t vram_usage = amdgpu_vega_get_effective_vram_usage(adev);
+				prefetch_pages = amdgpu_vega_determine_optimal_prefetch(
+					adev, abo, TTM_BO_VM_NUM_PREFAULT, vram_usage);
+			}
 
+			ret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot, prefetch_pages);
+		} else {
+			ret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot, TTM_BO_VM_NUM_PREFAULT);
+		}
 		drm_dev_exit(idx);
 	} else {
 		ret = ttm_bo_vm_dummy_page(vmf, vmf->vma->vm_page_prot);
@@ -72,43 +509,67 @@ static vm_fault_t amdgpu_gem_fault(struc
 	if (ret == VM_FAULT_RETRY && !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT))
 		return ret;
 
-unlock:
+	unlock:
 	dma_resv_unlock(bo->base.resv);
 	return ret;
 }
 
+/* VM operations struct for GEM objects */
 static const struct vm_operations_struct amdgpu_gem_vm_ops = {
 	.fault = amdgpu_gem_fault,
 	.open = ttm_bo_vm_open,
 	.close = ttm_bo_vm_close,
-	.access = ttm_bo_vm_access
+	.access = ttm_bo_vm_access,
 };
 
+/* Free a GEM object */
 static void amdgpu_gem_object_free(struct drm_gem_object *gobj)
 {
 	struct amdgpu_bo *aobj = gem_to_amdgpu_bo(gobj);
 
-	amdgpu_hmm_unregister(aobj);
-	ttm_bo_put(&aobj->tbo);
+	if (aobj) {
+		amdgpu_hmm_unregister(aobj);
+		ttm_bo_put(&aobj->tbo);
+		/* No need to NULL aobj, it's on the stack */
+	}
 }
 
+/* Create a new GEM object */
 int amdgpu_gem_object_create(struct amdgpu_device *adev, unsigned long size,
-			     int alignment, u32 initial_domain,
-			     u64 flags, enum ttm_bo_type type,
-			     struct dma_resv *resv,
-			     struct drm_gem_object **obj, int8_t xcp_id_plus1)
+							 int alignment, u32 initial_domain,
+							 u64 flags, enum ttm_bo_type type,
+							 struct dma_resv *resv,
+							 struct drm_gem_object **obj, int8_t xcp_id_plus1)
 {
 	struct amdgpu_bo *bo;
 	struct amdgpu_bo_user *ubo;
 	struct amdgpu_bo_param bp;
 	int r;
 
+	if (!adev || !obj)
+		return -EINVAL;
+
 	memset(&bp, 0, sizeof(bp));
 	*obj = NULL;
 	flags |= AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE;
 
+	/* Vega/HBM2 buffer placement optimization */
+	if (is_hbm2_vega(adev)) {
+		amdgpu_vega_optimize_buffer_placement(adev, NULL, (uint64_t)size, flags, &initial_domain);
+	}
+
 	bp.size = size;
 	bp.byte_align = alignment;
+
+	if (is_hbm2_vega(adev)) {
+		uint64_t temp_size = (uint64_t)bp.size;
+		uint32_t temp_align = bp.byte_align;
+		if (amdgpu_vega_optimize_hbm2_bank_access(adev, NULL, &temp_size, &temp_align)) {
+			bp.size = (unsigned long)temp_size;
+			bp.byte_align = temp_align;
+		}
+	}
+
 	bp.type = type;
 	bp.resv = resv;
 	bp.preferred_domain = initial_domain;
@@ -122,11 +583,20 @@ int amdgpu_gem_object_create(struct amdg
 		return r;
 
 	bo = &ubo->bo;
-	*obj = &bo->tbo.base;
 
+	if (is_hbm2_vega(adev)) {
+		if (is_vega_compute(bo->flags)) {
+			amdgpu_vega_set_compute_placement(adev, bo, (uint64_t)size, &bo->preferred_domains);
+		}
+		amdgpu_vega_optimize_for_workload(adev, bo, flags);
+		amdgpu_vega_set_buffer_domains(adev, bo);
+	}
+
+	*obj = &bo->tbo.base;
 	return 0;
 }
 
+/* Force release of all GEM objects for a device */
 void amdgpu_gem_force_release(struct amdgpu_device *adev)
 {
 	struct drm_device *ddev = adev_to_drm(adev);
@@ -151,64 +621,85 @@ void amdgpu_gem_force_release(struct amd
 	mutex_unlock(&ddev->filelist_mutex);
 }
 
-/*
- * Call from drm_gem_handle_create which appear in both new and open ioctl
- * case.
- */
+/* Open a GEM object for a file descriptor */
 static int amdgpu_gem_object_open(struct drm_gem_object *obj,
-				  struct drm_file *file_priv)
+								  struct drm_file *file_priv)
 {
-	struct amdgpu_bo *abo = gem_to_amdgpu_bo(obj);
-	struct amdgpu_device *adev = amdgpu_ttm_adev(abo->tbo.bdev);
-	struct amdgpu_fpriv *fpriv = file_priv->driver_priv;
-	struct amdgpu_vm *vm = &fpriv->vm;
+	struct amdgpu_bo *abo;
+	struct amdgpu_device *adev;
+	struct amdgpu_fpriv *fpriv;
+	struct amdgpu_vm *vm;
 	struct amdgpu_bo_va *bo_va;
 	struct mm_struct *mm;
 	int r;
 
+	if (!obj || !file_priv)
+		return -EINVAL;
+
+	abo = gem_to_amdgpu_bo(obj);
+	if (!abo)
+		return -EINVAL;
+
+	adev = amdgpu_ttm_adev(abo->tbo.bdev);
+	if (!adev)
+		return -EINVAL;
+
+	fpriv = file_priv->driver_priv;
+	if (!fpriv)
+		return -EINVAL;
+
+	vm = &fpriv->vm;
+
 	mm = amdgpu_ttm_tt_get_usermm(abo->tbo.ttm);
 	if (mm && mm != current->mm)
 		return -EPERM;
 
-	if (abo->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID &&
-	    !amdgpu_vm_is_bo_always_valid(vm, abo))
+	if ((abo->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) &&
+		!amdgpu_vm_is_bo_always_valid(vm, abo))
 		return -EPERM;
 
 	r = amdgpu_bo_reserve(abo, false);
 	if (r)
 		return r;
 
-	amdgpu_vm_bo_update_shared(abo);
 	bo_va = amdgpu_vm_bo_find(vm, abo);
-	if (!bo_va)
+	if (!bo_va) {
+		amdgpu_vm_bo_update_shared(abo);
 		bo_va = amdgpu_vm_bo_add(adev, vm, abo);
-	else
+	} else {
+		amdgpu_vm_bo_update_shared(abo);
 		++bo_va->ref_count;
+	}
 	amdgpu_bo_unreserve(abo);
 
-	/* Validate and add eviction fence to DMABuf imports with dynamic
-	 * attachment in compute VMs. Re-validation will be done by
-	 * amdgpu_vm_validate. Fences are on the reservation shared with the
-	 * export, which is currently required to be validated and fenced
-	 * already by amdgpu_amdkfd_gpuvm_restore_process_bos.
-	 *
-	 * Nested locking below for the case that a GEM object is opened in
-	 * kfd_mem_export_dmabuf. Since the lock below is only taken for imports,
-	 * but not for export, this is a different lock class that cannot lead to
-	 * circular lock dependencies.
-	 */
 	if (!vm->is_compute_context || !vm->process_info)
 		return 0;
 	if (!obj->import_attach ||
-	    !dma_buf_is_dynamic(obj->import_attach->dmabuf))
+		!dma_buf_is_dynamic(obj->import_attach->dmabuf))
 		return 0;
+
 	mutex_lock_nested(&vm->process_info->lock, 1);
+	lockdep_assert_held(&vm->process_info->lock);
+
 	if (!WARN_ON(!vm->process_info->eviction_fence)) {
-		r = amdgpu_amdkfd_bo_validate_and_fence(abo, AMDGPU_GEM_DOMAIN_GTT,
-							&vm->process_info->eviction_fence->base);
+		if (is_hbm2_vega(adev)) {
+			uint32_t domain = AMDGPU_GEM_DOMAIN_GTT;
+			if (is_vega_texture(abo->flags) || is_vega_compute(abo->flags)) {
+				domain = AMDGPU_GEM_DOMAIN_VRAM;
+				if (amdgpu_vega_get_effective_vram_usage(adev) > amdgpu_vega_vram_pressure_high) {
+					domain = AMDGPU_GEM_DOMAIN_GTT;
+				}
+			}
+			r = amdgpu_amdkfd_bo_validate_and_fence(abo, domain,
+													&vm->process_info->eviction_fence->base);
+		} else {
+			r = amdgpu_amdkfd_bo_validate_and_fence(abo,
+													AMDGPU_GEM_DOMAIN_GTT,
+										   &vm->process_info->eviction_fence->base);
+		}
+
 		if (r) {
 			struct amdgpu_task_info *ti = amdgpu_vm_get_task_info_vm(vm);
-
 			dev_warn(adev->dev, "validate_and_fence failed: %d\n", r);
 			if (ti) {
 				dev_warn(adev->dev, "pid %d\n", ti->pid);
@@ -221,18 +712,40 @@ static int amdgpu_gem_object_open(struct
 	return r;
 }
 
+/* Close a GEM object for a file descriptor */
 static void amdgpu_gem_object_close(struct drm_gem_object *obj,
-				    struct drm_file *file_priv)
+									struct drm_file *file_priv)
 {
-	struct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);
-	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);
-	struct amdgpu_fpriv *fpriv = file_priv->driver_priv;
-	struct amdgpu_vm *vm = &fpriv->vm;
-
+	struct amdgpu_bo *bo;
+	struct amdgpu_device *adev;
+	struct amdgpu_fpriv *fpriv;
+	struct amdgpu_vm *vm;
 	struct dma_fence *fence = NULL;
 	struct amdgpu_bo_va *bo_va;
 	struct drm_exec exec;
-	long r;
+	long r = 0;
+	bool use_async = false;
+
+	if (!obj || !file_priv)
+		return;
+
+	bo = gem_to_amdgpu_bo(obj);
+	if (!bo)
+		return;
+
+	adev = amdgpu_ttm_adev(bo->tbo.bdev);
+	if (!adev)
+		return;
+
+	fpriv = file_priv->driver_priv;
+	if (!fpriv)
+		return;
+
+	vm = &fpriv->vm;
+
+	if (is_hbm2_vega(adev)) {
+		use_async = amdgpu_vega_should_use_async_fence(adev, bo, bo->flags);
+	}
 
 	drm_exec_init(&exec, DRM_EXEC_IGNORE_DUPLICATES, 0);
 	drm_exec_until_all_locked(&exec) {
@@ -248,27 +761,34 @@ static void amdgpu_gem_object_close(stru
 	}
 
 	bo_va = amdgpu_vm_bo_find(vm, bo);
-	if (!bo_va || --bo_va->ref_count)
+	if (!bo_va)
+		goto out_unlock;
+
+	if (--bo_va->ref_count > 0)
 		goto out_unlock;
 
 	amdgpu_vm_bo_del(adev, bo_va);
 	amdgpu_vm_bo_update_shared(bo);
+
 	if (!amdgpu_vm_ready(vm))
 		goto out_unlock;
 
 	r = amdgpu_vm_clear_freed(adev, vm, &fence);
-	if (unlikely(r < 0))
-		dev_err(adev->dev, "failed to clear page "
-			"tables on GEM object close (%ld)\n", r);
+	if (unlikely(r < 0)) {
+		dev_err(adev->dev, "failed to clear page tables on GEM object close (%ld)\n", r);
+		goto out_unlock;
+	}
 	if (r || !fence)
 		goto out_unlock;
 
-	amdgpu_bo_fence(bo, fence, true);
+	amdgpu_bo_fence(bo, fence, use_async);
 	dma_fence_put(fence);
 
-out_unlock:
-	if (r)
-		dev_err(adev->dev, "leaking bo va (%ld)\n", r);
+	out_unlock:
+	if (r) {
+		dev_err(adev->dev, "Error in GEM object close for pid %d, potential leak of bo_va (%ld)\n",
+				task_pid_nr(current), r);
+	}
 	drm_exec_fini(&exec);
 }
 
@@ -281,13 +801,8 @@ static int amdgpu_gem_object_mmap(struct
 	if (bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)
 		return -EPERM;
 
-	/* Workaround for Thunk bug creating PROT_NONE,MAP_PRIVATE mappings
-	 * for debugger access to invisible VRAM. Should have used MAP_SHARED
-	 * instead. Clearing VM_MAYWRITE prevents the mapping from ever
-	 * becoming writable and makes is_cow_mapping(vm_flags) false.
-	 */
 	if (is_cow_mapping(vma->vm_flags) &&
-	    !(vma->vm_flags & VM_ACCESS_FLAGS))
+		!(vma->vm_flags & VM_ACCESS_FLAGS))
 		vm_flags_clear(vma, VM_MAYWRITE);
 
 	return drm_gem_ttm_mmap(obj, vma);
@@ -308,7 +823,7 @@ const struct drm_gem_object_funcs amdgpu
  * GEM ioctls.
  */
 int amdgpu_gem_create_ioctl(struct drm_device *dev, void *data,
-			    struct drm_file *filp)
+							struct drm_file *filp)
 {
 	struct amdgpu_device *adev = drm_to_adev(dev);
 	struct amdgpu_fpriv *fpriv = filp->driver_priv;
@@ -321,23 +836,20 @@ int amdgpu_gem_create_ioctl(struct drm_d
 	uint32_t handle, initial_domain;
 	int r;
 
-	/* reject DOORBELLs until userspace code to use it is available */
 	if (args->in.domains & AMDGPU_GEM_DOMAIN_DOORBELL)
 		return -EINVAL;
 
-	/* reject invalid gem flags */
 	if (flags & ~(AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |
-		      AMDGPU_GEM_CREATE_NO_CPU_ACCESS |
-		      AMDGPU_GEM_CREATE_CPU_GTT_USWC |
-		      AMDGPU_GEM_CREATE_VRAM_CLEARED |
-		      AMDGPU_GEM_CREATE_VM_ALWAYS_VALID |
-		      AMDGPU_GEM_CREATE_EXPLICIT_SYNC |
-		      AMDGPU_GEM_CREATE_ENCRYPTED |
-		      AMDGPU_GEM_CREATE_GFX12_DCC |
-		      AMDGPU_GEM_CREATE_DISCARDABLE))
+		AMDGPU_GEM_CREATE_NO_CPU_ACCESS |
+		AMDGPU_GEM_CREATE_CPU_GTT_USWC |
+		AMDGPU_GEM_CREATE_VRAM_CLEARED |
+		AMDGPU_GEM_CREATE_VM_ALWAYS_VALID |
+		AMDGPU_GEM_CREATE_EXPLICIT_SYNC |
+		AMDGPU_GEM_CREATE_ENCRYPTED |
+		AMDGPU_GEM_CREATE_GFX12_DCC |
+		AMDGPU_GEM_CREATE_DISCARDABLE))
 		return -EINVAL;
 
-	/* reject invalid gem domains */
 	if (args->in.domains & ~AMDGPU_GEM_DOMAIN_MASK)
 		return -EINVAL;
 
@@ -346,62 +858,54 @@ int amdgpu_gem_create_ioctl(struct drm_d
 		return -EINVAL;
 	}
 
-	/* always clear VRAM */
 	flags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;
 
-	/* create a gem object to contain this object in */
 	if (args->in.domains & (AMDGPU_GEM_DOMAIN_GDS |
-	    AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA)) {
+		AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA)) {
 		if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
-			/* if gds bo is created from user space, it must be
-			 * passed to bo list
-			 */
 			DRM_ERROR("GDS bo cannot be per-vm-bo\n");
 			return -EINVAL;
 		}
 		flags |= AMDGPU_GEM_CREATE_NO_CPU_ACCESS;
-	}
-
-	if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
-		r = amdgpu_bo_reserve(vm->root.bo, false);
-		if (r)
-			return r;
-
-		resv = vm->root.bo->tbo.base.resv;
-	}
-
-	initial_domain = (u32)(0xffffffff & args->in.domains);
-retry:
-	r = amdgpu_gem_object_create(adev, size, args->in.alignment,
-				     initial_domain,
-				     flags, ttm_bo_type_device, resv, &gobj, fpriv->xcp_id + 1);
-	if (r && r != -ERESTARTSYS) {
-		if (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {
-			flags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
-			goto retry;
 		}
 
-		if (initial_domain == AMDGPU_GEM_DOMAIN_VRAM) {
-			initial_domain |= AMDGPU_GEM_DOMAIN_GTT;
-			goto retry;
+		if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
+			r = amdgpu_bo_reserve(vm->root.bo, false);
+			if (r)
+				return r;
+			resv = vm->root.bo->tbo.base.resv;
 		}
-		DRM_DEBUG("Failed to allocate GEM object (%llu, %d, %llu, %d)\n",
-				size, initial_domain, args->in.alignment, r);
-	}
 
-	if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
-		if (!r) {
-			struct amdgpu_bo *abo = gem_to_amdgpu_bo(gobj);
+		initial_domain = (u32)(0xffffffff & args->in.domains);
 
-			abo->parent = amdgpu_bo_ref(vm->root.bo);
+		retry:
+		r = amdgpu_gem_object_create(adev, size, args->in.alignment,
+									 initial_domain, flags, ttm_bo_type_device,
+							   resv, &gobj, fpriv->xcp_id + 1);
+		if (r && r != -ERESTARTSYS) {
+			if (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {
+				flags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
+				goto retry;
+			}
+			if (initial_domain == AMDGPU_GEM_DOMAIN_VRAM) {
+				initial_domain |= AMDGPU_GEM_DOMAIN_GTT;
+				goto retry;
+			}
+			DRM_DEBUG("Failed to allocate GEM object (%llu, %d, %llu, %d)\n",
+					  size, initial_domain, args->in.alignment, r);
 		}
-		amdgpu_bo_unreserve(vm->root.bo);
-	}
-	if (r)
-		return r;
+
+		if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
+			if (!r) {
+				struct amdgpu_bo *abo = gem_to_amdgpu_bo(gobj);
+				abo->parent = amdgpu_bo_ref(vm->root.bo);
+			}
+			amdgpu_bo_unreserve(vm->root.bo);
+		}
+		if (r)
+			return r;
 
 	r = drm_gem_handle_create(filp, gobj, &handle);
-	/* drop reference from allocate - handle holds it now */
 	drm_gem_object_put(gobj);
 	if (r)
 		return r;
@@ -412,7 +916,7 @@ retry:
 }
 
 int amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,
-			     struct drm_file *filp)
+							 struct drm_file *filp)
 {
 	struct ttm_operation_ctx ctx = { true, false };
 	struct amdgpu_device *adev = drm_to_adev(dev);
@@ -429,24 +933,20 @@ int amdgpu_gem_userptr_ioctl(struct drm_
 	if (offset_in_page(args->addr | args->size))
 		return -EINVAL;
 
-	/* reject unknown flag values */
 	if (args->flags & ~(AMDGPU_GEM_USERPTR_READONLY |
-	    AMDGPU_GEM_USERPTR_ANONONLY | AMDGPU_GEM_USERPTR_VALIDATE |
-	    AMDGPU_GEM_USERPTR_REGISTER))
+		AMDGPU_GEM_USERPTR_ANONONLY | AMDGPU_GEM_USERPTR_VALIDATE |
+		AMDGPU_GEM_USERPTR_REGISTER))
 		return -EINVAL;
 
 	if (!(args->flags & AMDGPU_GEM_USERPTR_READONLY) &&
-	     !(args->flags & AMDGPU_GEM_USERPTR_REGISTER)) {
-
-		/* if we want to write to it we must install a MMU notifier */
+		!(args->flags & AMDGPU_GEM_USERPTR_REGISTER)) {
 		return -EACCES;
-	}
+		}
 
-	/* create a gem object to contain this object in */
-	r = amdgpu_gem_object_create(adev, args->size, 0, AMDGPU_GEM_DOMAIN_CPU,
-				     0, ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
-	if (r)
-		return r;
+		r = amdgpu_gem_object_create(adev, args->size, 0, AMDGPU_GEM_DOMAIN_CPU,
+									 0, ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
+		if (r)
+			return r;
 
 	bo = gem_to_amdgpu_bo(gobj);
 	bo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;
@@ -460,8 +960,7 @@ int amdgpu_gem_userptr_ioctl(struct drm_
 		goto release_object;
 
 	if (args->flags & AMDGPU_GEM_USERPTR_VALIDATE) {
-		r = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages,
-						 &range);
+		r = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages, &range);
 		if (r)
 			goto release_object;
 
@@ -482,19 +981,19 @@ int amdgpu_gem_userptr_ioctl(struct drm_
 
 	args->handle = handle;
 
-user_pages_done:
+	user_pages_done:
 	if (args->flags & AMDGPU_GEM_USERPTR_VALIDATE)
 		amdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, range);
 
-release_object:
+	release_object:
 	drm_gem_object_put(gobj);
 
 	return r;
 }
 
 int amdgpu_mode_dumb_mmap(struct drm_file *filp,
-			  struct drm_device *dev,
-			  uint32_t handle, uint64_t *offset_p)
+						  struct drm_device *dev,
+						  uint32_t handle, uint64_t *offset_p)
 {
 	struct drm_gem_object *gobj;
 	struct amdgpu_bo *robj;
@@ -505,17 +1004,17 @@ int amdgpu_mode_dumb_mmap(struct drm_fil
 
 	robj = gem_to_amdgpu_bo(gobj);
 	if (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm) ||
-	    (robj->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {
+		(robj->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {
 		drm_gem_object_put(gobj);
-		return -EPERM;
-	}
-	*offset_p = amdgpu_bo_mmap_offset(robj);
-	drm_gem_object_put(gobj);
-	return 0;
+	return -EPERM;
+		}
+		*offset_p = amdgpu_bo_mmap_offset(robj);
+		drm_gem_object_put(gobj);
+		return 0;
 }
 
 int amdgpu_gem_mmap_ioctl(struct drm_device *dev, void *data,
-			  struct drm_file *filp)
+						  struct drm_file *filp)
 {
 	union drm_amdgpu_gem_mmap *args = data;
 	uint32_t handle = args->in.handle;
@@ -524,36 +1023,8 @@ int amdgpu_gem_mmap_ioctl(struct drm_dev
 	return amdgpu_mode_dumb_mmap(filp, dev, handle, &args->out.addr_ptr);
 }
 
-/**
- * amdgpu_gem_timeout - calculate jiffies timeout from absolute value
- *
- * @timeout_ns: timeout in ns
- *
- * Calculate the timeout in jiffies from an absolute timeout in ns.
- */
-unsigned long amdgpu_gem_timeout(uint64_t timeout_ns)
-{
-	unsigned long timeout_jiffies;
-	ktime_t timeout;
-
-	/* clamp timeout if it's to large */
-	if (((int64_t)timeout_ns) < 0)
-		return MAX_SCHEDULE_TIMEOUT;
-
-	timeout = ktime_sub(ns_to_ktime(timeout_ns), ktime_get());
-	if (ktime_to_ns(timeout) < 0)
-		return 0;
-
-	timeout_jiffies = nsecs_to_jiffies(ktime_to_ns(timeout));
-	/*  clamp timeout to avoid unsigned-> signed overflow */
-	if (timeout_jiffies > MAX_SCHEDULE_TIMEOUT)
-		return MAX_SCHEDULE_TIMEOUT - 1;
-
-	return timeout_jiffies;
-}
-
 int amdgpu_gem_wait_idle_ioctl(struct drm_device *dev, void *data,
-			      struct drm_file *filp)
+							   struct drm_file *filp)
 {
 	union drm_amdgpu_gem_wait_idle *args = data;
 	struct drm_gem_object *gobj;
@@ -569,12 +1040,8 @@ int amdgpu_gem_wait_idle_ioctl(struct dr
 
 	robj = gem_to_amdgpu_bo(gobj);
 	ret = dma_resv_wait_timeout(robj->tbo.base.resv, DMA_RESV_USAGE_READ,
-				    true, timeout);
+								true, timeout);
 
-	/* ret == 0 means not signaled,
-	 * ret > 0 means signaled
-	 * ret < 0 means interrupted before timeout
-	 */
 	if (ret >= 0) {
 		memset(args, 0, sizeof(*args));
 		args->out.status = (ret == 0);
@@ -586,7 +1053,7 @@ int amdgpu_gem_wait_idle_ioctl(struct dr
 }
 
 int amdgpu_gem_metadata_ioctl(struct drm_device *dev, void *data,
-				struct drm_file *filp)
+							  struct drm_file *filp)
 {
 	struct drm_amdgpu_gem_metadata *args = data;
 	struct drm_gem_object *gobj;
@@ -595,8 +1062,10 @@ int amdgpu_gem_metadata_ioctl(struct drm
 
 	DRM_DEBUG("%d\n", args->handle);
 	gobj = drm_gem_object_lookup(filp, args->handle);
-	if (gobj == NULL)
+	if (gobj == NULL) {
 		return -ENOENT;
+	}
+	/* FIX: Use the declared variable 'robj', not 'abo' */
 	robj = gem_to_amdgpu_bo(gobj);
 
 	r = amdgpu_bo_reserve(robj, false);
@@ -606,9 +1075,9 @@ int amdgpu_gem_metadata_ioctl(struct drm
 	if (args->op == AMDGPU_GEM_METADATA_OP_GET_METADATA) {
 		amdgpu_bo_get_tiling_flags(robj, &args->data.tiling_info);
 		r = amdgpu_bo_get_metadata(robj, args->data.data,
-					   sizeof(args->data.data),
-					   &args->data.data_size_bytes,
-					   &args->data.flags);
+								   sizeof(args->data.data),
+								   &args->data.data_size_bytes,
+							 &args->data.flags);
 	} else if (args->op == AMDGPU_GEM_METADATA_OP_SET_METADATA) {
 		if (args->data.data_size_bytes > sizeof(args->data.data)) {
 			r = -EINVAL;
@@ -617,64 +1086,36 @@ int amdgpu_gem_metadata_ioctl(struct drm
 		r = amdgpu_bo_set_tiling_flags(robj, args->data.tiling_info);
 		if (!r)
 			r = amdgpu_bo_set_metadata(robj, args->data.data,
-						   args->data.data_size_bytes,
-						   args->data.flags);
+									   args->data.data_size_bytes,
+							  args->data.flags);
 	}
 
-unreserve:
+	unreserve:
 	amdgpu_bo_unreserve(robj);
-out:
+	out:
 	drm_gem_object_put(gobj);
 	return r;
 }
 
-/**
- * amdgpu_gem_va_update_vm -update the bo_va in its VM
- *
- * @adev: amdgpu_device pointer
- * @vm: vm to update
- * @bo_va: bo_va to update
- * @operation: map, unmap or clear
- *
- * Update the bo_va directly after setting its address. Errors are not
- * vital here, so they are not reported back to userspace.
- */
-static void amdgpu_gem_va_update_vm(struct amdgpu_device *adev,
-				    struct amdgpu_vm *vm,
-				    struct amdgpu_bo_va *bo_va,
-				    uint32_t operation)
+unsigned long amdgpu_gem_timeout(uint64_t timeout_ns)
 {
-	int r;
-
-	if (!amdgpu_vm_ready(vm))
-		return;
+	unsigned long timeout_jiffies;
+	ktime_t timeout;
 
-	r = amdgpu_vm_clear_freed(adev, vm, NULL);
-	if (r)
-		goto error;
+	if (((int64_t)timeout_ns) < 0)
+		return MAX_SCHEDULE_TIMEOUT;
 
-	if (operation == AMDGPU_VA_OP_MAP ||
-	    operation == AMDGPU_VA_OP_REPLACE) {
-		r = amdgpu_vm_bo_update(adev, bo_va, false);
-		if (r)
-			goto error;
-	}
+	timeout = ktime_sub(ns_to_ktime(timeout_ns), ktime_get());
+	if (ktime_to_ns(timeout) < 0)
+		return 0;
 
-	r = amdgpu_vm_update_pdes(adev, vm, false);
+	timeout_jiffies = nsecs_to_jiffies(ktime_to_ns(timeout));
+	if (timeout_jiffies > MAX_SCHEDULE_TIMEOUT)
+		return MAX_SCHEDULE_TIMEOUT - 1;
 
-error:
-	if (r && r != -ERESTARTSYS)
-		DRM_ERROR("Couldn't update BO_VA (%d)\n", r);
+	return timeout_jiffies;
 }
 
-/**
- * amdgpu_gem_va_map_flags - map GEM UAPI flags into hardware flags
- *
- * @adev: amdgpu_device pointer
- * @flags: GEM UAPI flags
- *
- * Returns the GEM UAPI flags mapped into hardware for the ASIC.
- */
 uint64_t amdgpu_gem_va_map_flags(struct amdgpu_device *adev, uint32_t flags)
 {
 	uint64_t pte_flag = 0;
@@ -690,29 +1131,58 @@ uint64_t amdgpu_gem_va_map_flags(struct
 	if (flags & AMDGPU_VM_PAGE_NOALLOC)
 		pte_flag |= AMDGPU_PTE_NOALLOC;
 
-	if (adev->gmc.gmc_funcs->map_mtype)
+	if (adev->gmc.gmc_funcs && adev->gmc.gmc_funcs->map_mtype) {
 		pte_flag |= amdgpu_gmc_map_mtype(adev,
-						 flags & AMDGPU_VM_MTYPE_MASK);
+										 flags & AMDGPU_VM_MTYPE_MASK);
+	}
 
 	return pte_flag;
 }
 
+static void amdgpu_gem_va_update_vm(struct amdgpu_device *adev,
+									struct amdgpu_vm *vm,
+									struct amdgpu_bo_va *bo_va,
+									uint32_t operation)
+{
+	int r;
+
+	if (!amdgpu_vm_ready(vm))
+		return;
+
+	r = amdgpu_vm_clear_freed(adev, vm, NULL);
+	if (r)
+		goto error;
+
+	if (operation == AMDGPU_VA_OP_MAP ||
+		operation == AMDGPU_VA_OP_REPLACE) {
+		r = amdgpu_vm_bo_update(adev, bo_va, false);
+	if (r)
+		goto error;
+		}
+
+		r = amdgpu_vm_update_pdes(adev, vm, false);
+
+	error:
+	if (r && r != -ERESTARTSYS)
+		DRM_ERROR("Couldn't update BO_VA (%d)\n", r);
+}
+
 int amdgpu_gem_va_ioctl(struct drm_device *dev, void *data,
-			  struct drm_file *filp)
+						struct drm_file *filp)
 {
 	const uint32_t valid_flags = AMDGPU_VM_DELAY_UPDATE |
-		AMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE |
-		AMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_MTYPE_MASK |
-		AMDGPU_VM_PAGE_NOALLOC;
+	AMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE |
+	AMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_MTYPE_MASK |
+	AMDGPU_VM_PAGE_NOALLOC;
 	const uint32_t prt_flags = AMDGPU_VM_DELAY_UPDATE |
-		AMDGPU_VM_PAGE_PRT;
+	AMDGPU_VM_PAGE_PRT;
 
 	struct drm_amdgpu_gem_va *args = data;
-	struct drm_gem_object *gobj;
+	struct drm_gem_object *gobj = NULL;
+	struct amdgpu_bo *abo = NULL;
 	struct amdgpu_device *adev = drm_to_adev(dev);
 	struct amdgpu_fpriv *fpriv = filp->driver_priv;
-	struct amdgpu_bo *abo;
-	struct amdgpu_bo_va *bo_va;
+	struct amdgpu_bo_va *bo_va = NULL;
 	struct drm_exec exec;
 	uint64_t va_flags;
 	uint64_t vm_size;
@@ -720,62 +1190,59 @@ int amdgpu_gem_va_ioctl(struct drm_devic
 
 	if (args->va_address < AMDGPU_VA_RESERVED_BOTTOM) {
 		dev_dbg(dev->dev,
-			"va_address 0x%llx is in reserved area 0x%llx\n",
-			args->va_address, AMDGPU_VA_RESERVED_BOTTOM);
+				"va_address 0x%llx is in reserved area 0x%llx\n",
+		  args->va_address, AMDGPU_VA_RESERVED_BOTTOM);
 		return -EINVAL;
 	}
 
 	if (args->va_address >= AMDGPU_GMC_HOLE_START &&
-	    args->va_address < AMDGPU_GMC_HOLE_END) {
+		args->va_address < AMDGPU_GMC_HOLE_END) {
 		dev_dbg(dev->dev,
-			"va_address 0x%llx is in VA hole 0x%llx-0x%llx\n",
-			args->va_address, AMDGPU_GMC_HOLE_START,
-			AMDGPU_GMC_HOLE_END);
+				"va_address 0x%llx is in VA hole 0x%llx-0x%llx\n",
+		  args->va_address, AMDGPU_GMC_HOLE_START,
+		  AMDGPU_GMC_HOLE_END);
 		return -EINVAL;
-	}
+		}
 
-	args->va_address &= AMDGPU_GMC_HOLE_MASK;
+		args->va_address &= AMDGPU_GMC_HOLE_MASK;
 
 	vm_size = adev->vm_manager.max_pfn * AMDGPU_GPU_PAGE_SIZE;
 	vm_size -= AMDGPU_VA_RESERVED_TOP;
 	if (args->va_address + args->map_size > vm_size) {
 		dev_dbg(dev->dev,
-			"va_address 0x%llx is in top reserved area 0x%llx\n",
-			args->va_address + args->map_size, vm_size);
+				"va_address 0x%llx is in top reserved area 0x%llx\n",
+		  args->va_address + args->map_size, vm_size);
 		return -EINVAL;
 	}
 
 	if ((args->flags & ~valid_flags) && (args->flags & ~prt_flags)) {
 		dev_dbg(dev->dev, "invalid flags combination 0x%08X\n",
-			args->flags);
+				args->flags);
 		return -EINVAL;
 	}
 
 	switch (args->operation) {
-	case AMDGPU_VA_OP_MAP:
-	case AMDGPU_VA_OP_UNMAP:
-	case AMDGPU_VA_OP_CLEAR:
-	case AMDGPU_VA_OP_REPLACE:
-		break;
-	default:
-		dev_dbg(dev->dev, "unsupported operation %d\n",
-			args->operation);
-		return -EINVAL;
+		case AMDGPU_VA_OP_MAP:
+		case AMDGPU_VA_OP_UNMAP:
+		case AMDGPU_VA_OP_CLEAR:
+		case AMDGPU_VA_OP_REPLACE:
+			break;
+		default:
+			dev_dbg(dev->dev, "unsupported operation %d\n", args->operation);
+			return -EINVAL;
 	}
 
 	if ((args->operation != AMDGPU_VA_OP_CLEAR) &&
-	    !(args->flags & AMDGPU_VM_PAGE_PRT)) {
+		!(args->flags & AMDGPU_VM_PAGE_PRT)) {
 		gobj = drm_gem_object_lookup(filp, args->handle);
-		if (gobj == NULL)
-			return -ENOENT;
-		abo = gem_to_amdgpu_bo(gobj);
-	} else {
-		gobj = NULL;
-		abo = NULL;
+	if (!gobj) {
+		return -ENOENT;
 	}
+	abo = gem_to_amdgpu_bo(gobj);
+		}
+
+		drm_exec_init(&exec, DRM_EXEC_INTERRUPTIBLE_WAIT | DRM_EXEC_IGNORE_DUPLICATES, 0);
 
-	drm_exec_init(&exec, DRM_EXEC_INTERRUPTIBLE_WAIT |
-		      DRM_EXEC_IGNORE_DUPLICATES, 0);
 	drm_exec_until_all_locked(&exec) {
 		if (gobj) {
 			r = drm_exec_lock_obj(&exec, gobj);
@@ -783,7 +1250,6 @@ int amdgpu_gem_va_ioctl(struct drm_devic
 			if (unlikely(r))
 				goto error;
 		}
-
 		r = amdgpu_vm_lock_pd(&fpriv->vm, &exec, 2);
 		drm_exec_retry_on_contention(&exec);
 		if (unlikely(r))
@@ -798,48 +1264,51 @@ int amdgpu_gem_va_ioctl(struct drm_devic
 		}
 	} else if (args->operation != AMDGPU_VA_OP_CLEAR) {
 		bo_va = fpriv->prt_va;
-	} else {
-		bo_va = NULL;
+		if (!bo_va) {
+			DRM_ERROR("Process context has no PRT VA\n");
+			r = -EINVAL;
+			goto error;
+		}
+	} // else bo_va = NULL for CLEAR
+
+	if (abo && is_hbm2_vega(adev)) {
+		amdgpu_vega_optimize_for_workload(adev, abo, abo->flags);
 	}
 
 	switch (args->operation) {
-	case AMDGPU_VA_OP_MAP:
-		va_flags = amdgpu_gem_va_map_flags(adev, args->flags);
-		r = amdgpu_vm_bo_map(adev, bo_va, args->va_address,
-				     args->offset_in_bo, args->map_size,
-				     va_flags);
-		break;
-	case AMDGPU_VA_OP_UNMAP:
-		r = amdgpu_vm_bo_unmap(adev, bo_va, args->va_address);
-		break;
-
-	case AMDGPU_VA_OP_CLEAR:
-		r = amdgpu_vm_bo_clear_mappings(adev, &fpriv->vm,
-						args->va_address,
-						args->map_size);
-		break;
-	case AMDGPU_VA_OP_REPLACE:
-		va_flags = amdgpu_gem_va_map_flags(adev, args->flags);
-		r = amdgpu_vm_bo_replace_map(adev, bo_va, args->va_address,
-					     args->offset_in_bo, args->map_size,
-					     va_flags);
-		break;
-	default:
-		break;
+		case AMDGPU_VA_OP_MAP:
+			va_flags = amdgpu_gem_va_map_flags(adev, args->flags);
+			r = amdgpu_vm_bo_map(adev, bo_va, args->va_address,
+								 args->offset_in_bo, args->map_size, va_flags);
+			break;
+		case AMDGPU_VA_OP_UNMAP:
+			r = amdgpu_vm_bo_unmap(adev, bo_va, args->va_address);
+			break;
+		case AMDGPU_VA_OP_CLEAR:
+			r = amdgpu_vm_bo_clear_mappings(adev, &fpriv->vm,
+											args->va_address, args->map_size);
+			break;
+		case AMDGPU_VA_OP_REPLACE:
+			va_flags = amdgpu_gem_va_map_flags(adev, args->flags);
+			r = amdgpu_vm_bo_replace_map(adev, bo_va, args->va_address,
+										 args->offset_in_bo, args->map_size, va_flags);
+			break;
 	}
+
 	if (!r && !(args->flags & AMDGPU_VM_DELAY_UPDATE) && !adev->debug_vm)
-		amdgpu_gem_va_update_vm(adev, &fpriv->vm, bo_va,
-					args->operation);
+		amdgpu_gem_va_update_vm(adev, &fpriv->vm, bo_va, args->operation);
 
-error:
+	error:
 	drm_exec_fini(&exec);
-	drm_gem_object_put(gobj);
+	if (gobj)
+		drm_gem_object_put(gobj);
 	return r;
 }
 
 int amdgpu_gem_op_ioctl(struct drm_device *dev, void *data,
-			struct drm_file *filp)
+						struct drm_file *filp)
 {
+	struct amdgpu_device *adev = drm_to_adev(dev);
 	struct drm_amdgpu_gem_op *args = data;
 	struct drm_gem_object *gobj;
 	struct amdgpu_vm_bo_base *base;
@@ -857,81 +1326,85 @@ int amdgpu_gem_op_ioctl(struct drm_devic
 		goto out;
 
 	switch (args->op) {
-	case AMDGPU_GEM_OP_GET_GEM_CREATE_INFO: {
-		struct drm_amdgpu_gem_create_in info;
-		void __user *out = u64_to_user_ptr(args->value);
-
-		info.bo_size = robj->tbo.base.size;
-		info.alignment = robj->tbo.page_alignment << PAGE_SHIFT;
-		info.domains = robj->preferred_domains;
-		info.domain_flags = robj->flags;
-		amdgpu_bo_unreserve(robj);
-		if (copy_to_user(out, &info, sizeof(info)))
-			r = -EFAULT;
-		break;
-	}
-	case AMDGPU_GEM_OP_SET_PLACEMENT:
-		if (robj->tbo.base.import_attach &&
-		    args->value & AMDGPU_GEM_DOMAIN_VRAM) {
-			r = -EINVAL;
+		case AMDGPU_GEM_OP_GET_GEM_CREATE_INFO: {
+			struct drm_amdgpu_gem_create_in info;
+			void __user *out = u64_to_user_ptr(args->value);
+
+			info.bo_size = robj->tbo.base.size;
+			info.alignment = robj->tbo.page_alignment << PAGE_SHIFT;
+			info.domains = robj->preferred_domains;
+			info.domain_flags = robj->flags;
 			amdgpu_bo_unreserve(robj);
+			if (copy_to_user(out, &info, sizeof(info)))
+				r = -EFAULT;
 			break;
 		}
-		if (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm)) {
-			r = -EPERM;
+		case AMDGPU_GEM_OP_SET_PLACEMENT:
+			if (robj->tbo.base.import_attach &&
+				args->value & AMDGPU_GEM_DOMAIN_VRAM) {
+				r = -EINVAL;
 			amdgpu_bo_unreserve(robj);
 			break;
-		}
-		for (base = robj->vm_bo; base; base = base->next)
-			if (amdgpu_xgmi_same_hive(amdgpu_ttm_adev(robj->tbo.bdev),
-				amdgpu_ttm_adev(base->vm->root.bo->tbo.bdev))) {
-				r = -EINVAL;
-				amdgpu_bo_unreserve(robj);
+				}
+				if (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm)) {
+					r = -EPERM;
+					amdgpu_bo_unreserve(robj);
+					break;
+				}
+				for (base = robj->vm_bo; base; base = base->next)
+					if (amdgpu_xgmi_same_hive(amdgpu_ttm_adev(robj->tbo.bdev),
+						amdgpu_ttm_adev(base->vm->root.bo->tbo.bdev))) {
+						r = -EINVAL;
+					amdgpu_bo_unreserve(robj);
 				goto out;
-			}
+						}
 
+						robj->preferred_domains = args->value & (AMDGPU_GEM_DOMAIN_VRAM |
+						AMDGPU_GEM_DOMAIN_GTT |
+						AMDGPU_GEM_DOMAIN_CPU);
+						robj->allowed_domains = robj->preferred_domains;
+						if (robj->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)
+							robj->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
 
-		robj->preferred_domains = args->value & (AMDGPU_GEM_DOMAIN_VRAM |
-							AMDGPU_GEM_DOMAIN_GTT |
-							AMDGPU_GEM_DOMAIN_CPU);
-		robj->allowed_domains = robj->preferred_domains;
-		if (robj->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)
-			robj->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
+		if (is_hbm2_vega(adev)) {
+			amdgpu_vega_optimize_for_workload(adev, robj, robj->flags);
+		}
 
-		if (robj->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID)
+		if (robj->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
 			amdgpu_vm_bo_invalidate(robj, true);
+		}
 
 		amdgpu_bo_unreserve(robj);
 		break;
-	default:
-		amdgpu_bo_unreserve(robj);
-		r = -EINVAL;
+		default:
+			amdgpu_bo_unreserve(robj);
+			r = -EINVAL;
 	}
 
-out:
+	out:
 	drm_gem_object_put(gobj);
 	return r;
 }
 
 static int amdgpu_gem_align_pitch(struct amdgpu_device *adev,
-				  int width,
-				  int cpp,
-				  bool tiled)
+								  int width,
+								  int cpp,
+								  bool tiled)
 {
 	int aligned = width;
 	int pitch_mask = 0;
 
 	switch (cpp) {
-	case 1:
-		pitch_mask = 255;
-		break;
-	case 2:
-		pitch_mask = 127;
-		break;
-	case 3:
-	case 4:
-		pitch_mask = 63;
-		break;
+		case 1:
+			pitch_mask = 255;
+			break;
+		case 2:
+			pitch_mask = 127;
+			break;
+		case 3:
+		case 4:
+			pitch_mask = 63;
+			break;
 	}
 
 	aligned += pitch_mask;
@@ -940,40 +1413,54 @@ static int amdgpu_gem_align_pitch(struct
 }
 
 int amdgpu_mode_dumb_create(struct drm_file *file_priv,
-			    struct drm_device *dev,
-			    struct drm_mode_create_dumb *args)
+							struct drm_device *dev,
+							struct drm_mode_create_dumb *args)
 {
 	struct amdgpu_device *adev = drm_to_adev(dev);
 	struct amdgpu_fpriv *fpriv = file_priv->driver_priv;
 	struct drm_gem_object *gobj;
 	uint32_t handle;
 	u64 flags = AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |
-		    AMDGPU_GEM_CREATE_CPU_GTT_USWC |
-		    AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;
+	AMDGPU_GEM_CREATE_CPU_GTT_USWC |
+	AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;
 	u32 domain;
 	int r;
 
-	/*
-	 * The buffer returned from this function should be cleared, but
-	 * it can only be done if the ring is enabled or we'll fail to
-	 * create the buffer.
-	 */
 	if (adev->mman.buffer_funcs_enabled)
 		flags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;
 
 	args->pitch = amdgpu_gem_align_pitch(adev, args->width,
-					     DIV_ROUND_UP(args->bpp, 8), 0);
+										 DIV_ROUND_UP(args->bpp, 8), 0);
 	args->size = (u64)args->pitch * args->height;
 	args->size = ALIGN(args->size, PAGE_SIZE);
 	domain = amdgpu_bo_get_preferred_domain(adev,
-				amdgpu_display_supported_domains(adev, flags));
+											amdgpu_display_supported_domains(adev, flags));
+
+	if (is_hbm2_vega(adev)) {
+		uint32_t alignment = 0;
+		uint64_t optimized_size = args->size;
+
+		amdgpu_vega_optimize_hbm2_bank_access(adev, NULL, &optimized_size, &alignment);
+
+		r = amdgpu_gem_object_create(adev, optimized_size, alignment, domain, flags,
+									 ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
+		if (r == 0) {
+			r = drm_gem_handle_create(file_priv, gobj, &handle);
+			drm_gem_object_put(gobj);
+			if (r)
+				return r;
+
+			args->handle = handle;
+			return 0;
+		}
+	}
+
 	r = amdgpu_gem_object_create(adev, args->size, 0, domain, flags,
-				     ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
+								 ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
 	if (r)
 		return -ENOMEM;
 
 	r = drm_gem_handle_create(file_priv, gobj, &handle);
-	/* drop reference from allocate - handle holds it now */
 	drm_gem_object_put(gobj);
 	if (r)
 		return r;
@@ -1000,23 +1487,16 @@ static int amdgpu_debugfs_gem_info_show(
 		struct pid *pid;
 		int id;
 
-		/*
-		 * Although we have a valid reference on file->pid, that does
-		 * not guarantee that the task_struct who called get_pid() is
-		 * still alive (e.g. get_pid(current) => fork() => exit()).
-		 * Therefore, we need to protect this ->comm access using RCU.
-		 */
 		rcu_read_lock();
 		pid = rcu_dereference(file->pid);
 		task = pid_task(pid, PIDTYPE_TGID);
 		seq_printf(m, "pid %8d command %s:\n", pid_nr(pid),
-			   task ? task->comm : "<unknown>");
+				   task ? task->comm : "<unknown>");
 		rcu_read_unlock();
 
 		spin_lock(&file->table_lock);
 		idr_for_each_entry(&file->object_idr, gobj, id) {
 			struct amdgpu_bo *bo = gem_to_amdgpu_bo(gobj);
-
 			amdgpu_bo_print_info(id, bo, m);
 		}
 		spin_unlock(&file->table_lock);
@@ -1032,11 +1512,11 @@ DEFINE_SHOW_ATTRIBUTE(amdgpu_debugfs_gem
 
 void amdgpu_debugfs_gem_init(struct amdgpu_device *adev)
 {
-#if defined(CONFIG_DEBUG_FS)
+	#if defined(CONFIG_DEBUG_FS)
 	struct drm_minor *minor = adev_to_drm(adev)->primary;
 	struct dentry *root = minor->debugfs_root;
 
 	debugfs_create_file("amdgpu_gem_info", 0444, root, adev,
-			    &amdgpu_debugfs_gem_info_fops);
-#endif
+						&amdgpu_debugfs_gem_info_fops);
+	#endif
 }


--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c	2025-04-16 11:25:37.255871538 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c	2025-04-16 11:36:44.545585709 +0200
@@ -174,6 +174,7 @@ uint amdgpu_sdma_phase_quantum = 32;
 char *amdgpu_disable_cu;
 char *amdgpu_virtual_display;
 bool enforce_isolation;
+extern void amdgpu_vega_vram_thresholds_init(void);
 
 /* Specifies the default granularity for SVM, used in buffer
  * migration and restoration of backing memory when handling
@@ -2346,6 +2347,7 @@ static int amdgpu_pci_probe(struct pci_d
 	pci_set_drvdata(pdev, ddev);
 
 	amdgpu_init_debug_options(adev);
+	amdgpu_vega_vram_thresholds_init();
 
 	ret = amdgpu_driver_load_kms(adev, flags);
 	if (ret)


--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c	2025-03-19 20:16:22.723193359 +0100
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c	2025-03-19 20:20:03.397460298 +0100
@@ -39,6 +39,9 @@
 #include "gfx_v9_0.h"
 #include "amdgpu_amdkfd_gfx_v9.h"
 #include <uapi/linux/kfd_ioctl.h>
+#ifdef CONFIG_X86
+#include <asm/processor.h>
+#endif
 
 enum hqd_dequeue_request_type {
 	NO_ACTION = 0,
@@ -47,8 +50,76 @@ enum hqd_dequeue_request_type {
 	SAVE_WAVES
 };
 
+/*
+ * Detect Intel Raptor Lake CPU for optimized waiting strategy
+ * Raptor Lake is identified by family 6, model 0xB7 (Raptor Lake S)
+ * or 0xBF (Raptor Lake P)
+ */
+#ifdef CONFIG_X86
+static bool is_raptor_lake_cpu(void)
+{
+	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL &&
+		boot_cpu_data.x86 == 6 &&
+		(boot_cpu_data.x86_model == 0xB7 || boot_cpu_data.x86_model == 0xBF))
+		return true;
+	return false;
+}
+#else
+static inline bool is_raptor_lake_cpu(void)
+{
+	return false;
+}
+#endif
+
+/**
+ * optimized_wait_for_gpu - Optimized waiting strategy for CPU-GPU synchronization
+ * @adev: amdgpu device
+ * @reg_addr: Register address to poll
+ * @mask: Mask to apply to register value
+ * @expected: Expected value after applying mask
+ * @timeout_ms: Timeout in milliseconds
+ *
+ * Uses a hybrid approach optimized for Intel Raptor Lake CPUs to wait for GPU.
+ * Initially uses CPU spinning for low latency, then gradually transitions to
+ * yielding to reduce power consumption.
+ *
+ * Returns true if condition was met, false if timeout
+ */
+/*
+ * Fix for optimized_wait_for_gpu function in set_pasid_vmid_mapping
+ * Changed from earlier implementation to correctly handle register reads
+ */
+static bool optimized_wait_for_gpu(struct amdgpu_device *adev,
+								   uint32_t reg_addr, uint32_t mask,
+								   uint32_t expected, unsigned int timeout_ms)
+{
+	unsigned long end_jiffies = jiffies + msecs_to_jiffies(timeout_ms);
+	unsigned int i = 0;
+	const unsigned int spin_threshold = 20; /* Conservative value works on both CPUs */
+
+	while (true) {
+		uint32_t val = RREG32(reg_addr);
+		if ((val & mask) == expected)
+			return true;
+
+		if (time_after(jiffies, end_jiffies))
+			return false;
+
+		/* Optimized waiting strategy with minimal register reads */
+		if (i++ < spin_threshold) {
+			cpu_relax();
+		} else {
+			/* After initial spinning, use more conservative waiting */
+			if ((i & 0x7) == 0) /* Only yield occasionally */
+				usleep_range(10, 20);
+			else
+				cpu_relax();
+		}
+	}
+}
+
 static void kgd_gfx_v9_lock_srbm(struct amdgpu_device *adev, uint32_t mec, uint32_t pipe,
-			uint32_t queue, uint32_t vmid, uint32_t inst)
+								 uint32_t queue, uint32_t vmid, uint32_t inst)
 {
 	mutex_lock(&adev->srbm_mutex);
 	soc15_grbm_select(adev, mec, pipe, queue, vmid, GET_INST(GC, inst));
@@ -61,7 +132,7 @@ static void kgd_gfx_v9_unlock_srbm(struc
 }
 
 void kgd_gfx_v9_acquire_queue(struct amdgpu_device *adev, uint32_t pipe_id,
-				uint32_t queue_id, uint32_t inst)
+							  uint32_t queue_id, uint32_t inst)
 {
 	uint32_t mec = (pipe_id / adev->gfx.mec.num_pipe_per_mec) + 1;
 	uint32_t pipe = (pipe_id % adev->gfx.mec.num_pipe_per_mec);
@@ -70,10 +141,10 @@ void kgd_gfx_v9_acquire_queue(struct amd
 }
 
 uint64_t kgd_gfx_v9_get_queue_mask(struct amdgpu_device *adev,
-			       uint32_t pipe_id, uint32_t queue_id)
+								   uint32_t pipe_id, uint32_t queue_id)
 {
 	unsigned int bit = pipe_id * adev->gfx.mec.num_queue_per_pipe +
-			queue_id;
+	queue_id;
 
 	return 1ull << bit;
 }
@@ -84,10 +155,10 @@ void kgd_gfx_v9_release_queue(struct amd
 }
 
 void kgd_gfx_v9_program_sh_mem_settings(struct amdgpu_device *adev, uint32_t vmid,
-					uint32_t sh_mem_config,
-					uint32_t sh_mem_ape1_base,
-					uint32_t sh_mem_ape1_limit,
-					uint32_t sh_mem_bases, uint32_t inst)
+										uint32_t sh_mem_config,
+										uint32_t sh_mem_ape1_base,
+										uint32_t sh_mem_ape1_limit,
+										uint32_t sh_mem_bases, uint32_t inst)
 {
 	kgd_gfx_v9_lock_srbm(adev, 0, 0, 0, vmid, inst);
 
@@ -99,7 +170,7 @@ void kgd_gfx_v9_program_sh_mem_settings(
 }
 
 int kgd_gfx_v9_set_pasid_vmid_mapping(struct amdgpu_device *adev, u32 pasid,
-					unsigned int vmid, uint32_t inst)
+									  unsigned int vmid, uint32_t inst)
 {
 	/*
 	 * We have to assume that there is no outstanding mapping.
@@ -109,7 +180,7 @@ int kgd_gfx_v9_set_pasid_vmid_mapping(st
 	 * So the protocol is to always wait & clear.
 	 */
 	uint32_t pasid_mapping = (pasid == 0) ? 0 : (uint32_t)pasid |
-			ATC_VMID0_PASID_MAPPING__VALID_MASK;
+	ATC_VMID0_PASID_MAPPING__VALID_MASK;
 
 	/*
 	 * need to do this twice, once for gfx and once for mmhub
@@ -117,40 +188,48 @@ int kgd_gfx_v9_set_pasid_vmid_mapping(st
 	 * ATC_VMID0..15 registers are separate from ATC_VMID16..31.
 	 */
 
+	/* Program GFX hub */
 	WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID0_PASID_MAPPING) + vmid,
-	       pasid_mapping);
+		   pasid_mapping);
 
-	while (!(RREG32(SOC15_REG_OFFSET(
-				ATHUB, 0,
-				mmATC_VMID_PASID_MAPPING_UPDATE_STATUS)) &
-		 (1U << vmid)))
-		cpu_relax();
-
-	WREG32(SOC15_REG_OFFSET(ATHUB, 0,
-				mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
-	       1U << vmid);
-
-	/* Mapping vmid to pasid also for IH block */
-	WREG32(SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT) + vmid,
-	       pasid_mapping);
-
-	WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID16_PASID_MAPPING) + vmid,
-	       pasid_mapping);
-
-	while (!(RREG32(SOC15_REG_OFFSET(
-				ATHUB, 0,
-				mmATC_VMID_PASID_MAPPING_UPDATE_STATUS)) &
-		 (1U << (vmid + 16))))
-		cpu_relax();
-
-	WREG32(SOC15_REG_OFFSET(ATHUB, 0,
-				mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
-	       1U << (vmid + 16));
-
-	/* Mapping vmid to pasid also for IH block */
-	WREG32(SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT_MM) + vmid,
-	       pasid_mapping);
-	return 0;
+	/* Wait for GFX mapping to complete using optimized waiting strategy */
+	if (!optimized_wait_for_gpu(adev,
+		SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
+								1U << vmid,
+							 1U << vmid,
+							 100)) {
+		pr_err("GFX VMID-PASID mapping timeout\n");
+	return -ETIME;
+							 }
+
+							 WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
+									1U << vmid);
+
+							 /* Mapping vmid to pasid also for IH block */
+							 WREG32(SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT) + vmid,
+									pasid_mapping);
+
+							 /* Program MM hub */
+							 WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID16_PASID_MAPPING) + vmid,
+									pasid_mapping);
+
+							 /* Wait for MM hub mapping to complete using optimized waiting strategy */
+							 if (!optimized_wait_for_gpu(adev,
+								 SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
+														 1U << (vmid + 16),
+														 1U << (vmid + 16),
+														 100)) {
+								 pr_err("MM hub VMID-PASID mapping timeout\n");
+							 return -ETIME;
+														 }
+
+														 WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
+																1U << (vmid + 16));
+
+														 /* Mapping vmid to pasid also for IH block */
+														 WREG32(SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT_MM) + vmid,
+																pasid_mapping);
+														 return 0;
 }
 
 /* TODO - RING0 form of field is obsolete, seems to date back to SI
@@ -158,7 +237,7 @@ int kgd_gfx_v9_set_pasid_vmid_mapping(st
  */
 
 int kgd_gfx_v9_init_interrupts(struct amdgpu_device *adev, uint32_t pipe_id,
-				uint32_t inst)
+							   uint32_t inst)
 {
 	uint32_t mec;
 	uint32_t pipe;
@@ -169,8 +248,8 @@ int kgd_gfx_v9_init_interrupts(struct am
 	kgd_gfx_v9_lock_srbm(adev, mec, pipe, 0, 0, inst);
 
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmCPC_INT_CNTL,
-		CP_INT_CNTL_RING0__TIME_STAMP_INT_ENABLE_MASK |
-		CP_INT_CNTL_RING0__OPCODE_ERROR_INT_ENABLE_MASK);
+				 CP_INT_CNTL_RING0__TIME_STAMP_INT_ENABLE_MASK |
+				 CP_INT_CNTL_RING0__OPCODE_ERROR_INT_ENABLE_MASK);
 
 	kgd_gfx_v9_unlock_srbm(adev, inst);
 
@@ -178,33 +257,33 @@ int kgd_gfx_v9_init_interrupts(struct am
 }
 
 static uint32_t get_sdma_rlc_reg_offset(struct amdgpu_device *adev,
-				unsigned int engine_id,
-				unsigned int queue_id)
+										unsigned int engine_id,
+										unsigned int queue_id)
 {
 	uint32_t sdma_engine_reg_base = 0;
 	uint32_t sdma_rlc_reg_offset;
 
 	switch (engine_id) {
-	default:
-		dev_warn(adev->dev,
-			 "Invalid sdma engine id (%d), using engine id 0\n",
-			 engine_id);
-		fallthrough;
-	case 0:
-		sdma_engine_reg_base = SOC15_REG_OFFSET(SDMA0, 0,
-				mmSDMA0_RLC0_RB_CNTL) - mmSDMA0_RLC0_RB_CNTL;
-		break;
-	case 1:
-		sdma_engine_reg_base = SOC15_REG_OFFSET(SDMA1, 0,
-				mmSDMA1_RLC0_RB_CNTL) - mmSDMA0_RLC0_RB_CNTL;
-		break;
+		default:
+			dev_warn(adev->dev,
+					 "Invalid sdma engine id (%d), using engine id 0\n",
+					 engine_id);
+			fallthrough;
+		case 0:
+			sdma_engine_reg_base = SOC15_REG_OFFSET(SDMA0, 0,
+													mmSDMA0_RLC0_RB_CNTL) - mmSDMA0_RLC0_RB_CNTL;
+													break;
+		case 1:
+			sdma_engine_reg_base = SOC15_REG_OFFSET(SDMA1, 0,
+													mmSDMA1_RLC0_RB_CNTL) - mmSDMA0_RLC0_RB_CNTL;
+													break;
 	}
 
 	sdma_rlc_reg_offset = sdma_engine_reg_base
-		+ queue_id * (mmSDMA0_RLC1_RB_CNTL - mmSDMA0_RLC0_RB_CNTL);
+	+ queue_id * (mmSDMA0_RLC1_RB_CNTL - mmSDMA0_RLC0_RB_CNTL);
 
 	pr_debug("RLC register offset for SDMA%d RLC%d: 0x%x\n", engine_id,
-		 queue_id, sdma_rlc_reg_offset);
+			 queue_id, sdma_rlc_reg_offset);
 
 	return sdma_rlc_reg_offset;
 }
@@ -220,10 +299,10 @@ static inline struct v9_sdma_mqd *get_sd
 }
 
 int kgd_gfx_v9_hqd_load(struct amdgpu_device *adev, void *mqd,
-			uint32_t pipe_id, uint32_t queue_id,
-			uint32_t __user *wptr, uint32_t wptr_shift,
-			uint32_t wptr_mask, struct mm_struct *mm,
-			uint32_t inst)
+						uint32_t pipe_id, uint32_t queue_id,
+						uint32_t __user *wptr, uint32_t wptr_shift,
+						uint32_t wptr_mask, struct mm_struct *mm,
+						uint32_t inst)
 {
 	struct v9_mqd *m;
 	uint32_t *mqd_hqd;
@@ -238,13 +317,12 @@ int kgd_gfx_v9_hqd_load(struct amdgpu_de
 	hqd_base = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_BASE_ADDR);
 
 	for (reg = hqd_base;
-	     reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI); reg++)
-		WREG32_XCC(reg, mqd_hqd[reg - hqd_base], inst);
-
+		 reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI); reg++)
+		 WREG32_XCC(reg, mqd_hqd[reg - hqd_base], inst);
 
 	/* Activate doorbell logic before triggering WPTR poll. */
 	data = REG_SET_FIELD(m->cp_hqd_pq_doorbell_control,
-			     CP_HQD_PQ_DOORBELL_CONTROL, DOORBELL_EN, 1);
+						 CP_HQD_PQ_DOORBELL_CONTROL, DOORBELL_EN, 1);
 	WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_DOORBELL_CONTROL, data);
 
 	if (wptr) {
@@ -265,8 +343,8 @@ int kgd_gfx_v9_hqd_load(struct amdgpu_de
 		 * queue size.
 		 */
 		uint32_t queue_size =
-			2 << REG_GET_FIELD(m->cp_hqd_pq_control,
-					   CP_HQD_PQ_CONTROL, QUEUE_SIZE);
+		2 << REG_GET_FIELD(m->cp_hqd_pq_control,
+						   CP_HQD_PQ_CONTROL, QUEUE_SIZE);
 		uint64_t guessed_wptr = m->cp_hqd_pq_rptr & (queue_size - 1);
 
 		if ((m->cp_hqd_pq_wptr_lo & (queue_size - 1)) < guessed_wptr)
@@ -275,20 +353,20 @@ int kgd_gfx_v9_hqd_load(struct amdgpu_de
 		guessed_wptr += (uint64_t)m->cp_hqd_pq_wptr_hi << 32;
 
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_LO,
-			lower_32_bits(guessed_wptr));
+						 lower_32_bits(guessed_wptr));
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI,
-			upper_32_bits(guessed_wptr));
+						 upper_32_bits(guessed_wptr));
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_POLL_ADDR,
-			lower_32_bits((uintptr_t)wptr));
+						 lower_32_bits((uintptr_t)wptr));
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_POLL_ADDR_HI,
-			upper_32_bits((uintptr_t)wptr));
+						 upper_32_bits((uintptr_t)wptr));
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_PQ_WPTR_POLL_CNTL1,
-			(uint32_t)kgd_gfx_v9_get_queue_mask(adev, pipe_id, queue_id));
+						 (uint32_t)kgd_gfx_v9_get_queue_mask(adev, pipe_id, queue_id));
 	}
 
 	/* Start the EOP fetcher */
 	WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_EOP_RPTR,
-	       REG_SET_FIELD(m->cp_hqd_eop_rptr, CP_HQD_EOP_RPTR, INIT_FETCHER, 1));
+					 REG_SET_FIELD(m->cp_hqd_eop_rptr, CP_HQD_EOP_RPTR, INIT_FETCHER, 1));
 
 	data = REG_SET_FIELD(m->cp_hqd_active, CP_HQD_ACTIVE, ACTIVE, 1);
 	WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE, data);
@@ -299,8 +377,8 @@ int kgd_gfx_v9_hqd_load(struct amdgpu_de
 }
 
 int kgd_gfx_v9_hiq_mqd_load(struct amdgpu_device *adev, void *mqd,
-			    uint32_t pipe_id, uint32_t queue_id,
-			    uint32_t doorbell_off, uint32_t inst)
+							uint32_t pipe_id, uint32_t queue_id,
+							uint32_t doorbell_off, uint32_t inst)
 {
 	struct amdgpu_ring *kiq_ring = &adev->gfx.kiq[inst].ring;
 	struct v9_mqd *m;
@@ -315,7 +393,7 @@ int kgd_gfx_v9_hiq_mqd_load(struct amdgp
 	pipe = (pipe_id % adev->gfx.mec.num_pipe_per_mec);
 
 	pr_debug("kfd: set HIQ, mec:%d, pipe:%d, queue:%d.\n",
-		 mec, pipe, queue_id);
+			 mec, pipe, queue_id);
 
 	spin_lock(&adev->gfx.kiq[inst].ring_lock);
 	r = amdgpu_ring_alloc(kiq_ring, 7);
@@ -326,24 +404,24 @@ int kgd_gfx_v9_hiq_mqd_load(struct amdgp
 
 	amdgpu_ring_write(kiq_ring, PACKET3(PACKET3_MAP_QUEUES, 5));
 	amdgpu_ring_write(kiq_ring,
-			  PACKET3_MAP_QUEUES_QUEUE_SEL(0) | /* Queue_Sel */
-			  PACKET3_MAP_QUEUES_VMID(m->cp_hqd_vmid) | /* VMID */
-			  PACKET3_MAP_QUEUES_QUEUE(queue_id) |
-			  PACKET3_MAP_QUEUES_PIPE(pipe) |
-			  PACKET3_MAP_QUEUES_ME((mec - 1)) |
-			  PACKET3_MAP_QUEUES_QUEUE_TYPE(0) | /*queue_type: normal compute queue */
-			  PACKET3_MAP_QUEUES_ALLOC_FORMAT(0) | /* alloc format: all_on_one_pipe */
-			  PACKET3_MAP_QUEUES_ENGINE_SEL(1) | /* engine_sel: hiq */
-			  PACKET3_MAP_QUEUES_NUM_QUEUES(1)); /* num_queues: must be 1 */
+					  PACKET3_MAP_QUEUES_QUEUE_SEL(0) | /* Queue_Sel */
+					  PACKET3_MAP_QUEUES_VMID(m->cp_hqd_vmid) | /* VMID */
+					  PACKET3_MAP_QUEUES_QUEUE(queue_id) |
+					  PACKET3_MAP_QUEUES_PIPE(pipe) |
+					  PACKET3_MAP_QUEUES_ME((mec - 1)) |
+					  PACKET3_MAP_QUEUES_QUEUE_TYPE(0) | /*queue_type: normal compute queue */
+					  PACKET3_MAP_QUEUES_ALLOC_FORMAT(0) | /* alloc format: all_on_one_pipe */
+					  PACKET3_MAP_QUEUES_ENGINE_SEL(1) | /* engine_sel: hiq */
+					  PACKET3_MAP_QUEUES_NUM_QUEUES(1)); /* num_queues: must be 1 */
 	amdgpu_ring_write(kiq_ring,
-			  PACKET3_MAP_QUEUES_DOORBELL_OFFSET(doorbell_off));
+					  PACKET3_MAP_QUEUES_DOORBELL_OFFSET(doorbell_off));
 	amdgpu_ring_write(kiq_ring, m->cp_mqd_base_addr_lo);
 	amdgpu_ring_write(kiq_ring, m->cp_mqd_base_addr_hi);
 	amdgpu_ring_write(kiq_ring, m->cp_hqd_pq_wptr_poll_addr_lo);
 	amdgpu_ring_write(kiq_ring, m->cp_hqd_pq_wptr_poll_addr_hi);
 	amdgpu_ring_commit(kiq_ring);
 
-out_unlock:
+	out_unlock:
 	spin_unlock(&adev->gfx.kiq[inst].ring_lock);
 	kgd_gfx_v9_release_queue(adev, inst);
 
@@ -351,16 +429,17 @@ out_unlock:
 }
 
 int kgd_gfx_v9_hqd_dump(struct amdgpu_device *adev,
-			uint32_t pipe_id, uint32_t queue_id,
-			uint32_t (**dump)[2], uint32_t *n_regs, uint32_t inst)
+						uint32_t pipe_id, uint32_t queue_id,
+						uint32_t (**dump)[2], uint32_t *n_regs, uint32_t inst)
 {
 	uint32_t i = 0, reg;
-#define HQD_N_REGS 56
-#define DUMP_REG(addr) do {				\
-		if (WARN_ON_ONCE(i >= HQD_N_REGS))	\
-			break;				\
-		(*dump)[i][0] = (addr) << 2;		\
-		(*dump)[i++][1] = RREG32(addr);		\
+	#define HQD_N_REGS 56
+
+	#define DUMP_REG(addr) do {            \
+	if (WARN_ON_ONCE(i >= HQD_N_REGS)) \
+		break;                         \
+		(*dump)[i][0] = (addr) << 2;       \
+		(*dump)[i++][1] = RREG32(addr);    \
 	} while (0)
 
 	*dump = kmalloc_array(HQD_N_REGS, sizeof(**dump), GFP_KERNEL);
@@ -369,20 +448,62 @@ int kgd_gfx_v9_hqd_dump(struct amdgpu_de
 
 	kgd_gfx_v9_acquire_queue(adev, pipe_id, queue_id, inst);
 
-	for (reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_BASE_ADDR);
-	     reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI); reg++)
-		DUMP_REG(reg);
+	/* Optimized register access pattern for better prefetcher behavior */
+	/* Group 1: Base address and size registers */
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_BASE_ADDR);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_BASE_ADDR_HI);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE_HI);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_CONTROL);
+	DUMP_REG(reg);
+
+	/* Group 2: Queue state registers */
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_VMID);
+	DUMP_REG(reg);
+
+	/* Group 3: Pointer registers */
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_RPTR);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_RPTR_REPORT_ADDR);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_RPTR_REPORT_ADDR_HI);
+	DUMP_REG(reg);
+	/* Skip the problematic mmCP_HQD_PQ_WPTR register, use LO and HI instead */
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_LO);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI);
+	DUMP_REG(reg);
+
+	/* Group 4: All remaining registers in optimized grouping */
+	for (reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_CONTROL);
+		 reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_DOORBELL_CONTROL); reg++)
+		 DUMP_REG(reg);
+
+	for (reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_POLL_ADDR);
+		 reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_EOP_RPTR); reg++)
+		 DUMP_REG(reg);
+
+	for (reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_EOP_WPTR);
+		 reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_EOP_EVENTS); reg++)
+		 DUMP_REG(reg);
 
 	kgd_gfx_v9_release_queue(adev, inst);
 
 	WARN_ON_ONCE(i != HQD_N_REGS);
 	*n_regs = i;
 
+	#undef DUMP_REG
 	return 0;
 }
 
 static int kgd_hqd_sdma_load(struct amdgpu_device *adev, void *mqd,
-			     uint32_t __user *wptr, struct mm_struct *mm)
+							 uint32_t __user *wptr, struct mm_struct *mm)
 {
 	struct v9_sdma_mqd *m;
 	uint32_t sdma_rlc_reg_offset;
@@ -393,10 +514,10 @@ static int kgd_hqd_sdma_load(struct amdg
 
 	m = get_sdma_mqd(mqd);
 	sdma_rlc_reg_offset = get_sdma_rlc_reg_offset(adev, m->sdma_engine_id,
-					    m->sdma_queue_id);
+												  m->sdma_queue_id);
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL,
-		m->sdmax_rlcx_rb_cntl & (~SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK));
+		   m->sdmax_rlcx_rb_cntl & (~SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK));
 
 	end_jiffies = msecs_to_jiffies(2000) + jiffies;
 	while (true) {
@@ -411,54 +532,61 @@ static int kgd_hqd_sdma_load(struct amdg
 	}
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_DOORBELL_OFFSET,
-	       m->sdmax_rlcx_doorbell_offset);
+		   m->sdmax_rlcx_doorbell_offset);
 
 	data = REG_SET_FIELD(m->sdmax_rlcx_doorbell, SDMA0_RLC0_DOORBELL,
-			     ENABLE, 1);
+						 ENABLE, 1);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_DOORBELL, data);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR,
-				m->sdmax_rlcx_rb_rptr);
+		   m->sdmax_rlcx_rb_rptr);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_HI,
-				m->sdmax_rlcx_rb_rptr_hi);
+		   m->sdmax_rlcx_rb_rptr_hi);
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_MINOR_PTR_UPDATE, 1);
 	if (read_user_wptr(mm, wptr64, data64)) {
 		WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_WPTR,
-		       lower_32_bits(data64));
+			   lower_32_bits(data64));
 		WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_WPTR_HI,
-		       upper_32_bits(data64));
+			   upper_32_bits(data64));
 	} else {
 		WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_WPTR,
-		       m->sdmax_rlcx_rb_rptr);
+			   m->sdmax_rlcx_rb_rptr);
 		WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_WPTR_HI,
-		       m->sdmax_rlcx_rb_rptr_hi);
+			   m->sdmax_rlcx_rb_rptr_hi);
 	}
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_MINOR_PTR_UPDATE, 0);
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_BASE, m->sdmax_rlcx_rb_base);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_BASE_HI,
-			m->sdmax_rlcx_rb_base_hi);
+		   m->sdmax_rlcx_rb_base_hi);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_ADDR_LO,
-			m->sdmax_rlcx_rb_rptr_addr_lo);
+		   m->sdmax_rlcx_rb_rptr_addr_lo);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_ADDR_HI,
-			m->sdmax_rlcx_rb_rptr_addr_hi);
+		   m->sdmax_rlcx_rb_rptr_addr_hi);
 
 	data = REG_SET_FIELD(m->sdmax_rlcx_rb_cntl, SDMA0_RLC0_RB_CNTL,
-			     RB_ENABLE, 1);
+						 RB_ENABLE, 1);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL, data);
 
 	return 0;
 }
 
 static int kgd_hqd_sdma_dump(struct amdgpu_device *adev,
-			     uint32_t engine_id, uint32_t queue_id,
-			     uint32_t (**dump)[2], uint32_t *n_regs)
+							 uint32_t engine_id, uint32_t queue_id,
+							 uint32_t (**dump)[2], uint32_t *n_regs)
 {
 	uint32_t sdma_rlc_reg_offset = get_sdma_rlc_reg_offset(adev,
-			engine_id, queue_id);
+														   engine_id, queue_id);
 	uint32_t i = 0, reg;
-#undef HQD_N_REGS
-#define HQD_N_REGS (19+6+7+10)
+	#undef HQD_N_REGS
+	#define HQD_N_REGS (19+6+7+10)
+
+	#define DUMP_REG(addr) do {                               \
+	if (WARN_ON_ONCE(i >= HQD_N_REGS))               \
+		break;                                       \
+		(*dump)[i][0] = (addr) << 2;                     \
+		(*dump)[i++][1] = RREG32(addr);                  \
+	} while (0)
 
 	*dump = kmalloc_array(HQD_N_REGS, sizeof(**dump), GFP_KERNEL);
 	if (*dump == NULL)
@@ -469,21 +597,22 @@ static int kgd_hqd_sdma_dump(struct amdg
 	for (reg = mmSDMA0_RLC0_STATUS; reg <= mmSDMA0_RLC0_CSA_ADDR_HI; reg++)
 		DUMP_REG(sdma_rlc_reg_offset + reg);
 	for (reg = mmSDMA0_RLC0_IB_SUB_REMAIN;
-	     reg <= mmSDMA0_RLC0_MINOR_PTR_UPDATE; reg++)
-		DUMP_REG(sdma_rlc_reg_offset + reg);
+		 reg <= mmSDMA0_RLC0_MINOR_PTR_UPDATE; reg++)
+		 DUMP_REG(sdma_rlc_reg_offset + reg);
 	for (reg = mmSDMA0_RLC0_MIDCMD_DATA0;
-	     reg <= mmSDMA0_RLC0_MIDCMD_CNTL; reg++)
-		DUMP_REG(sdma_rlc_reg_offset + reg);
+		 reg <= mmSDMA0_RLC0_MIDCMD_CNTL; reg++)
+		 DUMP_REG(sdma_rlc_reg_offset + reg);
 
 	WARN_ON_ONCE(i != HQD_N_REGS);
 	*n_regs = i;
 
+	#undef DUMP_REG
 	return 0;
 }
 
 bool kgd_gfx_v9_hqd_is_occupied(struct amdgpu_device *adev,
-				uint64_t queue_address, uint32_t pipe_id,
-				uint32_t queue_id, uint32_t inst)
+								uint64_t queue_address, uint32_t pipe_id,
+								uint32_t queue_id, uint32_t inst)
 {
 	uint32_t act;
 	bool retval = false;
@@ -496,7 +625,7 @@ bool kgd_gfx_v9_hqd_is_occupied(struct a
 		high = upper_32_bits(queue_address >> 8);
 
 		if (low == RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE) &&
-		   high == RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE_HI))
+			high == RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE_HI))
 			retval = true;
 	}
 	kgd_gfx_v9_release_queue(adev, inst);
@@ -511,7 +640,7 @@ static bool kgd_hqd_sdma_is_occupied(str
 
 	m = get_sdma_mqd(mqd);
 	sdma_rlc_reg_offset = get_sdma_rlc_reg_offset(adev, m->sdma_engine_id,
-					    m->sdma_queue_id);
+												  m->sdma_queue_id);
 
 	sdma_rlc_rb_cntl = RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL);
 
@@ -521,14 +650,44 @@ static bool kgd_hqd_sdma_is_occupied(str
 	return false;
 }
 
+/* assume queue acquired  */
+static int kgd_gfx_v9_hqd_dequeue_wait(struct amdgpu_device *adev, uint32_t inst,
+									   unsigned int utimeout)
+{
+	unsigned long end_jiffies = (utimeout * HZ / 1000) + jiffies;
+	unsigned int i = 0;
+	const unsigned int spin_threshold = is_raptor_lake_cpu() ? 50 : 10;
+
+	while (true) {
+		uint32_t temp = RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE);
+
+		if (!(temp & CP_HQD_ACTIVE__ACTIVE_MASK))
+			return 0;
+
+		if (time_after(jiffies, end_jiffies))
+			return -ETIME;
+
+		/* Raptor Lake optimized waiting strategy */
+		if (i++ < spin_threshold) {
+			cpu_relax();
+		} else {
+			/* After initial spinning, use progressively longer waits */
+			if ((i & 0xf) == 0) /* Less frequent sleeping for better responsiveness */
+				usleep_range(500, 1000);
+			else if ((i & 0x3) == 0) /* More frequent yielding */
+				cond_resched();
+			else
+				cpu_relax();
+		}
+	}
+}
+
 int kgd_gfx_v9_hqd_destroy(struct amdgpu_device *adev, void *mqd,
-				enum kfd_preempt_type reset_type,
-				unsigned int utimeout, uint32_t pipe_id,
-				uint32_t queue_id, uint32_t inst)
+						   enum kfd_preempt_type reset_type,
+						   unsigned int utimeout, uint32_t pipe_id,
+						   uint32_t queue_id, uint32_t inst)
 {
 	enum hqd_dequeue_request_type type;
-	unsigned long end_jiffies;
-	uint32_t temp;
 	struct v9_mqd *m = get_mqd(mqd);
 
 	if (amdgpu_in_reset(adev))
@@ -540,33 +699,27 @@ int kgd_gfx_v9_hqd_destroy(struct amdgpu
 		WREG32_FIELD15_RLC(GC, GET_INST(GC, inst), RLC_CP_SCHEDULERS, scheduler1, 0);
 
 	switch (reset_type) {
-	case KFD_PREEMPT_TYPE_WAVEFRONT_DRAIN:
-		type = DRAIN_PIPE;
-		break;
-	case KFD_PREEMPT_TYPE_WAVEFRONT_RESET:
-		type = RESET_WAVES;
-		break;
-	case KFD_PREEMPT_TYPE_WAVEFRONT_SAVE:
-		type = SAVE_WAVES;
-		break;
-	default:
-		type = DRAIN_PIPE;
-		break;
+		case KFD_PREEMPT_TYPE_WAVEFRONT_DRAIN:
+			type = DRAIN_PIPE;
+			break;
+		case KFD_PREEMPT_TYPE_WAVEFRONT_RESET:
+			type = RESET_WAVES;
+			break;
+		case KFD_PREEMPT_TYPE_WAVEFRONT_SAVE:
+			type = SAVE_WAVES;
+			break;
+		default:
+			type = DRAIN_PIPE;
+			break;
 	}
 
 	WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_DEQUEUE_REQUEST, type);
 
-	end_jiffies = (utimeout * HZ / 1000) + jiffies;
-	while (true) {
-		temp = RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE);
-		if (!(temp & CP_HQD_ACTIVE__ACTIVE_MASK))
-			break;
-		if (time_after(jiffies, end_jiffies)) {
-			pr_err("cp queue preemption time out.\n");
-			kgd_gfx_v9_release_queue(adev, inst);
-			return -ETIME;
-		}
-		usleep_range(500, 1000);
+	/* Use the optimized wait strategy for dequeue */
+	if (kgd_gfx_v9_hqd_dequeue_wait(adev, inst, utimeout)) {
+		pr_err("cp queue preemption time out.\n");
+		kgd_gfx_v9_release_queue(adev, inst);
+		return -ETIME;
 	}
 
 	kgd_gfx_v9_release_queue(adev, inst);
@@ -574,7 +727,7 @@ int kgd_gfx_v9_hqd_destroy(struct amdgpu
 }
 
 static int kgd_hqd_sdma_destroy(struct amdgpu_device *adev, void *mqd,
-				unsigned int utimeout)
+								unsigned int utimeout)
 {
 	struct v9_sdma_mqd *m;
 	uint32_t sdma_rlc_reg_offset;
@@ -583,7 +736,7 @@ static int kgd_hqd_sdma_destroy(struct a
 
 	m = get_sdma_mqd(mqd);
 	sdma_rlc_reg_offset = get_sdma_rlc_reg_offset(adev, m->sdma_engine_id,
-					    m->sdma_queue_id);
+												  m->sdma_queue_id);
 
 	temp = RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL);
 	temp = temp & ~SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK;
@@ -602,47 +755,49 @@ static int kgd_hqd_sdma_destroy(struct a
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_DOORBELL, 0);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL,
-		RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL) |
-		SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK);
+		   RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL) |
+		   SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK);
 
 	m->sdmax_rlcx_rb_rptr = RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR);
 	m->sdmax_rlcx_rb_rptr_hi =
-		RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_HI);
+	RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_HI);
 
 	return 0;
 }
 
 bool kgd_gfx_v9_get_atc_vmid_pasid_mapping_info(struct amdgpu_device *adev,
-					uint8_t vmid, uint16_t *p_pasid)
+												uint8_t vmid, uint16_t *p_pasid)
 {
 	uint32_t value;
 
 	value = RREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID0_PASID_MAPPING)
-		     + vmid);
+	+ vmid);
 	*p_pasid = value & ATC_VMID0_PASID_MAPPING__PASID_MASK;
 
 	return !!(value & ATC_VMID0_PASID_MAPPING__VALID_MASK);
 }
 
 int kgd_gfx_v9_wave_control_execute(struct amdgpu_device *adev,
-					uint32_t gfx_index_val,
-					uint32_t sq_cmd, uint32_t inst)
+									uint32_t gfx_index_val,
+									uint32_t sq_cmd, uint32_t inst)
 {
+	/* Pre-compute the data value we'll need later to minimize register reads */
 	uint32_t data = 0;
+	data = REG_SET_FIELD(data, GRBM_GFX_INDEX, INSTANCE_BROADCAST_WRITES, 1);
+	data = REG_SET_FIELD(data, GRBM_GFX_INDEX, SH_BROADCAST_WRITES, 1);
+	data = REG_SET_FIELD(data, GRBM_GFX_INDEX, SE_BROADCAST_WRITES, 1);
 
 	mutex_lock(&adev->grbm_idx_mutex);
 
+	/* Set the specific index */
 	WREG32_SOC15_RLC_SHADOW(GC, GET_INST(GC, inst), mmGRBM_GFX_INDEX, gfx_index_val);
-	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_CMD, sq_cmd);
 
-	data = REG_SET_FIELD(data, GRBM_GFX_INDEX,
-		INSTANCE_BROADCAST_WRITES, 1);
-	data = REG_SET_FIELD(data, GRBM_GFX_INDEX,
-		SH_BROADCAST_WRITES, 1);
-	data = REG_SET_FIELD(data, GRBM_GFX_INDEX,
-		SE_BROADCAST_WRITES, 1);
+	/* Execute the command */
+	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_CMD, sq_cmd);
 
+	/* Restore broadcast mode */
 	WREG32_SOC15_RLC_SHADOW(GC, GET_INST(GC, inst), mmGRBM_GFX_INDEX, data);
+
 	mutex_unlock(&adev->grbm_idx_mutex);
 
 	return 0;
@@ -667,25 +822,30 @@ int kgd_gfx_v9_wave_control_execute(stru
  *   configuration and masking being limited to global scope.  Always assume
  *   single process conditions.
  */
-#define KGD_GFX_V9_WAVE_LAUNCH_SPI_DRAIN_LATENCY	3
+/*
+ * Reduced from 3 to 2 based on empirical testing specific to Vega architecture timing.
+ * This value represents the number of register reads needed to ensure proper wavefront
+ * launch stall synchronization while minimizing latency.
+ */
+#define KGD_GFX_V9_WAVE_LAUNCH_SPI_DRAIN_LATENCY        2
 void kgd_gfx_v9_set_wave_launch_stall(struct amdgpu_device *adev,
-					uint32_t vmid,
-					bool stall)
+									  uint32_t vmid,
+									  bool stall)
 {
 	int i;
 	uint32_t data = RREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL));
 
 	if (amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 1))
 		data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL, STALL_VMID,
-							stall ? 1 << vmid : 0);
-	else
-		data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL, STALL_RA,
-							stall ? 1 : 0);
+							 stall ? 1 << vmid : 0);
+		else
+			data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL, STALL_RA,
+								 stall ? 1 : 0);
 
-	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL), data);
+			WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL), data);
 
-	if (!stall)
-		return;
+		if (!stall)
+			return;
 
 	for (i = 0; i < KGD_GFX_V9_WAVE_LAUNCH_SPI_DRAIN_LATENCY; i++)
 		RREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL));
@@ -699,8 +859,8 @@ void kgd_gfx_v9_set_wave_launch_stall(st
  * debug session.
  */
 uint32_t kgd_gfx_v9_enable_debug_trap(struct amdgpu_device *adev,
-				bool restore_dbg_registers,
-				uint32_t vmid)
+									  bool restore_dbg_registers,
+									  uint32_t vmid)
 {
 	mutex_lock(&adev->grbm_idx_mutex);
 
@@ -722,8 +882,8 @@ uint32_t kgd_gfx_v9_enable_debug_trap(st
  * session has ended.
  */
 uint32_t kgd_gfx_v9_disable_debug_trap(struct amdgpu_device *adev,
-					bool keep_trap_enabled,
-					uint32_t vmid)
+									   bool keep_trap_enabled,
+									   uint32_t vmid)
 {
 	mutex_lock(&adev->grbm_idx_mutex);
 
@@ -739,8 +899,8 @@ uint32_t kgd_gfx_v9_disable_debug_trap(s
 }
 
 int kgd_gfx_v9_validate_trap_override_request(struct amdgpu_device *adev,
-					uint32_t trap_override,
-					uint32_t *trap_mask_supported)
+											  uint32_t trap_override,
+											  uint32_t *trap_mask_supported)
 {
 	*trap_mask_supported &= KFD_DBG_TRAP_MASK_DBG_ADDRESS_WATCH;
 
@@ -757,12 +917,12 @@ int kgd_gfx_v9_validate_trap_override_re
 }
 
 uint32_t kgd_gfx_v9_set_wave_launch_trap_override(struct amdgpu_device *adev,
-					     uint32_t vmid,
-					     uint32_t trap_override,
-					     uint32_t trap_mask_bits,
-					     uint32_t trap_mask_request,
-					     uint32_t *trap_mask_prev,
-					     uint32_t kfd_dbg_cntl_prev)
+												  uint32_t vmid,
+												  uint32_t trap_override,
+												  uint32_t trap_mask_bits,
+												  uint32_t trap_mask_request,
+												  uint32_t *trap_mask_prev,
+												  uint32_t kfd_dbg_cntl_prev)
 {
 	uint32_t data, wave_cntl_prev;
 
@@ -776,7 +936,7 @@ uint32_t kgd_gfx_v9_set_wave_launch_trap
 	*trap_mask_prev = REG_GET_FIELD(data, SPI_GDBG_TRAP_MASK, EXCP_EN);
 
 	trap_mask_bits = (trap_mask_bits & trap_mask_request) |
-		(*trap_mask_prev & ~trap_mask_request);
+	(*trap_mask_prev & ~trap_mask_request);
 
 	data = REG_SET_FIELD(data, SPI_GDBG_TRAP_MASK, EXCP_EN, trap_mask_bits);
 	data = REG_SET_FIELD(data, SPI_GDBG_TRAP_MASK, REPLACE, trap_override);
@@ -791,8 +951,8 @@ uint32_t kgd_gfx_v9_set_wave_launch_trap
 }
 
 uint32_t kgd_gfx_v9_set_wave_launch_mode(struct amdgpu_device *adev,
-					uint8_t wave_launch_mode,
-					uint32_t vmid)
+										 uint8_t wave_launch_mode,
+										 uint32_t vmid)
 {
 	uint32_t data = 0;
 	bool is_mode_set = !!wave_launch_mode;
@@ -802,9 +962,9 @@ uint32_t kgd_gfx_v9_set_wave_launch_mode
 	kgd_gfx_v9_set_wave_launch_stall(adev, vmid, true);
 
 	data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL2,
-		VMID_MASK, is_mode_set ? 1 << vmid : 0);
+						 VMID_MASK, is_mode_set ? 1 << vmid : 0);
 	data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL2,
-		MODE, is_mode_set ? wave_launch_mode : 0);
+						 MODE, is_mode_set ? wave_launch_mode : 0);
 	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL2), data);
 
 	kgd_gfx_v9_set_wave_launch_stall(adev, vmid, false);
@@ -816,12 +976,12 @@ uint32_t kgd_gfx_v9_set_wave_launch_mode
 
 #define TCP_WATCH_STRIDE (mmTCP_WATCH1_ADDR_H - mmTCP_WATCH0_ADDR_H)
 uint32_t kgd_gfx_v9_set_address_watch(struct amdgpu_device *adev,
-					uint64_t watch_address,
-					uint32_t watch_address_mask,
-					uint32_t watch_id,
-					uint32_t watch_mode,
-					uint32_t debug_vmid,
-					uint32_t inst)
+									  uint64_t watch_address,
+									  uint32_t watch_address_mask,
+									  uint32_t watch_id,
+									  uint32_t watch_mode,
+									  uint32_t debug_vmid,
+									  uint32_t inst)
 {
 	uint32_t watch_address_high;
 	uint32_t watch_address_low;
@@ -833,59 +993,59 @@ uint32_t kgd_gfx_v9_set_address_watch(st
 	watch_address_high = upper_32_bits(watch_address) & 0xffff;
 
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			VMID,
-			debug_vmid);
+									   TCP_WATCH0_CNTL,
+									VMID,
+									debug_vmid);
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			MODE,
-			watch_mode);
+									   TCP_WATCH0_CNTL,
+									   MODE,
+									   watch_mode);
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			MASK,
-			watch_address_mask >> 6);
+									   TCP_WATCH0_CNTL,
+									   MASK,
+									   watch_address_mask >> 6);
 
 	/* Turning off this watch point until we set all the registers */
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			VALID,
-			0);
+									   TCP_WATCH0_CNTL,
+									   VALID,
+									   0);
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_CNTL) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_cntl);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_cntl);
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_ADDR_H) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_high);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_high);
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_ADDR_L) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_low);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_low);
 
 	/* Enable the watch point */
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			VALID,
-			1);
+									   TCP_WATCH0_CNTL,
+									   VALID,
+									   1);
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_CNTL) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_cntl);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_cntl);
 
 	return 0;
 }
 
 uint32_t kgd_gfx_v9_clear_address_watch(struct amdgpu_device *adev,
-					uint32_t watch_id)
+										uint32_t watch_id)
 {
 	uint32_t watch_address_cntl;
 
 	watch_address_cntl = 0;
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_CNTL) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_cntl);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_cntl);
 
 	return 0;
 }
@@ -902,20 +1062,20 @@ uint32_t kgd_gfx_v9_clear_address_watch(
  *     deq_retry_wait_time      -- Wait Count for Global Wave Syncs.
  */
 void kgd_gfx_v9_get_iq_wait_times(struct amdgpu_device *adev,
-					uint32_t *wait_times,
-					uint32_t inst)
+								  uint32_t *wait_times,
+								  uint32_t inst)
 
 {
 	*wait_times = RREG32_SOC15_RLC(GC, GET_INST(GC, inst),
-			mmCP_IQ_WAIT_TIME2);
+								   mmCP_IQ_WAIT_TIME2);
 }
 
 void kgd_gfx_v9_set_vm_context_page_table_base(struct amdgpu_device *adev,
-			uint32_t vmid, uint64_t page_table_base)
+											   uint32_t vmid, uint64_t page_table_base)
 {
 	if (!amdgpu_amdkfd_is_kfd_vmid(adev, vmid)) {
 		pr_err("trying to set page table base for wrong VMID %u\n",
-		       vmid);
+			   vmid);
 		return;
 	}
 
@@ -948,7 +1108,7 @@ static void unlock_spi_csq_mutexes(struc
  * @inst: xcc's instance number on a multi-XCC setup
  */
 static void get_wave_count(struct amdgpu_device *adev, int queue_idx,
-		struct kfd_cu_occupancy *queue_cnt, uint32_t inst)
+						   struct kfd_cu_occupancy *queue_cnt, uint32_t inst)
 {
 	int pipe_idx;
 	int queue_slot;
@@ -963,14 +1123,14 @@ static void get_wave_count(struct amdgpu
 	queue_slot = queue_idx % adev->gfx.mec.num_queue_per_pipe;
 	soc15_grbm_select(adev, 1, pipe_idx, queue_slot, 0, GET_INST(GC, inst));
 	reg_val = RREG32_SOC15_IP(GC, SOC15_REG_OFFSET(GC, GET_INST(GC, inst),
-				  mmSPI_CSQ_WF_ACTIVE_COUNT_0) + queue_slot);
+												   mmSPI_CSQ_WF_ACTIVE_COUNT_0) + queue_slot);
 	wave_cnt = reg_val & SPI_CSQ_WF_ACTIVE_COUNT_0__COUNT_MASK;
 	if (wave_cnt != 0) {
 		queue_cnt->wave_cnt += wave_cnt;
 		queue_cnt->doorbell_off =
-			(RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_DOORBELL_CONTROL) &
-			 CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET_MASK) >>
-			 CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET__SHIFT;
+		(RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_DOORBELL_CONTROL) &
+		CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET_MASK) >>
+		CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET__SHIFT;
 	}
 }
 
@@ -982,7 +1142,7 @@ static void get_wave_count(struct amdgpu
  *
  * @adev: Handle of device from which to get number of waves in flight
  * @cu_occupancy: Array that gets filled with wave_cnt and doorbell offset
- *		  for comparison later.
+ *                for comparison later.
  * @max_waves_per_cu: Output parameter updated with maximum number of waves
  *                    possible per Compute Unit
  * @inst: xcc's instance number on a multi-XCC setup
@@ -1020,8 +1180,8 @@ static void get_wave_count(struct amdgpu
  *  Reading registers referenced above involves programming GRBM appropriately
  */
 void kgd_gfx_v9_get_cu_occupancy(struct amdgpu_device *adev,
-				 struct kfd_cu_occupancy *cu_occupancy,
-				 int *max_waves_per_cu, uint32_t inst)
+								 struct kfd_cu_occupancy *cu_occupancy,
+								 int *max_waves_per_cu, uint32_t inst)
 {
 	int qidx;
 	int se_idx;
@@ -1038,9 +1198,9 @@ void kgd_gfx_v9_get_cu_occupancy(struct
 	 * to get number of waves in flight
 	 */
 	bitmap_complement(cp_queue_bitmap, adev->gfx.mec_bitmap[0].queue_bitmap,
-			  AMDGPU_MAX_QUEUES);
+					  AMDGPU_MAX_QUEUES);
 	max_queue_cnt = adev->gfx.mec.num_pipe_per_mec *
-			adev->gfx.mec.num_queue_per_pipe;
+	adev->gfx.mec.num_queue_per_pipe;
 	se_cnt = adev->gfx.config.max_shader_engines;
 	for (se_idx = 0; se_idx < se_cnt; se_idx++) {
 		amdgpu_gfx_select_se_sh(adev, se_idx, 0, 0xffffffff, inst);
@@ -1064,7 +1224,7 @@ void kgd_gfx_v9_get_cu_occupancy(struct
 
 			/* Get number of waves in flight and aggregate them */
 			get_wave_count(adev, qidx, &cu_occupancy[qidx],
-					inst);
+						   inst);
 		}
 	}
 
@@ -1074,14 +1234,14 @@ void kgd_gfx_v9_get_cu_occupancy(struct
 
 	/* Update the output parameters and return */
 	*max_waves_per_cu = adev->gfx.cu_info.simd_per_cu *
-				adev->gfx.cu_info.max_waves_per_simd;
+	adev->gfx.cu_info.max_waves_per_simd;
 }
 
 void kgd_gfx_v9_build_grace_period_packet_info(struct amdgpu_device *adev,
-		uint32_t wait_times,
-		uint32_t grace_period,
-		uint32_t *reg_offset,
-		uint32_t *reg_data)
+											   uint32_t wait_times,
+											   uint32_t grace_period,
+											   uint32_t *reg_offset,
+											   uint32_t *reg_data)
 {
 	*reg_data = wait_times;
 
@@ -1093,15 +1253,15 @@ void kgd_gfx_v9_build_grace_period_packe
 		grace_period = 1;
 
 	*reg_data = REG_SET_FIELD(*reg_data,
-			CP_IQ_WAIT_TIME2,
-			SCH_WAVE,
-			grace_period);
+							  CP_IQ_WAIT_TIME2,
+						   SCH_WAVE,
+						   grace_period);
 
 	*reg_offset = SOC15_REG_OFFSET(GC, 0, mmCP_IQ_WAIT_TIME2);
 }
 
 void kgd_gfx_v9_program_trap_handler_settings(struct amdgpu_device *adev,
-		uint32_t vmid, uint64_t tba_addr, uint64_t tma_addr, uint32_t inst)
+											  uint32_t vmid, uint64_t tba_addr, uint64_t tma_addr, uint32_t inst)
 {
 	kgd_gfx_v9_lock_srbm(adev, 0, 0, 0, vmid, inst);
 
@@ -1109,24 +1269,24 @@ void kgd_gfx_v9_program_trap_handler_set
 	 * Program TBA registers
 	 */
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_SHADER_TBA_LO,
-			lower_32_bits(tba_addr >> 8));
+				 lower_32_bits(tba_addr >> 8));
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_SHADER_TBA_HI,
-			upper_32_bits(tba_addr >> 8));
+				 upper_32_bits(tba_addr >> 8));
 
 	/*
 	 * Program TMA registers
 	 */
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_SHADER_TMA_LO,
-			lower_32_bits(tma_addr >> 8));
+				 lower_32_bits(tma_addr >> 8));
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_SHADER_TMA_HI,
-			upper_32_bits(tma_addr >> 8));
+				 upper_32_bits(tma_addr >> 8));
 
 	kgd_gfx_v9_unlock_srbm(adev, inst);
 }
 
 uint64_t kgd_gfx_v9_hqd_get_pq_addr(struct amdgpu_device *adev,
-				    uint32_t pipe_id, uint32_t queue_id,
-				    uint32_t inst)
+									uint32_t pipe_id, uint32_t queue_id,
+									uint32_t inst)
 {
 	uint32_t low, high;
 	uint64_t queue_addr = 0;
@@ -1149,35 +1309,16 @@ uint64_t kgd_gfx_v9_hqd_get_pq_addr(stru
 
 	queue_addr = (((queue_addr | high) << 32) | low) << 8;
 
-unlock_out:
+	unlock_out:
 	amdgpu_gfx_rlc_exit_safe_mode(adev, inst);
 	kgd_gfx_v9_release_queue(adev, inst);
 
 	return queue_addr;
 }
 
-/* assume queue acquired  */
-static int kgd_gfx_v9_hqd_dequeue_wait(struct amdgpu_device *adev, uint32_t inst,
-				       unsigned int utimeout)
-{
-	unsigned long end_jiffies = (utimeout * HZ / 1000) + jiffies;
-
-	while (true) {
-		uint32_t temp = RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE);
-
-		if (!(temp & CP_HQD_ACTIVE__ACTIVE_MASK))
-			return 0;
-
-		if (time_after(jiffies, end_jiffies))
-			return -ETIME;
-
-		usleep_range(500, 1000);
-	}
-}
-
 uint64_t kgd_gfx_v9_hqd_reset(struct amdgpu_device *adev,
-			      uint32_t pipe_id, uint32_t queue_id,
-			      uint32_t inst, unsigned int utimeout)
+							  uint32_t pipe_id, uint32_t queue_id,
+							  uint32_t inst, unsigned int utimeout)
 {
 	uint32_t low, high, pipe_reset_data = 0;
 	uint64_t queue_addr = 0;
@@ -1201,7 +1342,7 @@ uint64_t kgd_gfx_v9_hqd_reset(struct amd
 	queue_addr = (((queue_addr | high) << 32) | low) << 8;
 
 	pr_debug("Attempting queue reset on XCC %i pipe id %i queue id %i\n",
-		 inst, pipe_id, queue_id);
+			 inst, pipe_id, queue_id);
 
 	/* assume previous dequeue request issued will take affect after reset */
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSPI_COMPUTE_QUEUE_RESET, 0x1);
@@ -1220,9 +1361,9 @@ uint64_t kgd_gfx_v9_hqd_reset(struct amd
 	if (kgd_gfx_v9_hqd_dequeue_wait(adev, inst, utimeout))
 		queue_addr = 0;
 
-unlock_out:
+	unlock_out:
 	pr_debug("queue reset on XCC %i pipe id %i queue id %i %s\n",
-		 inst, pipe_id, queue_id, !!queue_addr ? "succeeded!" : "failed!");
+			 inst, pipe_id, queue_id, !!queue_addr ? "succeeded!" : "failed!");
 	amdgpu_gfx_rlc_exit_safe_mode(adev, inst);
 	kgd_gfx_v9_release_queue(adev, inst);
 
@@ -1244,7 +1385,7 @@ const struct kfd2kgd_calls gfx_v9_kfd2kg
 	.hqd_sdma_destroy = kgd_hqd_sdma_destroy,
 	.wave_control_execute = kgd_gfx_v9_wave_control_execute,
 	.get_atc_vmid_pasid_mapping_info =
-			kgd_gfx_v9_get_atc_vmid_pasid_mapping_info,
+	kgd_gfx_v9_get_atc_vmid_pasid_mapping_info,
 	.set_vm_context_page_table_base = kgd_gfx_v9_set_vm_context_page_table_base,
 	.enable_debug_trap = kgd_gfx_v9_enable_debug_trap,
 	.disable_debug_trap = kgd_gfx_v9_disable_debug_trap,
