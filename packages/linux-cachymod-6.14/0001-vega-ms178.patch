--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_irq.h	2025-04-25 10:51:21.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_irq.h	2025-04-26 19:30:33.996606337 +0200
@@ -25,6 +25,7 @@
 #define __AMDGPU_IRQ_H__
 
 #include <linux/irqdomain.h>
+#include <linux/irq_work.h>
 #include "soc15_ih_clientid.h"
 #include "amdgpu_ih.h"
 
@@ -82,24 +83,38 @@ struct amdgpu_irq {
 	bool				installed;
 	unsigned int			irq;
 	spinlock_t			lock;
+
 	/* interrupt sources */
 	struct amdgpu_irq_client	client[AMDGPU_IRQ_CLIENTID_MAX];
 
 	/* status, etc. */
-	bool				msi_enabled; /* msi enabled */
+	bool				msi_enabled;		/* MSI enabled */
 
 	/* interrupt rings */
 	struct amdgpu_ih_ring		ih, ih1, ih2, ih_soft;
 	const struct amdgpu_ih_funcs    *ih_funcs;
-	struct work_struct		ih1_work, ih2_work, ih_soft_work;
+
+	/* legacy workqueue bottom-halves (kept for structure stability) */
+	struct work_struct		ih1_work;
+	struct work_struct		ih2_work;
+	struct work_struct		ih_soft_work;
+
+	/* new fast bottom-halves executed via irq_work */
+	struct irq_work			ih1_iw;
+	struct irq_work			ih2_iw;
+	struct irq_work			ih_soft_iw;
+
+	/* self-IRQ source */
 	struct amdgpu_irq_src		self_irq;
 
-	/* gen irq stuff */
-	struct irq_domain		*domain; /* GPU irq controller domain */
+	/* generic IRQ infrastructure */
+	struct irq_domain		*domain;		/* GPU IRQ domain */
 	unsigned			virq[AMDGPU_MAX_IRQ_SRC_ID];
-	uint32_t                        srbm_soft_reset;
-	u32                             retry_cam_doorbell_index;
-	bool                            retry_cam_enabled;
+
+	/* misc */
+	uint32_t			srbm_soft_reset;
+	u32				retry_cam_doorbell_index;
+	bool				retry_cam_enabled;
 };
 
 enum interrupt_node_id_per_aid {


--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_irq.c	2025-04-25 10:51:21.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_irq.c	2025-04-26 19:35:40.185257128 +0200
@@ -43,6 +43,7 @@
  */
 
 #include <linux/irq.h>
+#include <linux/irq_work.h>
 #include <linux/pci.h>
 
 #include <drm/drm_vblank.h>
@@ -114,6 +115,16 @@ const int node_id_to_phys_map[NODEID_MAX
 	[XCD7_NODEID] = 7,
 };
 
+/* Fast bottom-half executed in soft-IRQ context */
+static void amdgpu_irq_handle_ih_soft_iw(struct irq_work *iw)
+{
+	struct amdgpu_device *adev =
+	container_of(iw, struct amdgpu_device, irq.ih_soft_iw);
+
+	/* same payload as the former workqueue handler */
+	amdgpu_ih_process(adev, &adev->irq.ih_soft);
+}
+
 /**
  * amdgpu_irq_disable_all - disable *all* interrupts
  *
@@ -123,31 +134,36 @@ const int node_id_to_phys_map[NODEID_MAX
  */
 void amdgpu_irq_disable_all(struct amdgpu_device *adev)
 {
-	unsigned long irqflags;
+	unsigned long flags;
 	unsigned int i, j, k;
-	int r;
 
-	spin_lock_irqsave(&adev->irq.lock, irqflags);
-	for (i = 0; i < AMDGPU_IRQ_CLIENTID_MAX; ++i) {
-		if (!adev->irq.client[i].sources)
+	spin_lock_irqsave(&adev->irq.lock, flags);
+
+	for (i = 0; i < AMDGPU_IRQ_CLIENTID_MAX; i++) {
+		struct amdgpu_irq_client *cl = &adev->irq.client[i];
+
+		if (!cl->sources)
 			continue;
 
-		for (j = 0; j < AMDGPU_MAX_IRQ_SRC_ID; ++j) {
-			struct amdgpu_irq_src *src = adev->irq.client[i].sources[j];
+		for (j = 0; j < AMDGPU_MAX_IRQ_SRC_ID; j++) {
+			struct amdgpu_irq_src *src = cl->sources[j];
 
 			if (!src || !src->funcs->set || !src->num_types)
 				continue;
 
-			for (k = 0; k < src->num_types; ++k) {
-				r = src->funcs->set(adev, src, k,
-						    AMDGPU_IRQ_STATE_DISABLE);
-				if (r)
-					DRM_ERROR("error disabling interrupt (%d)\n",
-						  r);
+			for (k = 0; k < src->num_types; k++) {
+				if (!atomic_read(&src->enabled_types[k]))
+					continue;
+
+				if (src->funcs->set(adev, src, k,
+					AMDGPU_IRQ_STATE_DISABLE))
+					DRM_ERROR("error disabling IRQ %u/%u\n",
+							  i, j);
 			}
 		}
 	}
-	spin_unlock_irqrestore(&adev->irq.lock, irqflags);
+
+	spin_unlock_irqrestore(&adev->irq.lock, flags);
 }
 
 /**
@@ -207,21 +223,6 @@ static void amdgpu_irq_handle_ih2(struct
 }
 
 /**
- * amdgpu_irq_handle_ih_soft - kick of processing for ih_soft
- *
- * @work: work structure in struct amdgpu_irq
- *
- * Kick of processing IH soft ring.
- */
-static void amdgpu_irq_handle_ih_soft(struct work_struct *work)
-{
-	struct amdgpu_device *adev = container_of(work, struct amdgpu_device,
-						  irq.ih_soft_work);
-
-	amdgpu_ih_process(adev, &adev->irq.ih_soft);
-}
-
-/**
  * amdgpu_msi_ok - check whether MSI functionality is enabled
  *
  * @adev: amdgpu device pointer (unused)
@@ -273,55 +274,64 @@ int amdgpu_irq_init(struct amdgpu_device
 	unsigned int irq, flags;
 	int r;
 
+	/* ---------------- generic setup ---------------- */
 	spin_lock_init(&adev->irq.lock);
 
-	/* Enable MSI if not disabled by module parameter */
 	adev->irq.msi_enabled = false;
+	flags = amdgpu_msi_ok(adev) ? PCI_IRQ_ALL_TYPES : PCI_IRQ_INTX;
 
-	if (!amdgpu_msi_ok(adev))
-		flags = PCI_IRQ_INTX;
-	else
-		flags = PCI_IRQ_ALL_TYPES;
-
-	/* we only need one vector */
 	r = pci_alloc_irq_vectors(adev->pdev, 1, 1, flags);
 	if (r < 0) {
-		dev_err(adev->dev, "Failed to alloc msi vectors\n");
+		dev_err(adev->dev, "failed to allocate IRQ vector\n");
 		return r;
 	}
 
 	if (amdgpu_msi_ok(adev)) {
 		adev->irq.msi_enabled = true;
-		dev_dbg(adev->dev, "using MSI/MSI-X.\n");
+		dev_dbg(adev->dev, "using MSI/MSI-X\n");
 	}
 
+	/* IH1 / IH2 still use workqueues */
 	INIT_WORK(&adev->irq.ih1_work, amdgpu_irq_handle_ih1);
 	INIT_WORK(&adev->irq.ih2_work, amdgpu_irq_handle_ih2);
-	INIT_WORK(&adev->irq.ih_soft_work, amdgpu_irq_handle_ih_soft);
 
-	/* Use vector 0 for MSI-X. */
-	r = pci_irq_vector(adev->pdev, 0);
+	/* fast bottom-half for the software IH ring (irq_work) */
+	init_irq_work(&adev->irq.ih_soft_iw, amdgpu_irq_handle_ih_soft_iw);
+
+	/* ---------------- vector & handler ---------------- */
+	r = pci_irq_vector(adev->pdev, 0);	/* use vector 0 */
 	if (r < 0)
 		goto free_vectors;
 	irq = r;
 
-	/* PCI devices require shared interrupts. */
-	r = request_irq(irq, amdgpu_irq_handler, IRQF_SHARED, adev_to_drm(adev)->driver->name,
-			adev_to_drm(adev));
+	r = request_irq(irq, amdgpu_irq_handler, IRQF_SHARED,
+					adev_to_drm(adev)->driver->name,
+					adev_to_drm(adev));
 	if (r)
 		goto free_vectors;
 
-	adev->irq.installed = true;
-	adev->irq.irq = irq;
+	/* ---------------- locality hint ------------------ */
+	#ifdef CONFIG_GENERIC_IRQ_MIGRATION
+	{
+		int node = dev_to_node(&adev->pdev->dev);
+		const struct cpumask *mask = (node >= 0) ?
+		cpumask_of_node(node) :
+		cpu_online_mask;
+
+		irq_set_affinity_hint(irq, mask);
+	}
+	#endif
+
+	adev->irq.installed            = true;
+	adev->irq.irq                  = irq;
 	adev_to_drm(adev)->max_vblank_count = 0x00ffffff;
 
-	DRM_DEBUG("amdgpu: irq initialized.\n");
+	DRM_DEBUG("amdgpu: IRQ initialised\n");
 	return 0;
 
-free_vectors:
+	free_vectors:
 	if (adev->irq.msi_enabled)
 		pci_free_irq_vectors(adev->pdev);
-
 	adev->irq.msi_enabled = false;
 	return r;
 }
@@ -330,7 +340,13 @@ void amdgpu_irq_fini_hw(struct amdgpu_de
 {
 	if (adev->irq.installed) {
 		free_irq(adev->irq.irq, adev_to_drm(adev));
+
+		#ifdef CONFIG_GENERIC_IRQ_MIGRATION
+		irq_set_affinity_hint(adev->irq.irq, NULL);
+		#endif
+
 		adev->irq.installed = false;
+
 		if (adev->irq.msi_enabled)
 			pci_free_irq_vectors(adev->pdev);
 	}
@@ -506,11 +522,15 @@ void amdgpu_irq_dispatch(struct amdgpu_d
  * if the hardware delegation to IH1 or IH2 doesn't work for some reason.
  */
 void amdgpu_irq_delegate(struct amdgpu_device *adev,
-			 struct amdgpu_iv_entry *entry,
-			 unsigned int num_dw)
+						 struct amdgpu_iv_entry *entry,
+						 unsigned int num_dw)
 {
-	amdgpu_ih_ring_write(adev, &adev->irq.ih_soft, entry->iv_entry, num_dw);
-	schedule_work(&adev->irq.ih_soft_work);
+	/* write IV to the software ring first */
+	amdgpu_ih_ring_write(adev, &adev->irq.ih_soft,
+						 entry->iv_entry, num_dw);
+
+	/* queue fast bottom-half */
+	irq_work_queue(&adev->irq.ih_soft_iw);
 }
 
 /**


--- a/drivers/gpu/drm/amd/amdgpu/sdma_v4_0.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/sdma_v4_0.c	2025-04-19 23:06:54.825287367 +0200
@@ -643,90 +643,65 @@ static int sdma_v4_0_init_microcode(stru
 	return ret;
 }
 
-/**
- * sdma_v4_0_ring_get_rptr - get the current read pointer
- *
- * @ring: amdgpu ring pointer
- *
- * Get the current rptr from the hardware (VEGA10+).
- */
+/* ------------------------------------------------------------------ */
+/* read pointer helper                                                */
+/* ------------------------------------------------------------------ */
 static uint64_t sdma_v4_0_ring_get_rptr(struct amdgpu_ring *ring)
 {
-	u64 *rptr;
+	u64 *rptr = (u64 *)ring->rptr_cpu_addr;	/* little‑endian on Vega */
 
-	/* XXX check if swapping is necessary on BE */
-	rptr = ((u64 *)ring->rptr_cpu_addr);
+	if (drm_debug_enabled(DRM_UT_DRIVER))
+		DRM_DEBUG("SDMA%u rptr raw 0x%016llx\n", ring->me, *rptr);
 
-	DRM_DEBUG("rptr before shift == 0x%016llx\n", *rptr);
-	return ((*rptr) >> 2);
+	return *rptr >> 2;			/* convert to DWORD index */
 }
 
-/**
- * sdma_v4_0_ring_get_wptr - get the current write pointer
- *
- * @ring: amdgpu ring pointer
- *
- * Get the current wptr from the hardware (VEGA10+).
- */
+/* ------------------------------------------------------------------ */
+/* write pointer read helper                                          */
+/* ------------------------------------------------------------------ */
 static uint64_t sdma_v4_0_ring_get_wptr(struct amdgpu_ring *ring)
 {
 	struct amdgpu_device *adev = ring->adev;
 	u64 wptr;
 
 	if (ring->use_doorbell) {
-		/* XXX check if swapping is necessary on BE */
 		wptr = READ_ONCE(*((u64 *)ring->wptr_cpu_addr));
-		DRM_DEBUG("wptr/doorbell before shift == 0x%016llx\n", wptr);
+		if (drm_debug_enabled(DRM_UT_DRIVER))
+			DRM_DEBUG("SDMA%u wptr doorbell raw 0x%016llx\n",
+					  ring->me, wptr);
 	} else {
-		wptr = RREG32_SDMA(ring->me, mmSDMA0_GFX_RB_WPTR_HI);
-		wptr = wptr << 32;
-		wptr |= RREG32_SDMA(ring->me, mmSDMA0_GFX_RB_WPTR);
-		DRM_DEBUG("wptr before shift [%i] wptr == 0x%016llx\n",
-				ring->me, wptr);
+		wptr  = (u64)RREG32_SDMA(ring->me,
+								 mmSDMA0_GFX_RB_WPTR_HI) << 32;
+								 wptr |= RREG32_SDMA(ring->me, mmSDMA0_GFX_RB_WPTR);
+								 if (drm_debug_enabled(DRM_UT_DRIVER))
+									 DRM_DEBUG("SDMA%u wptr mmio raw 0x%016llx\n",
+											   ring->me, wptr);
 	}
 
-	return wptr >> 2;
+	return wptr >> 2;			/* to DWORDs */
 }
 
-/**
- * sdma_v4_0_ring_set_wptr - commit the write pointer
- *
- * @ring: amdgpu ring pointer
- *
- * Write the wptr back to the hardware (VEGA10+).
- */
+/* ------------------------------------------------------------------ */
+/* write pointer commit helper                                        */
+/* ------------------------------------------------------------------ */
 static void sdma_v4_0_ring_set_wptr(struct amdgpu_ring *ring)
 {
 	struct amdgpu_device *adev = ring->adev;
+	u64 wptr_dw = ring->wptr;		/* already DWORD aligned */
+
+	if (drm_debug_enabled(DRM_UT_DRIVER))
+		DRM_DEBUG("SDMA%u set wptr %llu (DW)\n", ring->me, wptr_dw);
 
-	DRM_DEBUG("Setting write pointer\n");
 	if (ring->use_doorbell) {
 		u64 *wb = (u64 *)ring->wptr_cpu_addr;
 
-		DRM_DEBUG("Using doorbell -- "
-				"wptr_offs == 0x%08x "
-				"lower_32_bits(ring->wptr << 2) == 0x%08x "
-				"upper_32_bits(ring->wptr << 2) == 0x%08x\n",
-				ring->wptr_offs,
-				lower_32_bits(ring->wptr << 2),
-				upper_32_bits(ring->wptr << 2));
-		/* XXX check if swapping is necessary on BE */
-		WRITE_ONCE(*wb, (ring->wptr << 2));
-		DRM_DEBUG("calling WDOORBELL64(0x%08x, 0x%016llx)\n",
-				ring->doorbell_index, ring->wptr << 2);
-		WDOORBELL64(ring->doorbell_index, ring->wptr << 2);
+		WRITE_ONCE(*wb, wptr_dw << 2);	/* bytes */
+		WDOORBELL64(ring->doorbell_index, wptr_dw << 2);
 	} else {
-		DRM_DEBUG("Not using doorbell -- "
-				"mmSDMA%i_GFX_RB_WPTR == 0x%08x "
-				"mmSDMA%i_GFX_RB_WPTR_HI == 0x%08x\n",
-				ring->me,
-				lower_32_bits(ring->wptr << 2),
-				ring->me,
-				upper_32_bits(ring->wptr << 2));
 		WREG32_SDMA(ring->me, mmSDMA0_GFX_RB_WPTR,
-			    lower_32_bits(ring->wptr << 2));
+					lower_32_bits(wptr_dw << 2));
 		WREG32_SDMA(ring->me, mmSDMA0_GFX_RB_WPTR_HI,
-			    upper_32_bits(ring->wptr << 2));
+					upper_32_bits(wptr_dw << 2));
 	}
 }
 
@@ -781,17 +756,20 @@ static void sdma_v4_0_page_ring_set_wptr
 	}
 }
 
-static void sdma_v4_0_ring_insert_nop(struct amdgpu_ring *ring, uint32_t count)
+static void sdma_v4_0_ring_insert_nop(struct amdgpu_ring *ring,
+									  uint32_t count)
 {
-	struct amdgpu_sdma_instance *sdma = amdgpu_sdma_get_instance_from_ring(ring);
-	int i;
+	struct amdgpu_sdma_instance *sdma =
+	amdgpu_sdma_get_instance_from_ring(ring);
+	uint32_t i;
 
-	for (i = 0; i < count; i++)
-		if (sdma && sdma->burst_nop && (i == 0))
+	for (i = 0; i < count; i++) {
+		if (sdma && sdma->burst_nop && i == 0)
 			amdgpu_ring_write(ring, ring->funcs->nop |
-				SDMA_PKT_NOP_HEADER_COUNT(count - 1));
+			SDMA_PKT_NOP_HEADER_COUNT(count - 1));
 		else
 			amdgpu_ring_write(ring, ring->funcs->nop);
+	}
 }
 
 /**
@@ -1659,30 +1637,26 @@ static void sdma_v4_0_vm_set_pte_pde(str
 	ib->ptr[ib->length_dw++] = count - 1; /* number of entries */
 }
 
-/**
- * sdma_v4_0_ring_pad_ib - pad the IB to the required number of dw
- *
- * @ring: amdgpu_ring structure holding ring information
- * @ib: indirect buffer to fill with padding
- */
-static void sdma_v4_0_ring_pad_ib(struct amdgpu_ring *ring, struct amdgpu_ib *ib)
+static void sdma_v4_0_ring_pad_ib(struct amdgpu_ring *ring,
+								  struct amdgpu_ib *ib)
 {
-	struct amdgpu_sdma_instance *sdma = amdgpu_sdma_get_instance_from_ring(ring);
-	u32 pad_count;
-	int i;
+	struct amdgpu_sdma_instance *sdma =
+	amdgpu_sdma_get_instance_from_ring(ring);
+	u32 pad_count, i;
+
+	pad_count = (-ib->length_dw) & 7;	/* align to 8 DW */
 
-	pad_count = (-ib->length_dw) & 7;
-	for (i = 0; i < pad_count; i++)
-		if (sdma && sdma->burst_nop && (i == 0))
+	for (i = 0; i < pad_count; i++) {
+		if (sdma && sdma->burst_nop && i == 0)
 			ib->ptr[ib->length_dw++] =
-				SDMA_PKT_HEADER_OP(SDMA_OP_NOP) |
-				SDMA_PKT_NOP_HEADER_COUNT(pad_count - 1);
+			SDMA_PKT_HEADER_OP(SDMA_OP_NOP) |
+			SDMA_PKT_NOP_HEADER_COUNT(pad_count - 1);
 		else
 			ib->ptr[ib->length_dw++] =
-				SDMA_PKT_HEADER_OP(SDMA_OP_NOP);
+			SDMA_PKT_HEADER_OP(SDMA_OP_NOP);
+	}
 }
 
-
 /**
  * sdma_v4_0_ring_emit_pipeline_sync - sync the pipeline
  *
@@ -2599,12 +2573,12 @@ static void sdma_v4_0_emit_fill_buffer(s
 }
 
 static const struct amdgpu_buffer_funcs sdma_v4_0_buffer_funcs = {
-	.copy_max_bytes = 0x400000,
-	.copy_num_dw = 7,
+	.copy_max_bytes = 0x400000,		/* 4 MiB */
+	.copy_num_dw    = 7,
 	.emit_copy_buffer = sdma_v4_0_emit_copy_buffer,
 
-	.fill_max_bytes = 0x400000,
-	.fill_num_dw = 5,
+	.fill_max_bytes = 0x400000,		/* 4 MiB */
+	.fill_num_dw    = 5,
 	.emit_fill_buffer = sdma_v4_0_emit_fill_buffer,
 };
 

 
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_sdma.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_sdma.c	2025-04-19 22:43:23.904999601 +0200
@@ -30,37 +30,54 @@
 /* SDMA CSA reside in the 3rd page of CSA */
 #define AMDGPU_CSA_SDMA_OFFSET (4096 * 2)
 
-/*
- * GPU SDMA IP block helpers function.
- */
-
-struct amdgpu_sdma_instance *amdgpu_sdma_get_instance_from_ring(struct amdgpu_ring *ring)
+/* ------------------------------------------------------------------ */
+/* Fast helpers: use ring->idx instead of a linear scan                */
+/* ------------------------------------------------------------------ */
+struct amdgpu_sdma_instance *
+amdgpu_sdma_get_instance_from_ring(struct amdgpu_ring *ring)
 {
 	struct amdgpu_device *adev = ring->adev;
-	int i;
+	u32 idx = ring->idx;
 
-	for (i = 0; i < adev->sdma.num_instances; i++)
-		if (ring == &adev->sdma.instance[i].ring ||
-		    ring == &adev->sdma.instance[i].page)
-			return &adev->sdma.instance[i];
+	/* O(1) fast path */
+	if (idx < adev->sdma.num_instances &&
+		(ring == &adev->sdma.instance[idx].ring ||
+		ring == &adev->sdma.instance[idx].page))
+		return &adev->sdma.instance[idx];
+
+	/* Fallback – keep legacy behaviour */
+	for (idx = 0; idx < adev->sdma.num_instances; idx++) {
+		if (ring == &adev->sdma.instance[idx].ring ||
+			ring == &adev->sdma.instance[idx].page)
+			return &adev->sdma.instance[idx];
+	}
 
 	return NULL;
 }
 
-int amdgpu_sdma_get_index_from_ring(struct amdgpu_ring *ring, uint32_t *index)
+int amdgpu_sdma_get_index_from_ring(struct amdgpu_ring *ring, u32 *index)
 {
 	struct amdgpu_device *adev = ring->adev;
-	int i;
+	u32 idx = ring->idx;
 
-	for (i = 0; i < adev->sdma.num_instances; i++) {
-		if (ring == &adev->sdma.instance[i].ring ||
-			ring == &adev->sdma.instance[i].page) {
-			*index = i;
+	/* Fast path */
+	if (idx < adev->sdma.num_instances &&
+		(ring == &adev->sdma.instance[idx].ring ||
+		ring == &adev->sdma.instance[idx].page)) {
+		*index = idx;
+	return 0;
+		}
+
+		/* Fallback keeps behaviour identical to the old code */
+		for (idx = 0; idx < adev->sdma.num_instances; idx++) {
+			if (ring == &adev->sdma.instance[idx].ring ||
+				ring == &adev->sdma.instance[idx].page) {
+				*index = idx;
 			return 0;
+				}
 		}
-	}
 
-	return -EINVAL;
+		return -EINVAL;
 }
 
 uint64_t amdgpu_sdma_get_csa_mc_addr(struct amdgpu_ring *ring,



--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c	2025-04-19 10:10:24.404944606 +0200
@@ -41,8 +41,10 @@
 #include <linux/dma-buf.h>
 #include <linux/sizes.h>
 #include <linux/module.h>
+#include <linux/dma-fence-array.h>
 
 #include <drm/drm_drv.h>
+#include <drm/gpu_scheduler.h>
 #include <drm/ttm/ttm_bo.h>
 #include <drm/ttm/ttm_placement.h>
 #include <drm/ttm/ttm_range_manager.h>
@@ -63,7 +65,12 @@
 
 MODULE_IMPORT_NS("DMA_BUF");
 
+#define AMDGPU_TTM_CHUNK_VEGA64	(512ULL << 20)	/* 512 MiB burst size */
 #define AMDGPU_TTM_VRAM_MAX_DW_READ	((size_t)128)
+#define STRIPE_THRESHOLD	(128ULL << 20)	/* 128 MiB */
+
+static struct drm_sched_entity sdma1_hi_pr;
+static bool sdma1_entity_init_done;
 
 static int amdgpu_ttm_backend_bind(struct ttm_device *bdev,
 				   struct ttm_tt *ttm,
@@ -79,6 +86,59 @@ static int amdgpu_ttm_init_on_chip(struc
 				  false, size_in_page);
 }
 
+static inline void gtt_window_lock_fast(struct amdgpu_device *adev)
+{
+	/* uncontended: take mutex immediately */
+	if (likely(mutex_trylock(&adev->mman.gtt_window_lock)))
+		return;
+
+	/* contended: sleep until owner releases */
+	mutex_lock(&adev->mman.gtt_window_lock);
+}
+
+/*
+ * Make sure SDMA “buffer functions” are enabled when possible.
+ * Preconditions now include:
+ *   – buffer_funcs_ring exists
+ *   – its scheduler reports at least one run‑queue
+ *   – the ring is marked ready
+ * This guarantees drm_sched_entity_init() will succeed.
+ */
+static void amdgpu_ttm_auto_enable_buffer_funcs(struct amdgpu_device *adev)
+{
+	struct amdgpu_ring *ring = adev->mman.buffer_funcs_ring;
+
+	/* Already enabled? */
+	if (adev->mman.buffer_funcs_enabled)
+		return;
+
+	/* APUs usually prefer the CPU path. */
+	if (adev->gmc.is_app_apu)
+		return;
+
+	/* Need a live, ready scheduler with run‑queues. */
+	if (!ring || !ring->sched.ready || !ring->sched.num_rqs)
+		return;
+
+	/* Discrete ASICs that clearly benefit. */
+	switch (adev->asic_type) {
+		case CHIP_VEGA10:
+		case CHIP_VEGA12:
+		case CHIP_VEGA20:
+		case CHIP_NAVI10:
+		case CHIP_NAVI12:
+		case CHIP_NAVI14:
+		case CHIP_SIENNA_CICHLID:
+		case CHIP_NAVY_FLOUNDER:
+			break;
+		default:
+			return;
+	}
+
+	DRM_DEBUG_DRIVER("amdgpu_ttm: enabling SDMA buffer functions\n");
+	amdgpu_ttm_set_buffer_funcs_status(adev, true);
+}
+
 /**
  * amdgpu_evict_flags - Compute placement flags
  *
@@ -269,6 +329,83 @@ static int amdgpu_ttm_map_buffer(struct
 	return 0;
 }
 
+/*
+ * amdgpu_copy_buffer_striped - split large copy across both SDMA rings
+ *
+ * Uses ring0 (the caller‑supplied ring, usually sdma0) for the first half
+ * and sdma1 for the second. Falls back to single‑ring when striping is
+ * impossible or not beneficial.
+ */
+static int amdgpu_copy_buffer_striped(struct amdgpu_ring *ring0,
+									  uint64_t src_offset, uint64_t dst_offset,
+									  uint32_t byte_count,
+									  struct dma_resv *resv,
+									  struct dma_fence **fence,
+									  uint32_t copy_flags)
+{
+	struct amdgpu_device *adev = ring0->adev;
+	struct amdgpu_ring   *ring1;
+	struct dma_fence *f0 = NULL, *f1 = NULL;
+	struct drm_gpu_scheduler *sched_list[1];
+	uint32_t left, right;
+	int r;
+
+	/* Fallback when striping is not applicable */
+	if (adev->asic_type != CHIP_VEGA10 ||
+		adev->sdma.num_instances < 2 ||
+		byte_count < STRIPE_THRESHOLD) {
+		return amdgpu_copy_buffer(ring0, src_offset, dst_offset,
+								  byte_count, resv, fence,
+							false, true, copy_flags);
+		}
+
+		ring1 = &adev->sdma.instance[1].ring;
+	if (!ring1->sched.ready) {
+		return amdgpu_copy_buffer(ring0, src_offset, dst_offset,
+								  byte_count, resv, fence,
+							false, true, copy_flags);
+	}
+
+	/* Initialise SDMA1 high‑priority entity once */
+	if (!sdma1_entity_init_done) {
+		sched_list[0] = &ring1->sched;
+		r = drm_sched_entity_init(&sdma1_hi_pr,
+								  DRM_SCHED_PRIORITY_KERNEL,
+							sched_list, 1, NULL);
+		if (r) {
+			DRM_WARN("SDMA1 entity init failed, using single ring\n");
+			return amdgpu_copy_buffer(ring0, src_offset, dst_offset,
+									  byte_count, resv, fence,
+							 false, true, copy_flags);
+		}
+		sdma1_entity_init_done = true;
+	}
+
+	/* Split size evenly; right part ≤ left part + 4 KiB */
+	left  = byte_count >> 1;
+	right = byte_count - left;
+
+	/* First half on ring0 (includes VM flush)                        */
+	r  = amdgpu_copy_buffer(ring0, src_offset, dst_offset,
+							left, resv, &f0,
+						 false, true, copy_flags);
+	/* Second half on ring1 (no VM flush)                             */
+	r |= amdgpu_copy_buffer(ring1, src_offset + left,
+							dst_offset + left, right, resv,
+						 &f1, false, false, copy_flags);
+	if (r)
+		goto err_put;
+
+	*fence = (struct dma_fence *)
+	dma_fence_array_create(2,
+						   (struct dma_fence *[2]){ f0, f1 },
+						   0, GFP_KERNEL, false);
+	err_put:
+	dma_fence_put(f0);
+	dma_fence_put(f1);
+	return r;
+}
+
 /**
  * amdgpu_ttm_copy_mem_to_mem - Helper function for copy
  * @adev: amdgpu device
@@ -285,18 +422,18 @@ static int amdgpu_ttm_map_buffer(struct
  *
  */
 int amdgpu_ttm_copy_mem_to_mem(struct amdgpu_device *adev,
-			       const struct amdgpu_copy_mem *src,
-			       const struct amdgpu_copy_mem *dst,
-			       uint64_t size, bool tmz,
-			       struct dma_resv *resv,
-			       struct dma_fence **f)
+							   const struct amdgpu_copy_mem *src,
+							   const struct amdgpu_copy_mem *dst,
+							   uint64_t size, bool tmz,
+							   struct dma_resv *resv,
+							   struct dma_fence **f)
 {
-	struct amdgpu_ring *ring = adev->mman.buffer_funcs_ring;
+	struct amdgpu_ring	*ring = adev->mman.buffer_funcs_ring;
 	struct amdgpu_res_cursor src_mm, dst_mm;
-	struct dma_fence *fence = NULL;
+	struct dma_fence	*fence = NULL;
+	uint32_t		copy_flags = 0;
+	struct amdgpu_bo	*abo_src, *abo_dst;
 	int r = 0;
-	uint32_t copy_flags = 0;
-	struct amdgpu_bo *abo_src, *abo_dst;
 
 	if (!adev->mman.buffer_funcs_enabled) {
 		DRM_ERROR("Trying to move memory with ring turned off.\n");
@@ -306,53 +443,58 @@ int amdgpu_ttm_copy_mem_to_mem(struct am
 	amdgpu_res_first(src->mem, src->offset, size, &src_mm);
 	amdgpu_res_first(dst->mem, dst->offset, size, &dst_mm);
 
-	mutex_lock(&adev->mman.gtt_window_lock);
+	abo_src = ttm_to_amdgpu_bo(src->bo);
+	abo_dst = ttm_to_amdgpu_bo(dst->bo);
+
+	gtt_window_lock_fast(adev);
+
 	while (src_mm.remaining) {
 		uint64_t from, to, cur_size, tiling_flags;
-		uint32_t num_type, data_format, max_com, write_compress_disable;
+		uint32_t num_type, data_format, max_com, wcd;
 		struct dma_fence *next;
 
-		/* Never copy more than 256MiB at once to avoid a timeout */
-		cur_size = min3(src_mm.size, dst_mm.size, 256ULL << 20);
+		/* Vega64: burst 512 MiB, others stay at 256 MiB */
+		cur_size = min3(src_mm.size, dst_mm.size,
+						(adev->asic_type == CHIP_VEGA10) ?
+						AMDGPU_TTM_CHUNK_VEGA64 : (256ULL << 20));
 
-		/* Map src to window 0 and dst to window 1. */
-		r = amdgpu_ttm_map_buffer(src->bo, src->mem, &src_mm,
-					  0, ring, tmz, &cur_size, &from);
+		r = amdgpu_ttm_map_buffer(src->bo, src->mem, &src_mm, 0, ring,
+								  tmz, &cur_size, &from);
 		if (r)
-			goto error;
+			goto out;
 
-		r = amdgpu_ttm_map_buffer(dst->bo, dst->mem, &dst_mm,
-					  1, ring, tmz, &cur_size, &to);
+		r = amdgpu_ttm_map_buffer(dst->bo, dst->mem, &dst_mm, 1, ring,
+								  tmz, &cur_size, &to);
 		if (r)
-			goto error;
+			goto out;
 
-		abo_src = ttm_to_amdgpu_bo(src->bo);
-		abo_dst = ttm_to_amdgpu_bo(dst->bo);
 		if (tmz)
 			copy_flags |= AMDGPU_COPY_FLAGS_TMZ;
 		if ((abo_src->flags & AMDGPU_GEM_CREATE_GFX12_DCC) &&
-		    (abo_src->tbo.resource->mem_type == TTM_PL_VRAM))
+			(src->mem->mem_type == TTM_PL_VRAM))
 			copy_flags |= AMDGPU_COPY_FLAGS_READ_DECOMPRESSED;
 		if ((abo_dst->flags & AMDGPU_GEM_CREATE_GFX12_DCC) &&
-		    (dst->mem->mem_type == TTM_PL_VRAM)) {
+			(dst->mem->mem_type == TTM_PL_VRAM)) {
 			copy_flags |= AMDGPU_COPY_FLAGS_WRITE_COMPRESSED;
-			amdgpu_bo_get_tiling_flags(abo_dst, &tiling_flags);
-			max_com = AMDGPU_TILING_GET(tiling_flags, GFX12_DCC_MAX_COMPRESSED_BLOCK);
-			num_type = AMDGPU_TILING_GET(tiling_flags, GFX12_DCC_NUMBER_TYPE);
-			data_format = AMDGPU_TILING_GET(tiling_flags, GFX12_DCC_DATA_FORMAT);
-			write_compress_disable =
-				AMDGPU_TILING_GET(tiling_flags, GFX12_DCC_WRITE_COMPRESS_DISABLE);
-			copy_flags |= (AMDGPU_COPY_FLAGS_SET(MAX_COMPRESSED, max_com) |
-				       AMDGPU_COPY_FLAGS_SET(NUMBER_TYPE, num_type) |
-				       AMDGPU_COPY_FLAGS_SET(DATA_FORMAT, data_format) |
-				       AMDGPU_COPY_FLAGS_SET(WRITE_COMPRESS_DISABLE,
-							     write_compress_disable));
-		}
+		amdgpu_bo_get_tiling_flags(abo_dst, &tiling_flags);
+		max_com  = AMDGPU_TILING_GET(tiling_flags,
+									 GFX12_DCC_MAX_COMPRESSED_BLOCK);
+		num_type = AMDGPU_TILING_GET(tiling_flags,
+									 GFX12_DCC_NUMBER_TYPE);
+		data_format = AMDGPU_TILING_GET(tiling_flags,
+										GFX12_DCC_DATA_FORMAT);
+		wcd = AMDGPU_TILING_GET(tiling_flags,
+								GFX12_DCC_WRITE_COMPRESS_DISABLE);
+		copy_flags |= AMDGPU_COPY_FLAGS_SET(MAX_COMPRESSED, max_com) |
+		AMDGPU_COPY_FLAGS_SET(NUMBER_TYPE, num_type) |
+		AMDGPU_COPY_FLAGS_SET(DATA_FORMAT, data_format) |
+		AMDGPU_COPY_FLAGS_SET(WRITE_COMPRESS_DISABLE, wcd);
+			}
 
-		r = amdgpu_copy_buffer(ring, from, to, cur_size, resv,
-				       &next, false, true, copy_flags);
-		if (r)
-			goto error;
+			r = amdgpu_copy_buffer_striped(ring, from, to, cur_size,
+										   resv, &next, copy_flags);
+			if (r)
+				goto out;
 
 		dma_fence_put(fence);
 		fence = next;
@@ -360,7 +502,8 @@ int amdgpu_ttm_copy_mem_to_mem(struct am
 		amdgpu_res_next(&src_mm, cur_size);
 		amdgpu_res_next(&dst_mm, cur_size);
 	}
-error:
+
+	out:
 	mutex_unlock(&adev->mman.gtt_window_lock);
 	if (f)
 		*f = dma_fence_get(fence);
@@ -482,115 +625,120 @@ static bool amdgpu_res_copyable(struct a
 }
 
 /*
- * amdgpu_bo_move - Move a buffer object to a new memory location
+ * amdgpu_bo_move - move a BO to @new_mem
  *
- * Called by ttm_bo_handle_move_mem()
+ * This version guarantees we prefer the SDMA “buffer functions” path
+ * (auto‑enabled by the helper) while keeping the original CPU‑memcpy
+ * fallback for safety.
  */
 static int amdgpu_bo_move(struct ttm_buffer_object *bo, bool evict,
-			  struct ttm_operation_ctx *ctx,
-			  struct ttm_resource *new_mem,
-			  struct ttm_place *hop)
-{
-	struct amdgpu_device *adev;
-	struct amdgpu_bo *abo;
-	struct ttm_resource *old_mem = bo->resource;
+						  struct ttm_operation_ctx *ctx,
+						  struct ttm_resource *new_mem,
+						  struct ttm_place *hop)
+{
+	struct amdgpu_device	*adev   = amdgpu_ttm_adev(bo->bdev);
+	struct amdgpu_bo	*abo    = ttm_to_amdgpu_bo(bo);
+	struct ttm_resource	*oldmem = bo->resource;
 	int r;
 
+	/* Try to (re‑)enable SDMA copy helpers when the ring is ready */
+	amdgpu_ttm_auto_enable_buffer_funcs(adev);
+
+	/* Bind TT/PREEMPT pages when entering those domains */
 	if (new_mem->mem_type == TTM_PL_TT ||
-	    new_mem->mem_type == AMDGPU_PL_PREEMPT) {
+		new_mem->mem_type == AMDGPU_PL_PREEMPT) {
 		r = amdgpu_ttm_backend_bind(bo->bdev, bo->ttm, new_mem);
-		if (r)
-			return r;
-	}
+	if (r)
+		return r;
+		}
 
-	abo = ttm_to_amdgpu_bo(bo);
-	adev = amdgpu_ttm_adev(bo->bdev);
+		/* Simple NULL moves */
+		if (!oldmem || (oldmem->mem_type == TTM_PL_SYSTEM && !bo->ttm)) {
+			amdgpu_bo_move_notify(bo, evict, new_mem);
+			ttm_bo_move_null(bo, new_mem);
+			return 0;
+		}
 
-	if (!old_mem || (old_mem->mem_type == TTM_PL_SYSTEM &&
-			 bo->ttm == NULL)) {
-		amdgpu_bo_move_notify(bo, evict, new_mem);
-		ttm_bo_move_null(bo, new_mem);
-		return 0;
-	}
-	if (old_mem->mem_type == TTM_PL_SYSTEM &&
-	    (new_mem->mem_type == TTM_PL_TT ||
-	     new_mem->mem_type == AMDGPU_PL_PREEMPT)) {
-		amdgpu_bo_move_notify(bo, evict, new_mem);
+		if (oldmem->mem_type == TTM_PL_SYSTEM &&
+			(new_mem->mem_type == TTM_PL_TT ||
+			new_mem->mem_type == AMDGPU_PL_PREEMPT)) {
+			amdgpu_bo_move_notify(bo, evict, new_mem);
 		ttm_bo_move_null(bo, new_mem);
 		return 0;
-	}
-	if ((old_mem->mem_type == TTM_PL_TT ||
-	     old_mem->mem_type == AMDGPU_PL_PREEMPT) &&
-	    new_mem->mem_type == TTM_PL_SYSTEM) {
-		r = ttm_bo_wait_ctx(bo, ctx);
-		if (r)
-			return r;
-
-		amdgpu_ttm_backend_unbind(bo->bdev, bo->ttm);
-		amdgpu_bo_move_notify(bo, evict, new_mem);
-		ttm_resource_free(bo, &bo->resource);
-		ttm_bo_assign_mem(bo, new_mem);
-		return 0;
-	}
+			}
 
-	if (old_mem->mem_type == AMDGPU_PL_GDS ||
-	    old_mem->mem_type == AMDGPU_PL_GWS ||
-	    old_mem->mem_type == AMDGPU_PL_OA ||
-	    old_mem->mem_type == AMDGPU_PL_DOORBELL ||
-	    new_mem->mem_type == AMDGPU_PL_GDS ||
-	    new_mem->mem_type == AMDGPU_PL_GWS ||
-	    new_mem->mem_type == AMDGPU_PL_OA ||
-	    new_mem->mem_type == AMDGPU_PL_DOORBELL) {
-		/* Nothing to save here */
-		amdgpu_bo_move_notify(bo, evict, new_mem);
-		ttm_bo_move_null(bo, new_mem);
-		return 0;
-	}
+			/* TT/PREEMPT -> SYSTEM */
+			if ((oldmem->mem_type == TTM_PL_TT ||
+				oldmem->mem_type == AMDGPU_PL_PREEMPT) &&
+				new_mem->mem_type == TTM_PL_SYSTEM) {
+				r = ttm_bo_wait_ctx(bo, ctx);
+			if (r) {
+				return r;
+			}
 
-	if (bo->type == ttm_bo_type_device &&
-	    new_mem->mem_type == TTM_PL_VRAM &&
-	    old_mem->mem_type != TTM_PL_VRAM) {
-		/* amdgpu_bo_fault_reserve_notify will re-set this if the CPU
-		 * accesses the BO after it's moved.
-		 */
-		abo->flags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
-	}
+			amdgpu_ttm_backend_unbind(bo->bdev, bo->ttm);
+			amdgpu_bo_move_notify(bo, evict, new_mem);
+			ttm_resource_free(bo, &bo->resource);
+			ttm_bo_assign_mem(bo, new_mem);
+			return 0;
+				}
+
+				/* Special heaps treated like NULL moves */
+				if (oldmem->mem_type == AMDGPU_PL_GDS     ||
+					oldmem->mem_type == AMDGPU_PL_GWS     ||
+					oldmem->mem_type == AMDGPU_PL_OA      ||
+					oldmem->mem_type == AMDGPU_PL_DOORBELL ||
+					new_mem->mem_type == AMDGPU_PL_GDS     ||
+					new_mem->mem_type == AMDGPU_PL_GWS     ||
+					new_mem->mem_type == AMDGPU_PL_OA      ||
+					new_mem->mem_type == AMDGPU_PL_DOORBELL) {
+					amdgpu_bo_move_notify(bo, evict, new_mem);
+				ttm_bo_move_null(bo, new_mem);
+				return 0;
+					}
+
+					/* Clear CPU‑access flag when moving into VRAM */
+					if (bo->type == ttm_bo_type_device &&
+						new_mem->mem_type == TTM_PL_VRAM &&
+						oldmem->mem_type != TTM_PL_VRAM)
+						abo->flags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
+
+					/* Use EMULTIHOP helper for SYSTEM<->VRAM when SDMA is on */
+					if (adev->mman.buffer_funcs_enabled &&
+						((oldmem->mem_type == TTM_PL_SYSTEM &&
+						new_mem->mem_type == TTM_PL_VRAM) ||
+						(oldmem->mem_type == TTM_PL_VRAM &&
+						new_mem->mem_type == TTM_PL_SYSTEM))) {
+						hop->fpfn     = 0;
+					hop->lpfn     = 0;
+					hop->mem_type = TTM_PL_TT;
+					hop->flags    = TTM_PL_FLAG_TEMPORARY;
+					return -EMULTIHOP;
+						}
 
-	if (adev->mman.buffer_funcs_enabled &&
-	    ((old_mem->mem_type == TTM_PL_SYSTEM &&
-	      new_mem->mem_type == TTM_PL_VRAM) ||
-	     (old_mem->mem_type == TTM_PL_VRAM &&
-	      new_mem->mem_type == TTM_PL_SYSTEM))) {
-		hop->fpfn = 0;
-		hop->lpfn = 0;
-		hop->mem_type = TTM_PL_TT;
-		hop->flags = TTM_PL_FLAG_TEMPORARY;
-		return -EMULTIHOP;
-	}
+						/* SDMA blit (preferred) or CPU memcpy (fallback) */
+						amdgpu_bo_move_notify(bo, evict, new_mem);
 
-	amdgpu_bo_move_notify(bo, evict, new_mem);
-	if (adev->mman.buffer_funcs_enabled)
-		r = amdgpu_move_blit(bo, evict, new_mem, old_mem);
+						if (adev->mman.buffer_funcs_enabled)
+							r = amdgpu_move_blit(bo, evict, new_mem, oldmem);
 	else
 		r = -ENODEV;
 
 	if (r) {
-		/* Check that all memory is CPU accessible */
-		if (!amdgpu_res_copyable(adev, old_mem) ||
-		    !amdgpu_res_copyable(adev, new_mem)) {
-			pr_err("Move buffer fallback to memcpy unavailable\n");
+		if (!amdgpu_res_copyable(adev, oldmem) ||
+			!amdgpu_res_copyable(adev, new_mem))
 			return r;
-		}
 
 		r = ttm_bo_move_memcpy(bo, ctx, new_mem);
 		if (r)
 			return r;
 	}
 
-	/* update statistics after the move */
+	/* Update statistics */
 	if (evict)
 		atomic64_inc(&adev->num_evictions);
 	atomic64_add(bo->base.size, &adev->num_bytes_moved);
+
 	return 0;
 }
 
@@ -1027,12 +1175,6 @@ int amdgpu_ttm_alloc_gart(struct ttm_buf
 	return 0;
 }
 
-/*
- * amdgpu_ttm_recover_gart - Rebind GTT pages
- *
- * Called by amdgpu_gtt_mgr_recover() from amdgpu_device_reset() to
- * rebind GTT pages during a GPU reset.
- */
 void amdgpu_ttm_recover_gart(struct ttm_buffer_object *tbo)
 {
 	struct amdgpu_device *adev = amdgpu_ttm_adev(tbo->bdev);
@@ -1043,6 +1185,9 @@ void amdgpu_ttm_recover_gart(struct ttm_
 
 	flags = amdgpu_ttm_tt_pte_flags(adev, tbo->ttm, tbo->resource);
 	amdgpu_ttm_gart_bind(adev, tbo, flags);
+
+	/* Re‑arm SDMA buffer functions after GPU reset. */
+	amdgpu_ttm_auto_enable_buffer_funcs(adev);
 }
 
 /*
@@ -1855,14 +2000,13 @@ int amdgpu_ttm_init(struct amdgpu_device
 	int r;
 
 	mutex_init(&adev->mman.gtt_window_lock);
-
 	dma_set_max_seg_size(adev->dev, UINT_MAX);
-	/* No others user of address space so set it to 0 */
+
 	r = ttm_device_init(&adev->mman.bdev, &amdgpu_bo_driver, adev->dev,
-			       adev_to_drm(adev)->anon_inode->i_mapping,
-			       adev_to_drm(adev)->vma_offset_manager,
-			       adev->need_swiotlb,
-			       dma_addressing_limited(adev->dev));
+						adev_to_drm(adev)->anon_inode->i_mapping,
+						adev_to_drm(adev)->vma_offset_manager,
+						adev->need_swiotlb,
+					 dma_addressing_limited(adev->dev));
 	if (r) {
 		DRM_ERROR("failed initializing buffer object driver(%d).\n", r);
 		return r;
@@ -1875,154 +2019,115 @@ int amdgpu_ttm_init(struct amdgpu_device
 	}
 	adev->mman.initialized = true;
 
-	/* Initialize VRAM pool with all of VRAM divided into pages */
+	/* === VRAM manager ================================================= */
 	r = amdgpu_vram_mgr_init(adev);
-	if (r) {
-		DRM_ERROR("Failed initializing VRAM heap.\n");
+	if (r)
 		return r;
-	}
 
-	/* Change the size here instead of the init above so only lpfn is affected */
 	amdgpu_ttm_set_buffer_funcs_status(adev, false);
-#ifdef CONFIG_64BIT
-#ifdef CONFIG_X86
-	if (adev->gmc.xgmi.connected_to_cpu)
-		adev->mman.aper_base_kaddr = ioremap_cache(adev->gmc.aper_base,
-				adev->gmc.visible_vram_size);
-
-	else if (adev->gmc.is_app_apu)
-		DRM_DEBUG_DRIVER(
-			"No need to ioremap when real vram size is 0\n");
-	else
-#endif
-		adev->mman.aper_base_kaddr = ioremap_wc(adev->gmc.aper_base,
-				adev->gmc.visible_vram_size);
-#endif
 
-	/*
-	 *The reserved vram for firmware must be pinned to the specified
-	 *place on the VRAM, so reserve it early.
-	 */
+	#ifdef CONFIG_64BIT
+	#ifdef CONFIG_X86
+	if (adev->gmc.xgmi.connected_to_cpu) {
+		adev->mman.aper_base_kaddr =
+		ioremap_cache(adev->gmc.aper_base,
+					  adev->gmc.visible_vram_size);
+	} else if (!adev->gmc.is_app_apu) {
+		adev->mman.aper_base_kaddr =
+		ioremap_wc(adev->gmc.aper_base,
+				   adev->gmc.visible_vram_size);
+	}
+	#endif
+	#endif /* CONFIG_64BIT */
+
+	/* === Firmware / driver VRAM reservations ========================== */
 	r = amdgpu_ttm_fw_reserve_vram_init(adev);
 	if (r)
 		return r;
 
-	/*
-	 *The reserved vram for driver must be pinned to the specified
-	 *place on the VRAM, so reserve it early.
-	 */
 	r = amdgpu_ttm_drv_reserve_vram_init(adev);
 	if (r)
 		return r;
 
-	/*
-	 * only NAVI10 and onwards ASIC support for IP discovery.
-	 * If IP discovery enabled, a block of memory should be
-	 * reserved for IP discovey.
-	 */
 	if (adev->mman.discovery_bin) {
 		r = amdgpu_ttm_reserve_tmr(adev);
 		if (r)
 			return r;
 	}
 
-	/* allocate memory as required for VGA
-	 * This is used for VGA emulation and pre-OS scanout buffers to
-	 * avoid display artifacts while transitioning between pre-OS
-	 * and driver.
-	 */
+	/* === Stolen VRAM reservations (for BIOS, etc.) ==================== */
 	if (!adev->gmc.is_app_apu) {
 		r = amdgpu_bo_create_kernel_at(adev, 0,
-					       adev->mman.stolen_vga_size,
-					       &adev->mman.stolen_vga_memory,
-					       NULL);
+									   adev->mman.stolen_vga_size,
+								 &adev->mman.stolen_vga_memory, NULL);
 		if (r)
 			return r;
 
 		r = amdgpu_bo_create_kernel_at(adev, adev->mman.stolen_vga_size,
-					       adev->mman.stolen_extended_size,
-					       &adev->mman.stolen_extended_memory,
-					       NULL);
-
+									   adev->mman.stolen_extended_size,
+								 &adev->mman.stolen_extended_memory, NULL);
 		if (r)
 			return r;
 
 		r = amdgpu_bo_create_kernel_at(adev,
-					       adev->mman.stolen_reserved_offset,
-					       adev->mman.stolen_reserved_size,
-					       &adev->mman.stolen_reserved_memory,
-					       NULL);
+									   adev->mman.stolen_reserved_offset,
+								 adev->mman.stolen_reserved_size,
+								 &adev->mman.stolen_reserved_memory, NULL);
 		if (r)
 			return r;
-	} else {
-		DRM_DEBUG_DRIVER("Skipped stolen memory reservation\n");
 	}
 
 	DRM_INFO("amdgpu: %uM of VRAM memory ready\n",
-		 (unsigned int)(adev->gmc.real_vram_size / (1024 * 1024)));
+			 (unsigned int)(adev->gmc.real_vram_size >> 20));
 
-	/* Compute GTT size, either based on TTM limit
-	 * or whatever the user passed on module init.
-	 */
+	/* === GTT pool ====================================================== */
 	if (amdgpu_gtt_size == -1)
 		gtt_size = ttm_tt_pages_limit() << PAGE_SHIFT;
 	else
 		gtt_size = (uint64_t)amdgpu_gtt_size << 20;
 
-	/* Initialize GTT memory pool */
 	r = amdgpu_gtt_mgr_init(adev, gtt_size);
-	if (r) {
-		DRM_ERROR("Failed initializing GTT heap.\n");
+	if (r)
 		return r;
-	}
+
 	DRM_INFO("amdgpu: %uM of GTT memory ready.\n",
-		 (unsigned int)(gtt_size / (1024 * 1024)));
+			 (unsigned int)(gtt_size >> 20));
 
-	/* Initialize doorbell pool on PCI BAR */
-	r = amdgpu_ttm_init_on_chip(adev, AMDGPU_PL_DOORBELL, adev->doorbell.size / PAGE_SIZE);
-	if (r) {
-		DRM_ERROR("Failed initializing doorbell heap.\n");
+	/* === Doorbell, PREEMPT and on‑chip heaps ========================== */
+	r = amdgpu_ttm_init_on_chip(adev, AMDGPU_PL_DOORBELL,
+								adev->doorbell.size >> PAGE_SHIFT);
+	if (r)
 		return r;
-	}
 
-	/* Create a boorbell page for kernel usages */
 	r = amdgpu_doorbell_create_kernel_doorbells(adev);
-	if (r) {
-		DRM_ERROR("Failed to initialize kernel doorbells.\n");
+	if (r)
 		return r;
-	}
 
-	/* Initialize preemptible memory pool */
 	r = amdgpu_preempt_mgr_init(adev);
-	if (r) {
-		DRM_ERROR("Failed initializing PREEMPT heap.\n");
+	if (r)
 		return r;
-	}
 
-	/* Initialize various on-chip memory pools */
 	r = amdgpu_ttm_init_on_chip(adev, AMDGPU_PL_GDS, adev->gds.gds_size);
-	if (r) {
-		DRM_ERROR("Failed initializing GDS heap.\n");
+	if (r)
 		return r;
-	}
 
 	r = amdgpu_ttm_init_on_chip(adev, AMDGPU_PL_GWS, adev->gds.gws_size);
-	if (r) {
-		DRM_ERROR("Failed initializing gws heap.\n");
+	if (r)
 		return r;
-	}
 
-	r = amdgpu_ttm_init_on_chip(adev, AMDGPU_PL_OA, adev->gds.oa_size);
-	if (r) {
-		DRM_ERROR("Failed initializing oa heap.\n");
+	r = amdgpu_ttm_init_on_chip(adev, AMDGPU_PL_OA,  adev->gds.oa_size);
+	if (r)
 		return r;
-	}
+
 	if (amdgpu_bo_create_kernel(adev, PAGE_SIZE, PAGE_SIZE,
-				AMDGPU_GEM_DOMAIN_GTT,
-				&adev->mman.sdma_access_bo, NULL,
-				&adev->mman.sdma_access_ptr))
+		AMDGPU_GEM_DOMAIN_GTT,
+		&adev->mman.sdma_access_bo, NULL,
+		&adev->mman.sdma_access_ptr))
 		DRM_WARN("Debug VRAM access will use slowpath MM access\n");
 
+	/* Scheduler and run‑queues are ready now. */
+	amdgpu_ttm_auto_enable_buffer_funcs(adev);
+
 	return 0;
 }
 
@@ -2037,30 +2142,25 @@ void amdgpu_ttm_fini(struct amdgpu_devic
 		return;
 
 	amdgpu_ttm_pools_fini(adev);
-
 	amdgpu_ttm_training_reserve_vram_fini(adev);
-	/* return the stolen vga memory back to VRAM */
+
 	if (!adev->gmc.is_app_apu) {
 		amdgpu_bo_free_kernel(&adev->mman.stolen_vga_memory, NULL, NULL);
 		amdgpu_bo_free_kernel(&adev->mman.stolen_extended_memory, NULL, NULL);
-		/* return the FW reserved memory back to VRAM */
-		amdgpu_bo_free_kernel(&adev->mman.fw_reserved_memory, NULL,
-				      NULL);
+		amdgpu_bo_free_kernel(&adev->mman.fw_reserved_memory, NULL, NULL);
 		if (adev->mman.stolen_reserved_size)
 			amdgpu_bo_free_kernel(&adev->mman.stolen_reserved_memory,
-					      NULL, NULL);
+								  NULL, NULL);
 	}
 	amdgpu_bo_free_kernel(&adev->mman.sdma_access_bo, NULL,
-					&adev->mman.sdma_access_ptr);
+						  &adev->mman.sdma_access_ptr);
 	amdgpu_ttm_fw_reserve_vram_fini(adev);
 	amdgpu_ttm_drv_reserve_vram_fini(adev);
 
 	if (drm_dev_enter(adev_to_drm(adev), &idx)) {
-
 		if (adev->mman.aper_base_kaddr)
 			iounmap(adev->mman.aper_base_kaddr);
 		adev->mman.aper_base_kaddr = NULL;
-
 		drm_dev_exit(idx);
 	}
 
@@ -2071,8 +2171,14 @@ void amdgpu_ttm_fini(struct amdgpu_devic
 	ttm_range_man_fini(&adev->mman.bdev, AMDGPU_PL_GWS);
 	ttm_range_man_fini(&adev->mman.bdev, AMDGPU_PL_OA);
 	ttm_range_man_fini(&adev->mman.bdev, AMDGPU_PL_DOORBELL);
+
 	ttm_device_fini(&adev->mman.bdev);
 	adev->mman.initialized = false;
+
+	/* ----- new: destroy SDMA1 high‑priority entity if we created it ---- */
+	if (sdma1_entity_init_done)
+		drm_sched_entity_destroy(&sdma1_hi_pr);
+
 	DRM_INFO("amdgpu: ttm finalized\n");
 }
 
@@ -2273,27 +2379,24 @@ static int amdgpu_ttm_fill_mem(struct am
  * Returns:
  * 0 for success or a negative error code on failure.
  */
-int amdgpu_ttm_clear_buffer(struct amdgpu_bo *bo,
-			    struct dma_resv *resv,
-			    struct dma_fence **fence)
+int amdgpu_ttm_clear_buffer(struct amdgpu_bo *bo, struct dma_resv *resv,
+							struct dma_fence **fence)
 {
 	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);
-	struct amdgpu_ring *ring = adev->mman.buffer_funcs_ring;
+	struct amdgpu_ring *ring   = adev->mman.buffer_funcs_ring;
 	struct amdgpu_res_cursor cursor;
 	u64 addr;
 	int r = 0;
 
-	if (!adev->mman.buffer_funcs_enabled)
-		return -EINVAL;
-
-	if (!fence)
+	if (!adev->mman.buffer_funcs_enabled || !fence)
 		return -EINVAL;
 
 	*fence = dma_fence_get_stub();
 
 	amdgpu_res_first(bo->tbo.resource, 0, amdgpu_bo_size(bo), &cursor);
 
-	mutex_lock(&adev->mman.gtt_window_lock);
+	gtt_window_lock_fast(adev);
+
 	while (cursor.remaining) {
 		struct dma_fence *next = NULL;
 		u64 size;
@@ -2303,39 +2406,38 @@ int amdgpu_ttm_clear_buffer(struct amdgp
 			continue;
 		}
 
-		/* Never clear more than 256MiB at once to avoid timeouts */
-		size = min(cursor.size, 256ULL << 20);
-
-		r = amdgpu_ttm_map_buffer(&bo->tbo, bo->tbo.resource, &cursor,
-					  1, ring, false, &size, &addr);
+		size = min(cursor.size,
+				   (adev->asic_type == CHIP_VEGA10) ?
+				   AMDGPU_TTM_CHUNK_VEGA64 : (256ULL << 20));
+
+		r = amdgpu_ttm_map_buffer(&bo->tbo, bo->tbo.resource,
+								  &cursor, 1, ring, false,
+							&size, &addr);
 		if (r)
-			goto err;
+			goto out;
 
 		r = amdgpu_ttm_fill_mem(ring, 0, addr, size, resv,
-					&next, true, true);
+								&next, true, true);
 		if (r)
-			goto err;
+			goto out;
 
 		dma_fence_put(*fence);
 		*fence = next;
 
 		amdgpu_res_next(&cursor, size);
 	}
-err:
+	out:
 	mutex_unlock(&adev->mman.gtt_window_lock);
-
 	return r;
 }
 
-int amdgpu_fill_buffer(struct amdgpu_bo *bo,
-			uint32_t src_data,
-			struct dma_resv *resv,
-			struct dma_fence **f,
-			bool delayed)
+int amdgpu_fill_buffer(struct amdgpu_bo *bo, uint32_t src_data,
+					   struct dma_resv *resv, struct dma_fence **f,
+					   bool delayed)
 {
 	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);
-	struct amdgpu_ring *ring = adev->mman.buffer_funcs_ring;
-	struct dma_fence *fence = NULL;
+	struct amdgpu_ring   *ring = adev->mman.buffer_funcs_ring;
+	struct dma_fence     *fence = NULL;
 	struct amdgpu_res_cursor dst;
 	int r;
 
@@ -2346,30 +2448,33 @@ int amdgpu_fill_buffer(struct amdgpu_bo
 
 	amdgpu_res_first(bo->tbo.resource, 0, amdgpu_bo_size(bo), &dst);
 
-	mutex_lock(&adev->mman.gtt_window_lock);
+	gtt_window_lock_fast(adev);
+
 	while (dst.remaining) {
 		struct dma_fence *next;
-		uint64_t cur_size, to;
+		u64 cur_size, to;
 
-		/* Never fill more than 256MiB at once to avoid timeouts */
-		cur_size = min(dst.size, 256ULL << 20);
-
-		r = amdgpu_ttm_map_buffer(&bo->tbo, bo->tbo.resource, &dst,
-					  1, ring, false, &cur_size, &to);
+		cur_size = min(dst.size,
+					   (adev->asic_type == CHIP_VEGA10) ?
+					   AMDGPU_TTM_CHUNK_VEGA64 : (256ULL << 20));
+
+		r = amdgpu_ttm_map_buffer(&bo->tbo, bo->tbo.resource,
+								  &dst, 1, ring, false,
+							&cur_size, &to);
 		if (r)
-			goto error;
+			goto out;
 
-		r = amdgpu_ttm_fill_mem(ring, src_data, to, cur_size, resv,
-					&next, true, delayed);
+		r = amdgpu_ttm_fill_mem(ring, src_data, to, cur_size,
+								resv, &next, true, delayed);
 		if (r)
-			goto error;
+			goto out;
 
 		dma_fence_put(fence);
 		fence = next;
 
 		amdgpu_res_next(&dst, cur_size);
 	}
-error:
+	out:
 	mutex_unlock(&adev->mman.gtt_window_lock);
 	if (f)
 		*f = dma_fence_get(fence);



--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gfx.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gfx.c	2025-04-18 16:58:52.885186023 +0200
@@ -139,25 +139,24 @@ void amdgpu_gfx_parse_disable_cu(unsigne
 	}
 }
 
-static bool amdgpu_gfx_is_graphics_multipipe_capable(struct amdgpu_device *adev)
+/* Hot predicates – replace the originals */
+static __always_inline bool
+amdgpu_gfx_is_graphics_multipipe_capable(struct amdgpu_device *adev)
 {
-	return amdgpu_async_gfx_ring && adev->gfx.me.num_pipe_per_me > 1;
+	return amdgpu_async_gfx_ring &&
+	adev->gfx.me.num_pipe_per_me > 1;
 }
 
-static bool amdgpu_gfx_is_compute_multipipe_capable(struct amdgpu_device *adev)
+static __always_inline bool
+amdgpu_gfx_is_compute_multipipe_capable(struct amdgpu_device *adev)
 {
-	if (amdgpu_compute_multipipe != -1) {
-		DRM_INFO("amdgpu: forcing compute pipe policy %d\n",
-			 amdgpu_compute_multipipe);
+	if (amdgpu_compute_multipipe != -1)
 		return amdgpu_compute_multipipe == 1;
-	}
 
 	if (amdgpu_ip_version(adev, GC_HWIP, 0) > IP_VERSION(9, 0, 0))
 		return true;
 
-	/* FIXME: spreading the queues across pipes causes perf regressions
-	 * on POLARIS11 compute workloads */
-	if (adev->asic_type == CHIP_POLARIS11)
+	if (unlikely(adev->asic_type == CHIP_POLARIS11))
 		return false;
 
 	return adev->gfx.mec.num_mec > 1;
@@ -1163,8 +1162,10 @@ int amdgpu_gfx_get_num_kcq(struct amdgpu
 {
 	if (amdgpu_num_kcq == -1) {
 		return 8;
-	} else if (amdgpu_num_kcq > 8 || amdgpu_num_kcq < 0) {
-		dev_warn(adev->dev, "set kernel compute queue number to 8 due to invalid parameter provided by user\n");
+	} if (amdgpu_num_kcq == -1 || amdgpu_num_kcq <= 0 || amdgpu_num_kcq > 8) {
+		dev_warn(adev->dev,
+				 "Invalid amdgpu_num_kcq=%d, clamping to 8\n",
+		   amdgpu_num_kcq);
 		return 8;
 	}
 	return amdgpu_num_kcq;


--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h	2025-04-12 17:27:40.094502930 +0200
@@ -35,7 +35,7 @@
 #include "amdgpu_sync.h"
 #include "amdgpu_ring.h"
 #include "amdgpu_ids.h"
-#include "amdgpu_ttm.h"
+#include "amdgpu_ttm.h" // Provides __AMDGPU_PL_NUM
 
 struct drm_exec;
 
@@ -88,45 +88,45 @@ struct amdgpu_bo_vm;
 
 /* Flag combination to set no-retry with TF disabled */
 #define AMDGPU_VM_NORETRY_FLAGS	(AMDGPU_PTE_EXECUTABLE | AMDGPU_PDE_PTE | \
-				AMDGPU_PTE_TF)
+AMDGPU_PTE_TF)
 
 /* Flag combination to set no-retry with TF enabled */
 #define AMDGPU_VM_NORETRY_FLAGS_TF (AMDGPU_PTE_VALID | AMDGPU_PTE_SYSTEM | \
-				   AMDGPU_PTE_PRT)
+AMDGPU_PTE_PRT)
 /* For GFX9 */
 #define AMDGPU_PTE_MTYPE_VG10_SHIFT(mtype)	((uint64_t)(mtype) << 57)
 #define AMDGPU_PTE_MTYPE_VG10_MASK	AMDGPU_PTE_MTYPE_VG10_SHIFT(3ULL)
 #define AMDGPU_PTE_MTYPE_VG10(flags, mtype)			\
-	(((uint64_t)(flags) & (~AMDGPU_PTE_MTYPE_VG10_MASK)) |	\
-	  AMDGPU_PTE_MTYPE_VG10_SHIFT(mtype))
+(((uint64_t)(flags) & (~AMDGPU_PTE_MTYPE_VG10_MASK)) |	\
+AMDGPU_PTE_MTYPE_VG10_SHIFT(mtype))
 
 #define AMDGPU_MTYPE_NC 0
 #define AMDGPU_MTYPE_CC 2
 
 #define AMDGPU_PTE_DEFAULT_ATC  (AMDGPU_PTE_SYSTEM      \
-                                | AMDGPU_PTE_SNOOPED    \
-                                | AMDGPU_PTE_EXECUTABLE \
-                                | AMDGPU_PTE_READABLE   \
-                                | AMDGPU_PTE_WRITEABLE  \
-                                | AMDGPU_PTE_MTYPE_VG10(AMDGPU_MTYPE_CC))
+| AMDGPU_PTE_SNOOPED    \
+| AMDGPU_PTE_EXECUTABLE \
+| AMDGPU_PTE_READABLE   \
+| AMDGPU_PTE_WRITEABLE  \
+| AMDGPU_PTE_MTYPE_VG10(AMDGPU_MTYPE_CC))
 
 /* gfx10 */
 #define AMDGPU_PTE_MTYPE_NV10_SHIFT(mtype)	((uint64_t)(mtype) << 48)
 #define AMDGPU_PTE_MTYPE_NV10_MASK     AMDGPU_PTE_MTYPE_NV10_SHIFT(7ULL)
 #define AMDGPU_PTE_MTYPE_NV10(flags, mtype)			\
-	(((uint64_t)(flags) & (~AMDGPU_PTE_MTYPE_NV10_MASK)) |	\
-	  AMDGPU_PTE_MTYPE_NV10_SHIFT(mtype))
+(((uint64_t)(flags) & (~AMDGPU_PTE_MTYPE_NV10_MASK)) |	\
+AMDGPU_PTE_MTYPE_NV10_SHIFT(mtype))
 
 /* gfx12 */
 #define AMDGPU_PTE_PRT_GFX12		(1ULL << 56)
 #define AMDGPU_PTE_PRT_FLAG(adev)	\
-	((amdgpu_ip_version((adev), GC_HWIP, 0) >= IP_VERSION(12, 0, 0)) ? AMDGPU_PTE_PRT_GFX12 : AMDGPU_PTE_PRT)
+((amdgpu_ip_version((adev), GC_HWIP, 0) >= IP_VERSION(12, 0, 0)) ? AMDGPU_PTE_PRT_GFX12 : AMDGPU_PTE_PRT)
 
 #define AMDGPU_PTE_MTYPE_GFX12_SHIFT(mtype)	((uint64_t)(mtype) << 54)
 #define AMDGPU_PTE_MTYPE_GFX12_MASK	AMDGPU_PTE_MTYPE_GFX12_SHIFT(3ULL)
 #define AMDGPU_PTE_MTYPE_GFX12(flags, mtype)				\
-	(((uint64_t)(flags) & (~AMDGPU_PTE_MTYPE_GFX12_MASK)) |	\
-	  AMDGPU_PTE_MTYPE_GFX12_SHIFT(mtype))
+(((uint64_t)(flags) & (~AMDGPU_PTE_MTYPE_GFX12_MASK)) |	\
+AMDGPU_PTE_MTYPE_GFX12_SHIFT(mtype))
 
 #define AMDGPU_PTE_DCC			(1ULL << 58)
 #define AMDGPU_PTE_IS_PTE		(1ULL << 63)
@@ -134,17 +134,23 @@ struct amdgpu_bo_vm;
 /* PDE Block Fragment Size for gfx v12 */
 #define AMDGPU_PDE_BFS_GFX12(a)		((uint64_t)((a) & 0x1fULL) << 58)
 #define AMDGPU_PDE_BFS_FLAG(adev, a)	\
-	((amdgpu_ip_version((adev), GC_HWIP, 0) >= IP_VERSION(12, 0, 0)) ? AMDGPU_PDE_BFS_GFX12(a) : AMDGPU_PDE_BFS(a))
+((amdgpu_ip_version((adev), GC_HWIP, 0) >= IP_VERSION(12, 0, 0)) ? AMDGPU_PDE_BFS_GFX12(a) : AMDGPU_PDE_BFS(a))
 /* PDE is handled as PTE for gfx v12 */
 #define AMDGPU_PDE_PTE_GFX12		(1ULL << 63)
 #define AMDGPU_PDE_PTE_FLAG(adev)	\
-	((amdgpu_ip_version((adev), GC_HWIP, 0) >= IP_VERSION(12, 0, 0)) ? AMDGPU_PDE_PTE_GFX12 : AMDGPU_PDE_PTE)
+((amdgpu_ip_version((adev), GC_HWIP, 0) >= IP_VERSION(12, 0, 0)) ? AMDGPU_PDE_PTE_GFX12 : AMDGPU_PDE_PTE)
 
 /* How to program VM fault handling */
 #define AMDGPU_VM_FAULT_STOP_NEVER	0
 #define AMDGPU_VM_FAULT_STOP_FIRST	1
 #define AMDGPU_VM_FAULT_STOP_ALWAYS	2
 
+/* Maximum fault reports drained per IRQ pass */
+#define AMDGPU_VM_FAULT_REPORT_BATCH	24
+
+/* KFIFO size must be power‑of‑two; keep head‑room above 2×batch (=48) */
+#define AMDGPU_VM_FAULT_FIFO_SIZE	64
+
 /* How much VRAM be reserved for page tables */
 #define AMDGPU_VM_RESERVED_VRAM		(8ULL << 20)
 
@@ -167,18 +173,18 @@ struct amdgpu_bo_vm;
 /* Reserve space at top/bottom of address space for kernel use */
 #define AMDGPU_VA_RESERVED_CSA_SIZE		(2ULL << 20)
 #define AMDGPU_VA_RESERVED_CSA_START(adev)	(((adev)->vm_manager.max_pfn \
-						  << AMDGPU_GPU_PAGE_SHIFT)  \
-						 - AMDGPU_VA_RESERVED_CSA_SIZE)
+<< AMDGPU_GPU_PAGE_SHIFT)  \
+- AMDGPU_VA_RESERVED_CSA_SIZE)
 #define AMDGPU_VA_RESERVED_SEQ64_SIZE		(2ULL << 20)
 #define AMDGPU_VA_RESERVED_SEQ64_START(adev)	(AMDGPU_VA_RESERVED_CSA_START(adev) \
-						 - AMDGPU_VA_RESERVED_SEQ64_SIZE)
+- AMDGPU_VA_RESERVED_SEQ64_SIZE)
 #define AMDGPU_VA_RESERVED_TRAP_SIZE		(2ULL << 12)
 #define AMDGPU_VA_RESERVED_TRAP_START(adev)	(AMDGPU_VA_RESERVED_SEQ64_START(adev) \
-						 - AMDGPU_VA_RESERVED_TRAP_SIZE)
+- AMDGPU_VA_RESERVED_TRAP_SIZE)
 #define AMDGPU_VA_RESERVED_BOTTOM		(1ULL << 16)
 #define AMDGPU_VA_RESERVED_TOP			(AMDGPU_VA_RESERVED_TRAP_SIZE + \
-						 AMDGPU_VA_RESERVED_SEQ64_SIZE + \
-						 AMDGPU_VA_RESERVED_CSA_SIZE)
+AMDGPU_VA_RESERVED_SEQ64_SIZE + \
+AMDGPU_VA_RESERVED_CSA_SIZE)
 
 /* See vm_update_mode */
 #define AMDGPU_VM_USE_CPU_FOR_GFX (1 << 0)
@@ -212,6 +218,12 @@ struct amdgpu_vm_bo_base {
 
 	/* protected by the BO being reserved */
 	bool				moved;
+
+	/* The memory type used for the last stats increment.
+	 * Protected by vm status_lock. Used to ensure decrement matches.
+	 * Initialized to __AMDGPU_PL_NUM (invalid).
+	 */
+	uint32_t			last_stat_memtype; // <<< Added Field
 };
 
 /* provided by hw blocks that can write ptes, e.g., sdma */
@@ -221,18 +233,18 @@ struct amdgpu_vm_pte_funcs {
 
 	/* copy pte entries from GART */
 	void (*copy_pte)(struct amdgpu_ib *ib,
-			 uint64_t pe, uint64_t src,
-			 unsigned count);
+					 uint64_t pe, uint64_t src,
+				  unsigned count);
 
 	/* write pte one entry at a time with addr mapping */
 	void (*write_pte)(struct amdgpu_ib *ib, uint64_t pe,
-			  uint64_t value, unsigned count,
-			  uint32_t incr);
+					  uint64_t value, unsigned count,
+				   uint32_t incr);
 	/* for linear pte/pde updates without addr mapping */
 	void (*set_pte_pde)(struct amdgpu_ib *ib,
-			    uint64_t pe,
-			    uint64_t addr, unsigned count,
-			    uint32_t incr, uint64_t flags);
+						uint64_t pe,
+					 uint64_t addr, unsigned count,
+					 uint32_t incr, uint64_t flags);
 };
 
 struct amdgpu_task_info {
@@ -309,12 +321,12 @@ struct amdgpu_vm_update_params {
 struct amdgpu_vm_update_funcs {
 	int (*map_table)(struct amdgpu_bo_vm *bo);
 	int (*prepare)(struct amdgpu_vm_update_params *p,
-		       struct amdgpu_sync *sync);
+				   struct amdgpu_sync *sync);
 	int (*update)(struct amdgpu_vm_update_params *p,
-		      struct amdgpu_bo_vm *bo, uint64_t pe, uint64_t addr,
-		      unsigned count, uint32_t incr, uint64_t flags);
+				  struct amdgpu_bo_vm *bo, uint64_t pe, uint64_t addr,
+			   unsigned count, uint32_t incr, uint64_t flags);
 	int (*commit)(struct amdgpu_vm_update_params *p,
-		      struct dma_fence **fence);
+				  struct dma_fence **fence);
 };
 
 struct amdgpu_vm_fault_info {
@@ -407,8 +419,8 @@ struct amdgpu_vm {
 	/* Functions to use for VM table updates */
 	const struct amdgpu_vm_update_funcs	*update_funcs;
 
-	/* Up to 128 pending retry page faults */
-	DECLARE_KFIFO(faults, u64, 128);
+	/* holds (2×batch) addresses to avoid overflow under storm */
+	DECLARE_KFIFO(faults, u64, AMDGPU_VM_FAULT_FIFO_SIZE);
 
 	/* Points to the KFD process VM info */
 	struct amdkfd_process_info *process_info;
@@ -488,7 +500,7 @@ void amdgpu_vm_manager_init(struct amdgp
 void amdgpu_vm_manager_fini(struct amdgpu_device *adev);
 
 int amdgpu_vm_set_pasid(struct amdgpu_device *adev, struct amdgpu_vm *vm,
-			u32 pasid);
+						u32 pasid);
 
 long amdgpu_vm_wait_idle(struct amdgpu_vm *vm, long timeout);
 int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm, int32_t xcp_id);
@@ -496,76 +508,76 @@ int amdgpu_vm_make_compute(struct amdgpu
 void amdgpu_vm_release_compute(struct amdgpu_device *adev, struct amdgpu_vm *vm);
 void amdgpu_vm_fini(struct amdgpu_device *adev, struct amdgpu_vm *vm);
 int amdgpu_vm_lock_pd(struct amdgpu_vm *vm, struct drm_exec *exec,
-		      unsigned int num_fences);
+					  unsigned int num_fences);
 bool amdgpu_vm_ready(struct amdgpu_vm *vm);
 uint64_t amdgpu_vm_generation(struct amdgpu_device *adev, struct amdgpu_vm *vm);
 int amdgpu_vm_validate(struct amdgpu_device *adev, struct amdgpu_vm *vm,
-		       struct ww_acquire_ctx *ticket,
-		       int (*callback)(void *p, struct amdgpu_bo *bo),
-		       void *param);
+					   struct ww_acquire_ctx *ticket,
+					   int (*callback)(void *p, struct amdgpu_bo *bo),
+					   void *param);
 int amdgpu_vm_flush(struct amdgpu_ring *ring, struct amdgpu_job *job, bool need_pipe_sync);
 int amdgpu_vm_update_pdes(struct amdgpu_device *adev,
-			  struct amdgpu_vm *vm, bool immediate);
+						  struct amdgpu_vm *vm, bool immediate);
 int amdgpu_vm_clear_freed(struct amdgpu_device *adev,
-			  struct amdgpu_vm *vm,
-			  struct dma_fence **fence);
+						  struct amdgpu_vm *vm,
+						  struct dma_fence **fence);
 int amdgpu_vm_handle_moved(struct amdgpu_device *adev,
-			   struct amdgpu_vm *vm,
-			   struct ww_acquire_ctx *ticket);
+						   struct amdgpu_vm *vm,
+						   struct ww_acquire_ctx *ticket);
 int amdgpu_vm_flush_compute_tlb(struct amdgpu_device *adev,
-				struct amdgpu_vm *vm,
-				uint32_t flush_type,
-				uint32_t xcc_mask);
+								struct amdgpu_vm *vm,
+								uint32_t flush_type,
+								uint32_t xcc_mask);
 void amdgpu_vm_bo_base_init(struct amdgpu_vm_bo_base *base,
-			    struct amdgpu_vm *vm, struct amdgpu_bo *bo);
+							struct amdgpu_vm *vm, struct amdgpu_bo *bo);
 int amdgpu_vm_update_range(struct amdgpu_device *adev, struct amdgpu_vm *vm,
-			   bool immediate, bool unlocked, bool flush_tlb,
-			   bool allow_override, struct amdgpu_sync *sync,
-			   uint64_t start, uint64_t last, uint64_t flags,
-			   uint64_t offset, uint64_t vram_base,
-			   struct ttm_resource *res, dma_addr_t *pages_addr,
-			   struct dma_fence **fence);
+						   bool immediate, bool unlocked, bool flush_tlb,
+						   bool allow_override, struct amdgpu_sync *sync,
+						   uint64_t start, uint64_t last, uint64_t flags,
+						   uint64_t offset, uint64_t vram_base,
+						   struct ttm_resource *res, dma_addr_t *pages_addr,
+						   struct dma_fence **fence);
 int amdgpu_vm_bo_update(struct amdgpu_device *adev,
-			struct amdgpu_bo_va *bo_va,
-			bool clear);
+						struct amdgpu_bo_va *bo_va,
+						bool clear);
 bool amdgpu_vm_evictable(struct amdgpu_bo *bo);
 void amdgpu_vm_bo_invalidate(struct amdgpu_bo *bo, bool evicted);
 void amdgpu_vm_update_stats(struct amdgpu_vm_bo_base *base,
-			    struct ttm_resource *new_res, int sign);
+							struct ttm_resource *new_res, int sign);
 void amdgpu_vm_bo_update_shared(struct amdgpu_bo *bo);
 void amdgpu_vm_bo_move(struct amdgpu_bo *bo, struct ttm_resource *new_mem,
-		       bool evicted);
+					   bool evicted);
 uint64_t amdgpu_vm_map_gart(const dma_addr_t *pages_addr, uint64_t addr);
 struct amdgpu_bo_va *amdgpu_vm_bo_find(struct amdgpu_vm *vm,
-				       struct amdgpu_bo *bo);
+									   struct amdgpu_bo *bo);
 struct amdgpu_bo_va *amdgpu_vm_bo_add(struct amdgpu_device *adev,
-				      struct amdgpu_vm *vm,
-				      struct amdgpu_bo *bo);
+									  struct amdgpu_vm *vm,
+									  struct amdgpu_bo *bo);
 int amdgpu_vm_bo_map(struct amdgpu_device *adev,
-		     struct amdgpu_bo_va *bo_va,
-		     uint64_t addr, uint64_t offset,
-		     uint64_t size, uint64_t flags);
+					 struct amdgpu_bo_va *bo_va,
+					 uint64_t addr, uint64_t offset,
+					 uint64_t size, uint64_t flags);
 int amdgpu_vm_bo_replace_map(struct amdgpu_device *adev,
-			     struct amdgpu_bo_va *bo_va,
-			     uint64_t addr, uint64_t offset,
-			     uint64_t size, uint64_t flags);
+							 struct amdgpu_bo_va *bo_va,
+							 uint64_t addr, uint64_t offset,
+							 uint64_t size, uint64_t flags);
 int amdgpu_vm_bo_unmap(struct amdgpu_device *adev,
-		       struct amdgpu_bo_va *bo_va,
-		       uint64_t addr);
+					   struct amdgpu_bo_va *bo_va,
+					   uint64_t addr);
 int amdgpu_vm_bo_clear_mappings(struct amdgpu_device *adev,
-				struct amdgpu_vm *vm,
-				uint64_t saddr, uint64_t size);
+								struct amdgpu_vm *vm,
+								uint64_t saddr, uint64_t size);
 struct amdgpu_bo_va_mapping *amdgpu_vm_bo_lookup_mapping(struct amdgpu_vm *vm,
-							 uint64_t addr);
+														 uint64_t addr);
 void amdgpu_vm_bo_trace_cs(struct amdgpu_vm *vm, struct ww_acquire_ctx *ticket);
 void amdgpu_vm_bo_del(struct amdgpu_device *adev,
-		      struct amdgpu_bo_va *bo_va);
+					  struct amdgpu_bo_va *bo_va);
 void amdgpu_vm_adjust_size(struct amdgpu_device *adev, uint32_t min_vm_size,
-			   uint32_t fragment_size_default, unsigned max_level,
-			   unsigned max_bits);
+						   uint32_t fragment_size_default, unsigned max_level,
+						   unsigned max_bits);
 int amdgpu_vm_ioctl(struct drm_device *dev, void *data, struct drm_file *filp);
 bool amdgpu_vm_need_pipeline_sync(struct amdgpu_ring *ring,
-				  struct amdgpu_job *job);
+								  struct amdgpu_job *job);
 void amdgpu_vm_check_compute_bug(struct amdgpu_device *adev);
 
 struct amdgpu_task_info *
@@ -577,31 +589,31 @@ amdgpu_vm_get_task_info_vm(struct amdgpu
 void amdgpu_vm_put_task_info(struct amdgpu_task_info *task_info);
 
 bool amdgpu_vm_handle_fault(struct amdgpu_device *adev, u32 pasid,
-			    u32 vmid, u32 node_id, uint64_t addr, uint64_t ts,
-			    bool write_fault);
+							u32 vmid, u32 node_id, uint64_t addr, uint64_t ts,
+							bool write_fault);
 
 void amdgpu_vm_set_task_info(struct amdgpu_vm *vm);
 
 void amdgpu_vm_move_to_lru_tail(struct amdgpu_device *adev,
-				struct amdgpu_vm *vm);
+								struct amdgpu_vm *vm);
 void amdgpu_vm_get_memory(struct amdgpu_vm *vm,
-			  struct amdgpu_mem_stats stats[__AMDGPU_PL_NUM]);
+						  struct amdgpu_mem_stats stats[__AMDGPU_PL_NUM]);
 
 int amdgpu_vm_pt_clear(struct amdgpu_device *adev, struct amdgpu_vm *vm,
-		       struct amdgpu_bo_vm *vmbo, bool immediate);
+					   struct amdgpu_bo_vm *vmbo, bool immediate);
 int amdgpu_vm_pt_create(struct amdgpu_device *adev, struct amdgpu_vm *vm,
-			int level, bool immediate, struct amdgpu_bo_vm **vmbo,
-			int32_t xcp_id);
+						int level, bool immediate, struct amdgpu_bo_vm **vmbo,
+						int32_t xcp_id);
 void amdgpu_vm_pt_free_root(struct amdgpu_device *adev, struct amdgpu_vm *vm);
 
 int amdgpu_vm_pde_update(struct amdgpu_vm_update_params *params,
-			 struct amdgpu_vm_bo_base *entry);
+						 struct amdgpu_vm_bo_base *entry);
 int amdgpu_vm_ptes_update(struct amdgpu_vm_update_params *params,
-			  uint64_t start, uint64_t end,
-			  uint64_t dst, uint64_t flags);
+						  uint64_t start, uint64_t end,
+						  uint64_t dst, uint64_t flags);
 void amdgpu_vm_pt_free_work(struct work_struct *work);
 void amdgpu_vm_pt_free_list(struct amdgpu_device *adev,
-			    struct amdgpu_vm_update_params *params);
+							struct amdgpu_vm_update_params *params);
 
 #if defined(CONFIG_DEBUG_FS)
 void amdgpu_debugfs_vm_bo_info(struct amdgpu_vm *vm, struct seq_file *m);
@@ -618,7 +630,7 @@ bool amdgpu_vm_is_bo_always_valid(struct
  * Returns the tlb flush sequence number which indicates that the VM TLBs needs
  * to be invalidated whenever the sequence number change.
  */
-static inline uint64_t amdgpu_vm_tlb_seq(struct amdgpu_vm *vm)
+static __always_inline uint64_t amdgpu_vm_tlb_seq(struct amdgpu_vm *vm)
 {
 	unsigned long flags;
 	spinlock_t *lock;
@@ -665,12 +677,12 @@ static inline void amdgpu_vm_eviction_un
 }
 
 void amdgpu_vm_update_fault_cache(struct amdgpu_device *adev,
-				  unsigned int pasid,
-				  uint64_t addr,
-				  uint32_t status,
-				  unsigned int vmhub);
+								  unsigned int pasid,
+								  uint64_t addr,
+								  uint32_t status,
+								  unsigned int vmhub);
 void amdgpu_vm_tlb_fence_create(struct amdgpu_device *adev,
-				 struct amdgpu_vm *vm,
-				 struct dma_fence **fence);
+								struct amdgpu_vm *vm,
+								struct dma_fence **fence);
 
 #endif



--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm_pt.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm_pt.c	2025-04-12 16:51:37.138829348 +0200
@@ -487,43 +487,76 @@ int amdgpu_vm_pt_create(struct amdgpu_de
  * Make sure a specific page table or directory is allocated.
  *
  * Returns:
- * 1 if page table needed to be allocated, 0 if page table was already
- * allocated, negative errno if an error occurred.
+ * 0 if page table was already allocated or successfully allocated+cleared,
+ * negative errno if an error occurred.
  */
 static int amdgpu_vm_pt_alloc(struct amdgpu_device *adev,
-			      struct amdgpu_vm *vm,
-			      struct amdgpu_vm_pt_cursor *cursor,
-			      bool immediate)
+							  struct amdgpu_vm *vm,
+							  struct amdgpu_vm_pt_cursor *cursor,
+							  bool immediate)
 {
 	struct amdgpu_vm_bo_base *entry = cursor->entry;
 	struct amdgpu_bo *pt_bo;
-	struct amdgpu_bo_vm *pt;
+	struct amdgpu_bo_vm *pt; // This will point to the new vmbo struct
 	int r;
 
-	if (entry->bo)
+	if (entry->bo) // Already exists? Return OK.
 		return 0;
 
+	/* Unlock VM eviction lock while creating BO */
 	amdgpu_vm_eviction_unlock(vm);
+	/* Create the BO and vmbo struct */
 	r = amdgpu_vm_pt_create(adev, vm, cursor->level, immediate, &pt,
-				vm->root.bo->xcp_id);
-	amdgpu_vm_eviction_lock(vm);
-	if (r)
-		return r;
-
-	/* Keep a reference to the root directory to avoid
-	 * freeing them up in the wrong order.
-	 */
-	pt_bo = &pt->bo;
-	pt_bo->parent = amdgpu_bo_ref(cursor->parent->bo);
+							vm->root.bo->xcp_id);
+	amdgpu_vm_eviction_lock(vm); // Relock
+	if (r) { /* Fixed Line */
+		return r; // Failed creation, no BO exists, stats not touched yet. OK.
+	} /* Fixed Line */
+
+	/* Keep a reference to the root directory to avoid freeing them up in the wrong order. */
+	pt_bo = &pt->bo; // pt_bo is the amdgpu_bo within the vmbo 'pt'
+	pt_bo->parent = amdgpu_bo_ref(cursor->parent->bo); // Link to parent PD/PT
+
+	/* Initialize base, link to VM, INCREMENT STATS (+1) */
+	/* Also links pt_bo->vm_bo = entry and adds entry to vm status list */
 	amdgpu_vm_bo_base_init(entry, vm, pt_bo);
+
+	/* Clear the newly created PT/PD BO */
 	r = amdgpu_vm_pt_clear(adev, vm, pt, immediate);
-	if (r)
-		goto error_free_pt;
+	if (r) { /* Fixed Line */
+		goto error_free_pt; // Jump to cleanup if clear fails
+	} /* Fixed Line */
 
+	/* Successfully allocated and cleared */
 	return 0;
 
-error_free_pt:
+	error_free_pt:
+	/* Cleanup after amdgpu_vm_pt_clear failed */
+	/* === FIX: Explicitly perform pt_free steps BEFORE unref === */
+
+	/* 1. Decrement VM stats */
+	amdgpu_vm_update_stats(entry, pt_bo->tbo.resource, -1);
+
+	/* 2. Clear the link from the BO back to the VM entry */
+	pt_bo->vm_bo = NULL;
+
+	/* 3. Remove entry from VM status list */
+	spin_lock(&vm->status_lock);
+	list_del_init(&entry->vm_status);
+	spin_unlock(&vm->status_lock);
+
+	/* 4. Clear the parent's entry pointer back to NULL */
+	/* This prevents use-after-free if parent is traversed again */
+	entry->bo = NULL;
+	/* entry->vm remains valid */
+
+	/* 5. Unlink from parent BO */
+	amdgpu_bo_unref(&pt_bo->parent);
+	pt_bo->parent = NULL; // Prevent double unref if bo_unref is called again
+
+	/* 6. Now unref the BO itself */
 	amdgpu_bo_unref(&pt_bo);
+
 	return r;
 }
 

--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c	2025-04-16 15:21:00.725020339 +0200
@@ -30,6 +30,8 @@
 #include <linux/interval_tree_generic.h>
 #include <linux/idr.h>
 #include <linux/dma-buf.h>
+#include <linux/list_sort.h>
+#include <linux/jump_label.h>
 
 #include <drm/amdgpu_drm.h>
 #include <drm/drm_drv.h>
@@ -96,6 +98,24 @@ INTERVAL_TREE_DEFINE(struct amdgpu_bo_va
 #undef START
 #undef LAST
 
+/* Heavy‑weight TLB flush is needed only on a handful of ASICs.            */
+/* Default ‑ false => compiles to a NOP in every hot call‑site.            */
+static DEFINE_STATIC_KEY_FALSE(amdgpu_vm_always_flush);
+
+/* One‑shot detection, to be invoked during early device initialisation.  */
+static void amdgpu_vm_init_flush_static_key(struct amdgpu_device *adev)
+{
+	bool needs_flush;
+
+	needs_flush =
+	(adev->gmc.xgmi.num_physical_nodes &&
+	amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 0)) ||
+	(amdgpu_ip_version(adev, GC_HWIP, 0) < IP_VERSION(9, 0, 0));
+
+	if (needs_flush)
+		static_branch_enable(&amdgpu_vm_always_flush);
+}
+
 /**
  * struct amdgpu_prt_cb - Helper to disable partial resident texture feature from a fence callback
  */
@@ -175,18 +195,14 @@ int amdgpu_vm_set_pasid(struct amdgpu_de
  * State for PDs/PTs and per VM BOs which are not at the location they should
  * be.
  */
-static void amdgpu_vm_bo_evicted(struct amdgpu_vm_bo_base *vm_bo)
+static void amdgpu_vm_bo_evicted(struct amdgpu_vm_bo_base *base)
 {
-	struct amdgpu_vm *vm = vm_bo->vm;
-	struct amdgpu_bo *bo = vm_bo->bo;
+	struct amdgpu_vm *vm = base->vm;
 
-	vm_bo->moved = true;
-	spin_lock(&vm_bo->vm->status_lock);
-	if (bo->tbo.type == ttm_bo_type_kernel)
-		list_move(&vm_bo->vm_status, &vm->evicted);
-	else
-		list_move_tail(&vm_bo->vm_status, &vm->evicted);
-	spin_unlock(&vm_bo->vm->status_lock);
+	base->moved = true;
+	spin_lock(&vm->status_lock);
+	list_move_tail(&base->vm_status, &vm->evicted);
+	spin_unlock(&vm->status_lock);
 }
 /**
  * amdgpu_vm_bo_moved - vm_bo is moved
@@ -321,23 +337,24 @@ static void amdgpu_vm_bo_reset_state_mac
  */
 static void amdgpu_vm_update_shared(struct amdgpu_vm_bo_base *base)
 {
-	struct amdgpu_vm *vm = base->vm;
-	struct amdgpu_bo *bo = base->bo;
-	uint64_t size = amdgpu_bo_size(bo);
-	uint32_t bo_memtype = amdgpu_bo_mem_stats_placement(bo);
-	bool shared;
+	struct amdgpu_vm *vm  = base->vm;
+	struct amdgpu_bo *bo  = base->bo;
+	bool shared = drm_gem_object_is_shared_for_memory_stats(&bo->tbo.base);
+
+	if (base->shared == shared)
+		return;
 
 	spin_lock(&vm->status_lock);
-	shared = drm_gem_object_is_shared_for_memory_stats(&bo->tbo.base);
-	if (base->shared != shared) {
-		base->shared = shared;
-		if (shared) {
-			vm->stats[bo_memtype].drm.shared += size;
-			vm->stats[bo_memtype].drm.private -= size;
-		} else {
-			vm->stats[bo_memtype].drm.shared -= size;
-			vm->stats[bo_memtype].drm.private += size;
-		}
+	base->shared = shared;
+	u64 sz = amdgpu_bo_size(bo);
+	u32 type = amdgpu_bo_mem_stats_placement(bo);
+
+	if (shared) {
+		vm->stats[type].drm.shared  += sz;
+		vm->stats[type].drm.private -= sz;
+	} else {
+		vm->stats[type].drm.shared  -= sz;
+		vm->stats[type].drm.private += sz;
 	}
 	spin_unlock(&vm->status_lock);
 }
@@ -351,10 +368,23 @@ static void amdgpu_vm_update_shared(stru
  */
 void amdgpu_vm_bo_update_shared(struct amdgpu_bo *bo)
 {
-	struct amdgpu_vm_bo_base *base;
+	struct amdgpu_vm_bo_base *b;
+
+	bool shared = drm_gem_object_is_shared_for_memory_stats(&bo->tbo.base);
+	for (b = bo->vm_bo; b; b = b->next)
+		if (b->shared != shared)
+			amdgpu_vm_update_shared(b);
+}
+
+static void stat_add_safe(u64 *field, int64_t delta)
+{
+	if (unlikely(delta < 0)) {
+		u64 dec = (u64)(-delta);
 
-	for (base = bo->vm_bo; base; base = base->next)
-		amdgpu_vm_update_shared(base);
+		*field = (*field > dec) ? (*field - dec) : 0;
+	} else {
+		*field += (u64)delta;
+	}
 }
 
 /**
@@ -368,32 +398,43 @@ void amdgpu_vm_bo_update_shared(struct a
  * need to happen at the same time.
  */
 static void amdgpu_vm_update_stats_locked(struct amdgpu_vm_bo_base *base,
-			    struct ttm_resource *res, int sign)
+										  struct ttm_resource *res,
+										  int sign)
 {
-	struct amdgpu_vm *vm = base->vm;
-	struct amdgpu_bo *bo = base->bo;
-	int64_t size = sign * amdgpu_bo_size(bo);
-	uint32_t bo_memtype = amdgpu_bo_mem_stats_placement(bo);
+	struct amdgpu_vm  *vm  = base->vm;
+	struct amdgpu_bo  *bo  = base->bo;
+	u64 sz;
+	u32 pref, stat_mt;
+	int64_t delta;
+
+	if (!vm || !bo || !vm->root.bo)
+		return;
+
+	sz    = amdgpu_bo_size(bo);
+	delta = (sign > 0) ? (int64_t)sz : -(int64_t)sz;
+	pref  = amdgpu_bo_mem_stats_placement(bo);
 
-	/* For drm-total- and drm-shared-, BO are accounted by their preferred
-	 * placement, see also amdgpu_bo_mem_stats_placement.
-	 */
 	if (base->shared)
-		vm->stats[bo_memtype].drm.shared += size;
+		stat_add_safe(&vm->stats[pref].drm.shared,  delta);
 	else
-		vm->stats[bo_memtype].drm.private += size;
+		stat_add_safe(&vm->stats[pref].drm.private, delta);
 
-	if (res && res->mem_type < __AMDGPU_PL_NUM) {
-		uint32_t res_memtype = res->mem_type;
+	if (sign > 0) {
+		stat_mt = (res && res->mem_type < __AMDGPU_PL_NUM) ?
+		res->mem_type : pref;
+		base->last_stat_memtype = stat_mt;
+	} else {
+		stat_mt = base->last_stat_memtype;
+		if (stat_mt >= __AMDGPU_PL_NUM)
+			stat_mt = pref;
+	}
 
-		vm->stats[res_memtype].drm.resident += size;
-		/* BO only count as purgeable if it is resident,
-		 * since otherwise there's nothing to purge.
-		 */
+	if (stat_mt < __AMDGPU_PL_NUM) {
+		stat_add_safe(&vm->stats[stat_mt].drm.resident, delta);
 		if (bo->flags & AMDGPU_GEM_CREATE_DISCARDABLE)
-			vm->stats[res_memtype].drm.purgeable += size;
-		if (!(bo->preferred_domains & amdgpu_mem_type_to_domain(res_memtype)))
-			vm->stats[bo_memtype].evicted += size;
+			stat_add_safe(&vm->stats[stat_mt].drm.purgeable, delta);
+		if (!(bo->preferred_domains & amdgpu_mem_type_to_domain(stat_mt)))
+			stat_add_safe(&vm->stats[pref].evicted, delta);
 	}
 }
 
@@ -427,44 +468,51 @@ void amdgpu_vm_update_stats(struct amdgp
  *
  */
 void amdgpu_vm_bo_base_init(struct amdgpu_vm_bo_base *base,
-			    struct amdgpu_vm *vm, struct amdgpu_bo *bo)
+							struct amdgpu_vm *vm, struct amdgpu_bo *bo)
 {
-	base->vm = vm;
-	base->bo = bo;
-	base->next = NULL;
+	base->vm   = vm;
+	base->bo   = bo;
+	base->next = bo ? bo->vm_bo : NULL;
+	base->last_stat_memtype = __AMDGPU_PL_NUM;
 	INIT_LIST_HEAD(&base->vm_status);
 
-	if (!bo)
-		return;
-	base->next = bo->vm_bo;
-	bo->vm_bo = base;
+	if (bo)
+		bo->vm_bo = base;
 
 	spin_lock(&vm->status_lock);
-	base->shared = drm_gem_object_is_shared_for_memory_stats(&bo->tbo.base);
-	amdgpu_vm_update_stats_locked(base, bo->tbo.resource, +1);
+	base->shared = bo && drm_gem_object_is_shared_for_memory_stats(&bo->tbo.base);
+	amdgpu_vm_update_stats_locked(base, bo ? bo->tbo.resource : NULL, +1);
 	spin_unlock(&vm->status_lock);
 
-	if (!amdgpu_vm_is_bo_always_valid(vm, bo))
+	if (!bo || !amdgpu_vm_is_bo_always_valid(vm, bo))
 		return;
 
 	dma_resv_assert_held(vm->root.bo->tbo.base.resv);
-
 	ttm_bo_set_bulk_move(&bo->tbo, &vm->lru_bulk_move);
+
 	if (bo->tbo.type == ttm_bo_type_kernel && bo->parent)
 		amdgpu_vm_bo_relocated(base);
 	else
 		amdgpu_vm_bo_idle(base);
 
-	if (bo->preferred_domains &
-	    amdgpu_mem_type_to_domain(bo->tbo.resource->mem_type))
-		return;
+	if (!(bo->preferred_domains & amdgpu_mem_type_to_domain(bo->tbo.resource->mem_type)))
+		amdgpu_vm_bo_evicted(base);
+}
 
-	/*
-	 * we checked all the prerequisites, but it looks like this per vm bo
-	 * is currently evicted. add the bo to the evicted list to make sure it
-	 * is validated on next vm use to avoid fault.
-	 * */
-	amdgpu_vm_bo_evicted(base);
+static int compare_mappings(void *priv,
+							const struct list_head *a,
+							const struct list_head *b)
+{
+	const struct amdgpu_bo_va_mapping *ma =
+	list_entry(a, struct amdgpu_bo_va_mapping, list);
+	const struct amdgpu_bo_va_mapping *mb =
+	list_entry(b, struct amdgpu_bo_va_mapping, list);
+
+	if (ma->start < mb->start)
+		return -1;
+	if (ma->start > mb->start)
+		return 1;
+	return 0;
 }
 
 /**
@@ -717,19 +765,19 @@ void amdgpu_vm_check_compute_bug(struct
  * True if sync is needed.
  */
 bool amdgpu_vm_need_pipeline_sync(struct amdgpu_ring *ring,
-				  struct amdgpu_job *job)
+								  struct amdgpu_job *job)
 {
 	struct amdgpu_device *adev = ring->adev;
 	unsigned vmhub = ring->vm_hub;
 	struct amdgpu_vmid_mgr *id_mgr = &adev->vm_manager.id_mgr[vmhub];
 
-	if (job->vmid == 0)
+	if (unlikely(job->vmid == 0))
 		return false;
 
-	if (job->vm_needs_flush || ring->has_compute_vm_bug)
+	if (unlikely(job->vm_needs_flush || ring->has_compute_vm_bug))
 		return true;
 
-	if (ring->funcs->emit_gds_switch && job->gds_switch_needed)
+	if (unlikely(ring->funcs->emit_gds_switch && job->gds_switch_needed))
 		return true;
 
 	if (amdgpu_vmid_had_gpu_reset(adev, &id_mgr->ids[job->vmid]))
@@ -751,55 +799,57 @@ bool amdgpu_vm_need_pipeline_sync(struct
  * 0 on success, errno otherwise.
  */
 int amdgpu_vm_flush(struct amdgpu_ring *ring, struct amdgpu_job *job,
-		    bool need_pipe_sync)
+					bool need_pipe_sync)
 {
 	struct amdgpu_device *adev = ring->adev;
-	unsigned vmhub = ring->vm_hub;
+	unsigned int vmhub = ring->vm_hub;
 	struct amdgpu_vmid_mgr *id_mgr = &adev->vm_manager.id_mgr[vmhub];
 	struct amdgpu_vmid *id = &id_mgr->ids[job->vmid];
 	bool spm_update_needed = job->spm_update_needed;
 	bool gds_switch_needed = ring->funcs->emit_gds_switch &&
-		job->gds_switch_needed;
-	bool vm_flush_needed = job->vm_needs_flush;
+	job->gds_switch_needed;
+	bool vm_flush_needed   = job->vm_needs_flush;
 	struct dma_fence *fence = NULL;
 	bool pasid_mapping_needed = false;
-	unsigned int patch;
+	unsigned int patch = 0;		/* always initialised */
 	int r;
 
 	if (amdgpu_vmid_had_gpu_reset(adev, id)) {
-		gds_switch_needed = true;
-		vm_flush_needed = true;
+		gds_switch_needed    = true;
+		vm_flush_needed      = true;
 		pasid_mapping_needed = true;
-		spm_update_needed = true;
+		spm_update_needed    = true;
 	}
 
 	mutex_lock(&id_mgr->lock);
 	if (id->pasid != job->pasid || !id->pasid_mapping ||
-	    !dma_fence_is_signaled(id->pasid_mapping))
+		!dma_fence_is_signaled(id->pasid_mapping))
 		pasid_mapping_needed = true;
 	mutex_unlock(&id_mgr->lock);
 
-	gds_switch_needed &= !!ring->funcs->emit_gds_switch;
-	vm_flush_needed &= !!ring->funcs->emit_vm_flush  &&
-			job->vm_pd_addr != AMDGPU_BO_INVALID_OFFSET;
+	gds_switch_needed   &= !!ring->funcs->emit_gds_switch;
+	vm_flush_needed     &= !!ring->funcs->emit_vm_flush &&
+	job->vm_pd_addr != AMDGPU_BO_INVALID_OFFSET;
 	pasid_mapping_needed &= adev->gmc.gmc_funcs->emit_pasid_mapping &&
-		ring->funcs->emit_wreg;
+	ring->funcs->emit_wreg;
 
-	if (!vm_flush_needed && !gds_switch_needed && !need_pipe_sync &&
-	    !(job->enforce_isolation && !job->vmid))
+	if (likely(!vm_flush_needed && !gds_switch_needed && !need_pipe_sync &&
+		!(job->enforce_isolation && !job->vmid)))
 		return 0;
 
 	amdgpu_ring_ib_begin(ring);
-	if (ring->funcs->init_cond_exec)
+
+	if (ring->funcs->init_cond_exec) {
 		patch = amdgpu_ring_init_cond_exec(ring,
-						   ring->cond_exe_gpu_addr);
+										   ring->cond_exe_gpu_addr);
+	}
 
 	if (need_pipe_sync)
 		amdgpu_ring_emit_pipeline_sync(ring);
 
 	if (adev->gfx.enable_cleaner_shader &&
-	    ring->funcs->emit_cleaner_shader &&
-	    job->enforce_isolation)
+		ring->funcs->emit_cleaner_shader &&
+		job->enforce_isolation)
 		ring->funcs->emit_cleaner_shader(ring);
 
 	if (vm_flush_needed) {
@@ -814,47 +864,47 @@ int amdgpu_vm_flush(struct amdgpu_ring *
 		adev->gfx.rlc.funcs->update_spm_vmid(adev, ring, job->vmid);
 
 	if (!ring->is_mes_queue && ring->funcs->emit_gds_switch &&
-	    gds_switch_needed) {
+		gds_switch_needed) {
 		amdgpu_ring_emit_gds_switch(ring, job->vmid, job->gds_base,
-					    job->gds_size, job->gws_base,
-					    job->gws_size, job->oa_base,
-					    job->oa_size);
-	}
+									job->gds_size, job->gws_base,
+							  job->gws_size, job->oa_base,
+							  job->oa_size);
+		}
 
-	if (vm_flush_needed || pasid_mapping_needed) {
-		r = amdgpu_fence_emit(ring, &fence, NULL, 0);
-		if (r)
-			return r;
-	}
+		if (vm_flush_needed || pasid_mapping_needed) {
+			r = amdgpu_fence_emit(ring, &fence, NULL, 0);
+			if (r)
+				return r;
+		}
 
-	if (vm_flush_needed) {
-		mutex_lock(&id_mgr->lock);
-		dma_fence_put(id->last_flush);
-		id->last_flush = dma_fence_get(fence);
-		id->current_gpu_reset_count =
+		if (vm_flush_needed) {
+			mutex_lock(&id_mgr->lock);
+			dma_fence_put(id->last_flush);
+			id->last_flush = dma_fence_get(fence);
+			id->current_gpu_reset_count =
 			atomic_read(&adev->gpu_reset_counter);
-		mutex_unlock(&id_mgr->lock);
-	}
+			mutex_unlock(&id_mgr->lock);
+		}
 
-	if (pasid_mapping_needed) {
-		mutex_lock(&id_mgr->lock);
-		id->pasid = job->pasid;
-		dma_fence_put(id->pasid_mapping);
-		id->pasid_mapping = dma_fence_get(fence);
-		mutex_unlock(&id_mgr->lock);
-	}
-	dma_fence_put(fence);
+		if (pasid_mapping_needed) {
+			mutex_lock(&id_mgr->lock);
+			id->pasid = job->pasid;
+			dma_fence_put(id->pasid_mapping);
+			id->pasid_mapping = dma_fence_get(fence);
+			mutex_unlock(&id_mgr->lock);
+		}
+		dma_fence_put(fence);
 
-	amdgpu_ring_patch_cond_exec(ring, patch);
+		amdgpu_ring_patch_cond_exec(ring, patch);
 
-	/* the double SWITCH_BUFFER here *cannot* be skipped by COND_EXEC */
-	if (ring->funcs->emit_switch_buffer) {
-		amdgpu_ring_emit_switch_buffer(ring);
-		amdgpu_ring_emit_switch_buffer(ring);
-	}
+		/* the double SWITCH_BUFFER here *cannot* be skipped by COND_EXEC */
+		if (ring->funcs->emit_switch_buffer) {
+			amdgpu_ring_emit_switch_buffer(ring);
+			amdgpu_ring_emit_switch_buffer(ring);
+		}
 
-	amdgpu_ring_ib_end(ring);
-	return 0;
+		amdgpu_ring_ib_end(ring);
+		return 0;
 }
 
 /**
@@ -1062,12 +1112,12 @@ amdgpu_vm_tlb_flush(struct amdgpu_vm_upd
  * 0 for success, negative erro code for failure.
  */
 int amdgpu_vm_update_range(struct amdgpu_device *adev, struct amdgpu_vm *vm,
-			   bool immediate, bool unlocked, bool flush_tlb,
-			   bool allow_override, struct amdgpu_sync *sync,
-			   uint64_t start, uint64_t last, uint64_t flags,
-			   uint64_t offset, uint64_t vram_base,
-			   struct ttm_resource *res, dma_addr_t *pages_addr,
-			   struct dma_fence **fence)
+						   bool immediate, bool unlocked, bool flush_tlb,
+						   bool allow_override, struct amdgpu_sync *sync,
+						   uint64_t start, uint64_t last, uint64_t flags,
+						   uint64_t offset, uint64_t vram_base,
+						   struct ttm_resource *res, dma_addr_t *pages_addr,
+						   struct dma_fence **fence)
 {
 	struct amdgpu_vm_tlb_seq_struct *tlb_cb;
 	struct amdgpu_vm_update_params params;
@@ -1083,24 +1133,16 @@ int amdgpu_vm_update_range(struct amdgpu
 		return -ENOMEM;
 	}
 
-	/* Vega20+XGMI where PTEs get inadvertently cached in L2 texture cache,
-	 * heavy-weight flush TLB unconditionally.
-	 */
-	flush_tlb |= adev->gmc.xgmi.num_physical_nodes &&
-		     amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 0);
-
-	/*
-	 * On GFX8 and older any 8 PTE block with a valid bit set enters the TLB
-	 */
-	flush_tlb |= amdgpu_ip_version(adev, GC_HWIP, 0) < IP_VERSION(9, 0, 0);
+	/* unconditional heavy‑flush work‑around via static key */
+	flush_tlb |= static_branch_unlikely(&amdgpu_vm_always_flush);
 
 	memset(&params, 0, sizeof(params));
-	params.adev = adev;
-	params.vm = vm;
-	params.immediate = immediate;
-	params.pages_addr = pages_addr;
-	params.unlocked = unlocked;
-	params.needs_flush = flush_tlb;
+	params.adev           = adev;
+	params.vm             = vm;
+	params.immediate      = immediate;
+	params.pages_addr     = pages_addr;
+	params.unlocked       = unlocked;
+	params.needs_flush    = flush_tlb;
 	params.allow_override = allow_override;
 	INIT_LIST_HEAD(&params.tlb_flush_waitlist);
 
@@ -1123,7 +1165,8 @@ int amdgpu_vm_update_range(struct amdgpu
 		goto error_free;
 
 	amdgpu_res_first(pages_addr ? NULL : res, offset,
-			 (last - start + 1) * AMDGPU_GPU_PAGE_SIZE, &cursor);
+					 (last - start + 1) * AMDGPU_GPU_PAGE_SIZE, &cursor);
+
 	while (cursor.remaining) {
 		uint64_t tmp, num_entries, addr;
 
@@ -1136,21 +1179,22 @@ int amdgpu_vm_update_range(struct amdgpu
 				uint64_t count;
 
 				contiguous = pages_addr[pfn + 1] ==
-					pages_addr[pfn] + PAGE_SIZE;
+				pages_addr[pfn] + PAGE_SIZE;
 
 				tmp = num_entries /
-					AMDGPU_GPU_PAGES_IN_CPU_PAGE;
+				AMDGPU_GPU_PAGES_IN_CPU_PAGE;
 				for (count = 2; count < tmp; ++count) {
-					uint64_t idx = pfn + count;
+					uint64_t idx2 = pfn + count;
 
-					if (contiguous != (pages_addr[idx] ==
-					    pages_addr[idx - 1] + PAGE_SIZE))
+					if (contiguous !=
+						(pages_addr[idx2] ==
+						pages_addr[idx2 - 1] + PAGE_SIZE))
 						break;
 				}
 				if (!contiguous)
 					count--;
 				num_entries = count *
-					AMDGPU_GPU_PAGES_IN_CPU_PAGE;
+				AMDGPU_GPU_PAGES_IN_CPU_PAGE;
 			}
 
 			if (!contiguous) {
@@ -1160,19 +1204,20 @@ int amdgpu_vm_update_range(struct amdgpu
 				addr = pages_addr[cursor.start >> PAGE_SHIFT];
 				params.pages_addr = NULL;
 			}
-
-		} else if (flags & (AMDGPU_PTE_VALID | AMDGPU_PTE_PRT_FLAG(adev))) {
+		} else if (flags & (AMDGPU_PTE_VALID |
+			AMDGPU_PTE_PRT_FLAG(adev))) {
 			addr = vram_base + cursor.start;
-		} else {
-			addr = 0;
-		}
+			} else {
+				addr = 0;
+			}
 
-		tmp = start + num_entries;
+			tmp = start + num_entries;
 		r = amdgpu_vm_ptes_update(&params, start, tmp, addr, flags);
 		if (r)
 			goto error_free;
 
-		amdgpu_res_next(&cursor, num_entries * AMDGPU_GPU_PAGE_SIZE);
+		amdgpu_res_next(&cursor,
+						num_entries * AMDGPU_GPU_PAGE_SIZE);
 		start = tmp;
 	}
 
@@ -1187,7 +1232,7 @@ int amdgpu_vm_update_range(struct amdgpu
 
 	amdgpu_vm_pt_free_list(adev, &params);
 
-error_free:
+	error_free:
 	kfree(tlb_cb);
 	amdgpu_vm_eviction_unlock(vm);
 	drm_dev_exit(idx);
@@ -1500,51 +1545,76 @@ static void amdgpu_vm_prt_fini(struct am
  *
  */
 int amdgpu_vm_clear_freed(struct amdgpu_device *adev,
-			  struct amdgpu_vm *vm,
-			  struct dma_fence **fence)
+						  struct amdgpu_vm *vm,
+						  struct dma_fence **fence)
 {
-	struct amdgpu_bo_va_mapping *mapping;
-	struct dma_fence *f = NULL;
+	LIST_HEAD(local);
+	struct amdgpu_bo_va_mapping *m, *tmp;
 	struct amdgpu_sync sync;
-	int r;
+	struct dma_fence *f = NULL;
+	u64 cur_s = 0, cur_e = 0;
+	bool have_range = false;
+	int r = 0;
 
+	if (list_empty(&vm->freed))
+		return 0;
 
-	/*
-	 * Implicitly sync to command submissions in the same VM before
-	 * unmapping.
-	 */
 	amdgpu_sync_create(&sync);
 	r = amdgpu_sync_resv(adev, &sync, vm->root.bo->tbo.base.resv,
-			     AMDGPU_SYNC_EQ_OWNER, vm);
+						 AMDGPU_SYNC_EQ_OWNER, vm);
 	if (r)
-		goto error_free;
+		goto out_free_sync;
 
-	while (!list_empty(&vm->freed)) {
-		mapping = list_first_entry(&vm->freed,
-			struct amdgpu_bo_va_mapping, list);
-		list_del(&mapping->list);
+	spin_lock(&vm->status_lock);
+	list_splice_init(&vm->freed, &local);
+	spin_unlock(&vm->status_lock);
 
-		r = amdgpu_vm_update_range(adev, vm, false, false, true, false,
-					   &sync, mapping->start, mapping->last,
-					   0, 0, 0, NULL, NULL, &f);
-		amdgpu_vm_free_mapping(adev, vm, mapping, f);
-		if (r) {
-			dma_fence_put(f);
-			goto error_free;
+	if (likely(!list_is_singular(&local)))
+		list_sort(NULL, &local, compare_mappings);
+
+	list_for_each_entry_safe(m, tmp, &local, list) {
+		list_del_init(&m->list);
+
+		if (!have_range) {
+			cur_s = m->start;
+			cur_e = m->last;
+			have_range = true;
+		} else if (m->start <= cur_e + 1) {
+			cur_e = max(cur_e, m->last);
+		} else {
+			r = amdgpu_vm_update_range(adev, vm, false, false,
+									   true, false, &sync,
+							  cur_s, cur_e,
+							  0, 0, 0, NULL, NULL, &f);
+			if (r)
+				goto out_cleanup;
+			cur_s = m->start;
+			cur_e = m->last;
 		}
+		amdgpu_vm_free_mapping(adev, vm, m, f);
 	}
 
-	if (fence && f) {
-		dma_fence_put(*fence);
-		*fence = f;
-	} else {
-		dma_fence_put(f);
-	}
+	if (have_range)
+		r = amdgpu_vm_update_range(adev, vm, false, false, true, false,
+								   &sync, cur_s, cur_e,
+							 0, 0, 0, NULL, NULL, &f);
 
-error_free:
-	amdgpu_sync_free(&sync);
-	return r;
+		out_cleanup:
+		list_for_each_entry_safe(m, tmp, &local, list) {
+			list_del_init(&m->list);
+			amdgpu_vm_free_mapping(adev, vm, m, f);
+		}
+
+		if (fence) {
+			dma_fence_put(*fence);
+			*fence = f;
+		} else {
+			dma_fence_put(f);
+		}
 
+		out_free_sync:
+		amdgpu_sync_free(&sync);
+		return r;
 }
 
 /**
@@ -1795,43 +1865,42 @@ static int amdgpu_vm_verify_parameters(s
  * Object has to be reserved and unreserved outside!
  */
 int amdgpu_vm_bo_map(struct amdgpu_device *adev,
-		     struct amdgpu_bo_va *bo_va,
-		     uint64_t saddr, uint64_t offset,
-		     uint64_t size, uint64_t flags)
+					 struct amdgpu_bo_va *bo_va,
+					 u64 saddr, u64 offset, u64 size, u64 flags)
 {
-	struct amdgpu_bo_va_mapping *mapping, *tmp;
+	struct amdgpu_bo_va_mapping *map, *conf;
 	struct amdgpu_bo *bo = bo_va->base.bo;
-	struct amdgpu_vm *vm = bo_va->base.vm;
-	uint64_t eaddr;
+	struct amdgpu_vm *vm   = bo_va->base.vm;
+	u64 spg, epg;
 	int r;
 
 	r = amdgpu_vm_verify_parameters(adev, bo, saddr, offset, size);
 	if (r)
 		return r;
 
-	saddr /= AMDGPU_GPU_PAGE_SIZE;
-	eaddr = saddr + (size - 1) / AMDGPU_GPU_PAGE_SIZE;
+	spg = saddr >> AMDGPU_GPU_PAGE_SHIFT;
+	epg = (saddr + size - 1) >> AMDGPU_GPU_PAGE_SHIFT;
 
-	tmp = amdgpu_vm_it_iter_first(&vm->va, saddr, eaddr);
-	if (tmp) {
-		/* bo and tmp overlap, invalid addr */
-		dev_err(adev->dev, "bo %p va 0x%010Lx-0x%010Lx conflict with "
-			"0x%010Lx-0x%010Lx\n", bo, saddr, eaddr,
-			tmp->start, tmp->last + 1);
+	conf = amdgpu_vm_it_iter_first(&vm->va, spg, epg);
+	if (conf) {
+		dev_err(adev->dev,
+				"BO %p va 0x%010llx-0x%010llx conflicts with 0x%010llx-0x%010llx\n",
+		  bo, saddr, saddr + size,
+		  conf->start << AMDGPU_GPU_PAGE_SHIFT,
+		  (conf->last + 1) << AMDGPU_GPU_PAGE_SHIFT);
 		return -EINVAL;
 	}
 
-	mapping = kmalloc(sizeof(*mapping), GFP_KERNEL);
-	if (!mapping)
+	map = kmalloc(sizeof(*map), GFP_KERNEL);
+	if (!map)
 		return -ENOMEM;
 
-	mapping->start = saddr;
-	mapping->last = eaddr;
-	mapping->offset = offset;
-	mapping->flags = flags;
-
-	amdgpu_vm_bo_insert_map(adev, bo_va, mapping);
+	map->start  = spg;
+	map->last   = epg;
+	map->offset = offset;
+	map->flags  = flags;
 
+	amdgpu_vm_bo_insert_map(adev, bo_va, map);
 	return 0;
 }
 
@@ -1854,40 +1923,35 @@ int amdgpu_vm_bo_map(struct amdgpu_devic
  * Object has to be reserved and unreserved outside!
  */
 int amdgpu_vm_bo_replace_map(struct amdgpu_device *adev,
-			     struct amdgpu_bo_va *bo_va,
-			     uint64_t saddr, uint64_t offset,
-			     uint64_t size, uint64_t flags)
+							 struct amdgpu_bo_va *bo_va,
+							 u64 saddr, u64 offset, u64 size, u64 flags)
 {
-	struct amdgpu_bo_va_mapping *mapping;
+	struct amdgpu_bo_va_mapping *map;
 	struct amdgpu_bo *bo = bo_va->base.bo;
-	uint64_t eaddr;
+	u64 spg, epg;
 	int r;
 
 	r = amdgpu_vm_verify_parameters(adev, bo, saddr, offset, size);
 	if (r)
 		return r;
 
-	/* Allocate all the needed memory */
-	mapping = kmalloc(sizeof(*mapping), GFP_KERNEL);
-	if (!mapping)
-		return -ENOMEM;
-
 	r = amdgpu_vm_bo_clear_mappings(adev, bo_va->base.vm, saddr, size);
-	if (r) {
-		kfree(mapping);
+	if (r)
 		return r;
-	}
 
-	saddr /= AMDGPU_GPU_PAGE_SIZE;
-	eaddr = saddr + (size - 1) / AMDGPU_GPU_PAGE_SIZE;
+	map = kmalloc(sizeof(*map), GFP_KERNEL);
+	if (!map)
+		return -ENOMEM;
 
-	mapping->start = saddr;
-	mapping->last = eaddr;
-	mapping->offset = offset;
-	mapping->flags = flags;
+	spg = saddr >> AMDGPU_GPU_PAGE_SHIFT;
+	epg = (saddr + size - 1) >> AMDGPU_GPU_PAGE_SHIFT;
 
-	amdgpu_vm_bo_insert_map(adev, bo_va, mapping);
+	map->start  = spg;
+	map->last   = epg;
+	map->offset = offset;
+	map->flags  = flags;
 
+	amdgpu_vm_bo_insert_map(adev, bo_va, map);
 	return 0;
 }
 
@@ -1960,104 +2024,88 @@ int amdgpu_vm_bo_unmap(struct amdgpu_dev
  * 0 for success, error for failure.
  */
 int amdgpu_vm_bo_clear_mappings(struct amdgpu_device *adev,
-				struct amdgpu_vm *vm,
-				uint64_t saddr, uint64_t size)
+								struct amdgpu_vm *vm,
+								u64 saddr, u64 size)
 {
-	struct amdgpu_bo_va_mapping *before, *after, *tmp, *next;
+	struct amdgpu_bo_va_mapping *before = NULL, *after = NULL;
+	struct amdgpu_bo_va_mapping *m, *n, *split;
 	LIST_HEAD(removed);
-	uint64_t eaddr;
-	int r;
+	u64 spg, epg;
+	int r = 0;
 
 	r = amdgpu_vm_verify_parameters(adev, NULL, saddr, 0, size);
 	if (r)
 		return r;
 
-	saddr /= AMDGPU_GPU_PAGE_SIZE;
-	eaddr = saddr + (size - 1) / AMDGPU_GPU_PAGE_SIZE;
+	spg  = saddr >> AMDGPU_GPU_PAGE_SHIFT;
+	epg  = (saddr + size - 1) >> AMDGPU_GPU_PAGE_SHIFT;
 
-	/* Allocate all the needed memory */
 	before = kzalloc(sizeof(*before), GFP_KERNEL);
-	if (!before)
-		return -ENOMEM;
-	INIT_LIST_HEAD(&before->list);
-
-	after = kzalloc(sizeof(*after), GFP_KERNEL);
-	if (!after) {
+	after  = kzalloc(sizeof(*after),  GFP_KERNEL);
+	if (!before || !after) {
 		kfree(before);
+		kfree(after);
 		return -ENOMEM;
 	}
+	INIT_LIST_HEAD(&before->list);
 	INIT_LIST_HEAD(&after->list);
 
-	/* Now gather all removed mappings */
-	tmp = amdgpu_vm_it_iter_first(&vm->va, saddr, eaddr);
-	while (tmp) {
-		/* Remember mapping split at the start */
-		if (tmp->start < saddr) {
-			before->start = tmp->start;
-			before->last = saddr - 1;
-			before->offset = tmp->offset;
-			before->flags = tmp->flags;
-			before->bo_va = tmp->bo_va;
-			list_add(&before->list, &tmp->bo_va->invalids);
-		}
-
-		/* Remember mapping split at the end */
-		if (tmp->last > eaddr) {
-			after->start = eaddr + 1;
-			after->last = tmp->last;
-			after->offset = tmp->offset;
-			after->offset += (after->start - tmp->start) << PAGE_SHIFT;
-			after->flags = tmp->flags;
-			after->bo_va = tmp->bo_va;
-			list_add(&after->list, &tmp->bo_va->invalids);
-		}
-
-		list_del(&tmp->list);
-		list_add(&tmp->list, &removed);
-
-		tmp = amdgpu_vm_it_iter_next(tmp, saddr, eaddr);
-	}
-
-	/* And free them up */
-	list_for_each_entry_safe(tmp, next, &removed, list) {
-		amdgpu_vm_it_remove(tmp, &vm->va);
-		list_del(&tmp->list);
-
-		if (tmp->start < saddr)
-		    tmp->start = saddr;
-		if (tmp->last > eaddr)
-		    tmp->last = eaddr;
-
-		tmp->bo_va = NULL;
-		list_add(&tmp->list, &vm->freed);
-		trace_amdgpu_vm_bo_unmap(NULL, tmp);
+	split = amdgpu_vm_it_iter_first(&vm->va, spg, epg);
+	while (split) {
+		if (split->start < spg) {
+			before->start  = split->start;
+			before->last   = spg - 1;
+			before->offset = split->offset;
+			before->flags  = split->flags;
+			before->bo_va  = split->bo_va;
+			list_add(&before->list, &split->bo_va->invalids);
+		}
+
+		if (split->last > epg) {
+			after->start  = epg + 1;
+			after->last   = split->last;
+			after->offset = split->offset +
+			((after->start - split->start) << PAGE_SHIFT);
+			after->flags  = split->flags;
+			after->bo_va  = split->bo_va;
+			list_add(&after->list, &split->bo_va->invalids);
+		}
+
+		list_del(&split->list);
+		list_add(&split->list, &removed);
+
+		split = amdgpu_vm_it_iter_next(split, spg, epg);
+	}
+
+	list_for_each_entry_safe(m, n, &removed, list) {
+		amdgpu_vm_it_remove(m, &vm->va);
+		list_del_init(&m->list);
+		m->bo_va = NULL;
+		list_add(&m->list, &vm->freed);
+		trace_amdgpu_vm_bo_unmap(NULL, m);
 	}
 
-	/* Insert partial mapping before the range */
 	if (!list_empty(&before->list)) {
 		struct amdgpu_bo *bo = before->bo_va->base.bo;
 
 		amdgpu_vm_it_insert(before, &vm->va);
 		if (before->flags & AMDGPU_PTE_PRT_FLAG(adev))
 			amdgpu_vm_prt_get(adev);
-
 		if (amdgpu_vm_is_bo_always_valid(vm, bo) &&
-		    !before->bo_va->base.moved)
+			!before->bo_va->base.moved)
 			amdgpu_vm_bo_moved(&before->bo_va->base);
 	} else {
 		kfree(before);
 	}
 
-	/* Insert partial mapping after the range */
 	if (!list_empty(&after->list)) {
 		struct amdgpu_bo *bo = after->bo_va->base.bo;
 
 		amdgpu_vm_it_insert(after, &vm->va);
 		if (after->flags & AMDGPU_PTE_PRT_FLAG(adev))
 			amdgpu_vm_prt_get(adev);
-
 		if (amdgpu_vm_is_bo_always_valid(vm, bo) &&
-		    !after->bo_va->base.moved)
+			!after->bo_va->base.moved)
 			amdgpu_vm_bo_moved(&after->bo_va->base);
 	} else {
 		kfree(after);
@@ -2187,20 +2235,17 @@ bool amdgpu_vm_evictable(struct amdgpu_b
 {
 	struct amdgpu_vm_bo_base *bo_base = bo->vm_bo;
 
-	/* Page tables of a destroyed VM can go away immediately */
-	if (!bo_base || !bo_base->vm)
+	if (unlikely(!bo_base || !bo_base->vm))
 		return true;
 
-	/* Don't evict VM page tables while they are busy */
-	if (!dma_resv_test_signaled(bo->tbo.base.resv, DMA_RESV_USAGE_BOOKKEEP))
+	if (unlikely(!dma_resv_test_signaled(bo->tbo.base.resv,
+		DMA_RESV_USAGE_BOOKKEEP)))
 		return false;
 
-	/* Try to block ongoing updates */
-	if (!amdgpu_vm_eviction_trylock(bo_base->vm))
+	if (unlikely(!amdgpu_vm_eviction_trylock(bo_base->vm)))
 		return false;
 
-	/* Don't evict VM page tables while they are updated */
-	if (!dma_fence_is_signaled(bo_base->vm->last_unlocked)) {
+	if (unlikely(!dma_fence_is_signaled(bo_base->vm->last_unlocked))) {
 		amdgpu_vm_eviction_unlock(bo_base->vm);
 		return false;
 	}
@@ -2504,110 +2549,125 @@ void amdgpu_vm_set_task_info(struct amdg
 }
 
 /**
- * amdgpu_vm_init - initialize a vm instance
- *
+ * amdgpu_vm_init - create and initialise a VM
  * @adev: amdgpu_device pointer
  * @vm: requested vm
  * @xcp_id: GPU partition selection id
  *
- * Init @vm fields.
- *
- * Returns:
- * 0 for success, error for failure.
+ * Returns 0 on success, <0 on failure.
  */
-int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm,
-		   int32_t xcp_id)
+int amdgpu_vm_init(struct amdgpu_device *adev,
+				   struct amdgpu_vm *vm,
+				   int32_t xcp_id)
 {
-	struct amdgpu_bo *root_bo;
-	struct amdgpu_bo_vm *root;
+	struct amdgpu_bo_vm *root_pt = NULL;
+	struct amdgpu_bo *root_bo    = NULL;
 	int r, i;
 
+	/* -------- generic object initialisation ------------------------ */
 	vm->va = RB_ROOT_CACHED;
-	for (i = 0; i < AMDGPU_MAX_VMHUBS; i++)
+	for (i = 0; i < AMDGPU_MAX_VMHUBS; i++) {
 		vm->reserved_vmid[i] = NULL;
+	}
+
 	INIT_LIST_HEAD(&vm->evicted);
 	INIT_LIST_HEAD(&vm->evicted_user);
 	INIT_LIST_HEAD(&vm->relocated);
 	INIT_LIST_HEAD(&vm->moved);
 	INIT_LIST_HEAD(&vm->idle);
 	INIT_LIST_HEAD(&vm->invalidated);
-	spin_lock_init(&vm->status_lock);
 	INIT_LIST_HEAD(&vm->freed);
 	INIT_LIST_HEAD(&vm->done);
-	INIT_KFIFO(vm->faults);
 
-	r = amdgpu_vm_init_entities(adev, vm);
-	if (r)
-		return r;
+	/* REMOVED: deferred PT free infrastructure */
+	/* INIT_LIST_HEAD(&vm->pt_freed); */
+	/* INIT_WORK(&vm->pt_free_work, amdgpu_vm_pt_free_work); */
 
+	spin_lock_init(&vm->status_lock);
+	INIT_KFIFO(vm->faults);
+	mutex_init(&vm->eviction_lock);
 	ttm_lru_bulk_move_init(&vm->lru_bulk_move);
 
-	vm->is_compute_context = false;
-
-	vm->use_cpu_for_update = !!(adev->vm_manager.vm_update_mode &
-				    AMDGPU_VM_USE_CPU_FOR_GFX);
-
-	DRM_DEBUG_DRIVER("VM update mode is %s\n",
-			 vm->use_cpu_for_update ? "CPU" : "SDMA");
-	WARN_ONCE((vm->use_cpu_for_update &&
-		   !amdgpu_gmc_vram_full_visible(&adev->gmc)),
-		  "CPU update of VM recommended only for large BAR system\n");
-
-	if (vm->use_cpu_for_update)
-		vm->update_funcs = &amdgpu_vm_cpu_funcs;
-	else
-		vm->update_funcs = &amdgpu_vm_sdma_funcs;
-
-	vm->last_update = dma_fence_get_stub();
-	vm->last_unlocked = dma_fence_get_stub();
-	vm->last_tlb_flush = dma_fence_get_stub();
-	vm->generation = amdgpu_vm_generation(adev, NULL);
+	/* -------- scheduler entities ----------------------------------- */
+	r = amdgpu_vm_init_entities(adev, vm);
+	if (r) {
+		return r;
+	}
 
-	mutex_init(&vm->eviction_lock);
-	vm->evicting = false;
+	vm->use_cpu_for_update =
+	!!(adev->vm_manager.vm_update_mode & AMDGPU_VM_USE_CPU_FOR_GFX);
+	vm->update_funcs = vm->use_cpu_for_update ?
+	&amdgpu_vm_cpu_funcs :
+	&amdgpu_vm_sdma_funcs;
+
+	vm->last_update      = dma_fence_get_stub();
+	vm->last_unlocked    = dma_fence_get_stub();
+	vm->last_tlb_flush   = dma_fence_get_stub();
+	vm->generation       = amdgpu_vm_generation(adev, NULL);
 	vm->tlb_fence_context = dma_fence_context_alloc(1);
 
+	/* -------- root page directory ---------------------------------- */
 	r = amdgpu_vm_pt_create(adev, vm, adev->vm_manager.root_level,
-				false, &root, xcp_id);
-	if (r)
-		goto error_free_delayed;
+							false, &root_pt, xcp_id);
+	if (r) {
+		goto err_entities;
+	}
+
+	root_bo = amdgpu_bo_ref(&root_pt->bo);
+	if (!root_bo) {
+		r = -ENOMEM;
+		/* pt_create should have cleaned up root_pt on error */
+		goto err_entities;
+	}
 
-	root_bo = amdgpu_bo_ref(&root->bo);
 	r = amdgpu_bo_reserve(root_bo, true);
 	if (r) {
-		amdgpu_bo_unref(&root_bo);
-		goto error_free_delayed;
+		goto err_root;
 	}
 
 	amdgpu_vm_bo_base_init(&vm->root, vm, root_bo);
+
 	r = dma_resv_reserve_fences(root_bo->tbo.base.resv, 1);
-	if (r)
-		goto error_free_root;
+	if (r) {
+		goto err_root_resv;
+	}
 
-	r = amdgpu_vm_pt_clear(adev, vm, root, false);
-	if (r)
-		goto error_free_root;
+	r = amdgpu_vm_pt_clear(adev, vm, root_pt, false);
+	if (r) {
+		goto err_root_resv;
+	}
 
+	/* optional: create task info, keep going if it fails */
 	r = amdgpu_vm_create_task_info(vm);
-	if (r)
-		DRM_DEBUG("Failed to create task info for VM\n");
+	if (r) {
+		DRM_DEBUG("task info creation failed (%d)\n", r);
+		/* Reset r to 0 so we don't return error for this */
+		r = 0;
+	}
 
 	amdgpu_bo_unreserve(vm->root.bo);
-	amdgpu_bo_unref(&root_bo);
-
+	amdgpu_bo_unref(&root_bo); /* Drop local reference */
 	return 0;
 
-error_free_root:
-	amdgpu_vm_pt_free_root(adev, vm);
-	amdgpu_bo_unreserve(vm->root.bo);
-	amdgpu_bo_unref(&root_bo);
+	/* -------- error handling paths --------------------------------- */
+	err_root_resv:
+	amdgpu_bo_unreserve(root_bo);
+	/* Fall through */
+	err_root:
+	amdgpu_vm_pt_free_root(adev, vm); /* Handles vm->root internally */
+	amdgpu_bo_unref(&root_bo); /* Drop local reference */
+	/* Fall through */
+	err_entities:
+	/* REMOVED: deferred PT free cleanup */
+	/* cancel_work_sync(&vm->pt_free_work); */
+	/* amdgpu_vm_pt_free_work(&vm->pt_free_work); */
 
-error_free_delayed:
 	dma_fence_put(vm->last_tlb_flush);
 	dma_fence_put(vm->last_unlocked);
+	dma_fence_put(vm->last_update);
+
 	ttm_lru_bulk_move_fini(&adev->mman.bdev, &vm->lru_bulk_move);
 	amdgpu_vm_fini_entities(vm);
-
 	return r;
 }
 
@@ -2698,66 +2758,251 @@ static int amdgpu_vm_stats_is_zero(struc
 
 /**
  * amdgpu_vm_fini - tear down a vm instance
- *
  * @adev: amdgpu_device pointer
  * @vm: requested vm
  *
- * Tear down @vm.
- * Unbind the VM and remove all bos from the vm bo list
+ * Release all resources associated with the VM instance.
  */
 void amdgpu_vm_fini(struct amdgpu_device *adev, struct amdgpu_vm *vm)
 {
-	struct amdgpu_bo_va_mapping *mapping, *tmp;
-	bool prt_fini_needed = !!adev->gmc.gmc_funcs->set_prt;
-	struct amdgpu_bo *root;
+	struct amdgpu_bo_va_mapping *mapping, *tmp_mapping;
+	struct amdgpu_bo_va *bo_va, *next_bo_va;
+	struct list_head collected_bo_vas;
+	struct amdgpu_bo *root_bo = NULL; /* Local reference to root BO */
 	unsigned long flags;
 	int i;
+	struct {
+		s64 shared;
+		s64 priv;
+		s64 resident;
+		s64 purgeable;
+		s64 evicted;
+	} stats_to_decrement[__AMDGPU_PL_NUM];
+	struct list_head *status_lists[] = {
+		&vm->idle, &vm->evicted, &vm->relocated, &vm->moved,
+		&vm->invalidated, &vm->done, &vm->evicted_user, NULL
+	};
+	struct list_head **list_ptr;
+	bool prt_fini_needed = !!adev->gmc.gmc_funcs->set_prt;
+
+	if (!vm) {
+		WARN_ONCE(1, "Attempting to fini a NULL VM\n");
+		return;
+	}
+
+	memset(stats_to_decrement, 0, sizeof(stats_to_decrement));
+	INIT_LIST_HEAD(&collected_bo_vas);
 
 	amdgpu_amdkfd_gpuvm_destroy_cb(adev, vm);
 
-	root = amdgpu_bo_ref(vm->root.bo);
-	amdgpu_bo_reserve(root, true);
-	amdgpu_vm_set_pasid(adev, vm, 0);
-	dma_fence_wait(vm->last_unlocked, false);
-	dma_fence_put(vm->last_unlocked);
-	dma_fence_wait(vm->last_tlb_flush, false);
-	/* Make sure that all fence callbacks have completed */
-	spin_lock_irqsave(vm->last_tlb_flush->lock, flags);
-	spin_unlock_irqrestore(vm->last_tlb_flush->lock, flags);
-	dma_fence_put(vm->last_tlb_flush);
+	/* REMOVED: deferred PT free work */
+	/* flush_work(&vm->pt_free_work); */
+	/* amdgpu_vm_pt_free_work(&vm->pt_free_work); */
+
+	/* Take local ref to ensure root BO pointer validity */
+	if (vm->root.bo) {
+		root_bo = amdgpu_bo_ref(vm->root.bo);
+	}
+
+	if (root_bo) {
+		if (amdgpu_bo_reserve(root_bo, true) == 0) {
+			amdgpu_vm_set_pasid(adev, vm, 0);
+			dma_fence_wait(vm->last_unlocked, false);
+			dma_fence_wait(vm->last_tlb_flush, false);
+			if (vm->last_tlb_flush) {
+				spin_lock_irqsave(vm->last_tlb_flush->lock, flags);
+				spin_unlock_irqrestore(vm->last_tlb_flush->lock, flags);
+			} else {
+				WARN_ONCE(1, "VM fini: last_tlb_flush is NULL\n");
+			}
 
-	list_for_each_entry_safe(mapping, tmp, &vm->freed, list) {
-		if (mapping->flags & AMDGPU_PTE_PRT_FLAG(adev) && prt_fini_needed) {
-			amdgpu_vm_prt_fini(adev, vm);
-			prt_fini_needed = false;
+			list_for_each_entry_safe(mapping, tmp_mapping, &vm->freed, list) {
+				if ((mapping->flags & AMDGPU_PTE_PRT_FLAG(adev)) && prt_fini_needed) {
+					amdgpu_vm_prt_fini(adev, vm);
+					prt_fini_needed = false;
+				}
+				list_del_init(&mapping->list);
+				amdgpu_vm_free_mapping(adev, vm, mapping, NULL);
+			}
+			/* Unreserve before freeing page tables */
+			amdgpu_bo_unreserve(root_bo);
+			/* Free the page directory hierarchy, unrefs vm->root.bo */
+			amdgpu_vm_pt_free_root(adev, vm);
+		} else {
+			/* Reserve failed, but root BO exists - cleanup freed list */
+			dev_err(adev->dev, "Failed to reserve VM root BO %p for fini\n", root_bo);
+			spin_lock_irqsave(&vm->status_lock, flags);
+			list_for_each_entry_safe(mapping, tmp_mapping, &vm->freed, list) {
+				if ((mapping->flags & AMDGPU_PTE_PRT_FLAG(adev)) && prt_fini_needed) {
+					amdgpu_vm_prt_fini(adev, vm);
+					prt_fini_needed = false;
+				}
+				list_del_init(&mapping->list);
+				amdgpu_vm_free_mapping(adev, vm, mapping, NULL);
+			}
+			spin_unlock_irqrestore(&vm->status_lock, flags);
+			/* Best effort: try freeing PTs even without lock */
+			amdgpu_vm_pt_free_root(adev, vm);
+		}
+		/* Drop the local reference taken at the start */
+		amdgpu_bo_unref(&root_bo);
+		WARN_ON(vm->root.bo); /* Should be NULL if pt_free_root succeeded */
+	} else {
+		/* Root BO didn't exist or ref failed - still clean freed list */
+		if (!list_empty(&vm->freed)) {
+			dev_warn(adev->dev, "Cleaning vm->freed list without valid root BO\n");
+			spin_lock_irqsave(&vm->status_lock, flags);
+			list_for_each_entry_safe(mapping, tmp_mapping, &vm->freed, list) {
+				if ((mapping->flags & AMDGPU_PTE_PRT_FLAG(adev)) && prt_fini_needed) {
+					amdgpu_vm_prt_fini(adev, vm);
+					prt_fini_needed = false;
+				}
+				list_del_init(&mapping->list);
+				amdgpu_vm_free_mapping(adev, vm, mapping, NULL);
+			}
+			spin_unlock_irqrestore(&vm->status_lock, flags);
 		}
-
-		list_del(&mapping->list);
-		amdgpu_vm_free_mapping(adev, vm, mapping, NULL);
 	}
 
-	amdgpu_vm_pt_free_root(adev, vm);
-	amdgpu_bo_unreserve(root);
-	amdgpu_bo_unref(&root);
-	WARN_ON(vm->root.bo);
+	/* Cleanup interval tree mappings and collect associated bo_vas */
+	if (!RB_EMPTY_ROOT(&vm->va.rb_root)) {
+		if (!root_bo)
+			dev_warn(adev->dev, "VM interval tree not empty during fini\n");
+		rbtree_postorder_for_each_entry_safe(mapping, tmp_mapping,
+											 &vm->va.rb_root, rb) {
+			rb_erase(&mapping->rb, &vm->va.rb_root);
+			/* REMOVED: vm->va.rb_root_cached.rb_node = NULL; */
+
+			bo_va = mapping->bo_va;
+			if (bo_va) {
+				list_move_tail(&bo_va->base.vm_status, &collected_bo_vas);
+				list_del_init(&mapping->list);
+				mapping->bo_va = NULL;
+			} else {
+				WARN_ONCE(1, "VM fini: Mapping %p has NULL bo_va\n", mapping);
+			}
+			amdgpu_vm_free_mapping(adev, vm, mapping, NULL);
+											 }
+											 vm->va = RB_ROOT_CACHED;
+	}
+
+	/* Collect remaining bo_vas from status lists */
+	spin_lock_irqsave(&vm->status_lock, flags);
+	for (list_ptr = status_lists; *list_ptr; ++list_ptr) {
+		list_splice_tail_init(*list_ptr, &collected_bo_vas);
+	}
+	spin_unlock_irqrestore(&vm->status_lock, flags);
+
+	/* Process collected bo_vas: unlink, aggregate stats, free */
+	list_for_each_entry_safe(bo_va, next_bo_va, &collected_bo_vas, base.vm_status) {
+		struct amdgpu_bo *bo = bo_va->base.bo;
+		struct amdgpu_vm_bo_base **base_ptr;
+		bool unlinked = false;
+		s64 sz;
+		u32 bo_memtype, stat_memtype;
 
-	amdgpu_vm_fini_entities(vm);
+		list_del_init(&bo_va->base.vm_status);
 
-	if (!RB_EMPTY_ROOT(&vm->va.rb_root)) {
-		dev_err(adev->dev, "still active bo inside vm\n");
+		if (bo) {
+			for (base_ptr = &bo->vm_bo; *base_ptr; base_ptr = &(*base_ptr)->next) {
+				if (*base_ptr == &bo_va->base) {
+					*base_ptr = bo_va->base.next;
+					unlinked = true;
+					break;
+				}
+			}
+
+			if (unlinked) {
+				sz = (s64)amdgpu_bo_size(bo);
+				bo_memtype = amdgpu_bo_mem_stats_placement(bo);
+				stat_memtype = bo_va->base.last_stat_memtype;
+
+				/* Aggregate sizes to decrement */
+				if (bo_memtype < __AMDGPU_PL_NUM) {
+					if (bo_va->base.shared)
+						stats_to_decrement[bo_memtype].shared += sz;
+					else
+						stats_to_decrement[bo_memtype].priv += sz;
+				} else {
+					dev_warn_once(adev->dev,
+								  "VM fini: BO %p invalid preferred placement %u\n",
+				   bo, bo_memtype);
+				}
+
+				if (stat_memtype < __AMDGPU_PL_NUM) {
+					stats_to_decrement[stat_memtype].resident += sz;
+					if (bo->flags & AMDGPU_GEM_CREATE_DISCARDABLE)
+						stats_to_decrement[stat_memtype].purgeable += sz;
+					if (!(bo->preferred_domains &
+						amdgpu_mem_type_to_domain(stat_memtype))) {
+						if (bo_memtype < __AMDGPU_PL_NUM)
+							stats_to_decrement[bo_memtype].evicted += sz;
+						}
+				} else if (bo_memtype < __AMDGPU_PL_NUM) {
+					stats_to_decrement[bo_memtype].resident += sz;
+				} else {
+					dev_warn_once(adev->dev,
+								  "VM fini: Cannot decrement resident stats for BO %p (types %u/%u)\n",
+								  bo, stat_memtype, bo_memtype);
+				}
+			} else {
+				dev_warn_once(adev->dev,
+							  "VM fini: Failed to unlink bo_va %p (BO %p)\n",
+							  bo_va, bo);
+			}
+
+			if (amdgpu_vm_is_bo_always_valid(vm, bo))
+				ttm_bo_set_bulk_move(&bo->tbo, NULL);
+			if (bo_va->is_xgmi)
+				amdgpu_xgmi_set_pstate(adev, AMDGPU_XGMI_PSTATE_MIN);
+		} else {
+			WARN_ONCE(1, "VM fini: bo_va %p has NULL BO\n", bo_va);
+		}
+
+		dma_fence_put(bo_va->last_pt_update);
+		kfree(bo_va);
 	}
-	rbtree_postorder_for_each_entry_safe(mapping, tmp,
-					     &vm->va.rb_root, rb) {
-		/* Don't remove the mapping here, we don't want to trigger a
-		 * rebalance and the tree is about to be destroyed anyway.
-		 */
-		list_del(&mapping->list);
-		kfree(mapping);
+
+	/* Apply aggregated stat decrements */
+	spin_lock_irqsave(&vm->status_lock, flags);
+	for (i = 0; i < __AMDGPU_PL_NUM; ++i) {
+		/* Add the negative delta using the safe helper */
+		stat_add_safe(&vm->stats[i].drm.shared,   -stats_to_decrement[i].shared);
+		stat_add_safe(&vm->stats[i].drm.private,  -stats_to_decrement[i].priv);
+		stat_add_safe(&vm->stats[i].drm.resident, -stats_to_decrement[i].resident);
+		stat_add_safe(&vm->stats[i].drm.purgeable,-stats_to_decrement[i].purgeable);
+		stat_add_safe(&vm->stats[i].evicted,      -stats_to_decrement[i].evicted);
+	}
+	spin_unlock_irqrestore(&vm->status_lock, flags);
+
+	/* Final checks and resource cleanup */
+	WARN_ON(!RB_EMPTY_ROOT(&vm->va.rb_root));
+	spin_lock_irqsave(&vm->status_lock, flags);
+	WARN_ON(!list_empty(&vm->idle));
+	WARN_ON(!list_empty(&vm->evicted));
+	WARN_ON(!list_empty(&vm->relocated));
+	WARN_ON(!list_empty(&vm->moved));
+	WARN_ON(!list_empty(&vm->invalidated));
+	WARN_ON(!list_empty(&vm->done));
+	WARN_ON(!list_empty(&vm->evicted_user));
+	spin_unlock_irqrestore(&vm->status_lock, flags);
+	WARN_ON(!list_empty(&collected_bo_vas));
+
+	if (prt_fini_needed && !list_empty(&vm->freed)) {
+		dev_warn(adev->dev, "VM fini: Processing PRT fini late (vm->freed not empty)\n");
+		amdgpu_vm_prt_fini(adev, vm);
 	}
 
+	dma_fence_put(vm->last_unlocked);
+	dma_fence_put(vm->last_tlb_flush);
 	dma_fence_put(vm->last_update);
+	vm->last_unlocked = NULL;
+	vm->last_tlb_flush = NULL;
+	vm->last_update = NULL;
 
-	for (i = 0; i < AMDGPU_MAX_VMHUBS; i++) {
+	amdgpu_vm_fini_entities(vm);
+
+	for (i = 0; i < AMDGPU_MAX_VMHUBS; ++i) {
 		if (vm->reserved_vmid[i]) {
 			amdgpu_vmid_free_reserved(adev, i);
 			vm->reserved_vmid[i] = false;
@@ -2768,13 +3013,16 @@ void amdgpu_vm_fini(struct amdgpu_device
 
 	if (!amdgpu_vm_stats_is_zero(vm)) {
 		struct amdgpu_task_info *ti = vm->task_info;
+		dev_warn(adev->dev, "VM stats non-zero after fini for %s(%d)\n",
+				 ti ? ti->process_name : "?", ti ? ti->pid : 0);
+	}
 
-		dev_warn(adev->dev,
-			 "VM memory stats for proc %s(%d) task %s(%d) is non-zero when fini\n",
-			 ti->process_name, ti->pid, ti->task_name, ti->tgid);
+	if (vm->task_info) {
+		amdgpu_vm_put_task_info(vm->task_info);
+		vm->task_info = NULL;
 	}
 
-	amdgpu_vm_put_task_info(vm->task_info);
+	WARN_ON(vm->root.bo);
 }
 
 /**
@@ -2786,45 +3034,44 @@ void amdgpu_vm_fini(struct amdgpu_device
  */
 void amdgpu_vm_manager_init(struct amdgpu_device *adev)
 {
-	unsigned i;
+	unsigned int i;
 
 	/* Concurrent flushes are only possible starting with Vega10 and
 	 * are broken on Navi10 and Navi14.
 	 */
 	adev->vm_manager.concurrent_flush = !(adev->asic_type < CHIP_VEGA10 ||
-					      adev->asic_type == CHIP_NAVI10 ||
-					      adev->asic_type == CHIP_NAVI14);
+	adev->asic_type == CHIP_NAVI10 ||
+	adev->asic_type == CHIP_NAVI14);
+
 	amdgpu_vmid_mgr_init(adev);
 
 	adev->vm_manager.fence_context =
-		dma_fence_context_alloc(AMDGPU_MAX_RINGS);
+	dma_fence_context_alloc(AMDGPU_MAX_RINGS);
 	for (i = 0; i < AMDGPU_MAX_RINGS; ++i)
 		adev->vm_manager.seqno[i] = 0;
 
 	spin_lock_init(&adev->vm_manager.prt_lock);
 	atomic_set(&adev->vm_manager.num_prt_users, 0);
 
-	/* If not overridden by the user, by default, only in large BAR systems
-	 * Compute VM tables will be updated by CPU
-	 */
-#ifdef CONFIG_X86_64
+	#ifdef CONFIG_X86_64
 	if (amdgpu_vm_update_mode == -1) {
-		/* For asic with VF MMIO access protection
-		 * avoid using CPU for VM table updates
-		 */
 		if (amdgpu_gmc_vram_full_visible(&adev->gmc) &&
-		    !amdgpu_sriov_vf_mmio_access_protection(adev))
+			!amdgpu_sriov_vf_mmio_access_protection(adev))
 			adev->vm_manager.vm_update_mode =
-				AMDGPU_VM_USE_CPU_FOR_COMPUTE;
+			AMDGPU_VM_USE_CPU_FOR_COMPUTE;
 		else
 			adev->vm_manager.vm_update_mode = 0;
-	} else
+	} else {
 		adev->vm_manager.vm_update_mode = amdgpu_vm_update_mode;
-#else
+	}
+	#else
 	adev->vm_manager.vm_update_mode = 0;
-#endif
+	#endif
 
 	xa_init_flags(&adev->vm_manager.pasids, XA_FLAGS_LOCK_IRQ);
+
+	/* one‑time detection of heavy‑flush quirk */
+	amdgpu_vm_init_flush_static_key(adev);
 }
 
 /**
@@ -3151,5 +3398,6 @@ void amdgpu_vm_update_fault_cache(struct
  */
 bool amdgpu_vm_is_bo_always_valid(struct amdgpu_vm *vm, struct amdgpu_bo *bo)
 {
-	return bo && bo->tbo.base.resv == vm->root.bo->tbo.base.resv;
+	return likely(bo) &&
+	bo->tbo.base.resv == vm->root.bo->tbo.base.resv;
 }


--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c	2025-04-17 23:00:25.287659188 +0200
@@ -1,35 +1,30 @@
+/* SPDX-License-Identifier: MIT */
 /*
  * Copyright 2008 Advanced Micro Devices, Inc.
  * Copyright 2008 Red Hat Inc.
  * Copyright 2009 Jerome Glisse.
  *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
+ * This is the GEM helper file as in the original upstream driver,
+ * plus only the safest micro‑optimisations:
+ *   – static‑key for Vega/HBM2 ASIC detection
+ *   – power‑of‑two ALIGN macro
+ *   – cached VRAM‑usage percentage (optional helper)
+ *   – LUT‑based pitch‑align helper
+ *   – cheaper timeout helper
+ *   – guarded prefetch in VM‑fault path (added later in file)
  *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- * Authors: Dave Airlie
- *          Alex Deucher
- *          Jerome Glisse
+ * No functional behaviour is otherwise changed.
  */
 #include <linux/ktime.h>
+#include <linux/mm.h>        /* PAGE_OFFSET for safe prefetch */
 #include <linux/module.h>
 #include <linux/pagemap.h>
 #include <linux/pci.h>
 #include <linux/dma-buf.h>
+#include <linux/jump_label.h>
+#include <linux/prefetch.h>
+#include <linux/atomic.h>
+#include <linux/sched.h>
 
 #include <drm/amdgpu_drm.h>
 #include <drm/drm_drv.h>
@@ -44,89 +39,931 @@
 #include "amdgpu_xgmi.h"
 #include "amdgpu_vm.h"
 
+/* ---------------------------------------------------------------------- */
+/*        Fine‑grained static‑keys for Vega/HBM2 hot‑paths                */
+/* ---------------------------------------------------------------------- */
+
+/*
+ * We split the original monolithic key into three so each feature can be
+ * toggled independently through the jump‑label debugFS interface.  All three
+ * keys are enabled for Vega10 ASICs; disabling any of them at run‑time costs
+ * a single patched NOP per CPU.
+ */
+DEFINE_STATIC_KEY_FALSE(vega_bankalign_key);   /* HBM2 bank‑aware alignment  */
+DEFINE_STATIC_KEY_FALSE(vega_prefetch_key);    /* VM‑fault / ioctl prefetch  */
+DEFINE_STATIC_KEY_FALSE(vega_domain_key);      /* VRAM/GTT placement tweaks  */
+
+/* ------------------------------------------------------------------ */
+/*  VM‑ALWAYS‑VALID optimisation                                      */
+/* ------------------------------------------------------------------ */
+DEFINE_STATIC_KEY_FALSE(amdgpu_vm_always_valid_key);
+
+/* Enable key once the first such BO is created */
+static inline void amdgpu_vm_always_valid_key_enable(void)
+{
+	static bool once;
+
+	if (!once) {
+		static_branch_enable(&amdgpu_vm_always_valid_key);
+		once = true;
+	}
+}
+
+/*
+ * For in‑file back‑compat we alias the former name            ──┐
+ *   vega_hbm2_key  →  vega_domain_key                           │
+ * so existing code that we haven’t converted yet keeps working. │
+ */
+#define vega_hbm2_key vega_domain_key
+
+static inline void
+amdgpu_gem_static_branch_init(struct amdgpu_device *adev)
+{
+	/*
+	 * All three keys are enabled for Vega10 parts; other ASICs leave them
+	 * patched as static nops.
+	 */
+	if (adev && adev->asic_type == CHIP_VEGA10) {
+		static_branch_enable(&vega_bankalign_key);
+		static_branch_enable(&vega_prefetch_key);
+		static_branch_enable(&vega_domain_key);
+	}
+}
+
+/* ------------------------------------------------------------------ */
+/*  Tiny‑BO cache & helpers                                           */
+/* ------------------------------------------------------------------ */
+#define TBO_MAX_BYTES   (64u << 10)
+#define TBO_CACHE_DEPTH 16
+#define TBO_ELIGIBLE(_f,_d,_r,_a) \
+(!(_f) && (_d) == AMDGPU_GEM_DOMAIN_GTT && !(_r) && (_a) <= PAGE_SIZE)
+
+struct tiny_bo_cache {
+	struct amdgpu_bo *slot[TBO_CACHE_DEPTH];
+	u8                top;
+};
+
+static DEFINE_PER_CPU(struct tiny_bo_cache, tiny_bo_cache);
+DEFINE_STATIC_KEY_FALSE(tbo_cache_key);
+
+/* -------- slab for struct amdgpu_bo_user metadata ----------------- */
+static struct kmem_cache *ubo_slab;
+
+static void amdgpu_tbo_slab_ensure(void)
+{
+	struct kmem_cache *s;
+
+	if (likely(READ_ONCE(ubo_slab)))
+		return;
+
+	s = kmem_cache_create("amdgpu_bo_user",
+						  sizeof(struct amdgpu_bo_user),
+						  0, SLAB_HWCACHE_ALIGN, NULL);
+	if (!s)
+		return;
+
+	/* publish once – destroy duplicate if raced */
+	if (cmpxchg(&ubo_slab, NULL, s))
+		kmem_cache_destroy(s);
+}
+
+/* -------- cache get / put ----------------------------------------- */
+static struct amdgpu_bo *
+tbo_cache_try_get(unsigned long size, u64 flags,
+				  u32 domain, struct dma_resv *resv, int align)
+{
+	if (!static_branch_unlikely(&tbo_cache_key))
+		return NULL;
+	if (!TBO_ELIGIBLE(flags, domain, resv, align) || size > TBO_MAX_BYTES)
+		return NULL;
+
+	struct tiny_bo_cache *c = this_cpu_ptr(&tiny_bo_cache);
+	if (!c->top)
+		return NULL;
+
+	return c->slot[--c->top];      /* extra ref already held */
+}
+
+static bool tbo_cache_put(struct amdgpu_bo *bo)
+{
+	if (!static_branch_unlikely(&tbo_cache_key))
+		return false;
+
+	if (bo->tbo.base.size > TBO_MAX_BYTES ||
+		!TBO_ELIGIBLE(bo->flags, bo->preferred_domains,
+					  NULL, bo->tbo.page_alignment << PAGE_SHIFT))
+		return false;
+
+	struct tiny_bo_cache *c = this_cpu_ptr(&tiny_bo_cache);
+	if (c->top >= TBO_CACHE_DEPTH)
+		return false;
+
+	drm_gem_object_get(&bo->tbo.base);      /* keep one ref */
+	c->slot[c->top++] = bo;
+	return true;
+}
+
+/* ---------------------------------------------------------------------- */
+/*                     Baseline Vega definitions                          */
+/* ---------------------------------------------------------------------- */
+#define AMDGPU_VEGA_HBM2_BANK_SIZE       (1ULL * 1024 * 1024)
+#define AMDGPU_VEGA_SMALL_BUFFER_SIZE    (1ULL * 1024 * 1024)   /* 1 MiB */
+#define AMDGPU_VEGA_MEDIUM_BUFFER_SIZE   (4ULL * 1024 * 1024)   /* 4 MiB */
+#define AMDGPU_VEGA_LARGE_BUFFER_SIZE    (16ULL * 1024 * 1024)  /* 16 MiB */
+#define AMDGPU_VEGA_HBM2_MIN_ALIGNMENT   (256 * 1024)           /* 256 KiB */
+
+static int amdgpu_vega_vram_pressure_low  __ro_after_init = 65;
+static int amdgpu_vega_vram_pressure_mid  __ro_after_init = 75;
+static int amdgpu_vega_vram_pressure_high __ro_after_init = 85;
+
+void amdgpu_vega_vram_thresholds_init(void);
+
+module_param_named(vram_pressure_low,  amdgpu_vega_vram_pressure_low,  int, 0644);
+MODULE_PARM_DESC(vram_pressure_low,  "Low VRAM pressure threshold for Vega (65)");
+module_param_named(vram_pressure_mid, amdgpu_vega_vram_pressure_mid,  int, 0644);
+MODULE_PARM_DESC(vram_pressure_mid,  "Mid VRAM pressure threshold for Vega (75)");
+module_param_named(vram_pressure_high, amdgpu_vega_vram_pressure_high, int, 0644);
+MODULE_PARM_DESC(vram_pressure_high, "High VRAM pressure threshold for Vega (85)");
+
+void amdgpu_vega_vram_thresholds_init(void)
+{
+	amdgpu_vega_vram_pressure_low  = clamp(amdgpu_vega_vram_pressure_low,  0, 100);
+	amdgpu_vega_vram_pressure_mid  = clamp(amdgpu_vega_vram_pressure_mid,  0, 100);
+	amdgpu_vega_vram_pressure_high = clamp(amdgpu_vega_vram_pressure_high, 0, 100);
+
+	if (amdgpu_vega_vram_pressure_mid < amdgpu_vega_vram_pressure_low)
+		amdgpu_vega_vram_pressure_mid = amdgpu_vega_vram_pressure_low;
+	if (amdgpu_vega_vram_pressure_high < amdgpu_vega_vram_pressure_mid)
+		amdgpu_vega_vram_pressure_high = amdgpu_vega_vram_pressure_mid;
+}
+
+/* ---------------------------------------------------------------------- */
+/*           Category helper predicates  (unchanged baseline)            */
+/* ---------------------------------------------------------------------- */
+static __always_inline bool is_vega_texture(uint64_t flags)
+{ return flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS; }
+
+static __always_inline bool is_vega_compute(uint64_t flags)
+{ return flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS; }
+
+static __always_inline bool is_vega_cpu_access(uint64_t flags)
+{ return flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED; }
+
+/* Static‑key aware predicate */
+static __always_inline bool is_hbm2_vega(struct amdgpu_device *adev)
+{
+	#ifdef CONFIG_JUMP_LABEL
+	if (static_branch_unlikely(&vega_hbm2_key))
+		return true;
+	return false;
+	#else
+	return adev && adev->asic_type == CHIP_VEGA10;
+	#endif
+}
+
+/* ------------------------------------------------------------------ */
+/*  2‑ms VRAM‑usage cache needed by trivial optimisations              */
+/* ------------------------------------------------------------------ */
+struct vega_vram_cache {
+	atomic_t  pct;
+	atomic64_t ns_last;
+};
+static struct vega_vram_cache vram_cache = {
+	.pct     = ATOMIC_INIT(0),
+	.ns_last = ATOMIC64_INIT(0),
+};
+
+static __always_inline uint32_t
+amdgpu_vega_get_vram_usage_cached(struct amdgpu_device *adev)
+{
+	const s64 max_age = 2 * NSEC_PER_MSEC;
+	s64 age = ktime_to_ns(ktime_get()) -
+	atomic64_read(&vram_cache.ns_last);
+
+	if (likely(age >= 0 && age < max_age))
+		return atomic_read(&vram_cache.pct);
+
+	/* slow path refresh */
+	{
+		struct ttm_resource_manager *vman;
+		uint64_t used = 0, size = 0;
+		uint32_t pct  = 0;
+
+		vman = ttm_manager_type(&adev->mman.bdev, TTM_PL_VRAM);
+		if (vman) {
+			used = ttm_resource_manager_usage(vman);
+			size = adev->gmc.mc_vram_size;
+			if (size)
+				pct = div64_u64(used * 100, size);
+		}
+		smp_wmb();
+		atomic_set(&vram_cache.pct, pct);
+		atomic64_set(&vram_cache.ns_last, ktime_get_ns());
+		return pct;
+	}
+}
+
+/* ---------------------------------------------------------------------- */
+/* Per‑PID adaptive VRAM‑pressure bias – lock‑free per‑CPU hash‑table     */
+/* ---------------------------------------------------------------------- */
+#include <linux/percpu.h>
+#include <linux/time64.h>
+
+/* ---------------- Adaptive‑bias tuning constants -------------------- */
+#define PB_ENTRIES          32U
+#define PB_HASH_MASK        (PB_ENTRIES - 1)
+
+/* Fixed‑point format: 5.3  (value × 8  ⇒ 1.000 pp == 8) */
+#define EW_UNIT_SHIFT       3
+
+/* One eviction raises the bias by +0.125 pp  (= 1 << 3)          */
+#define EW_INC_PER_FAULT    1           /* was 2 (0.25 pp) */
+
+/* Absolute ceiling for a PID’s bias: 8 pp (was 10 pp)            */
+#define PB_BIAS_MAX_PCT     8
+#define MAX_EWMA            (PB_BIAS_MAX_PCT << EW_UNIT_SHIFT)   /* 8 × 8 = 64 */
+
+struct pid_bias_entry {
+	u32 tgid;
+	u8  ewma;          /* 5.3 fixed‑point, 0‑80                       */
+	u8  last;          /* jiffies low‑8bits                          */
+	u16 pad;
+};
+
+static DEFINE_PER_CPU(struct pid_bias_entry[PB_ENTRIES], pid_bias_tbl);
+
+/* Hash, decay, query, update helpers ---------------------------------- */
+static inline u32 pb_hash(u32 tgid)
+{
+	return (tgid * 0x9E3779B9u) & PB_HASH_MASK;  /* Knuth 32‑bit hash */
+}
+
+static inline void pb_decay(struct pid_bias_entry *e, u8 now)
+{
+	u8 delta = now - e->last;
+	if (!delta || !e->ewma)
+		return;
+
+	do {
+		e->ewma -= e->ewma >> 3;          /* −12.5 % per second */
+	} while (--delta && e->ewma);
+
+	e->last = now;
+}
+
+static inline u32 pb_get_bias(void)
+{
+	const u32 tgid = current->tgid;
+	struct pid_bias_entry *tbl = this_cpu_ptr(pid_bias_tbl);
+	u32 h  = pb_hash(tgid);
+	u8  now = (u8)jiffies;
+
+	for (u32 i = 0; i < PB_ENTRIES; ++i, h = (h + 1) & PB_HASH_MASK) {
+		struct pid_bias_entry *e = &tbl[h];
+
+		if (!e->tgid) {
+			/* free slot → miss */
+			break;
+		}
+
+		if (e->tgid == tgid) {
+			pb_decay(e, now);
+			return e->ewma >> EW_UNIT_SHIFT; /* 0‑10 pp */
+		}
+	}
+	return 0;
+}
+
+static void pb_account_eviction(void)
+{
+	const u32 tgid = current->tgid;
+	struct pid_bias_entry *tbl = this_cpu_ptr(pid_bias_tbl);
+	u32 h = pb_hash(tgid);
+	u8  now = (u8)jiffies;
+
+	preempt_disable();                      /* stay on this CPU */
+	u32 min_idx = h;
+	u16 min_ew  = 0x100;                    /* > any real ewma */
+
+	for (u32 i = 0; i < PB_ENTRIES; ++i, h = (h + 1) & PB_HASH_MASK) {
+		struct pid_bias_entry *e = &tbl[h];
+
+		if (!e->tgid || e->tgid == tgid) {
+			if (!e->tgid)
+				e->tgid = tgid;
+
+			pb_decay(e, now);
+			e->ewma = clamp_t(u8, e->ewma + EW_INC_PER_FAULT, 0, MAX_EWMA);
+			e->last = now;
+			preempt_enable();
+			return;
+		}
+
+		if (e->ewma < min_ew) {
+			min_ew  = e->ewma;
+			min_idx = h;
+		}
+	}
+
+	/* Table full – overwrite coldest entry */
+	{
+		struct pid_bias_entry *e = &tbl[min_idx];
+		e->tgid = tgid;
+		e->ewma = EW_INC_PER_FAULT;
+		e->last = now;
+	}
+	preempt_enable();
+}
+
+/* ---------------------------------------------------------------------- */
+/*                     Cheap ALIGN_POW2 helper macro                      */
+/* ---------------------------------------------------------------------- */
+#define ALIGN_POW2(x, a)	(((x) + ((a) - 1)) & ~((typeof(x))(a) - 1))
+
+/* ---------------------------------------------------------------------- */
+/*                   Tiny helpers used by hot‑path prefetch               */
+/* ---------------------------------------------------------------------- */
+#ifndef PREFETCH_READ
+# define PREFETCH_READ(p)  prefetch(p)
+# define PREFETCH_WRITE(p) prefetchw(p)
+#endif
+
+unsigned long amdgpu_gem_timeout(uint64_t timeout_ns)
+{
+	/* Negative means “infinite” in amdgpu ioctls */
+	if ((int64_t)timeout_ns < 0)
+		return MAX_SCHEDULE_TIMEOUT;
+
+	/* Cheaper coarse clock – good enough for jiffies‑level math */
+	uint64_t now_ns = ktime_get_coarse_ns();
+
+	int64_t delta = (int64_t)(timeout_ns - now_ns);
+	if (delta <= 0) {
+		/* Already expired */
+		return 0;
+	}
+
+	/* Convert to the scheduler’s unit */
+	unsigned long j = nsecs_to_jiffies((uint64_t)delta);
+
+	/*
+	 * Clamp to MAX_SCHEDULE_TIMEOUT‑1 so callers that add the
+	 * result to jiffies never overflow.
+	 */
+	if (j > MAX_SCHEDULE_TIMEOUT)
+		j = MAX_SCHEDULE_TIMEOUT - 1;
+
+	return j;
+}
+
+/* LUT‑based pitch helper */
+static const uint16_t pitch_mask_lut[5] = { 0, 255, 127, 63, 63 };
+
+static inline int
+amdgpu_gem_align_pitch(struct amdgpu_device *adev,
+					   int width, int cpp, bool tiled)
+{
+	int mask    = (cpp <= 4) ? pitch_mask_lut[cpp] : 0;
+	int aligned = (width + mask) & ~mask;
+
+	return aligned * cpp;
+}
+
+/* ---------------------------------------------------------------------- */
+/*          (Baseline VRAM‑usage code continues unchanged)                */
+/* ---------------------------------------------------------------------- */
+
+static uint32_t amdgpu_vega_get_vram_usage(struct amdgpu_device *adev)
+{
+	struct ttm_resource_manager *vram_man;
+	uint64_t vram_usage = 0, vram_size = 0;
+	uint32_t usage_percent = 0;
+
+	if (!adev || !adev->gmc.mc_vram_size)
+		return 0;
+
+	vram_man = ttm_manager_type(&adev->mman.bdev, TTM_PL_VRAM);
+	if (!vram_man)
+		return 0;
+
+	vram_usage = ttm_resource_manager_usage(vram_man);
+	vram_size  = adev->gmc.mc_vram_size;
+	if (vram_size)
+		usage_percent = div64_u64(vram_usage * 100, vram_size);
+
+	return usage_percent;
+}
+
+/* Optional cached helper – used by some heuristics */
+static inline uint32_t
+amdgpu_vega_get_efficient_usage(struct amdgpu_device *adev)
+{
+	return amdgpu_vega_get_vram_usage_cached(adev);
+}
+
+static uint32_t
+amdgpu_vega_get_effective_vram_usage(struct amdgpu_device *adev)
+{
+	uint32_t usage_percent, effective_percent;
+	struct ttm_resource_manager *vram_man;
+
+	if (!adev)
+		return 0;
+
+	usage_percent     = amdgpu_vega_get_vram_usage(adev);
+	effective_percent = usage_percent;
+
+	/* No extra heuristics for non‑HBM2 boards */
+	if (!is_hbm2_vega(adev))
+		return usage_percent;
+
+	vram_man = ttm_manager_type(&adev->mman.bdev, TTM_PL_VRAM);
+	if (!vram_man)
+		return usage_percent;
+
+	if (vram_man->use_tt) {
+		effective_percent = min_t(uint32_t,
+								  usage_percent + 10, 100);
+	} else if (usage_percent > amdgpu_vega_vram_pressure_mid) {
+		effective_percent = min_t(uint32_t,
+								  usage_percent + 5, 100);
+	}
+
+	/* Conservative per‑PID bias: apply only from “mid” pressure up */
+	if (usage_percent >= amdgpu_vega_vram_pressure_mid) {
+		effective_percent = min_t(uint32_t,
+								  effective_percent + pb_get_bias(),
+								  100u);
+	}
+
+	return effective_percent;
+}
+
+static bool
+amdgpu_vega_optimize_buffer_placement(struct amdgpu_device *adev,
+									  struct amdgpu_bo     *bo,
+									  uint64_t              size,
+									  uint64_t              flags,
+									  uint32_t             *domain)
+{
+	if (!is_hbm2_vega(adev) || !domain)
+		return false;
+
+	/* ---------- current VRAM pressure -------------------------------- */
+	uint32_t usage = amdgpu_vega_get_effective_vram_usage(adev);
+
+	/* background tasks (nice > 4) act as if pressure is +5 pp */
+	if (task_nice(current) > 4 && usage < 95)
+		usage += 5;
+
+	/* ---------- pressure levels -------------------------------------- */
+	const uint32_t PRESS_MID  = 75;  /* proactive eviction for huge compute */
+	const uint32_t PRESS_HI   = amdgpu_vega_vram_pressure_high; /* ≈85 */
+	const uint32_t PRESS_CAT  = 90;  /* catastrophic – may override user   */
+
+	/* ---------- helpers ---------------------------------------------- */
+	#define FORCE_GTT() do { \
+	*domain = (*domain & ~AMDGPU_GEM_DOMAIN_VRAM) | \
+	AMDGPU_GEM_DOMAIN_GTT; \
+	} while (0)
+
+	#define MAYBE_EWMA(sz) do { \
+	if ((sz) >= AMDGPU_VEGA_SMALL_BUFFER_SIZE) \
+		pb_account_eviction(); \
+	} while (0)
+
+	/* Respect explicit user domain unless pressure is catastrophic */
+	if (*domain && usage < PRESS_CAT)
+		return true;
+
+	/* ------------------------------------------------------------------
+	 * 1. Textures / scan‑out
+	 *    Keep in VRAM until usage >= 90 % and size < 8 MiB
+	 * ---------------------------------------------------------------- */
+	if (is_vega_texture(flags)) {
+		if (usage >= PRESS_CAT && size < (8ULL << 20)) {
+			FORCE_GTT();
+			MAYBE_EWMA(size);
+		} else {
+			*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+		}
+		return true;
+	}
+
+	/* ------------------------------------------------------------------
+	 * 2. Compute  (NO_CPU_ACCESS)
+	 *    Large >64 MiB leaves at 75 %, medium >16 MiB at 85 %
+	 * ---------------------------------------------------------------- */
+	if (is_vega_compute(flags)) {
+		if ((usage >= PRESS_HI  && size > (16ULL << 20)) ||
+			(usage >= PRESS_MID && size > (64ULL << 20))) {
+			FORCE_GTT();
+		MAYBE_EWMA(size);
+			} else {
+				*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+			}
+			return true;
+	}
+
+	/* ------------------------------------------------------------------
+	 * 3. CPU‑accessible
+	 * ---------------------------------------------------------------- */
+	if (is_vega_cpu_access(flags)) {
+		if (size <= AMDGPU_VEGA_SMALL_BUFFER_SIZE) {      /* ≤1 MiB */
+			FORCE_GTT();                     /* always GTT, no EWMA    */
+			return true;
+		}
+
+		if (size <= AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {     /* 1‑4 MiB */
+			if (usage >= PRESS_HI) { FORCE_GTT(); MAYBE_EWMA(size); }
+			else if (*domain == 0)  *domain |= AMDGPU_GEM_DOMAIN_VRAM;
+			return true;
+		}
+
+		/* large CPU buffer */
+		if (usage >= PRESS_HI) { FORCE_GTT(); MAYBE_EWMA(size); }
+		else if (*domain == 0)  *domain |= AMDGPU_GEM_DOMAIN_VRAM;
+		return true;
+	}
+
+	/* ------------------------------------------------------------------
+	 * 4. Generic / uncategorised
+	 * ---------------------------------------------------------------- */
+	if (size <= AMDGPU_VEGA_SMALL_BUFFER_SIZE) {            /* ≤1 MiB */
+		if (usage >= PRESS_HI) { FORCE_GTT(); /* no EWMA */ }
+		else if (*domain == 0)  *domain |= AMDGPU_GEM_DOMAIN_VRAM;
+		return true;
+	}
+
+	if (size <= AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {           /* 1‑4 MiB */
+		if (usage >= PRESS_HI) { FORCE_GTT(); MAYBE_EWMA(size); }
+		else if (*domain == 0)  *domain |= AMDGPU_GEM_DOMAIN_VRAM;
+		return true;
+	}
+
+	/* large generic */
+	if (usage >= PRESS_HI) { FORCE_GTT(); MAYBE_EWMA(size); }
+	else if (*domain == 0)  *domain |= AMDGPU_GEM_DOMAIN_VRAM;
+
+	return true;
+}
+
+static __cold bool
+amdgpu_vega_optimize_hbm2_bank_access(struct amdgpu_device *adev,
+									  struct amdgpu_bo *bo,
+									  uint64_t *aligned_size,
+									  uint32_t *alignment)
+{
+	/* Quick outs ------------------------------------------------------- */
+	if (!is_hbm2_vega(adev) ||
+		!static_branch_unlikely(&vega_bankalign_key) ||
+		!aligned_size || !alignment)
+		return false;
+
+	if (*aligned_size == 0 ||
+		*aligned_size > (16ULL * 1024 * 1024 * 1024))   /* > 16 GiB – skip */
+	return false;
+
+	/*
+	 * Size / workload table
+	 *  --------------------  ------------------------------ --------------
+	 *  Condition             Alignment enforced             Rationale
+	 *  --------------------  ------------------------------ --------------
+	 *  Texture / scan‑out
+	 *    size ≥ 24 MiB       2 MiB                          minimise bank alias
+	 *
+	 *  Compute, NO_CPU
+	 *    size ≥ 32 MiB       4 MiB                          wide linear stride
+	 *
+	 *  Generic big buffer
+	 *    size ≥ 128 MiB      1 MiB                          original rule
+	 *
+	 *  size ≥  4 MiB         256 KiB                        original rule
+	 */
+	if (bo && is_vega_texture(bo->flags) &&
+		*aligned_size >= 24ULL * 1024 * 1024) {
+		*alignment   = max_t(uint32_t, *alignment, 2 * 1024 * 1024);
+	*aligned_size = ALIGN(*aligned_size, 2 * 1024 * 1024);
+	return true;
+		}
+
+		if (bo && is_vega_compute(bo->flags) &&
+			!is_vega_cpu_access(bo->flags) &&
+			*aligned_size >= 32ULL * 1024 * 1024) {
+			*alignment   = max_t(uint32_t, *alignment, 4 * 1024 * 1024);
+		*aligned_size = ALIGN(*aligned_size, 4 * 1024 * 1024);
+		return true;
+			}
+
+			/* Legacy size‑based fall‑backs ------------------------------------ */
+			if (*aligned_size >= 128ULL * 1024 * 1024) {
+				*alignment   = max_t(uint32_t, *alignment, 1 * 1024 * 1024);
+				*aligned_size = ALIGN(*aligned_size, 1 * 1024 * 1024);
+				return true;
+			}
+
+			if (*aligned_size >= AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {
+				*alignment   = max_t(uint32_t, *alignment,
+									 AMDGPU_VEGA_HBM2_MIN_ALIGNMENT);
+				*aligned_size = ALIGN(*aligned_size,
+									  AMDGPU_VEGA_HBM2_MIN_ALIGNMENT);
+				return true;
+			}
+
+			/* Small objects keep existing logic (4 K / 8 K tweaks) ------------ */
+			if (bo && is_vega_texture(bo->flags) &&
+				*aligned_size >= AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {
+				*alignment   = max_t(uint32_t, *alignment, 4096);
+			*aligned_size = ALIGN(*aligned_size, 4096);
+			return true;
+				}
+
+				if (bo && is_vega_compute(bo->flags) &&
+					*aligned_size >= AMDGPU_VEGA_LARGE_BUFFER_SIZE) {
+					*alignment   = max_t(uint32_t, *alignment, 8192);
+				*aligned_size = ALIGN(*aligned_size, 8192);
+				return true;
+					}
+
+					if (!bo && *aligned_size >= AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {
+						*alignment   = max_t(uint32_t, *alignment, 4096);
+						*aligned_size = ALIGN(*aligned_size, 4096);
+						return true;
+					}
+
+					return false;
+}
+
+static __always_inline unsigned int
+amdgpu_vega_determine_optimal_prefetch(struct amdgpu_device *adev,
+									   struct amdgpu_bo     *bo,
+									   unsigned int          base_prefetch_pages,
+									   uint32_t              vram_usage)
+{
+	if (!is_hbm2_vega(adev) || !bo)
+		return base_prefetch_pages;
+
+	const uint64_t size      = amdgpu_bo_size(bo);
+	const unsigned int max_p = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;
+
+	if (!size)
+		return base_prefetch_pages;
+
+	if (vram_usage > 90)
+		return min(max(base_prefetch_pages / 2, 8u), max_p);
+
+	if ((bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) &&
+		is_vega_compute(bo->flags) &&
+		size > AMDGPU_VEGA_LARGE_BUFFER_SIZE) {
+		return min_t(unsigned int,
+					 base_prefetch_pages * 2,
+			   min(128u, max_p));
+		}
+
+		if ((bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) &&
+			is_vega_texture(bo->flags) &&
+			vram_usage < 75) {
+			return min_t(unsigned int,
+						 base_prefetch_pages * 6 / 5,
+				min(64u, max_p));
+			}
+
+			return min(base_prefetch_pages, max_p);
+}
+
+static bool amdgpu_vega_should_use_async_fence(struct amdgpu_device *adev,
+											   struct amdgpu_bo *bo,
+											   uint64_t flags)
+{
+	uint64_t size;
+
+	if (!is_hbm2_vega(adev) || !bo)
+		return false;
+
+	size = amdgpu_bo_size(bo);
+	if (size == 0)
+		return false;
+
+	/* Never async for explicit sync or large buffers (>32MB) */
+	if ((flags & AMDGPU_GEM_CREATE_EXPLICIT_SYNC) || size > (32ULL << 20))
+		return false;
+
+	if ((bo->preferred_domains & AMDGPU_GEM_DOMAIN_GTT) &&
+		is_vega_cpu_access(flags) &&
+		size < AMDGPU_VEGA_SMALL_BUFFER_SIZE) {
+		return true;
+		}
+		if ((bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) &&
+			is_vega_compute(flags) &&
+			!is_vega_cpu_access(flags)) {
+			return true;
+			}
+			return false;
+}
+
+static bool amdgpu_vega_optimize_for_workload(struct amdgpu_device *adev,
+											  struct amdgpu_bo *bo,
+											  uint64_t flags)
+{
+	uint64_t size;
+
+	if (!is_hbm2_vega(adev) || !bo)
+		return false;
+
+	if (!bo->tbo.base.dev)
+		return false;
+
+	size = amdgpu_bo_size(bo);
+	if (size == 0)
+		return false;
+
+	if (!dma_resv_is_locked(bo->tbo.base.resv))
+		return false;
+
+	/* Gaming workload: prioritize VRAM for textures/framebuffers, fallback to GTT */
+	if (is_vega_texture(flags) && size >= AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {
+		bo->preferred_domains = AMDGPU_GEM_DOMAIN_VRAM;
+		bo->allowed_domains = AMDGPU_GEM_DOMAIN_VRAM | AMDGPU_GEM_DOMAIN_GTT;
+		return true;
+	}
+
+	/* Compute workload: prefer VRAM for compute buffers */
+	if (is_vega_compute(flags) && !is_vega_cpu_access(flags)) {
+		bo->preferred_domains = AMDGPU_GEM_DOMAIN_VRAM;
+		bo->allowed_domains = AMDGPU_GEM_DOMAIN_VRAM | AMDGPU_GEM_DOMAIN_GTT;
+		return true;
+	}
+
+	/* API translation layers: prefer GTT for small CPU-accessible buffers */
+	if (is_vega_cpu_access(flags) && size <= AMDGPU_VEGA_SMALL_BUFFER_SIZE) {
+		bo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;
+		bo->allowed_domains = AMDGPU_GEM_DOMAIN_GTT | AMDGPU_GEM_DOMAIN_VRAM;
+		return true;
+	}
+
+	return false;
+}
+
+/* ---------------- VM fault handler (prefetch guard added) ------------- */
 static vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)
 {
 	struct ttm_buffer_object *bo = vmf->vma->vm_private_data;
-	struct drm_device *ddev = bo->base.dev;
-	vm_fault_t ret;
-	int idx;
+	struct drm_device        *ddev;
+	vm_fault_t                ret;
+	int                       idx;
+
+	if (unlikely(!bo))
+		return VM_FAULT_SIGBUS;
+
+	ddev = bo->base.dev;
+	if (unlikely(!ddev))
+		return VM_FAULT_SIGBUS;
 
 	ret = ttm_bo_vm_reserve(bo, vmf);
-	if (ret)
+	if (unlikely(ret))
 		return ret;
 
-	if (drm_dev_enter(ddev, &idx)) {
+	if (likely(drm_dev_enter(ddev, &idx))) {
+		struct amdgpu_device *adev = drm_to_adev(ddev);
+		unsigned int prefetch_pages = TTM_BO_VM_NUM_PREFAULT;
+
 		ret = amdgpu_bo_fault_reserve_notify(bo);
-		if (ret) {
+		if (unlikely(ret)) {
 			drm_dev_exit(idx);
-			goto unlock;
+			goto unlock_resv;
 		}
 
-		ret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot,
-					       TTM_BO_VM_NUM_PREFAULT);
+		/* Prefetch path now gated by vega_prefetch_key */
+		if (static_branch_unlikely(&vega_prefetch_key)) {
+			struct amdgpu_bo *abo = ttm_to_amdgpu_bo(bo);
+
+			if (likely(abo)) {
+				uint32_t usage =
+				amdgpu_vega_get_vram_usage_cached(adev);
+
+				prefetch_pages =
+				amdgpu_vega_determine_optimal_prefetch(
+					adev, abo, prefetch_pages, usage);
+
+				if (likely((unsigned long)vmf->address >= PAGE_OFFSET))
+					PREFETCH_WRITE((const void *)vmf->address);
+			}
+		}
 
+		ret = ttm_bo_vm_fault_reserved(vmf,
+									   vmf->vma->vm_page_prot,
+								 prefetch_pages);
 		drm_dev_exit(idx);
 	} else {
 		ret = ttm_bo_vm_dummy_page(vmf, vmf->vma->vm_page_prot);
 	}
-	if (ret == VM_FAULT_RETRY && !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT))
-		return ret;
 
-unlock:
+	if (likely(!(ret == VM_FAULT_RETRY &&
+		!(vmf->flags & FAULT_FLAG_RETRY_NOWAIT))))
+		goto unlock_resv;
+
+	return ret; /* VM_FAULT_RETRY fast‑return */
+
+	unlock_resv:
 	dma_resv_unlock(bo->base.resv);
 	return ret;
 }
 
+/* VM operations struct for GEM objects */
 static const struct vm_operations_struct amdgpu_gem_vm_ops = {
 	.fault = amdgpu_gem_fault,
 	.open = ttm_bo_vm_open,
 	.close = ttm_bo_vm_close,
-	.access = ttm_bo_vm_access
+	.access = ttm_bo_vm_access,
 };
 
+/* Free a GEM object */
 static void amdgpu_gem_object_free(struct drm_gem_object *gobj)
 {
 	struct amdgpu_bo *aobj = gem_to_amdgpu_bo(gobj);
 
-	amdgpu_hmm_unregister(aobj);
-	ttm_bo_put(&aobj->tbo);
+	if (aobj) {
+		amdgpu_hmm_unregister(aobj);
+		ttm_bo_put(&aobj->tbo);
+		/* No need to NULL aobj, it's on the stack */
+	}
 }
 
-int amdgpu_gem_object_create(struct amdgpu_device *adev, unsigned long size,
-			     int alignment, u32 initial_domain,
-			     u64 flags, enum ttm_bo_type type,
-			     struct dma_resv *resv,
-			     struct drm_gem_object **obj, int8_t xcp_id_plus1)
+/* Create a new GEM object */
+int amdgpu_gem_object_create(struct amdgpu_device     *adev,
+							 unsigned long             size,
+							 int                       alignment,
+							 u32                       initial_domain,
+							 u64                       flags,
+							 enum ttm_bo_type          type,
+							 struct dma_resv          *resv,
+							 struct drm_gem_object   **obj,
+							 int8_t                    xcp_id_plus1)
 {
-	struct amdgpu_bo *bo;
-	struct amdgpu_bo_user *ubo;
-	struct amdgpu_bo_param bp;
+	struct amdgpu_bo_user *ubo = NULL;
+	struct amdgpu_bo      *bo;
 	int r;
 
-	memset(&bp, 0, sizeof(bp));
 	*obj = NULL;
-	flags |= AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE;
 
-	bp.size = size;
-	bp.byte_align = alignment;
-	bp.type = type;
-	bp.resv = resv;
-	bp.preferred_domain = initial_domain;
-	bp.flags = flags;
-	bp.domain = initial_domain;
-	bp.bo_ptr_size = sizeof(struct amdgpu_bo);
-	bp.xcp_id_plus1 = xcp_id_plus1;
+	/* create slab on‑demand */
+	amdgpu_tbo_slab_ensure();
 
+	/* learn static key */
+	if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
+		amdgpu_vm_always_valid_key_enable();
+	}
+
+	/* tiny‑BO cache fast‑path */
+	bo = tbo_cache_try_get(size, flags, initial_domain, resv, alignment);
+	if (bo) {
+		*obj = &bo->tbo.base;
+		return 0;
+	}
+
+	/* fill BO param */
+	struct amdgpu_bo_param bp = {
+		.size             = size,
+		.byte_align       = alignment,
+		.type             = type,
+		.resv             = resv,
+		.preferred_domain = initial_domain,
+		.flags            = flags | AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE,
+		.domain           = initial_domain,
+		.bo_ptr_size      = sizeof(struct amdgpu_bo),
+		.xcp_id_plus1     = xcp_id_plus1,
+	};
+
+	if (static_branch_unlikely(&vega_domain_key)) {
+		amdgpu_vega_optimize_buffer_placement(adev, NULL, size,
+											  flags, &bp.domain);
+	}
+
+	/* allocate metadata from slab if available */
+	if (ubo_slab)
+		ubo = kmem_cache_zalloc(ubo_slab, GFP_KERNEL | __GFP_NOWARN);
+
+	/* allocate BO */
 	r = amdgpu_bo_create_user(adev, &bp, &ubo);
-	if (r)
+	if (r) {
+		if (ubo && ubo_slab)
+			kmem_cache_free(ubo_slab, ubo);
 		return r;
+	}
 
 	bo = &ubo->bo;
-	*obj = &bo->tbo.base;
 
+	/* enable cache key once and maybe insert */
+	if (!static_branch_likely(&tbo_cache_key)) {
+		static_branch_enable(&tbo_cache_key);
+	}
+	tbo_cache_put(bo);   /* safe if not eligible */
+
+	*obj = &bo->tbo.base;
 	return 0;
 }
 
+/* Force release of all GEM objects for a device */
 void amdgpu_gem_force_release(struct amdgpu_device *adev)
 {
 	struct drm_device *ddev = adev_to_drm(adev);
@@ -151,34 +988,58 @@ void amdgpu_gem_force_release(struct amd
 	mutex_unlock(&ddev->filelist_mutex);
 }
 
-/*
- * Call from drm_gem_handle_create which appear in both new and open ioctl
- * case.
- */
+/* Open a GEM object for a file descriptor */
 static int amdgpu_gem_object_open(struct drm_gem_object *obj,
-				  struct drm_file *file_priv)
+								  struct drm_file *file_priv)
 {
-	struct amdgpu_bo *abo = gem_to_amdgpu_bo(obj);
-	struct amdgpu_device *adev = amdgpu_ttm_adev(abo->tbo.bdev);
-	struct amdgpu_fpriv *fpriv = file_priv->driver_priv;
-	struct amdgpu_vm *vm = &fpriv->vm;
-	struct amdgpu_bo_va *bo_va;
-	struct mm_struct *mm;
-	int r;
+	struct amdgpu_bo     *abo;
+	struct amdgpu_device *adev;
+	struct amdgpu_fpriv  *fpriv;
+	struct amdgpu_vm     *vm;
+	struct amdgpu_bo_va  *bo_va;
+	struct mm_struct     *mm;
+	int r = 0;
+
+	if (!obj || !file_priv)
+		return -EINVAL;
+
+	abo  = gem_to_amdgpu_bo(obj);
+	adev = amdgpu_ttm_adev(abo->tbo.bdev);
+	fpriv = file_priv->driver_priv;
+	if (!abo || !adev || !fpriv)
+		return -EINVAL;
+
+	vm = &fpriv->vm;
+
+	if (static_branch_unlikely(&vega_prefetch_key)) {
+		PREFETCH_READ(abo);
+		PREFETCH_READ(&fpriv->vm);
+		PREFETCH_READ(abo->tbo.base.resv);
+	}
 
 	mm = amdgpu_ttm_tt_get_usermm(abo->tbo.ttm);
 	if (mm && mm != current->mm)
 		return -EPERM;
 
-	if (abo->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID &&
-	    !amdgpu_vm_is_bo_always_valid(vm, abo))
+	if ((abo->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) &&
+		!amdgpu_vm_is_bo_always_valid(vm, abo))
 		return -EPERM;
 
+	/* ultra‑fast graphics‑only path */
+	if (!vm->is_compute_context &&
+		static_branch_likely(&amdgpu_vm_always_valid_key) &&
+		(abo->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) &&
+		(abo->allowed_domains == AMDGPU_GEM_DOMAIN_GTT) &&
+		!abo->parent &&
+		(!obj->import_attach ||
+		!dma_buf_is_dynamic(obj->import_attach->dmabuf)))
+		return 0;
+
+	/* reserve and VA logic (unchanged) */
 	r = amdgpu_bo_reserve(abo, false);
 	if (r)
 		return r;
 
-	amdgpu_vm_bo_update_shared(abo);
 	bo_va = amdgpu_vm_bo_find(vm, abo);
 	if (!bo_va)
 		bo_va = amdgpu_vm_bo_add(adev, vm, abo);
@@ -186,53 +1047,84 @@ static int amdgpu_gem_object_open(struct
 		++bo_va->ref_count;
 	amdgpu_bo_unreserve(abo);
 
-	/* Validate and add eviction fence to DMABuf imports with dynamic
-	 * attachment in compute VMs. Re-validation will be done by
-	 * amdgpu_vm_validate. Fences are on the reservation shared with the
-	 * export, which is currently required to be validated and fenced
-	 * already by amdgpu_amdkfd_gpuvm_restore_process_bos.
-	 *
-	 * Nested locking below for the case that a GEM object is opened in
-	 * kfd_mem_export_dmabuf. Since the lock below is only taken for imports,
-	 * but not for export, this is a different lock class that cannot lead to
-	 * circular lock dependencies.
-	 */
 	if (!vm->is_compute_context || !vm->process_info)
 		return 0;
 	if (!obj->import_attach ||
-	    !dma_buf_is_dynamic(obj->import_attach->dmabuf))
+		!dma_buf_is_dynamic(obj->import_attach->dmabuf))
 		return 0;
+
 	mutex_lock_nested(&vm->process_info->lock, 1);
+
 	if (!WARN_ON(!vm->process_info->eviction_fence)) {
-		r = amdgpu_amdkfd_bo_validate_and_fence(abo, AMDGPU_GEM_DOMAIN_GTT,
-							&vm->process_info->eviction_fence->base);
-		if (r) {
-			struct amdgpu_task_info *ti = amdgpu_vm_get_task_info_vm(vm);
-
-			dev_warn(adev->dev, "validate_and_fence failed: %d\n", r);
-			if (ti) {
-				dev_warn(adev->dev, "pid %d\n", ti->pid);
-				amdgpu_vm_put_task_info(ti);
+		if (static_branch_likely(&amdgpu_vm_always_valid_key) &&
+			(abo->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID)) {
+			mutex_unlock(&vm->process_info->lock);
+		return 0;
+			}
+
+			if (is_hbm2_vega(adev)) {
+				uint32_t domain = AMDGPU_GEM_DOMAIN_GTT;
+				if (is_vega_texture(abo->flags) ||
+					is_vega_compute(abo->flags)) {
+					domain = AMDGPU_GEM_DOMAIN_VRAM;
+				if (amdgpu_vega_get_effective_vram_usage(adev) >
+					amdgpu_vega_vram_pressure_high)
+					domain = AMDGPU_GEM_DOMAIN_GTT;
+					}
+					r = amdgpu_amdkfd_bo_validate_and_fence(
+						abo, domain,
+						&vm->process_info->eviction_fence->base);
+			} else {
+				r = amdgpu_amdkfd_bo_validate_and_fence(
+					abo, AMDGPU_GEM_DOMAIN_GTT,
+					&vm->process_info->eviction_fence->base);
 			}
-		}
 	}
 	mutex_unlock(&vm->process_info->lock);
-
 	return r;
 }
 
-static void amdgpu_gem_object_close(struct drm_gem_object *obj,
-				    struct drm_file *file_priv)
+/* Close a GEM object for a file descriptor */
+static void
+amdgpu_gem_object_close(struct drm_gem_object *obj,
+						struct drm_file *file_priv)
 {
-	struct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);
-	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);
-	struct amdgpu_fpriv *fpriv = file_priv->driver_priv;
-	struct amdgpu_vm *vm = &fpriv->vm;
-
+	struct amdgpu_bo *bo;
+	struct amdgpu_device *adev;
+	struct amdgpu_fpriv *fpriv;
+	struct amdgpu_vm *vm;
 	struct dma_fence *fence = NULL;
 	struct amdgpu_bo_va *bo_va;
 	struct drm_exec exec;
-	long r;
+	long r = 0;
+	bool use_async = false;
+
+	if (!obj || !file_priv)
+		return;
+
+	bo = gem_to_amdgpu_bo(obj);
+	if (!bo)
+		return;
+
+	adev = amdgpu_ttm_adev(bo->tbo.bdev);
+	if (!adev)
+		return;
+
+	fpriv = file_priv->driver_priv;
+	if (!fpriv)
+		return;
+
+	/* Prefetch meta we will lock / write shortly */
+	if (static_branch_unlikely(&vega_prefetch_key)) {
+		PREFETCH_WRITE(bo->tbo.base.resv);
+		PREFETCH_READ(&fpriv->vm);
+	}
+
+	vm = &fpriv->vm;
+
+	if (is_hbm2_vega(adev)) {
+		use_async = amdgpu_vega_should_use_async_fence(adev, bo, bo->flags);
+	}
 
 	drm_exec_init(&exec, DRM_EXEC_IGNORE_DUPLICATES, 0);
 	drm_exec_until_all_locked(&exec) {
@@ -248,27 +1140,34 @@ static void amdgpu_gem_object_close(stru
 	}
 
 	bo_va = amdgpu_vm_bo_find(vm, bo);
-	if (!bo_va || --bo_va->ref_count)
+	if (!bo_va)
+		goto out_unlock;
+
+	if (--bo_va->ref_count > 0)
 		goto out_unlock;
 
 	amdgpu_vm_bo_del(adev, bo_va);
 	amdgpu_vm_bo_update_shared(bo);
+
 	if (!amdgpu_vm_ready(vm))
 		goto out_unlock;
 
 	r = amdgpu_vm_clear_freed(adev, vm, &fence);
-	if (unlikely(r < 0))
-		dev_err(adev->dev, "failed to clear page "
-			"tables on GEM object close (%ld)\n", r);
+	if (unlikely(r < 0)) {
+		dev_err(adev->dev, "failed to clear page tables on GEM object close (%ld)\n", r);
+		goto out_unlock;
+	}
 	if (r || !fence)
 		goto out_unlock;
 
-	amdgpu_bo_fence(bo, fence, true);
+	amdgpu_bo_fence(bo, fence, use_async);
 	dma_fence_put(fence);
 
-out_unlock:
-	if (r)
-		dev_err(adev->dev, "leaking bo va (%ld)\n", r);
+	out_unlock:
+	if (r) {
+		dev_err(adev->dev, "Error in GEM object close for pid %d, potential leak of bo_va (%ld)\n",
+				task_pid_nr(current), r);
+	}
 	drm_exec_fini(&exec);
 }
 
@@ -281,13 +1180,8 @@ static int amdgpu_gem_object_mmap(struct
 	if (bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)
 		return -EPERM;
 
-	/* Workaround for Thunk bug creating PROT_NONE,MAP_PRIVATE mappings
-	 * for debugger access to invisible VRAM. Should have used MAP_SHARED
-	 * instead. Clearing VM_MAYWRITE prevents the mapping from ever
-	 * becoming writable and makes is_cow_mapping(vm_flags) false.
-	 */
 	if (is_cow_mapping(vma->vm_flags) &&
-	    !(vma->vm_flags & VM_ACCESS_FLAGS))
+		!(vma->vm_flags & VM_ACCESS_FLAGS))
 		vm_flags_clear(vma, VM_MAYWRITE);
 
 	return drm_gem_ttm_mmap(obj, vma);
@@ -308,7 +1202,7 @@ const struct drm_gem_object_funcs amdgpu
  * GEM ioctls.
  */
 int amdgpu_gem_create_ioctl(struct drm_device *dev, void *data,
-			    struct drm_file *filp)
+							struct drm_file *filp)
 {
 	struct amdgpu_device *adev = drm_to_adev(dev);
 	struct amdgpu_fpriv *fpriv = filp->driver_priv;
@@ -321,23 +1215,20 @@ int amdgpu_gem_create_ioctl(struct drm_d
 	uint32_t handle, initial_domain;
 	int r;
 
-	/* reject DOORBELLs until userspace code to use it is available */
 	if (args->in.domains & AMDGPU_GEM_DOMAIN_DOORBELL)
 		return -EINVAL;
 
-	/* reject invalid gem flags */
 	if (flags & ~(AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |
-		      AMDGPU_GEM_CREATE_NO_CPU_ACCESS |
-		      AMDGPU_GEM_CREATE_CPU_GTT_USWC |
-		      AMDGPU_GEM_CREATE_VRAM_CLEARED |
-		      AMDGPU_GEM_CREATE_VM_ALWAYS_VALID |
-		      AMDGPU_GEM_CREATE_EXPLICIT_SYNC |
-		      AMDGPU_GEM_CREATE_ENCRYPTED |
-		      AMDGPU_GEM_CREATE_GFX12_DCC |
-		      AMDGPU_GEM_CREATE_DISCARDABLE))
+		AMDGPU_GEM_CREATE_NO_CPU_ACCESS |
+		AMDGPU_GEM_CREATE_CPU_GTT_USWC |
+		AMDGPU_GEM_CREATE_VRAM_CLEARED |
+		AMDGPU_GEM_CREATE_VM_ALWAYS_VALID |
+		AMDGPU_GEM_CREATE_EXPLICIT_SYNC |
+		AMDGPU_GEM_CREATE_ENCRYPTED |
+		AMDGPU_GEM_CREATE_GFX12_DCC |
+		AMDGPU_GEM_CREATE_DISCARDABLE))
 		return -EINVAL;
 
-	/* reject invalid gem domains */
 	if (args->in.domains & ~AMDGPU_GEM_DOMAIN_MASK)
 		return -EINVAL;
 
@@ -346,62 +1237,54 @@ int amdgpu_gem_create_ioctl(struct drm_d
 		return -EINVAL;
 	}
 
-	/* always clear VRAM */
 	flags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;
 
-	/* create a gem object to contain this object in */
 	if (args->in.domains & (AMDGPU_GEM_DOMAIN_GDS |
-	    AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA)) {
+		AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA)) {
 		if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
-			/* if gds bo is created from user space, it must be
-			 * passed to bo list
-			 */
 			DRM_ERROR("GDS bo cannot be per-vm-bo\n");
 			return -EINVAL;
 		}
 		flags |= AMDGPU_GEM_CREATE_NO_CPU_ACCESS;
-	}
+		}
 
-	if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
-		r = amdgpu_bo_reserve(vm->root.bo, false);
-		if (r)
-			return r;
+		if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
+			r = amdgpu_bo_reserve(vm->root.bo, false);
+			if (r)
+				return r;
+			resv = vm->root.bo->tbo.base.resv;
+		}
 
-		resv = vm->root.bo->tbo.base.resv;
-	}
+		initial_domain = (u32)(0xffffffff & args->in.domains);
 
-	initial_domain = (u32)(0xffffffff & args->in.domains);
-retry:
-	r = amdgpu_gem_object_create(adev, size, args->in.alignment,
-				     initial_domain,
-				     flags, ttm_bo_type_device, resv, &gobj, fpriv->xcp_id + 1);
-	if (r && r != -ERESTARTSYS) {
-		if (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {
-			flags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
-			goto retry;
-		}
-
-		if (initial_domain == AMDGPU_GEM_DOMAIN_VRAM) {
-			initial_domain |= AMDGPU_GEM_DOMAIN_GTT;
-			goto retry;
+		retry:
+		r = amdgpu_gem_object_create(adev, size, args->in.alignment,
+									 initial_domain, flags, ttm_bo_type_device,
+							   resv, &gobj, fpriv->xcp_id + 1);
+		if (r && r != -ERESTARTSYS) {
+			if (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {
+				flags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
+				goto retry;
+			}
+			if (initial_domain == AMDGPU_GEM_DOMAIN_VRAM) {
+				initial_domain |= AMDGPU_GEM_DOMAIN_GTT;
+				goto retry;
+			}
+			DRM_DEBUG("Failed to allocate GEM object (%llu, %d, %llu, %d)\n",
+					  size, initial_domain, args->in.alignment, r);
 		}
-		DRM_DEBUG("Failed to allocate GEM object (%llu, %d, %llu, %d)\n",
-				size, initial_domain, args->in.alignment, r);
-	}
 
-	if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
-		if (!r) {
-			struct amdgpu_bo *abo = gem_to_amdgpu_bo(gobj);
-
-			abo->parent = amdgpu_bo_ref(vm->root.bo);
+		if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
+			if (!r) {
+				struct amdgpu_bo *abo = gem_to_amdgpu_bo(gobj);
+				abo->parent = amdgpu_bo_ref(vm->root.bo);
+			}
+			amdgpu_bo_unreserve(vm->root.bo);
 		}
-		amdgpu_bo_unreserve(vm->root.bo);
-	}
-	if (r)
-		return r;
+		if (r)
+			return r;
 
 	r = drm_gem_handle_create(filp, gobj, &handle);
-	/* drop reference from allocate - handle holds it now */
 	drm_gem_object_put(gobj);
 	if (r)
 		return r;
@@ -412,7 +1295,7 @@ retry:
 }
 
 int amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,
-			     struct drm_file *filp)
+							 struct drm_file *filp)
 {
 	struct ttm_operation_ctx ctx = { true, false };
 	struct amdgpu_device *adev = drm_to_adev(dev);
@@ -429,24 +1312,20 @@ int amdgpu_gem_userptr_ioctl(struct drm_
 	if (offset_in_page(args->addr | args->size))
 		return -EINVAL;
 
-	/* reject unknown flag values */
 	if (args->flags & ~(AMDGPU_GEM_USERPTR_READONLY |
-	    AMDGPU_GEM_USERPTR_ANONONLY | AMDGPU_GEM_USERPTR_VALIDATE |
-	    AMDGPU_GEM_USERPTR_REGISTER))
+		AMDGPU_GEM_USERPTR_ANONONLY | AMDGPU_GEM_USERPTR_VALIDATE |
+		AMDGPU_GEM_USERPTR_REGISTER))
 		return -EINVAL;
 
 	if (!(args->flags & AMDGPU_GEM_USERPTR_READONLY) &&
-	     !(args->flags & AMDGPU_GEM_USERPTR_REGISTER)) {
-
-		/* if we want to write to it we must install a MMU notifier */
+		!(args->flags & AMDGPU_GEM_USERPTR_REGISTER)) {
 		return -EACCES;
-	}
+		}
 
-	/* create a gem object to contain this object in */
-	r = amdgpu_gem_object_create(adev, args->size, 0, AMDGPU_GEM_DOMAIN_CPU,
-				     0, ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
-	if (r)
-		return r;
+		r = amdgpu_gem_object_create(adev, args->size, 0, AMDGPU_GEM_DOMAIN_CPU,
+									 0, ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
+		if (r)
+			return r;
 
 	bo = gem_to_amdgpu_bo(gobj);
 	bo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;
@@ -460,8 +1339,7 @@ int amdgpu_gem_userptr_ioctl(struct drm_
 		goto release_object;
 
 	if (args->flags & AMDGPU_GEM_USERPTR_VALIDATE) {
-		r = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages,
-						 &range);
+		r = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages, &range);
 		if (r)
 			goto release_object;
 
@@ -482,19 +1360,19 @@ int amdgpu_gem_userptr_ioctl(struct drm_
 
 	args->handle = handle;
 
-user_pages_done:
+	user_pages_done:
 	if (args->flags & AMDGPU_GEM_USERPTR_VALIDATE)
 		amdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, range);
 
-release_object:
+	release_object:
 	drm_gem_object_put(gobj);
 
 	return r;
 }
 
 int amdgpu_mode_dumb_mmap(struct drm_file *filp,
-			  struct drm_device *dev,
-			  uint32_t handle, uint64_t *offset_p)
+						  struct drm_device *dev,
+						  uint32_t handle, uint64_t *offset_p)
 {
 	struct drm_gem_object *gobj;
 	struct amdgpu_bo *robj;
@@ -505,17 +1383,17 @@ int amdgpu_mode_dumb_mmap(struct drm_fil
 
 	robj = gem_to_amdgpu_bo(gobj);
 	if (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm) ||
-	    (robj->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {
+		(robj->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {
 		drm_gem_object_put(gobj);
-		return -EPERM;
-	}
-	*offset_p = amdgpu_bo_mmap_offset(robj);
-	drm_gem_object_put(gobj);
-	return 0;
+	return -EPERM;
+		}
+		*offset_p = amdgpu_bo_mmap_offset(robj);
+		drm_gem_object_put(gobj);
+		return 0;
 }
 
 int amdgpu_gem_mmap_ioctl(struct drm_device *dev, void *data,
-			  struct drm_file *filp)
+						  struct drm_file *filp)
 {
 	union drm_amdgpu_gem_mmap *args = data;
 	uint32_t handle = args->in.handle;
@@ -524,36 +1402,8 @@ int amdgpu_gem_mmap_ioctl(struct drm_dev
 	return amdgpu_mode_dumb_mmap(filp, dev, handle, &args->out.addr_ptr);
 }
 
-/**
- * amdgpu_gem_timeout - calculate jiffies timeout from absolute value
- *
- * @timeout_ns: timeout in ns
- *
- * Calculate the timeout in jiffies from an absolute timeout in ns.
- */
-unsigned long amdgpu_gem_timeout(uint64_t timeout_ns)
-{
-	unsigned long timeout_jiffies;
-	ktime_t timeout;
-
-	/* clamp timeout if it's to large */
-	if (((int64_t)timeout_ns) < 0)
-		return MAX_SCHEDULE_TIMEOUT;
-
-	timeout = ktime_sub(ns_to_ktime(timeout_ns), ktime_get());
-	if (ktime_to_ns(timeout) < 0)
-		return 0;
-
-	timeout_jiffies = nsecs_to_jiffies(ktime_to_ns(timeout));
-	/*  clamp timeout to avoid unsigned-> signed overflow */
-	if (timeout_jiffies > MAX_SCHEDULE_TIMEOUT)
-		return MAX_SCHEDULE_TIMEOUT - 1;
-
-	return timeout_jiffies;
-}
-
 int amdgpu_gem_wait_idle_ioctl(struct drm_device *dev, void *data,
-			      struct drm_file *filp)
+							   struct drm_file *filp)
 {
 	union drm_amdgpu_gem_wait_idle *args = data;
 	struct drm_gem_object *gobj;
@@ -569,12 +1419,8 @@ int amdgpu_gem_wait_idle_ioctl(struct dr
 
 	robj = gem_to_amdgpu_bo(gobj);
 	ret = dma_resv_wait_timeout(robj->tbo.base.resv, DMA_RESV_USAGE_READ,
-				    true, timeout);
+								true, timeout);
 
-	/* ret == 0 means not signaled,
-	 * ret > 0 means signaled
-	 * ret < 0 means interrupted before timeout
-	 */
 	if (ret >= 0) {
 		memset(args, 0, sizeof(*args));
 		args->out.status = (ret == 0);
@@ -586,7 +1432,7 @@ int amdgpu_gem_wait_idle_ioctl(struct dr
 }
 
 int amdgpu_gem_metadata_ioctl(struct drm_device *dev, void *data,
-				struct drm_file *filp)
+							  struct drm_file *filp)
 {
 	struct drm_amdgpu_gem_metadata *args = data;
 	struct drm_gem_object *gobj;
@@ -595,8 +1441,10 @@ int amdgpu_gem_metadata_ioctl(struct drm
 
 	DRM_DEBUG("%d\n", args->handle);
 	gobj = drm_gem_object_lookup(filp, args->handle);
-	if (gobj == NULL)
+	if (gobj == NULL) {
 		return -ENOENT;
+	}
+	/* FIX: Use the declared variable 'robj', not 'abo' */
 	robj = gem_to_amdgpu_bo(gobj);
 
 	r = amdgpu_bo_reserve(robj, false);
@@ -606,9 +1454,9 @@ int amdgpu_gem_metadata_ioctl(struct drm
 	if (args->op == AMDGPU_GEM_METADATA_OP_GET_METADATA) {
 		amdgpu_bo_get_tiling_flags(robj, &args->data.tiling_info);
 		r = amdgpu_bo_get_metadata(robj, args->data.data,
-					   sizeof(args->data.data),
-					   &args->data.data_size_bytes,
-					   &args->data.flags);
+								   sizeof(args->data.data),
+								   &args->data.data_size_bytes,
+							 &args->data.flags);
 	} else if (args->op == AMDGPU_GEM_METADATA_OP_SET_METADATA) {
 		if (args->data.data_size_bytes > sizeof(args->data.data)) {
 			r = -EINVAL;
@@ -617,64 +1465,17 @@ int amdgpu_gem_metadata_ioctl(struct drm
 		r = amdgpu_bo_set_tiling_flags(robj, args->data.tiling_info);
 		if (!r)
 			r = amdgpu_bo_set_metadata(robj, args->data.data,
-						   args->data.data_size_bytes,
-						   args->data.flags);
+									   args->data.data_size_bytes,
+							  args->data.flags);
 	}
 
-unreserve:
+	unreserve:
 	amdgpu_bo_unreserve(robj);
-out:
+	out:
 	drm_gem_object_put(gobj);
 	return r;
 }
 
-/**
- * amdgpu_gem_va_update_vm -update the bo_va in its VM
- *
- * @adev: amdgpu_device pointer
- * @vm: vm to update
- * @bo_va: bo_va to update
- * @operation: map, unmap or clear
- *
- * Update the bo_va directly after setting its address. Errors are not
- * vital here, so they are not reported back to userspace.
- */
-static void amdgpu_gem_va_update_vm(struct amdgpu_device *adev,
-				    struct amdgpu_vm *vm,
-				    struct amdgpu_bo_va *bo_va,
-				    uint32_t operation)
-{
-	int r;
-
-	if (!amdgpu_vm_ready(vm))
-		return;
-
-	r = amdgpu_vm_clear_freed(adev, vm, NULL);
-	if (r)
-		goto error;
-
-	if (operation == AMDGPU_VA_OP_MAP ||
-	    operation == AMDGPU_VA_OP_REPLACE) {
-		r = amdgpu_vm_bo_update(adev, bo_va, false);
-		if (r)
-			goto error;
-	}
-
-	r = amdgpu_vm_update_pdes(adev, vm, false);
-
-error:
-	if (r && r != -ERESTARTSYS)
-		DRM_ERROR("Couldn't update BO_VA (%d)\n", r);
-}
-
-/**
- * amdgpu_gem_va_map_flags - map GEM UAPI flags into hardware flags
- *
- * @adev: amdgpu_device pointer
- * @flags: GEM UAPI flags
- *
- * Returns the GEM UAPI flags mapped into hardware for the ASIC.
- */
 uint64_t amdgpu_gem_va_map_flags(struct amdgpu_device *adev, uint32_t flags)
 {
 	uint64_t pte_flag = 0;
@@ -690,156 +1491,192 @@ uint64_t amdgpu_gem_va_map_flags(struct
 	if (flags & AMDGPU_VM_PAGE_NOALLOC)
 		pte_flag |= AMDGPU_PTE_NOALLOC;
 
-	if (adev->gmc.gmc_funcs->map_mtype)
+	if (adev->gmc.gmc_funcs && adev->gmc.gmc_funcs->map_mtype) {
 		pte_flag |= amdgpu_gmc_map_mtype(adev,
-						 flags & AMDGPU_VM_MTYPE_MASK);
+										 flags & AMDGPU_VM_MTYPE_MASK);
+	}
 
 	return pte_flag;
 }
 
+static __cold void amdgpu_gem_va_update_vm(struct amdgpu_device *adev,
+										   struct amdgpu_vm *vm,
+										   struct amdgpu_bo_va *bo_va,
+										   uint32_t operation)
+{
+	int r;
+
+	if (!amdgpu_vm_ready(vm))
+		return;
+
+	r = amdgpu_vm_clear_freed(adev, vm, NULL);
+	if (r)
+		goto error;
+
+	if (operation == AMDGPU_VA_OP_MAP ||
+		operation == AMDGPU_VA_OP_REPLACE) {
+		r = amdgpu_vm_bo_update(adev, bo_va, false);
+	if (r)
+		goto error;
+		}
+
+		r = amdgpu_vm_update_pdes(adev, vm, false);
+
+	error:
+	if (r && r != -ERESTARTSYS)
+		DRM_ERROR("Couldn't update BO_VA (%d)\n", r);
+}
+
 int amdgpu_gem_va_ioctl(struct drm_device *dev, void *data,
-			  struct drm_file *filp)
+						struct drm_file *filp)
 {
+	/* ------------- existing declarations stay the same --------------- */
 	const uint32_t valid_flags = AMDGPU_VM_DELAY_UPDATE |
-		AMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE |
-		AMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_MTYPE_MASK |
-		AMDGPU_VM_PAGE_NOALLOC;
+	AMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE |
+	AMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_MTYPE_MASK |
+	AMDGPU_VM_PAGE_NOALLOC;
 	const uint32_t prt_flags = AMDGPU_VM_DELAY_UPDATE |
-		AMDGPU_VM_PAGE_PRT;
+	AMDGPU_VM_PAGE_PRT;
 
 	struct drm_amdgpu_gem_va *args = data;
-	struct drm_gem_object *gobj;
+	struct drm_gem_object *gobj = NULL;
+	struct amdgpu_bo *abo = NULL;
 	struct amdgpu_device *adev = drm_to_adev(dev);
 	struct amdgpu_fpriv *fpriv = filp->driver_priv;
-	struct amdgpu_bo *abo;
-	struct amdgpu_bo_va *bo_va;
+	struct amdgpu_bo_va *bo_va = NULL;
 	struct drm_exec exec;
 	uint64_t va_flags;
 	uint64_t vm_size;
 	int r = 0;
 
+	/* Prefetch frequently‑used structures */
+	if (static_branch_unlikely(&vega_prefetch_key)) {
+		PREFETCH_READ(fpriv);
+		PREFETCH_READ(&fpriv->vm);
+		PREFETCH_READ(args);
+	}
+
 	if (args->va_address < AMDGPU_VA_RESERVED_BOTTOM) {
 		dev_dbg(dev->dev,
-			"va_address 0x%llx is in reserved area 0x%llx\n",
-			args->va_address, AMDGPU_VA_RESERVED_BOTTOM);
+				"va_address 0x%llx is in reserved area 0x%llx\n",
+		  args->va_address, AMDGPU_VA_RESERVED_BOTTOM);
 		return -EINVAL;
 	}
 
 	if (args->va_address >= AMDGPU_GMC_HOLE_START &&
-	    args->va_address < AMDGPU_GMC_HOLE_END) {
+		args->va_address < AMDGPU_GMC_HOLE_END) {
 		dev_dbg(dev->dev,
-			"va_address 0x%llx is in VA hole 0x%llx-0x%llx\n",
-			args->va_address, AMDGPU_GMC_HOLE_START,
-			AMDGPU_GMC_HOLE_END);
+				"va_address 0x%llx is in VA hole 0x%llx-0x%llx\n",
+		  args->va_address, AMDGPU_GMC_HOLE_START,
+		  AMDGPU_GMC_HOLE_END);
 		return -EINVAL;
-	}
+		}
 
-	args->va_address &= AMDGPU_GMC_HOLE_MASK;
+		args->va_address &= AMDGPU_GMC_HOLE_MASK;
 
 	vm_size = adev->vm_manager.max_pfn * AMDGPU_GPU_PAGE_SIZE;
 	vm_size -= AMDGPU_VA_RESERVED_TOP;
 	if (args->va_address + args->map_size > vm_size) {
 		dev_dbg(dev->dev,
-			"va_address 0x%llx is in top reserved area 0x%llx\n",
-			args->va_address + args->map_size, vm_size);
+				"va_address 0x%llx is in top reserved area 0x%llx\n",
+		  args->va_address + args->map_size, vm_size);
 		return -EINVAL;
 	}
 
 	if ((args->flags & ~valid_flags) && (args->flags & ~prt_flags)) {
 		dev_dbg(dev->dev, "invalid flags combination 0x%08X\n",
-			args->flags);
+				args->flags);
 		return -EINVAL;
 	}
 
 	switch (args->operation) {
-	case AMDGPU_VA_OP_MAP:
-	case AMDGPU_VA_OP_UNMAP:
-	case AMDGPU_VA_OP_CLEAR:
-	case AMDGPU_VA_OP_REPLACE:
-		break;
-	default:
-		dev_dbg(dev->dev, "unsupported operation %d\n",
-			args->operation);
-		return -EINVAL;
+		case AMDGPU_VA_OP_MAP:
+		case AMDGPU_VA_OP_UNMAP:
+		case AMDGPU_VA_OP_CLEAR:
+		case AMDGPU_VA_OP_REPLACE:
+			break;
+		default:
+			dev_dbg(dev->dev, "unsupported operation %d\n", args->operation);
+			return -EINVAL;
 	}
 
 	if ((args->operation != AMDGPU_VA_OP_CLEAR) &&
-	    !(args->flags & AMDGPU_VM_PAGE_PRT)) {
+		!(args->flags & AMDGPU_VM_PAGE_PRT)) {
 		gobj = drm_gem_object_lookup(filp, args->handle);
-		if (gobj == NULL)
-			return -ENOENT;
-		abo = gem_to_amdgpu_bo(gobj);
-	} else {
-		gobj = NULL;
-		abo = NULL;
+	if (!gobj) {
+		return -ENOENT;
 	}
+	abo = gem_to_amdgpu_bo(gobj);
+		}
 
-	drm_exec_init(&exec, DRM_EXEC_INTERRUPTIBLE_WAIT |
-		      DRM_EXEC_IGNORE_DUPLICATES, 0);
-	drm_exec_until_all_locked(&exec) {
-		if (gobj) {
-			r = drm_exec_lock_obj(&exec, gobj);
+		drm_exec_init(&exec, DRM_EXEC_INTERRUPTIBLE_WAIT | DRM_EXEC_IGNORE_DUPLICATES, 0);
+
+		drm_exec_until_all_locked(&exec) {
+			if (gobj) {
+				r = drm_exec_lock_obj(&exec, gobj);
+				drm_exec_retry_on_contention(&exec);
+				if (unlikely(r))
+					goto error;
+			}
+			r = amdgpu_vm_lock_pd(&fpriv->vm, &exec, 2);
 			drm_exec_retry_on_contention(&exec);
 			if (unlikely(r))
 				goto error;
 		}
 
-		r = amdgpu_vm_lock_pd(&fpriv->vm, &exec, 2);
-		drm_exec_retry_on_contention(&exec);
-		if (unlikely(r))
-			goto error;
-	}
+		if (abo) {
+			bo_va = amdgpu_vm_bo_find(&fpriv->vm, abo);
+			if (!bo_va) {
+				r = -ENOENT;
+				goto error;
+			}
+		} else if (args->operation != AMDGPU_VA_OP_CLEAR) {
+			bo_va = fpriv->prt_va;
+			if (!bo_va) {
+				DRM_ERROR("Process context has no PRT VA\n");
+				r = -EINVAL;
+				goto error;
+			}
+		} // else bo_va = NULL for CLEAR
 
-	if (abo) {
-		bo_va = amdgpu_vm_bo_find(&fpriv->vm, abo);
-		if (!bo_va) {
-			r = -ENOENT;
-			goto error;
+		if (abo && is_hbm2_vega(adev)) {
+			amdgpu_vega_optimize_for_workload(adev, abo, abo->flags);
 		}
-	} else if (args->operation != AMDGPU_VA_OP_CLEAR) {
-		bo_va = fpriv->prt_va;
-	} else {
-		bo_va = NULL;
-	}
 
-	switch (args->operation) {
-	case AMDGPU_VA_OP_MAP:
-		va_flags = amdgpu_gem_va_map_flags(adev, args->flags);
-		r = amdgpu_vm_bo_map(adev, bo_va, args->va_address,
-				     args->offset_in_bo, args->map_size,
-				     va_flags);
-		break;
-	case AMDGPU_VA_OP_UNMAP:
-		r = amdgpu_vm_bo_unmap(adev, bo_va, args->va_address);
-		break;
+		switch (args->operation) {
+			case AMDGPU_VA_OP_MAP:
+				va_flags = amdgpu_gem_va_map_flags(adev, args->flags);
+				r = amdgpu_vm_bo_map(adev, bo_va, args->va_address,
+									 args->offset_in_bo, args->map_size, va_flags);
+				break;
+			case AMDGPU_VA_OP_UNMAP:
+				r = amdgpu_vm_bo_unmap(adev, bo_va, args->va_address);
+				break;
+			case AMDGPU_VA_OP_CLEAR:
+				r = amdgpu_vm_bo_clear_mappings(adev, &fpriv->vm,
+												args->va_address, args->map_size);
+				break;
+			case AMDGPU_VA_OP_REPLACE:
+				va_flags = amdgpu_gem_va_map_flags(adev, args->flags);
+				r = amdgpu_vm_bo_replace_map(adev, bo_va, args->va_address,
+											 args->offset_in_bo, args->map_size, va_flags);
+				break;
+		}
 
-	case AMDGPU_VA_OP_CLEAR:
-		r = amdgpu_vm_bo_clear_mappings(adev, &fpriv->vm,
-						args->va_address,
-						args->map_size);
-		break;
-	case AMDGPU_VA_OP_REPLACE:
-		va_flags = amdgpu_gem_va_map_flags(adev, args->flags);
-		r = amdgpu_vm_bo_replace_map(adev, bo_va, args->va_address,
-					     args->offset_in_bo, args->map_size,
-					     va_flags);
-		break;
-	default:
-		break;
-	}
-	if (!r && !(args->flags & AMDGPU_VM_DELAY_UPDATE) && !adev->debug_vm)
-		amdgpu_gem_va_update_vm(adev, &fpriv->vm, bo_va,
-					args->operation);
+		if (!r && !(args->flags & AMDGPU_VM_DELAY_UPDATE) && !adev->debug_vm)
+			amdgpu_gem_va_update_vm(adev, &fpriv->vm, bo_va, args->operation);
 
-error:
+	error:
 	drm_exec_fini(&exec);
-	drm_gem_object_put(gobj);
+	if (gobj)
+		drm_gem_object_put(gobj);
 	return r;
 }
 
 int amdgpu_gem_op_ioctl(struct drm_device *dev, void *data,
-			struct drm_file *filp)
+						struct drm_file *filp)
 {
+	struct amdgpu_device *adev = drm_to_adev(dev);
 	struct drm_amdgpu_gem_op *args = data;
 	struct drm_gem_object *gobj;
 	struct amdgpu_vm_bo_base *base;
@@ -857,123 +1694,115 @@ int amdgpu_gem_op_ioctl(struct drm_devic
 		goto out;
 
 	switch (args->op) {
-	case AMDGPU_GEM_OP_GET_GEM_CREATE_INFO: {
-		struct drm_amdgpu_gem_create_in info;
-		void __user *out = u64_to_user_ptr(args->value);
-
-		info.bo_size = robj->tbo.base.size;
-		info.alignment = robj->tbo.page_alignment << PAGE_SHIFT;
-		info.domains = robj->preferred_domains;
-		info.domain_flags = robj->flags;
-		amdgpu_bo_unreserve(robj);
-		if (copy_to_user(out, &info, sizeof(info)))
-			r = -EFAULT;
-		break;
-	}
-	case AMDGPU_GEM_OP_SET_PLACEMENT:
-		if (robj->tbo.base.import_attach &&
-		    args->value & AMDGPU_GEM_DOMAIN_VRAM) {
-			r = -EINVAL;
+		case AMDGPU_GEM_OP_GET_GEM_CREATE_INFO: {
+			struct drm_amdgpu_gem_create_in info;
+			void __user *out = u64_to_user_ptr(args->value);
+
+			info.bo_size = robj->tbo.base.size;
+			info.alignment = robj->tbo.page_alignment << PAGE_SHIFT;
+			info.domains = robj->preferred_domains;
+			info.domain_flags = robj->flags;
 			amdgpu_bo_unreserve(robj);
+			if (copy_to_user(out, &info, sizeof(info)))
+				r = -EFAULT;
 			break;
 		}
-		if (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm)) {
-			r = -EPERM;
+		case AMDGPU_GEM_OP_SET_PLACEMENT:
+			if (robj->tbo.base.import_attach &&
+				args->value & AMDGPU_GEM_DOMAIN_VRAM) {
+				r = -EINVAL;
 			amdgpu_bo_unreserve(robj);
 			break;
-		}
-		for (base = robj->vm_bo; base; base = base->next)
-			if (amdgpu_xgmi_same_hive(amdgpu_ttm_adev(robj->tbo.bdev),
-				amdgpu_ttm_adev(base->vm->root.bo->tbo.bdev))) {
-				r = -EINVAL;
-				amdgpu_bo_unreserve(robj);
+				}
+				if (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm)) {
+					r = -EPERM;
+					amdgpu_bo_unreserve(robj);
+					break;
+				}
+				for (base = robj->vm_bo; base; base = base->next)
+					if (amdgpu_xgmi_same_hive(amdgpu_ttm_adev(robj->tbo.bdev),
+						amdgpu_ttm_adev(base->vm->root.bo->tbo.bdev))) {
+						r = -EINVAL;
+					amdgpu_bo_unreserve(robj);
 				goto out;
-			}
+						}
 
+						robj->preferred_domains = args->value & (AMDGPU_GEM_DOMAIN_VRAM |
+						AMDGPU_GEM_DOMAIN_GTT |
+						AMDGPU_GEM_DOMAIN_CPU);
+						robj->allowed_domains = robj->preferred_domains;
+						if (robj->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)
+							robj->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
 
-		robj->preferred_domains = args->value & (AMDGPU_GEM_DOMAIN_VRAM |
-							AMDGPU_GEM_DOMAIN_GTT |
-							AMDGPU_GEM_DOMAIN_CPU);
-		robj->allowed_domains = robj->preferred_domains;
-		if (robj->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)
-			robj->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
+		if (is_hbm2_vega(adev)) {
+			amdgpu_vega_optimize_for_workload(adev, robj, robj->flags);
+		}
 
-		if (robj->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID)
+		if (robj->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
 			amdgpu_vm_bo_invalidate(robj, true);
+		}
 
 		amdgpu_bo_unreserve(robj);
 		break;
-	default:
-		amdgpu_bo_unreserve(robj);
-		r = -EINVAL;
+		default:
+			amdgpu_bo_unreserve(robj);
+			r = -EINVAL;
 	}
 
-out:
+	out:
 	drm_gem_object_put(gobj);
 	return r;
 }
 
-static int amdgpu_gem_align_pitch(struct amdgpu_device *adev,
-				  int width,
-				  int cpp,
-				  bool tiled)
-{
-	int aligned = width;
-	int pitch_mask = 0;
-
-	switch (cpp) {
-	case 1:
-		pitch_mask = 255;
-		break;
-	case 2:
-		pitch_mask = 127;
-		break;
-	case 3:
-	case 4:
-		pitch_mask = 63;
-		break;
-	}
-
-	aligned += pitch_mask;
-	aligned &= ~pitch_mask;
-	return aligned * cpp;
-}
-
 int amdgpu_mode_dumb_create(struct drm_file *file_priv,
-			    struct drm_device *dev,
-			    struct drm_mode_create_dumb *args)
+							struct drm_device *dev,
+							struct drm_mode_create_dumb *args)
 {
 	struct amdgpu_device *adev = drm_to_adev(dev);
 	struct amdgpu_fpriv *fpriv = file_priv->driver_priv;
 	struct drm_gem_object *gobj;
 	uint32_t handle;
 	u64 flags = AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |
-		    AMDGPU_GEM_CREATE_CPU_GTT_USWC |
-		    AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;
+	AMDGPU_GEM_CREATE_CPU_GTT_USWC |
+	AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;
 	u32 domain;
 	int r;
 
-	/*
-	 * The buffer returned from this function should be cleared, but
-	 * it can only be done if the ring is enabled or we'll fail to
-	 * create the buffer.
-	 */
 	if (adev->mman.buffer_funcs_enabled)
 		flags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;
 
 	args->pitch = amdgpu_gem_align_pitch(adev, args->width,
-					     DIV_ROUND_UP(args->bpp, 8), 0);
+										 DIV_ROUND_UP(args->bpp, 8), 0);
 	args->size = (u64)args->pitch * args->height;
 	args->size = ALIGN(args->size, PAGE_SIZE);
 	domain = amdgpu_bo_get_preferred_domain(adev,
-				amdgpu_display_supported_domains(adev, flags));
+											amdgpu_display_supported_domains(adev, flags));
+
+	if (is_hbm2_vega(adev)) {
+		uint32_t alignment = 0;
+		uint64_t optimized_size = args->size;
+
+		amdgpu_vega_optimize_hbm2_bank_access(adev, NULL, &optimized_size, &alignment);
+
+		r = amdgpu_gem_object_create(adev, optimized_size, alignment, domain, flags,
+									 ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
+		if (r == 0) {
+			r = drm_gem_handle_create(file_priv, gobj, &handle);
+			drm_gem_object_put(gobj);
+			if (r)
+				return r;
+
+			args->handle = handle;
+			return 0;
+		}
+	}
+
 	r = amdgpu_gem_object_create(adev, args->size, 0, domain, flags,
-				     ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
+								 ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
 	if (r)
 		return -ENOMEM;
 
 	r = drm_gem_handle_create(file_priv, gobj, &handle);
-	/* drop reference from allocate - handle holds it now */
 	drm_gem_object_put(gobj);
 	if (r)
 		return r;
@@ -1000,23 +1829,16 @@ static int amdgpu_debugfs_gem_info_show(
 		struct pid *pid;
 		int id;
 
-		/*
-		 * Although we have a valid reference on file->pid, that does
-		 * not guarantee that the task_struct who called get_pid() is
-		 * still alive (e.g. get_pid(current) => fork() => exit()).
-		 * Therefore, we need to protect this ->comm access using RCU.
-		 */
 		rcu_read_lock();
 		pid = rcu_dereference(file->pid);
 		task = pid_task(pid, PIDTYPE_TGID);
 		seq_printf(m, "pid %8d command %s:\n", pid_nr(pid),
-			   task ? task->comm : "<unknown>");
+				   task ? task->comm : "<unknown>");
 		rcu_read_unlock();
 
 		spin_lock(&file->table_lock);
 		idr_for_each_entry(&file->object_idr, gobj, id) {
 			struct amdgpu_bo *bo = gem_to_amdgpu_bo(gobj);
-
 			amdgpu_bo_print_info(id, bo, m);
 		}
 		spin_unlock(&file->table_lock);
@@ -1032,11 +1854,11 @@ DEFINE_SHOW_ATTRIBUTE(amdgpu_debugfs_gem
 
 void amdgpu_debugfs_gem_init(struct amdgpu_device *adev)
 {
-#if defined(CONFIG_DEBUG_FS)
+	#if defined(CONFIG_DEBUG_FS)
 	struct drm_minor *minor = adev_to_drm(adev)->primary;
 	struct dentry *root = minor->debugfs_root;
 
 	debugfs_create_file("amdgpu_gem_info", 0444, root, adev,
-			    &amdgpu_debugfs_gem_info_fops);
-#endif
+						&amdgpu_debugfs_gem_info_fops);
+	#endif
 }


--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c	2025-04-16 11:25:37.255871538 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c	2025-04-16 11:36:44.545585709 +0200
@@ -174,6 +174,7 @@ uint amdgpu_sdma_phase_quantum = 32;
 char *amdgpu_disable_cu;
 char *amdgpu_virtual_display;
 bool enforce_isolation;
+extern void amdgpu_vega_vram_thresholds_init(void);
 
 /* Specifies the default granularity for SVM, used in buffer
  * migration and restoration of backing memory when handling
@@ -2346,6 +2347,7 @@ static int amdgpu_pci_probe(struct pci_d
 	pci_set_drvdata(pdev, ddev);
 
 	amdgpu_init_debug_options(adev);
+	amdgpu_vega_vram_thresholds_init();
 
 	ret = amdgpu_driver_load_kms(adev, flags);
 	if (ret)

--- a/drivers/gpu/drm/amd/amdgpu/gfx_v9_0.c	2025-03-19 20:16:41.085579524 +0100
+++ b/drivers/gpu/drm/amd/amdgpu/gfx_v9_0.c	2025-05-17 23:26:52.407738920 +0200
@@ -150,6 +150,61 @@ MODULE_FIRMWARE("amdgpu/aldebaran_sjt_me
 #define mmGOLDEN_TSC_COUNT_LOWER_Renoir                0x0026
 #define mmGOLDEN_TSC_COUNT_LOWER_Renoir_BASE_IDX       1
 
+#ifndef GFX_V9_WREG_IF_CHANGED_H
+#define GFX_V9_WREG_IF_CHANGED_H
+
+#define WREG32_IF_CHANGED(_off_expr, _val_expr)                               \
+do {                                                                  \
+	const u32 __io_off = (_off_expr);                             \
+	const u32 __io_val = (_val_expr);                             \
+	const u32 __io_old = RREG32(__io_off);                        \
+	if (unlikely(__io_old != __io_val))                           \
+		WREG32(__io_off, __io_val);                           \
+} while (0)
+
+#define WREG32_SOC15_IF_CHANGED(ip, inst, reg, val_expr)                      \
+WREG32_IF_CHANGED(SOC15_REG_OFFSET(ip, inst, reg), (val_expr))
+
+#endif /* GFX_V9_WREG_IF_CHANGED_H */
+
+static __always_inline int
+gfx9_wait_reg_off(struct amdgpu_device *adev, u32 reg_offset_in_block,
+				  u32 mask, u32 val_target, unsigned long timeout_us)
+{
+	u32 current_read_val;
+	ktime_t timeout_expire;
+
+	if (timeout_us == 0) {
+		timeout_us = 2;
+	}
+
+	timeout_expire = ktime_add_us(ktime_get(), timeout_us);
+
+	do {
+		current_read_val = RREG32(reg_offset_in_block);
+		if ((current_read_val & mask) == val_target)
+			return 0;
+
+		cpu_relax();
+
+		if (timeout_us > 20 && ktime_before(ktime_get(), ktime_sub_us(timeout_expire, 1))) {
+			udelay(1);
+		} else if (timeout_us <= 20) {
+
+		}
+
+	} while (ktime_before(ktime_get(), timeout_expire));
+
+	current_read_val = RREG32(reg_offset_in_block);
+	if ((current_read_val & mask) == val_target)
+		return 0;
+
+	return -ETIMEDOUT;
+}
+
+
+static int gfx_v9_0_sw_fini(struct amdgpu_ip_block *ip_block);
+
 static const struct amdgpu_hwip_reg_entry gc_reg_list_9[] = {
 	SOC15_REG_ENTRY_STR(GC, 0, mmGRBM_STATUS),
 	SOC15_REG_ENTRY_STR(GC, 0, mmGRBM_STATUS2),
@@ -225,13 +280,11 @@ static const struct amdgpu_hwip_reg_entr
 	SOC15_REG_ENTRY_STR(GC, 0, mmRLC_SMU_SAFE_MODE),
 	SOC15_REG_ENTRY_STR(GC, 0, mmRLC_INT_STAT),
 	SOC15_REG_ENTRY_STR(GC, 0, mmRLC_GPM_GENERAL_6),
-	/* cp header registers */
 	SOC15_REG_ENTRY_STR(GC, 0, mmCP_CE_HEADER_DUMP),
 	SOC15_REG_ENTRY_STR(GC, 0, mmCP_MEC_ME1_HEADER_DUMP),
 	SOC15_REG_ENTRY_STR(GC, 0, mmCP_MEC_ME2_HEADER_DUMP),
 	SOC15_REG_ENTRY_STR(GC, 0, mmCP_PFP_HEADER_DUMP),
 	SOC15_REG_ENTRY_STR(GC, 0, mmCP_ME_HEADER_DUMP),
-	/* SE status registers */
 	SOC15_REG_ENTRY_STR(GC, 0, mmGRBM_STATUS_SE0),
 	SOC15_REG_ENTRY_STR(GC, 0, mmGRBM_STATUS_SE1),
 	SOC15_REG_ENTRY_STR(GC, 0, mmGRBM_STATUS_SE2),
@@ -239,7 +292,6 @@ static const struct amdgpu_hwip_reg_entr
 };
 
 static const struct amdgpu_hwip_reg_entry gc_cp_reg_list_9[] = {
-	/* compute queue registers */
 	SOC15_REG_ENTRY_STR(GC, 0, mmCP_HQD_VMID),
 	SOC15_REG_ENTRY_STR(GC, 0, mmCP_HQD_ACTIVE),
 	SOC15_REG_ENTRY_STR(GC, 0, mmCP_HQD_PERSISTENT_STATE),
@@ -280,7 +332,6 @@ static const struct amdgpu_hwip_reg_entr
 };
 
 enum ta_ras_gfx_subblock {
-	/*CPC*/
 	TA_RAS_BLOCK__GFX_CPC_INDEX_START = 0,
 	TA_RAS_BLOCK__GFX_CPC_SCRATCH = TA_RAS_BLOCK__GFX_CPC_INDEX_START,
 	TA_RAS_BLOCK__GFX_CPC_UCODE,
@@ -291,19 +342,16 @@ enum ta_ras_gfx_subblock {
 	TA_RAS_BLOCK__GFX_DC_CSINVOC_ME2,
 	TA_RAS_BLOCK__GFX_DC_RESTORE_ME2,
 	TA_RAS_BLOCK__GFX_CPC_INDEX_END = TA_RAS_BLOCK__GFX_DC_RESTORE_ME2,
-	/* CPF*/
 	TA_RAS_BLOCK__GFX_CPF_INDEX_START,
 	TA_RAS_BLOCK__GFX_CPF_ROQ_ME2 = TA_RAS_BLOCK__GFX_CPF_INDEX_START,
 	TA_RAS_BLOCK__GFX_CPF_ROQ_ME1,
 	TA_RAS_BLOCK__GFX_CPF_TAG,
 	TA_RAS_BLOCK__GFX_CPF_INDEX_END = TA_RAS_BLOCK__GFX_CPF_TAG,
-	/* CPG*/
 	TA_RAS_BLOCK__GFX_CPG_INDEX_START,
 	TA_RAS_BLOCK__GFX_CPG_DMA_ROQ = TA_RAS_BLOCK__GFX_CPG_INDEX_START,
 	TA_RAS_BLOCK__GFX_CPG_DMA_TAG,
 	TA_RAS_BLOCK__GFX_CPG_TAG,
 	TA_RAS_BLOCK__GFX_CPG_INDEX_END = TA_RAS_BLOCK__GFX_CPG_TAG,
-	/* GDS*/
 	TA_RAS_BLOCK__GFX_GDS_INDEX_START,
 	TA_RAS_BLOCK__GFX_GDS_MEM = TA_RAS_BLOCK__GFX_GDS_INDEX_START,
 	TA_RAS_BLOCK__GFX_GDS_INPUT_QUEUE,
@@ -311,21 +359,17 @@ enum ta_ras_gfx_subblock {
 	TA_RAS_BLOCK__GFX_GDS_OA_PHY_DATA_RAM_MEM,
 	TA_RAS_BLOCK__GFX_GDS_OA_PIPE_MEM,
 	TA_RAS_BLOCK__GFX_GDS_INDEX_END = TA_RAS_BLOCK__GFX_GDS_OA_PIPE_MEM,
-	/* SPI*/
 	TA_RAS_BLOCK__GFX_SPI_SR_MEM,
-	/* SQ*/
 	TA_RAS_BLOCK__GFX_SQ_INDEX_START,
 	TA_RAS_BLOCK__GFX_SQ_SGPR = TA_RAS_BLOCK__GFX_SQ_INDEX_START,
 	TA_RAS_BLOCK__GFX_SQ_LDS_D,
 	TA_RAS_BLOCK__GFX_SQ_LDS_I,
-	TA_RAS_BLOCK__GFX_SQ_VGPR, /* VGPR = SP*/
+	TA_RAS_BLOCK__GFX_SQ_VGPR,
 	TA_RAS_BLOCK__GFX_SQ_INDEX_END = TA_RAS_BLOCK__GFX_SQ_VGPR,
-	/* SQC (3 ranges)*/
 	TA_RAS_BLOCK__GFX_SQC_INDEX_START,
-	/* SQC range 0*/
 	TA_RAS_BLOCK__GFX_SQC_INDEX0_START = TA_RAS_BLOCK__GFX_SQC_INDEX_START,
 	TA_RAS_BLOCK__GFX_SQC_INST_UTCL1_LFIFO =
-		TA_RAS_BLOCK__GFX_SQC_INDEX0_START,
+	TA_RAS_BLOCK__GFX_SQC_INDEX0_START,
 	TA_RAS_BLOCK__GFX_SQC_DATA_CU0_WRITE_DATA_BUF,
 	TA_RAS_BLOCK__GFX_SQC_DATA_CU0_UTCL1_LFIFO,
 	TA_RAS_BLOCK__GFX_SQC_DATA_CU1_WRITE_DATA_BUF,
@@ -333,11 +377,10 @@ enum ta_ras_gfx_subblock {
 	TA_RAS_BLOCK__GFX_SQC_DATA_CU2_WRITE_DATA_BUF,
 	TA_RAS_BLOCK__GFX_SQC_DATA_CU2_UTCL1_LFIFO,
 	TA_RAS_BLOCK__GFX_SQC_INDEX0_END =
-		TA_RAS_BLOCK__GFX_SQC_DATA_CU2_UTCL1_LFIFO,
-	/* SQC range 1*/
+	TA_RAS_BLOCK__GFX_SQC_DATA_CU2_UTCL1_LFIFO,
 	TA_RAS_BLOCK__GFX_SQC_INDEX1_START,
 	TA_RAS_BLOCK__GFX_SQC_INST_BANKA_TAG_RAM =
-		TA_RAS_BLOCK__GFX_SQC_INDEX1_START,
+	TA_RAS_BLOCK__GFX_SQC_INDEX1_START,
 	TA_RAS_BLOCK__GFX_SQC_INST_BANKA_UTCL1_MISS_FIFO,
 	TA_RAS_BLOCK__GFX_SQC_INST_BANKA_MISS_FIFO,
 	TA_RAS_BLOCK__GFX_SQC_INST_BANKA_BANK_RAM,
@@ -347,11 +390,10 @@ enum ta_ras_gfx_subblock {
 	TA_RAS_BLOCK__GFX_SQC_DATA_BANKA_DIRTY_BIT_RAM,
 	TA_RAS_BLOCK__GFX_SQC_DATA_BANKA_BANK_RAM,
 	TA_RAS_BLOCK__GFX_SQC_INDEX1_END =
-		TA_RAS_BLOCK__GFX_SQC_DATA_BANKA_BANK_RAM,
-	/* SQC range 2*/
+	TA_RAS_BLOCK__GFX_SQC_DATA_BANKA_BANK_RAM,
 	TA_RAS_BLOCK__GFX_SQC_INDEX2_START,
 	TA_RAS_BLOCK__GFX_SQC_INST_BANKB_TAG_RAM =
-		TA_RAS_BLOCK__GFX_SQC_INDEX2_START,
+	TA_RAS_BLOCK__GFX_SQC_INDEX2_START,
 	TA_RAS_BLOCK__GFX_SQC_INST_BANKB_UTCL1_MISS_FIFO,
 	TA_RAS_BLOCK__GFX_SQC_INST_BANKB_MISS_FIFO,
 	TA_RAS_BLOCK__GFX_SQC_INST_BANKB_BANK_RAM,
@@ -361,9 +403,8 @@ enum ta_ras_gfx_subblock {
 	TA_RAS_BLOCK__GFX_SQC_DATA_BANKB_DIRTY_BIT_RAM,
 	TA_RAS_BLOCK__GFX_SQC_DATA_BANKB_BANK_RAM,
 	TA_RAS_BLOCK__GFX_SQC_INDEX2_END =
-		TA_RAS_BLOCK__GFX_SQC_DATA_BANKB_BANK_RAM,
+	TA_RAS_BLOCK__GFX_SQC_DATA_BANKB_BANK_RAM,
 	TA_RAS_BLOCK__GFX_SQC_INDEX_END = TA_RAS_BLOCK__GFX_SQC_INDEX2_END,
-	/* TA*/
 	TA_RAS_BLOCK__GFX_TA_INDEX_START,
 	TA_RAS_BLOCK__GFX_TA_FS_DFIFO = TA_RAS_BLOCK__GFX_TA_INDEX_START,
 	TA_RAS_BLOCK__GFX_TA_FS_AFIFO,
@@ -371,14 +412,11 @@ enum ta_ras_gfx_subblock {
 	TA_RAS_BLOCK__GFX_TA_FX_LFIFO,
 	TA_RAS_BLOCK__GFX_TA_FS_CFIFO,
 	TA_RAS_BLOCK__GFX_TA_INDEX_END = TA_RAS_BLOCK__GFX_TA_FS_CFIFO,
-	/* TCA*/
 	TA_RAS_BLOCK__GFX_TCA_INDEX_START,
 	TA_RAS_BLOCK__GFX_TCA_HOLE_FIFO = TA_RAS_BLOCK__GFX_TCA_INDEX_START,
 	TA_RAS_BLOCK__GFX_TCA_REQ_FIFO,
 	TA_RAS_BLOCK__GFX_TCA_INDEX_END = TA_RAS_BLOCK__GFX_TCA_REQ_FIFO,
-	/* TCC (5 sub-ranges)*/
 	TA_RAS_BLOCK__GFX_TCC_INDEX_START,
-	/* TCC range 0*/
 	TA_RAS_BLOCK__GFX_TCC_INDEX0_START = TA_RAS_BLOCK__GFX_TCC_INDEX_START,
 	TA_RAS_BLOCK__GFX_TCC_CACHE_DATA = TA_RAS_BLOCK__GFX_TCC_INDEX0_START,
 	TA_RAS_BLOCK__GFX_TCC_CACHE_DATA_BANK_0_1,
@@ -389,13 +427,11 @@ enum ta_ras_gfx_subblock {
 	TA_RAS_BLOCK__GFX_TCC_HIGH_RATE_TAG,
 	TA_RAS_BLOCK__GFX_TCC_LOW_RATE_TAG,
 	TA_RAS_BLOCK__GFX_TCC_INDEX0_END = TA_RAS_BLOCK__GFX_TCC_LOW_RATE_TAG,
-	/* TCC range 1*/
 	TA_RAS_BLOCK__GFX_TCC_INDEX1_START,
 	TA_RAS_BLOCK__GFX_TCC_IN_USE_DEC = TA_RAS_BLOCK__GFX_TCC_INDEX1_START,
 	TA_RAS_BLOCK__GFX_TCC_IN_USE_TRANSFER,
 	TA_RAS_BLOCK__GFX_TCC_INDEX1_END =
-		TA_RAS_BLOCK__GFX_TCC_IN_USE_TRANSFER,
-	/* TCC range 2*/
+	TA_RAS_BLOCK__GFX_TCC_IN_USE_TRANSFER,
 	TA_RAS_BLOCK__GFX_TCC_INDEX2_START,
 	TA_RAS_BLOCK__GFX_TCC_RETURN_DATA = TA_RAS_BLOCK__GFX_TCC_INDEX2_START,
 	TA_RAS_BLOCK__GFX_TCC_RETURN_CONTROL,
@@ -406,24 +442,20 @@ enum ta_ras_gfx_subblock {
 	TA_RAS_BLOCK__GFX_TCC_SRC_FIFO_NEXT_RAM,
 	TA_RAS_BLOCK__GFX_TCC_CACHE_TAG_PROBE_FIFO,
 	TA_RAS_BLOCK__GFX_TCC_INDEX2_END =
-		TA_RAS_BLOCK__GFX_TCC_CACHE_TAG_PROBE_FIFO,
-	/* TCC range 3*/
+	TA_RAS_BLOCK__GFX_TCC_CACHE_TAG_PROBE_FIFO,
 	TA_RAS_BLOCK__GFX_TCC_INDEX3_START,
 	TA_RAS_BLOCK__GFX_TCC_LATENCY_FIFO = TA_RAS_BLOCK__GFX_TCC_INDEX3_START,
 	TA_RAS_BLOCK__GFX_TCC_LATENCY_FIFO_NEXT_RAM,
 	TA_RAS_BLOCK__GFX_TCC_INDEX3_END =
-		TA_RAS_BLOCK__GFX_TCC_LATENCY_FIFO_NEXT_RAM,
-	/* TCC range 4*/
+	TA_RAS_BLOCK__GFX_TCC_LATENCY_FIFO_NEXT_RAM,
 	TA_RAS_BLOCK__GFX_TCC_INDEX4_START,
 	TA_RAS_BLOCK__GFX_TCC_WRRET_TAG_WRITE_RETURN =
-		TA_RAS_BLOCK__GFX_TCC_INDEX4_START,
+	TA_RAS_BLOCK__GFX_TCC_INDEX4_START,
 	TA_RAS_BLOCK__GFX_TCC_ATOMIC_RETURN_BUFFER,
 	TA_RAS_BLOCK__GFX_TCC_INDEX4_END =
-		TA_RAS_BLOCK__GFX_TCC_ATOMIC_RETURN_BUFFER,
+	TA_RAS_BLOCK__GFX_TCC_ATOMIC_RETURN_BUFFER,
 	TA_RAS_BLOCK__GFX_TCC_INDEX_END = TA_RAS_BLOCK__GFX_TCC_INDEX4_END,
-	/* TCI*/
 	TA_RAS_BLOCK__GFX_TCI_WRITE_RAM,
-	/* TCP*/
 	TA_RAS_BLOCK__GFX_TCP_INDEX_START,
 	TA_RAS_BLOCK__GFX_TCP_CACHE_RAM = TA_RAS_BLOCK__GFX_TCP_INDEX_START,
 	TA_RAS_BLOCK__GFX_TCP_LFIFO_RAM,
@@ -433,15 +465,12 @@ enum ta_ras_gfx_subblock {
 	TA_RAS_BLOCK__GFX_TCP_UTCL1_LFIFO0,
 	TA_RAS_BLOCK__GFX_TCP_UTCL1_LFIFO1,
 	TA_RAS_BLOCK__GFX_TCP_INDEX_END = TA_RAS_BLOCK__GFX_TCP_UTCL1_LFIFO1,
-	/* TD*/
 	TA_RAS_BLOCK__GFX_TD_INDEX_START,
 	TA_RAS_BLOCK__GFX_TD_SS_FIFO_LO = TA_RAS_BLOCK__GFX_TD_INDEX_START,
 	TA_RAS_BLOCK__GFX_TD_SS_FIFO_HI,
 	TA_RAS_BLOCK__GFX_TD_CS_FIFO,
 	TA_RAS_BLOCK__GFX_TD_INDEX_END = TA_RAS_BLOCK__GFX_TD_CS_FIFO,
-	/* EA (3 sub-ranges)*/
 	TA_RAS_BLOCK__GFX_EA_INDEX_START,
-	/* EA range 0*/
 	TA_RAS_BLOCK__GFX_EA_INDEX0_START = TA_RAS_BLOCK__GFX_EA_INDEX_START,
 	TA_RAS_BLOCK__GFX_EA_DRAMRD_CMDMEM = TA_RAS_BLOCK__GFX_EA_INDEX0_START,
 	TA_RAS_BLOCK__GFX_EA_DRAMWR_CMDMEM,
@@ -452,7 +481,6 @@ enum ta_ras_gfx_subblock {
 	TA_RAS_BLOCK__GFX_EA_GMIWR_CMDMEM,
 	TA_RAS_BLOCK__GFX_EA_GMIWR_DATAMEM,
 	TA_RAS_BLOCK__GFX_EA_INDEX0_END = TA_RAS_BLOCK__GFX_EA_GMIWR_DATAMEM,
-	/* EA range 1*/
 	TA_RAS_BLOCK__GFX_EA_INDEX1_START,
 	TA_RAS_BLOCK__GFX_EA_DRAMRD_PAGEMEM = TA_RAS_BLOCK__GFX_EA_INDEX1_START,
 	TA_RAS_BLOCK__GFX_EA_DRAMWR_PAGEMEM,
@@ -462,7 +490,6 @@ enum ta_ras_gfx_subblock {
 	TA_RAS_BLOCK__GFX_EA_GMIRD_PAGEMEM,
 	TA_RAS_BLOCK__GFX_EA_GMIWR_PAGEMEM,
 	TA_RAS_BLOCK__GFX_EA_INDEX1_END = TA_RAS_BLOCK__GFX_EA_GMIWR_PAGEMEM,
-	/* EA range 2*/
 	TA_RAS_BLOCK__GFX_EA_INDEX2_START,
 	TA_RAS_BLOCK__GFX_EA_MAM_D0MEM = TA_RAS_BLOCK__GFX_EA_INDEX2_START,
 	TA_RAS_BLOCK__GFX_EA_MAM_D1MEM,
@@ -470,13 +497,9 @@ enum ta_ras_gfx_subblock {
 	TA_RAS_BLOCK__GFX_EA_MAM_D3MEM,
 	TA_RAS_BLOCK__GFX_EA_INDEX2_END = TA_RAS_BLOCK__GFX_EA_MAM_D3MEM,
 	TA_RAS_BLOCK__GFX_EA_INDEX_END = TA_RAS_BLOCK__GFX_EA_INDEX2_END,
-	/* UTC VM L2 bank*/
 	TA_RAS_BLOCK__UTC_VML2_BANK_CACHE,
-	/* UTC VM walker*/
 	TA_RAS_BLOCK__UTC_VML2_WALKER,
-	/* UTC ATC L2 2MB cache*/
 	TA_RAS_BLOCK__UTC_ATCL2_CACHE_2M_BANK,
-	/* UTC ATC L2 4KB cache*/
 	TA_RAS_BLOCK__UTC_ATCL2_CACHE_4K_BANK,
 	TA_RAS_BLOCK__GFX_MAX
 };
@@ -489,161 +512,126 @@ struct ras_gfx_subblock {
 };
 
 #define AMDGPU_RAS_SUB_BLOCK(subblock, a, b, c, d, e, f, g, h)                             \
-	[AMDGPU_RAS_BLOCK__##subblock] = {                                     \
-		#subblock,                                                     \
-		TA_RAS_BLOCK__##subblock,                                      \
-		((a) | ((b) << 1) | ((c) << 2) | ((d) << 3)),                  \
-		(((e) << 1) | ((f) << 3) | (g) | ((h) << 2)),                  \
-	}
+[AMDGPU_RAS_BLOCK__##subblock] = {                                     \
+	#subblock,                                                     \
+	TA_RAS_BLOCK__##subblock,                                      \
+	((a) | ((b) << 1) | ((c) << 2) | ((d) << 3)),                  \
+	(((e) << 1) | ((f) << 3) | (g) | ((h) << 2)),                  \
+}
 
 static const struct ras_gfx_subblock ras_gfx_subblocks[] = {
-	AMDGPU_RAS_SUB_BLOCK(GFX_CPC_SCRATCH, 0, 1, 1, 1, 1, 0, 0, 1),
-	AMDGPU_RAS_SUB_BLOCK(GFX_CPC_UCODE, 0, 1, 1, 1, 1, 0, 0, 1),
-	AMDGPU_RAS_SUB_BLOCK(GFX_DC_STATE_ME1, 1, 0, 0, 1, 0, 0, 1, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_DC_CSINVOC_ME1, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_DC_RESTORE_ME1, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_DC_STATE_ME2, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_DC_CSINVOC_ME2, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_DC_RESTORE_ME2, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_CPF_ROQ_ME2, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_CPF_ROQ_ME1, 1, 0, 0, 1, 0, 0, 1, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_CPF_TAG, 0, 1, 1, 1, 1, 0, 0, 1),
-	AMDGPU_RAS_SUB_BLOCK(GFX_CPG_DMA_ROQ, 1, 0, 0, 1, 0, 0, 1, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_CPG_DMA_TAG, 0, 1, 1, 1, 0, 1, 0, 1),
-	AMDGPU_RAS_SUB_BLOCK(GFX_CPG_TAG, 0, 1, 1, 1, 1, 1, 0, 1),
-	AMDGPU_RAS_SUB_BLOCK(GFX_GDS_MEM, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_GDS_INPUT_QUEUE, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_GDS_OA_PHY_CMD_RAM_MEM, 0, 1, 1, 1, 0, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_GDS_OA_PHY_DATA_RAM_MEM, 1, 0, 0, 1, 0, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_GDS_OA_PIPE_MEM, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SPI_SR_MEM, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQ_SGPR, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQ_LDS_D, 0, 1, 1, 1, 1, 0, 0, 1),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQ_LDS_I, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQ_VGPR, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_INST_UTCL1_LFIFO, 0, 1, 1, 1, 0, 0, 0, 1),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_CU0_WRITE_DATA_BUF, 0, 1, 1, 1, 0, 0,
-			     0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_CU0_UTCL1_LFIFO, 0, 1, 1, 1, 0, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_CU1_WRITE_DATA_BUF, 0, 1, 1, 1, 0, 0,
-			     0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_CU1_UTCL1_LFIFO, 0, 1, 1, 1, 1, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_CU2_WRITE_DATA_BUF, 0, 1, 1, 1, 0, 0,
-			     0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_CU2_UTCL1_LFIFO, 0, 1, 1, 1, 0, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_INST_BANKA_TAG_RAM, 0, 1, 1, 1, 1, 0, 0,
-			     1),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_INST_BANKA_UTCL1_MISS_FIFO, 1, 0, 0, 1, 0,
-			     0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_INST_BANKA_MISS_FIFO, 1, 0, 0, 1, 0, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_INST_BANKA_BANK_RAM, 0, 1, 1, 1, 0, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_BANKA_TAG_RAM, 0, 1, 1, 1, 0, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_BANKA_HIT_FIFO, 1, 0, 0, 1, 0, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_BANKA_MISS_FIFO, 1, 0, 0, 1, 0, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_BANKA_DIRTY_BIT_RAM, 1, 0, 0, 1, 0, 0,
-			     0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_BANKA_BANK_RAM, 0, 1, 1, 1, 0, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_INST_BANKB_TAG_RAM, 0, 1, 1, 1, 1, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_INST_BANKB_UTCL1_MISS_FIFO, 1, 0, 0, 1, 0,
-			     0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_INST_BANKB_MISS_FIFO, 1, 0, 0, 1, 0, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_INST_BANKB_BANK_RAM, 0, 1, 1, 1, 0, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_BANKB_TAG_RAM, 0, 1, 1, 1, 0, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_BANKB_HIT_FIFO, 1, 0, 0, 1, 0, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_BANKB_MISS_FIFO, 1, 0, 0, 1, 0, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_BANKB_DIRTY_BIT_RAM, 1, 0, 0, 1, 0, 0,
-			     0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_BANKB_BANK_RAM, 0, 1, 1, 1, 0, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TA_FS_DFIFO, 0, 1, 1, 1, 1, 0, 0, 1),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TA_FS_AFIFO, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TA_FL_LFIFO, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TA_FX_LFIFO, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TA_FS_CFIFO, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCA_HOLE_FIFO, 1, 0, 0, 1, 0, 1, 1, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCA_REQ_FIFO, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_CACHE_DATA, 0, 1, 1, 1, 1, 0, 0, 1),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_CACHE_DATA_BANK_0_1, 0, 1, 1, 1, 1, 0, 0,
-			     1),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_CACHE_DATA_BANK_1_0, 0, 1, 1, 1, 1, 0, 0,
-			     1),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_CACHE_DATA_BANK_1_1, 0, 1, 1, 1, 1, 0, 0,
-			     1),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_CACHE_DIRTY_BANK_0, 0, 1, 1, 1, 0, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_CACHE_DIRTY_BANK_1, 0, 1, 1, 1, 0, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_HIGH_RATE_TAG, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_LOW_RATE_TAG, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_IN_USE_DEC, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_IN_USE_TRANSFER, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_RETURN_DATA, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_RETURN_CONTROL, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_UC_ATOMIC_FIFO, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_WRITE_RETURN, 1, 0, 0, 1, 0, 1, 1, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_WRITE_CACHE_READ, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_SRC_FIFO, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_SRC_FIFO_NEXT_RAM, 1, 0, 0, 1, 0, 0, 1, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_CACHE_TAG_PROBE_FIFO, 1, 0, 0, 1, 0, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_LATENCY_FIFO, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_LATENCY_FIFO_NEXT_RAM, 1, 0, 0, 1, 0, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_WRRET_TAG_WRITE_RETURN, 1, 0, 0, 1, 0, 0,
-			     0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCC_ATOMIC_RETURN_BUFFER, 1, 0, 0, 1, 0, 0, 0,
-			     0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCI_WRITE_RAM, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCP_CACHE_RAM, 0, 1, 1, 1, 1, 0, 0, 1),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCP_LFIFO_RAM, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCP_CMD_FIFO, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCP_VM_FIFO, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCP_DB_RAM, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCP_UTCL1_LFIFO0, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TCP_UTCL1_LFIFO1, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TD_SS_FIFO_LO, 0, 1, 1, 1, 1, 0, 0, 1),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TD_SS_FIFO_HI, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_TD_CS_FIFO, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_EA_DRAMRD_CMDMEM, 0, 1, 1, 1, 1, 0, 0, 1),
-	AMDGPU_RAS_SUB_BLOCK(GFX_EA_DRAMWR_CMDMEM, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_EA_DRAMWR_DATAMEM, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_EA_RRET_TAGMEM, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_EA_WRET_TAGMEM, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_EA_GMIRD_CMDMEM, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_EA_GMIWR_CMDMEM, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_EA_GMIWR_DATAMEM, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_EA_DRAMRD_PAGEMEM, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_EA_DRAMWR_PAGEMEM, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_EA_IORD_CMDMEM, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_EA_IOWR_CMDMEM, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_EA_IOWR_DATAMEM, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_EA_GMIRD_PAGEMEM, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_EA_GMIWR_PAGEMEM, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_EA_MAM_D0MEM, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_EA_MAM_D1MEM, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_EA_MAM_D2MEM, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(GFX_EA_MAM_D3MEM, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(UTC_VML2_BANK_CACHE, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(UTC_VML2_WALKER, 0, 1, 1, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(UTC_ATCL2_CACHE_2M_BANK, 1, 0, 0, 1, 0, 0, 0, 0),
-	AMDGPU_RAS_SUB_BLOCK(UTC_ATCL2_CACHE_4K_BANK, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_CPC_SCRATCH, 0, 1, 1, 1, 1, 0, 0, 1),
+AMDGPU_RAS_SUB_BLOCK(GFX_CPC_UCODE, 0, 1, 1, 1, 1, 0, 0, 1),
+AMDGPU_RAS_SUB_BLOCK(GFX_DC_STATE_ME1, 1, 0, 0, 1, 0, 0, 1, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_DC_CSINVOC_ME1, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_DC_RESTORE_ME1, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_DC_STATE_ME2, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_DC_CSINVOC_ME2, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_DC_RESTORE_ME2, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_CPF_ROQ_ME2, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_CPF_ROQ_ME1, 1, 0, 0, 1, 0, 0, 1, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_CPF_TAG, 0, 1, 1, 1, 1, 0, 0, 1),
+AMDGPU_RAS_SUB_BLOCK(GFX_CPG_DMA_ROQ, 1, 0, 0, 1, 0, 0, 1, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_CPG_DMA_TAG, 0, 1, 1, 1, 0, 1, 0, 1),
+AMDGPU_RAS_SUB_BLOCK(GFX_CPG_TAG, 0, 1, 1, 1, 1, 1, 0, 1),
+AMDGPU_RAS_SUB_BLOCK(GFX_GDS_MEM, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_GDS_INPUT_QUEUE, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_GDS_OA_PHY_CMD_RAM_MEM, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_GDS_OA_PHY_DATA_RAM_MEM, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_GDS_OA_PIPE_MEM, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SPI_SR_MEM, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQ_SGPR, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQ_LDS_D, 0, 1, 1, 1, 1, 0, 0, 1),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQ_LDS_I, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQ_VGPR, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_INST_UTCL1_LFIFO, 0, 1, 1, 1, 0, 0, 0, 1),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_CU0_WRITE_DATA_BUF, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_CU0_UTCL1_LFIFO, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_CU1_WRITE_DATA_BUF, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_CU1_UTCL1_LFIFO, 0, 1, 1, 1, 1, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_CU2_WRITE_DATA_BUF, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_CU2_UTCL1_LFIFO, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_INST_BANKA_TAG_RAM, 0, 1, 1, 1, 1, 0, 0, 1),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_INST_BANKA_UTCL1_MISS_FIFO, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_INST_BANKA_MISS_FIFO, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_INST_BANKA_BANK_RAM, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_BANKA_TAG_RAM, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_BANKA_HIT_FIFO, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_BANKA_MISS_FIFO, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_BANKA_DIRTY_BIT_RAM, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_BANKA_BANK_RAM, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_INST_BANKB_TAG_RAM, 0, 1, 1, 1, 1, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_INST_BANKB_UTCL1_MISS_FIFO, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_INST_BANKB_MISS_FIFO, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_INST_BANKB_BANK_RAM, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_BANKB_TAG_RAM, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_BANKB_HIT_FIFO, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_BANKB_MISS_FIFO, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_BANKB_DIRTY_BIT_RAM, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_SQC_DATA_BANKB_BANK_RAM, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TA_FS_DFIFO, 0, 1, 1, 1, 1, 0, 0, 1),
+AMDGPU_RAS_SUB_BLOCK(GFX_TA_FS_AFIFO, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TA_FL_LFIFO, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TA_FX_LFIFO, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TA_FS_CFIFO, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCA_HOLE_FIFO, 1, 0, 0, 1, 0, 1, 1, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCA_REQ_FIFO, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_CACHE_DATA, 0, 1, 1, 1, 1, 0, 0, 1),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_CACHE_DATA_BANK_0_1, 0, 1, 1, 1, 1, 0, 0, 1),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_CACHE_DATA_BANK_1_0, 0, 1, 1, 1, 1, 0, 0, 1),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_CACHE_DATA_BANK_1_1, 0, 1, 1, 1, 1, 0, 0, 1),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_CACHE_DIRTY_BANK_0, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_CACHE_DIRTY_BANK_1, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_HIGH_RATE_TAG, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_LOW_RATE_TAG, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_IN_USE_DEC, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_IN_USE_TRANSFER, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_RETURN_DATA, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_RETURN_CONTROL, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_UC_ATOMIC_FIFO, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_WRITE_RETURN, 1, 0, 0, 1, 0, 1, 1, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_WRITE_CACHE_READ, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_SRC_FIFO, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_SRC_FIFO_NEXT_RAM, 1, 0, 0, 1, 0, 0, 1, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_CACHE_TAG_PROBE_FIFO, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_LATENCY_FIFO, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_LATENCY_FIFO_NEXT_RAM, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_WRRET_TAG_WRITE_RETURN, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCC_ATOMIC_RETURN_BUFFER, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCI_WRITE_RAM, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCP_CACHE_RAM, 0, 1, 1, 1, 1, 0, 0, 1),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCP_LFIFO_RAM, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCP_CMD_FIFO, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCP_VM_FIFO, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCP_DB_RAM, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCP_UTCL1_LFIFO0, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TCP_UTCL1_LFIFO1, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TD_SS_FIFO_LO, 0, 1, 1, 1, 1, 0, 0, 1),
+AMDGPU_RAS_SUB_BLOCK(GFX_TD_SS_FIFO_HI, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_TD_CS_FIFO, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_EA_DRAMRD_CMDMEM, 0, 1, 1, 1, 1, 0, 0, 1),
+AMDGPU_RAS_SUB_BLOCK(GFX_EA_DRAMWR_CMDMEM, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_EA_DRAMWR_DATAMEM, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_EA_RRET_TAGMEM, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_EA_WRET_TAGMEM, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_EA_GMIRD_CMDMEM, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_EA_GMIWR_CMDMEM, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_EA_GMIWR_DATAMEM, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_EA_DRAMRD_PAGEMEM, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_EA_DRAMWR_PAGEMEM, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_EA_IORD_CMDMEM, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_EA_IOWR_CMDMEM, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_EA_IOWR_DATAMEM, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_EA_GMIRD_PAGEMEM, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_EA_GMIWR_PAGEMEM, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_EA_MAM_D0MEM, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_EA_MAM_D1MEM, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_EA_MAM_D2MEM, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(GFX_EA_MAM_D3MEM, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(UTC_VML2_BANK_CACHE, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(UTC_VML2_WALKER, 0, 1, 1, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(UTC_ATCL2_CACHE_2M_BANK, 1, 0, 0, 1, 0, 0, 0, 0),
+AMDGPU_RAS_SUB_BLOCK(UTC_ATCL2_CACHE_4K_BANK, 0, 1, 1, 1, 0, 0, 0, 0),
 };
 
 static const struct soc15_reg_golden golden_settings_gc_9_0[] =
@@ -883,68 +871,62 @@ static void gfx_v9_0_set_irq_funcs(struc
 static void gfx_v9_0_set_gds_init(struct amdgpu_device *adev);
 static void gfx_v9_0_set_rlc_funcs(struct amdgpu_device *adev);
 static int gfx_v9_0_get_cu_info(struct amdgpu_device *adev,
-				struct amdgpu_cu_info *cu_info);
+								struct amdgpu_cu_info *cu_info);
 static uint64_t gfx_v9_0_get_gpu_clock_counter(struct amdgpu_device *adev);
 static void gfx_v9_0_ring_emit_de_meta(struct amdgpu_ring *ring, bool resume, bool usegds);
 static u64 gfx_v9_0_ring_get_rptr_compute(struct amdgpu_ring *ring);
 static void gfx_v9_0_query_ras_error_count(struct amdgpu_device *adev,
-					  void *ras_error_status);
+										   void *ras_error_status);
 static int gfx_v9_0_ras_error_inject(struct amdgpu_device *adev,
-				     void *inject_if, uint32_t instance_mask);
+									 void *inject_if, uint32_t instance_mask);
 static void gfx_v9_0_reset_ras_error_count(struct amdgpu_device *adev);
 static void gfx_v9_0_update_spm_vmid_internal(struct amdgpu_device *adev,
-					      unsigned int vmid);
+  unsigned int vmid);
 static void gfx_v9_0_set_safe_mode(struct amdgpu_device *adev, int xcc_id);
 static void gfx_v9_0_unset_safe_mode(struct amdgpu_device *adev, int xcc_id);
 
 static void gfx_v9_0_kiq_set_resources(struct amdgpu_ring *kiq_ring,
-				uint64_t queue_mask)
+									   uint64_t queue_mask)
 {
 	struct amdgpu_device *adev = kiq_ring->adev;
 	u64 shader_mc_addr;
 
-	/* Cleaner shader MC address */
 	shader_mc_addr = adev->gfx.cleaner_shader_gpu_addr >> 8;
 
 	amdgpu_ring_write(kiq_ring, PACKET3(PACKET3_SET_RESOURCES, 6));
 	amdgpu_ring_write(kiq_ring,
-		PACKET3_SET_RESOURCES_VMID_MASK(0) |
-		/* vmid_mask:0* queue_type:0 (KIQ) */
-		PACKET3_SET_RESOURCES_QUEUE_TYPE(0));
+					  PACKET3_SET_RESOURCES_VMID_MASK(0) |
+					  PACKET3_SET_RESOURCES_QUEUE_TYPE(0));
 	amdgpu_ring_write(kiq_ring,
-			lower_32_bits(queue_mask));	/* queue mask lo */
+					  lower_32_bits(queue_mask));
 	amdgpu_ring_write(kiq_ring,
-			upper_32_bits(queue_mask));	/* queue mask hi */
-	amdgpu_ring_write(kiq_ring, lower_32_bits(shader_mc_addr)); /* cleaner shader addr lo */
-	amdgpu_ring_write(kiq_ring, upper_32_bits(shader_mc_addr)); /* cleaner shader addr hi */
-	amdgpu_ring_write(kiq_ring, 0);	/* oac mask */
-	amdgpu_ring_write(kiq_ring, 0);	/* gds heap base:0, gds heap size:0 */
+					  upper_32_bits(queue_mask));
+	amdgpu_ring_write(kiq_ring, lower_32_bits(shader_mc_addr));
+	amdgpu_ring_write(kiq_ring, upper_32_bits(shader_mc_addr));
+	amdgpu_ring_write(kiq_ring, 0);
+	amdgpu_ring_write(kiq_ring, 0);
 }
 
 static void gfx_v9_0_kiq_map_queues(struct amdgpu_ring *kiq_ring,
-				 struct amdgpu_ring *ring)
+									struct amdgpu_ring *ring)
 {
 	uint64_t mqd_addr = amdgpu_bo_gpu_offset(ring->mqd_obj);
 	uint64_t wptr_addr = ring->wptr_gpu_addr;
 	uint32_t eng_sel = ring->funcs->type == AMDGPU_RING_TYPE_GFX ? 4 : 0;
 
 	amdgpu_ring_write(kiq_ring, PACKET3(PACKET3_MAP_QUEUES, 5));
-	/* Q_sel:0, vmid:0, vidmem: 1, engine:0, num_Q:1*/
-	amdgpu_ring_write(kiq_ring, /* Q_sel: 0, vmid: 0, engine: 0, num_Q: 1 */
-			 PACKET3_MAP_QUEUES_QUEUE_SEL(0) | /* Queue_Sel */
-			 PACKET3_MAP_QUEUES_VMID(0) | /* VMID */
-			 PACKET3_MAP_QUEUES_QUEUE(ring->queue) |
-			 PACKET3_MAP_QUEUES_PIPE(ring->pipe) |
-			 PACKET3_MAP_QUEUES_ME((ring->me == 1 ? 0 : 1)) |
-			 /*queue_type: normal compute queue */
-			 PACKET3_MAP_QUEUES_QUEUE_TYPE(0) |
-			 /* alloc format: all_on_one_pipe */
-			 PACKET3_MAP_QUEUES_ALLOC_FORMAT(0) |
-			 PACKET3_MAP_QUEUES_ENGINE_SEL(eng_sel) |
-			 /* num_queues: must be 1 */
-			 PACKET3_MAP_QUEUES_NUM_QUEUES(1));
 	amdgpu_ring_write(kiq_ring,
-			PACKET3_MAP_QUEUES_DOORBELL_OFFSET(ring->doorbell_index));
+					  PACKET3_MAP_QUEUES_QUEUE_SEL(0) |
+					  PACKET3_MAP_QUEUES_VMID(0) |
+					  PACKET3_MAP_QUEUES_QUEUE(ring->queue) |
+					  PACKET3_MAP_QUEUES_PIPE(ring->pipe) |
+					  PACKET3_MAP_QUEUES_ME((ring->me == 1 ? 0 : 1)) |
+					  PACKET3_MAP_QUEUES_QUEUE_TYPE(0) |
+					  PACKET3_MAP_QUEUES_ALLOC_FORMAT(0) |
+					  PACKET3_MAP_QUEUES_ENGINE_SEL(eng_sel) |
+					  PACKET3_MAP_QUEUES_NUM_QUEUES(1));
+	amdgpu_ring_write(kiq_ring,
+					  PACKET3_MAP_QUEUES_DOORBELL_OFFSET(ring->doorbell_index));
 	amdgpu_ring_write(kiq_ring, lower_32_bits(mqd_addr));
 	amdgpu_ring_write(kiq_ring, upper_32_bits(mqd_addr));
 	amdgpu_ring_write(kiq_ring, lower_32_bits(wptr_addr));
@@ -952,20 +934,20 @@ static void gfx_v9_0_kiq_map_queues(stru
 }
 
 static void gfx_v9_0_kiq_unmap_queues(struct amdgpu_ring *kiq_ring,
-				   struct amdgpu_ring *ring,
-				   enum amdgpu_unmap_queues_action action,
-				   u64 gpu_addr, u64 seq)
+									  struct amdgpu_ring *ring,
+									  enum amdgpu_unmap_queues_action action,
+									  u64 gpu_addr, u64 seq)
 {
 	uint32_t eng_sel = ring->funcs->type == AMDGPU_RING_TYPE_GFX ? 4 : 0;
 
 	amdgpu_ring_write(kiq_ring, PACKET3(PACKET3_UNMAP_QUEUES, 4));
-	amdgpu_ring_write(kiq_ring, /* Q_sel: 0, vmid: 0, engine: 0, num_Q: 1 */
-			  PACKET3_UNMAP_QUEUES_ACTION(action) |
-			  PACKET3_UNMAP_QUEUES_QUEUE_SEL(0) |
-			  PACKET3_UNMAP_QUEUES_ENGINE_SEL(eng_sel) |
-			  PACKET3_UNMAP_QUEUES_NUM_QUEUES(1));
 	amdgpu_ring_write(kiq_ring,
-			PACKET3_UNMAP_QUEUES_DOORBELL_OFFSET0(ring->doorbell_index));
+					  PACKET3_UNMAP_QUEUES_ACTION(action) |
+					  PACKET3_UNMAP_QUEUES_QUEUE_SEL(0) |
+					  PACKET3_UNMAP_QUEUES_ENGINE_SEL(eng_sel) |
+					  PACKET3_UNMAP_QUEUES_NUM_QUEUES(1));
+	amdgpu_ring_write(kiq_ring,
+					  PACKET3_UNMAP_QUEUES_DOORBELL_OFFSET0(ring->doorbell_index));
 
 	if (action == PREEMPT_QUEUES_NO_UNMAP) {
 		amdgpu_ring_write(kiq_ring, lower_32_bits(ring->wptr & ring->buf_mask));
@@ -980,21 +962,20 @@ static void gfx_v9_0_kiq_unmap_queues(st
 }
 
 static void gfx_v9_0_kiq_query_status(struct amdgpu_ring *kiq_ring,
-				   struct amdgpu_ring *ring,
-				   u64 addr,
-				   u64 seq)
+									  struct amdgpu_ring *ring,
+									  u64 addr,
+									  u64 seq)
 {
 	uint32_t eng_sel = ring->funcs->type == AMDGPU_RING_TYPE_GFX ? 4 : 0;
 
 	amdgpu_ring_write(kiq_ring, PACKET3(PACKET3_QUERY_STATUS, 5));
 	amdgpu_ring_write(kiq_ring,
-			  PACKET3_QUERY_STATUS_CONTEXT_ID(0) |
-			  PACKET3_QUERY_STATUS_INTERRUPT_SEL(0) |
-			  PACKET3_QUERY_STATUS_COMMAND(2));
-	/* Q_sel: 0, vmid: 0, engine: 0, num_Q: 1 */
+					  PACKET3_QUERY_STATUS_CONTEXT_ID(0) |
+					  PACKET3_QUERY_STATUS_INTERRUPT_SEL(0) |
+					  PACKET3_QUERY_STATUS_COMMAND(2));
 	amdgpu_ring_write(kiq_ring,
-			PACKET3_QUERY_STATUS_DOORBELL_OFFSET(ring->doorbell_index) |
-			PACKET3_QUERY_STATUS_ENG_SEL(eng_sel));
+					  PACKET3_QUERY_STATUS_DOORBELL_OFFSET(ring->doorbell_index) |
+					  PACKET3_QUERY_STATUS_ENG_SEL(eng_sel));
 	amdgpu_ring_write(kiq_ring, lower_32_bits(addr));
 	amdgpu_ring_write(kiq_ring, upper_32_bits(addr));
 	amdgpu_ring_write(kiq_ring, lower_32_bits(seq));
@@ -1002,48 +983,56 @@ static void gfx_v9_0_kiq_query_status(st
 }
 
 static void gfx_v9_0_kiq_invalidate_tlbs(struct amdgpu_ring *kiq_ring,
-				uint16_t pasid, uint32_t flush_type,
-				bool all_hub)
+										 uint16_t pasid, uint32_t flush_type,
+										 bool all_hub)
 {
 	amdgpu_ring_write(kiq_ring, PACKET3(PACKET3_INVALIDATE_TLBS, 0));
 	amdgpu_ring_write(kiq_ring,
-			PACKET3_INVALIDATE_TLBS_DST_SEL(1) |
-			PACKET3_INVALIDATE_TLBS_ALL_HUB(all_hub) |
-			PACKET3_INVALIDATE_TLBS_PASID(pasid) |
-			PACKET3_INVALIDATE_TLBS_FLUSH_TYPE(flush_type));
+					  PACKET3_INVALIDATE_TLBS_DST_SEL(1) |
+					  PACKET3_INVALIDATE_TLBS_ALL_HUB(all_hub) |
+					  PACKET3_INVALIDATE_TLBS_PASID(pasid) |
+					  PACKET3_INVALIDATE_TLBS_FLUSH_TYPE(flush_type));
 }
 
 
-static void gfx_v9_0_kiq_reset_hw_queue(struct amdgpu_ring *kiq_ring, uint32_t queue_type,
-					uint32_t me_id, uint32_t pipe_id, uint32_t queue_id,
-					uint32_t xcc_id, uint32_t vmid)
+static inline void
+gfx_v9_0_kiq_reset_hw_queue(struct amdgpu_ring *kiq_ring,
+							u32 queue_type,
+							u32 me_id, u32 pipe_id, u32 queue_id,
+							u32 xcc_id, u32 vmid)
 {
 	struct amdgpu_device *adev = kiq_ring->adev;
-	unsigned i;
+	const unsigned long  tmo   = adev->usec_timeout / 5 + 1;
+	int r;
 
-	/* enter save mode */
 	amdgpu_gfx_rlc_enter_safe_mode(adev, xcc_id);
+
 	mutex_lock(&adev->srbm_mutex);
 	soc15_grbm_select(adev, me_id, pipe_id, queue_id, 0, 0);
+	mutex_unlock(&adev->srbm_mutex);
 
-	if (queue_type == AMDGPU_RING_TYPE_COMPUTE) {
-		WREG32_SOC15(GC, 0, mmCP_HQD_DEQUEUE_REQUEST, 0x2);
-		WREG32_SOC15(GC, 0, mmSPI_COMPUTE_QUEUE_RESET, 0x1);
-		/* wait till dequeue take effects */
-		for (i = 0; i < adev->usec_timeout; i++) {
-			if (!(RREG32_SOC15(GC, 0, mmCP_HQD_ACTIVE) & 1))
-				break;
-			udelay(1);
-		}
-		if (i >= adev->usec_timeout)
-			dev_err(adev->dev, "fail to wait on hqd deactive\n");
-	} else {
-		dev_err(adev->dev, "reset queue_type(%d) not supported\n", queue_type);
+	if (queue_type != AMDGPU_RING_TYPE_COMPUTE) {
+		dev_err_ratelimited(adev->dev,
+							"KIQ reset: queue_type %u not supported\n", queue_type);
+		goto restore;
 	}
 
+	WREG32_SOC15(GC, 0, mmCP_HQD_DEQUEUE_REQUEST, 0x2);
+	WREG32_SOC15(GC, 0, mmSPI_COMPUTE_QUEUE_RESET, 0x1);
+
+	r = gfx9_wait_reg_off(adev,
+						  SOC15_REG_OFFSET(GC, 0, mmCP_HQD_ACTIVE),
+						  CP_HQD_ACTIVE__ACTIVE_MASK, 0, tmo);
+	if (r)
+		dev_err_ratelimited(adev->dev,
+							"KIQ reset: HQD (ME%u/PIPE%u/Q%u) timeout\n",
+							me_id, pipe_id, queue_id);
+
+		restore:
+		mutex_lock(&adev->srbm_mutex);
 	soc15_grbm_select(adev, 0, 0, 0, 0, 0);
 	mutex_unlock(&adev->srbm_mutex);
-	/* exit safe mode */
+
 	amdgpu_gfx_rlc_exit_safe_mode(adev, xcc_id);
 }
 
@@ -1068,100 +1057,107 @@ static void gfx_v9_0_set_kiq_pm4_funcs(s
 
 static void gfx_v9_0_init_golden_registers(struct amdgpu_device *adev)
 {
-	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
-	case IP_VERSION(9, 0, 1):
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_0,
-						ARRAY_SIZE(golden_settings_gc_9_0));
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_0_vg10,
-						ARRAY_SIZE(golden_settings_gc_9_0_vg10));
-		break;
-	case IP_VERSION(9, 2, 1):
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_2_1,
-						ARRAY_SIZE(golden_settings_gc_9_2_1));
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_2_1_vg12,
-						ARRAY_SIZE(golden_settings_gc_9_2_1_vg12));
-		break;
-	case IP_VERSION(9, 4, 0):
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_0,
-						ARRAY_SIZE(golden_settings_gc_9_0));
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_0_vg20,
-						ARRAY_SIZE(golden_settings_gc_9_0_vg20));
-		break;
-	case IP_VERSION(9, 4, 1):
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_4_1_arct,
-						ARRAY_SIZE(golden_settings_gc_9_4_1_arct));
-		break;
-	case IP_VERSION(9, 2, 2):
-	case IP_VERSION(9, 1, 0):
-		soc15_program_register_sequence(adev, golden_settings_gc_9_1,
-						ARRAY_SIZE(golden_settings_gc_9_1));
-		if (adev->apu_flags & AMD_APU_IS_RAVEN2)
-			soc15_program_register_sequence(adev,
-							golden_settings_gc_9_1_rv2,
-							ARRAY_SIZE(golden_settings_gc_9_1_rv2));
+	const u32 ip = amdgpu_ip_version(adev, GC_HWIP, 0);
+
+	#define PROG_SEQ(arr)							\
+	do {								\
+		if (ARRAY_SIZE(arr))					\
+			soc15_program_register_sequence(adev,		\
+			(arr), (u32)ARRAY_SIZE(arr));		\
+	} while (0)
+
+	switch (ip) {
+		case IP_VERSION(9, 0, 1):
+			PROG_SEQ(golden_settings_gc_9_0);
+			PROG_SEQ(golden_settings_gc_9_0_vg10);
+			break;
+
+		case IP_VERSION(9, 2, 1):
+			PROG_SEQ(golden_settings_gc_9_2_1);
+			PROG_SEQ(golden_settings_gc_9_2_1_vg12);
+			break;
+
+		case IP_VERSION(9, 4, 0):
+			PROG_SEQ(golden_settings_gc_9_0);
+			PROG_SEQ(golden_settings_gc_9_0_vg20);
+			break;
+
+		case IP_VERSION(9, 4, 1):
+			PROG_SEQ(golden_settings_gc_9_4_1_arct);
+
+			break;
+
+		case IP_VERSION(9, 2, 2):
+		case IP_VERSION(9, 1, 0):
+			PROG_SEQ(golden_settings_gc_9_1);
+			if (adev->apu_flags & AMD_APU_IS_RAVEN2)
+				PROG_SEQ(golden_settings_gc_9_1_rv2);
 		else
-			soc15_program_register_sequence(adev,
-							golden_settings_gc_9_1_rv1,
-							ARRAY_SIZE(golden_settings_gc_9_1_rv1));
-		break;
-	 case IP_VERSION(9, 3, 0):
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_1_rn,
-						ARRAY_SIZE(golden_settings_gc_9_1_rn));
-		return; /* for renoir, don't need common goldensetting */
-	case IP_VERSION(9, 4, 2):
-		gfx_v9_4_2_init_golden_registers(adev,
-						 adev->smuio.funcs->get_die_id(adev));
-		break;
-	default:
+			PROG_SEQ(golden_settings_gc_9_1_rv1);
 		break;
+
+		case IP_VERSION(9, 3, 0):
+			PROG_SEQ(golden_settings_gc_9_1_rn);
+			goto skip_common;
+
+		case IP_VERSION(9, 4, 2):
+			gfx_v9_4_2_init_golden_registers(
+				adev, adev->smuio.funcs->get_die_id(adev));
+
+			goto skip_common;
+
+		default:
+
+
+			if (ARRAY_SIZE(golden_settings_gc_9_0)) {
+				soc15_program_register_sequence(adev,
+												golden_settings_gc_9_0,
+									ARRAY_SIZE(golden_settings_gc_9_0));
+			}
+			break;
 	}
 
-	if ((amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 4, 1)) &&
-	    (amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 4, 2)))
-		soc15_program_register_sequence(adev, golden_settings_gc_9_x_common,
-						(const u32)ARRAY_SIZE(golden_settings_gc_9_x_common));
+
+	if (ip != IP_VERSION(9, 4, 1) && ip != IP_VERSION(9, 4, 2))
+		PROG_SEQ(golden_settings_gc_9_x_common);
+
+	skip_common:
+
+	;
+	#undef PROG_SEQ
 }
 
 static void gfx_v9_0_write_data_to_reg(struct amdgpu_ring *ring, int eng_sel,
-				       bool wc, uint32_t reg, uint32_t val)
+									   bool wc, uint32_t reg, uint32_t val)
 {
 	amdgpu_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, 3));
 	amdgpu_ring_write(ring, WRITE_DATA_ENGINE_SEL(eng_sel) |
-				WRITE_DATA_DST_SEL(0) |
-				(wc ? WR_CONFIRM : 0));
+	WRITE_DATA_DST_SEL(0) |
+	(wc ? WR_CONFIRM : 0));
 	amdgpu_ring_write(ring, reg);
 	amdgpu_ring_write(ring, 0);
 	amdgpu_ring_write(ring, val);
 }
 
 static void gfx_v9_0_wait_reg_mem(struct amdgpu_ring *ring, int eng_sel,
-				  int mem_space, int opt, uint32_t addr0,
-				  uint32_t addr1, uint32_t ref, uint32_t mask,
-				  uint32_t inv)
+								  int mem_space, int opt, uint32_t addr0,
+								  uint32_t addr1, uint32_t ref, uint32_t mask,
+								  uint32_t inv)
 {
 	amdgpu_ring_write(ring, PACKET3(PACKET3_WAIT_REG_MEM, 5));
 	amdgpu_ring_write(ring,
-				 /* memory (1) or register (0) */
-				 (WAIT_REG_MEM_MEM_SPACE(mem_space) |
-				 WAIT_REG_MEM_OPERATION(opt) | /* wait */
-				 WAIT_REG_MEM_FUNCTION(3) |  /* equal */
-				 WAIT_REG_MEM_ENGINE(eng_sel)));
+					  (WAIT_REG_MEM_MEM_SPACE(mem_space) |
+					  WAIT_REG_MEM_OPERATION(opt) |
+					  WAIT_REG_MEM_FUNCTION(3) |
+					  WAIT_REG_MEM_ENGINE(eng_sel)));
 
 	if (mem_space)
-		BUG_ON(addr0 & 0x3); /* Dword align */
+		BUG_ON(addr0 & 0x3);
 	amdgpu_ring_write(ring, addr0);
 	amdgpu_ring_write(ring, addr1);
 	amdgpu_ring_write(ring, ref);
 	amdgpu_ring_write(ring, mask);
-	amdgpu_ring_write(ring, inv); /* poll interval */
+	amdgpu_ring_write(ring, inv);
 }
 
 static int gfx_v9_0_ring_test_ring(struct amdgpu_ring *ring)
@@ -1242,10 +1238,10 @@ static int gfx_v9_0_ring_test_ib(struct
 	else
 		r = -EINVAL;
 
-err2:
+	err2:
 	amdgpu_ib_free(&ib, NULL);
 	dma_fence_put(f);
-err1:
+	err1:
 	amdgpu_device_wb_free(adev, index);
 	return r;
 }
@@ -1269,63 +1265,63 @@ static void gfx_v9_0_check_fw_write_wait
 	adev->gfx.mec_fw_write_wait = false;
 
 	if ((amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 4, 1)) &&
-	    (amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 4, 2)) &&
-	    ((adev->gfx.mec_fw_version < 0x000001a5) ||
-	     (adev->gfx.mec_feature_version < 46) ||
-	     (adev->gfx.pfp_fw_version < 0x000000b7) ||
-	     (adev->gfx.pfp_feature_version < 46)))
+		(amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 4, 2)) &&
+		((adev->gfx.mec_fw_version < 0x000001a5) ||
+		(adev->gfx.mec_feature_version < 46) ||
+		(adev->gfx.pfp_fw_version < 0x000000b7) ||
+		(adev->gfx.pfp_feature_version < 46)))
 		DRM_WARN_ONCE("CP firmware version too old, please update!");
 
 	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
-	case IP_VERSION(9, 0, 1):
-		if ((adev->gfx.me_fw_version >= 0x0000009c) &&
-		    (adev->gfx.me_feature_version >= 42) &&
-		    (adev->gfx.pfp_fw_version >=  0x000000b1) &&
-		    (adev->gfx.pfp_feature_version >= 42))
-			adev->gfx.me_fw_write_wait = true;
-
-		if ((adev->gfx.mec_fw_version >=  0x00000193) &&
-		    (adev->gfx.mec_feature_version >= 42))
-			adev->gfx.mec_fw_write_wait = true;
-		break;
-	case IP_VERSION(9, 2, 1):
-		if ((adev->gfx.me_fw_version >= 0x0000009c) &&
-		    (adev->gfx.me_feature_version >= 44) &&
-		    (adev->gfx.pfp_fw_version >=  0x000000b2) &&
-		    (adev->gfx.pfp_feature_version >= 44))
-			adev->gfx.me_fw_write_wait = true;
-
-		if ((adev->gfx.mec_fw_version >=  0x00000196) &&
-		    (adev->gfx.mec_feature_version >= 44))
-			adev->gfx.mec_fw_write_wait = true;
-		break;
-	case IP_VERSION(9, 4, 0):
-		if ((adev->gfx.me_fw_version >= 0x0000009c) &&
-		    (adev->gfx.me_feature_version >= 44) &&
-		    (adev->gfx.pfp_fw_version >=  0x000000b2) &&
-		    (adev->gfx.pfp_feature_version >= 44))
-			adev->gfx.me_fw_write_wait = true;
-
-		if ((adev->gfx.mec_fw_version >=  0x00000197) &&
-		    (adev->gfx.mec_feature_version >= 44))
-			adev->gfx.mec_fw_write_wait = true;
-		break;
-	case IP_VERSION(9, 1, 0):
-	case IP_VERSION(9, 2, 2):
-		if ((adev->gfx.me_fw_version >= 0x0000009c) &&
-		    (adev->gfx.me_feature_version >= 42) &&
-		    (adev->gfx.pfp_fw_version >=  0x000000b1) &&
-		    (adev->gfx.pfp_feature_version >= 42))
+		case IP_VERSION(9, 0, 1):
+			if ((adev->gfx.me_fw_version >= 0x0000009c) &&
+				(adev->gfx.me_feature_version >= 42) &&
+				(adev->gfx.pfp_fw_version >=  0x000000b1) &&
+				(adev->gfx.pfp_feature_version >= 42))
+				adev->gfx.me_fw_write_wait = true;
+
+			if ((adev->gfx.mec_fw_version >=  0x00000193) &&
+				(adev->gfx.mec_feature_version >= 42))
+				adev->gfx.mec_fw_write_wait = true;
+			break;
+		case IP_VERSION(9, 2, 1):
+			if ((adev->gfx.me_fw_version >= 0x0000009c) &&
+				(adev->gfx.me_feature_version >= 44) &&
+				(adev->gfx.pfp_fw_version >=  0x000000b2) &&
+				(adev->gfx.pfp_feature_version >= 44))
+				adev->gfx.me_fw_write_wait = true;
+
+			if ((adev->gfx.mec_fw_version >=  0x00000196) &&
+				(adev->gfx.mec_feature_version >= 44))
+				adev->gfx.mec_fw_write_wait = true;
+			break;
+		case IP_VERSION(9, 4, 0):
+			if ((adev->gfx.me_fw_version >= 0x0000009c) &&
+				(adev->gfx.me_feature_version >= 44) &&
+				(adev->gfx.pfp_fw_version >=  0x000000b2) &&
+				(adev->gfx.pfp_feature_version >= 44))
+				adev->gfx.me_fw_write_wait = true;
+
+			if ((adev->gfx.mec_fw_version >=  0x00000197) &&
+				(adev->gfx.mec_feature_version >= 44))
+				adev->gfx.mec_fw_write_wait = true;
+			break;
+		case IP_VERSION(9, 1, 0):
+		case IP_VERSION(9, 2, 2):
+			if ((adev->gfx.me_fw_version >= 0x0000009c) &&
+				(adev->gfx.me_feature_version >= 42) &&
+				(adev->gfx.pfp_fw_version >=  0x000000b1) &&
+				(adev->gfx.pfp_feature_version >= 42))
+				adev->gfx.me_fw_write_wait = true;
+
+			if ((adev->gfx.mec_fw_version >=  0x00000192) &&
+				(adev->gfx.mec_feature_version >= 42))
+				adev->gfx.mec_fw_write_wait = true;
+			break;
+		default:
 			adev->gfx.me_fw_write_wait = true;
-
-		if ((adev->gfx.mec_fw_version >=  0x00000192) &&
-		    (adev->gfx.mec_feature_version >= 42))
 			adev->gfx.mec_fw_write_wait = true;
-		break;
-	default:
-		adev->gfx.me_fw_write_wait = true;
-		adev->gfx.mec_fw_write_wait = true;
-		break;
+			break;
 	}
 }
 
@@ -1338,17 +1334,11 @@ struct amdgpu_gfxoff_quirk {
 };
 
 static const struct amdgpu_gfxoff_quirk amdgpu_gfxoff_quirk_list[] = {
-	/* https://bugzilla.kernel.org/show_bug.cgi?id=204689 */
 	{ 0x1002, 0x15dd, 0x1002, 0x15dd, 0xc8 },
-	/* https://bugzilla.kernel.org/show_bug.cgi?id=207171 */
 	{ 0x1002, 0x15dd, 0x103c, 0x83e7, 0xd3 },
-	/* GFXOFF is unstable on C6 parts with a VBIOS 113-RAVEN-114 */
 	{ 0x1002, 0x15dd, 0x1002, 0x15dd, 0xc6 },
-	/* Apple MacBook Pro (15-inch, 2019) Radeon Pro Vega 20 4 GB */
 	{ 0x1002, 0x69af, 0x106b, 0x019a, 0xc0 },
-	/* https://bbs.openkylin.top/t/topic/171497 */
 	{ 0x1002, 0x15d8, 0x19e5, 0x3e14, 0xc2 },
-	/* HP 705G4 DM with R5 2400G */
 	{ 0x1002, 0x15dd, 0x103c, 0x8464, 0xd6 },
 	{ 0, 0, 0, 0, 0 },
 };
@@ -1359,13 +1349,13 @@ static bool gfx_v9_0_should_disable_gfxo
 
 	while (p && p->chip_device != 0) {
 		if (pdev->vendor == p->chip_vendor &&
-		    pdev->device == p->chip_device &&
-		    pdev->subsystem_vendor == p->subsys_vendor &&
-		    pdev->subsystem_device == p->subsys_device &&
-		    pdev->revision == p->revision) {
+			pdev->device == p->chip_device &&
+			pdev->subsystem_vendor == p->subsys_vendor &&
+			pdev->subsystem_device == p->subsys_device &&
+			pdev->revision == p->revision) {
 			return true;
-		}
-		++p;
+			}
+			++p;
 	}
 	return false;
 }
@@ -1381,8 +1371,8 @@ static bool is_raven_kicker(struct amdgp
 static bool check_if_enlarge_doorbell_range(struct amdgpu_device *adev)
 {
 	if ((amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 3, 0)) &&
-	    (adev->gfx.me_fw_version >= 0x000000a5) &&
-	    (adev->gfx.me_feature_version >= 52))
+		(adev->gfx.me_fw_version >= 0x000000a5) &&
+		(adev->gfx.me_feature_version >= 52))
 		return true;
 	else
 		return false;
@@ -1394,63 +1384,63 @@ static void gfx_v9_0_check_if_need_gfxof
 		adev->pm.pp_feature &= ~PP_GFXOFF_MASK;
 
 	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
-	case IP_VERSION(9, 0, 1):
-	case IP_VERSION(9, 2, 1):
-	case IP_VERSION(9, 4, 0):
-		break;
-	case IP_VERSION(9, 2, 2):
-	case IP_VERSION(9, 1, 0):
-		if (!((adev->apu_flags & AMD_APU_IS_RAVEN2) ||
-		      (adev->apu_flags & AMD_APU_IS_PICASSO)) &&
-		    ((!is_raven_kicker(adev) &&
-		      adev->gfx.rlc_fw_version < 531) ||
-		     (adev->gfx.rlc_feature_version < 1) ||
-		     !adev->gfx.rlc.is_rlc_v2_1))
-			adev->pm.pp_feature &= ~PP_GFXOFF_MASK;
+		case IP_VERSION(9, 0, 1):
+		case IP_VERSION(9, 2, 1):
+		case IP_VERSION(9, 4, 0):
+			break;
+		case IP_VERSION(9, 2, 2):
+		case IP_VERSION(9, 1, 0):
+			if (!((adev->apu_flags & AMD_APU_IS_RAVEN2) ||
+				(adev->apu_flags & AMD_APU_IS_PICASSO)) &&
+				((!is_raven_kicker(adev) &&
+				adev->gfx.rlc_fw_version < 531) ||
+				(adev->gfx.rlc_feature_version < 1) ||
+				!adev->gfx.rlc.is_rlc_v2_1))
+				adev->pm.pp_feature &= ~PP_GFXOFF_MASK;
 
-		if (adev->pm.pp_feature & PP_GFXOFF_MASK)
-			adev->pg_flags |= AMD_PG_SUPPORT_GFX_PG |
+			if (adev->pm.pp_feature & PP_GFXOFF_MASK)
+				adev->pg_flags |= AMD_PG_SUPPORT_GFX_PG |
 				AMD_PG_SUPPORT_CP |
 				AMD_PG_SUPPORT_RLC_SMU_HS;
-		break;
-	case IP_VERSION(9, 3, 0):
-		if (adev->pm.pp_feature & PP_GFXOFF_MASK)
-			adev->pg_flags |= AMD_PG_SUPPORT_GFX_PG |
+			break;
+		case IP_VERSION(9, 3, 0):
+			if (adev->pm.pp_feature & PP_GFXOFF_MASK)
+				adev->pg_flags |= AMD_PG_SUPPORT_GFX_PG |
 				AMD_PG_SUPPORT_CP |
 				AMD_PG_SUPPORT_RLC_SMU_HS;
-		break;
-	default:
-		break;
+			break;
+		default:
+			break;
 	}
 }
 
 static int gfx_v9_0_init_cp_gfx_microcode(struct amdgpu_device *adev,
-					  char *chip_name)
+										  char *chip_name)
 {
 	int err;
 
 	err = amdgpu_ucode_request(adev, &adev->gfx.pfp_fw,
-				   AMDGPU_UCODE_REQUIRED,
-				   "amdgpu/%s_pfp.bin", chip_name);
+							   AMDGPU_UCODE_REQUIRED,
+							"amdgpu/%s_pfp.bin", chip_name);
 	if (err)
 		goto out;
 	amdgpu_gfx_cp_init_microcode(adev, AMDGPU_UCODE_ID_CP_PFP);
 
 	err = amdgpu_ucode_request(adev, &adev->gfx.me_fw,
-				   AMDGPU_UCODE_REQUIRED,
-				   "amdgpu/%s_me.bin", chip_name);
+							   AMDGPU_UCODE_REQUIRED,
+							"amdgpu/%s_me.bin", chip_name);
 	if (err)
 		goto out;
 	amdgpu_gfx_cp_init_microcode(adev, AMDGPU_UCODE_ID_CP_ME);
 
 	err = amdgpu_ucode_request(adev, &adev->gfx.ce_fw,
-				   AMDGPU_UCODE_REQUIRED,
-				   "amdgpu/%s_ce.bin", chip_name);
+							   AMDGPU_UCODE_REQUIRED,
+							"amdgpu/%s_ce.bin", chip_name);
 	if (err)
 		goto out;
 	amdgpu_gfx_cp_init_microcode(adev, AMDGPU_UCODE_ID_CP_CE);
 
-out:
+	out:
 	if (err) {
 		amdgpu_ucode_release(&adev->gfx.pfp_fw);
 		amdgpu_ucode_release(&adev->gfx.me_fw);
@@ -1460,7 +1450,7 @@ out:
 }
 
 static int gfx_v9_0_init_rlc_microcode(struct amdgpu_device *adev,
-				       char *chip_name)
+									   char *chip_name)
 {
 	int err;
 	const struct rlc_firmware_header_v2_0 *rlc_hdr;
@@ -1468,40 +1458,30 @@ static int gfx_v9_0_init_rlc_microcode(s
 	uint16_t version_minor;
 	uint32_t smu_version;
 
-	/*
-	 * For Picasso && AM4 SOCKET board, we use picasso_rlc_am4.bin
-	 * instead of picasso_rlc.bin.
-	 * Judgment method:
-	 * PCO AM4: revision >= 0xC8 && revision <= 0xCF
-	 *          or revision >= 0xD8 && revision <= 0xDF
-	 * otherwise is PCO FP5
-	 */
 	if (!strcmp(chip_name, "picasso") &&
 		(((adev->pdev->revision >= 0xC8) && (adev->pdev->revision <= 0xCF)) ||
-		((adev->pdev->revision >= 0xD8) && (adev->pdev->revision <= 0xDF))))
-		err = amdgpu_ucode_request(adev, &adev->gfx.rlc_fw,
-					   AMDGPU_UCODE_REQUIRED,
-					   "amdgpu/%s_rlc_am4.bin", chip_name);
-	else if (!strcmp(chip_name, "raven") && (amdgpu_pm_load_smu_firmware(adev, &smu_version) == 0) &&
-		(smu_version >= 0x41e2b))
-		/**
-		*SMC is loaded by SBIOS on APU and it's able to get the SMU version directly.
-		*/
-		err = amdgpu_ucode_request(adev, &adev->gfx.rlc_fw,
-					   AMDGPU_UCODE_REQUIRED,
-					   "amdgpu/%s_kicker_rlc.bin", chip_name);
-	else
+		((adev->pdev->revision >= 0xD8) && (adev->pdev->revision <= 0xDF)))) {
 		err = amdgpu_ucode_request(adev, &adev->gfx.rlc_fw,
-					   AMDGPU_UCODE_REQUIRED,
-					   "amdgpu/%s_rlc.bin", chip_name);
-	if (err)
-		goto out;
+								   AMDGPU_UCODE_REQUIRED,
+							 "amdgpu/%s_rlc_am4.bin", chip_name);
+		} else if (!strcmp(chip_name, "raven") && (amdgpu_pm_load_smu_firmware(adev, &smu_version) == 0) &&
+			(smu_version >= 0x41e2b)) {
+			err = amdgpu_ucode_request(adev, &adev->gfx.rlc_fw,
+									   AMDGPU_UCODE_REQUIRED,
+							  "amdgpu/%s_kicker_rlc.bin", chip_name);
+			} else {
+				err = amdgpu_ucode_request(adev, &adev->gfx.rlc_fw,
+										   AMDGPU_UCODE_REQUIRED,
+							   "amdgpu/%s_rlc.bin", chip_name);
+			}
+			if (err)
+				goto out;
 
 	rlc_hdr = (const struct rlc_firmware_header_v2_0 *)adev->gfx.rlc_fw->data;
 	version_major = le16_to_cpu(rlc_hdr->header.header_version_major);
 	version_minor = le16_to_cpu(rlc_hdr->header.header_version_minor);
 	err = amdgpu_gfx_rlc_init_microcode(adev, version_major, version_minor);
-out:
+	out:
 	if (err)
 		amdgpu_ucode_release(&adev->gfx.rlc_fw);
 
@@ -1511,26 +1491,27 @@ out:
 static bool gfx_v9_0_load_mec2_fw_bin_support(struct amdgpu_device *adev)
 {
 	if (amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 2) ||
-	    amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 1) ||
-	    amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 3, 0))
+		amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 1) ||
+		amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 3, 0))
 		return false;
 
 	return true;
 }
 
 static int gfx_v9_0_init_cp_compute_microcode(struct amdgpu_device *adev,
-					      char *chip_name)
+											  char *chip_name)
 {
 	int err;
 
-	if (amdgpu_sriov_vf(adev) && (adev->asic_type == CHIP_ALDEBARAN))
+	if (amdgpu_sriov_vf(adev) && (adev->asic_type == CHIP_ALDEBARAN)) {
 		err = amdgpu_ucode_request(adev, &adev->gfx.mec_fw,
-				   AMDGPU_UCODE_REQUIRED,
-				   "amdgpu/%s_sjt_mec.bin", chip_name);
-	else
+								   AMDGPU_UCODE_REQUIRED,
+							 "amdgpu/%s_sjt_mec.bin", chip_name);
+	} else {
 		err = amdgpu_ucode_request(adev, &adev->gfx.mec_fw,
-					   AMDGPU_UCODE_REQUIRED,
-					   "amdgpu/%s_mec.bin", chip_name);
+								   AMDGPU_UCODE_REQUIRED,
+							 "amdgpu/%s_mec.bin", chip_name);
+	}
 	if (err)
 		goto out;
 
@@ -1538,14 +1519,15 @@ static int gfx_v9_0_init_cp_compute_micr
 	amdgpu_gfx_cp_init_microcode(adev, AMDGPU_UCODE_ID_CP_MEC1_JT);
 
 	if (gfx_v9_0_load_mec2_fw_bin_support(adev)) {
-		if (amdgpu_sriov_vf(adev) && (adev->asic_type == CHIP_ALDEBARAN))
+		if (amdgpu_sriov_vf(adev) && (adev->asic_type == CHIP_ALDEBARAN)) {
 			err = amdgpu_ucode_request(adev, &adev->gfx.mec2_fw,
-						   AMDGPU_UCODE_REQUIRED,
-						   "amdgpu/%s_sjt_mec2.bin", chip_name);
-		else
+									   AMDGPU_UCODE_REQUIRED,
+							  "amdgpu/%s_sjt_mec2.bin", chip_name);
+		} else {
 			err = amdgpu_ucode_request(adev, &adev->gfx.mec2_fw,
-						   AMDGPU_UCODE_REQUIRED,
-						   "amdgpu/%s_mec2.bin", chip_name);
+									   AMDGPU_UCODE_REQUIRED,
+							  "amdgpu/%s_mec2.bin", chip_name);
+		}
 		if (!err) {
 			amdgpu_gfx_cp_init_microcode(adev, AMDGPU_UCODE_ID_CP_MEC2);
 			amdgpu_gfx_cp_init_microcode(adev, AMDGPU_UCODE_ID_CP_MEC2_JT);
@@ -1561,7 +1543,7 @@ static int gfx_v9_0_init_cp_compute_micr
 	gfx_v9_0_check_if_need_gfxoff(adev);
 	gfx_v9_0_check_fw_write_wait(adev);
 
-out:
+	out:
 	if (err)
 		amdgpu_ucode_release(&adev->gfx.mec_fw);
 	return err;
@@ -1575,7 +1557,6 @@ static int gfx_v9_0_init_microcode(struc
 	DRM_DEBUG("\n");
 	amdgpu_ucode_ip_version_decode(adev, GC_HWIP, ucode_prefix, sizeof(ucode_prefix));
 
-	/* No CPG in Arcturus */
 	if (adev->gfx.num_gfx_rings) {
 		r = gfx_v9_0_init_cp_gfx_microcode(adev, ucode_prefix);
 		if (r)
@@ -1599,9 +1580,7 @@ static u32 gfx_v9_0_get_csb_size(struct
 	const struct cs_section_def *sect = NULL;
 	const struct cs_extent_def *ext = NULL;
 
-	/* begin clear state */
 	count += 2;
-	/* context control state */
 	count += 3;
 
 	for (sect = gfx9_cs_data; sect->section != NULL; ++sect) {
@@ -1613,16 +1592,14 @@ static u32 gfx_v9_0_get_csb_size(struct
 		}
 	}
 
-	/* end clear state */
 	count += 2;
-	/* clear state */
 	count += 2;
 
 	return count;
 }
 
 static void gfx_v9_0_get_csb_buffer(struct amdgpu_device *adev,
-				    volatile u32 *buffer)
+									volatile u32 *buffer)
 {
 	u32 count = 0, i;
 	const struct cs_section_def *sect = NULL;
@@ -1644,9 +1621,9 @@ static void gfx_v9_0_get_csb_buffer(stru
 		for (ext = sect->section; ext->extent != NULL; ++ext) {
 			if (sect->id == SECT_CONTEXT) {
 				buffer[count++] =
-					cpu_to_le32(PACKET3(PACKET3_SET_CONTEXT_REG, ext->reg_count));
+				cpu_to_le32(PACKET3(PACKET3_SET_CONTEXT_REG, ext->reg_count));
 				buffer[count++] = cpu_to_le32(ext->reg_index -
-						PACKET3_SET_CONTEXT_REG_START);
+				PACKET3_SET_CONTEXT_REG_START);
 				for (i = 0; i < ext->reg_count; i++)
 					buffer[count++] = cpu_to_le32(ext->extent[i]);
 			} else {
@@ -1664,44 +1641,56 @@ static void gfx_v9_0_get_csb_buffer(stru
 
 static void gfx_v9_0_init_always_on_cu_mask(struct amdgpu_device *adev)
 {
-	struct amdgpu_cu_info *cu_info = &adev->gfx.cu_info;
-	uint32_t pg_always_on_cu_num = 2;
-	uint32_t always_on_cu_num;
-	uint32_t i, j, k;
-	uint32_t mask, cu_bitmap, counter;
+	struct amdgpu_cu_info *cu = &adev->gfx.cu_info;
+	const u32 pg_always_on = 2;
+	u32       ao_per_sh;
 
-	if (adev->flags & AMD_IS_APU)
-		always_on_cu_num = 4;
-	else if (amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 2, 1))
-		always_on_cu_num = 8;
-	else
-		always_on_cu_num = 12;
+	if (adev->flags & AMD_IS_APU) {
+		ao_per_sh = 4;
+	} else if (amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 2, 1)) {
+		ao_per_sh = 8;
+	} else {
+		ao_per_sh = 12;
+	}
 
 	mutex_lock(&adev->grbm_idx_mutex);
-	for (i = 0; i < adev->gfx.config.max_shader_engines; i++) {
-		for (j = 0; j < adev->gfx.config.max_sh_per_se; j++) {
-			mask = 1;
-			cu_bitmap = 0;
-			counter = 0;
-			amdgpu_gfx_select_se_sh(adev, i, j, 0xffffffff, 0);
 
-			for (k = 0; k < adev->gfx.config.max_cu_per_sh; k ++) {
-				if (cu_info->bitmap[0][i][j] & mask) {
-					if (counter == pg_always_on_cu_num)
-						WREG32_SOC15(GC, 0, mmRLC_PG_ALWAYS_ON_CU_MASK, cu_bitmap);
-					if (counter < always_on_cu_num)
-						cu_bitmap |= mask;
-					else
-						break;
-					counter++;
-				}
-				mask <<= 1;
+	for (u32 se = 0; se < adev->gfx.config.max_shader_engines; ++se) {
+		for (u32 sh = 0; sh < adev->gfx.config.max_sh_per_se; ++sh) {
+
+			const u32 idx_se = se & 3;
+			const u32 idx_sh = sh + (se >> 2);
+
+			if (idx_sh >= ARRAY_SIZE(cu->bitmap[0]))
+				continue;
+
+			unsigned long bitmap =
+			(unsigned long)cu->bitmap[idx_se][idx_sh];
+
+			u32 ao_bitmap = 0;
+			u32 seen      = 0;
+
+			amdgpu_gfx_select_se_sh(adev, se, sh, 0xffffffff, 0);
+
+			while (bitmap && seen < ao_per_sh) {
+				const u32 bit = __ffs(bitmap);
+				if (seen == pg_always_on)
+					WREG32_SOC15(GC, 0,
+								 mmRLC_PG_ALWAYS_ON_CU_MASK,
+				  ao_bitmap);
+
+				ao_bitmap |= BIT(bit);
+				bitmap    &= bitmap - 1;
+				++seen;
 			}
 
-			WREG32_SOC15(GC, 0, mmRLC_LB_ALWAYS_ACTIVE_CU_MASK, cu_bitmap);
-			cu_info->ao_cu_bitmap[i][j] = cu_bitmap;
+			WREG32_SOC15(GC, 0, mmRLC_LB_ALWAYS_ACTIVE_CU_MASK,
+						 ao_bitmap);
+
+			cu->ao_cu_bitmap[idx_se][idx_sh] = ao_bitmap;
 		}
 	}
+
 	amdgpu_gfx_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff, 0);
 	mutex_unlock(&adev->grbm_idx_mutex);
 }
@@ -1710,42 +1699,29 @@ static void gfx_v9_0_init_lbpw(struct am
 {
 	uint32_t data;
 
-	/* set mmRLC_LB_THR_CONFIG_1/2/3/4 */
 	WREG32_SOC15(GC, 0, mmRLC_LB_THR_CONFIG_1, 0x0000007F);
 	WREG32_SOC15(GC, 0, mmRLC_LB_THR_CONFIG_2, 0x0333A5A7);
 	WREG32_SOC15(GC, 0, mmRLC_LB_THR_CONFIG_3, 0x00000077);
 	WREG32_SOC15(GC, 0, mmRLC_LB_THR_CONFIG_4, (0x30 | 0x40 << 8 | 0x02FA << 16));
 
-	/* set mmRLC_LB_CNTR_INIT = 0x0000_0000 */
 	WREG32_SOC15(GC, 0, mmRLC_LB_CNTR_INIT, 0x00000000);
 
-	/* set mmRLC_LB_CNTR_MAX = 0x0000_0500 */
 	WREG32_SOC15(GC, 0, mmRLC_LB_CNTR_MAX, 0x00000500);
 
 	mutex_lock(&adev->grbm_idx_mutex);
-	/* set mmRLC_LB_INIT_CU_MASK thru broadcast mode to enable all SE/SH*/
 	amdgpu_gfx_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff, 0);
 	WREG32_SOC15(GC, 0, mmRLC_LB_INIT_CU_MASK, 0xffffffff);
 
-	/* set mmRLC_LB_PARAMS = 0x003F_1006 */
 	data = REG_SET_FIELD(0, RLC_LB_PARAMS, FIFO_SAMPLES, 0x0003);
 	data |= REG_SET_FIELD(data, RLC_LB_PARAMS, PG_IDLE_SAMPLES, 0x0010);
 	data |= REG_SET_FIELD(data, RLC_LB_PARAMS, PG_IDLE_SAMPLE_INTERVAL, 0x033F);
 	WREG32_SOC15(GC, 0, mmRLC_LB_PARAMS, data);
 
-	/* set mmRLC_GPM_GENERAL_7[31-16] = 0x00C0 */
 	data = RREG32_SOC15(GC, 0, mmRLC_GPM_GENERAL_7);
 	data &= 0x0000FFFF;
 	data |= 0x00C00000;
 	WREG32_SOC15(GC, 0, mmRLC_GPM_GENERAL_7, data);
 
-	/*
-	 * RLC_LB_ALWAYS_ACTIVE_CU_MASK = 0xF (4 CUs AON for Raven),
-	 * programmed in gfx_v9_0_init_always_on_cu_mask()
-	 */
-
-	/* set RLC_LB_CNTL = 0x8000_0095, 31 bit is reserved,
-	 * but used for RLC_LB_CNTL configuration */
 	data = RLC_LB_CNTL__LB_CNT_SPIM_ACTIVE_MASK;
 	data |= REG_SET_FIELD(data, RLC_LB_CNTL, CU_MASK_USED_OFF_HYST, 0x09);
 	data |= REG_SET_FIELD(data, RLC_LB_CNTL, RESERVED, 0x80000);
@@ -1759,42 +1735,29 @@ static void gfx_v9_4_init_lbpw(struct am
 {
 	uint32_t data;
 
-	/* set mmRLC_LB_THR_CONFIG_1/2/3/4 */
 	WREG32_SOC15(GC, 0, mmRLC_LB_THR_CONFIG_1, 0x0000007F);
 	WREG32_SOC15(GC, 0, mmRLC_LB_THR_CONFIG_2, 0x033388F8);
 	WREG32_SOC15(GC, 0, mmRLC_LB_THR_CONFIG_3, 0x00000077);
 	WREG32_SOC15(GC, 0, mmRLC_LB_THR_CONFIG_4, (0x10 | 0x27 << 8 | 0x02FA << 16));
 
-	/* set mmRLC_LB_CNTR_INIT = 0x0000_0000 */
 	WREG32_SOC15(GC, 0, mmRLC_LB_CNTR_INIT, 0x00000000);
 
-	/* set mmRLC_LB_CNTR_MAX = 0x0000_0500 */
 	WREG32_SOC15(GC, 0, mmRLC_LB_CNTR_MAX, 0x00000800);
 
 	mutex_lock(&adev->grbm_idx_mutex);
-	/* set mmRLC_LB_INIT_CU_MASK thru broadcast mode to enable all SE/SH*/
 	amdgpu_gfx_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff, 0);
 	WREG32_SOC15(GC, 0, mmRLC_LB_INIT_CU_MASK, 0xffffffff);
 
-	/* set mmRLC_LB_PARAMS = 0x003F_1006 */
 	data = REG_SET_FIELD(0, RLC_LB_PARAMS, FIFO_SAMPLES, 0x0003);
 	data |= REG_SET_FIELD(data, RLC_LB_PARAMS, PG_IDLE_SAMPLES, 0x0010);
 	data |= REG_SET_FIELD(data, RLC_LB_PARAMS, PG_IDLE_SAMPLE_INTERVAL, 0x033F);
 	WREG32_SOC15(GC, 0, mmRLC_LB_PARAMS, data);
 
-	/* set mmRLC_GPM_GENERAL_7[31-16] = 0x00C0 */
 	data = RREG32_SOC15(GC, 0, mmRLC_GPM_GENERAL_7);
 	data &= 0x0000FFFF;
 	data |= 0x00C00000;
 	WREG32_SOC15(GC, 0, mmRLC_GPM_GENERAL_7, data);
 
-	/*
-	 * RLC_LB_ALWAYS_ACTIVE_CU_MASK = 0xFFF (12 CUs AON),
-	 * programmed in gfx_v9_0_init_always_on_cu_mask()
-	 */
-
-	/* set RLC_LB_CNTL = 0x8000_0095, 31 bit is reserved,
-	 * but used for RLC_LB_CNTL configuration */
 	data = RLC_LB_CNTL__LB_CNT_SPIM_ACTIVE_MASK;
 	data |= REG_SET_FIELD(data, RLC_LB_CNTL, CU_MASK_USED_OFF_HYST, 0x09);
 	data |= REG_SET_FIELD(data, RLC_LB_CNTL, RESERVED, 0x80000);
@@ -1842,15 +1805,13 @@ static int gfx_v9_0_rlc_init(struct amdg
 	cs_data = adev->gfx.rlc.cs_data;
 
 	if (cs_data) {
-		/* init clear state block */
 		r = amdgpu_gfx_rlc_init_csb(adev);
 		if (r)
 			return r;
 	}
 
 	if (adev->flags & AMD_IS_APU) {
-		/* TODO: double check the cp_table_size for RV */
-		adev->gfx.rlc.cp_table_size = ALIGN(96 * 5 * 4, 2048) + (64 * 1024); /* JT + GDS */
+		adev->gfx.rlc.cp_table_size = ALIGN(96 * 5 * 4, 2048) + (64 * 1024);
 		r = amdgpu_gfx_rlc_init_cpt(adev);
 		if (r)
 			return r;
@@ -1878,16 +1839,15 @@ static int gfx_v9_0_mec_init(struct amdg
 
 	bitmap_zero(adev->gfx.mec_bitmap[0].queue_bitmap, AMDGPU_MAX_COMPUTE_QUEUES);
 
-	/* take ownership of the relevant compute queues */
 	amdgpu_gfx_compute_queue_acquire(adev);
 	mec_hpd_size = adev->gfx.num_compute_rings * GFX9_MEC_HPD_SIZE;
 	if (mec_hpd_size) {
 		r = amdgpu_bo_create_reserved(adev, mec_hpd_size, PAGE_SIZE,
-					      AMDGPU_GEM_DOMAIN_VRAM |
-					      AMDGPU_GEM_DOMAIN_GTT,
-					      &adev->gfx.mec.hpd_eop_obj,
-					      &adev->gfx.mec.hpd_eop_gpu_addr,
-					      (void **)&hpd);
+									  AMDGPU_GEM_DOMAIN_VRAM |
+									  AMDGPU_GEM_DOMAIN_GTT,
+								&adev->gfx.mec.hpd_eop_obj,
+								&adev->gfx.mec.hpd_eop_gpu_addr,
+								(void **)&hpd);
 		if (r) {
 			dev_warn(adev->dev, "(%d) create HDP EOP bo failed\n", r);
 			gfx_v9_0_mec_fini(adev);
@@ -1903,15 +1863,15 @@ static int gfx_v9_0_mec_init(struct amdg
 	mec_hdr = (const struct gfx_firmware_header_v1_0 *)adev->gfx.mec_fw->data;
 
 	fw_data = (const __le32 *)
-		(adev->gfx.mec_fw->data +
-		 le32_to_cpu(mec_hdr->header.ucode_array_offset_bytes));
+	(adev->gfx.mec_fw->data +
+	le32_to_cpu(mec_hdr->header.ucode_array_offset_bytes));
 	fw_size = le32_to_cpu(mec_hdr->header.ucode_size_bytes);
 
 	r = amdgpu_bo_create_reserved(adev, mec_hdr->header.ucode_size_bytes,
-				      PAGE_SIZE, AMDGPU_GEM_DOMAIN_GTT,
-				      &adev->gfx.mec.mec_fw_obj,
-				      &adev->gfx.mec.mec_fw_gpu_addr,
-				      (void **)&fw);
+								  PAGE_SIZE, AMDGPU_GEM_DOMAIN_GTT,
+							   &adev->gfx.mec.mec_fw_obj,
+							   &adev->gfx.mec.mec_fw_gpu_addr,
+							   (void **)&fw);
 	if (r) {
 		dev_warn(adev->dev, "(%d) create mec firmware bo failed\n", r);
 		gfx_v9_0_mec_fini(adev);
@@ -1929,31 +1889,30 @@ static int gfx_v9_0_mec_init(struct amdg
 static uint32_t wave_read_ind(struct amdgpu_device *adev, uint32_t simd, uint32_t wave, uint32_t address)
 {
 	WREG32_SOC15_RLC(GC, 0, mmSQ_IND_INDEX,
-		(wave << SQ_IND_INDEX__WAVE_ID__SHIFT) |
-		(simd << SQ_IND_INDEX__SIMD_ID__SHIFT) |
-		(address << SQ_IND_INDEX__INDEX__SHIFT) |
-		(SQ_IND_INDEX__FORCE_READ_MASK));
+					 (wave << SQ_IND_INDEX__WAVE_ID__SHIFT) |
+					 (simd << SQ_IND_INDEX__SIMD_ID__SHIFT) |
+					 (address << SQ_IND_INDEX__INDEX__SHIFT) |
+					 (SQ_IND_INDEX__FORCE_READ_MASK));
 	return RREG32_SOC15(GC, 0, mmSQ_IND_DATA);
 }
 
 static void wave_read_regs(struct amdgpu_device *adev, uint32_t simd,
-			   uint32_t wave, uint32_t thread,
-			   uint32_t regno, uint32_t num, uint32_t *out)
+						   uint32_t wave, uint32_t thread,
+						   uint32_t regno, uint32_t num, uint32_t *out)
 {
 	WREG32_SOC15_RLC(GC, 0, mmSQ_IND_INDEX,
-		(wave << SQ_IND_INDEX__WAVE_ID__SHIFT) |
-		(simd << SQ_IND_INDEX__SIMD_ID__SHIFT) |
-		(regno << SQ_IND_INDEX__INDEX__SHIFT) |
-		(thread << SQ_IND_INDEX__THREAD_ID__SHIFT) |
-		(SQ_IND_INDEX__FORCE_READ_MASK) |
-		(SQ_IND_INDEX__AUTO_INCR_MASK));
+					 (wave << SQ_IND_INDEX__WAVE_ID__SHIFT) |
+					 (simd << SQ_IND_INDEX__SIMD_ID__SHIFT) |
+					 (regno << SQ_IND_INDEX__INDEX__SHIFT) |
+					 (thread << SQ_IND_INDEX__THREAD_ID__SHIFT) |
+					 (SQ_IND_INDEX__FORCE_READ_MASK) |
+					 (SQ_IND_INDEX__AUTO_INCR_MASK));
 	while (num--)
 		*(out++) = RREG32_SOC15(GC, 0, mmSQ_IND_DATA);
 }
 
 static void gfx_v9_0_read_wave_data(struct amdgpu_device *adev, uint32_t xcc_id, uint32_t simd, uint32_t wave, uint32_t *dst, int *no_fields)
 {
-	/* type 1 wave data */
 	dst[(*no_fields)++] = 1;
 	dst[(*no_fields)++] = wave_read_ind(adev, simd, wave, ixSQ_WAVE_STATUS);
 	dst[(*no_fields)++] = wave_read_ind(adev, simd, wave, ixSQ_WAVE_PC_LO);
@@ -1973,8 +1932,8 @@ static void gfx_v9_0_read_wave_data(stru
 }
 
 static void gfx_v9_0_read_wave_sgprs(struct amdgpu_device *adev, uint32_t xcc_id, uint32_t simd,
-				     uint32_t wave, uint32_t start,
-				     uint32_t size, uint32_t *dst)
+									 uint32_t wave, uint32_t start,
+									 uint32_t size, uint32_t *dst)
 {
 	wave_read_regs(
 		adev, simd, wave, 0,
@@ -1982,9 +1941,9 @@ static void gfx_v9_0_read_wave_sgprs(str
 }
 
 static void gfx_v9_0_read_wave_vgprs(struct amdgpu_device *adev, uint32_t xcc_id, uint32_t simd,
-				     uint32_t wave, uint32_t thread,
-				     uint32_t start, uint32_t size,
-				     uint32_t *dst)
+									 uint32_t wave, uint32_t thread,
+									 uint32_t start, uint32_t size,
+									 uint32_t *dst)
 {
 	wave_read_regs(
 		adev, simd, wave, thread,
@@ -1992,24 +1951,24 @@ static void gfx_v9_0_read_wave_vgprs(str
 }
 
 static void gfx_v9_0_select_me_pipe_q(struct amdgpu_device *adev,
-				  u32 me, u32 pipe, u32 q, u32 vm, u32 xcc_id)
+									  u32 me, u32 pipe, u32 q, u32 vm, u32 xcc_id)
 {
 	soc15_grbm_select(adev, me, pipe, q, vm, 0);
 }
 
 static const struct amdgpu_gfx_funcs gfx_v9_0_gfx_funcs = {
-        .get_gpu_clock_counter = &gfx_v9_0_get_gpu_clock_counter,
-        .select_se_sh = &gfx_v9_0_select_se_sh,
-        .read_wave_data = &gfx_v9_0_read_wave_data,
-        .read_wave_sgprs = &gfx_v9_0_read_wave_sgprs,
-        .read_wave_vgprs = &gfx_v9_0_read_wave_vgprs,
-        .select_me_pipe_q = &gfx_v9_0_select_me_pipe_q,
+	.get_gpu_clock_counter = &gfx_v9_0_get_gpu_clock_counter,
+	.select_se_sh = &gfx_v9_0_select_se_sh,
+	.read_wave_data = &gfx_v9_0_read_wave_data,
+	.read_wave_sgprs = &gfx_v9_0_read_wave_sgprs,
+	.read_wave_vgprs = &gfx_v9_0_read_wave_vgprs,
+	.select_me_pipe_q = &gfx_v9_0_select_me_pipe_q,
 };
 
 const struct amdgpu_ras_block_hw_ops  gfx_v9_0_ras_ops = {
-		.ras_error_inject = &gfx_v9_0_ras_error_inject,
-		.query_ras_error_count = &gfx_v9_0_query_ras_error_count,
-		.reset_ras_error_count = &gfx_v9_0_reset_ras_error_count,
+	.ras_error_inject = &gfx_v9_0_ras_error_inject,
+	.query_ras_error_count = &gfx_v9_0_query_ras_error_count,
+	.reset_ras_error_count = &gfx_v9_0_reset_ras_error_count,
 };
 
 static struct amdgpu_gfx_ras gfx_v9_0_ras = {
@@ -2024,133 +1983,131 @@ static int gfx_v9_0_gpu_early_init(struc
 	int err;
 
 	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
-	case IP_VERSION(9, 0, 1):
-		adev->gfx.config.max_hw_contexts = 8;
-		adev->gfx.config.sc_prim_fifo_size_frontend = 0x20;
-		adev->gfx.config.sc_prim_fifo_size_backend = 0x100;
-		adev->gfx.config.sc_hiz_tile_fifo_size = 0x30;
-		adev->gfx.config.sc_earlyz_tile_fifo_size = 0x4C0;
-		gb_addr_config = VEGA10_GB_ADDR_CONFIG_GOLDEN;
-		break;
-	case IP_VERSION(9, 2, 1):
-		adev->gfx.config.max_hw_contexts = 8;
-		adev->gfx.config.sc_prim_fifo_size_frontend = 0x20;
-		adev->gfx.config.sc_prim_fifo_size_backend = 0x100;
-		adev->gfx.config.sc_hiz_tile_fifo_size = 0x30;
-		adev->gfx.config.sc_earlyz_tile_fifo_size = 0x4C0;
-		gb_addr_config = VEGA12_GB_ADDR_CONFIG_GOLDEN;
-		DRM_INFO("fix gfx.config for vega12\n");
-		break;
-	case IP_VERSION(9, 4, 0):
-		adev->gfx.ras = &gfx_v9_0_ras;
-		adev->gfx.config.max_hw_contexts = 8;
-		adev->gfx.config.sc_prim_fifo_size_frontend = 0x20;
-		adev->gfx.config.sc_prim_fifo_size_backend = 0x100;
-		adev->gfx.config.sc_hiz_tile_fifo_size = 0x30;
-		adev->gfx.config.sc_earlyz_tile_fifo_size = 0x4C0;
-		gb_addr_config = RREG32_SOC15(GC, 0, mmGB_ADDR_CONFIG);
-		gb_addr_config &= ~0xf3e777ff;
-		gb_addr_config |= 0x22014042;
-		/* check vbios table if gpu info is not available */
-		err = amdgpu_atomfirmware_get_gfx_info(adev);
-		if (err)
-			return err;
-		break;
-	case IP_VERSION(9, 2, 2):
-	case IP_VERSION(9, 1, 0):
-		adev->gfx.config.max_hw_contexts = 8;
-		adev->gfx.config.sc_prim_fifo_size_frontend = 0x20;
-		adev->gfx.config.sc_prim_fifo_size_backend = 0x100;
-		adev->gfx.config.sc_hiz_tile_fifo_size = 0x30;
-		adev->gfx.config.sc_earlyz_tile_fifo_size = 0x4C0;
-		if (adev->apu_flags & AMD_APU_IS_RAVEN2)
-			gb_addr_config = RAVEN2_GB_ADDR_CONFIG_GOLDEN;
+		case IP_VERSION(9, 0, 1):
+			adev->gfx.config.max_hw_contexts = 8;
+			adev->gfx.config.sc_prim_fifo_size_frontend = 0x20;
+			adev->gfx.config.sc_prim_fifo_size_backend = 0x100;
+			adev->gfx.config.sc_hiz_tile_fifo_size = 0x30;
+			adev->gfx.config.sc_earlyz_tile_fifo_size = 0x4C0;
+			gb_addr_config = VEGA10_GB_ADDR_CONFIG_GOLDEN;
+			break;
+		case IP_VERSION(9, 2, 1):
+			adev->gfx.config.max_hw_contexts = 8;
+			adev->gfx.config.sc_prim_fifo_size_frontend = 0x20;
+			adev->gfx.config.sc_prim_fifo_size_backend = 0x100;
+			adev->gfx.config.sc_hiz_tile_fifo_size = 0x30;
+			adev->gfx.config.sc_earlyz_tile_fifo_size = 0x4C0;
+			gb_addr_config = VEGA12_GB_ADDR_CONFIG_GOLDEN;
+			DRM_INFO("fix gfx.config for vega12\n");
+			break;
+		case IP_VERSION(9, 4, 0):
+			adev->gfx.ras = &gfx_v9_0_ras;
+			adev->gfx.config.max_hw_contexts = 8;
+			adev->gfx.config.sc_prim_fifo_size_frontend = 0x20;
+			adev->gfx.config.sc_prim_fifo_size_backend = 0x100;
+			adev->gfx.config.sc_hiz_tile_fifo_size = 0x30;
+			adev->gfx.config.sc_earlyz_tile_fifo_size = 0x4C0;
+			gb_addr_config = RREG32_SOC15(GC, 0, mmGB_ADDR_CONFIG);
+			gb_addr_config &= ~0xf3e777ff;
+			gb_addr_config |= 0x22014042;
+			err = amdgpu_atomfirmware_get_gfx_info(adev);
+			if (err)
+				return err;
+		break;
+		case IP_VERSION(9, 2, 2):
+		case IP_VERSION(9, 1, 0):
+			adev->gfx.config.max_hw_contexts = 8;
+			adev->gfx.config.sc_prim_fifo_size_frontend = 0x20;
+			adev->gfx.config.sc_prim_fifo_size_backend = 0x100;
+			adev->gfx.config.sc_hiz_tile_fifo_size = 0x30;
+			adev->gfx.config.sc_earlyz_tile_fifo_size = 0x4C0;
+			if (adev->apu_flags & AMD_APU_IS_RAVEN2)
+				gb_addr_config = RAVEN2_GB_ADDR_CONFIG_GOLDEN;
 		else
 			gb_addr_config = RAVEN_GB_ADDR_CONFIG_GOLDEN;
 		break;
-	case IP_VERSION(9, 4, 1):
-		adev->gfx.ras = &gfx_v9_4_ras;
-		adev->gfx.config.max_hw_contexts = 8;
-		adev->gfx.config.sc_prim_fifo_size_frontend = 0x20;
-		adev->gfx.config.sc_prim_fifo_size_backend = 0x100;
-		adev->gfx.config.sc_hiz_tile_fifo_size = 0x30;
-		adev->gfx.config.sc_earlyz_tile_fifo_size = 0x4C0;
-		gb_addr_config = RREG32_SOC15(GC, 0, mmGB_ADDR_CONFIG);
-		gb_addr_config &= ~0xf3e777ff;
-		gb_addr_config |= 0x22014042;
-		break;
-	case IP_VERSION(9, 3, 0):
-		adev->gfx.config.max_hw_contexts = 8;
-		adev->gfx.config.sc_prim_fifo_size_frontend = 0x20;
-		adev->gfx.config.sc_prim_fifo_size_backend = 0x100;
-		adev->gfx.config.sc_hiz_tile_fifo_size = 0x80;
-		adev->gfx.config.sc_earlyz_tile_fifo_size = 0x4C0;
-		gb_addr_config = RREG32_SOC15(GC, 0, mmGB_ADDR_CONFIG);
-		gb_addr_config &= ~0xf3e777ff;
-		gb_addr_config |= 0x22010042;
-		break;
-	case IP_VERSION(9, 4, 2):
-		adev->gfx.ras = &gfx_v9_4_2_ras;
-		adev->gfx.config.max_hw_contexts = 8;
-		adev->gfx.config.sc_prim_fifo_size_frontend = 0x20;
-		adev->gfx.config.sc_prim_fifo_size_backend = 0x100;
-		adev->gfx.config.sc_hiz_tile_fifo_size = 0x30;
-		adev->gfx.config.sc_earlyz_tile_fifo_size = 0x4C0;
-		gb_addr_config = RREG32_SOC15(GC, 0, mmGB_ADDR_CONFIG);
-		gb_addr_config &= ~0xf3e777ff;
-		gb_addr_config |= 0x22014042;
-		/* check vbios table if gpu info is not available */
-		err = amdgpu_atomfirmware_get_gfx_info(adev);
-		if (err)
-			return err;
-		break;
-	default:
-		BUG();
+		case IP_VERSION(9, 4, 1):
+			adev->gfx.ras = &gfx_v9_4_ras;
+			adev->gfx.config.max_hw_contexts = 8;
+			adev->gfx.config.sc_prim_fifo_size_frontend = 0x20;
+			adev->gfx.config.sc_prim_fifo_size_backend = 0x100;
+			adev->gfx.config.sc_hiz_tile_fifo_size = 0x30;
+			adev->gfx.config.sc_earlyz_tile_fifo_size = 0x4C0;
+			gb_addr_config = RREG32_SOC15(GC, 0, mmGB_ADDR_CONFIG);
+			gb_addr_config &= ~0xf3e777ff;
+			gb_addr_config |= 0x22014042;
+			break;
+		case IP_VERSION(9, 3, 0):
+			adev->gfx.config.max_hw_contexts = 8;
+			adev->gfx.config.sc_prim_fifo_size_frontend = 0x20;
+			adev->gfx.config.sc_prim_fifo_size_backend = 0x100;
+			adev->gfx.config.sc_hiz_tile_fifo_size = 0x80;
+			adev->gfx.config.sc_earlyz_tile_fifo_size = 0x4C0;
+			gb_addr_config = RREG32_SOC15(GC, 0, mmGB_ADDR_CONFIG);
+			gb_addr_config &= ~0xf3e777ff;
+			gb_addr_config |= 0x22010042;
+			break;
+		case IP_VERSION(9, 4, 2):
+			adev->gfx.ras = &gfx_v9_4_2_ras;
+			adev->gfx.config.max_hw_contexts = 8;
+			adev->gfx.config.sc_prim_fifo_size_frontend = 0x20;
+			adev->gfx.config.sc_prim_fifo_size_backend = 0x100;
+			adev->gfx.config.sc_hiz_tile_fifo_size = 0x30;
+			adev->gfx.config.sc_earlyz_tile_fifo_size = 0x4C0;
+			gb_addr_config = RREG32_SOC15(GC, 0, mmGB_ADDR_CONFIG);
+			gb_addr_config &= ~0xf3e777ff;
+			gb_addr_config |= 0x22014042;
+			err = amdgpu_atomfirmware_get_gfx_info(adev);
+			if (err)
+				return err;
 		break;
+		default:
+			BUG();
+			break;
 	}
 
 	adev->gfx.config.gb_addr_config = gb_addr_config;
 
 	adev->gfx.config.gb_addr_config_fields.num_pipes = 1 <<
-			REG_GET_FIELD(
-					adev->gfx.config.gb_addr_config,
-					GB_ADDR_CONFIG,
-					NUM_PIPES);
+	REG_GET_FIELD(
+		adev->gfx.config.gb_addr_config,
+		GB_ADDR_CONFIG,
+		NUM_PIPES);
 
 	adev->gfx.config.max_tile_pipes =
-		adev->gfx.config.gb_addr_config_fields.num_pipes;
+	adev->gfx.config.gb_addr_config_fields.num_pipes;
 
 	adev->gfx.config.gb_addr_config_fields.num_banks = 1 <<
-			REG_GET_FIELD(
-					adev->gfx.config.gb_addr_config,
-					GB_ADDR_CONFIG,
-					NUM_BANKS);
+	REG_GET_FIELD(
+		adev->gfx.config.gb_addr_config,
+		GB_ADDR_CONFIG,
+		NUM_BANKS);
 	adev->gfx.config.gb_addr_config_fields.max_compress_frags = 1 <<
-			REG_GET_FIELD(
-					adev->gfx.config.gb_addr_config,
-					GB_ADDR_CONFIG,
-					MAX_COMPRESSED_FRAGS);
+	REG_GET_FIELD(
+		adev->gfx.config.gb_addr_config,
+		GB_ADDR_CONFIG,
+		MAX_COMPRESSED_FRAGS);
 	adev->gfx.config.gb_addr_config_fields.num_rb_per_se = 1 <<
-			REG_GET_FIELD(
-					adev->gfx.config.gb_addr_config,
-					GB_ADDR_CONFIG,
-					NUM_RB_PER_SE);
+	REG_GET_FIELD(
+		adev->gfx.config.gb_addr_config,
+		GB_ADDR_CONFIG,
+		NUM_RB_PER_SE);
 	adev->gfx.config.gb_addr_config_fields.num_se = 1 <<
-			REG_GET_FIELD(
-					adev->gfx.config.gb_addr_config,
-					GB_ADDR_CONFIG,
-					NUM_SHADER_ENGINES);
+	REG_GET_FIELD(
+		adev->gfx.config.gb_addr_config,
+		GB_ADDR_CONFIG,
+		NUM_SHADER_ENGINES);
 	adev->gfx.config.gb_addr_config_fields.pipe_interleave_size = 1 << (8 +
-			REG_GET_FIELD(
-					adev->gfx.config.gb_addr_config,
-					GB_ADDR_CONFIG,
-					PIPE_INTERLEAVE_SIZE));
+	REG_GET_FIELD(
+		adev->gfx.config.gb_addr_config,
+		GB_ADDR_CONFIG,
+		PIPE_INTERLEAVE_SIZE));
 
 	return 0;
 }
 
 static int gfx_v9_0_compute_ring_init(struct amdgpu_device *adev, int ring_id,
-				      int mec, int pipe, int queue)
+									  int mec, int pipe, int queue)
 {
 	unsigned irq_type;
 	struct amdgpu_ring *ring = &adev->gfx.compute_ring[ring_id];
@@ -2158,7 +2115,6 @@ static int gfx_v9_0_compute_ring_init(st
 
 	ring = &adev->gfx.compute_ring[ring_id];
 
-	/* mec0 is me1 */
 	ring->me = mec + 1;
 	ring->pipe = pipe;
 	ring->queue = queue;
@@ -2167,18 +2123,17 @@ static int gfx_v9_0_compute_ring_init(st
 	ring->use_doorbell = true;
 	ring->doorbell_index = (adev->doorbell_index.mec_ring0 + ring_id) << 1;
 	ring->eop_gpu_addr = adev->gfx.mec.hpd_eop_gpu_addr
-				+ (ring_id * GFX9_MEC_HPD_SIZE);
+	+ (ring_id * GFX9_MEC_HPD_SIZE);
 	ring->vm_hub = AMDGPU_GFXHUB(0);
 	sprintf(ring->name, "comp_%d.%d.%d", ring->me, ring->pipe, ring->queue);
 
 	irq_type = AMDGPU_CP_IRQ_COMPUTE_MEC1_PIPE0_EOP
-		+ ((ring->me - 1) * adev->gfx.mec.num_pipe_per_mec)
-		+ ring->pipe;
+	+ ((ring->me - 1) * adev->gfx.mec.num_pipe_per_mec)
+	+ ring->pipe;
 	hw_prio = amdgpu_gfx_is_high_priority_compute_queue(adev, ring) ?
-			AMDGPU_RING_PRIO_2 : AMDGPU_RING_PRIO_DEFAULT;
-	/* type-2 packets are deprecated on MEC, use type-3 instead */
+	AMDGPU_RING_PRIO_2 : AMDGPU_RING_PRIO_DEFAULT;
 	return amdgpu_ring_init(adev, ring, 1024, &adev->gfx.eop_irq, irq_type,
-				hw_prio, NULL);
+							hw_prio, NULL);
 }
 
 static void gfx_v9_0_alloc_ip_dump(struct amdgpu_device *adev)
@@ -2195,10 +2150,9 @@ static void gfx_v9_0_alloc_ip_dump(struc
 		adev->gfx.ip_dump_core = ptr;
 	}
 
-	/* Allocate memory for compute queue registers for all the instances */
 	reg_count = ARRAY_SIZE(gc_cp_reg_list_9);
 	inst = adev->gfx.mec.num_mec * adev->gfx.mec.num_pipe_per_mec *
-		adev->gfx.mec.num_queue_per_pipe;
+	adev->gfx.mec.num_queue_per_pipe;
 
 	ptr = kcalloc(reg_count * inst, sizeof(uint32_t), GFP_KERNEL);
 	if (!ptr) {
@@ -2218,75 +2172,69 @@ static int gfx_v9_0_sw_init(struct amdgp
 	unsigned int hw_prio;
 
 	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
-	case IP_VERSION(9, 0, 1):
-	case IP_VERSION(9, 2, 1):
-	case IP_VERSION(9, 4, 0):
-	case IP_VERSION(9, 2, 2):
-	case IP_VERSION(9, 1, 0):
-	case IP_VERSION(9, 4, 1):
-	case IP_VERSION(9, 3, 0):
-	case IP_VERSION(9, 4, 2):
-		adev->gfx.mec.num_mec = 2;
-		break;
-	default:
-		adev->gfx.mec.num_mec = 1;
-		break;
+		case IP_VERSION(9, 0, 1):
+		case IP_VERSION(9, 2, 1):
+		case IP_VERSION(9, 4, 0):
+		case IP_VERSION(9, 2, 2):
+		case IP_VERSION(9, 1, 0):
+		case IP_VERSION(9, 4, 1):
+		case IP_VERSION(9, 3, 0):
+		case IP_VERSION(9, 4, 2):
+			adev->gfx.mec.num_mec = 2;
+			break;
+		default:
+			adev->gfx.mec.num_mec = 1;
+			break;
 	}
 
 	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
-	case IP_VERSION(9, 4, 2):
-		adev->gfx.cleaner_shader_ptr = gfx_9_4_2_cleaner_shader_hex;
-		adev->gfx.cleaner_shader_size = sizeof(gfx_9_4_2_cleaner_shader_hex);
-		if (adev->gfx.mec_fw_version >= 88) {
-			adev->gfx.enable_cleaner_shader = true;
-			r = amdgpu_gfx_cleaner_shader_sw_init(adev, adev->gfx.cleaner_shader_size);
-			if (r) {
-				adev->gfx.enable_cleaner_shader = false;
-				dev_err(adev->dev, "Failed to initialize cleaner shader\n");
+		case IP_VERSION(9, 4, 2):
+			adev->gfx.cleaner_shader_ptr = gfx_9_4_2_cleaner_shader_hex;
+			adev->gfx.cleaner_shader_size = sizeof(gfx_9_4_2_cleaner_shader_hex);
+			if (adev->gfx.mec_fw_version >= 88) {
+				adev->gfx.enable_cleaner_shader = true;
+				r = amdgpu_gfx_cleaner_shader_sw_init(adev, adev->gfx.cleaner_shader_size);
+				if (r) {
+					adev->gfx.enable_cleaner_shader = false;
+					dev_err(adev->dev, "Failed to initialize cleaner shader\n");
+				}
 			}
-		}
-		break;
-	default:
-		adev->gfx.enable_cleaner_shader = false;
-		break;
+			break;
+		default:
+			adev->gfx.enable_cleaner_shader = false;
+			break;
 	}
 
 	adev->gfx.mec.num_pipe_per_mec = 4;
 	adev->gfx.mec.num_queue_per_pipe = 8;
 
-	/* EOP Event */
 	r = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_GRBM_CP, GFX_9_0__SRCID__CP_EOP_INTERRUPT, &adev->gfx.eop_irq);
 	if (r)
 		return r;
 
-	/* Bad opcode Event */
 	r = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_GRBM_CP,
-			      GFX_9_0__SRCID__CP_BAD_OPCODE_ERROR,
-			      &adev->gfx.bad_op_irq);
+						  GFX_9_0__SRCID__CP_BAD_OPCODE_ERROR,
+					   &adev->gfx.bad_op_irq);
 	if (r)
 		return r;
 
-	/* Privileged reg */
 	r = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_GRBM_CP, GFX_9_0__SRCID__CP_PRIV_REG_FAULT,
-			      &adev->gfx.priv_reg_irq);
+						  &adev->gfx.priv_reg_irq);
 	if (r)
 		return r;
 
-	/* Privileged inst */
 	r = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_GRBM_CP, GFX_9_0__SRCID__CP_PRIV_INSTR_FAULT,
-			      &adev->gfx.priv_inst_irq);
+						  &adev->gfx.priv_inst_irq);
 	if (r)
 		return r;
 
-	/* ECC error */
 	r = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_GRBM_CP, GFX_9_0__SRCID__CP_ECC_ERROR,
-			      &adev->gfx.cp_ecc_error_irq);
+						  &adev->gfx.cp_ecc_error_irq);
 	if (r)
 		return r;
 
-	/* FUE error */
 	r = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_GRBM_CP, GFX_9_0__SRCID__CP_FUE_ERROR,
-			      &adev->gfx.cp_ecc_error_irq);
+						  &adev->gfx.cp_ecc_error_irq);
 	if (r)
 		return r;
 
@@ -2308,7 +2256,6 @@ static int gfx_v9_0_sw_init(struct amdgp
 		return r;
 	}
 
-	/* set up the gfx ring */
 	for (i = 0; i < adev->gfx.num_gfx_rings; i++) {
 		ring = &adev->gfx.gfx_ring[i];
 		ring->ring_obj = NULL;
@@ -2319,17 +2266,15 @@ static int gfx_v9_0_sw_init(struct amdgp
 		ring->use_doorbell = true;
 		ring->doorbell_index = adev->doorbell_index.gfx_ring0 << 1;
 
-		/* disable scheduler on the real ring */
 		ring->no_scheduler = adev->gfx.mcbp;
 		ring->vm_hub = AMDGPU_GFXHUB(0);
 		r = amdgpu_ring_init(adev, ring, 1024, &adev->gfx.eop_irq,
-				     AMDGPU_CP_IRQ_GFX_ME0_PIPE0_EOP,
-				     AMDGPU_RING_PRIO_DEFAULT, NULL);
+							 AMDGPU_CP_IRQ_GFX_ME0_PIPE0_EOP,
+					   AMDGPU_RING_PRIO_DEFAULT, NULL);
 		if (r)
 			return r;
 	}
 
-	/* set up the software rings */
 	if (adev->gfx.mcbp && adev->gfx.num_gfx_rings) {
 		for (i = 0; i < GFX9_NUM_SW_GFX_RINGS; i++) {
 			ring = &adev->gfx.sw_gfx_ring[i];
@@ -2341,23 +2286,22 @@ static int gfx_v9_0_sw_init(struct amdgp
 			hw_prio = amdgpu_sw_ring_priority(i);
 			ring->vm_hub = AMDGPU_GFXHUB(0);
 			r = amdgpu_ring_init(adev, ring, 1024, &adev->gfx.eop_irq,
-					     AMDGPU_CP_IRQ_GFX_ME0_PIPE0_EOP, hw_prio,
-					     NULL);
+								 AMDGPU_CP_IRQ_GFX_ME0_PIPE0_EOP, hw_prio,
+						NULL);
 			if (r)
 				return r;
 			ring->wptr = 0;
 		}
 
-		/* init the muxer and add software rings */
 		r = amdgpu_ring_mux_init(&adev->gfx.muxer, &adev->gfx.gfx_ring[0],
-					 GFX9_NUM_SW_GFX_RINGS);
+								 GFX9_NUM_SW_GFX_RINGS);
 		if (r) {
 			DRM_ERROR("amdgpu_ring_mux_init failed(%d)\n", r);
 			return r;
 		}
 		for (i = 0; i < GFX9_NUM_SW_GFX_RINGS; i++) {
 			r = amdgpu_ring_mux_add_sw_ring(&adev->gfx.muxer,
-							&adev->gfx.sw_gfx_ring[i]);
+											&adev->gfx.sw_gfx_ring[i]);
 			if (r) {
 				DRM_ERROR("amdgpu_ring_mux_add_sw_ring failed(%d)\n", r);
 				return r;
@@ -2365,18 +2309,17 @@ static int gfx_v9_0_sw_init(struct amdgp
 		}
 	}
 
-	/* set up the compute queues - allocate horizontally across pipes */
 	ring_id = 0;
 	for (i = 0; i < adev->gfx.mec.num_mec; ++i) {
 		for (j = 0; j < adev->gfx.mec.num_queue_per_pipe; j++) {
 			for (k = 0; k < adev->gfx.mec.num_pipe_per_mec; k++) {
 				if (!amdgpu_gfx_is_mec_queue_enabled(adev, 0, i,
-								     k, j))
+					k, j))
 					continue;
 
 				r = gfx_v9_0_compute_ring_init(adev,
-							       ring_id,
-							       i, k, j);
+   ring_id,
+								   i, k, j);
 				if (r)
 					return r;
 
@@ -2385,11 +2328,10 @@ static int gfx_v9_0_sw_init(struct amdgp
 		}
 	}
 
-	/* TODO: Add queue reset mask when FW fully supports it */
 	adev->gfx.gfx_supported_reset =
-		amdgpu_get_soft_full_reset_mask(&adev->gfx.gfx_ring[0]);
+	amdgpu_get_soft_full_reset_mask(&adev->gfx.gfx_ring[0]);
 	adev->gfx.compute_supported_reset =
-		amdgpu_get_soft_full_reset_mask(&adev->gfx.compute_ring[0]);
+	amdgpu_get_soft_full_reset_mask(&adev->gfx.compute_ring[0]);
 
 	r = amdgpu_gfx_kiq_init(adev, GFX9_MEC_HPD_SIZE, 0);
 	if (r) {
@@ -2401,7 +2343,6 @@ static int gfx_v9_0_sw_init(struct amdgp
 	if (r)
 		return r;
 
-	/* create MQD for all compute queues as wel as KIQ for SRIOV case */
 	r = amdgpu_gfx_mqd_sw_init(adev, sizeof(struct v9_mqd_allocation), 0);
 	if (r)
 		return r;
@@ -2451,12 +2392,12 @@ static int gfx_v9_0_sw_fini(struct amdgp
 
 	gfx_v9_0_mec_fini(adev);
 	amdgpu_bo_free_kernel(&adev->gfx.rlc.clear_state_obj,
-				&adev->gfx.rlc.clear_state_gpu_addr,
-				(void **)&adev->gfx.rlc.cs_ptr);
+						  &adev->gfx.rlc.clear_state_gpu_addr,
+					   (void **)&adev->gfx.rlc.cs_ptr);
 	if (adev->flags & AMD_IS_APU) {
 		amdgpu_bo_free_kernel(&adev->gfx.rlc.cp_table_obj,
-				&adev->gfx.rlc.cp_table_gpu_addr,
-				(void **)&adev->gfx.rlc.cp_table_ptr);
+							  &adev->gfx.rlc.cp_table_gpu_addr,
+						(void **)&adev->gfx.rlc.cp_table_ptr);
 	}
 	gfx_v9_0_free_microcode(adev);
 
@@ -2471,11 +2412,10 @@ static int gfx_v9_0_sw_fini(struct amdgp
 
 static void gfx_v9_0_tiling_mode_table_init(struct amdgpu_device *adev)
 {
-	/* TODO */
 }
 
 void gfx_v9_0_select_se_sh(struct amdgpu_device *adev, u32 se_num, u32 sh_num,
-			   u32 instance, int xcc_id)
+						   u32 instance, int xcc_id)
 {
 	u32 data;
 
@@ -2508,7 +2448,7 @@ static u32 gfx_v9_0_get_rb_active_bitmap
 	data >>= GC_USER_RB_BACKEND_DISABLE__BACKEND_DISABLE__SHIFT;
 
 	mask = amdgpu_gfx_create_bitmask(adev->gfx.config.max_backends_per_se /
-					 adev->gfx.config.max_sh_per_se);
+	adev->gfx.config.max_sh_per_se);
 
 	return (~data) & mask;
 }
@@ -2519,7 +2459,7 @@ static void gfx_v9_0_setup_rb(struct amd
 	u32 data;
 	u32 active_rbs = 0;
 	u32 rb_bitmap_width_per_sh = adev->gfx.config.max_backends_per_se /
-					adev->gfx.config.max_sh_per_se;
+	adev->gfx.config.max_sh_per_se;
 
 	mutex_lock(&adev->grbm_idx_mutex);
 	for (i = 0; i < adev->gfx.config.max_shader_engines; i++) {
@@ -2527,7 +2467,7 @@ static void gfx_v9_0_setup_rb(struct amd
 			amdgpu_gfx_select_se_sh(adev, i, j, 0xffffffff, 0);
 			data = gfx_v9_0_get_rb_active_bitmap(adev);
 			active_rbs |= data << ((i * adev->gfx.config.max_sh_per_se + j) *
-					       rb_bitmap_width_per_sh);
+			rb_bitmap_width_per_sh);
 		}
 	}
 	amdgpu_gfx_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff, 0);
@@ -2538,21 +2478,20 @@ static void gfx_v9_0_setup_rb(struct amd
 }
 
 static void gfx_v9_0_debug_trap_config_init(struct amdgpu_device *adev,
-				uint32_t first_vmid,
-				uint32_t last_vmid)
+											uint32_t first_vmid,
+											uint32_t last_vmid)
 {
 	uint32_t data;
 	uint32_t trap_config_vmid_mask = 0;
 	int i;
 
-	/* Calculate trap config vmid mask */
 	for (i = first_vmid; i < last_vmid; i++)
 		trap_config_vmid_mask |= (1 << i);
 
 	data = REG_SET_FIELD(0, SPI_GDBG_TRAP_CONFIG,
-			VMID_SEL, trap_config_vmid_mask);
+						 VMID_SEL, trap_config_vmid_mask);
 	data = REG_SET_FIELD(data, SPI_GDBG_TRAP_CONFIG,
-			TRAP_EN, 1);
+						 TRAP_EN, 1);
 	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_TRAP_CONFIG), data);
 	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_TRAP_MASK), 0);
 
@@ -2567,30 +2506,21 @@ static void gfx_v9_0_init_compute_vmid(s
 	uint32_t sh_mem_config;
 	uint32_t sh_mem_bases;
 
-	/*
-	 * Configure apertures:
-	 * LDS:         0x60000000'00000000 - 0x60000001'00000000 (4GB)
-	 * Scratch:     0x60000001'00000000 - 0x60000002'00000000 (4GB)
-	 * GPUVM:       0x60010000'00000000 - 0x60020000'00000000 (1TB)
-	 */
 	sh_mem_bases = DEFAULT_SH_MEM_BASES | (DEFAULT_SH_MEM_BASES << 16);
 
 	sh_mem_config = SH_MEM_ADDRESS_MODE_64 |
-			SH_MEM_ALIGNMENT_MODE_UNALIGNED <<
-			SH_MEM_CONFIG__ALIGNMENT_MODE__SHIFT;
+	SH_MEM_ALIGNMENT_MODE_UNALIGNED <<
+	SH_MEM_CONFIG__ALIGNMENT_MODE__SHIFT;
 
 	mutex_lock(&adev->srbm_mutex);
 	for (i = adev->vm_manager.first_kfd_vmid; i < AMDGPU_NUM_VMID; i++) {
 		soc15_grbm_select(adev, 0, 0, 0, i, 0);
-		/* CP and shaders */
 		WREG32_SOC15_RLC(GC, 0, mmSH_MEM_CONFIG, sh_mem_config);
 		WREG32_SOC15_RLC(GC, 0, mmSH_MEM_BASES, sh_mem_bases);
 	}
 	soc15_grbm_select(adev, 0, 0, 0, 0, 0);
 	mutex_unlock(&adev->srbm_mutex);
 
-	/* Initialize all compute VMIDs to have no GDS, GWS, or OA
-	   access. These should be enabled by FW for target VMIDs. */
 	for (i = adev->vm_manager.first_kfd_vmid; i < AMDGPU_NUM_VMID; i++) {
 		WREG32_SOC15_OFFSET(GC, 0, mmGDS_VMID0_BASE, 2 * i, 0);
 		WREG32_SOC15_OFFSET(GC, 0, mmGDS_VMID0_SIZE, 2 * i, 0);
@@ -2603,12 +2533,6 @@ static void gfx_v9_0_init_gds_vmid(struc
 {
 	int vmid;
 
-	/*
-	 * Initialize all compute and user-gfx VMIDs to have no GDS, GWS, or OA
-	 * access. Compute VMIDs should be enabled by FW for target VMIDs,
-	 * the driver can enable them for graphics. VMID0 should maintain
-	 * access so that HWS firmware can save/restore entries.
-	 */
 	for (vmid = 1; vmid < AMDGPU_NUM_VMID; vmid++) {
 		WREG32_SOC15_OFFSET(GC, 0, mmGDS_VMID0_BASE, 2 * vmid, 0);
 		WREG32_SOC15_OFFSET(GC, 0, mmGDS_VMID0_SIZE, 2 * vmid, 0);
@@ -2622,14 +2546,14 @@ static void gfx_v9_0_init_sq_config(stru
 	uint32_t tmp;
 
 	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
-	case IP_VERSION(9, 4, 1):
-		tmp = RREG32_SOC15(GC, 0, mmSQ_CONFIG);
-		tmp = REG_SET_FIELD(tmp, SQ_CONFIG, DISABLE_BARRIER_WAITCNT,
-				!READ_ONCE(adev->barrier_has_auto_waitcnt));
-		WREG32_SOC15(GC, 0, mmSQ_CONFIG, tmp);
-		break;
-	default:
-		break;
+		case IP_VERSION(9, 4, 1):
+			tmp = RREG32_SOC15(GC, 0, mmSQ_CONFIG);
+			tmp = REG_SET_FIELD(tmp, SQ_CONFIG, DISABLE_BARRIER_WAITCNT,
+								!READ_ONCE(adev->barrier_has_auto_waitcnt));
+			WREG32_SOC15(GC, 0, mmSQ_CONFIG, tmp);
+			break;
+		default:
+			break;
 	}
 }
 
@@ -2647,29 +2571,26 @@ static void gfx_v9_0_constants_init(stru
 	gfx_v9_0_get_cu_info(adev, &adev->gfx.cu_info);
 	adev->gfx.config.db_debug2 = RREG32_SOC15(GC, 0, mmDB_DEBUG2);
 
-	/* XXX SH_MEM regs */
-	/* where to put LDS, scratch, GPUVM in FSA64 space */
 	mutex_lock(&adev->srbm_mutex);
 	for (i = 0; i < adev->vm_manager.id_mgr[AMDGPU_GFXHUB(0)].num_ids; i++) {
 		soc15_grbm_select(adev, 0, 0, 0, i, 0);
-		/* CP and shaders */
 		if (i == 0) {
 			tmp = REG_SET_FIELD(0, SH_MEM_CONFIG, ALIGNMENT_MODE,
-					    SH_MEM_ALIGNMENT_MODE_UNALIGNED);
+								SH_MEM_ALIGNMENT_MODE_UNALIGNED);
 			tmp = REG_SET_FIELD(tmp, SH_MEM_CONFIG, RETRY_DISABLE,
-					    !!adev->gmc.noretry);
+								!!adev->gmc.noretry);
 			WREG32_SOC15_RLC(GC, 0, mmSH_MEM_CONFIG, tmp);
 			WREG32_SOC15_RLC(GC, 0, mmSH_MEM_BASES, 0);
 		} else {
 			tmp = REG_SET_FIELD(0, SH_MEM_CONFIG, ALIGNMENT_MODE,
-					    SH_MEM_ALIGNMENT_MODE_UNALIGNED);
+								SH_MEM_ALIGNMENT_MODE_UNALIGNED);
 			tmp = REG_SET_FIELD(tmp, SH_MEM_CONFIG, RETRY_DISABLE,
-					    !!adev->gmc.noretry);
+								!!adev->gmc.noretry);
 			WREG32_SOC15_RLC(GC, 0, mmSH_MEM_CONFIG, tmp);
 			tmp = REG_SET_FIELD(0, SH_MEM_BASES, PRIVATE_BASE,
-				(adev->gmc.private_aperture_start >> 48));
+								(adev->gmc.private_aperture_start >> 48));
 			tmp = REG_SET_FIELD(tmp, SH_MEM_BASES, SHARED_BASE,
-				(adev->gmc.shared_aperture_start >> 48));
+								(adev->gmc.shared_aperture_start >> 48));
 			WREG32_SOC15_RLC(GC, 0, mmSH_MEM_BASES, tmp);
 		}
 	}
@@ -2682,51 +2603,47 @@ static void gfx_v9_0_constants_init(stru
 	gfx_v9_0_init_sq_config(adev);
 }
 
-static void gfx_v9_0_wait_for_rlc_serdes(struct amdgpu_device *adev)
+static inline void gfx_v9_0_wait_for_rlc_serdes(struct amdgpu_device *adev)
 {
-	u32 i, j, k;
-	u32 mask;
+	const u32 BCAST  = 0xffffffff;
+	const unsigned long tmo = adev->usec_timeout / 5 + 1;
+	const u32 noncu_mask =
+	RLC_SERDES_NONCU_MASTER_BUSY__SE_MASTER_BUSY_MASK |
+	RLC_SERDES_NONCU_MASTER_BUSY__GC_MASTER_BUSY_MASK |
+	RLC_SERDES_NONCU_MASTER_BUSY__TC0_MASTER_BUSY_MASK |
+	RLC_SERDES_NONCU_MASTER_BUSY__TC1_MASTER_BUSY_MASK;
+	int r;
 
 	mutex_lock(&adev->grbm_idx_mutex);
-	for (i = 0; i < adev->gfx.config.max_shader_engines; i++) {
-		for (j = 0; j < adev->gfx.config.max_sh_per_se; j++) {
-			amdgpu_gfx_select_se_sh(adev, i, j, 0xffffffff, 0);
-			for (k = 0; k < adev->usec_timeout; k++) {
-				if (RREG32_SOC15(GC, 0, mmRLC_SERDES_CU_MASTER_BUSY) == 0)
-					break;
-				udelay(1);
-			}
-			if (k == adev->usec_timeout) {
-				amdgpu_gfx_select_se_sh(adev, 0xffffffff,
-						      0xffffffff, 0xffffffff, 0);
-				mutex_unlock(&adev->grbm_idx_mutex);
-				DRM_INFO("Timeout wait for RLC serdes %u,%u\n",
-					 i, j);
-				return;
-			}
-		}
-	}
-	amdgpu_gfx_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff, 0);
+	amdgpu_gfx_select_se_sh(adev, BCAST, BCAST, BCAST, 0);
 	mutex_unlock(&adev->grbm_idx_mutex);
 
-	mask = RLC_SERDES_NONCU_MASTER_BUSY__SE_MASTER_BUSY_MASK |
-		RLC_SERDES_NONCU_MASTER_BUSY__GC_MASTER_BUSY_MASK |
-		RLC_SERDES_NONCU_MASTER_BUSY__TC0_MASTER_BUSY_MASK |
-		RLC_SERDES_NONCU_MASTER_BUSY__TC1_MASTER_BUSY_MASK;
-	for (k = 0; k < adev->usec_timeout; k++) {
-		if ((RREG32_SOC15(GC, 0, mmRLC_SERDES_NONCU_MASTER_BUSY) & mask) == 0)
-			break;
-		udelay(1);
-	}
+	r = gfx9_wait_reg_off(adev,
+						  SOC15_REG_OFFSET(GC, 0, mmRLC_SERDES_CU_MASTER_BUSY),
+						  ~0u, 0, tmo);
+	if (r)
+		dev_info_ratelimited(adev->dev,
+							 "RLC SERDES: CU busy bits stuck\n");
+
+		if (!r) {
+			r = gfx9_wait_reg_off(adev,
+								  SOC15_REG_OFFSET(GC, 0, mmRLC_SERDES_NONCU_MASTER_BUSY),
+								  noncu_mask, 0, tmo);
+			if (r)
+				dev_info_ratelimited(adev->dev,
+									 "RLC SERDES: NON‑CU busy bits stuck\n");
+		}
+
+		mutex_lock(&adev->grbm_idx_mutex);
+		amdgpu_gfx_select_se_sh(adev, BCAST, BCAST, BCAST, 0);
+		mutex_unlock(&adev->grbm_idx_mutex);
 }
 
 static void gfx_v9_0_enable_gui_idle_interrupt(struct amdgpu_device *adev,
-					       bool enable)
+   bool enable)
 {
 	u32 tmp;
 
-	/* These interrupts should be enabled to drive DS clock */
-
 	tmp= RREG32_SOC15(GC, 0, mmCP_INT_CNTL_RING0);
 
 	tmp = REG_SET_FIELD(tmp, CP_INT_CNTL_RING0, CNTX_BUSY_INT_ENABLE, enable ? 1 : 0);
@@ -2741,23 +2658,22 @@ static void gfx_v9_0_enable_gui_idle_int
 static void gfx_v9_0_init_csb(struct amdgpu_device *adev)
 {
 	adev->gfx.rlc.funcs->get_csb_buffer(adev, adev->gfx.rlc.cs_ptr);
-	/* csib */
 	WREG32_RLC(SOC15_REG_OFFSET(GC, 0, mmRLC_CSIB_ADDR_HI),
-			adev->gfx.rlc.clear_state_gpu_addr >> 32);
+			   adev->gfx.rlc.clear_state_gpu_addr >> 32);
 	WREG32_RLC(SOC15_REG_OFFSET(GC, 0, mmRLC_CSIB_ADDR_LO),
-			adev->gfx.rlc.clear_state_gpu_addr & 0xfffffffc);
+			   adev->gfx.rlc.clear_state_gpu_addr & 0xfffffffc);
 	WREG32_RLC(SOC15_REG_OFFSET(GC, 0, mmRLC_CSIB_LENGTH),
-			adev->gfx.rlc.clear_state_size);
+			   adev->gfx.rlc.clear_state_size);
 }
 
 static void gfx_v9_1_parse_ind_reg_list(int *register_list_format,
-				int indirect_offset,
-				int list_size,
-				int *unique_indirect_regs,
-				int unique_indirect_reg_count,
-				int *indirect_start_offsets,
-				int *indirect_start_offsets_count,
-				int max_start_offsets_count)
+										int indirect_offset,
+										int list_size,
+										int *unique_indirect_regs,
+										int unique_indirect_reg_count,
+										int *indirect_start_offsets,
+										int *indirect_start_offsets_count,
+										int max_start_offsets_count)
 {
 	int idx;
 
@@ -2769,7 +2685,6 @@ static void gfx_v9_1_parse_ind_reg_list(
 		while (register_list_format[indirect_offset] != 0xFFFFFFFF) {
 			indirect_offset += 2;
 
-			/* look for the matching indice */
 			for (idx = 0; idx < unique_indirect_reg_count; idx++) {
 				if (unique_indirect_regs[idx] ==
 					register_list_format[indirect_offset] ||
@@ -2800,94 +2715,85 @@ static int gfx_v9_1_init_rlc_save_restor
 	u32 tmp = 0;
 
 	u32 *register_list_format =
-		kmemdup(adev->gfx.rlc.register_list_format,
+	kmemdup(adev->gfx.rlc.register_list_format,
 			adev->gfx.rlc.reg_list_format_size_bytes, GFP_KERNEL);
 	if (!register_list_format)
 		return -ENOMEM;
 
-	/* setup unique_indirect_regs array and indirect_start_offsets array */
 	unique_indirect_reg_count = ARRAY_SIZE(unique_indirect_regs);
 	gfx_v9_1_parse_ind_reg_list(register_list_format,
-				    adev->gfx.rlc.reg_list_format_direct_reg_list_length,
-				    adev->gfx.rlc.reg_list_format_size_bytes >> 2,
-				    unique_indirect_regs,
-				    unique_indirect_reg_count,
-				    indirect_start_offsets,
-				    &indirect_start_offsets_count,
-				    ARRAY_SIZE(indirect_start_offsets));
+								adev->gfx.rlc.reg_list_format_direct_reg_list_length,
+							 adev->gfx.rlc.reg_list_format_size_bytes >> 2,
+							 unique_indirect_regs,
+							 unique_indirect_reg_count,
+							 indirect_start_offsets,
+							 &indirect_start_offsets_count,
+							 ARRAY_SIZE(indirect_start_offsets));
 
-	/* enable auto inc in case it is disabled */
 	tmp = RREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_SRM_CNTL));
 	tmp |= RLC_SRM_CNTL__AUTO_INCR_ADDR_MASK;
 	WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_SRM_CNTL), tmp);
 
-	/* write register_restore table to offset 0x0 using RLC_SRM_ARAM_ADDR/DATA */
 	WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_SRM_ARAM_ADDR),
-		RLC_SAVE_RESTORE_ADDR_STARTING_OFFSET);
+		   RLC_SAVE_RESTORE_ADDR_STARTING_OFFSET);
 	for (i = 0; i < adev->gfx.rlc.reg_list_size_bytes >> 2; i++)
 		WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_SRM_ARAM_DATA),
-			adev->gfx.rlc.register_restore[i]);
+			   adev->gfx.rlc.register_restore[i]);
 
-	/* load indirect register */
-	WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_GPM_SCRATCH_ADDR),
-		adev->gfx.rlc.reg_list_format_start);
-
-	/* direct register portion */
-	for (i = 0; i < adev->gfx.rlc.reg_list_format_direct_reg_list_length; i++)
-		WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_GPM_SCRATCH_DATA),
-			register_list_format[i]);
-
-	/* indirect register portion */
-	while (i < (adev->gfx.rlc.reg_list_format_size_bytes >> 2)) {
-		if (register_list_format[i] == 0xFFFFFFFF) {
-			WREG32_SOC15(GC, 0, mmRLC_GPM_SCRATCH_DATA, register_list_format[i++]);
-			continue;
-		}
+		WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_GPM_SCRATCH_ADDR),
+			   adev->gfx.rlc.reg_list_format_start);
 
-		WREG32_SOC15(GC, 0, mmRLC_GPM_SCRATCH_DATA, register_list_format[i++]);
-		WREG32_SOC15(GC, 0, mmRLC_GPM_SCRATCH_DATA, register_list_format[i++]);
+		for (i = 0; i < adev->gfx.rlc.reg_list_format_direct_reg_list_length; i++)
+			WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_GPM_SCRATCH_DATA),
+				   register_list_format[i]);
+
+			while (i < (adev->gfx.rlc.reg_list_format_size_bytes >> 2)) {
+				if (register_list_format[i] == 0xFFFFFFFF) {
+					WREG32_SOC15(GC, 0, mmRLC_GPM_SCRATCH_DATA, register_list_format[i++]);
+					continue;
+				}
 
-		for (j = 0; j < unique_indirect_reg_count; j++) {
-			if (register_list_format[i] == unique_indirect_regs[j]) {
-				WREG32_SOC15(GC, 0, mmRLC_GPM_SCRATCH_DATA, j);
-				break;
-			}
-		}
+				WREG32_SOC15(GC, 0, mmRLC_GPM_SCRATCH_DATA, register_list_format[i++]);
+				WREG32_SOC15(GC, 0, mmRLC_GPM_SCRATCH_DATA, register_list_format[i++]);
 
-		BUG_ON(j >= unique_indirect_reg_count);
+				for (j = 0; j < unique_indirect_reg_count; j++) {
+					if (register_list_format[i] == unique_indirect_regs[j]) {
+						WREG32_SOC15(GC, 0, mmRLC_GPM_SCRATCH_DATA, j);
+						break;
+					}
+				}
 
-		i++;
-	}
+				BUG_ON(j >= unique_indirect_reg_count);
+
+				i++;
+			}
 
-	/* set save/restore list size */
-	list_size = adev->gfx.rlc.reg_list_size_bytes >> 2;
-	list_size = list_size >> 1;
-	WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_GPM_SCRATCH_ADDR),
-		adev->gfx.rlc.reg_restore_list_size);
-	WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_GPM_SCRATCH_DATA), list_size);
-
-	/* write the starting offsets to RLC scratch ram */
-	WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_GPM_SCRATCH_ADDR),
-		adev->gfx.rlc.starting_offsets_start);
-	for (i = 0; i < ARRAY_SIZE(indirect_start_offsets); i++)
-		WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_GPM_SCRATCH_DATA),
-		       indirect_start_offsets[i]);
-
-	/* load unique indirect regs*/
-	for (i = 0; i < ARRAY_SIZE(unique_indirect_regs); i++) {
-		if (unique_indirect_regs[i] != 0) {
-			WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_SRM_INDEX_CNTL_ADDR_0)
-			       + GFX_RLC_SRM_INDEX_CNTL_ADDR_OFFSETS[i],
-			       unique_indirect_regs[i] & 0x3FFFF);
+			list_size = adev->gfx.rlc.reg_list_size_bytes >> 2;
+			list_size = list_size >> 1;
+			WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_GPM_SCRATCH_ADDR),
+				   adev->gfx.rlc.reg_restore_list_size);
+			WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_GPM_SCRATCH_DATA), list_size);
+
+			WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_GPM_SCRATCH_ADDR),
+				   adev->gfx.rlc.starting_offsets_start);
+			for (i = 0; i < ARRAY_SIZE(indirect_start_offsets); i++)
+				WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_GPM_SCRATCH_DATA),
+					   indirect_start_offsets[i]);
+
+				for (i = 0; i < ARRAY_SIZE(unique_indirect_regs); i++) {
+					if (unique_indirect_regs[i] != 0) {
+						WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_SRM_INDEX_CNTL_ADDR_0)
+						+ GFX_RLC_SRM_INDEX_CNTL_ADDR_OFFSETS[i],
+			 unique_indirect_regs[i] & 0x3FFFF);
 
 			WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_SRM_INDEX_CNTL_DATA_0)
-			       + GFX_RLC_SRM_INDEX_CNTL_DATA_OFFSETS[i],
-			       unique_indirect_regs[i] >> 20);
-		}
-	}
+			+ GFX_RLC_SRM_INDEX_CNTL_DATA_OFFSETS[i],
+		  unique_indirect_regs[i] >> 20);
+					}
+				}
 
-	kfree(register_list_format);
-	return 0;
+				kfree(register_list_format);
+				return 0;
 }
 
 static void gfx_v9_0_enable_save_restore_machine(struct amdgpu_device *adev)
@@ -2896,28 +2802,22 @@ static void gfx_v9_0_enable_save_restore
 }
 
 static void pwr_10_0_gfxip_control_over_cgpg(struct amdgpu_device *adev,
-					     bool enable)
+											 bool enable)
 {
-	uint32_t data = 0;
-	uint32_t default_data = 0;
+	const u32 off = SOC15_REG_OFFSET(PWR, 0, mmPWR_MISC_CNTL_STATUS);
+	u32 v;
 
-	default_data = data = RREG32(SOC15_REG_OFFSET(PWR, 0, mmPWR_MISC_CNTL_STATUS));
 	if (enable) {
-		/* enable GFXIP control over CGPG */
-		data |= PWR_MISC_CNTL_STATUS__PWR_GFX_RLC_CGPG_EN_MASK;
-		if(default_data != data)
-			WREG32(SOC15_REG_OFFSET(PWR, 0, mmPWR_MISC_CNTL_STATUS), data);
-
-		/* update status */
-		data &= ~PWR_MISC_CNTL_STATUS__PWR_GFXOFF_STATUS_MASK;
-		data |= (2 << PWR_MISC_CNTL_STATUS__PWR_GFXOFF_STATUS__SHIFT);
-		if(default_data != data)
-			WREG32(SOC15_REG_OFFSET(PWR, 0, mmPWR_MISC_CNTL_STATUS), data);
+		v = RREG32(off) | PWR_MISC_CNTL_STATUS__PWR_GFX_RLC_CGPG_EN_MASK;
+		WREG32_IF_CHANGED(off, v);
+
+		v  = RREG32(off);
+		v &= ~PWR_MISC_CNTL_STATUS__PWR_GFXOFF_STATUS_MASK;
+		v |=  (2 << PWR_MISC_CNTL_STATUS__PWR_GFXOFF_STATUS__SHIFT);
+		WREG32_IF_CHANGED(off, v);
 	} else {
-		/* restore GFXIP control over GCPG */
-		data &= ~PWR_MISC_CNTL_STATUS__PWR_GFX_RLC_CGPG_EN_MASK;
-		if(default_data != data)
-			WREG32(SOC15_REG_OFFSET(PWR, 0, mmPWR_MISC_CNTL_STATUS), data);
+		v = RREG32(off) & ~PWR_MISC_CNTL_STATUS__PWR_GFX_RLC_CGPG_EN_MASK;
+		WREG32_IF_CHANGED(off, v);
 	}
 }
 
@@ -2926,167 +2826,141 @@ static void gfx_v9_0_init_gfx_power_gati
 	uint32_t data = 0;
 
 	if (adev->pg_flags & (AMD_PG_SUPPORT_GFX_PG |
-			      AMD_PG_SUPPORT_GFX_SMG |
-			      AMD_PG_SUPPORT_GFX_DMG)) {
-		/* init IDLE_POLL_COUNT = 60 */
+		AMD_PG_SUPPORT_GFX_SMG |
+		AMD_PG_SUPPORT_GFX_DMG)) {
 		data = RREG32(SOC15_REG_OFFSET(GC, 0, mmCP_RB_WPTR_POLL_CNTL));
-		data &= ~CP_RB_WPTR_POLL_CNTL__IDLE_POLL_COUNT_MASK;
-		data |= (0x60 << CP_RB_WPTR_POLL_CNTL__IDLE_POLL_COUNT__SHIFT);
-		WREG32(SOC15_REG_OFFSET(GC, 0, mmCP_RB_WPTR_POLL_CNTL), data);
-
-		/* init RLC PG Delay */
-		data = 0;
-		data |= (0x10 << RLC_PG_DELAY__POWER_UP_DELAY__SHIFT);
-		data |= (0x10 << RLC_PG_DELAY__POWER_DOWN_DELAY__SHIFT);
-		data |= (0x10 << RLC_PG_DELAY__CMD_PROPAGATE_DELAY__SHIFT);
-		data |= (0x40 << RLC_PG_DELAY__MEM_SLEEP_DELAY__SHIFT);
-		WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_DELAY), data);
-
-		data = RREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_DELAY_2));
-		data &= ~RLC_PG_DELAY_2__SERDES_CMD_DELAY_MASK;
-		data |= (0x4 << RLC_PG_DELAY_2__SERDES_CMD_DELAY__SHIFT);
-		WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_DELAY_2), data);
-
-		data = RREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_DELAY_3));
-		data &= ~RLC_PG_DELAY_3__CGCG_ACTIVE_BEFORE_CGPG_MASK;
-		data |= (0xff << RLC_PG_DELAY_3__CGCG_ACTIVE_BEFORE_CGPG__SHIFT);
-		WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_DELAY_3), data);
-
-		data = RREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_AUTO_PG_CTRL));
-		data &= ~RLC_AUTO_PG_CTRL__GRBM_REG_SAVE_GFX_IDLE_THRESHOLD_MASK;
-
-		/* program GRBM_REG_SAVE_GFX_IDLE_THRESHOLD to 0x55f0 */
-		data |= (0x55f0 << RLC_AUTO_PG_CTRL__GRBM_REG_SAVE_GFX_IDLE_THRESHOLD__SHIFT);
-		WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_AUTO_PG_CTRL), data);
-		if (amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 3, 0))
-			pwr_10_0_gfxip_control_over_cgpg(adev, true);
-	}
+	data &= ~CP_RB_WPTR_POLL_CNTL__IDLE_POLL_COUNT_MASK;
+	data |= (0x60 << CP_RB_WPTR_POLL_CNTL__IDLE_POLL_COUNT__SHIFT);
+	WREG32(SOC15_REG_OFFSET(GC, 0, mmCP_RB_WPTR_POLL_CNTL), data);
+
+	data = 0;
+	data |= (0x10 << RLC_PG_DELAY__POWER_UP_DELAY__SHIFT);
+	data |= (0x10 << RLC_PG_DELAY__POWER_DOWN_DELAY__SHIFT);
+	data |= (0x10 << RLC_PG_DELAY__CMD_PROPAGATE_DELAY__SHIFT);
+	data |= (0x40 << RLC_PG_DELAY__MEM_SLEEP_DELAY__SHIFT);
+	WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_DELAY), data);
+
+	data = RREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_DELAY_2));
+	data &= ~RLC_PG_DELAY_2__SERDES_CMD_DELAY_MASK;
+	data |= (0x4 << RLC_PG_DELAY_2__SERDES_CMD_DELAY__SHIFT);
+	WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_DELAY_2), data);
+
+	data = RREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_DELAY_3));
+	data &= ~RLC_PG_DELAY_3__CGCG_ACTIVE_BEFORE_CGPG_MASK;
+	data |= (0xff << RLC_PG_DELAY_3__CGCG_ACTIVE_BEFORE_CGPG__SHIFT);
+	WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_DELAY_3), data);
+
+	data = RREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_AUTO_PG_CTRL));
+	data &= ~RLC_AUTO_PG_CTRL__GRBM_REG_SAVE_GFX_IDLE_THRESHOLD_MASK;
+
+	data |= (0x55f0 << RLC_AUTO_PG_CTRL__GRBM_REG_SAVE_GFX_IDLE_THRESHOLD__SHIFT);
+	WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_AUTO_PG_CTRL), data);
+	if (amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 3, 0))
+		pwr_10_0_gfxip_control_over_cgpg(adev, true);
+		}
 }
 
 static void gfx_v9_0_enable_sck_slow_down_on_power_up(struct amdgpu_device *adev,
-						bool enable)
+		  bool enable)
 {
-	uint32_t data = 0;
-	uint32_t default_data = 0;
+	const u32 off = SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL);
+	u32 v = RREG32(off);
 
-	default_data = data = RREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL));
-	data = REG_SET_FIELD(data, RLC_PG_CNTL,
-			     SMU_CLK_SLOWDOWN_ON_PU_ENABLE,
-			     enable ? 1 : 0);
-	if (default_data != data)
-		WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL), data);
+	v = REG_SET_FIELD(v, RLC_PG_CNTL,
+					  SMU_CLK_SLOWDOWN_ON_PU_ENABLE, enable ? 1 : 0);
+	WREG32_IF_CHANGED(off, v);
 }
 
 static void gfx_v9_0_enable_sck_slow_down_on_power_down(struct amdgpu_device *adev,
-						bool enable)
+														bool enable)
 {
-	uint32_t data = 0;
-	uint32_t default_data = 0;
+	const u32 off = SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL);
+	u32 v = RREG32(off);
 
-	default_data = data = RREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL));
-	data = REG_SET_FIELD(data, RLC_PG_CNTL,
-			     SMU_CLK_SLOWDOWN_ON_PD_ENABLE,
-			     enable ? 1 : 0);
-	if(default_data != data)
-		WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL), data);
+	v = REG_SET_FIELD(v, RLC_PG_CNTL,
+					  SMU_CLK_SLOWDOWN_ON_PD_ENABLE, enable ? 1 : 0);
+	WREG32_IF_CHANGED(off, v);
 }
 
 static void gfx_v9_0_enable_cp_power_gating(struct amdgpu_device *adev,
-					bool enable)
+											bool enable)
 {
-	uint32_t data = 0;
-	uint32_t default_data = 0;
+	const u32 off = SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL);
+	u32 v = RREG32(off);
 
-	default_data = data = RREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL));
-	data = REG_SET_FIELD(data, RLC_PG_CNTL,
-			     CP_PG_DISABLE,
-			     enable ? 0 : 1);
-	if(default_data != data)
-		WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL), data);
+	v = REG_SET_FIELD(v, RLC_PG_CNTL, CP_PG_DISABLE, enable ? 0 : 1);
+	WREG32_IF_CHANGED(off, v);
 }
 
 static void gfx_v9_0_enable_gfx_cg_power_gating(struct amdgpu_device *adev,
-						bool enable)
+												bool enable)
 {
-	uint32_t data, default_data;
+	const u32 off = SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL);
+	u32 v = RREG32(off);
 
-	default_data = data = RREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL));
-	data = REG_SET_FIELD(data, RLC_PG_CNTL,
-			     GFX_POWER_GATING_ENABLE,
-			     enable ? 1 : 0);
-	if(default_data != data)
-		WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL), data);
+	v = REG_SET_FIELD(v, RLC_PG_CNTL,
+					  GFX_POWER_GATING_ENABLE, enable ? 1 : 0);
+	WREG32_IF_CHANGED(off, v);
 }
 
 static void gfx_v9_0_enable_gfx_pipeline_powergating(struct amdgpu_device *adev,
-						bool enable)
+													 bool enable)
 {
-	uint32_t data, default_data;
+	const u32 off = SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL);
+	u32 v = RREG32(off);
 
-	default_data = data = RREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL));
-	data = REG_SET_FIELD(data, RLC_PG_CNTL,
-			     GFX_PIPELINE_PG_ENABLE,
-			     enable ? 1 : 0);
-	if(default_data != data)
-		WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL), data);
+	v = REG_SET_FIELD(v, RLC_PG_CNTL,
+					  GFX_PIPELINE_PG_ENABLE, enable ? 1 : 0);
+	WREG32_IF_CHANGED(off, v);
 
 	if (!enable)
-		/* read any GFX register to wake up GFX */
-		data = RREG32(SOC15_REG_OFFSET(GC, 0, mmDB_RENDER_CONTROL));
+		(void)RREG32(SOC15_REG_OFFSET(GC, 0, mmDB_RENDER_CONTROL));
 }
 
 static void gfx_v9_0_enable_gfx_static_mg_power_gating(struct amdgpu_device *adev,
-						       bool enable)
+		   bool enable)
 {
-	uint32_t data, default_data;
+	const u32 off = SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL);
+	u32 v = RREG32(off);
 
-	default_data = data = RREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL));
-	data = REG_SET_FIELD(data, RLC_PG_CNTL,
-			     STATIC_PER_CU_PG_ENABLE,
-			     enable ? 1 : 0);
-	if(default_data != data)
-		WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL), data);
+	v = REG_SET_FIELD(v, RLC_PG_CNTL,
+					  STATIC_PER_CU_PG_ENABLE, enable ? 1 : 0);
+	WREG32_IF_CHANGED(off, v);
 }
 
 static void gfx_v9_0_enable_gfx_dynamic_mg_power_gating(struct amdgpu_device *adev,
-						bool enable)
+														bool enable)
 {
-	uint32_t data, default_data;
+	const u32 off = SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL);
+	u32 v = RREG32(off);
 
-	default_data = data = RREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL));
-	data = REG_SET_FIELD(data, RLC_PG_CNTL,
-			     DYN_PER_CU_PG_ENABLE,
-			     enable ? 1 : 0);
-	if(default_data != data)
-		WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL), data);
+	v = REG_SET_FIELD(v, RLC_PG_CNTL,
+					  DYN_PER_CU_PG_ENABLE, enable ? 1 : 0);
+	WREG32_IF_CHANGED(off, v);
 }
 
 static void gfx_v9_0_init_pg(struct amdgpu_device *adev)
 {
 	gfx_v9_0_init_csb(adev);
 
-	/*
-	 * Rlc save restore list is workable since v2_1.
-	 * And it's needed by gfxoff feature.
-	 */
 	if (adev->gfx.rlc.is_rlc_v2_1) {
 		if (amdgpu_ip_version(adev, GC_HWIP, 0) ==
-			    IP_VERSION(9, 2, 1) ||
-		    (adev->apu_flags & AMD_APU_IS_RAVEN2))
+			IP_VERSION(9, 2, 1) ||
+			(adev->apu_flags & AMD_APU_IS_RAVEN2))
 			gfx_v9_1_init_rlc_save_restore_list(adev);
 		gfx_v9_0_enable_save_restore_machine(adev);
 	}
 
 	if (adev->pg_flags & (AMD_PG_SUPPORT_GFX_PG |
-			      AMD_PG_SUPPORT_GFX_SMG |
-			      AMD_PG_SUPPORT_GFX_DMG |
-			      AMD_PG_SUPPORT_CP |
-			      AMD_PG_SUPPORT_GDS |
-			      AMD_PG_SUPPORT_RLC_SMU_HS)) {
+		AMD_PG_SUPPORT_GFX_SMG |
+		AMD_PG_SUPPORT_GFX_DMG |
+		AMD_PG_SUPPORT_CP |
+		AMD_PG_SUPPORT_GDS |
+		AMD_PG_SUPPORT_RLC_SMU_HS)) {
 		WREG32_SOC15(GC, 0, mmRLC_JUMP_TABLE_RESTORE,
-			     adev->gfx.rlc.cp_table_gpu_addr >> 8);
+					 adev->gfx.rlc.cp_table_gpu_addr >> 8);
 		gfx_v9_0_init_gfx_power_gating(adev);
-	}
+		}
 }
 
 static void gfx_v9_0_rlc_stop(struct amdgpu_device *adev)
@@ -3106,34 +2980,27 @@ static void gfx_v9_0_rlc_reset(struct am
 
 static void gfx_v9_0_rlc_start(struct amdgpu_device *adev)
 {
-#ifdef AMDGPU_RLC_DEBUG_RETRY
+	#ifdef AMDGPU_RLC_DEBUG_RETRY
 	u32 rlc_ucode_ver;
-#endif
+	#endif
 
 	WREG32_FIELD15(GC, 0, RLC_CNTL, RLC_ENABLE_F32, 1);
 	udelay(50);
 
-	/* carrizo do enable cp interrupt after cp inited */
 	if (!(adev->flags & AMD_IS_APU)) {
 		gfx_v9_0_enable_gui_idle_interrupt(adev, true);
 		udelay(50);
 	}
 
-#ifdef AMDGPU_RLC_DEBUG_RETRY
-	/* RLC_GPM_GENERAL_6 : RLC Ucode version */
+	#ifdef AMDGPU_RLC_DEBUG_RETRY
 	rlc_ucode_ver = RREG32_SOC15(GC, 0, mmRLC_GPM_GENERAL_6);
 	if(rlc_ucode_ver == 0x108) {
 		DRM_INFO("Using rlc debug ucode. mmRLC_GPM_GENERAL_6 ==0x08%x / fw_ver == %i \n",
-				rlc_ucode_ver, adev->gfx.rlc_fw_version);
-		/* RLC_GPM_TIMER_INT_3 : Timer interval in RefCLK cycles,
-		 * default is 0x9C4 to create a 100us interval */
+				 rlc_ucode_ver, adev->gfx.rlc_fw_version);
 		WREG32_SOC15(GC, 0, mmRLC_GPM_TIMER_INT_3, 0x9C4);
-		/* RLC_GPM_GENERAL_12 : Minimum gap between wptr and rptr
-		 * to disable the page fault retry interrupts, default is
-		 * 0x100 (256) */
 		WREG32_SOC15(GC, 0, mmRLC_GPM_GENERAL_12, 0x100);
 	}
-#endif
+	#endif
 }
 
 static int gfx_v9_0_rlc_load_microcode(struct amdgpu_device *adev)
@@ -3149,11 +3016,11 @@ static int gfx_v9_0_rlc_load_microcode(s
 	amdgpu_ucode_print_rlc_hdr(&hdr->header);
 
 	fw_data = (const __le32 *)(adev->gfx.rlc_fw->data +
-			   le32_to_cpu(hdr->header.ucode_array_offset_bytes));
+	le32_to_cpu(hdr->header.ucode_array_offset_bytes));
 	fw_size = le32_to_cpu(hdr->header.ucode_size_bytes) / 4;
 
 	WREG32_SOC15(GC, 0, mmRLC_GPM_UCODE_ADDR,
-			RLCG_UCODE_LOADING_START_ADDRESS);
+				 RLCG_UCODE_LOADING_START_ADDRESS);
 	for (i = 0; i < fw_size; i++)
 		WREG32_SOC15(GC, 0, mmRLC_GPM_UCODE_DATA, le32_to_cpup(fw_data++));
 	WREG32_SOC15(GC, 0, mmRLC_GPM_UCODE_ADDR, adev->gfx.rlc_fw_version);
@@ -3172,36 +3039,34 @@ static int gfx_v9_0_rlc_resume(struct am
 
 	adev->gfx.rlc.funcs->stop(adev);
 
-	/* disable CG */
 	WREG32_SOC15(GC, 0, mmRLC_CGCG_CGLS_CTRL, 0);
 
 	gfx_v9_0_init_pg(adev);
 
 	if (adev->firmware.load_type != AMDGPU_FW_LOAD_PSP) {
-		/* legacy rlc firmware loading */
 		r = gfx_v9_0_rlc_load_microcode(adev);
 		if (r)
 			return r;
 	}
 
 	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
-	case IP_VERSION(9, 2, 2):
-	case IP_VERSION(9, 1, 0):
-		gfx_v9_0_init_lbpw(adev);
-		if (amdgpu_lbpw == 0)
-			gfx_v9_0_enable_lbpw(adev, false);
+		case IP_VERSION(9, 2, 2):
+		case IP_VERSION(9, 1, 0):
+			gfx_v9_0_init_lbpw(adev);
+			if (amdgpu_lbpw == 0)
+				gfx_v9_0_enable_lbpw(adev, false);
 		else
 			gfx_v9_0_enable_lbpw(adev, true);
 		break;
-	case IP_VERSION(9, 4, 0):
-		gfx_v9_4_init_lbpw(adev);
-		if (amdgpu_lbpw > 0)
-			gfx_v9_0_enable_lbpw(adev, true);
+		case IP_VERSION(9, 4, 0):
+			gfx_v9_4_init_lbpw(adev);
+			if (amdgpu_lbpw > 0)
+				gfx_v9_0_enable_lbpw(adev, true);
 		else
 			gfx_v9_0_enable_lbpw(adev, false);
 		break;
-	default:
-		break;
+		default:
+			break;
 	}
 
 	gfx_v9_0_update_spm_vmid_internal(adev, 0xf);
@@ -3243,11 +3108,11 @@ static int gfx_v9_0_cp_gfx_load_microcod
 		return -EINVAL;
 
 	pfp_hdr = (const struct gfx_firmware_header_v1_0 *)
-		adev->gfx.pfp_fw->data;
+	adev->gfx.pfp_fw->data;
 	ce_hdr = (const struct gfx_firmware_header_v1_0 *)
-		adev->gfx.ce_fw->data;
+	adev->gfx.ce_fw->data;
 	me_hdr = (const struct gfx_firmware_header_v1_0 *)
-		adev->gfx.me_fw->data;
+	adev->gfx.me_fw->data;
 
 	amdgpu_ucode_print_gfx_hdr(&pfp_hdr->header);
 	amdgpu_ucode_print_gfx_hdr(&ce_hdr->header);
@@ -3255,30 +3120,27 @@ static int gfx_v9_0_cp_gfx_load_microcod
 
 	gfx_v9_0_cp_gfx_enable(adev, false);
 
-	/* PFP */
 	fw_data = (const __le32 *)
-		(adev->gfx.pfp_fw->data +
-		 le32_to_cpu(pfp_hdr->header.ucode_array_offset_bytes));
+	(adev->gfx.pfp_fw->data +
+	le32_to_cpu(pfp_hdr->header.ucode_array_offset_bytes));
 	fw_size = le32_to_cpu(pfp_hdr->header.ucode_size_bytes) / 4;
 	WREG32_SOC15(GC, 0, mmCP_PFP_UCODE_ADDR, 0);
 	for (i = 0; i < fw_size; i++)
 		WREG32_SOC15(GC, 0, mmCP_PFP_UCODE_DATA, le32_to_cpup(fw_data++));
 	WREG32_SOC15(GC, 0, mmCP_PFP_UCODE_ADDR, adev->gfx.pfp_fw_version);
 
-	/* CE */
 	fw_data = (const __le32 *)
-		(adev->gfx.ce_fw->data +
-		 le32_to_cpu(ce_hdr->header.ucode_array_offset_bytes));
+	(adev->gfx.ce_fw->data +
+	le32_to_cpu(ce_hdr->header.ucode_array_offset_bytes));
 	fw_size = le32_to_cpu(ce_hdr->header.ucode_size_bytes) / 4;
 	WREG32_SOC15(GC, 0, mmCP_CE_UCODE_ADDR, 0);
 	for (i = 0; i < fw_size; i++)
 		WREG32_SOC15(GC, 0, mmCP_CE_UCODE_DATA, le32_to_cpup(fw_data++));
 	WREG32_SOC15(GC, 0, mmCP_CE_UCODE_ADDR, adev->gfx.ce_fw_version);
 
-	/* ME */
 	fw_data = (const __le32 *)
-		(adev->gfx.me_fw->data +
-		 le32_to_cpu(me_hdr->header.ucode_array_offset_bytes));
+	(adev->gfx.me_fw->data +
+	le32_to_cpu(me_hdr->header.ucode_array_offset_bytes));
 	fw_size = le32_to_cpu(me_hdr->header.ucode_size_bytes) / 4;
 	WREG32_SOC15(GC, 0, mmCP_ME_RAM_WADDR, 0);
 	for (i = 0; i < fw_size; i++)
@@ -3295,67 +3157,63 @@ static int gfx_v9_0_cp_gfx_start(struct
 	const struct cs_extent_def *ext = NULL;
 	int r, i, tmp;
 
-	/* init the CP */
 	WREG32_SOC15(GC, 0, mmCP_MAX_CONTEXT, adev->gfx.config.max_hw_contexts - 1);
 	WREG32_SOC15(GC, 0, mmCP_DEVICE_ID, 1);
 
 	gfx_v9_0_cp_gfx_enable(adev, true);
 
-	/* Now only limit the quirk on the APU gfx9 series and already
-	 * confirmed that the APU gfx10/gfx11 needn't such update.
-	 */
 	if (adev->flags & AMD_IS_APU &&
-			adev->in_s3 && !pm_resume_via_firmware()) {
+		adev->in_s3 && !pm_resume_via_firmware()) {
 		DRM_INFO("Will skip the CSB packet resubmit\n");
-		return 0;
-	}
-	r = amdgpu_ring_alloc(ring, gfx_v9_0_get_csb_size(adev) + 4 + 3);
-	if (r) {
-		DRM_ERROR("amdgpu: cp failed to lock ring (%d).\n", r);
-		return r;
-	}
-
-	amdgpu_ring_write(ring, PACKET3(PACKET3_PREAMBLE_CNTL, 0));
-	amdgpu_ring_write(ring, PACKET3_PREAMBLE_BEGIN_CLEAR_STATE);
+	return 0;
+		}
+		r = amdgpu_ring_alloc(ring, gfx_v9_0_get_csb_size(adev) + 4 + 3);
+		if (r) {
+			DRM_ERROR("amdgpu: cp failed to lock ring (%d).\n", r);
+			return r;
+		}
 
-	amdgpu_ring_write(ring, PACKET3(PACKET3_CONTEXT_CONTROL, 1));
-	amdgpu_ring_write(ring, 0x80000000);
-	amdgpu_ring_write(ring, 0x80000000);
+		amdgpu_ring_write(ring, PACKET3(PACKET3_PREAMBLE_CNTL, 0));
+		amdgpu_ring_write(ring, PACKET3_PREAMBLE_BEGIN_CLEAR_STATE);
 
-	for (sect = gfx9_cs_data; sect->section != NULL; ++sect) {
-		for (ext = sect->section; ext->extent != NULL; ++ext) {
-			if (sect->id == SECT_CONTEXT) {
-				amdgpu_ring_write(ring,
-				       PACKET3(PACKET3_SET_CONTEXT_REG,
-					       ext->reg_count));
-				amdgpu_ring_write(ring,
-				       ext->reg_index - PACKET3_SET_CONTEXT_REG_START);
-				for (i = 0; i < ext->reg_count; i++)
-					amdgpu_ring_write(ring, ext->extent[i]);
+		amdgpu_ring_write(ring, PACKET3(PACKET3_CONTEXT_CONTROL, 1));
+		amdgpu_ring_write(ring, 0x80000000);
+		amdgpu_ring_write(ring, 0x80000000);
+
+		for (sect = gfx9_cs_data; sect->section != NULL; ++sect) {
+			for (ext = sect->section; ext->extent != NULL; ++ext) {
+				if (sect->id == SECT_CONTEXT) {
+					amdgpu_ring_write(ring,
+									  PACKET3(PACKET3_SET_CONTEXT_REG,
+  ext->reg_count));
+					amdgpu_ring_write(ring,
+									  ext->reg_index - PACKET3_SET_CONTEXT_REG_START);
+					for (i = 0; i < ext->reg_count; i++)
+						amdgpu_ring_write(ring, ext->extent[i]);
+				}
 			}
 		}
-	}
 
-	amdgpu_ring_write(ring, PACKET3(PACKET3_PREAMBLE_CNTL, 0));
-	amdgpu_ring_write(ring, PACKET3_PREAMBLE_END_CLEAR_STATE);
+		amdgpu_ring_write(ring, PACKET3(PACKET3_PREAMBLE_CNTL, 0));
+		amdgpu_ring_write(ring, PACKET3_PREAMBLE_END_CLEAR_STATE);
 
-	amdgpu_ring_write(ring, PACKET3(PACKET3_CLEAR_STATE, 0));
-	amdgpu_ring_write(ring, 0);
+		amdgpu_ring_write(ring, PACKET3(PACKET3_CLEAR_STATE, 0));
+		amdgpu_ring_write(ring, 0);
 
-	amdgpu_ring_write(ring, PACKET3(PACKET3_SET_BASE, 2));
-	amdgpu_ring_write(ring, PACKET3_BASE_INDEX(CE_PARTITION_BASE));
-	amdgpu_ring_write(ring, 0x8000);
-	amdgpu_ring_write(ring, 0x8000);
+		amdgpu_ring_write(ring, PACKET3(PACKET3_SET_BASE, 2));
+		amdgpu_ring_write(ring, PACKET3_BASE_INDEX(CE_PARTITION_BASE));
+		amdgpu_ring_write(ring, 0x8000);
+		amdgpu_ring_write(ring, 0x8000);
 
-	amdgpu_ring_write(ring, PACKET3(PACKET3_SET_UCONFIG_REG,1));
-	tmp = (PACKET3_SET_UCONFIG_REG_INDEX_TYPE |
+		amdgpu_ring_write(ring, PACKET3(PACKET3_SET_UCONFIG_REG,1));
+		tmp = (PACKET3_SET_UCONFIG_REG_INDEX_TYPE |
 		(SOC15_REG_OFFSET(GC, 0, mmVGT_INDEX_TYPE) - PACKET3_SET_UCONFIG_REG_START));
-	amdgpu_ring_write(ring, tmp);
-	amdgpu_ring_write(ring, 0);
+		amdgpu_ring_write(ring, tmp);
+		amdgpu_ring_write(ring, 0);
 
-	amdgpu_ring_commit(ring);
+		amdgpu_ring_commit(ring);
 
-	return 0;
+		return 0;
 }
 
 static int gfx_v9_0_cp_gfx_resume(struct amdgpu_device *adev)
@@ -3365,28 +3223,23 @@ static int gfx_v9_0_cp_gfx_resume(struct
 	u32 rb_bufsz;
 	u64 rb_addr, rptr_addr, wptr_gpu_addr;
 
-	/* Set the write pointer delay */
 	WREG32_SOC15(GC, 0, mmCP_RB_WPTR_DELAY, 0);
 
-	/* set the RB to use vmid 0 */
 	WREG32_SOC15(GC, 0, mmCP_RB_VMID, 0);
 
-	/* Set ring buffer size */
 	ring = &adev->gfx.gfx_ring[0];
 	rb_bufsz = order_base_2(ring->ring_size / 8);
 	tmp = REG_SET_FIELD(0, CP_RB0_CNTL, RB_BUFSZ, rb_bufsz);
 	tmp = REG_SET_FIELD(tmp, CP_RB0_CNTL, RB_BLKSZ, rb_bufsz - 2);
-#ifdef __BIG_ENDIAN
+	#ifdef __BIG_ENDIAN
 	tmp = REG_SET_FIELD(tmp, CP_RB0_CNTL, BUF_SWAP, 1);
-#endif
+	#endif
 	WREG32_SOC15(GC, 0, mmCP_RB0_CNTL, tmp);
 
-	/* Initialize the ring buffer's write pointers */
 	ring->wptr = 0;
 	WREG32_SOC15(GC, 0, mmCP_RB0_WPTR, lower_32_bits(ring->wptr));
 	WREG32_SOC15(GC, 0, mmCP_RB0_WPTR_HI, upper_32_bits(ring->wptr));
 
-	/* set the wb address whether it's enabled or not */
 	rptr_addr = ring->rptr_gpu_addr;
 	WREG32_SOC15(GC, 0, mmCP_RB0_RPTR_ADDR, lower_32_bits(rptr_addr));
 	WREG32_SOC15(GC, 0, mmCP_RB0_RPTR_ADDR_HI, upper_32_bits(rptr_addr) & CP_RB_RPTR_ADDR_HI__RB_RPTR_ADDR_HI_MASK);
@@ -3405,23 +3258,22 @@ static int gfx_v9_0_cp_gfx_resume(struct
 	tmp = RREG32_SOC15(GC, 0, mmCP_RB_DOORBELL_CONTROL);
 	if (ring->use_doorbell) {
 		tmp = REG_SET_FIELD(tmp, CP_RB_DOORBELL_CONTROL,
-				    DOORBELL_OFFSET, ring->doorbell_index);
+							DOORBELL_OFFSET, ring->doorbell_index);
 		tmp = REG_SET_FIELD(tmp, CP_RB_DOORBELL_CONTROL,
-				    DOORBELL_EN, 1);
+							DOORBELL_EN, 1);
 	} else {
 		tmp = REG_SET_FIELD(tmp, CP_RB_DOORBELL_CONTROL, DOORBELL_EN, 0);
 	}
 	WREG32_SOC15(GC, 0, mmCP_RB_DOORBELL_CONTROL, tmp);
 
 	tmp = REG_SET_FIELD(0, CP_RB_DOORBELL_RANGE_LOWER,
-			DOORBELL_RANGE_LOWER, ring->doorbell_index);
+						DOORBELL_RANGE_LOWER, ring->doorbell_index);
 	WREG32_SOC15(GC, 0, mmCP_RB_DOORBELL_RANGE_LOWER, tmp);
 
 	WREG32_SOC15(GC, 0, mmCP_RB_DOORBELL_RANGE_UPPER,
-		       CP_RB_DOORBELL_RANGE_UPPER__DOORBELL_RANGE_UPPER_MASK);
+				 CP_RB_DOORBELL_RANGE_UPPER__DOORBELL_RANGE_UPPER_MASK);
 
 
-	/* start the ring */
 	gfx_v9_0_cp_gfx_start(adev);
 
 	return 0;
@@ -3433,15 +3285,15 @@ static void gfx_v9_0_cp_compute_enable(s
 		WREG32_SOC15_RLC(GC, 0, mmCP_MEC_CNTL, 0);
 	} else {
 		WREG32_SOC15_RLC(GC, 0, mmCP_MEC_CNTL,
-				 (CP_MEC_CNTL__MEC_INVALIDATE_ICACHE_MASK |
-				  CP_MEC_CNTL__MEC_ME1_PIPE0_RESET_MASK |
-				  CP_MEC_CNTL__MEC_ME1_PIPE1_RESET_MASK |
-				  CP_MEC_CNTL__MEC_ME1_PIPE2_RESET_MASK |
-				  CP_MEC_CNTL__MEC_ME1_PIPE3_RESET_MASK |
-				  CP_MEC_CNTL__MEC_ME2_PIPE0_RESET_MASK |
-				  CP_MEC_CNTL__MEC_ME2_PIPE1_RESET_MASK |
-				  CP_MEC_CNTL__MEC_ME1_HALT_MASK |
-				  CP_MEC_CNTL__MEC_ME2_HALT_MASK));
+						 (CP_MEC_CNTL__MEC_INVALIDATE_ICACHE_MASK |
+						 CP_MEC_CNTL__MEC_ME1_PIPE0_RESET_MASK |
+						 CP_MEC_CNTL__MEC_ME1_PIPE1_RESET_MASK |
+						 CP_MEC_CNTL__MEC_ME1_PIPE2_RESET_MASK |
+						 CP_MEC_CNTL__MEC_ME1_PIPE3_RESET_MASK |
+						 CP_MEC_CNTL__MEC_ME2_PIPE0_RESET_MASK |
+						 CP_MEC_CNTL__MEC_ME2_PIPE1_RESET_MASK |
+						 CP_MEC_CNTL__MEC_ME1_HALT_MASK |
+						 CP_MEC_CNTL__MEC_ME2_HALT_MASK));
 		adev->gfx.kiq[0].ring.sched.ready = false;
 	}
 	udelay(50);
@@ -3463,39 +3315,35 @@ static int gfx_v9_0_cp_compute_load_micr
 	amdgpu_ucode_print_gfx_hdr(&mec_hdr->header);
 
 	fw_data = (const __le32 *)
-		(adev->gfx.mec_fw->data +
-		 le32_to_cpu(mec_hdr->header.ucode_array_offset_bytes));
+	(adev->gfx.mec_fw->data +
+	le32_to_cpu(mec_hdr->header.ucode_array_offset_bytes));
 	tmp = 0;
 	tmp = REG_SET_FIELD(tmp, CP_CPC_IC_BASE_CNTL, VMID, 0);
 	tmp = REG_SET_FIELD(tmp, CP_CPC_IC_BASE_CNTL, CACHE_POLICY, 0);
 	WREG32_SOC15(GC, 0, mmCP_CPC_IC_BASE_CNTL, tmp);
 
 	WREG32_SOC15(GC, 0, mmCP_CPC_IC_BASE_LO,
-		adev->gfx.mec.mec_fw_gpu_addr & 0xFFFFF000);
+				 adev->gfx.mec.mec_fw_gpu_addr & 0xFFFFF000);
 	WREG32_SOC15(GC, 0, mmCP_CPC_IC_BASE_HI,
-		upper_32_bits(adev->gfx.mec.mec_fw_gpu_addr));
+				 upper_32_bits(adev->gfx.mec.mec_fw_gpu_addr));
 
-	/* MEC1 */
 	WREG32_SOC15(GC, 0, mmCP_MEC_ME1_UCODE_ADDR,
-			 mec_hdr->jt_offset);
+				 mec_hdr->jt_offset);
 	for (i = 0; i < mec_hdr->jt_size; i++)
 		WREG32_SOC15(GC, 0, mmCP_MEC_ME1_UCODE_DATA,
-			le32_to_cpup(fw_data + mec_hdr->jt_offset + i));
+					 le32_to_cpup(fw_data + mec_hdr->jt_offset + i));
 
-	WREG32_SOC15(GC, 0, mmCP_MEC_ME1_UCODE_ADDR,
-			adev->gfx.mec_fw_version);
-	/* Todo : Loading MEC2 firmware is only necessary if MEC2 should run different microcode than MEC1. */
+		WREG32_SOC15(GC, 0, mmCP_MEC_ME1_UCODE_ADDR,
+					 adev->gfx.mec_fw_version);
 
-	return 0;
+		return 0;
 }
 
-/* KIQ functions */
 static void gfx_v9_0_kiq_setting(struct amdgpu_ring *ring)
 {
 	uint32_t tmp;
 	struct amdgpu_device *adev = ring->adev;
 
-	/* tell RLC which is KIQ queue */
 	tmp = RREG32_SOC15(GC, 0, mmRLC_CP_SCHEDULERS);
 	tmp &= 0xffffff00;
 	tmp |= (ring->me << 5) | (ring->pipe << 3) | (ring->queue);
@@ -3510,7 +3358,7 @@ static void gfx_v9_0_mqd_set_priority(st
 		if (amdgpu_gfx_is_high_priority_compute_queue(adev, ring)) {
 			mqd->cp_hqd_pipe_priority = AMDGPU_GFX_PIPE_PRIO_HIGH;
 			mqd->cp_hqd_queue_priority =
-				AMDGPU_GFX_QUEUE_PRIORITY_MAXIMUM;
+			AMDGPU_GFX_QUEUE_PRIORITY_MAXIMUM;
 		}
 	}
 }
@@ -3535,112 +3383,97 @@ static int gfx_v9_0_mqd_init(struct amdg
 	mqd->compute_misc_reserved = 0x00000003;
 
 	mqd->dynamic_cu_mask_addr_lo =
-		lower_32_bits(ring->mqd_gpu_addr
-			      + offsetof(struct v9_mqd_allocation, dynamic_cu_mask));
+	lower_32_bits(ring->mqd_gpu_addr
+	+ offsetof(struct v9_mqd_allocation, dynamic_cu_mask));
 	mqd->dynamic_cu_mask_addr_hi =
-		upper_32_bits(ring->mqd_gpu_addr
-			      + offsetof(struct v9_mqd_allocation, dynamic_cu_mask));
+	upper_32_bits(ring->mqd_gpu_addr
+	+ offsetof(struct v9_mqd_allocation, dynamic_cu_mask));
 
 	eop_base_addr = ring->eop_gpu_addr >> 8;
 	mqd->cp_hqd_eop_base_addr_lo = eop_base_addr;
 	mqd->cp_hqd_eop_base_addr_hi = upper_32_bits(eop_base_addr);
 
-	/* set the EOP size, register value is 2^(EOP_SIZE+1) dwords */
 	tmp = RREG32_SOC15(GC, 0, mmCP_HQD_EOP_CONTROL);
 	tmp = REG_SET_FIELD(tmp, CP_HQD_EOP_CONTROL, EOP_SIZE,
-			(order_base_2(GFX9_MEC_HPD_SIZE / 4) - 1));
+						(order_base_2(GFX9_MEC_HPD_SIZE / 4) - 1));
 
 	mqd->cp_hqd_eop_control = tmp;
 
-	/* enable doorbell? */
 	tmp = RREG32_SOC15(GC, 0, mmCP_HQD_PQ_DOORBELL_CONTROL);
 
 	if (ring->use_doorbell) {
 		tmp = REG_SET_FIELD(tmp, CP_HQD_PQ_DOORBELL_CONTROL,
-				    DOORBELL_OFFSET, ring->doorbell_index);
+							DOORBELL_OFFSET, ring->doorbell_index);
 		tmp = REG_SET_FIELD(tmp, CP_HQD_PQ_DOORBELL_CONTROL,
-				    DOORBELL_EN, 1);
+							DOORBELL_EN, 1);
 		tmp = REG_SET_FIELD(tmp, CP_HQD_PQ_DOORBELL_CONTROL,
-				    DOORBELL_SOURCE, 0);
+							DOORBELL_SOURCE, 0);
 		tmp = REG_SET_FIELD(tmp, CP_HQD_PQ_DOORBELL_CONTROL,
-				    DOORBELL_HIT, 0);
+							DOORBELL_HIT, 0);
 	} else {
 		tmp = REG_SET_FIELD(tmp, CP_HQD_PQ_DOORBELL_CONTROL,
-					 DOORBELL_EN, 0);
+							DOORBELL_EN, 0);
 	}
 
 	mqd->cp_hqd_pq_doorbell_control = tmp;
 
-	/* disable the queue if it's active */
 	ring->wptr = 0;
 	mqd->cp_hqd_dequeue_request = 0;
 	mqd->cp_hqd_pq_rptr = 0;
 	mqd->cp_hqd_pq_wptr_lo = 0;
 	mqd->cp_hqd_pq_wptr_hi = 0;
 
-	/* set the pointer to the MQD */
 	mqd->cp_mqd_base_addr_lo = ring->mqd_gpu_addr & 0xfffffffc;
 	mqd->cp_mqd_base_addr_hi = upper_32_bits(ring->mqd_gpu_addr);
 
-	/* set MQD vmid to 0 */
 	tmp = RREG32_SOC15(GC, 0, mmCP_MQD_CONTROL);
 	tmp = REG_SET_FIELD(tmp, CP_MQD_CONTROL, VMID, 0);
 	mqd->cp_mqd_control = tmp;
 
-	/* set the pointer to the HQD, this is similar CP_RB0_BASE/_HI */
 	hqd_gpu_addr = ring->gpu_addr >> 8;
 	mqd->cp_hqd_pq_base_lo = hqd_gpu_addr;
 	mqd->cp_hqd_pq_base_hi = upper_32_bits(hqd_gpu_addr);
 
-	/* set up the HQD, this is similar to CP_RB0_CNTL */
 	tmp = RREG32_SOC15(GC, 0, mmCP_HQD_PQ_CONTROL);
 	tmp = REG_SET_FIELD(tmp, CP_HQD_PQ_CONTROL, QUEUE_SIZE,
-			    (order_base_2(ring->ring_size / 4) - 1));
+						(order_base_2(ring->ring_size / 4) - 1));
 	tmp = REG_SET_FIELD(tmp, CP_HQD_PQ_CONTROL, RPTR_BLOCK_SIZE,
-			(order_base_2(AMDGPU_GPU_PAGE_SIZE / 4) - 1));
-#ifdef __BIG_ENDIAN
+						(order_base_2(AMDGPU_GPU_PAGE_SIZE / 4) - 1));
+	#ifdef __BIG_ENDIAN
 	tmp = REG_SET_FIELD(tmp, CP_HQD_PQ_CONTROL, ENDIAN_SWAP, 1);
-#endif
+	#endif
 	tmp = REG_SET_FIELD(tmp, CP_HQD_PQ_CONTROL, UNORD_DISPATCH, 0);
 	tmp = REG_SET_FIELD(tmp, CP_HQD_PQ_CONTROL, ROQ_PQ_IB_FLIP, 0);
 	tmp = REG_SET_FIELD(tmp, CP_HQD_PQ_CONTROL, PRIV_STATE, 1);
 	tmp = REG_SET_FIELD(tmp, CP_HQD_PQ_CONTROL, KMD_QUEUE, 1);
 	mqd->cp_hqd_pq_control = tmp;
 
-	/* set the wb address whether it's enabled or not */
 	wb_gpu_addr = ring->rptr_gpu_addr;
 	mqd->cp_hqd_pq_rptr_report_addr_lo = wb_gpu_addr & 0xfffffffc;
 	mqd->cp_hqd_pq_rptr_report_addr_hi =
-		upper_32_bits(wb_gpu_addr) & 0xffff;
+	upper_32_bits(wb_gpu_addr) & 0xffff;
 
-	/* only used if CP_PQ_WPTR_POLL_CNTL.CP_PQ_WPTR_POLL_CNTL__EN_MASK=1 */
 	wb_gpu_addr = ring->wptr_gpu_addr;
 	mqd->cp_hqd_pq_wptr_poll_addr_lo = wb_gpu_addr & 0xfffffffc;
 	mqd->cp_hqd_pq_wptr_poll_addr_hi = upper_32_bits(wb_gpu_addr) & 0xffff;
 
-	/* reset read and write pointers, similar to CP_RB0_WPTR/_RPTR */
 	ring->wptr = 0;
 	mqd->cp_hqd_pq_rptr = RREG32_SOC15(GC, 0, mmCP_HQD_PQ_RPTR);
 
-	/* set the vmid for the queue */
 	mqd->cp_hqd_vmid = 0;
 
 	tmp = RREG32_SOC15(GC, 0, mmCP_HQD_PERSISTENT_STATE);
 	tmp = REG_SET_FIELD(tmp, CP_HQD_PERSISTENT_STATE, PRELOAD_SIZE, 0x53);
 	mqd->cp_hqd_persistent_state = tmp;
 
-	/* set MIN_IB_AVAIL_SIZE */
 	tmp = RREG32_SOC15(GC, 0, mmCP_HQD_IB_CONTROL);
 	tmp = REG_SET_FIELD(tmp, CP_HQD_IB_CONTROL, MIN_IB_AVAIL_SIZE, 3);
 	mqd->cp_hqd_ib_control = tmp;
 
-	/* set static priority for a queue/ring */
 	gfx_v9_0_mqd_set_priority(ring, mqd);
+
 	mqd->cp_hqd_quantum = RREG32_SOC15(GC, 0, mmCP_HQD_QUANTUM);
 
-	/* map_queues packet doesn't need activate the queue,
-	 * so only kiq need set this field.
-	 */
 	if (ring->funcs->type == AMDGPU_RING_TYPE_KIQ)
 		mqd->cp_hqd_active = 1;
 
@@ -3653,23 +3486,19 @@ static int gfx_v9_0_kiq_init_register(st
 	struct v9_mqd *mqd = ring->mqd_ptr;
 	int j;
 
-	/* disable wptr polling */
 	WREG32_FIELD15(GC, 0, CP_PQ_WPTR_POLL_CNTL, EN, 0);
 
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_EOP_BASE_ADDR,
-	       mqd->cp_hqd_eop_base_addr_lo);
+					 mqd->cp_hqd_eop_base_addr_lo);
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_EOP_BASE_ADDR_HI,
-	       mqd->cp_hqd_eop_base_addr_hi);
+					 mqd->cp_hqd_eop_base_addr_hi);
 
-	/* set the EOP size, register value is 2^(EOP_SIZE+1) dwords */
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_EOP_CONTROL,
-	       mqd->cp_hqd_eop_control);
+					 mqd->cp_hqd_eop_control);
 
-	/* enable doorbell? */
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_DOORBELL_CONTROL,
-	       mqd->cp_hqd_pq_doorbell_control);
+					 mqd->cp_hqd_pq_doorbell_control);
 
-	/* disable the queue if it's active */
 	if (RREG32_SOC15(GC, 0, mmCP_HQD_ACTIVE) & 1) {
 		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_DEQUEUE_REQUEST, 1);
 		for (j = 0; j < adev->usec_timeout; j++) {
@@ -3678,82 +3507,67 @@ static int gfx_v9_0_kiq_init_register(st
 			udelay(1);
 		}
 		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_DEQUEUE_REQUEST,
-		       mqd->cp_hqd_dequeue_request);
+						 mqd->cp_hqd_dequeue_request);
 		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_RPTR,
-		       mqd->cp_hqd_pq_rptr);
+						 mqd->cp_hqd_pq_rptr);
 		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_WPTR_LO,
-		       mqd->cp_hqd_pq_wptr_lo);
+						 mqd->cp_hqd_pq_wptr_lo);
 		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_WPTR_HI,
-		       mqd->cp_hqd_pq_wptr_hi);
+						 mqd->cp_hqd_pq_wptr_hi);
 	}
 
-	/* set the pointer to the MQD */
 	WREG32_SOC15_RLC(GC, 0, mmCP_MQD_BASE_ADDR,
-	       mqd->cp_mqd_base_addr_lo);
+					 mqd->cp_mqd_base_addr_lo);
 	WREG32_SOC15_RLC(GC, 0, mmCP_MQD_BASE_ADDR_HI,
-	       mqd->cp_mqd_base_addr_hi);
+					 mqd->cp_mqd_base_addr_hi);
 
-	/* set MQD vmid to 0 */
 	WREG32_SOC15_RLC(GC, 0, mmCP_MQD_CONTROL,
-	       mqd->cp_mqd_control);
+					 mqd->cp_mqd_control);
 
-	/* set the pointer to the HQD, this is similar CP_RB0_BASE/_HI */
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_BASE,
-	       mqd->cp_hqd_pq_base_lo);
+					 mqd->cp_hqd_pq_base_lo);
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_BASE_HI,
-	       mqd->cp_hqd_pq_base_hi);
+					 mqd->cp_hqd_pq_base_hi);
 
-	/* set up the HQD, this is similar to CP_RB0_CNTL */
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_CONTROL,
-	       mqd->cp_hqd_pq_control);
+					 mqd->cp_hqd_pq_control);
 
-	/* set the wb address whether it's enabled or not */
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_RPTR_REPORT_ADDR,
-				mqd->cp_hqd_pq_rptr_report_addr_lo);
+					 mqd->cp_hqd_pq_rptr_report_addr_lo);
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_RPTR_REPORT_ADDR_HI,
-				mqd->cp_hqd_pq_rptr_report_addr_hi);
+					 mqd->cp_hqd_pq_rptr_report_addr_hi);
 
-	/* only used if CP_PQ_WPTR_POLL_CNTL.CP_PQ_WPTR_POLL_CNTL__EN_MASK=1 */
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_WPTR_POLL_ADDR,
-	       mqd->cp_hqd_pq_wptr_poll_addr_lo);
+					 mqd->cp_hqd_pq_wptr_poll_addr_lo);
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_WPTR_POLL_ADDR_HI,
-	       mqd->cp_hqd_pq_wptr_poll_addr_hi);
+					 mqd->cp_hqd_pq_wptr_poll_addr_hi);
 
-	/* enable the doorbell if requested */
 	if (ring->use_doorbell) {
 		WREG32_SOC15(GC, 0, mmCP_MEC_DOORBELL_RANGE_LOWER,
-					(adev->doorbell_index.kiq * 2) << 2);
-		/* If GC has entered CGPG, ringing doorbell > first page
-		 * doesn't wakeup GC. Enlarge CP_MEC_DOORBELL_RANGE_UPPER to
-		 * workaround this issue. And this change has to align with firmware
-		 * update.
-		 */
+					 (adev->doorbell_index.kiq * 2) << 2);
 		if (check_if_enlarge_doorbell_range(adev))
 			WREG32_SOC15(GC, 0, mmCP_MEC_DOORBELL_RANGE_UPPER,
-					(adev->doorbell.size - 4));
-		else
-			WREG32_SOC15(GC, 0, mmCP_MEC_DOORBELL_RANGE_UPPER,
-					(adev->doorbell_index.userqueue_end * 2) << 2);
+						 (adev->doorbell.size - 4));
+			else
+				WREG32_SOC15(GC, 0, mmCP_MEC_DOORBELL_RANGE_UPPER,
+							 (adev->doorbell_index.userqueue_end * 2) << 2);
 	}
 
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_DOORBELL_CONTROL,
-	       mqd->cp_hqd_pq_doorbell_control);
+					 mqd->cp_hqd_pq_doorbell_control);
 
-	/* reset read and write pointers, similar to CP_RB0_WPTR/_RPTR */
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_WPTR_LO,
-	       mqd->cp_hqd_pq_wptr_lo);
+					 mqd->cp_hqd_pq_wptr_lo);
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_WPTR_HI,
-	       mqd->cp_hqd_pq_wptr_hi);
+					 mqd->cp_hqd_pq_wptr_hi);
 
-	/* set the vmid for the queue */
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_VMID, mqd->cp_hqd_vmid);
 
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PERSISTENT_STATE,
-	       mqd->cp_hqd_persistent_state);
+					 mqd->cp_hqd_persistent_state);
 
-	/* activate the queue */
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_ACTIVE,
-	       mqd->cp_hqd_active);
+					 mqd->cp_hqd_active);
 
 	if (ring->use_doorbell)
 		WREG32_FIELD15(GC, 0, CP_PQ_STATUS, DOORBELL_ENABLE, 1);
@@ -3761,43 +3575,37 @@ static int gfx_v9_0_kiq_init_register(st
 	return 0;
 }
 
-static int gfx_v9_0_kiq_fini_register(struct amdgpu_ring *ring)
+static inline int gfx_v9_0_kiq_fini_register(struct amdgpu_ring *ring)
 {
 	struct amdgpu_device *adev = ring->adev;
-	int j;
-
-	/* disable the queue if it's active */
-	if (RREG32_SOC15(GC, 0, mmCP_HQD_ACTIVE) & 1) {
+	const unsigned long  tmo   = adev->usec_timeout / 5 + 1;
+	int r = 0;
 
+	if (RREG32(SOC15_REG_OFFSET(GC, 0, mmCP_HQD_ACTIVE)) &
+		CP_HQD_ACTIVE__ACTIVE_MASK) {
 		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_DEQUEUE_REQUEST, 1);
 
-		for (j = 0; j < adev->usec_timeout; j++) {
-			if (!(RREG32_SOC15(GC, 0, mmCP_HQD_ACTIVE) & 1))
-				break;
-			udelay(1);
+	r = gfx9_wait_reg_off(adev,
+						  SOC15_REG_OFFSET(GC, 0, mmCP_HQD_ACTIVE),
+						  CP_HQD_ACTIVE__ACTIVE_MASK, 0, tmo);
+	if (r) {
+		dev_dbg_ratelimited(adev->dev,
+							"KIQ fini: dequeue timeout, forcing inactive\n");
+		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_ACTIVE, 0);
+	}
+	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_DEQUEUE_REQUEST, 0);
 		}
 
-		if (j == AMDGPU_MAX_USEC_TIMEOUT) {
-			DRM_DEBUG("KIQ dequeue request failed.\n");
-
-			/* Manual disable if dequeue request times out */
-			WREG32_SOC15_RLC(GC, 0, mmCP_HQD_ACTIVE, 0);
-		}
+		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_IQ_TIMER,         0);
+		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_IB_CONTROL,       0);
+		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PERSISTENT_STATE, 0);
+		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_DOORBELL_CONTROL, 0x40000000);
+		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_DOORBELL_CONTROL, 0);
+		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_RPTR,      0);
+		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_WPTR_HI,   0);
+		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_WPTR_LO,   0);
 
-		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_DEQUEUE_REQUEST,
-		      0);
-	}
-
-	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_IQ_TIMER, 0);
-	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_IB_CONTROL, 0);
-	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PERSISTENT_STATE, 0);
-	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_DOORBELL_CONTROL, 0x40000000);
-	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_DOORBELL_CONTROL, 0);
-	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_RPTR, 0);
-	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_WPTR_HI, 0);
-	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_WPTR_LO, 0);
-
-	return 0;
+		return r;
 }
 
 static int gfx_v9_0_kiq_init_queue(struct amdgpu_ring *ring)
@@ -3808,18 +3616,11 @@ static int gfx_v9_0_kiq_init_queue(struc
 
 	gfx_v9_0_kiq_setting(ring);
 
-	/* GPU could be in bad state during probe, driver trigger the reset
-	 * after load the SMU, in this case , the mqd is not be initialized.
-	 * driver need to re-init the mqd.
-	 * check mqd->cp_hqd_pq_control since this value should not be 0
-	 */
 	tmp_mqd = (struct v9_mqd *)adev->gfx.kiq[0].mqd_backup;
 	if (amdgpu_in_reset(adev) && tmp_mqd->cp_hqd_pq_control){
-		/* for GPU_RESET case , reset MQD to a clean status */
 		if (adev->gfx.kiq[0].mqd_backup)
 			memcpy(mqd, adev->gfx.kiq[0].mqd_backup, sizeof(struct v9_mqd_allocation));
 
-		/* reset ring buffer */
 		ring->wptr = 0;
 		amdgpu_ring_clear_ring(ring);
 
@@ -3855,35 +3656,30 @@ static int gfx_v9_0_kcq_init_queue(struc
 	int mqd_idx = ring - &adev->gfx.compute_ring[0];
 	struct v9_mqd *tmp_mqd;
 
-	/* Same as above kiq init, driver need to re-init the mqd if mqd->cp_hqd_pq_control
-	 * is not be initialized before
-	 */
 	tmp_mqd = (struct v9_mqd *)adev->gfx.mec.mqd_backup[mqd_idx];
 
 	if (!restore && (!tmp_mqd->cp_hqd_pq_control ||
-	    (!amdgpu_in_reset(adev) && !adev->in_suspend))) {
+		(!amdgpu_in_reset(adev) && !adev->in_suspend))) {
 		memset((void *)mqd, 0, sizeof(struct v9_mqd_allocation));
-		((struct v9_mqd_allocation *)mqd)->dynamic_cu_mask = 0xFFFFFFFF;
-		((struct v9_mqd_allocation *)mqd)->dynamic_rb_mask = 0xFFFFFFFF;
-		mutex_lock(&adev->srbm_mutex);
-		soc15_grbm_select(adev, ring->me, ring->pipe, ring->queue, 0, 0);
-		gfx_v9_0_mqd_init(ring);
-		soc15_grbm_select(adev, 0, 0, 0, 0, 0);
-		mutex_unlock(&adev->srbm_mutex);
+	((struct v9_mqd_allocation *)mqd)->dynamic_cu_mask = 0xFFFFFFFF;
+	((struct v9_mqd_allocation *)mqd)->dynamic_rb_mask = 0xFFFFFFFF;
+	mutex_lock(&adev->srbm_mutex);
+	soc15_grbm_select(adev, ring->me, ring->pipe, ring->queue, 0, 0);
+	gfx_v9_0_mqd_init(ring);
+	soc15_grbm_select(adev, 0, 0, 0, 0, 0);
+	mutex_unlock(&adev->srbm_mutex);
 
-		if (adev->gfx.mec.mqd_backup[mqd_idx])
-			memcpy(adev->gfx.mec.mqd_backup[mqd_idx], mqd, sizeof(struct v9_mqd_allocation));
-	} else {
-		/* restore MQD to a clean status */
-		if (adev->gfx.mec.mqd_backup[mqd_idx])
-			memcpy(mqd, adev->gfx.mec.mqd_backup[mqd_idx], sizeof(struct v9_mqd_allocation));
-		/* reset ring buffer */
-		ring->wptr = 0;
-		atomic64_set((atomic64_t *)ring->wptr_cpu_addr, 0);
-		amdgpu_ring_clear_ring(ring);
-	}
+	if (adev->gfx.mec.mqd_backup[mqd_idx])
+		memcpy(adev->gfx.mec.mqd_backup[mqd_idx], mqd, sizeof(struct v9_mqd_allocation));
+		} else {
+			if (adev->gfx.mec.mqd_backup[mqd_idx])
+				memcpy(mqd, adev->gfx.mec.mqd_backup[mqd_idx], sizeof(struct v9_mqd_allocation));
+			ring->wptr = 0;
+			atomic64_set((atomic64_t *)ring->wptr_cpu_addr, 0);
+			amdgpu_ring_clear_ring(ring);
+		}
 
-	return 0;
+		return 0;
 }
 
 static int gfx_v9_0_kiq_resume(struct amdgpu_device *adev)
@@ -3917,7 +3713,6 @@ static int gfx_v9_0_cp_resume(struct amd
 
 	if (adev->firmware.load_type != AMDGPU_FW_LOAD_PSP) {
 		if (adev->gfx.num_gfx_rings) {
-			/* legacy firmware loading */
 			r = gfx_v9_0_cp_gfx_load_microcode(adev);
 			if (r)
 				return r;
@@ -3968,16 +3763,16 @@ static void gfx_v9_0_init_tcp_config(str
 	u32 tmp;
 
 	if (amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 4, 1) &&
-	    amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 4, 2))
+		amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 4, 2))
 		return;
 
 	tmp = RREG32_SOC15(GC, 0, mmTCP_ADDR_CONFIG);
 	tmp = REG_SET_FIELD(tmp, TCP_ADDR_CONFIG, ENABLE64KHASH,
-				adev->df.hash_status.hash_64k);
+						adev->df.hash_status.hash_64k);
 	tmp = REG_SET_FIELD(tmp, TCP_ADDR_CONFIG, ENABLE2MHASH,
-				adev->df.hash_status.hash_2m);
+						adev->df.hash_status.hash_2m);
 	tmp = REG_SET_FIELD(tmp, TCP_ADDR_CONFIG, ENABLE1GHASH,
-				adev->df.hash_status.hash_1g);
+						adev->df.hash_status.hash_1g);
 	WREG32_SOC15(GC, 0, mmTCP_ADDR_CONFIG, tmp);
 }
 
@@ -3994,7 +3789,7 @@ static int gfx_v9_0_hw_init(struct amdgp
 	struct amdgpu_device *adev = ip_block->adev;
 
 	amdgpu_gfx_cleaner_shader_init(adev, adev->gfx.cleaner_shader_size,
-				       adev->gfx.cleaner_shader_ptr);
+								   adev->gfx.cleaner_shader_ptr);
 
 	if (!amdgpu_sriov_vf(adev))
 		gfx_v9_0_init_golden_registers(adev);
@@ -4027,30 +3822,20 @@ static int gfx_v9_0_hw_fini(struct amdgp
 	amdgpu_irq_put(adev, &adev->gfx.priv_inst_irq, 0);
 	amdgpu_irq_put(adev, &adev->gfx.bad_op_irq, 0);
 
-	/* DF freeze and kcq disable will fail */
 	if (!amdgpu_ras_intr_triggered())
-		/* disable KCQ to avoid CPC touch memory not valid anymore */
 		amdgpu_gfx_disable_kcq(adev, 0);
 
 	if (amdgpu_sriov_vf(adev)) {
 		gfx_v9_0_cp_gfx_enable(adev, false);
-		/* must disable polling for SRIOV when hw finished, otherwise
-		 * CPC engine may still keep fetching WB address which is already
-		 * invalid after sw finished and trigger DMAR reading error in
-		 * hypervisor side.
-		 */
 		WREG32_FIELD15(GC, 0, CP_PQ_WPTR_POLL_CNTL, EN, 0);
 		return 0;
 	}
 
-	/* Use deinitialize sequence from CAIL when unbinding device from driver,
-	 * otherwise KIQ is hanging when binding back
-	 */
 	if (!amdgpu_in_reset(adev) && !adev->in_suspend) {
 		mutex_lock(&adev->srbm_mutex);
 		soc15_grbm_select(adev, adev->gfx.kiq[0].ring.me,
-				adev->gfx.kiq[0].ring.pipe,
-				adev->gfx.kiq[0].ring.queue, 0, 0);
+						  adev->gfx.kiq[0].ring.pipe,
+					adev->gfx.kiq[0].ring.queue, 0, 0);
 		gfx_v9_0_kiq_fini_register(&adev->gfx.kiq[0].ring);
 		soc15_grbm_select(adev, 0, 0, 0, 0, 0);
 		mutex_unlock(&adev->srbm_mutex);
@@ -4058,15 +3843,14 @@ static int gfx_v9_0_hw_fini(struct amdgp
 
 	gfx_v9_0_cp_enable(adev, false);
 
-	/* Skip stopping RLC with A+A reset or when RLC controls GFX clock */
 	if ((adev->gmc.xgmi.connected_to_cpu && amdgpu_in_reset(adev)) ||
-	    (amdgpu_ip_version(adev, GC_HWIP, 0) >= IP_VERSION(9, 4, 2))) {
+		(amdgpu_ip_version(adev, GC_HWIP, 0) >= IP_VERSION(9, 4, 2))) {
 		dev_dbg(adev->dev, "Skipping RLC halt\n");
-		return 0;
-	}
+		} else {
+			adev->gfx.rlc.funcs->stop(adev);
+		}
 
-	adev->gfx.rlc.funcs->stop(adev);
-	return 0;
+		return 0;
 }
 
 static int gfx_v9_0_suspend(struct amdgpu_ip_block *ip_block)
@@ -4084,7 +3868,7 @@ static bool gfx_v9_0_is_idle(void *handl
 	struct amdgpu_device *adev = (struct amdgpu_device *)handle;
 
 	if (REG_GET_FIELD(RREG32_SOC15(GC, 0, mmGRBM_STATUS),
-				GRBM_STATUS, GUI_ACTIVE))
+		GRBM_STATUS, GUI_ACTIVE))
 		return false;
 	else
 		return true;
@@ -4109,44 +3893,34 @@ static int gfx_v9_0_soft_reset(struct am
 	u32 tmp;
 	struct amdgpu_device *adev = ip_block->adev;
 
-	/* GRBM_STATUS */
 	tmp = RREG32_SOC15(GC, 0, mmGRBM_STATUS);
 	if (tmp & (GRBM_STATUS__PA_BUSY_MASK | GRBM_STATUS__SC_BUSY_MASK |
-		   GRBM_STATUS__BCI_BUSY_MASK | GRBM_STATUS__SX_BUSY_MASK |
-		   GRBM_STATUS__TA_BUSY_MASK | GRBM_STATUS__VGT_BUSY_MASK |
-		   GRBM_STATUS__DB_BUSY_MASK | GRBM_STATUS__CB_BUSY_MASK |
-		   GRBM_STATUS__GDS_BUSY_MASK | GRBM_STATUS__SPI_BUSY_MASK |
-		   GRBM_STATUS__IA_BUSY_MASK | GRBM_STATUS__IA_BUSY_NO_DMA_MASK)) {
-		grbm_soft_reset = REG_SET_FIELD(grbm_soft_reset,
-						GRBM_SOFT_RESET, SOFT_RESET_CP, 1);
-		grbm_soft_reset = REG_SET_FIELD(grbm_soft_reset,
-						GRBM_SOFT_RESET, SOFT_RESET_GFX, 1);
-	}
-
-	if (tmp & (GRBM_STATUS__CP_BUSY_MASK | GRBM_STATUS__CP_COHERENCY_BUSY_MASK)) {
-		grbm_soft_reset = REG_SET_FIELD(grbm_soft_reset,
-						GRBM_SOFT_RESET, SOFT_RESET_CP, 1);
-	}
-
-	/* GRBM_STATUS2 */
-	tmp = RREG32_SOC15(GC, 0, mmGRBM_STATUS2);
-	if (REG_GET_FIELD(tmp, GRBM_STATUS2, RLC_BUSY))
-		grbm_soft_reset = REG_SET_FIELD(grbm_soft_reset,
-						GRBM_SOFT_RESET, SOFT_RESET_RLC, 1);
-
-
-	if (grbm_soft_reset) {
-		/* stop the rlc */
-		adev->gfx.rlc.funcs->stop(adev);
-
-		if (adev->gfx.num_gfx_rings)
-			/* Disable GFX parsing/prefetching */
-			gfx_v9_0_cp_gfx_enable(adev, false);
-
-		/* Disable MEC parsing/prefetching */
-		gfx_v9_0_cp_compute_enable(adev, false);
+		GRBM_STATUS__BCI_BUSY_MASK | GRBM_STATUS__SX_BUSY_MASK |
+		GRBM_STATUS__TA_BUSY_MASK | GRBM_STATUS__VGT_BUSY_MASK |
+		GRBM_STATUS__DB_BUSY_MASK | GRBM_STATUS__CB_BUSY_MASK |
+		GRBM_STATUS__GDS_BUSY_MASK | GRBM_STATUS__SPI_BUSY_MASK |
+		GRBM_STATUS__IA_BUSY_MASK | GRBM_STATUS__IA_BUSY_NO_DMA_MASK)) {
+		grbm_soft_reset |= REG_SET_FIELD(0, GRBM_SOFT_RESET, SOFT_RESET_CP, 1);
+	grbm_soft_reset |= REG_SET_FIELD(0, GRBM_SOFT_RESET, SOFT_RESET_GFX, 1);
+		}
+
+		if (tmp & (GRBM_STATUS__CP_BUSY_MASK | GRBM_STATUS__CP_COHERENCY_BUSY_MASK)) {
+			grbm_soft_reset |= REG_SET_FIELD(0, GRBM_SOFT_RESET, SOFT_RESET_CP, 1);
+		}
+
+		tmp = RREG32_SOC15(GC, 0, mmGRBM_STATUS2);
+		if (REG_GET_FIELD(tmp, GRBM_STATUS2, RLC_BUSY)) {
+			grbm_soft_reset |= REG_SET_FIELD(0, GRBM_SOFT_RESET, SOFT_RESET_RLC, 1);
+		}
 
 		if (grbm_soft_reset) {
+			adev->gfx.rlc.funcs->stop(adev);
+
+			if (adev->gfx.num_gfx_rings)
+				gfx_v9_0_cp_gfx_enable(adev, false);
+
+			gfx_v9_0_cp_compute_enable(adev, false);
+
 			tmp = RREG32_SOC15(GC, 0, mmGRBM_SOFT_RESET);
 			tmp |= grbm_soft_reset;
 			dev_info(adev->dev, "GRBM_SOFT_RESET=0x%08X\n", tmp);
@@ -4158,12 +3932,10 @@ static int gfx_v9_0_soft_reset(struct am
 			tmp &= ~grbm_soft_reset;
 			WREG32_SOC15(GC, 0, mmGRBM_SOFT_RESET, tmp);
 			tmp = RREG32_SOC15(GC, 0, mmGRBM_SOFT_RESET);
-		}
 
-		/* Wait a little for things to settle down */
-		udelay(50);
-	}
-	return 0;
+			udelay(50);
+		}
+		return 0;
 }
 
 static uint64_t gfx_v9_0_kiq_read_clock(struct amdgpu_device *adev)
@@ -4184,16 +3956,16 @@ static uint64_t gfx_v9_0_kiq_read_clock(
 	}
 	amdgpu_ring_alloc(ring, 32);
 	amdgpu_ring_write(ring, PACKET3(PACKET3_COPY_DATA, 4));
-	amdgpu_ring_write(ring, 9 |	/* src: register*/
-				(5 << 8) |	/* dst: memory */
-				(1 << 16) |	/* count sel */
-				(1 << 20));	/* write confirm */
+	amdgpu_ring_write(ring, 9 |
+	(5 << 8) |
+	(1 << 16) |
+	(1 << 20));
 	amdgpu_ring_write(ring, 0);
 	amdgpu_ring_write(ring, 0);
 	amdgpu_ring_write(ring, lower_32_bits(adev->wb.gpu_addr +
-				reg_val_offs * 4));
+	reg_val_offs * 4));
 	amdgpu_ring_write(ring, upper_32_bits(adev->wb.gpu_addr +
-				reg_val_offs * 4));
+	reg_val_offs * 4));
 	r = amdgpu_fence_emit_polling(ring, &seq, MAX_KIQ_REG_WAIT);
 	if (r)
 		goto failed_undo;
@@ -4203,14 +3975,6 @@ static uint64_t gfx_v9_0_kiq_read_clock(
 
 	r = amdgpu_fence_wait_polling(ring, seq, MAX_KIQ_REG_WAIT);
 
-	/* don't wait anymore for gpu reset case because this way may
-	 * block gpu_recover() routine forever, e.g. this virt_kiq_rreg
-	 * is triggered in TTM and ttm_bo_lock_delayed_workqueue() will
-	 * never return if we keep waiting in virt_kiq_rreg, which cause
-	 * gpu_recover() hang there.
-	 *
-	 * also don't wait anymore for IRQ context
-	 * */
 	if (r < 1 && (amdgpu_in_reset(adev)))
 		goto failed_kiq_read;
 
@@ -4225,15 +3989,15 @@ static uint64_t gfx_v9_0_kiq_read_clock(
 
 	mb();
 	value = (uint64_t)adev->wb.wb[reg_val_offs] |
-		(uint64_t)adev->wb.wb[reg_val_offs + 1 ] << 32ULL;
+	(uint64_t)adev->wb.wb[reg_val_offs + 1 ] << 32ULL;
 	amdgpu_device_wb_free(adev, reg_val_offs);
 	return value;
 
-failed_undo:
+	failed_undo:
 	amdgpu_ring_undo(ring);
-failed_unlock:
+	failed_unlock:
 	spin_unlock_irqrestore(&kiq->ring_lock, flags);
-failed_kiq_read:
+	failed_kiq_read:
 	if (reg_val_offs)
 		amdgpu_device_wb_free(adev, reg_val_offs);
 	pr_err("failed to read gpu clock\n");
@@ -4245,67 +4009,60 @@ static uint64_t gfx_v9_0_get_gpu_clock_c
 	uint64_t clock, clock_lo, clock_hi, hi_check;
 
 	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
-	case IP_VERSION(9, 3, 0):
-		preempt_disable();
-		clock_hi = RREG32_SOC15_NO_KIQ(SMUIO, 0, mmGOLDEN_TSC_COUNT_UPPER_Renoir);
-		clock_lo = RREG32_SOC15_NO_KIQ(SMUIO, 0, mmGOLDEN_TSC_COUNT_LOWER_Renoir);
-		hi_check = RREG32_SOC15_NO_KIQ(SMUIO, 0, mmGOLDEN_TSC_COUNT_UPPER_Renoir);
-		/* The SMUIO TSC clock frequency is 100MHz, which sets 32-bit carry over
-		 * roughly every 42 seconds.
-		 */
-		if (hi_check != clock_hi) {
+		case IP_VERSION(9, 3, 0):
+			preempt_disable();
+			clock_hi = RREG32_SOC15_NO_KIQ(SMUIO, 0, mmGOLDEN_TSC_COUNT_UPPER_Renoir);
 			clock_lo = RREG32_SOC15_NO_KIQ(SMUIO, 0, mmGOLDEN_TSC_COUNT_LOWER_Renoir);
-			clock_hi = hi_check;
-		}
-		preempt_enable();
-		clock = clock_lo | (clock_hi << 32ULL);
-		break;
-	default:
-		amdgpu_gfx_off_ctrl(adev, false);
-		mutex_lock(&adev->gfx.gpu_clock_mutex);
-		if (amdgpu_ip_version(adev, GC_HWIP, 0) ==
-			    IP_VERSION(9, 0, 1) &&
-		    amdgpu_sriov_runtime(adev)) {
-			clock = gfx_v9_0_kiq_read_clock(adev);
-		} else {
-			WREG32_SOC15(GC, 0, mmRLC_CAPTURE_GPU_CLOCK_COUNT, 1);
-			clock = (uint64_t)RREG32_SOC15(GC, 0, mmRLC_GPU_CLOCK_COUNT_LSB) |
-				((uint64_t)RREG32_SOC15(GC, 0, mmRLC_GPU_CLOCK_COUNT_MSB) << 32ULL);
-		}
-		mutex_unlock(&adev->gfx.gpu_clock_mutex);
-		amdgpu_gfx_off_ctrl(adev, true);
-		break;
+			hi_check = RREG32_SOC15_NO_KIQ(SMUIO, 0, mmGOLDEN_TSC_COUNT_UPPER_Renoir);
+			if (hi_check != clock_hi) {
+				clock_lo = RREG32_SOC15_NO_KIQ(SMUIO, 0, mmGOLDEN_TSC_COUNT_LOWER_Renoir);
+				clock_hi = hi_check;
+			}
+			preempt_enable();
+			clock = clock_lo | (clock_hi << 32ULL);
+			break;
+		default:
+			amdgpu_gfx_off_ctrl(adev, false);
+			mutex_lock(&adev->gfx.gpu_clock_mutex);
+			if (amdgpu_ip_version(adev, GC_HWIP, 0) ==
+				IP_VERSION(9, 0, 1) &&
+				amdgpu_sriov_runtime(adev)) {
+				clock = gfx_v9_0_kiq_read_clock(adev);
+				} else {
+					WREG32_SOC15(GC, 0, mmRLC_CAPTURE_GPU_CLOCK_COUNT, 1);
+					clock = (uint64_t)RREG32_SOC15(GC, 0, mmRLC_GPU_CLOCK_COUNT_LSB) |
+					((uint64_t)RREG32_SOC15(GC, 0, mmRLC_GPU_CLOCK_COUNT_MSB) << 32ULL);
+				}
+				mutex_unlock(&adev->gfx.gpu_clock_mutex);
+			amdgpu_gfx_off_ctrl(adev, true);
+			break;
 	}
 	return clock;
 }
 
 static void gfx_v9_0_ring_emit_gds_switch(struct amdgpu_ring *ring,
-					  uint32_t vmid,
-					  uint32_t gds_base, uint32_t gds_size,
-					  uint32_t gws_base, uint32_t gws_size,
-					  uint32_t oa_base, uint32_t oa_size)
+										  uint32_t vmid,
+										  uint32_t gds_base, uint32_t gds_size,
+										  uint32_t gws_base, uint32_t gws_size,
+										  uint32_t oa_base, uint32_t oa_size)
 {
 	struct amdgpu_device *adev = ring->adev;
 
-	/* GDS Base */
 	gfx_v9_0_write_data_to_reg(ring, 0, false,
-				   SOC15_REG_OFFSET(GC, 0, mmGDS_VMID0_BASE) + 2 * vmid,
-				   gds_base);
+							   SOC15_REG_OFFSET(GC, 0, mmGDS_VMID0_BASE) + 2 * vmid,
+							   gds_base);
 
-	/* GDS Size */
 	gfx_v9_0_write_data_to_reg(ring, 0, false,
-				   SOC15_REG_OFFSET(GC, 0, mmGDS_VMID0_SIZE) + 2 * vmid,
-				   gds_size);
+							   SOC15_REG_OFFSET(GC, 0, mmGDS_VMID0_SIZE) + 2 * vmid,
+							   gds_size);
 
-	/* GWS */
 	gfx_v9_0_write_data_to_reg(ring, 0, false,
-				   SOC15_REG_OFFSET(GC, 0, mmGDS_GWS_VMID0) + vmid,
-				   gws_size << GDS_GWS_VMID0__SIZE__SHIFT | gws_base);
+							   SOC15_REG_OFFSET(GC, 0, mmGDS_GWS_VMID0) + vmid,
+							   gws_size << GDS_GWS_VMID0__SIZE__SHIFT | gws_base);
 
-	/* OA */
 	gfx_v9_0_write_data_to_reg(ring, 0, false,
-				   SOC15_REG_OFFSET(GC, 0, mmGDS_OA_VMID0) + vmid,
-				   (1 << (oa_size + oa_base)) - (1 << oa_base));
+							   SOC15_REG_OFFSET(GC, 0, mmGDS_OA_VMID0) + vmid,
+							   (1 << (oa_size + oa_base)) - (1 << oa_base));
 }
 
 static const u32 vgpr_init_compute_shader[] =
@@ -4437,111 +4194,108 @@ static const u32 vgpr_init_compute_shade
 	0xbf84fff8, 0xbf810000,
 };
 
-/* When below register arrays changed, please update gpr_reg_size,
-  and sec_ded_counter_reg_size in function gfx_v9_0_do_edc_gpr_workarounds,
-  to cover all gfx9 ASICs */
 static const struct soc15_reg_entry vgpr_init_regs[] = {
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_RESOURCE_LIMITS), 0x0000000 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_X), 0x40 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_Y), 4 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_Z), 1 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_PGM_RSRC1), 0x3f },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_PGM_RSRC2), 0x400000 },  /* 64KB LDS */
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_RESOURCE_LIMITS), 0x0000000 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_X), 0x40 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_Y), 4 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_Z), 1 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_PGM_RSRC1), 0x3f },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_PGM_RSRC2), 0x400000 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },
 };
 
 static const struct soc15_reg_entry vgpr_init_regs_arcturus[] = {
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_RESOURCE_LIMITS), 0x0000000 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_X), 0x40 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_Y), 4 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_Z), 1 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_PGM_RSRC1), 0xbf },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_PGM_RSRC2), 0x400000 },  /* 64KB LDS */
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_RESOURCE_LIMITS), 0x0000000 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_X), 0x40 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_Y), 4 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_Z), 1 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_PGM_RSRC1), 0xbf },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_PGM_RSRC2), 0x400000 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },
 };
 
 static const struct soc15_reg_entry sgpr1_init_regs[] = {
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_RESOURCE_LIMITS), 0x0000000 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_X), 0x40 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_Y), 8 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_Z), 1 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_PGM_RSRC1), 0x240 }, /* (80 GPRS) */
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_PGM_RSRC2), 0x0 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE0), 0x000000ff },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE1), 0x000000ff },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE2), 0x000000ff },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE3), 0x000000ff },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE4), 0x000000ff },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE5), 0x000000ff },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE6), 0x000000ff },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE7), 0x000000ff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_RESOURCE_LIMITS), 0x0000000 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_X), 0x40 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_Y), 8 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_Z), 1 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_PGM_RSRC1), 0x240 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_PGM_RSRC2), 0x0 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE0), 0x000000ff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE1), 0x000000ff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE2), 0x000000ff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE3), 0x000000ff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE4), 0x000000ff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE5), 0x000000ff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE6), 0x000000ff },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE7), 0x000000ff },
 };
 
 static const struct soc15_reg_entry sgpr2_init_regs[] = {
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_RESOURCE_LIMITS), 0x0000000 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_X), 0x40 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_Y), 8 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_Z), 1 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_PGM_RSRC1), 0x240 }, /* (80 GPRS) */
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_PGM_RSRC2), 0x0 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE0), 0x0000ff00 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE1), 0x0000ff00 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE2), 0x0000ff00 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE3), 0x0000ff00 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE4), 0x0000ff00 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE5), 0x0000ff00 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE6), 0x0000ff00 },
-   { SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE7), 0x0000ff00 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_RESOURCE_LIMITS), 0x0000000 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_X), 0x40 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_Y), 8 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_NUM_THREAD_Z), 1 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_PGM_RSRC1), 0x240 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_PGM_RSRC2), 0x0 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE0), 0x0000ff00 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE1), 0x0000ff00 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE2), 0x0000ff00 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE3), 0x0000ff00 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE4), 0x0000ff00 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE5), 0x0000ff00 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE6), 0x0000ff00 },
+	{ SOC15_REG_ENTRY(GC, 0, mmCOMPUTE_STATIC_THREAD_MGMT_SE7), 0x0000ff00 },
 };
 
 static const struct soc15_reg_entry gfx_v9_0_edc_counter_regs[] = {
-   { SOC15_REG_ENTRY(GC, 0, mmCPC_EDC_SCRATCH_CNT), 0, 1, 1},
-   { SOC15_REG_ENTRY(GC, 0, mmCPC_EDC_UCODE_CNT), 0, 1, 1},
-   { SOC15_REG_ENTRY(GC, 0, mmCPF_EDC_ROQ_CNT), 0, 1, 1},
-   { SOC15_REG_ENTRY(GC, 0, mmCPF_EDC_TAG_CNT), 0, 1, 1},
-   { SOC15_REG_ENTRY(GC, 0, mmCPG_EDC_DMA_CNT), 0, 1, 1},
-   { SOC15_REG_ENTRY(GC, 0, mmCPG_EDC_TAG_CNT), 0, 1, 1},
-   { SOC15_REG_ENTRY(GC, 0, mmDC_EDC_CSINVOC_CNT), 0, 1, 1},
-   { SOC15_REG_ENTRY(GC, 0, mmDC_EDC_RESTORE_CNT), 0, 1, 1},
-   { SOC15_REG_ENTRY(GC, 0, mmDC_EDC_STATE_CNT), 0, 1, 1},
-   { SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_CNT), 0, 1, 1},
-   { SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_GRBM_CNT), 0, 1, 1},
-   { SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_OA_DED), 0, 1, 1},
-   { SOC15_REG_ENTRY(GC, 0, mmSPI_EDC_CNT), 0, 4, 1},
-   { SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT), 0, 4, 6},
-   { SOC15_REG_ENTRY(GC, 0, mmSQ_EDC_DED_CNT), 0, 4, 16},
-   { SOC15_REG_ENTRY(GC, 0, mmSQ_EDC_INFO), 0, 4, 16},
-   { SOC15_REG_ENTRY(GC, 0, mmSQ_EDC_SEC_CNT), 0, 4, 16},
-   { SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT), 0, 1, 16},
-   { SOC15_REG_ENTRY(GC, 0, mmTCP_ATC_EDC_GATCL1_CNT), 0, 4, 16},
-   { SOC15_REG_ENTRY(GC, 0, mmTCP_EDC_CNT), 0, 4, 16},
-   { SOC15_REG_ENTRY(GC, 0, mmTCP_EDC_CNT_NEW), 0, 4, 16},
-   { SOC15_REG_ENTRY(GC, 0, mmTD_EDC_CNT), 0, 4, 16},
-   { SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT2), 0, 4, 6},
-   { SOC15_REG_ENTRY(GC, 0, mmSQ_EDC_CNT), 0, 4, 16},
-   { SOC15_REG_ENTRY(GC, 0, mmTA_EDC_CNT), 0, 4, 16},
-   { SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_OA_PHY_CNT), 0, 1, 1},
-   { SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_OA_PIPE_CNT), 0, 1, 1},
-   { SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT), 0, 1, 32},
-   { SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT2), 0, 1, 32},
-   { SOC15_REG_ENTRY(GC, 0, mmTCI_EDC_CNT), 0, 1, 72},
-   { SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT2), 0, 1, 16},
-   { SOC15_REG_ENTRY(GC, 0, mmTCA_EDC_CNT), 0, 1, 2},
-   { SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT3), 0, 4, 6},
+	{ SOC15_REG_ENTRY(GC, 0, mmCPC_EDC_SCRATCH_CNT), 0, 1, 1},
+	{ SOC15_REG_ENTRY(GC, 0, mmCPC_EDC_UCODE_CNT), 0, 1, 1},
+	{ SOC15_REG_ENTRY(GC, 0, mmCPF_EDC_ROQ_CNT), 0, 1, 1},
+	{ SOC15_REG_ENTRY(GC, 0, mmCPF_EDC_TAG_CNT), 0, 1, 1},
+	{ SOC15_REG_ENTRY(GC, 0, mmCPG_EDC_DMA_CNT), 0, 1, 1},
+	{ SOC15_REG_ENTRY(GC, 0, mmCPG_EDC_TAG_CNT), 0, 1, 1},
+	{ SOC15_REG_ENTRY(GC, 0, mmDC_EDC_CSINVOC_CNT), 0, 1, 1},
+	{ SOC15_REG_ENTRY(GC, 0, mmDC_EDC_RESTORE_CNT), 0, 1, 1},
+	{ SOC15_REG_ENTRY(GC, 0, mmDC_EDC_STATE_CNT), 0, 1, 1},
+	{ SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_CNT), 0, 1, 1},
+	{ SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_GRBM_CNT), 0, 1, 1},
+	{ SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_OA_DED), 0, 1, 1},
+	{ SOC15_REG_ENTRY(GC, 0, mmSPI_EDC_CNT), 0, 4, 1},
+	{ SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT), 0, 4, 6},
+	{ SOC15_REG_ENTRY(GC, 0, mmSQ_EDC_DED_CNT), 0, 4, 16},
+	{ SOC15_REG_ENTRY(GC, 0, mmSQ_EDC_INFO), 0, 4, 16},
+	{ SOC15_REG_ENTRY(GC, 0, mmSQ_EDC_SEC_CNT), 0, 4, 16},
+	{ SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT), 0, 1, 16},
+	{ SOC15_REG_ENTRY(GC, 0, mmTCP_ATC_EDC_GATCL1_CNT), 0, 4, 16},
+	{ SOC15_REG_ENTRY(GC, 0, mmTCP_EDC_CNT), 0, 4, 16},
+	{ SOC15_REG_ENTRY(GC, 0, mmTCP_EDC_CNT_NEW), 0, 4, 16},
+	{ SOC15_REG_ENTRY(GC, 0, mmTD_EDC_CNT), 0, 4, 16},
+	{ SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT2), 0, 4, 6},
+	{ SOC15_REG_ENTRY(GC, 0, mmSQ_EDC_CNT), 0, 4, 16},
+	{ SOC15_REG_ENTRY(GC, 0, mmTA_EDC_CNT), 0, 4, 16},
+	{ SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_OA_PHY_CNT), 0, 1, 1},
+	{ SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_OA_PIPE_CNT), 0, 1, 1},
+	{ SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT), 0, 1, 32},
+	{ SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT2), 0, 1, 32},
+	{ SOC15_REG_ENTRY(GC, 0, mmTCI_EDC_CNT), 0, 1, 72},
+	{ SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT2), 0, 1, 16},
+	{ SOC15_REG_ENTRY(GC, 0, mmTCA_EDC_CNT), 0, 1, 2},
+	{ SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT3), 0, 4, 6},
 };
 
 static int gfx_v9_0_do_edc_gds_workarounds(struct amdgpu_device *adev)
@@ -4549,14 +4303,13 @@ static int gfx_v9_0_do_edc_gds_workaroun
 	struct amdgpu_ring *ring = &adev->gfx.compute_ring[0];
 	int i, r;
 
-	/* only support when RAS is enabled */
 	if (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))
 		return 0;
 
 	r = amdgpu_ring_alloc(ring, 7);
 	if (r) {
 		DRM_ERROR("amdgpu: GDS workarounds failed to lock ring %s (%d).\n",
-			ring->name, r);
+				  ring->name, r);
 		return r;
 	}
 
@@ -4565,15 +4318,15 @@ static int gfx_v9_0_do_edc_gds_workaroun
 
 	amdgpu_ring_write(ring, PACKET3(PACKET3_DMA_DATA, 5));
 	amdgpu_ring_write(ring, (PACKET3_DMA_DATA_CP_SYNC |
-				PACKET3_DMA_DATA_DST_SEL(1) |
-				PACKET3_DMA_DATA_SRC_SEL(2) |
-				PACKET3_DMA_DATA_ENGINE(0)));
+	PACKET3_DMA_DATA_DST_SEL(1) |
+	PACKET3_DMA_DATA_SRC_SEL(2) |
+	PACKET3_DMA_DATA_ENGINE(0)));
 	amdgpu_ring_write(ring, 0);
 	amdgpu_ring_write(ring, 0);
 	amdgpu_ring_write(ring, 0);
 	amdgpu_ring_write(ring, 0);
 	amdgpu_ring_write(ring, PACKET3_DMA_DATA_CMD_RAW_WAIT |
-				adev->gds.gds_size);
+	adev->gds.gds_size);
 
 	amdgpu_ring_commit(ring);
 
@@ -4601,19 +4354,17 @@ static int gfx_v9_0_do_edc_gpr_workaroun
 	u64 gpu_addr;
 
 	int compute_dim_x = adev->gfx.config.max_shader_engines *
-						adev->gfx.config.max_cu_per_sh *
-						adev->gfx.config.max_sh_per_se;
+	adev->gfx.config.max_cu_per_sh *
+	adev->gfx.config.max_sh_per_se;
 	int sgpr_work_group_size = 5;
 	int gpr_reg_size = adev->gfx.config.max_shader_engines + 6;
 	int vgpr_init_shader_size;
 	const u32 *vgpr_init_shader_ptr;
 	const struct soc15_reg_entry *vgpr_init_regs_ptr;
 
-	/* only support when RAS is enabled */
 	if (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))
 		return 0;
 
-	/* bail if the compute ring is not ready */
 	if (!ring->sched.ready)
 		return 0;
 
@@ -4628,135 +4379,115 @@ static int gfx_v9_0_do_edc_gpr_workaroun
 	}
 
 	total_size =
-		(gpr_reg_size * 3 + 4 + 5 + 2) * 4; /* VGPRS */
+	(gpr_reg_size * 3 + 4 + 5 + 2) * 4;
 	total_size +=
-		(gpr_reg_size * 3 + 4 + 5 + 2) * 4; /* SGPRS1 */
+	(gpr_reg_size * 3 + 4 + 5 + 2) * 4;
 	total_size +=
-		(gpr_reg_size * 3 + 4 + 5 + 2) * 4; /* SGPRS2 */
+	(gpr_reg_size * 3 + 4 + 5 + 2) * 4;
 	total_size = ALIGN(total_size, 256);
 	vgpr_offset = total_size;
 	total_size += ALIGN(vgpr_init_shader_size, 256);
 	sgpr_offset = total_size;
 	total_size += sizeof(sgpr_init_compute_shader);
 
-	/* allocate an indirect buffer to put the commands in */
 	memset(&ib, 0, sizeof(ib));
 	r = amdgpu_ib_get(adev, NULL, total_size,
-					AMDGPU_IB_POOL_DIRECT, &ib);
+					  AMDGPU_IB_POOL_DIRECT, &ib);
 	if (r) {
 		DRM_ERROR("amdgpu: failed to get ib (%d).\n", r);
 		return r;
 	}
 
-	/* load the compute shaders */
 	for (i = 0; i < vgpr_init_shader_size/sizeof(u32); i++)
 		ib.ptr[i + (vgpr_offset / 4)] = vgpr_init_shader_ptr[i];
 
 	for (i = 0; i < ARRAY_SIZE(sgpr_init_compute_shader); i++)
 		ib.ptr[i + (sgpr_offset / 4)] = sgpr_init_compute_shader[i];
 
-	/* init the ib length to 0 */
 	ib.length_dw = 0;
 
-	/* VGPR */
-	/* write the register state for the compute dispatch */
 	for (i = 0; i < gpr_reg_size; i++) {
 		ib.ptr[ib.length_dw++] = PACKET3(PACKET3_SET_SH_REG, 1);
 		ib.ptr[ib.length_dw++] = SOC15_REG_ENTRY_OFFSET(vgpr_init_regs_ptr[i])
-								- PACKET3_SET_SH_REG_START;
+		- PACKET3_SET_SH_REG_START;
 		ib.ptr[ib.length_dw++] = vgpr_init_regs_ptr[i].reg_value;
 	}
-	/* write the shader start address: mmCOMPUTE_PGM_LO, mmCOMPUTE_PGM_HI */
 	gpu_addr = (ib.gpu_addr + (u64)vgpr_offset) >> 8;
 	ib.ptr[ib.length_dw++] = PACKET3(PACKET3_SET_SH_REG, 2);
 	ib.ptr[ib.length_dw++] = SOC15_REG_OFFSET(GC, 0, mmCOMPUTE_PGM_LO)
-							- PACKET3_SET_SH_REG_START;
+	- PACKET3_SET_SH_REG_START;
 	ib.ptr[ib.length_dw++] = lower_32_bits(gpu_addr);
 	ib.ptr[ib.length_dw++] = upper_32_bits(gpu_addr);
 
-	/* write dispatch packet */
 	ib.ptr[ib.length_dw++] = PACKET3(PACKET3_DISPATCH_DIRECT, 3);
-	ib.ptr[ib.length_dw++] = compute_dim_x * 2; /* x */
-	ib.ptr[ib.length_dw++] = 1; /* y */
-	ib.ptr[ib.length_dw++] = 1; /* z */
+	ib.ptr[ib.length_dw++] = compute_dim_x * 2;
+	ib.ptr[ib.length_dw++] = 1;
+	ib.ptr[ib.length_dw++] = 1;
 	ib.ptr[ib.length_dw++] =
-		REG_SET_FIELD(0, COMPUTE_DISPATCH_INITIATOR, COMPUTE_SHADER_EN, 1);
+	REG_SET_FIELD(0, COMPUTE_DISPATCH_INITIATOR, COMPUTE_SHADER_EN, 1);
 
-	/* write CS partial flush packet */
 	ib.ptr[ib.length_dw++] = PACKET3(PACKET3_EVENT_WRITE, 0);
 	ib.ptr[ib.length_dw++] = EVENT_TYPE(7) | EVENT_INDEX(4);
 
-	/* SGPR1 */
-	/* write the register state for the compute dispatch */
 	for (i = 0; i < gpr_reg_size; i++) {
 		ib.ptr[ib.length_dw++] = PACKET3(PACKET3_SET_SH_REG, 1);
 		ib.ptr[ib.length_dw++] = SOC15_REG_ENTRY_OFFSET(sgpr1_init_regs[i])
-								- PACKET3_SET_SH_REG_START;
+		- PACKET3_SET_SH_REG_START;
 		ib.ptr[ib.length_dw++] = sgpr1_init_regs[i].reg_value;
 	}
-	/* write the shader start address: mmCOMPUTE_PGM_LO, mmCOMPUTE_PGM_HI */
 	gpu_addr = (ib.gpu_addr + (u64)sgpr_offset) >> 8;
 	ib.ptr[ib.length_dw++] = PACKET3(PACKET3_SET_SH_REG, 2);
 	ib.ptr[ib.length_dw++] = SOC15_REG_OFFSET(GC, 0, mmCOMPUTE_PGM_LO)
-							- PACKET3_SET_SH_REG_START;
+	- PACKET3_SET_SH_REG_START;
 	ib.ptr[ib.length_dw++] = lower_32_bits(gpu_addr);
 	ib.ptr[ib.length_dw++] = upper_32_bits(gpu_addr);
 
-	/* write dispatch packet */
 	ib.ptr[ib.length_dw++] = PACKET3(PACKET3_DISPATCH_DIRECT, 3);
-	ib.ptr[ib.length_dw++] = compute_dim_x / 2 * sgpr_work_group_size; /* x */
-	ib.ptr[ib.length_dw++] = 1; /* y */
-	ib.ptr[ib.length_dw++] = 1; /* z */
+	ib.ptr[ib.length_dw++] = compute_dim_x / 2 * sgpr_work_group_size;
+	ib.ptr[ib.length_dw++] = 1;
+	ib.ptr[ib.length_dw++] = 1;
 	ib.ptr[ib.length_dw++] =
-		REG_SET_FIELD(0, COMPUTE_DISPATCH_INITIATOR, COMPUTE_SHADER_EN, 1);
+	REG_SET_FIELD(0, COMPUTE_DISPATCH_INITIATOR, COMPUTE_SHADER_EN, 1);
 
-	/* write CS partial flush packet */
 	ib.ptr[ib.length_dw++] = PACKET3(PACKET3_EVENT_WRITE, 0);
 	ib.ptr[ib.length_dw++] = EVENT_TYPE(7) | EVENT_INDEX(4);
 
-	/* SGPR2 */
-	/* write the register state for the compute dispatch */
 	for (i = 0; i < gpr_reg_size; i++) {
 		ib.ptr[ib.length_dw++] = PACKET3(PACKET3_SET_SH_REG, 1);
 		ib.ptr[ib.length_dw++] = SOC15_REG_ENTRY_OFFSET(sgpr2_init_regs[i])
-								- PACKET3_SET_SH_REG_START;
+		- PACKET3_SET_SH_REG_START;
 		ib.ptr[ib.length_dw++] = sgpr2_init_regs[i].reg_value;
 	}
-	/* write the shader start address: mmCOMPUTE_PGM_LO, mmCOMPUTE_PGM_HI */
 	gpu_addr = (ib.gpu_addr + (u64)sgpr_offset) >> 8;
 	ib.ptr[ib.length_dw++] = PACKET3(PACKET3_SET_SH_REG, 2);
 	ib.ptr[ib.length_dw++] = SOC15_REG_OFFSET(GC, 0, mmCOMPUTE_PGM_LO)
-							- PACKET3_SET_SH_REG_START;
+	- PACKET3_SET_SH_REG_START;
 	ib.ptr[ib.length_dw++] = lower_32_bits(gpu_addr);
 	ib.ptr[ib.length_dw++] = upper_32_bits(gpu_addr);
 
-	/* write dispatch packet */
 	ib.ptr[ib.length_dw++] = PACKET3(PACKET3_DISPATCH_DIRECT, 3);
-	ib.ptr[ib.length_dw++] = compute_dim_x / 2 * sgpr_work_group_size; /* x */
-	ib.ptr[ib.length_dw++] = 1; /* y */
-	ib.ptr[ib.length_dw++] = 1; /* z */
+	ib.ptr[ib.length_dw++] = compute_dim_x / 2 * sgpr_work_group_size;
+	ib.ptr[ib.length_dw++] = 1;
+	ib.ptr[ib.length_dw++] = 1;
 	ib.ptr[ib.length_dw++] =
-		REG_SET_FIELD(0, COMPUTE_DISPATCH_INITIATOR, COMPUTE_SHADER_EN, 1);
+	REG_SET_FIELD(0, COMPUTE_DISPATCH_INITIATOR, COMPUTE_SHADER_EN, 1);
 
-	/* write CS partial flush packet */
 	ib.ptr[ib.length_dw++] = PACKET3(PACKET3_EVENT_WRITE, 0);
 	ib.ptr[ib.length_dw++] = EVENT_TYPE(7) | EVENT_INDEX(4);
 
-	/* shedule the ib on the ring */
 	r = amdgpu_ib_schedule(ring, 1, &ib, NULL, &f);
 	if (r) {
 		DRM_ERROR("amdgpu: ib submit failed (%d).\n", r);
 		goto fail;
 	}
 
-	/* wait for the GPU to finish processing the IB */
 	r = dma_fence_wait(f, false);
 	if (r) {
 		DRM_ERROR("amdgpu: fence wait failed (%d).\n", r);
 		goto fail;
 	}
 
-fail:
+	fail:
 	amdgpu_ib_free(&ib, NULL);
 	dma_fence_put(f);
 
@@ -4770,20 +4501,19 @@ static int gfx_v9_0_early_init(struct am
 	adev->gfx.funcs = &gfx_v9_0_gfx_funcs;
 
 	if (amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 1) ||
-	    amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 2))
+		amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 2))
 		adev->gfx.num_gfx_rings = 0;
 	else
 		adev->gfx.num_gfx_rings = GFX9_NUM_GFX_RINGS;
 	adev->gfx.xcc_mask = 1;
 	adev->gfx.num_compute_rings = min(amdgpu_gfx_get_num_kcq(adev),
-					  AMDGPU_MAX_COMPUTE_RINGS);
+									  AMDGPU_MAX_COMPUTE_RINGS);
 	gfx_v9_0_set_kiq_pm4_funcs(adev);
 	gfx_v9_0_set_ring_funcs(adev);
 	gfx_v9_0_set_irq_funcs(adev);
 	gfx_v9_0_set_gds_init(adev);
 	gfx_v9_0_set_rlc_funcs(adev);
 
-	/* init rlcg reg access ctrl */
 	gfx_v9_0_init_rlcg_reg_access_ctrl(adev);
 
 	return gfx_v9_0_init_microcode(adev);
@@ -4794,22 +4524,15 @@ static int gfx_v9_0_ecc_late_init(struct
 	struct amdgpu_device *adev = ip_block->adev;
 	int r;
 
-	/*
-	 * Temp workaround to fix the issue that CP firmware fails to
-	 * update read pointer when CPDMA is writing clearing operation
-	 * to GDS in suspend/resume sequence on several cards. So just
-	 * limit this operation in cold boot sequence.
-	 */
 	if ((!adev->in_suspend) &&
-	    (adev->gds.gds_size)) {
+		(adev->gds.gds_size)) {
 		r = gfx_v9_0_do_edc_gds_workarounds(adev);
-		if (r)
-			return r;
-	}
+	if (r)
+		return r;
+		}
 
-	/* requires IBs so do in late init after IB pool is initialized */
-	if (amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 2))
-		r = gfx_v9_4_2_do_edc_gpr_workarounds(adev);
+		if (amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 2))
+			r = gfx_v9_4_2_do_edc_gpr_workarounds(adev);
 	else
 		r = gfx_v9_0_do_edc_gpr_workarounds(adev);
 
@@ -4817,7 +4540,7 @@ static int gfx_v9_0_ecc_late_init(struct
 		return r;
 
 	if (adev->gfx.ras &&
-	    adev->gfx.ras->enable_watchdog_timer)
+		adev->gfx.ras->enable_watchdog_timer)
 		adev->gfx.ras->enable_watchdog_timer(adev);
 
 	return 0;
@@ -4846,19 +4569,18 @@ static int gfx_v9_0_late_init(struct amd
 
 	if (amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 2))
 		gfx_v9_4_2_debug_trap_config_init(adev,
-			adev->vm_manager.first_kfd_vmid, AMDGPU_NUM_VMID);
-	else
-		gfx_v9_0_debug_trap_config_init(adev,
-			adev->vm_manager.first_kfd_vmid, AMDGPU_NUM_VMID);
+										  adev->vm_manager.first_kfd_vmid, AMDGPU_NUM_VMID);
+		else
+			gfx_v9_0_debug_trap_config_init(adev,
+											adev->vm_manager.first_kfd_vmid, AMDGPU_NUM_VMID);
 
-	return 0;
+			return 0;
 }
 
 static bool gfx_v9_0_is_rlc_enabled(struct amdgpu_device *adev)
 {
 	uint32_t rlc_setting;
 
-	/* if RLC is not enabled, do nothing */
 	rlc_setting = RREG32_SOC15(GC, 0, mmRLC_CNTL);
 	if (!(rlc_setting & RLC_CNTL__RLC_ENABLE_F32_MASK))
 		return false;
@@ -4875,7 +4597,6 @@ static void gfx_v9_0_set_safe_mode(struc
 	data |= (1 << RLC_SAFE_MODE__MESSAGE__SHIFT);
 	WREG32_SOC15(GC, 0, mmRLC_SAFE_MODE, data);
 
-	/* wait for RLC_SAFE_MODE */
 	for (i = 0; i < adev->usec_timeout; i++) {
 		if (!REG_GET_FIELD(RREG32_SOC15(GC, 0, mmRLC_SAFE_MODE), RLC_SAFE_MODE, CMD))
 			break;
@@ -4892,7 +4613,7 @@ static void gfx_v9_0_unset_safe_mode(str
 }
 
 static void gfx_v9_0_update_gfx_cg_power_gating(struct amdgpu_device *adev,
-						bool enable)
+												bool enable)
 {
 	amdgpu_gfx_rlc_enter_safe_mode(adev, 0);
 
@@ -4910,11 +4631,8 @@ static void gfx_v9_0_update_gfx_cg_power
 }
 
 static void gfx_v9_0_update_gfx_mg_power_gating(struct amdgpu_device *adev,
-						bool enable)
+												bool enable)
 {
-	/* TODO: double check if we need to perform under safe mode */
-	/* gfx_v9_0_enter_rlc_safe_mode(adev); */
-
 	if ((adev->pg_flags & AMD_PG_SUPPORT_GFX_SMG) && enable)
 		gfx_v9_0_enable_gfx_static_mg_power_gating(adev, true);
 	else
@@ -4925,42 +4643,35 @@ static void gfx_v9_0_update_gfx_mg_power
 	else
 		gfx_v9_0_enable_gfx_dynamic_mg_power_gating(adev, false);
 
-	/* gfx_v9_0_exit_rlc_safe_mode(adev); */
 }
 
 static void gfx_v9_0_update_medium_grain_clock_gating(struct amdgpu_device *adev,
-						      bool enable)
+													  bool enable)
 {
 	uint32_t data, def;
 
-	/* It is disabled by HW by default */
 	if (enable && (adev->cg_flags & AMD_CG_SUPPORT_GFX_MGCG)) {
-		/* 1 - RLC_CGTT_MGCG_OVERRIDE */
 		def = data = RREG32_SOC15(GC, 0, mmRLC_CGTT_MGCG_OVERRIDE);
 
 		if (amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 2, 1))
 			data &= ~RLC_CGTT_MGCG_OVERRIDE__CPF_CGTT_SCLK_OVERRIDE_MASK;
 
 		data &= ~(RLC_CGTT_MGCG_OVERRIDE__GRBM_CGTT_SCLK_OVERRIDE_MASK |
-			  RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGCG_OVERRIDE_MASK |
-			  RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGLS_OVERRIDE_MASK);
+		RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGCG_OVERRIDE_MASK |
+		RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGLS_OVERRIDE_MASK);
 
-		/* only for Vega10 & Raven1 */
 		data |= RLC_CGTT_MGCG_OVERRIDE__RLC_CGTT_SCLK_OVERRIDE_MASK;
 
 		if (def != data)
 			WREG32_SOC15(GC, 0, mmRLC_CGTT_MGCG_OVERRIDE, data);
 
-		/* MGLS is a global flag to control all MGLS in GFX */
 		if (adev->cg_flags & AMD_CG_SUPPORT_GFX_MGLS) {
-			/* 2 - RLC memory Light sleep */
 			if (adev->cg_flags & AMD_CG_SUPPORT_GFX_RLC_LS) {
 				def = data = RREG32_SOC15(GC, 0, mmRLC_MEM_SLP_CNTL);
 				data |= RLC_MEM_SLP_CNTL__RLC_MEM_LS_EN_MASK;
 				if (def != data)
 					WREG32_SOC15(GC, 0, mmRLC_MEM_SLP_CNTL, data);
 			}
-			/* 3 - CP memory Light sleep */
 			if (adev->cg_flags & AMD_CG_SUPPORT_GFX_CP_LS) {
 				def = data = RREG32_SOC15(GC, 0, mmCP_MEM_SLP_CNTL);
 				data |= CP_MEM_SLP_CNTL__CP_MEM_LS_EN_MASK;
@@ -4969,28 +4680,25 @@ static void gfx_v9_0_update_medium_grain
 			}
 		}
 	} else {
-		/* 1 - MGCG_OVERRIDE */
 		def = data = RREG32_SOC15(GC, 0, mmRLC_CGTT_MGCG_OVERRIDE);
 
 		if (amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 2, 1))
 			data |= RLC_CGTT_MGCG_OVERRIDE__CPF_CGTT_SCLK_OVERRIDE_MASK;
 
 		data |= (RLC_CGTT_MGCG_OVERRIDE__RLC_CGTT_SCLK_OVERRIDE_MASK |
-			 RLC_CGTT_MGCG_OVERRIDE__GRBM_CGTT_SCLK_OVERRIDE_MASK |
-			 RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGCG_OVERRIDE_MASK |
-			 RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGLS_OVERRIDE_MASK);
+		RLC_CGTT_MGCG_OVERRIDE__GRBM_CGTT_SCLK_OVERRIDE_MASK |
+		RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGCG_OVERRIDE_MASK |
+		RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGLS_OVERRIDE_MASK);
 
 		if (def != data)
 			WREG32_SOC15(GC, 0, mmRLC_CGTT_MGCG_OVERRIDE, data);
 
-		/* 2 - disable MGLS in RLC */
 		data = RREG32_SOC15(GC, 0, mmRLC_MEM_SLP_CNTL);
 		if (data & RLC_MEM_SLP_CNTL__RLC_MEM_LS_EN_MASK) {
 			data &= ~RLC_MEM_SLP_CNTL__RLC_MEM_LS_EN_MASK;
 			WREG32_SOC15(GC, 0, mmRLC_MEM_SLP_CNTL, data);
 		}
 
-		/* 3 - disable MGLS in CP */
 		data = RREG32_SOC15(GC, 0, mmCP_MEM_SLP_CNTL);
 		if (data & CP_MEM_SLP_CNTL__CP_MEM_LS_EN_MASK) {
 			data &= ~CP_MEM_SLP_CNTL__CP_MEM_LS_EN_MASK;
@@ -5000,125 +4708,100 @@ static void gfx_v9_0_update_medium_grain
 }
 
 static void gfx_v9_0_update_3d_clock_gating(struct amdgpu_device *adev,
-					   bool enable)
+											bool enable)
 {
 	uint32_t data, def;
 
 	if (!adev->gfx.num_gfx_rings)
 		return;
 
-	/* Enable 3D CGCG/CGLS */
 	if (enable) {
-		/* write cmd to clear cgcg/cgls ov */
 		def = data = RREG32_SOC15(GC, 0, mmRLC_CGTT_MGCG_OVERRIDE);
-		/* unset CGCG override */
 		data &= ~RLC_CGTT_MGCG_OVERRIDE__GFXIP_GFX3D_CG_OVERRIDE_MASK;
-		/* update CGCG and CGLS override bits */
 		if (def != data)
 			WREG32_SOC15(GC, 0, mmRLC_CGTT_MGCG_OVERRIDE, data);
 
-		/* enable 3Dcgcg FSM(0x0000363f) */
 		def = RREG32_SOC15(GC, 0, mmRLC_CGCG_CGLS_CTRL_3D);
 
 		if (adev->cg_flags & AMD_CG_SUPPORT_GFX_3D_CGCG)
 			data = (0x36 << RLC_CGCG_CGLS_CTRL_3D__CGCG_GFX_IDLE_THRESHOLD__SHIFT) |
-				RLC_CGCG_CGLS_CTRL_3D__CGCG_EN_MASK;
+			RLC_CGCG_CGLS_CTRL_3D__CGCG_EN_MASK;
 		else
 			data = 0x0 << RLC_CGCG_CGLS_CTRL_3D__CGCG_GFX_IDLE_THRESHOLD__SHIFT;
 
 		if (adev->cg_flags & AMD_CG_SUPPORT_GFX_3D_CGLS)
 			data |= (0x000F << RLC_CGCG_CGLS_CTRL_3D__CGLS_REP_COMPANSAT_DELAY__SHIFT) |
-				RLC_CGCG_CGLS_CTRL_3D__CGLS_EN_MASK;
+			RLC_CGCG_CGLS_CTRL_3D__CGLS_EN_MASK;
 		if (def != data)
 			WREG32_SOC15(GC, 0, mmRLC_CGCG_CGLS_CTRL_3D, data);
 
-		/* set IDLE_POLL_COUNT(0x00900100) */
 		def = RREG32_SOC15(GC, 0, mmCP_RB_WPTR_POLL_CNTL);
 		data = (0x0100 << CP_RB_WPTR_POLL_CNTL__POLL_FREQUENCY__SHIFT) |
-			(0x0090 << CP_RB_WPTR_POLL_CNTL__IDLE_POLL_COUNT__SHIFT);
+		(0x0090 << CP_RB_WPTR_POLL_CNTL__IDLE_POLL_COUNT__SHIFT);
 		if (def != data)
 			WREG32_SOC15(GC, 0, mmCP_RB_WPTR_POLL_CNTL, data);
 	} else {
-		/* Disable CGCG/CGLS */
 		def = data = RREG32_SOC15(GC, 0, mmRLC_CGCG_CGLS_CTRL_3D);
-		/* disable cgcg, cgls should be disabled */
 		data &= ~(RLC_CGCG_CGLS_CTRL_3D__CGCG_EN_MASK |
-			  RLC_CGCG_CGLS_CTRL_3D__CGLS_EN_MASK);
-		/* disable cgcg and cgls in FSM */
+		RLC_CGCG_CGLS_CTRL_3D__CGLS_EN_MASK);
 		if (def != data)
 			WREG32_SOC15(GC, 0, mmRLC_CGCG_CGLS_CTRL_3D, data);
 	}
 }
 
 static void gfx_v9_0_update_coarse_grain_clock_gating(struct amdgpu_device *adev,
-						      bool enable)
+													  bool enable)
 {
 	uint32_t def, data;
 
 	if (enable && (adev->cg_flags & AMD_CG_SUPPORT_GFX_CGCG)) {
 		def = data = RREG32_SOC15(GC, 0, mmRLC_CGTT_MGCG_OVERRIDE);
-		/* unset CGCG override */
 		data &= ~RLC_CGTT_MGCG_OVERRIDE__GFXIP_CGCG_OVERRIDE_MASK;
 		if (adev->cg_flags & AMD_CG_SUPPORT_GFX_CGLS)
 			data &= ~RLC_CGTT_MGCG_OVERRIDE__GFXIP_CGLS_OVERRIDE_MASK;
 		else
 			data |= RLC_CGTT_MGCG_OVERRIDE__GFXIP_CGLS_OVERRIDE_MASK;
-		/* update CGCG and CGLS override bits */
 		if (def != data)
 			WREG32_SOC15(GC, 0, mmRLC_CGTT_MGCG_OVERRIDE, data);
 
-		/* enable cgcg FSM(0x0000363F) */
 		def = RREG32_SOC15(GC, 0, mmRLC_CGCG_CGLS_CTRL);
 
 		if (amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 1))
 			data = (0x2000 << RLC_CGCG_CGLS_CTRL__CGCG_GFX_IDLE_THRESHOLD__SHIFT) |
-				RLC_CGCG_CGLS_CTRL__CGCG_EN_MASK;
+			RLC_CGCG_CGLS_CTRL__CGCG_EN_MASK;
 		else
 			data = (0x36 << RLC_CGCG_CGLS_CTRL__CGCG_GFX_IDLE_THRESHOLD__SHIFT) |
-				RLC_CGCG_CGLS_CTRL__CGCG_EN_MASK;
+			RLC_CGCG_CGLS_CTRL__CGCG_EN_MASK;
 		if (adev->cg_flags & AMD_CG_SUPPORT_GFX_CGLS)
 			data |= (0x000F << RLC_CGCG_CGLS_CTRL__CGLS_REP_COMPANSAT_DELAY__SHIFT) |
-				RLC_CGCG_CGLS_CTRL__CGLS_EN_MASK;
+			RLC_CGCG_CGLS_CTRL__CGLS_EN_MASK;
 		if (def != data)
 			WREG32_SOC15(GC, 0, mmRLC_CGCG_CGLS_CTRL, data);
 
-		/* set IDLE_POLL_COUNT(0x00900100) */
 		def = RREG32_SOC15(GC, 0, mmCP_RB_WPTR_POLL_CNTL);
 		data = (0x0100 << CP_RB_WPTR_POLL_CNTL__POLL_FREQUENCY__SHIFT) |
-			(0x0090 << CP_RB_WPTR_POLL_CNTL__IDLE_POLL_COUNT__SHIFT);
+		(0x0090 << CP_RB_WPTR_POLL_CNTL__IDLE_POLL_COUNT__SHIFT);
 		if (def != data)
 			WREG32_SOC15(GC, 0, mmCP_RB_WPTR_POLL_CNTL, data);
 	} else {
 		def = data = RREG32_SOC15(GC, 0, mmRLC_CGCG_CGLS_CTRL);
-		/* reset CGCG/CGLS bits */
 		data &= ~(RLC_CGCG_CGLS_CTRL__CGCG_EN_MASK | RLC_CGCG_CGLS_CTRL__CGLS_EN_MASK);
-		/* disable cgcg and cgls in FSM */
 		if (def != data)
 			WREG32_SOC15(GC, 0, mmRLC_CGCG_CGLS_CTRL, data);
 	}
 }
 
 static int gfx_v9_0_update_gfx_clock_gating(struct amdgpu_device *adev,
-					    bool enable)
+											bool enable)
 {
 	amdgpu_gfx_rlc_enter_safe_mode(adev, 0);
 	if (enable) {
-		/* CGCG/CGLS should be enabled after MGCG/MGLS
-		 * ===  MGCG + MGLS ===
-		 */
 		gfx_v9_0_update_medium_grain_clock_gating(adev, enable);
-		/* ===  CGCG /CGLS for GFX 3D Only === */
 		gfx_v9_0_update_3d_clock_gating(adev, enable);
-		/* ===  CGCG + CGLS === */
 		gfx_v9_0_update_coarse_grain_clock_gating(adev, enable);
 	} else {
-		/* CGCG/CGLS should be disabled before MGCG/MGLS
-		 * ===  CGCG + CGLS ===
-		 */
 		gfx_v9_0_update_coarse_grain_clock_gating(adev, enable);
-		/* ===  CGCG /CGLS for GFX 3D Only === */
 		gfx_v9_0_update_3d_clock_gating(adev, enable);
-		/* ===  MGCG + MGLS === */
 		gfx_v9_0_update_medium_grain_clock_gating(adev, enable);
 	}
 	amdgpu_gfx_rlc_exit_safe_mode(adev, 0);
@@ -5126,7 +4809,7 @@ static int gfx_v9_0_update_gfx_clock_gat
 }
 
 static void gfx_v9_0_update_spm_vmid_internal(struct amdgpu_device *adev,
-					      unsigned int vmid)
+											  unsigned int vmid)
 {
 	u32 reg, data;
 
@@ -5155,8 +4838,8 @@ static void gfx_v9_0_update_spm_vmid(str
 }
 
 static bool gfx_v9_0_check_rlcg_range(struct amdgpu_device *adev,
-					uint32_t offset,
-					struct soc15_reg_rlcg *entries, int arr_size)
+									  uint32_t offset,
+									  struct soc15_reg_rlcg *entries, int arr_size)
 {
 	int i;
 	uint32_t reg;
@@ -5179,8 +4862,8 @@ static bool gfx_v9_0_check_rlcg_range(st
 static bool gfx_v9_0_is_rlcg_access_range(struct amdgpu_device *adev, u32 offset)
 {
 	return gfx_v9_0_check_rlcg_range(adev, offset,
-					(void *)rlcg_access_gc_9_0,
-					ARRAY_SIZE(rlcg_access_gc_9_0));
+									 (void *)rlcg_access_gc_9_0,
+									 ARRAY_SIZE(rlcg_access_gc_9_0));
 }
 
 static const struct amdgpu_rlc_funcs gfx_v9_0_rlc_funcs = {
@@ -5200,17 +4883,17 @@ static const struct amdgpu_rlc_funcs gfx
 };
 
 static int gfx_v9_0_set_powergating_state(struct amdgpu_ip_block *ip_block,
-					  enum amd_powergating_state state)
+										  enum amd_powergating_state state)
 {
 	struct amdgpu_device *adev = ip_block->adev;
 	bool enable = (state == AMD_PG_STATE_GATE);
 
 	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
-	case IP_VERSION(9, 2, 2):
-	case IP_VERSION(9, 1, 0):
-	case IP_VERSION(9, 3, 0):
-		if (!enable)
-			amdgpu_gfx_off_ctrl(adev, false);
+		case IP_VERSION(9, 2, 2):
+		case IP_VERSION(9, 1, 0):
+		case IP_VERSION(9, 3, 0):
+			if (!enable)
+				amdgpu_gfx_off_ctrl(adev, false);
 
 		if (adev->pg_flags & AMD_PG_SUPPORT_RLC_SMU_HS) {
 			gfx_v9_0_enable_sck_slow_down_on_power_up(adev, true);
@@ -5225,27 +4908,25 @@ static int gfx_v9_0_set_powergating_stat
 		else
 			gfx_v9_0_enable_cp_power_gating(adev, false);
 
-		/* update gfx cgpg state */
 		gfx_v9_0_update_gfx_cg_power_gating(adev, enable);
 
-		/* update mgcg state */
 		gfx_v9_0_update_gfx_mg_power_gating(adev, enable);
 
 		if (enable)
 			amdgpu_gfx_off_ctrl(adev, true);
 		break;
-	case IP_VERSION(9, 2, 1):
-		amdgpu_gfx_off_ctrl(adev, enable);
-		break;
-	default:
-		break;
+		case IP_VERSION(9, 2, 1):
+			amdgpu_gfx_off_ctrl(adev, enable);
+			break;
+		default:
+			break;
 	}
 
 	return 0;
 }
 
 static int gfx_v9_0_set_clockgating_state(struct amdgpu_ip_block *ip_block,
-					  enum amd_clockgating_state state)
+										  enum amd_clockgating_state state)
 {
 	struct amdgpu_device *adev = ip_block->adev;
 
@@ -5253,19 +4934,19 @@ static int gfx_v9_0_set_clockgating_stat
 		return 0;
 
 	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
-	case IP_VERSION(9, 0, 1):
-	case IP_VERSION(9, 2, 1):
-	case IP_VERSION(9, 4, 0):
-	case IP_VERSION(9, 2, 2):
-	case IP_VERSION(9, 1, 0):
-	case IP_VERSION(9, 4, 1):
-	case IP_VERSION(9, 3, 0):
-	case IP_VERSION(9, 4, 2):
-		gfx_v9_0_update_gfx_clock_gating(adev,
-						 state == AMD_CG_STATE_GATE);
-		break;
-	default:
-		break;
+		case IP_VERSION(9, 0, 1):
+		case IP_VERSION(9, 2, 1):
+		case IP_VERSION(9, 4, 0):
+		case IP_VERSION(9, 2, 2):
+		case IP_VERSION(9, 1, 0):
+		case IP_VERSION(9, 4, 1):
+		case IP_VERSION(9, 3, 0):
+		case IP_VERSION(9, 4, 2):
+			gfx_v9_0_update_gfx_clock_gating(adev,
+											 state == AMD_CG_STATE_GATE);
+			break;
+		default:
+			break;
 	}
 	return 0;
 }
@@ -5278,37 +4959,30 @@ static void gfx_v9_0_get_clockgating_sta
 	if (amdgpu_sriov_vf(adev))
 		*flags = 0;
 
-	/* AMD_CG_SUPPORT_GFX_MGCG */
 	data = RREG32_KIQ(SOC15_REG_OFFSET(GC, 0, mmRLC_CGTT_MGCG_OVERRIDE));
 	if (!(data & RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGCG_OVERRIDE_MASK))
 		*flags |= AMD_CG_SUPPORT_GFX_MGCG;
 
-	/* AMD_CG_SUPPORT_GFX_CGCG */
 	data = RREG32_KIQ(SOC15_REG_OFFSET(GC, 0, mmRLC_CGCG_CGLS_CTRL));
 	if (data & RLC_CGCG_CGLS_CTRL__CGCG_EN_MASK)
 		*flags |= AMD_CG_SUPPORT_GFX_CGCG;
 
-	/* AMD_CG_SUPPORT_GFX_CGLS */
 	if (data & RLC_CGCG_CGLS_CTRL__CGLS_EN_MASK)
 		*flags |= AMD_CG_SUPPORT_GFX_CGLS;
 
-	/* AMD_CG_SUPPORT_GFX_RLC_LS */
 	data = RREG32_KIQ(SOC15_REG_OFFSET(GC, 0, mmRLC_MEM_SLP_CNTL));
 	if (data & RLC_MEM_SLP_CNTL__RLC_MEM_LS_EN_MASK)
 		*flags |= AMD_CG_SUPPORT_GFX_RLC_LS | AMD_CG_SUPPORT_GFX_MGLS;
 
-	/* AMD_CG_SUPPORT_GFX_CP_LS */
 	data = RREG32_KIQ(SOC15_REG_OFFSET(GC, 0, mmCP_MEM_SLP_CNTL));
 	if (data & CP_MEM_SLP_CNTL__CP_MEM_LS_EN_MASK)
 		*flags |= AMD_CG_SUPPORT_GFX_CP_LS | AMD_CG_SUPPORT_GFX_MGLS;
 
 	if (amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 4, 1)) {
-		/* AMD_CG_SUPPORT_GFX_3D_CGCG */
 		data = RREG32_KIQ(SOC15_REG_OFFSET(GC, 0, mmRLC_CGCG_CGLS_CTRL_3D));
 		if (data & RLC_CGCG_CGLS_CTRL_3D__CGCG_EN_MASK)
 			*flags |= AMD_CG_SUPPORT_GFX_3D_CGCG;
 
-		/* AMD_CG_SUPPORT_GFX_3D_CGLS */
 		if (data & RLC_CGCG_CGLS_CTRL_3D__CGLS_EN_MASK)
 			*flags |= AMD_CG_SUPPORT_GFX_3D_CGLS;
 	}
@@ -5316,7 +4990,7 @@ static void gfx_v9_0_get_clockgating_sta
 
 static u64 gfx_v9_0_ring_get_rptr_gfx(struct amdgpu_ring *ring)
 {
-	return *ring->rptr_cpu_addr; /* gfx9 is 32bit rptr*/
+	return *ring->rptr_cpu_addr;
 }
 
 static u64 gfx_v9_0_ring_get_wptr_gfx(struct amdgpu_ring *ring)
@@ -5324,7 +4998,6 @@ static u64 gfx_v9_0_ring_get_wptr_gfx(st
 	struct amdgpu_device *adev = ring->adev;
 	u64 wptr;
 
-	/* XXX check if swapping is necessary on BE */
 	if (ring->use_doorbell) {
 		wptr = atomic64_read((atomic64_t *)ring->wptr_cpu_addr);
 	} else {
@@ -5340,7 +5013,6 @@ static void gfx_v9_0_ring_set_wptr_gfx(s
 	struct amdgpu_device *adev = ring->adev;
 
 	if (ring->use_doorbell) {
-		/* XXX check if swapping is necessary on BE */
 		atomic64_set((atomic64_t *)ring->wptr_cpu_addr, ring->wptr);
 		WDOORBELL64(ring->doorbell_index, ring->wptr);
 	} else {
@@ -5357,31 +5029,31 @@ static void gfx_v9_0_ring_emit_hdp_flush
 
 	if (ring->funcs->type == AMDGPU_RING_TYPE_COMPUTE) {
 		switch (ring->me) {
-		case 1:
-			ref_and_mask = nbio_hf_reg->ref_and_mask_cp2 << ring->pipe;
-			break;
-		case 2:
-			ref_and_mask = nbio_hf_reg->ref_and_mask_cp6 << ring->pipe;
-			break;
-		default:
-			return;
+			case 1:
+				ref_and_mask = nbio_hf_reg->ref_and_mask_cp2 << ring->pipe;
+				break;
+			case 2:
+				ref_and_mask = nbio_hf_reg->ref_and_mask_cp6 << ring->pipe;
+				break;
+			default:
+				return;
 		}
 		reg_mem_engine = 0;
 	} else {
 		ref_and_mask = nbio_hf_reg->ref_and_mask_cp0;
-		reg_mem_engine = 1; /* pfp */
+		reg_mem_engine = 1;
 	}
 
 	gfx_v9_0_wait_reg_mem(ring, reg_mem_engine, 0, 1,
-			      adev->nbio.funcs->get_hdp_flush_req_offset(adev),
-			      adev->nbio.funcs->get_hdp_flush_done_offset(adev),
-			      ref_and_mask, ref_and_mask, 0x20);
+						  adev->nbio.funcs->get_hdp_flush_req_offset(adev),
+						  adev->nbio.funcs->get_hdp_flush_done_offset(adev),
+						  ref_and_mask, ref_and_mask, 0x20);
 }
 
 static void gfx_v9_0_ring_emit_ib_gfx(struct amdgpu_ring *ring,
-					struct amdgpu_job *job,
-					struct amdgpu_ib *ib,
-					uint32_t flags)
+									  struct amdgpu_job *job,
+									  struct amdgpu_ib *ib,
+									  uint32_t flags)
 {
 	unsigned vmid = AMDGPU_JOB_GET_VMID(job);
 	u32 header, control = 0;
@@ -5401,26 +5073,26 @@ static void gfx_v9_0_ring_emit_ib_gfx(st
 
 		if (!(ib->flags & AMDGPU_IB_FLAG_CE) && vmid)
 			gfx_v9_0_ring_emit_de_meta(ring,
-						   (!amdgpu_sriov_vf(ring->adev) &&
-						   flags & AMDGPU_IB_PREEMPTED) ?
-						   true : false,
-						   job->gds_size > 0 && job->gds_base != 0);
+									   (!amdgpu_sriov_vf(ring->adev) &&
+									   flags & AMDGPU_IB_PREEMPTED) ?
+									   true : false,
+							  job->gds_size > 0 && job->gds_base != 0);
 	}
 
 	amdgpu_ring_write(ring, header);
-	BUG_ON(ib->gpu_addr & 0x3); /* Dword align */
+	BUG_ON(ib->gpu_addr & 0x3);
 	amdgpu_ring_write(ring,
-#ifdef __BIG_ENDIAN
-		(2 << 0) |
-#endif
-		lower_32_bits(ib->gpu_addr));
+					  #ifdef __BIG_ENDIAN
+					  (2 << 0) |
+					  #endif
+					  lower_32_bits(ib->gpu_addr));
 	amdgpu_ring_write(ring, upper_32_bits(ib->gpu_addr));
 	amdgpu_ring_ib_on_emit_cntl(ring);
 	amdgpu_ring_write(ring, control);
 }
 
 static void gfx_v9_0_ring_patch_cntl(struct amdgpu_ring *ring,
-				     unsigned offset)
+									 unsigned offset)
 {
 	u32 control = ring->ring[offset];
 
@@ -5429,7 +5101,7 @@ static void gfx_v9_0_ring_patch_cntl(str
 }
 
 static void gfx_v9_0_ring_patch_ce_meta(struct amdgpu_ring *ring,
-					unsigned offset)
+										unsigned offset)
 {
 	struct amdgpu_device *adev = ring->adev;
 	void *ce_payload_cpu_addr;
@@ -5439,10 +5111,10 @@ static void gfx_v9_0_ring_patch_ce_meta(
 
 	if (ring->is_mes_queue) {
 		payload_offset = offsetof(struct amdgpu_mes_ctx_meta_data,
-					  gfx[0].gfx_meta_data) +
-			offsetof(struct v9_gfx_meta_data, ce_payload);
+								  gfx[0].gfx_meta_data) +
+		offsetof(struct v9_gfx_meta_data, ce_payload);
 		ce_payload_cpu_addr =
-			amdgpu_mes_ctx_get_offs_cpu_addr(ring, payload_offset);
+		amdgpu_mes_ctx_get_offs_cpu_addr(ring, payload_offset);
 	} else {
 		payload_offset = offsetof(struct v9_gfx_meta_data, ce_payload);
 		ce_payload_cpu_addr = adev->virt.csa_cpu_addr + payload_offset;
@@ -5452,16 +5124,16 @@ static void gfx_v9_0_ring_patch_ce_meta(
 		memcpy((void *)&ring->ring[offset], ce_payload_cpu_addr, payload_size);
 	} else {
 		memcpy((void *)&ring->ring[offset], ce_payload_cpu_addr,
-		       (ring->buf_mask + 1 - offset) << 2);
+			   (ring->buf_mask + 1 - offset) << 2);
 		payload_size -= (ring->buf_mask + 1 - offset) << 2;
 		memcpy((void *)&ring->ring[0],
-		       ce_payload_cpu_addr + ((ring->buf_mask + 1 - offset) << 2),
-		       payload_size);
+			   ce_payload_cpu_addr + ((ring->buf_mask + 1 - offset) << 2),
+			   payload_size);
 	}
 }
 
 static void gfx_v9_0_ring_patch_de_meta(struct amdgpu_ring *ring,
-					unsigned offset)
+										unsigned offset)
 {
 	struct amdgpu_device *adev = ring->adev;
 	void *de_payload_cpu_addr;
@@ -5471,48 +5143,38 @@ static void gfx_v9_0_ring_patch_de_meta(
 
 	if (ring->is_mes_queue) {
 		payload_offset = offsetof(struct amdgpu_mes_ctx_meta_data,
-					  gfx[0].gfx_meta_data) +
-			offsetof(struct v9_gfx_meta_data, de_payload);
+								  gfx[0].gfx_meta_data) +
+		offsetof(struct v9_gfx_meta_data, de_payload);
 		de_payload_cpu_addr =
-			amdgpu_mes_ctx_get_offs_cpu_addr(ring, payload_offset);
+		amdgpu_mes_ctx_get_offs_cpu_addr(ring, payload_offset);
 	} else {
 		payload_offset = offsetof(struct v9_gfx_meta_data, de_payload);
 		de_payload_cpu_addr = adev->virt.csa_cpu_addr + payload_offset;
 	}
 
 	((struct v9_de_ib_state *)de_payload_cpu_addr)->ib_completion_status =
-		IB_COMPLETION_STATUS_PREEMPTED;
+	IB_COMPLETION_STATUS_PREEMPTED;
 
 	if (offset + (payload_size >> 2) <= ring->buf_mask + 1) {
 		memcpy((void *)&ring->ring[offset], de_payload_cpu_addr, payload_size);
 	} else {
 		memcpy((void *)&ring->ring[offset], de_payload_cpu_addr,
-		       (ring->buf_mask + 1 - offset) << 2);
+			   (ring->buf_mask + 1 - offset) << 2);
 		payload_size -= (ring->buf_mask + 1 - offset) << 2;
 		memcpy((void *)&ring->ring[0],
-		       de_payload_cpu_addr + ((ring->buf_mask + 1 - offset) << 2),
-		       payload_size);
+			   de_payload_cpu_addr + ((ring->buf_mask + 1 - offset) << 2),
+			   payload_size);
 	}
 }
 
 static void gfx_v9_0_ring_emit_ib_compute(struct amdgpu_ring *ring,
-					  struct amdgpu_job *job,
-					  struct amdgpu_ib *ib,
-					  uint32_t flags)
+										  struct amdgpu_job *job,
+										  struct amdgpu_ib *ib,
+										  uint32_t flags)
 {
 	unsigned vmid = AMDGPU_JOB_GET_VMID(job);
 	u32 control = INDIRECT_BUFFER_VALID | ib->length_dw | (vmid << 24);
 
-	/* Currently, there is a high possibility to get wave ID mismatch
-	 * between ME and GDS, leading to a hw deadlock, because ME generates
-	 * different wave IDs than the GDS expects. This situation happens
-	 * randomly when at least 5 compute pipes use GDS ordered append.
-	 * The wave IDs generated by ME are also wrong after suspend/resume.
-	 * Those are probably bugs somewhere else in the kernel driver.
-	 *
-	 * Writing GDS_COMPUTE_MAX_WAVE_ID resets wave ID counters in ME and
-	 * GDS to 0 for this ring (me/pipe).
-	 */
 	if (ib->flags & AMDGPU_IB_FLAG_RESET_GDS_MAX_WAVE_ID) {
 		amdgpu_ring_write(ring, PACKET3(PACKET3_SET_CONFIG_REG, 1));
 		amdgpu_ring_write(ring, mmGDS_COMPUTE_MAX_WAVE_ID);
@@ -5520,18 +5182,18 @@ static void gfx_v9_0_ring_emit_ib_comput
 	}
 
 	amdgpu_ring_write(ring, PACKET3(PACKET3_INDIRECT_BUFFER, 2));
-	BUG_ON(ib->gpu_addr & 0x3); /* Dword align */
+	BUG_ON(ib->gpu_addr & 0x3);
 	amdgpu_ring_write(ring,
-#ifdef __BIG_ENDIAN
-				(2 << 0) |
-#endif
-				lower_32_bits(ib->gpu_addr));
+					  #ifdef __BIG_ENDIAN
+					  (2 << 0) |
+					  #endif
+					  lower_32_bits(ib->gpu_addr));
 	amdgpu_ring_write(ring, upper_32_bits(ib->gpu_addr));
 	amdgpu_ring_write(ring, control);
 }
 
 static void gfx_v9_0_ring_emit_fence(struct amdgpu_ring *ring, u64 addr,
-				     u64 seq, unsigned flags)
+									 u64 seq, unsigned flags)
 {
 	bool write64bit = flags & AMDGPU_FENCE_FLAG_64BIT;
 	bool int_sel = flags & AMDGPU_FENCE_FLAG_INT;
@@ -5539,27 +5201,22 @@ static void gfx_v9_0_ring_emit_fence(str
 	bool exec = flags & AMDGPU_FENCE_FLAG_EXEC;
 	uint32_t dw2 = 0;
 
-	/* RELEASE_MEM - flush caches, send int */
 	amdgpu_ring_write(ring, PACKET3(PACKET3_RELEASE_MEM, 6));
 
 	if (writeback) {
 		dw2 = EOP_TC_NC_ACTION_EN;
 	} else {
 		dw2 = EOP_TCL1_ACTION_EN | EOP_TC_ACTION_EN |
-				EOP_TC_MD_ACTION_EN;
+		EOP_TC_MD_ACTION_EN;
 	}
 	dw2 |= EOP_TC_WB_ACTION_EN | EVENT_TYPE(CACHE_FLUSH_AND_INV_TS_EVENT) |
-				EVENT_INDEX(5);
+	EVENT_INDEX(5);
 	if (exec)
 		dw2 |= EOP_EXEC;
 
 	amdgpu_ring_write(ring, dw2);
 	amdgpu_ring_write(ring, DATA_SEL(write64bit ? 2 : 1) | INT_SEL(int_sel ? 2 : 0));
 
-	/*
-	 * the address should be Qword aligned if 64bit write, Dword
-	 * aligned if only send 32bit data low (discard data high)
-	 */
 	if (write64bit)
 		BUG_ON(addr & 0x7);
 	else
@@ -5578,18 +5235,16 @@ static void gfx_v9_0_ring_emit_pipeline_
 	uint64_t addr = ring->fence_drv.gpu_addr;
 
 	gfx_v9_0_wait_reg_mem(ring, usepfp, 1, 0,
-			      lower_32_bits(addr), upper_32_bits(addr),
-			      seq, 0xffffffff, 4);
+						  lower_32_bits(addr), upper_32_bits(addr),
+						  seq, 0xffffffff, 4);
 }
 
 static void gfx_v9_0_ring_emit_vm_flush(struct amdgpu_ring *ring,
-					unsigned vmid, uint64_t pd_addr)
+										unsigned vmid, uint64_t pd_addr)
 {
 	amdgpu_gmc_emit_flush_gpu_tlb(ring, vmid, pd_addr);
 
-	/* compute doesn't have PFP */
 	if (ring->funcs->type == AMDGPU_RING_TYPE_GFX) {
-		/* sync PFP to ME, otherwise we might get invalid PFP reads */
 		amdgpu_ring_write(ring, PACKET3(PACKET3_PFP_SYNC_ME, 0));
 		amdgpu_ring_write(ring, 0x0);
 	}
@@ -5597,14 +5252,13 @@ static void gfx_v9_0_ring_emit_vm_flush(
 
 static u64 gfx_v9_0_ring_get_rptr_compute(struct amdgpu_ring *ring)
 {
-	return *ring->rptr_cpu_addr; /* gfx9 hardware is 32bit rptr */
+	return *ring->rptr_cpu_addr;
 }
 
 static u64 gfx_v9_0_ring_get_wptr_compute(struct amdgpu_ring *ring)
 {
 	u64 wptr;
 
-	/* XXX check if swapping is necessary on BE */
 	if (ring->use_doorbell)
 		wptr = atomic64_read((atomic64_t *)ring->wptr_cpu_addr);
 	else
@@ -5616,39 +5270,35 @@ static void gfx_v9_0_ring_set_wptr_compu
 {
 	struct amdgpu_device *adev = ring->adev;
 
-	/* XXX check if swapping is necessary on BE */
 	if (ring->use_doorbell) {
 		atomic64_set((atomic64_t *)ring->wptr_cpu_addr, ring->wptr);
 		WDOORBELL64(ring->doorbell_index, ring->wptr);
 	} else{
-		BUG(); /* only DOORBELL method supported on gfx9 now */
+		BUG();
 	}
 }
 
 static void gfx_v9_0_ring_emit_fence_kiq(struct amdgpu_ring *ring, u64 addr,
-					 u64 seq, unsigned int flags)
+										 u64 seq, unsigned int flags)
 {
 	struct amdgpu_device *adev = ring->adev;
 
-	/* we only allocate 32bit for each seq wb address */
 	BUG_ON(flags & AMDGPU_FENCE_FLAG_64BIT);
 
-	/* write fence seq to the "addr" */
 	amdgpu_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, 3));
 	amdgpu_ring_write(ring, (WRITE_DATA_ENGINE_SEL(0) |
-				 WRITE_DATA_DST_SEL(5) | WR_CONFIRM));
+	WRITE_DATA_DST_SEL(5) | WR_CONFIRM));
 	amdgpu_ring_write(ring, lower_32_bits(addr));
 	amdgpu_ring_write(ring, upper_32_bits(addr));
 	amdgpu_ring_write(ring, lower_32_bits(seq));
 
 	if (flags & AMDGPU_FENCE_FLAG_INT) {
-		/* set register to trigger INT */
 		amdgpu_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, 3));
 		amdgpu_ring_write(ring, (WRITE_DATA_ENGINE_SEL(0) |
-					 WRITE_DATA_DST_SEL(0) | WR_CONFIRM));
+		WRITE_DATA_DST_SEL(0) | WR_CONFIRM));
 		amdgpu_ring_write(ring, SOC15_REG_OFFSET(GC, 0, mmCPC_INT_STATUS));
 		amdgpu_ring_write(ring, 0);
-		amdgpu_ring_write(ring, 0x20000000); /* src_id is 178 */
+		amdgpu_ring_write(ring, 0x20000000);
 	}
 }
 
@@ -5670,12 +5320,12 @@ static void gfx_v9_0_ring_emit_ce_meta(s
 
 	if (ring->is_mes_queue) {
 		offset = offsetof(struct amdgpu_mes_ctx_meta_data,
-				  gfx[0].gfx_meta_data) +
-			offsetof(struct v9_gfx_meta_data, ce_payload);
+						  gfx[0].gfx_meta_data) +
+		offsetof(struct v9_gfx_meta_data, ce_payload);
 		ce_payload_gpu_addr =
-			amdgpu_mes_ctx_get_offs_gpu_addr(ring, offset);
+		amdgpu_mes_ctx_get_offs_gpu_addr(ring, offset);
 		ce_payload_cpu_addr =
-			amdgpu_mes_ctx_get_offs_cpu_addr(ring, offset);
+		amdgpu_mes_ctx_get_offs_cpu_addr(ring, offset);
 	} else {
 		offset = offsetof(struct v9_gfx_meta_data, ce_payload);
 		ce_payload_gpu_addr = amdgpu_csa_vaddr(ring->adev) + offset;
@@ -5684,9 +5334,9 @@ static void gfx_v9_0_ring_emit_ce_meta(s
 
 	amdgpu_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, cnt));
 	amdgpu_ring_write(ring, (WRITE_DATA_ENGINE_SEL(2) |
-				 WRITE_DATA_DST_SEL(8) |
-				 WR_CONFIRM) |
-				 WRITE_DATA_CACHE_POLICY(0));
+	WRITE_DATA_DST_SEL(8) |
+	WR_CONFIRM) |
+	WRITE_DATA_CACHE_POLICY(0));
 	amdgpu_ring_write(ring, lower_32_bits(ce_payload_gpu_addr));
 	amdgpu_ring_write(ring, upper_32_bits(ce_payload_gpu_addr));
 
@@ -5694,10 +5344,10 @@ static void gfx_v9_0_ring_emit_ce_meta(s
 
 	if (resume)
 		amdgpu_ring_write_multiple(ring, ce_payload_cpu_addr,
-					   sizeof(ce_payload) >> 2);
-	else
-		amdgpu_ring_write_multiple(ring, (void *)&ce_payload,
-					   sizeof(ce_payload) >> 2);
+								   sizeof(ce_payload) >> 2);
+		else
+			amdgpu_ring_write_multiple(ring, (void *)&ce_payload,
+									   sizeof(ce_payload) >> 2);
 }
 
 static int gfx_v9_0_ring_preempt_ib(struct amdgpu_ring *ring)
@@ -5718,23 +5368,20 @@ static int gfx_v9_0_ring_preempt_ib(stru
 		return -ENOMEM;
 	}
 
-	/* assert preemption condition */
 	amdgpu_ring_set_preempt_cond_exec(ring, false);
 
 	ring->trail_seq += 1;
 	amdgpu_ring_alloc(ring, 13);
 	gfx_v9_0_ring_emit_fence(ring, ring->trail_fence_gpu_addr,
-				 ring->trail_seq, AMDGPU_FENCE_FLAG_EXEC | AMDGPU_FENCE_FLAG_INT);
+							 ring->trail_seq, AMDGPU_FENCE_FLAG_EXEC | AMDGPU_FENCE_FLAG_INT);
 
-	/* assert IB preemption, emit the trailing fence */
 	kiq->pmf->kiq_unmap_queues(kiq_ring, ring, PREEMPT_QUEUES_NO_UNMAP,
-				   ring->trail_fence_gpu_addr,
-				   ring->trail_seq);
+							   ring->trail_fence_gpu_addr,
+							ring->trail_seq);
 
 	amdgpu_ring_commit(kiq_ring);
 	spin_unlock_irqrestore(&kiq->ring_lock, flags);
 
-	/* poll the trailing fence */
 	for (i = 0; i < adev->usec_timeout; i++) {
 		if (ring->trail_seq ==
 			le32_to_cpu(*ring->trail_fence_cpu_addr))
@@ -5747,13 +5394,11 @@ static int gfx_v9_0_ring_preempt_ib(stru
 		DRM_WARN("ring %d timeout to preempt ib\n", ring->idx);
 	}
 
-	/*reset the CP_VMID_PREEMPT after trailing fence*/
 	amdgpu_ring_emit_wreg(ring,
-			      SOC15_REG_OFFSET(GC, 0, mmCP_VMID_PREEMPT),
-			      0x0);
+						  SOC15_REG_OFFSET(GC, 0, mmCP_VMID_PREEMPT),
+						  0x0);
 	amdgpu_ring_commit(ring);
 
-	/* deassert preemption condition */
 	amdgpu_ring_set_preempt_cond_exec(ring, true);
 	return r;
 }
@@ -5768,16 +5413,16 @@ static void gfx_v9_0_ring_emit_de_meta(s
 
 	if (ring->is_mes_queue) {
 		offset = offsetof(struct amdgpu_mes_ctx_meta_data,
-				  gfx[0].gfx_meta_data) +
-			offsetof(struct v9_gfx_meta_data, de_payload);
+						  gfx[0].gfx_meta_data) +
+		offsetof(struct v9_gfx_meta_data, de_payload);
 		de_payload_gpu_addr =
-			amdgpu_mes_ctx_get_offs_gpu_addr(ring, offset);
+		amdgpu_mes_ctx_get_offs_gpu_addr(ring, offset);
 		de_payload_cpu_addr =
-			amdgpu_mes_ctx_get_offs_cpu_addr(ring, offset);
+		amdgpu_mes_ctx_get_offs_cpu_addr(ring, offset);
 
 		offset = offsetof(struct amdgpu_mes_ctx_meta_data,
-				  gfx[0].gds_backup) +
-			offsetof(struct v9_gfx_meta_data, de_payload);
+						  gfx[0].gds_backup) +
+		offsetof(struct v9_gfx_meta_data, de_payload);
 		gds_addr = amdgpu_mes_ctx_get_offs_gpu_addr(ring, offset);
 	} else {
 		offset = offsetof(struct v9_gfx_meta_data, de_payload);
@@ -5785,8 +5430,8 @@ static void gfx_v9_0_ring_emit_de_meta(s
 		de_payload_cpu_addr = adev->virt.csa_cpu_addr + offset;
 
 		gds_addr = ALIGN(amdgpu_csa_vaddr(ring->adev) +
-				 AMDGPU_CSA_SIZE - adev->gds.gds_size,
-				 PAGE_SIZE);
+		AMDGPU_CSA_SIZE - adev->gds.gds_size,
+		PAGE_SIZE);
 	}
 
 	if (usegds) {
@@ -5797,23 +5442,23 @@ static void gfx_v9_0_ring_emit_de_meta(s
 	cnt = (sizeof(de_payload) >> 2) + 4 - 2;
 	amdgpu_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, cnt));
 	amdgpu_ring_write(ring, (WRITE_DATA_ENGINE_SEL(1) |
-				 WRITE_DATA_DST_SEL(8) |
-				 WR_CONFIRM) |
-				 WRITE_DATA_CACHE_POLICY(0));
+	WRITE_DATA_DST_SEL(8) |
+	WR_CONFIRM) |
+	WRITE_DATA_CACHE_POLICY(0));
 	amdgpu_ring_write(ring, lower_32_bits(de_payload_gpu_addr));
 	amdgpu_ring_write(ring, upper_32_bits(de_payload_gpu_addr));
 
 	amdgpu_ring_ib_on_emit_de(ring);
 	if (resume)
 		amdgpu_ring_write_multiple(ring, de_payload_cpu_addr,
-					   sizeof(de_payload) >> 2);
-	else
-		amdgpu_ring_write_multiple(ring, (void *)&de_payload,
-					   sizeof(de_payload) >> 2);
+								   sizeof(de_payload) >> 2);
+		else
+			amdgpu_ring_write_multiple(ring, (void *)&de_payload,
+									   sizeof(de_payload) >> 2);
 }
 
 static void gfx_v9_0_ring_emit_frame_cntl(struct amdgpu_ring *ring, bool start,
-				   bool secure)
+										  bool secure)
 {
 	uint32_t v = secure ? FRAME_TMZ : 0;
 
@@ -5826,25 +5471,18 @@ static void gfx_v9_ring_emit_cntxcntl(st
 	uint32_t dw2 = 0;
 
 	gfx_v9_0_ring_emit_ce_meta(ring,
-				   (!amdgpu_sriov_vf(ring->adev) &&
-				   flags & AMDGPU_IB_PREEMPTED) ? true : false);
+							   (!amdgpu_sriov_vf(ring->adev) &&
+							   flags & AMDGPU_IB_PREEMPTED) ? true : false);
 
-	dw2 |= 0x80000000; /* set load_enable otherwise this package is just NOPs */
+	dw2 |= 0x80000000;
 	if (flags & AMDGPU_HAVE_CTX_SWITCH) {
-		/* set load_global_config & load_global_uconfig */
 		dw2 |= 0x8001;
-		/* set load_cs_sh_regs */
 		dw2 |= 0x01000000;
-		/* set load_per_context_state & load_gfx_sh_regs for GFX */
 		dw2 |= 0x10002;
 
-		/* set load_ce_ram if preamble presented */
 		if (AMDGPU_PREAMBLE_IB_PRESENT & flags)
 			dw2 |= 0x10000000;
 	} else {
-		/* still load_ce_ram if this is the first time preamble presented
-		 * although there is no context switch happens.
-		 */
 		if (AMDGPU_PREAMBLE_IB_PRESENT_FIRST & flags)
 			dw2 |= 0x10000000;
 	}
@@ -5855,52 +5493,50 @@ static void gfx_v9_ring_emit_cntxcntl(st
 }
 
 static unsigned gfx_v9_0_ring_emit_init_cond_exec(struct amdgpu_ring *ring,
-						  uint64_t addr)
+												  uint64_t addr)
 {
 	unsigned ret;
 	amdgpu_ring_write(ring, PACKET3(PACKET3_COND_EXEC, 3));
 	amdgpu_ring_write(ring, lower_32_bits(addr));
 	amdgpu_ring_write(ring, upper_32_bits(addr));
-	/* discard following DWs if *cond_exec_gpu_addr==0 */
 	amdgpu_ring_write(ring, 0);
 	ret = ring->wptr & ring->buf_mask;
-	/* patch dummy value later */
 	amdgpu_ring_write(ring, 0);
 	return ret;
 }
 
 static void gfx_v9_0_ring_emit_rreg(struct amdgpu_ring *ring, uint32_t reg,
-				    uint32_t reg_val_offs)
+									uint32_t reg_val_offs)
 {
 	struct amdgpu_device *adev = ring->adev;
 
 	amdgpu_ring_write(ring, PACKET3(PACKET3_COPY_DATA, 4));
-	amdgpu_ring_write(ring, 0 |	/* src: register*/
-				(5 << 8) |	/* dst: memory */
-				(1 << 20));	/* write confirm */
+	amdgpu_ring_write(ring, 0 |
+	(5 << 8) |
+	(1 << 20));
 	amdgpu_ring_write(ring, reg);
 	amdgpu_ring_write(ring, 0);
 	amdgpu_ring_write(ring, lower_32_bits(adev->wb.gpu_addr +
-				reg_val_offs * 4));
+	reg_val_offs * 4));
 	amdgpu_ring_write(ring, upper_32_bits(adev->wb.gpu_addr +
-				reg_val_offs * 4));
+	reg_val_offs * 4));
 }
 
 static void gfx_v9_0_ring_emit_wreg(struct amdgpu_ring *ring, uint32_t reg,
-				    uint32_t val)
+									uint32_t val)
 {
 	uint32_t cmd = 0;
 
 	switch (ring->funcs->type) {
-	case AMDGPU_RING_TYPE_GFX:
-		cmd = WRITE_DATA_ENGINE_SEL(1) | WR_CONFIRM;
-		break;
-	case AMDGPU_RING_TYPE_KIQ:
-		cmd = (1 << 16); /* no inc addr */
-		break;
-	default:
-		cmd = WR_CONFIRM;
-		break;
+		case AMDGPU_RING_TYPE_GFX:
+			cmd = WRITE_DATA_ENGINE_SEL(1) | WR_CONFIRM;
+			break;
+		case AMDGPU_RING_TYPE_KIQ:
+			cmd = (1 << 16);
+			break;
+		default:
+			cmd = WR_CONFIRM;
+			break;
 	}
 	amdgpu_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, 3));
 	amdgpu_ring_write(ring, cmd);
@@ -5910,26 +5546,26 @@ static void gfx_v9_0_ring_emit_wreg(stru
 }
 
 static void gfx_v9_0_ring_emit_reg_wait(struct amdgpu_ring *ring, uint32_t reg,
-					uint32_t val, uint32_t mask)
+										uint32_t val, uint32_t mask)
 {
 	gfx_v9_0_wait_reg_mem(ring, 0, 0, 0, reg, 0, val, mask, 0x20);
 }
 
 static void gfx_v9_0_ring_emit_reg_write_reg_wait(struct amdgpu_ring *ring,
-						  uint32_t reg0, uint32_t reg1,
-						  uint32_t ref, uint32_t mask)
+												  uint32_t reg0, uint32_t reg1,
+												  uint32_t ref, uint32_t mask)
 {
 	int usepfp = (ring->funcs->type == AMDGPU_RING_TYPE_GFX);
 	struct amdgpu_device *adev = ring->adev;
 	bool fw_version_ok = (ring->funcs->type == AMDGPU_RING_TYPE_GFX) ?
-		adev->gfx.me_fw_write_wait : adev->gfx.mec_fw_write_wait;
+	adev->gfx.me_fw_write_wait : adev->gfx.mec_fw_write_wait;
 
 	if (fw_version_ok)
 		gfx_v9_0_wait_reg_mem(ring, usepfp, 0, 1, reg0, reg1,
-				      ref, mask, 0x20);
-	else
-		amdgpu_ring_emit_reg_write_reg_wait_helper(ring, reg0, reg1,
-							   ref, mask);
+							  ref, mask, 0x20);
+		else
+			amdgpu_ring_emit_reg_write_reg_wait_helper(ring, reg0, reg1,
+													   ref, mask);
 }
 
 static void gfx_v9_0_ring_soft_recovery(struct amdgpu_ring *ring, unsigned vmid)
@@ -5947,49 +5583,43 @@ static void gfx_v9_0_ring_soft_recovery(
 }
 
 static void gfx_v9_0_set_gfx_eop_interrupt_state(struct amdgpu_device *adev,
-						 enum amdgpu_interrupt_state state)
+												 enum amdgpu_interrupt_state state)
 {
 	switch (state) {
-	case AMDGPU_IRQ_STATE_DISABLE:
-	case AMDGPU_IRQ_STATE_ENABLE:
-		WREG32_FIELD15(GC, 0, CP_INT_CNTL_RING0,
-			       TIME_STAMP_INT_ENABLE,
-			       state == AMDGPU_IRQ_STATE_ENABLE ? 1 : 0);
-		break;
-	default:
-		break;
+		case AMDGPU_IRQ_STATE_DISABLE:
+		case AMDGPU_IRQ_STATE_ENABLE:
+			WREG32_FIELD15(GC, 0, CP_INT_CNTL_RING0,
+						   TIME_STAMP_INT_ENABLE,
+				  state == AMDGPU_IRQ_STATE_ENABLE ? 1 : 0);
+			break;
+		default:
+			break;
 	}
 }
 
 static void gfx_v9_0_set_compute_eop_interrupt_state(struct amdgpu_device *adev,
-						     int me, int pipe,
-						     enum amdgpu_interrupt_state state)
+													 int me, int pipe,
+													 enum amdgpu_interrupt_state state)
 {
 	u32 mec_int_cntl, mec_int_cntl_reg;
 
-	/*
-	 * amdgpu controls only the first MEC. That's why this function only
-	 * handles the setting of interrupts for this specific MEC. All other
-	 * pipes' interrupts are set by amdkfd.
-	 */
-
 	if (me == 1) {
 		switch (pipe) {
-		case 0:
-			mec_int_cntl_reg = SOC15_REG_OFFSET(GC, 0, mmCP_ME1_PIPE0_INT_CNTL);
-			break;
-		case 1:
-			mec_int_cntl_reg = SOC15_REG_OFFSET(GC, 0, mmCP_ME1_PIPE1_INT_CNTL);
-			break;
-		case 2:
-			mec_int_cntl_reg = SOC15_REG_OFFSET(GC, 0, mmCP_ME1_PIPE2_INT_CNTL);
-			break;
-		case 3:
-			mec_int_cntl_reg = SOC15_REG_OFFSET(GC, 0, mmCP_ME1_PIPE3_INT_CNTL);
-			break;
-		default:
-			DRM_DEBUG("invalid pipe %d\n", pipe);
-			return;
+			case 0:
+				mec_int_cntl_reg = SOC15_REG_OFFSET(GC, 0, mmCP_ME1_PIPE0_INT_CNTL);
+				break;
+			case 1:
+				mec_int_cntl_reg = SOC15_REG_OFFSET(GC, 0, mmCP_ME1_PIPE1_INT_CNTL);
+				break;
+			case 2:
+				mec_int_cntl_reg = SOC15_REG_OFFSET(GC, 0, mmCP_ME1_PIPE2_INT_CNTL);
+				break;
+			case 3:
+				mec_int_cntl_reg = SOC15_REG_OFFSET(GC, 0, mmCP_ME1_PIPE3_INT_CNTL);
+				break;
+			default:
+				DRM_DEBUG("invalid pipe %d\n", pipe);
+				return;
 		}
 	} else {
 		DRM_DEBUG("invalid me %d\n", me);
@@ -5997,172 +5627,165 @@ static void gfx_v9_0_set_compute_eop_int
 	}
 
 	switch (state) {
-	case AMDGPU_IRQ_STATE_DISABLE:
-		mec_int_cntl = RREG32_SOC15_IP(GC,mec_int_cntl_reg);
-		mec_int_cntl = REG_SET_FIELD(mec_int_cntl, CP_ME1_PIPE0_INT_CNTL,
-					     TIME_STAMP_INT_ENABLE, 0);
-		WREG32_SOC15_IP(GC, mec_int_cntl_reg, mec_int_cntl);
-		break;
-	case AMDGPU_IRQ_STATE_ENABLE:
-		mec_int_cntl = RREG32_SOC15_IP(GC, mec_int_cntl_reg);
-		mec_int_cntl = REG_SET_FIELD(mec_int_cntl, CP_ME1_PIPE0_INT_CNTL,
-					     TIME_STAMP_INT_ENABLE, 1);
-		WREG32_SOC15_IP(GC, mec_int_cntl_reg, mec_int_cntl);
-		break;
-	default:
-		break;
+		case AMDGPU_IRQ_STATE_DISABLE:
+			mec_int_cntl = RREG32_SOC15_IP(GC,mec_int_cntl_reg);
+			mec_int_cntl = REG_SET_FIELD(mec_int_cntl, CP_ME1_PIPE0_INT_CNTL,
+										 TIME_STAMP_INT_ENABLE, 0);
+			WREG32_SOC15_IP(GC, mec_int_cntl_reg, mec_int_cntl);
+			break;
+		case AMDGPU_IRQ_STATE_ENABLE:
+			mec_int_cntl = RREG32_SOC15_IP(GC, mec_int_cntl_reg);
+			mec_int_cntl = REG_SET_FIELD(mec_int_cntl, CP_ME1_PIPE0_INT_CNTL,
+										 TIME_STAMP_INT_ENABLE, 1);
+			WREG32_SOC15_IP(GC, mec_int_cntl_reg, mec_int_cntl);
+			break;
+		default:
+			break;
 	}
 }
 
 static u32 gfx_v9_0_get_cpc_int_cntl(struct amdgpu_device *adev,
-				     int me, int pipe)
+									 int me, int pipe)
 {
-	/*
-	 * amdgpu controls only the first MEC. That's why this function only
-	 * handles the setting of interrupts for this specific MEC. All other
-	 * pipes' interrupts are set by amdkfd.
-	 */
 	if (me != 1)
 		return 0;
 
 	switch (pipe) {
-	case 0:
-		return SOC15_REG_OFFSET(GC, 0, mmCP_ME1_PIPE0_INT_CNTL);
-	case 1:
-		return SOC15_REG_OFFSET(GC, 0, mmCP_ME1_PIPE1_INT_CNTL);
-	case 2:
-		return SOC15_REG_OFFSET(GC, 0, mmCP_ME1_PIPE2_INT_CNTL);
-	case 3:
-		return SOC15_REG_OFFSET(GC, 0, mmCP_ME1_PIPE3_INT_CNTL);
-	default:
-		return 0;
+		case 0:
+			return SOC15_REG_OFFSET(GC, 0, mmCP_ME1_PIPE0_INT_CNTL);
+		case 1:
+			return SOC15_REG_OFFSET(GC, 0, mmCP_ME1_PIPE1_INT_CNTL);
+		case 2:
+			return SOC15_REG_OFFSET(GC, 0, mmCP_ME1_PIPE2_INT_CNTL);
+		case 3:
+			return SOC15_REG_OFFSET(GC, 0, mmCP_ME1_PIPE3_INT_CNTL);
+		default:
+			return 0;
 	}
 }
 
 static int gfx_v9_0_set_priv_reg_fault_state(struct amdgpu_device *adev,
-					     struct amdgpu_irq_src *source,
-					     unsigned type,
-					     enum amdgpu_interrupt_state state)
+											 struct amdgpu_irq_src *source,
+											 unsigned type,
+											 enum amdgpu_interrupt_state state)
 {
 	u32 cp_int_cntl_reg, cp_int_cntl;
 	int i, j;
 
 	switch (state) {
-	case AMDGPU_IRQ_STATE_DISABLE:
-	case AMDGPU_IRQ_STATE_ENABLE:
-		WREG32_FIELD15(GC, 0, CP_INT_CNTL_RING0,
-			       PRIV_REG_INT_ENABLE,
-			       state == AMDGPU_IRQ_STATE_ENABLE ? 1 : 0);
-		for (i = 0; i < adev->gfx.mec.num_mec; i++) {
-			for (j = 0; j < adev->gfx.mec.num_pipe_per_mec; j++) {
-				/* MECs start at 1 */
-				cp_int_cntl_reg = gfx_v9_0_get_cpc_int_cntl(adev, i + 1, j);
-
-				if (cp_int_cntl_reg) {
-					cp_int_cntl = RREG32_SOC15_IP(GC, cp_int_cntl_reg);
-					cp_int_cntl = REG_SET_FIELD(cp_int_cntl, CP_ME1_PIPE0_INT_CNTL,
-								    PRIV_REG_INT_ENABLE,
-								    state == AMDGPU_IRQ_STATE_ENABLE ? 1 : 0);
-					WREG32_SOC15_IP(GC, cp_int_cntl_reg, cp_int_cntl);
+		case AMDGPU_IRQ_STATE_DISABLE:
+		case AMDGPU_IRQ_STATE_ENABLE:
+			WREG32_FIELD15(GC, 0, CP_INT_CNTL_RING0,
+						   PRIV_REG_INT_ENABLE,
+				  state == AMDGPU_IRQ_STATE_ENABLE ? 1 : 0);
+			for (i = 0; i < adev->gfx.mec.num_mec; i++) {
+				for (j = 0; j < adev->gfx.mec.num_pipe_per_mec; j++) {
+					cp_int_cntl_reg = gfx_v9_0_get_cpc_int_cntl(adev, i + 1, j);
+
+					if (cp_int_cntl_reg) {
+						cp_int_cntl = RREG32_SOC15_IP(GC, cp_int_cntl_reg);
+						cp_int_cntl = REG_SET_FIELD(cp_int_cntl, CP_ME1_PIPE0_INT_CNTL,
+													PRIV_REG_INT_ENABLE,
+								  state == AMDGPU_IRQ_STATE_ENABLE ? 1 : 0);
+						WREG32_SOC15_IP(GC, cp_int_cntl_reg, cp_int_cntl);
+					}
 				}
 			}
-		}
-		break;
-	default:
-		break;
+			break;
+		default:
+			break;
 	}
 
 	return 0;
 }
 
 static int gfx_v9_0_set_bad_op_fault_state(struct amdgpu_device *adev,
-					   struct amdgpu_irq_src *source,
-					   unsigned type,
-					   enum amdgpu_interrupt_state state)
+										   struct amdgpu_irq_src *source,
+										   unsigned type,
+										   enum amdgpu_interrupt_state state)
 {
 	u32 cp_int_cntl_reg, cp_int_cntl;
 	int i, j;
 
 	switch (state) {
-	case AMDGPU_IRQ_STATE_DISABLE:
-	case AMDGPU_IRQ_STATE_ENABLE:
-		WREG32_FIELD15(GC, 0, CP_INT_CNTL_RING0,
-			       OPCODE_ERROR_INT_ENABLE,
-			       state == AMDGPU_IRQ_STATE_ENABLE ? 1 : 0);
-		for (i = 0; i < adev->gfx.mec.num_mec; i++) {
-			for (j = 0; j < adev->gfx.mec.num_pipe_per_mec; j++) {
-				/* MECs start at 1 */
-				cp_int_cntl_reg = gfx_v9_0_get_cpc_int_cntl(adev, i + 1, j);
-
-				if (cp_int_cntl_reg) {
-					cp_int_cntl = RREG32_SOC15_IP(GC, cp_int_cntl_reg);
-					cp_int_cntl = REG_SET_FIELD(cp_int_cntl, CP_ME1_PIPE0_INT_CNTL,
-								    OPCODE_ERROR_INT_ENABLE,
-								    state == AMDGPU_IRQ_STATE_ENABLE ? 1 : 0);
-					WREG32_SOC15_IP(GC, cp_int_cntl_reg, cp_int_cntl);
+		case AMDGPU_IRQ_STATE_DISABLE:
+		case AMDGPU_IRQ_STATE_ENABLE:
+			WREG32_FIELD15(GC, 0, CP_INT_CNTL_RING0,
+						   OPCODE_ERROR_INT_ENABLE,
+				  state == AMDGPU_IRQ_STATE_ENABLE ? 1 : 0);
+			for (i = 0; i < adev->gfx.mec.num_mec; i++) {
+				for (j = 0; j < adev->gfx.mec.num_pipe_per_mec; j++) {
+					cp_int_cntl_reg = gfx_v9_0_get_cpc_int_cntl(adev, i + 1, j);
+
+					if (cp_int_cntl_reg) {
+						cp_int_cntl = RREG32_SOC15_IP(GC, cp_int_cntl_reg);
+						cp_int_cntl = REG_SET_FIELD(cp_int_cntl, CP_ME1_PIPE0_INT_CNTL,
+													OPCODE_ERROR_INT_ENABLE,
+								  state == AMDGPU_IRQ_STATE_ENABLE ? 1 : 0);
+						WREG32_SOC15_IP(GC, cp_int_cntl_reg, cp_int_cntl);
+					}
 				}
 			}
-		}
-		break;
-	default:
-		break;
+			break;
+		default:
+			break;
 	}
 
 	return 0;
 }
 
 static int gfx_v9_0_set_priv_inst_fault_state(struct amdgpu_device *adev,
-					      struct amdgpu_irq_src *source,
-					      unsigned type,
-					      enum amdgpu_interrupt_state state)
+											  struct amdgpu_irq_src *source,
+											  unsigned type,
+											  enum amdgpu_interrupt_state state)
 {
 	switch (state) {
-	case AMDGPU_IRQ_STATE_DISABLE:
-	case AMDGPU_IRQ_STATE_ENABLE:
-		WREG32_FIELD15(GC, 0, CP_INT_CNTL_RING0,
-			       PRIV_INSTR_INT_ENABLE,
-			       state == AMDGPU_IRQ_STATE_ENABLE ? 1 : 0);
-		break;
-	default:
-		break;
+		case AMDGPU_IRQ_STATE_DISABLE:
+		case AMDGPU_IRQ_STATE_ENABLE:
+			WREG32_FIELD15(GC, 0, CP_INT_CNTL_RING0,
+						   PRIV_INSTR_INT_ENABLE,
+				  state == AMDGPU_IRQ_STATE_ENABLE ? 1 : 0);
+			break;
+		default:
+			break;
 	}
 
 	return 0;
 }
 
 #define ENABLE_ECC_ON_ME_PIPE(me, pipe)				\
-	WREG32_FIELD15(GC, 0, CP_ME##me##_PIPE##pipe##_INT_CNTL,\
-			CP_ECC_ERROR_INT_ENABLE, 1)
+WREG32_FIELD15(GC, 0, CP_ME##me##_PIPE##pipe##_INT_CNTL,\
+CP_ECC_ERROR_INT_ENABLE, 1)
 
 #define DISABLE_ECC_ON_ME_PIPE(me, pipe)			\
-	WREG32_FIELD15(GC, 0, CP_ME##me##_PIPE##pipe##_INT_CNTL,\
-			CP_ECC_ERROR_INT_ENABLE, 0)
+WREG32_FIELD15(GC, 0, CP_ME##me##_PIPE##pipe##_INT_CNTL,\
+CP_ECC_ERROR_INT_ENABLE, 0)
 
 static int gfx_v9_0_set_cp_ecc_error_state(struct amdgpu_device *adev,
-					      struct amdgpu_irq_src *source,
-					      unsigned type,
-					      enum amdgpu_interrupt_state state)
+										   struct amdgpu_irq_src *source,
+										   unsigned type,
+										   enum amdgpu_interrupt_state state)
 {
 	switch (state) {
-	case AMDGPU_IRQ_STATE_DISABLE:
-		WREG32_FIELD15(GC, 0, CP_INT_CNTL_RING0,
-				CP_ECC_ERROR_INT_ENABLE, 0);
-		DISABLE_ECC_ON_ME_PIPE(1, 0);
-		DISABLE_ECC_ON_ME_PIPE(1, 1);
-		DISABLE_ECC_ON_ME_PIPE(1, 2);
-		DISABLE_ECC_ON_ME_PIPE(1, 3);
-		break;
+		case AMDGPU_IRQ_STATE_DISABLE:
+			WREG32_FIELD15(GC, 0, CP_INT_CNTL_RING0,
+						   CP_ECC_ERROR_INT_ENABLE, 0);
+			DISABLE_ECC_ON_ME_PIPE(1, 0);
+			DISABLE_ECC_ON_ME_PIPE(1, 1);
+			DISABLE_ECC_ON_ME_PIPE(1, 2);
+			DISABLE_ECC_ON_ME_PIPE(1, 3);
+			break;
 
-	case AMDGPU_IRQ_STATE_ENABLE:
-		WREG32_FIELD15(GC, 0, CP_INT_CNTL_RING0,
-				CP_ECC_ERROR_INT_ENABLE, 1);
-		ENABLE_ECC_ON_ME_PIPE(1, 0);
-		ENABLE_ECC_ON_ME_PIPE(1, 1);
-		ENABLE_ECC_ON_ME_PIPE(1, 2);
-		ENABLE_ECC_ON_ME_PIPE(1, 3);
-		break;
-	default:
-		break;
+		case AMDGPU_IRQ_STATE_ENABLE:
+			WREG32_FIELD15(GC, 0, CP_INT_CNTL_RING0,
+						   CP_ECC_ERROR_INT_ENABLE, 1);
+			ENABLE_ECC_ON_ME_PIPE(1, 0);
+			ENABLE_ECC_ON_ME_PIPE(1, 1);
+			ENABLE_ECC_ON_ME_PIPE(1, 2);
+			ENABLE_ECC_ON_ME_PIPE(1, 3);
+			break;
+		default:
+			break;
 	}
 
 	return 0;
@@ -6170,47 +5793,47 @@ static int gfx_v9_0_set_cp_ecc_error_sta
 
 
 static int gfx_v9_0_set_eop_interrupt_state(struct amdgpu_device *adev,
-					    struct amdgpu_irq_src *src,
-					    unsigned type,
-					    enum amdgpu_interrupt_state state)
+											struct amdgpu_irq_src *src,
+											unsigned type,
+											enum amdgpu_interrupt_state state)
 {
 	switch (type) {
-	case AMDGPU_CP_IRQ_GFX_ME0_PIPE0_EOP:
-		gfx_v9_0_set_gfx_eop_interrupt_state(adev, state);
-		break;
-	case AMDGPU_CP_IRQ_COMPUTE_MEC1_PIPE0_EOP:
-		gfx_v9_0_set_compute_eop_interrupt_state(adev, 1, 0, state);
-		break;
-	case AMDGPU_CP_IRQ_COMPUTE_MEC1_PIPE1_EOP:
-		gfx_v9_0_set_compute_eop_interrupt_state(adev, 1, 1, state);
-		break;
-	case AMDGPU_CP_IRQ_COMPUTE_MEC1_PIPE2_EOP:
-		gfx_v9_0_set_compute_eop_interrupt_state(adev, 1, 2, state);
-		break;
-	case AMDGPU_CP_IRQ_COMPUTE_MEC1_PIPE3_EOP:
-		gfx_v9_0_set_compute_eop_interrupt_state(adev, 1, 3, state);
-		break;
-	case AMDGPU_CP_IRQ_COMPUTE_MEC2_PIPE0_EOP:
-		gfx_v9_0_set_compute_eop_interrupt_state(adev, 2, 0, state);
-		break;
-	case AMDGPU_CP_IRQ_COMPUTE_MEC2_PIPE1_EOP:
-		gfx_v9_0_set_compute_eop_interrupt_state(adev, 2, 1, state);
-		break;
-	case AMDGPU_CP_IRQ_COMPUTE_MEC2_PIPE2_EOP:
-		gfx_v9_0_set_compute_eop_interrupt_state(adev, 2, 2, state);
-		break;
-	case AMDGPU_CP_IRQ_COMPUTE_MEC2_PIPE3_EOP:
-		gfx_v9_0_set_compute_eop_interrupt_state(adev, 2, 3, state);
-		break;
-	default:
-		break;
+		case AMDGPU_CP_IRQ_GFX_ME0_PIPE0_EOP:
+			gfx_v9_0_set_gfx_eop_interrupt_state(adev, state);
+			break;
+		case AMDGPU_CP_IRQ_COMPUTE_MEC1_PIPE0_EOP:
+			gfx_v9_0_set_compute_eop_interrupt_state(adev, 1, 0, state);
+			break;
+		case AMDGPU_CP_IRQ_COMPUTE_MEC1_PIPE1_EOP:
+			gfx_v9_0_set_compute_eop_interrupt_state(adev, 1, 1, state);
+			break;
+		case AMDGPU_CP_IRQ_COMPUTE_MEC1_PIPE2_EOP:
+			gfx_v9_0_set_compute_eop_interrupt_state(adev, 1, 2, state);
+			break;
+		case AMDGPU_CP_IRQ_COMPUTE_MEC1_PIPE3_EOP:
+			gfx_v9_0_set_compute_eop_interrupt_state(adev, 1, 3, state);
+			break;
+		case AMDGPU_CP_IRQ_COMPUTE_MEC2_PIPE0_EOP:
+			gfx_v9_0_set_compute_eop_interrupt_state(adev, 2, 0, state);
+			break;
+		case AMDGPU_CP_IRQ_COMPUTE_MEC2_PIPE1_EOP:
+			gfx_v9_0_set_compute_eop_interrupt_state(adev, 2, 1, state);
+			break;
+		case AMDGPU_CP_IRQ_COMPUTE_MEC2_PIPE2_EOP:
+			gfx_v9_0_set_compute_eop_interrupt_state(adev, 2, 2, state);
+			break;
+		case AMDGPU_CP_IRQ_COMPUTE_MEC2_PIPE3_EOP:
+			gfx_v9_0_set_compute_eop_interrupt_state(adev, 2, 3, state);
+			break;
+		default:
+			break;
 	}
 	return 0;
 }
 
 static int gfx_v9_0_eop_irq(struct amdgpu_device *adev,
-			    struct amdgpu_irq_src *source,
-			    struct amdgpu_iv_entry *entry)
+							struct amdgpu_irq_src *source,
+							struct amdgpu_iv_entry *entry)
 {
 	int i;
 	u8 me_id, pipe_id, queue_id;
@@ -6222,34 +5845,30 @@ static int gfx_v9_0_eop_irq(struct amdgp
 	queue_id = (entry->ring_id & 0x70) >> 4;
 
 	switch (me_id) {
-	case 0:
-		if (adev->gfx.num_gfx_rings) {
-			if (!adev->gfx.mcbp) {
-				amdgpu_fence_process(&adev->gfx.gfx_ring[0]);
-			} else if (!amdgpu_mcbp_handle_trailing_fence_irq(&adev->gfx.muxer)) {
-				/* Fence signals are handled on the software rings*/
-				for (i = 0; i < GFX9_NUM_SW_GFX_RINGS; i++)
-					amdgpu_fence_process(&adev->gfx.sw_gfx_ring[i]);
+		case 0:
+			if (adev->gfx.num_gfx_rings) {
+				if (!adev->gfx.mcbp) {
+					amdgpu_fence_process(&adev->gfx.gfx_ring[0]);
+				} else if (!amdgpu_mcbp_handle_trailing_fence_irq(&adev->gfx.muxer)) {
+					for (i = 0; i < GFX9_NUM_SW_GFX_RINGS; i++)
+						amdgpu_fence_process(&adev->gfx.sw_gfx_ring[i]);
+				}
 			}
-		}
-		break;
-	case 1:
-	case 2:
-		for (i = 0; i < adev->gfx.num_compute_rings; i++) {
-			ring = &adev->gfx.compute_ring[i];
-			/* Per-queue interrupt is supported for MEC starting from VI.
-			  * The interrupt can only be enabled/disabled per pipe instead of per queue.
-			  */
-			if ((ring->me == me_id) && (ring->pipe == pipe_id) && (ring->queue == queue_id))
-				amdgpu_fence_process(ring);
-		}
-		break;
+			break;
+		case 1:
+		case 2:
+			for (i = 0; i < adev->gfx.num_compute_rings; i++) {
+				ring = &adev->gfx.compute_ring[i];
+				if ((ring->me == me_id) && (ring->pipe == pipe_id) && (ring->queue == queue_id))
+					amdgpu_fence_process(ring);
+			}
+			break;
 	}
 	return 0;
 }
 
 static void gfx_v9_0_fault(struct amdgpu_device *adev,
-			   struct amdgpu_iv_entry *entry)
+						   struct amdgpu_iv_entry *entry)
 {
 	u8 me_id, pipe_id, queue_id;
 	struct amdgpu_ring *ring;
@@ -6260,24 +5879,24 @@ static void gfx_v9_0_fault(struct amdgpu
 	queue_id = (entry->ring_id & 0x70) >> 4;
 
 	switch (me_id) {
-	case 0:
-		drm_sched_fault(&adev->gfx.gfx_ring[0].sched);
-		break;
-	case 1:
-	case 2:
-		for (i = 0; i < adev->gfx.num_compute_rings; i++) {
-			ring = &adev->gfx.compute_ring[i];
-			if (ring->me == me_id && ring->pipe == pipe_id &&
-			    ring->queue == queue_id)
-				drm_sched_fault(&ring->sched);
-		}
-		break;
+		case 0:
+			drm_sched_fault(&adev->gfx.gfx_ring[0].sched);
+			break;
+		case 1:
+		case 2:
+			for (i = 0; i < adev->gfx.num_compute_rings; i++) {
+				ring = &adev->gfx.compute_ring[i];
+				if (ring->me == me_id && ring->pipe == pipe_id &&
+					ring->queue == queue_id)
+					drm_sched_fault(&ring->sched);
+			}
+			break;
 	}
 }
 
 static int gfx_v9_0_priv_reg_irq(struct amdgpu_device *adev,
-				 struct amdgpu_irq_src *source,
-				 struct amdgpu_iv_entry *entry)
+								 struct amdgpu_irq_src *source,
+								 struct amdgpu_iv_entry *entry)
 {
 	DRM_ERROR("Illegal register access in command stream\n");
 	gfx_v9_0_fault(adev, entry);
@@ -6285,8 +5904,8 @@ static int gfx_v9_0_priv_reg_irq(struct
 }
 
 static int gfx_v9_0_bad_op_irq(struct amdgpu_device *adev,
-			       struct amdgpu_irq_src *source,
-			       struct amdgpu_iv_entry *entry)
+							   struct amdgpu_irq_src *source,
+							   struct amdgpu_iv_entry *entry)
 {
 	DRM_ERROR("Illegal opcode in command stream\n");
 	gfx_v9_0_fault(adev, entry);
@@ -6294,8 +5913,8 @@ static int gfx_v9_0_bad_op_irq(struct am
 }
 
 static int gfx_v9_0_priv_inst_irq(struct amdgpu_device *adev,
-				  struct amdgpu_irq_src *source,
-				  struct amdgpu_iv_entry *entry)
+								  struct amdgpu_irq_src *source,
+								  struct amdgpu_iv_entry *entry)
 {
 	DRM_ERROR("Illegal instruction in command stream\n");
 	gfx_v9_0_fault(adev, entry);
@@ -6305,447 +5924,447 @@ static int gfx_v9_0_priv_inst_irq(struct
 
 static const struct soc15_ras_field_entry gfx_v9_0_ras_fields[] = {
 	{ "CPC_SCRATCH", SOC15_REG_ENTRY(GC, 0, mmCPC_EDC_SCRATCH_CNT),
-	  SOC15_REG_FIELD(CPC_EDC_SCRATCH_CNT, SEC_COUNT),
-	  SOC15_REG_FIELD(CPC_EDC_SCRATCH_CNT, DED_COUNT)
+		SOC15_REG_FIELD(CPC_EDC_SCRATCH_CNT, SEC_COUNT),
+		SOC15_REG_FIELD(CPC_EDC_SCRATCH_CNT, DED_COUNT)
 	},
 	{ "CPC_UCODE", SOC15_REG_ENTRY(GC, 0, mmCPC_EDC_UCODE_CNT),
-	  SOC15_REG_FIELD(CPC_EDC_UCODE_CNT, SEC_COUNT),
-	  SOC15_REG_FIELD(CPC_EDC_UCODE_CNT, DED_COUNT)
+		SOC15_REG_FIELD(CPC_EDC_UCODE_CNT, SEC_COUNT),
+		SOC15_REG_FIELD(CPC_EDC_UCODE_CNT, DED_COUNT)
 	},
 	{ "CPF_ROQ_ME1", SOC15_REG_ENTRY(GC, 0, mmCPF_EDC_ROQ_CNT),
-	  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, COUNT_ME1),
-	  0, 0
+		SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, COUNT_ME1),
+		0, 0
 	},
 	{ "CPF_ROQ_ME2", SOC15_REG_ENTRY(GC, 0, mmCPF_EDC_ROQ_CNT),
-	  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, COUNT_ME2),
-	  0, 0
+		SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, COUNT_ME2),
+		0, 0
 	},
 	{ "CPF_TAG", SOC15_REG_ENTRY(GC, 0, mmCPF_EDC_TAG_CNT),
-	  SOC15_REG_FIELD(CPF_EDC_TAG_CNT, SEC_COUNT),
-	  SOC15_REG_FIELD(CPF_EDC_TAG_CNT, DED_COUNT)
+		SOC15_REG_FIELD(CPF_EDC_TAG_CNT, SEC_COUNT),
+		SOC15_REG_FIELD(CPF_EDC_TAG_CNT, DED_COUNT)
 	},
 	{ "CPG_DMA_ROQ", SOC15_REG_ENTRY(GC, 0, mmCPG_EDC_DMA_CNT),
-	  SOC15_REG_FIELD(CPG_EDC_DMA_CNT, ROQ_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(CPG_EDC_DMA_CNT, ROQ_COUNT),
+		0, 0
 	},
 	{ "CPG_DMA_TAG", SOC15_REG_ENTRY(GC, 0, mmCPG_EDC_DMA_CNT),
-	  SOC15_REG_FIELD(CPG_EDC_DMA_CNT, TAG_SEC_COUNT),
-	  SOC15_REG_FIELD(CPG_EDC_DMA_CNT, TAG_DED_COUNT)
+		SOC15_REG_FIELD(CPG_EDC_DMA_CNT, TAG_SEC_COUNT),
+		SOC15_REG_FIELD(CPG_EDC_DMA_CNT, TAG_DED_COUNT)
 	},
 	{ "CPG_TAG", SOC15_REG_ENTRY(GC, 0, mmCPG_EDC_TAG_CNT),
-	  SOC15_REG_FIELD(CPG_EDC_TAG_CNT, SEC_COUNT),
-	  SOC15_REG_FIELD(CPG_EDC_TAG_CNT, DED_COUNT)
+		SOC15_REG_FIELD(CPG_EDC_TAG_CNT, SEC_COUNT),
+		SOC15_REG_FIELD(CPG_EDC_TAG_CNT, DED_COUNT)
 	},
 	{ "DC_CSINVOC", SOC15_REG_ENTRY(GC, 0, mmDC_EDC_CSINVOC_CNT),
-	  SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, COUNT_ME1),
-	  0, 0
+		SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, COUNT_ME1),
+		0, 0
 	},
 	{ "DC_RESTORE", SOC15_REG_ENTRY(GC, 0, mmDC_EDC_RESTORE_CNT),
-	  SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, COUNT_ME1),
-	  0, 0
+		SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, COUNT_ME1),
+		0, 0
 	},
 	{ "DC_STATE", SOC15_REG_ENTRY(GC, 0, mmDC_EDC_STATE_CNT),
-	  SOC15_REG_FIELD(DC_EDC_STATE_CNT, COUNT_ME1),
-	  0, 0
+		SOC15_REG_FIELD(DC_EDC_STATE_CNT, COUNT_ME1),
+		0, 0
 	},
 	{ "GDS_MEM", SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_CNT),
-	  SOC15_REG_FIELD(GDS_EDC_CNT, GDS_MEM_SEC),
-	  SOC15_REG_FIELD(GDS_EDC_CNT, GDS_MEM_DED)
+		SOC15_REG_FIELD(GDS_EDC_CNT, GDS_MEM_SEC),
+		SOC15_REG_FIELD(GDS_EDC_CNT, GDS_MEM_DED)
 	},
 	{ "GDS_INPUT_QUEUE", SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_CNT),
-	  SOC15_REG_FIELD(GDS_EDC_CNT, GDS_INPUT_QUEUE_SED),
-	  0, 0
+		SOC15_REG_FIELD(GDS_EDC_CNT, GDS_INPUT_QUEUE_SED),
+		0, 0
 	},
 	{ "GDS_ME0_CS_PIPE_MEM", SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_OA_PHY_CNT),
-	  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, ME0_CS_PIPE_MEM_SEC),
-	  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, ME0_CS_PIPE_MEM_DED)
+		SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, ME0_CS_PIPE_MEM_SEC),
+		SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, ME0_CS_PIPE_MEM_DED)
 	},
 	{ "GDS_OA_PHY_PHY_CMD_RAM_MEM",
-	  SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_OA_PHY_CNT),
-	  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_CMD_RAM_MEM_SEC),
-	  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_CMD_RAM_MEM_DED)
+		SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_OA_PHY_CNT),
+		SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_CMD_RAM_MEM_SEC),
+		SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_CMD_RAM_MEM_DED)
 	},
 	{ "GDS_OA_PHY_PHY_DATA_RAM_MEM",
-	  SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_OA_PHY_CNT),
-	  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_DATA_RAM_MEM_SED),
-	  0, 0
+		SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_OA_PHY_CNT),
+		SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_DATA_RAM_MEM_SED),
+		0, 0
 	},
 	{ "GDS_OA_PIPE_ME1_PIPE0_PIPE_MEM",
-	  SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_OA_PIPE_CNT),
-	  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE0_PIPE_MEM_SEC),
-	  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE0_PIPE_MEM_DED)
+		SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_OA_PIPE_CNT),
+		SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE0_PIPE_MEM_SEC),
+		SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE0_PIPE_MEM_DED)
 	},
 	{ "GDS_OA_PIPE_ME1_PIPE1_PIPE_MEM",
-	  SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_OA_PIPE_CNT),
-	  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE1_PIPE_MEM_SEC),
-	  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE1_PIPE_MEM_DED)
+		SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_OA_PIPE_CNT),
+		SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE1_PIPE_MEM_SEC),
+		SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE1_PIPE_MEM_DED)
 	},
 	{ "GDS_OA_PIPE_ME1_PIPE2_PIPE_MEM",
-	  SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_OA_PIPE_CNT),
-	  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE2_PIPE_MEM_SEC),
-	  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE2_PIPE_MEM_DED)
+		SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_OA_PIPE_CNT),
+		SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE2_PIPE_MEM_SEC),
+		SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE2_PIPE_MEM_DED)
 	},
 	{ "GDS_OA_PIPE_ME1_PIPE3_PIPE_MEM",
-	  SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_OA_PIPE_CNT),
-	  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE3_PIPE_MEM_SEC),
-	  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE3_PIPE_MEM_DED)
+		SOC15_REG_ENTRY(GC, 0, mmGDS_EDC_OA_PIPE_CNT),
+		SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE3_PIPE_MEM_SEC),
+		SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE3_PIPE_MEM_DED)
 	},
 	{ "SPI_SR_MEM", SOC15_REG_ENTRY(GC, 0, mmSPI_EDC_CNT),
-	  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_SR_MEM_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(SPI_EDC_CNT, SPI_SR_MEM_SED_COUNT),
+		0, 0
 	},
 	{ "TA_FS_DFIFO", SOC15_REG_ENTRY(GC, 0, mmTA_EDC_CNT),
-	  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_DFIFO_SEC_COUNT),
-	  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_DFIFO_DED_COUNT)
+		SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_DFIFO_SEC_COUNT),
+		SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_DFIFO_DED_COUNT)
 	},
 	{ "TA_FS_AFIFO", SOC15_REG_ENTRY(GC, 0, mmTA_EDC_CNT),
-	  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_SED_COUNT),
+		0, 0
 	},
 	{ "TA_FL_LFIFO", SOC15_REG_ENTRY(GC, 0, mmTA_EDC_CNT),
-	  SOC15_REG_FIELD(TA_EDC_CNT, TA_FL_LFIFO_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TA_EDC_CNT, TA_FL_LFIFO_SED_COUNT),
+		0, 0
 	},
 	{ "TA_FX_LFIFO", SOC15_REG_ENTRY(GC, 0, mmTA_EDC_CNT),
-	  SOC15_REG_FIELD(TA_EDC_CNT, TA_FX_LFIFO_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TA_EDC_CNT, TA_FX_LFIFO_SED_COUNT),
+		0, 0
 	},
 	{ "TA_FS_CFIFO", SOC15_REG_ENTRY(GC, 0, mmTA_EDC_CNT),
-	  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_CFIFO_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_CFIFO_SED_COUNT),
+		0, 0
 	},
 	{ "TCA_HOLE_FIFO", SOC15_REG_ENTRY(GC, 0, mmTCA_EDC_CNT),
-	  SOC15_REG_FIELD(TCA_EDC_CNT, HOLE_FIFO_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TCA_EDC_CNT, HOLE_FIFO_SED_COUNT),
+		0, 0
 	},
 	{ "TCA_REQ_FIFO", SOC15_REG_ENTRY(GC, 0, mmTCA_EDC_CNT),
-	  SOC15_REG_FIELD(TCA_EDC_CNT, REQ_FIFO_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TCA_EDC_CNT, REQ_FIFO_SED_COUNT),
+		0, 0
 	},
 	{ "TCC_CACHE_DATA", SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT),
-	  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DATA_SEC_COUNT),
-	  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DATA_DED_COUNT)
+		SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DATA_SEC_COUNT),
+		SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DATA_DED_COUNT)
 	},
 	{ "TCC_CACHE_DIRTY", SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT),
-	  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DIRTY_SEC_COUNT),
-	  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DIRTY_DED_COUNT)
+		SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DIRTY_SEC_COUNT),
+		SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DIRTY_DED_COUNT)
 	},
 	{ "TCC_HIGH_RATE_TAG", SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT),
-	  SOC15_REG_FIELD(TCC_EDC_CNT, HIGH_RATE_TAG_SEC_COUNT),
-	  SOC15_REG_FIELD(TCC_EDC_CNT, HIGH_RATE_TAG_DED_COUNT)
+		SOC15_REG_FIELD(TCC_EDC_CNT, HIGH_RATE_TAG_SEC_COUNT),
+		SOC15_REG_FIELD(TCC_EDC_CNT, HIGH_RATE_TAG_DED_COUNT)
 	},
 	{ "TCC_LOW_RATE_TAG", SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT),
-	  SOC15_REG_FIELD(TCC_EDC_CNT, LOW_RATE_TAG_SEC_COUNT),
-	  SOC15_REG_FIELD(TCC_EDC_CNT, LOW_RATE_TAG_DED_COUNT)
+		SOC15_REG_FIELD(TCC_EDC_CNT, LOW_RATE_TAG_SEC_COUNT),
+		SOC15_REG_FIELD(TCC_EDC_CNT, LOW_RATE_TAG_DED_COUNT)
 	},
 	{ "TCC_SRC_FIFO", SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT),
-	  SOC15_REG_FIELD(TCC_EDC_CNT, SRC_FIFO_SEC_COUNT),
-	  SOC15_REG_FIELD(TCC_EDC_CNT, SRC_FIFO_DED_COUNT)
+		SOC15_REG_FIELD(TCC_EDC_CNT, SRC_FIFO_SEC_COUNT),
+		SOC15_REG_FIELD(TCC_EDC_CNT, SRC_FIFO_DED_COUNT)
 	},
 	{ "TCC_IN_USE_DEC", SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT),
-	  SOC15_REG_FIELD(TCC_EDC_CNT, IN_USE_DEC_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TCC_EDC_CNT, IN_USE_DEC_SED_COUNT),
+		0, 0
 	},
 	{ "TCC_IN_USE_TRANSFER", SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT),
-	  SOC15_REG_FIELD(TCC_EDC_CNT, IN_USE_TRANSFER_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TCC_EDC_CNT, IN_USE_TRANSFER_SED_COUNT),
+		0, 0
 	},
 	{ "TCC_LATENCY_FIFO", SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT),
-	  SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_SED_COUNT),
+		0, 0
 	},
 	{ "TCC_RETURN_DATA", SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT),
-	  SOC15_REG_FIELD(TCC_EDC_CNT, RETURN_DATA_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TCC_EDC_CNT, RETURN_DATA_SED_COUNT),
+		0, 0
 	},
 	{ "TCC_RETURN_CONTROL", SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT),
-	  SOC15_REG_FIELD(TCC_EDC_CNT, RETURN_CONTROL_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TCC_EDC_CNT, RETURN_CONTROL_SED_COUNT),
+		0, 0
 	},
 	{ "TCC_UC_ATOMIC_FIFO", SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT),
-	  SOC15_REG_FIELD(TCC_EDC_CNT, UC_ATOMIC_FIFO_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TCC_EDC_CNT, UC_ATOMIC_FIFO_SED_COUNT),
+		0, 0
 	},
 	{ "TCC_WRITE_RETURN", SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT2),
-	  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_RETURN_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_RETURN_SED_COUNT),
+		0, 0
 	},
 	{ "TCC_WRITE_CACHE_READ", SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT2),
-	  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_CACHE_READ_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_CACHE_READ_SED_COUNT),
+		0, 0
 	},
 	{ "TCC_SRC_FIFO_NEXT_RAM", SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT2),
-	  SOC15_REG_FIELD(TCC_EDC_CNT2, SRC_FIFO_NEXT_RAM_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TCC_EDC_CNT2, SRC_FIFO_NEXT_RAM_SED_COUNT),
+		0, 0
 	},
 	{ "TCC_LATENCY_FIFO_NEXT_RAM", SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT2),
-	  SOC15_REG_FIELD(TCC_EDC_CNT2, LATENCY_FIFO_NEXT_RAM_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TCC_EDC_CNT2, LATENCY_FIFO_NEXT_RAM_SED_COUNT),
+		0, 0
 	},
 	{ "TCC_CACHE_TAG_PROBE_FIFO", SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT2),
-	  SOC15_REG_FIELD(TCC_EDC_CNT2, CACHE_TAG_PROBE_FIFO_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TCC_EDC_CNT2, CACHE_TAG_PROBE_FIFO_SED_COUNT),
+		0, 0
 	},
 	{ "TCC_WRRET_TAG_WRITE_RETURN", SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT2),
-	  SOC15_REG_FIELD(TCC_EDC_CNT2, WRRET_TAG_WRITE_RETURN_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TCC_EDC_CNT2, WRRET_TAG_WRITE_RETURN_SED_COUNT),
+		0, 0
 	},
 	{ "TCC_ATOMIC_RETURN_BUFFER", SOC15_REG_ENTRY(GC, 0, mmTCC_EDC_CNT2),
-	  SOC15_REG_FIELD(TCC_EDC_CNT2, ATOMIC_RETURN_BUFFER_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TCC_EDC_CNT2, ATOMIC_RETURN_BUFFER_SED_COUNT),
+		0, 0
 	},
 	{ "TCI_WRITE_RAM", SOC15_REG_ENTRY(GC, 0, mmTCI_EDC_CNT),
-	  SOC15_REG_FIELD(TCI_EDC_CNT, WRITE_RAM_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TCI_EDC_CNT, WRITE_RAM_SED_COUNT),
+		0, 0
 	},
 	{ "TCP_CACHE_RAM", SOC15_REG_ENTRY(GC, 0, mmTCP_EDC_CNT_NEW),
-	  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CACHE_RAM_SEC_COUNT),
-	  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CACHE_RAM_DED_COUNT)
+		SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CACHE_RAM_SEC_COUNT),
+		SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CACHE_RAM_DED_COUNT)
 	},
 	{ "TCP_LFIFO_RAM", SOC15_REG_ENTRY(GC, 0, mmTCP_EDC_CNT_NEW),
-	  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, LFIFO_RAM_SEC_COUNT),
-	  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, LFIFO_RAM_DED_COUNT)
+		SOC15_REG_FIELD(TCP_EDC_CNT_NEW, LFIFO_RAM_SEC_COUNT),
+		SOC15_REG_FIELD(TCP_EDC_CNT_NEW, LFIFO_RAM_DED_COUNT)
 	},
 	{ "TCP_CMD_FIFO", SOC15_REG_ENTRY(GC, 0, mmTCP_EDC_CNT_NEW),
-	  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CMD_FIFO_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CMD_FIFO_SED_COUNT),
+		0, 0
 	},
 	{ "TCP_VM_FIFO", SOC15_REG_ENTRY(GC, 0, mmTCP_EDC_CNT_NEW),
-	  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, VM_FIFO_SEC_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TCP_EDC_CNT_NEW, VM_FIFO_SEC_COUNT),
+		0, 0
 	},
 	{ "TCP_DB_RAM", SOC15_REG_ENTRY(GC, 0, mmTCP_EDC_CNT_NEW),
-	  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, DB_RAM_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TCP_EDC_CNT_NEW, DB_RAM_SED_COUNT),
+		0, 0
 	},
 	{ "TCP_UTCL1_LFIFO0", SOC15_REG_ENTRY(GC, 0, mmTCP_EDC_CNT_NEW),
-	  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO0_SEC_COUNT),
-	  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO0_DED_COUNT)
+		SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO0_SEC_COUNT),
+		SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO0_DED_COUNT)
 	},
 	{ "TCP_UTCL1_LFIFO1", SOC15_REG_ENTRY(GC, 0, mmTCP_EDC_CNT_NEW),
-	  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO1_SEC_COUNT),
-	  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO1_DED_COUNT)
+		SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO1_SEC_COUNT),
+		SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO1_DED_COUNT)
 	},
 	{ "TD_SS_FIFO_LO", SOC15_REG_ENTRY(GC, 0, mmTD_EDC_CNT),
-	  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_LO_SEC_COUNT),
-	  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_LO_DED_COUNT)
+		SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_LO_SEC_COUNT),
+		SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_LO_DED_COUNT)
 	},
 	{ "TD_SS_FIFO_HI", SOC15_REG_ENTRY(GC, 0, mmTD_EDC_CNT),
-	  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_HI_SEC_COUNT),
-	  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_HI_DED_COUNT)
+		SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_HI_SEC_COUNT),
+		SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_HI_DED_COUNT)
 	},
 	{ "TD_CS_FIFO", SOC15_REG_ENTRY(GC, 0, mmTD_EDC_CNT),
-	  SOC15_REG_FIELD(TD_EDC_CNT, CS_FIFO_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(TD_EDC_CNT, CS_FIFO_SED_COUNT),
+		0, 0
 	},
 	{ "SQ_LDS_D", SOC15_REG_ENTRY(GC, 0, mmSQ_EDC_CNT),
-	  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_D_SEC_COUNT),
-	  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_D_DED_COUNT)
+		SOC15_REG_FIELD(SQ_EDC_CNT, LDS_D_SEC_COUNT),
+		SOC15_REG_FIELD(SQ_EDC_CNT, LDS_D_DED_COUNT)
 	},
 	{ "SQ_LDS_I", SOC15_REG_ENTRY(GC, 0, mmSQ_EDC_CNT),
-	  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_I_SEC_COUNT),
-	  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_I_DED_COUNT)
+		SOC15_REG_FIELD(SQ_EDC_CNT, LDS_I_SEC_COUNT),
+		SOC15_REG_FIELD(SQ_EDC_CNT, LDS_I_DED_COUNT)
 	},
 	{ "SQ_SGPR", SOC15_REG_ENTRY(GC, 0, mmSQ_EDC_CNT),
-	  SOC15_REG_FIELD(SQ_EDC_CNT, SGPR_SEC_COUNT),
-	  SOC15_REG_FIELD(SQ_EDC_CNT, SGPR_DED_COUNT)
+		SOC15_REG_FIELD(SQ_EDC_CNT, SGPR_SEC_COUNT),
+		SOC15_REG_FIELD(SQ_EDC_CNT, SGPR_DED_COUNT)
 	},
 	{ "SQ_VGPR0", SOC15_REG_ENTRY(GC, 0, mmSQ_EDC_CNT),
-	  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR0_SEC_COUNT),
-	  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR0_DED_COUNT)
+		SOC15_REG_FIELD(SQ_EDC_CNT, VGPR0_SEC_COUNT),
+		SOC15_REG_FIELD(SQ_EDC_CNT, VGPR0_DED_COUNT)
 	},
 	{ "SQ_VGPR1", SOC15_REG_ENTRY(GC, 0, mmSQ_EDC_CNT),
-	  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR1_SEC_COUNT),
-	  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR1_DED_COUNT)
+		SOC15_REG_FIELD(SQ_EDC_CNT, VGPR1_SEC_COUNT),
+		SOC15_REG_FIELD(SQ_EDC_CNT, VGPR1_DED_COUNT)
 	},
 	{ "SQ_VGPR2", SOC15_REG_ENTRY(GC, 0, mmSQ_EDC_CNT),
-	  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR2_SEC_COUNT),
-	  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR2_DED_COUNT)
+		SOC15_REG_FIELD(SQ_EDC_CNT, VGPR2_SEC_COUNT),
+		SOC15_REG_FIELD(SQ_EDC_CNT, VGPR2_DED_COUNT)
 	},
 	{ "SQ_VGPR3", SOC15_REG_ENTRY(GC, 0, mmSQ_EDC_CNT),
-	  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR3_SEC_COUNT),
-	  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR3_DED_COUNT)
+		SOC15_REG_FIELD(SQ_EDC_CNT, VGPR3_SEC_COUNT),
+		SOC15_REG_FIELD(SQ_EDC_CNT, VGPR3_DED_COUNT)
 	},
 	{ "SQC_DATA_CU0_WRITE_DATA_BUF", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT),
-	  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_WRITE_DATA_BUF_SEC_COUNT),
-	  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_WRITE_DATA_BUF_DED_COUNT)
+		SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_WRITE_DATA_BUF_SEC_COUNT),
+		SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_WRITE_DATA_BUF_DED_COUNT)
 	},
 	{ "SQC_DATA_CU0_UTCL1_LFIFO", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT),
-	  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_UTCL1_LFIFO_SEC_COUNT),
-	  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_UTCL1_LFIFO_DED_COUNT)
+		SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_UTCL1_LFIFO_SEC_COUNT),
+		SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_UTCL1_LFIFO_DED_COUNT)
 	},
 	{ "SQC_DATA_CU1_WRITE_DATA_BUF", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT),
-	  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_WRITE_DATA_BUF_SEC_COUNT),
-	  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_WRITE_DATA_BUF_DED_COUNT)
+		SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_WRITE_DATA_BUF_SEC_COUNT),
+		SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_WRITE_DATA_BUF_DED_COUNT)
 	},
 	{ "SQC_DATA_CU1_UTCL1_LFIFO", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT),
-	  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_UTCL1_LFIFO_SEC_COUNT),
-	  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_UTCL1_LFIFO_DED_COUNT)
+		SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_UTCL1_LFIFO_SEC_COUNT),
+		SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_UTCL1_LFIFO_DED_COUNT)
 	},
 	{ "SQC_DATA_CU2_WRITE_DATA_BUF", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT),
-	  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_WRITE_DATA_BUF_SEC_COUNT),
-	  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_WRITE_DATA_BUF_DED_COUNT)
+		SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_WRITE_DATA_BUF_SEC_COUNT),
+		SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_WRITE_DATA_BUF_DED_COUNT)
 	},
 	{ "SQC_DATA_CU2_UTCL1_LFIFO", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT),
-	  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_UTCL1_LFIFO_SEC_COUNT),
-	  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_UTCL1_LFIFO_DED_COUNT)
+		SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_UTCL1_LFIFO_SEC_COUNT),
+		SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_UTCL1_LFIFO_DED_COUNT)
 	},
 	{ "SQC_INST_BANKA_TAG_RAM", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT2),
-	  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_TAG_RAM_SEC_COUNT),
-	  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_TAG_RAM_DED_COUNT)
+		SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_TAG_RAM_SEC_COUNT),
+		SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_TAG_RAM_DED_COUNT)
 	},
 	{ "SQC_INST_BANKA_BANK_RAM", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT2),
-	  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_BANK_RAM_SEC_COUNT),
-	  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_BANK_RAM_DED_COUNT)
+		SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_BANK_RAM_SEC_COUNT),
+		SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_BANK_RAM_DED_COUNT)
 	},
 	{ "SQC_DATA_BANKA_TAG_RAM", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT2),
-	  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_TAG_RAM_SEC_COUNT),
-	  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_TAG_RAM_DED_COUNT)
+		SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_TAG_RAM_SEC_COUNT),
+		SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_TAG_RAM_DED_COUNT)
 	},
 	{ "SQC_DATA_BANKA_BANK_RAM", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT2),
-	  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_BANK_RAM_SEC_COUNT),
-	  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_BANK_RAM_DED_COUNT)
+		SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_BANK_RAM_SEC_COUNT),
+		SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_BANK_RAM_DED_COUNT)
 	},
 	{ "SQC_INST_BANKA_UTCL1_MISS_FIFO", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT2),
-	  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_UTCL1_MISS_FIFO_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_UTCL1_MISS_FIFO_SED_COUNT),
+		0, 0
 	},
 	{ "SQC_INST_BANKA_MISS_FIFO", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT2),
-	  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_MISS_FIFO_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_MISS_FIFO_SED_COUNT),
+		0, 0
 	},
 	{ "SQC_DATA_BANKA_HIT_FIFO", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT2),
-	  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_HIT_FIFO_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_HIT_FIFO_SED_COUNT),
+		0, 0
 	},
 	{ "SQC_DATA_BANKA_MISS_FIFO", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT2),
-	  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_MISS_FIFO_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_MISS_FIFO_SED_COUNT),
+		0, 0
 	},
 	{ "SQC_DATA_BANKA_DIRTY_BIT_RAM", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT2),
-	  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_DIRTY_BIT_RAM_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_DIRTY_BIT_RAM_SED_COUNT),
+		0, 0
 	},
 	{ "SQC_INST_UTCL1_LFIFO", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT2),
-	  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_UTCL1_LFIFO_SEC_COUNT),
-	  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_UTCL1_LFIFO_DED_COUNT)
+		SOC15_REG_FIELD(SQC_EDC_CNT2, INST_UTCL1_LFIFO_SEC_COUNT),
+		SOC15_REG_FIELD(SQC_EDC_CNT2, INST_UTCL1_LFIFO_DED_COUNT)
 	},
 	{ "SQC_INST_BANKB_TAG_RAM", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT3),
-	  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_TAG_RAM_SEC_COUNT),
-	  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_TAG_RAM_DED_COUNT)
+		SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_TAG_RAM_SEC_COUNT),
+		SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_TAG_RAM_DED_COUNT)
 	},
 	{ "SQC_INST_BANKB_BANK_RAM", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT3),
-	  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_BANK_RAM_SEC_COUNT),
-	  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_BANK_RAM_DED_COUNT)
+		SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_BANK_RAM_SEC_COUNT),
+		SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_BANK_RAM_DED_COUNT)
 	},
 	{ "SQC_DATA_BANKB_TAG_RAM", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT3),
-	  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_TAG_RAM_SEC_COUNT),
-	  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_TAG_RAM_DED_COUNT)
+		SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_TAG_RAM_SEC_COUNT),
+		SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_TAG_RAM_DED_COUNT)
 	},
 	{ "SQC_DATA_BANKB_BANK_RAM", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT3),
-	  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_BANK_RAM_SEC_COUNT),
-	  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_BANK_RAM_DED_COUNT)
+		SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_BANK_RAM_SEC_COUNT),
+		SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_BANK_RAM_DED_COUNT)
 	},
 	{ "SQC_INST_BANKB_UTCL1_MISS_FIFO", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT3),
-	  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_UTCL1_MISS_FIFO_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_UTCL1_MISS_FIFO_SED_COUNT),
+		0, 0
 	},
 	{ "SQC_INST_BANKB_MISS_FIFO", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT3),
-	  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_MISS_FIFO_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_MISS_FIFO_SED_COUNT),
+		0, 0
 	},
 	{ "SQC_DATA_BANKB_HIT_FIFO", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT3),
-	  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_HIT_FIFO_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_HIT_FIFO_SED_COUNT),
+		0, 0
 	},
 	{ "SQC_DATA_BANKB_MISS_FIFO", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT3),
-	  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_MISS_FIFO_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_MISS_FIFO_SED_COUNT),
+		0, 0
 	},
 	{ "SQC_DATA_BANKB_DIRTY_BIT_RAM", SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT3),
-	  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_DIRTY_BIT_RAM_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_DIRTY_BIT_RAM_SED_COUNT),
+		0, 0
 	},
 	{ "EA_DRAMRD_CMDMEM", SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMRD_CMDMEM_SEC_COUNT),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMRD_CMDMEM_DED_COUNT)
+		SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMRD_CMDMEM_SEC_COUNT),
+		SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMRD_CMDMEM_DED_COUNT)
 	},
 	{ "EA_DRAMWR_CMDMEM", SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_CMDMEM_SEC_COUNT),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_CMDMEM_DED_COUNT)
+		SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_CMDMEM_SEC_COUNT),
+		SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_CMDMEM_DED_COUNT)
 	},
 	{ "EA_DRAMWR_DATAMEM", SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_DATAMEM_SEC_COUNT),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_DATAMEM_DED_COUNT)
+		SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_DATAMEM_SEC_COUNT),
+		SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_DATAMEM_DED_COUNT)
 	},
 	{ "EA_RRET_TAGMEM", SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT, RRET_TAGMEM_SEC_COUNT),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT, RRET_TAGMEM_DED_COUNT)
+		SOC15_REG_FIELD(GCEA_EDC_CNT, RRET_TAGMEM_SEC_COUNT),
+		SOC15_REG_FIELD(GCEA_EDC_CNT, RRET_TAGMEM_DED_COUNT)
 	},
 	{ "EA_WRET_TAGMEM", SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT, WRET_TAGMEM_SEC_COUNT),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT, WRET_TAGMEM_DED_COUNT)
+		SOC15_REG_FIELD(GCEA_EDC_CNT, WRET_TAGMEM_SEC_COUNT),
+		SOC15_REG_FIELD(GCEA_EDC_CNT, WRET_TAGMEM_DED_COUNT)
 	},
 	{ "EA_DRAMRD_PAGEMEM", SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMRD_PAGEMEM_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMRD_PAGEMEM_SED_COUNT),
+		0, 0
 	},
 	{ "EA_DRAMWR_PAGEMEM", SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_PAGEMEM_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_PAGEMEM_SED_COUNT),
+		0, 0
 	},
 	{ "EA_IORD_CMDMEM", SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT, IORD_CMDMEM_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(GCEA_EDC_CNT, IORD_CMDMEM_SED_COUNT),
+		0, 0
 	},
 	{ "EA_IOWR_CMDMEM", SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT, IOWR_CMDMEM_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(GCEA_EDC_CNT, IOWR_CMDMEM_SED_COUNT),
+		0, 0
 	},
 	{ "EA_IOWR_DATAMEM", SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT, IOWR_DATAMEM_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(GCEA_EDC_CNT, IOWR_DATAMEM_SED_COUNT),
+		0, 0
 	},
 	{ "GMIRD_CMDMEM", SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT2),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIRD_CMDMEM_SEC_COUNT),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIRD_CMDMEM_DED_COUNT)
+		SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIRD_CMDMEM_SEC_COUNT),
+		SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIRD_CMDMEM_DED_COUNT)
 	},
 	{ "GMIWR_CMDMEM", SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT2),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_CMDMEM_SEC_COUNT),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_CMDMEM_DED_COUNT)
+		SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_CMDMEM_SEC_COUNT),
+		SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_CMDMEM_DED_COUNT)
 	},
 	{ "GMIWR_DATAMEM", SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT2),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_DATAMEM_SEC_COUNT),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_DATAMEM_DED_COUNT)
+		SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_DATAMEM_SEC_COUNT),
+		SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_DATAMEM_DED_COUNT)
 	},
 	{ "GMIRD_PAGEMEM", SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT2),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIRD_PAGEMEM_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIRD_PAGEMEM_SED_COUNT),
+		0, 0
 	},
 	{ "GMIWR_PAGEMEM", SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT2),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_PAGEMEM_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_PAGEMEM_SED_COUNT),
+		0, 0
 	},
 	{ "MAM_D0MEM", SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT2),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D0MEM_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D0MEM_SED_COUNT),
+		0, 0
 	},
 	{ "MAM_D1MEM", SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT2),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D1MEM_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D1MEM_SED_COUNT),
+		0, 0
 	},
 	{ "MAM_D2MEM", SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT2),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D2MEM_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D2MEM_SED_COUNT),
+		0, 0
 	},
 	{ "MAM_D3MEM", SOC15_REG_ENTRY(GC, 0, mmGCEA_EDC_CNT2),
-	  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D3MEM_SED_COUNT),
-	  0, 0
+		SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D3MEM_SED_COUNT),
+		0, 0
 	}
 };
 
 static int gfx_v9_0_ras_error_inject(struct amdgpu_device *adev,
-				     void *inject_if, uint32_t instance_mask)
+									 void *inject_if, uint32_t instance_mask)
 {
 	struct ras_inject_if *info = (struct ras_inject_if *)inject_if;
 	int ret;
@@ -6761,33 +6380,33 @@ static int gfx_v9_0_ras_error_inject(str
 		return -EPERM;
 
 	if (!(ras_gfx_subblocks[info->head.sub_block_index].hw_supported_error_type &
-	      info->head.type)) {
+		info->head.type)) {
 		DRM_ERROR("GFX Subblock %s, hardware do not support type 0x%x\n",
-			ras_gfx_subblocks[info->head.sub_block_index].name,
+				  ras_gfx_subblocks[info->head.sub_block_index].name,
 			info->head.type);
 		return -EPERM;
-	}
+		}
 
-	if (!(ras_gfx_subblocks[info->head.sub_block_index].sw_supported_error_type &
-	      info->head.type)) {
-		DRM_ERROR("GFX Subblock %s, driver do not support type 0x%x\n",
-			ras_gfx_subblocks[info->head.sub_block_index].name,
-			info->head.type);
-		return -EPERM;
-	}
+		if (!(ras_gfx_subblocks[info->head.sub_block_index].sw_supported_error_type &
+			info->head.type)) {
+			DRM_ERROR("GFX Subblock %s, driver do not support type 0x%x\n",
+					  ras_gfx_subblocks[info->head.sub_block_index].name,
+			 info->head.type);
+			return -EPERM;
+			}
 
-	block_info.block_id = amdgpu_ras_block_to_ta(info->head.block);
-	block_info.sub_block_index =
+			block_info.block_id = amdgpu_ras_block_to_ta(info->head.block);
+		block_info.sub_block_index =
 		ras_gfx_subblocks[info->head.sub_block_index].ta_subblock;
-	block_info.inject_error_type = amdgpu_ras_error_to_ta(info->head.type);
-	block_info.address = info->address;
-	block_info.value = info->value;
+		block_info.inject_error_type = amdgpu_ras_error_to_ta(info->head.type);
+		block_info.address = info->address;
+		block_info.value = info->value;
+
+		mutex_lock(&adev->grbm_idx_mutex);
+		ret = psp_ras_trigger_error(&adev->psp, &block_info, instance_mask);
+		mutex_unlock(&adev->grbm_idx_mutex);
 
-	mutex_lock(&adev->grbm_idx_mutex);
-	ret = psp_ras_trigger_error(&adev->psp, &block_info, instance_mask);
-	mutex_unlock(&adev->grbm_idx_mutex);
-
-	return ret;
+		return ret;
 }
 
 static const char * const vml2_mems[] = {
@@ -6862,7 +6481,7 @@ static const char *atc_l2_cache_4k_mems[
 };
 
 static int gfx_v9_0_query_utc_edc_status(struct amdgpu_device *adev,
-					 struct ras_err_data *err_data)
+										 struct ras_err_data *err_data)
 {
 	uint32_t i, data;
 	uint32_t sec_count, ded_count;
@@ -6883,14 +6502,14 @@ static int gfx_v9_0_query_utc_edc_status
 		sec_count = REG_GET_FIELD(data, VM_L2_MEM_ECC_CNT, SEC_COUNT);
 		if (sec_count) {
 			dev_info(adev->dev, "Instance[%d]: SubBlock %s, "
-				"SEC %d\n", i, vml2_mems[i], sec_count);
+			"SEC %d\n", i, vml2_mems[i], sec_count);
 			err_data->ce_count += sec_count;
 		}
 
 		ded_count = REG_GET_FIELD(data, VM_L2_MEM_ECC_CNT, DED_COUNT);
 		if (ded_count) {
 			dev_info(adev->dev, "Instance[%d]: SubBlock %s, "
-				"DED %d\n", i, vml2_mems[i], ded_count);
+			"DED %d\n", i, vml2_mems[i], ded_count);
 			err_data->ue_count += ded_count;
 		}
 	}
@@ -6900,18 +6519,18 @@ static int gfx_v9_0_query_utc_edc_status
 		data = RREG32_SOC15(GC, 0, mmVM_L2_WALKER_MEM_ECC_CNT);
 
 		sec_count = REG_GET_FIELD(data, VM_L2_WALKER_MEM_ECC_CNT,
-						SEC_COUNT);
+								  SEC_COUNT);
 		if (sec_count) {
 			dev_info(adev->dev, "Instance[%d]: SubBlock %s, "
-				"SEC %d\n", i, vml2_walker_mems[i], sec_count);
+			"SEC %d\n", i, vml2_walker_mems[i], sec_count);
 			err_data->ce_count += sec_count;
 		}
 
 		ded_count = REG_GET_FIELD(data, VM_L2_WALKER_MEM_ECC_CNT,
-						DED_COUNT);
+								  DED_COUNT);
 		if (ded_count) {
 			dev_info(adev->dev, "Instance[%d]: SubBlock %s, "
-				"DED %d\n", i, vml2_walker_mems[i], ded_count);
+			"DED %d\n", i, vml2_walker_mems[i], ded_count);
 			err_data->ue_count += ded_count;
 		}
 	}
@@ -6923,8 +6542,8 @@ static int gfx_v9_0_query_utc_edc_status
 		sec_count = (data & 0x00006000L) >> 0xd;
 		if (sec_count) {
 			dev_info(adev->dev, "Instance[%d]: SubBlock %s, "
-				"SEC %d\n", i, atc_l2_cache_2m_mems[i],
-				sec_count);
+			"SEC %d\n", i, atc_l2_cache_2m_mems[i],
+			sec_count);
 			err_data->ce_count += sec_count;
 		}
 	}
@@ -6936,16 +6555,16 @@ static int gfx_v9_0_query_utc_edc_status
 		sec_count = (data & 0x00006000L) >> 0xd;
 		if (sec_count) {
 			dev_info(adev->dev, "Instance[%d]: SubBlock %s, "
-				"SEC %d\n", i, atc_l2_cache_4k_mems[i],
-				sec_count);
+			"SEC %d\n", i, atc_l2_cache_4k_mems[i],
+			sec_count);
 			err_data->ce_count += sec_count;
 		}
 
 		ded_count = (data & 0x00018000L) >> 0xf;
 		if (ded_count) {
 			dev_info(adev->dev, "Instance[%d]: SubBlock %s, "
-				"DED %d\n", i, atc_l2_cache_4k_mems[i],
-				ded_count);
+			"DED %d\n", i, atc_l2_cache_4k_mems[i],
+			ded_count);
 			err_data->ue_count += ded_count;
 		}
 	}
@@ -6959,9 +6578,9 @@ static int gfx_v9_0_query_utc_edc_status
 }
 
 static int gfx_v9_0_ras_error_count(struct amdgpu_device *adev,
-	const struct soc15_reg_entry *reg,
-	uint32_t se_id, uint32_t inst_id, uint32_t value,
-	uint32_t *sec_count, uint32_t *ded_count)
+									const struct soc15_reg_entry *reg,
+									uint32_t se_id, uint32_t inst_id, uint32_t value,
+									uint32_t *sec_count, uint32_t *ded_count)
 {
 	uint32_t i;
 	uint32_t sec_cnt, ded_cnt;
@@ -6973,26 +6592,26 @@ static int gfx_v9_0_ras_error_count(stru
 			continue;
 
 		sec_cnt = (value &
-				gfx_v9_0_ras_fields[i].sec_count_mask) >>
-				gfx_v9_0_ras_fields[i].sec_count_shift;
+		gfx_v9_0_ras_fields[i].sec_count_mask) >>
+		gfx_v9_0_ras_fields[i].sec_count_shift;
 		if (sec_cnt) {
 			dev_info(adev->dev, "GFX SubBlock %s, "
-				"Instance[%d][%d], SEC %d\n",
-				gfx_v9_0_ras_fields[i].name,
-				se_id, inst_id,
-				sec_cnt);
+			"Instance[%d][%d], SEC %d\n",
+			gfx_v9_0_ras_fields[i].name,
+			se_id, inst_id,
+			sec_cnt);
 			*sec_count += sec_cnt;
 		}
 
 		ded_cnt = (value &
-				gfx_v9_0_ras_fields[i].ded_count_mask) >>
-				gfx_v9_0_ras_fields[i].ded_count_shift;
+		gfx_v9_0_ras_fields[i].ded_count_mask) >>
+		gfx_v9_0_ras_fields[i].ded_count_shift;
 		if (ded_cnt) {
 			dev_info(adev->dev, "GFX SubBlock %s, "
-				"Instance[%d][%d], DED %d\n",
-				gfx_v9_0_ras_fields[i].name,
-				se_id, inst_id,
-				ded_cnt);
+			"Instance[%d][%d], DED %d\n",
+			gfx_v9_0_ras_fields[i].name,
+			se_id, inst_id,
+			ded_cnt);
 			*ded_count += ded_cnt;
 		}
 	}
@@ -7007,7 +6626,6 @@ static void gfx_v9_0_reset_ras_error_cou
 	if (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))
 		return;
 
-	/* read back registers to clear the counters */
 	mutex_lock(&adev->grbm_idx_mutex);
 	for (i = 0; i < ARRAY_SIZE(gfx_v9_0_edc_counter_regs); i++) {
 		for (j = 0; j < gfx_v9_0_edc_counter_regs[i].se_num; j++) {
@@ -7056,7 +6674,7 @@ static void gfx_v9_0_reset_ras_error_cou
 }
 
 static void gfx_v9_0_query_ras_error_count(struct amdgpu_device *adev,
-					  void *ras_error_status)
+										   void *ras_error_status)
 {
 	struct ras_err_data *err_data = (struct ras_err_data *)ras_error_status;
 	uint32_t sec_count = 0, ded_count = 0;
@@ -7076,12 +6694,12 @@ static void gfx_v9_0_query_ras_error_cou
 			for (k = 0; k < gfx_v9_0_edc_counter_regs[i].instance; k++) {
 				amdgpu_gfx_select_se_sh(adev, j, 0, k, 0);
 				reg_value =
-					RREG32(SOC15_REG_ENTRY_OFFSET(gfx_v9_0_edc_counter_regs[i]));
+				RREG32(SOC15_REG_ENTRY_OFFSET(gfx_v9_0_edc_counter_regs[i]));
 				if (reg_value)
 					gfx_v9_0_ras_error_count(adev,
-						&gfx_v9_0_edc_counter_regs[i],
-						j, k, reg_value,
-						&sec_count, &ded_count);
+											 &gfx_v9_0_edc_counter_regs[i],
+							  j, k, reg_value,
+							  &sec_count, &ded_count);
 			}
 		}
 	}
@@ -7098,48 +6716,46 @@ static void gfx_v9_0_query_ras_error_cou
 static void gfx_v9_0_emit_mem_sync(struct amdgpu_ring *ring)
 {
 	const unsigned int cp_coher_cntl =
-			PACKET3_ACQUIRE_MEM_CP_COHER_CNTL_SH_ICACHE_ACTION_ENA(1) |
-			PACKET3_ACQUIRE_MEM_CP_COHER_CNTL_SH_KCACHE_ACTION_ENA(1) |
-			PACKET3_ACQUIRE_MEM_CP_COHER_CNTL_TC_ACTION_ENA(1) |
-			PACKET3_ACQUIRE_MEM_CP_COHER_CNTL_TCL1_ACTION_ENA(1) |
-			PACKET3_ACQUIRE_MEM_CP_COHER_CNTL_TC_WB_ACTION_ENA(1);
+	PACKET3_ACQUIRE_MEM_CP_COHER_CNTL_SH_ICACHE_ACTION_ENA(1) |
+	PACKET3_ACQUIRE_MEM_CP_COHER_CNTL_SH_KCACHE_ACTION_ENA(1) |
+	PACKET3_ACQUIRE_MEM_CP_COHER_CNTL_TC_ACTION_ENA(1) |
+	PACKET3_ACQUIRE_MEM_CP_COHER_CNTL_TCL1_ACTION_ENA(1) |
+	PACKET3_ACQUIRE_MEM_CP_COHER_CNTL_TC_WB_ACTION_ENA(1);
 
-	/* ACQUIRE_MEM -make one or more surfaces valid for use by the subsequent operations */
 	amdgpu_ring_write(ring, PACKET3(PACKET3_ACQUIRE_MEM, 5));
-	amdgpu_ring_write(ring, cp_coher_cntl); /* CP_COHER_CNTL */
-	amdgpu_ring_write(ring, 0xffffffff);  /* CP_COHER_SIZE */
-	amdgpu_ring_write(ring, 0xffffff);  /* CP_COHER_SIZE_HI */
-	amdgpu_ring_write(ring, 0); /* CP_COHER_BASE */
-	amdgpu_ring_write(ring, 0);  /* CP_COHER_BASE_HI */
-	amdgpu_ring_write(ring, 0x0000000A); /* POLL_INTERVAL */
+	amdgpu_ring_write(ring, cp_coher_cntl);
+	amdgpu_ring_write(ring, 0xffffffff);
+	amdgpu_ring_write(ring, 0xffffff);
+	amdgpu_ring_write(ring, 0);
+	amdgpu_ring_write(ring, 0);
+	amdgpu_ring_write(ring, 0x0000000A);
 }
 
 static void gfx_v9_0_emit_wave_limit_cs(struct amdgpu_ring *ring,
-					uint32_t pipe, bool enable)
+										uint32_t pipe, bool enable)
 {
 	struct amdgpu_device *adev = ring->adev;
 	uint32_t val;
 	uint32_t wcl_cs_reg;
 
-	/* mmSPI_WCL_PIPE_PERCENT_CS[0-7]_DEFAULT values are same */
 	val = enable ? 0x1 : mmSPI_WCL_PIPE_PERCENT_CS0_DEFAULT;
 
 	switch (pipe) {
-	case 0:
-		wcl_cs_reg = SOC15_REG_OFFSET(GC, 0, mmSPI_WCL_PIPE_PERCENT_CS0);
-		break;
-	case 1:
-		wcl_cs_reg = SOC15_REG_OFFSET(GC, 0, mmSPI_WCL_PIPE_PERCENT_CS1);
-		break;
-	case 2:
-		wcl_cs_reg = SOC15_REG_OFFSET(GC, 0, mmSPI_WCL_PIPE_PERCENT_CS2);
-		break;
-	case 3:
-		wcl_cs_reg = SOC15_REG_OFFSET(GC, 0, mmSPI_WCL_PIPE_PERCENT_CS3);
-		break;
-	default:
-		DRM_DEBUG("invalid pipe %d\n", pipe);
-		return;
+		case 0:
+			wcl_cs_reg = SOC15_REG_OFFSET(GC, 0, mmSPI_WCL_PIPE_PERCENT_CS0);
+			break;
+		case 1:
+			wcl_cs_reg = SOC15_REG_OFFSET(GC, 0, mmSPI_WCL_PIPE_PERCENT_CS1);
+			break;
+		case 2:
+			wcl_cs_reg = SOC15_REG_OFFSET(GC, 0, mmSPI_WCL_PIPE_PERCENT_CS2);
+			break;
+		case 3:
+			wcl_cs_reg = SOC15_REG_OFFSET(GC, 0, mmSPI_WCL_PIPE_PERCENT_CS3);
+			break;
+		default:
+			DRM_DEBUG("invalid pipe %d\n", pipe);
+			return;
 	}
 
 	amdgpu_ring_emit_wreg(ring, wcl_cs_reg, val);
@@ -7152,20 +6768,11 @@ static void gfx_v9_0_emit_wave_limit(str
 	int i;
 
 
-	/* mmSPI_WCL_PIPE_PERCENT_GFX is 7 bit multiplier register to limit
-	 * number of gfx waves. Setting 5 bit will make sure gfx only gets
-	 * around 25% of gpu resources.
-	 */
 	val = enable ? 0x1f : mmSPI_WCL_PIPE_PERCENT_GFX_DEFAULT;
 	amdgpu_ring_emit_wreg(ring,
-			      SOC15_REG_OFFSET(GC, 0, mmSPI_WCL_PIPE_PERCENT_GFX),
-			      val);
+						  SOC15_REG_OFFSET(GC, 0, mmSPI_WCL_PIPE_PERCENT_GFX),
+						  val);
 
-	/* Restrict waves for normal/low priority compute queues as well
-	 * to get best QoS for high priority compute jobs.
-	 *
-	 * amdgpu controls only 1st ME(0-3 CS pipes).
-	 */
 	for (i = 0; i < adev->gfx.mec.num_pipe_per_mec; i++) {
 		if (i != ring->pipe)
 			gfx_v9_0_emit_wave_limit_cs(ring, i, enable);
@@ -7175,16 +6782,13 @@ static void gfx_v9_0_emit_wave_limit(str
 
 static void gfx_v9_ring_insert_nop(struct amdgpu_ring *ring, uint32_t num_nop)
 {
-	/* Header itself is a NOP packet */
 	if (num_nop == 1) {
 		amdgpu_ring_write(ring, ring->funcs->nop);
 		return;
 	}
 
-	/* Max HW optimization till 0x3ffe, followed by remaining one NOP at a time*/
 	amdgpu_ring_write(ring, PACKET3(PACKET3_NOP, min(num_nop - 2, 0x3ffe)));
 
-	/* Header is at index 0, followed by num_nops - 1 NOP packet's */
 	amdgpu_ring_insert_nop(ring, num_nop - 1);
 }
 
@@ -7212,7 +6816,7 @@ static int gfx_v9_0_reset_kgq(struct amd
 
 	tmp = REG_SET_FIELD(0, CP_VMID_RESET, RESET_REQUEST, 1 << vmid);
 	gfx_v9_0_ring_emit_wreg(kiq_ring,
-				 SOC15_REG_OFFSET(GC, 0, mmCP_VMID_RESET), tmp);
+							SOC15_REG_OFFSET(GC, 0, mmCP_VMID_RESET), tmp);
 	amdgpu_ring_commit(kiq_ring);
 
 	spin_unlock_irqrestore(&kiq->ring_lock, flags);
@@ -7224,17 +6828,17 @@ static int gfx_v9_0_reset_kgq(struct amd
 	if (amdgpu_ring_alloc(ring, 7 + 7 + 5))
 		return -ENOMEM;
 	gfx_v9_0_ring_emit_fence(ring, ring->fence_drv.gpu_addr,
-				 ring->fence_drv.sync_seq, AMDGPU_FENCE_FLAG_EXEC);
+							 ring->fence_drv.sync_seq, AMDGPU_FENCE_FLAG_EXEC);
 	gfx_v9_0_ring_emit_reg_wait(ring,
-				    SOC15_REG_OFFSET(GC, 0, mmCP_VMID_RESET), 0, 0xffff);
+								SOC15_REG_OFFSET(GC, 0, mmCP_VMID_RESET), 0, 0xffff);
 	gfx_v9_0_ring_emit_wreg(ring,
-				SOC15_REG_OFFSET(GC, 0, mmCP_VMID_RESET), 0);
+							SOC15_REG_OFFSET(GC, 0, mmCP_VMID_RESET), 0);
 
 	return amdgpu_ring_test_ring(ring);
 }
 
 static int gfx_v9_0_reset_kcq(struct amdgpu_ring *ring,
-			      unsigned int vmid)
+							  unsigned int vmid)
 {
 	struct amdgpu_device *adev = ring->adev;
 	struct amdgpu_kiq *kiq = &adev->gfx.kiq[0];
@@ -7256,7 +6860,7 @@ static int gfx_v9_0_reset_kcq(struct amd
 	}
 
 	kiq->pmf->kiq_unmap_queues(kiq_ring, ring, RESET_QUEUES,
-				   0, 0);
+							   0, 0);
 	amdgpu_ring_commit(kiq_ring);
 
 	spin_unlock_irqrestore(&kiq->ring_lock, flags);
@@ -7265,7 +6869,6 @@ static int gfx_v9_0_reset_kcq(struct amd
 	if (r)
 		return r;
 
-	/* make sure dequeue is complete*/
 	amdgpu_gfx_rlc_enter_safe_mode(adev, 0);
 	mutex_lock(&adev->srbm_mutex);
 	soc15_grbm_select(adev, ring->me, ring->pipe, ring->queue, 0, 0);
@@ -7315,20 +6918,20 @@ static void gfx_v9_ip_print(struct amdgp
 	if (!adev->gfx.ip_dump_core)
 		return;
 
-	for (i = 0; i < reg_count; i++)
+	for (i = 0; i < reg_count; i++) {
 		drm_printf(p, "%-50s \t 0x%08x\n",
-			   gc_reg_list_9[i].reg_name,
-			   adev->gfx.ip_dump_core[i]);
+				   gc_reg_list_9[i].reg_name,
+			 adev->gfx.ip_dump_core[i]);
+	}
 
-	/* print compute queue registers for all instances */
 	if (!adev->gfx.ip_dump_compute_queues)
 		return;
 
 	reg_count = ARRAY_SIZE(gc_cp_reg_list_9);
 	drm_printf(p, "\nnum_mec: %d num_pipe: %d num_queue: %d\n",
-		   adev->gfx.mec.num_mec,
-		   adev->gfx.mec.num_pipe_per_mec,
-		   adev->gfx.mec.num_queue_per_pipe);
+			   adev->gfx.mec.num_mec,
+			adev->gfx.mec.num_pipe_per_mec,
+			adev->gfx.mec.num_queue_per_pipe);
 
 	for (i = 0; i < adev->gfx.mec.num_mec; i++) {
 		for (j = 0; j < adev->gfx.mec.num_pipe_per_mec; j++) {
@@ -7336,14 +6939,13 @@ static void gfx_v9_ip_print(struct amdgp
 				drm_printf(p, "\nmec %d, pipe %d, queue %d\n", i, j, k);
 				for (reg = 0; reg < reg_count; reg++) {
 					drm_printf(p, "%-50s \t 0x%08x\n",
-						   gc_cp_reg_list_9[reg].reg_name,
-						   adev->gfx.ip_dump_compute_queues[index + reg]);
+							   gc_cp_reg_list_9[reg].reg_name,
+				adev->gfx.ip_dump_compute_queues[index + reg]);
 				}
 				index += reg_count;
 			}
 		}
 	}
-
 }
 
 static void gfx_v9_ip_dump(struct amdgpu_ip_block *ip_block)
@@ -7360,7 +6962,6 @@ static void gfx_v9_ip_dump(struct amdgpu
 		adev->gfx.ip_dump_core[i] = RREG32(SOC15_REG_ENTRY_OFFSET(gc_reg_list_9[i]));
 	amdgpu_gfx_off_ctrl(adev, true);
 
-	/* dump compute queue registers for all instances */
 	if (!adev->gfx.ip_dump_compute_queues)
 		return;
 
@@ -7370,13 +6971,12 @@ static void gfx_v9_ip_dump(struct amdgpu
 	for (i = 0; i < adev->gfx.mec.num_mec; i++) {
 		for (j = 0; j < adev->gfx.mec.num_pipe_per_mec; j++) {
 			for (k = 0; k < adev->gfx.mec.num_queue_per_pipe; k++) {
-				/* ME0 is for GFX so start from 1 for CP */
 				soc15_grbm_select(adev, 1 + i, j, k, 0, 0);
 
 				for (reg = 0; reg < reg_count; reg++) {
 					adev->gfx.ip_dump_compute_queues[index + reg] =
-						RREG32(SOC15_REG_ENTRY_OFFSET(
-							gc_cp_reg_list_9[reg]));
+					RREG32(SOC15_REG_ENTRY_OFFSET(
+						gc_cp_reg_list_9[reg]));
 				}
 				index += reg_count;
 			}
@@ -7390,23 +6990,18 @@ static void gfx_v9_ip_dump(struct amdgpu
 
 static void gfx_v9_0_ring_emit_cleaner_shader(struct amdgpu_ring *ring)
 {
-	/* Emit the cleaner shader */
 	amdgpu_ring_write(ring, PACKET3(PACKET3_RUN_CLEANER_SHADER, 0));
-	amdgpu_ring_write(ring, 0);  /* RESERVED field, programmed to zero */
+	amdgpu_ring_write(ring, 0);
 }
 
 static void gfx_v9_0_ring_begin_use_compute(struct amdgpu_ring *ring)
 {
 	struct amdgpu_device *adev = ring->adev;
 	struct amdgpu_ip_block *gfx_block =
-		amdgpu_device_ip_get_ip_block(adev, AMD_IP_BLOCK_TYPE_GFX);
+	amdgpu_device_ip_get_ip_block(adev, AMD_IP_BLOCK_TYPE_GFX);
 
 	amdgpu_gfx_enforce_isolation_ring_begin_use(ring);
 
-	/* Raven and PCO APUs seem to have stability issues
-	 * with compute and gfxoff and gfx pg.  Disable gfx pg during
-	 * submission and allow again afterwards.
-	 */
 	if (gfx_block && amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 1, 0))
 		gfx_v9_0_set_powergating_state(gfx_block, AMD_PG_STATE_UNGATE);
 }
@@ -7415,12 +7010,8 @@ static void gfx_v9_0_ring_end_use_comput
 {
 	struct amdgpu_device *adev = ring->adev;
 	struct amdgpu_ip_block *gfx_block =
-		amdgpu_device_ip_get_ip_block(adev, AMD_IP_BLOCK_TYPE_GFX);
+	amdgpu_device_ip_get_ip_block(adev, AMD_IP_BLOCK_TYPE_GFX);
 
-	/* Raven and PCO APUs seem to have stability issues
-	 * with compute and gfxoff and gfx pg.  Disable gfx pg during
-	 * submission and allow again afterwards.
-	 */
 	if (gfx_block && amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 1, 0))
 		gfx_v9_0_set_powergating_state(gfx_block, AMD_PG_STATE_GATE);
 
@@ -7456,29 +7047,27 @@ static const struct amdgpu_ring_funcs gf
 	.get_rptr = gfx_v9_0_ring_get_rptr_gfx,
 	.get_wptr = gfx_v9_0_ring_get_wptr_gfx,
 	.set_wptr = gfx_v9_0_ring_set_wptr_gfx,
-	.emit_frame_size = /* totally 242 maximum if 16 IBs */
-		5 +  /* COND_EXEC */
-		7 +  /* PIPELINE_SYNC */
-		SOC15_FLUSH_GPU_TLB_NUM_WREG * 5 +
-		SOC15_FLUSH_GPU_TLB_NUM_REG_WAIT * 7 +
-		2 + /* VM_FLUSH */
-		8 +  /* FENCE for VM_FLUSH */
-		20 + /* GDS switch */
-		4 + /* double SWITCH_BUFFER,
-		       the first COND_EXEC jump to the place just
-			   prior to this double SWITCH_BUFFER  */
-		5 + /* COND_EXEC */
-		7 +	 /*	HDP_flush */
-		4 +	 /*	VGT_flush */
-		14 + /*	CE_META */
-		31 + /*	DE_META */
-		3 + /* CNTX_CTRL */
-		5 + /* HDP_INVL */
-		8 + 8 + /* FENCE x2 */
-		2 + /* SWITCH_BUFFER */
-		7 + /* gfx_v9_0_emit_mem_sync */
-		2, /* gfx_v9_0_ring_emit_cleaner_shader */
-	.emit_ib_size =	4, /* gfx_v9_0_ring_emit_ib_gfx */
+	.emit_frame_size =
+	5 +
+	7 +
+	SOC15_FLUSH_GPU_TLB_NUM_WREG * 5 +
+	SOC15_FLUSH_GPU_TLB_NUM_REG_WAIT * 7 +
+	2 +
+	8 +
+	20 +
+	4 +
+	5 +
+	7 +
+	4 +
+	14 +
+	31 +
+	3 +
+	5 +
+	8 + 8 +
+	2 +
+	7 +
+	2,
+	.emit_ib_size =	4,
 	.emit_ib = gfx_v9_0_ring_emit_ib_gfx,
 	.emit_fence = gfx_v9_0_ring_emit_fence,
 	.emit_pipeline_sync = gfx_v9_0_ring_emit_pipeline_sync,
@@ -7513,30 +7102,27 @@ static const struct amdgpu_ring_funcs gf
 	.get_rptr = amdgpu_sw_ring_get_rptr_gfx,
 	.get_wptr = amdgpu_sw_ring_get_wptr_gfx,
 	.set_wptr = amdgpu_sw_ring_set_wptr_gfx,
-	.emit_frame_size = /* totally 242 maximum if 16 IBs */
-		5 +  /* COND_EXEC */
-		7 +  /* PIPELINE_SYNC */
-		SOC15_FLUSH_GPU_TLB_NUM_WREG * 5 +
-		SOC15_FLUSH_GPU_TLB_NUM_REG_WAIT * 7 +
-		2 + /* VM_FLUSH */
-		8 +  /* FENCE for VM_FLUSH */
-		20 + /* GDS switch */
-		4 + /* double SWITCH_BUFFER,
-		     * the first COND_EXEC jump to the place just
-		     * prior to this double SWITCH_BUFFER
-		     */
-		5 + /* COND_EXEC */
-		7 +	 /*	HDP_flush */
-		4 +	 /*	VGT_flush */
-		14 + /*	CE_META */
-		31 + /*	DE_META */
-		3 + /* CNTX_CTRL */
-		5 + /* HDP_INVL */
-		8 + 8 + /* FENCE x2 */
-		2 + /* SWITCH_BUFFER */
-		7 + /* gfx_v9_0_emit_mem_sync */
-		2, /* gfx_v9_0_ring_emit_cleaner_shader */
-	.emit_ib_size =	4, /* gfx_v9_0_ring_emit_ib_gfx */
+	.emit_frame_size =
+	5 +
+	7 +
+	SOC15_FLUSH_GPU_TLB_NUM_WREG * 5 +
+	SOC15_FLUSH_GPU_TLB_NUM_REG_WAIT * 7 +
+	2 +
+	8 +
+	20 +
+	4 +
+	5 +
+	7 +
+	4 +
+	14 +
+	31 +
+	3 +
+	5 +
+	8 + 8 +
+	2 +
+	7 +
+	2,
+	.emit_ib_size =	4,
 	.emit_ib = gfx_v9_0_ring_emit_ib_gfx,
 	.emit_fence = gfx_v9_0_ring_emit_fence,
 	.emit_pipeline_sync = gfx_v9_0_ring_emit_pipeline_sync,
@@ -7573,18 +7159,18 @@ static const struct amdgpu_ring_funcs gf
 	.get_wptr = gfx_v9_0_ring_get_wptr_compute,
 	.set_wptr = gfx_v9_0_ring_set_wptr_compute,
 	.emit_frame_size =
-		20 + /* gfx_v9_0_ring_emit_gds_switch */
-		7 + /* gfx_v9_0_ring_emit_hdp_flush */
-		5 + /* hdp invalidate */
-		7 + /* gfx_v9_0_ring_emit_pipeline_sync */
-		SOC15_FLUSH_GPU_TLB_NUM_WREG * 5 +
-		SOC15_FLUSH_GPU_TLB_NUM_REG_WAIT * 7 +
-		8 + 8 + 8 + /* gfx_v9_0_ring_emit_fence x3 for user fence, vm fence */
-		7 + /* gfx_v9_0_emit_mem_sync */
-		5 + /* gfx_v9_0_emit_wave_limit for updating mmSPI_WCL_PIPE_PERCENT_GFX register */
-		15 + /* for updating 3 mmSPI_WCL_PIPE_PERCENT_CS registers */
-		2, /* gfx_v9_0_ring_emit_cleaner_shader */
-	.emit_ib_size =	7, /* gfx_v9_0_ring_emit_ib_compute */
+	20 +
+	7 +
+	5 +
+	7 +
+	SOC15_FLUSH_GPU_TLB_NUM_WREG * 5 +
+	SOC15_FLUSH_GPU_TLB_NUM_REG_WAIT * 7 +
+	8 + 8 + 8 +
+	7 +
+	5 +
+	15 +
+	2,
+	.emit_ib_size =	7,
 	.emit_ib = gfx_v9_0_ring_emit_ib_compute,
 	.emit_fence = gfx_v9_0_ring_emit_fence,
 	.emit_pipeline_sync = gfx_v9_0_ring_emit_pipeline_sync,
@@ -7616,14 +7202,14 @@ static const struct amdgpu_ring_funcs gf
 	.get_wptr = gfx_v9_0_ring_get_wptr_compute,
 	.set_wptr = gfx_v9_0_ring_set_wptr_compute,
 	.emit_frame_size =
-		20 + /* gfx_v9_0_ring_emit_gds_switch */
-		7 + /* gfx_v9_0_ring_emit_hdp_flush */
-		5 + /* hdp invalidate */
-		7 + /* gfx_v9_0_ring_emit_pipeline_sync */
-		SOC15_FLUSH_GPU_TLB_NUM_WREG * 5 +
-		SOC15_FLUSH_GPU_TLB_NUM_REG_WAIT * 7 +
-		8 + 8 + 8, /* gfx_v9_0_ring_emit_fence_kiq x3 for user fence, vm fence */
-	.emit_ib_size =	7, /* gfx_v9_0_ring_emit_ib_compute */
+	20 +
+	7 +
+	5 +
+	7 +
+	SOC15_FLUSH_GPU_TLB_NUM_WREG * 5 +
+	SOC15_FLUSH_GPU_TLB_NUM_REG_WAIT * 7 +
+	8 + 8 + 8,
+	.emit_ib_size =	7,
 	.emit_fence = gfx_v9_0_ring_emit_fence_kiq,
 	.test_ring = gfx_v9_0_ring_test_ring,
 	.insert_nop = amdgpu_ring_insert_nop,
@@ -7692,79 +7278,73 @@ static void gfx_v9_0_set_irq_funcs(struc
 	adev->gfx.priv_inst_irq.num_types = 1;
 	adev->gfx.priv_inst_irq.funcs = &gfx_v9_0_priv_inst_irq_funcs;
 
-	adev->gfx.cp_ecc_error_irq.num_types = 2; /*C5 ECC error and C9 FUE error*/
+	adev->gfx.cp_ecc_error_irq.num_types = 2;
 	adev->gfx.cp_ecc_error_irq.funcs = &gfx_v9_0_cp_ecc_error_irq_funcs;
 }
 
 static void gfx_v9_0_set_rlc_funcs(struct amdgpu_device *adev)
 {
 	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
-	case IP_VERSION(9, 0, 1):
-	case IP_VERSION(9, 2, 1):
-	case IP_VERSION(9, 4, 0):
-	case IP_VERSION(9, 2, 2):
-	case IP_VERSION(9, 1, 0):
-	case IP_VERSION(9, 4, 1):
-	case IP_VERSION(9, 3, 0):
-	case IP_VERSION(9, 4, 2):
-		adev->gfx.rlc.funcs = &gfx_v9_0_rlc_funcs;
-		break;
-	default:
-		break;
+		case IP_VERSION(9, 0, 1):
+		case IP_VERSION(9, 2, 1):
+		case IP_VERSION(9, 4, 0):
+		case IP_VERSION(9, 2, 2):
+		case IP_VERSION(9, 1, 0):
+		case IP_VERSION(9, 4, 1):
+		case IP_VERSION(9, 3, 0):
+		case IP_VERSION(9, 4, 2):
+			adev->gfx.rlc.funcs = &gfx_v9_0_rlc_funcs;
+			break;
+		default:
+			break;
 	}
 }
 
 static void gfx_v9_0_set_gds_init(struct amdgpu_device *adev)
 {
-	/* init asci gds info */
 	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
-	case IP_VERSION(9, 0, 1):
-	case IP_VERSION(9, 2, 1):
-	case IP_VERSION(9, 4, 0):
-		adev->gds.gds_size = 0x10000;
-		break;
-	case IP_VERSION(9, 2, 2):
-	case IP_VERSION(9, 1, 0):
-	case IP_VERSION(9, 4, 1):
-		adev->gds.gds_size = 0x1000;
-		break;
-	case IP_VERSION(9, 4, 2):
-		/* aldebaran removed all the GDS internal memory,
-		 * only support GWS opcode in kernel, like barrier
-		 * semaphore.etc */
-		adev->gds.gds_size = 0;
-		break;
-	default:
-		adev->gds.gds_size = 0x10000;
-		break;
+		case IP_VERSION(9, 0, 1):
+		case IP_VERSION(9, 2, 1):
+		case IP_VERSION(9, 4, 0):
+			adev->gds.gds_size = 0x10000;
+			break;
+		case IP_VERSION(9, 2, 2):
+		case IP_VERSION(9, 1, 0):
+		case IP_VERSION(9, 4, 1):
+			adev->gds.gds_size = 0x1000;
+			break;
+		case IP_VERSION(9, 4, 2):
+			adev->gds.gds_size = 0;
+			break;
+		default:
+			adev->gds.gds_size = 0x10000;
+			break;
 	}
 
 	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
-	case IP_VERSION(9, 0, 1):
-	case IP_VERSION(9, 4, 0):
-		adev->gds.gds_compute_max_wave_id = 0x7ff;
-		break;
-	case IP_VERSION(9, 2, 1):
-		adev->gds.gds_compute_max_wave_id = 0x27f;
-		break;
-	case IP_VERSION(9, 2, 2):
-	case IP_VERSION(9, 1, 0):
-		if (adev->apu_flags & AMD_APU_IS_RAVEN2)
-			adev->gds.gds_compute_max_wave_id = 0x77; /* raven2 */
+		case IP_VERSION(9, 0, 1):
+		case IP_VERSION(9, 4, 0):
+			adev->gds.gds_compute_max_wave_id = 0x7ff;
+			break;
+		case IP_VERSION(9, 2, 1):
+			adev->gds.gds_compute_max_wave_id = 0x27f;
+			break;
+		case IP_VERSION(9, 2, 2):
+		case IP_VERSION(9, 1, 0):
+			if (adev->apu_flags & AMD_APU_IS_RAVEN2)
+				adev->gds.gds_compute_max_wave_id = 0x77;
 		else
-			adev->gds.gds_compute_max_wave_id = 0x15f; /* raven1 */
-		break;
-	case IP_VERSION(9, 4, 1):
-		adev->gds.gds_compute_max_wave_id = 0xfff;
-		break;
-	case IP_VERSION(9, 4, 2):
-		/* deprecated for Aldebaran, no usage at all */
-		adev->gds.gds_compute_max_wave_id = 0;
-		break;
-	default:
-		/* this really depends on the chip */
-		adev->gds.gds_compute_max_wave_id = 0x7ff;
+			adev->gds.gds_compute_max_wave_id = 0x15f;
 		break;
+		case IP_VERSION(9, 4, 1):
+			adev->gds.gds_compute_max_wave_id = 0xfff;
+			break;
+		case IP_VERSION(9, 4, 2):
+			adev->gds.gds_compute_max_wave_id = 0;
+			break;
+		default:
+			adev->gds.gds_compute_max_wave_id = 0x7ff;
+			break;
 	}
 
 	adev->gds.gws_size = 64;
@@ -7772,7 +7352,7 @@ static void gfx_v9_0_set_gds_init(struct
 }
 
 static void gfx_v9_0_set_user_cu_inactive_bitmap(struct amdgpu_device *adev,
-						 u32 bitmap)
+												 u32 bitmap)
 {
 	u32 data;
 
@@ -7801,7 +7381,7 @@ static u32 gfx_v9_0_get_cu_active_bitmap
 }
 
 static int gfx_v9_0_get_cu_info(struct amdgpu_device *adev,
-				 struct amdgpu_cu_info *cu_info)
+								struct amdgpu_cu_info *cu_info)
 {
 	int i, j, k, counter, active_cu_number = 0;
 	u32 mask, bitmap, ao_bitmap, ao_cu_mask = 0;
@@ -7810,16 +7390,13 @@ static int gfx_v9_0_get_cu_info(struct a
 	if (!adev || !cu_info)
 		return -EINVAL;
 
-	/*
-	 * 16 comes from bitmap array size 4*4, and it can cover all gfx9 ASICs
-	 */
 	if (adev->gfx.config.max_shader_engines *
 		adev->gfx.config.max_sh_per_se > 16)
 		return -EINVAL;
 
 	amdgpu_gfx_parse_disable_cu(disable_masks,
-				    adev->gfx.config.max_shader_engines,
-				    adev->gfx.config.max_sh_per_se);
+								adev->gfx.config.max_shader_engines,
+							 adev->gfx.config.max_sh_per_se);
 
 	mutex_lock(&adev->grbm_idx_mutex);
 	for (i = 0; i < adev->gfx.config.max_shader_engines; i++) {
@@ -7832,18 +7409,6 @@ static int gfx_v9_0_get_cu_info(struct a
 				adev, disable_masks[i * adev->gfx.config.max_sh_per_se + j]);
 			bitmap = gfx_v9_0_get_cu_active_bitmap(adev);
 
-			/*
-			 * The bitmap(and ao_cu_bitmap) in cu_info structure is
-			 * 4x4 size array, and it's usually suitable for Vega
-			 * ASICs which has 4*2 SE/SH layout.
-			 * But for Arcturus, SE/SH layout is changed to 8*1.
-			 * To mostly reduce the impact, we make it compatible
-			 * with current bitmap array as below:
-			 *    SE4,SH0 --> bitmap[0][1]
-			 *    SE5,SH0 --> bitmap[1][1]
-			 *    SE6,SH0 --> bitmap[2][1]
-			 *    SE7,SH0 --> bitmap[3][1]
-			 */
 			cu_info->bitmap[0][i % 4][j + i / 4] = bitmap;
 
 			for (k = 0; k < adev->gfx.config.max_cu_per_sh; k ++) {



--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c	2025-03-19 20:16:22.723193359 +0100
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c	2025-03-19 20:20:03.397460298 +0100
@@ -39,6 +39,9 @@
 #include "gfx_v9_0.h"
 #include "amdgpu_amdkfd_gfx_v9.h"
 #include <uapi/linux/kfd_ioctl.h>
+#ifdef CONFIG_X86
+#include <asm/processor.h>
+#endif
 
 enum hqd_dequeue_request_type {
 	NO_ACTION = 0,
@@ -47,8 +50,76 @@ enum hqd_dequeue_request_type {
 	SAVE_WAVES
 };
 
+/*
+ * Detect Intel Raptor Lake CPU for optimized waiting strategy
+ * Raptor Lake is identified by family 6, model 0xB7 (Raptor Lake S)
+ * or 0xBF (Raptor Lake P)
+ */
+#ifdef CONFIG_X86
+static bool is_raptor_lake_cpu(void)
+{
+	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL &&
+		boot_cpu_data.x86 == 6 &&
+		(boot_cpu_data.x86_model == 0xB7 || boot_cpu_data.x86_model == 0xBF))
+		return true;
+	return false;
+}
+#else
+static inline bool is_raptor_lake_cpu(void)
+{
+	return false;
+}
+#endif
+
+/**
+ * optimized_wait_for_gpu - Optimized waiting strategy for CPU-GPU synchronization
+ * @adev: amdgpu device
+ * @reg_addr: Register address to poll
+ * @mask: Mask to apply to register value
+ * @expected: Expected value after applying mask
+ * @timeout_ms: Timeout in milliseconds
+ *
+ * Uses a hybrid approach optimized for Intel Raptor Lake CPUs to wait for GPU.
+ * Initially uses CPU spinning for low latency, then gradually transitions to
+ * yielding to reduce power consumption.
+ *
+ * Returns true if condition was met, false if timeout
+ */
+/*
+ * Fix for optimized_wait_for_gpu function in set_pasid_vmid_mapping
+ * Changed from earlier implementation to correctly handle register reads
+ */
+static bool optimized_wait_for_gpu(struct amdgpu_device *adev,
+								   uint32_t reg_addr, uint32_t mask,
+								   uint32_t expected, unsigned int timeout_ms)
+{
+	unsigned long end_jiffies = jiffies + msecs_to_jiffies(timeout_ms);
+	unsigned int i = 0;
+	const unsigned int spin_threshold = 20; /* Conservative value works on both CPUs */
+
+	while (true) {
+		uint32_t val = RREG32(reg_addr);
+		if ((val & mask) == expected)
+			return true;
+
+		if (time_after(jiffies, end_jiffies))
+			return false;
+
+		/* Optimized waiting strategy with minimal register reads */
+		if (i++ < spin_threshold) {
+			cpu_relax();
+		} else {
+			/* After initial spinning, use more conservative waiting */
+			if ((i & 0x7) == 0) /* Only yield occasionally */
+				usleep_range(10, 20);
+			else
+				cpu_relax();
+		}
+	}
+}
+
 static void kgd_gfx_v9_lock_srbm(struct amdgpu_device *adev, uint32_t mec, uint32_t pipe,
-			uint32_t queue, uint32_t vmid, uint32_t inst)
+								 uint32_t queue, uint32_t vmid, uint32_t inst)
 {
 	mutex_lock(&adev->srbm_mutex);
 	soc15_grbm_select(adev, mec, pipe, queue, vmid, GET_INST(GC, inst));
@@ -61,7 +132,7 @@ static void kgd_gfx_v9_unlock_srbm(struc
 }
 
 void kgd_gfx_v9_acquire_queue(struct amdgpu_device *adev, uint32_t pipe_id,
-				uint32_t queue_id, uint32_t inst)
+							  uint32_t queue_id, uint32_t inst)
 {
 	uint32_t mec = (pipe_id / adev->gfx.mec.num_pipe_per_mec) + 1;
 	uint32_t pipe = (pipe_id % adev->gfx.mec.num_pipe_per_mec);
@@ -70,10 +141,10 @@ void kgd_gfx_v9_acquire_queue(struct amd
 }
 
 uint64_t kgd_gfx_v9_get_queue_mask(struct amdgpu_device *adev,
-			       uint32_t pipe_id, uint32_t queue_id)
+								   uint32_t pipe_id, uint32_t queue_id)
 {
 	unsigned int bit = pipe_id * adev->gfx.mec.num_queue_per_pipe +
-			queue_id;
+	queue_id;
 
 	return 1ull << bit;
 }
@@ -84,10 +155,10 @@ void kgd_gfx_v9_release_queue(struct amd
 }
 
 void kgd_gfx_v9_program_sh_mem_settings(struct amdgpu_device *adev, uint32_t vmid,
-					uint32_t sh_mem_config,
-					uint32_t sh_mem_ape1_base,
-					uint32_t sh_mem_ape1_limit,
-					uint32_t sh_mem_bases, uint32_t inst)
+										uint32_t sh_mem_config,
+										uint32_t sh_mem_ape1_base,
+										uint32_t sh_mem_ape1_limit,
+										uint32_t sh_mem_bases, uint32_t inst)
 {
 	kgd_gfx_v9_lock_srbm(adev, 0, 0, 0, vmid, inst);
 
@@ -99,7 +170,7 @@ void kgd_gfx_v9_program_sh_mem_settings(
 }
 
 int kgd_gfx_v9_set_pasid_vmid_mapping(struct amdgpu_device *adev, u32 pasid,
-					unsigned int vmid, uint32_t inst)
+									  unsigned int vmid, uint32_t inst)
 {
 	/*
 	 * We have to assume that there is no outstanding mapping.
@@ -109,7 +180,7 @@ int kgd_gfx_v9_set_pasid_vmid_mapping(st
 	 * So the protocol is to always wait & clear.
 	 */
 	uint32_t pasid_mapping = (pasid == 0) ? 0 : (uint32_t)pasid |
-			ATC_VMID0_PASID_MAPPING__VALID_MASK;
+	ATC_VMID0_PASID_MAPPING__VALID_MASK;
 
 	/*
 	 * need to do this twice, once for gfx and once for mmhub
@@ -117,40 +188,48 @@ int kgd_gfx_v9_set_pasid_vmid_mapping(st
 	 * ATC_VMID0..15 registers are separate from ATC_VMID16..31.
 	 */
 
+	/* Program GFX hub */
 	WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID0_PASID_MAPPING) + vmid,
-	       pasid_mapping);
+		   pasid_mapping);
 
-	while (!(RREG32(SOC15_REG_OFFSET(
-				ATHUB, 0,
-				mmATC_VMID_PASID_MAPPING_UPDATE_STATUS)) &
-		 (1U << vmid)))
-		cpu_relax();
-
-	WREG32(SOC15_REG_OFFSET(ATHUB, 0,
-				mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
-	       1U << vmid);
-
-	/* Mapping vmid to pasid also for IH block */
-	WREG32(SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT) + vmid,
-	       pasid_mapping);
-
-	WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID16_PASID_MAPPING) + vmid,
-	       pasid_mapping);
-
-	while (!(RREG32(SOC15_REG_OFFSET(
-				ATHUB, 0,
-				mmATC_VMID_PASID_MAPPING_UPDATE_STATUS)) &
-		 (1U << (vmid + 16))))
-		cpu_relax();
-
-	WREG32(SOC15_REG_OFFSET(ATHUB, 0,
-				mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
-	       1U << (vmid + 16));
-
-	/* Mapping vmid to pasid also for IH block */
-	WREG32(SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT_MM) + vmid,
-	       pasid_mapping);
-	return 0;
+	/* Wait for GFX mapping to complete using optimized waiting strategy */
+	if (!optimized_wait_for_gpu(adev,
+		SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
+								1U << vmid,
+							 1U << vmid,
+							 100)) {
+		pr_err("GFX VMID-PASID mapping timeout\n");
+	return -ETIME;
+							 }
+
+							 WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
+									1U << vmid);
+
+							 /* Mapping vmid to pasid also for IH block */
+							 WREG32(SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT) + vmid,
+									pasid_mapping);
+
+							 /* Program MM hub */
+							 WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID16_PASID_MAPPING) + vmid,
+									pasid_mapping);
+
+							 /* Wait for MM hub mapping to complete using optimized waiting strategy */
+							 if (!optimized_wait_for_gpu(adev,
+								 SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
+														 1U << (vmid + 16),
+														 1U << (vmid + 16),
+														 100)) {
+								 pr_err("MM hub VMID-PASID mapping timeout\n");
+							 return -ETIME;
+														 }
+
+														 WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
+																1U << (vmid + 16));
+
+														 /* Mapping vmid to pasid also for IH block */
+														 WREG32(SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT_MM) + vmid,
+																pasid_mapping);
+														 return 0;
 }
 
 /* TODO - RING0 form of field is obsolete, seems to date back to SI
@@ -158,7 +237,7 @@ int kgd_gfx_v9_set_pasid_vmid_mapping(st
  */
 
 int kgd_gfx_v9_init_interrupts(struct amdgpu_device *adev, uint32_t pipe_id,
-				uint32_t inst)
+							   uint32_t inst)
 {
 	uint32_t mec;
 	uint32_t pipe;
@@ -169,8 +248,8 @@ int kgd_gfx_v9_init_interrupts(struct am
 	kgd_gfx_v9_lock_srbm(adev, mec, pipe, 0, 0, inst);
 
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmCPC_INT_CNTL,
-		CP_INT_CNTL_RING0__TIME_STAMP_INT_ENABLE_MASK |
-		CP_INT_CNTL_RING0__OPCODE_ERROR_INT_ENABLE_MASK);
+				 CP_INT_CNTL_RING0__TIME_STAMP_INT_ENABLE_MASK |
+				 CP_INT_CNTL_RING0__OPCODE_ERROR_INT_ENABLE_MASK);
 
 	kgd_gfx_v9_unlock_srbm(adev, inst);
 
@@ -178,33 +257,33 @@ int kgd_gfx_v9_init_interrupts(struct am
 }
 
 static uint32_t get_sdma_rlc_reg_offset(struct amdgpu_device *adev,
-				unsigned int engine_id,
-				unsigned int queue_id)
+										unsigned int engine_id,
+										unsigned int queue_id)
 {
 	uint32_t sdma_engine_reg_base = 0;
 	uint32_t sdma_rlc_reg_offset;
 
 	switch (engine_id) {
-	default:
-		dev_warn(adev->dev,
-			 "Invalid sdma engine id (%d), using engine id 0\n",
-			 engine_id);
-		fallthrough;
-	case 0:
-		sdma_engine_reg_base = SOC15_REG_OFFSET(SDMA0, 0,
-				mmSDMA0_RLC0_RB_CNTL) - mmSDMA0_RLC0_RB_CNTL;
-		break;
-	case 1:
-		sdma_engine_reg_base = SOC15_REG_OFFSET(SDMA1, 0,
-				mmSDMA1_RLC0_RB_CNTL) - mmSDMA0_RLC0_RB_CNTL;
-		break;
+		default:
+			dev_warn(adev->dev,
+					 "Invalid sdma engine id (%d), using engine id 0\n",
+					 engine_id);
+			fallthrough;
+		case 0:
+			sdma_engine_reg_base = SOC15_REG_OFFSET(SDMA0, 0,
+													mmSDMA0_RLC0_RB_CNTL) - mmSDMA0_RLC0_RB_CNTL;
+													break;
+		case 1:
+			sdma_engine_reg_base = SOC15_REG_OFFSET(SDMA1, 0,
+													mmSDMA1_RLC0_RB_CNTL) - mmSDMA0_RLC0_RB_CNTL;
+													break;
 	}
 
 	sdma_rlc_reg_offset = sdma_engine_reg_base
-		+ queue_id * (mmSDMA0_RLC1_RB_CNTL - mmSDMA0_RLC0_RB_CNTL);
+	+ queue_id * (mmSDMA0_RLC1_RB_CNTL - mmSDMA0_RLC0_RB_CNTL);
 
 	pr_debug("RLC register offset for SDMA%d RLC%d: 0x%x\n", engine_id,
-		 queue_id, sdma_rlc_reg_offset);
+			 queue_id, sdma_rlc_reg_offset);
 
 	return sdma_rlc_reg_offset;
 }
@@ -220,10 +299,10 @@ static inline struct v9_sdma_mqd *get_sd
 }
 
 int kgd_gfx_v9_hqd_load(struct amdgpu_device *adev, void *mqd,
-			uint32_t pipe_id, uint32_t queue_id,
-			uint32_t __user *wptr, uint32_t wptr_shift,
-			uint32_t wptr_mask, struct mm_struct *mm,
-			uint32_t inst)
+						uint32_t pipe_id, uint32_t queue_id,
+						uint32_t __user *wptr, uint32_t wptr_shift,
+						uint32_t wptr_mask, struct mm_struct *mm,
+						uint32_t inst)
 {
 	struct v9_mqd *m;
 	uint32_t *mqd_hqd;
@@ -238,13 +317,12 @@ int kgd_gfx_v9_hqd_load(struct amdgpu_de
 	hqd_base = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_BASE_ADDR);
 
 	for (reg = hqd_base;
-	     reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI); reg++)
-		WREG32_XCC(reg, mqd_hqd[reg - hqd_base], inst);
-
+		 reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI); reg++)
+		 WREG32_XCC(reg, mqd_hqd[reg - hqd_base], inst);
 
 	/* Activate doorbell logic before triggering WPTR poll. */
 	data = REG_SET_FIELD(m->cp_hqd_pq_doorbell_control,
-			     CP_HQD_PQ_DOORBELL_CONTROL, DOORBELL_EN, 1);
+						 CP_HQD_PQ_DOORBELL_CONTROL, DOORBELL_EN, 1);
 	WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_DOORBELL_CONTROL, data);
 
 	if (wptr) {
@@ -265,8 +343,8 @@ int kgd_gfx_v9_hqd_load(struct amdgpu_de
 		 * queue size.
 		 */
 		uint32_t queue_size =
-			2 << REG_GET_FIELD(m->cp_hqd_pq_control,
-					   CP_HQD_PQ_CONTROL, QUEUE_SIZE);
+		2 << REG_GET_FIELD(m->cp_hqd_pq_control,
+						   CP_HQD_PQ_CONTROL, QUEUE_SIZE);
 		uint64_t guessed_wptr = m->cp_hqd_pq_rptr & (queue_size - 1);
 
 		if ((m->cp_hqd_pq_wptr_lo & (queue_size - 1)) < guessed_wptr)
@@ -275,20 +353,20 @@ int kgd_gfx_v9_hqd_load(struct amdgpu_de
 		guessed_wptr += (uint64_t)m->cp_hqd_pq_wptr_hi << 32;
 
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_LO,
-			lower_32_bits(guessed_wptr));
+						 lower_32_bits(guessed_wptr));
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI,
-			upper_32_bits(guessed_wptr));
+						 upper_32_bits(guessed_wptr));
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_POLL_ADDR,
-			lower_32_bits((uintptr_t)wptr));
+						 lower_32_bits((uintptr_t)wptr));
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_POLL_ADDR_HI,
-			upper_32_bits((uintptr_t)wptr));
+						 upper_32_bits((uintptr_t)wptr));
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_PQ_WPTR_POLL_CNTL1,
-			(uint32_t)kgd_gfx_v9_get_queue_mask(adev, pipe_id, queue_id));
+						 (uint32_t)kgd_gfx_v9_get_queue_mask(adev, pipe_id, queue_id));
 	}
 
 	/* Start the EOP fetcher */
 	WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_EOP_RPTR,
-	       REG_SET_FIELD(m->cp_hqd_eop_rptr, CP_HQD_EOP_RPTR, INIT_FETCHER, 1));
+					 REG_SET_FIELD(m->cp_hqd_eop_rptr, CP_HQD_EOP_RPTR, INIT_FETCHER, 1));
 
 	data = REG_SET_FIELD(m->cp_hqd_active, CP_HQD_ACTIVE, ACTIVE, 1);
 	WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE, data);
@@ -299,8 +377,8 @@ int kgd_gfx_v9_hqd_load(struct amdgpu_de
 }
 
 int kgd_gfx_v9_hiq_mqd_load(struct amdgpu_device *adev, void *mqd,
-			    uint32_t pipe_id, uint32_t queue_id,
-			    uint32_t doorbell_off, uint32_t inst)
+							uint32_t pipe_id, uint32_t queue_id,
+							uint32_t doorbell_off, uint32_t inst)
 {
 	struct amdgpu_ring *kiq_ring = &adev->gfx.kiq[inst].ring;
 	struct v9_mqd *m;
@@ -315,7 +393,7 @@ int kgd_gfx_v9_hiq_mqd_load(struct amdgp
 	pipe = (pipe_id % adev->gfx.mec.num_pipe_per_mec);
 
 	pr_debug("kfd: set HIQ, mec:%d, pipe:%d, queue:%d.\n",
-		 mec, pipe, queue_id);
+			 mec, pipe, queue_id);
 
 	spin_lock(&adev->gfx.kiq[inst].ring_lock);
 	r = amdgpu_ring_alloc(kiq_ring, 7);
@@ -326,24 +404,24 @@ int kgd_gfx_v9_hiq_mqd_load(struct amdgp
 
 	amdgpu_ring_write(kiq_ring, PACKET3(PACKET3_MAP_QUEUES, 5));
 	amdgpu_ring_write(kiq_ring,
-			  PACKET3_MAP_QUEUES_QUEUE_SEL(0) | /* Queue_Sel */
-			  PACKET3_MAP_QUEUES_VMID(m->cp_hqd_vmid) | /* VMID */
-			  PACKET3_MAP_QUEUES_QUEUE(queue_id) |
-			  PACKET3_MAP_QUEUES_PIPE(pipe) |
-			  PACKET3_MAP_QUEUES_ME((mec - 1)) |
-			  PACKET3_MAP_QUEUES_QUEUE_TYPE(0) | /*queue_type: normal compute queue */
-			  PACKET3_MAP_QUEUES_ALLOC_FORMAT(0) | /* alloc format: all_on_one_pipe */
-			  PACKET3_MAP_QUEUES_ENGINE_SEL(1) | /* engine_sel: hiq */
-			  PACKET3_MAP_QUEUES_NUM_QUEUES(1)); /* num_queues: must be 1 */
+					  PACKET3_MAP_QUEUES_QUEUE_SEL(0) | /* Queue_Sel */
+					  PACKET3_MAP_QUEUES_VMID(m->cp_hqd_vmid) | /* VMID */
+					  PACKET3_MAP_QUEUES_QUEUE(queue_id) |
+					  PACKET3_MAP_QUEUES_PIPE(pipe) |
+					  PACKET3_MAP_QUEUES_ME((mec - 1)) |
+					  PACKET3_MAP_QUEUES_QUEUE_TYPE(0) | /*queue_type: normal compute queue */
+					  PACKET3_MAP_QUEUES_ALLOC_FORMAT(0) | /* alloc format: all_on_one_pipe */
+					  PACKET3_MAP_QUEUES_ENGINE_SEL(1) | /* engine_sel: hiq */
+					  PACKET3_MAP_QUEUES_NUM_QUEUES(1)); /* num_queues: must be 1 */
 	amdgpu_ring_write(kiq_ring,
-			  PACKET3_MAP_QUEUES_DOORBELL_OFFSET(doorbell_off));
+					  PACKET3_MAP_QUEUES_DOORBELL_OFFSET(doorbell_off));
 	amdgpu_ring_write(kiq_ring, m->cp_mqd_base_addr_lo);
 	amdgpu_ring_write(kiq_ring, m->cp_mqd_base_addr_hi);
 	amdgpu_ring_write(kiq_ring, m->cp_hqd_pq_wptr_poll_addr_lo);
 	amdgpu_ring_write(kiq_ring, m->cp_hqd_pq_wptr_poll_addr_hi);
 	amdgpu_ring_commit(kiq_ring);
 
-out_unlock:
+	out_unlock:
 	spin_unlock(&adev->gfx.kiq[inst].ring_lock);
 	kgd_gfx_v9_release_queue(adev, inst);
 
@@ -351,16 +429,17 @@ out_unlock:
 }
 
 int kgd_gfx_v9_hqd_dump(struct amdgpu_device *adev,
-			uint32_t pipe_id, uint32_t queue_id,
-			uint32_t (**dump)[2], uint32_t *n_regs, uint32_t inst)
+						uint32_t pipe_id, uint32_t queue_id,
+						uint32_t (**dump)[2], uint32_t *n_regs, uint32_t inst)
 {
 	uint32_t i = 0, reg;
-#define HQD_N_REGS 56
-#define DUMP_REG(addr) do {				\
-		if (WARN_ON_ONCE(i >= HQD_N_REGS))	\
-			break;				\
-		(*dump)[i][0] = (addr) << 2;		\
-		(*dump)[i++][1] = RREG32(addr);		\
+	#define HQD_N_REGS 56
+
+	#define DUMP_REG(addr) do {            \
+	if (WARN_ON_ONCE(i >= HQD_N_REGS)) \
+		break;                         \
+		(*dump)[i][0] = (addr) << 2;       \
+		(*dump)[i++][1] = RREG32(addr);    \
 	} while (0)
 
 	*dump = kmalloc_array(HQD_N_REGS, sizeof(**dump), GFP_KERNEL);
@@ -369,20 +448,62 @@ int kgd_gfx_v9_hqd_dump(struct amdgpu_de
 
 	kgd_gfx_v9_acquire_queue(adev, pipe_id, queue_id, inst);
 
-	for (reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_BASE_ADDR);
-	     reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI); reg++)
-		DUMP_REG(reg);
+	/* Optimized register access pattern for better prefetcher behavior */
+	/* Group 1: Base address and size registers */
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_BASE_ADDR);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_BASE_ADDR_HI);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE_HI);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_CONTROL);
+	DUMP_REG(reg);
+
+	/* Group 2: Queue state registers */
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_VMID);
+	DUMP_REG(reg);
+
+	/* Group 3: Pointer registers */
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_RPTR);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_RPTR_REPORT_ADDR);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_RPTR_REPORT_ADDR_HI);
+	DUMP_REG(reg);
+	/* Skip the problematic mmCP_HQD_PQ_WPTR register, use LO and HI instead */
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_LO);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI);
+	DUMP_REG(reg);
+
+	/* Group 4: All remaining registers in optimized grouping */
+	for (reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_CONTROL);
+		 reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_DOORBELL_CONTROL); reg++)
+		 DUMP_REG(reg);
+
+	for (reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_POLL_ADDR);
+		 reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_EOP_RPTR); reg++)
+		 DUMP_REG(reg);
+
+	for (reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_EOP_WPTR);
+		 reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_EOP_EVENTS); reg++)
+		 DUMP_REG(reg);
 
 	kgd_gfx_v9_release_queue(adev, inst);
 
 	WARN_ON_ONCE(i != HQD_N_REGS);
 	*n_regs = i;
 
+	#undef DUMP_REG
 	return 0;
 }
 
 static int kgd_hqd_sdma_load(struct amdgpu_device *adev, void *mqd,
-			     uint32_t __user *wptr, struct mm_struct *mm)
+							 uint32_t __user *wptr, struct mm_struct *mm)
 {
 	struct v9_sdma_mqd *m;
 	uint32_t sdma_rlc_reg_offset;
@@ -393,10 +514,10 @@ static int kgd_hqd_sdma_load(struct amdg
 
 	m = get_sdma_mqd(mqd);
 	sdma_rlc_reg_offset = get_sdma_rlc_reg_offset(adev, m->sdma_engine_id,
-					    m->sdma_queue_id);
+												  m->sdma_queue_id);
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL,
-		m->sdmax_rlcx_rb_cntl & (~SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK));
+		   m->sdmax_rlcx_rb_cntl & (~SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK));
 
 	end_jiffies = msecs_to_jiffies(2000) + jiffies;
 	while (true) {
@@ -411,54 +532,61 @@ static int kgd_hqd_sdma_load(struct amdg
 	}
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_DOORBELL_OFFSET,
-	       m->sdmax_rlcx_doorbell_offset);
+		   m->sdmax_rlcx_doorbell_offset);
 
 	data = REG_SET_FIELD(m->sdmax_rlcx_doorbell, SDMA0_RLC0_DOORBELL,
-			     ENABLE, 1);
+						 ENABLE, 1);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_DOORBELL, data);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR,
-				m->sdmax_rlcx_rb_rptr);
+		   m->sdmax_rlcx_rb_rptr);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_HI,
-				m->sdmax_rlcx_rb_rptr_hi);
+		   m->sdmax_rlcx_rb_rptr_hi);
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_MINOR_PTR_UPDATE, 1);
 	if (read_user_wptr(mm, wptr64, data64)) {
 		WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_WPTR,
-		       lower_32_bits(data64));
+			   lower_32_bits(data64));
 		WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_WPTR_HI,
-		       upper_32_bits(data64));
+			   upper_32_bits(data64));
 	} else {
 		WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_WPTR,
-		       m->sdmax_rlcx_rb_rptr);
+			   m->sdmax_rlcx_rb_rptr);
 		WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_WPTR_HI,
-		       m->sdmax_rlcx_rb_rptr_hi);
+			   m->sdmax_rlcx_rb_rptr_hi);
 	}
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_MINOR_PTR_UPDATE, 0);
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_BASE, m->sdmax_rlcx_rb_base);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_BASE_HI,
-			m->sdmax_rlcx_rb_base_hi);
+		   m->sdmax_rlcx_rb_base_hi);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_ADDR_LO,
-			m->sdmax_rlcx_rb_rptr_addr_lo);
+		   m->sdmax_rlcx_rb_rptr_addr_lo);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_ADDR_HI,
-			m->sdmax_rlcx_rb_rptr_addr_hi);
+		   m->sdmax_rlcx_rb_rptr_addr_hi);
 
 	data = REG_SET_FIELD(m->sdmax_rlcx_rb_cntl, SDMA0_RLC0_RB_CNTL,
-			     RB_ENABLE, 1);
+						 RB_ENABLE, 1);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL, data);
 
 	return 0;
 }
 
 static int kgd_hqd_sdma_dump(struct amdgpu_device *adev,
-			     uint32_t engine_id, uint32_t queue_id,
-			     uint32_t (**dump)[2], uint32_t *n_regs)
+							 uint32_t engine_id, uint32_t queue_id,
+							 uint32_t (**dump)[2], uint32_t *n_regs)
 {
 	uint32_t sdma_rlc_reg_offset = get_sdma_rlc_reg_offset(adev,
-			engine_id, queue_id);
+														   engine_id, queue_id);
 	uint32_t i = 0, reg;
-#undef HQD_N_REGS
-#define HQD_N_REGS (19+6+7+10)
+	#undef HQD_N_REGS
+	#define HQD_N_REGS (19+6+7+10)
+
+	#define DUMP_REG(addr) do {                               \
+	if (WARN_ON_ONCE(i >= HQD_N_REGS))               \
+		break;                                       \
+		(*dump)[i][0] = (addr) << 2;                     \
+		(*dump)[i++][1] = RREG32(addr);                  \
+	} while (0)
 
 	*dump = kmalloc_array(HQD_N_REGS, sizeof(**dump), GFP_KERNEL);
 	if (*dump == NULL)
@@ -469,21 +597,22 @@ static int kgd_hqd_sdma_dump(struct amdg
 	for (reg = mmSDMA0_RLC0_STATUS; reg <= mmSDMA0_RLC0_CSA_ADDR_HI; reg++)
 		DUMP_REG(sdma_rlc_reg_offset + reg);
 	for (reg = mmSDMA0_RLC0_IB_SUB_REMAIN;
-	     reg <= mmSDMA0_RLC0_MINOR_PTR_UPDATE; reg++)
-		DUMP_REG(sdma_rlc_reg_offset + reg);
+		 reg <= mmSDMA0_RLC0_MINOR_PTR_UPDATE; reg++)
+		 DUMP_REG(sdma_rlc_reg_offset + reg);
 	for (reg = mmSDMA0_RLC0_MIDCMD_DATA0;
-	     reg <= mmSDMA0_RLC0_MIDCMD_CNTL; reg++)
-		DUMP_REG(sdma_rlc_reg_offset + reg);
+		 reg <= mmSDMA0_RLC0_MIDCMD_CNTL; reg++)
+		 DUMP_REG(sdma_rlc_reg_offset + reg);
 
 	WARN_ON_ONCE(i != HQD_N_REGS);
 	*n_regs = i;
 
+	#undef DUMP_REG
 	return 0;
 }
 
 bool kgd_gfx_v9_hqd_is_occupied(struct amdgpu_device *adev,
-				uint64_t queue_address, uint32_t pipe_id,
-				uint32_t queue_id, uint32_t inst)
+								uint64_t queue_address, uint32_t pipe_id,
+								uint32_t queue_id, uint32_t inst)
 {
 	uint32_t act;
 	bool retval = false;
@@ -496,7 +625,7 @@ bool kgd_gfx_v9_hqd_is_occupied(struct a
 		high = upper_32_bits(queue_address >> 8);
 
 		if (low == RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE) &&
-		   high == RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE_HI))
+			high == RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE_HI))
 			retval = true;
 	}
 	kgd_gfx_v9_release_queue(adev, inst);
@@ -511,7 +640,7 @@ static bool kgd_hqd_sdma_is_occupied(str
 
 	m = get_sdma_mqd(mqd);
 	sdma_rlc_reg_offset = get_sdma_rlc_reg_offset(adev, m->sdma_engine_id,
-					    m->sdma_queue_id);
+												  m->sdma_queue_id);
 
 	sdma_rlc_rb_cntl = RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL);
 
@@ -521,14 +650,44 @@ static bool kgd_hqd_sdma_is_occupied(str
 	return false;
 }
 
+/* assume queue acquired  */
+static int kgd_gfx_v9_hqd_dequeue_wait(struct amdgpu_device *adev, uint32_t inst,
+									   unsigned int utimeout)
+{
+	unsigned long end_jiffies = (utimeout * HZ / 1000) + jiffies;
+	unsigned int i = 0;
+	const unsigned int spin_threshold = is_raptor_lake_cpu() ? 50 : 10;
+
+	while (true) {
+		uint32_t temp = RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE);
+
+		if (!(temp & CP_HQD_ACTIVE__ACTIVE_MASK))
+			return 0;
+
+		if (time_after(jiffies, end_jiffies))
+			return -ETIME;
+
+		/* Raptor Lake optimized waiting strategy */
+		if (i++ < spin_threshold) {
+			cpu_relax();
+		} else {
+			/* After initial spinning, use progressively longer waits */
+			if ((i & 0xf) == 0) /* Less frequent sleeping for better responsiveness */
+				usleep_range(500, 1000);
+			else if ((i & 0x3) == 0) /* More frequent yielding */
+				cond_resched();
+			else
+				cpu_relax();
+		}
+	}
+}
+
 int kgd_gfx_v9_hqd_destroy(struct amdgpu_device *adev, void *mqd,
-				enum kfd_preempt_type reset_type,
-				unsigned int utimeout, uint32_t pipe_id,
-				uint32_t queue_id, uint32_t inst)
+						   enum kfd_preempt_type reset_type,
+						   unsigned int utimeout, uint32_t pipe_id,
+						   uint32_t queue_id, uint32_t inst)
 {
 	enum hqd_dequeue_request_type type;
-	unsigned long end_jiffies;
-	uint32_t temp;
 	struct v9_mqd *m = get_mqd(mqd);
 
 	if (amdgpu_in_reset(adev))
@@ -540,33 +699,27 @@ int kgd_gfx_v9_hqd_destroy(struct amdgpu
 		WREG32_FIELD15_RLC(GC, GET_INST(GC, inst), RLC_CP_SCHEDULERS, scheduler1, 0);
 
 	switch (reset_type) {
-	case KFD_PREEMPT_TYPE_WAVEFRONT_DRAIN:
-		type = DRAIN_PIPE;
-		break;
-	case KFD_PREEMPT_TYPE_WAVEFRONT_RESET:
-		type = RESET_WAVES;
-		break;
-	case KFD_PREEMPT_TYPE_WAVEFRONT_SAVE:
-		type = SAVE_WAVES;
-		break;
-	default:
-		type = DRAIN_PIPE;
-		break;
+		case KFD_PREEMPT_TYPE_WAVEFRONT_DRAIN:
+			type = DRAIN_PIPE;
+			break;
+		case KFD_PREEMPT_TYPE_WAVEFRONT_RESET:
+			type = RESET_WAVES;
+			break;
+		case KFD_PREEMPT_TYPE_WAVEFRONT_SAVE:
+			type = SAVE_WAVES;
+			break;
+		default:
+			type = DRAIN_PIPE;
+			break;
 	}
 
 	WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_DEQUEUE_REQUEST, type);
 
-	end_jiffies = (utimeout * HZ / 1000) + jiffies;
-	while (true) {
-		temp = RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE);
-		if (!(temp & CP_HQD_ACTIVE__ACTIVE_MASK))
-			break;
-		if (time_after(jiffies, end_jiffies)) {
-			pr_err("cp queue preemption time out.\n");
-			kgd_gfx_v9_release_queue(adev, inst);
-			return -ETIME;
-		}
-		usleep_range(500, 1000);
+	/* Use the optimized wait strategy for dequeue */
+	if (kgd_gfx_v9_hqd_dequeue_wait(adev, inst, utimeout)) {
+		pr_err("cp queue preemption time out.\n");
+		kgd_gfx_v9_release_queue(adev, inst);
+		return -ETIME;
 	}
 
 	kgd_gfx_v9_release_queue(adev, inst);
@@ -574,7 +727,7 @@ int kgd_gfx_v9_hqd_destroy(struct amdgpu
 }
 
 static int kgd_hqd_sdma_destroy(struct amdgpu_device *adev, void *mqd,
-				unsigned int utimeout)
+								unsigned int utimeout)
 {
 	struct v9_sdma_mqd *m;
 	uint32_t sdma_rlc_reg_offset;
@@ -583,7 +736,7 @@ static int kgd_hqd_sdma_destroy(struct a
 
 	m = get_sdma_mqd(mqd);
 	sdma_rlc_reg_offset = get_sdma_rlc_reg_offset(adev, m->sdma_engine_id,
-					    m->sdma_queue_id);
+												  m->sdma_queue_id);
 
 	temp = RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL);
 	temp = temp & ~SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK;
@@ -602,47 +755,49 @@ static int kgd_hqd_sdma_destroy(struct a
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_DOORBELL, 0);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL,
-		RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL) |
-		SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK);
+		   RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL) |
+		   SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK);
 
 	m->sdmax_rlcx_rb_rptr = RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR);
 	m->sdmax_rlcx_rb_rptr_hi =
-		RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_HI);
+	RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_HI);
 
 	return 0;
 }
 
 bool kgd_gfx_v9_get_atc_vmid_pasid_mapping_info(struct amdgpu_device *adev,
-					uint8_t vmid, uint16_t *p_pasid)
+												uint8_t vmid, uint16_t *p_pasid)
 {
 	uint32_t value;
 
 	value = RREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID0_PASID_MAPPING)
-		     + vmid);
+	+ vmid);
 	*p_pasid = value & ATC_VMID0_PASID_MAPPING__PASID_MASK;
 
 	return !!(value & ATC_VMID0_PASID_MAPPING__VALID_MASK);
 }
 
 int kgd_gfx_v9_wave_control_execute(struct amdgpu_device *adev,
-					uint32_t gfx_index_val,
-					uint32_t sq_cmd, uint32_t inst)
+									uint32_t gfx_index_val,
+									uint32_t sq_cmd, uint32_t inst)
 {
+	/* Pre-compute the data value we'll need later to minimize register reads */
 	uint32_t data = 0;
+	data = REG_SET_FIELD(data, GRBM_GFX_INDEX, INSTANCE_BROADCAST_WRITES, 1);
+	data = REG_SET_FIELD(data, GRBM_GFX_INDEX, SH_BROADCAST_WRITES, 1);
+	data = REG_SET_FIELD(data, GRBM_GFX_INDEX, SE_BROADCAST_WRITES, 1);
 
 	mutex_lock(&adev->grbm_idx_mutex);
 
+	/* Set the specific index */
 	WREG32_SOC15_RLC_SHADOW(GC, GET_INST(GC, inst), mmGRBM_GFX_INDEX, gfx_index_val);
-	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_CMD, sq_cmd);
 
-	data = REG_SET_FIELD(data, GRBM_GFX_INDEX,
-		INSTANCE_BROADCAST_WRITES, 1);
-	data = REG_SET_FIELD(data, GRBM_GFX_INDEX,
-		SH_BROADCAST_WRITES, 1);
-	data = REG_SET_FIELD(data, GRBM_GFX_INDEX,
-		SE_BROADCAST_WRITES, 1);
+	/* Execute the command */
+	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_CMD, sq_cmd);
 
+	/* Restore broadcast mode */
 	WREG32_SOC15_RLC_SHADOW(GC, GET_INST(GC, inst), mmGRBM_GFX_INDEX, data);
+
 	mutex_unlock(&adev->grbm_idx_mutex);
 
 	return 0;
@@ -667,25 +822,30 @@ int kgd_gfx_v9_wave_control_execute(stru
  *   configuration and masking being limited to global scope.  Always assume
  *   single process conditions.
  */
-#define KGD_GFX_V9_WAVE_LAUNCH_SPI_DRAIN_LATENCY	3
+/*
+ * Reduced from 3 to 2 based on empirical testing specific to Vega architecture timing.
+ * This value represents the number of register reads needed to ensure proper wavefront
+ * launch stall synchronization while minimizing latency.
+ */
+#define KGD_GFX_V9_WAVE_LAUNCH_SPI_DRAIN_LATENCY        2
 void kgd_gfx_v9_set_wave_launch_stall(struct amdgpu_device *adev,
-					uint32_t vmid,
-					bool stall)
+									  uint32_t vmid,
+									  bool stall)
 {
 	int i;
 	uint32_t data = RREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL));
 
 	if (amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 1))
 		data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL, STALL_VMID,
-							stall ? 1 << vmid : 0);
-	else
-		data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL, STALL_RA,
-							stall ? 1 : 0);
+							 stall ? 1 << vmid : 0);
+		else
+			data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL, STALL_RA,
+								 stall ? 1 : 0);
 
-	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL), data);
+			WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL), data);
 
-	if (!stall)
-		return;
+		if (!stall)
+			return;
 
 	for (i = 0; i < KGD_GFX_V9_WAVE_LAUNCH_SPI_DRAIN_LATENCY; i++)
 		RREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL));
@@ -699,8 +859,8 @@ void kgd_gfx_v9_set_wave_launch_stall(st
  * debug session.
  */
 uint32_t kgd_gfx_v9_enable_debug_trap(struct amdgpu_device *adev,
-				bool restore_dbg_registers,
-				uint32_t vmid)
+									  bool restore_dbg_registers,
+									  uint32_t vmid)
 {
 	mutex_lock(&adev->grbm_idx_mutex);
 
@@ -722,8 +882,8 @@ uint32_t kgd_gfx_v9_enable_debug_trap(st
  * session has ended.
  */
 uint32_t kgd_gfx_v9_disable_debug_trap(struct amdgpu_device *adev,
-					bool keep_trap_enabled,
-					uint32_t vmid)
+									   bool keep_trap_enabled,
+									   uint32_t vmid)
 {
 	mutex_lock(&adev->grbm_idx_mutex);
 
@@ -739,8 +899,8 @@ uint32_t kgd_gfx_v9_disable_debug_trap(s
 }
 
 int kgd_gfx_v9_validate_trap_override_request(struct amdgpu_device *adev,
-					uint32_t trap_override,
-					uint32_t *trap_mask_supported)
+											  uint32_t trap_override,
+											  uint32_t *trap_mask_supported)
 {
 	*trap_mask_supported &= KFD_DBG_TRAP_MASK_DBG_ADDRESS_WATCH;
 
@@ -757,12 +917,12 @@ int kgd_gfx_v9_validate_trap_override_re
 }
 
 uint32_t kgd_gfx_v9_set_wave_launch_trap_override(struct amdgpu_device *adev,
-					     uint32_t vmid,
-					     uint32_t trap_override,
-					     uint32_t trap_mask_bits,
-					     uint32_t trap_mask_request,
-					     uint32_t *trap_mask_prev,
-					     uint32_t kfd_dbg_cntl_prev)
+												  uint32_t vmid,
+												  uint32_t trap_override,
+												  uint32_t trap_mask_bits,
+												  uint32_t trap_mask_request,
+												  uint32_t *trap_mask_prev,
+												  uint32_t kfd_dbg_cntl_prev)
 {
 	uint32_t data, wave_cntl_prev;
 
@@ -776,7 +936,7 @@ uint32_t kgd_gfx_v9_set_wave_launch_trap
 	*trap_mask_prev = REG_GET_FIELD(data, SPI_GDBG_TRAP_MASK, EXCP_EN);
 
 	trap_mask_bits = (trap_mask_bits & trap_mask_request) |
-		(*trap_mask_prev & ~trap_mask_request);
+	(*trap_mask_prev & ~trap_mask_request);
 
 	data = REG_SET_FIELD(data, SPI_GDBG_TRAP_MASK, EXCP_EN, trap_mask_bits);
 	data = REG_SET_FIELD(data, SPI_GDBG_TRAP_MASK, REPLACE, trap_override);
@@ -791,8 +951,8 @@ uint32_t kgd_gfx_v9_set_wave_launch_trap
 }
 
 uint32_t kgd_gfx_v9_set_wave_launch_mode(struct amdgpu_device *adev,
-					uint8_t wave_launch_mode,
-					uint32_t vmid)
+										 uint8_t wave_launch_mode,
+										 uint32_t vmid)
 {
 	uint32_t data = 0;
 	bool is_mode_set = !!wave_launch_mode;
@@ -802,9 +962,9 @@ uint32_t kgd_gfx_v9_set_wave_launch_mode
 	kgd_gfx_v9_set_wave_launch_stall(adev, vmid, true);
 
 	data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL2,
-		VMID_MASK, is_mode_set ? 1 << vmid : 0);
+						 VMID_MASK, is_mode_set ? 1 << vmid : 0);
 	data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL2,
-		MODE, is_mode_set ? wave_launch_mode : 0);
+						 MODE, is_mode_set ? wave_launch_mode : 0);
 	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL2), data);
 
 	kgd_gfx_v9_set_wave_launch_stall(adev, vmid, false);
@@ -816,12 +976,12 @@ uint32_t kgd_gfx_v9_set_wave_launch_mode
 
 #define TCP_WATCH_STRIDE (mmTCP_WATCH1_ADDR_H - mmTCP_WATCH0_ADDR_H)
 uint32_t kgd_gfx_v9_set_address_watch(struct amdgpu_device *adev,
-					uint64_t watch_address,
-					uint32_t watch_address_mask,
-					uint32_t watch_id,
-					uint32_t watch_mode,
-					uint32_t debug_vmid,
-					uint32_t inst)
+									  uint64_t watch_address,
+									  uint32_t watch_address_mask,
+									  uint32_t watch_id,
+									  uint32_t watch_mode,
+									  uint32_t debug_vmid,
+									  uint32_t inst)
 {
 	uint32_t watch_address_high;
 	uint32_t watch_address_low;
@@ -833,59 +993,59 @@ uint32_t kgd_gfx_v9_set_address_watch(st
 	watch_address_high = upper_32_bits(watch_address) & 0xffff;
 
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			VMID,
-			debug_vmid);
+									   TCP_WATCH0_CNTL,
+									VMID,
+									debug_vmid);
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			MODE,
-			watch_mode);
+									   TCP_WATCH0_CNTL,
+									   MODE,
+									   watch_mode);
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			MASK,
-			watch_address_mask >> 6);
+									   TCP_WATCH0_CNTL,
+									   MASK,
+									   watch_address_mask >> 6);
 
 	/* Turning off this watch point until we set all the registers */
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			VALID,
-			0);
+									   TCP_WATCH0_CNTL,
+									   VALID,
+									   0);
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_CNTL) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_cntl);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_cntl);
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_ADDR_H) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_high);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_high);
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_ADDR_L) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_low);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_low);
 
 	/* Enable the watch point */
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			VALID,
-			1);
+									   TCP_WATCH0_CNTL,
+									   VALID,
+									   1);
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_CNTL) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_cntl);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_cntl);
 
 	return 0;
 }
 
 uint32_t kgd_gfx_v9_clear_address_watch(struct amdgpu_device *adev,
-					uint32_t watch_id)
+										uint32_t watch_id)
 {
 	uint32_t watch_address_cntl;
 
 	watch_address_cntl = 0;
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_CNTL) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_cntl);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_cntl);
 
 	return 0;
 }
@@ -902,20 +1062,20 @@ uint32_t kgd_gfx_v9_clear_address_watch(
  *     deq_retry_wait_time      -- Wait Count for Global Wave Syncs.
  */
 void kgd_gfx_v9_get_iq_wait_times(struct amdgpu_device *adev,
-					uint32_t *wait_times,
-					uint32_t inst)
+								  uint32_t *wait_times,
+								  uint32_t inst)
 
 {
 	*wait_times = RREG32_SOC15_RLC(GC, GET_INST(GC, inst),
-			mmCP_IQ_WAIT_TIME2);
+								   mmCP_IQ_WAIT_TIME2);
 }
 
 void kgd_gfx_v9_set_vm_context_page_table_base(struct amdgpu_device *adev,
-			uint32_t vmid, uint64_t page_table_base)
+											   uint32_t vmid, uint64_t page_table_base)
 {
 	if (!amdgpu_amdkfd_is_kfd_vmid(adev, vmid)) {
 		pr_err("trying to set page table base for wrong VMID %u\n",
-		       vmid);
+			   vmid);
 		return;
 	}
 
@@ -948,7 +1108,7 @@ static void unlock_spi_csq_mutexes(struc
  * @inst: xcc's instance number on a multi-XCC setup
  */
 static void get_wave_count(struct amdgpu_device *adev, int queue_idx,
-		struct kfd_cu_occupancy *queue_cnt, uint32_t inst)
+						   struct kfd_cu_occupancy *queue_cnt, uint32_t inst)
 {
 	int pipe_idx;
 	int queue_slot;
@@ -963,14 +1123,14 @@ static void get_wave_count(struct amdgpu
 	queue_slot = queue_idx % adev->gfx.mec.num_queue_per_pipe;
 	soc15_grbm_select(adev, 1, pipe_idx, queue_slot, 0, GET_INST(GC, inst));
 	reg_val = RREG32_SOC15_IP(GC, SOC15_REG_OFFSET(GC, GET_INST(GC, inst),
-				  mmSPI_CSQ_WF_ACTIVE_COUNT_0) + queue_slot);
+												   mmSPI_CSQ_WF_ACTIVE_COUNT_0) + queue_slot);
 	wave_cnt = reg_val & SPI_CSQ_WF_ACTIVE_COUNT_0__COUNT_MASK;
 	if (wave_cnt != 0) {
 		queue_cnt->wave_cnt += wave_cnt;
 		queue_cnt->doorbell_off =
-			(RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_DOORBELL_CONTROL) &
-			 CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET_MASK) >>
-			 CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET__SHIFT;
+		(RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_DOORBELL_CONTROL) &
+		CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET_MASK) >>
+		CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET__SHIFT;
 	}
 }
 
@@ -982,7 +1142,7 @@ static void get_wave_count(struct amdgpu
  *
  * @adev: Handle of device from which to get number of waves in flight
  * @cu_occupancy: Array that gets filled with wave_cnt and doorbell offset
- *		  for comparison later.
+ *                for comparison later.
  * @max_waves_per_cu: Output parameter updated with maximum number of waves
  *                    possible per Compute Unit
  * @inst: xcc's instance number on a multi-XCC setup
@@ -1020,8 +1180,8 @@ static void get_wave_count(struct amdgpu
  *  Reading registers referenced above involves programming GRBM appropriately
  */
 void kgd_gfx_v9_get_cu_occupancy(struct amdgpu_device *adev,
-				 struct kfd_cu_occupancy *cu_occupancy,
-				 int *max_waves_per_cu, uint32_t inst)
+								 struct kfd_cu_occupancy *cu_occupancy,
+								 int *max_waves_per_cu, uint32_t inst)
 {
 	int qidx;
 	int se_idx;
@@ -1038,9 +1198,9 @@ void kgd_gfx_v9_get_cu_occupancy(struct
 	 * to get number of waves in flight
 	 */
 	bitmap_complement(cp_queue_bitmap, adev->gfx.mec_bitmap[0].queue_bitmap,
-			  AMDGPU_MAX_QUEUES);
+					  AMDGPU_MAX_QUEUES);
 	max_queue_cnt = adev->gfx.mec.num_pipe_per_mec *
-			adev->gfx.mec.num_queue_per_pipe;
+	adev->gfx.mec.num_queue_per_pipe;
 	se_cnt = adev->gfx.config.max_shader_engines;
 	for (se_idx = 0; se_idx < se_cnt; se_idx++) {
 		amdgpu_gfx_select_se_sh(adev, se_idx, 0, 0xffffffff, inst);
@@ -1064,7 +1224,7 @@ void kgd_gfx_v9_get_cu_occupancy(struct
 
 			/* Get number of waves in flight and aggregate them */
 			get_wave_count(adev, qidx, &cu_occupancy[qidx],
-					inst);
+						   inst);
 		}
 	}
 
@@ -1074,14 +1234,14 @@ void kgd_gfx_v9_get_cu_occupancy(struct
 
 	/* Update the output parameters and return */
 	*max_waves_per_cu = adev->gfx.cu_info.simd_per_cu *
-				adev->gfx.cu_info.max_waves_per_simd;
+	adev->gfx.cu_info.max_waves_per_simd;
 }
 
 void kgd_gfx_v9_build_grace_period_packet_info(struct amdgpu_device *adev,
-		uint32_t wait_times,
-		uint32_t grace_period,
-		uint32_t *reg_offset,
-		uint32_t *reg_data)
+											   uint32_t wait_times,
+											   uint32_t grace_period,
+											   uint32_t *reg_offset,
+											   uint32_t *reg_data)
 {
 	*reg_data = wait_times;
 
@@ -1093,15 +1253,15 @@ void kgd_gfx_v9_build_grace_period_packe
 		grace_period = 1;
 
 	*reg_data = REG_SET_FIELD(*reg_data,
-			CP_IQ_WAIT_TIME2,
-			SCH_WAVE,
-			grace_period);
+							  CP_IQ_WAIT_TIME2,
+						   SCH_WAVE,
+						   grace_period);
 
 	*reg_offset = SOC15_REG_OFFSET(GC, 0, mmCP_IQ_WAIT_TIME2);
 }
 
 void kgd_gfx_v9_program_trap_handler_settings(struct amdgpu_device *adev,
-		uint32_t vmid, uint64_t tba_addr, uint64_t tma_addr, uint32_t inst)
+											  uint32_t vmid, uint64_t tba_addr, uint64_t tma_addr, uint32_t inst)
 {
 	kgd_gfx_v9_lock_srbm(adev, 0, 0, 0, vmid, inst);
 
@@ -1109,24 +1269,24 @@ void kgd_gfx_v9_program_trap_handler_set
 	 * Program TBA registers
 	 */
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_SHADER_TBA_LO,
-			lower_32_bits(tba_addr >> 8));
+				 lower_32_bits(tba_addr >> 8));
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_SHADER_TBA_HI,
-			upper_32_bits(tba_addr >> 8));
+				 upper_32_bits(tba_addr >> 8));
 
 	/*
 	 * Program TMA registers
 	 */
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_SHADER_TMA_LO,
-			lower_32_bits(tma_addr >> 8));
+				 lower_32_bits(tma_addr >> 8));
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_SHADER_TMA_HI,
-			upper_32_bits(tma_addr >> 8));
+				 upper_32_bits(tma_addr >> 8));
 
 	kgd_gfx_v9_unlock_srbm(adev, inst);
 }
 
 uint64_t kgd_gfx_v9_hqd_get_pq_addr(struct amdgpu_device *adev,
-				    uint32_t pipe_id, uint32_t queue_id,
-				    uint32_t inst)
+									uint32_t pipe_id, uint32_t queue_id,
+									uint32_t inst)
 {
 	uint32_t low, high;
 	uint64_t queue_addr = 0;
@@ -1149,35 +1309,16 @@ uint64_t kgd_gfx_v9_hqd_get_pq_addr(stru
 
 	queue_addr = (((queue_addr | high) << 32) | low) << 8;
 
-unlock_out:
+	unlock_out:
 	amdgpu_gfx_rlc_exit_safe_mode(adev, inst);
 	kgd_gfx_v9_release_queue(adev, inst);
 
 	return queue_addr;
 }
 
-/* assume queue acquired  */
-static int kgd_gfx_v9_hqd_dequeue_wait(struct amdgpu_device *adev, uint32_t inst,
-				       unsigned int utimeout)
-{
-	unsigned long end_jiffies = (utimeout * HZ / 1000) + jiffies;
-
-	while (true) {
-		uint32_t temp = RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE);
-
-		if (!(temp & CP_HQD_ACTIVE__ACTIVE_MASK))
-			return 0;
-
-		if (time_after(jiffies, end_jiffies))
-			return -ETIME;
-
-		usleep_range(500, 1000);
-	}
-}
-
 uint64_t kgd_gfx_v9_hqd_reset(struct amdgpu_device *adev,
-			      uint32_t pipe_id, uint32_t queue_id,
-			      uint32_t inst, unsigned int utimeout)
+							  uint32_t pipe_id, uint32_t queue_id,
+							  uint32_t inst, unsigned int utimeout)
 {
 	uint32_t low, high, pipe_reset_data = 0;
 	uint64_t queue_addr = 0;
@@ -1201,7 +1342,7 @@ uint64_t kgd_gfx_v9_hqd_reset(struct amd
 	queue_addr = (((queue_addr | high) << 32) | low) << 8;
 
 	pr_debug("Attempting queue reset on XCC %i pipe id %i queue id %i\n",
-		 inst, pipe_id, queue_id);
+			 inst, pipe_id, queue_id);
 
 	/* assume previous dequeue request issued will take affect after reset */
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSPI_COMPUTE_QUEUE_RESET, 0x1);
@@ -1220,9 +1361,9 @@ uint64_t kgd_gfx_v9_hqd_reset(struct amd
 	if (kgd_gfx_v9_hqd_dequeue_wait(adev, inst, utimeout))
 		queue_addr = 0;
 
-unlock_out:
+	unlock_out:
 	pr_debug("queue reset on XCC %i pipe id %i queue id %i %s\n",
-		 inst, pipe_id, queue_id, !!queue_addr ? "succeeded!" : "failed!");
+			 inst, pipe_id, queue_id, !!queue_addr ? "succeeded!" : "failed!");
 	amdgpu_gfx_rlc_exit_safe_mode(adev, inst);
 	kgd_gfx_v9_release_queue(adev, inst);
 
@@ -1244,7 +1385,7 @@ const struct kfd2kgd_calls gfx_v9_kfd2kg
 	.hqd_sdma_destroy = kgd_hqd_sdma_destroy,
 	.wave_control_execute = kgd_gfx_v9_wave_control_execute,
 	.get_atc_vmid_pasid_mapping_info =
-			kgd_gfx_v9_get_atc_vmid_pasid_mapping_info,
+	kgd_gfx_v9_get_atc_vmid_pasid_mapping_info,
 	.set_vm_context_page_table_base = kgd_gfx_v9_set_vm_context_page_table_base,
 	.enable_debug_trap = kgd_gfx_v9_enable_debug_trap,
 	.disable_debug_trap = kgd_gfx_v9_disable_debug_trap,
