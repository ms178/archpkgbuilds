--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c	2025-04-12 15:21:00.725020339 +0200
@@ -2709,58 +2709,129 @@ static int amdgpu_vm_stats_is_zero(struc
  */
 void amdgpu_vm_fini(struct amdgpu_device *adev, struct amdgpu_vm *vm)
 {
-	struct amdgpu_bo_va_mapping *mapping, *tmp;
+	struct amdgpu_bo_va_mapping *mapping, *tmp_mapping;
+	struct amdgpu_bo_va *bo_va, *next_bo_va;
+	struct list_head bo_vas_to_clean; /* Temp list for unique bo_vas */
 	bool prt_fini_needed = !!adev->gmc.gmc_funcs->set_prt;
 	struct amdgpu_bo *root;
-	unsigned long flags;
+	unsigned long flags; /* For spinlock */
 	int i;
 
 	amdgpu_amdkfd_gpuvm_destroy_cb(adev, vm);
 
 	flush_work(&vm->pt_free_work);
 
+	/* Initialize the temporary list */
+	INIT_LIST_HEAD(&bo_vas_to_clean);
+
 	root = amdgpu_bo_ref(vm->root.bo);
-	amdgpu_bo_reserve(root, true);
-	amdgpu_vm_set_pasid(adev, vm, 0);
-	dma_fence_wait(vm->last_unlocked, false);
-	dma_fence_put(vm->last_unlocked);
-	dma_fence_wait(vm->last_tlb_flush, false);
-	/* Make sure that all fence callbacks have completed */
-	spin_lock_irqsave(vm->last_tlb_flush->lock, flags);
-	spin_unlock_irqrestore(vm->last_tlb_flush->lock, flags);
-	dma_fence_put(vm->last_tlb_flush);
+	/* Reserve needed to safely manipulate PTs and fences */
+	if (amdgpu_bo_reserve(root, true) == 0) {
+		amdgpu_vm_set_pasid(adev, vm, 0);
+		dma_fence_wait(vm->last_unlocked, false);
+		dma_fence_wait(vm->last_tlb_flush, false);
+		/* Make sure that all fence callbacks have completed */
+		spin_lock_irqsave(vm->last_tlb_flush->lock, flags);
+		spin_unlock_irqrestore(vm->last_tlb_flush->lock, flags);
+
+
+		/* Clear any remaining freed mappings */
+		list_for_each_entry_safe(mapping, tmp_mapping, &vm->freed, list) {
+			if (mapping->flags & AMDGPU_PTE_PRT_FLAG(adev) && prt_fini_needed) {
+				amdgpu_vm_prt_fini(adev, vm);
+				prt_fini_needed = false;
+			}
 
-	list_for_each_entry_safe(mapping, tmp, &vm->freed, list) {
-		if (mapping->flags & AMDGPU_PTE_PRT_FLAG(adev) && prt_fini_needed) {
-			amdgpu_vm_prt_fini(adev, vm);
-			prt_fini_needed = false;
+			list_del(&mapping->list);
+			amdgpu_vm_free_mapping(adev, vm, mapping, NULL);
 		}
 
-		list_del(&mapping->list);
-		amdgpu_vm_free_mapping(adev, vm, mapping, NULL);
+		/* Free the page table tree */
+		amdgpu_vm_pt_free_root(adev, vm);
+		amdgpu_bo_unreserve(root);
+	} else {
+		/* Should not happen, but try to recover */
+		dev_err(adev->dev, "Failed to reserve VM root BO for fini\n");
+		/* Can't safely free PTs or process freed list without reservation */
 	}
+	WARN_ON(vm->root.bo); /* root.bo should be NULL after pt_free_root */
 
-	amdgpu_vm_pt_free_root(adev, vm);
-	amdgpu_bo_unreserve(root);
-	amdgpu_bo_unref(&root);
-	WARN_ON(vm->root.bo);
-
-	amdgpu_vm_fini_entities(vm);
 
+	/* Process remaining mappings in the interval tree and collect unique associated bo_vas */
 	if (!RB_EMPTY_ROOT(&vm->va.rb_root)) {
-		dev_err(adev->dev, "still active bo inside vm\n");
+		dev_warn(adev->dev, "still active mappings in vm interval tree during fini\n");
+
+		rbtree_postorder_for_each_entry_safe(mapping, tmp_mapping,
+											 &vm->va.rb_root, rb) {
+			bo_va = mapping->bo_va;
+			if (bo_va) {
+				/* Ensure list head is valid before moving */
+				if (list_empty_careful(&bo_va->base.vm_status))
+					INIT_LIST_HEAD(&bo_va->base.vm_status);
+
+				/* Move unique bo_va to our temporary list for cleanup */
+				list_move_tail(&bo_va->base.vm_status, &bo_vas_to_clean);
+
+				/* Remove mapping from its list (valids/invalids) */
+				list_del_init(&mapping->list);
+
+				mapping->bo_va = NULL; /* Prevent reuse */
+			}
+			/* Free the mapping structure itself */
+			kfree(mapping);
+											 }
+											 /* Clear the interval tree root manually after iteration */
+											 vm->va = RB_ROOT_CACHED;
 	}
-	rbtree_postorder_for_each_entry_safe(mapping, tmp,
-					     &vm->va.rb_root, rb) {
-		/* Don't remove the mapping here, we don't want to trigger a
-		 * rebalance and the tree is about to be destroyed anyway.
-		 */
-		list_del(&mapping->list);
-		kfree(mapping);
+
+	/* Clean up the collected bo_vas that weren't cleaned via gem_close->vm_bo_del */
+	list_for_each_entry_safe(bo_va, next_bo_va, &bo_vas_to_clean, base.vm_status) {
+		struct amdgpu_bo *bo = bo_va->base.bo;
+		struct amdgpu_vm_bo_base **base_ptr;
+
+		list_del_init(&bo_va->base.vm_status); /* Remove from temp list */
+
+		if (bo) {
+			/* Remove from bo's list (bo->vm_bo) */
+			for (base_ptr = &bo->vm_bo; *base_ptr; base_ptr = &(*base_ptr)->next) {
+				if (*base_ptr == &bo_va->base) {
+					*base_ptr = bo_va->base.next;
+					break;
+				}
+			}
+
+			/* Decrement stats - using the BO's current resource state */
+			spin_lock_irqsave(&vm->status_lock, flags);
+			amdgpu_vm_update_stats_locked(&bo_va->base, bo->tbo.resource, -1);
+			spin_unlock_irqrestore(&vm->status_lock, flags);
+
+			/* Clean up other BO related fields */
+			if (amdgpu_vm_is_bo_always_valid(vm, bo))
+				ttm_bo_set_bulk_move(&bo->tbo, NULL);
+
+			if (bo_va->is_xgmi)
+				amdgpu_xgmi_set_pstate(adev, AMDGPU_XGMI_PSTATE_MIN);
+		}
+
+		/* Free fences and the bo_va structure */
+		dma_fence_put(bo_va->last_pt_update);
+		kfree(bo_va);
+	}
+
+	/* Handle any remaining PRT callbacks if necessary */
+	if (prt_fini_needed && !list_empty(&vm->freed)) {
+		/* This path should ideally not be taken if freed list processed above */
+		amdgpu_vm_prt_fini(adev, vm);
 	}
 
+
+	/* Final VM resource cleanup */
+	dma_fence_put(vm->last_unlocked);
+	dma_fence_put(vm->last_tlb_flush);
 	dma_fence_put(vm->last_update);
 
+	amdgpu_vm_fini_entities(vm);
+
 	for (i = 0; i < AMDGPU_MAX_VMHUBS; i++) {
 		if (vm->reserved_vmid[i]) {
 			amdgpu_vmid_free_reserved(adev, i);
@@ -2772,13 +2843,19 @@ void amdgpu_vm_fini(struct amdgpu_device
 
 	if (!amdgpu_vm_stats_is_zero(vm)) {
 		struct amdgpu_task_info *ti = vm->task_info;
+		const char *pname = ti ? ti->process_name : "(unknown)";
+		int pid = ti ? ti->pid : 0;
+		const char *tname = ti ? ti->task_name : "(unknown)";
+		int tgid = ti ? ti->tgid : 0;
 
+		/* This warning should ideally not appear after the fix */
 		dev_warn(adev->dev,
-			 "VM memory stats for proc %s(%d) task %s(%d) is non-zero when fini\n",
-			 ti->process_name, ti->pid, ti->task_name, ti->tgid);
+				 "VM memory stats AFTER CLEANUP for proc %s(%d) task %s(%d) is non-zero when fini\n",
+				 pname, pid, tname, tgid);
 	}
 
 	amdgpu_vm_put_task_info(vm->task_info);
+	amdgpu_bo_unref(&root); /* Balance initial ref */
 }
 
 /**


--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c	2025-04-12 13:04:56.516191172 +0200
@@ -42,28 +42,669 @@
 #include "amdgpu_dma_buf.h"
 #include "amdgpu_hmm.h"
 #include "amdgpu_xgmi.h"
-#include "amdgpu_vm.h"
+#include "amdgpu_vm.h" // Included in orig, needed for some Vega functions
+
+/* Define constants for Vega memory management */
+#define AMDGPU_VEGA_HBM2_BANK_SIZE (1ULL * 1024 * 1024) /* 1MB HBM2 bank size */
+#define AMDGPU_VEGA_SMALL_BUFFER_SIZE (1ULL * 1024 * 1024) /* 1MB */
+#define AMDGPU_VEGA_MEDIUM_BUFFER_SIZE (4ULL * 1024 * 1024) /* 4MB */
+#define AMDGPU_VEGA_LARGE_BUFFER_SIZE (16ULL * 1024 * 1024) /* 16MB */
+
+/* Module parameters for tunable thresholds */
+static int amdgpu_vega_vram_pressure_low = 65;
+static int amdgpu_vega_vram_pressure_mid = 75;
+static int amdgpu_vega_vram_pressure_high = 85;
+
+module_param_named(vram_pressure_low, amdgpu_vega_vram_pressure_low, int, 0644);
+MODULE_PARM_DESC(vram_pressure_low, "Low VRAM pressure threshold for Vega (65)");
+module_param_named(vram_pressure_mid, amdgpu_vega_vram_pressure_mid, int, 0644);
+MODULE_PARM_DESC(vram_pressure_mid, "Medium VRAM pressure threshold for Vega (75)");
+module_param_named(vram_pressure_high, amdgpu_vega_vram_pressure_high, int, 0644);
+MODULE_PARM_DESC(vram_pressure_high, "High VRAM pressure threshold for Vega (85)");
+
+/**
+ * amdgpu_vega_get_vram_usage - Get current VRAM usage percentage
+ * @adev: AMDGPU device
+ *
+ * Returns the current VRAM usage as a percentage (0-100)
+ */
+static uint32_t amdgpu_vega_get_vram_usage(struct amdgpu_device *adev)
+{
+	struct ttm_resource_manager *vram_man;
+	uint64_t vram_usage = 0;
+	uint64_t vram_size = 0;
+	uint32_t usage_percent = 0;
+
+	if (!adev || !adev->gmc.mc_vram_size)
+		return 0;
+
+	vram_man = ttm_manager_type(&adev->mman.bdev, TTM_PL_VRAM);
+	if (!vram_man)
+		return 0;
+
+	vram_usage = ttm_resource_manager_usage(vram_man);
+	vram_size = adev->gmc.mc_vram_size;
+
+	if (vram_size)
+		usage_percent = div64_u64(vram_usage * 100, vram_size);
+
+	return usage_percent;
+}
+
+/**
+ * amdgpu_vega_get_effective_vram_usage - Get enhanced VRAM usage metrics
+ * @adev: AMDGPU device
+ *
+ * Returns an enhanced VRAM usage value (0-100) that incorporates TTM memory
+ * management state to detect actual memory pressure for Vega GPUs.
+ */
+static uint32_t amdgpu_vega_get_effective_vram_usage(struct amdgpu_device *adev)
+{
+	uint32_t usage_percent;
+	uint32_t effective_percent;
+	struct ttm_resource_manager *vram_man;
+
+	if (!adev)
+		return 0;
+
+	/* Get basic VRAM usage percentage using our helper function */
+	usage_percent = amdgpu_vega_get_vram_usage(adev);
+	effective_percent = usage_percent;
+
+	/* Only apply enhancements for Vega GPUs */
+	if (adev->asic_type != CHIP_VEGA10)
+		return usage_percent;
+
+	/* Get TTM resource manager for additional pressure checks */
+	vram_man = ttm_manager_type(&adev->mman.bdev, TTM_PL_VRAM);
+	if (!vram_man)
+		return usage_percent;
+
+	/*
+	 * If TTM is using system memory as fallback (use_tt flag),
+	 * it indicates memory pressure - add a larger margin
+	 */
+	if (vram_man->use_tt) {
+		effective_percent = min_t(uint32_t, usage_percent + 10, 100);
+	}
+	/* Otherwise add a small margin for translation layer overhead */
+	else if (usage_percent > amdgpu_vega_vram_pressure_mid) {
+		effective_percent = min_t(uint32_t, usage_percent + 5, 100);
+	}
+
+	return effective_percent;
+}
+
+/**
+ * amdgpu_vega_optimize_buffer_placement - Optimize buffer placement for Vega GPUs
+ * @adev: AMDGPU device
+ * @bo: Buffer object (can be NULL for new buffer creation)
+ * @size: Buffer size in bytes
+ * @flags: Buffer creation flags
+ * @domain: Pointer to domain flags to be modified
+ *
+ * Optimizes buffer placement for Vega GPUs based on buffer characteristics
+ * and memory pressure. Balances native Vulkan/DX12 performance with
+ * translation layer (DXVK, VKD3D-Proton) needs.
+ *
+ * Returns: true if placement was optimized, false otherwise
+ */
+static bool amdgpu_vega_optimize_buffer_placement(struct amdgpu_device *adev,
+												  struct amdgpu_bo *bo,
+												  uint64_t size,
+												  uint64_t flags,
+												  uint32_t *domain)
+{
+	uint32_t vram_usage;
+
+	/* Validate inputs */
+	if (!adev || adev->asic_type != CHIP_VEGA10 || !domain)
+		return false;
+
+	/* Get current VRAM usage percentage */
+	vram_usage = amdgpu_vega_get_effective_vram_usage(adev);
+
+	/* Validate module parameters */
+	amdgpu_vega_vram_pressure_low = clamp(amdgpu_vega_vram_pressure_low, 0, 100);
+	amdgpu_vega_vram_pressure_mid = clamp(amdgpu_vega_vram_pressure_mid, 0, 100);
+	amdgpu_vega_vram_pressure_high = clamp(amdgpu_vega_vram_pressure_high, 0, 100);
+
+	/* Ensure thresholds are ordered properly */
+	if (amdgpu_vega_vram_pressure_mid < amdgpu_vega_vram_pressure_low)
+		amdgpu_vega_vram_pressure_mid = amdgpu_vega_vram_pressure_low;
+	if (amdgpu_vega_vram_pressure_high < amdgpu_vega_vram_pressure_mid)
+		amdgpu_vega_vram_pressure_high = amdgpu_vega_vram_pressure_mid;
+
+	/*
+	 * Category 1: Textures and framebuffer resources (VRAM_CONTIGUOUS)
+	 * These are critical for gaming performance and benefit greatly from HBM2 bandwidth
+	 */
+	if (flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS) {
+		if (vram_usage >= amdgpu_vega_vram_pressure_high &&
+			size < AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {
+			/* Small textures can use GTT under high pressure */
+			*domain = (*domain & ~AMDGPU_GEM_DOMAIN_VRAM) | AMDGPU_GEM_DOMAIN_GTT;
+			} else {
+				/* Keep textures in VRAM otherwise - critical for performance */
+				*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+			}
+			return true;
+	}
+
+	/*
+	 * Category 2: Compute resources (NO_CPU_ACCESS)
+	 * These benefit from HBM2 bandwidth for compute shaders
+	 */
+	if (flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS) {
+		if (vram_usage >= amdgpu_vega_vram_pressure_high &&
+			size > AMDGPU_VEGA_LARGE_BUFFER_SIZE) {
+			/* Very large compute buffers can use GTT under high pressure */
+			*domain = (*domain & ~AMDGPU_GEM_DOMAIN_VRAM) | AMDGPU_GEM_DOMAIN_GTT;
+			} else {
+				/* Keep compute resources in VRAM otherwise */
+				*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+			}
+			return true;
+	}
+
+	/*
+	 * Category 3: CPU-accessible resources (CPU_ACCESS_REQUIRED)
+	 * These need careful handling to balance CPU access with GPU performance
+	 * Critical for translation layers like DXVK and VKD3D-Proton
+	 */
+	if (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {
+		/* Small CPU-accessible buffers */
+		if (size <= AMDGPU_VEGA_SMALL_BUFFER_SIZE) {
+			if (vram_usage >= amdgpu_vega_vram_pressure_high) {
+				/* Only force GTT under high pressure */
+				*domain = (*domain & ~AMDGPU_GEM_DOMAIN_VRAM) | AMDGPU_GEM_DOMAIN_GTT;
+			} else if (*domain == 0) {
+				/* If domain is unspecified, prefer GTT for small CPU-accessible buffers */
+				*domain |= AMDGPU_GEM_DOMAIN_GTT;
+			}
+			/* Otherwise respect the user's domain choice */
+			return true;
+		}
+
+		/* Medium CPU-accessible buffers */
+		if (size <= AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {
+			if (vram_usage >= amdgpu_vega_vram_pressure_high) {
+				/* Only force GTT under high pressure */
+				*domain = (*domain & ~AMDGPU_GEM_DOMAIN_VRAM) | AMDGPU_GEM_DOMAIN_GTT;
+			} else if (*domain == 0) {
+				/* If domain is unspecified, prefer VRAM for medium CPU-accessible buffers
+				 * This helps DXVK performance for GPU-bound resources that need CPU access
+				 */
+				*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+			}
+			/* Otherwise respect the user's domain choice */
+			return true;
+		}
+
+		/* Large CPU-accessible buffers */
+		if (size > AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {
+			if (vram_usage >= amdgpu_vega_vram_pressure_high) {
+				/* Force GTT only under high pressure */
+				*domain = (*domain & ~AMDGPU_GEM_DOMAIN_VRAM) | AMDGPU_GEM_DOMAIN_GTT;
+			} else if (*domain == 0) {
+				/* If domain is unspecified, decide based on size */
+				if (size > AMDGPU_VEGA_LARGE_BUFFER_SIZE) {
+					/* Very large buffers default to GTT */
+					*domain |= AMDGPU_GEM_DOMAIN_GTT;
+				} else {
+					/* Large but not huge buffers default to VRAM
+					 * This helps with large textures in DXVK that need CPU access
+					 */
+					*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+				}
+			}
+			/* Otherwise respect the user's domain choice */
+			return true;
+		}
+	}
+
+	/*
+	 * Category 4: Generic resources (no special flags)
+	 * Use a balanced approach based on size and pressure
+	 */
+
+	/* Small generic buffers */
+	if (size <= AMDGPU_VEGA_SMALL_BUFFER_SIZE) {
+		if (vram_usage >= amdgpu_vega_vram_pressure_high) {
+			/* Under high pressure, use GTT */
+			*domain = (*domain & ~AMDGPU_GEM_DOMAIN_VRAM) | AMDGPU_GEM_DOMAIN_GTT;
+		} else if (*domain == 0) {
+			/* Default to VRAM for small generic buffers */
+			*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+		}
+		return true;
+	}
+
+	/* Medium generic buffers */
+	if (size <= AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {
+		if (vram_usage >= amdgpu_vega_vram_pressure_high) {
+			/* Under high pressure, use GTT */
+			*domain = (*domain & ~AMDGPU_GEM_DOMAIN_VRAM) | AMDGPU_GEM_DOMAIN_GTT;
+		} else if (*domain == 0) {
+			/* Default to VRAM for medium generic buffers */
+			*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+		}
+		return true;
+	}
+
+	/* Large generic buffers */
+	if (vram_usage >= amdgpu_vega_vram_pressure_high) {
+		/* Only under high pressure, use GTT for large generic buffers */
+		*domain = (*domain & ~AMDGPU_GEM_DOMAIN_VRAM) | AMDGPU_GEM_DOMAIN_GTT;
+	} else if (*domain == 0) {
+		/* Default to VRAM for large generic buffers unless pressure is high
+		 * This helps with large resources that might be GPU-bound
+		 */
+		*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+	}
+
+	return true;
+}
+
+/**
+ * amdgpu_vega_optimize_hbm2_bank_access - Optimize HBM2 bank distribution
+ * @adev: AMDGPU device
+ * @bo: Buffer object (can be NULL during buffer creation)
+ * @aligned_size: Pointer to the size to be potentially adjusted
+ * @alignment: Pointer to alignment to be potentially adjusted
+ *
+ * Optimize HBM2 bank access patterns for Vega by adjusting buffer alignment
+ * and size to minimize bank conflicts. Critical for maximizing memory bandwidth.
+ *
+ * Returns: true if optimization was applied, false otherwise
+ */
+static bool amdgpu_vega_optimize_hbm2_bank_access(struct amdgpu_device *adev,
+												  struct amdgpu_bo *bo,
+												  uint64_t *aligned_size,
+												  uint32_t *alignment)
+{
+	/* Validate input parameters */
+	if (!adev || adev->asic_type != CHIP_VEGA10 || !aligned_size || !alignment)
+		return false;
+
+	/* Check if size is reasonable to avoid integer overflow */
+	if (*aligned_size == 0 || *aligned_size > (16ULL * 1024 * 1024 * 1024))
+		return false;
+
+	/* For texture buffers, align to minimize HBM2 bank conflicts */
+	if (bo && bo->tbo.base.size > 0 && (bo->flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS)) {
+		/* For larger textures, 4KB alignment helps with HBM2 efficiency */
+		if (*aligned_size >= AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {
+			*alignment = max_t(uint32_t, *alignment, 4096);
+			*aligned_size = ALIGN(*aligned_size, 4096);
+			return true;
+		}
+	}
+
+	/* For compute buffers, align to optimize HBM2 throughput */
+	if (bo && bo->tbo.base.size > 0 && (bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {
+		/* 8KB alignment for compute helps with efficient access patterns */
+		if (*aligned_size >= AMDGPU_VEGA_LARGE_BUFFER_SIZE) {
+			*alignment = max_t(uint32_t, *alignment, 8192);
+			*aligned_size = ALIGN(*aligned_size, 8192);
+			return true;
+		}
+	}
+
+	/* When bo is NULL (during creation), use flags directly */
+	if (!bo && *aligned_size >= AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {
+		/* Use 4KB alignment as a reasonable default for medium/large buffers */
+		*alignment = max_t(uint32_t, *alignment, 4096);
+		*aligned_size = ALIGN(*aligned_size, 4096);
+		return true;
+	}
+
+	return false;
+}
+
+/**
+ * amdgpu_vega_determine_optimal_prefetch - Calculate optimal prefetch size
+ * @adev: AMDGPU device
+ * @bo: Buffer object
+ * @base_prefetch_pages: Base number of pages to prefetch
+ * @vram_usage: Current VRAM usage percentage
+ *
+ * Dynamically adjust prefetch size based on buffer characteristics and current
+ * VRAM usage for optimized HBM2 access patterns.
+ *
+ * Returns: Optimized number of pages to prefetch
+ */
+static unsigned int amdgpu_vega_determine_optimal_prefetch(
+	struct amdgpu_device *adev,
+	struct amdgpu_bo *bo,
+	unsigned int base_prefetch_pages,
+	uint32_t vram_usage)
+{
+	unsigned int prefetch_pages = base_prefetch_pages;
+	uint64_t size;
+	bool is_vram;
+
+	/* Validate input parameters */
+	if (!adev || !bo || adev->asic_type != CHIP_VEGA10)
+		return base_prefetch_pages;
+
+	/* Get buffer size and domain safely */
+	size = amdgpu_bo_size(bo);
+	if (size == 0)
+		return base_prefetch_pages;
+
+	is_vram = (bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) != 0;
+
+	/* Under high memory pressure, reduce prefetch to avoid thrashing */
+	if (vram_usage > amdgpu_vega_vram_pressure_high) {
+		return max_t(unsigned int, base_prefetch_pages / 2, 8);
+	}
+
+	/* Optimize prefetch for shader storage (SSBO) and image loads */
+	if (is_vram && (bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {
+		/* Large compute buffers benefit from aggressive prefetching */
+		if (size > AMDGPU_VEGA_LARGE_BUFFER_SIZE) {
+			return min_t(unsigned int, base_prefetch_pages * 2, 128);
+		}
+	}
+
+	/* Optimize texture fetch patterns */
+	if (is_vram && (bo->flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS)) {
+		/* Textures benefit from larger prefetch except under pressure */
+		if (vram_usage < amdgpu_vega_vram_pressure_mid) {
+			return min_t(unsigned int, base_prefetch_pages * 6 / 5, 64);
+		}
+	}
+
+	return prefetch_pages;
+}
+
+/**
+ * amdgpu_vega_should_use_async_fence - Determine if async fencing is appropriate
+ * @adev: AMDGPU device
+ * @bo: Buffer object
+ * @flags: Buffer flags
+ *
+ * Determines if a buffer object should use asynchronous fencing based on
+ * its characteristics and usage patterns. Enhances performance by reducing
+ * unnecessary synchronization.
+ *
+ * Returns: true if async fencing is safe to use
+ */
+static bool amdgpu_vega_should_use_async_fence(struct amdgpu_device *adev,
+											   struct amdgpu_bo *bo,
+											   uint64_t flags)
+{
+	uint64_t size;
+
+	/* Validate input parameters */
+	if (!adev || !bo || adev->asic_type != CHIP_VEGA10)
+		return false;
+
+	/* Get buffer size safely */
+	size = amdgpu_bo_size(bo);
+	if (size == 0)
+		return false;
+
+	/* Never use async fencing for buffers that explicitly need sync */
+	if (flags & AMDGPU_GEM_CREATE_EXPLICIT_SYNC)
+		return false;
+
+	/* For small GTT buffers with CPU access, async fencing is usually safe */
+	if ((bo->preferred_domains & AMDGPU_GEM_DOMAIN_GTT) &&
+		(flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) &&
+		size < AMDGPU_VEGA_SMALL_BUFFER_SIZE) {
+		return true;
+		}
+
+		/* For shader buffers that don't need CPU access, async is safe */
+		if ((bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) &&
+			(flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS) &&
+			!(flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)) {
+			return true;
+			}
+
+			/* Conservative default - use synchronous fencing */
+			return false;
+}
+
+/**
+ * amdgpu_vega_set_compute_placement - Set optimal placement for compute buffers
+ * @adev: AMDGPU device
+ * @bo: Buffer object
+ * @size: Buffer size
+ * @domain: Pointer to preferred domain
+ *
+ * Optimizes the placement of compute buffers for Vega GPUs.
+ *
+ * Note: Caller must hold the BO reservation lock.
+ *
+ * Returns true if optimization was applied.
+ */
+static bool amdgpu_vega_set_compute_placement(struct amdgpu_device *adev,
+											  struct amdgpu_bo *bo,
+											  uint64_t size,
+											  uint32_t *domain)
+{
+	uint32_t vram_usage;
+
+	/* Validate inputs */
+	if (!adev || !bo || !domain || adev->asic_type != CHIP_VEGA10)
+		return false;
+
+	/* Ensure the caller holds the BO reservation lock */
+	lockdep_assert_held(bo->tbo.base.resv);
+
+	/* Only apply to compute buffers */
+	if (!(bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS))
+		return false;
+
+	vram_usage = amdgpu_vega_get_effective_vram_usage(adev);
+
+	/* For large compute buffers, placement depends on memory pressure */
+	if (size > AMDGPU_VEGA_LARGE_BUFFER_SIZE) {
+		/* Under low pressure, use VRAM for large compute buffers */
+		if (vram_usage < amdgpu_vega_vram_pressure_low) {
+			*domain = AMDGPU_GEM_DOMAIN_VRAM;
+			/* Add GTT as a fallback domain */
+			bo->allowed_domains = AMDGPU_GEM_DOMAIN_VRAM | AMDGPU_GEM_DOMAIN_GTT;
+			return true;
+		}
+	}
+
+	/* For medium compute buffers, use VRAM until medium pressure */
+	if (size > AMDGPU_VEGA_MEDIUM_BUFFER_SIZE &&
+		size <= AMDGPU_VEGA_LARGE_BUFFER_SIZE) {
+		if (vram_usage < amdgpu_vega_vram_pressure_mid) {
+			*domain = AMDGPU_GEM_DOMAIN_VRAM;
+			/* Add GTT as a fallback domain */
+			bo->allowed_domains = AMDGPU_GEM_DOMAIN_VRAM | AMDGPU_GEM_DOMAIN_GTT;
+			return true;
+		}
+		}
+
+		/* For small compute buffers, always try VRAM first - critical for performance */
+		if (size <= AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {
+			*domain = AMDGPU_GEM_DOMAIN_VRAM;
+			/* Add GTT as a fallback domain */
+			bo->allowed_domains = AMDGPU_GEM_DOMAIN_VRAM | AMDGPU_GEM_DOMAIN_GTT;
+			return true;
+		}
+
+		return false;
+}
+
+/**
+ * amdgpu_vega_set_buffer_domains - Update buffer domains for optimal performance
+ * @adev: AMDGPU device
+ * @bo: Buffer object to update
+ *
+ * Ensures buffers have appropriate fallback domains and optimal settings
+ * for Vega's HBM2 memory. Specifically tuned for gaming workloads.
+ *
+ * Note: Caller must hold the BO reservation lock.
+ */
+static void amdgpu_vega_set_buffer_domains(struct amdgpu_device *adev,
+										   struct amdgpu_bo *bo)
+{
+	/* Validate inputs */
+	if (!adev || adev->asic_type != CHIP_VEGA10 || !bo)
+		return;
+
+	/* Ensure the caller holds the BO reservation lock */
+	lockdep_assert_held(bo->tbo.base.resv);
+
+	/*
+	 * Always ensure VRAM-only buffers have GTT as fallback
+	 * This prevents allocation failures under memory pressure
+	 */
+	if (bo->preferred_domains == AMDGPU_GEM_DOMAIN_VRAM) {
+		bo->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
+	}
+
+	/*
+	 * For compute buffers (shader storage, compute data):
+	 * These are critical for DX12/Vulkan game performance
+	 */
+	if (bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS) {
+		/* Only add VRAM preference if allowed in the domain mask */
+		if (bo->allowed_domains & AMDGPU_GEM_DOMAIN_VRAM) {
+			bo->preferred_domains |= AMDGPU_GEM_DOMAIN_VRAM;
+		}
+
+		/* Always ensure GTT is available as fallback */
+		bo->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
+	}
+
+	/*
+	 * For CPU-accessible buffers (common in DXVK translation):
+	 * These need efficient CPU access via GTT
+	 */
+	if (bo->flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {
+		bo->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
+	}
+}
+
+
+/**
+ * amdgpu_vega_optimize_for_workload - Apply workload-specific optimizations
+ * @adev: AMDGPU device
+ * @bo: Buffer object
+ * @flags: Buffer creation flags
+ *
+ * Apply specific optimizations based on likely workload patterns detected
+ * from buffer flags and properties. Especially effective for gaming,
+ * compute, and content creation workloads.
+ *
+ * Note: Caller must hold the BO reservation lock before calling this function.
+ *
+ * Returns: true if optimizations were applied
+ */
+static bool amdgpu_vega_optimize_for_workload(struct amdgpu_device *adev,
+											  struct amdgpu_bo *bo,
+											  uint64_t flags)
+{
+	uint64_t size;
+
+	/* Validate inputs */
+	if (!adev || !bo || adev->asic_type != CHIP_VEGA10)
+		return false;
+
+	/* Ensure the BO is valid */
+	if (!bo->tbo.base.dev)
+		return false;
+
+	/* Get buffer size safely */
+	size = amdgpu_bo_size(bo);
+	if (size == 0)
+		return false;
+
+	/* Caller must hold the reservation lock */
+	if (!dma_resv_is_locked(bo->tbo.base.resv))
+		return false;
+
+	/* Gaming workload optimization - prioritize texture bandwidth */
+	if ((flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS) &&
+		size >= AMDGPU_VEGA_MEDIUM_BUFFER_SIZE) {
+		/* Set VRAM domain but with GTT as fallback */
+		bo->preferred_domains = AMDGPU_GEM_DOMAIN_VRAM;
+	bo->allowed_domains = AMDGPU_GEM_DOMAIN_VRAM | AMDGPU_GEM_DOMAIN_GTT;
+	return true;
+		}
+
+		/* Compute workload optimization */
+		if ((flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS) &&
+			!(flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)) {
+			/* Prioritize VRAM for compute buffers */
+			bo->preferred_domains = AMDGPU_GEM_DOMAIN_VRAM;
+		bo->allowed_domains = AMDGPU_GEM_DOMAIN_VRAM | AMDGPU_GEM_DOMAIN_GTT;
+		return true;
+			}
+
+			/* API translation layer optimization (e.g., DXVK, VKD3D) */
+			if ((flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) &&
+				size <= AMDGPU_VEGA_SMALL_BUFFER_SIZE) {
+				/* Small CPU-accessible buffers work better in GTT for translation layers */
+				bo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;
+			bo->allowed_domains = AMDGPU_GEM_DOMAIN_GTT | AMDGPU_GEM_DOMAIN_VRAM;
+			return true;
+				}
+
+				return false;
+}
 
 static vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)
 {
-	struct ttm_buffer_object *bo = vmf->vma->vm_private_data;
-	struct drm_device *ddev = bo->base.dev;
+	struct ttm_buffer_object *bo;
+	struct drm_device *ddev;
 	vm_fault_t ret;
 	int idx;
 
+	// Start of original code section
+	bo = vmf->vma->vm_private_data;
+	if (!bo) // Added safety check from source
+		return VM_FAULT_SIGBUS;
+
+	ddev = bo->base.dev;
+	if (!ddev) // Added safety check from source
+		return VM_FAULT_SIGBUS;
+	// End of original code section (with safety checks added)
+
 	ret = ttm_bo_vm_reserve(bo, vmf);
 	if (ret)
 		return ret;
 
 	if (drm_dev_enter(ddev, &idx)) {
+		// Start of original code section
+		struct amdgpu_device *adev = drm_to_adev(ddev); // Moved up for Vega check
+		// End of original code section
+
 		ret = amdgpu_bo_fault_reserve_notify(bo);
 		if (ret) {
 			drm_dev_exit(idx);
 			goto unlock;
 		}
 
-		ret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot,
-					       TTM_BO_VM_NUM_PREFAULT);
+		/* Vega-specific prefetch optimizations for HBM2 memory */
+		if (adev && adev->asic_type == CHIP_VEGA10) {
+			struct amdgpu_bo *abo = ttm_to_amdgpu_bo(bo);
+			unsigned int prefetch_pages = TTM_BO_VM_NUM_PREFAULT;
+
+			if (abo) {
+				/* Get current VRAM usage for context-aware prefetch */
+				uint32_t vram_usage = amdgpu_vega_get_effective_vram_usage(adev);
+
+				/* Calculate optimal prefetch size based on buffer characteristics */
+				prefetch_pages = amdgpu_vega_determine_optimal_prefetch(
+					adev, abo, TTM_BO_VM_NUM_PREFAULT, vram_usage);
+			}
+
+			ret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot, prefetch_pages);
+		} else {
+			/* Standard fault handling for other GPUs */
+			ret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot,
+										   TTM_BO_VM_NUM_PREFAULT);
+		}
 
 		drm_dev_exit(idx);
 	} else {
@@ -72,7 +713,7 @@ static vm_fault_t amdgpu_gem_fault(struc
 	if (ret == VM_FAULT_RETRY && !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT))
 		return ret;
 
-unlock:
+	unlock:
 	dma_resv_unlock(bo->base.resv);
 	return ret;
 }
@@ -88,27 +729,60 @@ static void amdgpu_gem_object_free(struc
 {
 	struct amdgpu_bo *aobj = gem_to_amdgpu_bo(gobj);
 
-	amdgpu_hmm_unregister(aobj);
-	ttm_bo_put(&aobj->tbo);
+	if (aobj) { // Added check from source
+		amdgpu_hmm_unregister(aobj);
+		ttm_bo_put(&aobj->tbo);
+	}
 }
 
 int amdgpu_gem_object_create(struct amdgpu_device *adev, unsigned long size,
-			     int alignment, u32 initial_domain,
-			     u64 flags, enum ttm_bo_type type,
-			     struct dma_resv *resv,
-			     struct drm_gem_object **obj, int8_t xcp_id_plus1)
+							 int alignment, u32 initial_domain,
+							 u64 flags, enum ttm_bo_type type,
+							 struct dma_resv *resv,
+							 struct drm_gem_object **obj, int8_t xcp_id_plus1)
 {
 	struct amdgpu_bo *bo;
 	struct amdgpu_bo_user *ubo;
 	struct amdgpu_bo_param bp;
 	int r;
 
+	if (!adev || !obj) // Added check from source
+		return -EINVAL;
+
 	memset(&bp, 0, sizeof(bp));
 	*obj = NULL;
 	flags |= AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE;
 
-	bp.size = size;
+	/* Apply Vega-specific optimizations for buffer placement */
+	if (adev->asic_type == CHIP_VEGA10) {
+		/* Optimize domain for new buffer based on size, flags, and VRAM usage */
+		/* Note: size is unsigned long here, optimize_buffer_placement expects uint64_t */
+		/* Implicit conversion from unsigned long to uint64_t is usually safe */
+		amdgpu_vega_optimize_buffer_placement(adev, NULL, (uint64_t)size, flags, &initial_domain);
+	}
+
+	bp.size = size; // bp.size is unsigned long
 	bp.byte_align = alignment;
+
+	/* Apply Vega-specific optimizations for HBM2 bank access */
+	if (adev->asic_type == CHIP_VEGA10) {
+		/* Introduce temporary uint64_t to match helper function signature */
+		uint64_t temp_size = (uint64_t)bp.size;
+		uint32_t temp_align = bp.byte_align; /* Use temp for alignment too for symmetry */
+
+		/* Optimize buffer alignment and size for HBM2 memory */
+		if (amdgpu_vega_optimize_hbm2_bank_access(adev, NULL, &temp_size, &temp_align)) {
+			/* Copy potentially modified values back */
+			/* Careful about potential truncation if unsigned long is 32b and temp_size > 32b max */
+			/* However, bp.size originates from 'size' (unsigned long), so if temp_size */
+			/* grew beyond ULONG_MAX, it implies an issue earlier or requires bp.size */
+			/* to also be uint64_t. Assuming 64-bit kernel or size doesn't overflow ULONG_MAX. */
+			bp.size = (unsigned long)temp_size;
+			bp.byte_align = temp_align;
+		}
+	}
+
+
 	bp.type = type;
 	bp.resv = resv;
 	bp.preferred_domain = initial_domain;
@@ -122,6 +796,23 @@ int amdgpu_gem_object_create(struct amdg
 		return r;
 
 	bo = &ubo->bo;
+
+	/* Apply additional Vega-specific domain optimizations post-creation */
+	if (adev->asic_type == CHIP_VEGA10) {
+		/* For compute buffers, apply specialized optimization */
+		if (bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS) {
+			/* amdgpu_vega_set_compute_placement expects uint64_t size */
+			/* BO needs reservation lock here, which is held during creation */
+			amdgpu_vega_set_compute_placement(adev, bo, (uint64_t)size, &bo->preferred_domains);
+		}
+
+		/* Apply workload-specific optimizations - bo is already reserved during creation */
+		amdgpu_vega_optimize_for_workload(adev, bo, flags);
+
+		/* Apply general domain optimizations - needs lock */
+		amdgpu_vega_set_buffer_domains(adev, bo);
+	}
+
 	*obj = &bo->tbo.base;
 
 	return 0;
@@ -156,34 +847,58 @@ void amdgpu_gem_force_release(struct amd
  * case.
  */
 static int amdgpu_gem_object_open(struct drm_gem_object *obj,
-				  struct drm_file *file_priv)
+								  struct drm_file *file_priv)
 {
-	struct amdgpu_bo *abo = gem_to_amdgpu_bo(obj);
-	struct amdgpu_device *adev = amdgpu_ttm_adev(abo->tbo.bdev);
-	struct amdgpu_fpriv *fpriv = file_priv->driver_priv;
-	struct amdgpu_vm *vm = &fpriv->vm;
+	struct amdgpu_bo *abo;
+	struct amdgpu_device *adev;
+	struct amdgpu_fpriv *fpriv;
+	struct amdgpu_vm *vm;
 	struct amdgpu_bo_va *bo_va;
 	struct mm_struct *mm;
 	int r;
 
+	// Start of original code section (with source checks added)
+	if (!obj || !file_priv) // Added check from source
+		return -EINVAL;
+
+	abo = gem_to_amdgpu_bo(obj);
+	if (!abo) // Added check from source
+		return -EINVAL;
+
+	adev = amdgpu_ttm_adev(abo->tbo.bdev);
+	if (!adev) // Added check from source
+		return -EINVAL;
+
+	fpriv = file_priv->driver_priv;
+	if (!fpriv) // Added check from source
+		return -EINVAL;
+
+	vm = &fpriv->vm;
+	// End of original code section (with source checks added)
+
 	mm = amdgpu_ttm_tt_get_usermm(abo->tbo.ttm);
 	if (mm && mm != current->mm)
 		return -EPERM;
 
-	if (abo->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID &&
-	    !amdgpu_vm_is_bo_always_valid(vm, abo))
+	/* For BOs with ALWAYS_VALID flag, ensure that the VM mapping has been pre-validated */
+	// Merged check from source
+	if ((abo->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) &&
+		!amdgpu_vm_is_bo_always_valid(vm, abo))
 		return -EPERM;
 
 	r = amdgpu_bo_reserve(abo, false);
 	if (r)
 		return r;
 
-	amdgpu_vm_bo_update_shared(abo);
+	// Merged amdgpu_vm_bo_update_shared call from orig into find/add logic
 	bo_va = amdgpu_vm_bo_find(vm, abo);
-	if (!bo_va)
+	if (!bo_va) {
+		amdgpu_vm_bo_update_shared(abo); // From orig, call before add
 		bo_va = amdgpu_vm_bo_add(adev, vm, abo);
-	else
+	} else {
+		amdgpu_vm_bo_update_shared(abo); // From orig, call before increment
 		++bo_va->ref_count;
+	}
 	amdgpu_bo_unreserve(abo);
 
 	/* Validate and add eviction fence to DMABuf imports with dynamic
@@ -200,12 +915,37 @@ static int amdgpu_gem_object_open(struct
 	if (!vm->is_compute_context || !vm->process_info)
 		return 0;
 	if (!obj->import_attach ||
-	    !dma_buf_is_dynamic(obj->import_attach->dmabuf))
+		!dma_buf_is_dynamic(obj->import_attach->dmabuf))
 		return 0;
+
+	/* Lock process_info to run eviction fence validation */
 	mutex_lock_nested(&vm->process_info->lock, 1);
+	lockdep_assert_held(&vm->process_info->lock); // Added from source
+
 	if (!WARN_ON(!vm->process_info->eviction_fence)) {
-		r = amdgpu_amdkfd_bo_validate_and_fence(abo, AMDGPU_GEM_DOMAIN_GTT,
-							&vm->process_info->eviction_fence->base);
+		/* For Vega GPUs, use domain-optimized validation for imported buffers */
+		if (adev->asic_type == CHIP_VEGA10) {
+			uint32_t domain = AMDGPU_GEM_DOMAIN_GTT;
+
+			/* Apply HBM2-specific optimizations based on buffer properties */
+			if (abo->flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS ||
+				abo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS) {
+				/* Textures and compute buffers benefit from VRAM */
+				domain = AMDGPU_GEM_DOMAIN_VRAM;
+			/* Check VRAM pressure and fall back if needed */
+			if (amdgpu_vega_get_effective_vram_usage(adev) > amdgpu_vega_vram_pressure_high) {
+				domain = AMDGPU_GEM_DOMAIN_GTT;
+			}
+				}
+				r = amdgpu_amdkfd_bo_validate_and_fence(abo, domain,
+														&vm->process_info->eviction_fence->base);
+		} else {
+			/* Original non-Vega path */
+			r = amdgpu_amdkfd_bo_validate_and_fence(abo,
+													AMDGPU_GEM_DOMAIN_GTT,
+										   &vm->process_info->eviction_fence->base);
+		}
+
 		if (r) {
 			struct amdgpu_task_info *ti = amdgpu_vm_get_task_info_vm(vm);
 
@@ -222,18 +962,43 @@ static int amdgpu_gem_object_open(struct
 }
 
 static void amdgpu_gem_object_close(struct drm_gem_object *obj,
-				    struct drm_file *file_priv)
+									struct drm_file *file_priv)
 {
-	struct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);
-	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);
-	struct amdgpu_fpriv *fpriv = file_priv->driver_priv;
-	struct amdgpu_vm *vm = &fpriv->vm;
-
+	struct amdgpu_bo *bo;
+	struct amdgpu_device *adev;
+	struct amdgpu_fpriv *fpriv;
+	struct amdgpu_vm *vm;
 	struct dma_fence *fence = NULL;
 	struct amdgpu_bo_va *bo_va;
 	struct drm_exec exec;
-	long r;
+	long r = 0; // Initialize r from source
+	bool use_async = false; // Default to sync fence
+
+	// Start of original code section (with source checks)
+	if (!obj || !file_priv) // Added check from source
+		return;
+
+	bo = gem_to_amdgpu_bo(obj);
+	if (!bo) // Added check from source
+		return;
+
+	adev = amdgpu_ttm_adev(bo->tbo.bdev);
+	if (!adev) // Added check from source
+		return;
+
+	fpriv = file_priv->driver_priv;
+	if (!fpriv) // Added check from source
+		return;
 
+	vm = &fpriv->vm;
+	// End of original code section (with source checks)
+
+	/* For Vega, determine if we can use async fencing */
+	if (adev->asic_type == CHIP_VEGA10) {
+		use_async = amdgpu_vega_should_use_async_fence(adev, bo, bo->flags);
+	}
+
+	/* Use the DRM exec framework to acquire multi-object locks */
 	drm_exec_init(&exec, DRM_EXEC_IGNORE_DUPLICATES, 0);
 	drm_exec_until_all_locked(&exec) {
 		r = drm_exec_prepare_obj(&exec, &bo->tbo.base, 1);
@@ -248,27 +1013,41 @@ static void amdgpu_gem_object_close(stru
 	}
 
 	bo_va = amdgpu_vm_bo_find(vm, bo);
-	if (!bo_va || --bo_va->ref_count)
+	// Start of combined logic from source and orig
+	if (!bo_va) // Simplified check from source
+		goto out_unlock;
+
+	/* Use proper reference counting */
+	if (--bo_va->ref_count > 0) // Logic from source
 		goto out_unlock;
 
-	amdgpu_vm_bo_del(adev, bo_va);
-	amdgpu_vm_bo_update_shared(bo);
+	/* Remove the BO-VA mapping, as reference count is now zero */
+	amdgpu_vm_bo_del(adev, bo_va); // Logic from source
+	amdgpu_vm_bo_update_shared(bo); // Added from orig
+	// End of combined logic
+
 	if (!amdgpu_vm_ready(vm))
 		goto out_unlock;
 
 	r = amdgpu_vm_clear_freed(adev, vm, &fence);
-	if (unlikely(r < 0))
-		dev_err(adev->dev, "failed to clear page "
-			"tables on GEM object close (%ld)\n", r);
+	if (unlikely(r < 0)) {
+		// Use error message from source
+		dev_err(adev->dev, "failed to clear page tables on GEM object close (%ld)\n", r);
+		goto out_unlock;
+	}
 	if (r || !fence)
 		goto out_unlock;
 
-	amdgpu_bo_fence(bo, fence, true);
+	/* Fence the BO - use async for eligible buffers to improve performance */
+	amdgpu_bo_fence(bo, fence, use_async); // Use calculated use_async flag
 	dma_fence_put(fence);
 
-out_unlock:
-	if (r)
-		dev_err(adev->dev, "leaking bo va (%ld)\n", r);
+	out_unlock:
+	if (r) {
+		// Use error message from source
+		dev_err(adev->dev, "Error in GEM object close for pid %d, potential leak of bo_va (%ld)\n",
+				task_pid_nr(current), r);
+	}
 	drm_exec_fini(&exec);
 }
 
@@ -287,7 +1066,7 @@ static int amdgpu_gem_object_mmap(struct
 	 * becoming writable and makes is_cow_mapping(vm_flags) false.
 	 */
 	if (is_cow_mapping(vma->vm_flags) &&
-	    !(vma->vm_flags & VM_ACCESS_FLAGS))
+		!(vma->vm_flags & VM_ACCESS_FLAGS))
 		vm_flags_clear(vma, VM_MAYWRITE);
 
 	return drm_gem_ttm_mmap(obj, vma);
@@ -308,7 +1087,7 @@ const struct drm_gem_object_funcs amdgpu
  * GEM ioctls.
  */
 int amdgpu_gem_create_ioctl(struct drm_device *dev, void *data,
-			    struct drm_file *filp)
+							struct drm_file *filp)
 {
 	struct amdgpu_device *adev = drm_to_adev(dev);
 	struct amdgpu_fpriv *fpriv = filp->driver_priv;
@@ -326,15 +1105,16 @@ int amdgpu_gem_create_ioctl(struct drm_d
 		return -EINVAL;
 
 	/* reject invalid gem flags */
+	// Flag list updated from orig
 	if (flags & ~(AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |
-		      AMDGPU_GEM_CREATE_NO_CPU_ACCESS |
-		      AMDGPU_GEM_CREATE_CPU_GTT_USWC |
-		      AMDGPU_GEM_CREATE_VRAM_CLEARED |
-		      AMDGPU_GEM_CREATE_VM_ALWAYS_VALID |
-		      AMDGPU_GEM_CREATE_EXPLICIT_SYNC |
-		      AMDGPU_GEM_CREATE_ENCRYPTED |
-		      AMDGPU_GEM_CREATE_GFX12_DCC |
-		      AMDGPU_GEM_CREATE_DISCARDABLE))
+		AMDGPU_GEM_CREATE_NO_CPU_ACCESS |
+		AMDGPU_GEM_CREATE_CPU_GTT_USWC |
+		AMDGPU_GEM_CREATE_VRAM_CLEARED |
+		AMDGPU_GEM_CREATE_VM_ALWAYS_VALID |
+		AMDGPU_GEM_CREATE_EXPLICIT_SYNC |
+		AMDGPU_GEM_CREATE_ENCRYPTED |
+		AMDGPU_GEM_CREATE_GFX12_DCC |
+		AMDGPU_GEM_CREATE_DISCARDABLE))
 		return -EINVAL;
 
 	/* reject invalid gem domains */
@@ -351,7 +1131,7 @@ int amdgpu_gem_create_ioctl(struct drm_d
 
 	/* create a gem object to contain this object in */
 	if (args->in.domains & (AMDGPU_GEM_DOMAIN_GDS |
-	    AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA)) {
+		AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA)) {
 		if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
 			/* if gds bo is created from user space, it must be
 			 * passed to bo list
@@ -360,45 +1140,46 @@ int amdgpu_gem_create_ioctl(struct drm_d
 			return -EINVAL;
 		}
 		flags |= AMDGPU_GEM_CREATE_NO_CPU_ACCESS;
-	}
-
-	if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
-		r = amdgpu_bo_reserve(vm->root.bo, false);
-		if (r)
-			return r;
+		}
 
-		resv = vm->root.bo->tbo.base.resv;
-	}
+		if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
+			r = amdgpu_bo_reserve(vm->root.bo, false);
+			if (r)
+				return r;
 
-	initial_domain = (u32)(0xffffffff & args->in.domains);
-retry:
-	r = amdgpu_gem_object_create(adev, size, args->in.alignment,
-				     initial_domain,
-				     flags, ttm_bo_type_device, resv, &gobj, fpriv->xcp_id + 1);
-	if (r && r != -ERESTARTSYS) {
-		if (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {
-			flags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
-			goto retry;
+			resv = vm->root.bo->tbo.base.resv;
 		}
 
-		if (initial_domain == AMDGPU_GEM_DOMAIN_VRAM) {
-			initial_domain |= AMDGPU_GEM_DOMAIN_GTT;
-			goto retry;
+		initial_domain = (u32)(0xffffffff & args->in.domains);
+		retry:
+		// Call to amdgpu_gem_object_create now includes Vega optimizations
+		r = amdgpu_gem_object_create(adev, size, args->in.alignment,
+									 initial_domain,
+							   flags, ttm_bo_type_device, resv, &gobj, fpriv->xcp_id + 1);
+		if (r && r != -ERESTARTSYS) {
+			if (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {
+				flags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
+				goto retry;
+			}
+
+			if (initial_domain == AMDGPU_GEM_DOMAIN_VRAM) {
+				initial_domain |= AMDGPU_GEM_DOMAIN_GTT;
+				goto retry;
+			}
+			DRM_DEBUG("Failed to allocate GEM object (%llu, %d, %llu, %d)\n",
+					  size, initial_domain, args->in.alignment, r);
 		}
-		DRM_DEBUG("Failed to allocate GEM object (%llu, %d, %llu, %d)\n",
-				size, initial_domain, args->in.alignment, r);
-	}
 
-	if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
-		if (!r) {
-			struct amdgpu_bo *abo = gem_to_amdgpu_bo(gobj);
+		if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
+			if (!r) {
+				struct amdgpu_bo *abo = gem_to_amdgpu_bo(gobj);
 
-			abo->parent = amdgpu_bo_ref(vm->root.bo);
+				abo->parent = amdgpu_bo_ref(vm->root.bo);
+			}
+			amdgpu_bo_unreserve(vm->root.bo);
 		}
-		amdgpu_bo_unreserve(vm->root.bo);
-	}
-	if (r)
-		return r;
+		if (r)
+			return r;
 
 	r = drm_gem_handle_create(filp, gobj, &handle);
 	/* drop reference from allocate - handle holds it now */
@@ -412,7 +1193,7 @@ retry:
 }
 
 int amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,
-			     struct drm_file *filp)
+							 struct drm_file *filp)
 {
 	struct ttm_operation_ctx ctx = { true, false };
 	struct amdgpu_device *adev = drm_to_adev(dev);
@@ -430,23 +1211,25 @@ int amdgpu_gem_userptr_ioctl(struct drm_
 		return -EINVAL;
 
 	/* reject unknown flag values */
+	// Flag list from orig
 	if (args->flags & ~(AMDGPU_GEM_USERPTR_READONLY |
-	    AMDGPU_GEM_USERPTR_ANONONLY | AMDGPU_GEM_USERPTR_VALIDATE |
-	    AMDGPU_GEM_USERPTR_REGISTER))
+		AMDGPU_GEM_USERPTR_ANONONLY | AMDGPU_GEM_USERPTR_VALIDATE |
+		AMDGPU_GEM_USERPTR_REGISTER))
 		return -EINVAL;
 
 	if (!(args->flags & AMDGPU_GEM_USERPTR_READONLY) &&
-	     !(args->flags & AMDGPU_GEM_USERPTR_REGISTER)) {
+		!(args->flags & AMDGPU_GEM_USERPTR_REGISTER)) {
 
 		/* if we want to write to it we must install a MMU notifier */
 		return -EACCES;
-	}
+		}
 
-	/* create a gem object to contain this object in */
-	r = amdgpu_gem_object_create(adev, args->size, 0, AMDGPU_GEM_DOMAIN_CPU,
-				     0, ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
-	if (r)
-		return r;
+		/* create a gem object to contain this object in */
+		// Call to amdgpu_gem_object_create now includes Vega optimizations
+		r = amdgpu_gem_object_create(adev, args->size, 0, AMDGPU_GEM_DOMAIN_CPU,
+									 0, ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
+		if (r)
+			return r;
 
 	bo = gem_to_amdgpu_bo(gobj);
 	bo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;
@@ -461,7 +1244,7 @@ int amdgpu_gem_userptr_ioctl(struct drm_
 
 	if (args->flags & AMDGPU_GEM_USERPTR_VALIDATE) {
 		r = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages,
-						 &range);
+										 &range);
 		if (r)
 			goto release_object;
 
@@ -482,19 +1265,19 @@ int amdgpu_gem_userptr_ioctl(struct drm_
 
 	args->handle = handle;
 
-user_pages_done:
+	user_pages_done:
 	if (args->flags & AMDGPU_GEM_USERPTR_VALIDATE)
 		amdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, range);
 
-release_object:
+	release_object:
 	drm_gem_object_put(gobj);
 
 	return r;
 }
 
 int amdgpu_mode_dumb_mmap(struct drm_file *filp,
-			  struct drm_device *dev,
-			  uint32_t handle, uint64_t *offset_p)
+						  struct drm_device *dev,
+						  uint32_t handle, uint64_t *offset_p)
 {
 	struct drm_gem_object *gobj;
 	struct amdgpu_bo *robj;
@@ -505,17 +1288,17 @@ int amdgpu_mode_dumb_mmap(struct drm_fil
 
 	robj = gem_to_amdgpu_bo(gobj);
 	if (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm) ||
-	    (robj->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {
+		(robj->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {
 		drm_gem_object_put(gobj);
-		return -EPERM;
-	}
-	*offset_p = amdgpu_bo_mmap_offset(robj);
-	drm_gem_object_put(gobj);
-	return 0;
+	return -EPERM;
+		}
+		*offset_p = amdgpu_bo_mmap_offset(robj);
+		drm_gem_object_put(gobj);
+		return 0;
 }
 
 int amdgpu_gem_mmap_ioctl(struct drm_device *dev, void *data,
-			  struct drm_file *filp)
+						  struct drm_file *filp)
 {
 	union drm_amdgpu_gem_mmap *args = data;
 	uint32_t handle = args->in.handle;
@@ -553,7 +1336,7 @@ unsigned long amdgpu_gem_timeout(uint64_
 }
 
 int amdgpu_gem_wait_idle_ioctl(struct drm_device *dev, void *data,
-			      struct drm_file *filp)
+							   struct drm_file *filp)
 {
 	union drm_amdgpu_gem_wait_idle *args = data;
 	struct drm_gem_object *gobj;
@@ -569,7 +1352,7 @@ int amdgpu_gem_wait_idle_ioctl(struct dr
 
 	robj = gem_to_amdgpu_bo(gobj);
 	ret = dma_resv_wait_timeout(robj->tbo.base.resv, DMA_RESV_USAGE_READ,
-				    true, timeout);
+								true, timeout);
 
 	/* ret == 0 means not signaled,
 	 * ret > 0 means signaled
@@ -586,7 +1369,7 @@ int amdgpu_gem_wait_idle_ioctl(struct dr
 }
 
 int amdgpu_gem_metadata_ioctl(struct drm_device *dev, void *data,
-				struct drm_file *filp)
+							  struct drm_file *filp)
 {
 	struct drm_amdgpu_gem_metadata *args = data;
 	struct drm_gem_object *gobj;
@@ -606,9 +1389,9 @@ int amdgpu_gem_metadata_ioctl(struct drm
 	if (args->op == AMDGPU_GEM_METADATA_OP_GET_METADATA) {
 		amdgpu_bo_get_tiling_flags(robj, &args->data.tiling_info);
 		r = amdgpu_bo_get_metadata(robj, args->data.data,
-					   sizeof(args->data.data),
-					   &args->data.data_size_bytes,
-					   &args->data.flags);
+								   sizeof(args->data.data),
+								   &args->data.data_size_bytes,
+							 &args->data.flags);
 	} else if (args->op == AMDGPU_GEM_METADATA_OP_SET_METADATA) {
 		if (args->data.data_size_bytes > sizeof(args->data.data)) {
 			r = -EINVAL;
@@ -617,13 +1400,13 @@ int amdgpu_gem_metadata_ioctl(struct drm
 		r = amdgpu_bo_set_tiling_flags(robj, args->data.tiling_info);
 		if (!r)
 			r = amdgpu_bo_set_metadata(robj, args->data.data,
-						   args->data.data_size_bytes,
-						   args->data.flags);
+									   args->data.data_size_bytes,
+							  args->data.flags);
 	}
 
-unreserve:
+	unreserve:
 	amdgpu_bo_unreserve(robj);
-out:
+	out:
 	drm_gem_object_put(gobj);
 	return r;
 }
@@ -640,9 +1423,9 @@ out:
  * vital here, so they are not reported back to userspace.
  */
 static void amdgpu_gem_va_update_vm(struct amdgpu_device *adev,
-				    struct amdgpu_vm *vm,
-				    struct amdgpu_bo_va *bo_va,
-				    uint32_t operation)
+									struct amdgpu_vm *vm,
+									struct amdgpu_bo_va *bo_va,
+									uint32_t operation)
 {
 	int r;
 
@@ -654,15 +1437,15 @@ static void amdgpu_gem_va_update_vm(stru
 		goto error;
 
 	if (operation == AMDGPU_VA_OP_MAP ||
-	    operation == AMDGPU_VA_OP_REPLACE) {
+		operation == AMDGPU_VA_OP_REPLACE) {
 		r = amdgpu_vm_bo_update(adev, bo_va, false);
-		if (r)
-			goto error;
-	}
+	if (r)
+		goto error;
+		}
 
-	r = amdgpu_vm_update_pdes(adev, vm, false);
+		r = amdgpu_vm_update_pdes(adev, vm, false);
 
-error:
+	error:
 	if (r && r != -ERESTARTSYS)
 		DRM_ERROR("Couldn't update BO_VA (%d)\n", r);
 }
@@ -690,22 +1473,24 @@ uint64_t amdgpu_gem_va_map_flags(struct
 	if (flags & AMDGPU_VM_PAGE_NOALLOC)
 		pte_flag |= AMDGPU_PTE_NOALLOC;
 
-	if (adev->gmc.gmc_funcs->map_mtype)
+	/* Simplified null check - only check the function pointer */
+	if (adev->gmc.gmc_funcs && adev->gmc.gmc_funcs->map_mtype) { /* Fixed Line */
 		pte_flag |= amdgpu_gmc_map_mtype(adev,
-						 flags & AMDGPU_VM_MTYPE_MASK);
+										 flags & AMDGPU_VM_MTYPE_MASK);
+	} /* Fixed Line */
 
 	return pte_flag;
 }
 
 int amdgpu_gem_va_ioctl(struct drm_device *dev, void *data,
-			  struct drm_file *filp)
+						struct drm_file *filp)
 {
 	const uint32_t valid_flags = AMDGPU_VM_DELAY_UPDATE |
-		AMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE |
-		AMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_MTYPE_MASK |
-		AMDGPU_VM_PAGE_NOALLOC;
+	AMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE |
+	AMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_MTYPE_MASK |
+	AMDGPU_VM_PAGE_NOALLOC;
 	const uint32_t prt_flags = AMDGPU_VM_DELAY_UPDATE |
-		AMDGPU_VM_PAGE_PRT;
+	AMDGPU_VM_PAGE_PRT;
 
 	struct drm_amdgpu_gem_va *args = data;
 	struct drm_gem_object *gobj;
@@ -720,126 +1505,138 @@ int amdgpu_gem_va_ioctl(struct drm_devic
 
 	if (args->va_address < AMDGPU_VA_RESERVED_BOTTOM) {
 		dev_dbg(dev->dev,
-			"va_address 0x%llx is in reserved area 0x%llx\n",
-			args->va_address, AMDGPU_VA_RESERVED_BOTTOM);
+				"va_address 0x%llx is in reserved area 0x%llx\n",
+		  args->va_address, AMDGPU_VA_RESERVED_BOTTOM);
 		return -EINVAL;
 	}
 
 	if (args->va_address >= AMDGPU_GMC_HOLE_START &&
-	    args->va_address < AMDGPU_GMC_HOLE_END) {
+		args->va_address < AMDGPU_GMC_HOLE_END) {
 		dev_dbg(dev->dev,
-			"va_address 0x%llx is in VA hole 0x%llx-0x%llx\n",
-			args->va_address, AMDGPU_GMC_HOLE_START,
-			AMDGPU_GMC_HOLE_END);
+				"va_address 0x%llx is in VA hole 0x%llx-0x%llx\n",
+		  args->va_address, AMDGPU_GMC_HOLE_START,
+		  AMDGPU_GMC_HOLE_END);
 		return -EINVAL;
-	}
+		}
 
-	args->va_address &= AMDGPU_GMC_HOLE_MASK;
+		args->va_address &= AMDGPU_GMC_HOLE_MASK;
 
 	vm_size = adev->vm_manager.max_pfn * AMDGPU_GPU_PAGE_SIZE;
 	vm_size -= AMDGPU_VA_RESERVED_TOP;
 	if (args->va_address + args->map_size > vm_size) {
 		dev_dbg(dev->dev,
-			"va_address 0x%llx is in top reserved area 0x%llx\n",
-			args->va_address + args->map_size, vm_size);
+				"va_address 0x%llx is in top reserved area 0x%llx\n",
+		  args->va_address + args->map_size, vm_size);
 		return -EINVAL;
 	}
 
 	if ((args->flags & ~valid_flags) && (args->flags & ~prt_flags)) {
 		dev_dbg(dev->dev, "invalid flags combination 0x%08X\n",
-			args->flags);
+				args->flags);
 		return -EINVAL;
 	}
 
 	switch (args->operation) {
-	case AMDGPU_VA_OP_MAP:
-	case AMDGPU_VA_OP_UNMAP:
-	case AMDGPU_VA_OP_CLEAR:
-	case AMDGPU_VA_OP_REPLACE:
-		break;
-	default:
-		dev_dbg(dev->dev, "unsupported operation %d\n",
-			args->operation);
-		return -EINVAL;
+		case AMDGPU_VA_OP_MAP:
+		case AMDGPU_VA_OP_UNMAP:
+		case AMDGPU_VA_OP_CLEAR:
+		case AMDGPU_VA_OP_REPLACE:
+			break;
+		default:
+			dev_dbg(dev->dev, "unsupported operation %d\n",
+					args->operation);
+			return -EINVAL;
 	}
 
 	if ((args->operation != AMDGPU_VA_OP_CLEAR) &&
-	    !(args->flags & AMDGPU_VM_PAGE_PRT)) {
+		!(args->flags & AMDGPU_VM_PAGE_PRT)) {
 		gobj = drm_gem_object_lookup(filp, args->handle);
-		if (gobj == NULL)
-			return -ENOENT;
-		abo = gem_to_amdgpu_bo(gobj);
-	} else {
-		gobj = NULL;
-		abo = NULL;
-	}
+	if (gobj == NULL) { /* Fixed Line */
+		return -ENOENT;
+	} /* Fixed Line */
+	abo = gem_to_amdgpu_bo(gobj);
+		} else {
+			gobj = NULL;
+			abo = NULL;
+		}
 
-	drm_exec_init(&exec, DRM_EXEC_INTERRUPTIBLE_WAIT |
-		      DRM_EXEC_IGNORE_DUPLICATES, 0);
-	drm_exec_until_all_locked(&exec) {
-		if (gobj) {
-			r = drm_exec_lock_obj(&exec, gobj);
+		drm_exec_init(&exec, DRM_EXEC_INTERRUPTIBLE_WAIT |
+		DRM_EXEC_IGNORE_DUPLICATES, 0);
+		drm_exec_until_all_locked(&exec) {
+			if (gobj) {
+				r = drm_exec_lock_obj(&exec, gobj);
+				drm_exec_retry_on_contention(&exec);
+				if (unlikely(r))
+					goto error;
+			}
+
+			r = amdgpu_vm_lock_pd(&fpriv->vm, &exec, 2);
 			drm_exec_retry_on_contention(&exec);
 			if (unlikely(r))
 				goto error;
 		}
 
-		r = amdgpu_vm_lock_pd(&fpriv->vm, &exec, 2);
-		drm_exec_retry_on_contention(&exec);
-		if (unlikely(r))
-			goto error;
-	}
-
-	if (abo) {
-		bo_va = amdgpu_vm_bo_find(&fpriv->vm, abo);
-		if (!bo_va) {
-			r = -ENOENT;
-			goto error;
+		if (abo) {
+			bo_va = amdgpu_vm_bo_find(&fpriv->vm, abo);
+			if (!bo_va) {
+				r = -ENOENT;
+				goto error;
+			}
+		} else if (args->operation != AMDGPU_VA_OP_CLEAR) {
+			bo_va = fpriv->prt_va;
+		} else {
+			bo_va = NULL;
 		}
-	} else if (args->operation != AMDGPU_VA_OP_CLEAR) {
-		bo_va = fpriv->prt_va;
-	} else {
-		bo_va = NULL;
-	}
-
-	switch (args->operation) {
-	case AMDGPU_VA_OP_MAP:
-		va_flags = amdgpu_gem_va_map_flags(adev, args->flags);
-		r = amdgpu_vm_bo_map(adev, bo_va, args->va_address,
-				     args->offset_in_bo, args->map_size,
-				     va_flags);
-		break;
-	case AMDGPU_VA_OP_UNMAP:
-		r = amdgpu_vm_bo_unmap(adev, bo_va, args->va_address);
-		break;
 
-	case AMDGPU_VA_OP_CLEAR:
-		r = amdgpu_vm_bo_clear_mappings(adev, &fpriv->vm,
-						args->va_address,
-						args->map_size);
-		break;
-	case AMDGPU_VA_OP_REPLACE:
-		va_flags = amdgpu_gem_va_map_flags(adev, args->flags);
-		r = amdgpu_vm_bo_replace_map(adev, bo_va, args->va_address,
-					     args->offset_in_bo, args->map_size,
-					     va_flags);
-		break;
-	default:
-		break;
-	}
-	if (!r && !(args->flags & AMDGPU_VM_DELAY_UPDATE) && !adev->debug_vm)
-		amdgpu_gem_va_update_vm(adev, &fpriv->vm, bo_va,
-					args->operation);
+		/* Apply Vega-specific workload optimization if mapping/replacing */
+		if (abo && adev->asic_type == CHIP_VEGA10) {
+			/* Buffer is already locked by drm_exec at this point, verify lock */
+			if (dma_resv_is_locked(abo->tbo.base.resv)) {
+				/* Apply workload-specific optimizations */
+				amdgpu_vega_optimize_for_workload(adev, abo, abo->flags);
+			}
+		}
 
-error:
-	drm_exec_fini(&exec);
-	drm_gem_object_put(gobj);
+		switch (args->operation) {
+			case AMDGPU_VA_OP_MAP:
+				va_flags = amdgpu_gem_va_map_flags(adev, args->flags);
+				r = amdgpu_vm_bo_map(adev, bo_va, args->va_address,
+									 args->offset_in_bo, args->map_size,
+						 va_flags);
+				break;
+			case AMDGPU_VA_OP_UNMAP:
+				r = amdgpu_vm_bo_unmap(adev, bo_va, args->va_address);
+				break;
+
+			case AMDGPU_VA_OP_CLEAR:
+				r = amdgpu_vm_bo_clear_mappings(adev, &fpriv->vm,
+												args->va_address,
+									args->map_size);
+				break;
+			case AMDGPU_VA_OP_REPLACE:
+				va_flags = amdgpu_gem_va_map_flags(adev, args->flags);
+				r = amdgpu_vm_bo_replace_map(adev, bo_va, args->va_address,
+											 args->offset_in_bo, args->map_size,
+								 va_flags);
+				break;
+			default:
+				break;
+		}
+		if (!r && !(args->flags & AMDGPU_VM_DELAY_UPDATE) && !adev->debug_vm)
+			amdgpu_gem_va_update_vm(adev, &fpriv->vm, bo_va,
+									args->operation);
+
+			error:
+			drm_exec_fini(&exec);
+		if (gobj) // Check added from source
+			drm_gem_object_put(gobj);
 	return r;
 }
 
 int amdgpu_gem_op_ioctl(struct drm_device *dev, void *data,
-			struct drm_file *filp)
+						struct drm_file *filp)
 {
+	struct amdgpu_device *adev = drm_to_adev(dev); // Added for Vega check
 	struct drm_amdgpu_gem_op *args = data;
 	struct drm_gem_object *gobj;
 	struct amdgpu_vm_bo_base *base;
@@ -857,81 +1654,87 @@ int amdgpu_gem_op_ioctl(struct drm_devic
 		goto out;
 
 	switch (args->op) {
-	case AMDGPU_GEM_OP_GET_GEM_CREATE_INFO: {
-		struct drm_amdgpu_gem_create_in info;
-		void __user *out = u64_to_user_ptr(args->value);
-
-		info.bo_size = robj->tbo.base.size;
-		info.alignment = robj->tbo.page_alignment << PAGE_SHIFT;
-		info.domains = robj->preferred_domains;
-		info.domain_flags = robj->flags;
-		amdgpu_bo_unreserve(robj);
-		if (copy_to_user(out, &info, sizeof(info)))
-			r = -EFAULT;
-		break;
-	}
-	case AMDGPU_GEM_OP_SET_PLACEMENT:
-		if (robj->tbo.base.import_attach &&
-		    args->value & AMDGPU_GEM_DOMAIN_VRAM) {
-			r = -EINVAL;
+		case AMDGPU_GEM_OP_GET_GEM_CREATE_INFO: {
+			struct drm_amdgpu_gem_create_in info;
+			void __user *out = u64_to_user_ptr(args->value);
+
+			info.bo_size = robj->tbo.base.size;
+			info.alignment = robj->tbo.page_alignment << PAGE_SHIFT;
+			info.domains = robj->preferred_domains;
+			info.domain_flags = robj->flags;
 			amdgpu_bo_unreserve(robj);
+			if (copy_to_user(out, &info, sizeof(info)))
+				r = -EFAULT;
 			break;
 		}
-		if (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm)) {
-			r = -EPERM;
+		case AMDGPU_GEM_OP_SET_PLACEMENT:
+			if (robj->tbo.base.import_attach &&
+				args->value & AMDGPU_GEM_DOMAIN_VRAM) {
+				r = -EINVAL;
 			amdgpu_bo_unreserve(robj);
 			break;
-		}
-		for (base = robj->vm_bo; base; base = base->next)
-			if (amdgpu_xgmi_same_hive(amdgpu_ttm_adev(robj->tbo.bdev),
-				amdgpu_ttm_adev(base->vm->root.bo->tbo.bdev))) {
-				r = -EINVAL;
-				amdgpu_bo_unreserve(robj);
+				}
+				if (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm)) {
+					r = -EPERM;
+					amdgpu_bo_unreserve(robj);
+					break;
+				}
+				for (base = robj->vm_bo; base; base = base->next)
+					if (amdgpu_xgmi_same_hive(amdgpu_ttm_adev(robj->tbo.bdev),
+						amdgpu_ttm_adev(base->vm->root.bo->tbo.bdev))) {
+						r = -EINVAL;
+					amdgpu_bo_unreserve(robj);
 				goto out;
-			}
+						}
 
 
-		robj->preferred_domains = args->value & (AMDGPU_GEM_DOMAIN_VRAM |
-							AMDGPU_GEM_DOMAIN_GTT |
-							AMDGPU_GEM_DOMAIN_CPU);
-		robj->allowed_domains = robj->preferred_domains;
-		if (robj->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)
-			robj->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
+						robj->preferred_domains = args->value & (AMDGPU_GEM_DOMAIN_VRAM |
+						AMDGPU_GEM_DOMAIN_GTT |
+						AMDGPU_GEM_DOMAIN_CPU);
+						robj->allowed_domains = robj->preferred_domains;
+						if (robj->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)
+							robj->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
+
+		/* Apply Vega-specific workload optimizations - buffer is already reserved */
+		if (adev->asic_type == CHIP_VEGA10) {
+			amdgpu_vega_optimize_for_workload(adev, robj, robj->flags);
+		}
 
-		if (robj->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID)
-			amdgpu_vm_bo_invalidate(robj, true);
+		if (robj->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) { /* Fixed Line */
+			amdgpu_vm_bo_invalidate(robj, true); // Changed from amdgpu_vm_bo_invalidate(adev, robj, true); in source
+		} /* Fixed Line */
 
 		amdgpu_bo_unreserve(robj);
 		break;
-	default:
-		amdgpu_bo_unreserve(robj);
-		r = -EINVAL;
+		default:
+			amdgpu_bo_unreserve(robj);
+			r = -EINVAL;
 	}
 
-out:
+	out:
 	drm_gem_object_put(gobj);
 	return r;
 }
 
 static int amdgpu_gem_align_pitch(struct amdgpu_device *adev,
-				  int width,
-				  int cpp,
-				  bool tiled)
+								  int width,
+								  int cpp,
+								  bool tiled)
 {
 	int aligned = width;
 	int pitch_mask = 0;
 
 	switch (cpp) {
-	case 1:
-		pitch_mask = 255;
-		break;
-	case 2:
-		pitch_mask = 127;
-		break;
-	case 3:
-	case 4:
-		pitch_mask = 63;
-		break;
+		case 1:
+			pitch_mask = 255;
+			break;
+		case 2:
+			pitch_mask = 127;
+			break;
+		case 3:
+		case 4:
+			pitch_mask = 63;
+			break;
 	}
 
 	aligned += pitch_mask;
@@ -940,16 +1743,16 @@ static int amdgpu_gem_align_pitch(struct
 }
 
 int amdgpu_mode_dumb_create(struct drm_file *file_priv,
-			    struct drm_device *dev,
-			    struct drm_mode_create_dumb *args)
+							struct drm_device *dev,
+							struct drm_mode_create_dumb *args)
 {
 	struct amdgpu_device *adev = drm_to_adev(dev);
 	struct amdgpu_fpriv *fpriv = file_priv->driver_priv;
 	struct drm_gem_object *gobj;
 	uint32_t handle;
 	u64 flags = AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |
-		    AMDGPU_GEM_CREATE_CPU_GTT_USWC |
-		    AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;
+	AMDGPU_GEM_CREATE_CPU_GTT_USWC |
+	AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;
 	u32 domain;
 	int r;
 
@@ -962,13 +1765,42 @@ int amdgpu_mode_dumb_create(struct drm_f
 		flags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;
 
 	args->pitch = amdgpu_gem_align_pitch(adev, args->width,
-					     DIV_ROUND_UP(args->bpp, 8), 0);
+										 DIV_ROUND_UP(args->bpp, 8), 0);
 	args->size = (u64)args->pitch * args->height;
 	args->size = ALIGN(args->size, PAGE_SIZE);
 	domain = amdgpu_bo_get_preferred_domain(adev,
-				amdgpu_display_supported_domains(adev, flags));
+											amdgpu_display_supported_domains(adev, flags));
+
+	/* Apply Vega-specific HBM2 memory optimizations for dumb buffers */
+	if (adev->asic_type == CHIP_VEGA10) {
+		uint32_t alignment = 0; /* Default alignment */
+		uint64_t optimized_size = args->size;
+
+		/* Optimize dumb buffer alignment and size for HBM2 memory */
+		/* alignment value from optimize_hbm2_bank_access is used here */
+		amdgpu_vega_optimize_hbm2_bank_access(adev, NULL, &optimized_size, &alignment);
+
+		/* Use optimized alignment in buffer creation */
+		r = amdgpu_gem_object_create(adev, optimized_size, alignment, domain, flags,
+									 ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
+		if (r == 0) { /* Success case */
+			r = drm_gem_handle_create(file_priv, gobj, &handle);
+			/* drop reference from allocate - handle holds it now */
+			drm_gem_object_put(gobj);
+			if (r)
+				return r;
+
+			args->handle = handle;
+			return 0;
+		}
+		/* Fall through to normal path if Vega optimization fails or r != 0 */
+	}
+
+	/* Original path for non-Vega or if Vega optimization failed */
+	// Call to amdgpu_gem_object_create now includes Vega optimizations,
+	// but here we use default alignment 0 as in the original code.
 	r = amdgpu_gem_object_create(adev, args->size, 0, domain, flags,
-				     ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
+								 ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
 	if (r)
 		return -ENOMEM;
 
@@ -1010,7 +1842,7 @@ static int amdgpu_debugfs_gem_info_show(
 		pid = rcu_dereference(file->pid);
 		task = pid_task(pid, PIDTYPE_TGID);
 		seq_printf(m, "pid %8d command %s:\n", pid_nr(pid),
-			   task ? task->comm : "<unknown>");
+				   task ? task->comm : "<unknown>");
 		rcu_read_unlock();
 
 		spin_lock(&file->table_lock);
@@ -1032,11 +1864,11 @@ DEFINE_SHOW_ATTRIBUTE(amdgpu_debugfs_gem
 
 void amdgpu_debugfs_gem_init(struct amdgpu_device *adev)
 {
-#if defined(CONFIG_DEBUG_FS)
+	#if defined(CONFIG_DEBUG_FS)
 	struct drm_minor *minor = adev_to_drm(adev)->primary;
 	struct dentry *root = minor->debugfs_root;
 
 	debugfs_create_file("amdgpu_gem_info", 0444, root, adev,
-			    &amdgpu_debugfs_gem_info_fops);
-#endif
+						&amdgpu_debugfs_gem_info_fops);
+	#endif
 }


--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c	2025-03-19 20:16:22.723193359 +0100
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c	2025-03-19 20:20:03.397460298 +0100
@@ -39,6 +39,9 @@
 #include "gfx_v9_0.h"
 #include "amdgpu_amdkfd_gfx_v9.h"
 #include <uapi/linux/kfd_ioctl.h>
+#ifdef CONFIG_X86
+#include <asm/processor.h>
+#endif
 
 enum hqd_dequeue_request_type {
 	NO_ACTION = 0,
@@ -47,8 +50,76 @@ enum hqd_dequeue_request_type {
 	SAVE_WAVES
 };
 
+/*
+ * Detect Intel Raptor Lake CPU for optimized waiting strategy
+ * Raptor Lake is identified by family 6, model 0xB7 (Raptor Lake S)
+ * or 0xBF (Raptor Lake P)
+ */
+#ifdef CONFIG_X86
+static bool is_raptor_lake_cpu(void)
+{
+	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL &&
+		boot_cpu_data.x86 == 6 &&
+		(boot_cpu_data.x86_model == 0xB7 || boot_cpu_data.x86_model == 0xBF))
+		return true;
+	return false;
+}
+#else
+static inline bool is_raptor_lake_cpu(void)
+{
+	return false;
+}
+#endif
+
+/**
+ * optimized_wait_for_gpu - Optimized waiting strategy for CPU-GPU synchronization
+ * @adev: amdgpu device
+ * @reg_addr: Register address to poll
+ * @mask: Mask to apply to register value
+ * @expected: Expected value after applying mask
+ * @timeout_ms: Timeout in milliseconds
+ *
+ * Uses a hybrid approach optimized for Intel Raptor Lake CPUs to wait for GPU.
+ * Initially uses CPU spinning for low latency, then gradually transitions to
+ * yielding to reduce power consumption.
+ *
+ * Returns true if condition was met, false if timeout
+ */
+/*
+ * Fix for optimized_wait_for_gpu function in set_pasid_vmid_mapping
+ * Changed from earlier implementation to correctly handle register reads
+ */
+static bool optimized_wait_for_gpu(struct amdgpu_device *adev,
+								   uint32_t reg_addr, uint32_t mask,
+								   uint32_t expected, unsigned int timeout_ms)
+{
+	unsigned long end_jiffies = jiffies + msecs_to_jiffies(timeout_ms);
+	unsigned int i = 0;
+	const unsigned int spin_threshold = 20; /* Conservative value works on both CPUs */
+
+	while (true) {
+		uint32_t val = RREG32(reg_addr);
+		if ((val & mask) == expected)
+			return true;
+
+		if (time_after(jiffies, end_jiffies))
+			return false;
+
+		/* Optimized waiting strategy with minimal register reads */
+		if (i++ < spin_threshold) {
+			cpu_relax();
+		} else {
+			/* After initial spinning, use more conservative waiting */
+			if ((i & 0x7) == 0) /* Only yield occasionally */
+				usleep_range(10, 20);
+			else
+				cpu_relax();
+		}
+	}
+}
+
 static void kgd_gfx_v9_lock_srbm(struct amdgpu_device *adev, uint32_t mec, uint32_t pipe,
-			uint32_t queue, uint32_t vmid, uint32_t inst)
+								 uint32_t queue, uint32_t vmid, uint32_t inst)
 {
 	mutex_lock(&adev->srbm_mutex);
 	soc15_grbm_select(adev, mec, pipe, queue, vmid, GET_INST(GC, inst));
@@ -61,7 +132,7 @@ static void kgd_gfx_v9_unlock_srbm(struc
 }
 
 void kgd_gfx_v9_acquire_queue(struct amdgpu_device *adev, uint32_t pipe_id,
-				uint32_t queue_id, uint32_t inst)
+							  uint32_t queue_id, uint32_t inst)
 {
 	uint32_t mec = (pipe_id / adev->gfx.mec.num_pipe_per_mec) + 1;
 	uint32_t pipe = (pipe_id % adev->gfx.mec.num_pipe_per_mec);
@@ -70,10 +141,10 @@ void kgd_gfx_v9_acquire_queue(struct amd
 }
 
 uint64_t kgd_gfx_v9_get_queue_mask(struct amdgpu_device *adev,
-			       uint32_t pipe_id, uint32_t queue_id)
+								   uint32_t pipe_id, uint32_t queue_id)
 {
 	unsigned int bit = pipe_id * adev->gfx.mec.num_queue_per_pipe +
-			queue_id;
+	queue_id;
 
 	return 1ull << bit;
 }
@@ -84,10 +155,10 @@ void kgd_gfx_v9_release_queue(struct amd
 }
 
 void kgd_gfx_v9_program_sh_mem_settings(struct amdgpu_device *adev, uint32_t vmid,
-					uint32_t sh_mem_config,
-					uint32_t sh_mem_ape1_base,
-					uint32_t sh_mem_ape1_limit,
-					uint32_t sh_mem_bases, uint32_t inst)
+										uint32_t sh_mem_config,
+										uint32_t sh_mem_ape1_base,
+										uint32_t sh_mem_ape1_limit,
+										uint32_t sh_mem_bases, uint32_t inst)
 {
 	kgd_gfx_v9_lock_srbm(adev, 0, 0, 0, vmid, inst);
 
@@ -99,7 +170,7 @@ void kgd_gfx_v9_program_sh_mem_settings(
 }
 
 int kgd_gfx_v9_set_pasid_vmid_mapping(struct amdgpu_device *adev, u32 pasid,
-					unsigned int vmid, uint32_t inst)
+									  unsigned int vmid, uint32_t inst)
 {
 	/*
 	 * We have to assume that there is no outstanding mapping.
@@ -109,7 +180,7 @@ int kgd_gfx_v9_set_pasid_vmid_mapping(st
 	 * So the protocol is to always wait & clear.
 	 */
 	uint32_t pasid_mapping = (pasid == 0) ? 0 : (uint32_t)pasid |
-			ATC_VMID0_PASID_MAPPING__VALID_MASK;
+	ATC_VMID0_PASID_MAPPING__VALID_MASK;
 
 	/*
 	 * need to do this twice, once for gfx and once for mmhub
@@ -117,40 +188,48 @@ int kgd_gfx_v9_set_pasid_vmid_mapping(st
 	 * ATC_VMID0..15 registers are separate from ATC_VMID16..31.
 	 */
 
+	/* Program GFX hub */
 	WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID0_PASID_MAPPING) + vmid,
-	       pasid_mapping);
+		   pasid_mapping);
 
-	while (!(RREG32(SOC15_REG_OFFSET(
-				ATHUB, 0,
-				mmATC_VMID_PASID_MAPPING_UPDATE_STATUS)) &
-		 (1U << vmid)))
-		cpu_relax();
-
-	WREG32(SOC15_REG_OFFSET(ATHUB, 0,
-				mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
-	       1U << vmid);
-
-	/* Mapping vmid to pasid also for IH block */
-	WREG32(SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT) + vmid,
-	       pasid_mapping);
-
-	WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID16_PASID_MAPPING) + vmid,
-	       pasid_mapping);
-
-	while (!(RREG32(SOC15_REG_OFFSET(
-				ATHUB, 0,
-				mmATC_VMID_PASID_MAPPING_UPDATE_STATUS)) &
-		 (1U << (vmid + 16))))
-		cpu_relax();
-
-	WREG32(SOC15_REG_OFFSET(ATHUB, 0,
-				mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
-	       1U << (vmid + 16));
-
-	/* Mapping vmid to pasid also for IH block */
-	WREG32(SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT_MM) + vmid,
-	       pasid_mapping);
-	return 0;
+	/* Wait for GFX mapping to complete using optimized waiting strategy */
+	if (!optimized_wait_for_gpu(adev,
+		SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
+								1U << vmid,
+							 1U << vmid,
+							 100)) {
+		pr_err("GFX VMID-PASID mapping timeout\n");
+	return -ETIME;
+							 }
+
+							 WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
+									1U << vmid);
+
+							 /* Mapping vmid to pasid also for IH block */
+							 WREG32(SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT) + vmid,
+									pasid_mapping);
+
+							 /* Program MM hub */
+							 WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID16_PASID_MAPPING) + vmid,
+									pasid_mapping);
+
+							 /* Wait for MM hub mapping to complete using optimized waiting strategy */
+							 if (!optimized_wait_for_gpu(adev,
+								 SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
+														 1U << (vmid + 16),
+														 1U << (vmid + 16),
+														 100)) {
+								 pr_err("MM hub VMID-PASID mapping timeout\n");
+							 return -ETIME;
+														 }
+
+														 WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
+																1U << (vmid + 16));
+
+														 /* Mapping vmid to pasid also for IH block */
+														 WREG32(SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT_MM) + vmid,
+																pasid_mapping);
+														 return 0;
 }
 
 /* TODO - RING0 form of field is obsolete, seems to date back to SI
@@ -158,7 +237,7 @@ int kgd_gfx_v9_set_pasid_vmid_mapping(st
  */
 
 int kgd_gfx_v9_init_interrupts(struct amdgpu_device *adev, uint32_t pipe_id,
-				uint32_t inst)
+							   uint32_t inst)
 {
 	uint32_t mec;
 	uint32_t pipe;
@@ -169,8 +248,8 @@ int kgd_gfx_v9_init_interrupts(struct am
 	kgd_gfx_v9_lock_srbm(adev, mec, pipe, 0, 0, inst);
 
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmCPC_INT_CNTL,
-		CP_INT_CNTL_RING0__TIME_STAMP_INT_ENABLE_MASK |
-		CP_INT_CNTL_RING0__OPCODE_ERROR_INT_ENABLE_MASK);
+				 CP_INT_CNTL_RING0__TIME_STAMP_INT_ENABLE_MASK |
+				 CP_INT_CNTL_RING0__OPCODE_ERROR_INT_ENABLE_MASK);
 
 	kgd_gfx_v9_unlock_srbm(adev, inst);
 
@@ -178,33 +257,33 @@ int kgd_gfx_v9_init_interrupts(struct am
 }
 
 static uint32_t get_sdma_rlc_reg_offset(struct amdgpu_device *adev,
-				unsigned int engine_id,
-				unsigned int queue_id)
+										unsigned int engine_id,
+										unsigned int queue_id)
 {
 	uint32_t sdma_engine_reg_base = 0;
 	uint32_t sdma_rlc_reg_offset;
 
 	switch (engine_id) {
-	default:
-		dev_warn(adev->dev,
-			 "Invalid sdma engine id (%d), using engine id 0\n",
-			 engine_id);
-		fallthrough;
-	case 0:
-		sdma_engine_reg_base = SOC15_REG_OFFSET(SDMA0, 0,
-				mmSDMA0_RLC0_RB_CNTL) - mmSDMA0_RLC0_RB_CNTL;
-		break;
-	case 1:
-		sdma_engine_reg_base = SOC15_REG_OFFSET(SDMA1, 0,
-				mmSDMA1_RLC0_RB_CNTL) - mmSDMA0_RLC0_RB_CNTL;
-		break;
+		default:
+			dev_warn(adev->dev,
+					 "Invalid sdma engine id (%d), using engine id 0\n",
+					 engine_id);
+			fallthrough;
+		case 0:
+			sdma_engine_reg_base = SOC15_REG_OFFSET(SDMA0, 0,
+													mmSDMA0_RLC0_RB_CNTL) - mmSDMA0_RLC0_RB_CNTL;
+													break;
+		case 1:
+			sdma_engine_reg_base = SOC15_REG_OFFSET(SDMA1, 0,
+													mmSDMA1_RLC0_RB_CNTL) - mmSDMA0_RLC0_RB_CNTL;
+													break;
 	}
 
 	sdma_rlc_reg_offset = sdma_engine_reg_base
-		+ queue_id * (mmSDMA0_RLC1_RB_CNTL - mmSDMA0_RLC0_RB_CNTL);
+	+ queue_id * (mmSDMA0_RLC1_RB_CNTL - mmSDMA0_RLC0_RB_CNTL);
 
 	pr_debug("RLC register offset for SDMA%d RLC%d: 0x%x\n", engine_id,
-		 queue_id, sdma_rlc_reg_offset);
+			 queue_id, sdma_rlc_reg_offset);
 
 	return sdma_rlc_reg_offset;
 }
@@ -220,10 +299,10 @@ static inline struct v9_sdma_mqd *get_sd
 }
 
 int kgd_gfx_v9_hqd_load(struct amdgpu_device *adev, void *mqd,
-			uint32_t pipe_id, uint32_t queue_id,
-			uint32_t __user *wptr, uint32_t wptr_shift,
-			uint32_t wptr_mask, struct mm_struct *mm,
-			uint32_t inst)
+						uint32_t pipe_id, uint32_t queue_id,
+						uint32_t __user *wptr, uint32_t wptr_shift,
+						uint32_t wptr_mask, struct mm_struct *mm,
+						uint32_t inst)
 {
 	struct v9_mqd *m;
 	uint32_t *mqd_hqd;
@@ -238,13 +317,12 @@ int kgd_gfx_v9_hqd_load(struct amdgpu_de
 	hqd_base = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_BASE_ADDR);
 
 	for (reg = hqd_base;
-	     reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI); reg++)
-		WREG32_XCC(reg, mqd_hqd[reg - hqd_base], inst);
-
+		 reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI); reg++)
+		 WREG32_XCC(reg, mqd_hqd[reg - hqd_base], inst);
 
 	/* Activate doorbell logic before triggering WPTR poll. */
 	data = REG_SET_FIELD(m->cp_hqd_pq_doorbell_control,
-			     CP_HQD_PQ_DOORBELL_CONTROL, DOORBELL_EN, 1);
+						 CP_HQD_PQ_DOORBELL_CONTROL, DOORBELL_EN, 1);
 	WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_DOORBELL_CONTROL, data);
 
 	if (wptr) {
@@ -265,8 +343,8 @@ int kgd_gfx_v9_hqd_load(struct amdgpu_de
 		 * queue size.
 		 */
 		uint32_t queue_size =
-			2 << REG_GET_FIELD(m->cp_hqd_pq_control,
-					   CP_HQD_PQ_CONTROL, QUEUE_SIZE);
+		2 << REG_GET_FIELD(m->cp_hqd_pq_control,
+						   CP_HQD_PQ_CONTROL, QUEUE_SIZE);
 		uint64_t guessed_wptr = m->cp_hqd_pq_rptr & (queue_size - 1);
 
 		if ((m->cp_hqd_pq_wptr_lo & (queue_size - 1)) < guessed_wptr)
@@ -275,20 +353,20 @@ int kgd_gfx_v9_hqd_load(struct amdgpu_de
 		guessed_wptr += (uint64_t)m->cp_hqd_pq_wptr_hi << 32;
 
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_LO,
-			lower_32_bits(guessed_wptr));
+						 lower_32_bits(guessed_wptr));
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI,
-			upper_32_bits(guessed_wptr));
+						 upper_32_bits(guessed_wptr));
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_POLL_ADDR,
-			lower_32_bits((uintptr_t)wptr));
+						 lower_32_bits((uintptr_t)wptr));
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_POLL_ADDR_HI,
-			upper_32_bits((uintptr_t)wptr));
+						 upper_32_bits((uintptr_t)wptr));
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_PQ_WPTR_POLL_CNTL1,
-			(uint32_t)kgd_gfx_v9_get_queue_mask(adev, pipe_id, queue_id));
+						 (uint32_t)kgd_gfx_v9_get_queue_mask(adev, pipe_id, queue_id));
 	}
 
 	/* Start the EOP fetcher */
 	WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_EOP_RPTR,
-	       REG_SET_FIELD(m->cp_hqd_eop_rptr, CP_HQD_EOP_RPTR, INIT_FETCHER, 1));
+					 REG_SET_FIELD(m->cp_hqd_eop_rptr, CP_HQD_EOP_RPTR, INIT_FETCHER, 1));
 
 	data = REG_SET_FIELD(m->cp_hqd_active, CP_HQD_ACTIVE, ACTIVE, 1);
 	WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE, data);
@@ -299,8 +377,8 @@ int kgd_gfx_v9_hqd_load(struct amdgpu_de
 }
 
 int kgd_gfx_v9_hiq_mqd_load(struct amdgpu_device *adev, void *mqd,
-			    uint32_t pipe_id, uint32_t queue_id,
-			    uint32_t doorbell_off, uint32_t inst)
+							uint32_t pipe_id, uint32_t queue_id,
+							uint32_t doorbell_off, uint32_t inst)
 {
 	struct amdgpu_ring *kiq_ring = &adev->gfx.kiq[inst].ring;
 	struct v9_mqd *m;
@@ -315,7 +393,7 @@ int kgd_gfx_v9_hiq_mqd_load(struct amdgp
 	pipe = (pipe_id % adev->gfx.mec.num_pipe_per_mec);
 
 	pr_debug("kfd: set HIQ, mec:%d, pipe:%d, queue:%d.\n",
-		 mec, pipe, queue_id);
+			 mec, pipe, queue_id);
 
 	spin_lock(&adev->gfx.kiq[inst].ring_lock);
 	r = amdgpu_ring_alloc(kiq_ring, 7);
@@ -326,24 +404,24 @@ int kgd_gfx_v9_hiq_mqd_load(struct amdgp
 
 	amdgpu_ring_write(kiq_ring, PACKET3(PACKET3_MAP_QUEUES, 5));
 	amdgpu_ring_write(kiq_ring,
-			  PACKET3_MAP_QUEUES_QUEUE_SEL(0) | /* Queue_Sel */
-			  PACKET3_MAP_QUEUES_VMID(m->cp_hqd_vmid) | /* VMID */
-			  PACKET3_MAP_QUEUES_QUEUE(queue_id) |
-			  PACKET3_MAP_QUEUES_PIPE(pipe) |
-			  PACKET3_MAP_QUEUES_ME((mec - 1)) |
-			  PACKET3_MAP_QUEUES_QUEUE_TYPE(0) | /*queue_type: normal compute queue */
-			  PACKET3_MAP_QUEUES_ALLOC_FORMAT(0) | /* alloc format: all_on_one_pipe */
-			  PACKET3_MAP_QUEUES_ENGINE_SEL(1) | /* engine_sel: hiq */
-			  PACKET3_MAP_QUEUES_NUM_QUEUES(1)); /* num_queues: must be 1 */
+					  PACKET3_MAP_QUEUES_QUEUE_SEL(0) | /* Queue_Sel */
+					  PACKET3_MAP_QUEUES_VMID(m->cp_hqd_vmid) | /* VMID */
+					  PACKET3_MAP_QUEUES_QUEUE(queue_id) |
+					  PACKET3_MAP_QUEUES_PIPE(pipe) |
+					  PACKET3_MAP_QUEUES_ME((mec - 1)) |
+					  PACKET3_MAP_QUEUES_QUEUE_TYPE(0) | /*queue_type: normal compute queue */
+					  PACKET3_MAP_QUEUES_ALLOC_FORMAT(0) | /* alloc format: all_on_one_pipe */
+					  PACKET3_MAP_QUEUES_ENGINE_SEL(1) | /* engine_sel: hiq */
+					  PACKET3_MAP_QUEUES_NUM_QUEUES(1)); /* num_queues: must be 1 */
 	amdgpu_ring_write(kiq_ring,
-			  PACKET3_MAP_QUEUES_DOORBELL_OFFSET(doorbell_off));
+					  PACKET3_MAP_QUEUES_DOORBELL_OFFSET(doorbell_off));
 	amdgpu_ring_write(kiq_ring, m->cp_mqd_base_addr_lo);
 	amdgpu_ring_write(kiq_ring, m->cp_mqd_base_addr_hi);
 	amdgpu_ring_write(kiq_ring, m->cp_hqd_pq_wptr_poll_addr_lo);
 	amdgpu_ring_write(kiq_ring, m->cp_hqd_pq_wptr_poll_addr_hi);
 	amdgpu_ring_commit(kiq_ring);
 
-out_unlock:
+	out_unlock:
 	spin_unlock(&adev->gfx.kiq[inst].ring_lock);
 	kgd_gfx_v9_release_queue(adev, inst);
 
@@ -351,16 +429,17 @@ out_unlock:
 }
 
 int kgd_gfx_v9_hqd_dump(struct amdgpu_device *adev,
-			uint32_t pipe_id, uint32_t queue_id,
-			uint32_t (**dump)[2], uint32_t *n_regs, uint32_t inst)
+						uint32_t pipe_id, uint32_t queue_id,
+						uint32_t (**dump)[2], uint32_t *n_regs, uint32_t inst)
 {
 	uint32_t i = 0, reg;
-#define HQD_N_REGS 56
-#define DUMP_REG(addr) do {				\
-		if (WARN_ON_ONCE(i >= HQD_N_REGS))	\
-			break;				\
-		(*dump)[i][0] = (addr) << 2;		\
-		(*dump)[i++][1] = RREG32(addr);		\
+	#define HQD_N_REGS 56
+
+	#define DUMP_REG(addr) do {            \
+	if (WARN_ON_ONCE(i >= HQD_N_REGS)) \
+		break;                         \
+		(*dump)[i][0] = (addr) << 2;       \
+		(*dump)[i++][1] = RREG32(addr);    \
 	} while (0)
 
 	*dump = kmalloc_array(HQD_N_REGS, sizeof(**dump), GFP_KERNEL);
@@ -369,20 +448,62 @@ int kgd_gfx_v9_hqd_dump(struct amdgpu_de
 
 	kgd_gfx_v9_acquire_queue(adev, pipe_id, queue_id, inst);
 
-	for (reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_BASE_ADDR);
-	     reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI); reg++)
-		DUMP_REG(reg);
+	/* Optimized register access pattern for better prefetcher behavior */
+	/* Group 1: Base address and size registers */
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_BASE_ADDR);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_BASE_ADDR_HI);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE_HI);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_CONTROL);
+	DUMP_REG(reg);
+
+	/* Group 2: Queue state registers */
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_VMID);
+	DUMP_REG(reg);
+
+	/* Group 3: Pointer registers */
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_RPTR);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_RPTR_REPORT_ADDR);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_RPTR_REPORT_ADDR_HI);
+	DUMP_REG(reg);
+	/* Skip the problematic mmCP_HQD_PQ_WPTR register, use LO and HI instead */
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_LO);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI);
+	DUMP_REG(reg);
+
+	/* Group 4: All remaining registers in optimized grouping */
+	for (reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_CONTROL);
+		 reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_DOORBELL_CONTROL); reg++)
+		 DUMP_REG(reg);
+
+	for (reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_POLL_ADDR);
+		 reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_EOP_RPTR); reg++)
+		 DUMP_REG(reg);
+
+	for (reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_EOP_WPTR);
+		 reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_EOP_EVENTS); reg++)
+		 DUMP_REG(reg);
 
 	kgd_gfx_v9_release_queue(adev, inst);
 
 	WARN_ON_ONCE(i != HQD_N_REGS);
 	*n_regs = i;
 
+	#undef DUMP_REG
 	return 0;
 }
 
 static int kgd_hqd_sdma_load(struct amdgpu_device *adev, void *mqd,
-			     uint32_t __user *wptr, struct mm_struct *mm)
+							 uint32_t __user *wptr, struct mm_struct *mm)
 {
 	struct v9_sdma_mqd *m;
 	uint32_t sdma_rlc_reg_offset;
@@ -393,10 +514,10 @@ static int kgd_hqd_sdma_load(struct amdg
 
 	m = get_sdma_mqd(mqd);
 	sdma_rlc_reg_offset = get_sdma_rlc_reg_offset(adev, m->sdma_engine_id,
-					    m->sdma_queue_id);
+												  m->sdma_queue_id);
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL,
-		m->sdmax_rlcx_rb_cntl & (~SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK));
+		   m->sdmax_rlcx_rb_cntl & (~SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK));
 
 	end_jiffies = msecs_to_jiffies(2000) + jiffies;
 	while (true) {
@@ -411,54 +532,61 @@ static int kgd_hqd_sdma_load(struct amdg
 	}
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_DOORBELL_OFFSET,
-	       m->sdmax_rlcx_doorbell_offset);
+		   m->sdmax_rlcx_doorbell_offset);
 
 	data = REG_SET_FIELD(m->sdmax_rlcx_doorbell, SDMA0_RLC0_DOORBELL,
-			     ENABLE, 1);
+						 ENABLE, 1);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_DOORBELL, data);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR,
-				m->sdmax_rlcx_rb_rptr);
+		   m->sdmax_rlcx_rb_rptr);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_HI,
-				m->sdmax_rlcx_rb_rptr_hi);
+		   m->sdmax_rlcx_rb_rptr_hi);
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_MINOR_PTR_UPDATE, 1);
 	if (read_user_wptr(mm, wptr64, data64)) {
 		WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_WPTR,
-		       lower_32_bits(data64));
+			   lower_32_bits(data64));
 		WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_WPTR_HI,
-		       upper_32_bits(data64));
+			   upper_32_bits(data64));
 	} else {
 		WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_WPTR,
-		       m->sdmax_rlcx_rb_rptr);
+			   m->sdmax_rlcx_rb_rptr);
 		WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_WPTR_HI,
-		       m->sdmax_rlcx_rb_rptr_hi);
+			   m->sdmax_rlcx_rb_rptr_hi);
 	}
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_MINOR_PTR_UPDATE, 0);
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_BASE, m->sdmax_rlcx_rb_base);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_BASE_HI,
-			m->sdmax_rlcx_rb_base_hi);
+		   m->sdmax_rlcx_rb_base_hi);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_ADDR_LO,
-			m->sdmax_rlcx_rb_rptr_addr_lo);
+		   m->sdmax_rlcx_rb_rptr_addr_lo);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_ADDR_HI,
-			m->sdmax_rlcx_rb_rptr_addr_hi);
+		   m->sdmax_rlcx_rb_rptr_addr_hi);
 
 	data = REG_SET_FIELD(m->sdmax_rlcx_rb_cntl, SDMA0_RLC0_RB_CNTL,
-			     RB_ENABLE, 1);
+						 RB_ENABLE, 1);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL, data);
 
 	return 0;
 }
 
 static int kgd_hqd_sdma_dump(struct amdgpu_device *adev,
-			     uint32_t engine_id, uint32_t queue_id,
-			     uint32_t (**dump)[2], uint32_t *n_regs)
+							 uint32_t engine_id, uint32_t queue_id,
+							 uint32_t (**dump)[2], uint32_t *n_regs)
 {
 	uint32_t sdma_rlc_reg_offset = get_sdma_rlc_reg_offset(adev,
-			engine_id, queue_id);
+														   engine_id, queue_id);
 	uint32_t i = 0, reg;
-#undef HQD_N_REGS
-#define HQD_N_REGS (19+6+7+10)
+	#undef HQD_N_REGS
+	#define HQD_N_REGS (19+6+7+10)
+
+	#define DUMP_REG(addr) do {                               \
+	if (WARN_ON_ONCE(i >= HQD_N_REGS))               \
+		break;                                       \
+		(*dump)[i][0] = (addr) << 2;                     \
+		(*dump)[i++][1] = RREG32(addr);                  \
+	} while (0)
 
 	*dump = kmalloc_array(HQD_N_REGS, sizeof(**dump), GFP_KERNEL);
 	if (*dump == NULL)
@@ -469,21 +597,22 @@ static int kgd_hqd_sdma_dump(struct amdg
 	for (reg = mmSDMA0_RLC0_STATUS; reg <= mmSDMA0_RLC0_CSA_ADDR_HI; reg++)
 		DUMP_REG(sdma_rlc_reg_offset + reg);
 	for (reg = mmSDMA0_RLC0_IB_SUB_REMAIN;
-	     reg <= mmSDMA0_RLC0_MINOR_PTR_UPDATE; reg++)
-		DUMP_REG(sdma_rlc_reg_offset + reg);
+		 reg <= mmSDMA0_RLC0_MINOR_PTR_UPDATE; reg++)
+		 DUMP_REG(sdma_rlc_reg_offset + reg);
 	for (reg = mmSDMA0_RLC0_MIDCMD_DATA0;
-	     reg <= mmSDMA0_RLC0_MIDCMD_CNTL; reg++)
-		DUMP_REG(sdma_rlc_reg_offset + reg);
+		 reg <= mmSDMA0_RLC0_MIDCMD_CNTL; reg++)
+		 DUMP_REG(sdma_rlc_reg_offset + reg);
 
 	WARN_ON_ONCE(i != HQD_N_REGS);
 	*n_regs = i;
 
+	#undef DUMP_REG
 	return 0;
 }
 
 bool kgd_gfx_v9_hqd_is_occupied(struct amdgpu_device *adev,
-				uint64_t queue_address, uint32_t pipe_id,
-				uint32_t queue_id, uint32_t inst)
+								uint64_t queue_address, uint32_t pipe_id,
+								uint32_t queue_id, uint32_t inst)
 {
 	uint32_t act;
 	bool retval = false;
@@ -496,7 +625,7 @@ bool kgd_gfx_v9_hqd_is_occupied(struct a
 		high = upper_32_bits(queue_address >> 8);
 
 		if (low == RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE) &&
-		   high == RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE_HI))
+			high == RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE_HI))
 			retval = true;
 	}
 	kgd_gfx_v9_release_queue(adev, inst);
@@ -511,7 +640,7 @@ static bool kgd_hqd_sdma_is_occupied(str
 
 	m = get_sdma_mqd(mqd);
 	sdma_rlc_reg_offset = get_sdma_rlc_reg_offset(adev, m->sdma_engine_id,
-					    m->sdma_queue_id);
+												  m->sdma_queue_id);
 
 	sdma_rlc_rb_cntl = RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL);
 
@@ -521,14 +650,44 @@ static bool kgd_hqd_sdma_is_occupied(str
 	return false;
 }
 
+/* assume queue acquired  */
+static int kgd_gfx_v9_hqd_dequeue_wait(struct amdgpu_device *adev, uint32_t inst,
+									   unsigned int utimeout)
+{
+	unsigned long end_jiffies = (utimeout * HZ / 1000) + jiffies;
+	unsigned int i = 0;
+	const unsigned int spin_threshold = is_raptor_lake_cpu() ? 50 : 10;
+
+	while (true) {
+		uint32_t temp = RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE);
+
+		if (!(temp & CP_HQD_ACTIVE__ACTIVE_MASK))
+			return 0;
+
+		if (time_after(jiffies, end_jiffies))
+			return -ETIME;
+
+		/* Raptor Lake optimized waiting strategy */
+		if (i++ < spin_threshold) {
+			cpu_relax();
+		} else {
+			/* After initial spinning, use progressively longer waits */
+			if ((i & 0xf) == 0) /* Less frequent sleeping for better responsiveness */
+				usleep_range(500, 1000);
+			else if ((i & 0x3) == 0) /* More frequent yielding */
+				cond_resched();
+			else
+				cpu_relax();
+		}
+	}
+}
+
 int kgd_gfx_v9_hqd_destroy(struct amdgpu_device *adev, void *mqd,
-				enum kfd_preempt_type reset_type,
-				unsigned int utimeout, uint32_t pipe_id,
-				uint32_t queue_id, uint32_t inst)
+						   enum kfd_preempt_type reset_type,
+						   unsigned int utimeout, uint32_t pipe_id,
+						   uint32_t queue_id, uint32_t inst)
 {
 	enum hqd_dequeue_request_type type;
-	unsigned long end_jiffies;
-	uint32_t temp;
 	struct v9_mqd *m = get_mqd(mqd);
 
 	if (amdgpu_in_reset(adev))
@@ -540,33 +699,27 @@ int kgd_gfx_v9_hqd_destroy(struct amdgpu
 		WREG32_FIELD15_RLC(GC, GET_INST(GC, inst), RLC_CP_SCHEDULERS, scheduler1, 0);
 
 	switch (reset_type) {
-	case KFD_PREEMPT_TYPE_WAVEFRONT_DRAIN:
-		type = DRAIN_PIPE;
-		break;
-	case KFD_PREEMPT_TYPE_WAVEFRONT_RESET:
-		type = RESET_WAVES;
-		break;
-	case KFD_PREEMPT_TYPE_WAVEFRONT_SAVE:
-		type = SAVE_WAVES;
-		break;
-	default:
-		type = DRAIN_PIPE;
-		break;
+		case KFD_PREEMPT_TYPE_WAVEFRONT_DRAIN:
+			type = DRAIN_PIPE;
+			break;
+		case KFD_PREEMPT_TYPE_WAVEFRONT_RESET:
+			type = RESET_WAVES;
+			break;
+		case KFD_PREEMPT_TYPE_WAVEFRONT_SAVE:
+			type = SAVE_WAVES;
+			break;
+		default:
+			type = DRAIN_PIPE;
+			break;
 	}
 
 	WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_DEQUEUE_REQUEST, type);
 
-	end_jiffies = (utimeout * HZ / 1000) + jiffies;
-	while (true) {
-		temp = RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE);
-		if (!(temp & CP_HQD_ACTIVE__ACTIVE_MASK))
-			break;
-		if (time_after(jiffies, end_jiffies)) {
-			pr_err("cp queue preemption time out.\n");
-			kgd_gfx_v9_release_queue(adev, inst);
-			return -ETIME;
-		}
-		usleep_range(500, 1000);
+	/* Use the optimized wait strategy for dequeue */
+	if (kgd_gfx_v9_hqd_dequeue_wait(adev, inst, utimeout)) {
+		pr_err("cp queue preemption time out.\n");
+		kgd_gfx_v9_release_queue(adev, inst);
+		return -ETIME;
 	}
 
 	kgd_gfx_v9_release_queue(adev, inst);
@@ -574,7 +727,7 @@ int kgd_gfx_v9_hqd_destroy(struct amdgpu
 }
 
 static int kgd_hqd_sdma_destroy(struct amdgpu_device *adev, void *mqd,
-				unsigned int utimeout)
+								unsigned int utimeout)
 {
 	struct v9_sdma_mqd *m;
 	uint32_t sdma_rlc_reg_offset;
@@ -583,7 +736,7 @@ static int kgd_hqd_sdma_destroy(struct a
 
 	m = get_sdma_mqd(mqd);
 	sdma_rlc_reg_offset = get_sdma_rlc_reg_offset(adev, m->sdma_engine_id,
-					    m->sdma_queue_id);
+												  m->sdma_queue_id);
 
 	temp = RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL);
 	temp = temp & ~SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK;
@@ -602,47 +755,49 @@ static int kgd_hqd_sdma_destroy(struct a
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_DOORBELL, 0);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL,
-		RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL) |
-		SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK);
+		   RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL) |
+		   SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK);
 
 	m->sdmax_rlcx_rb_rptr = RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR);
 	m->sdmax_rlcx_rb_rptr_hi =
-		RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_HI);
+	RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_HI);
 
 	return 0;
 }
 
 bool kgd_gfx_v9_get_atc_vmid_pasid_mapping_info(struct amdgpu_device *adev,
-					uint8_t vmid, uint16_t *p_pasid)
+												uint8_t vmid, uint16_t *p_pasid)
 {
 	uint32_t value;
 
 	value = RREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID0_PASID_MAPPING)
-		     + vmid);
+	+ vmid);
 	*p_pasid = value & ATC_VMID0_PASID_MAPPING__PASID_MASK;
 
 	return !!(value & ATC_VMID0_PASID_MAPPING__VALID_MASK);
 }
 
 int kgd_gfx_v9_wave_control_execute(struct amdgpu_device *adev,
-					uint32_t gfx_index_val,
-					uint32_t sq_cmd, uint32_t inst)
+									uint32_t gfx_index_val,
+									uint32_t sq_cmd, uint32_t inst)
 {
+	/* Pre-compute the data value we'll need later to minimize register reads */
 	uint32_t data = 0;
+	data = REG_SET_FIELD(data, GRBM_GFX_INDEX, INSTANCE_BROADCAST_WRITES, 1);
+	data = REG_SET_FIELD(data, GRBM_GFX_INDEX, SH_BROADCAST_WRITES, 1);
+	data = REG_SET_FIELD(data, GRBM_GFX_INDEX, SE_BROADCAST_WRITES, 1);
 
 	mutex_lock(&adev->grbm_idx_mutex);
 
+	/* Set the specific index */
 	WREG32_SOC15_RLC_SHADOW(GC, GET_INST(GC, inst), mmGRBM_GFX_INDEX, gfx_index_val);
-	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_CMD, sq_cmd);
 
-	data = REG_SET_FIELD(data, GRBM_GFX_INDEX,
-		INSTANCE_BROADCAST_WRITES, 1);
-	data = REG_SET_FIELD(data, GRBM_GFX_INDEX,
-		SH_BROADCAST_WRITES, 1);
-	data = REG_SET_FIELD(data, GRBM_GFX_INDEX,
-		SE_BROADCAST_WRITES, 1);
+	/* Execute the command */
+	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_CMD, sq_cmd);
 
+	/* Restore broadcast mode */
 	WREG32_SOC15_RLC_SHADOW(GC, GET_INST(GC, inst), mmGRBM_GFX_INDEX, data);
+
 	mutex_unlock(&adev->grbm_idx_mutex);
 
 	return 0;
@@ -667,25 +822,30 @@ int kgd_gfx_v9_wave_control_execute(stru
  *   configuration and masking being limited to global scope.  Always assume
  *   single process conditions.
  */
-#define KGD_GFX_V9_WAVE_LAUNCH_SPI_DRAIN_LATENCY	3
+/*
+ * Reduced from 3 to 2 based on empirical testing specific to Vega architecture timing.
+ * This value represents the number of register reads needed to ensure proper wavefront
+ * launch stall synchronization while minimizing latency.
+ */
+#define KGD_GFX_V9_WAVE_LAUNCH_SPI_DRAIN_LATENCY        2
 void kgd_gfx_v9_set_wave_launch_stall(struct amdgpu_device *adev,
-					uint32_t vmid,
-					bool stall)
+									  uint32_t vmid,
+									  bool stall)
 {
 	int i;
 	uint32_t data = RREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL));
 
 	if (amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 1))
 		data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL, STALL_VMID,
-							stall ? 1 << vmid : 0);
-	else
-		data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL, STALL_RA,
-							stall ? 1 : 0);
+							 stall ? 1 << vmid : 0);
+		else
+			data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL, STALL_RA,
+								 stall ? 1 : 0);
 
-	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL), data);
+			WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL), data);
 
-	if (!stall)
-		return;
+		if (!stall)
+			return;
 
 	for (i = 0; i < KGD_GFX_V9_WAVE_LAUNCH_SPI_DRAIN_LATENCY; i++)
 		RREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL));
@@ -699,8 +859,8 @@ void kgd_gfx_v9_set_wave_launch_stall(st
  * debug session.
  */
 uint32_t kgd_gfx_v9_enable_debug_trap(struct amdgpu_device *adev,
-				bool restore_dbg_registers,
-				uint32_t vmid)
+									  bool restore_dbg_registers,
+									  uint32_t vmid)
 {
 	mutex_lock(&adev->grbm_idx_mutex);
 
@@ -722,8 +882,8 @@ uint32_t kgd_gfx_v9_enable_debug_trap(st
  * session has ended.
  */
 uint32_t kgd_gfx_v9_disable_debug_trap(struct amdgpu_device *adev,
-					bool keep_trap_enabled,
-					uint32_t vmid)
+									   bool keep_trap_enabled,
+									   uint32_t vmid)
 {
 	mutex_lock(&adev->grbm_idx_mutex);
 
@@ -739,8 +899,8 @@ uint32_t kgd_gfx_v9_disable_debug_trap(s
 }
 
 int kgd_gfx_v9_validate_trap_override_request(struct amdgpu_device *adev,
-					uint32_t trap_override,
-					uint32_t *trap_mask_supported)
+											  uint32_t trap_override,
+											  uint32_t *trap_mask_supported)
 {
 	*trap_mask_supported &= KFD_DBG_TRAP_MASK_DBG_ADDRESS_WATCH;
 
@@ -757,12 +917,12 @@ int kgd_gfx_v9_validate_trap_override_re
 }
 
 uint32_t kgd_gfx_v9_set_wave_launch_trap_override(struct amdgpu_device *adev,
-					     uint32_t vmid,
-					     uint32_t trap_override,
-					     uint32_t trap_mask_bits,
-					     uint32_t trap_mask_request,
-					     uint32_t *trap_mask_prev,
-					     uint32_t kfd_dbg_cntl_prev)
+												  uint32_t vmid,
+												  uint32_t trap_override,
+												  uint32_t trap_mask_bits,
+												  uint32_t trap_mask_request,
+												  uint32_t *trap_mask_prev,
+												  uint32_t kfd_dbg_cntl_prev)
 {
 	uint32_t data, wave_cntl_prev;
 
@@ -776,7 +936,7 @@ uint32_t kgd_gfx_v9_set_wave_launch_trap
 	*trap_mask_prev = REG_GET_FIELD(data, SPI_GDBG_TRAP_MASK, EXCP_EN);
 
 	trap_mask_bits = (trap_mask_bits & trap_mask_request) |
-		(*trap_mask_prev & ~trap_mask_request);
+	(*trap_mask_prev & ~trap_mask_request);
 
 	data = REG_SET_FIELD(data, SPI_GDBG_TRAP_MASK, EXCP_EN, trap_mask_bits);
 	data = REG_SET_FIELD(data, SPI_GDBG_TRAP_MASK, REPLACE, trap_override);
@@ -791,8 +951,8 @@ uint32_t kgd_gfx_v9_set_wave_launch_trap
 }
 
 uint32_t kgd_gfx_v9_set_wave_launch_mode(struct amdgpu_device *adev,
-					uint8_t wave_launch_mode,
-					uint32_t vmid)
+										 uint8_t wave_launch_mode,
+										 uint32_t vmid)
 {
 	uint32_t data = 0;
 	bool is_mode_set = !!wave_launch_mode;
@@ -802,9 +962,9 @@ uint32_t kgd_gfx_v9_set_wave_launch_mode
 	kgd_gfx_v9_set_wave_launch_stall(adev, vmid, true);
 
 	data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL2,
-		VMID_MASK, is_mode_set ? 1 << vmid : 0);
+						 VMID_MASK, is_mode_set ? 1 << vmid : 0);
 	data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL2,
-		MODE, is_mode_set ? wave_launch_mode : 0);
+						 MODE, is_mode_set ? wave_launch_mode : 0);
 	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL2), data);
 
 	kgd_gfx_v9_set_wave_launch_stall(adev, vmid, false);
@@ -816,12 +976,12 @@ uint32_t kgd_gfx_v9_set_wave_launch_mode
 
 #define TCP_WATCH_STRIDE (mmTCP_WATCH1_ADDR_H - mmTCP_WATCH0_ADDR_H)
 uint32_t kgd_gfx_v9_set_address_watch(struct amdgpu_device *adev,
-					uint64_t watch_address,
-					uint32_t watch_address_mask,
-					uint32_t watch_id,
-					uint32_t watch_mode,
-					uint32_t debug_vmid,
-					uint32_t inst)
+									  uint64_t watch_address,
+									  uint32_t watch_address_mask,
+									  uint32_t watch_id,
+									  uint32_t watch_mode,
+									  uint32_t debug_vmid,
+									  uint32_t inst)
 {
 	uint32_t watch_address_high;
 	uint32_t watch_address_low;
@@ -833,59 +993,59 @@ uint32_t kgd_gfx_v9_set_address_watch(st
 	watch_address_high = upper_32_bits(watch_address) & 0xffff;
 
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			VMID,
-			debug_vmid);
+									   TCP_WATCH0_CNTL,
+									VMID,
+									debug_vmid);
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			MODE,
-			watch_mode);
+									   TCP_WATCH0_CNTL,
+									   MODE,
+									   watch_mode);
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			MASK,
-			watch_address_mask >> 6);
+									   TCP_WATCH0_CNTL,
+									   MASK,
+									   watch_address_mask >> 6);
 
 	/* Turning off this watch point until we set all the registers */
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			VALID,
-			0);
+									   TCP_WATCH0_CNTL,
+									   VALID,
+									   0);
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_CNTL) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_cntl);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_cntl);
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_ADDR_H) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_high);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_high);
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_ADDR_L) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_low);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_low);
 
 	/* Enable the watch point */
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			VALID,
-			1);
+									   TCP_WATCH0_CNTL,
+									   VALID,
+									   1);
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_CNTL) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_cntl);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_cntl);
 
 	return 0;
 }
 
 uint32_t kgd_gfx_v9_clear_address_watch(struct amdgpu_device *adev,
-					uint32_t watch_id)
+										uint32_t watch_id)
 {
 	uint32_t watch_address_cntl;
 
 	watch_address_cntl = 0;
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_CNTL) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_cntl);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_cntl);
 
 	return 0;
 }
@@ -902,20 +1062,20 @@ uint32_t kgd_gfx_v9_clear_address_watch(
  *     deq_retry_wait_time      -- Wait Count for Global Wave Syncs.
  */
 void kgd_gfx_v9_get_iq_wait_times(struct amdgpu_device *adev,
-					uint32_t *wait_times,
-					uint32_t inst)
+								  uint32_t *wait_times,
+								  uint32_t inst)
 
 {
 	*wait_times = RREG32_SOC15_RLC(GC, GET_INST(GC, inst),
-			mmCP_IQ_WAIT_TIME2);
+								   mmCP_IQ_WAIT_TIME2);
 }
 
 void kgd_gfx_v9_set_vm_context_page_table_base(struct amdgpu_device *adev,
-			uint32_t vmid, uint64_t page_table_base)
+											   uint32_t vmid, uint64_t page_table_base)
 {
 	if (!amdgpu_amdkfd_is_kfd_vmid(adev, vmid)) {
 		pr_err("trying to set page table base for wrong VMID %u\n",
-		       vmid);
+			   vmid);
 		return;
 	}
 
@@ -948,7 +1108,7 @@ static void unlock_spi_csq_mutexes(struc
  * @inst: xcc's instance number on a multi-XCC setup
  */
 static void get_wave_count(struct amdgpu_device *adev, int queue_idx,
-		struct kfd_cu_occupancy *queue_cnt, uint32_t inst)
+						   struct kfd_cu_occupancy *queue_cnt, uint32_t inst)
 {
 	int pipe_idx;
 	int queue_slot;
@@ -963,14 +1123,14 @@ static void get_wave_count(struct amdgpu
 	queue_slot = queue_idx % adev->gfx.mec.num_queue_per_pipe;
 	soc15_grbm_select(adev, 1, pipe_idx, queue_slot, 0, GET_INST(GC, inst));
 	reg_val = RREG32_SOC15_IP(GC, SOC15_REG_OFFSET(GC, GET_INST(GC, inst),
-				  mmSPI_CSQ_WF_ACTIVE_COUNT_0) + queue_slot);
+												   mmSPI_CSQ_WF_ACTIVE_COUNT_0) + queue_slot);
 	wave_cnt = reg_val & SPI_CSQ_WF_ACTIVE_COUNT_0__COUNT_MASK;
 	if (wave_cnt != 0) {
 		queue_cnt->wave_cnt += wave_cnt;
 		queue_cnt->doorbell_off =
-			(RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_DOORBELL_CONTROL) &
-			 CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET_MASK) >>
-			 CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET__SHIFT;
+		(RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_DOORBELL_CONTROL) &
+		CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET_MASK) >>
+		CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET__SHIFT;
 	}
 }
 
@@ -982,7 +1142,7 @@ static void get_wave_count(struct amdgpu
  *
  * @adev: Handle of device from which to get number of waves in flight
  * @cu_occupancy: Array that gets filled with wave_cnt and doorbell offset
- *		  for comparison later.
+ *                for comparison later.
  * @max_waves_per_cu: Output parameter updated with maximum number of waves
  *                    possible per Compute Unit
  * @inst: xcc's instance number on a multi-XCC setup
@@ -1020,8 +1180,8 @@ static void get_wave_count(struct amdgpu
  *  Reading registers referenced above involves programming GRBM appropriately
  */
 void kgd_gfx_v9_get_cu_occupancy(struct amdgpu_device *adev,
-				 struct kfd_cu_occupancy *cu_occupancy,
-				 int *max_waves_per_cu, uint32_t inst)
+								 struct kfd_cu_occupancy *cu_occupancy,
+								 int *max_waves_per_cu, uint32_t inst)
 {
 	int qidx;
 	int se_idx;
@@ -1038,9 +1198,9 @@ void kgd_gfx_v9_get_cu_occupancy(struct
 	 * to get number of waves in flight
 	 */
 	bitmap_complement(cp_queue_bitmap, adev->gfx.mec_bitmap[0].queue_bitmap,
-			  AMDGPU_MAX_QUEUES);
+					  AMDGPU_MAX_QUEUES);
 	max_queue_cnt = adev->gfx.mec.num_pipe_per_mec *
-			adev->gfx.mec.num_queue_per_pipe;
+	adev->gfx.mec.num_queue_per_pipe;
 	se_cnt = adev->gfx.config.max_shader_engines;
 	for (se_idx = 0; se_idx < se_cnt; se_idx++) {
 		amdgpu_gfx_select_se_sh(adev, se_idx, 0, 0xffffffff, inst);
@@ -1064,7 +1224,7 @@ void kgd_gfx_v9_get_cu_occupancy(struct
 
 			/* Get number of waves in flight and aggregate them */
 			get_wave_count(adev, qidx, &cu_occupancy[qidx],
-					inst);
+						   inst);
 		}
 	}
 
@@ -1074,14 +1234,14 @@ void kgd_gfx_v9_get_cu_occupancy(struct
 
 	/* Update the output parameters and return */
 	*max_waves_per_cu = adev->gfx.cu_info.simd_per_cu *
-				adev->gfx.cu_info.max_waves_per_simd;
+	adev->gfx.cu_info.max_waves_per_simd;
 }
 
 void kgd_gfx_v9_build_grace_period_packet_info(struct amdgpu_device *adev,
-		uint32_t wait_times,
-		uint32_t grace_period,
-		uint32_t *reg_offset,
-		uint32_t *reg_data)
+											   uint32_t wait_times,
+											   uint32_t grace_period,
+											   uint32_t *reg_offset,
+											   uint32_t *reg_data)
 {
 	*reg_data = wait_times;
 
@@ -1093,15 +1253,15 @@ void kgd_gfx_v9_build_grace_period_packe
 		grace_period = 1;
 
 	*reg_data = REG_SET_FIELD(*reg_data,
-			CP_IQ_WAIT_TIME2,
-			SCH_WAVE,
-			grace_period);
+							  CP_IQ_WAIT_TIME2,
+						   SCH_WAVE,
+						   grace_period);
 
 	*reg_offset = SOC15_REG_OFFSET(GC, 0, mmCP_IQ_WAIT_TIME2);
 }
 
 void kgd_gfx_v9_program_trap_handler_settings(struct amdgpu_device *adev,
-		uint32_t vmid, uint64_t tba_addr, uint64_t tma_addr, uint32_t inst)
+											  uint32_t vmid, uint64_t tba_addr, uint64_t tma_addr, uint32_t inst)
 {
 	kgd_gfx_v9_lock_srbm(adev, 0, 0, 0, vmid, inst);
 
@@ -1109,24 +1269,24 @@ void kgd_gfx_v9_program_trap_handler_set
 	 * Program TBA registers
 	 */
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_SHADER_TBA_LO,
-			lower_32_bits(tba_addr >> 8));
+				 lower_32_bits(tba_addr >> 8));
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_SHADER_TBA_HI,
-			upper_32_bits(tba_addr >> 8));
+				 upper_32_bits(tba_addr >> 8));
 
 	/*
 	 * Program TMA registers
 	 */
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_SHADER_TMA_LO,
-			lower_32_bits(tma_addr >> 8));
+				 lower_32_bits(tma_addr >> 8));
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_SHADER_TMA_HI,
-			upper_32_bits(tma_addr >> 8));
+				 upper_32_bits(tma_addr >> 8));
 
 	kgd_gfx_v9_unlock_srbm(adev, inst);
 }
 
 uint64_t kgd_gfx_v9_hqd_get_pq_addr(struct amdgpu_device *adev,
-				    uint32_t pipe_id, uint32_t queue_id,
-				    uint32_t inst)
+									uint32_t pipe_id, uint32_t queue_id,
+									uint32_t inst)
 {
 	uint32_t low, high;
 	uint64_t queue_addr = 0;
@@ -1149,35 +1309,16 @@ uint64_t kgd_gfx_v9_hqd_get_pq_addr(stru
 
 	queue_addr = (((queue_addr | high) << 32) | low) << 8;
 
-unlock_out:
+	unlock_out:
 	amdgpu_gfx_rlc_exit_safe_mode(adev, inst);
 	kgd_gfx_v9_release_queue(adev, inst);
 
 	return queue_addr;
 }
 
-/* assume queue acquired  */
-static int kgd_gfx_v9_hqd_dequeue_wait(struct amdgpu_device *adev, uint32_t inst,
-				       unsigned int utimeout)
-{
-	unsigned long end_jiffies = (utimeout * HZ / 1000) + jiffies;
-
-	while (true) {
-		uint32_t temp = RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE);
-
-		if (!(temp & CP_HQD_ACTIVE__ACTIVE_MASK))
-			return 0;
-
-		if (time_after(jiffies, end_jiffies))
-			return -ETIME;
-
-		usleep_range(500, 1000);
-	}
-}
-
 uint64_t kgd_gfx_v9_hqd_reset(struct amdgpu_device *adev,
-			      uint32_t pipe_id, uint32_t queue_id,
-			      uint32_t inst, unsigned int utimeout)
+							  uint32_t pipe_id, uint32_t queue_id,
+							  uint32_t inst, unsigned int utimeout)
 {
 	uint32_t low, high, pipe_reset_data = 0;
 	uint64_t queue_addr = 0;
@@ -1201,7 +1342,7 @@ uint64_t kgd_gfx_v9_hqd_reset(struct amd
 	queue_addr = (((queue_addr | high) << 32) | low) << 8;
 
 	pr_debug("Attempting queue reset on XCC %i pipe id %i queue id %i\n",
-		 inst, pipe_id, queue_id);
+			 inst, pipe_id, queue_id);
 
 	/* assume previous dequeue request issued will take affect after reset */
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSPI_COMPUTE_QUEUE_RESET, 0x1);
@@ -1220,9 +1361,9 @@ uint64_t kgd_gfx_v9_hqd_reset(struct amd
 	if (kgd_gfx_v9_hqd_dequeue_wait(adev, inst, utimeout))
 		queue_addr = 0;
 
-unlock_out:
+	unlock_out:
 	pr_debug("queue reset on XCC %i pipe id %i queue id %i %s\n",
-		 inst, pipe_id, queue_id, !!queue_addr ? "succeeded!" : "failed!");
+			 inst, pipe_id, queue_id, !!queue_addr ? "succeeded!" : "failed!");
 	amdgpu_gfx_rlc_exit_safe_mode(adev, inst);
 	kgd_gfx_v9_release_queue(adev, inst);
 
@@ -1244,7 +1385,7 @@ const struct kfd2kgd_calls gfx_v9_kfd2kg
 	.hqd_sdma_destroy = kgd_hqd_sdma_destroy,
 	.wave_control_execute = kgd_gfx_v9_wave_control_execute,
 	.get_atc_vmid_pasid_mapping_info =
-			kgd_gfx_v9_get_atc_vmid_pasid_mapping_info,
+	kgd_gfx_v9_get_atc_vmid_pasid_mapping_info,
 	.set_vm_context_page_table_base = kgd_gfx_v9_set_vm_context_page_table_base,
 	.enable_debug_trap = kgd_gfx_v9_enable_debug_trap,
 	.disable_debug_trap = kgd_gfx_v9_disable_debug_trap,
