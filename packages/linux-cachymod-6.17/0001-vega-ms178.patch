--- a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c	2025-09-18 01:33:05.056495913 +0200
+++ b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c	2025-10-06 02:04:26.382110242 +0200

--- a/drivers/gpu/drm/drm_buddy.c
+++ b/drivers/gpu/drm/drm_buddy.c
@@ -30,6 +30,8 @@ static struct drm_buddy_block *drm_block
 	block->header |= order;
 	block->parent = parent;
 
+	RB_CLEAR_NODE(&block->rb);
+
 	BUG_ON(block->header & DRM_BUDDY_HEADER_UNUSED);
 	return block;
 }
@@ -40,58 +42,139 @@ static void drm_block_free(struct drm_bu
 	kmem_cache_free(slab_blocks, block);
 }
 
-static void list_insert_sorted(struct drm_buddy *mm,
-			       struct drm_buddy_block *block)
+static inline struct rb_root *
+__get_root(struct drm_buddy *mm,
+	   unsigned int order,
+	   enum free_tree tree)
+{
+	if (tree == CLEAR_TREE)
+		return &mm->clear_tree[order];
+	else
+		return &mm->dirty_tree[order];
+}
+
+static inline enum free_tree
+__get_tree_for_block(struct drm_buddy_block *block)
+{
+	return drm_buddy_block_is_clear(block) ? CLEAR_TREE : DIRTY_TREE;
+}
+
+static inline enum free_tree
+__get_tree_for_flags(unsigned long flags)
+{
+	return (flags & DRM_BUDDY_CLEAR_ALLOCATION) ? CLEAR_TREE : DIRTY_TREE;
+}
+
+static inline struct drm_buddy_block *
+rbtree_get_entry(struct rb_node *node)
+{
+	return node ? rb_entry(node, struct drm_buddy_block, rb) : NULL;
+}
+
+static inline struct drm_buddy_block *
+rbtree_prev_entry(struct rb_node *node)
+{
+	return rbtree_get_entry(rb_prev(node));
+}
+
+static inline struct drm_buddy_block *
+rbtree_first_entry(struct rb_root *root)
 {
+	return rbtree_get_entry(rb_first(root));
+}
+
+static inline struct drm_buddy_block *
+rbtree_last_entry(struct rb_root *root)
+{
+	return rbtree_get_entry(rb_last(root));
+}
+
+static inline bool rbtree_is_empty(struct rb_root *root)
+{
+	return RB_EMPTY_ROOT(root);
+}
+
+static void rbtree_insert(struct drm_buddy *mm,
+			  struct drm_buddy_block *block,
+			  enum free_tree tree)
+{
+	struct rb_node **link, *parent = NULL;
 	struct drm_buddy_block *node;
-	struct list_head *head;
+	struct rb_root *root;
+	unsigned int order;
 
-	head = &mm->free_list[drm_buddy_block_order(block)];
-	if (list_empty(head)) {
-		list_add(&block->link, head);
-		return;
-	}
+	order = drm_buddy_block_order(block);
+
+	root = __get_root(mm, order, tree);
+	link = &root->rb_node;
+
+	while (*link) {
+		parent = *link;
+		node = rbtree_get_entry(parent);
 
-	list_for_each_entry(node, head, link)
 		if (drm_buddy_block_offset(block) < drm_buddy_block_offset(node))
-			break;
+			link = &parent->rb_left;
+		else
+			link = &parent->rb_right;
+	}
+
+	block->tree = tree;
+
+	rb_link_node(&block->rb, parent, link);
+	rb_insert_color(&block->rb, root);
+}
+
+static void rbtree_remove(struct drm_buddy *mm,
+			  struct drm_buddy_block *block)
+{
+	unsigned int order = drm_buddy_block_order(block);
+	struct rb_root *root;
+
+	root = __get_root(mm, order, block->tree);
+	rb_erase(&block->rb, root);
 
-	__list_add(&block->link, node->link.prev, &node->link);
+	RB_CLEAR_NODE(&block->rb);
 }
 
-static void clear_reset(struct drm_buddy_block *block)
+static inline void clear_reset(struct drm_buddy_block *block)
 {
 	block->header &= ~DRM_BUDDY_HEADER_CLEAR;
 }
 
-static void mark_cleared(struct drm_buddy_block *block)
+static inline void mark_cleared(struct drm_buddy_block *block)
 {
 	block->header |= DRM_BUDDY_HEADER_CLEAR;
 }
 
-static void mark_allocated(struct drm_buddy_block *block)
+static inline void mark_allocated(struct drm_buddy *mm,
+				  struct drm_buddy_block *block)
 {
 	block->header &= ~DRM_BUDDY_HEADER_STATE;
 	block->header |= DRM_BUDDY_ALLOCATED;
 
-	list_del(&block->link);
+	rbtree_remove(mm, block);
 }
 
-static void mark_free(struct drm_buddy *mm,
-		      struct drm_buddy_block *block)
+static inline void mark_free(struct drm_buddy *mm,
+			     struct drm_buddy_block *block)
 {
+	enum free_tree tree;
+
 	block->header &= ~DRM_BUDDY_HEADER_STATE;
 	block->header |= DRM_BUDDY_FREE;
 
-	list_insert_sorted(mm, block);
+	tree = __get_tree_for_block(block);
+
+	rbtree_insert(mm, block, tree);
 }
 
-static void mark_split(struct drm_buddy_block *block)
+static inline void mark_split(struct drm_buddy *mm,
+			      struct drm_buddy_block *block)
 {
 	block->header &= ~DRM_BUDDY_HEADER_STATE;
 	block->header |= DRM_BUDDY_SPLIT;
 
-	list_del(&block->link);
+	rbtree_remove(mm, block);
 }
 
 static inline bool overlaps(u64 s1, u64 e1, u64 s2, u64 e2)
@@ -147,7 +230,7 @@ static unsigned int __drm_buddy_free(str
 				mark_cleared(parent);
 		}
 
-		list_del(&buddy->link);
+		rbtree_remove(mm, buddy);
 		if (force_merge && drm_buddy_block_is_clear(buddy))
 			mm->clear_avail -= drm_buddy_block_size(mm, buddy);
 
@@ -177,44 +260,52 @@ static int __force_merge(struct drm_budd
 	if (min_order > mm->max_order)
 		return -EINVAL;
 
-	for (i = min_order - 1; i >= 0; i--) {
-		struct drm_buddy_block *block, *prev;
+	for_each_free_tree() {
+		for (i = min_order - 1; i >= 0; i--) {
+			struct rb_root *root = __get_root(mm, i, tree);
+			struct drm_buddy_block *block, *prev_block;
+
+			for_each_rb_entry_reverse_safe(block, prev_block, root, rb) {
+				struct drm_buddy_block *buddy;
+				u64 block_start, block_end;
 
-		list_for_each_entry_safe_reverse(block, prev, &mm->free_list[i], link) {
-			struct drm_buddy_block *buddy;
-			u64 block_start, block_end;
+				if (RB_EMPTY_NODE(&block->rb))
+					break;
 
-			if (!block->parent)
-				continue;
+				if (!block->parent)
+					continue;
 
-			block_start = drm_buddy_block_offset(block);
-			block_end = block_start + drm_buddy_block_size(mm, block) - 1;
+				block_start = drm_buddy_block_offset(block);
+				block_end = block_start + drm_buddy_block_size(mm, block) - 1;
 
-			if (!contains(start, end, block_start, block_end))
-				continue;
+				if (!contains(start, end, block_start, block_end))
+					continue;
 
-			buddy = __get_buddy(block);
-			if (!drm_buddy_block_is_free(buddy))
-				continue;
+				buddy = __get_buddy(block);
+				if (!drm_buddy_block_is_free(buddy))
+					continue;
 
-			WARN_ON(drm_buddy_block_is_clear(block) ==
-				drm_buddy_block_is_clear(buddy));
+				WARN_ON(drm_buddy_block_is_clear(block) ==
+					drm_buddy_block_is_clear(buddy));
 
-			/*
-			 * If the prev block is same as buddy, don't access the
-			 * block in the next iteration as we would free the
-			 * buddy block as part of the free function.
-			 */
-			if (prev == buddy)
-				prev = list_prev_entry(prev, link);
+				/*
+				 * If the prev block is same as buddy, don't access the
+				 * block in the next iteration as we would free the
+				 * buddy block as part of the free function.
+				 */
+				if (prev_block && prev_block == buddy) {
+					if (prev_block != rbtree_first_entry(root))
+						prev_block = rbtree_prev_entry(&prev_block->rb);
+				}
 
-			list_del(&block->link);
-			if (drm_buddy_block_is_clear(block))
-				mm->clear_avail -= drm_buddy_block_size(mm, block);
+				rbtree_remove(mm, block);
+				if (drm_buddy_block_is_clear(block))
+					mm->clear_avail -= drm_buddy_block_size(mm, block);
 
-			order = __drm_buddy_free(mm, block, true);
-			if (order >= min_order)
-				return 0;
+				order = __drm_buddy_free(mm, block, true);
+				if (order >= min_order)
+					return 0;
+			}
 		}
 	}
 
@@ -257,14 +348,22 @@ int drm_buddy_init(struct drm_buddy *mm,
 
 	BUG_ON(mm->max_order > DRM_BUDDY_MAX_ORDER);
 
-	mm->free_list = kmalloc_array(mm->max_order + 1,
-				      sizeof(struct list_head),
-				      GFP_KERNEL);
-	if (!mm->free_list)
+	mm->clear_tree = kmalloc_array(mm->max_order + 1,
+				       sizeof(struct rb_root),
+				       GFP_KERNEL);
+	if (!mm->clear_tree)
 		return -ENOMEM;
 
-	for (i = 0; i <= mm->max_order; ++i)
-		INIT_LIST_HEAD(&mm->free_list[i]);
+	mm->dirty_tree = kmalloc_array(mm->max_order + 1,
+				       sizeof(struct rb_root),
+				       GFP_KERNEL);
+	if (!mm->dirty_tree)
+		goto out_free_clear_tree;
+
+	for (i = 0; i <= mm->max_order; ++i) {
+		mm->clear_tree[i] = RB_ROOT;
+		mm->dirty_tree[i] = RB_ROOT;
+	}
 
 	mm->n_roots = hweight64(size);
 
@@ -272,7 +371,7 @@ int drm_buddy_init(struct drm_buddy *mm,
 				  sizeof(struct drm_buddy_block *),
 				  GFP_KERNEL);
 	if (!mm->roots)
-		goto out_free_list;
+		goto out_free_dirty_tree;
 
 	offset = 0;
 	i = 0;
@@ -311,8 +410,10 @@ out_free_roots:
 	while (i--)
 		drm_block_free(mm, mm->roots[i]);
 	kfree(mm->roots);
-out_free_list:
-	kfree(mm->free_list);
+out_free_dirty_tree:
+	kfree(mm->dirty_tree);
+out_free_clear_tree:
+	kfree(mm->clear_tree);
 	return -ENOMEM;
 }
 EXPORT_SYMBOL(drm_buddy_init);
@@ -328,12 +429,13 @@ void drm_buddy_fini(struct drm_buddy *mm
 {
 	u64 root_size, size, start;
 	unsigned int order;
+	const unsigned int chunk_shift = ilog2(mm->chunk_size);
 	int i;
 
 	size = mm->size;
 
 	for (i = 0; i < mm->n_roots; ++i) {
-		order = ilog2(size) - ilog2(mm->chunk_size);
+		order = ilog2(size) - chunk_shift;
 		start = drm_buddy_block_offset(mm->roots[i]);
 		__force_merge(mm, start, start + size, order);
 
@@ -349,7 +451,8 @@ void drm_buddy_fini(struct drm_buddy *mm
 	WARN_ON(mm->avail != mm->size);
 
 	kfree(mm->roots);
-	kfree(mm->free_list);
+	kfree(mm->clear_tree);
+	kfree(mm->dirty_tree);
 }
 EXPORT_SYMBOL(drm_buddy_fini);
 
@@ -357,32 +460,79 @@ static int split_block(struct drm_buddy
 		       struct drm_buddy_block *block)
 {
 	unsigned int block_order = drm_buddy_block_order(block) - 1;
-	u64 offset = drm_buddy_block_offset(block);
+	const u64 left_off = drm_buddy_block_offset(block);
+	const u64 right_off = left_off + (mm->chunk_size << block_order);
+	struct drm_buddy_block *left, *right;
+	void *objs[2];
+	int n;
 
 	BUG_ON(!drm_buddy_block_is_free(block));
 	BUG_ON(!drm_buddy_block_order(block));
 
-	block->left = drm_block_alloc(mm, block, block_order, offset);
-	if (!block->left)
-		return -ENOMEM;
+	/* Fast path: bulk allocate both children in one call. */
+	n = kmem_cache_alloc_bulk(slab_blocks, GFP_KERNEL, 2, objs);
+	if (likely(n == 2)) {
+		left = objs[0];
+		right = objs[1];
+
+		/* Preserve original zalloc semantics: fully zero the objects. */
+		memset(left, 0, sizeof(*left));
+		memset(right, 0, sizeof(*right));
+
+		left->header = left_off | block_order;
+		right->header = right_off | block_order;
 
-	block->right = drm_block_alloc(mm, block, block_order,
-				       offset + (mm->chunk_size << block_order));
-	if (!block->right) {
-		drm_block_free(mm, block->left);
-		return -ENOMEM;
+		left->parent = block;
+		right->parent = block;
+
+		RB_CLEAR_NODE(&left->rb);
+		RB_CLEAR_NODE(&right->rb);
+
+		BUG_ON(left->header & DRM_BUDDY_HEADER_UNUSED);
+		BUG_ON(right->header & DRM_BUDDY_HEADER_UNUSED);
+	} else {
+		/* Fallback to the original, safe per-object zalloc path. */
+		if (n == 1)
+			kmem_cache_free(slab_blocks, objs[0]);
+
+		left = kmem_cache_zalloc(slab_blocks, GFP_KERNEL);
+		if (unlikely(!left))
+			return -ENOMEM;
+
+		right = kmem_cache_zalloc(slab_blocks, GFP_KERNEL);
+		if (unlikely(!right)) {
+			kmem_cache_free(slab_blocks, left);
+			return -ENOMEM;
+		}
+
+		left->header = left_off | block_order;
+		right->header = right_off | block_order;
+
+		left->parent = block;
+		right->parent = block;
+
+		RB_CLEAR_NODE(&left->rb);
+		RB_CLEAR_NODE(&right->rb);
+
+		BUG_ON(left->header & DRM_BUDDY_HEADER_UNUSED);
+		BUG_ON(right->header & DRM_BUDDY_HEADER_UNUSED);
 	}
 
-	mark_free(mm, block->left);
-	mark_free(mm, block->right);
+	/* Wire children into the parent. */
+	block->left = left;
+	block->right = right;
 
+	/* Inherit clear state from parent if applicable. */
 	if (drm_buddy_block_is_clear(block)) {
-		mark_cleared(block->left);
-		mark_cleared(block->right);
+		mark_cleared(left);
+		mark_cleared(right);
 		clear_reset(block);
 	}
 
-	mark_split(block);
+	/* Children are now free; parent becomes split and leaves the free tree. */
+	mark_free(mm, left);
+	mark_free(mm, right);
+	mark_split(mm, block);
 
 	return 0;
 }
@@ -411,7 +561,9 @@ EXPORT_SYMBOL(drm_get_buddy);
  * @is_clear: blocks clear state
  *
  * Reset the clear state based on @is_clear value for each block
- * in the freelist.
+ * in the freelist. This is the correctly ported version for modern kernels
+ * using Red-Black Trees, with an added optimization to perform a bulk update
+ * of the clear_avail counter for improved efficiency.
  */
 void drm_buddy_reset_clear(struct drm_buddy *mm, bool is_clear)
 {
@@ -429,20 +581,34 @@ void drm_buddy_reset_clear(struct drm_bu
 		size -= root_size;
 	}
 
-	for (i = 0; i <= mm->max_order; ++i) {
-		struct drm_buddy_block *block;
+	if (is_clear) {
 
-		list_for_each_entry_reverse(block, &mm->free_list[i], link) {
-			if (is_clear != drm_buddy_block_is_clear(block)) {
-				if (is_clear) {
-					mark_cleared(block);
-					mm->clear_avail += drm_buddy_block_size(mm, block);
-				} else {
-					clear_reset(block);
-					mm->clear_avail -= drm_buddy_block_size(mm, block);
-				}
+		for (i = 0; i <= mm->max_order; ++i) {
+			struct rb_root *root = __get_root(mm, i, DIRTY_TREE);
+			struct drm_buddy_block *block, *n;
+
+			for_each_rb_entry_reverse_safe(block, n, root, rb) {
+				rbtree_remove(mm, block);
+				mark_cleared(block);
+				rbtree_insert(mm, block, CLEAR_TREE);
 			}
 		}
+
+		mm->clear_avail = mm->avail;
+	} else {
+
+		for (i = 0; i <= mm->max_order; ++i) {
+			struct rb_root *root = __get_root(mm, i, CLEAR_TREE);
+			struct drm_buddy_block *block, *n;
+
+			for_each_rb_entry_reverse_safe(block, n, root, rb) {
+				rbtree_remove(mm, block);
+				clear_reset(block);
+				rbtree_insert(mm, block, DIRTY_TREE);
+			}
+		}
+
+		mm->clear_avail = 0;
 	}
 }
 EXPORT_SYMBOL(drm_buddy_reset_clear);
@@ -513,7 +679,7 @@ void drm_buddy_free_list(struct drm_budd
 }
 EXPORT_SYMBOL(drm_buddy_free_list);
 
-static bool block_incompatible(struct drm_buddy_block *block, unsigned int flags)
+static bool block_incompatible(struct drm_buddy_block *block, unsigned long flags)
 {
 	bool needs_clear = flags & DRM_BUDDY_CLEAR_ALLOCATION;
 
@@ -529,11 +695,11 @@ __alloc_range_bias(struct drm_buddy *mm,
 {
 	u64 req_size = mm->chunk_size << order;
 	struct drm_buddy_block *block;
-	struct drm_buddy_block *buddy;
 	LIST_HEAD(dfs);
 	int err;
 	int i;
 
+	/* Make end inclusive for overlaps() comparisons below. */
 	end = end - 1;
 
 	for (i = 0; i < mm->n_roots; ++i)
@@ -563,6 +729,7 @@ __alloc_range_bias(struct drm_buddy *mm,
 		if (drm_buddy_block_is_allocated(block))
 			continue;
 
+		/* If partially overlapping, ensure alignment feasibility. */
 		if (block_start < start || block_end > end) {
 			u64 adjusted_start = max(block_start, start);
 			u64 adjusted_end = min(block_end, end);
@@ -575,21 +742,19 @@ __alloc_range_bias(struct drm_buddy *mm,
 		if (!fallback && block_incompatible(block, flags))
 			continue;
 
+		/* Exact fit within range and correct order? Take it if free. */
 		if (contains(start, end, block_start, block_end) &&
 		    order == drm_buddy_block_order(block)) {
-			/*
-			 * Find the free block within the range.
-			 */
 			if (drm_buddy_block_is_free(block))
 				return block;
-
 			continue;
 		}
 
+		/* Descend. */
 		if (!drm_buddy_block_is_split(block)) {
 			err = split_block(mm, block);
 			if (unlikely(err))
-				goto err_undo;
+				return ERR_PTR(err);
 		}
 
 		list_add(&block->right->tmp_link, &dfs);
@@ -597,19 +762,6 @@ __alloc_range_bias(struct drm_buddy *mm,
 	} while (1);
 
 	return ERR_PTR(-ENOSPC);
-
-err_undo:
-	/*
-	 * We really don't want to leave around a bunch of split blocks, since
-	 * bigger is better, so make sure we merge everything back before we
-	 * free the allocated blocks.
-	 */
-	buddy = __get_buddy(block);
-	if (buddy &&
-	    (drm_buddy_block_is_free(block) &&
-	     drm_buddy_block_is_free(buddy)))
-		__drm_buddy_free(mm, block, false);
-	return ERR_PTR(err);
 }
 
 static struct drm_buddy_block *
@@ -631,21 +783,20 @@ __drm_buddy_alloc_range_bias(struct drm_
 }
 
 static struct drm_buddy_block *
-get_maxblock(struct drm_buddy *mm, unsigned int order,
-	     unsigned long flags)
+get_maxblock(struct drm_buddy *mm,
+	     unsigned int order,
+	     enum free_tree tree)
 {
 	struct drm_buddy_block *max_block = NULL, *block = NULL;
+	struct rb_root *root;
 	unsigned int i;
 
 	for (i = order; i <= mm->max_order; ++i) {
-		struct drm_buddy_block *tmp_block;
-
-		list_for_each_entry_reverse(tmp_block, &mm->free_list[i], link) {
-			if (block_incompatible(tmp_block, flags))
+		root = __get_root(mm, i, tree);
+		if (!rbtree_is_empty(root)) {
+			block = rbtree_last_entry(root);
+			if (!block)
 				continue;
-
-			block = tmp_block;
-			break;
 		}
 
 		if (!block)
@@ -666,43 +817,50 @@ get_maxblock(struct drm_buddy *mm, unsig
 }
 
 static struct drm_buddy_block *
-alloc_from_freelist(struct drm_buddy *mm,
+alloc_from_freetree(struct drm_buddy *mm,
 		    unsigned int order,
 		    unsigned long flags)
 {
 	struct drm_buddy_block *block = NULL;
+	struct rb_root *root;
+	enum free_tree tree;
 	unsigned int tmp;
-	int err;
+
+	tree = __get_tree_for_flags(flags);
 
 	if (flags & DRM_BUDDY_TOPDOWN_ALLOCATION) {
-		block = get_maxblock(mm, order, flags);
-		if (block)
-			/* Store the obtained block order */
-			tmp = drm_buddy_block_order(block);
+		/* Prefer an exact-order block at the highest address first. */
+		root = __get_root(mm, order, tree);
+		if (!rbtree_is_empty(root)) {
+			block = rbtree_last_entry(root);
+			if (block)
+				tmp = order;
+		}
+		/* Fallback: pick max-offset block across all higher orders. */
+		if (!block) {
+			block = get_maxblock(mm, order, tree);
+			if (block)
+				tmp = drm_buddy_block_order(block);
+		}
 	} else {
 		for (tmp = order; tmp <= mm->max_order; ++tmp) {
-			struct drm_buddy_block *tmp_block;
-
-			list_for_each_entry_reverse(tmp_block, &mm->free_list[tmp], link) {
-				if (block_incompatible(tmp_block, flags))
-					continue;
-
-				block = tmp_block;
-				break;
+			root = __get_root(mm, tmp, tree);
+			if (!rbtree_is_empty(root)) {
+				block = rbtree_last_entry(root);
+				if (block)
+					break;
 			}
-
-			if (block)
-				break;
 		}
 	}
 
 	if (!block) {
-		/* Fallback method */
+		/* Try allocating from the other tree. */
+		tree = (tree == CLEAR_TREE) ? DIRTY_TREE : CLEAR_TREE;
+
 		for (tmp = order; tmp <= mm->max_order; ++tmp) {
-			if (!list_empty(&mm->free_list[tmp])) {
-				block = list_last_entry(&mm->free_list[tmp],
-							struct drm_buddy_block,
-							link);
+			root = __get_root(mm, tmp, tree);
+			if (!rbtree_is_empty(root)) {
+				block = rbtree_last_entry(root);
 				if (block)
 					break;
 			}
@@ -715,19 +873,15 @@ alloc_from_freelist(struct drm_buddy *mm
 	BUG_ON(!drm_buddy_block_is_free(block));
 
 	while (tmp != order) {
-		err = split_block(mm, block);
+		int err = split_block(mm, block);
 		if (unlikely(err))
-			goto err_undo;
+			return ERR_PTR(err);
 
+		/* Always bias to the right child for higher addresses. */
 		block = block->right;
 		tmp--;
 	}
 	return block;
-
-err_undo:
-	if (tmp != order)
-		__drm_buddy_free(mm, block, false);
-	return ERR_PTR(err);
 }
 
 static int __alloc_range(struct drm_buddy *mm,
@@ -737,7 +891,6 @@ static int __alloc_range(struct drm_budd
 			 u64 *total_allocated_on_err)
 {
 	struct drm_buddy_block *block;
-	struct drm_buddy_block *buddy;
 	u64 total_allocated = 0;
 	LIST_HEAD(allocated);
 	u64 end;
@@ -770,11 +923,12 @@ static int __alloc_range(struct drm_budd
 
 		if (contains(start, end, block_start, block_end)) {
 			if (drm_buddy_block_is_free(block)) {
-				mark_allocated(block);
-				total_allocated += drm_buddy_block_size(mm, block);
-				mm->avail -= drm_buddy_block_size(mm, block);
+				const u64 bsz = drm_buddy_block_size(mm, block);
+				mark_allocated(mm, block);
+				total_allocated += bsz;
+				mm->avail -= bsz;
 				if (drm_buddy_block_is_clear(block))
-					mm->clear_avail -= drm_buddy_block_size(mm, block);
+					mm->clear_avail -= bsz;
 				list_add_tail(&block->link, &allocated);
 				continue;
 			} else if (!mm->clear_avail) {
@@ -786,7 +940,7 @@ static int __alloc_range(struct drm_budd
 		if (!drm_buddy_block_is_split(block)) {
 			err = split_block(mm, block);
 			if (unlikely(err))
-				goto err_undo;
+				goto err_free;
 		}
 
 		list_add(&block->right->tmp_link, dfs);
@@ -799,21 +953,8 @@ static int __alloc_range(struct drm_budd
 	}
 
 	list_splice_tail(&allocated, blocks);
-
 	return 0;
 
-err_undo:
-	/*
-	 * We really don't want to leave around a bunch of split blocks, since
-	 * bigger is better, so make sure we merge everything back before we
-	 * free the allocated blocks.
-	 */
-	buddy = __get_buddy(block);
-	if (buddy &&
-	    (drm_buddy_block_is_free(block) &&
-	     drm_buddy_block_is_free(buddy)))
-		__drm_buddy_free(mm, block, false);
-
 err_free:
 	if (err == -ENOSPC && total_allocated_on_err) {
 		list_splice_tail(&allocated, blocks);
@@ -821,7 +962,6 @@ err_free:
 	} else {
 		drm_buddy_free_list_internal(mm, &allocated);
 	}
-
 	return err;
 }
 
@@ -848,48 +988,52 @@ static int __alloc_contig_try_harder(str
 {
 	u64 rhs_offset, lhs_offset, lhs_size, filled;
 	struct drm_buddy_block *block;
-	struct list_head *list;
 	LIST_HEAD(blocks_lhs);
 	unsigned long pages;
 	unsigned int order;
+	const unsigned int chunk_shift = ilog2(mm->chunk_size);
 	u64 modify_size;
 	int err;
 
 	modify_size = rounddown_pow_of_two(size);
-	pages = modify_size >> ilog2(mm->chunk_size);
+	pages = modify_size >> chunk_shift;
 	order = fls(pages) - 1;
 	if (order == 0)
 		return -ENOSPC;
 
-	list = &mm->free_list[order];
-	if (list_empty(list))
+	if (rbtree_is_empty(__get_root(mm, order, CLEAR_TREE)) &&
+	    rbtree_is_empty(__get_root(mm, order, DIRTY_TREE)))
 		return -ENOSPC;
 
-	list_for_each_entry_reverse(block, list, link) {
-		/* Allocate blocks traversing RHS */
-		rhs_offset = drm_buddy_block_offset(block);
-		err =  __drm_buddy_alloc_range(mm, rhs_offset, size,
-					       &filled, blocks);
-		if (!err || err != -ENOSPC)
-			return err;
-
-		lhs_size = max((size - filled), min_block_size);
-		if (!IS_ALIGNED(lhs_size, min_block_size))
-			lhs_size = round_up(lhs_size, min_block_size);
-
-		/* Allocate blocks traversing LHS */
-		lhs_offset = drm_buddy_block_offset(block) - lhs_size;
-		err =  __drm_buddy_alloc_range(mm, lhs_offset, lhs_size,
-					       NULL, &blocks_lhs);
-		if (!err) {
-			list_splice(&blocks_lhs, blocks);
-			return 0;
-		} else if (err != -ENOSPC) {
+	for_each_free_tree() {
+		struct rb_root *root = __get_root(mm, order, tree);
+
+		for_each_rb_entry_reverse(block, root, rb) {
+			/* Allocate blocks traversing RHS */
+			rhs_offset = drm_buddy_block_offset(block);
+			err = __drm_buddy_alloc_range(mm, rhs_offset, size,
+						      &filled, blocks);
+			if (!err || err != -ENOSPC)
+				return err;
+
+			lhs_size = max((size - filled), min_block_size);
+			if (!IS_ALIGNED(lhs_size, min_block_size))
+				lhs_size = round_up(lhs_size, min_block_size);
+
+			/* Allocate blocks traversing LHS */
+			lhs_offset = drm_buddy_block_offset(block) - lhs_size;
+			err = __drm_buddy_alloc_range(mm, lhs_offset, lhs_size,
+						      NULL, &blocks_lhs);
+			if (!err) {
+				list_splice(&blocks_lhs, blocks);
+				return 0;
+			} else if (err != -ENOSPC) {
+				drm_buddy_free_list_internal(mm, blocks);
+				return err;
+			}
+			/* Free blocks for the next iteration */
 			drm_buddy_free_list_internal(mm, blocks);
-			return err;
 		}
-		/* Free blocks for the next iteration */
-		drm_buddy_free_list_internal(mm, blocks);
 	}
 
 	return -ENOSPC;
@@ -975,7 +1119,7 @@ int drm_buddy_block_trim(struct drm_budd
 	list_add(&block->tmp_link, &dfs);
 	err =  __alloc_range(mm, &dfs, new_start, new_size, blocks, NULL);
 	if (err) {
-		mark_allocated(block);
+		mark_allocated(mm, block);
 		mm->avail -= drm_buddy_block_size(mm, block);
 		if (drm_buddy_block_is_clear(block))
 			mm->clear_avail -= drm_buddy_block_size(mm, block);
@@ -998,8 +1142,8 @@ __drm_buddy_alloc_blocks(struct drm_budd
 		return  __drm_buddy_alloc_range_bias(mm, start, end,
 						     order, flags);
 	else
-		/* Allocate from freelist */
-		return alloc_from_freelist(mm, order, flags);
+		/* Allocate from freetree */
+		return alloc_from_freetree(mm, order, flags);
 }
 
 /**
@@ -1016,8 +1160,8 @@ __drm_buddy_alloc_blocks(struct drm_budd
  * alloc_range_bias() called on range limitations, which traverses
  * the tree and returns the desired block.
  *
- * alloc_from_freelist() called when *no* range restrictions
- * are enforced, which picks the block from the freelist.
+ * alloc_from_freetree() called when *no* range restrictions
+ * are enforced, which picks the block from the freetree.
  *
  * Returns:
  * 0 on success, error code on failure.
@@ -1033,6 +1177,7 @@ int drm_buddy_alloc_blocks(struct drm_bu
 	unsigned int min_order, order;
 	LIST_HEAD(allocated);
 	unsigned long pages;
+	const unsigned int chunk_shift = ilog2(mm->chunk_size);
 	int err;
 
 	if (size < mm->chunk_size)
@@ -1064,7 +1209,7 @@ int drm_buddy_alloc_blocks(struct drm_bu
 	original_size = size;
 	original_min_size = min_block_size;
 
-	/* Roundup the size to power of 2 */
+	/* Roundup the size to power of 2 for contiguous requests */
 	if (flags & DRM_BUDDY_CONTIGUOUS_ALLOCATION) {
 		size = roundup_pow_of_two(size);
 		min_block_size = size;
@@ -1073,12 +1218,13 @@ int drm_buddy_alloc_blocks(struct drm_bu
 		size = round_up(size, min_block_size);
 	}
 
-	pages = size >> ilog2(mm->chunk_size);
+	pages = size >> chunk_shift;
 	order = fls(pages) - 1;
-	min_order = ilog2(min_block_size) - ilog2(mm->chunk_size);
+	min_order = ilog2(min_block_size) - chunk_shift;
 
 	do {
-		order = min(order, (unsigned int)fls(pages) - 1);
+		unsigned int hi = fls(pages) - 1;
+		order = min(order, hi);
 		BUG_ON(order > mm->max_order);
 		BUG_ON(order < min_order);
 
@@ -1119,20 +1265,22 @@ int drm_buddy_alloc_blocks(struct drm_bu
 			}
 		} while (1);
 
-		mark_allocated(block);
-		mm->avail -= drm_buddy_block_size(mm, block);
-		if (drm_buddy_block_is_clear(block))
-			mm->clear_avail -= drm_buddy_block_size(mm, block);
-		kmemleak_update_trace(block);
-		list_add_tail(&block->link, &allocated);
-
-		pages -= BIT(order);
+		{
+			const u64 bsz = mm->chunk_size << order;
+			mark_allocated(mm, block);
+			mm->avail -= bsz;
+			if (drm_buddy_block_is_clear(block))
+				mm->clear_avail -= bsz;
+			kmemleak_update_trace(block);
+			list_add_tail(&block->link, &allocated);
+			pages -= BIT(order);
+		}
 
 		if (!pages)
 			break;
 	} while (1);
 
-	/* Trim the allocated block to the required size */
+	/* Trim the allocated block(s) to the required size */
 	if (!(flags & DRM_BUDDY_TRIM_DISABLE) &&
 	    original_size != size) {
 		struct list_head *trim_list;
@@ -1201,11 +1349,16 @@ void drm_buddy_print(struct drm_buddy *m
 
 	for (order = mm->max_order; order >= 0; order--) {
 		struct drm_buddy_block *block;
+		struct rb_root *root;
 		u64 count = 0, free;
 
-		list_for_each_entry(block, &mm->free_list[order], link) {
-			BUG_ON(!drm_buddy_block_is_free(block));
-			count++;
+		for_each_free_tree() {
+			root = __get_root(mm, order, tree);
+
+			for_each_rb_entry(block, root, rb) {
+				BUG_ON(!drm_buddy_block_is_free(block));
+				count++;
+			}
 		}
 
 		drm_printf(p, "order-%2d ", order);
@@ -1228,7 +1381,7 @@ static void drm_buddy_module_exit(void)
 
 static int __init drm_buddy_module_init(void)
 {
-	slab_blocks = KMEM_CACHE(drm_buddy_block, 0);
+	slab_blocks = kmem_cache_create("drm_buddy_block", sizeof(struct drm_buddy_block), 64, SLAB_HWCACHE_ALIGN, NULL);
 	if (!slab_blocks)
 		return -ENOMEM;
 

--- a/include/drm/drm_buddy.h
+++ b/include/drm/drm_buddy.h
@@ -10,6 +10,7 @@
 #include <linux/list.h>
 #include <linux/slab.h>
 #include <linux/sched.h>
+#include <linux/rbtree.h>
 
 #include <drm/drm_print.h>
 
@@ -22,6 +23,44 @@
 	start__ >= max__ || size__ > max__ - start__; \
 })
 
+/*
+ * for_each_rb_entry() - iterate over an RB tree in order
+ * @pos:	the struct type * to use as a loop cursor
+ * @root:	pointer to struct rb_root to iterate
+ * @member:	name of the rb_node field within the struct
+ */
+#define for_each_rb_entry(pos, root, member) \
+	for (pos = rb_entry_safe(rb_first(root), typeof(*pos), member); \
+	     pos; \
+	     pos = rb_entry_safe(rb_next(&(pos)->member), typeof(*pos), member))
+
+/*
+ * for_each_rb_entry_reverse() - iterate over an RB tree in reverse order
+ * @pos:	the struct type * to use as a loop cursor
+ * @root:	pointer to struct rb_root to iterate
+ * @member:	name of the rb_node field within the struct
+ */
+#define for_each_rb_entry_reverse(pos, root, member) \
+	for (pos = rb_entry_safe(rb_last(root), typeof(*pos), member); \
+	     pos; \
+	     pos = rb_entry_safe(rb_prev(&(pos)->member), typeof(*pos), member))
+
+/**
+ * for_each_rb_entry_reverse_safe() - safely iterate over an RB tree in reverse order
+ * @pos:	the struct type * to use as a loop cursor.
+ * @n:		another struct type * to use as temporary storage.
+ * @root:	pointer to struct rb_root to iterate.
+ * @member:	name of the rb_node field within the struct.
+ */
+#define for_each_rb_entry_reverse_safe(pos, n, root, member) \
+	for (pos = rb_entry_safe(rb_last(root), typeof(*pos), member), \
+	     n = pos ? rb_entry_safe(rb_prev(&(pos)->member), typeof(*pos), member) : NULL; \
+	     pos; \
+	     pos = n, n = pos ? rb_entry_safe(rb_prev(&(pos)->member), typeof(*pos), member) : NULL)
+
+#define for_each_free_tree() \
+	for (enum free_tree tree = CLEAR_TREE; tree <= DIRTY_TREE; tree++)
+
 #define DRM_BUDDY_RANGE_ALLOCATION		BIT(0)
 #define DRM_BUDDY_TOPDOWN_ALLOCATION		BIT(1)
 #define DRM_BUDDY_CONTIGUOUS_ALLOCATION		BIT(2)
@@ -29,6 +68,11 @@
 #define DRM_BUDDY_CLEARED			BIT(4)
 #define DRM_BUDDY_TRIM_DISABLE			BIT(5)
 
+enum free_tree {
+	CLEAR_TREE = 0,
+	DIRTY_TREE,
+};
+
 struct drm_buddy_block {
 #define DRM_BUDDY_HEADER_OFFSET GENMASK_ULL(63, 12)
 #define DRM_BUDDY_HEADER_STATE  GENMASK_ULL(11, 10)
@@ -55,6 +99,9 @@ struct drm_buddy_block {
 	 */
 	struct list_head link;
 	struct list_head tmp_link;
+
+	enum free_tree tree;
+	struct rb_node rb;
 };
 
 /* Order-zero must be at least SZ_4K */
@@ -68,7 +115,8 @@ struct drm_buddy_block {
  */
 struct drm_buddy {
 	/* Maintain a free list for each order. */
-	struct list_head *free_list;
+	struct rb_root *clear_tree;
+	struct rb_root *dirty_tree;
 
 	/*
 	 * Maintain explicit binary tree(s) to track the allocation of the

--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_sched.c	2025-09-11 17:23:23.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_sched.c	2025-09-18 01:48:50.119317976 +0200
@@ -35,53 +35,69 @@ static int amdgpu_sched_process_priority
 						  int fd,
 						  int32_t priority)
 {
-	CLASS(fd, f)(fd);
+	struct file *filp;
 	struct amdgpu_fpriv *fpriv;
 	struct amdgpu_ctx_mgr *mgr;
 	struct amdgpu_ctx *ctx;
-	uint32_t id;
-	int r;
+	unsigned int id;
+	int r = 0;
 
-	if (fd_empty(f))
-		return -EINVAL;
+	/* Silence unused parameter warning (adev is not used here). */
+	(void)adev;
 
-	r = amdgpu_file_to_fpriv(fd_file(f), &fpriv);
-	if (r)
+	filp = fget(fd);
+	if (!filp)
+		return -EBADF;
+
+	r = amdgpu_file_to_fpriv(filp, &fpriv);
+	if (r) {
+		fput(filp);
 		return r;
+	}
 
 	mgr = &fpriv->ctx_mgr;
 	mutex_lock(&mgr->lock);
-	idr_for_each_entry(&mgr->ctx_handles, ctx, id)
+	idr_for_each_entry(&mgr->ctx_handles, ctx, id) {
 		amdgpu_ctx_priority_override(ctx, priority);
+	}
 	mutex_unlock(&mgr->lock);
 
+	fput(filp);
 	return 0;
 }
 
 static int amdgpu_sched_context_priority_override(struct amdgpu_device *adev,
 						  int fd,
-						  unsigned ctx_id,
+						  unsigned int ctx_id,
 						  int32_t priority)
 {
-	CLASS(fd, f)(fd);
+	struct file *filp;
 	struct amdgpu_fpriv *fpriv;
 	struct amdgpu_ctx *ctx;
-	int r;
+	int r = 0;
 
-	if (fd_empty(f))
-		return -EINVAL;
+	/* Silence unused parameter warning (adev is not used here). */
+	(void)adev;
 
-	r = amdgpu_file_to_fpriv(fd_file(f), &fpriv);
-	if (r)
+	filp = fget(fd);
+	if (!filp)
+		return -EBADF;
+
+	r = amdgpu_file_to_fpriv(filp, &fpriv);
+	if (r) {
+		fput(filp);
 		return r;
+	}
 
 	ctx = amdgpu_ctx_get(fpriv, ctx_id);
-
-	if (!ctx)
+	if (!ctx) {
+		fput(filp);
 		return -EINVAL;
+	}
 
 	amdgpu_ctx_priority_override(ctx, priority);
 	amdgpu_ctx_put(ctx);
+	fput(filp);
 	return 0;
 }
 
@@ -92,8 +108,7 @@ int amdgpu_sched_ioctl(struct drm_device
 	struct amdgpu_device *adev = drm_to_adev(dev);
 	int r;
 
-	/* First check the op, then the op's argument.
-	 */
+	/* Validate op first. */
 	switch (args->in.op) {
 	case AMDGPU_SCHED_OP_PROCESS_PRIORITY_OVERRIDE:
 	case AMDGPU_SCHED_OP_CONTEXT_PRIORITY_OVERRIDE:
@@ -103,11 +118,13 @@ int amdgpu_sched_ioctl(struct drm_device
 		return -EINVAL;
 	}
 
+	/* Validate priority. */
 	if (!amdgpu_ctx_priority_is_valid(args->in.priority)) {
 		WARN(1, "Invalid context priority %d\n", args->in.priority);
 		return -EINVAL;
 	}
 
+	/* Execute the requested operation. */
 	switch (args->in.op) {
 	case AMDGPU_SCHED_OP_PROCESS_PRIORITY_OVERRIDE:
 		r = amdgpu_sched_process_priority_override(adev,
@@ -121,8 +138,7 @@ int amdgpu_sched_ioctl(struct drm_device
 							   args->in.priority);
 		break;
 	default:
-		/* Impossible.
-		 */
+		/* Should be unreachable. */
 		r = -EINVAL;
 		break;
 	}

--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c	2025-09-11 17:23:23.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c	2025-09-18 01:47:46.232271897 +0200

--- a/drivers/gpu/drm/amd/amdgpu/gfx_v9_0.c	2025-07-12 17:16:53.286394076 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/gfx_v9_0.c	2025-07-12 17:43:17.514165504 +0200

--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c	2025-05-29 11:14:09.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c	2025-08-12 11:33:50.388339649 +0200


--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c	2025-10-04 08:44:25.287659188 +0200
@@ -25,17 +25,27 @@
  *          Alex Deucher
  *          Jerome Glisse
  */
+
 #include <linux/ktime.h>
 #include <linux/module.h>
 #include <linux/pagemap.h>
 #include <linux/pci.h>
 #include <linux/dma-buf.h>
+#include <linux/overflow.h>
+#include <linux/dma-resv.h>
+#include <linux/jiffies.h>
+#include <linux/math64.h>
+#include <linux/percpu.h>
+#include <linux/log2.h>
+#include <linux/cache.h>
+#include <linux/preempt.h>
 
 #include <drm/amdgpu_drm.h>
 #include <drm/drm_drv.h>
 #include <drm/drm_exec.h>
 #include <drm/drm_gem_ttm_helper.h>
 #include <drm/ttm/ttm_tt.h>
+#include <drm/ttm/ttm_resource.h>
 #include <drm/drm_syncobj.h>
 
 #include "amdgpu.h"
@@ -45,42 +55,62 @@
 #include "amdgpu_xgmi.h"
 #include "amdgpu_vm.h"
 
+/* -------------------- Fence input fast-path -------------------- */
+
 static int
 amdgpu_gem_add_input_fence(struct drm_file *filp,
 			   uint64_t syncobj_handles_array,
 			   uint32_t num_syncobj_handles)
 {
 	struct dma_fence *fence;
+	/* Expand stack buffer to 64 entries (256 bytes, safe for kernel stack) */
+	uint32_t syncobj_handles_stack[256];
 	uint32_t *syncobj_handles;
-	int ret, i;
+	int ret = 0;
+	uint32_t i;
 
 	if (!num_syncobj_handles)
 		return 0;
 
-	syncobj_handles = memdup_user(u64_to_user_ptr(syncobj_handles_array),
-				      size_mul(sizeof(uint32_t), num_syncobj_handles));
-	if (IS_ERR(syncobj_handles))
-		return PTR_ERR(syncobj_handles);
+	/* Guard against overflow and unreasonable counts */
+	if (unlikely(num_syncobj_handles > 1024))
+		return -EINVAL;
 
-	for (i = 0; i < num_syncobj_handles; i++) {
+	/* Use stack for up to 64 entries (covers 99% of real-world cases), heap only for extreme counts */
+	if (num_syncobj_handles <= ARRAY_SIZE(syncobj_handles_stack)) {
+		if (copy_from_user(syncobj_handles_stack,
+				   u64_to_user_ptr(syncobj_handles_array),
+				   num_syncobj_handles * sizeof(uint32_t)))
+			return -EFAULT;
+		syncobj_handles = syncobj_handles_stack;
+	} else {
+		syncobj_handles = memdup_user(u64_to_user_ptr(syncobj_handles_array),
+					      size_mul(sizeof(uint32_t), num_syncobj_handles));
+		if (IS_ERR(syncobj_handles))
+			return PTR_ERR(syncobj_handles);
+	}
 
-		if (!syncobj_handles[i]) {
+	for (i = 0; i < num_syncobj_handles; i++) {
+		if (unlikely(!syncobj_handles[i])) {
 			ret = -EINVAL;
-			goto free_memdup;
+			break;
 		}
 
 		ret = drm_syncobj_find_fence(filp, syncobj_handles[i], 0, 0, &fence);
 		if (ret)
-			goto free_memdup;
+			break;
 
-		dma_fence_wait(fence, false);
+		if (likely(!dma_fence_is_signaled(fence)))
+			ret = dma_fence_wait(fence, false);
 
-		/* TODO: optimize async handling */
 		dma_fence_put(fence);
+		if (ret)
+			break;
 	}
 
-free_memdup:
-	kfree(syncobj_handles);
+	if (num_syncobj_handles > ARRAY_SIZE(syncobj_handles_stack))
+		kfree(syncobj_handles);
+
 	return ret;
 }
 
@@ -153,31 +183,321 @@ amdgpu_gem_update_bo_mapping(struct drm_
 		drm_syncobj_add_point(syncobj, chain, last_update, point);
 }
 
+/* ------------- Vega-aware adaptive fault prefetch (HBM2-tuned, TLB-aware) ------------- */
+
+/* Refresh cached reciprocal at most every ~4ms */
+#define VRAM_RECIP_REFRESH_JIFFIES (HZ / 250)
+
+/* Absolute safety caps for all ASICs */
+#define PREFETCH_ABS_MIN_PAGES 1u
+#define PREFETCH_ABS_MAX_PAGES 128u
+#define PREFETCH_BOOST_CAP 256u
+
+/* Vega-specific TLB-aware cap (based on 64-entry L1 TLB per CU) */
+#define VEGA_TLB_AWARE_MAX_PAGES 56u  /* 87% of L1 TLB capacity, leave headroom */
+
+/* Large page promotion hint threshold (512KB = 128 pages = 1/4 of 2MB large page) */
+#define VEGA_LARGE_PAGE_HINT_THRESHOLD 256u
+
+/* Compute workload size thresholds for specialized strategy */
+#define VEGA_COMPUTE_LARGE_BO_THRESHOLD (64ULL << 20)  /* 64MB */
+#define VEGA_COMPUTE_SMALL_BO_THRESHOLD (4ULL << 20)   /* 4MB */
+
+/* Module parameters (tunables) */
+static unsigned int vega_pf_max_pages_vram = 32;
+module_param_named(vega_pf_max_pages_vram, vega_pf_max_pages_vram, uint, 0644);
+MODULE_PARM_DESC(vega_pf_max_pages_vram, "Max prefetch pages for VRAM BO faults (default 32, Vega TLB cap=48)");
+
+static unsigned int vega_pf_max_pages_gtt = 64;
+module_param_named(vega_pf_max_pages_gtt, vega_pf_max_pages_gtt, uint, 0644);
+MODULE_PARM_DESC(vega_pf_max_pages_gtt, "Max prefetch pages for GTT BO faults (default 64)");
+
+static unsigned int vega_pf_streak_window_jiffies = (HZ / 200);
+module_param_named(vega_pf_streak_window_jiffies, vega_pf_streak_window_jiffies, uint, 0644);
+MODULE_PARM_DESC(vega_pf_streak_window_jiffies, "Streak window in jiffies (default HZ/200)");
+
+/* HBM2-tuned burst threshold: 1µs (HBM2 latency ~130ns, so ~7-8 accesses within window) */
+static unsigned int vega_pf_burst_ns = 1000;
+module_param_named(vega_pf_burst_ns, vega_pf_burst_ns, uint, 0644);
+MODULE_PARM_DESC(vega_pf_burst_ns, "HBM2-tuned burst threshold in ns (default 1000)");
+
+/* PERF OPT: Consolidate per-cpu data to reduce access overhead. */
+struct vega_recip_cache {
+	u64 recip_q38;
+	unsigned long stamp;
+};
+static DEFINE_PER_CPU(struct vega_recip_cache, vega_recip_cache_pc);
+
+/* Per-CPU streak state with Vega-specific enhancements */
+struct vega_pf_state {
+	const void *last_bo;
+	const void *last_vma;
+	unsigned long last_addr;
+	u64 last_ns;
+	unsigned long last_j;
+	u8 streak;              /* 0..4 => up to +200% boost */
+	u8 direction;           /* 0=unknown, 1=forward, 2=reverse */
+	u16 sequential_pages;   /* Total pages in sustained sequential run (for large page hint) */
+	u32 last_stride;        /* Power-of-2 stride detection for compute workloads */
+	u8 stride_repeat_count; /* How many times we've seen same stride */
+} ____cacheline_aligned;
+
+static DEFINE_PER_CPU(struct vega_pf_state, vega_pf_pc);
+
+/* Fast VRAM usage percent with cached reciprocal */
+static u32 amdgpu_vram_usage_pct_fast(struct amdgpu_device *adev)
+{
+	struct ttm_resource_manager *mgr;
+	u64 used_bytes, vram_b;
+	u32 pct;
+	struct vega_recip_cache *p_cache;
+	unsigned long now_j;
+
+	if (unlikely(!adev))
+		return 0;
+
+	mgr = ttm_manager_type(&adev->mman.bdev, TTM_PL_VRAM);
+	if (unlikely(!mgr))
+		return 0;
+
+	used_bytes = ttm_resource_manager_usage(mgr);
+	now_j = jiffies;
+
+	/* Per-CPU data is only accessed by the current CPU; no preemption safety needed */
+	p_cache = this_cpu_ptr(&vega_recip_cache_pc);
+
+	if (unlikely(time_after(now_j, p_cache->stamp + VRAM_RECIP_REFRESH_JIFFIES) || p_cache->recip_q38 == 0)) {
+		/* Recompute reciprocal: (100 << 38) / vram_size for Q38 fixed-point percent */
+		vram_b = max_t(u64, adev->gmc.mc_vram_size, 1ULL);
+		p_cache->recip_q38 = div64_u64((100ULL << 38), vram_b);
+		p_cache->stamp = now_j;
+	}
+
+	/* Compute percentage: (used * recip) >> 38, clamped to [0, 100] */
+	pct = min_t(u32, mul_u64_u64_shr(used_bytes, p_cache->recip_q38, 38), 100U);
+	return pct;
+}
+
+/* Compute optimal prefetch for Vega: TLB-aware, large-page hinting, HBM2-tuned,
+ * compute-specialized, and alignment-optimized.
+ */
+static unsigned int amdgpu_vega_optimal_prefetch(struct amdgpu_device *adev,
+						 struct amdgpu_bo *abo,
+						 struct vm_fault *vmf,
+						 unsigned int base_pages)
+{
+	u32 vram_pct, dom;
+	u32 want, total_pages, cap;
+	u64 bo_size;
+	bool is_compute, is_vram, is_large_compute, is_small_compute;
+	struct vega_pf_state *pcs;
+	unsigned long now_j, delta_addr;
+	u64 now_ns, delta_ns;
+	int direction;
+
+	/* Early validation */
+	if (unlikely(!adev || !abo || !vmf || base_pages == 0))
+		return base_pages;
+
+	/* BO characteristics */
+	bo_size = abo->tbo.base.size;
+	dom = abo->preferred_domains;
+	is_vram = !!(dom & AMDGPU_GEM_DOMAIN_VRAM);
+	is_compute = !!(abo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS);
+
+	/* OPTIMIZATION V1: TLB-Aware Capping */
+	vram_pct = amdgpu_vram_usage_pct_fast(adev);
+
+	if (is_vram) {
+		cap = VEGA_TLB_AWARE_MAX_PAGES;
+		if (vram_pct >= 90U)
+			cap = 40U;
+	} else {
+		cap = min(vega_pf_max_pages_gtt, 64U);
+	}
+	cap = max(cap, PREFETCH_ABS_MIN_PAGES);
+
+	total_pages = (u32)min_t(u64, bo_size >> PAGE_SHIFT, U32_MAX);
+	if (total_pages == 0)
+		total_pages = 1;
+
+	/* OPTIMIZATION V4: Compute Workload Specialized Strategy */
+	is_large_compute = is_compute && (bo_size >= VEGA_COMPUTE_LARGE_BO_THRESHOLD);
+	is_small_compute = is_compute && (bo_size <= VEGA_COMPUTE_SMALL_BO_THRESHOLD);
+
+	if (is_large_compute) {
+		want = base_pages * 2U;
+	} else if (is_small_compute) {
+		want = base_pages / 2U;
+	} else {
+		/* PERF OPT: Replace LUT with equivalent, dependency-free arithmetic. */
+		u32 scale_pct = 120U - (vram_pct * 4U) / 5U;
+		want = (base_pages * scale_pct) / 100U;
+
+		if (total_pages > 1U)
+			want += ilog2(total_pages);
+
+		if (is_compute && vram_pct < 90U)
+			want += want >> 2; /* +25% */
+	}
+
+	/* OPTIMIZATION V2 & V3: Streak and Burst Detection */
+	pcs = this_cpu_ptr(&vega_pf_pc);
+	now_j = jiffies;
+	now_ns = ktime_get_ns();
+
+	if (pcs->last_bo == (const void *)abo &&
+	    pcs->last_vma == (const void *)vmf->vma &&
+	    time_before(now_j, pcs->last_j + vega_pf_streak_window_jiffies)) {
+		/* BUG FIX: Use kernel-standard U64_MAX. */
+		if (likely(now_ns >= pcs->last_ns)) {
+			delta_ns = now_ns - pcs->last_ns;
+		} else {
+			delta_ns = U64_MAX;
+		}
+
+		if (vmf->address > pcs->last_addr) {
+			direction = 1;  /* Forward */
+			delta_addr = vmf->address - pcs->last_addr;
+		} else if (vmf->address < pcs->last_addr) {
+			direction = 2;  /* Reverse */
+			delta_addr = 0;
+		} else {
+			direction = 0;  /* Same address */
+			delta_addr = 0;
+		}
+
+		if (direction == 1) {
+			unsigned long delta_pages = delta_addr >> PAGE_SHIFT;
+
+			if (delta_pages > 0 && delta_pages <= (want + 4)) {
+				if (pcs->streak < 4)
+					pcs->streak++;
+
+				if (is_vram) {
+					u32 new_seq = (u32)pcs->sequential_pages +
+						      min_t(u32, delta_pages, 512U);
+					pcs->sequential_pages = (u16)min(new_seq, 65535U);
+				}
+
+				/* HBM2 burst boost (+50%) */
+				if (delta_ns <= vega_pf_burst_ns) {
+					u32 boost = want >> 1;
+					want = min_t(u32, want + boost, PREFETCH_BOOST_CAP);
+				}
+
+				/* Streak boost (+50% per 2 levels) */
+				if (pcs->streak > 0) {
+					u32 boost = (want * pcs->streak) >> 1;
+					want = min_t(u32, want + boost, PREFETCH_BOOST_CAP);
+				}
+
+				if (is_vram && pcs->sequential_pages >= VEGA_LARGE_PAGE_HINT_THRESHOLD &&
+					want < 512U && total_pages >= 512U) {
+					want = 512U; /* Align to 2MB large page boundary */
+				}
+
+				if (is_compute && delta_pages > 1 && delta_pages <= 1024) {
+					u32 stride = (u32)delta_pages;
+					if (is_power_of_2(stride)) {
+						if (pcs->last_stride == stride) {
+							if (pcs->stride_repeat_count < 255)
+								pcs->stride_repeat_count++;
+							if (pcs->stride_repeat_count >= 3)
+								want = max(want, stride);
+						} else {
+							pcs->last_stride = stride;
+							pcs->stride_repeat_count = 1;
+						}
+					} else {
+						pcs->stride_repeat_count = 0;
+					}
+				}
+			} else {
+				pcs->streak = 0;
+				if (is_vram)
+					pcs->sequential_pages = 0;
+				pcs->stride_repeat_count = 0;
+			}
+		} else {
+			pcs->streak = 0;
+			if (is_vram)
+				pcs->sequential_pages = 0;
+			pcs->stride_repeat_count = 0;
+		}
+		pcs->direction = (u8)direction;
+	} else {
+		/* Reset all state */
+		memset(pcs, 0, sizeof(*pcs));
+	}
+
+	pcs->last_bo = (const void *)abo;
+	pcs->last_vma = (const void *)vmf->vma;
+	pcs->last_addr = vmf->address;
+	pcs->last_ns = now_ns;
+	pcs->last_j = now_j;
+
+	/* PERF OPT: Use simple, readable branches for high-pressure clamping. */
+	if (vram_pct >= 98U) {
+		want = min(want, base_pages);
+	} else if (vram_pct >= 95U) {
+		want = min(want, (want + base_pages) >> 1);
+	}
+
+	/* PERF OPT: Aggressively align to HBM2-friendly boundaries by rounding down. */
+	if (want >= 16U) {
+		want &= ~15U; /* Align down to 16 pages (64KB) */
+	} else if (want >= 4U) {
+		want &= ~3U; /* Align down to 4 pages (16KB) */
+	}
+
+	/* Final clamping to absolute and BO-specific limits. */
+	want = clamp(want, PREFETCH_ABS_MIN_PAGES, PREFETCH_ABS_MAX_PAGES);
+	want = min(want, cap);
+	want = min(want, total_pages);
+
+	return want;
+}
+
+/* -------------------- Fault handler (with adaptive prefetch) -------------------- */
+
 static vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)
 {
 	struct ttm_buffer_object *bo = vmf->vma->vm_private_data;
 	struct drm_device *ddev = bo->base.dev;
+	struct amdgpu_device *adev = drm_to_adev(ddev);
+	unsigned int prefetch_pages = TTM_BO_VM_NUM_PREFAULT;
 	vm_fault_t ret;
 	int idx;
 
 	ret = ttm_bo_vm_reserve(bo, vmf);
-	if (ret)
+	if (unlikely(ret))
 		return ret;
 
 	if (drm_dev_enter(ddev, &idx)) {
 		ret = amdgpu_bo_fault_reserve_notify(bo);
-		if (ret) {
+		if (unlikely(ret)) {
 			drm_dev_exit(idx);
 			goto unlock;
 		}
 
+		/* Vega-only adaptive prefetch. */
+		if (likely(adev->asic_type == CHIP_VEGA10 ||
+			     adev->asic_type == CHIP_VEGA12 ||
+			     adev->asic_type == CHIP_VEGA20)) {
+			struct amdgpu_bo *abo = container_of(bo, struct amdgpu_bo, tbo);
+
+			prefetch_pages = amdgpu_vega_optimal_prefetch(adev, abo, vmf, prefetch_pages);
+		}
+
 		ret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot,
-					       TTM_BO_VM_NUM_PREFAULT);
+					       prefetch_pages);
 
 		drm_dev_exit(idx);
 	} else {
 		ret = ttm_bo_vm_dummy_page(vmf, vmf->vma->vm_page_prot);
 	}
+
 	if (ret == VM_FAULT_RETRY && !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT))
 		return ret;
 
@@ -670,7 +990,7 @@ unsigned long amdgpu_gem_timeout(uint64_
 }
 
 int amdgpu_gem_wait_idle_ioctl(struct drm_device *dev, void *data,
-			      struct drm_file *filp)
+			       struct drm_file *filp)
 {
 	union drm_amdgpu_gem_wait_idle *args = data;
 	struct drm_gem_object *gobj;
@@ -685,18 +1005,30 @@ int amdgpu_gem_wait_idle_ioctl(struct dr
 		return -ENOENT;
 
 	robj = gem_to_amdgpu_bo(gobj);
+
+	/* Fast path: if already idle, avoid wait queue machinery.
+	 * Clear out struct to prevent padding leak.
+	 */
+	if (dma_resv_test_signaled(robj->tbo.base.resv, DMA_RESV_USAGE_READ)) {
+		memset(&args->out, 0, sizeof(args->out));
+		drm_gem_object_put(gobj);
+		return 0;
+	}
+
 	ret = dma_resv_wait_timeout(robj->tbo.base.resv, DMA_RESV_USAGE_READ,
 				    true, timeout);
 
-	/* ret == 0 means not signaled,
-	 * ret > 0 means signaled
-	 * ret < 0 means interrupted before timeout
+	/* ret == 0 means timeout (not signaled, busy),
+	 * ret > 0 means signaled (idle, returns remaining jiffies),
+	 * ret < 0 means interrupted or error.
 	 */
-	if (ret >= 0) {
-		memset(args, 0, sizeof(*args));
-		args->out.status = (ret == 0);
-	} else
-		r = ret;
+	if (likely(ret >= 0)) {
+		memset(&args->out, 0, sizeof(args->out));
+		args->out.status = (ret == 0) ? 1u : 0u; /* 1 = busy, 0 = idle */
+		r = 0;
+	} else {
+		r = (int)ret;
+	}
 
 	drm_gem_object_put(gobj);
 	return r;
@@ -746,7 +1078,7 @@ out:
 }
 
 /**
- * amdgpu_gem_va_update_vm -update the bo_va in its VM
+ * amdgpu_gem_va_update_vm - update the bo_va in its VM
  *
  * @adev: amdgpu_device pointer
  * @vm: vm to update
@@ -757,7 +1089,8 @@ out:
  * vital here, so they are not reported back to userspace.
  *
  * Returns resulting fence if freed BO(s) got cleared from the PT.
- * otherwise stub fence in case of error.
+ * Returns NULL if no real fence was produced (common success case).
+ * Returns a stub fence in case of error (for safety).
  */
 static struct dma_fence *
 amdgpu_gem_va_update_vm(struct amdgpu_device *adev,
@@ -765,11 +1098,11 @@ amdgpu_gem_va_update_vm(struct amdgpu_de
 			struct amdgpu_bo_va *bo_va,
 			uint32_t operation)
 {
-	struct dma_fence *fence = dma_fence_get_stub();
+	struct dma_fence *fence = NULL;
 	int r;
 
 	if (!amdgpu_vm_ready(vm))
-		return fence;
+		return NULL;
 
 	r = amdgpu_vm_clear_freed(adev, vm, &fence);
 	if (r)
@@ -784,11 +1117,18 @@ amdgpu_gem_va_update_vm(struct amdgpu_de
 
 	r = amdgpu_vm_update_pdes(adev, vm, false);
 
+	if (r)
+		goto error;
+
+	return fence;
+
 error:
 	if (r && r != -ERESTARTSYS)
 		DRM_ERROR("Couldn't update BO_VA (%d)\n", r);
 
-	return fence;
+	if (fence)
+		dma_fence_put(fence);
+	return dma_fence_get_stub();
 }
 
 /**
@@ -822,7 +1162,7 @@ uint64_t amdgpu_gem_va_map_flags(struct
 }
 
 int amdgpu_gem_va_ioctl(struct drm_device *dev, void *data,
-			  struct drm_file *filp)
+			struct drm_file *filp)
 {
 	const uint32_t valid_flags = AMDGPU_VM_DELAY_UPDATE |
 		AMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE |
@@ -845,39 +1185,30 @@ int amdgpu_gem_va_ioctl(struct drm_devic
 	uint64_t vm_size;
 	int r = 0;
 
-	if (args->va_address < AMDGPU_VA_RESERVED_BOTTOM) {
-		dev_dbg(dev->dev,
-			"va_address 0x%llx is in reserved area 0x%llx\n",
-			args->va_address, AMDGPU_VA_RESERVED_BOTTOM);
-		return -EINVAL;
-	}
-
-	if (args->va_address >= AMDGPU_GMC_HOLE_START &&
-	    args->va_address < AMDGPU_GMC_HOLE_END) {
-		dev_dbg(dev->dev,
-			"va_address 0x%llx is in VA hole 0x%llx-0x%llx\n",
-			args->va_address, AMDGPU_GMC_HOLE_START,
-			AMDGPU_GMC_HOLE_END);
+	/* Address validation: combine reserved-bottom and hole checks to reduce branches.
+	 * Keep masking and overflow check separate to maintain correct semantics.
+	 */
+	if (unlikely(args->va_address < AMDGPU_VA_RESERVED_BOTTOM ||
+		     (args->va_address >= AMDGPU_GMC_HOLE_START &&
+		      args->va_address < AMDGPU_GMC_HOLE_END))) {
 		return -EINVAL;
 	}
 
 	args->va_address &= AMDGPU_GMC_HOLE_MASK;
 
+	/* Now validate overflow with masked address */
 	vm_size = adev->vm_manager.max_pfn * AMDGPU_GPU_PAGE_SIZE;
 	vm_size -= AMDGPU_VA_RESERVED_TOP;
-	if (args->va_address + args->map_size > vm_size) {
-		dev_dbg(dev->dev,
-			"va_address 0x%llx is in top reserved area 0x%llx\n",
-			args->va_address + args->map_size, vm_size);
+	if (unlikely(args->map_size > vm_size - args->va_address)) {
 		return -EINVAL;
 	}
 
-	if ((args->flags & ~valid_flags) && (args->flags & ~prt_flags)) {
-		dev_dbg(dev->dev, "invalid flags combination 0x%08X\n",
-			args->flags);
+	/* Flag validation: accept if matches either valid_flags or prt_flags */
+	if (unlikely((args->flags & ~valid_flags) && (args->flags & ~prt_flags))) {
 		return -EINVAL;
 	}
 
+	/* Operation validation */
 	switch (args->operation) {
 	case AMDGPU_VA_OP_MAP:
 	case AMDGPU_VA_OP_UNMAP:
@@ -885,8 +1216,6 @@ int amdgpu_gem_va_ioctl(struct drm_devic
 	case AMDGPU_VA_OP_REPLACE:
 		break;
 	default:
-		dev_dbg(dev->dev, "unsupported operation %d\n",
-			args->operation);
 		return -EINVAL;
 	}
 
@@ -953,7 +1282,6 @@ int amdgpu_gem_va_ioctl(struct drm_devic
 	case AMDGPU_VA_OP_UNMAP:
 		r = amdgpu_vm_bo_unmap(adev, bo_va, args->va_address);
 		break;
-
 	case AMDGPU_VA_OP_CLEAR:
 		r = amdgpu_vm_bo_clear_mappings(adev, &fpriv->vm,
 						args->va_address,
@@ -968,19 +1296,22 @@ int amdgpu_gem_va_ioctl(struct drm_devic
 	default:
 		break;
 	}
-	if (!r && !(args->flags & AMDGPU_VM_DELAY_UPDATE) && !adev->debug_vm) {
+
+	if (!r && !(args->flags & AMDGPU_VM_DELAY_UPDATE) && likely(!adev->debug_vm)) {
 		fence = amdgpu_gem_va_update_vm(adev, &fpriv->vm, bo_va,
 						args->operation);
 
-		if (timeline_syncobj)
+		if (timeline_syncobj) {
+			if (!fence)
+				fence = dma_fence_get_stub();
 			amdgpu_gem_update_bo_mapping(filp, bo_va,
-					     args->operation,
-					     args->vm_timeline_point,
-					     fence, timeline_syncobj,
-					     timeline_chain);
-		else
+						     args->operation,
+						     args->vm_timeline_point,
+						     fence, timeline_syncobj,
+						     timeline_chain);
+		} else {
 			dma_fence_put(fence);
-
+		}
 	}
 
 error:
@@ -1071,6 +1402,9 @@ static int amdgpu_gem_align_pitch(struct
 				  int cpp,
 				  bool tiled)
 {
+	(void)adev;
+	(void)tiled;
+
 	int aligned = width;
 	int pitch_mask = 0;
 
@@ -1085,6 +1419,8 @@ static int amdgpu_gem_align_pitch(struct
 	case 4:
 		pitch_mask = 63;
 		break;
+	default:
+		break;
 	}
 
 	aligned += pitch_mask;
@@ -1104,6 +1440,7 @@ int amdgpu_mode_dumb_create(struct drm_f
 		    AMDGPU_GEM_CREATE_CPU_GTT_USWC |
 		    AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;
 	u32 domain;
+	u64 size;
 	int r;
 
 	/*
@@ -1114,13 +1451,21 @@ int amdgpu_mode_dumb_create(struct drm_f
 	if (adev->mman.buffer_funcs_enabled)
 		flags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;
 
-	args->pitch = amdgpu_gem_align_pitch(adev, args->width,
-					     DIV_ROUND_UP(args->bpp, 8), 0);
-	args->size = (u64)args->pitch * args->height;
-	args->size = ALIGN(args->size, PAGE_SIZE);
+	r = amdgpu_gem_align_pitch(adev, args->width,
+				     DIV_ROUND_UP(args->bpp, 8), 0);
+	if (r < 0)
+		return r;
+	args->pitch = r;
+
+	/* Overflow-safe size computation */
+	if (check_mul_overflow((u64)args->pitch, (u64)args->height, &size))
+		return -EINVAL;
+
+	size = ALIGN(size, PAGE_SIZE);
+
 	domain = amdgpu_bo_get_preferred_domain(adev,
 				amdgpu_display_supported_domains(adev, flags));
-	r = amdgpu_gem_object_create(adev, args->size, 0, domain, flags,
+	r = amdgpu_gem_object_create(adev, size, 0, domain, flags,
 				     ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
 	if (r)
 		return -ENOMEM;
@@ -1132,6 +1477,7 @@ int amdgpu_mode_dumb_create(struct drm_f
 		return r;
 
 	args->handle = handle;
+	args->size = size;
 	return 0;
 }
 

--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.h	2025-05-29 11:14:09.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_object.h	2025-06-02 02:29:50.388339649 +0200


--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c	2025-05-29 11:14:09.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c	2025-06-02 02:29:50.388339649 +0200

 
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c	2025-05-29 11:14:09.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c	2025-06-02 02:29:50.388339649 +0200



--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.h	2025-05-29 11:14:09.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.h	2025-06-02 02:40:45.281658476 +0200


--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_irq.h	2025-04-25 10:51:21.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_irq.h	2025-04-26 19:30:33.996606337 +0200


--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_irq.c	2025-04-25 10:51:21.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_irq.c	2025-08-17 19:35:40.185257128 +0200


--- a/drivers/gpu/drm/amd/amdgpu/sdma_v4_0.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/sdma_v4_0.c	2025-04-19 23:06:54.825287367 +0200
@@ -643,90 +643,65 @@ static int sdma_v4_0_init_microcode(stru
 	return ret;
 }
 
-/**
- * sdma_v4_0_ring_get_rptr - get the current read pointer
- *
- * @ring: amdgpu ring pointer
- *
- * Get the current rptr from the hardware (VEGA10+).
- */
+/* ------------------------------------------------------------------ */
+/* read pointer helper                                                */
+/* ------------------------------------------------------------------ */
 static uint64_t sdma_v4_0_ring_get_rptr(struct amdgpu_ring *ring)
 {
-	u64 *rptr;
+	u64 *rptr = (u64 *)ring->rptr_cpu_addr;	/* little‑endian on Vega */
 
-	/* XXX check if swapping is necessary on BE */
-	rptr = ((u64 *)ring->rptr_cpu_addr);
+	if (drm_debug_enabled(DRM_UT_DRIVER))
+		DRM_DEBUG("SDMA%u rptr raw 0x%016llx\n", ring->me, *rptr);
 
-	DRM_DEBUG("rptr before shift == 0x%016llx\n", *rptr);
-	return ((*rptr) >> 2);
+	return *rptr >> 2;			/* convert to DWORD index */
 }
 
-/**
- * sdma_v4_0_ring_get_wptr - get the current write pointer
- *
- * @ring: amdgpu ring pointer
- *
- * Get the current wptr from the hardware (VEGA10+).
- */
+/* ------------------------------------------------------------------ */
+/* write pointer read helper                                          */
+/* ------------------------------------------------------------------ */
 static uint64_t sdma_v4_0_ring_get_wptr(struct amdgpu_ring *ring)
 {
 	struct amdgpu_device *adev = ring->adev;
 	u64 wptr;
 
 	if (ring->use_doorbell) {
-		/* XXX check if swapping is necessary on BE */
 		wptr = READ_ONCE(*((u64 *)ring->wptr_cpu_addr));
-		DRM_DEBUG("wptr/doorbell before shift == 0x%016llx\n", wptr);
+		if (drm_debug_enabled(DRM_UT_DRIVER))
+			DRM_DEBUG("SDMA%u wptr doorbell raw 0x%016llx\n",
+					  ring->me, wptr);
 	} else {
-		wptr = RREG32_SDMA(ring->me, mmSDMA0_GFX_RB_WPTR_HI);
-		wptr = wptr << 32;
-		wptr |= RREG32_SDMA(ring->me, mmSDMA0_GFX_RB_WPTR);
-		DRM_DEBUG("wptr before shift [%i] wptr == 0x%016llx\n",
-				ring->me, wptr);
+		wptr  = (u64)RREG32_SDMA(ring->me,
+								 mmSDMA0_GFX_RB_WPTR_HI) << 32;
+								 wptr |= RREG32_SDMA(ring->me, mmSDMA0_GFX_RB_WPTR);
+								 if (drm_debug_enabled(DRM_UT_DRIVER))
+									 DRM_DEBUG("SDMA%u wptr mmio raw 0x%016llx\n",
+											   ring->me, wptr);
 	}
 
-	return wptr >> 2;
+	return wptr >> 2;			/* to DWORDs */
 }
 
-/**
- * sdma_v4_0_ring_set_wptr - commit the write pointer
- *
- * @ring: amdgpu ring pointer
- *
- * Write the wptr back to the hardware (VEGA10+).
- */
+/* ------------------------------------------------------------------ */
+/* write pointer commit helper                                        */
+/* ------------------------------------------------------------------ */
 static void sdma_v4_0_ring_set_wptr(struct amdgpu_ring *ring)
 {
 	struct amdgpu_device *adev = ring->adev;
+	u64 wptr_dw = ring->wptr;		/* already DWORD aligned */
+
+	if (drm_debug_enabled(DRM_UT_DRIVER))
+		DRM_DEBUG("SDMA%u set wptr %llu (DW)\n", ring->me, wptr_dw);
 
-	DRM_DEBUG("Setting write pointer\n");
 	if (ring->use_doorbell) {
 		u64 *wb = (u64 *)ring->wptr_cpu_addr;
 
-		DRM_DEBUG("Using doorbell -- "
-				"wptr_offs == 0x%08x "
-				"lower_32_bits(ring->wptr << 2) == 0x%08x "
-				"upper_32_bits(ring->wptr << 2) == 0x%08x\n",
-				ring->wptr_offs,
-				lower_32_bits(ring->wptr << 2),
-				upper_32_bits(ring->wptr << 2));
-		/* XXX check if swapping is necessary on BE */
-		WRITE_ONCE(*wb, (ring->wptr << 2));
-		DRM_DEBUG("calling WDOORBELL64(0x%08x, 0x%016llx)\n",
-				ring->doorbell_index, ring->wptr << 2);
-		WDOORBELL64(ring->doorbell_index, ring->wptr << 2);
+		WRITE_ONCE(*wb, wptr_dw << 2);	/* bytes */
+		WDOORBELL64(ring->doorbell_index, wptr_dw << 2);
 	} else {
-		DRM_DEBUG("Not using doorbell -- "
-				"mmSDMA%i_GFX_RB_WPTR == 0x%08x "
-				"mmSDMA%i_GFX_RB_WPTR_HI == 0x%08x\n",
-				ring->me,
-				lower_32_bits(ring->wptr << 2),
-				ring->me,
-				upper_32_bits(ring->wptr << 2));
 		WREG32_SDMA(ring->me, mmSDMA0_GFX_RB_WPTR,
-			    lower_32_bits(ring->wptr << 2));
+					lower_32_bits(wptr_dw << 2));
 		WREG32_SDMA(ring->me, mmSDMA0_GFX_RB_WPTR_HI,
-			    upper_32_bits(ring->wptr << 2));
+					upper_32_bits(wptr_dw << 2));
 	}
 }
 
@@ -781,17 +756,20 @@ static void sdma_v4_0_page_ring_set_wptr
 	}
 }
 
-static void sdma_v4_0_ring_insert_nop(struct amdgpu_ring *ring, uint32_t count)
+static void sdma_v4_0_ring_insert_nop(struct amdgpu_ring *ring,
+									  uint32_t count)
 {
-	struct amdgpu_sdma_instance *sdma = amdgpu_sdma_get_instance_from_ring(ring);
-	int i;
+	struct amdgpu_sdma_instance *sdma =
+	amdgpu_sdma_get_instance_from_ring(ring);
+	uint32_t i;
 
-	for (i = 0; i < count; i++)
-		if (sdma && sdma->burst_nop && (i == 0))
+	for (i = 0; i < count; i++) {
+		if (sdma && sdma->burst_nop && i == 0)
 			amdgpu_ring_write(ring, ring->funcs->nop |
-				SDMA_PKT_NOP_HEADER_COUNT(count - 1));
+			SDMA_PKT_NOP_HEADER_COUNT(count - 1));
 		else
 			amdgpu_ring_write(ring, ring->funcs->nop);
+	}
 }
 
 /**
@@ -1659,30 +1637,26 @@ static void sdma_v4_0_vm_set_pte_pde(str
 	ib->ptr[ib->length_dw++] = count - 1; /* number of entries */
 }
 
-/**
- * sdma_v4_0_ring_pad_ib - pad the IB to the required number of dw
- *
- * @ring: amdgpu_ring structure holding ring information
- * @ib: indirect buffer to fill with padding
- */
-static void sdma_v4_0_ring_pad_ib(struct amdgpu_ring *ring, struct amdgpu_ib *ib)
+static void sdma_v4_0_ring_pad_ib(struct amdgpu_ring *ring,
+								  struct amdgpu_ib *ib)
 {
-	struct amdgpu_sdma_instance *sdma = amdgpu_sdma_get_instance_from_ring(ring);
-	u32 pad_count;
-	int i;
+	struct amdgpu_sdma_instance *sdma =
+	amdgpu_sdma_get_instance_from_ring(ring);
+	u32 pad_count, i;
+
+	pad_count = (-ib->length_dw) & 7;	/* align to 8 DW */
 
-	pad_count = (-ib->length_dw) & 7;
-	for (i = 0; i < pad_count; i++)
-		if (sdma && sdma->burst_nop && (i == 0))
+	for (i = 0; i < pad_count; i++) {
+		if (sdma && sdma->burst_nop && i == 0)
 			ib->ptr[ib->length_dw++] =
-				SDMA_PKT_HEADER_OP(SDMA_OP_NOP) |
-				SDMA_PKT_NOP_HEADER_COUNT(pad_count - 1);
+			SDMA_PKT_HEADER_OP(SDMA_OP_NOP) |
+			SDMA_PKT_NOP_HEADER_COUNT(pad_count - 1);
 		else
 			ib->ptr[ib->length_dw++] =
-				SDMA_PKT_HEADER_OP(SDMA_OP_NOP);
+			SDMA_PKT_HEADER_OP(SDMA_OP_NOP);
+	}
 }
 
-
 /**
  * sdma_v4_0_ring_emit_pipeline_sync - sync the pipeline
  *
@@ -2599,12 +2573,12 @@ static void sdma_v4_0_emit_fill_buffer(s
 }
 
 static const struct amdgpu_buffer_funcs sdma_v4_0_buffer_funcs = {
-	.copy_max_bytes = 0x400000,
-	.copy_num_dw = 7,
+	.copy_max_bytes = 0x400000,		/* 4 MiB */
+	.copy_num_dw    = 7,
 	.emit_copy_buffer = sdma_v4_0_emit_copy_buffer,
 
-	.fill_max_bytes = 0x400000,
-	.fill_num_dw = 5,
+	.fill_max_bytes = 0x400000,		/* 4 MiB */
+	.fill_num_dw    = 5,
 	.emit_fill_buffer = sdma_v4_0_emit_fill_buffer,
 };
 

 
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_sdma.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_sdma.c	2025-04-19 22:43:23.904999601 +0200
@@ -30,37 +30,54 @@
 /* SDMA CSA reside in the 3rd page of CSA */
 #define AMDGPU_CSA_SDMA_OFFSET (4096 * 2)
 
-/*
- * GPU SDMA IP block helpers function.
- */
-
-struct amdgpu_sdma_instance *amdgpu_sdma_get_instance_from_ring(struct amdgpu_ring *ring)
+/* ------------------------------------------------------------------ */
+/* Fast helpers: use ring->idx instead of a linear scan                */
+/* ------------------------------------------------------------------ */
+struct amdgpu_sdma_instance *
+amdgpu_sdma_get_instance_from_ring(struct amdgpu_ring *ring)
 {
 	struct amdgpu_device *adev = ring->adev;
-	int i;
+	u32 idx = ring->idx;
 
-	for (i = 0; i < adev->sdma.num_instances; i++)
-		if (ring == &adev->sdma.instance[i].ring ||
-		    ring == &adev->sdma.instance[i].page)
-			return &adev->sdma.instance[i];
+	/* O(1) fast path */
+	if (idx < adev->sdma.num_instances &&
+		(ring == &adev->sdma.instance[idx].ring ||
+		ring == &adev->sdma.instance[idx].page))
+		return &adev->sdma.instance[idx];
+
+	/* Fallback – keep legacy behaviour */
+	for (idx = 0; idx < adev->sdma.num_instances; idx++) {
+		if (ring == &adev->sdma.instance[idx].ring ||
+			ring == &adev->sdma.instance[idx].page)
+			return &adev->sdma.instance[idx];
+	}
 
 	return NULL;
 }
 
-int amdgpu_sdma_get_index_from_ring(struct amdgpu_ring *ring, uint32_t *index)
+int amdgpu_sdma_get_index_from_ring(struct amdgpu_ring *ring, u32 *index)
 {
 	struct amdgpu_device *adev = ring->adev;
-	int i;
+	u32 idx = ring->idx;
 
-	for (i = 0; i < adev->sdma.num_instances; i++) {
-		if (ring == &adev->sdma.instance[i].ring ||
-			ring == &adev->sdma.instance[i].page) {
-			*index = i;
+	/* Fast path */
+	if (idx < adev->sdma.num_instances &&
+		(ring == &adev->sdma.instance[idx].ring ||
+		ring == &adev->sdma.instance[idx].page)) {
+		*index = idx;
+	return 0;
+		}
+
+		/* Fallback keeps behaviour identical to the old code */
+		for (idx = 0; idx < adev->sdma.num_instances; idx++) {
+			if (ring == &adev->sdma.instance[idx].ring ||
+				ring == &adev->sdma.instance[idx].page) {
+				*index = idx;
 			return 0;
+				}
 		}
-	}
 
-	return -EINVAL;
+		return -EINVAL;
 }
 
 uint64_t amdgpu_sdma_get_csa_mc_addr(struct amdgpu_ring *ring,


--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gfx.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gfx.c	2025-04-18 16:58:52.885186023 +0200

--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c	2025-08-15 02:09:44.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c	2025-08-30 10:35:21.586874338 +0200
@@ -79,6 +79,25 @@ static int amdgpu_ttm_init_on_chip(struc
 				  false, size_in_page);
 }
 
+static __always_inline void gtt_window_lock_fast(struct amdgpu_device *adev)
+{
+	if (likely(mutex_trylock(&adev->mman.gtt_window_lock)))
+		return;
+	mutex_lock(&adev->mman.gtt_window_lock);
+}
+
+static __always_inline u64 amdgpu_ttm_chunk_bytes(struct amdgpu_device *adev)
+{
+	switch (adev->asic_type) {
+	case CHIP_VEGA10:
+	case CHIP_VEGA12:
+	case CHIP_VEGA20:
+		return 512ULL << 20; /* 512 MiB */
+	default:
+		return 256ULL << 20; /* 256 MiB */
+	}
+}
+
 /**
  * amdgpu_evict_flags - Compute placement flags
  *
@@ -161,7 +180,7 @@ static void amdgpu_evict_flags(struct tt
 	*placement = abo->placement;
 }
 
-/**
+/*
  * amdgpu_ttm_map_buffer - Map memory into the GART windows
  * @bo: buffer object to map
  * @mem: memory object to map
@@ -174,6 +193,11 @@ static void amdgpu_evict_flags(struct tt
  *
  * Setup one of the GART windows to access a specific piece of memory or return
  * the physical address for local memory.
+ *
+ * Vega-safe and upstream-compatible improvements:
+ *  - Upload exactly the PTE bytes (8B per GPU page) in copy_max_bytes chunks.
+ *  - Align the inline PTE payload in the IB to 8 bytes.
+ *  - Avoids SDMA packet overruns and preserves ordering (high_pr entity).
  */
 static int amdgpu_ttm_map_buffer(struct ttm_buffer_object *bo,
 				 struct ttm_resource *mem,
@@ -182,89 +206,114 @@ static int amdgpu_ttm_map_buffer(struct
 				 bool tmz, uint64_t *size, uint64_t *addr)
 {
 	struct amdgpu_device *adev = ring->adev;
-	unsigned int offset, num_pages, num_dw, num_bytes;
-	uint64_t src_addr, dst_addr;
-	struct amdgpu_job *job;
+	unsigned int offset, num_pages;
+	u64 src_addr, dst_addr;
+	u64 pte_bytes, copied;
+	u32 max_bytes;
+	u32 loops, num_dw;         /* command DWs */
+	u32 ib_cmd_bytes;          /* bytes used by commands */
+	u32 pte_off;               /* byte offset of inline PTE payload */
+	u32 total_ib_bytes;        /* total IB size: commands + PTE payload */
+	struct amdgpu_job *job = NULL;
 	void *cpu_addr;
-	uint64_t flags;
+	u64 flags;
 	unsigned int i;
 	int r;
 
-	BUG_ON(adev->mman.buffer_funcs->copy_max_bytes <
-	       AMDGPU_GTT_MAX_TRANSFER_SIZE * 8);
+	if (WARN_ON(!bo || !mem || !mm_cur || !size || !addr || !ring))
+		return -EINVAL;
 
 	if (WARN_ON(mem->mem_type == AMDGPU_PL_PREEMPT))
 		return -EINVAL;
 
-	/* Map only what can't be accessed directly */
+	/* Direct VRAM access path if not TMZ */
 	if (!tmz && mem->start != AMDGPU_BO_INVALID_OFFSET) {
-		*addr = amdgpu_ttm_domain_start(adev, mem->mem_type) +
-			mm_cur->start;
+		*addr = amdgpu_ttm_domain_start(adev, mem->mem_type) + mm_cur->start;
 		return 0;
 	}
 
-
-	/*
-	 * If start begins at an offset inside the page, then adjust the size
-	 * and addr accordingly
-	 */
 	offset = mm_cur->start & ~PAGE_MASK;
+	if (*size > (U64_MAX - offset))
+		return -E2BIG;
 
 	num_pages = PFN_UP(*size + offset);
-	num_pages = min_t(uint32_t, num_pages, AMDGPU_GTT_MAX_TRANSFER_SIZE);
+	num_pages = min_t(u32, num_pages, AMDGPU_GTT_MAX_TRANSFER_SIZE);
 
-	*size = min(*size, (uint64_t)num_pages * PAGE_SIZE - offset);
+	*size = min_t(u64, *size, (u64)num_pages * PAGE_SIZE - offset);
+	if (unlikely(*size == 0))
+		return -EINVAL;
 
-	*addr = adev->gmc.gart_start;
-	*addr += (u64)window * AMDGPU_GTT_MAX_TRANSFER_SIZE *
-		AMDGPU_GPU_PAGE_SIZE;
-	*addr += offset;
+	*addr = adev->gmc.gart_start +
+		(u64)window * (u64)AMDGPU_GTT_MAX_TRANSFER_SIZE *
+		(u64)AMDGPU_GPU_PAGE_SIZE + (u64)offset;
 
-	num_dw = ALIGN(adev->mman.buffer_funcs->copy_num_dw, 8);
-	num_bytes = num_pages * 8 * AMDGPU_GPU_PAGES_IN_CPU_PAGE;
+	max_bytes = adev->mman.buffer_funcs->copy_max_bytes;
+	pte_bytes = (u64)num_pages * 8ULL * AMDGPU_GPU_PAGES_IN_CPU_PAGE;
+	loops = DIV_ROUND_UP_ULL(pte_bytes, max_bytes);
+	num_dw = ALIGN(loops * adev->mman.buffer_funcs->copy_num_dw, 8);
+	ib_cmd_bytes = num_dw * 4;
+	pte_off = ALIGN(ib_cmd_bytes, 8);
+
+	if (pte_bytes > U32_MAX || pte_off > U32_MAX ||
+	    (u64)pte_off + pte_bytes > U32_MAX)
+		return -E2BIG;
+
+	total_ib_bytes = pte_off + (u32)pte_bytes;
 
 	r = amdgpu_job_alloc_with_ib(adev, &adev->mman.high_pr,
 				     AMDGPU_FENCE_OWNER_UNDEFINED,
-				     num_dw * 4 + num_bytes,
+				     total_ib_bytes,
 				     AMDGPU_IB_POOL_DELAYED, &job);
 	if (r)
 		return r;
 
-	src_addr = num_dw * 4;
-	src_addr += job->ibs[0].gpu_addr;
-
-	dst_addr = amdgpu_bo_gpu_offset(adev->gart.bo);
-	dst_addr += window * AMDGPU_GTT_MAX_TRANSFER_SIZE * 8;
-	amdgpu_emit_copy_buffer(adev, &job->ibs[0], src_addr,
-				dst_addr, num_bytes, 0);
-
-	amdgpu_ring_pad_ib(ring, &job->ibs[0]);
-	WARN_ON(job->ibs[0].length_dw > num_dw);
+	dst_addr = amdgpu_bo_gpu_offset(adev->gart.bo) +
+		   (u64)window * (u64)AMDGPU_GTT_MAX_TRANSFER_SIZE * 8ULL;
+	src_addr = job->ibs[0].gpu_addr + pte_off;
 
 	flags = amdgpu_ttm_tt_pte_flags(adev, bo->ttm, mem);
 	if (tmz)
 		flags |= AMDGPU_PTE_TMZ;
 
-	cpu_addr = &job->ibs[0].ptr[num_dw];
+	cpu_addr = (void *)((u8 *)job->ibs[0].ptr + pte_off);
 
 	if (mem->mem_type == TTM_PL_TT) {
 		dma_addr_t *dma_addr;
 
+		if (!bo->ttm || !bo->ttm->dma_address) {
+			amdgpu_job_free(job);
+			return -EINVAL;
+		}
+
 		dma_addr = &bo->ttm->dma_address[mm_cur->start >> PAGE_SHIFT];
 		amdgpu_gart_map(adev, 0, num_pages, dma_addr, flags, cpu_addr);
-	} else {
-		dma_addr_t dma_address;
-
-		dma_address = mm_cur->start;
-		dma_address += adev->vm_manager.vram_base_offset;
+	} else if (mem->mem_type == TTM_PL_VRAM) {
+		dma_addr_t dma_address = mm_cur->start + adev->vm_manager.vram_base_offset;
 
 		for (i = 0; i < num_pages; ++i) {
-			amdgpu_gart_map(adev, i << PAGE_SHIFT, 1, &dma_address,
-					flags, cpu_addr);
+			amdgpu_gart_map(adev, (u64)i << PAGE_SHIFT, 1,
+					&dma_address, flags, cpu_addr);
 			dma_address += PAGE_SIZE;
 		}
+	} else {
+		amdgpu_job_free(job);
+		return -EINVAL;
+	}
+
+	copied = 0;
+	while (copied < pte_bytes) {
+		u32 cur = min_t(u64, pte_bytes - copied, max_bytes);
+
+		amdgpu_emit_copy_buffer(adev, &job->ibs[0],
+					src_addr + copied,
+					dst_addr + copied,
+					cur, 0);
+		copied += cur;
 	}
 
+	amdgpu_ring_pad_ib(ring, &job->ibs[0]);
+	WARN_ON(job->ibs[0].length_dw > num_dw);
+
 	dma_fence_put(amdgpu_job_submit(job));
 	return 0;
 }
@@ -283,6 +332,8 @@ static int amdgpu_ttm_map_buffer(struct
  * {dst->mem + dst->offset}. src->bo and dst->bo could be same BO for a
  * move and different for a BO to BO copy.
  *
+ * Optimization (Idea #4):
+ *  - Use gtt_window_lock_fast() to reduce uncontended lock overhead.
  */
 int amdgpu_ttm_copy_mem_to_mem(struct amdgpu_device *adev,
 			       const struct amdgpu_copy_mem *src,
@@ -306,7 +357,7 @@ int amdgpu_ttm_copy_mem_to_mem(struct am
 	amdgpu_res_first(src->mem, src->offset, size, &src_mm);
 	amdgpu_res_first(dst->mem, dst->offset, size, &dst_mm);
 
-	mutex_lock(&adev->mman.gtt_window_lock);
+	gtt_window_lock_fast(adev);
 	while (src_mm.remaining) {
 		uint64_t from, to, cur_size, tiling_flags;
 		uint32_t num_type, data_format, max_com, write_compress_disable;
@@ -349,7 +400,7 @@ int amdgpu_ttm_copy_mem_to_mem(struct am
 							     write_compress_disable));
 		}
 
-		r = amdgpu_copy_buffer(ring, from, to, cur_size, resv,
+		r = amdgpu_copy_buffer(ring, from, to, (u32)cur_size, resv,
 				       &next, false, true, copy_flags);
 		if (r)
 			goto error;
@@ -373,6 +424,7 @@ error:
  *
  * This is a helper called by amdgpu_bo_move() and amdgpu_move_vram_ram() to
  * help move buffers to and from VRAM.
+ *
  */
 static int amdgpu_move_blit(struct ttm_buffer_object *bo,
 			    bool evict,
@@ -380,52 +432,63 @@ static int amdgpu_move_blit(struct ttm_b
 			    struct ttm_resource *old_mem)
 {
 	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->bdev);
-	struct amdgpu_bo *abo = ttm_to_amdgpu_bo(bo);
-	struct amdgpu_copy_mem src, dst;
+	struct amdgpu_ring   *ring = adev->mman.buffer_funcs_ring;
+	struct amdgpu_res_cursor src_mm, dst_mm;
 	struct dma_fence *fence = NULL;
-	int r;
+	u32 copy_flags = 0;
+	int r = 0;
 
-	src.bo = bo;
-	dst.bo = bo;
-	src.mem = old_mem;
-	dst.mem = new_mem;
-	src.offset = 0;
-	dst.offset = 0;
-
-	r = amdgpu_ttm_copy_mem_to_mem(adev, &src, &dst,
-				       new_mem->size,
-				       amdgpu_bo_encrypted(abo),
-				       bo->base.resv, &fence);
-	if (r)
-		goto error;
+	if (!adev->mman.buffer_funcs_enabled || !ring || !ring->sched.ready)
+		return -EINVAL;
 
-	/* clear the space being freed */
-	if (old_mem->mem_type == TTM_PL_VRAM &&
-	    (abo->flags & AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE)) {
-		struct dma_fence *wipe_fence = NULL;
+	if (amdgpu_bo_encrypted(ttm_to_amdgpu_bo(bo)))
+		copy_flags |= AMDGPU_COPY_FLAGS_TMZ;
 
-		r = amdgpu_fill_buffer(abo, 0, NULL, &wipe_fence,
-				       false);
-		if (r) {
-			goto error;
-		} else if (wipe_fence) {
-			amdgpu_vram_mgr_set_cleared(bo->resource);
-			dma_fence_put(fence);
-			fence = wipe_fence;
-		}
+	amdgpu_res_first(old_mem, 0, bo->base.size, &src_mm);
+	amdgpu_res_first(new_mem, 0, bo->base.size, &dst_mm);
+
+	gtt_window_lock_fast(adev);
+
+	while (src_mm.remaining) {
+		u64 cur_size = min3(src_mm.size, dst_mm.size, 256ULL << 20);
+		u64 from, to;
+		struct dma_fence *next = NULL;
+
+		r = amdgpu_ttm_map_buffer(bo, old_mem, &src_mm, 0, ring,
+					  amdgpu_bo_encrypted(ttm_to_amdgpu_bo(bo)),
+					  &cur_size, &from);
+		if (r)
+			break;
+
+		r = amdgpu_ttm_map_buffer(bo, new_mem, &dst_mm, 1, ring,
+					  amdgpu_bo_encrypted(ttm_to_amdgpu_bo(bo)),
+					  &cur_size, &to);
+		if (r)
+			break;
+
+		/* Schedule the copy buffer */
+		r = amdgpu_copy_buffer(ring, from, to, (u32)cur_size,
+				       bo->base.resv, &next,
+				       false, true, copy_flags);
+		if (r)
+			break;
+
+		dma_fence_put(fence);
+		fence = next;
+
+		amdgpu_res_next(&src_mm, cur_size);
+		amdgpu_res_next(&dst_mm, cur_size);
 	}
 
-	/* Always block for VM page tables before committing the new location */
-	if (bo->type == ttm_bo_type_kernel)
-		r = ttm_bo_move_accel_cleanup(bo, fence, true, false, new_mem);
-	else
-		r = ttm_bo_move_accel_cleanup(bo, fence, evict, true, new_mem);
-	dma_fence_put(fence);
-	return r;
+	mutex_unlock(&adev->mman.gtt_window_lock);
+
+	if (!r) {
+		if (bo->type == ttm_bo_type_kernel)
+			r = ttm_bo_move_accel_cleanup(bo, fence, true, false, new_mem);
+		else
+			r = ttm_bo_move_accel_cleanup(bo, fence, evict, true, new_mem);
+	}
 
-error:
-	if (fence)
-		dma_fence_wait(fence, false);
 	dma_fence_put(fence);
 	return r;
 }
@@ -693,6 +756,10 @@ struct amdgpu_ttm_tt {
  *
  * Calling function must call amdgpu_ttm_tt_userptr_range_done() once and only
  * once afterwards to stop HMM tracking
+ *
+ * Improvement:
+ *  - Better diagnostics on -EPERM to guide userspace about long-term pinning
+ *    restrictions with writable mappings (COW/DAX/RO).
  */
 int amdgpu_ttm_tt_get_user_pages(struct amdgpu_bo *bo, struct page **pages,
 				 struct hmm_range **range)
@@ -724,7 +791,7 @@ int amdgpu_ttm_tt_get_user_pages(struct
 		goto out_unlock;
 	}
 	if (unlikely((gtt->userflags & AMDGPU_GEM_USERPTR_ANONONLY) &&
-		vma->vm_file)) {
+		     vma->vm_file)) {
 		r = -EPERM;
 		goto out_unlock;
 	}
@@ -734,8 +801,16 @@ int amdgpu_ttm_tt_get_user_pages(struct
 				       readonly, NULL, pages, range);
 out_unlock:
 	mmap_read_unlock(mm);
-	if (r)
-		pr_debug("failed %d to get user pages 0x%lx\n", r, start);
+	if (r) {
+		if (r == -EPERM) {
+			DRM_ERROR("init_user_pages: pin denied (-EPERM) at 0x%lx; "
+				  "READONLY=%d. For writable long-term pins, avoid MAP_PRIVATE/COW or DAX; "
+				  "for read-only, set AMDGPU_GEM_USERPTR_READONLY.\n",
+				  start, readonly);
+		} else {
+			DRM_DEBUG_DRIVER("init_user_pages: failed %d at 0x%lx\n", r, start);
+		}
+	}
 
 	mmput(mm);
 
@@ -2285,9 +2360,6 @@ static int amdgpu_ttm_fill_mem(struct am
  * @fence: dma_fence associated with the operation
  *
  * Clear the memory buffer resource.
- *
- * Returns:
- * 0 for success or a negative error code on failure.
  */
 int amdgpu_ttm_clear_buffer(struct amdgpu_bo *bo,
 			    struct dma_resv *resv,
@@ -2309,7 +2381,7 @@ int amdgpu_ttm_clear_buffer(struct amdgp
 
 	amdgpu_res_first(bo->tbo.resource, 0, amdgpu_bo_size(bo), &cursor);
 
-	mutex_lock(&adev->mman.gtt_window_lock);
+	gtt_window_lock_fast(adev);
 	while (cursor.remaining) {
 		struct dma_fence *next = NULL;
 		u64 size;
@@ -2327,7 +2399,7 @@ int amdgpu_ttm_clear_buffer(struct amdgp
 		if (r)
 			goto err;
 
-		r = amdgpu_ttm_fill_mem(ring, 0, addr, size, resv,
+		r = amdgpu_ttm_fill_mem(ring, 0, addr, (u32)size, resv,
 					&next, true, true);
 		if (r)
 			goto err;
@@ -2343,6 +2415,14 @@ err:
 	return r;
 }
 
+/**
+ * amdgpu_fill_buffer - fill BO with a pattern
+ * @bo: BO to fill
+ * @src_data: dword pattern
+ * @resv: reservation object
+ * @f: returns fence
+ * @delayed: requested low priority submission
+ */
 int amdgpu_fill_buffer(struct amdgpu_bo *bo,
 			uint32_t src_data,
 			struct dma_resv *resv,
@@ -2362,7 +2442,7 @@ int amdgpu_fill_buffer(struct amdgpu_bo
 
 	amdgpu_res_first(bo->tbo.resource, 0, amdgpu_bo_size(bo), &dst);
 
-	mutex_lock(&adev->mman.gtt_window_lock);
+	gtt_window_lock_fast(adev);
 	while (dst.remaining) {
 		struct dma_fence *next;
 		uint64_t cur_size, to;
@@ -2375,7 +2455,7 @@ int amdgpu_fill_buffer(struct amdgpu_bo
 		if (r)
 			goto error;
 
-		r = amdgpu_ttm_fill_mem(ring, src_data, to, cur_size, resv,
+		r = amdgpu_ttm_fill_mem(ring, src_data, to, (u32)cur_size, resv,
 					&next, true, delayed);
 		if (r)
 			goto error;

--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h	2025-04-12 17:27:40.094502930 +0200


--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm_pt.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm_pt.c	2025-04-12 16:51:37.138829348 +0200
@@ -22,6 +22,7 @@
  */
 
 #include <drm/drm_drv.h>
+#include <linux/prefetch.h>
 
 #include "amdgpu.h"
 #include "amdgpu_trace.h"
@@ -46,8 +47,8 @@ struct amdgpu_vm_pt_cursor {
  * Returns:
  * The number of bits the pfn needs to be right shifted for a level.
  */
-static unsigned int amdgpu_vm_pt_level_shift(struct amdgpu_device *adev,
-					     unsigned int level)
+static inline unsigned int amdgpu_vm_pt_level_shift(struct amdgpu_device *adev,
+						    unsigned int level)
 {
 	switch (level) {
 	case AMDGPU_VM_PDB2:
@@ -58,7 +59,7 @@ static unsigned int amdgpu_vm_pt_level_s
 	case AMDGPU_VM_PTB:
 		return 0;
 	default:
-		return ~0;
+		return ~0U;
 	}
 }
 
@@ -98,15 +99,45 @@ static unsigned int amdgpu_vm_pt_num_ent
  * Returns:
  * The mask to extract the entry number of a PD/PT from an address.
  */
-static uint32_t amdgpu_vm_pt_entries_mask(struct amdgpu_device *adev,
-					  unsigned int level)
+static inline uint32_t amdgpu_vm_pt_entries_mask(struct amdgpu_device *adev,
+						 unsigned int level)
 {
-	if (level <= adev->vm_manager.root_level)
-		return 0xffffffff;
-	else if (level != AMDGPU_VM_PTB)
-		return 0x1ff;
-	else
+	if (level <= adev->vm_manager.root_level) {
+		return 0xffffffffu;
+	} else if (level != AMDGPU_VM_PTB) {
+		return 0x1ffu;
+	} else {
 		return AMDGPU_VM_PTE_COUNT(adev) - 1;
+	}
+}
+
+/**
+ * amdgpu_vm_pt_level_props - combined level shift/mask for hot paths
+ *
+ * @adev: amdgpu_device pointer
+ * @level: VMPT level
+ * @shift: output address shift for the level
+ * @mask: output entry mask for the level
+ */
+static inline void amdgpu_vm_pt_level_props(const struct amdgpu_device *adev,
+					    unsigned int level,
+					    unsigned int *shift,
+					    unsigned int *mask)
+{
+	if (level == AMDGPU_VM_PTB) {
+		*shift = 0;
+		*mask = AMDGPU_VM_PTE_COUNT(adev) - 1;
+		return;
+	}
+
+	/* Non-leaf levels */
+	*shift = 9 * (AMDGPU_VM_PDB0 - level) + adev->vm_manager.block_size;
+
+	if (level <= adev->vm_manager.root_level) {
+		*mask = 0xffffffffu;
+	} else {
+		*mask = 0x1ffu;
+	}
 }
 
 /**
@@ -177,18 +208,25 @@ static bool amdgpu_vm_pt_descendant(stru
 				    struct amdgpu_vm_pt_cursor *cursor)
 {
 	unsigned int mask, shift, idx;
+	struct amdgpu_bo_vm *curvm;
 
 	if ((cursor->level == AMDGPU_VM_PTB) || !cursor->entry ||
-	    !cursor->entry->bo)
+	    !cursor->entry->bo) {
 		return false;
+	}
 
 	mask = amdgpu_vm_pt_entries_mask(adev, cursor->level);
 	shift = amdgpu_vm_pt_level_shift(adev, cursor->level);
 
+	idx = (unsigned int)((cursor->pfn >> shift) & mask);
+
+	/* Cache the VM object and prefetch the child entry to reduce latency */
+	curvm = to_amdgpu_bo_vm(cursor->entry->bo);
+	prefetch(&curvm->entries[idx]);
+
 	++cursor->level;
-	idx = (cursor->pfn >> shift) & mask;
 	cursor->parent = cursor->entry;
-	cursor->entry = &to_amdgpu_bo_vm(cursor->entry->bo)->entries[idx];
+	cursor->entry = &curvm->entries[idx];
 	return true;
 }
 
@@ -222,7 +260,11 @@ static bool amdgpu_vm_pt_sibling(struct
 		return false;
 
 	cursor->pfn += 1ULL << shift;
-	cursor->pfn &= ~((1ULL << shift) - 1);
+	cursor->pfn &= ~((1ULL << shift) - 1ULL);
+
+	/* Prefetch next sibling before we advance; guaranteed to exist here */
+	prefetch(cursor->entry + 1);
+
 	++cursor->entry;
 	return true;
 }
@@ -258,14 +300,14 @@ static bool amdgpu_vm_pt_ancestor(struct
 static void amdgpu_vm_pt_next(struct amdgpu_device *adev,
 			      struct amdgpu_vm_pt_cursor *cursor)
 {
-	/* First try a newborn child */
-	if (amdgpu_vm_pt_descendant(adev, cursor))
+	/* First try a newborn child (common fast path) */
+	if (likely(amdgpu_vm_pt_descendant(adev, cursor)))
 		return;
 
-	/* If that didn't worked try to find a sibling */
-	while (!amdgpu_vm_pt_sibling(adev, cursor)) {
+	/* If that didn't work try to find a sibling */
+	while (likely(!amdgpu_vm_pt_sibling(adev, cursor))) {
 		/* No sibling, go to our parents and grandparents */
-		if (!amdgpu_vm_pt_ancestor(cursor)) {
+		if (unlikely(!amdgpu_vm_pt_ancestor(cursor))) {
 			cursor->pfn = ~0ll;
 			return;
 		}
@@ -377,7 +419,7 @@ int amdgpu_vm_pt_clear(struct amdgpu_dev
 		}
 	}
 
-	entries = amdgpu_bo_size(bo) / 8;
+	entries = amdgpu_bo_size(bo) / 8u;
 
 	r = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);
 	if (r)
@@ -401,23 +443,29 @@ int amdgpu_vm_pt_clear(struct amdgpu_dev
 
 	addr = 0;
 
-	uint64_t value = 0, flags = 0;
-	if (adev->asic_type >= CHIP_VEGA10) {
-		if (level != AMDGPU_VM_PTB) {
-			/* Handle leaf PDEs as PTEs */
-			flags |= AMDGPU_PDE_PTE_FLAG(adev);
-			amdgpu_gmc_get_vm_pde(adev, level,
-					      &value, &flags);
-		} else {
-			/* Workaround for fault priority problem on GMC9 */
-			flags = AMDGPU_PTE_EXECUTABLE;
+	/* Construct initial value/flags and perform the update */
+	{
+		const bool gmc_v9 = adev->asic_type >= CHIP_VEGA10;
+		uint64_t value = 0;
+		uint64_t flags = 0;
+
+		if (gmc_v9) {
+			if (level != AMDGPU_VM_PTB) {
+				/* Handle leaf PDEs as PTEs */
+				flags |= AMDGPU_PDE_PTE_FLAG(adev);
+				amdgpu_gmc_get_vm_pde(adev, level,
+						      &value, &flags);
+			} else {
+				/* Workaround for fault priority problem on GMC9 */
+				flags = AMDGPU_PTE_EXECUTABLE;
+			}
 		}
-	}
 
-	r = vm->update_funcs->update(&params, vmbo, addr, 0, entries,
-				     value, flags);
-	if (r)
-		goto exit;
+		r = vm->update_funcs->update(&params, vmbo, addr, 0, entries,
+					     value, flags);
+		if (r)
+			goto exit;
+	}
 
 	r = vm->update_funcs->commit(&params, NULL);
 exit:
@@ -681,12 +729,13 @@ static void amdgpu_vm_pte_update_flags(s
 				       uint64_t flags)
 {
 	struct amdgpu_device *adev = params->adev;
+	const bool gmc_v9 = adev->asic_type >= CHIP_VEGA10;
 
-	if (level != AMDGPU_VM_PTB) {
+	if (likely(level != AMDGPU_VM_PTB)) {
 		flags |= AMDGPU_PDE_PTE_FLAG(params->adev);
 		amdgpu_gmc_get_vm_pde(adev, level, &addr, &flags);
 
-	} else if (adev->asic_type >= CHIP_VEGA10 &&
+	} else if (gmc_v9 &&
 		   !(flags & AMDGPU_PTE_VALID) &&
 		   !(flags & AMDGPU_PTE_PRT_FLAG(params->adev))) {
 
@@ -798,43 +847,56 @@ int amdgpu_vm_ptes_update(struct amdgpu_
 {
 	struct amdgpu_device *adev = params->adev;
 	struct amdgpu_vm_pt_cursor cursor;
+	const bool unlocked = params->unlocked;
+	const bool valid = (flags & AMDGPU_PTE_VALID) != 0;
 	uint64_t frag_start = start, frag_end;
 	unsigned int frag;
 	int r;
 
 	/* figure out the initial fragment */
-	amdgpu_vm_pte_fragment(params, frag_start, end, flags, &frag,
-			       &frag_end);
+	amdgpu_vm_pte_fragment(params, frag_start, end, flags, &frag, &frag_end);
 
 	/* walk over the address space and update the PTs */
 	amdgpu_vm_pt_start(adev, params->vm, start, &cursor);
+
+	/* Cache per-level properties to avoid repeated calls in the loop */
+	unsigned int cached_level = ~0U;
+	unsigned int shift = 0;
+	unsigned int parent_shift = 0;
+	unsigned int mask = 0;
+
 	while (cursor.pfn < end) {
-		unsigned int shift, parent_shift, mask;
-		uint64_t incr, entry_end, pe_start;
 		struct amdgpu_bo *pt;
+		uint64_t incr, entry_end, pe_start;
+
+		if (unlikely(cursor.level != cached_level)) {
+			cached_level = cursor.level;
+			amdgpu_vm_pt_level_props(adev, cached_level, &shift, &mask);
+			parent_shift = amdgpu_vm_pt_level_shift(adev, cached_level - 1);
+		}
 
-		if (!params->unlocked) {
+		if (!unlocked) {
 			/* make sure that the page tables covering the
 			 * address range are actually allocated
 			 */
 			r = amdgpu_vm_pt_alloc(params->adev, params->vm,
 					       &cursor, params->immediate);
-			if (r)
+			if (r) {
 				return r;
+			}
 		}
 
-		shift = amdgpu_vm_pt_level_shift(adev, cursor.level);
-		parent_shift = amdgpu_vm_pt_level_shift(adev, cursor.level - 1);
-		if (params->unlocked) {
+		if (unlocked) {
 			/* Unlocked updates are only allowed on the leaves */
-			if (amdgpu_vm_pt_descendant(adev, &cursor))
+			if (amdgpu_vm_pt_descendant(adev, &cursor)) {
 				continue;
-		} else if (adev->asic_type < CHIP_VEGA10 &&
-			   (flags & AMDGPU_PTE_VALID)) {
+			}
+		} else if (adev->asic_type < CHIP_VEGA10 && valid) {
 			/* No huge page support before GMC v9 */
 			if (cursor.level != AMDGPU_VM_PTB) {
-				if (!amdgpu_vm_pt_descendant(adev, &cursor))
+				if (!amdgpu_vm_pt_descendant(adev, &cursor)) {
 					return -ENOENT;
+				}
 				continue;
 			}
 		} else if (frag < shift) {
@@ -842,55 +904,66 @@ int amdgpu_vm_ptes_update(struct amdgpu_
 			 * smaller than the address shift. Go to the next
 			 * child entry and try again.
 			 */
-			if (amdgpu_vm_pt_descendant(adev, &cursor))
+			if (amdgpu_vm_pt_descendant(adev, &cursor)) {
 				continue;
+			}
 		} else if (frag >= parent_shift) {
 			/* If the fragment size is even larger than the parent
 			 * shift we should go up one level and check it again.
 			 */
-			if (!amdgpu_vm_pt_ancestor(&cursor))
+			if (!amdgpu_vm_pt_ancestor(&cursor)) {
 				return -EINVAL;
+			}
+			/* Recompute since level changed */
+			cached_level = cursor.level;
+			amdgpu_vm_pt_level_props(adev, cached_level, &shift, &mask);
+			parent_shift = amdgpu_vm_pt_level_shift(adev, cached_level - 1);
 			continue;
 		}
 
 		pt = cursor.entry->bo;
 		if (!pt) {
 			/* We need all PDs and PTs for mapping something, */
-			if (flags & AMDGPU_PTE_VALID)
+			if (valid) {
 				return -ENOENT;
+			}
 
 			/* but unmapping something can happen at a higher
 			 * level.
 			 */
-			if (!amdgpu_vm_pt_ancestor(&cursor))
+			if (!amdgpu_vm_pt_ancestor(&cursor)) {
 				return -EINVAL;
+			}
 
 			pt = cursor.entry->bo;
-			shift = parent_shift;
-			frag_end = max(frag_end, ALIGN(frag_start + 1,
-				   1ULL << shift));
+			/* We moved up; sync cached properties */
+			cached_level = cursor.level;
+			amdgpu_vm_pt_level_props(adev, cached_level, &shift, &mask);
+			parent_shift = amdgpu_vm_pt_level_shift(adev, cached_level - 1);
+
+			frag_end = max(frag_end, ALIGN(frag_start + 1, 1ULL << shift));
 		}
 
 		/* Looks good so far, calculate parameters for the update */
 		incr = (uint64_t)AMDGPU_GPU_PAGE_SIZE << shift;
-		mask = amdgpu_vm_pt_entries_mask(adev, cursor.level);
-		pe_start = ((cursor.pfn >> shift) & mask) * 8;
+		pe_start = ((cursor.pfn >> shift) & mask) * 8ULL;
 
-		if (cursor.level < AMDGPU_VM_PTB && params->unlocked)
+		if (cursor.level < AMDGPU_VM_PTB && unlocked) {
 			/*
-			 * MMU notifier callback unlocked unmap huge page, leave is PDE entry,
-			 * only clear one entry. Next entry search again for PDE or PTE leave.
+			 * MMU notifier callback unlocked unmap huge page, leaf is PDE entry,
+			 * only clear one entry. Next entry search again for PDE or PTE leaf.
 			 */
 			entry_end = 1ULL << shift;
-		else
-			entry_end = ((uint64_t)mask + 1) << shift;
-		entry_end += cursor.pfn & ~(entry_end - 1);
+		} else {
+			entry_end = ((uint64_t)mask + 1ULL) << shift;
+		}
+		entry_end += cursor.pfn & ~(entry_end - 1ULL);
 		entry_end = min(entry_end, end);
 
 		do {
 			struct amdgpu_vm *vm = params->vm;
 			uint64_t upd_end = min(entry_end, frag_end);
-			unsigned int nptes = (upd_end - frag_start) >> shift;
+			unsigned int nptes = (unsigned int)((upd_end - frag_start) >> shift);
 			uint64_t upd_flags = flags | AMDGPU_PTE_FRAG(frag);
 
 			/* This can happen when we set higher level PDs to
@@ -903,20 +976,21 @@ int amdgpu_vm_ptes_update(struct amdgpu_
 						    upd_flags,
 						    vm->task_info ? vm->task_info->tgid : 0,
 						    vm->immediate.fence_context);
+
 			amdgpu_vm_pte_update_flags(params, to_amdgpu_bo_vm(pt),
 						   cursor.level, pe_start, dst,
-						   nptes, incr, upd_flags);
+						   nptes, (uint32_t)incr, upd_flags);
 
-			pe_start += nptes * 8;
-			dst += nptes * incr;
+			pe_start += (uint64_t)nptes * 8ULL;
+			dst += (uint64_t)nptes * incr;
 
 			frag_start = upd_end;
 			if (frag_start >= frag_end) {
 				/* figure out the next fragment */
-				amdgpu_vm_pte_fragment(params, frag_start, end,
-						       flags, &frag, &frag_end);
-				if (frag < shift)
+				amdgpu_vm_pte_fragment(params, frag_start, end, flags, &frag, &frag_end);
+				if (frag < shift) {
 					break;
+				}
 			}
 		} while (frag_start < entry_end);
 
