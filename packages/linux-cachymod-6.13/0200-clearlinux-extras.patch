##
# Clear Linux Patches
# https://github.com/clearlinux-pkgs/linux
#
# Included in the CachyOS patch set
# Patch0109: 0109-initialize-ata-before-graphics.patch
# Patch0120: 0120-do-accept-in-LIFO-order-for-cache-efficiency.patch
# Patch0121: 0121-locking-rwsem-spin-faster.patch
#
# Patch0050: 0050-Revert-ext4-do-not-create-EA-inode-under-buffer-lock.patch
# Patch0102: 0102-increase-the-ext4-default-commit-age.patch
# Patch0104: 0104-pci-pme-wakeups.patch
# Patch0108: 0108-smpboot-reuse-timer-calibration.patch
# Patch0111: 0111-ipv4-tcp-allow-the-memory-tuning-for-tcp-to-go-a-lit.patch
# Patch0116: 0116-migrate-some-systemd-defaults-to-the-kernel-defaults.patch
# Patch0117: 0117-xattr-allow-setting-user.-attributes-on-symlinks-by-.patch
# Patch0128: 0128-itmt_epb-use-epb-to-scale-itmt.patch
# Patch0130: 0130-itmt2-ADL-fixes.patch
# Patch0131: 0131-add-a-per-cpu-minimum-high-watermark-an-tune-batch-s.patch
# Patch0135: 0135-initcall-only-print-non-zero-initcall-debug-to-speed.patch
# Patch0137: libsgrowdown.patch
# Patch0141: epp-retune.patch
# Patch0148: 0002-sched-core-add-some-branch-hints-based-on-gcov-analy.patch
# Patch0156: ratelimit-sched-yield.patch
# Patch0157: scale-net-alloc.patch
# Patch0158: 0158-clocksource-only-perform-extended-clocksource-checks.patch
# Patch0164: 0164-KVM-VMX-make-vmx-init-a-late-init-call-to-get-to-ini.patch
# Patch0165: slack.patch
# Patch0167: 0167-net-sock-increase-default-number-of-_SK_MEM_PACKETS-.patch
##

# Allow disabling the LDT (local descriptor table).

diff -uar a/arch/x86/Kconfig b/arch/x86/Kconfig
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -1247,7 +1247,7 @@
 	default X86_LEGACY_VM86
 
 config X86_16BIT
-	bool "Enable support for 16-bit segments" if EXPERT
+	bool "Enable support for 16-bit segments"
 	default y
 	depends on MODIFY_LDT_SYSCALL
 	help
@@ -2389,7 +2389,7 @@
 	  be set to 'N' under normal conditions.
 
 config MODIFY_LDT_SYSCALL
-	bool "Enable the LDT (local descriptor table)" if EXPERT
+	bool "Enable the LDT (local descriptor table)"
 	default y
 	help
 	  Linux can allow user programs to install a per-process x86

# Allow disabling legacy 16-bit UID syscall wrappers, and sysfs syscall support.

diff -uar a/init/Kconfig b/init/Kconfig
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1583,7 +1583,7 @@
 	  Only use this if you really know what you are doing.
 
 config UID16
-	bool "Enable 16-bit UID system calls" if EXPERT
+	bool "Enable 16-bit UID system calls"
 	depends on HAVE_UID16 && MULTIUSER
 	default y
 	help
@@ -1614,7 +1614,7 @@
 	  If unsure, leave the default option here.
 
 config SYSFS_SYSCALL
-	bool "Sysfs syscall support" if EXPERT
+	bool "Sysfs syscall support"
 	default y
 	help
 	  sys_sysfs is an obsolete system call no longer supported in libc.

# Disable more debug knobs.

diff -uar a/lib/Kconfig.debug b/lib/Kconfig.debug
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -198,7 +198,7 @@
 	  (about 3KB), but can make the kernel logs easier to read.
 
 config DEBUG_BUGVERBOSE
-	bool "Verbose BUG() reporting (adds 70K)" if DEBUG_KERNEL && EXPERT
+	bool "Verbose BUG() reporting (adds 70K)" if DEBUG_KERNEL
 	depends on BUG && (GENERIC_BUG || HAVE_DEBUG_BUGVERBOSE)
 	default y
 	help
@@ -891,7 +891,7 @@
 	  regions to be regularly checked for invalid topology.
 
 config DEBUG_MEMORY_INIT
-	bool "Debug memory initialisation" if EXPERT
+	bool "Debug memory initialisation"
 	default !EXPERT
 	help
 	  Enable this for additional checks during memory initialisation.

diff -uar a/mm/Kconfig.debug b/mm/Kconfig.debug
--- a/mm/Kconfig.debug
+++ b/mm/Kconfig.debug
@@ -47,7 +47,7 @@
 
 config SLUB_DEBUG
 	default y
-	bool "Enable SLUB debugging support" if EXPERT
+	bool "Enable SLUB debugging support"
 	depends on SYSFS && !SLUB_TINY
 	select STACKDEPOT if STACKTRACE_SUPPORT
 	help

# Patch0050: 0050-Revert-ext4-do-not-create-EA-inode-under-buffer-lock.patch

From c13c0e90c89beccfc740aad680f324c1aa6da590 Mon Sep 17 00:00:00 2001
From: Colin Ian King <colin.i.king@intel.com>
Date: Thu, 24 Oct 2024 12:11:04 +0100
Subject: [PATCH] Revert "ext4: do not create EA inode under buffer lock"

Currenlty this commit makes it much easier to hit an ABBA lock
bug, so reverting it reduces this risk, even though it's not
a final solution.

See: https://bugzilla.kernel.org/show_bug.cgi?id=219283

This reverts commit 0a46ef234756dca04623b7591e8ebb3440622f0b.
---
 fs/ext4/xattr.c | 113 +++++++++++++++++++++++++-----------------------
 1 file changed, 60 insertions(+), 53 deletions(-)

diff --git a/fs/ext4/xattr.c b/fs/ext4/xattr.c
index 46ce2f21fef9..d2feef9793d0 100644
--- a/fs/ext4/xattr.c
+++ b/fs/ext4/xattr.c
@@ -1625,7 +1625,6 @@ static struct inode *ext4_xattr_inode_lookup_create(handle_t *handle,
 static int ext4_xattr_set_entry(struct ext4_xattr_info *i,
 				struct ext4_xattr_search *s,
 				handle_t *handle, struct inode *inode,
-				struct inode *new_ea_inode,
 				bool is_block)
 {
 	struct ext4_xattr_entry *last, *next;
@@ -1633,6 +1632,7 @@ static int ext4_xattr_set_entry(struct ext4_xattr_info *i,
 	size_t min_offs = s->end - s->base, name_len = strlen(i->name);
 	int in_inode = i->in_inode;
 	struct inode *old_ea_inode = NULL;
+	struct inode *new_ea_inode = NULL;
 	size_t old_size, new_size;
 	int ret;
 
@@ -1717,11 +1717,38 @@ static int ext4_xattr_set_entry(struct ext4_xattr_info *i,
 			old_ea_inode = NULL;
 			goto out;
 		}
+	}
+	if (i->value && in_inode) {
+		WARN_ON_ONCE(!i->value_len);
+
+		new_ea_inode = ext4_xattr_inode_lookup_create(handle, inode,
+					i->value, i->value_len);
+		if (IS_ERR(new_ea_inode)) {
+			ret = PTR_ERR(new_ea_inode);
+			new_ea_inode = NULL;
+			goto out;
+		}
+	}
 
+	if (old_ea_inode) {
 		/* We are ready to release ref count on the old_ea_inode. */
 		ret = ext4_xattr_inode_dec_ref(handle, old_ea_inode);
-		if (ret)
+		if (ret) {
+			/* Release newly required ref count on new_ea_inode. */
+			if (new_ea_inode) {
+				int err;
+
+				err = ext4_xattr_inode_dec_ref(handle,
+							       new_ea_inode);
+				if (err)
+					ext4_warning_inode(new_ea_inode,
+						  "dec ref new_ea_inode err=%d",
+						  err);
+				ext4_xattr_inode_free_quota(inode, new_ea_inode,
+							    i->value_len);
+			}
 			goto out;
+		}
 
 		ext4_xattr_inode_free_quota(inode, old_ea_inode,
 					    le32_to_cpu(here->e_value_size));
@@ -1845,6 +1872,7 @@ static int ext4_xattr_set_entry(struct ext4_xattr_info *i,
 	ret = 0;
 out:
 	iput(old_ea_inode);
+	iput(new_ea_inode);
 	return ret;
 }
 
@@ -1907,20 +1935,8 @@ ext4_xattr_block_set(handle_t *handle, struct inode *inode,
 	size_t old_ea_inode_quota = 0;
 	unsigned int ea_ino;
 
-#define header(x) ((struct ext4_xattr_header *)(x))
-
-	/* If we need EA inode, prepare it before locking the buffer */
-	if (i->value && i->in_inode) {
-		WARN_ON_ONCE(!i->value_len);
 
-		ea_inode = ext4_xattr_inode_lookup_create(handle, inode,
-					i->value, i->value_len);
-		if (IS_ERR(ea_inode)) {
-			error = PTR_ERR(ea_inode);
-			ea_inode = NULL;
-			goto cleanup;
-		}
-	}
+#define header(x) ((struct ext4_xattr_header *)(x))
 
 	if (s->base) {
 		int offset = (char *)s->here - bs->bh->b_data;
@@ -1930,7 +1946,6 @@ ext4_xattr_block_set(handle_t *handle, struct inode *inode,
 						      EXT4_JTR_NONE);
 		if (error)
 			goto cleanup;
-
 		lock_buffer(bs->bh);
 
 		if (header(s->base)->h_refcount == cpu_to_le32(1)) {
@@ -1957,7 +1972,7 @@ ext4_xattr_block_set(handle_t *handle, struct inode *inode,
 			}
 			ea_bdebug(bs->bh, "modifying in-place");
 			error = ext4_xattr_set_entry(i, s, handle, inode,
-					     ea_inode, true /* is_block */);
+						     true /* is_block */);
 			ext4_xattr_block_csum_set(inode, bs->bh);
 			unlock_buffer(bs->bh);
 			if (error == -EFSCORRUPTED)
@@ -2025,13 +2040,29 @@ ext4_xattr_block_set(handle_t *handle, struct inode *inode,
 		s->end = s->base + sb->s_blocksize;
 	}
 
-	error = ext4_xattr_set_entry(i, s, handle, inode, ea_inode,
-				     true /* is_block */);
+	error = ext4_xattr_set_entry(i, s, handle, inode, true /* is_block */);
 	if (error == -EFSCORRUPTED)
 		goto bad_block;
 	if (error)
 		goto cleanup;
 
+	if (i->value && s->here->e_value_inum) {
+		/*
+		 * A ref count on ea_inode has been taken as part of the call to
+		 * ext4_xattr_set_entry() above. We would like to drop this
+		 * extra ref but we have to wait until the xattr block is
+		 * initialized and has its own ref count on the ea_inode.
+		 */
+		ea_ino = le32_to_cpu(s->here->e_value_inum);
+		error = ext4_xattr_inode_iget(inode, ea_ino,
+					      le32_to_cpu(s->here->e_hash),
+					      &ea_inode);
+		if (error) {
+			ea_inode = NULL;
+			goto cleanup;
+		}
+	}
+
 inserted:
 	if (!IS_LAST_ENTRY(s->first)) {
 		new_bh = ext4_xattr_block_cache_find(inode, header(s->base), &ce);
@@ -2189,16 +2220,17 @@ ext4_xattr_block_set(handle_t *handle, struct inode *inode,
 
 cleanup:
 	if (ea_inode) {
-		if (error) {
-			int error2;
+		int error2;
+
+		error2 = ext4_xattr_inode_dec_ref(handle, ea_inode);
+		if (error2)
+			ext4_warning_inode(ea_inode, "dec ref error=%d",
+					   error2);
 
-			error2 = ext4_xattr_inode_dec_ref(handle, ea_inode);
-			if (error2)
-				ext4_warning_inode(ea_inode, "dec ref error=%d",
-						   error2);
+		/* If there was an error, revert the quota charge. */
+		if (error)
 			ext4_xattr_inode_free_quota(inode, ea_inode,
 						    i_size_read(ea_inode));
-		}
 		iput(ea_inode);
 	}
 	if (ce)
@@ -2256,38 +2288,14 @@ int ext4_xattr_ibody_set(handle_t *handle, struct inode *inode,
 {
 	struct ext4_xattr_ibody_header *header;
 	struct ext4_xattr_search *s = &is->s;
-	struct inode *ea_inode = NULL;
 	int error;
 
 	if (!EXT4_INODE_HAS_XATTR_SPACE(inode))
 		return -ENOSPC;
 
-	/* If we need EA inode, prepare it before locking the buffer */
-	if (i->value && i->in_inode) {
-		WARN_ON_ONCE(!i->value_len);
-
-		ea_inode = ext4_xattr_inode_lookup_create(handle, inode,
-					i->value, i->value_len);
-		if (IS_ERR(ea_inode))
-			return PTR_ERR(ea_inode);
-	}
-	error = ext4_xattr_set_entry(i, s, handle, inode, ea_inode,
-				     false /* is_block */);
-	if (error) {
-		if (ea_inode) {
-			int error2;
-
-			error2 = ext4_xattr_inode_dec_ref(handle, ea_inode);
-			if (error2)
-				ext4_warning_inode(ea_inode, "dec ref error=%d",
-						   error2);
-
-			ext4_xattr_inode_free_quota(inode, ea_inode,
-						    i_size_read(ea_inode));
-			iput(ea_inode);
-		}
+	error = ext4_xattr_set_entry(i, s, handle, inode, false /* is_block */);
+	if (error)
 		return error;
-	}
 	header = IHDR(inode, ext4_raw_inode(&is->iloc));
 	if (!IS_LAST_ENTRY(s->first)) {
 		header->h_magic = cpu_to_le32(EXT4_XATTR_MAGIC);
@@ -2296,7 +2304,6 @@ int ext4_xattr_ibody_set(handle_t *handle, struct inode *inode,
 		header->h_magic = cpu_to_le32(0);
 		ext4_clear_inode_state(inode, EXT4_STATE_XATTR);
 	}
-	iput(ea_inode);
 	return 0;
 }
 
-- 
2.47.0

# Patch0102: 0102-increase-the-ext4-default-commit-age.patch

From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Arjan van de Ven <arjan@linux.intel.com>
Date: Mon, 11 Jan 2016 10:01:44 -0600
Subject: [PATCH] increase the ext4 default commit age

Both the VM and EXT4 have a "commit to disk after X seconds" time.
Currently the EXT4 time is shorter than our VM time, which is a bit
suboptional,
it's better for performance to let the VM do the writeouts in bulk
rather than something deep in the journalling layer.

(DISTRO TWEAK -- NOT FOR UPSTREAM)

Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
Signed-off-by: Jose Carlos Venegas Munoz <jose.carlos.venegas.munoz@intel.com>
---
 include/linux/jbd2.h | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/include/linux/jbd2.h b/include/linux/jbd2.h
index 9c3ada74ffb1..c4aef0bb2661 100644
--- a/include/linux/jbd2.h
+++ b/include/linux/jbd2.h
@@ -45,7 +45,7 @@
 /*
  * The default maximum commit age, in seconds.
  */
-#define JBD2_DEFAULT_MAX_COMMIT_AGE 5
+#define JBD2_DEFAULT_MAX_COMMIT_AGE 30
 
 #ifdef CONFIG_JBD2_DEBUG
 /*
-- 
https://clearlinux.org

# Patch0104: 0104-pci-pme-wakeups.patch

From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Arjan van de Ven <arjan@linux.intel.com>
Date: Mon, 14 Mar 2016 11:10:58 -0600
Subject: [PATCH] pci pme wakeups

Reduce wakeups for PME checks, which are a workaround for miswired
boards (sadly, too many of them) in laptops.
---
 drivers/pci/pci.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/drivers/pci/pci.c b/drivers/pci/pci.c
index d25122fbe98a..dbfb6aaa4a07 100644
--- a/drivers/pci/pci.c
+++ b/drivers/pci/pci.c
@@ -61,7 +61,7 @@ struct pci_pme_device {
 	struct pci_dev *dev;
 };
 
-#define PME_TIMEOUT 1000 /* How long between PME checks */
+#define PME_TIMEOUT 4000 /* How long between PME checks */
 
 /*
  * Following exit from Conventional Reset, devices must be ready within 1 sec
-- 
https://clearlinux.org

# Patch0108: 0108-smpboot-reuse-timer-calibration.patch

From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Arjan van de Ven <arjan@linux.intel.com>
Date: Wed, 11 Feb 2015 17:28:14 -0600
Subject: [PATCH] smpboot: reuse timer calibration

NO point recalibrating for known-constant tsc ...
saves 200ms+ of boot time.
---
 arch/x86/kernel/tsc.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index a698196377be..5f3ee7c31c8a 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1593,6 +1593,9 @@ unsigned long calibrate_delay_is_known(void)
 	if (!constant_tsc || !mask)
 		return 0;
 
+	if (cpu != 0)
+		return cpu_data(0).loops_per_jiffy;
+
 	sibling = cpumask_any_but(mask, cpu);
 	if (sibling < nr_cpu_ids)
 		return cpu_data(sibling).loops_per_jiffy;
-- 
https://clearlinux.org

# Patch0111: 0111-ipv4-tcp-allow-the-memory-tuning-for-tcp-to-go-a-lit.patch

From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Arjan van de Ven <arjan@linux.intel.com>
Date: Fri, 6 Jan 2017 15:34:09 +0000
Subject: [PATCH] ipv4/tcp: allow the memory tuning for tcp to go a little
 bigger than default

---
 net/ipv4/tcp.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/net/ipv4/tcp.c b/net/ipv4/tcp.c
index 28ff2a820f7c..c4f240da8d70 100644
--- a/net/ipv4/tcp.c
+++ b/net/ipv4/tcp.c
@@ -5129,8 +5129,8 @@ void __init tcp_init(void)
 	tcp_init_mem();
 	/* Set per-socket limits to no more than 1/128 the pressure threshold */
 	limit = nr_free_buffer_pages() << (PAGE_SHIFT - 7);
-	max_wshare = min(4UL*1024*1024, limit);
-	max_rshare = min(6UL*1024*1024, limit);
+	max_wshare = min(16UL*1024*1024, limit);
+	max_rshare = min(16UL*1024*1024, limit);
 
 	init_net.ipv4.sysctl_tcp_wmem[0] = PAGE_SIZE;
 	init_net.ipv4.sysctl_tcp_wmem[1] = 16*1024;
-- 
https://clearlinux.org

# Patch0116: 0116-migrate-some-systemd-defaults-to-the-kernel-defaults.patch

From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Auke Kok <auke-jan.h.kok@intel.com>
Date: Thu, 2 Aug 2018 12:03:22 -0700
Subject: [PATCH] migrate some systemd defaults to the kernel defaults.

These settings are needed to prevent networking issues when
the networking modules come up by default without explicit
settings, which breaks some cases.

We don't want the modprobe settings to be read at boot time
if we're not going to do anything else ever.
---
 drivers/net/dummy.c             | 2 +-
 include/uapi/linux/if_bonding.h | 2 +-
 2 files changed, 2 insertions(+), 2 deletions(-)

diff --git a/drivers/net/dummy.c b/drivers/net/dummy.c
index f82ad7419508..5e8faa70aad6 100644
--- a/drivers/net/dummy.c
+++ b/drivers/net/dummy.c
@@ -43,7 +43,7 @@
 
 #define DRV_NAME	"dummy"
 
-static int numdummies = 1;
+static int numdummies = 0;
 
 /* fake multicast ability */
 static void set_multicast_list(struct net_device *dev)
diff --git a/include/uapi/linux/if_bonding.h b/include/uapi/linux/if_bonding.h
index d174914a837d..bf8e2af101a3 100644
--- a/include/uapi/linux/if_bonding.h
+++ b/include/uapi/linux/if_bonding.h
@@ -82,7 +82,7 @@
 #define BOND_STATE_ACTIVE       0   /* link is active */
 #define BOND_STATE_BACKUP       1   /* link is backup */
 
-#define BOND_DEFAULT_MAX_BONDS  1   /* Default maximum number of devices to support */
+#define BOND_DEFAULT_MAX_BONDS  0   /* Default maximum number of devices to support */
 
 #define BOND_DEFAULT_TX_QUEUES 16   /* Default number of tx queues per device */
 
-- 
https://clearlinux.org

# Patch0117: 0117-xattr-allow-setting-user.-attributes-on-symlinks-by-.patch

From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Alan Cox <alan@linux.intel.com>
Date: Thu, 10 Mar 2016 15:11:28 +0000
Subject: [PATCH] xattr: allow setting user.* attributes on symlinks by owner

Kvmtool and clear containers supports using user attributes to label host
files with the virtual uid/guid of the file in the container. This allows an
end user to manage their files and a complete uid space without all the ugly
namespace stuff.

The one gap in the support is symlinks because an end user can change the
ownership of a symbolic link. We support attributes on these files as you
can already (as root) set security attributes on them.

The current rules seem slightly over-paranoid and as we have a use case this
patch enables updating the attributes on a symbolic link IFF you are the
owner of the synlink (as permissions are not usually meaningful on the link
itself).

Signed-off-by: Alan Cox <alan@linux.intel.com>
---
 fs/xattr.c | 15 ++++++++-------
 1 file changed, 8 insertions(+), 7 deletions(-)

diff --git a/fs/xattr.c b/fs/xattr.c
index 998045165916..62b6fb4dedee 100644
--- a/fs/xattr.c
+++ b/fs/xattr.c
@@ -139,16 +139,17 @@ xattr_permission(struct user_namespace *mnt_userns, struct inode *inode,
 	}
 
 	/*
-	 * In the user.* namespace, only regular files and directories can have
-	 * extended attributes. For sticky directories, only the owner and
-	 * privileged users can write attributes.
+	 * In the user.* namespace, only regular files, symbolic links, and
+	 * directories can have extended attributes. For symbolic links and
+	 * sticky directories, only the owner and privileged users can write
+	 * attributes.
 	 */
 	if (!strncmp(name, XATTR_USER_PREFIX, XATTR_USER_PREFIX_LEN)) {
-		if (!S_ISREG(inode->i_mode) && !S_ISDIR(inode->i_mode))
+		if (!S_ISREG(inode->i_mode) && !S_ISDIR(inode->i_mode) && !S_ISLNK(inode->i_mode))
 			return (mask & MAY_WRITE) ? -EPERM : -ENODATA;
-		if (S_ISDIR(inode->i_mode) && (inode->i_mode & S_ISVTX) &&
-		    (mask & MAY_WRITE) &&
-		    !inode_owner_or_capable(idmap, inode))
+		if (((S_ISDIR(inode->i_mode) && (inode->i_mode & S_ISVTX))
+		        || S_ISLNK(inode->i_mode)) && (mask & MAY_WRITE)
+		    && !inode_owner_or_capable(idmap, inode))
 			return -EPERM;
 	}
 
-- 
https://clearlinux.org

# Patch0131: 0131-add-a-per-cpu-minimum-high-watermark-an-tune-batch-s.patch

From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Arjan van de Ven <arjan.van.de.ven@intel.com>
Date: Tue, 23 Nov 2021 17:38:50 +0000
Subject: [PATCH] add a per cpu minimum high watermark an tune batch size

make sure there's at least 1024 per cpu pages... a reasonably small
amount for todays system
---
 mm/page_alloc.c | 5 +++--
 1 file changed, 3 insertions(+), 2 deletions(-)

diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index e6f211dcf82e..0ea48434ac7d 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -5397,11 +5397,11 @@ static int zone_batchsize(struct zone *zone)
 
 	/*
 	 * The number of pages to batch allocate is either ~0.1%
-	 * of the zone or 1MB, whichever is smaller. The batch
+	 * of the zone or 4MB, whichever is smaller. The batch
 	 * size is striking a balance between allocation latency
 	 * and zone lock contention.
 	 */
-	batch = min(zone_managed_pages(zone) >> 10, SZ_1M / PAGE_SIZE);
+	batch = min(zone_managed_pages(zone) >> 10, 4 * SZ_1M / PAGE_SIZE);
 	batch /= 4;		/* We effectively *= 4 below */
 	if (batch < 1)
 		batch = 1;
-- 
https://clearlinux.org

# Patch0137: libsgrowdown.patch

Place libraries right below the binary for PIE binaries, this helps code locality
(and thus performance).

--- linux-5.18.2/fs/binfmt_elf.c~	2022-06-06 06:49:00.000000000 +0000
+++ linux-5.18.2/fs/binfmt_elf.c	2022-08-10 13:53:04.878633166 +0000
@@ -1288,6 +1288,8 @@
 	mm = current->mm;
 	mm->end_code = end_code;
 	mm->start_code = start_code;
+	if (start_code >= ELF_ET_DYN_BASE)
+		mm->mmap_base = start_code;
 	mm->start_data = start_data;
 	mm->end_data = end_data;
 	mm->start_stack = bprm->p;

# Patch0148: 0002-sched-core-add-some-branch-hints-based-on-gcov-analy.patch

From ccf1330dad77ddc2a6d38192fa86f36fc7c24d92 Mon Sep 17 00:00:00 2001
From: Colin Ian King <colin.king@intel.com>
Date: Wed, 1 Feb 2023 11:53:51 +0000
Subject: [PATCH] sched/core: add some branch hints based on gcov analysis

Signed-off-by: Colin Ian King <colin.king@intel.com>
---
 kernel/sched/core.c | 8 ++++----
 1 file changed, 4 insertions(+), 4 deletions(-)

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index f730b6fe94a7..ee0ec4ab7d1c 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -596,7 +596,7 @@ void raw_spin_rq_lock_nested(struct rq *rq, int subclass)
 
 	/* Matches synchronize_rcu() in __sched_core_enable() */
 	preempt_disable();
-	if (sched_core_disabled()) {
+	if (likely(sched_core_disabled())) {
 		raw_spin_lock_nested(&rq->__lock, subclass);
 		/* preempt_count *MUST* be > 1 */
 		preempt_enable_no_resched();
@@ -809,7 +809,7 @@ void update_rq_clock(struct rq *rq)
 	scx_rq_clock_update(rq, clock);
 
 	delta = clock - rq->clock;
-	if (delta < 0)
+	if (unlikely(delta < 0))
 		return;
 	rq->clock += delta;
 
@@ -6112,7 +6112,7 @@ pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
 	struct rq *rq_i;
 	bool need_sync;
 
-	if (!sched_core_enabled(rq))
+	if (likely(!sched_core_enabled(rq)))
 		return __pick_next_task(rq, prev, rf);
 
 	cpu = cpu_of(rq);
@@ -7284,7 +7284,7 @@ SYSCALL_DEFINE0(sched_yield)
 #if !defined(CONFIG_PREEMPTION) || defined(CONFIG_PREEMPT_DYNAMIC)
 int __sched __cond_resched(void)
 {
-	if (should_resched(0) && !irqs_disabled()) {
+	if (unlikely(should_resched(0)) && !irqs_disabled()) {
 		preempt_schedule_common();
 		return 1;
 	}
-- 
2.39.1

--- a/kernel/sched/core.c	2025-03-20 12:24:56.192854379 +0100
+++ b/kernel/sched/core.c	2025-03-20 12:20:17.144480780 +0100
@@ -603,11 +603,23 @@ void raw_spin_rq_lock_nested(struct rq *
 		return;
 	}
 
+	/* Try lock acquisition once before entering retry loop */
+	lock = __rq_lockp(rq);
+	raw_spin_lock_nested(lock, subclass);
+
+	if (likely(lock == __rq_lockp(rq))) {
+		preempt_enable_no_resched();
+		return;
+	}
+
+	/* Lock pointer changed during acquisition, release and retry */
+	raw_spin_unlock(lock);
+
 	for (;;) {
 		lock = __rq_lockp(rq);
 		raw_spin_lock_nested(lock, subclass);
+
 		if (likely(lock == __rq_lockp(rq))) {
-			/* preempt_count *MUST* be > 1 */
 			preempt_enable_no_resched();
 			return;
 		}
@@ -793,26 +805,35 @@ static void update_rq_clock_task(struct
 void update_rq_clock(struct rq *rq)
 {
 	s64 delta;
-	u64 clock;
+	u64 clock, old_clock;
 
 	lockdep_assert_rq_held(rq);
 
 	if (rq->clock_update_flags & RQCF_ACT_SKIP)
 		return;
 
-#ifdef CONFIG_SCHED_DEBUG
+	#ifdef CONFIG_SCHED_DEBUG
 	if (sched_feat(WARN_DOUBLE_CLOCK))
 		SCHED_WARN_ON(rq->clock_update_flags & RQCF_UPDATED);
 	rq->clock_update_flags |= RQCF_UPDATED;
-#endif
+	#endif
+
+	/* Cache old clock value to reduce memory reads */
+	old_clock = rq->clock;
+
+	/* Get current clock value (expensive call) */
 	clock = sched_clock_cpu(cpu_of(rq));
 	scx_rq_clock_update(rq, clock);
 
-	delta = clock - rq->clock;
+	/* Calculate delta and reject backward time jumps */
+	delta = clock - old_clock;
 	if (unlikely(delta < 0))
 		return;
-	rq->clock += delta;
 
+	/* Update rq clock - using cached value improves performance */
+	rq->clock = old_clock + delta;
+
+	/* Update dependent clock values */
 	update_rq_clock_task(rq, delta);
 }

 
# Patch0156: ratelimit-sched-yield.patch

From 467904416b3786c9f2b29ca683d36cb2523ae7ce Mon Sep 17 00:00:00 2001
From: Colin Ian King <colin.i.king@intel.com>
Date: Thu, 17 Oct 2024 16:29:50 +0100
Subject: [PATCH] handle sched_yield gracefully when being hammered

Some misguided apps hammer sched_yield() in a tight loop (they should be using futexes instead)
which causes massive lock contention even if there is little work to do or to yield to.
rare limit yielding since the base scheduler does a pretty good job already about just
running the right things

Signed-off-by: Colin Ian King <colin.i.king@intel.com>
---
 kernel/sched/syscalls.c | 12 ++++++++++++
 1 file changed, 12 insertions(+)

diff --git a/kernel/sched/syscalls.c b/kernel/sched/syscalls.c
index ae1b42775ef9..441ac65f4f15 100644
--- a/kernel/sched/syscalls.c
+++ b/kernel/sched/syscalls.c
@@ -1352,10 +1352,22 @@ SYSCALL_DEFINE3(sched_getaffinity, pid_t, pid, unsigned int, len,
 	return ret;
 }
 
+static DEFINE_PER_CPU(unsigned long, last_yield);
+
 static void do_sched_yield(void)
 {
 	struct rq_flags rf;
 	struct rq *rq;
+	int cpu = raw_smp_processor_id();
+
+	cond_resched();
+
+	/* rate limit yielding to something sensible */
+
+	if (!time_after(jiffies, per_cpu(last_yield, cpu)))
+		return;
+
+	per_cpu(last_yield, cpu) = jiffies + msecs_to_jiffies(2);
 
 	rq = this_rq_lock_irq(&rf);
 
-- 
2.46.2

# Patch0157: scale-net-alloc.patch

diff --git a/include/net/sock.h b/include/net/sock.h
index 4e787285fc66..3e045f6eb6ee 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1585,10 +1585,17 @@ static inline void sk_mem_charge(struct sock *sk, int size)
 
 static inline void sk_mem_uncharge(struct sock *sk, int size)
 {
+	int reclaimable, reclaim_threshold;
+
+	reclaim_threshold = 64 * 1024;
 	if (!sk_has_account(sk))
 		return;
 	sk_forward_alloc_add(sk, size);
-	sk_mem_reclaim(sk);
+	reclaimable = sk->sk_forward_alloc - sk_unused_reserved_mem(sk);
+	if (reclaimable > reclaim_threshold) {
+		reclaimable -= reclaim_threshold;
+		__sk_mem_reclaim(sk, reclaimable);
+	}
 }
 
 /*

# Patch0164: 0164-KVM-VMX-make-vmx-init-a-late-init-call-to-get-to-ini.patch

From 4e6585f34be8b87fe5258233aaa8c002ab561897 Mon Sep 17 00:00:00 2001
From: Colin Ian King <colin.i.king@intel.com>
Date: Tue, 10 Oct 2023 12:41:00 +0100
Subject: [PATCH] KVM: VMX: make vmx_init a late init call to get to init process faster

Making vmx_init a late initcall improves QEMU kernel boot times to
get to the init process. Average of 100 boots, QEMU boot average
reduced from 0.776 seconds to 0.622 seconds (~19.8% faster) on
Alderlake i9-12900 and ~0.5% faster for non-QEMU UEFI boots.

Signed-off-by: Colin Ian King <colin.i.king@intel.com>
---
 arch/x86/kvm/vmx/vmx.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index bc6f0fea48b4..e671fbe70d5a 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -8690,4 +8690,4 @@ static int __init vmx_init(void)
 	kvm_x86_vendor_exit();
 	return r;
 }
-module_init(vmx_init);
+late_initcall(vmx_init);
-- 
2.42.0

# Patch0165: slack.patch

--- linux-6.5.1/init/init_task.c~	2023-09-02 07:13:30.000000000 +0000
+++ linux-6.5.1/init/init_task.c	2023-10-30 15:12:13.920976572 +0000
@@ -140,7 +140,7 @@
 	.journal_info	= NULL,
 	INIT_CPU_TIMERS(init_task)
 	.pi_lock	= __RAW_SPIN_LOCK_UNLOCKED(init_task.pi_lock),
-	.timer_slack_ns = 50000, /* 50 usec default slack */
+	.timer_slack_ns = 50, /* 50 nsec default slack */
 	.thread_pid	= &init_struct_pid,
 	.thread_node	= LIST_HEAD_INIT(init_signals.thread_head),
 #ifdef CONFIG_AUDIT

# Patch0167: 0167-net-sock-increase-default-number-of-_SK_MEM_PACKETS-.patch

From 4ba5a01513a6b3487613e7186cac4f3f2f4c5091 Mon Sep 17 00:00:00 2001
From: Colin Ian King <colin.i.king@intel.com>
Date: Wed, 24 Apr 2024 16:45:47 +0100
Subject: [PATCH] net: sock: increase default number of _SK_MEM_PACKETS to 1024

scale these by a factor of 4 to improve socket performance

Signed-off-by: Colin Ian King <colin.i.king@intel.com>
---
 include/net/sock.h | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/include/net/sock.h b/include/net/sock.h
index 54ca8dcbfb43..9adc51e8085b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2853,7 +2853,7 @@ void sk_get_meminfo(const struct sock *sk, u32 *meminfo);
  * platforms.  This makes socket queueing behavior and performance
  * not depend upon such differences.
  */
-#define _SK_MEM_PACKETS		256
+#define _SK_MEM_PACKETS		1024
 #define _SK_MEM_OVERHEAD	SKB_TRUESIZE(256)
 #define SK_WMEM_MAX		(_SK_MEM_OVERHEAD * _SK_MEM_PACKETS)
 #define SK_RMEM_MAX		(_SK_MEM_OVERHEAD * _SK_MEM_PACKETS)
-- 
2.44.0
