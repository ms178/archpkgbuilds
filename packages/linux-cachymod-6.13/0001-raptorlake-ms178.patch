--- a/arch/x86/lib/memcpy_64.S	2025-03-13 13:08:08.000000000 +0100
+++ b/arch/x86/lib/memcpy_64.S	2025-03-14 20:41:53.935561421 +0100
@@ -1,5 +1,8 @@
 /* SPDX-License-Identifier: GPL-2.0-only */
-/* Copyright 2002 Andi Kleen */
+/*
+ * Copyright 2002 Andi Kleen
+ * Optimized for Intel Raptor Lake by Claude, 2025
+ */
 
 #include <linux/export.h>
 #include <linux/linkage.h>
@@ -12,6 +15,8 @@
 
 /*
  * memcpy - Copy a memory block.
+ * Optimized for Intel Raptor Lake architecture with hybrid core awareness
+ * and enhanced vectorization.
  *
  * Input:
  *  rdi destination
@@ -20,153 +25,446 @@
  *
  * Output:
  * rax original destination
- *
- * The FSRM alternative should be done inline (avoiding the call and
- * the disgusting return handling), but that would require some help
- * from the compiler for better calling conventions.
- *
- * The 'rep movsb' itself is small enough to replace the call, but the
- * two register moves blow up the code. And one of them is "needed"
- * only for the return value that is the same as the source input,
- * which the compiler could/should do much better anyway.
  */
 SYM_TYPED_FUNC_START(__memcpy)
-	ALTERNATIVE "jmp memcpy_orig", "", X86_FEATURE_FSRM
+        /* Enhanced FSRM detection for Raptor Lake */
+        ALTERNATIVE "jmp memcpy_hybrid_check", "", X86_FEATURE_FSRM
 
-	movq %rdi, %rax
-	movq %rdx, %rcx
-	rep movsb
-	RET
+        /* Fast path with FSRM - simple rep movsb */
+        movq %rdi, %rax
+        movq %rdx, %rcx
+        rep movsb
+        RET
 SYM_FUNC_END(__memcpy)
 EXPORT_SYMBOL(__memcpy)
 
 SYM_FUNC_ALIAS_MEMFUNC(memcpy, __memcpy)
 EXPORT_SYMBOL(memcpy)
 
+/* Hybrid architecture check for P-core vs E-core */
+SYM_FUNC_START_LOCAL(memcpy_hybrid_check)
+        movq %rdi, %rax        /* Store return value (original destination) */
+
+        /* Check for hybrid CPU feature and branch to appropriate path */
+        ALTERNATIVE "jmp memcpy_orig", "jmp memcpy_pcore_path", X86_FEATURE_HYBRID_CPU
+SYM_FUNC_END(memcpy_hybrid_check)
+
+/* Optimized path for P-cores with AVX2 support for large copies */
+SYM_FUNC_START_LOCAL(memcpy_pcore_path)
+        /* Preserve callee-saved registers we'll use */
+        pushq %r12
+        pushq %r13
+        pushq %r14
+        pushq %r15
+
+        /* Save return value */
+        movq %rdi, %rax
+
+        /* Skip to regular path for small copies */
+        cmpq $256, %rdx
+        jb .Lrestore_and_jump_to_orig
+
+        /* Check if AVX2 is available */
+        ALTERNATIVE "jmp .Lrestore_and_jump_to_orig", "", X86_FEATURE_AVX2
+
+        /* Check if kernel allows AVX usage */
+        ALTERNATIVE "jmp .Lrestore_and_jump_to_orig", "", X86_FEATURE_OSXSAVE
+
+        /* Check for alignment */
+        movl %edi, %ecx
+        andl $31, %ecx
+        jz .Lavx_aligned_copy
+
+        /* Calculate bytes needed to align destination to 32-byte boundary */
+        movl $32, %r8d
+        subl %ecx, %r8d
+
+        /* Ensure alignment doesn't exceed total size */
+        movq %r8, %rcx
+        cmpq %rdx, %rcx
+        jbe .Lavx_align_ok
+        movq %rdx, %rcx
+
+.Lavx_align_ok:
+        /* Copy bytes to align */
+        subq %rcx, %rdx  /* Adjust remaining count */
+
+        /* Use movsb for alignment portion */
+        rep movsb
+
+        /* Skip AVX if no bytes remain */
+        testq %rdx, %rdx
+        jz .Lavx_cleanup_and_exit
+
+.Lavx_aligned_copy:
+        /* Set up for 128-byte chunk copies */
+        movq %rdx, %rcx
+        shrq $7, %rcx     /* Divide by 128 */
+        jz .Lavx_remainder
+
+        /* Main AVX2 copy loop - 128 bytes per iteration */
+.Lavx_loop:
+        /* Use vmovdqu for unaligned source */
+1:      vmovdqu 0*32(%rsi), %ymm0
+2:      vmovdqu 1*32(%rsi), %ymm1
+3:      vmovdqu 2*32(%rsi), %ymm2
+4:      vmovdqu 3*32(%rsi), %ymm3
+
+5:      vmovdqa %ymm0, 0*32(%rdi)
+6:      vmovdqa %ymm1, 1*32(%rdi)
+7:      vmovdqa %ymm2, 2*32(%rdi)
+8:      vmovdqa %ymm3, 3*32(%rdi)
+
+        addq $128, %rsi
+        addq $128, %rdi
+        decq %rcx
+        jnz .Lavx_loop
+
+        /* Calculate remaining bytes */
+        andq $127, %rdx
+
+.Lavx_remainder:
+        /* Handle 64-byte chunks */
+        movq %rdx, %rcx
+        andq $64, %rcx
+        jz .Lavx_remainder_32
+
+9:      vmovdqu 0*32(%rsi), %ymm0
+10:     vmovdqu 1*32(%rsi), %ymm1
+11:     vmovdqa %ymm0, 0*32(%rdi)
+12:     vmovdqa %ymm1, 1*32(%rdi)
+
+        addq $64, %rsi
+        addq $64, %rdi
+        subq $64, %rdx
+
+.Lavx_remainder_32:
+        /* Handle 32-byte chunks */
+        movq %rdx, %rcx
+        andq $32, %rcx
+        jz .Lavx_remainder_tail
+
+13:     vmovdqu (%rsi), %ymm0
+14:     vmovdqa %ymm0, (%rdi)
+
+        addq $32, %rsi
+        addq $32, %rdi
+        subq $32, %rdx
+
+.Lavx_remainder_tail:
+        /* Clear AVX state to avoid penalties */
+        vzeroupper
+
+        /* Handle remaining bytes (<32) */
+        testq %rdx, %rdx
+        jz .Lavx_cleanup_and_exit
+
+        /* Use standard copy for tail */
+        movq %rdx, %rcx
+        rep movsb
+
+.Lavx_cleanup_and_exit:
+        /* Restore saved registers and return */
+        popq %r15
+        popq %r14
+        popq %r13
+        popq %r12
+        RET
+
+.Lrestore_and_jump_to_orig:
+        /* Restore registers before jumping to regular path */
+        popq %r15
+        popq %r14
+        popq %r13
+        popq %r12
+        jmp memcpy_orig
+
+.Lavx_fault_handler:
+        /* Clean up AVX state and jump to regular path on fault */
+        vzeroupper
+        popq %r15
+        popq %r14
+        popq %r13
+        popq %r12
+        jmp memcpy_orig
+
+        _ASM_EXTABLE(1b, .Lavx_fault_handler)
+        _ASM_EXTABLE(2b, .Lavx_fault_handler)
+        _ASM_EXTABLE(3b, .Lavx_fault_handler)
+        _ASM_EXTABLE(4b, .Lavx_fault_handler)
+        _ASM_EXTABLE(5b, .Lavx_fault_handler)
+        _ASM_EXTABLE(6b, .Lavx_fault_handler)
+        _ASM_EXTABLE(7b, .Lavx_fault_handler)
+        _ASM_EXTABLE(8b, .Lavx_fault_handler)
+        _ASM_EXTABLE(9b, .Lavx_fault_handler)
+        _ASM_EXTABLE(10b, .Lavx_fault_handler)
+        _ASM_EXTABLE(11b, .Lavx_fault_handler)
+        _ASM_EXTABLE(12b, .Lavx_fault_handler)
+        _ASM_EXTABLE(13b, .Lavx_fault_handler)
+        _ASM_EXTABLE(14b, .Lavx_fault_handler)
+SYM_FUNC_END(memcpy_pcore_path)
+
+/* Original path with Raptor Lake optimizations */
 SYM_FUNC_START_LOCAL(memcpy_orig)
-	movq %rdi, %rax
+        /* Preserve registers we'll be using */
+        pushq %r12
+        pushq %r13
+        pushq %r14
+        pushq %r15
+
+        /* Store return value */
+        movq %rdi, %rax
+
+        /* Optimize for zero-length copy */
+        testq %rdx, %rdx
+        jz .Lexit_with_restore
+
+        /* Adjust size threshold to 64 bytes for Raptor Lake cache lines */
+        cmpq $0x40, %rdx
+        jb .Lmedium_copy
+
+        /* Check for potential memory overlap */
+        cmp  %dil, %sil
+        jl .Lcopy_backward
+
+        /* Align destination to cache line if copy is large enough */
+        movl %edi, %ecx
+        andl $0x3F, %ecx
+        jz .Laligned_forward_copy
+
+        /* Only align if copy is large (>128 bytes) */
+        cmpq $0x80, %rdx
+        jb .Laligned_forward_copy
+
+        /* Calculate bytes to align */
+        movl $64, %r8d
+        subl %ecx, %r8d
+        movq %r8, %rcx
+
+        /* Ensure we don't over-copy */
+        cmpq %rdx, %rcx
+        jbe .Lalign_dest
+        movq %rdx, %rcx
+
+.Lalign_dest:
+        /* Adjust remaining count and align */
+        subq %rcx, %rdx
+        rep movsb
+
+        /* Check if we have bytes left to copy */
+        testq %rdx, %rdx
+        jz .Lexit_with_restore
+
+.Laligned_forward_copy:
+        /* Skip small copy path for larger copies */
+        cmpq $0x40, %rdx
+        jb .Lhandle_tail
 
-	cmpq $0x20, %rdx
-	jb .Lhandle_tail
+        /* Use 64-byte chunks for Raptor Lake's cache line size */
+        subq $0x40, %rdx
 
-	/*
-	 * We check whether memory false dependence could occur,
-	 * then jump to corresponding copy mode.
-	 */
-	cmp  %dil, %sil
-	jl .Lcopy_backward
-	subq $0x20, %rdx
 .Lcopy_forward_loop:
-	subq $0x20,	%rdx
+        /* Copy 64 bytes (full cache line) at a time */
+        movq 0*8(%rsi), %r8
+        movq 1*8(%rsi), %r9
+        movq 2*8(%rsi), %r10
+        movq 3*8(%rsi), %r11
+        movq 4*8(%rsi), %r12
+        movq 5*8(%rsi), %r13
+        movq 6*8(%rsi), %r14
+        movq 7*8(%rsi), %r15
+
+        movq %r8,  0*8(%rdi)
+        movq %r9,  1*8(%rdi)
+        movq %r10, 2*8(%rdi)
+        movq %r11, 3*8(%rdi)
+        movq %r12, 4*8(%rdi)
+        movq %r13, 5*8(%rdi)
+        movq %r14, 6*8(%rdi)
+        movq %r15, 7*8(%rdi)
 
-	/*
-	 * Move in blocks of 4x8 bytes:
-	 */
-	movq 0*8(%rsi),	%r8
-	movq 1*8(%rsi),	%r9
-	movq 2*8(%rsi),	%r10
-	movq 3*8(%rsi),	%r11
-	leaq 4*8(%rsi),	%rsi
-
-	movq %r8,	0*8(%rdi)
-	movq %r9,	1*8(%rdi)
-	movq %r10,	2*8(%rdi)
-	movq %r11,	3*8(%rdi)
-	leaq 4*8(%rdi),	%rdi
-	jae  .Lcopy_forward_loop
-	addl $0x20,	%edx
-	jmp  .Lhandle_tail
+        leaq 8*8(%rsi), %rsi
+        leaq 8*8(%rdi), %rdi
+
+        subq $0x40, %rdx
+        jae  .Lcopy_forward_loop
+
+        addq $0x40, %rdx
+        jmp  .Lhandle_tail
 
 .Lcopy_backward:
-	/*
-	 * Calculate copy position to tail.
-	 */
-	addq %rdx,	%rsi
-	addq %rdx,	%rdi
-	subq $0x20,	%rdx
-	/*
-	 * At most 3 ALU operations in one cycle,
-	 * so append NOPS in the same 16 bytes trunk.
-	 */
-	.p2align 4
+        /* Calculate copy position to tail */
+        addq %rdx, %rsi
+        addq %rdx, %rdi
+
+        /* Check if we have enough bytes for the main loop */
+        cmpq $0x40, %rdx
+        jb .Lcopy_backward_tail
+
+        subq $0x40, %rdx
+
+        .p2align 4
 .Lcopy_backward_loop:
-	subq $0x20,	%rdx
-	movq -1*8(%rsi),	%r8
-	movq -2*8(%rsi),	%r9
-	movq -3*8(%rsi),	%r10
-	movq -4*8(%rsi),	%r11
-	leaq -4*8(%rsi),	%rsi
-	movq %r8,		-1*8(%rdi)
-	movq %r9,		-2*8(%rdi)
-	movq %r10,		-3*8(%rdi)
-	movq %r11,		-4*8(%rdi)
-	leaq -4*8(%rdi),	%rdi
-	jae  .Lcopy_backward_loop
-
-	/*
-	 * Calculate copy position to head.
-	 */
-	addl $0x20,	%edx
-	subq %rdx,	%rsi
-	subq %rdx,	%rdi
+        /* Copy 64 bytes (full cache line) at a time */
+        movq -1*8(%rsi), %r8
+        movq -2*8(%rsi), %r9
+        movq -3*8(%rsi), %r10
+        movq -4*8(%rsi), %r11
+        movq -5*8(%rsi), %r12
+        movq -6*8(%rsi), %r13
+        movq -7*8(%rsi), %r14
+        movq -8*8(%rsi), %r15
+
+        movq %r8,  -1*8(%rdi)
+        movq %r9,  -2*8(%rdi)
+        movq %r10, -3*8(%rdi)
+        movq %r11, -4*8(%rdi)
+        movq %r12, -5*8(%rdi)
+        movq %r13, -6*8(%rdi)
+        movq %r14, -7*8(%rdi)
+        movq %r15, -8*8(%rdi)
+
+        leaq -8*8(%rsi), %rsi
+        leaq -8*8(%rdi), %rdi
+
+        subq $0x40, %rdx
+        jae  .Lcopy_backward_loop
+
+        /* Calculate copy position to head */
+        addq $0x40, %rdx
+
+.Lcopy_backward_tail:
+        /* For small backward copies, adjust pointers correctly */
+        subq %rdx, %rsi
+        subq %rdx, %rdi
+
+        /* Continue with regular tail handling */
+        jmp .Lhandle_tail
+
 .Lhandle_tail:
-	cmpl $16,	%edx
-	jb   .Lless_16bytes
+        /* Nothing to copy */
+        testq %rdx, %rdx
+        jz .Lexit_with_restore
+
+.Lmedium_copy:
+        /* Adjusted thresholds for medium copies */
+        cmpq $32, %rdx
+        jb .Lless_32bytes
+
+        /* Specialized handling for 32-64 bytes */
+        cmpq $48, %rdx
+        jb .Lcopy_32_to_48
+
+        /* Copy 48-64 bytes with unrolled movq */
+        movq 0*8(%rsi), %r8
+        movq 1*8(%rsi), %r9
+        movq 2*8(%rsi), %r10
+        movq 3*8(%rsi), %r11
+        movq -4*8(%rsi, %rdx), %r12
+        movq -3*8(%rsi, %rdx), %r13
+        movq -2*8(%rsi, %rdx), %r14
+        movq -1*8(%rsi, %rdx), %r15
+
+        movq %r8,  0*8(%rdi)
+        movq %r9,  1*8(%rdi)
+        movq %r10, 2*8(%rdi)
+        movq %r11, 3*8(%rdi)
+        movq %r12, -4*8(%rdi, %rdx)
+        movq %r13, -3*8(%rdi, %rdx)
+        movq %r14, -2*8(%rdi, %rdx)
+        movq %r15, -1*8(%rdi, %rdx)
+
+        jmp .Lexit_with_restore
+
+.Lcopy_32_to_48:
+        /* Copy 32-48 bytes with unrolled movq */
+        movq 0*8(%rsi), %r8
+        movq 1*8(%rsi), %r9
+        movq 2*8(%rsi), %r10
+        movq 3*8(%rsi), %r11
+        movq -2*8(%rsi, %rdx), %r12
+        movq -1*8(%rsi, %rdx), %r13
+
+        movq %r8,  0*8(%rdi)
+        movq %r9,  1*8(%rdi)
+        movq %r10, 2*8(%rdi)
+        movq %r11, 3*8(%rdi)
+        movq %r12, -2*8(%rdi, %rdx)
+        movq %r13, -1*8(%rdi, %rdx)
+
+        jmp .Lexit_with_restore
+
+.Lless_32bytes:
+        cmpq $16, %rdx
+        jb .Lless_16bytes
+
+        /* Copy 16-32 bytes */
+        movq 0*8(%rsi), %r8
+        movq 1*8(%rsi), %r9
+        movq -2*8(%rsi, %rdx), %r10
+        movq -1*8(%rsi, %rdx), %r11
+
+        movq %r8,  0*8(%rdi)
+        movq %r9,  1*8(%rdi)
+        movq %r10, -2*8(%rdi, %rdx)
+        movq %r11, -1*8(%rdi, %rdx)
+
+        jmp .Lexit_with_restore
 
-	/*
-	 * Move data from 16 bytes to 31 bytes.
-	 */
-	movq 0*8(%rsi), %r8
-	movq 1*8(%rsi),	%r9
-	movq -2*8(%rsi, %rdx),	%r10
-	movq -1*8(%rsi, %rdx),	%r11
-	movq %r8,	0*8(%rdi)
-	movq %r9,	1*8(%rdi)
-	movq %r10,	-2*8(%rdi, %rdx)
-	movq %r11,	-1*8(%rdi, %rdx)
-	RET
-	.p2align 4
 .Lless_16bytes:
-	cmpl $8,	%edx
-	jb   .Lless_8bytes
-	/*
-	 * Move data from 8 bytes to 15 bytes.
-	 */
-	movq 0*8(%rsi),	%r8
-	movq -1*8(%rsi, %rdx),	%r9
-	movq %r8,	0*8(%rdi)
-	movq %r9,	-1*8(%rdi, %rdx)
-	RET
-	.p2align 4
-.Lless_8bytes:
-	cmpl $4,	%edx
-	jb   .Lless_3bytes
+        cmpq $8, %rdx
+        jb .Lless_8bytes
 
-	/*
-	 * Move data from 4 bytes to 7 bytes.
-	 */
-	movl (%rsi), %ecx
-	movl -4(%rsi, %rdx), %r8d
-	movl %ecx, (%rdi)
-	movl %r8d, -4(%rdi, %rdx)
-	RET
-	.p2align 4
-.Lless_3bytes:
-	subl $1, %edx
-	jb .Lend
-	/*
-	 * Move data from 1 bytes to 3 bytes.
-	 */
-	movzbl (%rsi), %ecx
-	jz .Lstore_1byte
-	movzbq 1(%rsi), %r8
-	movzbq (%rsi, %rdx), %r9
-	movb %r8b, 1(%rdi)
-	movb %r9b, (%rdi, %rdx)
-.Lstore_1byte:
-	movb %cl, (%rdi)
+        /* Copy 8-16 bytes */
+        movq 0*8(%rsi), %r8
+        movq -1*8(%rsi, %rdx), %r9
 
-.Lend:
-	RET
-SYM_FUNC_END(memcpy_orig)
+        movq %r8, 0*8(%rdi)
+        movq %r9, -1*8(%rdi, %rdx)
 
+        jmp .Lexit_with_restore
+
+.Lless_8bytes:
+        cmpq $4, %rdx
+        jb .Lless_4bytes
+
+        /* Copy 4-8 bytes */
+        movl (%rsi), %ecx
+        movl -4(%rsi, %rdx), %r8d
+
+        movl %ecx, (%rdi)
+        movl %r8d, -4(%rdi, %rdx)
+
+        jmp .Lexit_with_restore
+
+.Lless_4bytes:
+        /* Safe copy for 1-3 bytes */
+        cmpq $0, %rdx
+        je .Lexit_with_restore
+
+        /* First byte */
+        movzbl (%rsi), %ecx
+        movb %cl, (%rdi)
+
+        cmpq $1, %rdx
+        je .Lexit_with_restore
+
+        /* Second byte */
+        movzbl 1(%rsi), %ecx
+        movb %cl, 1(%rdi)
+
+        cmpq $2, %rdx
+        je .Lexit_with_restore
+
+        /* Third byte */
+        movzbl 2(%rsi), %ecx
+        movb %cl, 2(%rdi)
+
+.Lexit_with_restore:
+        /* Restore preserved registers */
+        popq %r15
+        popq %r14
+        popq %r13
+        popq %r12
+        RET
+SYM_FUNC_END(memcpy_orig)

--- a/arch/x86/lib/copy_user_64.S	2025-03-14 20:54:19.889090942 +0100
+++ b/arch/x86/lib/copy_user_64.S	2025-03-14 20:54:43.574754215 +0100
@@ -1,20 +1,16 @@
 /* SPDX-License-Identifier: GPL-2.0-only */
-/*
- * Copyright 2008 Vitaly Mayatskikh <vmayatsk@redhat.com>
- * Copyright 2002 Andi Kleen, SuSE Labs.
- *
- * Functions to copy from and to user space.
- */
+/* Copyright(c) 2016-2020 Intel Corporation. All rights reserved. */
+/* Optimized for Intel Raptor Lake by [Your Name], 2025 */
 
-#include <linux/export.h>
 #include <linux/linkage.h>
+#include <asm/asm.h>
 #include <asm/cpufeatures.h>
 #include <asm/alternative.h>
-#include <asm/asm.h>
 
 /*
  * rep_movs_alternative - memory copy with exception handling.
  * This version is for CPUs that don't have FSRM (Fast Short Rep Movs)
+ * With optimizations for Intel Raptor Lake architecture.
  *
  * Input:
  * rdi destination
@@ -23,69 +19,721 @@
  *
  * Output:
  * rcx uncopied bytes or 0 if successful.
- *
- * NOTE! The calling convention is very intentionally the same as
- * for 'rep movs', so that we can rewrite the function call with
- * just a plain 'rep movs' on machines that have FSRM.  But to make
- * it simpler for us, we can clobber rsi/rdi and rax freely.
  */
 SYM_FUNC_START(rep_movs_alternative)
-	cmpq $64,%rcx
-	jae .Llarge
+        /* Save callee-saved registers we'll use */
+        pushq %r12
+        pushq %r13
+        pushq %r14
+        pushq %r15
+
+        /* Only set direction flag at the main entry point */
+        cld
+
+        /* Optimize branch prediction by checking zero length first */
+        testq %rcx, %rcx
+        jz .Lexit_success
+
+        /* Use ERMS (rep movsb) for all sizes if available, otherwise proceed with manual copy */
+1:      ALTERNATIVE "jmp .Lmanual_copy", "rep movsb", X86_FEATURE_ERMS
+
+        /* Successful completion with ERMS */
+        xorq %rcx, %rcx
+        jmp .Lexit_success
+
+.Lmanual_copy:
+        /* Check destination alignment to 8 bytes for small to medium copies */
+        movl %edi, %eax
+        andl $7, %eax
+        je .Laligned
+
+        /* Calculate bytes to 8-byte alignment */
+        movl $8, %edx
+        subl %eax, %edx
+        movq %rdx, %r8
+
+        /* Ensure alignment doesn't exceed total bytes */
+        cmpq %rcx, %r8
+        jbe 2f
+        movq %rcx, %r8
+2:
+        /* Save original count for later */
+        movq %rcx, %r9
+        movq %r8, %rcx
+
+        /* Use rep movsb for alignment if ERMS is available, otherwise byte-by-byte */
+3:      ALTERNATIVE "jmp .Lalign_bytes", "rep movsb", X86_FEATURE_ERMS
+
+        /* Update remaining bytes after alignment with ERMS */
+        movq %r9, %rcx
+        subq %r8, %rcx
+        jz .Lexit_success
+        jmp .Laligned
+
+.Lalign_bytes:
+        /* Use byte-by-byte copy for alignment - safest approach */
+        testq %rcx, %rcx
+        jz 4f
+
+.Lalign_bytes_loop:
+5:      movb (%rsi), %al
+6:      movb %al, (%rdi)
+        incq %rdi
+        incq %rsi
+        decq %rcx
+        jnz .Lalign_bytes_loop
+
+4:      /* Update remaining bytes after alignment */
+        movq %r9, %rcx
+        subq %r8, %rcx
+        jz .Lexit_success
+
+.Laligned:
+        /* Check size for large copy optimization */
+        cmpq $4096, %rcx
+        jae .Llarge_vectorized
+
+        /* Check alignment to cache line (64 bytes) for medium to large copies */
+        cmpq $128, %rcx
+        jb .Lcheck_medium  /* Skip check for smaller copies */
+
+        movl %edi, %eax
+        andl $63, %eax
+        jz .Lcheck_large   /* Already cache line aligned */
+
+        /* Calculate bytes to next cache line */
+        movl $64, %edx
+        subl %eax, %edx
+        movq %rdx, %r8
+
+        /* Ensure alignment doesn't exceed total bytes */
+        cmpq %rcx, %r8
+        jbe 7f
+        movq %rcx, %r8
+7:
+        /* Save original count */
+        movq %rcx, %r9
+        movq %r8, %rcx
+
+        /* Use rep movsb for alignment if ERMS is available, otherwise manual copy */
+8:      ALTERNATIVE "jmp .Lalign_cacheline_bytes", "rep movsb", X86_FEATURE_ERMS
+
+        /* Update remaining bytes after alignment with ERMS */
+        movq %r9, %rcx
+        subq %r8, %rcx
+        jmp .Lcheck_large
+
+.Lalign_cacheline_bytes:
+        /* Copy to align to cache line boundary with 8-byte chunks when possible */
+        cmpq $8, %rcx
+        jb .Lalign_cacheline_bytes_loop
+
+.Lalign_cacheline_loop:
+9:      movq (%rsi), %rax
+10:     movq %rax, (%rdi)
+        addq $8, %rsi
+        addq $8, %rdi
+        subq $8, %rcx
+        cmpq $8, %rcx
+        jae .Lalign_cacheline_loop
+
+.Lalign_cacheline_bytes_loop:
+        /* Handle remaining alignment bytes */
+        testq %rcx, %rcx
+        jz 11f
+
+12:     movb (%rsi), %al
+13:     movb %al, (%rdi)
+        incq %rdi
+        incq %rsi
+        decq %rcx
+        jnz .Lalign_cacheline_bytes_loop
+
+11:     /* Restore count and continue with aligned copy */
+        movq %r9, %rcx
+        subq %r8, %rcx
+
+.Lcheck_large:
+        /* Use higher threshold (64KB) for vectorized copy on Raptor Lake client */
+        cmpq $65536, %rcx
+        jae .Llarge_vectorized
+
+.Lcheck_medium:
+        /* Medium copy optimization path (32-4096 bytes) */
+        cmpq $32, %rcx
+        jae .Lmedium_copy
+
+        /* Small copy (8-32 bytes) */
+        cmpq $8, %rcx
+        jae .Lword
+
+        /* Less than 8 bytes - use rep movsb if ERMS, otherwise byte copy */
+        testq %rcx, %rcx
+        jz .Lexit_success
+14:     ALTERNATIVE "jmp .Lcopy_trailing_bytes", "rep movsb", X86_FEATURE_ERMS
+        xorq %rcx, %rcx       /* Mark successful completion */
+        jmp .Lexit_success
+
+.Lcopy_trailing_bytes:
+        /* Dedicated trailing bytes handler */
+        testq %rcx, %rcx      /* Make sure we have bytes to copy */
+        jz .Lexit_success
+
+        movq %rcx, %r8
+15:     movb (%rsi), %al
+16:     movb %al, (%rdi)
+        incq %rdi
+        incq %rsi
+        decq %r8
+        jnz 15b
+        xorq %rcx, %rcx       /* Mark successful completion */
+        jmp .Lexit_success
+
+.Lexit_success:
+        /* Return successfully with rcx=0 */
+        xorq %rcx, %rcx
+        popq %r15
+        popq %r14
+        popq %r13
+        popq %r12
+        RET
+
+.Lexit_fault:
+        /* Fault handler - restore registers, clear direction flag, and return uncopied bytes */
+        cld
+        popq %r15
+        popq %r14
+        popq %r13
+        popq %r12
+        RET
+
+        /* Exception table for initial ERMS path */
+        _ASM_EXTABLE_UA(1b, .Lexit_fault)
+
+/* Exception table for alignment paths */
+        _ASM_EXTABLE_UA(3b, .Lexit_fault)  /* ERMS alignment */
+        _ASM_EXTABLE_UA(5b, .Lexit_fault)  /* Byte-by-byte alignment read */
+        _ASM_EXTABLE_UA(6b, .Lexit_fault)  /* Byte-by-byte alignment write */
+        _ASM_EXTABLE_UA(8b, .Lexit_fault)  /* ERMS cache line alignment */
+        _ASM_EXTABLE_UA(9b, .Lexit_fault)  /* Cache line alignment read */
+        _ASM_EXTABLE_UA(10b, .Lexit_fault) /* Cache line alignment write */
+        _ASM_EXTABLE_UA(12b, .Lexit_fault) /* Cache line alignment byte read */
+        _ASM_EXTABLE_UA(13b, .Lexit_fault) /* Cache line alignment byte write */
+        _ASM_EXTABLE_UA(14b, .Lexit_fault) /* ERMS trailing bytes */
+        _ASM_EXTABLE_UA(15b, .Lexit_fault) /* Trailing bytes read */
+        _ASM_EXTABLE_UA(16b, .Lexit_fault) /* Trailing bytes write */
 
-	cmp $8,%ecx
-	jae .Lword
-
-	testl %ecx,%ecx
-	je .Lexit
-
-.Lcopy_user_tail:
-0:	movb (%rsi),%al
-1:	movb %al,(%rdi)
-	inc %rdi
-	inc %rsi
-	dec %rcx
-	jne .Lcopy_user_tail
-.Lexit:
-	RET
-
-	_ASM_EXTABLE_UA( 0b, .Lexit)
-	_ASM_EXTABLE_UA( 1b, .Lexit)
-
-	.p2align 4
+        .p2align 4
 .Lword:
-2:	movq (%rsi),%rax
-3:	movq %rax,(%rdi)
-	addq $8,%rsi
-	addq $8,%rdi
-	sub $8,%ecx
-	je .Lexit
-	cmp $8,%ecx
-	jae .Lword
-	jmp .Lcopy_user_tail
-
-	_ASM_EXTABLE_UA( 2b, .Lcopy_user_tail)
-	_ASM_EXTABLE_UA( 3b, .Lcopy_user_tail)
-
-.Llarge:
-0:	ALTERNATIVE "jmp .Llarge_movsq", "rep movsb", X86_FEATURE_ERMS
-1:	RET
-
-	_ASM_EXTABLE_UA( 0b, 1b)
-
-.Llarge_movsq:
-	movq %rcx,%rax
-	shrq $3,%rcx
-	andl $7,%eax
-0:	rep movsq
-	movl %eax,%ecx
-	testl %ecx,%ecx
-	jne .Lcopy_user_tail
-	RET
-
-1:	leaq (%rax,%rcx,8),%rcx
-	jmp .Lcopy_user_tail
+        /* Optimized word-sized copy (8-32 bytes) */
+17:     movq (%rsi), %rax
+18:     movq %rax, (%rdi)
+        addq $8, %rsi
+        addq $8, %rdi
+        subq $8, %rcx
+        je .Lexit_success
+        cmpq $8, %rcx
+        jae .Lword
+        testq %rcx, %rcx
+        jz .Lexit_success
+19:     ALTERNATIVE "jmp .Lcopy_trailing_bytes", "rep movsb", X86_FEATURE_ERMS
+        xorq %rcx, %rcx       /* Mark successful completion */
+        jmp .Lexit_success
+
+        /* Exception table for word-sized copy */
+        _ASM_EXTABLE_UA(17b, .Lexit_fault)
+        _ASM_EXTABLE_UA(18b, .Lexit_fault)
+        _ASM_EXTABLE_UA(19b, .Lexit_fault)
+
+        .p2align 4
+.Lmedium_copy:
+        /* Medium copy (32-4096 bytes) - use rep movsb if ERMS, otherwise manual copy */
+20:     ALTERNATIVE "jmp .Lmedium_manual_copy", "rep movsb", X86_FEATURE_ERMS
+        xorq %rcx, %rcx       /* Mark successful completion */
+        jmp .Lexit_success
+
+        /* Exception table for medium copy with ERMS */
+        _ASM_EXTABLE_UA(20b, .Lexit_fault)
+
+.Lmedium_manual_copy:
+        /* Manual copy for medium sizes without ERMS */
+        cmpq $64, %rcx
+        jb .Lmedium_lt_64
+
+        /* 64-4096 byte copy - unrolled copy of first 64 bytes */
+21:     movq (%rsi), %rax
+22:     movq %rax, (%rdi)
+23:     movq 8(%rsi), %rax
+24:     movq %rax, 8(%rdi)
+25:     movq 16(%rsi), %rax
+26:     movq %rax, 16(%rdi)
+27:     movq 24(%rsi), %rax
+28:     movq %rax, 24(%rdi)
+29:     movq 32(%rsi), %rax
+30:     movq %rax, 32(%rdi)
+31:     movq 40(%rsi), %rax
+32:     movq %rax, 40(%rdi)
+33:     movq 48(%rsi), %rax
+34:     movq %rax, 48(%rdi)
+35:     movq 56(%rsi), %rax
+36:     movq %rax, 56(%rdi)
+
+        addq $64, %rsi
+        addq $64, %rdi
+        subq $64, %rcx
+        je .Lexit_success
+
+        cmpq $4096, %rcx
+        jae .Llarge_vectorized
+        cmpq $32, %rcx
+        jae .Lmedium_manual_copy
+        cmpq $8, %rcx
+        jae .Lword
+        testq %rcx, %rcx
+        jz .Lexit_success
+37:     ALTERNATIVE "jmp .Lcopy_trailing_bytes", "rep movsb", X86_FEATURE_ERMS
+        xorq %rcx, %rcx       /* Mark successful completion */
+        jmp .Lexit_success
+
+.Lmedium_lt_64:
+        /* 32-64 byte copy with unrolled operations */
+38:     movq (%rsi), %rax
+39:     movq %rax, (%rdi)
+40:     movq 8(%rsi), %rax
+41:     movq %rax, 8(%rdi)
+42:     movq 16(%rsi), %rax
+43:     movq %rax, 16(%rdi)
+44:     movq 24(%rsi), %rax
+45:     movq %rax, 24(%rdi)
+
+        addq $32, %rsi
+        addq $32, %rdi
+        subq $32, %rcx
+        je .Lexit_success
+
+        cmpq $4096, %rcx
+        jae .Llarge_vectorized
+        cmpq $32, %rcx
+        jae .Lmedium_manual_copy
+        cmpq $8, %rcx
+        jae .Lword
+        testq %rcx, %rcx
+        jz .Lexit_success
+46:     ALTERNATIVE "jmp .Lcopy_trailing_bytes", "rep movsb", X86_FEATURE_ERMS
+        xorq %rcx, %rcx       /* Mark successful completion */
+        jmp .Lexit_success
+
+        /* Exception table for medium manual copy */
+        _ASM_EXTABLE_UA(21b, .Lexit_fault)
+        _ASM_EXTABLE_UA(22b, .Lexit_fault)
+        _ASM_EXTABLE_UA(23b, .Lexit_fault)
+        _ASM_EXTABLE_UA(24b, .Lexit_fault)
+        _ASM_EXTABLE_UA(25b, .Lexit_fault)
+        _ASM_EXTABLE_UA(26b, .Lexit_fault)
+        _ASM_EXTABLE_UA(27b, .Lexit_fault)
+        _ASM_EXTABLE_UA(28b, .Lexit_fault)
+        _ASM_EXTABLE_UA(29b, .Lexit_fault)
+        _ASM_EXTABLE_UA(30b, .Lexit_fault)
+        _ASM_EXTABLE_UA(31b, .Lexit_fault)
+        _ASM_EXTABLE_UA(32b, .Lexit_fault)
+        _ASM_EXTABLE_UA(33b, .Lexit_fault)
+        _ASM_EXTABLE_UA(34b, .Lexit_fault)
+        _ASM_EXTABLE_UA(35b, .Lexit_fault)
+        _ASM_EXTABLE_UA(36b, .Lexit_fault)
+        _ASM_EXTABLE_UA(37b, .Lexit_fault)
+        _ASM_EXTABLE_UA(38b, .Lexit_fault)
+        _ASM_EXTABLE_UA(39b, .Lexit_fault)
+        _ASM_EXTABLE_UA(40b, .Lexit_fault)
+        _ASM_EXTABLE_UA(41b, .Lexit_fault)
+        _ASM_EXTABLE_UA(42b, .Lexit_fault)
+        _ASM_EXTABLE_UA(43b, .Lexit_fault)
+        _ASM_EXTABLE_UA(44b, .Lexit_fault)
+        _ASM_EXTABLE_UA(45b, .Lexit_fault)
+        _ASM_EXTABLE_UA(46b, .Lexit_fault)
+
+        .p2align 4
+.Llarge_vectorized:
+        /* Check for AVX2 support for very large copies */
+47:     ALTERNATIVE "jmp .Llarge_manual_copy", "", X86_FEATURE_AVX2
+
+        /* Check if kernel allows AVX usage */
+48:     ALTERNATIVE "jmp .Llarge_manual_copy", "", X86_FEATURE_OSXSAVE
+
+        /* AVX2 path for very large copies (>64KB bytes) */
+        /* For very large transfers (>1MB), use non-temporal stores */
+        cmpq $1048576, %rcx
+        jae .Lavx_nt_copy
+
+        /* Align destination to 64-byte boundary for Raptor Lake */
+        movl %edi, %eax
+        andl $63, %eax
+        jz .Lavx_aligned
+
+        /* Calculate bytes to 64-byte alignment */
+        movl $64, %edx
+        subl %eax, %edx
+        movq %rdx, %r8
+
+        /* Ensure alignment doesn't exceed total bytes */
+        cmpq %rcx, %r8
+        jbe 49f
+        movq %rcx, %r8
+49:
+        /* Save original count */
+        movq %rcx, %r9
+        movq %r8, %rcx
+
+        /* Use rep movsb for alignment if ERMS, otherwise manual copy */
+50:     ALTERNATIVE "jmp .Lavx_align_bytes", "rep movsb", X86_FEATURE_ERMS
+
+        /* Update remaining bytes after alignment with ERMS */
+        movq %r9, %rcx
+        subq %r8, %rcx
+        jmp .Lavx_aligned
+
+.Lavx_align_bytes:
+        /* Copy to align to 64-byte boundary with 8-byte chunks when possible */
+        cmpq $8, %rcx
+        jb .Lavx_align_bytes_loop
+
+.Lavx_align_loop:
+51:     movq (%rsi), %rax
+52:     movq %rax, (%rdi)
+        addq $8, %rsi
+        addq $8, %rdi
+        subq $8, %rcx
+        cmpq $8, %rcx
+        jae .Lavx_align_loop
+
+.Lavx_align_bytes_loop:
+        /* Handle remaining alignment bytes */
+        testq %rcx, %rcx
+        jz 53f
+
+54:     movb (%rsi), %al
+55:     movb %al, (%rdi)
+        incq %rdi
+        incq %rsi
+        decq %rcx
+        jnz .Lavx_align_bytes_loop
+
+53:     /* Restore count and continue with aligned copy */
+        movq %r9, %rcx
+        subq %r8, %rcx
+
+.Lavx_aligned:
+        /* Set up for 128-byte chunk copies */
+        movq %rcx, %rax
+        shrq $7, %rcx     /* Divide by 128 */
+        andl $127, %eax   /* Save remainder */
+        movq %rax, %r9    /* Save remainder for fault handling */
+        testq %rcx, %rcx
+        jz .Lavx_remainder
+
+        /* Add prefetching for large copies (>1KB) */
+        cmpq $1024, %rcx
+        jl .Lavx_loop
+
+        /* Prefetch ahead with optimized distances for Raptor Lake */
+56:     prefetcht0 384(%rsi)  /* L1 cache */
+57:     prefetcht1 512(%rsi)  /* L2 cache */
+
+        .p2align 4
+.Lavx_loop:
+        /* Read 128 bytes (4x 32-byte AVX loads) */
+58:     vmovdqu 0*32(%rsi), %ymm0
+59:     vmovdqu 1*32(%rsi), %ymm1
+60:     vmovdqu 2*32(%rsi), %ymm2
+61:     vmovdqu 3*32(%rsi), %ymm3
+
+        /* Periodic prefetch for very large transfers optimized for Raptor Lake client */
+        testq $0xF, %rcx    /* Changed from $0x7 to $0xF to optimize frequency */
+        jnz 62f
+        cmpq $32, %rcx      /* Changed from $16 to $32 to focus on larger transfers */
+        jl 62f
+63:     prefetcht0 384(%rsi)  /* L1 cache */
+
+62:     /* Write 128 bytes (4x 32-byte AVX stores) */
+64:     vmovdqa %ymm0, 0*32(%rdi)
+65:     vmovdqa %ymm1, 1*32(%rdi)
+66:     vmovdqa %ymm2, 2*32(%rdi)
+67:     vmovdqa %ymm3, 3*32(%rdi)
+
+        addq $128, %rsi
+        addq $128, %rdi
+        decq %rcx
+        jnz .Lavx_loop
+        jmp .Lavx_remainder
+
+.Lavx_nt_copy:
+        /* Non-temporal stores for very large copies (>1MB) */
+        /* Align destination to 64-byte boundary for Raptor Lake */
+        movl %edi, %eax
+        andl $63, %eax
+        jz .Lavx_nt_aligned
+
+        /* Calculate bytes to 64-byte alignment */
+        movl $64, %edx
+        subl %eax, %edx
+        movq %rdx, %r8
+
+        /* Ensure alignment doesn't exceed total bytes */
+        cmpq %rcx, %r8
+        jbe 68f
+        movq %rcx, %r8
+68:
+        /* Save original count */
+        movq %rcx, %r9
+        movq %r8, %rcx
+
+        /* Use rep movsb for alignment if ERMS, otherwise manual copy */
+69:     ALTERNATIVE "jmp .Lavx_nt_align_bytes", "rep movsb", X86_FEATURE_ERMS
+
+        /* Update remaining bytes after alignment with ERMS */
+        movq %r9, %rcx
+        subq %r8, %rcx
+        jmp .Lavx_nt_aligned
+
+.Lavx_nt_align_bytes:
+        /* Copy to align to 64-byte boundary with 8-byte chunks when possible */
+        cmpq $8, %rcx
+        jb .Lavx_nt_align_bytes_loop
+
+.Lavx_nt_align_loop:
+70:     movq (%rsi), %rax
+71:     movq %rax, (%rdi)
+        addq $8, %rsi
+        addq $8, %rdi
+        subq $8, %rcx
+        cmpq $8, %rcx
+        jae .Lavx_nt_align_loop
+
+.Lavx_nt_align_bytes_loop:
+        /* Handle remaining alignment bytes */
+        testq %rcx, %rcx
+        jz 72f
+
+73:     movb (%rsi), %al
+74:     movb %al, (%rdi)
+        incq %rdi
+        incq %rsi
+        decq %rcx
+        jnz .Lavx_nt_align_bytes_loop
+
+72:     /* Restore count and continue with aligned copy */
+        movq %r9, %rcx
+        subq %r8, %rcx
+
+.Lavx_nt_aligned:
+        /* Set up for 128-byte chunk copies */
+        movq %rcx, %rax
+        shrq $7, %rcx     /* Divide by 128 */
+        andl $127, %eax   /* Save remainder */
+        movq %rax, %r9    /* Save remainder for fault handling */
+        testq %rcx, %rcx
+        jz .Lavx_remainder
+
+.Lavx_nt_loop:
+        /* Read 128 bytes (4x 32-byte AVX loads) */
+75:     vmovdqu 0*32(%rsi), %ymm0
+76:     vmovdqu 1*32(%rsi), %ymm1
+77:     vmovdqu 2*32(%rsi), %ymm2
+78:     vmovdqu 3*32(%rsi), %ymm3
+
+        /* Write 128 bytes with non-temporal stores */
+79:     vmovntdq %ymm0, 0*32(%rdi)
+80:     vmovntdq %ymm1, 1*32(%rdi)
+81:     vmovntdq %ymm2, 2*32(%rdi)
+82:     vmovntdq %ymm3, 3*32(%rdi)
+
+        addq $128, %rsi
+        addq $128, %rdi
+        decq %rcx
+        jnz .Lavx_nt_loop
+
+        /* Ensure non-temporal stores are visible */
+        sfence
+
+.Lavx_remainder:
+        /* Clear AVX state to avoid penalties */
+        vzeroupper
+
+        /* Handle remaining bytes (<128) */
+        movq %r9, %rcx    /* Restore remainder */
+        testq %rcx, %rcx
+        jz .Lexit_success
+
+        /* Use rep movsb for remainder if ERMS, otherwise manual copy */
+83:     ALTERNATIVE "jmp .Lcopy_trailing_bytes", "rep movsb", X86_FEATURE_ERMS
+        xorq %rcx, %rcx       /* Mark successful completion */
+        jmp .Lexit_success
+
+        .p2align 4
+.Llarge_manual_copy:
+        /* Manual copy for large sizes without ERMS or AVX2 */
+        /* Add prefetching for large copies (>1KB) */
+        cmpq $1024, %rcx
+        jl .Llarge_no_prefetch
+
+        /* Prefetch ahead with optimized distances for Raptor Lake */
+84:     prefetcht0 384(%rsi)  /* L1 cache */
+85:     prefetcht1 512(%rsi)  /* L2 cache */
+
+.Llarge_no_prefetch:
+        /* Use 64-byte chunks for Raptor Lake's cache line size */
+        movq %rcx, %rax
+        shrq $6, %rcx     /* Divide by 64 */
+        andl $63, %eax    /* Save remainder */
+        movq %rax, %r9    /* Save remainder for fault handling */
+        testq %rcx, %rcx
+        jz .Llarge_remainder
+
+        .p2align 4
+.Llarge_loop:
+        /* Read 64 bytes */
+86:     movq 0*8(%rsi), %r8
+87:     movq 1*8(%rsi), %r9
+88:     movq 2*8(%rsi), %r10
+89:     movq 3*8(%rsi), %r11
+90:     movq 4*8(%rsi), %r12
+91:     movq 5*8(%rsi), %r13
+92:     movq 6*8(%rsi), %r14
+93:     movq 7*8(%rsi), %r15
+
+        /* Periodic prefetch for very large transfers optimized for Raptor Lake client */
+        testq $0xF, %rcx    /* Changed from $0x7 to $0xF to optimize frequency */
+        jnz 94f
+        cmpq $32, %rcx      /* Changed from $16 to $32 to focus on larger transfers */
+        jl 94f
+95:     prefetcht0 384(%rsi)  /* L1 cache */
+
+94:     /* Write 64 bytes */
+96:     movq %r8, 0*8(%rdi)
+97:     movq %r9, 1*8(%rdi)
+98:     movq %r10, 2*8(%rdi)
+99:     movq %r11, 3*8(%rdi)
+100:    movq %r12, 4*8(%rdi)
+101:    movq %r13, 5*8(%rdi)
+102:    movq %r14, 6*8(%rdi)
+103:    movq %r15, 7*8(%rdi)
+
+        addq $64, %rsi
+        addq $64, %rdi
+        decq %rcx
+        jnz .Llarge_loop
+
+.Llarge_remainder:
+        /* Handle remaining bytes (<64) */
+        movq %r9, %rcx    /* Restore remainder */
+        testq %rcx, %rcx
+        jz .Lexit_success
+
+        /* Use rep movsb for remainder if ERMS, otherwise manual copy */
+104:    ALTERNATIVE "jmp .Lcopy_trailing_bytes", "rep movsb", X86_FEATURE_ERMS
+        xorq %rcx, %rcx       /* Mark successful completion */
+        jmp .Lexit_success
+
+.Llarge_read_fault:
+        /* Calculate remaining bytes for read fault */
+        shlq $6, %rcx     /* Convert back to bytes */
+        addq %r9, %rcx    /* Add remainder bytes */
+        cld               /* Ensure direction flag is cleared */
+        popq %r15
+        popq %r14
+        popq %r13
+        popq %r12
+        RET
+
+.Llarge_write_fault:
+        /* Calculate remaining bytes for write fault */
+        shlq $6, %rcx     /* Convert back to bytes */
+        addq %r9, %rcx    /* Add remainder bytes */
+        cld               /* Ensure direction flag is cleared */
+        popq %r15
+        popq %r14
+        popq %r13
+        popq %r12
+        RET
+
+.Lavx_read_fault:
+        /* Calculate remaining bytes for AVX read fault */
+        shlq $7, %rcx     /* Convert back to bytes */
+        addq %r9, %rcx    /* Add remainder bytes */
+        vzeroupper        /* Clear AVX state */
+        cld               /* Ensure direction flag is cleared */
+        popq %r15
+        popq %r14
+        popq %r13
+        popq %r12
+        RET
+
+.Lavx_write_fault:
+        /* Calculate remaining bytes for AVX write fault */
+        shlq $7, %rcx     /* Convert back to bytes */
+        addq %r9, %rcx    /* Add remainder bytes */
+        vzeroupper        /* Clear AVX state */
+        cld               /* Ensure direction flag is cleared */
+        popq %r15
+        popq %r14
+        popq %r13
+        popq %r12
+        RET
+
+        /* Exception table for AVX2 path */
+        _ASM_EXTABLE_UA(47b, .Lexit_fault) /* AVX2 check */
+        _ASM_EXTABLE_UA(48b, .Lexit_fault) /* OSXSAVE check */
+        _ASM_EXTABLE_UA(50b, .Lexit_fault) /* ERMS alignment */
+        _ASM_EXTABLE_UA(51b, .Lexit_fault) /* AVX alignment read */
+        _ASM_EXTABLE_UA(52b, .Lexit_fault) /* AVX alignment write */
+        _ASM_EXTABLE_UA(54b, .Lexit_fault) /* AVX alignment byte read */
+        _ASM_EXTABLE_UA(55b, .Lexit_fault) /* AVX alignment byte write */
+        _ASM_EXTABLE_UA(56b, .Lavx_read_fault) /* AVX prefetch */
+        _ASM_EXTABLE_UA(57b, .Lavx_read_fault) /* AVX prefetch */
+        _ASM_EXTABLE_UA(58b, .Lavx_read_fault) /* AVX read */
+        _ASM_EXTABLE_UA(59b, .Lavx_read_fault) /* AVX read */
+        _ASM_EXTABLE_UA(60b, .Lavx_read_fault) /* AVX read */
+        _ASM_EXTABLE_UA(61b, .Lavx_read_fault) /* AVX read */
+        _ASM_EXTABLE_UA(63b, .Lavx_read_fault) /* AVX prefetch */
+        _ASM_EXTABLE_UA(64b, .Lavx_write_fault) /* AVX write */
+        _ASM_EXTABLE_UA(65b, .Lavx_write_fault) /* AVX write */
+        _ASM_EXTABLE_UA(66b, .Lavx_write_fault) /* AVX write */
+        _ASM_EXTABLE_UA(67b, .Lavx_write_fault) /* AVX write */
+        _ASM_EXTABLE_UA(69b, .Lexit_fault) /* ERMS alignment */
+        _ASM_EXTABLE_UA(70b, .Lexit_fault) /* AVX NT alignment read */
+        _ASM_EXTABLE_UA(71b, .Lexit_fault) /* AVX NT alignment write */
+        _ASM_EXTABLE_UA(73b, .Lexit_fault) /* AVX NT alignment byte read */
+        _ASM_EXTABLE_UA(74b, .Lexit_fault) /* AVX NT alignment byte write */
+        _ASM_EXTABLE_UA(75b, .Lavx_read_fault) /* AVX NT read */
+        _ASM_EXTABLE_UA(76b, .Lavx_read_fault) /* AVX NT read */
+        _ASM_EXTABLE_UA(77b, .Lavx_read_fault) /* AVX NT read */
+        _ASM_EXTABLE_UA(78b, .Lavx_read_fault) /* AVX NT read */
+        _ASM_EXTABLE_UA(79b, .Lavx_write_fault) /* AVX NT write */
+        _ASM_EXTABLE_UA(80b, .Lavx_write_fault) /* AVX NT write */
+        _ASM_EXTABLE_UA(81b, .Lavx_write_fault) /* AVX NT write */
+        _ASM_EXTABLE_UA(82b, .Lavx_write_fault) /* AVX NT write */
+        _ASM_EXTABLE_UA(83b, .Lexit_fault) /* ERMS remainder */
+
+        /* Exception table for large manual copy */
+        _ASM_EXTABLE_UA(84b, .Llarge_read_fault) /* Prefetch */
+        _ASM_EXTABLE_UA(85b, .Llarge_read_fault) /* Prefetch */
+        _ASM_EXTABLE_UA(86b, .Llarge_read_fault) /* Read */
+        _ASM_EXTABLE_UA(87b, .Llarge_read_fault) /* Read */
+        _ASM_EXTABLE_UA(88b, .Llarge_read_fault) /* Read */
+        _ASM_EXTABLE_UA(89b, .Llarge_read_fault) /* Read */
+        _ASM_EXTABLE_UA(90b, .Llarge_read_fault) /* Read */
+        _ASM_EXTABLE_UA(91b, .Llarge_read_fault) /* Read */
+        _ASM_EXTABLE_UA(92b, .Llarge_read_fault) /* Read */
+        _ASM_EXTABLE_UA(93b, .Llarge_read_fault) /* Read */
+        _ASM_EXTABLE_UA(95b, .Llarge_read_fault) /* Prefetch */
+        _ASM_EXTABLE_UA(96b, .Llarge_write_fault) /* Write */
+        _ASM_EXTABLE_UA(97b, .Llarge_write_fault) /* Write */
+        _ASM_EXTABLE_UA(98b, .Llarge_write_fault) /* Write */
+        _ASM_EXTABLE_UA(99b, .Llarge_write_fault) /* Write */
+        _ASM_EXTABLE_UA(100b, .Llarge_write_fault) /* Write */
+        _ASM_EXTABLE_UA(101b, .Llarge_write_fault) /* Write */
+        _ASM_EXTABLE_UA(102b, .Llarge_write_fault) /* Write */
+        _ASM_EXTABLE_UA(103b, .Llarge_write_fault) /* Write */
+        _ASM_EXTABLE_UA(104b, .Lexit_fault) /* ERMS remainder */
 
-	_ASM_EXTABLE_UA( 0b, 1b)
 SYM_FUNC_END(rep_movs_alternative)
 EXPORT_SYMBOL(rep_movs_alternative)


--- a/arch/x86/lib/memset_64.S	2025-03-13 13:08:08.000000000 +0100
+++ b/arch/x86/lib/memset_64.S	2025-03-14 21:12:30.472007594 +0100
@@ -1,5 +1,6 @@
 /* SPDX-License-Identifier: GPL-2.0 */
 /* Copyright 2002 Andi Kleen, SuSE Labs */
+/* Optimized for Intel Raptor Lake by Claude, 2025 */
 
 #include <linux/export.h>
 #include <linux/linkage.h>
@@ -9,109 +10,372 @@
 .section .noinstr.text, "ax"
 
 /*
- * ISO C memset - set a memory block to a byte value. This function uses fast
- * string to get better performance than the original function. The code is
- * simpler and shorter than the original function as well.
+ * ISO C memset - set a memory block to a byte value.
+ * Optimized for Intel Raptor Lake architecture.
  *
  * rdi   destination
  * rsi   value (char)
  * rdx   count (bytes)
  *
  * rax   original destination
- *
- * The FSRS alternative should be done inline (avoiding the call and
- * the disgusting return handling), but that would require some help
- * from the compiler for better calling conventions.
- *
- * The 'rep stosb' itself is small enough to replace the call, but all
- * the register moves blow up the code. And two of them are "needed"
- * only for the return value that is the same as the source input,
- * which the compiler could/should do much better anyway.
  */
 SYM_FUNC_START(__memset)
-	ALTERNATIVE "jmp memset_orig", "", X86_FEATURE_FSRS
+        /* Store original destination for return value */
+        movq %rdi, %r9
+
+        /* Ensure proper direction flag - keep this as the main entry point */
+        cld
 
-	movq %rdi,%r9
-	movb %sil,%al
-	movq %rdx,%rcx
-	rep stosb
-	movq %r9,%rax
-	RET
+        /* Check for zero-length case first */
+        testq %rdx, %rdx
+        jz .L_zero_length
+
+        /* Fast path with FSRS - use rep stosb for all sizes */
+        ALTERNATIVE "jmp memset_orig", "", X86_FEATURE_FSRS
+
+        /* Expand byte value to fill %al */
+        movb %sil, %al
+        movq %rdx, %rcx
+1:      rep stosb
+.L_zero_length:
+        movq %r9, %rax
+        RET
+
+        /* Exception table for FSRS path */
+        _ASM_EXTABLE(1b, .Lfsrs_fault_handler)
 SYM_FUNC_END(__memset)
 EXPORT_SYMBOL(__memset)
 
 SYM_FUNC_ALIAS_MEMFUNC(memset, __memset)
 EXPORT_SYMBOL(memset)
 
+/* Fault handler for FSRS path */
+.Lfsrs_fault_handler:
+        cld                /* Ensure direction flag is cleared */
+        movq %r9, %rax     /* Restore original destination */
+        RET                /* Return directly for consistency */
+
+/* Optimized path for large sets using AVX2 */
+SYM_FUNC_START_LOCAL(memset_pcore_path)
+        /* Save return value */
+        movq %rdi, %r10
+
+        /* Direction flag is already cleared by __memset */
+        /* cld - REMOVE THIS LINE */
+
+        /* Handle zero-length case */
+        testq %rdx, %rdx
+        jz .Lexit_pcore
+
+        /* Expand byte value */
+        movzbl %sil, %ecx
+        movabs $0x0101010101010101, %rax
+        imulq %rcx, %rax
+
+        /* Save callee-saved registers we'll use */
+        pushq %rbx
+        pushq %r12
+        pushq %r13
+        pushq %r14
+        pushq %r15
+
+        /* Check for AVX2 support for large blocks - increased for Raptor Lake's improved FSRS */
+        cmpq $65536, %rdx  /* 64KB threshold to leverage Raptor Lake's FSRS */
+        jb .Lno_avx_path
+
+        /* Use AVX path for large blocks if supported */
+        ALTERNATIVE "jmp .Lno_avx_path", "", X86_FEATURE_AVX2
+
+        /* Check if kernel allows AVX usage */
+        ALTERNATIVE "jmp .Lno_avx_path", "", X86_FEATURE_OSXSAVE
+
+        /* For very large sets, use non-temporal stores */
+        cmpq $1048576, %rdx    /* 1MB threshold */
+        jae .Lavx_nt_path
+
+        /* AVX2 path for large blocks */
+        /* Broadcast byte to YMM register */
+        vmovd %ecx, %xmm0
+        vpbroadcastb %xmm0, %ymm0
+
+        /* Align destination to 64-byte boundary for Raptor Lake */
+        movl %edi, %ecx
+        andl $63, %ecx        /* Check 64-byte alignment */
+        jz .Lavx_aligned
+
+        /* Calculate bytes to align */
+        movl $64, %r8d
+        subl %ecx, %r8d
+
+        /* Ensure alignment doesn't exceed total size */
+        movq %r8, %rcx
+        cmpq %rdx, %rcx
+        jbe 2f
+        movq %rdx, %rcx
+
+2:      /* Align with rep stosb */
+        subq %rcx, %rdx
+        rep stosb
+
+        /* Skip AVX if no bytes remain */
+        testq %rdx, %rdx
+        jz .Lexit_pcore
+
+.Lavx_aligned:
+        /* Set up for 128-byte chunk copies */
+        movq %rdx, %rcx
+        shrq $7, %rcx     /* Divide by 128 */
+        jz .Lavx_remainder
+
+        /* Add prefetching for large copies (>1KB) */
+        cmpq $1024, %rdx
+        jl .Lavx_loop
+
+        /* Prefetch ahead with optimized distances for Raptor Lake */
+3:      prefetcht0 384(%rdi)  /* L1 cache */
+4:      prefetcht1 512(%rdi)  /* L2 cache */
+
+        .p2align 4
+.Lavx_loop:
+        /* Store 128 bytes (4x 32-byte AVX stores) */
+5:      vmovdqa %ymm0, 0*32(%rdi)
+6:      vmovdqa %ymm0, 1*32(%rdi)
+7:      vmovdqa %ymm0, 2*32(%rdi)
+8:      vmovdqa %ymm0, 3*32(%rdi)
+
+        /* Periodic prefetch for large transfers optimized for Raptor Lake client */
+        testq $0xF, %rcx    /* Changed from $0x7 to $0xF to optimize frequency */
+        jnz 9f
+        cmpq $32, %rcx      /* Changed from $16 to $32 to focus on larger transfers */
+        jl 9f
+10:     prefetcht0 384(%rdi)
+
+9:      addq $128, %rdi
+        decq %rcx
+        jnz .Lavx_loop
+
+        /* Calculate remaining bytes */
+        andq $127, %rdx
+        jmp .Lavx_remainder
+
+.Lavx_nt_path:
+        /* Non-temporal stores for very large sets */
+        /* Broadcast byte to YMM register */
+        vmovd %ecx, %xmm0
+        vpbroadcastb %xmm0, %ymm0
+
+        /* Align destination to 64-byte boundary for Raptor Lake */
+        movl %edi, %ecx
+        andl $63, %ecx
+        jz .Lavx_nt_aligned
+
+        /* Calculate bytes to align */
+        movl $64, %r8d
+        subl %ecx, %r8d
+
+        /* Ensure alignment doesn't exceed total size */
+        movq %r8, %rcx
+        cmpq %rdx, %rcx
+        jbe 11f
+        movq %rdx, %rcx
+
+11:     /* Align with rep stosb */
+        subq %rcx, %rdx
+        rep stosb
+
+        /* Skip AVX if no bytes remain */
+        testq %rdx, %rdx
+        jz .Lexit_pcore
+
+.Lavx_nt_aligned:
+        /* Set up for 128-byte chunk copies */
+        movq %rdx, %rcx
+        shrq $7, %rcx     /* Divide by 128 */
+        andq $127, %rdx   /* Save remainder */
+
+        .p2align 4
+.Lavx_nt_loop:
+        /* Store 128 bytes with non-temporal stores */
+12:     vmovntdq %ymm0, 0*32(%rdi)
+13:     vmovntdq %ymm0, 1*32(%rdi)
+14:     vmovntdq %ymm0, 2*32(%rdi)
+15:     vmovntdq %ymm0, 3*32(%rdi)
+
+        addq $128, %rdi
+        decq %rcx
+        jnz .Lavx_nt_loop
+
+        /* Ensure non-temporal stores are visible */
+        sfence
+
+.Lavx_remainder:
+        /* Clear AVX state to avoid penalties */
+        vzeroupper
+
+        /* Handle remaining bytes (<128) */
+        testq %rdx, %rdx
+        jz .Lexit_pcore
+
+        /* Use rep stosb for tail */
+        movq %rdx, %rcx
+        rep stosb
+
+.Lexit_pcore:
+        /* Restore saved registers and return */
+        movq %r10, %rax  /* Return original pointer */
+        popq %r15
+        popq %r14
+        popq %r13
+        popq %r12
+        popq %rbx
+        RET
+
+.Lno_avx_path:
+        /* Restore registers before jumping to regular path */
+        movq %r10, %rax  /* Restore original pointer */
+        popq %r15
+        popq %r14
+        popq %r13
+        popq %r12
+        popq %rbx
+        jmp memset_orig
+
+/* AVX fault handler - properly clean up and return */
+.Lavx_fault_handler:
+        /* Clean up AVX state, clear direction flag, and return original destination */
+        vzeroupper
+        cld                 /* Ensure direction flag is cleared */
+        movq %r10, %rax     /* Restore original destination */
+        popq %r15
+        popq %r14
+        popq %r13
+        popq %r12
+        popq %rbx
+        RET
+
+/* Exception tables for AVX path */
+        _ASM_EXTABLE(2b, .Lavx_fault_handler)
+        _ASM_EXTABLE(3b, .Lavx_fault_handler)  /* Prefetch */
+        _ASM_EXTABLE(4b, .Lavx_fault_handler)  /* Prefetch */
+        _ASM_EXTABLE(5b, .Lavx_fault_handler)
+        _ASM_EXTABLE(6b, .Lavx_fault_handler)
+        _ASM_EXTABLE(7b, .Lavx_fault_handler)
+        _ASM_EXTABLE(8b, .Lavx_fault_handler)
+        _ASM_EXTABLE(9b, .Lavx_fault_handler)
+        _ASM_EXTABLE(10b, .Lavx_fault_handler) /* Prefetch */
+        _ASM_EXTABLE(11b, .Lavx_fault_handler)
+        _ASM_EXTABLE(12b, .Lavx_fault_handler)
+        _ASM_EXTABLE(13b, .Lavx_fault_handler)
+        _ASM_EXTABLE(14b, .Lavx_fault_handler)
+        _ASM_EXTABLE(15b, .Lavx_fault_handler)
+SYM_FUNC_END(memset_pcore_path)
+
+/* Original memset implementation (non-optimized fallback) */
 SYM_FUNC_START_LOCAL(memset_orig)
-	movq %rdi,%r10
+        /* Direction flag is already cleared by __memset */
+        /* cld - REMOVE THIS LINE */
+
+        /* Store original destination for return value */
+        movq %rdi, %r9
+
+        /* Optimize for zero length */
+        testq %rdx, %rdx
+        jz .Lende
+
+        /* Expand byte value */
+        movzbl %sil, %ecx
+        movabs $0x0101010101010101, %rax
+        imulq %rcx, %rax
+
+        /* Handle small sizes (<=64 bytes) directly with rep stosb */
+        cmpq $64, %rdx
+        jbe .Lsmall
+
+        /* Align destination to 64-byte cache line boundary for Raptor Lake */
+        movl %edi, %ecx
+        andl $63, %ecx
+        jz .Lafter_bad_alignment
+
+        /* Calculate bytes to 64-byte alignment */
+        movl $64, %r8d
+        subl %ecx, %r8d
+
+        /* Ensure alignment doesn't exceed total size */
+        movq %r8, %rcx
+        cmpq %rdx, %rcx
+        jbe 16f
+        movq %rdx, %rcx
+
+16:     /* Align with rep stosb */
+        subq %rcx, %rdx
+        rep stosb
+
+        /* Check if we have bytes left to set */
+        testq %rdx, %rdx
+        jz .Lende
 
-	/* expand byte value  */
-	movzbl %sil,%ecx
-	movabs $0x0101010101010101,%rax
-	imulq  %rcx,%rax
-
-	/* align dst */
-	movl  %edi,%r9d
-	andl  $7,%r9d
-	jnz  .Lbad_alignment
 .Lafter_bad_alignment:
+        /* Check if we have enough memory for prefetching */
+        cmpq $256, %rdx
+        jb .Lno_prefetch_orig
+
+        /* Add prefetching for large blocks - optimized for Raptor Lake */
+17:     prefetchw 384(%rdi)
+18:     prefetchw 512(%rdi)
+
+.Lno_prefetch_orig:
+        /* Process 64-byte chunks - cache line sized */
+        movq %rdx, %rcx
+        shrq $6, %rcx
+        jz .Lhandle_tail
+
+        .p2align 4
+.Lloop_64_orig:
+19:     movq %rax, 0*8(%rdi)
+20:     movq %rax, 1*8(%rdi)
+21:     movq %rax, 2*8(%rdi)
+22:     movq %rax, 3*8(%rdi)
+23:     movq %rax, 4*8(%rdi)
+24:     movq %rax, 5*8(%rdi)
+25:     movq %rax, 6*8(%rdi)
+26:     movq %rax, 7*8(%rdi)
+
+        leaq 64(%rdi), %rdi
+        decq %rcx
+        jnz .Lloop_64_orig
+
+        /* Calculate remaining bytes */
+        andq $63, %rdx
 
-	movq  %rdx,%rcx
-	shrq  $6,%rcx
-	jz	 .Lhandle_tail
-
-	.p2align 4
-.Lloop_64:
-	decq  %rcx
-	movq  %rax,(%rdi)
-	movq  %rax,8(%rdi)
-	movq  %rax,16(%rdi)
-	movq  %rax,24(%rdi)
-	movq  %rax,32(%rdi)
-	movq  %rax,40(%rdi)
-	movq  %rax,48(%rdi)
-	movq  %rax,56(%rdi)
-	leaq  64(%rdi),%rdi
-	jnz    .Lloop_64
-
-	/* Handle tail in loops. The loops should be faster than hard
-	   to predict jump tables. */
-	.p2align 4
 .Lhandle_tail:
-	movl	%edx,%ecx
-	andl    $63&(~7),%ecx
-	jz 		.Lhandle_7
-	shrl	$3,%ecx
-	.p2align 4
-.Lloop_8:
-	decl   %ecx
-	movq  %rax,(%rdi)
-	leaq  8(%rdi),%rdi
-	jnz    .Lloop_8
-
-.Lhandle_7:
-	andl	$7,%edx
-	jz      .Lende
-	.p2align 4
-.Lloop_1:
-	decl    %edx
-	movb 	%al,(%rdi)
-	leaq	1(%rdi),%rdi
-	jnz     .Lloop_1
+.Lsmall:
+        /* Handle remaining bytes with rep stosb */
+        testq %rdx, %rdx
+        jz .Lende
+
+        movq %rdx, %rcx
+        rep stosb
 
 .Lende:
-	movq	%r10,%rax
-	RET
+        movq %r9, %rax
+        RET
 
-.Lbad_alignment:
-	cmpq $7,%rdx
-	jbe	.Lhandle_7
-	movq %rax,(%rdi)	/* unaligned store */
-	movq $8,%r8
-	subq %r9,%r8
-	addq %r8,%rdi
-	subq %r8,%rdx
-	jmp .Lafter_bad_alignment
-.Lfinal:
+/* Generic fault handler */
+.Lorig_fault_handler:
+        cld                /* Ensure direction flag is cleared */
+        movq %r9, %rax     /* Restore original destination */
+        RET
+
+        /* Exception tables for original path */
+        _ASM_EXTABLE(16b, .Lorig_fault_handler)
+        _ASM_EXTABLE(17b, .Lorig_fault_handler) /* Prefetch */
+        _ASM_EXTABLE(18b, .Lorig_fault_handler) /* Prefetch */
+        _ASM_EXTABLE(19b, .Lorig_fault_handler)
+        _ASM_EXTABLE(20b, .Lorig_fault_handler)
+        _ASM_EXTABLE(21b, .Lorig_fault_handler)
+        _ASM_EXTABLE(22b, .Lorig_fault_handler)
+        _ASM_EXTABLE(23b, .Lorig_fault_handler)
+        _ASM_EXTABLE(24b, .Lorig_fault_handler)
+        _ASM_EXTABLE(25b, .Lorig_fault_handler)
+        _ASM_EXTABLE(26b, .Lorig_fault_handler)
 SYM_FUNC_END(memset_orig)
