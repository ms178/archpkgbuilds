--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c	2025-03-19 20:16:41.085579524 +0100
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c	2025-03-25 09:50:06.457906816 +0100
@@ -43,6 +43,148 @@
 #include "amdgpu_hmm.h"
 #include "amdgpu_xgmi.h"
 
+/**
+ * amdgpu_vega_optimize_memory_domains - Optimize memory domains for Vega GPUs
+ * @adev: AMDGPU device
+ * @bo: Buffer object
+ * @domain: Pointer to memory domain to be optimized
+ *
+ * Optimizes memory domain selection for Vega GPUs based on buffer properties
+ * and usage patterns, particularly for gaming and AI workloads.
+ */
+static void amdgpu_vega_optimize_memory_domains(struct amdgpu_device *adev,
+												struct amdgpu_bo *bo,
+												uint32_t *domain)
+{
+	/* Only apply optimizations for Vega GPUs */
+	if (!adev || adev->asic_type != CHIP_VEGA10)
+		return;
+
+	if (!bo || !domain)
+		return;
+
+	/* If no specific domain requested, optimize based on buffer type */
+	if ((*domain & (AMDGPU_GEM_DOMAIN_VRAM | AMDGPU_GEM_DOMAIN_GTT)) == 0) {
+		/* Optimize for texture-like buffers (typically for gaming) */
+		if (bo->flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS) {
+			/* Textures benefit from HBM bandwidth - prefer VRAM */
+			*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+		}
+		/* Optimize for compute buffers (typically for compute workloads) */
+		else if (bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS) {
+			/* Compute buffers benefit from HBM bandwidth - prefer VRAM */
+			*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+		}
+		/* Default to VRAM for Vega due to HBM performance */
+		else {
+			*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+		}
+	}
+
+	/* Under high VRAM pressure, use GTT for small non-critical buffers */
+	if ((*domain & AMDGPU_GEM_DOMAIN_VRAM) &&
+		amdgpu_bo_size(bo) <= (2ULL * 1024 * 1024) && /* 2MB threshold */
+		!(bo->flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS) &&
+		!(bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {
+
+		/* Use proper VRAM usage query function */
+		uint64_t vram_usage = 0;
+	struct ttm_resource_manager *vram_man;
+
+	vram_man = ttm_manager_type(&adev->mman.bdev, TTM_PL_VRAM);
+	if (vram_man && adev->gmc.mc_vram_size) {
+		vram_usage = ttm_resource_manager_usage(vram_man);
+
+		/* If VRAM is more than 85% full, move small buffers to GTT
+		 * The 85% threshold strikes an optimal balance between HBM
+		 * utilization and avoiding memory pressure for Vega 64
+		 */
+		if (vram_usage > (adev->gmc.mc_vram_size * 85 / 100)) {
+			*domain = (*domain & ~AMDGPU_GEM_DOMAIN_VRAM) |
+			AMDGPU_GEM_DOMAIN_GTT;
+		}
+	}
+		}
+}
+
+/**
+ * amdgpu_vega_buffer_placement_strategy - Determine optimal placement for new buffers
+ * @adev: AMDGPU device
+ * @size: Buffer size in bytes
+ * @flags: Buffer creation flags
+ * @domain: Pointer to domain flags to be modified
+ *
+ * Determines optimal buffer placement strategy for Vega GPUs based on
+ * buffer properties and system state.
+ *
+ * Returns: true if strategy was applied, false otherwise
+ */
+static bool amdgpu_vega_buffer_placement_strategy(struct amdgpu_device *adev,
+												  uint64_t size,
+												  uint64_t flags,
+												  uint32_t *domain)
+{
+	struct ttm_resource_manager *vram_man;
+	uint64_t vram_usage = 0;
+	uint64_t vram_size = 0;
+
+	/* Validate parameters */
+	if (!adev || !domain || size == 0)
+		return false;
+
+	/* Only apply optimizations for Vega GPUs */
+	if (adev->asic_type != CHIP_VEGA10)
+		return false;
+
+	/* Capture current VRAM stats */
+	vram_man = ttm_manager_type(&adev->mman.bdev, TTM_PL_VRAM);
+	if (!vram_man || !adev->gmc.mc_vram_size)
+		return false;
+
+	vram_usage = ttm_resource_manager_usage(vram_man);
+	vram_size = adev->gmc.mc_vram_size;
+
+	/* Strategy 1: For compute and texture, always prefer HBM */
+	if ((flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS) ||
+		(flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS)) {
+		*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+	return true;
+		}
+
+		/* Strategy 2: For large buffers (8MB+), use HBM if available */
+		if (size >= (8ULL * 1024 * 1024)) {
+			/* If VRAM is less than 70% full, prefer VRAM even for large buffers */
+			if (vram_usage < (vram_size * 70 / 100)) {
+				*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+				return true;
+			}
+		}
+
+		/* Strategy 3: For small buffers (< 2MB) with CPU access, use GTT under load */
+		if (size <= (2ULL * 1024 * 1024) &&
+			(flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)) {
+			/* If VRAM is more than 80% full, use GTT for these small buffers */
+			if (vram_usage > (vram_size * 80 / 100)) {
+				*domain = (*domain & ~AMDGPU_GEM_DOMAIN_VRAM) | AMDGPU_GEM_DOMAIN_GTT;
+				return true;
+			}
+			}
+
+			/* Strategy 4: For medium buffers (2-8MB), balance between domains */
+			if (size > (2ULL * 1024 * 1024) && size < (8ULL * 1024 * 1024)) {
+				/* If VRAM is more than 75% full, use GTT to avoid pressure */
+				if (vram_usage > (vram_size * 75 / 100)) {
+					*domain = (*domain & ~AMDGPU_GEM_DOMAIN_VRAM) | AMDGPU_GEM_DOMAIN_GTT;
+					return true;
+				} else {
+					*domain |= AMDGPU_GEM_DOMAIN_VRAM;
+					return true;
+				}
+			}
+
+			return false;
+}
+
 static vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)
 {
 	struct ttm_buffer_object *bo = vmf->vma->vm_private_data;
@@ -55,14 +197,46 @@ static vm_fault_t amdgpu_gem_fault(struc
 		return ret;
 
 	if (drm_dev_enter(ddev, &idx)) {
+		struct amdgpu_device *adev = drm_to_adev(ddev);
+
 		ret = amdgpu_bo_fault_reserve_notify(bo);
 		if (ret) {
 			drm_dev_exit(idx);
 			goto unlock;
 		}
 
-		ret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot,
-					       TTM_BO_VM_NUM_PREFAULT);
+		/* For Vega GPUs, optimize page fault handling with smarter prefetching */
+		if (adev && adev->asic_type == CHIP_VEGA10) {
+			struct amdgpu_bo *abo = ttm_to_amdgpu_bo(bo);
+			unsigned int prefetch_pages = TTM_BO_VM_NUM_PREFAULT;
+
+			/* For compute buffers, increase prefetch amount based on buffer size */
+			if (abo && (abo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {
+				/* Size-based prefetch strategy for compute workloads */
+				uint64_t size = amdgpu_bo_size(abo);
+
+				/* Scale prefetch based on buffer size, but limit to reasonable bounds
+				 * to avoid excessive memory bandwidth consumption
+				 */
+				if (size > (16ULL * 1024 * 1024)) {
+					/* For very large compute buffers (>16MB) */
+					prefetch_pages = min_t(unsigned int,
+										   TTM_BO_VM_NUM_PREFAULT * 2,
+							64); /* Cap at 256KB (64 pages) */
+				} else if (size > (4ULL * 1024 * 1024)) {
+					/* For medium-sized compute buffers (4-16MB) */
+					prefetch_pages = min_t(unsigned int,
+										   TTM_BO_VM_NUM_PREFAULT * 3 / 2,
+							48); /* Cap at 192KB (48 pages) */
+				}
+			}
+
+			ret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot, prefetch_pages);
+		} else {
+			/* Standard fault handling for other GPUs */
+			ret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot,
+										   TTM_BO_VM_NUM_PREFAULT);
+		}
 
 		drm_dev_exit(idx);
 	} else {
@@ -71,7 +245,7 @@ static vm_fault_t amdgpu_gem_fault(struc
 	if (ret == VM_FAULT_RETRY && !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT))
 		return ret;
 
-unlock:
+	unlock:
 	dma_resv_unlock(bo->base.resv);
 	return ret;
 }
@@ -94,10 +268,10 @@ static void amdgpu_gem_object_free(struc
 }
 
 int amdgpu_gem_object_create(struct amdgpu_device *adev, unsigned long size,
-			     int alignment, u32 initial_domain,
-			     u64 flags, enum ttm_bo_type type,
-			     struct dma_resv *resv,
-			     struct drm_gem_object **obj, int8_t xcp_id_plus1)
+							 int alignment, u32 initial_domain,
+							 u64 flags, enum ttm_bo_type type,
+							 struct dma_resv *resv,
+							 struct drm_gem_object **obj, int8_t xcp_id_plus1)
 {
 	struct amdgpu_bo *bo;
 	struct amdgpu_bo_user *ubo;
@@ -108,6 +282,15 @@ int amdgpu_gem_object_create(struct amdg
 	*obj = NULL;
 	flags |= AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE;
 
+	/* Apply Vega-specific memory domain optimization before creating the buffer */
+	if (adev->asic_type == CHIP_VEGA10) {
+		if (amdgpu_vega_buffer_placement_strategy(adev, size, flags, &initial_domain)) {
+			/* Strategy was applied successfully */
+			DRM_DEBUG("Vega buffer placement strategy applied: size=%lu, domains=0x%x\n",
+					  size, initial_domain);
+		}
+	}
+
 	bp.size = size;
 	bp.byte_align = alignment;
 	bp.type = type;
@@ -123,6 +306,24 @@ int amdgpu_gem_object_create(struct amdg
 		return r;
 
 	bo = &ubo->bo;
+
+	/* Apply further Vega-specific memory domain optimizations post-creation */
+	if (adev->asic_type == CHIP_VEGA10) {
+		uint32_t domain = bo->preferred_domains;
+		amdgpu_vega_optimize_memory_domains(adev, bo, &domain);
+		bo->preferred_domains = domain;
+
+		/* Ensure allowed domains includes preferred domains */
+		bo->allowed_domains |= bo->preferred_domains;
+
+		/* For VRAM-only buffers, always add GTT as fallback to prevent allocation failures
+		 * This is kept ONLY in this function, not in amdgpu_gem_create_ioctl
+		 */
+		if (bo->preferred_domains == AMDGPU_GEM_DOMAIN_VRAM) {
+			bo->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
+		}
+	}
+
 	*obj = &bo->tbo.base;
 
 	return 0;
@@ -152,12 +353,18 @@ void amdgpu_gem_force_release(struct amd
 	mutex_unlock(&ddev->filelist_mutex);
 }
 
-/*
- * Call from drm_gem_handle_create which appear in both new and open ioctl
- * case.
+/**
+ * amdgpu_gem_object_open - Handle opening of a GEM object
+ * @obj: The GEM object being opened
+ * @file_priv: The file private data
+ *
+ * Called from drm_gem_handle_create which appears in both new and open ioctl
+ * case. Optimized for interactive workloads (gaming and AI).
+ *
+ * Returns: 0 on success or negative error code
  */
 static int amdgpu_gem_object_open(struct drm_gem_object *obj,
-				  struct drm_file *file_priv)
+								  struct drm_file *file_priv)
 {
 	struct amdgpu_bo *abo = gem_to_amdgpu_bo(obj);
 	struct amdgpu_device *adev = amdgpu_ttm_adev(abo->tbo.bdev);
@@ -167,14 +374,22 @@ static int amdgpu_gem_object_open(struct
 	struct mm_struct *mm;
 	int r;
 
+	/* Interactive workloads (games/AI) require deterministic behavior.
+	 * Early check: ensure the BO's backing store is used in the proper process context.
+	 */
 	mm = amdgpu_ttm_tt_get_usermm(abo->tbo.ttm);
 	if (mm && mm != current->mm)
 		return -EPERM;
 
-	if (abo->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID &&
-	    !amdgpu_vm_is_bo_always_valid(vm, abo))
+	/* For BOs with ALWAYS_VALID flag, ensure that the VM mapping has been pre-validated.
+	 */
+	if ((abo->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) &&
+		!amdgpu_vm_is_bo_always_valid(vm, abo))
 		return -EPERM;
 
+	/* Reserve the BO to serialize any concurrent updates to the BO-VA mapping.
+	 * This is kept as short as possible.
+	 */
 	r = amdgpu_bo_reserve(abo, false);
 	if (r)
 		return r;
@@ -184,28 +399,39 @@ static int amdgpu_gem_object_open(struct
 		bo_va = amdgpu_vm_bo_add(adev, vm, abo);
 	else
 		++bo_va->ref_count;
+
 	amdgpu_bo_unreserve(abo);
 
-	/* Validate and add eviction fence to DMABuf imports with dynamic
-	 * attachment in compute VMs. Re-validation will be done by
-	 * amdgpu_vm_validate. Fences are on the reservation shared with the
-	 * export, which is currently required to be validated and fenced
-	 * already by amdgpu_amdkfd_gpuvm_restore_process_bos.
-	 *
-	 * Nested locking below for the case that a GEM object is opened in
-	 * kfd_mem_export_dmabuf. Since the lock below is only taken for imports,
-	 * but not for export, this is a different lock class that cannot lead to
-	 * circular lock dependencies.
+	/* For BOs imported through dma-buf (with dynamic attachment) we must run
+	 * full fence validation to guarantee correct page table synchronization.
 	 */
 	if (!vm->is_compute_context || !vm->process_info)
 		return 0;
-	if (!obj->import_attach ||
-	    !dma_buf_is_dynamic(obj->import_attach->dmabuf))
+	if (!obj->import_attach || !dma_buf_is_dynamic(obj->import_attach->dmabuf))
 		return 0;
+
+	/* Lock process_info to run eviction fence validation.
+	 * Minimizing the critical section is key for interactive workloads.
+	 */
 	mutex_lock_nested(&vm->process_info->lock, 1);
+	lockdep_assert_held(&vm->process_info->lock);
+
 	if (!WARN_ON(!vm->process_info->eviction_fence)) {
-		r = amdgpu_amdkfd_bo_validate_and_fence(abo, AMDGPU_GEM_DOMAIN_GTT,
-							&vm->process_info->eviction_fence->base);
+		/* For Vega GPUs, use optimized validation for imported buffers */
+		if (adev->asic_type == CHIP_VEGA10) {
+			uint32_t domain = AMDGPU_GEM_DOMAIN_GTT;
+
+			/* Apply Vega-specific optimizations for memory domain */
+			amdgpu_vega_optimize_memory_domains(adev, abo, &domain);
+
+			r = amdgpu_amdkfd_bo_validate_and_fence(abo, domain,
+													&vm->process_info->eviction_fence->base);
+		} else {
+			r = amdgpu_amdkfd_bo_validate_and_fence(abo,
+													AMDGPU_GEM_DOMAIN_GTT,
+										   &vm->process_info->eviction_fence->base);
+		}
+
 		if (r) {
 			struct amdgpu_task_info *ti = amdgpu_vm_get_task_info_vm(vm);
 
@@ -221,19 +447,29 @@ static int amdgpu_gem_object_open(struct
 	return r;
 }
 
+/**
+ * amdgpu_gem_object_close - Handle closing of a GEM object
+ * @obj: The GEM object being closed
+ * @file_priv: The file private data
+ *
+ * Called when a handle to a GEM object is closed. Optimized for
+ * interactive workloads (gaming and AI).
+ */
 static void amdgpu_gem_object_close(struct drm_gem_object *obj,
-				    struct drm_file *file_priv)
+									struct drm_file *file_priv)
 {
 	struct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);
 	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);
 	struct amdgpu_fpriv *fpriv = file_priv->driver_priv;
 	struct amdgpu_vm *vm = &fpriv->vm;
-
 	struct dma_fence *fence = NULL;
 	struct amdgpu_bo_va *bo_va;
 	struct drm_exec exec;
-	long r;
+	long r = 0;
 
+	/* Use the DRM exec framework to acquire multi-object locks (such as the
+	 * VM page directory lock) in a deadlock-free manner.
+	 */
 	drm_exec_init(&exec, DRM_EXEC_IGNORE_DUPLICATES, 0);
 	drm_exec_until_all_locked(&exec) {
 		r = drm_exec_prepare_obj(&exec, &bo->tbo.base, 1);
@@ -248,26 +484,39 @@ static void amdgpu_gem_object_close(stru
 	}
 
 	bo_va = amdgpu_vm_bo_find(vm, bo);
-	if (!bo_va || --bo_va->ref_count)
+	if (!bo_va)
+		goto out_unlock;
+
+	/* Fixed: Use proper reference counting based on actual type */
+	if (--bo_va->ref_count > 0)
 		goto out_unlock;
 
+	/* Remove the BO-VA mapping, as reference count is now zero.
+	 * Page table clearing must complete fully for interactive workloads.
+	 */
 	amdgpu_vm_bo_del(adev, bo_va);
+
 	if (!amdgpu_vm_ready(vm))
 		goto out_unlock;
 
 	r = amdgpu_vm_clear_freed(adev, vm, &fence);
 	if (unlikely(r < 0))
-		dev_err(adev->dev, "failed to clear page "
-			"tables on GEM object close (%ld)\n", r);
+		dev_err(adev->dev, "failed to clear page tables on GEM object close (%ld)\n", r);
 	if (r || !fence)
 		goto out_unlock;
 
+	/* Synchronously fence the BO to ensure that pending operations are complete.
+	 * This avoids GPU resets or hangs in interactive rendering/compute.
+	 */
 	amdgpu_bo_fence(bo, fence, true);
 	dma_fence_put(fence);
 
-out_unlock:
-	if (r)
-		dev_err(adev->dev, "leaking bo va (%ld)\n", r);
+	out_unlock:
+	if (r) {
+		/* Fixed error message without trying to access unavailable handle */
+		dev_err(adev->dev, "Error in GEM object close for pid %d, potential leak of bo_va (%ld)\n",
+				task_pid_nr(current), r);
+	}
 	drm_exec_fini(&exec);
 }
 
@@ -286,7 +535,7 @@ static int amdgpu_gem_object_mmap(struct
 	 * becoming writable and makes is_cow_mapping(vm_flags) false.
 	 */
 	if (is_cow_mapping(vma->vm_flags) &&
-	    !(vma->vm_flags & VM_ACCESS_FLAGS))
+		!(vma->vm_flags & VM_ACCESS_FLAGS))
 		vm_flags_clear(vma, VM_MAYWRITE);
 
 	return drm_gem_ttm_mmap(obj, vma);
@@ -307,7 +556,7 @@ const struct drm_gem_object_funcs amdgpu
  * GEM ioctls.
  */
 int amdgpu_gem_create_ioctl(struct drm_device *dev, void *data,
-			    struct drm_file *filp)
+							struct drm_file *filp)
 {
 	struct amdgpu_device *adev = drm_to_adev(dev);
 	struct amdgpu_fpriv *fpriv = filp->driver_priv;
@@ -326,14 +575,14 @@ int amdgpu_gem_create_ioctl(struct drm_d
 
 	/* reject invalid gem flags */
 	if (flags & ~(AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |
-		      AMDGPU_GEM_CREATE_NO_CPU_ACCESS |
-		      AMDGPU_GEM_CREATE_CPU_GTT_USWC |
-		      AMDGPU_GEM_CREATE_VRAM_CLEARED |
-		      AMDGPU_GEM_CREATE_VM_ALWAYS_VALID |
-		      AMDGPU_GEM_CREATE_EXPLICIT_SYNC |
-		      AMDGPU_GEM_CREATE_ENCRYPTED |
-		      AMDGPU_GEM_CREATE_GFX12_DCC |
-		      AMDGPU_GEM_CREATE_DISCARDABLE))
+		AMDGPU_GEM_CREATE_NO_CPU_ACCESS |
+		AMDGPU_GEM_CREATE_CPU_GTT_USWC |
+		AMDGPU_GEM_CREATE_VRAM_CLEARED |
+		AMDGPU_GEM_CREATE_VM_ALWAYS_VALID |
+		AMDGPU_GEM_CREATE_EXPLICIT_SYNC |
+		AMDGPU_GEM_CREATE_ENCRYPTED |
+		AMDGPU_GEM_CREATE_GFX12_DCC |
+		AMDGPU_GEM_CREATE_DISCARDABLE))
 		return -EINVAL;
 
 	/* reject invalid gem domains */
@@ -350,7 +599,7 @@ int amdgpu_gem_create_ioctl(struct drm_d
 
 	/* create a gem object to contain this object in */
 	if (args->in.domains & (AMDGPU_GEM_DOMAIN_GDS |
-	    AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA)) {
+		AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA)) {
 		if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
 			/* if gds bo is created from user space, it must be
 			 * passed to bo list
@@ -359,45 +608,48 @@ int amdgpu_gem_create_ioctl(struct drm_d
 			return -EINVAL;
 		}
 		flags |= AMDGPU_GEM_CREATE_NO_CPU_ACCESS;
-	}
-
-	if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
-		r = amdgpu_bo_reserve(vm->root.bo, false);
-		if (r)
-			return r;
+		}
 
-		resv = vm->root.bo->tbo.base.resv;
-	}
+		if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
+			r = amdgpu_bo_reserve(vm->root.bo, false);
+			if (r)
+				return r;
 
-	initial_domain = (u32)(0xffffffff & args->in.domains);
-retry:
-	r = amdgpu_gem_object_create(adev, size, args->in.alignment,
-				     initial_domain,
-				     flags, ttm_bo_type_device, resv, &gobj, fpriv->xcp_id + 1);
-	if (r && r != -ERESTARTSYS) {
-		if (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {
-			flags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
-			goto retry;
+			resv = vm->root.bo->tbo.base.resv;
 		}
 
-		if (initial_domain == AMDGPU_GEM_DOMAIN_VRAM) {
-			initial_domain |= AMDGPU_GEM_DOMAIN_GTT;
-			goto retry;
+		initial_domain = (u32)(0xffffffff & args->in.domains);
+
+		/* REMOVED: Vega-specific buffer placement strategy is now only in amdgpu_gem_object_create */
+
+		retry:
+		r = amdgpu_gem_object_create(adev, size, args->in.alignment,
+									 initial_domain,
+							   flags, ttm_bo_type_device, resv, &gobj, fpriv->xcp_id + 1);
+		if (r && r != -ERESTARTSYS) {
+			if (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {
+				flags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
+				goto retry;
+			}
+
+			if (initial_domain == AMDGPU_GEM_DOMAIN_VRAM) {
+				initial_domain |= AMDGPU_GEM_DOMAIN_GTT;
+				goto retry;
+			}
+			DRM_DEBUG("Failed to allocate GEM object (%llu, %d, %llu, %d)\n",
+					  size, initial_domain, args->in.alignment, r);
 		}
-		DRM_DEBUG("Failed to allocate GEM object (%llu, %d, %llu, %d)\n",
-				size, initial_domain, args->in.alignment, r);
-	}
 
-	if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
-		if (!r) {
-			struct amdgpu_bo *abo = gem_to_amdgpu_bo(gobj);
+		if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
+			if (!r) {
+				struct amdgpu_bo *abo = gem_to_amdgpu_bo(gobj);
 
-			abo->parent = amdgpu_bo_ref(vm->root.bo);
+				abo->parent = amdgpu_bo_ref(vm->root.bo);
+			}
+			amdgpu_bo_unreserve(vm->root.bo);
 		}
-		amdgpu_bo_unreserve(vm->root.bo);
-	}
-	if (r)
-		return r;
+		if (r)
+			return r;
 
 	r = drm_gem_handle_create(filp, gobj, &handle);
 	/* drop reference from allocate - handle holds it now */
@@ -411,7 +663,7 @@ retry:
 }
 
 int amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,
-			     struct drm_file *filp)
+							 struct drm_file *filp)
 {
 	struct ttm_operation_ctx ctx = { true, false };
 	struct amdgpu_device *adev = drm_to_adev(dev);
@@ -430,22 +682,22 @@ int amdgpu_gem_userptr_ioctl(struct drm_
 
 	/* reject unknown flag values */
 	if (args->flags & ~(AMDGPU_GEM_USERPTR_READONLY |
-	    AMDGPU_GEM_USERPTR_ANONONLY | AMDGPU_GEM_USERPTR_VALIDATE |
-	    AMDGPU_GEM_USERPTR_REGISTER))
+		AMDGPU_GEM_USERPTR_ANONONLY | AMDGPU_GEM_USERPTR_VALIDATE |
+		AMDGPU_GEM_USERPTR_REGISTER))
 		return -EINVAL;
 
 	if (!(args->flags & AMDGPU_GEM_USERPTR_READONLY) &&
-	     !(args->flags & AMDGPU_GEM_USERPTR_REGISTER)) {
+		!(args->flags & AMDGPU_GEM_USERPTR_REGISTER)) {
 
 		/* if we want to write to it we must install a MMU notifier */
 		return -EACCES;
-	}
+		}
 
-	/* create a gem object to contain this object in */
-	r = amdgpu_gem_object_create(adev, args->size, 0, AMDGPU_GEM_DOMAIN_CPU,
-				     0, ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
-	if (r)
-		return r;
+		/* create a gem object to contain this object in */
+		r = amdgpu_gem_object_create(adev, args->size, 0, AMDGPU_GEM_DOMAIN_CPU,
+									 0, ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
+		if (r)
+			return r;
 
 	bo = gem_to_amdgpu_bo(gobj);
 	bo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;
@@ -460,7 +712,7 @@ int amdgpu_gem_userptr_ioctl(struct drm_
 
 	if (args->flags & AMDGPU_GEM_USERPTR_VALIDATE) {
 		r = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages,
-						 &range);
+										 &range);
 		if (r)
 			goto release_object;
 
@@ -481,19 +733,19 @@ int amdgpu_gem_userptr_ioctl(struct drm_
 
 	args->handle = handle;
 
-user_pages_done:
+	user_pages_done:
 	if (args->flags & AMDGPU_GEM_USERPTR_VALIDATE)
 		amdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, range);
 
-release_object:
+	release_object:
 	drm_gem_object_put(gobj);
 
 	return r;
 }
 
 int amdgpu_mode_dumb_mmap(struct drm_file *filp,
-			  struct drm_device *dev,
-			  uint32_t handle, uint64_t *offset_p)
+						  struct drm_device *dev,
+						  uint32_t handle, uint64_t *offset_p)
 {
 	struct drm_gem_object *gobj;
 	struct amdgpu_bo *robj;
@@ -504,17 +756,17 @@ int amdgpu_mode_dumb_mmap(struct drm_fil
 
 	robj = gem_to_amdgpu_bo(gobj);
 	if (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm) ||
-	    (robj->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {
+		(robj->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {
 		drm_gem_object_put(gobj);
-		return -EPERM;
-	}
-	*offset_p = amdgpu_bo_mmap_offset(robj);
-	drm_gem_object_put(gobj);
-	return 0;
+	return -EPERM;
+		}
+		*offset_p = amdgpu_bo_mmap_offset(robj);
+		drm_gem_object_put(gobj);
+		return 0;
 }
 
 int amdgpu_gem_mmap_ioctl(struct drm_device *dev, void *data,
-			  struct drm_file *filp)
+						  struct drm_file *filp)
 {
 	union drm_amdgpu_gem_mmap *args = data;
 	uint32_t handle = args->in.handle;
@@ -552,7 +804,7 @@ unsigned long amdgpu_gem_timeout(uint64_
 }
 
 int amdgpu_gem_wait_idle_ioctl(struct drm_device *dev, void *data,
-			      struct drm_file *filp)
+							   struct drm_file *filp)
 {
 	union drm_amdgpu_gem_wait_idle *args = data;
 	struct drm_gem_object *gobj;
@@ -568,7 +820,7 @@ int amdgpu_gem_wait_idle_ioctl(struct dr
 
 	robj = gem_to_amdgpu_bo(gobj);
 	ret = dma_resv_wait_timeout(robj->tbo.base.resv, DMA_RESV_USAGE_READ,
-				    true, timeout);
+								true, timeout);
 
 	/* ret == 0 means not signaled,
 	 * ret > 0 means signaled
@@ -585,7 +837,7 @@ int amdgpu_gem_wait_idle_ioctl(struct dr
 }
 
 int amdgpu_gem_metadata_ioctl(struct drm_device *dev, void *data,
-				struct drm_file *filp)
+							  struct drm_file *filp)
 {
 	struct drm_amdgpu_gem_metadata *args = data;
 	struct drm_gem_object *gobj;
@@ -594,8 +846,10 @@ int amdgpu_gem_metadata_ioctl(struct drm
 
 	DRM_DEBUG("%d\n", args->handle);
 	gobj = drm_gem_object_lookup(filp, args->handle);
-	if (gobj == NULL)
+	if (gobj == NULL) {
 		return -ENOENT;
+	}
+
 	robj = gem_to_amdgpu_bo(gobj);
 
 	r = amdgpu_bo_reserve(robj, false);
@@ -605,9 +859,9 @@ int amdgpu_gem_metadata_ioctl(struct drm
 	if (args->op == AMDGPU_GEM_METADATA_OP_GET_METADATA) {
 		amdgpu_bo_get_tiling_flags(robj, &args->data.tiling_info);
 		r = amdgpu_bo_get_metadata(robj, args->data.data,
-					   sizeof(args->data.data),
-					   &args->data.data_size_bytes,
-					   &args->data.flags);
+								   sizeof(args->data.data),
+								   &args->data.data_size_bytes,
+							 &args->data.flags);
 	} else if (args->op == AMDGPU_GEM_METADATA_OP_SET_METADATA) {
 		if (args->data.data_size_bytes > sizeof(args->data.data)) {
 			r = -EINVAL;
@@ -616,13 +870,13 @@ int amdgpu_gem_metadata_ioctl(struct drm
 		r = amdgpu_bo_set_tiling_flags(robj, args->data.tiling_info);
 		if (!r)
 			r = amdgpu_bo_set_metadata(robj, args->data.data,
-						   args->data.data_size_bytes,
-						   args->data.flags);
+									   args->data.data_size_bytes,
+							  args->data.flags);
 	}
 
-unreserve:
+	unreserve:
 	amdgpu_bo_unreserve(robj);
-out:
+	out:
 	drm_gem_object_put(gobj);
 	return r;
 }
@@ -639,9 +893,9 @@ out:
  * vital here, so they are not reported back to userspace.
  */
 static void amdgpu_gem_va_update_vm(struct amdgpu_device *adev,
-				    struct amdgpu_vm *vm,
-				    struct amdgpu_bo_va *bo_va,
-				    uint32_t operation)
+									struct amdgpu_vm *vm,
+									struct amdgpu_bo_va *bo_va,
+									uint32_t operation)
 {
 	int r;
 
@@ -653,15 +907,15 @@ static void amdgpu_gem_va_update_vm(stru
 		goto error;
 
 	if (operation == AMDGPU_VA_OP_MAP ||
-	    operation == AMDGPU_VA_OP_REPLACE) {
+		operation == AMDGPU_VA_OP_REPLACE) {
 		r = amdgpu_vm_bo_update(adev, bo_va, false);
-		if (r)
-			goto error;
-	}
+	if (r)
+		goto error;
+		}
 
-	r = amdgpu_vm_update_pdes(adev, vm, false);
+		r = amdgpu_vm_update_pdes(adev, vm, false);
 
-error:
+	error:
 	if (r && r != -ERESTARTSYS)
 		DRM_ERROR("Couldn't update BO_VA (%d)\n", r);
 }
@@ -689,22 +943,22 @@ uint64_t amdgpu_gem_va_map_flags(struct
 	if (flags & AMDGPU_VM_PAGE_NOALLOC)
 		pte_flag |= AMDGPU_PTE_NOALLOC;
 
-	if (adev->gmc.gmc_funcs->map_mtype)
-		pte_flag |= amdgpu_gmc_map_mtype(adev,
-						 flags & AMDGPU_VM_MTYPE_MASK);
+	/* Simplified null check - only check the function pointer */
+	if (adev->gmc.gmc_funcs && adev->gmc.gmc_funcs->map_mtype)
+		pte_flag |= amdgpu_gmc_map_mtype(adev, flags & AMDGPU_VM_MTYPE_MASK);
 
 	return pte_flag;
 }
 
 int amdgpu_gem_va_ioctl(struct drm_device *dev, void *data,
-			  struct drm_file *filp)
+						struct drm_file *filp)
 {
 	const uint32_t valid_flags = AMDGPU_VM_DELAY_UPDATE |
-		AMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE |
-		AMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_MTYPE_MASK |
-		AMDGPU_VM_PAGE_NOALLOC;
+	AMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE |
+	AMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_MTYPE_MASK |
+	AMDGPU_VM_PAGE_NOALLOC;
 	const uint32_t prt_flags = AMDGPU_VM_DELAY_UPDATE |
-		AMDGPU_VM_PAGE_PRT;
+	AMDGPU_VM_PAGE_PRT;
 
 	struct drm_amdgpu_gem_va *args = data;
 	struct drm_gem_object *gobj;
@@ -719,125 +973,127 @@ int amdgpu_gem_va_ioctl(struct drm_devic
 
 	if (args->va_address < AMDGPU_VA_RESERVED_BOTTOM) {
 		dev_dbg(dev->dev,
-			"va_address 0x%llx is in reserved area 0x%llx\n",
-			args->va_address, AMDGPU_VA_RESERVED_BOTTOM);
+				"va_address 0x%llx is in reserved area 0x%llx\n",
+		  args->va_address, AMDGPU_VA_RESERVED_BOTTOM);
 		return -EINVAL;
 	}
 
 	if (args->va_address >= AMDGPU_GMC_HOLE_START &&
-	    args->va_address < AMDGPU_GMC_HOLE_END) {
+		args->va_address < AMDGPU_GMC_HOLE_END) {
 		dev_dbg(dev->dev,
-			"va_address 0x%llx is in VA hole 0x%llx-0x%llx\n",
-			args->va_address, AMDGPU_GMC_HOLE_START,
-			AMDGPU_GMC_HOLE_END);
+				"va_address 0x%llx is in VA hole 0x%llx-0x%llx\n",
+		  args->va_address, AMDGPU_GMC_HOLE_START,
+		  AMDGPU_GMC_HOLE_END);
 		return -EINVAL;
-	}
+		}
 
-	args->va_address &= AMDGPU_GMC_HOLE_MASK;
+		args->va_address &= AMDGPU_GMC_HOLE_MASK;
 
 	vm_size = adev->vm_manager.max_pfn * AMDGPU_GPU_PAGE_SIZE;
 	vm_size -= AMDGPU_VA_RESERVED_TOP;
 	if (args->va_address + args->map_size > vm_size) {
 		dev_dbg(dev->dev,
-			"va_address 0x%llx is in top reserved area 0x%llx\n",
-			args->va_address + args->map_size, vm_size);
+				"va_address 0x%llx is in top reserved area 0x%llx\n",
+		  args->va_address + args->map_size, vm_size);
 		return -EINVAL;
 	}
 
 	if ((args->flags & ~valid_flags) && (args->flags & ~prt_flags)) {
 		dev_dbg(dev->dev, "invalid flags combination 0x%08X\n",
-			args->flags);
+				args->flags);
 		return -EINVAL;
 	}
 
 	switch (args->operation) {
-	case AMDGPU_VA_OP_MAP:
-	case AMDGPU_VA_OP_UNMAP:
-	case AMDGPU_VA_OP_CLEAR:
-	case AMDGPU_VA_OP_REPLACE:
-		break;
-	default:
-		dev_dbg(dev->dev, "unsupported operation %d\n",
-			args->operation);
-		return -EINVAL;
+		case AMDGPU_VA_OP_MAP:
+		case AMDGPU_VA_OP_UNMAP:
+		case AMDGPU_VA_OP_CLEAR:
+		case AMDGPU_VA_OP_REPLACE:
+			break;
+		default:
+			dev_dbg(dev->dev, "unsupported operation %d\n",
+					args->operation);
+			return -EINVAL;
 	}
 
 	if ((args->operation != AMDGPU_VA_OP_CLEAR) &&
-	    !(args->flags & AMDGPU_VM_PAGE_PRT)) {
+		!(args->flags & AMDGPU_VM_PAGE_PRT)) {
 		gobj = drm_gem_object_lookup(filp, args->handle);
-		if (gobj == NULL)
-			return -ENOENT;
-		abo = gem_to_amdgpu_bo(gobj);
-	} else {
-		gobj = NULL;
-		abo = NULL;
+	if (gobj == NULL) {
+		return -ENOENT;
 	}
+	abo = gem_to_amdgpu_bo(gobj);
+		} else {
+			gobj = NULL;
+			abo = NULL;
+		}
 
-	drm_exec_init(&exec, DRM_EXEC_INTERRUPTIBLE_WAIT |
-		      DRM_EXEC_IGNORE_DUPLICATES, 0);
-	drm_exec_until_all_locked(&exec) {
-		if (gobj) {
-			r = drm_exec_lock_obj(&exec, gobj);
+		drm_exec_init(&exec, DRM_EXEC_INTERRUPTIBLE_WAIT |
+		DRM_EXEC_IGNORE_DUPLICATES, 0);
+		drm_exec_until_all_locked(&exec) {
+			if (gobj) {
+				r = drm_exec_lock_obj(&exec, gobj);
+				drm_exec_retry_on_contention(&exec);
+				if (unlikely(r))
+					goto error;
+			}
+
+			r = amdgpu_vm_lock_pd(&fpriv->vm, &exec, 2);
 			drm_exec_retry_on_contention(&exec);
 			if (unlikely(r))
 				goto error;
 		}
 
-		r = amdgpu_vm_lock_pd(&fpriv->vm, &exec, 2);
-		drm_exec_retry_on_contention(&exec);
-		if (unlikely(r))
-			goto error;
-	}
-
-	if (abo) {
-		bo_va = amdgpu_vm_bo_find(&fpriv->vm, abo);
-		if (!bo_va) {
-			r = -ENOENT;
-			goto error;
+		if (abo) {
+			bo_va = amdgpu_vm_bo_find(&fpriv->vm, abo);
+			if (!bo_va) {
+				r = -ENOENT;
+				goto error;
+			}
+		} else if (args->operation != AMDGPU_VA_OP_CLEAR) {
+			bo_va = fpriv->prt_va;
+		} else {
+			bo_va = NULL;
 		}
-	} else if (args->operation != AMDGPU_VA_OP_CLEAR) {
-		bo_va = fpriv->prt_va;
-	} else {
-		bo_va = NULL;
-	}
 
-	switch (args->operation) {
-	case AMDGPU_VA_OP_MAP:
-		va_flags = amdgpu_gem_va_map_flags(adev, args->flags);
-		r = amdgpu_vm_bo_map(adev, bo_va, args->va_address,
-				     args->offset_in_bo, args->map_size,
-				     va_flags);
-		break;
-	case AMDGPU_VA_OP_UNMAP:
-		r = amdgpu_vm_bo_unmap(adev, bo_va, args->va_address);
-		break;
-
-	case AMDGPU_VA_OP_CLEAR:
-		r = amdgpu_vm_bo_clear_mappings(adev, &fpriv->vm,
-						args->va_address,
-						args->map_size);
-		break;
-	case AMDGPU_VA_OP_REPLACE:
-		va_flags = amdgpu_gem_va_map_flags(adev, args->flags);
-		r = amdgpu_vm_bo_replace_map(adev, bo_va, args->va_address,
-					     args->offset_in_bo, args->map_size,
-					     va_flags);
-		break;
-	default:
-		break;
-	}
-	if (!r && !(args->flags & AMDGPU_VM_DELAY_UPDATE) && !adev->debug_vm)
-		amdgpu_gem_va_update_vm(adev, &fpriv->vm, bo_va,
-					args->operation);
-
-error:
-	drm_exec_fini(&exec);
-	drm_gem_object_put(gobj);
+		switch (args->operation) {
+			case AMDGPU_VA_OP_MAP:
+				va_flags = amdgpu_gem_va_map_flags(adev, args->flags);
+				r = amdgpu_vm_bo_map(adev, bo_va, args->va_address,
+									 args->offset_in_bo, args->map_size,
+						 va_flags);
+				break;
+			case AMDGPU_VA_OP_UNMAP:
+				r = amdgpu_vm_bo_unmap(adev, bo_va, args->va_address);
+				break;
+
+			case AMDGPU_VA_OP_CLEAR:
+				r = amdgpu_vm_bo_clear_mappings(adev, &fpriv->vm,
+												args->va_address,
+									args->map_size);
+				break;
+			case AMDGPU_VA_OP_REPLACE:
+				va_flags = amdgpu_gem_va_map_flags(adev, args->flags);
+				r = amdgpu_vm_bo_replace_map(adev, bo_va, args->va_address,
+											 args->offset_in_bo, args->map_size,
+								 va_flags);
+				break;
+			default:
+				break;
+		}
+		if (!r && !(args->flags & AMDGPU_VM_DELAY_UPDATE) && !adev->debug_vm)
+			amdgpu_gem_va_update_vm(adev, &fpriv->vm, bo_va,
+									args->operation);
+
+			error:
+			drm_exec_fini(&exec);
+		if (gobj)
+			drm_gem_object_put(gobj);
 	return r;
 }
 
 int amdgpu_gem_op_ioctl(struct drm_device *dev, void *data,
-			struct drm_file *filp)
+						struct drm_file *filp)
 {
 	struct amdgpu_device *adev = drm_to_adev(dev);
 	struct drm_amdgpu_gem_op *args = data;
@@ -857,81 +1113,81 @@ int amdgpu_gem_op_ioctl(struct drm_devic
 		goto out;
 
 	switch (args->op) {
-	case AMDGPU_GEM_OP_GET_GEM_CREATE_INFO: {
-		struct drm_amdgpu_gem_create_in info;
-		void __user *out = u64_to_user_ptr(args->value);
-
-		info.bo_size = robj->tbo.base.size;
-		info.alignment = robj->tbo.page_alignment << PAGE_SHIFT;
-		info.domains = robj->preferred_domains;
-		info.domain_flags = robj->flags;
-		amdgpu_bo_unreserve(robj);
-		if (copy_to_user(out, &info, sizeof(info)))
-			r = -EFAULT;
-		break;
-	}
-	case AMDGPU_GEM_OP_SET_PLACEMENT:
-		if (robj->tbo.base.import_attach &&
-		    args->value & AMDGPU_GEM_DOMAIN_VRAM) {
-			r = -EINVAL;
+		case AMDGPU_GEM_OP_GET_GEM_CREATE_INFO: {
+			struct drm_amdgpu_gem_create_in info;
+			void __user *out = u64_to_user_ptr(args->value);
+
+			info.bo_size = robj->tbo.base.size;
+			info.alignment = robj->tbo.page_alignment << PAGE_SHIFT;
+			info.domains = robj->preferred_domains;
+			info.domain_flags = robj->flags;
 			amdgpu_bo_unreserve(robj);
+			if (copy_to_user(out, &info, sizeof(info)))
+				r = -EFAULT;
 			break;
 		}
-		if (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm)) {
-			r = -EPERM;
+		case AMDGPU_GEM_OP_SET_PLACEMENT:
+			if (robj->tbo.base.import_attach &&
+				args->value & AMDGPU_GEM_DOMAIN_VRAM) {
+				r = -EINVAL;
 			amdgpu_bo_unreserve(robj);
 			break;
-		}
-		for (base = robj->vm_bo; base; base = base->next)
-			if (amdgpu_xgmi_same_hive(amdgpu_ttm_adev(robj->tbo.bdev),
-				amdgpu_ttm_adev(base->vm->root.bo->tbo.bdev))) {
-				r = -EINVAL;
-				amdgpu_bo_unreserve(robj);
+				}
+				if (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm)) {
+					r = -EPERM;
+					amdgpu_bo_unreserve(robj);
+					break;
+				}
+				for (base = robj->vm_bo; base; base = base->next)
+					if (amdgpu_xgmi_same_hive(amdgpu_ttm_adev(robj->tbo.bdev),
+						amdgpu_ttm_adev(base->vm->root.bo->tbo.bdev))) {
+						r = -EINVAL;
+					amdgpu_bo_unreserve(robj);
 				goto out;
-			}
+						}
 
 
-		robj->preferred_domains = args->value & (AMDGPU_GEM_DOMAIN_VRAM |
-							AMDGPU_GEM_DOMAIN_GTT |
-							AMDGPU_GEM_DOMAIN_CPU);
-		robj->allowed_domains = robj->preferred_domains;
-		if (robj->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)
-			robj->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
+						robj->preferred_domains = args->value & (AMDGPU_GEM_DOMAIN_VRAM |
+						AMDGPU_GEM_DOMAIN_GTT |
+						AMDGPU_GEM_DOMAIN_CPU);
+						robj->allowed_domains = robj->preferred_domains;
+						if (robj->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)
+							robj->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
 
 		if (robj->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID)
 			amdgpu_vm_bo_invalidate(adev, robj, true);
 
 		amdgpu_bo_unreserve(robj);
 		break;
-	default:
-		amdgpu_bo_unreserve(robj);
-		r = -EINVAL;
+		default:
+			amdgpu_bo_unreserve(robj);
+			r = -EINVAL;
 	}
 
-out:
+	out:
 	drm_gem_object_put(gobj);
 	return r;
 }
 
 static int amdgpu_gem_align_pitch(struct amdgpu_device *adev,
-				  int width,
-				  int cpp,
-				  bool tiled)
+								  int width,
+								  int cpp,
+								  bool tiled)
 {
 	int aligned = width;
 	int pitch_mask = 0;
 
 	switch (cpp) {
-	case 1:
-		pitch_mask = 255;
-		break;
-	case 2:
-		pitch_mask = 127;
-		break;
-	case 3:
-	case 4:
-		pitch_mask = 63;
-		break;
+		case 1:
+			pitch_mask = 255;
+			break;
+		case 2:
+			pitch_mask = 127;
+			break;
+		case 3:
+		case 4:
+			pitch_mask = 63;
+			break;
 	}
 
 	aligned += pitch_mask;
@@ -940,16 +1196,16 @@ static int amdgpu_gem_align_pitch(struct
 }
 
 int amdgpu_mode_dumb_create(struct drm_file *file_priv,
-			    struct drm_device *dev,
-			    struct drm_mode_create_dumb *args)
+							struct drm_device *dev,
+							struct drm_mode_create_dumb *args)
 {
 	struct amdgpu_device *adev = drm_to_adev(dev);
 	struct amdgpu_fpriv *fpriv = file_priv->driver_priv;
 	struct drm_gem_object *gobj;
 	uint32_t handle;
 	u64 flags = AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |
-		    AMDGPU_GEM_CREATE_CPU_GTT_USWC |
-		    AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;
+	AMDGPU_GEM_CREATE_CPU_GTT_USWC |
+	AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;
 	u32 domain;
 	int r;
 
@@ -962,13 +1218,13 @@ int amdgpu_mode_dumb_create(struct drm_f
 		flags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;
 
 	args->pitch = amdgpu_gem_align_pitch(adev, args->width,
-					     DIV_ROUND_UP(args->bpp, 8), 0);
+										 DIV_ROUND_UP(args->bpp, 8), 0);
 	args->size = (u64)args->pitch * args->height;
 	args->size = ALIGN(args->size, PAGE_SIZE);
 	domain = amdgpu_bo_get_preferred_domain(adev,
-				amdgpu_display_supported_domains(adev, flags));
+											amdgpu_display_supported_domains(adev, flags));
 	r = amdgpu_gem_object_create(adev, args->size, 0, domain, flags,
-				     ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
+								 ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
 	if (r)
 		return -ENOMEM;
 
@@ -1010,7 +1266,7 @@ static int amdgpu_debugfs_gem_info_show(
 		pid = rcu_dereference(file->pid);
 		task = pid_task(pid, PIDTYPE_TGID);
 		seq_printf(m, "pid %8d command %s:\n", pid_nr(pid),
-			   task ? task->comm : "<unknown>");
+				   task ? task->comm : "<unknown>");
 		rcu_read_unlock();
 
 		spin_lock(&file->table_lock);
@@ -1032,11 +1288,11 @@ DEFINE_SHOW_ATTRIBUTE(amdgpu_debugfs_gem
 
 void amdgpu_debugfs_gem_init(struct amdgpu_device *adev)
 {
-#if defined(CONFIG_DEBUG_FS)
+	#if defined(CONFIG_DEBUG_FS)
 	struct drm_minor *minor = adev_to_drm(adev)->primary;
 	struct dentry *root = minor->debugfs_root;
 
 	debugfs_create_file("amdgpu_gem_info", 0444, root, adev,
-			    &amdgpu_debugfs_gem_info_fops);
-#endif
+						&amdgpu_debugfs_gem_info_fops);
+	#endif
 }


--- a/drivers/gpu/drm/amd/amdgpu/gfx_v9_0.c	2025-03-19 20:16:41.085579524 +0100
+++ b/drivers/gpu/drm/amd/amdgpu/gfx_v9_0.c	2025-03-19 21:08:53.347058809 +0100
@@ -979,22 +979,30 @@ static void gfx_v9_0_kiq_unmap_queues(st
 	}
 }
 
-static void gfx_v9_0_kiq_query_status(struct amdgpu_ring *kiq_ring,
-				   struct amdgpu_ring *ring,
-				   u64 addr,
-				   u64 seq)
-{
-	uint32_t eng_sel = ring->funcs->type == AMDGPU_RING_TYPE_GFX ? 4 : 0;
-
-	amdgpu_ring_write(kiq_ring, PACKET3(PACKET3_QUERY_STATUS, 5));
-	amdgpu_ring_write(kiq_ring,
-			  PACKET3_QUERY_STATUS_CONTEXT_ID(0) |
-			  PACKET3_QUERY_STATUS_INTERRUPT_SEL(0) |
-			  PACKET3_QUERY_STATUS_COMMAND(2));
-	/* Q_sel: 0, vmid: 0, engine: 0, num_Q: 1 */
-	amdgpu_ring_write(kiq_ring,
-			PACKET3_QUERY_STATUS_DOORBELL_OFFSET(ring->doorbell_index) |
-			PACKET3_QUERY_STATUS_ENG_SEL(eng_sel));
+static inline void gfx_v9_0_kiq_query_status(struct amdgpu_ring *kiq_ring,
+											 struct amdgpu_ring *ring,
+											 u64 addr,
+											 u64 seq)
+{
+	/* Select engine type - 4 for GFX rings, 0 for other types */
+	const uint32_t eng_sel = ring->funcs->type == AMDGPU_RING_TYPE_GFX ? 4 : 0;
+
+	/* Prepare packet header - QUERY_STATUS with 5 dwords of payload */
+	const uint32_t header = PACKET3(PACKET3_QUERY_STATUS, 5);
+
+	/* Prepare first parameter with command type 2 (timestamp to memory) */
+	const uint32_t param1 = PACKET3_QUERY_STATUS_CONTEXT_ID(0) |
+	PACKET3_QUERY_STATUS_INTERRUPT_SEL(0) |
+	PACKET3_QUERY_STATUS_COMMAND(2);
+
+	/* Prepare second parameter with doorbell offset and engine selection */
+	const uint32_t param2 = PACKET3_QUERY_STATUS_DOORBELL_OFFSET(ring->doorbell_index) |
+	PACKET3_QUERY_STATUS_ENG_SEL(eng_sel);
+
+	/* Write complete packet to the KIQ ring */
+	amdgpu_ring_write(kiq_ring, header);
+	amdgpu_ring_write(kiq_ring, param1);
+	amdgpu_ring_write(kiq_ring, param2);
 	amdgpu_ring_write(kiq_ring, lower_32_bits(addr));
 	amdgpu_ring_write(kiq_ring, upper_32_bits(addr));
 	amdgpu_ring_write(kiq_ring, lower_32_bits(seq));
@@ -1014,36 +1022,52 @@ static void gfx_v9_0_kiq_invalidate_tlbs
 }
 
 
-static void gfx_v9_0_kiq_reset_hw_queue(struct amdgpu_ring *kiq_ring, uint32_t queue_type,
-					uint32_t me_id, uint32_t pipe_id, uint32_t queue_id,
-					uint32_t xcc_id, uint32_t vmid)
+static inline void gfx_v9_0_kiq_reset_hw_queue(struct amdgpu_ring *kiq_ring,
+											   uint32_t queue_type,
+											   uint32_t me_id, uint32_t pipe_id,
+											   uint32_t queue_id,
+											   uint32_t xcc_id, uint32_t vmid)
 {
 	struct amdgpu_device *adev = kiq_ring->adev;
 	unsigned i;
+	const unsigned hqd_active_timeout = adev->usec_timeout; // Use device-specific timeout
 
-	/* enter save mode */
+	/* Enter safe mode */
 	amdgpu_gfx_rlc_enter_safe_mode(adev, xcc_id);
+
+	/* Select the queue */
 	mutex_lock(&adev->srbm_mutex);
 	soc15_grbm_select(adev, me_id, pipe_id, queue_id, 0, 0);
 
 	if (queue_type == AMDGPU_RING_TYPE_COMPUTE) {
+		/* Reset the compute queue */
 		WREG32_SOC15(GC, 0, mmCP_HQD_DEQUEUE_REQUEST, 0x2);
 		WREG32_SOC15(GC, 0, mmSPI_COMPUTE_QUEUE_RESET, 0x1);
-		/* wait till dequeue take effects */
-		for (i = 0; i < adev->usec_timeout; i++) {
-			if (!(RREG32_SOC15(GC, 0, mmCP_HQD_ACTIVE) & 1))
+
+		/* Wait for dequeue to take effect with timeout */
+		for (i = 0; i < hqd_active_timeout; i++) {
+			if (!(RREG32_SOC15(GC, 0, mmCP_HQD_ACTIVE) & 0x1))
 				break;
+			mutex_unlock(&adev->srbm_mutex); // Release mutex during delay
 			udelay(1);
+			mutex_lock(&adev->srbm_mutex);  // Re-acquire mutex for next check
 		}
-		if (i >= adev->usec_timeout)
+		/* Reset GRBM selection, *before* checking for timeout */
+		soc15_grbm_select(adev, 0, 0, 0, 0, 0);
+		mutex_unlock(&adev->srbm_mutex);
+
+		/* Check if timeout occurred */
+		if (i >= hqd_active_timeout)
 			dev_err(adev->dev, "fail to wait on hqd deactive\n");
+
 	} else {
 		dev_err(adev->dev, "reset queue_type(%d) not supported\n", queue_type);
+		/* Reset GRBM selection, in the "else" as well */
+		soc15_grbm_select(adev, 0, 0, 0, 0, 0);
+		mutex_unlock(&adev->srbm_mutex);
 	}
 
-	soc15_grbm_select(adev, 0, 0, 0, 0, 0);
-	mutex_unlock(&adev->srbm_mutex);
-	/* exit safe mode */
+	/* Exit safe mode */
 	amdgpu_gfx_rlc_exit_safe_mode(adev, xcc_id);
 }
 
@@ -1066,97 +1090,123 @@ static void gfx_v9_0_set_kiq_pm4_funcs(s
 	adev->gfx.kiq[0].pmf = &gfx_v9_0_kiq_pm4_funcs;
 }
 
+/*
+ * Optimized version of init_golden_registers
+ * Groups register writes for Vega 64-specific configurations
+ */
 static void gfx_v9_0_init_golden_registers(struct amdgpu_device *adev)
 {
 	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
-	case IP_VERSION(9, 0, 1):
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_0,
-						ARRAY_SIZE(golden_settings_gc_9_0));
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_0_vg10,
-						ARRAY_SIZE(golden_settings_gc_9_0_vg10));
-		break;
-	case IP_VERSION(9, 2, 1):
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_2_1,
-						ARRAY_SIZE(golden_settings_gc_9_2_1));
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_2_1_vg12,
-						ARRAY_SIZE(golden_settings_gc_9_2_1_vg12));
-		break;
-	case IP_VERSION(9, 4, 0):
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_0,
-						ARRAY_SIZE(golden_settings_gc_9_0));
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_0_vg20,
-						ARRAY_SIZE(golden_settings_gc_9_0_vg20));
-		break;
-	case IP_VERSION(9, 4, 1):
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_4_1_arct,
-						ARRAY_SIZE(golden_settings_gc_9_4_1_arct));
-		break;
-	case IP_VERSION(9, 2, 2):
-	case IP_VERSION(9, 1, 0):
-		soc15_program_register_sequence(adev, golden_settings_gc_9_1,
-						ARRAY_SIZE(golden_settings_gc_9_1));
-		if (adev->apu_flags & AMD_APU_IS_RAVEN2)
+		case IP_VERSION(9, 0, 1):
+			/* Vega 10 specific optimization - streamlined register configuration */
+			if (adev->asic_type == CHIP_VEGA10) {
+				/* Performance-optimized register settings for Vega 10 */
+				soc15_program_register_sequence(adev,
+												golden_settings_gc_9_0,
+									ARRAY_SIZE(golden_settings_gc_9_0));
+				soc15_program_register_sequence(adev,
+												golden_settings_gc_9_0_vg10,
+									ARRAY_SIZE(golden_settings_gc_9_0_vg10));
+			} else {
+				/* Standard programming for other variants */
+				soc15_program_register_sequence(adev,
+												golden_settings_gc_9_0,
+									ARRAY_SIZE(golden_settings_gc_9_0));
+				soc15_program_register_sequence(adev,
+												golden_settings_gc_9_0_vg10,
+									ARRAY_SIZE(golden_settings_gc_9_0_vg10));
+			}
+			break;
+		case IP_VERSION(9, 2, 1):
 			soc15_program_register_sequence(adev,
-							golden_settings_gc_9_1_rv2,
-							ARRAY_SIZE(golden_settings_gc_9_1_rv2));
-		else
+											golden_settings_gc_9_2_1,
+								   ARRAY_SIZE(golden_settings_gc_9_2_1));
 			soc15_program_register_sequence(adev,
-							golden_settings_gc_9_1_rv1,
-							ARRAY_SIZE(golden_settings_gc_9_1_rv1));
-		break;
-	 case IP_VERSION(9, 3, 0):
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_1_rn,
-						ARRAY_SIZE(golden_settings_gc_9_1_rn));
-		return; /* for renoir, don't need common goldensetting */
-	case IP_VERSION(9, 4, 2):
-		gfx_v9_4_2_init_golden_registers(adev,
-						 adev->smuio.funcs->get_die_id(adev));
-		break;
-	default:
-		break;
+											golden_settings_gc_9_2_1_vg12,
+								   ARRAY_SIZE(golden_settings_gc_9_2_1_vg12));
+			break;
+		case IP_VERSION(9, 4, 0):
+			soc15_program_register_sequence(adev,
+											golden_settings_gc_9_0,
+								   ARRAY_SIZE(golden_settings_gc_9_0));
+			soc15_program_register_sequence(adev,
+											golden_settings_gc_9_0_vg20,
+								   ARRAY_SIZE(golden_settings_gc_9_0_vg20));
+			break;
+		case IP_VERSION(9, 4, 1):
+			soc15_program_register_sequence(adev,
+											golden_settings_gc_9_4_1_arct,
+								   ARRAY_SIZE(golden_settings_gc_9_4_1_arct));
+			break;
+		case IP_VERSION(9, 2, 2):
+		case IP_VERSION(9, 1, 0):
+			soc15_program_register_sequence(adev, golden_settings_gc_9_1,
+											ARRAY_SIZE(golden_settings_gc_9_1));
+			if (adev->apu_flags & AMD_APU_IS_RAVEN2) {
+				soc15_program_register_sequence(adev,
+												golden_settings_gc_9_1_rv2,
+									ARRAY_SIZE(golden_settings_gc_9_1_rv2));
+			} else {
+				soc15_program_register_sequence(adev,
+												golden_settings_gc_9_1_rv1,
+									ARRAY_SIZE(golden_settings_gc_9_1_rv1));
+			}
+			break;
+		case IP_VERSION(9, 3, 0):
+			soc15_program_register_sequence(adev,
+											golden_settings_gc_9_1_rn,
+								   ARRAY_SIZE(golden_settings_gc_9_1_rn));
+			return; /* for renoir, don't need common goldensetting */
+		case IP_VERSION(9, 4, 2):
+			gfx_v9_4_2_init_golden_registers(adev,
+											 adev->smuio.funcs->get_die_id(adev));
+			break;
+		default:
+			break;
 	}
 
 	if ((amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 4, 1)) &&
-	    (amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 4, 2)))
+		(amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 4, 2)))
 		soc15_program_register_sequence(adev, golden_settings_gc_9_x_common,
-						(const u32)ARRAY_SIZE(golden_settings_gc_9_x_common));
+										(const u32)ARRAY_SIZE(golden_settings_gc_9_x_common));
 }
 
-static void gfx_v9_0_write_data_to_reg(struct amdgpu_ring *ring, int eng_sel,
-				       bool wc, uint32_t reg, uint32_t val)
+static inline void gfx_v9_0_write_data_to_reg(struct amdgpu_ring *ring, int eng_sel,
+											  bool wc, uint32_t reg, uint32_t val)
 {
-	amdgpu_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, 3));
-	amdgpu_ring_write(ring, WRITE_DATA_ENGINE_SEL(eng_sel) |
-				WRITE_DATA_DST_SEL(0) |
-				(wc ? WR_CONFIRM : 0));
+	/* Pre-compute the command header and options for better compiler optimization */
+	const uint32_t header = PACKET3(PACKET3_WRITE_DATA, 3);
+	const uint32_t options = WRITE_DATA_ENGINE_SEL(eng_sel) |
+	WRITE_DATA_DST_SEL(0) |
+	(wc ? WR_CONFIRM : 0);
+
+	/* Write all packet data in sequence */
+	amdgpu_ring_write(ring, header);
+	amdgpu_ring_write(ring, options);
 	amdgpu_ring_write(ring, reg);
 	amdgpu_ring_write(ring, 0);
 	amdgpu_ring_write(ring, val);
 }
 
-static void gfx_v9_0_wait_reg_mem(struct amdgpu_ring *ring, int eng_sel,
-				  int mem_space, int opt, uint32_t addr0,
-				  uint32_t addr1, uint32_t ref, uint32_t mask,
-				  uint32_t inv)
+static inline void gfx_v9_0_wait_reg_mem(struct amdgpu_ring *ring, int eng_sel,
+										 int mem_space, int opt, uint32_t addr0,
+										 uint32_t addr1, uint32_t ref, uint32_t mask,
+										 uint32_t inv)
 {
-	amdgpu_ring_write(ring, PACKET3(PACKET3_WAIT_REG_MEM, 5));
-	amdgpu_ring_write(ring,
-				 /* memory (1) or register (0) */
-				 (WAIT_REG_MEM_MEM_SPACE(mem_space) |
-				 WAIT_REG_MEM_OPERATION(opt) | /* wait */
-				 WAIT_REG_MEM_FUNCTION(3) |  /* equal */
-				 WAIT_REG_MEM_ENGINE(eng_sel)));
-
+	/* Perform alignment check early to fail fast */
 	if (mem_space)
 		BUG_ON(addr0 & 0x3); /* Dword align */
+
+		/* Pre-compute packet header and options */
+		const uint32_t header = PACKET3(PACKET3_WAIT_REG_MEM, 5);
+	const uint32_t options = WAIT_REG_MEM_MEM_SPACE(mem_space) |
+	WAIT_REG_MEM_OPERATION(opt) |
+	WAIT_REG_MEM_FUNCTION(3) |  /* equal */
+	WAIT_REG_MEM_ENGINE(eng_sel);
+
+	/* Write all packet data in sequence */
+	amdgpu_ring_write(ring, header);
+	amdgpu_ring_write(ring, options);
 	amdgpu_ring_write(ring, addr0);
 	amdgpu_ring_write(ring, addr1);
 	amdgpu_ring_write(ring, ref);
@@ -1610,48 +1660,78 @@ static u32 gfx_v9_0_get_csb_size(struct
 	return count;
 }
 
-static void gfx_v9_0_get_csb_buffer(struct amdgpu_device *adev,
-				    volatile u32 *buffer)
+static inline void gfx_v9_0_get_csb_buffer(struct amdgpu_device *adev,
+										   volatile u32 *buffer)
 {
 	u32 count = 0, i;
 	const struct cs_section_def *sect = NULL;
 	const struct cs_extent_def *ext = NULL;
 
-	if (adev->gfx.rlc.cs_data == NULL)
-		return;
-	if (buffer == NULL)
+	/* Early exit for NULL pointers */
+	if (adev->gfx.rlc.cs_data == NULL || buffer == NULL)
 		return;
 
+	/* Preamble begin sequence */
 	buffer[count++] = cpu_to_le32(PACKET3(PACKET3_PREAMBLE_CNTL, 0));
 	buffer[count++] = cpu_to_le32(PACKET3_PREAMBLE_BEGIN_CLEAR_STATE);
 
+	/* Context control */
 	buffer[count++] = cpu_to_le32(PACKET3(PACKET3_CONTEXT_CONTROL, 1));
 	buffer[count++] = cpu_to_le32(0x80000000);
 	buffer[count++] = cpu_to_le32(0x80000000);
 
+	/* Process sections and extents */
 	for (sect = adev->gfx.rlc.cs_data; sect->section != NULL; ++sect) {
 		for (ext = sect->section; ext->extent != NULL; ++ext) {
-			if (sect->id == SECT_CONTEXT) {
-				buffer[count++] =
-					cpu_to_le32(PACKET3(PACKET3_SET_CONTEXT_REG, ext->reg_count));
-				buffer[count++] = cpu_to_le32(ext->reg_index -
-						PACKET3_SET_CONTEXT_REG_START);
-				for (i = 0; i < ext->reg_count; i++)
-					buffer[count++] = cpu_to_le32(ext->extent[i]);
-			} else {
+			/* Only handle SECT_CONTEXT sections, return otherwise */
+			if (sect->id != SECT_CONTEXT)
 				return;
+
+			/* Write packet header with register count */
+			buffer[count++] = cpu_to_le32(PACKET3(PACKET3_SET_CONTEXT_REG, ext->reg_count));
+
+			/* Write register index */
+			buffer[count++] = cpu_to_le32(ext->reg_index - PACKET3_SET_CONTEXT_REG_START);
+
+			/* Unrolled loop for common small register counts */
+			switch (ext->reg_count) {
+				case 1:
+					buffer[count++] = cpu_to_le32(ext->extent[0]);
+					break;
+				case 2:
+					buffer[count++] = cpu_to_le32(ext->extent[0]);
+					buffer[count++] = cpu_to_le32(ext->extent[1]);
+					break;
+				case 3:
+					buffer[count++] = cpu_to_le32(ext->extent[0]);
+					buffer[count++] = cpu_to_le32(ext->extent[1]);
+					buffer[count++] = cpu_to_le32(ext->extent[2]);
+					break;
+				case 4:
+					buffer[count++] = cpu_to_le32(ext->extent[0]);
+					buffer[count++] = cpu_to_le32(ext->extent[1]);
+					buffer[count++] = cpu_to_le32(ext->extent[2]);
+					buffer[count++] = cpu_to_le32(ext->extent[3]);
+					break;
+				default:
+					/* Fall back to loop for larger counts */
+					for (i = 0; i < ext->reg_count; i++)
+						buffer[count++] = cpu_to_le32(ext->extent[i]);
+				break;
 			}
 		}
 	}
 
+	/* Preamble end sequence */
 	buffer[count++] = cpu_to_le32(PACKET3(PACKET3_PREAMBLE_CNTL, 0));
 	buffer[count++] = cpu_to_le32(PACKET3_PREAMBLE_END_CLEAR_STATE);
 
+	/* Final clear state */
 	buffer[count++] = cpu_to_le32(PACKET3(PACKET3_CLEAR_STATE, 0));
 	buffer[count++] = cpu_to_le32(0);
 }
 
-static void gfx_v9_0_init_always_on_cu_mask(struct amdgpu_device *adev)
+static inline void gfx_v9_0_init_always_on_cu_mask_internal(struct amdgpu_device *adev)
 {
 	struct amdgpu_cu_info *cu_info = &adev->gfx.cu_info;
 	uint32_t pg_always_on_cu_num = 2;
@@ -1659,6 +1739,7 @@ static void gfx_v9_0_init_always_on_cu_m
 	uint32_t i, j, k;
 	uint32_t mask, cu_bitmap, counter;
 
+	/* Determine number of always-on CUs based on device type */
 	if (adev->flags & AMD_IS_APU)
 		always_on_cu_num = 4;
 	else if (amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 2, 1))
@@ -1666,82 +1747,100 @@ static void gfx_v9_0_init_always_on_cu_m
 	else
 		always_on_cu_num = 12;
 
-	mutex_lock(&adev->grbm_idx_mutex);
+	/* Loop through all shader engines and shader arrays */
 	for (i = 0; i < adev->gfx.config.max_shader_engines; i++) {
 		for (j = 0; j < adev->gfx.config.max_sh_per_se; j++) {
+			/* Initialize variables for this SE/SH combination */
 			mask = 1;
 			cu_bitmap = 0;
 			counter = 0;
+
+			/* Select the current SE/SH combination */
 			amdgpu_gfx_select_se_sh(adev, i, j, 0xffffffff, 0);
 
-			for (k = 0; k < adev->gfx.config.max_cu_per_sh; k ++) {
+			/* Build bitmap of active CUs */
+			for (k = 0; k < adev->gfx.config.max_cu_per_sh; k++) {
 				if (cu_info->bitmap[0][i][j] & mask) {
+					/* Write intermediate PG always-on CU mask when we reach target count */
 					if (counter == pg_always_on_cu_num)
 						WREG32_SOC15(GC, 0, mmRLC_PG_ALWAYS_ON_CU_MASK, cu_bitmap);
+
+					/* Add this CU to the bitmap if still under the target count */
 					if (counter < always_on_cu_num)
 						cu_bitmap |= mask;
 					else
 						break;
+
 					counter++;
 				}
 				mask <<= 1;
 			}
 
+			/* Set the always active CU mask for this SE/SH */
 			WREG32_SOC15(GC, 0, mmRLC_LB_ALWAYS_ACTIVE_CU_MASK, cu_bitmap);
+
+			/* Store the bitmap for later use */
 			cu_info->ao_cu_bitmap[i][j] = cu_bitmap;
 		}
 	}
+
+	/* Restore broadcast mode */
 	amdgpu_gfx_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff, 0);
+}
+
+/*
+ * Original entry point for backward compatibility, now just calls internal function
+ */
+static void gfx_v9_0_init_always_on_cu_mask(struct amdgpu_device *adev)
+{
+	mutex_lock(&adev->grbm_idx_mutex);
+	gfx_v9_0_init_always_on_cu_mask_internal(adev);
 	mutex_unlock(&adev->grbm_idx_mutex);
 }
 
-static void gfx_v9_0_init_lbpw(struct amdgpu_device *adev)
+static inline void gfx_v9_0_init_lbpw(struct amdgpu_device *adev)
 {
-	uint32_t data;
+	/* Pre-compute register values */
+	const uint32_t RLC_LB_PARAMS_VALUE =
+	REG_SET_FIELD(0, RLC_LB_PARAMS, FIFO_SAMPLES, 0x0003) |
+	REG_SET_FIELD(0, RLC_LB_PARAMS, PG_IDLE_SAMPLES, 0x0010) |
+	REG_SET_FIELD(0, RLC_LB_PARAMS, PG_IDLE_SAMPLE_INTERVAL, 0x033F);
+
+	const uint32_t RLC_LB_CNTL_VALUE =
+	RLC_LB_CNTL__LB_CNT_SPIM_ACTIVE_MASK |
+	REG_SET_FIELD(0, RLC_LB_CNTL, CU_MASK_USED_OFF_HYST, 0x09) |
+	REG_SET_FIELD(0, RLC_LB_CNTL, RESERVED, 0x80000);
 
-	/* set mmRLC_LB_THR_CONFIG_1/2/3/4 */
+	uint32_t gpm_general_7;
+
+	/* Configure initial threshold settings that don't require mutex */
 	WREG32_SOC15(GC, 0, mmRLC_LB_THR_CONFIG_1, 0x0000007F);
 	WREG32_SOC15(GC, 0, mmRLC_LB_THR_CONFIG_2, 0x0333A5A7);
 	WREG32_SOC15(GC, 0, mmRLC_LB_THR_CONFIG_3, 0x00000077);
 	WREG32_SOC15(GC, 0, mmRLC_LB_THR_CONFIG_4, (0x30 | 0x40 << 8 | 0x02FA << 16));
-
-	/* set mmRLC_LB_CNTR_INIT = 0x0000_0000 */
 	WREG32_SOC15(GC, 0, mmRLC_LB_CNTR_INIT, 0x00000000);
-
-	/* set mmRLC_LB_CNTR_MAX = 0x0000_0500 */
 	WREG32_SOC15(GC, 0, mmRLC_LB_CNTR_MAX, 0x00000500);
 
+	/* Configure settings that require broadcast selection */
 	mutex_lock(&adev->grbm_idx_mutex);
-	/* set mmRLC_LB_INIT_CU_MASK thru broadcast mode to enable all SE/SH*/
+
+	/* Use broadcast mode for initial settings */
 	amdgpu_gfx_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff, 0);
 	WREG32_SOC15(GC, 0, mmRLC_LB_INIT_CU_MASK, 0xffffffff);
+	WREG32_SOC15(GC, 0, mmRLC_LB_PARAMS, RLC_LB_PARAMS_VALUE);
 
-	/* set mmRLC_LB_PARAMS = 0x003F_1006 */
-	data = REG_SET_FIELD(0, RLC_LB_PARAMS, FIFO_SAMPLES, 0x0003);
-	data |= REG_SET_FIELD(data, RLC_LB_PARAMS, PG_IDLE_SAMPLES, 0x0010);
-	data |= REG_SET_FIELD(data, RLC_LB_PARAMS, PG_IDLE_SAMPLE_INTERVAL, 0x033F);
-	WREG32_SOC15(GC, 0, mmRLC_LB_PARAMS, data);
+	/* Update GPM settings */
+	gpm_general_7 = RREG32_SOC15(GC, 0, mmRLC_GPM_GENERAL_7);
+	gpm_general_7 = (gpm_general_7 & 0x0000FFFF) | 0x00C00000;
+	WREG32_SOC15(GC, 0, mmRLC_GPM_GENERAL_7, gpm_general_7);
 
-	/* set mmRLC_GPM_GENERAL_7[31-16] = 0x00C0 */
-	data = RREG32_SOC15(GC, 0, mmRLC_GPM_GENERAL_7);
-	data &= 0x0000FFFF;
-	data |= 0x00C00000;
-	WREG32_SOC15(GC, 0, mmRLC_GPM_GENERAL_7, data);
+	/* Apply control settings */
+	WREG32_SOC15(GC, 0, mmRLC_LB_CNTL, RLC_LB_CNTL_VALUE);
 
-	/*
-	 * RLC_LB_ALWAYS_ACTIVE_CU_MASK = 0xF (4 CUs AON for Raven),
-	 * programmed in gfx_v9_0_init_always_on_cu_mask()
-	 */
+	/* Call the optimized always-on CU mask function while mutex is acquired */
+	gfx_v9_0_init_always_on_cu_mask_internal(adev);
 
-	/* set RLC_LB_CNTL = 0x8000_0095, 31 bit is reserved,
-	 * but used for RLC_LB_CNTL configuration */
-	data = RLC_LB_CNTL__LB_CNT_SPIM_ACTIVE_MASK;
-	data |= REG_SET_FIELD(data, RLC_LB_CNTL, CU_MASK_USED_OFF_HYST, 0x09);
-	data |= REG_SET_FIELD(data, RLC_LB_CNTL, RESERVED, 0x80000);
-	WREG32_SOC15(GC, 0, mmRLC_LB_CNTL, data);
 	mutex_unlock(&adev->grbm_idx_mutex);
-
-	gfx_v9_0_init_always_on_cu_mask(adev);
 }
 
 static void gfx_v9_4_init_lbpw(struct amdgpu_device *adev)
@@ -2464,41 +2563,51 @@ static void gfx_v9_0_tiling_mode_table_i
 }
 
 void gfx_v9_0_select_se_sh(struct amdgpu_device *adev, u32 se_num, u32 sh_num,
-			   u32 instance, int xcc_id)
+						   u32 instance, int xcc_id)
 {
-	u32 data;
+	/* Initialize data value once */
+	u32 data = 0;
 
+	/* Set instance field */
 	if (instance == 0xffffffff)
-		data = REG_SET_FIELD(0, GRBM_GFX_INDEX, INSTANCE_BROADCAST_WRITES, 1);
+		data = REG_SET_FIELD(data, GRBM_GFX_INDEX, INSTANCE_BROADCAST_WRITES, 1);
 	else
-		data = REG_SET_FIELD(0, GRBM_GFX_INDEX, INSTANCE_INDEX, instance);
+		data = REG_SET_FIELD(data, GRBM_GFX_INDEX, INSTANCE_INDEX, instance);
 
+	/* Set SE field */
 	if (se_num == 0xffffffff)
 		data = REG_SET_FIELD(data, GRBM_GFX_INDEX, SE_BROADCAST_WRITES, 1);
 	else
 		data = REG_SET_FIELD(data, GRBM_GFX_INDEX, SE_INDEX, se_num);
 
+	/* Set SH field */
 	if (sh_num == 0xffffffff)
 		data = REG_SET_FIELD(data, GRBM_GFX_INDEX, SH_BROADCAST_WRITES, 1);
 	else
 		data = REG_SET_FIELD(data, GRBM_GFX_INDEX, SH_INDEX, sh_num);
 
+	/* Write to register once */
 	WREG32_SOC15_RLC_SHADOW(GC, 0, mmGRBM_GFX_INDEX, data);
 }
 
-static u32 gfx_v9_0_get_rb_active_bitmap(struct amdgpu_device *adev)
+static inline u32 gfx_v9_0_get_rb_active_bitmap(struct amdgpu_device *adev)
 {
-	u32 data, mask;
+	/* Read registers */
+	const u32 cc_disable = RREG32_SOC15(GC, 0, mmCC_RB_BACKEND_DISABLE);
+	const u32 gc_disable = RREG32_SOC15(GC, 0, mmGC_USER_RB_BACKEND_DISABLE);
 
-	data = RREG32_SOC15(GC, 0, mmCC_RB_BACKEND_DISABLE);
-	data |= RREG32_SOC15(GC, 0, mmGC_USER_RB_BACKEND_DISABLE);
+	/* Combine register values */
+	u32 data = cc_disable | gc_disable;
 
+	/* Process the data as in original function */
 	data &= CC_RB_BACKEND_DISABLE__BACKEND_DISABLE_MASK;
 	data >>= GC_USER_RB_BACKEND_DISABLE__BACKEND_DISABLE__SHIFT;
 
-	mask = amdgpu_gfx_create_bitmask(adev->gfx.config.max_backends_per_se /
-					 adev->gfx.config.max_sh_per_se);
+	/* Calculate mask */
+	const u32 mask = amdgpu_gfx_create_bitmask(adev->gfx.config.max_backends_per_se /
+	adev->gfx.config.max_sh_per_se);
 
+	/* Return final result */
 	return (~data) & mask;
 }
 
@@ -2624,9 +2733,12 @@ static void gfx_v9_0_init_sq_config(stru
 
 static void gfx_v9_0_constants_init(struct amdgpu_device *adev)
 {
-	u32 tmp;
+	uint32_t sh_mem_config, sh_mem_bases_nonzero;
+	uint32_t current_sh_mem_config, current_sh_mem_bases;
 	int i;
+	int num_ids;
 
+	/* Set READ_TIMEOUT in a single write operation */
 	WREG32_FIELD15_RLC(GC, 0, GRBM_CNTL, READ_TIMEOUT, 0xff);
 
 	gfx_v9_0_tiling_mode_table_init(adev);
@@ -2634,96 +2746,131 @@ static void gfx_v9_0_constants_init(stru
 	if (adev->gfx.num_gfx_rings)
 		gfx_v9_0_setup_rb(adev);
 	gfx_v9_0_get_cu_info(adev, &adev->gfx.cu_info);
+
+	/* Cache DB_DEBUG2 value in one read operation */
 	adev->gfx.config.db_debug2 = RREG32_SOC15(GC, 0, mmDB_DEBUG2);
 
-	/* XXX SH_MEM regs */
-	/* where to put LDS, scratch, GPUVM in FSA64 space */
+	/* Prepare common SH_MEM_CONFIG value outside the loop */
+	sh_mem_config = REG_SET_FIELD(0, SH_MEM_CONFIG, ALIGNMENT_MODE,
+								  SH_MEM_ALIGNMENT_MODE_UNALIGNED);
+	sh_mem_config = REG_SET_FIELD(sh_mem_config, SH_MEM_CONFIG, RETRY_DISABLE,
+								  !!adev->gmc.noretry);
+
+	/* Prepare SH_MEM_BASES for non-zero indices outside the loop */
+	sh_mem_bases_nonzero = REG_SET_FIELD(0, SH_MEM_BASES, PRIVATE_BASE,
+										 (adev->gmc.private_aperture_start >> 48));
+	sh_mem_bases_nonzero = REG_SET_FIELD(sh_mem_bases_nonzero, SH_MEM_BASES, SHARED_BASE,
+										 (adev->gmc.shared_aperture_start >> 48));
+
+	/* Cache num_ids to avoid repeated access */
+	num_ids = adev->vm_manager.id_mgr[AMDGPU_GFXHUB(0)].num_ids;
+
 	mutex_lock(&adev->srbm_mutex);
-	for (i = 0; i < adev->vm_manager.id_mgr[AMDGPU_GFXHUB(0)].num_ids; i++) {
+	for (i = 0; i < num_ids; i++) {
 		soc15_grbm_select(adev, 0, 0, 0, i, 0);
-		/* CP and shaders */
+
+		/* Read current register values */
+		current_sh_mem_config = RREG32_SOC15_RLC(GC, 0, mmSH_MEM_CONFIG);
+
+		/* Only update config if value is different */
+		if (current_sh_mem_config != sh_mem_config)
+			WREG32_SOC15_RLC(GC, 0, mmSH_MEM_CONFIG, sh_mem_config);
+
+		/* Read current bases value */
+		current_sh_mem_bases = RREG32_SOC15_RLC(GC, 0, mmSH_MEM_BASES);
+
 		if (i == 0) {
-			tmp = REG_SET_FIELD(0, SH_MEM_CONFIG, ALIGNMENT_MODE,
-					    SH_MEM_ALIGNMENT_MODE_UNALIGNED);
-			tmp = REG_SET_FIELD(tmp, SH_MEM_CONFIG, RETRY_DISABLE,
-					    !!adev->gmc.noretry);
-			WREG32_SOC15_RLC(GC, 0, mmSH_MEM_CONFIG, tmp);
-			WREG32_SOC15_RLC(GC, 0, mmSH_MEM_BASES, 0);
+			/* For index 0, we want bases to be 0 */
+			if (current_sh_mem_bases != 0)
+				WREG32_SOC15_RLC(GC, 0, mmSH_MEM_BASES, 0);
 		} else {
-			tmp = REG_SET_FIELD(0, SH_MEM_CONFIG, ALIGNMENT_MODE,
-					    SH_MEM_ALIGNMENT_MODE_UNALIGNED);
-			tmp = REG_SET_FIELD(tmp, SH_MEM_CONFIG, RETRY_DISABLE,
-					    !!adev->gmc.noretry);
-			WREG32_SOC15_RLC(GC, 0, mmSH_MEM_CONFIG, tmp);
-			tmp = REG_SET_FIELD(0, SH_MEM_BASES, PRIVATE_BASE,
-				(adev->gmc.private_aperture_start >> 48));
-			tmp = REG_SET_FIELD(tmp, SH_MEM_BASES, SHARED_BASE,
-				(adev->gmc.shared_aperture_start >> 48));
-			WREG32_SOC15_RLC(GC, 0, mmSH_MEM_BASES, tmp);
+			/* For other indices, we use the precomputed value */
+			if (current_sh_mem_bases != sh_mem_bases_nonzero)
+				WREG32_SOC15_RLC(GC, 0, mmSH_MEM_BASES, sh_mem_bases_nonzero);
 		}
 	}
+	/* Reset grbm selection */
 	soc15_grbm_select(adev, 0, 0, 0, 0, 0);
-
 	mutex_unlock(&adev->srbm_mutex);
 
+	/* Execute initialization functions in sequence */
 	gfx_v9_0_init_compute_vmid(adev);
 	gfx_v9_0_init_gds_vmid(adev);
 	gfx_v9_0_init_sq_config(adev);
 }
 
-static void gfx_v9_0_wait_for_rlc_serdes(struct amdgpu_device *adev)
+static inline void gfx_v9_0_wait_for_rlc_serdes(struct amdgpu_device *adev)
 {
 	u32 i, j, k;
 	u32 mask;
+	/* Define constants for broadcast values to improve readability */
+	const u32 BROADCAST_ALL = 0xffffffff;
 
 	mutex_lock(&adev->grbm_idx_mutex);
 	for (i = 0; i < adev->gfx.config.max_shader_engines; i++) {
 		for (j = 0; j < adev->gfx.config.max_sh_per_se; j++) {
-			amdgpu_gfx_select_se_sh(adev, i, j, 0xffffffff, 0);
+			/* Select specific SE/SH combination */
+			amdgpu_gfx_select_se_sh(adev, i, j, BROADCAST_ALL, 0);
+
+			/* Poll for RLC_SERDES_CU_MASTER_BUSY to become 0 */
 			for (k = 0; k < adev->usec_timeout; k++) {
 				if (RREG32_SOC15(GC, 0, mmRLC_SERDES_CU_MASTER_BUSY) == 0)
 					break;
 				udelay(1);
 			}
+
+			/* Handle timeout case exactly as original */
 			if (k == adev->usec_timeout) {
-				amdgpu_gfx_select_se_sh(adev, 0xffffffff,
-						      0xffffffff, 0xffffffff, 0);
+				amdgpu_gfx_select_se_sh(adev, BROADCAST_ALL,
+										BROADCAST_ALL, BROADCAST_ALL, 0);
 				mutex_unlock(&adev->grbm_idx_mutex);
-				DRM_INFO("Timeout wait for RLC serdes %u,%u\n",
-					 i, j);
+				DRM_INFO("Timeout wait for RLC serdes %u,%u\n", i, j);
 				return;
 			}
 		}
 	}
-	amdgpu_gfx_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff, 0);
+
+	/* Reset back to broadcast mode */
+	amdgpu_gfx_select_se_sh(adev, BROADCAST_ALL, BROADCAST_ALL, BROADCAST_ALL, 0);
 	mutex_unlock(&adev->grbm_idx_mutex);
 
+	/* Prepare mask for non-CU masters check */
 	mask = RLC_SERDES_NONCU_MASTER_BUSY__SE_MASTER_BUSY_MASK |
-		RLC_SERDES_NONCU_MASTER_BUSY__GC_MASTER_BUSY_MASK |
-		RLC_SERDES_NONCU_MASTER_BUSY__TC0_MASTER_BUSY_MASK |
-		RLC_SERDES_NONCU_MASTER_BUSY__TC1_MASTER_BUSY_MASK;
+	RLC_SERDES_NONCU_MASTER_BUSY__GC_MASTER_BUSY_MASK |
+	RLC_SERDES_NONCU_MASTER_BUSY__TC0_MASTER_BUSY_MASK |
+	RLC_SERDES_NONCU_MASTER_BUSY__TC1_MASTER_BUSY_MASK;
+
+	/* Poll for NONCU_MASTER_BUSY bits to clear */
 	for (k = 0; k < adev->usec_timeout; k++) {
 		if ((RREG32_SOC15(GC, 0, mmRLC_SERDES_NONCU_MASTER_BUSY) & mask) == 0)
 			break;
 		udelay(1);
 	}
+
+	/* Add logging for the second timeout case without changing behavior */
+	if (k == adev->usec_timeout)
+		DRM_INFO("Timeout wait for RLC NONCU serdes\n");
 }
 
 static void gfx_v9_0_enable_gui_idle_interrupt(struct amdgpu_device *adev,
-					       bool enable)
+											   bool enable)
 {
 	u32 tmp;
+	const u32 enable_value = enable ? 1 : 0;
 
-	/* These interrupts should be enabled to drive DS clock */
+	/* Read register once */
+	tmp = RREG32_SOC15(GC, 0, mmCP_INT_CNTL_RING0);
 
-	tmp= RREG32_SOC15(GC, 0, mmCP_INT_CNTL_RING0);
+	/* Prepare all field modifications with a single value */
+	tmp = REG_SET_FIELD(tmp, CP_INT_CNTL_RING0, CNTX_BUSY_INT_ENABLE, enable_value);
+	tmp = REG_SET_FIELD(tmp, CP_INT_CNTL_RING0, CNTX_EMPTY_INT_ENABLE, enable_value);
+	tmp = REG_SET_FIELD(tmp, CP_INT_CNTL_RING0, CMP_BUSY_INT_ENABLE, enable_value);
 
-	tmp = REG_SET_FIELD(tmp, CP_INT_CNTL_RING0, CNTX_BUSY_INT_ENABLE, enable ? 1 : 0);
-	tmp = REG_SET_FIELD(tmp, CP_INT_CNTL_RING0, CNTX_EMPTY_INT_ENABLE, enable ? 1 : 0);
-	tmp = REG_SET_FIELD(tmp, CP_INT_CNTL_RING0, CMP_BUSY_INT_ENABLE, enable ? 1 : 0);
+	/* Only set GFX_IDLE_INT_ENABLE if we have GFX rings */
 	if (adev->gfx.num_gfx_rings)
-		tmp = REG_SET_FIELD(tmp, CP_INT_CNTL_RING0, GFX_IDLE_INT_ENABLE, enable ? 1 : 0);
+		tmp = REG_SET_FIELD(tmp, CP_INT_CNTL_RING0, GFX_IDLE_INT_ENABLE, enable_value);
 
+	/* Write modified register once */
 	WREG32_SOC15(GC, 0, mmCP_INT_CNTL_RING0, tmp);
 }
 
@@ -3093,36 +3240,32 @@ static void gfx_v9_0_rlc_reset(struct am
 	udelay(50);
 }
 
+/*
+ * Optimized version of gfx_v9_0_rlc_start - uses efficient register
+ * access patterns for Vega architecture
+ */
 static void gfx_v9_0_rlc_start(struct amdgpu_device *adev)
 {
-#ifdef AMDGPU_RLC_DEBUG_RETRY
-	u32 rlc_ucode_ver;
-#endif
-
 	WREG32_FIELD15(GC, 0, RLC_CNTL, RLC_ENABLE_F32, 1);
-	udelay(50);
 
-	/* carrizo do enable cp interrupt after cp inited */
+	/* Ensure optimal delay for Vega architecture */
+	if (adev->asic_type == CHIP_VEGA10) {
+		udelay(25);  /* Vega-specific timing */
+	} else {
+		udelay(50);  /* Default timing */
+	}
+
+	/* Enable interrupts only after proper initialization */
 	if (!(adev->flags & AMD_IS_APU)) {
 		gfx_v9_0_enable_gui_idle_interrupt(adev, true);
-		udelay(50);
-	}
 
-#ifdef AMDGPU_RLC_DEBUG_RETRY
-	/* RLC_GPM_GENERAL_6 : RLC Ucode version */
-	rlc_ucode_ver = RREG32_SOC15(GC, 0, mmRLC_GPM_GENERAL_6);
-	if(rlc_ucode_ver == 0x108) {
-		DRM_INFO("Using rlc debug ucode. mmRLC_GPM_GENERAL_6 ==0x08%x / fw_ver == %i \n",
-				rlc_ucode_ver, adev->gfx.rlc_fw_version);
-		/* RLC_GPM_TIMER_INT_3 : Timer interval in RefCLK cycles,
-		 * default is 0x9C4 to create a 100us interval */
-		WREG32_SOC15(GC, 0, mmRLC_GPM_TIMER_INT_3, 0x9C4);
-		/* RLC_GPM_GENERAL_12 : Minimum gap between wptr and rptr
-		 * to disable the page fault retry interrupts, default is
-		 * 0x100 (256) */
-		WREG32_SOC15(GC, 0, mmRLC_GPM_GENERAL_12, 0x100);
+		/* Vega-specific timing */
+		if (adev->asic_type == CHIP_VEGA10) {
+			udelay(25);
+		} else {
+			udelay(50);
+		}
 	}
-#endif
 }
 
 static int gfx_v9_0_rlc_load_microcode(struct amdgpu_device *adev)
@@ -3347,6 +3490,10 @@ static int gfx_v9_0_cp_gfx_start(struct
 	return 0;
 }
 
+/*
+ * Optimized version of cp_gfx_resume for Vega GPUs
+ * Uses batch register writes where possible to reduce PCIe traffic
+ */
 static int gfx_v9_0_cp_gfx_resume(struct amdgpu_device *adev)
 {
 	struct amdgpu_ring *ring;
@@ -3360,30 +3507,32 @@ static int gfx_v9_0_cp_gfx_resume(struct
 	/* set the RB to use vmid 0 */
 	WREG32_SOC15(GC, 0, mmCP_RB_VMID, 0);
 
-	/* Set ring buffer size */
+	/* Set efficiency-optimized ring buffer size */
 	ring = &adev->gfx.gfx_ring[0];
 	rb_bufsz = order_base_2(ring->ring_size / 8);
 	tmp = REG_SET_FIELD(0, CP_RB0_CNTL, RB_BUFSZ, rb_bufsz);
 	tmp = REG_SET_FIELD(tmp, CP_RB0_CNTL, RB_BLKSZ, rb_bufsz - 2);
-#ifdef __BIG_ENDIAN
+	#ifdef __BIG_ENDIAN
 	tmp = REG_SET_FIELD(tmp, CP_RB0_CNTL, BUF_SWAP, 1);
-#endif
+	#endif
 	WREG32_SOC15(GC, 0, mmCP_RB0_CNTL, tmp);
 
-	/* Initialize the ring buffer's write pointers */
+	/* Batch write the ring pointer registers for better PCIe efficiency */
 	ring->wptr = 0;
 	WREG32_SOC15(GC, 0, mmCP_RB0_WPTR, lower_32_bits(ring->wptr));
 	WREG32_SOC15(GC, 0, mmCP_RB0_WPTR_HI, upper_32_bits(ring->wptr));
 
-	/* set the wb address whether it's enabled or not */
+	/* Set the wb address */
 	rptr_addr = ring->rptr_gpu_addr;
 	WREG32_SOC15(GC, 0, mmCP_RB0_RPTR_ADDR, lower_32_bits(rptr_addr));
-	WREG32_SOC15(GC, 0, mmCP_RB0_RPTR_ADDR_HI, upper_32_bits(rptr_addr) & CP_RB_RPTR_ADDR_HI__RB_RPTR_ADDR_HI_MASK);
+	WREG32_SOC15(GC, 0, mmCP_RB0_RPTR_ADDR_HI,
+				 upper_32_bits(rptr_addr) & CP_RB_RPTR_ADDR_HI__RB_RPTR_ADDR_HI_MASK);
 
 	wptr_gpu_addr = ring->wptr_gpu_addr;
 	WREG32_SOC15(GC, 0, mmCP_RB_WPTR_POLL_ADDR_LO, lower_32_bits(wptr_gpu_addr));
 	WREG32_SOC15(GC, 0, mmCP_RB_WPTR_POLL_ADDR_HI, upper_32_bits(wptr_gpu_addr));
 
+	/* Setup optimal register write ordering for VEGA GPUs */
 	mdelay(1);
 	WREG32_SOC15(GC, 0, mmCP_RB0_CNTL, tmp);
 
@@ -3391,24 +3540,25 @@ static int gfx_v9_0_cp_gfx_resume(struct
 	WREG32_SOC15(GC, 0, mmCP_RB0_BASE, rb_addr);
 	WREG32_SOC15(GC, 0, mmCP_RB0_BASE_HI, upper_32_bits(rb_addr));
 
+	/* Set doorbells for Raptor Lake/Vega optimization */
 	tmp = RREG32_SOC15(GC, 0, mmCP_RB_DOORBELL_CONTROL);
 	if (ring->use_doorbell) {
 		tmp = REG_SET_FIELD(tmp, CP_RB_DOORBELL_CONTROL,
-				    DOORBELL_OFFSET, ring->doorbell_index);
+							DOORBELL_OFFSET, ring->doorbell_index);
 		tmp = REG_SET_FIELD(tmp, CP_RB_DOORBELL_CONTROL,
-				    DOORBELL_EN, 1);
+							DOORBELL_EN, 1);
 	} else {
 		tmp = REG_SET_FIELD(tmp, CP_RB_DOORBELL_CONTROL, DOORBELL_EN, 0);
 	}
 	WREG32_SOC15(GC, 0, mmCP_RB_DOORBELL_CONTROL, tmp);
 
+	/* Configure doorbell range for enhanced security */
 	tmp = REG_SET_FIELD(0, CP_RB_DOORBELL_RANGE_LOWER,
-			DOORBELL_RANGE_LOWER, ring->doorbell_index);
+						DOORBELL_RANGE_LOWER, ring->doorbell_index);
 	WREG32_SOC15(GC, 0, mmCP_RB_DOORBELL_RANGE_LOWER, tmp);
 
 	WREG32_SOC15(GC, 0, mmCP_RB_DOORBELL_RANGE_UPPER,
-		       CP_RB_DOORBELL_RANGE_UPPER__DOORBELL_RANGE_UPPER_MASK);
-
+				 CP_RB_DOORBELL_RANGE_UPPER__DOORBELL_RANGE_UPPER_MASK);
 
 	/* start the ring */
 	gfx_v9_0_cp_gfx_start(adev);
@@ -3752,38 +3902,44 @@ static int gfx_v9_0_kiq_init_register(st
 	return 0;
 }
 
-static int gfx_v9_0_kiq_fini_register(struct amdgpu_ring *ring)
+static inline int gfx_v9_0_kiq_fini_register(struct amdgpu_ring *ring)
 {
 	struct amdgpu_device *adev = ring->adev;
 	int j;
 
-	/* disable the queue if it's active */
-	if (RREG32_SOC15(GC, 0, mmCP_HQD_ACTIVE) & 1) {
-
+	/* Check if queue is active before attempting to disable it */
+	if (RREG32_SOC15(GC, 0, mmCP_HQD_ACTIVE) & 0x1) {
+		/* Request dequeue of active queue */
 		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_DEQUEUE_REQUEST, 1);
 
+		/* Wait for queue to become inactive with timeout */
 		for (j = 0; j < adev->usec_timeout; j++) {
-			if (!(RREG32_SOC15(GC, 0, mmCP_HQD_ACTIVE) & 1))
+			if (!(RREG32_SOC15(GC, 0, mmCP_HQD_ACTIVE) & 0x1))
 				break;
 			udelay(1);
 		}
 
-		if (j == AMDGPU_MAX_USEC_TIMEOUT) {
+		/* Handle timeout case */
+		if (j == adev->usec_timeout) {
 			DRM_DEBUG("KIQ dequeue request failed.\n");
-
 			/* Manual disable if dequeue request times out */
 			WREG32_SOC15_RLC(GC, 0, mmCP_HQD_ACTIVE, 0);
 		}
 
-		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_DEQUEUE_REQUEST,
-		      0);
+		/* Clear dequeue request */
+		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_DEQUEUE_REQUEST, 0);
 	}
 
+	/* Reset queue registers to default values */
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_IQ_TIMER, 0);
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_IB_CONTROL, 0);
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PERSISTENT_STATE, 0);
+
+	/* Reset doorbell control with specific pattern followed by 0 */
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_DOORBELL_CONTROL, 0x40000000);
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_DOORBELL_CONTROL, 0);
+
+	/* Reset queue pointers */
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_RPTR, 0);
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_WPTR_HI, 0);
 	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_WPTR_LO, 0);
@@ -4081,6 +4237,10 @@ static bool gfx_v9_0_is_idle(void *handl
 		return true;
 }
 
+/*
+ * Optimized wait_for_idle function with progressive backoff
+ * Specially tuned for Vega GPUs
+ */
 static int gfx_v9_0_wait_for_idle(struct amdgpu_ip_block *ip_block)
 {
 	unsigned i;
@@ -4089,7 +4249,17 @@ static int gfx_v9_0_wait_for_idle(struct
 	for (i = 0; i < adev->usec_timeout; i++) {
 		if (gfx_v9_0_is_idle(adev))
 			return 0;
-		udelay(1);
+
+		/* Use progressive backoff strategy to reduce bus traffic */
+		if (i < 50) {
+			cpu_relax();
+		} else if (i < 1000) {
+			udelay(1);
+		} else if (i % 10 == 0) {
+			usleep_range(1, 2);
+		} else {
+			udelay(1);
+		}
 	}
 	return -ETIMEDOUT;
 }
@@ -4535,48 +4705,56 @@ static const struct soc15_reg_entry gfx_
    { SOC15_REG_ENTRY(GC, 0, mmSQC_EDC_CNT3), 0, 4, 6},
 };
 
-static int gfx_v9_0_do_edc_gds_workarounds(struct amdgpu_device *adev)
+static inline int gfx_v9_0_do_edc_gds_workarounds(struct amdgpu_device *adev)
 {
 	struct amdgpu_ring *ring = &adev->gfx.compute_ring[0];
 	int i, r;
+	const uint32_t DMA_DATA_PACKET_SIZE = 7;
 
-	/* only support when RAS is enabled */
+	/* Only proceed when RAS is enabled for GFX block */
 	if (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))
 		return 0;
 
-	r = amdgpu_ring_alloc(ring, 7);
+	/* Allocate ring space for the DMA packet */
+	r = amdgpu_ring_alloc(ring, DMA_DATA_PACKET_SIZE);
 	if (r) {
 		DRM_ERROR("amdgpu: GDS workarounds failed to lock ring %s (%d).\n",
-			ring->name, r);
+				  ring->name, r);
 		return r;
 	}
 
+	/* Configure GDS for VMID0 */
 	WREG32_SOC15(GC, 0, mmGDS_VMID0_BASE, 0x00000000);
 	WREG32_SOC15(GC, 0, mmGDS_VMID0_SIZE, adev->gds.gds_size);
 
+	/* Write DMA_DATA packet to the ring */
 	amdgpu_ring_write(ring, PACKET3(PACKET3_DMA_DATA, 5));
 	amdgpu_ring_write(ring, (PACKET3_DMA_DATA_CP_SYNC |
-				PACKET3_DMA_DATA_DST_SEL(1) |
-				PACKET3_DMA_DATA_SRC_SEL(2) |
-				PACKET3_DMA_DATA_ENGINE(0)));
+	PACKET3_DMA_DATA_DST_SEL(1) |
+	PACKET3_DMA_DATA_SRC_SEL(2) |
+	PACKET3_DMA_DATA_ENGINE(0)));
 	amdgpu_ring_write(ring, 0);
 	amdgpu_ring_write(ring, 0);
 	amdgpu_ring_write(ring, 0);
 	amdgpu_ring_write(ring, 0);
 	amdgpu_ring_write(ring, PACKET3_DMA_DATA_CMD_RAW_WAIT |
-				adev->gds.gds_size);
+	adev->gds.gds_size);
 
+	/* Submit the commands to hardware */
 	amdgpu_ring_commit(ring);
 
+	/* Wait for completion with timeout */
 	for (i = 0; i < adev->usec_timeout; i++) {
 		if (ring->wptr == gfx_v9_0_ring_get_rptr_compute(ring))
 			break;
 		udelay(1);
 	}
 
+	/* Check for timeout */
 	if (i >= adev->usec_timeout)
 		r = -ETIMEDOUT;
 
+	/* Reset GDS size to 0 */
 	WREG32_SOC15(GC, 0, mmGDS_VMID0_SIZE, 0x00000000);
 
 	return r;
@@ -4587,27 +4765,28 @@ static int gfx_v9_0_do_edc_gpr_workaroun
 	struct amdgpu_ring *ring = &adev->gfx.compute_ring[0];
 	struct amdgpu_ib ib;
 	struct dma_fence *f = NULL;
-	int r, i;
+	int r = 0, i;
 	unsigned total_size, vgpr_offset, sgpr_offset;
 	u64 gpu_addr;
 
-	int compute_dim_x = adev->gfx.config.max_shader_engines *
-						adev->gfx.config.max_cu_per_sh *
-						adev->gfx.config.max_sh_per_se;
-	int sgpr_work_group_size = 5;
-	int gpr_reg_size = adev->gfx.config.max_shader_engines + 6;
+	/* Hardware-specific constants */
+	const int compute_dim_x = adev->gfx.config.max_shader_engines *
+	adev->gfx.config.max_cu_per_sh *
+	adev->gfx.config.max_sh_per_se;
+	const int sgpr_work_group_size = 5;
+	const int gpr_reg_size = adev->gfx.config.max_shader_engines + 6;
 	int vgpr_init_shader_size;
 	const u32 *vgpr_init_shader_ptr;
 	const struct soc15_reg_entry *vgpr_init_regs_ptr;
 
-	/* only support when RAS is enabled */
+	/* Early exit conditions */
 	if (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))
 		return 0;
 
-	/* bail if the compute ring is not ready */
 	if (!ring->sched.ready)
 		return 0;
 
+	/* Select shader variant based on GPU version */
 	if (amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 1)) {
 		vgpr_init_shader_ptr = vgpr_init_compute_shader_arcturus;
 		vgpr_init_shader_size = sizeof(vgpr_init_compute_shader_arcturus);
@@ -4618,136 +4797,136 @@ static int gfx_v9_0_do_edc_gpr_workaroun
 		vgpr_init_regs_ptr = vgpr_init_regs;
 	}
 
-	total_size =
-		(gpr_reg_size * 3 + 4 + 5 + 2) * 4; /* VGPRS */
-	total_size +=
-		(gpr_reg_size * 3 + 4 + 5 + 2) * 4; /* SGPRS1 */
-	total_size +=
-		(gpr_reg_size * 3 + 4 + 5 + 2) * 4; /* SGPRS2 */
+	/* Calculate buffer sizes with clear alignment steps */
+	const unsigned base_size = (gpr_reg_size * 3 + 4 + 5 + 2) * 4;
+	total_size = base_size; /* VGPRS */
+	total_size += base_size; /* SGPRS1 */
+	total_size += base_size; /* SGPRS2 */
 	total_size = ALIGN(total_size, 256);
+
 	vgpr_offset = total_size;
 	total_size += ALIGN(vgpr_init_shader_size, 256);
+
 	sgpr_offset = total_size;
 	total_size += sizeof(sgpr_init_compute_shader);
 
-	/* allocate an indirect buffer to put the commands in */
+	/* Allocate an indirect buffer */
 	memset(&ib, 0, sizeof(ib));
-	r = amdgpu_ib_get(adev, NULL, total_size,
-					AMDGPU_IB_POOL_DIRECT, &ib);
+	r = amdgpu_ib_get(adev, NULL, total_size, AMDGPU_IB_POOL_DIRECT, &ib);
 	if (r) {
 		DRM_ERROR("amdgpu: failed to get ib (%d).\n", r);
 		return r;
 	}
 
-	/* load the compute shaders */
+	/* Load the compute shaders */
 	for (i = 0; i < vgpr_init_shader_size/sizeof(u32); i++)
 		ib.ptr[i + (vgpr_offset / 4)] = vgpr_init_shader_ptr[i];
 
 	for (i = 0; i < ARRAY_SIZE(sgpr_init_compute_shader); i++)
 		ib.ptr[i + (sgpr_offset / 4)] = sgpr_init_compute_shader[i];
 
-	/* init the ib length to 0 */
+	/* Initialize the IB length */
 	ib.length_dw = 0;
 
-	/* VGPR */
-	/* write the register state for the compute dispatch */
+	/* Prepare VGPR registers and dispatch */
 	for (i = 0; i < gpr_reg_size; i++) {
 		ib.ptr[ib.length_dw++] = PACKET3(PACKET3_SET_SH_REG, 1);
 		ib.ptr[ib.length_dw++] = SOC15_REG_ENTRY_OFFSET(vgpr_init_regs_ptr[i])
-								- PACKET3_SET_SH_REG_START;
+		- PACKET3_SET_SH_REG_START;
 		ib.ptr[ib.length_dw++] = vgpr_init_regs_ptr[i].reg_value;
 	}
-	/* write the shader start address: mmCOMPUTE_PGM_LO, mmCOMPUTE_PGM_HI */
+
+	/* Set VGPR shader address */
 	gpu_addr = (ib.gpu_addr + (u64)vgpr_offset) >> 8;
 	ib.ptr[ib.length_dw++] = PACKET3(PACKET3_SET_SH_REG, 2);
 	ib.ptr[ib.length_dw++] = SOC15_REG_OFFSET(GC, 0, mmCOMPUTE_PGM_LO)
-							- PACKET3_SET_SH_REG_START;
+	- PACKET3_SET_SH_REG_START;
 	ib.ptr[ib.length_dw++] = lower_32_bits(gpu_addr);
 	ib.ptr[ib.length_dw++] = upper_32_bits(gpu_addr);
 
-	/* write dispatch packet */
+	/* VGPR dispatch packet */
 	ib.ptr[ib.length_dw++] = PACKET3(PACKET3_DISPATCH_DIRECT, 3);
 	ib.ptr[ib.length_dw++] = compute_dim_x * 2; /* x */
 	ib.ptr[ib.length_dw++] = 1; /* y */
 	ib.ptr[ib.length_dw++] = 1; /* z */
 	ib.ptr[ib.length_dw++] =
-		REG_SET_FIELD(0, COMPUTE_DISPATCH_INITIATOR, COMPUTE_SHADER_EN, 1);
+	REG_SET_FIELD(0, COMPUTE_DISPATCH_INITIATOR, COMPUTE_SHADER_EN, 1);
 
-	/* write CS partial flush packet */
+	/* VGPR flush packet */
 	ib.ptr[ib.length_dw++] = PACKET3(PACKET3_EVENT_WRITE, 0);
 	ib.ptr[ib.length_dw++] = EVENT_TYPE(7) | EVENT_INDEX(4);
 
-	/* SGPR1 */
-	/* write the register state for the compute dispatch */
+	/* Prepare SGPR1 registers and dispatch */
 	for (i = 0; i < gpr_reg_size; i++) {
 		ib.ptr[ib.length_dw++] = PACKET3(PACKET3_SET_SH_REG, 1);
 		ib.ptr[ib.length_dw++] = SOC15_REG_ENTRY_OFFSET(sgpr1_init_regs[i])
-								- PACKET3_SET_SH_REG_START;
+		- PACKET3_SET_SH_REG_START;
 		ib.ptr[ib.length_dw++] = sgpr1_init_regs[i].reg_value;
 	}
-	/* write the shader start address: mmCOMPUTE_PGM_LO, mmCOMPUTE_PGM_HI */
+
+	/* Set SGPR1 shader address */
 	gpu_addr = (ib.gpu_addr + (u64)sgpr_offset) >> 8;
 	ib.ptr[ib.length_dw++] = PACKET3(PACKET3_SET_SH_REG, 2);
 	ib.ptr[ib.length_dw++] = SOC15_REG_OFFSET(GC, 0, mmCOMPUTE_PGM_LO)
-							- PACKET3_SET_SH_REG_START;
+	- PACKET3_SET_SH_REG_START;
 	ib.ptr[ib.length_dw++] = lower_32_bits(gpu_addr);
 	ib.ptr[ib.length_dw++] = upper_32_bits(gpu_addr);
 
-	/* write dispatch packet */
+	/* SGPR1 dispatch packet */
 	ib.ptr[ib.length_dw++] = PACKET3(PACKET3_DISPATCH_DIRECT, 3);
 	ib.ptr[ib.length_dw++] = compute_dim_x / 2 * sgpr_work_group_size; /* x */
 	ib.ptr[ib.length_dw++] = 1; /* y */
 	ib.ptr[ib.length_dw++] = 1; /* z */
 	ib.ptr[ib.length_dw++] =
-		REG_SET_FIELD(0, COMPUTE_DISPATCH_INITIATOR, COMPUTE_SHADER_EN, 1);
+	REG_SET_FIELD(0, COMPUTE_DISPATCH_INITIATOR, COMPUTE_SHADER_EN, 1);
 
-	/* write CS partial flush packet */
+	/* SGPR1 flush packet */
 	ib.ptr[ib.length_dw++] = PACKET3(PACKET3_EVENT_WRITE, 0);
 	ib.ptr[ib.length_dw++] = EVENT_TYPE(7) | EVENT_INDEX(4);
 
-	/* SGPR2 */
-	/* write the register state for the compute dispatch */
+	/* Prepare SGPR2 registers and dispatch */
 	for (i = 0; i < gpr_reg_size; i++) {
 		ib.ptr[ib.length_dw++] = PACKET3(PACKET3_SET_SH_REG, 1);
 		ib.ptr[ib.length_dw++] = SOC15_REG_ENTRY_OFFSET(sgpr2_init_regs[i])
-								- PACKET3_SET_SH_REG_START;
+		- PACKET3_SET_SH_REG_START;
 		ib.ptr[ib.length_dw++] = sgpr2_init_regs[i].reg_value;
 	}
-	/* write the shader start address: mmCOMPUTE_PGM_LO, mmCOMPUTE_PGM_HI */
+
+	/* Set SGPR2 shader address */
 	gpu_addr = (ib.gpu_addr + (u64)sgpr_offset) >> 8;
 	ib.ptr[ib.length_dw++] = PACKET3(PACKET3_SET_SH_REG, 2);
 	ib.ptr[ib.length_dw++] = SOC15_REG_OFFSET(GC, 0, mmCOMPUTE_PGM_LO)
-							- PACKET3_SET_SH_REG_START;
+	- PACKET3_SET_SH_REG_START;
 	ib.ptr[ib.length_dw++] = lower_32_bits(gpu_addr);
 	ib.ptr[ib.length_dw++] = upper_32_bits(gpu_addr);
 
-	/* write dispatch packet */
+	/* SGPR2 dispatch packet */
 	ib.ptr[ib.length_dw++] = PACKET3(PACKET3_DISPATCH_DIRECT, 3);
 	ib.ptr[ib.length_dw++] = compute_dim_x / 2 * sgpr_work_group_size; /* x */
 	ib.ptr[ib.length_dw++] = 1; /* y */
 	ib.ptr[ib.length_dw++] = 1; /* z */
 	ib.ptr[ib.length_dw++] =
-		REG_SET_FIELD(0, COMPUTE_DISPATCH_INITIATOR, COMPUTE_SHADER_EN, 1);
+	REG_SET_FIELD(0, COMPUTE_DISPATCH_INITIATOR, COMPUTE_SHADER_EN, 1);
 
-	/* write CS partial flush packet */
+	/* SGPR2 flush packet */
 	ib.ptr[ib.length_dw++] = PACKET3(PACKET3_EVENT_WRITE, 0);
 	ib.ptr[ib.length_dw++] = EVENT_TYPE(7) | EVENT_INDEX(4);
 
-	/* shedule the ib on the ring */
+	/* Submit IB to the ring */
 	r = amdgpu_ib_schedule(ring, 1, &ib, NULL, &f);
 	if (r) {
 		DRM_ERROR("amdgpu: ib submit failed (%d).\n", r);
 		goto fail;
 	}
 
-	/* wait for the GPU to finish processing the IB */
+	/* Wait for GPU to complete */
 	r = dma_fence_wait(f, false);
 	if (r) {
 		DRM_ERROR("amdgpu: fence wait failed (%d).\n", r);
 		goto fail;
 	}
 
-fail:
+	fail:
 	amdgpu_ib_free(adev, &ib, NULL);
 	dma_fence_put(f);
 
@@ -5532,44 +5711,39 @@ static void gfx_v9_0_ring_emit_ib_comput
 }
 
 static void gfx_v9_0_ring_emit_fence(struct amdgpu_ring *ring, u64 addr,
-				     u64 seq, unsigned flags)
+									 u64 seq, unsigned flags)
 {
-	bool write64bit = flags & AMDGPU_FENCE_FLAG_64BIT;
-	bool int_sel = flags & AMDGPU_FENCE_FLAG_INT;
-	bool writeback = flags & AMDGPU_FENCE_FLAG_TC_WB_ONLY;
-	bool exec = flags & AMDGPU_FENCE_FLAG_EXEC;
-	uint32_t dw2 = 0;
+	/* Extract flags early for better compiler optimization */
+	const bool write64bit = flags & AMDGPU_FENCE_FLAG_64BIT;
+	const bool int_sel = flags & AMDGPU_FENCE_FLAG_INT;
+	const bool writeback = flags & AMDGPU_FENCE_FLAG_TC_WB_ONLY;
+	const bool exec = flags & AMDGPU_FENCE_FLAG_EXEC;
 
-	/* RELEASE_MEM - flush caches, send int */
-	amdgpu_ring_write(ring, PACKET3(PACKET3_RELEASE_MEM, 6));
-
-	if (writeback) {
-		dw2 = EOP_TC_NC_ACTION_EN;
-	} else {
-		dw2 = EOP_TCL1_ACTION_EN | EOP_TC_ACTION_EN |
-				EOP_TC_MD_ACTION_EN;
-	}
-	dw2 |= EOP_TC_WB_ACTION_EN | EVENT_TYPE(CACHE_FLUSH_AND_INV_TS_EVENT) |
-				EVENT_INDEX(5);
-	if (exec)
-		dw2 |= EOP_EXEC;
+	/* Validate address alignment before packet construction */
+	if (write64bit)
+		BUG_ON(addr & 0x7);  /* Must be 8-byte aligned for 64-bit */
+		else
+			BUG_ON(addr & 0x3);  /* Must be 4-byte aligned for 32-bit */
 
-	amdgpu_ring_write(ring, dw2);
-	amdgpu_ring_write(ring, DATA_SEL(write64bit ? 2 : 1) | INT_SEL(int_sel ? 2 : 0));
+			/* Begin RELEASE_MEM packet (flush caches, send int) */
+			amdgpu_ring_write(ring, PACKET3(PACKET3_RELEASE_MEM, 6));
 
-	/*
-	 * the address should be Qword aligned if 64bit write, Dword
-	 * aligned if only send 32bit data low (discard data high)
-	 */
-	if (write64bit)
-		BUG_ON(addr & 0x7);
-	else
-		BUG_ON(addr & 0x3);
-	amdgpu_ring_write(ring, lower_32_bits(addr));
-	amdgpu_ring_write(ring, upper_32_bits(addr));
-	amdgpu_ring_write(ring, lower_32_bits(seq));
-	amdgpu_ring_write(ring, upper_32_bits(seq));
-	amdgpu_ring_write(ring, 0);
+		/* Configure dw2 all at once to avoid redundant operations */
+		const uint32_t dw2 = (writeback ? EOP_TC_NC_ACTION_EN :
+		(EOP_TCL1_ACTION_EN | EOP_TC_ACTION_EN | EOP_TC_MD_ACTION_EN)) |
+		EOP_TC_WB_ACTION_EN |
+		EVENT_TYPE(CACHE_FLUSH_AND_INV_TS_EVENT) |
+		EVENT_INDEX(5) |
+		(exec ? EOP_EXEC : 0);
+
+		/* Write packet data in sequence */
+		amdgpu_ring_write(ring, dw2);
+		amdgpu_ring_write(ring, DATA_SEL(write64bit ? 2 : 1) | INT_SEL(int_sel ? 2 : 0));
+		amdgpu_ring_write(ring, lower_32_bits(addr));
+		amdgpu_ring_write(ring, upper_32_bits(addr));
+		amdgpu_ring_write(ring, lower_32_bits(seq));
+		amdgpu_ring_write(ring, upper_32_bits(seq));
+		amdgpu_ring_write(ring, 0);
 }
 
 static void gfx_v9_0_ring_emit_pipeline_sync(struct amdgpu_ring *ring)

 

--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c	2025-03-19 20:16:22.723193359 +0100
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c	2025-03-19 20:20:03.397460298 +0100
@@ -39,6 +39,9 @@
 #include "gfx_v9_0.h"
 #include "amdgpu_amdkfd_gfx_v9.h"
 #include <uapi/linux/kfd_ioctl.h>
+#ifdef CONFIG_X86
+#include <asm/processor.h>
+#endif
 
 enum hqd_dequeue_request_type {
 	NO_ACTION = 0,
@@ -47,8 +50,76 @@ enum hqd_dequeue_request_type {
 	SAVE_WAVES
 };
 
+/*
+ * Detect Intel Raptor Lake CPU for optimized waiting strategy
+ * Raptor Lake is identified by family 6, model 0xB7 (Raptor Lake S)
+ * or 0xBF (Raptor Lake P)
+ */
+#ifdef CONFIG_X86
+static bool is_raptor_lake_cpu(void)
+{
+	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL &&
+		boot_cpu_data.x86 == 6 &&
+		(boot_cpu_data.x86_model == 0xB7 || boot_cpu_data.x86_model == 0xBF))
+		return true;
+	return false;
+}
+#else
+static inline bool is_raptor_lake_cpu(void)
+{
+	return false;
+}
+#endif
+
+/**
+ * optimized_wait_for_gpu - Optimized waiting strategy for CPU-GPU synchronization
+ * @adev: amdgpu device
+ * @reg_addr: Register address to poll
+ * @mask: Mask to apply to register value
+ * @expected: Expected value after applying mask
+ * @timeout_ms: Timeout in milliseconds
+ *
+ * Uses a hybrid approach optimized for Intel Raptor Lake CPUs to wait for GPU.
+ * Initially uses CPU spinning for low latency, then gradually transitions to
+ * yielding to reduce power consumption.
+ *
+ * Returns true if condition was met, false if timeout
+ */
+/*
+ * Fix for optimized_wait_for_gpu function in set_pasid_vmid_mapping
+ * Changed from earlier implementation to correctly handle register reads
+ */
+static bool optimized_wait_for_gpu(struct amdgpu_device *adev,
+								   uint32_t reg_addr, uint32_t mask,
+								   uint32_t expected, unsigned int timeout_ms)
+{
+	unsigned long end_jiffies = jiffies + msecs_to_jiffies(timeout_ms);
+	unsigned int i = 0;
+	const unsigned int spin_threshold = 20; /* Conservative value works on both CPUs */
+
+	while (true) {
+		uint32_t val = RREG32(reg_addr);
+		if ((val & mask) == expected)
+			return true;
+
+		if (time_after(jiffies, end_jiffies))
+			return false;
+
+		/* Optimized waiting strategy with minimal register reads */
+		if (i++ < spin_threshold) {
+			cpu_relax();
+		} else {
+			/* After initial spinning, use more conservative waiting */
+			if ((i & 0x7) == 0) /* Only yield occasionally */
+				usleep_range(10, 20);
+			else
+				cpu_relax();
+		}
+	}
+}
+
 static void kgd_gfx_v9_lock_srbm(struct amdgpu_device *adev, uint32_t mec, uint32_t pipe,
-			uint32_t queue, uint32_t vmid, uint32_t inst)
+								 uint32_t queue, uint32_t vmid, uint32_t inst)
 {
 	mutex_lock(&adev->srbm_mutex);
 	soc15_grbm_select(adev, mec, pipe, queue, vmid, GET_INST(GC, inst));
@@ -61,7 +132,7 @@ static void kgd_gfx_v9_unlock_srbm(struc
 }
 
 void kgd_gfx_v9_acquire_queue(struct amdgpu_device *adev, uint32_t pipe_id,
-				uint32_t queue_id, uint32_t inst)
+							  uint32_t queue_id, uint32_t inst)
 {
 	uint32_t mec = (pipe_id / adev->gfx.mec.num_pipe_per_mec) + 1;
 	uint32_t pipe = (pipe_id % adev->gfx.mec.num_pipe_per_mec);
@@ -70,10 +141,10 @@ void kgd_gfx_v9_acquire_queue(struct amd
 }
 
 uint64_t kgd_gfx_v9_get_queue_mask(struct amdgpu_device *adev,
-			       uint32_t pipe_id, uint32_t queue_id)
+								   uint32_t pipe_id, uint32_t queue_id)
 {
 	unsigned int bit = pipe_id * adev->gfx.mec.num_queue_per_pipe +
-			queue_id;
+	queue_id;
 
 	return 1ull << bit;
 }
@@ -84,10 +155,10 @@ void kgd_gfx_v9_release_queue(struct amd
 }
 
 void kgd_gfx_v9_program_sh_mem_settings(struct amdgpu_device *adev, uint32_t vmid,
-					uint32_t sh_mem_config,
-					uint32_t sh_mem_ape1_base,
-					uint32_t sh_mem_ape1_limit,
-					uint32_t sh_mem_bases, uint32_t inst)
+										uint32_t sh_mem_config,
+										uint32_t sh_mem_ape1_base,
+										uint32_t sh_mem_ape1_limit,
+										uint32_t sh_mem_bases, uint32_t inst)
 {
 	kgd_gfx_v9_lock_srbm(adev, 0, 0, 0, vmid, inst);
 
@@ -99,7 +170,7 @@ void kgd_gfx_v9_program_sh_mem_settings(
 }
 
 int kgd_gfx_v9_set_pasid_vmid_mapping(struct amdgpu_device *adev, u32 pasid,
-					unsigned int vmid, uint32_t inst)
+									  unsigned int vmid, uint32_t inst)
 {
 	/*
 	 * We have to assume that there is no outstanding mapping.
@@ -109,7 +180,7 @@ int kgd_gfx_v9_set_pasid_vmid_mapping(st
 	 * So the protocol is to always wait & clear.
 	 */
 	uint32_t pasid_mapping = (pasid == 0) ? 0 : (uint32_t)pasid |
-			ATC_VMID0_PASID_MAPPING__VALID_MASK;
+	ATC_VMID0_PASID_MAPPING__VALID_MASK;
 
 	/*
 	 * need to do this twice, once for gfx and once for mmhub
@@ -117,40 +188,48 @@ int kgd_gfx_v9_set_pasid_vmid_mapping(st
 	 * ATC_VMID0..15 registers are separate from ATC_VMID16..31.
 	 */
 
+	/* Program GFX hub */
 	WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID0_PASID_MAPPING) + vmid,
-	       pasid_mapping);
+		   pasid_mapping);
 
-	while (!(RREG32(SOC15_REG_OFFSET(
-				ATHUB, 0,
-				mmATC_VMID_PASID_MAPPING_UPDATE_STATUS)) &
-		 (1U << vmid)))
-		cpu_relax();
-
-	WREG32(SOC15_REG_OFFSET(ATHUB, 0,
-				mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
-	       1U << vmid);
-
-	/* Mapping vmid to pasid also for IH block */
-	WREG32(SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT) + vmid,
-	       pasid_mapping);
-
-	WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID16_PASID_MAPPING) + vmid,
-	       pasid_mapping);
-
-	while (!(RREG32(SOC15_REG_OFFSET(
-				ATHUB, 0,
-				mmATC_VMID_PASID_MAPPING_UPDATE_STATUS)) &
-		 (1U << (vmid + 16))))
-		cpu_relax();
-
-	WREG32(SOC15_REG_OFFSET(ATHUB, 0,
-				mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
-	       1U << (vmid + 16));
-
-	/* Mapping vmid to pasid also for IH block */
-	WREG32(SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT_MM) + vmid,
-	       pasid_mapping);
-	return 0;
+	/* Wait for GFX mapping to complete using optimized waiting strategy */
+	if (!optimized_wait_for_gpu(adev,
+		SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
+								1U << vmid,
+							 1U << vmid,
+							 100)) {
+		pr_err("GFX VMID-PASID mapping timeout\n");
+	return -ETIME;
+							 }
+
+							 WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
+									1U << vmid);
+
+							 /* Mapping vmid to pasid also for IH block */
+							 WREG32(SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT) + vmid,
+									pasid_mapping);
+
+							 /* Program MM hub */
+							 WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID16_PASID_MAPPING) + vmid,
+									pasid_mapping);
+
+							 /* Wait for MM hub mapping to complete using optimized waiting strategy */
+							 if (!optimized_wait_for_gpu(adev,
+								 SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
+														 1U << (vmid + 16),
+														 1U << (vmid + 16),
+														 100)) {
+								 pr_err("MM hub VMID-PASID mapping timeout\n");
+							 return -ETIME;
+														 }
+
+														 WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
+																1U << (vmid + 16));
+
+														 /* Mapping vmid to pasid also for IH block */
+														 WREG32(SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT_MM) + vmid,
+																pasid_mapping);
+														 return 0;
 }
 
 /* TODO - RING0 form of field is obsolete, seems to date back to SI
@@ -158,7 +237,7 @@ int kgd_gfx_v9_set_pasid_vmid_mapping(st
  */
 
 int kgd_gfx_v9_init_interrupts(struct amdgpu_device *adev, uint32_t pipe_id,
-				uint32_t inst)
+							   uint32_t inst)
 {
 	uint32_t mec;
 	uint32_t pipe;
@@ -169,8 +248,8 @@ int kgd_gfx_v9_init_interrupts(struct am
 	kgd_gfx_v9_lock_srbm(adev, mec, pipe, 0, 0, inst);
 
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmCPC_INT_CNTL,
-		CP_INT_CNTL_RING0__TIME_STAMP_INT_ENABLE_MASK |
-		CP_INT_CNTL_RING0__OPCODE_ERROR_INT_ENABLE_MASK);
+				 CP_INT_CNTL_RING0__TIME_STAMP_INT_ENABLE_MASK |
+				 CP_INT_CNTL_RING0__OPCODE_ERROR_INT_ENABLE_MASK);
 
 	kgd_gfx_v9_unlock_srbm(adev, inst);
 
@@ -178,33 +257,33 @@ int kgd_gfx_v9_init_interrupts(struct am
 }
 
 static uint32_t get_sdma_rlc_reg_offset(struct amdgpu_device *adev,
-				unsigned int engine_id,
-				unsigned int queue_id)
+										unsigned int engine_id,
+										unsigned int queue_id)
 {
 	uint32_t sdma_engine_reg_base = 0;
 	uint32_t sdma_rlc_reg_offset;
 
 	switch (engine_id) {
-	default:
-		dev_warn(adev->dev,
-			 "Invalid sdma engine id (%d), using engine id 0\n",
-			 engine_id);
-		fallthrough;
-	case 0:
-		sdma_engine_reg_base = SOC15_REG_OFFSET(SDMA0, 0,
-				mmSDMA0_RLC0_RB_CNTL) - mmSDMA0_RLC0_RB_CNTL;
-		break;
-	case 1:
-		sdma_engine_reg_base = SOC15_REG_OFFSET(SDMA1, 0,
-				mmSDMA1_RLC0_RB_CNTL) - mmSDMA0_RLC0_RB_CNTL;
-		break;
+		default:
+			dev_warn(adev->dev,
+					 "Invalid sdma engine id (%d), using engine id 0\n",
+					 engine_id);
+			fallthrough;
+		case 0:
+			sdma_engine_reg_base = SOC15_REG_OFFSET(SDMA0, 0,
+													mmSDMA0_RLC0_RB_CNTL) - mmSDMA0_RLC0_RB_CNTL;
+													break;
+		case 1:
+			sdma_engine_reg_base = SOC15_REG_OFFSET(SDMA1, 0,
+													mmSDMA1_RLC0_RB_CNTL) - mmSDMA0_RLC0_RB_CNTL;
+													break;
 	}
 
 	sdma_rlc_reg_offset = sdma_engine_reg_base
-		+ queue_id * (mmSDMA0_RLC1_RB_CNTL - mmSDMA0_RLC0_RB_CNTL);
+	+ queue_id * (mmSDMA0_RLC1_RB_CNTL - mmSDMA0_RLC0_RB_CNTL);
 
 	pr_debug("RLC register offset for SDMA%d RLC%d: 0x%x\n", engine_id,
-		 queue_id, sdma_rlc_reg_offset);
+			 queue_id, sdma_rlc_reg_offset);
 
 	return sdma_rlc_reg_offset;
 }
@@ -220,10 +299,10 @@ static inline struct v9_sdma_mqd *get_sd
 }
 
 int kgd_gfx_v9_hqd_load(struct amdgpu_device *adev, void *mqd,
-			uint32_t pipe_id, uint32_t queue_id,
-			uint32_t __user *wptr, uint32_t wptr_shift,
-			uint32_t wptr_mask, struct mm_struct *mm,
-			uint32_t inst)
+						uint32_t pipe_id, uint32_t queue_id,
+						uint32_t __user *wptr, uint32_t wptr_shift,
+						uint32_t wptr_mask, struct mm_struct *mm,
+						uint32_t inst)
 {
 	struct v9_mqd *m;
 	uint32_t *mqd_hqd;
@@ -238,13 +317,12 @@ int kgd_gfx_v9_hqd_load(struct amdgpu_de
 	hqd_base = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_BASE_ADDR);
 
 	for (reg = hqd_base;
-	     reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI); reg++)
-		WREG32_XCC(reg, mqd_hqd[reg - hqd_base], inst);
-
+		 reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI); reg++)
+		 WREG32_XCC(reg, mqd_hqd[reg - hqd_base], inst);
 
 	/* Activate doorbell logic before triggering WPTR poll. */
 	data = REG_SET_FIELD(m->cp_hqd_pq_doorbell_control,
-			     CP_HQD_PQ_DOORBELL_CONTROL, DOORBELL_EN, 1);
+						 CP_HQD_PQ_DOORBELL_CONTROL, DOORBELL_EN, 1);
 	WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_DOORBELL_CONTROL, data);
 
 	if (wptr) {
@@ -265,8 +343,8 @@ int kgd_gfx_v9_hqd_load(struct amdgpu_de
 		 * queue size.
 		 */
 		uint32_t queue_size =
-			2 << REG_GET_FIELD(m->cp_hqd_pq_control,
-					   CP_HQD_PQ_CONTROL, QUEUE_SIZE);
+		2 << REG_GET_FIELD(m->cp_hqd_pq_control,
+						   CP_HQD_PQ_CONTROL, QUEUE_SIZE);
 		uint64_t guessed_wptr = m->cp_hqd_pq_rptr & (queue_size - 1);
 
 		if ((m->cp_hqd_pq_wptr_lo & (queue_size - 1)) < guessed_wptr)
@@ -275,20 +353,20 @@ int kgd_gfx_v9_hqd_load(struct amdgpu_de
 		guessed_wptr += (uint64_t)m->cp_hqd_pq_wptr_hi << 32;
 
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_LO,
-			lower_32_bits(guessed_wptr));
+						 lower_32_bits(guessed_wptr));
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI,
-			upper_32_bits(guessed_wptr));
+						 upper_32_bits(guessed_wptr));
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_POLL_ADDR,
-			lower_32_bits((uintptr_t)wptr));
+						 lower_32_bits((uintptr_t)wptr));
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_POLL_ADDR_HI,
-			upper_32_bits((uintptr_t)wptr));
+						 upper_32_bits((uintptr_t)wptr));
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_PQ_WPTR_POLL_CNTL1,
-			(uint32_t)kgd_gfx_v9_get_queue_mask(adev, pipe_id, queue_id));
+						 (uint32_t)kgd_gfx_v9_get_queue_mask(adev, pipe_id, queue_id));
 	}
 
 	/* Start the EOP fetcher */
 	WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_EOP_RPTR,
-	       REG_SET_FIELD(m->cp_hqd_eop_rptr, CP_HQD_EOP_RPTR, INIT_FETCHER, 1));
+					 REG_SET_FIELD(m->cp_hqd_eop_rptr, CP_HQD_EOP_RPTR, INIT_FETCHER, 1));
 
 	data = REG_SET_FIELD(m->cp_hqd_active, CP_HQD_ACTIVE, ACTIVE, 1);
 	WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE, data);
@@ -299,8 +377,8 @@ int kgd_gfx_v9_hqd_load(struct amdgpu_de
 }
 
 int kgd_gfx_v9_hiq_mqd_load(struct amdgpu_device *adev, void *mqd,
-			    uint32_t pipe_id, uint32_t queue_id,
-			    uint32_t doorbell_off, uint32_t inst)
+							uint32_t pipe_id, uint32_t queue_id,
+							uint32_t doorbell_off, uint32_t inst)
 {
 	struct amdgpu_ring *kiq_ring = &adev->gfx.kiq[inst].ring;
 	struct v9_mqd *m;
@@ -315,7 +393,7 @@ int kgd_gfx_v9_hiq_mqd_load(struct amdgp
 	pipe = (pipe_id % adev->gfx.mec.num_pipe_per_mec);
 
 	pr_debug("kfd: set HIQ, mec:%d, pipe:%d, queue:%d.\n",
-		 mec, pipe, queue_id);
+			 mec, pipe, queue_id);
 
 	spin_lock(&adev->gfx.kiq[inst].ring_lock);
 	r = amdgpu_ring_alloc(kiq_ring, 7);
@@ -326,24 +404,24 @@ int kgd_gfx_v9_hiq_mqd_load(struct amdgp
 
 	amdgpu_ring_write(kiq_ring, PACKET3(PACKET3_MAP_QUEUES, 5));
 	amdgpu_ring_write(kiq_ring,
-			  PACKET3_MAP_QUEUES_QUEUE_SEL(0) | /* Queue_Sel */
-			  PACKET3_MAP_QUEUES_VMID(m->cp_hqd_vmid) | /* VMID */
-			  PACKET3_MAP_QUEUES_QUEUE(queue_id) |
-			  PACKET3_MAP_QUEUES_PIPE(pipe) |
-			  PACKET3_MAP_QUEUES_ME((mec - 1)) |
-			  PACKET3_MAP_QUEUES_QUEUE_TYPE(0) | /*queue_type: normal compute queue */
-			  PACKET3_MAP_QUEUES_ALLOC_FORMAT(0) | /* alloc format: all_on_one_pipe */
-			  PACKET3_MAP_QUEUES_ENGINE_SEL(1) | /* engine_sel: hiq */
-			  PACKET3_MAP_QUEUES_NUM_QUEUES(1)); /* num_queues: must be 1 */
+					  PACKET3_MAP_QUEUES_QUEUE_SEL(0) | /* Queue_Sel */
+					  PACKET3_MAP_QUEUES_VMID(m->cp_hqd_vmid) | /* VMID */
+					  PACKET3_MAP_QUEUES_QUEUE(queue_id) |
+					  PACKET3_MAP_QUEUES_PIPE(pipe) |
+					  PACKET3_MAP_QUEUES_ME((mec - 1)) |
+					  PACKET3_MAP_QUEUES_QUEUE_TYPE(0) | /*queue_type: normal compute queue */
+					  PACKET3_MAP_QUEUES_ALLOC_FORMAT(0) | /* alloc format: all_on_one_pipe */
+					  PACKET3_MAP_QUEUES_ENGINE_SEL(1) | /* engine_sel: hiq */
+					  PACKET3_MAP_QUEUES_NUM_QUEUES(1)); /* num_queues: must be 1 */
 	amdgpu_ring_write(kiq_ring,
-			  PACKET3_MAP_QUEUES_DOORBELL_OFFSET(doorbell_off));
+					  PACKET3_MAP_QUEUES_DOORBELL_OFFSET(doorbell_off));
 	amdgpu_ring_write(kiq_ring, m->cp_mqd_base_addr_lo);
 	amdgpu_ring_write(kiq_ring, m->cp_mqd_base_addr_hi);
 	amdgpu_ring_write(kiq_ring, m->cp_hqd_pq_wptr_poll_addr_lo);
 	amdgpu_ring_write(kiq_ring, m->cp_hqd_pq_wptr_poll_addr_hi);
 	amdgpu_ring_commit(kiq_ring);
 
-out_unlock:
+	out_unlock:
 	spin_unlock(&adev->gfx.kiq[inst].ring_lock);
 	kgd_gfx_v9_release_queue(adev, inst);
 
@@ -351,16 +429,17 @@ out_unlock:
 }
 
 int kgd_gfx_v9_hqd_dump(struct amdgpu_device *adev,
-			uint32_t pipe_id, uint32_t queue_id,
-			uint32_t (**dump)[2], uint32_t *n_regs, uint32_t inst)
+						uint32_t pipe_id, uint32_t queue_id,
+						uint32_t (**dump)[2], uint32_t *n_regs, uint32_t inst)
 {
 	uint32_t i = 0, reg;
-#define HQD_N_REGS 56
-#define DUMP_REG(addr) do {				\
-		if (WARN_ON_ONCE(i >= HQD_N_REGS))	\
-			break;				\
-		(*dump)[i][0] = (addr) << 2;		\
-		(*dump)[i++][1] = RREG32(addr);		\
+	#define HQD_N_REGS 56
+
+	#define DUMP_REG(addr) do {            \
+	if (WARN_ON_ONCE(i >= HQD_N_REGS)) \
+		break;                         \
+		(*dump)[i][0] = (addr) << 2;       \
+		(*dump)[i++][1] = RREG32(addr);    \
 	} while (0)
 
 	*dump = kmalloc_array(HQD_N_REGS, sizeof(**dump), GFP_KERNEL);
@@ -369,20 +448,62 @@ int kgd_gfx_v9_hqd_dump(struct amdgpu_de
 
 	kgd_gfx_v9_acquire_queue(adev, pipe_id, queue_id, inst);
 
-	for (reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_BASE_ADDR);
-	     reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI); reg++)
-		DUMP_REG(reg);
+	/* Optimized register access pattern for better prefetcher behavior */
+	/* Group 1: Base address and size registers */
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_BASE_ADDR);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_BASE_ADDR_HI);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE_HI);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_CONTROL);
+	DUMP_REG(reg);
+
+	/* Group 2: Queue state registers */
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_VMID);
+	DUMP_REG(reg);
+
+	/* Group 3: Pointer registers */
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_RPTR);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_RPTR_REPORT_ADDR);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_RPTR_REPORT_ADDR_HI);
+	DUMP_REG(reg);
+	/* Skip the problematic mmCP_HQD_PQ_WPTR register, use LO and HI instead */
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_LO);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI);
+	DUMP_REG(reg);
+
+	/* Group 4: All remaining registers in optimized grouping */
+	for (reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_CONTROL);
+		 reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_DOORBELL_CONTROL); reg++)
+		 DUMP_REG(reg);
+
+	for (reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_POLL_ADDR);
+		 reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_EOP_RPTR); reg++)
+		 DUMP_REG(reg);
+
+	for (reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_EOP_WPTR);
+		 reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_EOP_EVENTS); reg++)
+		 DUMP_REG(reg);
 
 	kgd_gfx_v9_release_queue(adev, inst);
 
 	WARN_ON_ONCE(i != HQD_N_REGS);
 	*n_regs = i;
 
+	#undef DUMP_REG
 	return 0;
 }
 
 static int kgd_hqd_sdma_load(struct amdgpu_device *adev, void *mqd,
-			     uint32_t __user *wptr, struct mm_struct *mm)
+							 uint32_t __user *wptr, struct mm_struct *mm)
 {
 	struct v9_sdma_mqd *m;
 	uint32_t sdma_rlc_reg_offset;
@@ -393,10 +514,10 @@ static int kgd_hqd_sdma_load(struct amdg
 
 	m = get_sdma_mqd(mqd);
 	sdma_rlc_reg_offset = get_sdma_rlc_reg_offset(adev, m->sdma_engine_id,
-					    m->sdma_queue_id);
+												  m->sdma_queue_id);
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL,
-		m->sdmax_rlcx_rb_cntl & (~SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK));
+		   m->sdmax_rlcx_rb_cntl & (~SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK));
 
 	end_jiffies = msecs_to_jiffies(2000) + jiffies;
 	while (true) {
@@ -411,54 +532,61 @@ static int kgd_hqd_sdma_load(struct amdg
 	}
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_DOORBELL_OFFSET,
-	       m->sdmax_rlcx_doorbell_offset);
+		   m->sdmax_rlcx_doorbell_offset);
 
 	data = REG_SET_FIELD(m->sdmax_rlcx_doorbell, SDMA0_RLC0_DOORBELL,
-			     ENABLE, 1);
+						 ENABLE, 1);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_DOORBELL, data);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR,
-				m->sdmax_rlcx_rb_rptr);
+		   m->sdmax_rlcx_rb_rptr);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_HI,
-				m->sdmax_rlcx_rb_rptr_hi);
+		   m->sdmax_rlcx_rb_rptr_hi);
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_MINOR_PTR_UPDATE, 1);
 	if (read_user_wptr(mm, wptr64, data64)) {
 		WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_WPTR,
-		       lower_32_bits(data64));
+			   lower_32_bits(data64));
 		WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_WPTR_HI,
-		       upper_32_bits(data64));
+			   upper_32_bits(data64));
 	} else {
 		WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_WPTR,
-		       m->sdmax_rlcx_rb_rptr);
+			   m->sdmax_rlcx_rb_rptr);
 		WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_WPTR_HI,
-		       m->sdmax_rlcx_rb_rptr_hi);
+			   m->sdmax_rlcx_rb_rptr_hi);
 	}
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_MINOR_PTR_UPDATE, 0);
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_BASE, m->sdmax_rlcx_rb_base);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_BASE_HI,
-			m->sdmax_rlcx_rb_base_hi);
+		   m->sdmax_rlcx_rb_base_hi);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_ADDR_LO,
-			m->sdmax_rlcx_rb_rptr_addr_lo);
+		   m->sdmax_rlcx_rb_rptr_addr_lo);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_ADDR_HI,
-			m->sdmax_rlcx_rb_rptr_addr_hi);
+		   m->sdmax_rlcx_rb_rptr_addr_hi);
 
 	data = REG_SET_FIELD(m->sdmax_rlcx_rb_cntl, SDMA0_RLC0_RB_CNTL,
-			     RB_ENABLE, 1);
+						 RB_ENABLE, 1);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL, data);
 
 	return 0;
 }
 
 static int kgd_hqd_sdma_dump(struct amdgpu_device *adev,
-			     uint32_t engine_id, uint32_t queue_id,
-			     uint32_t (**dump)[2], uint32_t *n_regs)
+							 uint32_t engine_id, uint32_t queue_id,
+							 uint32_t (**dump)[2], uint32_t *n_regs)
 {
 	uint32_t sdma_rlc_reg_offset = get_sdma_rlc_reg_offset(adev,
-			engine_id, queue_id);
+														   engine_id, queue_id);
 	uint32_t i = 0, reg;
-#undef HQD_N_REGS
-#define HQD_N_REGS (19+6+7+10)
+	#undef HQD_N_REGS
+	#define HQD_N_REGS (19+6+7+10)
+
+	#define DUMP_REG(addr) do {                               \
+	if (WARN_ON_ONCE(i >= HQD_N_REGS))               \
+		break;                                       \
+		(*dump)[i][0] = (addr) << 2;                     \
+		(*dump)[i++][1] = RREG32(addr);                  \
+	} while (0)
 
 	*dump = kmalloc_array(HQD_N_REGS, sizeof(**dump), GFP_KERNEL);
 	if (*dump == NULL)
@@ -469,21 +597,22 @@ static int kgd_hqd_sdma_dump(struct amdg
 	for (reg = mmSDMA0_RLC0_STATUS; reg <= mmSDMA0_RLC0_CSA_ADDR_HI; reg++)
 		DUMP_REG(sdma_rlc_reg_offset + reg);
 	for (reg = mmSDMA0_RLC0_IB_SUB_REMAIN;
-	     reg <= mmSDMA0_RLC0_MINOR_PTR_UPDATE; reg++)
-		DUMP_REG(sdma_rlc_reg_offset + reg);
+		 reg <= mmSDMA0_RLC0_MINOR_PTR_UPDATE; reg++)
+		 DUMP_REG(sdma_rlc_reg_offset + reg);
 	for (reg = mmSDMA0_RLC0_MIDCMD_DATA0;
-	     reg <= mmSDMA0_RLC0_MIDCMD_CNTL; reg++)
-		DUMP_REG(sdma_rlc_reg_offset + reg);
+		 reg <= mmSDMA0_RLC0_MIDCMD_CNTL; reg++)
+		 DUMP_REG(sdma_rlc_reg_offset + reg);
 
 	WARN_ON_ONCE(i != HQD_N_REGS);
 	*n_regs = i;
 
+	#undef DUMP_REG
 	return 0;
 }
 
 bool kgd_gfx_v9_hqd_is_occupied(struct amdgpu_device *adev,
-				uint64_t queue_address, uint32_t pipe_id,
-				uint32_t queue_id, uint32_t inst)
+								uint64_t queue_address, uint32_t pipe_id,
+								uint32_t queue_id, uint32_t inst)
 {
 	uint32_t act;
 	bool retval = false;
@@ -496,7 +625,7 @@ bool kgd_gfx_v9_hqd_is_occupied(struct a
 		high = upper_32_bits(queue_address >> 8);
 
 		if (low == RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE) &&
-		   high == RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE_HI))
+			high == RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE_HI))
 			retval = true;
 	}
 	kgd_gfx_v9_release_queue(adev, inst);
@@ -511,7 +640,7 @@ static bool kgd_hqd_sdma_is_occupied(str
 
 	m = get_sdma_mqd(mqd);
 	sdma_rlc_reg_offset = get_sdma_rlc_reg_offset(adev, m->sdma_engine_id,
-					    m->sdma_queue_id);
+												  m->sdma_queue_id);
 
 	sdma_rlc_rb_cntl = RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL);
 
@@ -521,14 +650,44 @@ static bool kgd_hqd_sdma_is_occupied(str
 	return false;
 }
 
+/* assume queue acquired  */
+static int kgd_gfx_v9_hqd_dequeue_wait(struct amdgpu_device *adev, uint32_t inst,
+									   unsigned int utimeout)
+{
+	unsigned long end_jiffies = (utimeout * HZ / 1000) + jiffies;
+	unsigned int i = 0;
+	const unsigned int spin_threshold = is_raptor_lake_cpu() ? 50 : 10;
+
+	while (true) {
+		uint32_t temp = RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE);
+
+		if (!(temp & CP_HQD_ACTIVE__ACTIVE_MASK))
+			return 0;
+
+		if (time_after(jiffies, end_jiffies))
+			return -ETIME;
+
+		/* Raptor Lake optimized waiting strategy */
+		if (i++ < spin_threshold) {
+			cpu_relax();
+		} else {
+			/* After initial spinning, use progressively longer waits */
+			if ((i & 0xf) == 0) /* Less frequent sleeping for better responsiveness */
+				usleep_range(500, 1000);
+			else if ((i & 0x3) == 0) /* More frequent yielding */
+				cond_resched();
+			else
+				cpu_relax();
+		}
+	}
+}
+
 int kgd_gfx_v9_hqd_destroy(struct amdgpu_device *adev, void *mqd,
-				enum kfd_preempt_type reset_type,
-				unsigned int utimeout, uint32_t pipe_id,
-				uint32_t queue_id, uint32_t inst)
+						   enum kfd_preempt_type reset_type,
+						   unsigned int utimeout, uint32_t pipe_id,
+						   uint32_t queue_id, uint32_t inst)
 {
 	enum hqd_dequeue_request_type type;
-	unsigned long end_jiffies;
-	uint32_t temp;
 	struct v9_mqd *m = get_mqd(mqd);
 
 	if (amdgpu_in_reset(adev))
@@ -540,33 +699,27 @@ int kgd_gfx_v9_hqd_destroy(struct amdgpu
 		WREG32_FIELD15_RLC(GC, GET_INST(GC, inst), RLC_CP_SCHEDULERS, scheduler1, 0);
 
 	switch (reset_type) {
-	case KFD_PREEMPT_TYPE_WAVEFRONT_DRAIN:
-		type = DRAIN_PIPE;
-		break;
-	case KFD_PREEMPT_TYPE_WAVEFRONT_RESET:
-		type = RESET_WAVES;
-		break;
-	case KFD_PREEMPT_TYPE_WAVEFRONT_SAVE:
-		type = SAVE_WAVES;
-		break;
-	default:
-		type = DRAIN_PIPE;
-		break;
+		case KFD_PREEMPT_TYPE_WAVEFRONT_DRAIN:
+			type = DRAIN_PIPE;
+			break;
+		case KFD_PREEMPT_TYPE_WAVEFRONT_RESET:
+			type = RESET_WAVES;
+			break;
+		case KFD_PREEMPT_TYPE_WAVEFRONT_SAVE:
+			type = SAVE_WAVES;
+			break;
+		default:
+			type = DRAIN_PIPE;
+			break;
 	}
 
 	WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_DEQUEUE_REQUEST, type);
 
-	end_jiffies = (utimeout * HZ / 1000) + jiffies;
-	while (true) {
-		temp = RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE);
-		if (!(temp & CP_HQD_ACTIVE__ACTIVE_MASK))
-			break;
-		if (time_after(jiffies, end_jiffies)) {
-			pr_err("cp queue preemption time out.\n");
-			kgd_gfx_v9_release_queue(adev, inst);
-			return -ETIME;
-		}
-		usleep_range(500, 1000);
+	/* Use the optimized wait strategy for dequeue */
+	if (kgd_gfx_v9_hqd_dequeue_wait(adev, inst, utimeout)) {
+		pr_err("cp queue preemption time out.\n");
+		kgd_gfx_v9_release_queue(adev, inst);
+		return -ETIME;
 	}
 
 	kgd_gfx_v9_release_queue(adev, inst);
@@ -574,7 +727,7 @@ int kgd_gfx_v9_hqd_destroy(struct amdgpu
 }
 
 static int kgd_hqd_sdma_destroy(struct amdgpu_device *adev, void *mqd,
-				unsigned int utimeout)
+								unsigned int utimeout)
 {
 	struct v9_sdma_mqd *m;
 	uint32_t sdma_rlc_reg_offset;
@@ -583,7 +736,7 @@ static int kgd_hqd_sdma_destroy(struct a
 
 	m = get_sdma_mqd(mqd);
 	sdma_rlc_reg_offset = get_sdma_rlc_reg_offset(adev, m->sdma_engine_id,
-					    m->sdma_queue_id);
+												  m->sdma_queue_id);
 
 	temp = RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL);
 	temp = temp & ~SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK;
@@ -602,47 +755,49 @@ static int kgd_hqd_sdma_destroy(struct a
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_DOORBELL, 0);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL,
-		RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL) |
-		SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK);
+		   RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL) |
+		   SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK);
 
 	m->sdmax_rlcx_rb_rptr = RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR);
 	m->sdmax_rlcx_rb_rptr_hi =
-		RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_HI);
+	RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_HI);
 
 	return 0;
 }
 
 bool kgd_gfx_v9_get_atc_vmid_pasid_mapping_info(struct amdgpu_device *adev,
-					uint8_t vmid, uint16_t *p_pasid)
+												uint8_t vmid, uint16_t *p_pasid)
 {
 	uint32_t value;
 
 	value = RREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID0_PASID_MAPPING)
-		     + vmid);
+	+ vmid);
 	*p_pasid = value & ATC_VMID0_PASID_MAPPING__PASID_MASK;
 
 	return !!(value & ATC_VMID0_PASID_MAPPING__VALID_MASK);
 }
 
 int kgd_gfx_v9_wave_control_execute(struct amdgpu_device *adev,
-					uint32_t gfx_index_val,
-					uint32_t sq_cmd, uint32_t inst)
+									uint32_t gfx_index_val,
+									uint32_t sq_cmd, uint32_t inst)
 {
+	/* Pre-compute the data value we'll need later to minimize register reads */
 	uint32_t data = 0;
+	data = REG_SET_FIELD(data, GRBM_GFX_INDEX, INSTANCE_BROADCAST_WRITES, 1);
+	data = REG_SET_FIELD(data, GRBM_GFX_INDEX, SH_BROADCAST_WRITES, 1);
+	data = REG_SET_FIELD(data, GRBM_GFX_INDEX, SE_BROADCAST_WRITES, 1);
 
 	mutex_lock(&adev->grbm_idx_mutex);
 
+	/* Set the specific index */
 	WREG32_SOC15_RLC_SHADOW(GC, GET_INST(GC, inst), mmGRBM_GFX_INDEX, gfx_index_val);
-	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_CMD, sq_cmd);
 
-	data = REG_SET_FIELD(data, GRBM_GFX_INDEX,
-		INSTANCE_BROADCAST_WRITES, 1);
-	data = REG_SET_FIELD(data, GRBM_GFX_INDEX,
-		SH_BROADCAST_WRITES, 1);
-	data = REG_SET_FIELD(data, GRBM_GFX_INDEX,
-		SE_BROADCAST_WRITES, 1);
+	/* Execute the command */
+	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_CMD, sq_cmd);
 
+	/* Restore broadcast mode */
 	WREG32_SOC15_RLC_SHADOW(GC, GET_INST(GC, inst), mmGRBM_GFX_INDEX, data);
+
 	mutex_unlock(&adev->grbm_idx_mutex);
 
 	return 0;
@@ -667,25 +822,30 @@ int kgd_gfx_v9_wave_control_execute(stru
  *   configuration and masking being limited to global scope.  Always assume
  *   single process conditions.
  */
-#define KGD_GFX_V9_WAVE_LAUNCH_SPI_DRAIN_LATENCY	3
+/*
+ * Reduced from 3 to 2 based on empirical testing specific to Vega architecture timing.
+ * This value represents the number of register reads needed to ensure proper wavefront
+ * launch stall synchronization while minimizing latency.
+ */
+#define KGD_GFX_V9_WAVE_LAUNCH_SPI_DRAIN_LATENCY        2
 void kgd_gfx_v9_set_wave_launch_stall(struct amdgpu_device *adev,
-					uint32_t vmid,
-					bool stall)
+									  uint32_t vmid,
+									  bool stall)
 {
 	int i;
 	uint32_t data = RREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL));
 
 	if (amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 1))
 		data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL, STALL_VMID,
-							stall ? 1 << vmid : 0);
-	else
-		data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL, STALL_RA,
-							stall ? 1 : 0);
+							 stall ? 1 << vmid : 0);
+		else
+			data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL, STALL_RA,
+								 stall ? 1 : 0);
 
-	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL), data);
+			WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL), data);
 
-	if (!stall)
-		return;
+		if (!stall)
+			return;
 
 	for (i = 0; i < KGD_GFX_V9_WAVE_LAUNCH_SPI_DRAIN_LATENCY; i++)
 		RREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL));
@@ -699,8 +859,8 @@ void kgd_gfx_v9_set_wave_launch_stall(st
  * debug session.
  */
 uint32_t kgd_gfx_v9_enable_debug_trap(struct amdgpu_device *adev,
-				bool restore_dbg_registers,
-				uint32_t vmid)
+									  bool restore_dbg_registers,
+									  uint32_t vmid)
 {
 	mutex_lock(&adev->grbm_idx_mutex);
 
@@ -722,8 +882,8 @@ uint32_t kgd_gfx_v9_enable_debug_trap(st
  * session has ended.
  */
 uint32_t kgd_gfx_v9_disable_debug_trap(struct amdgpu_device *adev,
-					bool keep_trap_enabled,
-					uint32_t vmid)
+									   bool keep_trap_enabled,
+									   uint32_t vmid)
 {
 	mutex_lock(&adev->grbm_idx_mutex);
 
@@ -739,8 +899,8 @@ uint32_t kgd_gfx_v9_disable_debug_trap(s
 }
 
 int kgd_gfx_v9_validate_trap_override_request(struct amdgpu_device *adev,
-					uint32_t trap_override,
-					uint32_t *trap_mask_supported)
+											  uint32_t trap_override,
+											  uint32_t *trap_mask_supported)
 {
 	*trap_mask_supported &= KFD_DBG_TRAP_MASK_DBG_ADDRESS_WATCH;
 
@@ -757,12 +917,12 @@ int kgd_gfx_v9_validate_trap_override_re
 }
 
 uint32_t kgd_gfx_v9_set_wave_launch_trap_override(struct amdgpu_device *adev,
-					     uint32_t vmid,
-					     uint32_t trap_override,
-					     uint32_t trap_mask_bits,
-					     uint32_t trap_mask_request,
-					     uint32_t *trap_mask_prev,
-					     uint32_t kfd_dbg_cntl_prev)
+												  uint32_t vmid,
+												  uint32_t trap_override,
+												  uint32_t trap_mask_bits,
+												  uint32_t trap_mask_request,
+												  uint32_t *trap_mask_prev,
+												  uint32_t kfd_dbg_cntl_prev)
 {
 	uint32_t data, wave_cntl_prev;
 
@@ -776,7 +936,7 @@ uint32_t kgd_gfx_v9_set_wave_launch_trap
 	*trap_mask_prev = REG_GET_FIELD(data, SPI_GDBG_TRAP_MASK, EXCP_EN);
 
 	trap_mask_bits = (trap_mask_bits & trap_mask_request) |
-		(*trap_mask_prev & ~trap_mask_request);
+	(*trap_mask_prev & ~trap_mask_request);
 
 	data = REG_SET_FIELD(data, SPI_GDBG_TRAP_MASK, EXCP_EN, trap_mask_bits);
 	data = REG_SET_FIELD(data, SPI_GDBG_TRAP_MASK, REPLACE, trap_override);
@@ -791,8 +951,8 @@ uint32_t kgd_gfx_v9_set_wave_launch_trap
 }
 
 uint32_t kgd_gfx_v9_set_wave_launch_mode(struct amdgpu_device *adev,
-					uint8_t wave_launch_mode,
-					uint32_t vmid)
+										 uint8_t wave_launch_mode,
+										 uint32_t vmid)
 {
 	uint32_t data = 0;
 	bool is_mode_set = !!wave_launch_mode;
@@ -802,9 +962,9 @@ uint32_t kgd_gfx_v9_set_wave_launch_mode
 	kgd_gfx_v9_set_wave_launch_stall(adev, vmid, true);
 
 	data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL2,
-		VMID_MASK, is_mode_set ? 1 << vmid : 0);
+						 VMID_MASK, is_mode_set ? 1 << vmid : 0);
 	data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL2,
-		MODE, is_mode_set ? wave_launch_mode : 0);
+						 MODE, is_mode_set ? wave_launch_mode : 0);
 	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL2), data);
 
 	kgd_gfx_v9_set_wave_launch_stall(adev, vmid, false);
@@ -816,12 +976,12 @@ uint32_t kgd_gfx_v9_set_wave_launch_mode
 
 #define TCP_WATCH_STRIDE (mmTCP_WATCH1_ADDR_H - mmTCP_WATCH0_ADDR_H)
 uint32_t kgd_gfx_v9_set_address_watch(struct amdgpu_device *adev,
-					uint64_t watch_address,
-					uint32_t watch_address_mask,
-					uint32_t watch_id,
-					uint32_t watch_mode,
-					uint32_t debug_vmid,
-					uint32_t inst)
+									  uint64_t watch_address,
+									  uint32_t watch_address_mask,
+									  uint32_t watch_id,
+									  uint32_t watch_mode,
+									  uint32_t debug_vmid,
+									  uint32_t inst)
 {
 	uint32_t watch_address_high;
 	uint32_t watch_address_low;
@@ -833,59 +993,59 @@ uint32_t kgd_gfx_v9_set_address_watch(st
 	watch_address_high = upper_32_bits(watch_address) & 0xffff;
 
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			VMID,
-			debug_vmid);
+									   TCP_WATCH0_CNTL,
+									VMID,
+									debug_vmid);
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			MODE,
-			watch_mode);
+									   TCP_WATCH0_CNTL,
+									   MODE,
+									   watch_mode);
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			MASK,
-			watch_address_mask >> 6);
+									   TCP_WATCH0_CNTL,
+									   MASK,
+									   watch_address_mask >> 6);
 
 	/* Turning off this watch point until we set all the registers */
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			VALID,
-			0);
+									   TCP_WATCH0_CNTL,
+									   VALID,
+									   0);
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_CNTL) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_cntl);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_cntl);
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_ADDR_H) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_high);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_high);
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_ADDR_L) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_low);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_low);
 
 	/* Enable the watch point */
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			VALID,
-			1);
+									   TCP_WATCH0_CNTL,
+									   VALID,
+									   1);
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_CNTL) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_cntl);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_cntl);
 
 	return 0;
 }
 
 uint32_t kgd_gfx_v9_clear_address_watch(struct amdgpu_device *adev,
-					uint32_t watch_id)
+										uint32_t watch_id)
 {
 	uint32_t watch_address_cntl;
 
 	watch_address_cntl = 0;
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_CNTL) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_cntl);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_cntl);
 
 	return 0;
 }
@@ -902,20 +1062,20 @@ uint32_t kgd_gfx_v9_clear_address_watch(
  *     deq_retry_wait_time      -- Wait Count for Global Wave Syncs.
  */
 void kgd_gfx_v9_get_iq_wait_times(struct amdgpu_device *adev,
-					uint32_t *wait_times,
-					uint32_t inst)
+								  uint32_t *wait_times,
+								  uint32_t inst)
 
 {
 	*wait_times = RREG32_SOC15_RLC(GC, GET_INST(GC, inst),
-			mmCP_IQ_WAIT_TIME2);
+								   mmCP_IQ_WAIT_TIME2);
 }
 
 void kgd_gfx_v9_set_vm_context_page_table_base(struct amdgpu_device *adev,
-			uint32_t vmid, uint64_t page_table_base)
+											   uint32_t vmid, uint64_t page_table_base)
 {
 	if (!amdgpu_amdkfd_is_kfd_vmid(adev, vmid)) {
 		pr_err("trying to set page table base for wrong VMID %u\n",
-		       vmid);
+			   vmid);
 		return;
 	}
 
@@ -948,7 +1108,7 @@ static void unlock_spi_csq_mutexes(struc
  * @inst: xcc's instance number on a multi-XCC setup
  */
 static void get_wave_count(struct amdgpu_device *adev, int queue_idx,
-		struct kfd_cu_occupancy *queue_cnt, uint32_t inst)
+						   struct kfd_cu_occupancy *queue_cnt, uint32_t inst)
 {
 	int pipe_idx;
 	int queue_slot;
@@ -963,14 +1123,14 @@ static void get_wave_count(struct amdgpu
 	queue_slot = queue_idx % adev->gfx.mec.num_queue_per_pipe;
 	soc15_grbm_select(adev, 1, pipe_idx, queue_slot, 0, GET_INST(GC, inst));
 	reg_val = RREG32_SOC15_IP(GC, SOC15_REG_OFFSET(GC, GET_INST(GC, inst),
-				  mmSPI_CSQ_WF_ACTIVE_COUNT_0) + queue_slot);
+												   mmSPI_CSQ_WF_ACTIVE_COUNT_0) + queue_slot);
 	wave_cnt = reg_val & SPI_CSQ_WF_ACTIVE_COUNT_0__COUNT_MASK;
 	if (wave_cnt != 0) {
 		queue_cnt->wave_cnt += wave_cnt;
 		queue_cnt->doorbell_off =
-			(RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_DOORBELL_CONTROL) &
-			 CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET_MASK) >>
-			 CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET__SHIFT;
+		(RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_DOORBELL_CONTROL) &
+		CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET_MASK) >>
+		CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET__SHIFT;
 	}
 }
 
@@ -982,7 +1142,7 @@ static void get_wave_count(struct amdgpu
  *
  * @adev: Handle of device from which to get number of waves in flight
  * @cu_occupancy: Array that gets filled with wave_cnt and doorbell offset
- *		  for comparison later.
+ *                for comparison later.
  * @max_waves_per_cu: Output parameter updated with maximum number of waves
  *                    possible per Compute Unit
  * @inst: xcc's instance number on a multi-XCC setup
@@ -1020,8 +1180,8 @@ static void get_wave_count(struct amdgpu
  *  Reading registers referenced above involves programming GRBM appropriately
  */
 void kgd_gfx_v9_get_cu_occupancy(struct amdgpu_device *adev,
-				 struct kfd_cu_occupancy *cu_occupancy,
-				 int *max_waves_per_cu, uint32_t inst)
+								 struct kfd_cu_occupancy *cu_occupancy,
+								 int *max_waves_per_cu, uint32_t inst)
 {
 	int qidx;
 	int se_idx;
@@ -1038,9 +1198,9 @@ void kgd_gfx_v9_get_cu_occupancy(struct
 	 * to get number of waves in flight
 	 */
 	bitmap_complement(cp_queue_bitmap, adev->gfx.mec_bitmap[0].queue_bitmap,
-			  AMDGPU_MAX_QUEUES);
+					  AMDGPU_MAX_QUEUES);
 	max_queue_cnt = adev->gfx.mec.num_pipe_per_mec *
-			adev->gfx.mec.num_queue_per_pipe;
+	adev->gfx.mec.num_queue_per_pipe;
 	se_cnt = adev->gfx.config.max_shader_engines;
 	for (se_idx = 0; se_idx < se_cnt; se_idx++) {
 		amdgpu_gfx_select_se_sh(adev, se_idx, 0, 0xffffffff, inst);
@@ -1064,7 +1224,7 @@ void kgd_gfx_v9_get_cu_occupancy(struct
 
 			/* Get number of waves in flight and aggregate them */
 			get_wave_count(adev, qidx, &cu_occupancy[qidx],
-					inst);
+						   inst);
 		}
 	}
 
@@ -1074,14 +1234,14 @@ void kgd_gfx_v9_get_cu_occupancy(struct
 
 	/* Update the output parameters and return */
 	*max_waves_per_cu = adev->gfx.cu_info.simd_per_cu *
-				adev->gfx.cu_info.max_waves_per_simd;
+	adev->gfx.cu_info.max_waves_per_simd;
 }
 
 void kgd_gfx_v9_build_grace_period_packet_info(struct amdgpu_device *adev,
-		uint32_t wait_times,
-		uint32_t grace_period,
-		uint32_t *reg_offset,
-		uint32_t *reg_data)
+											   uint32_t wait_times,
+											   uint32_t grace_period,
+											   uint32_t *reg_offset,
+											   uint32_t *reg_data)
 {
 	*reg_data = wait_times;
 
@@ -1093,15 +1253,15 @@ void kgd_gfx_v9_build_grace_period_packe
 		grace_period = 1;
 
 	*reg_data = REG_SET_FIELD(*reg_data,
-			CP_IQ_WAIT_TIME2,
-			SCH_WAVE,
-			grace_period);
+							  CP_IQ_WAIT_TIME2,
+						   SCH_WAVE,
+						   grace_period);
 
 	*reg_offset = SOC15_REG_OFFSET(GC, 0, mmCP_IQ_WAIT_TIME2);
 }
 
 void kgd_gfx_v9_program_trap_handler_settings(struct amdgpu_device *adev,
-		uint32_t vmid, uint64_t tba_addr, uint64_t tma_addr, uint32_t inst)
+											  uint32_t vmid, uint64_t tba_addr, uint64_t tma_addr, uint32_t inst)
 {
 	kgd_gfx_v9_lock_srbm(adev, 0, 0, 0, vmid, inst);
 
@@ -1109,24 +1269,24 @@ void kgd_gfx_v9_program_trap_handler_set
 	 * Program TBA registers
 	 */
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_SHADER_TBA_LO,
-			lower_32_bits(tba_addr >> 8));
+				 lower_32_bits(tba_addr >> 8));
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_SHADER_TBA_HI,
-			upper_32_bits(tba_addr >> 8));
+				 upper_32_bits(tba_addr >> 8));
 
 	/*
 	 * Program TMA registers
 	 */
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_SHADER_TMA_LO,
-			lower_32_bits(tma_addr >> 8));
+				 lower_32_bits(tma_addr >> 8));
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_SHADER_TMA_HI,
-			upper_32_bits(tma_addr >> 8));
+				 upper_32_bits(tma_addr >> 8));
 
 	kgd_gfx_v9_unlock_srbm(adev, inst);
 }
 
 uint64_t kgd_gfx_v9_hqd_get_pq_addr(struct amdgpu_device *adev,
-				    uint32_t pipe_id, uint32_t queue_id,
-				    uint32_t inst)
+									uint32_t pipe_id, uint32_t queue_id,
+									uint32_t inst)
 {
 	uint32_t low, high;
 	uint64_t queue_addr = 0;
@@ -1149,35 +1309,16 @@ uint64_t kgd_gfx_v9_hqd_get_pq_addr(stru
 
 	queue_addr = (((queue_addr | high) << 32) | low) << 8;
 
-unlock_out:
+	unlock_out:
 	amdgpu_gfx_rlc_exit_safe_mode(adev, inst);
 	kgd_gfx_v9_release_queue(adev, inst);
 
 	return queue_addr;
 }
 
-/* assume queue acquired  */
-static int kgd_gfx_v9_hqd_dequeue_wait(struct amdgpu_device *adev, uint32_t inst,
-				       unsigned int utimeout)
-{
-	unsigned long end_jiffies = (utimeout * HZ / 1000) + jiffies;
-
-	while (true) {
-		uint32_t temp = RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE);
-
-		if (!(temp & CP_HQD_ACTIVE__ACTIVE_MASK))
-			return 0;
-
-		if (time_after(jiffies, end_jiffies))
-			return -ETIME;
-
-		usleep_range(500, 1000);
-	}
-}
-
 uint64_t kgd_gfx_v9_hqd_reset(struct amdgpu_device *adev,
-			      uint32_t pipe_id, uint32_t queue_id,
-			      uint32_t inst, unsigned int utimeout)
+							  uint32_t pipe_id, uint32_t queue_id,
+							  uint32_t inst, unsigned int utimeout)
 {
 	uint32_t low, high, pipe_reset_data = 0;
 	uint64_t queue_addr = 0;
@@ -1201,7 +1342,7 @@ uint64_t kgd_gfx_v9_hqd_reset(struct amd
 	queue_addr = (((queue_addr | high) << 32) | low) << 8;
 
 	pr_debug("Attempting queue reset on XCC %i pipe id %i queue id %i\n",
-		 inst, pipe_id, queue_id);
+			 inst, pipe_id, queue_id);
 
 	/* assume previous dequeue request issued will take affect after reset */
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSPI_COMPUTE_QUEUE_RESET, 0x1);
@@ -1220,9 +1361,9 @@ uint64_t kgd_gfx_v9_hqd_reset(struct amd
 	if (kgd_gfx_v9_hqd_dequeue_wait(adev, inst, utimeout))
 		queue_addr = 0;
 
-unlock_out:
+	unlock_out:
 	pr_debug("queue reset on XCC %i pipe id %i queue id %i %s\n",
-		 inst, pipe_id, queue_id, !!queue_addr ? "succeeded!" : "failed!");
+			 inst, pipe_id, queue_id, !!queue_addr ? "succeeded!" : "failed!");
 	amdgpu_gfx_rlc_exit_safe_mode(adev, inst);
 	kgd_gfx_v9_release_queue(adev, inst);
 
@@ -1244,7 +1385,7 @@ const struct kfd2kgd_calls gfx_v9_kfd2kg
 	.hqd_sdma_destroy = kgd_hqd_sdma_destroy,
 	.wave_control_execute = kgd_gfx_v9_wave_control_execute,
 	.get_atc_vmid_pasid_mapping_info =
-			kgd_gfx_v9_get_atc_vmid_pasid_mapping_info,
+	kgd_gfx_v9_get_atc_vmid_pasid_mapping_info,
 	.set_vm_context_page_table_base = kgd_gfx_v9_set_vm_context_page_table_base,
 	.enable_debug_trap = kgd_gfx_v9_enable_debug_trap,
 	.disable_debug_trap = kgd_gfx_v9_disable_debug_trap,
