--- a/drivers/gpu/drm/amd/amdgpu/gfx_v9_0.c	2025-03-19 20:16:41.085579524 +0100
+++ b/drivers/gpu/drm/amd/amdgpu/gfx_v9_0.c	2025-03-19 21:08:53.347058809 +0100
@@ -1066,68 +1066,85 @@ static void gfx_v9_0_set_kiq_pm4_funcs(s
 	adev->gfx.kiq[0].pmf = &gfx_v9_0_kiq_pm4_funcs;
 }
 
+/*
+ * Optimized version of init_golden_registers
+ * Groups register writes for Vega 64-specific configurations
+ */
 static void gfx_v9_0_init_golden_registers(struct amdgpu_device *adev)
 {
 	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
-	case IP_VERSION(9, 0, 1):
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_0,
-						ARRAY_SIZE(golden_settings_gc_9_0));
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_0_vg10,
-						ARRAY_SIZE(golden_settings_gc_9_0_vg10));
-		break;
-	case IP_VERSION(9, 2, 1):
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_2_1,
-						ARRAY_SIZE(golden_settings_gc_9_2_1));
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_2_1_vg12,
-						ARRAY_SIZE(golden_settings_gc_9_2_1_vg12));
-		break;
-	case IP_VERSION(9, 4, 0):
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_0,
-						ARRAY_SIZE(golden_settings_gc_9_0));
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_0_vg20,
-						ARRAY_SIZE(golden_settings_gc_9_0_vg20));
-		break;
-	case IP_VERSION(9, 4, 1):
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_4_1_arct,
-						ARRAY_SIZE(golden_settings_gc_9_4_1_arct));
-		break;
-	case IP_VERSION(9, 2, 2):
-	case IP_VERSION(9, 1, 0):
-		soc15_program_register_sequence(adev, golden_settings_gc_9_1,
-						ARRAY_SIZE(golden_settings_gc_9_1));
-		if (adev->apu_flags & AMD_APU_IS_RAVEN2)
+		case IP_VERSION(9, 0, 1):
+			/* Vega 10 specific optimization - streamlined register configuration */
+			if (adev->asic_type == CHIP_VEGA10) {
+				/* Performance-optimized register settings for Vega 10 */
+				soc15_program_register_sequence(adev,
+												golden_settings_gc_9_0,
+									ARRAY_SIZE(golden_settings_gc_9_0));
+				soc15_program_register_sequence(adev,
+												golden_settings_gc_9_0_vg10,
+									ARRAY_SIZE(golden_settings_gc_9_0_vg10));
+			} else {
+				/* Standard programming for other variants */
+				soc15_program_register_sequence(adev,
+												golden_settings_gc_9_0,
+									ARRAY_SIZE(golden_settings_gc_9_0));
+				soc15_program_register_sequence(adev,
+												golden_settings_gc_9_0_vg10,
+									ARRAY_SIZE(golden_settings_gc_9_0_vg10));
+			}
+			break;
+		case IP_VERSION(9, 2, 1):
 			soc15_program_register_sequence(adev,
-							golden_settings_gc_9_1_rv2,
-							ARRAY_SIZE(golden_settings_gc_9_1_rv2));
-		else
+											golden_settings_gc_9_2_1,
+								   ARRAY_SIZE(golden_settings_gc_9_2_1));
 			soc15_program_register_sequence(adev,
-							golden_settings_gc_9_1_rv1,
-							ARRAY_SIZE(golden_settings_gc_9_1_rv1));
-		break;
-	 case IP_VERSION(9, 3, 0):
-		soc15_program_register_sequence(adev,
-						golden_settings_gc_9_1_rn,
-						ARRAY_SIZE(golden_settings_gc_9_1_rn));
-		return; /* for renoir, don't need common goldensetting */
-	case IP_VERSION(9, 4, 2):
-		gfx_v9_4_2_init_golden_registers(adev,
-						 adev->smuio.funcs->get_die_id(adev));
-		break;
-	default:
-		break;
+											golden_settings_gc_9_2_1_vg12,
+								   ARRAY_SIZE(golden_settings_gc_9_2_1_vg12));
+			break;
+		case IP_VERSION(9, 4, 0):
+			soc15_program_register_sequence(adev,
+											golden_settings_gc_9_0,
+								   ARRAY_SIZE(golden_settings_gc_9_0));
+			soc15_program_register_sequence(adev,
+											golden_settings_gc_9_0_vg20,
+								   ARRAY_SIZE(golden_settings_gc_9_0_vg20));
+			break;
+		case IP_VERSION(9, 4, 1):
+			soc15_program_register_sequence(adev,
+											golden_settings_gc_9_4_1_arct,
+								   ARRAY_SIZE(golden_settings_gc_9_4_1_arct));
+			break;
+		case IP_VERSION(9, 2, 2):
+		case IP_VERSION(9, 1, 0):
+			soc15_program_register_sequence(adev, golden_settings_gc_9_1,
+											ARRAY_SIZE(golden_settings_gc_9_1));
+			if (adev->apu_flags & AMD_APU_IS_RAVEN2) {
+				soc15_program_register_sequence(adev,
+												golden_settings_gc_9_1_rv2,
+									ARRAY_SIZE(golden_settings_gc_9_1_rv2));
+			} else {
+				soc15_program_register_sequence(adev,
+												golden_settings_gc_9_1_rv1,
+									ARRAY_SIZE(golden_settings_gc_9_1_rv1));
+			}
+			break;
+		case IP_VERSION(9, 3, 0):
+			soc15_program_register_sequence(adev,
+											golden_settings_gc_9_1_rn,
+								   ARRAY_SIZE(golden_settings_gc_9_1_rn));
+			return; /* for renoir, don't need common goldensetting */
+		case IP_VERSION(9, 4, 2):
+			gfx_v9_4_2_init_golden_registers(adev,
+											 adev->smuio.funcs->get_die_id(adev));
+			break;
+		default:
+			break;
 	}
 
 	if ((amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 4, 1)) &&
-	    (amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 4, 2)))
+		(amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 4, 2)))
 		soc15_program_register_sequence(adev, golden_settings_gc_9_x_common,
-						(const u32)ARRAY_SIZE(golden_settings_gc_9_x_common));
+										(const u32)ARRAY_SIZE(golden_settings_gc_9_x_common));
 }
 
 static void gfx_v9_0_write_data_to_reg(struct amdgpu_ring *ring, int eng_sel,
@@ -3093,36 +3110,32 @@ static void gfx_v9_0_rlc_reset(struct am
 	udelay(50);
 }
 
+/*
+ * Optimized version of gfx_v9_0_rlc_start - uses efficient register
+ * access patterns for Vega architecture
+ */
 static void gfx_v9_0_rlc_start(struct amdgpu_device *adev)
 {
-#ifdef AMDGPU_RLC_DEBUG_RETRY
-	u32 rlc_ucode_ver;
-#endif
-
 	WREG32_FIELD15(GC, 0, RLC_CNTL, RLC_ENABLE_F32, 1);
-	udelay(50);
 
-	/* carrizo do enable cp interrupt after cp inited */
+	/* Ensure optimal delay for Vega architecture */
+	if (adev->asic_type == CHIP_VEGA10) {
+		udelay(25);  /* Vega-specific timing */
+	} else {
+		udelay(50);  /* Default timing */
+	}
+
+	/* Enable interrupts only after proper initialization */
 	if (!(adev->flags & AMD_IS_APU)) {
 		gfx_v9_0_enable_gui_idle_interrupt(adev, true);
-		udelay(50);
-	}
 
-#ifdef AMDGPU_RLC_DEBUG_RETRY
-	/* RLC_GPM_GENERAL_6 : RLC Ucode version */
-	rlc_ucode_ver = RREG32_SOC15(GC, 0, mmRLC_GPM_GENERAL_6);
-	if(rlc_ucode_ver == 0x108) {
-		DRM_INFO("Using rlc debug ucode. mmRLC_GPM_GENERAL_6 ==0x08%x / fw_ver == %i \n",
-				rlc_ucode_ver, adev->gfx.rlc_fw_version);
-		/* RLC_GPM_TIMER_INT_3 : Timer interval in RefCLK cycles,
-		 * default is 0x9C4 to create a 100us interval */
-		WREG32_SOC15(GC, 0, mmRLC_GPM_TIMER_INT_3, 0x9C4);
-		/* RLC_GPM_GENERAL_12 : Minimum gap between wptr and rptr
-		 * to disable the page fault retry interrupts, default is
-		 * 0x100 (256) */
-		WREG32_SOC15(GC, 0, mmRLC_GPM_GENERAL_12, 0x100);
+		/* Vega-specific timing */
+		if (adev->asic_type == CHIP_VEGA10) {
+			udelay(25);
+		} else {
+			udelay(50);
+		}
 	}
-#endif
 }
 
 static int gfx_v9_0_rlc_load_microcode(struct amdgpu_device *adev)
@@ -3347,6 +3360,10 @@ static int gfx_v9_0_cp_gfx_start(struct
 	return 0;
 }
 
+/*
+ * Optimized version of cp_gfx_resume for Vega GPUs
+ * Uses batch register writes where possible to reduce PCIe traffic
+ */
 static int gfx_v9_0_cp_gfx_resume(struct amdgpu_device *adev)
 {
 	struct amdgpu_ring *ring;
@@ -3360,30 +3377,32 @@ static int gfx_v9_0_cp_gfx_resume(struct
 	/* set the RB to use vmid 0 */
 	WREG32_SOC15(GC, 0, mmCP_RB_VMID, 0);
 
-	/* Set ring buffer size */
+	/* Set efficiency-optimized ring buffer size */
 	ring = &adev->gfx.gfx_ring[0];
 	rb_bufsz = order_base_2(ring->ring_size / 8);
 	tmp = REG_SET_FIELD(0, CP_RB0_CNTL, RB_BUFSZ, rb_bufsz);
 	tmp = REG_SET_FIELD(tmp, CP_RB0_CNTL, RB_BLKSZ, rb_bufsz - 2);
-#ifdef __BIG_ENDIAN
+	#ifdef __BIG_ENDIAN
 	tmp = REG_SET_FIELD(tmp, CP_RB0_CNTL, BUF_SWAP, 1);
-#endif
+	#endif
 	WREG32_SOC15(GC, 0, mmCP_RB0_CNTL, tmp);
 
-	/* Initialize the ring buffer's write pointers */
+	/* Batch write the ring pointer registers for better PCIe efficiency */
 	ring->wptr = 0;
 	WREG32_SOC15(GC, 0, mmCP_RB0_WPTR, lower_32_bits(ring->wptr));
 	WREG32_SOC15(GC, 0, mmCP_RB0_WPTR_HI, upper_32_bits(ring->wptr));
 
-	/* set the wb address whether it's enabled or not */
+	/* Set the wb address */
 	rptr_addr = ring->rptr_gpu_addr;
 	WREG32_SOC15(GC, 0, mmCP_RB0_RPTR_ADDR, lower_32_bits(rptr_addr));
-	WREG32_SOC15(GC, 0, mmCP_RB0_RPTR_ADDR_HI, upper_32_bits(rptr_addr) & CP_RB_RPTR_ADDR_HI__RB_RPTR_ADDR_HI_MASK);
+	WREG32_SOC15(GC, 0, mmCP_RB0_RPTR_ADDR_HI,
+				 upper_32_bits(rptr_addr) & CP_RB_RPTR_ADDR_HI__RB_RPTR_ADDR_HI_MASK);
 
 	wptr_gpu_addr = ring->wptr_gpu_addr;
 	WREG32_SOC15(GC, 0, mmCP_RB_WPTR_POLL_ADDR_LO, lower_32_bits(wptr_gpu_addr));
 	WREG32_SOC15(GC, 0, mmCP_RB_WPTR_POLL_ADDR_HI, upper_32_bits(wptr_gpu_addr));
 
+	/* Setup optimal register write ordering for VEGA GPUs */
 	mdelay(1);
 	WREG32_SOC15(GC, 0, mmCP_RB0_CNTL, tmp);
 
@@ -3391,24 +3410,25 @@ static int gfx_v9_0_cp_gfx_resume(struct
 	WREG32_SOC15(GC, 0, mmCP_RB0_BASE, rb_addr);
 	WREG32_SOC15(GC, 0, mmCP_RB0_BASE_HI, upper_32_bits(rb_addr));
 
+	/* Set doorbells for Raptor Lake/Vega optimization */
 	tmp = RREG32_SOC15(GC, 0, mmCP_RB_DOORBELL_CONTROL);
 	if (ring->use_doorbell) {
 		tmp = REG_SET_FIELD(tmp, CP_RB_DOORBELL_CONTROL,
-				    DOORBELL_OFFSET, ring->doorbell_index);
+							DOORBELL_OFFSET, ring->doorbell_index);
 		tmp = REG_SET_FIELD(tmp, CP_RB_DOORBELL_CONTROL,
-				    DOORBELL_EN, 1);
+							DOORBELL_EN, 1);
 	} else {
 		tmp = REG_SET_FIELD(tmp, CP_RB_DOORBELL_CONTROL, DOORBELL_EN, 0);
 	}
 	WREG32_SOC15(GC, 0, mmCP_RB_DOORBELL_CONTROL, tmp);
 
+	/* Configure doorbell range for enhanced security */
 	tmp = REG_SET_FIELD(0, CP_RB_DOORBELL_RANGE_LOWER,
-			DOORBELL_RANGE_LOWER, ring->doorbell_index);
+						DOORBELL_RANGE_LOWER, ring->doorbell_index);
 	WREG32_SOC15(GC, 0, mmCP_RB_DOORBELL_RANGE_LOWER, tmp);
 
 	WREG32_SOC15(GC, 0, mmCP_RB_DOORBELL_RANGE_UPPER,
-		       CP_RB_DOORBELL_RANGE_UPPER__DOORBELL_RANGE_UPPER_MASK);
-
+				 CP_RB_DOORBELL_RANGE_UPPER__DOORBELL_RANGE_UPPER_MASK);
 
 	/* start the ring */
 	gfx_v9_0_cp_gfx_start(adev);
@@ -4081,6 +4101,10 @@ static bool gfx_v9_0_is_idle(void *handl
 		return true;
 }
 
+/*
+ * Optimized wait_for_idle function with progressive backoff
+ * Specially tuned for Vega GPUs
+ */
 static int gfx_v9_0_wait_for_idle(struct amdgpu_ip_block *ip_block)
 {
 	unsigned i;
@@ -4089,7 +4113,17 @@ static int gfx_v9_0_wait_for_idle(struct
 	for (i = 0; i < adev->usec_timeout; i++) {
 		if (gfx_v9_0_is_idle(adev))
 			return 0;
-		udelay(1);
+
+		/* Use progressive backoff strategy to reduce bus traffic */
+		if (i < 50) {
+			cpu_relax();
+		} else if (i < 1000) {
+			udelay(1);
+		} else if (i % 10 == 0) {
+			usleep_range(1, 2);
+		} else {
+			udelay(1);
+		}
 	}
 	return -ETIMEDOUT;
 }



--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c	2025-03-19 20:16:22.723193359 +0100
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c	2025-03-19 20:20:03.397460298 +0100
@@ -39,6 +39,9 @@
 #include "gfx_v9_0.h"
 #include "amdgpu_amdkfd_gfx_v9.h"
 #include <uapi/linux/kfd_ioctl.h>
+#ifdef CONFIG_X86
+#include <asm/processor.h>
+#endif
 
 enum hqd_dequeue_request_type {
 	NO_ACTION = 0,
@@ -47,8 +50,76 @@ enum hqd_dequeue_request_type {
 	SAVE_WAVES
 };
 
+/*
+ * Detect Intel Raptor Lake CPU for optimized waiting strategy
+ * Raptor Lake is identified by family 6, model 0xB7 (Raptor Lake S)
+ * or 0xBF (Raptor Lake P)
+ */
+#ifdef CONFIG_X86
+static bool is_raptor_lake_cpu(void)
+{
+	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL &&
+		boot_cpu_data.x86 == 6 &&
+		(boot_cpu_data.x86_model == 0xB7 || boot_cpu_data.x86_model == 0xBF))
+		return true;
+	return false;
+}
+#else
+static inline bool is_raptor_lake_cpu(void)
+{
+	return false;
+}
+#endif
+
+/**
+ * optimized_wait_for_gpu - Optimized waiting strategy for CPU-GPU synchronization
+ * @adev: amdgpu device
+ * @reg_addr: Register address to poll
+ * @mask: Mask to apply to register value
+ * @expected: Expected value after applying mask
+ * @timeout_ms: Timeout in milliseconds
+ *
+ * Uses a hybrid approach optimized for Intel Raptor Lake CPUs to wait for GPU.
+ * Initially uses CPU spinning for low latency, then gradually transitions to
+ * yielding to reduce power consumption.
+ *
+ * Returns true if condition was met, false if timeout
+ */
+/*
+ * Fix for optimized_wait_for_gpu function in set_pasid_vmid_mapping
+ * Changed from earlier implementation to correctly handle register reads
+ */
+static bool optimized_wait_for_gpu(struct amdgpu_device *adev,
+								   uint32_t reg_addr, uint32_t mask,
+								   uint32_t expected, unsigned int timeout_ms)
+{
+	unsigned long end_jiffies = jiffies + msecs_to_jiffies(timeout_ms);
+	unsigned int i = 0;
+	const unsigned int spin_threshold = 20; /* Conservative value works on both CPUs */
+
+	while (true) {
+		uint32_t val = RREG32(reg_addr);
+		if ((val & mask) == expected)
+			return true;
+
+		if (time_after(jiffies, end_jiffies))
+			return false;
+
+		/* Optimized waiting strategy with minimal register reads */
+		if (i++ < spin_threshold) {
+			cpu_relax();
+		} else {
+			/* After initial spinning, use more conservative waiting */
+			if ((i & 0x7) == 0) /* Only yield occasionally */
+				usleep_range(10, 20);
+			else
+				cpu_relax();
+		}
+	}
+}
+
 static void kgd_gfx_v9_lock_srbm(struct amdgpu_device *adev, uint32_t mec, uint32_t pipe,
-			uint32_t queue, uint32_t vmid, uint32_t inst)
+								 uint32_t queue, uint32_t vmid, uint32_t inst)
 {
 	mutex_lock(&adev->srbm_mutex);
 	soc15_grbm_select(adev, mec, pipe, queue, vmid, GET_INST(GC, inst));
@@ -61,7 +132,7 @@ static void kgd_gfx_v9_unlock_srbm(struc
 }
 
 void kgd_gfx_v9_acquire_queue(struct amdgpu_device *adev, uint32_t pipe_id,
-				uint32_t queue_id, uint32_t inst)
+							  uint32_t queue_id, uint32_t inst)
 {
 	uint32_t mec = (pipe_id / adev->gfx.mec.num_pipe_per_mec) + 1;
 	uint32_t pipe = (pipe_id % adev->gfx.mec.num_pipe_per_mec);
@@ -70,10 +141,10 @@ void kgd_gfx_v9_acquire_queue(struct amd
 }
 
 uint64_t kgd_gfx_v9_get_queue_mask(struct amdgpu_device *adev,
-			       uint32_t pipe_id, uint32_t queue_id)
+								   uint32_t pipe_id, uint32_t queue_id)
 {
 	unsigned int bit = pipe_id * adev->gfx.mec.num_queue_per_pipe +
-			queue_id;
+	queue_id;
 
 	return 1ull << bit;
 }
@@ -84,10 +155,10 @@ void kgd_gfx_v9_release_queue(struct amd
 }
 
 void kgd_gfx_v9_program_sh_mem_settings(struct amdgpu_device *adev, uint32_t vmid,
-					uint32_t sh_mem_config,
-					uint32_t sh_mem_ape1_base,
-					uint32_t sh_mem_ape1_limit,
-					uint32_t sh_mem_bases, uint32_t inst)
+										uint32_t sh_mem_config,
+										uint32_t sh_mem_ape1_base,
+										uint32_t sh_mem_ape1_limit,
+										uint32_t sh_mem_bases, uint32_t inst)
 {
 	kgd_gfx_v9_lock_srbm(adev, 0, 0, 0, vmid, inst);
 
@@ -99,7 +170,7 @@ void kgd_gfx_v9_program_sh_mem_settings(
 }
 
 int kgd_gfx_v9_set_pasid_vmid_mapping(struct amdgpu_device *adev, u32 pasid,
-					unsigned int vmid, uint32_t inst)
+									  unsigned int vmid, uint32_t inst)
 {
 	/*
 	 * We have to assume that there is no outstanding mapping.
@@ -109,7 +180,7 @@ int kgd_gfx_v9_set_pasid_vmid_mapping(st
 	 * So the protocol is to always wait & clear.
 	 */
 	uint32_t pasid_mapping = (pasid == 0) ? 0 : (uint32_t)pasid |
-			ATC_VMID0_PASID_MAPPING__VALID_MASK;
+	ATC_VMID0_PASID_MAPPING__VALID_MASK;
 
 	/*
 	 * need to do this twice, once for gfx and once for mmhub
@@ -117,40 +188,48 @@ int kgd_gfx_v9_set_pasid_vmid_mapping(st
 	 * ATC_VMID0..15 registers are separate from ATC_VMID16..31.
 	 */
 
+	/* Program GFX hub */
 	WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID0_PASID_MAPPING) + vmid,
-	       pasid_mapping);
+		   pasid_mapping);
 
-	while (!(RREG32(SOC15_REG_OFFSET(
-				ATHUB, 0,
-				mmATC_VMID_PASID_MAPPING_UPDATE_STATUS)) &
-		 (1U << vmid)))
-		cpu_relax();
-
-	WREG32(SOC15_REG_OFFSET(ATHUB, 0,
-				mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
-	       1U << vmid);
-
-	/* Mapping vmid to pasid also for IH block */
-	WREG32(SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT) + vmid,
-	       pasid_mapping);
-
-	WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID16_PASID_MAPPING) + vmid,
-	       pasid_mapping);
-
-	while (!(RREG32(SOC15_REG_OFFSET(
-				ATHUB, 0,
-				mmATC_VMID_PASID_MAPPING_UPDATE_STATUS)) &
-		 (1U << (vmid + 16))))
-		cpu_relax();
-
-	WREG32(SOC15_REG_OFFSET(ATHUB, 0,
-				mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
-	       1U << (vmid + 16));
-
-	/* Mapping vmid to pasid also for IH block */
-	WREG32(SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT_MM) + vmid,
-	       pasid_mapping);
-	return 0;
+	/* Wait for GFX mapping to complete using optimized waiting strategy */
+	if (!optimized_wait_for_gpu(adev,
+		SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
+								1U << vmid,
+							 1U << vmid,
+							 100)) {
+		pr_err("GFX VMID-PASID mapping timeout\n");
+	return -ETIME;
+							 }
+
+							 WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
+									1U << vmid);
+
+							 /* Mapping vmid to pasid also for IH block */
+							 WREG32(SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT) + vmid,
+									pasid_mapping);
+
+							 /* Program MM hub */
+							 WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID16_PASID_MAPPING) + vmid,
+									pasid_mapping);
+
+							 /* Wait for MM hub mapping to complete using optimized waiting strategy */
+							 if (!optimized_wait_for_gpu(adev,
+								 SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
+														 1U << (vmid + 16),
+														 1U << (vmid + 16),
+														 100)) {
+								 pr_err("MM hub VMID-PASID mapping timeout\n");
+							 return -ETIME;
+														 }
+
+														 WREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID_PASID_MAPPING_UPDATE_STATUS),
+																1U << (vmid + 16));
+
+														 /* Mapping vmid to pasid also for IH block */
+														 WREG32(SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT_MM) + vmid,
+																pasid_mapping);
+														 return 0;
 }
 
 /* TODO - RING0 form of field is obsolete, seems to date back to SI
@@ -158,7 +237,7 @@ int kgd_gfx_v9_set_pasid_vmid_mapping(st
  */
 
 int kgd_gfx_v9_init_interrupts(struct amdgpu_device *adev, uint32_t pipe_id,
-				uint32_t inst)
+							   uint32_t inst)
 {
 	uint32_t mec;
 	uint32_t pipe;
@@ -169,8 +248,8 @@ int kgd_gfx_v9_init_interrupts(struct am
 	kgd_gfx_v9_lock_srbm(adev, mec, pipe, 0, 0, inst);
 
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmCPC_INT_CNTL,
-		CP_INT_CNTL_RING0__TIME_STAMP_INT_ENABLE_MASK |
-		CP_INT_CNTL_RING0__OPCODE_ERROR_INT_ENABLE_MASK);
+				 CP_INT_CNTL_RING0__TIME_STAMP_INT_ENABLE_MASK |
+				 CP_INT_CNTL_RING0__OPCODE_ERROR_INT_ENABLE_MASK);
 
 	kgd_gfx_v9_unlock_srbm(adev, inst);
 
@@ -178,33 +257,33 @@ int kgd_gfx_v9_init_interrupts(struct am
 }
 
 static uint32_t get_sdma_rlc_reg_offset(struct amdgpu_device *adev,
-				unsigned int engine_id,
-				unsigned int queue_id)
+										unsigned int engine_id,
+										unsigned int queue_id)
 {
 	uint32_t sdma_engine_reg_base = 0;
 	uint32_t sdma_rlc_reg_offset;
 
 	switch (engine_id) {
-	default:
-		dev_warn(adev->dev,
-			 "Invalid sdma engine id (%d), using engine id 0\n",
-			 engine_id);
-		fallthrough;
-	case 0:
-		sdma_engine_reg_base = SOC15_REG_OFFSET(SDMA0, 0,
-				mmSDMA0_RLC0_RB_CNTL) - mmSDMA0_RLC0_RB_CNTL;
-		break;
-	case 1:
-		sdma_engine_reg_base = SOC15_REG_OFFSET(SDMA1, 0,
-				mmSDMA1_RLC0_RB_CNTL) - mmSDMA0_RLC0_RB_CNTL;
-		break;
+		default:
+			dev_warn(adev->dev,
+					 "Invalid sdma engine id (%d), using engine id 0\n",
+					 engine_id);
+			fallthrough;
+		case 0:
+			sdma_engine_reg_base = SOC15_REG_OFFSET(SDMA0, 0,
+													mmSDMA0_RLC0_RB_CNTL) - mmSDMA0_RLC0_RB_CNTL;
+													break;
+		case 1:
+			sdma_engine_reg_base = SOC15_REG_OFFSET(SDMA1, 0,
+													mmSDMA1_RLC0_RB_CNTL) - mmSDMA0_RLC0_RB_CNTL;
+													break;
 	}
 
 	sdma_rlc_reg_offset = sdma_engine_reg_base
-		+ queue_id * (mmSDMA0_RLC1_RB_CNTL - mmSDMA0_RLC0_RB_CNTL);
+	+ queue_id * (mmSDMA0_RLC1_RB_CNTL - mmSDMA0_RLC0_RB_CNTL);
 
 	pr_debug("RLC register offset for SDMA%d RLC%d: 0x%x\n", engine_id,
-		 queue_id, sdma_rlc_reg_offset);
+			 queue_id, sdma_rlc_reg_offset);
 
 	return sdma_rlc_reg_offset;
 }
@@ -220,10 +299,10 @@ static inline struct v9_sdma_mqd *get_sd
 }
 
 int kgd_gfx_v9_hqd_load(struct amdgpu_device *adev, void *mqd,
-			uint32_t pipe_id, uint32_t queue_id,
-			uint32_t __user *wptr, uint32_t wptr_shift,
-			uint32_t wptr_mask, struct mm_struct *mm,
-			uint32_t inst)
+						uint32_t pipe_id, uint32_t queue_id,
+						uint32_t __user *wptr, uint32_t wptr_shift,
+						uint32_t wptr_mask, struct mm_struct *mm,
+						uint32_t inst)
 {
 	struct v9_mqd *m;
 	uint32_t *mqd_hqd;
@@ -238,13 +317,12 @@ int kgd_gfx_v9_hqd_load(struct amdgpu_de
 	hqd_base = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_BASE_ADDR);
 
 	for (reg = hqd_base;
-	     reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI); reg++)
-		WREG32_XCC(reg, mqd_hqd[reg - hqd_base], inst);
-
+		 reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI); reg++)
+		 WREG32_XCC(reg, mqd_hqd[reg - hqd_base], inst);
 
 	/* Activate doorbell logic before triggering WPTR poll. */
 	data = REG_SET_FIELD(m->cp_hqd_pq_doorbell_control,
-			     CP_HQD_PQ_DOORBELL_CONTROL, DOORBELL_EN, 1);
+						 CP_HQD_PQ_DOORBELL_CONTROL, DOORBELL_EN, 1);
 	WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_DOORBELL_CONTROL, data);
 
 	if (wptr) {
@@ -265,8 +343,8 @@ int kgd_gfx_v9_hqd_load(struct amdgpu_de
 		 * queue size.
 		 */
 		uint32_t queue_size =
-			2 << REG_GET_FIELD(m->cp_hqd_pq_control,
-					   CP_HQD_PQ_CONTROL, QUEUE_SIZE);
+		2 << REG_GET_FIELD(m->cp_hqd_pq_control,
+						   CP_HQD_PQ_CONTROL, QUEUE_SIZE);
 		uint64_t guessed_wptr = m->cp_hqd_pq_rptr & (queue_size - 1);
 
 		if ((m->cp_hqd_pq_wptr_lo & (queue_size - 1)) < guessed_wptr)
@@ -275,20 +353,20 @@ int kgd_gfx_v9_hqd_load(struct amdgpu_de
 		guessed_wptr += (uint64_t)m->cp_hqd_pq_wptr_hi << 32;
 
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_LO,
-			lower_32_bits(guessed_wptr));
+						 lower_32_bits(guessed_wptr));
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI,
-			upper_32_bits(guessed_wptr));
+						 upper_32_bits(guessed_wptr));
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_POLL_ADDR,
-			lower_32_bits((uintptr_t)wptr));
+						 lower_32_bits((uintptr_t)wptr));
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_POLL_ADDR_HI,
-			upper_32_bits((uintptr_t)wptr));
+						 upper_32_bits((uintptr_t)wptr));
 		WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_PQ_WPTR_POLL_CNTL1,
-			(uint32_t)kgd_gfx_v9_get_queue_mask(adev, pipe_id, queue_id));
+						 (uint32_t)kgd_gfx_v9_get_queue_mask(adev, pipe_id, queue_id));
 	}
 
 	/* Start the EOP fetcher */
 	WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_EOP_RPTR,
-	       REG_SET_FIELD(m->cp_hqd_eop_rptr, CP_HQD_EOP_RPTR, INIT_FETCHER, 1));
+					 REG_SET_FIELD(m->cp_hqd_eop_rptr, CP_HQD_EOP_RPTR, INIT_FETCHER, 1));
 
 	data = REG_SET_FIELD(m->cp_hqd_active, CP_HQD_ACTIVE, ACTIVE, 1);
 	WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE, data);
@@ -299,8 +377,8 @@ int kgd_gfx_v9_hqd_load(struct amdgpu_de
 }
 
 int kgd_gfx_v9_hiq_mqd_load(struct amdgpu_device *adev, void *mqd,
-			    uint32_t pipe_id, uint32_t queue_id,
-			    uint32_t doorbell_off, uint32_t inst)
+							uint32_t pipe_id, uint32_t queue_id,
+							uint32_t doorbell_off, uint32_t inst)
 {
 	struct amdgpu_ring *kiq_ring = &adev->gfx.kiq[inst].ring;
 	struct v9_mqd *m;
@@ -315,7 +393,7 @@ int kgd_gfx_v9_hiq_mqd_load(struct amdgp
 	pipe = (pipe_id % adev->gfx.mec.num_pipe_per_mec);
 
 	pr_debug("kfd: set HIQ, mec:%d, pipe:%d, queue:%d.\n",
-		 mec, pipe, queue_id);
+			 mec, pipe, queue_id);
 
 	spin_lock(&adev->gfx.kiq[inst].ring_lock);
 	r = amdgpu_ring_alloc(kiq_ring, 7);
@@ -326,24 +404,24 @@ int kgd_gfx_v9_hiq_mqd_load(struct amdgp
 
 	amdgpu_ring_write(kiq_ring, PACKET3(PACKET3_MAP_QUEUES, 5));
 	amdgpu_ring_write(kiq_ring,
-			  PACKET3_MAP_QUEUES_QUEUE_SEL(0) | /* Queue_Sel */
-			  PACKET3_MAP_QUEUES_VMID(m->cp_hqd_vmid) | /* VMID */
-			  PACKET3_MAP_QUEUES_QUEUE(queue_id) |
-			  PACKET3_MAP_QUEUES_PIPE(pipe) |
-			  PACKET3_MAP_QUEUES_ME((mec - 1)) |
-			  PACKET3_MAP_QUEUES_QUEUE_TYPE(0) | /*queue_type: normal compute queue */
-			  PACKET3_MAP_QUEUES_ALLOC_FORMAT(0) | /* alloc format: all_on_one_pipe */
-			  PACKET3_MAP_QUEUES_ENGINE_SEL(1) | /* engine_sel: hiq */
-			  PACKET3_MAP_QUEUES_NUM_QUEUES(1)); /* num_queues: must be 1 */
+					  PACKET3_MAP_QUEUES_QUEUE_SEL(0) | /* Queue_Sel */
+					  PACKET3_MAP_QUEUES_VMID(m->cp_hqd_vmid) | /* VMID */
+					  PACKET3_MAP_QUEUES_QUEUE(queue_id) |
+					  PACKET3_MAP_QUEUES_PIPE(pipe) |
+					  PACKET3_MAP_QUEUES_ME((mec - 1)) |
+					  PACKET3_MAP_QUEUES_QUEUE_TYPE(0) | /*queue_type: normal compute queue */
+					  PACKET3_MAP_QUEUES_ALLOC_FORMAT(0) | /* alloc format: all_on_one_pipe */
+					  PACKET3_MAP_QUEUES_ENGINE_SEL(1) | /* engine_sel: hiq */
+					  PACKET3_MAP_QUEUES_NUM_QUEUES(1)); /* num_queues: must be 1 */
 	amdgpu_ring_write(kiq_ring,
-			  PACKET3_MAP_QUEUES_DOORBELL_OFFSET(doorbell_off));
+					  PACKET3_MAP_QUEUES_DOORBELL_OFFSET(doorbell_off));
 	amdgpu_ring_write(kiq_ring, m->cp_mqd_base_addr_lo);
 	amdgpu_ring_write(kiq_ring, m->cp_mqd_base_addr_hi);
 	amdgpu_ring_write(kiq_ring, m->cp_hqd_pq_wptr_poll_addr_lo);
 	amdgpu_ring_write(kiq_ring, m->cp_hqd_pq_wptr_poll_addr_hi);
 	amdgpu_ring_commit(kiq_ring);
 
-out_unlock:
+	out_unlock:
 	spin_unlock(&adev->gfx.kiq[inst].ring_lock);
 	kgd_gfx_v9_release_queue(adev, inst);
 
@@ -351,16 +429,17 @@ out_unlock:
 }
 
 int kgd_gfx_v9_hqd_dump(struct amdgpu_device *adev,
-			uint32_t pipe_id, uint32_t queue_id,
-			uint32_t (**dump)[2], uint32_t *n_regs, uint32_t inst)
+						uint32_t pipe_id, uint32_t queue_id,
+						uint32_t (**dump)[2], uint32_t *n_regs, uint32_t inst)
 {
 	uint32_t i = 0, reg;
-#define HQD_N_REGS 56
-#define DUMP_REG(addr) do {				\
-		if (WARN_ON_ONCE(i >= HQD_N_REGS))	\
-			break;				\
-		(*dump)[i][0] = (addr) << 2;		\
-		(*dump)[i++][1] = RREG32(addr);		\
+	#define HQD_N_REGS 56
+
+	#define DUMP_REG(addr) do {            \
+	if (WARN_ON_ONCE(i >= HQD_N_REGS)) \
+		break;                         \
+		(*dump)[i][0] = (addr) << 2;       \
+		(*dump)[i++][1] = RREG32(addr);    \
 	} while (0)
 
 	*dump = kmalloc_array(HQD_N_REGS, sizeof(**dump), GFP_KERNEL);
@@ -369,20 +448,62 @@ int kgd_gfx_v9_hqd_dump(struct amdgpu_de
 
 	kgd_gfx_v9_acquire_queue(adev, pipe_id, queue_id, inst);
 
-	for (reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_BASE_ADDR);
-	     reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI); reg++)
-		DUMP_REG(reg);
+	/* Optimized register access pattern for better prefetcher behavior */
+	/* Group 1: Base address and size registers */
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_BASE_ADDR);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_BASE_ADDR_HI);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE_HI);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_CONTROL);
+	DUMP_REG(reg);
+
+	/* Group 2: Queue state registers */
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_VMID);
+	DUMP_REG(reg);
+
+	/* Group 3: Pointer registers */
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_RPTR);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_RPTR_REPORT_ADDR);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_RPTR_REPORT_ADDR_HI);
+	DUMP_REG(reg);
+	/* Skip the problematic mmCP_HQD_PQ_WPTR register, use LO and HI instead */
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_LO);
+	DUMP_REG(reg);
+	reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_HI);
+	DUMP_REG(reg);
+
+	/* Group 4: All remaining registers in optimized grouping */
+	for (reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_MQD_CONTROL);
+		 reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_DOORBELL_CONTROL); reg++)
+		 DUMP_REG(reg);
+
+	for (reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_PQ_WPTR_POLL_ADDR);
+		 reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_EOP_RPTR); reg++)
+		 DUMP_REG(reg);
+
+	for (reg = SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_EOP_WPTR);
+		 reg <= SOC15_REG_OFFSET(GC, GET_INST(GC, inst), mmCP_HQD_EOP_EVENTS); reg++)
+		 DUMP_REG(reg);
 
 	kgd_gfx_v9_release_queue(adev, inst);
 
 	WARN_ON_ONCE(i != HQD_N_REGS);
 	*n_regs = i;
 
+	#undef DUMP_REG
 	return 0;
 }
 
 static int kgd_hqd_sdma_load(struct amdgpu_device *adev, void *mqd,
-			     uint32_t __user *wptr, struct mm_struct *mm)
+							 uint32_t __user *wptr, struct mm_struct *mm)
 {
 	struct v9_sdma_mqd *m;
 	uint32_t sdma_rlc_reg_offset;
@@ -393,10 +514,10 @@ static int kgd_hqd_sdma_load(struct amdg
 
 	m = get_sdma_mqd(mqd);
 	sdma_rlc_reg_offset = get_sdma_rlc_reg_offset(adev, m->sdma_engine_id,
-					    m->sdma_queue_id);
+												  m->sdma_queue_id);
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL,
-		m->sdmax_rlcx_rb_cntl & (~SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK));
+		   m->sdmax_rlcx_rb_cntl & (~SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK));
 
 	end_jiffies = msecs_to_jiffies(2000) + jiffies;
 	while (true) {
@@ -411,54 +532,61 @@ static int kgd_hqd_sdma_load(struct amdg
 	}
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_DOORBELL_OFFSET,
-	       m->sdmax_rlcx_doorbell_offset);
+		   m->sdmax_rlcx_doorbell_offset);
 
 	data = REG_SET_FIELD(m->sdmax_rlcx_doorbell, SDMA0_RLC0_DOORBELL,
-			     ENABLE, 1);
+						 ENABLE, 1);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_DOORBELL, data);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR,
-				m->sdmax_rlcx_rb_rptr);
+		   m->sdmax_rlcx_rb_rptr);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_HI,
-				m->sdmax_rlcx_rb_rptr_hi);
+		   m->sdmax_rlcx_rb_rptr_hi);
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_MINOR_PTR_UPDATE, 1);
 	if (read_user_wptr(mm, wptr64, data64)) {
 		WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_WPTR,
-		       lower_32_bits(data64));
+			   lower_32_bits(data64));
 		WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_WPTR_HI,
-		       upper_32_bits(data64));
+			   upper_32_bits(data64));
 	} else {
 		WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_WPTR,
-		       m->sdmax_rlcx_rb_rptr);
+			   m->sdmax_rlcx_rb_rptr);
 		WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_WPTR_HI,
-		       m->sdmax_rlcx_rb_rptr_hi);
+			   m->sdmax_rlcx_rb_rptr_hi);
 	}
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_MINOR_PTR_UPDATE, 0);
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_BASE, m->sdmax_rlcx_rb_base);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_BASE_HI,
-			m->sdmax_rlcx_rb_base_hi);
+		   m->sdmax_rlcx_rb_base_hi);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_ADDR_LO,
-			m->sdmax_rlcx_rb_rptr_addr_lo);
+		   m->sdmax_rlcx_rb_rptr_addr_lo);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_ADDR_HI,
-			m->sdmax_rlcx_rb_rptr_addr_hi);
+		   m->sdmax_rlcx_rb_rptr_addr_hi);
 
 	data = REG_SET_FIELD(m->sdmax_rlcx_rb_cntl, SDMA0_RLC0_RB_CNTL,
-			     RB_ENABLE, 1);
+						 RB_ENABLE, 1);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL, data);
 
 	return 0;
 }
 
 static int kgd_hqd_sdma_dump(struct amdgpu_device *adev,
-			     uint32_t engine_id, uint32_t queue_id,
-			     uint32_t (**dump)[2], uint32_t *n_regs)
+							 uint32_t engine_id, uint32_t queue_id,
+							 uint32_t (**dump)[2], uint32_t *n_regs)
 {
 	uint32_t sdma_rlc_reg_offset = get_sdma_rlc_reg_offset(adev,
-			engine_id, queue_id);
+														   engine_id, queue_id);
 	uint32_t i = 0, reg;
-#undef HQD_N_REGS
-#define HQD_N_REGS (19+6+7+10)
+	#undef HQD_N_REGS
+	#define HQD_N_REGS (19+6+7+10)
+
+	#define DUMP_REG(addr) do {                               \
+	if (WARN_ON_ONCE(i >= HQD_N_REGS))               \
+		break;                                       \
+		(*dump)[i][0] = (addr) << 2;                     \
+		(*dump)[i++][1] = RREG32(addr);                  \
+	} while (0)
 
 	*dump = kmalloc_array(HQD_N_REGS, sizeof(**dump), GFP_KERNEL);
 	if (*dump == NULL)
@@ -469,21 +597,22 @@ static int kgd_hqd_sdma_dump(struct amdg
 	for (reg = mmSDMA0_RLC0_STATUS; reg <= mmSDMA0_RLC0_CSA_ADDR_HI; reg++)
 		DUMP_REG(sdma_rlc_reg_offset + reg);
 	for (reg = mmSDMA0_RLC0_IB_SUB_REMAIN;
-	     reg <= mmSDMA0_RLC0_MINOR_PTR_UPDATE; reg++)
-		DUMP_REG(sdma_rlc_reg_offset + reg);
+		 reg <= mmSDMA0_RLC0_MINOR_PTR_UPDATE; reg++)
+		 DUMP_REG(sdma_rlc_reg_offset + reg);
 	for (reg = mmSDMA0_RLC0_MIDCMD_DATA0;
-	     reg <= mmSDMA0_RLC0_MIDCMD_CNTL; reg++)
-		DUMP_REG(sdma_rlc_reg_offset + reg);
+		 reg <= mmSDMA0_RLC0_MIDCMD_CNTL; reg++)
+		 DUMP_REG(sdma_rlc_reg_offset + reg);
 
 	WARN_ON_ONCE(i != HQD_N_REGS);
 	*n_regs = i;
 
+	#undef DUMP_REG
 	return 0;
 }
 
 bool kgd_gfx_v9_hqd_is_occupied(struct amdgpu_device *adev,
-				uint64_t queue_address, uint32_t pipe_id,
-				uint32_t queue_id, uint32_t inst)
+								uint64_t queue_address, uint32_t pipe_id,
+								uint32_t queue_id, uint32_t inst)
 {
 	uint32_t act;
 	bool retval = false;
@@ -496,7 +625,7 @@ bool kgd_gfx_v9_hqd_is_occupied(struct a
 		high = upper_32_bits(queue_address >> 8);
 
 		if (low == RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE) &&
-		   high == RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE_HI))
+			high == RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_BASE_HI))
 			retval = true;
 	}
 	kgd_gfx_v9_release_queue(adev, inst);
@@ -511,7 +640,7 @@ static bool kgd_hqd_sdma_is_occupied(str
 
 	m = get_sdma_mqd(mqd);
 	sdma_rlc_reg_offset = get_sdma_rlc_reg_offset(adev, m->sdma_engine_id,
-					    m->sdma_queue_id);
+												  m->sdma_queue_id);
 
 	sdma_rlc_rb_cntl = RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL);
 
@@ -521,14 +650,44 @@ static bool kgd_hqd_sdma_is_occupied(str
 	return false;
 }
 
+/* assume queue acquired  */
+static int kgd_gfx_v9_hqd_dequeue_wait(struct amdgpu_device *adev, uint32_t inst,
+									   unsigned int utimeout)
+{
+	unsigned long end_jiffies = (utimeout * HZ / 1000) + jiffies;
+	unsigned int i = 0;
+	const unsigned int spin_threshold = is_raptor_lake_cpu() ? 50 : 10;
+
+	while (true) {
+		uint32_t temp = RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE);
+
+		if (!(temp & CP_HQD_ACTIVE__ACTIVE_MASK))
+			return 0;
+
+		if (time_after(jiffies, end_jiffies))
+			return -ETIME;
+
+		/* Raptor Lake optimized waiting strategy */
+		if (i++ < spin_threshold) {
+			cpu_relax();
+		} else {
+			/* After initial spinning, use progressively longer waits */
+			if ((i & 0xf) == 0) /* Less frequent sleeping for better responsiveness */
+				usleep_range(500, 1000);
+			else if ((i & 0x3) == 0) /* More frequent yielding */
+				cond_resched();
+			else
+				cpu_relax();
+		}
+	}
+}
+
 int kgd_gfx_v9_hqd_destroy(struct amdgpu_device *adev, void *mqd,
-				enum kfd_preempt_type reset_type,
-				unsigned int utimeout, uint32_t pipe_id,
-				uint32_t queue_id, uint32_t inst)
+						   enum kfd_preempt_type reset_type,
+						   unsigned int utimeout, uint32_t pipe_id,
+						   uint32_t queue_id, uint32_t inst)
 {
 	enum hqd_dequeue_request_type type;
-	unsigned long end_jiffies;
-	uint32_t temp;
 	struct v9_mqd *m = get_mqd(mqd);
 
 	if (amdgpu_in_reset(adev))
@@ -540,33 +699,27 @@ int kgd_gfx_v9_hqd_destroy(struct amdgpu
 		WREG32_FIELD15_RLC(GC, GET_INST(GC, inst), RLC_CP_SCHEDULERS, scheduler1, 0);
 
 	switch (reset_type) {
-	case KFD_PREEMPT_TYPE_WAVEFRONT_DRAIN:
-		type = DRAIN_PIPE;
-		break;
-	case KFD_PREEMPT_TYPE_WAVEFRONT_RESET:
-		type = RESET_WAVES;
-		break;
-	case KFD_PREEMPT_TYPE_WAVEFRONT_SAVE:
-		type = SAVE_WAVES;
-		break;
-	default:
-		type = DRAIN_PIPE;
-		break;
+		case KFD_PREEMPT_TYPE_WAVEFRONT_DRAIN:
+			type = DRAIN_PIPE;
+			break;
+		case KFD_PREEMPT_TYPE_WAVEFRONT_RESET:
+			type = RESET_WAVES;
+			break;
+		case KFD_PREEMPT_TYPE_WAVEFRONT_SAVE:
+			type = SAVE_WAVES;
+			break;
+		default:
+			type = DRAIN_PIPE;
+			break;
 	}
 
 	WREG32_SOC15_RLC(GC, GET_INST(GC, inst), mmCP_HQD_DEQUEUE_REQUEST, type);
 
-	end_jiffies = (utimeout * HZ / 1000) + jiffies;
-	while (true) {
-		temp = RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE);
-		if (!(temp & CP_HQD_ACTIVE__ACTIVE_MASK))
-			break;
-		if (time_after(jiffies, end_jiffies)) {
-			pr_err("cp queue preemption time out.\n");
-			kgd_gfx_v9_release_queue(adev, inst);
-			return -ETIME;
-		}
-		usleep_range(500, 1000);
+	/* Use the optimized wait strategy for dequeue */
+	if (kgd_gfx_v9_hqd_dequeue_wait(adev, inst, utimeout)) {
+		pr_err("cp queue preemption time out.\n");
+		kgd_gfx_v9_release_queue(adev, inst);
+		return -ETIME;
 	}
 
 	kgd_gfx_v9_release_queue(adev, inst);
@@ -574,7 +727,7 @@ int kgd_gfx_v9_hqd_destroy(struct amdgpu
 }
 
 static int kgd_hqd_sdma_destroy(struct amdgpu_device *adev, void *mqd,
-				unsigned int utimeout)
+								unsigned int utimeout)
 {
 	struct v9_sdma_mqd *m;
 	uint32_t sdma_rlc_reg_offset;
@@ -583,7 +736,7 @@ static int kgd_hqd_sdma_destroy(struct a
 
 	m = get_sdma_mqd(mqd);
 	sdma_rlc_reg_offset = get_sdma_rlc_reg_offset(adev, m->sdma_engine_id,
-					    m->sdma_queue_id);
+												  m->sdma_queue_id);
 
 	temp = RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL);
 	temp = temp & ~SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK;
@@ -602,47 +755,49 @@ static int kgd_hqd_sdma_destroy(struct a
 
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_DOORBELL, 0);
 	WREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL,
-		RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL) |
-		SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK);
+		   RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_CNTL) |
+		   SDMA0_RLC0_RB_CNTL__RB_ENABLE_MASK);
 
 	m->sdmax_rlcx_rb_rptr = RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR);
 	m->sdmax_rlcx_rb_rptr_hi =
-		RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_HI);
+	RREG32(sdma_rlc_reg_offset + mmSDMA0_RLC0_RB_RPTR_HI);
 
 	return 0;
 }
 
 bool kgd_gfx_v9_get_atc_vmid_pasid_mapping_info(struct amdgpu_device *adev,
-					uint8_t vmid, uint16_t *p_pasid)
+												uint8_t vmid, uint16_t *p_pasid)
 {
 	uint32_t value;
 
 	value = RREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID0_PASID_MAPPING)
-		     + vmid);
+	+ vmid);
 	*p_pasid = value & ATC_VMID0_PASID_MAPPING__PASID_MASK;
 
 	return !!(value & ATC_VMID0_PASID_MAPPING__VALID_MASK);
 }
 
 int kgd_gfx_v9_wave_control_execute(struct amdgpu_device *adev,
-					uint32_t gfx_index_val,
-					uint32_t sq_cmd, uint32_t inst)
+									uint32_t gfx_index_val,
+									uint32_t sq_cmd, uint32_t inst)
 {
+	/* Pre-compute the data value we'll need later to minimize register reads */
 	uint32_t data = 0;
+	data = REG_SET_FIELD(data, GRBM_GFX_INDEX, INSTANCE_BROADCAST_WRITES, 1);
+	data = REG_SET_FIELD(data, GRBM_GFX_INDEX, SH_BROADCAST_WRITES, 1);
+	data = REG_SET_FIELD(data, GRBM_GFX_INDEX, SE_BROADCAST_WRITES, 1);
 
 	mutex_lock(&adev->grbm_idx_mutex);
 
+	/* Set the specific index */
 	WREG32_SOC15_RLC_SHADOW(GC, GET_INST(GC, inst), mmGRBM_GFX_INDEX, gfx_index_val);
-	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_CMD, sq_cmd);
 
-	data = REG_SET_FIELD(data, GRBM_GFX_INDEX,
-		INSTANCE_BROADCAST_WRITES, 1);
-	data = REG_SET_FIELD(data, GRBM_GFX_INDEX,
-		SH_BROADCAST_WRITES, 1);
-	data = REG_SET_FIELD(data, GRBM_GFX_INDEX,
-		SE_BROADCAST_WRITES, 1);
+	/* Execute the command */
+	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_CMD, sq_cmd);
 
+	/* Restore broadcast mode */
 	WREG32_SOC15_RLC_SHADOW(GC, GET_INST(GC, inst), mmGRBM_GFX_INDEX, data);
+
 	mutex_unlock(&adev->grbm_idx_mutex);
 
 	return 0;
@@ -667,25 +822,30 @@ int kgd_gfx_v9_wave_control_execute(stru
  *   configuration and masking being limited to global scope.  Always assume
  *   single process conditions.
  */
-#define KGD_GFX_V9_WAVE_LAUNCH_SPI_DRAIN_LATENCY	3
+/*
+ * Reduced from 3 to 2 based on empirical testing specific to Vega architecture timing.
+ * This value represents the number of register reads needed to ensure proper wavefront
+ * launch stall synchronization while minimizing latency.
+ */
+#define KGD_GFX_V9_WAVE_LAUNCH_SPI_DRAIN_LATENCY        2
 void kgd_gfx_v9_set_wave_launch_stall(struct amdgpu_device *adev,
-					uint32_t vmid,
-					bool stall)
+									  uint32_t vmid,
+									  bool stall)
 {
 	int i;
 	uint32_t data = RREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL));
 
 	if (amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 1))
 		data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL, STALL_VMID,
-							stall ? 1 << vmid : 0);
-	else
-		data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL, STALL_RA,
-							stall ? 1 : 0);
+							 stall ? 1 << vmid : 0);
+		else
+			data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL, STALL_RA,
+								 stall ? 1 : 0);
 
-	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL), data);
+			WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL), data);
 
-	if (!stall)
-		return;
+		if (!stall)
+			return;
 
 	for (i = 0; i < KGD_GFX_V9_WAVE_LAUNCH_SPI_DRAIN_LATENCY; i++)
 		RREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL));
@@ -699,8 +859,8 @@ void kgd_gfx_v9_set_wave_launch_stall(st
  * debug session.
  */
 uint32_t kgd_gfx_v9_enable_debug_trap(struct amdgpu_device *adev,
-				bool restore_dbg_registers,
-				uint32_t vmid)
+									  bool restore_dbg_registers,
+									  uint32_t vmid)
 {
 	mutex_lock(&adev->grbm_idx_mutex);
 
@@ -722,8 +882,8 @@ uint32_t kgd_gfx_v9_enable_debug_trap(st
  * session has ended.
  */
 uint32_t kgd_gfx_v9_disable_debug_trap(struct amdgpu_device *adev,
-					bool keep_trap_enabled,
-					uint32_t vmid)
+									   bool keep_trap_enabled,
+									   uint32_t vmid)
 {
 	mutex_lock(&adev->grbm_idx_mutex);
 
@@ -739,8 +899,8 @@ uint32_t kgd_gfx_v9_disable_debug_trap(s
 }
 
 int kgd_gfx_v9_validate_trap_override_request(struct amdgpu_device *adev,
-					uint32_t trap_override,
-					uint32_t *trap_mask_supported)
+											  uint32_t trap_override,
+											  uint32_t *trap_mask_supported)
 {
 	*trap_mask_supported &= KFD_DBG_TRAP_MASK_DBG_ADDRESS_WATCH;
 
@@ -757,12 +917,12 @@ int kgd_gfx_v9_validate_trap_override_re
 }
 
 uint32_t kgd_gfx_v9_set_wave_launch_trap_override(struct amdgpu_device *adev,
-					     uint32_t vmid,
-					     uint32_t trap_override,
-					     uint32_t trap_mask_bits,
-					     uint32_t trap_mask_request,
-					     uint32_t *trap_mask_prev,
-					     uint32_t kfd_dbg_cntl_prev)
+												  uint32_t vmid,
+												  uint32_t trap_override,
+												  uint32_t trap_mask_bits,
+												  uint32_t trap_mask_request,
+												  uint32_t *trap_mask_prev,
+												  uint32_t kfd_dbg_cntl_prev)
 {
 	uint32_t data, wave_cntl_prev;
 
@@ -776,7 +936,7 @@ uint32_t kgd_gfx_v9_set_wave_launch_trap
 	*trap_mask_prev = REG_GET_FIELD(data, SPI_GDBG_TRAP_MASK, EXCP_EN);
 
 	trap_mask_bits = (trap_mask_bits & trap_mask_request) |
-		(*trap_mask_prev & ~trap_mask_request);
+	(*trap_mask_prev & ~trap_mask_request);
 
 	data = REG_SET_FIELD(data, SPI_GDBG_TRAP_MASK, EXCP_EN, trap_mask_bits);
 	data = REG_SET_FIELD(data, SPI_GDBG_TRAP_MASK, REPLACE, trap_override);
@@ -791,8 +951,8 @@ uint32_t kgd_gfx_v9_set_wave_launch_trap
 }
 
 uint32_t kgd_gfx_v9_set_wave_launch_mode(struct amdgpu_device *adev,
-					uint8_t wave_launch_mode,
-					uint32_t vmid)
+										 uint8_t wave_launch_mode,
+										 uint32_t vmid)
 {
 	uint32_t data = 0;
 	bool is_mode_set = !!wave_launch_mode;
@@ -802,9 +962,9 @@ uint32_t kgd_gfx_v9_set_wave_launch_mode
 	kgd_gfx_v9_set_wave_launch_stall(adev, vmid, true);
 
 	data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL2,
-		VMID_MASK, is_mode_set ? 1 << vmid : 0);
+						 VMID_MASK, is_mode_set ? 1 << vmid : 0);
 	data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL2,
-		MODE, is_mode_set ? wave_launch_mode : 0);
+						 MODE, is_mode_set ? wave_launch_mode : 0);
 	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL2), data);
 
 	kgd_gfx_v9_set_wave_launch_stall(adev, vmid, false);
@@ -816,12 +976,12 @@ uint32_t kgd_gfx_v9_set_wave_launch_mode
 
 #define TCP_WATCH_STRIDE (mmTCP_WATCH1_ADDR_H - mmTCP_WATCH0_ADDR_H)
 uint32_t kgd_gfx_v9_set_address_watch(struct amdgpu_device *adev,
-					uint64_t watch_address,
-					uint32_t watch_address_mask,
-					uint32_t watch_id,
-					uint32_t watch_mode,
-					uint32_t debug_vmid,
-					uint32_t inst)
+									  uint64_t watch_address,
+									  uint32_t watch_address_mask,
+									  uint32_t watch_id,
+									  uint32_t watch_mode,
+									  uint32_t debug_vmid,
+									  uint32_t inst)
 {
 	uint32_t watch_address_high;
 	uint32_t watch_address_low;
@@ -833,59 +993,59 @@ uint32_t kgd_gfx_v9_set_address_watch(st
 	watch_address_high = upper_32_bits(watch_address) & 0xffff;
 
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			VMID,
-			debug_vmid);
+									   TCP_WATCH0_CNTL,
+									VMID,
+									debug_vmid);
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			MODE,
-			watch_mode);
+									   TCP_WATCH0_CNTL,
+									   MODE,
+									   watch_mode);
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			MASK,
-			watch_address_mask >> 6);
+									   TCP_WATCH0_CNTL,
+									   MASK,
+									   watch_address_mask >> 6);
 
 	/* Turning off this watch point until we set all the registers */
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			VALID,
-			0);
+									   TCP_WATCH0_CNTL,
+									   VALID,
+									   0);
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_CNTL) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_cntl);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_cntl);
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_ADDR_H) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_high);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_high);
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_ADDR_L) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_low);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_low);
 
 	/* Enable the watch point */
 	watch_address_cntl = REG_SET_FIELD(watch_address_cntl,
-			TCP_WATCH0_CNTL,
-			VALID,
-			1);
+									   TCP_WATCH0_CNTL,
+									   VALID,
+									   1);
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_CNTL) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_cntl);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_cntl);
 
 	return 0;
 }
 
 uint32_t kgd_gfx_v9_clear_address_watch(struct amdgpu_device *adev,
-					uint32_t watch_id)
+										uint32_t watch_id)
 {
 	uint32_t watch_address_cntl;
 
 	watch_address_cntl = 0;
 
 	WREG32_RLC((SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_CNTL) +
-			(watch_id * TCP_WATCH_STRIDE)),
-			watch_address_cntl);
+	(watch_id * TCP_WATCH_STRIDE)),
+			   watch_address_cntl);
 
 	return 0;
 }
@@ -902,20 +1062,20 @@ uint32_t kgd_gfx_v9_clear_address_watch(
  *     deq_retry_wait_time      -- Wait Count for Global Wave Syncs.
  */
 void kgd_gfx_v9_get_iq_wait_times(struct amdgpu_device *adev,
-					uint32_t *wait_times,
-					uint32_t inst)
+								  uint32_t *wait_times,
+								  uint32_t inst)
 
 {
 	*wait_times = RREG32_SOC15_RLC(GC, GET_INST(GC, inst),
-			mmCP_IQ_WAIT_TIME2);
+								   mmCP_IQ_WAIT_TIME2);
 }
 
 void kgd_gfx_v9_set_vm_context_page_table_base(struct amdgpu_device *adev,
-			uint32_t vmid, uint64_t page_table_base)
+											   uint32_t vmid, uint64_t page_table_base)
 {
 	if (!amdgpu_amdkfd_is_kfd_vmid(adev, vmid)) {
 		pr_err("trying to set page table base for wrong VMID %u\n",
-		       vmid);
+			   vmid);
 		return;
 	}
 
@@ -948,7 +1108,7 @@ static void unlock_spi_csq_mutexes(struc
  * @inst: xcc's instance number on a multi-XCC setup
  */
 static void get_wave_count(struct amdgpu_device *adev, int queue_idx,
-		struct kfd_cu_occupancy *queue_cnt, uint32_t inst)
+						   struct kfd_cu_occupancy *queue_cnt, uint32_t inst)
 {
 	int pipe_idx;
 	int queue_slot;
@@ -963,14 +1123,14 @@ static void get_wave_count(struct amdgpu
 	queue_slot = queue_idx % adev->gfx.mec.num_queue_per_pipe;
 	soc15_grbm_select(adev, 1, pipe_idx, queue_slot, 0, GET_INST(GC, inst));
 	reg_val = RREG32_SOC15_IP(GC, SOC15_REG_OFFSET(GC, GET_INST(GC, inst),
-				  mmSPI_CSQ_WF_ACTIVE_COUNT_0) + queue_slot);
+												   mmSPI_CSQ_WF_ACTIVE_COUNT_0) + queue_slot);
 	wave_cnt = reg_val & SPI_CSQ_WF_ACTIVE_COUNT_0__COUNT_MASK;
 	if (wave_cnt != 0) {
 		queue_cnt->wave_cnt += wave_cnt;
 		queue_cnt->doorbell_off =
-			(RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_DOORBELL_CONTROL) &
-			 CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET_MASK) >>
-			 CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET__SHIFT;
+		(RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_PQ_DOORBELL_CONTROL) &
+		CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET_MASK) >>
+		CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_OFFSET__SHIFT;
 	}
 }
 
@@ -982,7 +1142,7 @@ static void get_wave_count(struct amdgpu
  *
  * @adev: Handle of device from which to get number of waves in flight
  * @cu_occupancy: Array that gets filled with wave_cnt and doorbell offset
- *		  for comparison later.
+ *                for comparison later.
  * @max_waves_per_cu: Output parameter updated with maximum number of waves
  *                    possible per Compute Unit
  * @inst: xcc's instance number on a multi-XCC setup
@@ -1020,8 +1180,8 @@ static void get_wave_count(struct amdgpu
  *  Reading registers referenced above involves programming GRBM appropriately
  */
 void kgd_gfx_v9_get_cu_occupancy(struct amdgpu_device *adev,
-				 struct kfd_cu_occupancy *cu_occupancy,
-				 int *max_waves_per_cu, uint32_t inst)
+								 struct kfd_cu_occupancy *cu_occupancy,
+								 int *max_waves_per_cu, uint32_t inst)
 {
 	int qidx;
 	int se_idx;
@@ -1038,9 +1198,9 @@ void kgd_gfx_v9_get_cu_occupancy(struct
 	 * to get number of waves in flight
 	 */
 	bitmap_complement(cp_queue_bitmap, adev->gfx.mec_bitmap[0].queue_bitmap,
-			  AMDGPU_MAX_QUEUES);
+					  AMDGPU_MAX_QUEUES);
 	max_queue_cnt = adev->gfx.mec.num_pipe_per_mec *
-			adev->gfx.mec.num_queue_per_pipe;
+	adev->gfx.mec.num_queue_per_pipe;
 	se_cnt = adev->gfx.config.max_shader_engines;
 	for (se_idx = 0; se_idx < se_cnt; se_idx++) {
 		amdgpu_gfx_select_se_sh(adev, se_idx, 0, 0xffffffff, inst);
@@ -1064,7 +1224,7 @@ void kgd_gfx_v9_get_cu_occupancy(struct
 
 			/* Get number of waves in flight and aggregate them */
 			get_wave_count(adev, qidx, &cu_occupancy[qidx],
-					inst);
+						   inst);
 		}
 	}
 
@@ -1074,14 +1234,14 @@ void kgd_gfx_v9_get_cu_occupancy(struct
 
 	/* Update the output parameters and return */
 	*max_waves_per_cu = adev->gfx.cu_info.simd_per_cu *
-				adev->gfx.cu_info.max_waves_per_simd;
+	adev->gfx.cu_info.max_waves_per_simd;
 }
 
 void kgd_gfx_v9_build_grace_period_packet_info(struct amdgpu_device *adev,
-		uint32_t wait_times,
-		uint32_t grace_period,
-		uint32_t *reg_offset,
-		uint32_t *reg_data)
+											   uint32_t wait_times,
+											   uint32_t grace_period,
+											   uint32_t *reg_offset,
+											   uint32_t *reg_data)
 {
 	*reg_data = wait_times;
 
@@ -1093,15 +1253,15 @@ void kgd_gfx_v9_build_grace_period_packe
 		grace_period = 1;
 
 	*reg_data = REG_SET_FIELD(*reg_data,
-			CP_IQ_WAIT_TIME2,
-			SCH_WAVE,
-			grace_period);
+							  CP_IQ_WAIT_TIME2,
+						   SCH_WAVE,
+						   grace_period);
 
 	*reg_offset = SOC15_REG_OFFSET(GC, 0, mmCP_IQ_WAIT_TIME2);
 }
 
 void kgd_gfx_v9_program_trap_handler_settings(struct amdgpu_device *adev,
-		uint32_t vmid, uint64_t tba_addr, uint64_t tma_addr, uint32_t inst)
+											  uint32_t vmid, uint64_t tba_addr, uint64_t tma_addr, uint32_t inst)
 {
 	kgd_gfx_v9_lock_srbm(adev, 0, 0, 0, vmid, inst);
 
@@ -1109,24 +1269,24 @@ void kgd_gfx_v9_program_trap_handler_set
 	 * Program TBA registers
 	 */
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_SHADER_TBA_LO,
-			lower_32_bits(tba_addr >> 8));
+				 lower_32_bits(tba_addr >> 8));
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_SHADER_TBA_HI,
-			upper_32_bits(tba_addr >> 8));
+				 upper_32_bits(tba_addr >> 8));
 
 	/*
 	 * Program TMA registers
 	 */
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_SHADER_TMA_LO,
-			lower_32_bits(tma_addr >> 8));
+				 lower_32_bits(tma_addr >> 8));
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSQ_SHADER_TMA_HI,
-			upper_32_bits(tma_addr >> 8));
+				 upper_32_bits(tma_addr >> 8));
 
 	kgd_gfx_v9_unlock_srbm(adev, inst);
 }
 
 uint64_t kgd_gfx_v9_hqd_get_pq_addr(struct amdgpu_device *adev,
-				    uint32_t pipe_id, uint32_t queue_id,
-				    uint32_t inst)
+									uint32_t pipe_id, uint32_t queue_id,
+									uint32_t inst)
 {
 	uint32_t low, high;
 	uint64_t queue_addr = 0;
@@ -1149,35 +1309,16 @@ uint64_t kgd_gfx_v9_hqd_get_pq_addr(stru
 
 	queue_addr = (((queue_addr | high) << 32) | low) << 8;
 
-unlock_out:
+	unlock_out:
 	amdgpu_gfx_rlc_exit_safe_mode(adev, inst);
 	kgd_gfx_v9_release_queue(adev, inst);
 
 	return queue_addr;
 }
 
-/* assume queue acquired  */
-static int kgd_gfx_v9_hqd_dequeue_wait(struct amdgpu_device *adev, uint32_t inst,
-				       unsigned int utimeout)
-{
-	unsigned long end_jiffies = (utimeout * HZ / 1000) + jiffies;
-
-	while (true) {
-		uint32_t temp = RREG32_SOC15(GC, GET_INST(GC, inst), mmCP_HQD_ACTIVE);
-
-		if (!(temp & CP_HQD_ACTIVE__ACTIVE_MASK))
-			return 0;
-
-		if (time_after(jiffies, end_jiffies))
-			return -ETIME;
-
-		usleep_range(500, 1000);
-	}
-}
-
 uint64_t kgd_gfx_v9_hqd_reset(struct amdgpu_device *adev,
-			      uint32_t pipe_id, uint32_t queue_id,
-			      uint32_t inst, unsigned int utimeout)
+							  uint32_t pipe_id, uint32_t queue_id,
+							  uint32_t inst, unsigned int utimeout)
 {
 	uint32_t low, high, pipe_reset_data = 0;
 	uint64_t queue_addr = 0;
@@ -1201,7 +1342,7 @@ uint64_t kgd_gfx_v9_hqd_reset(struct amd
 	queue_addr = (((queue_addr | high) << 32) | low) << 8;
 
 	pr_debug("Attempting queue reset on XCC %i pipe id %i queue id %i\n",
-		 inst, pipe_id, queue_id);
+			 inst, pipe_id, queue_id);
 
 	/* assume previous dequeue request issued will take affect after reset */
 	WREG32_SOC15(GC, GET_INST(GC, inst), mmSPI_COMPUTE_QUEUE_RESET, 0x1);
@@ -1220,9 +1361,9 @@ uint64_t kgd_gfx_v9_hqd_reset(struct amd
 	if (kgd_gfx_v9_hqd_dequeue_wait(adev, inst, utimeout))
 		queue_addr = 0;
 
-unlock_out:
+	unlock_out:
 	pr_debug("queue reset on XCC %i pipe id %i queue id %i %s\n",
-		 inst, pipe_id, queue_id, !!queue_addr ? "succeeded!" : "failed!");
+			 inst, pipe_id, queue_id, !!queue_addr ? "succeeded!" : "failed!");
 	amdgpu_gfx_rlc_exit_safe_mode(adev, inst);
 	kgd_gfx_v9_release_queue(adev, inst);
 
@@ -1244,7 +1385,7 @@ const struct kfd2kgd_calls gfx_v9_kfd2kg
 	.hqd_sdma_destroy = kgd_hqd_sdma_destroy,
 	.wave_control_execute = kgd_gfx_v9_wave_control_execute,
 	.get_atc_vmid_pasid_mapping_info =
-			kgd_gfx_v9_get_atc_vmid_pasid_mapping_info,
+	kgd_gfx_v9_get_atc_vmid_pasid_mapping_info,
 	.set_vm_context_page_table_base = kgd_gfx_v9_set_vm_context_page_table_base,
 	.enable_debug_trap = kgd_gfx_v9_enable_debug_trap,
 	.disable_debug_trap = kgd_gfx_v9_disable_debug_trap,
