From e002974fb1e278c5a2d29dcaf6ca032b98d32399 Mon Sep 17 00:00:00 2001
From: Peter Jung <admin@ptr1337.dev>
Date: Wed, 2 Nov 2022 13:04:10 +0100
Subject: [PATCH] sched/fair: Choose the CPU where short task is running during
 wake up

At LPC 2022 Real-time and Scheduling Micro Conference we presented
the cross CPU wakeup issue. This patch is a text version of the
talk, and hopefully, we can clarify the problem and appreciate any
feedback.

The main purpose of this change is to avoid too many crosses CPU
wake up when the system is busy. Please refer to the commit log
of [PATCH 2/2] for detail.

This patch set is composed of two parts. The first part is to introduce
the definition of a short-duration task. The second part leverages the
first part to choose a CPU where only one short-duration task is running
on. This CPU is chosen as the candidate to place a woken task.

This version is modified based on the following feedback on v1:
1. Tim suggested raising the bar to choose a CPU with a short-duration
   task, by checking if the short-duration task is the only runnable
   task on the target CPU.
2. To address Peter's concern: would this patch inhibit spreading the
   workload when there are idle CPUs around? The patch would only take
   effect when the system is relatively busy, and only choose the CPU
   where only one short-duration task is running.
3. Prateek, Honglwei and Hillf suggsted to prefer previous idle CPU to the
   CPU with short-duration task running.

v1 link: https://lore.kernel.org/lkml/20220915165407.1776363-1-yu.c.chen@intel.com/

Chen Yu (2):
  sched/fair: Introduce short duration task check
  sched/fair: Choose the CPU where short task is running during wake up

Signed-off-by: Peter Jung <admin@ptr1337.dev>
---
 include/linux/sched.h   |  9 ++++
 kernel/sched/core.c     |  2 +
 kernel/sched/fair.c     | 99 +++++++++++++++++++++++++++++++++++++++++
 kernel/sched/features.h |  1 +
 4 files changed, 111 insertions(+)

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 86e2daea0175..ce9d65d590b3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -549,6 +549,15 @@ struct sched_entity {
 	u64				prev_sum_exec_runtime;
 
 	u64				nr_migrations;
+	/*
+	 * The 'snapshot' of sum_exec_runtime when task
+	 * voluntarily switches out. This is used to
+	 * calculate the average duration below.
+	 */
+	u64				prev_sum_runtime_vol;
+	/* average duration of a task */
+	u64				dur_avg;
+
 #ifdef CONFIG_SCHED_BORE
 	u64				burst_time;
 	u8				burst_score;
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 015821972e7c..02aaed51ad93 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -4350,6 +4350,8 @@ static void __sched_fork(unsigned long clone_flags, struct task_struct *p)
 	p->se.prev_sum_exec_runtime	= 0;
 	p->se.nr_migrations		= 0;
 	p->se.vruntime			= 0;
+	p->se.dur_avg			= 0;
+	p->se.prev_sum_runtime_vol	= 0;
 	INIT_LIST_HEAD(&p->se.group_node);
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 72f28af4009f..758175acb874 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -6175,6 +6175,19 @@ static int wake_wide(struct task_struct *p)
 	return 1;
 }
 
+/*
+ * If a task switches in and then voluntarily relinquishes the
+ * CPU quickly, it is regarded as a short duration task.
+ * sysctl_sched_min_granularity is chosen as the threshold,
+ * as this value is the minimal slice if there are too many
+ * runnable tasks, see __sched_period().
+ */
+static inline int is_short_task(struct task_struct *p)
+{
+	return sched_feat(SIS_SHORT) &&
+		(p->se.dur_avg <= sysctl_sched_min_granularity);
+}
+
 /*
  * The purpose of wake_affine() is to quickly determine on which CPU we can run
  * soonest. For the purpose of speed we only consider the waking and previous
@@ -6211,6 +6224,11 @@ wake_affine_idle(int this_cpu, int prev_cpu, int sync)
 	if (available_idle_cpu(prev_cpu))
 		return prev_cpu;
 
+	/* The only running task is a short duration one. */
+	if (cpu_rq(this_cpu)->nr_running == 1 &&
+	    is_short_task(cpu_curr(this_cpu)))
+		return this_cpu;
+
 	return nr_cpumask_bits;
 }
 
@@ -6589,6 +6607,23 @@ static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, bool
 			/* overloaded LLC is unlikely to have idle cpu/core */
 			if (nr == 1)
 				return -1;
+
+			/*
+			 * If nr is smaller than 60% of llc_weight, it
+			 * indicates that the util_avg% is higher than 50%.
+			 * This is calculated by SIS_UTIL in
+			 * update_idle_cpu_scan(). The 50% util_avg indicates
+			 * a half-busy LLC domain. System busier than this
+			 * level could lower its bar to choose a compromised
+			 * "idle" CPU, so as to avoid the overhead of cross
+			 * CPU wakeup. If the task on target CPU is a short
+			 * duration one, and it is the only running task, pick
+			 * target directly.
+			 */
+			if (!has_idle_core && (5 * nr < 3 * sd->span_weight) &&
+			    cpu_rq(target)->nr_running == 1 &&
+			    is_short_task(cpu_curr(target)))
+				return target;
 		}
 	}
 
@@ -7735,6 +7770,70 @@ static void put_prev_task_fair(struct rq *rq, struct task_struct *prev)
 {
 	struct sched_entity *se = &prev->se;
 	struct cfs_rq *cfs_rq;
+	u64 this_dur_avg, last_dur_avg;
+	long delta;
+
+	/*
+	 * Calculate the task's average duration.
+	 *
+	 * Only consider that task voluntarily relinquishes the CPU.
+	 * For example, suppose on CPU1, task p1 and p2 runs
+	 * alternatively:
+	 *
+	 * --------------------> time
+	 *
+	 * | p1 runs 1ms | p2 preempt p1 | p1 runs 0.5ms and sleeps |
+	 * ^             ^               ^                          ^
+	 * |_____________|               |__________________________|
+	 *        |____________________________________|
+	 *                   p1's duration
+	 *
+	 *
+	 * The duration of p1 is 1.5ms rather than 0.5ms or 1ms
+	 * in above case. That is to say, the duration period starts
+	 * when task p1 switches in, ends when task p1 voluntarily
+	 * relinquishes the CPU. This duration descibes the "nature"
+	 * of a task: If a task is not preempted, how long it will
+	 * run.
+	 *
+	 * The Exponential Weighted Moving Average (EWMA)
+	 * is used to calculate the average duration.
+	 * Borrowed from util_est_update():
+	 *
+	 *  ewma(t) = w * this_dur_avg + (1-w) * ewma(t-1)
+	 *
+	 * When 'w' is 0.125, it becomes update_avg().
+	 * This indicates that we care about approximately
+	 * the recent 1 / 0.125 = 8 history duration.
+	 */
+	if (sched_feat(SIS_SHORT) && !prev->on_rq) {
+		/*
+		 * sum_exec_runtime has been updated in update_curr()
+		 * because we reach here via dequeue.
+		 */
+		this_dur_avg = se->sum_exec_runtime - se->prev_sum_runtime_vol;
+		/*
+		 * Record the accumulated runtime when task voluntarily
+		 * switches out. End of old duration period, a new period
+		 * starts.
+		 */
+		se->prev_sum_runtime_vol = se->sum_exec_runtime;
+
+		last_dur_avg = se->dur_avg;
+		delta = this_dur_avg - last_dur_avg;
+		/* consider large change to avoid frequent update */
+		if (abs(delta) >= sysctl_sched_min_granularity) {
+			/*
+			 * If it is the first time the task starts to
+			 * record dur_avg, discard the initial value 0.
+			 * Otherwise, calculate the EWMA.
+			 */
+			if (unlikely(!this_dur_avg))
+				se->dur_avg = this_dur_avg;
+			else
+				update_avg(&se->dur_avg, this_dur_avg);
+		}
+	}
 
 	for_each_sched_entity(se) {
 		cfs_rq = cfs_rq_of(se);
diff --git a/kernel/sched/features.h b/kernel/sched/features.h
index 01f3393c38d3..db19463ab890 100644
--- a/kernel/sched/features.h
+++ b/kernel/sched/features.h
@@ -74,6 +74,7 @@ SCHED_FEAT(TTWU_QUEUE, true)
  */
 SCHED_FEAT(SIS_PROP, false)
 SCHED_FEAT(SIS_UTIL, true)
+SCHED_FEAT(SIS_SHORT, true)
 
 /*
  * Issue a WARN when we do multiple update_rq_clock() calls
-- 
2.38.1.381.gc03801e19c

