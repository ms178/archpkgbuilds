From 482ff6af070d68e9c4bbbb86f443118ccdcf6ace Mon Sep 17 00:00:00 2001
From: Peter Jung <admin@ptr1337.dev>
Date: Thu, 1 Sep 2022 12:48:42 +0200
Subject: [PATCH] Revert "Extend-DAMOS-for-Proactive-LRU-lists-Sorting"

This reverts commit 6e18e6ac4ef7882bec7e4f3b86a807bd4a151698.
---
 Documentation/admin-guide/mm/damon/index.rst  |   1 -
 .../admin-guide/mm/damon/lru_sort.rst         | 294 ----------
 Documentation/admin-guide/mm/damon/usage.rst  |   2 -
 include/linux/damon.h                         |   4 -
 mm/damon/Kconfig                              |   8 -
 mm/damon/Makefile                             |   1 -
 mm/damon/core-test.h                          |   6 +-
 mm/damon/core.c                               |  21 +-
 mm/damon/dbgfs.c                              |  66 +--
 mm/damon/lru_sort.c                           | 541 ------------------
 mm/damon/ops-common.c                         |  42 --
 mm/damon/ops-common.h                         |   2 -
 mm/damon/paddr.c                              |  60 +-
 mm/damon/reclaim.c                            |   9 +-
 mm/damon/sysfs.c                              |   2 -
 mm/damon/vaddr-test.h                         |  36 +-
 mm/damon/vaddr.c                              |  71 +--
 17 files changed, 94 insertions(+), 1072 deletions(-)
 delete mode 100644 Documentation/admin-guide/mm/damon/lru_sort.rst
 delete mode 100644 mm/damon/lru_sort.c

diff --git a/Documentation/admin-guide/mm/damon/index.rst b/Documentation/admin-guide/mm/damon/index.rst
index 53762770e0e4..61aff88347f3 100644
--- a/Documentation/admin-guide/mm/damon/index.rst
+++ b/Documentation/admin-guide/mm/damon/index.rst
@@ -14,4 +14,3 @@ optimize those.
    start
    usage
    reclaim
-   lru_sort
diff --git a/Documentation/admin-guide/mm/damon/lru_sort.rst b/Documentation/admin-guide/mm/damon/lru_sort.rst
deleted file mode 100644
index c09cace80651..000000000000
--- a/Documentation/admin-guide/mm/damon/lru_sort.rst
+++ /dev/null
@@ -1,294 +0,0 @@
-.. SPDX-License-Identifier: GPL-2.0
-
-=============================
-DAMON-based LRU-lists Sorting
-=============================
-
-DAMON-based LRU-lists Sorting (DAMON_LRU_SORT) is a static kernel module that
-aimed to be used for proactive and lightweight data access pattern based
-(de)prioritization of pages on their LRU-lists for making LRU-lists a more
-trusworthy data access pattern source.
-
-Where Proactive LRU-lists Sorting is Required?
-==============================================
-
-As page-granularity access checking overhead could be significant on huge
-systems, LRU lists are normally not proactively sorted but partially and
-reactively sorted for special events including specific user requests, system
-calls and memory pressure.  As a result, LRU lists are sometimes not so
-perfectly prepared to be used as a trustworthy access pattern source for some
-situations including reclamation target pages selection under sudden memory
-pressure.
-
-Because DAMON can identify access patterns of best-effort accuracy while
-inducing only user-specified range of overhead, proactively running
-DAMON_LRU_SORT could be helpful for making LRU lists more trustworthy access
-pattern source with low and controlled overhead.
-
-How It Works?
-=============
-
-DAMON_LRU_SORT finds hot pages (pages of memory regions that showing access
-rates that higher than a user-specified threshold) and cold pages (pages of
-memory regions that showing no access for a time that longer than a
-user-specified threshold) using DAMON, and prioritizes hot pages while
-deprioritizing cold pages on their LRU-lists.  To avoid it consuming too much
-CPU for the prioritizations, a CPU time usage limit can be configured.  Under
-the limit, it prioritizes and deprioritizes more hot and cold pages first,
-respectively.  System administrators can also configure under what situation
-this scheme should automatically activated and deactivated with three memory
-pressure watermarks.
-
-Its default parameters for hotness/coldness thresholds and CPU quota limit are
-conservatively chosen.  That is, the module under its default parameters could
-be widely used without harm for common situations while providing a level of
-benefits for systems having clear hot/cold access patterns under memory
-pressure while consuming only a limited small portion of CPU time.
-
-Interface: Module Parameters
-============================
-
-To use this feature, you should first ensure your system is running on a kernel
-that is built with ``CONFIG_DAMON_LRU_SORT=y``.
-
-To let sysadmins enable or disable it and tune for the given system,
-DAMON_LRU_SORT utilizes module parameters.  That is, you can put
-``damon_lru_sort.<parameter>=<value>`` on the kernel boot command line or write
-proper values to ``/sys/modules/damon_lru_sort/parameters/<parameter>`` files.
-
-Below are the description of each parameter.
-
-enabled
--------
-
-Enable or disable DAMON_LRU_SORT.
-
-You can enable DAMON_LRU_SORT by setting the value of this parameter as ``Y``.
-Setting it as ``N`` disables DAMON_LRU_SORT.  Note that DAMON_LRU_SORT could do
-no real monitoring and LRU-lists sorting due to the watermarks-based activation
-condition.  Refer to below descriptions for the watermarks parameter for this.
-
-commit_inputs
--------------
-
-Make DAMON_LRU_SORT reads the input parameters again, except ``enabled``.
-
-Input parameters that updated while DAMON_LRU_SORT is running are not applied
-by default.  Once this parameter is set as ``Y``, DAMON_LRU_SORT reads values
-of parametrs except ``enabled`` again.  Once the re-reading is done, this
-parameter is set as ``N``.  If invalid parameters are found while the
-re-reading, DAMON_LRU_SORT will be disabled.
-
-hot_thres_access_freq
----------------------
-
-Access frequency threshold for hot memory regions identification in permil.
-
-If a memory region is accessed in frequency of this or higher, DAMON_LRU_SORT
-identifies the region as hot, and mark it as accessed on the LRU list, so that
-it could not be reclaimed under memory pressure.  50% by default.
-
-cold_min_age
-------------
-
-Time threshold for cold memory regions identification in microseconds.
-
-If a memory region is not accessed for this or longer time, DAMON_LRU_SORT
-identifies the region as cold, and mark it as unaccessed on the LRU list, so
-that it could be reclaimed first under memory pressure.  120 seconds by
-default.
-
-quota_ms
---------
-
-Limit of time for trying the LRU lists sorting in milliseconds.
-
-DAMON_LRU_SORT tries to use only up to this time within a time window
-(quota_reset_interval_ms) for trying LRU lists sorting.  This can be used
-for limiting CPU consumption of DAMON_LRU_SORT.  If the value is zero, the
-limit is disabled.
-
-10 ms by default.
-
-quota_reset_interval_ms
------------------------
-
-The time quota charge reset interval in milliseconds.
-
-The charge reset interval for the quota of time (quota_ms).  That is,
-DAMON_LRU_SORT does not try LRU-lists sorting for more than quota_ms
-milliseconds or quota_sz bytes within quota_reset_interval_ms milliseconds.
-
-1 second by default.
-
-wmarks_interval
----------------
-
-The watermarks check time interval in microseconds.
-
-Minimal time to wait before checking the watermarks, when DAMON_LRU_SORT is
-enabled but inactive due to its watermarks rule.  5 seconds by default.
-
-wmarks_high
------------
-
-Free memory rate (per thousand) for the high watermark.
-
-If free memory of the system in bytes per thousand bytes is higher than this,
-DAMON_LRU_SORT becomes inactive, so it does nothing but periodically checks the
-watermarks.  200 (20%) by default.
-
-wmarks_mid
-----------
-
-Free memory rate (per thousand) for the middle watermark.
-
-If free memory of the system in bytes per thousand bytes is between this and
-the low watermark, DAMON_LRU_SORT becomes active, so starts the monitoring and
-the LRU-lists sorting.  150 (15%) by default.
-
-wmarks_low
-----------
-
-Free memory rate (per thousand) for the low watermark.
-
-If free memory of the system in bytes per thousand bytes is lower than this,
-DAMON_LRU_SORT becomes inactive, so it does nothing but periodically checks the
-watermarks.  50 (5%) by default.
-
-sample_interval
----------------
-
-Sampling interval for the monitoring in microseconds.
-
-The sampling interval of DAMON for the cold memory monitoring.  Please refer to
-the DAMON documentation (:doc:`usage`) for more detail.  5ms by default.
-
-aggr_interval
--------------
-
-Aggregation interval for the monitoring in microseconds.
-
-The aggregation interval of DAMON for the cold memory monitoring.  Please
-refer to the DAMON documentation (:doc:`usage`) for more detail.  100ms by
-default.
-
-min_nr_regions
---------------
-
-Minimum number of monitoring regions.
-
-The minimal number of monitoring regions of DAMON for the cold memory
-monitoring.  This can be used to set lower-bound of the monitoring quality.
-But, setting this too high could result in increased monitoring overhead.
-Please refer to the DAMON documentation (:doc:`usage`) for more detail.  10 by
-default.
-
-max_nr_regions
---------------
-
-Maximum number of monitoring regions.
-
-The maximum number of monitoring regions of DAMON for the cold memory
-monitoring.  This can be used to set upper-bound of the monitoring overhead.
-However, setting this too low could result in bad monitoring quality.  Please
-refer to the DAMON documentation (:doc:`usage`) for more detail.  1000 by
-defaults.
-
-monitor_region_start
---------------------
-
-Start of target memory region in physical address.
-
-The start physical address of memory region that DAMON_LRU_SORT will do work
-against.  By default, biggest System RAM is used as the region.
-
-monitor_region_end
-------------------
-
-End of target memory region in physical address.
-
-The end physical address of memory region that DAMON_LRU_SORT will do work
-against.  By default, biggest System RAM is used as the region.
-
-kdamond_pid
------------
-
-PID of the DAMON thread.
-
-If DAMON_LRU_SORT is enabled, this becomes the PID of the worker thread.  Else,
--1.
-
-nr_lru_sort_tried_hot_regions
------------------------------
-
-Number of hot memory regions that tried to be LRU-sorted.
-
-bytes_lru_sort_tried_hot_regions
---------------------------------
-
-Total bytes of hot memory regions that tried to be LRU-sorted.
-
-nr_lru_sorted_hot_regions
--------------------------
-
-Number of hot memory regions that successfully be LRU-sorted.
-
-bytes_lru_sorted_hot_regions
-----------------------------
-
-Total bytes of hot memory regions that successfully be LRU-sorted.
-
-nr_hot_quota_exceeds
---------------------
-
-Number of times that the time quota limit for hot regions have exceeded.
-
-nr_lru_sort_tried_cold_regions
-------------------------------
-
-Number of cold memory regions that tried to be LRU-sorted.
-
-bytes_lru_sort_tried_cold_regions
----------------------------------
-
-Total bytes of cold memory regions that tried to be LRU-sorted.
-
-nr_lru_sorted_cold_regions
---------------------------
-
-Number of cold memory regions that successfully be LRU-sorted.
-
-bytes_lru_sorted_cold_regions
------------------------------
-
-Total bytes of cold memory regions that successfully be LRU-sorted.
-
-nr_cold_quota_exceeds
----------------------
-
-Number of times that the time quota limit for cold regions have exceeded.
-
-Example
-=======
-
-Below runtime example commands make DAMON_LRU_SORT to find memory regions
-having >=50% access frequency and LRU-prioritize while LRU-deprioritizing
-memory regions that not accessed for 120 seconds.  The prioritization and
-deprioritization is limited to be done using only up to 1% CPU time to avoid
-DAMON_LRU_SORT consuming too much CPU time for the (de)prioritization.  It also
-asks DAMON_LRU_SORT to do nothing if the system's free memory rate is more than
-50%, but start the real works if it becomes lower than 40%.  If DAMON_RECLAIM
-doesn't make progress and therefore the free memory rate becomes lower than
-20%, it asks DAMON_LRU_SORT to do nothing again, so that we can fall back to
-the LRU-list based page granularity reclamation. ::
-
-    # cd /sys/modules/damon_lru_sort/parameters
-    # echo 500 > hot_thres_access_freq
-    # echo 120000000 > cold_min_age
-    # echo 10 > quota_ms
-    # echo 1000 > quota_reset_interval_ms
-    # echo 500 > wmarks_high
-    # echo 400 > wmarks_mid
-    # echo 200 > wmarks_low
-    # echo Y > enabled
diff --git a/Documentation/admin-guide/mm/damon/usage.rst b/Documentation/admin-guide/mm/damon/usage.rst
index d822bf6355ce..1bb7b72414b2 100644
--- a/Documentation/admin-guide/mm/damon/usage.rst
+++ b/Documentation/admin-guide/mm/damon/usage.rst
@@ -264,8 +264,6 @@ that can be written to and read from the file and their meaning are as below.
  - ``pageout``: Call ``madvise()`` for the region with ``MADV_PAGEOUT``
  - ``hugepage``: Call ``madvise()`` for the region with ``MADV_HUGEPAGE``
  - ``nohugepage``: Call ``madvise()`` for the region with ``MADV_NOHUGEPAGE``
- - ``lru_prio``: Prioritize the region on its LRU lists.
- - ``lru_deprio``: Deprioritize the region on its LRU lists.
  - ``stat``: Do nothing but count the statistics
 
 schemes/<N>/access_pattern/
diff --git a/include/linux/damon.h b/include/linux/damon.h
index 01748597f9ac..7c62da31ce4b 100644
--- a/include/linux/damon.h
+++ b/include/linux/damon.h
@@ -86,8 +86,6 @@ struct damon_target {
  * @DAMOS_PAGEOUT:	Call ``madvise()`` for the region with MADV_PAGEOUT.
  * @DAMOS_HUGEPAGE:	Call ``madvise()`` for the region with MADV_HUGEPAGE.
  * @DAMOS_NOHUGEPAGE:	Call ``madvise()`` for the region with MADV_NOHUGEPAGE.
- * @DAMOS_LRU_PRIO:	Prioritize the region on its LRU lists.
- * @DAMOS_LRU_DEPRIO:	Deprioritize the region on its LRU lists.
  * @DAMOS_STAT:		Do nothing but count the stat.
  * @NR_DAMOS_ACTIONS:	Total number of DAMOS actions
  */
@@ -97,8 +95,6 @@ enum damos_action {
 	DAMOS_PAGEOUT,
 	DAMOS_HUGEPAGE,
 	DAMOS_NOHUGEPAGE,
-	DAMOS_LRU_PRIO,
-	DAMOS_LRU_DEPRIO,
 	DAMOS_STAT,		/* Do nothing but only record the stat */
 	NR_DAMOS_ACTIONS,
 };
diff --git a/mm/damon/Kconfig b/mm/damon/Kconfig
index 66265e3a9c65..9b559c76d6dd 100644
--- a/mm/damon/Kconfig
+++ b/mm/damon/Kconfig
@@ -92,12 +92,4 @@ config DAMON_RECLAIM
 	  reclamation under light memory pressure, while the traditional page
 	  scanning-based reclamation is used for heavy pressure.
 
-config DAMON_LRU_SORT
-	bool "Build DAMON-based LRU-lists sorting (DAMON_LRU_SORT)"
-	depends on DAMON_PADDR
-	help
-	  This builds the DAMON-based LRU-lists sorting subsystem.  It tries to
-	  protect frequently accessed (hot) pages while rarely accessed (cold)
-	  pages reclaimed first under memory pressure.
-
 endmenu
diff --git a/mm/damon/Makefile b/mm/damon/Makefile
index 3e6b8ad73858..dbf7190b4144 100644
--- a/mm/damon/Makefile
+++ b/mm/damon/Makefile
@@ -6,4 +6,3 @@ obj-$(CONFIG_DAMON_PADDR)	+= ops-common.o paddr.o
 obj-$(CONFIG_DAMON_SYSFS)	+= sysfs.o
 obj-$(CONFIG_DAMON_DBGFS)	+= dbgfs.o
 obj-$(CONFIG_DAMON_RECLAIM)	+= reclaim.o
-obj-$(CONFIG_DAMON_LRU_SORT)	+= lru_sort.o
diff --git a/mm/damon/core-test.h b/mm/damon/core-test.h
index 45db79d28fdc..573669566f84 100644
--- a/mm/damon/core-test.h
+++ b/mm/damon/core-test.h
@@ -126,7 +126,7 @@ static void damon_test_split_at(struct kunit *test)
 	t = damon_new_target();
 	r = damon_new_region(0, 100);
 	damon_add_region(r, t);
-	damon_split_region_at(t, r, 25);
+	damon_split_region_at(c, t, r, 25);
 	KUNIT_EXPECT_EQ(test, r->ar.start, 0ul);
 	KUNIT_EXPECT_EQ(test, r->ar.end, 25ul);
 
@@ -219,14 +219,14 @@ static void damon_test_split_regions_of(struct kunit *test)
 	t = damon_new_target();
 	r = damon_new_region(0, 22);
 	damon_add_region(r, t);
-	damon_split_regions_of(t, 2);
+	damon_split_regions_of(c, t, 2);
 	KUNIT_EXPECT_LE(test, damon_nr_regions(t), 2u);
 	damon_free_target(t);
 
 	t = damon_new_target();
 	r = damon_new_region(0, 220);
 	damon_add_region(r, t);
-	damon_split_regions_of(t, 4);
+	damon_split_regions_of(c, t, 4);
 	KUNIT_EXPECT_LE(test, damon_nr_regions(t), 4u);
 	damon_free_target(t);
 	damon_destroy_ctx(c);
diff --git a/mm/damon/core.c b/mm/damon/core.c
index 9964b9d00768..7d25dc582fe3 100644
--- a/mm/damon/core.c
+++ b/mm/damon/core.c
@@ -658,8 +658,9 @@ static void kdamond_reset_aggregated(struct damon_ctx *c)
 	}
 }
 
-static void damon_split_region_at(struct damon_target *t,
-				  struct damon_region *r, unsigned long sz_r);
+static void damon_split_region_at(struct damon_ctx *ctx,
+		struct damon_target *t, struct damon_region *r,
+		unsigned long sz_r);
 
 static bool __damos_valid_target(struct damon_region *r, struct damos *s)
 {
@@ -725,7 +726,7 @@ static void damon_do_apply_schemes(struct damon_ctx *c,
 						continue;
 					sz = DAMON_MIN_REGION;
 				}
-				damon_split_region_at(t, r, sz);
+				damon_split_region_at(c, t, r, sz);
 				r = damon_next_region(r);
 				sz = r->ar.end - r->ar.start;
 			}
@@ -744,7 +745,7 @@ static void damon_do_apply_schemes(struct damon_ctx *c,
 						DAMON_MIN_REGION);
 				if (!sz)
 					goto update_stat;
-				damon_split_region_at(t, r, sz);
+				damon_split_region_at(c, t, r, sz);
 			}
 			ktime_get_coarse_ts64(&begin);
 			sz_applied = c->ops.apply_scheme(c, t, r, s);
@@ -927,8 +928,9 @@ static void kdamond_merge_regions(struct damon_ctx *c, unsigned int threshold,
  * r		the region to be split
  * sz_r		size of the first sub-region that will be made
  */
-static void damon_split_region_at(struct damon_target *t,
-				  struct damon_region *r, unsigned long sz_r)
+static void damon_split_region_at(struct damon_ctx *ctx,
+		struct damon_target *t, struct damon_region *r,
+		unsigned long sz_r)
 {
 	struct damon_region *new;
 
@@ -945,7 +947,8 @@ static void damon_split_region_at(struct damon_target *t,
 }
 
 /* Split every region in the given target into 'nr_subs' regions */
-static void damon_split_regions_of(struct damon_target *t, int nr_subs)
+static void damon_split_regions_of(struct damon_ctx *ctx,
+				     struct damon_target *t, int nr_subs)
 {
 	struct damon_region *r, *next;
 	unsigned long sz_region, sz_sub = 0;
@@ -966,7 +969,7 @@ static void damon_split_regions_of(struct damon_target *t, int nr_subs)
 			if (sz_sub == 0 || sz_sub >= sz_region)
 				continue;
 
-			damon_split_region_at(t, r, sz_sub);
+			damon_split_region_at(ctx, t, r, sz_sub);
 			sz_region = sz_sub;
 		}
 	}
@@ -1001,7 +1004,7 @@ static void kdamond_split_regions(struct damon_ctx *ctx)
 		nr_subregions = 3;
 
 	damon_for_each_target(t, ctx)
-		damon_split_regions_of(t, nr_subregions);
+		damon_split_regions_of(ctx, t, nr_subregions);
 
 	last_nr_regions = nr_regions;
 }
diff --git a/mm/damon/dbgfs.c b/mm/damon/dbgfs.c
index f36b33f99baa..53ba8b1e619c 100644
--- a/mm/damon/dbgfs.c
+++ b/mm/damon/dbgfs.c
@@ -97,31 +97,6 @@ static ssize_t dbgfs_attrs_write(struct file *file,
 	return ret;
 }
 
-/*
- * Return corresponding dbgfs' scheme action value (int) for the given
- * damos_action if the given damos_action value is valid and supported by
- * dbgfs, negative error code otherwise.
- */
-static int damos_action_to_dbgfs_scheme_action(enum damos_action action)
-{
-	switch (action) {
-	case DAMOS_WILLNEED:
-		return 0;
-	case DAMOS_COLD:
-		return 1;
-	case DAMOS_PAGEOUT:
-		return 2;
-	case DAMOS_HUGEPAGE:
-		return 3;
-	case DAMOS_NOHUGEPAGE:
-		return 4;
-	case DAMOS_STAT:
-		return 5;
-	default:
-		return -EINVAL;
-	}
-}
-
 static ssize_t sprint_schemes(struct damon_ctx *c, char *buf, ssize_t len)
 {
 	struct damos *s;
@@ -134,7 +109,7 @@ static ssize_t sprint_schemes(struct damon_ctx *c, char *buf, ssize_t len)
 				s->min_sz_region, s->max_sz_region,
 				s->min_nr_accesses, s->max_nr_accesses,
 				s->min_age_region, s->max_age_region,
-				damos_action_to_dbgfs_scheme_action(s->action),
+				s->action,
 				s->quota.ms, s->quota.sz,
 				s->quota.reset_interval,
 				s->quota.weight_sz,
@@ -185,27 +160,18 @@ static void free_schemes_arr(struct damos **schemes, ssize_t nr_schemes)
 	kfree(schemes);
 }
 
-/*
- * Return corresponding damos_action for the given dbgfs input for a scheme
- * action if the input is valid, negative error code otherwise.
- */
-static enum damos_action dbgfs_scheme_action_to_damos_action(int dbgfs_action)
+static bool damos_action_valid(int action)
 {
-	switch (dbgfs_action) {
-	case 0:
-		return DAMOS_WILLNEED;
-	case 1:
-		return DAMOS_COLD;
-	case 2:
-		return DAMOS_PAGEOUT;
-	case 3:
-		return DAMOS_HUGEPAGE;
-	case 4:
-		return DAMOS_NOHUGEPAGE;
-	case 5:
-		return DAMOS_STAT;
+	switch (action) {
+	case DAMOS_WILLNEED:
+	case DAMOS_COLD:
+	case DAMOS_PAGEOUT:
+	case DAMOS_HUGEPAGE:
+	case DAMOS_NOHUGEPAGE:
+	case DAMOS_STAT:
+		return true;
 	default:
-		return -EINVAL;
+		return false;
 	}
 }
 
@@ -223,8 +189,7 @@ static struct damos **str_to_schemes(const char *str, ssize_t len,
 	int pos = 0, parsed, ret;
 	unsigned long min_sz, max_sz;
 	unsigned int min_nr_a, max_nr_a, min_age, max_age;
-	unsigned int action_input;
-	enum damos_action action;
+	unsigned int action;
 
 	schemes = kmalloc_array(max_nr_schemes, sizeof(scheme),
 			GFP_KERNEL);
@@ -239,7 +204,7 @@ static struct damos **str_to_schemes(const char *str, ssize_t len,
 		ret = sscanf(&str[pos],
 				"%lu %lu %u %u %u %u %u %lu %lu %lu %u %u %u %u %lu %lu %lu %lu%n",
 				&min_sz, &max_sz, &min_nr_a, &max_nr_a,
-				&min_age, &max_age, &action_input, &quota.ms,
+				&min_age, &max_age, &action, &quota.ms,
 				&quota.sz, &quota.reset_interval,
 				&quota.weight_sz, &quota.weight_nr_accesses,
 				&quota.weight_age, &wmarks.metric,
@@ -247,8 +212,7 @@ static struct damos **str_to_schemes(const char *str, ssize_t len,
 				&wmarks.low, &parsed);
 		if (ret != 18)
 			break;
-		action = dbgfs_scheme_action_to_damos_action(action_input);
-		if ((int)action < 0)
+		if (!damos_action_valid(action))
 			goto fail;
 
 		if (min_sz > max_sz || min_nr_a > max_nr_a || min_age > max_age)
@@ -1049,7 +1013,7 @@ static int __init __damon_dbgfs_init(void)
 				fops[i]);
 	dbgfs_fill_ctx_dir(dbgfs_root, dbgfs_ctxs[0]);
 
-	dbgfs_dirs = kmalloc(sizeof(dbgfs_root), GFP_KERNEL);
+	dbgfs_dirs = kmalloc_array(1, sizeof(dbgfs_root), GFP_KERNEL);
 	if (!dbgfs_dirs) {
 		debugfs_remove(dbgfs_root);
 		return -ENOMEM;
diff --git a/mm/damon/lru_sort.c b/mm/damon/lru_sort.c
deleted file mode 100644
index a3674532fa67..000000000000
--- a/mm/damon/lru_sort.c
+++ /dev/null
@@ -1,541 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0
-/*
- * DAMON-based LRU-lists Sorting
- *
- * Author: SeongJae Park <sj@kernel.org>
- */
-
-#define pr_fmt(fmt) "damon-lru-sort: " fmt
-
-#include <linux/damon.h>
-#include <linux/ioport.h>
-#include <linux/module.h>
-#include <linux/sched.h>
-#include <linux/workqueue.h>
-
-#ifdef MODULE_PARAM_PREFIX
-#undef MODULE_PARAM_PREFIX
-#endif
-#define MODULE_PARAM_PREFIX "damon_lru_sort."
-
-/*
- * Enable or disable DAMON_LRU_SORT.
- *
- * You can enable DAMON_LRU_SORT by setting the value of this parameter as
- * ``Y``.  Setting it as ``N`` disables DAMON_LRU_SORT.  Note that
- * DAMON_LRU_SORT could do no real monitoring and LRU-lists sorting due to the
- * watermarks-based activation condition.  Refer to below descriptions for the
- * watermarks parameter for this.
- */
-static bool enabled __read_mostly;
-
-/*
- * Make DAMON_LRU_SORT reads the input parameters again, except ``enabled``.
- *
- * Input parameters that updated while DAMON_LRU_SORT is running are not
- * applied by default.  Once this parameter is set as ``Y``, DAMON_LRU_SORT
- * reads values of parametrs except ``enabled`` again.  Once the re-reading is
- * done, this parameter is set as ``N``.  If invalid parameters are found while
- * the re-reading, DAMON_LRU_SORT will be disabled.
- */
-static bool commit_inputs __read_mostly;
-module_param(commit_inputs, bool, 0600);
-
-/*
- * Access frequency threshold for hot memory regions identification in permil.
- *
- * If a memory region is accessed in frequency of this or higher,
- * DAMON_LRU_SORT identifies the region as hot, and mark it as accessed on the
- * LRU list, so that it could not be reclaimed under memory pressure.  50% by
- * default.
- */
-static unsigned long hot_thres_access_freq = 500;
-module_param(hot_thres_access_freq, ulong, 0600);
-
-/*
- * Time threshold for cold memory regions identification in microseconds.
- *
- * If a memory region is not accessed for this or longer time, DAMON_LRU_SORT
- * identifies the region as cold, and mark it as unaccessed on the LRU list, so
- * that it could be reclaimed first under memory pressure.  120 seconds by
- * default.
- */
-static unsigned long cold_min_age __read_mostly = 120000000;
-module_param(cold_min_age, ulong, 0600);
-
-/*
- * Limit of time for trying the LRU lists sorting in milliseconds.
- *
- * DAMON_LRU_SORT tries to use only up to this time within a time window
- * (quota_reset_interval_ms) for trying LRU lists sorting.  This can be used
- * for limiting CPU consumption of DAMON_LRU_SORT.  If the value is zero, the
- * limit is disabled.
- *
- * 10 ms by default.
- */
-static unsigned long quota_ms __read_mostly = 10;
-module_param(quota_ms, ulong, 0600);
-
-/*
- * The time quota charge reset interval in milliseconds.
- *
- * The charge reset interval for the quota of time (quota_ms).  That is,
- * DAMON_LRU_SORT does not try LRU-lists sorting for more than quota_ms
- * milliseconds or quota_sz bytes within quota_reset_interval_ms milliseconds.
- *
- * 1 second by default.
- */
-static unsigned long quota_reset_interval_ms __read_mostly = 1000;
-module_param(quota_reset_interval_ms, ulong, 0600);
-
-/*
- * The watermarks check time interval in microseconds.
- *
- * Minimal time to wait before checking the watermarks, when DAMON_LRU_SORT is
- * enabled but inactive due to its watermarks rule.  5 seconds by default.
- */
-static unsigned long wmarks_interval __read_mostly = 5000000;
-module_param(wmarks_interval, ulong, 0600);
-
-/*
- * Free memory rate (per thousand) for the high watermark.
- *
- * If free memory of the system in bytes per thousand bytes is higher than
- * this, DAMON_LRU_SORT becomes inactive, so it does nothing but periodically
- * checks the watermarks.  200 (20%) by default.
- */
-static unsigned long wmarks_high __read_mostly = 200;
-module_param(wmarks_high, ulong, 0600);
-
-/*
- * Free memory rate (per thousand) for the middle watermark.
- *
- * If free memory of the system in bytes per thousand bytes is between this and
- * the low watermark, DAMON_LRU_SORT becomes active, so starts the monitoring
- * and the LRU-lists sorting.  150 (15%) by default.
- */
-static unsigned long wmarks_mid __read_mostly = 150;
-module_param(wmarks_mid, ulong, 0600);
-
-/*
- * Free memory rate (per thousand) for the low watermark.
- *
- * If free memory of the system in bytes per thousand bytes is lower than this,
- * DAMON_LRU_SORT becomes inactive, so it does nothing but periodically checks
- * the watermarks.  50 (5%) by default.
- */
-static unsigned long wmarks_low __read_mostly = 50;
-module_param(wmarks_low, ulong, 0600);
-
-/*
- * Sampling interval for the monitoring in microseconds.
- *
- * The sampling interval of DAMON for the hot/cold memory monitoring.  Please
- * refer to the DAMON documentation for more detail.  5 ms by default.
- */
-static unsigned long sample_interval __read_mostly = 5000;
-module_param(sample_interval, ulong, 0600);
-
-/*
- * Aggregation interval for the monitoring in microseconds.
- *
- * The aggregation interval of DAMON for the hot/cold memory monitoring.
- * Please refer to the DAMON documentation for more detail.  100 ms by default.
- */
-static unsigned long aggr_interval __read_mostly = 100000;
-module_param(aggr_interval, ulong, 0600);
-
-/*
- * Minimum number of monitoring regions.
- *
- * The minimal number of monitoring regions of DAMON for the hot/cold memory
- * monitoring.  This can be used to set lower-bound of the monitoring quality.
- * But, setting this too high could result in increased monitoring overhead.
- * Please refer to the DAMON documentation for more detail.  10 by default.
- */
-static unsigned long min_nr_regions __read_mostly = 10;
-module_param(min_nr_regions, ulong, 0600);
-
-/*
- * Maximum number of monitoring regions.
- *
- * The maximum number of monitoring regions of DAMON for the hot/cold memory
- * monitoring.  This can be used to set upper-bound of the monitoring overhead.
- * However, setting this too low could result in bad monitoring quality.
- * Please refer to the DAMON documentation for more detail.  1000 by default.
- */
-static unsigned long max_nr_regions __read_mostly = 1000;
-module_param(max_nr_regions, ulong, 0600);
-
-/*
- * Start of the target memory region in physical address.
- *
- * The start physical address of memory region that DAMON_LRU_SORT will do work
- * against.  By default, biggest System RAM is used as the region.
- */
-static unsigned long monitor_region_start __read_mostly;
-module_param(monitor_region_start, ulong, 0600);
-
-/*
- * End of the target memory region in physical address.
- *
- * The end physical address of memory region that DAMON_LRU_SORT will do work
- * against.  By default, biggest System RAM is used as the region.
- */
-static unsigned long monitor_region_end __read_mostly;
-module_param(monitor_region_end, ulong, 0600);
-
-/*
- * PID of the DAMON thread
- *
- * If DAMON_LRU_SORT is enabled, this becomes the PID of the worker thread.
- * Else, -1.
- */
-static int kdamond_pid __read_mostly = -1;
-module_param(kdamond_pid, int, 0400);
-
-/*
- * Number of hot memory regions that tried to be LRU-sorted.
- */
-static unsigned long nr_lru_sort_tried_hot_regions __read_mostly;
-module_param(nr_lru_sort_tried_hot_regions, ulong, 0400);
-
-/*
- * Total bytes of hot memory regions that tried to be LRU-sorted.
- */
-static unsigned long bytes_lru_sort_tried_hot_regions __read_mostly;
-module_param(bytes_lru_sort_tried_hot_regions, ulong, 0400);
-
-/*
- * Number of hot memory regions that successfully be LRU-sorted.
- */
-static unsigned long nr_lru_sorted_hot_regions __read_mostly;
-module_param(nr_lru_sorted_hot_regions, ulong, 0400);
-
-/*
- * Total bytes of hot memory regions that successfully be LRU-sorted.
- */
-static unsigned long bytes_lru_sorted_hot_regions __read_mostly;
-module_param(bytes_lru_sorted_hot_regions, ulong, 0400);
-
-/*
- * Number of times that the time quota limit for hot regions have exceeded
- */
-static unsigned long nr_hot_quota_exceeds __read_mostly;
-module_param(nr_hot_quota_exceeds, ulong, 0400);
-
-/*
- * Number of cold memory regions that tried to be LRU-sorted.
- */
-static unsigned long nr_lru_sort_tried_cold_regions __read_mostly;
-module_param(nr_lru_sort_tried_cold_regions, ulong, 0400);
-
-/*
- * Total bytes of cold memory regions that tried to be LRU-sorted.
- */
-static unsigned long bytes_lru_sort_tried_cold_regions __read_mostly;
-module_param(bytes_lru_sort_tried_cold_regions, ulong, 0400);
-
-/*
- * Number of cold memory regions that successfully be LRU-sorted.
- */
-static unsigned long nr_lru_sorted_cold_regions __read_mostly;
-module_param(nr_lru_sorted_cold_regions, ulong, 0400);
-
-/*
- * Total bytes of cold memory regions that successfully be LRU-sorted.
- */
-static unsigned long bytes_lru_sorted_cold_regions __read_mostly;
-module_param(bytes_lru_sorted_cold_regions, ulong, 0400);
-
-/*
- * Number of times that the time quota limit for cold regions have exceeded
- */
-static unsigned long nr_cold_quota_exceeds __read_mostly;
-module_param(nr_cold_quota_exceeds, ulong, 0400);
-
-static struct damon_ctx *ctx;
-static struct damon_target *target;
-
-static int walk_system_ram(struct resource *res, void *arg)
-{
-	struct damon_addr_range *r = arg;
-
-	if (r->end - r->start < resource_size(res)) {
-		r->start = res->start;
-		r->end = res->end;
-	}
-	return 0;
-}
-
-/*
- * Find biggest 'System RAM' resource and store its start and end address in
- * @start and @end, respectively.  If no System RAM is found, returns false.
- */
-static bool get_monitoring_region(struct damon_addr_range *range)
-{
-	walk_system_ram_res(0, ULONG_MAX, range, walk_system_ram);
-	if (range->end <= range->start)
-		return false;
-
-	return true;
-}
-
-/* Create a DAMON-based operation scheme for hot memory regions */
-static struct damos *damon_lru_sort_new_hot_scheme(unsigned int hot_thres)
-{
-	struct damos_watermarks wmarks = {
-		.metric = DAMOS_WMARK_FREE_MEM_RATE,
-		.interval = wmarks_interval,
-		.high = wmarks_high,
-		.mid = wmarks_mid,
-		.low = wmarks_low,
-	};
-	struct damos_quota quota = {
-		/*
-		 * Do not try LRU-lists sorting of hot pages for more than half
-		 * of quota_ms milliseconds within quota_reset_interval_ms.
-		 */
-		.ms = quota_ms / 2,
-		.sz = 0,
-		.reset_interval = quota_reset_interval_ms,
-		/* Within the quota, mark hotter regions accessed first. */
-		.weight_sz = 0,
-		.weight_nr_accesses = 1,
-		.weight_age = 0,
-	};
-	struct damos *scheme = damon_new_scheme(
-			/* Find regions having PAGE_SIZE or larger size */
-			PAGE_SIZE, ULONG_MAX,
-			/* and accessed for more than the threshold */
-			hot_thres, UINT_MAX,
-			/* no matter its age */
-			0, UINT_MAX,
-			/* prioritize those on LRU lists, as soon as found */
-			DAMOS_LRU_PRIO,
-			/* under the quota. */
-			&quota,
-			/* (De)activate this according to the watermarks. */
-			&wmarks);
-
-	return scheme;
-}
-
-/* Create a DAMON-based operation scheme for cold memory regions */
-static struct damos *damon_lru_sort_new_cold_scheme(unsigned int cold_thres)
-{
-	struct damos_watermarks wmarks = {
-		.metric = DAMOS_WMARK_FREE_MEM_RATE,
-		.interval = wmarks_interval,
-		.high = wmarks_high,
-		.mid = wmarks_mid,
-		.low = wmarks_low,
-	};
-	struct damos_quota quota = {
-		/*
-		 * Do not try LRU-lists sorting of cold pages for more than
-		 * half of quota_ms milliseconds within
-		 * quota_reset_interval_ms.
-		 */
-		.ms = quota_ms / 2,
-		.sz = 0,
-		.reset_interval = quota_reset_interval_ms,
-		/* Within the quota, mark colder regions not accessed first. */
-		.weight_sz = 0,
-		.weight_nr_accesses = 0,
-		.weight_age = 1,
-	};
-	struct damos *scheme = damon_new_scheme(
-			/* Find regions having PAGE_SIZE or larger size */
-			PAGE_SIZE, ULONG_MAX,
-			/* and not accessed at all */
-			0, 0,
-			/* for cold_thres or more micro-seconds, and */
-			cold_thres, UINT_MAX,
-			/* mark those as not accessed, as soon as found */
-			DAMOS_LRU_DEPRIO,
-			/* under the quota. */
-			&quota,
-			/* (De)activate this according to the watermarks. */
-			&wmarks);
-
-	return scheme;
-}
-
-static int damon_lru_sort_apply_parameters(void)
-{
-	struct damos *scheme, *next_scheme;
-	struct damon_addr_range addr_range;
-	unsigned int hot_thres, cold_thres;
-	int err = 0;
-
-	if (monitor_region_start > monitor_region_end)
-		return -EINVAL;
-	if (!monitor_region_end)
-		return -EINVAL;
-
-	addr_range.start = monitor_region_start;
-	addr_range.end = monitor_region_end;
-	if (!get_monitoring_region(&addr_range))
-		return -EINVAL;
-
-	err = damon_set_attrs(ctx, sample_interval, aggr_interval, 0,
-			min_nr_regions, max_nr_regions);
-	if (err)
-		return err;
-
-	/* free previously set schemes */
-	damon_for_each_scheme_safe(scheme, next_scheme, ctx)
-		damon_destroy_scheme(scheme);
-
-	/* aggr_interval / sample_interval is the maximum nr_accesses */
-	hot_thres = aggr_interval / sample_interval * hot_thres_access_freq /
-		1000;
-	scheme = damon_lru_sort_new_hot_scheme(hot_thres);
-	if (!scheme)
-		return -ENOMEM;
-	damon_add_scheme(ctx, scheme);
-
-	cold_thres = cold_min_age / aggr_interval;
-	scheme = damon_lru_sort_new_cold_scheme(cold_thres);
-	if (!scheme)
-		return -ENOMEM;
-	damon_add_scheme(ctx, scheme);
-
-	return damon_set_regions(target, &addr_range, 1);
-}
-
-static int damon_lru_sort_turn(bool on)
-{
-	int err;
-
-	if (!on) {
-		err = damon_stop(&ctx, 1);
-		if (!err)
-			kdamond_pid = -1;
-		return err;
-	}
-
-	err = damon_lru_sort_apply_parameters();
-	if (err)
-		return err;
-
-	err = damon_start(&ctx, 1, true);
-	if (err)
-		return err;
-	kdamond_pid = ctx->kdamond->pid;
-	return 0;
-}
-
-static struct delayed_work damon_lru_sort_timer;
-static void damon_lru_sort_timer_fn(struct work_struct *work)
-{
-	static bool last_enabled;
-	bool now_enabled;
-
-	now_enabled = enabled;
-	if (last_enabled != now_enabled) {
-		if (!damon_lru_sort_turn(now_enabled))
-			last_enabled = now_enabled;
-		else
-			enabled = last_enabled;
-	}
-}
-static DECLARE_DELAYED_WORK(damon_lru_sort_timer, damon_lru_sort_timer_fn);
-
-static bool damon_lru_sort_initialized;
-
-static int damon_lru_sort_enabled_store(const char *val,
-		const struct kernel_param *kp)
-{
-	int rc = param_set_bool(val, kp);
-
-	if (rc < 0)
-		return rc;
-
-	if (!damon_lru_sort_initialized)
-		return rc;
-
-	schedule_delayed_work(&damon_lru_sort_timer, 0);
-
-	return 0;
-}
-
-static const struct kernel_param_ops enabled_param_ops = {
-	.set = damon_lru_sort_enabled_store,
-	.get = param_get_bool,
-};
-
-module_param_cb(enabled, &enabled_param_ops, &enabled, 0600);
-MODULE_PARM_DESC(enabled,
-	"Enable or disable DAMON_LRU_SORT (default: disabled)");
-
-static int damon_lru_sort_handle_commit_inputs(void)
-{
-	int err;
-
-	if (!commit_inputs)
-		return 0;
-
-	err = damon_lru_sort_apply_parameters();
-	commit_inputs = false;
-	return err;
-}
-
-static int damon_lru_sort_after_aggregation(struct damon_ctx *c)
-{
-	struct damos *s;
-
-	/* update the stats parameter */
-	damon_for_each_scheme(s, c) {
-		if (s->action == DAMOS_LRU_PRIO) {
-			nr_lru_sort_tried_hot_regions = s->stat.nr_tried;
-			bytes_lru_sort_tried_hot_regions = s->stat.sz_tried;
-			nr_lru_sorted_hot_regions = s->stat.nr_applied;
-			bytes_lru_sorted_hot_regions = s->stat.sz_applied;
-			nr_hot_quota_exceeds = s->stat.qt_exceeds;
-		} else if (s->action == DAMOS_LRU_DEPRIO) {
-			nr_lru_sort_tried_cold_regions = s->stat.nr_tried;
-			bytes_lru_sort_tried_cold_regions = s->stat.sz_tried;
-			nr_lru_sorted_cold_regions = s->stat.nr_applied;
-			bytes_lru_sorted_cold_regions = s->stat.sz_applied;
-			nr_cold_quota_exceeds = s->stat.qt_exceeds;
-		}
-	}
-
-	return damon_lru_sort_handle_commit_inputs();
-}
-
-static int damon_lru_sort_after_wmarks_check(struct damon_ctx *c)
-{
-	return damon_lru_sort_handle_commit_inputs();
-}
-
-static int __init damon_lru_sort_init(void)
-{
-	ctx = damon_new_ctx();
-	if (!ctx)
-		return -ENOMEM;
-
-	if (damon_select_ops(ctx, DAMON_OPS_PADDR)) {
-		damon_destroy_ctx(ctx);
-		return -EINVAL;
-	}
-
-	ctx->callback.after_wmarks_check = damon_lru_sort_after_wmarks_check;
-	ctx->callback.after_aggregation = damon_lru_sort_after_aggregation;
-
-	target = damon_new_target();
-	if (!target) {
-		damon_destroy_ctx(ctx);
-		return -ENOMEM;
-	}
-	damon_add_target(ctx, target);
-
-	schedule_delayed_work(&damon_lru_sort_timer, 0);
-
-	damon_lru_sort_initialized = true;
-	return 0;
-}
-
-module_init(damon_lru_sort_init);
diff --git a/mm/damon/ops-common.c b/mm/damon/ops-common.c
index b1335de200e7..10ef20b2003f 100644
--- a/mm/damon/ops-common.c
+++ b/mm/damon/ops-common.c
@@ -130,45 +130,3 @@ int damon_pageout_score(struct damon_ctx *c, struct damon_region *r,
 	/* Return coldness of the region */
 	return DAMOS_MAX_SCORE - hotness;
 }
-
-int damon_hot_score(struct damon_ctx *c, struct damon_region *r,
-			struct damos *s)
-{
-	unsigned int max_nr_accesses;
-	int freq_subscore;
-	unsigned int age_in_sec;
-	int age_in_log, age_subscore;
-	unsigned int freq_weight = s->quota.weight_nr_accesses;
-	unsigned int age_weight = s->quota.weight_age;
-	int hotness;
-
-	max_nr_accesses = c->aggr_interval / c->sample_interval;
-	freq_subscore = r->nr_accesses * DAMON_MAX_SUBSCORE / max_nr_accesses;
-
-	age_in_sec = (unsigned long)r->age * c->aggr_interval / 1000000;
-	for (age_in_log = 0; age_in_log < DAMON_MAX_AGE_IN_LOG && age_in_sec;
-			age_in_log++, age_in_sec >>= 1)
-		;
-
-	/* If frequency is 0, higher age means it's colder */
-	if (freq_subscore == 0)
-		age_in_log *= -1;
-
-	/*
-	 * Now age_in_log is in [-DAMON_MAX_AGE_IN_LOG, DAMON_MAX_AGE_IN_LOG].
-	 * Scale it to be in [0, 100] and set it as age subscore.
-	 */
-	age_in_log += DAMON_MAX_AGE_IN_LOG;
-	age_subscore = age_in_log * DAMON_MAX_SUBSCORE /
-		DAMON_MAX_AGE_IN_LOG / 2;
-
-	hotness = (freq_weight * freq_subscore + age_weight * age_subscore);
-	if (freq_weight + age_weight)
-		hotness /= freq_weight + age_weight;
-	/*
-	 * Transform it to fit in [0, DAMOS_MAX_SCORE]
-	 */
-	hotness = hotness * DAMOS_MAX_SCORE / DAMON_MAX_SUBSCORE;
-
-	return hotness;
-}
diff --git a/mm/damon/ops-common.h b/mm/damon/ops-common.h
index 52329ff361cd..e790cb5f8fe0 100644
--- a/mm/damon/ops-common.h
+++ b/mm/damon/ops-common.h
@@ -14,5 +14,3 @@ void damon_pmdp_mkold(pmd_t *pmd, struct mm_struct *mm, unsigned long addr);
 
 int damon_pageout_score(struct damon_ctx *c, struct damon_region *r,
 			struct damos *s);
-int damon_hot_score(struct damon_ctx *c, struct damon_region *r,
-			struct damos *s);
diff --git a/mm/damon/paddr.c b/mm/damon/paddr.c
index dc131c6a5403..b40ff5811bb2 100644
--- a/mm/damon/paddr.c
+++ b/mm/damon/paddr.c
@@ -204,11 +204,16 @@ static unsigned int damon_pa_check_accesses(struct damon_ctx *ctx)
 	return max_nr_accesses;
 }
 
-static unsigned long damon_pa_pageout(struct damon_region *r)
+static unsigned long damon_pa_apply_scheme(struct damon_ctx *ctx,
+		struct damon_target *t, struct damon_region *r,
+		struct damos *scheme)
 {
 	unsigned long addr, applied;
 	LIST_HEAD(page_list);
 
+	if (scheme->action != DAMOS_PAGEOUT)
+		return 0;
+
 	for (addr = r->ar.start; addr < r->ar.end; addr += PAGE_SIZE) {
 		struct page *page = damon_get_page(PHYS_PFN(addr));
 
@@ -233,55 +238,6 @@ static unsigned long damon_pa_pageout(struct damon_region *r)
 	return applied * PAGE_SIZE;
 }
 
-static unsigned long damon_pa_mark_accessed(struct damon_region *r)
-{
-	unsigned long addr, applied = 0;
-
-	for (addr = r->ar.start; addr < r->ar.end; addr += PAGE_SIZE) {
-		struct page *page = damon_get_page(PHYS_PFN(addr));
-
-		if (!page)
-			continue;
-		mark_page_accessed(page);
-		put_page(page);
-		applied++;
-	}
-	return applied * PAGE_SIZE;
-}
-
-static unsigned long damon_pa_deactivate_pages(struct damon_region *r)
-{
-	unsigned long addr, applied = 0;
-
-	for (addr = r->ar.start; addr < r->ar.end; addr += PAGE_SIZE) {
-		struct page *page = damon_get_page(PHYS_PFN(addr));
-
-		if (!page)
-			continue;
-		deactivate_page(page);
-		put_page(page);
-		applied++;
-	}
-	return applied * PAGE_SIZE;
-}
-
-static unsigned long damon_pa_apply_scheme(struct damon_ctx *ctx,
-		struct damon_target *t, struct damon_region *r,
-		struct damos *scheme)
-{
-	switch (scheme->action) {
-	case DAMOS_PAGEOUT:
-		return damon_pa_pageout(r);
-	case DAMOS_LRU_PRIO:
-		return damon_pa_mark_accessed(r);
-	case DAMOS_LRU_DEPRIO:
-		return damon_pa_deactivate_pages(r);
-	default:
-		break;
-	}
-	return 0;
-}
-
 static int damon_pa_scheme_score(struct damon_ctx *context,
 		struct damon_target *t, struct damon_region *r,
 		struct damos *scheme)
@@ -289,10 +245,6 @@ static int damon_pa_scheme_score(struct damon_ctx *context,
 	switch (scheme->action) {
 	case DAMOS_PAGEOUT:
 		return damon_pageout_score(context, r, scheme);
-	case DAMOS_LRU_PRIO:
-		return damon_hot_score(context, r, scheme);
-	case DAMOS_LRU_DEPRIO:
-		return damon_pageout_score(context, r, scheme);
 	default:
 		break;
 	}
diff --git a/mm/damon/reclaim.c b/mm/damon/reclaim.c
index d2d74200cd96..0b3c7396cb90 100644
--- a/mm/damon/reclaim.c
+++ b/mm/damon/reclaim.c
@@ -379,14 +379,13 @@ static bool damon_reclaim_initialized;
 static int enabled_store(const char *val,
 		const struct kernel_param *kp)
 {
-	int rc;
+	int rc = param_set_bool(val, kp);
+
+	if (rc < 0)
+		return rc;
 
 	/* system_wq might not initialized yet */
 	if (!damon_reclaim_initialized)
-		return -EINVAL;
-
-	rc = param_set_bool(val, kp);
-	if (rc < 0)
 		return rc;
 
 	if (enabled)
diff --git a/mm/damon/sysfs.c b/mm/damon/sysfs.c
index 0f65db55fa28..09f9e8ca3d1f 100644
--- a/mm/damon/sysfs.c
+++ b/mm/damon/sysfs.c
@@ -762,8 +762,6 @@ static const char * const damon_sysfs_damos_action_strs[] = {
 	"pageout",
 	"hugepage",
 	"nohugepage",
-	"lru_prio",
-	"lru_deprio",
 	"stat",
 };
 
diff --git a/mm/damon/vaddr-test.h b/mm/damon/vaddr-test.h
index bce37c487540..d4f55f349100 100644
--- a/mm/damon/vaddr-test.h
+++ b/mm/damon/vaddr-test.h
@@ -14,19 +14,33 @@
 
 #include <kunit/test.h>
 
-static void __link_vmas(struct maple_tree *mt, struct vm_area_struct *vmas,
-			ssize_t nr_vmas)
+static void __link_vmas(struct vm_area_struct *vmas, ssize_t nr_vmas)
 {
-	int i;
-	MA_STATE(mas, mt, 0, 0);
+	int i, j;
+	unsigned long largest_gap, gap;
 
 	if (!nr_vmas)
 		return;
 
-	mas_lock(&mas);
-	for (i = 0; i < nr_vmas; i++)
-		vma_mas_store(&vmas[i], &mas);
-	mas_unlock(&mas);
+	for (i = 0; i < nr_vmas - 1; i++) {
+		vmas[i].vm_next = &vmas[i + 1];
+
+		vmas[i].vm_rb.rb_left = NULL;
+		vmas[i].vm_rb.rb_right = &vmas[i + 1].vm_rb;
+
+		largest_gap = 0;
+		for (j = i; j < nr_vmas; j++) {
+			if (j == 0)
+				continue;
+			gap = vmas[j].vm_start - vmas[j - 1].vm_end;
+			if (gap > largest_gap)
+				largest_gap = gap;
+		}
+		vmas[i].rb_subtree_gap = largest_gap;
+	}
+	vmas[i].vm_next = NULL;
+	vmas[i].vm_rb.rb_right = NULL;
+	vmas[i].rb_subtree_gap = 0;
 }
 
 /*
@@ -58,7 +72,6 @@ static void __link_vmas(struct maple_tree *mt, struct vm_area_struct *vmas,
  */
 static void damon_test_three_regions_in_vmas(struct kunit *test)
 {
-	static struct mm_struct mm;
 	struct damon_addr_range regions[3] = {0,};
 	/* 10-20-25, 200-210-220, 300-305, 307-330 */
 	struct vm_area_struct vmas[] = {
@@ -70,10 +83,9 @@ static void damon_test_three_regions_in_vmas(struct kunit *test)
 		(struct vm_area_struct) {.vm_start = 307, .vm_end = 330},
 	};
 
-	mt_init_flags(&mm.mm_mt, MM_MT_FLAGS);
-	__link_vmas(&mm.mm_mt, vmas, ARRAY_SIZE(vmas));
+	__link_vmas(vmas, 6);
 
-	__damon_va_three_regions(&mm, regions);
+	__damon_va_three_regions(&vmas[0], regions);
 
 	KUNIT_EXPECT_EQ(test, 10ul, regions[0].start);
 	KUNIT_EXPECT_EQ(test, 25ul, regions[0].end);
diff --git a/mm/damon/vaddr.c b/mm/damon/vaddr.c
index 52c7799ecc1b..3c7b9d6dca95 100644
--- a/mm/damon/vaddr.c
+++ b/mm/damon/vaddr.c
@@ -113,38 +113,37 @@ static unsigned long sz_range(struct damon_addr_range *r)
  *
  * Returns 0 if success, or negative error code otherwise.
  */
-static int __damon_va_three_regions(struct mm_struct *mm,
+static int __damon_va_three_regions(struct vm_area_struct *vma,
 				       struct damon_addr_range regions[3])
 {
-	struct damon_addr_range first_gap = {0}, second_gap = {0};
-	VMA_ITERATOR(vmi, mm, 0);
-	struct vm_area_struct *vma, *prev = NULL;
-	unsigned long start;
-
-	/*
-	 * Find the two biggest gaps so that first_gap > second_gap > others.
-	 * If this is too slow, it can be optimised to examine the maple
-	 * tree gaps.
-	 */
-	for_each_vma(vmi, vma) {
-		unsigned long gap;
-
-		if (!prev) {
+	struct damon_addr_range gap = {0}, first_gap = {0}, second_gap = {0};
+	struct vm_area_struct *last_vma = NULL;
+	unsigned long start = 0;
+	struct rb_root rbroot;
+
+	/* Find two biggest gaps so that first_gap > second_gap > others */
+	for (; vma; vma = vma->vm_next) {
+		if (!last_vma) {
 			start = vma->vm_start;
 			goto next;
 		}
-		gap = vma->vm_start - prev->vm_end;
-
-		if (gap > sz_range(&first_gap)) {
-			second_gap = first_gap;
-			first_gap.start = prev->vm_end;
-			first_gap.end = vma->vm_start;
-		} else if (gap > sz_range(&second_gap)) {
-			second_gap.start = prev->vm_end;
-			second_gap.end = vma->vm_start;
+
+		if (vma->rb_subtree_gap <= sz_range(&second_gap)) {
+			rbroot.rb_node = &vma->vm_rb;
+			vma = rb_entry(rb_last(&rbroot),
+					struct vm_area_struct, vm_rb);
+			goto next;
+		}
+
+		gap.start = last_vma->vm_end;
+		gap.end = vma->vm_start;
+		if (sz_range(&gap) > sz_range(&second_gap)) {
+			swap(gap, second_gap);
+			if (sz_range(&second_gap) > sz_range(&first_gap))
+				swap(second_gap, first_gap);
 		}
 next:
-		prev = vma;
+		last_vma = vma;
 	}
 
 	if (!sz_range(&second_gap) || !sz_range(&first_gap))
@@ -160,7 +159,7 @@ static int __damon_va_three_regions(struct mm_struct *mm,
 	regions[1].start = ALIGN(first_gap.end, DAMON_MIN_REGION);
 	regions[1].end = ALIGN(second_gap.start, DAMON_MIN_REGION);
 	regions[2].start = ALIGN(second_gap.end, DAMON_MIN_REGION);
-	regions[2].end = ALIGN(prev->vm_end, DAMON_MIN_REGION);
+	regions[2].end = ALIGN(last_vma->vm_end, DAMON_MIN_REGION);
 
 	return 0;
 }
@@ -181,7 +180,7 @@ static int damon_va_three_regions(struct damon_target *t,
 		return -EINVAL;
 
 	mmap_read_lock(mm);
-	rc = __damon_va_three_regions(mm, regions);
+	rc = __damon_va_three_regions(mm->mmap, regions);
 	mmap_read_unlock(mm);
 
 	mmput(mm);
@@ -303,14 +302,9 @@ static int damon_mkold_pmd_entry(pmd_t *pmd, unsigned long addr,
 	pte_t *pte;
 	spinlock_t *ptl;
 
-	if (pmd_trans_huge(*pmd)) {
+	if (pmd_huge(*pmd)) {
 		ptl = pmd_lock(walk->mm, pmd);
-		if (!pmd_present(*pmd)) {
-			spin_unlock(ptl);
-			return 0;
-		}
-
-		if (pmd_trans_huge(*pmd)) {
+		if (pmd_huge(*pmd)) {
 			damon_pmdp_mkold(pmd, walk->mm, addr);
 			spin_unlock(ptl);
 			return 0;
@@ -435,14 +429,9 @@ static int damon_young_pmd_entry(pmd_t *pmd, unsigned long addr,
 	struct damon_young_walk_private *priv = walk->private;
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
-	if (pmd_trans_huge(*pmd)) {
+	if (pmd_huge(*pmd)) {
 		ptl = pmd_lock(walk->mm, pmd);
-		if (!pmd_present(*pmd)) {
-			spin_unlock(ptl);
-			return 0;
-		}
-
-		if (!pmd_trans_huge(*pmd)) {
+		if (!pmd_huge(*pmd)) {
 			spin_unlock(ptl);
 			goto regular_page;
 		}
-- 
2.37.3

