From 4105118d42579bd80ed520e384fc8d08dd21c3f3 Mon Sep 17 00:00:00 2001
From: Samuel Pitoiset <samuel.pitoiset@gmail.com>
Date: Tue, 6 Feb 2024 16:09:31 +0100
Subject: [PATCH 1/9] radv: get the pipeline layout info from the push constant
 token with DGC

They must be compatible and this allows us to remove some pipeline
references (useful for experimenting pipeline binds).

Signed-off-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 .../vulkan/radv_device_generated_commands.c   | 23 +++++++++++--------
 src/amd/vulkan/radv_private.h                 |  2 ++
 2 files changed, 16 insertions(+), 9 deletions(-)

diff --git a/src/amd/vulkan/radv_device_generated_commands.c b/src/amd/vulkan/radv_device_generated_commands.c
index eab1e9cd06c9..e9bb284aba59 100644
--- a/src/amd/vulkan/radv_device_generated_commands.c
+++ b/src/amd/vulkan/radv_device_generated_commands.c
@@ -130,8 +130,9 @@ radv_get_sequence_size(const struct radv_indirect_command_layout *layout, struct
             /* One PKT3_SET_SH_REG writing all inline push constants. */
             *cmd_size += (3 * util_bitcount64(layout->push_constant_mask)) * 4;
       }
-      if (need_copy)
-         *upload_size += align(pipeline->push_constant_size + 16 * pipeline->dynamic_offset_count, 16);
+      if (need_copy) {
+         *upload_size += align(layout->push_constant_size + 16 * layout->dynamic_offset_count, 16);
+      }
    }
 
    if (device->sqtt.bo) {
@@ -1499,13 +1500,17 @@ radv_CreateIndirectCommandsLayoutNV(VkDevice _device, const VkIndirectCommandsLa
          if (pCreateInfo->pTokens[i].vertexDynamicStride)
             layout->vbo_offsets[pCreateInfo->pTokens[i].vertexBindingUnit] |= DGC_DYNAMIC_STRIDE;
          break;
-      case VK_INDIRECT_COMMANDS_TOKEN_TYPE_PUSH_CONSTANT_NV:
+      case VK_INDIRECT_COMMANDS_TOKEN_TYPE_PUSH_CONSTANT_NV: {
+         RADV_FROM_HANDLE(radv_pipeline_layout, pipeline_layout, pCreateInfo->pTokens[i].pushconstantPipelineLayout);
          for (unsigned j = pCreateInfo->pTokens[i].pushconstantOffset / 4, k = 0;
               k < pCreateInfo->pTokens[i].pushconstantSize / 4; ++j, ++k) {
             layout->push_constant_mask |= 1ull << j;
             layout->push_constant_offsets[j] = pCreateInfo->pTokens[i].offset + k * 4;
          }
+         layout->push_constant_size = pipeline_layout->push_constant_size;
+         layout->dynamic_offset_count = pipeline_layout->dynamic_offset_count;
          break;
+      }
       case VK_INDIRECT_COMMANDS_TOKEN_TYPE_DRAW_MESH_TASKS_NV:
          layout->draw_mesh_tasks = true;
          layout->draw_params_offset = pCreateInfo->pTokens[i].offset;
@@ -1789,7 +1794,7 @@ radv_prepare_dgc(struct radv_cmd_buffer *cmd_buffer, const VkGeneratedCommandsIn
       .stream_addr = stream_addr,
    };
 
-   upload_size = pipeline->push_constant_size + 16 * pipeline->dynamic_offset_count +
+   upload_size = layout->push_constant_size + 16 * layout->dynamic_offset_count +
                  sizeof(layout->push_constant_offsets) + ARRAY_SIZE(pipeline->shaders) * 12;
    if (!layout->push_constant_mask)
       upload_size = 0;
@@ -1841,19 +1846,19 @@ radv_prepare_dgc(struct radv_cmd_buffer *cmd_buffer, const VkGeneratedCommandsIn
 
       params.push_constant_shader_cnt = idx;
 
-      params.const_copy_size = pipeline->push_constant_size + 16 * pipeline->dynamic_offset_count;
+      params.const_copy_size = layout->push_constant_size + 16 * layout->dynamic_offset_count;
       params.push_constant_mask = layout->push_constant_mask;
 
       memcpy(upload_data, layout->push_constant_offsets, sizeof(layout->push_constant_offsets));
       upload_data = (char *)upload_data + sizeof(layout->push_constant_offsets);
 
-      memcpy(upload_data, cmd_buffer->push_constants, pipeline->push_constant_size);
-      upload_data = (char *)upload_data + pipeline->push_constant_size;
+      memcpy(upload_data, cmd_buffer->push_constants, layout->push_constant_size);
+      upload_data = (char *)upload_data + layout->push_constant_size;
 
       struct radv_descriptor_state *descriptors_state =
          radv_get_descriptors_state(cmd_buffer, pGeneratedCommandsInfo->pipelineBindPoint);
-      memcpy(upload_data, descriptors_state->dynamic_buffers, 16 * pipeline->dynamic_offset_count);
-      upload_data = (char *)upload_data + 16 * pipeline->dynamic_offset_count;
+      memcpy(upload_data, descriptors_state->dynamic_buffers, 16 * layout->dynamic_offset_count);
+      upload_data = (char *)upload_data + 16 * layout->dynamic_offset_count;
    }
 
    radv_buffer_init(&token_buffer, cmd_buffer->device, cmd_buffer->upload.upload_bo, upload_size, upload_offset);
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index bd2830df3615..b0fd9fb539a2 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -3254,6 +3254,8 @@ struct radv_indirect_command_layout {
 
    uint64_t push_constant_mask;
    uint32_t push_constant_offsets[MAX_PUSH_CONSTANTS_SIZE / 4];
+   uint32_t push_constant_size;
+   uint32_t dynamic_offset_count;
 
    uint32_t ibo_type_32;
    uint32_t ibo_type_8;
-- 
GitLab


From 8104727409d593bc1a7f9bb014c32da910acdbf7 Mon Sep 17 00:00:00 2001
From: Samuel Pitoiset <samuel.pitoiset@gmail.com>
Date: Tue, 6 Feb 2024 18:17:12 +0100
Subject: [PATCH 2/9] radv: add a helper to calculate the compute resource
 limits

Signed-off-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 src/amd/vulkan/radv_pipeline_compute.c | 35 +++++++++++++++-----------
 1 file changed, 20 insertions(+), 15 deletions(-)

diff --git a/src/amd/vulkan/radv_pipeline_compute.c b/src/amd/vulkan/radv_pipeline_compute.c
index 71322281a6a3..68a4e6eeecd9 100644
--- a/src/amd/vulkan/radv_pipeline_compute.c
+++ b/src/amd/vulkan/radv_pipeline_compute.c
@@ -53,16 +53,31 @@
 #include "sid.h"
 #include "vk_format.h"
 
-void
-radv_emit_compute_shader(const struct radv_physical_device *pdevice, struct radeon_cmdbuf *cs,
-                         const struct radv_shader *shader)
+static uint32_t
+radv_get_compute_resource_limits(const struct radv_physical_device *pdevice, const struct radv_shader *cs)
 {
-   uint64_t va = radv_shader_get_va(shader);
    unsigned threads_per_threadgroup;
    unsigned threadgroups_per_cu = 1;
    unsigned waves_per_threadgroup;
    unsigned max_waves_per_sh = 0;
 
+   /* Calculate best compute resource limits. */
+   threads_per_threadgroup = cs->info.cs.block_size[0] * cs->info.cs.block_size[1] * cs->info.cs.block_size[2];
+   waves_per_threadgroup = DIV_ROUND_UP(threads_per_threadgroup, cs->info.wave_size);
+
+   if (pdevice->rad_info.gfx_level >= GFX10 && waves_per_threadgroup == 1)
+      threadgroups_per_cu = 2;
+
+   return ac_get_compute_resource_limits(&pdevice->rad_info, waves_per_threadgroup, max_waves_per_sh,
+                                         threadgroups_per_cu);
+}
+
+void
+radv_emit_compute_shader(const struct radv_physical_device *pdevice, struct radeon_cmdbuf *cs,
+                         const struct radv_shader *shader)
+{
+   uint64_t va = radv_shader_get_va(shader);
+
    radeon_set_sh_reg(cs, R_00B830_COMPUTE_PGM_LO, va >> 8);
 
    radeon_set_sh_reg_seq(cs, R_00B848_COMPUTE_PGM_RSRC1, 2);
@@ -72,17 +87,7 @@ radv_emit_compute_shader(const struct radv_physical_device *pdevice, struct rade
       radeon_set_sh_reg(cs, R_00B8A0_COMPUTE_PGM_RSRC3, shader->config.rsrc3);
    }
 
-   /* Calculate best compute resource limits. */
-   threads_per_threadgroup =
-      shader->info.cs.block_size[0] * shader->info.cs.block_size[1] * shader->info.cs.block_size[2];
-   waves_per_threadgroup = DIV_ROUND_UP(threads_per_threadgroup, shader->info.wave_size);
-
-   if (pdevice->rad_info.gfx_level >= GFX10 && waves_per_threadgroup == 1)
-      threadgroups_per_cu = 2;
-
-   radeon_set_sh_reg(
-      cs, R_00B854_COMPUTE_RESOURCE_LIMITS,
-      ac_get_compute_resource_limits(&pdevice->rad_info, waves_per_threadgroup, max_waves_per_sh, threadgroups_per_cu));
+   radeon_set_sh_reg(cs, R_00B854_COMPUTE_RESOURCE_LIMITS, radv_get_compute_resource_limits(pdevice, shader));
 
    radeon_set_sh_reg_seq(cs, R_00B81C_COMPUTE_NUM_THREAD_X, 3);
    radeon_emit(cs, S_00B81C_NUM_THREAD_FULL(shader->info.cs.block_size[0]));
-- 
GitLab


From b2a847950fbbbe6403353e1592b0d7350411fc57 Mon Sep 17 00:00:00 2001
From: Samuel Pitoiset <samuel.pitoiset@gmail.com>
Date: Wed, 7 Feb 2024 08:18:59 +0100
Subject: [PATCH 3/9] radv: add a helper to update the scratch info for cmdbuf

Signed-off-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 src/amd/vulkan/radv_cmd_buffer.c | 39 ++++++++++++++++++--------------
 1 file changed, 22 insertions(+), 17 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index d2ddd87e6894..bd44bdc562ad 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -6578,12 +6578,30 @@ radv_bind_task_shader(struct radv_cmd_buffer *cmd_buffer, const struct radv_shad
    cmd_buffer->task_rings_needed = true;
 }
 
-/* This function binds/unbinds a shader to the cmdbuffer state. */
 static void
-radv_bind_shader(struct radv_cmd_buffer *cmd_buffer, struct radv_shader *shader, gl_shader_stage stage)
+radv_update_scratch_info(struct radv_cmd_buffer *cmd_buffer, struct radv_shader *shader)
 {
    const struct radv_device *device = cmd_buffer->device;
 
+   if (mesa_to_vk_shader_stage(shader->info.stage) & RADV_GRAPHICS_STAGE_BITS) {
+      cmd_buffer->scratch_size_per_wave_needed =
+         MAX2(cmd_buffer->scratch_size_per_wave_needed, shader->config.scratch_bytes_per_wave);
+
+      const unsigned max_stage_waves = radv_get_max_scratch_waves(device, shader);
+      cmd_buffer->scratch_waves_wanted = MAX2(cmd_buffer->scratch_waves_wanted, max_stage_waves);
+   } else {
+      cmd_buffer->compute_scratch_size_per_wave_needed =
+         MAX2(cmd_buffer->compute_scratch_size_per_wave_needed, shader->config.scratch_bytes_per_wave);
+
+      const unsigned max_stage_waves = radv_get_max_scratch_waves(device, shader);
+      cmd_buffer->compute_scratch_waves_wanted = MAX2(cmd_buffer->compute_scratch_waves_wanted, max_stage_waves);
+   }
+}
+
+/* This function binds/unbinds a shader to the cmdbuffer state. */
+static void
+radv_bind_shader(struct radv_cmd_buffer *cmd_buffer, struct radv_shader *shader, gl_shader_stage stage)
+{
    if (!shader) {
       cmd_buffer->state.shaders[stage] = NULL;
       cmd_buffer->state.active_stages &= ~mesa_to_vk_shader_stage(stage);
@@ -6623,14 +6641,7 @@ radv_bind_shader(struct radv_cmd_buffer *cmd_buffer, struct radv_shader *shader,
    case MESA_SHADER_TASK:
       radv_bind_task_shader(cmd_buffer, shader);
       break;
-   case MESA_SHADER_COMPUTE: {
-      cmd_buffer->compute_scratch_size_per_wave_needed =
-         MAX2(cmd_buffer->compute_scratch_size_per_wave_needed, shader->config.scratch_bytes_per_wave);
-
-      const unsigned max_stage_waves = radv_get_max_scratch_waves(device, shader);
-      cmd_buffer->compute_scratch_waves_wanted = MAX2(cmd_buffer->compute_scratch_waves_wanted, max_stage_waves);
-      break;
-   }
+   case MESA_SHADER_COMPUTE:
    case MESA_SHADER_INTERSECTION:
       /* no-op */
       break;
@@ -6641,13 +6652,7 @@ radv_bind_shader(struct radv_cmd_buffer *cmd_buffer, struct radv_shader *shader,
    cmd_buffer->state.shaders[stage] = shader;
    cmd_buffer->state.active_stages |= mesa_to_vk_shader_stage(stage);
 
-   if (mesa_to_vk_shader_stage(stage) & RADV_GRAPHICS_STAGE_BITS) {
-      cmd_buffer->scratch_size_per_wave_needed =
-         MAX2(cmd_buffer->scratch_size_per_wave_needed, shader->config.scratch_bytes_per_wave);
-
-      const unsigned max_stage_waves = radv_get_max_scratch_waves(device, shader);
-      cmd_buffer->scratch_waves_wanted = MAX2(cmd_buffer->scratch_waves_wanted, max_stage_waves);
-   }
+   radv_update_scratch_info(cmd_buffer, shader);
 }
 
 static void
-- 
GitLab


From 22d5e6ed7d0056418b41f44546821209acc73483 Mon Sep 17 00:00:00 2001
From: Samuel Pitoiset <samuel.pitoiset@gmail.com>
Date: Tue, 6 Feb 2024 18:20:14 +0100
Subject: [PATCH 4/9] radv: add a function to get compute pipeline metadata for
 DGC

This struct will be used to emit a compute pipeline from the prepare
DGC shader.

Signed-off-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 src/amd/vulkan/radv_pipeline_compute.c | 38 ++++++++++++++++++++++++++
 src/amd/vulkan/radv_private.h          | 18 ++++++++++++
 2 files changed, 56 insertions(+)

diff --git a/src/amd/vulkan/radv_pipeline_compute.c b/src/amd/vulkan/radv_pipeline_compute.c
index 68a4e6eeecd9..7fe48f9716d0 100644
--- a/src/amd/vulkan/radv_pipeline_compute.c
+++ b/src/amd/vulkan/radv_pipeline_compute.c
@@ -72,6 +72,44 @@ radv_get_compute_resource_limits(const struct radv_physical_device *pdevice, con
                                          threadgroups_per_cu);
 }
 
+void
+radv_get_compute_pipeline_metadata(const struct radv_device *device, const struct radv_compute_pipeline *pipeline,
+                                   struct radv_compute_pipeline_metadata *metadata)
+{
+   const struct radv_shader *cs = pipeline->base.shaders[MESA_SHADER_COMPUTE];
+   uint32_t upload_sgpr = 0, inline_sgpr = 0;
+
+   memset(metadata, 0, sizeof(*metadata));
+
+   metadata->shader_va = radv_shader_get_va(cs) >> 8;
+   metadata->rsrc1 = cs->config.rsrc1;
+   metadata->rsrc2 = cs->config.rsrc2;
+   metadata->rsrc3 = cs->config.rsrc3;
+   metadata->compute_resource_limits = radv_get_compute_resource_limits(device->physical_device, cs);
+   metadata->block_size_x = cs->info.cs.block_size[0];
+   metadata->block_size_y = cs->info.cs.block_size[1];
+   metadata->block_size_z = cs->info.cs.block_size[2];
+   metadata->wave32 = cs->info.wave_size == 32;
+
+   const struct radv_userdata_info *grid_size_loc = radv_get_user_sgpr(cs, AC_UD_CS_GRID_SIZE);
+   if (grid_size_loc->sgpr_idx != -1) {
+      metadata->grid_base_sgpr = (cs->info.user_data_0 + 4 * grid_size_loc->sgpr_idx - SI_SH_REG_OFFSET) >> 2;
+   }
+
+   const struct radv_userdata_info *push_constant_loc = radv_get_user_sgpr(cs, AC_UD_PUSH_CONSTANTS);
+   if (push_constant_loc->sgpr_idx != -1) {
+      upload_sgpr = (cs->info.user_data_0 + 4 * push_constant_loc->sgpr_idx - SI_SH_REG_OFFSET) >> 2;
+   }
+
+   const struct radv_userdata_info *inline_push_constant_loc = radv_get_user_sgpr(cs, AC_UD_INLINE_PUSH_CONSTANTS);
+   if (inline_push_constant_loc->sgpr_idx != -1) {
+      inline_sgpr = (cs->info.user_data_0 + 4 * inline_push_constant_loc->sgpr_idx - SI_SH_REG_OFFSET) >> 2;
+   }
+
+   metadata->push_const_sgpr = upload_sgpr | (inline_sgpr << 16);
+   metadata->inline_push_const_mask = cs->info.inline_push_constant_mask;
+}
+
 void
 radv_emit_compute_shader(const struct radv_physical_device *pdevice, struct radeon_cmdbuf *cs,
                          const struct radv_shader *shader)
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index b0fd9fb539a2..c787c38d5531 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -3744,6 +3744,24 @@ radv_uses_image_float32_atomics(const struct radv_device *device)
 
 bool radv_device_fault_detection_enabled(const struct radv_device *device);
 
+struct radv_compute_pipeline_metadata {
+   uint32_t shader_va;
+   uint32_t rsrc1;
+   uint32_t rsrc2;
+   uint32_t rsrc3;
+   uint32_t compute_resource_limits;
+   uint32_t block_size_x;
+   uint32_t block_size_y;
+   uint32_t block_size_z;
+   uint32_t wave32;
+   uint32_t grid_base_sgpr;
+   uint32_t push_const_sgpr;
+   uint64_t inline_push_const_mask;
+};
+
+void radv_get_compute_pipeline_metadata(const struct radv_device *device, const struct radv_compute_pipeline *pipeline,
+                                        struct radv_compute_pipeline_metadata *metadata);
+
 #define RADV_FROM_HANDLE(__radv_type, __name, __handle) VK_FROM_HANDLE(__radv_type, __name, __handle)
 
 VK_DEFINE_HANDLE_CASTS(radv_cmd_buffer, vk.base, VkCommandBuffer, VK_OBJECT_TYPE_COMMAND_BUFFER)
-- 
GitLab


From 2410e818fd073f922861aa825321d8426cd5b399 Mon Sep 17 00:00:00 2001
From: Samuel Pitoiset <samuel.pitoiset@gmail.com>
Date: Tue, 6 Feb 2024 18:21:45 +0100
Subject: [PATCH 5/9] radv: add support for
 VK_PIPELINE_CREATE_INDIRECT_BINDABLE_BIT_NV

This stores the VA/size of the indirect buffer to be used in
vkCmdUpdatePipelineIndirectBufferNV.

Signed-off-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 src/amd/vulkan/radv_pipeline_compute.c | 8 ++++++++
 src/amd/vulkan/radv_private.h          | 5 +++++
 2 files changed, 13 insertions(+)

diff --git a/src/amd/vulkan/radv_pipeline_compute.c b/src/amd/vulkan/radv_pipeline_compute.c
index 7fe48f9716d0..a6860d6ff977 100644
--- a/src/amd/vulkan/radv_pipeline_compute.c
+++ b/src/amd/vulkan/radv_pipeline_compute.c
@@ -308,6 +308,14 @@ radv_compute_pipeline_create(VkDevice _device, VkPipelineCache _cache, const VkC
    struct radv_shader_stage_key stage_key =
       radv_pipeline_get_shader_key(device, &pCreateInfo->stage, pipeline->base.create_flags, pCreateInfo->pNext);
 
+   if (pipeline->base.create_flags & VK_PIPELINE_CREATE_INDIRECT_BINDABLE_BIT_NV) {
+      const VkComputePipelineIndirectBufferInfoNV *indirect_buffer =
+         vk_find_struct_const(pCreateInfo->pNext, COMPUTE_PIPELINE_INDIRECT_BUFFER_INFO_NV);
+
+      pipeline->indirect.va = indirect_buffer->deviceAddress;
+      pipeline->indirect.size = indirect_buffer->size;
+   }
+
    result = radv_compute_pipeline_compile(pipeline, pipeline_layout, device, cache, &stage_key, &pCreateInfo->stage,
                                           creation_feedback);
    if (result != VK_SUCCESS) {
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index c787c38d5531..8708735ea3ce 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -2315,6 +2315,11 @@ struct radv_graphics_pipeline {
 
 struct radv_compute_pipeline {
    struct radv_pipeline base;
+
+   struct {
+      uint64_t va;
+      uint64_t size;
+   } indirect;
 };
 
 struct radv_ray_tracing_group {
-- 
GitLab


From 5f9864115446679464c27a456ee491fb357ca0ca Mon Sep 17 00:00:00 2001
From: Samuel Pitoiset <samuel.pitoiset@gmail.com>
Date: Tue, 6 Feb 2024 18:27:15 +0100
Subject: [PATCH 6/9] radv: implement vkGetPipelineIndirectXXX() for DGC

Signed-off-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 src/amd/vulkan/radv_device_generated_commands.c | 15 ++++++++++++---
 1 file changed, 12 insertions(+), 3 deletions(-)

diff --git a/src/amd/vulkan/radv_device_generated_commands.c b/src/amd/vulkan/radv_device_generated_commands.c
index e9bb284aba59..ab2fb0eb9e18 100644
--- a/src/amd/vulkan/radv_device_generated_commands.c
+++ b/src/amd/vulkan/radv_device_generated_commands.c
@@ -1936,14 +1936,23 @@ radv_prepare_dgc(struct radv_cmd_buffer *cmd_buffer, const VkGeneratedCommandsIn
 
 /* VK_NV_device_generated_commands_compute */
 VKAPI_ATTR void VKAPI_CALL
-radv_GetPipelineIndirectMemoryRequirementsNV(VkDevice device, const VkComputePipelineCreateInfo *pCreateInfo,
+radv_GetPipelineIndirectMemoryRequirementsNV(VkDevice _device, const VkComputePipelineCreateInfo *pCreateInfo,
                                              VkMemoryRequirements2 *pMemoryRequirements)
 {
-   unreachable("radv: unimplemented vkGetPipelineIndirectMemoryRequirementsNV");
+   VkMemoryRequirements *reqs = &pMemoryRequirements->memoryRequirements;
+   const uint32_t size = sizeof(struct radv_compute_pipeline_metadata);
+   RADV_FROM_HANDLE(radv_device, device, _device);
+
+   reqs->memoryTypeBits = ((1u << device->physical_device->memory_properties.memoryTypeCount) - 1u) &
+                          ~device->physical_device->memory_types_32bit;
+   reqs->alignment = 4;
+   reqs->size = align(size, reqs->alignment);
 }
 
 VKAPI_ATTR VkDeviceAddress VKAPI_CALL
 radv_GetPipelineIndirectDeviceAddressNV(VkDevice device, const VkPipelineIndirectDeviceAddressInfoNV *pInfo)
 {
-   unreachable("radv: unimplemented vkGetPipelineIndirectDeviceAddressNV");
+   RADV_FROM_HANDLE(radv_pipeline, pipeline, pInfo->pipeline);
+
+   return radv_pipeline_to_compute(pipeline)->indirect.va;
 }
-- 
GitLab


From 6e6747b98db1d8fe609ebbca37d6894a4527ecb8 Mon Sep 17 00:00:00 2001
From: Samuel Pitoiset <samuel.pitoiset@gmail.com>
Date: Tue, 6 Feb 2024 18:27:49 +0100
Subject: [PATCH 7/9] radv: implement vkCmdUpdatePipelineIndirectBufferNV()

Signed-off-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 src/amd/vulkan/radv_cmd_buffer.c | 18 ++++++++++++++++--
 1 file changed, 16 insertions(+), 2 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index bd44bdc562ad..3d120943b382 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -11734,9 +11734,23 @@ radv_CmdBindPipelineShaderGroupNV(VkCommandBuffer commandBuffer, VkPipelineBindP
 /* VK_NV_device_generated_commands_compute */
 VKAPI_ATTR void VKAPI_CALL
 radv_CmdUpdatePipelineIndirectBufferNV(VkCommandBuffer commandBuffer, VkPipelineBindPoint pipelineBindPoint,
-                                       VkPipeline pipeline)
+                                       VkPipeline _pipeline)
 {
-   unreachable("radv: unimplemented vkCmdUpdatePipelineIndirectBufferNV");
+   RADV_FROM_HANDLE(radv_cmd_buffer, cmd_buffer, commandBuffer);
+   RADV_FROM_HANDLE(radv_pipeline, pipeline, _pipeline);
+   const struct radv_compute_pipeline *compute_pipeline = radv_pipeline_to_compute(pipeline);
+   struct radv_shader *cs = compute_pipeline->base.shaders[MESA_SHADER_COMPUTE];
+   const uint64_t va = compute_pipeline->indirect.va;
+   struct radv_compute_pipeline_metadata metadata;
+
+   radv_get_compute_pipeline_metadata(cmd_buffer->device, compute_pipeline, &metadata);
+
+   assert(sizeof(metadata) <= compute_pipeline->indirect.size);
+   radv_write_data(cmd_buffer, V_370_ME, va, sizeof(metadata) / 4, (const uint32_t *)&metadata, false);
+
+   radv_cs_add_buffer(cmd_buffer->device->ws, cmd_buffer->cs, cs->bo);
+
+   radv_update_scratch_info(cmd_buffer, cs);
 }
 
 /* VK_EXT_descriptor_buffer */
-- 
GitLab


From 37e6b042bedbc829825ca28d6a23fe9d333fa67b Mon Sep 17 00:00:00 2001
From: Samuel Pitoiset <samuel.pitoiset@gmail.com>
Date: Tue, 6 Feb 2024 18:33:49 +0100
Subject: [PATCH 8/9] radv: implement indirect compute pipeline binds with DGC

This also supports push constants.

Signed-off-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 src/amd/vulkan/radv_cmd_buffer.c              |   3 +-
 .../vulkan/radv_device_generated_commands.c   | 382 +++++++++++++++---
 src/amd/vulkan/radv_private.h                 |   3 +
 3 files changed, 320 insertions(+), 68 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index 3d120943b382..405d41f816b4 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -10295,7 +10295,8 @@ radv_dgc_before_dispatch(struct radv_cmd_buffer *cmd_buffer)
    if (compute_shader->info.cs.regalloc_hang_bug)
       cmd_buffer->state.flush_bits |= RADV_CMD_FLAG_PS_PARTIAL_FLUSH | RADV_CMD_FLAG_CS_PARTIAL_FLUSH;
 
-   radv_emit_compute_pipeline(cmd_buffer, pipeline);
+   if (pipeline)
+      radv_emit_compute_pipeline(cmd_buffer, pipeline);
    radv_emit_cache_flush(cmd_buffer);
 
    radv_upload_compute_shader_descriptors(cmd_buffer, VK_PIPELINE_BIND_POINT_COMPUTE);
diff --git a/src/amd/vulkan/radv_device_generated_commands.c b/src/amd/vulkan/radv_device_generated_commands.c
index ab2fb0eb9e18..22e95f18f915 100644
--- a/src/amd/vulkan/radv_device_generated_commands.c
+++ b/src/amd/vulkan/radv_device_generated_commands.c
@@ -35,13 +35,37 @@ radv_get_sequence_size_compute(const struct radv_indirect_command_layout *layout
                                const struct radv_compute_pipeline *pipeline, uint32_t *cmd_size)
 {
    const struct radv_device *device = container_of(layout->base.device, struct radv_device, vk);
-   struct radv_shader *cs = radv_get_shader(pipeline->base.shaders, MESA_SHADER_COMPUTE);
 
    /* dispatch */
    *cmd_size += 5 * 4;
 
-   const struct radv_userdata_info *loc = radv_get_user_sgpr(cs, AC_UD_CS_GRID_SIZE);
-   if (loc->sgpr_idx != -1) {
+   if (pipeline) {
+      struct radv_shader *cs = radv_get_shader(pipeline->base.shaders, MESA_SHADER_COMPUTE);
+      const struct radv_userdata_info *loc = radv_get_user_sgpr(cs, AC_UD_CS_GRID_SIZE);
+      if (loc->sgpr_idx != -1) {
+         if (device->load_grid_size_from_user_sgpr) {
+            /* PKT3_SET_SH_REG for immediate values */
+            *cmd_size += 5 * 4;
+         } else {
+            /* PKT3_SET_SH_REG for pointer */
+            *cmd_size += 4 * 4;
+         }
+      }
+   } else {
+      /* COMPUTE_PGM_{LO,RSRC1,RSRC2}*/
+      *cmd_size += 7 * 4;
+
+      if (device->physical_device->rad_info.gfx_level >= GFX10) {
+         /* COMPUTE_PGM_RSRC3 */
+         *cmd_size += 3 * 4;
+      }
+
+      /* COMPUTE_{RESOURCE_LIMITS,NUM_THREADS_X} */
+      *cmd_size += 8 * 4;
+
+      /* Assume the compute shader needs grid size because we can't know the information for
+       * indirect pipelines.
+       */
       if (device->load_grid_size_from_user_sgpr) {
          /* PKT3_SET_SH_REG for immediate values */
          *cmd_size += 5 * 4;
@@ -116,20 +140,32 @@ radv_get_sequence_size(const struct radv_indirect_command_layout *layout, struct
    if (layout->push_constant_mask) {
       bool need_copy = false;
 
-      for (unsigned i = 0; i < ARRAY_SIZE(pipeline->shaders); ++i) {
-         if (!pipeline->shaders[i])
-            continue;
+      if (pipeline) {
+         for (unsigned i = 0; i < ARRAY_SIZE(pipeline->shaders); ++i) {
+            if (!pipeline->shaders[i])
+               continue;
 
-         struct radv_userdata_locations *locs = &pipeline->shaders[i]->info.user_sgprs_locs;
-         if (locs->shader_data[AC_UD_PUSH_CONSTANTS].sgpr_idx >= 0) {
-            /* One PKT3_SET_SH_REG for emitting push constants pointer (32-bit) */
-            *cmd_size += 3 * 4;
-            need_copy = true;
+            struct radv_userdata_locations *locs = &pipeline->shaders[i]->info.user_sgprs_locs;
+            if (locs->shader_data[AC_UD_PUSH_CONSTANTS].sgpr_idx >= 0) {
+               /* One PKT3_SET_SH_REG for emitting push constants pointer (32-bit) */
+               *cmd_size += 3 * 4;
+               need_copy = true;
+            }
+            if (locs->shader_data[AC_UD_INLINE_PUSH_CONSTANTS].sgpr_idx >= 0)
+               /* One PKT3_SET_SH_REG writing all inline push constants. */
+               *cmd_size += (3 * util_bitcount64(layout->push_constant_mask)) * 4;
          }
-         if (locs->shader_data[AC_UD_INLINE_PUSH_CONSTANTS].sgpr_idx >= 0)
-            /* One PKT3_SET_SH_REG writing all inline push constants. */
-            *cmd_size += (3 * util_bitcount64(layout->push_constant_mask)) * 4;
+      } else {
+         /* Assume the compute shader needs both user SGPRs because we can't know the information
+          * for indirect pipelines.
+          */
+         assert(layout->pipeline_bind_point == VK_PIPELINE_BIND_POINT_COMPUTE);
+         *cmd_size += 3 * 4;
+         need_copy = true;
+
+         *cmd_size += (3 * util_bitcount64(layout->push_constant_mask)) * 4;
       }
+
       if (need_copy) {
          *upload_size += align(layout->push_constant_size + 16 * layout->dynamic_offset_count, 16);
       }
@@ -145,7 +181,7 @@ radv_get_sequence_size(const struct radv_indirect_command_layout *layout, struct
       radv_get_sequence_size_graphics(layout, graphics_pipeline, cmd_size, upload_size);
    } else {
       assert(layout->pipeline_bind_point == VK_PIPELINE_BIND_POINT_COMPUTE);
-      struct radv_compute_pipeline *compute_pipeline = radv_pipeline_to_compute(pipeline);
+      struct radv_compute_pipeline *compute_pipeline = pipeline ? radv_pipeline_to_compute(pipeline) : NULL;
       radv_get_sequence_size_compute(layout, compute_pipeline, cmd_size);
    }
 }
@@ -231,6 +267,9 @@ struct radv_dgc_params {
 
    uint8_t is_dispatch;
    uint8_t use_preamble;
+
+   uint8_t bind_pipeline;
+   uint16_t pipeline_params_offset;
 };
 
 enum {
@@ -287,6 +326,15 @@ dgc_emit(nir_builder *b, struct dgc_cmdbuf *cs, nir_def *value)
    nir_pack_64_2x32((b), nir_load_push_constant((b), 2, 32, nir_imm_int((b), 0),                                       \
                                                 .base = offsetof(struct radv_dgc_params, field), .range = 8))
 
+/* Pipeline metadata */
+#define load_metadata32(b, field)                                                                                      \
+   nir_load_global(                                                                                                    \
+      b, nir_iadd(b, pipeline_va, nir_imm_int64(b, offsetof(struct radv_compute_pipeline_metadata, field))), 4, 1, 32)
+
+#define load_metadata64(b, field)                                                                                      \
+   nir_load_global(                                                                                                    \
+      b, nir_iadd(b, pipeline_va, nir_imm_int64(b, offsetof(struct radv_compute_pipeline_metadata, field))), 4, 1, 64)
+
 static nir_def *
 nir_pkt3_base(nir_builder *b, unsigned op, nir_def *len, bool predicate)
 {
@@ -844,9 +892,103 @@ dgc_emit_index_buffer(nir_builder *b, struct dgc_cmdbuf *cs, nir_def *stream_buf
 /**
  * Emit VK_INDIRECT_COMMANDS_TOKEN_TYPE_PUSH_CONSTANT_NV.
  */
+static nir_def *
+dgc_get_push_constant_shader_cnt(nir_builder *b, nir_def *stream_buf, nir_def *stream_base,
+                                 nir_def *pipeline_params_offset)
+{
+   nir_def *res1, *res2;
+
+   nir_push_if(b, nir_ieq_imm(b, load_param8(b, bind_pipeline), 1));
+   {
+      nir_def *stream_offset = nir_iadd(b, pipeline_params_offset, stream_base);
+      nir_def *pipeline_va = nir_load_ssbo(b, 1, 64, stream_buf, stream_offset);
+
+      res1 = nir_b2i32(b, nir_ine_imm(b, load_metadata32(b, push_const_sgpr), 0));
+   }
+   nir_push_else(b, 0);
+   {
+      res2 = load_param16(b, push_constant_shader_cnt);
+   }
+   nir_pop_if(b, 0);
+
+   return nir_if_phi(b, res1, res2);
+}
+
+static nir_def *
+dgc_get_upload_sgpr(nir_builder *b, nir_def *stream_buf, nir_def *stream_base, nir_def *param_buf,
+                    nir_def *param_offset, nir_def *cur_shader_idx, nir_def *pipeline_params_offset)
+{
+   nir_def *res1, *res2;
+
+   nir_push_if(b, nir_ieq_imm(b, load_param8(b, bind_pipeline), 1));
+   {
+      nir_def *stream_offset = nir_iadd(b, pipeline_params_offset, stream_base);
+      nir_def *pipeline_va = nir_load_ssbo(b, 1, 64, stream_buf, stream_offset);
+
+      res1 = load_metadata32(b, push_const_sgpr);
+   }
+   nir_push_else(b, 0);
+   {
+      res2 = nir_load_ssbo(b, 1, 32, param_buf, nir_iadd(b, param_offset, nir_imul_imm(b, cur_shader_idx, 12)));
+   }
+   nir_pop_if(b, 0);
+
+   nir_def *res = nir_if_phi(b, res1, res2);
+
+   return nir_ubfe_imm(b, res, 0, 16);
+}
+
+static nir_def *
+dgc_get_inline_sgpr(nir_builder *b, nir_def *stream_buf, nir_def *stream_base, nir_def *param_buf,
+                    nir_def *param_offset, nir_def *cur_shader_idx, nir_def *pipeline_params_offset)
+{
+   nir_def *res1, *res2;
+
+   nir_push_if(b, nir_ieq_imm(b, load_param8(b, bind_pipeline), 1));
+   {
+      nir_def *stream_offset = nir_iadd(b, pipeline_params_offset, stream_base);
+      nir_def *pipeline_va = nir_load_ssbo(b, 1, 64, stream_buf, stream_offset);
+
+      res1 = load_metadata32(b, push_const_sgpr);
+   }
+   nir_push_else(b, 0);
+   {
+      res2 = nir_load_ssbo(b, 1, 32, param_buf, nir_iadd(b, param_offset, nir_imul_imm(b, cur_shader_idx, 12)));
+   }
+   nir_pop_if(b, 0);
+
+   nir_def *res = nir_if_phi(b, res1, res2);
+
+   return nir_ubfe_imm(b, res, 16, 16);
+}
+
+static nir_def *
+dgc_get_inline_mask(nir_builder *b, nir_def *stream_buf, nir_def *stream_base, nir_def *param_buf,
+                    nir_def *param_offset, nir_def *cur_shader_idx, nir_def *pipeline_params_offset)
+{
+   nir_def *res1, *res2;
+
+   nir_push_if(b, nir_ieq_imm(b, load_param8(b, bind_pipeline), 1));
+   {
+      nir_def *stream_offset = nir_iadd(b, pipeline_params_offset, stream_base);
+      nir_def *pipeline_va = nir_load_ssbo(b, 1, 64, stream_buf, stream_offset);
+
+      res1 = load_metadata64(b, inline_push_const_mask);
+   }
+   nir_push_else(b, 0);
+   {
+      nir_def *reg_info = nir_load_ssbo(
+         b, 2, 32, param_buf, nir_iadd(b, param_offset, nir_iadd_imm(b, nir_imul_imm(b, cur_shader_idx, 12), 4)));
+      res2 = nir_pack_64_2x32(b, nir_channels(b, reg_info, 0x3));
+   }
+   nir_pop_if(b, 0);
+
+   return nir_if_phi(b, res1, res2);
+}
+
 static void
 dgc_emit_push_constant(nir_builder *b, struct dgc_cmdbuf *cs, nir_def *stream_buf, nir_def *stream_base,
-                       nir_def *push_const_mask, nir_variable *upload_offset)
+                       nir_def *pipeline_params_offset, nir_def *push_const_mask, nir_variable *upload_offset)
 {
    nir_def *vbo_cnt = load_param8(b, vbo_cnt);
    nir_def *const_copy = nir_ine_imm(b, load_param8(b, const_copy), 0);
@@ -901,7 +1043,7 @@ dgc_emit_push_constant(nir_builder *b, struct dgc_cmdbuf *cs, nir_def *stream_bu
 
    nir_variable *shader_idx = nir_variable_create(b->shader, nir_var_shader_temp, glsl_uint_type(), "shader_idx");
    nir_store_var(b, shader_idx, nir_imm_int(b, 0), 0x1);
-   nir_def *shader_cnt = load_param16(b, push_constant_shader_cnt);
+   nir_def *shader_cnt = dgc_get_push_constant_shader_cnt(b, stream_buf, stream_base, pipeline_params_offset);
 
    nir_push_loop(b);
    {
@@ -912,11 +1054,12 @@ dgc_emit_push_constant(nir_builder *b, struct dgc_cmdbuf *cs, nir_def *stream_bu
       }
       nir_pop_if(b, NULL);
 
-      nir_def *reg_info =
-         nir_load_ssbo(b, 3, 32, param_buf, nir_iadd(b, param_offset, nir_imul_imm(b, cur_shader_idx, 12)));
-      nir_def *upload_sgpr = nir_ubfe_imm(b, nir_channel(b, reg_info, 0), 0, 16);
-      nir_def *inline_sgpr = nir_ubfe_imm(b, nir_channel(b, reg_info, 0), 16, 16);
-      nir_def *inline_mask = nir_pack_64_2x32(b, nir_channels(b, reg_info, 0x6));
+      nir_def *upload_sgpr = dgc_get_upload_sgpr(b, stream_buf, stream_base, param_buf, param_offset, cur_shader_idx,
+                                                 pipeline_params_offset);
+      nir_def *inline_sgpr = dgc_get_inline_sgpr(b, stream_buf, stream_base, param_buf, param_offset, cur_shader_idx,
+                                                 pipeline_params_offset);
+      nir_def *inline_mask = dgc_get_inline_mask(b, stream_buf, stream_base, param_buf, param_offset, cur_shader_idx,
+                                                 pipeline_params_offset);
 
       nir_push_if(b, nir_ine_imm(b, upload_sgpr, 0));
       {
@@ -1124,9 +1267,53 @@ dgc_emit_vertex_buffer(nir_builder *b, struct dgc_cmdbuf *cs, nir_def *stream_bu
 /**
  * For emitting VK_INDIRECT_COMMANDS_TOKEN_TYPE_DISPATCH_NV.
  */
+static nir_def *
+dgc_get_grid_sgpr(nir_builder *b, nir_def *stream_buf, nir_def *stream_base, nir_def *pipeline_params_offset)
+{
+   nir_def *res1, *res2;
+
+   nir_push_if(b, nir_ieq_imm(b, load_param8(b, bind_pipeline), 1));
+   {
+      nir_def *stream_offset = nir_iadd(b, pipeline_params_offset, stream_base);
+      nir_def *pipeline_va = nir_load_ssbo(b, 1, 64, stream_buf, stream_offset);
+      res1 = load_metadata32(b, grid_base_sgpr);
+   }
+   nir_push_else(b, 0);
+   {
+      res2 = load_param16(b, grid_base_sgpr);
+   }
+   nir_pop_if(b, 0);
+
+   return nir_if_phi(b, res1, res2);
+}
+
+static nir_def *
+dgc_get_dispatch_initiator(nir_builder *b, nir_def *stream_buf, nir_def *stream_base, nir_def *pipeline_params_offset)
+{
+   nir_def *res1, *res2;
+
+   nir_push_if(b, nir_ieq_imm(b, load_param8(b, bind_pipeline), 1));
+   {
+      nir_def *stream_offset = nir_iadd(b, pipeline_params_offset, stream_base);
+      nir_def *pipeline_va = nir_load_ssbo(b, 1, 64, stream_buf, stream_offset);
+
+      nir_def *dispatch_initiator = load_param32(b, dispatch_initiator);
+      nir_def *wave32 = nir_ieq_imm(b, load_metadata32(b, wave32), 1);
+      res1 = nir_bcsel(b, wave32, nir_ior_imm(b, dispatch_initiator, S_00B800_CS_W32_EN(1)), dispatch_initiator);
+   }
+   nir_push_else(b, 0);
+   {
+      res2 = load_param32(b, dispatch_initiator);
+   }
+   nir_pop_if(b, 0);
+
+   return nir_if_phi(b, res1, res2);
+}
+
 static void
 dgc_emit_dispatch(nir_builder *b, struct dgc_cmdbuf *cs, nir_def *stream_buf, nir_def *stream_base,
-                  nir_def *dispatch_params_offset, nir_def *sequence_id, const struct radv_device *device)
+                  nir_def *dispatch_params_offset, nir_def *pipeline_params_offset, nir_def *sequence_id,
+                  const struct radv_device *device)
 {
    nir_def *stream_offset = nir_iadd(b, dispatch_params_offset, stream_base);
 
@@ -1135,7 +1322,7 @@ dgc_emit_dispatch(nir_builder *b, struct dgc_cmdbuf *cs, nir_def *stream_buf, ni
    nir_def *wg_y = nir_channel(b, dispatch_data, 1);
    nir_def *wg_z = nir_channel(b, dispatch_data, 2);
 
-   nir_def *grid_sgpr = load_param16(b, grid_base_sgpr);
+   nir_def *grid_sgpr = dgc_get_grid_sgpr(b, stream_buf, stream_base, pipeline_params_offset);
    nir_push_if(b, nir_ine_imm(b, grid_sgpr, 0));
    {
       if (device->load_grid_size_from_user_sgpr) {
@@ -1151,7 +1338,8 @@ dgc_emit_dispatch(nir_builder *b, struct dgc_cmdbuf *cs, nir_def *stream_buf, ni
       dgc_emit_sqtt_begin_api_marker(b, cs, ApiCmdDispatch);
       dgc_emit_sqtt_marker_event_with_dims(b, cs, sequence_id, wg_x, wg_y, wg_z, EventCmdDispatch);
 
-      dgc_emit_dispatch_direct(b, cs, wg_x, wg_y, wg_z, load_param32(b, dispatch_initiator));
+      nir_def *dispatch_initiator = dgc_get_dispatch_initiator(b, stream_buf, stream_base, pipeline_params_offset);
+      dgc_emit_dispatch_direct(b, cs, wg_x, wg_y, wg_z, dispatch_initiator);
 
       dgc_emit_sqtt_thread_trace_marker(b, cs);
       dgc_emit_sqtt_end_api_marker(b, cs, ApiCmdDispatch);
@@ -1195,6 +1383,48 @@ dgc_emit_draw_mesh_tasks(nir_builder *b, struct dgc_cmdbuf *cs, nir_def *stream_
    nir_pop_if(b, NULL);
 }
 
+/**
+ * Emit VK_INDIRECT_COMMANDS_TOKEN_TYPE_PIPELINE_NV.
+ */
+static void
+dgc_emit_set_sh_reg_seq(nir_builder *b, struct dgc_cmdbuf *cs, unsigned reg, unsigned num)
+{
+   nir_def *values[2] = {
+      nir_imm_int(b, PKT3(PKT3_SET_SH_REG, num, false)),
+      nir_imm_int(b, (reg - SI_SH_REG_OFFSET) >> 2),
+   };
+   dgc_emit(b, cs, nir_vec(b, values, 2));
+}
+
+static void
+dgc_emit_bind_pipeline(nir_builder *b, struct dgc_cmdbuf *cs, nir_def *stream_buf, nir_def *stream_base,
+                       nir_def *pipeline_params_offset, const struct radv_device *device)
+{
+   nir_def *stream_offset = nir_iadd(b, pipeline_params_offset, stream_base);
+
+   nir_def *pipeline_va = nir_load_ssbo(b, 1, 64, stream_buf, stream_offset);
+
+   dgc_emit_set_sh_reg_seq(b, cs, R_00B830_COMPUTE_PGM_LO, 1);
+   dgc_emit(b, cs, load_metadata32(b, shader_va));
+
+   dgc_emit_set_sh_reg_seq(b, cs, R_00B848_COMPUTE_PGM_RSRC1, 2);
+   dgc_emit(b, cs, load_metadata32(b, rsrc1));
+   dgc_emit(b, cs, load_metadata32(b, rsrc2));
+
+   if (device->physical_device->rad_info.gfx_level >= GFX10) {
+      dgc_emit_set_sh_reg_seq(b, cs, R_00B8A0_COMPUTE_PGM_RSRC3, 1);
+      dgc_emit(b, cs, load_metadata32(b, rsrc3));
+   }
+
+   dgc_emit_set_sh_reg_seq(b, cs, R_00B854_COMPUTE_RESOURCE_LIMITS, 1);
+   dgc_emit(b, cs, load_metadata32(b, compute_resource_limits));
+
+   dgc_emit_set_sh_reg_seq(b, cs, R_00B81C_COMPUTE_NUM_THREAD_X, 3);
+   dgc_emit(b, cs, load_metadata32(b, block_size_x));
+   dgc_emit(b, cs, load_metadata32(b, block_size_y));
+   dgc_emit(b, cs, load_metadata32(b, block_size_z));
+}
+
 static nir_shader *
 build_dgc_prepare_shader(struct radv_device *dev)
 {
@@ -1264,7 +1494,14 @@ build_dgc_prepare_shader(struct radv_device *dev)
       nir_def *push_const_mask = load_param64(&b, push_constant_mask);
       nir_push_if(&b, nir_ine_imm(&b, push_const_mask, 0));
       {
-         dgc_emit_push_constant(&b, &cmd_buf, stream_buf, stream_base, push_const_mask, upload_offset);
+         dgc_emit_push_constant(&b, &cmd_buf, stream_buf, stream_base, load_param16(&b, pipeline_params_offset),
+                                push_const_mask, upload_offset);
+      }
+      nir_pop_if(&b, 0);
+
+      nir_push_if(&b, nir_ieq_imm(&b, load_param8(&b, bind_pipeline), 1));
+      {
+         dgc_emit_bind_pipeline(&b, &cmd_buf, stream_buf, stream_base, load_param16(&b, pipeline_params_offset), dev);
       }
       nir_pop_if(&b, 0);
 
@@ -1317,8 +1554,8 @@ build_dgc_prepare_shader(struct radv_device *dev)
       }
       nir_push_else(&b, NULL);
       {
-         dgc_emit_dispatch(&b, &cmd_buf, stream_buf, stream_base, load_param16(&b, dispatch_params_offset), sequence_id,
-                           dev);
+         dgc_emit_dispatch(&b, &cmd_buf, stream_buf, stream_base, load_param16(&b, dispatch_params_offset),
+                           load_param16(&b, pipeline_params_offset), sequence_id, dev);
       }
       nir_pop_if(&b, NULL);
 
@@ -1515,6 +1752,10 @@ radv_CreateIndirectCommandsLayoutNV(VkDevice _device, const VkIndirectCommandsLa
          layout->draw_mesh_tasks = true;
          layout->draw_params_offset = pCreateInfo->pTokens[i].offset;
          break;
+      case VK_INDIRECT_COMMANDS_TOKEN_TYPE_PIPELINE_NV:
+         layout->bind_pipeline = true;
+         layout->pipeline_params_offset = pCreateInfo->pTokens[i].offset;
+         break;
       default:
          unreachable("Unhandled token type");
       }
@@ -1732,8 +1973,6 @@ radv_prepare_dgc_compute(struct radv_cmd_buffer *cmd_buffer, const VkGeneratedCo
 {
    VK_FROM_HANDLE(radv_indirect_command_layout, layout, pGeneratedCommandsInfo->indirectCommandsLayout);
    VK_FROM_HANDLE(radv_pipeline, pipeline, pGeneratedCommandsInfo->pipeline);
-   struct radv_compute_pipeline *compute_pipeline = radv_pipeline_to_compute(pipeline);
-   struct radv_shader *cs = radv_get_shader(compute_pipeline->base.shaders, MESA_SHADER_COMPUTE);
 
    *upload_size = MAX2(*upload_size, 16);
 
@@ -1742,20 +1981,26 @@ radv_prepare_dgc_compute(struct radv_cmd_buffer *cmd_buffer, const VkGeneratedCo
       return;
    }
 
-   uint32_t dispatch_initiator = cmd_buffer->device->dispatch_initiator;
-   dispatch_initiator |= S_00B800_FORCE_START_AT_000(1);
-   if (cs->info.wave_size == 32) {
-      assert(cmd_buffer->device->physical_device->rad_info.gfx_level >= GFX10);
-      dispatch_initiator |= S_00B800_CS_W32_EN(1);
-   }
-
    params->dispatch_params_offset = layout->dispatch_params_offset;
-   params->dispatch_initiator = dispatch_initiator;
+   params->dispatch_initiator = cmd_buffer->device->dispatch_initiator | S_00B800_FORCE_START_AT_000(1);
    params->is_dispatch = 1;
 
-   const struct radv_userdata_info *loc = radv_get_user_sgpr(cs, AC_UD_CS_GRID_SIZE);
-   if (loc->sgpr_idx != -1) {
-      params->grid_base_sgpr = (cs->info.user_data_0 + 4 * loc->sgpr_idx - SI_SH_REG_OFFSET) >> 2;
+   if (pipeline) {
+      struct radv_compute_pipeline *compute_pipeline = radv_pipeline_to_compute(pipeline);
+      struct radv_shader *cs = radv_get_shader(compute_pipeline->base.shaders, MESA_SHADER_COMPUTE);
+
+      if (cs->info.wave_size == 32) {
+         assert(cmd_buffer->device->physical_device->rad_info.gfx_level >= GFX10);
+         params->dispatch_initiator |= S_00B800_CS_W32_EN(1);
+      }
+
+      const struct radv_userdata_info *loc = radv_get_user_sgpr(cs, AC_UD_CS_GRID_SIZE);
+      if (loc->sgpr_idx != -1) {
+         params->grid_base_sgpr = (cs->info.user_data_0 + 4 * loc->sgpr_idx - SI_SH_REG_OFFSET) >> 2;
+      }
+   } else {
+      params->bind_pipeline = 1;
+      params->pipeline_params_offset = layout->pipeline_params_offset;
    }
 }
 
@@ -1812,35 +2057,38 @@ radv_prepare_dgc(struct radv_cmd_buffer *cmd_buffer, const VkGeneratedCommandsIn
       upload_data = (char *)upload_data + ARRAY_SIZE(pipeline->shaders) * 12;
 
       unsigned idx = 0;
-      for (unsigned i = 0; i < ARRAY_SIZE(pipeline->shaders); ++i) {
-         if (!pipeline->shaders[i])
-            continue;
-
-         const struct radv_shader *shader = pipeline->shaders[i];
-         const struct radv_userdata_locations *locs = &shader->info.user_sgprs_locs;
-         if (locs->shader_data[AC_UD_PUSH_CONSTANTS].sgpr_idx >= 0)
-            params.const_copy = 1;
-
-         if (locs->shader_data[AC_UD_PUSH_CONSTANTS].sgpr_idx >= 0 ||
-             locs->shader_data[AC_UD_INLINE_PUSH_CONSTANTS].sgpr_idx >= 0) {
-            unsigned upload_sgpr = 0;
-            unsigned inline_sgpr = 0;
 
-            if (locs->shader_data[AC_UD_PUSH_CONSTANTS].sgpr_idx >= 0) {
-               upload_sgpr = (shader->info.user_data_0 + 4 * locs->shader_data[AC_UD_PUSH_CONSTANTS].sgpr_idx -
-                              SI_SH_REG_OFFSET) >>
-                             2;
-            }
+      if (pipeline) {
+         for (unsigned i = 0; i < ARRAY_SIZE(pipeline->shaders); ++i) {
+            if (!pipeline->shaders[i])
+               continue;
+
+            const struct radv_shader *shader = pipeline->shaders[i];
+            const struct radv_userdata_locations *locs = &shader->info.user_sgprs_locs;
+            if (locs->shader_data[AC_UD_PUSH_CONSTANTS].sgpr_idx >= 0)
+               params.const_copy = 1;
+
+            if (locs->shader_data[AC_UD_PUSH_CONSTANTS].sgpr_idx >= 0 ||
+                locs->shader_data[AC_UD_INLINE_PUSH_CONSTANTS].sgpr_idx >= 0) {
+               unsigned upload_sgpr = 0;
+               unsigned inline_sgpr = 0;
+
+               if (locs->shader_data[AC_UD_PUSH_CONSTANTS].sgpr_idx >= 0) {
+                  upload_sgpr = (shader->info.user_data_0 + 4 * locs->shader_data[AC_UD_PUSH_CONSTANTS].sgpr_idx -
+                                 SI_SH_REG_OFFSET) >>
+                                2;
+               }
 
-            if (locs->shader_data[AC_UD_INLINE_PUSH_CONSTANTS].sgpr_idx >= 0) {
-               inline_sgpr = (shader->info.user_data_0 + 4 * locs->shader_data[AC_UD_INLINE_PUSH_CONSTANTS].sgpr_idx -
-                              SI_SH_REG_OFFSET) >>
-                             2;
-               desc[idx * 3 + 1] = pipeline->shaders[i]->info.inline_push_constant_mask;
-               desc[idx * 3 + 2] = pipeline->shaders[i]->info.inline_push_constant_mask >> 32;
+               if (locs->shader_data[AC_UD_INLINE_PUSH_CONSTANTS].sgpr_idx >= 0) {
+                  inline_sgpr = (shader->info.user_data_0 +
+                                 4 * locs->shader_data[AC_UD_INLINE_PUSH_CONSTANTS].sgpr_idx - SI_SH_REG_OFFSET) >>
+                                2;
+                  desc[idx * 3 + 1] = pipeline->shaders[i]->info.inline_push_constant_mask;
+                  desc[idx * 3 + 2] = pipeline->shaders[i]->info.inline_push_constant_mask >> 32;
+               }
+               desc[idx * 3] = upload_sgpr | (inline_sgpr << 16);
+               ++idx;
             }
-            desc[idx * 3] = upload_sgpr | (inline_sgpr << 16);
-            ++idx;
          }
       }
 
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index 8708735ea3ce..3e94301c6a2c 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -3254,6 +3254,9 @@ struct radv_indirect_command_layout {
 
    uint16_t dispatch_params_offset;
 
+   bool bind_pipeline;
+   uint16_t pipeline_params_offset;
+
    uint32_t bind_vbo_mask;
    uint32_t vbo_offsets[MAX_VBS];
 
-- 
GitLab


From 1a0b624541e9f8d27fc703ca4dd71c64646ba0a7 Mon Sep 17 00:00:00 2001
From: Samuel Pitoiset <samuel.pitoiset@gmail.com>
Date: Tue, 6 Feb 2024 11:33:35 +0100
Subject: [PATCH 9/9] radv: enable deviceGeneratedComputePipelines

This is supported now and it's passing
dEQP-VK.dgc.nv.compute.layout.pipeline_*.

Signed-off-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 src/amd/vulkan/radv_physical_device.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/src/amd/vulkan/radv_physical_device.c b/src/amd/vulkan/radv_physical_device.c
index 283b88e80815..9e0ded29f54b 100644
--- a/src/amd/vulkan/radv_physical_device.c
+++ b/src/amd/vulkan/radv_physical_device.c
@@ -1108,7 +1108,7 @@ radv_physical_device_get_features(const struct radv_physical_device *pdevice, st
 
       /* VK_NV_device_generated_commands_compute */
       .deviceGeneratedCompute = true,
-      .deviceGeneratedComputePipelines = false,
+      .deviceGeneratedComputePipelines = true,
       .deviceGeneratedComputeCaptureReplay = false,
 
       /* VK_KHR_cooperative_matrix */
-- 
GitLab

