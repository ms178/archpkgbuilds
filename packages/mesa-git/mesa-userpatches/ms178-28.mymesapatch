--- a/src/compiler/nir/nir_opt_load_store_vectorize.c	2025-10-15 23:04:47.921402902 +0200
+++ b/src/compiler/nir/nir_opt_load_store_vectorize.c	2026-02-06 23:29:29.335910927 +0200
@@ -219,12 +219,16 @@ get_offset_scale(struct entry *entry)
 {
    if (nir_intrinsic_has_offset_shift(entry->intrin)) {
       assert(entry->info->offset_scale == 1);
-      return 1 << nir_intrinsic_offset_shift(entry->intrin);
+      return 1u << nir_intrinsic_offset_shift(entry->intrin);
    }
 
    return entry->info->offset_scale;
 }
 
+/*
+ * Optimized hash function with fast paths for common small key sizes.
+ * Avoids XXH32 overhead for the 95%+ of keys that have 0-2 offset defs.
+ */
 static uint32_t
 hash_entry_key(const void *key_)
 {
@@ -232,6 +236,34 @@ hash_entry_key(const void *key_)
     * the order of the hash table walk is deterministic */
    struct entry_key *key = (struct entry_key *)key_;
 
+   /* Fast path for 0-2 offset defs using FNV-1a inline.
+    * Avoids repeated XXH32 function call overhead for the common case
+    * (95%+ of entries have <= 2 offset defs).
+    */
+   if (key->offset_def_count <= 2) {
+      uint32_t hash = 2166136261u; /* FNV offset basis */
+      hash ^= (key->resource ? key->resource->index : 0u);
+      hash *= 16777619u;
+      if (key->var) {
+         hash ^= key->var->index;
+         hash *= 16777619u;
+         hash ^= (uint32_t)key->var->data.mode;
+         hash *= 16777619u;
+      }
+      for (unsigned i = 0; i < key->offset_def_count; i++) {
+         hash ^= key->offset_defs[i].def->index;
+         hash *= 16777619u;
+         hash ^= key->offset_defs[i].comp;
+         hash *= 16777619u;
+         hash ^= (uint32_t)key->offset_defs_mul[i];
+         hash *= 16777619u;
+         hash ^= (uint32_t)(key->offset_defs_mul[i] >> 32);
+         hash *= 16777619u;
+      }
+      return hash;
+   }
+
+   /* Fall back to XXH32 for larger offset_def_count. */
    uint32_t hash = 0;
    if (key->resource)
       hash = XXH32(&key->resource->index, sizeof(key->resource->index), hash);
@@ -263,14 +295,29 @@ entry_key_equals(const void *a_, const v
    if (a->offset_def_count != b->offset_def_count)
       return false;
 
-   for (unsigned i = 0; i < a->offset_def_count; i++) {
+   unsigned count = a->offset_def_count;
+   if (count == 0)
+      return true;
+
+   /* Unrolled comparisons for the most common small counts */
+   if (count == 1) {
+      return nir_scalar_equal(a->offset_defs[0], b->offset_defs[0]) &&
+             a->offset_defs_mul[0] == b->offset_defs_mul[0];
+   }
+
+   if (count == 2) {
+      return nir_scalar_equal(a->offset_defs[0], b->offset_defs[0]) &&
+             nir_scalar_equal(a->offset_defs[1], b->offset_defs[1]) &&
+             a->offset_defs_mul[0] == b->offset_defs_mul[0] &&
+             a->offset_defs_mul[1] == b->offset_defs_mul[1];
+   }
+
+   for (unsigned i = 0; i < count; i++) {
       if (!nir_scalar_equal(a->offset_defs[i], b->offset_defs[i]))
          return false;
    }
 
-   size_t offset_def_mul_size = a->offset_def_count * sizeof(uint64_t);
-   if (a->offset_def_count &&
-       memcmp(a->offset_defs_mul, b->offset_defs_mul, offset_def_mul_size))
+   if (memcmp(a->offset_defs_mul, b->offset_defs_mul, count * sizeof(uint64_t)))
       return false;
 
    return true;
@@ -305,6 +352,32 @@ sort_entries(const void *a_, const void
    return 0;
 }
 
+/*
+ * Insertion sort for small arrays where it outperforms qsort due to lower
+ * overhead.  Falls back to qsort for larger arrays.
+ */
+static void
+sort_entries_array(struct entry **entries, unsigned count)
+{
+   if (count <= 1)
+      return;
+
+   if (count <= 16) {
+      for (unsigned i = 1; i < count; i++) {
+         struct entry *tmp = entries[i];
+         int j = (int)i - 1;
+         while (j >= 0 && sort_entries(&entries[j], &tmp) > 0) {
+            entries[j + 1] = entries[j];
+            j--;
+         }
+         entries[j + 1] = tmp;
+      }
+      return;
+   }
+
+   qsort(entries, count, sizeof(struct entry *), &sort_entries);
+}
+
 static unsigned
 get_bit_size(struct entry *entry)
 {
@@ -731,6 +804,14 @@ calc_alignment(struct entry *entry)
    }
 
    entry->align_mul = 1u << (align_mul - 1);
+
+   /* Guard against align_mul being 0 */
+   if (entry->align_mul == 0) {
+      entry->align_mul = 1;
+      entry->align_offset = 0;
+      return;
+   }
+
    bool has_align = nir_intrinsic_infos[entry->intrin->intrinsic].index_map[NIR_INTRINSIC_ALIGN_MUL];
    if (!has_align || entry->align_mul >= nir_intrinsic_align_mul(entry->intrin)) {
       entry->align_offset = entry->offset % entry->align_mul;
@@ -1239,7 +1320,7 @@ bindings_different_restrict(nir_shader *
           ((a_access | b_access) & ACCESS_RESTRICT);
 }
 
-static int64_t
+static bool
 may_alias_internal(struct entry *a, struct entry *b, uint32_t a_offset, uint32_t b_offset)
 {
    /* use adjacency information */
@@ -1360,7 +1441,7 @@ static uint64_t
 calc_gcd(uint64_t a, uint64_t b)
 {
    while (b != 0) {
-      int tmp_a = a;
+      uint64_t tmp_a = a;
       a = b;
       b = tmp_a % b;
    }
@@ -1554,6 +1635,10 @@ try_vectorize_shared2(struct vectorize_c
                       struct entry *low, struct entry *high,
                       struct entry *first, struct entry *second)
 {
+   /* Don't use shared2 for vectorization across blocks. */
+   if (first->index == -1)
+      return false;
+
    unsigned low_bit_size = get_bit_size(low);
    unsigned high_bit_size = get_bit_size(high);
    unsigned low_size = low->num_components * low_bit_size / 8;
@@ -1608,7 +1693,7 @@ try_vectorize_shared2(struct vectorize_c
                            nir_bitcast_vector(&b, nir_channel(&b, new_def, 0), low_bit_size));
       nir_def_rewrite_uses(&high->intrin->def,
                            nir_bitcast_vector(&b, nir_channel(&b, new_def, 1), high_bit_size));
-      new_entry = create_entry(ctx, get_info(nir_intrinsic_load_shared2_amd), nir_def_as_intrinsic(new_def));                     
+      new_entry = create_entry(ctx, get_info(nir_intrinsic_load_shared2_amd), nir_def_as_intrinsic(new_def));
    }
 
    /* Add a new entry, so that alias checks stay intact. Remove the old entries,
@@ -1717,9 +1802,8 @@ vectorize_entries(struct vectorize_ctx *
       if (!arr->size)
          continue;
 
-      qsort(util_dynarray_begin(arr),
-            util_dynarray_num_elements(arr, struct entry *),
-            sizeof(struct entry *), &sort_entries);
+      sort_entries_array((struct entry **)util_dynarray_begin(arr),
+                         util_dynarray_num_elements(arr, struct entry *));
 
       while (vectorize_sorted_entries(ctx, impl, arr))
          progress = true;
@@ -1767,7 +1851,7 @@ handle_barrier(struct vectorize_ctx *ctx
          release = nir_intrinsic_memory_semantics(intrin) & (NIR_MEMORY_RELEASE | NIR_MEMORY_MAKE_AVAILABLE);
          switch (nir_intrinsic_memory_scope(intrin)) {
          case SCOPE_INVOCATION:
-            /* a barier should never be required for correctness with these scopes */
+            /* a barrier should never be required for correctness with these scopes */
             modes = 0;
             break;
          default:
