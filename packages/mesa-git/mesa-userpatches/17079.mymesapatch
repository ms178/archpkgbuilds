From be0fef224e9f8011d9ef4f36d8a717ab9374dccc Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Thu, 19 May 2022 16:56:56 +0100
Subject: [PATCH 01/13] aco: make flat access latency match mtbuf/mubuf/mimg
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
Reviewed-by: Daniel Schürmann <daniel@schuermann.dev>
---
 src/amd/compiler/aco_statistics.cpp | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/src/amd/compiler/aco_statistics.cpp b/src/amd/compiler/aco_statistics.cpp
index 9dd5aeb98ce6..6d5e2267eca0 100644
--- a/src/amd/compiler/aco_statistics.cpp
+++ b/src/amd/compiler/aco_statistics.cpp
@@ -221,9 +221,9 @@ get_wait_counter_info(aco_ptr<Instruction>& instr)
    if (instr->isFlatLike()) {
       unsigned lgkm = instr->isFlat() ? 20 : 0;
       if (!instr->definitions.empty())
-         return wait_counter_info(230, 0, lgkm, 0);
+         return wait_counter_info(320, 0, lgkm, 0);
       else
-         return wait_counter_info(0, 0, lgkm, 230);
+         return wait_counter_info(0, 0, lgkm, 320);
    }
 
    if (instr->isSMEM()) {
-- 
GitLab


From 973c3ab9a13a42bf24affbe51691cab20eef4282 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Thu, 19 May 2022 17:54:38 +0100
Subject: [PATCH 02/13] aco: include flat-like in vmem clause statistics
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
Reviewed-by: Daniel Schürmann <daniel@schuermann.dev>
---
 src/amd/compiler/aco_statistics.cpp | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/src/amd/compiler/aco_statistics.cpp b/src/amd/compiler/aco_statistics.cpp
index 6d5e2267eca0..1ad1fb476e03 100644
--- a/src/amd/compiler/aco_statistics.cpp
+++ b/src/amd/compiler/aco_statistics.cpp
@@ -447,7 +447,8 @@ collect_preasm_stats(Program* program)
          if (instr->opcode == aco_opcode::p_constaddr)
             program->statistics[statistic_instructions] += 2;
 
-         if (instr->isVMEM() && !instr->operands.empty()) {
+         if ((instr->isVMEM() || instr->isScratch() || instr->isGlobal()) &&
+             !instr->operands.empty()) {
             if (std::none_of(vmem_clause.begin(), vmem_clause.end(),
                              [&](Instruction* other)
                              { return should_form_clause(instr.get(), other); }))
-- 
GitLab


From 763568383c1d8a17f0ebd7544d7c5dd8dc9d0921 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Thu, 19 May 2022 15:18:36 +0100
Subject: [PATCH 03/13] aco: make FLAT_instruction::offset signed
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
Reviewed-by: Daniel Schürmann <daniel@schuermann.dev>
---
 src/amd/compiler/aco_assembler.cpp             | 11 +++++++----
 src/amd/compiler/aco_instruction_selection.cpp | 10 ++++------
 src/amd/compiler/aco_ir.cpp                    | 12 ++++++++++++
 src/amd/compiler/aco_ir.h                      |  5 ++++-
 src/amd/compiler/aco_opcodes.py                |  2 +-
 src/amd/compiler/aco_print_ir.cpp              |  2 +-
 6 files changed, 29 insertions(+), 13 deletions(-)

diff --git a/src/amd/compiler/aco_assembler.cpp b/src/amd/compiler/aco_assembler.cpp
index 4bccd57e4b56..e302795869da 100644
--- a/src/amd/compiler/aco_assembler.cpp
+++ b/src/amd/compiler/aco_assembler.cpp
@@ -507,16 +507,19 @@ emit_instruction(asm_context& ctx, std::vector<uint32_t>& out, Instruction* inst
       FLAT_instruction& flat = instr->flatlike();
       uint32_t encoding = (0b110111 << 26);
       encoding |= opcode << 18;
-      if (ctx.gfx_level <= GFX9) {
-         assert(flat.offset <= 0x1fff);
+      if (ctx.gfx_level == GFX9 || ctx.gfx_level >= GFX11) {
+         if (instr->isFlat())
+            assert(flat.offset <= 0xfff);
+         else
+            assert(flat.offset >= -4096 && flat.offset < 4096);
          encoding |= flat.offset & 0x1fff;
-      } else if (instr->isFlat()) {
+      } else if (ctx.gfx_level <= GFX8 || instr->isFlat()) {
          /* GFX10 has a 12-bit immediate OFFSET field,
           * but it has a hw bug: it ignores the offset, called FlatSegmentOffsetBug
           */
          assert(flat.offset == 0);
       } else {
-         assert(flat.offset <= 0xfff);
+         assert(flat.offset >= -2048 && flat.offset <= 2047);
          encoding |= flat.offset & 0xfff;
       }
       if (instr->isScratch())
diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 790ea9aed1f3..876c15bb643f 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -4467,12 +4467,10 @@ lower_global_address(Builder& bld, uint32_t offset_in, Temp* address_inout,
 
    uint64_t max_const_offset_plus_one =
       1; /* GFX7/8/9: FLAT loads do not support constant offsets */
-   if (bld.program->gfx_level >= GFX10)
-      max_const_offset_plus_one =
-         2048; /* GLOBAL has a 11-bit signed offset field (12 bits if signed) */
-   else if (bld.program->gfx_level == GFX6 || bld.program->gfx_level == GFX9)
-      max_const_offset_plus_one =
-         4096; /* MUBUF/GLOBAL has a 12-bit unsigned offset field (13 bits if signed for GLOBAL) */
+   if (bld.program->gfx_level >= GFX9)
+      max_const_offset_plus_one = bld.program->dev.scratch_global_offset_max;
+   else if (bld.program->gfx_level == GFX6)
+      max_const_offset_plus_one = 4096; /* MUBUF has a 12-bit unsigned offset field */
    uint64_t excess_offset = const_offset - (const_offset % max_const_offset_plus_one);
    const_offset %= max_const_offset_plus_one;
 
diff --git a/src/amd/compiler/aco_ir.cpp b/src/amd/compiler/aco_ir.cpp
index 45ae5bef6939..5d325f863a22 100644
--- a/src/amd/compiler/aco_ir.cpp
+++ b/src/amd/compiler/aco_ir.cpp
@@ -155,6 +155,18 @@ init_program(Program* program, Stage stage, const struct aco_shader_info* info,
        program->family == CHIP_ARCTURUS || program->family == CHIP_ALDEBARAN)
       program->dev.fused_mad_mix = true;
 
+   if (program->gfx_level >= GFX11) {
+      program->dev.scratch_global_offset_min = -4096;
+      program->dev.scratch_global_offset_max = 4095;
+   } else if (program->gfx_level >= GFX10 || program->gfx_level == GFX8) {
+      program->dev.scratch_global_offset_min = -2048;
+      program->dev.scratch_global_offset_max = 2047;
+   } else if (program->gfx_level == GFX9) {
+      /* The minimum is actually -4096, but negative offsets are broken when SADDR is used. */
+      program->dev.scratch_global_offset_min = 0;
+      program->dev.scratch_global_offset_max = 4095;
+   }
+
    program->wgp_mode = wgp_mode;
 
    program->progress = CompilationProgress::after_isel;
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index 0a00de4f46ef..bc989875741c 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -1646,7 +1646,7 @@ struct FLAT_instruction : public Instruction {
    bool nv : 1;
    bool disable_wqm : 1; /* Require an exec mask without helper invocations */
    uint8_t padding0 : 2;
-   uint16_t offset; /* Vega/Navi only */
+   int16_t offset; /* Vega/Navi only */
    uint16_t padding1;
 };
 static_assert(sizeof(FLAT_instruction) == sizeof(Instruction) + 8, "Unexpected padding");
@@ -2066,6 +2066,9 @@ struct DeviceInfo {
    bool fused_mad_mix = false;
    bool xnack_enabled = false;
    bool sram_ecc_enabled = false;
+
+   int16_t scratch_global_offset_min;
+   int16_t scratch_global_offset_max;
 };
 
 enum class CompilationProgress {
diff --git a/src/amd/compiler/aco_opcodes.py b/src/amd/compiler/aco_opcodes.py
index 509de093d262..64a4b398c957 100644
--- a/src/amd/compiler/aco_opcodes.py
+++ b/src/amd/compiler/aco_opcodes.py
@@ -158,7 +158,7 @@ class Format(Enum):
          return [('uint8_t', 'opsel_lo', None),
                  ('uint8_t', 'opsel_hi', None)]
       elif self in [Format.FLAT, Format.GLOBAL, Format.SCRATCH]:
-         return [('uint16_t', 'offset', 0),
+         return [('int16_t', 'offset', 0),
                  ('memory_sync_info', 'sync', 'memory_sync_info()'),
                  ('bool', 'glc', 'false'),
                  ('bool', 'slc', 'false'),
diff --git a/src/amd/compiler/aco_print_ir.cpp b/src/amd/compiler/aco_print_ir.cpp
index 11ede169dca5..49f133f873c7 100644
--- a/src/amd/compiler/aco_print_ir.cpp
+++ b/src/amd/compiler/aco_print_ir.cpp
@@ -484,7 +484,7 @@ print_instr_format_specific(const Instruction* instr, FILE* output)
    case Format::SCRATCH: {
       const FLAT_instruction& flat = instr->flatlike();
       if (flat.offset)
-         fprintf(output, " offset:%u", flat.offset);
+         fprintf(output, " offset:%d", flat.offset);
       if (flat.glc)
          fprintf(output, " glc");
       if (flat.dlc)
-- 
GitLab


From 0270f334457caed57bb2abcd18924ff6feabf837 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Thu, 19 May 2022 15:55:53 +0100
Subject: [PATCH 04/13] aco: improve support for scratch_* instructions
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
Reviewed-by: Daniel Schürmann <daniel@schuermann.dev>
---
 src/amd/compiler/aco_assembler.cpp      |  6 +++++-
 src/amd/compiler/aco_builder_h.py       |  3 ++-
 src/amd/compiler/aco_insert_waitcnt.cpp |  3 ++-
 src/amd/compiler/aco_validate.cpp       | 14 +++++++++++---
 4 files changed, 20 insertions(+), 6 deletions(-)

diff --git a/src/amd/compiler/aco_assembler.cpp b/src/amd/compiler/aco_assembler.cpp
index e302795869da..0a6feff5f4b7 100644
--- a/src/amd/compiler/aco_assembler.cpp
+++ b/src/amd/compiler/aco_assembler.cpp
@@ -547,7 +547,11 @@ emit_instruction(asm_context& ctx, std::vector<uint32_t>& out, Instruction* inst
          encoding |= instr->operands[1].physReg() << 16;
       } else if (instr->format != Format::FLAT ||
                  ctx.gfx_level >= GFX10) { /* SADDR is actually used with FLAT on GFX10 */
-         if (ctx.gfx_level <= GFX9)
+         /* For GFX10.3 scratch, 0x7F disables both ADDR and SADDR, unlike sgpr_null, which only
+          * disables SADDR.
+          */
+         if (ctx.gfx_level <= GFX9 ||
+             (instr->format == Format::SCRATCH && instr->operands[0].isUndefined()))
             encoding |= 0x7F << 16;
          else
             encoding |= sgpr_null << 16;
diff --git a/src/amd/compiler/aco_builder_h.py b/src/amd/compiler/aco_builder_h.py
index 8177c36207c4..e62ad43c0e67 100644
--- a/src/amd/compiler/aco_builder_h.py
+++ b/src/amd/compiler/aco_builder_h.py
@@ -541,7 +541,8 @@ formats = [("pseudo", [Format.PSEUDO], 'Pseudo_instruction', list(itertools.prod
            ("vop2_e64", [Format.VOP2, Format.VOP3], 'VOP3_instruction', itertools.product([1, 2], [2, 3])),
            ("vopc_e64", [Format.VOPC, Format.VOP3], 'VOP3_instruction', itertools.product([1, 2], [2])),
            ("flat", [Format.FLAT], 'FLAT_instruction', [(0, 3), (1, 2)]),
-           ("global", [Format.GLOBAL], 'FLAT_instruction', [(0, 3), (1, 2)])]
+           ("global", [Format.GLOBAL], 'FLAT_instruction', [(0, 3), (1, 2)]),
+           ("scratch", [Format.SCRATCH], 'FLAT_instruction', [(0, 3), (1, 2)])]
 formats = [(f if len(f) == 5 else f + ('',)) for f in formats]
 %>\\
 % for name, formats, struct, shapes, extra_field_setup in formats:
diff --git a/src/amd/compiler/aco_insert_waitcnt.cpp b/src/amd/compiler/aco_insert_waitcnt.cpp
index ad41b5315d89..076f636f266f 100644
--- a/src/amd/compiler/aco_insert_waitcnt.cpp
+++ b/src/amd/compiler/aco_insert_waitcnt.cpp
@@ -673,7 +673,8 @@ gen(Instruction* instr, wait_ctx& ctx)
    case Format::MUBUF:
    case Format::MTBUF:
    case Format::MIMG:
-   case Format::GLOBAL: {
+   case Format::GLOBAL:
+   case Format::SCRATCH: {
       wait_event ev =
          !instr->definitions.empty() || ctx.gfx_level < GFX10 ? event_vmem : event_vmem_store;
       update_counters(ctx, ev, get_sync_info(instr));
diff --git a/src/amd/compiler/aco_validate.cpp b/src/amd/compiler/aco_validate.cpp
index 87b5beefbaf5..db013e183530 100644
--- a/src/amd/compiler/aco_validate.cpp
+++ b/src/amd/compiler/aco_validate.cpp
@@ -262,7 +262,8 @@ validate_ir(Program* program)
                bool can_be_undef = is_phi(instr) || instr->isEXP() || instr->isReduction() ||
                                    instr->opcode == aco_opcode::p_create_vector ||
                                    (flat && i == 1) || (instr->isMIMG() && (i == 1 || i == 2)) ||
-                                   ((instr->isMUBUF() || instr->isMTBUF()) && i == 1);
+                                   ((instr->isMUBUF() || instr->isMTBUF()) && i == 1) ||
+                                   (instr->isScratch() && i == 0);
                check(can_be_undef, "Undefs can only be used in certain operands", instr.get());
             } else {
                check(instr->operands[i].isFixed() || instr->operands[i].isTemp() ||
@@ -658,13 +659,20 @@ validate_ir(Program* program)
                   instr.get());
             FALLTHROUGH;
          case Format::GLOBAL:
-         case Format::SCRATCH: {
             check(
                instr->operands[0].isTemp() && instr->operands[0].regClass().type() == RegType::vgpr,
-               "FLAT/GLOBAL/SCRATCH address must be vgpr", instr.get());
+               "FLAT/GLOBAL address must be vgpr", instr.get());
+            FALLTHROUGH;
+         case Format::SCRATCH: {
+            check(instr->operands[0].hasRegClass() &&
+                     instr->operands[0].regClass().type() == RegType::vgpr,
+                  "FLAT/GLOBAL/SCRATCH address must be undefined or vgpr", instr.get());
             check(instr->operands[1].hasRegClass() &&
                      instr->operands[1].regClass().type() == RegType::sgpr,
                   "FLAT/GLOBAL/SCRATCH sgpr address must be undefined or sgpr", instr.get());
+            if (instr->format == Format::SCRATCH && program->gfx_level < GFX10_3)
+               check(instr->operands[0].isTemp() || instr->operands[1].isTemp(),
+                     "SCRATCH must have either SADDR or ADDR operand", instr.get());
             if (!instr->definitions.empty())
                check(instr->definitions[0].getTemp().type() == RegType::vgpr,
                      "FLAT/GLOBAL/SCRATCH result must be vgpr", instr.get());
-- 
GitLab


From 1a31e9f8ed0f48cffdcaf922264ab11675f8193a Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Thu, 19 May 2022 15:19:12 +0100
Subject: [PATCH 05/13] aco: combine additions and constants into scratch
 load/store
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
Reviewed-by: Daniel Schürmann <daniel@schuermann.dev>
---
 src/amd/compiler/aco_optimizer.cpp | 38 ++++++++++++++++++++++++++++++
 1 file changed, 38 insertions(+)

diff --git a/src/amd/compiler/aco_optimizer.cpp b/src/amd/compiler/aco_optimizer.cpp
index 87d824b8f8b6..f8ceb61f276b 100644
--- a/src/amd/compiler/aco_optimizer.cpp
+++ b/src/amd/compiler/aco_optimizer.cpp
@@ -1233,6 +1233,20 @@ is_op_canonicalized(opt_ctx& ctx, Operand op)
    return false;
 }
 
+bool
+is_scratch_offset_valid(opt_ctx& ctx, Instruction* instr, int32_t offset)
+{
+   bool negative_unaligned_scratch_offset_bug = ctx.program->gfx_level == GFX10;
+   int32_t min = ctx.program->dev.scratch_global_offset_min;
+   int32_t max = ctx.program->dev.scratch_global_offset_max;
+
+   bool has_vgpr_offset = instr && !instr->operands[0].isUndefined();
+   if (negative_unaligned_scratch_offset_bug && has_vgpr_offset && offset < 0 && offset % 4)
+      return false;
+
+   return offset >= min && offset <= max;
+}
+
 void
 label_instruction(opt_ctx& ctx, aco_ptr<Instruction>& instr)
 {
@@ -1416,6 +1430,30 @@ label_instruction(opt_ctx& ctx, aco_ptr<Instruction>& instr)
          }
       }
 
+      /* SCRATCH: propagate constants and combine additions */
+      else if (instr->isScratch()) {
+         FLAT_instruction& scratch = instr->scratch();
+         Temp base;
+         uint32_t offset;
+         while (info.is_temp())
+            info = ctx.info[info.temp.id()];
+
+         if (i <= 1 && parse_base_offset(ctx, instr.get(), i, &base, &offset, false) &&
+             base.regClass() == instr->operands[i].regClass() &&
+             is_scratch_offset_valid(ctx, instr.get(), scratch.offset + (int32_t)offset)) {
+            instr->operands[i].setTemp(base);
+            scratch.offset += (int32_t)offset;
+            continue;
+         } else if (i <= 1 && info.is_constant_or_literal(32) &&
+                    ctx.program->gfx_level >= GFX10_3 &&
+                    is_scratch_offset_valid(ctx, NULL, scratch.offset + (int32_t)info.val)) {
+            /* GFX10.3+ can disable both SADDR and ADDR. */
+            instr->operands[i] = Operand(instr->operands[i].regClass());
+            scratch.offset += (int32_t)info.val;
+            continue;
+         }
+      }
+
       /* DS: combine additions */
       else if (instr->isDS()) {
 
-- 
GitLab


From 73881e1a4910ca98c06821f12c89d1f2585988c2 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Thu, 19 May 2022 15:34:04 +0100
Subject: [PATCH 06/13] aco: handle subtractions in parse_base_offset
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
Reviewed-by: Daniel Schürmann <daniel@schuermann.dev>
---
 src/amd/compiler/aco_optimizer.cpp | 32 +++++++++++++++++++++++++++---
 1 file changed, 29 insertions(+), 3 deletions(-)

diff --git a/src/amd/compiler/aco_optimizer.cpp b/src/amd/compiler/aco_optimizer.cpp
index f8ceb61f276b..e7c2cac53c76 100644
--- a/src/amd/compiler/aco_optimizer.cpp
+++ b/src/amd/compiler/aco_optimizer.cpp
@@ -755,12 +755,29 @@ parse_base_offset(opt_ctx& ctx, Instruction* instr, unsigned op_index, Temp* bas
 
    Instruction* add_instr = ctx.info[tmp.id()].instr;
 
+   unsigned mask = 0x3;
+   bool is_sub = false;
    switch (add_instr->opcode) {
    case aco_opcode::v_add_u32:
    case aco_opcode::v_add_co_u32:
    case aco_opcode::v_add_co_u32_e64:
    case aco_opcode::s_add_i32:
    case aco_opcode::s_add_u32: break;
+   case aco_opcode::v_sub_u32:
+   case aco_opcode::v_sub_i32:
+   case aco_opcode::v_sub_co_u32:
+   case aco_opcode::v_sub_co_u32_e64:
+   case aco_opcode::s_sub_u32:
+   case aco_opcode::s_sub_i32:
+      mask = 0x2;
+      is_sub = true;
+      break;
+   case aco_opcode::v_subrev_u32:
+   case aco_opcode::v_subrev_co_u32:
+   case aco_opcode::v_subrev_co_u32_e64:
+      mask = 0x1;
+      is_sub = true;
+      break;
    default: return false;
    }
    if (prevent_overflow && !add_instr->definitions[0].isNUW())
@@ -769,12 +786,12 @@ parse_base_offset(opt_ctx& ctx, Instruction* instr, unsigned op_index, Temp* bas
    if (add_instr->usesModifiers())
       return false;
 
-   for (unsigned i = 0; i < 2; i++) {
+   u_foreach_bit (i, mask) {
       if (add_instr->operands[i].isConstant()) {
-         *offset = add_instr->operands[i].constantValue();
+         *offset = add_instr->operands[i].constantValue() * (uint32_t)(is_sub ? -1 : 1);
       } else if (add_instr->operands[i].isTemp() &&
                  ctx.info[add_instr->operands[i].tempId()].is_constant_or_literal(32)) {
-         *offset = ctx.info[add_instr->operands[i].tempId()].val;
+         *offset = ctx.info[add_instr->operands[i].tempId()].val * (uint32_t)(is_sub ? -1 : 1);
       } else {
          continue;
       }
@@ -1873,6 +1890,15 @@ label_instruction(opt_ctx& ctx, aco_ptr<Instruction>& instr)
    case aco_opcode::s_add_i32:
    case aco_opcode::s_add_u32:
    case aco_opcode::v_subbrev_co_u32:
+   case aco_opcode::v_sub_u32:
+   case aco_opcode::v_sub_i32:
+   case aco_opcode::v_sub_co_u32:
+   case aco_opcode::v_sub_co_u32_e64:
+   case aco_opcode::s_sub_u32:
+   case aco_opcode::s_sub_i32:
+   case aco_opcode::v_subrev_u32:
+   case aco_opcode::v_subrev_co_u32:
+   case aco_opcode::v_subrev_co_u32_e64:
       ctx.info[instr->definitions[0].tempId()].set_add_sub(instr.get());
       break;
    case aco_opcode::s_not_b32:
-- 
GitLab


From a08c5dfb946642d41cb6ed9d5accb027b5406482 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Thu, 19 May 2022 16:09:13 +0100
Subject: [PATCH 07/13] aco: refactor VGPR spill/reload lowering
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
Reviewed-by: Daniel Schürmann <daniel@schuermann.dev>
---
 src/amd/compiler/aco_spill.cpp | 239 +++++++++++++++++----------------
 1 file changed, 124 insertions(+), 115 deletions(-)

diff --git a/src/amd/compiler/aco_spill.cpp b/src/amd/compiler/aco_spill.cpp
index 70fbc1aa298d..848df3dfe953 100644
--- a/src/amd/compiler/aco_spill.cpp
+++ b/src/amd/compiler/aco_spill.cpp
@@ -82,6 +82,10 @@ struct spill_ctx {
    std::set<Instruction*> unused_remats;
    unsigned wave_size;
 
+   unsigned sgpr_spill_slots;
+   unsigned vgpr_spill_slots;
+   Temp scratch_rsrc;
+
    spill_ctx(const RegisterDemand target_pressure_, Program* program_,
              std::vector<std::vector<RegisterDemand>> register_demand_)
        : target_pressure(target_pressure_), program(program_),
@@ -1383,19 +1387,25 @@ spill_block(spill_ctx& ctx, unsigned block_idx)
 }
 
 Temp
-load_scratch_resource(spill_ctx& ctx, Temp& scratch_offset,
-                      std::vector<aco_ptr<Instruction>>& instructions, unsigned offset,
-                      bool is_top_level)
+load_scratch_resource(spill_ctx& ctx, Temp& scratch_offset, Block& block,
+                      std::vector<aco_ptr<Instruction>>& instructions, unsigned offset)
 {
    Builder bld(ctx.program);
-   if (is_top_level) {
+   if (block.kind & block_kind_top_level) {
       bld.reset(&instructions);
    } else {
-      /* find p_logical_end */
-      unsigned idx = instructions.size() - 1;
-      while (instructions[idx]->opcode != aco_opcode::p_logical_end)
-         idx--;
-      bld.reset(&instructions, std::next(instructions.begin(), idx));
+      for (int block_idx = block.index; block_idx >= 0; block_idx--) {
+         if (!(ctx.program->blocks[block_idx].kind & block_kind_top_level))
+            continue;
+
+         /* find p_logical_end */
+         std::vector<aco_ptr<Instruction>>& prev_instructions = ctx.program->blocks[block_idx].instructions;
+         unsigned idx = prev_instructions.size() - 1;
+         while (prev_instructions[idx]->opcode != aco_opcode::p_logical_end)
+            idx--;
+         bld.reset(&prev_instructions, std::next(prev_instructions.begin(), idx));
+         break;
+      }
    }
 
    Temp private_segment_buffer = ctx.program->private_segment_buffer;
@@ -1427,6 +1437,99 @@ load_scratch_resource(spill_ctx& ctx, Temp& scratch_offset,
                      Operand::c32(-1u), Operand::c32(rsrc_conf));
 }
 
+void
+setup_vgpr_spill_reload(spill_ctx& ctx, Block& block,
+                        std::vector<aco_ptr<Instruction>>& instructions, uint32_t spill_slot,
+                        unsigned* offset)
+{
+   Temp scratch_offset = ctx.program->scratch_offset;
+
+   *offset = spill_slot * 4;
+
+   bool add_offset_to_sgpr = ctx.program->config->scratch_bytes_per_wave / ctx.program->wave_size +
+                                ctx.vgpr_spill_slots * 4 >
+                             4096;
+   if (!add_offset_to_sgpr)
+      *offset += ctx.program->config->scratch_bytes_per_wave / ctx.program->wave_size;
+
+   if (ctx.scratch_rsrc == Temp()) {
+      unsigned rsrc_offset = add_offset_to_sgpr ? ctx.program->config->scratch_bytes_per_wave : 0;
+      ctx.scratch_rsrc =
+         load_scratch_resource(ctx, scratch_offset, block, instructions, rsrc_offset);
+   }
+}
+
+void
+spill_vgpr(spill_ctx& ctx, Block& block, std::vector<aco_ptr<Instruction>>& instructions,
+           aco_ptr<Instruction>& spill, std::vector<uint32_t>& slots)
+{
+   ctx.program->config->spilled_vgprs += spill->operands[0].size();
+
+   uint32_t spill_id = spill->operands[1].constantValue();
+   uint32_t spill_slot = slots[spill_id];
+
+   unsigned offset;
+   setup_vgpr_spill_reload(ctx, block, instructions, spill_slot, &offset);
+
+   assert(spill->operands[0].isTemp());
+   Temp temp = spill->operands[0].getTemp();
+   assert(temp.type() == RegType::vgpr && !temp.is_linear());
+
+   Builder bld(ctx.program, &instructions);
+   if (temp.size() > 1) {
+      Instruction* split{create_instruction<Pseudo_instruction>(aco_opcode::p_split_vector,
+                                                                Format::PSEUDO, 1, temp.size())};
+      split->operands[0] = Operand(temp);
+      for (unsigned i = 0; i < temp.size(); i++)
+         split->definitions[i] = bld.def(v1);
+      bld.insert(split);
+      for (unsigned i = 0; i < temp.size(); i++, offset += 4) {
+         Temp elem = split->definitions[i].getTemp();
+         Instruction* instr =
+            bld.mubuf(aco_opcode::buffer_store_dword, ctx.scratch_rsrc, Operand(v1),
+                      ctx.program->scratch_offset, elem, offset, false, true);
+         instr->mubuf().sync = memory_sync_info(storage_vgpr_spill, semantic_private);
+      }
+   } else {
+      Instruction* instr = bld.mubuf(aco_opcode::buffer_store_dword, ctx.scratch_rsrc, Operand(v1),
+                                     ctx.program->scratch_offset, temp, offset, false, true);
+      instr->mubuf().sync = memory_sync_info(storage_vgpr_spill, semantic_private);
+   }
+}
+
+void
+reload_vgpr(spill_ctx& ctx, Block& block, std::vector<aco_ptr<Instruction>>& instructions,
+            aco_ptr<Instruction>& reload, std::vector<uint32_t>& slots)
+{
+   uint32_t spill_id = reload->operands[0].constantValue();
+   uint32_t spill_slot = slots[spill_id];
+
+   unsigned offset;
+   setup_vgpr_spill_reload(ctx, block, instructions, spill_slot, &offset);
+
+   Definition def = reload->definitions[0];
+
+   Builder bld(ctx.program, &instructions);
+   if (def.size() > 1) {
+      Instruction* vec{create_instruction<Pseudo_instruction>(aco_opcode::p_create_vector,
+                                                              Format::PSEUDO, def.size(), 1)};
+      vec->definitions[0] = def;
+      for (unsigned i = 0; i < def.size(); i++, offset += 4) {
+         Temp tmp = bld.tmp(v1);
+         vec->operands[i] = Operand(tmp);
+         Instruction* instr =
+            bld.mubuf(aco_opcode::buffer_load_dword, Definition(tmp), ctx.scratch_rsrc, Operand(v1),
+                      ctx.program->scratch_offset, offset, false, true);
+         instr->mubuf().sync = memory_sync_info(storage_vgpr_spill, semantic_private);
+      }
+      bld.insert(vec);
+   } else {
+      Instruction* instr = bld.mubuf(aco_opcode::buffer_load_dword, def, ctx.scratch_rsrc,
+                                     Operand(v1), ctx.program->scratch_offset, offset, false, true);
+      instr->mubuf().sync = memory_sync_info(storage_vgpr_spill, semantic_private);
+   }
+}
+
 void
 add_interferences(spill_ctx& ctx, std::vector<bool>& is_assigned, std::vector<uint32_t>& slots,
                   std::vector<bool>& slots_used, unsigned id)
@@ -1442,8 +1545,7 @@ add_interferences(spill_ctx& ctx, std::vector<bool>& is_assigned, std::vector<ui
 }
 
 unsigned
-find_available_slot(std::vector<bool>& used, unsigned wave_size, unsigned size, bool is_sgpr,
-                    unsigned* num_slots)
+find_available_slot(std::vector<bool>& used, unsigned wave_size, unsigned size, bool is_sgpr)
 {
    unsigned wave_size_minus_one = wave_size - 1;
    unsigned slot = 0;
@@ -1479,7 +1581,7 @@ void
 assign_spill_slots_helper(spill_ctx& ctx, RegType type, std::vector<bool>& is_assigned,
                           std::vector<uint32_t>& slots, unsigned* num_slots)
 {
-   std::vector<bool> slots_used(*num_slots);
+   std::vector<bool> slots_used;
 
    /* assign slots for ids with affinities first */
    for (std::vector<uint32_t>& vec : ctx.affinities) {
@@ -1493,9 +1595,8 @@ assign_spill_slots_helper(spill_ctx& ctx, RegType type, std::vector<bool>& is_as
          add_interferences(ctx, is_assigned, slots, slots_used, id);
       }
 
-      unsigned slot =
-         find_available_slot(slots_used, ctx.wave_size, ctx.interferences[vec[0]].first.size(),
-                             type == RegType::sgpr, num_slots);
+      unsigned slot = find_available_slot(
+         slots_used, ctx.wave_size, ctx.interferences[vec[0]].first.size(), type == RegType::sgpr);
 
       for (unsigned id : vec) {
          assert(!is_assigned[id]);
@@ -1514,9 +1615,8 @@ assign_spill_slots_helper(spill_ctx& ctx, RegType type, std::vector<bool>& is_as
 
       add_interferences(ctx, is_assigned, slots, slots_used, id);
 
-      unsigned slot =
-         find_available_slot(slots_used, ctx.wave_size, ctx.interferences[id].first.size(),
-                             type == RegType::sgpr, num_slots);
+      unsigned slot = find_available_slot(
+         slots_used, ctx.wave_size, ctx.interferences[id].first.size(), type == RegType::sgpr);
 
       slots[id] = slot;
       is_assigned[id] = true;
@@ -1547,9 +1647,8 @@ assign_spill_slots(spill_ctx& ctx, unsigned spills_to_vgpr)
          assert(i != id);
 
    /* for each spill slot, assign as many spill ids as possible */
-   unsigned sgpr_spill_slots = 0, vgpr_spill_slots = 0;
-   assign_spill_slots_helper(ctx, RegType::sgpr, is_assigned, slots, &sgpr_spill_slots);
-   assign_spill_slots_helper(ctx, RegType::vgpr, is_assigned, slots, &vgpr_spill_slots);
+   assign_spill_slots_helper(ctx, RegType::sgpr, is_assigned, slots, &ctx.sgpr_spill_slots);
+   assign_spill_slots_helper(ctx, RegType::vgpr, is_assigned, slots, &ctx.vgpr_spill_slots);
 
    for (unsigned id = 0; id < is_assigned.size(); id++)
       assert(is_assigned[id] || !ctx.is_reloaded[id]);
@@ -1569,11 +1668,10 @@ assign_spill_slots(spill_ctx& ctx, unsigned spills_to_vgpr)
    }
 
    /* hope, we didn't mess up */
-   std::vector<Temp> vgpr_spill_temps((sgpr_spill_slots + ctx.wave_size - 1) / ctx.wave_size);
+   std::vector<Temp> vgpr_spill_temps((ctx.sgpr_spill_slots + ctx.wave_size - 1) / ctx.wave_size);
    assert(vgpr_spill_temps.size() <= spills_to_vgpr);
 
    /* replace pseudo instructions with actual hardware instructions */
-   Temp scratch_offset = ctx.program->scratch_offset, scratch_rsrc = Temp();
    unsigned last_top_level_block_idx = 0;
    std::vector<bool> reload_in_loop(vgpr_spill_temps.size());
    for (Block& block : ctx.program->blocks) {
@@ -1639,53 +1737,7 @@ assign_spill_slots(spill_ctx& ctx, unsigned spills_to_vgpr)
             } else if (!is_assigned[spill_id]) {
                unreachable("No spill slot assigned for spill id");
             } else if (ctx.interferences[spill_id].first.type() == RegType::vgpr) {
-               /* spill vgpr */
-               ctx.program->config->spilled_vgprs += (*it)->operands[0].size();
-               uint32_t spill_slot = slots[spill_id];
-               bool add_offset_to_sgpr =
-                  ctx.program->config->scratch_bytes_per_wave / ctx.program->wave_size +
-                     vgpr_spill_slots * 4 >
-                  4096;
-               unsigned base_offset =
-                  add_offset_to_sgpr
-                     ? 0
-                     : ctx.program->config->scratch_bytes_per_wave / ctx.program->wave_size;
-
-               /* check if the scratch resource descriptor already exists */
-               if (scratch_rsrc == Temp()) {
-                  unsigned offset =
-                     add_offset_to_sgpr ? ctx.program->config->scratch_bytes_per_wave : 0;
-                  scratch_rsrc = load_scratch_resource(
-                     ctx, scratch_offset,
-                     last_top_level_block_idx == block.index
-                        ? instructions
-                        : ctx.program->blocks[last_top_level_block_idx].instructions,
-                     offset, last_top_level_block_idx == block.index);
-               }
-
-               unsigned offset = base_offset + spill_slot * 4;
-               aco_opcode opcode = aco_opcode::buffer_store_dword;
-               assert((*it)->operands[0].isTemp());
-               Temp temp = (*it)->operands[0].getTemp();
-               assert(temp.type() == RegType::vgpr && !temp.is_linear());
-               if (temp.size() > 1) {
-                  Instruction* split{create_instruction<Pseudo_instruction>(
-                     aco_opcode::p_split_vector, Format::PSEUDO, 1, temp.size())};
-                  split->operands[0] = Operand(temp);
-                  for (unsigned i = 0; i < temp.size(); i++)
-                     split->definitions[i] = bld.def(v1);
-                  bld.insert(split);
-                  for (unsigned i = 0; i < temp.size(); i++) {
-                     Instruction* instr =
-                        bld.mubuf(opcode, scratch_rsrc, Operand(v1), scratch_offset,
-                                  split->definitions[i].getTemp(), offset + i * 4, false, true);
-                     instr->mubuf().sync = memory_sync_info(storage_vgpr_spill, semantic_private);
-                  }
-               } else {
-                  Instruction* instr = bld.mubuf(opcode, scratch_rsrc, Operand(v1), scratch_offset,
-                                                 temp, offset, false, true);
-                  instr->mubuf().sync = memory_sync_info(storage_vgpr_spill, semantic_private);
-               }
+               spill_vgpr(ctx, block, instructions, *it, slots);
             } else {
                ctx.program->config->spilled_sgprs += (*it)->operands[0].size();
 
@@ -1727,50 +1779,7 @@ assign_spill_slots(spill_ctx& ctx, unsigned spills_to_vgpr)
             if (!is_assigned[spill_id]) {
                unreachable("No spill slot assigned for spill id");
             } else if (ctx.interferences[spill_id].first.type() == RegType::vgpr) {
-               /* reload vgpr */
-               uint32_t spill_slot = slots[spill_id];
-               bool add_offset_to_sgpr =
-                  ctx.program->config->scratch_bytes_per_wave / ctx.program->wave_size +
-                     vgpr_spill_slots * 4 >
-                  4096;
-               unsigned base_offset =
-                  add_offset_to_sgpr
-                     ? 0
-                     : ctx.program->config->scratch_bytes_per_wave / ctx.program->wave_size;
-
-               /* check if the scratch resource descriptor already exists */
-               if (scratch_rsrc == Temp()) {
-                  unsigned offset =
-                     add_offset_to_sgpr ? ctx.program->config->scratch_bytes_per_wave : 0;
-                  scratch_rsrc = load_scratch_resource(
-                     ctx, scratch_offset,
-                     last_top_level_block_idx == block.index
-                        ? instructions
-                        : ctx.program->blocks[last_top_level_block_idx].instructions,
-                     offset, last_top_level_block_idx == block.index);
-               }
-
-               unsigned offset = base_offset + spill_slot * 4;
-               aco_opcode opcode = aco_opcode::buffer_load_dword;
-               Definition def = (*it)->definitions[0];
-               if (def.size() > 1) {
-                  Instruction* vec{create_instruction<Pseudo_instruction>(
-                     aco_opcode::p_create_vector, Format::PSEUDO, def.size(), 1)};
-                  vec->definitions[0] = def;
-                  for (unsigned i = 0; i < def.size(); i++) {
-                     Temp tmp = bld.tmp(v1);
-                     vec->operands[i] = Operand(tmp);
-                     Instruction* instr =
-                        bld.mubuf(opcode, Definition(tmp), scratch_rsrc, Operand(v1),
-                                  scratch_offset, offset + i * 4, false, true);
-                     instr->mubuf().sync = memory_sync_info(storage_vgpr_spill, semantic_private);
-                  }
-                  bld.insert(vec);
-               } else {
-                  Instruction* instr = bld.mubuf(opcode, def, scratch_rsrc, Operand(v1),
-                                                 scratch_offset, offset, false, true);
-                  instr->mubuf().sync = memory_sync_info(storage_vgpr_spill, semantic_private);
-               }
+               reload_vgpr(ctx, block, instructions, *it, slots);
             } else {
                uint32_t spill_slot = slots[spill_id];
                reload_in_loop[spill_slot / ctx.wave_size] = block.loop_nest_depth > 0;
@@ -1812,7 +1821,7 @@ assign_spill_slots(spill_ctx& ctx, unsigned spills_to_vgpr)
 
    /* update required scratch memory */
    ctx.program->config->scratch_bytes_per_wave +=
-      align(vgpr_spill_slots * 4 * ctx.program->wave_size, 1024);
+      align(ctx.vgpr_spill_slots * 4 * ctx.program->wave_size, 1024);
 
    /* SSA elimination inserts copies for logical phis right before p_logical_end
     * So if a linear vgpr is used between that p_logical_end and the branch,
-- 
GitLab


From 220060ddbd1b4b4d8e250f7040e5218918900266 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Wed, 25 May 2022 17:21:10 +0100
Subject: [PATCH 08/13] aco: avoid WAW hazard with BVH MIMG and other VMEM
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

According to LLVM, image_bvh64_intersect_ray does not write results in
order with other VMEM instructions.

fossil-db (navi21):
Totals from 7 (0.00% of 162293) affected shaders:
Instrs: 39978 -> 39985 (+0.02%)
CodeSize: 219356 -> 219384 (+0.01%)

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
Reviewed-by: Daniel Schürmann <daniel@schuermann.dev>
---
 src/amd/compiler/aco_insert_waitcnt.cpp | 61 ++++++++++++++-----------
 1 file changed, 34 insertions(+), 27 deletions(-)

diff --git a/src/amd/compiler/aco_insert_waitcnt.cpp b/src/amd/compiler/aco_insert_waitcnt.cpp
index 076f636f266f..9d6f0991316c 100644
--- a/src/amd/compiler/aco_insert_waitcnt.cpp
+++ b/src/amd/compiler/aco_insert_waitcnt.cpp
@@ -79,6 +79,12 @@ enum counter_type : uint8_t {
    num_counters = 4,
 };
 
+enum vmem_type : uint8_t {
+   vmem_nosampler = 1 << 0,
+   vmem_sampler = 1 << 1,
+   vmem_bvh = 1 << 2,
+};
+
 static const uint16_t exp_events =
    event_exp_pos | event_exp_param | event_exp_mrt_null | event_gds_gpr_lock | event_vmem_gpr_lock;
 static const uint16_t lgkm_events = event_smem | event_lds | event_gds | event_flat | event_sendmsg;
@@ -111,27 +117,22 @@ struct wait_entry {
    uint8_t counters; /* use counter_type notion */
    bool wait_on_read : 1;
    bool logical : 1;
-   bool has_vmem_nosampler : 1;
-   bool has_vmem_sampler : 1;
+   uint8_t vmem_types : 4;
 
    wait_entry(wait_event event_, wait_imm imm_, bool logical_, bool wait_on_read_)
        : imm(imm_), events(event_), counters(get_counters_for_event(event_)),
-         wait_on_read(wait_on_read_), logical(logical_), has_vmem_nosampler(false),
-         has_vmem_sampler(false)
+         wait_on_read(wait_on_read_), logical(logical_), vmem_types(0)
    {}
 
    bool join(const wait_entry& other)
    {
       bool changed = (other.events & ~events) || (other.counters & ~counters) ||
-                     (other.wait_on_read && !wait_on_read) ||
-                     (other.has_vmem_nosampler && !has_vmem_nosampler) ||
-                     (other.has_vmem_sampler && !has_vmem_sampler);
+                     (other.wait_on_read && !wait_on_read) || (other.vmem_types & !vmem_types);
       events |= other.events;
       counters |= other.counters;
       changed |= imm.combine(other.imm);
       wait_on_read |= other.wait_on_read;
-      has_vmem_nosampler |= other.has_vmem_nosampler;
-      has_vmem_sampler |= other.has_vmem_sampler;
+      vmem_types |= other.vmem_types;
       assert(logical == other.logical);
       return changed;
    }
@@ -148,8 +149,7 @@ struct wait_entry {
       if (counter == counter_vm) {
          imm.vm = wait_imm::unset_counter;
          events &= ~event_vmem;
-         has_vmem_nosampler = false;
-         has_vmem_sampler = false;
+         vmem_types = 0;
       }
 
       if (counter == counter_exp) {
@@ -242,6 +242,19 @@ struct wait_ctx {
    }
 };
 
+uint8_t
+get_vmem_type(Instruction* instr)
+{
+   if (instr->opcode == aco_opcode::image_bvh64_intersect_ray)
+      return vmem_bvh;
+   else if (instr->isMIMG() && !instr->operands[1].isUndefined() &&
+            instr->operands[1].regClass() == s4)
+      return vmem_sampler;
+   else if (instr->isVMEM())
+      return vmem_nosampler;
+   return 0;
+}
+
 void
 check_instr(wait_ctx& ctx, wait_imm& wait, Instruction* instr)
 {
@@ -270,11 +283,9 @@ check_instr(wait_ctx& ctx, wait_imm& wait, Instruction* instr)
             continue;
 
          /* Vector Memory reads and writes return in the order they were issued */
-         bool has_sampler = instr->isMIMG() && !instr->operands[1].isUndefined() &&
-                            instr->operands[1].regClass() == s4;
-         if (instr->isVMEM() && ((it->second.events & vm_events) == event_vmem) &&
-             it->second.has_vmem_nosampler == !has_sampler &&
-             it->second.has_vmem_sampler == has_sampler)
+         uint8_t vmem_type = get_vmem_type(instr);
+         if (vmem_type && ((it->second.events & vm_events) == event_vmem) &&
+             it->second.vmem_types == vmem_type)
             continue;
 
          /* LDS reads and writes return in the order they were issued. same for GDS */
@@ -568,7 +579,7 @@ update_counters_for_flat_load(wait_ctx& ctx, memory_sync_info sync = memory_sync
 
 void
 insert_wait_entry(wait_ctx& ctx, PhysReg reg, RegClass rc, wait_event event, bool wait_on_read,
-                  bool has_sampler = false)
+                  uint8_t vmem_types = 0)
 {
    uint16_t counters = get_counters_for_event(event);
    wait_imm imm;
@@ -582,8 +593,7 @@ insert_wait_entry(wait_ctx& ctx, PhysReg reg, RegClass rc, wait_event event, boo
       imm.vs = 0;
 
    wait_entry new_entry(event, imm, !rc.is_linear(), wait_on_read);
-   new_entry.has_vmem_nosampler = (event & event_vmem) && !has_sampler;
-   new_entry.has_vmem_sampler = (event & event_vmem) && has_sampler;
+   new_entry.vmem_types |= vmem_types;
 
    for (unsigned i = 0; i < rc.size(); i++) {
       auto it = ctx.gpr_map.emplace(PhysReg{reg.reg() + i}, new_entry);
@@ -593,16 +603,16 @@ insert_wait_entry(wait_ctx& ctx, PhysReg reg, RegClass rc, wait_event event, boo
 }
 
 void
-insert_wait_entry(wait_ctx& ctx, Operand op, wait_event event, bool has_sampler = false)
+insert_wait_entry(wait_ctx& ctx, Operand op, wait_event event, uint8_t vmem_types = 0)
 {
    if (!op.isConstant() && !op.isUndefined())
-      insert_wait_entry(ctx, op.physReg(), op.regClass(), event, false, has_sampler);
+      insert_wait_entry(ctx, op.physReg(), op.regClass(), event, false, vmem_types);
 }
 
 void
-insert_wait_entry(wait_ctx& ctx, Definition def, wait_event event, bool has_sampler = false)
+insert_wait_entry(wait_ctx& ctx, Definition def, wait_event event, uint8_t vmem_types = 0)
 {
-   insert_wait_entry(ctx, def.physReg(), def.regClass(), event, true, has_sampler);
+   insert_wait_entry(ctx, def.physReg(), def.regClass(), event, true, vmem_types);
 }
 
 void
@@ -679,11 +689,8 @@ gen(Instruction* instr, wait_ctx& ctx)
          !instr->definitions.empty() || ctx.gfx_level < GFX10 ? event_vmem : event_vmem_store;
       update_counters(ctx, ev, get_sync_info(instr));
 
-      bool has_sampler = instr->isMIMG() && !instr->operands[1].isUndefined() &&
-                         instr->operands[1].regClass() == s4;
-
       if (!instr->definitions.empty())
-         insert_wait_entry(ctx, instr->definitions[0], ev, has_sampler);
+         insert_wait_entry(ctx, instr->definitions[0], ev, get_vmem_type(instr));
 
       if (ctx.gfx_level == GFX6 && instr->format != Format::MIMG && instr->operands.size() == 4) {
          ctx.exp_cnt++;
-- 
GitLab


From 886ef18a4059fdba84af84ae9b3823eadd0b3ec4 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Wed, 25 May 2022 17:21:50 +0100
Subject: [PATCH 09/13] aco: include scratch/global in VMEM WAW optimization
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

fossil-db (navi21):
Totals from 2 (0.00% of 162293) affected shaders:
Instrs: 4788 -> 4785 (-0.06%)
CodeSize: 25884 -> 25872 (-0.05%)
Latency: 255008 -> 252950 (-0.81%)
InvThroughput: 170005 -> 168633 (-0.81%)
VClause: 206 -> 205 (-0.49%)

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
Reviewed-by: Daniel Schürmann <daniel@schuermann.dev>
---
 src/amd/compiler/aco_insert_waitcnt.cpp | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/src/amd/compiler/aco_insert_waitcnt.cpp b/src/amd/compiler/aco_insert_waitcnt.cpp
index 9d6f0991316c..b8be00c1fcfd 100644
--- a/src/amd/compiler/aco_insert_waitcnt.cpp
+++ b/src/amd/compiler/aco_insert_waitcnt.cpp
@@ -250,7 +250,7 @@ get_vmem_type(Instruction* instr)
    else if (instr->isMIMG() && !instr->operands[1].isUndefined() &&
             instr->operands[1].regClass() == s4)
       return vmem_sampler;
-   else if (instr->isVMEM())
+   else if (instr->isVMEM() || instr->isScratch() || instr->isGlobal())
       return vmem_nosampler;
    return 0;
 }
-- 
GitLab


From 5a8d0f66c437b62f699b20a30c941f2db9efe89d Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Thu, 19 May 2022 18:21:34 +0100
Subject: [PATCH 10/13] aco: treat flat-like as vmem in some scheduling
 heuristics
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

fossil-db (navi21):
Totals from 12 (0.01% of 162293) affected shaders:
Instrs: 48754 -> 48762 (+0.02%)
CodeSize: 267092 -> 267124 (+0.01%)
Latency: 1293798 -> 1292303 (-0.12%); split: -0.12%, +0.00%
InvThroughput: 854599 -> 853578 (-0.12%)
VClause: 1623 -> 1619 (-0.25%)
SClause: 1187 -> 1188 (+0.08%); split: -0.08%, +0.17%

fossil-db (vega10):
Totals from 1 (0.00% of 161355) affected shaders:
Latency: 18720 -> 18848 (+0.68%)
InvThroughput: 5775 -> 5776 (+0.02%)
SClause: 12 -> 11 (-8.33%)

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
Reviewed-by: Daniel Schürmann <daniel@schuermann.dev>
---
 src/amd/compiler/aco_scheduler.cpp | 9 +++++----
 1 file changed, 5 insertions(+), 4 deletions(-)

diff --git a/src/amd/compiler/aco_scheduler.cpp b/src/amd/compiler/aco_scheduler.cpp
index 6fc84fa0e7f8..3aeb30c5ccf6 100644
--- a/src/amd/compiler/aco_scheduler.cpp
+++ b/src/amd/compiler/aco_scheduler.cpp
@@ -676,8 +676,9 @@ schedule_SMEM(sched_ctx& ctx, Block* block, std::vector<RegisterDemand>& registe
          break;
       /* only move VMEM instructions below descriptor loads. be more aggressive at higher num_waves
        * to help create more vmem clauses */
-      if (candidate->isVMEM() && (cursor.insert_idx - cursor.source_idx > (ctx.num_waves * 4) ||
-                                  current->operands[0].size() == 4))
+      if ((candidate->isVMEM() || candidate->isFlatLike()) &&
+          (cursor.insert_idx - cursor.source_idx > (ctx.num_waves * 4) ||
+           current->operands[0].size() == 4))
          break;
       /* don't move descriptor loads below buffer loads */
       if (candidate->format == Format::SMEM && current->operands[0].size() == 4 &&
@@ -733,7 +734,7 @@ schedule_SMEM(sched_ctx& ctx, Block* block, std::vector<RegisterDemand>& registe
       /* check if candidate depends on current */
       bool is_dependency = !found_dependency && !ctx.mv.upwards_check_deps(up_cursor);
       /* no need to steal from following VMEM instructions */
-      if (is_dependency && candidate->isVMEM())
+      if (is_dependency && (candidate->isVMEM() || candidate->isFlatLike()))
          break;
 
       if (found_dependency) {
@@ -766,7 +767,7 @@ schedule_SMEM(sched_ctx& ctx, Block* block, std::vector<RegisterDemand>& registe
       MoveResult res = ctx.mv.upwards_move(up_cursor);
       if (res == move_fail_ssa || res == move_fail_rar) {
          /* no need to steal from following VMEM instructions */
-         if (res == move_fail_ssa && candidate->isVMEM())
+         if (res == move_fail_ssa && (candidate->isVMEM() || candidate->isFlatLike()))
             break;
          add_to_hazard_query(&hq, candidate.get());
          ctx.mv.upwards_skip(up_cursor);
-- 
GitLab


From f97b1e224971b38bdb73e3b5713e76f9bbbd99a9 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Thu, 19 May 2022 14:12:08 +0100
Subject: [PATCH 11/13] aco: initialize scratch base registers on GFX9-GFX10.3
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

fossil-db (navi21):
Totals from 1142 (0.70% of 162293) affected shaders:
Instrs: 271636 -> 271974 (+0.12%)
CodeSize: 1532020 -> 1533792 (+0.12%)
Latency: 7484066 -> 7485698 (+0.02%)
InvThroughput: 4048824 -> 4049579 (+0.02%)
SClause: 4171 -> 4212 (+0.98%)
PreSGPRs: 11203 -> 12276 (+9.58%)

fossil-db (vega10):
Totals from 3327 (2.06% of 161355) affected shaders:
Instrs: 257413 -> 257601 (+0.07%)
CodeSize: 1424244 -> 1425372 (+0.08%)
Latency: 8598402 -> 8600466 (+0.02%)
InvThroughput: 7906335 -> 7908234 (+0.02%)
SClause: 4932 -> 4973 (+0.83%)
PreSGPRs: 22010 -> 25405 (+15.42%)

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
Reviewed-by: Daniel Schürmann <daniel@schuermann.dev>
---
 src/amd/compiler/aco_dead_code_analysis.cpp   |  3 +-
 src/amd/compiler/aco_insert_exec_mask.cpp     |  8 +++-
 .../compiler/aco_instruction_selection.cpp    | 11 ++++-
 src/amd/compiler/aco_ir.cpp                   |  3 +-
 src/amd/compiler/aco_ir.h                     |  3 +-
 src/amd/compiler/aco_live_var_analysis.cpp    |  8 ++--
 src/amd/compiler/aco_lower_to_hw_instr.cpp    | 41 +++++++++++++++++++
 src/amd/compiler/aco_opcodes.py               |  2 +
 src/amd/compiler/aco_scheduler.cpp            |  3 +-
 9 files changed, 72 insertions(+), 10 deletions(-)

diff --git a/src/amd/compiler/aco_dead_code_analysis.cpp b/src/amd/compiler/aco_dead_code_analysis.cpp
index 3d565f0c1419..d558b688030f 100644
--- a/src/amd/compiler/aco_dead_code_analysis.cpp
+++ b/src/amd/compiler/aco_dead_code_analysis.cpp
@@ -83,7 +83,8 @@ process_block(dce_ctx& ctx, Block& block)
 bool
 is_dead(const std::vector<uint16_t>& uses, Instruction* instr)
 {
-   if (instr->definitions.empty() || instr->isBranch())
+   if (instr->definitions.empty() || instr->isBranch() ||
+       instr->opcode == aco_opcode::p_init_scratch)
       return false;
    if (std::any_of(instr->definitions.begin(), instr->definitions.end(),
                    [&uses](const Definition& def) { return !def.isTemp() || uses[def.tempId()]; }))
diff --git a/src/amd/compiler/aco_insert_exec_mask.cpp b/src/amd/compiler/aco_insert_exec_mask.cpp
index e1dd39299108..c96ee88c92e1 100644
--- a/src/amd/compiler/aco_insert_exec_mask.cpp
+++ b/src/amd/compiler/aco_insert_exec_mask.cpp
@@ -249,6 +249,12 @@ add_coupling_code(exec_ctx& ctx, Block* block, std::vector<aco_ptr<Instruction>>
       assert(startpgm->opcode == aco_opcode::p_startpgm);
       bld.insert(std::move(startpgm));
 
+      unsigned count = 1;
+      if (block->instructions[1]->opcode == aco_opcode::p_init_scratch) {
+         bld.insert(std::move(block->instructions[1]));
+         count++;
+      }
+
       Operand start_exec(bld.lm);
 
       /* exec seems to need to be manually initialized with combined shaders */
@@ -274,7 +280,7 @@ add_coupling_code(exec_ctx& ctx, Block* block, std::vector<aco_ptr<Instruction>>
          ctx.info[0].exec.emplace_back(start_exec, mask);
       }
 
-      return 1;
+      return count;
    }
 
    /* loop entry block */
diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 876c15bb643f..4980c040bf22 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -11207,9 +11207,16 @@ add_startpgm(struct isel_context* ctx)
     * handling spilling.
     */
    ctx->program->private_segment_buffer = get_arg(ctx, ctx->args->ring_offsets);
-   if (ctx->args->ac.scratch_offset.used) {
-      /* FIXME: Fix scratch loads/stores on GFX11. */
+   if (ctx->program->gfx_level <= GFX10_3) {
       ctx->program->scratch_offset = get_arg(ctx, ctx->args->ac.scratch_offset);
+
+      if (ctx->program->gfx_level >= GFX9) {
+         Operand scratch_offset(ctx->program->scratch_offset);
+         scratch_offset.setLateKill(true);
+         Builder bld(ctx->program, ctx->block);
+         bld.pseudo(aco_opcode::p_init_scratch, bld.def(s2), bld.def(s1, scc),
+                    ctx->program->private_segment_buffer, scratch_offset);
+      }
    }
 
    if (ctx->stage.has(SWStage::VS) && ctx->program->info.vs.dynamic_inputs) {
diff --git a/src/amd/compiler/aco_ir.cpp b/src/amd/compiler/aco_ir.cpp
index 5d325f863a22..b74af9417d83 100644
--- a/src/amd/compiler/aco_ir.cpp
+++ b/src/amd/compiler/aco_ir.cpp
@@ -592,7 +592,8 @@ needs_exec_mask(const Instruction* instr)
       case aco_opcode::p_end_linear_vgpr:
       case aco_opcode::p_logical_start:
       case aco_opcode::p_logical_end:
-      case aco_opcode::p_startpgm: return instr->reads_exec();
+      case aco_opcode::p_startpgm:
+      case aco_opcode::p_init_scratch: return instr->reads_exec();
       default: break;
       }
    }
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index bc989875741c..f2721aa0d958 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -451,6 +451,8 @@ struct PhysReg {
 
 /* helper expressions for special registers */
 static constexpr PhysReg m0{124};
+static constexpr PhysReg flat_scr_lo{102}; /* GFX8-GFX9, encoded differently on GFX6-7 */
+static constexpr PhysReg flat_scr_hi{103}; /* GFX8-GFX9, encoded differently on GFX6-7 */
 static constexpr PhysReg vcc{106};
 static constexpr PhysReg vcc_hi{107};
 static constexpr PhysReg tba{108}; /* GFX6-GFX8 */
@@ -2104,7 +2106,6 @@ public:
    bool early_rast = false; /* whether rasterization can start as soon as the 1st DONE pos export */
 
    bool needs_vcc = false;
-   bool needs_flat_scr = false;
 
    CompilationProgress progress;
 
diff --git a/src/amd/compiler/aco_live_var_analysis.cpp b/src/amd/compiler/aco_live_var_analysis.cpp
index 0449fa1cdff4..0d6274d6b827 100644
--- a/src/amd/compiler/aco_live_var_analysis.cpp
+++ b/src/amd/compiler/aco_live_var_analysis.cpp
@@ -293,12 +293,14 @@ calc_waves_per_workgroup(Program* program)
 uint16_t
 get_extra_sgprs(Program* program)
 {
+   /* We don't use this register on GFX6-8 and it's removed on GFX10+. */
+   bool needs_flat_scr = program->config->scratch_bytes_per_wave && program->gfx_level == GFX9;
+
    if (program->gfx_level >= GFX10) {
-      assert(!program->needs_flat_scr);
       assert(!program->dev.xnack_enabled);
       return 0;
    } else if (program->gfx_level >= GFX8) {
-      if (program->needs_flat_scr)
+      if (needs_flat_scr)
          return 6;
       else if (program->dev.xnack_enabled)
          return 4;
@@ -308,7 +310,7 @@ get_extra_sgprs(Program* program)
          return 0;
    } else {
       assert(!program->dev.xnack_enabled);
-      if (program->needs_flat_scr)
+      if (needs_flat_scr)
          return 4;
       else if (program->needs_vcc)
          return 2;
diff --git a/src/amd/compiler/aco_lower_to_hw_instr.cpp b/src/amd/compiler/aco_lower_to_hw_instr.cpp
index 20069adc1c25..1920dfbe6b9b 100644
--- a/src/amd/compiler/aco_lower_to_hw_instr.cpp
+++ b/src/amd/compiler/aco_lower_to_hw_instr.cpp
@@ -2327,6 +2327,47 @@ lower_to_hw_instr(Program* program)
                }
                break;
             }
+            case aco_opcode::p_init_scratch: {
+               assert(program->gfx_level >= GFX8 && program->gfx_level <= GFX10_3);
+               if (!program->config->scratch_bytes_per_wave)
+                  break;
+
+               Operand scratch_addr = instr->operands[0];
+               Operand scratch_addr_lo(scratch_addr.physReg(), s1);
+               if (program->stage != compute_cs) {
+                  bld.smem(aco_opcode::s_load_dwordx2, instr->definitions[0], scratch_addr,
+                           Operand::zero());
+                  scratch_addr_lo.setFixed(instr->definitions[0].physReg());
+               }
+               Operand scratch_addr_hi(scratch_addr_lo.physReg().advance(4), s1);
+
+               /* Since we know what the high 16 bits of scratch_hi is, we can set all the high 16
+                * bits in the same instruction that we add the carry.
+                */
+               uint32_t hi_add = 0xffff0000 - S_008F04_SWIZZLE_ENABLE_GFX6(1);
+
+               if (program->gfx_level >= GFX10) {
+                  Operand scratch_lo(instr->definitions[0].physReg(), s1);
+                  Operand scratch_hi(instr->definitions[0].physReg().advance(4), s1);
+
+                  bld.sop2(aco_opcode::s_add_u32, Definition(scratch_lo.physReg(), s1),
+                           Definition(scc, s1), scratch_addr_lo, instr->operands[1]);
+                  bld.sop2(aco_opcode::s_addc_u32, Definition(scratch_hi.physReg(), s1),
+                           Definition(scc, s1), scratch_addr_hi, Operand::c32(hi_add),
+                           Operand(scc, s1));
+
+                  /* "((size - 1) << 11) | register" (FLAT_SCRATCH_LO/HI is encoded as register
+                   * 20/21) */
+                  bld.sopk(aco_opcode::s_setreg_b32, scratch_lo, (31 << 11) | 20);
+                  bld.sopk(aco_opcode::s_setreg_b32, scratch_hi, (31 << 11) | 21);
+               } else {
+                  bld.sop2(aco_opcode::s_add_u32, Definition(flat_scr_lo, s1), Definition(scc, s1),
+                           scratch_addr_lo, instr->operands[1]);
+                  bld.sop2(aco_opcode::s_addc_u32, Definition(flat_scr_hi, s1), Definition(scc, s1),
+                           scratch_addr_hi, Operand::c32(hi_add), Operand(scc, s1));
+               }
+               break;
+            }
             default: break;
             }
          } else if (instr->isBranch()) {
diff --git a/src/amd/compiler/aco_opcodes.py b/src/amd/compiler/aco_opcodes.py
index 64a4b398c957..820e09b1989e 100644
--- a/src/amd/compiler/aco_opcodes.py
+++ b/src/amd/compiler/aco_opcodes.py
@@ -318,6 +318,8 @@ opcode("p_extract") # src1=index, src2=bits, src3=signext
 # (src0 & ((1 << bits) - 1)) << (index * bits)
 opcode("p_insert") # src1=index, src2=bits
 
+opcode("p_init_scratch")
+
 
 # SOP2 instructions: 2 scalar inputs, 1 scalar output (+optional scc)
 SOP2 = {
diff --git a/src/amd/compiler/aco_scheduler.cpp b/src/amd/compiler/aco_scheduler.cpp
index 3aeb30c5ccf6..0edd7862b6d2 100644
--- a/src/amd/compiler/aco_scheduler.cpp
+++ b/src/amd/compiler/aco_scheduler.cpp
@@ -573,7 +573,8 @@ perform_hazard_query(hazard_query* query, Instruction* instr, bool upwards)
 
    /* don't move non-reorderable instructions */
    if (instr->opcode == aco_opcode::s_memtime || instr->opcode == aco_opcode::s_memrealtime ||
-       instr->opcode == aco_opcode::s_setprio || instr->opcode == aco_opcode::s_getreg_b32)
+       instr->opcode == aco_opcode::s_setprio || instr->opcode == aco_opcode::s_getreg_b32 ||
+       instr->opcode == aco_opcode::p_init_scratch)
       return hazard_fail_unreorderable;
 
    memory_event_set instr_set;
-- 
GitLab


From 289fef472a8d6e26ad55f22aabf25cf4cfc7495c Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Thu, 19 May 2022 14:12:08 +0100
Subject: [PATCH 12/13] aco: use scratch_* for scratch load/store on GFX9+
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

fossil-db (navi21):
Totals from 52 (0.03% of 162293) affected shaders:
Instrs: 83190 -> 82145 (-1.26%)
CodeSize: 454892 -> 447260 (-1.68%); split: -1.68%, +0.00%
VGPRs: 4768 -> 4672 (-2.01%)
Latency: 1490887 -> 1487170 (-0.25%); split: -0.68%, +0.43%
InvThroughput: 935500 -> 933060 (-0.26%); split: -0.72%, +0.46%
VClause: 2715 -> 2632 (-3.06%); split: -4.53%, +1.47%
SClause: 1902 -> 1883 (-1.00%)
Copies: 8839 -> 8496 (-3.88%)
PreSGPRs: 2012 -> 1807 (-10.19%)
PreVGPRs: 3282 -> 3192 (-2.74%)

fossil-db (vega10):
Totals from 41 (0.03% of 161355) affected shaders:
Instrs: 35772 -> 35699 (-0.20%)
CodeSize: 187040 -> 186584 (-0.24%)
VGPRs: 4044 -> 4072 (+0.69%)
Latency: 243088 -> 242379 (-0.29%)
InvThroughput: 180301 -> 179783 (-0.29%)
VClause: 1204 -> 1216 (+1.00%)
SClause: 653 -> 637 (-2.45%)
Copies: 3736 -> 3704 (-0.86%); split: -0.88%, +0.03%
PreSGPRs: 1331 -> 1207 (-9.32%)

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
Reviewed-by: Daniel Schürmann <daniel@schuermann.dev>
---
 .../compiler/aco_instruction_selection.cpp    | 114 +++++++++++++++---
 1 file changed, 100 insertions(+), 14 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 4980c040bf22..66adb1651950 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -4421,7 +4421,47 @@ mubuf_load_callback(Builder& bld, const LoadEmitInfo& info, Temp offset, unsigne
 }
 
 const EmitLoadParameters mubuf_load_params{mubuf_load_callback, true, true, 4096};
-const EmitLoadParameters scratch_load_params{mubuf_load_callback, false, true, 4096};
+
+Temp
+scratch_load_callback(Builder& bld, const LoadEmitInfo& info, Temp offset, unsigned bytes_needed,
+                      unsigned align_, unsigned const_offset, Temp dst_hint)
+{
+   unsigned bytes_size = 0;
+   aco_opcode op;
+   if (bytes_needed == 1 || align_ % 2u) {
+      bytes_size = 1;
+      op = aco_opcode::scratch_load_ubyte;
+   } else if (bytes_needed == 2 || align_ % 4u) {
+      bytes_size = 2;
+      op = aco_opcode::scratch_load_ushort;
+   } else if (bytes_needed <= 4) {
+      bytes_size = 4;
+      op = aco_opcode::scratch_load_dword;
+   } else if (bytes_needed <= 8) {
+      bytes_size = 8;
+      op = aco_opcode::scratch_load_dwordx2;
+   } else if (bytes_needed <= 12) {
+      bytes_size = 12;
+      op = aco_opcode::scratch_load_dwordx3;
+   } else {
+      bytes_size = 16;
+      op = aco_opcode::scratch_load_dwordx4;
+   }
+   RegClass rc = RegClass::get(RegType::vgpr, bytes_size);
+   Temp val = dst_hint.id() && rc == dst_hint.regClass() ? dst_hint : bld.tmp(rc);
+   aco_ptr<FLAT_instruction> flat{create_instruction<FLAT_instruction>(op, Format::SCRATCH, 2, 1)};
+   flat->operands[0] = offset.regClass() == s1 ? Operand(v1) : Operand(offset);
+   flat->operands[1] = offset.regClass() == s1 ? Operand(offset) : Operand(s1);
+   flat->sync = info.sync;
+   flat->offset = const_offset;
+   flat->definitions[0] = Definition(val);
+   bld.insert(std::move(flat));
+
+   return val;
+}
+
+const EmitLoadParameters scratch_mubuf_load_params{mubuf_load_callback, false, true, 4096};
+const EmitLoadParameters scratch_flat_load_params{scratch_load_callback, false, true, 2048};
 
 Temp
 get_gfx6_global_rsrc(Builder& bld, Temp addr)
@@ -7498,27 +7538,40 @@ void
 visit_load_scratch(isel_context* ctx, nir_intrinsic_instr* instr)
 {
    Builder bld(ctx->program, ctx->block);
-   Temp rsrc = get_scratch_resource(ctx);
-   Temp offset = as_vgpr(ctx, get_ssa_temp(ctx, instr->src[0].ssa));
    Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
 
-   LoadEmitInfo info = {Operand(offset), dst, instr->dest.ssa.num_components,
-                        instr->dest.ssa.bit_size / 8u, rsrc};
+   LoadEmitInfo info = {Operand(v1), dst, instr->dest.ssa.num_components,
+                        instr->dest.ssa.bit_size / 8u};
    info.align_mul = nir_intrinsic_align_mul(instr);
    info.align_offset = nir_intrinsic_align_offset(instr);
    info.swizzle_component_size = ctx->program->gfx_level <= GFX8 ? 4 : 0;
    info.sync = memory_sync_info(storage_scratch, semantic_private);
-   info.soffset = ctx->program->scratch_offset;
-   emit_load(ctx, bld, info, scratch_load_params);
+   if (ctx->program->gfx_level >= GFX9) {
+      if (nir_src_is_const(instr->src[0])) {
+         uint32_t max = ctx->program->dev.scratch_global_offset_max + 1;
+         info.offset =
+            bld.copy(bld.def(s1), Operand::c32(ROUND_DOWN_TO(nir_src_as_uint(instr->src[0]), max)));
+         info.const_offset = nir_src_as_uint(instr->src[0]) % max;
+      } else {
+         info.offset = Operand(get_ssa_temp(ctx, instr->src[0].ssa));
+      }
+      EmitLoadParameters params = scratch_flat_load_params;
+      params.max_const_offset_plus_one = ctx->program->dev.scratch_global_offset_max + 1;
+      emit_load(ctx, bld, info, params);
+   } else {
+      info.resource = get_scratch_resource(ctx);
+      info.offset = Operand(as_vgpr(ctx, get_ssa_temp(ctx, instr->src[0].ssa)));
+      info.soffset = ctx->program->scratch_offset;
+      emit_load(ctx, bld, info, scratch_mubuf_load_params);
+   }
 }
 
 void
 visit_store_scratch(isel_context* ctx, nir_intrinsic_instr* instr)
 {
    Builder bld(ctx->program, ctx->block);
-   Temp rsrc = get_scratch_resource(ctx);
    Temp data = as_vgpr(ctx, get_ssa_temp(ctx, instr->src[0].ssa));
-   Temp offset = as_vgpr(ctx, get_ssa_temp(ctx, instr->src[1].ssa));
+   Temp offset = get_ssa_temp(ctx, instr->src[1].ssa);
 
    unsigned elem_size_bytes = instr->src[0].ssa->bit_size / 8;
    unsigned writemask = util_widen_mask(nir_intrinsic_write_mask(instr), elem_size_bytes);
@@ -7530,11 +7583,44 @@ visit_store_scratch(isel_context* ctx, nir_intrinsic_instr* instr)
    split_buffer_store(ctx, instr, false, RegType::vgpr, data, writemask, swizzle_component_size,
                       &write_count, write_datas, offsets);
 
-   for (unsigned i = 0; i < write_count; i++) {
-      aco_opcode op = get_buffer_store_op(write_datas[i].bytes());
-      Instruction* mubuf = bld.mubuf(op, rsrc, offset, ctx->program->scratch_offset, write_datas[i],
-                                     offsets[i], true, true);
-      mubuf->mubuf().sync = memory_sync_info(storage_scratch, semantic_private);
+   if (ctx->program->gfx_level >= GFX9) {
+      uint32_t max = ctx->program->dev.scratch_global_offset_max + 1;
+      offset = nir_src_is_const(instr->src[1]) ? Temp(0, s1) : offset;
+      uint32_t base_const_offset =
+         nir_src_is_const(instr->src[1]) ? nir_src_as_uint(instr->src[1]) : 0;
+
+      for (unsigned i = 0; i < write_count; i++) {
+         aco_opcode op;
+         switch (write_datas[i].bytes()) {
+         case 1: op = aco_opcode::scratch_store_byte; break;
+         case 2: op = aco_opcode::scratch_store_short; break;
+         case 4: op = aco_opcode::scratch_store_dword; break;
+         case 8: op = aco_opcode::scratch_store_dwordx2; break;
+         case 12: op = aco_opcode::scratch_store_dwordx3; break;
+         case 16: op = aco_opcode::scratch_store_dwordx4; break;
+         default: unreachable("Unexpected store size");
+         }
+
+         uint32_t const_offset = base_const_offset + offsets[i];
+         assert(const_offset < max || offset.id() == 0);
+
+         Operand addr = offset.regClass() == s1 ? Operand(v1) : Operand(offset);
+         Operand saddr = offset.regClass() == s1 ? Operand(offset) : Operand(s1);
+         if (offset.id() == 0)
+            saddr = bld.copy(bld.def(s1), Operand::c32(ROUND_DOWN_TO(const_offset, max)));
+
+         bld.scratch(op, addr, saddr, write_datas[i], const_offset % max,
+                     memory_sync_info(storage_scratch, semantic_private));
+      }
+   } else {
+      Temp rsrc = get_scratch_resource(ctx);
+      offset = as_vgpr(ctx, offset);
+      for (unsigned i = 0; i < write_count; i++) {
+         aco_opcode op = get_buffer_store_op(write_datas[i].bytes());
+         Instruction* mubuf = bld.mubuf(op, rsrc, offset, ctx->program->scratch_offset,
+                                        write_datas[i], offsets[i], true, true);
+         mubuf->mubuf().sync = memory_sync_info(storage_scratch, semantic_private);
+      }
    }
 }
 
-- 
GitLab


From e30d90f747265399fd3199288ce0a6fdae140f26 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Thu, 19 May 2022 16:09:13 +0100
Subject: [PATCH 13/13] aco: use scratch_* for VGPR spill/reload on GFX9+
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

fossil-db (navi21):
Totals from 12 (0.01% of 162293) affected shaders:
Instrs: 122808 -> 122782 (-0.02%); split: -0.11%, +0.09%
CodeSize: 711248 -> 710788 (-0.06%); split: -0.16%, +0.10%
SpillSGPRs: 928 -> 831 (-10.45%)
SpillVGPRs: 1626 -> 1624 (-0.12%)
Latency: 4960285 -> 4932547 (-0.56%)
InvThroughput: 2574083 -> 2559953 (-0.55%)
VClause: 3404 -> 3402 (-0.06%)
Copies: 36992 -> 37181 (+0.51%); split: -0.05%, +0.56%
Branches: 3582 -> 3585 (+0.08%)
PreVGPRs: 3055 -> 3057 (+0.07%)

fossil-db (vega10):
Totals from 12 (0.01% of 161355) affected shaders:
Instrs: 124817 -> 124383 (-0.35%); split: -0.46%, +0.12%
CodeSize: 705116 -> 703664 (-0.21%); split: -0.44%, +0.23%
SpillSGPRs: 1012 -> 898 (-11.26%)
SpillVGPRs: 1632 -> 1624 (-0.49%)
Scratch: 201728 -> 200704 (-0.51%)
Latency: 6160115 -> 6266025 (+1.72%); split: -0.34%, +2.06%
InvThroughput: 6440203 -> 6544595 (+1.62%); split: -0.35%, +1.97%
VClause: 3409 -> 3423 (+0.41%)
Copies: 37929 -> 37748 (-0.48%); split: -1.16%, +0.69%
Branches: 3851 -> 3855 (+0.10%); split: -0.13%, +0.23%
PreVGPRs: 3053 -> 3055 (+0.07%)

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
Reviewed-by: Daniel Schürmann <daniel@schuermann.dev>
---
 src/amd/compiler/aco_spill.cpp | 76 +++++++++++++++++++++++++---------
 1 file changed, 56 insertions(+), 20 deletions(-)

diff --git a/src/amd/compiler/aco_spill.cpp b/src/amd/compiler/aco_spill.cpp
index 848df3dfe953..4b1099e8b289 100644
--- a/src/amd/compiler/aco_spill.cpp
+++ b/src/amd/compiler/aco_spill.cpp
@@ -1408,6 +1408,10 @@ load_scratch_resource(spill_ctx& ctx, Temp& scratch_offset, Block& block,
       }
    }
 
+   /* GFX9+ uses scratch_* instructions, which don't use a resource. Return a SADDR instead. */
+   if (ctx.program->gfx_level >= GFX9)
+      return bld.copy(bld.def(s1), Operand::c32(offset));
+
    Temp private_segment_buffer = ctx.program->private_segment_buffer;
    if (ctx.program->stage.hw != HWStage::CS)
       private_segment_buffer =
@@ -1445,17 +1449,29 @@ setup_vgpr_spill_reload(spill_ctx& ctx, Block& block,
    Temp scratch_offset = ctx.program->scratch_offset;
 
    *offset = spill_slot * 4;
-
-   bool add_offset_to_sgpr = ctx.program->config->scratch_bytes_per_wave / ctx.program->wave_size +
-                                ctx.vgpr_spill_slots * 4 >
-                             4096;
-   if (!add_offset_to_sgpr)
-      *offset += ctx.program->config->scratch_bytes_per_wave / ctx.program->wave_size;
-
-   if (ctx.scratch_rsrc == Temp()) {
-      unsigned rsrc_offset = add_offset_to_sgpr ? ctx.program->config->scratch_bytes_per_wave : 0;
-      ctx.scratch_rsrc =
-         load_scratch_resource(ctx, scratch_offset, block, instructions, rsrc_offset);
+   if (ctx.program->gfx_level >= GFX9) {
+      *offset += ctx.program->dev.scratch_global_offset_min;
+
+      if (ctx.scratch_rsrc == Temp()) {
+         int32_t saddr = ctx.program->config->scratch_bytes_per_wave / ctx.program->wave_size -
+                         ctx.program->dev.scratch_global_offset_min;
+         ctx.scratch_rsrc =
+            load_scratch_resource(ctx, scratch_offset, block, instructions, saddr);
+      }
+   } else {
+      bool add_offset_to_sgpr =
+         ctx.program->config->scratch_bytes_per_wave / ctx.program->wave_size +
+            ctx.vgpr_spill_slots * 4 >
+         4096;
+      if (!add_offset_to_sgpr)
+         *offset += ctx.program->config->scratch_bytes_per_wave / ctx.program->wave_size;
+
+      if (ctx.scratch_rsrc == Temp()) {
+         unsigned rsrc_offset =
+            add_offset_to_sgpr ? ctx.program->config->scratch_bytes_per_wave : 0;
+         ctx.scratch_rsrc =
+            load_scratch_resource(ctx, scratch_offset, block, instructions, rsrc_offset);
+      }
    }
 }
 
@@ -1485,11 +1501,19 @@ spill_vgpr(spill_ctx& ctx, Block& block, std::vector<aco_ptr<Instruction>>& inst
       bld.insert(split);
       for (unsigned i = 0; i < temp.size(); i++, offset += 4) {
          Temp elem = split->definitions[i].getTemp();
-         Instruction* instr =
-            bld.mubuf(aco_opcode::buffer_store_dword, ctx.scratch_rsrc, Operand(v1),
-                      ctx.program->scratch_offset, elem, offset, false, true);
-         instr->mubuf().sync = memory_sync_info(storage_vgpr_spill, semantic_private);
+         if (ctx.program->gfx_level >= GFX9) {
+            bld.scratch(aco_opcode::scratch_store_dword, Operand(v1), ctx.scratch_rsrc, elem,
+                        offset, memory_sync_info(storage_vgpr_spill, semantic_private));
+         } else {
+            Instruction* instr =
+               bld.mubuf(aco_opcode::buffer_store_dword, ctx.scratch_rsrc, Operand(v1),
+                         ctx.program->scratch_offset, elem, offset, false, true);
+            instr->mubuf().sync = memory_sync_info(storage_vgpr_spill, semantic_private);
+         }
       }
+   } else if (ctx.program->gfx_level >= GFX9) {
+      bld.scratch(aco_opcode::scratch_store_dword, Operand(v1), ctx.scratch_rsrc, temp, offset,
+                  memory_sync_info(storage_vgpr_spill, semantic_private));
    } else {
       Instruction* instr = bld.mubuf(aco_opcode::buffer_store_dword, ctx.scratch_rsrc, Operand(v1),
                                      ctx.program->scratch_offset, temp, offset, false, true);
@@ -1517,12 +1541,21 @@ reload_vgpr(spill_ctx& ctx, Block& block, std::vector<aco_ptr<Instruction>>& ins
       for (unsigned i = 0; i < def.size(); i++, offset += 4) {
          Temp tmp = bld.tmp(v1);
          vec->operands[i] = Operand(tmp);
-         Instruction* instr =
-            bld.mubuf(aco_opcode::buffer_load_dword, Definition(tmp), ctx.scratch_rsrc, Operand(v1),
-                      ctx.program->scratch_offset, offset, false, true);
-         instr->mubuf().sync = memory_sync_info(storage_vgpr_spill, semantic_private);
+         if (ctx.program->gfx_level >= GFX9) {
+            bld.scratch(aco_opcode::scratch_load_dword, Definition(tmp), Operand(v1),
+                        ctx.scratch_rsrc, offset,
+                        memory_sync_info(storage_vgpr_spill, semantic_private));
+         } else {
+            Instruction* instr =
+               bld.mubuf(aco_opcode::buffer_load_dword, Definition(tmp), ctx.scratch_rsrc,
+                         Operand(v1), ctx.program->scratch_offset, offset, false, true);
+            instr->mubuf().sync = memory_sync_info(storage_vgpr_spill, semantic_private);
+         }
       }
       bld.insert(vec);
+   } else if (ctx.program->gfx_level >= GFX9) {
+      bld.scratch(aco_opcode::scratch_load_dword, def, Operand(v1), ctx.scratch_rsrc, offset,
+                  memory_sync_info(storage_vgpr_spill, semantic_private));
    } else {
       Instruction* instr = bld.mubuf(aco_opcode::buffer_load_dword, def, ctx.scratch_rsrc,
                                      Operand(v1), ctx.program->scratch_offset, offset, false, true);
@@ -1907,7 +1940,10 @@ spill(Program* program, live& live_vars)
    }
    /* add extra SGPRs required for spilling VGPRs */
    if (demand.vgpr + extra_vgprs > vgpr_limit) {
-      extra_sgprs = 5; /* scratch_resource (s4) + scratch_offset (s1) */
+      if (program->gfx_level >= GFX9)
+         extra_sgprs = 1; /* SADDR */
+      else
+         extra_sgprs = 5; /* scratch_resource (s4) + scratch_offset (s1) */
       if (demand.sgpr + extra_sgprs > sgpr_limit) {
          /* re-calculate in case something has changed */
          unsigned sgpr_spills = demand.sgpr + extra_sgprs - sgpr_limit;
-- 
GitLab

