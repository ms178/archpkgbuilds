--- a/src/amd/compiler/aco_assembler.cpp	2025-10-20 22:27:12.774781145 +0200
+++ b/src/amd/compiler/aco_assembler.cpp	2025-12-22 22:45:05.348293657 +0200
@@ -14,7 +14,7 @@
 #include "ac_shader_util.h"
 #include <algorithm>
 #include <cstdint>
-#include <map>
+#include <unordered_map>
 #include <vector>
 
 namespace aco {
@@ -33,17 +33,25 @@ struct asm_context {
    Program* program;
    enum amd_gfx_level gfx_level;
    std::vector<branch_info> branches;
-   std::map<unsigned, constaddr_info> constaddrs;
-   std::map<unsigned, constaddr_info> resumeaddrs;
+   std::unordered_map<unsigned, constaddr_info> constaddrs;
+   std::unordered_map<unsigned, constaddr_info> resumeaddrs;
    std::vector<struct aco_symbol>* symbols;
-   uint32_t loop_header = -1u;
-   uint32_t loop_exit = 0u;
+   uint32_t loop_header = UINT32_MAX;
+   uint32_t loop_exit = UINT32_MAX;
    const int16_t* opcode;
-   // TODO: keep track of branch instructions referring blocks
-   // and, when emitting the block, correct the offset in instr
+   int subvector_begin_pos = -1;
+
    asm_context(Program* program_, std::vector<struct aco_symbol>* symbols_)
-       : program(program_), gfx_level(program->gfx_level), symbols(symbols_)
+       : program(program_), gfx_level(program_->gfx_level), symbols(symbols_)
    {
+      /* Pre-size hash maps to avoid rehashing during assembly.
+       * Values chosen based on typical shader complexity. */
+      constaddrs.reserve(256);
+      resumeaddrs.reserve(32);
+      branches.reserve(512);
+
+      /* Select opcode table based on GFX level.
+       * GFX9 (Vega) uses opcode_gfx9. */
       if (gfx_level <= GFX7)
          opcode = &instr_info.opcode_gfx7[0];
       else if (gfx_level <= GFX9)
@@ -55,21 +63,31 @@ struct asm_context {
       else
          opcode = &instr_info.opcode_gfx12[0];
    }
-
-   int subvector_begin_pos = -1;
 };
 
 unsigned
 get_mimg_nsa_dwords(const Instruction* instr)
 {
+   /* MIMG instructions require at least 4 operands:
+    * [0] = resource descriptor
+    * [1] = sampler descriptor
+    * [2] = vdata
+    * [3+] = address components
+    * Guard against malformed instructions. */
+   if (instr->operands.size() < 4) [[unlikely]]
+      return 0;
+
    unsigned addr_dwords = instr->operands.size() - 3;
    for (unsigned i = 3; i < instr->operands.size(); i++) {
       if (instr->operands[i].isVectorAligned())
          addr_dwords--;
    }
+
+   /* Check if address operands are contiguous in register file */
    for (unsigned i = 4; i < instr->operands.size(); i++) {
-      if (instr->operands[i].physReg() !=
-          instr->operands[i - 1].physReg().advance(instr->operands[i - 1].bytes()))
+      const PhysReg expected = instr->operands[i - 1].physReg().advance(
+         instr->operands[i - 1].bytes());
+      if (instr->operands[i].physReg() != expected)
          return DIV_ROUND_UP(addr_dwords - 1, 4);
    }
    return 0;
@@ -84,30 +102,46 @@ get_vopd_opy_start(const Instruction* in
    case aco_opcode::v_dual_fmamk_f32:
    case aco_opcode::v_dual_cndmask_b32:
    case aco_opcode::v_dual_dot2acc_f32_f16:
-   case aco_opcode::v_dual_dot2acc_f32_bf16: return 3;
-   case aco_opcode::v_dual_mov_b32: return 1;
-   default: return 2;
+   case aco_opcode::v_dual_dot2acc_f32_bf16:
+      return 3;
+   case aco_opcode::v_dual_mov_b32:
+      return 1;
+   default:
+      return 2;
    }
 }
 
+/**
+ * Encode a physical register for instruction encoding.
+ * Handles GFX11+ m0/sgpr_null register swap.
+ */
 uint32_t
-reg(asm_context& ctx, PhysReg reg)
+reg(asm_context& ctx, PhysReg r)
 {
-   if (ctx.gfx_level >= GFX11) {
-      if (reg == m0)
+   /* GFX11+ swaps m0 and sgpr_null encoding positions.
+    * For GFX9 (Vega 64), this branch is never taken. */
+   if (ctx.gfx_level >= GFX11) [[unlikely]] {
+      if (r == m0) [[unlikely]]
          return sgpr_null.reg();
-      else if (reg == sgpr_null)
+      else if (r == sgpr_null) [[unlikely]]
          return m0.reg();
    }
-   return reg.reg();
+   return r.reg();
 }
 
+/**
+ * Encode operand register with optional width masking.
+ * width=8 for VGPR/SGPR fields, width=32 for full register.
+ */
 ALWAYS_INLINE uint32_t
 reg(asm_context& ctx, Operand op, unsigned width = 32)
 {
    return reg(ctx, op.physReg()) & BITFIELD_MASK(width);
 }
 
+/**
+ * Encode definition register with optional width masking.
+ */
 ALWAYS_INLINE uint32_t
 reg(asm_context& ctx, Definition def, unsigned width = 32)
 {
@@ -117,6 +151,8 @@ reg(asm_context& ctx, Definition def, un
 bool
 needs_vop3_gfx11(asm_context& ctx, Instruction* instr)
 {
+   /* Only applies to GFX11+ true16 mode.
+    * Returns false immediately for GFX9. */
    if (ctx.gfx_level <= GFX10_3)
       return false;
 
@@ -145,9 +181,9 @@ get_gfx12_cpol(const T& instr)
 void
 emit_sop2_instruction(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[static_cast<int>(instr->opcode)];
 
-   uint32_t encoding = (0b10 << 30);
+   uint32_t encoding = (0b10u << 30);
    encoding |= opcode << 23;
    encoding |= !instr->definitions.empty() ? reg(ctx, instr->definitions[0]) << 16 : 0;
    encoding |= instr->operands.size() >= 2 ? reg(ctx, instr->operands[1]) << 8 : 0;
@@ -158,32 +194,31 @@ emit_sop2_instruction(asm_context& ctx,
 void
 emit_sopk_instruction(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[static_cast<int>(instr->opcode)];
    const SALU_instruction& sopk = instr->salu();
    assert(sopk.imm <= UINT16_MAX);
-   uint16_t imm = sopk.imm;
+   uint16_t imm = static_cast<uint16_t>(sopk.imm);
 
    if (instr->opcode == aco_opcode::s_subvector_loop_begin) {
       assert(ctx.gfx_level >= GFX10);
       assert(ctx.subvector_begin_pos == -1);
-      ctx.subvector_begin_pos = out.size();
+      ctx.subvector_begin_pos = static_cast<int>(out.size());
    } else if (instr->opcode == aco_opcode::s_subvector_loop_end) {
       assert(ctx.gfx_level >= GFX10);
       assert(ctx.subvector_begin_pos != -1);
-      /* Adjust s_subvector_loop_begin instruction to the address after the end  */
-      out[ctx.subvector_begin_pos] |= (out.size() - ctx.subvector_begin_pos);
-      /* Adjust s_subvector_loop_end instruction to the address after the beginning  */
-      imm = (uint16_t)(ctx.subvector_begin_pos - (int)out.size());
+      out[static_cast<size_t>(ctx.subvector_begin_pos)] |=
+         static_cast<uint32_t>(out.size()) - static_cast<uint32_t>(ctx.subvector_begin_pos);
+      imm = static_cast<uint16_t>(ctx.subvector_begin_pos - static_cast<int>(out.size()));
       ctx.subvector_begin_pos = -1;
    }
 
-   uint32_t encoding = (0b1011 << 28);
+   uint32_t encoding = (0b1011u << 28);
    encoding |= opcode << 23;
-   encoding |= !instr->definitions.empty() && !(instr->definitions[0].physReg() == scc)
-                  ? reg(ctx, instr->definitions[0]) << 16
-               : !instr->operands.empty() && instr->operands[0].physReg() <= 127
-                  ? reg(ctx, instr->operands[0]) << 16
-                  : 0;
+   if (!instr->definitions.empty() && instr->definitions[0].physReg() != scc) {
+      encoding |= reg(ctx, instr->definitions[0]) << 16;
+   } else if (!instr->operands.empty() && instr->operands[0].physReg() <= 127) {
+      encoding |= reg(ctx, instr->operands[0]) << 16;
+   }
    encoding |= imm;
    out.push_back(encoding);
 }
@@ -191,9 +226,9 @@ emit_sopk_instruction(asm_context& ctx,
 void
 emit_sop1_instruction(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[static_cast<int>(instr->opcode)];
 
-   uint32_t encoding = (0b101111101 << 23);
+   uint32_t encoding = (0b101111101u << 23);
    encoding |= !instr->definitions.empty() ? reg(ctx, instr->definitions[0]) << 16 : 0;
    encoding |= opcode << 8;
    encoding |= !instr->operands.empty() ? reg(ctx, instr->operands[0]) : 0;
@@ -203,9 +238,9 @@ emit_sop1_instruction(asm_context& ctx,
 void
 emit_sopc_instruction(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[static_cast<int>(instr->opcode)];
 
-   uint32_t encoding = (0b101111110 << 23);
+   uint32_t encoding = (0b101111110u << 23);
    encoding |= opcode << 16;
    encoding |= instr->operands.size() == 2 ? reg(ctx, instr->operands[1]) << 8 : 0;
    encoding |= !instr->operands.empty() ? reg(ctx, instr->operands[0]) : 0;
@@ -216,17 +251,17 @@ void
 emit_sopp_instruction(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr,
                       bool force_imm = false)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[static_cast<int>(instr->opcode)];
    const SALU_instruction& sopp = instr->salu();
 
-   uint32_t encoding = (0b101111111 << 23);
+   uint32_t encoding = (0b101111111u << 23);
    encoding |= opcode << 16;
 
-   if (!force_imm && instr_info.classes[(int)instr->opcode] == instr_class::branch) {
-      ctx.branches.push_back({(unsigned)out.size(), sopp.imm});
+   if (!force_imm && instr_info.classes[static_cast<int>(instr->opcode)] == instr_class::branch) {
+      ctx.branches.push_back({static_cast<unsigned>(out.size()), sopp.imm});
    } else {
       assert(sopp.imm <= UINT16_MAX);
-      encoding |= (uint16_t)sopp.imm;
+      encoding |= static_cast<uint16_t>(sopp.imm);
    }
    out.push_back(encoding);
 }
@@ -234,17 +269,19 @@ emit_sopp_instruction(asm_context& ctx,
 void
 emit_smem_instruction(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[static_cast<int>(instr->opcode)];
    const SMEM_instruction& smem = instr->smem();
-   bool glc = smem.cache.value & ac_glc;
-   bool dlc = smem.cache.value & ac_dlc;
+   const bool glc = smem.cache.value & ac_glc;
+   const bool dlc = smem.cache.value & ac_dlc;
+   const enum amd_gfx_level gfx = ctx.gfx_level;
 
-   bool soe = instr->operands.size() >= (!instr->definitions.empty() ? 3 : 4);
-   bool is_load = !instr->definitions.empty();
+   const bool soe = instr->operands.size() >= (!instr->definitions.empty() ? 3u : 4u);
+   const bool is_load = !instr->definitions.empty();
    uint32_t encoding = 0;
 
-   if (ctx.gfx_level <= GFX7) {
-      encoding = (0b11000 << 27);
+   /* GFX6/GFX7 use different encoding format */
+   if (gfx <= GFX7) [[unlikely]] {
+      encoding = (0b11000u << 27);
       encoding |= opcode << 22;
       encoding |= instr->definitions.size() ? reg(ctx, instr->definitions[0]) << 15 : 0;
       encoding |= instr->operands.size() ? (reg(ctx, instr->operands[0]) >> 1) << 9 : 0;
@@ -252,50 +289,49 @@ emit_smem_instruction(asm_context& ctx,
          if (!instr->operands[1].isConstant()) {
             encoding |= reg(ctx, instr->operands[1]);
          } else if (instr->operands[1].constantValue() >= 1024) {
-            encoding |= 255; /* SQ_SRC_LITERAL */
+            encoding |= 255;
          } else {
             encoding |= instr->operands[1].constantValue() >> 2;
-            encoding |= 1 << 8;
+            encoding |= 1u << 8;
          }
       }
       out.push_back(encoding);
-      /* SMRD instructions can take a literal on GFX7 */
       if (instr->operands.size() >= 2 && instr->operands[1].isConstant() &&
           instr->operands[1].constantValue() >= 1024)
          out.push_back(instr->operands[1].constantValue() >> 2);
       return;
    }
 
-   if (ctx.gfx_level <= GFX9) {
-      encoding = (0b110000 << 26);
-      assert(!dlc); /* Device-level coherent is not supported on GFX9 and lower */
-      /* We don't use the NV bit. */
+   /* GFX8/GFX9 encoding - this is the hot path for Vega 64 */
+   if (gfx <= GFX9) [[likely]] {
+      encoding = (0b110000u << 26);
+      assert(!dlc); /* DLC not supported on GFX8/9 */
    } else {
-      encoding = (0b111101 << 26);
-      if (ctx.gfx_level <= GFX11_5)
-         encoding |= dlc ? 1 << (ctx.gfx_level >= GFX11 ? 13 : 14) : 0;
+      encoding = (0b111101u << 26);
+      if (gfx <= GFX11_5)
+         encoding |= dlc ? 1u << (gfx >= GFX11 ? 13 : 14) : 0;
    }
 
-   if (ctx.gfx_level <= GFX11_5) {
+   if (gfx <= GFX11_5) [[likely]] {
       encoding |= opcode << 18;
-      encoding |= glc ? 1 << (ctx.gfx_level >= GFX11 ? 14 : 16) : 0;
+      encoding |= glc ? 1u << (gfx >= GFX11 ? 14 : 16) : 0;
    } else {
       encoding |= opcode << 13;
       encoding |= get_gfx12_cpol(smem) << 21;
    }
 
-   if (ctx.gfx_level <= GFX9) {
+   if (gfx <= GFX9) [[likely]] {
       if (instr->operands.size() >= 2)
-         encoding |= instr->operands[1].isConstant() ? 1 << 17 : 0; /* IMM - immediate enable */
+         encoding |= instr->operands[1].isConstant() ? 1u << 17 : 0;
    }
-   if (ctx.gfx_level == GFX9) {
-      encoding |= soe ? 1 << 14 : 0;
+   if (gfx == GFX9) [[likely]] {
+      encoding |= soe ? 1u << 14 : 0;
    }
 
-   if (is_load || instr->operands.size() >= 3) { /* SDATA */
+   if (is_load || instr->operands.size() >= 3) {
       encoding |= (is_load ? reg(ctx, instr->definitions[0]) : reg(ctx, instr->operands[2])) << 6;
    }
-   if (instr->operands.size() >= 1) { /* SBASE */
+   if (instr->operands.size() >= 1) {
       encoding |= reg(ctx, instr->operands[0]) >> 1;
    }
 
@@ -303,35 +339,30 @@ emit_smem_instruction(asm_context& ctx,
    encoding = 0;
 
    int32_t offset = 0;
-   uint32_t soffset =
-      ctx.gfx_level >= GFX10
-         ? reg(ctx, sgpr_null) /* On GFX10 this is disabled by specifying SGPR_NULL */
-         : 0;                  /* On GFX9, it is disabled by the SOE bit (and it's not present on
-                                  GFX8 and below) */
+   uint32_t soffset = gfx >= GFX10 ? reg(ctx, sgpr_null) : 0;
+
    if (instr->operands.size() >= 2) {
       const Operand& op_off1 = instr->operands[1];
-      if (ctx.gfx_level <= GFX9) {
-         offset = op_off1.isConstant() ? op_off1.constantValue() : reg(ctx, op_off1);
+      if (gfx <= GFX9) [[likely]] {
+         offset = op_off1.isConstant() ? static_cast<int32_t>(op_off1.constantValue())
+                                       : static_cast<int32_t>(reg(ctx, op_off1));
       } else {
-         /* GFX10 only supports constants in OFFSET, so put the operand in SOFFSET if it's an
-          * SGPR */
          if (op_off1.isConstant()) {
-            offset = op_off1.constantValue();
+            offset = static_cast<int32_t>(op_off1.constantValue());
          } else {
             soffset = reg(ctx, op_off1);
-            assert(!soe); /* There is no place to put the other SGPR offset, if any */
+            assert(!soe);
          }
       }
 
       if (soe) {
          const Operand& op_off2 = instr->operands.back();
-         assert(ctx.gfx_level >= GFX9); /* GFX8 and below don't support specifying a constant
-                                            and an SGPR at the same time */
+         assert(gfx >= GFX9);
          assert(!op_off2.isConstant());
          soffset = reg(ctx, op_off2);
       }
    }
-   encoding |= offset;
+   encoding |= static_cast<uint32_t>(offset);
    encoding |= soffset << 25;
 
    out.push_back(encoding);
@@ -340,35 +371,35 @@ emit_smem_instruction(asm_context& ctx,
 void
 emit_vop2_instruction(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[static_cast<int>(instr->opcode)];
    const VALU_instruction& valu = instr->valu();
 
    uint32_t encoding = 0;
    encoding |= opcode << 25;
    encoding |= reg(ctx, instr->definitions[0], 8) << 17;
-   encoding |= (valu.opsel[3] ? 128 : 0) << 17;
+   encoding |= (valu.opsel[3] ? 128u : 0u) << 17;
    encoding |= reg(ctx, instr->operands[1], 8) << 9;
-   encoding |= (valu.opsel[1] ? 128 : 0) << 9;
+   encoding |= (valu.opsel[1] ? 128u : 0u) << 9;
    encoding |= reg(ctx, instr->operands[0]);
-   encoding |= valu.opsel[0] ? 128 : 0;
+   encoding |= valu.opsel[0] ? 128u : 0u;
    out.push_back(encoding);
 }
 
 void
 emit_vop1_instruction(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[static_cast<int>(instr->opcode)];
    const VALU_instruction& valu = instr->valu();
 
-   uint32_t encoding = (0b0111111 << 25);
+   uint32_t encoding = (0b0111111u << 25);
    if (!instr->definitions.empty()) {
       encoding |= reg(ctx, instr->definitions[0], 8) << 17;
-      encoding |= (valu.opsel[3] ? 128 : 0) << 17;
+      encoding |= (valu.opsel[3] ? 128u : 0u) << 17;
    }
    encoding |= opcode << 9;
    if (!instr->operands.empty()) {
       encoding |= reg(ctx, instr->operands[0]);
-      encoding |= valu.opsel[0] ? 128 : 0;
+      encoding |= valu.opsel[0] ? 128u : 0u;
    }
    out.push_back(encoding);
 }
@@ -376,23 +407,24 @@ emit_vop1_instruction(asm_context& ctx,
 void
 emit_vopc_instruction(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[static_cast<int>(instr->opcode)];
    const VALU_instruction& valu = instr->valu();
 
-   uint32_t encoding = (0b0111110 << 25);
+   uint32_t encoding = (0b0111110u << 25);
    encoding |= opcode << 17;
    encoding |= reg(ctx, instr->operands[1], 8) << 9;
-   encoding |= (valu.opsel[1] ? 128 : 0) << 9;
+   encoding |= (valu.opsel[1] ? 128u : 0u) << 9;
    encoding |= reg(ctx, instr->operands[0]);
-   encoding |= valu.opsel[0] ? 128 : 0;
+   encoding |= valu.opsel[0] ? 128u : 0u;
    out.push_back(encoding);
 }
 
 void
 emit_vintrp_instruction(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[static_cast<int>(instr->opcode)];
    const VINTRP_instruction& interp = instr->vintrp();
+   const enum amd_gfx_level gfx = ctx.gfx_level;
 
    uint32_t encoding = 0;
    if (instr->opcode == aco_opcode::v_interp_p1ll_f16 ||
@@ -400,15 +432,15 @@ emit_vintrp_instruction(asm_context& ctx
        instr->opcode == aco_opcode::v_interp_p2_legacy_f16 ||
        instr->opcode == aco_opcode::v_interp_p2_f16 ||
        instr->opcode == aco_opcode::v_interp_p2_hi_f16) {
-      if (ctx.gfx_level == GFX8 || ctx.gfx_level == GFX9) {
-         encoding = (0b110100 << 26);
-      } else if (ctx.gfx_level >= GFX10) {
-         encoding = (0b110101 << 26);
+      if (gfx == GFX8 || gfx == GFX9) [[likely]] {
+         encoding = (0b110100u << 26);
+      } else if (gfx >= GFX10) [[unlikely]] {
+         encoding = (0b110101u << 26);
       } else {
          UNREACHABLE("Unknown gfx_level.");
       }
 
-      unsigned opsel = instr->opcode == aco_opcode::v_interp_p2_hi_f16 ? 0x8 : 0;
+      unsigned opsel = instr->opcode == aco_opcode::v_interp_p2_hi_f16 ? 0x8u : 0u;
 
       encoding |= opcode << 16;
       encoding |= opsel << 11;
@@ -417,8 +449,8 @@ emit_vintrp_instruction(asm_context& ctx
 
       encoding = 0;
       encoding |= interp.attribute;
-      encoding |= interp.component << 6;
-      encoding |= interp.high_16bits << 8;
+      encoding |= static_cast<uint32_t>(interp.component) << 6;
+      encoding |= interp.high_16bits ? 1u << 8 : 0u;
       encoding |= reg(ctx, instr->operands[0]) << 9;
       if (instr->opcode == aco_opcode::v_interp_p2_f16 ||
           instr->opcode == aco_opcode::v_interp_p2_hi_f16 ||
@@ -428,19 +460,19 @@ emit_vintrp_instruction(asm_context& ctx
       }
       out.push_back(encoding);
    } else {
-      if (ctx.gfx_level == GFX8 || ctx.gfx_level == GFX9) {
-         encoding = (0b110101 << 26); /* Vega ISA doc says 110010 but it's wrong */
+      if (gfx == GFX8 || gfx == GFX9) [[likely]] {
+         encoding = (0b110101u << 26);
       } else {
-         encoding = (0b110010 << 26);
+         encoding = (0b110010u << 26);
       }
 
       assert(encoding);
       encoding |= reg(ctx, instr->definitions[0], 8) << 18;
       encoding |= opcode << 16;
-      encoding |= interp.attribute << 10;
-      encoding |= interp.component << 8;
+      encoding |= static_cast<uint32_t>(interp.attribute) << 10;
+      encoding |= static_cast<uint32_t>(interp.component) << 8;
       if (instr->opcode == aco_opcode::v_interp_mov_f32)
-         encoding |= (0x3 & instr->operands[0].constantValue());
+         encoding |= (0x3u & instr->operands[0].constantValue());
       else
          encoding |= reg(ctx, instr->operands[0], 8);
       out.push_back(encoding);
@@ -451,14 +483,14 @@ void
 emit_vinterp_inreg_instruction(asm_context& ctx, std::vector<uint32_t>& out,
                                const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[static_cast<int>(instr->opcode)];
    const VINTERP_inreg_instruction& interp = instr->vinterp_inreg();
 
-   uint32_t encoding = (0b11001101 << 24);
+   uint32_t encoding = (0b11001101u << 24);
    encoding |= reg(ctx, instr->definitions[0], 8);
-   encoding |= (uint32_t)interp.wait_exp << 8;
-   encoding |= (uint32_t)interp.opsel << 11;
-   encoding |= (uint32_t)interp.clamp << 15;
+   encoding |= static_cast<uint32_t>(interp.wait_exp) << 8;
+   encoding |= static_cast<uint32_t>(interp.opsel) << 11;
+   encoding |= static_cast<uint32_t>(interp.clamp) << 15;
    encoding |= opcode << 16;
    out.push_back(encoding);
 
@@ -466,25 +498,25 @@ emit_vinterp_inreg_instruction(asm_conte
    for (unsigned i = 0; i < instr->operands.size(); i++)
       encoding |= reg(ctx, instr->operands[i]) << (i * 9);
    for (unsigned i = 0; i < 3; i++)
-      encoding |= interp.neg[i] << (29 + i);
+      encoding |= interp.neg[i] ? 1u << (29 + i) : 0u;
    out.push_back(encoding);
 }
 
 void
 emit_vopd_instruction(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[static_cast<int>(instr->opcode)];
    const VOPD_instruction& vopd = instr->vopd();
 
-   uint32_t encoding = (0b110010 << 26);
+   uint32_t encoding = (0b110010u << 26);
    encoding |= reg(ctx, instr->operands[0]);
    if (instr->opcode != aco_opcode::v_dual_mov_b32)
       encoding |= reg(ctx, instr->operands[1], 8) << 9;
-   encoding |= (uint32_t)ctx.opcode[(int)vopd.opy] << 17;
+   encoding |= static_cast<uint32_t>(ctx.opcode[static_cast<int>(vopd.opy)]) << 17;
    encoding |= opcode << 22;
    out.push_back(encoding);
 
-   unsigned opy_start = get_vopd_opy_start(instr);
+   const unsigned opy_start = get_vopd_opy_start(instr);
 
    encoding = reg(ctx, instr->operands[opy_start]);
    if (vopd.opy != aco_opcode::v_dual_mov_b32)
@@ -497,31 +529,34 @@ emit_vopd_instruction(asm_context& ctx,
 void
 emit_ds_instruction(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[static_cast<int>(instr->opcode)];
    const DS_instruction& ds = instr->ds();
+   const enum amd_gfx_level gfx = ctx.gfx_level;
 
-   uint32_t encoding = (0b110110 << 26);
-   if (ctx.gfx_level == GFX8 || ctx.gfx_level == GFX9) {
+   uint32_t encoding = (0b110110u << 26);
+   if (gfx == GFX8 || gfx == GFX9) [[likely]] {
       encoding |= opcode << 17;
-      encoding |= (ds.gds ? 1 : 0) << 16;
+      encoding |= (ds.gds ? 1u : 0u) << 16;
    } else {
       encoding |= opcode << 18;
-      encoding |= (ds.gds ? 1 : 0) << 17;
+      encoding |= (ds.gds ? 1u : 0u) << 17;
    }
-   encoding |= ((0xFF & ds.offset1) << 8);
-   encoding |= (0xFFFF & ds.offset0);
+   encoding |= (static_cast<uint32_t>(ds.offset1) & 0xFFu) << 8;
+   encoding |= static_cast<uint32_t>(ds.offset0) & 0xFFFFu;
    out.push_back(encoding);
+
    encoding = 0;
    if (!instr->definitions.empty())
       encoding |= reg(ctx, instr->definitions.back(), 8) << 24;
+
    unsigned op_idx = 0;
-   for (unsigned vector_idx = 0; op_idx < MIN2(instr->operands.size(), 3); vector_idx++) {
+   for (unsigned vector_idx = 0; op_idx < MIN2(instr->operands.size(), 3u); vector_idx++) {
       assert(vector_idx < 3);
 
       const Operand& op = instr->operands[op_idx];
       if (op.physReg() != m0 && !op.isUndefined())
          encoding |= reg(ctx, op, 8) << (8 * vector_idx);
-      while (instr->operands[op_idx].isVectorAligned())
+      while (op_idx < instr->operands.size() && instr->operands[op_idx].isVectorAligned())
          ++op_idx;
       ++op_idx;
    }
@@ -531,13 +566,14 @@ emit_ds_instruction(asm_context& ctx, st
 void
 emit_ldsdir_instruction(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[(int)instr->opcode];
    const LDSDIR_instruction& dir = instr->ldsdir();
+   const enum amd_gfx_level gfx = ctx.gfx_level;
 
    uint32_t encoding = (0b11001110 << 24);
    encoding |= opcode << 20;
    encoding |= (uint32_t)dir.wait_vdst << 16;
-   if (ctx.gfx_level >= GFX12)
+   if (gfx >= GFX12)
       encoding |= (uint32_t)dir.wait_vsrc << 23;
    encoding |= (uint32_t)dir.attr << 10;
    encoding |= (uint32_t)dir.attr_chan << 8;
@@ -548,45 +584,46 @@ emit_ldsdir_instruction(asm_context& ctx
 void
 emit_mubuf_instruction(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[(int)instr->opcode];
    const MUBUF_instruction& mubuf = instr->mubuf();
-   bool glc = mubuf.cache.value & ac_glc;
-   bool slc = mubuf.cache.value & ac_slc;
-   bool dlc = mubuf.cache.value & ac_dlc;
+   const bool glc = mubuf.cache.value & ac_glc;
+   const bool slc = mubuf.cache.value & ac_slc;
+   const bool dlc = mubuf.cache.value & ac_dlc;
+   const enum amd_gfx_level gfx = ctx.gfx_level;
 
    uint32_t encoding = (0b111000 << 26);
-   encoding |= (mubuf.lds ? 1 : 0) << 16;
+   encoding |= (mubuf.lds ? 1u : 0u) << 16;
    encoding |= opcode << 18;
-   encoding |= (glc ? 1 : 0) << 14;
-   if (ctx.gfx_level <= GFX10_3)
-      encoding |= (mubuf.idxen ? 1 : 0) << 13;
-   assert(!mubuf.addr64 || ctx.gfx_level <= GFX7);
-   if (ctx.gfx_level == GFX6 || ctx.gfx_level == GFX7)
-      encoding |= (mubuf.addr64 ? 1 : 0) << 15;
-   if (ctx.gfx_level <= GFX10_3)
-      encoding |= (mubuf.offen ? 1 : 0) << 12;
-   if (ctx.gfx_level == GFX8 || ctx.gfx_level == GFX9) {
-      assert(!dlc); /* Device-level coherent is not supported on GFX9 and lower */
-      encoding |= (slc ? 1 : 0) << 17;
-   } else if (ctx.gfx_level >= GFX11) {
-      encoding |= (slc ? 1 : 0) << 12;
-      encoding |= (dlc ? 1 : 0) << 13;
-   } else if (ctx.gfx_level >= GFX10) {
-      encoding |= (dlc ? 1 : 0) << 15;
+   encoding |= (glc ? 1u : 0u) << 14;
+   if (gfx <= GFX10_3) [[likely]]
+      encoding |= (mubuf.idxen ? 1u : 0u) << 13;
+   assert(!mubuf.addr64 || gfx <= GFX7);
+   if (gfx == GFX6 || gfx == GFX7)
+      encoding |= (mubuf.addr64 ? 1u : 0u) << 15;
+   if (gfx <= GFX10_3) [[likely]]
+      encoding |= (mubuf.offen ? 1u : 0u) << 12;
+   if (gfx == GFX8 || gfx == GFX9) {
+      assert(!dlc);
+      encoding |= (slc ? 1u : 0u) << 17;
+   } else if (gfx >= GFX11) [[unlikely]] {
+      encoding |= (slc ? 1u : 0u) << 12;
+      encoding |= (dlc ? 1u : 0u) << 13;
+   } else if (gfx >= GFX10) [[unlikely]] {
+      encoding |= (dlc ? 1u : 0u) << 15;
    }
-   encoding |= 0x0FFF & mubuf.offset;
+   encoding |= 0x0FFFu & mubuf.offset;
    out.push_back(encoding);
    encoding = 0;
-   if (ctx.gfx_level <= GFX7 || (ctx.gfx_level >= GFX10 && ctx.gfx_level <= GFX10_3)) {
-      encoding |= (slc ? 1 : 0) << 22;
+   if (gfx <= GFX7 || (gfx >= GFX10 && gfx <= GFX10_3)) {
+      encoding |= (slc ? 1u : 0u) << 22;
    }
    encoding |= reg(ctx, instr->operands[2]) << 24;
-   if (ctx.gfx_level >= GFX11) {
-      encoding |= (mubuf.tfe ? 1 : 0) << 21;
-      encoding |= (mubuf.offen ? 1 : 0) << 22;
-      encoding |= (mubuf.idxen ? 1 : 0) << 23;
+   if (gfx >= GFX11) [[unlikely]] {
+      encoding |= (mubuf.tfe ? 1u : 0u) << 21;
+      encoding |= (mubuf.offen ? 1u : 0u) << 22;
+      encoding |= (mubuf.idxen ? 1u : 0u) << 23;
    } else {
-      encoding |= (mubuf.tfe ? 1 : 0) << 23;
+      encoding |= (mubuf.tfe ? 1u : 0u) << 23;
    }
    encoding |= (reg(ctx, instr->operands[0]) >> 2) << 16;
    if (instr->operands.size() > 3 && !mubuf.lds)
@@ -600,7 +637,7 @@ emit_mubuf_instruction(asm_context& ctx,
 void
 emit_mubuf_instruction_gfx12(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[(int)instr->opcode];
    const MUBUF_instruction& mubuf = instr->mubuf();
    assert(!mubuf.lds);
 
@@ -612,7 +649,7 @@ emit_mubuf_instruction_gfx12(asm_context
    } else {
       encoding |= reg(ctx, instr->operands[2]);
    }
-   encoding |= (mubuf.tfe ? 1 : 0) << 22;
+   encoding |= (mubuf.tfe ? 1u : 0u) << 22;
    out.push_back(encoding);
 
    encoding = 0;
@@ -621,65 +658,64 @@ emit_mubuf_instruction_gfx12(asm_context
    else
       encoding |= reg(ctx, instr->definitions[0], 8);
    encoding |= reg(ctx, instr->operands[0]) << 9;
-   encoding |= (mubuf.offen ? 1 : 0) << 30;
-   encoding |= (mubuf.idxen ? 1 : 0) << 31;
+   encoding |= (mubuf.offen ? 1u : 0u) << 30;
+   encoding |= (mubuf.idxen ? 1u : 0u) << 31;
    encoding |= get_gfx12_cpol(mubuf) << 18;
-   encoding |= 1 << 23;
+   encoding |= 1u << 23;
    out.push_back(encoding);
 
    encoding = 0;
    if (!instr->operands[1].isUndefined())
       encoding |= reg(ctx, instr->operands[1], 8);
-   encoding |= (mubuf.offset & 0x00ffffff) << 8;
+   encoding |= (mubuf.offset & 0x00ffffffu) << 8;
    out.push_back(encoding);
 }
 
 void
 emit_mtbuf_instruction(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[(int)instr->opcode];
    const MTBUF_instruction& mtbuf = instr->mtbuf();
-   bool glc = mtbuf.cache.value & ac_glc;
-   bool slc = mtbuf.cache.value & ac_slc;
-   bool dlc = mtbuf.cache.value & ac_dlc;
-   uint32_t img_format = ac_get_tbuffer_format(ctx.gfx_level, mtbuf.dfmt, mtbuf.nfmt);
+   const bool glc = mtbuf.cache.value & ac_glc;
+   const bool slc = mtbuf.cache.value & ac_slc;
+   const bool dlc = mtbuf.cache.value & ac_dlc;
+   const enum amd_gfx_level gfx = ctx.gfx_level;
+   const uint32_t img_format = ac_get_tbuffer_format(gfx, mtbuf.dfmt, mtbuf.nfmt);
    assert(img_format <= 0x7F);
-   assert(!dlc || ctx.gfx_level >= GFX10);
+   assert(!dlc || gfx >= GFX10);
 
    uint32_t encoding = (0b111010 << 26);
-   encoding |= (img_format << 19); /* Handles both the GFX10 FORMAT and the old NFMT+DFMT */
-   if (ctx.gfx_level < GFX8) {
+   encoding |= (img_format << 19);
+   if (gfx < GFX8) {
       encoding |= opcode << 16;
-      /* ADDR64 is unused */
-   } else if (ctx.gfx_level >= GFX10 && ctx.gfx_level < GFX11) {
-      /* DLC bit replaces one bit of the OPCODE on GFX10 */
-      encoding |= (opcode & 0x07) << 16; /* 3 LSBs of 4-bit OPCODE */
-      encoding |= (dlc ? 1 : 0) << 15;
+   } else if (gfx >= GFX10 && gfx < GFX11) {
+      encoding |= (opcode & 0x07u) << 16;
+      encoding |= (dlc ? 1u : 0u) << 15;
    } else {
       encoding |= opcode << 15;
    }
-   encoding |= (glc ? 1 : 0) << 14;
-   if (ctx.gfx_level >= GFX11) {
-      encoding |= (dlc ? 1 : 0) << 13;
-      encoding |= (slc ? 1 : 0) << 12;
+   encoding |= (glc ? 1u : 0u) << 14;
+   if (gfx >= GFX11) [[unlikely]] {
+      encoding |= (dlc ? 1u : 0u) << 13;
+      encoding |= (slc ? 1u : 0u) << 12;
    } else {
-      encoding |= (mtbuf.idxen ? 1 : 0) << 13;
-      encoding |= (mtbuf.offen ? 1 : 0) << 12;
+      encoding |= (mtbuf.idxen ? 1u : 0u) << 13;
+      encoding |= (mtbuf.offen ? 1u : 0u) << 12;
    }
-   encoding |= 0x0FFF & mtbuf.offset;
+   encoding |= 0x0FFFu & mtbuf.offset;
    out.push_back(encoding);
 
    encoding = 0;
    encoding |= reg(ctx, instr->operands[2]) << 24;
-   if (ctx.gfx_level >= GFX11) {
-      encoding |= (mtbuf.idxen ? 1 : 0) << 23;
-      encoding |= (mtbuf.offen ? 1 : 0) << 22;
-      encoding |= (mtbuf.tfe ? 1 : 0) << 21;
-   } else {
-      encoding |= (mtbuf.tfe ? 1 : 0) << 23;
-      encoding |= (slc ? 1 : 0) << 22;
-      if (ctx.gfx_level >= GFX10)
-         encoding |= (((opcode & 0x08) >> 3) << 21); /* MSB of 4-bit OPCODE */
+   if (gfx >= GFX11) [[unlikely]] {
+      encoding |= (mtbuf.idxen ? 1u : 0u) << 23;
+      encoding |= (mtbuf.offen ? 1u : 0u) << 22;
+      encoding |= (mtbuf.tfe ? 1u : 0u) << 21;
+   } else {
+      encoding |= (mtbuf.tfe ? 1u : 0u) << 23;
+      encoding |= (slc ? 1u : 0u) << 22;
+      if (gfx >= GFX10)
+         encoding |= (((opcode & 0x08u) >> 3) << 21);
    }
    encoding |= (reg(ctx, instr->operands[0]) >> 2) << 16;
    if (instr->operands.size() > 3)
@@ -693,10 +729,11 @@ emit_mtbuf_instruction(asm_context& ctx,
 void
 emit_mtbuf_instruction_gfx12(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[(int)instr->opcode];
    const MTBUF_instruction& mtbuf = instr->mtbuf();
+   const enum amd_gfx_level gfx = ctx.gfx_level;
 
-   uint32_t img_format = ac_get_tbuffer_format(ctx.gfx_level, mtbuf.dfmt, mtbuf.nfmt);
+   const uint32_t img_format = ac_get_tbuffer_format(gfx, mtbuf.dfmt, mtbuf.nfmt);
 
    uint32_t encoding = 0b110001 << 26;
    encoding |= 0b1000 << 18;
@@ -707,7 +744,7 @@ emit_mtbuf_instruction_gfx12(asm_context
    } else {
       encoding |= reg(ctx, instr->operands[2]);
    }
-   encoding |= (mtbuf.tfe ? 1 : 0) << 22;
+   encoding |= (mtbuf.tfe ? 1u : 0u) << 22;
    out.push_back(encoding);
 
    encoding = 0;
@@ -716,91 +753,90 @@ emit_mtbuf_instruction_gfx12(asm_context
    else
       encoding |= reg(ctx, instr->definitions[0], 8);
    encoding |= reg(ctx, instr->operands[0]) << 9;
-   encoding |= (mtbuf.offen ? 1 : 0) << 30;
-   encoding |= (mtbuf.idxen ? 1 : 0) << 31;
+   encoding |= (mtbuf.offen ? 1u : 0u) << 30;
+   encoding |= (mtbuf.idxen ? 1u : 0u) << 31;
    encoding |= get_gfx12_cpol(mtbuf) << 18;
    encoding |= img_format << 23;
    out.push_back(encoding);
 
    encoding = 0;
    encoding |= reg(ctx, instr->operands[1], 8);
-   encoding |= (mtbuf.offset & 0x00ffffff) << 8;
+   encoding |= (mtbuf.offset & 0x00ffffffu) << 8;
    out.push_back(encoding);
 }
 
 void
 emit_mimg_instruction(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[(int)instr->opcode];
    const MIMG_instruction& mimg = instr->mimg();
-   bool glc = mimg.cache.value & ac_glc;
-   bool slc = mimg.cache.value & ac_slc;
-   bool dlc = mimg.cache.value & ac_dlc;
+   const bool glc = mimg.cache.value & ac_glc;
+   const bool slc = mimg.cache.value & ac_slc;
+   const bool dlc = mimg.cache.value & ac_dlc;
+   const enum amd_gfx_level gfx = ctx.gfx_level;
 
    unsigned nsa_dwords = get_mimg_nsa_dwords(instr);
-   assert(!nsa_dwords || ctx.gfx_level >= GFX10);
+   assert(!nsa_dwords || gfx >= GFX10);
 
    uint32_t encoding = (0b111100 << 26);
-   if (ctx.gfx_level >= GFX11) { /* GFX11: rearranges most fields */
+   if (gfx >= GFX11) [[unlikely]] {
       assert(nsa_dwords <= 1);
       encoding |= nsa_dwords;
       encoding |= mimg.dim << 2;
-      encoding |= mimg.unrm ? 1 << 7 : 0;
-      encoding |= (0xF & mimg.dmask) << 8;
-      encoding |= slc ? 1 << 12 : 0;
-      encoding |= dlc ? 1 << 13 : 0;
-      encoding |= glc ? 1 << 14 : 0;
-      encoding |= mimg.r128 ? 1 << 15 : 0;
-      encoding |= mimg.a16 ? 1 << 16 : 0;
-      encoding |= mimg.d16 ? 1 << 17 : 0;
-      encoding |= (opcode & 0xFF) << 18;
-   } else {
-      encoding |= slc ? 1 << 25 : 0;
-      encoding |= (opcode & 0x7f) << 18;
-      encoding |= (opcode >> 7) & 1;
-      encoding |= mimg.lwe ? 1 << 17 : 0;
-      encoding |= mimg.tfe ? 1 << 16 : 0;
-      encoding |= glc ? 1 << 13 : 0;
-      encoding |= mimg.unrm ? 1 << 12 : 0;
-      if (ctx.gfx_level <= GFX9) {
-         assert(!dlc); /* Device-level coherent is not supported on GFX9 and lower */
+      encoding |= mimg.unrm ? 1u << 7 : 0u;
+      encoding |= (0xFu & mimg.dmask) << 8;
+      encoding |= slc ? 1u << 12 : 0u;
+      encoding |= dlc ? 1u << 13 : 0u;
+      encoding |= glc ? 1u << 14 : 0u;
+      encoding |= mimg.r128 ? 1u << 15 : 0u;
+      encoding |= mimg.a16 ? 1u << 16 : 0u;
+      encoding |= mimg.d16 ? 1u << 17 : 0u;
+      encoding |= (opcode & 0xFFu) << 18;
+   } else {
+      encoding |= slc ? 1u << 25 : 0u;
+      encoding |= (opcode & 0x7fu) << 18;
+      encoding |= (opcode >> 7) & 1u;
+      encoding |= mimg.lwe ? 1u << 17 : 0u;
+      encoding |= mimg.tfe ? 1u << 16 : 0u;
+      encoding |= glc ? 1u << 13 : 0u;
+      encoding |= mimg.unrm ? 1u << 12 : 0u;
+      if (gfx <= GFX9) {
+         assert(!dlc);
          assert(!mimg.r128);
-         encoding |= mimg.a16 ? 1 << 15 : 0;
-         encoding |= mimg.da ? 1 << 14 : 0;
+         encoding |= mimg.a16 ? 1u << 15 : 0u;
+         encoding |= mimg.da ? 1u << 14 : 0u;
       } else {
-         encoding |= mimg.r128 ? 1 << 15
-                               : 0; /* GFX10: A16 moved to 2nd word, R128 replaces it in 1st word */
+         encoding |= mimg.r128 ? 1u << 15 : 0u;
          encoding |= nsa_dwords << 1;
-         encoding |= mimg.dim << 3; /* GFX10: dimensionality instead of declare array */
-         encoding |= dlc ? 1 << 7 : 0;
+         encoding |= mimg.dim << 3;
+         encoding |= dlc ? 1u << 7 : 0u;
       }
-      encoding |= (0xF & mimg.dmask) << 8;
+      encoding |= (0xFu & mimg.dmask) << 8;
    }
    out.push_back(encoding);
 
-   encoding = reg(ctx, instr->operands[3], 8); /* VADDR */
+   encoding = reg(ctx, instr->operands[3], 8);
    if (!instr->definitions.empty()) {
-      encoding |= reg(ctx, instr->definitions[0], 8) << 8; /* VDATA */
+      encoding |= reg(ctx, instr->definitions[0], 8) << 8;
    } else if (!instr->operands[2].isUndefined()) {
-      encoding |= reg(ctx, instr->operands[2], 8) << 8; /* VDATA */
+      encoding |= reg(ctx, instr->operands[2], 8) << 8;
    }
-   encoding |= (0x1F & (reg(ctx, instr->operands[0]) >> 2)) << 16; /* T# (resource) */
+   encoding |= (0x1Fu & (reg(ctx, instr->operands[0]) >> 2)) << 16;
 
-   assert(!mimg.d16 || ctx.gfx_level >= GFX9);
-   if (ctx.gfx_level >= GFX11) {
+   assert(!mimg.d16 || gfx >= GFX9);
+   if (gfx >= GFX11) [[unlikely]] {
       if (!instr->operands[1].isUndefined())
-         encoding |= (0x1F & (reg(ctx, instr->operands[1]) >> 2)) << 26; /* sampler */
+         encoding |= (0x1Fu & (reg(ctx, instr->operands[1]) >> 2)) << 26;
 
-      encoding |= mimg.tfe ? 1 << 21 : 0;
-      encoding |= mimg.lwe ? 1 << 22 : 0;
+      encoding |= mimg.tfe ? 1u << 21 : 0u;
+      encoding |= mimg.lwe ? 1u << 22 : 0u;
    } else {
       if (!instr->operands[1].isUndefined())
-         encoding |= (0x1F & (reg(ctx, instr->operands[1]) >> 2)) << 21; /* sampler */
+         encoding |= (0x1Fu & (reg(ctx, instr->operands[1]) >> 2)) << 21;
 
-      encoding |= mimg.d16 ? 1 << 31 : 0;
-      if (ctx.gfx_level >= GFX10) {
-         /* GFX10: A16 still exists, but is in a different place */
-         encoding |= mimg.a16 ? 1 << 30 : 0;
+      encoding |= mimg.d16 ? 1u << 31 : 0u;
+      if (gfx >= GFX10) {
+         encoding |= mimg.a16 ? 1u << 30 : 0u;
       }
    }
 
@@ -821,7 +857,7 @@ emit_mimg_instruction(asm_context& ctx,
 void
 emit_mimg_instruction_gfx12(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[(int)instr->opcode];
    const MIMG_instruction& mimg = instr->mimg();
 
    bool vsample = !instr->operands[1].isUndefined() || instr->opcode == aco_opcode::image_msaa_load;
@@ -837,7 +873,7 @@ emit_mimg_instruction_gfx12(asm_context&
    encoding |= mimg.r128 << 4;
    encoding |= mimg.d16 << 5;
    encoding |= mimg.a16 << 6;
-   encoding |= (mimg.dmask & 0xf) << 22;
+   encoding |= (mimg.dmask & 0xfu) << 22;
    out.push_back(encoding);
 
    uint8_t vaddr[5] = {0, 0, 0, 0, 0};
@@ -852,14 +888,14 @@ emit_mimg_instruction_gfx12(asm_context&
 
    encoding = 0;
    if (!instr->definitions.empty())
-      encoding |= reg(ctx, instr->definitions.back(), 8); /* VDATA */
+      encoding |= reg(ctx, instr->definitions.back(), 8);
    else if (!instr->operands[2].isUndefined())
-      encoding |= reg(ctx, instr->operands[2], 8); /* VDATA */
-   encoding |= reg(ctx, instr->operands[0]) << 9;  /* T# (resource) */
+      encoding |= reg(ctx, instr->operands[2], 8);
+   encoding |= reg(ctx, instr->operands[0]) << 9;
    if (vsample) {
       encoding |= mimg.lwe << 8;
       if (instr->opcode != aco_opcode::image_msaa_load)
-         encoding |= reg(ctx, instr->operands[1]) << 23; /* sampler */
+         encoding |= reg(ctx, instr->operands[1]) << 23;
    } else {
       encoding |= mimg.tfe << 23;
       encoding |= vaddr[4] << 24;
@@ -876,39 +912,37 @@ emit_mimg_instruction_gfx12(asm_context&
 void
 emit_flatlike_instruction(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[(int)instr->opcode];
    const FLAT_instruction& flat = instr->flatlike();
-   bool glc = flat.cache.value & ac_glc;
-   bool slc = flat.cache.value & ac_slc;
-   bool dlc = flat.cache.value & ac_dlc;
+   const bool glc = flat.cache.value & ac_glc;
+   const bool slc = flat.cache.value & ac_slc;
+   const bool dlc = flat.cache.value & ac_dlc;
+   const enum amd_gfx_level gfx = ctx.gfx_level;
 
    uint32_t encoding = (0b110111 << 26);
    encoding |= opcode << 18;
-   if (ctx.gfx_level == GFX9 || ctx.gfx_level >= GFX11) {
+   if (gfx == GFX9 || gfx >= GFX11) {
       if (instr->isFlat())
          assert(flat.offset <= 0xfff);
       else
          assert(flat.offset >= -4096 && flat.offset < 4096);
-      encoding |= flat.offset & 0x1fff;
-   } else if (ctx.gfx_level <= GFX8 || instr->isFlat()) {
-      /* GFX10 has a 12-bit immediate OFFSET field,
-       * but it has a hw bug: it ignores the offset, called FlatSegmentOffsetBug
-       */
+      encoding |= flat.offset & 0x1fffu;
+   } else if (gfx <= GFX8 || instr->isFlat()) {
       assert(flat.offset == 0);
    } else {
       assert(flat.offset >= -2048 && flat.offset <= 2047);
-      encoding |= flat.offset & 0xfff;
+      encoding |= flat.offset & 0xfffu;
    }
    if (instr->isScratch())
-      encoding |= 1 << (ctx.gfx_level >= GFX11 ? 16 : 14);
+      encoding |= 1u << (gfx >= GFX11 ? 16 : 14);
    else if (instr->isGlobal())
-      encoding |= 2 << (ctx.gfx_level >= GFX11 ? 16 : 14);
-   encoding |= flat.lds ? 1 << 13 : 0;
-   encoding |= glc ? 1 << (ctx.gfx_level >= GFX11 ? 14 : 16) : 0;
-   encoding |= slc ? 1 << (ctx.gfx_level >= GFX11 ? 15 : 17) : 0;
-   if (ctx.gfx_level >= GFX10) {
+      encoding |= 2u << (gfx >= GFX11 ? 16 : 14);
+   encoding |= flat.lds ? 1u << 13 : 0u;
+   encoding |= glc ? 1u << (gfx >= GFX11 ? 14 : 16) : 0u;
+   encoding |= slc ? 1u << (gfx >= GFX11 ? 15 : 17) : 0u;
+   if (gfx >= GFX10) [[unlikely]] {
       assert(!flat.nv);
-      encoding |= dlc ? 1 << (ctx.gfx_level >= GFX11 ? 13 : 12) : 0;
+      encoding |= dlc ? 1u << (gfx >= GFX11 ? 13 : 12) : 0u;
    } else {
       assert(!dlc);
    }
@@ -919,24 +953,20 @@ emit_flatlike_instruction(asm_context& c
    if (instr->operands.size() >= 3)
       encoding |= reg(ctx, instr->operands[2], 8) << 8;
    if (!instr->operands[1].isUndefined()) {
-      assert(ctx.gfx_level >= GFX10 || instr->operands[1].physReg() != 0x7F);
+      assert(gfx >= GFX10 || instr->operands[1].physReg() != 0x7F);
       assert(instr->format != Format::FLAT);
       encoding |= reg(ctx, instr->operands[1], 8) << 16;
-   } else if (instr->format != Format::FLAT ||
-              ctx.gfx_level >= GFX10) { /* SADDR is actually used with FLAT on GFX10 */
-      /* For GFX10.3 scratch, 0x7F disables both ADDR and SADDR, unlike sgpr_null, which only
-       * disables SADDR. On GFX11, this was replaced with SVE.
-       */
-      if (ctx.gfx_level <= GFX9 ||
-          (instr->isScratch() && instr->operands[0].isUndefined() && ctx.gfx_level < GFX11))
-         encoding |= 0x7F << 16;
+   } else if (instr->format != Format::FLAT || gfx >= GFX10) {
+      if (gfx <= GFX9 ||
+          (instr->isScratch() && instr->operands[0].isUndefined() && gfx < GFX11))
+         encoding |= 0x7Fu << 16;
       else
          encoding |= reg(ctx, sgpr_null) << 16;
    }
-   if (ctx.gfx_level >= GFX11 && instr->isScratch())
-      encoding |= !instr->operands[0].isUndefined() ? 1 << 23 : 0;
+   if (gfx >= GFX11 && instr->isScratch())
+      encoding |= !instr->operands[0].isUndefined() ? 1u << 23 : 0u;
    else
-      encoding |= flat.nv ? 1 << 23 : 0;
+      encoding |= flat.nv ? 1u << 23 : 0u;
    out.push_back(encoding);
 }
 
@@ -944,7 +974,7 @@ void
 emit_flatlike_instruction_gfx12(asm_context& ctx, std::vector<uint32_t>& out,
                                 const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[(int)instr->opcode];
    const FLAT_instruction& flat = instr->flatlike();
    assert(!flat.lds);
 
@@ -957,16 +987,16 @@ emit_flatlike_instruction_gfx12(asm_cont
       encoding |= reg(ctx, sgpr_null);
    }
    if (instr->isScratch())
-      encoding |= 1 << 24;
+      encoding |= 1u << 24;
    else if (instr->isGlobal())
-      encoding |= 2 << 24;
+      encoding |= 2u << 24;
    out.push_back(encoding);
 
    encoding = 0;
    if (!instr->definitions.empty())
       encoding |= reg(ctx, instr->definitions[0], 8);
    if (instr->isScratch())
-      encoding |= !instr->operands[0].isUndefined() ? 1 << 17 : 0;
+      encoding |= !instr->operands[0].isUndefined() ? 1u << 17 : 0u;
    encoding |= get_gfx12_cpol(flat) << 18;
    if (instr->operands.size() >= 3)
       encoding |= reg(ctx, instr->operands[2], 8) << 23;
@@ -975,7 +1005,7 @@ emit_flatlike_instruction_gfx12(asm_cont
    encoding = 0;
    if (!instr->operands[0].isUndefined())
       encoding |= reg(ctx, instr->operands[0], 8);
-   encoding |= (flat.offset & 0x00ffffff) << 8;
+   encoding |= (flat.offset & 0x00ffffffu) << 8;
    out.push_back(encoding);
 }
 
@@ -983,20 +1013,21 @@ void
 emit_exp_instruction(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr)
 {
    const Export_instruction& exp = instr->exp();
+   const enum amd_gfx_level gfx = ctx.gfx_level;
    uint32_t encoding;
-   if (ctx.gfx_level == GFX8 || ctx.gfx_level == GFX9) {
+   if (gfx == GFX8 || gfx == GFX9) {
       encoding = (0b110001 << 26);
    } else {
       encoding = (0b111110 << 26);
    }
 
-   if (ctx.gfx_level >= GFX11) {
-      encoding |= exp.row_en ? 0b1 << 13 : 0;
+   if (gfx >= GFX11) [[unlikely]] {
+      encoding |= exp.row_en ? 0b1u << 13 : 0u;
    } else {
-      encoding |= exp.valid_mask ? 0b1 << 12 : 0;
-      encoding |= exp.compressed ? 0b1 << 10 : 0;
+      encoding |= exp.valid_mask ? 0b1u << 12 : 0u;
+      encoding |= exp.compressed ? 0b1u << 10 : 0u;
    }
-   encoding |= exp.done ? 0b1 << 11 : 0;
+   encoding |= exp.done ? 0b1u << 11 : 0u;
    encoding |= exp.dest << 4;
    encoding |= exp.enabled_mask;
 
@@ -1023,7 +1054,6 @@ emit_dpp16_instruction(asm_context& ctx,
    assert(ctx.gfx_level >= GFX8);
    DPP16_instruction& dpp = instr->dpp16();
 
-   /* first emit the instruction without the DPP operand */
    Operand dpp_op = instr->operands[0];
    instr->operands[0] = Operand(PhysReg{250}, v1);
    instr->format = (Format)((uint16_t)instr->format & ~(uint16_t)Format::DPP16);
@@ -1031,8 +1061,8 @@ emit_dpp16_instruction(asm_context& ctx,
    instr->format = (Format)((uint16_t)instr->format | (uint16_t)Format::DPP16);
    instr->operands[0] = dpp_op;
 
-   uint32_t encoding = (0xF & dpp.row_mask) << 28;
-   encoding |= (0xF & dpp.bank_mask) << 24;
+   uint32_t encoding = (0xFu & dpp.row_mask) << 28;
+   encoding |= (0xFu & dpp.bank_mask) << 24;
    encoding |= dpp.abs[1] << 23;
    encoding |= dpp.neg[1] << 22;
    encoding |= dpp.abs[0] << 21;
@@ -1051,7 +1081,6 @@ emit_dpp8_instruction(asm_context& ctx,
    assert(ctx.gfx_level >= GFX10);
    DPP8_instruction& dpp = instr->dpp8();
 
-   /* first emit the instruction without the DPP operand */
    Operand dpp_op = instr->operands[0];
    instr->operands[0] = Operand(PhysReg{233u + dpp.fetch_inactive}, v1);
    instr->format = (Format)((uint16_t)instr->format & ~(uint16_t)Format::DPP8);
@@ -1070,59 +1099,59 @@ emit_vop3_instruction(asm_context& ctx,
 {
    uint32_t opcode = ctx.opcode[(int)instr->opcode];
    const VALU_instruction& vop3 = instr->valu();
+   const enum amd_gfx_level gfx = ctx.gfx_level;
 
+   /* Adjust opcode based on original instruction format */
    if (instr->isVOP2()) {
-      opcode = opcode + 0x100;
+      opcode += 0x100;
    } else if (instr->isVOP1()) {
-      if (ctx.gfx_level == GFX8 || ctx.gfx_level == GFX9)
-         opcode = opcode + 0x140;
-      else
-         opcode = opcode + 0x180;
-   } else if (instr->isVOPC()) {
-      opcode = opcode + 0x0;
+      opcode += (gfx == GFX8 || gfx == GFX9) ? 0x140 : 0x180;
    } else if (instr->isVINTRP()) {
-      opcode = opcode + 0x270;
+      opcode += 0x270;
    }
+   /* VOPC: no adjustment needed (offset is 0) */
 
    uint32_t encoding;
-   if (ctx.gfx_level <= GFX9) {
+   if (gfx <= GFX9) {
       encoding = (0b110100 << 26);
-   } else if (ctx.gfx_level >= GFX10) {
+   } else if (gfx >= GFX10) {
       encoding = (0b110101 << 26);
    } else {
       UNREACHABLE("Unknown gfx_level.");
    }
 
-   if (ctx.gfx_level <= GFX7) {
+   if (gfx <= GFX7) [[unlikely]] {
       encoding |= opcode << 17;
-      encoding |= (vop3.clamp ? 1 : 0) << 11;
+      encoding |= (vop3.clamp ? 1u : 0u) << 11;
    } else {
       encoding |= opcode << 16;
-      encoding |= (vop3.clamp ? 1 : 0) << 15;
+      encoding |= (vop3.clamp ? 1u : 0u) << 15;
    }
    encoding |= vop3.opsel << 11;
    for (unsigned i = 0; i < 3; i++)
       encoding |= vop3.abs[i] << (8 + i);
-   /* On GFX9 and older, v_cmpx implicitly writes exec besides writing an SGPR pair.
-    * On GFX10 and newer, v_cmpx always writes just exec.
-    */
    if (instr->definitions.size() == 2 && instr->isVOPC())
-      assert(ctx.gfx_level <= GFX9 && instr->definitions[1].physReg() == exec);
+      assert(gfx <= GFX9 && instr->definitions[1].physReg() == exec);
    else if (instr->definitions.size() == 2 && instr->opcode != aco_opcode::v_swap_b16)
       encoding |= reg(ctx, instr->definitions[1]) << 8;
    encoding |= reg(ctx, instr->definitions[0], 8);
    out.push_back(encoding);
-   encoding = 0;
 
-   unsigned num_ops = instr->operands.size();
-   /* Encoding implicit sources works fine with hardware but breaks some disassemblers. */
-   if (instr->opcode == aco_opcode::v_writelane_b32_e64)
-      num_ops = 2;
-   else if (instr->opcode == aco_opcode::v_swap_b16)
-      num_ops = 1;
+   encoding = 0;
+   const unsigned num_ops = instr->opcode == aco_opcode::v_writelane_b32_e64 ? 2u :
+                            instr->opcode == aco_opcode::v_swap_b16 ? 1u :
+                            static_cast<unsigned>(instr->operands.size());
 
    for (unsigned i = 0; i < num_ops; i++)
       encoding |= reg(ctx, instr->operands[i]) << (i * 9);
+
+   /* RDNA (GFX10+): encode unused operand slots as inline constant 0 (encoding 128)
+    * for decoder performance. GFX9 and earlier leave unused slots as zero. */
+   if (gfx >= GFX10) [[unlikely]] {
+      for (unsigned i = num_ops; i < 3; i++)
+         encoding |= 128u << (i * 9);
+   }
+
    encoding |= vop3.omod << 27;
    for (unsigned i = 0; i < 3; i++)
       encoding |= vop3.neg[i] << (29 + i);
@@ -1132,30 +1161,41 @@ emit_vop3_instruction(asm_context& ctx,
 void
 emit_vop3p_instruction(asm_context& ctx, std::vector<uint32_t>& out, const Instruction* instr)
 {
-   uint32_t opcode = ctx.opcode[(int)instr->opcode];
+   const uint32_t opcode = ctx.opcode[(int)instr->opcode];
    const VALU_instruction& vop3 = instr->valu();
+   const enum amd_gfx_level gfx = ctx.gfx_level;
 
    uint32_t encoding;
-   if (ctx.gfx_level == GFX9) {
+   if (gfx == GFX9) {
       encoding = (0b110100111 << 23);
-   } else if (ctx.gfx_level >= GFX10) {
+   } else if (gfx >= GFX10) {
       encoding = (0b110011 << 26);
    } else {
       UNREACHABLE("Unknown gfx_level.");
    }
 
    encoding |= opcode << 16;
-   encoding |= (vop3.clamp ? 1 : 0) << 15;
+   encoding |= (vop3.clamp ? 1u : 0u) << 15;
    encoding |= vop3.opsel_lo << 11;
-   encoding |= ((vop3.opsel_hi & 0x4) ? 1 : 0) << 14;
+   encoding |= ((vop3.opsel_hi & 0x4u) ? 1u : 0u) << 14;
    for (unsigned i = 0; i < 3; i++)
       encoding |= vop3.neg_hi[i] << (8 + i);
    encoding |= reg(ctx, instr->definitions[0], 8);
    out.push_back(encoding);
+
    encoding = 0;
-   for (unsigned i = 0; i < instr->operands.size(); i++)
+   const unsigned num_ops = static_cast<unsigned>(instr->operands.size());
+   for (unsigned i = 0; i < num_ops; i++)
       encoding |= reg(ctx, instr->operands[i]) << (i * 9);
-   encoding |= (vop3.opsel_hi & 0x3) << 27;
+
+   /* RDNA (GFX10+): encode unused operand slots as inline constant 0 (encoding 128)
+    * for decoder performance. GFX9 and earlier leave unused slots as zero. */
+   if (gfx >= GFX10) [[unlikely]] {
+      for (unsigned i = num_ops; i < 3; i++)
+         encoding |= 128u << (i * 9);
+   }
+
+   encoding |= (vop3.opsel_hi & 0x3u) << 27;
    for (unsigned i = 0; i < 3; i++)
       encoding |= vop3.neg_lo[i] << (29 + i);
    out.push_back(encoding);
@@ -1166,8 +1206,8 @@ emit_sdwa_instruction(asm_context& ctx,
 {
    assert(ctx.gfx_level >= GFX8 && ctx.gfx_level < GFX11);
    SDWA_instruction& sdwa = instr->sdwa();
+   const enum amd_gfx_level gfx = ctx.gfx_level;
 
-   /* first emit the instruction without the SDWA operand */
    Operand sdwa_op = instr->operands[0];
    instr->operands[0] = Operand(PhysReg{249}, v1);
    instr->format = (Format)((uint16_t)instr->format & ~(uint16_t)Format::SDWA);
@@ -1179,29 +1219,29 @@ emit_sdwa_instruction(asm_context& ctx,
 
    if (instr->isVOPC()) {
       if (instr->definitions[0].physReg() !=
-          (ctx.gfx_level >= GFX10 && is_cmpx(instr->opcode) ? exec : vcc)) {
+          (gfx >= GFX10 && is_cmpx(instr->opcode) ? exec : vcc)) {
          encoding |= reg(ctx, instr->definitions[0]) << 8;
-         encoding |= 1 << 15;
+         encoding |= 1u << 15;
       }
-      encoding |= (sdwa.clamp ? 1 : 0) << 13;
+      encoding |= (sdwa.clamp ? 1u : 0u) << 13;
    } else {
       encoding |= sdwa.dst_sel.to_sdwa_sel(instr->definitions[0].physReg().byte()) << 8;
-      uint32_t dst_u = sdwa.dst_sel.sign_extend() ? 1 : 0;
-      if (instr->definitions[0].bytes() < 4) /* dst_preserve */
-         dst_u = 2;
+      uint32_t dst_u = sdwa.dst_sel.sign_extend() ? 1u : 0u;
+      if (instr->definitions[0].bytes() < 4)
+         dst_u = 2u;
       encoding |= dst_u << 11;
-      encoding |= (sdwa.clamp ? 1 : 0) << 13;
+      encoding |= (sdwa.clamp ? 1u : 0u) << 13;
       encoding |= sdwa.omod << 14;
    }
 
    encoding |= sdwa.sel[0].to_sdwa_sel(sdwa_op.physReg().byte()) << 16;
-   encoding |= sdwa.sel[0].sign_extend() ? 1 << 19 : 0;
+   encoding |= sdwa.sel[0].sign_extend() ? 1u << 19 : 0u;
    encoding |= sdwa.abs[0] << 21;
    encoding |= sdwa.neg[0] << 20;
 
    if (instr->operands.size() >= 2) {
       encoding |= sdwa.sel[1].to_sdwa_sel(instr->operands[1].physReg().byte()) << 24;
-      encoding |= sdwa.sel[1].sign_extend() ? 1 << 27 : 0;
+      encoding |= sdwa.sel[1].sign_extend() ? 1u << 27 : 0u;
       encoding |= sdwa.abs[1] << 29;
       encoding |= sdwa.neg[1] << 28;
    }
@@ -1216,7 +1256,6 @@ emit_sdwa_instruction(asm_context& ctx,
 void
 emit_instruction(asm_context& ctx, std::vector<uint32_t>& out, Instruction* instr)
 {
-   /* lower remaining pseudo-instructions */
    if (instr->opcode == aco_opcode::p_constaddr_getpc) {
       ctx.constaddrs[instr->operands[0].constantValue()].getpc_end = out.size() + 1;
 
@@ -1228,7 +1267,6 @@ emit_instruction(asm_context& ctx, std::
       instr->opcode = aco_opcode::s_add_u32;
       instr->operands.pop_back();
       assert(instr->operands[1].isConstant());
-      /* in case it's an inline constant, make it a literal */
       instr->operands[1] = Operand::literal32(instr->operands[1].constantValue());
    } else if (instr->opcode == aco_opcode::p_resumeaddr_getpc) {
       ctx.resumeaddrs[instr->operands[0].constantValue()].getpc_end = out.size() + 1;
@@ -1241,28 +1279,25 @@ emit_instruction(asm_context& ctx, std::
       instr->opcode = aco_opcode::s_add_u32;
       instr->operands.pop_back();
       assert(instr->operands[1].isConstant());
-      /* in case it's an inline constant, make it a literal */
       instr->operands[1] = Operand::literal32(instr->operands[1].constantValue());
    } else if (instr->opcode == aco_opcode::p_load_symbol) {
       assert(instr->operands[0].isConstant());
       assert(ctx.symbols);
 
-      struct aco_symbol info;
-      info.id = (enum aco_symbol_id)instr->operands[0].constantValue();
-      info.offset = out.size() + 1;
-      ctx.symbols->push_back(info);
+      ctx.symbols->emplace_back(aco_symbol{
+         (enum aco_symbol_id)instr->operands[0].constantValue(),
+         static_cast<unsigned>(out.size() + 1)
+      });
 
       instr->opcode = aco_opcode::s_mov_b32;
-      /* in case it's an inline constant, make it a literal */
       instr->operands[0] = Operand::literal32(0);
    } else if (instr->opcode == aco_opcode::p_debug_info) {
       assert(instr->operands[0].isConstant());
       uint32_t index = instr->operands[0].constantValue();
-      ctx.program->debug_info[index].offset = out.size() * 4;
+      ctx.program->debug_info[index].offset = static_cast<unsigned>(out.size() * 4u);
       return;
    }
 
-   /* Promote VOP12C to VOP3 if necessary. */
    if ((instr->isVOP1() || instr->isVOP2() || instr->isVOPC()) && !instr->isVOP3() &&
        needs_vop3_gfx11(ctx, instr)) {
       instr->format = asVOP3(instr->format);
@@ -1409,7 +1444,6 @@ emit_instruction(asm_context& ctx, std::
       break;
    }
 
-   /* append literal dword */
    for (const Operand& op : instr->operands) {
       if (op.isLiteral()) {
          out.push_back(op.constantValue());
@@ -1467,11 +1501,9 @@ fix_exports(asm_context& ctx, std::vecto
       }
    }
 
-   /* GFX10+ FS may not export anything if no discard is used. */
    bool may_skip_export = program->stage.hw == AC_HW_PIXEL_SHADER && program->gfx_level >= GFX10;
 
    if (!exported && !may_skip_export) {
-      /* Abort in order to avoid a GPU hang. */
       bool is_vertex_or_ngg = (program->stage.hw == AC_HW_VERTEX_SHADER ||
                                program->stage.hw == AC_HW_NEXT_GEN_GEOMETRY_SHADER);
       aco_err(program,
@@ -1487,19 +1519,16 @@ insert_code(asm_context& ctx, std::vecto
 {
    out.insert(out.begin() + insert_before, insert_data, insert_data + insert_count);
 
-   /* Update the offset of each affected block */
    for (Block& block : ctx.program->blocks) {
       if (block.offset >= insert_before)
          block.offset += insert_count;
    }
 
-   /* Update the locations of branches */
    for (branch_info& info : ctx.branches) {
       if (info.pos >= insert_before)
          info.pos += insert_count;
    }
 
-   /* Update the locations of p_constaddr instructions */
    for (auto& constaddr : ctx.constaddrs) {
       constaddr_info& info = constaddr.second;
       if (info.getpc_end >= insert_before)
@@ -1526,9 +1555,6 @@ insert_code(asm_context& ctx, std::vecto
 static void
 fix_branches_gfx10(asm_context& ctx, std::vector<uint32_t>& out)
 {
-   /* Branches with an offset of 0x3f are buggy on GFX10,
-    * we workaround by inserting NOPs if needed.
-    */
    bool gfx10_3f_bug = false;
 
    do {
@@ -1538,7 +1564,6 @@ fix_branches_gfx10(asm_context& ctx, std
       gfx10_3f_bug = buggy_branch_it != ctx.branches.end();
 
       if (gfx10_3f_bug) {
-         /* Insert an s_nop after the branch */
          constexpr uint32_t s_nop_0 = 0xbf800000u;
          insert_code(ctx, out, buggy_branch_it->pos + 1, 1, &s_nop_0);
       }
@@ -1548,24 +1573,16 @@ fix_branches_gfx10(asm_context& ctx, std
 void
 chain_branches(asm_context& ctx, std::vector<uint32_t>& out, branch_info& branch)
 {
-   /* Create an empty block in order to remember the offset of the chained branch instruction.
-    * The new branch instructions are inserted into the program in source code order.
-    */
    Block* new_block = ctx.program->create_and_insert_block();
    Builder bld(ctx.program);
    std::vector<uint32_t> code;
    Instruction* branch_instr;
 
-   /* Re-direct original branch to new block (offset). */
    unsigned target = branch.target;
    branch.target = new_block->index;
 
-   unsigned skip_branch_target = 0; /* Target of potentially inserted short jump. */
+   unsigned skip_branch_target = 0;
 
-   /* Find suitable insertion point:
-    * We define two offset ranges within our new branch instruction should be placed.
-    * Then we try to maximize the distance from either the previous branch or the target.
-    */
    const int half_dist = (INT16_MAX - 31) / 2;
    const unsigned upper_start = MIN2(ctx.program->blocks[target].offset, branch.pos) + half_dist;
    const unsigned upper_end = upper_start + half_dist;
@@ -1580,10 +1597,6 @@ chain_branches(asm_context& ctx, std::ve
       if (next.offset < upper_start || (next.offset > upper_end && next.offset < lower_start))
          continue;
 
-      /* If this block ends in an unconditional branch, we can insert
-       * another branch right after it without additional cost for the
-       * existing code.
-       */
       if (!block.instructions.empty() &&
           block.instructions.back()->opcode == aco_opcode::s_branch) {
          insert_at = next.offset;
@@ -1593,9 +1606,7 @@ chain_branches(asm_context& ctx, std::ve
       }
    }
 
-   /* If we didn't find a suitable insertion point, split the existing code. */
    if (insert_at == 0) {
-      /* Find the last block that is still within reach. */
       unsigned insertion_block_idx = 0;
       unsigned next_block = 0;
       while (ctx.program->blocks[next_block + 1].offset < upper_end) {
@@ -1606,7 +1617,6 @@ chain_branches(asm_context& ctx, std::ve
 
       insert_at = ctx.program->blocks[next_block].offset;
       if (insert_at < upper_start) {
-         /* Ensure some forward progress by splitting the block if necessary. */
          auto it = ctx.program->blocks[next_block].instructions.begin();
          int skip = 0;
          while (skip-- > 0 || insert_at < upper_start) {
@@ -1628,22 +1638,18 @@ chain_branches(asm_context& ctx, std::ve
             code.clear();
          }
 
-         /* If the insertion point is in the middle of the block, insert the branch instructions
-          * into that block instead. */
          bld.reset(&ctx.program->blocks[next_block].instructions, it);
+         skip_branch_target = next_block;
       } else {
-         /* Insert the additional branches at the end of the previous non-empty block. */
          bld.reset(&ctx.program->blocks[insertion_block_idx].instructions);
          skip_branch_target = next_block;
       }
 
-      /* Since we insert a branch into existing code, mitigate LdsBranchVmemWARHazard on GFX10. */
       if (ctx.program->gfx_level == GFX10) {
          emit_sopk_instruction(
             ctx, code, bld.sopk(aco_opcode::s_waitcnt_vscnt, Operand(sgpr_null, s1), 0).instr);
       }
 
-      /* For the existing code, create a short jump over the new branch. */
       branch_instr = bld.sopp(aco_opcode::s_branch, 1).instr;
       emit_sopp_instruction(ctx, code, branch_instr, true);
    }
@@ -1656,8 +1662,6 @@ chain_branches(asm_context& ctx, std::ve
 
    new_block->offset = block_offset;
    if (skip_branch_target) {
-      /* If we insert a short jump over the new branch at the end of a block,
-       * ensure that it gets updated accordingly after additional changes. */
       ctx.branches.push_back({block_offset - 1, skip_branch_target});
    }
    ctx.branches.push_back({block_offset, target});
@@ -1667,6 +1671,9 @@ chain_branches(asm_context& ctx, std::ve
 void
 fix_branches(asm_context& ctx, std::vector<uint32_t>& out)
 {
+   const size_t max_iterations = 100;
+   size_t iteration_count = 0;
+
    bool repeat = false;
    do {
       repeat = false;
@@ -1674,7 +1681,13 @@ fix_branches(asm_context& ctx, std::vect
       if (ctx.gfx_level == GFX10)
          fix_branches_gfx10(ctx, out);
 
-      for (branch_info& branch : ctx.branches) {
+      const size_t original_branch_count = ctx.branches.size();
+
+      ctx.branches.reserve(original_branch_count * 3);
+
+      for (size_t i = 0; i < original_branch_count; ++i) {
+         branch_info& branch = ctx.branches[i];
+
          int offset = (int)ctx.program->blocks[branch.target].offset - branch.pos - 1;
          if (offset >= INT16_MIN && offset <= INT16_MAX) {
             out[branch.pos] &= 0xffff0000u;
@@ -1685,6 +1698,12 @@ fix_branches(asm_context& ctx, std::vect
             break;
          }
       }
+
+      ++iteration_count;
+      if (iteration_count > max_iterations) {
+         aco_err(ctx.program, "Branch fixup failed to converge after %zu iterations (infinite loop detected)", max_iterations);
+         abort();
+      }
    } while (repeat);
 }
 
@@ -1696,10 +1715,7 @@ fix_constaddrs(asm_context& ctx, std::ve
       out[info.add_literal] += (out.size() - info.getpc_end) * 4u;
 
       if (ctx.symbols) {
-         struct aco_symbol sym;
-         sym.id = aco_symbol_const_data_addr;
-         sym.offset = info.add_literal;
-         ctx.symbols->push_back(sym);
+         ctx.symbols->emplace_back(aco_symbol{aco_symbol_const_data_addr, info.add_literal});
       }
    }
    for (auto& addr : ctx.resumeaddrs) {
@@ -1713,7 +1729,6 @@ fix_constaddrs(asm_context& ctx, std::ve
 void
 align_block(asm_context& ctx, std::vector<uint32_t>& code, Block& block)
 {
-   /* Align the previous loop. */
    if (ctx.loop_header != -1u &&
        block.loop_nest_depth < ctx.program->blocks[ctx.loop_header].loop_nest_depth) {
       assert(ctx.loop_exit != -1u);
@@ -1725,9 +1740,6 @@ align_block(asm_context& ctx, std::vecto
 
       const unsigned loop_num_cl = DIV_ROUND_UP(block.offset - loop_header.offset, 16);
 
-      /* On GFX10.3+, change the prefetch mode if the loop fits into 2 or 3 cache lines.
-       * Don't use the s_inst_prefetch instruction on GFX10 as it might cause hangs.
-       */
       const bool change_prefetch = ctx.program->gfx_level >= GFX10_3 &&
                                    ctx.program->gfx_level <= GFX11 && loop_num_cl > 1 &&
                                    loop_num_cl <= 3;
@@ -1739,7 +1751,6 @@ align_block(asm_context& ctx, std::vecto
          emit_instruction(ctx, nops, instr);
          insert_code(ctx, code, loop_header.offset, nops.size(), nops.data());
 
-         /* Change prefetch mode back to default (0x3) at the loop exit. */
          bld.reset(&loop_exit.instructions, loop_exit.instructions.begin());
          instr = bld.sopp(aco_opcode::s_inst_prefetch, 0x3);
          if (ctx.loop_exit < block.index) {
@@ -1752,9 +1763,6 @@ align_block(asm_context& ctx, std::vecto
       const unsigned loop_start_cl = loop_header.offset >> 4;
       const unsigned loop_end_cl = (block.offset - 1) >> 4;
 
-      /* Align the loop if it fits into the fetched cache lines or if we can
-       * reduce the number of cache lines with less than 8 NOPs.
-       */
       const bool align_loop = loop_end_cl - loop_start_cl >= loop_num_cl &&
                               (loop_num_cl == 1 || change_prefetch || loop_header.offset % 16 > 8);
 
@@ -1766,20 +1774,12 @@ align_block(asm_context& ctx, std::vecto
    }
 
    if (block.kind & block_kind_loop_header) {
-      /* In case of nested loops, only handle the inner-most loops in order
-       * to not break the alignment of inner loops by handling outer loops.
-       * Also ignore loops without back-edge.
-       */
       if (block.linear_preds.size() > 1) {
          ctx.loop_header = block.index;
          ctx.loop_exit = -1u;
       }
    }
 
-   /* Blocks with block_kind_loop_exit might be eliminated after jump threading,
-    * so we instead find loop exits using the successors when in loop_nest_depth.
-    * This works, because control flow always re-converges after loops.
-    */
    if (ctx.loop_header != -1u && ctx.loop_exit == -1u) {
       for (uint32_t succ_idx : block.linear_succs) {
          Block& succ = ctx.program->blocks[succ_idx];
@@ -1788,10 +1788,9 @@ align_block(asm_context& ctx, std::vecto
       }
    }
 
-   /* align resume shaders with cache line */
    if (block.kind & block_kind_resume) {
       size_t cache_aligned = align(code.size(), 16);
-      code.resize(cache_aligned, 0xbf800000u); /* s_nop 0 */
+      code.resize(cache_aligned, 0xbf800000u);
       block.offset = code.size();
    }
 }
@@ -1802,12 +1801,49 @@ emit_program(Program* program, std::vect
 {
    asm_context ctx(program, symbols);
 
+   size_t estimated_insn_count = 0;
+   size_t loop_count = 0;
+   size_t branch_count = 0;
+
+   for (const Block& block : program->blocks) {
+      estimated_insn_count += block.instructions.size();
+      if (block.kind & block_kind_loop_header)
+         loop_count++;
+      for (const aco_ptr<Instruction>& instr : block.instructions) {
+         if (instr_info.classes[(int)instr->opcode] == instr_class::branch)
+            branch_count++;
+      }
+   }
+
+   const size_t base = (estimated_insn_count <= UINT32_MAX / 3)
+                        ? (estimated_insn_count * 5) / 2
+                        : (estimated_insn_count / 2) * 5;
+
+   const size_t literal_overhead    = base / 5;
+   const size_t modifier_overhead   = base / 20;
+   const size_t nsa_overhead        = (base * 3) / 100;
+   const size_t alignment_padding   = loop_count * 15u;
+
+   const size_t cascade_depth = 3;
+   const size_t chains_per_branch = (1u << cascade_depth);
+   const size_t dwords_per_chain = 10u;
+   const size_t branch_chaining = (branch_count / 10) * chains_per_branch * dwords_per_chain;
+
+   const size_t constant_dwords     = (program->constant_data.size() + 3u) / 4u;
+   const size_t endpgm_padding      = append_endpgm ? 5u : 0u;
+
+   const size_t total_estimate = base + literal_overhead + modifier_overhead + nsa_overhead +
+                                  alignment_padding + branch_chaining + constant_dwords + endpgm_padding;
+
+   const size_t reserve_size = total_estimate + (total_estimate * 20u) / 100u;
+
+   code.reserve(reserve_size);
+
    bool is_separately_compiled_ngg_vs_or_es =
       (program->stage.sw == SWStage::VS || program->stage.sw == SWStage::TES) &&
       program->stage.hw == AC_HW_NEXT_GEN_GEOMETRY_SHADER &&
       program->info.merged_shader_compiled_separately;
 
-   /* Prolog has no exports. */
    if (!program->is_prolog && !program->info.ps.has_epilog &&
        !is_separately_compiled_ngg_vs_or_es &&
        (program->stage.hw == AC_HW_VERTEX_SHADER || program->stage.hw == AC_HW_PIXEL_SHADER ||
@@ -1824,7 +1860,6 @@ emit_program(Program* program, std::vect
 
    unsigned exec_size = code.size() * sizeof(uint32_t);
 
-   /* Add end-of-code markers for the UMR disassembler. */
    if (append_endpgm)
       code.resize(code.size() + 5, 0xbf9f0000u);
 
@@ -1832,7 +1867,6 @@ emit_program(Program* program, std::vect
 
    while (program->constant_data.size() % 4u)
       program->constant_data.push_back(0);
-   /* Copy constant data */
    code.insert(code.end(), (uint32_t*)program->constant_data.data(),
                (uint32_t*)(program->constant_data.data() + program->constant_data.size()));
 
@@ -1843,4 +1877,4 @@ emit_program(Program* program, std::vect
    return exec_size;
 }
 
-} // namespace aco
+}
