--- a/src/compiler/nir/nir_opt_algebraic.py	2025-07-02 10:00:26.949844780 +0200
+++ b/src/compiler/nir/nir_opt_algebraic.py	2025-07-02 10:54:46.910581907 +0200
@@ -2704,6 +2704,42 @@ optimizations.extend([
 
    (('imul_high@16', a, b), ('i2i16', ('ishr', ('imul24_relaxed', ('i2i32', a), ('i2i32', b)), 16)), 'options->lower_mul_high16'),
    (('umul_high@16', a, b), ('u2u16', ('ushr', ('umul24_relaxed', ('u2u32', a), ('u2u32', b)), 16)), 'options->lower_mul_high16'),
+
+    # 1. clamp(x, min, max)  →  bcsel  (NaN-safe, ±0-safe)
+    #
+    #    Variant A :  min(max(a, min), max)
+    (('fmin', ('fmax(is_used_once)', 'a', '#minval'), '#maxval'),
+     ('bcsel', ('flt', 'a', 'minval'),                 # a < min ?
+               'minval',
+               ('bcsel', ('flt', 'maxval', 'a'),       # a > max ?
+                        'maxval',
+                        'a'))),
+
+    #    Variant B :  max(min(a, max), min)
+    (('fmax', ('fmin(is_used_once)', 'a', '#maxval'), '#minval'),
+     ('bcsel', ('flt', 'maxval', 'a'),                 # a > max ?
+               'maxval',
+               ('bcsel', ('flt', 'a', 'minval'),       # a < min ?
+                        'minval',
+                        'a'))),
+
+    # 2.  Open-coded signed bit-field extract  →  ibitfield_extract
+    (('ishr@32',
+      ('ishl@32', 'a',
+        ('isub', 32, ('iadd', '#offset(is_ult_32)', '#width(is_ult_32)'))),
+      ('isub', 32, '#width')),
+     ('ibitfield_extract', 'a', 'offset', 'width'),
+     '!options->lower_bitfield_extract'),
+
+    # 3.  Bit-wise mux (a&M) | (b&~M)  →  bitfield_select
+    #
+    #    Note the constant in the search pattern (#mask) and the
+    #    non-constant alias (mask) in the replacement – required by
+    #    nir_algebraic’s validator.
+    (('ior', ('iand', 'a', '#mask'),
+              ('iand', 'b', ('inot', '#mask'))),
+     ('bitfield_select', 'mask', 'a', 'b'),
+     'options->has_bitfield_select'),
 ])
 
 for bit_size in [8, 16, 32, 64]:
@@ -3299,11 +3335,13 @@ optimizations.extend([
 """
 optimizations.extend([
     (('fquantize2f16', 'a@32'),
-     ('bcsel', ('!flt', ('!fabs', a), math.ldexp(1.0, -14)),
-               ('iand', a, 1 << 31),
-               ('!f2f32', ('!f2f16_rtne', a))),
+     ('bcsel', ('fneu', 'a', 'a'),                      # NaN → keep NaN
+               'a',
+               ('bcsel', ('flt', ('fabs', 'a'), 0.00006103515625),  # |a| < 2^-14 ?
+                         ('fmul', 'a', 0.0),           # signed zero
+                         ('f2f32', ('f2f16_rtne', 'a')))),
      'options->lower_fquantize2f16')
-    ])
+])
 
 for s in range(0, 31):
     mask = 0xffffffff << s
@@ -3537,6 +3575,18 @@ late_optimizations = [
    # Drivers do not actually implement udiv_aligned_4, it is just used to
    # optimize scratch lowering.
    (('udiv_aligned_4', a), ('ushr', a, 2)),
+
+   # Reconstruct 2-way dot product. This maps to V_DOT2_F32_F16 on GFX9 for
+   # packed 16-bit floats, providing a significant performance boost.
+    (('fadd@32',
+       ('fmul@32(is_only_used_by_fadd)',
+         ('f2f32', 'ax@16'), ('f2f32', 'bx@16')),
+       ('fmul@32(is_only_used_by_fadd)',
+         ('f2f32', 'ay@16'), ('f2f32', 'by@16'))),
+     ('fdot2@32',
+        ('vec2@32', ('f2f32', 'ax'), ('f2f32', 'ay')),
+        ('vec2@32', ('f2f32', 'bx'), ('f2f32', 'by'))),
+     '!options->lower_fdph')
 ]
 
 # re-combine inexact mul+add to ffma. Do this before fsub so that a * b - c
@@ -3613,6 +3663,19 @@ late_optimizations.extend([
    (('vec2(is_only_used_as_float)', ('fneg@16', a), b), ('fmul', ('vec2', a, b), ('vec2', -1.0, 1.0)), 'options->vectorize_vec2_16bit'),
    (('vec2(is_only_used_as_float)', a, ('fneg@16', b)), ('fmul', ('vec2', a, b), ('vec2', 1.0, -1.0)), 'options->vectorize_vec2_16bit'),
 
+   # Re-vectorize component-wise bcsel for packed-math targets like GFX9.
+   # This must run late to catch scalarized components from early lowering. It
+   # fuses two scalar 16-bit selects with a common condition back into a
+   # vector select, which can be emitted as a single V_CNDMASK_B32 instruction.
+   # All bit-sizes must be explicit for the type-matcher.
+    (('vec2@16',
+       ('bcsel@16(is_used_once)', 'c@1', 'ax@16', 'bx@16'),
+       ('bcsel@16(is_used_once)', 'c@1', 'ay@16', 'by@16')),
+     ('bcsel@16', 'c',
+                ('vec2@16', 'ax', 'ay'),
+                ('vec2@16', 'bx', 'by')),
+     '!options->vectorize_vec2_16bit'),
+
    # These are duplicated from the main optimizations table.  The late
    # patterns that rearrange expressions like x - .5 < 0 to x < .5 can create
    # new patterns like these.  The patterns that compare with zero are removed
