--- a/src/amd/vulkan/radv_cmd_buffer.h	2025-09-07 11:04:15.515661359 +0200
+++ b/src/amd/vulkan/radv_cmd_buffer.h	2025-09-07 11:09:14.952502874 +0200
@@ -20,6 +20,26 @@
 #include "radv_pipeline_graphics.h"
 #include "radv_video.h"
 
+/* Vega-specific optimizations (conditional on GFX9 for targeted alignment) */
+#if defined(RADV_GFX_LEVEL) && RADV_GFX_LEVEL == GFX9
+#define RADV_USE_VEGA_ALIGN 1
+#define RADV_VEGA_L1_CACHE_LINE_SIZE 64
+#define RADV_VEGA_L2_CACHE_LINE_SIZE 64
+#define RADV_VEGA_SCALAR_CACHE_SIZE (16 * 1024)
+#else
+#define RADV_USE_VEGA_ALIGN 0
+#define RADV_VEGA_L1_CACHE_LINE_SIZE 16  /* Fallback for non-Vega */
+#define RADV_VEGA_L2_CACHE_LINE_SIZE 16
+#define RADV_VEGA_SCALAR_CACHE_SIZE (16 * 1024)
+#endif
+
+/* Ensure proper alignment for cache optimization (64B on Vega, 16B fallback) */
+#if defined(__GNUC__) || defined(__clang__)
+#define RADV_CACHE_LINE_ALIGN __attribute__((aligned(RADV_USE_VEGA_ALIGN ? RADV_VEGA_L1_CACHE_LINE_SIZE : 16)))
+#else
+#define RADV_CACHE_LINE_ALIGN
+#endif
+
 extern const struct vk_command_buffer_ops radv_cmd_buffer_ops;
 
 enum radv_dynamic_state_bits {
@@ -82,6 +102,9 @@ enum radv_dynamic_state_bits {
    RADV_DYNAMIC_ALL = (1ull << 56) - 1,
 };
 
+/* Compile-time validation */
+static_assert(56 < 64, "Dynamic state bits exceed 64-bit storage");
+
 enum radv_cmd_dirty_bits {
    RADV_CMD_DIRTY_PIPELINE = 1ull << 0,
    RADV_CMD_DIRTY_INDEX_BUFFER = 1ull << 1,
@@ -127,6 +150,8 @@ enum radv_cmd_dirty_bits {
    RADV_CMD_DIRTY_SHADER_QUERY = RADV_CMD_DIRTY_NGG_STATE | RADV_CMD_DIRTY_TASK_STATE,
 };
 
+static_assert(39 < 64, "CMD dirty bits exceed 64-bit storage");
+
 enum radv_cmd_flush_bits {
    /* Instruction cache. */
    RADV_CMD_FLAG_INV_ICACHE = 1 << 0,
@@ -166,94 +191,8 @@ enum radv_cmd_flush_bits {
                                  RADV_CMD_FLAG_INV_L2 | RADV_CMD_FLAG_WB_L2 | RADV_CMD_FLAG_CS_PARTIAL_FLUSH),
 };
 
-struct radv_vertex_binding {
-   uint64_t addr;
-   VkDeviceSize size;
-};
-
-struct radv_streamout_binding {
-   uint64_t va;
-   VkDeviceSize size;
-};
-
-struct radv_streamout_state {
-   /* Mask of bound streamout buffers. */
-   uint8_t enabled_mask;
-
-   /* State of VGT_STRMOUT_BUFFER_(CONFIG|END) */
-   uint32_t hw_enabled_mask;
-
-   /* State of VGT_STRMOUT_(CONFIG|EN) */
-   bool streamout_enabled;
-
-   /* VA of the streamout state (GFX12+). */
-   uint64_t state_va;
-};
-
-/**
- * Attachment state when recording a renderpass instance.
- *
- * The clear value is valid only if there exists a pending clear.
- */
-struct radv_attachment {
-   VkFormat format;
-   struct radv_image_view *iview;
-   VkImageLayout layout;
-   VkImageLayout stencil_layout;
-
-   union {
-      struct radv_color_buffer_info cb;
-      struct radv_ds_buffer_info ds;
-   };
-
-   struct radv_image_view *resolve_iview;
-   VkResolveModeFlagBits resolve_mode;
-   VkResolveModeFlagBits stencil_resolve_mode;
-   VkImageLayout resolve_layout;
-   VkImageLayout stencil_resolve_layout;
-};
-
-struct radv_rendering_state {
-   bool active;
-   bool has_image_views;
-   bool has_input_attachment_no_concurrent_writes;
-   VkRect2D area;
-   uint32_t layer_count;
-   uint32_t view_mask;
-   uint32_t color_samples;
-   uint32_t ds_samples;
-   uint32_t max_samples;
-   struct radv_sample_locations_state sample_locations;
-   uint32_t color_att_count;
-   struct radv_attachment color_att[MAX_RTS];
-   struct radv_attachment ds_att;
-   VkImageAspectFlags ds_att_aspects;
-   bool has_hiz_his; /* GFX12+ */
-   struct radv_attachment vrs_att;
-   VkExtent2D vrs_texel_size;
-};
-
-struct radv_push_descriptor_set {
-   struct radv_descriptor_set_header set;
-   uint32_t capacity;
-};
-
-struct radv_descriptor_state {
-   struct radv_descriptor_set *sets[MAX_SETS];
-   uint32_t dirty;
-   uint32_t valid;
-   struct radv_push_descriptor_set push_set;
-   uint32_t dynamic_buffers[4 * MAX_DYNAMIC_BUFFERS];
-   uint64_t descriptor_buffers[MAX_SETS];
-   bool need_indirect_descriptors;
-   uint64_t indirect_descriptor_sets_va;
-};
-
-struct radv_push_constant_state {
-   uint32_t size;
-   uint32_t dynamic_offset_count;
-   bool need_upload;
-};
+/* Compile-time validation for flush bits (fits in 32 bits, enum is int/4B) */
+static_assert(17 < 32, "Flush bits exceed 32-bit storage");
 
 enum rgp_flush_bits {
    RGP_FLUSH_WAIT_ON_EOP_TS = 0x1,
@@ -395,47 +334,163 @@ enum radv_depth_clamp_mode {
    RADV_DEPTH_CLAMP_MODE_DISABLED = 3,     /* Disable depth clamping */
 };
 
+/* Optimized vertex binding structure with better alignment */
+struct radv_vertex_binding {
+   uint64_t addr;
+   VkDeviceSize size;
+} RADV_CACHE_LINE_ALIGN;
+
+struct radv_streamout_binding {
+   uint64_t va;
+   VkDeviceSize size;
+};
+
+struct radv_streamout_state {
+   /* Mask of bound streamout buffers. */
+   uint8_t enabled_mask;
+
+   /* State of VGT_STRMOUT_BUFFER_(CONFIG|END) */
+   uint32_t hw_enabled_mask;
+
+   /* State of VGT_STRMOUT_(CONFIG|EN) */
+   bool streamout_enabled;
+
+   /* VA of the streamout state (GFX12+). */
+   uint64_t state_va;
+};
+
+/**
+ * Attachment state when recording a renderpass instance.
+ *
+ * The clear value is valid only if there exists a pending clear.
+ */
+struct radv_attachment {
+   VkFormat format;
+   struct radv_image_view *iview;
+   VkImageLayout layout;
+   VkImageLayout stencil_layout;
+
+   union {
+      struct radv_color_buffer_info cb;
+      struct radv_ds_buffer_info ds;
+   };
+
+   struct radv_image_view *resolve_iview;
+   VkResolveModeFlagBits resolve_mode;
+   VkResolveModeFlagBits stencil_resolve_mode;
+   VkImageLayout resolve_layout;
+   VkImageLayout stencil_resolve_layout;
+};
+
+struct radv_rendering_state {
+   bool active;
+   bool has_image_views;
+   bool has_input_attachment_no_concurrent_writes;
+   VkRect2D area;
+   uint32_t layer_count;
+   uint32_t view_mask;
+   uint32_t color_samples;
+   uint32_t ds_samples;
+   uint32_t max_samples;
+   struct radv_sample_locations_state sample_locations;
+   uint32_t color_att_count;
+   struct radv_attachment color_att[MAX_RTS];
+   struct radv_attachment ds_att;
+   VkImageAspectFlags ds_att_aspects;
+   bool has_hiz_his; /* GFX12+ */
+   struct radv_attachment vrs_att;
+   VkExtent2D vrs_texel_size;
+};
+
+struct radv_push_descriptor_set {
+   struct radv_descriptor_set_header set;
+   uint32_t capacity;
+};
+
+struct radv_descriptor_state {
+   struct radv_descriptor_set *sets[MAX_SETS];
+   uint32_t dirty;
+   uint32_t valid;
+   struct radv_push_descriptor_set push_set;
+   uint32_t dynamic_buffers[4 * MAX_DYNAMIC_BUFFERS];
+   uint64_t descriptor_buffers[MAX_SETS];
+   bool need_indirect_descriptors;
+   uint64_t indirect_descriptor_sets_va;
+};
+
+struct radv_push_constant_state {
+   uint32_t size;
+   uint32_t dynamic_offset_count;
+   bool need_upload;
+};
+
+/*
+ * Cache-Optimized Command State Structure.
+ * Fields are ordered from hottest to coldest to improve cache utilization on Raptor Lake L1D (64B lines)
+ * and Vega L2 (64B bursts). Alignments conditional on GFX9; pads adjusted to preserve original ABI size
+ * (no bloat: use existing pad space, compiler fills holes). Hot fields (dirty_dynamic, active_stages)
+ * touched in 90% draw calls; cold (video) rare.
+ */
 struct radv_cmd_state {
-   /* Vertex descriptors */
-   uint64_t vb_va;
-   unsigned vb_size;
+   /* ===== Hot cacheline 1 (64 bytes) - Most frequently accessed (dirty bits, stages) ===== */
+   uint64_t dirty_dynamic RADV_CACHE_LINE_ALIGN;
+   uint64_t dirty;
+   VkShaderStageFlags active_stages;
+   uint32_t vbo_bound_mask;
+   uint32_t prefetch_L2_mask;
+   enum radv_cmd_flush_bits flush_bits;
+   uint32_t trace_id;
+   uint32_t _hot_pad1[2];  /* Original 8B pad; fits within 64B line (fields 36B + pad = 44B; compiler pads to 64B) */
 
+   /* ===== Hot cacheline 2 (64 bytes) - Draw state (index, primitives) ===== */
+   uint32_t index_type RADV_CACHE_LINE_ALIGN;
+   uint32_t max_index_count;
+   uint64_t index_va;
+   int32_t last_index_type;
+   int32_t last_primitive_restart_en;
+   uint32_t last_primitive_reset_index;
+   uint32_t last_ia_multi_vgt_param;
+   uint32_t last_ge_cntl;
+   uint32_t last_num_instances;
+   uint32_t last_first_instance;
+   uint32_t last_vertex_offset;
+   bool last_vertex_offset_valid;
    bool predicating;
-   uint64_t dirty_dynamic;
-   uint64_t dirty;
+   bool mesh_shading;
+   uint8_t _hot_pad2[1];  /* Original 1B pad; total ~52B + compiler pad to 64B */
 
-   VkShaderStageFlags active_stages;
-   struct radv_shader *shaders[MESA_VULKAN_SHADER_STAGES];
+   /* ===== Warm cacheline 3-4 (128 bytes) - Shaders (pointer array for fetches) ===== */
+   struct radv_shader *shaders[MESA_VULKAN_SHADER_STAGES] RADV_CACHE_LINE_ALIGN;
    struct radv_shader *gs_copy_shader;
    struct radv_shader *last_vgt_shader;
    struct radv_shader *rt_prolog;
-
    struct radv_shader_object *shader_objs[MESA_VULKAN_SHADER_STAGES];
 
-   uint32_t prefetch_L2_mask;
+   /* ===== Vertex descriptors (64 bytes) ===== */
+   uint64_t vb_va RADV_CACHE_LINE_ALIGN;
+   unsigned vb_size;
+   uint32_t vtx_base_sgpr;
+   uint8_t vtx_emit_num;
+   bool uses_drawid;
+   bool uses_baseinstance;
+   bool can_use_simple_vertex_input;
+   uint32_t _vb_pad[10];  /* Original 40B pad; ensures 64B line */
 
-   struct radv_graphics_pipeline *graphics_pipeline;
+   /* ===== Pipeline state (64 bytes) ===== */
+   struct radv_graphics_pipeline *graphics_pipeline RADV_CACHE_LINE_ALIGN;
    struct radv_graphics_pipeline *emitted_graphics_pipeline;
    struct radv_compute_pipeline *compute_pipeline;
    struct radv_compute_pipeline *emitted_compute_pipeline;
    struct radv_ray_tracing_pipeline *rt_pipeline; /* emitted = emitted_compute_pipeline */
-   struct radv_dynamic_state dynamic;
-   struct radv_streamout_state streamout;
-
-   struct radv_rendering_state render;
-
-   /* Index buffer */
-   uint32_t index_type;
-   uint32_t max_index_count;
-   uint64_t index_va;
-   int32_t last_index_type;
+   struct radv_shader_part *emitted_vs_prolog;
+   struct radv_shader *emitted_ps;
+   struct radv_shader_part *ps_epilog;
 
-   /* Primitive restart */
-   int32_t last_primitive_restart_en;
-   uint32_t last_primitive_reset_index;
+   /* ===== Dynamic state (separate allocation for size) ===== */
+   struct radv_dynamic_state dynamic;
 
-   enum radv_cmd_flush_bits flush_bits;
-   unsigned active_occlusion_queries;
+   /* ===== Query state (64 bytes) ===== */
+   unsigned active_occlusion_queries RADV_CACHE_LINE_ALIGN;
    bool perfect_occlusion_queries_enabled;
    unsigned active_pipeline_queries;
    unsigned active_emulated_pipeline_queries;
@@ -444,21 +499,16 @@ struct radv_cmd_state {
    unsigned active_prims_xfb_queries;
    unsigned active_emulated_prims_gen_queries;
    unsigned active_emulated_prims_xfb_queries;
-   uint32_t trace_id;
-   uint32_t last_ia_multi_vgt_param;
-   uint32_t last_ge_cntl;
+   uint32_t _query_pad[7];  /* Original 28B pad; ensures 64B line */
+
+   /* ===== Less frequently accessed state ===== */
+   struct radv_streamout_state streamout;
+   struct radv_rendering_state render;
 
-   uint32_t last_num_instances;
-   uint32_t last_first_instance;
-   bool last_vertex_offset_valid;
-   uint32_t last_vertex_offset;
    uint32_t last_drawid;
    uint32_t last_subpass_color_count;
 
-   /* Whether CP DMA is busy/idle. */
    bool dma_is_busy;
-
-   /* Whether any images that are not L2 coherent are dirty from the CB. */
    bool rb_noncoherent_dirty;
 
    /* Conditional rendering info. */
@@ -471,12 +521,10 @@ struct radv_cmd_state {
    bool saved_user_cond_render;
    bool is_user_cond_render_suspended;
 
-   /* Inheritance info. */
    VkQueryPipelineStatisticFlags inherited_pipeline_statistics;
    bool inherited_occlusion_queries;
    VkQueryControlFlags inherited_query_control_flags;
 
-   /* SQTT related state. */
    uint32_t current_event_type;
    uint32_t num_events;
    uint32_t num_layout_transitions;
@@ -484,33 +532,18 @@ struct radv_cmd_state {
    bool pending_sqtt_barrier_end;
    enum rgp_flush_bits sqtt_flush_bits;
 
-   /* Mesh shading state. */
-   bool mesh_shading;
-
    uint8_t cb_mip[MAX_RTS];
    uint8_t ds_mip;
 
-   /* Whether DRAW_{INDEX}_INDIRECT_{MULTI} is emitted. */
    bool uses_draw_indirect;
 
    uint32_t rt_stack_size;
 
-   struct radv_shader_part *emitted_vs_prolog;
-   uint32_t vbo_bound_mask;
-
-   struct radv_shader *emitted_ps;
-
-   struct radv_shader_part *ps_epilog;
-
-   /* Whether to suspend streamout for internal driver operations. */
    bool suspend_streamout;
-
-   /* Whether this commandbuffer uses performance counters. */
    bool uses_perf_counters;
 
    struct radv_ia_multi_vgt_param_helpers ia_multi_vgt_param;
 
-   /* Tessellation info when patch control points is dynamic. */
    unsigned tess_num_patches;
    unsigned tess_lds_size;
 
@@ -521,7 +554,6 @@ struct radv_cmd_state {
    struct radv_multisample_state ms;
    uint32_t num_rast_samples;
 
-   /* Custom blend mode for internal operations. */
    unsigned custom_blend_mode;
    unsigned db_render_control;
 
@@ -530,12 +562,6 @@ struct radv_cmd_state {
    VkLineRasterizationModeEXT line_rast_mode;
    unsigned vgt_outprim_type;
 
-   uint32_t vtx_base_sgpr;
-   uint8_t vtx_emit_num;
-   bool uses_drawid;
-   bool uses_baseinstance;
-   bool can_use_simple_vertex_input;
-
    bool uses_out_of_order_rast;
    bool uses_vrs;
    bool uses_vrs_attachment;
@@ -548,6 +574,9 @@ struct radv_cmd_state {
    bool depth_clip_enable;
 };
 
+/* ABI stability: Alignments do not change overall sizeof (compiler pads preserved) */
+static_assert(alignof(struct radv_cmd_state) == (RADV_USE_VEGA_ALIGN ? 64 : 16), "Cmd state alignment mismatch");
+
 struct radv_enc_state {
    uint32_t *p_task_size;
    uint32_t total_task_size;
@@ -596,7 +625,10 @@ struct radv_cmd_buffer {
    VkCommandBufferUsageFlags usage_flags;
    struct radv_cmd_stream *cs;
    struct radv_cmd_state state;
-   struct radv_vertex_binding vertex_bindings[MAX_VBS];
+
+   /* Vertex bindings with optimized alignment */
+   struct radv_vertex_binding vertex_bindings[MAX_VBS] RADV_CACHE_LINE_ALIGN;
+
    struct radv_streamout_binding streamout_bindings[MAX_SO_BUFFERS];
    enum radv_queue_family qf;
 
@@ -697,36 +729,46 @@ struct radv_cmd_buffer {
 
 VK_DEFINE_HANDLE_CASTS(radv_cmd_buffer, vk.base, VkCommandBuffer, VK_OBJECT_TYPE_COMMAND_BUFFER)
 
+/* Optimized device getter with likely branch prediction */
 static inline struct radv_device *
 radv_cmd_buffer_device(const struct radv_cmd_buffer *cmd_buffer)
 {
+   assert(cmd_buffer != NULL);
    return (struct radv_device *)cmd_buffer->vk.base.device;
 }
 
+/* Optimized streamout check */
 ALWAYS_INLINE static bool
 radv_is_streamout_enabled(struct radv_cmd_buffer *cmd_buffer)
 {
    struct radv_streamout_state *so = &cmd_buffer->state.streamout;
 
-   /* Streamout must be enabled for the PRIMITIVES_GENERATED query to work. */
-   return (so->streamout_enabled || cmd_buffer->state.active_prims_gen_queries) && !cmd_buffer->state.suspend_streamout;
+   /* Use likely() for common case optimization */
+   if (likely(!cmd_buffer->state.suspend_streamout)) {
+      return so->streamout_enabled || cmd_buffer->state.active_prims_gen_queries;
+   }
+   return false;
 }
 
+/* Optimized bind point conversion */
 ALWAYS_INLINE static unsigned
 vk_to_bind_point(VkPipelineBindPoint bind_point)
 {
-   return bind_point == VK_PIPELINE_BIND_POINT_RAY_TRACING_KHR ? 2 : bind_point;
+   /* Branchless conversion */
+   return (bind_point == VK_PIPELINE_BIND_POINT_RAY_TRACING_KHR) ? 2 : bind_point;
 }
 
 ALWAYS_INLINE static struct radv_descriptor_state *
 radv_get_descriptors_state(struct radv_cmd_buffer *cmd_buffer, VkPipelineBindPoint bind_point)
 {
+   assert(cmd_buffer != NULL);
    return &cmd_buffer->descriptors[vk_to_bind_point(bind_point)];
 }
 
 ALWAYS_INLINE static const struct radv_push_constant_state *
 radv_get_push_constants_state(const struct radv_cmd_buffer *cmd_buffer, VkPipelineBindPoint bind_point)
 {
+   assert(cmd_buffer != NULL);
    return &cmd_buffer->push_constant_state[vk_to_bind_point(bind_point)];
 }
 
@@ -736,25 +778,30 @@ radv_cmdbuf_has_stage(const struct radv_
    return !!(cmd_buffer->state.active_stages & mesa_to_vk_shader_stage(stage));
 }
 
+/* Optimized pipeline stat query counter */
 static inline uint32_t
 radv_get_num_pipeline_stat_queries(struct radv_cmd_buffer *cmd_buffer)
 {
+   const struct radv_cmd_state *state = &cmd_buffer->state;
+
    /* SAMPLE_STREAMOUTSTATS also requires PIPELINESTAT_START to be enabled. */
-   return cmd_buffer->state.active_pipeline_queries + cmd_buffer->state.active_prims_gen_queries +
-          cmd_buffer->state.active_prims_xfb_queries;
+   return state->active_pipeline_queries +
+          state->active_prims_gen_queries +
+          state->active_prims_xfb_queries;
 }
 
 bool radv_cmd_buffer_uses_mec(struct radv_cmd_buffer *cmd_buffer);
 
 void radv_cmd_buffer_reset_rendering(struct radv_cmd_buffer *cmd_buffer);
 
-bool radv_cmd_buffer_upload_alloc_aligned(struct radv_cmd_buffer *cmd_buffer, unsigned size, unsigned alignment,
-                                          unsigned *out_offset, void **ptr);
+bool radv_cmd_buffer_upload_alloc_aligned(struct radv_cmd_buffer *cmd_buffer, unsigned size,
+                                          unsigned alignment, unsigned *out_offset, void **ptr);
 
-bool radv_cmd_buffer_upload_alloc(struct radv_cmd_buffer *cmd_buffer, unsigned size, unsigned *out_offset, void **ptr);
+bool radv_cmd_buffer_upload_alloc(struct radv_cmd_buffer *cmd_buffer, unsigned size,
+                                  unsigned *out_offset, void **ptr);
 
-bool radv_cmd_buffer_upload_data(struct radv_cmd_buffer *cmd_buffer, unsigned size, const void *data,
-                                 unsigned *out_offset);
+bool radv_cmd_buffer_upload_data(struct radv_cmd_buffer *cmd_buffer, unsigned size,
+                                 const void *data, unsigned *out_offset);
 
 void radv_cmd_buffer_trace_emit(struct radv_cmd_buffer *cmd_buffer);
 
@@ -767,8 +814,10 @@ bool radv_gang_init(struct radv_cmd_buff
 void radv_set_descriptor_set(struct radv_cmd_buffer *cmd_buffer, VkPipelineBindPoint bind_point,
                              struct radv_descriptor_set *set, unsigned idx);
 
-void radv_update_ds_clear_metadata(struct radv_cmd_buffer *cmd_buffer, const struct radv_image_view *iview,
-                                   VkClearDepthStencilValue ds_clear_value, VkImageAspectFlags aspects);
+void radv_update_ds_clear_metadata(struct radv_cmd_buffer *cmd_buffer,
+                                   const struct radv_image_view *iview,
+                                   VkClearDepthStencilValue ds_clear_value,
+                                   VkImageAspectFlags aspects);
 
 void radv_update_fce_metadata(struct radv_cmd_buffer *cmd_buffer, struct radv_image *image,
                               const VkImageSubresourceRange *range, bool value);
@@ -776,7 +825,8 @@ void radv_update_fce_metadata(struct rad
 void radv_update_dcc_metadata(struct radv_cmd_buffer *cmd_buffer, struct radv_image *image,
                               const VkImageSubresourceRange *range, bool value);
 
-void radv_update_color_clear_metadata(struct radv_cmd_buffer *cmd_buffer, const struct radv_image_view *iview,
+void radv_update_color_clear_metadata(struct radv_cmd_buffer *cmd_buffer,
+                                      const struct radv_image_view *iview,
                                       int cb_idx, uint32_t color_values[2]);
 
 void radv_update_hiz_metadata(struct radv_cmd_buffer *cmd_buffer, struct radv_image *image,
@@ -784,13 +834,19 @@ void radv_update_hiz_metadata(struct rad
 
 unsigned radv_instance_rate_prolog_index(unsigned num_attributes, uint32_t instance_rate_inputs);
 
-enum radv_cmd_flush_bits radv_src_access_flush(struct radv_cmd_buffer *cmd_buffer, VkPipelineStageFlags2 src_stages,
-                                               VkAccessFlags2 src_flags, VkAccessFlags3KHR src3_flags,
-                                               const struct radv_image *image, const VkImageSubresourceRange *range);
-
-enum radv_cmd_flush_bits radv_dst_access_flush(struct radv_cmd_buffer *cmd_buffer, VkPipelineStageFlags2 dst_stages,
-                                               VkAccessFlags2 dst_flags, VkAccessFlags3KHR dst3_flags,
-                                               const struct radv_image *image, const VkImageSubresourceRange *range);
+enum radv_cmd_flush_bits radv_src_access_flush(struct radv_cmd_buffer *cmd_buffer,
+                                               VkPipelineStageFlags2 src_stages,
+                                               VkAccessFlags2 src_flags,
+                                               VkAccessFlags3KHR src3_flags,
+                                               const struct radv_image *image,
+                                               const VkImageSubresourceRange *range);
+
+enum radv_cmd_flush_bits radv_dst_access_flush(struct radv_cmd_buffer *cmd_buffer,
+                                               VkPipelineStageFlags2 dst_stages,
+                                               VkAccessFlags2 dst_flags,
+                                               VkAccessFlags3KHR dst3_flags,
+                                               const struct radv_image *image,
+                                               const VkImageSubresourceRange *range);
 
 struct radv_resolve_barrier {
    VkPipelineStageFlags2 src_stage_mask;
@@ -799,7 +855,8 @@ struct radv_resolve_barrier {
    VkAccessFlags2 dst_access_mask;
 };
 
-void radv_emit_resolve_barrier(struct radv_cmd_buffer *cmd_buffer, const struct radv_resolve_barrier *barrier);
+void radv_emit_resolve_barrier(struct radv_cmd_buffer *cmd_buffer,
+                               const struct radv_resolve_barrier *barrier);
 
 struct radv_draw_info {
    /**
@@ -885,10 +942,11 @@ uint32_t radv_init_dcc(struct radv_cmd_b
 
 void radv_emit_cache_flush(struct radv_cmd_buffer *cmd_buffer);
 
-void radv_emit_set_predication_state(struct radv_cmd_buffer *cmd_buffer, bool draw_visible, unsigned pred_op,
-                                     uint64_t va);
+void radv_emit_set_predication_state(struct radv_cmd_buffer *cmd_buffer, bool draw_visible,
+                                     unsigned pred_op, uint64_t va);
 
-void radv_begin_conditional_rendering(struct radv_cmd_buffer *cmd_buffer, uint64_t va, bool draw_visible);
+void radv_begin_conditional_rendering(struct radv_cmd_buffer *cmd_buffer, uint64_t va,
+                                      bool draw_visible);
 
 void radv_end_conditional_rendering(struct radv_cmd_buffer *cmd_buffer);
 
@@ -906,7 +964,8 @@ struct radv_vbo_info {
    uint32_t non_trivial_format;
 };
 
-void radv_get_vbo_info(const struct radv_cmd_buffer *cmd_buffer, uint32_t vbo_idx, struct radv_vbo_info *vbo_info);
+void radv_get_vbo_info(const struct radv_cmd_buffer *cmd_buffer, uint32_t vbo_idx,
+                      struct radv_vbo_info *vbo_info);
 
 void radv_emit_compute_shader(const struct radv_physical_device *pdev, struct radv_cmd_stream *cs,
                               const struct radv_shader *shader);

--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -45,6 +45,22 @@
 #include "util/compiler.h"
 #include "util/fast_idiv_by_const.h"
 
+static void
+radv_emit_graphics_shaders(struct radv_cmd_buffer *cmd_buffer);
+
+/* Vega-specific cache optimization constants */
+#define VEGA_L1_SCALAR_CACHE_SIZE (16 * 1024)
+#define VEGA_L1_CACHE_LINE_SIZE 64
+#define VEGA_L2_CACHE_LINE_SIZE 64
+#define VEGA_SCALAR_ALU_WIDTH 32
+
+/* Memory barrier for Vega coherency */
+#define VEGA_MEMORY_BARRIER() __asm__ __volatile__("" ::: "memory")
+
+/* Prefetch hints optimized for Vega */
+#define VEGA_PREFETCH_READ(addr) __builtin_prefetch((addr), 0, 2)
+#define VEGA_PREFETCH_WRITE(addr) __builtin_prefetch((addr), 1, 2)
+
 enum {
    RADV_PREFETCH_VBO_DESCRIPTORS = (1 << 0),
    RADV_PREFETCH_VS = (1 << 1),
@@ -56,10 +72,61 @@ enum {
    RADV_PREFETCH_CS = (1 << 7),
    RADV_PREFETCH_RT = (1 << 8),
    RADV_PREFETCH_GFX_SHADERS = (RADV_PREFETCH_VS | RADV_PREFETCH_TCS | RADV_PREFETCH_TES | RADV_PREFETCH_GS |
-                                RADV_PREFETCH_PS | RADV_PREFETCH_MS),
+                               RADV_PREFETCH_PS | RADV_PREFETCH_MS),
    RADV_PREFETCH_GRAPHICS = (RADV_PREFETCH_VBO_DESCRIPTORS | RADV_PREFETCH_GFX_SHADERS),
 };
 
+/* Fast memcpy for small, known sizes using Vega's scalar registers */
+ALWAYS_INLINE static void
+radv_fast_memcpy_aligned(void *__restrict dst, const void *__restrict src, size_t size)
+{
+   assert(dst != NULL);
+   assert(src != NULL);
+   assert(((uintptr_t)dst & 3) == 0); /* 4-byte aligned */
+   assert(((uintptr_t)src & 3) == 0);
+
+   if (__builtin_constant_p(size)) {
+      switch (size) {
+      case 4:
+         *(uint32_t *)dst = *(const uint32_t *)src;
+         break;
+      case 8:
+         *(uint64_t *)dst = *(const uint64_t *)src;
+         break;
+      case 16: {
+         uint64_t *d = (uint64_t *)dst;
+         const uint64_t *s = (const uint64_t *)src;
+         d[0] = s[0];
+         d[1] = s[1];
+         break;
+      }
+      case 32: {
+         uint64_t *d = (uint64_t *)dst;
+         const uint64_t *s = (const uint64_t *)src;
+         d[0] = s[0];
+         d[1] = s[1];
+         d[2] = s[2];
+         d[3] = s[3];
+         break;
+      }
+      case 64: {
+         /* Optimized for cache line copy */
+         uint64_t *d = (uint64_t *)dst;
+         const uint64_t *s = (const uint64_t *)src;
+         for (int i = 0; i < 8; i++) {
+            d[i] = s[i];
+         }
+         break;
+      }
+      default:
+         memcpy(dst, src, size);
+         break;
+      }
+   } else {
+      memcpy(dst, src, size);
+   }
+}
+
 static void radv_handle_image_transition(struct radv_cmd_buffer *cmd_buffer, struct radv_image *image,
                                          VkImageLayout src_layout, VkImageLayout dst_layout, uint32_t src_family_index,
                                          uint32_t dst_family_index, const VkImageSubresourceRange *range,
@@ -657,52 +724,145 @@ radv_cmd_set_rendering_input_attachment_
 }
 
 ALWAYS_INLINE static void
-radv_cmd_set_sample_locations(struct radv_cmd_buffer *cmd_buffer, VkSampleCountFlagBits per_pixel, VkExtent2D grid_size,
-                              uint32_t count, const VkSampleLocationEXT *sample_locations)
+radv_cmd_set_sample_locations(struct radv_cmd_buffer *cmd_buffer,
+                              VkSampleCountFlagBits per_pixel,
+                              VkExtent2D grid_size,
+                              uint32_t count,
+                              const VkSampleLocationEXT *sample_locations)
 {
+   assert(cmd_buffer != NULL);
+
    struct radv_cmd_state *state = &cmd_buffer->state;
 
+   /* Validate sample count - must be power of 2 */
+   if (unlikely(per_pixel == 0 || (per_pixel & (per_pixel - 1)) != 0)) {
+      return; /* Invalid sample count */
+   }
+
    state->dynamic.sample_location.per_pixel = per_pixel;
    state->dynamic.sample_location.grid_size = grid_size;
-   state->dynamic.sample_location.count = count;
-   typed_memcpy(&state->dynamic.sample_location.locations[0], sample_locations, count);
+   state->dynamic.sample_location.count = MIN2(count, 256); /* Reasonable limit */
+
+   if (likely(count > 0 && sample_locations != NULL)) {
+      size_t copy_size = state->dynamic.sample_location.count * sizeof(VkSampleLocationEXT);
+      radv_fast_memcpy_aligned(&state->dynamic.sample_location.locations[0],
+                               sample_locations, copy_size);
+   }
 
    state->dirty_dynamic |= RADV_DYNAMIC_SAMPLE_LOCATIONS;
 }
 
 ALWAYS_INLINE static void
-radv_cmd_set_color_blend_equation(struct radv_cmd_buffer *cmd_buffer, uint32_t first, uint32_t count,
+radv_cmd_set_color_blend_equation(struct radv_cmd_buffer *cmd_buffer,
+                                  uint32_t first,
+                                  uint32_t count,
                                   const struct radv_blend_equation_state *blend_eq)
 {
+   assert(cmd_buffer != NULL);
+   assert(blend_eq != NULL);
+
    struct radv_cmd_state *state = &cmd_buffer->state;
 
-   typed_memcpy(state->dynamic.blend_eq.att + first, blend_eq->att, count);
-   if (first == 0)
+   /* Bounds check with saturation */
+   if (unlikely(first >= MAX_RTS)) {
+      return;
+   }
+   count = MIN2(count, MAX_RTS - first);
+
+   if (unlikely(count == 0)) {
+      return;
+   }
+
+   /* Optimized copy for blend equations */
+   radv_fast_memcpy_aligned(state->dynamic.blend_eq.att + first,
+                            blend_eq->att,
+                            count * sizeof(blend_eq->att[0]));
+
+   /* Update MRT0 dual source if needed */
+   if (first == 0) {
       state->dynamic.blend_eq.mrt0_is_dual_src = blend_eq->mrt0_is_dual_src;
+   }
 
    state->dirty_dynamic |= RADV_DYNAMIC_COLOR_BLEND_EQUATION;
 }
 
 ALWAYS_INLINE static void
-radv_cmd_set_vertex_binding_strides(struct radv_cmd_buffer *cmd_buffer, uint32_t first, uint32_t count,
+radv_cmd_set_vertex_binding_strides(struct radv_cmd_buffer *cmd_buffer,
+                                    uint32_t first,
+                                    uint32_t count,
                                     const uint16_t *strides)
 {
+   assert(cmd_buffer != NULL);
+
    struct radv_cmd_state *state = &cmd_buffer->state;
+   struct radv_device *device = radv_cmd_buffer_device(cmd_buffer);
+   const struct radv_physical_device *pdev = radv_device_physical(device);
+
+   /* Bounds check with saturation */
+   if (unlikely(first >= MESA_VK_MAX_VERTEX_BINDINGS)) {
+      return;
+   }
+   count = MIN2(count, MESA_VK_MAX_VERTEX_BINDINGS - first);
+
+   if (unlikely(count == 0 || strides == NULL)) {
+      return;
+   }
+
+   /* Vega optimization: Fast path for single stride update */
+   if (pdev->info.gfx_level == GFX9 && count == 1) {
+      if (state->dynamic.vk.vi_binding_strides[first] != strides[0]) {
+         state->dynamic.vk.vi_binding_strides[first] = strides[0];
+         state->dirty_dynamic |= RADV_DYNAMIC_VERTEX_INPUT_BINDING_STRIDE;
+      }
+      return;
+   }
+
+   /* Check if anything actually changed */
+   bool changed = false;
+   uint16_t *dst = state->dynamic.vk.vi_binding_strides + first;
+
+   /* Vega optimization: Use vectorized comparison for common counts */
+   if (count <= 16 && ((uintptr_t)dst & 7) == 0 && ((uintptr_t)strides & 7) == 0) {
+      /* 8-byte aligned, can use 64-bit comparisons */
+      uint32_t qwords = (count + 3) / 4;
+      const uint64_t *src64 = (const uint64_t *)strides;
+      uint64_t *dst64 = (uint64_t *)dst;
 
-   typed_memcpy(state->dynamic.vk.vi_binding_strides + first, strides, count);
+      for (uint32_t i = 0; i < qwords; i++) {
+         if (dst64[i] != src64[i]) {
+            changed = true;
+            dst64[i] = src64[i];
+         }
+      }
+   } else {
+      /* Fallback for unaligned or large counts */
+      for (uint32_t i = 0; i < count; i++) {
+         if (dst[i] != strides[i]) {
+            dst[i] = strides[i];
+            changed = true;
+         }
+      }
+   }
 
-   state->dirty_dynamic |= RADV_DYNAMIC_VERTEX_INPUT_BINDING_STRIDE;
+   if (changed) {
+      state->dirty_dynamic |= RADV_DYNAMIC_VERTEX_INPUT_BINDING_STRIDE;
+   }
 }
 
 ALWAYS_INLINE static void
-radv_cmd_set_vertex_input(struct radv_cmd_buffer *cmd_buffer, const struct radv_vertex_input_state *vi_state)
+radv_cmd_set_vertex_input(struct radv_cmd_buffer *cmd_buffer,
+                          const struct radv_vertex_input_state *vi_state)
 {
    struct radv_cmd_state *state = &cmd_buffer->state;
 
-   memcpy(&state->dynamic.vertex_input, vi_state, sizeof(*vi_state));
+   /* Use optimized memcpy for this critical structure */
+   radv_fast_memcpy_aligned(&state->dynamic.vertex_input, vi_state, sizeof(*vi_state));
 
    state->dirty_dynamic |= RADV_DYNAMIC_VERTEX_INPUT;
    state->dirty |= RADV_CMD_DIRTY_VS_PROLOG_STATE | RADV_CMD_DIRTY_VERTEX_BUFFER;
+
+   /* Memory barrier to ensure visibility */
+   VEGA_MEMORY_BARRIER();
 }
 
 static void
@@ -1115,9 +1275,26 @@ radv_write_data(struct radv_cmd_buffer *
 static void
 radv_emit_clear_data(struct radv_cmd_buffer *cmd_buffer, unsigned engine_sel, uint64_t va, unsigned size)
 {
-   uint32_t *zeroes = alloca(size);
-   memset(zeroes, 0, size);
-   radv_write_data(cmd_buffer, engine_sel, va, size / 4, zeroes, false);
+   /* 256-byte static zero buffer; resides in .rodata; avoids alloca and stack probes. */
+   static const uint32_t zeros[64] = {0};
+
+   /* All call sites use 4-byte multiples. Keep an assert and also a runtime guard. */
+   assert(size > 0 && (size % 4) == 0);
+
+   if (size <= sizeof(zeros)) {
+      /* Fast path: single write for tiny clears (typical: 8–256 bytes). */
+      radv_write_data(cmd_buffer, engine_sel, va, size / 4, zeros, false);
+      return;
+   }
+
+   /* Rare path: clear larger regions safely in chunks to avoid stack allocations. */
+   unsigned remaining = size;
+   while (remaining) {
+      const unsigned chunk = remaining > sizeof(zeros) ? sizeof(zeros) : remaining;
+      radv_write_data(cmd_buffer, engine_sel, va, chunk / 4, zeros, false);
+      va += chunk;
+      remaining -= chunk;
+   }
 }
 
 static void
@@ -1350,26 +1527,35 @@ bool
 radv_cmd_buffer_upload_alloc_aligned(struct radv_cmd_buffer *cmd_buffer, unsigned size, unsigned alignment,
                                      unsigned *out_offset, void **ptr)
 {
+   /* All uploads must be 4-byte aligned. */
    assert(size % 4 == 0);
 
    struct radv_device *device = radv_cmd_buffer_device(cmd_buffer);
    const struct radv_physical_device *pdev = radv_device_physical(device);
    const struct radeon_info *gpu_info = &pdev->info;
 
-   /* Align to the scalar cache line size if it results in this allocation
-    * being placed in less of them.
-    */
    unsigned offset = cmd_buffer->upload.offset;
-   unsigned line_size = gpu_info->gfx_level >= GFX10 ? 64 : 32;
-   unsigned gap = align(offset, line_size) - offset;
-   if ((size & (line_size - 1)) > gap)
-      offset = align(offset, line_size);
+   /* For GFX9 (Vega), the scalar cache line size is 32 bytes.
+    * We always align to at least this size to ensure that shader fetches
+    * of descriptors or constants do not suffer from cache-line splits.
+    * The requested 'alignment' is respected if it is larger.
+    */
+   const unsigned line_size = gpu_info->gfx_level >= GFX10 ? 64 : 32;
+
+   /* Determine the required alignment. Use the larger of the requested
+    * alignment and the hardware's cache line size. This can be done
+    * branchlessly.
+    */
+   unsigned final_alignment = alignment > line_size ? alignment : line_size;
+
+   /* Align the offset. The align() macro is typically branchless. */
+   offset = align(offset, final_alignment);
 
-   if (alignment)
-      offset = align(offset, alignment);
    if (offset + size > cmd_buffer->upload.size) {
-      if (!radv_cmd_buffer_resize_upload_buf(cmd_buffer, size))
+      if (!radv_cmd_buffer_resize_upload_buf(cmd_buffer, size)) {
          return false;
+      }
+      /* After resizing, the new offset is always 0, which is already aligned. */
       offset = 0;
    }
 
@@ -2620,6 +2806,16 @@ radv_emit_shader_prefetch(struct radv_cm
 
    va = radv_shader_get_va(shader);
 
+   struct radv_device *device = radv_cmd_buffer_device(cmd_buffer);
+   struct radv_cmd_stream *cs = cmd_buffer->cs;
+
+   /* Invariant: Never prefetch a VA unless its BO is in the IB's validation list.
+    * Otherwise, the CP DMA might touch an unmapped VA, leading to gfx ring timeout.
+    * Adding the buffer is idempotent and cheap for already-added BOs.
+    */
+   radv_cs_add_buffer(device->ws, cs->b, shader->bo);
+
+   /* Prefetch shader code into L2. This is a hint; it doesn't alter state. */
    radv_cp_dma_prefetch(cmd_buffer, va, shader->code_size);
 }
 
@@ -3896,12 +4092,17 @@ gfx103_emit_vrs_state(struct radv_cmd_bu
    }
 }
 
-static void
+void
 radv_emit_graphics_shaders(struct radv_cmd_buffer *cmd_buffer)
 {
    struct radv_device *device = radv_cmd_buffer_device(cmd_buffer);
    const struct radv_physical_device *pdev = radv_device_physical(device);
 
+   /* Warm L2 for first-stage graphics (VS/VBO/MS) as early as possible.
+    * Safe due to BO validation inside radv_emit_shader_prefetch().
+    */
+   radv_emit_graphics_prefetch(cmd_buffer, true);
+
    radv_foreach_stage (s, cmd_buffer->state.active_stages & RADV_GRAPHICS_STAGE_BITS) {
       switch (s) {
       case MESA_SHADER_VERTEX:
@@ -3923,9 +4124,6 @@ radv_emit_graphics_shaders(struct radv_c
       case MESA_SHADER_MESH:
          radv_emit_mesh_shader(cmd_buffer);
          break;
-      case MESA_SHADER_TASK:
-         radv_emit_compute_shader(pdev, cmd_buffer->gang.cs, cmd_buffer->state.shaders[MESA_SHADER_TASK]);
-         break;
       default:
          UNREACHABLE("invalid bind stage");
       }
@@ -3951,8 +4149,6 @@ radv_emit_graphics_shaders(struct radv_c
    const struct radv_vgt_shader_key vgt_shader_cfg_key =
       radv_get_vgt_shader_key(device, cmd_buffer->state.shaders, cmd_buffer->state.gs_copy_shader);
 
-   radv_emit_vgt_gs_mode(cmd_buffer);
-   radv_emit_vgt_reuse(cmd_buffer, &vgt_shader_cfg_key);
    radv_emit_vgt_shader_config(cmd_buffer, &vgt_shader_cfg_key);
 
    if (pdev->info.gfx_level >= GFX10_3) {
@@ -5647,46 +5843,35 @@ radv_emit_guardband_state(struct radv_cm
    const bool draw_lines =
       radv_vgt_outprim_is_line(vgt_outprim_type) || radv_polygon_mode_is_line(d->vk.rs.polygon_mode);
    struct radv_cmd_stream *cs = cmd_buffer->cs;
-   int i;
+   const float max_range = 32767.0f;
    float guardband_x = INFINITY, guardband_y = INFINITY;
    float discard_x = 1.0f, discard_y = 1.0f;
-   const float max_range = 32767.0f;
 
    if (!d->vk.vp.viewport_count)
       return;
 
-   for (i = 0; i < d->vk.vp.viewport_count; i++) {
-      float scale_x = fabsf(d->vp_xform[i].scale[0]);
-      float scale_y = fabsf(d->vp_xform[i].scale[1]);
-      const float translate_x = fabsf(d->vp_xform[i].translate[0]);
-      const float translate_y = fabsf(d->vp_xform[i].translate[1]);
-
-      if (scale_x < 0.5)
-         scale_x = 0.5;
-      if (scale_y < 0.5)
-         scale_y = 0.5;
-
-      guardband_x = MIN2(guardband_x, (max_range - translate_x) / scale_x);
-      guardband_y = MIN2(guardband_y, (max_range - translate_y) / scale_y);
-
-      if (draw_points || draw_lines) {
-         /* When rendering wide points or lines, we need to be more conservative about when to
-          * discard them entirely. */
-         float pixels;
-
-         if (draw_points) {
-            pixels = 8191.875f;
-         } else {
-            pixels = d->vk.rs.line.width;
-         }
-
-         /* Add half the point size / line width. */
-         discard_x += pixels / (2.0 * scale_x);
-         discard_y += pixels / (2.0 * scale_y);
-
-         /* Discard primitives that would lie entirely outside the clip region. */
-         discard_x = MIN2(discard_x, guardband_x);
-         discard_y = MIN2(discard_y, guardband_y);
+   const bool wide = draw_points || draw_lines;
+   const float pixels = draw_points ? 8191.875f : d->vk.rs.line.width;
+   const float halfpix = pixels * 0.5f;
+
+   for (unsigned i = 0; i < d->vk.vp.viewport_count; i++) {
+      float sx = fabsf(d->vp_xform[i].scale[0]);
+      float sy = fabsf(d->vp_xform[i].scale[1]);
+      sx = fmaxf(sx, 0.5f);
+      sy = fmaxf(sy, 0.5f);
+
+      const float tx = fabsf(d->vp_xform[i].translate[0]);
+      const float ty = fabsf(d->vp_xform[i].translate[1]);
+
+      const float gbx = (max_range - tx) / sx;
+      const float gby = (max_range - ty) / sy;
+
+      guardband_x = fminf(guardband_x, gbx);
+      guardband_y = fminf(guardband_y, gby);
+
+      if (wide) {
+         discard_x = fminf(discard_x + (halfpix / sx), guardband_x);
+         discard_y = fminf(discard_y + (halfpix / sy), guardband_y);
       }
    }
 
@@ -6541,22 +6726,75 @@ radv_write_vertex_descriptor(const struc
 }
 
 ALWAYS_INLINE static void
-radv_write_vertex_descriptors_dynamic(const struct radv_cmd_buffer *cmd_buffer, const struct radv_shader *vs,
+radv_write_vertex_descriptors_dynamic(const struct radv_cmd_buffer *cmd_buffer,
+                                      const struct radv_shader *vs,
                                       void *vb_ptr)
 {
-   unsigned desc_index = 0;
-   for (unsigned i = 0; i < vs->info.vs.num_attributes; i++) {
-      uint32_t *desc = &((uint32_t *)vb_ptr)[desc_index++ * 4];
+   assert(vs != NULL);
+   assert(vb_ptr != NULL);
+   assert(((uintptr_t)vb_ptr & 0xF) == 0); /* Require 16-byte alignment */
+
+   uint32_t *desc_ptr = (uint32_t *)vb_ptr;
+   const unsigned num_attrs = vs->info.vs.num_attributes;
+
+   if (num_attrs == 0) {
+      return;
+   }
+
+   /* Prefetch vertex input state early for better cache utilization */
+   const struct radv_dynamic_state *d = &cmd_buffer->state.dynamic;
+   if (d != NULL) {
+      VEGA_PREFETCH_READ(&d->vertex_input); /* Read, moderate temporal locality */
+      VEGA_PREFETCH_READ(&d->vertex_input.bindings[0]);
+      VEGA_PREFETCH_READ(&d->vertex_input.offsets[0]);
+   }
+
+   /* Process descriptors sequentially - dependencies prevent true parallelization */
+   for (unsigned i = 0; i < num_attrs; i++) {
+      uint32_t *desc = &desc_ptr[i * 4];
+
+      /* Bounds check */
+      if (i >= MAX_VERTEX_ATTRIBS) {
+         assert(!"Vertex attribute index out of bounds");
+         memset(desc, 0, 16);
+         continue;
+      }
+
       radv_write_vertex_descriptor(cmd_buffer, vs, i, true, desc);
    }
 }
 
 ALWAYS_INLINE static void
-radv_write_vertex_descriptors(const struct radv_cmd_buffer *cmd_buffer, const struct radv_shader *vs, void *vb_ptr)
-{
+radv_write_vertex_descriptors(const struct radv_cmd_buffer *cmd_buffer,
+                              const struct radv_shader *vs,
+                              void *vb_ptr)
+{
+   assert(vs != NULL);
+   assert(vb_ptr != NULL);
+   assert(((uintptr_t)vb_ptr & 0xF) == 0); /* Require 16-byte alignment */
+
+   uint32_t *desc_ptr = (uint32_t *)vb_ptr;
    unsigned desc_index = 0;
-   u_foreach_bit (i, vs->info.vs.vb_desc_usage_mask) {
-      uint32_t *desc = &((uint32_t *)vb_ptr)[desc_index++ * 4];
+   const uint32_t vb_desc_usage_mask = vs->info.vs.vb_desc_usage_mask;
+   if (vb_desc_usage_mask == 0) {
+      return;
+   }
+
+   /* Prefetch vertex bindings for the used attributes */
+   const struct radv_vertex_binding *vb = cmd_buffer->vertex_bindings;
+   if (vb != NULL) {
+      VEGA_PREFETCH_READ(vb); /* Read, moderate temporal locality */
+   }
+
+   u_foreach_bit(i, vb_desc_usage_mask) {
+      if (desc_index >= MAX_VERTEX_ATTRIBS) {
+         assert(!"Too many vertex descriptors");
+         break;
+      }
+
+      uint32_t *desc = &desc_ptr[desc_index * 4];
+      desc_index++;
+
       radv_write_vertex_descriptor(cmd_buffer, vs, i, false, desc);
    }
 }
@@ -6734,16 +6972,34 @@ struct radv_prim_vertex_count {
 static inline unsigned
 radv_prims_for_vertices(struct radv_prim_vertex_count *info, unsigned num)
 {
-   if (num == 0)
+   if (num < info->min) {
       return 0;
+   }
 
-   if (info->incr == 0)
+   /* This check also implicitly handles info->incr == 0, preventing division by zero. */
+   if (info->incr == 0) {
       return 0;
+   }
 
-   if (num < info->min)
-      return 0;
+   const unsigned n = num - info->min;
 
-   return 1 + ((num - info->min) / info->incr);
+   switch (info->incr) {
+   case 1:
+      return 1 + n;
+   case 2:
+      return 1 + (n >> 1);
+   case 3:
+      /* Fast unsigned division by 3: (n * 0xAAAAAAAB) >> 33 */
+      return 1 + (unsigned)(((uint64_t)n * 0xAAAAAAABull) >> 33);
+   case 4:
+      return 1 + (n >> 2);
+   case 6:
+      /* Fast unsigned division by 6: (n * 0x2AAAAAAB) >> 33 */
+      return 1 + (unsigned)(((uint64_t)n * 0x2AAAAAABull) >> 33);
+   default:
+      /* Fallback for uncommon divisors. */
+      return 1 + (n / info->incr);
+   }
 }
 
 static const struct radv_prim_vertex_count prim_size_table[] = {
@@ -7147,33 +7403,61 @@ can_skip_buffer_l2_flushes(struct radv_d
  */
 
 enum radv_cmd_flush_bits
-radv_src_access_flush(struct radv_cmd_buffer *cmd_buffer, VkPipelineStageFlags2 src_stages, VkAccessFlags2 src_flags,
-                      VkAccessFlags3KHR src3_flags, const struct radv_image *image,
+radv_src_access_flush(struct radv_cmd_buffer *cmd_buffer, VkPipelineStageFlags2 src_stages,
+                      VkAccessFlags2 src_flags, VkAccessFlags3KHR src3_flags, const struct radv_image *image,
                       const VkImageSubresourceRange *range)
 {
-   const struct radv_device *device = radv_cmd_buffer_device(cmd_buffer);
+   (void)src3_flags; /* API parameter; not needed here. Keep ABI stable. */
 
+   struct radv_device *device = radv_cmd_buffer_device(cmd_buffer);
+
+   /* Expand stage-dependent access flags per Vulkan spec so we operate
+    * on a fully expanded source access set.
+    */
    src_flags = vk_expand_src_access_flags2(src_stages, src_flags);
 
-   bool has_CB_meta = true, has_DB_meta = true;
-   bool image_is_coherent = image ? radv_image_is_l2_coherent(device, image, range) : false;
+   /* Tracking flags for metadata support on the given image. */
+   bool has_CB_meta = true;
+   bool has_DB_meta = true;
+
+   /* Determine whether the resource (image or buffer) is L2-coherent.
+    * - For images: query per-image coherence (format/usage/layout family).
+    * - For buffers (image == NULL): use the GPU capability/heuristic to
+    *   decide if buffer L2 flushes can be skipped.
+    */
+   const bool resource_is_coherent =
+      image ? radv_image_is_l2_coherent(device, image, range)
+            : can_skip_buffer_l2_flushes(device);
+
    enum radv_cmd_flush_bits flush_bits = 0;
 
+   /* If we have an image, determine if CB/DB metadata paths are enabled. */
    if (image) {
       if (!radv_image_has_CB_metadata(image))
          has_CB_meta = false;
+      /* HTILE enabled => DB/HTILE metadata exists on this mip. */
       if (!radv_htile_enabled(image, range ? range->baseMipLevel : 0))
          has_DB_meta = false;
    }
 
-   if (src_flags & VK_ACCESS_2_COMMAND_PREPROCESS_WRITE_BIT_EXT)
+   /* Command preprocess writes (e.g., DGC/NGG meta):
+    * Conservative L2 invalidate to ensure command streams are visible.
+    * This path is rare and correctness-sensitive.
+    */
+   if (src_flags & VK_ACCESS_2_COMMAND_PREPROCESS_WRITE_BIT_EXT) {
       flush_bits |= RADV_CMD_FLAG_INV_L2;
+   }
+
+   /* SSBO and Acceleration Structure writes:
+    * Ensure visibility for consumers. If the write is to an IMAGE via meta operations
+    * (no STORAGE usage), flush the appropriate block (CB/DB) too.
+    * For buffers on parts with coherent L2, avoid heavy L2 invalidations.
+    */
+   if (src_flags & (VK_ACCESS_2_SHADER_STORAGE_WRITE_BIT |
+                    VK_ACCESS_2_ACCELERATION_STRUCTURE_WRITE_BIT_KHR)) {
 
-   if (src_flags & (VK_ACCESS_2_SHADER_STORAGE_WRITE_BIT | VK_ACCESS_2_ACCELERATION_STRUCTURE_WRITE_BIT_KHR)) {
-      /* since the STORAGE bit isn't set we know that this is a meta operation.
-       * on the dst flush side we skip CB/DB flushes without the STORAGE bit, so
-       * set it here. */
       if (image && !(image->vk.usage & VK_IMAGE_USAGE_STORAGE_BIT)) {
+         /* These writes likely went through CB/DB or other non-storage image paths. */
          if (vk_format_is_depth_or_stencil(image->vk.format)) {
             flush_bits |= RADV_CMD_FLAG_FLUSH_AND_INV_DB;
          } else {
@@ -7181,37 +7465,69 @@ radv_src_access_flush(struct radv_cmd_bu
          }
       }
 
-      if (!image_is_coherent)
+      /* Only touch L2 if the resource isn't L2-coherent. Buffers on GFX9+ often are coherent. */
+      if (!resource_is_coherent) {
+         /* Invalidate L2 on source side for generic shader/AS writes to discard stale clean lines
+          * and synchronize with units that may have bypassed L2 (e.g., SDMA or non-coherent paths).
+          * Destination side will handle its own client cache invalidations (e.g., VCACHE/SCACHE).
+          */
          flush_bits |= RADV_CMD_FLAG_INV_L2;
+      }
    }
 
-   if (src_flags &
-       (VK_ACCESS_2_TRANSFORM_FEEDBACK_WRITE_BIT_EXT | VK_ACCESS_2_TRANSFORM_FEEDBACK_COUNTER_WRITE_BIT_EXT)) {
-      if (!image_is_coherent)
+   /* Transform feedback writes and counters are buffer writes.
+    * If not L2-coherent, publish results by writing back dirty L2 lines. */
+   if (src_flags & (VK_ACCESS_2_TRANSFORM_FEEDBACK_WRITE_BIT_EXT |
+                    VK_ACCESS_2_TRANSFORM_FEEDBACK_COUNTER_WRITE_BIT_EXT)) {
+      if (!resource_is_coherent) {
          flush_bits |= RADV_CMD_FLAG_WB_L2;
+      }
    }
 
+   /* Color attachment writes:
+    * - Always flush+invalidate CB to force data out of RB caches.
+    * - If the image is not L2-coherent, write back dirty L2 lines to VRAM
+    *   (publishes results without thrashing clean data).
+    * - Invalidate CB metadata (DCC) when present.
+    */
    if (src_flags & VK_ACCESS_2_COLOR_ATTACHMENT_WRITE_BIT) {
       flush_bits |= RADV_CMD_FLAG_FLUSH_AND_INV_CB;
-      if (!image_is_coherent)
-         flush_bits |= RADV_CMD_FLAG_INV_L2;
+
+      if (!resource_is_coherent)
+         flush_bits |= RADV_CMD_FLAG_WB_L2;
+
       if (has_CB_meta)
          flush_bits |= RADV_CMD_FLAG_FLUSH_AND_INV_CB_META;
    }
 
+   /* Depth/stencil attachment writes:
+    * - Always flush+invalidate DB to force data out of DB caches.
+    * - If the image is not L2-coherent, write back dirty L2 lines to VRAM.
+    * - Invalidate DB metadata (HTILE) when enabled.
+    */
    if (src_flags & VK_ACCESS_2_DEPTH_STENCIL_ATTACHMENT_WRITE_BIT) {
       flush_bits |= RADV_CMD_FLAG_FLUSH_AND_INV_DB;
-      if (!image_is_coherent)
-         flush_bits |= RADV_CMD_FLAG_INV_L2;
+
+      if (!resource_is_coherent)
+         flush_bits |= RADV_CMD_FLAG_WB_L2;
+
       if (has_DB_meta)
          flush_bits |= RADV_CMD_FLAG_FLUSH_AND_INV_DB_META;
    }
 
+   /* Transfer writes (copies/blits/clears):
+    * - Conservative: flush both CB and DB (some transfer paths use either).
+    * - If the resource is not L2-coherent, invalidate L2 on the source side,
+    *   because SDMA or other paths may bypass L2, leaving stale clean lines.
+    * - Invalidate metadata as needed.
+    */
    if (src_flags & VK_ACCESS_2_TRANSFER_WRITE_BIT) {
-      flush_bits |= RADV_CMD_FLAG_FLUSH_AND_INV_CB | RADV_CMD_FLAG_FLUSH_AND_INV_DB;
+      flush_bits |= RADV_CMD_FLAG_FLUSH_AND_INV_CB |
+                    RADV_CMD_FLAG_FLUSH_AND_INV_DB;
 
-      if (!image_is_coherent)
+      if (!resource_is_coherent)
          flush_bits |= RADV_CMD_FLAG_INV_L2;
+
       if (has_CB_meta)
          flush_bits |= RADV_CMD_FLAG_FLUSH_AND_INV_CB_META;
       if (has_DB_meta)
