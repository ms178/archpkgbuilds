From d3519f28152f20d133476a1a4a0dac9e6595dfc7 Mon Sep 17 00:00:00 2001
From: Kannan Perumal Reddiar <kannan.reddiar@intel.com>
Date: Fri, 30 May 2025 11:14:17 +0530
Subject: [PATCH 1/4] util/cpu_detect: SHA extension support detection in x86

SHA extension support is detected via CPUID instruction:
leaf 07H, sub-leaf 0, EBX bit 29. If this bit is set,
the processor supports the SHA instruction extensions.
---
 src/util/u_cpu_detect.c | 2 ++
 src/util/u_cpu_detect.h | 1 +
 2 files changed, 3 insertions(+)

diff --git a/src/util/u_cpu_detect.c b/src/util/u_cpu_detect.c
index d9bf9cca7e72b..3c06f41313230 100644
--- a/src/util/u_cpu_detect.c
+++ b/src/util/u_cpu_detect.c
@@ -839,6 +839,7 @@ _util_cpu_detect_once(void)
          uint32_t regs7[4];
          cpuid_count(0x00000007, 0x00000000, regs7);
          util_cpu_caps.has_clflushopt = (regs7[1] >> 23) & 1;
+         util_cpu_caps.has_sha_extn = (regs7[1] >> 29) & 1;
          if (util_cpu_caps.has_avx) {
             util_cpu_caps.has_avx2 = (regs7[1] >> 5) & 1;
 
@@ -928,6 +929,7 @@ _util_cpu_detect_once(void)
       printf("util_cpu_caps.has_clflushopt = %u\n", util_cpu_caps.has_clflushopt);
       printf("util_cpu_caps.num_L3_caches = %u\n", util_cpu_caps.num_L3_caches);
       printf("util_cpu_caps.num_cpu_mask_bits = %u\n", util_cpu_caps.num_cpu_mask_bits);
+      printf("util_cpu_caps.has_sha_extn = %u\n", util_cpu_caps.has_sha_extn);
    }
    _util_cpu_caps_state.caps = util_cpu_caps;
 
diff --git a/src/util/u_cpu_detect.h b/src/util/u_cpu_detect.h
index ef4c38db2636e..6f42a1e11fc0a 100644
--- a/src/util/u_cpu_detect.h
+++ b/src/util/u_cpu_detect.h
@@ -103,6 +103,7 @@ struct util_cpu_caps_t {
    unsigned has_avx512vbmi:1;
 
    unsigned has_clflushopt:1;
+   unsigned has_sha_extn:1;
 
    unsigned num_L3_caches;
    unsigned num_cpu_mask_bits;
-- 
GitLab


From ff92fd61313522f8b1e0cc8f49c0b781c1c8b9c5 Mon Sep 17 00:00:00 2001
From: Kannan Perumal Reddiar <kannan.reddiar@intel.com>
Date: Fri, 30 May 2025 15:01:38 +0530
Subject: [PATCH 2/4] util/sha1: Optimize SHA1 performance for x86 CPUs

This patch enhances the SHA1 implementation by integrating
x86-specific intrinsics to reduce CPU cycle usage.

Key improvements include:
 - Replacing standard operations with x86 intrinsics for
   improved efficiency.
 - Refactoring the main loop to utilize SIMD instructions.

The computed hash values were manually verified using a
standalone test bench, both with and without the optimizations.
No differences in hash output were observed.
With the optimizations enabled, a ~50% reduction in CPU time
for SHA1 computation was achieved.
---
 src/util/sha1/sha1.c | 222 ++++++++++++++++++++++++++++++++++++++++++-
 1 file changed, 220 insertions(+), 2 deletions(-)

diff --git a/src/util/sha1/sha1.c b/src/util/sha1/sha1.c
index ed043fe39de9c..5a37b4d35ee8f 100644
--- a/src/util/sha1/sha1.c
+++ b/src/util/sha1/sha1.c
@@ -16,6 +16,11 @@
 
 #include <stdint.h>
 #include <string.h>
+#include "util/detect_arch.h"
+#if DETECT_ARCH_X86 || DETECT_ARCH_X86_64
+#include <immintrin.h>
+#endif
+#include "util/u_cpu_detect.h"
 #include "u_endian.h"
 #include "macros.h"
 #include "sha1.h"
@@ -101,6 +106,194 @@ SHA1Transform(uint32_t state[5], const uint8_t buffer[SHA1_BLOCK_LENGTH])
 	a = b = c = d = e = 0;
 }
 
+#if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && \
+    (__SHA__) && \
+    (UTIL_ARCH_LITTLE_ENDIAN)
+static void
+SHA1Transform_x86(uint32_t state[5], const uint8_t data[])
+{
+   __m128i abcd, abcd_save, e0, e0_save, e1;
+   __m128i msg0, msg1, msg2, msg3;
+   const __m128i MASK = _mm_set_epi64x(0x0001020304050607ULL,
+				       0x08090a0b0c0d0e0fULL);
+
+   /* Load initial values */
+   abcd = _mm_loadu_si128((const __m128i*) state);
+   e0 = _mm_set_epi32(state[4], 0, 0, 0);
+
+   abcd = _mm_shuffle_epi32(abcd, 0x1B);
+
+   /* Save current state  */
+   abcd_save = abcd;
+   e0_save = e0;
+
+   /* Rounds 0-3 */
+   msg0 = _mm_loadu_si128((const __m128i*)(data + 0));
+   msg0 = _mm_shuffle_epi8(msg0, MASK);
+
+   e0 = _mm_add_epi32(e0, msg0);
+   e1 = abcd;
+   abcd = _mm_sha1rnds4_epu32(abcd, e0, 0);
+
+   /* Rounds 4-7 */
+   msg1 = _mm_loadu_si128((const __m128i*)(data + 16));
+   msg1 = _mm_shuffle_epi8(msg1, MASK);
+   e1 = _mm_sha1nexte_epu32(e1, msg1);
+   e0 = abcd;
+   abcd = _mm_sha1rnds4_epu32(abcd, e1, 0);
+   msg0 = _mm_sha1msg1_epu32(msg0, msg1);
+
+   /* Rounds 8-11 */
+   msg2 = _mm_loadu_si128((const __m128i*)(data + 32));
+   msg2 = _mm_shuffle_epi8(msg2, MASK);
+   e0 = _mm_sha1nexte_epu32(e0, msg2);
+   e1 = abcd;
+   abcd = _mm_sha1rnds4_epu32(abcd, e0, 0);
+   msg1 = _mm_sha1msg1_epu32(msg1, msg2);
+   msg0 = _mm_xor_si128(msg0, msg2);
+
+   /* Rounds 12-15 */
+   msg3 = _mm_loadu_si128((const __m128i*)(data + 48));
+   msg3 = _mm_shuffle_epi8(msg3, MASK);
+   e1 = _mm_sha1nexte_epu32(e1, msg3);
+   e0 = abcd;
+   msg0 = _mm_sha1msg2_epu32(msg0, msg3);
+   abcd = _mm_sha1rnds4_epu32(abcd, e1, 0);
+   msg2 = _mm_sha1msg1_epu32(msg2, msg3);
+   msg1 = _mm_xor_si128(msg1, msg3);
+
+   /* Rounds 16-19 */
+   e0 = _mm_sha1nexte_epu32(e0, msg0);
+   e1 = abcd;
+   msg1 = _mm_sha1msg2_epu32(msg1, msg0);
+   abcd = _mm_sha1rnds4_epu32(abcd, e0, 0);
+   msg3 = _mm_sha1msg1_epu32(msg3, msg0);
+   msg2 = _mm_xor_si128(msg2, msg0);
+
+   /* Rounds 20-23 */
+   e1 = _mm_sha1nexte_epu32(e1, msg1);
+   e0 = abcd;
+   msg2 = _mm_sha1msg2_epu32(msg2, msg1);
+   abcd = _mm_sha1rnds4_epu32(abcd, e1, 1);
+   msg0 = _mm_sha1msg1_epu32(msg0, msg1);
+   msg3 = _mm_xor_si128(msg3, msg1);
+
+   /* Rounds 24-27 */
+   e0 = _mm_sha1nexte_epu32(e0, msg2);
+   e1 = abcd;
+   msg3 = _mm_sha1msg2_epu32(msg3, msg2);
+   abcd = _mm_sha1rnds4_epu32(abcd, e0, 1);
+   msg1 = _mm_sha1msg1_epu32(msg1, msg2);
+   msg0 = _mm_xor_si128(msg0, msg2);
+
+   /* Rounds 28-31 */
+   e1 = _mm_sha1nexte_epu32(e1, msg3);
+   e0 = abcd;
+   msg0 = _mm_sha1msg2_epu32(msg0, msg3);
+   abcd = _mm_sha1rnds4_epu32(abcd, e1, 1);
+   msg2 = _mm_sha1msg1_epu32(msg2, msg3);
+   msg1 = _mm_xor_si128(msg1, msg3);
+
+   /* Rounds 32-35 */
+   e0 = _mm_sha1nexte_epu32(e0, msg0);
+   e1 = abcd;
+   msg1 = _mm_sha1msg2_epu32(msg1, msg0);
+   abcd = _mm_sha1rnds4_epu32(abcd, e0, 1);
+   msg3 = _mm_sha1msg1_epu32(msg3, msg0);
+   msg2 = _mm_xor_si128(msg2, msg0);
+
+   /* Rounds 36-39 */
+   e1 = _mm_sha1nexte_epu32(e1, msg1);
+   e0 = abcd;
+   msg2 = _mm_sha1msg2_epu32(msg2, msg1);
+   abcd = _mm_sha1rnds4_epu32(abcd, e1, 1);
+   msg0 = _mm_sha1msg1_epu32(msg0, msg1);
+   msg3 = _mm_xor_si128(msg3, msg1);
+
+   /* Rounds 40-43 */
+   e0 = _mm_sha1nexte_epu32(e0, msg2);
+   e1 = abcd;
+   msg3 = _mm_sha1msg2_epu32(msg3, msg2);
+   abcd = _mm_sha1rnds4_epu32(abcd, e0, 2);
+   msg1 = _mm_sha1msg1_epu32(msg1, msg2);
+   msg0 = _mm_xor_si128(msg0, msg2);
+
+   /* Rounds 44-47 */
+   e1 = _mm_sha1nexte_epu32(e1, msg3);
+   e0 = abcd;
+   msg0 = _mm_sha1msg2_epu32(msg0, msg3);
+   abcd = _mm_sha1rnds4_epu32(abcd, e1, 2);
+   msg2 = _mm_sha1msg1_epu32(msg2, msg3);
+   msg1 = _mm_xor_si128(msg1, msg3);
+
+   /* Rounds 48-51 */
+   e0 = _mm_sha1nexte_epu32(e0, msg0);
+   e1 = abcd;
+   msg1 = _mm_sha1msg2_epu32(msg1, msg0);
+   abcd = _mm_sha1rnds4_epu32(abcd, e0, 2);
+   msg3 = _mm_sha1msg1_epu32(msg3, msg0);
+   msg2 = _mm_xor_si128(msg2, msg0);
+
+   /* Rounds 52-55 */
+   e1 = _mm_sha1nexte_epu32(e1, msg1);
+   e0 = abcd;
+   msg2 = _mm_sha1msg2_epu32(msg2, msg1);
+   abcd = _mm_sha1rnds4_epu32(abcd, e1, 2);
+   msg0 = _mm_sha1msg1_epu32(msg0, msg1);
+   msg3 = _mm_xor_si128(msg3, msg1);
+
+   /* Rounds 56-59 */
+   e0 = _mm_sha1nexte_epu32(e0, msg2);
+   e1 = abcd;
+   msg3 = _mm_sha1msg2_epu32(msg3, msg2);
+   abcd = _mm_sha1rnds4_epu32(abcd, e0, 2);
+   msg1 = _mm_sha1msg1_epu32(msg1, msg2);
+   msg0 = _mm_xor_si128(msg0, msg2);
+
+   /* Rounds 60-63 */
+   e1 = _mm_sha1nexte_epu32(e1, msg3);
+   e0 = abcd;
+   msg0 = _mm_sha1msg2_epu32(msg0, msg3);
+   abcd = _mm_sha1rnds4_epu32(abcd, e1, 3);
+   msg2 = _mm_sha1msg1_epu32(msg2, msg3);
+   msg1 = _mm_xor_si128(msg1, msg3);
+
+   /* Rounds 64-67 */
+   e0 = _mm_sha1nexte_epu32(e0, msg0);
+   e1 = abcd;
+   msg1 = _mm_sha1msg2_epu32(msg1, msg0);
+   abcd = _mm_sha1rnds4_epu32(abcd, e0, 3);
+   msg3 = _mm_sha1msg1_epu32(msg3, msg0);
+   msg2 = _mm_xor_si128(msg2, msg0);
+
+   /* Rounds 68-71 */
+   e1 = _mm_sha1nexte_epu32(e1, msg1);
+   e0 = abcd;
+   msg2 = _mm_sha1msg2_epu32(msg2, msg1);
+   abcd = _mm_sha1rnds4_epu32(abcd, e1, 3);
+   msg3 = _mm_xor_si128(msg3, msg1);
+
+   /* Rounds 72-75 */
+   e0 = _mm_sha1nexte_epu32(e0, msg2);
+   e1 = abcd;
+   msg3 = _mm_sha1msg2_epu32(msg3, msg2);
+   abcd = _mm_sha1rnds4_epu32(abcd, e0, 3);
+
+   /* Rounds 76-79 */
+   e1 = _mm_sha1nexte_epu32(e1, msg3);
+   e0 = abcd;
+   abcd = _mm_sha1rnds4_epu32(abcd, e1, 3);
+
+   /* Combine state */
+   e0 = _mm_sha1nexte_epu32(e0, e0_save);
+   abcd = _mm_add_epi32(abcd, abcd_save);
+
+   /* Save state */
+   abcd = _mm_shuffle_epi32(abcd, 0x1B);
+   _mm_storeu_si128((__m128i*) state, abcd);
+   state[4] = _mm_extract_epi32(e0, 3);
+}
+#endif
 
 /*
  * SHA1Init - Initialize new context
@@ -126,15 +319,40 @@ void
 SHA1Update(SHA1_CTX *context, const uint8_t *data, size_t len)
 {
 	size_t i, j;
+#if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && \
+    (__SHA__) && \
+    (UTIL_ARCH_LITTLE_ENDIAN)
+	const struct util_cpu_caps_t *cpu_caps = util_get_cpu_caps();
+#endif
 
 	j = (size_t)((context->count >> 3) & 63);
 	context->count += (len << 3);
 	if ((j + len) > 63) {
 		(void)memcpy(&context->buffer[j], data, (i = 64-j));
-		SHA1Transform(context->state, context->buffer);
+#if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && \
+    (__SHA__) && \
+    (UTIL_ARCH_LITTLE_ENDIAN)
+		if (cpu_caps->has_sha_extn) {
+			SHA1Transform_x86(context->state,
+					context->buffer);
+		}
+		else
+#endif
+			SHA1Transform(context->state,
+					context->buffer);
 		for ( ; i + 63 < len; i += 64) {
 			assume(len >= 64);
-			SHA1Transform(context->state, (uint8_t *)&data[i]);
+#if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && \
+    (__SHA__) && \
+    (UTIL_ARCH_LITTLE_ENDIAN)
+			if (cpu_caps->has_sha_extn) {
+				SHA1Transform_x86(context->state,
+					  (uint8_t *)&data[i]);
+			}
+			else
+#endif
+				SHA1Transform(context->state,
+					(uint8_t *)&data[i]);
 		}
 		j = 0;
 	} else {
-- 
GitLab


From 7a1e7e4bbf11173b595462184a3c3d3feba2f9c9 Mon Sep 17 00:00:00 2001
From: Kannan Perumal Reddiar <kannan.reddiar@intel.com>
Date: Sun, 10 Aug 2025 20:34:21 +0530
Subject: [PATCH 3/4] util/sha1: Addressed review comments

---
 src/util/sha1/sha1.c | 73 +++++++++++++++++++-------------------------
 1 file changed, 32 insertions(+), 41 deletions(-)

diff --git a/src/util/sha1/sha1.c b/src/util/sha1/sha1.c
index 5a37b4d35ee8f..6caf2d21318cd 100644
--- a/src/util/sha1/sha1.c
+++ b/src/util/sha1/sha1.c
@@ -106,9 +106,8 @@ SHA1Transform(uint32_t state[5], const uint8_t buffer[SHA1_BLOCK_LENGTH])
 	a = b = c = d = e = 0;
 }
 
-#if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && \
-    (__SHA__) && \
-    (UTIL_ARCH_LITTLE_ENDIAN)
+#if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && (__SHA__) && \
+   (UTIL_ARCH_LITTLE_ENDIAN)
 static void
 SHA1Transform_x86(uint32_t state[5], const uint8_t data[])
 {
@@ -318,47 +317,39 @@ SHA1Init(SHA1_CTX *context)
 void
 SHA1Update(SHA1_CTX *context, const uint8_t *data, size_t len)
 {
-	size_t i, j;
-#if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && \
-    (__SHA__) && \
-    (UTIL_ARCH_LITTLE_ENDIAN)
-	const struct util_cpu_caps_t *cpu_caps = util_get_cpu_caps();
+#if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && (__SHA__) && \
+   (UTIL_ARCH_LITTLE_ENDIAN)
+   const struct util_cpu_caps_t *cpu_caps = util_get_cpu_caps();
 #endif
-
-	j = (size_t)((context->count >> 3) & 63);
-	context->count += (len << 3);
-	if ((j + len) > 63) {
-		(void)memcpy(&context->buffer[j], data, (i = 64-j));
-#if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && \
-    (__SHA__) && \
-    (UTIL_ARCH_LITTLE_ENDIAN)
-		if (cpu_caps->has_sha_extn) {
-			SHA1Transform_x86(context->state,
-					context->buffer);
-		}
-		else
+   size_t j = (size_t)((context->count >> 3) & 63);
+   context->count += (len << 3);
+
+   size_t i = 0;
+   if (__builtin_expect(j + len > 63, 0)) {
+      (void)memcpy(&context->buffer[j], data, (i = 64-j));
+      if (
+#if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && (__SHA__) && \
+   (UTIL_ARCH_LITTLE_ENDIAN)
+         cpu_caps->has_sha_extn ?
+         (SHA1Transform_x86(context->state, context->buffer),0) :
 #endif
-			SHA1Transform(context->state,
-					context->buffer);
-		for ( ; i + 63 < len; i += 64) {
-			assume(len >= 64);
-#if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && \
-    (__SHA__) && \
-    (UTIL_ARCH_LITTLE_ENDIAN)
-			if (cpu_caps->has_sha_extn) {
-				SHA1Transform_x86(context->state,
-					  (uint8_t *)&data[i]);
-			}
-			else
+         (SHA1Transform(context->state, context->buffer),0)
+      ) {
+      }
+
+      /* len >= i is guaranteed because we're in the branch where j+len>63 */
+      for ( ; i + 63 < len; i += 64) {
+#if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && (__SHA__) && \
+   (UTIL_ARCH_LITTLE_ENDIAN)
+         if (__builtin_expect(cpu_caps->has_sha_extn,1))
+            SHA1Transform_x86(context->state, data + i);
+         else
 #endif
-				SHA1Transform(context->state,
-					(uint8_t *)&data[i]);
-		}
-		j = 0;
-	} else {
-		i = 0;
-	}
-	(void)memcpy(&context->buffer[j], &data[i], len - i);
+            SHA1Transform(context->state, data + i);
+      }
+      j = 0;
+   }
+   (void)memcpy(&context->buffer[j], data + i, len - i);
 }
 
 
-- 
GitLab


From a237cb669e3a2ddfecc671f0c1056bb56ef34409 Mon Sep 17 00:00:00 2001
From: Kannan Perumal Reddiar <kannan.reddiar@intel.com>
Date: Tue, 12 Aug 2025 13:46:44 +0530
Subject: [PATCH 4/4] util/sha1: Review comments incorporated

---
 src/util/sha1/sha1.c | 20 +++++++++++---------
 1 file changed, 11 insertions(+), 9 deletions(-)

diff --git a/src/util/sha1/sha1.c b/src/util/sha1/sha1.c
index 6caf2d21318cd..01a9c329733ef 100644
--- a/src/util/sha1/sha1.c
+++ b/src/util/sha1/sha1.c
@@ -325,23 +325,24 @@ SHA1Update(SHA1_CTX *context, const uint8_t *data, size_t len)
    context->count += (len << 3);
 
    size_t i = 0;
-   if (__builtin_expect(j + len > 63, 0)) {
+   /* Check if buffer + new data exceeds 64 bytes */
+   if (j + len > 63) {
+      /* Fill buffer and process one block of 64 bytes */
       (void)memcpy(&context->buffer[j], data, (i = 64-j));
-      if (
 #if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && (__SHA__) && \
    (UTIL_ARCH_LITTLE_ENDIAN)
-         cpu_caps->has_sha_extn ?
-         (SHA1Transform_x86(context->state, context->buffer),0) :
+      if (likely(cpu_caps->has_sha_extn))
+         SHA1Transform_x86(context->state, context->buffer);
+      else
 #endif
-         (SHA1Transform(context->state, context->buffer),0)
-      ) {
-      }
+         SHA1Transform(context->state, context->buffer);
 
-      /* len >= i is guaranteed because we're in the branch where j+len>63 */
+      /* Processes remaining each 64 byte chunk directly from data */
       for ( ; i + 63 < len; i += 64) {
+         assume(len >= 64);
 #if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && (__SHA__) && \
    (UTIL_ARCH_LITTLE_ENDIAN)
-         if (__builtin_expect(cpu_caps->has_sha_extn,1))
+         if (likely(cpu_caps->has_sha_extn))
             SHA1Transform_x86(context->state, data + i);
          else
 #endif
@@ -349,6 +350,7 @@ SHA1Update(SHA1_CTX *context, const uint8_t *data, size_t len)
       }
       j = 0;
    }
+   /* Copy any remaining bytes < 64 to the buffer for future processing */
    (void)memcpy(&context->buffer[j], data + i, len - i);
 }
 
-- 
GitLab

