From 2442119420c2ceaaf878c0da4addb71397822880 Mon Sep 17 00:00:00 2001
From: Kannan Perumal Reddiar <kannan.reddiar@intel.com>
Date: Fri, 30 May 2025 11:14:17 +0530
Subject: [PATCH 1/2] util/cpu_detect: SHA extension support detection in x86

SHA extension support is detected via CPUID instruction:
leaf 07H, sub-leaf 0, EBX bit 29. If this bit is set,
the processor supports the SHA instruction extensions.
---
 src/util/u_cpu_detect.c | 2 ++
 src/util/u_cpu_detect.h | 1 +
 2 files changed, 3 insertions(+)

diff --git a/src/util/u_cpu_detect.c b/src/util/u_cpu_detect.c
index d67a72ee1dbbe..a9b63614c9621 100644
--- a/src/util/u_cpu_detect.c
+++ b/src/util/u_cpu_detect.c
@@ -839,6 +839,7 @@ _util_cpu_detect_once(void)
          uint32_t regs7[4];
          cpuid_count(0x00000007, 0x00000000, regs7);
          util_cpu_caps.has_clflushopt = (regs7[1] >> 23) & 1;
+         util_cpu_caps.has_sha_extn = (regs7[1] >> 29) & 1;
          if (util_cpu_caps.has_avx) {
             util_cpu_caps.has_avx2 = (regs7[1] >> 5) & 1;
 
@@ -928,6 +929,7 @@ _util_cpu_detect_once(void)
       printf("util_cpu_caps.has_clflushopt = %u\n", util_cpu_caps.has_clflushopt);
       printf("util_cpu_caps.num_L3_caches = %u\n", util_cpu_caps.num_L3_caches);
       printf("util_cpu_caps.num_cpu_mask_bits = %u\n", util_cpu_caps.num_cpu_mask_bits);
+      printf("util_cpu_caps.has_sha_extn = %u\n", util_cpu_caps.has_sha_extn);
    }
    _util_cpu_caps_state.caps = util_cpu_caps;
 
diff --git a/src/util/u_cpu_detect.h b/src/util/u_cpu_detect.h
index ef4c38db2636e..6f42a1e11fc0a 100644
--- a/src/util/u_cpu_detect.h
+++ b/src/util/u_cpu_detect.h
@@ -103,6 +103,7 @@ struct util_cpu_caps_t {
    unsigned has_avx512vbmi:1;
 
    unsigned has_clflushopt:1;
+   unsigned has_sha_extn:1;
 
    unsigned num_L3_caches;
    unsigned num_cpu_mask_bits;
-- 
GitLab


From cfe7e8aa9198dd3ada1b261f358d7170ae2c987c Mon Sep 17 00:00:00 2001
From: Kannan Perumal Reddiar <kannan.reddiar@intel.com>
Date: Fri, 30 May 2025 15:01:38 +0530
Subject: [PATCH 2/2] util/sha1: Optimize SHA1 performance for x86 CPUs

This patch enhances the SHA1 implementation by integrating
x86-specific intrinsics to reduce CPU cycle usage.

Key improvements include:
 - Replacing standard operations with x86 intrinsics for
   improved efficiency.
 - Refactoring the main loop to utilize SIMD instructions.

The computed hash values were manually verified using a
standalone test bench, both with and without the optimizations.
No differences in hash output were observed.
With the optimizations enabled, a ~50% reduction in CPU time
for SHA1 computation was achieved.
---
 src/util/sha1/sha1.c | 222 ++++++++++++++++++++++++++++++++++++++++++-
 1 file changed, 220 insertions(+), 2 deletions(-)

diff --git a/src/util/sha1/sha1.c b/src/util/sha1/sha1.c
index ed043fe39de9c..5a37b4d35ee8f 100644
--- a/src/util/sha1/sha1.c
+++ b/src/util/sha1/sha1.c
@@ -16,6 +16,11 @@
 
 #include <stdint.h>
 #include <string.h>
+#include "util/detect_arch.h"
+#if DETECT_ARCH_X86 || DETECT_ARCH_X86_64
+#include <immintrin.h>
+#endif
+#include "util/u_cpu_detect.h"
 #include "u_endian.h"
 #include "macros.h"
 #include "sha1.h"
@@ -101,6 +106,194 @@ SHA1Transform(uint32_t state[5], const uint8_t buffer[SHA1_BLOCK_LENGTH])
 	a = b = c = d = e = 0;
 }
 
+#if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && \
+    (__SHA__) && \
+    (UTIL_ARCH_LITTLE_ENDIAN)
+static void
+SHA1Transform_x86(uint32_t state[5], const uint8_t data[])
+{
+   __m128i abcd, abcd_save, e0, e0_save, e1;
+   __m128i msg0, msg1, msg2, msg3;
+   const __m128i MASK = _mm_set_epi64x(0x0001020304050607ULL,
+				       0x08090a0b0c0d0e0fULL);
+
+   /* Load initial values */
+   abcd = _mm_loadu_si128((const __m128i*) state);
+   e0 = _mm_set_epi32(state[4], 0, 0, 0);
+
+   abcd = _mm_shuffle_epi32(abcd, 0x1B);
+
+   /* Save current state  */
+   abcd_save = abcd;
+   e0_save = e0;
+
+   /* Rounds 0-3 */
+   msg0 = _mm_loadu_si128((const __m128i*)(data + 0));
+   msg0 = _mm_shuffle_epi8(msg0, MASK);
+
+   e0 = _mm_add_epi32(e0, msg0);
+   e1 = abcd;
+   abcd = _mm_sha1rnds4_epu32(abcd, e0, 0);
+
+   /* Rounds 4-7 */
+   msg1 = _mm_loadu_si128((const __m128i*)(data + 16));
+   msg1 = _mm_shuffle_epi8(msg1, MASK);
+   e1 = _mm_sha1nexte_epu32(e1, msg1);
+   e0 = abcd;
+   abcd = _mm_sha1rnds4_epu32(abcd, e1, 0);
+   msg0 = _mm_sha1msg1_epu32(msg0, msg1);
+
+   /* Rounds 8-11 */
+   msg2 = _mm_loadu_si128((const __m128i*)(data + 32));
+   msg2 = _mm_shuffle_epi8(msg2, MASK);
+   e0 = _mm_sha1nexte_epu32(e0, msg2);
+   e1 = abcd;
+   abcd = _mm_sha1rnds4_epu32(abcd, e0, 0);
+   msg1 = _mm_sha1msg1_epu32(msg1, msg2);
+   msg0 = _mm_xor_si128(msg0, msg2);
+
+   /* Rounds 12-15 */
+   msg3 = _mm_loadu_si128((const __m128i*)(data + 48));
+   msg3 = _mm_shuffle_epi8(msg3, MASK);
+   e1 = _mm_sha1nexte_epu32(e1, msg3);
+   e0 = abcd;
+   msg0 = _mm_sha1msg2_epu32(msg0, msg3);
+   abcd = _mm_sha1rnds4_epu32(abcd, e1, 0);
+   msg2 = _mm_sha1msg1_epu32(msg2, msg3);
+   msg1 = _mm_xor_si128(msg1, msg3);
+
+   /* Rounds 16-19 */
+   e0 = _mm_sha1nexte_epu32(e0, msg0);
+   e1 = abcd;
+   msg1 = _mm_sha1msg2_epu32(msg1, msg0);
+   abcd = _mm_sha1rnds4_epu32(abcd, e0, 0);
+   msg3 = _mm_sha1msg1_epu32(msg3, msg0);
+   msg2 = _mm_xor_si128(msg2, msg0);
+
+   /* Rounds 20-23 */
+   e1 = _mm_sha1nexte_epu32(e1, msg1);
+   e0 = abcd;
+   msg2 = _mm_sha1msg2_epu32(msg2, msg1);
+   abcd = _mm_sha1rnds4_epu32(abcd, e1, 1);
+   msg0 = _mm_sha1msg1_epu32(msg0, msg1);
+   msg3 = _mm_xor_si128(msg3, msg1);
+
+   /* Rounds 24-27 */
+   e0 = _mm_sha1nexte_epu32(e0, msg2);
+   e1 = abcd;
+   msg3 = _mm_sha1msg2_epu32(msg3, msg2);
+   abcd = _mm_sha1rnds4_epu32(abcd, e0, 1);
+   msg1 = _mm_sha1msg1_epu32(msg1, msg2);
+   msg0 = _mm_xor_si128(msg0, msg2);
+
+   /* Rounds 28-31 */
+   e1 = _mm_sha1nexte_epu32(e1, msg3);
+   e0 = abcd;
+   msg0 = _mm_sha1msg2_epu32(msg0, msg3);
+   abcd = _mm_sha1rnds4_epu32(abcd, e1, 1);
+   msg2 = _mm_sha1msg1_epu32(msg2, msg3);
+   msg1 = _mm_xor_si128(msg1, msg3);
+
+   /* Rounds 32-35 */
+   e0 = _mm_sha1nexte_epu32(e0, msg0);
+   e1 = abcd;
+   msg1 = _mm_sha1msg2_epu32(msg1, msg0);
+   abcd = _mm_sha1rnds4_epu32(abcd, e0, 1);
+   msg3 = _mm_sha1msg1_epu32(msg3, msg0);
+   msg2 = _mm_xor_si128(msg2, msg0);
+
+   /* Rounds 36-39 */
+   e1 = _mm_sha1nexte_epu32(e1, msg1);
+   e0 = abcd;
+   msg2 = _mm_sha1msg2_epu32(msg2, msg1);
+   abcd = _mm_sha1rnds4_epu32(abcd, e1, 1);
+   msg0 = _mm_sha1msg1_epu32(msg0, msg1);
+   msg3 = _mm_xor_si128(msg3, msg1);
+
+   /* Rounds 40-43 */
+   e0 = _mm_sha1nexte_epu32(e0, msg2);
+   e1 = abcd;
+   msg3 = _mm_sha1msg2_epu32(msg3, msg2);
+   abcd = _mm_sha1rnds4_epu32(abcd, e0, 2);
+   msg1 = _mm_sha1msg1_epu32(msg1, msg2);
+   msg0 = _mm_xor_si128(msg0, msg2);
+
+   /* Rounds 44-47 */
+   e1 = _mm_sha1nexte_epu32(e1, msg3);
+   e0 = abcd;
+   msg0 = _mm_sha1msg2_epu32(msg0, msg3);
+   abcd = _mm_sha1rnds4_epu32(abcd, e1, 2);
+   msg2 = _mm_sha1msg1_epu32(msg2, msg3);
+   msg1 = _mm_xor_si128(msg1, msg3);
+
+   /* Rounds 48-51 */
+   e0 = _mm_sha1nexte_epu32(e0, msg0);
+   e1 = abcd;
+   msg1 = _mm_sha1msg2_epu32(msg1, msg0);
+   abcd = _mm_sha1rnds4_epu32(abcd, e0, 2);
+   msg3 = _mm_sha1msg1_epu32(msg3, msg0);
+   msg2 = _mm_xor_si128(msg2, msg0);
+
+   /* Rounds 52-55 */
+   e1 = _mm_sha1nexte_epu32(e1, msg1);
+   e0 = abcd;
+   msg2 = _mm_sha1msg2_epu32(msg2, msg1);
+   abcd = _mm_sha1rnds4_epu32(abcd, e1, 2);
+   msg0 = _mm_sha1msg1_epu32(msg0, msg1);
+   msg3 = _mm_xor_si128(msg3, msg1);
+
+   /* Rounds 56-59 */
+   e0 = _mm_sha1nexte_epu32(e0, msg2);
+   e1 = abcd;
+   msg3 = _mm_sha1msg2_epu32(msg3, msg2);
+   abcd = _mm_sha1rnds4_epu32(abcd, e0, 2);
+   msg1 = _mm_sha1msg1_epu32(msg1, msg2);
+   msg0 = _mm_xor_si128(msg0, msg2);
+
+   /* Rounds 60-63 */
+   e1 = _mm_sha1nexte_epu32(e1, msg3);
+   e0 = abcd;
+   msg0 = _mm_sha1msg2_epu32(msg0, msg3);
+   abcd = _mm_sha1rnds4_epu32(abcd, e1, 3);
+   msg2 = _mm_sha1msg1_epu32(msg2, msg3);
+   msg1 = _mm_xor_si128(msg1, msg3);
+
+   /* Rounds 64-67 */
+   e0 = _mm_sha1nexte_epu32(e0, msg0);
+   e1 = abcd;
+   msg1 = _mm_sha1msg2_epu32(msg1, msg0);
+   abcd = _mm_sha1rnds4_epu32(abcd, e0, 3);
+   msg3 = _mm_sha1msg1_epu32(msg3, msg0);
+   msg2 = _mm_xor_si128(msg2, msg0);
+
+   /* Rounds 68-71 */
+   e1 = _mm_sha1nexte_epu32(e1, msg1);
+   e0 = abcd;
+   msg2 = _mm_sha1msg2_epu32(msg2, msg1);
+   abcd = _mm_sha1rnds4_epu32(abcd, e1, 3);
+   msg3 = _mm_xor_si128(msg3, msg1);
+
+   /* Rounds 72-75 */
+   e0 = _mm_sha1nexte_epu32(e0, msg2);
+   e1 = abcd;
+   msg3 = _mm_sha1msg2_epu32(msg3, msg2);
+   abcd = _mm_sha1rnds4_epu32(abcd, e0, 3);
+
+   /* Rounds 76-79 */
+   e1 = _mm_sha1nexte_epu32(e1, msg3);
+   e0 = abcd;
+   abcd = _mm_sha1rnds4_epu32(abcd, e1, 3);
+
+   /* Combine state */
+   e0 = _mm_sha1nexte_epu32(e0, e0_save);
+   abcd = _mm_add_epi32(abcd, abcd_save);
+
+   /* Save state */
+   abcd = _mm_shuffle_epi32(abcd, 0x1B);
+   _mm_storeu_si128((__m128i*) state, abcd);
+   state[4] = _mm_extract_epi32(e0, 3);
+}
+#endif
 
 /*
  * SHA1Init - Initialize new context
@@ -126,15 +319,40 @@ void
 SHA1Update(SHA1_CTX *context, const uint8_t *data, size_t len)
 {
 	size_t i, j;
+#if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && \
+    (__SHA__) && \
+    (UTIL_ARCH_LITTLE_ENDIAN)
+	const struct util_cpu_caps_t *cpu_caps = util_get_cpu_caps();
+#endif
 
 	j = (size_t)((context->count >> 3) & 63);
 	context->count += (len << 3);
 	if ((j + len) > 63) {
 		(void)memcpy(&context->buffer[j], data, (i = 64-j));
-		SHA1Transform(context->state, context->buffer);
+#if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && \
+    (__SHA__) && \
+    (UTIL_ARCH_LITTLE_ENDIAN)
+		if (cpu_caps->has_sha_extn) {
+			SHA1Transform_x86(context->state,
+					context->buffer);
+		}
+		else
+#endif
+			SHA1Transform(context->state,
+					context->buffer);
 		for ( ; i + 63 < len; i += 64) {
 			assume(len >= 64);
-			SHA1Transform(context->state, (uint8_t *)&data[i]);
+#if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && \
+    (__SHA__) && \
+    (UTIL_ARCH_LITTLE_ENDIAN)
+			if (cpu_caps->has_sha_extn) {
+				SHA1Transform_x86(context->state,
+					  (uint8_t *)&data[i]);
+			}
+			else
+#endif
+				SHA1Transform(context->state,
+					(uint8_t *)&data[i]);
 		}
 		j = 0;
 	} else {
-- 
GitLab

--- a/src/util/sha1/sha1.c
+++ b/src/util/sha1/sha1.c
@@ -25,340 +25,303 @@
 #include "macros.h"
 #include "sha1.h"
 
-#define rol(value, bits) (((value) << (bits)) | ((value) >> (32 - (bits))))
+static inline uint32_t
+rotl32(uint32_t v, unsigned bits)
+{
+    #if defined(__has_builtin)
+    #  if __has_builtin(__builtin_rotateleft32)
+    return __builtin_rotateleft32(v, bits);
+    #  endif
+    #endif
+    return (v << bits) | (v >> (32u - bits));
+}
 
-/*
- * blk0() and blk() perform the initial expand.
- * I got the idea of expanding during the round function from SSLeay
- */
-#if UTIL_ARCH_LITTLE_ENDIAN
-# define blk0(i) (block->l[i] = (rol(block->l[i],24)&0xFF00FF00) \
-    |(rol(block->l[i],8)&0x00FF00FF))
-#else
-# define blk0(i) block->l[i]
-#endif
-#define blk(i) (block->l[i&15] = rol(block->l[(i+13)&15]^block->l[(i+8)&15] \
-    ^block->l[(i+2)&15]^block->l[i&15],1))
+/* Local macros for SHA1Transform */
+#define R0(v,w,x,y,z,i) z+=((w&(x^y))^y)+blk0(i)+0x5A827999+rotl32(v,5); w=rotl32(w,30);
+#define R1(v,w,x,y,z,i) z+=((w&(x^y))^y)+blk(i)+0x5A827999+rotl32(v,5);  w=rotl32(w,30);
+#define R2(v,w,x,y,z,i) z+=(w^x^y)+blk(i)+0x6ED9EBA1+rotl32(v,5);         w=rotl32(w,30);
+#define R3(v,w,x,y,z,i) z+=(((w|x)&y)|(w&x))+blk(i)+0x8F1BBCDC+rotl32(v,5); w=rotl32(w,30);
+#define R4(v,w,x,y,z,i) z+=(w^x^y)+blk(i)+0xCA62C1D6+rotl32(v,5);         w=rotl32(w,30);
 
-/*
- * (R0+R1), R2, R3, R4 are the different operations (rounds) used in SHA1
- */
-#define R0(v,w,x,y,z,i) z+=((w&(x^y))^y)+blk0(i)+0x5A827999+rol(v,5);w=rol(w,30);
-#define R1(v,w,x,y,z,i) z+=((w&(x^y))^y)+blk(i)+0x5A827999+rol(v,5);w=rol(w,30);
-#define R2(v,w,x,y,z,i) z+=(w^x^y)+blk(i)+0x6ED9EBA1+rol(v,5);w=rol(w,30);
-#define R3(v,w,x,y,z,i) z+=(((w|x)&y)|(w&x))+blk(i)+0x8F1BBCDC+rol(v,5);w=rol(w,30);
-#define R4(v,w,x,y,z,i) z+=(w^x^y)+blk(i)+0xCA62C1D6+rol(v,5);w=rol(w,30);
-
-typedef union {
-	uint8_t c[64];
-	uint32_t l[16];
-} CHAR64LONG16;
-
-/*
- * Hash a single 512-bit block. This is the core of the algorithm.
- */
-static void
+static inline void __attribute__((always_inline))
 SHA1Transform(uint32_t state[5], const uint8_t buffer[SHA1_BLOCK_LENGTH])
 {
-	uint32_t a, b, c, d, e;
-	uint8_t workspace[SHA1_BLOCK_LENGTH];
-	CHAR64LONG16 *block = (CHAR64LONG16 *)workspace;
-
-	(void)memcpy(block, buffer, SHA1_BLOCK_LENGTH);
-
-	/* Copy context->state[] to working vars */
-	a = state[0];
-	b = state[1];
-	c = state[2];
-	d = state[3];
-	e = state[4];
-
-	/* 4 rounds of 20 operations each. Loop unrolled. */
-	R0(a,b,c,d,e, 0); R0(e,a,b,c,d, 1); R0(d,e,a,b,c, 2); R0(c,d,e,a,b, 3);
-	R0(b,c,d,e,a, 4); R0(a,b,c,d,e, 5); R0(e,a,b,c,d, 6); R0(d,e,a,b,c, 7);
-	R0(c,d,e,a,b, 8); R0(b,c,d,e,a, 9); R0(a,b,c,d,e,10); R0(e,a,b,c,d,11);
-	R0(d,e,a,b,c,12); R0(c,d,e,a,b,13); R0(b,c,d,e,a,14); R0(a,b,c,d,e,15);
-	R1(e,a,b,c,d,16); R1(d,e,a,b,c,17); R1(c,d,e,a,b,18); R1(b,c,d,e,a,19);
-	R2(a,b,c,d,e,20); R2(e,a,b,c,d,21); R2(d,e,a,b,c,22); R2(c,d,e,a,b,23);
-	R2(b,c,d,e,a,24); R2(a,b,c,d,e,25); R2(e,a,b,c,d,26); R2(d,e,a,b,c,27);
-	R2(c,d,e,a,b,28); R2(b,c,d,e,a,29); R2(a,b,c,d,e,30); R2(e,a,b,c,d,31);
-	R2(d,e,a,b,c,32); R2(c,d,e,a,b,33); R2(b,c,d,e,a,34); R2(a,b,c,d,e,35);
-	R2(e,a,b,c,d,36); R2(d,e,a,b,c,37); R2(c,d,e,a,b,38); R2(b,c,d,e,a,39);
-	R3(a,b,c,d,e,40); R3(e,a,b,c,d,41); R3(d,e,a,b,c,42); R3(c,d,e,a,b,43);
-	R3(b,c,d,e,a,44); R3(a,b,c,d,e,45); R3(e,a,b,c,d,46); R3(d,e,a,b,c,47);
-	R3(c,d,e,a,b,48); R3(b,c,d,e,a,49); R3(a,b,c,d,e,50); R3(e,a,b,c,d,51);
-	R3(d,e,a,b,c,52); R3(c,d,e,a,b,53); R3(b,c,d,e,a,54); R3(a,b,c,d,e,55);
-	R3(e,a,b,c,d,56); R3(d,e,a,b,c,57); R3(c,d,e,a,b,58); R3(b,c,d,e,a,59);
-	R4(a,b,c,d,e,60); R4(e,a,b,c,d,61); R4(d,e,a,b,c,62); R4(c,d,e,a,b,63);
-	R4(b,c,d,e,a,64); R4(a,b,c,d,e,65); R4(e,a,b,c,d,66); R4(d,e,a,b,c,67);
-	R4(c,d,e,a,b,68); R4(b,c,d,e,a,69); R4(a,b,c,d,e,70); R4(e,a,b,c,d,71);
-	R4(d,e,a,b,c,72); R4(c,d,e,a,b,73); R4(b,c,d,e,a,74); R4(a,b,c,d,e,75);
-	R4(e,a,b,c,d,76); R4(d,e,a,b,c,77); R4(c,d,e,a,b,78); R4(b,c,d,e,a,79);
-
-	/* Add the working vars back into context.state[] */
-	state[0] += a;
-	state[1] += b;
-	state[2] += c;
-	state[3] += d;
-	state[4] += e;
+    uint32_t a, b, c, d, e;
+    uint32_t block[16];
 
-	/* Wipe variables */
-	a = b = c = d = e = 0;
+    /* Copy input to local buffer and convert endianness if needed */
+    #if UTIL_ARCH_LITTLE_ENDIAN
+    for (int i = 0; i < 16; i++) {
+        block[i] = ((uint32_t)buffer[i*4] << 24) |
+        ((uint32_t)buffer[i*4+1] << 16) |
+        ((uint32_t)buffer[i*4+2] << 8) |
+        (uint32_t)buffer[i*4+3];
+    }
+    #else
+    memcpy(block, buffer, SHA1_BLOCK_LENGTH);
+    #endif
+
+    /* Local helper macros */
+    #define blk0(i) block[i]
+    #define blk(i) (block[i & 15] = rotl32(block[(i+13)&15] ^ block[(i+8)&15] ^ block[(i+2)&15] ^ block[i&15], 1))
+
+    a = state[0]; b = state[1]; c = state[2]; d = state[3]; e = state[4];
+
+    R0(a,b,c,d,e, 0); R0(e,a,b,c,d, 1); R0(d,e,a,b,c, 2); R0(c,d,e,a,b, 3);
+    R0(b,c,d,e,a, 4); R0(a,b,c,d,e, 5); R0(e,a,b,c,d, 6); R0(d,e,a,b,c, 7);
+    R0(c,d,e,a,b, 8); R0(b,c,d,e,a, 9); R0(a,b,c,d,e,10); R0(e,a,b,c,d,11);
+    R0(d,e,a,b,c,12); R0(c,d,e,a,b,13); R0(b,c,d,e,a,14); R0(a,b,c,d,e,15);
+    R1(e,a,b,c,d,16); R1(d,e,a,b,c,17); R1(c,d,e,a,b,18); R1(b,c,d,e,a,19);
+    R2(a,b,c,d,e,20); R2(e,a,b,c,d,21); R2(d,e,a,b,c,22); R2(c,d,e,a,b,23);
+    R2(b,c,d,e,a,24); R2(a,b,c,d,e,25); R2(e,a,b,c,d,26); R2(d,e,a,b,c,27);
+    R2(c,d,e,a,b,28); R2(b,c,d,e,a,29); R2(a,b,c,d,e,30); R2(e,a,b,c,d,31);
+    R2(d,e,a,b,c,32); R2(c,d,e,a,b,33); R2(b,c,d,e,a,34); R2(a,b,c,d,e,35);
+    R2(e,a,b,c,d,36); R2(d,e,a,b,c,37); R2(c,d,e,a,b,38); R2(b,c,d,e,a,39);
+    R3(a,b,c,d,e,40); R3(e,a,b,c,d,41); R3(d,e,a,b,c,42); R3(c,d,e,a,b,43);
+    R3(b,c,d,e,a,44); R3(a,b,c,d,e,45); R3(e,a,b,c,d,46); R3(d,e,a,b,c,47);
+    R3(c,d,e,a,b,48); R3(b,c,d,e,a,49); R3(a,b,c,d,e,50); R3(e,a,b,c,d,51);
+    R3(d,e,a,b,c,52); R3(c,d,e,a,b,53); R3(b,c,d,e,a,54); R3(a,b,c,d,e,55);
+    R3(e,a,b,c,d,56); R3(d,e,a,b,c,57); R3(c,d,e,a,b,58); R3(b,c,d,e,a,59);
+    R4(a,b,c,d,e,60); R4(e,a,b,c,d,61); R4(d,e,a,b,c,62); R4(c,d,e,a,b,63);
+    R4(b,c,d,e,a,64); R4(a,b,c,d,e,65); R4(e,a,b,c,d,66); R4(d,e,a,b,c,67);
+    R4(c,d,e,a,b,68); R4(b,c,d,e,a,69); R4(a,b,c,d,e,70); R4(e,a,b,c,d,71);
+    R4(d,e,a,b,c,72); R4(c,d,e,a,b,73); R4(b,c,d,e,a,74); R4(a,b,c,d,e,75);
+    R4(e,a,b,c,d,76); R4(d,e,a,b,c,77); R4(c,d,e,a,b,78); R4(b,c,d,e,a,79);
+
+    state[0] += a; state[1] += b; state[2] += c; state[3] += d; state[4] += e;
+
+    /* Clean up local macros */
+    #undef blk0
+    #undef blk
 }
 
 #if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && \
-    (__SHA__) && \
-    (UTIL_ARCH_LITTLE_ENDIAN)
+defined(__SHA__) && \
+(UTIL_ARCH_LITTLE_ENDIAN)
 static void
 SHA1Transform_x86(uint32_t state[5], const uint8_t data[])
 {
-   __m128i abcd, abcd_save, e0, e0_save, e1;
-   __m128i msg0, msg1, msg2, msg3;
-   const __m128i MASK = _mm_set_epi64x(0x0001020304050607ULL,
-				       0x08090a0b0c0d0e0fULL);
-
-   /* Load initial values */
-   abcd = _mm_loadu_si128((const __m128i*) state);
-   e0 = _mm_set_epi32(state[4], 0, 0, 0);
-
-   abcd = _mm_shuffle_epi32(abcd, 0x1B);
-
-   /* Save current state  */
-   abcd_save = abcd;
-   e0_save = e0;
-
-   /* Rounds 0-3 */
-   msg0 = _mm_loadu_si128((const __m128i*)(data + 0));
-   msg0 = _mm_shuffle_epi8(msg0, MASK);
-
-   e0 = _mm_add_epi32(e0, msg0);
-   e1 = abcd;
-   abcd = _mm_sha1rnds4_epu32(abcd, e0, 0);
-
-   /* Rounds 4-7 */
-   msg1 = _mm_loadu_si128((const __m128i*)(data + 16));
-   msg1 = _mm_shuffle_epi8(msg1, MASK);
-   e1 = _mm_sha1nexte_epu32(e1, msg1);
-   e0 = abcd;
-   abcd = _mm_sha1rnds4_epu32(abcd, e1, 0);
-   msg0 = _mm_sha1msg1_epu32(msg0, msg1);
-
-   /* Rounds 8-11 */
-   msg2 = _mm_loadu_si128((const __m128i*)(data + 32));
-   msg2 = _mm_shuffle_epi8(msg2, MASK);
-   e0 = _mm_sha1nexte_epu32(e0, msg2);
-   e1 = abcd;
-   abcd = _mm_sha1rnds4_epu32(abcd, e0, 0);
-   msg1 = _mm_sha1msg1_epu32(msg1, msg2);
-   msg0 = _mm_xor_si128(msg0, msg2);
-
-   /* Rounds 12-15 */
-   msg3 = _mm_loadu_si128((const __m128i*)(data + 48));
-   msg3 = _mm_shuffle_epi8(msg3, MASK);
-   e1 = _mm_sha1nexte_epu32(e1, msg3);
-   e0 = abcd;
-   msg0 = _mm_sha1msg2_epu32(msg0, msg3);
-   abcd = _mm_sha1rnds4_epu32(abcd, e1, 0);
-   msg2 = _mm_sha1msg1_epu32(msg2, msg3);
-   msg1 = _mm_xor_si128(msg1, msg3);
-
-   /* Rounds 16-19 */
-   e0 = _mm_sha1nexte_epu32(e0, msg0);
-   e1 = abcd;
-   msg1 = _mm_sha1msg2_epu32(msg1, msg0);
-   abcd = _mm_sha1rnds4_epu32(abcd, e0, 0);
-   msg3 = _mm_sha1msg1_epu32(msg3, msg0);
-   msg2 = _mm_xor_si128(msg2, msg0);
-
-   /* Rounds 20-23 */
-   e1 = _mm_sha1nexte_epu32(e1, msg1);
-   e0 = abcd;
-   msg2 = _mm_sha1msg2_epu32(msg2, msg1);
-   abcd = _mm_sha1rnds4_epu32(abcd, e1, 1);
-   msg0 = _mm_sha1msg1_epu32(msg0, msg1);
-   msg3 = _mm_xor_si128(msg3, msg1);
-
-   /* Rounds 24-27 */
-   e0 = _mm_sha1nexte_epu32(e0, msg2);
-   e1 = abcd;
-   msg3 = _mm_sha1msg2_epu32(msg3, msg2);
-   abcd = _mm_sha1rnds4_epu32(abcd, e0, 1);
-   msg1 = _mm_sha1msg1_epu32(msg1, msg2);
-   msg0 = _mm_xor_si128(msg0, msg2);
-
-   /* Rounds 28-31 */
-   e1 = _mm_sha1nexte_epu32(e1, msg3);
-   e0 = abcd;
-   msg0 = _mm_sha1msg2_epu32(msg0, msg3);
-   abcd = _mm_sha1rnds4_epu32(abcd, e1, 1);
-   msg2 = _mm_sha1msg1_epu32(msg2, msg3);
-   msg1 = _mm_xor_si128(msg1, msg3);
-
-   /* Rounds 32-35 */
-   e0 = _mm_sha1nexte_epu32(e0, msg0);
-   e1 = abcd;
-   msg1 = _mm_sha1msg2_epu32(msg1, msg0);
-   abcd = _mm_sha1rnds4_epu32(abcd, e0, 1);
-   msg3 = _mm_sha1msg1_epu32(msg3, msg0);
-   msg2 = _mm_xor_si128(msg2, msg0);
-
-   /* Rounds 36-39 */
-   e1 = _mm_sha1nexte_epu32(e1, msg1);
-   e0 = abcd;
-   msg2 = _mm_sha1msg2_epu32(msg2, msg1);
-   abcd = _mm_sha1rnds4_epu32(abcd, e1, 1);
-   msg0 = _mm_sha1msg1_epu32(msg0, msg1);
-   msg3 = _mm_xor_si128(msg3, msg1);
-
-   /* Rounds 40-43 */
-   e0 = _mm_sha1nexte_epu32(e0, msg2);
-   e1 = abcd;
-   msg3 = _mm_sha1msg2_epu32(msg3, msg2);
-   abcd = _mm_sha1rnds4_epu32(abcd, e0, 2);
-   msg1 = _mm_sha1msg1_epu32(msg1, msg2);
-   msg0 = _mm_xor_si128(msg0, msg2);
-
-   /* Rounds 44-47 */
-   e1 = _mm_sha1nexte_epu32(e1, msg3);
-   e0 = abcd;
-   msg0 = _mm_sha1msg2_epu32(msg0, msg3);
-   abcd = _mm_sha1rnds4_epu32(abcd, e1, 2);
-   msg2 = _mm_sha1msg1_epu32(msg2, msg3);
-   msg1 = _mm_xor_si128(msg1, msg3);
-
-   /* Rounds 48-51 */
-   e0 = _mm_sha1nexte_epu32(e0, msg0);
-   e1 = abcd;
-   msg1 = _mm_sha1msg2_epu32(msg1, msg0);
-   abcd = _mm_sha1rnds4_epu32(abcd, e0, 2);
-   msg3 = _mm_sha1msg1_epu32(msg3, msg0);
-   msg2 = _mm_xor_si128(msg2, msg0);
-
-   /* Rounds 52-55 */
-   e1 = _mm_sha1nexte_epu32(e1, msg1);
-   e0 = abcd;
-   msg2 = _mm_sha1msg2_epu32(msg2, msg1);
-   abcd = _mm_sha1rnds4_epu32(abcd, e1, 2);
-   msg0 = _mm_sha1msg1_epu32(msg0, msg1);
-   msg3 = _mm_xor_si128(msg3, msg1);
-
-   /* Rounds 56-59 */
-   e0 = _mm_sha1nexte_epu32(e0, msg2);
-   e1 = abcd;
-   msg3 = _mm_sha1msg2_epu32(msg3, msg2);
-   abcd = _mm_sha1rnds4_epu32(abcd, e0, 2);
-   msg1 = _mm_sha1msg1_epu32(msg1, msg2);
-   msg0 = _mm_xor_si128(msg0, msg2);
-
-   /* Rounds 60-63 */
-   e1 = _mm_sha1nexte_epu32(e1, msg3);
-   e0 = abcd;
-   msg0 = _mm_sha1msg2_epu32(msg0, msg3);
-   abcd = _mm_sha1rnds4_epu32(abcd, e1, 3);
-   msg2 = _mm_sha1msg1_epu32(msg2, msg3);
-   msg1 = _mm_xor_si128(msg1, msg3);
-
-   /* Rounds 64-67 */
-   e0 = _mm_sha1nexte_epu32(e0, msg0);
-   e1 = abcd;
-   msg1 = _mm_sha1msg2_epu32(msg1, msg0);
-   abcd = _mm_sha1rnds4_epu32(abcd, e0, 3);
-   msg3 = _mm_sha1msg1_epu32(msg3, msg0);
-   msg2 = _mm_xor_si128(msg2, msg0);
-
-   /* Rounds 68-71 */
-   e1 = _mm_sha1nexte_epu32(e1, msg1);
-   e0 = abcd;
-   msg2 = _mm_sha1msg2_epu32(msg2, msg1);
-   abcd = _mm_sha1rnds4_epu32(abcd, e1, 3);
-   msg3 = _mm_xor_si128(msg3, msg1);
-
-   /* Rounds 72-75 */
-   e0 = _mm_sha1nexte_epu32(e0, msg2);
-   e1 = abcd;
-   msg3 = _mm_sha1msg2_epu32(msg3, msg2);
-   abcd = _mm_sha1rnds4_epu32(abcd, e0, 3);
-
-   /* Rounds 76-79 */
-   e1 = _mm_sha1nexte_epu32(e1, msg3);
-   e0 = abcd;
-   abcd = _mm_sha1rnds4_epu32(abcd, e1, 3);
-
-   /* Combine state */
-   e0 = _mm_sha1nexte_epu32(e0, e0_save);
-   abcd = _mm_add_epi32(abcd, abcd_save);
-
-   /* Save state */
-   abcd = _mm_shuffle_epi32(abcd, 0x1B);
-   _mm_storeu_si128((__m128i*) state, abcd);
-   state[4] = _mm_extract_epi32(e0, 3);
+    __m128i abcd, abcd_save, e0, e0_save, e1;
+    __m128i msg0, msg1, msg2, msg3;
+    const __m128i MASK = _mm_set_epi64x(0x0001020304050607ULL,
+                                        0x08090a0b0c0d0e0fULL);
+
+    /* Load initial values */
+    abcd = _mm_loadu_si128((const __m128i*) state);
+    e0 = _mm_set_epi32(state[4], 0, 0, 0);
+
+    abcd = _mm_shuffle_epi32(abcd, 0x1B);
+
+    /* Save current state  */
+    abcd_save = abcd;
+    e0_save = e0;
+
+    /* Rounds 0-3 */
+    msg0 = _mm_loadu_si128((const __m128i*)(data + 0));
+    msg0 = _mm_shuffle_epi8(msg0, MASK);
+
+    e0 = _mm_add_epi32(e0, msg0);
+    e1 = abcd;
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 0);
+
+    /* Rounds 4-7 */
+    msg1 = _mm_loadu_si128((const __m128i*)(data + 16));
+    msg1 = _mm_shuffle_epi8(msg1, MASK);
+    e1 = _mm_sha1nexte_epu32(e1, msg1);
+    e0 = abcd;
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 0);
+    msg0 = _mm_sha1msg1_epu32(msg0, msg1);
+
+    /* Rounds 8-11 */
+    msg2 = _mm_loadu_si128((const __m128i*)(data + 32));
+    msg2 = _mm_shuffle_epi8(msg2, MASK);
+    e0 = _mm_sha1nexte_epu32(e0, msg2);
+    e1 = abcd;
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 0);
+    msg1 = _mm_sha1msg1_epu32(msg1, msg2);
+    msg0 = _mm_xor_si128(msg0, msg2);
+
+    /* Rounds 12-15 */
+    msg3 = _mm_loadu_si128((const __m128i*)(data + 48));
+    msg3 = _mm_shuffle_epi8(msg3, MASK);
+    e1 = _mm_sha1nexte_epu32(e1, msg3);
+    e0 = abcd;
+    msg0 = _mm_sha1msg2_epu32(msg0, msg3);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 0);
+    msg2 = _mm_sha1msg1_epu32(msg2, msg3);
+    msg1 = _mm_xor_si128(msg1, msg3);
+
+    /* Rounds 16-19 */
+    e0 = _mm_sha1nexte_epu32(e0, msg0);
+    e1 = abcd;
+    msg1 = _mm_sha1msg2_epu32(msg1, msg0);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 0);
+    msg3 = _mm_sha1msg1_epu32(msg3, msg0);
+    msg2 = _mm_xor_si128(msg2, msg0);
+
+    /* Rounds 20-23 */
+    e1 = _mm_sha1nexte_epu32(e1, msg1);
+    e0 = abcd;
+    msg2 = _mm_sha1msg2_epu32(msg2, msg1);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 1);
+    msg0 = _mm_sha1msg1_epu32(msg0, msg1);
+    msg3 = _mm_xor_si128(msg3, msg1);
+
+    /* Rounds 24-27 */
+    e0 = _mm_sha1nexte_epu32(e0, msg2);
+    e1 = abcd;
+    msg3 = _mm_sha1msg2_epu32(msg3, msg2);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 1);
+    msg1 = _mm_sha1msg1_epu32(msg1, msg2);
+    msg0 = _mm_xor_si128(msg0, msg2);
+
+    /* Rounds 28-31 */
+    e1 = _mm_sha1nexte_epu32(e1, msg3);
+    e0 = abcd;
+    msg0 = _mm_sha1msg2_epu32(msg0, msg3);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 1);
+    msg2 = _mm_sha1msg1_epu32(msg2, msg3);
+    msg1 = _mm_xor_si128(msg1, msg3);
+
+    /* Rounds 32-35 */
+    e0 = _mm_sha1nexte_epu32(e0, msg0);
+    e1 = abcd;
+    msg1 = _mm_sha1msg2_epu32(msg1, msg0);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 1);
+    msg3 = _mm_sha1msg1_epu32(msg3, msg0);
+    msg2 = _mm_xor_si128(msg2, msg0);
+
+    /* Rounds 36-39 */
+    e1 = _mm_sha1nexte_epu32(e1, msg1);
+    e0 = abcd;
+    msg2 = _mm_sha1msg2_epu32(msg2, msg1);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 1);
+    msg0 = _mm_sha1msg1_epu32(msg0, msg1);
+    msg3 = _mm_xor_si128(msg3, msg1);
+
+    /* Rounds 40-43 */
+    e0 = _mm_sha1nexte_epu32(e0, msg2);
+    e1 = abcd;
+    msg3 = _mm_sha1msg2_epu32(msg3, msg2);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 2);
+    msg1 = _mm_sha1msg1_epu32(msg1, msg2);
+    msg0 = _mm_xor_si128(msg0, msg2);
+
+    /* Rounds 44-47 */
+    e1 = _mm_sha1nexte_epu32(e1, msg3);
+    e0 = abcd;
+    msg0 = _mm_sha1msg2_epu32(msg0, msg3);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 2);
+    msg2 = _mm_sha1msg1_epu32(msg2, msg3);
+    msg1 = _mm_xor_si128(msg1, msg3);
+
+    /* Rounds 48-51 */
+    e0 = _mm_sha1nexte_epu32(e0, msg0);
+    e1 = abcd;
+    msg1 = _mm_sha1msg2_epu32(msg1, msg0);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 2);
+    msg3 = _mm_sha1msg1_epu32(msg3, msg0);
+    msg2 = _mm_xor_si128(msg2, msg0);
+
+    /* Rounds 52-55 */
+    e1 = _mm_sha1nexte_epu32(e1, msg1);
+    e0 = abcd;
+    msg2 = _mm_sha1msg2_epu32(msg2, msg1);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 2);
+    msg0 = _mm_sha1msg1_epu32(msg0, msg1);
+    msg3 = _mm_xor_si128(msg3, msg1);
+
+    /* Rounds 56-59 */
+    e0 = _mm_sha1nexte_epu32(e0, msg2);
+    e1 = abcd;
+    msg3 = _mm_sha1msg2_epu32(msg3, msg2);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 2);
+    msg1 = _mm_sha1msg1_epu32(msg1, msg2);
+    msg0 = _mm_xor_si128(msg0, msg2);
+
+    /* Rounds 60-63 */
+    e1 = _mm_sha1nexte_epu32(e1, msg3);
+    e0 = abcd;
+    msg0 = _mm_sha1msg2_epu32(msg0, msg3);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 3);
+    msg2 = _mm_sha1msg1_epu32(msg2, msg3);
+    msg1 = _mm_xor_si128(msg1, msg3);
+
+    /* Rounds 64-67 */
+    e0 = _mm_sha1nexte_epu32(e0, msg0);
+    e1 = abcd;
+    msg1 = _mm_sha1msg2_epu32(msg1, msg0);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 3);
+    msg3 = _mm_sha1msg1_epu32(msg3, msg0);
+    msg2 = _mm_xor_si128(msg2, msg0);
+
+    /* Rounds 68-71 */
+    e1 = _mm_sha1nexte_epu32(e1, msg1);
+    e0 = abcd;
+    msg2 = _mm_sha1msg2_epu32(msg2, msg1);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 3);
+    msg3 = _mm_xor_si128(msg3, msg1);
+
+    /* Rounds 72-75 */
+    e0 = _mm_sha1nexte_epu32(e0, msg2);
+    e1 = abcd;
+    msg3 = _mm_sha1msg2_epu32(msg3, msg2);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 3);
+
+    /* Rounds 76-79 */
+    e1 = _mm_sha1nexte_epu32(e1, msg3);
+    e0 = abcd;
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 3);
+
+    /* Combine state */
+    e0 = _mm_sha1nexte_epu32(e0, e0_save);
+    abcd = _mm_add_epi32(abcd, abcd_save);
+
+    /* Save state */
+    abcd = _mm_shuffle_epi32(abcd, 0x1B);
+    _mm_storeu_si128((__m128i*) state, abcd);
+    state[4] = _mm_extract_epi32(e0, 3);
 }
 #endif
 
-/*
- * SHA1Init - Initialize new context
- */
-void
-SHA1Init(SHA1_CTX *context)
+void SHA1Init(SHA1_CTX *ctx)
 {
-
-	/* SHA1 initialization constants */
-	context->count = 0;
-	context->state[0] = 0x67452301;
-	context->state[1] = 0xEFCDAB89;
-	context->state[2] = 0x98BADCFE;
-	context->state[3] = 0x10325476;
-	context->state[4] = 0xC3D2E1F0;
+    ctx->count = 0;
+    ctx->state[0]=0x67452301; ctx->state[1]=0xEFCDAB89;
+    ctx->state[2]=0x98BADCFE; ctx->state[3]=0x10325476;
+    ctx->state[4]=0xC3D2E1F0;
 }
 
-
-/*
- * Run your data through this.
- */
-void
-SHA1Update(SHA1_CTX *context, const uint8_t *data, size_t len)
+void SHA1Update(SHA1_CTX *ctx, const uint8_t *data, size_t len)
 {
-	size_t i, j;
-#if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && \
-    (__SHA__) && \
-    (UTIL_ARCH_LITTLE_ENDIAN)
-	const struct util_cpu_caps_t *cpu_caps = util_get_cpu_caps();
-#endif
-
-	j = (size_t)((context->count >> 3) & 63);
-	context->count += (len << 3);
-	if ((j + len) > 63) {
-		(void)memcpy(&context->buffer[j], data, (i = 64-j));
-#if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && \
-    (__SHA__) && \
-    (UTIL_ARCH_LITTLE_ENDIAN)
-		if (cpu_caps->has_sha_extn) {
-			SHA1Transform_x86(context->state,
-					context->buffer);
-		}
-		else
-#endif
-			SHA1Transform(context->state,
-					context->buffer);
-		for ( ; i + 63 < len; i += 64) {
-			assume(len >= 64);
-#if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && \
-    (__SHA__) && \
-    (UTIL_ARCH_LITTLE_ENDIAN)
-			if (cpu_caps->has_sha_extn) {
-				SHA1Transform_x86(context->state,
-					  (uint8_t *)&data[i]);
-			}
-			else
-#endif
-				SHA1Transform(context->state,
-					(uint8_t *)&data[i]);
-		}
-		j = 0;
-	} else {
-		i = 0;
-	}
-	(void)memcpy(&context->buffer[j], &data[i], len - i);
+    #if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && defined(__SHA__) && (UTIL_ARCH_LITTLE_ENDIAN)
+    const struct util_cpu_caps_t *cpu_caps = util_get_cpu_caps();
+    #endif
+    size_t j = (size_t)((ctx->count >> 3) & 63);
+    ctx->count += (uint64_t)len << 3;
+
+    size_t i = 0;
+    if (__builtin_expect(j + len > 63, 0)) {
+        memcpy(&ctx->buffer[j], data, (i = 64 - j));
+        if (
+            #if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && defined(__SHA__) && (UTIL_ARCH_LITTLE_ENDIAN)
+            cpu_caps->has_sha_extn ?
+            (SHA1Transform_x86(ctx->state, ctx->buffer),0) :
+            #endif
+            (SHA1Transform(ctx->state, ctx->buffer),0)
+        ) { }
+        /* len >= i is guaranteed because we're in the branch where j+len>63 */
+        for (; i + 63 < len; i += 64) {
+            #if (DETECT_ARCH_X86 || DETECT_ARCH_X86_64) && defined(__SHA__) && (UTIL_ARCH_LITTLE_ENDIAN)
+            if (__builtin_expect(cpu_caps->has_sha_extn,1))
+                SHA1Transform_x86(ctx->state, data + i);
+            else
+                #endif
+                SHA1Transform(ctx->state, data + i);
+        }
+        j = 0;
+    }
+    memcpy(&ctx->buffer[j], data + i, len - i);
 }
 
 
@@ -368,28 +331,28 @@ SHA1Update(SHA1_CTX *context, const uint
 static void
 SHA1Pad(SHA1_CTX *context)
 {
-	uint8_t finalcount[8];
-	uint32_t i;
+    uint8_t finalcount[8];
+    uint32_t i;
 
-	for (i = 0; i < 8; i++) {
-		finalcount[i] = (uint8_t)((context->count >>
-		    ((7 - (i & 7)) * 8)) & 255);	/* Endian independent */
-	}
-	SHA1Update(context, (uint8_t *)"\200", 1);
-	while ((context->count & 504) != 448)
-		SHA1Update(context, (uint8_t *)"\0", 1);
-	SHA1Update(context, finalcount, 8); /* Should cause a SHA1Transform() */
+    for (i = 0; i < 8; i++) {
+        finalcount[i] = (uint8_t)((context->count >>
+        ((7 - (i & 7)) * 8)) & 255);	/* Endian independent */
+    }
+    SHA1Update(context, (uint8_t *)"\200", 1);
+    while ((context->count & 504) != 448)
+        SHA1Update(context, (uint8_t *)"\0", 1);
+    SHA1Update(context, finalcount, 8); /* Should cause a SHA1Transform() */
 }
 
 void
 SHA1Final(uint8_t digest[SHA1_DIGEST_LENGTH], SHA1_CTX *context)
 {
-	uint32_t i;
+    uint32_t i;
 
-	SHA1Pad(context);
-	for (i = 0; i < SHA1_DIGEST_LENGTH; i++) {
-		digest[i] = (uint8_t)
-		   ((context->state[i>>2] >> ((3-(i & 3)) * 8) ) & 255);
-	}
-	memset(context, 0, sizeof(*context));
+    SHA1Pad(context);
+    for (i = 0; i < SHA1_DIGEST_LENGTH; i++) {
+        digest[i] = (uint8_t)
+        ((context->state[i>>2] >> ((3-(i & 3)) * 8) ) & 255);
+    }
+    memset(context, 0, sizeof(*context));
 }
