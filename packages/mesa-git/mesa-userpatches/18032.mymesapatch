diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index 1634c2db549312a86308a3de058812682b7253a2..464ab57f384867cccefdfa6960fe59c12d55015d 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -304,32 +304,34 @@ radv_destroy_cmd_buffer(struct vk_command_buffer *vk_cmd_buffer)
 {
    struct radv_cmd_buffer *cmd_buffer = container_of(vk_cmd_buffer, struct radv_cmd_buffer, vk);

-   list_for_each_entry_safe (struct radv_cmd_buffer_upload, up, &cmd_buffer->upload.list, list) {
-      radv_rmv_log_command_buffer_bo_destroy(cmd_buffer->device, up->upload_bo);
-      cmd_buffer->device->ws->buffer_destroy(cmd_buffer->device->ws, up->upload_bo);
-      list_del(&up->list);
-      free(up);
-   }
+   if (cmd_buffer->qf != RADV_QUEUE_SPARSE) {
+      list_for_each_entry_safe (struct radv_cmd_buffer_upload, up, &cmd_buffer->upload.list, list) {
+         radv_rmv_log_command_buffer_bo_destroy(cmd_buffer->device, up->upload_bo);
+         cmd_buffer->device->ws->buffer_destroy(cmd_buffer->device->ws, up->upload_bo);
+         list_del(&up->list);
+         free(up);
+      }

-   if (cmd_buffer->upload.upload_bo) {
-      radv_rmv_log_command_buffer_bo_destroy(cmd_buffer->device, cmd_buffer->upload.upload_bo);
-      cmd_buffer->device->ws->buffer_destroy(cmd_buffer->device->ws, cmd_buffer->upload.upload_bo);
-   }
+      if (cmd_buffer->upload.upload_bo) {
+         radv_rmv_log_command_buffer_bo_destroy(cmd_buffer->device, cmd_buffer->upload.upload_bo);
+         cmd_buffer->device->ws->buffer_destroy(cmd_buffer->device->ws, cmd_buffer->upload.upload_bo);
+      }

-   if (cmd_buffer->cs)
-      cmd_buffer->device->ws->cs_destroy(cmd_buffer->cs);
-   if (cmd_buffer->gang.cs)
-      cmd_buffer->device->ws->cs_destroy(cmd_buffer->gang.cs);
+      if (cmd_buffer->cs)
+         cmd_buffer->device->ws->cs_destroy(cmd_buffer->cs);
+      if (cmd_buffer->gang.cs)
+         cmd_buffer->device->ws->cs_destroy(cmd_buffer->gang.cs);

-   for (unsigned i = 0; i < MAX_BIND_POINTS; i++) {
-      struct radv_descriptor_set_header *set = &cmd_buffer->descriptors[i].push_set.set;
-      free(set->mapped_ptr);
-      if (set->layout)
-         vk_descriptor_set_layout_unref(&cmd_buffer->device->vk, &set->layout->vk);
-      vk_object_base_finish(&set->base);
-   }
+      for (unsigned i = 0; i < MAX_BIND_POINTS; i++) {
+         struct radv_descriptor_set_header *set = &cmd_buffer->descriptors[i].push_set.set;
+         free(set->mapped_ptr);
+         if (set->layout)
+            vk_descriptor_set_layout_unref(&cmd_buffer->device->vk, &set->layout->vk);
+         vk_object_base_finish(&set->base);
+      }

-   vk_object_base_finish(&cmd_buffer->meta_push_descriptors.base);
+      vk_object_base_finish(&cmd_buffer->meta_push_descriptors.base);
+   }

    vk_command_buffer_finish(&cmd_buffer->vk);
    vk_free(&cmd_buffer->vk.pool->alloc, cmd_buffer);
@@ -352,24 +354,27 @@ radv_create_cmd_buffer(struct vk_command_pool *pool, struct vk_command_buffer **
       return result;
    }

-   list_inithead(&cmd_buffer->upload.list);
-
    cmd_buffer->device = device;

    cmd_buffer->qf = vk_queue_to_radv(device->physical_device, pool->queue_family_index);

-   ring = radv_queue_family_to_ring(device->physical_device, cmd_buffer->qf);
+   if (cmd_buffer->qf != RADV_QUEUE_SPARSE) {
+      list_inithead(&cmd_buffer->upload.list);

-   cmd_buffer->cs = device->ws->cs_create(device->ws, ring, cmd_buffer->vk.level == VK_COMMAND_BUFFER_LEVEL_SECONDARY);
-   if (!cmd_buffer->cs) {
-      radv_destroy_cmd_buffer(&cmd_buffer->vk);
-      return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
-   }
+      ring = radv_queue_family_to_ring(device->physical_device, cmd_buffer->qf);

-   vk_object_base_init(&device->vk, &cmd_buffer->meta_push_descriptors.base, VK_OBJECT_TYPE_DESCRIPTOR_SET);
+      cmd_buffer->cs =
+         device->ws->cs_create(device->ws, ring, cmd_buffer->vk.level == VK_COMMAND_BUFFER_LEVEL_SECONDARY);
+      if (!cmd_buffer->cs) {
+         radv_destroy_cmd_buffer(&cmd_buffer->vk);
+         return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
+      }

-   for (unsigned i = 0; i < MAX_BIND_POINTS; i++)
-      vk_object_base_init(&device->vk, &cmd_buffer->descriptors[i].push_set.set.base, VK_OBJECT_TYPE_DESCRIPTOR_SET);
+      vk_object_base_init(&device->vk, &cmd_buffer->meta_push_descriptors.base, VK_OBJECT_TYPE_DESCRIPTOR_SET);
+
+      for (unsigned i = 0; i < MAX_BIND_POINTS; i++)
+         vk_object_base_init(&device->vk, &cmd_buffer->descriptors[i].push_set.set.base, VK_OBJECT_TYPE_DESCRIPTOR_SET);
+   }

    *cmd_buffer_out = &cmd_buffer->vk;

@@ -389,6 +394,9 @@ radv_reset_cmd_buffer(struct vk_command_buffer *vk_cmd_buffer, UNUSED VkCommandB

    vk_command_buffer_reset(&cmd_buffer->vk);

+   if (cmd_buffer->qf == RADV_QUEUE_SPARSE)
+      return;
+
    cmd_buffer->device->ws->cs_reset(cmd_buffer->cs);
    if (cmd_buffer->gang.cs)
       cmd_buffer->device->ws->cs_reset(cmd_buffer->gang.cs);
@@ -5757,6 +5765,9 @@ radv_BeginCommandBuffer(VkCommandBuffer commandBuffer, const VkCommandBufferBegi

    vk_command_buffer_begin(&cmd_buffer->vk, pBeginInfo);

+   if (cmd_buffer->qf == RADV_QUEUE_SPARSE)
+      return result;
+
    memset(&cmd_buffer->state, 0, sizeof(cmd_buffer->state));
    cmd_buffer->state.last_index_type = -1;
    cmd_buffer->state.last_num_instances = -1;
@@ -6185,6 +6196,9 @@ radv_EndCommandBuffer(VkCommandBuffer commandBuffer)
 {
    RADV_FROM_HANDLE(radv_cmd_buffer, cmd_buffer, commandBuffer);

+   if (cmd_buffer->qf == RADV_QUEUE_SPARSE)
+      return vk_command_buffer_end(&cmd_buffer->vk);
+
    radv_emit_mip_change_flush_default(cmd_buffer);

    if (cmd_buffer->qf == RADV_QUEUE_GENERAL || cmd_buffer->qf == RADV_QUEUE_COMPUTE) {
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index d51cbd3e173fa6a426f066f2ae97c6010a2b217e..e5b7814dca4f6ce67cd4b0a2d73763b1a7f012f9 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -259,6 +259,7 @@ enum radv_queue_family {
    RADV_QUEUE_GENERAL,
    RADV_QUEUE_COMPUTE,
    RADV_QUEUE_TRANSFER,
+   RADV_QUEUE_SPARSE,
    RADV_QUEUE_VIDEO_DEC,
    RADV_QUEUE_VIDEO_ENC,
    RADV_MAX_QUEUE_FAMILIES,

diff --git a/src/amd/vulkan/radv_image.c b/src/amd/vulkan/radv_image.c
index b6193065c12149b77f1ee0f254884a2ac195c34c..d5512d35960178b79477f11300a64d8ba4cd980c 100644
--- a/src/amd/vulkan/radv_image.c
+++ b/src/amd/vulkan/radv_image.c
@@ -1932,6 +1932,9 @@ radv_image_create(VkDevice _device, const struct radv_image_create_info *create_
          else
             image->queue_family_mask |=
                1u << vk_queue_to_radv(device->physical_device, pCreateInfo->pQueueFamilyIndices[i]);
+
+      /* This queue never really accesses the image. */
+      image->queue_family_mask &= ~(1u << RADV_QUEUE_SPARSE);
    }

    const VkExternalMemoryImageCreateInfo *external_info =

diff --git a/src/amd/vulkan/layers/radv_sqtt_layer.c b/src/amd/vulkan/layers/radv_sqtt_layer.c
index 726dc4372f892c59ad21e8359f4e931cb21c816b..733da8cb73a1c1ed609b0eff6bf6e3abc28f2034 100644
--- a/src/amd/vulkan/layers/radv_sqtt_layer.c
+++ b/src/amd/vulkan/layers/radv_sqtt_layer.c
@@ -323,7 +323,7 @@ radv_describe_begin_cmd_buffer(struct radv_cmd_buffer *cmd_buffer)
    marker.device_id_low = device_id;
    marker.device_id_high = device_id >> 32;
    marker.queue = cmd_buffer->qf;
-   marker.queue_flags = VK_QUEUE_COMPUTE_BIT | VK_QUEUE_TRANSFER_BIT | VK_QUEUE_SPARSE_BINDING_BIT;
+   marker.queue_flags = VK_QUEUE_COMPUTE_BIT | VK_QUEUE_TRANSFER_BIT;

    if (cmd_buffer->qf == RADV_QUEUE_GENERAL)
       marker.queue_flags |= VK_QUEUE_GRAPHICS_BIT;
diff --git a/src/amd/vulkan/radv_physical_device.c b/src/amd/vulkan/radv_physical_device.c
index 8e42c42a26aad9b558692564a874dd40dc35e193..6f250fb0e259e3ded8aab05ee7fb11409217c247 100644
--- a/src/amd/vulkan/radv_physical_device.c
+++ b/src/amd/vulkan/radv_physical_device.c
@@ -170,6 +170,9 @@ radv_physical_device_init_queue_table(struct radv_physical_device *pdevice)
          idx++;
       }
    }
+
+   pdevice->vk_queue_to_radv[idx++] = RADV_QUEUE_SPARSE;
+
    pdevice->num_queues = idx;
 }

@@ -2027,7 +2030,7 @@ static void
 radv_get_physical_device_queue_family_properties(struct radv_physical_device *pdevice, uint32_t *pCount,
                                                  VkQueueFamilyProperties **pQueueFamilyProperties)
 {
-   int num_queue_families = 1;
+   int num_queue_families = 2;
    int idx;
    if (pdevice->rad_info.ip[AMD_IP_COMPUTE].num_queues > 0 &&
        !(pdevice->instance->debug_flags & RADV_DEBUG_NO_COMPUTE_QUEUE))
@@ -2049,8 +2052,7 @@ radv_get_physical_device_queue_family_properties(struct radv_physical_device *pd
    idx = 0;
    if (*pCount >= 1) {
       *pQueueFamilyProperties[idx] = (VkQueueFamilyProperties){
-         .queueFlags =
-            VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT | VK_QUEUE_TRANSFER_BIT | VK_QUEUE_SPARSE_BINDING_BIT,
+         .queueFlags = VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT | VK_QUEUE_TRANSFER_BIT,
          .queueCount = 1,
          .timestampValidBits = 64,
          .minImageTransferGranularity = (VkExtent3D){1, 1, 1},
@@ -2062,7 +2064,7 @@ radv_get_physical_device_queue_family_properties(struct radv_physical_device *pd
        !(pdevice->instance->debug_flags & RADV_DEBUG_NO_COMPUTE_QUEUE)) {
       if (*pCount > idx) {
          *pQueueFamilyProperties[idx] = (VkQueueFamilyProperties){
-            .queueFlags = VK_QUEUE_COMPUTE_BIT | VK_QUEUE_TRANSFER_BIT | VK_QUEUE_SPARSE_BINDING_BIT,
+            .queueFlags = VK_QUEUE_COMPUTE_BIT | VK_QUEUE_TRANSFER_BIT,
             .queueCount = pdevice->rad_info.ip[AMD_IP_COMPUTE].num_queues,
             .timestampValidBits = 64,
             .minImageTransferGranularity = (VkExtent3D){1, 1, 1},
@@ -2071,6 +2073,16 @@ radv_get_physical_device_queue_family_properties(struct radv_physical_device *pd
       }
    }

+   if (*pCount > idx) {
+      *pQueueFamilyProperties[idx] = (VkQueueFamilyProperties){
+         .queueFlags = VK_QUEUE_SPARSE_BINDING_BIT,
+         .queueCount = 1,
+         .timestampValidBits = 64,
+         .minImageTransferGranularity = (VkExtent3D){1, 1, 1},
+      };
+      idx++;
+   }
+
    if (pdevice->instance->perftest_flags & RADV_PERFTEST_VIDEO_DECODE) {
       if (pdevice->rad_info.ip[pdevice->vid_decode_ip].num_queues > 0) {
          if (*pCount > idx) {
@@ -2108,9 +2120,10 @@ radv_GetPhysicalDeviceQueueFamilyProperties2(VkPhysicalDevice physicalDevice, ui
       &pQueueFamilyProperties[0].queueFamilyProperties,
       &pQueueFamilyProperties[1].queueFamilyProperties,
       &pQueueFamilyProperties[2].queueFamilyProperties,
+      &pQueueFamilyProperties[3].queueFamilyProperties,
    };
    radv_get_physical_device_queue_family_properties(pdevice, pCount, properties);
-   assert(*pCount <= 3);
+   assert(*pCount <= 4);

    for (uint32_t i = 0; i < *pCount; i++) {
       vk_foreach_struct (ext, pQueueFamilyProperties[i].pNext) {
diff --git a/src/amd/vulkan/radv_queue.c b/src/amd/vulkan/radv_queue.c
index e151a6d4a4f45874f07676732b93d19d10f63bcf..141ca70f80f9ae107bdbb635a3c6e65520453e9b 100644
--- a/src/amd/vulkan/radv_queue.c
+++ b/src/amd/vulkan/radv_queue.c
@@ -1699,17 +1699,51 @@ fail:
 }

 static VkResult
-radv_queue_submit(struct vk_queue *vqueue, struct vk_queue_submit *submission)
+radv_queue_sparse_submit(struct vk_queue *vqueue, struct vk_queue_submit *submission)
 {
    struct radv_queue *queue = (struct radv_queue *)vqueue;
+   struct radv_device *device = queue->device;
    VkResult result;

-   radv_rmv_log_submit(queue->device, radv_queue_ring(queue));
+   result = radv_queue_submit_bind_sparse_memory(device, submission);
+   if (result != VK_SUCCESS)
+      goto fail;

-   result = radv_queue_submit_bind_sparse_memory(queue->device, submission);
+   /* We do a CPU wait here, in part to avoid more winsys mechanisms. In the likely kernel explicit
+    * sync mechanism, we'd need to do a CPU wait anyway. Haven't seen this be a perf issue yet, but
+    * we have to make sure the queue always has its submission thread enabled. */
+   result = vk_sync_wait_many(&device->vk, submission->wait_count, submission->waits, 0, UINT64_MAX);
    if (result != VK_SUCCESS)
       goto fail;

+   /* Ignore all the commandbuffers. They're necessarily empty anyway. */
+
+   for (unsigned i = 0; i < submission->signal_count; ++i) {
+      result = vk_sync_signal(&device->vk, submission->signals[i].sync, submission->signals[i].signal_value);
+      if (result != VK_SUCCESS)
+         goto fail;
+   }
+
+fail:
+   if (result != VK_SUCCESS && result != VK_ERROR_DEVICE_LOST) {
+      /* When something bad happened during the submission, such as
+       * an out of memory issue, it might be hard to recover from
+       * this inconsistent state. To avoid this sort of problem, we
+       * assume that we are in a really bad situation and return
+       * VK_ERROR_DEVICE_LOST to ensure the clients do not attempt
+       * to submit the same job again to this device.
+       */
+      result = vk_device_set_lost(&queue->device->vk, "vkQueueSubmit() failed");
+   }
+   return result;
+}
+
+static VkResult
+radv_queue_submit(struct vk_queue *vqueue, struct vk_queue_submit *submission)
+{
+   struct radv_queue *queue = (struct radv_queue *)vqueue;
+   VkResult result;
+
    if (!submission->command_buffer_count && !submission->wait_count && !submission->signal_count)
       return VK_SUCCESS;

@@ -1719,7 +1753,6 @@ radv_queue_submit(struct vk_queue *vqueue, struct vk_queue_submit *submission)
       result = radv_queue_submit_normal(queue, submission);
    }

-fail:
    if (result != VK_SUCCESS && result != VK_ERROR_DEVICE_LOST) {
       /* When something bad happened during the submission, such as
        * an out of memory issue, it might be hard to recover from
@@ -1776,7 +1809,12 @@ radv_queue_init(struct radv_device *device, struct radv_queue *queue, int idx,
          goto fail;
    }

-   queue->vk.driver_submit = radv_queue_submit;
+   if (queue->state.qf == RADV_QUEUE_SPARSE) {
+      queue->vk.driver_submit = radv_queue_sparse_submit;
+      vk_queue_enable_submit_thread(&queue->vk);
+   } else {
+      queue->vk.driver_submit = radv_queue_submit;
+   }
    return VK_SUCCESS;
 fail:
    vk_queue_finish(&queue->vk);

diff --git a/include/drm-uapi/amdgpu_drm.h b/include/drm-uapi/amdgpu_drm.h
index 79b14828d542a691f187a160f29bebcea04ee935..9f2877071b6f543eaea21e4ebd31b1be10da2f19 100644
--- a/include/drm-uapi/amdgpu_drm.h
+++ b/include/drm-uapi/amdgpu_drm.h
@@ -226,6 +226,7 @@ union drm_amdgpu_bo_list {
 #define AMDGPU_CTX_OP_QUERY_STATE2	4
 #define AMDGPU_CTX_OP_GET_STABLE_PSTATE	5
 #define AMDGPU_CTX_OP_SET_STABLE_PSTATE	6
+#define AMDGPU_CTX_OP_SET_IMPLICIT_SYNC	7

 /* GPU reset status */
 #define AMDGPU_CTX_NO_RESET		0
@@ -268,6 +269,8 @@ union drm_amdgpu_bo_list {
 #define AMDGPU_CTX_STABLE_PSTATE_MIN_MCLK  3
 #define AMDGPU_CTX_STABLE_PSTATE_PEAK  4

+#define AMDGPU_CTX_IMPICIT_SYNC_ENABLED 1
+
 struct drm_amdgpu_ctx_in {
 	/** AMDGPU_CTX_OP_* */
 	__u32	op;

diff --git a/src/amd/common/ac_gpu_info.c b/src/amd/common/ac_gpu_info.c
index 2636ebf83c307948879705d33016f26561e58d4a..185dfdfaddffc55584b08845e0ba4b971bbd5237 100644
--- a/src/amd/common/ac_gpu_info.c
+++ b/src/amd/common/ac_gpu_info.c
@@ -1314,6 +1314,7 @@ bool ac_query_gpu_info(int fd, void *dev_p, struct radeon_info *info)
    }

    info->has_stable_pstate = info->drm_minor >= 45;
+   info->has_explicit_sync_contexts = info->drm_minor >= 53;

    if (info->gfx_level >= GFX11) {
       info->pc_lines = 1024;
@@ -1751,6 +1752,7 @@ void ac_print_gpu_info(const struct radeon_info *info, FILE *f)
    fprintf(f, "    has_eqaa_surface_allocator = %u\n", info->has_eqaa_surface_allocator);
    fprintf(f, "    has_sparse_vm_mappings = %u\n", info->has_sparse_vm_mappings);
    fprintf(f, "    has_stable_pstate = %u\n", info->has_stable_pstate);
+   fprintf(f, "    has_explicit_sync_contexts = %u\n", info->has_explicit_sync_contexts);
    fprintf(f, "    has_scheduled_fence_dependency = %u\n", info->has_scheduled_fence_dependency);
    fprintf(f, "    has_gang_submit = %u\n", info->has_gang_submit);
    fprintf(f, "    register_shadowing_required = %u\n", info->register_shadowing_required);
diff --git a/src/amd/common/ac_gpu_info.h b/src/amd/common/ac_gpu_info.h
index 8f79698d72b3512abde93ceed39fe800cf0861fb..e6a77425497b8115cb613ee37d95b97c81012c22 100644
--- a/src/amd/common/ac_gpu_info.h
+++ b/src/amd/common/ac_gpu_info.h
@@ -200,6 +200,7 @@ struct radeon_info {
    bool has_gang_submit;
    bool has_pcie_bandwidth_info;
    bool has_stable_pstate;
+   bool has_explicit_sync_contexts;
    /* Whether SR-IOV is enabled or amdgpu.mcbp=1 was set on the kernel command line. */
    bool register_shadowing_required;
    bool has_tmz_support;

diff --git a/src/amd/vulkan/radv_queue.c b/src/amd/vulkan/radv_queue.c
index 141ca70f80f9ae107bdbb635a3c6e65520453e9b..1e7c748d765c6cbd33cdb48f1476bb5dbb9027b4 100644
--- a/src/amd/vulkan/radv_queue.c
+++ b/src/amd/vulkan/radv_queue.c
@@ -1705,16 +1705,24 @@ radv_queue_sparse_submit(struct vk_queue *vqueue, struct vk_queue_submit *submis
    struct radv_device *device = queue->device;
    VkResult result;

+   if (device->physical_device->rad_info.has_explicit_sync_contexts) {
+      /* We do a CPU wait here, as the sparse binds get executed immediately. */
+      result =
+         vk_sync_wait_many(&device->vk, submission->wait_count, submission->waits, VK_SYNC_WAIT_COMPLETE, UINT64_MAX);
+      if (result != VK_SUCCESS)
+         goto fail;
+   }
+
    result = radv_queue_submit_bind_sparse_memory(device, submission);
    if (result != VK_SUCCESS)
       goto fail;

-   /* We do a CPU wait here, in part to avoid more winsys mechanisms. In the likely kernel explicit
-    * sync mechanism, we'd need to do a CPU wait anyway. Haven't seen this be a perf issue yet, but
-    * we have to make sure the queue always has its submission thread enabled. */
-   result = vk_sync_wait_many(&device->vk, submission->wait_count, submission->waits, 0, UINT64_MAX);
-   if (result != VK_SUCCESS)
-      goto fail;
+   if (!device->physical_device->rad_info.has_explicit_sync_contexts) {
+      /* We do a CPU wait here, so that all dependencies have finished before we start signaling the signal vk_syncs. */
+      result = vk_sync_wait_many(&device->vk, submission->wait_count, submission->waits, 0, UINT64_MAX);
+      if (result != VK_SUCCESS)
+         goto fail;
+   }

    /* Ignore all the commandbuffers. They're necessarily empty anyway. */

diff --git a/src/amd/vulkan/radv_queue.c b/src/amd/vulkan/radv_queue.c
index 1e7c748d765c6cbd33cdb48f1476bb5dbb9027b4..340a8b9f3b34579486d6c09b006cd1cf0e04babe 100644
--- a/src/amd/vulkan/radv_queue.c
+++ b/src/amd/vulkan/radv_queue.c
@@ -811,28 +811,30 @@ radv_init_compute_state(struct radeon_cmdbuf *cs, struct radv_device *device)
 }
 
 static VkResult
-radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *device,
+radv_update_preamble_cs(struct radv_queue_state *queue_state, struct radv_queue *queue,
                         const struct radv_queue_ring_info *needs)
 {
+   struct radv_device *device = queue->device;
    struct radeon_winsys *ws = device->ws;
-   struct radeon_winsys_bo *scratch_bo = queue->scratch_bo;
-   struct radeon_winsys_bo *descriptor_bo = queue->descriptor_bo;
-   struct radeon_winsys_bo *compute_scratch_bo = queue->compute_scratch_bo;
-   struct radeon_winsys_bo *esgs_ring_bo = queue->esgs_ring_bo;
-   struct radeon_winsys_bo *gsvs_ring_bo = queue->gsvs_ring_bo;
-   struct radeon_winsys_bo *tess_rings_bo = queue->tess_rings_bo;
-   struct radeon_winsys_bo *task_rings_bo = queue->task_rings_bo;
-   struct radeon_winsys_bo *mesh_scratch_ring_bo = queue->mesh_scratch_ring_bo;
-   struct radeon_winsys_bo *attr_ring_bo = queue->attr_ring_bo;
-   struct radeon_winsys_bo *gds_bo = queue->gds_bo;
-   struct radeon_winsys_bo *gds_oa_bo = queue->gds_oa_bo;
+   struct radeon_winsys_bo *scratch_bo = queue_state->scratch_bo;
+   struct radeon_winsys_bo *descriptor_bo = queue_state->descriptor_bo;
+   struct radeon_winsys_bo *compute_scratch_bo = queue_state->compute_scratch_bo;
+   struct radeon_winsys_bo *esgs_ring_bo = queue_state->esgs_ring_bo;
+   struct radeon_winsys_bo *gsvs_ring_bo = queue_state->gsvs_ring_bo;
+   struct radeon_winsys_bo *tess_rings_bo = queue_state->tess_rings_bo;
+   struct radeon_winsys_bo *task_rings_bo = queue_state->task_rings_bo;
+   struct radeon_winsys_bo *mesh_scratch_ring_bo = queue_state->mesh_scratch_ring_bo;
+   struct radeon_winsys_bo *attr_ring_bo = queue_state->attr_ring_bo;
+   struct radeon_winsys_bo *gds_bo = queue_state->gds_bo;
+   struct radeon_winsys_bo *gds_oa_bo = queue_state->gds_oa_bo;
    struct radeon_cmdbuf *dest_cs[3] = {0};
    const uint32_t ring_bo_flags = RADEON_FLAG_NO_CPU_ACCESS | RADEON_FLAG_NO_INTERPROCESS_SHARING;
    VkResult result = VK_SUCCESS;
 
-   const bool add_sample_positions = !queue->ring_info.sample_positions && needs->sample_positions;
+   const bool add_sample_positions = !queue_state->ring_info.sample_positions && needs->sample_positions;
    const uint32_t scratch_size = needs->scratch_size_per_wave * needs->scratch_waves;
-   const uint32_t queue_scratch_size = queue->ring_info.scratch_size_per_wave * queue->ring_info.scratch_waves;
+   const uint32_t queue_scratch_size =
+      queue_state->ring_info.scratch_size_per_wave * queue_state->ring_info.scratch_waves;
 
    if (scratch_size > queue_scratch_size) {
       result = ws->buffer_create(ws, scratch_size, 4096, RADEON_DOMAIN_VRAM, ring_bo_flags, RADV_BO_PRIORITY_SCRATCH, 0,
@@ -844,7 +846,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
 
    const uint32_t compute_scratch_size = needs->compute_scratch_size_per_wave * needs->compute_scratch_waves;
    const uint32_t compute_queue_scratch_size =
-      queue->ring_info.compute_scratch_size_per_wave * queue->ring_info.compute_scratch_waves;
+      queue_state->ring_info.compute_scratch_size_per_wave * queue_state->ring_info.compute_scratch_waves;
    if (compute_scratch_size > compute_queue_scratch_size) {
       result = ws->buffer_create(ws, compute_scratch_size, 4096, RADEON_DOMAIN_VRAM, ring_bo_flags,
                                  RADV_BO_PRIORITY_SCRATCH, 0, &compute_scratch_bo);
@@ -853,7 +855,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
       radv_rmv_log_command_buffer_bo_create(device, compute_scratch_bo, 0, 0, compute_scratch_size);
    }
 
-   if (needs->esgs_ring_size > queue->ring_info.esgs_ring_size) {
+   if (needs->esgs_ring_size > queue_state->ring_info.esgs_ring_size) {
       result = ws->buffer_create(ws, needs->esgs_ring_size, 4096, RADEON_DOMAIN_VRAM, ring_bo_flags,
                                  RADV_BO_PRIORITY_SCRATCH, 0, &esgs_ring_bo);
       if (result != VK_SUCCESS)
@@ -861,7 +863,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
       radv_rmv_log_command_buffer_bo_create(device, esgs_ring_bo, 0, 0, needs->esgs_ring_size);
    }
 
-   if (needs->gsvs_ring_size > queue->ring_info.gsvs_ring_size) {
+   if (needs->gsvs_ring_size > queue_state->ring_info.gsvs_ring_size) {
       result = ws->buffer_create(ws, needs->gsvs_ring_size, 4096, RADEON_DOMAIN_VRAM, ring_bo_flags,
                                  RADV_BO_PRIORITY_SCRATCH, 0, &gsvs_ring_bo);
       if (result != VK_SUCCESS)
@@ -869,7 +871,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
       radv_rmv_log_command_buffer_bo_create(device, gsvs_ring_bo, 0, 0, needs->gsvs_ring_size);
    }
 
-   if (!queue->ring_info.tess_rings && needs->tess_rings) {
+   if (!queue_state->ring_info.tess_rings && needs->tess_rings) {
       uint64_t tess_rings_size =
          device->physical_device->hs.tess_offchip_ring_offset + device->physical_device->hs.tess_offchip_ring_size;
       result = ws->buffer_create(ws, tess_rings_size, 256, RADEON_DOMAIN_VRAM, ring_bo_flags, RADV_BO_PRIORITY_SCRATCH,
@@ -879,7 +881,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
       radv_rmv_log_command_buffer_bo_create(device, tess_rings_bo, 0, 0, tess_rings_size);
    }
 
-   if (!queue->ring_info.task_rings && needs->task_rings) {
+   if (!queue_state->ring_info.task_rings && needs->task_rings) {
       assert(device->physical_device->rad_info.gfx_level >= GFX10_3);
 
       /* We write the control buffer from the CPU, so need to grant CPU access to the BO.
@@ -900,7 +902,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
          goto fail;
    }
 
-   if (!queue->ring_info.mesh_scratch_ring && needs->mesh_scratch_ring) {
+   if (!queue_state->ring_info.mesh_scratch_ring && needs->mesh_scratch_ring) {
       assert(device->physical_device->rad_info.gfx_level >= GFX10_3);
       result = ws->buffer_create(ws, RADV_MESH_SCRATCH_NUM_ENTRIES * RADV_MESH_SCRATCH_ENTRY_BYTES, 256,
                                  RADEON_DOMAIN_VRAM, ring_bo_flags, RADV_BO_PRIORITY_SCRATCH, 0, &mesh_scratch_ring_bo);
@@ -911,7 +913,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
                                             RADV_MESH_SCRATCH_NUM_ENTRIES * RADV_MESH_SCRATCH_ENTRY_BYTES);
    }
 
-   if (needs->attr_ring_size > queue->ring_info.attr_ring_size) {
+   if (needs->attr_ring_size > queue_state->ring_info.attr_ring_size) {
       assert(device->physical_device->rad_info.gfx_level >= GFX11);
       result = ws->buffer_create(ws, needs->attr_ring_size, 2 * 1024 * 1024 /* 2MiB */, RADEON_DOMAIN_VRAM,
                                  RADEON_FLAG_32BIT | RADEON_FLAG_DISCARDABLE | ring_bo_flags, RADV_BO_PRIORITY_SCRATCH,
@@ -921,7 +923,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
       radv_rmv_log_command_buffer_bo_create(device, attr_ring_bo, 0, 0, needs->attr_ring_size);
    }
 
-   if (!queue->ring_info.gds && needs->gds) {
+   if (!queue_state->ring_info.gds && needs->gds) {
       assert(device->physical_device->rad_info.gfx_level >= GFX10);
 
       /* 4 streamout GDS counters.
@@ -939,7 +941,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
          goto fail;
    }
 
-   if (!queue->ring_info.gds_oa && needs->gds_oa) {
+   if (!queue_state->ring_info.gds_oa && needs->gds_oa) {
       assert(device->physical_device->rad_info.gfx_level >= GFX10);
 
       result = ws->buffer_create(ws, 4, 1, RADEON_DOMAIN_OA, ring_bo_flags, RADV_BO_PRIORITY_SCRATCH, 0, &gds_oa_bo);
@@ -960,11 +962,11 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
     * when it uses the task shader rings. The task rings BO is shared between the
     * GFX and compute queues and already initialized here.
     */
-   if ((queue->qf == RADV_QUEUE_COMPUTE && !descriptor_bo && task_rings_bo) || scratch_bo != queue->scratch_bo ||
-       esgs_ring_bo != queue->esgs_ring_bo || gsvs_ring_bo != queue->gsvs_ring_bo ||
-       tess_rings_bo != queue->tess_rings_bo || task_rings_bo != queue->task_rings_bo ||
-       mesh_scratch_ring_bo != queue->mesh_scratch_ring_bo || attr_ring_bo != queue->attr_ring_bo ||
-       add_sample_positions) {
+   if ((queue_state->qf == RADV_QUEUE_COMPUTE && !descriptor_bo && task_rings_bo) ||
+       scratch_bo != queue_state->scratch_bo || esgs_ring_bo != queue_state->esgs_ring_bo ||
+       gsvs_ring_bo != queue_state->gsvs_ring_bo || tess_rings_bo != queue_state->tess_rings_bo ||
+       task_rings_bo != queue_state->task_rings_bo || mesh_scratch_ring_bo != queue_state->mesh_scratch_ring_bo ||
+       attr_ring_bo != queue_state->attr_ring_bo || add_sample_positions) {
       uint32_t size = 0;
       if (gsvs_ring_bo || esgs_ring_bo || tess_rings_bo || task_rings_bo || mesh_scratch_ring_bo || attr_ring_bo ||
           add_sample_positions) {
@@ -982,7 +984,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
          goto fail;
    }
 
-   if (descriptor_bo != queue->descriptor_bo) {
+   if (descriptor_bo != queue_state->descriptor_bo) {
       uint32_t *map = (uint32_t *)ws->buffer_map(descriptor_bo);
       if (!map)
          goto fail;
@@ -1012,7 +1014,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
    for (int i = 0; i < 3; ++i) {
       enum rgp_flush_bits sqtt_flush_bits = 0;
       struct radeon_cmdbuf *cs = NULL;
-      cs = ws->cs_create(ws, radv_queue_family_to_ring(device->physical_device, queue->qf), false);
+      cs = ws->cs_create(ws, radv_queue_family_to_ring(device->physical_device, queue_state->qf), false);
       if (!cs) {
          result = VK_ERROR_OUT_OF_HOST_MEMORY;
          goto fail;
@@ -1025,10 +1027,10 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
          radv_cs_add_buffer(ws, cs, scratch_bo);
 
       /* Emit initial configuration. */
-      switch (queue->qf) {
+      switch (queue_state->qf) {
       case RADV_QUEUE_GENERAL:
-         if (queue->uses_shadow_regs)
-            radv_emit_shadow_regs_preamble(cs, device, queue);
+         if (queue_state->uses_shadow_regs)
+            radv_emit_shadow_regs_preamble(cs, device, queue_state);
          radv_init_graphics_state(cs, device);
 
          if (esgs_ring_bo || gsvs_ring_bo || tess_rings_bo || task_rings_bo) {
@@ -1068,7 +1070,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
       if (i < 2) {
          /* The two initial preambles have a cache flush at the beginning. */
          const enum amd_gfx_level gfx_level = device->physical_device->rad_info.gfx_level;
-         const bool is_mec = queue->qf == RADV_QUEUE_COMPUTE && gfx_level >= GFX7;
+         const bool is_mec = queue_state->qf == RADV_QUEUE_COMPUTE && gfx_level >= GFX7;
          enum radv_cmd_flush_bits flush_bits = RADV_CMD_FLAG_INV_ICACHE | RADV_CMD_FLAG_INV_SCACHE |
                                                RADV_CMD_FLAG_INV_VCACHE | RADV_CMD_FLAG_INV_L2 |
                                                RADV_CMD_FLAG_START_PIPELINE_STATS;
@@ -1076,7 +1078,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
          if (i == 0) {
             /* The full flush preamble should also wait for previous shader work to finish. */
             flush_bits |= RADV_CMD_FLAG_CS_PARTIAL_FLUSH;
-            if (queue->qf == RADV_QUEUE_GENERAL)
+            if (queue_state->qf == RADV_QUEUE_GENERAL)
                flush_bits |= RADV_CMD_FLAG_PS_PARTIAL_FLUSH;
          }
 
@@ -1088,91 +1090,106 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
          goto fail;
    }
 
-   if (queue->initial_full_flush_preamble_cs)
-      ws->cs_destroy(queue->initial_full_flush_preamble_cs);
+   if (queue_state->initial_full_flush_preamble_cs || queue_state->initial_preamble_cs ||
+       queue_state->continue_preamble_cs || scratch_bo != queue_state->scratch_bo ||
+       compute_scratch_bo != queue_state->compute_scratch_bo || esgs_ring_bo != queue_state->esgs_ring_bo ||
+       gsvs_ring_bo != queue_state->gsvs_ring_bo || descriptor_bo != queue_state->descriptor_bo) {
 
-   if (queue->initial_preamble_cs)
-      ws->cs_destroy(queue->initial_preamble_cs);
+      if (device->physical_device->rad_info.has_explicit_sync_contexts) {
+         bool success =
+            queue->device->ws->ctx_wait_idle(queue->hw_ctx, radv_queue_ring(queue), queue->vk.index_in_family);
+         if (!success) {
+            result = VK_ERROR_DEVICE_LOST;
+            goto fail;
+         }
+      }
+   }
 
-   if (queue->continue_preamble_cs)
-      ws->cs_destroy(queue->continue_preamble_cs);
+   if (queue_state->initial_full_flush_preamble_cs)
+      ws->cs_destroy(queue_state->initial_full_flush_preamble_cs);
+
+   if (queue_state->initial_preamble_cs)
+      ws->cs_destroy(queue_state->initial_preamble_cs);
 
-   queue->initial_full_flush_preamble_cs = dest_cs[0];
-   queue->initial_preamble_cs = dest_cs[1];
-   queue->continue_preamble_cs = dest_cs[2];
+   if (queue_state->continue_preamble_cs)
+      ws->cs_destroy(queue_state->continue_preamble_cs);
 
-   if (scratch_bo != queue->scratch_bo) {
-      if (queue->scratch_bo) {
-         ws->buffer_destroy(ws, queue->scratch_bo);
-         radv_rmv_log_command_buffer_bo_destroy(device, queue->scratch_bo);
+   queue_state->initial_full_flush_preamble_cs = dest_cs[0];
+   queue_state->initial_preamble_cs = dest_cs[1];
+   queue_state->continue_preamble_cs = dest_cs[2];
+
+   if (scratch_bo != queue_state->scratch_bo) {
+      if (queue_state->scratch_bo) {
+         ws->buffer_destroy(ws, queue_state->scratch_bo);
+         radv_rmv_log_command_buffer_bo_destroy(device, queue_state->scratch_bo);
       }
-      queue->scratch_bo = scratch_bo;
+      queue_state->scratch_bo = scratch_bo;
    }
 
-   if (compute_scratch_bo != queue->compute_scratch_bo) {
-      if (queue->compute_scratch_bo) {
-         ws->buffer_destroy(ws, queue->compute_scratch_bo);
-         radv_rmv_log_command_buffer_bo_destroy(device, queue->compute_scratch_bo);
+   if (compute_scratch_bo != queue_state->compute_scratch_bo) {
+      if (queue_state->compute_scratch_bo) {
+         ws->buffer_destroy(ws, queue_state->compute_scratch_bo);
+         radv_rmv_log_command_buffer_bo_destroy(device, queue_state->compute_scratch_bo);
       }
-      queue->compute_scratch_bo = compute_scratch_bo;
+      queue_state->compute_scratch_bo = compute_scratch_bo;
    }
 
-   if (esgs_ring_bo != queue->esgs_ring_bo) {
-      if (queue->esgs_ring_bo) {
-         ws->buffer_destroy(ws, queue->esgs_ring_bo);
-         radv_rmv_log_command_buffer_bo_destroy(device, queue->esgs_ring_bo);
+   if (esgs_ring_bo != queue_state->esgs_ring_bo) {
+      if (queue_state->esgs_ring_bo) {
+         ws->buffer_destroy(ws, queue_state->esgs_ring_bo);
+         radv_rmv_log_command_buffer_bo_destroy(device, queue_state->esgs_ring_bo);
       }
-      queue->esgs_ring_bo = esgs_ring_bo;
+      queue_state->esgs_ring_bo = esgs_ring_bo;
    }
 
-   if (gsvs_ring_bo != queue->gsvs_ring_bo) {
-      if (queue->gsvs_ring_bo) {
-         ws->buffer_destroy(ws, queue->gsvs_ring_bo);
-         radv_rmv_log_command_buffer_bo_destroy(device, queue->gsvs_ring_bo);
+   if (gsvs_ring_bo != queue_state->gsvs_ring_bo) {
+      if (queue_state->gsvs_ring_bo) {
+         ws->buffer_destroy(ws, queue_state->gsvs_ring_bo);
+         radv_rmv_log_command_buffer_bo_destroy(device, queue_state->gsvs_ring_bo);
       }
-      queue->gsvs_ring_bo = gsvs_ring_bo;
+      queue_state->gsvs_ring_bo = gsvs_ring_bo;
    }
 
-   if (descriptor_bo != queue->descriptor_bo) {
-      if (queue->descriptor_bo)
-         ws->buffer_destroy(ws, queue->descriptor_bo);
-      queue->descriptor_bo = descriptor_bo;
+   if (descriptor_bo != queue_state->descriptor_bo) {
+      if (queue_state->descriptor_bo)
+         ws->buffer_destroy(ws, queue_state->descriptor_bo);
+      queue_state->descriptor_bo = descriptor_bo;
    }
 
-   queue->tess_rings_bo = tess_rings_bo;
-   queue->task_rings_bo = task_rings_bo;
-   queue->mesh_scratch_ring_bo = mesh_scratch_ring_bo;
-   queue->attr_ring_bo = attr_ring_bo;
-   queue->gds_bo = gds_bo;
-   queue->gds_oa_bo = gds_oa_bo;
-   queue->ring_info = *needs;
+   queue_state->tess_rings_bo = tess_rings_bo;
+   queue_state->task_rings_bo = task_rings_bo;
+   queue_state->mesh_scratch_ring_bo = mesh_scratch_ring_bo;
+   queue_state->attr_ring_bo = attr_ring_bo;
+   queue_state->gds_bo = gds_bo;
+   queue_state->gds_oa_bo = gds_oa_bo;
+   queue_state->ring_info = *needs;
    return VK_SUCCESS;
 fail:
    for (int i = 0; i < ARRAY_SIZE(dest_cs); ++i)
       if (dest_cs[i])
          ws->cs_destroy(dest_cs[i]);
-   if (descriptor_bo && descriptor_bo != queue->descriptor_bo)
+   if (descriptor_bo && descriptor_bo != queue_state->descriptor_bo)
       ws->buffer_destroy(ws, descriptor_bo);
-   if (scratch_bo && scratch_bo != queue->scratch_bo)
+   if (scratch_bo && scratch_bo != queue_state->scratch_bo)
       ws->buffer_destroy(ws, scratch_bo);
-   if (compute_scratch_bo && compute_scratch_bo != queue->compute_scratch_bo)
+   if (compute_scratch_bo && compute_scratch_bo != queue_state->compute_scratch_bo)
       ws->buffer_destroy(ws, compute_scratch_bo);
-   if (esgs_ring_bo && esgs_ring_bo != queue->esgs_ring_bo)
+   if (esgs_ring_bo && esgs_ring_bo != queue_state->esgs_ring_bo)
       ws->buffer_destroy(ws, esgs_ring_bo);
-   if (gsvs_ring_bo && gsvs_ring_bo != queue->gsvs_ring_bo)
+   if (gsvs_ring_bo && gsvs_ring_bo != queue_state->gsvs_ring_bo)
       ws->buffer_destroy(ws, gsvs_ring_bo);
-   if (tess_rings_bo && tess_rings_bo != queue->tess_rings_bo)
+   if (tess_rings_bo && tess_rings_bo != queue_state->tess_rings_bo)
       ws->buffer_destroy(ws, tess_rings_bo);
-   if (task_rings_bo && task_rings_bo != queue->task_rings_bo)
+   if (task_rings_bo && task_rings_bo != queue_state->task_rings_bo)
       ws->buffer_destroy(ws, task_rings_bo);
-   if (attr_ring_bo && attr_ring_bo != queue->attr_ring_bo)
+   if (attr_ring_bo && attr_ring_bo != queue_state->attr_ring_bo)
       ws->buffer_destroy(ws, attr_ring_bo);
-   if (gds_bo && gds_bo != queue->gds_bo) {
-      ws->buffer_make_resident(ws, queue->gds_bo, false);
+   if (gds_bo && gds_bo != queue_state->gds_bo) {
+      ws->buffer_make_resident(ws, queue_state->gds_bo, false);
       ws->buffer_destroy(ws, gds_bo);
    }
-   if (gds_oa_bo && gds_oa_bo != queue->gds_oa_bo) {
-      ws->buffer_make_resident(ws, queue->gds_oa_bo, false);
+   if (gds_oa_bo && gds_oa_bo != queue_state->gds_oa_bo) {
+      ws->buffer_make_resident(ws, queue_state->gds_oa_bo, false);
       ws->buffer_destroy(ws, gds_oa_bo);
    }
 
@@ -1180,20 +1197,22 @@ fail:
 }
 
 static VkResult
-radv_update_preambles(struct radv_queue_state *queue, struct radv_device *device,
+radv_update_preambles(struct radv_queue_state *queue_state, struct radv_queue *queue,
                       struct vk_command_buffer *const *cmd_buffers, uint32_t cmd_buffer_count, bool *use_perf_counters,
                       bool *has_follower)
 {
-   if (queue->qf != RADV_QUEUE_GENERAL && queue->qf != RADV_QUEUE_COMPUTE)
+   if (queue_state->qf != RADV_QUEUE_GENERAL && queue_state->qf != RADV_QUEUE_COMPUTE)
       return VK_SUCCESS;
 
+   struct radv_device *device = queue->device;
+
    /* Figure out the needs of the current submission.
     * Start by copying the queue's current info.
     * This is done because we only allow two possible behaviours for these buffers:
     * - Grow when the newly needed amount is larger than what we had
     * - Allocate the max size and reuse it, but don't free it until the queue is destroyed
     */
-   struct radv_queue_ring_info needs = queue->ring_info;
+   struct radv_queue_ring_info needs = queue_state->ring_info;
    *use_perf_counters = false;
    *has_follower = false;
 
@@ -1225,7 +1244,7 @@ radv_update_preambles(struct radv_queue_state *queue, struct radv_device *device
          ? MIN2(needs.compute_scratch_waves, UINT32_MAX / needs.compute_scratch_size_per_wave)
          : 0;
 
-   if (device->physical_device->rad_info.gfx_level >= GFX11 && queue->qf == RADV_QUEUE_GENERAL) {
+   if (device->physical_device->rad_info.gfx_level >= GFX11 && queue_state->qf == RADV_QUEUE_GENERAL) {
       needs.attr_ring_size =
          device->physical_device->rad_info.attribute_ring_size_per_se * device->physical_device->rad_info.max_se;
    }
@@ -1234,19 +1253,21 @@ radv_update_preambles(struct radv_queue_state *queue, struct radv_device *device
     * Note that it's not possible for any of the needed values to be less
     * than what the queue already had, because we only ever increase the allocated size.
     */
-   if (queue->initial_full_flush_preamble_cs && queue->ring_info.scratch_size_per_wave == needs.scratch_size_per_wave &&
-       queue->ring_info.scratch_waves == needs.scratch_waves &&
-       queue->ring_info.compute_scratch_size_per_wave == needs.compute_scratch_size_per_wave &&
-       queue->ring_info.compute_scratch_waves == needs.compute_scratch_waves &&
-       queue->ring_info.esgs_ring_size == needs.esgs_ring_size &&
-       queue->ring_info.gsvs_ring_size == needs.gsvs_ring_size && queue->ring_info.tess_rings == needs.tess_rings &&
-       queue->ring_info.task_rings == needs.task_rings &&
-       queue->ring_info.mesh_scratch_ring == needs.mesh_scratch_ring &&
-       queue->ring_info.attr_ring_size == needs.attr_ring_size && queue->ring_info.gds == needs.gds &&
-       queue->ring_info.gds_oa == needs.gds_oa && queue->ring_info.sample_positions == needs.sample_positions)
+   if (queue_state->initial_full_flush_preamble_cs &&
+       queue_state->ring_info.scratch_size_per_wave == needs.scratch_size_per_wave &&
+       queue_state->ring_info.scratch_waves == needs.scratch_waves &&
+       queue_state->ring_info.compute_scratch_size_per_wave == needs.compute_scratch_size_per_wave &&
+       queue_state->ring_info.compute_scratch_waves == needs.compute_scratch_waves &&
+       queue_state->ring_info.esgs_ring_size == needs.esgs_ring_size &&
+       queue_state->ring_info.gsvs_ring_size == needs.gsvs_ring_size &&
+       queue_state->ring_info.tess_rings == needs.tess_rings && queue_state->ring_info.task_rings == needs.task_rings &&
+       queue_state->ring_info.mesh_scratch_ring == needs.mesh_scratch_ring &&
+       queue_state->ring_info.attr_ring_size == needs.attr_ring_size && queue_state->ring_info.gds == needs.gds &&
+       queue_state->ring_info.gds_oa == needs.gds_oa &&
+       queue_state->ring_info.sample_positions == needs.sample_positions)
       return VK_SUCCESS;
 
-   return radv_update_preamble_cs(queue, device, &needs);
+   return radv_update_preamble_cs(queue_state, queue, &needs);
 }
 
 static VkResult
@@ -1401,7 +1422,7 @@ radv_update_gang_preambles(struct radv_queue *queue)
    needs.compute_scratch_waves = queue->state.ring_info.scratch_waves;
    needs.task_rings = queue->state.ring_info.task_rings;
 
-   r = radv_update_preamble_cs(queue->follower_state, queue->device, &needs);
+   r = radv_update_preamble_cs(queue->follower_state, queue, &needs);
    if (r != VK_SUCCESS)
       return r;
 
@@ -1516,8 +1537,8 @@ radv_queue_submit_normal(struct radv_queue *queue, struct vk_queue_submit *submi
    uint32_t wait_count = submission->wait_count;
    struct vk_sync_wait *waits = submission->waits;
 
-   result = radv_update_preambles(&queue->state, queue->device, submission->command_buffers,
-                                  submission->command_buffer_count, &use_perf_counters, &use_ace);
+   result = radv_update_preambles(&queue->state, queue, submission->command_buffers, submission->command_buffer_count,
+                                  &use_perf_counters, &use_ace);
    if (result != VK_SUCCESS)
       return result;
 
diff --git a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_cs.c b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_cs.c
index 3bf264c4b80b5e5370db291550a9cbb046e3c84f..8514025b0f937d41b1ac07c2b0ed44174f6e13c2 100644
--- a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_cs.c
+++ b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_cs.c
@@ -1393,9 +1393,20 @@ radv_amdgpu_ctx_create(struct radeon_winsys *_ws, enum radeon_ctx_priority prior
       goto fail_alloc;
    }

+   if (ws->info.has_explicit_sync_contexts) {
+      r = amdgpu_cs_ctx_stable_pstate(ctx->ctx, AMDGPU_CTX_OP_SET_IMPLICIT_SYNC, 0, NULL);
+      if (r) {
+         fprintf(stderr, "radv/amdgpu: failed to set the context to implicit sync %d %s\n", r, strerror(r));
+         result = VK_ERROR_UNKNOWN;
+         goto fail_sync;
+      }
+   }
+
    *rctx = (struct radeon_winsys_ctx *)ctx;
    return VK_SUCCESS;

+fail_sync:
+   ctx->ws->base.buffer_destroy(&ctx->ws->base, ctx->fence_bo);
 fail_alloc:
    amdgpu_cs_ctx_free(ctx->ctx);
 fail_create:
