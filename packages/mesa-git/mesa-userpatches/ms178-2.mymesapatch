--- a/src/amd/compiler/aco_scheduler_ilp.cpp	2025-09-18 00:13:47.617964283 +0200
+++ b/src/amd/compiler/aco_scheduler_ilp.cpp	2025-09-18 00:46:20.947672554 +0200
@@ -10,23 +10,15 @@
 #include "util/macros.h"
 
 #include <limits>
-
-/*
- * This pass implements a simple forward list-scheduler which works on a small
- * partial DAG of 16 nodes at any time. Only ALU instructions are scheduled
- * entirely freely. Memory load instructions must be kept in-order and any other
- * instruction must not be re-scheduled at all.
- *
- * The main goal of this scheduler is to create more memory clauses, schedule
- * memory loads early, and to improve ALU instruction level parallelism.
- */
+#include <algorithm>
+#include <cassert>
 
 namespace aco {
 namespace {
 
 constexpr unsigned num_nodes = 16;
 using mask_t = uint16_t;
-static_assert(std::numeric_limits<mask_t>::digits >= num_nodes);
+static_assert(std::numeric_limits<mask_t>::digits >= num_nodes, "mask_t too small for DAG nodes");
 
 struct VOPDInfo {
    VOPDInfo() : can_be_opx(0), is_dst_odd(0), src_banks(0), has_literal(0), is_commutative(0) {}
@@ -41,15 +33,15 @@ struct VOPDInfo {
 };
 
 struct InstrInfo {
-   Instruction* instr;
-   int16_t wait_cycles;          /* estimated remaining cycles until instruction can be issued. */
-   mask_t dependency_mask;       /* bitmask of nodes which have to be scheduled before this node. */
-   mask_t write_for_read_mask;   /* bitmask of nodes in the DAG that have a RaW dependency. */
-   uint8_t next_non_reorderable; /* index of next non-reorderable instruction node after this one. */
+   Instruction* instr = nullptr;
+   int16_t wait_cycles = 0;         /* estimated remaining cycles until instruction can be issued. */
+   mask_t dependency_mask = 0;      /* bitmask of nodes which have to be scheduled before this node. */
+   mask_t write_for_read_mask = 0;  /* bitmask of nodes in the DAG that have a RaW dependency. */
+   uint8_t next_non_reorderable = UINT8_MAX; /* index of next non-reorderable instruction node after this one. */
 };
 
 struct RegisterInfo {
-   mask_t read_mask; /* bitmask of nodes which have to be scheduled before the next write. */
+   mask_t read_mask = 0; /* bitmask of nodes which have to be scheduled before the next write. */
    uint16_t latency : 11; /* estimated outstanding latency of last register write outside the DAG. */
    uint16_t direct_dependency : 4;     /* node that has to be scheduled before any other access. */
    uint16_t has_direct_dependency : 1; /* whether there is an unscheduled direct dependency. */
@@ -59,14 +51,14 @@ struct SchedILPContext {
    Program* program;
    bool is_vopd = false;
    InstrInfo nodes[num_nodes];
-   RegisterInfo regs[512];
+   RegisterInfo regs[512] = {};
    BITSET_DECLARE(reg_has_latency, 512) = { 0 };
    mask_t non_reorder_mask = 0; /* bitmask of instruction nodes which should not be reordered. */
    mask_t active_mask = 0;      /* bitmask of valid instruction nodes. */
    uint8_t next_non_reorderable = UINT8_MAX; /* index of next node which should not be reordered. */
    uint8_t last_non_reorderable = UINT8_MAX; /* index of last node which should not be reordered. */
-   bool potential_partial_clause; /* indicates that last_non_reorderable is the last instruction in
-                                     the DAG, meaning the clause might continue outside of it. */
+   bool potential_partial_clause = false; /* indicates that last_non_reorderable is the last instruction in
+                                             the DAG, meaning the clause might continue outside of it. */
 
    /* VOPD scheduler: */
    VOPDInfo vopd[num_nodes];
@@ -360,24 +352,29 @@ get_cycle_info_with_mem_latency(const Sc
 {
    Instruction_cycle_info cycle_info = get_cycle_info(*ctx.program, *instr);
 
-   /* Based on get_wait_counter_info in aco_statistics.cpp. */
+   /* Based on get_wait_counter_info in aco_statistics.cpp; tuned for GFX9 as a slight bias. */
+   const bool is_gfx9 = ctx.program->gfx_level == GFX9;
+
    if (instr->isVMEM() || instr->isFlatLike()) {
-      cycle_info.latency = 320;
+      cycle_info.latency = is_gfx9 ? 380 : 320;
    } else if (instr->isSMEM()) {
       if (instr->operands.empty()) {
          cycle_info.latency = 1;
-      } else if (instr->operands[0].size() == 2 ||
-                 (instr->operands[1].isConstant() &&
-                  (instr->operands.size() < 3 || instr->operands[2].isConstant()))) {
-         /* Likely cached. */
-         cycle_info.latency = 30;
       } else {
-         cycle_info.latency = 200;
+         const bool op0_is_16 = instr->operands[0].size() == 2;
+         const bool op1_const = instr->operands.size() > 1 && instr->operands[1].isConstant();
+         const bool op2_const = instr->operands.size() < 3 || (instr->operands.size() > 2 && instr->operands[2].isConstant());
+         if (op0_is_16 || (op1_const && op2_const)) {
+            /* Likely cached. */
+            cycle_info.latency = is_gfx9 ? 24 : 30;
+         } else {
+            cycle_info.latency = is_gfx9 ? 160 : 200;
+         }
       }
    } else if (instr->isLDSDIR()) {
-      cycle_info.latency = 13;
+      cycle_info.latency = is_gfx9 ? 11 : 13;
    } else if (instr->isDS()) {
-      cycle_info.latency = 20;
+      cycle_info.latency = is_gfx9 ? 22 : 20;
    }
 
    return cycle_info;
@@ -403,6 +400,9 @@ add_entry(SchedILPContext& ctx, Instruct
    entry.instr = instr;
    entry.wait_cycles = 0;
    entry.write_for_read_mask = 0;
+   entry.dependency_mask = 0;
+   entry.next_non_reorderable = UINT8_MAX;
+
    const mask_t mask = BITFIELD_BIT(idx);
    bool reorder = can_reorder(instr);
    ctx.active_mask |= mask;
@@ -421,22 +421,27 @@ add_entry(SchedILPContext& ctx, Instruct
       assert(op.isFixed());
       unsigned reg = op.physReg();
       if (reg >= max_sgpr && reg != scc && reg < min_vgpr) {
+         /* Don't reorder if we touch POPS_EXITING_WAVE_ID (control dep) */
          reorder &= reg != pops_exiting_wave_id;
          continue;
       }
 
       for (unsigned i = 0; i < op.size(); i++) {
-         RegisterInfo& reg_info = ctx.regs[reg + i];
+         unsigned r = reg + i;
+         if (r >= 512)
+            continue; /* Defensive guard; should not happen with valid RA. */
+
+         RegisterInfo& reg_info = ctx.regs[r];
 
          /* Add register reads. */
          reg_info.read_mask |= mask;
 
          if (reg_info.has_direct_dependency) {
             /* A previous dependency is still part of the DAG. */
-            ctx.nodes[ctx.regs[reg].direct_dependency].write_for_read_mask |= mask;
+            ctx.nodes[ctx.regs[r].direct_dependency].write_for_read_mask |= mask;
             entry.dependency_mask |= BITFIELD_BIT(reg_info.direct_dependency);
-         } else if (BITSET_TEST(ctx.reg_has_latency, reg + i)) {
-            entry.wait_cycles = MAX2(entry.wait_cycles, reg_info.latency);
+         } else if (BITSET_TEST(ctx.reg_has_latency, r)) {
+            entry.wait_cycles = MAX2(entry.wait_cycles, (int)reg_info.latency);
          }
       }
    }
@@ -464,7 +469,11 @@ add_entry(SchedILPContext& ctx, Instruct
    mask_t write_dep_mask = 0;
    for (const Definition& def : instr->definitions) {
       for (unsigned i = 0; i < def.size(); i++) {
-         RegisterInfo& reg_info = ctx.regs[def.physReg().reg() + i];
+         unsigned r = def.physReg().reg() + i;
+         if (r >= 512)
+            continue; /* Defensive guard; should not happen with valid RA. */
+
+         RegisterInfo& reg_info = ctx.regs[r];
 
          /* Add all previous register reads and writes to the dependencies. */
          write_dep_mask |= reg_info.read_mask;
@@ -472,7 +481,7 @@ add_entry(SchedILPContext& ctx, Instruct
 
          /* This register write is a direct dependency for all following reads. */
          reg_info.has_direct_dependency = 1;
-         reg_info.direct_dependency = idx;
+         reg_info.direct_dependency = idx & 0xF;
       }
    }
 
@@ -490,7 +499,7 @@ add_entry(SchedILPContext& ctx, Instruct
 
       /* Just don't reorder these at all. */
       if (!is_memory_instr(instr) || instr->definitions.empty() ||
-          get_sync_info(instr).semantics & semantic_volatile || ctx.is_vopd) {
+          (get_sync_info(instr).semantics & semantic_volatile) || ctx.is_vopd) {
          /* Add all previous instructions as dependencies. */
          entry.dependency_mask = ctx.active_mask & ~ctx.non_reorder_mask;
       }
@@ -553,7 +562,10 @@ remove_entry(SchedILPContext& ctx, const
          continue;
 
       for (unsigned i = 0; i < op.size(); i++) {
-         RegisterInfo& reg_info = ctx.regs[reg + i];
+         unsigned r = reg + i;
+         if (r >= 512)
+            continue;
+         RegisterInfo& reg_info = ctx.regs[r];
          reg_info.read_mask &= mask;
       }
    }
@@ -568,24 +580,26 @@ remove_entry(SchedILPContext& ctx, const
 
    for (const Definition& def : instr->definitions) {
       for (unsigned i = 0; i < def.size(); i++) {
-         unsigned reg = def.physReg().reg() + i;
-         ctx.regs[reg].read_mask &= mask;
-         if (ctx.regs[reg].has_direct_dependency && ctx.regs[reg].direct_dependency == idx) {
-            ctx.regs[reg].has_direct_dependency = false;
+         unsigned r = def.physReg().reg() + i;
+         if (r >= 512)
+            continue;
+         ctx.regs[r].read_mask &= mask;
+         if (ctx.regs[r].has_direct_dependency && ctx.regs[r].direct_dependency == idx) {
+            ctx.regs[r].has_direct_dependency = false;
             if (!ctx.is_vopd) {
-               BITSET_SET(ctx.reg_has_latency, reg);
-               ctx.regs[reg].latency = latency;
+               BITSET_SET(ctx.reg_has_latency, r);
+               ctx.regs[r].latency = (latency > 0) ? (uint16_t)latency : 0;
             }
          }
       }
    }
 
    for (unsigned i = 0; i < num_nodes; i++) {
-      ctx.nodes[i].dependency_mask &= mask;
-      ctx.nodes[i].wait_cycles -= stall;
-      if (ctx.nodes[idx].write_for_read_mask & BITFIELD_BIT(i) && !ctx.is_vopd) {
-         ctx.nodes[i].wait_cycles = MAX2(ctx.nodes[i].wait_cycles, latency);
-      }
+     ctx.nodes[i].dependency_mask &= mask;
+     ctx.nodes[i].wait_cycles -= stall;
+     if ((ctx.nodes[idx].write_for_read_mask & BITFIELD_BIT(i)) && !ctx.is_vopd) {
+        ctx.nodes[i].wait_cycles = MAX2(ctx.nodes[i].wait_cycles, (int16_t)latency);
+     }
    }
 
    if (ctx.next_non_reorderable == idx) {
@@ -660,7 +674,7 @@ select_instruction_ilp(const SchedILPCon
    bool prefer_vintrp = ctx.prev_info.instr && ctx.prev_info.instr->isVINTRP();
 
    /* Select the instruction with lowest wait_cycles of all candidates. */
-   unsigned idx = -1u;
+   unsigned idx = (unsigned)-1;
    bool idx_vintrp = false;
    int32_t wait_cycles = INT32_MAX;
    u_foreach_bit (i, mask) {
@@ -672,7 +686,7 @@ select_instruction_ilp(const SchedILPCon
 
       bool is_vintrp = prefer_vintrp && candidate.instr->isVINTRP();
 
-      if (idx == -1u || (is_vintrp && !idx_vintrp) ||
+      if (idx == (unsigned)-1 || (is_vintrp && !idx_vintrp) ||
           (is_vintrp == idx_vintrp && candidate.wait_cycles < wait_cycles)) {
          idx = i;
          idx_vintrp = is_vintrp;
@@ -680,7 +694,7 @@ select_instruction_ilp(const SchedILPCon
       }
    }
 
-   if (idx != -1u)
+   if (idx != (unsigned)-1)
       return idx;
 
    /* Select the next non-reorderable instruction. (it must have no dependencies) */
@@ -748,7 +762,7 @@ select_instruction_vopd(const SchedILPCo
    int num_vopd_odd_minus_even =
       (int)util_bitcount(ctx.vopd_odd_mask & mask) - (int)util_bitcount(ctx.vopd_even_mask & mask);
 
-   unsigned cur = -1u;
+   unsigned cur = (unsigned)-1;
    u_foreach_bit (i, mask) {
       const InstrInfo& candidate = ctx.nodes[i];
 
@@ -756,7 +770,7 @@ select_instruction_vopd(const SchedILPCo
       if (candidate.dependency_mask)
          continue;
 
-      if (cur == -1u) {
+      if (cur == (unsigned)-1) {
          cur = i;
          *vopd_compat = can_use_vopd(ctx, i);
       } else if (compare_nodes_vopd(ctx, num_vopd_odd_minus_even, vopd_compat, cur, i)) {
@@ -764,7 +778,7 @@ select_instruction_vopd(const SchedILPCo
       }
    }
 
-   assert(cur != -1u);
+   assert(cur != (unsigned)-1);
    return cur;
 }
 
@@ -773,8 +787,9 @@ get_vopd_opcode_operands(const SchedILPC
                          bool swap, aco_opcode* op, unsigned* num_operands, Operand* operands)
 {
    *op = info.op;
-   *num_operands += instr->operands.size();
+   const unsigned copy_count = instr->operands.size();
    std::copy(instr->operands.begin(), instr->operands.end(), operands);
+   *num_operands += copy_count;
 
    if (instr->opcode == aco_opcode::v_bfrev_b32) {
       operands[0] = Operand::get_const(ctx.program->gfx_level,
@@ -830,7 +845,7 @@ create_vopd_instruction(const SchedILPCo
 
    aco_opcode x_op, y_op;
    unsigned num_operands = 0;
-   Operand operands[6];
+   Operand operands[6]; /* VOP2 + VOP2 at most */
    get_vopd_opcode_operands(ctx, x, x_info, swap_x, &x_op, &num_operands, operands);
    get_vopd_opcode_operands(ctx, y, y_info, swap_y, &y_op, &num_operands, operands + num_operands);
 
@@ -848,6 +863,7 @@ void
 do_schedule(SchedILPContext& ctx, It& insert_it, It& remove_it, It instructions_begin,
             It instructions_end)
 {
+   /* Prime the DAG with up to num_nodes instructions */
    for (unsigned i = 0; i < num_nodes; i++) {
       if (remove_it == instructions_end)
          break;
@@ -864,7 +880,9 @@ do_schedule(SchedILPContext& ctx, It& in
       Instruction* next_instr = ctx.nodes[next_idx].instr;
 
       if (vopd_compat) {
-         std::prev(insert_it)->reset(create_vopd_instruction(ctx, next_idx, vopd_compat));
+         /* Replace the previously emitted instruction with a fused VOPD */
+         auto prev_it = std::prev(insert_it);
+         prev_it->reset(create_vopd_instruction(ctx, next_idx, vopd_compat));
          ctx.prev_info.instr = NULL;
       } else {
          (insert_it++)->reset(next_instr);
@@ -894,10 +912,13 @@ schedule_ilp(Program* program)
    for (Block& block : program->blocks) {
       if (block.instructions.empty())
          continue;
+
       auto it = block.instructions.begin();
       auto insert_it = block.instructions.begin();
       do_schedule(ctx, insert_it, it, block.instructions.begin(), block.instructions.end());
       block.instructions.resize(insert_it - block.instructions.begin());
+
+      /* Reset latency tracking at block ends/branches to avoid leaking inter-block timing */
       if (block.linear_succs.empty() || block.instructions.back()->opcode == aco_opcode::s_branch)
          BITSET_ZERO(ctx.reg_has_latency);
    }
@@ -913,6 +934,9 @@ schedule_vopd(Program* program)
    ctx.is_vopd = true;
 
    for (Block& block : program->blocks) {
+      if (block.instructions.empty())
+         continue;
+
       auto it = block.instructions.rbegin();
       auto insert_it = block.instructions.rbegin();
       do_schedule(ctx, insert_it, it, block.instructions.rbegin(), block.instructions.rend());


--- a/src/amd/compiler/aco_scheduler.cpp	2025-09-17 17:58:35.259844050 +0200
+++ b/src/amd/compiler/aco_scheduler.cpp	2026-01-24 21:23:13.018177664 +0200
@@ -10,6 +10,10 @@
 #include "common/amdgfxregs.h"
 
 #include <algorithm>
+#include <climits>
+#include <cstdint>
+#include <cstring>
+#include <iterator>
 #include <vector>
 
 #define SMEM_WINDOW_SIZE    (256 - ctx.occupancy_factor * 16)
@@ -23,7 +27,7 @@
 /* creating clauses decreases def-use distances, so make it less aggressive the lower num_waves is */
 #define VMEM_CLAUSE_MAX_GRAB_DIST       (ctx.occupancy_factor * 2)
 #define VMEM_STORE_CLAUSE_MAX_GRAB_DIST (ctx.occupancy_factor * 4)
-#define POS_EXP_MAX_MOVES         512
+#define POS_EXP_MAX_MOVES               512
 
 namespace aco {
 
@@ -88,7 +92,8 @@ struct MoveState {
    Instruction* current;
    bool improved_rar;
 
-   std::vector<bool> depends_on;
+   /* Use byte vector instead of vector<bool> to avoid bit-proxy overhead in this hot code. */
+   std::vector<uint8_t> depends_on;
    aco::unordered_map<uint32_t, int> rar_dependencies; /* temp-id -> index relative to insert_idx */
    MoveState() : rar_dependencies(m) {}
 
@@ -129,17 +134,49 @@ struct sched_ctx {
  * Instructions will only be moved if the register pressure won't exceed a certain bound.
  */
 
-template <typename T>
+/* Fast path for single-element moves (hot): avoid std::rotate overhead for num==1. */
+template <typename It>
+static inline void
+move_one(It begin_it, size_t idx, size_t before)
+{
+   using diff_t = typename std::iterator_traits<It>::difference_type;
+
+   if (idx < before) {
+      auto first = std::next(begin_it, (diff_t)idx);
+      auto last = std::next(begin_it, (diff_t)(before - 1));
+      auto tmp = std::move(*first);
+      std::move(std::next(first), std::next(first, (diff_t)(before - idx)), first);
+      *last = std::move(tmp);
+   } else if (idx > before) {
+      auto first = std::next(begin_it, (diff_t)before);
+      auto pos = std::next(begin_it, (diff_t)idx);
+      auto tmp = std::move(*pos);
+      std::move_backward(first, pos, std::next(pos));
+      *first = std::move(tmp);
+   }
+}
+
+template <typename It>
 void
-move_element(T begin_it, size_t idx, size_t before, int num = 1)
+move_element(It begin_it, size_t idx, size_t before, int num = 1)
 {
+   using diff_t = typename std::iterator_traits<It>::difference_type;
+
+   if (idx == before || num <= 0)
+      return;
+
+   if (num == 1) {
+      move_one(begin_it, idx, before);
+      return;
+   }
+
    if (idx < before) {
-      auto begin = std::next(begin_it, idx);
-      auto end = std::next(begin_it, before);
+      auto begin = std::next(begin_it, (diff_t)idx);
+      auto end = std::next(begin_it, (diff_t)before);
       std::rotate(begin, begin + num, end);
-   } else if (idx > before) {
-      auto begin = std::next(begin_it, before);
-      auto end = std::next(begin_it, idx + 1);
+   } else { /* idx > before */
+      auto begin = std::next(begin_it, (diff_t)before);
+      auto end = std::next(begin_it, (diff_t)(idx + 1));
       std::rotate(begin, end - num, end);
    }
 }
@@ -164,13 +201,13 @@ MoveState::downwards_init(int current_id
 {
    improved_rar = improved_rar_;
 
-   std::fill(depends_on.begin(), depends_on.end(), false);
+   std::fill(depends_on.begin(), depends_on.end(), 0);
    if (improved_rar)
       rar_dependencies.clear();
 
    for (const Operand& op : current->operands) {
       if (op.isTemp()) {
-         depends_on[op.tempId()] = true;
+         depends_on[op.tempId()] = 1;
          if (improved_rar && op.isFirstKill())
             rar_dependencies[op.tempId()] = -1;
       }
@@ -240,7 +277,7 @@ MoveState::downwards_move(DownwardsCurso
       return move_fail_pressure;
 
    /* move the candidate below the memory load */
-   move_element(block->instructions.begin(), cursor.source_idx, cursor.insert_idx);
+   move_element(block->instructions.begin(), (size_t)cursor.source_idx, (size_t)cursor.insert_idx);
    cursor.insert_idx--;
    cursor.insert_idx_clause--;
 
@@ -371,8 +408,8 @@ MoveState::downwards_move_clause(Downwar
    }
 
    /* Move the clause before the memory instruction. */
-   move_element(block->instructions.begin(), clause_begin_idx + 1, cursor.insert_idx_clause,
-                clause_size);
+   move_element(block->instructions.begin(), (size_t)(clause_begin_idx + 1),
+                (size_t)cursor.insert_idx_clause, clause_size);
 
    cursor.source_idx = clause_begin_idx;
    cursor.insert_idx_clause -= clause_size;
@@ -388,7 +425,7 @@ MoveState::downwards_skip(DownwardsCurso
 
    for (const Operand& op : instr->operands) {
       if (op.isTemp()) {
-         depends_on[op.tempId()] = true;
+         depends_on[op.tempId()] = 1;
          if (improved_rar && op.isFirstKill())
             rar_dependencies[op.tempId()] = cursor.source_idx - cursor.insert_idx;
       }
@@ -421,12 +458,12 @@ MoveState::upwards_init(int source_idx,
 {
    improved_rar = improved_rar_;
 
-   std::fill(depends_on.begin(), depends_on.end(), false);
+   std::fill(depends_on.begin(), depends_on.end(), 0);
    rar_dependencies.clear();
 
    for (const Definition& def : current->definitions) {
       if (def.isTemp())
-         depends_on[def.tempId()] = true;
+         depends_on[def.tempId()] = 1;
    }
 
    return UpwardsCursor(source_idx);
@@ -480,7 +517,7 @@ MoveState::upwards_move(UpwardsCursor& c
       return move_fail_pressure;
 
    /* move the candidate above the insert_idx */
-   move_element(block->instructions.begin(), cursor.source_idx, cursor.insert_idx);
+   move_element(block->instructions.begin(), (size_t)cursor.source_idx, (size_t)cursor.insert_idx);
 
    /* update register pressure */
    block->instructions[cursor.insert_idx]->register_demand = new_demand;
@@ -504,7 +541,7 @@ MoveState::upwards_skip(UpwardsCursor& c
       aco_ptr<Instruction>& instr = block->instructions[cursor.source_idx];
       for (const Definition& def : instr->definitions) {
          if (def.isTemp())
-            depends_on[def.tempId()] = true;
+            depends_on[def.tempId()] = 1;
       }
       for (const Operand& op : instr->operands) {
          if (op.isTemp())
@@ -531,7 +568,7 @@ get_sync_info_with_hack(const Instructio
    return sync;
 }
 
-bool
+static constexpr inline bool
 is_reorderable(const Instruction* instr)
 {
    return instr->opcode != aco_opcode::s_memtime && instr->opcode != aco_opcode::s_memrealtime &&
@@ -582,7 +619,7 @@ init_hazard_query(const sched_ctx& ctx,
    query->contains_sendmsg = false;
    query->uses_exec = false;
    query->writes_exec = false;
-   memset(&query->mem_events, 0, sizeof(query->mem_events));
+   std::memset(&query->mem_events, 0, sizeof(query->mem_events));
    query->aliasing_storage = 0;
    query->aliasing_storage_smem = 0;
 }
@@ -706,7 +743,7 @@ perform_hazard_query(hazard_query* query
       return hazard_fail_export;
 
    memory_event_set instr_set;
-   memset(&instr_set, 0, sizeof(instr_set));
+   std::memset(&instr_set, 0, sizeof(instr_set));
    memory_sync_info sync = get_sync_info_with_hack(instr);
    add_memory_event(query->program, &instr_set, instr, &sync);
 
@@ -767,13 +804,13 @@ perform_hazard_query(hazard_query* query
    return hazard_success;
 }
 
-unsigned
-get_likely_cost(Instruction* instr)
+static constexpr inline unsigned
+get_likely_cost(const Instruction* instr)
 {
    if (instr->opcode == aco_opcode::p_split_vector ||
        instr->opcode == aco_opcode::p_extract_vector) {
       unsigned cost = 0;
-      for (Definition def : instr->definitions) {
+      for (const Definition& def : instr->definitions) {
          if (instr->operands[0].isKill() &&
              def.regClass().type() == instr->operands[0].regClass().type())
             continue;
@@ -782,7 +819,7 @@ get_likely_cost(Instruction* instr)
       return cost;
    } else if (instr->opcode == aco_opcode::p_create_vector) {
       unsigned cost = 0;
-      for (Operand op : instr->operands) {
+      for (const Operand& op : instr->operands) {
          if (op.isTemp() && op.isFirstKill() &&
              op.regClass().type() == instr->definitions[0].regClass().type())
             continue;
@@ -795,6 +832,13 @@ get_likely_cost(Instruction* instr)
    }
 }
 
+static constexpr inline int
+vgpr_headroom(const RegisterDemand& max_regs, const RegisterDemand& demand)
+{
+   const int margin = max_regs.vgpr - demand.vgpr;
+   return margin > 0 ? margin : 0;
+}
+
 void
 schedule_SMEM(sched_ctx& ctx, Block* block, Instruction* current, int idx)
 {
@@ -816,6 +860,15 @@ schedule_SMEM(sched_ctx& ctx, Block* blo
 
    DownwardsCursor cursor = ctx.mv.downwards_init(idx, false);
 
+   /* Vega/GFX9: be a bit more aggressive if we have plenty of VGPR headroom. */
+   if (ctx.gfx_level <= GFX9) {
+      const int margin = vgpr_headroom(ctx.mv.max_registers, cursor.insert_demand);
+      if (margin >= 32) {
+         window_size = window_size + window_size / 8;
+         max_moves = max_moves + max_moves / 10;
+      }
+   }
+
    for (int candidate_idx = idx - 1; k < max_moves && candidate_idx > (int)idx - window_size;
         candidate_idx--) {
       assert(candidate_idx >= 0);
@@ -831,15 +884,17 @@ schedule_SMEM(sched_ctx& ctx, Block* blo
       /* break when encountering another MEM instruction, logical_start or barriers */
       if (!is_reorderable(candidate.get()))
          break;
+
       /* only move VMEM instructions below descriptor loads. be more aggressive at higher num_waves
        * to help create more vmem clauses */
       if ((candidate->isVMEM() || candidate->isFlatLike()) &&
           (cursor.insert_idx - cursor.source_idx > (ctx.occupancy_factor * 4) ||
-           current->operands[0].size() == 4))
+           (!current->operands.empty() && current->operands[0].size() == 4)))
          break;
+
       /* don't move descriptor loads below buffer loads */
-      if (candidate->isSMEM() && !candidate->operands.empty() && current->operands[0].size() == 4 &&
-          candidate->operands[0].size() == 2)
+      if (candidate->isSMEM() && !candidate->operands.empty() && !current->operands.empty() &&
+          current->operands[0].size() == 4 && candidate->operands[0].size() == 2)
          break;
 
       bool can_move_down = true;
@@ -958,6 +1013,16 @@ schedule_VMEM(sched_ctx& ctx, Block* blo
 
    DownwardsCursor cursor = ctx.mv.downwards_init(idx, true);
 
+   /* Vega/GFX9: slightly increase search window/grab distance when VGPR headroom is good. */
+   if (ctx.gfx_level <= GFX9) {
+      const int margin = vgpr_headroom(ctx.mv.max_registers, cursor.insert_demand);
+      if (margin >= 32) {
+         clause_max_grab_dist = VMEM_CLAUSE_MAX_GRAB_DIST + 8;
+         window_size = window_size + window_size / 8;
+         max_moves = max_moves + max_moves / 10;
+      }
+   }
+
    for (int candidate_idx = idx - 1; k < max_moves && candidate_idx > (int)idx - window_size;
         candidate_idx--) {
       assert(candidate_idx == cursor.source_idx);
@@ -1066,7 +1131,7 @@ schedule_VMEM(sched_ctx& ctx, Block* blo
          /* don't move up dependencies of other VMEM instructions */
          for (const Definition& def : candidate->definitions) {
             if (def.isTemp())
-               ctx.mv.depends_on[def.tempId()] = true;
+               ctx.mv.depends_on[def.tempId()] = 1;
          }
       }
 
@@ -1173,8 +1238,8 @@ void
 schedule_position_export(sched_ctx& ctx, Block* block, Instruction* current, int idx)
 {
    assert(idx != 0);
-   int window_size = POS_EXP_WINDOW_SIZE / ctx.schedule_pos_export_div;
-   int max_moves = POS_EXP_MAX_MOVES / ctx.schedule_pos_export_div;
+   int window_size = POS_EXP_WINDOW_SIZE / (int)ctx.schedule_pos_export_div;
+   int max_moves = POS_EXP_MAX_MOVES / (int)ctx.schedule_pos_export_div;
    int16_t k = 0;
 
    DownwardsCursor cursor = ctx.mv.downwards_init(idx, true);
@@ -1230,6 +1295,9 @@ schedule_VMEM_store(sched_ctx& ctx, Bloc
    DownwardsCursor cursor = ctx.mv.downwards_init(idx, true);
 
    for (int16_t k = 0; k < VMEM_STORE_CLAUSE_MAX_GRAB_DIST;) {
+      if (cursor.source_idx < 0)
+         break;
+
       aco_ptr<Instruction>& candidate = block->instructions[cursor.source_idx];
       if (!is_reorderable(candidate.get()))
          break;
@@ -1245,7 +1313,7 @@ schedule_VMEM_store(sched_ctx& ctx, Bloc
 
       add_to_hazard_query(&hq, candidate.get());
       ctx.mv.downwards_skip(cursor);
-      k += get_likely_cost(candidate.get());
+      k += (int16_t)get_likely_cost(candidate.get());
    }
 }
 
@@ -1268,31 +1336,31 @@ schedule_block(sched_ctx& ctx, Program*
          unsigned target = current->exp().dest;
          if (target >= V_008DFC_SQ_EXP_POS && target < V_008DFC_SQ_EXP_PRIM) {
             ctx.mv.current = current;
-            schedule_position_export(ctx, block, current, idx);
+            schedule_position_export(ctx, block, current, (int)idx);
          }
       }
 
       if (current->definitions.empty()) {
          if ((current->isVMEM() || current->isFlatLike()) && program->gfx_level >= GFX11) {
             ctx.mv.current = current;
-            schedule_VMEM_store(ctx, block, current, idx);
+            schedule_VMEM_store(ctx, block, current, (int)idx);
          }
          continue;
       }
 
       if (current->isVMEM() || current->isFlatLike()) {
          ctx.mv.current = current;
-         schedule_VMEM(ctx, block, current, idx);
+         schedule_VMEM(ctx, block, current, (int)idx);
       }
 
       if (current->isSMEM()) {
          ctx.mv.current = current;
-         schedule_SMEM(ctx, block, current, idx);
+         schedule_SMEM(ctx, block, current, (int)idx);
       }
 
       if (current->isLDSDIR() || (current->isDS() && !current->ds().gds)) {
          ctx.mv.current = current;
-         schedule_LDS(ctx, block, current, idx);
+         schedule_LDS(ctx, block, current, (int)idx);
       }
    }
 
@@ -1328,7 +1396,7 @@ schedule_program(Program* program)
    /* If we already have less waves than the minimum, don't reduce them further.
     * Otherwise, sacrifice some waves and use more VGPRs, in order to improve scheduling.
     */
-   int vgpr_demand = std::max<int>(24, usage.vgpr) + 12 * reg_file_multiple;
+   int vgpr_demand = std::max<int>(24, usage.vgpr) + (int)(12 * reg_file_multiple);
    int target_waves = std::max(wave_minimum, program->dev.physical_vgprs / vgpr_demand);
    target_waves = max_suitable_waves(program, std::min<int>(program->num_waves, target_waves));
    assert(target_waves >= program->min_waves);
