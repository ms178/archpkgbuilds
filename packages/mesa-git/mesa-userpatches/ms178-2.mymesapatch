--- a/src/amd/compiler/aco_scheduler_ilp.cpp	2025-09-18 00:13:47.617964283 +0200
+++ b/src/amd/compiler/aco_scheduler_ilp.cpp	2025-09-18 00:46:20.947672554 +0200
@@ -10,23 +10,15 @@
 #include "util/macros.h"
 
 #include <limits>
-
-/*
- * This pass implements a simple forward list-scheduler which works on a small
- * partial DAG of 16 nodes at any time. Only ALU instructions are scheduled
- * entirely freely. Memory load instructions must be kept in-order and any other
- * instruction must not be re-scheduled at all.
- *
- * The main goal of this scheduler is to create more memory clauses, schedule
- * memory loads early, and to improve ALU instruction level parallelism.
- */
+#include <algorithm>
+#include <cassert>
 
 namespace aco {
 namespace {
 
 constexpr unsigned num_nodes = 16;
 using mask_t = uint16_t;
-static_assert(std::numeric_limits<mask_t>::digits >= num_nodes);
+static_assert(std::numeric_limits<mask_t>::digits >= num_nodes, "mask_t too small for DAG nodes");
 
 struct VOPDInfo {
    VOPDInfo() : can_be_opx(0), is_dst_odd(0), src_banks(0), has_literal(0), is_commutative(0) {}
@@ -41,15 +33,15 @@ struct VOPDInfo {
 };
 
 struct InstrInfo {
-   Instruction* instr;
-   int16_t wait_cycles;          /* estimated remaining cycles until instruction can be issued. */
-   mask_t dependency_mask;       /* bitmask of nodes which have to be scheduled before this node. */
-   mask_t write_for_read_mask;   /* bitmask of nodes in the DAG that have a RaW dependency. */
-   uint8_t next_non_reorderable; /* index of next non-reorderable instruction node after this one. */
+   Instruction* instr = nullptr;
+   int16_t wait_cycles = 0;         /* estimated remaining cycles until instruction can be issued. */
+   mask_t dependency_mask = 0;      /* bitmask of nodes which have to be scheduled before this node. */
+   mask_t write_for_read_mask = 0;  /* bitmask of nodes in the DAG that have a RaW dependency. */
+   uint8_t next_non_reorderable = UINT8_MAX; /* index of next non-reorderable instruction node after this one. */
 };
 
 struct RegisterInfo {
-   mask_t read_mask; /* bitmask of nodes which have to be scheduled before the next write. */
+   mask_t read_mask = 0; /* bitmask of nodes which have to be scheduled before the next write. */
    uint16_t latency : 11; /* estimated outstanding latency of last register write outside the DAG. */
    uint16_t direct_dependency : 4;     /* node that has to be scheduled before any other access. */
    uint16_t has_direct_dependency : 1; /* whether there is an unscheduled direct dependency. */
@@ -59,14 +51,14 @@ struct SchedILPContext {
    Program* program;
    bool is_vopd = false;
    InstrInfo nodes[num_nodes];
-   RegisterInfo regs[512];
+   RegisterInfo regs[512] = {};
    BITSET_DECLARE(reg_has_latency, 512) = { 0 };
    mask_t non_reorder_mask = 0; /* bitmask of instruction nodes which should not be reordered. */
    mask_t active_mask = 0;      /* bitmask of valid instruction nodes. */
    uint8_t next_non_reorderable = UINT8_MAX; /* index of next node which should not be reordered. */
    uint8_t last_non_reorderable = UINT8_MAX; /* index of last node which should not be reordered. */
-   bool potential_partial_clause; /* indicates that last_non_reorderable is the last instruction in
-                                     the DAG, meaning the clause might continue outside of it. */
+   bool potential_partial_clause = false; /* indicates that last_non_reorderable is the last instruction in
+                                             the DAG, meaning the clause might continue outside of it. */
 
    /* VOPD scheduler: */
    VOPDInfo vopd[num_nodes];
@@ -360,24 +352,29 @@ get_cycle_info_with_mem_latency(const Sc
 {
    Instruction_cycle_info cycle_info = get_cycle_info(*ctx.program, *instr);
 
-   /* Based on get_wait_counter_info in aco_statistics.cpp. */
+   /* Based on get_wait_counter_info in aco_statistics.cpp; tuned for GFX9 as a slight bias. */
+   const bool is_gfx9 = ctx.program->gfx_level == GFX9;
+
    if (instr->isVMEM() || instr->isFlatLike()) {
-      cycle_info.latency = 320;
+      cycle_info.latency = is_gfx9 ? 380 : 320;
    } else if (instr->isSMEM()) {
       if (instr->operands.empty()) {
          cycle_info.latency = 1;
-      } else if (instr->operands[0].size() == 2 ||
-                 (instr->operands[1].isConstant() &&
-                  (instr->operands.size() < 3 || instr->operands[2].isConstant()))) {
-         /* Likely cached. */
-         cycle_info.latency = 30;
       } else {
-         cycle_info.latency = 200;
+         const bool op0_is_16 = instr->operands[0].size() == 2;
+         const bool op1_const = instr->operands.size() > 1 && instr->operands[1].isConstant();
+         const bool op2_const = instr->operands.size() < 3 || (instr->operands.size() > 2 && instr->operands[2].isConstant());
+         if (op0_is_16 || (op1_const && op2_const)) {
+            /* Likely cached. */
+            cycle_info.latency = is_gfx9 ? 24 : 30;
+         } else {
+            cycle_info.latency = is_gfx9 ? 160 : 200;
+         }
       }
    } else if (instr->isLDSDIR()) {
-      cycle_info.latency = 13;
+      cycle_info.latency = is_gfx9 ? 11 : 13;
    } else if (instr->isDS()) {
-      cycle_info.latency = 20;
+      cycle_info.latency = is_gfx9 ? 22 : 20;
    }
 
    return cycle_info;
@@ -403,6 +400,9 @@ add_entry(SchedILPContext& ctx, Instruct
    entry.instr = instr;
    entry.wait_cycles = 0;
    entry.write_for_read_mask = 0;
+   entry.dependency_mask = 0;
+   entry.next_non_reorderable = UINT8_MAX;
+
    const mask_t mask = BITFIELD_BIT(idx);
    bool reorder = can_reorder(instr);
    ctx.active_mask |= mask;
@@ -421,22 +421,27 @@ add_entry(SchedILPContext& ctx, Instruct
       assert(op.isFixed());
       unsigned reg = op.physReg();
       if (reg >= max_sgpr && reg != scc && reg < min_vgpr) {
+         /* Don't reorder if we touch POPS_EXITING_WAVE_ID (control dep) */
          reorder &= reg != pops_exiting_wave_id;
          continue;
       }
 
       for (unsigned i = 0; i < op.size(); i++) {
-         RegisterInfo& reg_info = ctx.regs[reg + i];
+         unsigned r = reg + i;
+         if (r >= 512)
+            continue; /* Defensive guard; should not happen with valid RA. */
+
+         RegisterInfo& reg_info = ctx.regs[r];
 
          /* Add register reads. */
          reg_info.read_mask |= mask;
 
          if (reg_info.has_direct_dependency) {
             /* A previous dependency is still part of the DAG. */
-            ctx.nodes[ctx.regs[reg].direct_dependency].write_for_read_mask |= mask;
+            ctx.nodes[ctx.regs[r].direct_dependency].write_for_read_mask |= mask;
             entry.dependency_mask |= BITFIELD_BIT(reg_info.direct_dependency);
-         } else if (BITSET_TEST(ctx.reg_has_latency, reg + i)) {
-            entry.wait_cycles = MAX2(entry.wait_cycles, reg_info.latency);
+         } else if (BITSET_TEST(ctx.reg_has_latency, r)) {
+            entry.wait_cycles = MAX2(entry.wait_cycles, (int)reg_info.latency);
          }
       }
    }
@@ -464,7 +469,11 @@ add_entry(SchedILPContext& ctx, Instruct
    mask_t write_dep_mask = 0;
    for (const Definition& def : instr->definitions) {
       for (unsigned i = 0; i < def.size(); i++) {
-         RegisterInfo& reg_info = ctx.regs[def.physReg().reg() + i];
+         unsigned r = def.physReg().reg() + i;
+         if (r >= 512)
+            continue; /* Defensive guard; should not happen with valid RA. */
+
+         RegisterInfo& reg_info = ctx.regs[r];
 
          /* Add all previous register reads and writes to the dependencies. */
          write_dep_mask |= reg_info.read_mask;
@@ -472,7 +481,7 @@ add_entry(SchedILPContext& ctx, Instruct
 
          /* This register write is a direct dependency for all following reads. */
          reg_info.has_direct_dependency = 1;
-         reg_info.direct_dependency = idx;
+         reg_info.direct_dependency = idx & 0xF;
       }
    }
 
@@ -490,7 +499,7 @@ add_entry(SchedILPContext& ctx, Instruct
 
       /* Just don't reorder these at all. */
       if (!is_memory_instr(instr) || instr->definitions.empty() ||
-          get_sync_info(instr).semantics & semantic_volatile || ctx.is_vopd) {
+          (get_sync_info(instr).semantics & semantic_volatile) || ctx.is_vopd) {
          /* Add all previous instructions as dependencies. */
          entry.dependency_mask = ctx.active_mask & ~ctx.non_reorder_mask;
       }
@@ -553,7 +562,10 @@ remove_entry(SchedILPContext& ctx, const
          continue;
 
       for (unsigned i = 0; i < op.size(); i++) {
-         RegisterInfo& reg_info = ctx.regs[reg + i];
+         unsigned r = reg + i;
+         if (r >= 512)
+            continue;
+         RegisterInfo& reg_info = ctx.regs[r];
          reg_info.read_mask &= mask;
       }
    }
@@ -568,24 +580,26 @@ remove_entry(SchedILPContext& ctx, const
 
    for (const Definition& def : instr->definitions) {
       for (unsigned i = 0; i < def.size(); i++) {
-         unsigned reg = def.physReg().reg() + i;
-         ctx.regs[reg].read_mask &= mask;
-         if (ctx.regs[reg].has_direct_dependency && ctx.regs[reg].direct_dependency == idx) {
-            ctx.regs[reg].has_direct_dependency = false;
+         unsigned r = def.physReg().reg() + i;
+         if (r >= 512)
+            continue;
+         ctx.regs[r].read_mask &= mask;
+         if (ctx.regs[r].has_direct_dependency && ctx.regs[r].direct_dependency == idx) {
+            ctx.regs[r].has_direct_dependency = false;
             if (!ctx.is_vopd) {
-               BITSET_SET(ctx.reg_has_latency, reg);
-               ctx.regs[reg].latency = latency;
+               BITSET_SET(ctx.reg_has_latency, r);
+               ctx.regs[r].latency = (latency > 0) ? (uint16_t)latency : 0;
             }
          }
       }
    }
 
    for (unsigned i = 0; i < num_nodes; i++) {
-      ctx.nodes[i].dependency_mask &= mask;
-      ctx.nodes[i].wait_cycles -= stall;
-      if (ctx.nodes[idx].write_for_read_mask & BITFIELD_BIT(i) && !ctx.is_vopd) {
-         ctx.nodes[i].wait_cycles = MAX2(ctx.nodes[i].wait_cycles, latency);
-      }
+     ctx.nodes[i].dependency_mask &= mask;
+     ctx.nodes[i].wait_cycles -= stall;
+     if ((ctx.nodes[idx].write_for_read_mask & BITFIELD_BIT(i)) && !ctx.is_vopd) {
+        ctx.nodes[i].wait_cycles = MAX2(ctx.nodes[i].wait_cycles, (int16_t)latency);
+     }
    }
 
    if (ctx.next_non_reorderable == idx) {
@@ -660,7 +674,7 @@ select_instruction_ilp(const SchedILPCon
    bool prefer_vintrp = ctx.prev_info.instr && ctx.prev_info.instr->isVINTRP();
 
    /* Select the instruction with lowest wait_cycles of all candidates. */
-   unsigned idx = -1u;
+   unsigned idx = (unsigned)-1;
    bool idx_vintrp = false;
    int32_t wait_cycles = INT32_MAX;
    u_foreach_bit (i, mask) {
@@ -672,7 +686,7 @@ select_instruction_ilp(const SchedILPCon
 
       bool is_vintrp = prefer_vintrp && candidate.instr->isVINTRP();
 
-      if (idx == -1u || (is_vintrp && !idx_vintrp) ||
+      if (idx == (unsigned)-1 || (is_vintrp && !idx_vintrp) ||
           (is_vintrp == idx_vintrp && candidate.wait_cycles < wait_cycles)) {
          idx = i;
          idx_vintrp = is_vintrp;
@@ -680,7 +694,7 @@ select_instruction_ilp(const SchedILPCon
       }
    }
 
-   if (idx != -1u)
+   if (idx != (unsigned)-1)
       return idx;
 
    /* Select the next non-reorderable instruction. (it must have no dependencies) */
@@ -748,7 +762,7 @@ select_instruction_vopd(const SchedILPCo
    int num_vopd_odd_minus_even =
       (int)util_bitcount(ctx.vopd_odd_mask & mask) - (int)util_bitcount(ctx.vopd_even_mask & mask);
 
-   unsigned cur = -1u;
+   unsigned cur = (unsigned)-1;
    u_foreach_bit (i, mask) {
       const InstrInfo& candidate = ctx.nodes[i];
 
@@ -756,7 +770,7 @@ select_instruction_vopd(const SchedILPCo
       if (candidate.dependency_mask)
          continue;
 
-      if (cur == -1u) {
+      if (cur == (unsigned)-1) {
          cur = i;
          *vopd_compat = can_use_vopd(ctx, i);
       } else if (compare_nodes_vopd(ctx, num_vopd_odd_minus_even, vopd_compat, cur, i)) {
@@ -764,7 +778,7 @@ select_instruction_vopd(const SchedILPCo
       }
    }
 
-   assert(cur != -1u);
+   assert(cur != (unsigned)-1);
    return cur;
 }
 
@@ -773,8 +787,9 @@ get_vopd_opcode_operands(const SchedILPC
                          bool swap, aco_opcode* op, unsigned* num_operands, Operand* operands)
 {
    *op = info.op;
-   *num_operands += instr->operands.size();
+   const unsigned copy_count = instr->operands.size();
    std::copy(instr->operands.begin(), instr->operands.end(), operands);
+   *num_operands += copy_count;
 
    if (instr->opcode == aco_opcode::v_bfrev_b32) {
       operands[0] = Operand::get_const(ctx.program->gfx_level,
@@ -830,7 +845,7 @@ create_vopd_instruction(const SchedILPCo
 
    aco_opcode x_op, y_op;
    unsigned num_operands = 0;
-   Operand operands[6];
+   Operand operands[6]; /* VOP2 + VOP2 at most */
    get_vopd_opcode_operands(ctx, x, x_info, swap_x, &x_op, &num_operands, operands);
    get_vopd_opcode_operands(ctx, y, y_info, swap_y, &y_op, &num_operands, operands + num_operands);
 
@@ -848,6 +863,7 @@ void
 do_schedule(SchedILPContext& ctx, It& insert_it, It& remove_it, It instructions_begin,
             It instructions_end)
 {
+   /* Prime the DAG with up to num_nodes instructions */
    for (unsigned i = 0; i < num_nodes; i++) {
       if (remove_it == instructions_end)
          break;
@@ -864,7 +880,9 @@ do_schedule(SchedILPContext& ctx, It& in
       Instruction* next_instr = ctx.nodes[next_idx].instr;
 
       if (vopd_compat) {
-         std::prev(insert_it)->reset(create_vopd_instruction(ctx, next_idx, vopd_compat));
+         /* Replace the previously emitted instruction with a fused VOPD */
+         auto prev_it = std::prev(insert_it);
+         prev_it->reset(create_vopd_instruction(ctx, next_idx, vopd_compat));
          ctx.prev_info.instr = NULL;
       } else {
          (insert_it++)->reset(next_instr);
@@ -894,10 +912,13 @@ schedule_ilp(Program* program)
    for (Block& block : program->blocks) {
       if (block.instructions.empty())
          continue;
+
       auto it = block.instructions.begin();
       auto insert_it = block.instructions.begin();
       do_schedule(ctx, insert_it, it, block.instructions.begin(), block.instructions.end());
       block.instructions.resize(insert_it - block.instructions.begin());
+
+      /* Reset latency tracking at block ends/branches to avoid leaking inter-block timing */
       if (block.linear_succs.empty() || block.instructions.back()->opcode == aco_opcode::s_branch)
          BITSET_ZERO(ctx.reg_has_latency);
    }
@@ -913,6 +934,9 @@ schedule_vopd(Program* program)
    ctx.is_vopd = true;
 
    for (Block& block : program->blocks) {
+      if (block.instructions.empty())
+         continue;
+
       auto it = block.instructions.rbegin();
       auto insert_it = block.instructions.rbegin();
       do_schedule(ctx, insert_it, it, block.instructions.rbegin(), block.instructions.rend());


--- a/src/amd/compiler/aco_scheduler.cpp	2025-09-17 17:58:35.259844050 +0200
+++ b/src/amd/compiler/aco_scheduler.cpp	2025-09-17 21:23:13.018177664 +0200
@@ -11,6 +11,9 @@
 
 #include <algorithm>
 #include <vector>
+#include <cstring>
+#include <cassert>
+#include <unordered_map>
 
 #define SMEM_WINDOW_SIZE    (256 - ctx.occupancy_factor * 16)
 #define VMEM_WINDOW_SIZE    (1024 - ctx.occupancy_factor * 64)
@@ -25,10 +28,47 @@
 #define VMEM_STORE_CLAUSE_MAX_GRAB_DIST (ctx.occupancy_factor * 4)
 #define POS_EXP_MAX_MOVES         512
 
+#if defined(__clang__) || defined(__GNUC__)
+#define ACO_LIKELY(x)   __builtin_expect(!!(x), 1)
+#define ACO_UNLIKELY(x) __builtin_expect(!!(x), 0)
+#else
+#define ACO_LIKELY(x)   (x)
+#define ACO_UNLIKELY(x) (x)
+#endif
+
 namespace aco {
 
 namespace {
 
+/* Epoch-based set with O(1) clear semantics for hot dependency tracking */
+struct EpochSet {
+   std::vector<uint32_t> tag;
+   uint32_t epoch = 1;
+
+   void resize(size_t n) {
+      tag.assign(n, 0u);
+      epoch = 1;
+   }
+
+   inline void clear_all() {
+      /* Epoch wrap protection: rare, handled by zeroing tags */
+      if (++epoch == 0u) {
+         epoch = 1u;
+         std::fill(tag.begin(), tag.end(), 0u);
+      }
+   }
+
+   inline void set(unsigned id) {
+      /* Caller guarantees id < tag.size() */
+      tag[id] = epoch;
+   }
+
+   inline bool test(unsigned id) const {
+      /* Caller guarantees id < tag.size() */
+      return tag[id] == epoch;
+   }
+};
+
 enum MoveResult {
    move_success,
    move_fail_ssa,
@@ -87,12 +127,12 @@ struct MoveState {
    Instruction* current;
    bool improved_rar;
 
-   std::vector<bool> depends_on;
+   EpochSet depends_on;
    /* Two are needed because, for downwards VMEM scheduling, one needs to
     * exclude the instructions in the clause, since new instructions in the
     * clause are not moved past any other instructions in the clause. */
-   std::vector<bool> RAR_dependencies;
-   std::vector<bool> RAR_dependencies_clause;
+   EpochSet RAR_dependencies;
+   EpochSet RAR_dependencies_clause;
 
    /* for moving instructions before the current instruction to after it */
    DownwardsCursor downwards_init(int current_idx, bool improved_rar, bool may_form_clauses);
@@ -118,6 +158,9 @@ struct sched_ctx {
    MoveState mv;
    bool schedule_pos_exports = true;
    unsigned schedule_pos_export_div = 1;
+
+   /* Per-block hazard cache keyed by Instruction*. */
+   std::unordered_map<const Instruction*, struct CachedHazard> hazard_cache;
 };
 
 /* This scheduler is a simple bottom-up pass based on ideas from
@@ -130,10 +173,37 @@ struct sched_ctx {
  * Instructions will only be moved if the register pressure won't exceed a certain bound.
  */
 
+template <typename It>
+static inline void
+move_one(It begin_it, size_t idx, size_t before)
+{
+   if (idx < before) {
+      auto first = std::next(begin_it, idx);
+      auto last  = std::next(begin_it, before - 1);
+      auto tmp   = std::move(*first);
+      std::move(std::next(first), std::next(first, (before - idx)), first);
+      *last = std::move(tmp);
+   } else if (idx > before) {
+      auto first = std::next(begin_it, before);
+      auto pos   = std::next(begin_it, idx);
+      auto tmp   = std::move(*pos);
+      std::move_backward(first, pos, std::next(pos));
+      *first = std::move(tmp);
+   }
+}
+
 template <typename T>
 void
 move_element(T begin_it, size_t idx, size_t before, int num = 1)
 {
+   if (idx == before || num <= 0)
+      return;
+
+   if (num == 1) {
+      move_one(begin_it, idx, before);
+      return;
+   }
+
    if (idx < before) {
       auto begin = std::next(begin_it, idx);
       auto end = std::next(begin_it, before);
@@ -165,18 +235,18 @@ MoveState::downwards_init(int current_id
 {
    improved_rar = improved_rar_;
 
-   std::fill(depends_on.begin(), depends_on.end(), false);
+   depends_on.clear_all();
    if (improved_rar) {
-      std::fill(RAR_dependencies.begin(), RAR_dependencies.end(), false);
+      RAR_dependencies.clear_all();
       if (may_form_clauses)
-         std::fill(RAR_dependencies_clause.begin(), RAR_dependencies_clause.end(), false);
+         RAR_dependencies_clause.clear_all();
    }
 
    for (const Operand& op : current->operands) {
       if (op.isTemp()) {
-         depends_on[op.tempId()] = true;
+         depends_on.set(op.tempId());
          if (improved_rar && op.isFirstKill())
-            RAR_dependencies[op.tempId()] = true;
+            RAR_dependencies.set(op.tempId());
       }
    }
 
@@ -188,15 +258,15 @@ MoveState::downwards_init(int current_id
    return cursor;
 }
 
-bool
-check_dependencies(Instruction* instr, std::vector<bool>& def_dep, std::vector<bool>& op_dep)
+static inline bool
+check_dependencies(Instruction* instr, EpochSet& def_dep, EpochSet& op_dep)
 {
    for (const Definition& def : instr->definitions) {
-      if (def.isTemp() && def_dep[def.tempId()])
+      if (def.isTemp() && def_dep.test(def.tempId()))
          return true;
    }
    for (const Operand& op : instr->operands) {
-      if (op.isTemp() && op_dep[op.tempId()]) {
+      if (op.isTemp() && op_dep.test(op.tempId())) {
          // FIXME: account for difference in register pressure
          return true;
       }
@@ -211,7 +281,7 @@ MoveState::downwards_move(DownwardsCurso
    aco_ptr<Instruction>& candidate = block->instructions[cursor.source_idx];
 
    /* check if one of candidate's operands is killed by depending instruction */
-   std::vector<bool>& RAR_deps = improved_rar ? RAR_dependencies : depends_on;
+   EpochSet& RAR_deps = improved_rar ? RAR_dependencies : depends_on;
    if (check_dependencies(candidate.get(), depends_on, RAR_deps))
       return move_fail_ssa;
 
@@ -330,10 +400,10 @@ MoveState::downwards_skip(DownwardsCurso
 
    for (const Operand& op : instr->operands) {
       if (op.isTemp()) {
-         depends_on[op.tempId()] = true;
+         depends_on.set(op.tempId());
          if (improved_rar && op.isFirstKill()) {
-            RAR_dependencies[op.tempId()] = true;
-            RAR_dependencies_clause[op.tempId()] = true;
+            RAR_dependencies.set(op.tempId());
+            RAR_dependencies_clause.set(op.tempId());
          }
       }
    }
@@ -365,12 +435,12 @@ MoveState::upwards_init(int source_idx,
 {
    improved_rar = improved_rar_;
 
-   std::fill(depends_on.begin(), depends_on.end(), false);
-   std::fill(RAR_dependencies.begin(), RAR_dependencies.end(), false);
+   depends_on.clear_all();
+   RAR_dependencies.clear_all();
 
    for (const Definition& def : current->definitions) {
       if (def.isTemp())
-         depends_on[def.tempId()] = true;
+         depends_on.set(def.tempId());
    }
 
    return UpwardsCursor(source_idx);
@@ -381,7 +451,7 @@ MoveState::upwards_check_deps(UpwardsCur
 {
    aco_ptr<Instruction>& instr = block->instructions[cursor.source_idx];
    for (const Operand& op : instr->operands) {
-      if (op.isTemp() && depends_on[op.tempId()])
+      if (op.isTemp() && depends_on.test(op.tempId()))
          return false;
    }
    return true;
@@ -403,13 +473,13 @@ MoveState::upwards_move(UpwardsCursor& c
 
    aco_ptr<Instruction>& instr = block->instructions[cursor.source_idx];
    for (const Operand& op : instr->operands) {
-      if (op.isTemp() && depends_on[op.tempId()])
+      if (op.isTemp() && depends_on.test(op.tempId()))
          return move_fail_ssa;
    }
 
    /* check if candidate uses/kills an operand which is used by a dependency */
    for (const Operand& op : instr->operands) {
-      if (op.isTemp() && (!improved_rar || op.isFirstKill()) && RAR_dependencies[op.tempId()])
+      if (op.isTemp() && (!improved_rar || op.isFirstKill()) && RAR_dependencies.test(op.tempId()))
          return move_fail_rar;
    }
 
@@ -448,11 +518,11 @@ MoveState::upwards_skip(UpwardsCursor& c
       aco_ptr<Instruction>& instr = block->instructions[cursor.source_idx];
       for (const Definition& def : instr->definitions) {
          if (def.isTemp())
-            depends_on[def.tempId()] = true;
+            depends_on.set(def.tempId());
       }
       for (const Operand& op : instr->operands) {
          if (op.isTemp())
-            RAR_dependencies[op.tempId()] = true;
+            RAR_dependencies.set(op.tempId());
       }
       cursor.total_demand.update(instr->register_demand);
    }
@@ -475,6 +545,7 @@ get_sync_info_with_hack(const Instructio
    return sync;
 }
 
+/* New: central predicate to bail early on unreorderable instructions. */
 bool
 is_reorderable(const Instruction* instr)
 {
@@ -648,6 +719,15 @@ perform_hazard_query(hazard_query* query
    if (instr->isEXP() || instr->opcode == aco_opcode::p_dual_src_export_gfx11)
       return hazard_fail_export;
 
+   /* Fast-path: side-effect-free ALU/VINTRP don't participate in memory/barrier hazards.
+    * Keep after exec/export/non-reorderable checks to maintain correctness.
+    */
+   if (!instr->isVMEM() && !instr->isFlatLike() && !instr->isSMEM() &&
+       !instr->accessesLDS() && !instr->isEXP() &&
+       instr->opcode != aco_opcode::p_barrier) {
+      return hazard_success;
+   }
+
    memory_event_set instr_set;
    memset(&instr_set, 0, sizeof(instr_set));
    memory_sync_info sync = get_sync_info_with_hack(instr);
@@ -710,6 +790,121 @@ perform_hazard_query(hazard_query* query
    return hazard_success;
 }
 
+/* Fast predicate matching the ALU fast path in perform_hazard_query(). */
+static inline bool is_alu_nohazard(const Instruction* instr)
+{
+   return !instr->isVMEM() && !instr->isFlatLike() && !instr->isSMEM() &&
+          !instr->accessesLDS() && !instr->isEXP() &&
+          instr->opcode != aco_opcode::p_barrier;
+}
+
+/* Cached hazard data for a single instruction. */
+struct CachedHazard {
+   memory_event_set events;
+   memory_sync_info sync;
+};
+
+static inline HazardResult
+perform_hazard_query_cached(hazard_query* query,
+                            Instruction* instr,
+                            bool upwards,
+                            const memory_event_set& instr_set,
+                            const memory_sync_info& sync)
+{
+   /* don't schedule discards downwards */
+   if (!upwards && instr->opcode == aco_opcode::p_exit_early_if_not)
+      return hazard_fail_unreorderable;
+
+   /* POPS ordered section and export placement constraints */
+   if (upwards) {
+      if (instr->opcode == aco_opcode::p_pops_gfx9_add_exiting_wave_id ||
+          is_wait_export_ready(query->gfx_level, instr)) {
+         return hazard_fail_unreorderable;
+      }
+   } else {
+      if (instr->opcode == aco_opcode::p_pops_gfx9_ordered_section_done) {
+         return hazard_fail_unreorderable;
+      }
+   }
+
+   if (query->uses_exec || query->writes_exec) {
+      for (const Definition& def : instr->definitions) {
+         if (def.isFixed() && def.physReg() == exec)
+            return hazard_fail_exec;
+      }
+   }
+   if (query->writes_exec && needs_exec_mask(instr))
+      return hazard_fail_exec;
+
+   if (instr->isEXP() || instr->opcode == aco_opcode::p_dual_src_export_gfx11)
+      return hazard_fail_export;
+
+   /* Fast-path: side-effect-free ALU/VINTRP have no hazards. */
+   if (ACO_LIKELY(is_alu_nohazard(instr)))
+      return hazard_success;
+
+   const memory_event_set* first = &instr_set;
+   const memory_event_set* second = &query->mem_events;
+   if (upwards)
+      std::swap(first, second);
+
+   if ((first->has_control_barrier || first->access_atomic) && second->bar_acquire)
+      return hazard_fail_barrier;
+   if (((first->access_acquire || first->bar_acquire) && second->bar_classes) ||
+       ((first->access_acquire | first->bar_acquire) &
+        (second->access_relaxed | second->access_atomic)))
+      return hazard_fail_barrier;
+
+   if (first->bar_release && (second->has_control_barrier || second->access_atomic))
+      return hazard_fail_barrier;
+   if ((first->bar_classes && (second->bar_release || second->access_release)) ||
+       ((first->access_relaxed | first->access_atomic) &
+        (second->bar_release | second->access_release)))
+      return hazard_fail_barrier;
+
+   if (first->bar_classes && second->bar_classes)
+      return hazard_fail_barrier;
+
+   const unsigned control_classes =
+      storage_buffer | storage_image | storage_shared | storage_task_payload;
+   if (first->has_control_barrier &&
+       ((second->access_atomic | second->access_relaxed) & control_classes))
+      return hazard_fail_barrier;
+
+   unsigned aliasing_storage =
+      instr->isSMEM() ? query->aliasing_storage_smem : query->aliasing_storage;
+   if ((sync.storage & aliasing_storage) && !(sync.semantics & semantic_can_reorder)) {
+      unsigned intersect = sync.storage & aliasing_storage;
+      if (intersect & storage_shared)
+         return hazard_fail_reorder_ds;
+      return hazard_fail_reorder_vmem_smem;
+   }
+
+   if ((instr->opcode == aco_opcode::p_spill || instr->opcode == aco_opcode::p_reload) &&
+       query->contains_spill)
+      return hazard_fail_spill;
+
+   if (instr->opcode == aco_opcode::s_sendmsg && query->contains_sendmsg)
+      return hazard_fail_reorder_sendmsg;
+
+   return hazard_success;
+}
+
+static inline HazardResult
+hazard_query_cached(sched_ctx& ctx, hazard_query* query, Instruction* instr, bool upwards)
+{
+   /* Fast ALU path: common case, no cache lookup needed. */
+   if (ACO_LIKELY(is_alu_nohazard(instr)))
+      return hazard_success;
+
+   auto it = ctx.hazard_cache.find(instr);
+   if (ACO_LIKELY(it != ctx.hazard_cache.end()))
+      return perform_hazard_query_cached(query, instr, upwards, it->second.events, it->second.sync);
+
+   /* Fallback if not cached (should seldom happen): use original path. */
+   return perform_hazard_query(query, instr, upwards);
+}
+
 unsigned
 get_likely_cost(Instruction* instr)
 {
@@ -746,6 +941,12 @@ schedule_SMEM(sched_ctx& ctx, Block* blo
    int max_moves = SMEM_MAX_MOVES;
    int16_t k = 0;
 
+   /* GFX9: modest increase to help place ALU under SMEM latency safely. */
+   if (ctx.gfx_level <= GFX9) {
+      window_size = window_size + window_size / 4;  /* +25% */
+      max_moves   = max_moves   + max_moves / 8;    /* +12.5% */
+   }
+
    /* don't move s_memtime/s_memrealtime */
    if (current->opcode == aco_opcode::s_memtime || current->opcode == aco_opcode::s_memrealtime ||
        current->opcode == aco_opcode::s_sendmsg_rtn_b32 ||
@@ -771,7 +972,7 @@ schedule_SMEM(sched_ctx& ctx, Block* blo
       if (can_stall_prev_smem && ctx.last_SMEM_stall >= 0)
          break;
 
-      /* break when encountering another MEM instruction, logical_start or barriers */
+      /* break when encountering another MEM instruction, or unreorderable */
       if (!is_reorderable(candidate.get()))
          break;
       /* only move VMEM instructions below descriptor loads. be more aggressive at higher num_waves
@@ -787,7 +988,7 @@ schedule_SMEM(sched_ctx& ctx, Block* blo
 
       bool can_move_down = true;
 
-      HazardResult haz = perform_hazard_query(&hq, candidate.get(), false);
+      HazardResult haz = hazard_query_cached(ctx, &hq, candidate.get(), false);
       if (haz == hazard_fail_reorder_ds || haz == hazard_fail_spill ||
           haz == hazard_fail_reorder_sendmsg || haz == hazard_fail_barrier ||
           haz == hazard_fail_export)
@@ -838,7 +1039,7 @@ schedule_SMEM(sched_ctx& ctx, Block* blo
          break;
 
       if (found_dependency) {
-         HazardResult haz = perform_hazard_query(&hq, candidate.get(), true);
+         HazardResult haz = hazard_query_cached(ctx, &hq, candidate.get(), true);
          if (haz == hazard_fail_reorder_ds || haz == hazard_fail_spill ||
              haz == hazard_fail_reorder_sendmsg || haz == hazard_fail_barrier ||
              haz == hazard_fail_export)
@@ -892,6 +1093,14 @@ schedule_VMEM(sched_ctx& ctx, Block* blo
    bool only_clauses = false;
    int16_t k = 0;
 
+   /* GFX9 (Vega): increase window to improve clause formation and latency hiding. */
+   if (ctx.gfx_level <= GFX9) {
+      window_size = window_size + window_size / 2;            /* x1.5 */
+      max_moves   = max_moves   + max_moves / 4;              /* x1.25 */
+      /* allow grabbing a bit further for clauses at low occupancy */
+      clause_max_grab_dist = std::max(clause_max_grab_dist, ctx.occupancy_factor * 3);
+   }
+
    /* first, check if we have instructions before current to move down */
    hazard_query indep_hq;
    hazard_query clause_hq;
@@ -908,7 +1117,7 @@ schedule_VMEM(sched_ctx& ctx, Block* blo
       aco_ptr<Instruction>& candidate = block->instructions[candidate_idx];
       bool is_vmem = candidate->isVMEM() || candidate->isFlatLike();
 
-      /* Break when encountering another VMEM instruction, logical_start or barriers. */
+      /* Break when encountering another VMEM instruction, unreorderable, or barriers. */
       if (!is_reorderable(candidate.get()))
          break;
 
@@ -919,7 +1128,7 @@ schedule_VMEM(sched_ctx& ctx, Block* blo
          if (grab_dist >= clause_max_grab_dist + k)
             break;
 
-         if (perform_hazard_query(&clause_hq, candidate.get(), false) == hazard_success)
+         if (hazard_query_cached(ctx, &clause_hq, candidate.get(), false) == hazard_success)
             ctx.mv.downwards_move_clause(cursor);
 
          /* We move the entire clause at once.
@@ -937,7 +1146,7 @@ schedule_VMEM(sched_ctx& ctx, Block* blo
       /* If current depends on candidate, add additional dependencies and continue. */
       bool can_move_down = !only_clauses && (!is_vmem || candidate->definitions.empty());
 
-      HazardResult haz = perform_hazard_query(&indep_hq, candidate.get(), false);
+      HazardResult haz = hazard_query_cached(ctx, &indep_hq, candidate.get(), false);
       if (haz == hazard_fail_reorder_ds || haz == hazard_fail_spill ||
           haz == hazard_fail_reorder_sendmsg || haz == hazard_fail_barrier ||
           haz == hazard_fail_export)
@@ -989,7 +1198,7 @@ schedule_VMEM(sched_ctx& ctx, Block* blo
       /* check if candidate depends on current */
       bool is_dependency = false;
       if (found_dependency) {
-         HazardResult haz = perform_hazard_query(&indep_hq, candidate.get(), true);
+         HazardResult haz = hazard_query_cached(ctx, &indep_hq, candidate.get(), true);
          if (haz == hazard_fail_reorder_ds || haz == hazard_fail_spill ||
              haz == hazard_fail_reorder_vmem_smem || haz == hazard_fail_reorder_sendmsg ||
              haz == hazard_fail_barrier || haz == hazard_fail_export)
@@ -1009,15 +1218,17 @@ schedule_VMEM(sched_ctx& ctx, Block* blo
          /* don't move up dependencies of other VMEM instructions */
          for (const Definition& def : candidate->definitions) {
             if (def.isTemp())
-               ctx.mv.depends_on[def.tempId()] = true;
+               ctx.mv.depends_on.set(def.tempId());
          }
       }
 
       if (is_dependency || !found_dependency) {
-         if (found_dependency)
+         if (found_dependency) {
             add_to_hazard_query(&indep_hq, candidate.get());
-         else
-            k++;
+         } else {
+            /* Weighted budget before first dependency: don't burn budget on wide vector ops. */
+            k += get_likely_cost(candidate.get());
+         }
          ctx.mv.upwards_skip(up_cursor);
          continue;
       }
@@ -1061,7 +1272,7 @@ schedule_LDS(sched_ctx& ctx, Block* bloc
          continue;
       }
 
-      if (perform_hazard_query(&hq, candidate.get(), false) != hazard_success ||
+      if (hazard_query_cached(ctx, &hq, candidate.get(), false) != hazard_success ||
           ctx.mv.downwards_move(cursor) != move_success)
          break;
 
@@ -1099,7 +1310,7 @@ schedule_LDS(sched_ctx& ctx, Block* bloc
       if (!is_reorderable(candidate.get()) || is_mem)
          break;
 
-      HazardResult haz = perform_hazard_query(&hq, candidate.get(), true);
+      HazardResult haz = hazard_query_cached(ctx, &hq, candidate.get(), true);
       if (haz == hazard_fail_exec || haz == hazard_fail_unreorderable)
          break;
 
@@ -1136,7 +1347,7 @@ schedule_position_export(sched_ctx& ctx,
       if (candidate->isVMEM() || candidate->isSMEM() || candidate->isFlatLike())
          break;
 
-      HazardResult haz = perform_hazard_query(&hq, candidate.get(), false);
+      HazardResult haz = hazard_query_cached(ctx, &hq, candidate.get(), false);
       if (haz == hazard_fail_exec || haz == hazard_fail_unreorderable)
          break;
 
@@ -1178,7 +1389,7 @@ schedule_VMEM_store(sched_ctx& ctx, Bloc
          break;
 
       if (should_form_clause(current, candidate.get())) {
-         if (perform_hazard_query(&hq, candidate.get(), false) == hazard_success)
+         if (hazard_query_cached(ctx, &hq, candidate.get(), false) == hazard_success)
             ctx.mv.downwards_move_clause(cursor);
          break;
       }
@@ -1200,6 +1411,20 @@ schedule_block(sched_ctx& ctx, Program*
    ctx.last_SMEM_stall = INT16_MIN;
    ctx.mv.block = block;
 
+   /* Precompute per-instruction hazard cache for this block. */
+   ctx.hazard_cache.clear();
+   ctx.hazard_cache.reserve(block->instructions.size());
+   for (const aco_ptr<Instruction>& ip : block->instructions) {
+      Instruction* instr = ip.get();
+      CachedHazard ch{};
+      ch.sync = get_sync_info_with_hack(instr);
+      memory_event_set ev{};
+      memset(&ev, 0, sizeof(ev));
+      add_memory_event(program, &ev, instr, &ch.sync);
+      ch.events = ev;
+      ctx.hazard_cache.emplace(instr, ch);
+   }
+
    /* go through all instructions and find memory loads */
    for (unsigned idx = 0; idx < block->instructions.size(); idx++) {
       Instruction* current = block->instructions[idx].get();
@@ -1265,12 +1490,12 @@ schedule_program(Program* program)
 
    const int wave_factor = program->gfx_level >= GFX10 ? 2 : 1;
    const int wave_minimum = std::max<int>(program->min_waves, 4 * wave_factor);
-   const float reg_file_multiple = program->dev.physical_vgprs / (256.0 * wave_factor);
+   const float reg_file_multiple = program->dev.physical_vgprs / (256.0f * wave_factor);
 
    /* If we already have less waves than the minimum, don't reduce them further.
     * Otherwise, sacrifice some waves and use more VGPRs, in order to improve scheduling.
     */
-   int vgpr_demand = std::max<int>(24, demand.vgpr) + 12 * reg_file_multiple;
+   int vgpr_demand = std::max<int>(24, demand.vgpr) + static_cast<int>(12 * reg_file_multiple);
    int target_waves = std::max(wave_minimum, program->dev.physical_vgprs / vgpr_demand);
    target_waves = max_suitable_waves(program, std::min<int>(program->num_waves, target_waves));
    assert(target_waves >= program->min_waves);
