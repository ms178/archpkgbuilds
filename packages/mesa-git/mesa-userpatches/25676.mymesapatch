From 8d0153ca639226f1d751b7441d179934d2f5e77e Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Daniel=20Sch=C3=BCrmann?= <daniel@schuermann.dev>
Date: Mon, 30 Oct 2023 16:19:20 +0100
Subject: [PATCH 1/2] aco: fix should_form_clause() for memory instructions
 without operands

In particular, this applies to s_memtime and s_memrealtime.
---
 src/amd/compiler/aco_ir.cpp | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/src/amd/compiler/aco_ir.cpp b/src/amd/compiler/aco_ir.cpp
index cd8d1efee80b..fa377a0f9b27 100644
--- a/src/amd/compiler/aco_ir.cpp
+++ b/src/amd/compiler/aco_ir.cpp
@@ -1308,6 +1308,9 @@ should_form_clause(const Instruction* a, const Instruction* b)
    if (a->format != b->format)
       return false;
 
+   if (a->operands.empty() || b->operands.empty())
+      return false;
+
    /* Assume loads which don't use descriptors might load from similar addresses. */
    if (a->isFlatLike())
       return true;
-- 
GitLab


From 99d2b4f527db29fe5b802c671a5a2f3a035bf7fa Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Daniel=20Sch=C3=BCrmann?= <daniel@schuermann.dev>
Date: Thu, 12 Oct 2023 10:52:45 +0200
Subject: [PATCH 2/2] aco: add new post-RA scheduler for ILP

Totals from 77188 (97.30% of 79330) affected shaders: (GFX11)
Instrs: 44390015 -> 43261379 (-2.54%); split: -2.58%, +0.03%
CodeSize: 227864516 -> 223345352 (-1.98%); split: -2.01%, +0.03%
Latency: 310053227 -> 299061198 (-3.55%); split: -3.64%, +0.10%
InvThroughput: 50481049 -> 49581116 (-1.78%); split: -1.80%, +0.01%
VClause: 869359 -> 833549 (-4.12%); split: -4.12%, +0.00%
SClause: 1483982 -> 1335393 (-10.01%); split: -10.02%, +0.00%
---
 src/amd/compiler/README.md             |   7 +-
 src/amd/compiler/aco_interface.cpp     |   4 +
 src/amd/compiler/aco_ir.cpp            |  26 +-
 src/amd/compiler/aco_ir.h              |   2 +
 src/amd/compiler/aco_scheduler_ilp.cpp | 405 +++++++++++++++++++++++++
 src/amd/compiler/meson.build           |   1 +
 6 files changed, 432 insertions(+), 13 deletions(-)
 create mode 100644 src/amd/compiler/aco_scheduler_ilp.cpp

diff --git a/src/amd/compiler/README.md b/src/amd/compiler/README.md
index 774a52ce7ea3..6dac15be3e6b 100644
--- a/src/amd/compiler/README.md
+++ b/src/amd/compiler/README.md
@@ -97,6 +97,10 @@ The next step is a pass out of SSA by inserting parallelcopies at the end of blo
 Most pseudo instructions are lowered to actual machine instructions.
 These are mostly parallel copy instructions created by instruction selection or register allocation and spill/reload code.
 
+#### ILP Scheduling
+
+This second scheduler works on registers rather than SSA-values to determine dependencies. It implements a forward list scheduling algorithm using a partial dependency graph of few instructions at a time and aims to create larger memory clauses and improve ILP.
+
 #### Insert wait states
 
 GCN requires some wait states to be manually inserted in order to ensure correct behavior on memory instructions and some register dependencies.
@@ -249,7 +253,8 @@ We also have `ACO_DEBUG` options:
 * `force-waitcnt` - Forces ACO to emit a wait state after each instruction when there is something to wait for. Harms performance.
 * `novn` - Disables the ACO value numbering stage.
 * `noopt` - Disables the ACO optimizer.
-* `nosched` - Disables the ACO scheduler.
+* `nosched` - Disables the ACO pre-RA and post-RA scheduler.
+* `nosched-ilp` - Disables the ACO post-RA ILP scheduler.
 
 Note that you need to **combine these options into a comma-separated list**, for example: `RADV_DEBUG=nocache,shaders` otherwise only the last one will take effect. (This is how all environment variables work, yet this is an often made mistake.) Example:
 
diff --git a/src/amd/compiler/aco_interface.cpp b/src/amd/compiler/aco_interface.cpp
index 6ae3405b3e1f..5ffabd6a86fe 100644
--- a/src/amd/compiler/aco_interface.cpp
+++ b/src/amd/compiler/aco_interface.cpp
@@ -199,6 +199,10 @@ aco_postprocess_shader(const struct aco_compiler_options* options,
    aco::lower_to_hw_instr(program.get());
    validate(program.get());
 
+   /* Schedule hardware instructions for ILP */
+   if (!options->optimisations_disabled && !(aco::debug_flags & aco::DEBUG_NO_SCHED_ILP))
+      aco::schedule_ilp(program.get());
+
    /* Insert Waitcnt */
    aco::insert_wait_states(program.get());
    aco::insert_NOPs(program.get());
diff --git a/src/amd/compiler/aco_ir.cpp b/src/amd/compiler/aco_ir.cpp
index fa377a0f9b27..cf8dcc8c3026 100644
--- a/src/amd/compiler/aco_ir.cpp
+++ b/src/amd/compiler/aco_ir.cpp
@@ -36,18 +36,20 @@ thread_local aco::monotonic_buffer_resource* instruction_buffer = nullptr;
 
 uint64_t debug_flags = 0;
 
-static const struct debug_control aco_debug_options[] = {{"validateir", DEBUG_VALIDATE_IR},
-                                                         {"validatera", DEBUG_VALIDATE_RA},
-                                                         {"novalidateir", DEBUG_NO_VALIDATE_IR},
-                                                         {"perfwarn", DEBUG_PERFWARN},
-                                                         {"force-waitcnt", DEBUG_FORCE_WAITCNT},
-                                                         {"force-waitdeps", DEBUG_FORCE_WAITDEPS},
-                                                         {"novn", DEBUG_NO_VN},
-                                                         {"noopt", DEBUG_NO_OPT},
-                                                         {"nosched", DEBUG_NO_SCHED},
-                                                         {"perfinfo", DEBUG_PERF_INFO},
-                                                         {"liveinfo", DEBUG_LIVE_INFO},
-                                                         {NULL, 0}};
+static const struct debug_control aco_debug_options[] = {
+   {"validateir", DEBUG_VALIDATE_IR},
+   {"validatera", DEBUG_VALIDATE_RA},
+   {"novalidateir", DEBUG_NO_VALIDATE_IR},
+   {"perfwarn", DEBUG_PERFWARN},
+   {"force-waitcnt", DEBUG_FORCE_WAITCNT},
+   {"force-waitdeps", DEBUG_FORCE_WAITDEPS},
+   {"novn", DEBUG_NO_VN},
+   {"noopt", DEBUG_NO_OPT},
+   {"nosched", DEBUG_NO_SCHED | DEBUG_NO_SCHED_ILP},
+   {"nosched-ilp", DEBUG_NO_SCHED_ILP},
+   {"perfinfo", DEBUG_PERF_INFO},
+   {"liveinfo", DEBUG_LIVE_INFO},
+   {NULL, 0}};
 
 static once_flag init_once_flag = ONCE_FLAG_INIT;
 
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index acb935347967..13e8adabd22b 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -57,6 +57,7 @@ enum {
    DEBUG_LIVE_INFO = 0x100,
    DEBUG_FORCE_WAITDEPS = 0x200,
    DEBUG_NO_VALIDATE_IR = 0x400,
+   DEBUG_NO_SCHED_ILP = 0x800,
 };
 
 enum storage_class : uint8_t {
@@ -2200,6 +2201,7 @@ void register_allocation(Program* program, std::vector<IDSet>& live_out_per_bloc
 void ssa_elimination(Program* program);
 void lower_to_hw_instr(Program* program);
 void schedule_program(Program* program, live& live_vars);
+void schedule_ilp(Program* program);
 void spill(Program* program, live& live_vars);
 void insert_wait_states(Program* program);
 bool dealloc_vgprs(Program* program);
diff --git a/src/amd/compiler/aco_scheduler_ilp.cpp b/src/amd/compiler/aco_scheduler_ilp.cpp
new file mode 100644
index 000000000000..dc6ff27f4531
--- /dev/null
+++ b/src/amd/compiler/aco_scheduler_ilp.cpp
@@ -0,0 +1,405 @@
+/*
+ * Copyright 2023 Valve Corporation
+ * SPDX-License-Identifier: MIT
+ */
+
+#include "aco_ir.h"
+
+#include "util/bitscan.h"
+
+/*
+ * This pass implements a simple forward list-scheduler which works on a small
+ * partial DAG of 16 nodes at any time. Only ALU instructions are scheduled
+ * entirely freely. Memory load instructions must be kept in-order and any other
+ * instruction must not be re-scheduled at all.
+ *
+ * The main goal of this scheduler is to create more memory clauses, schedule
+ * memory loads early, and to improve ALU instruction level parallelism.
+ */
+
+namespace aco {
+namespace {
+
+struct InstrInfo {
+   Instruction* instr;
+   int32_t priority;
+   uint16_t dependency_mask; /* bitmask of nodes which have to be scheduled before this node */
+   uint8_t next_non_reorderable; /* inlined linked list of non-reorderable instruction nodes */
+   bool potential_clause; /* indicates that this instruction is not (yet) immediately followed by a
+                             reorderable instruction */
+};
+
+struct RegisterInfo {
+   uint16_t read_mask; /* bitmask of nodes which have to be scheduled before the next write. */
+   int8_t latency;     /* estimated latency of last register write. */
+   uint8_t direct_dependency : 4;     /* node that has to be scheduled before any other access. */
+   uint8_t has_direct_dependency : 1; /* whether there is an unscheduled direct dependency. */
+   uint8_t padding : 3;
+};
+
+struct SchedILPContext {
+   Program* program;
+   InstrInfo nodes[16];
+   RegisterInfo regs[512];
+   uint16_t non_reorder_mask = 0; /* bitmask of instruction nodes which should not be reordered. */
+   uint16_t active_mask = 0;      /* bitmask of valid instruction nodes */
+   uint8_t next_non_reorderable = UINT8_MAX; /* index of next node which should not be reordered. */
+   uint8_t last_non_reorderable = UINT8_MAX; /* index of last node which should not be reordered. */
+};
+
+/**
+ * Returns true for side-effect free SALU and VALU instructions.
+ */
+bool
+can_reorder(Instruction* instr)
+{
+   if (instr->isVALU() || instr->isVINTRP())
+      return true;
+   if (!instr->isSALU() || instr->isSOPP())
+      return false;
+
+   switch (instr->opcode) {
+   /* SOP2 */
+   case aco_opcode::s_cbranch_g_fork:
+   case aco_opcode::s_rfe_restore_b64:
+   /* SOP1 */
+   case aco_opcode::s_setpc_b64:
+   case aco_opcode::s_swappc_b64:
+   case aco_opcode::s_rfe_b64:
+   case aco_opcode::s_cbranch_join:
+   case aco_opcode::s_set_gpr_idx_idx:
+   case aco_opcode::s_sendmsg_rtn_b32:
+   case aco_opcode::s_sendmsg_rtn_b64:
+   /* SOPK */
+   case aco_opcode::s_cbranch_i_fork:
+   case aco_opcode::s_getreg_b32:
+   case aco_opcode::s_setreg_b32:
+   case aco_opcode::s_setreg_imm32_b32:
+   case aco_opcode::s_call_b64:
+   case aco_opcode::s_waitcnt_vscnt:
+   case aco_opcode::s_waitcnt_vmcnt:
+   case aco_opcode::s_waitcnt_expcnt:
+   case aco_opcode::s_waitcnt_lgkmcnt:
+   case aco_opcode::s_subvector_loop_begin:
+   case aco_opcode::s_subvector_loop_end:
+   /* SOPC */
+   case aco_opcode::s_setvskip:
+   case aco_opcode::s_set_gpr_idx_on: return false;
+   default: break;
+   }
+
+   return true;
+}
+
+unsigned
+get_latency(Instruction* instr)
+{
+   /* Note, that these are not accurate latency estimations. */
+   if (instr->isVALU())
+      return 5;
+   if (instr->isSALU())
+      return 2;
+   if (instr->isVMEM() || instr->isFlatLike())
+      return 32;
+   if (instr->isSMEM())
+      return 5;
+   if (instr->isDS() || instr->isLDSDIR())
+      return 2;
+
+   return 0;
+}
+
+bool
+is_memory_instr(Instruction* instr)
+{
+   /* For memory instructions, we allow to reorder them with ALU if it helps
+    * to form larger clauses or to increase def-use distances.
+    */
+   return instr->isVMEM() || instr->isFlatLike() || instr->isSMEM() || instr->isDS();
+}
+
+void
+add_entry(SchedILPContext& ctx, Instruction* instr, uint32_t idx)
+{
+   InstrInfo& entry = ctx.nodes[idx];
+   entry.instr = instr;
+   entry.priority = 0;
+   const uint16_t mask = (1 << idx);
+   bool reorder = can_reorder(instr);
+   ctx.active_mask |= mask;
+
+   for (const Operand& op : instr->operands) {
+      assert(op.isFixed());
+      unsigned reg = op.physReg();
+      if (reg >= 128 && reg != scc && reg < 256) {
+         reorder &= reg != pops_exiting_wave_id;
+         continue;
+      }
+
+      for (unsigned i = 0; i < op.size(); i++) {
+         RegisterInfo& reg_info = ctx.regs[reg + i];
+
+         /* Add register reads. */
+         reg_info.read_mask |= mask;
+
+         int cycles_since_reg_write = 16;
+         if (reg_info.has_direct_dependency) {
+            /* A previous dependency is still part of the DAG. */
+            entry.dependency_mask |= (1 << reg_info.direct_dependency);
+            cycles_since_reg_write = ctx.nodes[reg_info.direct_dependency].priority;
+         }
+
+         if (reg_info.latency) {
+            /* Ignore and reset register latencies for memory loads and other non-reorderable
+             * instructions. We schedule these as early as possible anyways.
+             */
+            if (reorder && reg_info.latency > cycles_since_reg_write) {
+               entry.priority = MIN2(entry.priority, cycles_since_reg_write - reg_info.latency);
+
+               /* If a previous register write created some latency, ensure that this
+                * is the first read of the register by making this instruction a direct
+                * dependency of all following register reads.
+                */
+               reg_info.has_direct_dependency = 1;
+               reg_info.direct_dependency = idx;
+            }
+            reg_info.latency = 0;
+         }
+      }
+   }
+
+   /* Check if this instructions reads implicit registers. */
+   if (needs_exec_mask(instr)) {
+      for (unsigned reg = exec_lo; reg <= exec_hi; reg++) {
+         if (ctx.regs[reg].has_direct_dependency)
+            entry.dependency_mask |= (1 << ctx.regs[reg].direct_dependency);
+         ctx.regs[reg].read_mask |= mask;
+      }
+   }
+   if (ctx.program->gfx_level < GFX10 && instr->isScratch()) {
+      for (unsigned reg = flat_scr_lo; reg <= flat_scr_hi; reg++) {
+         if (ctx.regs[reg].has_direct_dependency)
+            entry.dependency_mask |= (1 << ctx.regs[reg].direct_dependency);
+         ctx.regs[reg].read_mask |= mask;
+      }
+   }
+
+   for (const Definition& def : instr->definitions) {
+      for (unsigned i = 0; i < def.size(); i++) {
+         RegisterInfo& reg_info = ctx.regs[def.physReg().reg() + i];
+
+         /* Add all previous register reads and writes to the dependencies. */
+         entry.dependency_mask |= reg_info.read_mask;
+         reg_info.read_mask = mask;
+
+         /* This register write is a direct dependency for all following reads. */
+         reg_info.has_direct_dependency = 1;
+         reg_info.direct_dependency = idx;
+
+         /* Add latency information for the next register read. */
+         reg_info.latency = get_latency(instr);
+      }
+   }
+
+   if (!reorder) {
+      ctx.non_reorder_mask |= mask;
+
+      /* Set this node as last non-reorderable instruction */
+      if (ctx.next_non_reorderable == UINT8_MAX) {
+         ctx.next_non_reorderable = idx;
+      } else {
+         ctx.nodes[ctx.last_non_reorderable].next_non_reorderable = idx;
+      }
+      ctx.last_non_reorderable = idx;
+      entry.next_non_reorderable = UINT8_MAX;
+
+      /* Just don't reorder these at all. */
+      if (!is_memory_instr(instr) || instr->definitions.empty() ||
+          get_sync_info(instr).semantics & semantic_volatile) {
+         /* Add all previous instructions as dependencies. */
+         entry.dependency_mask = ctx.active_mask;
+      }
+
+      /* Remove non-reorderable instructions from dependencies, since WaR dependencies can interfere
+       * with clause formation. This should be fine, since these are always scheduled in-order and
+       * any cases that are actually a concern for clause formation are added as transitive
+       * dependencies. */
+      entry.dependency_mask &= ~ctx.non_reorder_mask;
+      entry.potential_clause = true;
+   } else if (ctx.last_non_reorderable != UINT8_MAX) {
+      ctx.nodes[ctx.last_non_reorderable].potential_clause = false;
+   }
+
+   entry.dependency_mask &= ~mask;
+
+   for (unsigned i = 0; i < 16; i++) {
+      if (!ctx.nodes[i].instr || i == idx)
+         continue;
+
+      /* Add transitive dependencies. */
+      if (entry.dependency_mask & (1 << i))
+         entry.dependency_mask |= ctx.nodes[i].dependency_mask;
+
+      /* increment base priority */
+      ctx.nodes[i].priority++;
+   }
+}
+
+void
+remove_entry(SchedILPContext& ctx, Instruction* instr, uint32_t idx)
+{
+   const uint16_t mask = ~(1 << idx);
+   ctx.active_mask &= mask;
+
+   for (const Operand& op : instr->operands) {
+      unsigned reg = op.physReg();
+      if (reg >= 128 && reg != scc && reg < 256)
+         continue;
+
+      for (unsigned i = 0; i < op.size(); i++) {
+         RegisterInfo& reg_info = ctx.regs[reg + i];
+         reg_info.read_mask &= mask;
+         reg_info.has_direct_dependency &= reg_info.direct_dependency != idx;
+      }
+   }
+   if (needs_exec_mask(instr)) {
+      ctx.regs[exec_lo].read_mask &= mask;
+      ctx.regs[exec_hi].read_mask &= mask;
+   }
+   if (ctx.program->gfx_level < GFX10 && instr->isScratch()) {
+      ctx.regs[flat_scr_lo].read_mask &= mask;
+      ctx.regs[flat_scr_hi].read_mask &= mask;
+   }
+   for (const Definition& def : instr->definitions) {
+      for (unsigned i = 0; i < def.size(); i++) {
+         unsigned reg = def.physReg().reg() + i;
+         ctx.regs[reg].read_mask &= mask;
+         ctx.regs[reg].has_direct_dependency &= ctx.regs[reg].direct_dependency != idx;
+      }
+   }
+
+   for (unsigned i = 0; i < 16; i++)
+      ctx.nodes[i].dependency_mask &= mask;
+
+   if (ctx.next_non_reorderable == idx) {
+      ctx.non_reorder_mask &= mask;
+      ctx.next_non_reorderable = ctx.nodes[idx].next_non_reorderable;
+      if (ctx.last_non_reorderable == idx)
+         ctx.last_non_reorderable = UINT8_MAX;
+   }
+}
+
+/**
+ * Returns a bitfield of nodes which have to be scheduled before the
+ * next non-reorderable instruction.
+ * If the next non-reorderable instruction can form a clause, returns the
+ * dependencies of the entire clause.
+ */
+uint16_t
+collect_clause_dependencies(SchedILPContext& ctx, uint8_t next, uint16_t clause_mask)
+{
+   const InstrInfo& entry = ctx.nodes[next];
+   uint16_t dependencies = entry.dependency_mask;
+   clause_mask |= (entry.potential_clause << next);
+
+   if (!is_memory_instr(entry.instr))
+      return dependencies;
+
+   /* If this is potentially an "open" clause, meaning that the clause might
+    * consist of instruction not yet added to the DAG, consider all previous
+    * instructions as dependencies. This prevents splitting of larger, already
+    * formed clauses.
+    */
+   if (next == ctx.last_non_reorderable && entry.potential_clause)
+      return (~clause_mask & ctx.active_mask) | dependencies;
+
+   if (entry.next_non_reorderable == UINT8_MAX)
+      return dependencies;
+
+   /* Check if this can form a clause with the following non-reorderable instruction */
+   if (should_form_clause(entry.instr, ctx.nodes[entry.next_non_reorderable].instr)) {
+      uint16_t clause_deps =
+         collect_clause_dependencies(ctx, entry.next_non_reorderable, clause_mask);
+
+      /* if the following clause is independent from us, add their dependencies */
+      if (!(clause_deps & (1 << next)))
+         dependencies |= clause_deps;
+   }
+
+   return dependencies;
+}
+
+/**
+ * Returns the index of the next instruction to be selected.
+ */
+unsigned
+select_instruction(SchedILPContext& ctx)
+{
+   unsigned mask = ctx.active_mask;
+
+   /* First, collect all dependencies of the next non-reorderable instruction(s).
+    * These make up the list of possible candidates.
+    */
+   if (ctx.next_non_reorderable != UINT8_MAX)
+      mask = collect_clause_dependencies(ctx, ctx.next_non_reorderable, 0);
+
+   /* If the next non-reorderable instruction has no dependencies, select it */
+   if (mask == 0)
+      return ctx.next_non_reorderable;
+
+   /* Otherwise, select the instruction with highest priority of all candidates. */
+   unsigned idx = -1u;
+   int32_t priority = INT32_MIN;
+   u_foreach_bit (i, mask) {
+      const InstrInfo& candidate = ctx.nodes[i];
+
+      /* Check if the candidate has pending dependencies. */
+      if (candidate.dependency_mask)
+         continue;
+
+      if (idx == -1u || candidate.priority > priority) {
+         idx = i;
+         priority = candidate.priority;
+      }
+   }
+
+   assert(idx != -1u);
+   return idx;
+}
+
+} // namespace
+
+void
+schedule_ilp(Program* program)
+{
+   SchedILPContext ctx = { program };
+
+   for (Block& block : program->blocks) {
+      auto it = block.instructions.begin();
+      for (unsigned i = 0; i < 16; i++) {
+         if (it == block.instructions.end())
+            break;
+
+         add_entry(ctx, (it++)->get(), i);
+      }
+
+      auto insert_it = block.instructions.begin();
+      while (insert_it != block.instructions.end()) {
+         unsigned next_idx = select_instruction(ctx);
+         Instruction* next_instr = ctx.nodes[next_idx].instr;
+         remove_entry(ctx, next_instr, next_idx);
+         (insert_it++)->reset(next_instr);
+         ctx.nodes[next_idx].instr = NULL;
+
+         if (it != block.instructions.end()) {
+            add_entry(ctx, (it++)->get(), next_idx);
+         } else if (ctx.last_non_reorderable != UINT8_MAX) {
+            ctx.nodes[ctx.last_non_reorderable].potential_clause = false;
+            ctx.last_non_reorderable = UINT8_MAX;
+         }
+      }
+      assert(it == block.instructions.end());
+   }
+}
+
+} // namespace aco
diff --git a/src/amd/compiler/meson.build b/src/amd/compiler/meson.build
index dc6f7a9071b4..f624d84626fc 100644
--- a/src/amd/compiler/meson.build
+++ b/src/amd/compiler/meson.build
@@ -75,6 +75,7 @@ libaco_files = files(
   'aco_print_ir.cpp',
   'aco_reindex_ssa.cpp',
   'aco_scheduler.cpp',
+  'aco_scheduler_ilp.cpp',
   'aco_spill.cpp',
   'aco_ssa_elimination.cpp',
   'aco_statistics.cpp',
-- 
GitLab

