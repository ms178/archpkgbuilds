From 02663e915ccfe12ca5d25f6408cccb252ae449b5 Mon Sep 17 00:00:00 2001
From: Bas Nieuwenhuizen <bas@basnieuwenhuizen.nl>
Date: Wed, 8 Jun 2022 00:37:09 +0200
Subject: [PATCH 1/3] radv: Add implementation of cmd buffers for a sparse
 binding queue.

None of the commands are allowed on these ...
---
 src/amd/vulkan/radv_cmd_buffer.c | 96 ++++++++++++++++++--------------
 src/amd/vulkan/radv_private.h    |  1 +
 2 files changed, 56 insertions(+), 41 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index 95d7c72f0fbd..6384f3fdfb74 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -323,36 +323,38 @@ radv_destroy_cmd_buffer(struct vk_command_buffer *vk_cmd_buffer)
 {
    struct radv_cmd_buffer *cmd_buffer = container_of(vk_cmd_buffer, struct radv_cmd_buffer, vk);

-   list_for_each_entry_safe (struct radv_cmd_buffer_upload, up, &cmd_buffer->upload.list, list) {
-      radv_rmv_log_command_buffer_bo_destroy(cmd_buffer->device, up->upload_bo);
-      cmd_buffer->device->ws->buffer_destroy(cmd_buffer->device->ws, up->upload_bo);
-      list_del(&up->list);
-      free(up);
-   }
+   if (cmd_buffer->qf != RADV_QUEUE_SPARSE) {
+      list_for_each_entry_safe (struct radv_cmd_buffer_upload, up, &cmd_buffer->upload.list, list) {
+         radv_rmv_log_command_buffer_bo_destroy(cmd_buffer->device, up->upload_bo);
+         cmd_buffer->device->ws->buffer_destroy(cmd_buffer->device->ws, up->upload_bo);
+         list_del(&up->list);
+         free(up);
+      }

-   if (cmd_buffer->upload.upload_bo) {
-      radv_rmv_log_command_buffer_bo_destroy(cmd_buffer->device, cmd_buffer->upload.upload_bo);
-      cmd_buffer->device->ws->buffer_destroy(cmd_buffer->device->ws, cmd_buffer->upload.upload_bo);
-   }
+      if (cmd_buffer->upload.upload_bo) {
+         radv_rmv_log_command_buffer_bo_destroy(cmd_buffer->device, cmd_buffer->upload.upload_bo);
+         cmd_buffer->device->ws->buffer_destroy(cmd_buffer->device->ws, cmd_buffer->upload.upload_bo);
+      }

-   if (cmd_buffer->cs)
-      cmd_buffer->device->ws->cs_destroy(cmd_buffer->cs);
-   if (cmd_buffer->gang.cs)
-      cmd_buffer->device->ws->cs_destroy(cmd_buffer->gang.cs);
-   if (cmd_buffer->transfer.copy_temp)
-      cmd_buffer->device->ws->buffer_destroy(cmd_buffer->device->ws, cmd_buffer->transfer.copy_temp);
+      if (cmd_buffer->cs)
+         cmd_buffer->device->ws->cs_destroy(cmd_buffer->cs);
+      if (cmd_buffer->gang.cs)
+         cmd_buffer->device->ws->cs_destroy(cmd_buffer->gang.cs);
+      if (cmd_buffer->transfer.copy_temp)
+         cmd_buffer->device->ws->buffer_destroy(cmd_buffer->device->ws, cmd_buffer->transfer.copy_temp);

-   radv_cmd_buffer_finish_shader_part_cache(cmd_buffer);
+      radv_cmd_buffer_finish_shader_part_cache(cmd_buffer);

-   for (unsigned i = 0; i < MAX_BIND_POINTS; i++) {
-      struct radv_descriptor_set_header *set = &cmd_buffer->descriptors[i].push_set.set;
-      free(set->mapped_ptr);
-      if (set->layout)
-         vk_descriptor_set_layout_unref(&cmd_buffer->device->vk, &set->layout->vk);
-      vk_object_base_finish(&set->base);
-   }
+      for (unsigned i = 0; i < MAX_BIND_POINTS; i++) {
+         struct radv_descriptor_set_header *set = &cmd_buffer->descriptors[i].push_set.set;
+         free(set->mapped_ptr);
+         if (set->layout)
+            vk_descriptor_set_layout_unref(&cmd_buffer->device->vk, &set->layout->vk);
+         vk_object_base_finish(&set->base);
+      }

-   vk_object_base_finish(&cmd_buffer->meta_push_descriptors.base);
+      vk_object_base_finish(&cmd_buffer->meta_push_descriptors.base);
+   }

    vk_command_buffer_finish(&cmd_buffer->vk);
    vk_free(&cmd_buffer->vk.pool->alloc, cmd_buffer);
@@ -375,29 +377,32 @@ radv_create_cmd_buffer(struct vk_command_pool *pool, struct vk_command_buffer **
       return result;
    }

-   if (!radv_cmd_buffer_init_shader_part_cache(device, cmd_buffer)) {
-      radv_destroy_cmd_buffer(&cmd_buffer->vk);
-      return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
-   }
-
-   list_inithead(&cmd_buffer->upload.list);
-
    cmd_buffer->device = device;

    cmd_buffer->qf = vk_queue_to_radv(device->physical_device, pool->queue_family_index);

-   ring = radv_queue_family_to_ring(device->physical_device, cmd_buffer->qf);
+   if (cmd_buffer->qf != RADV_QUEUE_SPARSE) {
+      list_inithead(&cmd_buffer->upload.list);

-   cmd_buffer->cs = device->ws->cs_create(device->ws, ring, cmd_buffer->vk.level == VK_COMMAND_BUFFER_LEVEL_SECONDARY);
-   if (!cmd_buffer->cs) {
-      radv_destroy_cmd_buffer(&cmd_buffer->vk);
-      return vk_error(device, VK_ERROR_OUT_OF_DEVICE_MEMORY);
-   }
+      if (!radv_cmd_buffer_init_shader_part_cache(device, cmd_buffer)) {
+         radv_destroy_cmd_buffer(&cmd_buffer->vk);
+         return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
+      }

-   vk_object_base_init(&device->vk, &cmd_buffer->meta_push_descriptors.base, VK_OBJECT_TYPE_DESCRIPTOR_SET);
+      ring = radv_queue_family_to_ring(device->physical_device, cmd_buffer->qf);
+
+      cmd_buffer->cs =
+         device->ws->cs_create(device->ws, ring, cmd_buffer->vk.level == VK_COMMAND_BUFFER_LEVEL_SECONDARY);
+      if (!cmd_buffer->cs) {
+         radv_destroy_cmd_buffer(&cmd_buffer->vk);
+         return vk_error(device, VK_ERROR_OUT_OF_DEVICE_MEMORY);
+      }

-   for (unsigned i = 0; i < MAX_BIND_POINTS; i++)
-      vk_object_base_init(&device->vk, &cmd_buffer->descriptors[i].push_set.set.base, VK_OBJECT_TYPE_DESCRIPTOR_SET);
+      vk_object_base_init(&device->vk, &cmd_buffer->meta_push_descriptors.base, VK_OBJECT_TYPE_DESCRIPTOR_SET);
+
+      for (unsigned i = 0; i < MAX_BIND_POINTS; i++)
+         vk_object_base_init(&device->vk, &cmd_buffer->descriptors[i].push_set.set.base, VK_OBJECT_TYPE_DESCRIPTOR_SET);
+   }

    *cmd_buffer_out = &cmd_buffer->vk;

@@ -417,6 +422,9 @@ radv_reset_cmd_buffer(struct vk_command_buffer *vk_cmd_buffer, UNUSED VkCommandB

    vk_command_buffer_reset(&cmd_buffer->vk);

+   if (cmd_buffer->qf == RADV_QUEUE_SPARSE)
+      return;
+
    cmd_buffer->device->ws->cs_reset(cmd_buffer->cs);
    if (cmd_buffer->gang.cs)
       cmd_buffer->device->ws->cs_reset(cmd_buffer->gang.cs);
@@ -5625,6 +5633,9 @@ radv_BeginCommandBuffer(VkCommandBuffer commandBuffer, const VkCommandBufferBegi

    vk_command_buffer_begin(&cmd_buffer->vk, pBeginInfo);

+   if (cmd_buffer->qf == RADV_QUEUE_SPARSE)
+      return result;
+
    memset(&cmd_buffer->state, 0, sizeof(cmd_buffer->state));
    cmd_buffer->state.last_index_type = -1;
    cmd_buffer->state.last_num_instances = -1;
@@ -6053,6 +6064,9 @@ radv_EndCommandBuffer(VkCommandBuffer commandBuffer)
 {
    RADV_FROM_HANDLE(radv_cmd_buffer, cmd_buffer, commandBuffer);

+   if (cmd_buffer->qf == RADV_QUEUE_SPARSE)
+      return vk_command_buffer_end(&cmd_buffer->vk);
+
    radv_emit_mip_change_flush_default(cmd_buffer);

    const bool is_gfx_or_ace = cmd_buffer->qf == RADV_QUEUE_GENERAL || cmd_buffer->qf == RADV_QUEUE_COMPUTE;
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index 09d7543f8d32..d55d7a5f9f83 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -261,6 +261,7 @@ enum radv_queue_family {
    RADV_QUEUE_GENERAL,
    RADV_QUEUE_COMPUTE,
    RADV_QUEUE_TRANSFER,
+   RADV_QUEUE_SPARSE,
    RADV_QUEUE_VIDEO_DEC,
    RADV_QUEUE_VIDEO_ENC,
    RADV_MAX_QUEUE_FAMILIES,
--
GitLab


From c1e84330f25cd8bdc5372205e51a2357df8242b9 Mon Sep 17 00:00:00 2001
From: Bas Nieuwenhuizen <bas@basnieuwenhuizen.nl>
Date: Wed, 8 Jun 2022 00:37:50 +0200
Subject: [PATCH 2/3] radv: Remove the sparse binding queue from coherent
 images.

Never access the image on the queue family, so no need.

(Technically not sure if this is needed for Vulkan, somewhat of
 a backstop in case apps do it)
---
 src/amd/vulkan/radv_image.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/src/amd/vulkan/radv_image.c b/src/amd/vulkan/radv_image.c
index 5c7a1ad71e83..46b9328cb14a 100644
--- a/src/amd/vulkan/radv_image.c
+++ b/src/amd/vulkan/radv_image.c
@@ -1921,6 +1921,9 @@ radv_image_create(VkDevice _device, const struct radv_image_create_info *create_
          else
             image->queue_family_mask |=
                1u << vk_queue_to_radv(device->physical_device, pCreateInfo->pQueueFamilyIndices[i]);
+
+      /* This queue never really accesses the image. */
+      image->queue_family_mask &= ~(1u << RADV_QUEUE_SPARSE);
    }

    const VkExternalMemoryImageCreateInfo *external_info =
--
GitLab


From 1edb2949a12e52be82c342d899900fc80d30f2e7 Mon Sep 17 00:00:00 2001
From: Bas Nieuwenhuizen <bas@basnieuwenhuizen.nl>
Date: Wed, 8 Jun 2022 02:18:37 +0200
Subject: [PATCH 3/3] radv: Move sparse binding into a dedicated queue.

1) This better reflects the reality that we only have one timeline
   of sparse binding changes.

2) Allows making it a threaded queue from the start in prep of
   explicit sync stuff.
---
 src/amd/vulkan/layers/radv_sqtt_layer.c |  2 +-
 src/amd/vulkan/radv_physical_device.c   | 23 +++++++++---
 src/amd/vulkan/radv_queue.c             | 48 ++++++++++++++++++++++---
 3 files changed, 62 insertions(+), 11 deletions(-)

diff --git a/src/amd/vulkan/layers/radv_sqtt_layer.c b/src/amd/vulkan/layers/radv_sqtt_layer.c
index bcdf55fb21a6..1ec1383eb2fc 100644
--- a/src/amd/vulkan/layers/radv_sqtt_layer.c
+++ b/src/amd/vulkan/layers/radv_sqtt_layer.c
@@ -333,7 +333,7 @@ radv_describe_begin_cmd_buffer(struct radv_cmd_buffer *cmd_buffer)
    marker.device_id_low = device_id;
    marker.device_id_high = device_id >> 32;
    marker.queue = cmd_buffer->qf;
-   marker.queue_flags = VK_QUEUE_COMPUTE_BIT | VK_QUEUE_TRANSFER_BIT | VK_QUEUE_SPARSE_BINDING_BIT;
+   marker.queue_flags = VK_QUEUE_COMPUTE_BIT | VK_QUEUE_TRANSFER_BIT;

    if (cmd_buffer->qf == RADV_QUEUE_GENERAL)
       marker.queue_flags |= VK_QUEUE_GRAPHICS_BIT;
diff --git a/src/amd/vulkan/radv_physical_device.c b/src/amd/vulkan/radv_physical_device.c
index 5c50552424a1..48bc4b94373b 100644
--- a/src/amd/vulkan/radv_physical_device.c
+++ b/src/amd/vulkan/radv_physical_device.c
@@ -173,6 +173,9 @@ radv_physical_device_init_queue_table(struct radv_physical_device *pdevice)
          idx++;
       }
    }
+
+   pdevice->vk_queue_to_radv[idx++] = RADV_QUEUE_SPARSE;
+
    pdevice->num_queues = idx;
 }

@@ -2062,7 +2065,7 @@ static void
 radv_get_physical_device_queue_family_properties(struct radv_physical_device *pdevice, uint32_t *pCount,
                                                  VkQueueFamilyProperties **pQueueFamilyProperties)
 {
-   int num_queue_families = 1;
+   int num_queue_families = 2;
    int idx;
    if (pdevice->rad_info.ip[AMD_IP_COMPUTE].num_queues > 0 &&
        !(pdevice->instance->debug_flags & RADV_DEBUG_NO_COMPUTE_QUEUE))
@@ -2084,8 +2087,7 @@ radv_get_physical_device_queue_family_properties(struct radv_physical_device *pd
    idx = 0;
    if (*pCount >= 1) {
       *pQueueFamilyProperties[idx] = (VkQueueFamilyProperties){
-         .queueFlags =
-            VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT | VK_QUEUE_TRANSFER_BIT | VK_QUEUE_SPARSE_BINDING_BIT,
+         .queueFlags = VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT | VK_QUEUE_TRANSFER_BIT,
          .queueCount = 1,
          .timestampValidBits = 64,
          .minImageTransferGranularity = (VkExtent3D){1, 1, 1},
@@ -2097,7 +2099,7 @@ radv_get_physical_device_queue_family_properties(struct radv_physical_device *pd
        !(pdevice->instance->debug_flags & RADV_DEBUG_NO_COMPUTE_QUEUE)) {
       if (*pCount > idx) {
          *pQueueFamilyProperties[idx] = (VkQueueFamilyProperties){
-            .queueFlags = VK_QUEUE_COMPUTE_BIT | VK_QUEUE_TRANSFER_BIT | VK_QUEUE_SPARSE_BINDING_BIT,
+            .queueFlags = VK_QUEUE_COMPUTE_BIT | VK_QUEUE_TRANSFER_BIT,
             .queueCount = pdevice->rad_info.ip[AMD_IP_COMPUTE].num_queues,
             .timestampValidBits = 64,
             .minImageTransferGranularity = (VkExtent3D){1, 1, 1},
@@ -2106,6 +2108,16 @@ radv_get_physical_device_queue_family_properties(struct radv_physical_device *pd
       }
    }

+   if (*pCount > idx) {
+      *pQueueFamilyProperties[idx] = (VkQueueFamilyProperties){
+         .queueFlags = VK_QUEUE_SPARSE_BINDING_BIT,
+         .queueCount = 1,
+         .timestampValidBits = 64,
+         .minImageTransferGranularity = (VkExtent3D){1, 1, 1},
+      };
+      idx++;
+   }
+
    if (pdevice->instance->perftest_flags & RADV_PERFTEST_VIDEO_DECODE) {
       if (pdevice->rad_info.ip[pdevice->vid_decode_ip].num_queues > 0) {
          if (*pCount > idx) {
@@ -2143,9 +2155,10 @@ radv_GetPhysicalDeviceQueueFamilyProperties2(VkPhysicalDevice physicalDevice, ui
       &pQueueFamilyProperties[0].queueFamilyProperties,
       &pQueueFamilyProperties[1].queueFamilyProperties,
       &pQueueFamilyProperties[2].queueFamilyProperties,
+      &pQueueFamilyProperties[3].queueFamilyProperties,
    };
    radv_get_physical_device_queue_family_properties(pdevice, pCount, properties);
-   assert(*pCount <= 3);
+   assert(*pCount <= 4);

    for (uint32_t i = 0; i < *pCount; i++) {
       vk_foreach_struct (ext, pQueueFamilyProperties[i].pNext) {
diff --git a/src/amd/vulkan/radv_queue.c b/src/amd/vulkan/radv_queue.c
index abdfae500fe3..85aa03c21259 100644
--- a/src/amd/vulkan/radv_queue.c
+++ b/src/amd/vulkan/radv_queue.c
@@ -1683,17 +1683,51 @@ fail:
 }

 static VkResult
-radv_queue_submit(struct vk_queue *vqueue, struct vk_queue_submit *submission)
+radv_queue_sparse_submit(struct vk_queue *vqueue, struct vk_queue_submit *submission)
 {
    struct radv_queue *queue = (struct radv_queue *)vqueue;
+   struct radv_device *device = queue->device;
    VkResult result;

-   radv_rmv_log_submit(queue->device, radv_queue_ring(queue));
+   result = radv_queue_submit_bind_sparse_memory(device, submission);
+   if (result != VK_SUCCESS)
+      goto fail;

-   result = radv_queue_submit_bind_sparse_memory(queue->device, submission);
+   /* We do a CPU wait here, in part to avoid more winsys mechanisms. In the likely kernel explicit
+    * sync mechanism, we'd need to do a CPU wait anyway. Haven't seen this be a perf issue yet, but
+    * we have to make sure the queue always has its submission thread enabled. */
+   result = vk_sync_wait_many(&device->vk, submission->wait_count, submission->waits, 0, UINT64_MAX);
    if (result != VK_SUCCESS)
       goto fail;

+   /* Ignore all the commandbuffers. They're necessarily empty anyway. */
+
+   for (unsigned i = 0; i < submission->signal_count; ++i) {
+      result = vk_sync_signal(&device->vk, submission->signals[i].sync, submission->signals[i].signal_value);
+      if (result != VK_SUCCESS)
+         goto fail;
+   }
+
+fail:
+   if (result != VK_SUCCESS && result != VK_ERROR_DEVICE_LOST) {
+      /* When something bad happened during the submission, such as
+       * an out of memory issue, it might be hard to recover from
+       * this inconsistent state. To avoid this sort of problem, we
+       * assume that we are in a really bad situation and return
+       * VK_ERROR_DEVICE_LOST to ensure the clients do not attempt
+       * to submit the same job again to this device.
+       */
+      result = vk_device_set_lost(&queue->device->vk, "vkQueueSubmit() failed");
+   }
+   return result;
+}
+
+static VkResult
+radv_queue_submit(struct vk_queue *vqueue, struct vk_queue_submit *submission)
+{
+   struct radv_queue *queue = (struct radv_queue *)vqueue;
+   VkResult result;
+
    if (!submission->command_buffer_count && !submission->wait_count && !submission->signal_count)
       return VK_SUCCESS;

@@ -1703,7 +1737,6 @@ radv_queue_submit(struct vk_queue *vqueue, struct vk_queue_submit *submission)
       result = radv_queue_submit_normal(queue, submission);
    }

-fail:
    if (result != VK_SUCCESS && result != VK_ERROR_DEVICE_LOST) {
       /* When something bad happened during the submission, such as
        * an out of memory issue, it might be hard to recover from
@@ -1760,7 +1793,12 @@ radv_queue_init(struct radv_device *device, struct radv_queue *queue, int idx,
          goto fail;
    }

-   queue->vk.driver_submit = radv_queue_submit;
+   if (queue->state.qf == RADV_QUEUE_SPARSE) {
+      queue->vk.driver_submit = radv_queue_sparse_submit;
+      vk_queue_enable_submit_thread(&queue->vk);
+   } else {
+      queue->vk.driver_submit = radv_queue_submit;
+   }
    return VK_SUCCESS;
 fail:
    vk_queue_finish(&queue->vk);
--
GitLab

From 9ad32d143b9457d619ca5ed1d752d219e2897437 Mon Sep 17 00:00:00 2001
From: Bas Nieuwenhuizen <bas@basnieuwenhuizen.nl>
Date: Sun, 3 Apr 2022 15:47:57 +0200
Subject: [PATCH 04/12] amd: Update kernel header.

---
 include/drm-uapi/amdgpu_drm.h | 2 ++
 1 file changed, 2 insertions(+)

diff --git a/include/drm-uapi/amdgpu_drm.h b/include/drm-uapi/amdgpu_drm.h
index ad21c613fec8..30e070e76f1b 100644
--- a/include/drm-uapi/amdgpu_drm.h
+++ b/include/drm-uapi/amdgpu_drm.h
@@ -564,6 +564,8 @@ struct drm_amdgpu_gem_op {
 #define AMDGPU_VM_MTYPE_RW		(5 << 5)
 /* don't allocate MALL */
 #define AMDGPU_VM_PAGE_NOALLOC		(1 << 9)
+/* don't sync on unmap */
+#define AMDGPU_VM_EXPLICIT_SYNC		(1 << 10)
 
 struct drm_amdgpu_gem_va {
 	/** GEM object handle */
-- 
GitLab


From aa13bae2ca7903649579270cf764288acf98f3b0 Mon Sep 17 00:00:00 2001
From: Bas Nieuwenhuizen <bas@basnieuwenhuizen.nl>
Date: Tue, 2 Aug 2022 02:52:38 +0200
Subject: [PATCH 05/12] ac: Add bit for explicit sync support in gpu info.

WIP pending final drm minor number to be used.
---
 src/amd/common/ac_gpu_info.c | 2 ++
 src/amd/common/ac_gpu_info.h | 1 +
 2 files changed, 3 insertions(+)

diff --git a/src/amd/common/ac_gpu_info.c b/src/amd/common/ac_gpu_info.c
index ac0c26b49f4e..e1f9184fd078 100644
--- a/src/amd/common/ac_gpu_info.c
+++ b/src/amd/common/ac_gpu_info.c
@@ -1357,6 +1357,7 @@ bool ac_query_gpu_info(int fd, void *dev_p, struct radeon_info *info,
    }
 
    info->has_stable_pstate = info->drm_minor >= 45;
+   info->has_explicit_sync_vm_ops = info->drm_minor >= 55;
 
    if (info->gfx_level >= GFX11) {
       info->pc_lines = 1024;
@@ -1807,6 +1808,7 @@ void ac_print_gpu_info(const struct radeon_info *info, FILE *f)
    fprintf(f, "    has_eqaa_surface_allocator = %u\n", info->has_eqaa_surface_allocator);
    fprintf(f, "    has_sparse_vm_mappings = %u\n", info->has_sparse_vm_mappings);
    fprintf(f, "    has_stable_pstate = %u\n", info->has_stable_pstate);
+   fprintf(f, "    has_explicit_sync_vm_ops = %u\n", info->has_explicit_sync_vm_ops);
    fprintf(f, "    has_scheduled_fence_dependency = %u\n", info->has_scheduled_fence_dependency);
    fprintf(f, "    has_gang_submit = %u\n", info->has_gang_submit);
    fprintf(f, "    has_gpuvm_fault_query = %u\n", info->has_gpuvm_fault_query);
diff --git a/src/amd/common/ac_gpu_info.h b/src/amd/common/ac_gpu_info.h
index e2550c1428ce..c8c51db8c7dc 100644
--- a/src/amd/common/ac_gpu_info.h
+++ b/src/amd/common/ac_gpu_info.h
@@ -207,6 +207,7 @@ struct radeon_info {
    bool has_gpuvm_fault_query;
    bool has_pcie_bandwidth_info;
    bool has_stable_pstate;
+   bool has_explicit_sync_vm_ops;
    /* Whether SR-IOV is enabled or amdgpu.mcbp=1 was set on the kernel command line. */
    bool register_shadowing_required;
    bool has_tmz_support;
-- 
GitLab


From eddcfa8363c3ef0ab23abdfcb31fbdd8e1c77222 Mon Sep 17 00:00:00 2001
From: Bas Nieuwenhuizen <bas@basnieuwenhuizen.nl>
Date: Tue, 2 Aug 2022 01:17:00 +0200
Subject: [PATCH 06/12] radv: Wait before sparse binding changes.

Without implicit sync sparse mappings don't wait on previous
submissions anymore, and will attempt to execute immediately. Hence
we need to wait on all dependencies before executing.

The sparse queue already uses a submission thread to make the app
not block on this.
---
 src/amd/vulkan/radv_queue.c | 20 ++++++++++++++------
 1 file changed, 14 insertions(+), 6 deletions(-)

diff --git a/src/amd/vulkan/radv_queue.c b/src/amd/vulkan/radv_queue.c
index 85aa03c21259..f2f785258087 100644
--- a/src/amd/vulkan/radv_queue.c
+++ b/src/amd/vulkan/radv_queue.c
@@ -1689,16 +1689,24 @@ radv_queue_sparse_submit(struct vk_queue *vqueue, struct vk_queue_submit *submis
    struct radv_device *device = queue->device;
    VkResult result;
 
+   if (device->physical_device->rad_info.has_explicit_sync_contexts) {
+      /* We do a CPU wait here, as the sparse binds get executed immediately. */
+      result =
+         vk_sync_wait_many(&device->vk, submission->wait_count, submission->waits, VK_SYNC_WAIT_COMPLETE, UINT64_MAX);
+      if (result != VK_SUCCESS)
+         goto fail;
+   }
+
    result = radv_queue_submit_bind_sparse_memory(device, submission);
    if (result != VK_SUCCESS)
       goto fail;
 
-   /* We do a CPU wait here, in part to avoid more winsys mechanisms. In the likely kernel explicit
-    * sync mechanism, we'd need to do a CPU wait anyway. Haven't seen this be a perf issue yet, but
-    * we have to make sure the queue always has its submission thread enabled. */
-   result = vk_sync_wait_many(&device->vk, submission->wait_count, submission->waits, 0, UINT64_MAX);
-   if (result != VK_SUCCESS)
-      goto fail;
+   if (!device->physical_device->rad_info.has_explicit_sync_contexts) {
+      /* We do a CPU wait here, so that all dependencies have finished before we start signaling the signal vk_syncs. */
+      result = vk_sync_wait_many(&device->vk, submission->wait_count, submission->waits, 0, UINT64_MAX);
+      if (result != VK_SUCCESS)
+         goto fail;
+   }
 
    /* Ignore all the commandbuffers. They're necessarily empty anyway. */
 
-- 
GitLab


From 21bf4e34981f3f49a104d28f514816e0194c71a8 Mon Sep 17 00:00:00 2001
From: Bas Nieuwenhuizen <bas@basnieuwenhuizen.nl>
Date: Tue, 2 Aug 2022 03:11:04 +0200
Subject: [PATCH 07/12] radv: Wait for queue idle before deleting preamble
 buffers.

Without implicit sync we might unmap these buffers immediately. Better
make sure they are not in use. I think this is uncommon enough that we
can just wait instead of setting up async delete functionality.
---
 src/amd/vulkan/radv_queue.c | 236 +++++++++++++++++++-----------------
 1 file changed, 128 insertions(+), 108 deletions(-)

diff --git a/src/amd/vulkan/radv_queue.c b/src/amd/vulkan/radv_queue.c
index f2f785258087..3b8b6fde49ce 100644
--- a/src/amd/vulkan/radv_queue.c
+++ b/src/amd/vulkan/radv_queue.c
@@ -816,28 +816,30 @@ radv_init_compute_state(struct radeon_cmdbuf *cs, struct radv_device *device)
 }
 
 static VkResult
-radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *device,
+radv_update_preamble_cs(struct radv_queue_state *queue_state, struct radv_queue *queue,
                         const struct radv_queue_ring_info *needs)
 {
+   struct radv_device *device = queue->device;
    struct radeon_winsys *ws = device->ws;
-   struct radeon_winsys_bo *scratch_bo = queue->scratch_bo;
-   struct radeon_winsys_bo *descriptor_bo = queue->descriptor_bo;
-   struct radeon_winsys_bo *compute_scratch_bo = queue->compute_scratch_bo;
-   struct radeon_winsys_bo *esgs_ring_bo = queue->esgs_ring_bo;
-   struct radeon_winsys_bo *gsvs_ring_bo = queue->gsvs_ring_bo;
-   struct radeon_winsys_bo *tess_rings_bo = queue->tess_rings_bo;
-   struct radeon_winsys_bo *task_rings_bo = queue->task_rings_bo;
-   struct radeon_winsys_bo *mesh_scratch_ring_bo = queue->mesh_scratch_ring_bo;
-   struct radeon_winsys_bo *attr_ring_bo = queue->attr_ring_bo;
-   struct radeon_winsys_bo *gds_bo = queue->gds_bo;
-   struct radeon_winsys_bo *gds_oa_bo = queue->gds_oa_bo;
+   struct radeon_winsys_bo *scratch_bo = queue_state->scratch_bo;
+   struct radeon_winsys_bo *descriptor_bo = queue_state->descriptor_bo;
+   struct radeon_winsys_bo *compute_scratch_bo = queue_state->compute_scratch_bo;
+   struct radeon_winsys_bo *esgs_ring_bo = queue_state->esgs_ring_bo;
+   struct radeon_winsys_bo *gsvs_ring_bo = queue_state->gsvs_ring_bo;
+   struct radeon_winsys_bo *tess_rings_bo = queue_state->tess_rings_bo;
+   struct radeon_winsys_bo *task_rings_bo = queue_state->task_rings_bo;
+   struct radeon_winsys_bo *mesh_scratch_ring_bo = queue_state->mesh_scratch_ring_bo;
+   struct radeon_winsys_bo *attr_ring_bo = queue_state->attr_ring_bo;
+   struct radeon_winsys_bo *gds_bo = queue_state->gds_bo;
+   struct radeon_winsys_bo *gds_oa_bo = queue_state->gds_oa_bo;
    struct radeon_cmdbuf *dest_cs[3] = {0};
    const uint32_t ring_bo_flags = RADEON_FLAG_NO_CPU_ACCESS | RADEON_FLAG_NO_INTERPROCESS_SHARING;
    VkResult result = VK_SUCCESS;
 
-   const bool add_sample_positions = !queue->ring_info.sample_positions && needs->sample_positions;
+   const bool add_sample_positions = !queue_state->ring_info.sample_positions && needs->sample_positions;
    const uint32_t scratch_size = needs->scratch_size_per_wave * needs->scratch_waves;
-   const uint32_t queue_scratch_size = queue->ring_info.scratch_size_per_wave * queue->ring_info.scratch_waves;
+   const uint32_t queue_scratch_size =
+      queue_state->ring_info.scratch_size_per_wave * queue_state->ring_info.scratch_waves;
 
    if (scratch_size > queue_scratch_size) {
       result = ws->buffer_create(ws, scratch_size, 4096, RADEON_DOMAIN_VRAM, ring_bo_flags, RADV_BO_PRIORITY_SCRATCH, 0,
@@ -849,7 +851,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
 
    const uint32_t compute_scratch_size = needs->compute_scratch_size_per_wave * needs->compute_scratch_waves;
    const uint32_t compute_queue_scratch_size =
-      queue->ring_info.compute_scratch_size_per_wave * queue->ring_info.compute_scratch_waves;
+      queue_state->ring_info.compute_scratch_size_per_wave * queue_state->ring_info.compute_scratch_waves;
    if (compute_scratch_size > compute_queue_scratch_size) {
       result = ws->buffer_create(ws, compute_scratch_size, 4096, RADEON_DOMAIN_VRAM, ring_bo_flags,
                                  RADV_BO_PRIORITY_SCRATCH, 0, &compute_scratch_bo);
@@ -858,7 +860,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
       radv_rmv_log_command_buffer_bo_create(device, compute_scratch_bo, 0, 0, compute_scratch_size);
    }
 
-   if (needs->esgs_ring_size > queue->ring_info.esgs_ring_size) {
+   if (needs->esgs_ring_size > queue_state->ring_info.esgs_ring_size) {
       result = ws->buffer_create(ws, needs->esgs_ring_size, 4096, RADEON_DOMAIN_VRAM, ring_bo_flags,
                                  RADV_BO_PRIORITY_SCRATCH, 0, &esgs_ring_bo);
       if (result != VK_SUCCESS)
@@ -866,7 +868,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
       radv_rmv_log_command_buffer_bo_create(device, esgs_ring_bo, 0, 0, needs->esgs_ring_size);
    }
 
-   if (needs->gsvs_ring_size > queue->ring_info.gsvs_ring_size) {
+   if (needs->gsvs_ring_size > queue_state->ring_info.gsvs_ring_size) {
       result = ws->buffer_create(ws, needs->gsvs_ring_size, 4096, RADEON_DOMAIN_VRAM, ring_bo_flags,
                                  RADV_BO_PRIORITY_SCRATCH, 0, &gsvs_ring_bo);
       if (result != VK_SUCCESS)
@@ -874,7 +876,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
       radv_rmv_log_command_buffer_bo_create(device, gsvs_ring_bo, 0, 0, needs->gsvs_ring_size);
    }
 
-   if (!queue->ring_info.tess_rings && needs->tess_rings) {
+   if (!queue_state->ring_info.tess_rings && needs->tess_rings) {
       uint64_t tess_rings_size =
          device->physical_device->hs.tess_offchip_ring_offset + device->physical_device->hs.tess_offchip_ring_size;
       result = ws->buffer_create(ws, tess_rings_size, 256, RADEON_DOMAIN_VRAM, ring_bo_flags, RADV_BO_PRIORITY_SCRATCH,
@@ -884,7 +886,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
       radv_rmv_log_command_buffer_bo_create(device, tess_rings_bo, 0, 0, tess_rings_size);
    }
 
-   if (!queue->ring_info.task_rings && needs->task_rings) {
+   if (!queue_state->ring_info.task_rings && needs->task_rings) {
       assert(device->physical_device->rad_info.gfx_level >= GFX10_3);
 
       /* We write the control buffer from the CPU, so need to grant CPU access to the BO.
@@ -905,7 +907,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
          goto fail;
    }
 
-   if (!queue->ring_info.mesh_scratch_ring && needs->mesh_scratch_ring) {
+   if (!queue_state->ring_info.mesh_scratch_ring && needs->mesh_scratch_ring) {
       assert(device->physical_device->rad_info.gfx_level >= GFX10_3);
       result = ws->buffer_create(ws, RADV_MESH_SCRATCH_NUM_ENTRIES * RADV_MESH_SCRATCH_ENTRY_BYTES, 256,
                                  RADEON_DOMAIN_VRAM, ring_bo_flags, RADV_BO_PRIORITY_SCRATCH, 0, &mesh_scratch_ring_bo);
@@ -916,7 +918,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
                                             RADV_MESH_SCRATCH_NUM_ENTRIES * RADV_MESH_SCRATCH_ENTRY_BYTES);
    }
 
-   if (needs->attr_ring_size > queue->ring_info.attr_ring_size) {
+   if (needs->attr_ring_size > queue_state->ring_info.attr_ring_size) {
       assert(device->physical_device->rad_info.gfx_level >= GFX11);
       result = ws->buffer_create(ws, needs->attr_ring_size, 2 * 1024 * 1024 /* 2MiB */, RADEON_DOMAIN_VRAM,
                                  RADEON_FLAG_32BIT | RADEON_FLAG_DISCARDABLE | ring_bo_flags, RADV_BO_PRIORITY_SCRATCH,
@@ -926,7 +928,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
       radv_rmv_log_command_buffer_bo_create(device, attr_ring_bo, 0, 0, needs->attr_ring_size);
    }
 
-   if (!queue->ring_info.gds && needs->gds) {
+   if (!queue_state->ring_info.gds && needs->gds) {
       assert(device->physical_device->rad_info.gfx_level >= GFX10);
 
       /* 4 streamout GDS counters.
@@ -944,7 +946,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
          goto fail;
    }
 
-   if (!queue->ring_info.gds_oa && needs->gds_oa) {
+   if (!queue_state->ring_info.gds_oa && needs->gds_oa) {
       assert(device->physical_device->rad_info.gfx_level >= GFX10);
 
       result = ws->buffer_create(ws, 1, 1, RADEON_DOMAIN_OA, ring_bo_flags, RADV_BO_PRIORITY_SCRATCH, 0, &gds_oa_bo);
@@ -965,11 +967,11 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
     * when it uses the task shader rings. The task rings BO is shared between the
     * GFX and compute queues and already initialized here.
     */
-   if ((queue->qf == RADV_QUEUE_COMPUTE && !descriptor_bo && task_rings_bo) || scratch_bo != queue->scratch_bo ||
-       esgs_ring_bo != queue->esgs_ring_bo || gsvs_ring_bo != queue->gsvs_ring_bo ||
-       tess_rings_bo != queue->tess_rings_bo || task_rings_bo != queue->task_rings_bo ||
-       mesh_scratch_ring_bo != queue->mesh_scratch_ring_bo || attr_ring_bo != queue->attr_ring_bo ||
-       add_sample_positions) {
+   if ((queue_state->qf == RADV_QUEUE_COMPUTE && !descriptor_bo && task_rings_bo) ||
+       scratch_bo != queue_state->scratch_bo || esgs_ring_bo != queue_state->esgs_ring_bo ||
+       gsvs_ring_bo != queue_state->gsvs_ring_bo || tess_rings_bo != queue_state->tess_rings_bo ||
+       task_rings_bo != queue_state->task_rings_bo || mesh_scratch_ring_bo != queue_state->mesh_scratch_ring_bo ||
+       attr_ring_bo != queue_state->attr_ring_bo || add_sample_positions) {
       const uint32_t size = 304;
 
       result = ws->buffer_create(ws, size, 4096, RADEON_DOMAIN_VRAM,
@@ -979,7 +981,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
          goto fail;
    }
 
-   if (descriptor_bo != queue->descriptor_bo) {
+   if (descriptor_bo != queue_state->descriptor_bo) {
       uint32_t *map = (uint32_t *)ws->buffer_map(descriptor_bo);
       if (!map)
          goto fail;
@@ -994,7 +996,7 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
    for (int i = 0; i < 3; ++i) {
       enum rgp_flush_bits sqtt_flush_bits = 0;
       struct radeon_cmdbuf *cs = NULL;
-      cs = ws->cs_create(ws, radv_queue_family_to_ring(device->physical_device, queue->qf), false);
+      cs = ws->cs_create(ws, radv_queue_family_to_ring(device->physical_device, queue_state->qf), false);
       if (!cs) {
          result = VK_ERROR_OUT_OF_DEVICE_MEMORY;
          goto fail;
@@ -1007,10 +1009,10 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
          radv_cs_add_buffer(ws, cs, scratch_bo);
 
       /* Emit initial configuration. */
-      switch (queue->qf) {
+      switch (queue_state->qf) {
       case RADV_QUEUE_GENERAL:
-         if (queue->uses_shadow_regs)
-            radv_emit_shadow_regs_preamble(cs, device, queue);
+         if (queue_state->uses_shadow_regs)
+            radv_emit_shadow_regs_preamble(cs, device, queue_state);
          radv_init_graphics_state(cs, device);
 
          if (esgs_ring_bo || gsvs_ring_bo || tess_rings_bo || task_rings_bo) {
@@ -1057,11 +1059,11 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
          if (i == 0) {
             /* The full flush preamble should also wait for previous shader work to finish. */
             flush_bits |= RADV_CMD_FLAG_CS_PARTIAL_FLUSH;
-            if (queue->qf == RADV_QUEUE_GENERAL)
+            if (queue_state->qf == RADV_QUEUE_GENERAL)
                flush_bits |= RADV_CMD_FLAG_PS_PARTIAL_FLUSH;
          }
 
-         si_cs_emit_cache_flush(ws, cs, gfx_level, NULL, 0, queue->qf, flush_bits, &sqtt_flush_bits, 0);
+         si_cs_emit_cache_flush(ws, cs, gfx_level, NULL, 0, queue_state->qf, flush_bits, &sqtt_flush_bits, 0);
       }
 
       result = ws->cs_finalize(cs);
@@ -1069,91 +1071,106 @@ radv_update_preamble_cs(struct radv_queue_state *queue, struct radv_device *devi
          goto fail;
    }
 
-   if (queue->initial_full_flush_preamble_cs)
-      ws->cs_destroy(queue->initial_full_flush_preamble_cs);
+   if (queue_state->initial_full_flush_preamble_cs || queue_state->initial_preamble_cs ||
+       queue_state->continue_preamble_cs || scratch_bo != queue_state->scratch_bo ||
+       compute_scratch_bo != queue_state->compute_scratch_bo || esgs_ring_bo != queue_state->esgs_ring_bo ||
+       gsvs_ring_bo != queue_state->gsvs_ring_bo || descriptor_bo != queue_state->descriptor_bo) {
 
-   if (queue->initial_preamble_cs)
-      ws->cs_destroy(queue->initial_preamble_cs);
+      if (device->physical_device->rad_info.has_explicit_sync_vm_ops) {
+         bool success =
+            queue->device->ws->ctx_wait_idle(queue->hw_ctx, radv_queue_ring(queue), queue->vk.index_in_family);
+         if (!success) {
+            result = VK_ERROR_DEVICE_LOST;
+            goto fail;
+         }
+      }
+   }
 
-   if (queue->continue_preamble_cs)
-      ws->cs_destroy(queue->continue_preamble_cs);
+   if (queue_state->initial_full_flush_preamble_cs)
+      ws->cs_destroy(queue_state->initial_full_flush_preamble_cs);
+
+   if (queue_state->initial_preamble_cs)
+      ws->cs_destroy(queue_state->initial_preamble_cs);
+
+   if (queue_state->continue_preamble_cs)
+      ws->cs_destroy(queue_state->continue_preamble_cs);
 
-   queue->initial_full_flush_preamble_cs = dest_cs[0];
-   queue->initial_preamble_cs = dest_cs[1];
-   queue->continue_preamble_cs = dest_cs[2];
+   queue_state->initial_full_flush_preamble_cs = dest_cs[0];
+   queue_state->initial_preamble_cs = dest_cs[1];
+   queue_state->continue_preamble_cs = dest_cs[2];
 
-   if (scratch_bo != queue->scratch_bo) {
-      if (queue->scratch_bo) {
-         ws->buffer_destroy(ws, queue->scratch_bo);
-         radv_rmv_log_command_buffer_bo_destroy(device, queue->scratch_bo);
+   if (scratch_bo != queue_state->scratch_bo) {
+      if (queue_state->scratch_bo) {
+         ws->buffer_destroy(ws, queue_state->scratch_bo);
+         radv_rmv_log_command_buffer_bo_destroy(device, queue_state->scratch_bo);
       }
-      queue->scratch_bo = scratch_bo;
+      queue_state->scratch_bo = scratch_bo;
    }
 
-   if (compute_scratch_bo != queue->compute_scratch_bo) {
-      if (queue->compute_scratch_bo) {
-         ws->buffer_destroy(ws, queue->compute_scratch_bo);
-         radv_rmv_log_command_buffer_bo_destroy(device, queue->compute_scratch_bo);
+   if (compute_scratch_bo != queue_state->compute_scratch_bo) {
+      if (queue_state->compute_scratch_bo) {
+         ws->buffer_destroy(ws, queue_state->compute_scratch_bo);
+         radv_rmv_log_command_buffer_bo_destroy(device, queue_state->compute_scratch_bo);
       }
-      queue->compute_scratch_bo = compute_scratch_bo;
+      queue_state->compute_scratch_bo = compute_scratch_bo;
    }
 
-   if (esgs_ring_bo != queue->esgs_ring_bo) {
-      if (queue->esgs_ring_bo) {
-         ws->buffer_destroy(ws, queue->esgs_ring_bo);
-         radv_rmv_log_command_buffer_bo_destroy(device, queue->esgs_ring_bo);
+   if (esgs_ring_bo != queue_state->esgs_ring_bo) {
+      if (queue_state->esgs_ring_bo) {
+         ws->buffer_destroy(ws, queue_state->esgs_ring_bo);
+         radv_rmv_log_command_buffer_bo_destroy(device, queue_state->esgs_ring_bo);
       }
-      queue->esgs_ring_bo = esgs_ring_bo;
+      queue_state->esgs_ring_bo = esgs_ring_bo;
    }
 
-   if (gsvs_ring_bo != queue->gsvs_ring_bo) {
-      if (queue->gsvs_ring_bo) {
-         ws->buffer_destroy(ws, queue->gsvs_ring_bo);
-         radv_rmv_log_command_buffer_bo_destroy(device, queue->gsvs_ring_bo);
+   if (gsvs_ring_bo != queue_state->gsvs_ring_bo) {
+      if (queue_state->gsvs_ring_bo) {
+         ws->buffer_destroy(ws, queue_state->gsvs_ring_bo);
+         radv_rmv_log_command_buffer_bo_destroy(device, queue_state->gsvs_ring_bo);
       }
-      queue->gsvs_ring_bo = gsvs_ring_bo;
+      queue_state->gsvs_ring_bo = gsvs_ring_bo;
    }
 
-   if (descriptor_bo != queue->descriptor_bo) {
-      if (queue->descriptor_bo)
-         ws->buffer_destroy(ws, queue->descriptor_bo);
-      queue->descriptor_bo = descriptor_bo;
+   if (descriptor_bo != queue_state->descriptor_bo) {
+      if (queue_state->descriptor_bo)
+         ws->buffer_destroy(ws, queue_state->descriptor_bo);
+      queue_state->descriptor_bo = descriptor_bo;
    }
 
-   queue->tess_rings_bo = tess_rings_bo;
-   queue->task_rings_bo = task_rings_bo;
-   queue->mesh_scratch_ring_bo = mesh_scratch_ring_bo;
-   queue->attr_ring_bo = attr_ring_bo;
-   queue->gds_bo = gds_bo;
-   queue->gds_oa_bo = gds_oa_bo;
-   queue->ring_info = *needs;
+   queue_state->tess_rings_bo = tess_rings_bo;
+   queue_state->task_rings_bo = task_rings_bo;
+   queue_state->mesh_scratch_ring_bo = mesh_scratch_ring_bo;
+   queue_state->attr_ring_bo = attr_ring_bo;
+   queue_state->gds_bo = gds_bo;
+   queue_state->gds_oa_bo = gds_oa_bo;
+   queue_state->ring_info = *needs;
    return VK_SUCCESS;
 fail:
    for (int i = 0; i < ARRAY_SIZE(dest_cs); ++i)
       if (dest_cs[i])
          ws->cs_destroy(dest_cs[i]);
-   if (descriptor_bo && descriptor_bo != queue->descriptor_bo)
+   if (descriptor_bo && descriptor_bo != queue_state->descriptor_bo)
       ws->buffer_destroy(ws, descriptor_bo);
-   if (scratch_bo && scratch_bo != queue->scratch_bo)
+   if (scratch_bo && scratch_bo != queue_state->scratch_bo)
       ws->buffer_destroy(ws, scratch_bo);
-   if (compute_scratch_bo && compute_scratch_bo != queue->compute_scratch_bo)
+   if (compute_scratch_bo && compute_scratch_bo != queue_state->compute_scratch_bo)
       ws->buffer_destroy(ws, compute_scratch_bo);
-   if (esgs_ring_bo && esgs_ring_bo != queue->esgs_ring_bo)
+   if (esgs_ring_bo && esgs_ring_bo != queue_state->esgs_ring_bo)
       ws->buffer_destroy(ws, esgs_ring_bo);
-   if (gsvs_ring_bo && gsvs_ring_bo != queue->gsvs_ring_bo)
+   if (gsvs_ring_bo && gsvs_ring_bo != queue_state->gsvs_ring_bo)
       ws->buffer_destroy(ws, gsvs_ring_bo);
-   if (tess_rings_bo && tess_rings_bo != queue->tess_rings_bo)
+   if (tess_rings_bo && tess_rings_bo != queue_state->tess_rings_bo)
       ws->buffer_destroy(ws, tess_rings_bo);
-   if (task_rings_bo && task_rings_bo != queue->task_rings_bo)
+   if (task_rings_bo && task_rings_bo != queue_state->task_rings_bo)
       ws->buffer_destroy(ws, task_rings_bo);
-   if (attr_ring_bo && attr_ring_bo != queue->attr_ring_bo)
+   if (attr_ring_bo && attr_ring_bo != queue_state->attr_ring_bo)
       ws->buffer_destroy(ws, attr_ring_bo);
-   if (gds_bo && gds_bo != queue->gds_bo) {
-      ws->buffer_make_resident(ws, queue->gds_bo, false);
+   if (gds_bo && gds_bo != queue_state->gds_bo) {
+      ws->buffer_make_resident(ws, queue_state->gds_bo, false);
       ws->buffer_destroy(ws, gds_bo);
    }
-   if (gds_oa_bo && gds_oa_bo != queue->gds_oa_bo) {
-      ws->buffer_make_resident(ws, queue->gds_oa_bo, false);
+   if (gds_oa_bo && gds_oa_bo != queue_state->gds_oa_bo) {
+      ws->buffer_make_resident(ws, queue_state->gds_oa_bo, false);
       ws->buffer_destroy(ws, gds_oa_bo);
    }
 
@@ -1161,27 +1178,28 @@ fail:
 }
 
 static VkResult
-radv_update_preambles(struct radv_queue_state *queue, struct radv_device *device,
+radv_update_preambles(struct radv_queue_state *queue_state, struct radv_queue *queue,
                       struct vk_command_buffer *const *cmd_buffers, uint32_t cmd_buffer_count, bool *use_perf_counters,
                       bool *has_follower)
 {
-   if (queue->qf != RADV_QUEUE_GENERAL && queue->qf != RADV_QUEUE_COMPUTE) {
+   if (queue_state->qf != RADV_QUEUE_GENERAL && queue_state->qf != RADV_QUEUE_COMPUTE) {
       for (uint32_t j = 0; j < cmd_buffer_count; j++) {
          struct radv_cmd_buffer *cmd_buffer = container_of(cmd_buffers[j], struct radv_cmd_buffer, vk);
 
          *has_follower |= !!cmd_buffer->gang.cs;
       }
-
       return VK_SUCCESS;
    }
 
+   struct radv_device *device = queue->device;
+
    /* Figure out the needs of the current submission.
     * Start by copying the queue's current info.
     * This is done because we only allow two possible behaviours for these buffers:
     * - Grow when the newly needed amount is larger than what we had
     * - Allocate the max size and reuse it, but don't free it until the queue is destroyed
     */
-   struct radv_queue_ring_info needs = queue->ring_info;
+   struct radv_queue_ring_info needs = queue_state->ring_info;
    *use_perf_counters = false;
    *has_follower = false;
 
@@ -1213,7 +1231,7 @@ radv_update_preambles(struct radv_queue_state *queue, struct radv_device *device
          ? MIN2(needs.compute_scratch_waves, UINT32_MAX / needs.compute_scratch_size_per_wave)
          : 0;
 
-   if (device->physical_device->rad_info.gfx_level >= GFX11 && queue->qf == RADV_QUEUE_GENERAL) {
+   if (device->physical_device->rad_info.gfx_level >= GFX11 && queue_state->qf == RADV_QUEUE_GENERAL) {
       needs.attr_ring_size =
          device->physical_device->rad_info.attribute_ring_size_per_se * device->physical_device->rad_info.max_se;
    }
@@ -1222,19 +1240,21 @@ radv_update_preambles(struct radv_queue_state *queue, struct radv_device *device
     * Note that it's not possible for any of the needed values to be less
     * than what the queue already had, because we only ever increase the allocated size.
     */
-   if (queue->initial_full_flush_preamble_cs && queue->ring_info.scratch_size_per_wave == needs.scratch_size_per_wave &&
-       queue->ring_info.scratch_waves == needs.scratch_waves &&
-       queue->ring_info.compute_scratch_size_per_wave == needs.compute_scratch_size_per_wave &&
-       queue->ring_info.compute_scratch_waves == needs.compute_scratch_waves &&
-       queue->ring_info.esgs_ring_size == needs.esgs_ring_size &&
-       queue->ring_info.gsvs_ring_size == needs.gsvs_ring_size && queue->ring_info.tess_rings == needs.tess_rings &&
-       queue->ring_info.task_rings == needs.task_rings &&
-       queue->ring_info.mesh_scratch_ring == needs.mesh_scratch_ring &&
-       queue->ring_info.attr_ring_size == needs.attr_ring_size && queue->ring_info.gds == needs.gds &&
-       queue->ring_info.gds_oa == needs.gds_oa && queue->ring_info.sample_positions == needs.sample_positions)
+   if (queue_state->initial_full_flush_preamble_cs &&
+       queue_state->ring_info.scratch_size_per_wave == needs.scratch_size_per_wave &&
+       queue_state->ring_info.scratch_waves == needs.scratch_waves &&
+       queue_state->ring_info.compute_scratch_size_per_wave == needs.compute_scratch_size_per_wave &&
+       queue_state->ring_info.compute_scratch_waves == needs.compute_scratch_waves &&
+       queue_state->ring_info.esgs_ring_size == needs.esgs_ring_size &&
+       queue_state->ring_info.gsvs_ring_size == needs.gsvs_ring_size &&
+       queue_state->ring_info.tess_rings == needs.tess_rings && queue_state->ring_info.task_rings == needs.task_rings &&
+       queue_state->ring_info.mesh_scratch_ring == needs.mesh_scratch_ring &&
+       queue_state->ring_info.attr_ring_size == needs.attr_ring_size && queue_state->ring_info.gds == needs.gds &&
+       queue_state->ring_info.gds_oa == needs.gds_oa &&
+       queue_state->ring_info.sample_positions == needs.sample_positions)
       return VK_SUCCESS;
 
-   return radv_update_preamble_cs(queue, device, &needs);
+   return radv_update_preamble_cs(queue_state, queue, &needs);
 }
 
 static VkResult
@@ -1379,7 +1399,7 @@ radv_update_gang_preambles(struct radv_queue *queue)
    needs.compute_scratch_waves = queue->state.ring_info.scratch_waves;
    needs.task_rings = queue->state.ring_info.task_rings;
 
-   r = radv_update_preamble_cs(queue->follower_state, queue->device, &needs);
+   r = radv_update_preamble_cs(queue->follower_state, queue, &needs);
    if (r != VK_SUCCESS)
       return r;
 
@@ -1494,8 +1514,8 @@ radv_queue_submit_normal(struct radv_queue *queue, struct vk_queue_submit *submi
    uint32_t wait_count = submission->wait_count;
    struct vk_sync_wait *waits = submission->waits;
 
-   result = radv_update_preambles(&queue->state, queue->device, submission->command_buffers,
-                                  submission->command_buffer_count, &use_perf_counters, &use_ace);
+   result = radv_update_preambles(&queue->state, queue, submission->command_buffers, submission->command_buffer_count,
+                                  &use_perf_counters, &use_ace);
    if (result != VK_SUCCESS)
       return result;
 
@@ -1689,7 +1709,7 @@ radv_queue_sparse_submit(struct vk_queue *vqueue, struct vk_queue_submit *submis
    struct radv_device *device = queue->device;
    VkResult result;
 
-   if (device->physical_device->rad_info.has_explicit_sync_contexts) {
+   if (device->physical_device->rad_info.has_explicit_sync_vm_ops) {
       /* We do a CPU wait here, as the sparse binds get executed immediately. */
       result =
          vk_sync_wait_many(&device->vk, submission->wait_count, submission->waits, VK_SYNC_WAIT_COMPLETE, UINT64_MAX);
@@ -1701,7 +1721,7 @@ radv_queue_sparse_submit(struct vk_queue *vqueue, struct vk_queue_submit *submis
    if (result != VK_SUCCESS)
       goto fail;
 
-   if (!device->physical_device->rad_info.has_explicit_sync_contexts) {
+   if (!device->physical_device->rad_info.has_explicit_sync_vm_ops) {
       /* We do a CPU wait here, so that all dependencies have finished before we start signaling the signal vk_syncs. */
       result = vk_sync_wait_many(&device->vk, submission->wait_count, submission->waits, 0, UINT64_MAX);
       if (result != VK_SUCCESS)
-- 
GitLab


From ddba8bb1d67a7c83be81bae95d7af1cfda373d35 Mon Sep 17 00:00:00 2001
From: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
Date: Tue, 22 Aug 2023 22:05:49 +0900
Subject: [PATCH 08/12] radv: Disable implicit sync for VM ops.

---
 src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c
index b71b62b4f421..cb81f58e6308 100644
--- a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c
+++ b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c
@@ -59,6 +59,9 @@ radv_amdgpu_bo_va_op(struct radv_amdgpu_winsys *ws, amdgpu_bo_handle bo, uint64_
          flags |= AMDGPU_VM_PAGE_WRITEABLE;
    }
 
+   if (ws->info.has_explicit_sync_vm_ops)
+      flags |= AMDGPU_VM_EXPLICIT_SYNC;
+
    size = align64(size, getpagesize());
 
    return amdgpu_bo_va_op_raw(ws->dev, bo, offset, size, addr, flags, ops);
-- 
GitLab
