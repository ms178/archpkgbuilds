From d34e4c9808e24009711b02393abda06f7df080c9 Mon Sep 17 00:00:00 2001
From: Georg Lehmann <dadschoorse@gmail.com>
Date: Sat, 27 May 2023 11:27:12 +0200
Subject: [PATCH 1/6] nir: add single bit test opcodes

These directly map to amd's SALU s_bitcmp0/1.
For VALU we can use v_cmp_class_f32 if the second source is constant.
---
 src/compiler/nir/nir.c          |  2 ++
 src/compiler/nir/nir_opcodes.py | 38 ++++++++++++++++++++-------------
 2 files changed, 25 insertions(+), 15 deletions(-)

diff --git a/src/compiler/nir/nir.c b/src/compiler/nir/nir.c
index 30174ff965bd6..788c9fb5051a6 100644
--- a/src/compiler/nir/nir.c
+++ b/src/compiler/nir/nir.c
@@ -3218,6 +3218,8 @@ nir_alu_instr_is_comparison(const nir_alu_instr *instr)
    CASE_ALL_SIZES(nir_op_uge)
    CASE_ALL_SIZES(nir_op_ieq)
    CASE_ALL_SIZES(nir_op_ine)
+   CASE_ALL_SIZES(nir_op_bit_test0)
+   CASE_ALL_SIZES(nir_op_bit_test1)
    case nir_op_inot:
       return true;
    default:
diff --git a/src/compiler/nir/nir_opcodes.py b/src/compiler/nir/nir_opcodes.py
index ef4a732757546..28bea39558bfe 100644
--- a/src/compiler/nir/nir_opcodes.py
+++ b/src/compiler/nir/nir_opcodes.py
@@ -571,30 +571,32 @@ if (src0.z < 0 && absZ >= absX && absZ >= absY) dst.x = 5;
 unop_reduce("fsum", 1, tfloat, tfloat, "{src}", "{src0} + {src1}", "{src}",
             description = "Sum of vector components")
 
-def binop_convert(name, out_type, in_type, alg_props, const_expr, description=""):
-   opcode(name, 0, out_type, [0, 0], [in_type, in_type],
+def binop_convert(name, out_type, in_type1, alg_props, const_expr, description="", in_type2=None):
+   if in_type2 is None:
+      in_type2 = in_type1
+   opcode(name, 0, out_type, [0, 0], [in_type1, in_type2],
           False, alg_props, const_expr, description)
 
 def binop(name, ty, alg_props, const_expr, description = ""):
    binop_convert(name, ty, ty, alg_props, const_expr, description)
 
-def binop_compare(name, ty, alg_props, const_expr, description = ""):
-   binop_convert(name, tbool1, ty, alg_props, const_expr, description)
+def binop_compare(name, ty, alg_props, const_expr, description = "", ty2=None):
+   binop_convert(name, tbool1, ty, alg_props, const_expr, description, ty2)
 
-def binop_compare8(name, ty, alg_props, const_expr, description = ""):
-   binop_convert(name, tbool8, ty, alg_props, const_expr, description)
+def binop_compare8(name, ty, alg_props, const_expr, description = "", ty2=None):
+   binop_convert(name, tbool8, ty, alg_props, const_expr, description, ty2)
 
-def binop_compare16(name, ty, alg_props, const_expr, description = ""):
-   binop_convert(name, tbool16, ty, alg_props, const_expr, description)
+def binop_compare16(name, ty, alg_props, const_expr, description = "", ty2=None):
+   binop_convert(name, tbool16, ty, alg_props, const_expr, description, ty2)
 
-def binop_compare32(name, ty, alg_props, const_expr, description = ""):
-   binop_convert(name, tbool32, ty, alg_props, const_expr, description)
+def binop_compare32(name, ty, alg_props, const_expr, description = "", ty2=None):
+   binop_convert(name, tbool32, ty, alg_props, const_expr, description, ty2)
 
-def binop_compare_all_sizes(name, ty, alg_props, const_expr, description = ""):
-   binop_compare(name, ty, alg_props, const_expr, description)
-   binop_compare8(name + "8", ty, alg_props, const_expr, description)
-   binop_compare16(name + "16", ty, alg_props, const_expr, description)
-   binop_compare32(name + "32", ty, alg_props, const_expr, description)
+def binop_compare_all_sizes(name, ty, alg_props, const_expr, description = "", ty2=None):
+   binop_compare(name, ty, alg_props, const_expr, description, ty2)
+   binop_compare8(name + "8", ty, alg_props, const_expr, description, ty2)
+   binop_compare16(name + "16", ty, alg_props, const_expr, description, ty2)
+   binop_compare32(name + "32", ty, alg_props, const_expr, description, ty2)
 
 def binop_horiz(name, out_size, out_type, src1_size, src1_type, src2_size,
                 src2_type, const_expr, description = ""):
@@ -847,6 +849,12 @@ binop_compare_all_sizes("ine", tint, _2src_commutative, "src0 != src1")
 binop_compare_all_sizes("ult", tuint, "", "src0 < src1")
 binop_compare_all_sizes("uge", tuint, "", "src0 >= src1")
 
+binop_compare_all_sizes("bit_test1", tuint, "", "((uint64_t)src0 >> (src1 & (bit_size - 1)) & 0x1) == 0x1",
+   "only uses the least significant bits like SM5 shifts", tuint32)
+
+binop_compare_all_sizes("bit_test0", tuint, "", "((uint64_t)src0 >> (src1 & (bit_size - 1)) & 0x1) == 0x0",
+   "only uses the least significant bits like SM5 shifts", tuint32)
+
 # integer-aware GLSL-style comparisons that compare floats and ints
 
 binop_reduce_all_sizes("ball_fequal",  1, tfloat, "{src0} == {src1}",
-- 
GitLab


From d21a6636bb6fca8ed3972cf1ca79d647fd4d6382 Mon Sep 17 00:00:00 2001
From: Georg Lehmann <dadschoorse@gmail.com>
Date: Sat, 27 May 2023 12:04:37 +0200
Subject: [PATCH 2/6] nir/lower_bit_size: mask bit test src1 like shifts

---
 src/compiler/nir/nir_lower_bit_size.c | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/src/compiler/nir/nir_lower_bit_size.c b/src/compiler/nir/nir_lower_bit_size.c
index 9d81bb350918f..ca2f48926a36b 100644
--- a/src/compiler/nir/nir_lower_bit_size.c
+++ b/src/compiler/nir/nir_lower_bit_size.c
@@ -64,7 +64,8 @@ lower_alu_instr(nir_builder *bld, nir_alu_instr *alu, unsigned bit_size)
       if (nir_alu_type_get_type_size(type) == 0)
          src = convert_to_bit_size(bld, src, type, bit_size);
 
-      if (i == 1 && (op == nir_op_ishl || op == nir_op_ishr || op == nir_op_ushr)) {
+      if (i == 1 && (op == nir_op_ishl || op == nir_op_ishr || op == nir_op_ushr ||
+                     op == nir_op_bit_test0 || op == nir_op_bit_test1)) {
          assert(util_is_power_of_two_nonzero(dst_bit_size));
          src = nir_iand(bld, src, nir_imm_int(bld, dst_bit_size - 1));
       }
-- 
GitLab


From 460f1f1fe4c968bffe1de3100a7e6f76ad79b4d6 Mon Sep 17 00:00:00 2001
From: Georg Lehmann <dadschoorse@gmail.com>
Date: Sat, 27 May 2023 14:37:57 +0200
Subject: [PATCH 3/6] aco: implement nir_op_bit_test0/1

---
 .../compiler/aco_instruction_selection.cpp    | 143 +++++++++++++++++-
 1 file changed, 142 insertions(+), 1 deletion(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index fb6abaa60afb4..cc8f9c5a23181 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -1065,7 +1065,6 @@ emit_sopc_instruction(isel_context* ctx, nir_alu_instr* instr, aco_opcode op, Te
    assert(dst.regClass() == bld.lm);
    assert(src0.type() == RegType::sgpr);
    assert(src1.type() == RegType::sgpr);
-   assert(src0.regClass() == src1.regClass());
 
    /* Emit the SALU comparison instruction */
    Temp cmp = bld.sopc(op, bld.scc(bld.def(s1)), src0, src1);
@@ -3909,6 +3908,148 @@ visit_alu_instr(isel_context* ctx, nir_alu_instr* instr)
                       aco_opcode::v_cmp_ge_u64, aco_opcode::s_cmp_ge_u32);
       break;
    }
+   case nir_op_bit_test0:
+   case nir_op_bit_test1: {
+      assert(instr->src[0].src.ssa->bit_size != 1);
+      bool test0 = instr->op == nir_op_bit_test0;
+      Temp src0 = get_alu_src(ctx, instr->src[0]);
+      Temp src1 = get_alu_src(ctx, instr->src[1]);
+      bool use_valu = src0.type() == RegType::vgpr || src1.type() == RegType::vgpr;
+      if (!use_valu) {
+         aco_opcode op = instr->src[0].src.ssa->bit_size == 64 ? aco_opcode::s_bitcmp1_b64
+                                                               : aco_opcode::s_bitcmp1_b32;
+         if (test0)
+            op = instr->src[0].src.ssa->bit_size == 64 ? aco_opcode::s_bitcmp0_b64
+                                                       : aco_opcode::s_bitcmp0_b32;
+         emit_sopc_instruction(ctx, instr, op, dst);
+         break;
+      }
+
+      /* We do not have a VALU version of s_bitcmp.
+       * But if the second source is constant, we can use
+       * v_cmp_class_f32's LUT to check the bit.
+       * The LUT only has 10 entries, so extract a higher byte if we have to.
+       * For sign bits comparision with 0 is better because v_cmp_class
+       * can't be inverted.
+       */
+      if (nir_src_is_const(instr->src[1].src)) {
+         uint32_t bit = nir_alu_src_as_uint(instr->src[1]);
+         bit &= instr->src[0].src.ssa->bit_size - 1;
+
+         if (src0.regClass() == v2) {
+            src0 = bld.pseudo(aco_opcode::p_extract_vector, bld.def(v1), src0,
+                              Operand::c32((bit & 32) != 0));
+            bit &= 31;
+         }
+
+         /* Avoid snan for bit 24. */
+         if (bit == 24) {
+            src0 = bld.pseudo(aco_opcode::p_extract, bld.def(v1), src0, Operand::c32(1),
+                              Operand::c32(16), Operand::c32(0));
+            bit &= 0xf;
+         }
+
+         if (bit == 31) {
+            bld.vopc(test0 ? aco_opcode::v_cmp_le_i32 : aco_opcode::v_cmp_gt_i32, Definition(dst),
+                     Operand::c32(0), src0);
+            break;
+         }
+
+         if (bit == 15 && ctx->program->gfx_level >= GFX8) {
+            bld.vopc(test0 ? aco_opcode::v_cmp_le_i16 : aco_opcode::v_cmp_gt_i16, Definition(dst),
+                     Operand::c32(0), src0);
+            break;
+         }
+
+         const bool can_sdwa = ctx->program->gfx_level >= GFX8 && ctx->program->gfx_level < GFX11;
+         if ((bit & 0x7) == 7 && ((test0 && can_sdwa) || bit != 7)) {
+            src0 = bld.pseudo(aco_opcode::p_extract, bld.def(v1), src0, Operand::c32(bit / 8),
+                              Operand::c32(8), Operand::c32(1));
+            bld.vopc(test0 ? aco_opcode::v_cmp_le_i32 : aco_opcode::v_cmp_gt_i32, Definition(dst),
+                     Operand::c32(0), src0);
+            break;
+         }
+
+         /* avoid +inf if we can use sdwa+qnan */
+         if (bit > (can_sdwa ? 0x8 : 0x9)) {
+            src0 = bld.pseudo(aco_opcode::p_extract, bld.def(v1), src0, Operand::c32(bit / 8),
+                              Operand::c32(8), Operand::c32(0));
+            bit &= 0x7;
+         }
+
+         static const std::pair<uint32_t, bool> float_lut[10] = {
+            {0x7f800001, false}, /* snan */
+            {-1, false},         /* qnan */
+            {0xff800000, false}, /* -inf */
+            {0xbf800000, false}, /* -normal (-1.0) */
+            {1, true},           /* -denormal */
+            {0, true},           /* -0.0 */
+            {0, false},          /* +0.0 */
+            {1, false},          /* +denormal */
+            {0x3f800000, false}, /* +normal (+1.0) */
+            {0x7f800000, false}, /* +inf */
+         };
+
+         Temp tmp = test0 ? bld.tmp(bld.lm) : dst;
+         if (ctx->program->gfx_level >= GFX8 && bit == 0) {
+            /* this can use s_movk. */
+            bld.vopc(aco_opcode::v_cmp_class_f16, Definition(tmp),
+                     bld.copy(bld.def(s1), Operand::c32(0x7c01)), src0);
+         } else {
+            VALU_instruction& res =
+               bld.vopc(aco_opcode::v_cmp_class_f32, Definition(tmp),
+                        bld.copy(bld.def(s1), Operand::c32(float_lut[bit].first)), src0)
+                  ->valu();
+            if (float_lut[bit].second) {
+               res.format = asVOP3(res.format);
+               res.neg[0] = true;
+            }
+         }
+
+         if (test0)
+            bld.sop1(Builder::s_not, Definition(dst), bld.def(s1, scc), tmp);
+
+         break;
+      }
+
+      if (instr->src[0].src.ssa->bit_size == 16) {
+         if (ctx->program->gfx_level < GFX10)
+            src1 = bld.vop2_e64(aco_opcode::v_lshlrev_b16, bld.def(v2b), src1, Operand::c32(1));
+         else
+            src1 = bld.vop3(aco_opcode::v_lshlrev_b16_e64, bld.def(v2b), src1, Operand::c32(1));
+
+         Temp res = bld.vop2(aco_opcode::v_and_b32, bld.def(v2b), src0, src1);
+
+         aco_opcode op = test0 ? aco_opcode::v_cmp_eq_i16 : aco_opcode::v_cmp_lg_i16;
+         bld.vopc(op, Definition(dst), Operand::c32(0), res);
+      } else if (instr->src[0].src.ssa->bit_size == 32) {
+         Temp res = bld.vop3(aco_opcode::v_bfe_u32, bld.def(v1), src0, src1, Operand::c32(1));
+
+         aco_opcode op = test0 ? aco_opcode::v_cmp_eq_i32 : aco_opcode::v_cmp_lg_i32;
+         bld.vopc(op, Definition(dst), Operand::c32(0), res);
+      } else if (instr->src[0].src.ssa->bit_size == 64) {
+         if (ctx->program->gfx_level < GFX8)
+            src1 = bld.vop3(aco_opcode::v_lshl_b64, bld.def(v2), Operand::c32(1), src1);
+         else
+            src1 = bld.vop3(aco_opcode::v_lshlrev_b64, bld.def(v2), src1, Operand::c32(1));
+
+         Temp src0_lo = bld.tmp(v1), src0_hi = bld.tmp(v1);
+         Temp src1_lo = bld.tmp(v1), src1_hi = bld.tmp(v1);
+         bld.pseudo(aco_opcode::p_split_vector, Definition(src0_lo), Definition(src0_hi), src0);
+         bld.pseudo(aco_opcode::p_split_vector, Definition(src1_lo), Definition(src1_hi), src1);
+
+         Temp lo = bld.vop2(aco_opcode::v_and_b32, bld.def(v1), src0_lo, src1_lo);
+         Temp hi = bld.vop2(aco_opcode::v_and_b32, bld.def(v1), src0_hi, src1_hi);
+
+         Temp res = bld.pseudo(aco_opcode::p_create_vector, bld.def(v2), lo, hi);
+
+         aco_opcode op = test0 ? aco_opcode::v_cmp_eq_i64 : aco_opcode::v_cmp_lg_i64;
+         bld.vopc(op, Definition(dst), Operand::c32(0), res);
+      } else {
+         isel_err(&instr->instr, "Unimplemented NIR instr bit size");
+      }
+      break;
+   }
    case nir_op_fddx:
    case nir_op_fddy:
    case nir_op_fddx_fine:
-- 
GitLab


From 01d0a023d522a1e6abea1bf340bc0f85519ef3af Mon Sep 17 00:00:00 2001
From: Georg Lehmann <dadschoorse@gmail.com>
Date: Sat, 27 May 2023 15:28:10 +0200
Subject: [PATCH 4/6] nir/opt_algebraic: combine bit_test

---
 src/compiler/nir/nir.h                |  3 +++
 src/compiler/nir/nir_opt_algebraic.py | 27 +++++++++++++++++++++++++++
 2 files changed, 30 insertions(+)

diff --git a/src/compiler/nir/nir.h b/src/compiler/nir/nir.h
index 16e5e6b014434..92ded2dd46e1b 100644
--- a/src/compiler/nir/nir.h
+++ b/src/compiler/nir/nir.h
@@ -3838,6 +3838,9 @@ typedef struct nir_shader_compiler_options {
    /** Backend supports pack_half_2x16_rtz_split. */
    bool has_pack_half_2x16_rtz;
 
+   /** Backend supports bit_test0/bit_test1. */
+   bool has_bit_test;
+
    /**
     * Is this the Intel vec4 backend?
     *
diff --git a/src/compiler/nir/nir_opt_algebraic.py b/src/compiler/nir/nir_opt_algebraic.py
index 1c553c4038bde..a1a584bc825ca 100644
--- a/src/compiler/nir/nir_opt_algebraic.py
+++ b/src/compiler/nir/nir_opt_algebraic.py
@@ -1730,6 +1730,7 @@ optimizations.extend([
    (('uabs_usub', a, b), ('bcsel', ('ult', a, b), ('ineg', ('isub', a, b)), ('isub', a, b))),
    # This is correct.  We don't need isub_sat because the result type is unsigned, so it cannot overflow.
    (('uabs_isub', a, b), ('bcsel', ('ilt', a, b), ('ineg', ('isub', a, b)), ('isub', a, b))),
+   (('bit_test0', a, b), ('inot', ('bit_test1', a, b))),
 
    # Propagate negation up multiplication chains
    (('fmul(is_used_by_non_fsat)', ('fneg', a), b), ('fneg', ('fmul', a, b))),
@@ -3017,6 +3018,32 @@ late_optimizations.extend([
    (('extract_i8', ('extract_u8', a, b), 0), ('extract_i8', a, b)),
    (('extract_u8', ('extract_i8', a, b), 0), ('extract_u8', a, b)),
    (('extract_u8', ('extract_u8', a, b), 0), ('extract_u8', a, b)),
+
+   # open coded bit test
+   (('ine', ('iand', a, '#b(is_pos_power_of_two)'), 0), ('bit_test1', a, ('find_lsb', b)), 'options->has_bit_test'),
+   (('ieq', ('iand', a, '#b(is_pos_power_of_two)'), 0), ('bit_test0', a, ('find_lsb', b)), 'options->has_bit_test'),
+   (('ine', ('iand', a, '#b(is_pos_power_of_two)'), b), ('bit_test0', a, ('find_lsb', b)), 'options->has_bit_test'),
+   (('ieq', ('iand', a, '#b(is_pos_power_of_two)'), b), ('bit_test1', a, ('find_lsb', b)), 'options->has_bit_test'),
+   (('ine', ('iand', a, ('ishl', 1, b)), 0), ('bit_test1', a, b), 'options->has_bit_test'),
+   (('ieq', ('iand', a, ('ishl', 1, b)), 0), ('bit_test0', a, b), 'options->has_bit_test'),
+   (('ine', ('iand', a, ('ishl', 1, b)), ('ishl', 1, b)), ('bit_test0', a, b), 'options->has_bit_test'),
+   (('ieq', ('iand', a, ('ishl', 1, b)), ('ishl', 1, b)), ('bit_test1', a, b), 'options->has_bit_test'),
+   (('bit_test0', ('ushr', a, b), 0), ('bit_test0', a, b)),
+   (('bit_test0', ('ishr', a, b), 0), ('bit_test0', a, b)),
+   (('bit_test1', ('ushr', a, b), 0), ('bit_test1', a, b)),
+   (('bit_test1', ('ishr', a, b), 0), ('bit_test1', a, b)),
+   (('ine', ('ubfe', a, b, 1), 0), ('bit_test1', a, b), 'options->has_bit_test'),
+   (('ieq', ('ubfe', a, b, 1), 0), ('bit_test0', a, b), 'options->has_bit_test'),
+   (('ine', ('ubfe', a, b, 1), 1), ('bit_test0', a, b), 'options->has_bit_test'),
+   (('ieq', ('ubfe', a, b, 1), 1), ('bit_test1', a, b), 'options->has_bit_test'),
+   (('ine', ('ibfe', a, b, 1), 0), ('bit_test1', a, b), 'options->has_bit_test'),
+   (('ieq', ('ibfe', a, b, 1), 0), ('bit_test0', a, b), 'options->has_bit_test'),
+   (('ine', ('ibfe', a, b, 1), -1), ('bit_test0', a, b), 'options->has_bit_test'),
+   (('ieq', ('ibfe', a, b, 1), -1), ('bit_test1', a, b), 'options->has_bit_test'),
+   (('inot', ('bit_test1', a, b)), ('bit_test0', a, b)),
+   (('inot', ('bit_test0', a, b)), ('bit_test1', a, b)),
+   (('bit_test1', ('inot', a), b), ('bit_test0', a, b)),
+   (('bit_test0', ('inot', a), b), ('bit_test1', a, b)),
 ])
 
 # A few more extract cases we'd rather leave late
-- 
GitLab


From 0c76e7c5b1753c98d59b7264eb7e75ff0e5f043d Mon Sep 17 00:00:00 2001
From: Georg Lehmann <dadschoorse@gmail.com>
Date: Sat, 27 May 2023 15:31:07 +0200
Subject: [PATCH 5/6] radv: set has_bit_test for aco

Foz-DB Navi21:
Totals from 15285 (11.52% of 132657) affected shaders:
VGPRs: 1019136 -> 1019000 (-0.01%); split: -0.02%, +0.01%
SpillSGPRs: 10889 -> 10909 (+0.18%)
SpillVGPRs: 901 -> 914 (+1.44%); split: -0.89%, +2.33%
CodeSize: 103578640 -> 103523220 (-0.05%); split: -0.08%, +0.03%
MaxWaves: 259782 -> 259820 (+0.01%)
Instrs: 19247383 -> 19223764 (-0.12%); split: -0.15%, +0.02%
Latency: 323877613 -> 323684655 (-0.06%); split: -0.10%, +0.04%
InvThroughput: 62505295 -> 62386541 (-0.19%); split: -0.21%, +0.02%
VClause: 366162 -> 366136 (-0.01%); split: -0.03%, +0.02%
SClause: 786505 -> 785527 (-0.12%); split: -0.22%, +0.10%
Copies: 1348920 -> 1349209 (+0.02%); split: -0.26%, +0.29%
Branches: 456331 -> 456324 (-0.00%); split: -0.01%, +0.00%
PreSGPRs: 849542 -> 849402 (-0.02%); split: -0.02%, +0.01%
PreVGPRs: 925300 -> 924678 (-0.07%)
---
 src/amd/vulkan/radv_pipeline.c | 2 ++
 src/amd/vulkan/radv_shader.c   | 1 +
 2 files changed, 3 insertions(+)

diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index e9c58ef76dc32..9aa779c376c41 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -381,6 +381,8 @@ lower_bit_size_callback(const nir_instr *instr, void *_)
       case nir_op_ine:
       case nir_op_ult:
       case nir_op_uge:
+      case nir_op_bit_test0:
+      case nir_op_bit_test1:
          return (bit_size == 8 || !(chip >= GFX8 && nir_dest_is_divergent(alu->dest.dest))) ? 32
                                                                                             : 0;
       default:
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 8dacbae139b1e..f5f931853f187 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -107,6 +107,7 @@ get_nir_options_for_stage(struct radv_physical_device *device, gl_shader_stage s
          device->rad_info.has_accelerated_dot_product && device->rad_info.gfx_level < GFX11,
       .has_find_msb_rev = true,
       .has_pack_half_2x16_rtz = true,
+      .has_bit_test = !device->use_llvm,
       .use_scoped_barrier = true,
 #ifdef LLVM_AVAILABLE
       .has_fmulz = !device->use_llvm || LLVM_VERSION_MAJOR >= 12,
-- 
GitLab

