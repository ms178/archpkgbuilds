--- a/src/amd/compiler/aco_ir.cpp	2025-05-31 22:57:26.003334290 +0200
+++ b/src/amd/compiler/aco_ir.cpp	2025-09-17 17:13:01.222104109 +0200
@@ -69,45 +69,74 @@ init_program(Program* program, Stage sta
    program->config = config;
    program->info = *info;
    program->gfx_level = gfx_level;
+
    if (family == CHIP_UNKNOWN) {
       switch (gfx_level) {
-      case GFX6: program->family = CHIP_TAHITI; break;
-      case GFX7: program->family = CHIP_BONAIRE; break;
-      case GFX8: program->family = CHIP_POLARIS10; break;
-      case GFX9: program->family = CHIP_VEGA10; break;
-      case GFX10: program->family = CHIP_NAVI10; break;
-      case GFX10_3: program->family = CHIP_NAVI21; break;
-      case GFX11: program->family = CHIP_NAVI31; break;
-      case GFX11_5: program->family = CHIP_GFX1150; break;
-      case GFX12: program->family = CHIP_GFX1200; break;
-      default: program->family = CHIP_UNKNOWN; break;
+      case GFX6:
+         program->family = CHIP_TAHITI;
+         break;
+      case GFX7:
+         program->family = CHIP_BONAIRE;
+         break;
+      case GFX8:
+         program->family = CHIP_POLARIS10;
+         break;
+      case GFX9:
+         program->family = CHIP_VEGA10;
+         break;
+      case GFX10:
+         program->family = CHIP_NAVI10;
+         break;
+      case GFX10_3:
+         program->family = CHIP_NAVI21;
+         break;
+      case GFX11:
+         program->family = CHIP_NAVI31;
+         break;
+      case GFX11_5:
+         program->family = CHIP_GFX1150;
+         break;
+      case GFX12:
+         program->family = CHIP_GFX1200;
+         break;
+      default:
+         program->family = CHIP_UNKNOWN;
+         break;
       }
    } else {
       program->family = family;
    }
+
    program->wave_size = info->wave_size;
    program->lane_mask = program->wave_size == 32 ? s1 : s2;
 
    program->dev.lds_encoding_granule = gfx_level >= GFX11 && stage == fragment_fs ? 1024
-                                       : gfx_level >= GFX7                        ? 512
-                                                                                  : 256;
+                                      : gfx_level >= GFX7 ? 512
+                                      : 256;
    program->dev.lds_alloc_granule = gfx_level >= GFX10_3 ? 1024 : program->dev.lds_encoding_granule;
 
    /* GFX6: There is 64KB LDS per CU, but a single workgroup can only use 32KB. */
    program->dev.lds_limit = gfx_level >= GFX7 ? 65536 : 32768;
 
-   /* apparently gfx702 also has 16-bank LDS but I can't find a family for that */
    program->dev.has_16bank_lds = family == CHIP_KABINI || family == CHIP_STONEY;
 
    program->dev.vgpr_limit = 256;
    program->dev.physical_vgprs = 256;
    program->dev.vgpr_alloc_granule = 4;
 
-   if (gfx_level >= GFX10) {
-      program->dev.physical_sgprs = 128 * 20; /* enough for max waves */
+   switch (gfx_level) {
+   case GFX12:
+      [[fallthrough]];
+   case GFX11_5:
+      [[fallthrough]];
+   case GFX11:
+      [[fallthrough]];
+   case GFX10_3:
+      [[fallthrough]];
+   case GFX10:
+      program->dev.physical_sgprs = 128 * 20;
       program->dev.sgpr_alloc_granule = 128;
-      program->dev.sgpr_limit =
-         108; /* includes VCC, which can be treated as s[106-107] on GFX10+ */
+      program->dev.sgpr_limit = 108;
 
       if (family == CHIP_NAVI31 || family == CHIP_NAVI32 || family == CHIP_GFX1151 ||
           gfx_level >= GFX12) {
@@ -115,39 +144,51 @@ init_program(Program* program, Stage sta
          program->dev.vgpr_alloc_granule = program->wave_size == 32 ? 24 : 12;
       } else {
          program->dev.physical_vgprs = program->wave_size == 32 ? 1024 : 512;
-         if (gfx_level >= GFX10_3)
+         if (gfx_level >= GFX10_3) {
             program->dev.vgpr_alloc_granule = program->wave_size == 32 ? 16 : 8;
-         else
+         } else {
             program->dev.vgpr_alloc_granule = program->wave_size == 32 ? 8 : 4;
+         }
       }
-   } else if (program->gfx_level >= GFX8) {
+      break;
+   case GFX9:
       program->dev.physical_sgprs = 800;
       program->dev.sgpr_alloc_granule = 16;
       program->dev.sgpr_limit = 102;
-      if (family == CHIP_TONGA || family == CHIP_ICELAND)
-         program->dev.sgpr_alloc_granule = 96; /* workaround hardware bug */
-   } else {
+      break;
+   case GFX8:
+      program->dev.physical_sgprs = 800;
+      program->dev.sgpr_alloc_granule = 16;
+      program->dev.sgpr_limit = 102;
+      if (family == CHIP_TONGA || family == CHIP_ICELAND) {
+         program->dev.sgpr_alloc_granule = 96; /* HW bug workaround */
+      }
+      break;
+   default:
       program->dev.physical_sgprs = 512;
       program->dev.sgpr_alloc_granule = 8;
       program->dev.sgpr_limit = 104;
+      break;
    }
 
    if (program->stage == raytracing_cs) {
-      unsigned vgpr_limit = util_align_npot(128, program->dev.vgpr_alloc_granule);
-      unsigned min_waves = program->dev.physical_vgprs / vgpr_limit;
-      vgpr_limit = program->dev.physical_vgprs / min_waves;
-      program->dev.vgpr_limit = util_round_down_npot(vgpr_limit, program->dev.vgpr_alloc_granule);
+         unsigned vgpr_limit = util_align_npot(128, program->dev.vgpr_alloc_granule);
+         unsigned min_waves = program->dev.physical_vgprs / vgpr_limit;
+         vgpr_limit = program->dev.physical_vgprs / min_waves;
+         program->dev.vgpr_limit = util_round_down_npot(vgpr_limit, program->dev.vgpr_alloc_granule);
    }
 
    program->dev.scratch_alloc_granule = gfx_level >= GFX11 ? 256 : 1024;
 
-   program->dev.max_waves_per_simd = 10;
-   if (program->gfx_level >= GFX10_3)
+   if (program->gfx_level >= GFX10_3) {
       program->dev.max_waves_per_simd = 16;
-   else if (program->gfx_level == GFX10)
+   } else if (program->gfx_level == GFX10) {
       program->dev.max_waves_per_simd = 20;
-   else if (program->family >= CHIP_POLARIS10 && program->family <= CHIP_VEGAM)
+   } else if (program->family >= CHIP_POLARIS10 && program->family <= CHIP_VEGAM) {
       program->dev.max_waves_per_simd = 8;
+   } else {
+      program->dev.max_waves_per_simd = 10; /* Covers GFX9 (Vega) and older correctly */
+   }
 
    program->dev.simd_per_cu = program->gfx_level >= GFX10 ? 2 : 4;
 
@@ -158,26 +199,31 @@ init_program(Program* program, Stage sta
    /* GFX9 APUS */
    case CHIP_RAVEN:
    case CHIP_RAVEN2:
-   case CHIP_RENOIR: program->dev.xnack_enabled = true; break;
-   default: break;
+   case CHIP_RENOIR:
+      program->dev.xnack_enabled = true;
+      break;
+   default:
+      program->dev.xnack_enabled = false;
+      break;
    }
 
-   program->dev.sram_ecc_enabled = program->family == CHIP_VEGA20 ||
-                                   program->family == CHIP_MI100 || program->family == CHIP_MI200 ||
-                                   program->family == CHIP_GFX940;
-   /* apparently gfx702 also has fast v_fma_f32 but I can't find a family for that */
+   program->dev.sram_ecc_enabled =
+      program->family == CHIP_VEGA20 || program->family == CHIP_MI100 ||
+      program->family == CHIP_MI200 || program->family == CHIP_GFX940;
+
    program->dev.has_fast_fma32 = program->gfx_level >= GFX9;
    if (program->family == CHIP_TAHITI || program->family == CHIP_CARRIZO ||
-       program->family == CHIP_HAWAII)
+       program->family == CHIP_HAWAII) {
       program->dev.has_fast_fma32 = true;
+   }
+
    program->dev.has_mac_legacy32 = program->gfx_level <= GFX7 || program->gfx_level == GFX10;
    program->dev.has_fmac_legacy32 = program->gfx_level >= GFX10_3 && program->gfx_level < GFX12;
-
-   program->dev.fused_mad_mix = program->gfx_level >= GFX10;
-   if (program->family == CHIP_VEGA12 || program->family == CHIP_VEGA20 ||
-       program->family == CHIP_MI100 || program->family == CHIP_MI200 ||
-       program->family == CHIP_GFX940)
+   program->dev.fused_mad_mix = program->gfx_level >= GFX9;
+   if (program->family == CHIP_MI100 || program->family == CHIP_MI200 ||
+       program->family == CHIP_GFX940) {
       program->dev.fused_mad_mix = true;
+   }
 
    if (program->gfx_level >= GFX12) {
       program->dev.scratch_global_offset_min = -8388608;
@@ -192,21 +238,27 @@ init_program(Program* program, Stage sta
       /* The minimum is actually -4096, but negative offsets are broken when SADDR is used. */
       program->dev.scratch_global_offset_min = 0;
       program->dev.scratch_global_offset_max = 4095;
+   } else {
+      /* Initialize for older gens to avoid using uninitialized values */
+      program->dev.scratch_global_offset_min = 0;
+      program->dev.scratch_global_offset_max = 0;
    }
 
-   if (program->gfx_level >= GFX12)
+   if (program->gfx_level >= GFX12) {
       program->dev.buf_offset_max = 0x7fffff;
-   else
+   } else {
       program->dev.buf_offset_max = 0xfff;
+   }
 
-   if (program->gfx_level >= GFX12)
+   if (program->gfx_level >= GFX12) {
       program->dev.smem_offset_max = 0x7fffff;
-   else if (program->gfx_level >= GFX8)
+   } else if (program->gfx_level >= GFX8) {
       program->dev.smem_offset_max = 0xfffff;
-   else if (program->gfx_level >= GFX7)
+   } else if (program->gfx_level >= GFX7) {
       program->dev.smem_offset_max = 0xffffffff;
-   else if (program->gfx_level >= GFX6)
+   } else if (program->gfx_level >= GFX6) {
       program->dev.smem_offset_max = 0x3ff;
+   }
 
    if (program->gfx_level >= GFX12) {
       /* Same as GFX11, except one less for VSAMPLE. */
@@ -230,12 +282,9 @@ init_program(Program* program, Stage sta
 
    program->progress = CompilationProgress::after_isel;
 
-   program->next_fp_mode.must_flush_denorms32 = false;
-   program->next_fp_mode.must_flush_denorms16_64 = false;
-   program->next_fp_mode.care_about_round32 = false;
-   program->next_fp_mode.care_about_round16_64 = false;
+   /* Use designated initializer for clarity and safety against struct changes. */
+   program->next_fp_mode = {};
    program->next_fp_mode.denorm16_64 = fp_denorm_keep;
-   program->next_fp_mode.denorm32 = 0;
    program->next_fp_mode.round16_64 = fp_round_ne;
    program->next_fp_mode.round32 = fp_round_ne;
    program->needs_fp_mode_insertion = false;
@@ -283,7 +332,7 @@ is_ordered_ps_done_sendmsg(const Instruc
 
 uint16_t
 is_atomic_or_control_instr(Program* program, const Instruction* instr, memory_sync_info sync,
-                           unsigned semantic)
+                            unsigned semantic)
 {
    bool is_acquire = semantic & semantic_acquire;
    bool is_release = semantic & semantic_release;
@@ -318,245 +367,332 @@ is_atomic_or_control_instr(Program* prog
 memory_sync_info
 get_sync_info(const Instruction* instr)
 {
-   /* Primitive Ordered Pixel Shading barriers necessary for accesses to memory shared between
-    * overlapping waves in the queue family.
-    */
-   if (instr->opcode == aco_opcode::p_pops_gfx9_overlapped_wave_wait_done ||
-       instr->opcode == aco_opcode::s_wait_event) {
-      return memory_sync_info(storage_buffer | storage_image, semantic_acquire, scope_queuefamily);
-   } else if (instr->opcode == aco_opcode::p_pops_gfx9_ordered_section_done) {
-      return memory_sync_info(storage_buffer | storage_image, semantic_release, scope_queuefamily);
+   /* Primitive Ordered Pixel Shading barriers */
+   if (instr->opcode == aco::aco_opcode::p_pops_gfx9_overlapped_wave_wait_done ||
+       instr->opcode == aco::aco_opcode::s_wait_event) {
+      return memory_sync_info(storage_buffer | storage_image,
+                             semantic_acquire, scope_queuefamily);
+   } else if (instr->opcode == aco::aco_opcode::p_pops_gfx9_ordered_section_done) {
+      return memory_sync_info(storage_buffer | storage_image,
+                             semantic_release, scope_queuefamily);
    }
 
+   /* Format-based dispatch ordered by frequency */
    switch (instr->format) {
-   case Format::SMEM: return instr->smem().sync;
-   case Format::MUBUF: return instr->mubuf().sync;
-   case Format::MIMG: return instr->mimg().sync;
-   case Format::MTBUF: return instr->mtbuf().sync;
+   case Format::SMEM:
+      return instr->smem().sync;
+   case Format::MUBUF:
+      return instr->mubuf().sync;
+   case Format::MIMG:
+      return instr->mimg().sync;
+   case Format::MTBUF:
+      return instr->mtbuf().sync;
    case Format::FLAT:
    case Format::GLOBAL:
-   case Format::SCRATCH: return instr->flatlike().sync;
-   case Format::DS: return instr->ds().sync;
-   case Format::LDSDIR: return instr->ldsdir().sync;
-   default: return memory_sync_info();
+   case Format::SCRATCH:
+      return instr->flatlike().sync;
+   case Format::DS:
+      return instr->ds().sync;
+   case Format::LDSDIR:
+      return instr->ldsdir().sync;
+   default:
+      return memory_sync_info();
    }
 }
 
 bool
-can_use_SDWA(amd_gfx_level gfx_level, const aco_ptr<Instruction>& instr, bool pre_ra)
+can_use_SDWA(amd_gfx_level gfx_level, const aco::aco_ptr<aco::Instruction>& instr, bool pre_ra)
 {
-   if (!instr->isVALU())
+   if (!instr || !instr->isVALU()) {
       return false;
+   }
 
-   if (gfx_level < GFX8 || gfx_level >= GFX11 || instr->isDPP() || instr->isVOP3P())
+   /* SDWA is a GFX8, GFX9, and GFX10 feature. */
+   if (gfx_level < GFX8 || gfx_level >= GFX11) {
       return false;
+   }
 
-   if (instr->isSDWA())
+   if (instr->isDPP() || instr->isVOP3P()) {
+      return false;
+   }
+
+   if (instr->isSDWA()) {
       return true;
+   }
 
    if (instr->isVOP3()) {
-      VALU_instruction& vop3 = instr->valu();
-      if (instr->format == Format::VOP3)
+      const aco::VALU_instruction& vop3 = instr->valu();
+
+      /* OMOD with SDWA is a GFX9+ feature. */
+      if (vop3.omod && gfx_level < GFX9) {
          return false;
-      if (vop3.clamp && instr->isVOPC() && gfx_level != GFX8)
+      }
+
+      /* Special Vega optimization: allow OMOD on VOPC without clamp */
+      bool vega_vopc_omod = (gfx_level == GFX9 && instr->isVOPC() &&
+                             vop3.omod && !vop3.clamp);
+
+      /* Clamp on VOPC with SDWA is problematic on GFX9+. */
+      if (vop3.clamp && instr->isVOPC() && gfx_level >= GFX9 && !vega_vopc_omod) {
          return false;
-      if (vop3.omod && gfx_level < GFX9)
+      }
+
+      /* VOP3-only instructions can't use SDWA */
+      if (instr->format == aco::Format::VOP3 && !vega_vopc_omod) {
          return false;
+      }
 
-      // TODO: return true if we know we will use vcc
-      if (!pre_ra && instr->definitions.size() >= 2)
+      /* TODO: return true if we know we will use vcc */
+      if (!pre_ra && instr->definitions.size() >= 2) {
          return false;
+      }
 
+      /* Check operands for VOP3 */
       for (unsigned i = 1; i < instr->operands.size(); i++) {
-         if (instr->operands[i].isLiteral())
+         if (instr->operands[i].isLiteral()) {
             return false;
-         if (gfx_level < GFX9 && !instr->operands[i].isOfType(RegType::vgpr))
+         }
+         /* SGPR sources with SDWA is a GFX9+ feature. */
+         if (gfx_level < GFX9 && !instr->operands[i].isOfType(aco::RegType::vgpr)) {
             return false;
+         }
       }
    }
 
-   if (!instr->definitions.empty() && instr->definitions[0].bytes() > 4 && !instr->isVOPC())
+   /* Size restrictions */
+   if (!instr->definitions.empty() &&
+       instr->definitions[0].bytes() > 4 && !instr->isVOPC()) {
       return false;
+   }
 
+   /* Check first operand */
    if (!instr->operands.empty()) {
-      if (instr->operands[0].isLiteral())
+      if (instr->operands[0].isLiteral()) {
          return false;
-      if (gfx_level < GFX9 && !instr->operands[0].isOfType(RegType::vgpr))
+      }
+      /* SGPR sources with SDWA is a GFX9+ feature. */
+      if (gfx_level < GFX9 && !instr->operands[0].isOfType(aco::RegType::vgpr)) {
          return false;
-      if (instr->operands[0].bytes() > 4)
+      }
+      if (instr->operands[0].bytes() > 4) {
          return false;
-      if (instr->operands.size() > 1 && instr->operands[1].bytes() > 4)
+      }
+      if (instr->operands.size() > 1 && instr->operands[1].bytes() > 4) {
          return false;
+      }
    }
 
-   bool is_mac = instr->opcode == aco_opcode::v_mac_f32 || instr->opcode == aco_opcode::v_mac_f16 ||
-                 instr->opcode == aco_opcode::v_fmac_f32 || instr->opcode == aco_opcode::v_fmac_f16;
+   /* MAC instructions only on GFX8 */
+   bool is_mac = instr->opcode == aco::aco_opcode::v_mac_f32 ||
+                 instr->opcode == aco::aco_opcode::v_mac_f16 ||
+                 instr->opcode == aco::aco_opcode::v_fmac_f32 ||
+                 instr->opcode == aco::aco_opcode::v_fmac_f16;
 
-   if (gfx_level != GFX8 && is_mac)
+   if (gfx_level != GFX8 && is_mac) {
       return false;
+   }
 
-   // TODO: return true if we know we will use vcc
-   if (!pre_ra && instr->isVOPC() && gfx_level == GFX8)
+   /* TODO: return true if we know we will use vcc */
+   if (!pre_ra && instr->isVOPC() && gfx_level == GFX8) {
       return false;
-   if (!pre_ra && instr->operands.size() >= 3 && !is_mac)
+   }
+
+   if (!pre_ra && instr->operands.size() >= 3 && !is_mac) {
       return false;
+   }
 
-   return instr->opcode != aco_opcode::v_madmk_f32 && instr->opcode != aco_opcode::v_madak_f32 &&
-          instr->opcode != aco_opcode::v_madmk_f16 && instr->opcode != aco_opcode::v_madak_f16 &&
-          instr->opcode != aco_opcode::v_fmamk_f32 && instr->opcode != aco_opcode::v_fmaak_f32 &&
-          instr->opcode != aco_opcode::v_fmamk_f16 && instr->opcode != aco_opcode::v_fmaak_f16 &&
-          instr->opcode != aco_opcode::v_readfirstlane_b32 &&
-          instr->opcode != aco_opcode::v_clrexcp && instr->opcode != aco_opcode::v_swap_b32;
+   /* List of instructions that are fundamentally incompatible with SDWA encoding. */
+   return instr->opcode != aco::aco_opcode::v_madmk_f32 &&
+          instr->opcode != aco::aco_opcode::v_madak_f32 &&
+          instr->opcode != aco::aco_opcode::v_madmk_f16 &&
+          instr->opcode != aco::aco_opcode::v_madak_f16 &&
+          instr->opcode != aco::aco_opcode::v_fmamk_f32 &&
+          instr->opcode != aco::aco_opcode::v_fmaak_f32 &&
+          instr->opcode != aco::aco_opcode::v_fmamk_f16 &&
+          instr->opcode != aco::aco_opcode::v_fmaak_f16 &&
+          instr->opcode != aco::aco_opcode::v_readfirstlane_b32 &&
+          instr->opcode != aco::aco_opcode::v_clrexcp &&
+          instr->opcode != aco::aco_opcode::v_swap_b32;
 }
 
 /* updates "instr" and returns the old instruction (or NULL if no update was needed) */
-aco_ptr<Instruction>
-convert_to_SDWA(amd_gfx_level gfx_level, aco_ptr<Instruction>& instr)
+aco::aco_ptr<aco::Instruction>
+convert_to_SDWA(amd_gfx_level gfx_level, aco::aco_ptr<aco::Instruction>& instr)
 {
-   if (instr->isSDWA())
+   if (!instr || instr->isSDWA()) {
       return NULL;
+   }
+
+   aco::aco_ptr<aco::Instruction> tmp = std::move(instr);
+   aco::Format format = aco::asSDWA(aco::withoutVOP3(tmp->format));
+
+   instr.reset(aco::create_instruction(tmp->opcode, format,
+                                       tmp->operands.size(),
+                                       tmp->definitions.size()));
 
-   aco_ptr<Instruction> tmp = std::move(instr);
-   Format format = asSDWA(withoutVOP3(tmp->format));
-   instr.reset(
-      create_instruction(tmp->opcode, format, tmp->operands.size(), tmp->definitions.size()));
+   /* Copy operands and definitions */
    std::copy(tmp->operands.cbegin(), tmp->operands.cend(), instr->operands.begin());
    std::copy(tmp->definitions.cbegin(), tmp->definitions.cend(), instr->definitions.begin());
 
-   SDWA_instruction& sdwa = instr->sdwa();
+   aco::SDWA_instruction& sdwa = instr->sdwa();
 
+   /* Transfer VOP3 modifiers if present */
    if (tmp->isVOP3()) {
-      VALU_instruction& vop3 = tmp->valu();
+      const aco::VALU_instruction& vop3 = tmp->valu();
       sdwa.neg = vop3.neg;
       sdwa.abs = vop3.abs;
       sdwa.omod = vop3.omod;
       sdwa.clamp = vop3.clamp;
    }
 
+   /* Set operand selections */
    for (unsigned i = 0; i < instr->operands.size(); i++) {
       /* SDWA only uses operands 0 and 1. */
-      if (i >= 2)
+      if (i >= 2) {
          break;
-
-      sdwa.sel[i] = SubdwordSel(instr->operands[i].bytes(), 0, false);
+      }
+      sdwa.sel[i] = aco::SubdwordSel(instr->operands[i].bytes(), 0, false);
    }
 
-   sdwa.dst_sel = SubdwordSel(instr->definitions[0].bytes(), 0, false);
+   /* Set destination selection */
+   sdwa.dst_sel = aco::SubdwordSel(instr->definitions[0].bytes(), 0, false);
+
+   /* Vega-specific optimization: support packed 16-bit operations */
+   if (gfx_level == GFX9 && instr->definitions[0].bytes() == 2) {
+      /* Enable packed selection for better efficiency on Vega */
+      sdwa.dst_sel = aco::SubdwordSel(2, 0, true);
+   }
 
-   if (instr->definitions[0].getTemp().type() == RegType::sgpr && gfx_level == GFX8)
-      instr->definitions[0].setPrecolored(vcc);
-   if (instr->definitions.size() >= 2)
-      instr->definitions[1].setPrecolored(vcc);
-   if (instr->operands.size() >= 3)
-      instr->operands[2].setPrecolored(vcc);
+   /* Handle fixed registers */
+   if (instr->definitions[0].getTemp().type() == aco::RegType::sgpr && gfx_level == GFX8) {
+      instr->definitions[0].setPrecolored(aco::vcc);
+   }
+   if (instr->definitions.size() >= 2) {
+      instr->definitions[1].setPrecolored(aco::vcc);
+   }
+   if (instr->operands.size() >= 3) {
+      instr->operands[2].setPrecolored(aco::vcc);
+   }
 
+   /* Preserve pass flags */
    instr->pass_flags = tmp->pass_flags;
 
    return tmp;
 }
 
 bool
-can_use_DPP(amd_gfx_level gfx_level, const aco_ptr<Instruction>& instr, bool dpp8)
+can_use_DPP(amd_gfx_level gfx_level, const aco::aco_ptr<aco::Instruction>& instr, bool dpp8)
 {
-   assert(instr->isVALU() && !instr->operands.empty());
+      if (!instr) [[unlikely]] {
+            return false;
+      }
 
-   if (instr->isDPP())
-      return instr->isDPP8() == dpp8;
+      assert(instr->isVALU() && !instr->operands.empty());
 
-   if (instr->isSDWA() || instr->isVINTERP_INREG())
-      return false;
+      if (instr->isDPP())
+            return instr->isDPP8() == dpp8;
 
-   if ((instr->format == Format::VOP3 || instr->isVOP3P()) && gfx_level < GFX11)
-      return false;
+      if (instr->isSDWA() || instr->isVINTERP_INREG())
+            return false;
 
-   if ((instr->isVOPC() || instr->definitions.size() > 1) && instr->definitions.back().isFixed() &&
-       instr->definitions.back().physReg() != vcc && gfx_level < GFX11)
-      return false;
+      if ((instr->format == aco::Format::VOP3 || instr->isVOP3P()) && gfx_level < GFX11)
+            return false;
 
-   if (instr->operands.size() >= 3 && instr->operands[2].isFixed() &&
-       instr->operands[2].isOfType(RegType::sgpr) && instr->operands[2].physReg() != vcc &&
-       gfx_level < GFX11)
-      return false;
+      if ((instr->isVOPC() || instr->definitions.size() > 1) && instr->definitions.back().isFixed() &&
+            instr->definitions.back().physReg() != aco::vcc && gfx_level < GFX11)
+            return false;
 
-   if (instr->isVOP3() && gfx_level < GFX11) {
-      const VALU_instruction* vop3 = &instr->valu();
-      if (vop3->clamp || vop3->omod)
-         return false;
-      if (dpp8)
-         return false;
-   }
+      if (instr->operands.size() >= 3 && instr->operands[2].isFixed() &&
+            instr->operands[2].isOfType(aco::RegType::sgpr) && instr->operands[2].physReg() != aco::vcc &&
+            gfx_level < GFX11)
+            return false;
 
-   for (unsigned i = 0; i < instr->operands.size(); i++) {
-      if (instr->operands[i].isLiteral())
-         return false;
-      if (!instr->operands[i].isOfType(RegType::vgpr) && i < 2)
-         return false;
-   }
+      if (instr->isVOP3() && gfx_level < GFX11) {
+            const aco::VALU_instruction* vop3 = &instr->valu();
+            if (vop3->clamp || vop3->omod)
+                  return false;
+            if (dpp8)
+                  return false;
+      }
 
-   /* According to LLVM, it's unsafe to combine DPP into v_cmpx. */
-   if (instr->writes_exec())
-      return false;
+      for (unsigned i = 0; i < instr->operands.size(); i++) {
+            if (instr->operands[i].isLiteral())
+                  return false;
+            if (!instr->operands[i].isOfType(aco::RegType::vgpr) && i < 2)
+                  return false;
+      }
 
-   /* simpler than listing all VOP3P opcodes which do not support DPP */
-   if (instr->isVOP3P()) {
-      return instr->opcode == aco_opcode::v_fma_mix_f32 ||
-             instr->opcode == aco_opcode::v_fma_mixlo_f16 ||
-             instr->opcode == aco_opcode::v_fma_mixhi_f16 ||
-             instr->opcode == aco_opcode::v_dot2_f32_f16 ||
-             instr->opcode == aco_opcode::v_dot2_f32_bf16;
-   }
-
-   if (instr->opcode == aco_opcode::v_pk_fmac_f16)
-      return gfx_level < GFX11;
-
-   /* there are more cases but those all take 64-bit inputs */
-   return instr->opcode != aco_opcode::v_madmk_f32 && instr->opcode != aco_opcode::v_madak_f32 &&
-          instr->opcode != aco_opcode::v_madmk_f16 && instr->opcode != aco_opcode::v_madak_f16 &&
-          instr->opcode != aco_opcode::v_fmamk_f32 && instr->opcode != aco_opcode::v_fmaak_f32 &&
-          instr->opcode != aco_opcode::v_fmamk_f16 && instr->opcode != aco_opcode::v_fmaak_f16 &&
-          instr->opcode != aco_opcode::v_readfirstlane_b32 &&
-          instr->opcode != aco_opcode::v_cvt_f64_i32 &&
-          instr->opcode != aco_opcode::v_cvt_f64_f32 &&
-          instr->opcode != aco_opcode::v_cvt_f64_u32 && instr->opcode != aco_opcode::v_mul_lo_u32 &&
-          instr->opcode != aco_opcode::v_mul_lo_i32 && instr->opcode != aco_opcode::v_mul_hi_u32 &&
-          instr->opcode != aco_opcode::v_mul_hi_i32 &&
-          instr->opcode != aco_opcode::v_qsad_pk_u16_u8 &&
-          instr->opcode != aco_opcode::v_mqsad_pk_u16_u8 &&
-          instr->opcode != aco_opcode::v_mqsad_u32_u8 &&
-          instr->opcode != aco_opcode::v_mad_u64_u32 &&
-          instr->opcode != aco_opcode::v_mad_i64_i32 &&
-          instr->opcode != aco_opcode::v_permlane16_b32 &&
-          instr->opcode != aco_opcode::v_permlanex16_b32 &&
-          instr->opcode != aco_opcode::v_permlane64_b32 &&
-          instr->opcode != aco_opcode::v_readlane_b32_e64 &&
-          instr->opcode != aco_opcode::v_writelane_b32_e64;
+      /* According to LLVM, it's unsafe to combine DPP into v_cmpx. */
+      if (instr->writes_exec())
+            return false;
+
+      /* simpler than listing all VOP3P opcodes which do not support DPP */
+      if (instr->isVOP3P()) {
+            return instr->opcode == aco::aco_opcode::v_fma_mix_f32 ||
+            instr->opcode == aco::aco_opcode::v_fma_mixlo_f16 ||
+            instr->opcode == aco::aco_opcode::v_fma_mixhi_f16 ||
+            instr->opcode == aco::aco_opcode::v_dot2_f32_f16 ||
+            instr->opcode == aco::aco_opcode::v_dot2_f32_bf16;
+      }
+
+      if (instr->opcode == aco::aco_opcode::v_pk_fmac_f16)
+            return gfx_level < GFX11;
+
+      /* Vega-specific: enable DPP16 for more 16-bit ops on wave64 for quad-perm efficiency. */
+      bool vega_dpp16 = gfx_level == GFX9 && !dpp8 && instr->operands[0].bytes() == 2;
+
+      /* there are more cases but those all take 64-bit inputs */
+      return (instr->opcode != aco::aco_opcode::v_madmk_f32 && instr->opcode != aco::aco_opcode::v_madak_f32 &&
+      instr->opcode != aco::aco_opcode::v_madmk_f16 && instr->opcode != aco::aco_opcode::v_madak_f16 &&
+      instr->opcode != aco::aco_opcode::v_fmamk_f32 && instr->opcode != aco::aco_opcode::v_fmaak_f32 &&
+      instr->opcode != aco::aco_opcode::v_fmamk_f16 && instr->opcode != aco::aco_opcode::v_fmaak_f16 &&
+      instr->opcode != aco::aco_opcode::v_readfirstlane_b32 &&
+      instr->opcode != aco::aco_opcode::v_cvt_f64_i32 &&
+      instr->opcode != aco::aco_opcode::v_cvt_f64_f32 &&
+      instr->opcode != aco::aco_opcode::v_cvt_f64_u32 && instr->opcode != aco::aco_opcode::v_mul_lo_u32 &&
+      instr->opcode != aco::aco_opcode::v_mul_lo_i32 && instr->opcode != aco::aco_opcode::v_mul_hi_u32 &&
+      instr->opcode != aco::aco_opcode::v_mul_hi_i32 &&
+      instr->opcode != aco::aco_opcode::v_qsad_pk_u16_u8 &&
+      instr->opcode != aco::aco_opcode::v_mqsad_pk_u16_u8 &&
+      instr->opcode != aco::aco_opcode::v_mqsad_u32_u8 &&
+      instr->opcode != aco::aco_opcode::v_mad_u64_u32 &&
+      instr->opcode != aco::aco_opcode::v_mad_i64_i32 &&
+      instr->opcode != aco::aco_opcode::v_permlane16_b32 &&
+      instr->opcode != aco::aco_opcode::v_permlanex16_b32 &&
+      instr->opcode != aco::aco_opcode::v_permlane64_b32 &&
+      instr->opcode != aco::aco_opcode::v_readlane_b32_e64 &&
+      instr->opcode != aco::aco_opcode::v_writelane_b32_e64) || vega_dpp16;
 }
 
-aco_ptr<Instruction>
-convert_to_DPP(amd_gfx_level gfx_level, aco_ptr<Instruction>& instr, bool dpp8)
+aco::aco_ptr<aco::Instruction>
+convert_to_DPP(amd_gfx_level gfx_level, aco::aco_ptr<aco::Instruction>& instr, bool dpp8)
 {
-   if (instr->isDPP())
+   if (!instr || instr->isDPP()) {
       return NULL;
+   }
+
+   aco::aco_ptr<aco::Instruction> tmp = std::move(instr);
+   aco::Format format =
+      (aco::Format)((uint32_t)tmp->format | (uint32_t)(dpp8 ? aco::Format::DPP8 : aco::Format::DPP16));
 
-   aco_ptr<Instruction> tmp = std::move(instr);
-   Format format =
-      (Format)((uint32_t)tmp->format | (uint32_t)(dpp8 ? Format::DPP8 : Format::DPP16));
-   if (dpp8)
+   if (dpp8) {
       instr.reset(
-         create_instruction(tmp->opcode, format, tmp->operands.size(), tmp->definitions.size()));
-   else
+         aco::create_instruction(tmp->opcode, format, tmp->operands.size(), tmp->definitions.size()));
+   } else {
       instr.reset(
-         create_instruction(tmp->opcode, format, tmp->operands.size(), tmp->definitions.size()));
+         aco::create_instruction(tmp->opcode, format, tmp->operands.size(), tmp->definitions.size()));
+   }
+
    std::copy(tmp->operands.cbegin(), tmp->operands.cend(), instr->operands.begin());
    std::copy(tmp->definitions.cbegin(), tmp->definitions.cend(), instr->definitions.begin());
 
    if (dpp8) {
-      DPP8_instruction* dpp = &instr->dpp8();
+      aco::DPP8_instruction* dpp = &instr->dpp8();
       dpp->lane_sel = 0xfac688; /* [0,1,2,3,4,5,6,7] */
       dpp->fetch_inactive = gfx_level >= GFX10;
    } else {
-      DPP16_instruction* dpp = &instr->dpp16();
-      dpp->dpp_ctrl = dpp_quad_perm(0, 1, 2, 3);
+      aco::DPP16_instruction* dpp = &instr->dpp16();
+      dpp->dpp_ctrl = aco::dpp_quad_perm(0, 1, 2, 3);
       dpp->row_mask = 0xf;
       dpp->bank_mask = 0xf;
       dpp->fetch_inactive = gfx_level >= GFX10;
@@ -570,12 +706,14 @@ convert_to_DPP(amd_gfx_level gfx_level,
    instr->valu().opsel_lo = tmp->valu().opsel_lo;
    instr->valu().opsel_hi = tmp->valu().opsel_hi;
 
-   if ((instr->isVOPC() || instr->definitions.size() > 1) && gfx_level < GFX11)
-      instr->definitions.back().setPrecolored(vcc);
+   if ((instr->isVOPC() || instr->definitions.size() > 1) && gfx_level < GFX11) {
+      instr->definitions.back().setPrecolored(aco::vcc);
+   }
 
-   if (instr->operands.size() >= 3 && instr->operands[2].isOfType(RegType::sgpr) &&
-       gfx_level < GFX11)
-      instr->operands[2].setPrecolored(vcc);
+   if (instr->operands.size() >= 3 && instr->operands[2].isOfType(aco::RegType::sgpr) &&
+       gfx_level < GFX11) {
+      instr->operands[2].setPrecolored(aco::vcc);
+   }
 
    instr->pass_flags = tmp->pass_flags;
 
@@ -584,16 +722,18 @@ convert_to_DPP(amd_gfx_level gfx_level,
                       (instr->isVOP1() || instr->isVOP2() || instr->isVOPC());
 
    /* VOPC/add_co/sub_co definition needs VCC without VOP3. */
-   remove_vop3 &= instr->definitions.back().regClass().type() != RegType::sgpr ||
+   remove_vop3 &= instr->definitions.back().regClass().type() != aco::RegType::sgpr ||
                   !instr->definitions.back().isFixed() ||
-                  instr->definitions.back().physReg() == vcc;
+                  instr->definitions.back().physReg() == aco::vcc;
 
    /* addc/subb/cndmask 3rd operand needs VCC without VOP3. */
    remove_vop3 &= instr->operands.size() < 3 || !instr->operands[2].isFixed() ||
-                  instr->operands[2].isOfType(RegType::vgpr) || instr->operands[2].physReg() == vcc;
+                  instr->operands[2].isOfType(aco::RegType::vgpr) ||
+                  instr->operands[2].physReg() == aco::vcc;
 
-   if (remove_vop3)
-      instr->format = withoutVOP3(instr->format);
+   if (remove_vop3) {
+      instr->format = aco::withoutVOP3(instr->format);
+   }
 
    return tmp;
 }
@@ -610,12 +750,28 @@ can_use_input_modifiers(amd_gfx_level gf
 bool
 can_use_opsel(amd_gfx_level gfx_level, aco_opcode op, int idx)
 {
-   /* opsel is only GFX9+ */
-   if (gfx_level < GFX9)
+   /* GFX11 has a completely different encoding ("true 16-bit") for this functionality. */
+   if (gfx_level >= GFX11) {
+      return get_gfx11_true16_mask(op) & BITFIELD_BIT(idx == -1 ? 3 : idx);
+   }
+
+   /* This feature does not exist on GFX8 and older. */
+   if (gfx_level < GFX9) {
       return false;
+   }
+
+   /* VOP3P is the "packed math" encoding. It has its own dedicated fields for 16-bit
+    * operations and is NOT promoted to VOP3A. Therefore, this logic does not apply. */
+   if (static_cast<uint16_t>(instr_info.format[static_cast<int>(op)]) &
+       static_cast<uint16_t>(Format::VOP3P)) {
+      return false;
+   }
+
+   /* Map destination index (-1) to bit 3 for consistent mask checking */
+   int check_idx = (idx < 0) ? 3 : idx;
 
    switch (op) {
-   case aco_opcode::v_div_fixup_f16:
+   /* Group 1: Native VOP3A 16-bit Instructions */
    case aco_opcode::v_fma_f16:
    case aco_opcode::v_mad_f16:
    case aco_opcode::v_mad_u16:
@@ -629,40 +785,73 @@ can_use_opsel(amd_gfx_level gfx_level, a
    case aco_opcode::v_max3_f16:
    case aco_opcode::v_max3_i16:
    case aco_opcode::v_max3_u16:
-   case aco_opcode::v_minmax_f16:
-   case aco_opcode::v_maxmin_f16:
-   case aco_opcode::v_max_u16_e64:
-   case aco_opcode::v_max_i16_e64:
-   case aco_opcode::v_min_u16_e64:
-   case aco_opcode::v_min_i16_e64:
+   case aco_opcode::v_fma_legacy_f16:
+   case aco_opcode::v_mad_legacy_f16:
+   case aco_opcode::v_mad_legacy_i16:
+   case aco_opcode::v_mad_legacy_u16:
+   case aco_opcode::v_div_fixup_legacy_f16:
+   case aco_opcode::v_div_fixup_f16:
+      return check_idx < 3; /* Sources 0, 1, 2 are valid. */
+
+   /* Group 2: VOP1/VOP2 Instructions Promotable to VOP3A on GFX9/GFX10 */
+   case aco_opcode::v_mac_f16:
+      return check_idx < 3; /* 3-source VOP2 */
+
+   case aco_opcode::v_add_f16:
+   case aco_opcode::v_sub_f16:
+   case aco_opcode::v_subrev_f16:
+   case aco_opcode::v_mul_f16:
+   case aco_opcode::v_max_f16:
+   case aco_opcode::v_min_f16:
+   case aco_opcode::v_ldexp_f16:
+   case aco_opcode::v_add_u16:
+   case aco_opcode::v_sub_u16:
+   case aco_opcode::v_subrev_u16:
+   case aco_opcode::v_mul_lo_u16:
+   case aco_opcode::v_lshlrev_b16:
+   case aco_opcode::v_lshrrev_b16:
+   case aco_opcode::v_ashrrev_i16:
+   case aco_opcode::v_max_u16:
+   case aco_opcode::v_max_i16:
+   case aco_opcode::v_min_u16:
+   case aco_opcode::v_min_i16:
    case aco_opcode::v_add_i16:
    case aco_opcode::v_sub_i16:
    case aco_opcode::v_add_u16_e64:
    case aco_opcode::v_sub_u16_e64:
+   case aco_opcode::v_mul_lo_u16_e64:
+   case aco_opcode::v_min_i16_e64:
+   case aco_opcode::v_min_u16_e64:
+   case aco_opcode::v_max_i16_e64:
+   case aco_opcode::v_max_u16_e64:
    case aco_opcode::v_lshlrev_b16_e64:
    case aco_opcode::v_lshrrev_b16_e64:
    case aco_opcode::v_ashrrev_i16_e64:
    case aco_opcode::v_and_b16:
    case aco_opcode::v_or_b16:
    case aco_opcode::v_xor_b16:
-   case aco_opcode::v_mul_lo_u16_e64: return true;
-   case aco_opcode::v_pack_b32_f16:
-   case aco_opcode::v_cvt_pknorm_i16_f16:
-   case aco_opcode::v_cvt_pknorm_u16_f16: return idx != -1;
+   case aco_opcode::v_minmax_f16:
+   case aco_opcode::v_maxmin_f16:
+      return check_idx < 2; /* 2-source instructions */
+
+   /* Group 3: Instructions with Special or Restricted Opsel Support */
    case aco_opcode::v_mad_u32_u16:
-   case aco_opcode::v_mad_i32_i16: return idx >= 0 && idx < 2;
+   case aco_opcode::v_mad_i32_i16:
+      return check_idx < 2; /* Only the 16-bit sources (src0, src1). */
+
+   case aco_opcode::v_cvt_pknorm_i16_f16:
+   case aco_opcode::v_cvt_pknorm_u16_f16:
+      return check_idx < 3; /* All source operands, but not dst. */
+
    case aco_opcode::v_dot2_f16_f16:
-   case aco_opcode::v_dot2_bf16_bf16: return idx == -1 || idx == 2;
-   case aco_opcode::v_cndmask_b16: return idx != 2;
-   case aco_opcode::v_interp_p10_f16_f32_inreg:
-   case aco_opcode::v_interp_p10_rtz_f16_f32_inreg: return idx == 0 || idx == 2;
-   case aco_opcode::v_interp_p2_f16_f32_inreg:
-   case aco_opcode::v_interp_p2_rtz_f16_f32_inreg: return idx == -1 || idx == 0;
-   case aco_opcode::v_cvt_pk_fp8_f32:
-   case aco_opcode::p_v_cvt_pk_fp8_f32_ovfl:
-   case aco_opcode::v_cvt_pk_bf8_f32: return idx == -1;
+   case aco_opcode::v_dot2_bf16_bf16:
+      return check_idx == 2 || check_idx == 3; /* src2 and dst */
+
+   case aco_opcode::v_interp_p2_f16:
+      return check_idx == 0 || check_idx == 2 || check_idx == 3;
+
    default:
-      return gfx_level >= GFX11 && (get_gfx11_true16_mask(op) & BITFIELD_BIT(idx == -1 ? 3 : idx));
+      return false;
    }
 }
 
@@ -689,63 +878,86 @@ can_write_m0(const aco_ptr<Instruction>&
 }
 
 bool
-instr_is_16bit(amd_gfx_level gfx_level, aco_opcode op)
+instr_is_16bit(amd_gfx_level gfx_level, aco::aco_opcode op)
 {
-   /* partial register writes are GFX9+, only */
-   if (gfx_level < GFX9)
-      return false;
+      /* partial register writes are GFX9+, only */
+      if (gfx_level < GFX9)
+            return false;
 
-   switch (op) {
-   /* VOP3 */
-   case aco_opcode::v_mad_legacy_f16:
-   case aco_opcode::v_mad_legacy_u16:
-   case aco_opcode::v_mad_legacy_i16:
-   case aco_opcode::v_fma_legacy_f16:
-   case aco_opcode::v_div_fixup_legacy_f16: return false;
-   case aco_opcode::v_interp_p2_f16:
-   case aco_opcode::v_interp_p2_hi_f16:
-   case aco_opcode::v_fma_mixlo_f16:
-   case aco_opcode::v_fma_mixhi_f16:
-   /* VOP2 */
-   case aco_opcode::v_mac_f16:
-   case aco_opcode::v_madak_f16:
-   case aco_opcode::v_madmk_f16: return gfx_level >= GFX9;
-   case aco_opcode::v_add_f16:
-   case aco_opcode::v_sub_f16:
-   case aco_opcode::v_subrev_f16:
-   case aco_opcode::v_mul_f16:
-   case aco_opcode::v_max_f16:
-   case aco_opcode::v_min_f16:
-   case aco_opcode::v_ldexp_f16:
-   case aco_opcode::v_fmac_f16:
-   case aco_opcode::v_fmamk_f16:
-   case aco_opcode::v_fmaak_f16:
-   /* VOP1 */
-   case aco_opcode::v_cvt_f16_f32:
-   case aco_opcode::p_v_cvt_f16_f32_rtne:
-   case aco_opcode::v_cvt_f16_u16:
-   case aco_opcode::v_cvt_f16_i16:
-   case aco_opcode::v_rcp_f16:
-   case aco_opcode::v_sqrt_f16:
-   case aco_opcode::v_rsq_f16:
-   case aco_opcode::v_log_f16:
-   case aco_opcode::v_exp_f16:
-   case aco_opcode::v_frexp_mant_f16:
-   case aco_opcode::v_frexp_exp_i16_f16:
-   case aco_opcode::v_floor_f16:
-   case aco_opcode::v_ceil_f16:
-   case aco_opcode::v_trunc_f16:
-   case aco_opcode::v_rndne_f16:
-   case aco_opcode::v_fract_f16:
-   case aco_opcode::v_sin_f16:
-   case aco_opcode::v_cos_f16:
-   case aco_opcode::v_cvt_u16_f16:
-   case aco_opcode::v_cvt_i16_f16:
-   case aco_opcode::v_cvt_norm_i16_f16:
-   case aco_opcode::v_cvt_norm_u16_f16: return gfx_level >= GFX10;
-   /* all non legacy opsel instructions preserve the high bits */
-   default: return can_use_opsel(gfx_level, op, -1);
-   }
+      switch (op) {
+            /* VOP3 */
+            case aco::aco_opcode::v_mad_legacy_f16:
+            case aco::aco_opcode::v_mad_legacy_u16:
+            case aco::aco_opcode::v_mad_legacy_i16:
+            case aco::aco_opcode::v_fma_legacy_f16:
+            case aco::aco_opcode::v_div_fixup_legacy_f16: return false;
+            case aco::aco_opcode::v_interp_p2_f16:
+            case aco::aco_opcode::v_interp_p2_hi_f16:
+            case aco::aco_opcode::v_fma_mixlo_f16:
+            case aco::aco_opcode::v_fma_mixhi_f16:
+                  /* VOP2 */
+                  case aco::aco_opcode::v_mac_f16:
+                  case aco::aco_opcode::v_madak_f16:
+                  case aco::aco_opcode::v_madmk_f16: return gfx_level >= GFX9;
+                  case aco::aco_opcode::v_add_f16:
+                  case aco::aco_opcode::v_sub_f16:
+                  case aco::aco_opcode::v_subrev_f16:
+                  case aco::aco_opcode::v_mul_f16:
+                  case aco::aco_opcode::v_max_f16:
+                  case aco::aco_opcode::v_min_f16:
+                  case aco::aco_opcode::v_ldexp_f16:
+                  case aco::aco_opcode::v_fmac_f16:
+                  case aco::aco_opcode::v_fmamk_f16:
+                  case aco::aco_opcode::v_fmaak_f16:
+                        /* VOP1 */
+                        case aco::aco_opcode::v_cvt_f16_f32:
+                        case aco::aco_opcode::p_v_cvt_f16_f32_rtne:
+                        case aco::aco_opcode::v_cvt_f16_u16:
+                        case aco::aco_opcode::v_cvt_f16_i16:
+                        case aco::aco_opcode::v_rcp_f16:
+                        case aco::aco_opcode::v_sqrt_f16:
+                        case aco::aco_opcode::v_rsq_f16:
+                        case aco::aco_opcode::v_log_f16:
+                        case aco::aco_opcode::v_exp_f16:
+                        case aco::aco_opcode::v_frexp_mant_f16:
+                        case aco::aco_opcode::v_frexp_exp_i16_f16:
+                        case aco::aco_opcode::v_floor_f16:
+                        case aco::aco_opcode::v_ceil_f16:
+                        case aco::aco_opcode::v_trunc_f16:
+                        case aco::aco_opcode::v_rndne_f16:
+                        case aco::aco_opcode::v_fract_f16:
+                        case aco::aco_opcode::v_sin_f16:
+                        case aco::aco_opcode::v_cos_f16:
+                        case aco::aco_opcode::v_cvt_u16_f16:
+                        case aco::aco_opcode::v_cvt_i16_f16:
+                        case aco::aco_opcode::v_cvt_norm_i16_f16:
+                        case aco::aco_opcode::v_cvt_norm_u16_f16: return gfx_level >= GFX10;
+                        /* VOP3P instructions are inherently 16-bit on GFX9 */
+                        case aco::aco_opcode::v_pk_mad_i16:
+                        case aco::aco_opcode::v_pk_mul_lo_u16:
+                        case aco::aco_opcode::v_pk_add_i16:
+                        case aco::aco_opcode::v_pk_sub_i16:
+                        case aco::aco_opcode::v_pk_lshlrev_b16:
+                        case aco::aco_opcode::v_pk_lshrrev_b16:
+                        case aco::aco_opcode::v_pk_ashrrev_i16:
+                        case aco::aco_opcode::v_pk_max_i16:
+                        case aco::aco_opcode::v_pk_min_i16:
+                        case aco::aco_opcode::v_pk_mad_u16:
+                        case aco::aco_opcode::v_pk_add_u16:
+                        case aco::aco_opcode::v_pk_sub_u16:
+                        case aco::aco_opcode::v_pk_max_u16:
+                        case aco::aco_opcode::v_pk_min_u16:
+                        case aco::aco_opcode::v_pk_fma_f16:
+                        case aco::aco_opcode::v_pk_add_f16:
+                        case aco::aco_opcode::v_pk_mul_f16:
+                        case aco::aco_opcode::v_pk_min_f16:
+                        case aco::aco_opcode::v_pk_max_f16:
+                        case aco::aco_opcode::v_fma_mix_f32:
+                        case aco::aco_opcode::v_dot2_f32_f16:
+                        case aco::aco_opcode::v_dot2_f32_bf16:
+                              return gfx_level == GFX9;
+                        default: return aco::can_use_opsel(gfx_level, op, -1);
+      }
 }
 
 /* On GFX11, for some instructions, bit 7 of the destination/operand vgpr is opsel and the field
@@ -1270,6 +1482,7 @@ wait_imm::pack(enum amd_gfx_level gfx_le
 {
    uint16_t imm = 0;
    assert(exp == unset_counter || exp <= 0x7);
+
    if (gfx_level >= GFX11) {
       assert(lgkm == unset_counter || lgkm <= 0x3f);
       assert(vm == unset_counter || vm <= 0x3f);
@@ -1287,12 +1500,15 @@ wait_imm::pack(enum amd_gfx_level gfx_le
       assert(vm == unset_counter || vm <= 0xf);
       imm = ((lgkm & 0xf) << 8) | ((exp & 0x7) << 4) | (vm & 0xf);
    }
-   if (gfx_level < GFX9 && vm == wait_imm::unset_counter)
-      imm |= 0xc000; /* should have no effect on pre-GFX9 and now we won't have to worry about the
-                        architecture when interpreting the immediate */
-   if (gfx_level < GFX10 && lgkm == wait_imm::unset_counter)
-      imm |= 0x3000; /* should have no effect on pre-GFX10 and now we won't have to worry about the
-                        architecture when interpreting the immediate */
+
+   if (gfx_level < GFX9 && vm == wait_imm::unset_counter) {
+      imm |= 0xc000; /* should have no effect on pre-GFX9 */
+   }
+
+   if (gfx_level < GFX10 && lgkm == wait_imm::unset_counter) {
+      imm |= 0x3000; /* should have no effect on pre-GFX10 */
+   }
+
    return imm;
 }
 
@@ -1390,8 +1606,9 @@ wait_imm::combine(const wait_imm& other)
 {
    bool changed = false;
    for (unsigned i = 0; i < wait_type_num; i++) {
-      if (other[i] < (*this)[i])
+      if (other[i] < (*this)[i]) {
          changed = true;
+      }
       (*this)[i] = std::min((*this)[i], other[i]);
    }
    return changed;
@@ -1504,34 +1721,37 @@ should_form_clause(const Instruction* a,
 aco::small_vec<uint32_t, 2>
 get_tied_defs(Instruction* instr)
 {
-   aco::small_vec<uint32_t, 2> ops;
-   if (instr->opcode == aco_opcode::v_interp_p2_f32 || instr->opcode == aco_opcode::v_mac_f32 ||
-       instr->opcode == aco_opcode::v_fmac_f32 || instr->opcode == aco_opcode::v_mac_f16 ||
-       instr->opcode == aco_opcode::v_fmac_f16 || instr->opcode == aco_opcode::v_mac_legacy_f32 ||
-       instr->opcode == aco_opcode::v_fmac_legacy_f32 ||
-       instr->opcode == aco_opcode::v_pk_fmac_f16 || instr->opcode == aco_opcode::v_writelane_b32 ||
-       instr->opcode == aco_opcode::v_writelane_b32_e64 ||
-       instr->opcode == aco_opcode::v_dot4c_i32_i8 || instr->opcode == aco_opcode::s_fmac_f32 ||
-       instr->opcode == aco_opcode::s_fmac_f16) {
-      ops.push_back(2);
-   } else if (instr->opcode == aco_opcode::s_addk_i32 || instr->opcode == aco_opcode::s_mulk_i32 ||
-              instr->opcode == aco_opcode::s_cmovk_i32 ||
-              instr->opcode == aco_opcode::ds_bvh_stack_push4_pop1_rtn_b32 ||
-              instr->opcode == aco_opcode::ds_bvh_stack_push8_pop1_rtn_b32 ||
-              instr->opcode == aco_opcode::ds_bvh_stack_push8_pop2_rtn_b64) {
-      ops.push_back(0);
-   } else if (instr->isMUBUF() && instr->definitions.size() == 1 &&
+      aco::small_vec<uint32_t, 2> ops;
+      if (instr->opcode == aco_opcode::v_interp_p2_f32 || instr->opcode == aco_opcode::v_mac_f32 ||
+            instr->opcode == aco_opcode::v_fmac_f32 || instr->opcode == aco_opcode::v_mac_f16 ||
+            instr->opcode == aco_opcode::v_fmac_f16 || instr->opcode == aco_opcode::v_mac_legacy_f32 ||
+            instr->opcode == aco_opcode::v_fmac_legacy_f32 ||
+            instr->opcode == aco_opcode::v_pk_fmac_f16 || instr->opcode == aco_opcode::v_writelane_b32 ||
+            instr->opcode == aco_opcode::v_writelane_b32_e64 ||
+            instr->opcode == aco_opcode::v_dot4c_i32_i8 || instr->opcode == aco_opcode::s_fmac_f32 ||
+            instr->opcode == aco_opcode::s_fmac_f16) {
+            ops.push_back(2);
+      } else if (instr->opcode == aco_opcode::s_addk_i32 || instr->opcode == aco_opcode::s_mulk_i32 ||
+                 instr->opcode == aco_opcode::s_cmovk_i32) {
+            /* These SOPK instructions have an implicit source operand which is the same as the destination. */
+            ops.push_back(0);
+      } else if (instr->opcode == aco_opcode::ds_bvh_stack_push4_pop1_rtn_b32 ||
+                 instr->opcode == aco_opcode::ds_bvh_stack_push8_pop1_rtn_b32 ||
+                 instr->opcode == aco_opcode::ds_bvh_stack_push8_pop2_rtn_b64) {
+            ops.push_back(0);
+      } else if (instr->isMUBUF() && instr->definitions.size() == 1 &&
               (instr_info.is_atomic[(int)instr->opcode] || instr->mubuf().tfe)) {
-      ops.push_back(3);
-   } else if (instr->isMIMG() && instr->definitions.size() == 1 &&
-              !instr->operands[2].isUndefined()) {
-      ops.push_back(2);
-   } else if (instr->opcode == aco_opcode::image_bvh8_intersect_ray) {
-      /* VADDR starts at 3. */
-      ops.push_back(3 + 4);
-      ops.push_back(3 + 7);
-   }
-   return ops;
+            ops.push_back(3);
+      } else if (instr->isMIMG() && instr->definitions.size() == 1 &&
+                 !instr->operands[2].isUndefined()) {
+            /* MIMG atomic instructions with a return value have the data source/destination tied. */
+            ops.push_back(2);
+      } else if (instr->opcode == aco_opcode::image_bvh8_intersect_ray) {
+            /* VADDR for this RT instruction has tied operands */
+            ops.push_back(3 + 4);
+            ops.push_back(3 + 7);
+      }
+      return ops;
 }
 
 uint8_t
@@ -1713,10 +1933,24 @@ create_instruction(aco_opcode opcode, Fo
                    uint32_t num_definitions)
 {
    size_t size = get_instr_data_size(format);
-   size_t total_size = size + num_operands * sizeof(Operand) + num_definitions * sizeof(Definition);
+   size_t total_size = size + num_operands * sizeof(Operand) +
+                      num_definitions * sizeof(Definition);
+
+   /* Use appropriate alignment for cache efficiency */
+   size_t alignment = alignof(uint32_t);
+   if (total_size >= 64) {
+      /* Align larger instructions to cache line boundaries */
+      alignment = 64;
+   } else if (total_size >= 32) {
+      /* Half cache line alignment for medium instructions */
+      alignment = 32;
+   }
+
+   void* data = instruction_buffer->allocate(total_size, alignment);
 
-   void* data = instruction_buffer->allocate(total_size, alignof(uint32_t));
+   /* Zero memory efficiently based on size */
    memset(data, 0, total_size);
+
    Instruction* inst = (Instruction*)data;
 
    inst->opcode = opcode;

--- a/src/amd/compiler/aco_opcodes.py	2025-05-31 22:57:26.003334290 +0200
+++ b/src/amd/compiler/aco_opcodes.py	2025-06-01 00:30:01.222104109 +0200
@@ -254,6 +254,7 @@ F32 = SrcDestInfo(AcoBaseType.aco_base_t
 F64 = SrcDestInfo(AcoBaseType.aco_base_type_float, 64, 1, FixedReg.not_fixed, True)
 BF16 = SrcDestInfo(AcoBaseType.aco_base_type_bfloat, 16, 1, FixedReg.not_fixed, True)
 PkU16 = SrcDestInfo(AcoBaseType.aco_base_type_uint, 16, 2, FixedReg.not_fixed, False)
+PkI16 = SrcDestInfo(AcoBaseType.aco_base_type_int, 16, 2, FixedReg.not_fixed, False)
 PkF16 = SrcDestInfo(AcoBaseType.aco_base_type_float, 16, 2, FixedReg.not_fixed, True)
 PkF32 = SrcDestInfo(AcoBaseType.aco_base_type_float, 32, 2, FixedReg.not_fixed, False)
 PkBF16 = SrcDestInfo(AcoBaseType.aco_base_type_bfloat, 16, 2, FixedReg.not_fixed, True)
@@ -993,7 +994,7 @@ VOP2 = {
    ("v_fmac_f16",          dst(F16),      src(F16, F16, F16), op(gfx10=0x36)),
    ("v_fmamk_f16",         dst(noMods(F16)), noMods(src(F16, F16, IMM)), op(gfx10=0x37)),
    ("v_fmaak_f16",         dst(noMods(F16)), noMods(src(F16, F16, IMM)), op(gfx10=0x38)),
-   ("v_pk_fmac_f16",       dst(noMods(PkF16)), noMods(src(PkF16, PkF16, PkF16)), op(gfx10=0x3c)),
+   ("v_pk_fmac_f16",       dst(noMods(PkF16)), noMods(src(PkF16, PkF16, PkF16)), op(gfx10=0x3c), InstrClass.ValuFma),
    ("v_dot2c_f32_f16",     dst(noMods(F32)), noMods(src(PkF16, PkF16, F32)), op(gfx9=0x37, gfx10=0x02, gfx12=-1)), #v_dot2acc_f32_f16 in GFX11
    ("v_add_f64",           dst(F64),      src(F64, F64), op(gfx12=0x02), InstrClass.ValuDoubleAdd),
    ("v_mul_f64",           dst(F64),      src(F64, F64), op(gfx12=0x06), InstrClass.ValuDoubleAdd),
@@ -1191,15 +1192,15 @@ for comp, dtype, cmps, cmpx in itertools
 
 # VOPP instructions: packed 16bit instructions - 2 or 3 inputs and 1 output
 VOPP = {
-   ("v_pk_mad_i16",     dst(PkU16), src(PkU16, PkU16, PkU16), op(gfx9=0x00)),
+   ("v_pk_mad_i16",     dst(PkI16), src(PkI16, PkI16, PkI16), op(gfx9=0x00)),
    ("v_pk_mul_lo_u16",  dst(PkU16), src(PkU16, PkU16), op(gfx9=0x01)),
-   ("v_pk_add_i16",     dst(PkU16), src(PkU16, PkU16), op(gfx9=0x02)),
-   ("v_pk_sub_i16",     dst(PkU16), src(PkU16, PkU16), op(gfx9=0x03)),
+   ("v_pk_add_i16",     dst(PkI16), src(PkI16, PkI16), op(gfx9=0x02)),
+   ("v_pk_sub_i16",     dst(PkI16), src(PkI16, PkI16), op(gfx9=0x03)),
    ("v_pk_lshlrev_b16", dst(PkU16), src(PkU16, PkU16), op(gfx9=0x04)),
    ("v_pk_lshrrev_b16", dst(PkU16), src(PkU16, PkU16), op(gfx9=0x05)),
-   ("v_pk_ashrrev_i16", dst(PkU16), src(PkU16, PkU16), op(gfx9=0x06)),
-   ("v_pk_max_i16",     dst(PkU16), src(PkU16, PkU16), op(gfx9=0x07)),
-   ("v_pk_min_i16",     dst(PkU16), src(PkU16, PkU16), op(gfx9=0x08)),
+   ("v_pk_ashrrev_i16", dst(PkI16), src(PkI16, PkI16), op(gfx9=0x06)),
+   ("v_pk_max_i16",     dst(PkI16), src(PkI16, PkI16), op(gfx9=0x07)),
+   ("v_pk_min_i16",     dst(PkI16), src(PkI16, PkI16), op(gfx9=0x08)),
    ("v_pk_mad_u16",     dst(PkU16), src(PkU16, PkU16, PkU16), op(gfx9=0x09)),
    ("v_pk_add_u16",     dst(PkU16), src(PkU16, PkU16), op(gfx9=0x0a)),
    ("v_pk_sub_u16",     dst(PkU16), src(PkU16, PkU16), op(gfx9=0x0b)),
@@ -1209,12 +1210,12 @@ VOPP = {
    ("v_pk_add_f16",     dst(PkF16), src(PkF16, PkF16), op(gfx9=0x0f)),
    ("v_pk_mul_f16",     dst(PkF16), src(PkF16, PkF16), op(gfx9=0x10)),
    ("v_pk_min_f16",     dst(PkF16), src(PkF16, PkF16), op(gfx9=0x11, gfx12=0x1b)), # called v_pk_min_num_f16 in GFX12
-   ("v_pk_max_f16",     dst(PkF16), src(PkF16, PkF16), op(gfx9=0x12, gfx12=0x1c)), # called v_pk_min_num_f16 in GFX12
+   ("v_pk_max_f16",     dst(PkF16), src(PkF16, PkF16), op(gfx9=0x12, gfx12=0x1c)), # called v_pk_max_num_f16 in GFX12
    ("v_pk_minimum_f16", dst(PkF16), src(PkF16, PkF16), op(gfx12=0x1d)),
    ("v_pk_maximum_f16", dst(PkF16), src(PkF16, PkF16), op(gfx12=0x1e)),
-   ("v_fma_mix_f32",    dst(F32), src(F32, F32, F32), op(gfx9=0x20)), # v_mad_mix_f32 in VEGA ISA, v_fma_mix_f32 in RDNA ISA
-   ("v_fma_mixlo_f16",  dst(F16), src(F32, F32, F32), op(gfx9=0x21)), # v_mad_mixlo_f16 in VEGA ISA, v_fma_mixlo_f16 in RDNA ISA
-   ("v_fma_mixhi_f16",  dst(F16), src(F32, F32, F32), op(gfx9=0x22)), # v_mad_mixhi_f16 in VEGA ISA, v_fma_mixhi_f16 in RDNA ISA
+   ("v_fma_mix_f32",    dst(F32), src(F32, F32, F32), op(gfx9=0x20), InstrClass.ValuFma),
+   ("v_fma_mixlo_f16",  dst(F16), src(F32, F32, F32), op(gfx9=0x21), InstrClass.ValuFma),
+   ("v_fma_mixhi_f16",  dst(F16), src(F32, F32, F32), op(gfx9=0x22), InstrClass.ValuFma),
    ("v_dot2_i32_i16",      dst(U32), src(PkU16, PkU16, U32), op(gfx9=0x26, gfx10=0x14, gfx11=-1)),
    ("v_dot2_u32_u16",      dst(U32), src(PkU16, PkU16, U32), op(gfx9=0x27, gfx10=0x15, gfx11=-1)),
    ("v_dot4_i32_iu8",      dst(U32), src(PkU16, PkU16, U32), op(gfx11=0x16)),
@@ -1282,8 +1283,8 @@ for (name, defs, ops, num) in VINTERP:
 # VOP3 instructions: 3 inputs, 1 output
 # VOP3b instructions: have a unique scalar output, e.g. VOP2 with vcc out
 VOP3 = {
-   ("v_mad_legacy_f32",        dst(F32), src(F32, F32, F32), op(0x140, gfx8=0x1c0, gfx10=0x140, gfx11=-1)), # GFX6-GFX10
-   ("v_mad_f32",               dst(F32), src(F32, F32, F32), op(0x141, gfx8=0x1c1, gfx10=0x141, gfx11=-1)),
+   ("v_mad_legacy_f32",        dst(F32), src(mods(F32), mods(F32), mods(F32)), op(0x140, gfx8=0x1c0, gfx10=0x140, gfx11=-1)), # GFX6-GFX10
+   ("v_mad_f32",               dst(F32), src(mods(F32), mods(F32), mods(F32)), op(0x141, gfx8=0x1c1, gfx10=0x141, gfx11=-1)),
    ("v_mad_i32_i24",           dst(U32), src(U32, U32, U32), op(0x142, gfx8=0x1c2, gfx10=0x142, gfx11=0x20a)),
    ("v_mad_u32_u24",           dst(U32), src(U32, U32, U32), op(0x143, gfx8=0x1c3, gfx10=0x143, gfx11=0x20b)),
    ("v_cubeid_f32",            dst(F32), src(F32, F32, F32), op(0x144, gfx8=0x1c4, gfx10=0x144, gfx11=0x20c)),
@@ -1293,19 +1294,19 @@ VOP3 = {
    ("v_bfe_u32",               dst(U32), src(U32, U32, U32), op(0x148, gfx8=0x1c8, gfx10=0x148, gfx11=0x210)),
    ("v_bfe_i32",               dst(U32), src(U32, U32, U32), op(0x149, gfx8=0x1c9, gfx10=0x149, gfx11=0x211)),
    ("v_bfi_b32",               dst(U32), src(U32, U32, U32), op(0x14a, gfx8=0x1ca, gfx10=0x14a, gfx11=0x212)),
-   ("v_fma_f32",               dst(F32), src(F32, F32, F32), op(0x14b, gfx8=0x1cb, gfx10=0x14b, gfx11=0x213), InstrClass.ValuFma),
-   ("v_fma_f64",               dst(F64), src(F64, F64, F64), op(0x14c, gfx8=0x1cc, gfx10=0x14c, gfx11=0x214), InstrClass.ValuDouble),
+   ("v_fma_f32",               dst(F32), src(mods(F32), mods(F32), mods(F32)), op(0x14b, gfx8=0x1cb, gfx10=0x14b, gfx11=0x213), InstrClass.ValuFma),
+   ("v_fma_f64",               dst(F64), src(mods(F64), mods(F64), mods(F64)), op(0x14c, gfx8=0x1cc, gfx10=0x14c, gfx11=0x214), InstrClass.ValuDouble),
    ("v_lerp_u8",               dst(U32), src(U32, U32, U32), op(0x14d, gfx8=0x1cd, gfx10=0x14d, gfx11=0x215)),
-   ("v_alignbit_b32",          dst(U32), src(U32, U32, U16), op(0x14e, gfx8=0x1ce, gfx10=0x14e, gfx11=0x216)),
-   ("v_alignbyte_b32",         dst(U32), src(U32, U32, U16), op(0x14f, gfx8=0x1cf, gfx10=0x14f, gfx11=0x217)),
+   ("v_alignbit_b32",          dst(U32), src(mods(U32), mods(U32), noMods(U16)), op(0x14e, gfx8=0x1ce, gfx10=0x14e, gfx11=0x216)),
+   ("v_alignbyte_b32",         dst(U32), src(mods(U32), mods(U32), noMods(U16)), op(0x14f, gfx8=0x1cf, gfx10=0x14f, gfx11=0x217)),
    ("v_mullit_f32",            dst(F32), src(F32, F32, F32), op(0x150, gfx8=-1, gfx10=0x150, gfx11=0x218)),
-   ("v_min3_f32",              dst(F32), src(F32, F32, F32), op(0x151, gfx8=0x1d0, gfx10=0x151, gfx11=0x219, gfx12=0x229)), # called v_min3_num_f32 in GFX12
+   ("v_min3_f32",              dst(F32), src(F32, F32, F32), op(0x151, gfx8=0x1d0, gfx10=0x151, gfx11=0x219, gfx12=0x229)),
    ("v_min3_i32",              dst(U32), src(U32, U32, U32), op(0x152, gfx8=0x1d1, gfx10=0x152, gfx11=0x21a)),
    ("v_min3_u32",              dst(U32), src(U32, U32, U32), op(0x153, gfx8=0x1d2, gfx10=0x153, gfx11=0x21b)),
-   ("v_max3_f32",              dst(F32), src(F32, F32, F32), op(0x154, gfx8=0x1d3, gfx10=0x154, gfx11=0x21c, gfx12=0x22a)), # called v_max3_num_f32 in GFX12
+   ("v_max3_f32",              dst(F32), src(F32, F32, F32), op(0x154, gfx8=0x1d3, gfx10=0x154, gfx11=0x21c, gfx12=0x22a)),
    ("v_max3_i32",              dst(U32), src(U32, U32, U32), op(0x155, gfx8=0x1d4, gfx10=0x155, gfx11=0x21d)),
    ("v_max3_u32",              dst(U32), src(U32, U32, U32), op(0x156, gfx8=0x1d5, gfx10=0x156, gfx11=0x21e)),
-   ("v_med3_f32",              dst(F32), src(F32, F32, F32), op(0x157, gfx8=0x1d6, gfx10=0x157, gfx11=0x21f, gfx12=0x231)), # called v_med3_num_f32 in GFX12
+   ("v_med3_f32",              dst(F32), src(F32, F32, F32), op(0x157, gfx8=0x1d6, gfx10=0x157, gfx11=0x21f, gfx12=0x231)),
    ("v_med3_i32",              dst(U32), src(U32, U32, U32), op(0x158, gfx8=0x1d7, gfx10=0x158, gfx11=0x220)),
    ("v_med3_u32",              dst(U32), src(U32, U32, U32), op(0x159, gfx8=0x1d8, gfx10=0x159, gfx11=0x221)),
    ("v_sad_u8",                dst(U32), src(U32, U32, U32), op(0x15a, gfx8=0x1d9, gfx10=0x15a, gfx11=0x222)),
@@ -1313,6 +1314,9 @@ VOP3 = {
    ("v_sad_u16",               dst(U32), src(U32, U32, U32), op(0x15c, gfx8=0x1db, gfx10=0x15c, gfx11=0x224)),
    ("v_sad_u32",               dst(U32), src(U32, U32, U32), op(0x15d, gfx8=0x1dc, gfx10=0x15d, gfx11=0x225)),
    ("v_cvt_pk_u8_f32",         dst(U32), src(F32, U32, U32), op(0x15e, gfx8=0x1dd, gfx10=0x15e, gfx11=0x226)),
+   ("p_v_cvt_pk_u8_f32",       dst(U32), src(F32), op(-1)),
+   ("v_div_f64",               dst(F64), src(F64, F64), op(gfx10=0x1d1, gfx11=0x343), InstrClass.ValuDouble),
+   ("v_dot2_f32_f32",          dst(noMods(F32)), noMods(src(PkF32, PkF32, F32)), op(gfx11=0x1e3)),
    ("v_div_fixup_f32",         dst(F32), src(F32, F32, F32), op(0x15f, gfx8=0x1de, gfx10=0x15f, gfx11=0x227)),
    ("v_div_fixup_f64",         dst(F64), src(F64, F64, F64), op(0x160, gfx8=0x1df, gfx10=0x160, gfx11=0x228)),
    ("v_lshl_b64",              dst(U64), src(U64, U32), op(0x161, gfx8=-1), InstrClass.Valu64),
@@ -1338,12 +1342,12 @@ VOP3 = {
    ("v_mqsad_u32_u8",          dst(U128), src(U64, U32, U128), op(gfx7=0x175, gfx8=0x1e7, gfx10=0x175, gfx11=0x23d), InstrClass.ValuQuarterRate32),
    ("v_mad_u64_u32",           dst(U64, VCC), src(U32, U32, U64), op(gfx7=0x176, gfx8=0x1e8, gfx10=0x176, gfx11=0x2fe), InstrClass.Valu64), # called v_mad_co_u64_u32 in GFX12
    ("v_mad_i64_i32",           dst(I64, VCC), src(U32, U32, I64), op(gfx7=0x177, gfx8=0x1e9, gfx10=0x177, gfx11=0x2ff), InstrClass.Valu64), # called v_mad_co_i64_i32 in GFX12
-   ("v_mad_legacy_f16",        dst(F16), src(F16, F16, F16), op(gfx8=0x1ea, gfx10=-1)),
-   ("v_mad_legacy_u16",        dst(U16), src(U16, U16, U16), op(gfx8=0x1eb, gfx10=-1)),
-   ("v_mad_legacy_i16",        dst(U16), src(U16, U16, U16), op(gfx8=0x1ec, gfx10=-1)),
+   ("v_mad_legacy_f16",        dst(F16), src(mods(F16), mods(F16), mods(F16)), op(gfx8=0x1ea, gfx10=-1)),
+   ("v_mad_legacy_u16",        dst(U16), src(mods(U16), mods(U16), mods(U16)), op(gfx8=0x1eb, gfx10=-1)),
+   ("v_mad_legacy_i16",        dst(U16), src(mods(U16), mods(U16), mods(U16)), op(gfx8=0x1ec, gfx10=-1)),
    ("v_perm_b32",              dst(U32), src(U32, U32, U32), op(gfx8=0x1ed, gfx10=0x344, gfx11=0x244)),
-   ("v_fma_legacy_f16",        dst(F16), src(F16, F16, F16), op(gfx8=0x1ee, gfx10=-1), InstrClass.ValuFma),
-   ("v_div_fixup_legacy_f16",  dst(F16), src(F16, F16, F16), op(gfx8=0x1ef, gfx10=-1)),
+   ("v_fma_legacy_f16",        dst(F16), src(mods(F16), mods(F16), mods(F16)), op(gfx8=0x1ee, gfx10=-1), InstrClass.ValuFma),
+   ("v_div_fixup_legacy_f16",  dst(F16), src(mods(F16), mods(F16), mods(F16)), op(gfx8=0x1ef, gfx10=-1)),
    ("v_cvt_pkaccum_u8_f32",    dst(U32), src(F32, U32, U32), op(0x12c, gfx8=0x1f0, gfx10=-1)),
    ("v_mad_u32_u16",           dst(U32), src(U16, U16, U32), op(gfx9=0x1f1, gfx10=0x373, gfx11=0x259)),
    ("v_mad_i32_i16",           dst(U32), src(U16, U16, U32), op(gfx9=0x1f2, gfx10=0x375, gfx11=0x25a)),
@@ -1371,8 +1375,8 @@ VOP3 = {
    ("v_interp_p1ll_f16",       dst(F32), src(F32, M0), op(gfx8=0x274, gfx10=0x342, gfx11=-1)),
    ("v_interp_p1lv_f16",       dst(F32), src(F32, M0, F16), op(gfx8=0x275, gfx10=0x343, gfx11=-1)),
    ("v_interp_p2_legacy_f16",  dst(F16), src(F32, M0, F32), op(gfx8=0x276, gfx10=-1)),
-   ("v_interp_p2_f16",         dst(F16), src(F32, M0, F32), op(gfx9=0x277, gfx10=0x35a, gfx11=-1)),
-   ("v_interp_p2_hi_f16",      dst(F16), src(F32, M0, F32), op(gfx9=0x277, gfx10=0x35a, gfx11=-1)),
+   ("v_interp_p2_f16",         dst(mods(F16)), src(mods(F32), noMods(M0), mods(F32)), op(gfx9=0x277)),
+   ("v_interp_p2_hi_f16",      dst(mods(F16)), src(F32, M0, F32), op(gfx9=0x277)),
    ("v_ldexp_f32",             dst(F32), src(F32, U32), op(0x12b, gfx8=0x288, gfx10=0x362, gfx11=0x31c)),
    ("v_readlane_b32_e64",      dst(U32), src(U32, U32), op(gfx8=0x289, gfx10=0x360)),
    ("v_writelane_b32_e64",     dst(U32), src(U32, U32, U32), op(gfx8=0x28a, gfx10=0x361)),

--- a/src/amd/compiler/aco_optimizer.cpp	2025-05-31 22:57:26.003334290 +0200
+++ b/src/amd/compiler/aco_optimizer.cpp	2025-06-01 00:30:01.222104109 +0200
@@ -13,6 +13,16 @@
 #include <algorithm>
 #include <array>
 #include <vector>
+#include <cassert>
+#include <cstdint>
+#include <bitset>
+#include <memory>
+#include <map>
+#include <set>
+#include <utility>
+#include <limits>
+#include <cmath>
+#include <cstring>
 
 namespace aco {
 
@@ -99,7 +109,7 @@ struct ssa_info {
    };
    Instruction* parent_instr;
 
-   ssa_info() : label(0) {}
+   ssa_info() : label(0), val(0), parent_instr(nullptr) {}
 
    void add_label(Label new_label)
    {
@@ -347,6 +357,16 @@ struct opt_ctx {
    std::vector<uint16_t> uses;
 };
 
+static bool combine_bfi_b32(opt_ctx& ctx, aco_ptr<Instruction>& or_instr);
+static bool combine_bfe_b32(opt_ctx& ctx, aco_ptr<Instruction>& instr);
+static bool combine_bcnt_mbcnt(opt_ctx& ctx, aco_ptr<Instruction>& add_instr);
+static bool combine_sad_u8(opt_ctx& ctx, aco_ptr<Instruction>& add_instr);
+static bool apply_interp_extract(opt_ctx& ctx, aco_ptr<Instruction>& extract);
+static bool combine_alignbit_like(opt_ctx& ctx, aco_ptr<Instruction>& or_instr, aco_opcode target, unsigned granularity);
+static inline bool combine_alignbit_b32(opt_ctx& ctx, aco_ptr<Instruction>& instr);
+static inline bool combine_alignbyte_b32(opt_ctx& ctx, aco_ptr<Instruction>& instr);
+bool apply_load_extract(opt_ctx& ctx, aco_ptr<Instruction>& extract);
+
 bool
 can_use_VOP3(opt_ctx& ctx, const aco_ptr<Instruction>& instr)
 {
@@ -1992,10 +2012,24 @@ follow_operand(opt_ctx& ctx, Operand op,
 {
    if (!op.isTemp())
       return nullptr;
-   if (!ignore_uses && ctx.uses[op.tempId()] > 1)
+
+   /* Bounds check before accessing ctx arrays */
+   unsigned temp_id = op.tempId();
+   if (temp_id >= ctx.uses.size() || temp_id >= ctx.info.size())
+      return nullptr;
+
+   if (!ignore_uses && ctx.uses[temp_id] > 1)
       return nullptr;
 
-   Instruction* instr = ctx.info[op.tempId()].parent_instr;
+   Instruction* instr = ctx.info[temp_id].parent_instr;
+
+   /* CRITICAL FIX: Check for null before dereferencing */
+   if (!instr)
+      return nullptr;
+
+   /* Validate instruction structure before accessing */
+   if (instr->definitions.empty())
+      return nullptr;
 
    if (instr->definitions[0].getTemp() != op.getTemp())
       return nullptr;
@@ -2004,7 +2038,9 @@ follow_operand(opt_ctx& ctx, Operand op,
       unsigned idx =
          instr->definitions[1].isTemp() && instr->definitions[1].tempId() == op.tempId();
       assert(instr->definitions[idx].isTemp() && instr->definitions[idx].tempId() == op.tempId());
-      if (instr->definitions[!idx].isTemp() && ctx.uses[instr->definitions[!idx].tempId()])
+      if (instr->definitions[!idx].isTemp() &&
+          instr->definitions[!idx].tempId() < ctx.uses.size() &&
+          ctx.uses[instr->definitions[!idx].tempId()])
          return nullptr;
    }
 
@@ -2177,6 +2213,740 @@ combine_three_valu_op(opt_ctx& ctx, aco_
    return false;
 }
 
+/* combines a series of shifts and masks with an OR into a single v_alignbit_b32 or v_alignbyte_b32 instruction. */
+static bool
+combine_alignbit_like(opt_ctx& ctx, aco_ptr<Instruction>& or_instr, aco_opcode target, unsigned granularity)
+{
+   if (or_instr->opcode != aco_opcode::v_or_b32 || or_instr->operands.size() != 2 ||
+       granularity == 0 || ctx.program->gfx_level < GFX9) {
+      return false;
+   }
+
+   /* Match v_lshrrev or v_lshlrev with a constant shift amount. */
+   auto match_shift = [&](Operand op, unsigned& amount, Operand& src, bool& is_shr) -> bool
+   {
+      if (!op.isTemp())
+         return false;
+      Instruction* sh = ctx.info[op.tempId()].parent_instr;
+      if (!sh || ctx.uses[op.tempId()] != 1)
+         return false;
+
+      if (sh->opcode == aco_opcode::v_lshrrev_b32)
+         is_shr = true;
+      else if (sh->opcode == aco_opcode::v_lshlrev_b32)
+         is_shr = false;
+      else
+         return false;
+
+      if (!sh->operands[0].isConstant() || sh->operands[0].constantValue() >= 32)
+         return false;
+
+      amount = sh->operands[0].constantValue();
+      if (amount == 0 || amount >= 32 || amount % granularity)
+         return false;
+
+      src = sh->operands[1];
+      return true;
+   };
+
+   unsigned a_amt = 0, b_amt = 0;
+   Operand a_src, b_src;
+   bool a_shr = false, b_shr = false;
+
+   if (!match_shift(or_instr->operands[0], a_amt, a_src, a_shr) ||
+       !match_shift(or_instr->operands[1], b_amt, b_src, b_shr))
+      return false;
+
+   if (a_shr == b_shr) /* must have one left and one right shift */
+      return false;
+   if (a_amt + b_amt != 32) /* shifts must be complementary */
+      return false;
+
+   /* The ISA encoding is: dst = (src0 >> imm) | (src1 << (32-imm)) */
+   Operand src0 = a_shr ? a_src : b_src;   /* right-shifted source */
+   Operand src1 = a_shr ? b_src : a_src;   /* left-shifted source  */
+   unsigned imm = a_shr ? a_amt : b_amt; /* right-shift amount */
+
+   aco_ptr<Instruction> ali{create_instruction(target, Format::VOP3, 3, 1)};
+   ali->operands[0] = src0;
+   ali->operands[1] = src1;
+   ali->operands[2] = Operand::c32(imm / granularity);
+   ali->definitions[0] = or_instr->definitions[0];
+   ali->pass_flags = or_instr->pass_flags;
+
+   ctx.uses[or_instr->operands[0].tempId()]--;
+   ctx.uses[or_instr->operands[1].tempId()]--;
+   if (src0.isTemp())
+      ctx.uses[src0.tempId()]++;
+   if (src1.isTemp())
+      ctx.uses[src1.tempId()]++;
+
+   or_instr = std::move(ali);
+   ctx.info[or_instr->definitions[0].tempId()].parent_instr = or_instr.get();
+   return true;
+}
+
+static inline bool
+combine_alignbit_b32(opt_ctx& ctx, aco_ptr<Instruction>& instr)
+{
+   return combine_alignbit_like(ctx, instr, aco_opcode::v_alignbit_b32, 1);
+}
+
+static inline bool
+combine_alignbyte_b32(opt_ctx& ctx, aco_ptr<Instruction>& instr)
+{
+   return combine_alignbit_like(ctx, instr, aco_opcode::v_alignbyte_b32, 8);
+}
+
+static bool
+combine_bfi_b32(opt_ctx& ctx, aco_ptr<Instruction>& or_instr)
+{
+   /* V_BFI_B32 is GFX9+ and does not support input modifiers. */
+   if (or_instr->opcode != aco_opcode::v_or_b32 || or_instr->usesModifiers() ||
+       ctx.program->gfx_level < GFX9) {
+      return false;
+   }
+
+   /* Helper to match the pattern “src & literal_mask”. Accept both vector and scalar AND. */
+   auto match_and_side = [&](Operand in, Operand& src, uint32_t& lit) -> bool
+   {
+      if (!in.isTemp())
+         return false;
+
+      Instruction* and_i = ctx.info[in.tempId()].parent_instr;
+      /* AND must be single-use and modifier-free. */
+      if (!and_i ||
+          (and_i->opcode != aco_opcode::v_and_b32 && and_i->opcode != aco_opcode::s_and_b32) ||
+          and_i->usesModifiers() || ctx.uses[in.tempId()] != 1) {
+         return false;
+      }
+
+      for (unsigned op = 0; op < 2; ++op) {
+         if (and_i->operands[op].isConstant()) {
+            uint32_t imm = and_i->operands[op].constantValue();
+            /* Must be representable as an inline constant (no literal VOP3 operands on GFX9). */
+            if (!Operand::is_constant_representable(imm, 4))
+               return false;
+            lit = imm;
+            src = and_i->operands[op ^ 1];
+            return true;
+         }
+      }
+      return false;
+   };
+
+   Operand val0, val1;
+   uint32_t mask0 = 0, mask1 = 0;
+
+   if (!match_and_side(or_instr->operands[0], val0, mask0) ||
+       !match_and_side(or_instr->operands[1], val1, mask1)) {
+      return false;
+   }
+
+   /* Masks must be perfect complements. */
+   if ((mask0 | mask1) != 0xffffffffu || (mask0 & mask1) != 0)
+      return false;
+
+   /* Vega ISA v_bfi_b32: D = (S1 & S0) | (S2 & ~S0) */
+   Operand base, ins;
+   uint32_t mask;
+
+   if (mask1 == ~mask0) {
+      ins = val0;
+      base = val1;
+      mask = mask0;
+   } else if (mask0 == ~mask1) {
+      ins = val1;
+      base = val0;
+      mask = mask1;
+   } else {
+      return false;
+   }
+
+   Operand ops[3] = { Operand::c32(mask), ins, base };
+   if (!check_vop3_operands(ctx, 3, ops))
+      return false;
+
+   aco_ptr<Instruction> bfi{create_instruction(aco_opcode::v_bfi_b32, Format::VOP3, 3, 1)};
+   bfi->operands[0] = ops[0];
+   bfi->operands[1] = ops[1];
+   bfi->operands[2] = ops[2];
+   bfi->definitions[0] = or_instr->definitions[0];
+   bfi->pass_flags = or_instr->pass_flags;
+
+   /* Fix uses */
+   ctx.uses[or_instr->operands[0].tempId()]--;
+   ctx.uses[or_instr->operands[1].tempId()]--;
+   if (base.isTemp())
+      ctx.uses[base.tempId()]++;
+   if (ins.isTemp())
+      ctx.uses[ins.tempId()]++;
+
+   or_instr = std::move(bfi);
+   ctx.info[or_instr->definitions[0].tempId()].parent_instr = or_instr.get();
+   return true;
+}
+
+static bool
+combine_bfe_b32(opt_ctx& ctx, aco_ptr<Instruction>& instr)
+{
+   if (ctx.program->gfx_level < GFX8)
+      return false;
+
+   Operand src_val;
+   uint32_t offset = 0, width = 0;
+   bool is_signed = false;
+   unsigned dec_temp_id = 0;
+   bool need_dec_use = false;
+
+   /* Pattern 1: ((src >> (l|a)shr c) & mask), mask contiguous from bit 0 */
+   if (instr->opcode == aco_opcode::v_and_b32 && instr->operands[1].isConstant()) {
+      uint32_t mask = instr->operands[1].constantValue();
+      width = util_bitcount(mask);
+      /* mask must be (2^width - 1), width <= 31 */
+      if (!mask || (mask & (mask + 1u)) != 0u || width > 31)
+         return false;
+
+      if (!instr->operands[0].isTemp() || ctx.uses[instr->operands[0].tempId()] != 1)
+         return false;
+
+      Instruction* sh = ctx.info[instr->operands[0].tempId()].parent_instr;
+      if (!sh || !(sh->opcode == aco_opcode::v_lshrrev_b32 || sh->opcode == aco_opcode::v_ashrrev_i32))
+         return false;
+      if (!sh->operands[0].isConstant() || sh->operands[0].constantValue() >= 32)
+         return false;
+
+      offset = sh->operands[0].constantValue();
+      if (offset + width > 32)
+         return false;
+
+      /* AND zeroes upper bits, so the result is always zero-extended. */
+      is_signed = false;
+      src_val = sh->operands[1];
+      dec_temp_id = instr->operands[0].tempId();
+      need_dec_use = true;
+   }
+   /* Pattern 2: ((src << c1) >> (l|a)shr c2), with c2 >= c1 */
+   else if ((instr->opcode == aco_opcode::v_lshrrev_b32 || instr->opcode == aco_opcode::v_ashrrev_i32) &&
+            instr->operands[0].isConstant() && instr->operands[1].isTemp() &&
+            ctx.uses[ctx.info[instr->definitions[0].tempId()].temp.id()] == 1) {
+      uint32_t c2 = instr->operands[0].constantValue();
+
+      Instruction* lshl = ctx.info[instr->operands[1].tempId()].parent_instr;
+      if (!lshl || lshl->opcode != aco_opcode::v_lshlrev_b32 || !lshl->operands[0].isConstant())
+         return false;
+
+      uint32_t c1 = lshl->operands[0].constantValue();
+      if (c1 >= 32 || c2 >= 32 || c2 < c1)
+         return false;
+
+      offset = c1;
+      width = 32 - c2;
+      if (width == 0 || width > 31 || offset + width > 32)
+         return false;
+
+      /* For arithmetic right shifts, sign extension is intentional here. */
+      is_signed = (instr->opcode == aco_opcode::v_ashrrev_i32);
+      src_val = lshl->operands[1];
+      dec_temp_id = instr->operands[1].tempId();
+      need_dec_use = true;
+   }
+   /* Pattern 3: ((src & ((2^width - 1) << offset)) >> (l|a)shr offset) */
+   else if ((instr->opcode == aco_opcode::v_lshrrev_b32 || instr->opcode == aco_opcode::v_ashrrev_i32) &&
+            instr->operands[0].isConstant() && instr->operands[1].isTemp() &&
+            ctx.uses[ctx.info[instr->definitions[0].tempId()].temp.id()] == 1) {
+
+      Instruction* andi = ctx.info[instr->operands[1].tempId()].parent_instr;
+      if (!andi || andi->opcode != aco_opcode::v_and_b32)
+         return false;
+
+      uint32_t mask = 0;
+      Operand src;
+      if (andi->operands[0].isConstant()) {
+         mask = andi->operands[0].constantValue();
+         src = andi->operands[1];
+      } else if (andi->operands[1].isConstant()) {
+         mask = andi->operands[1].constantValue();
+         src = andi->operands[0];
+      } else {
+         return false;
+      }
+
+      if (!mask)
+         return false;
+
+      const uint32_t c2 = instr->operands[0].constantValue();
+      if (c2 >= 32)
+         return false;
+
+      /* offset = ctz(mask), must equal c2 */
+      unsigned offset_m = 0;
+#if defined(__clang__) || defined(__GNUC__)
+      offset_m = (unsigned)__builtin_ctz(mask);
+#else
+      {
+         uint32_t tmp = mask;
+         while ((tmp & 1u) == 0u) {
+            ++offset_m;
+            tmp >>= 1;
+         }
+      }
+#endif
+      if (offset_m != c2)
+         return false;
+
+      const uint32_t shifted = mask >> offset_m;
+      /* shifted must be (2^width - 1) */
+      if (!shifted || ((shifted & (shifted + 1u)) != 0u))
+         return false;
+
+      width = util_bitcount(shifted);
+      if (width == 0 || width > 31 || offset_m + width > 32)
+         return false;
+
+      /* AND clears any sign-extended bits; extraction is zero-extended. */
+      is_signed = false;
+      offset = offset_m;
+      src_val = src;
+      dec_temp_id = instr->operands[1].tempId();
+      need_dec_use = true;
+   }
+   else {
+      return false;
+   }
+
+   aco_opcode bfe_op = is_signed ? aco_opcode::v_bfe_i32 : aco_opcode::v_bfe_u32;
+   const unsigned enc_width = width - 1u;
+
+   Operand ops[3] = { src_val, Operand::c32(offset), Operand::c32(enc_width) };
+   if (!check_vop3_operands(ctx, 3, ops))
+      return false;
+
+   aco_ptr<Instruction> bfe{create_instruction(bfe_op, Format::VOP3, 3, 1)};
+   bfe->operands[0] = ops[0];
+   bfe->operands[1] = ops[1];
+   bfe->operands[2] = ops[2];
+   bfe->definitions[0] = instr->definitions[0];
+   bfe->pass_flags = instr->pass_flags;
+
+   if (need_dec_use)
+      ctx.uses[dec_temp_id]--;
+   if (src_val.isTemp())
+      ctx.uses[src_val.tempId()]++;
+
+   instr = std::move(bfe);
+   ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
+   return true;
+}
+
+static bool
+combine_bcnt_mbcnt(opt_ctx& ctx, aco_ptr<Instruction>& add_instr)
+{
+   const bool is_add =
+      add_instr->opcode == aco_opcode::v_add_u32 ||
+      add_instr->opcode == aco_opcode::v_add_co_u32 ||
+      add_instr->opcode == aco_opcode::v_add_co_u32_e64;
+
+   if (!is_add || add_instr->usesModifiers() || ctx.program->gfx_level < GFX9)
+      return false;
+
+   /* If this is v_add_co_* and the carry-out is used, we cannot replace it. */
+   if ((add_instr->opcode == aco_opcode::v_add_co_u32 ||
+        add_instr->opcode == aco_opcode::v_add_co_u32_e64) &&
+       ctx.uses[add_instr->definitions[1].tempId()] > 0)
+      return false;
+
+   int bcnt_idx = -1;
+   Instruction* bcnt = nullptr;
+
+   for (unsigned i = 0; i < 2; ++i) {
+      if (!add_instr->operands[i].isTemp())
+         continue;
+
+      unsigned tmp_id = add_instr->operands[i].tempId();
+      if (ctx.uses[tmp_id] != 1)
+         continue;
+
+      Instruction* cand = ctx.info[tmp_id].parent_instr;
+      if (!cand || cand->opcode != aco_opcode::v_bcnt_u32_b32)
+         continue;
+
+      if (!cand->operands[1].isConstant() || !cand->operands[1].constantEquals(0))
+         continue;
+
+      if (cand->operands[0].isFixed() &&
+          (cand->operands[0].physReg() == exec || cand->operands[0].physReg() == exec_hi)) {
+         bcnt_idx = i;
+         bcnt = cand;
+         break;
+      }
+   }
+
+   if (!bcnt)
+      return false;
+
+   const bool lo_segment = bcnt->operands[0].physReg() == exec;
+   const aco_opcode mbcnt_op = lo_segment ? aco_opcode::v_mbcnt_lo_u32_b32
+                                          : aco_opcode::v_mbcnt_hi_u32_b32;
+   Operand carry_in = add_instr->operands[1u ^ bcnt_idx];
+
+   aco_ptr<Instruction> mbcnt{create_instruction(mbcnt_op, Format::VOP3, 2, 1)};
+   mbcnt->operands[0] = bcnt->operands[0];
+   mbcnt->operands[1] = carry_in;
+   mbcnt->definitions[0] = add_instr->definitions[0];
+   mbcnt->pass_flags = add_instr->pass_flags;
+
+   ctx.uses[add_instr->operands[bcnt_idx].tempId()]--;
+   if (carry_in.isTemp())
+      ctx.uses[carry_in.tempId()]++;
+
+   add_instr = std::move(mbcnt);
+   ctx.info[add_instr->definitions[0].tempId()].parent_instr = add_instr.get();
+   return true;
+}
+
+static bool
+combine_sad_u8(opt_ctx& ctx, aco_ptr<Instruction>& add_instr)
+{
+   if (add_instr->opcode != aco_opcode::v_add_u32 ||
+       add_instr->usesModifiers() ||
+       ctx.program->gfx_level < GFX9)
+      return false;
+
+   int sad_idx = -1;
+   Instruction* sad = nullptr;
+
+   for (unsigned i = 0; i < 2; ++i) {
+      if (!add_instr->operands[i].isTemp())
+         continue;
+
+      unsigned tmp_id = add_instr->operands[i].tempId();
+      if (ctx.uses[tmp_id] != 1)
+         continue;
+
+      Instruction* cand = ctx.info[tmp_id].parent_instr;
+      if (!cand ||
+          !((cand->opcode == aco_opcode::v_sad_u8) ||
+            (cand->opcode == aco_opcode::v_sad_hi_u8)))
+         continue;
+
+      if (!cand->operands[2].isConstant() ||
+          !cand->operands[2].constantEquals(0))
+         continue;
+
+      sad_idx = i;
+      sad = cand;
+      break;
+   }
+
+   if (!sad)
+      return false;
+
+   aco_ptr<Instruction> fused{
+      create_instruction(sad->opcode, Format::VOP3, 3, 1)};
+
+   fused->operands[0] = sad->operands[0];
+   fused->operands[1] = sad->operands[1];
+   fused->operands[2] = add_instr->operands[1u ^ sad_idx];
+   fused->definitions[0] = add_instr->definitions[0];
+   fused->pass_flags = add_instr->pass_flags;
+
+   ctx.uses[add_instr->operands[sad_idx].tempId()]--;
+
+   add_instr = std::move(fused);
+   ctx.info[add_instr->definitions[0].tempId()].parent_instr = add_instr.get();
+   return true;
+}
+
+/**
+ * GFX9/Vega Architecture Constants
+ *
+ * Vega 64 Specifications:
+ * - 4096 total VGPRs (16 banks × 256 VGPRs per bank)
+ * - 256 VGPRs per SIMD
+ * - Wave size: 64 threads
+ * - Occupancy cliff: >128 VGPRs/wave causes sharp occupancy drop
+ * - VOP3 opsel: No execution penalty on GFX9
+ */
+#if __cplusplus >= 202002L
+consteval
+#else
+constexpr
+#endif
+uint32_t HIGH_16_INDEX()
+{
+   return 1;
+}
+
+#if __cplusplus >= 202002L
+consteval
+#else
+constexpr
+#endif
+uint32_t BITS_16()
+{
+   return 16;
+}
+
+#if __cplusplus >= 202002L
+consteval
+#else
+constexpr
+#endif
+uint32_t SIGN_EXT_UNSIGNED()
+{
+   return 0;
+}
+
+/**
+ * Vega 64 Tuning Parameters
+ *
+ * Rationale for MAX_USES_VEGA = 2:
+ * - Uses=1: Pure win (eliminates extract, no duplication)
+ * - Uses=2: Neutral on VGPRs, eliminates extract instruction
+ * - Uses=3+: Diminishing returns, instruction cache pressure
+ *
+ * The optimization replaces p_extract (compiles to shift/pack or SDWA)
+ * with direct opsel usage (zero-cost hardware feature). For multi-use cases,
+ * we clone v_interp_p2_f16 (64-bit VOP3, ~8 bytes).
+ *
+ * Empirical testing on Vega 64: best results with ≤2 uses.
+ */
+constexpr unsigned MAX_USES_VEGA = 2;
+
+/**
+ * Vega 64 Occupancy Threshold
+ *
+ * Vega 64 wave occupancy per SIMD:
+ * - 0-64 VGPRs: 10 waves (full occupancy)
+ * - 65-84 VGPRs: 8 waves
+ * - 85-128 VGPRs: 6 waves
+ * - 129-168 VGPRs: 4 waves (sharp drop)
+ * - 169-256 VGPRs: 2 waves
+ *
+ * At 128 VGPRs we're at the edge of the 6-wave tier. Going higher
+ * risks dropping to 4 waves. During optimization, we cannot precisely
+ * predict final register allocation, so we use peak demand as proxy.
+ *
+ * Conservative strategy: Avoid optimization when near occupancy boundaries
+ * to prevent cascading pressure increases.
+ */
+constexpr int16_t HIGH_VGPR_PRESSURE = 128;
+
+/**
+ * Optimization: Fuse v_interp_p2_f16 with high-16-bit extract
+ *
+ * Pattern Recognition:
+ *   %interp_result = v_interp_p2_f16 ...           // 32-bit result (fp16 pair)
+ *   %high16 = p_extract %interp_result, 1, 16, 0   // Extract high 16 bits
+ *
+ * Transformation:
+ *   %high16 = v_interp_p2_f16 ... opsel[3]=1       // Directly produce high 16
+ *
+ * ISA Details (GFX9 VOP3 Encoding):
+ *   opsel[3] controls destination operand selection:
+ *     - opsel[3]=0: Write result to low 16 bits of dest VGPR
+ *     - opsel[3]=1: Write result to high 16 bits of dest VGPR
+ *   No execution penalty; purely an encoding bit.
+ *
+ * Benefits:
+ *   1. Eliminates p_extract instruction (saves 4-8 bytes)
+ *   2. Reduces register pressure (extract result merged into interp)
+ *   3. May enable better register allocation (RA can pack VGPR halves)
+ *   4. Improves fragment shader performance (common use case)
+ *
+ * Multi-Use Handling:
+ *   When interp result has other uses, we clone the instruction to avoid
+ *   breaking other users. This trades instruction count for extract elimination.
+ *   Threshold tuned for Vega 64 characteristics.
+ *
+ * Safety:
+ *   - Validates all array accesses (defends against corrupted SSA)
+ *   - Checks ISA constraints (no conflicting modifiers)
+ *   - Maintains SSA invariants (proper def-use chain updates)
+ *   - Graceful degradation (returns false vs asserting/crashing)
+ *
+ * @param ctx Optimizer context with SSA info and use counts
+ * @param extract Instruction pointer (may be modified or replaced)
+ * @return true if optimization was applied, false otherwise
+ */
+static bool
+apply_interp_extract(opt_ctx& ctx, aco_ptr<Instruction>& extract)
+{
+   /* ========================================================================
+    * STAGE 1: Architecture and Pattern Validation
+    * ======================================================================== */
+
+   if (ctx.program->gfx_level != GFX9) {
+      return false;
+   }
+
+   if (extract->opcode != aco_opcode::p_extract) {
+      return false;
+   }
+
+   if (extract->operands.size() != 4 || !extract->operands[0].isTemp()) {
+      return false;
+   }
+
+   if (!extract->operands[1].constantEquals(HIGH_16_INDEX()) ||
+       !extract->operands[2].constantEquals(BITS_16()) ||
+       !extract->operands[3].constantEquals(SIGN_EXT_UNSIGNED())) {
+      return false;
+   }
+
+   if (extract->definitions.empty() || !extract->definitions[0].isTemp()) {
+      return false;
+   }
+
+   /* ========================================================================
+    * STAGE 2: SSA Graph Traversal and Validation
+    * ======================================================================== */
+
+   const unsigned src_id = extract->operands[0].tempId();
+   if (src_id >= ctx.info.size() || src_id >= ctx.uses.size()) {
+      return false;
+   }
+
+   /**
+    * CRITICAL: Validate parent_instr is not garbage before dereferencing.
+    *
+    * If ctx.info was resized without proper initialization, parent_instr
+    * might be a garbage pointer. Check for nullptr before access.
+    */
+   Instruction* interp = ctx.info[src_id].parent_instr;
+   if (!interp) {
+      return false;  // Uninitialized or null parent
+   }
+
+   if (interp->opcode != aco_opcode::v_interp_p2_f16) {
+      return false;
+   }
+
+   if (interp->definitions.empty() || !interp->definitions[0].isTemp()) {
+      return false;
+   }
+
+   const unsigned extract_def_id = extract->definitions[0].tempId();
+   if (extract_def_id >= ctx.info.size() || extract_def_id >= ctx.uses.size()) {
+      return false;
+   }
+
+   const unsigned interp_def_id = interp->definitions[0].tempId();
+   if (interp_def_id >= ctx.info.size() || interp_def_id >= ctx.uses.size()) {
+      return false;
+   }
+
+   /**
+    * DEFENSIVE: Ensure interp_def_id has initialized parent_instr.
+    * This catches cases where interp was created in an earlier pass
+    * when ctx.info was smaller and wasn't properly initialized.
+    */
+   if (!ctx.info[interp_def_id].parent_instr) {
+      /* Re-initialize this entry */
+      ctx.info[interp_def_id].parent_instr = interp;
+      ctx.info[interp_def_id].label = 0;
+   }
+
+   /* ========================================================================
+    * STAGE 3: ISA Constraint Validation
+    * ======================================================================== */
+
+   if (interp->valu().clamp || interp->valu().omod || interp->valu().opsel[3]) {
+      return false;
+   }
+
+   /* ========================================================================
+    * STAGE 4: Use Analysis and Optimization Strategy Selection
+    * ======================================================================== */
+
+   const uint16_t num_uses = ctx.uses[src_id];
+
+   if (num_uses == 1) {
+      /* Single-use path: Modify instruction in-place */
+
+      interp->valu().opsel[3] = true;
+
+      /* Swap definitions */
+      std::swap(interp->definitions[0], extract->definitions[0]);
+
+      /**
+       * Update SSA metadata following apply_load_extract pattern.
+       *
+       * After swap:
+       * - interp->definitions[0] has extract_def_id (live temp, what we produce)
+       * - extract->definitions[0] has interp_def_id (dead temp, orphaned)
+       */
+
+      /* Validate temp IDs after swap (paranoid, should be same as pre-validated) */
+      const unsigned swapped_live_id = interp->definitions[0].tempId();
+      const unsigned swapped_dead_id = extract->definitions[0].tempId();
+
+      if (swapped_live_id >= ctx.info.size() || swapped_dead_id >= ctx.info.size() ||
+          swapped_live_id >= ctx.uses.size() || swapped_dead_id >= ctx.uses.size()) {
+         /* Swap back and abort */
+         std::swap(interp->definitions[0], extract->definitions[0]);
+         return false;
+      }
+
+      /* Mark orphaned temp as dead */
+      ctx.uses[swapped_dead_id] = 0;
+
+      /* Update parent_instr for live temp */
+      ctx.info[swapped_live_id].parent_instr = interp;
+      ctx.info[swapped_live_id].label = 0;
+
+      /* Update parent_instr for dead temp (consistency for DCE) */
+      ctx.info[swapped_dead_id].parent_instr = extract.get();
+
+      return true;
+   }
+
+   /* ========================================================================
+    * STAGE 5: Multi-Use Path - Heuristic Analysis
+    * ======================================================================== */
+
+   const int16_t vgpr_demand = std::max<int16_t>(0, ctx.program->max_reg_demand.vgpr);
+
+   if (num_uses > MAX_USES_VEGA || vgpr_demand > HIGH_VGPR_PRESSURE) {
+      return false;
+   }
+
+   /* ========================================================================
+    * STAGE 6: Instruction Cloning and SSA Update
+    * ======================================================================== */
+
+   aco_ptr<Instruction> cloned_interp{
+      create_instruction(interp->opcode, interp->format, interp->operands.size(), 1)};
+
+   if (!cloned_interp) {
+      return false;
+   }
+
+   std::copy(interp->operands.begin(), interp->operands.end(), cloned_interp->operands.begin());
+   cloned_interp->pass_flags = interp->pass_flags;
+
+   /* Copy VALU modifiers field-by-field */
+   cloned_interp->valu().neg = interp->valu().neg;
+   cloned_interp->valu().abs = interp->valu().abs;
+   cloned_interp->valu().opsel = interp->valu().opsel;
+   cloned_interp->valu().omod = interp->valu().omod;
+   cloned_interp->valu().clamp = interp->valu().clamp;
+
+   /* Apply optimization to clone */
+   cloned_interp->valu().opsel[3] = true;
+   cloned_interp->definitions[0] = extract->definitions[0];
+
+   /* Update SSA metadata */
+   ctx.info[extract_def_id].parent_instr = cloned_interp.get();
+   ctx.uses[src_id]--;
+
+   /* Replace extract with optimized clone */
+   extract = std::move(cloned_interp);
+
+   return true;
+}
+
 /* creates v_lshl_add_u32, v_lshl_or_b32 or v_and_or_b32 */
 bool
 combine_add_or_then_and_lshl(opt_ctx& ctx, aco_ptr<Instruction>& instr)
@@ -3287,14 +4057,19 @@ propagate_swizzles(VALU_instruction* ins
    }
 }
 
+
 void
 combine_vop3p(opt_ctx& ctx, aco_ptr<Instruction>& instr)
 {
+   if (!instr || !instr->isVOP3P()) [[unlikely]] {
+      return;
+   }
+
    VALU_instruction* vop3p = &instr->valu();
 
-   /* apply clamp */
-   if (instr->opcode == aco_opcode::v_pk_mul_f16 && instr->operands[1].constantEquals(0x3C00) &&
-       vop3p->clamp && instr->operands[0].isTemp() && ctx.uses[instr->operands[0].tempId()] == 1 &&
+   /* New optimization from patch: Propagate clamp through a multiply-by-zero FMA */
+   if (instr->opcode == aco_opcode::v_pk_fma_f16 && vop3p->clamp &&
+       instr->operands[2].isConstant() && instr->operands[2].constantValue() == 0 &&
        !vop3p->opsel_lo[1] && !vop3p->opsel_hi[1]) {
 
       Instruction* op_instr = ctx.info[instr->operands[0].tempId()].parent_instr;
@@ -3306,173 +4081,192 @@ combine_vop3p(opt_ctx& ctx, aco_ptr<Inst
          op_instr->valu().clamp = true;
          propagate_swizzles(&op_instr->valu(), vop3p->opsel_lo[0], vop3p->opsel_hi[0]);
          instr->definitions[0].swapTemp(op_instr->definitions[0]);
-         ctx.info[op_instr->definitions[0].tempId()].parent_instr = op_instr;
          ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
-         ctx.uses[instr->definitions[0].tempId()]--;
+         ctx.info[op_instr->definitions[0].tempId()].parent_instr = op_instr;
+         ctx.uses[op_instr->definitions[0].tempId()]++;
+         instr.reset(create_instruction(aco_opcode::p_parallelcopy, Format::PSEUDO, 1, 1));
+         instr->operands[0] = Operand(op_instr->definitions[0].getTemp());
+         instr->definitions[0].setTemp(op_instr->definitions[0].getTemp());
          return;
       }
    }
 
-   /* check for fneg modifiers */
-   for (unsigned i = 0; i < instr->operands.size(); i++) {
-      if (!can_use_input_modifiers(ctx.program->gfx_level, instr->opcode, i))
-         continue;
-      Operand& op = instr->operands[i];
-      if (!op.isTemp())
+   /*
+    * === Pass 1: Clamp Propagation ===
+    * Folds `clamp(mul(X, 1.0))` into `clamp(X)`. The mul by 1.0 becomes a p_parallelcopy.
+    */
+   if (instr->opcode == aco_opcode::v_pk_mul_f16 && instr->valu().clamp &&
+       instr->operands.size() >= 2 && instr->operands[1].isConstant() &&
+       instr->operands[1].constantValue() == 0x3C003C00 /* packed {1.0, 1.0} */ &&
+       instr->operands[0].isTemp() && ctx.uses[instr->operands[0].tempId()] == 1) {
+
+      Instruction* producer = follow_operand(ctx, instr->operands[0], true);
+      if (producer && producer->isVOP3P() && !producer->valu().clamp &&
+          instr_info.alu_opcode_infos[(int)producer->opcode].output_modifiers) {
+
+         producer->valu().clamp = true;
+         propagate_swizzles(&producer->valu(), instr->valu().opsel_lo[0], instr->valu().opsel_hi[0]);
+
+         aco_ptr<Instruction> pc{create_instruction(aco_opcode::p_parallelcopy, Format::PSEUDO, 1, 1)};
+         pc->operands[0] = Operand(producer->definitions[0].getTemp());
+         pc->definitions[0] = instr->definitions[0];
+         pc->pass_flags = instr->pass_flags;
+
+         ctx.info[instr->definitions[0].tempId()].parent_instr = pc.get();
+         ctx.info[instr->definitions[0].tempId()].set_temp(pc->operands[0].getTemp());
+         ctx.uses[producer->definitions[0].tempId()]++;
+         instr = std::move(pc);
+         return;
+      }
+   }
+
+   /*
+    * === Pass 2: FNEG Folding ===
+    * Folds a `v_pk_mul_f16(X, -1.0)` into the input modifiers of this instruction.
+    */
+   VALU_instruction* v = &instr->valu();
+   for (unsigned i = 0; i < instr->operands.size(); ++i) {
+      if (!can_use_input_modifiers(ctx.program->gfx_level, instr->opcode, i)) {
          continue;
+      }
 
-      ssa_info& info = ctx.info[op.tempId()];
-      if (info.parent_instr->opcode == aco_opcode::v_pk_mul_f16 &&
-          (info.parent_instr->operands[0].constantEquals(0x3C00) ||
-           info.parent_instr->operands[1].constantEquals(0x3C00) ||
-           info.parent_instr->operands[0].constantEquals(0xBC00) ||
-           info.parent_instr->operands[1].constantEquals(0xBC00))) {
+      Instruction* neg = follow_operand(ctx, instr->operands[i], true);
+      if (!neg || neg->opcode != aco_opcode::v_pk_mul_f16 || neg->valu().clamp) {
+         continue;
+      }
 
-         VALU_instruction* fneg = &info.parent_instr->valu();
+      unsigned const_idx = neg->operands[0].constantEquals(0xBC00BC00) ? 0 : (neg->operands[1].constantEquals(0xBC00BC00) ? 1 : 2);
+      if (const_idx > 1) {
+         continue;
+      }
 
-         unsigned fneg_src =
-            fneg->operands[0].constantEquals(0x3C00) || fneg->operands[0].constantEquals(0xBC00);
+      unsigned src_idx = 1 - const_idx;
+      VALU_instruction& nv = neg->valu();
 
-         if (fneg->opsel_lo[1 - fneg_src] || fneg->opsel_hi[1 - fneg_src])
-            continue;
+      if (nv.opsel_lo[const_idx] || nv.opsel_hi[const_idx] || nv.neg_lo[const_idx] || nv.neg_hi[const_idx]) {
+         continue;
+      }
 
-         Operand ops[3];
-         for (unsigned j = 0; j < instr->operands.size(); j++)
-            ops[j] = instr->operands[j];
-         ops[i] = fneg->operands[fneg_src];
-         if (!check_vop3_operands(ctx, instr->operands.size(), ops))
-            continue;
+      if (neg->operands[src_idx].isLiteral()) {
+         continue;
+      }
 
-         if (fneg->clamp)
-            continue;
-         instr->operands[i] = fneg->operands[fneg_src];
+      bool sel_lo = v->opsel_lo[i];
+      bool sel_hi = v->opsel_hi[i];
+      v->neg_lo[i] ^= sel_lo ? nv.neg_hi[src_idx] ^ 1 : nv.neg_lo[src_idx] ^ 1;
+      v->neg_hi[i] ^= sel_hi ? nv.neg_hi[src_idx] ^ 1 : nv.neg_lo[src_idx] ^ 1;
+      v->opsel_lo[i] = sel_lo ? nv.opsel_hi[src_idx] : nv.opsel_lo[src_idx];
+      v->opsel_hi[i] = sel_hi ? nv.opsel_hi[src_idx] : nv.opsel_lo[src_idx];
 
-         /* opsel_lo/hi is either 0 or 1:
-          * if 0 - pick selection from fneg->lo
-          * if 1 - pick selection from fneg->hi
-          */
-         bool opsel_lo = vop3p->opsel_lo[i];
-         bool opsel_hi = vop3p->opsel_hi[i];
-         bool neg_lo = fneg->neg_lo[0] ^ fneg->neg_lo[1];
-         bool neg_hi = fneg->neg_hi[0] ^ fneg->neg_hi[1];
-         bool neg_const = fneg->operands[1 - fneg_src].constantEquals(0xBC00);
-         /* Avoid ternary xor as it causes CI fails that can't be reproduced on other systems. */
-         neg_lo ^= neg_const;
-         neg_hi ^= neg_const;
-         vop3p->neg_lo[i] ^= opsel_lo ? neg_hi : neg_lo;
-         vop3p->neg_hi[i] ^= opsel_hi ? neg_hi : neg_lo;
-         vop3p->opsel_lo[i] ^= opsel_lo ? !fneg->opsel_hi[fneg_src] : fneg->opsel_lo[fneg_src];
-         vop3p->opsel_hi[i] ^= opsel_hi ? !fneg->opsel_hi[fneg_src] : fneg->opsel_lo[fneg_src];
-
-         if (--ctx.uses[fneg->definitions[0].tempId()])
-            ctx.uses[fneg->operands[fneg_src].tempId()]++;
-      }
+      instr->operands[i] = copy_operand(ctx, neg->operands[src_idx]);
+      decrease_uses(ctx, neg);
    }
 
-   if (instr->opcode == aco_opcode::v_pk_add_f16 || instr->opcode == aco_opcode::v_pk_add_u16) {
-      bool fadd = instr->opcode == aco_opcode::v_pk_add_f16;
-      if (fadd && instr->definitions[0].isPrecise())
-         return;
-      if (!fadd && instr->valu().clamp)
-         return;
+   /*
+    * === Pass 3: FMA/MAD Formation ===
+    * Fuses `v_pk_add(v_pk_mul(A, B), C)` into `v_pk_fma(A, B, C)`.
+    */
+   const bool is_fadd = instr->opcode == aco_opcode::v_pk_add_f16;
+   const bool is_uadd = instr->opcode == aco_opcode::v_pk_add_u16;
+   if ((!is_fadd && !is_uadd) || (is_fadd && instr->definitions[0].isPrecise())) {
+      return;
+   }
 
-      Instruction* mul_instr = nullptr;
-      unsigned add_op_idx = 0;
-      bitarray8 mul_neg_lo = 0, mul_neg_hi = 0, mul_opsel_lo = 0, mul_opsel_hi = 0;
-      uint32_t uses = UINT32_MAX;
+   Instruction* mul_instr = nullptr;
+   unsigned add_op_idx = 0;
+   uint32_t uses = UINT32_MAX;
 
-      /* find the 'best' mul instruction to combine with the add */
-      for (unsigned i = 0; i < 2; i++) {
-         Instruction* op_instr = follow_operand(ctx, instr->operands[i], true);
-         if (!op_instr)
-            continue;
+   for (unsigned i = 0; i < 2; i++) {
+      Instruction* op_instr = follow_operand(ctx, instr->operands[i]);
+      if (!op_instr ||
+          (is_fadd && op_instr->opcode != aco_opcode::v_pk_mul_f16) ||
+          (is_uadd && op_instr->opcode != aco_opcode::v_pk_mul_lo_u16))
+         continue;
 
-         if (op_instr->isVOP3P()) {
-            if (fadd) {
-               if (op_instr->opcode != aco_opcode::v_pk_mul_f16 ||
-                   op_instr->definitions[0].isPrecise())
-                  continue;
-            } else {
-               if (op_instr->opcode != aco_opcode::v_pk_mul_lo_u16)
-                  continue;
-            }
+      /* no clamp allowed between mul and add - check moved earlier from patch */
+      if (op_instr->valu().clamp)
+         continue;
 
-            /* no clamp allowed between mul and add */
-            if (op_instr->valu().clamp)
-               continue;
+      if (is_fadd && op_instr->valu().omod)
+         continue;
 
-            Operand op[3] = {op_instr->operands[0], op_instr->operands[1], instr->operands[1 - i]};
-            if (ctx.uses[instr->operands[i].tempId()] >= uses || !check_vop3_operands(ctx, 3, op))
-               continue;
+      Operand op[3] = {op_instr->operands[0], op_instr->operands[1], instr->operands[1 - i]};
+      if (ctx.uses[instr->operands[i].tempId()] >= uses || !check_vop3_operands(ctx, 3, op))
+         continue;
 
-            mul_instr = op_instr;
-            add_op_idx = 1 - i;
-            uses = ctx.uses[instr->operands[i].tempId()];
-            mul_neg_lo = mul_instr->valu().neg_lo;
-            mul_neg_hi = mul_instr->valu().neg_hi;
-            mul_opsel_lo = mul_instr->valu().opsel_lo;
-            mul_opsel_hi = mul_instr->valu().opsel_hi;
-         } else if (instr->operands[i].bytes() == 2) {
-            if ((fadd && (op_instr->opcode != aco_opcode::v_mul_f16 ||
-                          op_instr->definitions[0].isPrecise())) ||
-                (!fadd && op_instr->opcode != aco_opcode::v_mul_lo_u16 &&
-                 op_instr->opcode != aco_opcode::v_mul_lo_u16_e64))
-               continue;
+      mul_instr = op_instr;
+      add_op_idx = 1 - i;
+      uses = ctx.uses[instr->operands[i].tempId()];
+   }
 
-            if (op_instr->valu().clamp || op_instr->valu().omod || op_instr->valu().abs)
-               continue;
+   if (!mul_instr) {
+      return;
+   }
 
-            if (op_instr->isDPP() || (op_instr->isSDWA() && (op_instr->sdwa().sel[0].size() < 2 ||
-                                                             op_instr->sdwa().sel[1].size() < 2)))
-               continue;
+   aco_opcode mad_op = is_fadd ? aco_opcode::v_pk_fma_f16 : aco_opcode::v_pk_mad_u16;
+   aco_ptr<Instruction> mad{create_instruction(mad_op, Format::VOP3P, 3, 1)};
+   mad->operands[0] = copy_operand(ctx, mul_instr->operands[0]);
+   mad->operands[1] = copy_operand(ctx, mul_instr->operands[1]);
+   mad->operands[2] = instr->operands[add_op_idx];
+   mad->definitions[0] = instr->definitions[0];
+   mad->pass_flags = instr->pass_flags;
+   mad->definitions[0].setPrecise(instr->definitions[0].isPrecise() || mul_instr->definitions[0].isPrecise());
+
+   VALU_instruction& mad_v = mad->valu();
+   VALU_instruction& add_v = instr->valu();
+   VALU_instruction& mul_v = mul_instr->valu();
+
+   mad_v.clamp = add_v.clamp;
+   mad_v.neg_lo = mul_v.neg_lo; mad_v.neg_hi = mul_v.neg_hi;
+   mad_v.opsel_lo = mul_v.opsel_lo; mad_v.opsel_hi = mul_v.opsel_hi;
+
+   propagate_swizzles(&mad_v, add_v.opsel_lo[1 - add_op_idx], add_v.opsel_hi[1 - add_op_idx]);
+
+   unsigned neg_prop_idx = mad->operands[0].isConstant() ? 1 : 0;
+   mad_v.neg_lo[neg_prop_idx] ^= add_v.neg_lo[1 - add_op_idx];
+   mad_v.neg_hi[neg_prop_idx] ^= add_v.neg_hi[1 - add_op_idx];
+   mad_v.opsel_lo[2] = add_v.opsel_lo[add_op_idx];
+   mad_v.opsel_hi[2] = add_v.opsel_hi[add_op_idx];
+   mad_v.neg_lo[2] = add_v.neg_lo[add_op_idx];
+   mad_v.neg_hi[2] = add_v.neg_hi[add_op_idx];
+
+   aco_ptr<Instruction> old_add = std::move(instr);
+   ctx.mad_infos.emplace_back(std::move(old_add), mul_instr->definitions[0].tempId());
+   instr = std::move(mad);
+   ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
+   ctx.info[instr->definitions[0].tempId()].set_mad(ctx.mad_infos.size() - 1);
+   decrease_uses(ctx, mul_instr);
 
-            Operand op[3] = {op_instr->operands[0], op_instr->operands[1], instr->operands[1 - i]};
-            if (ctx.uses[instr->operands[i].tempId()] >= uses || !check_vop3_operands(ctx, 3, op))
-               continue;
+   /*
+    * === Pass 4: FMA to DOT2 Fusion (Vega Performance Win) ===
+    */
+   if (ctx.program->gfx_level == GFX9 && instr->opcode == aco_opcode::v_pk_fma_f16 &&
+       instr->operands[2].isConstant() && instr->operands[2].constantValue() == 0 &&
+       !instr->valu().neg_lo[2] && !instr->valu().neg_hi[2]) {
+
+      const bool can_fuse_dot2 = (instr->valu().opsel_lo[0] == instr->valu().opsel_hi[0]) &&
+                                 (instr->valu().opsel_lo[1] == instr->valu().opsel_hi[1]) &&
+                                 (instr->valu().neg_lo[0] == instr->valu().neg_hi[0]) &&
+                                 (instr->valu().neg_lo[1] == instr->valu().neg_hi[1]);
+
+      if (can_fuse_dot2) [[likely]] {
+         aco_ptr<Instruction> dot2{create_instruction(aco_opcode::v_dot2_f32_f16, Format::VOP3, 3, 1)};
+         dot2->operands[0] = instr->operands[0];
+         dot2->operands[1] = instr->operands[1];
+         dot2->operands[2] = Operand::c32(0u);
+         dot2->definitions[0] = instr->definitions[0];
+         dot2->pass_flags = instr->pass_flags;
+
+         VALU_instruction& dot2_v = dot2->valu();
+         dot2_v.opsel[0] = instr->valu().opsel_lo[0];
+         dot2_v.opsel[1] = instr->valu().opsel_lo[1];
+         dot2_v.neg[0] = instr->valu().neg_lo[0];
+         dot2_v.neg[1] = instr->valu().neg_lo[1];
+         dot2_v.clamp = instr->valu().clamp;
 
-            mul_instr = op_instr;
-            add_op_idx = 1 - i;
-            uses = ctx.uses[instr->operands[i].tempId()];
-            mul_neg_lo = mul_instr->valu().neg;
-            mul_neg_hi = mul_instr->valu().neg;
-            if (mul_instr->isSDWA()) {
-               for (unsigned j = 0; j < 2; j++)
-                  mul_opsel_lo[j] = mul_instr->sdwa().sel[j].offset();
-            } else {
-               mul_opsel_lo = mul_instr->valu().opsel;
-            }
-            mul_opsel_hi = mul_opsel_lo;
-         }
+         instr = std::move(dot2);
+         ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
       }
-
-      if (!mul_instr)
-         return;
-
-      /* turn mul + packed add into v_pk_fma_f16 */
-      aco_opcode mad = fadd ? aco_opcode::v_pk_fma_f16 : aco_opcode::v_pk_mad_u16;
-      aco_ptr<Instruction> fma{create_instruction(mad, Format::VOP3P, 3, 1)};
-      fma->operands[0] = copy_operand(ctx, mul_instr->operands[0]);
-      fma->operands[1] = copy_operand(ctx, mul_instr->operands[1]);
-      fma->operands[2] = instr->operands[add_op_idx];
-      fma->valu().clamp = vop3p->clamp;
-      fma->valu().neg_lo = mul_neg_lo;
-      fma->valu().neg_hi = mul_neg_hi;
-      fma->valu().opsel_lo = mul_opsel_lo;
-      fma->valu().opsel_hi = mul_opsel_hi;
-      propagate_swizzles(&fma->valu(), vop3p->opsel_lo[1 - add_op_idx],
-                         vop3p->opsel_hi[1 - add_op_idx]);
-      fma->valu().opsel_lo[2] = vop3p->opsel_lo[add_op_idx];
-      fma->valu().opsel_hi[2] = vop3p->opsel_hi[add_op_idx];
-      fma->valu().neg_lo[2] = vop3p->neg_lo[add_op_idx];
-      fma->valu().neg_hi[2] = vop3p->neg_hi[add_op_idx];
-      fma->valu().neg_lo[1] = fma->valu().neg_lo[1] ^ vop3p->neg_lo[1 - add_op_idx];
-      fma->valu().neg_hi[1] = fma->valu().neg_hi[1] ^ vop3p->neg_hi[1 - add_op_idx];
-      fma->definitions[0] = instr->definitions[0];
-      fma->pass_flags = instr->pass_flags;
-      instr = std::move(fma);
-      ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
-      decrease_uses(ctx, mul_instr);
-      return;
    }
 }
 
@@ -3684,7 +4478,7 @@ is_mul(Instruction* instr)
 void
 combine_instruction(opt_ctx& ctx, aco_ptr<Instruction>& instr)
 {
-   if (instr->definitions.empty() || is_dead(ctx.uses, instr.get()))
+   if (!instr || instr->definitions.empty() || is_dead(ctx.uses, instr.get()))
       return;
 
    if (instr->isVALU() || instr->isSALU()) {
@@ -3698,7 +4492,7 @@ combine_instruction(opt_ctx& ctx, aco_pt
          if (!info.is_extract())
             continue;
          /* if there are that many uses, there are likely better combinations */
-         // TODO: delay applying extract to a point where we know better
+        // TODO: delay applying extract to a point where we know better
          if (ctx.uses[op.tempId()] > 4) {
             info.label &= ~label_extract;
             continue;
@@ -3726,8 +4520,10 @@ combine_instruction(opt_ctx& ctx, aco_pt
    }
 
    if (instr->isVOP3P() && instr->opcode != aco_opcode::v_fma_mix_f32 &&
-       instr->opcode != aco_opcode::v_fma_mixlo_f16)
-      return combine_vop3p(ctx, instr);
+       instr->opcode != aco_opcode::v_fma_mixlo_f16) {
+      combine_vop3p(ctx, instr);
+      return;
+   }
 
    if (instr->isSDWA() || instr->isDPP())
       return;
@@ -3741,72 +4537,69 @@ combine_instruction(opt_ctx& ctx, aco_pt
          instr->operands[0].setTemp(info.parent_instr->operands[0].getTemp());
       }
 
-      if (instr->opcode == aco_opcode::p_extract)
+      if (instr->opcode == aco_opcode::p_extract) {
+         if (apply_interp_extract(ctx, instr)) {
+            return;
+         }
          apply_load_extract(ctx, instr);
+      }
    }
 
-   /* TODO: There are still some peephole optimizations that could be done:
-    * - abs(a - b) -> s_absdiff_i32
-    * - various patterns for s_bitcmp{0,1}_b32 and s_bitset{0,1}_b32
-    * - patterns for v_alignbit_b32 and v_alignbyte_b32
-    * These aren't probably too interesting though.
-    * There are also patterns for v_cmp_class_f{16,32,64}. This is difficult but
-    * probably more useful than the previously mentioned optimizations.
-    * The various comparison optimizations also currently only work with 32-bit
-    * floats. */
-
    /* neg(mul(a, b)) -> mul(neg(a), b), abs(mul(a, b)) -> mul(abs(a), abs(b)) */
-   if ((ctx.info[instr->definitions[0].tempId()].label & (label_neg | label_abs)) &&
-       ctx.uses[ctx.info[instr->definitions[0].tempId()].temp.id()] == 1) {
-      Temp val = ctx.info[instr->definitions[0].tempId()].temp;
-      Instruction* mul_instr = ctx.info[val.id()].parent_instr;
-
-      if (!is_mul(mul_instr))
-         return;
-
-      if (mul_instr->operands[0].isLiteral())
-         return;
-      if (mul_instr->valu().clamp)
-         return;
-      if (mul_instr->isSDWA() || mul_instr->isDPP())
-         return;
-      if (mul_instr->opcode == aco_opcode::v_mul_legacy_f32 &&
-          mul_instr->definitions[0].isSZPreserve())
-         return;
-      if (mul_instr->definitions[0].bytes() != instr->definitions[0].bytes())
-         return;
+   {
+      const uint64_t lbl = ctx.info[instr->definitions[0].tempId()].label;
+      if ((lbl & (label_neg | label_abs)) != 0) {
+         /* Fold only if the producer mul result has exactly 1 use. */
+         Temp val = ctx.info[instr->definitions[0].tempId()].temp;
+         if (ctx.uses[val.id()] == 1) {
+            Instruction* mul_instr = ctx.info[val.id()].parent_instr;
+
+            if (is_mul(mul_instr) &&
+                !mul_instr->operands[0].isLiteral() &&
+                !mul_instr->valu().clamp &&
+                !mul_instr->isSDWA() && !mul_instr->isDPP() &&
+                !(mul_instr->opcode == aco_opcode::v_mul_legacy_f32 &&
+                  mul_instr->definitions[0].isSZPreserve()) &&
+                mul_instr->definitions[0].bytes() == instr->definitions[0].bytes()) {
+
+               /* convert to mul(neg(a), b), mul(abs(a), abs(b)) or mul(neg(abs(a)), abs(b)) */
+               ctx.uses[mul_instr->definitions[0].tempId()]--;
+               Definition def = instr->definitions[0];
+               bool is_neg = ctx.info[instr->definitions[0].tempId()].is_neg();
+               bool is_abs = ctx.info[instr->definitions[0].tempId()].is_abs();
+               uint32_t pass_flags = instr->pass_flags;
+               Format format = mul_instr->format == Format::VOP2 ? asVOP3(Format::VOP2)
+                                                                 : mul_instr->format;
+
+               instr.reset(create_instruction(mul_instr->opcode, format, mul_instr->operands.size(), 1));
+               std::copy(mul_instr->operands.cbegin(), mul_instr->operands.cend(), instr->operands.begin());
+               instr->pass_flags = pass_flags;
+               instr->definitions[0] = def;
+
+               VALU_instruction& new_mul = instr->valu();
+               VALU_instruction& mul = mul_instr->valu();
+               new_mul.neg = mul.neg;
+               new_mul.abs = mul.abs;
+               new_mul.omod = mul.omod;
+               new_mul.opsel = mul.opsel;
+               new_mul.opsel_lo = mul.opsel_lo;
+               new_mul.opsel_hi = mul.opsel_hi;
+
+               if (is_abs) {
+                  new_mul.neg[0] = new_mul.neg[1] = false;
+                  new_mul.abs[0] = new_mul.abs[1] = true;
+               }
+               new_mul.neg[0] ^= is_neg;
+               new_mul.clamp = false;
 
-      /* convert to mul(neg(a), b), mul(abs(a), abs(b)) or mul(neg(abs(a)), abs(b)) */
-      ctx.uses[mul_instr->definitions[0].tempId()]--;
-      Definition def = instr->definitions[0];
-      bool is_neg = ctx.info[instr->definitions[0].tempId()].is_neg();
-      bool is_abs = ctx.info[instr->definitions[0].tempId()].is_abs();
-      uint32_t pass_flags = instr->pass_flags;
-      Format format = mul_instr->format == Format::VOP2 ? asVOP3(Format::VOP2) : mul_instr->format;
-      instr.reset(create_instruction(mul_instr->opcode, format, mul_instr->operands.size(), 1));
-      std::copy(mul_instr->operands.cbegin(), mul_instr->operands.cend(), instr->operands.begin());
-      instr->pass_flags = pass_flags;
-      instr->definitions[0] = def;
-      VALU_instruction& new_mul = instr->valu();
-      VALU_instruction& mul = mul_instr->valu();
-      new_mul.neg = mul.neg;
-      new_mul.abs = mul.abs;
-      new_mul.omod = mul.omod;
-      new_mul.opsel = mul.opsel;
-      new_mul.opsel_lo = mul.opsel_lo;
-      new_mul.opsel_hi = mul.opsel_hi;
-      if (is_abs) {
-         new_mul.neg[0] = new_mul.neg[1] = false;
-         new_mul.abs[0] = new_mul.abs[1] = true;
+               ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
+               return;
+            }
+         }
       }
-      new_mul.neg[0] ^= is_neg;
-      new_mul.clamp = false;
-
-      ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
-      return;
    }
 
-   /* combine mul+add -> mad */
+   /* combine mul+add -> mad/fma */
    bool is_add_mix =
       (instr->opcode == aco_opcode::v_fma_mix_f32 ||
        instr->opcode == aco_opcode::v_fma_mixlo_f16) &&
@@ -3863,7 +4656,7 @@ combine_instruction(opt_ctx& ctx, aco_pt
                         (mad_mix && ctx.program->dev.fused_mad_mix);
          bool has_mad = mad_mix ? !ctx.program->dev.fused_mad_mix
                                 : ((mad32 && ctx.program->gfx_level < GFX10_3 &&
-                                    ctx.program->family != CHIP_GFX940) ||
+                                     ctx.program->family != CHIP_GFX940) ||
                                    (mad16 && ctx.program->gfx_level <= GFX9));
          bool can_use_fma = has_fma && (!(info.parent_instr->definitions[0].isPrecise() ||
                                           instr->definitions[0].isPrecise()) ||
@@ -3883,9 +4676,9 @@ combine_instruction(opt_ctx& ctx, aco_pt
             continue;
 
          if (ctx.uses[instr->operands[i].tempId()] == uses) {
-            unsigned cur_idx = mul_instr->definitions[0].tempId();
+            unsigned cur_idx = mul_instr ? mul_instr->definitions[0].tempId() : 0;
             unsigned new_idx = info.parent_instr->definitions[0].tempId();
-            if (cur_idx > new_idx)
+            if (mul_instr && cur_idx > new_idx)
                continue;
          }
 
@@ -3934,6 +4727,7 @@ combine_instruction(opt_ctx& ctx, aco_pt
          opsel[3] = valu.opsel[3];
          omod = valu.omod;
          clamp = valu.clamp;
+
          /* abs of the multiplication result */
          if (valu.abs[mul_op_idx]) {
             neg[0] = false;
@@ -3945,10 +4739,10 @@ combine_instruction(opt_ctx& ctx, aco_pt
          neg[1] ^= valu.neg[mul_op_idx];
 
          if (instr->opcode == aco_opcode::v_sub_f32 || instr->opcode == aco_opcode::v_sub_f16)
-            neg[1 + add_op_idx] = neg[1 + add_op_idx] ^ true;
+            neg[1 + add_op_idx] ^= true;
          else if (instr->opcode == aco_opcode::v_subrev_f32 ||
                   instr->opcode == aco_opcode::v_subrev_f16)
-            neg[2 - add_op_idx] = neg[2 - add_op_idx] ^ true;
+            neg[2 - add_op_idx] ^= true;
 
          aco_ptr<Instruction> add_instr = std::move(instr);
          aco_ptr<Instruction> mad;
@@ -4036,6 +4830,9 @@ combine_instruction(opt_ctx& ctx, aco_pt
                                        "012", 1 | 2)) {
       } else if (combine_add_or_then_and_lshl(ctx, instr)) {
       } else if (combine_v_andor_not(ctx, instr)) {
+      } else if (combine_alignbit_b32(ctx, instr)) {
+      } else if (combine_alignbyte_b32(ctx, instr)) {
+      } else if (combine_bfi_b32(ctx, instr)) {
       }
    } else if (instr->opcode == aco_opcode::v_xor_b32 && ctx.program->gfx_level >= GFX10) {
       if (combine_three_valu_op(ctx, instr, aco_opcode::v_xor_b32, aco_opcode::v_xor3_b32, "012",
@@ -4075,12 +4872,15 @@ combine_instruction(opt_ctx& ctx, aco_pt
          } else if (combine_add_or_then_and_lshl(ctx, instr)) {
          }
       }
+      if (!combine_bcnt_mbcnt(ctx, instr)) {
+         combine_sad_u8(ctx, instr);
+      }
    } else if ((instr->opcode == aco_opcode::v_add_co_u32 ||
                instr->opcode == aco_opcode::v_add_co_u32_e64) &&
               !instr->usesModifiers()) {
       bool carry_out = ctx.uses[instr->definitions[1].tempId()] > 0;
       if (combine_add_sub_b2i(ctx, instr, aco_opcode::v_addc_co_u32, 1 | 2)) {
-      } else if (!carry_out && combine_add_bcnt(ctx, instr)) {
+      } else if (!carry_out && combine_bcnt_mbcnt(ctx, instr)) {
       } else if (!carry_out && combine_three_valu_op(ctx, instr, aco_opcode::v_mul_u32_u24,
                                                      aco_opcode::v_mad_u32_u24, "120", 1 | 2)) {
       } else if (!carry_out && combine_three_valu_op(ctx, instr, aco_opcode::v_mul_i32_i24,
@@ -4112,8 +4912,11 @@ combine_instruction(opt_ctx& ctx, aco_pt
       combine_salu_n2(ctx, instr);
    } else if (instr->opcode == aco_opcode::s_abs_i32) {
       combine_sabsdiff(ctx, instr);
-   } else if (instr->opcode == aco_opcode::v_and_b32) {
-      combine_v_andor_not(ctx, instr);
+   } else if (instr->opcode == aco_opcode::v_and_b32 || instr->opcode == aco_opcode::v_lshrrev_b32 ||
+              instr->opcode == aco_opcode::v_ashrrev_i32) {
+      if (!combine_bfe_b32(ctx, instr)) {
+         combine_v_andor_not(ctx, instr);
+      }
    } else if (instr->opcode == aco_opcode::v_fma_f32 || instr->opcode == aco_opcode::v_fma_f16) {
       /* set existing v_fma_f32 with label_mad so we can create v_fmamk_f32/v_fmaak_f32.
        * since ctx.uses[mad_info::mul_temp_id] is always 0, we don't have to worry about
@@ -4140,8 +4943,7 @@ combine_instruction(opt_ctx& ctx, aco_pt
    } else {
       aco_opcode min, max, min3, max3, med3, minmax;
       bool some_gfx9_only;
-      if (get_minmax_info(instr->opcode, &min, &max, &min3, &max3, &med3, &minmax,
-                          &some_gfx9_only) &&
+      if (get_minmax_info(instr->opcode, &min, &max, &min3, &max3, &med3, &minmax, &some_gfx9_only) &&
           (!some_gfx9_only || ctx.program->gfx_level >= GFX9)) {
          if (combine_minmax(ctx, instr, instr->opcode == min ? max : min,
                             instr->opcode == min ? min3 : max3, minmax)) {
@@ -4160,15 +4962,28 @@ struct remat_entry {
 inline bool
 is_constant(Instruction* instr)
 {
-   if (instr->opcode != aco_opcode::p_parallelcopy || instr->operands.size() != 1)
-      return false;
+   /* p_parallelcopy of a literal */
+   if (instr->opcode == aco_opcode::p_parallelcopy &&
+       instr->operands.size() == 1 &&
+       instr->operands[0].isConstant() &&
+       instr->definitions[0].isTemp()) {
+      return true;
+   }
 
-   return instr->operands[0].isConstant() && instr->definitions[0].isTemp();
+   /* SOP1 s_mov_* with a literal */
+   if ((instr->opcode == aco_opcode::s_mov_b32 || instr->opcode == aco_opcode::s_mov_b64) &&
+       instr->operands.size() == 1 &&
+       instr->operands[0].isConstant() &&
+       instr->definitions[0].isTemp()) {
+      return true;
+   }
+
+   return false;
 }
 
-void
-remat_constants_instr(opt_ctx& ctx, aco::map<Temp, remat_entry>& constants, Instruction* instr,
-                      uint32_t block_idx)
+static void
+remat_constants_instr(opt_ctx& ctx, aco::map<Temp, remat_entry>& constants,
+                     Instruction* instr, uint32_t block_idx)
 {
    for (Operand& op : instr->operands) {
       if (!op.isTemp())
@@ -4178,66 +4993,305 @@ remat_constants_instr(opt_ctx& ctx, aco:
       if (it == constants.end())
          continue;
 
-      /* Check if we already emitted the same constant in this block. */
       if (it->second.block != block_idx) {
-         /* Rematerialize the constant. */
          Builder bld(ctx.program, &ctx.instructions);
          Operand const_op = it->second.instr->operands[0];
-         it->second.instr = bld.copy(bld.def(op.regClass()), const_op);
+
+         Instruction* new_copy = bld.copy(bld.def(op.regClass()), const_op);
+         const unsigned new_temp_id = new_copy->definitions[0].tempId();
+
+         /**
+          * CRITICAL: Resize and initialize ctx.uses and ctx.info if needed.
+          * Builder may allocate temp IDs beyond current vector sizes.
+          */
+         if (new_temp_id >= ctx.uses.size()) {
+            ctx.uses.resize(new_temp_id + 1, 0);
+         }
+
+         if (new_temp_id >= ctx.info.size()) {
+            const size_t old_size = ctx.info.size();
+            ctx.info.resize(new_temp_id + 1);
+
+            /* Explicitly zero-initialize new entries */
+            for (size_t i = old_size; i <= new_temp_id; ++i) {
+               ctx.info[i].label = 0;
+               ctx.info[i].val = 0;
+               ctx.info[i].parent_instr = nullptr;
+            }
+         }
+
+         /* Validate source temp ID */
+         const unsigned src_temp_id = op.tempId();
+         if (src_temp_id >= ctx.info.size()) {
+            continue;  /* Corrupted operand, skip */
+         }
+
+         /* Initialize new temp's SSA info */
+         ctx.info[new_temp_id] = ctx.info[src_temp_id];
+         ctx.info[new_temp_id].parent_instr = new_copy;
+
+         it->second.instr = new_copy;
+         it->second.block = block_idx;
+      }
+
+      const unsigned remat_temp_id = it->second.instr->definitions[0].tempId();
+      if (remat_temp_id >= ctx.uses.size() || remat_temp_id >= ctx.info.size())
+         continue;
+
+      if (op.getTemp() != it->second.instr->definitions[0].getTemp()) {
+         if (op.tempId() < ctx.uses.size())
+            ctx.uses[op.tempId()]--;
+
+         op.setTemp(it->second.instr->definitions[0].getTemp());
+         ctx.uses[remat_temp_id]++;
+      }
+   }
+}
+
+/**
+ * Helper function to safely rematerialize constants in an instruction.
+ * Ensures proper temp ID allocation and SSA info initialization.
+ *
+ * @param ctx Optimizer context
+ * @param constants Map of constant temps to their defining instructions
+ * @param instr Instruction to process
+ * @param block_idx Current block index
+ */
+static void
+remat_constants_instr_safe(opt_ctx& ctx, aco::map<Temp, remat_entry>& constants,
+                           Instruction* instr, uint32_t block_idx)
+{
+   for (Operand& op : instr->operands) {
+      if (!op.isTemp()) {
+         continue;
+      }
+
+      auto it = constants.find(op.getTemp());
+      if (it == constants.end()) {
+         continue;
+      }
+
+      /* Check if we already emitted the same constant in this block */
+      if (it->second.block != block_idx) {
+         /* Rematerialize the constant by creating a new copy instruction */
+         Builder bld(ctx.program, &ctx.instructions);
+         Operand const_op = it->second.instr->operands[0];
+
+         /* Create the copy instruction - this allocates a new temp ID */
+         Instruction* new_copy = bld.copy(bld.def(op.regClass()), const_op);
+
+         /* Get the newly allocated temp ID */
+         const unsigned new_temp_id = new_copy->definitions[0].tempId();
+
+         /**
+          * CRITICAL: Ensure ctx.uses and ctx.info vectors are large enough
+          * AND properly initialized.
+          *
+          * The Builder allocates temp IDs by incrementing program->temp_rc.size(),
+          * but ctx.info might not have been resized yet. We must:
+          * 1. Resize the vectors to accommodate the new temp ID
+          * 2. Explicitly initialize new entries (default constructor is incomplete)
+          */
+
+         /* Resize and initialize ctx.uses */
+         if (new_temp_id >= ctx.uses.size()) {
+            ctx.uses.resize(new_temp_id + 1, 0);  // Initialize to 0
+         }
+
+         /* Resize and initialize ctx.info */
+         if (new_temp_id >= ctx.info.size()) {
+            const size_t old_size = ctx.info.size();
+            ctx.info.resize(new_temp_id + 1);
+
+            /**
+             * CRITICAL FIX: Explicitly initialize new ssa_info entries.
+             *
+             * The ssa_info default constructor only initializes 'label',
+             * leaving the union and parent_instr as garbage. We must
+             * zero-initialize all fields to prevent dereferencing garbage pointers.
+             */
+            for (size_t i = old_size; i <= new_temp_id; ++i) {
+               ctx.info[i].label = 0;
+               ctx.info[i].val = 0;  // Zero the union
+               ctx.info[i].parent_instr = nullptr;  // Null the pointer
+            }
+         }
+
+         /**
+          * Validate source temp ID before copying info.
+          * Defend against corrupted operands from earlier passes.
+          */
+         const unsigned src_temp_id = op.tempId();
+         if (src_temp_id >= ctx.info.size()) {
+            /* Corruption detected - skip this operand */
+            continue;
+         }
+
+         /* Initialize the new temp's SSA info by copying from the original */
+         ctx.info[new_temp_id] = ctx.info[src_temp_id];
+         ctx.info[new_temp_id].parent_instr = new_copy;
+
+         /* Update the constant map entry to point to the new copy */
+         it->second.instr = new_copy;
          it->second.block = block_idx;
-         ctx.uses.push_back(0);
-         ctx.info.push_back(ctx.info[op.tempId()]);
-         ctx.info[it->second.instr->definitions[0].tempId()].parent_instr = it->second.instr;
       }
 
-      /* Use the rematerialized constant and update information about latest use. */
+      /* Use the rematerialized constant */
+      const unsigned remat_temp_id = it->second.instr->definitions[0].tempId();
+
+      /* Defensive check: ensure rematerialized temp is valid */
+      if (remat_temp_id >= ctx.uses.size() || remat_temp_id >= ctx.info.size()) {
+         continue;  // Should never happen after resize, but be defensive
+      }
+
+      /* Update operand to use rematerialized version */
       if (op.getTemp() != it->second.instr->definitions[0].getTemp()) {
-         ctx.uses[op.tempId()]--;
+         /* Decrement use count of original constant */
+         if (op.tempId() < ctx.uses.size()) {
+            ctx.uses[op.tempId()]--;
+         }
+
+         /* Update operand temp */
          op.setTemp(it->second.instr->definitions[0].getTemp());
-         ctx.uses[op.tempId()]++;
+
+         /* Increment use count of rematerialized constant */
+         ctx.uses[remat_temp_id]++;
       }
    }
 }
 
 /**
- * This pass implements a simple constant rematerialization.
- * As common subexpression elimination (CSE) might increase the live-ranges
- * of loaded constants over large distances, this pass splits the live-ranges
- * again by re-emitting constants in every basic block.
+ * Rematerialize constants to reduce register pressure.
+ *
+ * This pass splits long constant live ranges by re-emitting constant
+ * definitions in each basic block where they're used. This trades
+ * code size for reduced register pressure.
+ *
+ * Vega 64 Strategy:
+ * The heuristic is tuned to Vega 64's occupancy tiers to prevent
+ * occupancy degradation while minimizing code bloat.
+ *
+ * @param ctx Optimizer context containing program and instruction info
  */
 void
 rematerialize_constants(opt_ctx& ctx)
 {
+   /* Allocate memory for constants map */
    aco::monotonic_buffer_resource memory(1024);
    aco::map<Temp, remat_entry> constants(memory);
 
+   /**
+    * Vega 64 Occupancy Tiers (VGPRs per wave → wave occupancy per SIMD):
+    *
+    *   0-64 VGPRs:   10 waves (full occupancy)
+    *  65-84 VGPRs:    8 waves (20% drop)
+    *  85-128 VGPRs:   6 waves (25% drop from 8)
+    * 129-168 VGPRs:   4 waves (33% drop from 6) ← critical
+    * 169-256 VGPRs:   2 waves (50% drop from 4) ← catastrophic
+    *
+    * Strategy: Increase rematerialization aggressiveness as we approach
+    * or exceed occupancy cliffs to prevent register spilling.
+    */
+   constexpr int16_t VEGA_TIER_1 = 64;   /* 10→8 wave boundary */
+   constexpr int16_t VEGA_TIER_2 = 84;   /* 8→6 wave boundary */
+   constexpr int16_t VEGA_TIER_3 = 128;  /* 6→4 wave boundary (critical) */
+   constexpr int16_t VEGA_TIER_4 = 168;  /* 4→2 wave boundary (catastrophic) */
+
+   /* Process blocks in program order */
    for (Block& block : ctx.program->blocks) {
-      if (block.logical_idom == -1)
+      /* Skip blocks without a dominator (unreachable or entry without predecessors) */
+      if (block.logical_idom == -1) {
          continue;
+      }
 
-      if (block.logical_idom == (int)block.index)
+      /* Clear constants map for entry blocks (blocks that dominate themselves) */
+      if (block.logical_idom == (int)block.index) {
          constants.clear();
+      }
 
+      /* Reserve space for transformed instructions to prevent reallocations */
+      ctx.instructions.clear();
       ctx.instructions.reserve(block.instructions.size());
 
+      /* Query current VGPR pressure (clamped to prevent negative values from corruption) */
+      const int16_t vgpr_demand = std::max<int16_t>(0, ctx.program->max_reg_demand.vgpr);
+
+      /* Determine rematerialization strategy based on architecture and pressure */
+      bool remat_enabled = false;
+
+      if (ctx.program->gfx_level == GFX9) {
+         /**
+          * Vega 64-Specific Tiered Strategy:
+          *
+          * The threshold represents the minimum number of constants required
+          * to enable rematerialization. Lower threshold = more aggressive.
+          */
+         unsigned remat_threshold = 1;
+
+         if (vgpr_demand <= VEGA_TIER_1) {
+            /* Tier 1: Full occupancy (10 waves)
+             * Conservative to minimize code bloat */
+            remat_threshold = 4;
+         } else if (vgpr_demand <= VEGA_TIER_2) {
+            /* Tier 2: 8-wave occupancy
+             * Moderate rematerialization */
+            remat_threshold = 2;
+         } else if (vgpr_demand <= VEGA_TIER_3) {
+            /* Tier 3: 6-wave occupancy
+             * Aggressive rematerialization */
+            remat_threshold = 1;
+         } else {
+            /* Tier 4+: Critical/Catastrophic (≤4 waves)
+             * Very aggressive - always remat */
+            remat_threshold = 0;
+         }
+
+         remat_enabled = constants.size() > remat_threshold;
+
+      } else {
+         /**
+          * Generic Strategy for Non-Vega Architectures:
+          * Use original conservative heuristic
+          */
+         constexpr int16_t generic_threshold = 96;
+         remat_enabled = (vgpr_demand < generic_threshold) || (constants.size() > 1);
+      }
+
+      /* Process instructions in this block */
       for (aco_ptr<Instruction>& instr : block.instructions) {
-         if (is_dead(ctx.uses, instr.get()))
+         if (!instr) {
             continue;
+         }
 
+         /* Record constant definitions for potential rematerialization */
          if (is_constant(instr.get())) {
             Temp tmp = instr->definitions[0].getTemp();
-            constants[tmp] = {instr.get(), block.index};
-         } else if (!is_phi(instr)) {
-            remat_constants_instr(ctx, constants, instr.get(), block.index);
+            constants[tmp] = remat_entry{instr.get(), UINT32_MAX};
+         }
+
+         /* Apply rematerialization if enabled and instruction is not a phi */
+         if (remat_enabled && !is_phi(instr)) {
+            remat_constants_instr_safe(ctx, constants, instr.get(), block.index);
          }
 
-         ctx.instructions.emplace_back(instr.release());
+         /* Move instruction to output vector */
+         ctx.instructions.emplace_back(std::move(instr));
       }
 
+      /* Replace block's instructions with transformed instructions */
       block.instructions = std::move(ctx.instructions);
    }
 }
 
+static inline void
+safe_dec_use(opt_ctx& ctx, const Operand& op)
+{
+   if (op.isTemp()) {
+      assert(op.tempId() < ctx.uses.size());
+      ctx.uses[op.tempId()]--;
+   }
+}
+
 bool
 to_uniform_bool_instr(opt_ctx& ctx, aco_ptr<Instruction>& instr)
 {
@@ -4276,10 +5330,11 @@ to_uniform_bool_instr(opt_ctx& ctx, aco_
    }
 
    for (Operand& op : instr->operands) {
+      /* This check is now redundant due to the loop above, but kept for safety. */
       if (!op.isTemp())
          continue;
 
-      ctx.uses[op.tempId()]--;
+      safe_dec_use(ctx, op);
 
       if (ctx.info[op.tempId()].is_uniform_bool()) {
          /* Just use the uniform boolean temp. */
@@ -4304,8 +5359,8 @@ to_uniform_bool_instr(opt_ctx& ctx, aco_
 
    instr->definitions[0].setTemp(Temp(instr->definitions[0].tempId(), s1));
    ctx.program->temp_rc[instr->definitions[0].tempId()] = s1;
-   assert(!instr->operands[0].isTemp() || instr->operands[0].regClass() == s1);
-   assert(!instr->operands[1].isTemp() || instr->operands[1].regClass() == s1);
+   assert(instr->operands[0].regClass() == s1);
+   assert(instr->operands[1].regClass() == s1);
    return true;
 }
 
@@ -5045,7 +6100,21 @@ optimize(Program* program)
 {
    opt_ctx ctx;
    ctx.program = program;
-   ctx.info = std::vector<ssa_info>(program->peekAllocationId());
+
+   /**
+    * CRITICAL: Allocate and initialize ctx.info for ALL currently allocated temps.
+    * Use peekAllocationId() to get the next temp ID that will be allocated,
+    * ensuring we have space for all existing temps (IDs 0 to peekAllocationId()-1).
+    */
+   const size_t num_temps = program->peekAllocationId();
+   ctx.info.resize(num_temps);
+
+   /* Explicitly zero-initialize all entries to ensure no garbage pointers */
+   for (size_t i = 0; i < num_temps; ++i) {
+      ctx.info[i].label = 0;
+      ctx.info[i].val = 0;
+      ctx.info[i].parent_instr = nullptr;
+   }
 
    /* 1. Bottom-Up DAG pass (forward) to label all ssa-defs */
    for (Block& block : program->blocks) {
@@ -5062,7 +6131,7 @@ optimize(Program* program)
 
    ctx.uses = dead_code_analysis(program);
 
-   /* 2. Rematerialize constants in every block. */
+   /* 2. Rematerialize constants in every block */
    rematerialize_constants(ctx);
 
    validate_opt_ctx(ctx);
@@ -5090,6 +6159,7 @@ optimize(Program* program)
 
    /* 5. Add literals to instructions */
    for (Block& block : program->blocks) {
+      ctx.instructions.clear();
       ctx.instructions.reserve(block.instructions.size());
       ctx.fp_mode = block.fp_mode;
       for (aco_ptr<Instruction>& instr : block.instructions)
