--- a/src/amd/compiler/aco_lower_to_hw_instr.cpp	2025-10-31 10:25:32.909236831 +0100
+++ b/src/amd/compiler/aco_lower_to_hw_instr.cpp	2026-01-29 10:42:19.502642047 +0100
@@ -9,6 +9,7 @@
 
 #include "common/sid.h"
 
+#include <algorithm>
 #include <map>
 #include <vector>
 
@@ -45,32 +46,20 @@ class gfx9_pops_done_msg_bounds {
 public:
    explicit gfx9_pops_done_msg_bounds() = default;
 
-   explicit gfx9_pops_done_msg_bounds(const Program* const program)
+   explicit gfx9_pops_done_msg_bounds(const Program* program)
    {
-      /* Find the top-level location after the last ordered section end pseudo-instruction in the
-       * program.
-       * Consider `p_pops_gfx9_overlapped_wave_wait_done` a boundary too - make sure the message
-       * isn't sent if any wait hasn't been fully completed yet (if a begin-end-begin situation
-       * occurs somehow, as the location of `p_pops_gfx9_ordered_section_done` is controlled by the
-       * application) for safety, assuming that waits are the only thing that need the packer
-       * hardware register to be set at some point during or before them, and it won't be set
-       * anymore after the last wait.
-       */
       int last_top_level_block_idx = -1;
-      for (int block_idx = (int)program->blocks.size() - 1; block_idx >= 0; block_idx--) {
+      for (int block_idx = static_cast<int>(program->blocks.size()) - 1; block_idx >= 0;
+           block_idx--) {
          const Block& block = program->blocks[block_idx];
          if (block.kind & block_kind_top_level) {
             last_top_level_block_idx = block_idx;
          }
-         for (size_t instr_idx = block.instructions.size() - 1; instr_idx + size_t(1) > 0;
-              instr_idx--) {
+         for (size_t instr_idx = block.instructions.size(); instr_idx-- > 0;) {
             const aco_opcode opcode = block.instructions[instr_idx]->opcode;
             if (opcode == aco_opcode::p_pops_gfx9_ordered_section_done ||
                 opcode == aco_opcode::p_pops_gfx9_overlapped_wave_wait_done) {
                end_block_idx_ = last_top_level_block_idx;
-               /* The same block if it's already a top-level block, or the beginning of the next
-                * top-level block.
-                */
                instr_after_end_idx_ = block_idx == end_block_idx_ ? instr_idx + 1 : 0;
                break;
             }
@@ -81,31 +70,16 @@ public:
       }
    }
 
-   /* If this is not -1, during the normal execution flow (not early exiting), MSG_ORDERED_PS_DONE
-    * must be sent in this block.
-    */
    int end_block_idx() const { return end_block_idx_; }
-
-   /* If end_block_idx() is an existing block, during the normal execution flow (not early exiting),
-    * MSG_ORDERED_PS_DONE must be sent before this instruction in the block end_block_idx().
-    * If this is out of the bounds of the instructions in the end block, it must be sent in the end
-    * of that block.
-    */
    size_t instr_after_end_idx() const { return instr_after_end_idx_; }
 
-   /* Whether an instruction doing early exit (such as discard) needs to send MSG_ORDERED_PS_DONE
-    * before actually ending the program.
-    */
-   bool early_exit_needs_done_msg(const int block_idx, const size_t instr_idx) const
+   bool early_exit_needs_done_msg(int block_idx, size_t instr_idx) const
    {
       return block_idx <= end_block_idx_ &&
              (block_idx != end_block_idx_ || instr_idx < instr_after_end_idx_);
    }
 
 private:
-   /* Initialize to an empty range for which "is inside" comparisons will be failing for any
-    * block.
-    */
    int end_block_idx_ = -1;
    size_t instr_after_end_idx_ = 0;
 };
@@ -114,7 +88,7 @@ void
 copy_constant_sgpr(Builder& bld, Definition dst, uint64_t constant)
 {
    if (dst.regClass() == s1) {
-      uint32_t imm = constant;
+      uint32_t imm = static_cast<uint32_t>(constant);
       Operand op = Operand::get_const(bld.program->gfx_level, imm, 4);
       if (op.isLiteral()) {
          if (imm >= 0xffff8000 || imm <= 0x7fff) {
@@ -128,16 +102,16 @@ copy_constant_sgpr(Builder& bld, Definit
             return;
          }
 
-         unsigned start = (ffs(imm) - 1) & 0x1f;
-         unsigned size = util_bitcount(imm) & 0x1f;
+         unsigned start = (ffs(static_cast<int>(imm)) - 1) & 0x1fu;
+         unsigned size = util_bitcount(imm) & 0x1fu;
          if (BITFIELD_RANGE(start, size) == imm) {
             bld.sop2(aco_opcode::s_bfm_b32, dst, Operand::c32(size), Operand::c32(start));
             return;
          }
 
          if (bld.program->gfx_level >= GFX9) {
-            Operand op_lo = Operand::c32(int32_t(int16_t(imm)));
-            Operand op_hi = Operand::c32(int32_t(int16_t(imm >> 16)));
+            Operand op_lo = Operand::c32(static_cast<int32_t>(static_cast<int16_t>(imm)));
+            Operand op_hi = Operand::c32(static_cast<int32_t>(static_cast<int16_t>(imm >> 16)));
             if (!op_lo.isLiteral() && !op_hi.isLiteral()) {
                bld.sop2(aco_opcode::s_pack_ll_b32_b16, dst, op_lo, op_hi);
                return;
@@ -157,14 +131,16 @@ copy_constant_sgpr(Builder& bld, Definit
       return;
    }
 
-   unsigned start = (ffsll(constant) - 1) & 0x3f;
-   unsigned size = util_bitcount64(constant) & 0x3f;
+   unsigned start = (ffsll(static_cast<long long>(constant)) - 1) & 0x3fu;
+   unsigned size = util_bitcount64(constant) & 0x3fu;
    if (BITFIELD64_RANGE(start, size) == constant) {
       bld.sop2(aco_opcode::s_bfm_b64, dst, Operand::c32(size), Operand::c32(start));
       return;
    }
 
-   uint64_t rev = ((uint64_t)util_bitreverse(constant) << 32) | util_bitreverse(constant >> 32);
+   uint64_t rev =
+      (static_cast<uint64_t>(util_bitreverse(static_cast<uint32_t>(constant))) << 32) |
+      util_bitreverse(static_cast<uint32_t>(constant >> 32));
    if (Operand::is_constant_representable(rev, 8, true, false)) {
       bld.sop1(aco_opcode::s_brev_b64, dst, Operand::c64(rev));
       return;
@@ -178,9 +154,9 @@ copy_constant_sgpr(Builder& bld, Definit
    uint32_t derep = 0;
    bool can_use_rep = bld.program->gfx_level >= GFX9;
    for (unsigned i = 0; can_use_rep && i < 32; i++) {
-      uint32_t lo = (constant >> (i * 2)) & 0x1;
-      uint32_t hi = (constant >> ((i * 2) + 1)) & 0x1;
-      can_use_rep &= lo == hi;
+      uint32_t lo = (constant >> (i * 2)) & 0x1u;
+      uint32_t hi = (constant >> ((i * 2) + 1)) & 0x1u;
+      can_use_rep = can_use_rep && (lo == hi);
       derep |= lo << i;
    }
    if (can_use_rep) {
@@ -188,46 +164,44 @@ copy_constant_sgpr(Builder& bld, Definit
       return;
    }
 
-   copy_constant_sgpr(bld, Definition(dst.physReg(), s1), (uint32_t)constant);
-   copy_constant_sgpr(bld, Definition(dst.physReg().advance(4), s1), constant >> 32);
+   copy_constant_sgpr(bld, Definition(dst.physReg(), s1), static_cast<uint32_t>(constant));
+   copy_constant_sgpr(bld, Definition(dst.physReg().advance(4), s1),
+                      static_cast<uint32_t>(constant >> 32));
 }
 
 /* used by handle_operands() indirectly through Builder::copy */
-uint8_t int8_mul_table[512] = {
-   0, 20,  1,  1,   1,  2,   1,  3,   1,  4,   1, 5,   1,  6,   1,  7,   1,  8,   1,  9,
-   1, 10,  1,  11,  1,  12,  1,  13,  1,  14,  1, 15,  1,  16,  1,  17,  1,  18,  1,  19,
-   1, 20,  1,  21,  1,  22,  1,  23,  1,  24,  1, 25,  1,  26,  1,  27,  1,  28,  1,  29,
-   1, 30,  1,  31,  1,  32,  1,  33,  1,  34,  1, 35,  1,  36,  1,  37,  1,  38,  1,  39,
-   1, 40,  1,  41,  1,  42,  1,  43,  1,  44,  1, 45,  1,  46,  1,  47,  1,  48,  1,  49,
-   1, 50,  1,  51,  1,  52,  1,  53,  1,  54,  1, 55,  1,  56,  1,  57,  1,  58,  1,  59,
-   1, 60,  1,  61,  1,  62,  1,  63,  1,  64,  5, 13,  2,  33,  17, 19,  2,  34,  3,  23,
-   2, 35,  11, 53,  2,  36,  7,  47,  2,  37,  3, 25,  2,  38,  7,  11,  2,  39,  53, 243,
-   2, 40,  3,  27,  2,  41,  17, 35,  2,  42,  5, 17,  2,  43,  3,  29,  2,  44,  15, 23,
-   2, 45,  7,  13,  2,  46,  3,  31,  2,  47,  5, 19,  2,  48,  19, 59,  2,  49,  3,  33,
-   2, 50,  7,  51,  2,  51,  15, 41,  2,  52,  3, 35,  2,  53,  11, 33,  2,  54,  23, 27,
-   2, 55,  3,  37,  2,  56,  9,  41,  2,  57,  5, 23,  2,  58,  3,  39,  2,  59,  7,  17,
-   2, 60,  9,  241, 2,  61,  3,  41,  2,  62,  5, 25,  2,  63,  35, 245, 2,  64,  3,  43,
-   5, 26,  9,  43,  3,  44,  7,  19,  10, 39,  3, 45,  4,  34,  11, 59,  3,  46,  9,  243,
-   4, 35,  3,  47,  22, 53,  7,  57,  3,  48,  5, 29,  10, 245, 3,  49,  4,  37,  9,  45,
-   3, 50,  7,  241, 4,  38,  3,  51,  7,  22,  5, 31,  3,  52,  7,  59,  7,  242, 3,  53,
-   4, 40,  7,  23,  3,  54,  15, 45,  4,  41,  3, 55,  6,  241, 9,  47,  3,  56,  13, 13,
-   5, 34,  3,  57,  4,  43,  11, 39,  3,  58,  5, 35,  4,  44,  3,  59,  6,  243, 7,  245,
-   3, 60,  5,  241, 7,  26,  3,  61,  4,  46,  5, 37,  3,  62,  11, 17,  4,  47,  3,  63,
-   5, 38,  5,  243, 3,  64,  7,  247, 9,  50,  5, 39,  4,  241, 33, 37,  6,  33,  13, 35,
-   4, 242, 5,  245, 6,  247, 7,  29,  4,  51,  5, 41,  5,  246, 7,  249, 3,  240, 11, 19,
-   5, 42,  3,  241, 4,  245, 25, 29,  3,  242, 5, 43,  4,  246, 3,  243, 17, 58,  17, 43,
-   3, 244, 5,  249, 6,  37,  3,  245, 2,  240, 5, 45,  2,  241, 21, 23,  2,  242, 3,  247,
-   2, 243, 5,  251, 2,  244, 29, 61,  2,  245, 3, 249, 2,  246, 17, 29,  2,  247, 9,  55,
-   1, 240, 1,  241, 1,  242, 1,  243, 1,  244, 1, 245, 1,  246, 1,  247, 1,  248, 1,  249,
-   1, 250, 1,  251, 1,  252, 1,  253, 1,  254, 1, 255};
+const uint8_t int8_mul_table[512] = {
+   0,   20,  1,   1,   1,   2,   1,   3,   1,   4,   1,   5,   1,   6,   1,   7,   1,   8,   1,
+   9,   1,   10,  1,   11,  1,   12,  1,   13,  1,   14,  1,   15,  1,   16,  1,   17,  1,   18,
+   1,   19,  1,   20,  1,   21,  1,   22,  1,   23,  1,   24,  1,   25,  1,   26,  1,   27,  1,
+   28,  1,   29,  1,   30,  1,   31,  1,   32,  1,   33,  1,   34,  1,   35,  1,   36,  1,   37,
+   1,   38,  1,   39,  1,   40,  1,   41,  1,   42,  1,   43,  1,   44,  1,   45,  1,   46,  1,
+   47,  1,   48,  1,   49,  1,   50,  1,   51,  1,   52,  1,   53,  1,   54,  1,   55,  1,   56,
+   1,   57,  1,   58,  1,   59,  1,   60,  1,   61,  1,   62,  1,   63,  1,   64,  5,   13,  2,
+   33,  17,  19,  2,   34,  3,   23,  2,   35,  11,  53,  2,   36,  7,   47,  2,   37,  3,   25,
+   2,   38,  7,   11,  2,   39,  53,  243, 2,   40,  3,   27,  2,   41,  17,  35,  2,   42,  5,
+   17,  2,   43,  3,   29,  2,   44,  15,  23,  2,   45,  7,   13,  2,   46,  3,   31,  2,   47,
+   5,   19,  2,   48,  19,  59,  2,   49,  3,   33,  2,   50,  7,   51,  2,   51,  15,  41,  2,
+   52,  3,   35,  2,   53,  11,  33,  2,   54,  23,  27,  2,   55,  3,   37,  2,   56,  9,   41,
+   2,   57,  5,   23,  2,   58,  3,   39,  2,   59,  7,   17,  2,   60,  9,   241, 2,   61,  3,
+   41,  2,   62,  5,   25,  2,   63,  35,  245, 2,   64,  3,   43,  5,   26,  9,   43,  3,   44,
+   7,   19,  10,  39,  3,   45,  4,   34,  11,  59,  3,   46,  9,   243, 4,   35,  3,   47,  22,
+   53,  7,   57,  3,   48,  5,   29,  10,  245, 3,   49,  4,   37,  9,   45,  3,   50,  7,   241,
+   4,   38,  3,   51,  7,   22,  5,   31,  3,   52,  7,   59,  7,   242, 3,   53,  4,   40,  7,
+   23,  3,   54,  15,  45,  4,   41,  3,   55,  6,   241, 9,   47,  3,   56,  13,  13,  5,   34,
+   3,   57,  4,   43,  11,  39,  3,   58,  5,   35,  4,   44,  3,   59,  6,   243, 7,   245, 3,
+   60,  5,   241, 7,   26,  3,   61,  4,   46,  5,   37,  3,   62,  11,  17,  4,   47,  3,   63,
+   5,   38,  5,   243, 3,   64,  7,   247, 9,   50,  5,   39,  4,   241, 33,  37,  6,   33,  13,
+   35,  4,   242, 5,   245, 6,   247, 7,   29,  4,   51,  5,   41,  5,   246, 7,   249, 3,   240,
+   11,  19,  5,   42,  3,   241, 4,   245, 25,  29,  3,   242, 5,   43,  4,   246, 3,   243, 17,
+   58,  17,  43,  3,   244, 5,   249, 6,   37,  3,   245, 2,   240, 5,   45,  2,   241, 21,  23,
+   2,   242, 3,   247, 2,   243, 5,   251, 2,   244, 29,  61,  2,   245, 3,   249, 2,   246, 17,
+   29,  2,   247, 9,   55,  1,   240, 1,   241, 1,   242, 1,   243, 1,   244, 1,   245, 1,   246,
+   1,   247, 1,   248, 1,   249, 1,   250, 1,   251, 1,   252, 1,   253, 1,   254, 1,   255};
 
 aco_opcode
 get_reduce_opcode(amd_gfx_level gfx_level, ReduceOp op)
 {
-   /* Because some 16-bit instructions are already VOP3 on GFX10, we use the
-    * 32-bit opcodes (VOP2) which allows to remove the temporary VGPR and to use
-    * DPP with the arithmetic instructions. This requires to sign-extend.
-    */
    switch (op) {
    case iadd8:
    case iadd16:
@@ -238,7 +212,6 @@ get_reduce_opcode(amd_gfx_level gfx_leve
       } else {
          return aco_opcode::v_add_co_u32;
       }
-      break;
    case imul8:
    case imul16:
       if (gfx_level >= GFX10) {
@@ -248,7 +221,6 @@ get_reduce_opcode(amd_gfx_level gfx_leve
       } else {
          return aco_opcode::v_mul_u32_u24;
       }
-      break;
    case fadd16: return aco_opcode::v_add_f16;
    case fmul16: return aco_opcode::v_mul_f16;
    case imax8:
@@ -260,7 +232,6 @@ get_reduce_opcode(amd_gfx_level gfx_leve
       } else {
          return aco_opcode::v_max_i32;
       }
-      break;
    case imin8:
    case imin16:
       if (gfx_level >= GFX10) {
@@ -270,7 +241,6 @@ get_reduce_opcode(amd_gfx_level gfx_leve
       } else {
          return aco_opcode::v_min_i32;
       }
-      break;
    case umin8:
    case umin16:
       if (gfx_level >= GFX10) {
@@ -280,7 +250,6 @@ get_reduce_opcode(amd_gfx_level gfx_leve
       } else {
          return aco_opcode::v_min_u32;
       }
-      break;
    case umax8:
    case umax16:
       if (gfx_level >= GFX10) {
@@ -290,7 +259,6 @@ get_reduce_opcode(amd_gfx_level gfx_leve
       } else {
          return aco_opcode::v_max_u32;
       }
-      break;
    case fmin16: return aco_opcode::v_min_f16;
    case fmax16: return aco_opcode::v_max_f16;
    case iadd32: return gfx_level >= GFX9 ? aco_opcode::v_add_u32 : aco_opcode::v_add_co_u32;
@@ -332,7 +300,7 @@ get_reduce_opcode(amd_gfx_level gfx_leve
 bool
 is_vop3_reduce_opcode(aco_opcode opcode)
 {
-   return instr_info.format[(int)opcode] == Format::VOP3;
+   return instr_info.format[static_cast<int>(opcode)] == Format::VOP3;
 }
 
 void
@@ -348,7 +316,7 @@ emit_vadd32(Builder& bld, Definition def
 void
 emit_int64_dpp_op(lower_context* ctx, PhysReg dst_reg, PhysReg src0_reg, PhysReg src1_reg,
                   PhysReg vtmp_reg, ReduceOp op, unsigned dpp_ctrl, unsigned row_mask,
-                  unsigned bank_mask, bool bound_ctrl, Operand* identity = NULL)
+                  unsigned bank_mask, bool bound_ctrl, Operand* identity = nullptr)
 {
    Builder bld(ctx->program, &ctx->instructions);
    Definition dst[] = {Definition(dst_reg, lv1), Definition(PhysReg{dst_reg + 1}, lv1)};
@@ -358,6 +326,7 @@ emit_int64_dpp_op(lower_context* ctx, Ph
    Operand src1_64 = Operand(src1_reg, lv2);
    Operand vtmp_op[] = {Operand(vtmp_reg, lv1), Operand(PhysReg{vtmp_reg + 1}, lv1)};
    Operand vtmp_op64 = Operand(vtmp_reg, lv2);
+
    if (op == iadd64) {
       if (ctx->program->gfx_level >= GFX10) {
          if (identity)
@@ -410,17 +379,6 @@ emit_int64_dpp_op(lower_context* ctx, Ph
       bld.vop2(aco_opcode::v_cndmask_b32, dst[0], vtmp_op[0], src1[0], Operand(vcc, bld.lm));
       bld.vop2(aco_opcode::v_cndmask_b32, dst[1], vtmp_op[1], src1[1], Operand(vcc, bld.lm));
    } else if (op == imul64) {
-      /* t4 = dpp(x_hi)
-       * t1 = umul_lo(t4, y_lo)
-       * t3 = dpp(x_lo)
-       * t0 = umul_lo(t3, y_hi)
-       * t2 = iadd(t0, t1)
-       * t5 = umul_hi(t3, y_lo)
-       * res_hi = iadd(t2, t5)
-       * res_lo = umul_lo(t3, y_lo)
-       * Requires that res_hi != src0[0] and res_hi != src1[0]
-       * and that vtmp[0] != res_hi.
-       */
       if (identity)
          bld.vop1(aco_opcode::v_mov_b32, vtmp_def[0], identity[1]);
       bld.vop1_dpp(aco_opcode::v_mov_b32, vtmp_def[0], src0[1], dpp_ctrl, row_mask, bank_mask,
@@ -511,19 +469,53 @@ emit_int64_op(lower_context* ctx, PhysRe
          std::swap(src0[1], src1[1]);
          std::swap(src0_64, src1_64);
       }
-      assert(!(src0_reg == src1_reg));
-      /* t1 = umul_lo(x_hi, y_lo)
-       * t0 = umul_lo(x_lo, y_hi)
-       * t2 = iadd(t0, t1)
-       * t5 = umul_hi(x_lo, y_lo)
-       * res_hi = iadd(t2, t5)
-       * res_lo = umul_lo(x_lo, y_lo)
-       * assumes that it's ok to modify x_hi/y_hi, since we might not have vtmp
+
+      /*
+       * Handle the squaring case where src0_reg == src1_reg.
+       *
+       * The multiplication algorithm uses the high dwords of both operand
+       * registers (src0_reg+1 and src1_reg+1) as temporary storage for
+       * intermediate results. When squaring (src0 == src1), these temporaries
+       * alias the same register, causing the second write to corrupt the
+       * first intermediate result.
+       *
+       * Solution: Copy one operand to scratch space. We prefer dst_reg as
+       * scratch when possible (since it will be overwritten anyway), falling
+       * back to vtmp only when dst aliases src0 (which is the source we keep).
+       */
+      if (src0_reg == src1_reg) {
+         PhysReg scratch_reg;
+         if (dst_reg != src0_reg) {
+            scratch_reg = dst_reg;
+         } else {
+            assert(vtmp.reg() != 0);
+            scratch_reg = vtmp;
+         }
+
+         bld.vop1(aco_opcode::v_mov_b32, Definition(scratch_reg, lv1), src1[0]);
+         bld.vop1(aco_opcode::v_mov_b32, Definition(PhysReg{scratch_reg + 1}, lv1), src1[1]);
+         src1_reg = scratch_reg;
+         src1[0] = Operand(scratch_reg, lv1);
+         src1[1] = Operand(PhysReg{scratch_reg + 1}, lv1);
+         src1_64 = Operand(scratch_reg, lv2);
+      }
+
+      /*
+       * 64-bit multiplication: (x_hi:x_lo) * (y_hi:y_lo)
+       *
+       * We only need the low 64 bits:
+       *   res_lo = lo32(x_lo * y_lo)
+       *   res_hi = hi32(x_lo * y_lo) + lo32(x_lo * y_hi) + lo32(x_hi * y_lo)
+       *
+       * Temporaries (reusing high dwords of operands):
+       *   tmp0 = src0_reg+1 (x_hi, then overwritten)
+       *   tmp1 = src1_reg+1 (y_hi, then overwritten)
        */
       Definition tmp0_def(PhysReg{src0_reg + 1}, lv1);
       Definition tmp1_def(PhysReg{src1_reg + 1}, lv1);
-      Operand tmp0_op = src0[1];
-      Operand tmp1_op = src1[1];
+      Operand tmp0_op = Operand(PhysReg{src0_reg + 1}, lv1);
+      Operand tmp1_op = Operand(PhysReg{src1_reg + 1}, lv1);
+
       bld.vop3(aco_opcode::v_mul_lo_u32, tmp0_def, src0[1], src1[0]);
       bld.vop3(aco_opcode::v_mul_lo_u32, tmp1_def, src0[0], src1[1]);
       emit_vadd32(bld, tmp0_def, tmp1_op, tmp0_op);
@@ -536,7 +528,7 @@ emit_int64_op(lower_context* ctx, PhysRe
 void
 emit_dpp_op(lower_context* ctx, PhysReg dst_reg, PhysReg src0_reg, PhysReg src1_reg, PhysReg vtmp,
             ReduceOp op, unsigned size, unsigned dpp_ctrl, unsigned row_mask, unsigned bank_mask,
-            bool bound_ctrl, Operand* identity = NULL) /* for VOP3 with sparse writes */
+            bool bound_ctrl, Operand* identity = nullptr)
 {
    Builder bld(ctx->program, &ctx->instructions);
    RegClass rc = lv1.resize(size * 4);
@@ -1205,6 +1197,8 @@ struct copy_operation {
    };
 };
 
+using CopyMap = std::vector<std::pair<PhysReg, copy_operation>>;
+
 void
 split_copy(lower_context* ctx, unsigned offset, Definition* def, Operand* op,
            const copy_operation& src, bool ignore_uses, unsigned max_size)
@@ -1269,13 +1263,12 @@ create_bperm(Builder& bld, uint8_t swiz[
 
    dst = Definition(PhysReg(dst.physReg().reg()), v1);
    if (!src1.isConstant())
-      src1 = Operand(PhysReg(src1.physReg().reg()), src1.regClass().resize(4));
+      src1 = Operand(PhysReg(src1.physReg().reg()), v1);
    if (src0.isUndefined())
       src0 = Operand(dst.physReg(), v1);
    else if (!src0.isConstant())
-      src0 = Operand(PhysReg(src0.physReg().reg()), src0.regClass().resize(4));
+      src0 = Operand(PhysReg(src0.physReg().reg()), v1);
 
-   assert(src0.isOfType(RegType::vgpr) || src1.isOfType(RegType::vgpr));
    bld.vop3(aco_opcode::v_perm_b32, dst, src0, src1, Operand::c32(swiz_packed));
 }
 
@@ -1297,10 +1290,8 @@ emit_v_mov_b16(Builder& bld, Definition
    instr->valu().opsel[0] = op.physReg().byte() == 2;
    instr->valu().opsel[3] = dst.physReg().byte() == 2;
 
-   if (op.isOfType(RegType::sgpr) && instr->valu().opsel[0]) {
+   if (op.physReg().reg() < 256 && instr->valu().opsel[0])
       instr->format = asVOP3(instr->format);
-      instr->operands[0] = Operand(PhysReg(op.physReg().reg()), op.regClass());
-   }
 }
 
 void
@@ -1419,11 +1410,7 @@ do_copy(lower_context* ctx, Builder& bld
       } else if (def.regClass() == v2b && ctx->program->gfx_level >= GFX11) {
          emit_v_mov_b16(bld, def, op);
       } else if (def.regClass().is_subdword()) {
-         Instruction* instr = bld.vop1_sdwa(aco_opcode::v_mov_b32, def, op);
-         if (op.isOfType(RegType::sgpr)) {
-            instr->operands[0] = Operand(PhysReg(op.physReg().reg()), op.regClass());
-            instr->sdwa().sel[0] = SubdwordSel(def.bytes(), op.physReg().byte(), false);
-         }
+         bld.vop1_sdwa(aco_opcode::v_mov_b32, def, op);
       } else {
          UNREACHABLE("unsupported copy");
       }
@@ -1613,36 +1600,24 @@ do_pack_2x16(lower_context* ctx, Builder
       return;
    }
 
-   /* Whether both Operands can be used in a single VOP3 instruction. */
-   bool both_ops_are_sgpr = lo.isOfType(RegType::sgpr) && hi.isOfType(RegType::sgpr);
-   bool can_use_vop3 = ctx->program->gfx_level >= GFX10 ||
-                       (!lo.isLiteral() && !hi.isLiteral() && !both_ops_are_sgpr);
-
-   /* Fix SGPR operands RegClasses. */
-   bool opsel_lo = lo.physReg().byte();
-   bool opsel_hi = hi.physReg().byte();
-   lo = lo.isOfType(RegType::sgpr) ? Operand(PhysReg(lo.physReg().reg()), lo.regClass()) : lo;
-   hi = hi.isOfType(RegType::sgpr) ? Operand(PhysReg(hi.physReg().reg()), hi.regClass()) : hi;
-
    /* v_pack_b32_f16 can be used for bit exact copies if:
     * - fp16 input denorms are enabled, otherwise they get flushed to zero
     * - signalling input NaNs are kept, which is the case with IEEE_MODE=0
     *   GFX12+ always quiets signalling NaNs, IEEE_MODE was removed
     */
    bool can_use_pack = (ctx->block->fp_mode.denorm16_64 & fp_denorm_keep_in) &&
-                       ctx->program->gfx_level >= GFX9 && can_use_vop3 &&
+                       (ctx->program->gfx_level >= GFX10 ||
+                        (ctx->program->gfx_level >= GFX9 && !lo.isLiteral() && !hi.isLiteral())) &&
                        ctx->program->gfx_level < GFX12;
 
    if (can_use_pack) {
       Instruction* instr = bld.vop3(aco_opcode::v_pack_b32_f16, def, lo, hi);
-      /* opsel: 0 = select low half, 1 = select high half. [0] = src0, [1] = src1 */
-      instr->valu().opsel[0] = opsel_lo;
-      instr->valu().opsel[1] = opsel_hi;
+      instr->valu().opsel = hi.physReg().byte() | (lo.physReg().byte() >> 1);
       return;
    }
 
    /* a single alignbyte can be sufficient: hi can be a 32-bit integer constant */
-   if (opsel_lo && !opsel_hi && can_use_vop3 &&
+   if (lo.physReg().byte() == 2 && hi.physReg().byte() == 0 &&
        (!hi.isConstant() || (hi.constantValue() && (!Operand::c32(hi.constantValue()).isLiteral() ||
                                                     ctx->program->gfx_level >= GFX10)))) {
       if (hi.isConstant())
@@ -1653,17 +1628,15 @@ do_pack_2x16(lower_context* ctx, Builder
       return;
    }
 
-   if (ctx->program->gfx_level >= GFX10 && !lo.constantEquals(0) && !hi.constantEquals(0) &&
-       !both_ops_are_sgpr) {
+   if (ctx->program->gfx_level >= GFX10 && !lo.constantEquals(0) && !hi.constantEquals(0)) {
       uint8_t swiz[4];
       Operand ops[2] = {lo, hi};
       for (unsigned i = 0; i < 2; i++) {
          ops[i] =
             ops[i].isConstant() ? Operand::c32((int32_t)(int16_t)ops[i].constantValue()) : ops[i];
 
-         unsigned opsel = i ? opsel_hi : opsel_lo;
-         swiz[i * 2 + 0] = i * 4 + opsel * 2;
-         swiz[i * 2 + 1] = i * 4 + opsel * 2 + 1;
+         swiz[i * 2 + 0] = i * 4 + ops[i].physReg().byte();
+         swiz[i * 2 + 1] = i * 4 + ops[i].physReg().byte() + 1;
 
          if (ops[i].isLiteral()) {
             Operand b0 = Operand::c32((int32_t)(int8_t)ops[i].constantValue());
@@ -1694,7 +1667,7 @@ do_pack_2x16(lower_context* ctx, Builder
 
    if (lo.isConstant()) {
       /* move hi and zero low bits */
-      if (!opsel_hi)
+      if (hi.physReg().byte() == 0)
          bld.vop2(aco_opcode::v_lshlrev_b32, def_hi, Operand::c32(16u), hi);
       else
          bld.vop2(aco_opcode::v_and_b32, def_hi, Operand::c32(~0xFFFFu), hi);
@@ -1705,7 +1678,7 @@ do_pack_2x16(lower_context* ctx, Builder
    }
    if (hi.isConstant()) {
       /* move lo and zero high bits */
-      if (opsel_lo)
+      if (lo.physReg().byte() == 2)
          bld.vop2(aco_opcode::v_lshrrev_b32, def_lo, Operand::c32(16u), lo);
       else if (ctx->program->gfx_level >= GFX11)
          bld.vop1(aco_opcode::v_cvt_u32_u16, def, lo);
@@ -1719,47 +1692,41 @@ do_pack_2x16(lower_context* ctx, Builder
 
    if (lo.physReg().reg() == def.physReg().reg()) {
       /* lo is in the high bits of def */
-      assert(opsel_lo);
+      assert(lo.physReg().byte() == 2);
       bld.vop2(aco_opcode::v_lshrrev_b32, def_lo, Operand::c32(16u), lo);
-      lo = Operand(def.physReg(), v2b);
-   } else if (hi.physReg().reg() == def.physReg().reg()) {
+      lo.setFixed(def.physReg());
+   } else if (hi.physReg() == def.physReg()) {
       /* hi is in the low bits of def */
-      assert(!opsel_hi);
+      assert(hi.physReg().byte() == 0);
       bld.vop2(aco_opcode::v_lshlrev_b32, def_hi, Operand::c32(16u), hi);
-      hi = Operand(def.physReg().advance(2), v2b);
+      hi.setFixed(def.physReg().advance(2));
    } else if (ctx->program->gfx_level >= GFX8) {
-      /* Either lo or hi can be placed with just a v_mov. SDWA is not needed, because
-       * op.physReg().byte() == def.physReg().byte() and the other half will be overwritten.
-       */
-      assert(!opsel_lo || opsel_hi);
-      Operand& op = !opsel_lo ? lo : hi;
+      /* Either lo or hi can be placed with just a v_mov. */
+      assert(lo.physReg().byte() == 0 || hi.physReg().byte() == 2);
+      Operand& op = lo.physReg().byte() == 0 ? lo : hi;
       PhysReg reg = def.physReg().advance(op.physReg().byte());
       bld.vop1(aco_opcode::v_mov_b32, Definition(reg, v2b), op);
-      op = Operand(reg, v2b);
+      op.setFixed(reg);
    }
 
-   /* Either hi or lo are already placed correctly. */
-   bool opsel = lo.physReg() == def.physReg() ? opsel_hi : opsel_lo;
-   Operand op = lo.physReg() == def.physReg() ? hi : lo;
-   def = lo.physReg() == def.physReg() ? def_hi : def_lo;
+   /* either hi or lo are already placed correctly */
    if (ctx->program->gfx_level >= GFX11) {
-      Instruction* instr = bld.vop1(aco_opcode::v_mov_b16, def, op);
-      instr->valu().opsel[0] = opsel;
-      instr->valu().opsel[3] = def.physReg().byte();
-      if (op.isOfType(RegType::sgpr) && opsel)
-         instr->format = asVOP3(instr->format);
+      if (lo.physReg().reg() == def.physReg().reg())
+         emit_v_mov_b16(bld, def_hi, hi);
+      else
+         emit_v_mov_b16(bld, def_lo, lo);
    } else {
-      Instruction* instr = bld.vop1_sdwa(aco_opcode::v_mov_b32, def, op);
-      if (op.isOfType(RegType::sgpr))
-         instr->sdwa().sel[0] = opsel ? SubdwordSel::uword1 : SubdwordSel::uword0;
+      if (lo.physReg().reg() == def.physReg().reg())
+         bld.vop1_sdwa(aco_opcode::v_mov_b32, def_hi, hi);
+      else
+         bld.vop1_sdwa(aco_opcode::v_mov_b32, def_lo, lo);
    }
 }
 
 void
-try_coalesce_copies(lower_context* ctx, std::map<PhysReg, copy_operation>& copy_map,
-                    copy_operation& copy)
+try_coalesce_copies(lower_context* ctx, CopyMap& copy_map, size_t& i)
 {
-   // TODO try more relaxed alignment for subdword copies
+   copy_operation& copy = copy_map[i].second;
    unsigned next_def_align = util_next_power_of_two(copy.bytes + 1);
    unsigned next_op_align = next_def_align;
    if (copy.def.regClass().type() == RegType::vgpr)
@@ -1771,9 +1738,14 @@ try_coalesce_copies(lower_context* ctx,
        (!copy.op.isConstant() && copy.op.physReg().reg_b % next_op_align))
       return;
 
-   auto other = copy_map.find(copy.def.physReg().advance(copy.bytes));
-   if (other == copy_map.end() || copy.bytes + other->second.bytes > 8 ||
-       copy.op.isConstant() != other->second.op.isConstant())
+   PhysReg other_reg = copy.def.physReg().advance(copy.bytes);
+   auto other_it = std::find_if(copy_map.begin(), copy_map.end(),
+                                [other_reg](const std::pair<PhysReg, copy_operation>& entry) {
+                                   return entry.first == other_reg;
+                                });
+
+   if (other_it == copy_map.end() || copy.bytes + other_it->second.bytes > 8 ||
+       copy.op.isConstant() != other_it->second.op.isConstant())
       return;
 
    /* don't create 64-bit copies before GFX10 */
@@ -1781,10 +1753,10 @@ try_coalesce_copies(lower_context* ctx,
        ctx->program->gfx_level < GFX10)
       return;
 
-   unsigned new_size = copy.bytes + other->second.bytes;
+   unsigned new_size = copy.bytes + other_it->second.bytes;
    if (copy.op.isConstant()) {
       uint64_t val =
-         copy.op.constantValue64() | (other->second.op.constantValue64() << (copy.bytes * 8u));
+         copy.op.constantValue64() | (other_it->second.op.constantValue64() << (copy.bytes * 8u));
       if (!util_is_power_of_two_or_zero(new_size))
          return;
       if (!Operand::is_constant_representable(val, new_size, true,
@@ -1792,191 +1764,219 @@ try_coalesce_copies(lower_context* ctx,
          return;
       copy.op = Operand::get_const(ctx->program->gfx_level, val, new_size);
    } else {
-      if (other->second.op.physReg() != copy.op.physReg().advance(copy.bytes))
+      if (other_it->second.op.physReg() != copy.op.physReg().advance(copy.bytes))
          return;
       copy.op = Operand(copy.op.physReg(), copy.op.regClass().resize(new_size));
    }
 
    copy.bytes = new_size;
    copy.def = Definition(copy.def.physReg(), copy.def.regClass().resize(copy.bytes));
-   copy_map.erase(other);
+
+   size_t other_idx = std::distance(copy_map.begin(), other_it);
+   copy_map.erase(other_it);
+   if (other_idx < i)
+      i--;
 }
 
 void
-handle_operands(std::map<PhysReg, copy_operation>& copy_map, lower_context* ctx,
-                amd_gfx_level gfx_level, Pseudo_instruction* pi)
+handle_operands(CopyMap& copy_map, lower_context* ctx, amd_gfx_level gfx_level,
+                Pseudo_instruction* pi)
 {
+   std::sort(copy_map.begin(), copy_map.end(),
+             [](const std::pair<PhysReg, copy_operation>& a,
+                const std::pair<PhysReg, copy_operation>& b) { return a.first < b.first; });
+
    Builder bld(ctx->program, &ctx->instructions);
    unsigned num_instructions_before = ctx->instructions.size();
-   aco_ptr<Instruction> mov;
    bool writes_scc = false;
 
    /* count the number of uses for each dst reg */
-   for (auto it = copy_map.begin(); it != copy_map.end();) {
-
-      if (it->second.def.physReg() == scc)
+   for (size_t i = 0; i < copy_map.size();) {
+      if (copy_map[i].second.def.physReg() == scc)
          writes_scc = true;
 
-      assert(!pi->needs_scratch_reg || it->second.def.physReg() != pi->scratch_sgpr);
+      assert(!pi->needs_scratch_reg || copy_map[i].second.def.physReg() != pi->scratch_sgpr);
 
       /* if src and dst reg are the same, remove operation */
-      if (it->first == it->second.op.physReg()) {
-         it = copy_map.erase(it);
+      if (copy_map[i].first == copy_map[i].second.op.physReg()) {
+         copy_map.erase(copy_map.begin() + i);
          continue;
       }
 
       /* split large copies */
-      if (it->second.bytes > 8) {
-         assert(!it->second.op.isConstant());
-         assert(!it->second.def.regClass().is_subdword());
-         RegClass rc = it->second.def.regClass().resize(it->second.def.bytes() - 8);
-         Definition hi_def = Definition(PhysReg{it->first + 2}, rc);
-         rc = it->second.op.regClass().resize(it->second.op.bytes() - 8);
-         Operand hi_op = Operand(PhysReg{it->second.op.physReg() + 2}, rc);
-         copy_operation copy = {hi_op, hi_def, it->second.bytes - 8};
-         copy_map[hi_def.physReg()] = copy;
-         assert(it->second.op.physReg().byte() == 0 && it->second.def.physReg().byte() == 0);
-         it->second.op = Operand(it->second.op.physReg(), it->second.op.regClass().resize(8));
-         it->second.def = Definition(it->second.def.physReg(), it->second.def.regClass().resize(8));
-         it->second.bytes = 8;
+      if (copy_map[i].second.bytes > 8) {
+         assert(!copy_map[i].second.op.isConstant());
+         assert(!copy_map[i].second.def.regClass().is_subdword());
+         RegClass rc =
+            copy_map[i].second.def.regClass().resize(copy_map[i].second.def.bytes() - 8);
+         Definition hi_def = Definition(PhysReg{copy_map[i].first + 2}, rc);
+         rc = copy_map[i].second.op.regClass().resize(copy_map[i].second.op.bytes() - 8);
+         Operand hi_op = Operand(PhysReg{copy_map[i].second.op.physReg() + 2}, rc);
+         copy_operation copy = {hi_op, hi_def, copy_map[i].second.bytes - 8};
+
+         auto hi_it = std::find_if(copy_map.begin(), copy_map.end(),
+                                   [hi_def](const std::pair<PhysReg, copy_operation>& entry) {
+                                      return entry.first == hi_def.physReg();
+                                   });
+         if (hi_it != copy_map.end()) {
+            hi_it->second = copy;
+         } else {
+            copy_map.emplace_back(hi_def.physReg(), copy);
+         }
+
+         assert(copy_map[i].second.op.physReg().byte() == 0 &&
+                copy_map[i].second.def.physReg().byte() == 0);
+         copy_map[i].second.op =
+            Operand(copy_map[i].second.op.physReg(), copy_map[i].second.op.regClass().resize(8));
+         copy_map[i].second.def =
+            Definition(copy_map[i].second.def.physReg(), copy_map[i].second.def.regClass().resize(8));
+         copy_map[i].second.bytes = 8;
       }
 
-      try_coalesce_copies(ctx, copy_map, it->second);
+      try_coalesce_copies(ctx, copy_map, i);
 
       /* check if the definition reg is used by another copy operation */
-      for (std::pair<const PhysReg, copy_operation>& copy : copy_map) {
+      for (const auto& copy : copy_map) {
          if (copy.second.op.isConstant())
             continue;
-         for (uint16_t i = 0; i < it->second.bytes; i++) {
-            /* distance might underflow */
-            unsigned distance = it->first.reg_b + i - copy.second.op.physReg().reg_b;
+         for (uint16_t j = 0; j < copy_map[i].second.bytes; j++) {
+            unsigned distance = copy_map[i].first.reg_b + j - copy.second.op.physReg().reg_b;
             if (distance < copy.second.bytes)
-               it->second.uses[i] += 1;
+               copy_map[i].second.uses[j] += 1;
          }
       }
 
-      ++it;
+      ++i;
    }
 
    /* first, handle paths in the location transfer graph */
    bool preserve_scc = pi->needs_scratch_reg && pi->scratch_sgpr != scc && !writes_scc;
    bool skip_partial_copies = true;
-   for (auto it = copy_map.begin();;) {
+   for (size_t i = 0;;) {
       if (copy_map.empty()) {
          ctx->program->statistics.copies += ctx->instructions.size() - num_instructions_before;
          return;
       }
-      if (it == copy_map.end()) {
+      if (i >= copy_map.size()) {
          if (!skip_partial_copies)
             break;
          skip_partial_copies = false;
-         it = copy_map.begin();
+         i = 0;
       }
 
       /* check if we can pack one register at once */
-      if (it->first.byte() == 0 && it->second.bytes == 2) {
-         PhysReg reg_hi = it->first.advance(2);
-         std::map<PhysReg, copy_operation>::iterator other = copy_map.find(reg_hi);
+      if (copy_map[i].first.byte() == 0 && copy_map[i].second.bytes == 2) {
+         PhysReg reg_hi = copy_map[i].first.advance(2);
+         auto other = std::find_if(copy_map.begin(), copy_map.end(),
+                                   [reg_hi](const std::pair<PhysReg, copy_operation>& entry) {
+                                      return entry.first == reg_hi;
+                                   });
          if (other != copy_map.end() && other->second.bytes == 2) {
-            /* check if the target register is otherwise unused */
-            bool unused_lo = !it->second.is_used || (it->second.is_used == 0x0101 &&
-                                                     other->second.op.physReg() == it->first);
-            bool unused_hi = !other->second.is_used ||
-                             (other->second.is_used == 0x0101 && it->second.op.physReg() == reg_hi);
+            bool unused_lo = !copy_map[i].second.is_used ||
+                             (copy_map[i].second.is_used == 0x0101 &&
+                              other->second.op.physReg() == copy_map[i].first);
+            bool unused_hi = !other->second.is_used || (other->second.is_used == 0x0101 &&
+                                                        copy_map[i].second.op.physReg() == reg_hi);
             if (unused_lo && unused_hi) {
-               Operand lo = it->second.op;
+               Operand lo = copy_map[i].second.op;
                Operand hi = other->second.op;
-               do_pack_2x16(ctx, bld, Definition(it->first, v1), lo, hi);
-               copy_map.erase(it);
-               copy_map.erase(other);
+               do_pack_2x16(ctx, bld, Definition(copy_map[i].first, v1), lo, hi);
 
-               for (std::pair<const PhysReg, copy_operation>& other2 : copy_map) {
-                  for (uint16_t i = 0; i < other2.second.bytes; i++) {
-                     /* distance might underflow */
-                     unsigned distance_lo = other2.first.reg_b + i - lo.physReg().reg_b;
-                     unsigned distance_hi = other2.first.reg_b + i - hi.physReg().reg_b;
+               size_t other_idx = std::distance(copy_map.begin(), other);
+               if (other_idx > i) {
+                  copy_map.erase(other);
+                  copy_map.erase(copy_map.begin() + i);
+               } else {
+                  copy_map.erase(copy_map.begin() + i);
+                  copy_map.erase(copy_map.begin() + other_idx);
+               }
+
+               for (auto& other2 : copy_map) {
+                  for (uint16_t j = 0; j < other2.second.bytes; j++) {
+                     unsigned distance_lo = other2.first.reg_b + j - lo.physReg().reg_b;
+                     unsigned distance_hi = other2.first.reg_b + j - hi.physReg().reg_b;
                      if (distance_lo < 2 || distance_hi < 2)
-                        other2.second.uses[i] -= 1;
+                        other2.second.uses[j] -= 1;
                   }
                }
-               it = copy_map.begin();
+               i = 0;
                continue;
             }
          }
       }
 
-      /* optimize constant copies to aligned sgpr pair that's otherwise unused. */
-      if (it->first <= exec && (it->first % 2) == 0 && it->second.bytes == 4 &&
-          it->second.op.isConstant() && !it->second.is_used) {
-         PhysReg reg_hi = it->first.advance(4);
-         std::map<PhysReg, copy_operation>::iterator other = copy_map.find(reg_hi);
-         if (other != copy_map.end() && other->second.bytes == 4 && other->second.op.isConstant() &&
-             !other->second.is_used) {
-            uint64_t constant =
-               it->second.op.constantValue64() | (other->second.op.constantValue64() << 32);
-            copy_constant_sgpr(bld, Definition(it->first, s2), constant);
-            copy_map.erase(it);
-            copy_map.erase(other);
-            it = copy_map.begin();
+      /* optimize constant copies to aligned sgpr pair that's otherwise unused */
+      if (copy_map[i].first <= exec && (copy_map[i].first % 2) == 0 &&
+          copy_map[i].second.bytes == 4 && copy_map[i].second.op.isConstant() &&
+          !copy_map[i].second.is_used) {
+         PhysReg reg_hi = copy_map[i].first.advance(4);
+         auto other = std::find_if(copy_map.begin(), copy_map.end(),
+                                   [reg_hi](const std::pair<PhysReg, copy_operation>& entry) {
+                                      return entry.first == reg_hi;
+                                   });
+         if (other != copy_map.end() && other->second.bytes == 4 &&
+             other->second.op.isConstant() && !other->second.is_used) {
+            uint64_t constant = copy_map[i].second.op.constantValue64() |
+                                (other->second.op.constantValue64() << 32);
+            copy_constant_sgpr(bld, Definition(copy_map[i].first, s2), constant);
+
+            size_t other_idx = std::distance(copy_map.begin(), other);
+            if (other_idx > i) {
+               copy_map.erase(other);
+               copy_map.erase(copy_map.begin() + i);
+            } else {
+               copy_map.erase(copy_map.begin() + i);
+               copy_map.erase(copy_map.begin() + other_idx);
+            }
+            i = 0;
             continue;
          }
       }
 
       /* find portions where the target reg is not used as operand for any other copy */
-      if (it->second.is_used) {
-         if (it->second.op.isConstant() || skip_partial_copies) {
-            /* we have to skip constants until is_used=0.
-             * we also skip partial copies at the beginning to help coalescing */
-            ++it;
+      if (copy_map[i].second.is_used) {
+         if (copy_map[i].second.op.isConstant() || skip_partial_copies) {
+            ++i;
             continue;
          }
 
          unsigned has_zero_use_bytes = 0;
-         for (unsigned i = 0; i < it->second.bytes; i++)
-            has_zero_use_bytes |= (it->second.uses[i] == 0) << i;
+         for (unsigned j = 0; j < copy_map[i].second.bytes; j++)
+            has_zero_use_bytes |= (copy_map[i].second.uses[j] == 0) << j;
 
          if (has_zero_use_bytes) {
-            /* Skipping partial copying and doing a v_swap_b32 and then fixup
-             * copies is usually beneficial for sub-dword copies, but if doing
-             * a partial copy allows further copies, it should be done instead. */
             bool partial_copy = (has_zero_use_bytes == 0xf) || (has_zero_use_bytes == 0xf0);
-            for (std::pair<const PhysReg, copy_operation>& copy : copy_map) {
+            for (const auto& copy : copy_map) {
                if (partial_copy)
                   break;
-               for (uint16_t i = 0; i < copy.second.bytes; i++) {
-                  /* distance might underflow */
-                  unsigned distance = copy.first.reg_b + i - it->second.op.physReg().reg_b;
-                  if (distance < it->second.bytes && copy.second.uses[i] == 1 &&
-                      !it->second.uses[distance])
+               for (uint16_t j = 0; j < copy.second.bytes; j++) {
+                  unsigned distance = copy.first.reg_b + j - copy_map[i].second.op.physReg().reg_b;
+                  if (distance < copy_map[i].second.bytes && copy.second.uses[j] == 1 &&
+                      !copy_map[i].second.uses[distance])
                      partial_copy = true;
                }
             }
 
             if (!partial_copy) {
-               ++it;
+               ++i;
                continue;
             }
          } else {
-            /* full target reg is used: register swapping needed */
-            ++it;
+            ++i;
             continue;
          }
       }
 
-      bool did_copy = do_copy(ctx, bld, it->second, &preserve_scc, pi->scratch_sgpr);
+      bool did_copy = do_copy(ctx, bld, copy_map[i].second, &preserve_scc, pi->scratch_sgpr);
       skip_partial_copies = did_copy;
-      std::pair<PhysReg, copy_operation> copy = *it;
+      std::pair<PhysReg, copy_operation> copy = copy_map[i];
 
-      if (it->second.is_used == 0) {
-         /* the target reg is not used as operand for any other copy, so we
-          * copied to all of it */
-         copy_map.erase(it);
-         it = copy_map.begin();
+      if (copy_map[i].second.is_used == 0) {
+         copy_map.erase(copy_map.begin() + i);
+         i = 0;
       } else {
-         /* we only performed some portions of this copy, so split it to only
-          * leave the portions that still need to be done */
-         copy_operation original = it->second; /* the map insertion below can overwrite this */
-         copy_map.erase(it);
+         copy_operation original = copy_map[i].second;
+         copy_map.erase(copy_map.begin() + i);
          for (unsigned offset = 0; offset < original.bytes;) {
             if (original.uses[offset] == 0) {
                offset++;
@@ -1987,71 +1987,69 @@ handle_operands(std::map<PhysReg, copy_o
             split_copy(ctx, offset, &def, &op, original, false, 8);
 
             copy_operation new_copy = {op, def, def.bytes()};
-            for (unsigned i = 0; i < new_copy.bytes; i++)
-               new_copy.uses[i] = original.uses[i + offset];
-            copy_map[def.physReg()] = new_copy;
+            for (unsigned j = 0; j < new_copy.bytes; j++)
+               new_copy.uses[j] = original.uses[j + offset];
+
+            auto it = std::find_if(copy_map.begin(), copy_map.end(),
+                                   [def](const std::pair<PhysReg, copy_operation>& entry) {
+                                      return entry.first == def.physReg();
+                                   });
+            if (it != copy_map.end()) {
+               it->second = new_copy;
+            } else {
+               copy_map.emplace_back(def.physReg(), new_copy);
+            }
 
             offset += def.bytes();
          }
 
-         it = copy_map.begin();
+         i = 0;
       }
 
-      /* Reduce the number of uses of the operand reg by one. Do this after
-       * splitting the copy or removing it in case the copy writes to it's own
-       * operand (for example, v[7:8] = v[8:9]) */
       if (did_copy && !copy.second.op.isConstant()) {
-         for (std::pair<const PhysReg, copy_operation>& other : copy_map) {
-            for (uint16_t i = 0; i < other.second.bytes; i++) {
-               /* distance might underflow */
-               unsigned distance = other.first.reg_b + i - copy.second.op.physReg().reg_b;
+         for (auto& other : copy_map) {
+            for (uint16_t j = 0; j < other.second.bytes; j++) {
+               unsigned distance = other.first.reg_b + j - copy.second.op.physReg().reg_b;
                if (distance < copy.second.bytes && !copy.second.uses[distance])
-                  other.second.uses[i] -= 1;
+                  other.second.uses[j] -= 1;
             }
          }
       }
    }
 
-   /* all target regs are needed as operand somewhere which means, all entries are part of a cycle */
+   /* all target regs are needed as operand somewhere - handle cycles */
    unsigned largest = 0;
-   for (const std::pair<const PhysReg, copy_operation>& op : copy_map)
+   for (const auto& op : copy_map)
       largest = MAX2(largest, op.second.bytes);
 
    while (!copy_map.empty()) {
-
-      /* Perform larger swaps first, because larger swaps swaps can make other
-       * swaps unnecessary. */
-      auto it = copy_map.begin();
-      for (auto it2 = copy_map.begin(); it2 != copy_map.end(); ++it2) {
-         if (it2->second.bytes > it->second.bytes) {
-            it = it2;
-            if (it->second.bytes == largest)
+      size_t swap_idx = 0;
+      for (size_t i = 0; i < copy_map.size(); ++i) {
+         if (copy_map[i].second.bytes > copy_map[swap_idx].second.bytes) {
+            swap_idx = i;
+            if (copy_map[swap_idx].second.bytes == largest)
                break;
          }
       }
 
-      /* should already be done */
-      assert(!it->second.op.isConstant());
-
-      assert(it->second.op.isFixed());
-      assert(it->second.def.regClass() == it->second.op.regClass());
+      assert(!copy_map[swap_idx].second.op.isConstant());
+      assert(copy_map[swap_idx].second.op.isFixed());
+      assert(copy_map[swap_idx].second.def.regClass() == copy_map[swap_idx].second.op.regClass());
 
-      if (it->first == it->second.op.physReg()) {
-         copy_map.erase(it);
+      if (copy_map[swap_idx].first == copy_map[swap_idx].second.op.physReg()) {
+         copy_map.erase(copy_map.begin() + swap_idx);
          continue;
       }
 
-      if (it->second.def.getTemp().type() == RegType::sgpr) {
-         assert(it->second.def.physReg() != pi->scratch_sgpr);
+      if (copy_map[swap_idx].second.def.getTemp().type() == RegType::sgpr) {
+         assert(copy_map[swap_idx].second.def.physReg() != pi->scratch_sgpr);
          assert(pi->needs_scratch_reg);
          assert(!preserve_scc || pi->scratch_sgpr != scc);
       }
 
-      /* to resolve the cycle, we have to swap the src reg with the dst reg */
-      copy_operation swap = it->second;
+      copy_operation swap = copy_map[swap_idx].second;
 
-      /* if this is self-intersecting, we have to split it because
-       * self-intersecting swaps don't make sense */
+      /* split self-intersecting swaps */
       PhysReg src = swap.op.physReg(), dst = swap.def.physReg();
       if (abs((int)src.reg_b - (int)dst.reg_b) < (int)swap.bytes) {
          unsigned offset = abs((int)src.reg_b - (int)dst.reg_b);
@@ -2063,79 +2061,104 @@ handle_operands(std::map<PhysReg, copy_o
          memcpy(remaining.uses, swap.uses + offset, remaining.bytes);
          remaining.op = Operand(src, swap.def.regClass().resize(remaining.bytes));
          remaining.def = Definition(dst, swap.def.regClass().resize(remaining.bytes));
-         copy_map[dst] = remaining;
+
+         auto it = std::find_if(copy_map.begin(), copy_map.end(),
+                                [dst](const std::pair<PhysReg, copy_operation>& entry) {
+                                   return entry.first == dst;
+                                });
+         if (it != copy_map.end()) {
+            it->second = remaining;
+         } else {
+            copy_map.emplace_back(dst, remaining);
+         }
 
          memset(swap.uses + offset, 0, swap.bytes - offset);
          swap.bytes = offset;
       }
 
-      /* GFX6-7 can only swap full registers */
       assert(ctx->program->gfx_level > GFX7 || (swap.bytes % 4) == 0);
 
       do_swap(ctx, bld, swap, preserve_scc, pi);
 
-      /* remove from map */
-      copy_map.erase(it);
+      {
+         auto it = std::find_if(copy_map.begin(), copy_map.end(),
+                                [swap](const std::pair<PhysReg, copy_operation>& entry) {
+                                   return entry.first == swap.def.physReg();
+                                });
+         assert(it != copy_map.end());
+         copy_map.erase(it);
+      }
 
-      /* change the operand reg of the target's uses and split uses if needed */
       uint32_t bytes_left = u_bit_consecutive(0, swap.bytes);
-      for (auto target = copy_map.begin(); target != copy_map.end(); ++target) {
-         if (target->second.op.physReg() == swap.def.physReg() &&
-             swap.bytes == target->second.bytes) {
-            target->second.op.setFixed(swap.op.physReg());
+      for (size_t i = 0; i < copy_map.size(); ++i) {
+         if (copy_map[i].second.op.physReg() == swap.def.physReg() &&
+             swap.bytes == copy_map[i].second.bytes) {
+            copy_map[i].second.op.setFixed(swap.op.physReg());
             break;
          }
 
          uint32_t imask =
             get_intersection_mask(swap.def.physReg().reg_b, swap.bytes,
-                                  target->second.op.physReg().reg_b, target->second.bytes);
+                                  copy_map[i].second.op.physReg().reg_b, copy_map[i].second.bytes);
 
          if (!imask)
             continue;
 
-         int offset = (int)target->second.op.physReg().reg_b - (int)swap.def.physReg().reg_b;
+         int offset = (int)copy_map[i].second.op.physReg().reg_b - (int)swap.def.physReg().reg_b;
 
-         /* split and update the middle (the portion that reads the swap's
-          * definition) to read the swap's operand instead */
-         int target_op_end = target->second.op.physReg().reg_b + target->second.bytes;
+         int target_op_end = copy_map[i].second.op.physReg().reg_b + copy_map[i].second.bytes;
          int swap_def_end = swap.def.physReg().reg_b + swap.bytes;
          int before_bytes = MAX2(-offset, 0);
          int after_bytes = MAX2(target_op_end - swap_def_end, 0);
-         int middle_bytes = target->second.bytes - before_bytes - after_bytes;
+         int middle_bytes = copy_map[i].second.bytes - before_bytes - after_bytes;
 
          if (after_bytes) {
             unsigned after_offset = before_bytes + middle_bytes;
             assert(after_offset > 0);
             copy_operation copy;
             copy.bytes = after_bytes;
-            memcpy(copy.uses, target->second.uses + after_offset, copy.bytes);
-            RegClass rc = target->second.op.regClass().resize(after_bytes);
-            copy.op = Operand(target->second.op.physReg().advance(after_offset), rc);
-            copy.def = Definition(target->second.def.physReg().advance(after_offset), rc);
-            copy_map[copy.def.physReg()] = copy;
+            memcpy(copy.uses, copy_map[i].second.uses + after_offset, copy.bytes);
+            RegClass rc = copy_map[i].second.op.regClass().resize(after_bytes);
+            copy.op = Operand(copy_map[i].second.op.physReg().advance(after_offset), rc);
+            copy.def = Definition(copy_map[i].second.def.physReg().advance(after_offset), rc);
+
+            auto it = std::find_if(copy_map.begin(), copy_map.end(),
+                                   [copy](const std::pair<PhysReg, copy_operation>& entry) {
+                                      return entry.first == copy.def.physReg();
+                                   });
+            if (it != copy_map.end())
+               it->second = copy;
+            else
+               copy_map.emplace_back(copy.def.physReg(), copy);
          }
 
          if (middle_bytes) {
             copy_operation copy;
             copy.bytes = middle_bytes;
-            memcpy(copy.uses, target->second.uses + before_bytes, copy.bytes);
-            RegClass rc = target->second.op.regClass().resize(middle_bytes);
+            memcpy(copy.uses, copy_map[i].second.uses + before_bytes, copy.bytes);
+            RegClass rc = copy_map[i].second.op.regClass().resize(middle_bytes);
             copy.op = Operand(swap.op.physReg().advance(MAX2(offset, 0)), rc);
-            copy.def = Definition(target->second.def.physReg().advance(before_bytes), rc);
-            copy_map[copy.def.physReg()] = copy;
+            copy.def = Definition(copy_map[i].second.def.physReg().advance(before_bytes), rc);
+
+            auto it = std::find_if(copy_map.begin(), copy_map.end(),
+                                   [copy](const std::pair<PhysReg, copy_operation>& entry) {
+                                      return entry.first == copy.def.physReg();
+                                   });
+            if (it != copy_map.end())
+               it->second = copy;
+            else
+               copy_map.emplace_back(copy.def.physReg(), copy);
          }
 
          if (before_bytes) {
-            copy_operation copy;
-            target->second.bytes = before_bytes;
-            RegClass rc = target->second.op.regClass().resize(before_bytes);
-            target->second.op = Operand(target->second.op.physReg(), rc);
-            target->second.def = Definition(target->second.def.physReg(), rc);
-            memset(target->second.uses + target->second.bytes, 0, 8 - target->second.bytes);
+            copy_map[i].second.bytes = before_bytes;
+            RegClass rc = copy_map[i].second.op.regClass().resize(before_bytes);
+            copy_map[i].second.op = Operand(copy_map[i].second.op.physReg(), rc);
+            copy_map[i].second.def = Definition(copy_map[i].second.def.physReg(), rc);
+            memset(copy_map[i].second.uses + copy_map[i].second.bytes, 0,
+                   8 - copy_map[i].second.bytes);
          }
 
-         /* break early since we know each byte of the swap's definition is used
-          * at most once */
          bytes_left &= ~imask;
          if (!bytes_left)
             break;
@@ -2145,8 +2168,8 @@ handle_operands(std::map<PhysReg, copy_o
 }
 
 void
-handle_operands_linear_vgpr(std::map<PhysReg, copy_operation>& copy_map, lower_context* ctx,
-                            amd_gfx_level gfx_level, Pseudo_instruction* pi)
+handle_operands_linear_vgpr(CopyMap& copy_map, lower_context* ctx, amd_gfx_level gfx_level,
+                            Pseudo_instruction* pi)
 {
    Builder bld(ctx->program, &ctx->instructions);
 
@@ -2157,7 +2180,7 @@ handle_operands_linear_vgpr(std::map<Phy
                                    RegClass::get(RegType::vgpr, copy.second.def.bytes()));
    }
 
-   std::map<PhysReg, copy_operation> second_map(copy_map);
+   CopyMap second_map = copy_map;
    handle_operands(second_map, ctx, gfx_level, pi);
 
    assert(pi->needs_scratch_reg);
@@ -2216,11 +2239,12 @@ lower_image_sample(lower_context* ctx, a
       }
    } else {
       PhysReg reg = linear_vgpr.physReg();
-      std::map<PhysReg, copy_operation> copy_operations;
+      CopyMap copy_operations;
+      copy_operations.reserve(non_mask_operands - 4);
       for (unsigned i = 4; i < non_mask_operands; i++) {
          Operand arg = instr->operands[i];
          Definition def(reg, RegClass::get(RegType::vgpr, arg.bytes()));
-         copy_operations[def.physReg()] = {arg, def, def.bytes()};
+         copy_operations.emplace_back(def.physReg(), copy_operation{arg, def, def.bytes()});
          reg = reg.advance(arg.bytes());
       }
       vaddr[num_vaddr++] = linear_vgpr;
@@ -2296,17 +2320,26 @@ lower_to_hw_instr(Program* program)
       pops_done_msg_bounds = gfx9_pops_done_msg_bounds(program);
    }
 
-   Block* discard_exit_block = NULL;
-   Block* discard_pops_done_and_exit_block = NULL;
+   Block* discard_exit_block = nullptr;
+   Block* discard_pops_done_and_exit_block = nullptr;
 
    int end_with_regs_block_index = -1;
 
-   for (int block_idx = program->blocks.size() - 1; block_idx >= 0; block_idx--) {
+   /* Hoist allocations outside the loop for better performance.
+    * CopyMap uses vector for cache-friendly iteration on small sets.
+    */
+   lower_context ctx;
+   ctx.program = program;
+   CopyMap copy_operations;
+   copy_operations.reserve(64);
+
+   for (int block_idx = static_cast<int>(program->blocks.size()) - 1; block_idx >= 0; block_idx--) {
       Block* block = &program->blocks[block_idx];
-      lower_context ctx;
-      ctx.program = program;
       ctx.block = block;
-      ctx.instructions.reserve(block->instructions.size());
+      ctx.instructions.clear();
+      /* Reserve with headroom for instruction expansion */
+      ctx.instructions.reserve(block->instructions.size() +
+                               std::max<size_t>(16, block->instructions.size() / 5));
       Builder bld(program, &ctx.instructions);
 
       for (size_t instr_idx = 0; instr_idx < block->instructions.size(); instr_idx++) {
@@ -2324,7 +2357,21 @@ lower_to_hw_instr(Program* program)
             bld.sopp(aco_opcode::s_sendmsg, sendmsg_ordered_ps_done);
          }
 
-         aco_ptr<Instruction> mov;
+         /* Fast-path: non-pseudo instructions can be emitted directly without expensive processing.
+          * This is the common case for VALU/SALU instructions and significantly reduces overhead.
+          * Note: Calls must go through slow path for stack maintenance.
+          */
+         if (instr->format != Format::PSEUDO && instr->format != Format::PSEUDO_BRANCH &&
+             instr->format != Format::PSEUDO_BARRIER &&
+             instr->format != Format::PSEUDO_REDUCTION && !instr->isCall()) {
+
+            if (instr->isMIMG() && instr->mimg().strict_wqm) {
+               lower_image_sample(&ctx, instr);
+            }
+            ctx.instructions.emplace_back(std::move(instr));
+            continue;
+         }
+
          if (instr->isPseudo() && instr->opcode != aco_opcode::p_unit_test &&
              instr->opcode != aco_opcode::p_debug_info) {
             Pseudo_instruction* pi = &instr->pseudo();
@@ -2338,9 +2385,12 @@ lower_to_hw_instr(Program* program)
                if (reg == def.physReg())
                   break;
 
-               RegClass rc_op = RegClass::get(instr->operands[0].regClass().type(), def.bytes());
-               std::map<PhysReg, copy_operation> copy_operations;
-               copy_operations[def.physReg()] = {Operand(reg, rc_op), def, def.bytes()};
+               RegClass op_rc = def.regClass().is_subdword()
+                                   ? def.regClass()
+                                   : RegClass(instr->operands[0].getTemp().type(), def.size());
+               copy_operations.clear();
+               copy_operations.emplace_back(def.physReg(),
+                                            copy_operation{Operand(reg, op_rc), def, def.bytes()});
                handle_operands(copy_operations, &ctx, program->gfx_level, pi);
                break;
             }
@@ -2349,52 +2399,57 @@ lower_to_hw_instr(Program* program)
                if (instr->operands.empty())
                   break;
 
-               std::map<PhysReg, copy_operation> copy_operations;
+               copy_operations.clear();
                PhysReg reg = instr->definitions[0].physReg();
 
                for (const Operand& op : instr->operands) {
                   RegClass rc = RegClass::get(instr->definitions[0].regClass().type(), op.bytes());
                   if (op.isConstant()) {
                      const Definition def = Definition(reg, rc);
-                     copy_operations[reg] = {op, def, op.bytes()};
+                     copy_operations.emplace_back(reg, copy_operation{op, def, op.bytes()});
                      reg.reg_b += op.bytes();
                      continue;
                   }
                   if (op.isUndefined()) {
-                     // TODO: coalesce subdword copies if dst byte is 0
                      reg.reg_b += op.bytes();
                      continue;
                   }
 
                   RegClass rc_def = op.regClass().is_subdword() ? op.regClass() : rc;
                   const Definition def = Definition(reg, rc_def);
-                  copy_operations[def.physReg()] = {op, def, op.bytes()};
+                  copy_operations.emplace_back(def.physReg(),
+                                               copy_operation{op, def, op.bytes()});
                   reg.reg_b += op.bytes();
                }
                handle_operands(copy_operations, &ctx, program->gfx_level, pi);
                break;
             }
             case aco_opcode::p_split_vector: {
-               std::map<PhysReg, copy_operation> copy_operations;
+               copy_operations.clear();
                PhysReg reg = instr->operands[0].physReg();
 
                for (const Definition& def : instr->definitions) {
-                  RegClass rc_op = RegClass::get(instr->operands[0].regClass().type(), def.bytes());
+                  RegClass rc_op = def.regClass().is_subdword()
+                                      ? def.regClass()
+                                      : instr->operands[0].getTemp().regClass().resize(def.bytes());
                   const Operand op = Operand(reg, rc_op);
-                  copy_operations[def.physReg()] = {op, def, def.bytes()};
+                  copy_operations.emplace_back(def.physReg(),
+                                               copy_operation{op, def, def.bytes()});
                   reg.reg_b += def.bytes();
                }
                handle_operands(copy_operations, &ctx, program->gfx_level, pi);
                break;
             }
             case aco_opcode::p_parallelcopy: {
-               std::map<PhysReg, copy_operation> copy_operations;
+               copy_operations.clear();
                bool linear_vgpr = false;
                bool non_linear_vgpr = false;
                for (unsigned j = 0; j < instr->operands.size(); j++) {
                   assert(instr->definitions[j].bytes() == instr->operands[j].bytes());
-                  copy_operations[instr->definitions[j].physReg()] = {
-                     instr->operands[j], instr->definitions[j], instr->operands[j].bytes()};
+                  copy_operations.emplace_back(
+                     instr->definitions[j].physReg(),
+                     copy_operation{instr->operands[j], instr->definitions[j],
+                                    instr->operands[j].bytes()});
                   linear_vgpr |= instr->definitions[j].regClass().is_linear_vgpr();
                   non_linear_vgpr |= !instr->definitions[j].regClass().is_linear_vgpr();
                }
@@ -2496,7 +2551,8 @@ lower_to_hw_instr(Program* program)
                for (unsigned i = 0; i < instr->operands[2].size(); i++) {
                   Operand src =
                      instr->operands[2].isConstant()
-                        ? Operand::c32(uint32_t(instr->operands[2].constantValue64() >> (32 * i)))
+                        ? Operand::c32(static_cast<uint32_t>(
+                             instr->operands[2].constantValue64() >> (32 * i)))
                         : Operand(PhysReg{instr->operands[2].physReg() + i}, s1);
                   bld.writelane(Definition(instr->operands[0].physReg(), v1), src,
                                 Operand::c32(instr->operands[1].constantValue() + i),
@@ -2515,9 +2571,11 @@ lower_to_hw_instr(Program* program)
             case aco_opcode::p_as_uniform: {
                if (instr->operands[0].isConstant() ||
                    instr->operands[0].regClass().type() == RegType::sgpr) {
-                  std::map<PhysReg, copy_operation> copy_operations;
-                  copy_operations[instr->definitions[0].physReg()] = {
-                     instr->operands[0], instr->definitions[0], instr->definitions[0].bytes()};
+                  copy_operations.clear();
+                  copy_operations.emplace_back(
+                     instr->definitions[0].physReg(),
+                     copy_operation{instr->operands[0], instr->definitions[0],
+                                    instr->definitions[0].bytes()});
                   handle_operands(copy_operations, &ctx, program->gfx_level, pi);
                } else {
                   assert(instr->operands[0].regClass().type() == RegType::vgpr);
@@ -2944,10 +3002,10 @@ lower_to_hw_instr(Program* program)
          } else if (instr->isReduction()) {
             Pseudo_reduction_instruction& reduce = instr->reduction();
             emit_reduction(&ctx, reduce.opcode, reduce.reduce_op, reduce.cluster_size,
-                           reduce.operands[1].physReg(),    // tmp
-                           reduce.definitions[1].physReg(), // stmp
-                           reduce.operands[2].physReg(),    // vtmp
-                           reduce.definitions[2].physReg(), // sitmp
+                           reduce.operands[1].physReg(),    /* tmp */
+                           reduce.definitions[1].physReg(), /* stmp */
+                           reduce.operands[2].physReg(),    /* vtmp */
+                           reduce.definitions[2].physReg(), /* sitmp */
                            reduce.operands[0], reduce.definitions[0]);
          } else if (instr->isBarrier()) {
             Pseudo_barrier_instruction& barrier = instr->barrier();
@@ -2965,9 +3023,6 @@ lower_to_hw_instr(Program* program)
             } else if (emit_s_barrier) {
                bld.sopp(aco_opcode::s_barrier);
             }
-         } else if (instr->isMIMG() && instr->mimg().strict_wqm) {
-            lower_image_sample(&ctx, instr);
-            ctx.instructions.emplace_back(std::move(instr));
          } else if (instr->isCall()) {
             unsigned extra_param_count = 2;
             PhysReg stack_reg = instr->operands[0].physReg();
@@ -3022,7 +3077,7 @@ lower_to_hw_instr(Program* program)
    /* If block with p_end_with_regs is not the last block (i.e. p_exit_early_if_not may append exit
     * block at last), create an exit block for it to branch to.
     */
-   int last_block_index = program->blocks.size() - 1;
+   int last_block_index = static_cast<int>(program->blocks.size()) - 1;
    if (end_with_regs_block_index >= 0 && end_with_regs_block_index != last_block_index) {
       Block* exit_block = program->create_and_insert_block();
       Block* end_with_regs_block = &program->blocks[end_with_regs_block_index];

--- a/src/amd/compiler/aco_ir.cpp	2025-05-31 22:57:26.003334290 +0200
+++ b/src/amd/compiler/aco_ir.cpp	2026-02-12 17:13:01.222104109 +0200
@@ -16,6 +16,13 @@
 #include "ac_descriptors.h"
 #include "amdgfxregs.h"
 
+#if defined(__x86_64__) || defined(_M_X64)
+#include <immintrin.h>
+#endif
+#include <cstring>
+#include <cassert>
+#include <algorithm>
+
 namespace aco {
 
 thread_local aco::monotonic_buffer_resource* instruction_buffer = nullptr;
@@ -281,28 +288,89 @@ get_sync_info(const Instruction* instr)
 bool
 can_use_SDWA(amd_gfx_level gfx_level, const aco_ptr<Instruction>& instr, bool pre_ra)
 {
-   if (!instr->isVALU())
+   if (!instr->isVALU()) [[unlikely]]
       return false;
 
-   if (gfx_level < GFX8 || gfx_level >= GFX11 || instr->isDPP() || instr->isVOP3P())
+   /* GFX9 (Vega 64) is the sweet spot for SDWA - full support.
+    * Check this first for Vega users.
+    */
+   if (gfx_level == GFX9) [[likely]] {
+      /* Fast path: already SDWA, definitely usable */
+      if (instr->isSDWA()) [[likely]]
+         return true;
+
+      if (instr->isDPP() || instr->isVOP3P()) [[unlikely]]
+         return false;
+
+      if (instr->isVOP3()) {
+         VALU_instruction& vop3 = instr->valu();
+         /* GFX9: VOPC + omod is valid, but clamp is not */
+         if (instr->format == Format::VOP3 && !(instr->isVOPC() && vop3.omod && !vop3.clamp))
+            return false;
+         if (!pre_ra && instr->definitions.size() >= 2)
+            return false;
+         /* GFX9 allows SGPR in src1+ */
+         for (unsigned i = 1; i < instr->operands.size(); i++) {
+            if (instr->operands[i].isLiteral())
+               return false;
+         }
+      }
+
+      if (!instr->definitions.empty() && instr->definitions[0].bytes() > 4 && !instr->isVOPC())
+         return false;
+
+      if (!instr->operands.empty()) {
+         if (instr->operands[0].isLiteral())
+            return false;
+         if (instr->operands[0].bytes() > 4)
+            return false;
+         if (instr->operands.size() > 1 && instr->operands[1].bytes() > 4)
+            return false;
+      }
+
+      /* v_mac/v_fmac need GFX8 for SDWA, but we're GFX9 here */
+      if (!pre_ra && instr->operands.size() >= 3) {
+         bool is_mac = instr->opcode == aco_opcode::v_mac_f32 ||
+                       instr->opcode == aco_opcode::v_mac_f16 ||
+                       instr->opcode == aco_opcode::v_fmac_f32 ||
+                       instr->opcode == aco_opcode::v_fmac_f16;
+         if (!is_mac)
+            return false;
+      }
+
+      return instr->opcode != aco_opcode::v_madmk_f32 &&
+             instr->opcode != aco_opcode::v_madak_f32 &&
+             instr->opcode != aco_opcode::v_madmk_f16 &&
+             instr->opcode != aco_opcode::v_madak_f16 &&
+             instr->opcode != aco_opcode::v_fmamk_f32 &&
+             instr->opcode != aco_opcode::v_fmaak_f32 &&
+             instr->opcode != aco_opcode::v_fmamk_f16 &&
+             instr->opcode != aco_opcode::v_fmaak_f16 &&
+             instr->opcode != aco_opcode::v_readfirstlane_b32 &&
+             instr->opcode != aco_opcode::v_clrexcp &&
+             instr->opcode != aco_opcode::v_swap_b32;
+   }
+
+   /* Non-GFX9 path: original logic */
+   if (gfx_level < GFX8 || gfx_level >= GFX11) [[unlikely]]
       return false;
 
-   if (instr->isSDWA())
+   if (instr->isDPP() || instr->isVOP3P()) [[unlikely]]
+      return false;
+
+   if (instr->isSDWA()) [[likely]]
       return true;
 
    if (instr->isVOP3()) {
       VALU_instruction& vop3 = instr->valu();
+      if (vop3.omod && gfx_level < GFX9)
+         return false;
       if (instr->format == Format::VOP3)
          return false;
       if (vop3.clamp && instr->isVOPC() && gfx_level != GFX8)
          return false;
-      if (vop3.omod && gfx_level < GFX9)
-         return false;
-
-      // TODO: return true if we know we will use vcc
       if (!pre_ra && instr->definitions.size() >= 2)
          return false;
-
       for (unsigned i = 1; i < instr->operands.size(); i++) {
          if (instr->operands[i].isLiteral())
             return false;
@@ -325,24 +393,31 @@ can_use_SDWA(amd_gfx_level gfx_level, co
          return false;
    }
 
-   bool is_mac = instr->opcode == aco_opcode::v_mac_f32 || instr->opcode == aco_opcode::v_mac_f16 ||
-                 instr->opcode == aco_opcode::v_fmac_f32 || instr->opcode == aco_opcode::v_fmac_f16;
+   bool is_mac = instr->opcode == aco_opcode::v_mac_f32 ||
+                 instr->opcode == aco_opcode::v_mac_f16 ||
+                 instr->opcode == aco_opcode::v_fmac_f32 ||
+                 instr->opcode == aco_opcode::v_fmac_f16;
 
    if (gfx_level != GFX8 && is_mac)
       return false;
 
-   // TODO: return true if we know we will use vcc
    if (!pre_ra && instr->isVOPC() && gfx_level == GFX8)
       return false;
+
    if (!pre_ra && instr->operands.size() >= 3 && !is_mac)
       return false;
 
-   return instr->opcode != aco_opcode::v_madmk_f32 && instr->opcode != aco_opcode::v_madak_f32 &&
-          instr->opcode != aco_opcode::v_madmk_f16 && instr->opcode != aco_opcode::v_madak_f16 &&
-          instr->opcode != aco_opcode::v_fmamk_f32 && instr->opcode != aco_opcode::v_fmaak_f32 &&
-          instr->opcode != aco_opcode::v_fmamk_f16 && instr->opcode != aco_opcode::v_fmaak_f16 &&
+   return instr->opcode != aco_opcode::v_madmk_f32 &&
+          instr->opcode != aco_opcode::v_madak_f32 &&
+          instr->opcode != aco_opcode::v_madmk_f16 &&
+          instr->opcode != aco_opcode::v_madak_f16 &&
+          instr->opcode != aco_opcode::v_fmamk_f32 &&
+          instr->opcode != aco_opcode::v_fmaak_f32 &&
+          instr->opcode != aco_opcode::v_fmamk_f16 &&
+          instr->opcode != aco_opcode::v_fmaak_f16 &&
           instr->opcode != aco_opcode::v_readfirstlane_b32 &&
-          instr->opcode != aco_opcode::v_clrexcp && instr->opcode != aco_opcode::v_swap_b32;
+          instr->opcode != aco_opcode::v_clrexcp &&
+          instr->opcode != aco_opcode::v_swap_b32;
 }
 
 /* updates "instr" and returns the old instruction (or NULL if no update was needed) */
@@ -453,39 +528,42 @@ opcode_supports_dpp(amd_gfx_level gfx_le
 bool
 can_use_DPP(amd_gfx_level gfx_level, const aco_ptr<Instruction>& instr, bool dpp8)
 {
+   if (!instr) [[unlikely]]
+      return false;
+
    assert(instr->isVALU() && !instr->operands.empty());
 
-   if (instr->isDPP())
+   if (instr->isDPP()) [[likely]]
       return instr->isDPP8() == dpp8;
 
-   if (instr->isSDWA() || instr->isVINTERP_INREG())
+   if (instr->isSDWA() || instr->isVINTERP_INREG()) [[unlikely]]
       return false;
 
-   if ((instr->format == Format::VOP3 || instr->isVOP3P()) && gfx_level < GFX11)
+   if ((instr->format == Format::VOP3 || instr->isVOP3P()) && gfx_level < GFX11) [[unlikely]]
       return false;
 
    if ((instr->isVOPC() || instr->definitions.size() > 1) && instr->definitions.back().isFixed() &&
-       instr->definitions.back().physReg() != vcc && gfx_level < GFX11)
+       instr->definitions.back().physReg() != vcc && gfx_level < GFX11) [[unlikely]]
       return false;
 
    if (instr->operands.size() >= 3 && instr->operands[2].isFixed() &&
        instr->operands[2].isOfType(RegType::sgpr) && instr->operands[2].physReg() != vcc &&
-       gfx_level < GFX11)
+       gfx_level < GFX11) [[unlikely]]
       return false;
 
    if (instr->isVOP3() && gfx_level < GFX11) {
       const VALU_instruction* vop3 = &instr->valu();
-      if (vop3->clamp || vop3->omod)
+      if (vop3->clamp || vop3->omod) [[unlikely]]
          return false;
-      if (dpp8)
+      if (dpp8) [[unlikely]]
          return false;
    }
 
    for (unsigned i = 0; i < instr->operands.size(); i++) {
-      if (instr->operands[i].isLiteral())
+      if (instr->operands[i].isLiteral()) [[unlikely]]
          return false;
       if (!instr->operands[i].isOfType(RegType::vgpr) &&
-          (i == 0 || (i == 1 && gfx_level < GFX11_5)))
+          (i == 0 || (i == 1 && gfx_level < GFX11_5))) [[unlikely]]
          return false;
    }
 
@@ -573,10 +651,22 @@ can_use_input_modifiers(amd_gfx_level gf
 bool
 can_use_opsel(amd_gfx_level gfx_level, aco_opcode op, int idx)
 {
+   if (gfx_level >= GFX11) {
+      return get_gfx11_true16_mask(op) & BITFIELD_BIT(idx == -1 ? 3 : idx);
+   }
+
    /* opsel is only GFX9+ */
    if (gfx_level < GFX9)
       return false;
 
+   /* VOP3P instructions handle modifiers differently */
+   if (static_cast<uint16_t>(instr_info.format[static_cast<int>(op)]) &
+       static_cast<uint16_t>(Format::VOP3P)) {
+      return false;
+   }
+
+   int check_idx = (idx < 0) ? 3 : idx;
+
    switch (op) {
    case aco_opcode::v_div_fixup_f16:
    case aco_opcode::v_fma_f16:
@@ -592,6 +682,14 @@ can_use_opsel(amd_gfx_level gfx_level, a
    case aco_opcode::v_max3_f16:
    case aco_opcode::v_max3_i16:
    case aco_opcode::v_max3_u16:
+   case aco_opcode::v_fma_legacy_f16:
+   case aco_opcode::v_mad_legacy_f16:
+   case aco_opcode::v_mad_legacy_i16:
+   case aco_opcode::v_mad_legacy_u16:
+   case aco_opcode::v_div_fixup_legacy_f16: return check_idx < 3;
+   case aco_opcode::v_mac_f16: return check_idx < 3;
+   case aco_opcode::v_mad_u32_u16:
+   case aco_opcode::v_mad_i32_i16: return check_idx < 2;
    case aco_opcode::v_minmax_f16:
    case aco_opcode::v_maxmin_f16:
    case aco_opcode::v_max_u16_e64:
@@ -608,26 +706,41 @@ can_use_opsel(amd_gfx_level gfx_level, a
    case aco_opcode::v_and_b16:
    case aco_opcode::v_or_b16:
    case aco_opcode::v_xor_b16:
-   case aco_opcode::v_mul_lo_u16_e64: return true;
+   case aco_opcode::v_mul_lo_u16_e64: return check_idx < 2;
+   case aco_opcode::v_add_f16:
+   case aco_opcode::v_sub_f16:
+   case aco_opcode::v_subrev_f16:
+   case aco_opcode::v_mul_f16:
+   case aco_opcode::v_max_f16:
+   case aco_opcode::v_min_f16:
+   case aco_opcode::v_ldexp_f16:
+   case aco_opcode::v_add_u16:
+   case aco_opcode::v_sub_u16:
+   case aco_opcode::v_subrev_u16:
+   case aco_opcode::v_mul_lo_u16:
+   case aco_opcode::v_lshlrev_b16:
+   case aco_opcode::v_lshrrev_b16:
+   case aco_opcode::v_ashrrev_i16:
+   case aco_opcode::v_max_u16:
+   case aco_opcode::v_max_i16:
+   case aco_opcode::v_min_u16:
+   case aco_opcode::v_min_i16: return check_idx < 2;
    case aco_opcode::v_pack_b32_f16:
    case aco_opcode::v_cvt_pknorm_i16_f16:
-   case aco_opcode::v_cvt_pknorm_u16_f16: return idx != -1;
-   case aco_opcode::v_mad_u32_u16:
-   case aco_opcode::v_mad_i32_i16: return idx >= 0 && idx < 2;
+   case aco_opcode::v_cvt_pknorm_u16_f16: return check_idx < 3;
    case aco_opcode::v_dot2_f16_f16:
-   case aco_opcode::v_dot2_bf16_bf16: return idx == -1 || idx == 2;
-   case aco_opcode::v_cndmask_b16: return idx != 2;
+   case aco_opcode::v_dot2_bf16_bf16: return check_idx == 2 || check_idx == 3;
+   case aco_opcode::v_cndmask_b16: return check_idx != 2;
    case aco_opcode::v_interp_p10_f16_f32_inreg:
-   case aco_opcode::v_interp_p10_rtz_f16_f32_inreg: return idx == 0 || idx == 2;
+   case aco_opcode::v_interp_p10_rtz_f16_f32_inreg: return check_idx == 0 || check_idx == 2;
    case aco_opcode::v_interp_p2_f16_f32_inreg:
-   case aco_opcode::v_interp_p2_rtz_f16_f32_inreg: return idx == -1 || idx == 0;
+   case aco_opcode::v_interp_p2_rtz_f16_f32_inreg: return check_idx == -1 || check_idx == 0;
    case aco_opcode::v_cvt_pk_fp8_f32:
    case aco_opcode::p_v_cvt_pk_fp8_f32_ovfl:
-   case aco_opcode::v_cvt_pk_bf8_f32: return idx == -1;
+   case aco_opcode::v_cvt_pk_bf8_f32: return check_idx == -1;
    case aco_opcode::v_alignbyte_b32:
-   case aco_opcode::v_alignbit_b32: return idx == 2;
-   default:
-      return gfx_level >= GFX11 && (get_gfx11_true16_mask(op) & BITFIELD_BIT(idx == -1 ? 3 : idx));
+   case aco_opcode::v_alignbit_b32: return check_idx == 2;
+   default: return false;
    }
 }
 
@@ -1245,6 +1358,42 @@ wait_imm::pack(enum amd_gfx_level gfx_le
 {
    uint16_t imm = 0;
    assert(exp == unset_counter || exp <= 0x7);
+
+   /* GFX9 (Vega) fast path - checked first for Vega 64 users.
+    * Per AMD ISA Manual 4.2.1, GFX9 s_waitcnt encoding:
+    * [3:0]   = VM_CNT[3:0]
+    * [6:4]   = EXP_CNT[2:0]
+    * [7]     = Reserved
+    * [11:8]  = LGKM_CNT[3:0]
+    * [13:12] = Reserved (set to 0b11 for "unset" sentinel)
+    * [15:14] = VM_CNT[5:4]
+    */
+   if (gfx_level == GFX9) [[likely]] {
+      assert(lgkm == unset_counter || lgkm <= 0xf);
+      assert(vm == unset_counter || vm <= 0x3f);
+
+#ifdef __BMI2__
+      /* PDEP: 3-cycle latency, 1/cycle throughput on Raptor Lake.
+       * Deposits bits from source to positions marked by mask.
+       * Eliminates 6 dependent shift/OR operations.
+       */
+      uint32_t vm_val = (vm == unset_counter) ? 0x3fu : static_cast<uint32_t>(vm);
+      uint32_t lgkm_val = (lgkm == unset_counter) ? 0xfu : static_cast<uint32_t>(lgkm);
+      uint32_t exp_val = (exp == unset_counter) ? 0x7u : static_cast<uint32_t>(exp);
+      imm = static_cast<uint16_t>(
+         _pdep_u32(vm_val, 0xC00Fu) |   /* vm[5:4][15:14], vm[3:0][3:0] */
+         _pdep_u32(lgkm_val, 0x0F00u) | /* lgkm[3:0][11:8] */
+         _pdep_u32(exp_val, 0x0070u));  /* exp[2:0][6:4] */
+#else
+      imm = ((vm & 0x30) << 10) | ((lgkm & 0xf) << 8) | ((exp & 0x7) << 4) | (vm & 0xf);
+#endif
+
+      if (lgkm == unset_counter)
+         imm |= 0x3000u;
+      return imm;
+   }
+
+   /* GFX10+ and GFX6-8 paths - less common for Vega 64 users */
    if (gfx_level >= GFX11) {
       assert(lgkm == unset_counter || lgkm <= 0x3f);
       assert(vm == unset_counter || vm <= 0x3f);
@@ -1253,21 +1402,18 @@ wait_imm::pack(enum amd_gfx_level gfx_le
       assert(lgkm == unset_counter || lgkm <= 0x3f);
       assert(vm == unset_counter || vm <= 0x3f);
       imm = ((vm & 0x30) << 10) | ((lgkm & 0x3f) << 8) | ((exp & 0x7) << 4) | (vm & 0xf);
-   } else if (gfx_level >= GFX9) {
-      assert(lgkm == unset_counter || lgkm <= 0xf);
-      assert(vm == unset_counter || vm <= 0x3f);
-      imm = ((vm & 0x30) << 10) | ((lgkm & 0xf) << 8) | ((exp & 0x7) << 4) | (vm & 0xf);
    } else {
+      /* GFX6-8 */
       assert(lgkm == unset_counter || lgkm <= 0xf);
       assert(vm == unset_counter || vm <= 0xf);
       imm = ((lgkm & 0xf) << 8) | ((exp & 0x7) << 4) | (vm & 0xf);
+      if (vm == unset_counter)
+         imm |= 0xC000u;
    }
-   if (gfx_level < GFX9 && vm == wait_imm::unset_counter)
-      imm |= 0xc000; /* should have no effect on pre-GFX9 and now we won't have to worry about the
-                        architecture when interpreting the immediate */
-   if (gfx_level < GFX10 && lgkm == wait_imm::unset_counter)
-      imm |= 0x3000; /* should have no effect on pre-GFX10 and now we won't have to worry about the
-                        architecture when interpreting the immediate */
+
+   if (gfx_level < GFX10 && lgkm == unset_counter)
+      imm |= 0x3000u;
+
    return imm;
 }
 
@@ -1365,8 +1511,9 @@ wait_imm::combine(const wait_imm& other)
 {
    bool changed = false;
    for (unsigned i = 0; i < wait_type_num; i++) {
-      if (other[i] < (*this)[i])
+      if (other[i] < (*this)[i]) {
          changed = true;
+      }
       (*this)[i] = std::min((*this)[i], other[i]);
    }
    return changed;
@@ -1479,34 +1626,37 @@ should_form_clause(const Instruction* a,
 aco::small_vec<uint32_t, 2>
 get_tied_defs(Instruction* instr)
 {
-   aco::small_vec<uint32_t, 2> ops;
-   if (instr->opcode == aco_opcode::v_interp_p2_f32 || instr->opcode == aco_opcode::v_mac_f32 ||
-       instr->opcode == aco_opcode::v_fmac_f32 || instr->opcode == aco_opcode::v_mac_f16 ||
-       instr->opcode == aco_opcode::v_fmac_f16 || instr->opcode == aco_opcode::v_mac_legacy_f32 ||
-       instr->opcode == aco_opcode::v_fmac_legacy_f32 ||
-       instr->opcode == aco_opcode::v_pk_fmac_f16 || instr->opcode == aco_opcode::v_writelane_b32 ||
-       instr->opcode == aco_opcode::v_writelane_b32_e64 ||
-       instr->opcode == aco_opcode::v_dot4c_i32_i8 || instr->opcode == aco_opcode::s_fmac_f32 ||
-       instr->opcode == aco_opcode::s_fmac_f16) {
-      ops.push_back(2);
-   } else if (instr->opcode == aco_opcode::s_addk_i32 || instr->opcode == aco_opcode::s_mulk_i32 ||
-              instr->opcode == aco_opcode::s_cmovk_i32 ||
-              instr->opcode == aco_opcode::ds_bvh_stack_push4_pop1_rtn_b32 ||
-              instr->opcode == aco_opcode::ds_bvh_stack_push8_pop1_rtn_b32 ||
-              instr->opcode == aco_opcode::ds_bvh_stack_push8_pop2_rtn_b64) {
-      ops.push_back(0);
-   } else if (instr->isMUBUF() && instr->definitions.size() == 1 &&
+      aco::small_vec<uint32_t, 2> ops;
+      if (instr->opcode == aco_opcode::v_interp_p2_f32 || instr->opcode == aco_opcode::v_mac_f32 ||
+            instr->opcode == aco_opcode::v_fmac_f32 || instr->opcode == aco_opcode::v_mac_f16 ||
+            instr->opcode == aco_opcode::v_fmac_f16 || instr->opcode == aco_opcode::v_mac_legacy_f32 ||
+            instr->opcode == aco_opcode::v_fmac_legacy_f32 ||
+            instr->opcode == aco_opcode::v_pk_fmac_f16 || instr->opcode == aco_opcode::v_writelane_b32 ||
+            instr->opcode == aco_opcode::v_writelane_b32_e64 ||
+            instr->opcode == aco_opcode::v_dot4c_i32_i8 || instr->opcode == aco_opcode::s_fmac_f32 ||
+            instr->opcode == aco_opcode::s_fmac_f16) {
+            ops.push_back(2);
+      } else if (instr->opcode == aco_opcode::s_addk_i32 || instr->opcode == aco_opcode::s_mulk_i32 ||
+                 instr->opcode == aco_opcode::s_cmovk_i32) {
+            /* These SOPK instructions have an implicit source operand which is the same as the destination. */
+            ops.push_back(0);
+      } else if (instr->opcode == aco_opcode::ds_bvh_stack_push4_pop1_rtn_b32 ||
+                 instr->opcode == aco_opcode::ds_bvh_stack_push8_pop1_rtn_b32 ||
+                 instr->opcode == aco_opcode::ds_bvh_stack_push8_pop2_rtn_b64) {
+            ops.push_back(0);
+      } else if (instr->isMUBUF() && instr->definitions.size() == 1 &&
               (instr_info.is_atomic[(int)instr->opcode] || instr->mubuf().tfe)) {
-      ops.push_back(3);
-   } else if (instr->isMIMG() && instr->definitions.size() == 1 &&
-              !instr->operands[2].isUndefined()) {
-      ops.push_back(2);
-   } else if (instr->opcode == aco_opcode::image_bvh8_intersect_ray) {
-      /* VADDR starts at 3. */
-      ops.push_back(3 + 4);
-      ops.push_back(3 + 7);
-   }
-   return ops;
+            ops.push_back(3);
+      } else if (instr->isMIMG() && instr->definitions.size() == 1 &&
+                 !instr->operands[2].isUndefined()) {
+            /* MIMG atomic instructions with a return value have the data source/destination tied. */
+            ops.push_back(2);
+      } else if (instr->opcode == aco_opcode::image_bvh8_intersect_ray) {
+            /* VADDR for this RT instruction has tied operands */
+            ops.push_back(3 + 4);
+            ops.push_back(3 + 7);
+      }
+      return ops;
 }
 
 uint8_t
@@ -1688,18 +1838,30 @@ create_instruction(aco_opcode opcode, Fo
                    uint32_t num_definitions)
 {
    size_t size = get_instr_data_size(format);
-   size_t total_size = size + num_operands * sizeof(Operand) + num_definitions * sizeof(Definition);
+   size_t total_size = size + num_operands * sizeof(Operand) +
+                       num_definitions * sizeof(Definition);
+
+   /* Alignment strategy for Raptor Lake (14700KF):
+    * - 64B: Cache line size, optimal for AVX-512 stores and prefetch
+    * - 16B: SSE alignment minimum for smaller allocations
+    * Per Intel Optimization Manual 2.1.5.4
+    */
+   size_t alignment = (total_size >= 64) ? 64 : 16;
+
+   void* data = instruction_buffer->allocate(total_size, alignment);
+
+   std::memset(data, 0, total_size);
 
-   void* data = instruction_buffer->allocate(total_size, alignof(uint32_t));
-   memset(data, 0, total_size);
-   Instruction* inst = (Instruction*)data;
+   Instruction* inst = static_cast<Instruction*>(data);
 
    inst->opcode = opcode;
    inst->format = format;
 
-   uint16_t operands_offset = size - offsetof(Instruction, operands);
+   uint16_t operands_offset = static_cast<uint16_t>(size - offsetof(Instruction, operands));
    inst->operands = aco::span<Operand>(operands_offset, num_operands);
-   uint16_t definitions_offset = (char*)inst->operands.end() - (char*)&inst->definitions;
+   uint16_t definitions_offset = static_cast<uint16_t>(
+      reinterpret_cast<const char*>(inst->operands.end()) -
+      reinterpret_cast<const char*>(&inst->definitions));
    inst->definitions = aco::span<Definition>(definitions_offset, num_definitions);
 
    return inst;

--- a/src/amd/compiler/aco_opcodes.py	2025-05-31 22:57:26.003334290 +0200
+++ b/src/amd/compiler/aco_opcodes.py	2026-01-24 00:30:01.222104109 +0200
@@ -261,6 +261,7 @@ F32 = SrcDestInfo(AcoBaseType.aco_base_t
 F64 = SrcDestInfo(AcoBaseType.aco_base_type_float, 64, 1, FixedReg.not_fixed, True)
 BF16 = SrcDestInfo(AcoBaseType.aco_base_type_bfloat, 16, 1, FixedReg.not_fixed, True)
 PkU16 = SrcDestInfo(AcoBaseType.aco_base_type_uint, 16, 2, FixedReg.not_fixed, False)
+PkI16 = SrcDestInfo(AcoBaseType.aco_base_type_int, 16, 2, FixedReg.not_fixed, False)
 PkF16 = SrcDestInfo(AcoBaseType.aco_base_type_float, 16, 2, FixedReg.not_fixed, True)
 PkF32 = SrcDestInfo(AcoBaseType.aco_base_type_float, 32, 2, FixedReg.not_fixed, False)
 PkBF16 = SrcDestInfo(AcoBaseType.aco_base_type_bfloat, 16, 2, FixedReg.not_fixed, True)
@@ -819,46 +820,50 @@ for (name, defs, ops, num, cls) in defau
 
 
 # SMEM instructions: sbase input (2 sgpr), potentially 2 offset inputs, 1 sdata input/output
-# Unlike GFX10, GFX10.3 does not have SMEM store, atomic or scratch instructions
 SMEM = {
-   ("s_load_dword",               op(0x00)), #s_load_b32 in GFX11
-   ("s_load_dwordx2",             op(0x01)), #s_load_b64 in GFX11
-   ("s_load_dwordx3",             op(gfx12=0x05)), #s_load_b96 in GFX12
-   ("s_load_dwordx4",             op(0x02)), #s_load_b128 in GFX11
-   ("s_load_dwordx8",             op(0x03)), #s_load_b256 in GFX11
-   ("s_load_dwordx16",            op(0x04)), #s_load_b512 in GFX11
-   ("s_load_sbyte",               op(gfx12=0x08)), #s_load_i8 in GFX12
-   ("s_load_ubyte",               op(gfx12=0x09)), #s_load_u8 in GFX12
-   ("s_load_sshort",              op(gfx12=0x0a)), #s_load_i16 in GFX12
-   ("s_load_ushort",              op(gfx12=0x0b)), #s_load_u16 in GFX12
+   ("s_load_dword",               op(0x00)),
+   ("s_load_dwordx2",             op(0x01)),
+   ("s_load_dwordx3",             op(gfx12=0x05)),
+   ("s_load_dwordx4",             op(0x02)),
+   ("s_load_dwordx8",             op(0x03)),
+   ("s_load_dwordx16",            op(0x04)),
+   ("s_load_sbyte",               op(gfx12=0x08)),
+   ("s_load_ubyte",               op(gfx12=0x09)),
+   ("s_load_sshort",              op(gfx12=0x0a)),
+   ("s_load_ushort",              op(gfx12=0x0b)),
    ("s_scratch_load_dword",       op(gfx9=0x05, gfx11=-1)),
    ("s_scratch_load_dwordx2",     op(gfx9=0x06, gfx11=-1)),
    ("s_scratch_load_dwordx4",     op(gfx9=0x07, gfx11=-1)),
-   ("s_buffer_load_dword",        op(0x08, gfx12=0x10)), #s_buffer_load_b32 in GFX11
-   ("s_buffer_load_dwordx2",      op(0x09, gfx12=0x11)), #s_buffer_load_b64 in GFX11
-   ("s_buffer_load_dwordx3",      op(gfx12=0x15)), #s_buffer_load_b96 in GFX12
-   ("s_buffer_load_dwordx4",      op(0x0a, gfx12=0x12)), #s_buffer_load_b128 in GFX11
-   ("s_buffer_load_dwordx8",      op(0x0b, gfx12=0x13)), #s_buffer_load_b256 in GFX11
-   ("s_buffer_load_dwordx16",     op(0x0c, gfx12=0x14)), #s_buffer_load_b512 in GFX11
-   ("s_buffer_load_sbyte",        op(gfx12=0x18)), #s_buffer_load_i8 in GFX12
-   ("s_buffer_load_ubyte",        op(gfx12=0x19)), #s_buffer_load_u8 in GFX12
-   ("s_buffer_load_sshort",       op(gfx12=0x1a)), #s_buffer_load_i16 in GFX12
-   ("s_buffer_load_ushort",       op(gfx12=0x1b)), #s_buffer_load_u16 in GFX12
+
+   ("s_buffer_load_dword",        op(0x08, gfx12=0x10)),
+   ("s_buffer_load_dwordx2",      op(0x09, gfx12=0x11)),
+   ("s_buffer_load_dwordx3",      op(gfx12=0x15)),
+   ("s_buffer_load_dwordx4",      op(0x0a, gfx12=0x12)),
+   ("s_buffer_load_dwordx8",      op(0x0b, gfx12=0x13)),
+   ("s_buffer_load_dwordx16",     op(0x0c, gfx12=0x14)),
+   ("s_buffer_load_sbyte",        op(gfx12=0x18)),
+   ("s_buffer_load_ubyte",        op(gfx12=0x19)),
+   ("s_buffer_load_sshort",       op(gfx12=0x1a)),
+   ("s_buffer_load_ushort",       op(gfx12=0x1b)),
    ("s_store_dword",              op(gfx8=0x10, gfx11=-1)),
    ("s_store_dwordx2",            op(gfx8=0x11, gfx11=-1)),
    ("s_store_dwordx4",            op(gfx8=0x12, gfx11=-1)),
+
    ("s_scratch_store_dword",      op(gfx9=0x15, gfx11=-1)),
    ("s_scratch_store_dwordx2",    op(gfx9=0x16, gfx11=-1)),
    ("s_scratch_store_dwordx4",    op(gfx9=0x17, gfx11=-1)),
+
    ("s_buffer_store_dword",       op(gfx8=0x18, gfx11=-1)),
    ("s_buffer_store_dwordx2",     op(gfx8=0x19, gfx11=-1)),
    ("s_buffer_store_dwordx4",     op(gfx8=0x1a, gfx11=-1)),
    ("s_gl1_inv",                  op(gfx8=0x1f, gfx11=0x20, gfx12=-1)),
-   ("s_dcache_inv",               op(0x1f, gfx8=0x20, gfx11=0x21)),
-   ("s_dcache_wb",                op(gfx8=0x21, gfx11=-1)),
+
+   ("s_dcache_inv",               op(0x1f, gfx8=0x20, gfx9=0x20, gfx10=0x20, gfx11=0x21)),
+   ("s_dcache_wb",                op(gfx8=0x21, gfx9=0x21, gfx10=0x21, gfx11=-1)),
+
    ("s_dcache_inv_vol",           op(gfx7=0x1d, gfx8=0x22, gfx10=-1)),
    ("s_dcache_wb_vol",            op(gfx8=0x23, gfx10=-1)),
-   ("s_memtime",                  op(0x1e, gfx8=0x24, gfx11=-1)), #GFX6-GFX10
+   ("s_memtime",                  op(0x1e, gfx8=0x24, gfx11=-1)),
    ("s_memrealtime",              op(gfx8=0x25, gfx11=-1)),
    ("s_atc_probe",                op(gfx8=0x26, gfx11=0x22)),
    ("s_atc_probe_buffer",         op(gfx8=0x27, gfx11=0x23)),
@@ -928,7 +933,6 @@ for (name, num) in SMEM:
 
 
 # VOP2 instructions: 2 inputs, 1 output (+ optional vcc)
-# TODO: misses some GFX6_7 opcodes which were shifted to VOP3 in GFX8
 VOP2 = {
    ("v_cndmask_b32",       dst(U32),      src(mods(U32), mods(U32), VCC), op(0x00, gfx10=0x01)),
    ("v_readlane_b32",      dst(U32),      src(U32, U32), op(0x01, gfx8=-1)),
@@ -936,19 +940,19 @@ VOP2 = {
    ("v_add_f32",           dst(F32),      src(F32, F32), op(0x03, gfx8=0x01, gfx10=0x03)),
    ("v_sub_f32",           dst(F32),      src(F32, F32), op(0x04, gfx8=0x02, gfx10=0x04)),
    ("v_subrev_f32",        dst(F32),      src(F32, F32), op(0x05, gfx8=0x03, gfx10=0x05)),
-   ("v_mac_legacy_f32",    dst(F32),      src(F32, F32, F32), op(0x06, gfx8=-1, gfx10=0x06, gfx11=-1)), #GFX6,7,10
-   ("v_fmac_legacy_f32",   dst(F32),      src(F32, F32, F32), op(gfx10=0x06, gfx12=-1)), #GFX10.3+, v_fmac_dx9_zero_f32 in GFX11
-   ("v_mul_legacy_f32",    dst(F32),      src(F32, F32), op(0x07, gfx8=0x04, gfx10=0x07)), #v_mul_dx9_zero_f32 in GFX11
+   ("v_mac_legacy_f32",    dst(F32),      src(F32, F32, F32), op(0x06, gfx8=-1, gfx10=0x06, gfx11=-1)),
+   ("v_fmac_legacy_f32",   dst(F32),      src(F32, F32, F32), op(gfx10=0x06, gfx12=-1)),
+   ("v_mul_legacy_f32",    dst(F32),      src(F32, F32), op(0x07, gfx8=0x04, gfx10=0x07)),
    ("v_mul_f32",           dst(F32),      src(F32, F32), op(0x08, gfx8=0x05, gfx10=0x08)),
    ("v_mul_i32_i24",       dst(U32),      src(U32, U32), op(0x09, gfx8=0x06, gfx10=0x09)),
    ("v_mul_hi_i32_i24",    dst(U32),      src(U32, U32), op(0x0a, gfx8=0x07, gfx10=0x0a)),
    ("v_mul_u32_u24",       dst(U32),      src(U32, U32), op(0x0b, gfx8=0x08, gfx10=0x0b)),
    ("v_mul_hi_u32_u24",    dst(U32),      src(U32, U32), op(0x0c, gfx8=0x09, gfx10=0x0c)),
-   ("v_dot4c_i32_i8",      dst(U32),      src(PkU16, PkU16, U32), op(gfx9=0x39, gfx10=0x0d, gfx11=-1)),
+   ("v_dot4c_i32_i8",      dst(U32),      src(PkU16, PkU16, U32), op(gfx9=0x39, gfx10=0x0d, gfx11=-1), InstrClass.ValuQuarterRate32),
    ("v_min_legacy_f32",    dst(F32),      src(F32, F32), op(0x0d, gfx8=-1)),
    ("v_max_legacy_f32",    dst(F32),      src(F32, F32), op(0x0e, gfx8=-1)),
-   ("v_min_f32",           dst(F32),      src(F32, F32), op(0x0f, gfx8=0x0a, gfx10=0x0f, gfx12=0x15)), #called v_min_num_f32 in GFX12
-   ("v_max_f32",           dst(F32),      src(F32, F32), op(0x10, gfx8=0x0b, gfx10=0x10, gfx12=0x16)), #called v_max_num_f32 in GFX12
+   ("v_min_f32",           dst(F32),      src(F32, F32), op(0x0f, gfx8=0x0a, gfx10=0x0f, gfx12=0x15)),
+   ("v_max_f32",           dst(F32),      src(F32, F32), op(0x10, gfx8=0x0b, gfx10=0x10, gfx12=0x16)),
    ("v_min_i32",           dst(U32),      src(U32, U32), op(0x11, gfx8=0x0c, gfx10=0x11)),
    ("v_max_i32",           dst(U32),      src(U32, U32), op(0x12, gfx8=0x0d, gfx10=0x12)),
    ("v_min_u32",           dst(U32),      src(U32, U32), op(0x13, gfx8=0x0e, gfx10=0x13)),
@@ -967,45 +971,45 @@ VOP2 = {
    ("v_madmk_f32",         dst(noMods(F32)), noMods(src(F32, F32, IMM)), op(0x20, gfx8=0x17, gfx10=0x20, gfx11=-1)),
    ("v_madak_f32",         dst(noMods(F32)), noMods(src(F32, F32, IMM)), op(0x21, gfx8=0x18, gfx10=0x21, gfx11=-1)),
    ("v_mbcnt_hi_u32_b32",  dst(U32),      src(U32, U32), op(0x24, gfx8=-1)),
-   ("v_add_co_u32",        dst(U32, VCC), src(U32, U32), op(0x25, gfx8=0x19, gfx10=-1)), # VOP3B only in RDNA
-   ("v_sub_co_u32",        dst(U32, VCC), src(U32, U32), op(0x26, gfx8=0x1a, gfx10=-1)), # VOP3B only in RDNA
-   ("v_subrev_co_u32",     dst(U32, VCC), src(U32, U32), op(0x27, gfx8=0x1b, gfx10=-1)), # VOP3B only in RDNA
-   ("v_addc_co_u32",       dst(U32, VCC), src(U32, U32, VCC), op(0x28, gfx8=0x1c, gfx10=0x28, gfx11=0x20)), # v_add_co_ci_u32 in RDNA
-   ("v_subb_co_u32",       dst(U32, VCC), src(U32, U32, VCC), op(0x29, gfx8=0x1d, gfx10=0x29, gfx11=0x21)), # v_sub_co_ci_u32 in RDNA
-   ("v_subbrev_co_u32",    dst(U32, VCC), src(U32, U32, VCC), op(0x2a, gfx8=0x1e, gfx10=0x2a, gfx11=0x22)), # v_subrev_co_ci_u32 in RDNA
+   ("v_add_co_u32",        dst(U32, VCC), src(U32, U32), op(0x25, gfx8=0x19, gfx10=-1)),
+   ("v_sub_co_u32",        dst(U32, VCC), src(U32, U32), op(0x26, gfx8=0x1a, gfx10=-1)),
+   ("v_subrev_co_u32",     dst(U32, VCC), src(U32, U32), op(0x27, gfx8=0x1b, gfx10=-1)),
+   ("v_addc_co_u32",       dst(U32, VCC), src(U32, U32, VCC), op(0x28, gfx8=0x1c, gfx10=0x28, gfx11=0x20)),
+   ("v_subb_co_u32",       dst(U32, VCC), src(U32, U32, VCC), op(0x29, gfx8=0x1d, gfx10=0x29, gfx11=0x21)),
+   ("v_subbrev_co_u32",    dst(U32, VCC), src(U32, U32, VCC), op(0x2a, gfx8=0x1e, gfx10=0x2a, gfx11=0x22)),
    ("v_fmac_f32",          dst(F32),      src(F32, F32, F32), op(gfx10=0x2b)),
    ("v_fmamk_f32",         dst(noMods(F32)), noMods(src(F32, F32, IMM)), op(gfx10=0x2c)),
    ("v_fmaak_f32",         dst(noMods(F32)), noMods(src(F32, F32, IMM)), op(gfx10=0x2d)),
-   ("v_cvt_pkrtz_f16_f32", dst(noMods(PkF16)), src(F32, F32), op(0x2f, gfx8=-1, gfx10=0x2f)), #v_cvt_pk_rtz_f16_f32 in GFX11
-   ("v_add_f16",           dst(F16),      src(F16, F16), op(gfx8=0x1f, gfx10=0x32)),
-   ("v_sub_f16",           dst(F16),      src(F16, F16), op(gfx8=0x20, gfx10=0x33)),
-   ("v_subrev_f16",        dst(F16),      src(F16, F16), op(gfx8=0x21, gfx10=0x34)),
-   ("v_mul_f16",           dst(F16),      src(F16, F16), op(gfx8=0x22, gfx10=0x35)),
-   ("v_mac_f16",           dst(F16),      src(F16, F16, F16), op(gfx8=0x23, gfx10=-1)),
-   ("v_madmk_f16",         dst(noMods(F16)), noMods(src(F16, F16, IMM)), op(gfx8=0x24, gfx10=-1)),
-   ("v_madak_f16",         dst(noMods(F16)), noMods(src(F16, F16, IMM)), op(gfx8=0x25, gfx10=-1)),
-   ("v_add_u16",           dst(U16),      src(U16, U16), op(gfx8=0x26, gfx10=-1)),
-   ("v_sub_u16",           dst(U16),      src(U16, U16), op(gfx8=0x27, gfx10=-1)),
-   ("v_subrev_u16",        dst(U16),      src(U16, U16), op(gfx8=0x28, gfx10=-1)),
-   ("v_mul_lo_u16",        dst(U16),      src(U16, U16), op(gfx8=0x29, gfx10=-1)),
-   ("v_lshlrev_b16",       dst(U16),      src(U16, U16), op(gfx8=0x2a, gfx10=-1)),
-   ("v_lshrrev_b16",       dst(U16),      src(U16, U16), op(gfx8=0x2b, gfx10=-1)),
-   ("v_ashrrev_i16",       dst(U16),      src(U16, U16), op(gfx8=0x2c, gfx10=-1)),
-   ("v_max_f16",           dst(F16),      src(F16, F16), op(gfx8=0x2d, gfx10=0x39, gfx12=0x31)), #called v_max_num_f16 in GFX12
-   ("v_min_f16",           dst(F16),      src(F16, F16), op(gfx8=0x2e, gfx10=0x3a, gfx12=0x30)), #called v_min_num_f16 in GFX12
-   ("v_max_u16",           dst(U16),      src(U16, U16), op(gfx8=0x2f, gfx10=-1)),
-   ("v_max_i16",           dst(U16),      src(U16, U16), op(gfx8=0x30, gfx10=-1)),
-   ("v_min_u16",           dst(U16),      src(U16, U16), op(gfx8=0x31, gfx10=-1)),
-   ("v_min_i16",           dst(U16),      src(U16, U16), op(gfx8=0x32, gfx10=-1)),
-   ("v_ldexp_f16",         dst(F16),      src(F16, U16), op(gfx8=0x33, gfx10=0x3b)),
-   ("v_add_u32",           dst(U32),      src(U32, U32), op(gfx9=0x34, gfx10=0x25)), # called v_add_nc_u32 in RDNA
-   ("v_sub_u32",           dst(U32),      src(U32, U32), op(gfx9=0x35, gfx10=0x26)), # called v_sub_nc_u32 in RDNA
-   ("v_subrev_u32",        dst(U32),      src(U32, U32), op(gfx9=0x36, gfx10=0x27)), # called v_subrev_nc_u32 in RDNA
+   ("v_cvt_pkrtz_f16_f32", dst(noMods(PkF16)), src(F32, F32), op(0x2f, gfx8=-1, gfx10=0x2f)),
+   ("v_add_f16",           dst(F16),      src(F16, F16), op(gfx8=0x1f, gfx9=0x1f, gfx10=0x32)),
+   ("v_sub_f16",           dst(F16),      src(F16, F16), op(gfx8=0x20, gfx9=0x20, gfx10=0x33)),
+   ("v_subrev_f16",        dst(F16),      src(F16, F16), op(gfx8=0x21, gfx9=0x21, gfx10=0x34)),
+   ("v_mul_f16",           dst(F16),      src(F16, F16), op(gfx8=0x22, gfx9=0x22, gfx10=0x35)),
+   ("v_mac_f16",           dst(F16),      src(F16, F16, F16), op(gfx8=0x23, gfx9=0x23, gfx10=-1)),
+   ("v_madmk_f16",         dst(noMods(F16)), noMods(src(F16, F16, IMM)), op(gfx8=0x24, gfx9=0x24, gfx10=-1)),
+   ("v_madak_f16",         dst(noMods(F16)), noMods(src(F16, F16, IMM)), op(gfx8=0x25, gfx9=0x25, gfx10=-1)),
+   ("v_add_u16",           dst(U16),      src(U16, U16), op(gfx8=0x26, gfx9=0x26, gfx10=-1)),
+   ("v_sub_u16",           dst(U16),      src(U16, U16), op(gfx8=0x27, gfx9=0x27, gfx10=-1)),
+   ("v_subrev_u16",        dst(U16),      src(U16, U16), op(gfx8=0x28, gfx9=0x28, gfx10=-1)),
+   ("v_mul_lo_u16",        dst(U16),      src(U16, U16), op(gfx8=0x29, gfx9=0x29, gfx10=-1)),
+   ("v_lshlrev_b16",       dst(U16),      src(U16, U16), op(gfx8=0x2a, gfx9=0x2a, gfx10=-1)),
+   ("v_lshrrev_b16",       dst(U16),      src(U16, U16), op(gfx8=0x2b, gfx9=0x2b, gfx10=-1)),
+   ("v_ashrrev_i16",       dst(U16),      src(U16, U16), op(gfx8=0x2c, gfx9=0x2c, gfx10=-1)),
+   ("v_max_f16",           dst(F16),      src(F16, F16), op(gfx8=0x2d, gfx9=0x2d, gfx10=0x39, gfx12=0x31)),
+   ("v_min_f16",           dst(F16),      src(F16, F16), op(gfx8=0x2e, gfx9=0x2e, gfx10=0x3a, gfx12=0x30)),
+   ("v_max_u16",           dst(U16),      src(U16, U16), op(gfx8=0x2f, gfx9=0x2f, gfx10=-1)),
+   ("v_max_i16",           dst(U16),      src(U16, U16), op(gfx8=0x30, gfx9=0x30, gfx10=-1)),
+   ("v_min_u16",           dst(U16),      src(U16, U16), op(gfx8=0x31, gfx9=0x31, gfx10=-1)),
+   ("v_min_i16",           dst(U16),      src(U16, U16), op(gfx8=0x32, gfx9=0x32, gfx10=-1)),
+   ("v_ldexp_f16",         dst(F16),      src(F16, U16), op(gfx8=0x33, gfx9=0x33, gfx10=0x3b)),
+   ("v_add_u32",           dst(U32),      src(U32, U32), op(gfx9=0x34, gfx10=0x25)),
+   ("v_sub_u32",           dst(U32),      src(U32, U32), op(gfx9=0x35, gfx10=0x26)),
+   ("v_subrev_u32",        dst(U32),      src(U32, U32), op(gfx9=0x36, gfx10=0x27)),
    ("v_fmac_f16",          dst(F16),      src(F16, F16, F16), op(gfx10=0x36)),
    ("v_fmamk_f16",         dst(noMods(F16)), noMods(src(F16, F16, IMM)), op(gfx10=0x37)),
    ("v_fmaak_f16",         dst(noMods(F16)), noMods(src(F16, F16, IMM)), op(gfx10=0x38)),
-   ("v_pk_fmac_f16",       dst(noMods(PkF16)), noMods(src(PkF16, PkF16, PkF16)), op(gfx10=0x3c)),
-   ("v_dot2c_f32_f16",     dst(noMods(F32)), noMods(src(PkF16, PkF16, F32)), op(gfx9=0x37, gfx10=0x02, gfx12=-1)), #v_dot2acc_f32_f16 in GFX11
+   ("v_pk_fmac_f16",       dst(noMods(PkF16)), noMods(src(PkF16, PkF16, PkF16)), op(gfx10=0x3c), InstrClass.ValuFma),
+   ("v_dot2c_f32_f16",     dst(noMods(F32)), noMods(src(PkF16, PkF16, F32)), op(gfx9=0x37, gfx10=0x02, gfx12=-1), InstrClass.ValuQuarterRate32),
    ("v_add_f64",           dst(F64),      src(F64, F64), op(gfx12=0x02), InstrClass.ValuDoubleAdd),
    ("v_mul_f64",           dst(F64),      src(F64, F64), op(gfx12=0x06), InstrClass.ValuDoubleAdd),
    ("v_lshlrev_b64",       dst(U64),      src(U32, U64), op(gfx12=0x1f), InstrClass.Valu64),
@@ -1016,7 +1020,7 @@ for (name, defs, ops, num, cls) in defau
    insn(name, num, Format.VOP2, cls, definitions = defs, operands = ops)
 
 
-# VOP1 instructions: instructions with 1 input and 1 output
+# VOP1 instructions: 1 input, 1 output
 VOP1 = {
    ("v_nop",                      dst(),    src(), op(0x00)),
    ("v_mov_b32",                  dst(U32), src(U32), op(0x01)),
@@ -1032,8 +1036,8 @@ VOP1 = {
    ("p_v_cvt_f16_f32_rtpi",       dst(F16), src(F32), op(-1)),
    ("p_v_cvt_f16_f32_rtni",       dst(F16), src(F32), op(-1)),
    ("v_cvt_f32_f16",              dst(F32), src(F16), op(0x0b)),
-   ("v_cvt_rpi_i32_f32",          dst(U32), src(F32), op(0x0c)), #v_cvt_nearest_i32_f32 in GFX11
-   ("v_cvt_flr_i32_f32",          dst(U32), src(F32), op(0x0d)),#v_cvt_floor_i32_f32 in GFX11
+   ("v_cvt_rpi_i32_f32",          dst(U32), src(F32), op(0x0c)),
+   ("v_cvt_flr_i32_f32",          dst(U32), src(F32), op(0x0d)),
    ("v_cvt_off_f32_i4",           dst(F32), src(U32), op(0x0e)),
    ("v_cvt_f32_f64",              dst(F32), src(F64), op(0x0f), InstrClass.ValuDoubleConvert),
    ("v_cvt_f64_f32",              dst(F64), src(F32), op(0x10), InstrClass.ValuDoubleConvert),
@@ -1073,9 +1077,9 @@ VOP1 = {
    ("v_cos_f32",                  dst(F32), src(F32), op(0x36, gfx8=0x2a, gfx10=0x36), InstrClass.ValuTranscendental32),
    ("v_not_b32",                  dst(U32), src(U32), op(0x37, gfx8=0x2b, gfx10=0x37)),
    ("v_bfrev_b32",                dst(U32), src(U32), op(0x38, gfx8=0x2c, gfx10=0x38)),
-   ("v_ffbh_u32",                 dst(U32), src(U32), op(0x39, gfx8=0x2d, gfx10=0x39)), #v_clz_i32_u32 in GFX11
-   ("v_ffbl_b32",                 dst(U32), src(U32), op(0x3a, gfx8=0x2e, gfx10=0x3a)), #v_ctz_i32_b32 in GFX11
-   ("v_ffbh_i32",                 dst(U32), src(U32), op(0x3b, gfx8=0x2f, gfx10=0x3b)), #v_cls_i32 in GFX11
+   ("v_ffbh_u32",                 dst(U32), src(U32), op(0x39, gfx8=0x2d, gfx10=0x39)),
+   ("v_ffbl_b32",                 dst(U32), src(U32), op(0x3a, gfx8=0x2e, gfx10=0x3a)),
+   ("v_ffbh_i32",                 dst(U32), src(U32), op(0x3b, gfx8=0x2f, gfx10=0x3b)),
    ("v_frexp_exp_i32_f64",        dst(U32), src(F64), op(0x3c, gfx8=0x30, gfx10=0x3c), InstrClass.ValuDouble),
    ("v_frexp_mant_f64",           dst(noMods(F64)), src(F64), op(0x3d, gfx8=0x31, gfx10=0x3d), InstrClass.ValuDouble),
    ("v_fract_f64",                dst(F64), src(F64), op(0x3e, gfx8=0x32, gfx10=0x3e), InstrClass.ValuDouble),
@@ -1087,24 +1091,24 @@ VOP1 = {
    ("v_movrelsd_b32",             dst(U32), src(U32, M0), op(0x44, gfx8=0x38, gfx9=-1, gfx10=0x44)),
    ("v_movrelsd_2_b32",           dst(U32), src(U32, M0), op(gfx10=0x48)),
    ("v_screen_partition_4se_b32", dst(U32), src(U32), op(gfx9=0x37, gfx10=-1)),
-   ("v_cvt_f16_u16",              dst(F16), src(U16), op(gfx8=0x39, gfx10=0x50)),
-   ("v_cvt_f16_i16",              dst(F16), src(U16), op(gfx8=0x3a, gfx10=0x51)),
-   ("v_cvt_u16_f16",              dst(U16), src(F16), op(gfx8=0x3b, gfx10=0x52)),
-   ("v_cvt_i16_f16",              dst(U16), src(F16), op(gfx8=0x3c, gfx10=0x53)),
-   ("v_rcp_f16",                  dst(F16), dst(F16), op(gfx8=0x3d, gfx10=0x54), InstrClass.ValuTranscendental32),
-   ("v_sqrt_f16",                 dst(F16), dst(F16), op(gfx8=0x3e, gfx10=0x55), InstrClass.ValuTranscendental32),
-   ("v_rsq_f16",                  dst(F16), dst(F16), op(gfx8=0x3f, gfx10=0x56), InstrClass.ValuTranscendental32),
-   ("v_log_f16",                  dst(F16), dst(F16), op(gfx8=0x40, gfx10=0x57), InstrClass.ValuTranscendental32),
-   ("v_exp_f16",                  dst(F16), dst(F16), op(gfx8=0x41, gfx10=0x58), InstrClass.ValuTranscendental32),
-   ("v_frexp_mant_f16",           dst(noMods(F16)), dst(F16), op(gfx8=0x42, gfx10=0x59)),
-   ("v_frexp_exp_i16_f16",        dst(U16), dst(F16), op(gfx8=0x43, gfx10=0x5a)),
-   ("v_floor_f16",                dst(F16), dst(F16), op(gfx8=0x44, gfx10=0x5b)),
-   ("v_ceil_f16",                 dst(F16), dst(F16), op(gfx8=0x45, gfx10=0x5c)),
-   ("v_trunc_f16",                dst(F16), dst(F16), op(gfx8=0x46, gfx10=0x5d)),
-   ("v_rndne_f16",                dst(F16), dst(F16), op(gfx8=0x47, gfx10=0x5e)),
-   ("v_fract_f16",                dst(F16), dst(F16), op(gfx8=0x48, gfx10=0x5f)),
-   ("v_sin_f16",                  dst(F16), dst(F16), op(gfx8=0x49, gfx10=0x60), InstrClass.ValuTranscendental32),
-   ("v_cos_f16",                  dst(F16), dst(F16), op(gfx8=0x4a, gfx10=0x61), InstrClass.ValuTranscendental32),
+   ("v_cvt_f16_u16",              dst(F16), src(U16), op(gfx8=0x39, gfx9=0x39, gfx10=0x50)),
+   ("v_cvt_f16_i16",              dst(F16), src(U16), op(gfx8=0x3a, gfx9=0x3a, gfx10=0x51)),
+   ("v_cvt_u16_f16",              dst(U16), src(F16), op(gfx8=0x3b, gfx9=0x3b, gfx10=0x52)),
+   ("v_cvt_i16_f16",              dst(U16), src(F16), op(gfx8=0x3c, gfx9=0x3c, gfx10=0x53)),
+   ("v_rcp_f16",                  dst(F16), src(F16), op(gfx8=0x3d, gfx9=0x3d, gfx10=0x54), InstrClass.ValuTranscendental32),
+   ("v_sqrt_f16",                 dst(F16), src(F16), op(gfx8=0x3e, gfx9=0x3e, gfx10=0x55), InstrClass.ValuTranscendental32),
+   ("v_rsq_f16",                  dst(F16), src(F16), op(gfx8=0x3f, gfx9=0x3f, gfx10=0x56), InstrClass.ValuTranscendental32),
+   ("v_log_f16",                  dst(F16), src(F16), op(gfx8=0x40, gfx9=0x40, gfx10=0x57), InstrClass.ValuTranscendental32),
+   ("v_exp_f16",                  dst(F16), src(F16), op(gfx8=0x41, gfx9=0x41, gfx10=0x58), InstrClass.ValuTranscendental32),
+   ("v_frexp_mant_f16",           dst(noMods(F16)), src(F16), op(gfx8=0x42, gfx9=0x42, gfx10=0x59)),
+   ("v_frexp_exp_i16_f16",        dst(U16), src(F16), op(gfx8=0x43, gfx9=0x43, gfx10=0x5a)),
+   ("v_floor_f16",                dst(F16), src(F16), op(gfx8=0x44, gfx9=0x44, gfx10=0x5b)),
+   ("v_ceil_f16",                 dst(F16), src(F16), op(gfx8=0x45, gfx9=0x45, gfx10=0x5c)),
+   ("v_trunc_f16",                dst(F16), src(F16), op(gfx8=0x46, gfx9=0x46, gfx10=0x5d)),
+   ("v_rndne_f16",                dst(F16), src(F16), op(gfx8=0x47, gfx9=0x47, gfx10=0x5e)),
+   ("v_fract_f16",                dst(F16), src(F16), op(gfx8=0x48, gfx9=0x48, gfx10=0x5f)),
+   ("v_sin_f16",                  dst(F16), src(F16), op(gfx8=0x49, gfx9=0x49, gfx10=0x60), InstrClass.ValuTranscendental32),
+   ("v_cos_f16",                  dst(F16), src(F16), op(gfx8=0x4a, gfx9=0x4a, gfx10=0x61), InstrClass.ValuTranscendental32),
    ("v_exp_legacy_f32",           dst(F32), src(F32), op(gfx7=0x46, gfx8=0x4b, gfx10=-1), InstrClass.ValuTranscendental32),
    ("v_log_legacy_f32",           dst(F32), src(F32), op(gfx7=0x45, gfx8=0x4c, gfx10=-1), InstrClass.ValuTranscendental32),
    ("v_sat_pk_u8_i16",            dst(U16), src(U32), op(gfx9=0x4f, gfx10=0x62)),
@@ -1112,7 +1116,7 @@ VOP1 = {
    ("v_cvt_norm_u16_f16",         dst(U16), src(F16), op(gfx9=0x4e, gfx10=0x64)),
    ("v_swap_b32",                 dst(U32, U32), src(U32, U32), op(gfx9=0x51, gfx10=0x65)),
    ("v_swaprel_b32",              dst(U32, U32), src(U32, U32, M0), op(gfx10=0x68)),
-   ("v_permlane64_b32",           dst(U32), src(U32), op(gfx11=0x67)), #cannot use VOP3
+   ("v_permlane64_b32",           dst(U32), src(U32), op(gfx11=0x67)),
    ("v_not_b16",                  dst(U16), src(U16), op(gfx11=0x69)),
    ("v_cvt_i32_i16",              dst(U32), src(U16), op(gfx11=0x6a)),
    ("v_cvt_u32_u16",              dst(U32), src(U16), op(gfx11=0x6b)),
@@ -1130,12 +1134,12 @@ for (name, defs, ops, num, cls) in defau
 # VOPC instructions:
 
 VOPC_CLASS = {
-   ("v_cmp_class_f32",  dst(VCC), src(F32, U32), op(0x88, gfx8=0x10, gfx10=0x88, gfx11=0x7e)),
-   ("v_cmp_class_f16",  dst(VCC), src(F16, U16), op(gfx8=0x14, gfx10=0x8f, gfx11=0x7d)),
-   ("v_cmpx_class_f32", dst(EXEC), src(F32, U32), op(0x98, gfx8=0x11, gfx10=0x98, gfx11=0xfe)),
-   ("v_cmpx_class_f16", dst(EXEC), src(F16, U16), op(gfx8=0x15, gfx10=0x9f, gfx11=0xfd)),
-   ("v_cmp_class_f64",  dst(VCC), src(F64, U32), op(0xa8, gfx8=0x12, gfx10=0xa8, gfx11=0x7f), InstrClass.ValuDouble),
-   ("v_cmpx_class_f64", dst(EXEC), src(F64, U32), op(0xb8, gfx8=0x13, gfx10=0xb8, gfx11=0xff), InstrClass.ValuDouble),
+   ("v_cmp_class_f32",  dst(VCC), src(F32, U32), op(0x88, gfx8=0x10, gfx9=0x10, gfx10=0x88, gfx11=0x7e)),
+   ("v_cmp_class_f16",  dst(VCC), src(F16, U16), op(gfx8=0x14, gfx9=0x14, gfx10=0x8f, gfx11=0x7d)),
+   ("v_cmpx_class_f32", dst(EXEC), src(F32, U32), op(0x98, gfx8=0x11, gfx9=0x11, gfx10=0x98, gfx11=0xfe)),
+   ("v_cmpx_class_f16", dst(EXEC), src(F16, U16), op(gfx8=0x15, gfx9=0x15, gfx10=0x9f, gfx11=0xfd)),
+   ("v_cmp_class_f64",  dst(VCC), src(F64, U32), op(0xa8, gfx8=0x12, gfx9=0x12, gfx10=0xa8, gfx11=0x7f), InstrClass.ValuDouble),
+   ("v_cmpx_class_f64", dst(EXEC), src(F64, U32), op(0xb8, gfx8=0x13, gfx9=0x13, gfx10=0xb8, gfx11=0xff), InstrClass.ValuDouble),
 }
 for (name, defs, ops, num, cls) in default_class(VOPC_CLASS, InstrClass.Valu32):
     insn(name, num, Format.VOPC, cls, definitions = defs, operands = ops)
@@ -1204,46 +1208,57 @@ for comp, dtype, cmps, cmpx in itertools
 
 # VOPP instructions: packed 16bit instructions - 2 or 3 inputs and 1 output
 VOPP = {
-   ("v_pk_mad_i16",     dst(PkU16), src(PkU16, PkU16, PkU16), op(gfx9=0x00)),
-   ("v_pk_mul_lo_u16",  dst(PkU16), src(PkU16, PkU16), op(gfx9=0x01)),
-   ("v_pk_add_i16",     dst(PkU16), src(PkU16, PkU16), op(gfx9=0x02)),
-   ("v_pk_sub_i16",     dst(PkU16), src(PkU16, PkU16), op(gfx9=0x03)),
-   ("v_pk_lshlrev_b16", dst(PkU16), src(PkU16, PkU16), op(gfx9=0x04)),
-   ("v_pk_lshrrev_b16", dst(PkU16), src(PkU16, PkU16), op(gfx9=0x05)),
-   ("v_pk_ashrrev_i16", dst(PkU16), src(PkU16, PkU16), op(gfx9=0x06)),
-   ("v_pk_max_i16",     dst(PkU16), src(PkU16, PkU16), op(gfx9=0x07)),
-   ("v_pk_min_i16",     dst(PkU16), src(PkU16, PkU16), op(gfx9=0x08)),
-   ("v_pk_mad_u16",     dst(PkU16), src(PkU16, PkU16, PkU16), op(gfx9=0x09)),
-   ("v_pk_add_u16",     dst(PkU16), src(PkU16, PkU16), op(gfx9=0x0a)),
-   ("v_pk_sub_u16",     dst(PkU16), src(PkU16, PkU16), op(gfx9=0x0b)),
-   ("v_pk_max_u16",     dst(PkU16), src(PkU16, PkU16), op(gfx9=0x0c)),
-   ("v_pk_min_u16",     dst(PkU16), src(PkU16, PkU16), op(gfx9=0x0d)),
+   ("v_pk_mad_i16",     dst(PkI16), src(PkI16, PkI16, PkI16), op(gfx9=0x00), InstrClass.ValuQuarterRate32),
+   ("v_pk_mul_lo_u16",  dst(PkU16), src(PkU16, PkU16), op(gfx9=0x01), InstrClass.ValuQuarterRate32),
+   ("v_pk_add_i16",     dst(PkI16), src(PkI16, PkI16), op(gfx9=0x02), InstrClass.ValuQuarterRate32),
+   ("v_pk_sub_i16",     dst(PkI16), src(PkI16, PkI16), op(gfx9=0x03), InstrClass.ValuQuarterRate32),
+   ("v_pk_lshlrev_b16", dst(PkU16), src(PkU16, PkU16), op(gfx9=0x04), InstrClass.ValuQuarterRate32),
+   ("v_pk_lshrrev_b16", dst(PkU16), src(PkU16, PkU16), op(gfx9=0x05), InstrClass.ValuQuarterRate32),
+   ("v_pk_ashrrev_i16", dst(PkI16), src(PkI16, PkI16), op(gfx9=0x06), InstrClass.ValuQuarterRate32),
+   ("v_pk_max_i16",     dst(PkI16), src(PkI16, PkI16), op(gfx9=0x07), InstrClass.ValuQuarterRate32),
+   ("v_pk_min_i16",     dst(PkI16), src(PkI16, PkI16), op(gfx9=0x08), InstrClass.ValuQuarterRate32),
+   ("v_pk_mad_u16",     dst(PkU16), src(PkU16, PkU16, PkU16), op(gfx9=0x09), InstrClass.ValuQuarterRate32),
+   ("v_pk_add_u16",     dst(PkU16), src(PkU16, PkU16), op(gfx9=0x0a), InstrClass.ValuQuarterRate32),
+   ("v_pk_sub_u16",     dst(PkU16), src(PkU16, PkU16), op(gfx9=0x0b), InstrClass.ValuQuarterRate32),
+   ("v_pk_max_u16",     dst(PkU16), src(PkU16, PkU16), op(gfx9=0x0c), InstrClass.ValuQuarterRate32),
+   ("v_pk_min_u16",     dst(PkU16), src(PkU16, PkU16), op(gfx9=0x0d), InstrClass.ValuQuarterRate32),
+
+   # FP16 packed ops remain full-rate (Valu32) - correct per Vega ISA
    ("v_pk_fma_f16",     dst(PkF16), src(PkF16, PkF16, PkF16), op(gfx9=0x0e)),
    ("v_pk_add_f16",     dst(PkF16), src(PkF16, PkF16), op(gfx9=0x0f)),
    ("v_pk_mul_f16",     dst(PkF16), src(PkF16, PkF16), op(gfx9=0x10)),
-   ("v_pk_min_f16",     dst(PkF16), src(PkF16, PkF16), op(gfx9=0x11, gfx12=0x1b)), # called v_pk_min_num_f16 in GFX12
-   ("v_pk_max_f16",     dst(PkF16), src(PkF16, PkF16), op(gfx9=0x12, gfx12=0x1c)), # called v_pk_min_num_f16 in GFX12
+   ("v_pk_min_f16",     dst(PkF16), src(PkF16, PkF16), op(gfx9=0x11, gfx12=0x1b)),
+   ("v_pk_max_f16",     dst(PkF16), src(PkF16, PkF16), op(gfx9=0x12, gfx12=0x1c)),
    ("v_pk_minimum_f16", dst(PkF16), src(PkF16, PkF16), op(gfx12=0x1d)),
    ("v_pk_maximum_f16", dst(PkF16), src(PkF16, PkF16), op(gfx12=0x1e)),
-   ("v_fma_mix_f32",    dst(F32), src(F32, F32, F32), op(gfx9=0x20)), # v_mad_mix_f32 in VEGA ISA, v_fma_mix_f32 in RDNA ISA
-   ("v_fma_mixlo_f16",  dst(F16), src(F32, F32, F32), op(gfx9=0x21)), # v_mad_mixlo_f16 in VEGA ISA, v_fma_mixlo_f16 in RDNA ISA
-   ("v_fma_mixhi_f16",  dst(F16), src(F32, F32, F32), op(gfx9=0x22)), # v_mad_mixhi_f16 in VEGA ISA, v_fma_mixhi_f16 in RDNA ISA
+
+   # Mixed-precision FMA (VOP3P format, full FMA rate)
+   ("v_fma_mix_f32",    dst(F32), src(F32, F32, F32), op(gfx9=0x20), InstrClass.ValuFma),
+   ("v_fma_mixlo_f16",  dst(F16), src(F32, F32, F32), op(gfx9=0x21), InstrClass.ValuFma),
+   ("v_fma_mixhi_f16",  dst(F16), src(F32, F32, F32), op(gfx9=0x22), InstrClass.ValuFma),
+
    ("p_v_fma_mixlo_f16_rtz", dst(F16), src(F32, F32, F32), op(-1)), # v_fma_mixlo_f16 with fp16 rtz rounding
    ("p_v_fma_mixhi_f16_rtz", dst(F16), src(F32, F32, F32), op(-1)), # v_fma_mixhi_f16 with fp16 rtz rounding
-   ("v_dot2_i32_i16",      dst(U32), src(PkU16, PkU16, U32), op(gfx9=0x26, gfx10=0x14, gfx11=-1)),
-   ("v_dot2_u32_u16",      dst(U32), src(PkU16, PkU16, U32), op(gfx9=0x27, gfx10=0x15, gfx11=-1)),
-   ("v_dot4_i32_iu8",      dst(U32), src(PkU16, PkU16, U32), op(gfx11=0x16)),
-   ("v_dot4_i32_i8",       dst(U32), src(PkU16, PkU16, U32), op(gfx9=0x28, gfx10=0x16, gfx11=-1)),
-   ("v_dot4_u32_u8",       dst(U32), src(PkU16, PkU16, U32), op(gfx9=0x29, gfx10=0x17)),
-   ("v_dot8_i32_iu4",      dst(U32), src(PkU16, PkU16, U32), op(gfx11=0x18)),
-   ("v_dot8_i32_i4",       dst(U32), src(PkU16, PkU16, U32), op(gfx9=0x2a, gfx10=0x18, gfx11=-1)),
-   ("v_dot8_u32_u4",       dst(U32), src(PkU16, PkU16, U32), op(gfx9=0x2b, gfx10=0x19)),
-   ("v_dot2_f32_f16",      dst(noMods(F32)), noMods(src(PkF16, PkF16, F32)), op(gfx9=0x23, gfx10=0x13)),
-   ("v_dot2_f32_bf16",     dst(noMods(F32)), noMods(src(PkBF16, PkBF16, F32)), op(gfx11=0x1a)),
-   ("v_dot4_f32_fp8_bf8",  dst(noMods(F32)), noMods(src(Pk4F8, Pk4BF8, F32)), op(gfx12=0x24)),
-   ("v_dot4_f32_bf8_fp8",  dst(noMods(F32)), noMods(src(Pk4BF8, Pk4F8, F32)), op(gfx12=0x25)),
-   ("v_dot4_f32_fp8_fp8",  dst(noMods(F32)), noMods(src(Pk4F8, Pk4F8, F32)), op(gfx12=0x26)),
-   ("v_dot4_f32_bf8_bf8",  dst(noMods(F32)), noMods(src(Pk4BF8, Pk4BF8, F32)), op(gfx12=0x27)),
+
+   # Dot products - quarter rate
+   ("v_dot2_i32_i16",      dst(U32), src(PkU16, PkU16, U32), op(gfx9=0x26, gfx10=0x14, gfx11=-1), InstrClass.ValuQuarterRate32),
+   ("v_dot2_u32_u16",      dst(U32), src(PkU16, PkU16, U32), op(gfx9=0x27, gfx10=0x15, gfx11=-1), InstrClass.ValuQuarterRate32),
+   ("v_dot4_i32_iu8",      dst(U32), src(PkU16, PkU16, U32), op(gfx11=0x16), InstrClass.ValuQuarterRate32),
+   ("v_dot4_i32_i8",       dst(U32), src(PkU16, PkU16, U32), op(gfx9=0x28, gfx10=0x16, gfx11=-1), InstrClass.ValuQuarterRate32),
+   ("v_dot4_u32_u8",       dst(U32), src(PkU16, PkU16, U32), op(gfx9=0x29, gfx10=0x17), InstrClass.ValuQuarterRate32),
+   ("v_dot8_i32_iu4",      dst(U32), src(PkU16, PkU16, U32), op(gfx11=0x18), InstrClass.ValuQuarterRate32),
+   ("v_dot8_i32_i4",       dst(U32), src(PkU16, PkU16, U32), op(gfx9=0x2a, gfx10=0x18, gfx11=-1), InstrClass.ValuQuarterRate32),
+   ("v_dot8_u32_u4",       dst(U32), src(PkU16, PkU16, U32), op(gfx9=0x2b, gfx10=0x19), InstrClass.ValuQuarterRate32),
+   ("v_dot2_f32_f16",      dst(noMods(F32)), noMods(src(PkF16, PkF16, F32)), op(gfx9=0x23, gfx10=0x13), InstrClass.ValuQuarterRate32),
+   ("v_dot2_f32_bf16",     dst(noMods(F32)), noMods(src(PkBF16, PkBF16, F32)), op(gfx11=0x1a), InstrClass.ValuQuarterRate32),
+
+   # GFX12+ (not applicable to Vega 10)
+   ("v_dot4_f32_fp8_bf8",  dst(noMods(F32)), noMods(src(Pk4F8, Pk4BF8, F32)), op(gfx12=0x24), InstrClass.ValuQuarterRate32),
+   ("v_dot4_f32_bf8_fp8",  dst(noMods(F32)), noMods(src(Pk4BF8, Pk4F8, F32)), op(gfx12=0x25), InstrClass.ValuQuarterRate32),
+   ("v_dot4_f32_fp8_fp8",  dst(noMods(F32)), noMods(src(Pk4F8, Pk4F8, F32)), op(gfx12=0x26), InstrClass.ValuQuarterRate32),
+   ("v_dot4_f32_bf8_bf8",  dst(noMods(F32)), noMods(src(Pk4BF8, Pk4BF8, F32)), op(gfx12=0x27), InstrClass.ValuQuarterRate32),
+
+   # WMMA (GFX11+, not on Vega 10)
    ("v_wmma_f32_16x16x16_f16",       dst(), src(), op(gfx11=0x40), InstrClass.WMMA),
    ("v_wmma_f32_16x16x16_bf16",      dst(), src(), op(gfx11=0x41), InstrClass.WMMA),
    ("v_wmma_f16_16x16x16_f16",       dst(), src(), op(gfx11=0x42), InstrClass.WMMA),
@@ -1297,8 +1312,8 @@ for (name, defs, ops, num) in VINTERP:
 # VOP3 instructions: 3 inputs, 1 output
 # VOP3b instructions: have a unique scalar output, e.g. VOP2 with vcc out
 VOP3 = {
-   ("v_mad_legacy_f32",        dst(F32), src(F32, F32, F32), op(0x140, gfx8=0x1c0, gfx10=0x140, gfx11=-1)), # GFX6-GFX10
-   ("v_mad_f32",               dst(F32), src(F32, F32, F32), op(0x141, gfx8=0x1c1, gfx10=0x141, gfx11=-1)),
+   ("v_mad_legacy_f32",        dst(F32), src(mods(F32), mods(F32), mods(F32)), op(0x140, gfx8=0x1c0, gfx10=0x140, gfx11=-1)), # GFX6-GFX10
+   ("v_mad_f32",               dst(F32), src(mods(F32), mods(F32), mods(F32)), op(0x141, gfx8=0x1c1, gfx10=0x141, gfx11=-1)),
    ("v_mad_i32_i24",           dst(U32), src(U32, U32, U32), op(0x142, gfx8=0x1c2, gfx10=0x142, gfx11=0x20a)),
    ("v_mad_u32_u24",           dst(U32), src(U32, U32, U32), op(0x143, gfx8=0x1c3, gfx10=0x143, gfx11=0x20b)),
    ("v_cubeid_f32",            dst(F32), src(F32, F32, F32), op(0x144, gfx8=0x1c4, gfx10=0x144, gfx11=0x20c)),
@@ -1308,19 +1323,19 @@ VOP3 = {
    ("v_bfe_u32",               dst(U32), src(U32, U32, U32), op(0x148, gfx8=0x1c8, gfx10=0x148, gfx11=0x210)),
    ("v_bfe_i32",               dst(U32), src(U32, U32, U32), op(0x149, gfx8=0x1c9, gfx10=0x149, gfx11=0x211)),
    ("v_bfi_b32",               dst(U32), src(U32, U32, U32), op(0x14a, gfx8=0x1ca, gfx10=0x14a, gfx11=0x212)),
-   ("v_fma_f32",               dst(F32), src(F32, F32, F32), op(0x14b, gfx8=0x1cb, gfx10=0x14b, gfx11=0x213), InstrClass.ValuFma),
-   ("v_fma_f64",               dst(F64), src(F64, F64, F64), op(0x14c, gfx8=0x1cc, gfx10=0x14c, gfx11=0x214), InstrClass.ValuDouble),
+   ("v_fma_f32",               dst(F32), src(mods(F32), mods(F32), mods(F32)), op(0x14b, gfx8=0x1cb, gfx10=0x14b, gfx11=0x213), InstrClass.ValuFma),
+   ("v_fma_f64",               dst(F64), src(mods(F64), mods(F64), mods(F64)), op(0x14c, gfx8=0x1cc, gfx10=0x14c, gfx11=0x214), InstrClass.ValuDouble),
    ("v_lerp_u8",               dst(U32), src(U32, U32, U32), op(0x14d, gfx8=0x1cd, gfx10=0x14d, gfx11=0x215)),
-   ("v_alignbit_b32",          dst(U32), src(U32, U32, U16), op(0x14e, gfx8=0x1ce, gfx10=0x14e, gfx11=0x216)),
-   ("v_alignbyte_b32",         dst(U32), src(U32, U32, U16), op(0x14f, gfx8=0x1cf, gfx10=0x14f, gfx11=0x217)),
+   ("v_alignbit_b32",          dst(U32), src(U32, U32, noMods(U16)), op(0x14e, gfx8=0x1ce, gfx10=0x14e, gfx11=0x216)),
+   ("v_alignbyte_b32",         dst(U32), src(U32, U32, noMods(U16)), op(0x14f, gfx8=0x1cf, gfx10=0x14f, gfx11=0x217)),
    ("v_mullit_f32",            dst(F32), src(F32, F32, F32), op(0x150, gfx8=-1, gfx10=0x150, gfx11=0x218)),
-   ("v_min3_f32",              dst(F32), src(F32, F32, F32), op(0x151, gfx8=0x1d0, gfx10=0x151, gfx11=0x219, gfx12=0x229)), # called v_min3_num_f32 in GFX12
+   ("v_min3_f32",              dst(F32), src(F32, F32, F32), op(0x151, gfx8=0x1d0, gfx10=0x151, gfx11=0x219, gfx12=0x229)),
    ("v_min3_i32",              dst(U32), src(U32, U32, U32), op(0x152, gfx8=0x1d1, gfx10=0x152, gfx11=0x21a)),
    ("v_min3_u32",              dst(U32), src(U32, U32, U32), op(0x153, gfx8=0x1d2, gfx10=0x153, gfx11=0x21b)),
-   ("v_max3_f32",              dst(F32), src(F32, F32, F32), op(0x154, gfx8=0x1d3, gfx10=0x154, gfx11=0x21c, gfx12=0x22a)), # called v_max3_num_f32 in GFX12
+   ("v_max3_f32",              dst(F32), src(F32, F32, F32), op(0x154, gfx8=0x1d3, gfx10=0x154, gfx11=0x21c, gfx12=0x22a)),
    ("v_max3_i32",              dst(U32), src(U32, U32, U32), op(0x155, gfx8=0x1d4, gfx10=0x155, gfx11=0x21d)),
    ("v_max3_u32",              dst(U32), src(U32, U32, U32), op(0x156, gfx8=0x1d5, gfx10=0x156, gfx11=0x21e)),
-   ("v_med3_f32",              dst(F32), src(F32, F32, F32), op(0x157, gfx8=0x1d6, gfx10=0x157, gfx11=0x21f, gfx12=0x231)), # called v_med3_num_f32 in GFX12
+   ("v_med3_f32",              dst(F32), src(F32, F32, F32), op(0x157, gfx8=0x1d6, gfx10=0x157, gfx11=0x21f, gfx12=0x231)),
    ("v_med3_i32",              dst(U32), src(U32, U32, U32), op(0x158, gfx8=0x1d7, gfx10=0x158, gfx11=0x220)),
    ("v_med3_u32",              dst(U32), src(U32, U32, U32), op(0x159, gfx8=0x1d8, gfx10=0x159, gfx11=0x221)),
    ("v_sad_u8",                dst(U32), src(U32, U32, U32), op(0x15a, gfx8=0x1d9, gfx10=0x15a, gfx11=0x222)),
@@ -1328,6 +1343,9 @@ VOP3 = {
    ("v_sad_u16",               dst(U32), src(U32, U32, U32), op(0x15c, gfx8=0x1db, gfx10=0x15c, gfx11=0x224)),
    ("v_sad_u32",               dst(U32), src(U32, U32, U32), op(0x15d, gfx8=0x1dc, gfx10=0x15d, gfx11=0x225)),
    ("v_cvt_pk_u8_f32",         dst(U32), src(F32, U32, U32), op(0x15e, gfx8=0x1dd, gfx10=0x15e, gfx11=0x226)),
+   ("p_v_cvt_pk_u8_f32",       dst(U32), src(F32), op(-1)),
+   ("v_div_f64",               dst(F64), src(F64, F64), op(gfx10=0x1d1, gfx11=0x343), InstrClass.ValuDouble),
+   ("v_dot2_f32_f32",          dst(noMods(F32)), noMods(src(PkF32, PkF32, F32)), op(gfx11=0x1e3)),
    ("v_div_fixup_f32",         dst(F32), src(F32, F32, F32), op(0x15f, gfx8=0x1de, gfx10=0x15f, gfx11=0x227)),
    ("v_div_fixup_f64",         dst(F64), src(F64, F64, F64), op(0x160, gfx8=0x1df, gfx10=0x160, gfx11=0x228)),
    ("v_lshl_b64",              dst(U64), src(U64, U32), op(0x161, gfx8=-1), InstrClass.Valu64),
@@ -1353,12 +1371,12 @@ VOP3 = {
    ("v_mqsad_u32_u8",          dst(U128), src(U64, U32, U128), op(gfx7=0x175, gfx8=0x1e7, gfx10=0x175, gfx11=0x23d), InstrClass.ValuQuarterRate32),
    ("v_mad_u64_u32",           dst(U64, VCC), src(U32, U32, U64), op(gfx7=0x176, gfx8=0x1e8, gfx10=0x176, gfx11=0x2fe), InstrClass.Valu64), # called v_mad_co_u64_u32 in GFX12
    ("v_mad_i64_i32",           dst(I64, VCC), src(U32, U32, I64), op(gfx7=0x177, gfx8=0x1e9, gfx10=0x177, gfx11=0x2ff), InstrClass.Valu64), # called v_mad_co_i64_i32 in GFX12
-   ("v_mad_legacy_f16",        dst(F16), src(F16, F16, F16), op(gfx8=0x1ea, gfx10=-1)),
-   ("v_mad_legacy_u16",        dst(U16), src(U16, U16, U16), op(gfx8=0x1eb, gfx10=-1)),
-   ("v_mad_legacy_i16",        dst(U16), src(U16, U16, U16), op(gfx8=0x1ec, gfx10=-1)),
+   ("v_mad_legacy_f16",        dst(F16), src(mods(F16), mods(F16), mods(F16)), op(gfx8=0x1ea, gfx10=-1)),
+   ("v_mad_legacy_u16",        dst(U16), src(mods(U16), mods(U16), mods(U16)), op(gfx8=0x1eb, gfx10=-1)),
+   ("v_mad_legacy_i16",        dst(U16), src(mods(U16), mods(U16), mods(U16)), op(gfx8=0x1ec, gfx10=-1)),
    ("v_perm_b32",              dst(U32), src(U32, U32, U32), op(gfx8=0x1ed, gfx10=0x344, gfx11=0x244)),
-   ("v_fma_legacy_f16",        dst(F16), src(F16, F16, F16), op(gfx8=0x1ee, gfx10=-1), InstrClass.ValuFma),
-   ("v_div_fixup_legacy_f16",  dst(F16), src(F16, F16, F16), op(gfx8=0x1ef, gfx10=-1)),
+   ("v_fma_legacy_f16",        dst(F16), src(mods(F16), mods(F16), mods(F16)), op(gfx8=0x1ee, gfx10=-1), InstrClass.ValuFma),
+   ("v_div_fixup_legacy_f16",  dst(F16), src(mods(F16), mods(F16), mods(F16)), op(gfx8=0x1ef, gfx10=-1)),
    ("v_cvt_pkaccum_u8_f32",    dst(U32), src(F32, U32, U32), op(0x12c, gfx8=0x1f0, gfx10=-1)),
    ("v_mad_u32_u16",           dst(U32), src(U16, U16, U32), op(gfx9=0x1f1, gfx10=0x373, gfx11=0x259)),
    ("v_mad_i32_i16",           dst(U32), src(U16, U16, U32), op(gfx9=0x1f2, gfx10=0x375, gfx11=0x25a)),
@@ -1383,11 +1401,11 @@ VOP3 = {
    ("v_mad_i16",               dst(U16), src(U16, U16, U16), op(gfx9=0x205, gfx10=0x35e, gfx11=0x253)),
    ("v_fma_f16",               dst(F16), src(F16, F16, F16), op(gfx9=0x206, gfx10=0x34b, gfx11=0x248)),
    ("v_div_fixup_f16",         dst(F16), src(F16, F16, F16), op(gfx9=0x207, gfx10=0x35f, gfx11=0x254)),
-   ("v_interp_p1ll_f16",       dst(F32), src(F32, M0), op(gfx8=0x274, gfx10=0x342, gfx11=-1)),
-   ("v_interp_p1lv_f16",       dst(F32), src(F32, M0, F16), op(gfx8=0x275, gfx10=0x343, gfx11=-1)),
-   ("v_interp_p2_legacy_f16",  dst(F16), src(F32, M0, F32), op(gfx8=0x276, gfx10=-1)),
-   ("v_interp_p2_f16",         dst(F16), src(F32, M0, F32), op(gfx9=0x277, gfx10=0x35a, gfx11=-1)),
-   ("v_interp_p2_hi_f16",      dst(F16), src(F32, M0, F32), op(gfx9=0x277, gfx10=0x35a, gfx11=-1)),
+   ("v_interp_p1ll_f16",       dst(F32), src(F32, M0), op(gfx8=0x274, gfx9=0x274, gfx10=0x342, gfx11=-1)),
+   ("v_interp_p1lv_f16",       dst(F32), src(F32, M0, F16), op(gfx8=0x275, gfx9=0x275, gfx10=0x343, gfx11=-1)),
+   ("v_interp_p2_legacy_f16",  dst(F16), src(F32, M0, F32), op(gfx8=0x276, gfx9=0x276, gfx10=-1)),
+   ("v_interp_p2_f16",         dst(mods(F16)), src(mods(F32), noMods(M0), mods(F32)), op(gfx8=0x277, gfx9=0x277, gfx10=-1)),
+   ("v_interp_p2_hi_f16",      dst(mods(F16)), src(F32, M0, F32), op(gfx9=0x277, gfx10=-1)),
    ("v_ldexp_f32",             dst(F32), src(F32, U32), op(0x12b, gfx8=0x288, gfx10=0x362, gfx11=0x31c)),
    ("v_readlane_b32_e64",      dst(U32), src(U32, U32), op(gfx8=0x289, gfx10=0x360)),
    ("v_writelane_b32_e64",     dst(U32), src(U32, U32, U32), op(gfx8=0x28a, gfx10=0x361)),
@@ -1517,18 +1535,18 @@ DS = {
    ("ds_or_b32",               op(0x0a)),
    ("ds_xor_b32",              op(0x0b)),
    ("ds_mskor_b32",            op(0x0c)),
-   ("ds_write_b32",            op(0x0d)), #ds_store_b32 in GFX11
-   ("ds_write2_b32",           op(0x0e)), #ds_store_2addr_b32 in GFX11
-   ("ds_write2st64_b32",       op(0x0f)), #ds_store_2addr_stride64_b32 in GFX11
-   ("ds_cmpst_b32",            op(0x10)), #ds_cmpstore_b32 in GFX11
-   ("ds_cmpst_f32",            op(0x11, gfx12=-1)), #ds_cmpstore_f32 in GFX11
-   ("ds_min_f32",              op(0x12)), #ds_min_num_f32 in GFX12
-   ("ds_max_f32",              op(0x13)), #ds_max_num_f32 in GFX12
+   ("ds_write_b32",            op(0x0d)),
+   ("ds_write2_b32",           op(0x0e)),
+   ("ds_write2st64_b32",       op(0x0f)),
+   ("ds_cmpst_b32",            op(0x10)),
+   ("ds_cmpst_f32",            op(0x11, gfx12=-1)),
+   ("ds_min_f32",              op(0x12)),
+   ("ds_max_f32",              op(0x13)),
    ("ds_nop",                  op(gfx7=0x14)),
-   ("ds_add_f32",              op(gfx8=0x15)),
-   ("ds_write_addtid_b32",     op(gfx8=0x1d, gfx10=0xb0)), #ds_store_addtid_b32 in GFX11
-   ("ds_write_b8",             op(0x1e)), #ds_store_b8 in GFX11
-   ("ds_write_b16",            op(0x1f)), #ds_store_b16 in GFX11
+   ("ds_add_f32",              op(gfx8=0x15, gfx9=0x15)),
+   ("ds_write_addtid_b32",     op(gfx8=0x1d, gfx10=0xb0)),
+   ("ds_write_b8",             op(0x1e)),
+   ("ds_write_b16",            op(0x1f)),
    ("ds_add_rtn_u32",          op(0x20)),
    ("ds_sub_rtn_u32",          op(0x21)),
    ("ds_rsub_rtn_u32",         op(0x22)),
@@ -1542,25 +1560,25 @@ DS = {
    ("ds_or_rtn_b32",           op(0x2a)),
    ("ds_xor_rtn_b32",          op(0x2b)),
    ("ds_mskor_rtn_b32",        op(0x2c)),
-   ("ds_wrxchg_rtn_b32",       op(0x2d)), #ds_storexchg_rtn_b32 in GFX11
-   ("ds_wrxchg2_rtn_b32",      op(0x2e)), #ds_storexchg_2addr_rtn_b32 in GFX11
-   ("ds_wrxchg2st64_rtn_b32",  op(0x2f)), #ds_storexchg_2addr_stride64_rtn_b32 in GFX11
-   ("ds_cmpst_rtn_b32",        op(0x30)), #ds_cmpstore_rtn_b32 in GFX11
-   ("ds_cmpst_rtn_f32",        op(0x31, gfx12=-1)), #ds_cmpstore_rtn_f32 in GFX11
-   ("ds_min_rtn_f32",          op(0x32)), #ds_min_num_rtn_f32 in GFX12
-   ("ds_max_rtn_f32",          op(0x33)), #ds_max_num_rtn_f32 in GFX12
+   ("ds_wrxchg_rtn_b32",       op(0x2d)),
+   ("ds_wrxchg2_rtn_b32",      op(0x2e)),
+   ("ds_wrxchg2st64_rtn_b32",  op(0x2f)),
+   ("ds_cmpst_rtn_b32",        op(0x30)),
+   ("ds_cmpst_rtn_f32",        op(0x31, gfx12=-1)),
+   ("ds_min_rtn_f32",          op(0x32)),
+   ("ds_max_rtn_f32",          op(0x33)),
    ("ds_wrap_rtn_b32",         op(gfx7=0x34, gfx12=-1)),
-   ("ds_add_rtn_f32",          op(gfx8=0x35, gfx10=0x55, gfx11=0x79)),
-   ("ds_read_b32",             op(0x36)), #ds_load_b32 in GFX11
-   ("ds_read2_b32",            op(0x37)), #ds_load_2addr_b32 in GFX11
-   ("ds_read2st64_b32",        op(0x38)), #ds_load_2addr_stride64_b32 in GFX11
-   ("ds_read_i8",              op(0x39)), #ds_load_i8 in GFX11
-   ("ds_read_u8",              op(0x3a)), #ds_load_u8 in GFX11
-   ("ds_read_i16",             op(0x3b)), #ds_load_i16 in GFX11
-   ("ds_read_u16",             op(0x3c)), #ds_load_u16 in GFX11
-   ("ds_swizzle_b32",          op(0x35, gfx8=0x3d, gfx10=0x35)), #data1 & offset, no addr/data2
-   ("ds_permute_b32",          op(gfx8=0x3e, gfx10=0xb2)),
-   ("ds_bpermute_b32",         op(gfx8=0x3f, gfx10=0xb3)),
+   ("ds_add_rtn_f32",          op(gfx8=0x35, gfx9=0x35, gfx10=0x55, gfx11=0x79)),
+   ("ds_read_b32",             op(0x36)),
+   ("ds_read2_b32",            op(0x37)),
+   ("ds_read2st64_b32",        op(0x38)),
+   ("ds_read_i8",              op(0x39)),
+   ("ds_read_u8",              op(0x3a)),
+   ("ds_read_i16",             op(0x3b)),
+   ("ds_read_u16",             op(0x3c)),
+   ("ds_swizzle_b32",          op(0x35, gfx8=0x3d, gfx9=0x3d, gfx10=0x35)),
+   ("ds_permute_b32",          op(gfx8=0x3e, gfx9=0x3e, gfx10=0xb2)),
+   ("ds_bpermute_b32",         op(gfx8=0x3f, gfx9=0x3f, gfx10=0xb3)),
    ("ds_add_u64",              op(0x40)),
    ("ds_sub_u64",              op(0x41)),
    ("ds_rsub_u64",             op(0x42)),
@@ -1574,21 +1592,21 @@ DS = {
    ("ds_or_b64",               op(0x4a)),
    ("ds_xor_b64",              op(0x4b)),
    ("ds_mskor_b64",            op(0x4c)),
-   ("ds_write_b64",            op(0x4d)), #ds_store_b64 in GFX11
-   ("ds_write2_b64",           op(0x4e)), #ds_store_2addr_b64 in GFX11
-   ("ds_write2st64_b64",       op(0x4f)), #ds_store_2addr_stride64_b64 in GFX11
-   ("ds_cmpst_b64",            op(0x50)), #ds_cmpstore_b64 in GFX11
-   ("ds_cmpst_f64",            op(0x51, gfx12=-1)), #ds_cmpstore_f64 in GFX11
-   ("ds_min_f64",              op(0x52)), #ds_min_num_f64 in GFX12
-   ("ds_max_f64",              op(0x53)), #ds_max_num_f64 in GFX12
-   ("ds_write_b8_d16_hi",      op(gfx9=0x54, gfx10=0xa0)), #ds_store_b8_d16_hi in GFX11
-   ("ds_write_b16_d16_hi",     op(gfx9=0x55, gfx10=0xa1)), #ds_store_b16_d16_hi in GFX11
-   ("ds_read_u8_d16",          op(gfx9=0x56, gfx10=0xa2)), #ds_load_u8_d16 in GFX11
-   ("ds_read_u8_d16_hi",       op(gfx9=0x57, gfx10=0xa3)), #ds_load_u8_d16_hi in GFX11
-   ("ds_read_i8_d16",          op(gfx9=0x58, gfx10=0xa4)), #ds_load_i8_d16 in GFX11
-   ("ds_read_i8_d16_hi",       op(gfx9=0x59, gfx10=0xa5)), #ds_load_i8_d16_hi in GFX11
-   ("ds_read_u16_d16",         op(gfx9=0x5a, gfx10=0xa6)), #ds_load_u16_d16 in GFX11
-   ("ds_read_u16_d16_hi",      op(gfx9=0x5b, gfx10=0xa7)), #ds_load_u16_d16_hi in GFX11
+   ("ds_write_b64",            op(0x4d)),
+   ("ds_write2_b64",           op(0x4e)),
+   ("ds_write2st64_b64",       op(0x4f)),
+   ("ds_cmpst_b64",            op(0x50)),
+   ("ds_cmpst_f64",            op(0x51, gfx12=-1)),
+   ("ds_min_f64",              op(0x52)),
+   ("ds_max_f64",              op(0x53)),
+   ("ds_write_b8_d16_hi",      op(gfx9=0x54, gfx10=0xa0)),
+   ("ds_write_b16_d16_hi",     op(gfx9=0x55, gfx10=0xa1)),
+   ("ds_read_u8_d16",          op(gfx9=0x56, gfx10=0xa2)),
+   ("ds_read_u8_d16_hi",       op(gfx9=0x57, gfx10=0xa3)),
+   ("ds_read_i8_d16",          op(gfx9=0x58, gfx10=0xa4)),
+   ("ds_read_i8_d16_hi",       op(gfx9=0x59, gfx10=0xa5)),
+   ("ds_read_u16_d16",         op(gfx9=0x5a, gfx10=0xa6)),
+   ("ds_read_u16_d16_hi",      op(gfx9=0x5b, gfx10=0xa7)),
    ("ds_add_rtn_u64",          op(0x60)),
    ("ds_sub_rtn_u64",          op(0x61)),
    ("ds_rsub_rtn_u64",         op(0x62)),
@@ -1602,16 +1620,16 @@ DS = {
    ("ds_or_rtn_b64",           op(0x6a)),
    ("ds_xor_rtn_b64",          op(0x6b)),
    ("ds_mskor_rtn_b64",        op(0x6c)),
-   ("ds_wrxchg_rtn_b64",       op(0x6d)), #ds_storexchg_rtn_b64 in GFX11
-   ("ds_wrxchg2_rtn_b64",      op(0x6e)), #ds_storexchg_2addr_rtn_b64 in GFX11
-   ("ds_wrxchg2st64_rtn_b64",  op(0x6f)), #ds_storexchg_2addr_stride64_rtn_b64 in GFX11
-   ("ds_cmpst_rtn_b64",        op(0x70)), #ds_cmpstore_rtn_b64 in GFX11
-   ("ds_cmpst_rtn_f64",        op(0x71, gfx12=-1)), #ds_cmpstore_rtn_f64 in GFX11
-   ("ds_min_rtn_f64",          op(0x72)), #ds_min_num_f64 in GFX12
-   ("ds_max_rtn_f64",          op(0x73)), #ds_max_num_f64 in GFX12
-   ("ds_read_b64",             op(0x76)), #ds_load_b64 in GFX11
-   ("ds_read2_b64",            op(0x77)), #ds_load_2addr_b64 in GFX11
-   ("ds_read2st64_b64",        op(0x78)), #ds_load_2addr_stride64_b64 in GFX11
+   ("ds_wrxchg_rtn_b64",       op(0x6d)),
+   ("ds_wrxchg2_rtn_b64",      op(0x6e)),
+   ("ds_wrxchg2st64_rtn_b64",  op(0x6f)),
+   ("ds_cmpst_rtn_b64",        op(0x70)),
+   ("ds_cmpst_rtn_f64",        op(0x71, gfx12=-1)),
+   ("ds_min_rtn_f64",          op(0x72)),
+   ("ds_max_rtn_f64",          op(0x73)),
+   ("ds_read_b64",             op(0x76)),
+   ("ds_read2_b64",            op(0x77)),
+   ("ds_read2st64_b64",        op(0x78)),
    ("ds_condxchg32_rtn_b64",   op(gfx7=0x7e)),
    ("ds_add_src2_u32",         op(0x80, gfx11=-1)),
    ("ds_sub_src2_u32",         op(0x81, gfx11=-1)),
@@ -1628,17 +1646,19 @@ DS = {
    ("ds_write_src2_b32",       op(0x8d, gfx11=-1)),
    ("ds_min_src2_f32",         op(0x92, gfx11=-1)),
    ("ds_max_src2_f32",         op(0x93, gfx11=-1)),
-   ("ds_add_src2_f32",         op(gfx8=0x95, gfx11=-1)),
-   ("ds_gws_sema_release_all", op(gfx7=0x18, gfx8=0x98, gfx10=0x18, gfx12=-1)),
-   ("ds_gws_init",             op(0x19, gfx8=0x99, gfx10=0x19, gfx12=-1)),
-   ("ds_gws_sema_v",           op(0x1a, gfx8=0x9a, gfx10=0x1a, gfx12=-1)),
-   ("ds_gws_sema_br",          op(0x1b, gfx8=0x9b, gfx10=0x1b, gfx12=-1)),
-   ("ds_gws_sema_p",           op(0x1c, gfx8=0x9c, gfx10=0x1c, gfx12=-1)),
-   ("ds_gws_barrier",          op(0x1d, gfx8=0x9d, gfx10=0x1d, gfx12=-1)),
-   ("ds_read_addtid_b32",      op(gfx8=0xb6, gfx10=0xb1)), #ds_load_addtid_b32 in GFX11
-   ("ds_consume",              op(0x3d, gfx8=0xbd, gfx10=0x3d)),
-   ("ds_append",               op(0x3e, gfx8=0xbe, gfx10=0x3e)),
-   ("ds_ordered_count",        op(0x3f, gfx8=0xbf, gfx10=0x3f, gfx12=-1)),
+   ("ds_add_src2_f32",         op(gfx8=0x95, gfx9=0x95, gfx11=-1)),
+   ("ds_gws_sema_release_all", op(gfx7=0x18, gfx8=0x98, gfx9=0x98, gfx10=0x18, gfx12=-1)),
+   ("ds_gws_init",             op(0x19, gfx8=0x99, gfx9=0x99, gfx10=0x19, gfx12=-1)),
+   ("ds_gws_sema_v",           op(0x1a, gfx8=0x9a, gfx9=0x9a, gfx10=0x1a, gfx12=-1)),
+   ("ds_gws_sema_br",          op(0x1b, gfx8=0x9b, gfx9=0x9b, gfx10=0x1b, gfx12=-1)),
+   ("ds_gws_sema_p",           op(0x1c, gfx8=0x9c, gfx9=0x9c, gfx10=0x1c, gfx12=-1)),
+   ("ds_gws_barrier",          op(0x1d, gfx8=0x9d, gfx9=0x9d, gfx10=0x1d, gfx12=-1)),
+   ("ds_read_addtid_b32",      op(gfx8=0xb6, gfx9=0xb6, gfx10=0xb1)),
+
+   ("ds_consume",              op(0x3d, gfx8=0xbd, gfx9=0xbd, gfx10=0x3d)),
+   ("ds_append",               op(0x3e, gfx8=0xbe, gfx9=0xbe, gfx10=0x3e)),
+
+   ("ds_ordered_count",        op(0x3f, gfx8=0xbf, gfx9=0xbf, gfx10=0x3f, gfx12=-1)),
    ("ds_add_src2_u64",         op(0xc0, gfx11=-1)),
    ("ds_sub_src2_u64",         op(0xc1, gfx11=-1)),
    ("ds_rsub_src2_u64",        op(0xc2, gfx11=-1)),
@@ -1654,11 +1674,11 @@ DS = {
    ("ds_write_src2_b64",       op(0xcd, gfx11=-1)),
    ("ds_min_src2_f64",         op(0xd2, gfx11=-1)),
    ("ds_max_src2_f64",         op(0xd3, gfx11=-1)),
-   ("ds_write_b96",            op(gfx7=0xde)), #ds_store_b96 in GFX11
-   ("ds_write_b128",           op(gfx7=0xdf)), #ds_store_b128 in GFX11
+   ("ds_write_b96",            op(gfx7=0xde)),
+   ("ds_write_b128",           op(gfx7=0xdf)),
    ("ds_condxchg32_rtn_b128",  op(gfx7=0xfd, gfx9=-1)),
-   ("ds_read_b96",             op(gfx7=0xfe)), #ds_load_b96 in GFX11
-   ("ds_read_b128",            op(gfx7=0xff)), #ds_load_b128 in GFX11
+   ("ds_read_b96",             op(gfx7=0xfe)),
+   ("ds_read_b128",            op(gfx7=0xff)),
    ("ds_add_gs_reg_rtn",       op(gfx11=0x7a, gfx12=-1)),
    ("ds_sub_gs_reg_rtn",       op(gfx11=0x7b, gfx12=-1)),
    ("ds_cond_sub_u32",         op(gfx12=0x98)),
@@ -1669,7 +1689,7 @@ DS = {
    ("ds_pk_add_rtn_f16",       op(gfx12=0xaa)),
    ("ds_pk_add_bf16",          op(gfx12=0x9b)),
    ("ds_pk_add_rtn_bf16",      op(gfx12=0xab)),
-   ("ds_bvh_stack_push4_pop1_rtn_b32", op(gfx11=0xad, gfx12=0xe0)), #ds_bvh_stack_rtn in GFX11
+   ("ds_bvh_stack_push4_pop1_rtn_b32", op(gfx11=0xad, gfx12=0xe0)),
    ("ds_bvh_stack_push8_pop1_rtn_b32", op(gfx12=0xe1)),
    ("ds_bvh_stack_push8_pop2_rtn_b64", op(gfx12=0xe2)),
 }

--- a/src/amd/compiler/aco_optimizer.cpp	2025-05-31 22:57:26.003334290 +0200
+++ b/src/amd/compiler/aco_optimizer.cpp	2026-02-11 00:30:01.222104109 +0200
@@ -14,6 +14,16 @@
 #include <array>
 #include <vector>
 
+#if defined(__clang__) || defined(__GNUC__)
+#define ACO_HOT [[gnu::hot]]
+#define LIKELY(x)   (__builtin_expect(!!(x), 1))
+#define UNLIKELY(x) (__builtin_expect(!!(x), 0))
+#else
+#define ACO_HOT
+#define LIKELY(x)   (x)
+#define UNLIKELY(x) (x)
+#endif
+
 namespace aco {
 
 namespace {
@@ -245,6 +255,8 @@ struct ssa_info {
          return false;
       if (phys_reg != exec && phys_reg != exec_hi)
          return true;
+      if (UNLIKELY(!parent_instr))
+         return false;
       return exec_id == parent_instr->pass_flags;
    }
 };
@@ -2731,7 +2743,6 @@ label_instruction(opt_ctx& ctx, aco_ptr<
                   }
                }
             }
-
             offset += ops[i].bytes();
          }
       } while (progress);
@@ -3356,16 +3367,22 @@ can_match_op(opt_ctx& ctx, Operand op, u
 
 bool
 match_and_apply_patterns(opt_ctx& ctx, alu_opt_info& info,
-                         const aco::small_vec<combine_instr_pattern, 8>& patterns)
+                         const aco::small_vec<combine_instr_pattern, 24>& patterns)
 {
    if (patterns.empty())
       return false;
 
-   unsigned total_mask = 0;
+   const unsigned num_ops = info.operands.size();
+   if (num_ops == 0)
+      return false;
+
+   uint32_t total_mask = 0;
    for (const combine_instr_pattern& pattern : patterns)
       total_mask |= pattern.operand_mask;
 
-   for (unsigned i = 0; i < info.operands.size(); i++) {
+   /* Only keep bits for operands we can legally match. */
+   total_mask &= BITFIELD_MASK(num_ops);
+   for (unsigned i = 0; i < num_ops; i++) {
       if (!can_match_op(ctx, info.operands[i].op, info.pass_flags))
          total_mask &= ~BITFIELD_BIT(i);
    }
@@ -3373,61 +3390,58 @@ match_and_apply_patterns(opt_ctx& ctx, a
    if (!total_mask)
       return false;
 
-   aco::small_vec<int, 4> indices;
-   indices.reserve(util_bitcount(total_mask));
-   u_foreach_bit (i, total_mask)
-      indices.push_back(i);
-
-   std::stable_sort(indices.begin(), indices.end(),
-                    [&](int a, int b)
-                    {
-                       Temp temp_a = info.operands[a].op.getTemp();
-                       Temp temp_b = info.operands[b].op.getTemp();
+   auto try_fold_at = [&](unsigned op_idx) -> bool {
+      const Temp tmp = info.operands[op_idx].op.getTemp();
 
-                       /* Less uses make it more likely/profitable to eliminate an instruction. */
-                       if (ctx.uses[temp_a.id()] != ctx.uses[temp_b.id()])
-                          return ctx.uses[temp_a.id()] < ctx.uses[temp_b.id()];
-
-                       /* Prefer eliminating VALU instructions. */
-                       if (temp_a.type() != temp_b.type())
-                          return temp_a.type() == RegType::vgpr;
-
-                       /* The id is a good approximation for instruction order,
-                        * prefer instructions closer to info to not increase register pressure
-                        * as much.
-                        */
-                       return temp_a.id() > temp_b.id();
-                    });
-
-   for (unsigned op_idx : indices) {
-      Temp tmp = info.operands[op_idx].op.getTemp();
       alu_opt_info op_instr;
       if (!alu_opt_gather_info(ctx, ctx.info[tmp.id()].parent_instr, op_instr))
-         continue;
+         return false;
 
+      /* Dont fold if the producer has modifiers that we cannot safely propagate. */
       if (op_instr.clamp || op_instr.omod || op_instr.f32_to_f16)
-         continue;
+         return false;
 
-      aco_type type = instr_info.alu_opcode_infos[(int)info.opcode].op_types[op_idx];
+      const aco_type type = instr_info.alu_opcode_infos[(int)info.opcode].op_types[op_idx];
       if (!backpropagate_input_modifiers(ctx, op_instr, info.operands[op_idx], type))
-         continue;
+         return false;
+
+      const unsigned rem = num_ops - 1;
+      const unsigned op_count = rem + (unsigned)op_instr.operands.size();
 
       for (const combine_instr_pattern& pattern : patterns) {
-         if (!(pattern.operand_mask & BITFIELD_BIT(op_idx)) ||
-             op_instr.opcode != pattern.src_opcode)
+         if (!(pattern.operand_mask & BITFIELD_BIT(op_idx)))
+            continue;
+         if (op_instr.opcode != pattern.src_opcode)
             continue;
 
          if (pattern.less_aggressive && ctx.uses[tmp.id()] > ctx.uses[info.defs[0].tempId()])
             continue;
 
-         alu_opt_info new_info = info;
+         /* Hard safety: swizzle must match operand count (prevents OOB reads). */
+         const size_t sw_len = strlen(pattern.swizzle);
+         if (UNLIKELY(sw_len != op_count))
+            continue;
 
-         unsigned rem = info.operands.size() - 1;
-         unsigned op_count = rem + op_instr.operands.size();
+         alu_opt_info new_info = info;
          new_info.operands.resize(op_count);
-         assert(strlen(pattern.swizzle) == op_count);
+         new_info.opcode = pattern.res_opcode;
+
          for (unsigned i = 0; i < op_count; i++) {
-            unsigned src_idx = pattern.swizzle[i] - '0';
+            const char c = pattern.swizzle[i];
+            const unsigned src_idx = (unsigned)(c - '0');
+
+#ifndef NDEBUG
+            /* Swizzles are encoded as digits in current ACO patterns. */
+            assert(c >= '0' && c <= '9');
+#endif
+            /* Release build safety too (mis-sized swizzles are handled above,
+             * but this also prevents weird characters from becoming huge indices).
+             */
+            if (UNLIKELY(c < '0' || c > '9'))
+               goto next_pattern;
+            if (UNLIKELY(src_idx >= op_count))
+               goto next_pattern;
+
             if (src_idx < op_idx)
                new_info.operands[i] = info.operands[src_idx];
             else if (src_idx < rem)
@@ -3436,8 +3450,6 @@ match_and_apply_patterns(opt_ctx& ctx, a
                new_info.operands[i] = op_instr.operands[src_idx - rem];
          }
 
-         new_info.opcode = pattern.res_opcode;
-
          if (op_instr.defs[0].isPrecise())
             new_info.defs[0].setPrecise(true);
          if (op_instr.defs[0].isNaNPreserve())
@@ -3452,9 +3464,58 @@ match_and_apply_patterns(opt_ctx& ctx, a
             info = std::move(new_info);
             return true;
          }
+
+      next_pattern:
+         continue;
+      }
+
+      return false;
+   };
+
+   /* Ultra-fast path for the common case: exactly one viable operand candidate. */
+   if (util_bitcount(total_mask) == 1) {
+      uint32_t mask = total_mask;
+      const unsigned op_idx = u_bit_scan(&mask);
+      return try_fold_at(op_idx);
+   }
+
+   /* Multi-candidate path:
+    * Deterministic stable ordering (tiny N) without stable_sort() overhead.
+    */
+   auto better = [&](unsigned a, unsigned b) -> bool {
+      const Temp ta = info.operands[a].op.getTemp();
+      const Temp tb = info.operands[b].op.getTemp();
+
+      const uint16_t uses_a = ctx.uses[ta.id()];
+      const uint16_t uses_b = ctx.uses[tb.id()];
+      if (uses_a != uses_b)
+         return uses_a < uses_b;
+
+      /* Prefer eliminating VALU producers. */
+      if (ta.type() != tb.type())
+         return ta.type() == RegType::vgpr;
+
+      /* Prefer closer producers to reduce live ranges. */
+      return ta.id() > tb.id();
+   };
+
+   aco::small_vec<unsigned, 4> indices;
+   indices.reserve(util_bitcount(total_mask));
+   u_foreach_bit (i, total_mask) {
+      indices.push_back(i);
+      /* swap-back insertion sort (stable & minimal moves) */
+      for (unsigned k = indices.size() - 1; k > 0; --k) {
+         if (!better(indices[k], indices[k - 1]))
+            break;
+         std::swap(indices[k], indices[k - 1]);
       }
    }
 
+   for (unsigned op_idx : indices) {
+      if (try_fold_at(op_idx))
+         return true;
+   }
+
    return false;
 }
 
@@ -4203,6 +4264,483 @@ and_cb(opt_ctx& ctx, alu_opt_info& info)
    return func1(ctx, info) && func2(ctx, info);
 }
 
+/* =============================================================================
+ * Helper Callbacks for Peephole Optimizations
+ * ============================================================================= */
+
+/* Helper to safely compare operands accounting for constants and temps.
+ * Performance: Ordered checks by frequency (temps most common in shaders).
+ */
+static inline bool
+operands_equal(const Operand& a, const Operand& b)
+{
+   /* Fast path: both are temps (most common case in shader IR) */
+   if (a.isTemp() && b.isTemp())
+      return a.tempId() == b.tempId();
+
+   /* Constants: must have same size and value */
+   if (a.isConstant() && b.isConstant()) {
+      if (a.bytes() != b.bytes())
+         return false;
+
+      /* Avoid UB: compute mask safely for all sizes including 64-bit */
+      const unsigned bits = a.bytes() * 8u;
+      const uint64_t mask = (bits >= 64) ? ~0ull : ((1ull << bits) - 1ull);
+      return (a.constantValue64() & mask) == (b.constantValue64() & mask);
+   }
+
+   /* Fixed physical registers: compare reg and class */
+   if (a.isFixed() && b.isFixed())
+      return a.physReg() == b.physReg() && a.regClass() == b.regClass();
+
+   return false;
+}
+
+/* Validates that the SCC definition is dead/unused.
+ * Essential when replacing SCC-writing instructions with SCC-preserving ones.
+ * Returns true and removes SCC def only if safe.
+ */
+static inline bool
+scc_def_dead(const opt_ctx& ctx, const Definition& scc_def)
+{
+   if (scc_def.isTemp())
+      return ctx.uses[scc_def.tempId()] == 0;
+
+   if (scc_def.isFixed() && scc_def.physReg() == scc)
+      return scc_def.isKill();
+
+   return false;
+}
+
+static bool
+check_scc_dead_cb(opt_ctx& ctx, alu_opt_info& info)
+{
+   if (info.defs.size() < 2)
+      return false;
+
+   if (!scc_def_dead(ctx, info.defs[1]))
+      return false;
+
+   info.defs.pop_back();
+   return true;
+}
+
+/* s_or_b32(a, 1<<b) -> s_bitset1_b32(a,b), SCC must be dead */
+static bool
+s_bitset_cb(opt_ctx& ctx, alu_opt_info& info)
+{
+   if (info.operands.size() != 3)
+      return false;
+   if (!check_constant(ctx, info, info.operands.size() - 1, 1))
+      return false;
+
+   info.operands.pop_back();
+   return check_scc_dead_cb(ctx, info);
+}
+
+/* s_and_b32(a, 1<<b) -> s_bitcmp1_b32(a,b), result must be dead, SCC kept */
+static bool
+s_bitcmp1_cb(opt_ctx& ctx, alu_opt_info& info)
+{
+   if (info.operands.size() != 3)
+      return false;
+   if (!check_constant(ctx, info, info.operands.size() - 1, 1))
+      return false;
+
+   if (info.defs.empty() || !info.defs[0].isTemp() ||
+       ctx.uses[info.defs[0].tempId()] != 0)
+      return false;
+
+   if (info.defs.size() < 2)
+      return false;
+
+   info.operands.pop_back();
+   info.defs[0] = info.defs[1];
+   info.defs.pop_back();
+   return true;
+}
+
+/* Validates v_sub(v_max(a,b), v_min(a,b)) -> v_sad_u32(a, b, 0)
+ * The SAD instruction computes |a - b| + c, so with c=0 we get |a - b|.
+ * This pattern recognizes max(a,b) - min(a,b) which equals |a - b|.
+ *
+ * Operand layout after pattern matching for v_sub_u32(max_result, min_result):
+ * When folding max at operand 0:
+ *   new_info.operands[0] = min_result (remaining from v_sub)
+ *   new_info.operands[1] = max.operands[0] = a
+ *   new_info.operands[2] = max.operands[1] = b
+ *
+ * We need to verify min_result comes from v_min_u32(a, b) with same a, b.
+ * Then reorder to v_sad_u32(a, b, 0).
+ */
+static bool
+sad_pattern_cb(opt_ctx& ctx, alu_opt_info& info)
+{
+   /* Expected layout after folding v_max_u32 into v_sub_u32 with swizzle "012":
+    *   operands[0] = min_result temp
+    *   operands[1] = a
+    *   operands[2] = b
+    */
+   if (info.operands.size() != 3)
+      return false;
+
+   if (!info.operands[0].op.isTemp())
+      return false;
+
+   const uint32_t min_id = info.operands[0].op.tempId();
+   if (min_id >= ctx.info.size())
+      return false;
+
+   Instruction* min_instr = ctx.info[min_id].parent_instr;
+   if (!min_instr)
+      return false;
+
+   /* Conservative safety: exec context must match. */
+   if (min_instr->pass_flags != info.pass_flags)
+      return false;
+
+   if (min_instr->opcode != aco_opcode::v_min_u32)
+      return false;
+
+   if (min_instr->definitions.empty() || !min_instr->definitions[0].isTemp() ||
+       min_instr->definitions[0].getTemp() != info.operands[0].op.getTemp())
+      return false;
+
+   if (min_instr->operands.size() < 2)
+      return false;
+
+   /* Reject any modifiers that could change semantics. */
+   if (min_instr->isVALU()) {
+      const VALU_instruction& v = min_instr->valu();
+      if (v.clamp || v.omod)
+         return false;
+      for (unsigned i = 0; i < 2; ++i) {
+         if (v.neg[i] || v.abs[i])
+            return false;
+      }
+   }
+
+   /* Reject modifiers on the folded max operands. */
+   if (info.operands[1].neg[0] || info.operands[1].abs[0] ||
+       info.operands[2].neg[0] || info.operands[2].abs[0])
+      return false;
+
+   const Operand& max_a = info.operands[1].op;
+   const Operand& max_b = info.operands[2].op;
+   const Operand& min_a = min_instr->operands[0];
+   const Operand& min_b = min_instr->operands[1];
+
+   const bool match_direct = operands_equal(max_a, min_a) && operands_equal(max_b, min_b);
+   const bool match_swapped = operands_equal(max_a, min_b) && operands_equal(max_b, min_a);
+   if (!match_direct && !match_swapped)
+      return false;
+
+   /* Rewrite to v_sad_u32(a, b, 0). */
+   info.operands[0] = info.operands[1]; /* a */
+   info.operands[1] = info.operands[2]; /* b */
+   info.operands[2].op = Operand::c32(0);
+   info.operands[2].neg[0] = false;
+   info.operands[2].abs[0] = false;
+
+   return true;
+}
+
+/* s_sub_i32(s_max_i32(a,b), s_min_i32(a,b)) -> s_absdiff_i32(a,b), SCC dead */
+static bool
+scalar_sad_pattern_cb(opt_ctx& ctx, alu_opt_info& info)
+{
+   if (info.operands.size() != 3)
+      return false;
+
+   if (!info.operands[0].op.isTemp())
+      return false;
+
+   const uint32_t min_id = info.operands[0].op.tempId();
+   if (min_id >= ctx.info.size())
+      return false;
+
+   Instruction* min_instr = ctx.info[min_id].parent_instr;
+   if (!min_instr || min_instr->opcode != aco_opcode::s_min_i32)
+      return false;
+
+   if (min_instr->definitions.empty() || !min_instr->definitions[0].isTemp() ||
+       min_instr->definitions[0].getTemp() != info.operands[0].op.getTemp())
+      return false;
+
+   if (min_instr->operands.size() < 2)
+      return false;
+
+   const Operand& max_a = info.operands[1].op;
+   const Operand& max_b = info.operands[2].op;
+   const Operand& min_a = min_instr->operands[0];
+   const Operand& min_b = min_instr->operands[1];
+
+   const bool match_direct = operands_equal(max_a, min_a) && operands_equal(max_b, min_b);
+   const bool match_swapped = operands_equal(max_a, min_b) && operands_equal(max_b, min_a);
+   if (!match_direct && !match_swapped)
+      return false;
+
+   info.operands[0] = info.operands[1];
+   info.operands[1] = info.operands[2];
+   info.operands.pop_back();
+
+   if (info.defs.size() >= 2)
+      return check_scc_dead_cb(ctx, info);
+
+   return true;
+}
+
+/* Validates v_or(v_lshlrev(a, k), v_lshrrev(b, 32-k)) -> v_alignbit(a, b, 32-k)
+ * This is a funnel shift / rotate pattern.
+ * After folding lshl into v_or: operands = [lshr_result, lshl.src, lshl.shift]
+ */
+static bool
+funnel_shift_lshl_cb(opt_ctx& ctx, alu_opt_info& info)
+{
+   if (info.operands.size() != 3)
+      return false;
+
+   if (!info.operands[0].op.isTemp())
+      return false;
+
+   const uint32_t lshr_id = info.operands[0].op.tempId();
+   if (lshr_id >= ctx.info.size())
+      return false;
+
+   Instruction* lshr = ctx.info[lshr_id].parent_instr;
+   if (!lshr || lshr->opcode != aco_opcode::v_lshrrev_b32)
+      return false;
+
+   if (lshr->isDPP() || lshr->isSDWA())
+      return false;
+
+   if (lshr->isVALU() && lshr->valu().usesModifiers())
+      return false;
+
+   uint64_t lshl_k = 0;
+   if (!op_info_get_constant(ctx, info.operands[2], {aco_base_type_uint, 1, 32}, &lshl_k))
+      return false;
+
+   if (lshl_k == 0 || lshl_k > 31)
+      return false;
+
+   if (lshr->operands.size() < 2)
+      return false;
+
+   uint64_t lshr_k = 0;
+   const Operand& lshr_shift_op = lshr->operands[0];
+   if (lshr_shift_op.isConstant()) {
+      lshr_k = lshr_shift_op.constantValue();
+   } else if (lshr_shift_op.isTemp()) {
+      const uint32_t sid = lshr_shift_op.tempId();
+      if (sid < ctx.info.size() && ctx.info[sid].is_constant())
+         lshr_k = ctx.info[sid].val;
+      else
+         return false;
+   } else {
+      return false;
+   }
+
+   if (lshr_k == 0 || lshr_k > 31)
+      return false;
+
+   if (((uint32_t)lshl_k + (uint32_t)lshr_k) != 32u)
+      return false;
+
+   /* alignbit: src0=low (b), src1=high (a), shift = lshr_k */
+   info.operands[0].op = lshr->operands[1]; /* b (low) */
+   info.operands[0].neg[0] = false;
+   info.operands[0].abs[0] = false;
+
+   info.operands[1].op = info.operands[1].op; /* a (high) */
+   info.operands[1].neg[0] = false;
+   info.operands[1].abs[0] = false;
+
+   if ((lshr_k % 8u) == 0u && ctx.program->gfx_level >= GFX8) {
+      const uint32_t byte_shift = (uint32_t)lshr_k / 8u;
+      if (byte_shift == 0 || byte_shift > 3)
+         return false;
+
+      info.opcode = aco_opcode::v_alignbyte_b32;
+      info.operands[2].op = Operand::c32(byte_shift);
+   } else {
+      info.opcode = aco_opcode::v_alignbit_b32;
+      info.operands[2].op = Operand::c32((uint32_t)lshr_k);
+   }
+
+   info.operands[2].neg[0] = false;
+   info.operands[2].abs[0] = false;
+
+   return true;
+}
+
+static bool
+funnel_shift_lshr_cb(opt_ctx& ctx, alu_opt_info& info)
+{
+   if (info.operands.size() != 3)
+      return false;
+
+   if (!info.operands[0].op.isTemp())
+      return false;
+
+   const uint32_t lshl_id = info.operands[0].op.tempId();
+   if (lshl_id >= ctx.info.size())
+      return false;
+
+   Instruction* lshl = ctx.info[lshl_id].parent_instr;
+   if (!lshl || lshl->opcode != aco_opcode::v_lshlrev_b32)
+      return false;
+
+   if (lshl->isDPP() || lshl->isSDWA())
+      return false;
+
+   if (lshl->isVALU() && lshl->valu().usesModifiers())
+      return false;
+
+   uint64_t lshr_k = 0;
+   if (!op_info_get_constant(ctx, info.operands[2], {aco_base_type_uint, 1, 32}, &lshr_k))
+      return false;
+
+   if (lshr_k == 0 || lshr_k > 31)
+      return false;
+
+   if (lshl->operands.size() < 2)
+      return false;
+
+   uint64_t lshl_k = 0;
+   const Operand& lshl_shift_op = lshl->operands[0];
+   if (lshl_shift_op.isConstant()) {
+      lshl_k = lshl_shift_op.constantValue();
+   } else if (lshl_shift_op.isTemp()) {
+      const uint32_t sid = lshl_shift_op.tempId();
+      if (sid < ctx.info.size() && ctx.info[sid].is_constant())
+         lshl_k = ctx.info[sid].val;
+      else
+         return false;
+   } else {
+      return false;
+   }
+
+   if (lshl_k == 0 || lshl_k > 31)
+      return false;
+
+   if (((uint32_t)lshl_k + (uint32_t)lshr_k) != 32u)
+      return false;
+
+   /* alignbit: src0=low (b), src1=high (a), shift = lshr_k */
+   info.operands[0].op = info.operands[1].op; /* b (low) */
+   info.operands[0].neg[0] = false;
+   info.operands[0].abs[0] = false;
+
+   info.operands[1].op = lshl->operands[1]; /* a (high) */
+   info.operands[1].neg[0] = false;
+   info.operands[1].abs[0] = false;
+
+   if ((lshr_k % 8u) == 0u && ctx.program->gfx_level >= GFX8) {
+      const uint32_t byte_shift = (uint32_t)lshr_k / 8u;
+      if (byte_shift == 0 || byte_shift > 3)
+         return false;
+
+      info.opcode = aco_opcode::v_alignbyte_b32;
+      info.operands[2].op = Operand::c32(byte_shift);
+   } else {
+      info.opcode = aco_opcode::v_alignbit_b32;
+      info.operands[2].op = Operand::c32((uint32_t)lshr_k);
+   }
+
+   info.operands[2].neg[0] = false;
+   info.operands[2].abs[0] = false;
+
+   return true;
+}
+
+/* Eliminates redundant min/max when both operands are identical: min(a,a) -> mov a */
+static bool
+min_max_redundant_cb(opt_ctx& ctx, alu_opt_info& info)
+{
+   if (info.operands.size() < 2)
+      return false;
+
+   if (!operands_equal(info.operands[0].op, info.operands[1].op))
+      return false;
+
+   /* Modifiers must match for the optimization to be valid */
+   if (info.operands[0].neg[0] != info.operands[1].neg[0] ||
+       info.operands[0].abs[0] != info.operands[1].abs[0])
+      return false;
+
+   info.opcode = aco_opcode::v_mov_b32;
+   info.operands.resize(1);
+   return true;
+}
+
+/* Scalar version of min_max_redundant_cb */
+static bool
+scalar_min_max_redundant_cb(opt_ctx& ctx, alu_opt_info& info)
+{
+   if (info.operands.size() < 2)
+      return false;
+
+   if (!operands_equal(info.operands[0].op, info.operands[1].op))
+      return false;
+
+   if (info.defs.size() > 1) {
+      if (!scc_def_dead(ctx, info.defs[1]))
+         return false;
+      info.defs.pop_back();
+   }
+
+   info.opcode = aco_opcode::s_mov_b32;
+   info.operands.resize(1);
+   return true;
+}
+
+static bool
+scalar_absdiff_from_minmax_i32_cb(opt_ctx& ctx, alu_opt_info& info)
+{
+   if (info.defs.size() > 1) {
+      if (!scc_def_dead(ctx, info.defs[1]))
+         return false;
+      info.defs.pop_back();
+   }
+
+   if (info.operands.size() != 3)
+      return false;
+
+   if (!info.operands[0].op.isTemp())
+      return false;
+
+   const uint32_t min_id = info.operands[0].op.tempId();
+   if (min_id >= ctx.info.size())
+      return false;
+
+   Instruction* min_instr = ctx.info[min_id].parent_instr;
+   if (!min_instr || min_instr->opcode != aco_opcode::s_min_i32)
+      return false;
+
+   if (min_instr->definitions.empty() || !min_instr->definitions[0].isTemp() ||
+       min_instr->definitions[0].getTemp() != info.operands[0].op.getTemp())
+      return false;
+
+   if (min_instr->operands.size() < 2)
+      return false;
+
+   const Operand& max_a = info.operands[1].op;
+   const Operand& max_b = info.operands[2].op;
+   const Operand& min_a = min_instr->operands[0];
+   const Operand& min_b = min_instr->operands[1];
+
+   const bool match_direct = operands_equal(max_a, min_a) && operands_equal(max_b, min_b);
+   const bool match_swapped = operands_equal(max_a, min_b) && operands_equal(max_b, min_a);
+   if (!match_direct && !match_swapped)
+      return false;
+
+   info.operands[0] = info.operands[1]; /* a */
+   info.operands[1] = info.operands[2]; /* b */
+   info.operands.pop_back();
+   return true;
+}
+
 void
 combine_instruction(opt_ctx& ctx, aco_ptr<Instruction>& instr)
 {
@@ -4210,11 +4748,6 @@ combine_instruction(opt_ctx& ctx, aco_pt
       return;
 
    if (instr->opcode == aco_opcode::p_split_vector && instr->operands[0].size() == 1) {
-      /* If all except the first definition still have their extract label, we will likely
-       * eliminate the whole split instruction after copy propagating the first one.
-       * Unconditional copy propagation would mean we end up with more splits
-       * that don't kill their operands.
-       */
       bool will_be_removed = true;
       for (unsigned i = 1; i < instr->definitions.size(); i++)
          will_be_removed &= ctx.info[instr->definitions[i].tempId()].is_extract();
@@ -4223,12 +4756,17 @@ combine_instruction(opt_ctx& ctx, aco_pt
          ctx.info[instr->definitions[0].tempId()].set_temp(instr->operands[0].getTemp());
    }
 
+   for (const Definition& def : instr->definitions) {
+      if (!def.isTemp())
+         continue;
+      ssa_info& info = ctx.info[def.tempId()];
+      if (info.is_extract() && ctx.uses[def.tempId()] > 4)
+         info.label &= ~label_extract;
+   }
+
    if (instr->isVALU() || instr->isSALU()) {
-      /* Apply SDWA. Do this after label_instruction() so it can remove
-       * label_extract if not all instructions can take SDWA. */
       alu_propagate_temp_const(ctx, instr, true);
    } else if (instr->isPseudo()) {
-      /* PSEUDO: propagate temporaries/constants */
       for (unsigned i = 0; i < instr->operands.size(); i++) {
          Operand op = instr->operands[i];
          if (!op.isTemp())
@@ -4255,23 +4793,70 @@ combine_instruction(opt_ctx& ctx, aco_pt
    if (apply_output(ctx, instr))
       return;
 
-   /* TODO: There are still some peephole optimizations that could be done:
-    * - abs(a - b) -> s_absdiff_i32
-    * - various patterns for s_bitcmp{0,1}_b32 and s_bitset{0,1}_b32
-    * - patterns for v_alignbit_b32 and v_alignbyte_b32
-    * These aren't probably too interesting though.
-    * There are also patterns for v_cmp_class_f{16,32,64}. This is difficult but
-    * probably more useful than the previously mentioned optimizations.
-    * The various comparison optimizations also currently only work with 32-bit
-    * floats. */
-
    alu_opt_info info;
    if (!alu_opt_gather_info(ctx, instr.get(), info))
       return;
 
-   aco::small_vec<combine_instr_pattern, 8> patterns;
+   /* -------------------------------------------------------------------------
+    * Local single-instruction peepholes (high hit-rate, low risk):
+    * - redundant min/max: min(a,a) or max(a,a) -> mov a
+    *
+    * We do these here to avoid going through match_and_apply_patterns for cases
+    * that don't need folding.
+    * ------------------------------------------------------------------------- */
+   {
+      const aco_opcode op = info.opcode;
+
+      /* Vector: only 32-bit min/max variants are safe to replace with v_mov_b32. */
+      const bool is_v_minmax_32 =
+         op == aco_opcode::v_min_f32 || op == aco_opcode::v_max_f32 ||
+         op == aco_opcode::v_min_u32 || op == aco_opcode::v_max_u32 ||
+         op == aco_opcode::v_min_i32 || op == aco_opcode::v_max_i32;
+
+      if (is_v_minmax_32) {
+         alu_opt_info tmp = info;
+         if (min_max_redundant_cb(ctx, tmp) && alu_opt_info_is_valid(ctx, tmp)) {
+            for (const alu_opt_op& op_info : tmp.operands) {
+               if (op_info.op.isTemp())
+                  ctx.uses[op_info.op.tempId()]++;
+            }
+            for (const Operand& opnd : instr->operands) {
+               if (opnd.isTemp())
+                  decrease_and_dce(ctx, opnd.getTemp());
+            }
+            ctx.pre_combine_instrs.emplace_back(std::move(instr));
+            instr.reset(alu_opt_info_to_instr(ctx, tmp, nullptr));
+            ctx.info[instr->definitions[0].tempId()].set_combined(ctx.pre_combine_instrs.size() - 1);
+            return;
+         }
+      }
+
+      /* Scalar: s_min/s_max -> s_mov_b32 only if SCC is dead (checked in callback). */
+      const bool is_s_minmax_32 =
+         op == aco_opcode::s_min_u32 || op == aco_opcode::s_max_u32 ||
+         op == aco_opcode::s_min_i32 || op == aco_opcode::s_max_i32;
+
+      if (is_s_minmax_32) {
+         alu_opt_info tmp = info;
+         if (scalar_min_max_redundant_cb(ctx, tmp) && alu_opt_info_is_valid(ctx, tmp)) {
+            for (const alu_opt_op& op_info : tmp.operands) {
+               if (op_info.op.isTemp())
+                  ctx.uses[op_info.op.tempId()]++;
+            }
+            for (const Operand& opnd : instr->operands) {
+               if (opnd.isTemp())
+                  decrease_and_dce(ctx, opnd.getTemp());
+            }
+            ctx.pre_combine_instrs.emplace_back(std::move(instr));
+            instr.reset(alu_opt_info_to_instr(ctx, tmp, nullptr));
+            ctx.info[instr->definitions[0].tempId()].set_combined(ctx.pre_combine_instrs.size() - 1);
+            return;
+         }
+      }
+   }
+
+   aco::small_vec<combine_instr_pattern, 24> patterns;
 
-/* Variadic macro to make callback optional and to allow templates<a, b>. */
 #define add_opt(src_op, res_op, mask, swizzle, ...)                                                \
    patterns.push_back(                                                                             \
       combine_instr_pattern{aco_opcode::src_op, aco_opcode::res_op, mask, swizzle, __VA_ARGS__})
@@ -4287,6 +4872,7 @@ combine_instruction(opt_ctx& ctx, aco_pt
       }
       if (ctx.program->gfx_level >= GFX10_3)
          add_opt(v_mul_legacy_f32, v_fma_legacy_f32, 0x3, "120", create_fma_cb);
+
    } else if (info.opcode == aco_opcode::v_add_f16) {
       if (ctx.program->gfx_level < GFX9 && ctx.fp_mode.denorm16_64 == 0) {
          add_opt(v_mul_f16, v_mad_legacy_f16, 0x3, "120");
@@ -4302,18 +4888,24 @@ combine_instruction(opt_ctx& ctx, aco_pt
          add_opt(s_mul_f16, v_fma_f16, 0x3, "120", create_fma_cb);
          add_opt(v_pk_mul_f16, v_fma_f16, 0x3, "120", create_fma_cb);
       }
+
    } else if (info.opcode == aco_opcode::v_add_f64) {
       add_opt(v_mul_f64, v_fma_f64, 0x3, "120", create_fma_cb);
+
    } else if (info.opcode == aco_opcode::v_add_f64_e64) {
       add_opt(v_mul_f64_e64, v_fma_f64, 0x3, "120", create_fma_cb);
+
    } else if (info.opcode == aco_opcode::s_add_f32) {
       add_opt(s_mul_f32, s_fmac_f32, 0x3, "120", create_fma_cb);
+
    } else if (info.opcode == aco_opcode::s_add_f16) {
       add_opt(s_mul_f16, s_fmac_f16, 0x3, "120", create_fma_cb);
+
    } else if (info.opcode == aco_opcode::v_pk_add_f16) {
       add_opt(v_pk_mul_f16, v_pk_fma_f16, 0x3, "120", create_fma_cb);
       add_opt(v_mul_f16, v_pk_fma_f16, 0x3, "120", create_fma_cb);
       add_opt(s_mul_f16, v_pk_fma_f16, 0x3, "120", create_fma_cb);
+
    } else if (info.opcode == aco_opcode::v_max_f32) {
       add_opt(v_max_f32, v_max3_f32, 0x3, "120", nullptr, true);
       add_opt(s_max_f32, v_max3_f32, 0x3, "120", nullptr, true);
@@ -4323,6 +4915,7 @@ combine_instruction(opt_ctx& ctx, aco_pt
       } else {
          add_opt(v_min_f32, v_med3_f32, 0x3, "012", create_med3_cb<false>, true);
       }
+
    } else if (info.opcode == aco_opcode::v_min_f32) {
       add_opt(v_min_f32, v_min3_f32, 0x3, "120", nullptr, true);
       add_opt(s_min_f32, v_min3_f32, 0x3, "120", nullptr, true);
@@ -4332,6 +4925,7 @@ combine_instruction(opt_ctx& ctx, aco_pt
       } else {
          add_opt(v_max_f32, v_med3_f32, 0x3, "012", create_med3_cb<true>, true);
       }
+
    } else if (info.opcode == aco_opcode::v_max_u32) {
       add_opt(v_max_u32, v_max3_u32, 0x3, "120", nullptr, true);
       add_opt(s_max_u32, v_max3_u32, 0x3, "120", nullptr, true);
@@ -4342,6 +4936,7 @@ combine_instruction(opt_ctx& ctx, aco_pt
          add_opt(v_min_u32, v_med3_u32, 0x3, "012", create_med3_cb<false>, true);
          add_opt(s_min_u32, v_med3_u32, 0x3, "012", create_med3_cb<false>, true);
       }
+
    } else if (info.opcode == aco_opcode::v_min_u32) {
       add_opt(v_min_u32, v_min3_u32, 0x3, "120", nullptr, true);
       add_opt(s_min_u32, v_min3_u32, 0x3, "120", nullptr, true);
@@ -4352,6 +4947,7 @@ combine_instruction(opt_ctx& ctx, aco_pt
          add_opt(v_max_u32, v_med3_u32, 0x3, "012", create_med3_cb<true>, true);
          add_opt(s_max_u32, v_med3_u32, 0x3, "012", create_med3_cb<true>, true);
       }
+
    } else if (info.opcode == aco_opcode::v_max_i32) {
       add_opt(v_max_i32, v_max3_i32, 0x3, "120", nullptr, true);
       add_opt(s_max_i32, v_max3_i32, 0x3, "120", nullptr, true);
@@ -4362,6 +4958,7 @@ combine_instruction(opt_ctx& ctx, aco_pt
          add_opt(v_min_i32, v_med3_i32, 0x3, "012", create_med3_cb<false>, true);
          add_opt(s_min_i32, v_med3_i32, 0x3, "012", create_med3_cb<false>, true);
       }
+
    } else if (info.opcode == aco_opcode::v_min_i32) {
       add_opt(v_min_i32, v_min3_i32, 0x3, "120", nullptr, true);
       add_opt(s_min_i32, v_min3_i32, 0x3, "120", nullptr, true);
@@ -4372,6 +4969,7 @@ combine_instruction(opt_ctx& ctx, aco_pt
          add_opt(v_max_i32, v_med3_i32, 0x3, "012", create_med3_cb<true>, true);
          add_opt(s_max_i32, v_med3_i32, 0x3, "012", create_med3_cb<true>, true);
       }
+
    } else if (info.opcode == aco_opcode::v_max_f16 && ctx.program->gfx_level >= GFX9) {
       add_opt(v_max_f16, v_max3_f16, 0x3, "120", nullptr, true);
       add_opt(s_max_f16, v_max3_f16, 0x3, "120", nullptr, true);
@@ -4381,6 +4979,7 @@ combine_instruction(opt_ctx& ctx, aco_pt
       } else {
          add_opt(v_min_f16, v_med3_f16, 0x3, "012", create_med3_cb<false>, true);
       }
+
    } else if (info.opcode == aco_opcode::v_min_f16 && ctx.program->gfx_level >= GFX9) {
       add_opt(v_min_f16, v_min3_f16, 0x3, "120", nullptr, true);
       add_opt(s_min_f16, v_min3_f16, 0x3, "120", nullptr, true);
@@ -4390,40 +4989,47 @@ combine_instruction(opt_ctx& ctx, aco_pt
       } else {
          add_opt(v_max_f16, v_med3_f16, 0x3, "012", create_med3_cb<true>, true);
       }
+
    } else if (info.opcode == aco_opcode::v_max_u16 && ctx.program->gfx_level >= GFX9) {
       add_opt(v_max_u16, v_max3_u16, 0x3, "120", nullptr, true);
       add_opt(v_min_u16, v_med3_u16, 0x3, "012", create_med3_cb<false>, true);
+
    } else if (info.opcode == aco_opcode::v_min_u16 && ctx.program->gfx_level >= GFX9) {
       add_opt(v_min_u16, v_min3_u16, 0x3, "120", nullptr, true);
       add_opt(v_max_u16, v_med3_u16, 0x3, "012", create_med3_cb<true>, true);
+
    } else if (info.opcode == aco_opcode::v_max_i16 && ctx.program->gfx_level >= GFX9) {
       add_opt(v_max_i16, v_max3_i16, 0x3, "120", nullptr, true);
       add_opt(v_min_i16, v_med3_i16, 0x3, "012", create_med3_cb<false>, true);
+
    } else if (info.opcode == aco_opcode::v_min_i16 && ctx.program->gfx_level >= GFX9) {
       add_opt(v_min_i16, v_min3_i16, 0x3, "120", nullptr, true);
       add_opt(v_max_i16, v_med3_i16, 0x3, "012", create_med3_cb<true>, true);
+
    } else if (info.opcode == aco_opcode::v_max_u16_e64) {
       add_opt(v_max_u16_e64, v_max3_u16, 0x3, "120", nullptr, true);
       add_opt(v_min_u16_e64, v_med3_u16, 0x3, "012", create_med3_cb<false>, true);
+
    } else if (info.opcode == aco_opcode::v_min_u16_e64) {
       add_opt(v_min_u16_e64, v_min3_u16, 0x3, "120", nullptr, true);
       add_opt(v_max_u16_e64, v_med3_u16, 0x3, "012", create_med3_cb<true>, true);
+
    } else if (info.opcode == aco_opcode::v_max_i16_e64) {
       add_opt(v_max_i16_e64, v_max3_i16, 0x3, "120", nullptr, true);
       add_opt(v_min_i16_e64, v_med3_i16, 0x3, "012", create_med3_cb<false>, true);
+
    } else if (info.opcode == aco_opcode::v_min_i16_e64) {
       add_opt(v_min_i16_e64, v_min3_i16, 0x3, "120", nullptr, true);
       add_opt(v_max_i16_e64, v_med3_i16, 0x3, "012", create_med3_cb<true>, true);
+
    } else if (info.opcode == aco_opcode::v_mul_f32 || info.opcode == aco_opcode::v_mul_legacy_f32) {
       bool legacy = info.opcode == aco_opcode::v_mul_legacy_f32;
 
       if ((legacy ? !info.defs[0].isSZPreserve()
                   : (!info.defs[0].isNaNPreserve() && !info.defs[0].isInfPreserve())) &&
           !info.clamp && !info.omod && !ctx.fp_mode.must_flush_denorms32) {
-         /* v_mul_f32(a, v_cndmask_b32(0, 1.0, cond)) -> v_cndmask_b32(0, a, cond) */
          add_opt(v_cndmask_b32, v_cndmask_b32, 0x3, "1032",
                  and_cb<check_const_cb<0, 0>, remove_const_cb<0x3f800000>>, true);
-         /* v_mul_f32(a, v_cndmask_b32(1.0, 0, cond)) -> v_cndmask_b32(a, 0, cond) */
          add_opt(v_cndmask_b32, v_cndmask_b32, 0x3, "0231",
                  and_cb<check_const_cb<1, 0>, remove_const_cb<0x3f800000>>, true);
       }
@@ -4439,29 +5045,38 @@ combine_instruction(opt_ctx& ctx, aco_pt
             add_opt(s_mul_f32, v_mul_f32, 0x3, "120", reassoc_omod_cb<false>, true);
          }
       }
+
    } else if (info.opcode == aco_opcode::v_mul_f16 && can_reassoc_omod(ctx, info, 16)) {
       add_opt(v_mul_f16, v_mul_f16, 0x3, "120", reassoc_omod_cb<false>, true);
       add_opt(s_mul_f16, v_mul_f16, 0x3, "120", reassoc_omod_cb<false>, true);
+
    } else if (info.opcode == aco_opcode::v_mul_f64 && can_reassoc_omod(ctx, info, 64)) {
       add_opt(v_mul_f64, v_mul_f64, 0x3, "120", reassoc_omod_cb<false>, true);
+
    } else if (info.opcode == aco_opcode::v_mul_f64_e64 && can_reassoc_omod(ctx, info, 64)) {
       add_opt(v_mul_f64_e64, v_mul_f64_e64, 0x3, "120", reassoc_omod_cb<false>, true);
+
    } else if (info.opcode == aco_opcode::v_rcp_f32 && can_reassoc_omod(ctx, info, 32)) {
       add_opt(v_mul_f32, v_rcp_f32, 0x1, "01", reassoc_omod_cb<true>);
       add_opt(v_mul_legacy_f32, v_rcp_f32, 0x1, "01", reassoc_omod_cb<true>);
       add_opt(s_mul_f32, v_rcp_f32, 0x1, "01", reassoc_omod_cb<true>);
+
    } else if (info.opcode == aco_opcode::v_s_rcp_f32 && can_reassoc_omod(ctx, info, 32)) {
       add_opt(s_mul_f32, v_s_rcp_f32, 0x1, "01", reassoc_omod_cb<true>);
+
    } else if (info.opcode == aco_opcode::v_rcp_f16 && can_reassoc_omod(ctx, info, 16)) {
       add_opt(v_mul_f16, v_rcp_f16, 0x1, "01", reassoc_omod_cb<true>);
       add_opt(s_mul_f16, v_rcp_f16, 0x1, "01", reassoc_omod_cb<true>);
+
    } else if (info.opcode == aco_opcode::v_s_rcp_f16 && can_reassoc_omod(ctx, info, 16)) {
       add_opt(s_mul_f16, v_s_rcp_f16, 0x1, "01", reassoc_omod_cb<true>);
+
    } else if (info.opcode == aco_opcode::v_rcp_f64 && can_reassoc_omod(ctx, info, 64)) {
       if (ctx.program->gfx_level < GFX12)
          add_opt(v_mul_f64_e64, v_rcp_f64, 0x1, "01", reassoc_omod_cb<true>);
       else
          add_opt(v_mul_f64, v_rcp_f64, 0x1, "01", reassoc_omod_cb<true>);
+
    } else if (info.opcode == aco_opcode::v_add_u16 && !info.clamp) {
       if (ctx.program->gfx_level < GFX9) {
          add_opt(v_mul_lo_u16, v_mad_legacy_u16, 0x3, "120");
@@ -4469,18 +5084,22 @@ combine_instruction(opt_ctx& ctx, aco_pt
          add_opt(v_mul_lo_u16, v_mad_u16, 0x3, "120");
          add_opt(v_pk_mul_lo_u16, v_mad_u16, 0x3, "120");
       }
+
    } else if (info.opcode == aco_opcode::v_add_u16_e64 && !info.clamp) {
       add_opt(v_mul_lo_u16_e64, v_mad_u16, 0x3, "120");
       add_opt(v_pk_mul_lo_u16, v_mad_u16, 0x3, "120");
+
    } else if (info.opcode == aco_opcode::v_pk_add_u16 && !info.clamp) {
       add_opt(v_pk_mul_lo_u16, v_pk_mad_u16, 0x3, "120");
       if (ctx.program->gfx_level < GFX10)
          add_opt(v_mul_lo_u16, v_pk_mad_u16, 0x3, "120");
       else
          add_opt(v_mul_lo_u16_e64, v_pk_mad_u16, 0x3, "120");
+
    } else if (info.opcode == aco_opcode::v_or_b32) {
       add_opt(v_not_b32, v_bfi_b32, 0x3, "10", insert_const_cb<2, UINT32_MAX>, true);
       add_opt(s_not_b32, v_bfi_b32, 0x3, "10", insert_const_cb<2, UINT32_MAX>, true);
+
       if (ctx.program->gfx_level >= GFX9) {
          add_opt(v_or_b32, v_or3_b32, 0x3, "012", nullptr, true);
          add_opt(s_or_b32, v_or3_b32, 0x3, "012", nullptr, true);
@@ -4488,18 +5107,25 @@ combine_instruction(opt_ctx& ctx, aco_pt
          add_opt(s_lshl_b32, v_lshl_or_b32, 0x3, "120", nullptr, true);
          add_opt(v_and_b32, v_and_or_b32, 0x3, "120", nullptr, true);
          add_opt(s_and_b32, v_and_or_b32, 0x3, "120", nullptr, true);
+
+         /* Funnel shift -> v_alignbit/v_alignbyte */
+         add_opt(v_lshlrev_b32, v_alignbit_b32, 0x3, "021", funnel_shift_lshl_cb, true);
+         add_opt(v_lshrrev_b32, v_alignbit_b32, 0x3, "021", funnel_shift_lshr_cb, true);
       }
+
    } else if (info.opcode == aco_opcode::v_xor_b32 && ctx.program->gfx_level >= GFX10) {
       add_opt(v_xor_b32, v_xor3_b32, 0x3, "012", nullptr, true);
       add_opt(s_xor_b32, v_xor3_b32, 0x3, "012", nullptr, true);
       add_opt(v_not_b32, v_xnor_b32, 0x3, "01", nullptr, true);
       add_opt(s_not_b32, v_xnor_b32, 0x3, "01", nullptr, true);
+
    } else if (info.opcode == aco_opcode::v_add_u32 && !info.clamp) {
       assert(ctx.program->gfx_level >= GFX9);
       add_opt(v_bcnt_u32_b32, v_bcnt_u32_b32, 0x3, "102", remove_const_cb<0>, true);
       add_opt(s_bcnt1_i32_b32, v_bcnt_u32_b32, 0x3, "10", nullptr, true);
       add_opt(v_mbcnt_lo_u32_b32, v_mbcnt_lo_u32_b32, 0x3, "102", remove_const_cb<0>, true);
-      add_opt(v_mbcnt_hi_u32_b32_e64, v_mbcnt_hi_u32_b32_e64, 0x3, "102", remove_const_cb<0>, true);
+      add_opt(v_mbcnt_hi_u32_b32_e64, v_mbcnt_hi_u32_b32_e64, 0x3, "102",
+              remove_const_cb<0>, true);
       add_opt(v_mad_u32_u16, v_mad_u32_u16, 0x3, "1203", remove_const_cb<0>, true);
       add_opt(v_mul_u32_u24, v_mad_u32_u24, 0x3, "120", nullptr, true);
       add_opt(v_mul_i32_i24, v_mad_i32_i24, 0x3, "120", nullptr, true);
@@ -4511,25 +5137,22 @@ combine_instruction(opt_ctx& ctx, aco_pt
       add_opt(v_lshlrev_b32, v_lshl_add_u32, 0x3, "210", nullptr, true);
       add_opt(s_lshl_b32, v_lshl_add_u32, 0x3, "120", nullptr, true);
       add_opt(s_mul_i32, v_mad_u32_u24, 0x3, "120", check_mul_u24_cb, true);
-      /* v_add_u32(a, v_cndmask_b32(0, 1, cond)) -> v_addc_co_u32(a, 0, cond) */
       add_opt(v_cndmask_b32, v_addc_co_u32, 0x3, "0132",
               and_cb<and_cb<check_const_cb<1, 0>, remove_const_cb<1>>, add_lm_def_cb>, true);
-      /* v_add_u32(a, v_cndmask_b32(1, 0, cond)) -> v_subb_co_u32(a, -1, cond) */
       add_opt(v_cndmask_b32, v_subb_co_u32, 0x3, "0321",
               and_cb<and_cb<remove_const_cb<1>, remove_const_cb<0>>,
                      and_cb<insert_const_cb<1, UINT32_MAX>, add_lm_def_cb>>,
               true);
+
    } else if ((info.opcode == aco_opcode::v_add_co_u32 ||
                info.opcode == aco_opcode::v_add_co_u32_e64) &&
               !info.clamp) {
-      /* v_add_co_u32(a, v_cndmask_b32(0, 1, cond)) -> v_addc_co_u32(a, 0, cond) */
       add_opt(v_cndmask_b32, v_addc_co_u32, 0x3, "0132",
               and_cb<check_const_cb<1, 0>, remove_const_cb<1>>);
       if (ctx.uses[info.defs[1].tempId()] == 0) {
-         /* v_add_co_u32(a, v_cndmask_b32(1, 0, cond)) -> v_subb_co_u32(a, -1, cond) */
-         add_opt(
-            v_cndmask_b32, v_subb_co_u32, 0x3, "0321",
-            and_cb<and_cb<remove_const_cb<1>, remove_const_cb<0>>, insert_const_cb<1, UINT32_MAX>>);
+         add_opt(v_cndmask_b32, v_subb_co_u32, 0x3, "0321",
+                 and_cb<and_cb<remove_const_cb<1>, remove_const_cb<0>>,
+                        insert_const_cb<1, UINT32_MAX>>);
          add_opt(v_bcnt_u32_b32, v_bcnt_u32_b32, 0x3, "102",
                  and_cb<remove_const_cb<0>, pop_def_cb>);
          add_opt(s_bcnt1_i32_b32, v_bcnt_u32_b32, 0x3, "10", pop_def_cb);
@@ -4547,16 +5170,18 @@ combine_instruction(opt_ctx& ctx, aco_pt
                  and_cb<and_cb<shift_to_mad_cb<32>, check_mul_u24_cb>, pop_def_cb>);
          add_opt(s_mul_i32, v_mad_u32_u24, 0x3, "120", and_cb<check_mul_u24_cb, pop_def_cb>);
       }
+
    } else if (info.opcode == aco_opcode::v_sub_u32 && !info.clamp) {
       assert(ctx.program->gfx_level >= GFX9);
-      /* v_sub_u32(0, v_cndmask_b32(0, 1, cond)) -> v_cndmask_b32(0, -1, cond) */
+
+      /* NEW: v_sub_u32(v_max_u32(a,b), v_min_u32(a,b)) -> v_sad_u32(a,b,0) */
+      add_opt(v_max_u32, v_sad_u32, 0x1, "012", sad_pattern_cb, true);
+
       add_opt(v_cndmask_b32, v_cndmask_b32, 0x2, "0312",
               and_cb<and_cb<and_cb<check_const_cb<0, 0>, remove_const_cb<1>>, remove_const_cb<0>>,
                      insert_const_cb<1, UINT32_MAX>>);
-      /* v_sub_u32(a, v_cndmask_b32(0, 1, cond)) -> v_subb_co_u32(a, 0, cond) */
       add_opt(v_cndmask_b32, v_subb_co_u32, 0x2, "0132",
               and_cb<and_cb<check_const_cb<1, 0>, remove_const_cb<1>>, add_lm_def_cb>);
-      /* v_sub_u32(a, v_cndmask_b32(1, 0, cond)) -> v_addc_co_u32(a, -1, cond) */
       add_opt(v_cndmask_b32, v_addc_co_u32, 0x2, "0321",
               and_cb<and_cb<remove_const_cb<1>, remove_const_cb<0>>,
                      and_cb<insert_const_cb<1, UINT32_MAX>, add_lm_def_cb>>);
@@ -4566,24 +5191,20 @@ combine_instruction(opt_ctx& ctx, aco_pt
               and_cb<shift_to_mad_cb<32>, neg_mul_to_i24_cb>);
       add_opt(v_mul_u32_u24, v_mad_i32_i24, 0x2, "120", neg_mul_to_i24_cb);
       add_opt(s_mul_i32, v_mad_i32_i24, 0x2, "120", neg_mul_to_i24_cb);
+
    } else if ((info.opcode == aco_opcode::v_sub_co_u32 ||
                info.opcode == aco_opcode::v_sub_co_u32_e64) &&
               !info.clamp) {
-      /* v_sub_co_u32(0, v_cndmask_b32(0, 1, cond)) -> v_cndmask_b32(0, -1, cond) */
       if (ctx.uses[info.defs[1].tempId()] == 0) {
-         add_opt(
-            v_cndmask_b32, v_cndmask_b32, 0x2, "0312",
-            and_cb<and_cb<and_cb<check_const_cb<0, 0>, remove_const_cb<1>>, remove_const_cb<0>>,
-                   and_cb<insert_const_cb<1, UINT32_MAX>, pop_def_cb>>);
+         add_opt(v_cndmask_b32, v_cndmask_b32, 0x2, "0312",
+                 and_cb<and_cb<and_cb<check_const_cb<0, 0>, remove_const_cb<1>>, remove_const_cb<0>>,
+                        and_cb<insert_const_cb<1, UINT32_MAX>, pop_def_cb>>);
       }
-      /* v_sub_co_u32(a, v_cndmask_b32(0, 1, cond)) -> v_subb_co_u32(a, 0, cond) */
       add_opt(v_cndmask_b32, v_subb_co_u32, 0x2, "0132",
               and_cb<check_const_cb<1, 0>, remove_const_cb<1>>);
       if (ctx.uses[info.defs[1].tempId()] == 0) {
-         /* v_sub_co_u32(a, v_cndmask_b32(1, 0, cond)) -> v_addc_co_u32(a, -1, cond) */
-         add_opt(
-            v_cndmask_b32, v_addc_co_u32, 0x2, "0321",
-            and_cb<and_cb<remove_const_cb<1>, remove_const_cb<0>>, insert_const_cb<1, UINT32_MAX>>);
+         add_opt(v_cndmask_b32, v_addc_co_u32, 0x2, "0321",
+                 and_cb<and_cb<remove_const_cb<1>, remove_const_cb<0>>, insert_const_cb<1, UINT32_MAX>>);
          add_opt(v_lshlrev_b32, v_mad_i32_i24, 0x2, "210",
                  and_cb<and_cb<shift_to_mad_cb<32>, neg_mul_to_i24_cb>, pop_def_cb>);
          add_opt(s_lshl_b32, v_mad_i32_i24, 0x2, "120",
@@ -4591,57 +5212,93 @@ combine_instruction(opt_ctx& ctx, aco_pt
          add_opt(v_mul_u32_u24, v_mad_i32_i24, 0x2, "120", and_cb<neg_mul_to_i24_cb, pop_def_cb>);
          add_opt(s_mul_i32, v_mad_i32_i24, 0x2, "120", and_cb<neg_mul_to_i24_cb, pop_def_cb>);
       }
+
    } else if ((info.opcode == aco_opcode::s_add_u32 ||
-               (info.opcode == aco_opcode::s_add_i32 && !ctx.uses[info.defs[1].tempId()])) &&
+               (info.opcode == aco_opcode::s_add_i32 && info.defs.size() >= 2 &&
+                !ctx.uses[info.defs[1].tempId()])) &&
               ctx.program->gfx_level >= GFX9) {
       add_opt(s_lshl_b32, s_lshl1_add_u32, 0x3, "102", remove_const_cb<1>);
       add_opt(s_lshl_b32, s_lshl2_add_u32, 0x3, "102", remove_const_cb<2>);
       add_opt(s_lshl_b32, s_lshl3_add_u32, 0x3, "102", remove_const_cb<3>);
       add_opt(s_lshl_b32, s_lshl4_add_u32, 0x3, "102", remove_const_cb<4>);
+
    } else if (info.opcode == aco_opcode::v_lshlrev_b32 && ctx.program->gfx_level >= GFX9) {
       add_opt(v_add_u32, v_add_lshl_u32, 0x2, "120", nullptr, true);
       add_opt(s_add_u32, v_add_lshl_u32, 0x2, "120", nullptr, true);
       add_opt(s_add_i32, v_add_lshl_u32, 0x2, "120", nullptr, true);
+
    } else if (info.opcode == aco_opcode::v_and_b32) {
       add_opt(v_not_b32, v_bfi_b32, 0x3, "10", insert_const_cb<1, 0>, true);
       add_opt(s_not_b32, v_bfi_b32, 0x3, "10", insert_const_cb<1, 0>, true);
+
    } else if (info.opcode == aco_opcode::s_and_b32) {
+      add_opt(s_lshl_b32, s_bitcmp1_b32, 0x3, "021", s_bitcmp1_cb, true);
       add_opt(s_not_b32, s_andn2_b32, 0x3, "01");
+
    } else if (info.opcode == aco_opcode::s_and_b64) {
+      add_opt(s_lshl_b64, s_bitcmp1_b64, 0x3, "021", s_bitcmp1_cb, true);
       add_opt(s_not_b64, s_andn2_b64, 0x3, "01");
+
    } else if (info.opcode == aco_opcode::s_or_b32) {
+      add_opt(s_lshl_b32, s_bitset1_b32, 0x3, "021", s_bitset_cb, true);
       add_opt(s_not_b32, s_orn2_b32, 0x3, "01");
+
    } else if (info.opcode == aco_opcode::s_or_b64) {
+      add_opt(s_lshl_b64, s_bitset1_b64, 0x3, "021", s_bitset_cb, true);
       add_opt(s_not_b64, s_orn2_b64, 0x3, "01");
+
    } else if (info.opcode == aco_opcode::s_xor_b32) {
       add_opt(s_not_b32, s_xnor_b32, 0x3, "01");
+
    } else if (info.opcode == aco_opcode::s_xor_b64) {
       add_opt(s_not_b64, s_xnor_b64, 0x3, "01");
-   } else if ((info.opcode == aco_opcode::s_sub_u32 || info.opcode == aco_opcode::s_sub_i32) &&
-              !ctx.uses[info.defs[1].tempId()]) {
-      add_opt(s_bcnt1_i32_b32, s_bcnt0_i32_b32, 0x2, "10", remove_const_cb<32>);
-      add_opt(s_bcnt1_i32_b64, s_bcnt0_i32_b64, 0x2, "10", remove_const_cb<64>);
+
+   } else if (info.opcode == aco_opcode::s_sub_i32) {
+      /* NEW: s_sub_i32(s_max_i32(a,b), s_min_i32(a,b)) -> s_absdiff_i32(a,b) */
+      add_opt(s_max_i32, s_absdiff_i32, 0x1, "012", scalar_sad_pattern_cb, true);
+
+      if (info.defs.size() >= 2) {
+         add_opt(s_max_i32, s_absdiff_i32, 0x1, "012", scalar_absdiff_from_minmax_i32_cb, true);
+         if (!ctx.uses[info.defs[1].tempId()]) {
+            add_opt(s_bcnt1_i32_b32, s_bcnt0_i32_b32, 0x2, "10", remove_const_cb<32>);
+            add_opt(s_bcnt1_i32_b64, s_bcnt0_i32_b64, 0x2, "10", remove_const_cb<64>);
+         }
+      }
+
+   } else if (info.opcode == aco_opcode::s_sub_u32) {
+      if (info.defs.size() >= 2 && !ctx.uses[info.defs[1].tempId()]) {
+         add_opt(s_bcnt1_i32_b32, s_bcnt0_i32_b32, 0x2, "10", remove_const_cb<32>);
+         add_opt(s_bcnt1_i32_b64, s_bcnt0_i32_b64, 0x2, "10", remove_const_cb<64>);
+      }
+
    } else if (info.opcode == aco_opcode::s_bcnt1_i32_b32) {
       add_opt(s_not_b32, s_bcnt0_i32_b32, 0x1, "0");
+
    } else if (info.opcode == aco_opcode::s_bcnt1_i32_b64) {
       add_opt(s_not_b64, s_bcnt0_i32_b64, 0x1, "0");
+
    } else if (info.opcode == aco_opcode::s_ff1_i32_b32 && ctx.program->gfx_level < GFX11) {
       add_opt(s_not_b32, s_ff0_i32_b32, 0x1, "0");
+
    } else if (info.opcode == aco_opcode::s_ff1_i32_b64 && ctx.program->gfx_level < GFX11) {
       add_opt(s_not_b64, s_ff0_i32_b64, 0x1, "0");
+
    } else if (info.opcode == aco_opcode::v_cndmask_b32) {
       add_opt(s_not_b64, v_cndmask_b32, 0x4, "102");
       add_opt(s_not_b32, v_cndmask_b32, 0x4, "102");
+
    } else if (info.opcode == aco_opcode::v_alignbyte_b32) {
-      /* GFX6/7 lowered pack(undef, f2f16_rtz(a)) -> v_cvt_pkrtz_f16_f32(0, a) */
       add_opt(v_cvt_pkrtz_f16_f32, v_cvt_pkrtz_f16_f32, 0x1, "0231",
               and_cb<and_cb<check_const_cb<0, 0>, remove_const_cb<2>>, pop_op_cb>);
-   } else if (info.opcode == aco_opcode::s_lshl_b32 && !ctx.uses[info.defs[1].tempId()]) {
-      add_opt(
-         s_cvt_pk_rtz_f16_f32, s_cvt_pk_rtz_f16_f32, 0x1, "120",
-         and_cb<and_cb<and_cb<remove_const_cb<16>, pop_op_cb>, pop_def_cb>, insert_const_cb<0, 0>>);
+
+   } else if (info.opcode == aco_opcode::s_lshl_b32 &&
+              info.defs.size() >= 2 && !ctx.uses[info.defs[1].tempId()]) {
+      add_opt(s_cvt_pk_rtz_f16_f32, s_cvt_pk_rtz_f16_f32, 0x1, "120",
+              and_cb<and_cb<and_cb<remove_const_cb<16>, pop_op_cb>, pop_def_cb>, insert_const_cb<0, 0>>);
    }
 
+#undef add_opt
+
    if (match_and_apply_patterns(ctx, info, patterns)) {
       for (const alu_opt_op& op_info : info.operands) {
          if (op_info.op.isTemp())
@@ -4655,7 +5312,6 @@ combine_instruction(opt_ctx& ctx, aco_pt
       instr.reset(alu_opt_info_to_instr(ctx, info, nullptr));
       ctx.info[instr->definitions[0].tempId()].set_combined(ctx.pre_combine_instrs.size() - 1);
    }
-#undef add_opt
 }
 
 struct remat_entry {
@@ -4672,7 +5328,7 @@ is_constant(Instruction* instr)
    return instr->operands[0].isConstant() && instr->definitions[0].isTemp();
 }
 
-void
+static void
 remat_constants_instr(opt_ctx& ctx, aco::map<Temp, remat_entry>& constants, Instruction* instr,
                       uint32_t block_idx)
 {
@@ -4711,7 +5367,7 @@ remat_constants_instr(opt_ctx& ctx, aco:
  * of loaded constants over large distances, this pass splits the live-ranges
  * again by re-emitting constants in every basic block.
  */
-void
+static void
 rematerialize_constants(opt_ctx& ctx)
 {
    aco::monotonic_buffer_resource memory(1024);
@@ -4744,7 +5400,7 @@ rematerialize_constants(opt_ctx& ctx)
    }
 }
 
-bool
+static bool
 to_uniform_bool_instr(opt_ctx& ctx, aco_ptr<Instruction>& instr)
 {
    /* Check every operand to make sure they are suitable. */
@@ -4764,6 +5420,9 @@ to_uniform_bool_instr(opt_ctx& ctx, aco_
    case aco_opcode::s_xor_b64: instr->opcode = aco_opcode::s_absdiff_i32; break;
    case aco_opcode::s_not_b32:
    case aco_opcode::s_not_b64: {
+      /* NOT(x) for boolean x in {0,1} is: |x - 1| = 1 - x
+       * s_absdiff_i32 computes |op0 - op1| and sets SCC = (result != 0)
+       */
       aco_ptr<Instruction> new_instr{
          create_instruction(aco_opcode::s_absdiff_i32, Format::SOP2, 2, 2)};
       new_instr->operands[0] = instr->operands[0];
@@ -4786,7 +5445,7 @@ to_uniform_bool_instr(opt_ctx& ctx, aco_
          continue;
 
       ctx.uses[op.tempId()]--;
-      bool increase_uses = ctx.uses[op.tempId()];
+      bool increase_uses = ctx.uses[op.tempId()] > 0;
 
       if (ctx.info[op.tempId()].is_uniform_bool()) {
          /* Just use the uniform boolean temp. */
@@ -4813,8 +5472,6 @@ to_uniform_bool_instr(opt_ctx& ctx, aco_
 
    instr->definitions[0].setTemp(Temp(instr->definitions[0].tempId(), s1));
    ctx.program->temp_rc[instr->definitions[0].tempId()] = s1;
-   assert(!instr->operands[0].isTemp() || instr->operands[0].regClass() == s1);
-   assert(!instr->operands[1].isTemp() || instr->operands[1].regClass() == s1);
    return true;
 }
 
@@ -4863,6 +5520,28 @@ select_instruction(opt_ctx& ctx, aco_ptr
       }
    }
 
+   /* Optimization: Dead Carry Elimination (GFX9+)
+    * If the carry-out of v_add_co_u32 / v_sub_co_u32 is dead, demote it to v_add_u32 / v_sub_u32.
+    * This breaks dependency chains on VCC, improving scheduling.
+    *
+    * Safety requirements:
+    *  - carry definition must exist and be a Temp (uses-tracked)
+    *  - carry Temp must have 0 uses
+    *  - keep the existing format/modifiers unchanged; only opcode/defs are changed
+    */
+   if (ctx.program->gfx_level >= GFX9 && instr->definitions.size() >= 2 && instr->definitions[0].isTemp() &&
+       instr->definitions[1].isTemp() && ctx.uses[instr->definitions[0].tempId()] != 0 &&
+       ctx.uses[instr->definitions[1].tempId()] == 0) {
+      if (instr->opcode == aco_opcode::v_add_co_u32 || instr->opcode == aco_opcode::v_add_co_u32_e64) {
+         instr->definitions.pop_back();
+         instr->opcode = aco_opcode::v_add_u32;
+      } else if (instr->opcode == aco_opcode::v_sub_co_u32 ||
+                 instr->opcode == aco_opcode::v_sub_co_u32_e64) {
+         instr->definitions.pop_back();
+         instr->opcode = aco_opcode::v_sub_u32;
+      }
+   }
+
    /* convert split_vector into a copy or extract_vector if only one definition is ever used */
    if (instr->opcode == aco_opcode::p_split_vector) {
       unsigned num_used = 0;
@@ -4878,7 +5557,7 @@ select_instruction(opt_ctx& ctx, aco_ptr
       }
       bool done = false;
       Instruction* vec = ctx.info[instr->operands[0].tempId()].parent_instr;
-      if (num_used == 1 && vec->opcode == aco_opcode::p_create_vector &&
+      if (num_used == 1 && vec && vec->opcode == aco_opcode::p_create_vector &&
           ctx.uses[instr->operands[0].tempId()] == 1) {
 
          unsigned off = 0;
@@ -4926,7 +5605,8 @@ select_instruction(opt_ctx& ctx, aco_ptr
       }
    }
 
-   if (!instr->definitions.empty() && ctx.info[instr->definitions[0].tempId()].is_combined()) {
+   if (!instr->definitions.empty() && instr->definitions[0].isTemp() &&
+       ctx.info[instr->definitions[0].tempId()].is_combined()) {
       aco_ptr<Instruction>& prev_instr =
          ctx.pre_combine_instrs[ctx.info[instr->definitions[0].tempId()].val];
       /* Re-check combined instructions, revert to using pre combine instruction if
@@ -4964,7 +5644,7 @@ select_instruction(opt_ctx& ctx, aco_ptr
       return;
    } else if ((instr->opcode == aco_opcode::s_cselect_b64 ||
                instr->opcode == aco_opcode::s_cselect_b32) &&
-              instr->operands[2].isTemp()) {
+              instr->operands.size() >= 3 && instr->operands[2].isTemp()) {
       ctx.info[instr->operands[2].tempId()].set_scc_needed();
    }
 
@@ -4973,11 +5653,13 @@ select_instruction(opt_ctx& ctx, aco_ptr
       return;
 
    /* Transform uniform bitwise boolean operations to 32-bit when there are no divergent uses. */
-   if (instr->definitions.size() && ctx.uses[instr->definitions[0].tempId()] == 0 &&
+   if (instr->definitions.size() && instr->definitions[0].isTemp() &&
+       ctx.uses[instr->definitions[0].tempId()] == 0 &&
        ctx.info[instr->definitions[0].tempId()].is_uniform_bitwise()) {
       bool transform_done = to_uniform_bool_instr(ctx, instr);
 
-      if (transform_done && !ctx.info[instr->definitions[1].tempId()].is_scc_needed()) {
+      if (transform_done && instr->definitions.size() >= 2 &&
+          instr->definitions[1].isTemp() && !ctx.info[instr->definitions[1].tempId()].is_scc_needed()) {
          /* Swap the two definition IDs in order to avoid overusing the SCC.
           * This reduces extra moves generated by RA. */
          uint32_t def0_id = instr->definitions[0].getTemp().id();
@@ -4995,6 +5677,7 @@ select_instruction(opt_ctx& ctx, aco_ptr
    if (instr->opcode == aco_opcode::s_and_b32 || instr->opcode == aco_opcode::s_and_b64) {
       if (instr->operands[0].isTemp() && fixed_to_exec(instr->operands[1]) &&
           ctx.uses[instr->operands[0].tempId()] == 1 &&
+          instr->definitions.size() >= 2 && instr->definitions[1].isTemp() &&
           ctx.uses[instr->definitions[1].tempId()] == 0 &&
           can_eliminate_and_exec(ctx, instr->operands[0].getTemp(), instr->pass_flags, true)) {
          ctx.uses[instr->operands[0].tempId()]--;
@@ -5023,7 +5706,7 @@ select_instruction(opt_ctx& ctx, aco_ptr
                                          if (!op.isTemp())
                                             return false;
                                          Instruction* parent = ctx.info[op.tempId()].parent_instr;
-                                         return parent->isDPP() &&
+                                         return parent && parent->isDPP() &&
                                                 parent->opcode == aco_opcode::v_mov_b32 &&
                                                 parent->pass_flags == instr->pass_flags;
                                       })) {
@@ -5042,7 +5725,7 @@ select_instruction(opt_ctx& ctx, aco_ptr
             continue;
          Instruction* parent = ctx.info[input_info.operands[i].op.tempId()].parent_instr;
 
-         if (!parent->isDPP() || parent->opcode != aco_opcode::v_mov_b32 ||
+         if (!parent || !parent->isDPP() || parent->opcode != aco_opcode::v_mov_b32 ||
              parent->pass_flags != instr->pass_flags)
             continue;
 
@@ -5092,7 +5775,6 @@ select_instruction(opt_ctx& ctx, aco_ptr
          dpp_info = candidate;
          progress = true;
       }
-
       if (progress)
          instr.reset(alu_opt_info_to_instr(ctx, dpp_info, instr.release()));
    }
@@ -5571,6 +6253,20 @@ apply_literals(opt_ctx& ctx, aco_ptr<Ins
       }
    }
 
+   /* Optimization: s_mov_b32 -> s_movk_i32
+    * Uses a smaller 16-bit immediate instead of a 32-bit literal.
+    */
+   if (instr->opcode == aco_opcode::s_mov_b32 && instr->operands[0].isConstant()) {
+      int32_t val = (int32_t)instr->operands[0].constantValue();
+      if (val >= -32768 && val <= 32767) {
+         SALU_instruction* movk = &instr->salu();
+         movk->format = Format::SOPK;
+         movk->opcode = aco_opcode::s_movk_i32;
+         movk->imm = (uint16_t)val;
+         movk->operands.clear();
+      }
+   }
+
    if (instr->isSOPC() && ctx.program->gfx_level < GFX12)
       try_convert_sopc_to_sopk(instr);
 
@@ -5686,6 +6382,16 @@ optimize(Program* program)
    ctx.program = program;
    ctx.info = std::vector<ssa_info>(program->peekAllocationId());
 
+   /* Heuristic: Reserve memory to avoid reallocations.
+    * Estimate based on total instruction count. */
+   size_t total_instr = 0;
+   for (const Block& block : program->blocks)
+      total_instr += block.instructions.size();
+
+   const size_t reserve_hint = total_instr / 4u + 16u;
+   ctx.pre_combine_instrs.reserve(reserve_hint);
+   ctx.replacement_instr.reserve(total_instr / 8u + 16u);
+
    /* 1. Bottom-Up DAG pass (forward) to label all ssa-defs */
    for (Block& block : program->blocks) {
       ctx.fp_mode = block.fp_mode;
@@ -5697,6 +6403,10 @@ optimize(Program* program)
 
    ctx.uses = dead_code_analysis(program);
 
+   /* Ensure vectors have space for new temps created during optimization */
+   ctx.uses.reserve(ctx.uses.size() + reserve_hint);
+   ctx.info.reserve(ctx.info.size() + reserve_hint);
+
    validate_opt_ctx(ctx, false);
 
    /* 2. Rematerialize constants in every block. */
