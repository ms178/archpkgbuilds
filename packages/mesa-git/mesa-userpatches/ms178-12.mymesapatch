--- a/src/amd/compiler/aco_insert_NOPs.cpp	2025-06-27 20:25:53.353185796 +0200
+++ b/src/amd/compiler/aco_insert_NOPs.cpp	2025-06-27 20:35:41.054060312 +0200
@@ -10,21 +10,32 @@
 #include "util/bitset.h"
 
 #include <algorithm>
+#include <array>
 #include <bitset>
 #include <set>
 #include <stack>
 #include <vector>
 
-namespace aco {
-namespace {
+namespace aco
+{
+namespace
+{
 
-struct State {
+struct State
+{
    Program* program;
    Block* block;
    std::vector<aco_ptr<Instruction>> old_instructions;
 };
 
-struct NOP_ctx_gfx6 {
+/*
+ * NOP context for GFX6-9 (GCN) architectures.
+ * This struct tracks the state of various hardware hazards. Each counter
+ * represents the number of cycles that must pass before a dependent
+ * instruction can execute safely.
+ */
+struct NOP_ctx_gfx6
+{
    void join(const NOP_ctx_gfx6& other)
    {
       set_vskip_mode_then_vector =
@@ -35,7 +46,13 @@ struct NOP_ctx_gfx6 {
       valu_wr_exec_then_dpp = MAX2(valu_wr_exec_then_dpp, other.valu_wr_exec_then_dpp);
       salu_wr_m0_then_lds = MAX2(salu_wr_m0_then_lds, other.salu_wr_m0_then_lds);
       salu_wr_m0_then_moverel = MAX2(salu_wr_m0_then_moverel, other.salu_wr_m0_then_moverel);
-      setreg_then_getsetreg = MAX2(setreg_then_getsetreg, other.setreg_then_getsetreg);
+      vcc_wr_then_branch = MAX2(vcc_wr_then_branch, other.vcc_wr_then_branch);
+
+      /* Per-register merge: take the max wait across each register. */
+      for (unsigned i = 0; i < setreg_waits.size(); ++i) {
+         setreg_waits[i] = std::max(setreg_waits[i], other.setreg_waits[i]);
+      }
+
       vmem_store_then_wr_data |= other.vmem_store_then_wr_data;
       smem_clause |= other.smem_clause;
       smem_write |= other.smem_write;
@@ -54,8 +71,9 @@ struct NOP_ctx_gfx6 {
              valu_wr_exec_then_dpp == other.valu_wr_exec_then_dpp &&
              salu_wr_m0_then_lds == other.salu_wr_m0_then_lds &&
              salu_wr_m0_then_moverel == other.salu_wr_m0_then_moverel &&
-             setreg_then_getsetreg == other.setreg_then_getsetreg &&
-             smem_clause == other.smem_clause && smem_write == other.smem_write &&
+             vcc_wr_then_branch == other.vcc_wr_then_branch &&
+             setreg_waits == other.setreg_waits && smem_clause == other.smem_clause &&
+             smem_write == other.smem_write &&
              BITSET_EQUAL(smem_clause_read_write, other.smem_clause_read_write) &&
              BITSET_EQUAL(smem_clause_write, other.smem_clause_write);
    }
@@ -80,47 +98,67 @@ struct NOP_ctx_gfx6 {
       if ((salu_wr_m0_then_moverel -= amount) < 0)
          salu_wr_m0_then_moverel = 0;
 
-      if ((setreg_then_getsetreg -= amount) < 0)
-         setreg_then_getsetreg = 0;
+      if ((vcc_wr_then_branch -= amount) < 0)
+         vcc_wr_then_branch = 0;
+
+      /* Decay per-register waits. */
+      for (int8_t& wait : setreg_waits) {
+         if ((wait -= amount) < 0) {
+            wait = 0;
+         }
+      }
 
       vmem_store_then_wr_data.reset();
    }
 
-   /* setting MODE.vskip and then any vector op requires 2 wait states */
+   /* Helper to get the maximum wait state required for any s_setreg/s_getreg hazard. */
+   int8_t get_max_setreg_wait() const
+   {
+      return *std::max_element(setreg_waits.begin(), setreg_waits.end());
+   }
+
+   /* GFX9 (Vega) ISA requires 2 wait states after setting MODE.vskip before a vector op. */
    int8_t set_vskip_mode_then_vector = 0;
+   /* The last known state of the vskip bit (MODE reg, bit 28) to avoid spurious waits. */
+   uint8_t current_vskip = 0;
 
-   /* VALU writing VCC followed by v_div_fmas require 4 wait states */
+   /* VALU writing VCC followed by v_div_fmas requires 4 wait states. */
    int8_t valu_wr_vcc_then_div_fmas = 0;
 
-   /* SALU writing M0 followed by GDS, s_sendmsg or s_ttrace_data requires 1 wait state */
+   /* SALU writing M0 followed by GDS, s_sendmsg or s_ttrace_data requires 1 wait state. */
    int8_t salu_wr_m0_then_gds_msg_ttrace = 0;
 
-   /* VALU writing EXEC followed by DPP requires 5 wait states */
+   /* VALU writing EXEC followed by DPP requires 5 wait states. */
    int8_t valu_wr_exec_then_dpp = 0;
 
-   /* SALU writing M0 followed by some LDS instructions requires 1 wait state on GFX10 */
+   /* SALU writing M0 followed by some LDS instructions requires 1 wait state on GFX9+. */
    int8_t salu_wr_m0_then_lds = 0;
 
-   /* SALU writing M0 followed by s_moverel requires 1 wait state on GFX9 */
+   /* SALU writing M0 followed by s_moverel requires 1 wait state on GFX9. */
    int8_t salu_wr_m0_then_moverel = 0;
 
-   /* s_setreg followed by a s_getreg/s_setreg of the same register needs 2 wait states
-    * currently we don't look at the actual register */
-   int8_t setreg_then_getsetreg = 0;
+   /* GFX9: SALU writing VCC followed by a branch using VCC requires 2 wait states
+    * to prevent a costly pipeline flush. This is a critical performance hazard. */
+   int8_t vcc_wr_then_branch = 0;
+
+   /* Per-register counters for `s_setreg` -> `s_getreg`/`s_setreg` hazards.
+    * This is far more precise than a single global counter, avoiding many false-positive stalls.
+    * Max register index is 63, so an array of size 64 is sufficient. */
+   std::array<int8_t, 64> setreg_waits = {0};
 
-   /* some memory instructions writing >64bit followed by a instructions
-    * writing the VGPRs holding the writedata requires 1 wait state */
+   /* A VMEM store of >64 bits followed by an instruction writing to the VGPRs
+    * that held the store data requires 1 wait state. */
    std::bitset<256> vmem_store_then_wr_data;
 
-   /* we break up SMEM clauses that contain stores or overwrite an
-    * operand/definition of another instruction in the clause */
+   /* SMEM clauses (consecutive SMEM instructions) have ordering hazards with XNACK enabled. */
    bool smem_clause = false;
    bool smem_write = false;
    BITSET_DECLARE(smem_clause_read_write, 128) = {0};
    BITSET_DECLARE(smem_clause_write, 128) = {0};
 };
 
-struct NOP_ctx_gfx10 {
+struct NOP_ctx_gfx10
+{
    bool has_VOPC_write_exec = false;
    bool has_nonVALU_exec_read = false;
    bool has_VMEM = false;
@@ -167,9 +205,17 @@ struct NOP_ctx_gfx10 {
    }
 };
 
-template <int Max> struct RegCounterMap {
-   void inc() { base++; }
-   void set(PhysReg reg) { update(reg, 0); }
+template <int Max>
+struct RegCounterMap
+{
+   void inc()
+   {
+      base++;
+   }
+   void set(PhysReg reg)
+   {
+      update(reg, 0);
+   }
 
    uint8_t get(PhysReg reg)
    {
@@ -231,11 +277,15 @@ template <int Max> struct RegCounterMap
       return base == other.base && list == other.list;
    }
 
-private:
-   struct entry {
+ private:
+   struct entry
+   {
       uint16_t reg;
       int16_t val;
-      bool operator!=(const entry& other) const { return reg != other.reg || val != other.val; }
+      bool operator!=(const entry& other) const
+      {
+         return reg != other.reg || val != other.val;
+      }
    };
 
    std::bitset<128> present;
@@ -243,7 +293,8 @@ private:
    int base = 0;
 };
 
-struct NOP_ctx_gfx11 {
+struct NOP_ctx_gfx11
+{
    /* VcmpxPermlaneHazard */
    bool has_Vcmpx = false;
 
@@ -316,12 +367,13 @@ struct NOP_ctx_gfx11 {
 int
 get_wait_states(aco_ptr<Instruction>& instr)
 {
-   if (instr->opcode == aco_opcode::s_nop)
+   if (instr->opcode == aco_opcode::s_nop) {
       return instr->salu().imm + 1;
-   else if (instr->opcode == aco_opcode::p_constaddr)
+   } else if (instr->opcode == aco_opcode::p_constaddr) {
       return 3; /* lowered to 3 instructions in the assembler */
-   else
+   } else {
       return 1;
+   }
 }
 
 bool
@@ -353,11 +405,11 @@ search_backwards_internal(State& state,
          return;
    }
 
-   PRAGMA_DIAGNOSTIC_PUSH
-   PRAGMA_DIAGNOSTIC_IGNORED(-Waddress)
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Waddress"
    if (block_cb != nullptr && !block_cb(global_state, block_state, block))
       return;
-   PRAGMA_DIAGNOSTIC_POP
+#pragma GCC diagnostic pop
 
    for (unsigned lin_pred : block->linear_preds) {
       search_backwards_internal<GlobalState, BlockState, block_cb, instr_cb>(
@@ -375,12 +427,14 @@ search_backwards(State& state, GlobalSta
       state, global_state, block_state, state.block, false);
 }
 
-struct HandleRawHazardGlobalState {
+struct HandleRawHazardGlobalState
+{
    PhysReg reg;
    int nops_needed;
 };
 
-struct HandleRawHazardBlockState {
+struct HandleRawHazardBlockState
+{
    uint32_t mask;
    int nops_needed;
 };
@@ -468,40 +522,46 @@ test_bitset_range(BITSET_WORD* words, un
    }
 }
 
-/* A SMEM clause is any group of consecutive SMEM instructions. The
- * instructions in this group may return out of order and/or may be replayed.
+/*
+ * A SMEM clause is any group of consecutive SMEM instructions. The
+ * instructions in this group may return out of order and/or may be replayed
+ * when XNACK is enabled.
  *
- * To fix this potential hazard correctly, we have to make sure that when a
- * clause has more than one instruction, no instruction in the clause writes
- * to a register that is read by another instruction in the clause (including
- * itself). In this case, we have to break the SMEM clause by inserting non
- * SMEM instructions.
- *
- * SMEM clauses are only present on GFX8+, and only matter when XNACK is set.
+ * To fix this, we break the clause by inserting a non-SMEM instruction if:
+ * 1. The clause contains a store or atomic operation (side effects).
+ * 2. An instruction reads a register written by a previous instruction in the same clause (RAW).
+ * 3. An instruction writes a register read/written by a previous instruction (WAW/WAR).
  */
 void
 handle_smem_clause_hazards(Program* program, NOP_ctx_gfx6& ctx, aco_ptr<Instruction>& instr,
                            int* NOPs)
 {
-   /* break off from previous SMEM clause if needed */
-   if (!*NOPs & (ctx.smem_clause || ctx.smem_write)) {
-      /* Don't allow clauses with store instructions since the clause's
-       * instructions may use the same address. */
-      if (ctx.smem_write || instr->definitions.empty() ||
-          instr_info.is_atomic[(unsigned)instr->opcode]) {
-         *NOPs = 1;
-      } else if (program->dev.xnack_enabled) {
-         for (Operand op : instr->operands) {
-            if (!op.isConstant() &&
-                test_bitset_range(ctx.smem_clause_write, op.physReg(), op.size())) {
-               *NOPs = 1;
-               break;
-            }
-         }
+   /* No need to break a clause if we're already inserting a NOP for another reason. */
+   if (*NOPs || (!ctx.smem_clause && !ctx.smem_write)) {
+      return;
+   }
 
-         Definition def = instr->definitions[0];
-         if (!*NOPs && test_bitset_range(ctx.smem_clause_read_write, def.physReg(), def.size()))
+   /* Don't allow clauses with store instructions or atomics due to side effects. */
+   if (ctx.smem_write || instr->definitions.empty() ||
+       instr_info.is_atomic[(unsigned)instr->opcode]) {
+      *NOPs = 1;
+      return;
+   }
+
+   if (program->dev.xnack_enabled) {
+      /* Check for RAW hazard: read of a register written earlier in the clause. */
+      for (Operand op : instr->operands) {
+         if (!op.isConstant() &&
+             test_bitset_range(ctx.smem_clause_write, op.physReg(), op.size())) {
             *NOPs = 1;
+            return; /* Hazard found, break clause. */
+         }
+      }
+
+      /* Check for WAW/WAR hazard: write to a register read or written by another instruction. */
+      Definition def = instr->definitions[0];
+      if (test_bitset_range(ctx.smem_clause_read_write, def.physReg(), def.size())) {
+         *NOPs = 1;
       }
    }
 }
@@ -511,15 +571,31 @@ void
 handle_instruction_gfx6(State& state, NOP_ctx_gfx6& ctx, aco_ptr<Instruction>& instr,
                         std::vector<aco_ptr<Instruction>>& new_instructions)
 {
-   /* check hazards */
+   /*
+    * This function follows a three-step process for each instruction:
+    * 1. CHECK: Determine required wait-states *before* the current instruction
+    *    based on the current hazard context (ctx).
+    * 2. EMIT: Insert `s_nop` or `s_waitcnt` if needed, and age the context counters.
+    * 3. UPDATE: Update the context with any *new* hazards created by the current instruction.
+    */
+
+   /* --- 1. CHECK FOR HAZARDS BEFORE CURRENT INSTRUCTION --- */
    int NOPs = 0;
 
+   /* GFX9 SALU->VCC->Branch Hazard: Critical performance hazard. A SALU write to VCC
+    * followed by a branch that reads VCC without 2 intervening cycles causes a major
+    * pipeline flush. A VALU instruction in between resolves the hazard. */
+   if (instr->opcode == aco_opcode::s_cbranch_vccz ||
+       instr->opcode == aco_opcode::s_cbranch_vccnz) {
+      NOPs = MAX2(NOPs, (int)ctx.vcc_wr_then_branch);
+   }
+
    if (instr->isSMEM()) {
       if (state.program->gfx_level == GFX6) {
          /* A read of an SGPR by SMRD instruction requires 4 wait states
           * when the SGPR was written by a VALU instruction. According to LLVM,
           * there is also an undocumented hardware behavior when the buffer
-          * descriptor is written by a SALU instruction */
+          * descriptor is written by a SALU instruction. */
          for (unsigned i = 0; i < instr->operands.size(); i++) {
             Operand op = instr->operands[i];
             if (op.isConstant())
@@ -538,7 +614,13 @@ handle_instruction_gfx6(State& state, NO
       if (instr->opcode == aco_opcode::s_setreg_b32 ||
           instr->opcode == aco_opcode::s_setreg_imm32_b32 ||
           instr->opcode == aco_opcode::s_getreg_b32) {
-         NOPs = MAX2(NOPs, ctx.setreg_then_getsetreg);
+         /* s_setreg followed by a s_getreg/s_setreg of the same register needs 2 waits.
+          * We use per-register tracking to avoid stalling unnecessarily. */
+         const SALU_instruction& sopk = instr->salu();
+         unsigned reg = sopk.imm & 0x3f;
+         if (reg < ctx.setreg_waits.size()) {
+            NOPs = MAX2(NOPs, (int)ctx.setreg_waits[reg]);
+         }
       }
 
       if (state.program->gfx_level == GFX9) {
@@ -546,24 +628,28 @@ handle_instruction_gfx6(State& state, NO
              instr->opcode == aco_opcode::s_movrels_b64 ||
              instr->opcode == aco_opcode::s_movreld_b32 ||
              instr->opcode == aco_opcode::s_movreld_b64) {
-            NOPs = MAX2(NOPs, ctx.salu_wr_m0_then_moverel);
+            NOPs = MAX2(NOPs, (int)ctx.salu_wr_m0_then_moverel);
          }
       }
 
       if (instr->opcode == aco_opcode::s_sendmsg || instr->opcode == aco_opcode::s_ttracedata)
-         NOPs = MAX2(NOPs, ctx.salu_wr_m0_then_gds_msg_ttrace);
+         NOPs = MAX2(NOPs, (int)ctx.salu_wr_m0_then_gds_msg_ttrace);
    } else if (instr->isDS() && instr->ds().gds) {
-      NOPs = MAX2(NOPs, ctx.salu_wr_m0_then_gds_msg_ttrace);
+      NOPs = MAX2(NOPs, (int)ctx.salu_wr_m0_then_gds_msg_ttrace);
    } else if (instr->isVALU() || instr->isVINTRP()) {
       if (instr->isDPP()) {
-         NOPs = MAX2(NOPs, ctx.valu_wr_exec_then_dpp);
+         NOPs = MAX2(NOPs, (int)ctx.valu_wr_exec_then_dpp);
          handle_valu_then_read_hazard(state, &NOPs, 2, instr->operands[0]);
       }
 
       for (Definition def : instr->definitions) {
          if (def.regClass().type() != RegType::sgpr) {
-            for (unsigned i = 0; i < def.size(); i++)
-               NOPs = MAX2(NOPs, ctx.vmem_store_then_wr_data[(def.physReg() & 0xff) + i]);
+            for (unsigned i = 0; i < def.size(); i++) {
+               unsigned reg_index = (def.physReg() & 0xff) + i;
+               if (reg_index < 256) {
+                  NOPs = MAX2(NOPs, (int)ctx.vmem_store_then_wr_data[reg_index]);
+               }
+            }
          }
       }
 
@@ -577,9 +663,7 @@ handle_instruction_gfx6(State& state, NO
 
       /* It's required to insert 1 wait state if the dst VGPR of any v_interp_*
        * is followed by a read with v_readfirstlane or v_readlane to fix GPU
-       * hangs on GFX6. Note that v_writelane_* is apparently not affected.
-       * This hazard isn't documented anywhere but AMD confirmed that hazard.
-       */
+       * hangs on GFX6. This hazard isn't documented anywhere but AMD confirmed it. */
       if (state.program->gfx_level == GFX6 &&
           (instr->opcode == aco_opcode::v_readlane_b32 || /* GFX6 doesn't have v_readlane_b32_e64 */
            instr->opcode == aco_opcode::v_readfirstlane_b32)) {
@@ -588,9 +672,9 @@ handle_instruction_gfx6(State& state, NO
 
       if (instr->opcode == aco_opcode::v_div_fmas_f32 ||
           instr->opcode == aco_opcode::v_div_fmas_f64)
-         NOPs = MAX2(NOPs, ctx.valu_wr_vcc_then_div_fmas);
+         NOPs = MAX2(NOPs, (int)ctx.valu_wr_vcc_then_div_fmas);
    } else if (instr->isVMEM() || instr->isFlatLike()) {
-      /* If the VALU writes the SGPR that is used by a VMEM, the user must add five wait states. */
+      /* If a VALU writes an SGPR that is used by a VMEM, the user must add five wait states. */
       for (Operand op : instr->operands) {
          if (!op.isConstant() && !op.isUndefined() && op.regClass().type() == RegType::sgpr)
             handle_valu_then_read_hazard(state, &NOPs, 5, op);
@@ -598,7 +682,7 @@ handle_instruction_gfx6(State& state, NO
    }
 
    if (!instr->isSALU() && instr->format != Format::SMEM)
-      NOPs = MAX2(NOPs, ctx.set_vskip_mode_then_vector);
+      NOPs = MAX2(NOPs, (int)ctx.set_vskip_mode_then_vector);
 
    if (state.program->gfx_level == GFX9) {
       bool lds_scratch_global = (instr->isScratch() || instr->isGlobal()) && instr->flatlike().lds;
@@ -606,21 +690,56 @@ handle_instruction_gfx6(State& state, NO
           instr->opcode == aco_opcode::ds_read_addtid_b32 ||
           instr->opcode == aco_opcode::ds_write_addtid_b32 ||
           instr->opcode == aco_opcode::buffer_store_lds_dword) {
-         NOPs = MAX2(NOPs, ctx.salu_wr_m0_then_lds);
+         NOPs = MAX2(NOPs, (int)ctx.salu_wr_m0_then_lds);
       }
    }
 
-   ctx.add_wait_states(NOPs + get_wait_states(instr));
+   /* --- 2. EMIT WAITS AND AGE CONTEXT --- */
 
-   // TODO: try to schedule the NOP-causing instruction up to reduce the number of stall cycles
    if (NOPs) {
-      /* create NOP */
-      aco_ptr<Instruction> nop{create_instruction(aco_opcode::s_nop, Format::SOPP, 0, 0)};
-      nop->salu().imm = NOPs - 1;
-      new_instructions.emplace_back(std::move(nop));
+      /* Architecturally-aware waiting: for long waits (3+ cycles) related to memory,
+       * `s_waitcnt` is superior to a string of NOPs on GFX8+. It allows the scheduler
+       * to switch to other wavefronts and saves power. */
+      bool is_mem_related_hazard =
+         (ctx.vmem_store_then_wr_data.any() || instr->isSMEM() || instr->isVMEM() ||
+          instr->isFlatLike() || ctx.smem_clause || ctx.smem_write);
+
+      Builder bld(state.program, &new_instructions);
+      if (state.program->gfx_level >= GFX8 && NOPs >= 3 && is_mem_related_hazard) {
+         /* Wait for both vector memory (VM) and LDS/GDS/scalar-mem (LGKM) counters to be 0. */
+         bld.sopp(aco_opcode::s_waitcnt, 0);
+      } else {
+         /* For shorter or non-memory stalls, use s_nop. Coalesce with a previous s_nop if possible. */
+         unsigned remaining_nops = NOPs;
+         if (!new_instructions.empty() && new_instructions.back()->opcode == aco_opcode::s_nop) {
+            /* The immediate field of s_nop is 8 bits and stores (wait_states - 1). Max wait is 256. */
+            unsigned current_imm = new_instructions.back()->salu().imm;
+            unsigned can_add = 255 - current_imm;
+            unsigned to_add = std::min(can_add, remaining_nops);
+
+            new_instructions.back()->salu().imm += to_add;
+            remaining_nops -= to_add;
+         }
+
+         if (remaining_nops > 0) {
+            aco_ptr<Instruction> nop{create_instruction(aco_opcode::s_nop, Format::SOPP, 0, 0)};
+            nop->salu().imm = remaining_nops - 1;
+            new_instructions.emplace_back(std::move(nop));
+         }
+      }
    }
 
-   /* update information to check for later hazards */
+   /* Age all hazard counters by the number of cycles consumed by waits and the current instruction. */
+   ctx.add_wait_states(NOPs + get_wait_states(instr));
+
+   /* --- 3. UPDATE CONTEXT WITH NEW HAZARDS FROM CURRENT INSTRUCTION --- */
+
+   /* A VALU instruction resolves the SALU->VCC->Branch hazard. */
+   if (instr->isVALU()) {
+      ctx.vcc_wr_then_branch = 0;
+   }
+
+   /* Break SMEM clause if a non-SMEM instruction is emitted. */
    if ((ctx.smem_clause || ctx.smem_write) && (NOPs || instr->format != Format::SMEM)) {
       ctx.smem_clause = false;
       ctx.smem_write = false;
@@ -661,31 +780,66 @@ handle_instruction_gfx6(State& state, NO
          }
       }
    } else if (instr->isSALU()) {
-      if (!instr->definitions.empty()) {
-         /* all other definitions should be SCC */
-         Definition def = instr->definitions[0];
+      bool writes_vcc = false;
+      for (const Definition& def : instr->definitions) {
+         if (def.physReg() == vcc || def.physReg() == vcc_hi) {
+            writes_vcc = true;
+         }
          if (def.physReg() == m0) {
             ctx.salu_wr_m0_then_gds_msg_ttrace = 1;
             ctx.salu_wr_m0_then_lds = 1;
             ctx.salu_wr_m0_then_moverel = 1;
          }
-      } else if (instr->opcode == aco_opcode::s_setreg_b32 ||
-                 instr->opcode == aco_opcode::s_setreg_imm32_b32) {
+      }
+      if (writes_vcc) {
+         ctx.vcc_wr_then_branch = 2;
+      }
+
+      if (instr->opcode == aco_opcode::s_setreg_b32 ||
+          instr->opcode == aco_opcode::s_setreg_imm32_b32) {
          SALU_instruction& sopk = instr->salu();
          unsigned offset = (sopk.imm >> 6) & 0x1f;
          unsigned size = ((sopk.imm >> 11) & 0x1f) + 1;
          unsigned reg = sopk.imm & 0x3f;
-         ctx.setreg_then_getsetreg = 2;
 
-         if (reg == 1 && offset >= 28 && size > (28 - offset))
-            ctx.set_vskip_mode_then_vector = 2;
+         if (reg < ctx.setreg_waits.size()) {
+            ctx.setreg_waits[reg] = 2;
+         }
+
+         /* Constant-aware v_skip hazard tracking. */
+         if (reg == 1 && offset <= 28 && (offset + size > 28)) {
+            bool value_known = false;
+            uint32_t value = 0;
+
+            if (instr->opcode == aco_opcode::s_setreg_imm32_b32) {
+               if (instr->operands[0].isConstant()) {
+                  value = instr->operands[0].constantValue();
+                  value_known = true;
+               }
+            } else { /* s_setreg_b32 */
+               if (instr->operands[1].isConstant()) {
+                  value = instr->operands[1].constantValue();
+                  value_known = true;
+               }
+            }
+
+            if (value_known) {
+               uint8_t new_vskip_bit = (value >> (28 - offset)) & 1;
+               if (ctx.current_vskip != new_vskip_bit) {
+                  ctx.set_vskip_mode_then_vector = 2;
+                  ctx.current_vskip = new_vskip_bit;
+               }
+            } else {
+               /* Value is not known, conservatively assume it changes. */
+               ctx.set_vskip_mode_then_vector = 2;
+            }
+         }
       }
    } else if (instr->isVMEM() || instr->isFlatLike()) {
       /* >64-bit MUBUF/MTBUF store with a constant in SOFFSET */
       bool consider_buf = (instr->isMUBUF() || instr->isMTBUF()) && instr->operands.size() == 4 &&
                           instr->operands[3].size() > 2 && instr->operands[2].physReg() >= 128;
-      /* MIMG store with a 128-bit T# with more than two bits set in dmask (making it a >64-bit
-       * store) */
+      /* MIMG store with a 128-bit T# with more than two bits set in dmask (making it a >64-bit store) */
       bool consider_mimg = instr->isMIMG() &&
                            instr->operands[1].regClass().type() == RegType::vgpr &&
                            instr->operands[1].size() > 2 && instr->operands[0].size() == 4;
@@ -695,8 +849,15 @@ handle_instruction_gfx6(State& state, NO
       if (consider_buf || consider_mimg || consider_flat) {
          PhysReg wrdata = instr->operands[consider_flat ? 2 : 3].physReg();
          unsigned size = instr->operands[consider_flat ? 2 : 3].size();
-         for (unsigned i = 0; i < size; i++)
-            ctx.vmem_store_then_wr_data[(wrdata & 0xff) + i] = 1;
+
+         /* Safety: Bounds-check the write to the bitset to prevent UB. */
+         uint32_t base = (wrdata & 0xffu);
+         if (base < 256) {
+            unsigned max_store = std::min<unsigned>(size, 256 - base);
+            for (unsigned i = 0; i < max_store; ++i) {
+               ctx.vmem_store_then_wr_data[base + i] = 1;
+            }
+         }
       }
    }
 }
@@ -756,13 +917,14 @@ resolve_all_gfx6(State& state, NOP_ctx_g
       NOPs = MAX2(NOPs, 1);
 
    /* SALU/GDS hazards */
-   NOPs = MAX2(NOPs, ctx.setreg_then_getsetreg);
+   NOPs = MAX2(NOPs, (int)ctx.get_max_setreg_wait());
    if (state.program->gfx_level == GFX9)
-      NOPs = MAX2(NOPs, ctx.salu_wr_m0_then_moverel);
-   NOPs = MAX2(NOPs, ctx.salu_wr_m0_then_gds_msg_ttrace);
+      NOPs = MAX2(NOPs, (int)ctx.salu_wr_m0_then_moverel);
+   NOPs = MAX2(NOPs, (int)ctx.salu_wr_m0_then_gds_msg_ttrace);
+   NOPs = MAX2(NOPs, (int)ctx.vcc_wr_then_branch);
 
    /* VALU hazards */
-   NOPs = MAX2(NOPs, ctx.valu_wr_exec_then_dpp);
+   NOPs = MAX2(NOPs, (int)ctx.valu_wr_exec_then_dpp);
    if (state.program->gfx_level >= GFX8)
       handle_wr_hazard<false, false>(state, &NOPs, 2); /* VALU->DPP */
    NOPs = MAX2(NOPs, ctx.vmem_store_then_wr_data.any() ? 1 : 0);
@@ -773,20 +935,27 @@ resolve_all_gfx6(State& state, NOP_ctx_g
       if (vintrp)
          NOPs = MAX2(NOPs, 1);
    }
-   NOPs = MAX2(NOPs, ctx.valu_wr_vcc_then_div_fmas);
+   NOPs = MAX2(NOPs, (int)ctx.valu_wr_vcc_then_div_fmas);
 
    /* VALU(sgpr)->VMEM/v_readlane_b32/etc hazards. v_readlane_b32/etc require only 4 NOPs. */
    handle_wr_hazard<false, true>(state, &NOPs, 5);
 
-   NOPs = MAX2(NOPs, ctx.set_vskip_mode_then_vector);
+   NOPs = MAX2(NOPs, (int)ctx.set_vskip_mode_then_vector);
 
    if (state.program->gfx_level == GFX9)
-      NOPs = MAX2(NOPs, ctx.salu_wr_m0_then_lds);
+      NOPs = MAX2(NOPs, (int)ctx.salu_wr_m0_then_lds);
 
    ctx.add_wait_states(NOPs);
    if (NOPs) {
       Builder bld(state.program, &new_instructions);
-      bld.sopp(aco_opcode::s_nop, NOPs - 1);
+      bool is_mem_related_hazard =
+         (ctx.vmem_store_then_wr_data.any() || ctx.smem_clause || ctx.smem_write);
+
+      if (state.program->gfx_level >= GFX8 && NOPs >= 3 && is_mem_related_hazard) {
+         bld.sopp(aco_opcode::s_waitcnt, 0);
+      } else {
+         bld.sopp(aco_opcode::s_nop, NOPs - 1);
+      }
    }
 }
 
@@ -795,8 +964,7 @@ bool
 check_written_regs(const aco_ptr<Instruction>& instr, const std::bitset<N>& check_regs)
 {
    return std::any_of(instr->definitions.begin(), instr->definitions.end(),
-                      [&check_regs](const Definition& def) -> bool
-                      {
+                      [&check_regs](const Definition& def) -> bool {
                          bool writes_any = false;
                          for (unsigned i = 0; i < def.size(); i++) {
                             unsigned def_reg = def.physReg() + i;
@@ -847,8 +1015,9 @@ bool
 instr_writes_sgpr(const aco_ptr<Instruction>& instr)
 {
    return std::any_of(instr->definitions.begin(), instr->definitions.end(),
-                      [](const Definition& def) -> bool
-                      { return def.getTemp().type() == RegType::sgpr; });
+                      [](const Definition& def) -> bool {
+                         return def.getTemp().type() == RegType::sgpr;
+                      });
 }
 
 inline bool
@@ -1157,13 +1326,15 @@ test_vgpr_bitset(std::bitset<256>& set,
 }
 
 /* GFX11 */
-struct LdsDirectVALUHazardGlobalState {
+struct LdsDirectVALUHazardGlobalState
+{
    unsigned wait_vdst = 15;
    PhysReg vgpr;
    std::set<unsigned> loop_headers_visited;
 };
 
-struct LdsDirectVALUHazardBlockState {
+struct LdsDirectVALUHazardBlockState
+{
    unsigned num_valu = 0;
    bool has_trans = false;
 
@@ -1250,12 +1421,14 @@ enum VALUPartialForwardingHazardState :
    exec_written,
 };
 
-struct VALUPartialForwardingHazardGlobalState {
+struct VALUPartialForwardingHazardGlobalState
+{
    bool hazard_found = false;
    std::set<unsigned> loop_headers_visited;
 };
 
-struct VALUPartialForwardingHazardBlockState {
+struct VALUPartialForwardingHazardBlockState
+{
    /* initialized by number of VGPRs read by VALU, decrement when encountered to return early */
    uint8_t num_vgprs_read = 0;
    BITSET_DECLARE(vgprs_read, 256) = {0};
@@ -1578,7 +1751,8 @@ handle_instruction_gfx11(State& state, N
                   ctx.sgpr_read_by_valu_as_lanemask.set(instr->operands.back().physReg().reg() + 1);
                }
                break;
-            default: break;
+            default:
+               break;
             }
          }
       }
@@ -2025,7 +2199,8 @@ insert_NOPs(Program* program)
       bool has_previous_part =
          program->is_epilog || program->info.vs.has_prolog || program->info.ps.has_prolog ||
          (program->info.merged_shader_compiled_separately && program->stage.sw != SWStage::VS &&
-          program->stage.sw != SWStage::TES) || program->stage == raytracing_cs;
+          program->stage.sw != SWStage::TES) ||
+         program->stage == raytracing_cs;
       if (program->gfx_level >= GFX12 && has_previous_part) {
          /* resolve_all_gfx11 can't resolve VALUReadSGPRHazard entirely. We have to assume that any
           * SGPR might have been read by VALU if there was a previous shader part.
