From 7512c9f3a86016159b7407a6893b975637b8cd93 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Thu, 20 Apr 2023 15:06:53 +0200
Subject: [PATCH 1/2] aco: Don't emulate Wave64 bpermute when workgroup size is
 <= 32.

---
 src/amd/compiler/aco_instruction_selection.cpp | 16 +++++++++++-----
 1 file changed, 11 insertions(+), 5 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 5ff0415da697..afecc2fe59ad 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -192,6 +192,15 @@ emit_bpermute(isel_context* ctx, Builder& bld, Temp index, Temp data)
    if (index.regClass() == s1)
       return bld.readlane(bld.def(s1), data, index);
 
+   if ((ctx->options->gfx_level >= GFX8 && ctx->options->gfx_level <= GFX9) ||
+       (ctx->options->gfx_level >= GFX10 && (ctx->program->wave_size == 32 || ctx->program->workgroup_size <= 32))) {
+      /* GFX8-9 / GFX10+ Wave32: bpermute works normally.
+       * We can also just use this on GFX10+ when the workgroup size is <= 32.
+       */
+      Temp index_x4 = bld.vop2(aco_opcode::v_lshlrev_b32, bld.def(v1), Operand::c32(2u), index);
+      return bld.ds(aco_opcode::ds_bpermute_b32, bld.def(v1), index_x4, data);
+   }
+
    /* Avoid using shared VGPRs for shuffle on GFX10 when the shader consists
     * of multiple binaries, because the VGPR use is not known when choosing
     * which registers to use for the shared VGPRs.
@@ -211,7 +220,8 @@ emit_bpermute(isel_context* ctx, Builder& bld, Temp index, Temp data)
 
       return bld.pseudo(aco_opcode::p_bpermute_gfx6, bld.def(v1), bld.def(bld.lm),
                         bld.def(bld.lm, vcc), index_op, input_data);
-   } else if (ctx->options->gfx_level >= GFX10 && ctx->program->wave_size == 64) {
+   } else {
+      assert(ctx->options->gfx_level >= GFX10 && ctx->program->wave_size == 64);
 
       /* GFX10 wave64 mode: emulate full-wave bpermute */
       Temp index_is_lo =
@@ -242,10 +252,6 @@ emit_bpermute(isel_context* ctx, Builder& bld, Temp index, Temp data)
                            bld.def(s1, scc), Operand(v1.as_linear()), index_x4, input_data,
                            same_half);
       }
-   } else {
-      /* GFX8-9 or GFX10 wave32: bpermute works normally */
-      Temp index_x4 = bld.vop2(aco_opcode::v_lshlrev_b32, bld.def(v1), Operand::c32(2u), index);
-      return bld.ds(aco_opcode::ds_bpermute_b32, bld.def(v1), index_x4, data);
    }
 }
 
-- 
GitLab


From b89483da66b28453a2d86ec7a06b65d32ec2437f Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Thu, 20 Apr 2023 15:22:52 +0200
Subject: [PATCH 2/2] aco: Emit simpler reductions when workgroup size <= 32.

---
 src/amd/compiler/aco_lower_to_hw_instr.cpp | 11 +++++++----
 1 file changed, 7 insertions(+), 4 deletions(-)

diff --git a/src/amd/compiler/aco_lower_to_hw_instr.cpp b/src/amd/compiler/aco_lower_to_hw_instr.cpp
index 61fe7ebb66cb..917736dc5e70 100644
--- a/src/amd/compiler/aco_lower_to_hw_instr.cpp
+++ b/src/amd/compiler/aco_lower_to_hw_instr.cpp
@@ -477,6 +477,10 @@ emit_reduction(lower_context* ctx, aco_opcode op, ReduceOp reduce_op, unsigned c
    assert(cluster_size == ctx->program->wave_size || op == aco_opcode::p_reduce);
    assert(cluster_size <= ctx->program->wave_size);
 
+   /* Emit simpler instruction sequence when the workgroup is small. */
+   if (ctx->program->workgroup_size <= 32)
+      cluster_size = MIN2(cluster_size, 32);
+
    Builder bld(ctx->program, &ctx->instructions);
 
    Operand identity[2];
@@ -659,7 +663,7 @@ emit_reduction(lower_context* ctx, aco_opcode op, ReduceOp reduce_op, unsigned c
          }
          bld.sop1(Builder::s_mov, Definition(exec, bld.lm), Operand::c64(UINT64_MAX));
 
-         if (ctx->program->wave_size == 64) {
+         if (cluster_size == 64) {
             /* fill in the gap in row 2 */
             for (unsigned i = 0; i < src.size(); i++) {
                bld.readlane(Definition(PhysReg{sitmp + i}, s1), Operand(PhysReg{tmp + i}, v1),
@@ -728,7 +732,6 @@ emit_reduction(lower_context* ctx, aco_opcode op, ReduceOp reduce_op, unsigned c
       }
       FALLTHROUGH;
    case aco_opcode::p_inclusive_scan:
-      assert(cluster_size == ctx->program->wave_size);
       if (ctx->program->gfx_level <= GFX7) {
          emit_ds_swizzle(bld, vtmp, tmp, src.size(), ds_pattern_bitmode(0x1e, 0x00, 0x00));
          bld.sop1(aco_opcode::s_mov_b32, Definition(exec_lo, s1), Operand::c32(0xAAAAAAAAu));
@@ -793,7 +796,7 @@ emit_reduction(lower_context* ctx, aco_opcode op, ReduceOp reduce_op, unsigned c
          }
          emit_op(ctx, tmp, tmp, vtmp, PhysReg{0}, reduce_op, src.size());
 
-         if (ctx->program->wave_size == 64) {
+         if (cluster_size == 64) {
             bld.sop2(aco_opcode::s_bfm_b64, Definition(exec, s2), Operand::c32(32u),
                      Operand::c32(32u));
             for (unsigned i = 0; i < src.size(); i++)
@@ -828,7 +831,7 @@ emit_reduction(lower_context* ctx, aco_opcode op, ReduceOp reduce_op, unsigned c
    if (dst.regClass().type() == RegType::sgpr) {
       for (unsigned k = 0; k < src.size(); k++) {
          bld.readlane(Definition(PhysReg{dst.physReg() + k}, s1), Operand(PhysReg{tmp + k}, v1),
-                      Operand::c32(ctx->program->wave_size - 1));
+                      Operand::c32(cluster_size - 1));
       }
    } else if (dst.physReg() != tmp) {
       for (unsigned k = 0; k < src.size(); k++) {
-- 
GitLab

