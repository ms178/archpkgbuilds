From d3387d5d33239eb3584786bcaa83cf532c0856e6 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Fri, 6 Oct 2023 01:14:04 +0200
Subject: [PATCH 1/3] radv: Implement buffer/image copies on transfer queues.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Previously, RADV only had a simple implementation of
image to buffer copies using the SDMA for the PRIME copy.

This commit replaces that with a full-featured implementation
that includes buffer to image and image to buffer copies and
removes the assumptions that the PRIME copy had, as well as
adds new helper functions which will be shared with other copy
functions in upcoming commits.

Unaligned buffer/image copies require a workaround, which
will be implemented by a future commit.

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
Reviewed-by: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
---
 src/amd/vulkan/meta/radv_meta_copy.c |  18 +-
 src/amd/vulkan/radv_private.h        |   4 +-
 src/amd/vulkan/radv_sdma.c           | 561 ++++++++++++++++++++++-----
 3 files changed, 474 insertions(+), 109 deletions(-)

diff --git a/src/amd/vulkan/meta/radv_meta_copy.c b/src/amd/vulkan/meta/radv_meta_copy.c
index ede2cb92f76cd..67f5d31472b62 100644
--- a/src/amd/vulkan/meta/radv_meta_copy.c
+++ b/src/amd/vulkan/meta/radv_meta_copy.c
@@ -89,6 +89,16 @@ static void
 copy_buffer_to_image(struct radv_cmd_buffer *cmd_buffer, struct radv_buffer *buffer, struct radv_image *image,
                      VkImageLayout layout, const VkBufferImageCopy2 *region)
 {
+   const struct radv_device *device = cmd_buffer->device;
+
+   if (cmd_buffer->qf == RADV_QUEUE_TRANSFER) {
+      struct radeon_cmdbuf *cs = cmd_buffer->cs;
+      radv_cs_add_buffer(device->ws, cs, image->bindings[0].bo);
+      radv_cs_add_buffer(device->ws, cs, buffer->bo);
+      radv_sdma_copy_buffer_image(device, cs, image, buffer, region, true);
+      return;
+   }
+
    struct radv_meta_saved_state saved_state;
    bool cs;
 
@@ -237,15 +247,9 @@ copy_image_to_buffer(struct radv_cmd_buffer *cmd_buffer, struct radv_buffer *buf
    struct radv_device *device = cmd_buffer->device;
    if (cmd_buffer->qf == RADV_QUEUE_TRANSFER) {
       struct radeon_cmdbuf *cs = cmd_buffer->cs;
-      /* RADV_QUEUE_TRANSFER should only be used for the prime blit */
-      assert(!region->imageOffset.x && !region->imageOffset.y && !region->imageOffset.z);
-      assert(image->vk.image_type == VK_IMAGE_TYPE_2D);
-      assert(image->vk.extent.width == region->imageExtent.width);
-      assert(image->vk.extent.height == region->imageExtent.height);
-      ASSERTED bool res = radv_sdma_copy_image(device, cs, image, buffer, region);
-      assert(res);
       radv_cs_add_buffer(device->ws, cs, image->bindings[0].bo);
       radv_cs_add_buffer(device->ws, cs, buffer->bo);
+      radv_sdma_copy_buffer_image(device, cs, image, buffer, region, false);
       return;
    }
 
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index 77e1206a7f862..ad66229ad3846 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -3135,8 +3135,8 @@ void radv_rra_trace_init(struct radv_device *device);
 VkResult radv_rra_dump_trace(VkQueue vk_queue, char *filename);
 void radv_rra_trace_finish(VkDevice vk_device, struct radv_rra_trace_data *data);
 
-bool radv_sdma_copy_image(struct radv_device *device, struct radeon_cmdbuf *cs, struct radv_image *image,
-                          struct radv_buffer *buffer, const VkBufferImageCopy2 *region);
+void radv_sdma_copy_buffer_image(const struct radv_device *device, struct radeon_cmdbuf *cs, struct radv_image *image,
+                                 struct radv_buffer *buffer, const VkBufferImageCopy2 *region, bool to_image);
 void radv_sdma_copy_buffer(const struct radv_device *device, struct radeon_cmdbuf *cs, uint64_t src_va, uint64_t dst_va,
                            uint64_t size);
 
diff --git a/src/amd/vulkan/radv_sdma.c b/src/amd/vulkan/radv_sdma.c
index f2cc71c47a111..23d82f7f322d8 100644
--- a/src/amd/vulkan/radv_sdma.c
+++ b/src/amd/vulkan/radv_sdma.c
@@ -1,6 +1,7 @@
 /*
  * Copyright 2010 Jerome Glisse <glisse@freedesktop.org>
  * Copyright 2015-2021 Advanced Micro Devices, Inc.
+ * Copyright 2023 Valve Corporation
  * All Rights Reserved.
  *
  * Permission is hereby granted, free of charge, to any person obtaining a
@@ -27,119 +28,340 @@
 #include "radv_cs.h"
 #include "radv_private.h"
 
-static bool
-radv_sdma_v4_v5_copy_image_to_buffer(struct radv_device *device, struct radeon_cmdbuf *cs, struct radv_image *image,
-                                     struct radv_buffer *buffer, const VkBufferImageCopy2 *region)
-{
-   assert(image->plane_count == 1);
-   unsigned bpp = image->planes[0].surface.bpe;
-   uint64_t dst_address = buffer->bo->va;
-   uint64_t src_address = image->bindings[0].bo->va + image->planes[0].surface.u.gfx9.surf_offset;
-   unsigned src_pitch = image->planes[0].surface.u.gfx9.surf_pitch;
-   unsigned copy_width = DIV_ROUND_UP(image->vk.extent.width, image->planes[0].surface.blk_w);
-   unsigned copy_height = DIV_ROUND_UP(image->vk.extent.height, image->planes[0].surface.blk_h);
-   bool tmz = false;
+struct radv_sdma_linear_info {
+   uint64_t va;
+   unsigned pitch;
+   unsigned slice_pitch;
+   unsigned bpp;
+   unsigned blk_w;
+   unsigned blk_h;
+};
 
-   /* Linear -> linear sub-window copy. */
-   if (image->planes[0].surface.is_linear) {
-      bool is_v5_2 = device->physical_device->rad_info.gfx_level >= GFX10_3;
-      uint64_t bytes = (uint64_t)src_pitch * copy_height * bpp;
-      uint32_t chunk_size = 1u << (is_v5_2 ? 30 : 22);
-      uint32_t chunk_count = DIV_ROUND_UP(bytes, chunk_size);
+struct radv_sdma_tiled_info {
+   VkExtent3D extent;
+   uint64_t va;
+   uint64_t meta_va;
+   uint32_t meta_config;
+   uint32_t info_dword;
+   uint32_t header_dword;
+   unsigned bpp;
+   unsigned blk_w;
+   unsigned blk_h;
+};
+
+ALWAYS_INLINE static void
+radv_sdma_check_extent(const VkExtent3D extent)
+{
+   assert(extent.width <= (1 << 14));
+   assert(extent.height <= (1 << 14));
+   assert(extent.depth <= (1 << 13));
+}
+
+ALWAYS_INLINE static void
+radv_sdma_check_offset(const VkOffset3D offset)
+{
+   assert(offset.x < (1 << 14));
+   assert(offset.y < (1 << 14));
+   assert(offset.z < (1 << 11));
+}
+
+ALWAYS_INLINE static void
+radv_sdma_check_pitches(const unsigned pitch, const unsigned slice_pitch, const unsigned bpp, const bool uses_depth)
+{
+   ASSERTED const unsigned pitch_alignment = MAX2(1, 4 / bpp);
+   assert(pitch);
+   assert(pitch <= (1 << 14));
+   assert(radv_is_aligned(pitch, pitch_alignment));
+
+   if (uses_depth) {
+      ASSERTED const unsigned slice_pitch_alignment = 4;
+      assert(slice_pitch);
+      assert(slice_pitch <= (1 << 28));
+      assert(radv_is_aligned(slice_pitch, slice_pitch_alignment));
+   }
+}
+
+ALWAYS_INLINE static enum gfx9_resource_type
+radv_sdma_surface_resource_type(const struct radv_device *const device, const struct radeon_surf *const surf)
+{
+   if (device->physical_device->rad_info.gfx_level >= GFX10) {
+      /* Use the 2D resource type for rotated or Z swizzles. */
+      if ((surf->u.gfx9.resource_type == RADEON_RESOURCE_1D || surf->u.gfx9.resource_type == RADEON_RESOURCE_3D) &&
+          (surf->micro_tile_mode == RADEON_MICRO_MODE_RENDER || surf->micro_tile_mode == RADEON_MICRO_MODE_DEPTH))
+         return RADEON_RESOURCE_2D;
+   }
+
+   return surf->u.gfx9.resource_type;
+}
+
+ALWAYS_INLINE static VkFormat
+radv_sdma_format_from_aspect_mask(const VkImageAspectFlags aspectMask, const VkFormat format)
+{
+   if (aspectMask & VK_IMAGE_ASPECT_DEPTH_BIT)
+      return vk_format_depth_only(format);
+   else if (aspectMask & VK_IMAGE_ASPECT_STENCIL_BIT)
+      return vk_format_stencil_only(format);
+
+   return format;
+}
+
+ALWAYS_INLINE static uint32_t
+radv_sdma_surface_type_from_aspect_mask(const VkImageAspectFlags aspectMask)
+{
+   if (aspectMask & VK_IMAGE_ASPECT_DEPTH_BIT)
+      return 1;
+   else if (aspectMask & VK_IMAGE_ASPECT_STENCIL_BIT)
+      return 2;
+
+   return 0;
+}
+
+ALWAYS_INLINE static unsigned
+radv_sdma_max_uncompressed_block_size(const struct radv_device *const device, const struct radv_image *const image,
+                                      const unsigned bpp)
+{
+   if (device->physical_device->rad_info.gfx_level < GFX10 && image->vk.samples > 1) {
+      if (bpp == 1)
+         return V_028C78_MAX_BLOCK_SIZE_64B;
+      else if (bpp == 2)
+         return V_028C78_MAX_BLOCK_SIZE_128B;
+   }
+
+   return V_028C78_MAX_BLOCK_SIZE_256B;
+}
+
+ALWAYS_INLINE static unsigned
+radv_sdma_get_img_offset_z(const struct radv_image *const image, const VkImageSubresourceLayers subresource,
+                           const VkOffset3D offset)
+{
+   const struct radeon_surf *const surf = &image->planes[0].surface;
+   const bool is_3d = surf->u.gfx9.resource_type == RADEON_RESOURCE_3D;
+   return is_3d ? offset.z : subresource.baseArrayLayer;
+}
+
+ALWAYS_INLINE static unsigned
+radv_sdma_get_copy_extent_depth(const struct radv_image *const image, const VkImageSubresourceLayers subresource,
+                                const VkExtent3D extent)
+{
+   const struct radeon_surf *const surf = &image->planes[0].surface;
+   const bool is_3d = surf->u.gfx9.resource_type == RADEON_RESOURCE_3D;
+   if (is_3d)
+      return extent.depth;
+
+   if (subresource.layerCount == VK_REMAINING_ARRAY_LAYERS)
+      return image->vk.array_layers - subresource.baseArrayLayer;
+
+   return subresource.layerCount;
+}
+
+ALWAYS_INLINE static unsigned
+radv_sdma_get_image_extent_depth(const struct radv_image *const image)
+{
+   const struct radeon_surf *const surf = &image->planes[0].surface;
+   const bool is_3d = surf->u.gfx9.resource_type == RADEON_RESOURCE_3D;
+   if (is_3d)
+      return image->vk.extent.depth;
+
+   return image->vk.array_layers;
+}
+
+ALWAYS_INLINE static VkOffset3D
+radv_sdma_get_img_offset(const struct radv_image *const image, const VkImageSubresourceLayers subresource,
+                         VkOffset3D offset)
+{
+   offset.z = radv_sdma_get_img_offset_z(image, subresource, offset);
+   return offset;
+}
+
+ALWAYS_INLINE static VkExtent3D
+radv_sdma_get_copy_extent(const struct radv_image *const image, const VkImageSubresourceLayers subresource,
+                          VkExtent3D extent)
+{
+   extent.depth = radv_sdma_get_copy_extent_depth(image, subresource, extent);
+   return extent;
+}
+
+ALWAYS_INLINE static VkExtent3D
+radv_sdma_pixel_extent_to_blocks(const VkExtent3D extent, const unsigned blk_w, const unsigned blk_h)
+{
+   const VkExtent3D r = {
+      .width = DIV_ROUND_UP(extent.width, blk_w),
+      .height = DIV_ROUND_UP(extent.height, blk_h),
+      .depth = extent.depth,
+   };
+
+   return r;
+}
+
+ALWAYS_INLINE static VkOffset3D
+radv_sdma_pixel_offset_to_blocks(const VkOffset3D offset, const unsigned blk_w, const unsigned blk_h)
+{
+   const VkOffset3D r = {
+      .x = DIV_ROUND_UP(offset.x, blk_w),
+      .y = DIV_ROUND_UP(offset.y, blk_h),
+      .z = offset.z,
+   };
+
+   return r;
+}
+
+ALWAYS_INLINE static unsigned
+radv_sdma_pixels_to_blocks(const unsigned linear_pitch, const unsigned blk_w)
+{
+   return DIV_ROUND_UP(linear_pitch, blk_w);
+}
+
+ALWAYS_INLINE static unsigned
+radv_sdma_pixel_area_to_blocks(const unsigned linear_slice_pitch, const unsigned blk_w, const unsigned blk_h)
+{
+   return DIV_ROUND_UP(DIV_ROUND_UP(linear_slice_pitch, blk_w), blk_h);
+}
 
-      ASSERTED unsigned cdw_max = radeon_check_space(device->ws, cs, 7 * chunk_count);
+static struct radv_sdma_linear_info
+radv_sdma_get_linear_buf_info(const struct radv_buffer *const buffer, const struct radv_image *const image,
+                              const VkBufferImageCopy2 *const region)
+{
+   const unsigned pitch = (region->bufferRowLength ? region->bufferRowLength : region->imageExtent.width);
+   const unsigned slice_pitch =
+      (region->bufferImageHeight ? region->bufferImageHeight : region->imageExtent.height) * pitch;
+
+   const struct radeon_surf *surf = &image->planes[0].surface;
+   const struct radv_sdma_linear_info info = {
+      .va = radv_buffer_get_va(buffer->bo) + buffer->offset + region->bufferOffset,
+      .pitch = pitch,
+      .slice_pitch = slice_pitch,
+      .bpp = surf->bpe,
+      .blk_w = surf->blk_w,
+      .blk_h = surf->blk_h,
+   };
+
+   return info;
+}
 
-      src_address += image->planes[0].surface.u.gfx9.offset[0];
+static struct radv_sdma_linear_info
+radv_sdma_get_linear_img_info(const struct radv_image *const image, const VkImageSubresourceLayers subresource)
+{
+   const struct radeon_surf *surf = &image->planes[0].surface;
+
+   if (!surf->is_linear) {
+      const struct radv_sdma_linear_info empty_info = {0};
+      return empty_info;
+   }
 
-      for (int i = 0; i < chunk_count; i++) {
-         uint32_t size = MIN2(chunk_size, bytes);
-         radeon_emit(cs, CIK_SDMA_PACKET(CIK_SDMA_OPCODE_COPY, CIK_SDMA_COPY_SUB_OPCODE_LINEAR, (tmz ? 4 : 0)));
-         radeon_emit(cs, size - 1);
-         radeon_emit(cs, 0);
-         radeon_emit(cs, src_address);
-         radeon_emit(cs, src_address >> 32);
-         radeon_emit(cs, dst_address);
-         radeon_emit(cs, dst_address >> 32);
+   const struct radv_sdma_linear_info info = {
+      .va = image->bindings[0].bo->va + image->bindings[0].offset + surf->u.gfx9.surf_offset +
+            surf->u.gfx9.offset[subresource.mipLevel],
+      .pitch = surf->u.gfx9.pitch[subresource.mipLevel],
+      .slice_pitch = surf->blk_w * surf->blk_h * surf->u.gfx9.surf_slice_size / surf->bpe,
+      .bpp = surf->bpe,
+      .blk_w = surf->blk_w,
+      .blk_h = surf->blk_h,
+   };
 
-         src_address += size;
-         dst_address += size;
-         bytes -= size;
-      }
+   return info;
+}
 
-      assert(cs->cdw <= cdw_max);
+static uint32_t
+radv_sdma_get_metadata_config(const struct radv_device *const device, const struct radv_image *const image,
+                              const VkImageSubresourceLayers subresource)
+{
+   /* Only SDMA 5 supports metadata. */
+   const bool is_v5 = device->physical_device->rad_info.gfx_level >= GFX10;
 
-      return true;
+   if (!is_v5 || !(radv_dcc_enabled(image, subresource.mipLevel) || radv_image_has_htile(image))) {
+      return 0;
    }
-   /* Tiled sub-window copy -> Linear */
-   else {
-      unsigned tiled_width = copy_width;
-      unsigned tiled_height = copy_height;
-      unsigned linear_pitch = region->bufferRowLength;
-      uint64_t linear_slice_pitch = (uint64_t)region->bufferRowLength * copy_height;
-      uint64_t tiled_address = src_address;
-      uint64_t linear_address = dst_address;
-      bool is_v5 = device->physical_device->rad_info.gfx_level >= GFX10;
-      /* Only SDMA 5 supports DCC with SDMA */
-      bool dcc = radv_dcc_enabled(image, 0) && is_v5;
-
-      /* Check if everything fits into the bitfields */
-      if (!(tiled_width < (1 << 14) && tiled_height < (1 << 14) && linear_pitch < (1 << 14) &&
-            linear_slice_pitch < (1 << 28) && copy_width < (1 << 14) && copy_height < (1 << 14)))
-         return false;
-
-      ASSERTED unsigned cdw_max = radeon_check_space(device->ws, cs, 14 + (dcc ? 3 : 0));
-
-      radeon_emit(cs, CIK_SDMA_PACKET(CIK_SDMA_OPCODE_COPY, CIK_SDMA_COPY_SUB_OPCODE_TILED_SUB_WINDOW, (tmz ? 4 : 0)) |
-                         dcc << 19 | (is_v5 ? 0 : 0 /* tiled->buffer.b.b.last_level */) << 20 | 1u << 31);
-      radeon_emit(cs, (uint32_t)tiled_address | (image->planes[0].surface.tile_swizzle << 8));
-      radeon_emit(cs, (uint32_t)(tiled_address >> 32));
-      radeon_emit(cs, 0);
-      radeon_emit(cs, ((tiled_width - 1) << 16));
-      radeon_emit(cs, (tiled_height - 1));
-      radeon_emit(cs, util_logbase2(bpp) | image->planes[0].surface.u.gfx9.swizzle_mode << 3 |
-                         image->planes[0].surface.u.gfx9.resource_type << 9 |
-                         (is_v5 ? 0 /* tiled->buffer.b.b.last_level */ : image->planes[0].surface.u.gfx9.epitch) << 16);
-      radeon_emit(cs, (uint32_t)linear_address);
-      radeon_emit(cs, (uint32_t)(linear_address >> 32));
-      radeon_emit(cs, 0);
-      radeon_emit(cs, ((linear_pitch - 1) << 16));
-      radeon_emit(cs, linear_slice_pitch - 1);
-      radeon_emit(cs, (copy_width - 1) | ((copy_height - 1) << 16));
-      radeon_emit(cs, 0);
-
-      if (dcc) {
-         uint64_t md_address = tiled_address + image->planes[0].surface.meta_offset;
-         const struct util_format_description *desc;
-         VkFormat format = image->vk.format;
-         unsigned hw_fmt, hw_type;
-
-         desc = vk_format_description(image->vk.format);
-         hw_fmt = ac_get_cb_format(device->physical_device->rad_info.gfx_level, vk_format_to_pipe_format(format));
-         hw_type = radv_translate_buffer_numformat(desc, vk_format_get_first_non_void_channel(format));
-
-         /* Add metadata */
-         radeon_emit(cs, (uint32_t)md_address);
-         radeon_emit(cs, (uint32_t)(md_address >> 32));
-         radeon_emit(cs, hw_fmt | vi_alpha_is_on_msb(device, format) << 8 | hw_type << 9 |
-                            image->planes[0].surface.u.gfx9.color.dcc.max_compressed_block_size << 24 |
-                            V_028C78_MAX_BLOCK_SIZE_256B << 26 | tmz << 29 |
-                            image->planes[0].surface.u.gfx9.color.dcc.pipe_aligned << 31);
-      }
-
-      assert(cs->cdw <= cdw_max);
-
-      return true;
+
+   const struct radeon_surf *const surf = &image->planes[0].surface;
+   const VkFormat format = radv_sdma_format_from_aspect_mask(subresource.aspectMask, image->vk.format);
+   const struct util_format_description *desc = vk_format_description(format);
+
+   const uint32_t data_format =
+      ac_get_cb_format(device->physical_device->rad_info.gfx_level, vk_format_to_pipe_format(format));
+   const uint32_t color_transform_disable = 0;
+   const uint32_t alpha_is_on_msb = vi_alpha_is_on_msb(device, format);
+   const uint32_t number_type = radv_translate_buffer_numformat(desc, vk_format_get_first_non_void_channel(format));
+   const uint32_t surface_type = radv_sdma_surface_type_from_aspect_mask(subresource.aspectMask);
+   const uint32_t max_comp_block_size = surf->u.gfx9.color.dcc.max_compressed_block_size;
+   const uint32_t max_uncomp_block_size = radv_sdma_max_uncompressed_block_size(device, image, surf->bpe);
+   const uint32_t meta_tmz = 0;
+   const uint32_t pipe_aligned = surf->u.gfx9.color.dcc.pipe_aligned;
+
+   return data_format | color_transform_disable << 7 | alpha_is_on_msb << 8 | number_type << 9 | surface_type << 12 |
+          max_comp_block_size << 24 | max_uncomp_block_size << 26 | meta_tmz << 29 | pipe_aligned << 31;
+}
+
+static uint32_t
+radv_sdma_get_tiled_info_dword(const struct radv_device *const device, const struct radv_image *const image,
+                               const VkImageSubresourceLayers subresource)
+{
+   const struct radeon_surf *const surf = &image->planes[0].surface;
+   const uint32_t element_size = util_logbase2(surf->bpe);
+   const uint32_t swizzle_mode = surf->has_stencil ? surf->u.gfx9.zs.stencil_swizzle_mode : surf->u.gfx9.swizzle_mode;
+   const enum gfx9_resource_type dimension = radv_sdma_surface_resource_type(device, surf);
+   const uint32_t info = element_size | swizzle_mode << 3 | dimension << 9;
+
+   if (device->physical_device->rad_info.gfx_level >= GFX10) {
+      const uint32_t mip_max = MAX2(image->vk.mip_levels, 1);
+      const uint32_t mip_id = subresource.mipLevel;
+
+      return info | (mip_max - 1) << 16 | mip_id << 20;
+   } else if (device->physical_device->rad_info.gfx_level == GFX9) {
+      return info | surf->u.gfx9.epitch << 16;
+   } else {
+      unreachable("unsupported gfx_level");
    }
+}
+
+static uint32_t
+radv_sdma_get_tiled_header_dword(const struct radv_device *const device, const struct radv_image *const image,
+                                 const VkImageSubresourceLayers subresource)
+{
+   if (device->physical_device->rad_info.gfx_level >= GFX10)
+      return 0;
 
-   return false;
+   assert(device->physical_device->rad_info.gfx_level == GFX9);
+
+   const uint32_t mip_max = MAX2(image->vk.mip_levels, 1);
+   const uint32_t mip_id = subresource.mipLevel;
+   return (mip_max - 1) << 20 | mip_id << 24;
 }
 
-bool
-radv_sdma_copy_image(struct radv_device *device, struct radeon_cmdbuf *cs, struct radv_image *image,
-                     struct radv_buffer *buffer, const VkBufferImageCopy2 *region)
+static struct radv_sdma_tiled_info
+radv_sdma_get_tiled_img_info(const struct radv_device *const device, const struct radv_image *const image,
+                             const VkImageSubresourceLayers subresource)
 {
-   assert(device->physical_device->rad_info.gfx_level >= GFX9);
-   return radv_sdma_v4_v5_copy_image_to_buffer(device, cs, image, buffer, region);
+   const struct radeon_surf *const surf = &image->planes[0].surface;
+
+   if (surf->is_linear) {
+      const struct radv_sdma_tiled_info empty_info = {0};
+      return empty_info;
+   }
+
+   assert(surf->u.gfx9.resource_type != RADEON_RESOURCE_1D);
+
+   const uint32_t meta_config = radv_sdma_get_metadata_config(device, image, subresource);
+   const uint64_t meta_va = image->bindings[0].bo->va + image->bindings[0].offset + surf->meta_offset;
+
+   struct radv_sdma_tiled_info info = {
+      .bpp = surf->bpe,
+      .va = (image->bindings[0].bo->va + image->bindings[0].offset + surf->u.gfx9.surf_offset) | surf->tile_swizzle
+                                                                                                    << 8,
+      .meta_va = meta_config ? meta_va : 0,
+      .meta_config = meta_config,
+      .extent =
+         {
+            .width = image->vk.extent.width,
+            .height = image->vk.extent.height,
+            .depth = radv_sdma_get_image_extent_depth(image),
+         },
+      .info_dword = radv_sdma_get_tiled_info_dword(device, image, subresource),
+      .header_dword = radv_sdma_get_tiled_header_dword(device, image, subresource),
+      .blk_w = surf->blk_w,
+      .blk_h = surf->blk_h,
+   };
+
+   return info;
 }
 
 void
@@ -185,3 +407,142 @@ radv_sdma_copy_buffer(const struct radv_device *device, struct radeon_cmdbuf *cs
       size -= csize;
    }
 }
+
+static void
+radv_sdma_emit_copy_linear_sub_window(const struct radv_device *device, struct radeon_cmdbuf *cs,
+                                      const struct radv_sdma_linear_info *const src, const struct radv_sdma_linear_info *const dst,
+                                      const VkOffset3D src_pix_offset, const VkOffset3D dst_pix_offset,
+                                      const VkExtent3D pix_extent)
+{
+   /* This packet is the same since SDMA v2.4, haven't bothered to check older versions.
+    * The main difference is the bitfield sizes:
+    *
+    * v2.4 - src/dst_pitch: 14 bits, rect_z: 11 bits
+    * v4.0 - src/dst_pitch: 19 bits, rect_z: 11 bits
+    * v5.0 - src/dst_pitch: 19 bits, rect_z: 13 bits
+    *
+    * We currently use the smallest limits (from SDMA v2.4).
+    */
+
+   assert(device->physical_device->rad_info.gfx_level >= GFX8);
+
+   const VkOffset3D src_off = radv_sdma_pixel_offset_to_blocks(src_pix_offset, src->blk_w, src->blk_h);
+   const VkOffset3D dst_off = radv_sdma_pixel_offset_to_blocks(dst_pix_offset, dst->blk_w, dst->blk_h);
+   const VkExtent3D ext = radv_sdma_pixel_extent_to_blocks(pix_extent, src->blk_w, src->blk_h);
+   const unsigned src_pitch = radv_sdma_pixels_to_blocks(src->pitch, src->blk_w);
+   const unsigned dst_pitch = radv_sdma_pixels_to_blocks(dst->pitch, dst->blk_w);
+   const unsigned src_slice_pitch = radv_sdma_pixel_area_to_blocks(src->slice_pitch, src->blk_w, src->blk_h);
+   const unsigned dst_slice_pitch = radv_sdma_pixel_area_to_blocks(dst->slice_pitch, dst->blk_w, dst->blk_h);
+   const bool tmz = false;
+
+   assert(src->bpp == dst->bpp);
+   assert(util_is_power_of_two_nonzero(src->bpp));
+   radv_sdma_check_offset(src_off);
+   radv_sdma_check_offset(dst_off);
+   radv_sdma_check_extent(ext);
+   radv_sdma_check_pitches(src->pitch, src->slice_pitch, src->bpp, false);
+   radv_sdma_check_pitches(dst->pitch, dst->slice_pitch, dst->bpp, false);
+
+   ASSERTED unsigned cdw_end = radeon_check_space(device->ws, cs, 13);
+
+   radeon_emit(cs, CIK_SDMA_PACKET(CIK_SDMA_OPCODE_COPY, CIK_SDMA_COPY_SUB_OPCODE_LINEAR_SUB_WINDOW, 0) | tmz << 18 |
+                      util_logbase2(src->bpp) << 29);
+   radeon_emit(cs, src->va);
+   radeon_emit(cs, src->va >> 32);
+   radeon_emit(cs, src_off.x | src_off.y << 16);
+   radeon_emit(cs, src_off.z | (src_pitch - 1) << 13);
+   radeon_emit(cs, src_slice_pitch - 1);
+   radeon_emit(cs, dst->va);
+   radeon_emit(cs, dst->va >> 32);
+   radeon_emit(cs, dst_off.x | dst_off.y << 16);
+   radeon_emit(cs, dst_off.z | (dst_pitch - 1) << 13);
+   radeon_emit(cs, dst_slice_pitch - 1);
+   radeon_emit(cs, (ext.width - 1) | (ext.height - 1) << 16);
+   radeon_emit(cs, (ext.depth - 1));
+
+   assert(cs->cdw == cdw_end);
+}
+
+static void
+radv_sdma_emit_copy_tiled_sub_window(const struct radv_device *device, struct radeon_cmdbuf *cs,
+                                     const struct radv_sdma_tiled_info *const tiled, const struct radv_sdma_linear_info *const linear,
+                                     const VkOffset3D tiled_pix_offset, const VkOffset3D linear_pix_offset,
+                                     const VkExtent3D pix_extent, const bool detile)
+{
+   /* This packet has significant differences between SDMA v2.4, v4 and v5.
+    * We currently only support the SDMA v4.0+ versions.
+    */
+
+   assert(device->physical_device->rad_info.gfx_level >= GFX9);
+
+   if (device->physical_device->rad_info.gfx_level == GFX9) {
+      /* SDMA v4 doesn't support any image metadata. */
+      assert(!tiled->meta_va);
+   }
+
+   const VkOffset3D linear_off = radv_sdma_pixel_offset_to_blocks(linear_pix_offset, linear->blk_w, linear->blk_h);
+   const VkOffset3D tiled_off = radv_sdma_pixel_offset_to_blocks(tiled_pix_offset, tiled->blk_w, tiled->blk_h);
+   const VkExtent3D tiled_ext = radv_sdma_pixel_extent_to_blocks(tiled->extent, tiled->blk_w, tiled->blk_h);
+   const VkExtent3D ext = radv_sdma_pixel_extent_to_blocks(pix_extent, tiled->blk_w, tiled->blk_h);
+   const unsigned linear_pitch = radv_sdma_pixels_to_blocks(linear->pitch, tiled->blk_w);
+   const unsigned linear_slice_pitch = radv_sdma_pixel_area_to_blocks(linear->slice_pitch, tiled->blk_w, tiled->blk_h);
+   const bool tmz = false;
+   const bool dcc = !!tiled->meta_va;
+   const bool uses_depth = linear_off.z != 0 || tiled_off.z != 0 || ext.depth != 1;
+
+   assert(util_is_power_of_two_nonzero(tiled->bpp));
+   radv_sdma_check_offset(linear_off);
+   radv_sdma_check_offset(tiled_off);
+   radv_sdma_check_extent(tiled_ext);
+   radv_sdma_check_extent(ext);
+   radv_sdma_check_pitches(linear_pitch, linear_slice_pitch, tiled->bpp, uses_depth);
+
+   ASSERTED unsigned cdw_end = radeon_check_space(device->ws, cs, 14 + (dcc ? 3 : 0));
+
+   radeon_emit(cs, CIK_SDMA_PACKET(CIK_SDMA_OPCODE_COPY, CIK_SDMA_COPY_SUB_OPCODE_TILED_SUB_WINDOW, 0) | tmz << 18 |
+                      dcc << 19 | detile << 31 | tiled->header_dword);
+   radeon_emit(cs, tiled->va);
+   radeon_emit(cs, tiled->va >> 32);
+   radeon_emit(cs, tiled_off.x | tiled_off.y << 16);
+   radeon_emit(cs, tiled_off.z | (tiled_ext.width - 1) << 16);
+   radeon_emit(cs, (tiled_ext.height - 1) | (tiled_ext.depth - 1) << 16);
+   radeon_emit(cs, tiled->info_dword);
+   radeon_emit(cs, linear->va);
+   radeon_emit(cs, linear->va >> 32);
+   radeon_emit(cs, linear_off.x | linear_off.y << 16);
+   radeon_emit(cs, linear_off.z | (linear_pitch - 1) << 16);
+   radeon_emit(cs, linear_slice_pitch - 1);
+   radeon_emit(cs, (ext.width - 1) | (ext.height - 1) << 16);
+   radeon_emit(cs, (ext.depth - 1));
+
+   if (tiled->meta_va) {
+      const unsigned write_compress_enable = !detile;
+      radeon_emit(cs, tiled->meta_va);
+      radeon_emit(cs, tiled->meta_va >> 32);
+      radeon_emit(cs, tiled->meta_config | write_compress_enable << 28);
+   }
+
+   assert(cs->cdw == cdw_end);
+}
+
+void
+radv_sdma_copy_buffer_image(const struct radv_device *device, struct radeon_cmdbuf *cs, struct radv_image *image,
+                            struct radv_buffer *buffer, const VkBufferImageCopy2 *region, bool to_image)
+{
+   const struct radv_sdma_linear_info buf_info = radv_sdma_get_linear_buf_info(buffer, image, region);
+   const VkExtent3D extent = radv_sdma_get_copy_extent(image, region->imageSubresource, region->imageExtent);
+   const VkOffset3D img_offset = radv_sdma_get_img_offset(image, region->imageSubresource, region->imageOffset);
+   const VkOffset3D zero_offset = {0};
+
+   if (image->planes[0].surface.is_linear) {
+      const struct radv_sdma_linear_info linear = radv_sdma_get_linear_img_info(image, region->imageSubresource);
+
+      if (to_image)
+         radv_sdma_emit_copy_linear_sub_window(device, cs, &buf_info, &linear, zero_offset, img_offset, extent);
+      else
+         radv_sdma_emit_copy_linear_sub_window(device, cs, &linear, &buf_info, img_offset, zero_offset, extent);
+   } else {
+      const struct radv_sdma_tiled_info tiled = radv_sdma_get_tiled_img_info(device, image, region->imageSubresource);
+      radv_sdma_emit_copy_tiled_sub_window(device, cs, &tiled, &buf_info, img_offset, zero_offset, extent, !to_image);
+   }
+}
-- 
GitLab


From 46a7718a3e958ea5df62c4ea0b617120f2c3086c Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Thu, 5 Oct 2023 13:35:58 +0200
Subject: [PATCH 2/3] radv: Add temporary BO for transfer queues.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Some copy operations are poorly supported by the SDMA hardware,
meaning that the built-in packets don't support them, so we will
need to work around that by copying to and from a temporary BO.

The size of the temporary buffer was chosen so that it can fit
at least one full pixel row of the largest possible image.

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
Reviewed-by: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
---
 src/amd/vulkan/meta/radv_meta_copy.c | 20 ++++++++++++++++++++
 src/amd/vulkan/radv_cmd_buffer.c     |  2 ++
 src/amd/vulkan/radv_constants.h      |  5 +++++
 src/amd/vulkan/radv_private.h        |  5 +++++
 4 files changed, 32 insertions(+)

diff --git a/src/amd/vulkan/meta/radv_meta_copy.c b/src/amd/vulkan/meta/radv_meta_copy.c
index 67f5d31472b62..31f5b1fa5a273 100644
--- a/src/amd/vulkan/meta/radv_meta_copy.c
+++ b/src/amd/vulkan/meta/radv_meta_copy.c
@@ -85,6 +85,26 @@ radv_image_is_renderable(const struct radv_device *device, const struct radv_ima
    return true;
 }
 
+static bool
+alloc_transfer_temp_bo(struct radv_cmd_buffer *cmd_buffer)
+{
+   if (cmd_buffer->transfer.copy_temp)
+      return true;
+
+   const struct radv_device *const device = cmd_buffer->device;
+   const VkResult r = device->ws->buffer_create(device->ws, RADV_SDMA_TRANSFER_TEMP_BYTES, 4096, RADEON_DOMAIN_VRAM,
+                                                RADEON_FLAG_NO_CPU_ACCESS | RADEON_FLAG_NO_INTERPROCESS_SHARING,
+                                                RADV_BO_PRIORITY_SCRATCH, 0, &cmd_buffer->transfer.copy_temp);
+
+   if (r != VK_SUCCESS) {
+      vk_command_buffer_set_error(&cmd_buffer->vk, r);
+      return false;
+   }
+
+   radv_cs_add_buffer(device->ws, cmd_buffer->cs, cmd_buffer->transfer.copy_temp);
+   return true;
+}
+
 static void
 copy_buffer_to_image(struct radv_cmd_buffer *cmd_buffer, struct radv_buffer *buffer, struct radv_image *image,
                      VkImageLayout layout, const VkBufferImageCopy2 *region)
diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index a5a905adac25c..61f18d4cd310e 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -312,6 +312,8 @@ radv_destroy_cmd_buffer(struct vk_command_buffer *vk_cmd_buffer)
       cmd_buffer->device->ws->cs_destroy(cmd_buffer->cs);
    if (cmd_buffer->gang.cs)
       cmd_buffer->device->ws->cs_destroy(cmd_buffer->gang.cs);
+   if (cmd_buffer->transfer.copy_temp)
+      cmd_buffer->device->ws->buffer_destroy(cmd_buffer->device->ws, cmd_buffer->transfer.copy_temp);
 
    for (unsigned i = 0; i < MAX_BIND_POINTS; i++) {
       struct radv_descriptor_set_header *set = &cmd_buffer->descriptors[i].push_set.set;
diff --git a/src/amd/vulkan/radv_constants.h b/src/amd/vulkan/radv_constants.h
index 4178138ce11ee..796a1c87f63cb 100644
--- a/src/amd/vulkan/radv_constants.h
+++ b/src/amd/vulkan/radv_constants.h
@@ -166,4 +166,9 @@
 /* Number of samples for line smooth lowering (hw requirement). */
 #define RADV_NUM_SMOOTH_AA_SAMPLES 4
 
+/* Size of the temporary buffer allocated for transfer queue copy command workarounds.
+ * The size is chosen so that it can fit two lines of (1 << 14) blocks at 16 bpp.
+ */
+#define RADV_SDMA_TRANSFER_TEMP_BYTES (2 * (1 << 14) * 16)
+
 #endif /* RADV_CONSTANTS_H */
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index ad66229ad3846..6894406f2810a 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -1871,6 +1871,11 @@ struct radv_cmd_buffer {
       struct rvcn_decode_buffer_s *decode_buffer;
    } video;
 
+   struct {
+      /* Temporary space for some transfer queue copy command workarounds. */
+      struct radeon_winsys_bo *copy_temp;
+   } transfer;
+
    uint64_t shader_upload_seq;
 
    uint32_t sqtt_cb_id;
-- 
GitLab


From b987081534810d86ed76a432420babfcc3a7597e Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Fri, 6 Oct 2023 01:29:01 +0200
Subject: [PATCH 3/3] radv: Implement workaround for unaligned buffer/image
 copies.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

When the pitch or slice pitch isn't properly aligned,
the SDMA HW is unable to copy between tiled images and buffers.

To work around this, we process the image chunk by chunk,
copying the data to a temporary buffer which uses supported
pitches, and then copy it to the intended destination.

The implementation assumes that at least one pixel row of the
image will fit into the temporary buffer, and will try to copy
as many rows at once as possible. Sadly, this still results in
a lot of packets being generated for large images.

A possibe future improvement is to copy the image slice by slice
when only the slice pitch is misaligned. However, that is out
of scope for this commit.

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
Reviewed-by: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
---
 src/amd/vulkan/meta/radv_meta_copy.c |  19 ++++
 src/amd/vulkan/radv_private.h        |   6 +
 src/amd/vulkan/radv_sdma.c           | 162 +++++++++++++++++++++++++++
 3 files changed, 187 insertions(+)

diff --git a/src/amd/vulkan/meta/radv_meta_copy.c b/src/amd/vulkan/meta/radv_meta_copy.c
index 31f5b1fa5a273..fdde69f33b279 100644
--- a/src/amd/vulkan/meta/radv_meta_copy.c
+++ b/src/amd/vulkan/meta/radv_meta_copy.c
@@ -115,6 +115,15 @@ copy_buffer_to_image(struct radv_cmd_buffer *cmd_buffer, struct radv_buffer *buf
       struct radeon_cmdbuf *cs = cmd_buffer->cs;
       radv_cs_add_buffer(device->ws, cs, image->bindings[0].bo);
       radv_cs_add_buffer(device->ws, cs, buffer->bo);
+
+      if (radv_sdma_use_unaligned_buffer_image_copy(device, image, buffer, region)) {
+         if (!alloc_transfer_temp_bo(cmd_buffer))
+            return;
+
+         radv_sdma_copy_buffer_image_unaligned(device, cs, image, buffer, region, cmd_buffer->transfer.copy_temp, true);
+         return;
+      }
+
       radv_sdma_copy_buffer_image(device, cs, image, buffer, region, true);
       return;
    }
@@ -269,6 +278,16 @@ copy_image_to_buffer(struct radv_cmd_buffer *cmd_buffer, struct radv_buffer *buf
       struct radeon_cmdbuf *cs = cmd_buffer->cs;
       radv_cs_add_buffer(device->ws, cs, image->bindings[0].bo);
       radv_cs_add_buffer(device->ws, cs, buffer->bo);
+
+      if (radv_sdma_use_unaligned_buffer_image_copy(device, image, buffer, region)) {
+         if (!alloc_transfer_temp_bo(cmd_buffer))
+            return;
+
+         radv_sdma_copy_buffer_image_unaligned(device, cs, image, buffer, region, cmd_buffer->transfer.copy_temp,
+                                               false);
+         return;
+      }
+
       radv_sdma_copy_buffer_image(device, cs, image, buffer, region, false);
       return;
    }
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index 6894406f2810a..e6de5b7784f95 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -3142,6 +3142,12 @@ void radv_rra_trace_finish(VkDevice vk_device, struct radv_rra_trace_data *data)
 
 void radv_sdma_copy_buffer_image(const struct radv_device *device, struct radeon_cmdbuf *cs, struct radv_image *image,
                                  struct radv_buffer *buffer, const VkBufferImageCopy2 *region, bool to_image);
+bool radv_sdma_use_unaligned_buffer_image_copy(const struct radv_device *device, const struct radv_image *image,
+                                               const struct radv_buffer *buffer, const VkBufferImageCopy2 *region);
+void radv_sdma_copy_buffer_image_unaligned(const struct radv_device *device, struct radeon_cmdbuf *cs,
+                                           struct radv_image *image, struct radv_buffer *buffer,
+                                           const VkBufferImageCopy2 *region, struct radeon_winsys_bo *temp_bo,
+                                           bool to_image);
 void radv_sdma_copy_buffer(const struct radv_device *device, struct radeon_cmdbuf *cs, uint64_t src_va, uint64_t dst_va,
                            uint64_t size);
 
diff --git a/src/amd/vulkan/radv_sdma.c b/src/amd/vulkan/radv_sdma.c
index 23d82f7f322d8..1eaefd7a6adcf 100644
--- a/src/amd/vulkan/radv_sdma.c
+++ b/src/amd/vulkan/radv_sdma.c
@@ -49,6 +49,17 @@ struct radv_sdma_tiled_info {
    unsigned blk_h;
 };
 
+struct radv_sdma_chunked_copy_info {
+   unsigned bpp;
+   unsigned blk_w;
+   unsigned blk_h;
+   unsigned row_pitch_alignment;
+   unsigned extent_horizontal_blocks;
+   unsigned extent_vertical_blocks;
+   unsigned aligned_row_pitch;
+   unsigned num_rows_per_copy;
+};
+
 ALWAYS_INLINE static void
 radv_sdma_check_extent(const VkExtent3D extent)
 {
@@ -217,6 +228,42 @@ radv_sdma_pixel_area_to_blocks(const unsigned linear_slice_pitch, const unsigned
    return DIV_ROUND_UP(DIV_ROUND_UP(linear_slice_pitch, blk_w), blk_h);
 }
 
+static struct radv_sdma_chunked_copy_info
+radv_sdma_get_chunked_copy_info(const struct radv_device *const device, const struct radv_image *const image,
+                                const VkExtent3D extent)
+{
+   const struct radeon_surf *const surf = &image->planes[0].surface;
+
+   const unsigned bpp = surf->bpe;
+   const unsigned blk_w = surf->blk_w;
+   const unsigned blk_h = surf->blk_h;
+   const unsigned row_pitch_alignment = 4;
+   const unsigned extent_horizontal_blocks = DIV_ROUND_UP(extent.width, blk_w);
+   const unsigned extent_vertical_blocks = DIV_ROUND_UP(extent.height, blk_h);
+   const unsigned aligned_row_pitch = ALIGN(extent_horizontal_blocks, row_pitch_alignment);
+   const unsigned aligned_row_bytes = aligned_row_pitch * bpp;
+
+   /* Assume that we can always copy at least one full row at a time. */
+   const unsigned max_num_rows_per_copy = MIN2(RADV_SDMA_TRANSFER_TEMP_BYTES / aligned_row_bytes, extent.height);
+   assert(max_num_rows_per_copy);
+
+   /* Ensure that the number of rows copied at a time is a power of two. */
+   const unsigned num_rows_per_copy = MAX2(1, util_next_power_of_two(max_num_rows_per_copy + 1) / 2);
+
+   const struct radv_sdma_chunked_copy_info r = {
+      .bpp = bpp,
+      .blk_w = blk_w,
+      .blk_h = blk_h,
+      .row_pitch_alignment = row_pitch_alignment,
+      .extent_horizontal_blocks = extent_horizontal_blocks,
+      .extent_vertical_blocks = extent_vertical_blocks,
+      .aligned_row_pitch = aligned_row_pitch,
+      .num_rows_per_copy = num_rows_per_copy,
+   };
+
+   return r;
+}
+
 static struct radv_sdma_linear_info
 radv_sdma_get_linear_buf_info(const struct radv_buffer *const buffer, const struct radv_image *const image,
                               const VkBufferImageCopy2 *const region)
@@ -364,6 +411,14 @@ radv_sdma_get_tiled_img_info(const struct radv_device *const device, const struc
    return info;
 }
 
+static void
+radv_sdma_emit_nop(const struct radv_device *device, struct radeon_cmdbuf *cs)
+{
+   /* SDMA NOP acts as a fence command and causes the SDMA engine to wait for pending copy operations. */
+   radeon_check_space(device->ws, cs, 1);
+   radeon_emit(cs, CIK_SDMA_PACKET(CIK_SDMA_OPCODE_NOP, 0, 0));
+}
+
 void
 radv_sdma_copy_buffer(const struct radv_device *device, struct radeon_cmdbuf *cs, uint64_t src_va, uint64_t dst_va,
                       uint64_t size)
@@ -546,3 +601,110 @@ radv_sdma_copy_buffer_image(const struct radv_device *device, struct radeon_cmdb
       radv_sdma_emit_copy_tiled_sub_window(device, cs, &tiled, &buf_info, img_offset, zero_offset, extent, !to_image);
    }
 }
+
+bool
+radv_sdma_use_unaligned_buffer_image_copy(const struct radv_device *device, const struct radv_image *image,
+                                          const struct radv_buffer *buffer, const VkBufferImageCopy2 *region)
+{
+   const struct radeon_surf *const surf = &image->planes[0].surface;
+   const enum amd_gfx_level gfx_level = device->physical_device->rad_info.gfx_level;
+   const unsigned pitch_alignment = gfx_level >= GFX10 ? MAX2(1, 4 / surf->bpe) : 4;
+   const unsigned pitch = (region->bufferRowLength ? region->bufferRowLength : region->imageExtent.width);
+   const unsigned pitch_blocks = radv_sdma_pixels_to_blocks(pitch, surf->blk_w);
+
+   if (!radv_is_aligned(pitch_blocks, pitch_alignment))
+      return true;
+
+   const unsigned offset_z = radv_sdma_get_img_offset_z(image, region->imageSubresource, region->imageOffset);
+   const unsigned extent_depth = radv_sdma_get_copy_extent_depth(image, region->imageSubresource, region->imageExtent);
+   const bool uses_depth = offset_z != 0 || extent_depth != 1;
+   if (!surf->is_linear && uses_depth) {
+      const unsigned slice_pitch =
+         (region->bufferImageHeight ? region->bufferImageHeight : region->imageExtent.height) * pitch;
+      const unsigned slice_pitch_blocks = radv_sdma_pixel_area_to_blocks(slice_pitch, surf->blk_w, surf->blk_h);
+
+      if (!radv_is_aligned(slice_pitch_blocks, 4))
+         return true;
+   }
+
+   return false;
+}
+
+void
+radv_sdma_copy_buffer_image_unaligned(const struct radv_device *device, struct radeon_cmdbuf *cs,
+                                      struct radv_image *image, struct radv_buffer *buffer,
+                                      const VkBufferImageCopy2 *region, struct radeon_winsys_bo *temp_bo, bool to_image)
+{
+   const bool is_linear = image->planes[0].surface.is_linear;
+   const VkOffset3D base_offset = radv_sdma_get_img_offset(image, region->imageSubresource, region->imageOffset);
+   const VkExtent3D base_extent = radv_sdma_get_copy_extent(image, region->imageSubresource, region->imageExtent);
+   const struct radv_sdma_chunked_copy_info info = radv_sdma_get_chunked_copy_info(device, image, base_extent);
+   const struct radv_sdma_linear_info buf = radv_sdma_get_linear_buf_info(buffer, image, region);
+   const struct radv_sdma_linear_info linear = radv_sdma_get_linear_img_info(image, region->imageSubresource);
+   const struct radv_sdma_tiled_info tiled = radv_sdma_get_tiled_img_info(device, image, region->imageSubresource);
+
+   struct radv_sdma_linear_info tmp = {
+      .va = temp_bo->va,
+      .bpp = info.bpp,
+      .blk_w = info.blk_w,
+      .blk_h = info.blk_h,
+      .pitch = info.aligned_row_pitch * info.blk_w,
+      .slice_pitch = info.aligned_row_pitch * info.blk_w * info.extent_vertical_blocks * info.blk_h,
+   };
+
+   const VkOffset3D zero_offset = {0};
+   VkExtent3D extent = base_extent;
+   VkOffset3D offset = base_offset;
+   const unsigned buf_pitch_blocks = DIV_ROUND_UP(buf.pitch, info.blk_w);
+   const unsigned buf_slice_pitch_blocks = DIV_ROUND_UP(DIV_ROUND_UP(buf.slice_pitch, info.blk_w), info.blk_h);
+   assert(buf_pitch_blocks);
+   assert(buf_slice_pitch_blocks);
+   extent.depth = 1;
+
+   for (unsigned slice = 0; slice < base_extent.depth; ++slice) {
+      for (unsigned row = 0; row < info.extent_vertical_blocks; row += info.num_rows_per_copy) {
+         const unsigned rows = MIN2(info.extent_vertical_blocks - row, info.num_rows_per_copy);
+
+         offset.y = base_offset.y + row * info.blk_h;
+         offset.z = base_offset.z + slice;
+         extent.height = rows * info.blk_h;
+         tmp.slice_pitch = tmp.pitch * rows * info.blk_h;
+
+         if (!to_image) {
+            /* Copy the rows from the source image to the temporary buffer. */
+            if (is_linear)
+               radv_sdma_emit_copy_linear_sub_window(device, cs, &linear, &tmp, offset, zero_offset, extent);
+            else
+               radv_sdma_emit_copy_tiled_sub_window(device, cs, &tiled, &tmp, offset, zero_offset, extent, true);
+
+            /* Wait for the copy to finish. */
+            radv_sdma_emit_nop(device, cs);
+         }
+
+         /* buffer to image: copy each row from source buffer to temporary buffer.
+          * image to buffer: copy each row from temporary buffer to destination buffer.
+          */
+         for (unsigned r = 0; r < rows; ++r) {
+            const uint64_t buf_va =
+               buf.va + slice * buf_slice_pitch_blocks * info.bpp + (row + r) * buf_pitch_blocks * info.bpp;
+            const uint64_t tmp_va = tmp.va + r * info.aligned_row_pitch * info.bpp;
+            radv_sdma_copy_buffer(device, cs, to_image ? buf_va : tmp_va, to_image ? tmp_va : buf_va,
+                                  info.extent_horizontal_blocks * info.bpp);
+         }
+
+         /* Wait for the copy to finish. */
+         radv_sdma_emit_nop(device, cs);
+
+         if (to_image) {
+            /* Copy the rows from the temporary buffer to the destination image. */
+            if (is_linear)
+               radv_sdma_emit_copy_linear_sub_window(device, cs, &tmp, &linear, zero_offset, offset, extent);
+            else
+               radv_sdma_emit_copy_tiled_sub_window(device, cs, &tiled, &tmp, offset, zero_offset, extent, false);
+
+            /* Wait for the copy to finish. */
+            radv_sdma_emit_nop(device, cs);
+         }
+      }
+   }
+}
-- 
GitLab

