From 82efff9a4716419ea118b5dc7ef9088dbf0ecf76 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 15 Jun 2024 00:16:08 -0400
Subject: [PATCH 1/6] nir/opt_load_store_vectorize: add entry::num_components

We will represent vec6..vec7, vec9..vec15 loads with 8 and 16
components respectively, so we need to track how many components
we really use.

This is a prerequisite for optimal merging up to vec16. Example:
    Step 1: vec4 + vec3 ==> vec7as8 (last component unused)
    Step 2: vec1 + vec7as8 ==> vec8 (last unused component dropped)

Without using the number of components read, the same example would end up
doing:
    Step 1: vec4 + vec3 ==> vec8
    Step 2: vec1 + vec8 ==> vec9 (fail)
---
 .../nir/nir_opt_load_store_vectorize.c        | 30 +++++++++++--------
 1 file changed, 17 insertions(+), 13 deletions(-)

diff --git a/src/compiler/nir/nir_opt_load_store_vectorize.c b/src/compiler/nir/nir_opt_load_store_vectorize.c
index 29c4beae3aed1..2c16e22e3eda7 100644
--- a/src/compiler/nir/nir_opt_load_store_vectorize.c
+++ b/src/compiler/nir/nir_opt_load_store_vectorize.c
@@ -144,6 +144,7 @@ struct entry {
 
    nir_instr *instr;
    nir_intrinsic_instr *intrin;
+   unsigned num_components;
    const struct intrinsic_info *info;
    enum gl_access_qualifier access;
    bool is_store;
@@ -535,6 +536,7 @@ create_entry(void *mem_ctx,
    entry->instr = &intrin->instr;
    entry->info = info;
    entry->is_store = entry->info->value_src >= 0;
+   entry->num_components = intrin->num_components;
 
    if (entry->info->deref_src >= 0) {
       entry->deref = nir_src_as_deref(intrin->src[entry->info->deref_src]);
@@ -628,8 +630,8 @@ new_bitsize_acceptable(struct vectorize_ctx *ctx, unsigned new_bit_size,
       return false;
 
    if (low->is_store) {
-      unsigned low_size = low->intrin->num_components * get_bit_size(low);
-      unsigned high_size = high->intrin->num_components * get_bit_size(high);
+      unsigned low_size = low->num_components * get_bit_size(low);
+      unsigned high_size = high->num_components * get_bit_size(high);
 
       if (low_size % new_bit_size != 0)
          return false;
@@ -717,6 +719,7 @@ vectorize_loads(nir_builder *b, struct vectorize_ctx *ctx,
 
    /* update the intrinsic */
    first->intrin->num_components = new_num_components;
+   first->num_components = util_last_bit(nir_def_components_read(data));
 
    const struct intrinsic_info *info = first->info;
 
@@ -775,7 +778,7 @@ vectorize_stores(nir_builder *b, struct vectorize_ctx *ctx,
                  unsigned new_bit_size, unsigned new_num_components,
                  unsigned high_start)
 {
-   ASSERTED unsigned low_size = low->intrin->num_components * get_bit_size(low);
+   ASSERTED unsigned low_size = low->num_components * get_bit_size(low);
    assert(low_size % new_bit_size == 0);
 
    b->cursor = nir_before_instr(second->instr);
@@ -821,6 +824,7 @@ vectorize_stores(nir_builder *b, struct vectorize_ctx *ctx,
    /* update the intrinsic */
    nir_intrinsic_set_write_mask(second->intrin, write_mask);
    second->intrin->num_components = data->num_components;
+   second->num_components = data->num_components;
 
    const struct intrinsic_info *info = second->info;
    assert(info->value_src >= 0);
@@ -940,11 +944,11 @@ may_alias(nir_shader *shader, struct entry *a, struct entry *b)
    /* TODO: we can look closer at the entry keys */
    int64_t diff = compare_entries(a, b);
    if (diff != INT64_MAX) {
-      /* with atomics, intrin->num_components can be 0 */
+      /* with atomics, nir_intrinsic_instr::num_components can be 0 */
       if (diff < 0)
-         return llabs(diff) < MAX2(b->intrin->num_components, 1u) * (get_bit_size(b) / 8u);
+         return llabs(diff) < MAX2(b->num_components, 1u) * (get_bit_size(b) / 8u);
       else
-         return diff < MAX2(a->intrin->num_components, 1u) * (get_bit_size(a) / 8u);
+         return diff < MAX2(a->num_components, 1u) * (get_bit_size(a) / 8u);
    }
 
    /* TODO: we can use deref information */
@@ -1105,8 +1109,8 @@ try_vectorize(nir_function_impl *impl, struct vectorize_ctx *ctx,
    /* gather information */
    unsigned low_bit_size = get_bit_size(low);
    unsigned high_bit_size = get_bit_size(high);
-   unsigned low_size = low->intrin->num_components * low_bit_size;
-   unsigned high_size = high->intrin->num_components * high_bit_size;
+   unsigned low_size = low->num_components * low_bit_size;
+   unsigned high_size = high->num_components * high_bit_size;
    unsigned new_size = MAX2(diff * 8u + high_size, low_size);
 
    /* find a good bit size for the new load/store */
@@ -1153,8 +1157,8 @@ try_vectorize_shared2(struct vectorize_ctx *ctx,
 
    unsigned low_bit_size = get_bit_size(low);
    unsigned high_bit_size = get_bit_size(high);
-   unsigned low_size = low->intrin->num_components * low_bit_size / 8;
-   unsigned high_size = high->intrin->num_components * high_bit_size / 8;
+   unsigned low_size = low->num_components * low_bit_size / 8;
+   unsigned high_size = high->num_components * high_bit_size / 8;
    if ((low_size != 4 && low_size != 8) || (high_size != 4 && high_size != 8))
       return false;
    if (low_size != high_size)
@@ -1175,9 +1179,9 @@ try_vectorize_shared2(struct vectorize_ctx *ctx,
       return false;
 
    if (first->is_store) {
-      if (nir_intrinsic_write_mask(low->intrin) != BITFIELD_MASK(low->intrin->num_components))
+      if (nir_intrinsic_write_mask(low->intrin) != BITFIELD_MASK(low->num_components))
          return false;
-      if (nir_intrinsic_write_mask(high->intrin) != BITFIELD_MASK(high->intrin->num_components))
+      if (nir_intrinsic_write_mask(high->intrin) != BITFIELD_MASK(high->num_components))
          return false;
    }
 
@@ -1243,7 +1247,7 @@ vectorize_sorted_entries(struct vectorize_ctx *ctx, nir_function_impl *impl,
          struct entry *second = low->index < high->index ? high : low;
 
          uint64_t diff = high->offset_signed - low->offset_signed;
-         bool separate = diff > get_bit_size(low) / 8u * low->intrin->num_components;
+         bool separate = diff > get_bit_size(low) / 8u * low->num_components;
          if (separate) {
             if (!ctx->options->has_shared2_amd ||
                 get_variable_mode(first) != nir_var_mem_shared)
-- 
GitLab


From 6444289a44b926574035f155ecb1e09424a3cee7 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 15 Jun 2024 00:36:12 -0400
Subject: [PATCH 2/6] nir: add hole_size parameter into the vectorize callback

It will be used to allow merging loads with a hole between them.

Reviewed-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/common/ac_shader_util.c                        | 4 ++--
 src/amd/common/ac_shader_util.h                        | 4 ++--
 src/asahi/compiler/agx_compile.c                       | 7 +++++--
 src/broadcom/compiler/nir_to_vir.c                     | 4 ++++
 src/compiler/nir/nir.h                                 | 3 +++
 src/compiler/nir/nir_lower_shader_calls.c              | 3 ++-
 src/compiler/nir/nir_opt_load_store_vectorize.c        | 2 +-
 src/compiler/nir/tests/load_store_vectorizer_tests.cpp | 7 +++++--
 src/freedreno/ir3/ir3_nir.c                            | 5 ++++-
 src/gallium/auxiliary/nir/nir_to_tgsi.c                | 4 +++-
 src/gallium/drivers/r300/compiler/r300_nir.c           | 7 ++++---
 src/intel/compiler/brw_nir.c                           | 3 ++-
 src/intel/compiler/elk/elk_nir.c                       | 3 ++-
 src/microsoft/compiler/nir_to_dxil.c                   | 3 ++-
 src/nouveau/compiler/nak_nir.c                         | 3 +++
 15 files changed, 44 insertions(+), 18 deletions(-)

diff --git a/src/amd/common/ac_shader_util.c b/src/amd/common/ac_shader_util.c
index 3fbc19425753d..17681434a0730 100644
--- a/src/amd/common/ac_shader_util.c
+++ b/src/amd/common/ac_shader_util.c
@@ -97,10 +97,10 @@ void ac_set_nir_options(struct radeon_info *info, bool use_llvm,
 
 bool
 ac_nir_mem_vectorize_callback(unsigned align_mul, unsigned align_offset, unsigned bit_size,
-                              unsigned num_components, nir_intrinsic_instr *low,
+                              unsigned num_components, unsigned hole_size, nir_intrinsic_instr *low,
                               nir_intrinsic_instr *high, void *data)
 {
-   if (num_components > 4)
+   if (num_components > 4 || hole_size)
       return false;
 
    bool is_scratch = false;
diff --git a/src/amd/common/ac_shader_util.h b/src/amd/common/ac_shader_util.h
index d8b6e9b78c03a..d2b024bedef6f 100644
--- a/src/amd/common/ac_shader_util.h
+++ b/src/amd/common/ac_shader_util.h
@@ -243,8 +243,8 @@ void ac_set_nir_options(struct radeon_info *info, bool use_llvm,
                         nir_shader_compiler_options *options);
 
 bool ac_nir_mem_vectorize_callback(unsigned align_mul, unsigned align_offset, unsigned bit_size,
-                                   unsigned num_components, nir_intrinsic_instr *low,
-                                   nir_intrinsic_instr *high, void *data);
+                                   unsigned num_components, unsigned hole_size,
+                                   nir_intrinsic_instr *low, nir_intrinsic_instr *high, void *data);
 
 unsigned ac_get_spi_shader_z_format(bool writes_z, bool writes_stencil, bool writes_samplemask,
                                     bool writes_mrt0_alpha);
diff --git a/src/asahi/compiler/agx_compile.c b/src/asahi/compiler/agx_compile.c
index 0ab832fa57f55..a4c2a905bc2b7 100644
--- a/src/asahi/compiler/agx_compile.c
+++ b/src/asahi/compiler/agx_compile.c
@@ -2667,9 +2667,12 @@ agx_optimize_loop_nir(nir_shader *nir)
 
 static bool
 mem_vectorize_cb(unsigned align_mul, unsigned align_offset, unsigned bit_size,
-                 unsigned num_components, nir_intrinsic_instr *low,
-                 nir_intrinsic_instr *high, void *data)
+                 unsigned num_components, unsigned hole_size,
+                 nir_intrinsic_instr *low, nir_intrinsic_instr *high, void *data)
 {
+   if (hole_size)
+      return false;
+
    /* Must be aligned to the size of the load */
    unsigned align = nir_combined_align(align_mul, align_offset);
    if ((bit_size / 8) > align)
diff --git a/src/broadcom/compiler/nir_to_vir.c b/src/broadcom/compiler/nir_to_vir.c
index 0b2f7c5455e1e..c2cb94e79448b 100644
--- a/src/broadcom/compiler/nir_to_vir.c
+++ b/src/broadcom/compiler/nir_to_vir.c
@@ -2025,10 +2025,14 @@ static bool
 mem_vectorize_callback(unsigned align_mul, unsigned align_offset,
                        unsigned bit_size,
                        unsigned num_components,
+                       unsigned hole_size,
                        nir_intrinsic_instr *low,
                        nir_intrinsic_instr *high,
                        void *data)
 {
+        if (hole_size)
+                return false;
+
         /* TMU general access only supports 32-bit vectors */
         if (bit_size > 32)
                 return false;
diff --git a/src/compiler/nir/nir.h b/src/compiler/nir/nir.h
index c8f8a18c661c1..66f9ddfbee028 100644
--- a/src/compiler/nir/nir.h
+++ b/src/compiler/nir/nir.h
@@ -5671,6 +5671,9 @@ typedef bool (*nir_should_vectorize_mem_func)(unsigned align_mul,
                                               unsigned align_offset,
                                               unsigned bit_size,
                                               unsigned num_components,
+                                              /* The hole between low and
+                                               * high if they are not adjacent. */
+                                              unsigned hole_size,
                                               nir_intrinsic_instr *low,
                                               nir_intrinsic_instr *high,
                                               void *data);
diff --git a/src/compiler/nir/nir_lower_shader_calls.c b/src/compiler/nir/nir_lower_shader_calls.c
index 624ede07a9f10..275aadec3a8a4 100644
--- a/src/compiler/nir/nir_lower_shader_calls.c
+++ b/src/compiler/nir/nir_lower_shader_calls.c
@@ -1930,6 +1930,7 @@ should_vectorize(unsigned align_mul,
                  unsigned align_offset,
                  unsigned bit_size,
                  unsigned num_components,
+                 unsigned hole_size,
                  nir_intrinsic_instr *low, nir_intrinsic_instr *high,
                  void *data)
 {
@@ -1943,7 +1944,7 @@ should_vectorize(unsigned align_mul,
    struct stack_op_vectorizer_state *state = data;
 
    return state->driver_callback(align_mul, align_offset,
-                                 bit_size, num_components,
+                                 bit_size, num_components, hole_size,
                                  low, high, state->driver_data);
 }
 
diff --git a/src/compiler/nir/nir_opt_load_store_vectorize.c b/src/compiler/nir/nir_opt_load_store_vectorize.c
index 2c16e22e3eda7..632f763d81647 100644
--- a/src/compiler/nir/nir_opt_load_store_vectorize.c
+++ b/src/compiler/nir/nir_opt_load_store_vectorize.c
@@ -624,7 +624,7 @@ new_bitsize_acceptable(struct vectorize_ctx *ctx, unsigned new_bit_size,
 
    if (!ctx->options->callback(low->align_mul,
                                low->align_offset,
-                               new_bit_size, new_num_components,
+                               new_bit_size, new_num_components, 0,
                                low->intrin, high->intrin,
                                ctx->options->cb_data))
       return false;
diff --git a/src/compiler/nir/tests/load_store_vectorizer_tests.cpp b/src/compiler/nir/tests/load_store_vectorizer_tests.cpp
index ff707f7a6d289..72db7ad0142dc 100644
--- a/src/compiler/nir/tests/load_store_vectorizer_tests.cpp
+++ b/src/compiler/nir/tests/load_store_vectorizer_tests.cpp
@@ -71,7 +71,7 @@ protected:
 
    static bool mem_vectorize_callback(unsigned align_mul, unsigned align_offset,
                                       unsigned bit_size,
-                                      unsigned num_components,
+                                      unsigned num_components, unsigned hole_size,
                                       nir_intrinsic_instr *low, nir_intrinsic_instr *high,
                                       void *data);
    static void shared_type_info(const struct glsl_type *type, unsigned *size, unsigned *align);
@@ -336,10 +336,13 @@ bool nir_load_store_vectorize_test::test_alu_def(
 
 bool nir_load_store_vectorize_test::mem_vectorize_callback(
    unsigned align_mul, unsigned align_offset, unsigned bit_size,
-   unsigned num_components,
+   unsigned num_components, unsigned hole_size,
    nir_intrinsic_instr *low, nir_intrinsic_instr *high,
    void *data)
 {
+   if (hole_size)
+      return false;
+
    /* Calculate a simple alignment, like how nir_intrinsic_align() does. */
    uint32_t align = align_mul;
    if (align_offset)
diff --git a/src/freedreno/ir3/ir3_nir.c b/src/freedreno/ir3/ir3_nir.c
index a3426e86d4112..71fddc58aada1 100644
--- a/src/freedreno/ir3/ir3_nir.c
+++ b/src/freedreno/ir3/ir3_nir.c
@@ -106,9 +106,12 @@ ir3_nir_should_scalarize_mem(const nir_instr *instr, const void *data)
 static bool
 ir3_nir_should_vectorize_mem(unsigned align_mul, unsigned align_offset,
                              unsigned bit_size, unsigned num_components,
-                             nir_intrinsic_instr *low,
+                             unsigned hole_size, nir_intrinsic_instr *low,
                              nir_intrinsic_instr *high, void *data)
 {
+   if (hole_size)
+      return false;
+
    struct ir3_compiler *compiler = data;
    unsigned byte_size = bit_size / 8;
 
diff --git a/src/gallium/auxiliary/nir/nir_to_tgsi.c b/src/gallium/auxiliary/nir/nir_to_tgsi.c
index 90924630e8724..3446031854ba5 100644
--- a/src/gallium/auxiliary/nir/nir_to_tgsi.c
+++ b/src/gallium/auxiliary/nir/nir_to_tgsi.c
@@ -3256,13 +3256,15 @@ ntt_should_vectorize_instr(const nir_instr *instr, const void *data)
    return 4;
 }
 
+/* TODO: These parameters are wrong. */
 static bool
 ntt_should_vectorize_io(unsigned align, unsigned bit_size,
                         unsigned num_components, unsigned high_offset,
+                        unsigned hole_size,
                         nir_intrinsic_instr *low, nir_intrinsic_instr *high,
                         void *data)
 {
-   if (bit_size != 32)
+   if (bit_size != 32 || hole_size)
       return false;
 
    /* Our offset alignment should aways be at least 4 bytes */
diff --git a/src/gallium/drivers/r300/compiler/r300_nir.c b/src/gallium/drivers/r300/compiler/r300_nir.c
index ac52cd8cdfbf0..6201fa6f28baf 100644
--- a/src/gallium/drivers/r300/compiler/r300_nir.c
+++ b/src/gallium/drivers/r300/compiler/r300_nir.c
@@ -52,13 +52,14 @@ r300_should_vectorize_instr(const nir_instr *instr, const void *data)
    return 4;
 }
 
+/* TODO: These parameters are wrong. */
 static bool
 r300_should_vectorize_io(unsigned align, unsigned bit_size,
                         unsigned num_components, unsigned high_offset,
-                        nir_intrinsic_instr *low, nir_intrinsic_instr *high,
-                        void *data)
+                        unsigned hole_size, nir_intrinsic_instr *low,
+                        nir_intrinsic_instr *high, void *data)
 {
-   if (bit_size != 32)
+   if (bit_size != 32 || hole_size)
       return false;
 
    /* Our offset alignment should always be at least 4 bytes */
diff --git a/src/intel/compiler/brw_nir.c b/src/intel/compiler/brw_nir.c
index 40ef84d537943..7d87d09c75696 100644
--- a/src/intel/compiler/brw_nir.c
+++ b/src/intel/compiler/brw_nir.c
@@ -1387,6 +1387,7 @@ bool
 brw_nir_should_vectorize_mem(unsigned align_mul, unsigned align_offset,
                              unsigned bit_size,
                              unsigned num_components,
+                             unsigned hole_size,
                              nir_intrinsic_instr *low,
                              nir_intrinsic_instr *high,
                              void *data)
@@ -1395,7 +1396,7 @@ brw_nir_should_vectorize_mem(unsigned align_mul, unsigned align_offset,
     * those back into 32-bit ones anyway and UBO loads aren't split in NIR so
     * we don't want to make a mess for the back-end.
     */
-   if (bit_size > 32)
+   if (bit_size > 32 || hole_size)
       return false;
 
    if (low->intrinsic == nir_intrinsic_load_global_const_block_intel ||
diff --git a/src/intel/compiler/elk/elk_nir.c b/src/intel/compiler/elk/elk_nir.c
index 639f600cdaabe..5ddd1e8cd122a 100644
--- a/src/intel/compiler/elk/elk_nir.c
+++ b/src/intel/compiler/elk/elk_nir.c
@@ -1134,6 +1134,7 @@ bool
 elk_nir_should_vectorize_mem(unsigned align_mul, unsigned align_offset,
                              unsigned bit_size,
                              unsigned num_components,
+                             unsigned hole_size,
                              nir_intrinsic_instr *low,
                              nir_intrinsic_instr *high,
                              void *data)
@@ -1142,7 +1143,7 @@ elk_nir_should_vectorize_mem(unsigned align_mul, unsigned align_offset,
     * those back into 32-bit ones anyway and UBO loads aren't split in NIR so
     * we don't want to make a mess for the back-end.
     */
-   if (bit_size > 32)
+   if (bit_size > 32 || hole_size)
       return false;
 
    if (low->intrinsic == nir_intrinsic_load_global_const_block_intel ||
diff --git a/src/microsoft/compiler/nir_to_dxil.c b/src/microsoft/compiler/nir_to_dxil.c
index 6bffeee8f1ec1..afdeb61ced438 100644
--- a/src/microsoft/compiler/nir_to_dxil.c
+++ b/src/microsoft/compiler/nir_to_dxil.c
@@ -6213,10 +6213,11 @@ vectorize_filter(
    unsigned align_offset,
    unsigned bit_size,
    unsigned num_components,
+   unsigned hole_size,
    nir_intrinsic_instr *low, nir_intrinsic_instr *high,
    void *data)
 {
-   return util_is_power_of_two_nonzero(num_components);
+   return !hole_size && util_is_power_of_two_nonzero(num_components);
 }
 
 struct lower_mem_bit_sizes_data {
diff --git a/src/nouveau/compiler/nak_nir.c b/src/nouveau/compiler/nak_nir.c
index ac5eb4d47e40d..b8d27bb5fc5d5 100644
--- a/src/nouveau/compiler/nak_nir.c
+++ b/src/nouveau/compiler/nak_nir.c
@@ -802,6 +802,9 @@ nak_mem_vectorize_cb(unsigned align_mul, unsigned align_offset,
     */
    assert(util_is_power_of_two_nonzero(align_mul));
 
+   if (hole_size)
+      return false;
+
    unsigned max_bytes = 128u / 8u;
    if (low->intrinsic == nir_intrinsic_ldc_nv ||
        low->intrinsic == nir_intrinsic_ldcx_nv)
-- 
GitLab


From e9d4027e008166fe77ef236b893667463382c7e4 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 15 Jun 2024 01:11:12 -0400
Subject: [PATCH 3/6] nir: reject unsupported component counts from all
 vectorize callbacks

If you allow an unsupported component count in the callback for loads,
nir_opt_load_store_vectorize will align num_components to the next supported
vector size, essentially overfetching.

This changes all callbacks to reject it. AMD will enable it in a later commit.
---
 src/broadcom/compiler/nir_to_vir.c                     | 2 +-
 src/compiler/nir/tests/load_store_vectorizer_tests.cpp | 2 +-
 src/freedreno/ir3/ir3_nir.c                            | 2 +-
 src/gallium/auxiliary/nir/nir_to_tgsi.c                | 2 +-
 src/gallium/drivers/r300/compiler/r300_nir.c           | 2 +-
 src/intel/compiler/brw_nir.c                           | 2 +-
 src/intel/compiler/elk/elk_nir.c                       | 2 +-
 7 files changed, 7 insertions(+), 7 deletions(-)

diff --git a/src/broadcom/compiler/nir_to_vir.c b/src/broadcom/compiler/nir_to_vir.c
index c2cb94e79448b..3b9a3af6ff7ae 100644
--- a/src/broadcom/compiler/nir_to_vir.c
+++ b/src/broadcom/compiler/nir_to_vir.c
@@ -2030,7 +2030,7 @@ mem_vectorize_callback(unsigned align_mul, unsigned align_offset,
                        nir_intrinsic_instr *high,
                        void *data)
 {
-        if (hole_size)
+        if (hole_size || !nir_num_components_valid(num_components))
                 return false;
 
         /* TMU general access only supports 32-bit vectors */
diff --git a/src/compiler/nir/tests/load_store_vectorizer_tests.cpp b/src/compiler/nir/tests/load_store_vectorizer_tests.cpp
index 72db7ad0142dc..1ee699b96a4ec 100644
--- a/src/compiler/nir/tests/load_store_vectorizer_tests.cpp
+++ b/src/compiler/nir/tests/load_store_vectorizer_tests.cpp
@@ -340,7 +340,7 @@ bool nir_load_store_vectorize_test::mem_vectorize_callback(
    nir_intrinsic_instr *low, nir_intrinsic_instr *high,
    void *data)
 {
-   if (hole_size)
+   if (hole_size || !nir_num_components_valid(num_components))
       return false;
 
    /* Calculate a simple alignment, like how nir_intrinsic_align() does. */
diff --git a/src/freedreno/ir3/ir3_nir.c b/src/freedreno/ir3/ir3_nir.c
index 71fddc58aada1..73c9bb59c6040 100644
--- a/src/freedreno/ir3/ir3_nir.c
+++ b/src/freedreno/ir3/ir3_nir.c
@@ -109,7 +109,7 @@ ir3_nir_should_vectorize_mem(unsigned align_mul, unsigned align_offset,
                              unsigned hole_size, nir_intrinsic_instr *low,
                              nir_intrinsic_instr *high, void *data)
 {
-   if (hole_size)
+   if (hole_size || !nir_num_components_valid(num_components))
       return false;
 
    struct ir3_compiler *compiler = data;
diff --git a/src/gallium/auxiliary/nir/nir_to_tgsi.c b/src/gallium/auxiliary/nir/nir_to_tgsi.c
index 3446031854ba5..84bd18ab48329 100644
--- a/src/gallium/auxiliary/nir/nir_to_tgsi.c
+++ b/src/gallium/auxiliary/nir/nir_to_tgsi.c
@@ -3264,7 +3264,7 @@ ntt_should_vectorize_io(unsigned align, unsigned bit_size,
                         nir_intrinsic_instr *low, nir_intrinsic_instr *high,
                         void *data)
 {
-   if (bit_size != 32 || hole_size)
+   if (bit_size != 32 || hole_size || !nir_num_components_valid(num_components))
       return false;
 
    /* Our offset alignment should aways be at least 4 bytes */
diff --git a/src/gallium/drivers/r300/compiler/r300_nir.c b/src/gallium/drivers/r300/compiler/r300_nir.c
index 6201fa6f28baf..d74d7e34f9a8b 100644
--- a/src/gallium/drivers/r300/compiler/r300_nir.c
+++ b/src/gallium/drivers/r300/compiler/r300_nir.c
@@ -59,7 +59,7 @@ r300_should_vectorize_io(unsigned align, unsigned bit_size,
                         unsigned hole_size, nir_intrinsic_instr *low,
                         nir_intrinsic_instr *high, void *data)
 {
-   if (bit_size != 32 || hole_size)
+   if (bit_size != 32 || hole_size || !nir_num_components_valid(num_components))
       return false;
 
    /* Our offset alignment should always be at least 4 bytes */
diff --git a/src/intel/compiler/brw_nir.c b/src/intel/compiler/brw_nir.c
index 7d87d09c75696..20e5fdef2862b 100644
--- a/src/intel/compiler/brw_nir.c
+++ b/src/intel/compiler/brw_nir.c
@@ -1396,7 +1396,7 @@ brw_nir_should_vectorize_mem(unsigned align_mul, unsigned align_offset,
     * those back into 32-bit ones anyway and UBO loads aren't split in NIR so
     * we don't want to make a mess for the back-end.
     */
-   if (bit_size > 32 || hole_size)
+   if (bit_size > 32 || hole_size || !nir_num_components_valid(num_components))
       return false;
 
    if (low->intrinsic == nir_intrinsic_load_global_const_block_intel ||
diff --git a/src/intel/compiler/elk/elk_nir.c b/src/intel/compiler/elk/elk_nir.c
index 5ddd1e8cd122a..69b3680233e7a 100644
--- a/src/intel/compiler/elk/elk_nir.c
+++ b/src/intel/compiler/elk/elk_nir.c
@@ -1143,7 +1143,7 @@ elk_nir_should_vectorize_mem(unsigned align_mul, unsigned align_offset,
     * those back into 32-bit ones anyway and UBO loads aren't split in NIR so
     * we don't want to make a mess for the back-end.
     */
-   if (bit_size > 32 || hole_size)
+   if (bit_size > 32 || hole_size || !nir_num_components_valid(num_components))
       return false;
 
    if (low->intrinsic == nir_intrinsic_load_global_const_block_intel ||
-- 
GitLab


From 17f1b5001f94c12e477c73b8753f8dab1aedd2c3 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 15 Jun 2024 04:35:15 -0400
Subject: [PATCH 4/6] nir/opt_load_store_vectorize: allow overfetching, merge
 overfetched loads

New load merging transformations, examples:
    vec4 + vec3 ==> vec8(read=0x7f) (because NIR doesn't have vec7)
    vec1 + vec8(read=0x7f) ==> vec8(read=0xff) (the unused component is dropped)

Not merged:
    vec8(read=0xfe) + vec1
    - unused components at the beginning are kept
---
 .../nir/nir_opt_load_store_vectorize.c        |  38 ++++-
 .../nir/tests/load_store_vectorizer_tests.cpp | 142 +++++++++++++++++-
 2 files changed, 172 insertions(+), 8 deletions(-)

diff --git a/src/compiler/nir/nir_opt_load_store_vectorize.c b/src/compiler/nir/nir_opt_load_store_vectorize.c
index 632f763d81647..9a6ffb8334f34 100644
--- a/src/compiler/nir/nir_opt_load_store_vectorize.c
+++ b/src/compiler/nir/nir_opt_load_store_vectorize.c
@@ -536,7 +536,9 @@ create_entry(void *mem_ctx,
    entry->instr = &intrin->instr;
    entry->info = info;
    entry->is_store = entry->info->value_src >= 0;
-   entry->num_components = intrin->num_components;
+   entry->num_components =
+      entry->is_store ? intrin->num_components :
+                        util_last_bit(nir_def_components_read(&intrin->def));
 
    if (entry->info->deref_src >= 0) {
       entry->deref = nir_src_as_deref(intrin->src[entry->info->deref_src]);
@@ -609,8 +611,18 @@ new_bitsize_acceptable(struct vectorize_ctx *ctx, unsigned new_bit_size,
       return false;
 
    unsigned new_num_components = size / new_bit_size;
-   if (!nir_num_components_valid(new_num_components))
-      return false;
+
+   if (low->is_store) {
+      if (!nir_num_components_valid(new_num_components))
+         return false;
+   } else {
+      /* Invalid component counts must be rejected by the callback, otherwise
+       * the load will overfetch by aligning the number to the next valid
+       * component count.
+       */
+      if (new_num_components > NIR_MAX_VEC_COMPONENTS)
+         return false;
+   }
 
    unsigned high_offset = high->offset_signed - low->offset_signed;
 
@@ -693,14 +705,27 @@ vectorize_loads(nir_builder *b, struct vectorize_ctx *ctx,
 
    b->cursor = nir_after_instr(first->instr);
 
+   /* Align num_components to a supported vector size. Drivers can reject
+    * this in the callback by returning false for invalid num_components.
+    */
+   while (!nir_num_components_valid(new_num_components))
+      new_num_components++;
+
    /* update the load's destination size and extract data for each of the original loads */
    data->num_components = new_num_components;
    data->bit_size = new_bit_size;
 
    nir_def *low_def = nir_extract_bits(
       b, &data, 1, 0, low->intrin->num_components, low_bit_size);
+
+   /* If we are merging e.g. vec1 + vec7_as_8 ==> vec8, we need to create
+    * a vec8 move for the second load even though it only uses 7 components.
+    * Do it by doubling the data source. The extra unused component will be
+    * dead code.
+    */
    nir_def *high_def = nir_extract_bits(
-      b, &data, 1, high_start, high->intrin->num_components, high_bit_size);
+      b, (nir_def*[]){data, data}, 2, high_start,
+      high->intrin->num_components, high_bit_size);
 
    /* convert booleans */
    low_def = low_bool ? nir_i2b(b, low_def) : nir_mov(b, low_def);
@@ -755,9 +780,12 @@ vectorize_loads(nir_builder *b, struct vectorize_ctx *ctx,
       uint32_t high_base = nir_intrinsic_range_base(high->intrin);
       uint32_t low_end = low_base + nir_intrinsic_range(low->intrin);
       uint32_t high_end = high_base + nir_intrinsic_range(high->intrin);
+      /* If we trimmed an overfetching load, we need to trim the range too. */
+      uint32_t range = MIN2(MAX2(low_end, high_end) - low_base,
+                            new_num_components * new_bit_size / 8);
 
       nir_intrinsic_set_range_base(first->intrin, low_base);
-      nir_intrinsic_set_range(first->intrin, MAX2(low_end, high_end) - low_base);
+      nir_intrinsic_set_range(first->intrin, range);
    } else if (nir_intrinsic_has_base(first->intrin) && info->base_src == -1 && info->deref_src == -1) {
       nir_intrinsic_set_base(first->intrin, nir_intrinsic_base(low->intrin));
    }
diff --git a/src/compiler/nir/tests/load_store_vectorizer_tests.cpp b/src/compiler/nir/tests/load_store_vectorizer_tests.cpp
index 1ee699b96a4ec..9ce1eca82a6b2 100644
--- a/src/compiler/nir/tests/load_store_vectorizer_tests.cpp
+++ b/src/compiler/nir/tests/load_store_vectorizer_tests.cpp
@@ -81,6 +81,8 @@ protected:
    std::map<unsigned, nir_alu_instr*> movs;
    std::map<unsigned, nir_alu_src*> loads;
    std::map<unsigned, nir_def*> res_map;
+   unsigned max_components = 4;
+   bool overfetch = false;
 };
 
 std::string
@@ -88,7 +90,7 @@ nir_load_store_vectorize_test::swizzle(nir_alu_instr *instr, int src)
 {
    std::string swizzle;
    for (unsigned i = 0; i < nir_ssa_alu_instr_src_components(instr, src); i++) {
-      swizzle += "xyzw"[instr->src[src].swizzle[i]];
+      swizzle += "xyzwefghijklmnop"[instr->src[src].swizzle[i]];
    }
 
    return swizzle;
@@ -141,6 +143,7 @@ nir_load_store_vectorize_test::run_vectorizer(nir_variable_mode modes,
    opts.callback = mem_vectorize_callback;
    opts.modes = modes;
    opts.robust_modes = robust_modes;
+   opts.cb_data = this;
    bool progress = nir_opt_load_store_vectorize(b->shader, &opts);
 
    if (progress) {
@@ -340,7 +343,10 @@ bool nir_load_store_vectorize_test::mem_vectorize_callback(
    nir_intrinsic_instr *low, nir_intrinsic_instr *high,
    void *data)
 {
-   if (hole_size || !nir_num_components_valid(num_components))
+   nir_load_store_vectorize_test *test = (nir_load_store_vectorize_test *)data;
+
+   if (hole_size ||
+       (!test->overfetch && !nir_num_components_valid(num_components)))
       return false;
 
    /* Calculate a simple alignment, like how nir_intrinsic_align() does. */
@@ -350,7 +356,7 @@ bool nir_load_store_vectorize_test::mem_vectorize_callback(
 
    /* Require scalar alignment and less than 5 components. */
    return align % (bit_size / 8) == 0 &&
-          num_components <= 4;
+          (test->overfetch || num_components <= test->max_components);
 }
 
 void nir_load_store_vectorize_test::shared_type_info(
@@ -2017,3 +2023,133 @@ TEST_F(nir_load_store_vectorize_test, ubo_alignment_const_100)
    EXPECT_EQ(nir_intrinsic_align_mul(load), NIR_ALIGN_MUL_MAX);
    EXPECT_EQ(nir_intrinsic_align_offset(load), 100);
 }
+
+TEST_F(nir_load_store_vectorize_test, ubo_overfetch_vec6_as_vec8)
+{
+   create_load(nir_var_mem_ubo, 0, 0, 0x1, 32, 4);
+   create_load(nir_var_mem_ubo, 0, 16, 0x2, 32, 2);
+
+   nir_validate_shader(b->shader, NULL);
+   ASSERT_EQ(count_intrinsics(nir_intrinsic_load_ubo), 2);
+
+   this->overfetch = true;
+   EXPECT_TRUE(run_vectorizer(nir_var_mem_ubo));
+   this->overfetch = false;
+
+   ASSERT_EQ(count_intrinsics(nir_intrinsic_load_ubo), 1);
+
+   nir_intrinsic_instr *load = get_intrinsic(nir_intrinsic_load_ubo, 0);
+   ASSERT_EQ(load->def.bit_size, 32);
+   ASSERT_EQ(load->def.num_components, 8);
+   ASSERT_EQ(nir_intrinsic_range_base(load), 0);
+   ASSERT_EQ(nir_intrinsic_range(load), 24);
+   ASSERT_EQ(nir_def_components_read(&load->def), 0x3f);
+   ASSERT_EQ(nir_src_as_uint(load->src[1]), 0);
+   EXPECT_INSTR_SWIZZLES(movs[0x1], load, "xyzw");
+   EXPECT_INSTR_SWIZZLES(movs[0x2], load, "ef");
+}
+
+TEST_F(nir_load_store_vectorize_test, ubo_overfetch_vec7_as_vec8)
+{
+   create_load(nir_var_mem_ubo, 0, 0, 0x1, 32, 4);
+   create_load(nir_var_mem_ubo, 0, 16, 0x2, 32, 3);
+
+   nir_validate_shader(b->shader, NULL);
+   ASSERT_EQ(count_intrinsics(nir_intrinsic_load_ubo), 2);
+
+   this->overfetch = true;
+   EXPECT_TRUE(run_vectorizer(nir_var_mem_ubo));
+   this->overfetch = false;
+
+   ASSERT_EQ(count_intrinsics(nir_intrinsic_load_ubo), 1);
+
+   nir_intrinsic_instr *load = get_intrinsic(nir_intrinsic_load_ubo, 0);
+   ASSERT_EQ(load->def.bit_size, 32);
+   ASSERT_EQ(load->def.num_components, 8);
+   ASSERT_EQ(nir_intrinsic_range_base(load), 0);
+   ASSERT_EQ(nir_intrinsic_range(load), 28);
+   ASSERT_EQ(nir_def_components_read(&load->def), 0x7f);
+   ASSERT_EQ(nir_src_as_uint(load->src[1]), 0);
+   EXPECT_INSTR_SWIZZLES(movs[0x1], load, "xyzw");
+   EXPECT_INSTR_SWIZZLES(movs[0x2], load, "efg");
+}
+
+TEST_F(nir_load_store_vectorize_test, ubo_overfetch_vec7_as_vec8_disallowed)
+{
+   create_load(nir_var_mem_ubo, 0, 0, 0x1, 32, 4);
+   create_load(nir_var_mem_ubo, 0, 16, 0x2, 32, 3);
+
+   nir_validate_shader(b->shader, NULL);
+   ASSERT_EQ(count_intrinsics(nir_intrinsic_load_ubo), 2);
+
+   EXPECT_TRUE(run_vectorizer(nir_var_mem_ubo));
+   ASSERT_EQ(count_intrinsics(nir_intrinsic_load_ubo), 2);
+}
+
+TEST_F(nir_load_store_vectorize_test, ubo_overfetch_vec11_as_vec16)
+{
+   create_load(nir_var_mem_ubo, 0, 0, 0x1, 32, 8);
+   create_load(nir_var_mem_ubo, 0, 32, 0x2, 32, 3);
+
+   nir_validate_shader(b->shader, NULL);
+   ASSERT_EQ(count_intrinsics(nir_intrinsic_load_ubo), 2);
+
+   this->overfetch = true;
+   EXPECT_TRUE(run_vectorizer(nir_var_mem_ubo));
+   this->overfetch = false;
+
+   ASSERT_EQ(count_intrinsics(nir_intrinsic_load_ubo), 1);
+
+   nir_intrinsic_instr *load = get_intrinsic(nir_intrinsic_load_ubo, 0);
+   ASSERT_EQ(load->def.bit_size, 32);
+   ASSERT_EQ(load->def.num_components, 16);
+   ASSERT_EQ(nir_intrinsic_range_base(load), 0);
+   ASSERT_EQ(nir_intrinsic_range(load), 44);
+   ASSERT_EQ(nir_def_components_read(&load->def), 0x7ff);
+   ASSERT_EQ(nir_src_as_uint(load->src[1]), 0);
+   EXPECT_INSTR_SWIZZLES(movs[0x1], load, "xyzwefgh");
+   EXPECT_INSTR_SWIZZLES(movs[0x2], load, "ijk");
+}
+
+TEST_F(nir_load_store_vectorize_test, ubo_vec1_vec7as8)
+{
+   create_load(nir_var_mem_ubo, 0, 0, 0x1, 32, 1);
+   create_load(nir_var_mem_ubo, 0, 4, 0x2, 32, 8);
+   movs[0x2]->src[0].swizzle[7] = 6; /* use only components 0..6 */
+
+   nir_validate_shader(b->shader, NULL);
+   ASSERT_EQ(count_intrinsics(nir_intrinsic_load_ubo), 2);
+
+   this->max_components = 8;
+   EXPECT_TRUE(run_vectorizer(nir_var_mem_ubo));
+   this->max_components = 4;
+
+   ASSERT_EQ(count_intrinsics(nir_intrinsic_load_ubo), 1);
+
+   nir_intrinsic_instr *load = get_intrinsic(nir_intrinsic_load_ubo, 0);
+   ASSERT_EQ(load->def.bit_size, 32);
+   ASSERT_EQ(load->def.num_components, 8);
+   ASSERT_EQ(nir_intrinsic_range_base(load), 0);
+   ASSERT_EQ(nir_intrinsic_range(load), 32);
+   ASSERT_EQ(nir_def_components_read(&load->def), 0xff);
+   ASSERT_EQ(nir_src_as_uint(load->src[1]), 0);
+   EXPECT_INSTR_SWIZZLES(movs[0x1], load, "x");
+   EXPECT_INSTR_SWIZZLES(movs[0x2], load, "yzwefghh");
+}
+
+TEST_F(nir_load_store_vectorize_test, ubo_vec7as8_vec1)
+{
+   create_load(nir_var_mem_ubo, 0, 0, 0x1, 32, 8);
+   movs[0x1]->src[0].swizzle[0] = 1; /* use only components 1..7 */
+   create_load(nir_var_mem_ubo, 0, 32, 0x2, 32, 1);
+
+   nir_validate_shader(b->shader, NULL);
+   ASSERT_EQ(count_intrinsics(nir_intrinsic_load_ubo), 2);
+
+   this->max_components = 8;
+   EXPECT_TRUE(run_vectorizer(nir_var_mem_ubo));
+   this->max_components = 4;
+
+   /* TODO: This is not merged by the pass, but we could implement it. */
+   ASSERT_EQ(count_intrinsics(nir_intrinsic_load_ubo), 2);
+}
-- 
GitLab


From 581fbe9616bcd1a1847cb843a6ba9d302b9bc8d9 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Fri, 24 May 2024 14:47:39 -0400
Subject: [PATCH 5/6] nir/opt_load_store_vectorize: allow a 4-byte hole between
 2 loads

If there is a 4-byte hole between 2 loads, drivers can now optionally
vectorize the loads by including the hole between them, e.g.:
    4B load + 4B hole + 8B load --> 16B load

All vectorize callbacks already reject all holes, but AMD will want to
allow it.

radeonsi+ACO with the new vectorization callback:

TOTALS FROM AFFECTED SHADERS (25248/58918)
  VGPRs: 871116 -> 871872 (0.09 %)
  Spilled SGPRs: 397 -> 407 (2.52 %)
  Code Size: 43074536 -> 42496352 (-1.34 %) bytes
---
 .../nir/nir_opt_load_store_vectorize.c        | 16 +++-
 .../nir/tests/load_store_vectorizer_tests.cpp | 85 ++++++++++++++++++-
 2 files changed, 98 insertions(+), 3 deletions(-)

diff --git a/src/compiler/nir/nir_opt_load_store_vectorize.c b/src/compiler/nir/nir_opt_load_store_vectorize.c
index 9a6ffb8334f34..093d889c4b4c5 100644
--- a/src/compiler/nir/nir_opt_load_store_vectorize.c
+++ b/src/compiler/nir/nir_opt_load_store_vectorize.c
@@ -634,9 +634,14 @@ new_bitsize_acceptable(struct vectorize_ctx *ctx, unsigned new_bit_size,
    if (new_bit_size / common_bit_size > NIR_MAX_VEC_COMPONENTS)
       return false;
 
+   unsigned low_size = low->intrin->num_components * get_bit_size(low) / 8;
+   /* The hole size can be less than 0 if low and high instructions overlap. */
+   unsigned hole_size =
+      MAX2(high->offset_signed - (low->offset_signed + low_size), 0);
+
    if (!ctx->options->callback(low->align_mul,
                                low->align_offset,
-                               new_bit_size, new_num_components, 0,
+                               new_bit_size, new_num_components, hole_size,
                                low->intrin, high->intrin,
                                ctx->options->cb_data))
       return false;
@@ -1275,7 +1280,14 @@ vectorize_sorted_entries(struct vectorize_ctx *ctx, nir_function_impl *impl,
          struct entry *second = low->index < high->index ? high : low;
 
          uint64_t diff = high->offset_signed - low->offset_signed;
-         bool separate = diff > get_bit_size(low) / 8u * low->num_components;
+         /* Allow overfetching by 4 bytes except shared memory, which can be later
+          * rejected by the callback.
+          */
+         unsigned max_hole =
+            first->is_store || get_variable_mode(first) == nir_var_mem_shared ? 0 : 4;
+         bool separate =
+            diff > max_hole + get_bit_size(low) / 8u * low->num_components;
+
          if (separate) {
             if (!ctx->options->has_shared2_amd ||
                 get_variable_mode(first) != nir_var_mem_shared)
diff --git a/src/compiler/nir/tests/load_store_vectorizer_tests.cpp b/src/compiler/nir/tests/load_store_vectorizer_tests.cpp
index 9ce1eca82a6b2..348076dfd0f53 100644
--- a/src/compiler/nir/tests/load_store_vectorizer_tests.cpp
+++ b/src/compiler/nir/tests/load_store_vectorizer_tests.cpp
@@ -83,6 +83,7 @@ protected:
    std::map<unsigned, nir_def*> res_map;
    unsigned max_components = 4;
    bool overfetch = false;
+   unsigned max_hole_size = 0;
 };
 
 std::string
@@ -345,7 +346,9 @@ bool nir_load_store_vectorize_test::mem_vectorize_callback(
 {
    nir_load_store_vectorize_test *test = (nir_load_store_vectorize_test *)data;
 
-   if (hole_size ||
+   assert(hole_size <= 4);
+
+   if (hole_size > test->max_hole_size ||
        (!test->overfetch && !nir_num_components_valid(num_components)))
       return false;
 
@@ -2153,3 +2156,83 @@ TEST_F(nir_load_store_vectorize_test, ubo_vec7as8_vec1)
    /* TODO: This is not merged by the pass, but we could implement it. */
    ASSERT_EQ(count_intrinsics(nir_intrinsic_load_ubo), 2);
 }
+
+TEST_F(nir_load_store_vectorize_test, ubo_vec2_hole1_vec1_disallowed)
+{
+   create_load(nir_var_mem_ubo, 0, 0, 0x1, 32, 2);
+   create_load(nir_var_mem_ubo, 0, 12, 0x2, 32, 1);
+
+   nir_validate_shader(b->shader, NULL);
+   ASSERT_EQ(count_intrinsics(nir_intrinsic_load_ubo), 2);
+
+   EXPECT_TRUE(run_vectorizer(nir_var_mem_ubo));
+   ASSERT_EQ(count_intrinsics(nir_intrinsic_load_ubo), 2);
+}
+
+TEST_F(nir_load_store_vectorize_test, ubo_vec2_hole1_vec1)
+{
+   create_load(nir_var_mem_ubo, 0, 0, 0x1, 32, 2);
+   create_load(nir_var_mem_ubo, 0, 12, 0x2, 32, 1);
+
+   nir_validate_shader(b->shader, NULL);
+   ASSERT_EQ(count_intrinsics(nir_intrinsic_load_ubo), 2);
+
+   this->max_hole_size = 4;
+   EXPECT_TRUE(run_vectorizer(nir_var_mem_ubo));
+   this->max_hole_size = 0;
+
+   ASSERT_EQ(count_intrinsics(nir_intrinsic_load_ubo), 1);
+
+   nir_intrinsic_instr *load = get_intrinsic(nir_intrinsic_load_ubo, 0);
+   ASSERT_EQ(load->def.bit_size, 32);
+   ASSERT_EQ(load->def.num_components, 4);
+   ASSERT_EQ(nir_intrinsic_range_base(load), 0);
+   ASSERT_EQ(nir_intrinsic_range(load), 16);
+   ASSERT_EQ(nir_def_components_read(&load->def), 1 | 2 | 8);
+   ASSERT_EQ(nir_src_as_uint(load->src[1]), 0);
+   EXPECT_INSTR_SWIZZLES(movs[0x1], load, "xy");
+   EXPECT_INSTR_SWIZZLES(movs[0x2], load, "w");
+}
+
+TEST_F(nir_load_store_vectorize_test, ubo_vec2_hole2_vec4_disallowed)
+{
+   create_load(nir_var_mem_ubo, 0, 0, 0x1, 32, 2);
+   create_load(nir_var_mem_ubo, 0, 16, 0x2, 32, 1);
+
+   nir_validate_shader(b->shader, NULL);
+   ASSERT_EQ(count_intrinsics(nir_intrinsic_load_ubo), 2);
+
+   /* The pass only allows 4-byte holes. */
+   this->max_hole_size = 8;
+   EXPECT_TRUE(run_vectorizer(nir_var_mem_ubo));
+   this->max_hole_size = 0;
+
+   ASSERT_EQ(count_intrinsics(nir_intrinsic_load_ubo), 2);
+}
+
+TEST_F(nir_load_store_vectorize_test, ubo_vec3_hole1_vec3)
+{
+   create_load(nir_var_mem_ubo, 0, 0, 0x1, 32, 3);
+   create_load(nir_var_mem_ubo, 0, 16, 0x2, 32, 3);
+
+   nir_validate_shader(b->shader, NULL);
+   ASSERT_EQ(count_intrinsics(nir_intrinsic_load_ubo), 2);
+
+   this->overfetch = true;
+   this->max_hole_size = 4;
+   EXPECT_TRUE(run_vectorizer(nir_var_mem_ubo));
+   this->max_hole_size = 0;
+   this->overfetch = false;
+
+   ASSERT_EQ(count_intrinsics(nir_intrinsic_load_ubo), 1);
+
+   nir_intrinsic_instr *load = get_intrinsic(nir_intrinsic_load_ubo, 0);
+   ASSERT_EQ(load->def.bit_size, 32);
+   ASSERT_EQ(load->def.num_components, 8);
+   ASSERT_EQ(nir_intrinsic_range_base(load), 0);
+   ASSERT_EQ(nir_intrinsic_range(load), 28);
+   ASSERT_EQ(nir_def_components_read(&load->def), 0x77);
+   ASSERT_EQ(nir_src_as_uint(load->src[1]), 0);
+   EXPECT_INSTR_SWIZZLES(movs[0x1], load, "xyz");
+   EXPECT_INSTR_SWIZZLES(movs[0x2], load, "efg");
+}
-- 
GitLab


From 0acb7bb7eaba23f5e645157adc2e27afa6089b4b Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Fri, 24 May 2024 23:40:43 -0400
Subject: [PATCH 6/6] nir/opt_load_store_vectorize: vectorize load_smem_amd

radeonsi+ACO with the new vectorization callback:

TOTALS FROM AFFECTED SHADERS (19508/58918)
  VGPRs: 708672 -> 708864 (0.03 %)
  Code Size: 31458688 -> 31217160 (-0.77 %) bytes
  Max Waves: 305960 -> 305952 (-0.00 %)

Reviewed-by: Qiang Yu <yuq825@gmail.com>
---
 src/compiler/nir/nir_opt_load_store_vectorize.c | 1 +
 1 file changed, 1 insertion(+)

diff --git a/src/compiler/nir/nir_opt_load_store_vectorize.c b/src/compiler/nir/nir_opt_load_store_vectorize.c
index 093d889c4b4c5..8fb398dc8fb1f 100644
--- a/src/compiler/nir/nir_opt_load_store_vectorize.c
+++ b/src/compiler/nir/nir_opt_load_store_vectorize.c
@@ -104,6 +104,7 @@ get_info(nir_intrinsic_op op)
       LOAD(nir_var_mem_global, global_constant_uniform_block_intel, -1, 0, -1)
       INFO(nir_var_mem_ubo, ldc_nv, false, 0, 1, -1, -1)
       INFO(nir_var_mem_ubo, ldcx_nv, false, 0, 1, -1, -1)
+      LOAD(nir_var_mem_constant, smem_amd, 0, 1, -1)
    default:
       break;
 #undef ATOMIC
-- 
GitLab

