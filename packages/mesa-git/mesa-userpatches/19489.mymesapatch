From 22a9632bfacb2a8ce075ab340da156a21d58afa7 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Wed, 8 Jun 2022 11:09:35 +0800
Subject: [PATCH 01/45] radeonsi: use nir_lower_gs_intrinsics
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Replace some llvm code.

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/llvm/ac_nir_to_llvm.c                 |  3 --
 src/amd/llvm/ac_shader_abi.h                  |  2 -
 .../drivers/radeonsi/gfx10_shader_ngg.c       | 28 ++++-------
 .../drivers/radeonsi/si_shader_internal.h     |  3 +-
 .../drivers/radeonsi/si_shader_llvm_gs.c      | 47 ++++---------------
 src/gallium/drivers/radeonsi/si_shader_nir.c  |  3 ++
 6 files changed, 22 insertions(+), 64 deletions(-)

diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index 7b69c1ca0130..ceadbbbb4a69 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -4053,9 +4053,6 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
                                        false);
       break;
    }
-   case nir_intrinsic_emit_vertex:
-      ctx->abi->emit_vertex(ctx->abi, nir_intrinsic_stream_id(instr), ctx->abi->outputs);
-      break;
    case nir_intrinsic_emit_vertex_with_counter: {
       unsigned stream = nir_intrinsic_stream_id(instr);
       LLVMValueRef next_vertex = get_src(ctx, instr->src[0]);
diff --git a/src/amd/llvm/ac_shader_abi.h b/src/amd/llvm/ac_shader_abi.h
index 00f32c941a2c..1268669a5f58 100644
--- a/src/amd/llvm/ac_shader_abi.h
+++ b/src/amd/llvm/ac_shader_abi.h
@@ -68,8 +68,6 @@ struct ac_shader_abi {
 
    void (*export_vertex)(struct ac_shader_abi *abi);
 
-   void (*emit_vertex)(struct ac_shader_abi *abi, unsigned stream, LLVMValueRef *addrs);
-
    void (*emit_primitive)(struct ac_shader_abi *abi, unsigned stream);
 
    void (*emit_vertex_with_counter)(struct ac_shader_abi *abi, unsigned stream,
diff --git a/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c b/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
index 192cd0bd20ff..b78957c0fe9d 100644
--- a/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
+++ b/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
@@ -1885,27 +1885,12 @@ static LLVMValueRef ngg_gs_get_emit_primflag_ptr(struct si_shader_context *ctx,
    return LLVMBuildGEP2(ctx->ac.builder, vertexptr.pointee_type, vertexptr.value, gep_idx, 3, "");
 }
 
-void gfx10_ngg_gs_emit_vertex(struct si_shader_context *ctx, unsigned stream, LLVMValueRef *addrs)
+void gfx10_ngg_gs_emit_vertex(struct si_shader_context *ctx, unsigned stream,
+                              LLVMValueRef vertexidx, LLVMValueRef *addrs)
 {
    const struct si_shader_selector *sel = ctx->shader->selector;
    const struct si_shader_info *info = &sel->info;
    LLVMBuilderRef builder = ctx->ac.builder;
-   LLVMValueRef tmp;
-   const LLVMValueRef vertexidx = LLVMBuildLoad2(builder, ctx->ac.i32, ctx->gs_next_vertex[stream], "");
-
-   /* If this thread has already emitted the declared maximum number of
-    * vertices, skip the write: excessive vertex emissions are not
-    * supposed to have any effect.
-    */
-   const LLVMValueRef can_emit =
-      LLVMBuildICmp(builder, LLVMIntULT, vertexidx,
-                    LLVMConstInt(ctx->ac.i32, sel->info.base.gs.vertices_out, false), "");
-
-   tmp = LLVMBuildAdd(builder, vertexidx, ctx->ac.i32_1, "");
-   tmp = LLVMBuildSelect(builder, can_emit, tmp, vertexidx, "");
-   LLVMBuildStore(builder, tmp, ctx->gs_next_vertex[stream]);
-
-   ac_build_ifcc(&ctx->ac, can_emit, 9001);
 
    const struct ac_llvm_pointer vertexptr = ngg_gs_emit_vertex_ptr(ctx, gfx10_get_thread_id_in_tg(ctx), vertexidx);
    unsigned out_idx = 0;
@@ -1923,6 +1908,13 @@ void gfx10_ngg_gs_emit_vertex(struct si_shader_context *ctx, unsigned stream, LL
    }
    assert(out_idx * 4 == info->gsvs_vertex_size);
 
+   /* Store the current number of emitted vertices to zero out remaining
+    * primitive flags in case the geometry shader doesn't emit the maximum
+    * number of vertices.
+    */
+   LLVMValueRef tmp = LLVMBuildAdd(builder, vertexidx, ctx->ac.i32_1, "");
+   LLVMBuildStore(builder, tmp, ctx->gs_next_vertex[stream]);
+
    /* Determine and store whether this vertex completed a primitive. */
    const LLVMValueRef curverts = LLVMBuildLoad2(builder, ctx->ac.i32, ctx->gs_curprim_verts[stream], "");
 
@@ -1955,8 +1947,6 @@ void gfx10_ngg_gs_emit_vertex(struct si_shader_context *ctx, unsigned stream, LL
    tmp = LLVMBuildLoad2(builder, ctx->ac.i32, ctx->gs_generated_prims[stream], "");
    tmp = LLVMBuildAdd(builder, tmp, LLVMBuildZExt(builder, iscompleteprim, ctx->ac.i32, ""), "");
    LLVMBuildStore(builder, tmp, ctx->gs_generated_prims[stream]);
-
-   ac_build_endif(&ctx->ac, 9001);
 }
 
 void gfx10_ngg_gs_emit_begin(struct si_shader_context *ctx)
diff --git a/src/gallium/drivers/radeonsi/si_shader_internal.h b/src/gallium/drivers/radeonsi/si_shader_internal.h
index 3ce257881b68..daa3766c69cb 100644
--- a/src/gallium/drivers/radeonsi/si_shader_internal.h
+++ b/src/gallium/drivers/radeonsi/si_shader_internal.h
@@ -188,7 +188,8 @@ void gfx10_ngg_culling_build_end(struct si_shader_context *ctx);
 void gfx10_ngg_build_end(struct si_shader_context *ctx);
 void gfx10_ngg_atomic_add_prim_count(struct ac_shader_abi *abi, unsigned stream,
                                      LLVMValueRef prim_count, enum ac_prim_count count_type);
-void gfx10_ngg_gs_emit_vertex(struct si_shader_context *ctx, unsigned stream, LLVMValueRef *addrs);
+void gfx10_ngg_gs_emit_vertex(struct si_shader_context *ctx, unsigned stream,
+                              LLVMValueRef vertexidx, LLVMValueRef *addrs);
 void gfx10_ngg_gs_emit_begin(struct si_shader_context *ctx);
 void gfx10_ngg_gs_build_end(struct si_shader_context *ctx);
 unsigned gfx10_ngg_get_scratch_dw_size(struct si_shader *shader);
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c b/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c
index 363267119bce..c5b1c123231d 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c
@@ -162,48 +162,23 @@ void si_llvm_gs_build_end(struct si_shader_context *ctx)
 }
 
 /* Emit one vertex from the geometry shader */
-static void si_llvm_emit_vertex(struct ac_shader_abi *abi, unsigned stream, LLVMValueRef *addrs)
+static void si_llvm_emit_vertex(struct ac_shader_abi *abi, unsigned stream,
+                                LLVMValueRef vertexidx, LLVMValueRef *addrs)
 {
    struct si_shader_context *ctx = si_shader_context_from_abi(abi);
 
    if (ctx->shader->key.ge.as_ngg) {
-      gfx10_ngg_gs_emit_vertex(ctx, stream, addrs);
+      gfx10_ngg_gs_emit_vertex(ctx, stream, vertexidx, addrs);
       return;
    }
 
    struct si_shader_info *info = &ctx->shader->selector->info;
    struct si_shader *shader = ctx->shader;
    LLVMValueRef soffset = ac_get_arg(&ctx->ac, ctx->args.gs2vs_offset);
-   LLVMValueRef gs_next_vertex;
-   LLVMValueRef can_emit;
-   unsigned chan, offset;
-   int i;
-
-   /* Write vertex attribute values to GSVS ring */
-   gs_next_vertex = LLVMBuildLoad2(ctx->ac.builder, ctx->ac.i32, ctx->gs_next_vertex[stream], "");
 
-   /* If this thread has already emitted the declared maximum number of
-    * vertices, skip the write: excessive vertex emissions are not
-    * supposed to have any effect.
-    *
-    * If the shader has no writes to memory, kill it instead. This skips
-    * further memory loads and may allow LLVM to skip to the end
-    * altogether.
-    */
-   can_emit =
-      LLVMBuildICmp(ctx->ac.builder, LLVMIntULT, gs_next_vertex,
-                    LLVMConstInt(ctx->ac.i32, shader->selector->info.base.gs.vertices_out, 0), "");
-
-   bool use_kill = !info->base.writes_memory;
-   if (use_kill) {
-      ac_build_kill_if_false(&ctx->ac, can_emit);
-   } else {
-      ac_build_ifcc(&ctx->ac, can_emit, 6505);
-   }
-
-   offset = 0;
-   for (i = 0; i < info->num_outputs; i++) {
-      for (chan = 0; chan < 4; chan++) {
+   unsigned offset = 0;
+   for (unsigned i = 0; i < info->num_outputs; i++) {
+      for (unsigned chan = 0; chan < 4; chan++) {
          if (!(info->output_usagemask[i] & (1 << chan)) ||
              ((info->output_streams[i] >> (2 * chan)) & 3) != stream)
             continue;
@@ -213,7 +188,7 @@ static void si_llvm_emit_vertex(struct ac_shader_abi *abi, unsigned stream, LLVM
             LLVMConstInt(ctx->ac.i32, offset * shader->selector->info.base.gs.vertices_out, 0);
          offset++;
 
-         voffset = LLVMBuildAdd(ctx->ac.builder, voffset, gs_next_vertex, "");
+         voffset = LLVMBuildAdd(ctx->ac.builder, voffset, vertexidx, "");
          voffset = LLVMBuildMul(ctx->ac.builder, voffset, LLVMConstInt(ctx->ac.i32, 4, 0), "");
 
          out_val = ac_to_integer(&ctx->ac, out_val);
@@ -223,9 +198,6 @@ static void si_llvm_emit_vertex(struct ac_shader_abi *abi, unsigned stream, LLVM
       }
    }
 
-   gs_next_vertex = LLVMBuildAdd(ctx->ac.builder, gs_next_vertex, ctx->ac.i32_1, "");
-   LLVMBuildStore(ctx->ac.builder, gs_next_vertex, ctx->gs_next_vertex[stream]);
-
    /* Signal vertex emission if vertex data was written. */
    if (offset) {
       ac_build_sendmsg(&ctx->ac, AC_SENDMSG_GS_OP_EMIT | AC_SENDMSG_GS | (stream << 8),
@@ -234,9 +206,6 @@ static void si_llvm_emit_vertex(struct ac_shader_abi *abi, unsigned stream, LLVM
       ctx->gs_emitted_vertices = LLVMBuildAdd(ctx->ac.builder, ctx->gs_emitted_vertices,
                                               ctx->ac.i32_1, "vert");
    }
-
-   if (!use_kill)
-      ac_build_endif(&ctx->ac, 6505);
 }
 
 /* Cut one primitive from the geometry shader */
@@ -601,6 +570,6 @@ struct si_shader *si_generate_gs_copy_shader(struct si_screen *sscreen,
 
 void si_llvm_init_gs_callbacks(struct si_shader_context *ctx)
 {
-   ctx->abi.emit_vertex = si_llvm_emit_vertex;
+   ctx->abi.emit_vertex_with_counter = si_llvm_emit_vertex;
    ctx->abi.emit_primitive = si_llvm_emit_primitive;
 }
diff --git a/src/gallium/drivers/radeonsi/si_shader_nir.c b/src/gallium/drivers/radeonsi/si_shader_nir.c
index 6fac0ae9f5d5..b0c2e4ff5b52 100644
--- a/src/gallium/drivers/radeonsi/si_shader_nir.c
+++ b/src/gallium/drivers/radeonsi/si_shader_nir.c
@@ -297,6 +297,9 @@ static void si_lower_nir(struct si_screen *sscreen, struct nir_shader *nir)
        nir->info.stage == MESA_SHADER_GEOMETRY)
       NIR_PASS_V(nir, nir_lower_io_to_scalar, nir_var_shader_out);
 
+   if (nir->info.stage == MESA_SHADER_GEOMETRY)
+      NIR_PASS_V(nir, nir_lower_gs_intrinsics, nir_lower_gs_intrinsics_per_stream);
+
    if (nir->info.stage == MESA_SHADER_COMPUTE) {
       if (nir->info.cs.derivative_group == DERIVATIVE_GROUP_QUADS) {
          /* If we are shuffling local_invocation_id for quad derivatives, we
-- 
GitLab


From aa4725d18b71c11388232aaa1a1c81e942ec7a20 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Mon, 26 Sep 2022 14:36:55 +0800
Subject: [PATCH 02/45] radeonsi: implement
 nir_intrinsic_load_provoking_vtx_in_prim_amd
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/gallium/drivers/radeonsi/si_shader_llvm.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index 075f8d13dbf0..681a6af904d5 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -900,6 +900,9 @@ static LLVMValueRef si_llvm_load_intrinsic(struct ac_shader_abi *abi, nir_intrin
       else
          return ctx->ac.i1true;
 
+   case nir_intrinsic_load_provoking_vtx_in_prim_amd:
+      return GET_FIELD(ctx, GS_STATE_PROVOKING_VTX_INDEX);
+
    case nir_intrinsic_load_pipeline_stat_query_enabled_amd: {
       LLVMValueRef enabled = GET_FIELD(ctx, GS_STATE_PIPELINE_STATS_EMU);
       return LLVMBuildTrunc(ctx->ac.builder, enabled, ctx->ac.i1, "");
-- 
GitLab


From 5a2b7e47e98b5c1276eb4fc9ef2909a7fc19552e Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Sat, 11 Jun 2022 15:29:50 +0800
Subject: [PATCH 03/45] radeonsi: implement export_vertex abi
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Used by ngg lower.

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 .../drivers/radeonsi/gfx10_shader_ngg.c       | 29 +++++++++++++++++++
 src/gallium/drivers/radeonsi/si_shader.c      |  9 ++++++
 .../drivers/radeonsi/si_shader_internal.h     |  1 +
 src/gallium/drivers/radeonsi/si_shader_llvm.c |  9 +++++-
 4 files changed, 47 insertions(+), 1 deletion(-)

diff --git a/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c b/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
index b78957c0fe9d..8ecfd74e24fa 100644
--- a/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
+++ b/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
@@ -1753,6 +1753,35 @@ void gfx10_ngg_build_end(struct si_shader_context *ctx)
    ac_build_endif(&ctx->ac, 6002);
 }
 
+void gfx10_ngg_export_vertex(struct ac_shader_abi *abi)
+{
+   struct si_shader_context *ctx = si_shader_context_from_abi(abi);
+   struct si_shader_info *info = &ctx->shader->selector->info;
+   struct si_shader_output_values outputs[PIPE_MAX_SHADER_OUTPUTS];
+   LLVMValueRef *addrs = ctx->abi.outputs;
+
+   unsigned num_outputs = info->num_outputs;
+   /* if needed, nir ngg lower will append primitive id export at last */
+   if (ctx->shader->key.ge.mono.u.vs_export_prim_id)
+      num_outputs++;
+
+   for (unsigned i = 0; i < num_outputs; i++) {
+      if (i < info->num_outputs) {
+         outputs[i].semantic = info->output_semantic[i];
+         outputs[i].vertex_streams = info->output_streams[i];
+      } else {
+         outputs[i].semantic = VARYING_SLOT_PRIMITIVE_ID;
+         outputs[i].vertex_streams = 0;
+      }
+
+      for (unsigned j = 0; j < 4; j++)
+         outputs[i].values[j] =
+            LLVMBuildLoad2(ctx->ac.builder, ctx->ac.f32, addrs[4 * i + j], "");
+   }
+
+   si_llvm_build_vs_exports(ctx, NULL, outputs, num_outputs);
+}
+
 void gfx10_ngg_atomic_add_prim_count(struct ac_shader_abi *abi, unsigned stream,
                                      LLVMValueRef prim_count, enum ac_prim_count count_type)
 {
diff --git a/src/gallium/drivers/radeonsi/si_shader.c b/src/gallium/drivers/radeonsi/si_shader.c
index feda45dade7d..dd21c9182def 100644
--- a/src/gallium/drivers/radeonsi/si_shader.c
+++ b/src/gallium/drivers/radeonsi/si_shader.c
@@ -1682,6 +1682,15 @@ static void si_nir_assign_param_offsets(nir_shader *nir, struct si_shader *shade
          assert(intr->num_components == 1); /* only scalar stores expected */
          nir_io_semantics sem = nir_intrinsic_io_semantics(intr);
 
+         /* primitive id output is added by ngg lowering, so we don't have its
+          * output info pre-build in si_shader_info. It's handled at last of
+          * this function.
+          */
+         if ((nir->info.stage == MESA_SHADER_VERTEX ||
+              nir->info.stage == MESA_SHADER_TESS_EVAL) &&
+             sem.location == VARYING_SLOT_PRIMITIVE_ID)
+            continue;
+
          /* Assign the param index if it's unassigned. */
          if (nir_slot_is_varying(sem.location) && !sem.no_varying &&
              (sem.gs_streams & 0x3) == 0 &&
diff --git a/src/gallium/drivers/radeonsi/si_shader_internal.h b/src/gallium/drivers/radeonsi/si_shader_internal.h
index daa3766c69cb..405deed969b6 100644
--- a/src/gallium/drivers/radeonsi/si_shader_internal.h
+++ b/src/gallium/drivers/radeonsi/si_shader_internal.h
@@ -186,6 +186,7 @@ void gfx10_ngg_build_export_prim(struct si_shader_context *ctx, LLVMValueRef use
                                  LLVMValueRef prim_passthrough);
 void gfx10_ngg_culling_build_end(struct si_shader_context *ctx);
 void gfx10_ngg_build_end(struct si_shader_context *ctx);
+void gfx10_ngg_export_vertex(struct ac_shader_abi *abi);
 void gfx10_ngg_atomic_add_prim_count(struct ac_shader_abi *abi, unsigned stream,
                                      LLVMValueRef prim_count, enum ac_prim_count count_type);
 void gfx10_ngg_gs_emit_vertex(struct si_shader_context *ctx, unsigned stream,
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index 681a6af904d5..ea61cdd1e2cc 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -965,6 +965,7 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
    ctx->abi.intrinsic_load = si_llvm_load_intrinsic;
    ctx->abi.load_user_clip_plane = si_llvm_load_user_clip_plane;
    ctx->abi.load_streamout_buffer = si_llvm_load_streamout_buffer;
+   ctx->abi.export_vertex = gfx10_ngg_export_vertex;
    ctx->abi.atomic_add_prim_count = gfx10_ngg_atomic_add_prim_count;
 
    si_llvm_init_resource_callbacks(ctx);
@@ -1217,7 +1218,13 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
                                 info->options & SI_PROFILE_CLAMP_DIV_BY_ZERO;
    ctx->abi.use_waterfall_for_divergent_tex_samplers = true;
 
-   for (unsigned i = 0; i < info->num_outputs; i++) {
+   unsigned num_outputs = info->num_outputs;
+   /* need extra output to hold primitive id added by nir ngg lower */
+   if (ctx->stage <= MESA_SHADER_GEOMETRY && shader->key.ge.as_ngg &&
+       ctx->shader->key.ge.mono.u.vs_export_prim_id)
+      num_outputs++;
+
+   for (unsigned i = 0; i < num_outputs; i++) {
       LLVMTypeRef type = ctx->ac.f32;
 
       /* Only FS uses unpacked f16. Other stages pack 16-bit outputs into low and high bits of f32. */
-- 
GitLab


From 818d74d5a991fa62a9aaf91ad0d5be0dc41aae34 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Fri, 22 Jul 2022 20:01:26 +0800
Subject: [PATCH 04/45] radeonsi: implement two lds base load intrinsics
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

LDS will be accessed starting from esgs_ring which has offset 0.
So ngg_scratch and ngg_emit base address is just the offset from
the esgs_ring base.

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/llvm/ac_nir_to_llvm.c                 | 2 ++
 src/gallium/drivers/radeonsi/si_shader_llvm.c | 6 ++++++
 2 files changed, 8 insertions(+)

diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index ceadbbbb4a69..fa303c9ad373 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -3645,6 +3645,8 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
    case nir_intrinsic_load_prim_gen_query_enabled_amd:
    case nir_intrinsic_load_prim_xfb_query_enabled_amd:
    case nir_intrinsic_load_clamp_vertex_color_amd:
+   case nir_intrinsic_load_lds_ngg_scratch_base_amd:
+   case nir_intrinsic_load_lds_ngg_gs_out_vertex_base_amd:
       result = ctx->abi->intrinsic_load(ctx->abi, instr->intrinsic);
       break;
    case nir_intrinsic_load_user_clip_plane:
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index ea61cdd1e2cc..94abe102a112 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -922,6 +922,12 @@ static LLVMValueRef si_llvm_load_intrinsic(struct ac_shader_abi *abi, nir_intrin
    case nir_intrinsic_load_ring_attr_amd:
       return si_llvm_build_attr_ring_desc(ctx);
 
+   case nir_intrinsic_load_lds_ngg_scratch_base_amd:
+      return LLVMBuildBitCast(ctx->ac.builder, ctx->gs_ngg_scratch.value, ctx->ac.i32, "");
+
+   case nir_intrinsic_load_lds_ngg_gs_out_vertex_base_amd:
+      return LLVMBuildBitCast(ctx->ac.builder, ctx->gs_ngg_emit, ctx->ac.i32, "");
+
    default:
       return NULL;
    }
-- 
GitLab


From 05fb00f1cd2ecd7c10fbda53a67fc65e47279d60 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Sat, 23 Jul 2022 18:30:45 +0800
Subject: [PATCH 05/45] radeonsi: fix NGG VS primitive ID load
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

When NGG VS need to export primitive ID, it will load it in GS
threads, so need to use gs_prim_id arg. Current nir to llvm
translator check vs_prim_id present to use vs_prim_id first.

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/gallium/drivers/radeonsi/si_shader.c | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/src/gallium/drivers/radeonsi/si_shader.c b/src/gallium/drivers/radeonsi/si_shader.c
index dd21c9182def..73dd8d4ee0df 100644
--- a/src/gallium/drivers/radeonsi/si_shader.c
+++ b/src/gallium/drivers/radeonsi/si_shader.c
@@ -334,7 +334,8 @@ static void declare_vs_input_vgprs(struct si_shader_context *ctx, unsigned *num_
    } else if (ctx->screen->info.gfx_level >= GFX10) {
       ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, NULL); /* user VGPR */
       ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT,
-                 &ctx->args.vs_prim_id); /* user vgpr or PrimID (legacy) */
+                 /* user vgpr or PrimID (legacy) */
+                 shader->key.ge.as_ngg ? NULL : &ctx->args.vs_prim_id);
       ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.instance_id);
    } else {
       ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.instance_id);
-- 
GitLab


From 011dd5683353f1592781218d7324037553dbbb05 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Sun, 12 Jun 2022 20:36:39 +0800
Subject: [PATCH 06/45] radeonsi: replace llvm ngg vs/tes with nir lowering
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 .../drivers/radeonsi/gfx10_shader_ngg.c       |  66 ++++++-
 src/gallium/drivers/radeonsi/si_shader.c      | 122 +++++++-----
 src/gallium/drivers/radeonsi/si_shader.h      |   3 +-
 .../drivers/radeonsi/si_shader_internal.h     |  14 +-
 src/gallium/drivers/radeonsi/si_shader_llvm.c | 175 ++++++------------
 .../drivers/radeonsi/si_shader_llvm_gs.c      |   2 +-
 .../drivers/radeonsi/si_shader_llvm_vs.c      |  18 +-
 src/gallium/drivers/radeonsi/si_shader_nir.c  |  21 ++-
 8 files changed, 215 insertions(+), 206 deletions(-)

diff --git a/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c b/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
index 8ecfd74e24fa..c2dc9d4fd6f1 100644
--- a/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
+++ b/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
@@ -119,6 +119,37 @@ static LLVMValueRef ngg_get_vertices_per_prim(struct si_shader_context *ctx, uns
    }
 }
 
+unsigned gfx10_ngg_get_vertices_per_prim(struct si_shader *shader)
+{
+   const struct si_shader_info *info = &shader->selector->info;
+
+   if (shader->selector->stage == MESA_SHADER_GEOMETRY)
+      return u_vertices_per_prim(info->base.gs.output_primitive);
+   else if (shader->selector->stage == MESA_SHADER_VERTEX) {
+      if (info->base.vs.blit_sgprs_amd) {
+         /* Blits always use axis-aligned rectangles with 3 vertices. */
+         return 3;
+      } else if (shader->key.ge.opt.ngg_culling & SI_NGG_CULL_LINES)
+         return 2;
+      else {
+         /* We always build up all three indices for the prim export
+          * independent of the primitive type. The additional garbage
+          * data shouldn't hurt. This is used by exports and streamout.
+          */
+         return 3;
+      }
+   } else {
+      assert(shader->selector->stage == MESA_SHADER_TESS_EVAL);
+
+      if (info->base.tess.point_mode)
+         return 1;
+      else if (info->base.tess._primitive_mode == TESS_PRIMITIVE_ISOLINES)
+         return 2;
+      else
+         return 3;
+   }
+}
+
 bool gfx10_ngg_export_prim_early(struct si_shader *shader)
 {
    struct si_shader_selector *sel = shader->selector;
@@ -2398,11 +2429,17 @@ static void clamp_gsprims_to_esverts(unsigned *max_gsprims, unsigned max_esverts
 unsigned gfx10_ngg_get_scratch_dw_size(struct si_shader *shader)
 {
    const struct si_shader_selector *sel = shader->selector;
+   bool uses_streamout = si_shader_uses_streamout(shader);
 
-   if (sel->stage == MESA_SHADER_GEOMETRY && si_shader_uses_streamout(shader))
-      return 44;
-
-   return 8;
+   if (sel->stage == MESA_SHADER_GEOMETRY) {
+      return uses_streamout ? 44 : 8;
+   } else {
+      return ac_ngg_get_scratch_lds_size(sel->stage,
+                                         si_get_max_workgroup_size(shader),
+                                         shader->wave_size,
+                                         uses_streamout,
+                                         shader->key.ge.opt.ngg_culling) / 4;
+   }
 }
 
 /**
@@ -2469,8 +2506,25 @@ retry_select_mode:
       }
    } else {
       /* VS and TES. */
-      /* LDS size for passing data from ES to GS. */
-      esvert_lds_size = ngg_nogs_vertex_size(shader);
+
+      bool uses_instance_id = gs_sel->info.uses_instanceid;
+      bool uses_primitive_id = gs_sel->info.uses_primid;
+      if (gs_stage == MESA_SHADER_VERTEX) {
+         uses_instance_id |=
+            shader->key.ge.part.vs.prolog.instance_divisor_is_one ||
+            shader->key.ge.part.vs.prolog.instance_divisor_is_fetched;
+      } else {
+         uses_primitive_id |= shader->key.ge.mono.u.vs_export_prim_id;
+      }
+
+      esvert_lds_size = ac_ngg_nogs_get_pervertex_lds_size(
+         gs_stage, gs_sel->info.num_outputs,
+         si_shader_uses_streamout(shader),
+         shader->key.ge.mono.u.vs_export_prim_id,
+         gfx10_ngg_writes_user_edgeflags(shader),
+         shader->key.ge.opt.ngg_culling,
+         uses_instance_id,
+         uses_primitive_id) / 4;
    }
 
    unsigned max_gsprims = max_gsprims_base;
diff --git a/src/gallium/drivers/radeonsi/si_shader.c b/src/gallium/drivers/radeonsi/si_shader.c
index 73dd8d4ee0df..ef8643655114 100644
--- a/src/gallium/drivers/radeonsi/si_shader.c
+++ b/src/gallium/drivers/radeonsi/si_shader.c
@@ -227,7 +227,7 @@ unsigned si_get_max_workgroup_size(const struct si_shader *shader)
    switch (shader->selector->stage) {
    case MESA_SHADER_VERTEX:
    case MESA_SHADER_TESS_EVAL:
-      return shader->key.ge.as_ngg ? 128 : 0;
+      return shader->key.ge.as_ngg ? shader->selector->screen->ngg_subgroup_size : 0;
 
    case MESA_SHADER_TESS_CTRL:
       /* Return this so that LLVM doesn't remove s_barrier
@@ -397,7 +397,7 @@ void si_add_arg_checked(struct ac_shader_args *args, enum ac_arg_regfile file, u
    ac_add_arg(args, file, registers, type, arg);
 }
 
-void si_init_shader_args(struct si_shader_context *ctx, bool ngg_cull_shader)
+void si_init_shader_args(struct si_shader_context *ctx)
 {
    struct si_shader *shader = ctx->shader;
    unsigned i, num_returns, num_return_sgprs;
@@ -613,36 +613,12 @@ void si_init_shader_args(struct si_shader_context *ctx, bool ngg_cull_shader)
          declare_tes_input_vgprs(ctx);
       }
 
-      if ((ctx->shader->key.ge.as_es || ngg_cull_shader) &&
+      if (ctx->shader->key.ge.as_es &&
           (ctx->stage == MESA_SHADER_VERTEX || ctx->stage == MESA_SHADER_TESS_EVAL)) {
-         unsigned num_user_sgprs, num_vgprs;
-
-         if (ctx->stage == MESA_SHADER_VERTEX && ngg_cull_shader) {
-            /* For the NGG cull shader, add 1 SGPR to hold
-             * the vertex buffer pointer.
-             */
-            num_user_sgprs = GFX9_GS_NUM_USER_SGPR + 1;
-
-            if (shader->selector->info.num_vbos_in_user_sgprs) {
-               assert(num_user_sgprs <= SI_SGPR_VS_VB_DESCRIPTOR_FIRST);
-               num_user_sgprs =
-                  SI_SGPR_VS_VB_DESCRIPTOR_FIRST + shader->selector->info.num_vbos_in_user_sgprs * 4;
-            }
-         } else {
-            num_user_sgprs = GFX9_GS_NUM_USER_SGPR;
-         }
-
-         /* The NGG cull shader has to return all 9 VGPRs.
-          *
-          * The normal merged ESGS shader only has to return the 5 VGPRs
-          * for the GS stage.
-          */
-         num_vgprs = ngg_cull_shader ? 9 : 5;
-
          /* ES return values are inputs to GS. */
-         for (i = 0; i < 8 + num_user_sgprs; i++)
+         for (i = 0; i < 8 + GFX9_GS_NUM_USER_SGPR; i++)
             ac_add_return(&ctx->args, AC_ARG_SGPR);
-         for (i = 0; i < num_vgprs; i++)
+         for (i = 0; i < 5; i++)
             ac_add_return(&ctx->args, AC_ARG_VGPR);
       }
       break;
@@ -1403,17 +1379,13 @@ static void si_dump_shader_key(const struct si_shader *shader, FILE *f)
 }
 
 bool si_vs_needs_prolog(const struct si_shader_selector *sel,
-                        const struct si_vs_prolog_bits *prolog_key,
-                        const union si_shader_key *key, bool ngg_cull_shader,
-                        bool is_gs)
+                        const struct si_vs_prolog_bits *prolog_key)
 {
    assert(sel->stage == MESA_SHADER_VERTEX);
 
    /* VGPR initialization fixup for Vega10 and Raven is always done in the
     * VS prolog. */
-   return sel->info.vs_needs_prolog || prolog_key->ls_vgpr_fix ||
-          /* The 2nd VS prolog loads input VGPRs from LDS */
-          (key->ge.opt.ngg_culling && !ngg_cull_shader && !is_gs);
+   return sel->info.vs_needs_prolog || prolog_key->ls_vgpr_fix;
 }
 
 /**
@@ -1422,13 +1394,12 @@ bool si_vs_needs_prolog(const struct si_shader_selector *sel,
  *
  * \param info             Shader info of the vertex shader.
  * \param num_input_sgprs  Number of input SGPRs for the vertex shader.
- * \param has_old_  Whether the preceding shader part is the NGG cull shader.
  * \param prolog_key       Key of the VS prolog
  * \param shader_out       The vertex shader, or the next shader if merging LS+HS or ES+GS.
  * \param key              Output shader part key.
  */
 void si_get_vs_prolog_key(const struct si_shader_info *info, unsigned num_input_sgprs,
-                          bool ngg_cull_shader, const struct si_vs_prolog_bits *prolog_key,
+                          const struct si_vs_prolog_bits *prolog_key,
                           struct si_shader *shader_out, union si_shader_part_key *key)
 {
    memset(key, 0, sizeof(*key));
@@ -1440,10 +1411,6 @@ void si_get_vs_prolog_key(const struct si_shader_info *info, unsigned num_input_
    key->vs_prolog.as_es = shader_out->key.ge.as_es;
    key->vs_prolog.as_ngg = shader_out->key.ge.as_ngg;
 
-   if (shader_out->selector->stage != MESA_SHADER_GEOMETRY &&
-       !ngg_cull_shader && shader_out->key.ge.opt.ngg_culling)
-      key->vs_prolog.load_vgprs_after_culling = 1;
-
    if (shader_out->selector->stage == MESA_SHADER_TESS_CTRL) {
       key->vs_prolog.as_ls = 1;
       key->vs_prolog.num_merged_next_stage_vgprs = 2;
@@ -1647,6 +1614,68 @@ static bool si_lower_io_to_mem(struct si_shader *shader, nir_shader *nir,
    return false;
 }
 
+static void si_lower_ngg(struct si_shader *shader, nir_shader *nir)
+{
+   struct si_shader_selector *sel = shader->selector;
+   const union si_shader_key *key = &shader->key;
+   assert(key->ge.as_ngg);
+
+   ac_nir_lower_ngg_options options = {
+      .family = sel->screen->info.family,
+      .gfx_level = sel->screen->info.gfx_level,
+      .max_workgroup_size = si_get_max_workgroup_size(shader),
+      .wave_size = shader->wave_size,
+      .can_cull = !!key->ge.opt.ngg_culling,
+      .disable_streamout = key->ge.opt.remove_streamout,
+      .vs_output_param_offset = shader->info.vs_output_param_offset,
+   };
+
+   if (nir->info.stage == MESA_SHADER_VERTEX ||
+       nir->info.stage == MESA_SHADER_TESS_EVAL) {
+      /* Per instance inputs, used to remove instance load after culling. */
+      unsigned instance_rate_inputs = 0;
+
+      if (nir->info.stage == MESA_SHADER_VERTEX) {
+         instance_rate_inputs =
+            key->ge.part.vs.prolog.instance_divisor_is_one |
+            key->ge.part.vs.prolog.instance_divisor_is_fetched;
+
+         /* Manually mark the instance ID used, so the shader can repack it. */
+         if (instance_rate_inputs)
+            BITSET_SET(nir->info.system_values_read, SYSTEM_VALUE_INSTANCE_ID);
+      } else {
+         /* Manually mark the primitive ID used, so the shader can repack it. */
+         if (key->ge.mono.u.vs_export_prim_id)
+            BITSET_SET(nir->info.system_values_read, SYSTEM_VALUE_PRIMITIVE_ID);
+      }
+
+      unsigned clip_plane_enable =
+         SI_NGG_CULL_GET_CLIP_PLANE_ENABLE(key->ge.opt.ngg_culling);
+      unsigned clipdist_mask =
+         (sel->info.clipdist_mask & clip_plane_enable) | sel->info.culldist_mask;
+
+      options.num_vertices_per_primitive = gfx10_ngg_get_vertices_per_prim(shader);
+      options.early_prim_export = gfx10_ngg_export_prim_early(shader);
+      options.passthrough = gfx10_is_ngg_passthrough(shader);
+      options.use_edgeflags = gfx10_edgeflags_have_effect(shader);
+      options.has_gen_prim_query = options.has_xfb_prim_query =
+         sel->screen->use_ngg_streamout && !sel->info.base.vs.blit_sgprs_amd;
+      options.primitive_id_location =
+         key->ge.mono.u.vs_export_prim_id ? sel->info.num_outputs : -1;
+      options.instance_rate_inputs = instance_rate_inputs;
+      options.clipdist_enable_mask = clipdist_mask;
+      options.user_clip_plane_enable_mask = clip_plane_enable;
+
+      NIR_PASS_V(nir, ac_nir_lower_ngg_nogs, &options);
+   }
+
+   /* may generate some subgroup op like ballot */
+   NIR_PASS_V(nir, nir_lower_subgroups, &si_nir_subgroups_options);
+
+   /* may generate some vector output store */
+   NIR_PASS_V(nir, nir_lower_io_to_scalar, nir_var_shader_out);
+}
+
 struct nir_shader *si_deserialize_shader(struct si_shader_selector *sel)
 {
    struct pipe_screen *screen = &sel->screen->b;
@@ -1878,6 +1907,12 @@ struct nir_shader *si_get_nir_shader(struct si_shader *shader, bool *free_nir,
    if (is_last_vgt_stage)
       si_assign_param_offsets(nir, shader);
 
+   /* Only lower last VGT NGG shader stage. */
+   if (sel->stage < MESA_SHADER_GEOMETRY && key->ge.as_ngg && !key->ge.as_es) {
+      si_lower_ngg(shader, nir);
+      opt_offsets = true;
+   }
+
    if (progress2 || opt_offsets)
       si_nir_opts(sel->screen, nir, false);
 
@@ -2176,13 +2211,12 @@ static bool si_get_vs_prolog(struct si_screen *sscreen, struct ac_llvm_compiler
 {
    struct si_shader_selector *vs = main_part->selector;
 
-   if (!si_vs_needs_prolog(vs, key, &shader->key, false,
-                           shader->selector->stage == MESA_SHADER_GEOMETRY))
+   if (!si_vs_needs_prolog(vs, key))
       return true;
 
    /* Get the prolog. */
    union si_shader_part_key prolog_key;
-   si_get_vs_prolog_key(&vs->info, main_part->info.num_input_sgprs, false, key, shader,
+   si_get_vs_prolog_key(&vs->info, main_part->info.num_input_sgprs, key, shader,
                         &prolog_key);
 
    shader->prolog =
diff --git a/src/gallium/drivers/radeonsi/si_shader.h b/src/gallium/drivers/radeonsi/si_shader.h
index b1ff7fe654d4..83cad1a8e21e 100644
--- a/src/gallium/drivers/radeonsi/si_shader.h
+++ b/src/gallium/drivers/radeonsi/si_shader.h
@@ -613,7 +613,6 @@ union si_shader_part_key {
       unsigned as_ls : 1;
       unsigned as_es : 1;
       unsigned as_ngg : 1;
-      unsigned load_vgprs_after_culling : 1;
       /* Prologs for monolithic shaders shouldn't set EXEC. */
       unsigned is_monolithic : 1;
    } vs_prolog;
@@ -1002,6 +1001,8 @@ struct si_shader *si_generate_gs_copy_shader(struct si_screen *sscreen,
                                              struct util_debug_callback *debug);
 
 /* si_shader_nir.c */
+extern const nir_lower_subgroups_options si_nir_subgroups_options;
+
 void si_nir_opts(struct si_screen *sscreen, struct nir_shader *nir, bool first);
 void si_nir_late_opts(nir_shader *nir);
 char *si_finalize_nir(struct pipe_screen *screen, void *nirptr);
diff --git a/src/gallium/drivers/radeonsi/si_shader_internal.h b/src/gallium/drivers/radeonsi/si_shader_internal.h
index 405deed969b6..164c20a55be3 100644
--- a/src/gallium/drivers/radeonsi/si_shader_internal.h
+++ b/src/gallium/drivers/radeonsi/si_shader_internal.h
@@ -161,13 +161,12 @@ bool si_is_multi_part_shader(struct si_shader *shader);
 bool si_is_merged_shader(struct si_shader *shader);
 void si_add_arg_checked(struct ac_shader_args *args, enum ac_arg_regfile file, unsigned registers,
                         enum ac_arg_type type, struct ac_arg *arg, unsigned idx);
-void si_init_shader_args(struct si_shader_context *ctx, bool ngg_cull_shader);
+void si_init_shader_args(struct si_shader_context *ctx);
 unsigned si_get_max_workgroup_size(const struct si_shader *shader);
 bool si_vs_needs_prolog(const struct si_shader_selector *sel,
-                        const struct si_vs_prolog_bits *prolog_key,
-                        const union si_shader_key *key, bool ngg_cull_shader, bool is_gs);
+                        const struct si_vs_prolog_bits *prolog_key);
 void si_get_vs_prolog_key(const struct si_shader_info *info, unsigned num_input_sgprs,
-                          bool ngg_cull_shader, const struct si_vs_prolog_bits *prolog_key,
+                          const struct si_vs_prolog_bits *prolog_key,
                           struct si_shader *shader_out, union si_shader_part_key *key);
 struct nir_shader *si_get_nir_shader(struct si_shader *shader, bool *free_nir,
                                      uint64_t tcs_vgpr_only_inputs);
@@ -180,6 +179,7 @@ void si_fix_resource_usage(struct si_screen *sscreen, struct si_shader *shader);
 
 /* gfx10_shader_ngg.c */
 LLVMValueRef gfx10_get_thread_id_in_tg(struct si_shader_context *ctx);
+unsigned gfx10_ngg_get_vertices_per_prim(struct si_shader *shader);
 bool gfx10_ngg_export_prim_early(struct si_shader *shader);
 void gfx10_ngg_build_sendmsg_gs_alloc_req(struct si_shader_context *ctx);
 void gfx10_ngg_build_export_prim(struct si_shader_context *ctx, LLVMValueRef user_edgeflags[3],
@@ -205,7 +205,7 @@ void si_llvm_context_init(struct si_shader_context *ctx, struct si_screen *sscre
                           struct ac_llvm_compiler *compiler, unsigned wave_size);
 void si_llvm_create_func(struct si_shader_context *ctx, const char *name, LLVMTypeRef *return_types,
                          unsigned num_return_elems, unsigned max_workgroup_size);
-void si_llvm_create_main_func(struct si_shader_context *ctx, bool ngg_cull_shader);
+void si_llvm_create_main_func(struct si_shader_context *ctx);
 void si_llvm_optimize_module(struct si_shader_context *ctx);
 void si_llvm_dispose(struct si_shader_context *ctx);
 LLVMValueRef si_buffer_load_const(struct si_shader_context *ctx, LLVMValueRef resource,
@@ -228,7 +228,7 @@ void si_build_wrapper_function(struct si_shader_context *ctx, struct ac_llvm_poi
                                enum ac_arg_type *main_arg_types,
                                bool same_thread_count);
 bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shader,
-                           struct nir_shader *nir, bool free_nir, bool ngg_cull_shader);
+                           struct nir_shader *nir, bool free_nir);
 bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *compiler,
                             struct si_shader *shader, const struct pipe_stream_output_info *so,
                             struct util_debug_callback *debug, struct nir_shader *nir,
@@ -278,6 +278,6 @@ void si_llvm_build_vs_exports(struct si_shader_context *ctx, LLVMValueRef num_ex
                               struct si_shader_output_values *outputs, unsigned noutput);
 void si_llvm_vs_build_end(struct si_shader_context *ctx);
 void si_llvm_build_vs_prolog(struct si_shader_context *ctx, union si_shader_part_key *key);
-void si_llvm_init_vs_callbacks(struct si_shader_context *ctx, bool ngg_cull_shader);
+void si_llvm_init_vs_callbacks(struct si_shader_context *ctx);
 
 #endif
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index 94abe102a112..07d47691e168 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -197,21 +197,21 @@ void si_llvm_create_func(struct si_shader_context *ctx, const char *name, LLVMTy
    ac_llvm_set_target_features(ctx->main_fn.value, &ctx->ac);
 }
 
-void si_llvm_create_main_func(struct si_shader_context *ctx, bool ngg_cull_shader)
+void si_llvm_create_main_func(struct si_shader_context *ctx)
 {
    struct si_shader *shader = ctx->shader;
    LLVMTypeRef returns[AC_MAX_ARGS];
    unsigned i;
 
-   si_init_shader_args(ctx, ngg_cull_shader);
+   si_init_shader_args(ctx);
 
    for (i = 0; i < ctx->args.num_sgprs_returned; i++)
       returns[i] = ctx->ac.i32; /* SGPR */
    for (; i < ctx->args.return_count; i++)
       returns[i] = ctx->ac.f32; /* VGPR */
 
-   si_llvm_create_func(ctx, ngg_cull_shader ? "ngg_cull_main" : "main", returns,
-                       ctx->args.return_count, si_get_max_workgroup_size(shader));
+   si_llvm_create_func(ctx, "main", returns, ctx->args.return_count,
+                       si_get_max_workgroup_size(shader));
 
    /* Reserve register locations for VGPR inputs the PS prolog may need. */
    if (ctx->stage == MESA_SHADER_FRAGMENT && !ctx->shader->is_monolithic) {
@@ -954,7 +954,7 @@ static LLVMValueRef si_llvm_load_streamout_buffer(struct ac_shader_abi *abi, uns
 }
 
 bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shader,
-                           struct nir_shader *nir, bool free_nir, bool ngg_cull_shader)
+                           struct nir_shader *nir, bool free_nir)
 {
    struct si_shader_selector *sel = shader->selector;
    const struct si_shader_info *info = &sel->info;
@@ -975,7 +975,7 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
    ctx->abi.atomic_add_prim_count = gfx10_ngg_atomic_add_prim_count;
 
    si_llvm_init_resource_callbacks(ctx);
-   si_llvm_create_main_func(ctx, ngg_cull_shader);
+   si_llvm_create_main_func(ctx);
 
    if (ctx->stage <= MESA_SHADER_GEOMETRY &&
        (ctx->shader->key.ge.as_es || ctx->stage == MESA_SHADER_GEOMETRY))
@@ -983,7 +983,7 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
 
    switch (ctx->stage) {
    case MESA_SHADER_VERTEX:
-      si_llvm_init_vs_callbacks(ctx, ngg_cull_shader);
+      si_llvm_init_vs_callbacks(ctx);
 
       /* preload instance_divisor_constbuf to be used for input load after culling */
       if (ctx->shader->key.ge.opt.ngg_culling &&
@@ -1100,56 +1100,49 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
        * determined during linking / PM4 creation.
        */
       si_llvm_declare_esgs_ring(ctx);
+      ctx->ac.lds.value = ctx->esgs_ring;
+      ctx->ac.lds.pointee_type = ctx->ac.i32;
 
       /* This is really only needed when streamout and / or vertex
        * compaction is enabled.
        */
-      if (!ctx->gs_ngg_scratch.value && (ctx->so.num_outputs || shader->key.ge.opt.ngg_culling)) {
+      if (si_shader_uses_streamout(shader) || shader->key.ge.opt.ngg_culling) {
          LLVMTypeRef asi32 = LLVMArrayType(ctx->ac.i32, gfx10_ngg_get_scratch_dw_size(shader));
          ctx->gs_ngg_scratch = (struct ac_llvm_pointer) {
             .value = LLVMAddGlobalInAddressSpace(ctx->ac.module, asi32, "ngg_scratch", AC_ADDR_SPACE_LDS),
             .pointee_type = asi32
          };
          LLVMSetInitializer(ctx->gs_ngg_scratch.value, LLVMGetUndef(asi32));
-         LLVMSetAlignment(ctx->gs_ngg_scratch.value, 4);
+         LLVMSetAlignment(ctx->gs_ngg_scratch.value, 8);
       }
    }
 
    /* For merged shaders (VS-TCS, VS-GS, TES-GS): */
    if (ctx->screen->info.gfx_level >= GFX9 && si_is_merged_shader(shader)) {
-      /* TES is special because it has only 1 shader part if NGG shader culling is disabled,
-       * and therefore it doesn't use the wrapper function.
+      /* Set EXEC = ~0 before the first shader. For monolithic shaders, the wrapper
+       * function does this.
        */
-      bool no_wrapper_func = ctx->stage == MESA_SHADER_TESS_EVAL && !shader->key.ge.as_es &&
-                             !shader->key.ge.opt.ngg_culling;
+      if (ctx->stage == MESA_SHADER_TESS_EVAL) {
+         /* TES has only 1 shader part, therefore it doesn't use the wrapper function. */
+         if (!shader->is_monolithic || !shader->key.ge.as_es)
+            ac_init_exec_full_mask(&ctx->ac);
+      } else if (ctx->stage == MESA_SHADER_VERTEX) {
+         /* If the prolog is present, EXEC is set there instead. */
+         if (!si_vs_needs_prolog(sel, &shader->key.ge.part.vs.prolog)) {
+            /* When no prolog, only mono VS with TCS/GS present has wrapper function. */
+            if (!(shader->is_monolithic && (shader->key.ge.as_ls || shader->key.ge.as_es)))
+               ac_init_exec_full_mask(&ctx->ac);
+         }
+      }
 
-      /* Set EXEC = ~0 before the first shader. If the prolog is present, EXEC is set there
-       * instead. For monolithic shaders, the wrapper function does this.
-       */
-      if ((!shader->is_monolithic || no_wrapper_func) &&
-          (ctx->stage == MESA_SHADER_TESS_EVAL ||
-           (ctx->stage == MESA_SHADER_VERTEX &&
-            !si_vs_needs_prolog(sel, &shader->key.ge.part.vs.prolog, &shader->key, ngg_cull_shader,
-                                false))))
-         ac_init_exec_full_mask(&ctx->ac);
-
-      /* NGG VS and NGG TES: Send gs_alloc_req and the prim export at the beginning to decrease
-       * register usage.
+      /* NGG VS and NGG TES: nir ngg lowering send gs_alloc_req at the beginning when culling
+       * is disabled, but GFX10 may hang if not all waves are launched before gs_alloc_req.
+       * We work around this HW bug by inserting a barrier before gs_alloc_req.
        */
-      if ((ctx->stage == MESA_SHADER_VERTEX || ctx->stage == MESA_SHADER_TESS_EVAL) &&
-          shader->key.ge.as_ngg && !shader->key.ge.as_es && !shader->key.ge.opt.ngg_culling) {
-         /* GFX10 requires a barrier before gs_alloc_req due to a hw bug. */
-         if (ctx->screen->info.gfx_level == GFX10)
-            ac_build_s_barrier(&ctx->ac, ctx->stage);
-
-         gfx10_ngg_build_sendmsg_gs_alloc_req(ctx);
-
-         /* Build the primitive export at the beginning
-          * of the shader if possible.
-          */
-         if (gfx10_ngg_export_prim_early(shader))
-            gfx10_ngg_build_export_prim(ctx, NULL, NULL);
-      }
+      if (ctx->screen->info.gfx_level == GFX10 &&
+          (ctx->stage == MESA_SHADER_VERTEX || ctx->stage == MESA_SHADER_TESS_EVAL) &&
+          shader->key.ge.as_ngg && !shader->key.ge.as_es && !shader->key.ge.opt.ngg_culling)
+         ac_build_s_barrier(&ctx->ac, ctx->stage);
 
       /* NGG GS: Initialize LDS and insert s_barrier, which must not be inside the if statement. */
       if (ctx->stage == MESA_SHADER_GEOMETRY && shader->key.ge.as_ngg)
@@ -1164,10 +1157,8 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
           * not here.
           */
          thread_enabled = si_is_gs_thread(ctx); /* 2nd shader: thread enabled bool */
-      } else if (((shader->key.ge.as_ls || shader->key.ge.as_es) && !shader->is_monolithic) ||
-                 (shader->key.ge.as_ngg && !shader->key.ge.as_es)) {
-         /* This is NGG VS or NGG TES or VS before GS or TES before GS or VS before TCS.
-          * For monolithic LS (VS before TCS) and ES (VS before GS and TES before GS),
+      } else if ((shader->key.ge.as_ls || shader->key.ge.as_es) && !shader->is_monolithic) {
+         /* For monolithic LS (VS before TCS) and ES (VS before GS and TES before GS),
           * the if statement is inserted by the wrapper function.
           */
          thread_enabled = si_is_es_thread(ctx); /* 1st shader: thread enabled bool */
@@ -1253,11 +1244,7 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
          si_llvm_ls_build_end(ctx);
       else if (shader->key.ge.as_es)
          si_llvm_es_build_end(ctx);
-      else if (ngg_cull_shader)
-         gfx10_ngg_culling_build_end(ctx);
-      else if (shader->key.ge.as_ngg)
-         gfx10_ngg_build_end(ctx);
-      else
+      else if (!shader->key.ge.as_ngg)
          si_llvm_vs_build_end(ctx);
       break;
 
@@ -1268,11 +1255,7 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
    case MESA_SHADER_TESS_EVAL:
       if (ctx->shader->key.ge.as_es)
          si_llvm_es_build_end(ctx);
-      else if (ngg_cull_shader)
-         gfx10_ngg_culling_build_end(ctx);
-      else if (ctx->shader->key.ge.as_ngg)
-         gfx10_ngg_build_end(ctx);
-      else
+      else if (!ctx->shader->key.ge.as_ngg)
          si_llvm_vs_build_end(ctx);
       break;
 
@@ -1323,84 +1306,30 @@ bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *
    si_llvm_context_init(&ctx, sscreen, compiler, shader->wave_size);
    ctx.so = *so;
 
-   struct ac_llvm_pointer ngg_cull_main_fn = {};
-   if (sel->stage <= MESA_SHADER_TESS_EVAL && shader->key.ge.opt.ngg_culling) {
-      if (!si_llvm_translate_nir(&ctx, shader, nir, false, true)) {
-         si_llvm_dispose(&ctx);
-         return false;
-      }
-      ngg_cull_main_fn = ctx.main_fn;
-      ctx.main_fn.value = NULL;
-   }
-
-   if (!si_llvm_translate_nir(&ctx, shader, nir, free_nir, false)) {
+   if (!si_llvm_translate_nir(&ctx, shader, nir, free_nir)) {
       si_llvm_dispose(&ctx);
       return false;
    }
 
-   if (shader->is_monolithic && sel->stage == MESA_SHADER_VERTEX) {
-      struct ac_llvm_pointer parts[4];
-      unsigned num_parts = 0;
-      bool first_is_prolog = false;
-      struct ac_llvm_pointer main_fn = ctx.main_fn;
+   if (shader->is_monolithic && sel->stage == MESA_SHADER_VERTEX &&
+       si_vs_needs_prolog(sel, &shader->key.ge.part.vs.prolog)) {
+      struct ac_llvm_pointer parts[2];
+      parts[1] = ctx.main_fn;
 
-      /* Preserve main arguments. */
-      enum ac_arg_type main_arg_types[AC_MAX_ARGS];
-      for (int i = 0; i < ctx.args.arg_count; i++)
-         main_arg_types[i] = ctx.args.args[i].type;
-      main_arg_types[MIN2(AC_MAX_ARGS - 1, ctx.args.arg_count)] = AC_ARG_INVALID;
-
-      if (ngg_cull_main_fn.value) {
-         if (si_vs_needs_prolog(sel, &shader->key.ge.part.vs.prolog, &shader->key, true, false)) {
-            union si_shader_part_key prolog_key;
-            si_get_vs_prolog_key(&sel->info, shader->info.num_input_sgprs, true,
-                                 &shader->key.ge.part.vs.prolog, shader, &prolog_key);
-            prolog_key.vs_prolog.is_monolithic = true;
-            si_llvm_build_vs_prolog(&ctx, &prolog_key);
-            parts[num_parts++] = ctx.main_fn;
-            first_is_prolog = true;
-         }
-         parts[num_parts++] = ngg_cull_main_fn;
-      }
-
-      if (si_vs_needs_prolog(sel, &shader->key.ge.part.vs.prolog, &shader->key, false, false)) {
-         union si_shader_part_key prolog_key;
-         si_get_vs_prolog_key(&sel->info, shader->info.num_input_sgprs, false,
-                              &shader->key.ge.part.vs.prolog, shader, &prolog_key);
-         prolog_key.vs_prolog.is_monolithic = true;
-         si_llvm_build_vs_prolog(&ctx, &prolog_key);
-         parts[num_parts++] = ctx.main_fn;
-         if (num_parts == 1)
-            first_is_prolog = true;
-      }
-      parts[num_parts++] = main_fn;
-
-      si_build_wrapper_function(&ctx, parts, num_parts, first_is_prolog ? 1 : 0, 0, main_arg_types, false);
-   } else if (shader->is_monolithic && sel->stage == MESA_SHADER_TESS_EVAL && ngg_cull_main_fn.value) {
-      struct ac_llvm_pointer parts[3], prolog, main_fn = ctx.main_fn;
-
-      /* Preserve main arguments. */
+       /* Preserve main arguments. */
       enum ac_arg_type main_arg_types[AC_MAX_ARGS];
       for (int i = 0; i < ctx.args.arg_count; i++)
          main_arg_types[i] = ctx.args.args[i].type;
       main_arg_types[MIN2(AC_MAX_ARGS - 1, ctx.args.arg_count)] = AC_ARG_INVALID;
 
-      /* We reuse the VS prolog code for TES just to load the input VGPRs from LDS. */
       union si_shader_part_key prolog_key;
-      memset(&prolog_key, 0, sizeof(prolog_key));
-      prolog_key.vs_prolog.num_input_sgprs = shader->info.num_input_sgprs;
-      prolog_key.vs_prolog.num_merged_next_stage_vgprs = 5;
-      prolog_key.vs_prolog.as_ngg = 1;
-      prolog_key.vs_prolog.load_vgprs_after_culling = 1;
+      si_get_vs_prolog_key(&sel->info, shader->info.num_input_sgprs,
+                           &shader->key.ge.part.vs.prolog, shader, &prolog_key);
       prolog_key.vs_prolog.is_monolithic = true;
       si_llvm_build_vs_prolog(&ctx, &prolog_key);
-      prolog = ctx.main_fn;
-
-      parts[0] = ngg_cull_main_fn;
-      parts[1] = prolog;
-      parts[2] = main_fn;
+      parts[0] = ctx.main_fn;
 
-      si_build_wrapper_function(&ctx, parts, 3, 0, 0, main_arg_types, false);
+      si_build_wrapper_function(&ctx, parts, 2, 1, 0, main_arg_types, false);
    } else if (shader->is_monolithic && sel->stage == MESA_SHADER_TESS_CTRL) {
       /* Preserve main arguments. */
       enum ac_arg_type main_arg_types[AC_MAX_ARGS];
@@ -1409,7 +1338,7 @@ bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *
          struct si_shader_selector *ls = shader->key.ge.part.tcs.ls;
          struct ac_llvm_pointer parts[4];
          bool vs_needs_prolog =
-            si_vs_needs_prolog(ls, &shader->key.ge.part.tcs.ls_prolog, &shader->key, false, false);
+            si_vs_needs_prolog(ls, &shader->key.ge.part.tcs.ls_prolog);
 
          /* TCS main part */
          parts[2] = ctx.main_fn;
@@ -1432,7 +1361,7 @@ bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *
          nir = si_get_nir_shader(&shader_ls, &free_nir, sel->info.tcs_vgpr_only_inputs);
          si_update_shader_binary_info(shader, nir);
 
-         if (!si_llvm_translate_nir(&ctx, &shader_ls, nir, free_nir, false)) {
+         if (!si_llvm_translate_nir(&ctx, &shader_ls, nir, free_nir)) {
             si_llvm_dispose(&ctx);
             return false;
          }
@@ -1446,7 +1375,7 @@ bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *
          /* LS prolog */
          if (vs_needs_prolog) {
             union si_shader_part_key vs_prolog_key;
-            si_get_vs_prolog_key(&ls->info, shader_ls.info.num_input_sgprs, false,
+            si_get_vs_prolog_key(&ls->info, shader_ls.info.num_input_sgprs,
                                  &shader->key.ge.part.tcs.ls_prolog, shader, &vs_prolog_key);
             vs_prolog_key.vs_prolog.is_monolithic = true;
             si_llvm_build_vs_prolog(&ctx, &vs_prolog_key);
@@ -1503,7 +1432,7 @@ bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *
          nir = si_get_nir_shader(&shader_es, &free_nir, 0);
          si_update_shader_binary_info(shader, nir);
 
-         if (!si_llvm_translate_nir(&ctx, &shader_es, nir, free_nir, false)) {
+         if (!si_llvm_translate_nir(&ctx, &shader_es, nir, free_nir)) {
             si_llvm_dispose(&ctx);
             return false;
          }
@@ -1517,9 +1446,9 @@ bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *
 
          /* ES prolog */
          if (es->stage == MESA_SHADER_VERTEX &&
-             si_vs_needs_prolog(es, &shader->key.ge.part.gs.vs_prolog, &shader->key, false, true)) {
+             si_vs_needs_prolog(es, &shader->key.ge.part.gs.vs_prolog)) {
             union si_shader_part_key vs_prolog_key;
-            si_get_vs_prolog_key(&es->info, shader_es.info.num_input_sgprs, false,
+            si_get_vs_prolog_key(&es->info, shader_es.info.num_input_sgprs,
                                  &shader->key.ge.part.gs.vs_prolog, shader, &vs_prolog_key);
             vs_prolog_key.vs_prolog.is_monolithic = true;
             si_llvm_build_vs_prolog(&ctx, &vs_prolog_key);
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c b/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c
index c5b1c123231d..7deecd389d0e 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c
@@ -462,7 +462,7 @@ struct si_shader *si_generate_gs_copy_shader(struct si_screen *sscreen,
    builder = ctx.ac.builder;
 
    /* Build the main function. */
-   si_llvm_create_main_func(&ctx, false);
+   si_llvm_create_main_func(&ctx);
 
    ctx.gsvs_ring[0] =
       ac_build_load_to_sgpr(&ctx.ac,
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c b/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c
index 31ba30d5f32f..12d454489748 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c
@@ -698,7 +698,9 @@ void si_llvm_build_vs_exports(struct si_shader_context *ctx, LLVMValueRef num_ex
       ac_build_export(&ctx->ac, &pos_args[i]);
    }
 
-   if (!shader->info.nr_param_exports)
+   if (!shader->info.nr_param_exports ||
+       /* GFX11 VS/TES param export is handled in nir */
+       (ctx->screen->info.gfx_level >= GFX11 && ctx->stage != MESA_SHADER_GEOMETRY))
       return;
 
    /* Build parameter exports. Use 2 loops to export params in ascending order.
@@ -895,18 +897,6 @@ void si_llvm_build_vs_prolog(struct si_shader_context *ctx, union si_shader_part
       }
    }
 
-   /* The culling code stored the LDS addresses of the VGPRs into those VGPRs. Load them. */
-   if (key->vs_prolog.load_vgprs_after_culling) {
-      for (i = 5; i <= 8; i++) {
-         bool is_tes_rel_patch_id = i == 7;
-         LLVMTypeRef t = is_tes_rel_patch_id ? ctx->ac.i8 : ctx->ac.i32;
-         input_vgprs[i] = LLVMBuildIntToPtr(ctx->ac.builder, input_vgprs[i], LLVMPointerType(t, AC_ADDR_SPACE_LDS), "");
-         input_vgprs[i] = LLVMBuildLoad2(ctx->ac.builder, t, input_vgprs[i], "");
-         if (is_tes_rel_patch_id)
-            input_vgprs[i] = LLVMBuildZExt(ctx->ac.builder, input_vgprs[i], ctx->ac.i32, "");
-      }
-   }
-
    unsigned vertex_id_vgpr = first_vs_vgpr;
    unsigned instance_id_vgpr = ctx->screen->info.gfx_level >= GFX10
                                   ? first_vs_vgpr + 3
@@ -960,7 +950,7 @@ void si_llvm_build_vs_prolog(struct si_shader_context *ctx, union si_shader_part
    si_llvm_build_ret(ctx, ret);
 }
 
-void si_llvm_init_vs_callbacks(struct si_shader_context *ctx, bool ngg_cull_shader)
+void si_llvm_init_vs_callbacks(struct si_shader_context *ctx)
 {
    ctx->abi.load_inputs = si_load_vs_input;
 }
diff --git a/src/gallium/drivers/radeonsi/si_shader_nir.c b/src/gallium/drivers/radeonsi/si_shader_nir.c
index b0c2e4ff5b52..413dce3cfa4f 100644
--- a/src/gallium/drivers/radeonsi/si_shader_nir.c
+++ b/src/gallium/drivers/radeonsi/si_shader_nir.c
@@ -240,6 +240,16 @@ static bool si_lower_intrinsics(nir_shader *nir)
                                         NULL);
 }
 
+const nir_lower_subgroups_options si_nir_subgroups_options = {
+   .subgroup_size = 64,
+   .ballot_bit_size = 64,
+   .ballot_components = 1,
+   .lower_to_scalar = true,
+   .lower_subgroup_masks = true,
+   .lower_vote_trivial = false,
+   .lower_vote_eq = true,
+};
+
 /**
  * Perform "lowering" operations on the NIR that are run once when the shader
  * selector is created.
@@ -269,16 +279,7 @@ static void si_lower_nir(struct si_screen *sscreen, struct nir_shader *nir)
 
    NIR_PASS_V(nir, si_lower_intrinsics);
 
-   const nir_lower_subgroups_options subgroups_options = {
-      .subgroup_size = 64,
-      .ballot_bit_size = 64,
-      .ballot_components = 1,
-      .lower_to_scalar = true,
-      .lower_subgroup_masks = true,
-      .lower_vote_trivial = false,
-      .lower_vote_eq = true,
-   };
-   NIR_PASS_V(nir, nir_lower_subgroups, &subgroups_options);
+   NIR_PASS_V(nir, nir_lower_subgroups, &si_nir_subgroups_options);
 
    NIR_PASS_V(nir, nir_lower_discard_or_demote,
               (sscreen->debug_flags & DBG(FS_CORRECT_DERIVS_AFTER_KILL)) ||
-- 
GitLab


From 82bc19c0d3d83eba0ee60eba3cf8da707a5f7e86 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Sun, 12 Jun 2022 21:02:26 +0800
Subject: [PATCH 07/45] radeonsi: replace llvm ngg gs with nir lowering
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 .../drivers/radeonsi/gfx10_shader_ngg.c       | 31 +++----------------
 src/gallium/drivers/radeonsi/si_shader.c      | 16 ++++++++--
 src/gallium/drivers/radeonsi/si_shader_llvm.c | 31 ++++++-------------
 .../drivers/radeonsi/si_shader_llvm_vs.c      |  4 +--
 src/gallium/drivers/radeonsi/si_shader_nir.c  | 12 +++++--
 5 files changed, 39 insertions(+), 55 deletions(-)

diff --git a/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c b/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
index c2dc9d4fd6f1..2c5826de9643 100644
--- a/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
+++ b/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
@@ -2011,22 +2011,9 @@ void gfx10_ngg_gs_emit_vertex(struct si_shader_context *ctx, unsigned stream,
 
 void gfx10_ngg_gs_emit_begin(struct si_shader_context *ctx)
 {
-   /* Zero out the part of LDS scratch that is used to accumulate the
-    * per-stream generated primitive count.
-    */
    LLVMBuilderRef builder = ctx->ac.builder;
-   struct ac_llvm_pointer scratchptr = ctx->gs_ngg_scratch;
-   LLVMValueRef tid = gfx10_get_thread_id_in_tg(ctx);
    LLVMValueRef tmp;
 
-   tmp = LLVMBuildICmp(builder, LLVMIntULT, tid, LLVMConstInt(ctx->ac.i32, 4, false), "");
-   ac_build_ifcc(&ctx->ac, tmp, 5090);
-   {
-      LLVMValueRef ptr = ac_build_gep0(&ctx->ac, scratchptr, tid);
-      LLVMBuildStore(builder, ctx->ac.i32_0, ptr);
-   }
-   ac_build_endif(&ctx->ac, 5090);
-
    if (ctx->screen->info.gfx_level < GFX11) {
       tmp = si_is_gs_thread(ctx);
       ac_build_ifcc(&ctx->ac, tmp, 15090);
@@ -2049,9 +2036,6 @@ void gfx10_ngg_gs_emit_begin(struct si_shader_context *ctx)
          }
       ac_build_endif(&ctx->ac, 15090);
    }
-
-   ac_build_waitcnt(&ctx->ac, AC_WAIT_LGKM);
-   ac_build_s_barrier(&ctx->ac, ctx->stage);
 }
 
 void gfx10_ngg_gs_build_end(struct si_shader_context *ctx)
@@ -2429,17 +2413,12 @@ static void clamp_gsprims_to_esverts(unsigned *max_gsprims, unsigned max_esverts
 unsigned gfx10_ngg_get_scratch_dw_size(struct si_shader *shader)
 {
    const struct si_shader_selector *sel = shader->selector;
-   bool uses_streamout = si_shader_uses_streamout(shader);
 
-   if (sel->stage == MESA_SHADER_GEOMETRY) {
-      return uses_streamout ? 44 : 8;
-   } else {
-      return ac_ngg_get_scratch_lds_size(sel->stage,
-                                         si_get_max_workgroup_size(shader),
-                                         shader->wave_size,
-                                         uses_streamout,
-                                         shader->key.ge.opt.ngg_culling) / 4;
-   }
+   return ac_ngg_get_scratch_lds_size(sel->stage,
+                                      si_get_max_workgroup_size(shader),
+                                      shader->wave_size,
+                                      si_shader_uses_streamout(shader),
+                                      shader->key.ge.opt.ngg_culling) / 4;
 }
 
 /**
diff --git a/src/gallium/drivers/radeonsi/si_shader.c b/src/gallium/drivers/radeonsi/si_shader.c
index ef8643655114..314ab3d6f912 100644
--- a/src/gallium/drivers/radeonsi/si_shader.c
+++ b/src/gallium/drivers/radeonsi/si_shader.c
@@ -235,7 +235,8 @@ unsigned si_get_max_workgroup_size(const struct si_shader *shader)
       return shader->selector->screen->info.gfx_level >= GFX7 ? 128 : 0;
 
    case MESA_SHADER_GEOMETRY:
-      return shader->selector->screen->info.gfx_level >= GFX9 ? 128 : 0;
+      /* ngg_subgroup_size is only the input size. GS can always generate up to 256 vertices. */
+      return shader->selector->screen->info.gfx_level >= GFX9 ? 256 : 0;
 
    case MESA_SHADER_COMPUTE:
       break; /* see below */
@@ -1667,6 +1668,14 @@ static void si_lower_ngg(struct si_shader *shader, nir_shader *nir)
       options.user_clip_plane_enable_mask = clip_plane_enable;
 
       NIR_PASS_V(nir, ac_nir_lower_ngg_nogs, &options);
+   } else {
+      assert(nir->info.stage == MESA_SHADER_GEOMETRY);
+
+      options.gs_out_vtx_bytes = sel->info.gsvs_vertex_size;
+      options.has_gen_prim_query = options.has_xfb_prim_query =
+         sel->screen->use_ngg_streamout;
+
+      NIR_PASS_V(nir, ac_nir_lower_ngg_gs, &options);
    }
 
    /* may generate some subgroup op like ballot */
@@ -1908,7 +1917,7 @@ struct nir_shader *si_get_nir_shader(struct si_shader *shader, bool *free_nir,
       si_assign_param_offsets(nir, shader);
 
    /* Only lower last VGT NGG shader stage. */
-   if (sel->stage < MESA_SHADER_GEOMETRY && key->ge.as_ngg && !key->ge.as_es) {
+   if (sel->stage <= MESA_SHADER_GEOMETRY && key->ge.as_ngg && !key->ge.as_es) {
       si_lower_ngg(shader, nir);
       opt_offsets = true;
    }
@@ -1960,7 +1969,8 @@ bool si_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *compi
    struct nir_shader *nir = si_get_nir_shader(shader, &free_nir, 0);
 
    struct pipe_stream_output_info so = {};
-   if (si_shader_uses_streamout(shader))
+   /* NGG streamout has been lowered to buffer store in nir. */
+   if (!sscreen->use_ngg_streamout && si_shader_uses_streamout(shader))
       nir_gather_stream_output_info(nir, &so);
 
    /* Dump NIR before doing NIR->LLVM conversion in case the
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index 07d47691e168..dd5f5745c910 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -1007,32 +1007,22 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
    case MESA_SHADER_GEOMETRY:
       si_llvm_init_gs_callbacks(ctx);
 
-      if (!ctx->shader->key.ge.as_ngg)
-         si_preload_gs_rings(ctx);
-
-      for (unsigned i = 0; i < 4; i++)
-         ctx->gs_next_vertex[i] = ac_build_alloca(&ctx->ac, ctx->ac.i32, "");
-
-      if (shader->key.ge.as_ngg) {
-         for (unsigned i = 0; i < 4; ++i) {
-            ctx->gs_curprim_verts[i] = ac_build_alloca(&ctx->ac, ctx->ac.i32, "");
-            ctx->gs_generated_prims[i] = ac_build_alloca(&ctx->ac, ctx->ac.i32, "");
-         }
-
-         assert(!ctx->gs_ngg_scratch.value);
+      if (ctx->shader->key.ge.as_ngg) {
          LLVMTypeRef ai32 = LLVMArrayType(ctx->ac.i32, gfx10_ngg_get_scratch_dw_size(shader));
          ctx->gs_ngg_scratch = (struct ac_llvm_pointer) {
             .value = LLVMAddGlobalInAddressSpace(ctx->ac.module, ai32, "ngg_scratch", AC_ADDR_SPACE_LDS),
             .pointee_type = ai32
          };
          LLVMSetInitializer(ctx->gs_ngg_scratch.value, LLVMGetUndef(ai32));
-         LLVMSetAlignment(ctx->gs_ngg_scratch.value, 4);
+         LLVMSetAlignment(ctx->gs_ngg_scratch.value, 8);
 
          ctx->gs_ngg_emit = LLVMAddGlobalInAddressSpace(
             ctx->ac.module, LLVMArrayType(ctx->ac.i32, 0), "ngg_emit", AC_ADDR_SPACE_LDS);
          LLVMSetLinkage(ctx->gs_ngg_emit, LLVMExternalLinkage);
          LLVMSetAlignment(ctx->gs_ngg_emit, 4);
       } else {
+         si_preload_gs_rings(ctx);
+
          ctx->gs_emitted_vertices = LLVMConstInt(ctx->ac.i32, 0, false);
       }
       break;
@@ -1144,17 +1134,17 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
           shader->key.ge.as_ngg && !shader->key.ge.as_es && !shader->key.ge.opt.ngg_culling)
          ac_build_s_barrier(&ctx->ac, ctx->stage);
 
-      /* NGG GS: Initialize LDS and insert s_barrier, which must not be inside the if statement. */
+      /* NGG GS: handle GS_STATE_PIPELINE_STATS_EMU */
       if (ctx->stage == MESA_SHADER_GEOMETRY && shader->key.ge.as_ngg)
          gfx10_ngg_gs_emit_begin(ctx);
 
       LLVMValueRef thread_enabled = NULL;
 
-      if (ctx->stage == MESA_SHADER_GEOMETRY ||
+      if ((ctx->stage == MESA_SHADER_GEOMETRY && !shader->key.ge.as_ngg) ||
           (ctx->stage == MESA_SHADER_TESS_CTRL && !shader->is_monolithic)) {
          /* Wrap both shaders in an if statement according to the number of enabled threads
           * there. For monolithic TCS, the if statement is inserted by the wrapper function,
-          * not here.
+          * not here. For NGG GS, the if statement is inserted by nir lowering.
           */
          thread_enabled = si_is_gs_thread(ctx); /* 2nd shader: thread enabled bool */
       } else if ((shader->key.ge.as_ls || shader->key.ge.as_es) && !shader->is_monolithic) {
@@ -1200,8 +1190,7 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
                 ctx->ac.wave_size % sel->info.base.tess.tcs_vertices_out != 0)
                ac_build_s_barrier(&ctx->ac, ctx->stage);
          }
-      } else if (ctx->stage == MESA_SHADER_GEOMETRY && !shader->key.ge.as_ngg) {
-         /* gfx10_ngg_gs_emit_begin inserts the barrier for NGG. */
+      } else if (ctx->stage == MESA_SHADER_GEOMETRY) {
          ac_build_waitcnt(&ctx->ac, AC_WAIT_LGKM);
          ac_build_s_barrier(&ctx->ac, ctx->stage);
       }
@@ -1260,9 +1249,7 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
       break;
 
    case MESA_SHADER_GEOMETRY:
-      if (ctx->shader->key.ge.as_ngg)
-         gfx10_ngg_gs_build_end(ctx);
-      else
+      if (!ctx->shader->key.ge.as_ngg)
          si_llvm_gs_build_end(ctx);
       break;
 
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c b/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c
index 12d454489748..597ea18eb952 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c
@@ -699,8 +699,8 @@ void si_llvm_build_vs_exports(struct si_shader_context *ctx, LLVMValueRef num_ex
    }
 
    if (!shader->info.nr_param_exports ||
-       /* GFX11 VS/TES param export is handled in nir */
-       (ctx->screen->info.gfx_level >= GFX11 && ctx->stage != MESA_SHADER_GEOMETRY))
+       /* GFX11 param export is handled in nir */
+       ctx->screen->info.gfx_level >= GFX11)
       return;
 
    /* Build parameter exports. Use 2 loops to export params in ascending order.
diff --git a/src/gallium/drivers/radeonsi/si_shader_nir.c b/src/gallium/drivers/radeonsi/si_shader_nir.c
index 413dce3cfa4f..c704bfd312fe 100644
--- a/src/gallium/drivers/radeonsi/si_shader_nir.c
+++ b/src/gallium/drivers/radeonsi/si_shader_nir.c
@@ -298,8 +298,16 @@ static void si_lower_nir(struct si_screen *sscreen, struct nir_shader *nir)
        nir->info.stage == MESA_SHADER_GEOMETRY)
       NIR_PASS_V(nir, nir_lower_io_to_scalar, nir_var_shader_out);
 
-   if (nir->info.stage == MESA_SHADER_GEOMETRY)
-      NIR_PASS_V(nir, nir_lower_gs_intrinsics, nir_lower_gs_intrinsics_per_stream);
+   if (nir->info.stage == MESA_SHADER_GEOMETRY) {
+      unsigned flags = nir_lower_gs_intrinsics_per_stream;
+      if (sscreen->use_ngg) {
+         flags |= nir_lower_gs_intrinsics_count_primitives |
+            nir_lower_gs_intrinsics_count_vertices_per_primitive |
+            nir_lower_gs_intrinsics_overwrite_incomplete;
+      }
+
+      NIR_PASS_V(nir, nir_lower_gs_intrinsics, flags);
+   }
 
    if (nir->info.stage == MESA_SHADER_COMPUTE) {
       if (nir->info.cs.derivative_group == DERIVATIVE_GROUP_QUADS) {
-- 
GitLab


From 9a9d4bb43a909dc1980d07dd2b2d8297f99e8a7f Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Thu, 16 Jun 2022 18:25:56 +0800
Subject: [PATCH 08/45] radeonsi: remove unused ngg llvm code
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 .../drivers/radeonsi/gfx10_shader_ngg.c       | 2292 +----------------
 .../drivers/radeonsi/si_shader_internal.h     |   11 -
 .../drivers/radeonsi/si_shader_llvm_gs.c      |   10 +-
 3 files changed, 38 insertions(+), 2275 deletions(-)

diff --git a/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c b/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
index 2c5826de9643..36b67ffc5453 100644
--- a/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
+++ b/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
@@ -21,12 +21,9 @@
  * USE OR OTHER DEALINGS IN THE SOFTWARE.
  */
 
-#include "ac_llvm_cull.h"
 #include "si_pipe.h"
 #include "si_query.h"
 #include "si_shader_internal.h"
-#include "sid.h"
-#include "util/u_memory.h"
 #include "util/u_prim.h"
 
 static LLVMValueRef get_wave_id_in_tg(struct si_shader_context *ctx)
@@ -34,11 +31,6 @@ static LLVMValueRef get_wave_id_in_tg(struct si_shader_context *ctx)
    return si_unpack_param(ctx, ctx->args.merged_wave_info, 24, 4);
 }
 
-static LLVMValueRef get_tgsize(struct si_shader_context *ctx)
-{
-   return si_unpack_param(ctx, ctx->args.merged_wave_info, 28, 4);
-}
-
 LLVMValueRef gfx10_get_thread_id_in_tg(struct si_shader_context *ctx)
 {
    LLVMBuilderRef builder = ctx->ac.builder;
@@ -48,1740 +40,57 @@ LLVMValueRef gfx10_get_thread_id_in_tg(struct si_shader_context *ctx)
    return LLVMBuildAdd(builder, tmp, ac_get_thread_id(&ctx->ac), "");
 }
 
-static LLVMValueRef ngg_get_vtx_cnt(struct si_shader_context *ctx)
-{
-   return si_unpack_param(ctx, ctx->args.gs_tg_info, 12, 9);
-}
-
-static LLVMValueRef ngg_get_prim_cnt(struct si_shader_context *ctx)
-{
-   return si_unpack_param(ctx, ctx->args.gs_tg_info, 22, 9);
-}
-
-static LLVMValueRef ngg_get_ordered_id(struct si_shader_context *ctx)
-{
-   return si_unpack_param(ctx, ctx->args.gs_tg_info, 0, 12);
-}
-
 static LLVMValueRef ngg_get_query_buf(struct si_shader_context *ctx)
 {
-   return ac_build_load_to_sgpr(&ctx->ac, ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->internal_bindings),
-                                LLVMConstInt(ctx->ac.i32, SI_GS_QUERY_BUF, false));
-}
-
-static LLVMValueRef ngg_get_emulated_counters_buf(struct si_shader_context *ctx)
-{
-   return ac_build_load_to_sgpr(&ctx->ac, ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->internal_bindings),
-                                LLVMConstInt(ctx->ac.i32, SI_GS_QUERY_EMULATED_COUNTERS_BUF, false));
-}
-
-/**
- * Return the number of vertices as a constant in \p num_vertices,
- * and return a more precise value as LLVMValueRef from the function.
- */
-static LLVMValueRef ngg_get_vertices_per_prim(struct si_shader_context *ctx, unsigned *num_vertices)
-{
-   const struct si_shader_info *info = &ctx->shader->selector->info;
-
-   if (ctx->stage == MESA_SHADER_GEOMETRY) {
-      *num_vertices = u_vertices_per_prim(info->base.gs.output_primitive);
-      return LLVMConstInt(ctx->ac.i32, *num_vertices, false);
-   } else if (ctx->stage == MESA_SHADER_VERTEX) {
-      if (info->base.vs.blit_sgprs_amd) {
-         /* Blits always use axis-aligned rectangles with 3 vertices. */
-         *num_vertices = 3;
-         return LLVMConstInt(ctx->ac.i32, 3, 0);
-      } else if (ctx->shader->key.ge.opt.ngg_culling & SI_NGG_CULL_LINES) {
-         *num_vertices = 2;
-         return LLVMConstInt(ctx->ac.i32, 2, 0);
-      } else {
-         /* We always build up all three indices for the prim export
-          * independent of the primitive type. The additional garbage
-          * data shouldn't hurt. This is used by exports and streamout.
-          */
-         *num_vertices = 3;
-
-         /* Extract OUTPRIM field. */
-         LLVMValueRef num = GET_FIELD(ctx, GS_STATE_OUTPRIM);
-         return LLVMBuildAdd(ctx->ac.builder, num, ctx->ac.i32_1, "");
-      }
-   } else {
-      assert(ctx->stage == MESA_SHADER_TESS_EVAL);
-
-      if (info->base.tess.point_mode)
-         *num_vertices = 1;
-      else if (info->base.tess._primitive_mode == TESS_PRIMITIVE_ISOLINES)
-         *num_vertices = 2;
-      else
-         *num_vertices = 3;
-
-      return LLVMConstInt(ctx->ac.i32, *num_vertices, false);
-   }
-}
-
-unsigned gfx10_ngg_get_vertices_per_prim(struct si_shader *shader)
-{
-   const struct si_shader_info *info = &shader->selector->info;
-
-   if (shader->selector->stage == MESA_SHADER_GEOMETRY)
-      return u_vertices_per_prim(info->base.gs.output_primitive);
-   else if (shader->selector->stage == MESA_SHADER_VERTEX) {
-      if (info->base.vs.blit_sgprs_amd) {
-         /* Blits always use axis-aligned rectangles with 3 vertices. */
-         return 3;
-      } else if (shader->key.ge.opt.ngg_culling & SI_NGG_CULL_LINES)
-         return 2;
-      else {
-         /* We always build up all three indices for the prim export
-          * independent of the primitive type. The additional garbage
-          * data shouldn't hurt. This is used by exports and streamout.
-          */
-         return 3;
-      }
-   } else {
-      assert(shader->selector->stage == MESA_SHADER_TESS_EVAL);
-
-      if (info->base.tess.point_mode)
-         return 1;
-      else if (info->base.tess._primitive_mode == TESS_PRIMITIVE_ISOLINES)
-         return 2;
-      else
-         return 3;
-   }
-}
-
-bool gfx10_ngg_export_prim_early(struct si_shader *shader)
-{
-   struct si_shader_selector *sel = shader->selector;
-
-   assert(shader->key.ge.as_ngg && !shader->key.ge.as_es);
-
-   return sel->stage != MESA_SHADER_GEOMETRY &&
-          !gfx10_ngg_writes_user_edgeflags(shader);
-}
-
-void gfx10_ngg_build_sendmsg_gs_alloc_req(struct si_shader_context *ctx)
-{
-   /* Newer chips can use PRIMGEN_PASSTHRU_NO_MSG to skip gs_alloc_req for NGG passthrough. */
-   if (gfx10_is_ngg_passthrough(ctx->shader) &&
-       ctx->screen->info.family >= CHIP_NAVI23)
-      return;
-
-   ac_build_sendmsg_gs_alloc_req(&ctx->ac, get_wave_id_in_tg(ctx), ngg_get_vtx_cnt(ctx),
-                                 ngg_get_prim_cnt(ctx));
-}
-
-void gfx10_ngg_build_export_prim(struct si_shader_context *ctx, LLVMValueRef user_edgeflags[3],
-                                 LLVMValueRef prim_passthrough)
-{
-   LLVMBuilderRef builder = ctx->ac.builder;
-
-   if (gfx10_is_ngg_passthrough(ctx->shader) || ctx->shader->key.ge.opt.ngg_culling) {
-      ac_build_ifcc(&ctx->ac, si_is_gs_thread(ctx), 6001);
-      {
-         struct ac_ngg_prim prim = {};
-
-         if (prim_passthrough)
-            prim.passthrough = prim_passthrough;
-         else
-            prim.passthrough = ac_get_arg(&ctx->ac, ctx->args.gs_vtx_offset[0]);
-
-         /* This is only used with NGG culling, which returns the NGG
-          * passthrough prim export encoding.
-          */
-         if (gfx10_ngg_writes_user_edgeflags(ctx->shader)) {
-            unsigned all_bits_no_edgeflags = ~SI_NGG_PRIM_EDGE_FLAG_BITS;
-            LLVMValueRef edgeflags = LLVMConstInt(ctx->ac.i32, all_bits_no_edgeflags, 0);
-
-            unsigned num_vertices;
-            ngg_get_vertices_per_prim(ctx, &num_vertices);
-
-            for (unsigned i = 0; i < num_vertices; i++) {
-               unsigned shift = 9 + i * 10;
-               LLVMValueRef edge;
-
-               edge = LLVMBuildLoad2(builder, ctx->ac.i1, user_edgeflags[i], "");
-               edge = LLVMBuildZExt(builder, edge, ctx->ac.i32, "");
-               edge = LLVMBuildShl(builder, edge, LLVMConstInt(ctx->ac.i32, shift, 0), "");
-               edgeflags = LLVMBuildOr(builder, edgeflags, edge, "");
-            }
-            prim.passthrough = LLVMBuildAnd(builder, prim.passthrough, edgeflags, "");
-         }
-
-         ac_build_export_prim(&ctx->ac, &prim);
-      }
-      ac_build_endif(&ctx->ac, 6001);
-      return;
-   }
-
-   ac_build_ifcc(&ctx->ac, si_is_gs_thread(ctx), 6001);
-   {
-      struct ac_ngg_prim prim = {};
-
-      ngg_get_vertices_per_prim(ctx, &prim.num_vertices);
-
-      prim.isnull = ctx->ac.i1false;
-
-      if (gfx10_edgeflags_have_effect(ctx->shader))
-         prim.edgeflags = ac_pack_edgeflags_for_export(&ctx->ac, &ctx->args);
-      else
-         prim.edgeflags = ctx->ac.i32_0;
-
-      for (unsigned i = 0; i < prim.num_vertices; ++i)
-         prim.index[i] = si_unpack_param(ctx, ctx->args.gs_vtx_offset[i / 2], (i & 1) * 16, 16);
-
-      if (gfx10_ngg_writes_user_edgeflags(ctx->shader)) {
-         LLVMValueRef edgeflags = ctx->ac.i32_0;
-
-         for (unsigned i = 0; i < prim.num_vertices; ++i) {
-            LLVMValueRef edge;
-
-            edge = LLVMBuildLoad2(ctx->ac.builder, ctx->ac.i1, user_edgeflags[i], "");
-            edge = LLVMBuildZExt(ctx->ac.builder, edge, ctx->ac.i32, "");
-            edge = LLVMBuildShl(ctx->ac.builder, edge, LLVMConstInt(ctx->ac.i32, 9 + i*10, 0), "");
-            edgeflags = LLVMBuildOr(ctx->ac.builder, edgeflags, edge, "");
-         }
-         prim.edgeflags = LLVMBuildAnd(ctx->ac.builder, prim.edgeflags, edgeflags, "");
-      }
-
-      ac_build_export_prim(&ctx->ac, &prim);
-   }
-   ac_build_endif(&ctx->ac, 6001);
-}
-
-static void build_streamout_vertex(struct si_shader_context *ctx, LLVMValueRef *so_buffer,
-                                   LLVMValueRef *wg_offset_dw, unsigned stream,
-                                   LLVMValueRef offset_vtx, struct ac_llvm_pointer vertexptr)
-{
-   struct si_shader_info *info = &ctx->shader->selector->info;
-   struct pipe_stream_output_info *so = &ctx->so;
-   LLVMBuilderRef builder = ctx->ac.builder;
-   LLVMValueRef offset[4] = {};
-   LLVMValueRef tmp;
-
-   for (unsigned buffer = 0; buffer < 4; ++buffer) {
-      if (!wg_offset_dw[buffer])
-         continue;
-
-      tmp = LLVMBuildMul(builder, offset_vtx, LLVMConstInt(ctx->ac.i32, so->stride[buffer], false),
-                         "");
-      tmp = LLVMBuildAdd(builder, wg_offset_dw[buffer], tmp, "");
-      offset[buffer] = LLVMBuildShl(builder, tmp, LLVMConstInt(ctx->ac.i32, 2, false), "");
-   }
-
-   for (unsigned i = 0; i < so->num_outputs; ++i) {
-      if (so->output[i].stream != stream)
-         continue;
-
-      unsigned reg = so->output[i].register_index;
-      struct si_shader_output_values out;
-      out.semantic = info->output_semantic[reg];
-
-      for (unsigned comp = 0; comp < 4; comp++) {
-         LLVMValueRef idx = LLVMConstInt(ctx->ac.i32, 4 * reg + comp, false);
-         LLVMValueRef v = ac_build_gep0(&ctx->ac, vertexptr, idx);
-         out.values[comp] = LLVMBuildLoad2(builder, ac_build_gep0_type(vertexptr.t, idx), v, "");
-         out.vertex_streams = info->output_streams[reg];
-      }
-
-      si_llvm_streamout_store_output(ctx, so_buffer, offset, &so->output[i], &out);
-   }
-}
-
-struct ngg_streamout {
-   LLVMValueRef num_vertices;
-
-   /* per-thread data */
-   LLVMValueRef prim_enable[4];        /* i1 per stream */
-   struct ac_llvm_pointer vertices[3]; /* [N x i32] addrspace(LDS)* */
-
-   /* Output */
-   LLVMValueRef emit[4]; /* per-stream emitted primitives (only valid for used streams) */
-};
-
-/**
- * Build streamout logic.
- *
- * Implies a barrier.
- *
- * Writes number of emitted primitives to gs_ngg_scratch[4:8].
- *
- * Clobbers gs_ngg_scratch[8:].
- */
-static void build_streamout(struct si_shader_context *ctx, struct ngg_streamout *nggso)
-{
-   struct si_shader_info *info = &ctx->shader->selector->info;
-   struct pipe_stream_output_info *so = &ctx->so;
-   LLVMBuilderRef builder = ctx->ac.builder;
-   struct ac_llvm_pointer arg = ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->internal_bindings);
-   LLVMValueRef tid = gfx10_get_thread_id_in_tg(ctx);
-   LLVMValueRef tmp, tmp2;
-   LLVMValueRef i32_2 = LLVMConstInt(ctx->ac.i32, 2, false);
-   LLVMValueRef i32_4 = LLVMConstInt(ctx->ac.i32, 4, false);
-   LLVMValueRef i32_8 = LLVMConstInt(ctx->ac.i32, 8, false);
-   LLVMValueRef so_buffer[4] = {};
-   unsigned max_num_vertices = 1 + (nggso->vertices[1].value ? 1 : 0) + (nggso->vertices[2].value ? 1 : 0);
-   LLVMValueRef prim_stride_dw[4] = {};
-   LLVMValueRef prim_stride_dw_vgpr = LLVMGetUndef(ctx->ac.i32);
-   int stream_for_buffer[4] = {-1, -1, -1, -1};
-   unsigned bufmask_for_stream[4] = {};
-   bool isgs = ctx->stage == MESA_SHADER_GEOMETRY;
-   unsigned scratch_emit_base = isgs ? 4 : 0;
-   LLVMValueRef scratch_emit_basev = isgs ? i32_4 : ctx->ac.i32_0;
-   unsigned scratch_offset_base = isgs ? 8 : 4;
-   LLVMValueRef scratch_offset_basev = isgs ? i32_8 : i32_4;
-
-   /* Determine the mapping of streamout buffers to vertex streams. */
-   for (unsigned i = 0; i < so->num_outputs; ++i) {
-      unsigned buf = so->output[i].output_buffer;
-      unsigned stream = so->output[i].stream;
-      assert(stream_for_buffer[buf] < 0 || stream_for_buffer[buf] == stream);
-      stream_for_buffer[buf] = stream;
-      bufmask_for_stream[stream] |= 1 << buf;
-   }
-
-   for (unsigned buffer = 0; buffer < 4; ++buffer) {
-      if (stream_for_buffer[buffer] == -1)
-         continue;
-
-      assert(so->stride[buffer]);
-
-      tmp = LLVMConstInt(ctx->ac.i32, so->stride[buffer], false);
-      prim_stride_dw[buffer] = LLVMBuildMul(builder, tmp, nggso->num_vertices, "");
-      prim_stride_dw_vgpr =
-         ac_build_writelane(&ctx->ac, prim_stride_dw_vgpr, prim_stride_dw[buffer],
-                            LLVMConstInt(ctx->ac.i32, buffer, false));
-
-      so_buffer[buffer] = ac_build_load_to_sgpr(
-         &ctx->ac, arg, LLVMConstInt(ctx->ac.i32, SI_VS_STREAMOUT_BUF0 + buffer, false));
-   }
-
-   tmp = LLVMBuildICmp(builder, LLVMIntEQ, get_wave_id_in_tg(ctx), ctx->ac.i32_0, "");
-   ac_build_ifcc(&ctx->ac, tmp, 5200);
-   {
-      LLVMTypeRef gdsptr = LLVMPointerType(ctx->ac.i32, AC_ADDR_SPACE_GDS);
-      LLVMValueRef gdsbase = LLVMBuildIntToPtr(builder, ctx->ac.i32_0, gdsptr, "");
-
-      /* Advance the streamout offsets in GDS. */
-      LLVMValueRef offsets_vgpr = ac_build_alloca_undef(&ctx->ac, ctx->ac.i32, "");
-      LLVMValueRef generated_by_stream_vgpr = ac_build_alloca_undef(&ctx->ac, ctx->ac.i32, "");
-
-      tmp = LLVMBuildICmp(builder, LLVMIntULT, ac_get_thread_id(&ctx->ac), i32_4, "");
-      ac_build_ifcc(&ctx->ac, tmp, 5210);
-      {
-         if (isgs) {
-            LLVMValueRef vt = ac_build_gep0(&ctx->ac, ctx->gs_ngg_scratch, tid);
-            tmp = LLVMBuildLoad2(builder, ac_build_gep0_type(ctx->gs_ngg_scratch.t, tid), vt, "");
-         } else {
-            tmp = ac_build_writelane(&ctx->ac, ctx->ac.i32_0, ngg_get_prim_cnt(ctx), ctx->ac.i32_0);
-         }
-         LLVMBuildStore(builder, tmp, generated_by_stream_vgpr);
-
-         unsigned swizzle[4];
-         int unused_stream = -1;
-         for (unsigned stream = 0; stream < 4; ++stream) {
-            if (!info->num_stream_output_components[stream]) {
-               unused_stream = stream;
-               break;
-            }
-         }
-         for (unsigned buffer = 0; buffer < 4; ++buffer) {
-            if (stream_for_buffer[buffer] >= 0) {
-               swizzle[buffer] = stream_for_buffer[buffer];
-            } else {
-               assert(unused_stream >= 0);
-               swizzle[buffer] = unused_stream;
-            }
-         }
-
-         tmp = ac_build_quad_swizzle(&ctx->ac, tmp, swizzle[0], swizzle[1], swizzle[2], swizzle[3]);
-         tmp = LLVMBuildMul(builder, tmp, prim_stride_dw_vgpr, "");
-
-         LLVMValueRef args[8] = {
-            LLVMBuildIntToPtr(builder, ngg_get_ordered_id(ctx), gdsptr, ""),
-            ctx->ac.i32_0,                             /* value to add */
-            ctx->ac.i32_0,                             /* ordering */
-            ctx->ac.i32_0,                             /* scope */
-            ctx->ac.i1false,                           /* isVolatile */
-            LLVMConstInt(ctx->ac.i32, 1 << 24, false), /* OA index, bits 24+: lane count */
-            ctx->ac.i1true,                            /* wave release */
-            ctx->ac.i1true,                            /* wave done */
-         };
-
-         if (ctx->screen->info.gfx_level >= GFX11) {
-            /* Gfx11 GDS instructions only operate on the first active lane. All other lanes are
-             * ignored. So are their EXEC bits. This uses the mutex feature of ds_ordered_count
-             * to emulate a multi-dword atomic.
-             *
-             * This is the expected code:
-             *    ds_ordered_count release=0 done=0   // lock mutex
-             *    ds_add_rtn_u32 dwords_written0
-             *    ds_add_rtn_u32 dwords_written1
-             *    ds_add_rtn_u32 dwords_written2
-             *    ds_add_rtn_u32 dwords_written3
-             *    ds_ordered_count release=1 done=1   // unlock mutex
-             *
-             * TODO: Increment GDS_STRMOUT registers instead of GDS memory.
-             */
-            LLVMValueRef dwords_written[4] = {tmp, tmp, tmp, tmp};
-
-            /* Move all 4 VGPRs from other lanes to lane 0. */
-            for (unsigned i = 1; i < 4; i++) {
-               if (ctx->shader->selector->info.base.xfb_stride[i])
-                  dwords_written[i] = ac_build_quad_swizzle(&ctx->ac, tmp, i, i, i, i);
-            }
-
-            /* Set release=0 to start a GDS mutex. Set done=0 because it's not the last one. */
-            args[6] = args[7] = ctx->ac.i1false;
-            ac_build_intrinsic(&ctx->ac, "llvm.amdgcn.ds.ordered.add", ctx->ac.i32,
-                               args, ARRAY_SIZE(args), 0);
-            ac_build_waitcnt(&ctx->ac, AC_WAIT_LGKM);
-
-            for (unsigned i = 0; i < 4; i++) {
-               if (ctx->shader->selector->info.base.xfb_stride[i]) {
-                  LLVMValueRef gds_ptr =
-                     ac_build_gep_ptr(&ctx->ac, ctx->ac.i32, gdsbase, LLVMConstInt(ctx->ac.i32, i, 0));
-
-                  dwords_written[i] = LLVMBuildAtomicRMW(builder, LLVMAtomicRMWBinOpAdd,
-                                                         gds_ptr, dwords_written[i],
-                                                         LLVMAtomicOrderingMonotonic, false);
-               }
-            }
-
-            /* TODO: This might not be needed if GDS executes instructions in order. */
-            ac_build_waitcnt(&ctx->ac, AC_WAIT_LGKM);
-
-            /* Set release=1 to end a GDS mutex. Set done=1 because it's the last one. */
-            args[6] = args[7] = ctx->ac.i1true;
-            ac_build_intrinsic(&ctx->ac, "llvm.amdgcn.ds.ordered.add", ctx->ac.i32,
-                               args, ARRAY_SIZE(args), 0);
-
-            tmp = dwords_written[0];
-            for (unsigned i = 1; i < 4; i++) {
-               if (ctx->shader->selector->info.base.xfb_stride[i]) {
-                  dwords_written[i] = ac_build_readlane(&ctx->ac, dwords_written[i], ctx->ac.i32_0);
-                  tmp = ac_build_writelane(&ctx->ac, tmp, dwords_written[i], LLVMConstInt(ctx->ac.i32, i, 0));
-               }
-            }
-         } else {
-            args[1] = tmp; /* value to add */
-            args[5] = LLVMConstInt(ctx->ac.i32, 4 << 24, false), /* bits 24+: lane count */
-
-            tmp = ac_build_intrinsic(&ctx->ac, "llvm.amdgcn.ds.ordered.add", ctx->ac.i32,
-                                     args, ARRAY_SIZE(args), 0);
-         }
-
-         /* Keep offsets in a VGPR for quick retrieval via readlane by
-          * the first wave for bounds checking, and also store in LDS
-          * for retrieval by all waves later. */
-         LLVMBuildStore(builder, tmp, offsets_vgpr);
-
-         tmp2 = LLVMBuildAdd(builder, ac_get_thread_id(&ctx->ac), scratch_offset_basev, "");
-         tmp2 = ac_build_gep0(&ctx->ac, ctx->gs_ngg_scratch, tmp2);
-         LLVMBuildStore(builder, tmp, tmp2);
-      }
-      ac_build_endif(&ctx->ac, 5210);
-
-      /* Determine the max emit per buffer. This is done via the SALU, in part
-       * because LLVM can't generate divide-by-multiply if we try to do this
-       * via VALU with one lane per buffer.
-       */
-      LLVMValueRef max_emit[4] = {};
-      for (unsigned buffer = 0; buffer < 4; ++buffer) {
-         if (stream_for_buffer[buffer] == -1)
-            continue;
-
-         LLVMValueRef bufsize_dw = LLVMBuildLShr(
-            builder, LLVMBuildExtractElement(builder, so_buffer[buffer], i32_2, ""), i32_2, "");
-
-         tmp = LLVMBuildLoad2(builder, ctx->ac.i32, offsets_vgpr, "");
-         LLVMValueRef offset_dw =
-            ac_build_readlane(&ctx->ac, tmp, LLVMConstInt(ctx->ac.i32, buffer, false));
-
-         tmp = LLVMBuildSub(builder, bufsize_dw, offset_dw, "");
-         tmp = LLVMBuildUDiv(builder, tmp, prim_stride_dw[buffer], "");
-
-         tmp2 = LLVMBuildICmp(builder, LLVMIntULT, bufsize_dw, offset_dw, "");
-         max_emit[buffer] = LLVMBuildSelect(builder, tmp2, ctx->ac.i32_0, tmp, "");
-      }
-
-      /* Determine the number of emitted primitives per stream and fixup the
-       * GDS counter if necessary.
-       *
-       * This is complicated by the fact that a single stream can emit to
-       * multiple buffers (but luckily not vice versa).
-       */
-      LLVMValueRef emit_vgpr = ctx->ac.i32_0;
-
-      for (unsigned stream = 0; stream < 4; ++stream) {
-         if (!info->num_stream_output_components[stream])
-            continue;
-
-         tmp = LLVMBuildLoad2(builder, ctx->ac.i32, generated_by_stream_vgpr, "");
-         LLVMValueRef generated =
-            ac_build_readlane(&ctx->ac, tmp, LLVMConstInt(ctx->ac.i32, stream, false));
-
-         LLVMValueRef emit = generated;
-         for (unsigned buffer = 0; buffer < 4; ++buffer) {
-            if (stream_for_buffer[buffer] == stream)
-               emit = ac_build_umin(&ctx->ac, emit, max_emit[buffer]);
-         }
-
-         emit_vgpr =
-            ac_build_writelane(&ctx->ac, emit_vgpr, emit, LLVMConstInt(ctx->ac.i32, stream, false));
-
-         /* Fixup the offset using a plain GDS atomic if we overflowed. */
-         tmp = LLVMBuildICmp(builder, LLVMIntULT, emit, generated, "");
-         ac_build_ifcc(&ctx->ac, tmp, 5221); /* scalar branch */
-         tmp = LLVMBuildLShr(builder, LLVMConstInt(ctx->ac.i32, bufmask_for_stream[stream], false),
-                             ac_get_thread_id(&ctx->ac), "");
-         tmp = LLVMBuildTrunc(builder, tmp, ctx->ac.i1, "");
-         ac_build_ifcc(&ctx->ac, tmp, 5222);
-         {
-            tmp = LLVMBuildSub(builder, generated, emit, "");
-            tmp = LLVMBuildMul(builder, tmp, prim_stride_dw_vgpr, "");
-
-            if (ctx->screen->info.gfx_level >= GFX11) {
-               /* Gfx11 GDS instructions only operate on the first active lane.
-                * This is an unrolled waterfall loop. We only get here when we overflow,
-                * so it doesn't have to be fast.
-                */
-               for (unsigned i = 0; i < 4; i++) {
-                  if (bufmask_for_stream[stream] & BITFIELD_BIT(i)) {
-                     LLVMValueRef index = LLVMConstInt(ctx->ac.i32, i, 0);
-
-                     ac_build_ifcc(&ctx->ac, LLVMBuildICmp(builder, LLVMIntEQ, tid, index, ""), 0);
-                     LLVMBuildAtomicRMW(builder, LLVMAtomicRMWBinOpSub,
-                                        LLVMBuildGEP2(builder, gdsptr, gdsbase, &index, 1, ""),
-                                        tmp, LLVMAtomicOrderingMonotonic, false);
-                     ac_build_endif(&ctx->ac, 0);
-                  }
-               }
-            } else {
-               LLVMBuildAtomicRMW(builder, LLVMAtomicRMWBinOpSub,
-                                  LLVMBuildGEP2(builder, gdsptr, gdsbase, &tid, 1, ""),
-                                  tmp, LLVMAtomicOrderingMonotonic, false);
-            }
-         }
-         ac_build_endif(&ctx->ac, 5222);
-         ac_build_endif(&ctx->ac, 5221);
-      }
-
-      tmp = LLVMBuildICmp(builder, LLVMIntULT, ac_get_thread_id(&ctx->ac), i32_4, "");
-      ac_build_ifcc(&ctx->ac, tmp, 5225);
-      {
-         tmp = LLVMBuildAdd(builder, ac_get_thread_id(&ctx->ac), scratch_emit_basev, "");
-         tmp = ac_build_gep0(&ctx->ac, ctx->gs_ngg_scratch, tmp);
-         LLVMBuildStore(builder, emit_vgpr, tmp);
-      }
-      ac_build_endif(&ctx->ac, 5225);
-   }
-   ac_build_endif(&ctx->ac, 5200);
-
-   /* Determine the workgroup-relative per-thread / primitive offset into
-    * the streamout buffers */
-   struct ac_wg_scan primemit_scan[4] = {};
-
-   if (isgs) {
-      for (unsigned stream = 0; stream < 4; ++stream) {
-         if (!info->num_stream_output_components[stream])
-            continue;
-
-         primemit_scan[stream].stage = ctx->stage;
-         primemit_scan[stream].enable_exclusive = true;
-         primemit_scan[stream].op = nir_op_iadd;
-         primemit_scan[stream].src = nggso->prim_enable[stream];
-         primemit_scan[stream].scratch = ac_build_gep0(
-            &ctx->ac, ctx->gs_ngg_scratch,
-            LLVMConstInt(ctx->ac.i32, 12 + 8 * stream, false));
-         primemit_scan[stream].waveidx = get_wave_id_in_tg(ctx);
-         primemit_scan[stream].numwaves = get_tgsize(ctx);
-         if (ctx->stage == MESA_SHADER_GEOMETRY) {
-            /* ngg_subgroup_size is only the input size. GS can always generate up to 256 vertices. */
-            primemit_scan[stream].maxwaves = DIV_ROUND_UP(256, ctx->ac.wave_size);
-         } else {
-            primemit_scan[stream].maxwaves = DIV_ROUND_UP(ctx->screen->ngg_subgroup_size,
-                                                          ctx->ac.wave_size);
-         }
-         ac_build_wg_scan_top(&ctx->ac, &primemit_scan[stream]);
-      }
-   }
-
-   ac_build_waitcnt(&ctx->ac, AC_WAIT_LGKM);
-   ac_build_s_barrier(&ctx->ac, ctx->stage);
-
-   /* Fetch the per-buffer offsets and per-stream emit counts in all waves. */
-   LLVMValueRef wgoffset_dw[4] = {};
-
-   {
-      LLVMValueRef scratch_vgpr;
-
-      LLVMValueRef idx = ac_get_thread_id(&ctx->ac);
-      LLVMValueRef v = ac_build_gep0(&ctx->ac, ctx->gs_ngg_scratch, idx);
-      scratch_vgpr = LLVMBuildLoad2(builder, ac_build_gep0_type(ctx->gs_ngg_scratch.t, idx), v, "");
-
-      for (unsigned buffer = 0; buffer < 4; ++buffer) {
-         if (stream_for_buffer[buffer] >= 0) {
-            wgoffset_dw[buffer] =
-               ac_build_readlane(&ctx->ac, scratch_vgpr,
-                                 LLVMConstInt(ctx->ac.i32, scratch_offset_base + buffer, false));
-         }
-      }
-
-      for (unsigned stream = 0; stream < 4; ++stream) {
-         if (info->num_stream_output_components[stream]) {
-            nggso->emit[stream] =
-               ac_build_readlane(&ctx->ac, scratch_vgpr,
-                                 LLVMConstInt(ctx->ac.i32, scratch_emit_base + stream, false));
-         }
-      }
-   }
-
-   /* Write out primitive data */
-   for (unsigned stream = 0; stream < 4; ++stream) {
-      if (!info->num_stream_output_components[stream])
-         continue;
-
-      if (isgs) {
-         ac_build_wg_scan_bottom(&ctx->ac, &primemit_scan[stream]);
-      } else {
-         primemit_scan[stream].result_exclusive = tid;
-      }
-
-      tmp = LLVMBuildICmp(builder, LLVMIntULT, primemit_scan[stream].result_exclusive,
-                          nggso->emit[stream], "");
-      tmp = LLVMBuildAnd(builder, tmp, nggso->prim_enable[stream], "");
-      ac_build_ifcc(&ctx->ac, tmp, 5240);
-      {
-         LLVMValueRef offset_vtx =
-            LLVMBuildMul(builder, primemit_scan[stream].result_exclusive, nggso->num_vertices, "");
-
-         for (unsigned i = 0; i < max_num_vertices; ++i) {
-            tmp = LLVMBuildICmp(builder, LLVMIntULT, LLVMConstInt(ctx->ac.i32, i, false),
-                                nggso->num_vertices, "");
-            ac_build_ifcc(&ctx->ac, tmp, 5241);
-            build_streamout_vertex(ctx, so_buffer, wgoffset_dw, stream, offset_vtx,
-                                   nggso->vertices[i]);
-            ac_build_endif(&ctx->ac, 5241);
-            offset_vtx = LLVMBuildAdd(builder, offset_vtx, ctx->ac.i32_1, "");
-         }
-      }
-      ac_build_endif(&ctx->ac, 5240);
-   }
-}
-
-/* LDS layout of ES vertex data for NGG culling. */
-enum
-{
-   /* Byte 0: Boolean ES thread accepted (unculled) flag.
-    * Byte 1: New ES thread ID, loaded by GS to prepare the prim export value.
-    * Byte 2: TES rel patch ID
-    * Byte 3: 8-bit clip distance mask: 1 means the clip distance is negative.
-    *         The mask from all vertices is AND'ed. If the result is non-zero,
-    *         the primitive is culled.
-    */
-   lds_byte0_accept_flag = 0,
-   lds_byte1_new_thread_id,
-   lds_byte2_tes_rel_patch_id,
-   lds_byte3_clipdist_neg_mask,
-
-   lds_packed_data = 0, /* lds_byteN_... */
-   lds_pos_cull_x_div_w,
-   lds_pos_cull_y_div_w,
-   lds_pos_cull_w,
-
-   lds_pos_x = lds_packed_data + 1,
-   lds_pos_y,
-   lds_pos_z,
-   lds_pos_w,
-   /* If VS: */
-   lds_vertex_id,
-   lds_instance_id, /* optional */
-   /* If TES: */
-   lds_tes_u = lds_vertex_id,
-   lds_tes_v = lds_instance_id,
-   lds_tes_patch_id, /* optional */
-};
-
-static LLVMValueRef si_build_gep_i8_var(struct si_shader_context *ctx, LLVMValueRef ptr,
-                                        LLVMValueRef index)
-{
-#if LLVM_VERSION_MAJOR < 14
-   LLVMTypeRef pi8 = LLVMPointerType(ctx->ac.i8, AC_ADDR_SPACE_LDS);
-   ptr = LLVMBuildPointerCast(ctx->ac.builder, ptr, pi8, "");
-#endif
-
-   return LLVMBuildGEP2(ctx->ac.builder, ctx->ac.i8, ptr, &index, 1, "");
-}
-
-static LLVMValueRef si_build_gep_i8(struct si_shader_context *ctx, LLVMValueRef ptr,
-                                    unsigned byte_index)
-{
-   assert(byte_index < 4);
-   return si_build_gep_i8_var(ctx, ptr, LLVMConstInt(ctx->ac.i32, byte_index, 0));
-}
-
-static unsigned ngg_nogs_vertex_size(struct si_shader *shader)
-{
-   unsigned lds_vertex_size = 0;
-
-   /* The edgeflag is always stored in the last element that's also
-    * used for padding to reduce LDS bank conflicts. */
-   if (si_shader_uses_streamout(shader))
-      lds_vertex_size = 4 * shader->selector->info.num_outputs + 1;
-   if (gfx10_ngg_writes_user_edgeflags(shader))
-      lds_vertex_size = MAX2(lds_vertex_size, 1);
-
-   /* LDS size for passing data from GS to ES.
-    * GS stores Primitive IDs into LDS at the address corresponding
-    * to the ES thread of the provoking vertex. All ES threads
-    * load and export PrimitiveID for their thread.
-    */
-   if (shader->selector->stage == MESA_SHADER_VERTEX && shader->key.ge.mono.u.vs_export_prim_id)
-      lds_vertex_size = MAX2(lds_vertex_size, 1);
-
-   if (shader->key.ge.opt.ngg_culling) {
-      if (shader->selector->stage == MESA_SHADER_VERTEX) {
-         STATIC_ASSERT(lds_instance_id + 1 == 7);
-         lds_vertex_size = MAX2(lds_vertex_size, 7);
-      } else {
-         assert(shader->selector->stage == MESA_SHADER_TESS_EVAL);
-
-         if (shader->selector->info.uses_primid || shader->key.ge.mono.u.vs_export_prim_id) {
-            STATIC_ASSERT(lds_tes_patch_id + 2 == 9); /* +1 for LDS padding */
-            lds_vertex_size = MAX2(lds_vertex_size, 9);
-         } else {
-            STATIC_ASSERT(lds_tes_v + 1 == 7);
-            lds_vertex_size = MAX2(lds_vertex_size, 7);
-         }
-      }
-   }
-
-   return lds_vertex_size;
-}
-
-/**
- * Returns an `[N x i32] addrspace(LDS)*` pointing at contiguous LDS storage
- * for the vertex outputs.
- */
-static struct ac_llvm_pointer ngg_nogs_vertex_ptr(struct si_shader_context *ctx, LLVMValueRef vtxid)
-{
-   /* The extra dword is used to avoid LDS bank conflicts. */
-   unsigned vertex_size = ngg_nogs_vertex_size(ctx->shader);
-   LLVMTypeRef ai32 = LLVMArrayType(ctx->ac.i32, vertex_size);
-   return (struct ac_llvm_pointer) {
-      .value = LLVMBuildGEP2(ctx->ac.builder, ai32, ctx->esgs_ring, &vtxid, 1, ""),
-      .pointee_type = ai32
-   };
-}
-
-static LLVMValueRef si_insert_input_v4i32(struct si_shader_context *ctx, LLVMValueRef ret,
-                                          struct ac_arg param, unsigned return_index)
-{
-   LLVMValueRef v = ac_get_arg(&ctx->ac, param);
-
-   for (unsigned i = 0; i < 4; i++) {
-      ret = LLVMBuildInsertValue(ctx->ac.builder, ret, ac_llvm_extract_elem(&ctx->ac, v, i),
-                                 return_index + i, "");
-   }
-   return ret;
-}
-
-static void load_vertex_counts(struct si_shader_context *ctx, struct ac_llvm_pointer lds,
-                               unsigned max_waves, LLVMValueRef tid,
-                               LLVMValueRef *total_count,
-                               LLVMValueRef *prefix_sum)
-{
-   LLVMBuilderRef builder = ctx->ac.builder;
-   LLVMValueRef i8vec4_lane = ac_build_alloca_undef(&ctx->ac, ctx->ac.i32, "");
-   unsigned num_i8vec4 = DIV_ROUND_UP(max_waves, 4);
-
-   /* If all threads loaded the vertex counts, it would cause many LDS bank conflicts
-    * and the performance could decrease up to WaveSize times (32x or 64x).
-    *
-    * Therefore, only load the i-th tuple of vertex counts in the i-th thread. Other threads will
-    * get them through readlane. 4 8-bit vertex counts are loaded per thread.
-    */
-   ac_build_ifcc(&ctx->ac, LLVMBuildICmp(builder, LLVMIntULT, tid,
-                                         LLVMConstInt(ctx->ac.i32, num_i8vec4, 0), ""), 17771);
-   LLVMValueRef v = ac_build_gep0(&ctx->ac, lds, tid);
-   LLVMBuildStore(builder, LLVMBuildLoad2(builder, ac_build_gep0_type(lds.t, tid), v, ""), i8vec4_lane);
-   ac_build_endif(&ctx->ac, 17771);
-
-   /* Compute the number of ES waves. */
-   LLVMValueRef num_waves = get_tgsize(ctx);
-
-   /* Compute a byte mask where each byte is either 0 or 0xff depending on whether the wave
-    * exists. We need the mask to clear uninitialized bytes in LDS and to compute the prefix sum.
-    *
-    * 8 waves: valid_mask = ~0ull >> (64 - num_waves * 8)
-    * 4 waves: valid_mask = ~0 >> (32 - num_waves * 8)
-    */
-   LLVMValueRef num_waves8 = LLVMBuildShl(builder, num_waves, LLVMConstInt(ctx->ac.i32, 3, 0), "");
-   LLVMValueRef valid_mask;
-
-   if (max_waves > 4) {
-      LLVMValueRef num_waves8_rev = LLVMBuildSub(builder, LLVMConstInt(ctx->ac.i32, 64, 0),
-                                                 num_waves8, "");
-      valid_mask = LLVMBuildLShr(builder, LLVMConstInt(ctx->ac.i64, ~0ull, 0),
-                                 LLVMBuildZExt(builder, num_waves8_rev, ctx->ac.i64, ""), "");
-   } else {
-      LLVMValueRef num_waves8_rev = LLVMBuildSub(builder, LLVMConstInt(ctx->ac.i32, 32, 0),
-                                                 num_waves8, "");
-      valid_mask = LLVMBuildLShr(builder, LLVMConstInt(ctx->ac.i32, ~0, 0), num_waves8_rev, "");
-   }
-
-   /* Compute a byte mask where bytes below wave_id are 0xff, else they are 0.
-    *
-    * prefix_mask = ~(~0 << (wave_id * 8))
-    */
-   LLVMTypeRef type = max_waves > 4 ? ctx->ac.i64 : ctx->ac.i32;
-   LLVMValueRef wave_id8 = LLVMBuildShl(builder, get_wave_id_in_tg(ctx),
-                                        LLVMConstInt(ctx->ac.i32, 3, 0), "");
-   LLVMValueRef prefix_mask =
-      LLVMBuildNot(builder, LLVMBuildShl(builder, LLVMConstInt(type, ~0ull, 0),
-                                         LLVMBuildZExt(builder, wave_id8, type, ""), ""), "");
-
-   /* Compute the total vertex count and the vertex count of previous waves (prefix). */
-   *total_count = ctx->ac.i32_0;
-   *prefix_sum = ctx->ac.i32_0;
-
-   for (unsigned i = 0; i < num_i8vec4; i++) {
-      LLVMValueRef i8vec4;
-
-      i8vec4 = ac_build_readlane_no_opt_barrier(&ctx->ac, LLVMBuildLoad2(builder, ctx->ac.i32, i8vec4_lane, ""),
-                                                LLVMConstInt(ctx->ac.i32, i, 0));
-      /* Inactive waves have uninitialized vertex counts. Set them to 0 using this. */
-      i8vec4 = LLVMBuildAnd(builder, i8vec4,
-                            ac_unpack_param(&ctx->ac, valid_mask, 32 * i, 32), "");
-      /* Compute the sum of all i8vec4 components and add it to the result. */
-      *total_count = ac_build_intrinsic(&ctx->ac, "llvm.amdgcn.sad.u8", ctx->ac.i32,
-                                        (LLVMValueRef[]){i8vec4, ctx->ac.i32_0, *total_count},
-                                        3, AC_FUNC_ATTR_READNONE);
-      ac_set_range_metadata(&ctx->ac, *total_count, 0, 64*4 + 1); /* the result is at most 64*4 */
-
-      /* Compute the sum of the vertex counts of all previous waves. */
-      i8vec4 = LLVMBuildAnd(builder, i8vec4,
-                                ac_unpack_param(&ctx->ac, prefix_mask, 32 * i, 32), "");
-      *prefix_sum = ac_build_intrinsic(&ctx->ac, "llvm.amdgcn.sad.u8", ctx->ac.i32,
-                                       (LLVMValueRef[]){i8vec4, ctx->ac.i32_0, *prefix_sum},
-                                       3, AC_FUNC_ATTR_READNONE);
-      ac_set_range_metadata(&ctx->ac, *prefix_sum, 0, 64*4 + 1); /* the result is at most 64*4 */
-   }
-   *total_count = ac_build_readlane_no_opt_barrier(&ctx->ac, *total_count, NULL);
-}
-
-/**
- * Given a total thread count, update total and per-wave thread counts in input SGPRs
- * and return the per-wave thread count.
- *
- * \param new_num_threads    Total thread count on the input, per-wave thread count on the output.
- * \param tg_info            tg_info SGPR value
- * \param tg_info_num_bits   the bit size of thread count field in tg_info
- * \param tg_info_shift      the bit offset of the thread count field in tg_info
- * \param wave_info          merged_wave_info SGPR value
- * \param wave_info_num_bits the bit size of thread count field in merged_wave_info
- * \param wave_info_shift    the bit offset of the thread count field in merged_wave_info
- */
-static void update_thread_counts(struct si_shader_context *ctx, LLVMValueRef *new_num_threads,
-                                 LLVMValueRef *tg_info, unsigned tg_info_num_bits,
-                                 unsigned tg_info_shift, LLVMValueRef *wave_info,
-                                 unsigned wave_info_num_bits, unsigned wave_info_shift)
-{
-   LLVMBuilderRef builder = ctx->ac.builder;
-
-   /* Update the total thread count. */
-   unsigned tg_info_mask = ~(u_bit_consecutive(0, tg_info_num_bits) << tg_info_shift);
-   *tg_info = LLVMBuildAnd(builder, *tg_info, LLVMConstInt(ctx->ac.i32, tg_info_mask, 0), "");
-   *tg_info = LLVMBuildOr(
-      builder, *tg_info,
-      LLVMBuildShl(builder, *new_num_threads, LLVMConstInt(ctx->ac.i32, tg_info_shift, 0), ""), "");
-
-   /* Update the per-wave thread count. */
-   LLVMValueRef prev_threads = LLVMBuildMul(builder, get_wave_id_in_tg(ctx),
-                                            LLVMConstInt(ctx->ac.i32, ctx->ac.wave_size, 0), "");
-   *new_num_threads = LLVMBuildSub(builder, *new_num_threads, prev_threads, "");
-   *new_num_threads = ac_build_imax(&ctx->ac, *new_num_threads, ctx->ac.i32_0);
-   *new_num_threads =
-      ac_build_imin(&ctx->ac, *new_num_threads, LLVMConstInt(ctx->ac.i32, ctx->ac.wave_size, 0));
-   unsigned wave_info_mask = ~(u_bit_consecutive(0, wave_info_num_bits) << wave_info_shift);
-   *wave_info = LLVMBuildAnd(builder, *wave_info, LLVMConstInt(ctx->ac.i32, wave_info_mask, 0), "");
-   *wave_info = LLVMBuildOr(
-      builder, *wave_info,
-      LLVMBuildShl(builder, *new_num_threads, LLVMConstInt(ctx->ac.i32, wave_info_shift, 0), ""),
-      "");
-}
-
-static void gfx10_build_primitive_accepted(struct ac_llvm_context *ac, LLVMValueRef accepted,
-                                           void *userdata)
-{
-   struct si_shader_context *ctx = container_of(ac, struct si_shader_context, ac);
-   LLVMValueRef *params = (LLVMValueRef *)userdata;
-   LLVMValueRef gs_accepted = params[0];
-   struct ac_llvm_pointer *gs_vtxptr = (struct ac_llvm_pointer *)params[1];
-
-   unsigned num_vertices;
-   ngg_get_vertices_per_prim(ctx, &num_vertices);
-
-   ac_build_ifcc(&ctx->ac, accepted, 0);
-   LLVMBuildStore(ctx->ac.builder, ctx->ac.i32_1, gs_accepted);
-
-   if (gs_vtxptr) {
-      for (unsigned vtx = 0; vtx < num_vertices; vtx++) {
-         LLVMBuildStore(ctx->ac.builder, ctx->ac.i8_1,
-                        si_build_gep_i8(ctx, gs_vtxptr[vtx].value, lds_byte0_accept_flag));
-      }
-   }
-   ac_build_endif(&ctx->ac, 0);
-}
-
-static void add_clipdist_bit(struct si_shader_context *ctx, LLVMValueRef distance, unsigned i,
-                             LLVMValueRef *packed_data)
-{
-   LLVMValueRef neg = LLVMBuildFCmp(ctx->ac.builder, LLVMRealOLT, distance, ctx->ac.f32_0, "");
-   neg = LLVMBuildZExt(ctx->ac.builder, neg, ctx->ac.i32, "");
-   /* Put the negative distance flag into lds_byte3_clipdist_neg_mask. */
-   neg = LLVMBuildShl(ctx->ac.builder, neg, LLVMConstInt(ctx->ac.i32, 24 + i, 0), "");
-   *packed_data = LLVMBuildOr(ctx->ac.builder, *packed_data, neg, "");
-}
-
-static bool add_clipdist_bits_for_clipvertex(struct si_shader_context *ctx,
-                                             unsigned clipdist_enable,
-                                             LLVMValueRef clipvertex[4],
-                                             LLVMValueRef *packed_data)
-{
-   struct ac_export_args clipdist[2];
-   bool added = false;
-
-   si_llvm_clipvertex_to_clipdist(ctx, clipdist, clipvertex);
-
-   for (unsigned j = 0; j < 8; j++) {
-      if (!(clipdist_enable & BITFIELD_BIT(j)))
-         continue;
-
-      LLVMValueRef distance = clipdist[j / 4].out[j % 4];
-      add_clipdist_bit(ctx, distance, j, packed_data);
-      added = true;
-   }
-   return added;
-}
-
-static void cull_primitive(struct si_shader_context *ctx,
-                           LLVMValueRef pos[3][4], LLVMValueRef clipdist_accepted,
-                           LLVMValueRef out_prim_accepted, struct ac_llvm_pointer gs_vtxptr_accept[3])
-{
-   struct si_shader *shader = ctx->shader;
-   LLVMBuilderRef builder = ctx->ac.builder;
-
-   LLVMValueRef vp_scale[2] = {}, vp_translate[2] = {}, small_prim_precision = NULL;
-   LLVMValueRef clip_half_line_width[2] = {};
-
-   /* Load the viewport state for small prim culling. */
-   bool prim_is_lines = shader->key.ge.opt.ngg_culling & SI_NGG_CULL_LINES;
-   struct ac_llvm_pointer small_prim_cull_info_arg = ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->small_prim_cull_info);
-   /* Lines will always use the non-AA viewport transformation. */
-   LLVMValueRef vp = ac_build_load_to_sgpr(&ctx->ac, small_prim_cull_info_arg,
-                                           prim_is_lines ? ctx->ac.i32_1 : ctx->ac.i32_0);
-   vp = LLVMBuildBitCast(builder, vp, ctx->ac.v4f32, "");
-   vp_scale[0] = ac_llvm_extract_elem(&ctx->ac, vp, 0);
-   vp_scale[1] = ac_llvm_extract_elem(&ctx->ac, vp, 1);
-   vp_translate[0] = ac_llvm_extract_elem(&ctx->ac, vp, 2);
-   vp_translate[1] = ac_llvm_extract_elem(&ctx->ac, vp, 3);
-
-   /* Execute culling code. */
-   struct ac_cull_options options = {};
-   options.cull_view_xy = true;
-   options.cull_w = true;
-
-   if (prim_is_lines) {
-      small_prim_cull_info_arg.t = ctx->ac.v2f32;
-      LLVMValueRef terms = ac_build_load_to_sgpr(&ctx->ac, small_prim_cull_info_arg, LLVMConstInt(ctx->ac.i32, 4, 0));
-      terms = LLVMBuildBitCast(builder, terms, ctx->ac.v2f32, "");
-      clip_half_line_width[0] = ac_llvm_extract_elem(&ctx->ac, terms, 0);
-      clip_half_line_width[1] = ac_llvm_extract_elem(&ctx->ac, terms, 1);
-      small_prim_precision = GET_FIELD(ctx, GS_STATE_SMALL_PRIM_PRECISION_NO_AA);
-
-      options.num_vertices = 2;
-      options.cull_small_prims = shader->key.ge.opt.ngg_culling & SI_NGG_CULL_SMALL_LINES_DIAMOND_EXIT;
-
-      assert(!(shader->key.ge.opt.ngg_culling & SI_NGG_CULL_BACK_FACE));
-      assert(!(shader->key.ge.opt.ngg_culling & SI_NGG_CULL_FRONT_FACE));
-   } else {
-      /* Get the small prim filter precision. */
-      small_prim_precision = GET_FIELD(ctx, GS_STATE_SMALL_PRIM_PRECISION);
-
-      options.num_vertices = 3;
-      options.cull_front = shader->key.ge.opt.ngg_culling & SI_NGG_CULL_FRONT_FACE;
-      options.cull_back = shader->key.ge.opt.ngg_culling & SI_NGG_CULL_BACK_FACE;
-      options.cull_small_prims = true; /* this would only be false with conservative rasterization */
-      options.cull_zero_area = options.cull_front || options.cull_back;
-   }
-
-   /* Extract the small prim precision. */
-   small_prim_precision =
-      LLVMBuildOr(builder, small_prim_precision, LLVMConstInt(ctx->ac.i32, 0x70, 0), "");
-   small_prim_precision =
-      LLVMBuildShl(builder, small_prim_precision, LLVMConstInt(ctx->ac.i32, 23, 0), "");
-   small_prim_precision = LLVMBuildBitCast(builder, small_prim_precision, ctx->ac.f32, "");
-
-   /* Tell ES threads whether their vertex survived. */
-   LLVMValueRef params[] = {
-      out_prim_accepted,
-      (void*)gs_vtxptr_accept,
-   };
-   ac_cull_primitive(&ctx->ac, pos, clipdist_accepted, vp_scale, vp_translate,
-                     small_prim_precision, clip_half_line_width,
-                     &options, gfx10_build_primitive_accepted, params);
-}
-
-/**
- * Cull primitives for NGG VS or TES, then compact vertices, which happens
- * before the VS or TES main function. Return values for the main function.
- * Also return the position, which is passed to the shader as an input,
- * so that we don't compute it twice.
- */
-void gfx10_ngg_culling_build_end(struct si_shader_context *ctx)
-{
-   struct si_shader *shader = ctx->shader;
-   struct si_shader_selector *sel = shader->selector;
-   struct si_shader_info *info = &sel->info;
-   LLVMBuilderRef builder = ctx->ac.builder;
-   LLVMValueRef *addrs = ctx->abi.outputs;
-   unsigned max_waves = DIV_ROUND_UP(ctx->screen->ngg_subgroup_size, ctx->ac.wave_size);
-
-   assert(shader->key.ge.opt.ngg_culling);
-   assert(shader->key.ge.as_ngg);
-   assert(sel->stage == MESA_SHADER_VERTEX ||
-          (sel->stage == MESA_SHADER_TESS_EVAL && !shader->key.ge.as_es));
-
-   struct ac_llvm_pointer es_vtxptr = ngg_nogs_vertex_ptr(ctx, gfx10_get_thread_id_in_tg(ctx));
-   LLVMValueRef packed_data = ctx->ac.i32_0;
-   LLVMValueRef position[4] = {};
-   unsigned pos_index = 0;
-   unsigned clip_plane_enable = SI_NGG_CULL_GET_CLIP_PLANE_ENABLE(shader->key.ge.opt.ngg_culling);
-   unsigned clipdist_enable = (sel->info.clipdist_mask & clip_plane_enable) | sel->info.culldist_mask;
-   bool has_clipdist_mask = false;
-
-   for (unsigned i = 0; i < info->num_outputs; i++) {
-      LLVMValueRef clipvertex[4];
-      unsigned base;
-
-      switch (info->output_semantic[i]) {
-      case VARYING_SLOT_POS:
-         /* If we are going to cull everything (rasterizer_discard), discard
-          * the position. This is useful for analyzing maximum theoretical
-          * performance without VS input loads.
-          */
-         if (shader->key.ge.opt.ngg_culling & SI_NGG_CULL_FRONT_FACE &&
-             shader->key.ge.opt.ngg_culling & SI_NGG_CULL_BACK_FACE) {
-            for (unsigned j = 0; j < 4; j++)
-               LLVMBuildStore(builder, LLVMGetUndef(ctx->ac.f32), addrs[4 * i + j]);
-            break;
-         }
-
-         pos_index = i;
-         for (unsigned j = 0; j < 4; j++) {
-            position[j] = LLVMBuildLoad2(ctx->ac.builder, ctx->ac.f32, addrs[4 * i + j], "");
-         }
-
-         /* Store Position.W into LDS. */
-         LLVMBuildStore(
-            builder, ac_to_integer(&ctx->ac, position[3]),
-            ac_build_gep0(&ctx->ac, es_vtxptr, LLVMConstInt(ctx->ac.i32, lds_pos_cull_w, 0)));
-
-         /* Store Position.XY / W into LDS. */
-         for (unsigned chan = 0; chan < 2; chan++) {
-            LLVMValueRef val = ac_build_fdiv(&ctx->ac, position[chan], position[3]);
-            LLVMBuildStore(
-               builder, ac_to_integer(&ctx->ac, val),
-               ac_build_gep0(&ctx->ac, es_vtxptr, LLVMConstInt(ctx->ac.i32, lds_pos_cull_x_div_w + chan, 0)));
-         }
-         break;
-
-      case VARYING_SLOT_CLIP_DIST0:
-      case VARYING_SLOT_CLIP_DIST1:
-         base = info->output_semantic[i] == VARYING_SLOT_CLIP_DIST1 ? 4 : 0;
-
-         for (unsigned j = 0; j < 4; j++) {
-            unsigned index = base + j;
-
-            if (!(clipdist_enable & BITFIELD_BIT(index)))
-               continue;
-
-            LLVMValueRef distance = LLVMBuildLoad2(ctx->ac.builder, ctx->ac.f32, addrs[4 * i + j], "");
-            add_clipdist_bit(ctx, distance, index, &packed_data);
-            has_clipdist_mask = true;
-         }
-         break;
-
-      case VARYING_SLOT_CLIP_VERTEX:
-         for (unsigned j = 0; j < 4; j++)
-            clipvertex[j] = LLVMBuildLoad2(ctx->ac.builder, ctx->ac.f32, addrs[4 * i + j], "");
-
-         if (add_clipdist_bits_for_clipvertex(ctx, clipdist_enable, clipvertex, &packed_data))
-            has_clipdist_mask = true;
-         break;
-      }
-   }
-
-   if (clip_plane_enable && !sel->info.clipdist_mask) {
-      /* When clip planes are enabled and there are no clip distance outputs,
-       * we should use user clip planes and cull against the position.
-       */
-      assert(!has_clipdist_mask);
-      if (add_clipdist_bits_for_clipvertex(ctx, clipdist_enable, position, &packed_data))
-         has_clipdist_mask = true;
-   }
-
-   /* Initialize the packed data. */
-   LLVMBuildStore(
-      builder, packed_data,
-      ac_build_gep0(&ctx->ac, es_vtxptr, LLVMConstInt(ctx->ac.i32, lds_packed_data, 0)));
-   ac_build_endif(&ctx->ac, ctx->merged_wrap_if_label);
-
-   ac_build_waitcnt(&ctx->ac, AC_WAIT_LGKM);
-   ac_build_s_barrier(&ctx->ac, ctx->stage);
-
-   LLVMValueRef tid = ac_get_thread_id(&ctx->ac);
-
-   unsigned num_vertices;
-   ngg_get_vertices_per_prim(ctx, &num_vertices);
-
-   /* The hardware requires that there are no holes between unculled vertices,
-    * which means we have to pack ES threads, i.e. reduce the ES thread count
-    * and move ES input VGPRs to lower threads. The upside is that varyings
-    * are only fetched and computed for unculled vertices.
-    *
-    * Vertex compaction:
-    *
-    * Part 1: Store the surviving vertex count for each wave in LDS.
-    *   - The GS culling code notifies ES threads which vertices were accepted.
-    *   - Barrier
-    *   - ES threads will compute the vertex count and store it in LDS.
-    * - Barrier
-    * - Each wave loads the vertex counts from LDS.
-    *
-    * Part 2: Compact ES threads:
-    * - Compute the prefix sum for each surviving vertex. This is the new thread ID
-    *   of the vertex.
-    * - Write input VGPRs and vertex positions for each surviving vertex into the LDS
-    *   address of the new thread ID.
-    * - Now kill all waves that have inactive threads.
-    * - Barrier
-    * - Update vertex indices and null flag in the GS input VGPRs.
-    *
-    * Part 3: Update inputs GPRs
-    * - For all waves, update per-wave thread counts in input SGPRs.
-    * - In ES threads, update the ES input VGPRs (VertexID, InstanceID, TES inputs).
-    */
-
-   LLVMValueRef vtxindex[3];
-   for (unsigned i = 0; i < num_vertices; ++i)
-      vtxindex[i] = si_unpack_param(ctx, ctx->args.gs_vtx_offset[i / 2], (i & 1) * 16, 16);
-
-   struct ac_llvm_pointer gs_vtxptr[3];
-   for (unsigned i = 0; i < num_vertices; i++)
-      gs_vtxptr[i] = ngg_nogs_vertex_ptr(ctx, vtxindex[i]);
-
-   es_vtxptr = ngg_nogs_vertex_ptr(ctx, gfx10_get_thread_id_in_tg(ctx));
-
-   /* Adding these optimization barriers improves the generated code as follows. Crazy right?
-    *
-    * - s_mov_b32 s4, 0xffff
-    * - v_lshrrev_b32_e32 v10, 16, v0
-    * - v_and_b32_e32 v12, s4, v0
-    * - v_and_b32_e32 v11, s4, v1
-    *   s_bfe_u32 s4, s3, 0x80008
-    * - s_mov_b64 s[8:9], 0
-    * - v_mul_u32_u24_e32 v0, 28, v10
-    * - v_mul_u32_u24_e32 v9, 28, v12
-    * - v_mul_u32_u24_e32 v1, 28, v11
-    * + v_mov_b32_e32 v11, 28
-    *   v_cmp_gt_u32_e32 vcc, s4, v2
-    * + s_mov_b64 s[8:9], 0
-    *   s_waitcnt lgkmcnt(0)
-    *   s_barrier
-    * + v_mul_u32_u24_sdwa v10, v0, v11 dst_sel:DWORD dst_unused:UNUSED_PAD src0_sel:WORD_0 src1_sel:DWORD
-    * + v_mul_u32_u24_sdwa v23, v0, v11 dst_sel:DWORD dst_unused:UNUSED_PAD src0_sel:WORD_1 src1_sel:DWORD
-    * + v_mul_u32_u24_sdwa v0, v1, v11 dst_sel:DWORD dst_unused:UNUSED_PAD src0_sel:WORD_0 src1_sel:DWORD
-    *   s_and_saveexec_b64 s[44:45], vcc
-    *   s_cbranch_execz BB2_8
-    * - v_mul_u32_u24_e32 v16, 28, v12
-    * - v_mul_u32_u24_e32 v17, 28, v11
-    * - v_mul_u32_u24_e32 v18, 28, v10
-    */
-   for (unsigned i = 0; i < num_vertices; i++)
-      ac_build_optimization_barrier(&ctx->ac, &gs_vtxptr[i].value, false);
-
-   LLVMValueRef gs_accepted = ac_build_alloca(&ctx->ac, ctx->ac.i32, "");
-
-   /* Do culling in GS threads. */
-   ac_build_ifcc(&ctx->ac, si_is_gs_thread(ctx), 16002);
-   {
-      /* Load positions. */
-      LLVMValueRef pos[3][4] = {};
-      LLVMValueRef clipdist_neg_mask = NULL;
-
-      for (unsigned vtx = 0; vtx < num_vertices; vtx++) {
-         for (unsigned chan = 0; chan < 4; chan++) {
-            unsigned index;
-            if (chan == 0 || chan == 1)
-               index = lds_pos_cull_x_div_w + chan;
-            else if (chan == 3)
-               index = lds_pos_cull_w;
-            else
-               continue;
-
-            LLVMValueRef idx = LLVMConstInt(ctx->ac.i32, index, 0);
-            LLVMValueRef v = ac_build_gep0(&ctx->ac, gs_vtxptr[vtx], idx);
-            pos[vtx][chan] = LLVMBuildLoad2(builder, ac_build_gep0_type(gs_vtxptr[vtx].t, idx), v, "");
-            pos[vtx][chan] = ac_to_float(&ctx->ac, pos[vtx][chan]);
-         }
-
-         if (has_clipdist_mask) {
-            /* Load and AND clip distance masks. Each bit means whether that clip distance is
-             * negative. If all masks are AND'ed and the result is 0, the primitive isn't culled
-             * by clip distances.
-             */
-            LLVMValueRef addr = si_build_gep_i8(ctx, gs_vtxptr[vtx].value, lds_byte3_clipdist_neg_mask);
-            LLVMValueRef mask = LLVMBuildLoad2(builder, ctx->ac.i8, addr, "");
-            if (!clipdist_neg_mask)
-               clipdist_neg_mask = mask;
-            else
-               clipdist_neg_mask = LLVMBuildAnd(builder, clipdist_neg_mask, mask, "");
-         }
-      }
-
-      LLVMValueRef clipdist_accepted =
-         has_clipdist_mask ? LLVMBuildICmp(builder, LLVMIntEQ, clipdist_neg_mask, ctx->ac.i8_0, "")
-                           : ctx->ac.i1true;
-
-      cull_primitive(ctx, pos, clipdist_accepted, gs_accepted, gs_vtxptr);
-   }
-   ac_build_endif(&ctx->ac, 16002);
-
-   ac_build_waitcnt(&ctx->ac, AC_WAIT_LGKM);
-   ac_build_s_barrier(&ctx->ac, ctx->stage);
-
-   gs_accepted = LLVMBuildLoad2(builder, ctx->ac.i32, gs_accepted, "");
-
-   LLVMValueRef vertex_accepted = ac_build_alloca(&ctx->ac, ctx->ac.i1, "");
-   LLVMValueRef vertex_mask = ac_build_alloca(&ctx->ac, ctx->ac.iN_wavemask, "");
-
-   /* Convert the per-vertex accept flag to a vertex thread mask, store it in registers. */
-   ac_build_ifcc(&ctx->ac, si_is_es_thread(ctx), 16007);
-   {
-      LLVMValueRef accepted =
-         LLVMBuildLoad2(builder, ctx->ac.i8, si_build_gep_i8(ctx, es_vtxptr.value, lds_byte0_accept_flag), "");
-      accepted = LLVMBuildICmp(builder, LLVMIntNE, accepted, ctx->ac.i8_0, "");
-      LLVMValueRef mask = ac_get_i1_sgpr_mask(&ctx->ac, accepted);
-
-      LLVMBuildStore(builder, accepted, vertex_accepted);
-      LLVMBuildStore(builder, mask, vertex_mask);
-   }
-   ac_build_endif(&ctx->ac, 16007);
-
-   /* Store the per-wave vertex count to LDS. Non-ES waves store 0. */
-   vertex_mask = LLVMBuildLoad2(builder, ctx->ac.iN_wavemask, vertex_mask, "");
-   ac_build_ifcc(&ctx->ac, LLVMBuildICmp(builder, LLVMIntEQ, tid, ctx->ac.i32_0, ""), 16008);
-   {
-      LLVMValueRef vertex_count = ac_build_bit_count(&ctx->ac, vertex_mask);
-      LLVMBuildStore(builder, LLVMBuildTrunc(builder, vertex_count, ctx->ac.i8, ""),
-                     si_build_gep_i8_var(ctx, ctx->gs_ngg_scratch.value, get_wave_id_in_tg(ctx)));
-   }
-   ac_build_endif(&ctx->ac, 16008);
-
-   ac_build_waitcnt(&ctx->ac, AC_WAIT_LGKM);
-   ac_build_s_barrier(&ctx->ac, ctx->stage);
-
-   /* Load the vertex masks and compute the new ES thread count. */
-   LLVMValueRef new_num_es_threads, prefix_sum, kill_wave;
-   load_vertex_counts(ctx, ctx->gs_ngg_scratch, max_waves, tid, &new_num_es_threads,
-                      &prefix_sum);
-
-   bool uses_instance_id = ctx->stage == MESA_SHADER_VERTEX &&
-                           (sel->info.uses_instanceid ||
-                            shader->key.ge.part.vs.prolog.instance_divisor_is_one ||
-                            shader->key.ge.part.vs.prolog.instance_divisor_is_fetched);
-   bool uses_tes_prim_id = ctx->stage == MESA_SHADER_TESS_EVAL &&
-                           (sel->info.uses_primid || shader->key.ge.mono.u.vs_export_prim_id);
-
-   /* ES threads compute their prefix sum, which is the new ES thread ID.
-    * Then they write the vertex position and input VGPRs into the LDS address
-    * of the new thread ID. It will be used to load input VGPRs by compacted
-    * threads.
-    */
-   vertex_accepted = LLVMBuildLoad2(builder, ctx->ac.i1, vertex_accepted, "");
-   ac_build_ifcc(&ctx->ac, vertex_accepted, 16009);
-   {
-      /* Add the number of bits set in vertex_mask up to the current thread ID - 1
-       * to get the prefix sum.
-       */
-      prefix_sum = LLVMBuildAdd(builder, prefix_sum, ac_build_mbcnt(&ctx->ac, vertex_mask), "");
-
-      LLVMValueRef new_id = prefix_sum;
-      struct ac_llvm_pointer new_vtx = ngg_nogs_vertex_ptr(ctx, new_id);
-
-      LLVMBuildStore(builder, LLVMBuildTrunc(builder, new_id, ctx->ac.i8, ""),
-                     si_build_gep_i8(ctx, es_vtxptr.value, lds_byte1_new_thread_id));
-
-      /* Store Position.XYZW into LDS. */
-      for (unsigned chan = 0; chan < 4; chan++) {
-         LLVMBuildStore(
-            builder, ac_to_integer(&ctx->ac,
-                                   LLVMBuildLoad2(builder, ctx->ac.f32, addrs[4 * pos_index + chan], "")),
-            ac_build_gep0(&ctx->ac, new_vtx, LLVMConstInt(ctx->ac.i32, lds_pos_x + chan, 0)));
-      }
-
-      /* Store VertexID and InstanceID into LDS. ES threads will have to load them
-       * from LDS after vertex compaction and use them instead of their own
-       * system values.
-       */
-      if (ctx->stage == MESA_SHADER_VERTEX) {
-         LLVMBuildStore(
-            builder, ctx->abi.vertex_id,
-            ac_build_gep0(&ctx->ac, new_vtx, LLVMConstInt(ctx->ac.i32, lds_vertex_id, 0)));
-         if (uses_instance_id) {
-            LLVMBuildStore(
-               builder, ctx->abi.instance_id,
-               ac_build_gep0(&ctx->ac, new_vtx, LLVMConstInt(ctx->ac.i32, lds_instance_id, 0)));
-         }
-      } else {
-         assert(ctx->stage == MESA_SHADER_TESS_EVAL);
-         LLVMBuildStore(builder, ac_to_integer(&ctx->ac, ac_get_arg(&ctx->ac, ctx->args.tes_u)),
-                        ac_build_gep0(&ctx->ac, new_vtx, LLVMConstInt(ctx->ac.i32, lds_tes_u, 0)));
-         LLVMBuildStore(builder, ac_to_integer(&ctx->ac, ac_get_arg(&ctx->ac, ctx->args.tes_v)),
-                        ac_build_gep0(&ctx->ac, new_vtx, LLVMConstInt(ctx->ac.i32, lds_tes_v, 0)));
-         LLVMBuildStore(builder, LLVMBuildTrunc(builder, ac_get_arg(&ctx->ac, ctx->args.tes_rel_patch_id), ctx->ac.i8, ""),
-                        si_build_gep_i8(ctx, new_vtx.value, lds_byte2_tes_rel_patch_id));
-         if (uses_tes_prim_id) {
-            LLVMBuildStore(
-               builder, ac_get_arg(&ctx->ac, ctx->args.tes_patch_id),
-               ac_build_gep0(&ctx->ac, new_vtx, LLVMConstInt(ctx->ac.i32, lds_tes_patch_id, 0)));
-         }
-      }
-   }
-   ac_build_endif(&ctx->ac, 16009);
-
-   /* If all vertices are culled, set the primitive count to 0, so that all waves are culled here. */
-   LLVMValueRef num_primitives = ngg_get_prim_cnt(ctx);
-   num_primitives = LLVMBuildSelect(builder,
-                                    LLVMBuildICmp(builder, LLVMIntEQ, new_num_es_threads,
-                                                  ctx->ac.i32_0, ""),
-                                    ctx->ac.i32_0, num_primitives, "");
-   /* Kill waves that have inactive threads. */
-   kill_wave = LLVMBuildICmp(builder, LLVMIntULE,
-                             ac_build_imax(&ctx->ac, new_num_es_threads, num_primitives),
-                             LLVMBuildMul(builder, get_wave_id_in_tg(ctx),
-                                          LLVMConstInt(ctx->ac.i32, ctx->ac.wave_size, 0), ""),
-                             "");
-   ac_build_ifcc(&ctx->ac, kill_wave, 19202);
-   {
-      /* If we are killing wave 0, send that there are no primitives
-       * in this threadgroup.
-       */
-      ac_build_sendmsg_gs_alloc_req(&ctx->ac, get_wave_id_in_tg(ctx), ctx->ac.i32_0, ctx->ac.i32_0);
-      ac_build_s_endpgm(&ctx->ac);
-   }
-   ac_build_endif(&ctx->ac, 19202);
-
-   ac_build_waitcnt(&ctx->ac, AC_WAIT_LGKM);
-   ac_build_s_barrier(&ctx->ac, ctx->stage);
-
-   /* Send the final vertex and primitive counts. */
-   ac_build_sendmsg_gs_alloc_req(&ctx->ac, get_wave_id_in_tg(ctx), new_num_es_threads,
-                                 ngg_get_prim_cnt(ctx));
-
-   /* Update thread counts in SGPRs. */
-   LLVMValueRef new_gs_tg_info = ac_get_arg(&ctx->ac, ctx->args.gs_tg_info);
-   LLVMValueRef new_merged_wave_info = ac_get_arg(&ctx->ac, ctx->args.merged_wave_info);
-
-   /* This also converts the thread count from the total count to the per-wave count. */
-   update_thread_counts(ctx, &new_num_es_threads, &new_gs_tg_info, 9, 12, &new_merged_wave_info, 8,
-                        0);
-
-   /* Update vertex indices in VGPR0 (same format as NGG passthrough).
-    *
-    * Set the null flag at the beginning (culled), and then
-    * overwrite it for accepted primitives.
-    */
-   LLVMValueRef new_vgpr0 =
-      ac_build_alloca_init(&ctx->ac, LLVMConstInt(ctx->ac.i32, 1u << 31, 0), "");
-
-   /* Get vertex indices after vertex compaction. */
-   ac_build_ifcc(&ctx->ac, LLVMBuildTrunc(builder, gs_accepted, ctx->ac.i1, ""), 16011);
-   {
-      struct ac_ngg_prim prim = {};
-      prim.num_vertices = num_vertices;
-      prim.isnull = ctx->ac.i1false;
-
-      if (gfx10_edgeflags_have_effect(shader))
-         prim.edgeflags = ac_pack_edgeflags_for_export(&ctx->ac, &ctx->args);
-      else
-         prim.edgeflags = ctx->ac.i32_0;
-
-      for (unsigned vtx = 0; vtx < num_vertices; vtx++) {
-         prim.index[vtx] = LLVMBuildLoad2(
-            builder, ctx->ac.i8, si_build_gep_i8(ctx, gs_vtxptr[vtx].value, lds_byte1_new_thread_id), "");
-         prim.index[vtx] = LLVMBuildZExt(builder, prim.index[vtx], ctx->ac.i32, "");
-      }
-
-      /* Set the new GS input VGPR. */
-      LLVMBuildStore(builder, ac_pack_prim_export(&ctx->ac, &prim), new_vgpr0);
-   }
-   ac_build_endif(&ctx->ac, 16011);
-
-   if (gfx10_ngg_export_prim_early(shader))
-      gfx10_ngg_build_export_prim(ctx, NULL, LLVMBuildLoad2(builder, ctx->ac.i32, new_vgpr0, ""));
-
-   /* Prepare LDS addresses of the new ES input VGPRs. */
-   LLVMValueRef input_vgpr_addresses[4] = {
-      ac_build_gep0(&ctx->ac, es_vtxptr, LLVMConstInt(ctx->ac.i32, lds_vertex_id, 0)),
-      ac_build_gep0(&ctx->ac, es_vtxptr, LLVMConstInt(ctx->ac.i32, lds_instance_id, 0)),
-   };
-   if (ctx->stage == MESA_SHADER_TESS_EVAL) {
-      input_vgpr_addresses[2] = si_build_gep_i8(ctx, es_vtxptr.v, lds_byte2_tes_rel_patch_id);
-      if (uses_tes_prim_id) {
-         input_vgpr_addresses[3] = ac_build_gep0(&ctx->ac, es_vtxptr,
-                                                 LLVMConstInt(ctx->ac.i32, lds_tes_patch_id, 0));
-      }
-   }
-
-   /* Return values for the main function. */
-   LLVMValueRef ret = ctx->return_value;
-   LLVMValueRef val;
-
-   ret = LLVMBuildInsertValue(ctx->ac.builder, ret, new_gs_tg_info, 2, "");
-   ret = LLVMBuildInsertValue(ctx->ac.builder, ret, new_merged_wave_info, 3, "");
-   if (ctx->stage == MESA_SHADER_TESS_EVAL)
-      ret = si_insert_input_ret(ctx, ret, ctx->args.tess_offchip_offset, 4);
-   if (ctx->ac.gfx_level >= GFX11)
-      ret = si_insert_input_ret(ctx, ret, ctx->args.gs_attr_offset, 5);
-
-   ret = si_insert_input_ptr(ctx, ret, ctx->internal_bindings, 8 + SI_SGPR_INTERNAL_BINDINGS);
-   ret = si_insert_input_ptr(ctx, ret, ctx->bindless_samplers_and_images,
-                             8 + SI_SGPR_BINDLESS_SAMPLERS_AND_IMAGES);
-   ret = si_insert_input_ptr(ctx, ret, ctx->const_and_shader_buffers,
-                             8 + SI_SGPR_CONST_AND_SHADER_BUFFERS);
-   ret = si_insert_input_ptr(ctx, ret, ctx->samplers_and_images, 8 + SI_SGPR_SAMPLERS_AND_IMAGES);
-   ret = si_insert_input_ptr(ctx, ret, ctx->vs_state_bits, 8 + SI_SGPR_VS_STATE_BITS);
-   if (ctx->ac.gfx_level >= GFX11)
-      ret = si_insert_input_ptr(ctx, ret, ctx->gs_attr_address, 8 + GFX9_SGPR_ATTRIBUTE_RING_ADDR);
-
-   if (ctx->stage == MESA_SHADER_VERTEX) {
-      ret = si_insert_input_ptr(ctx, ret, ctx->args.base_vertex, 8 + SI_SGPR_BASE_VERTEX);
-      ret = si_insert_input_ptr(ctx, ret, ctx->args.draw_id, 8 + SI_SGPR_DRAWID);
-      ret = si_insert_input_ptr(ctx, ret, ctx->args.start_instance, 8 + SI_SGPR_START_INSTANCE);
-      ret = si_insert_input_ptr(ctx, ret, ctx->args.vertex_buffers, 8 + GFX9_GS_NUM_USER_SGPR);
-
-      for (unsigned i = 0; i < shader->selector->info.num_vbos_in_user_sgprs; i++) {
-         ret = si_insert_input_v4i32(ctx, ret, ctx->vb_descriptors[i],
-                                     8 + SI_SGPR_VS_VB_DESCRIPTOR_FIRST + i * 4);
-      }
-   } else {
-      assert(ctx->stage == MESA_SHADER_TESS_EVAL);
-      ret = si_insert_input_ptr(ctx, ret, ctx->tcs_offchip_layout, 8 + SI_SGPR_TES_OFFCHIP_LAYOUT);
-      ret = si_insert_input_ptr(ctx, ret, ctx->tes_offchip_addr, 8 + SI_SGPR_TES_OFFCHIP_ADDR);
-   }
-
-   unsigned vgpr;
-   if (ctx->stage == MESA_SHADER_VERTEX) {
-      if (shader->selector->info.num_vbos_in_user_sgprs) {
-         vgpr = 8 + SI_SGPR_VS_VB_DESCRIPTOR_FIRST + shader->selector->info.num_vbos_in_user_sgprs * 4;
-      } else {
-         vgpr = 8 + GFX9_GS_NUM_USER_SGPR + 1;
-      }
-   } else {
-      vgpr = 8 + GFX9_GS_NUM_USER_SGPR;
-   }
-
-   val = LLVMBuildLoad2(builder, ctx->ac.i32, new_vgpr0, "");
-   ret = LLVMBuildInsertValue(builder, ret, ac_to_float(&ctx->ac, val), vgpr++, "");
-   vgpr++; /* gs_vtx_offset[1] = offsets of vertices 2-3  */
-
-   ret = si_insert_input_ret_float(ctx, ret, ctx->args.gs_prim_id, vgpr++);
-   ret = si_insert_input_ret_float(ctx, ret, ctx->args.gs_invocation_id, vgpr++);
-   vgpr++; /* gs_vtx_offset[2] = offsets of vertices 4-5 */
-
-   /* Set the input VPGRs to the corresponding LDS addresses where the VGPR values are
-    * stored. The VS prolog will load them.
-    */
-   if (ctx->stage == MESA_SHADER_VERTEX) {
-      val = LLVMBuildPtrToInt(builder, input_vgpr_addresses[0], ctx->ac.i32, "");
-      ret = LLVMBuildInsertValue(builder, ret, ac_to_float(&ctx->ac, val), vgpr++,
-                                 ""); /* VGPR5 - VertexID */
-      vgpr += 2;
-      if (uses_instance_id) {
-         val = LLVMBuildPtrToInt(builder, input_vgpr_addresses[1], ctx->ac.i32, "");
-         ret = LLVMBuildInsertValue(builder, ret, ac_to_float(&ctx->ac, val), vgpr++,
-                                    ""); /* VGPR8 - InstanceID */
-      } else {
-         vgpr++;
-      }
-   } else {
-      assert(ctx->stage == MESA_SHADER_TESS_EVAL);
-      unsigned num_vgprs = uses_tes_prim_id ? 4 : 3;
-      for (unsigned i = 0; i < num_vgprs; i++) {
-         val = LLVMBuildPtrToInt(builder, input_vgpr_addresses[i], ctx->ac.i32, "");
-         ret = LLVMBuildInsertValue(builder, ret, ac_to_float(&ctx->ac, val), vgpr++, "");
-      }
-      if (num_vgprs == 3)
-         vgpr++;
-   }
-
-   /* These two also use LDS. */
-   if (gfx10_ngg_writes_user_edgeflags(shader) ||
-       (ctx->stage == MESA_SHADER_VERTEX && shader->key.ge.mono.u.vs_export_prim_id)) {
-      ac_build_waitcnt(&ctx->ac, AC_WAIT_LGKM);
-      ac_build_s_barrier(&ctx->ac, ctx->stage);
-   }
-
-   ctx->return_value = ret;
+   return ac_build_load_to_sgpr(&ctx->ac, ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->internal_bindings),
+                                LLVMConstInt(ctx->ac.i32, SI_GS_QUERY_BUF, false));
 }
 
-/**
- * Emit the end of an API VS or TES shader compiled as ESGS shader.
- */
-void gfx10_ngg_build_end(struct si_shader_context *ctx)
+static LLVMValueRef ngg_get_emulated_counters_buf(struct si_shader_context *ctx)
 {
-   struct si_shader_selector *sel = ctx->shader->selector;
-   struct si_shader_info *info = &sel->info;
-   struct si_shader_output_values outputs[PIPE_MAX_SHADER_OUTPUTS];
-   LLVMBuilderRef builder = ctx->ac.builder;
-   LLVMValueRef *addrs = ctx->abi.outputs;
-   LLVMValueRef tmp, tmp2;
-
-   assert(!ctx->shader->is_gs_copy_shader);
-   assert(info->num_outputs <= AC_LLVM_MAX_OUTPUTS);
-
-   struct ac_llvm_pointer vertex_ptr = {};
-
-   if (ctx->so.num_outputs || gfx10_ngg_writes_user_edgeflags(ctx->shader))
-      vertex_ptr = ngg_nogs_vertex_ptr(ctx, gfx10_get_thread_id_in_tg(ctx));
-
-   for (unsigned i = 0; i < info->num_outputs; i++) {
-      outputs[i].semantic = info->output_semantic[i];
+   return ac_build_load_to_sgpr(&ctx->ac, ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->internal_bindings),
+                                LLVMConstInt(ctx->ac.i32, SI_GS_QUERY_EMULATED_COUNTERS_BUF, false));
+}
 
-      for (unsigned j = 0; j < 4; j++) {
-         outputs[i].vertex_streams = info->output_streams[i];
+unsigned gfx10_ngg_get_vertices_per_prim(struct si_shader *shader)
+{
+   const struct si_shader_info *info = &shader->selector->info;
 
-         /* TODO: we may store more outputs than streamout needs,
-          * but streamout performance isn't that important.
+   if (shader->selector->stage == MESA_SHADER_GEOMETRY)
+      return u_vertices_per_prim(info->base.gs.output_primitive);
+   else if (shader->selector->stage == MESA_SHADER_VERTEX) {
+      if (info->base.vs.blit_sgprs_amd) {
+         /* Blits always use axis-aligned rectangles with 3 vertices. */
+         return 3;
+      } else if (shader->key.ge.opt.ngg_culling & SI_NGG_CULL_LINES)
+         return 2;
+      else {
+         /* We always build up all three indices for the prim export
+          * independent of the primitive type. The additional garbage
+          * data shouldn't hurt. This is used by exports and streamout.
           */
-         if (ctx->so.num_outputs) {
-            LLVMValueRef idx = LLVMConstInt(ctx->ac.i32, 4 * i + j, false);
-            tmp = ac_build_gep0(&ctx->ac, vertex_ptr, idx);
-            tmp2 = LLVMBuildLoad2(builder, ac_build_gep0_type(vertex_ptr.t, idx), addrs[4 * i + j], "");
-            LLVMTypeRef type = ac_to_integer_type(&ctx->ac, ctx->ac.f32);
-            tmp2 = LLVMBuildBitCast(ctx->ac.builder, tmp2, type, "");
-            LLVMBuildStore(builder, tmp2, tmp);
-         }
-      }
-
-      /* Store the edgeflag at the end (if streamout is enabled) */
-      if (info->output_semantic[i] == VARYING_SLOT_EDGE && gfx10_ngg_writes_user_edgeflags(ctx->shader)) {
-         LLVMValueRef edgeflag = LLVMBuildLoad2(builder, ctx->ac.f32, addrs[4 * i], "");
-         /* The output is a float, but the hw expects a 1-bit integer. */
-         edgeflag = LLVMBuildFPToUI(ctx->ac.builder, edgeflag, ctx->ac.i32, "");
-         edgeflag = ac_build_umin(&ctx->ac, edgeflag, ctx->ac.i32_1);
-
-         tmp = LLVMConstInt(ctx->ac.i32, ngg_nogs_vertex_size(ctx->shader) - 1, 0);
-         tmp = ac_build_gep0(&ctx->ac, vertex_ptr, tmp);
-         LLVMBuildStore(builder, edgeflag, tmp);
+         return 3;
       }
-   }
-
-   bool unterminated_es_if_block =
-      !ctx->so.num_outputs && !gfx10_ngg_writes_user_edgeflags(ctx->shader) &&
-      !ctx->screen->use_ngg_streamout && /* no query buffer */
-      (ctx->stage != MESA_SHADER_VERTEX || !ctx->shader->key.ge.mono.u.vs_export_prim_id);
-
-   if (!unterminated_es_if_block)
-      ac_build_endif(&ctx->ac, ctx->merged_wrap_if_label);
-
-   LLVMValueRef is_gs_thread = si_is_gs_thread(ctx);
-   LLVMValueRef is_es_thread = si_is_es_thread(ctx);
-   LLVMValueRef vtxindex[3];
-
-   if (ctx->shader->key.ge.opt.ngg_culling || gfx10_is_ngg_passthrough(ctx->shader)) {
-      for (unsigned i = 0; i < 3; ++i)
-         vtxindex[i] = si_unpack_param(ctx, ctx->args.gs_vtx_offset[0], 10 * i, 9);
    } else {
-      for (unsigned i = 0; i < 3; ++i)
-         vtxindex[i] = si_unpack_param(ctx, ctx->args.gs_vtx_offset[i / 2], (i & 1) * 16, 16);
-   }
-
-   /* Determine the number of vertices per primitive. */
-   unsigned num_vertices;
-   LLVMValueRef num_vertices_val = ngg_get_vertices_per_prim(ctx, &num_vertices);
-
-   /* Streamout */
-   LLVMValueRef emitted_prims = NULL;
-
-   if (ctx->so.num_outputs) {
-      assert(!unterminated_es_if_block);
-
-      struct ngg_streamout nggso = {};
-      nggso.num_vertices = num_vertices_val;
-      nggso.prim_enable[0] = is_gs_thread;
-
-      for (unsigned i = 0; i < num_vertices; ++i)
-         nggso.vertices[i] = ngg_nogs_vertex_ptr(ctx, vtxindex[i]);
-
-      build_streamout(ctx, &nggso);
-      emitted_prims = nggso.emit[0];
-   }
-
-   LLVMValueRef user_edgeflags[3] = {};
-
-   if (gfx10_ngg_writes_user_edgeflags(ctx->shader)) {
-      assert(!unterminated_es_if_block);
-
-      /* Streamout already inserted the barrier, so don't insert it again. */
-      if (!ctx->so.num_outputs) {
-         ac_build_waitcnt(&ctx->ac, AC_WAIT_LGKM);
-         ac_build_s_barrier(&ctx->ac, ctx->stage);
-      }
-
-      ac_build_ifcc(&ctx->ac, is_gs_thread, 5400);
-      /* Load edge flags from ES threads and store them into VGPRs in GS threads. */
-      for (unsigned i = 0; i < num_vertices; i++) {
-         struct ac_llvm_pointer vt = ngg_nogs_vertex_ptr(ctx, vtxindex[i]);
-         tmp2 = LLVMConstInt(ctx->ac.i32, ngg_nogs_vertex_size(ctx->shader) - 1, 0);
-         tmp = LLVMBuildLoad2(builder, ac_build_gep0_type(vt.t, tmp2),
-                              ac_build_gep0(&ctx->ac, vt, tmp2), "");
-         tmp = LLVMBuildTrunc(builder, tmp, ctx->ac.i1, "");
-
-         user_edgeflags[i] = ac_build_alloca_init(&ctx->ac, tmp, "");
-      }
-      ac_build_endif(&ctx->ac, 5400);
-   }
-
-   /* Copy Primitive IDs from GS threads to the LDS address corresponding
-    * to the ES thread of the provoking vertex.
-    */
-   if (ctx->stage == MESA_SHADER_VERTEX && ctx->shader->key.ge.mono.u.vs_export_prim_id) {
-      assert(!unterminated_es_if_block);
-
-      /* Streamout and edge flags use LDS. Make it idle, so that we can reuse it. */
-      if (ctx->so.num_outputs || gfx10_ngg_writes_user_edgeflags(ctx->shader)) {
-         ac_build_waitcnt(&ctx->ac, AC_WAIT_LGKM);
-         ac_build_s_barrier(&ctx->ac, ctx->stage);
-      }
-
-      ac_build_ifcc(&ctx->ac, is_gs_thread, 5400);
-      /* Extract the PROVOKING_VTX_INDEX field. */
-      LLVMValueRef provoking_vtx_in_prim = GET_FIELD(ctx, GS_STATE_PROVOKING_VTX_INDEX);
-
-      /* provoking_vtx_index = vtxindex[provoking_vtx_in_prim]; */
-      LLVMValueRef indices = ac_build_gather_values(&ctx->ac, vtxindex, 3);
-      LLVMValueRef provoking_vtx_index =
-         LLVMBuildExtractElement(builder, indices, provoking_vtx_in_prim, "");
-      struct ac_llvm_pointer vertex_ptr = ngg_nogs_vertex_ptr(ctx, provoking_vtx_index);
-
-      LLVMBuildStore(builder, ac_get_arg(&ctx->ac, ctx->args.gs_prim_id),
-                     ac_build_gep0(&ctx->ac, vertex_ptr, ctx->ac.i32_0));
-      ac_build_endif(&ctx->ac, 5400);
-   }
-
-   /* Update query buffer */
-   if (ctx->screen->use_ngg_streamout && !info->base.vs.blit_sgprs_amd) {
-      assert(!unterminated_es_if_block);
-
-      tmp = GET_FIELD(ctx, GS_STATE_STREAMOUT_QUERY_ENABLED);
-      tmp = LLVMBuildTrunc(builder, tmp, ctx->ac.i1, "");
-      ac_build_ifcc(&ctx->ac, tmp, 5029); /* if (STREAMOUT_QUERY_ENABLED) */
-      tmp = LLVMBuildICmp(builder, LLVMIntEQ, get_wave_id_in_tg(ctx), ctx->ac.i32_0, "");
-      ac_build_ifcc(&ctx->ac, tmp, 5030);
-      tmp = LLVMBuildICmp(builder, LLVMIntULE, ac_get_thread_id(&ctx->ac),
-                          ctx->so.num_outputs ? ctx->ac.i32_1 : ctx->ac.i32_0, "");
-      ac_build_ifcc(&ctx->ac, tmp, 5031);
-      {
-         LLVMValueRef args[] = {
-            ngg_get_prim_cnt(ctx),
-            ngg_get_query_buf(ctx),
-            LLVMConstInt(ctx->ac.i32, 16, false), /* offset of stream[0].generated_primitives */
-            ctx->ac.i32_0,                        /* soffset */
-            ctx->ac.i32_0,                        /* cachepolicy */
-         };
-
-         if (ctx->so.num_outputs) {
-            args[0] = ac_build_writelane(&ctx->ac, args[0], emitted_prims, ctx->ac.i32_1);
-            args[2] = ac_build_writelane(&ctx->ac, args[2], LLVMConstInt(ctx->ac.i32, 24, false),
-                                         ctx->ac.i32_1);
-         }
-
-         /* TODO: should this be 64-bit atomics? */
-         ac_build_intrinsic(&ctx->ac, "llvm.amdgcn.raw.buffer.atomic.add.i32", ctx->ac.i32, args, 5,
-                            0);
-      }
-      ac_build_endif(&ctx->ac, 5031);
-      ac_build_endif(&ctx->ac, 5030);
-      ac_build_endif(&ctx->ac, 5029);
-   }
+      assert(shader->selector->stage == MESA_SHADER_TESS_EVAL);
 
-   /* Build the primitive export. */
-   if (!gfx10_ngg_export_prim_early(ctx->shader)) {
-      assert(!unterminated_es_if_block);
-      gfx10_ngg_build_export_prim(ctx, user_edgeflags, NULL);
+      if (info->base.tess.point_mode)
+         return 1;
+      else if (info->base.tess._primitive_mode == TESS_PRIMITIVE_ISOLINES)
+         return 2;
+      else
+         return 3;
    }
+}
 
-   /* Export per-vertex data (positions and parameters). */
-   if (!unterminated_es_if_block)
-      ac_build_ifcc(&ctx->ac, is_es_thread, 6002);
-   {
-      unsigned i;
-
-      /* Unconditionally (re-)load the values for proper SSA form. */
-      for (i = 0; i < info->num_outputs; i++) {
-         /* If the NGG cull shader part computed the position, don't
-          * use the position from the current shader part. Instead,
-          * load it from LDS.
-          */
-         if (info->output_semantic[i] == VARYING_SLOT_POS &&
-             ctx->shader->key.ge.opt.ngg_culling) {
-            vertex_ptr = ngg_nogs_vertex_ptr(ctx, gfx10_get_thread_id_in_tg(ctx));
-
-            for (unsigned j = 0; j < 4; j++) {
-               tmp = LLVMConstInt(ctx->ac.i32, lds_pos_x + j, 0);
-               LLVMValueRef v = ac_build_gep0(&ctx->ac, vertex_ptr, tmp);
-               tmp = LLVMBuildLoad2(builder, ac_build_gep0_type(vertex_ptr.t, tmp), v, "");
-               outputs[i].values[j] = LLVMBuildBitCast(ctx->ac.builder, tmp,
-                                                       ac_to_float_type(&ctx->ac, ctx->ac.f32), "");
-            }
-         } else {
-            for (unsigned j = 0; j < 4; j++) {
-               outputs[i].values[j] = LLVMBuildLoad2(builder, ctx->ac.f32, addrs[4 * i + j], "");
-            }
-         }
-      }
-
-      if (ctx->shader->key.ge.mono.u.vs_export_prim_id) {
-         outputs[i].semantic = VARYING_SLOT_PRIMITIVE_ID;
-         outputs[i].vertex_streams = 0;
-
-         if (ctx->stage == MESA_SHADER_VERTEX) {
-            /* Wait for LDS stores to finish. */
-            ac_build_waitcnt(&ctx->ac, AC_WAIT_LGKM);
-            ac_build_s_barrier(&ctx->ac, ctx->stage);
-
-            struct ac_llvm_pointer vt = ngg_nogs_vertex_ptr(ctx, gfx10_get_thread_id_in_tg(ctx));
-            outputs[i].values[0] = LLVMBuildLoad2(
-               builder, ac_build_gep0_type(vt.t, ctx->ac.i32_0), ac_build_gep0(&ctx->ac, vt, ctx->ac.i32_0), "");
-         } else {
-            assert(ctx->stage == MESA_SHADER_TESS_EVAL);
-            outputs[i].values[0] = si_get_primitive_id(ctx, 0);
-         }
+bool gfx10_ngg_export_prim_early(struct si_shader *shader)
+{
+   struct si_shader_selector *sel = shader->selector;
 
-         outputs[i].values[0] = LLVMBuildBitCast(ctx->ac.builder, outputs[i].values[0], ctx->ac.f32, "");
-         for (unsigned j = 1; j < 4; j++)
-            outputs[i].values[j] = LLVMGetUndef(ctx->ac.f32);
-         i++;
-      }
+   assert(shader->key.ge.as_ngg && !shader->key.ge.as_es);
 
-      si_llvm_build_vs_exports(ctx, NULL, outputs, i);
-   }
-   ac_build_endif(&ctx->ac, 6002);
+   return sel->stage != MESA_SHADER_GEOMETRY &&
+          !gfx10_ngg_writes_user_edgeflags(shader);
 }
 
 void gfx10_ngg_export_vertex(struct ac_shader_abi *abi)
@@ -1843,172 +152,6 @@ void gfx10_ngg_atomic_add_prim_count(struct ac_shader_abi *abi, unsigned stream,
                       ctx->ac.i32, args, 5, 0);
 }
 
-static struct ac_llvm_pointer ngg_gs_get_vertex_storage(struct si_shader_context *ctx)
-{
-   const struct si_shader_selector *sel = ctx->shader->selector;
-   const struct si_shader_info *info = &sel->info;
-
-   LLVMTypeRef elements[2] = {
-      LLVMArrayType(ctx->ac.i32, 4 * info->num_outputs),
-      LLVMArrayType(ctx->ac.i8, 4),
-   };
-   LLVMTypeRef type = LLVMStructTypeInContext(ctx->ac.context, elements, 2, false);
-   return (struct ac_llvm_pointer) {
-      .value = ctx->gs_ngg_emit,
-      .pointee_type = LLVMArrayType(type, 0)
-   };
-}
-
-/**
- * Return a pointer to the LDS storage reserved for the N'th vertex, where N
- * is in emit order; that is:
- * - at the shader end, N is the threadidx (relative to the entire threadgroup)
- * - during vertex emit, i.e. while the API GS shader invocation is running,
- *   N = threadidx * gs.vertices_out + emitidx
- *
- * Goals of the LDS memory layout:
- * 1. Eliminate bank conflicts on write for geometry shaders that have all emits
- *    in uniform control flow
- * 2. Eliminate bank conflicts on read for export if, additionally, there is no
- *    culling
- * 3. Agnostic to the number of waves (since we don't know it before compiling)
- * 4. Allow coalescing of LDS instructions (ds_write_b128 etc.)
- * 5. Avoid wasting memory.
- *
- * We use an AoS layout due to point 4 (this also helps point 3). In an AoS
- * layout, elimination of bank conflicts requires that each vertex occupy an
- * odd number of dwords. We use the additional dword to store the output stream
- * index as well as a flag to indicate whether this vertex ends a primitive
- * for rasterization.
- *
- * Swizzling is required to satisfy points 1 and 2 simultaneously.
- *
- * Vertices are stored in export order (gsthread * gs.vertices_out + emitidx).
- * Indices are swizzled in groups of 32, which ensures point 1 without
- * disturbing point 2.
- *
- * \return an LDS pointer to type {[N x i32], [4 x i8]}
- */
-static struct ac_llvm_pointer ngg_gs_vertex_ptr(struct si_shader_context *ctx, LLVMValueRef vertexidx)
-{
-   struct si_shader_selector *sel = ctx->shader->selector;
-   LLVMBuilderRef builder = ctx->ac.builder;
-   struct ac_llvm_pointer storage = ngg_gs_get_vertex_storage(ctx);
-
-   /* gs.vertices_out = 2^(write_stride_2exp) * some odd number */
-   unsigned write_stride_2exp = ffs(sel->info.base.gs.vertices_out) - 1;
-   if (write_stride_2exp) {
-      LLVMValueRef row = LLVMBuildLShr(builder, vertexidx, LLVMConstInt(ctx->ac.i32, 5, false), "");
-      LLVMValueRef swizzle = LLVMBuildAnd(
-         builder, row, LLVMConstInt(ctx->ac.i32, (1u << write_stride_2exp) - 1, false), "");
-      vertexidx = LLVMBuildXor(builder, vertexidx, swizzle, "");
-   }
-
-   return (struct ac_llvm_pointer) {
-      .value = ac_build_gep0(&ctx->ac, storage, vertexidx),
-      .pointee_type = ac_build_gep0_type(storage.t, vertexidx)
-   };
-}
-
-static struct ac_llvm_pointer ngg_gs_emit_vertex_ptr(struct si_shader_context *ctx, LLVMValueRef gsthread,
-                                                        LLVMValueRef emitidx)
-{
-   struct si_shader_selector *sel = ctx->shader->selector;
-   LLVMBuilderRef builder = ctx->ac.builder;
-   LLVMValueRef tmp;
-
-   tmp = LLVMConstInt(ctx->ac.i32, sel->info.base.gs.vertices_out, false);
-   tmp = LLVMBuildMul(builder, tmp, gsthread, "");
-   const LLVMValueRef vertexidx = LLVMBuildAdd(builder, tmp, emitidx, "");
-   return ngg_gs_vertex_ptr(ctx, vertexidx);
-}
-
-static LLVMValueRef ngg_gs_get_emit_output_ptr(struct si_shader_context *ctx,
-                                               struct ac_llvm_pointer vertexptr, unsigned out_idx)
-{
-   LLVMValueRef gep_idx[3] = {
-      ctx->ac.i32_0, /* implied C-style array */
-      ctx->ac.i32_0, /* first struct entry */
-      LLVMConstInt(ctx->ac.i32, out_idx, false),
-   };
-   return LLVMBuildGEP2(ctx->ac.builder, vertexptr.pointee_type, vertexptr.value, gep_idx, 3, "");
-}
-
-static LLVMValueRef ngg_gs_get_emit_primflag_ptr(struct si_shader_context *ctx,
-                                                 struct ac_llvm_pointer vertexptr, unsigned stream)
-{
-   LLVMValueRef gep_idx[3] = {
-      ctx->ac.i32_0, /* implied C-style array */
-      ctx->ac.i32_1, /* second struct entry */
-      LLVMConstInt(ctx->ac.i32, stream, false),
-   };
-   return LLVMBuildGEP2(ctx->ac.builder, vertexptr.pointee_type, vertexptr.value, gep_idx, 3, "");
-}
-
-void gfx10_ngg_gs_emit_vertex(struct si_shader_context *ctx, unsigned stream,
-                              LLVMValueRef vertexidx, LLVMValueRef *addrs)
-{
-   const struct si_shader_selector *sel = ctx->shader->selector;
-   const struct si_shader_info *info = &sel->info;
-   LLVMBuilderRef builder = ctx->ac.builder;
-
-   const struct ac_llvm_pointer vertexptr = ngg_gs_emit_vertex_ptr(ctx, gfx10_get_thread_id_in_tg(ctx), vertexidx);
-   unsigned out_idx = 0;
-   for (unsigned i = 0; i < info->num_outputs; i++) {
-      for (unsigned chan = 0; chan < 4; chan++, out_idx++) {
-         if (!(info->output_usagemask[i] & (1 << chan)) ||
-             ((info->output_streams[i] >> (2 * chan)) & 3) != stream)
-            continue;
-
-         LLVMValueRef out_val = LLVMBuildLoad2(builder, ctx->ac.f32, addrs[4 * i + chan], "");
-         LLVMTypeRef as_int = ac_to_integer_type(&ctx->ac, ctx->ac.f32);
-         out_val = LLVMBuildBitCast(ctx->ac.builder, out_val, as_int, "");
-         LLVMBuildStore(builder, out_val, ngg_gs_get_emit_output_ptr(ctx, vertexptr, out_idx));
-      }
-   }
-   assert(out_idx * 4 == info->gsvs_vertex_size);
-
-   /* Store the current number of emitted vertices to zero out remaining
-    * primitive flags in case the geometry shader doesn't emit the maximum
-    * number of vertices.
-    */
-   LLVMValueRef tmp = LLVMBuildAdd(builder, vertexidx, ctx->ac.i32_1, "");
-   LLVMBuildStore(builder, tmp, ctx->gs_next_vertex[stream]);
-
-   /* Determine and store whether this vertex completed a primitive. */
-   const LLVMValueRef curverts = LLVMBuildLoad2(builder, ctx->ac.i32, ctx->gs_curprim_verts[stream], "");
-
-   tmp = LLVMConstInt(ctx->ac.i32, u_vertices_per_prim(sel->info.base.gs.output_primitive) - 1, false);
-   const LLVMValueRef iscompleteprim = LLVMBuildICmp(builder, LLVMIntUGE, curverts, tmp, "");
-
-   /* Since the geometry shader emits triangle strips, we need to
-    * track which primitive is odd and swap vertex indices to get
-    * the correct vertex order.
-    */
-   LLVMValueRef is_odd = ctx->ac.i1false;
-   if (stream == 0 && u_vertices_per_prim(sel->info.base.gs.output_primitive) == 3) {
-      tmp = LLVMBuildAnd(builder, curverts, ctx->ac.i32_1, "");
-      is_odd = LLVMBuildICmp(builder, LLVMIntEQ, tmp, ctx->ac.i32_1, "");
-   }
-
-   tmp = LLVMBuildAdd(builder, curverts, ctx->ac.i32_1, "");
-   LLVMBuildStore(builder, tmp, ctx->gs_curprim_verts[stream]);
-
-   /* The per-vertex primitive flag encoding:
-    *   bit 0: whether this vertex finishes a primitive
-    *   bit 1: whether the primitive is odd (if we are emitting triangle strips)
-    */
-   tmp = LLVMBuildZExt(builder, iscompleteprim, ctx->ac.i8, "");
-   tmp = LLVMBuildOr(
-      builder, tmp,
-      LLVMBuildShl(builder, LLVMBuildZExt(builder, is_odd, ctx->ac.i8, ""), ctx->ac.i8_1, ""), "");
-   LLVMBuildStore(builder, tmp, ngg_gs_get_emit_primflag_ptr(ctx, vertexptr, stream));
-
-   tmp = LLVMBuildLoad2(builder, ctx->ac.i32, ctx->gs_generated_prims[stream], "");
-   tmp = LLVMBuildAdd(builder, tmp, LLVMBuildZExt(builder, iscompleteprim, ctx->ac.i32, ""), "");
-   LLVMBuildStore(builder, tmp, ctx->gs_generated_prims[stream]);
-}
-
 void gfx10_ngg_gs_emit_begin(struct si_shader_context *ctx)
 {
    LLVMBuilderRef builder = ctx->ac.builder;
@@ -2038,369 +181,6 @@ void gfx10_ngg_gs_emit_begin(struct si_shader_context *ctx)
    }
 }
 
-void gfx10_ngg_gs_build_end(struct si_shader_context *ctx)
-{
-   const struct si_shader_selector *sel = ctx->shader->selector;
-   const struct si_shader_info *info = &sel->info;
-   const unsigned verts_per_prim = u_vertices_per_prim(sel->info.base.gs.output_primitive);
-   LLVMBuilderRef builder = ctx->ac.builder;
-   LLVMValueRef i8_0 = LLVMConstInt(ctx->ac.i8, 0, false);
-   LLVMValueRef tmp, tmp2;
-
-   /* Zero out remaining (non-emitted) primitive flags.
-    *
-    * Note: Alternatively, we could pass the relevant gs_next_vertex to
-    *       the emit threads via LDS. This is likely worse in the expected
-    *       typical case where each GS thread emits the full set of
-    *       vertices.
-    */
-   for (unsigned stream = 0; stream < 4; ++stream) {
-      if (!info->num_stream_output_components[stream])
-         continue;
-
-      const LLVMValueRef gsthread = gfx10_get_thread_id_in_tg(ctx);
-
-      ac_build_bgnloop(&ctx->ac, 5100);
-
-      const LLVMValueRef vertexidx = LLVMBuildLoad2(builder, ctx->ac.i32, ctx->gs_next_vertex[stream], "");
-      tmp = LLVMBuildICmp(builder, LLVMIntUGE, vertexidx,
-                          LLVMConstInt(ctx->ac.i32, sel->info.base.gs.vertices_out, false), "");
-      ac_build_ifcc(&ctx->ac, tmp, 5101);
-      ac_build_break(&ctx->ac);
-      ac_build_endif(&ctx->ac, 5101);
-
-      tmp = LLVMBuildAdd(builder, vertexidx, ctx->ac.i32_1, "");
-      LLVMBuildStore(builder, tmp, ctx->gs_next_vertex[stream]);
-
-      struct ac_llvm_pointer vt = ngg_gs_emit_vertex_ptr(ctx, gsthread, vertexidx);
-      LLVMBuildStore(builder, i8_0, ngg_gs_get_emit_primflag_ptr(ctx, vt, stream));
-
-      ac_build_endloop(&ctx->ac, 5100);
-   }
-
-   /* Accumulate generated primitives counts across the entire threadgroup. */
-   for (unsigned stream = 0; stream < 4; ++stream) {
-      if (!info->num_stream_output_components[stream])
-         continue;
-
-      LLVMValueRef numprims = LLVMBuildLoad2(builder, ctx->ac.i32, ctx->gs_generated_prims[stream], "");
-      numprims = ac_build_reduce(&ctx->ac, numprims, nir_op_iadd, ctx->ac.wave_size);
-
-      tmp = LLVMBuildICmp(builder, LLVMIntEQ, ac_get_thread_id(&ctx->ac), ctx->ac.i32_0, "");
-      ac_build_ifcc(&ctx->ac, tmp, 5105);
-      {
-         LLVMBuildAtomicRMW(
-            builder, LLVMAtomicRMWBinOpAdd,
-            ac_build_gep0(&ctx->ac, ctx->gs_ngg_scratch, LLVMConstInt(ctx->ac.i32, stream, false)),
-            numprims, LLVMAtomicOrderingMonotonic, false);
-      }
-      ac_build_endif(&ctx->ac, 5105);
-   }
-
-   ac_build_endif(&ctx->ac, ctx->merged_wrap_if_label);
-
-   ac_build_waitcnt(&ctx->ac, AC_WAIT_LGKM);
-   ac_build_s_barrier(&ctx->ac, ctx->stage);
-
-   const LLVMValueRef tid = gfx10_get_thread_id_in_tg(ctx);
-   LLVMValueRef num_emit_threads = ngg_get_prim_cnt(ctx);
-
-   /* Streamout */
-   if (ctx->so.num_outputs) {
-      struct ngg_streamout nggso = {};
-
-      nggso.num_vertices = LLVMConstInt(ctx->ac.i32, verts_per_prim, false);
-
-      struct ac_llvm_pointer vertexptr = ngg_gs_vertex_ptr(ctx, tid);
-      for (unsigned stream = 0; stream < 4; ++stream) {
-         if (!info->num_stream_output_components[stream])
-            continue;
-
-         tmp = LLVMBuildLoad2(builder, ctx->ac.i8, ngg_gs_get_emit_primflag_ptr(ctx, vertexptr, stream), "");
-         tmp = LLVMBuildTrunc(builder, tmp, ctx->ac.i1, "");
-         tmp2 = LLVMBuildICmp(builder, LLVMIntULT, tid, num_emit_threads, "");
-         nggso.prim_enable[stream] = LLVMBuildAnd(builder, tmp, tmp2, "");
-      }
-
-      for (unsigned i = 0; i < verts_per_prim; ++i) {
-         tmp = LLVMBuildSub(builder, tid, LLVMConstInt(ctx->ac.i32, verts_per_prim - i - 1, false),
-                            "");
-         struct ac_llvm_pointer vt = ngg_gs_vertex_ptr(ctx, tmp);
-         nggso.vertices[i].t = ac_build_gep0_type(vt.t, ctx->ac.i32_0);
-         nggso.vertices[i].v = ac_build_gep0(&ctx->ac, vt, ctx->ac.i32_0);
-      }
-
-      build_streamout(ctx, &nggso);
-   }
-
-   /* Write shader query data. */
-   if (ctx->screen->use_ngg_streamout) {
-      tmp = GET_FIELD(ctx, GS_STATE_STREAMOUT_QUERY_ENABLED);
-      tmp = LLVMBuildTrunc(builder, tmp, ctx->ac.i1, "");
-      ac_build_ifcc(&ctx->ac, tmp, 5109); /* if (STREAMOUT_QUERY_ENABLED) */
-      unsigned num_query_comps = ctx->so.num_outputs ? 8 : 4;
-      tmp = LLVMBuildICmp(builder, LLVMIntULT, tid,
-                          LLVMConstInt(ctx->ac.i32, num_query_comps, false), "");
-      ac_build_ifcc(&ctx->ac, tmp, 5110);
-      {
-         LLVMValueRef offset;
-         tmp = tid;
-         if (ctx->so.num_outputs)
-            tmp = LLVMBuildAnd(builder, tmp, LLVMConstInt(ctx->ac.i32, 3, false), "");
-         offset = LLVMBuildNUWMul(builder, tmp, LLVMConstInt(ctx->ac.i32, 32, false), "");
-         if (ctx->so.num_outputs) {
-            tmp = LLVMBuildLShr(builder, tid, LLVMConstInt(ctx->ac.i32, 2, false), "");
-            tmp = LLVMBuildNUWMul(builder, tmp, LLVMConstInt(ctx->ac.i32, 8, false), "");
-            offset = LLVMBuildAdd(builder, offset, tmp, "");
-         }
-
-         tmp = LLVMBuildLoad2(builder, ctx->ac.i32, ac_build_gep0(&ctx->ac, ctx->gs_ngg_scratch, tid), "");
-         LLVMValueRef args[] = {
-            tmp,           ngg_get_query_buf(ctx),
-            offset,        LLVMConstInt(ctx->ac.i32, 16, false), /* soffset */
-            ctx->ac.i32_0,                                       /* cachepolicy */
-         };
-         ac_build_intrinsic(&ctx->ac, "llvm.amdgcn.raw.buffer.atomic.add.i32", ctx->ac.i32, args, 5,
-                            0);
-      }
-      ac_build_endif(&ctx->ac, 5110);
-      ac_build_endif(&ctx->ac, 5109);
-   }
-
-   /* Cull primitives. */
-   if (ctx->shader->key.ge.opt.ngg_culling) {
-      assert(info->num_stream_output_components[0]);
-
-      struct ac_llvm_pointer gs_vtxptr = ngg_gs_vertex_ptr(ctx, tid);
-      LLVMValueRef live = LLVMBuildLoad2(builder, ctx->ac.i8, ngg_gs_get_emit_primflag_ptr(ctx, gs_vtxptr, 0), "");
-      live = LLVMBuildTrunc(builder, live, ctx->ac.i1, "");
-      LLVMValueRef is_emit = LLVMBuildICmp(builder, LLVMIntULT, tid, num_emit_threads, "");
-      LLVMValueRef prim_enable = LLVMBuildAnd(builder, live, is_emit, "");
-
-      /* Wait for streamout to finish before we kill primitives. */
-      if (ctx->so.num_outputs) {
-         ac_build_waitcnt(&ctx->ac, AC_WAIT_LGKM);
-         ac_build_s_barrier(&ctx->ac, ctx->stage);
-      }
-
-      ac_build_ifcc(&ctx->ac, prim_enable, 0);
-      {
-         struct ac_llvm_pointer vtxptr[3] = {};
-         LLVMValueRef pos[3][4] = {};
-
-         for (unsigned i = 0; i < verts_per_prim; i++) {
-            tmp = LLVMBuildSub(builder, tid, LLVMConstInt(ctx->ac.i32, verts_per_prim - i - 1, false), "");
-            struct ac_llvm_pointer vt = ngg_gs_vertex_ptr(ctx, tmp);
-            vtxptr[i].t = ac_build_gep0_type(vt.t, ctx->ac.i32_0);
-            vtxptr[i].v = ac_build_gep0(&ctx->ac, vt, ctx->ac.i32_0);
-         }
-
-         for (unsigned i = 0; i < info->num_outputs; i++) {
-            /* If the stream index is non-zero for all channels, skip the output. */
-            if (info->output_streams[i] & 0x3 &&
-                (info->output_streams[i] >> 2) & 0x3 &&
-                (info->output_streams[i] >> 4) & 0x3 &&
-                (info->output_streams[i] >> 6) & 0x3)
-               continue;
-
-            switch (info->output_semantic[i]) {
-            case VARYING_SLOT_POS:
-               /* Load the positions from LDS. */
-               for (unsigned vert = 0; vert < verts_per_prim; vert++) {
-                  for (unsigned comp = 0; comp < 4; comp++) {
-                     /* Z is not needed. */
-                     if (comp == 2)
-                        continue;
-
-                     LLVMValueRef idx = LLVMConstInt(ctx->ac.i32, 4 * i + comp, false);
-                     tmp = ac_build_gep0(&ctx->ac, vtxptr[vert], idx);
-                     pos[vert][comp] = LLVMBuildLoad2(builder,
-                                                      ac_build_gep0_type(vtxptr[vert].t, idx),
-                                                      tmp, "");
-                     pos[vert][comp] = ac_to_float(&ctx->ac, pos[vert][comp]);
-                  }
-               }
-
-               /* Divide XY by W. */
-               for (unsigned vert = 0; vert < verts_per_prim; vert++) {
-                  for (unsigned comp = 0; comp < 2; comp++)
-                     pos[vert][comp] = ac_build_fdiv(&ctx->ac, pos[vert][comp], pos[vert][3]);
-               }
-               break;
-            }
-         }
-
-         LLVMValueRef clipdist_accepted = ctx->ac.i1true; /* TODO */
-         LLVMValueRef accepted = ac_build_alloca(&ctx->ac, ctx->ac.i32, "");
-
-         cull_primitive(ctx, pos, clipdist_accepted, accepted, NULL);
-
-         accepted = LLVMBuildLoad2(builder, ctx->ac.i32, accepted, "");
-         LLVMValueRef rejected = LLVMBuildNot(builder, LLVMBuildTrunc(builder, accepted, ctx->ac.i1, ""), "");
-
-         ac_build_ifcc(&ctx->ac, rejected, 0);
-         LLVMBuildStore(builder, ctx->ac.i8_0, ngg_gs_get_emit_primflag_ptr(ctx, gs_vtxptr, 0));
-         ac_build_endif(&ctx->ac, 0);
-      }
-      ac_build_endif(&ctx->ac, 0);
-
-      ac_build_waitcnt(&ctx->ac, AC_WAIT_LGKM);
-      ac_build_s_barrier(&ctx->ac, ctx->stage);
-   }
-
-   /* Determine vertex liveness. */
-   LLVMValueRef vertliveptr = ac_build_alloca(&ctx->ac, ctx->ac.i1, "vertexlive");
-
-   tmp = LLVMBuildICmp(builder, LLVMIntULT, tid, num_emit_threads, "");
-   ac_build_ifcc(&ctx->ac, tmp, 5120);
-   {
-      for (unsigned i = 0; i < verts_per_prim; ++i) {
-         const LLVMValueRef primidx =
-            LLVMBuildAdd(builder, tid, LLVMConstInt(ctx->ac.i32, i, false), "");
-
-         if (i > 0) {
-            tmp = LLVMBuildICmp(builder, LLVMIntULT, primidx, num_emit_threads, "");
-            ac_build_ifcc(&ctx->ac, tmp, 5121 + i);
-         }
-
-         /* Load primitive liveness */
-         struct ac_llvm_pointer vt = ngg_gs_vertex_ptr(ctx, primidx);
-         tmp = LLVMBuildLoad2(builder, ctx->ac.i8, ngg_gs_get_emit_primflag_ptr(ctx, vt, 0), "");
-         const LLVMValueRef primlive = LLVMBuildTrunc(builder, tmp, ctx->ac.i1, "");
-
-         tmp = LLVMBuildLoad2(builder, ctx->ac.i1, vertliveptr, "");
-         tmp = LLVMBuildOr(builder, tmp, primlive, ""), LLVMBuildStore(builder, tmp, vertliveptr);
-
-         if (i > 0)
-            ac_build_endif(&ctx->ac, 5121 + i);
-      }
-   }
-   ac_build_endif(&ctx->ac, 5120);
-
-   /* Inclusive scan addition across the current wave. */
-   LLVMValueRef vertlive = LLVMBuildLoad2(builder, ctx->ac.i1, vertliveptr, "");
-   struct ac_wg_scan vertlive_scan = {};
-   vertlive_scan.stage = ctx->stage;
-   vertlive_scan.op = nir_op_iadd;
-   vertlive_scan.enable_reduce = true;
-   vertlive_scan.enable_exclusive = true;
-   vertlive_scan.src = vertlive;
-   vertlive_scan.scratch = ac_build_gep0(&ctx->ac, ctx->gs_ngg_scratch, ctx->ac.i32_0);
-   vertlive_scan.waveidx = get_wave_id_in_tg(ctx);
-   vertlive_scan.numwaves = get_tgsize(ctx);
-   vertlive_scan.maxwaves = DIV_ROUND_UP(256, ctx->ac.wave_size);
-
-   ac_build_wg_scan(&ctx->ac, &vertlive_scan);
-
-   /* Skip all exports (including index exports) when possible. */
-   LLVMValueRef have_exports =
-      LLVMBuildICmp(builder, LLVMIntNE, vertlive_scan.result_reduce, ctx->ac.i32_0, "");
-   num_emit_threads = LLVMBuildSelect(builder, have_exports, num_emit_threads, ctx->ac.i32_0, "");
-
-   /* Allocate export space. Send this message as early as possible, to
-    * hide the latency of the SQ <-> SPI roundtrip.
-    */
-   ac_build_sendmsg_gs_alloc_req(&ctx->ac, get_wave_id_in_tg(ctx), vertlive_scan.result_reduce,
-                                 num_emit_threads);
-
-   /* Setup the reverse vertex compaction permutation. We re-use stream 1
-    * of the primitive liveness flags, relying on the fact that each
-    * threadgroup can have at most 256 threads. */
-   ac_build_ifcc(&ctx->ac, vertlive, 5130);
-   {
-      struct ac_llvm_pointer vt = ngg_gs_vertex_ptr(ctx, vertlive_scan.result_exclusive);
-      tmp2 = LLVMBuildTrunc(builder, tid, ctx->ac.i8, "");
-      LLVMBuildStore(builder, tmp2, ngg_gs_get_emit_primflag_ptr(ctx, vt, 1));
-   }
-   ac_build_endif(&ctx->ac, 5130);
-
-   ac_build_waitcnt(&ctx->ac, AC_WAIT_LGKM);
-   ac_build_s_barrier(&ctx->ac, ctx->stage);
-
-   /* Export primitive data */
-   tmp = LLVMBuildICmp(builder, LLVMIntULT, tid, num_emit_threads, "");
-   ac_build_ifcc(&ctx->ac, tmp, 5140);
-   {
-      LLVMValueRef flags;
-      struct ac_ngg_prim prim = {};
-      prim.num_vertices = verts_per_prim;
-
-      struct ac_llvm_pointer vt = ngg_gs_vertex_ptr(ctx, tid);
-      flags = LLVMBuildLoad2(builder, ctx->ac.i8, ngg_gs_get_emit_primflag_ptr(ctx, vt, 0), "");
-      prim.isnull = LLVMBuildNot(builder, LLVMBuildTrunc(builder, flags, ctx->ac.i1, ""), "");
-      prim.edgeflags = ctx->ac.i32_0;
-
-      for (unsigned i = 0; i < verts_per_prim; ++i) {
-         prim.index[i] = LLVMBuildSub(builder, vertlive_scan.result_exclusive,
-                                      LLVMConstInt(ctx->ac.i32, verts_per_prim - i - 1, false), "");
-      }
-
-      /* Geometry shaders output triangle strips, but NGG expects triangles. */
-      if (verts_per_prim == 3) {
-         LLVMValueRef is_odd = LLVMBuildLShr(builder, flags, ctx->ac.i8_1, "");
-         is_odd = LLVMBuildTrunc(builder, is_odd, ctx->ac.i1, "");
-         LLVMValueRef flatshade_first = LLVMBuildICmp(
-            builder, LLVMIntEQ, GET_FIELD(ctx, GS_STATE_PROVOKING_VTX_INDEX), ctx->ac.i32_0, "");
-
-         ac_build_triangle_strip_indices_to_triangle(&ctx->ac, is_odd, flatshade_first, prim.index);
-      }
-
-      ac_build_export_prim(&ctx->ac, &prim);
-
-      if (ctx->screen->info.gfx_level < GFX11) {
-         tmp = GET_FIELD(ctx, GS_STATE_PIPELINE_STATS_EMU);
-         tmp = LLVMBuildTrunc(builder, tmp, ctx->ac.i1, "");
-         ac_build_ifcc(&ctx->ac, tmp, 5229); /* if (GS_PIPELINE_STATS_EMU) */
-         ac_build_ifcc(&ctx->ac, LLVMBuildNot(builder, prim.isnull, ""), 5237);
-         {
-            LLVMValueRef args[] = {
-               ctx->ac.i32_1,
-               ngg_get_emulated_counters_buf(ctx),
-               LLVMConstInt(ctx->ac.i32,
-                            si_query_pipestat_end_dw_offset(ctx->screen, PIPE_STAT_QUERY_GS_PRIMITIVES) * 4,
-                            false),
-               ctx->ac.i32_0,                            /* soffset */
-               ctx->ac.i32_0,                            /* cachepolicy */
-            };
-
-            ac_build_intrinsic(&ctx->ac, "llvm.amdgcn.raw.buffer.atomic.add.i32", ctx->ac.i32, args, 5, 0);
-         }
-         ac_build_endif(&ctx->ac, 5237);
-         ac_build_endif(&ctx->ac, 5229);
-      }
-   }
-   ac_build_endif(&ctx->ac, 5140);
-
-   /* Export position and parameter data */
-   LLVMValueRef num_export_threads = vertlive_scan.result_reduce;
-   tmp = LLVMBuildICmp(builder, LLVMIntULT, tid, num_export_threads, "");
-   ac_build_ifcc(&ctx->ac, tmp, 5145);
-   {
-      struct si_shader_output_values outputs[PIPE_MAX_SHADER_OUTPUTS];
-
-      struct ac_llvm_pointer vertexptr = ngg_gs_vertex_ptr(ctx, tid);
-      tmp = LLVMBuildLoad2(builder, ctx->ac.i8, ngg_gs_get_emit_primflag_ptr(ctx, vertexptr, 1), "");
-      tmp = LLVMBuildZExt(builder, tmp, ctx->ac.i32, "");
-      vertexptr = ngg_gs_vertex_ptr(ctx, tmp);
-
-      unsigned out_idx = 0;
-      for (unsigned i = 0; i < info->num_outputs; i++) {
-         outputs[i].semantic = info->output_semantic[i];
-
-         for (unsigned j = 0; j < 4; j++, out_idx++) {
-            tmp = ngg_gs_get_emit_output_ptr(ctx, vertexptr, out_idx);
-            tmp = LLVMBuildLoad2(builder, ctx->ac.i32, tmp, "");
-            assert(LLVMGetTypeKind(LLVMTypeOf(tmp)) != LLVMPointerTypeKind);
-            outputs[i].values[j] = ac_to_float(&ctx->ac, tmp);
-            outputs[i].vertex_streams = info->output_streams[i];
-         }
-      }
-
-      si_llvm_build_vs_exports(ctx, num_export_threads, outputs, info->num_outputs);
-   }
-   ac_build_endif(&ctx->ac, 5145);
-}
-
 static void clamp_gsprims_to_esverts(unsigned *max_gsprims, unsigned max_esverts,
                                      unsigned min_verts_per_prim, bool use_adjacency)
 {
diff --git a/src/gallium/drivers/radeonsi/si_shader_internal.h b/src/gallium/drivers/radeonsi/si_shader_internal.h
index 164c20a55be3..0017c5b90cf8 100644
--- a/src/gallium/drivers/radeonsi/si_shader_internal.h
+++ b/src/gallium/drivers/radeonsi/si_shader_internal.h
@@ -141,9 +141,6 @@ struct si_shader_context {
    LLVMValueRef tess_offchip_ring;
    LLVMValueRef instance_divisor_constbuf;
 
-   LLVMValueRef gs_next_vertex[4];
-   LLVMValueRef gs_curprim_verts[4];
-   LLVMValueRef gs_generated_prims[4];
    LLVMValueRef gs_ngg_emit;
    struct ac_llvm_pointer gs_ngg_scratch;
    LLVMValueRef return_value;
@@ -181,18 +178,10 @@ void si_fix_resource_usage(struct si_screen *sscreen, struct si_shader *shader);
 LLVMValueRef gfx10_get_thread_id_in_tg(struct si_shader_context *ctx);
 unsigned gfx10_ngg_get_vertices_per_prim(struct si_shader *shader);
 bool gfx10_ngg_export_prim_early(struct si_shader *shader);
-void gfx10_ngg_build_sendmsg_gs_alloc_req(struct si_shader_context *ctx);
-void gfx10_ngg_build_export_prim(struct si_shader_context *ctx, LLVMValueRef user_edgeflags[3],
-                                 LLVMValueRef prim_passthrough);
-void gfx10_ngg_culling_build_end(struct si_shader_context *ctx);
-void gfx10_ngg_build_end(struct si_shader_context *ctx);
 void gfx10_ngg_export_vertex(struct ac_shader_abi *abi);
 void gfx10_ngg_atomic_add_prim_count(struct ac_shader_abi *abi, unsigned stream,
                                      LLVMValueRef prim_count, enum ac_prim_count count_type);
-void gfx10_ngg_gs_emit_vertex(struct si_shader_context *ctx, unsigned stream,
-                              LLVMValueRef vertexidx, LLVMValueRef *addrs);
 void gfx10_ngg_gs_emit_begin(struct si_shader_context *ctx);
-void gfx10_ngg_gs_build_end(struct si_shader_context *ctx);
 unsigned gfx10_ngg_get_scratch_dw_size(struct si_shader *shader);
 bool gfx10_ngg_calculate_subgroup_info(struct si_shader *shader);
 
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c b/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c
index 7deecd389d0e..f5510c2bc6c2 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c
@@ -167,10 +167,7 @@ static void si_llvm_emit_vertex(struct ac_shader_abi *abi, unsigned stream,
 {
    struct si_shader_context *ctx = si_shader_context_from_abi(abi);
 
-   if (ctx->shader->key.ge.as_ngg) {
-      gfx10_ngg_gs_emit_vertex(ctx, stream, vertexidx, addrs);
-      return;
-   }
+   assert(!ctx->shader->key.ge.as_ngg);
 
    struct si_shader_info *info = &ctx->shader->selector->info;
    struct si_shader *shader = ctx->shader;
@@ -213,10 +210,7 @@ static void si_llvm_emit_primitive(struct ac_shader_abi *abi, unsigned stream)
 {
    struct si_shader_context *ctx = si_shader_context_from_abi(abi);
 
-   if (ctx->shader->key.ge.as_ngg) {
-      LLVMBuildStore(ctx->ac.builder, ctx->ac.i32_0, ctx->gs_curprim_verts[stream]);
-      return;
-   }
+   assert(!ctx->shader->key.ge.as_ngg);
 
    /* Signal primitive cut */
    ac_build_sendmsg(&ctx->ac, AC_SENDMSG_GS_OP_CUT | AC_SENDMSG_GS | (stream << 8),
-- 
GitLab


From b663ec10cae65010cd07bb0201878628279b397d Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Sun, 3 Jul 2022 17:32:33 +0800
Subject: [PATCH 09/45] ac/llvm: remove unused llvm cull
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/llvm/ac_llvm_cull.c | 359 ------------------------------------
 src/amd/llvm/ac_llvm_cull.h |  63 -------
 src/amd/llvm/meson.build    |   2 -
 3 files changed, 424 deletions(-)
 delete mode 100644 src/amd/llvm/ac_llvm_cull.c
 delete mode 100644 src/amd/llvm/ac_llvm_cull.h

diff --git a/src/amd/llvm/ac_llvm_cull.c b/src/amd/llvm/ac_llvm_cull.c
deleted file mode 100644
index d37a9f847f6e..000000000000
--- a/src/amd/llvm/ac_llvm_cull.c
+++ /dev/null
@@ -1,359 +0,0 @@
-/*
- * Copyright 2019 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the
- * "Software"), to deal in the Software without restriction, including
- * without limitation the rights to use, copy, modify, merge, publish,
- * distribute, sub license, and/or sell copies of the Software, and to
- * permit persons to whom the Software is furnished to do so, subject to
- * the following conditions:
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
- * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
- * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
- * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
- * USE OR OTHER DEALINGS IN THE SOFTWARE.
- *
- * The above copyright notice and this permission notice (including the
- * next paragraph) shall be included in all copies or substantial portions
- * of the Software.
- *
- */
-
-#include "ac_llvm_cull.h"
-
-#include <llvm-c/Core.h>
-
-struct ac_position_w_info {
-   /* If a primitive intersects the W=0 plane, it causes a reflection
-    * of the determinant used for face culling. Every vertex behind
-    * the W=0 plane negates the determinant, so having 2 vertices behind
-    * the plane has no effect. This is i1 true if the determinant should be
-    * negated.
-    */
-   LLVMValueRef w_reflection;
-
-   /* If we simplify the "-w <= p <= w" view culling equation, we get
-    * "-w <= w", which can't be satisfied when w is negative.
-    * In perspective projection, a negative W means that the primitive
-    * is behind the viewer, but the equation is independent of the type
-    * of projection.
-    *
-    * w_accepted is false when all W are negative and therefore
-    * the primitive is invisible.
-    */
-   LLVMValueRef w_accepted;
-
-   /* The bounding box culling doesn't work and should be skipped when this is true. */
-   LLVMValueRef any_w_negative;
-};
-
-static void ac_analyze_position_w(struct ac_llvm_context *ctx, LLVMValueRef pos[3][4],
-                                  struct ac_position_w_info *w, unsigned num_vertices)
-{
-   LLVMBuilderRef builder = ctx->builder;
-   LLVMValueRef all_w_negative = ctx->i1true;
-
-   w->w_reflection = ctx->i1false;
-   w->any_w_negative = ctx->i1false;
-
-   for (unsigned i = 0; i < num_vertices; i++) {
-      LLVMValueRef neg_w;
-
-      neg_w = LLVMBuildFCmp(builder, LLVMRealOLT, pos[i][3], ctx->f32_0, "");
-      /* If neg_w is true, negate w_reflection. */
-      w->w_reflection = LLVMBuildXor(builder, w->w_reflection, neg_w, "");
-      w->any_w_negative = LLVMBuildOr(builder, w->any_w_negative, neg_w, "");
-      all_w_negative = LLVMBuildAnd(builder, all_w_negative, neg_w, "");
-   }
-   w->w_accepted = LLVMBuildNot(builder, all_w_negative, "");
-}
-
-/* Perform front/back face culling and return true if the primitive is accepted. */
-static LLVMValueRef ac_cull_face(struct ac_llvm_context *ctx, LLVMValueRef pos[3][4],
-                                 struct ac_position_w_info *w, bool cull_front, bool cull_back,
-                                 bool cull_zero_area)
-{
-   LLVMBuilderRef builder = ctx->builder;
-
-   if (cull_front && cull_back)
-      return ctx->i1false;
-
-   if (!cull_front && !cull_back && !cull_zero_area)
-      return ctx->i1true;
-
-   /* Front/back face culling. Also if the determinant == 0, the triangle
-    * area is 0.
-    */
-   LLVMValueRef det_t0 = LLVMBuildFSub(builder, pos[2][0], pos[0][0], "");
-   LLVMValueRef det_t1 = LLVMBuildFSub(builder, pos[1][1], pos[0][1], "");
-   LLVMValueRef det_t2 = LLVMBuildFSub(builder, pos[0][0], pos[1][0], "");
-   LLVMValueRef det_t3 = LLVMBuildFSub(builder, pos[0][1], pos[2][1], "");
-   /* t0 * t1 - t2 * t3  =  t2 * -t3 + t0 * t1  =  fma(t2, -t3, t0 * t1) */
-   LLVMValueRef det = ac_build_fmad(ctx, det_t2, LLVMBuildFNeg(builder, det_t3, ""),
-                                    LLVMBuildFMul(builder, det_t0, det_t1, ""));
-
-   /* Negative W negates the determinant. */
-   det = LLVMBuildSelect(builder, w->w_reflection, LLVMBuildFNeg(builder, det, ""), det, "");
-
-   LLVMValueRef accepted = NULL;
-   if (cull_front) {
-      LLVMRealPredicate cond = cull_zero_area ? LLVMRealOGT : LLVMRealOGE;
-      accepted = LLVMBuildFCmp(builder, cond, det, ctx->f32_0, "");
-   } else if (cull_back) {
-      LLVMRealPredicate cond = cull_zero_area ? LLVMRealOLT : LLVMRealOLE;
-      accepted = LLVMBuildFCmp(builder, cond, det, ctx->f32_0, "");
-   } else if (cull_zero_area) {
-      accepted = LLVMBuildFCmp(builder, LLVMRealONE, det, ctx->f32_0, "");
-   }
-
-   if (accepted) {
-      /* Don't reject NaN and +/-infinity, these are tricky.
-       * Just trust fixed-function HW to handle these cases correctly.
-       */
-      accepted = LLVMBuildOr(builder, accepted, ac_build_is_inf_or_nan(ctx, det), "");
-   }
-
-   return accepted;
-}
-
-static void rotate_45degrees(struct ac_llvm_context *ctx, LLVMValueRef v[2])
-{
-   /* sin(45) == cos(45) */
-   LLVMValueRef sincos45 = LLVMConstReal(ctx->f32, 0.707106781);
-
-   /* x2  =  x*cos45 - y*sin45  =  x*sincos45 - y*sincos45
-    * y2  =  x*sin45 + y*cos45  =  x*sincos45 + y*sincos45
-    */
-   LLVMValueRef first = LLVMBuildFMul(ctx->builder, v[0], sincos45, "");
-
-   /* Doing 2x ffma while duplicating the multiplication is 33% faster than fmul+fadd+fadd. */
-   LLVMValueRef result[2] = {
-      ac_build_fmad(ctx, LLVMBuildFNeg(ctx->builder, v[1], ""), sincos45, first),
-      ac_build_fmad(ctx, v[1], sincos45, first),
-   };
-
-   memcpy(v, result, sizeof(result));
-}
-
-/* Perform view culling and small primitive elimination and return true
- * if the primitive is accepted and initially_accepted == true. */
-static void cull_bbox(struct ac_llvm_context *ctx, LLVMValueRef pos[3][4],
-                      LLVMValueRef initially_accepted, struct ac_position_w_info *w,
-                      LLVMValueRef vp_scale[2], LLVMValueRef vp_translate[2],
-                      LLVMValueRef small_prim_precision,
-                      LLVMValueRef clip_half_line_width[2],
-                      struct ac_cull_options *options,
-                      ac_cull_accept_func accept_func, void *userdata)
-{
-   LLVMBuilderRef builder = ctx->builder;
-
-   if (!options->cull_view_xy && !options->cull_view_near_z && !options->cull_view_far_z &&
-       !options->cull_small_prims) {
-      if (accept_func)
-         accept_func(ctx, initially_accepted, userdata);
-      return;
-   }
-
-   ac_build_ifcc(ctx, initially_accepted, 10000000);
-   {
-      LLVMValueRef bbox_min[3], bbox_max[3];
-      LLVMValueRef accepted = ctx->i1true;
-
-      /* Compute the primitive bounding box for easy culling. */
-      for (unsigned chan = 0; chan < (options->cull_view_near_z ||
-                                      options->cull_view_far_z ? 3 : 2); chan++) {
-         assert(options->num_vertices >= 2);
-         bbox_min[chan] = ac_build_fmin(ctx, pos[0][chan], pos[1][chan]);
-         bbox_max[chan] = ac_build_fmax(ctx, pos[0][chan], pos[1][chan]);
-
-         if (options->num_vertices == 3) {
-            bbox_min[chan] = ac_build_fmin(ctx, bbox_min[chan], pos[2][chan]);
-            bbox_max[chan] = ac_build_fmax(ctx, bbox_max[chan], pos[2][chan]);
-         }
-
-         if (clip_half_line_width[chan]) {
-            bbox_min[chan] = LLVMBuildFSub(builder, bbox_min[chan], clip_half_line_width[chan], "");
-            bbox_max[chan] = LLVMBuildFAdd(builder, bbox_max[chan], clip_half_line_width[chan], "");
-         }
-      }
-
-      /* View culling. */
-      if (options->cull_view_xy || options->cull_view_near_z || options->cull_view_far_z) {
-         for (unsigned chan = 0; chan < 3; chan++) {
-            LLVMValueRef visible;
-
-            if ((options->cull_view_xy && chan <= 1) || (options->cull_view_near_z && chan == 2)) {
-               float t = chan == 2 && options->use_halfz_clip_space ? 0 : -1;
-               visible = LLVMBuildFCmp(builder, LLVMRealOGE, bbox_max[chan],
-                                       LLVMConstReal(ctx->f32, t), "");
-               accepted = LLVMBuildAnd(builder, accepted, visible, "");
-            }
-
-            if ((options->cull_view_xy && chan <= 1) || (options->cull_view_far_z && chan == 2)) {
-               visible = LLVMBuildFCmp(builder, LLVMRealOLE, bbox_min[chan], ctx->f32_1, "");
-               accepted = LLVMBuildAnd(builder, accepted, visible, "");
-            }
-         }
-      }
-
-      /* Small primitive culling - triangles. */
-      if (options->cull_small_prims && options->num_vertices == 3) {
-         /* Assuming a sample position at (0.5, 0.5), if we round
-          * the bounding box min/max extents and the results of
-          * the rounding are equal in either the X or Y direction,
-          * the bounding box does not intersect the sample.
-          *
-          * See these GDC slides for pictures:
-          * https://frostbite-wp-prd.s3.amazonaws.com/wp-content/uploads/2016/03/29204330/GDC_2016_Compute.pdf
-          */
-         LLVMValueRef min, max, not_equal[2], visible;
-
-         for (unsigned chan = 0; chan < 2; chan++) {
-            /* Convert the position to screen-space coordinates. */
-            min = ac_build_fmad(ctx, bbox_min[chan], vp_scale[chan], vp_translate[chan]);
-            max = ac_build_fmad(ctx, bbox_max[chan], vp_scale[chan], vp_translate[chan]);
-            /* Scale the bounding box according to the precision of
-             * the rasterizer and the number of MSAA samples. */
-            min = LLVMBuildFSub(builder, min, small_prim_precision, "");
-            max = LLVMBuildFAdd(builder, max, small_prim_precision, "");
-
-            /* Determine if the bbox intersects the sample point.
-             * It also works for MSAA, but vp_scale, vp_translate,
-             * and small_prim_precision are computed differently.
-             */
-            min = ac_build_round(ctx, min);
-            max = ac_build_round(ctx, max);
-            not_equal[chan] = LLVMBuildFCmp(builder, LLVMRealONE, min, max, "");
-         }
-         visible = LLVMBuildAnd(builder, not_equal[0], not_equal[1], "");
-         accepted = LLVMBuildAnd(builder, accepted, visible, "");
-      }
-
-      /* Small primitive culling - lines. */
-      if (options->cull_small_prims && options->num_vertices == 2) {
-         /* This only works with lines without perpendicular end caps (lines with perpendicular
-          * end caps are rasterized as quads and thus can't be culled as small prims in 99% of
-          * cases because line_width >= 1).
-          *
-          * This takes advantage of the diamont exit rule, which says that every pixel
-          * has a diamond inside it touching the pixel boundary and only if a line exits
-          * the diamond, that pixel is filled. If a line enters the diamond or stays
-          * outside the diamond, the pixel isn't filled.
-          *
-          * This algorithm is a little simpler than that. The space outside all diamonds also
-          * has the same diamond shape, which we'll call corner diamonds.
-          *
-          * The idea is to cull all lines that are entirely inside a diamond, including
-          * corner diamonds. If a line is entirely inside a diamond, it can be culled because
-          * it doesn't exit it. If a line is entirely inside a corner diamond, it can be culled
-          * because it doesn't enter any diamond and thus can't exit any diamond.
-          *
-          * The viewport is rotated by 45 degress to turn diamonds into squares, and a bounding
-          * box test is used to determine whether a line is entirely inside any square (diamond).
-          *
-          * The line width doesn't matter. Wide lines only duplicate filled pixels in either X or
-          * Y direction from the filled pixels. MSAA also doesn't matter. MSAA should ideally use
-          * perpendicular end caps that enable quad rasterization for lines. Thus, this should
-          * always use non-MSAA viewport transformation and non-MSAA small prim precision.
-          *
-          * A good test is piglit/lineloop because it draws 10k subpixel lines in a circle.
-          * It should contain no holes if this matches hw behavior.
-          */
-         LLVMValueRef v0[2], v1[2];
-
-         /* Get vertex positions in pixels. */
-         for (unsigned chan = 0; chan < 2; chan++) {
-            v0[chan] = ac_build_fmad(ctx, pos[0][chan], vp_scale[chan], vp_translate[chan]);
-            v1[chan] = ac_build_fmad(ctx, pos[1][chan], vp_scale[chan], vp_translate[chan]);
-         }
-
-         /* Rotate the viewport by 45 degress, so that diamonds become squares. */
-         rotate_45degrees(ctx, v0);
-         rotate_45degrees(ctx, v1);
-
-         LLVMValueRef not_equal[2];
-
-         for (unsigned chan = 0; chan < 2; chan++) {
-            /* The width of each square is sqrt(0.5), so scale it to 1 because we want
-             * round() to give us the position of the closest center of a square (diamond).
-             */
-            v0[chan] = LLVMBuildFMul(builder, v0[chan], LLVMConstReal(ctx->f32, 1.414213562), "");
-            v1[chan] = LLVMBuildFMul(builder, v1[chan], LLVMConstReal(ctx->f32, 1.414213562), "");
-
-            /* Compute the bounding box around both vertices. We do this because we must
-             * enlarge the line area by the precision of the rasterizer.
-             */
-            LLVMValueRef min = ac_build_fmin(ctx, v0[chan], v1[chan]);
-            LLVMValueRef max = ac_build_fmax(ctx, v0[chan], v1[chan]);
-
-            /* Enlarge the bounding box by the precision of the rasterizer. */
-            min = LLVMBuildFSub(builder, min, small_prim_precision, "");
-            max = LLVMBuildFAdd(builder, max, small_prim_precision, "");
-
-            /* Round the bounding box corners. If both rounded corners are equal,
-             * the bounding box is entirely inside a square (diamond).
-             */
-            min = ac_build_round(ctx, min);
-            max = ac_build_round(ctx, max);
-            not_equal[chan] = LLVMBuildFCmp(builder, LLVMRealONE, min, max, "");
-         }
-
-         accepted = LLVMBuildAnd(builder, accepted,
-                                 LLVMBuildOr(builder, not_equal[0], not_equal[1], ""), "");
-      }
-
-      /* Disregard the bounding box culling if any W is negative because the code
-       * doesn't work with that.
-       */
-      accepted = LLVMBuildOr(builder, accepted, w->any_w_negative, "");
-
-      if (accept_func)
-         accept_func(ctx, accepted, userdata);
-   }
-   ac_build_endif(ctx, 10000000);
-}
-
-/**
- * Return i1 true if the primitive is accepted (not culled).
- *
- * \param pos                   Vertex positions 3x vec4
- * \param initially_accepted    AND'ed with the result. Some computations can be
- *                              skipped if this is false.
- * \param vp_scale              Viewport scale XY.
- *                              For MSAA, multiply them by the number of samples.
- * \param vp_translate          Viewport translation XY.
- *                              For MSAA, multiply them by the number of samples.
- * \param small_prim_precision  Precision of small primitive culling. This should
- *                              be the same as or greater than the precision of
- *                              the rasterizer. Set to num_samples / 2^subpixel_bits.
- *                              subpixel_bits are defined by the quantization mode.
- * \param options               See ac_cull_options.
- * \param accept_func           Callback invoked in the inner-most branch where the primitive is accepted.
- */
-void ac_cull_primitive(struct ac_llvm_context *ctx, LLVMValueRef pos[3][4],
-                       LLVMValueRef initially_accepted, LLVMValueRef vp_scale[2],
-                       LLVMValueRef vp_translate[2], LLVMValueRef small_prim_precision,
-                       LLVMValueRef clip_half_line_width[2], struct ac_cull_options *options,
-                       ac_cull_accept_func accept_func, void *userdata)
-{
-   struct ac_position_w_info w;
-   ac_analyze_position_w(ctx, pos, &w, options->num_vertices);
-
-   /* W culling. */
-   LLVMValueRef accepted = options->cull_w ? w.w_accepted : ctx->i1true;
-   accepted = LLVMBuildAnd(ctx->builder, accepted, initially_accepted, "");
-
-   /* Face culling. */
-   accepted = LLVMBuildAnd(
-      ctx->builder, accepted,
-      ac_cull_face(ctx, pos, &w, options->cull_front, options->cull_back, options->cull_zero_area),
-      "");
-
-   /* View culling and small primitive elimination. */
-   cull_bbox(ctx, pos, accepted, &w, vp_scale, vp_translate, small_prim_precision,
-             clip_half_line_width, options, accept_func, userdata);
-}
diff --git a/src/amd/llvm/ac_llvm_cull.h b/src/amd/llvm/ac_llvm_cull.h
deleted file mode 100644
index dc978d3fe04e..000000000000
--- a/src/amd/llvm/ac_llvm_cull.h
+++ /dev/null
@@ -1,63 +0,0 @@
-/*
- * Copyright 2019 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the
- * "Software"), to deal in the Software without restriction, including
- * without limitation the rights to use, copy, modify, merge, publish,
- * distribute, sub license, and/or sell copies of the Software, and to
- * permit persons to whom the Software is furnished to do so, subject to
- * the following conditions:
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
- * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
- * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
- * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
- * USE OR OTHER DEALINGS IN THE SOFTWARE.
- *
- * The above copyright notice and this permission notice (including the
- * next paragraph) shall be included in all copies or substantial portions
- * of the Software.
- *
- */
-
-#ifndef AC_LLVM_CULL_H
-#define AC_LLVM_CULL_H
-
-#include "ac_llvm_build.h"
-
-struct ac_cull_options {
-   /* In general, I recommend setting all to true except view Z culling,
-    * which isn't so effective because W culling is cheaper and partially
-    * replaces near Z culling, and you don't need to set Position.z
-    * if Z culling is disabled.
-    *
-    * If something doesn't work, turn some of these off to find out what.
-    */
-   bool cull_front;
-   bool cull_back;
-   bool cull_view_xy;
-   bool cull_view_near_z;
-   bool cull_view_far_z;
-   bool cull_small_prims;
-   bool cull_zero_area;
-   bool cull_w; /* cull primitives with all W < 0 */
-
-   bool use_halfz_clip_space;
-
-   uint8_t num_vertices; /* 1..3 */
-};
-
-/* Callback invoked in the inner-most branch where the primitive is accepted. */
-typedef void (*ac_cull_accept_func)(struct ac_llvm_context *ctx, LLVMValueRef accepted,
-                                    void *userdata);
-
-void ac_cull_primitive(struct ac_llvm_context *ctx, LLVMValueRef pos[3][4],
-                       LLVMValueRef initially_accepted, LLVMValueRef vp_scale[2],
-                       LLVMValueRef vp_translate[2], LLVMValueRef small_prim_precision,
-                       LLVMValueRef clip_half_line_width[2], struct ac_cull_options *options,
-                       ac_cull_accept_func accept_func, void *userdata);
-
-#endif
diff --git a/src/amd/llvm/meson.build b/src/amd/llvm/meson.build
index d52709eb6716..183dc607fe69 100644
--- a/src/amd/llvm/meson.build
+++ b/src/amd/llvm/meson.build
@@ -21,8 +21,6 @@
 amd_common_llvm_files = files(
   'ac_llvm_build.c',
   'ac_llvm_build.h',
-  'ac_llvm_cull.c',
-  'ac_llvm_cull.h',
   'ac_llvm_helper.cpp',
   'ac_llvm_util.c',
   'ac_llvm_util.h',
-- 
GitLab


From 930185b8b0a8371d2c40b583239e0e3bed4ccc85 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Sun, 9 Oct 2022 10:30:24 +0800
Subject: [PATCH 10/45] radeonsi: cleanup si_llvm_build_vs_exports gfx11 code
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

It's now completely handled in ac_nir_lower_ngg.c
export_vertex_params_gfx11.

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 .../drivers/radeonsi/gfx10_shader_ngg.c       |  2 +-
 .../drivers/radeonsi/si_shader_internal.h     |  3 +-
 src/gallium/drivers/radeonsi/si_shader_llvm.c |  2 +-
 .../drivers/radeonsi/si_shader_llvm_gs.c      |  2 +-
 .../drivers/radeonsi/si_shader_llvm_vs.c      | 66 ++-----------------
 5 files changed, 9 insertions(+), 66 deletions(-)

diff --git a/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c b/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
index 36b67ffc5453..2f6489b3c895 100644
--- a/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
+++ b/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
@@ -119,7 +119,7 @@ void gfx10_ngg_export_vertex(struct ac_shader_abi *abi)
             LLVMBuildLoad2(ctx->ac.builder, ctx->ac.f32, addrs[4 * i + j], "");
    }
 
-   si_llvm_build_vs_exports(ctx, NULL, outputs, num_outputs);
+   si_llvm_build_vs_exports(ctx, outputs, num_outputs);
 }
 
 void gfx10_ngg_atomic_add_prim_count(struct ac_shader_abi *abi, unsigned stream,
diff --git a/src/gallium/drivers/radeonsi/si_shader_internal.h b/src/gallium/drivers/radeonsi/si_shader_internal.h
index 0017c5b90cf8..1a9a49faf279 100644
--- a/src/gallium/drivers/radeonsi/si_shader_internal.h
+++ b/src/gallium/drivers/radeonsi/si_shader_internal.h
@@ -222,7 +222,6 @@ bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *
                             struct si_shader *shader, const struct pipe_stream_output_info *so,
                             struct util_debug_callback *debug, struct nir_shader *nir,
                             bool free_nir);
-LLVMValueRef si_llvm_build_attr_ring_desc(struct si_shader_context *ctx);
 
 /* si_shader_llvm_gs.c */
 LLVMValueRef si_is_es_thread(struct si_shader_context *ctx);
@@ -263,7 +262,7 @@ void si_llvm_streamout_store_output(struct si_shader_context *ctx, LLVMValueRef
                                     struct si_shader_output_values *shader_out);
 void si_llvm_emit_streamout(struct si_shader_context *ctx, struct si_shader_output_values *outputs,
                             unsigned noutput, unsigned stream);
-void si_llvm_build_vs_exports(struct si_shader_context *ctx, LLVMValueRef num_export_threads,
+void si_llvm_build_vs_exports(struct si_shader_context *ctx,
                               struct si_shader_output_values *outputs, unsigned noutput);
 void si_llvm_vs_build_end(struct si_shader_context *ctx);
 void si_llvm_build_vs_prolog(struct si_shader_context *ctx, union si_shader_part_key *key);
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index dd5f5745c910..b255310198fc 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -732,7 +732,7 @@ static LLVMValueRef si_get_num_vertices_per_prim(struct si_shader_context *ctx)
    return LLVMConstInt(ctx->ac.i32, num_vertices, false);
 }
 
-LLVMValueRef si_llvm_build_attr_ring_desc(struct si_shader_context *ctx)
+static LLVMValueRef si_llvm_build_attr_ring_desc(struct si_shader_context *ctx)
 {
    struct si_shader *shader = ctx->shader;
 
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c b/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c
index f5510c2bc6c2..440a6d25dbb9 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c
@@ -526,7 +526,7 @@ struct si_shader *si_generate_gs_copy_shader(struct si_screen *sscreen,
 
       if (stream == 0) {
          si_vertex_color_clamping(&ctx, outputs, gsinfo->num_outputs);
-         si_llvm_build_vs_exports(&ctx, NULL, outputs, gsinfo->num_outputs);
+         si_llvm_build_vs_exports(&ctx, outputs, gsinfo->num_outputs);
       }
 
       LLVMBuildBr(builder, end_bb);
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c b/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c
index 597ea18eb952..417073ee2510 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c
@@ -503,10 +503,8 @@ static void si_llvm_init_vs_export_args(struct si_shader_context *ctx, const LLV
 /**
  * Generate export instructions for hardware VS shader stage or NGG GS stage
  * (position and parameter data only).
- *
- * \param num_export_threads  The number of threads that are active for exports. Only used by gfx11.
  */
-void si_llvm_build_vs_exports(struct si_shader_context *ctx, LLVMValueRef num_export_threads,
+void si_llvm_build_vs_exports(struct si_shader_context *ctx,
                               struct si_shader_output_values *outputs, unsigned noutput)
 {
    struct si_shader *shader = ctx->shader;
@@ -720,63 +718,9 @@ void si_llvm_build_vs_exports(struct si_shader_context *ctx, LLVMValueRef num_ex
                                   &param_exports[offset]);
    }
 
-   if (ctx->screen->info.gfx_level >= GFX11) {
-      /* Store primitive exports to alloca variables, so that we can read them outside this branch. */
-      for (unsigned i = 0; i < shader->info.nr_param_exports; i++) {
-         for (unsigned chan = 0; chan < 4; chan++) {
-            param_exports[i].out[chan] =
-               ac_build_alloca_init(&ctx->ac, param_exports[i].out[chan], "");
-         }
-      }
-      ac_build_endif(&ctx->ac, 0);
-
-      if (!num_export_threads)
-         num_export_threads = si_unpack_param(ctx, ctx->args.merged_wave_info, 0, 8);
-
-      /* We should always store full vec4s in groups of 8 lanes for the best performance even if
-       * some of them are garbage or have unused components, so align the number of export threads
-       * to 8.
-       */
-      num_export_threads = LLVMBuildAdd(ctx->ac.builder, num_export_threads,
-                                        LLVMConstInt(ctx->ac.i32, 7, 0), "");
-      num_export_threads = LLVMBuildAnd(ctx->ac.builder, num_export_threads,
-                                        LLVMConstInt(ctx->ac.i32, ~7, 0), "");
-      ac_build_ifcc(&ctx->ac,
-                    LLVMBuildICmp(ctx->ac.builder, LLVMIntULT,
-                                  ac_get_thread_id(&ctx->ac), num_export_threads, ""), 0);
-
-      LLVMValueRef attr_rsrc = si_llvm_build_attr_ring_desc(ctx);
-      LLVMValueRef attr_offset = LLVMBuildShl(ctx->ac.builder,
-                                              si_unpack_param(ctx, ctx->args.gs_attr_offset, 0, 15),
-                                              LLVMConstInt(ctx->ac.i32, 9, 0), ""); /* 512B increments */
-      LLVMValueRef vindex = gfx10_get_thread_id_in_tg(ctx);
-
-      LLVMValueRef soffset[32];
-
-      /* Compute scalar offsets first. */
-      for (unsigned i = 0; i < shader->info.nr_param_exports; i++) {
-         soffset[i] = LLVMBuildAdd(ctx->ac.builder, attr_offset,
-                                   LLVMConstInt(ctx->ac.i32, 32 * i * 16, 0), "");
-      }
-
-      /* Write attributes to the attribute ring buffer. */
-      for (unsigned i = 0; i < shader->info.nr_param_exports; i++) {
-         for (unsigned chan = 0; chan < 4; chan++) {
-            param_exports[i].out[chan] =
-               LLVMBuildLoad2(ctx->ac.builder, ctx->ac.f32, param_exports[i].out[chan], "");
-         }
-
-         LLVMValueRef vdata = ac_build_gather_values_extended(&ctx->ac, param_exports[i].out,
-                                                              4, 1, false);
-
-         ac_build_buffer_store_dword(&ctx->ac, attr_rsrc, vdata, vindex,
-                                     ctx->ac.i32_0, soffset[i], ac_swizzled);
-      }
-   } else {
-      /* Export attributes using parameter exports. */
-      for (unsigned i = 0; i < shader->info.nr_param_exports; i++)
-         ac_build_export(&ctx->ac, &param_exports[i]);
-   }
+   /* Export attributes using parameter exports. */
+   for (unsigned i = 0; i < shader->info.nr_param_exports; i++)
+      ac_build_export(&ctx->ac, &param_exports[i]);
 }
 
 void si_llvm_vs_build_end(struct si_shader_context *ctx)
@@ -813,7 +757,7 @@ void si_llvm_vs_build_end(struct si_shader_context *ctx)
       i++;
    }
 
-   si_llvm_build_vs_exports(ctx, NULL, outputs, i);
+   si_llvm_build_vs_exports(ctx, outputs, i);
    FREE(outputs);
 }
 
-- 
GitLab


From d8cb644ef228416160fb80b41faa7c9f4ce57a1d Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Fri, 5 Aug 2022 16:24:05 +0800
Subject: [PATCH 12/45] radeonsi: use native shader info when init streamout
 args
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

We are going to init shader args earlier, there is no such
pipe_stream_output_info when that time.

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/gallium/drivers/radeonsi/si_shader.c | 13 ++++++-------
 1 file changed, 6 insertions(+), 7 deletions(-)

diff --git a/src/gallium/drivers/radeonsi/si_shader.c b/src/gallium/drivers/radeonsi/si_shader.c
index 314ab3d6f912..186ff0c1df36 100644
--- a/src/gallium/drivers/radeonsi/si_shader.c
+++ b/src/gallium/drivers/radeonsi/si_shader.c
@@ -196,8 +196,7 @@ static void si_dump_streamout(struct pipe_stream_output_info *so)
    }
 }
 
-static void declare_streamout_params(struct si_shader_context *ctx,
-                                     struct pipe_stream_output_info *so)
+static void declare_streamout_params(struct si_shader_context *ctx)
 {
    if (ctx->screen->use_ngg_streamout) {
       if (ctx->stage == MESA_SHADER_TESS_EVAL)
@@ -206,7 +205,7 @@ static void declare_streamout_params(struct si_shader_context *ctx,
    }
 
    /* Streamout SGPRs. */
-   if (so->num_outputs) {
+   if (si_shader_uses_streamout(ctx->shader)) {
       ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.streamout_config);
       ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.streamout_write_index);
    } else if (ctx->stage == MESA_SHADER_TESS_EVAL) {
@@ -215,7 +214,7 @@ static void declare_streamout_params(struct si_shader_context *ctx,
 
    /* A streamout buffer offset is loaded if the stride is non-zero. */
    for (int i = 0; i < 4; i++) {
-      if (!so->stride[i])
+      if (!ctx->shader->selector->info.base.xfb_stride[i])
          continue;
 
       ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.streamout_offset[i]);
@@ -431,7 +430,7 @@ void si_init_shader_args(struct si_shader_context *ctx)
       ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->vs_state_bits);
 
       if (ctx->shader->is_gs_copy_shader) {
-         declare_streamout_params(ctx, &ctx->so);
+         declare_streamout_params(ctx);
          /* VGPRs */
          declare_vs_input_vgprs(ctx, &num_prolog_vgprs);
          break;
@@ -447,7 +446,7 @@ void si_init_shader_args(struct si_shader_context *ctx)
       } else if (shader->key.ge.as_ls) {
          /* no extra parameters */
       } else {
-         declare_streamout_params(ctx, &ctx->so);
+         declare_streamout_params(ctx);
       }
 
       /* VGPRs */
@@ -636,7 +635,7 @@ void si_init_shader_args(struct si_shader_context *ctx)
          ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
          ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.es2gs_offset);
       } else {
-         declare_streamout_params(ctx, &ctx->so);
+         declare_streamout_params(ctx);
          ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.tess_offchip_offset);
       }
 
-- 
GitLab


From c141f9d54b53843047c4aa9820f49d3be0acad98 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Mon, 8 Aug 2022 22:21:26 +0800
Subject: [PATCH 13/45] radeonsi: separate shader args from llvm
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Move shader args out of llvm context, so that we can init
it before get nir. This is for creating a nir lower abi pass
which load args directly in nir.

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 .../drivers/radeonsi/gfx10_shader_ngg.c       |   8 +-
 src/gallium/drivers/radeonsi/si_shader.c      | 542 +++++++++---------
 src/gallium/drivers/radeonsi/si_shader.h      |   2 +-
 .../drivers/radeonsi/si_shader_internal.h     |  53 +-
 src/gallium/drivers/radeonsi/si_shader_llvm.c | 116 ++--
 .../drivers/radeonsi/si_shader_llvm_gs.c      |  59 +-
 .../drivers/radeonsi/si_shader_llvm_ps.c      |  48 +-
 .../radeonsi/si_shader_llvm_resources.c       |  14 +-
 .../drivers/radeonsi/si_shader_llvm_tess.c    | 138 ++---
 .../drivers/radeonsi/si_shader_llvm_vs.c      |  31 +-
 10 files changed, 522 insertions(+), 489 deletions(-)

diff --git a/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c b/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
index 2f6489b3c895..d38c9453f7c6 100644
--- a/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
+++ b/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
@@ -28,7 +28,7 @@
 
 static LLVMValueRef get_wave_id_in_tg(struct si_shader_context *ctx)
 {
-   return si_unpack_param(ctx, ctx->args.merged_wave_info, 24, 4);
+   return si_unpack_param(ctx, ctx->args->ac.merged_wave_info, 24, 4);
 }
 
 LLVMValueRef gfx10_get_thread_id_in_tg(struct si_shader_context *ctx)
@@ -42,13 +42,15 @@ LLVMValueRef gfx10_get_thread_id_in_tg(struct si_shader_context *ctx)
 
 static LLVMValueRef ngg_get_query_buf(struct si_shader_context *ctx)
 {
-   return ac_build_load_to_sgpr(&ctx->ac, ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->internal_bindings),
+   return ac_build_load_to_sgpr(&ctx->ac,
+                                ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->internal_bindings),
                                 LLVMConstInt(ctx->ac.i32, SI_GS_QUERY_BUF, false));
 }
 
 static LLVMValueRef ngg_get_emulated_counters_buf(struct si_shader_context *ctx)
 {
-   return ac_build_load_to_sgpr(&ctx->ac, ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->internal_bindings),
+   return ac_build_load_to_sgpr(&ctx->ac,
+                                ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->internal_bindings),
                                 LLVMConstInt(ctx->ac.i32, SI_GS_QUERY_EMULATED_COUNTERS_BUF, false));
 }
 
diff --git a/src/gallium/drivers/radeonsi/si_shader.c b/src/gallium/drivers/radeonsi/si_shader.c
index 186ff0c1df36..5e8f1f29337c 100644
--- a/src/gallium/drivers/radeonsi/si_shader.c
+++ b/src/gallium/drivers/radeonsi/si_shader.c
@@ -196,28 +196,30 @@ static void si_dump_streamout(struct pipe_stream_output_info *so)
    }
 }
 
-static void declare_streamout_params(struct si_shader_context *ctx)
+static void declare_streamout_params(struct si_shader_args *args, struct si_shader *shader)
 {
-   if (ctx->screen->use_ngg_streamout) {
-      if (ctx->stage == MESA_SHADER_TESS_EVAL)
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+   struct si_shader_selector *sel = shader->selector;
+
+   if (sel->screen->use_ngg_streamout) {
+      if (sel->stage == MESA_SHADER_TESS_EVAL)
+         ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
       return;
    }
 
    /* Streamout SGPRs. */
-   if (si_shader_uses_streamout(ctx->shader)) {
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.streamout_config);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.streamout_write_index);
-   } else if (ctx->stage == MESA_SHADER_TESS_EVAL) {
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+   if (si_shader_uses_streamout(shader)) {
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.streamout_config);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.streamout_write_index);
+   } else if (sel->stage == MESA_SHADER_TESS_EVAL) {
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
    }
 
    /* A streamout buffer offset is loaded if the stride is non-zero. */
    for (int i = 0; i < 4; i++) {
-      if (!ctx->shader->selector->info.base.xfb_stride[i])
+      if (!sel->info.base.xfb_stride[i])
          continue;
 
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.streamout_offset[i]);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.streamout_offset[i]);
    }
 }
 
@@ -256,131 +258,135 @@ unsigned si_get_max_workgroup_size(const struct si_shader *shader)
    return max_work_group_size;
 }
 
-static void declare_const_and_shader_buffers(struct si_shader_context *ctx, bool assign_params)
+static void declare_const_and_shader_buffers(struct si_shader_args *args,
+                                             struct si_shader *shader,
+                                             bool assign_params)
 {
    enum ac_arg_type const_shader_buf_type;
 
-   if (ctx->shader->selector->info.base.num_ubos == 1 &&
-       ctx->shader->selector->info.base.num_ssbos == 0)
+   if (shader->selector->info.base.num_ubos == 1 &&
+       shader->selector->info.base.num_ssbos == 0)
       const_shader_buf_type = AC_ARG_CONST_FLOAT_PTR;
    else
       const_shader_buf_type = AC_ARG_CONST_DESC_PTR;
 
    ac_add_arg(
-      &ctx->args, AC_ARG_SGPR, 1, const_shader_buf_type,
-      assign_params ? &ctx->const_and_shader_buffers : &ctx->other_const_and_shader_buffers);
+      &args->ac, AC_ARG_SGPR, 1, const_shader_buf_type,
+      assign_params ? &args->const_and_shader_buffers : &args->other_const_and_shader_buffers);
 }
 
-static void declare_samplers_and_images(struct si_shader_context *ctx, bool assign_params)
+static void declare_samplers_and_images(struct si_shader_args *args, bool assign_params)
 {
-   ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_CONST_IMAGE_PTR,
-              assign_params ? &ctx->samplers_and_images : &ctx->other_samplers_and_images);
+   ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_CONST_IMAGE_PTR,
+              assign_params ? &args->samplers_and_images : &args->other_samplers_and_images);
 }
 
-static void declare_per_stage_desc_pointers(struct si_shader_context *ctx, bool assign_params)
+static void declare_per_stage_desc_pointers(struct si_shader_args *args,
+                                            struct si_shader *shader,
+                                            bool assign_params)
 {
-   declare_const_and_shader_buffers(ctx, assign_params);
-   declare_samplers_and_images(ctx, assign_params);
+   declare_const_and_shader_buffers(args, shader, assign_params);
+   declare_samplers_and_images(args, assign_params);
 }
 
-static void declare_global_desc_pointers(struct si_shader_context *ctx)
+static void declare_global_desc_pointers(struct si_shader_args *args)
 {
-   ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_CONST_DESC_PTR, &ctx->internal_bindings);
-   ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_CONST_IMAGE_PTR,
-              &ctx->bindless_samplers_and_images);
+   ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_CONST_DESC_PTR, &args->internal_bindings);
+   ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_CONST_IMAGE_PTR,
+              &args->bindless_samplers_and_images);
 }
 
-static void declare_vb_descriptor_input_sgprs(struct si_shader_context *ctx)
+static void declare_vb_descriptor_input_sgprs(struct si_shader_args *args,
+                                              struct si_shader *shader)
 {
-   ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_CONST_DESC_PTR, &ctx->args.vertex_buffers);
+   ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_CONST_DESC_PTR, &args->ac.vertex_buffers);
 
-   unsigned num_vbos_in_user_sgprs = ctx->shader->selector->info.num_vbos_in_user_sgprs;
+   unsigned num_vbos_in_user_sgprs = shader->selector->info.num_vbos_in_user_sgprs;
    if (num_vbos_in_user_sgprs) {
-      unsigned user_sgprs = ctx->args.num_sgprs_used;
+      unsigned user_sgprs = args->ac.num_sgprs_used;
 
-      if (si_is_merged_shader(ctx->shader))
+      if (si_is_merged_shader(shader))
          user_sgprs -= 8;
       assert(user_sgprs <= SI_SGPR_VS_VB_DESCRIPTOR_FIRST);
 
       /* Declare unused SGPRs to align VB descriptors to 4 SGPRs (hw requirement). */
       for (unsigned i = user_sgprs; i < SI_SGPR_VS_VB_DESCRIPTOR_FIRST; i++)
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* unused */
+         ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* unused */
 
-      assert(num_vbos_in_user_sgprs <= ARRAY_SIZE(ctx->vb_descriptors));
+      assert(num_vbos_in_user_sgprs <= ARRAY_SIZE(args->vb_descriptors));
       for (unsigned i = 0; i < num_vbos_in_user_sgprs; i++)
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, 4, AC_ARG_INT, &ctx->vb_descriptors[i]);
+         ac_add_arg(&args->ac, AC_ARG_SGPR, 4, AC_ARG_INT, &args->vb_descriptors[i]);
    }
 }
 
-static void declare_vs_input_vgprs(struct si_shader_context *ctx, unsigned *num_prolog_vgprs)
+static void declare_vs_input_vgprs(struct si_shader_args *args, struct si_shader *shader,
+                                   unsigned *num_prolog_vgprs)
 {
-   struct si_shader *shader = ctx->shader;
-
-   ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.vertex_id);
+   ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.vertex_id);
    if (shader->key.ge.as_ls) {
-      if (ctx->screen->info.gfx_level >= GFX11) {
-         ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, NULL); /* user VGPR */
-         ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, NULL); /* user VGPR */
-         ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.instance_id);
-      } else if (ctx->screen->info.gfx_level >= GFX10) {
-         ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.vs_rel_patch_id);
-         ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, NULL); /* user VGPR */
-         ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.instance_id);
+      if (shader->selector->screen->info.gfx_level >= GFX11) {
+         ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, NULL); /* user VGPR */
+         ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, NULL); /* user VGPR */
+         ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.instance_id);
+      } else if (shader->selector->screen->info.gfx_level >= GFX10) {
+         ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.vs_rel_patch_id);
+         ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, NULL); /* user VGPR */
+         ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.instance_id);
       } else {
-         ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.vs_rel_patch_id);
-         ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.instance_id);
-         ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, NULL); /* unused */
+         ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.vs_rel_patch_id);
+         ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.instance_id);
+         ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, NULL); /* unused */
       }
-   } else if (ctx->screen->info.gfx_level >= GFX10) {
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, NULL); /* user VGPR */
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT,
+   } else if (shader->selector->screen->info.gfx_level >= GFX10) {
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, NULL); /* user VGPR */
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT,
                  /* user vgpr or PrimID (legacy) */
-                 shader->key.ge.as_ngg ? NULL : &ctx->args.vs_prim_id);
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.instance_id);
+                 shader->key.ge.as_ngg ? NULL : &args->ac.vs_prim_id);
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.instance_id);
    } else {
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.instance_id);
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.vs_prim_id);
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, NULL); /* unused */
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.instance_id);
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.vs_prim_id);
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, NULL); /* unused */
    }
 
    if (!shader->is_gs_copy_shader) {
       /* Vertex load indices. */
       if (shader->selector->info.num_inputs) {
-         ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->vertex_index0);
+         ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->vertex_index0);
          for (unsigned i = 1; i < shader->selector->info.num_inputs; i++)
-            ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, NULL);
+            ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, NULL);
       }
       *num_prolog_vgprs += shader->selector->info.num_inputs;
    }
 }
 
-static void declare_vs_blit_inputs(struct si_shader_context *ctx, unsigned vs_blit_property)
+static void declare_vs_blit_inputs(struct si_shader_args *args, unsigned vs_blit_property)
 {
-   ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->vs_blit_inputs); /* i16 x1, y1 */
-   ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);                 /* i16 x1, y1 */
-   ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL);               /* depth */
+   ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->vs_blit_inputs); /* i16 x1, y1 */
+   ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);                  /* i16 x1, y1 */
+   ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL);                /* depth */
 
    if (vs_blit_property == SI_VS_BLIT_SGPRS_POS_COLOR) {
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL); /* color0 */
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL); /* color1 */
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL); /* color2 */
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL); /* color3 */
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL); /* color0 */
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL); /* color1 */
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL); /* color2 */
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL); /* color3 */
    } else if (vs_blit_property == SI_VS_BLIT_SGPRS_POS_TEXCOORD) {
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL); /* texcoord.x1 */
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL); /* texcoord.y1 */
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL); /* texcoord.x2 */
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL); /* texcoord.y2 */
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL); /* texcoord.z */
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL); /* texcoord.w */
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL); /* texcoord.x1 */
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL); /* texcoord.y1 */
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL); /* texcoord.x2 */
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL); /* texcoord.y2 */
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL); /* texcoord.z */
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL); /* texcoord.w */
    }
 }
 
-static void declare_tes_input_vgprs(struct si_shader_context *ctx)
+static void declare_tes_input_vgprs(struct si_shader_args *args)
 {
-   ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_FLOAT, &ctx->args.tes_u);
-   ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_FLOAT, &ctx->args.tes_v);
-   ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.tes_rel_patch_id);
-   ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.tes_patch_id);
+   ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_FLOAT, &args->ac.tes_u);
+   ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_FLOAT, &args->ac.tes_v);
+   ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.tes_rel_patch_id);
+   ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.tes_patch_id);
 }
 
 enum
@@ -397,138 +403,139 @@ void si_add_arg_checked(struct ac_shader_args *args, enum ac_arg_regfile file, u
    ac_add_arg(args, file, registers, type, arg);
 }
 
-void si_init_shader_args(struct si_shader_context *ctx)
+void si_init_shader_args(struct si_shader *shader, struct si_shader_args *args)
 {
-   struct si_shader *shader = ctx->shader;
    unsigned i, num_returns, num_return_sgprs;
    unsigned num_prolog_vgprs = 0;
-   unsigned stage = ctx->stage;
+   struct si_shader_selector *sel = shader->selector;
+   unsigned stage = shader->is_gs_copy_shader ? MESA_SHADER_VERTEX : sel->stage;
+   unsigned stage_case = stage;
 
-   memset(&ctx->args, 0, sizeof(ctx->args));
+   memset(args, 0, sizeof(*args));
 
    /* Set MERGED shaders. */
-   if (ctx->screen->info.gfx_level >= GFX9 && stage <= MESA_SHADER_GEOMETRY) {
+   if (sel->screen->info.gfx_level >= GFX9 && stage <= MESA_SHADER_GEOMETRY) {
       if (shader->key.ge.as_ls || stage == MESA_SHADER_TESS_CTRL)
-         stage = SI_SHADER_MERGED_VERTEX_TESSCTRL; /* LS or HS */
+         stage_case = SI_SHADER_MERGED_VERTEX_TESSCTRL; /* LS or HS */
       else if (shader->key.ge.as_es || shader->key.ge.as_ngg || stage == MESA_SHADER_GEOMETRY)
-         stage = SI_SHADER_MERGED_VERTEX_OR_TESSEVAL_GEOMETRY;
+         stage_case = SI_SHADER_MERGED_VERTEX_OR_TESSEVAL_GEOMETRY;
    }
 
-   switch (stage) {
+   switch (stage_case) {
    case MESA_SHADER_VERTEX:
-      declare_global_desc_pointers(ctx);
+      declare_global_desc_pointers(args);
 
-      if (shader->selector->info.base.vs.blit_sgprs_amd) {
-         declare_vs_blit_inputs(ctx, shader->selector->info.base.vs.blit_sgprs_amd);
+      if (sel->info.base.vs.blit_sgprs_amd) {
+         declare_vs_blit_inputs(args, sel->info.base.vs.blit_sgprs_amd);
 
          /* VGPRs */
-         declare_vs_input_vgprs(ctx, &num_prolog_vgprs);
+         declare_vs_input_vgprs(args, shader, &num_prolog_vgprs);
          break;
       }
 
-      declare_per_stage_desc_pointers(ctx, true);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->vs_state_bits);
+      declare_per_stage_desc_pointers(args, shader, true);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->vs_state_bits);
 
-      if (ctx->shader->is_gs_copy_shader) {
-         declare_streamout_params(ctx);
+      if (shader->is_gs_copy_shader) {
+         declare_streamout_params(args, shader);
          /* VGPRs */
-         declare_vs_input_vgprs(ctx, &num_prolog_vgprs);
+         declare_vs_input_vgprs(args, shader, &num_prolog_vgprs);
          break;
       }
 
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.base_vertex);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.draw_id);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.start_instance);
-      declare_vb_descriptor_input_sgprs(ctx);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.base_vertex);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.draw_id);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.start_instance);
+      declare_vb_descriptor_input_sgprs(args, shader);
 
       if (shader->key.ge.as_es) {
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.es2gs_offset);
+         ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.es2gs_offset);
       } else if (shader->key.ge.as_ls) {
          /* no extra parameters */
       } else {
-         declare_streamout_params(ctx);
+         declare_streamout_params(args, shader);
       }
 
       /* VGPRs */
-      declare_vs_input_vgprs(ctx, &num_prolog_vgprs);
+      declare_vs_input_vgprs(args, shader, &num_prolog_vgprs);
       break;
 
    case MESA_SHADER_TESS_CTRL: /* GFX6-GFX8 */
-      declare_global_desc_pointers(ctx);
-      declare_per_stage_desc_pointers(ctx, true);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->tcs_offchip_layout);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->tcs_out_lds_offsets);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->tcs_out_lds_layout);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->vs_state_bits);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.tess_offchip_offset);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.tcs_factor_offset);
+      declare_global_desc_pointers(args);
+      declare_per_stage_desc_pointers(args, shader, true);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->tcs_offchip_layout);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->tcs_out_lds_offsets);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->tcs_out_lds_layout);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->vs_state_bits);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.tess_offchip_offset);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.tcs_factor_offset);
 
       /* VGPRs */
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.tcs_patch_id);
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.tcs_rel_ids);
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.tcs_patch_id);
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.tcs_rel_ids);
 
       /* param_tcs_offchip_offset and param_tcs_factor_offset are
        * placed after the user SGPRs.
        */
       for (i = 0; i < GFX6_TCS_NUM_USER_SGPR + 2; i++)
-         ac_add_return(&ctx->args, AC_ARG_SGPR);
+         ac_add_return(&args->ac, AC_ARG_SGPR);
       for (i = 0; i < 11; i++)
-         ac_add_return(&ctx->args, AC_ARG_VGPR);
+         ac_add_return(&args->ac, AC_ARG_VGPR);
       break;
 
    case SI_SHADER_MERGED_VERTEX_TESSCTRL:
       /* Merged stages have 8 system SGPRs at the beginning. */
       /* Gfx9-10: SPI_SHADER_USER_DATA_ADDR_LO/HI_HS */
       /* Gfx11+:  SPI_SHADER_PGM_LO/HI_HS */
-      declare_per_stage_desc_pointers(ctx, ctx->stage == MESA_SHADER_TESS_CTRL);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.tess_offchip_offset);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.merged_wave_info);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.tcs_factor_offset);
-      if (ctx->screen->info.gfx_level >= GFX11)
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.tcs_wave_id);
+      declare_per_stage_desc_pointers(args, shader, stage == MESA_SHADER_TESS_CTRL);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.tess_offchip_offset);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.merged_wave_info);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.tcs_factor_offset);
+      if (sel->screen->info.gfx_level >= GFX11)
+         ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.tcs_wave_id);
       else
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.scratch_offset);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* unused */
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* unused */
-
-      declare_global_desc_pointers(ctx);
-      declare_per_stage_desc_pointers(ctx, ctx->stage == MESA_SHADER_VERTEX);
-
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->vs_state_bits);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.base_vertex);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.draw_id);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.start_instance);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->tcs_offchip_layout);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->tcs_out_lds_offsets);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->tcs_out_lds_layout);
-      if (ctx->stage == MESA_SHADER_VERTEX)
-         declare_vb_descriptor_input_sgprs(ctx);
+         ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.scratch_offset);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* unused */
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* unused */
+
+      declare_global_desc_pointers(args);
+      declare_per_stage_desc_pointers(args, shader, stage == MESA_SHADER_VERTEX);
+
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->vs_state_bits);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.base_vertex);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.draw_id);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.start_instance);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->tcs_offchip_layout);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->tcs_out_lds_offsets);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->tcs_out_lds_layout);
+      if (stage == MESA_SHADER_VERTEX)
+         declare_vb_descriptor_input_sgprs(args, shader);
 
       /* VGPRs (first TCS, then VS) */
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.tcs_patch_id);
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.tcs_rel_ids);
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.tcs_patch_id);
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.tcs_rel_ids);
 
-      if (ctx->stage == MESA_SHADER_VERTEX) {
-         declare_vs_input_vgprs(ctx, &num_prolog_vgprs);
+      if (stage == MESA_SHADER_VERTEX) {
+         declare_vs_input_vgprs(args, shader, &num_prolog_vgprs);
 
          /* LS return values are inputs to the TCS main shader part. */
          for (i = 0; i < 8 + GFX9_TCS_NUM_USER_SGPR; i++)
-            ac_add_return(&ctx->args, AC_ARG_SGPR);
+            ac_add_return(&args->ac, AC_ARG_SGPR);
          for (i = 0; i < 2; i++)
-            ac_add_return(&ctx->args, AC_ARG_VGPR);
+            ac_add_return(&args->ac, AC_ARG_VGPR);
 
          /* VS outputs passed via VGPRs to TCS. */
          if (shader->key.ge.opt.same_patch_vertices) {
             unsigned num_outputs = util_last_bit64(shader->selector->info.outputs_written);
             for (i = 0; i < num_outputs * 4; i++)
-               ac_add_return(&ctx->args, AC_ARG_VGPR);
+               ac_add_return(&args->ac, AC_ARG_VGPR);
          }
       } else {
          /* TCS inputs are passed via VGPRs from VS. */
          if (shader->key.ge.opt.same_patch_vertices) {
             unsigned num_inputs = util_last_bit64(shader->previous_stage_sel->info.outputs_written);
             for (i = 0; i < num_inputs * 4; i++)
-               ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_FLOAT, NULL);
+               ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_FLOAT, NULL);
          }
 
          /* TCS return values are inputs to the TCS epilog.
@@ -538,9 +545,9 @@ void si_init_shader_args(struct si_shader_context *ctx)
           * should be passed to the epilog.
           */
          for (i = 0; i <= 8 + GFX9_SGPR_TCS_OUT_LAYOUT; i++)
-            ac_add_return(&ctx->args, AC_ARG_SGPR);
+            ac_add_return(&args->ac, AC_ARG_SGPR);
          for (i = 0; i < 11; i++)
-            ac_add_return(&ctx->args, AC_ARG_VGPR);
+            ac_add_return(&args->ac, AC_ARG_VGPR);
       }
       break;
 
@@ -548,157 +555,157 @@ void si_init_shader_args(struct si_shader_context *ctx)
       /* Merged stages have 8 system SGPRs at the beginning. */
       /* Gfx9-10: SPI_SHADER_USER_DATA_ADDR_LO/HI_GS */
       /* Gfx11+:  SPI_SHADER_PGM_LO/HI_GS */
-      declare_per_stage_desc_pointers(ctx, ctx->stage == MESA_SHADER_GEOMETRY);
+      declare_per_stage_desc_pointers(args, shader, stage == MESA_SHADER_GEOMETRY);
 
-      if (ctx->shader->key.ge.as_ngg)
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.gs_tg_info);
+      if (shader->key.ge.as_ngg)
+         ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.gs_tg_info);
       else
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.gs2vs_offset);
+         ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.gs2vs_offset);
 
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.merged_wave_info);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.tess_offchip_offset);
-      if (ctx->screen->info.gfx_level >= GFX11)
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.gs_attr_offset);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.merged_wave_info);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.tess_offchip_offset);
+      if (sel->screen->info.gfx_level >= GFX11)
+         ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.gs_attr_offset);
       else
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.scratch_offset);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* unused */
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* unused */
+         ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.scratch_offset);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* unused */
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* unused */
 
-      declare_global_desc_pointers(ctx);
-      if (ctx->stage != MESA_SHADER_VERTEX || !shader->selector->info.base.vs.blit_sgprs_amd) {
+      declare_global_desc_pointers(args);
+      if (stage != MESA_SHADER_VERTEX || !sel->info.base.vs.blit_sgprs_amd) {
          declare_per_stage_desc_pointers(
-            ctx, (ctx->stage == MESA_SHADER_VERTEX || ctx->stage == MESA_SHADER_TESS_EVAL));
+            args, shader, (stage == MESA_SHADER_VERTEX || stage == MESA_SHADER_TESS_EVAL));
       }
 
-      if (ctx->stage == MESA_SHADER_VERTEX && shader->selector->info.base.vs.blit_sgprs_amd) {
-         declare_vs_blit_inputs(ctx, shader->selector->info.base.vs.blit_sgprs_amd);
+      if (stage == MESA_SHADER_VERTEX && sel->info.base.vs.blit_sgprs_amd) {
+         declare_vs_blit_inputs(args, sel->info.base.vs.blit_sgprs_amd);
       } else {
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->vs_state_bits);
-
-         if (ctx->stage == MESA_SHADER_VERTEX) {
-            ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.base_vertex);
-            ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.draw_id);
-            ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.start_instance);
-         } else if (ctx->stage == MESA_SHADER_TESS_EVAL) {
-            ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->tcs_offchip_layout);
-            ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->tes_offchip_addr);
-            ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* unused */
+         ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->vs_state_bits);
+
+         if (stage == MESA_SHADER_VERTEX) {
+            ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.base_vertex);
+            ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.draw_id);
+            ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.start_instance);
+         } else if (stage == MESA_SHADER_TESS_EVAL) {
+            ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->tcs_offchip_layout);
+            ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->tes_offchip_addr);
+            ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* unused */
          } else {
             /* GS */
-            ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* unused */
-            ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* unused */
-            ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* unused */
+            ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* unused */
+            ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* unused */
+            ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* unused */
          }
 
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_CONST_DESC_PTR, &ctx->small_prim_cull_info);
-         if (ctx->screen->info.gfx_level >= GFX11)
-            ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->gs_attr_address);
+         ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_CONST_DESC_PTR, &args->small_prim_cull_info);
+         if (sel->screen->info.gfx_level >= GFX11)
+            ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->gs_attr_address);
          else
-            ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* unused */
+            ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* unused */
 
-         if (ctx->stage == MESA_SHADER_VERTEX)
-            declare_vb_descriptor_input_sgprs(ctx);
+         if (stage == MESA_SHADER_VERTEX)
+            declare_vb_descriptor_input_sgprs(args, shader);
       }
 
       /* VGPRs (first GS, then VS/TES) */
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.gs_vtx_offset[0]);
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.gs_vtx_offset[1]);
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.gs_prim_id);
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.gs_invocation_id);
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.gs_vtx_offset[2]);
-
-      if (ctx->stage == MESA_SHADER_VERTEX) {
-         declare_vs_input_vgprs(ctx, &num_prolog_vgprs);
-      } else if (ctx->stage == MESA_SHADER_TESS_EVAL) {
-         declare_tes_input_vgprs(ctx);
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.gs_vtx_offset[0]);
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.gs_vtx_offset[1]);
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.gs_prim_id);
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.gs_invocation_id);
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.gs_vtx_offset[2]);
+
+      if (stage == MESA_SHADER_VERTEX) {
+         declare_vs_input_vgprs(args, shader, &num_prolog_vgprs);
+      } else if (stage == MESA_SHADER_TESS_EVAL) {
+         declare_tes_input_vgprs(args);
       }
 
-      if (ctx->shader->key.ge.as_es &&
-          (ctx->stage == MESA_SHADER_VERTEX || ctx->stage == MESA_SHADER_TESS_EVAL)) {
+      if (shader->key.ge.as_es &&
+          (stage == MESA_SHADER_VERTEX || stage == MESA_SHADER_TESS_EVAL)) {
          /* ES return values are inputs to GS. */
          for (i = 0; i < 8 + GFX9_GS_NUM_USER_SGPR; i++)
-            ac_add_return(&ctx->args, AC_ARG_SGPR);
+            ac_add_return(&args->ac, AC_ARG_SGPR);
          for (i = 0; i < 5; i++)
-            ac_add_return(&ctx->args, AC_ARG_VGPR);
+            ac_add_return(&args->ac, AC_ARG_VGPR);
       }
       break;
 
    case MESA_SHADER_TESS_EVAL:
-      declare_global_desc_pointers(ctx);
-      declare_per_stage_desc_pointers(ctx, true);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->vs_state_bits);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->tcs_offchip_layout);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->tes_offchip_addr);
+      declare_global_desc_pointers(args);
+      declare_per_stage_desc_pointers(args, shader, true);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->vs_state_bits);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->tcs_offchip_layout);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->tes_offchip_addr);
 
       if (shader->key.ge.as_es) {
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.tess_offchip_offset);
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.es2gs_offset);
+         ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.tess_offchip_offset);
+         ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+         ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.es2gs_offset);
       } else {
-         declare_streamout_params(ctx);
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.tess_offchip_offset);
+         declare_streamout_params(args, shader);
+         ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.tess_offchip_offset);
       }
 
       /* VGPRs */
-      declare_tes_input_vgprs(ctx);
+      declare_tes_input_vgprs(args);
       break;
 
    case MESA_SHADER_GEOMETRY:
-      declare_global_desc_pointers(ctx);
-      declare_per_stage_desc_pointers(ctx, true);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.gs2vs_offset);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.gs_wave_id);
+      declare_global_desc_pointers(args);
+      declare_per_stage_desc_pointers(args, shader, true);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.gs2vs_offset);
+      ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.gs_wave_id);
 
       /* VGPRs */
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.gs_vtx_offset[0]);
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.gs_vtx_offset[1]);
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.gs_prim_id);
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.gs_vtx_offset[2]);
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.gs_vtx_offset[3]);
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.gs_vtx_offset[4]);
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.gs_vtx_offset[5]);
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.gs_invocation_id);
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.gs_vtx_offset[0]);
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.gs_vtx_offset[1]);
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.gs_prim_id);
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.gs_vtx_offset[2]);
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.gs_vtx_offset[3]);
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.gs_vtx_offset[4]);
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.gs_vtx_offset[5]);
+      ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.gs_invocation_id);
       break;
 
    case MESA_SHADER_FRAGMENT:
-      declare_global_desc_pointers(ctx);
-      declare_per_stage_desc_pointers(ctx, true);
-      si_add_arg_checked(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL, SI_PARAM_ALPHA_REF);
-      si_add_arg_checked(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.prim_mask,
+      declare_global_desc_pointers(args);
+      declare_per_stage_desc_pointers(args, shader, true);
+      si_add_arg_checked(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL, SI_PARAM_ALPHA_REF);
+      si_add_arg_checked(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.prim_mask,
                          SI_PARAM_PRIM_MASK);
 
-      si_add_arg_checked(&ctx->args, AC_ARG_VGPR, 2, AC_ARG_INT, &ctx->args.persp_sample,
+      si_add_arg_checked(&args->ac, AC_ARG_VGPR, 2, AC_ARG_INT, &args->ac.persp_sample,
                          SI_PARAM_PERSP_SAMPLE);
-      si_add_arg_checked(&ctx->args, AC_ARG_VGPR, 2, AC_ARG_INT, &ctx->args.persp_center,
+      si_add_arg_checked(&args->ac, AC_ARG_VGPR, 2, AC_ARG_INT, &args->ac.persp_center,
                          SI_PARAM_PERSP_CENTER);
-      si_add_arg_checked(&ctx->args, AC_ARG_VGPR, 2, AC_ARG_INT, &ctx->args.persp_centroid,
+      si_add_arg_checked(&args->ac, AC_ARG_VGPR, 2, AC_ARG_INT, &args->ac.persp_centroid,
                          SI_PARAM_PERSP_CENTROID);
-      si_add_arg_checked(&ctx->args, AC_ARG_VGPR, 3, AC_ARG_INT, NULL, SI_PARAM_PERSP_PULL_MODEL);
-      si_add_arg_checked(&ctx->args, AC_ARG_VGPR, 2, AC_ARG_INT, &ctx->args.linear_sample,
+      si_add_arg_checked(&args->ac, AC_ARG_VGPR, 3, AC_ARG_INT, NULL, SI_PARAM_PERSP_PULL_MODEL);
+      si_add_arg_checked(&args->ac, AC_ARG_VGPR, 2, AC_ARG_INT, &args->ac.linear_sample,
                          SI_PARAM_LINEAR_SAMPLE);
-      si_add_arg_checked(&ctx->args, AC_ARG_VGPR, 2, AC_ARG_INT, &ctx->args.linear_center,
+      si_add_arg_checked(&args->ac, AC_ARG_VGPR, 2, AC_ARG_INT, &args->ac.linear_center,
                          SI_PARAM_LINEAR_CENTER);
-      si_add_arg_checked(&ctx->args, AC_ARG_VGPR, 2, AC_ARG_INT, &ctx->args.linear_centroid,
+      si_add_arg_checked(&args->ac, AC_ARG_VGPR, 2, AC_ARG_INT, &args->ac.linear_centroid,
                          SI_PARAM_LINEAR_CENTROID);
-      si_add_arg_checked(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_FLOAT, NULL, SI_PARAM_LINE_STIPPLE_TEX);
-      si_add_arg_checked(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_FLOAT, &ctx->args.frag_pos[0],
+      si_add_arg_checked(&args->ac, AC_ARG_VGPR, 1, AC_ARG_FLOAT, NULL, SI_PARAM_LINE_STIPPLE_TEX);
+      si_add_arg_checked(&args->ac, AC_ARG_VGPR, 1, AC_ARG_FLOAT, &args->ac.frag_pos[0],
                          SI_PARAM_POS_X_FLOAT);
-      si_add_arg_checked(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_FLOAT, &ctx->args.frag_pos[1],
+      si_add_arg_checked(&args->ac, AC_ARG_VGPR, 1, AC_ARG_FLOAT, &args->ac.frag_pos[1],
                          SI_PARAM_POS_Y_FLOAT);
-      si_add_arg_checked(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_FLOAT, &ctx->args.frag_pos[2],
+      si_add_arg_checked(&args->ac, AC_ARG_VGPR, 1, AC_ARG_FLOAT, &args->ac.frag_pos[2],
                          SI_PARAM_POS_Z_FLOAT);
-      si_add_arg_checked(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_FLOAT, &ctx->args.frag_pos[3],
+      si_add_arg_checked(&args->ac, AC_ARG_VGPR, 1, AC_ARG_FLOAT, &args->ac.frag_pos[3],
                          SI_PARAM_POS_W_FLOAT);
-      shader->info.face_vgpr_index = ctx->args.num_vgprs_used;
-      si_add_arg_checked(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.front_face,
+      shader->info.face_vgpr_index = args->ac.num_vgprs_used;
+      si_add_arg_checked(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.front_face,
                          SI_PARAM_FRONT_FACE);
-      shader->info.ancillary_vgpr_index = ctx->args.num_vgprs_used;
-      si_add_arg_checked(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.ancillary,
+      shader->info.ancillary_vgpr_index = args->ac.num_vgprs_used;
+      si_add_arg_checked(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.ancillary,
                          SI_PARAM_ANCILLARY);
-      shader->info.sample_coverage_vgpr_index = ctx->args.num_vgprs_used;
-      si_add_arg_checked(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_FLOAT, &ctx->args.sample_coverage,
+      shader->info.sample_coverage_vgpr_index = args->ac.num_vgprs_used;
+      si_add_arg_checked(&args->ac, AC_ARG_VGPR, 1, AC_ARG_FLOAT, &args->ac.sample_coverage,
                          SI_PARAM_SAMPLE_COVERAGE);
-      si_add_arg_checked(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->pos_fixed_pt,
+      si_add_arg_checked(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->pos_fixed_pt,
                          SI_PARAM_POS_FIXED_PT);
 
       /* Color inputs from the prolog. */
@@ -706,7 +713,7 @@ void si_init_shader_args(struct si_shader_context *ctx)
          unsigned num_color_elements = util_bitcount(shader->selector->info.colors_read);
 
          for (i = 0; i < num_color_elements; i++)
-            ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_FLOAT, NULL);
+            ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_FLOAT, NULL);
 
          num_prolog_vgprs += num_color_elements;
       }
@@ -718,67 +725,67 @@ void si_init_shader_args(struct si_shader_context *ctx)
                     shader->selector->info.writes_samplemask + 1 /* SampleMaskIn */;
 
       for (i = 0; i < num_return_sgprs; i++)
-         ac_add_return(&ctx->args, AC_ARG_SGPR);
+         ac_add_return(&args->ac, AC_ARG_SGPR);
       for (; i < num_returns; i++)
-         ac_add_return(&ctx->args, AC_ARG_VGPR);
+         ac_add_return(&args->ac, AC_ARG_VGPR);
       break;
 
    case MESA_SHADER_COMPUTE:
-      declare_global_desc_pointers(ctx);
-      declare_per_stage_desc_pointers(ctx, true);
+      declare_global_desc_pointers(args);
+      declare_per_stage_desc_pointers(args, shader, true);
       if (shader->selector->info.uses_grid_size)
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, 3, AC_ARG_INT, &ctx->args.num_work_groups);
+         ac_add_arg(&args->ac, AC_ARG_SGPR, 3, AC_ARG_INT, &args->ac.num_work_groups);
       if (shader->selector->info.uses_variable_block_size)
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->block_size);
+         ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->block_size);
 
       unsigned cs_user_data_dwords =
          shader->selector->info.base.cs.user_data_components_amd;
       if (cs_user_data_dwords) {
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, cs_user_data_dwords, AC_ARG_INT, &ctx->cs_user_data);
+         ac_add_arg(&args->ac, AC_ARG_SGPR, cs_user_data_dwords, AC_ARG_INT, &args->cs_user_data);
       }
 
       /* Some descriptors can be in user SGPRs. */
       /* Shader buffers in user SGPRs. */
       for (unsigned i = 0; i < shader->selector->cs_num_shaderbufs_in_user_sgprs; i++) {
-         while (ctx->args.num_sgprs_used % 4 != 0)
-            ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+         while (args->ac.num_sgprs_used % 4 != 0)
+            ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
 
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, 4, AC_ARG_INT, &ctx->cs_shaderbuf[i]);
+         ac_add_arg(&args->ac, AC_ARG_SGPR, 4, AC_ARG_INT, &args->cs_shaderbuf[i]);
       }
       /* Images in user SGPRs. */
       for (unsigned i = 0; i < shader->selector->cs_num_images_in_user_sgprs; i++) {
          unsigned num_sgprs = BITSET_TEST(shader->selector->info.base.image_buffers, i) ? 4 : 8;
 
-         while (ctx->args.num_sgprs_used % num_sgprs != 0)
-            ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+         while (args->ac.num_sgprs_used % num_sgprs != 0)
+            ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
 
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, num_sgprs, AC_ARG_INT, &ctx->cs_image[i]);
+         ac_add_arg(&args->ac, AC_ARG_SGPR, num_sgprs, AC_ARG_INT, &args->cs_image[i]);
       }
 
       /* Hardware SGPRs. */
       for (i = 0; i < 3; i++) {
          if (shader->selector->info.uses_block_id[i]) {
-            ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.workgroup_ids[i]);
+            ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.workgroup_ids[i]);
          }
       }
       if (shader->selector->info.uses_subgroup_info)
-         ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.tg_size);
+         ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &args->ac.tg_size);
 
       /* Hardware VGPRs. */
       /* Thread IDs are packed in VGPR0, 10 bits per component or stored in 3 separate VGPRs */
-      if (ctx->screen->info.gfx_level >= GFX11 ||
-          (!ctx->screen->info.has_graphics && ctx->screen->info.family >= CHIP_MI200))
-         ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &ctx->args.local_invocation_ids);
+      if (sel->screen->info.gfx_level >= GFX11 ||
+          (!sel->screen->info.has_graphics && sel->screen->info.family >= CHIP_MI200))
+         ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.local_invocation_ids);
       else
-         ac_add_arg(&ctx->args, AC_ARG_VGPR, 3, AC_ARG_INT, &ctx->args.local_invocation_ids);
+         ac_add_arg(&args->ac, AC_ARG_VGPR, 3, AC_ARG_INT, &args->ac.local_invocation_ids);
       break;
    default:
       assert(0 && "unimplemented shader");
       return;
    }
 
-   shader->info.num_input_sgprs = ctx->args.num_sgprs_used;
-   shader->info.num_input_vgprs = ctx->args.num_vgprs_used;
+   shader->info.num_input_sgprs = args->ac.num_sgprs_used;
+   shader->info.num_input_vgprs = args->ac.num_vgprs_used;
 
    assert(shader->info.num_input_vgprs >= num_prolog_vgprs);
    shader->info.num_input_vgprs -= num_prolog_vgprs;
@@ -1964,6 +1971,10 @@ bool si_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *compi
                        struct si_shader *shader, struct util_debug_callback *debug)
 {
    struct si_shader_selector *sel = shader->selector;
+
+   struct si_shader_args args;
+   si_init_shader_args(shader, &args);
+
    bool free_nir;
    struct nir_shader *nir = si_get_nir_shader(shader, &free_nir, 0);
 
@@ -2021,7 +2032,7 @@ bool si_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *compi
     * with PS and NGG VS), but monolithic shaders should be compiled
     * by LLVM due to more complicated compilation.
     */
-   if (!si_llvm_compile_shader(sscreen, compiler, shader, &so, debug, nir, free_nir))
+   if (!si_llvm_compile_shader(sscreen, compiler, shader, &args, &so, debug, nir, free_nir))
       return false;
 
    shader->config.float_mode = float_mode;
@@ -2193,6 +2204,9 @@ si_get_shader_part(struct si_screen *sscreen, struct si_shader_part **list,
    ctx.shader = &shader;
    ctx.stage = stage;
 
+   struct si_shader_args args;
+   ctx.args = &args;
+
    build(&ctx, key);
 
    /* Compile. */
diff --git a/src/gallium/drivers/radeonsi/si_shader.h b/src/gallium/drivers/radeonsi/si_shader.h
index 83cad1a8e21e..53c589f2e8b1 100644
--- a/src/gallium/drivers/radeonsi/si_shader.h
+++ b/src/gallium/drivers/radeonsi/si_shader.h
@@ -292,7 +292,7 @@ enum
 } while (0)
 
 /* This is called during shader compilation and returns LLVMValueRef. */
-#define GET_FIELD(ctx, field) si_unpack_param((ctx), (ctx)->vs_state_bits, field##__SHIFT, \
+#define GET_FIELD(ctx, field) si_unpack_param((ctx), (ctx)->args->vs_state_bits, field##__SHIFT, \
                                              util_bitcount(field##__MASK))
 
 enum
diff --git a/src/gallium/drivers/radeonsi/si_shader_internal.h b/src/gallium/drivers/radeonsi/si_shader_internal.h
index 1a9a49faf279..83472047bac8 100644
--- a/src/gallium/drivers/radeonsi/si_shader_internal.h
+++ b/src/gallium/drivers/radeonsi/si_shader_internal.h
@@ -36,28 +36,8 @@ struct si_shader_output_values {
    ubyte semantic;
 };
 
-struct si_shader_context {
-   struct ac_llvm_context ac;
-   struct si_shader *shader;
-   struct si_screen *screen;
-   struct pipe_stream_output_info so;
-
-   gl_shader_stage stage;
-
-   /* For clamping the non-constant index in resource indexing: */
-   unsigned num_const_buffers;
-   unsigned num_shader_buffers;
-   unsigned num_images;
-   unsigned num_samplers;
-
-   struct ac_shader_args args;
-   struct ac_shader_abi abi;
-
-   LLVMBasicBlockRef merged_wrap_if_entry_block;
-   int merged_wrap_if_label;
-
-   struct ac_llvm_pointer main_fn;
-   LLVMTypeRef return_type;
+struct si_shader_args {
+   struct ac_shader_args ac;
 
    struct ac_arg const_and_shader_buffers;
    struct ac_arg samplers_and_images;
@@ -132,6 +112,30 @@ struct si_shader_context {
    struct ac_arg cs_user_data;
    struct ac_arg cs_shaderbuf[3];
    struct ac_arg cs_image[3];
+};
+
+struct si_shader_context {
+   struct ac_llvm_context ac;
+   struct si_shader *shader;
+   struct si_screen *screen;
+   struct pipe_stream_output_info so;
+
+   gl_shader_stage stage;
+
+   /* For clamping the non-constant index in resource indexing: */
+   unsigned num_const_buffers;
+   unsigned num_shader_buffers;
+   unsigned num_images;
+   unsigned num_samplers;
+
+   struct si_shader_args *args;
+   struct ac_shader_abi abi;
+
+   LLVMBasicBlockRef merged_wrap_if_entry_block;
+   int merged_wrap_if_label;
+
+   struct ac_llvm_pointer main_fn;
+   LLVMTypeRef return_type;
 
    struct ac_llvm_compiler *compiler;
 
@@ -158,7 +162,7 @@ bool si_is_multi_part_shader(struct si_shader *shader);
 bool si_is_merged_shader(struct si_shader *shader);
 void si_add_arg_checked(struct ac_shader_args *args, enum ac_arg_regfile file, unsigned registers,
                         enum ac_arg_type type, struct ac_arg *arg, unsigned idx);
-void si_init_shader_args(struct si_shader_context *ctx);
+void si_init_shader_args(struct si_shader *shader, struct si_shader_args *args);
 unsigned si_get_max_workgroup_size(const struct si_shader *shader);
 bool si_vs_needs_prolog(const struct si_shader_selector *sel,
                         const struct si_vs_prolog_bits *prolog_key);
@@ -219,7 +223,8 @@ void si_build_wrapper_function(struct si_shader_context *ctx, struct ac_llvm_poi
 bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shader,
                            struct nir_shader *nir, bool free_nir);
 bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *compiler,
-                            struct si_shader *shader, const struct pipe_stream_output_info *so,
+                            struct si_shader *shader, struct si_shader_args *args,
+                            const struct pipe_stream_output_info *so,
                             struct util_debug_callback *debug, struct nir_shader *nir,
                             bool free_nir);
 
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index b255310198fc..a7316ad54283 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -181,7 +181,7 @@ void si_llvm_create_func(struct si_shader_context *ctx, const char *name, LLVMTy
 
    /* Setup the function */
    ctx->return_type = ret_type;
-   ctx->main_fn = ac_build_main(&ctx->args, &ctx->ac, call_conv, name, ret_type, ctx->ac.module);
+   ctx->main_fn = ac_build_main(&ctx->args->ac, &ctx->ac, call_conv, name, ret_type, ctx->ac.module);
    ctx->return_value = LLVMGetUndef(ctx->return_type);
 
    if (ctx->screen->info.address32_hi) {
@@ -203,14 +203,12 @@ void si_llvm_create_main_func(struct si_shader_context *ctx)
    LLVMTypeRef returns[AC_MAX_ARGS];
    unsigned i;
 
-   si_init_shader_args(ctx);
-
-   for (i = 0; i < ctx->args.num_sgprs_returned; i++)
+   for (i = 0; i < ctx->args->ac.num_sgprs_returned; i++)
       returns[i] = ctx->ac.i32; /* SGPR */
-   for (; i < ctx->args.return_count; i++)
+   for (; i < ctx->args->ac.return_count; i++)
       returns[i] = ctx->ac.f32; /* VGPR */
 
-   si_llvm_create_func(ctx, "main", returns, ctx->args.return_count,
+   si_llvm_create_func(ctx, "main", returns, ctx->args->ac.return_count,
                        si_get_max_workgroup_size(shader));
 
    /* Reserve register locations for VGPR inputs the PS prolog may need. */
@@ -244,11 +242,11 @@ void si_llvm_create_main_func(struct si_shader_context *ctx)
     * API shader they appear as normal arguments.
     */
    if (ctx->stage == MESA_SHADER_VERTEX) {
-      ctx->abi.vertex_id = ac_get_arg(&ctx->ac, ctx->args.vertex_id);
-      ctx->abi.instance_id = ac_get_arg(&ctx->ac, ctx->args.instance_id);
+      ctx->abi.vertex_id = ac_get_arg(&ctx->ac, ctx->args->ac.vertex_id);
+      ctx->abi.instance_id = ac_get_arg(&ctx->ac, ctx->args->ac.instance_id);
    } else if (ctx->stage == MESA_SHADER_FRAGMENT) {
-      ctx->abi.persp_centroid = ac_get_arg(&ctx->ac, ctx->args.persp_centroid);
-      ctx->abi.linear_centroid = ac_get_arg(&ctx->ac, ctx->args.linear_centroid);
+      ctx->abi.persp_centroid = ac_get_arg(&ctx->ac, ctx->args->ac.persp_centroid);
+      ctx->abi.linear_centroid = ac_get_arg(&ctx->ac, ctx->args->ac.linear_centroid);
    }
 }
 
@@ -387,15 +385,15 @@ LLVMValueRef si_get_primitive_id(struct si_shader_context *ctx, unsigned swizzle
 
    switch (ctx->stage) {
    case MESA_SHADER_VERTEX:
-      return ac_get_arg(&ctx->ac, ctx->args.vs_prim_id);
+      return ac_get_arg(&ctx->ac, ctx->args->ac.vs_prim_id);
    case MESA_SHADER_TESS_CTRL:
-      return ac_get_arg(&ctx->ac, ctx->args.tcs_patch_id);
+      return ac_get_arg(&ctx->ac, ctx->args->ac.tcs_patch_id);
    case MESA_SHADER_TESS_EVAL:
       return ctx->abi.tes_patch_id_replaced ?
          ctx->abi.tes_patch_id_replaced :
-         ac_get_arg(&ctx->ac, ctx->args.tes_patch_id);
+         ac_get_arg(&ctx->ac, ctx->args->ac.tes_patch_id);
    case MESA_SHADER_GEOMETRY:
-      return ac_get_arg(&ctx->ac, ctx->args.gs_prim_id);
+      return ac_get_arg(&ctx->ac, ctx->args->ac.gs_prim_id);
    default:
       assert(0);
       return ctx->ac.i32_0;
@@ -445,7 +443,7 @@ void si_build_wrapper_function(struct si_shader_context *ctx, struct ac_llvm_poi
    unsigned num_sgprs, num_vgprs;
    unsigned gprs;
 
-   memset(&ctx->args, 0, sizeof(ctx->args));
+   memset(ctx->args, 0, sizeof(*ctx->args));
 
    for (unsigned i = 0; i < num_parts; ++i) {
       ac_add_function_attr(ctx->ac.context, parts[i].value, -1, AC_FUNC_ATTR_ALWAYSINLINE);
@@ -476,13 +474,13 @@ void si_build_wrapper_function(struct si_shader_context *ctx, struct ac_llvm_poi
 
    gprs = 0;
    while (gprs < num_sgprs + num_vgprs) {
-      LLVMValueRef param = LLVMGetParam(parts[main_part].value, ctx->args.arg_count);
+      LLVMValueRef param = LLVMGetParam(parts[main_part].value, ctx->args->ac.arg_count);
       LLVMTypeRef type = LLVMTypeOf(param);
       unsigned size = ac_get_type_size(type) / 4;
-      enum ac_arg_type arg_type = main_arg_types[ctx->args.arg_count];
+      enum ac_arg_type arg_type = main_arg_types[ctx->args->ac.arg_count];
       assert(arg_type != AC_ARG_INVALID);
 
-      ac_add_arg(&ctx->args, gprs < num_sgprs ? AC_ARG_SGPR : AC_ARG_VGPR, size, arg_type, NULL);
+      ac_add_arg(&ctx->args->ac, gprs < num_sgprs ? AC_ARG_SGPR : AC_ARG_VGPR, size, arg_type, NULL);
 
       assert(ac_is_sgpr_param(param) == (gprs < num_sgprs));
       assert(gprs + size <= num_sgprs + num_vgprs &&
@@ -522,10 +520,10 @@ void si_build_wrapper_function(struct si_shader_context *ctx, struct ac_llvm_poi
    num_out = 0;
    num_out_sgpr = 0;
 
-   for (unsigned i = 0; i < ctx->args.arg_count; ++i) {
+   for (unsigned i = 0; i < ctx->args->ac.arg_count; ++i) {
       LLVMValueRef param = LLVMGetParam(ctx->main_fn.value, i);
       LLVMTypeRef param_type = LLVMTypeOf(param);
-      LLVMTypeRef out_type = ctx->args.args[i].file == AC_ARG_SGPR ? ctx->ac.i32 : ctx->ac.f32;
+      LLVMTypeRef out_type = ctx->args->ac.args[i].file == AC_ARG_SGPR ? ctx->ac.i32 : ctx->ac.f32;
       unsigned size = ac_get_type_size(param_type) / 4;
 
       if (size == 1) {
@@ -553,7 +551,7 @@ void si_build_wrapper_function(struct si_shader_context *ctx, struct ac_llvm_poi
                LLVMBuildExtractElement(builder, param, LLVMConstInt(ctx->ac.i32, j, 0), "");
       }
 
-      if (ctx->args.args[i].file == AC_ARG_SGPR)
+      if (ctx->args->ac.args[i].file == AC_ARG_SGPR)
          num_out_sgpr = num_out;
    }
 
@@ -738,12 +736,13 @@ static LLVMValueRef si_llvm_build_attr_ring_desc(struct si_shader_context *ctx)
 
    LLVMValueRef attr_address;
    if (ctx->stage == MESA_SHADER_VERTEX && shader->selector->info.base.vs.blit_sgprs_amd) {
-      struct ac_llvm_pointer ring_ptr = ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->internal_bindings);
+      struct ac_llvm_pointer ring_ptr =
+         ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->internal_bindings);
       ring_ptr.pointee_type = ctx->ac.i32;
       attr_address = ac_build_load_to_sgpr(&ctx->ac, ring_ptr,
                                            LLVMConstInt(ctx->ac.i32, SI_GS_ATTRIBUTE_RING * 4, 0));
    } else {
-      attr_address = ac_get_arg(&ctx->ac, ctx->gs_attr_address);
+      attr_address = ac_get_arg(&ctx->ac, ctx->args->gs_attr_address);
    }
 
    unsigned stride = 16 * shader->info.nr_param_exports;
@@ -770,7 +769,7 @@ static LLVMValueRef si_llvm_load_intrinsic(struct ac_shader_abi *abi, nir_intrin
 
    switch (op) {
    case nir_intrinsic_load_first_vertex:
-      return ac_get_arg(&ctx->ac, ctx->args.base_vertex);
+      return ac_get_arg(&ctx->ac, ctx->args->ac.base_vertex);
 
    case nir_intrinsic_load_base_vertex: {
       /* For non-indexed draws, the base vertex set by the driver
@@ -779,7 +778,7 @@ static LLVMValueRef si_llvm_load_intrinsic(struct ac_shader_abi *abi, nir_intrin
        */
       LLVMValueRef indexed = GET_FIELD(ctx, VS_STATE_INDEXED);
       indexed = LLVMBuildTrunc(ctx->ac.builder, indexed, ctx->ac.i1, "");
-      return LLVMBuildSelect(ctx->ac.builder, indexed, ac_get_arg(&ctx->ac, ctx->args.base_vertex),
+      return LLVMBuildSelect(ctx->ac.builder, indexed, ac_get_arg(&ctx->ac, ctx->args->ac.base_vertex),
                              ctx->ac.i32_0, "");
    }
 
@@ -787,9 +786,9 @@ static LLVMValueRef si_llvm_load_intrinsic(struct ac_shader_abi *abi, nir_intrin
       assert(ctx->shader->selector->info.base.workgroup_size_variable &&
              ctx->shader->selector->info.uses_variable_block_size);
       LLVMValueRef chan[3] = {
-         si_unpack_param(ctx, ctx->block_size, 0, 10),
-         si_unpack_param(ctx, ctx->block_size, 10, 10),
-         si_unpack_param(ctx, ctx->block_size, 20, 10),
+         si_unpack_param(ctx, ctx->args->block_size, 0, 10),
+         si_unpack_param(ctx, ctx->args->block_size, 10, 10),
+         si_unpack_param(ctx, ctx->args->block_size, 20, 10),
       };
       return ac_build_gather_values(&ctx->ac, chan, 3);
    }
@@ -797,7 +796,10 @@ static LLVMValueRef si_llvm_load_intrinsic(struct ac_shader_abi *abi, nir_intrin
    case nir_intrinsic_load_tess_level_outer_default:
    case nir_intrinsic_load_tess_level_inner_default: {
       LLVMValueRef slot = LLVMConstInt(ctx->ac.i32, SI_HS_CONST_DEFAULT_TESS_LEVELS, 0);
-      LLVMValueRef buf = ac_build_load_to_sgpr(&ctx->ac, ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->internal_bindings), slot);
+      LLVMValueRef buf =
+         ac_build_load_to_sgpr(&ctx->ac,
+                               ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->internal_bindings),
+                               slot);
       int offset = op == nir_intrinsic_load_tess_level_inner_default ? 4 : 0;
       LLVMValueRef val[4];
 
@@ -808,14 +810,14 @@ static LLVMValueRef si_llvm_load_intrinsic(struct ac_shader_abi *abi, nir_intrin
 
    case nir_intrinsic_load_patch_vertices_in:
       if (ctx->stage == MESA_SHADER_TESS_CTRL)
-         return si_unpack_param(ctx, ctx->tcs_out_lds_layout, 13, 6);
+         return si_unpack_param(ctx, ctx->args->tcs_out_lds_layout, 13, 6);
       else if (ctx->stage == MESA_SHADER_TESS_EVAL)
          return si_get_num_tcs_out_vertices(ctx);
       else
          return NULL;
 
    case nir_intrinsic_load_sample_mask_in:
-      return ac_to_integer(&ctx->ac, ac_get_arg(&ctx->ac, ctx->args.sample_coverage));
+      return ac_to_integer(&ctx->ac, ac_get_arg(&ctx->ac, ctx->args->ac.sample_coverage));
 
    case nir_intrinsic_load_lshs_vertex_stride_amd:
       return LLVMBuildShl(ctx->ac.builder, si_get_tcs_in_vertex_dw_stride(ctx),
@@ -823,17 +825,17 @@ static LLVMValueRef si_llvm_load_intrinsic(struct ac_shader_abi *abi, nir_intrin
 
    case nir_intrinsic_load_tcs_num_patches_amd:
       return LLVMBuildAdd(ctx->ac.builder,
-                          si_unpack_param(ctx, ctx->tcs_offchip_layout, 0, 6),
+                          si_unpack_param(ctx, ctx->args->tcs_offchip_layout, 0, 6),
                           ctx->ac.i32_1, "");
 
    case nir_intrinsic_load_hs_out_patch_data_offset_amd:
-      return si_unpack_param(ctx, ctx->tcs_offchip_layout, 11, 21);
+      return si_unpack_param(ctx, ctx->args->tcs_offchip_layout, 11, 21);
 
    case nir_intrinsic_load_ring_tess_offchip_amd:
       return ctx->tess_offchip_ring;
 
    case nir_intrinsic_load_ring_tess_offchip_offset_amd:
-      return ac_get_arg(&ctx->ac, ctx->args.tess_offchip_offset);
+      return ac_get_arg(&ctx->ac, ctx->args->ac.tess_offchip_offset);
 
    case nir_intrinsic_load_tess_rel_patch_id_amd:
       return si_get_rel_patch_id(ctx);
@@ -842,17 +844,17 @@ static LLVMValueRef si_llvm_load_intrinsic(struct ac_shader_abi *abi, nir_intrin
       return ctx->esgs_ring;
 
    case nir_intrinsic_load_ring_es2gs_offset_amd:
-      return ac_get_arg(&ctx->ac, ctx->args.es2gs_offset);
+      return ac_get_arg(&ctx->ac, ctx->args->ac.es2gs_offset);
 
    case nir_intrinsic_load_clip_half_line_width_amd: {
-      LLVMValueRef ptr = ac_get_arg(&ctx->ac, ctx->small_prim_cull_info);
+      LLVMValueRef ptr = ac_get_arg(&ctx->ac, ctx->args->small_prim_cull_info);
       return ac_build_load_to_sgpr(&ctx->ac,
          (struct ac_llvm_pointer) { .t = ctx->ac.v2f32, .v = ptr }, LLVMConstInt(ctx->ac.i32, 4, 0));
    }
 
    case nir_intrinsic_load_viewport_xy_scale_and_offset: {
       bool prim_is_lines = ctx->shader->key.ge.opt.ngg_culling & SI_NGG_CULL_LINES;
-      struct ac_llvm_pointer ptr = ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->small_prim_cull_info);
+      struct ac_llvm_pointer ptr = ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->small_prim_cull_info);
       LLVMValueRef terms =
          ac_build_load_to_sgpr(&ctx->ac, ptr, prim_is_lines ? ctx->ac.i32_1 : ctx->ac.i32_0);
       return LLVMBuildBitCast(ctx->ac.builder, terms, ctx->ac.v4f32, "");
@@ -936,7 +938,7 @@ static LLVMValueRef si_llvm_load_intrinsic(struct ac_shader_abi *abi, nir_intrin
 static LLVMValueRef si_llvm_load_user_clip_plane(struct ac_shader_abi *abi, unsigned ucp_id)
 {
    struct si_shader_context *ctx = si_shader_context_from_abi(abi);
-   struct ac_llvm_pointer ptr = ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->internal_bindings);
+   struct ac_llvm_pointer ptr = ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->internal_bindings);
    LLVMValueRef constbuf_index = LLVMConstInt(ctx->ac.i32, SI_VS_CONST_CLIP_PLANES, 0);
    LLVMValueRef const_resource = ac_build_load_to_sgpr(&ctx->ac, ptr, constbuf_index);
    LLVMValueRef addr = LLVMConstInt(ctx->ac.i32, ucp_id * 16, 0);
@@ -947,7 +949,7 @@ static LLVMValueRef si_llvm_load_user_clip_plane(struct ac_shader_abi *abi, unsi
 static LLVMValueRef si_llvm_load_streamout_buffer(struct ac_shader_abi *abi, unsigned buffer)
 {
    struct si_shader_context *ctx = si_shader_context_from_abi(abi);
-   struct ac_llvm_pointer buf_ptr = ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->internal_bindings);
+   struct ac_llvm_pointer buf_ptr = ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->internal_bindings);
 
    return ac_build_load_to_sgpr(
       &ctx->ac, buf_ptr, LLVMConstInt(ctx->ac.i32, SI_VS_STREAMOUT_BUF0 + buffer, false));
@@ -988,7 +990,7 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
       /* preload instance_divisor_constbuf to be used for input load after culling */
       if (ctx->shader->key.ge.opt.ngg_culling &&
           ctx->shader->key.ge.part.vs.prolog.instance_divisor_is_fetched) {
-         struct ac_llvm_pointer buf = ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->internal_bindings);
+         struct ac_llvm_pointer buf = ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->internal_bindings);
          ctx->instance_divisor_constbuf =
             ac_build_load_to_sgpr(
                &ctx->ac, buf, LLVMConstInt(ctx->ac.i32, SI_VS_CONST_INSTANCE_DIVISORS, 0));
@@ -1070,7 +1072,7 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
 
    case MESA_SHADER_COMPUTE:
       if (nir->info.cs.user_data_components_amd) {
-         ctx->abi.user_data = ac_get_arg(&ctx->ac, ctx->cs_user_data);
+         ctx->abi.user_data = ac_get_arg(&ctx->ac, ctx->args->cs_user_data);
          ctx->abi.user_data = ac_build_expand_to_vec4(&ctx->ac, ctx->abi.user_data,
                                                       nir->info.cs.user_data_components_amd);
       }
@@ -1224,7 +1226,7 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
       }
    }
 
-   if (!ac_nir_translate(&ctx->ac, &ctx->abi, &ctx->args, nir))
+   if (!ac_nir_translate(&ctx->ac, &ctx->abi, &ctx->args->ac, nir))
       return false;
 
    switch (sel->stage) {
@@ -1283,7 +1285,8 @@ static bool si_should_optimize_less(struct ac_llvm_compiler *compiler,
 }
 
 bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *compiler,
-                            struct si_shader *shader, const struct pipe_stream_output_info *so,
+                            struct si_shader *shader, struct si_shader_args *args,
+                            const struct pipe_stream_output_info *so,
                             struct util_debug_callback *debug, struct nir_shader *nir,
                             bool free_nir)
 {
@@ -1292,6 +1295,7 @@ bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *
 
    si_llvm_context_init(&ctx, sscreen, compiler, shader->wave_size);
    ctx.so = *so;
+   ctx.args = args;
 
    if (!si_llvm_translate_nir(&ctx, shader, nir, free_nir)) {
       si_llvm_dispose(&ctx);
@@ -1305,9 +1309,9 @@ bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *
 
        /* Preserve main arguments. */
       enum ac_arg_type main_arg_types[AC_MAX_ARGS];
-      for (int i = 0; i < ctx.args.arg_count; i++)
-         main_arg_types[i] = ctx.args.args[i].type;
-      main_arg_types[MIN2(AC_MAX_ARGS - 1, ctx.args.arg_count)] = AC_ARG_INVALID;
+      for (int i = 0; i < ctx.args->ac.arg_count; i++)
+         main_arg_types[i] = ctx.args->ac.args[i].type;
+      main_arg_types[MIN2(AC_MAX_ARGS - 1, ctx.args->ac.arg_count)] = AC_ARG_INVALID;
 
       union si_shader_part_key prolog_key;
       si_get_vs_prolog_key(&sel->info, shader->info.num_input_sgprs,
@@ -1345,6 +1349,7 @@ bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *
          shader_ls.key.ge.opt.inline_uniforms = false; /* only TCS can inline uniforms */
          shader_ls.is_monolithic = true;
 
+         si_init_shader_args(&shader_ls, ctx.args);
          nir = si_get_nir_shader(&shader_ls, &free_nir, sel->info.tcs_vgpr_only_inputs);
          si_update_shader_binary_info(shader, nir);
 
@@ -1355,9 +1360,9 @@ bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *
          shader->info.uses_instanceid |= ls->info.uses_instanceid;
          parts[1] = ctx.main_fn;
 
-         for (int i = 0; i < ctx.args.arg_count; i++)
-            main_arg_types[i] = ctx.args.args[i].type;
-         main_arg_types[MIN2(AC_MAX_ARGS - 1, ctx.args.arg_count)] = AC_ARG_INVALID;
+         for (int i = 0; i < ctx.args->ac.arg_count; i++)
+            main_arg_types[i] = ctx.args->ac.args[i].type;
+         main_arg_types[MIN2(AC_MAX_ARGS - 1, ctx.args->ac.arg_count)] = AC_ARG_INVALID;
 
          /* LS prolog */
          if (vs_needs_prolog) {
@@ -1383,9 +1388,9 @@ bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *
 
          parts[0] = ctx.main_fn;
 
-         for (int i = 0; i < ctx.args.arg_count; i++)
-            main_arg_types[i] = ctx.args.args[i].type;
-         main_arg_types[MIN2(AC_MAX_ARGS - 1, ctx.args.arg_count)] = AC_ARG_INVALID;
+         for (int i = 0; i < ctx.args->ac.arg_count; i++)
+            main_arg_types[i] = ctx.args->ac.args[i].type;
+         main_arg_types[MIN2(AC_MAX_ARGS - 1, ctx.args->ac.arg_count)] = AC_ARG_INVALID;
 
          memset(&epilog_key, 0, sizeof(epilog_key));
          epilog_key.tcs_epilog.states = shader->key.ge.part.tcs.epilog;
@@ -1416,6 +1421,7 @@ bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *
          shader_es.key.ge.opt.kill_outputs = 0;
          shader_es.is_monolithic = true;
 
+         si_init_shader_args(&shader_es, ctx.args);
          nir = si_get_nir_shader(&shader_es, &free_nir, 0);
          si_update_shader_binary_info(shader, nir);
 
@@ -1427,9 +1433,9 @@ bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *
          es_main = ctx.main_fn;
 
          /* Preserve main (= es_main) arguments. */
-         for (int i = 0; i < ctx.args.arg_count; i++)
-            main_arg_types[i] = ctx.args.args[i].type;
-         main_arg_types[MIN2(AC_MAX_ARGS - 1, ctx.args.arg_count)] = AC_ARG_INVALID;
+         for (int i = 0; i < ctx.args->ac.arg_count; i++)
+            main_arg_types[i] = ctx.args->ac.args[i].type;
+         main_arg_types[MIN2(AC_MAX_ARGS - 1, ctx.args->ac.arg_count)] = AC_ARG_INVALID;
 
          /* ES prolog */
          if (es->stage == MESA_SHADER_VERTEX &&
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c b/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c
index 440a6d25dbb9..f3c4cb0aff6c 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c
@@ -33,14 +33,14 @@ LLVMValueRef si_is_es_thread(struct si_shader_context *ctx)
 {
    /* Return true if the current thread should execute an ES thread. */
    return LLVMBuildICmp(ctx->ac.builder, LLVMIntULT, ac_get_thread_id(&ctx->ac),
-                        si_unpack_param(ctx, ctx->args.merged_wave_info, 0, 8), "");
+                        si_unpack_param(ctx, ctx->args->ac.merged_wave_info, 0, 8), "");
 }
 
 LLVMValueRef si_is_gs_thread(struct si_shader_context *ctx)
 {
    /* Return true if the current thread should execute a GS thread. */
    return LLVMBuildICmp(ctx->ac.builder, LLVMIntULT, ac_get_thread_id(&ctx->ac),
-                        si_unpack_param(ctx, ctx->args.merged_wave_info, 8, 8), "");
+                        si_unpack_param(ctx, ctx->args->ac.merged_wave_info, 8, 8), "");
 }
 
 /* Pass GS inputs from ES to GS on GFX9. */
@@ -51,34 +51,34 @@ static void si_set_es_return_value_for_gs(struct si_shader_context *ctx)
 
    LLVMValueRef ret = ctx->return_value;
 
-   ret = si_insert_input_ptr(ctx, ret, ctx->other_const_and_shader_buffers, 0);
-   ret = si_insert_input_ptr(ctx, ret, ctx->other_samplers_and_images, 1);
+   ret = si_insert_input_ptr(ctx, ret, ctx->args->other_const_and_shader_buffers, 0);
+   ret = si_insert_input_ptr(ctx, ret, ctx->args->other_samplers_and_images, 1);
    if (ctx->shader->key.ge.as_ngg)
-      ret = si_insert_input_ptr(ctx, ret, ctx->args.gs_tg_info, 2);
+      ret = si_insert_input_ptr(ctx, ret, ctx->args->ac.gs_tg_info, 2);
    else
-      ret = si_insert_input_ret(ctx, ret, ctx->args.gs2vs_offset, 2);
-   ret = si_insert_input_ret(ctx, ret, ctx->args.merged_wave_info, 3);
+      ret = si_insert_input_ret(ctx, ret, ctx->args->ac.gs2vs_offset, 2);
+   ret = si_insert_input_ret(ctx, ret, ctx->args->ac.merged_wave_info, 3);
    if (ctx->screen->info.gfx_level >= GFX11)
-      ret = si_insert_input_ret(ctx, ret, ctx->args.gs_attr_offset, 5);
+      ret = si_insert_input_ret(ctx, ret, ctx->args->ac.gs_attr_offset, 5);
    else
-      ret = si_insert_input_ret(ctx, ret, ctx->args.scratch_offset, 5);
-   ret = si_insert_input_ptr(ctx, ret, ctx->internal_bindings, 8 + SI_SGPR_INTERNAL_BINDINGS);
-   ret = si_insert_input_ptr(ctx, ret, ctx->bindless_samplers_and_images,
+      ret = si_insert_input_ret(ctx, ret, ctx->args->ac.scratch_offset, 5);
+   ret = si_insert_input_ptr(ctx, ret, ctx->args->internal_bindings, 8 + SI_SGPR_INTERNAL_BINDINGS);
+   ret = si_insert_input_ptr(ctx, ret, ctx->args->bindless_samplers_and_images,
                              8 + SI_SGPR_BINDLESS_SAMPLERS_AND_IMAGES);
    if (ctx->screen->use_ngg) {
-      ret = si_insert_input_ptr(ctx, ret, ctx->vs_state_bits, 8 + SI_SGPR_VS_STATE_BITS);
-      ret = si_insert_input_ptr(ctx, ret, ctx->small_prim_cull_info, 8 + GFX9_SGPR_SMALL_PRIM_CULL_INFO);
+      ret = si_insert_input_ptr(ctx, ret, ctx->args->vs_state_bits, 8 + SI_SGPR_VS_STATE_BITS);
+      ret = si_insert_input_ptr(ctx, ret, ctx->args->small_prim_cull_info, 8 + GFX9_SGPR_SMALL_PRIM_CULL_INFO);
       if (ctx->screen->info.gfx_level >= GFX11)
-         ret = si_insert_input_ptr(ctx, ret, ctx->gs_attr_address, 8 + GFX9_SGPR_ATTRIBUTE_RING_ADDR);
+         ret = si_insert_input_ptr(ctx, ret, ctx->args->gs_attr_address, 8 + GFX9_SGPR_ATTRIBUTE_RING_ADDR);
    }
 
    unsigned vgpr = 8 + GFX9_GS_NUM_USER_SGPR;
 
-   ret = si_insert_input_ret_float(ctx, ret, ctx->args.gs_vtx_offset[0], vgpr++);
-   ret = si_insert_input_ret_float(ctx, ret, ctx->args.gs_vtx_offset[1], vgpr++);
-   ret = si_insert_input_ret_float(ctx, ret, ctx->args.gs_prim_id, vgpr++);
-   ret = si_insert_input_ret_float(ctx, ret, ctx->args.gs_invocation_id, vgpr++);
-   ret = si_insert_input_ret_float(ctx, ret, ctx->args.gs_vtx_offset[2], vgpr++);
+   ret = si_insert_input_ret_float(ctx, ret, ctx->args->ac.gs_vtx_offset[0], vgpr++);
+   ret = si_insert_input_ret_float(ctx, ret, ctx->args->ac.gs_vtx_offset[1], vgpr++);
+   ret = si_insert_input_ret_float(ctx, ret, ctx->args->ac.gs_prim_id, vgpr++);
+   ret = si_insert_input_ret_float(ctx, ret, ctx->args->ac.gs_invocation_id, vgpr++);
+   ret = si_insert_input_ret_float(ctx, ret, ctx->args->ac.gs_vtx_offset[2], vgpr++);
    ctx->return_value = ret;
 }
 
@@ -91,14 +91,15 @@ void si_llvm_es_build_end(struct si_shader_context *ctx)
 static LLVMValueRef si_get_gs_wave_id(struct si_shader_context *ctx)
 {
    if (ctx->screen->info.gfx_level >= GFX9)
-      return si_unpack_param(ctx, ctx->args.merged_wave_info, 16, 8);
+      return si_unpack_param(ctx, ctx->args->ac.merged_wave_info, 16, 8);
    else
-      return ac_get_arg(&ctx->ac, ctx->args.gs_wave_id);
+      return ac_get_arg(&ctx->ac, ctx->args->ac.gs_wave_id);
 }
 
 static LLVMValueRef ngg_get_emulated_counters_buf(struct si_shader_context *ctx)
 {
-   return ac_build_load_to_sgpr(&ctx->ac, ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->internal_bindings),
+   return ac_build_load_to_sgpr(&ctx->ac,
+                                ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->internal_bindings),
                                 LLVMConstInt(ctx->ac.i32, SI_GS_QUERY_EMULATED_COUNTERS_BUF, false));
 }
 
@@ -171,7 +172,7 @@ static void si_llvm_emit_vertex(struct ac_shader_abi *abi, unsigned stream,
 
    struct si_shader_info *info = &ctx->shader->selector->info;
    struct si_shader *shader = ctx->shader;
-   LLVMValueRef soffset = ac_get_arg(&ctx->ac, ctx->args.gs2vs_offset);
+   LLVMValueRef soffset = ac_get_arg(&ctx->ac, ctx->args->ac.gs2vs_offset);
 
    unsigned offset = 0;
    for (unsigned i = 0; i < info->num_outputs; i++) {
@@ -225,7 +226,7 @@ void si_preload_esgs_ring(struct si_shader_context *ctx)
       LLVMValueRef offset = LLVMConstInt(ctx->ac.i32, SI_RING_ESGS, 0);
 
       ctx->esgs_ring = ac_build_load_to_sgpr(&ctx->ac,
-         ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->internal_bindings), offset);
+         ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->internal_bindings), offset);
 
       if (ctx->stage != MESA_SHADER_GEOMETRY) {
          LLVMValueRef desc1 = LLVMBuildExtractElement(builder, ctx->esgs_ring, ctx->ac.i32_1, "");
@@ -267,7 +268,7 @@ void si_preload_gs_rings(struct si_shader_context *ctx)
    LLVMBuilderRef builder = ctx->ac.builder;
    LLVMValueRef offset = LLVMConstInt(ctx->ac.i32, SI_RING_GSVS, 0);
    LLVMValueRef base_ring = ac_build_load_to_sgpr(&ctx->ac,
-      ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->internal_bindings), offset);
+      ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->internal_bindings), offset);
 
    /* The conceptual layout of the GSVS ring is
     *   v0c0 .. vLv0 v0c1 .. vLc1 ..
@@ -453,6 +454,10 @@ struct si_shader *si_generate_gs_copy_shader(struct si_screen *sscreen,
    ctx.stage = MESA_SHADER_VERTEX;
    ctx.so = *so;
 
+   struct si_shader_args args;
+   si_init_shader_args(shader, &args);
+   ctx.args = &args;
+
    builder = ctx.ac.builder;
 
    /* Build the main function. */
@@ -460,7 +465,7 @@ struct si_shader *si_generate_gs_copy_shader(struct si_screen *sscreen,
 
    ctx.gsvs_ring[0] =
       ac_build_load_to_sgpr(&ctx.ac,
-         ac_get_ptr_arg(&ctx.ac, &ctx.args, ctx.internal_bindings), LLVMConstInt(ctx.ac.i32, SI_RING_GSVS, 0));
+         ac_get_ptr_arg(&ctx.ac, &ctx.args->ac, ctx.args->internal_bindings), LLVMConstInt(ctx.ac.i32, SI_RING_GSVS, 0));
 
    LLVMValueRef voffset =
       LLVMBuildMul(ctx.ac.builder, ctx.abi.vertex_id, LLVMConstInt(ctx.ac.i32, 4, 0), "");
@@ -469,7 +474,7 @@ struct si_shader *si_generate_gs_copy_shader(struct si_screen *sscreen,
    LLVMValueRef stream_id;
 
    if (!sscreen->use_ngg_streamout && ctx.so.num_outputs)
-      stream_id = si_unpack_param(&ctx, ctx.args.streamout_config, 24, 2);
+      stream_id = si_unpack_param(&ctx, ctx.args->ac.streamout_config, 24, 2);
    else
       stream_id = ctx.ac.i32_0;
 
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm_ps.c b/src/gallium/drivers/radeonsi/si_shader_llvm_ps.c
index 218a3c2a3001..9e8d9fa50d91 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm_ps.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm_ps.c
@@ -28,7 +28,7 @@
 
 LLVMValueRef si_get_sample_id(struct si_shader_context *ctx)
 {
-   return si_unpack_param(ctx, ctx->args.ancillary, 8, 4);
+   return si_unpack_param(ctx, ctx->args->ac.ancillary, 8, 4);
 }
 
 static LLVMValueRef load_sample_position(struct ac_shader_abi *abi, LLVMValueRef sample_id)
@@ -36,7 +36,7 @@ static LLVMValueRef load_sample_position(struct ac_shader_abi *abi, LLVMValueRef
    struct si_shader_context *ctx = si_shader_context_from_abi(abi);
    LLVMValueRef buf_index = LLVMConstInt(ctx->ac.i32, SI_PS_CONST_SAMPLE_POSITIONS, 0);
    LLVMValueRef resource = ac_build_load_to_sgpr(
-      &ctx->ac, ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->internal_bindings), buf_index);
+      &ctx->ac, ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->internal_bindings), buf_index);
 
    /* offset = sample_id * 8  (8 = 2 floats containing samplepos.xy) */
    LLVMValueRef offset0 =
@@ -65,7 +65,7 @@ static LLVMValueRef si_nir_emit_fbfetch(struct ac_shader_abi *abi)
    STATIC_ASSERT(SI_PS_IMAGE_COLORBUF0 % 2 == 0);
    STATIC_ASSERT(SI_PS_IMAGE_COLORBUF0_FMASK % 2 == 0);
 
-   ptr = ac_get_arg(&ctx->ac, ctx->internal_bindings);
+   ptr = ac_get_arg(&ctx->ac, ctx->args->internal_bindings);
    ptr =
       LLVMBuildPointerCast(ctx->ac.builder, ptr, ac_array_in_const32_addr_space(ctx->ac.v8i32), "");
    struct ac_llvm_pointer desc = { .v = ptr, .t = ctx->ac.v8i32 };
@@ -74,14 +74,14 @@ static LLVMValueRef si_nir_emit_fbfetch(struct ac_shader_abi *abi)
 
    unsigned chan = 0;
 
-   args.coords[chan++] = si_unpack_param(ctx, ctx->pos_fixed_pt, 0, 16);
+   args.coords[chan++] = si_unpack_param(ctx, ctx->args->pos_fixed_pt, 0, 16);
 
    if (!ctx->shader->key.ps.mono.fbfetch_is_1D)
-      args.coords[chan++] = si_unpack_param(ctx, ctx->pos_fixed_pt, 16, 16);
+      args.coords[chan++] = si_unpack_param(ctx, ctx->args->pos_fixed_pt, 16, 16);
 
    /* Get the current render target layer index. */
    if (ctx->shader->key.ps.mono.fbfetch_layered)
-      args.coords[chan++] = si_unpack_param(ctx, ctx->args.ancillary, 16, 11);
+      args.coords[chan++] = si_unpack_param(ctx, ctx->args->ac.ancillary, 16, 11);
 
    if (ctx->shader->key.ps.mono.fbfetch_msaa)
       args.coords[chan++] = si_get_sample_id(ctx);
@@ -575,7 +575,7 @@ void si_llvm_build_ps_prolog(struct si_shader_context *ctx, union si_shader_part
    LLVMValueRef ret, func;
    int num_returns, i, num_color_channels;
 
-   memset(&ctx->args, 0, sizeof(ctx->args));
+   memset(ctx->args, 0, sizeof(*ctx->args));
 
    /* Declare inputs. */
    LLVMTypeRef return_types[AC_MAX_ARGS];
@@ -584,7 +584,7 @@ void si_llvm_build_ps_prolog(struct si_shader_context *ctx, union si_shader_part
    assert(key->ps_prolog.num_input_sgprs + key->ps_prolog.num_input_vgprs + num_color_channels <=
           AC_MAX_ARGS);
    for (i = 0; i < key->ps_prolog.num_input_sgprs; i++) {
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
       return_types[num_returns++] = ctx->ac.i32;
    }
 
@@ -601,7 +601,7 @@ void si_llvm_build_ps_prolog(struct si_shader_context *ctx, union si_shader_part
          /* POS_FIXED_PT is always last. */
          arg = &pos_fixed_pt;
       }
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_FLOAT, arg);
+      ac_add_arg(&ctx->args->ac, AC_ARG_VGPR, 1, AC_ARG_FLOAT, arg);
       return_types[num_returns++] = ctx->ac.f32;
    }
 
@@ -617,7 +617,7 @@ void si_llvm_build_ps_prolog(struct si_shader_context *ctx, union si_shader_part
     * but it will prevent the compiler from overwriting them unintentionally.
     */
    ret = ctx->return_value;
-   for (i = 0; i < ctx->args.arg_count; i++) {
+   for (i = 0; i < ctx->args->ac.arg_count; i++) {
       LLVMValueRef p = LLVMGetParam(func, i);
       ret = LLVMBuildInsertValue(ctx->ac.builder, ret, p, i, "");
    }
@@ -769,7 +769,7 @@ void si_llvm_build_ps_prolog(struct si_shader_context *ctx, union si_shader_part
       while (writemask) {
          unsigned chan = u_bit_scan(&writemask);
          ret = LLVMBuildInsertValue(ctx->ac.builder, ret, color[chan],
-                                    ctx->args.arg_count + color_out_idx++, "");
+                                    ctx->args->ac.arg_count + color_out_idx++, "");
       }
    }
 
@@ -831,22 +831,22 @@ void si_llvm_build_ps_epilog(struct si_shader_context *ctx, union si_shader_part
    struct si_ps_exports exp = {};
    LLVMValueRef color[8][4] = {};
 
-   memset(&ctx->args, 0, sizeof(ctx->args));
+   memset(ctx->args, 0, sizeof(*ctx->args));
 
    /* Declare input SGPRs. */
-   ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->internal_bindings);
-   ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->bindless_samplers_and_images);
-   ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->const_and_shader_buffers);
-   ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->samplers_and_images);
-   si_add_arg_checked(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL, SI_PARAM_ALPHA_REF);
+   ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args->internal_bindings);
+   ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args->bindless_samplers_and_images);
+   ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args->const_and_shader_buffers);
+   ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args->samplers_and_images);
+   si_add_arg_checked(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_FLOAT, NULL, SI_PARAM_ALPHA_REF);
 
    /* Declare input VGPRs. */
    unsigned required_num_params =
-      ctx->args.num_sgprs_used + util_bitcount(key->ps_epilog.colors_written) * 4 +
+      ctx->args->ac.num_sgprs_used + util_bitcount(key->ps_epilog.colors_written) * 4 +
       key->ps_epilog.writes_z + key->ps_epilog.writes_stencil + key->ps_epilog.writes_samplemask;
 
-   while (ctx->args.arg_count < required_num_params)
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_FLOAT, NULL);
+   while (ctx->args->ac.arg_count < required_num_params)
+      ac_add_arg(&ctx->args->ac, AC_ARG_VGPR, 1, AC_ARG_FLOAT, NULL);
 
    /* Create the function. */
    si_llvm_create_func(ctx, "ps_epilog", NULL, 0, 0);
@@ -854,7 +854,7 @@ void si_llvm_build_ps_epilog(struct si_shader_context *ctx, union si_shader_part
    ac_llvm_add_target_dep_function_attr(ctx->main_fn.value, "InitialPSInputAddr", 0xffffff);
 
    /* Prepare color. */
-   unsigned vgpr = ctx->args.num_sgprs_used;
+   unsigned vgpr = ctx->args->ac.num_sgprs_used;
    unsigned colors_written = key->ps_epilog.colors_written;
 
    while (colors_written) {
@@ -886,7 +886,7 @@ void si_llvm_build_ps_epilog(struct si_shader_context *ctx, union si_shader_part
        key->ps_epilog.writes_samplemask ||
        mrtz_alpha) {
       LLVMValueRef depth = NULL, stencil = NULL, samplemask = NULL;
-      unsigned vgpr_index = ctx->args.num_sgprs_used +
+      unsigned vgpr_index = ctx->args->ac.num_sgprs_used +
                             util_bitcount(key->ps_epilog.colors_written) * 4;
 
       if (key->ps_epilog.writes_z)
@@ -939,8 +939,8 @@ void si_llvm_build_monolithic_ps(struct si_shader_context *ctx, struct si_shader
    struct ac_llvm_pointer main_fn = ctx->main_fn;
    /* Preserve main arguments. */
    enum ac_arg_type main_arg_types[AC_MAX_ARGS];
-   for (int i = 0; i < ctx->args.arg_count; i++)
-      main_arg_types[i] = ctx->args.args[i].type;
+   for (int i = 0; i < ctx->args->ac.arg_count; i++)
+      main_arg_types[i] = ctx->args->ac.args[i].type;
 
 
    union si_shader_part_key prolog_key;
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm_resources.c b/src/gallium/drivers/radeonsi/si_shader_llvm_resources.c
index 7900ba9da861..3fcbe3b34d37 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm_resources.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm_resources.c
@@ -55,7 +55,7 @@ static LLVMValueRef si_llvm_bound_index(struct si_shader_context *ctx, LLVMValue
 
 static LLVMValueRef load_const_buffer_desc_fast_path(struct si_shader_context *ctx)
 {
-   LLVMValueRef ptr = ac_get_arg(&ctx->ac, ctx->const_and_shader_buffers);
+   LLVMValueRef ptr = ac_get_arg(&ctx->ac, ctx->args->const_and_shader_buffers);
    struct si_shader_selector *sel = ctx->shader->selector;
 
    /* Do the bounds checking with a descriptor, because
@@ -103,7 +103,7 @@ static LLVMValueRef load_ubo(struct ac_shader_abi *abi, LLVMValueRef index)
       LLVMBuildAdd(ctx->ac.builder, index, LLVMConstInt(ctx->ac.i32, SI_NUM_SHADER_BUFFERS, 0), "");
 
    return ac_build_load_to_sgpr(&ctx->ac,
-                                ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->const_and_shader_buffers),
+                                ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->const_and_shader_buffers),
                                 index);
 }
 
@@ -114,14 +114,14 @@ static LLVMValueRef load_ssbo(struct ac_shader_abi *abi, LLVMValueRef index, boo
    /* Fast path if the shader buffer is in user SGPRs. */
    if (LLVMIsConstant(index) &&
        LLVMConstIntGetZExtValue(index) < ctx->shader->selector->cs_num_shaderbufs_in_user_sgprs)
-      return ac_get_arg(&ctx->ac, ctx->cs_shaderbuf[LLVMConstIntGetZExtValue(index)]);
+      return ac_get_arg(&ctx->ac, ctx->args->cs_shaderbuf[LLVMConstIntGetZExtValue(index)]);
 
    index = si_llvm_bound_index(ctx, index, ctx->num_shader_buffers);
    index = LLVMBuildSub(ctx->ac.builder, LLVMConstInt(ctx->ac.i32, SI_NUM_SHADER_BUFFERS - 1, 0),
                         index, "");
 
    return ac_build_load_to_sgpr(&ctx->ac,
-                                ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->const_and_shader_buffers),
+                                ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->const_and_shader_buffers),
                                 index);
 }
 
@@ -256,7 +256,7 @@ static LLVMValueRef si_nir_load_sampler_desc(struct ac_shader_abi *abi, unsigned
    assert(desc_type <= AC_DESC_BUFFER);
 
    if (bindless) {
-      struct ac_llvm_pointer list = ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->bindless_samplers_and_images);
+      struct ac_llvm_pointer list = ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->bindless_samplers_and_images);
 
       /* dynamic_index is the bindless handle */
       if (image) {
@@ -288,7 +288,7 @@ static LLVMValueRef si_nir_load_sampler_desc(struct ac_shader_abi *abi, unsigned
    if (const_index >= num_slots)
       const_index = base_index;
 
-   struct ac_llvm_pointer list = ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->samplers_and_images);
+   struct ac_llvm_pointer list = ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->samplers_and_images);
    LLVMValueRef index = LLVMConstInt(ctx->ac.i32, const_index, false);
 
    if (dynamic_index) {
@@ -311,7 +311,7 @@ static LLVMValueRef si_nir_load_sampler_desc(struct ac_shader_abi *abi, unsigned
       if (!dynamic_index &&
           const_index < ctx->shader->selector->cs_num_images_in_user_sgprs &&
           (desc_type == AC_DESC_IMAGE || desc_type == AC_DESC_BUFFER)) {
-         LLVMValueRef rsrc = ac_get_arg(&ctx->ac, ctx->cs_image[const_index]);
+         LLVMValueRef rsrc = ac_get_arg(&ctx->ac, ctx->args->cs_image[const_index]);
 
          if (desc_type == AC_DESC_IMAGE)
             rsrc = fixup_image_desc(ctx, rsrc, write);
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm_tess.c b/src/gallium/drivers/radeonsi/si_shader_llvm_tess.c
index 5a5665a51f6b..c180ea6e5243 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm_tess.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm_tess.c
@@ -30,12 +30,12 @@ LLVMValueRef si_get_rel_patch_id(struct si_shader_context *ctx)
 {
    switch (ctx->stage) {
    case MESA_SHADER_TESS_CTRL:
-      return si_unpack_param(ctx, ctx->args.tcs_rel_ids, 0, 8);
+      return si_unpack_param(ctx, ctx->args->ac.tcs_rel_ids, 0, 8);
 
    case MESA_SHADER_TESS_EVAL:
       return ctx->abi.tes_rel_patch_id_replaced ?
          ctx->abi.tes_rel_patch_id_replaced :
-         ac_get_arg(&ctx->ac, ctx->args.tes_rel_patch_id);
+         ac_get_arg(&ctx->ac, ctx->args->ac.tes_rel_patch_id);
 
    default:
       assert(0);
@@ -83,7 +83,7 @@ static LLVMValueRef get_tcs_out_patch_stride(struct si_shader_context *ctx)
 
 static LLVMValueRef get_tcs_out_patch0_patch_data_offset(struct si_shader_context *ctx)
 {
-   return si_unpack_param(ctx, ctx->tcs_out_lds_offsets, 16, 16);
+   return si_unpack_param(ctx, ctx->args->tcs_out_lds_offsets, 16, 16);
 }
 
 static LLVMValueRef get_tcs_out_current_patch_data_offset(struct si_shader_context *ctx)
@@ -106,7 +106,7 @@ LLVMValueRef si_get_num_tcs_out_vertices(struct si_shader_context *ctx)
       return LLVMConstInt(ctx->ac.i32, tcs_out_vertices, 0);
 
    return LLVMBuildAdd(ctx->ac.builder,
-                       si_unpack_param(ctx, ctx->tcs_offchip_layout, 6, 5), ctx->ac.i32_1, "");
+                       si_unpack_param(ctx, ctx->args->tcs_offchip_layout, 6, 5), ctx->ac.i32_1, "");
 }
 
 LLVMValueRef si_get_tcs_in_vertex_dw_stride(struct si_shader_context *ctx)
@@ -157,7 +157,7 @@ static LLVMValueRef get_tcs_tes_buffer_address(struct si_shader_context *ctx,
    LLVMValueRef param_stride, constant16;
 
    vertices_per_patch = si_get_num_tcs_out_vertices(ctx);
-   num_patches = si_unpack_param(ctx, ctx->tcs_offchip_layout, 0, 6);
+   num_patches = si_unpack_param(ctx, ctx->args->tcs_offchip_layout, 0, 6);
    num_patches = LLVMBuildAdd(ctx->ac.builder, num_patches, ctx->ac.i32_1, "");
    total_vertices = LLVMBuildMul(ctx->ac.builder, vertices_per_patch, num_patches, "");
 
@@ -174,7 +174,7 @@ static LLVMValueRef get_tcs_tes_buffer_address(struct si_shader_context *ctx,
    base_addr = LLVMBuildMul(ctx->ac.builder, base_addr, constant16, "");
 
    if (!vertex_index) {
-      LLVMValueRef patch_data_offset = si_unpack_param(ctx, ctx->tcs_offchip_layout, 11, 21);
+      LLVMValueRef patch_data_offset = si_unpack_param(ctx, ctx->args->tcs_offchip_layout, 11, 21);
 
       base_addr = LLVMBuildAdd(ctx->ac.builder, base_addr, patch_data_offset, "");
    }
@@ -218,7 +218,7 @@ static LLVMValueRef get_tess_ring_descriptor(struct si_shader_context *ctx, enum
 {
    LLVMBuilderRef builder = ctx->ac.builder;
    LLVMValueRef addr = ac_get_arg(
-      &ctx->ac, ring == TESS_OFFCHIP_RING_TES ? ctx->tes_offchip_addr : ctx->tcs_out_lds_layout);
+      &ctx->ac, ring == TESS_OFFCHIP_RING_TES ? ctx->args->tes_offchip_addr : ctx->args->tcs_out_lds_layout);
 
    /* TCS only receives high 13 bits of the address. */
    if (ring == TESS_OFFCHIP_RING_TCS || ring == TCS_FACTOR_RING) {
@@ -270,7 +270,7 @@ static LLVMValueRef si_nir_load_tcs_varyings(struct ac_shader_abi *abi, LLVMType
 
    ubyte semantic = info->input[driver_location].semantic;
    /* Load the TCS input from a VGPR. */
-   unsigned func_param = ctx->args.tcs_rel_ids.arg_index + 1 +
+   unsigned func_param = ctx->args->ac.tcs_rel_ids.arg_index + 1 +
       si_shader_io_get_unique_index(semantic, false) * 4;
 
    LLVMValueRef value[4];
@@ -384,7 +384,7 @@ static void si_write_tess_factors(struct si_shader_context *ctx, union si_shader
    buffer = get_tess_ring_descriptor(ctx, TCS_FACTOR_RING);
 
    /* Get the offset. */
-   tf_base = ac_get_arg(&ctx->ac, ctx->args.tcs_factor_offset);
+   tf_base = ac_get_arg(&ctx->ac, ctx->args->ac.tcs_factor_offset);
    byteoffset =
       LLVMBuildMul(ctx->ac.builder, rel_patch_id, LLVMConstInt(ctx->ac.i32, 4 * stride, 0), "");
    offset = 0;
@@ -418,7 +418,7 @@ static void si_write_tess_factors(struct si_shader_context *ctx, union si_shader
       unsigned param_outer, param_inner;
 
       buf = get_tess_ring_descriptor(ctx, TESS_OFFCHIP_RING_TCS);
-      base = ac_get_arg(&ctx->ac, ctx->args.tess_offchip_offset);
+      base = ac_get_arg(&ctx->ac, ctx->args->ac.tess_offchip_offset);
 
       param_outer = si_shader_io_get_unique_index_patch(VARYING_SLOT_TESS_LEVEL_OUTER);
       tf_outer_offset = get_tcs_tes_buffer_address(ctx, rel_patch_id, NULL,
@@ -449,7 +449,7 @@ void si_llvm_tcs_build_end(struct si_shader_context *ctx)
    LLVMValueRef rel_patch_id, invocation_id, tf_lds_offset;
 
    rel_patch_id = si_get_rel_patch_id(ctx);
-   invocation_id = si_unpack_param(ctx, ctx->args.tcs_rel_ids, 8, 5);
+   invocation_id = si_unpack_param(ctx, ctx->args->ac.tcs_rel_ids, 8, 5);
    tf_lds_offset = get_tcs_out_current_patch_data_offset(ctx);
 
    if (ctx->screen->info.gfx_level >= GFX9 && !ctx->shader->is_monolithic) {
@@ -477,18 +477,18 @@ void si_llvm_tcs_build_end(struct si_shader_context *ctx)
 
    if (ctx->screen->info.gfx_level >= GFX9) {
       ret =
-         si_insert_input_ret(ctx, ret, ctx->tcs_offchip_layout, 8 + GFX9_SGPR_TCS_OFFCHIP_LAYOUT);
-      ret = si_insert_input_ret(ctx, ret, ctx->tcs_out_lds_layout, 8 + GFX9_SGPR_TCS_OUT_LAYOUT);
+         si_insert_input_ret(ctx, ret, ctx->args->tcs_offchip_layout, 8 + GFX9_SGPR_TCS_OFFCHIP_LAYOUT);
+      ret = si_insert_input_ret(ctx, ret, ctx->args->tcs_out_lds_layout, 8 + GFX9_SGPR_TCS_OUT_LAYOUT);
       /* Tess offchip and tess factor offsets are at the beginning. */
-      ret = si_insert_input_ret(ctx, ret, ctx->args.tess_offchip_offset, 2);
-      ret = si_insert_input_ret(ctx, ret, ctx->args.tcs_factor_offset, 4);
+      ret = si_insert_input_ret(ctx, ret, ctx->args->ac.tess_offchip_offset, 2);
+      ret = si_insert_input_ret(ctx, ret, ctx->args->ac.tcs_factor_offset, 4);
       vgpr = 8 + GFX9_SGPR_TCS_OUT_LAYOUT + 1;
    } else {
-      ret = si_insert_input_ret(ctx, ret, ctx->tcs_offchip_layout, GFX6_SGPR_TCS_OFFCHIP_LAYOUT);
-      ret = si_insert_input_ret(ctx, ret, ctx->tcs_out_lds_layout, GFX6_SGPR_TCS_OUT_LAYOUT);
+      ret = si_insert_input_ret(ctx, ret, ctx->args->tcs_offchip_layout, GFX6_SGPR_TCS_OFFCHIP_LAYOUT);
+      ret = si_insert_input_ret(ctx, ret, ctx->args->tcs_out_lds_layout, GFX6_SGPR_TCS_OUT_LAYOUT);
       /* Tess offchip and tess factor offsets are after user SGPRs. */
-      ret = si_insert_input_ret(ctx, ret, ctx->args.tess_offchip_offset, GFX6_TCS_NUM_USER_SGPR);
-      ret = si_insert_input_ret(ctx, ret, ctx->args.tcs_factor_offset, GFX6_TCS_NUM_USER_SGPR + 1);
+      ret = si_insert_input_ret(ctx, ret, ctx->args->ac.tess_offchip_offset, GFX6_TCS_NUM_USER_SGPR);
+      ret = si_insert_input_ret(ctx, ret, ctx->args->ac.tcs_factor_offset, GFX6_TCS_NUM_USER_SGPR + 1);
       vgpr = GFX6_TCS_NUM_USER_SGPR + 2;
    }
 
@@ -542,30 +542,30 @@ static void si_set_ls_return_value_for_tcs(struct si_shader_context *ctx)
 
    LLVMValueRef ret = ctx->return_value;
 
-   ret = si_insert_input_ptr(ctx, ret, ctx->other_const_and_shader_buffers, 0);
-   ret = si_insert_input_ptr(ctx, ret, ctx->other_samplers_and_images, 1);
-   ret = si_insert_input_ret(ctx, ret, ctx->args.tess_offchip_offset, 2);
-   ret = si_insert_input_ret(ctx, ret, ctx->args.merged_wave_info, 3);
-   ret = si_insert_input_ret(ctx, ret, ctx->args.tcs_factor_offset, 4);
+   ret = si_insert_input_ptr(ctx, ret, ctx->args->other_const_and_shader_buffers, 0);
+   ret = si_insert_input_ptr(ctx, ret, ctx->args->other_samplers_and_images, 1);
+   ret = si_insert_input_ret(ctx, ret, ctx->args->ac.tess_offchip_offset, 2);
+   ret = si_insert_input_ret(ctx, ret, ctx->args->ac.merged_wave_info, 3);
+   ret = si_insert_input_ret(ctx, ret, ctx->args->ac.tcs_factor_offset, 4);
    if (ctx->screen->info.gfx_level <= GFX10_3)
-      ret = si_insert_input_ret(ctx, ret, ctx->args.scratch_offset, 5);
+      ret = si_insert_input_ret(ctx, ret, ctx->args->ac.scratch_offset, 5);
 
-   ret = si_insert_input_ptr(ctx, ret, ctx->internal_bindings, 8 + SI_SGPR_INTERNAL_BINDINGS);
-   ret = si_insert_input_ptr(ctx, ret, ctx->bindless_samplers_and_images,
+   ret = si_insert_input_ptr(ctx, ret, ctx->args->internal_bindings, 8 + SI_SGPR_INTERNAL_BINDINGS);
+   ret = si_insert_input_ptr(ctx, ret, ctx->args->bindless_samplers_and_images,
                              8 + SI_SGPR_BINDLESS_SAMPLERS_AND_IMAGES);
 
-   ret = si_insert_input_ret(ctx, ret, ctx->vs_state_bits, 8 + SI_SGPR_VS_STATE_BITS);
+   ret = si_insert_input_ret(ctx, ret, ctx->args->vs_state_bits, 8 + SI_SGPR_VS_STATE_BITS);
 
-   ret = si_insert_input_ret(ctx, ret, ctx->tcs_offchip_layout, 8 + GFX9_SGPR_TCS_OFFCHIP_LAYOUT);
-   ret = si_insert_input_ret(ctx, ret, ctx->tcs_out_lds_offsets, 8 + GFX9_SGPR_TCS_OUT_OFFSETS);
-   ret = si_insert_input_ret(ctx, ret, ctx->tcs_out_lds_layout, 8 + GFX9_SGPR_TCS_OUT_LAYOUT);
+   ret = si_insert_input_ret(ctx, ret, ctx->args->tcs_offchip_layout, 8 + GFX9_SGPR_TCS_OFFCHIP_LAYOUT);
+   ret = si_insert_input_ret(ctx, ret, ctx->args->tcs_out_lds_offsets, 8 + GFX9_SGPR_TCS_OUT_OFFSETS);
+   ret = si_insert_input_ret(ctx, ret, ctx->args->tcs_out_lds_layout, 8 + GFX9_SGPR_TCS_OUT_LAYOUT);
 
    unsigned vgpr = 8 + GFX9_TCS_NUM_USER_SGPR;
    ret = LLVMBuildInsertValue(ctx->ac.builder, ret,
-                              ac_to_float(&ctx->ac, ac_get_arg(&ctx->ac, ctx->args.tcs_patch_id)),
+                              ac_to_float(&ctx->ac, ac_get_arg(&ctx->ac, ctx->args->ac.tcs_patch_id)),
                               vgpr++, "");
    ret = LLVMBuildInsertValue(ctx->ac.builder, ret,
-                              ac_to_float(&ctx->ac, ac_get_arg(&ctx->ac, ctx->args.tcs_rel_ids)),
+                              ac_to_float(&ctx->ac, ac_get_arg(&ctx->ac, ctx->args->ac.tcs_rel_ids)),
                               vgpr++, "");
    ctx->return_value = ret;
 }
@@ -604,54 +604,54 @@ void si_llvm_ls_build_end(struct si_shader_context *ctx)
  */
 void si_llvm_build_tcs_epilog(struct si_shader_context *ctx, union si_shader_part_key *key)
 {
-   memset(&ctx->args, 0, sizeof(ctx->args));
+   memset(ctx->args, 0, sizeof(*ctx->args));
 
    if (ctx->screen->info.gfx_level >= GFX9) {
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.tess_offchip_offset);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* wave info */
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.tcs_factor_offset);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->tcs_offchip_layout);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->tcs_out_lds_layout);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args->ac.tess_offchip_offset);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL); /* wave info */
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args->ac.tcs_factor_offset);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args->tcs_offchip_layout);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args->tcs_out_lds_layout);
    } else {
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->tcs_offchip_layout);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->tcs_out_lds_layout);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.tess_offchip_offset);
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args.tcs_factor_offset);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args->tcs_offchip_layout);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args->tcs_out_lds_layout);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, NULL);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args->ac.tess_offchip_offset);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &ctx->args->ac.tcs_factor_offset);
    }
 
-   ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, NULL); /* VGPR gap */
-   ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, NULL); /* VGPR gap */
+   ac_add_arg(&ctx->args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, NULL); /* VGPR gap */
+   ac_add_arg(&ctx->args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, NULL); /* VGPR gap */
    struct ac_arg rel_patch_id; /* patch index within the wave (REL_PATCH_ID) */
-   ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &rel_patch_id);
+   ac_add_arg(&ctx->args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &rel_patch_id);
    struct ac_arg invocation_id; /* invocation ID within the patch */
-   ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &invocation_id);
+   ac_add_arg(&ctx->args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &invocation_id);
    struct ac_arg
       tcs_out_current_patch_data_offset; /* LDS offset where tess factors should be loaded from */
-   ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &tcs_out_current_patch_data_offset);
+   ac_add_arg(&ctx->args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &tcs_out_current_patch_data_offset);
 
    struct ac_arg tess_factors[6];
    for (unsigned i = 0; i < 6; i++)
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &tess_factors[i]);
+      ac_add_arg(&ctx->args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &tess_factors[i]);
 
    /* Create the function. */
    si_llvm_create_func(ctx, "tcs_epilog", NULL, 0, ctx->screen->info.gfx_level >= GFX7 ? 128 : 0);
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c b/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c
index 417073ee2510..734a15504b89 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c
@@ -100,7 +100,7 @@ static void load_input_vs(struct si_shader_context *ctx, unsigned input_index, L
        */
       LLVMValueRef sel_y1 = LLVMBuildICmp(ctx->ac.builder, LLVMIntNE, vertex_id, ctx->ac.i32_1, "");
 
-      unsigned param_vs_blit_inputs = ctx->vs_blit_inputs.arg_index;
+      unsigned param_vs_blit_inputs = ctx->args->vs_blit_inputs.arg_index;
       if (input_index == 0) {
          /* Position: */
          LLVMValueRef x1y1 = LLVMGetParam(ctx->main_fn.value, param_vs_blit_inputs);
@@ -165,11 +165,11 @@ static void load_input_vs(struct si_shader_context *ctx, unsigned input_index, L
    LLVMValueRef tmp;
 
    if (input_index < num_vbos_in_user_sgprs) {
-      vb_desc = ac_get_arg(&ctx->ac, ctx->vb_descriptors[input_index]);
+      vb_desc = ac_get_arg(&ctx->ac, ctx->args->vb_descriptors[input_index]);
    } else {
       unsigned index = input_index - num_vbos_in_user_sgprs;
       vb_desc = ac_build_load_to_sgpr(
-         &ctx->ac, ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->args.vertex_buffers),
+         &ctx->ac, ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->ac.vertex_buffers),
          LLVMConstInt(ctx->ac.i32, index, 0));
    }
 
@@ -182,10 +182,11 @@ static void load_input_vs(struct si_shader_context *ctx, unsigned input_index, L
 
       vertex_index = get_vertex_index(ctx, &ctx->shader->key.ge.part.vs.prolog,
                                       input_index, ctx->instance_divisor_constbuf,
-                                      ctx->args.start_instance.arg_index,
-                                      ctx->args.base_vertex.arg_index);
+                                      ctx->args->ac.start_instance.arg_index,
+                                      ctx->args->ac.base_vertex.arg_index);
    } else {
-      vertex_index = LLVMGetParam(ctx->main_fn.value, ctx->vertex_index0.arg_index + input_index);
+      vertex_index = LLVMGetParam(ctx->main_fn.value,
+                                  ctx->args->vertex_index0.arg_index + input_index);
    }
 
    /* Use the open-coded implementation for all loads of doubles and
@@ -382,7 +383,7 @@ void si_llvm_emit_streamout(struct si_shader_context *ctx, struct si_shader_outp
    int i;
 
    /* Get bits [22:16], i.e. (so_param >> 16) & 127; */
-   LLVMValueRef so_vtx_count = si_unpack_param(ctx, ctx->args.streamout_config, 16, 7);
+   LLVMValueRef so_vtx_count = si_unpack_param(ctx, ctx->args->ac.streamout_config, 16, 7);
 
    LLVMValueRef tid = ac_get_thread_id(&ctx->ac);
 
@@ -400,7 +401,7 @@ void si_llvm_emit_streamout(struct si_shader_context *ctx, struct si_shader_outp
        *                attrib_offset
        */
 
-      LLVMValueRef so_write_index = ac_get_arg(&ctx->ac, ctx->args.streamout_write_index);
+      LLVMValueRef so_write_index = ac_get_arg(&ctx->ac, ctx->args->ac.streamout_write_index);
 
       /* Compute (streamout_write_index + thread_id). */
       so_write_index = LLVMBuildAdd(builder, so_write_index, tid, "");
@@ -409,7 +410,7 @@ void si_llvm_emit_streamout(struct si_shader_context *ctx, struct si_shader_outp
        * enabled buffer. */
       LLVMValueRef so_write_offset[4] = {};
       LLVMValueRef so_buffers[4];
-      struct ac_llvm_pointer arg = ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->internal_bindings);
+      struct ac_llvm_pointer arg = ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->internal_bindings);
 
       for (i = 0; i < 4; i++) {
          if (!so->stride[i])
@@ -419,7 +420,7 @@ void si_llvm_emit_streamout(struct si_shader_context *ctx, struct si_shader_outp
 
          so_buffers[i] = ac_build_load_to_sgpr(&ctx->ac, arg, offset);
 
-         LLVMValueRef so_offset = ac_get_arg(&ctx->ac, ctx->args.streamout_offset[i]);
+         LLVMValueRef so_offset = ac_get_arg(&ctx->ac, ctx->args->ac.streamout_offset[i]);
          so_offset = LLVMBuildMul(builder, so_offset, LLVMConstInt(ctx->ac.i32, 4, 0), "");
 
          so_write_offset[i] = ac_build_imad(
@@ -452,7 +453,7 @@ void si_llvm_clipvertex_to_clipdist(struct si_shader_context *ctx,
    LLVMValueRef base_elt;
    LLVMValueRef constbuf_index = LLVMConstInt(ctx->ac.i32, SI_VS_CONST_CLIP_PLANES, 0);
    LLVMValueRef const_resource = ac_build_load_to_sgpr(
-      &ctx->ac, ac_get_ptr_arg(&ctx->ac, &ctx->args, ctx->internal_bindings), constbuf_index);
+      &ctx->ac, ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->internal_bindings), constbuf_index);
    unsigned clipdist_mask = ctx->shader->selector->info.clipdist_mask &
                             ~ctx->shader->key.ge.opt.kill_clip_distances;
 
@@ -791,7 +792,7 @@ void si_llvm_build_vs_prolog(struct si_shader_context *ctx, union si_shader_part
    unsigned num_all_input_regs = key->vs_prolog.num_input_sgprs + num_input_vgprs;
    unsigned user_sgpr_base = key->vs_prolog.num_merged_next_stage_vgprs ? 8 : 0;
 
-   memset(&ctx->args, 0, sizeof(ctx->args));
+   memset(ctx->args, 0, sizeof(*ctx->args));
 
    /* 4 preloaded VGPRs + vertex load indices as prolog outputs */
    returns = alloca((num_all_input_regs + key->vs_prolog.num_inputs) * sizeof(LLVMTypeRef));
@@ -799,13 +800,13 @@ void si_llvm_build_vs_prolog(struct si_shader_context *ctx, union si_shader_part
 
    /* Declare input and output SGPRs. */
    for (i = 0; i < key->vs_prolog.num_input_sgprs; i++) {
-      ac_add_arg(&ctx->args, AC_ARG_SGPR, 1, AC_ARG_INT, &input_sgpr_param[i]);
+      ac_add_arg(&ctx->args->ac, AC_ARG_SGPR, 1, AC_ARG_INT, &input_sgpr_param[i]);
       returns[num_returns++] = ctx->ac.i32;
    }
 
    /* Preloaded VGPRs (outputs must be floats) */
    for (i = 0; i < num_input_vgprs; i++) {
-      ac_add_arg(&ctx->args, AC_ARG_VGPR, 1, AC_ARG_INT, &input_vgpr_param[i]);
+      ac_add_arg(&ctx->args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &input_vgpr_param[i]);
       returns[num_returns++] = ctx->ac.f32;
    }
 
@@ -888,7 +889,7 @@ void si_llvm_build_vs_prolog(struct si_shader_context *ctx, union si_shader_part
                                             user_sgpr_base + SI_SGPR_BASE_VERTEX);
 
       index = ac_to_float(&ctx->ac, index);
-      ret = LLVMBuildInsertValue(ctx->ac.builder, ret, index, ctx->args.arg_count + i, "");
+      ret = LLVMBuildInsertValue(ctx->ac.builder, ret, index, ctx->args->ac.arg_count + i, "");
    }
 
    si_llvm_build_ret(ctx, ret);
-- 
GitLab


From 3dea6c256f182bb7a79d90d6d34d99edaf954910 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Wed, 10 Aug 2022 14:48:18 +0800
Subject: [PATCH 14/45] ac/llvm: nir_load_smem_amd support 32bit base address
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

For radeonsi which use 32bit address in ac_build_load_to_sgpr().

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/llvm/ac_nir_to_llvm.c      | 10 ++++++++--
 src/compiler/nir/nir_intrinsics.py |  2 +-
 2 files changed, 9 insertions(+), 3 deletions(-)

diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index fa303c9ad373..f0c476f496bf 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -4386,11 +4386,17 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
       LLVMValueRef base = get_src(ctx, instr->src[0]);
       LLVMValueRef offset = get_src(ctx, instr->src[1]);
 
+      bool is_addr_32bit = nir_src_bit_size(instr->src[0]) == 32;
+      int addr_space = is_addr_32bit ? AC_ADDR_SPACE_CONST_32BIT : AC_ADDR_SPACE_CONST;
+
       LLVMTypeRef result_type = get_def_type(ctx, &instr->dest.ssa);
-      LLVMTypeRef byte_ptr_type = LLVMPointerType(ctx->ac.i8, AC_ADDR_SPACE_CONST);
+      LLVMTypeRef byte_ptr_type = LLVMPointerType(ctx->ac.i8, addr_space);
 
       LLVMValueRef addr = LLVMBuildIntToPtr(ctx->ac.builder, base, byte_ptr_type, "");
-      addr = LLVMBuildGEP2(ctx->ac.builder, ctx->ac.i8, addr, &offset, 1, "");
+      /* see ac_build_load_custom() for 32bit/64bit addr GEP difference */
+      addr = is_addr_32bit ?
+         LLVMBuildInBoundsGEP2(ctx->ac.builder, ctx->ac.i8, addr, &offset, 1, "") :
+         LLVMBuildGEP2(ctx->ac.builder, ctx->ac.i8, addr, &offset, 1, "");
 
       LLVMSetMetadata(addr, ctx->ac.uniform_md_kind, ctx->ac.empty_md);
       result = LLVMBuildLoad2(ctx->ac.builder, result_type, addr, "");
diff --git a/src/compiler/nir/nir_intrinsics.py b/src/compiler/nir/nir_intrinsics.py
index d9db0afc3b45..98643fa44d36 100644
--- a/src/compiler/nir/nir_intrinsics.py
+++ b/src/compiler/nir/nir_intrinsics.py
@@ -1474,7 +1474,7 @@ intrinsic("load_force_vrs_rates_amd", dest_comp=1, bit_sizes=[32], flags=[CAN_EL
 intrinsic("load_scalar_arg_amd", dest_comp=0, bit_sizes=[32], indices=[BASE, ARG_UPPER_BOUND_U32_AMD], flags=[CAN_ELIMINATE, CAN_REORDER])
 intrinsic("load_vector_arg_amd", dest_comp=0, bit_sizes=[32], indices=[BASE, ARG_UPPER_BOUND_U32_AMD], flags=[CAN_ELIMINATE, CAN_REORDER])
 
-# src[] = { 64-bit base address, 32-bit offset }.
+# src[] = { 32/64-bit base address, 32-bit offset }.
 intrinsic("load_smem_amd", src_comp=[1, 1], dest_comp=0, bit_sizes=[32],
                            indices=[ALIGN_MUL, ALIGN_OFFSET],
                            flags=[CAN_ELIMINATE, CAN_REORDER])
-- 
GitLab


From 49ef891bcf656fc2516315f54c45891c63bfda80 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Wed, 10 Aug 2022 16:57:37 +0800
Subject: [PATCH 15/45] nir,ac/llvm: add nir_load_smem_buffer_amd
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Used by radeonsi to load const buffer.

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/llvm/ac_nir_to_llvm.c              | 9 +++++++++
 src/compiler/nir/nir_divergence_analysis.c | 1 +
 src/compiler/nir/nir_intrinsics.py         | 5 +++++
 3 files changed, 15 insertions(+)

diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index f0c476f496bf..e035254f89fe 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -4403,6 +4403,15 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
       LLVMSetMetadata(result, ctx->ac.invariant_load_md_kind, ctx->ac.empty_md);
       break;
    }
+   case nir_intrinsic_load_smem_buffer_amd: {
+      LLVMValueRef descriptor = get_src(ctx, instr->src[0]);
+      LLVMValueRef offset = get_src(ctx, instr->src[1]);
+      unsigned num_components = instr->dest.ssa.num_components;
+
+      result = ac_build_buffer_load(&ctx->ac, descriptor, num_components, NULL, offset, NULL,
+                                    ctx->ac.i32, 0, true, true);
+      break;
+   }
    case nir_intrinsic_ordered_xfb_counter_add_amd: {
       /* must be called in a single lane of a workgroup. */
       LLVMTypeRef gdsptr = LLVMPointerType(ctx->ac.i32, AC_ADDR_SPACE_GDS);
diff --git a/src/compiler/nir/nir_divergence_analysis.c b/src/compiler/nir/nir_divergence_analysis.c
index 1a039d90aad9..9ad6e30811c0 100644
--- a/src/compiler/nir/nir_divergence_analysis.c
+++ b/src/compiler/nir/nir_divergence_analysis.c
@@ -185,6 +185,7 @@ visit_intrinsic(nir_shader *shader, nir_intrinsic_instr *instr)
    case nir_intrinsic_load_tess_level_outer_default:
    case nir_intrinsic_load_scalar_arg_amd:
    case nir_intrinsic_load_smem_amd:
+   case nir_intrinsic_load_smem_buffer_amd:
    case nir_intrinsic_load_rt_dynamic_callable_stack_base_amd:
    case nir_intrinsic_load_global_const_block_intel:
    case nir_intrinsic_load_reloc_const_intel:
diff --git a/src/compiler/nir/nir_intrinsics.py b/src/compiler/nir/nir_intrinsics.py
index 98643fa44d36..bc386b25faf6 100644
--- a/src/compiler/nir/nir_intrinsics.py
+++ b/src/compiler/nir/nir_intrinsics.py
@@ -1479,6 +1479,11 @@ intrinsic("load_smem_amd", src_comp=[1, 1], dest_comp=0, bit_sizes=[32],
                            indices=[ALIGN_MUL, ALIGN_OFFSET],
                            flags=[CAN_ELIMINATE, CAN_REORDER])
 
+# src[] = { descriptor, offset }
+intrinsic("load_smem_buffer_amd", src_comp=[4, 1], dest_comp=0, bit_sizes=[32],
+                                  indices=[ALIGN_MUL, ALIGN_OFFSET],
+                                  flags=[CAN_ELIMINATE, CAN_REORDER])
+
 # src[] = { offset }.
 intrinsic("load_shared2_amd", [1], dest_comp=2, indices=[OFFSET0, OFFSET1, ST64], flags=[CAN_ELIMINATE])
 
-- 
GitLab


From c4aedd6b4e701023bde671075e0b05b18749dfd2 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Wed, 10 Aug 2022 19:18:15 +0800
Subject: [PATCH 16/45] ac/nir: add ac_nir_unpack_arg
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/common/ac_nir.c | 8 ++++++++
 src/amd/common/ac_nir.h | 4 ++++
 2 files changed, 12 insertions(+)

diff --git a/src/amd/common/ac_nir.c b/src/amd/common/ac_nir.c
index 9c4051bc07ab..933bfd44d52a 100644
--- a/src/amd/common/ac_nir.c
+++ b/src/amd/common/ac_nir.c
@@ -35,6 +35,14 @@ ac_nir_load_arg(nir_builder *b, const struct ac_shader_args *ac_args, struct ac_
       return nir_load_vector_arg_amd(b, num_components, .base = arg.arg_index);
 }
 
+nir_ssa_def *
+ac_nir_unpack_arg(nir_builder *b, const struct ac_shader_args *ac_args, struct ac_arg arg,
+                  unsigned rshift, unsigned bitwidth)
+{
+   nir_ssa_def *value = ac_nir_load_arg(b, ac_args, arg);
+   return nir_ubfe_imm(b, value, rshift, bitwidth);
+}
+
 /**
  * This function takes an I/O intrinsic like load/store_input,
  * and emits a sequence that calculates the full offset of that instruction,
diff --git a/src/amd/common/ac_nir.h b/src/amd/common/ac_nir.h
index 86aaf55b0266..65a9d07441fe 100644
--- a/src/amd/common/ac_nir.h
+++ b/src/amd/common/ac_nir.h
@@ -63,6 +63,10 @@ typedef void (*ac_nir_cull_accepted)(nir_builder *b, void *state);
 nir_ssa_def *
 ac_nir_load_arg(nir_builder *b, const struct ac_shader_args *ac_args, struct ac_arg arg);
 
+nir_ssa_def *
+ac_nir_unpack_arg(nir_builder *b, const struct ac_shader_args *ac_args, struct ac_arg arg,
+                  unsigned rshift, unsigned bitwidth);
+
 nir_ssa_def *
 ac_nir_calc_io_offset(nir_builder *b,
                       nir_intrinsic_instr *intrin,
-- 
GitLab


From 73378da266610aa555ca3b685350fa1f75983f91 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Wed, 10 Aug 2022 22:26:49 +0800
Subject: [PATCH 17/45] radeonsi: add si_nir_lower_abi pass
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

This pass is for lower intrinsics to driver spec nir instructions,
so that each compiler backend don't need to implement their own.
Like radv_nir_lower_abi().

Currently only lower intrinsics in si_llvm_load_intrinsic().

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/gallium/drivers/radeonsi/meson.build      |   1 +
 .../drivers/radeonsi/si_nir_lower_abi.c       | 263 ++++++++++++++++++
 src/gallium/drivers/radeonsi/si_shader.c      |   8 +-
 .../drivers/radeonsi/si_shader_internal.h     |   7 +-
 src/gallium/drivers/radeonsi/si_shader_llvm.c |   4 +-
 5 files changed, 276 insertions(+), 7 deletions(-)
 create mode 100644 src/gallium/drivers/radeonsi/si_nir_lower_abi.c

diff --git a/src/gallium/drivers/radeonsi/meson.build b/src/gallium/drivers/radeonsi/meson.build
index 12ceae6e9b76..18099fa28fe3 100644
--- a/src/gallium/drivers/radeonsi/meson.build
+++ b/src/gallium/drivers/radeonsi/meson.build
@@ -45,6 +45,7 @@ files_libradeonsi = files(
   'si_public.h',
   'si_query.c',
   'si_query.h',
+  'si_nir_lower_abi.c',
   'si_nir_optim.c',
   'si_sdma_copy_image.c',
   'si_shader.c',
diff --git a/src/gallium/drivers/radeonsi/si_nir_lower_abi.c b/src/gallium/drivers/radeonsi/si_nir_lower_abi.c
new file mode 100644
index 000000000000..dcc44ee2bdc7
--- /dev/null
+++ b/src/gallium/drivers/radeonsi/si_nir_lower_abi.c
@@ -0,0 +1,263 @@
+/*
+ * Copyright 2022 Advanced Micro Devices, Inc.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * on the rights to use, copy, modify, merge, publish, distribute, sub
+ * license, and/or sell copies of the Software, and to permit persons to whom
+ * the Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE AUTHOR(S) AND/OR THEIR SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "nir_builder.h"
+#include "util/u_prim.h"
+
+#include "ac_nir.h"
+#include "si_pipe.h"
+#include "si_query.h"
+#include "si_state.h"
+#include "si_shader_internal.h"
+
+struct lower_abi_state {
+   struct si_shader *shader;
+   struct si_shader_args *args;
+};
+
+#define GET_FIELD_NIR(field) \
+   ac_nir_unpack_arg(b, &args->ac, args->vs_state_bits, \
+                     field##__SHIFT, util_bitcount(field##__MASK))
+
+static nir_ssa_def *load_internal_binding(nir_builder *b, struct si_shader_args *args,
+                                          unsigned slot)
+{
+   nir_ssa_def *addr = ac_nir_load_arg(b, &args->ac, args->internal_bindings);
+   return nir_load_smem_amd(b, 4, addr, nir_imm_int(b, slot * 16));
+}
+
+static nir_ssa_def *get_num_vert_per_prim(nir_builder *b, struct si_shader *shader,
+                                          struct si_shader_args *args)
+{
+   const struct si_shader_info *info = &shader->selector->info;
+   gl_shader_stage stage = shader->selector->stage;
+
+   unsigned num_vertices;
+   if (stage == MESA_SHADER_GEOMETRY)
+      num_vertices = u_vertices_per_prim(info->base.gs.output_primitive);
+   else if (stage == MESA_SHADER_VERTEX) {
+      if (info->base.vs.blit_sgprs_amd)
+         num_vertices = 3;
+      else if (shader->key.ge.opt.ngg_culling & SI_NGG_CULL_LINES)
+         num_vertices = 2;
+      else {
+         /* Extract OUTPRIM field. */
+         nir_ssa_def *num = GET_FIELD_NIR(GS_STATE_OUTPRIM);
+         return nir_iadd_imm(b, num, 1);
+      }
+   } else {
+      assert(stage == MESA_SHADER_TESS_EVAL);
+
+      if (info->base.tess.point_mode)
+         num_vertices = 1;
+      else if (info->base.tess._primitive_mode == TESS_PRIMITIVE_ISOLINES)
+         num_vertices = 2;
+      else
+         num_vertices = 3;
+   }
+   return nir_imm_int(b, num_vertices);
+}
+
+static bool lower_abi_instr(nir_builder *b, nir_instr *instr, struct lower_abi_state *s)
+{
+   if (instr->type != nir_instr_type_intrinsic)
+      return false;
+
+   nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
+
+   struct si_shader *shader = s->shader;
+   struct si_shader_args *args = s->args;
+   struct si_shader_selector *sel = shader->selector;
+   union si_shader_key *key = &shader->key;
+   gl_shader_stage stage = sel->stage;
+
+   b->cursor = nir_before_instr(instr);
+
+   nir_ssa_def *replacement = NULL;
+
+   switch (intrin->intrinsic) {
+   case nir_intrinsic_load_first_vertex:
+      replacement = ac_nir_load_arg(b, &args->ac, args->ac.base_vertex);
+      break;
+   case nir_intrinsic_load_base_vertex: {
+      nir_ssa_def *indexed = GET_FIELD_NIR(VS_STATE_INDEXED);
+      indexed = nir_i2b(b, indexed);
+
+      nir_ssa_def *base_vertex = ac_nir_load_arg(b, &args->ac, args->ac.base_vertex);
+      replacement = nir_bcsel(b, indexed, base_vertex, nir_imm_int(b, 0));
+      break;
+   }
+   case nir_intrinsic_load_workgroup_size: {
+      assert(sel->info.base.workgroup_size_variable && sel->info.uses_variable_block_size);
+
+      nir_ssa_def *block_size = ac_nir_load_arg(b, &args->ac, args->block_size);
+      nir_ssa_def *comp[] = {
+         nir_ubfe_imm(b, block_size, 0, 10),
+         nir_ubfe_imm(b, block_size, 10, 10),
+         nir_ubfe_imm(b, block_size, 20, 10),
+      };
+      replacement = nir_vec(b, comp, 3);
+      break;
+   }
+   case nir_intrinsic_load_tess_level_outer_default:
+   case nir_intrinsic_load_tess_level_inner_default: {
+      nir_ssa_def *buf = load_internal_binding(b, args, SI_HS_CONST_DEFAULT_TESS_LEVELS);
+      unsigned num_components = intrin->dest.ssa.num_components;
+      unsigned offset =
+         intrin->intrinsic == nir_intrinsic_load_tess_level_inner_default ? 16 : 0;
+      replacement = nir_load_smem_buffer_amd(b, num_components, buf, nir_imm_int(b, offset));
+      break;
+   }
+   case nir_intrinsic_load_patch_vertices_in:
+      if (stage == MESA_SHADER_TESS_CTRL)
+         replacement = ac_nir_unpack_arg(b, &args->ac, args->tcs_out_lds_layout, 13, 6);
+      else if (stage == MESA_SHADER_TESS_EVAL) {
+         nir_ssa_def *tmp = ac_nir_unpack_arg(b, &args->ac, args->tcs_offchip_layout, 6, 5);
+         replacement = nir_iadd_imm(b, tmp, 1);
+      } else
+         unreachable("no nir_load_patch_vertices_in");
+      break;
+   case nir_intrinsic_load_sample_mask_in:
+      replacement = ac_nir_load_arg(b, &args->ac, args->ac.sample_coverage);
+      break;
+   case nir_intrinsic_load_lshs_vertex_stride_amd:
+      if (stage == MESA_SHADER_VERTEX)
+         replacement = nir_imm_int(b, sel->info.lshs_vertex_stride);
+      else if (stage == MESA_SHADER_TESS_CTRL)
+         replacement = sel->screen->info.gfx_level >= GFX9 && shader->is_monolithic ?
+            nir_imm_int(b, key->ge.part.tcs.ls->info.lshs_vertex_stride) :
+            nir_ishl_imm(b, GET_FIELD_NIR(VS_STATE_LS_OUT_VERTEX_SIZE), 2);
+      else
+         unreachable("no nir_load_lshs_vertex_stride_amd");
+      break;
+   case nir_intrinsic_load_tcs_num_patches_amd: {
+      nir_ssa_def *tmp = ac_nir_unpack_arg(b, &args->ac, args->tcs_offchip_layout, 0, 6);
+      replacement = nir_iadd_imm(b, tmp, 1);
+      break;
+   }
+   case nir_intrinsic_load_hs_out_patch_data_offset_amd:
+      replacement = ac_nir_unpack_arg(b, &args->ac, args->tcs_offchip_layout, 11, 21);
+      break;
+   case nir_intrinsic_load_ring_tess_offchip_offset_amd:
+      replacement = ac_nir_load_arg(b, &args->ac, args->ac.tess_offchip_offset);
+      break;
+   case nir_intrinsic_load_ring_es2gs_offset_amd:
+      replacement = ac_nir_load_arg(b, &args->ac, args->ac.es2gs_offset);
+      break;
+   case nir_intrinsic_load_clip_half_line_width_amd: {
+      nir_ssa_def *addr = ac_nir_load_arg(b, &args->ac, args->small_prim_cull_info);
+      replacement = nir_load_smem_amd(b, 2, addr, nir_imm_int(b, 32));
+      break;
+   }
+   case nir_intrinsic_load_viewport_xy_scale_and_offset: {
+      bool prim_is_lines = key->ge.opt.ngg_culling & SI_NGG_CULL_LINES;
+      nir_ssa_def *addr = ac_nir_load_arg(b, &args->ac, args->small_prim_cull_info);
+      unsigned offset = prim_is_lines ? 16 : 0;
+      replacement = nir_load_smem_amd(b, 4, addr, nir_imm_int(b, offset));
+      break;
+   }
+   case nir_intrinsic_load_num_vertices_per_primitive_amd:
+      replacement = get_num_vert_per_prim(b, shader, args);
+      break;
+   case nir_intrinsic_load_cull_ccw_amd:
+      /* radeonsi embed cw/ccw info into front/back face enabled */
+      replacement = nir_imm_bool(b, false);
+      break;
+   case nir_intrinsic_load_cull_any_enabled_amd:
+      replacement = nir_imm_bool(b, !!key->ge.opt.ngg_culling);
+      break;
+   case nir_intrinsic_load_cull_back_face_enabled_amd:
+      replacement = nir_imm_bool(b, key->ge.opt.ngg_culling & SI_NGG_CULL_BACK_FACE);
+      break;
+   case nir_intrinsic_load_cull_front_face_enabled_amd:
+      replacement = nir_imm_bool(b, key->ge.opt.ngg_culling & SI_NGG_CULL_FRONT_FACE);
+      break;
+   case nir_intrinsic_load_cull_small_prim_precision_amd: {
+      nir_ssa_def *small_prim_precision =
+         key->ge.opt.ngg_culling & SI_NGG_CULL_LINES ?
+         GET_FIELD_NIR(GS_STATE_SMALL_PRIM_PRECISION_NO_AA) :
+         GET_FIELD_NIR(GS_STATE_SMALL_PRIM_PRECISION);
+
+      /* Extract the small prim precision. */
+      small_prim_precision = nir_ior_imm(b, small_prim_precision, 0x70);
+      replacement = nir_ishl_imm(b, small_prim_precision, 23);
+      break;
+   }
+   case nir_intrinsic_load_cull_small_primitives_enabled_amd: {
+      unsigned mask = SI_NGG_CULL_LINES | SI_NGG_CULL_SMALL_LINES_DIAMOND_EXIT;
+      replacement = nir_imm_bool(b, (key->ge.opt.ngg_culling & mask) != SI_NGG_CULL_LINES);
+      break;
+   }
+   case nir_intrinsic_load_provoking_vtx_in_prim_amd:
+      replacement = GET_FIELD_NIR(GS_STATE_PROVOKING_VTX_INDEX);
+      break;
+   case nir_intrinsic_load_pipeline_stat_query_enabled_amd:
+      replacement = nir_i2b(b, GET_FIELD_NIR(GS_STATE_PIPELINE_STATS_EMU));
+      break;
+   case nir_intrinsic_load_prim_gen_query_enabled_amd:
+   case nir_intrinsic_load_prim_xfb_query_enabled_amd:
+      replacement = nir_i2b(b, GET_FIELD_NIR(GS_STATE_STREAMOUT_QUERY_ENABLED));
+      break;
+   case nir_intrinsic_load_clamp_vertex_color_amd:
+      replacement = nir_i2b(b, GET_FIELD_NIR(VS_STATE_CLAMP_VERTEX_COLOR));
+      break;
+   default:
+      return false;
+   }
+
+   if (replacement)
+      nir_ssa_def_rewrite_uses(&intrin->dest.ssa, replacement);
+
+   nir_instr_remove(instr);
+   nir_instr_free(instr);
+
+   return true;
+}
+
+bool si_nir_lower_abi(nir_shader *nir, struct si_shader *shader, struct si_shader_args *args)
+{
+   struct lower_abi_state state = {
+      .shader = shader,
+      .args = args,
+   };
+
+   nir_function_impl *impl = nir_shader_get_entrypoint(nir);
+
+   nir_builder b;
+   nir_builder_init(&b, impl);
+
+   bool progress = false;
+   nir_foreach_block_safe(block, impl) {
+      nir_foreach_instr_safe(instr, block) {
+         progress |= lower_abi_instr(&b, instr, &state);
+      }
+   }
+
+   nir_metadata preserved = progress ?
+      nir_metadata_dominance | nir_metadata_block_index :
+      nir_metadata_all;
+   nir_metadata_preserve(impl, preserved);
+
+   return progress;
+}
diff --git a/src/gallium/drivers/radeonsi/si_shader.c b/src/gallium/drivers/radeonsi/si_shader.c
index 5e8f1f29337c..e21ab6700f71 100644
--- a/src/gallium/drivers/radeonsi/si_shader.c
+++ b/src/gallium/drivers/radeonsi/si_shader.c
@@ -1789,8 +1789,8 @@ static void si_assign_param_offsets(nir_shader *nir, struct si_shader *shader)
    si_nir_assign_param_offsets(nir, shader, slot_remap);
 }
 
-struct nir_shader *si_get_nir_shader(struct si_shader *shader, bool *free_nir,
-                                     uint64_t tcs_vgpr_only_inputs)
+struct nir_shader *si_get_nir_shader(struct si_shader *shader, struct si_shader_args *args,
+                                     bool *free_nir, uint64_t tcs_vgpr_only_inputs)
 {
    struct si_shader_selector *sel = shader->selector;
    const union si_shader_key *key = &shader->key;
@@ -1928,6 +1928,8 @@ struct nir_shader *si_get_nir_shader(struct si_shader *shader, bool *free_nir,
       opt_offsets = true;
    }
 
+   NIR_PASS(progress2, nir, si_nir_lower_abi, shader, args);
+
    if (progress2 || opt_offsets)
       si_nir_opts(sel->screen, nir, false);
 
@@ -1976,7 +1978,7 @@ bool si_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *compi
    si_init_shader_args(shader, &args);
 
    bool free_nir;
-   struct nir_shader *nir = si_get_nir_shader(shader, &free_nir, 0);
+   struct nir_shader *nir = si_get_nir_shader(shader, &args, &free_nir, 0);
 
    struct pipe_stream_output_info so = {};
    /* NGG streamout has been lowered to buffer store in nir. */
diff --git a/src/gallium/drivers/radeonsi/si_shader_internal.h b/src/gallium/drivers/radeonsi/si_shader_internal.h
index 83472047bac8..4df29868f74e 100644
--- a/src/gallium/drivers/radeonsi/si_shader_internal.h
+++ b/src/gallium/drivers/radeonsi/si_shader_internal.h
@@ -169,8 +169,8 @@ bool si_vs_needs_prolog(const struct si_shader_selector *sel,
 void si_get_vs_prolog_key(const struct si_shader_info *info, unsigned num_input_sgprs,
                           const struct si_vs_prolog_bits *prolog_key,
                           struct si_shader *shader_out, union si_shader_part_key *key);
-struct nir_shader *si_get_nir_shader(struct si_shader *shader, bool *free_nir,
-                                     uint64_t tcs_vgpr_only_inputs);
+struct nir_shader *si_get_nir_shader(struct si_shader *shader, struct si_shader_args *args,
+                                     bool *free_nir, uint64_t tcs_vgpr_only_inputs);
 void si_get_tcs_epilog_key(struct si_shader *shader, union si_shader_part_key *key);
 bool si_need_ps_prolog(const union si_shader_part_key *key);
 void si_get_ps_prolog_key(struct si_shader *shader, union si_shader_part_key *key,
@@ -189,6 +189,9 @@ void gfx10_ngg_gs_emit_begin(struct si_shader_context *ctx);
 unsigned gfx10_ngg_get_scratch_dw_size(struct si_shader *shader);
 bool gfx10_ngg_calculate_subgroup_info(struct si_shader *shader);
 
+/* si_nir_lower_abi.c */
+bool si_nir_lower_abi(nir_shader *nir, struct si_shader *shader, struct si_shader_args *args);
+
 /* si_shader_llvm.c */
 bool si_compile_llvm(struct si_screen *sscreen, struct si_shader_binary *binary,
                      struct ac_shader_config *conf, struct ac_llvm_compiler *compiler,
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index a7316ad54283..0258c430ad21 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -1350,7 +1350,7 @@ bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *
          shader_ls.is_monolithic = true;
 
          si_init_shader_args(&shader_ls, ctx.args);
-         nir = si_get_nir_shader(&shader_ls, &free_nir, sel->info.tcs_vgpr_only_inputs);
+         nir = si_get_nir_shader(&shader_ls, ctx.args, &free_nir, sel->info.tcs_vgpr_only_inputs);
          si_update_shader_binary_info(shader, nir);
 
          if (!si_llvm_translate_nir(&ctx, &shader_ls, nir, free_nir)) {
@@ -1422,7 +1422,7 @@ bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *
          shader_es.is_monolithic = true;
 
          si_init_shader_args(&shader_es, ctx.args);
-         nir = si_get_nir_shader(&shader_es, &free_nir, 0);
+         nir = si_get_nir_shader(&shader_es, ctx.args, &free_nir, 0);
          si_update_shader_binary_info(shader, nir);
 
          if (!si_llvm_translate_nir(&ctx, &shader_es, nir, free_nir)) {
-- 
GitLab


From fd1b57fa4a90b777fb8be6c8d6e5aa2d1d0c5156 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Wed, 10 Aug 2022 22:38:37 +0800
Subject: [PATCH 18/45] radeonsi: remove si_llvm_load_intrinsic intrinsics
 lowered
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 .../drivers/radeonsi/si_shader_internal.h     |   2 -
 src/gallium/drivers/radeonsi/si_shader_llvm.c | 174 ------------------
 .../drivers/radeonsi/si_shader_llvm_tess.c    |  24 +--
 3 files changed, 1 insertion(+), 199 deletions(-)

diff --git a/src/gallium/drivers/radeonsi/si_shader_internal.h b/src/gallium/drivers/radeonsi/si_shader_internal.h
index 4df29868f74e..494937e1cde2 100644
--- a/src/gallium/drivers/radeonsi/si_shader_internal.h
+++ b/src/gallium/drivers/radeonsi/si_shader_internal.h
@@ -242,8 +242,6 @@ void si_llvm_init_gs_callbacks(struct si_shader_context *ctx);
 
 /* si_shader_llvm_tess.c */
 LLVMValueRef si_get_rel_patch_id(struct si_shader_context *ctx);
-LLVMValueRef si_get_tcs_in_vertex_dw_stride(struct si_shader_context *ctx);
-LLVMValueRef si_get_num_tcs_out_vertices(struct si_shader_context *ctx);
 void si_llvm_preload_tess_rings(struct si_shader_context *ctx);
 void si_llvm_ls_build_end(struct si_shader_context *ctx);
 void si_llvm_build_tcs_epilog(struct si_shader_context *ctx, union si_shader_part_key *key);
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index 0258c430ad21..a04327a04620 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -700,36 +700,6 @@ void si_build_wrapper_function(struct si_shader_context *ctx, struct ac_llvm_poi
       LLVMBuildRet(builder, ret);
 }
 
-static LLVMValueRef si_get_num_vertices_per_prim(struct si_shader_context *ctx)
-{
-   const struct si_shader_info *info = &ctx->shader->selector->info;
-
-   unsigned num_vertices;
-   if (ctx->stage == MESA_SHADER_GEOMETRY) {
-      num_vertices = u_vertices_per_prim(info->base.gs.output_primitive);
-   } else if (ctx->stage == MESA_SHADER_VERTEX) {
-      if (info->base.vs.blit_sgprs_amd) {
-         num_vertices = 3;
-      } else if (ctx->shader->key.ge.opt.ngg_culling & SI_NGG_CULL_LINES) {
-         num_vertices = 2;
-      } else {
-         /* Extract OUTPRIM field. */
-         LLVMValueRef num = GET_FIELD(ctx, GS_STATE_OUTPRIM);
-         return LLVMBuildAdd(ctx->ac.builder, num, ctx->ac.i32_1, "");
-      }
-   } else {
-      assert(ctx->stage == MESA_SHADER_TESS_EVAL);
-
-      if (info->base.tess.point_mode)
-         num_vertices = 1;
-      else if (info->base.tess._primitive_mode == TESS_PRIMITIVE_ISOLINES)
-         num_vertices = 2;
-      else
-         num_vertices = 3;
-   }
-   return LLVMConstInt(ctx->ac.i32, num_vertices, false);
-}
-
 static LLVMValueRef si_llvm_build_attr_ring_desc(struct si_shader_context *ctx)
 {
    struct si_shader *shader = ctx->shader;
@@ -768,159 +738,15 @@ static LLVMValueRef si_llvm_load_intrinsic(struct ac_shader_abi *abi, nir_intrin
    struct si_shader_context *ctx = si_shader_context_from_abi(abi);
 
    switch (op) {
-   case nir_intrinsic_load_first_vertex:
-      return ac_get_arg(&ctx->ac, ctx->args->ac.base_vertex);
-
-   case nir_intrinsic_load_base_vertex: {
-      /* For non-indexed draws, the base vertex set by the driver
-       * (for direct draws) or the CP (for indirect draws) is the
-       * first vertex ID, but GLSL expects 0 to be returned.
-       */
-      LLVMValueRef indexed = GET_FIELD(ctx, VS_STATE_INDEXED);
-      indexed = LLVMBuildTrunc(ctx->ac.builder, indexed, ctx->ac.i1, "");
-      return LLVMBuildSelect(ctx->ac.builder, indexed, ac_get_arg(&ctx->ac, ctx->args->ac.base_vertex),
-                             ctx->ac.i32_0, "");
-   }
-
-   case nir_intrinsic_load_workgroup_size: {
-      assert(ctx->shader->selector->info.base.workgroup_size_variable &&
-             ctx->shader->selector->info.uses_variable_block_size);
-      LLVMValueRef chan[3] = {
-         si_unpack_param(ctx, ctx->args->block_size, 0, 10),
-         si_unpack_param(ctx, ctx->args->block_size, 10, 10),
-         si_unpack_param(ctx, ctx->args->block_size, 20, 10),
-      };
-      return ac_build_gather_values(&ctx->ac, chan, 3);
-   }
-
-   case nir_intrinsic_load_tess_level_outer_default:
-   case nir_intrinsic_load_tess_level_inner_default: {
-      LLVMValueRef slot = LLVMConstInt(ctx->ac.i32, SI_HS_CONST_DEFAULT_TESS_LEVELS, 0);
-      LLVMValueRef buf =
-         ac_build_load_to_sgpr(&ctx->ac,
-                               ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->internal_bindings),
-                               slot);
-      int offset = op == nir_intrinsic_load_tess_level_inner_default ? 4 : 0;
-      LLVMValueRef val[4];
-
-      for (int i = 0; i < 4; i++)
-         val[i] = si_buffer_load_const(ctx, buf, LLVMConstInt(ctx->ac.i32, (offset + i) * 4, 0));
-      return ac_build_gather_values(&ctx->ac, val, 4);
-   }
-
-   case nir_intrinsic_load_patch_vertices_in:
-      if (ctx->stage == MESA_SHADER_TESS_CTRL)
-         return si_unpack_param(ctx, ctx->args->tcs_out_lds_layout, 13, 6);
-      else if (ctx->stage == MESA_SHADER_TESS_EVAL)
-         return si_get_num_tcs_out_vertices(ctx);
-      else
-         return NULL;
-
-   case nir_intrinsic_load_sample_mask_in:
-      return ac_to_integer(&ctx->ac, ac_get_arg(&ctx->ac, ctx->args->ac.sample_coverage));
-
-   case nir_intrinsic_load_lshs_vertex_stride_amd:
-      return LLVMBuildShl(ctx->ac.builder, si_get_tcs_in_vertex_dw_stride(ctx),
-                          LLVMConstInt(ctx->ac.i32, 2, 0), "");
-
-   case nir_intrinsic_load_tcs_num_patches_amd:
-      return LLVMBuildAdd(ctx->ac.builder,
-                          si_unpack_param(ctx, ctx->args->tcs_offchip_layout, 0, 6),
-                          ctx->ac.i32_1, "");
-
-   case nir_intrinsic_load_hs_out_patch_data_offset_amd:
-      return si_unpack_param(ctx, ctx->args->tcs_offchip_layout, 11, 21);
-
    case nir_intrinsic_load_ring_tess_offchip_amd:
       return ctx->tess_offchip_ring;
 
-   case nir_intrinsic_load_ring_tess_offchip_offset_amd:
-      return ac_get_arg(&ctx->ac, ctx->args->ac.tess_offchip_offset);
-
    case nir_intrinsic_load_tess_rel_patch_id_amd:
       return si_get_rel_patch_id(ctx);
 
    case nir_intrinsic_load_ring_esgs_amd:
       return ctx->esgs_ring;
 
-   case nir_intrinsic_load_ring_es2gs_offset_amd:
-      return ac_get_arg(&ctx->ac, ctx->args->ac.es2gs_offset);
-
-   case nir_intrinsic_load_clip_half_line_width_amd: {
-      LLVMValueRef ptr = ac_get_arg(&ctx->ac, ctx->args->small_prim_cull_info);
-      return ac_build_load_to_sgpr(&ctx->ac,
-         (struct ac_llvm_pointer) { .t = ctx->ac.v2f32, .v = ptr }, LLVMConstInt(ctx->ac.i32, 4, 0));
-   }
-
-   case nir_intrinsic_load_viewport_xy_scale_and_offset: {
-      bool prim_is_lines = ctx->shader->key.ge.opt.ngg_culling & SI_NGG_CULL_LINES;
-      struct ac_llvm_pointer ptr = ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->small_prim_cull_info);
-      LLVMValueRef terms =
-         ac_build_load_to_sgpr(&ctx->ac, ptr, prim_is_lines ? ctx->ac.i32_1 : ctx->ac.i32_0);
-      return LLVMBuildBitCast(ctx->ac.builder, terms, ctx->ac.v4f32, "");
-   }
-
-   case nir_intrinsic_load_num_vertices_per_primitive_amd:
-      return si_get_num_vertices_per_prim(ctx);
-
-   case nir_intrinsic_load_cull_ccw_amd:
-      /* radeonsi embed cw/ccw info into front/back face enabled */
-      return ctx->ac.i1false;
-
-   case nir_intrinsic_load_cull_any_enabled_amd:
-      return ctx->shader->key.ge.opt.ngg_culling ? ctx->ac.i1true : ctx->ac.i1false;
-
-   case nir_intrinsic_load_cull_back_face_enabled_amd:
-      return ctx->shader->key.ge.opt.ngg_culling & SI_NGG_CULL_BACK_FACE ?
-         ctx->ac.i1true : ctx->ac.i1false;
-
-   case nir_intrinsic_load_cull_front_face_enabled_amd:
-      return ctx->shader->key.ge.opt.ngg_culling & SI_NGG_CULL_FRONT_FACE ?
-         ctx->ac.i1true : ctx->ac.i1false;
-
-   case nir_intrinsic_load_cull_small_prim_precision_amd: {
-      LLVMValueRef small_prim_precision =
-         ctx->shader->key.ge.opt.ngg_culling & SI_NGG_CULL_LINES ?
-         GET_FIELD(ctx, GS_STATE_SMALL_PRIM_PRECISION_NO_AA) :
-         GET_FIELD(ctx, GS_STATE_SMALL_PRIM_PRECISION);
-
-      /* Extract the small prim precision. */
-      small_prim_precision =
-         LLVMBuildOr(ctx->ac.builder, small_prim_precision,
-                     LLVMConstInt(ctx->ac.i32, 0x70, 0), "");
-      small_prim_precision =
-         LLVMBuildShl(ctx->ac.builder, small_prim_precision,
-                      LLVMConstInt(ctx->ac.i32, 23, 0), "");
-
-      return LLVMBuildBitCast(ctx->ac.builder, small_prim_precision, ctx->ac.f32, "");
-   }
-
-   case nir_intrinsic_load_cull_small_primitives_enabled_amd:
-      if (ctx->shader->key.ge.opt.ngg_culling & SI_NGG_CULL_LINES)
-         return ctx->shader->key.ge.opt.ngg_culling & SI_NGG_CULL_SMALL_LINES_DIAMOND_EXIT ?
-            ctx->ac.i1true : ctx->ac.i1false;
-      else
-         return ctx->ac.i1true;
-
-   case nir_intrinsic_load_provoking_vtx_in_prim_amd:
-      return GET_FIELD(ctx, GS_STATE_PROVOKING_VTX_INDEX);
-
-   case nir_intrinsic_load_pipeline_stat_query_enabled_amd: {
-      LLVMValueRef enabled = GET_FIELD(ctx, GS_STATE_PIPELINE_STATS_EMU);
-      return LLVMBuildTrunc(ctx->ac.builder, enabled, ctx->ac.i1, "");
-   }
-
-   case nir_intrinsic_load_prim_gen_query_enabled_amd:
-   case nir_intrinsic_load_prim_xfb_query_enabled_amd: {
-      LLVMValueRef enabled = GET_FIELD(ctx, GS_STATE_STREAMOUT_QUERY_ENABLED);
-      return LLVMBuildTrunc(ctx->ac.builder, enabled, ctx->ac.i1, "");
-   }
-
-   case nir_intrinsic_load_clamp_vertex_color_amd: {
-      LLVMValueRef enabled = GET_FIELD(ctx, VS_STATE_CLAMP_VERTEX_COLOR);
-      return LLVMBuildTrunc(ctx->ac.builder, enabled, ctx->ac.i1, "");
-   }
-
    case nir_intrinsic_load_ring_attr_amd:
       return si_llvm_build_attr_ring_desc(ctx);
 
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm_tess.c b/src/gallium/drivers/radeonsi/si_shader_llvm_tess.c
index c180ea6e5243..05076b4d5f7e 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm_tess.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm_tess.c
@@ -95,7 +95,7 @@ static LLVMValueRef get_tcs_out_current_patch_data_offset(struct si_shader_conte
    return ac_build_imad(&ctx->ac, patch_stride, rel_patch_id, patch0_patch_data_offset);
 }
 
-LLVMValueRef si_get_num_tcs_out_vertices(struct si_shader_context *ctx)
+static LLVMValueRef si_get_num_tcs_out_vertices(struct si_shader_context *ctx)
 {
    unsigned tcs_out_vertices =
       ctx->shader->selector ? ctx->shader->selector->info.base.tess.tcs_vertices_out
@@ -109,28 +109,6 @@ LLVMValueRef si_get_num_tcs_out_vertices(struct si_shader_context *ctx)
                        si_unpack_param(ctx, ctx->args->tcs_offchip_layout, 6, 5), ctx->ac.i32_1, "");
 }
 
-LLVMValueRef si_get_tcs_in_vertex_dw_stride(struct si_shader_context *ctx)
-{
-   unsigned stride;
-
-   switch (ctx->stage) {
-   case MESA_SHADER_VERTEX:
-      stride = ctx->shader->selector->info.lshs_vertex_stride / 4;
-      return LLVMConstInt(ctx->ac.i32, stride, 0);
-
-   case MESA_SHADER_TESS_CTRL:
-      if (ctx->screen->info.gfx_level >= GFX9 && ctx->shader->is_monolithic) {
-         stride = ctx->shader->key.ge.part.tcs.ls->info.lshs_vertex_stride / 4;
-         return LLVMConstInt(ctx->ac.i32, stride, 0);
-      }
-      return GET_FIELD(ctx, VS_STATE_LS_OUT_VERTEX_SIZE);
-
-   default:
-      assert(0);
-      return NULL;
-   }
-}
-
 /* The offchip buffer layout for TCS->TES is
  *
  * - attribute 0 of patch 0 vertex 0
-- 
GitLab


From 7c9cc4dde2b2930f891a2e1f68607434f52da9b2 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Wed, 10 Aug 2022 22:49:25 +0800
Subject: [PATCH 19/45] ac/llvm: remove lowered abi->intrinsic_load()
 intrinsics
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/llvm/ac_nir_to_llvm.c | 24 ------------------------
 1 file changed, 24 deletions(-)

diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index e035254f89fe..82c68e0c72d0 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -3615,36 +3615,12 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
    }
    case nir_intrinsic_load_base_vertex:
    case nir_intrinsic_load_first_vertex:
-   case nir_intrinsic_load_workgroup_size:
-   case nir_intrinsic_load_tess_level_outer_default:
-   case nir_intrinsic_load_tess_level_inner_default:
    case nir_intrinsic_load_tess_rel_patch_id_amd:
-   case nir_intrinsic_load_patch_vertices_in:
-   case nir_intrinsic_load_sample_mask_in:
-   case nir_intrinsic_load_viewport_xy_scale_and_offset:
    case nir_intrinsic_load_ring_tess_factors_amd:
    case nir_intrinsic_load_ring_tess_offchip_amd:
-   case nir_intrinsic_load_ring_tess_offchip_offset_amd:
    case nir_intrinsic_load_ring_esgs_amd:
-   case nir_intrinsic_load_ring_es2gs_offset_amd:
    case nir_intrinsic_load_ring_attr_amd:
    case nir_intrinsic_load_ring_gsvs_amd:
-   case nir_intrinsic_load_lshs_vertex_stride_amd:
-   case nir_intrinsic_load_tcs_num_patches_amd:
-   case nir_intrinsic_load_hs_out_patch_data_offset_amd:
-   case nir_intrinsic_load_clip_half_line_width_amd:
-   case nir_intrinsic_load_num_vertices_per_primitive_amd:
-   case nir_intrinsic_load_cull_ccw_amd:
-   case nir_intrinsic_load_cull_any_enabled_amd:
-   case nir_intrinsic_load_cull_back_face_enabled_amd:
-   case nir_intrinsic_load_cull_front_face_enabled_amd:
-   case nir_intrinsic_load_cull_small_prim_precision_amd:
-   case nir_intrinsic_load_cull_small_primitives_enabled_amd:
-   case nir_intrinsic_load_provoking_vtx_in_prim_amd:
-   case nir_intrinsic_load_pipeline_stat_query_enabled_amd:
-   case nir_intrinsic_load_prim_gen_query_enabled_amd:
-   case nir_intrinsic_load_prim_xfb_query_enabled_amd:
-   case nir_intrinsic_load_clamp_vertex_color_amd:
    case nir_intrinsic_load_lds_ngg_scratch_base_amd:
    case nir_intrinsic_load_lds_ngg_gs_out_vertex_base_amd:
       result = ctx->abi->intrinsic_load(ctx->abi, instr->intrinsic);
-- 
GitLab


From 834728d744c85b9a64fef551cdc81c29cedbe487 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Wed, 10 Aug 2022 23:28:11 +0800
Subject: [PATCH 20/45] ac/llvm,radeonsi: lower nir_load_user_clip_plane in abi
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/llvm/ac_nir_to_llvm.c                   |  3 ---
 src/amd/llvm/ac_shader_abi.h                    |  2 --
 src/gallium/drivers/radeonsi/si_nir_lower_abi.c |  6 ++++++
 src/gallium/drivers/radeonsi/si_shader_llvm.c   | 12 ------------
 4 files changed, 6 insertions(+), 17 deletions(-)

diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index 82c68e0c72d0..087a08b4a9b7 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -3625,9 +3625,6 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
    case nir_intrinsic_load_lds_ngg_gs_out_vertex_base_amd:
       result = ctx->abi->intrinsic_load(ctx->abi, instr->intrinsic);
       break;
-   case nir_intrinsic_load_user_clip_plane:
-      result = ctx->abi->load_user_clip_plane(ctx->abi, nir_intrinsic_ucp_id(instr));
-      break;
    case nir_intrinsic_load_streamout_buffer_amd:
       result = ctx->abi->load_streamout_buffer(ctx->abi, nir_intrinsic_base(instr));
       break;
diff --git a/src/amd/llvm/ac_shader_abi.h b/src/amd/llvm/ac_shader_abi.h
index 1268669a5f58..d20a69823296 100644
--- a/src/amd/llvm/ac_shader_abi.h
+++ b/src/amd/llvm/ac_shader_abi.h
@@ -116,8 +116,6 @@ struct ac_shader_abi {
 
    LLVMValueRef (*load_sample_position)(struct ac_shader_abi *abi, LLVMValueRef sample_id);
 
-   LLVMValueRef (*load_user_clip_plane)(struct ac_shader_abi *abi, unsigned ucp_id);
-
    LLVMValueRef (*load_streamout_buffer)(struct ac_shader_abi *abi, unsigned buffer);
 
    LLVMValueRef (*emit_fbfetch)(struct ac_shader_abi *abi);
diff --git a/src/gallium/drivers/radeonsi/si_nir_lower_abi.c b/src/gallium/drivers/radeonsi/si_nir_lower_abi.c
index dcc44ee2bdc7..d899b5b619fd 100644
--- a/src/gallium/drivers/radeonsi/si_nir_lower_abi.c
+++ b/src/gallium/drivers/radeonsi/si_nir_lower_abi.c
@@ -222,6 +222,12 @@ static bool lower_abi_instr(nir_builder *b, nir_instr *instr, struct lower_abi_s
    case nir_intrinsic_load_clamp_vertex_color_amd:
       replacement = nir_i2b(b, GET_FIELD_NIR(VS_STATE_CLAMP_VERTEX_COLOR));
       break;
+   case nir_intrinsic_load_user_clip_plane: {
+      nir_ssa_def *buf = load_internal_binding(b, args, SI_VS_CONST_CLIP_PLANES);
+      unsigned offset = nir_intrinsic_ucp_id(intrin) * 16;
+      replacement = nir_load_smem_buffer_amd(b, 4, buf, nir_imm_int(b, offset));
+      break;
+   }
    default:
       return false;
    }
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index a04327a04620..971ffd516d3d 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -761,17 +761,6 @@ static LLVMValueRef si_llvm_load_intrinsic(struct ac_shader_abi *abi, nir_intrin
    }
 }
 
-static LLVMValueRef si_llvm_load_user_clip_plane(struct ac_shader_abi *abi, unsigned ucp_id)
-{
-   struct si_shader_context *ctx = si_shader_context_from_abi(abi);
-   struct ac_llvm_pointer ptr = ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->internal_bindings);
-   LLVMValueRef constbuf_index = LLVMConstInt(ctx->ac.i32, SI_VS_CONST_CLIP_PLANES, 0);
-   LLVMValueRef const_resource = ac_build_load_to_sgpr(&ctx->ac, ptr, constbuf_index);
-   LLVMValueRef addr = LLVMConstInt(ctx->ac.i32, ucp_id * 16, 0);
-   return ac_build_buffer_load(&ctx->ac, const_resource, 4, NULL, addr, NULL,
-                               ctx->ac.f32, 0, true, true);
-}
-
 static LLVMValueRef si_llvm_load_streamout_buffer(struct ac_shader_abi *abi, unsigned buffer)
 {
    struct si_shader_context *ctx = si_shader_context_from_abi(abi);
@@ -797,7 +786,6 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
    ctx->num_images = info->base.num_images;
 
    ctx->abi.intrinsic_load = si_llvm_load_intrinsic;
-   ctx->abi.load_user_clip_plane = si_llvm_load_user_clip_plane;
    ctx->abi.load_streamout_buffer = si_llvm_load_streamout_buffer;
    ctx->abi.export_vertex = gfx10_ngg_export_vertex;
    ctx->abi.atomic_add_prim_count = gfx10_ngg_atomic_add_prim_count;
-- 
GitLab


From 501307aaf733b1d3cf8a9e9efa175e5760a43c5b Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Wed, 10 Aug 2022 23:34:25 +0800
Subject: [PATCH 21/45] ac/llvm,radeonsi: lower nir_load_streamout_buffer_amd
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/llvm/ac_nir_to_llvm.c                   |  3 ---
 src/amd/llvm/ac_shader_abi.h                    |  2 --
 src/gallium/drivers/radeonsi/si_nir_lower_abi.c |  5 +++++
 src/gallium/drivers/radeonsi/si_shader_llvm.c   | 10 ----------
 4 files changed, 5 insertions(+), 15 deletions(-)

diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index 087a08b4a9b7..b804e31b65e2 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -3625,9 +3625,6 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
    case nir_intrinsic_load_lds_ngg_gs_out_vertex_base_amd:
       result = ctx->abi->intrinsic_load(ctx->abi, instr->intrinsic);
       break;
-   case nir_intrinsic_load_streamout_buffer_amd:
-      result = ctx->abi->load_streamout_buffer(ctx->abi, nir_intrinsic_base(instr));
-      break;
    case nir_intrinsic_load_merged_wave_info_amd:
       result = ac_get_arg(&ctx->ac, ctx->args->merged_wave_info);
       break;
diff --git a/src/amd/llvm/ac_shader_abi.h b/src/amd/llvm/ac_shader_abi.h
index d20a69823296..36519b995f0e 100644
--- a/src/amd/llvm/ac_shader_abi.h
+++ b/src/amd/llvm/ac_shader_abi.h
@@ -116,8 +116,6 @@ struct ac_shader_abi {
 
    LLVMValueRef (*load_sample_position)(struct ac_shader_abi *abi, LLVMValueRef sample_id);
 
-   LLVMValueRef (*load_streamout_buffer)(struct ac_shader_abi *abi, unsigned buffer);
-
    LLVMValueRef (*emit_fbfetch)(struct ac_shader_abi *abi);
 
    LLVMValueRef (*intrinsic_load)(struct ac_shader_abi *abi, nir_intrinsic_op op);
diff --git a/src/gallium/drivers/radeonsi/si_nir_lower_abi.c b/src/gallium/drivers/radeonsi/si_nir_lower_abi.c
index d899b5b619fd..9af01f7d795d 100644
--- a/src/gallium/drivers/radeonsi/si_nir_lower_abi.c
+++ b/src/gallium/drivers/radeonsi/si_nir_lower_abi.c
@@ -228,6 +228,11 @@ static bool lower_abi_instr(nir_builder *b, nir_instr *instr, struct lower_abi_s
       replacement = nir_load_smem_buffer_amd(b, 4, buf, nir_imm_int(b, offset));
       break;
    }
+   case nir_intrinsic_load_streamout_buffer_amd: {
+      unsigned slot = SI_VS_STREAMOUT_BUF0 + nir_intrinsic_base(intrin);
+      replacement = load_internal_binding(b, args, slot);
+      break;
+   }
    default:
       return false;
    }
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index 971ffd516d3d..c1d91d4198ac 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -761,15 +761,6 @@ static LLVMValueRef si_llvm_load_intrinsic(struct ac_shader_abi *abi, nir_intrin
    }
 }
 
-static LLVMValueRef si_llvm_load_streamout_buffer(struct ac_shader_abi *abi, unsigned buffer)
-{
-   struct si_shader_context *ctx = si_shader_context_from_abi(abi);
-   struct ac_llvm_pointer buf_ptr = ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->internal_bindings);
-
-   return ac_build_load_to_sgpr(
-      &ctx->ac, buf_ptr, LLVMConstInt(ctx->ac.i32, SI_VS_STREAMOUT_BUF0 + buffer, false));
-}
-
 bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shader,
                            struct nir_shader *nir, bool free_nir)
 {
@@ -786,7 +777,6 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
    ctx->num_images = info->base.num_images;
 
    ctx->abi.intrinsic_load = si_llvm_load_intrinsic;
-   ctx->abi.load_streamout_buffer = si_llvm_load_streamout_buffer;
    ctx->abi.export_vertex = gfx10_ngg_export_vertex;
    ctx->abi.atomic_add_prim_count = gfx10_ngg_atomic_add_prim_count;
 
-- 
GitLab


From 32d4ac14e7a47548da7607bcfb9a4a4d60a14528 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Thu, 11 Aug 2022 10:17:16 +0800
Subject: [PATCH 22/45] nir,ac/llvm: add nir_buffer_atomic_add_amd
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Used by radeonsi for lower nir_atomic_add_gen/xfb_prim_count_amd.

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/llvm/ac_nir_to_llvm.c              | 20 ++++++++++++++++++++
 src/compiler/nir/nir_divergence_analysis.c |  1 +
 src/compiler/nir/nir_intrinsics.py         |  3 +++
 3 files changed, 24 insertions(+)

diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index b804e31b65e2..ea8607cd1f7e 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -4315,6 +4315,26 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
       ac_build_atomic_rmw(&ctx->ac, LLVMAtomicRMWBinOpAdd, gds_base, store_val, "workgroup-one-as");
       break;
    }
+   case nir_intrinsic_buffer_atomic_add_amd: {
+      LLVMValueRef desc = get_src(ctx, instr->src[0]);
+      LLVMValueRef data = get_src(ctx, instr->src[1]);
+      unsigned base = nir_intrinsic_base(instr);
+      LLVMTypeRef return_type = LLVMTypeOf(data);
+
+      LLVMValueRef args[] = {
+         data, desc,
+         LLVMConstInt(ctx->ac.i32, base, false),
+         ctx->ac.i32_0, /* soffset */
+         ctx->ac.i32_0, /* cachepolicy */
+      };
+
+      char name[64], type[8];
+      ac_build_type_name_for_intr(return_type, type, sizeof(type));
+      snprintf(name, sizeof(name), "llvm.amdgcn.raw.buffer.atomic.add.%s", type);
+
+      result = ac_build_intrinsic(&ctx->ac, name, return_type, args, 5, 0);
+      break;
+   }
    case nir_intrinsic_export_vertex_amd:
       ctx->abi->export_vertex(ctx->abi);
       break;
diff --git a/src/compiler/nir/nir_divergence_analysis.c b/src/compiler/nir/nir_divergence_analysis.c
index 9ad6e30811c0..dedc30eea7b4 100644
--- a/src/compiler/nir/nir_divergence_analysis.c
+++ b/src/compiler/nir/nir_divergence_analysis.c
@@ -649,6 +649,7 @@ visit_intrinsic(nir_shader *shader, nir_intrinsic_instr *instr)
    case nir_intrinsic_load_packed_passthrough_primitive_amd:
    case nir_intrinsic_load_initial_edgeflags_amd:
    case nir_intrinsic_gds_atomic_add_amd:
+   case nir_intrinsic_buffer_atomic_add_amd:
    case nir_intrinsic_load_rt_arg_scratch_offset_amd:
    case nir_intrinsic_load_intersection_opaque_amd:
    case nir_intrinsic_load_vector_arg_amd:
diff --git a/src/compiler/nir/nir_intrinsics.py b/src/compiler/nir/nir_intrinsics.py
index bc386b25faf6..10a18fe24ad6 100644
--- a/src/compiler/nir/nir_intrinsics.py
+++ b/src/compiler/nir/nir_intrinsics.py
@@ -1329,6 +1329,9 @@ store("global_amd", [1, 1], indices=[BASE, ACCESS, ALIGN_MUL, ALIGN_OFFSET, WRIT
 # Same as shared_atomic_add, but with GDS. src[] = {store_val, gds_addr, m0}
 intrinsic("gds_atomic_add_amd",  src_comp=[1, 1, 1], dest_comp=1, indices=[BASE])
 
+# src[] = { descriptor, add_value }
+intrinsic("buffer_atomic_add_amd", src_comp=[4, 1], dest_comp=1, indices=[BASE])
+
 # src[] = { sample_id, num_samples }
 intrinsic("load_sample_positions_amd", src_comp=[1, 1], dest_comp=2, flags=[CAN_ELIMINATE, CAN_REORDER])
 
-- 
GitLab


From 2bbc6714ae96599ee9a22921a1537eb216a5e940 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Thu, 11 Aug 2022 10:19:47 +0800
Subject: [PATCH 23/45] ac/llvm,radeonsi: lower nir primitive counter add
 intrinsics
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/llvm/ac_nir_to_llvm.c                 | 15 ----------
 src/amd/llvm/ac_shader_abi.h                  |  9 ------
 .../drivers/radeonsi/gfx10_shader_ngg.c       | 30 -------------------
 .../drivers/radeonsi/si_nir_lower_abi.c       | 20 +++++++++++++
 .../drivers/radeonsi/si_shader_internal.h     |  2 --
 src/gallium/drivers/radeonsi/si_shader_llvm.c |  1 -
 6 files changed, 20 insertions(+), 57 deletions(-)

diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index ea8607cd1f7e..538f64f4da14 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -4465,21 +4465,6 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
       result = ac_build_gather_values(&ctx->ac, global_count, instr->num_components);
       break;
    }
-   case nir_intrinsic_atomic_add_gs_emit_prim_count_amd:
-      ctx->abi->atomic_add_prim_count(ctx->abi, ~0U, get_src(ctx, instr->src[0]),
-                                      ac_prim_count_gs_emit);
-      break;
-   case nir_intrinsic_atomic_add_gen_prim_count_amd:
-   case nir_intrinsic_atomic_add_xfb_prim_count_amd: {
-      LLVMValueRef prim_count = get_src(ctx, instr->src[0]);
-      unsigned stream = nir_intrinsic_stream_id(instr);
-      enum ac_prim_count count_type =
-         instr->intrinsic == nir_intrinsic_atomic_add_gen_prim_count_amd ?
-         ac_prim_count_gen : ac_prim_count_xfb;
-
-      ctx->abi->atomic_add_prim_count(ctx->abi, stream, prim_count, count_type);
-      break;
-   }
    default:
       fprintf(stderr, "Unknown intrinsic: ");
       nir_print_instr(&instr->instr, stderr);
diff --git a/src/amd/llvm/ac_shader_abi.h b/src/amd/llvm/ac_shader_abi.h
index 36519b995f0e..83aadfba544d 100644
--- a/src/amd/llvm/ac_shader_abi.h
+++ b/src/amd/llvm/ac_shader_abi.h
@@ -34,12 +34,6 @@
 
 #define AC_LLVM_MAX_OUTPUTS (VARYING_SLOT_VAR31 + 1)
 
-enum ac_prim_count {
-   ac_prim_count_gs_emit,
-   ac_prim_count_gen,
-   ac_prim_count_xfb,
-};
-
 /* Document the shader ABI during compilation. This is what allows radeonsi and
  * radv to share a compiler backend.
  */
@@ -73,9 +67,6 @@ struct ac_shader_abi {
    void (*emit_vertex_with_counter)(struct ac_shader_abi *abi, unsigned stream,
                                     LLVMValueRef vertexidx, LLVMValueRef *addrs);
 
-   void (*atomic_add_prim_count)(struct ac_shader_abi *abi, unsigned stream,
-                                 LLVMValueRef prim_count, enum ac_prim_count count_type);
-
    LLVMValueRef (*load_inputs)(struct ac_shader_abi *abi,
                                unsigned driver_location, unsigned component,
                                unsigned num_components, unsigned vertex_index,
diff --git a/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c b/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
index d38c9453f7c6..17d0bea74418 100644
--- a/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
+++ b/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
@@ -124,36 +124,6 @@ void gfx10_ngg_export_vertex(struct ac_shader_abi *abi)
    si_llvm_build_vs_exports(ctx, outputs, num_outputs);
 }
 
-void gfx10_ngg_atomic_add_prim_count(struct ac_shader_abi *abi, unsigned stream,
-                                     LLVMValueRef prim_count, enum ac_prim_count count_type)
-{
-   struct si_shader_context *ctx = si_shader_context_from_abi(abi);
-
-   unsigned offset;
-   LLVMValueRef query_buf;
-   if (count_type == ac_prim_count_gs_emit) {
-      offset = si_query_pipestat_end_dw_offset(ctx->screen, PIPE_STAT_QUERY_GS_PRIMITIVES) * 4;
-      query_buf = ngg_get_emulated_counters_buf(ctx);
-   } else {
-      offset = count_type == ac_prim_count_gen ?
-         offsetof(struct gfx10_sh_query_buffer_mem, stream[stream].generated_primitives) :
-         offsetof(struct gfx10_sh_query_buffer_mem, stream[stream].emitted_primitives);
-
-      query_buf = ngg_get_query_buf(ctx);
-   }
-
-   LLVMValueRef args[] = {
-      prim_count,
-      query_buf,
-      LLVMConstInt(ctx->ac.i32, offset, false),
-      ctx->ac.i32_0, /* soffset */
-      ctx->ac.i32_0, /* cachepolicy */
-   };
-
-   ac_build_intrinsic(&ctx->ac, "llvm.amdgcn.raw.buffer.atomic.add.i32",
-                      ctx->ac.i32, args, 5, 0);
-}
-
 void gfx10_ngg_gs_emit_begin(struct si_shader_context *ctx)
 {
    LLVMBuilderRef builder = ctx->ac.builder;
diff --git a/src/gallium/drivers/radeonsi/si_nir_lower_abi.c b/src/gallium/drivers/radeonsi/si_nir_lower_abi.c
index 9af01f7d795d..1a5fe33136a6 100644
--- a/src/gallium/drivers/radeonsi/si_nir_lower_abi.c
+++ b/src/gallium/drivers/radeonsi/si_nir_lower_abi.c
@@ -233,6 +233,26 @@ static bool lower_abi_instr(nir_builder *b, nir_instr *instr, struct lower_abi_s
       replacement = load_internal_binding(b, args, slot);
       break;
    }
+   case nir_intrinsic_atomic_add_gs_emit_prim_count_amd:
+   case nir_intrinsic_atomic_add_gen_prim_count_amd:
+   case nir_intrinsic_atomic_add_xfb_prim_count_amd: {
+      unsigned offset;
+      nir_ssa_def *buf;
+      if (intrin->intrinsic == nir_intrinsic_atomic_add_gs_emit_prim_count_amd) {
+         buf = load_internal_binding(b, args, SI_GS_QUERY_EMULATED_COUNTERS_BUF);
+         offset = si_query_pipestat_end_dw_offset(sel->screen, PIPE_STAT_QUERY_GS_PRIMITIVES) * 4;
+      } else {
+         unsigned stream = nir_intrinsic_stream_id(intrin);
+         buf = load_internal_binding(b, args, SI_GS_QUERY_BUF);
+         offset = intrin->intrinsic == nir_intrinsic_atomic_add_gen_prim_count_amd ?
+            offsetof(struct gfx10_sh_query_buffer_mem, stream[stream].generated_primitives) :
+            offsetof(struct gfx10_sh_query_buffer_mem, stream[stream].emitted_primitives);
+      }
+
+      nir_ssa_def *prim_count = intrin->src[0].ssa;
+      nir_buffer_atomic_add_amd(b, 32, buf, prim_count, .base = offset);
+      break;
+   }
    default:
       return false;
    }
diff --git a/src/gallium/drivers/radeonsi/si_shader_internal.h b/src/gallium/drivers/radeonsi/si_shader_internal.h
index 494937e1cde2..93e3925ce25e 100644
--- a/src/gallium/drivers/radeonsi/si_shader_internal.h
+++ b/src/gallium/drivers/radeonsi/si_shader_internal.h
@@ -183,8 +183,6 @@ LLVMValueRef gfx10_get_thread_id_in_tg(struct si_shader_context *ctx);
 unsigned gfx10_ngg_get_vertices_per_prim(struct si_shader *shader);
 bool gfx10_ngg_export_prim_early(struct si_shader *shader);
 void gfx10_ngg_export_vertex(struct ac_shader_abi *abi);
-void gfx10_ngg_atomic_add_prim_count(struct ac_shader_abi *abi, unsigned stream,
-                                     LLVMValueRef prim_count, enum ac_prim_count count_type);
 void gfx10_ngg_gs_emit_begin(struct si_shader_context *ctx);
 unsigned gfx10_ngg_get_scratch_dw_size(struct si_shader *shader);
 bool gfx10_ngg_calculate_subgroup_info(struct si_shader *shader);
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index c1d91d4198ac..fc9ea32c1615 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -778,7 +778,6 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
 
    ctx->abi.intrinsic_load = si_llvm_load_intrinsic;
    ctx->abi.export_vertex = gfx10_ngg_export_vertex;
-   ctx->abi.atomic_add_prim_count = gfx10_ngg_atomic_add_prim_count;
 
    si_llvm_init_resource_callbacks(ctx);
    si_llvm_create_main_func(ctx);
-- 
GitLab


From bad77f6e5c1432cc3609ba3a95227959f447b12d Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Tue, 1 Nov 2022 15:52:53 +0800
Subject: [PATCH 24/45] ac/llvm,radeonsi: lower attribute ring intrinsics in
 nir
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/llvm/ac_nir_to_llvm.c                 |  6 ----
 .../drivers/radeonsi/si_nir_lower_abi.c       | 36 +++++++++++++++++++
 src/gallium/drivers/radeonsi/si_shader_llvm.c | 36 -------------------
 3 files changed, 36 insertions(+), 42 deletions(-)

diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index 538f64f4da14..53a080c545a4 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -3628,12 +3628,6 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
    case nir_intrinsic_load_merged_wave_info_amd:
       result = ac_get_arg(&ctx->ac, ctx->args->merged_wave_info);
       break;
-   case nir_intrinsic_load_ring_attr_offset_amd: {
-      LLVMValueRef offset = ac_get_arg(&ctx->ac, ctx->args->gs_attr_offset);
-      offset = ac_unpack_param(&ctx->ac, offset, 0, 15);
-      result = LLVMBuildShl(ctx->ac.builder, offset, LLVMConstInt(ctx->ac.i32, 9, false), "");
-      break;
-   }
    case nir_intrinsic_load_ordered_id_amd:
       result = ac_unpack_param(&ctx->ac, ac_get_arg(&ctx->ac, ctx->args->gs_tg_info), 0, 12);
       break;
diff --git a/src/gallium/drivers/radeonsi/si_nir_lower_abi.c b/src/gallium/drivers/radeonsi/si_nir_lower_abi.c
index 1a5fe33136a6..455ea09cbfdb 100644
--- a/src/gallium/drivers/radeonsi/si_nir_lower_abi.c
+++ b/src/gallium/drivers/radeonsi/si_nir_lower_abi.c
@@ -79,6 +79,34 @@ static nir_ssa_def *get_num_vert_per_prim(nir_builder *b, struct si_shader *shad
    return nir_imm_int(b, num_vertices);
 }
 
+static nir_ssa_def *build_attr_ring_desc(nir_builder *b, struct si_shader *shader,
+                                         struct si_shader_args *args)
+{
+   struct si_shader_selector *sel = shader->selector;
+
+   nir_ssa_def *attr_address =
+      sel->stage == MESA_SHADER_VERTEX && sel->info.base.vs.blit_sgprs_amd ?
+      load_internal_binding(b, args, SI_GS_ATTRIBUTE_RING) :
+      ac_nir_load_arg(b, &args->ac, args->gs_attr_address);
+
+   unsigned stride = 16 * shader->info.nr_param_exports;
+   nir_ssa_def *comp[] = {
+      attr_address,
+      nir_imm_int(b, S_008F04_BASE_ADDRESS_HI(sel->screen->info.address32_hi) |
+                  S_008F04_STRIDE(stride) |
+                  S_008F04_SWIZZLE_ENABLE_GFX11(3) /* 16B */),
+      nir_imm_int(b, 0xffffffff),
+      nir_imm_int(b, S_008F0C_DST_SEL_X(V_008F0C_SQ_SEL_X) |
+                  S_008F0C_DST_SEL_Y(V_008F0C_SQ_SEL_Y) |
+                  S_008F0C_DST_SEL_Z(V_008F0C_SQ_SEL_Z) |
+                  S_008F0C_DST_SEL_W(V_008F0C_SQ_SEL_W) |
+                  S_008F0C_FORMAT(V_008F0C_GFX11_FORMAT_32_32_32_32_FLOAT) |
+                  S_008F0C_INDEX_STRIDE(2) /* 32 elements */),
+   };
+
+   return nir_vec(b, comp, 4);
+}
+
 static bool lower_abi_instr(nir_builder *b, nir_instr *instr, struct lower_abi_state *s)
 {
    if (instr->type != nir_instr_type_intrinsic)
@@ -253,6 +281,14 @@ static bool lower_abi_instr(nir_builder *b, nir_instr *instr, struct lower_abi_s
       nir_buffer_atomic_add_amd(b, 32, buf, prim_count, .base = offset);
       break;
    }
+   case nir_intrinsic_load_ring_attr_amd:
+      replacement = build_attr_ring_desc(b, shader, args);
+      break;
+   case nir_intrinsic_load_ring_attr_offset_amd: {
+      nir_ssa_def *offset = ac_nir_unpack_arg(b, &args->ac, args->ac.gs_attr_offset, 0, 15);
+      replacement = nir_ishl_imm(b, offset, 9);
+      break;
+   }
    default:
       return false;
    }
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index fc9ea32c1615..f669040da12d 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -700,39 +700,6 @@ void si_build_wrapper_function(struct si_shader_context *ctx, struct ac_llvm_poi
       LLVMBuildRet(builder, ret);
 }
 
-static LLVMValueRef si_llvm_build_attr_ring_desc(struct si_shader_context *ctx)
-{
-   struct si_shader *shader = ctx->shader;
-
-   LLVMValueRef attr_address;
-   if (ctx->stage == MESA_SHADER_VERTEX && shader->selector->info.base.vs.blit_sgprs_amd) {
-      struct ac_llvm_pointer ring_ptr =
-         ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->internal_bindings);
-      ring_ptr.pointee_type = ctx->ac.i32;
-      attr_address = ac_build_load_to_sgpr(&ctx->ac, ring_ptr,
-                                           LLVMConstInt(ctx->ac.i32, SI_GS_ATTRIBUTE_RING * 4, 0));
-   } else {
-      attr_address = ac_get_arg(&ctx->ac, ctx->args->gs_attr_address);
-   }
-
-   unsigned stride = 16 * shader->info.nr_param_exports;
-   LLVMValueRef attr_desc[4] = {
-      attr_address,
-      LLVMConstInt(ctx->ac.i32, S_008F04_BASE_ADDRESS_HI(ctx->screen->info.address32_hi) |
-                   S_008F04_STRIDE(stride) |
-                   S_008F04_SWIZZLE_ENABLE_GFX11(3) /* 16B */, 0),
-      LLVMConstInt(ctx->ac.i32, 0xffffffff, 0),
-      LLVMConstInt(ctx->ac.i32, S_008F0C_DST_SEL_X(V_008F0C_SQ_SEL_X) |
-                   S_008F0C_DST_SEL_Y(V_008F0C_SQ_SEL_Y) |
-                   S_008F0C_DST_SEL_Z(V_008F0C_SQ_SEL_Z) |
-                   S_008F0C_DST_SEL_W(V_008F0C_SQ_SEL_W) |
-                   S_008F0C_FORMAT(V_008F0C_GFX11_FORMAT_32_32_32_32_FLOAT) |
-                   S_008F0C_INDEX_STRIDE(2) /* 32 elements */, 0),
-   };
-
-   return ac_build_gather_values(&ctx->ac, attr_desc, 4);
-}
-
 static LLVMValueRef si_llvm_load_intrinsic(struct ac_shader_abi *abi, nir_intrinsic_op op)
 {
    struct si_shader_context *ctx = si_shader_context_from_abi(abi);
@@ -747,9 +714,6 @@ static LLVMValueRef si_llvm_load_intrinsic(struct ac_shader_abi *abi, nir_intrin
    case nir_intrinsic_load_ring_esgs_amd:
       return ctx->esgs_ring;
 
-   case nir_intrinsic_load_ring_attr_amd:
-      return si_llvm_build_attr_ring_desc(ctx);
-
    case nir_intrinsic_load_lds_ngg_scratch_base_amd:
       return LLVMBuildBitCast(ctx->ac.builder, ctx->gs_ngg_scratch.value, ctx->ac.i32, "");
 
-- 
GitLab


From 3c4b17dc5c9b41a7432ded325ea2b0a7f17a07d5 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Tue, 16 Aug 2022 18:29:03 +0800
Subject: [PATCH 26/45] radeonsi: add si_nir_lower_resource pass

Replace the load_ubo abi.

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/gallium/drivers/radeonsi/meson.build      |   1 +
 .../drivers/radeonsi/si_nir_lower_resource.c  | 143 ++++++++++++++++++
 src/gallium/drivers/radeonsi/si_shader.c      |   2 +
 .../drivers/radeonsi/si_shader_internal.h     |   4 +
 .../radeonsi/si_shader_llvm_resources.c       |  55 -------
 5 files changed, 150 insertions(+), 55 deletions(-)
 create mode 100644 src/gallium/drivers/radeonsi/si_nir_lower_resource.c

diff --git a/src/gallium/drivers/radeonsi/meson.build b/src/gallium/drivers/radeonsi/meson.build
index 18099fa28fe3..40e9f2542231 100644
--- a/src/gallium/drivers/radeonsi/meson.build
+++ b/src/gallium/drivers/radeonsi/meson.build
@@ -46,6 +46,7 @@ files_libradeonsi = files(
   'si_query.c',
   'si_query.h',
   'si_nir_lower_abi.c',
+  'si_nir_lower_resource.c',
   'si_nir_optim.c',
   'si_sdma_copy_image.c',
   'si_shader.c',
diff --git a/src/gallium/drivers/radeonsi/si_nir_lower_resource.c b/src/gallium/drivers/radeonsi/si_nir_lower_resource.c
new file mode 100644
index 000000000000..02149d97016c
--- /dev/null
+++ b/src/gallium/drivers/radeonsi/si_nir_lower_resource.c
@@ -0,0 +1,143 @@
+/*
+ * Copyright 2022 Advanced Micro Devices, Inc.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * on the rights to use, copy, modify, merge, publish, distribute, sub
+ * license, and/or sell copies of the Software, and to permit persons to whom
+ * the Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE AUTHOR(S) AND/OR THEIR SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+/*
+ * This lowering pass converts index based buffer/image/texture access to
+ * explicite descriptor based, which simplify the compiler backend translation.
+ *
+ * For example: load_ubo(1) -> load_ubo(vec4), where the vec4 is the buffer
+ * descriptor with index==1, so compiler backend don't need to do index-to-descriptor
+ * finding which is the most complicated part (move to nir now).
+ */
+
+#include "nir_builder.h"
+
+#include "ac_nir.h"
+#include "si_pipe.h"
+#include "si_shader_internal.h"
+#include "sid.h"
+
+struct lower_resource_state {
+   struct si_shader *shader;
+   struct si_shader_args *args;
+};
+
+static nir_ssa_def *load_ubo_desc_fast_path(nir_builder *b, nir_ssa_def *addr_lo,
+                                            struct si_shader_selector *sel)
+{
+   nir_ssa_def *addr_hi =
+      nir_imm_int(b, S_008F04_BASE_ADDRESS_HI(sel->screen->info.address32_hi));
+
+   uint32_t rsrc3 =
+      S_008F0C_DST_SEL_X(V_008F0C_SQ_SEL_X) | S_008F0C_DST_SEL_Y(V_008F0C_SQ_SEL_Y) |
+      S_008F0C_DST_SEL_Z(V_008F0C_SQ_SEL_Z) | S_008F0C_DST_SEL_W(V_008F0C_SQ_SEL_W);
+
+   if (sel->screen->info.gfx_level >= GFX11)
+      rsrc3 |= S_008F0C_FORMAT(V_008F0C_GFX11_FORMAT_32_FLOAT) |
+               S_008F0C_OOB_SELECT(V_008F0C_OOB_SELECT_RAW);
+   else if (sel->screen->info.gfx_level >= GFX10)
+      rsrc3 |= S_008F0C_FORMAT(V_008F0C_GFX10_FORMAT_32_FLOAT) |
+               S_008F0C_OOB_SELECT(V_008F0C_OOB_SELECT_RAW) | S_008F0C_RESOURCE_LEVEL(1);
+   else
+      rsrc3 |= S_008F0C_NUM_FORMAT(V_008F0C_BUF_NUM_FORMAT_FLOAT) |
+               S_008F0C_DATA_FORMAT(V_008F0C_BUF_DATA_FORMAT_32);
+
+   return nir_vec4(b, addr_lo, addr_hi, nir_imm_int(b, sel->info.constbuf0_num_slots * 16),
+                   nir_imm_int(b, rsrc3));
+}
+
+static nir_ssa_def *clamp_index(nir_builder *b, nir_ssa_def *index, unsigned max)
+{
+   if (util_is_power_of_two_or_zero(max))
+      return nir_iand_imm(b, index, max - 1);
+   else {
+      nir_ssa_def *clamp = nir_imm_int(b, max - 1);
+      nir_ssa_def *cond = nir_uge(b, clamp, index);
+      return nir_bcsel(b, cond, index, clamp);
+   }
+}
+
+static nir_ssa_def *load_ubo_desc(nir_builder *b, nir_ssa_def *index,
+                                  struct lower_resource_state *s)
+{
+   struct si_shader_selector *sel = s->shader->selector;
+
+   nir_ssa_def *addr = ac_nir_load_arg(b, &s->args->ac, s->args->const_and_shader_buffers);
+
+   if (sel->info.base.num_ubos == 1 && sel->info.base.num_ssbos == 0)
+      return load_ubo_desc_fast_path(b, addr, sel);
+
+   index = clamp_index(b, index, sel->info.base.num_ubos);
+   index = nir_iadd_imm(b, index, SI_NUM_SHADER_BUFFERS);
+
+   nir_ssa_def *offset = nir_ishl_imm(b, index, 4);
+   return nir_load_smem_amd(b, 4, addr, offset);
+}
+
+static bool lower_resource_intrinsic(nir_builder *b, nir_intrinsic_instr *intrin,
+                                     struct lower_resource_state *s)
+{
+   switch (intrin->intrinsic) {
+   case nir_intrinsic_load_ubo: {
+      assert(!(nir_intrinsic_access(intrin) & ACCESS_NON_UNIFORM));
+
+      nir_ssa_def *desc = load_ubo_desc(b, intrin->src[0].ssa, s);
+      nir_instr_rewrite_src_ssa(&intrin->instr, &intrin->src[0], desc);
+      break;
+   }
+   default:
+      return false;
+   }
+
+   return true;
+}
+
+static bool lower_resource_instr(nir_builder *b, nir_instr *instr, void *state)
+{
+   struct lower_resource_state *s = (struct lower_resource_state *)state;
+
+   b->cursor = nir_before_instr(instr);
+
+   switch (instr->type) {
+   case nir_instr_type_intrinsic: {
+      nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
+      return lower_resource_intrinsic(b, intrin, s);
+   }
+   default:
+      return false;
+   }
+}
+
+bool si_nir_lower_resource(nir_shader *nir, struct si_shader *shader,
+                           struct si_shader_args *args)
+{
+   struct lower_resource_state state = {
+      .shader = shader,
+      .args = args,
+   };
+
+   return nir_shader_instructions_pass(nir, lower_resource_instr,
+                                       nir_metadata_dominance | nir_metadata_block_index,
+                                       &state);
+}
diff --git a/src/gallium/drivers/radeonsi/si_shader.c b/src/gallium/drivers/radeonsi/si_shader.c
index e21ab6700f71..7679b8cbfc41 100644
--- a/src/gallium/drivers/radeonsi/si_shader.c
+++ b/src/gallium/drivers/radeonsi/si_shader.c
@@ -1886,6 +1886,8 @@ struct nir_shader *si_get_nir_shader(struct si_shader *shader, struct si_shader_
    if (sel->stage == MESA_SHADER_FRAGMENT && key->ps.mono.point_smoothing)
       NIR_PASS(progress, nir, nir_lower_point_smooth);
 
+   NIR_PASS(progress, nir, si_nir_lower_resource, shader, args);
+
    bool is_last_vgt_stage =
       (sel->stage == MESA_SHADER_VERTEX ||
        sel->stage == MESA_SHADER_TESS_EVAL ||
diff --git a/src/gallium/drivers/radeonsi/si_shader_internal.h b/src/gallium/drivers/radeonsi/si_shader_internal.h
index 93e3925ce25e..3a0ad27b6c25 100644
--- a/src/gallium/drivers/radeonsi/si_shader_internal.h
+++ b/src/gallium/drivers/radeonsi/si_shader_internal.h
@@ -190,6 +190,10 @@ bool gfx10_ngg_calculate_subgroup_info(struct si_shader *shader);
 /* si_nir_lower_abi.c */
 bool si_nir_lower_abi(nir_shader *nir, struct si_shader *shader, struct si_shader_args *args);
 
+/* si_nir_lower_resource.c */
+bool si_nir_lower_resource(nir_shader *nir, struct si_shader *shader,
+                           struct si_shader_args *args);
+
 /* si_shader_llvm.c */
 bool si_compile_llvm(struct si_screen *sscreen, struct si_shader_binary *binary,
                      struct ac_shader_config *conf, struct ac_llvm_compiler *compiler,
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm_resources.c b/src/gallium/drivers/radeonsi/si_shader_llvm_resources.c
index 3fcbe3b34d37..2af7bea471b6 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm_resources.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm_resources.c
@@ -53,60 +53,6 @@ static LLVMValueRef si_llvm_bound_index(struct si_shader_context *ctx, LLVMValue
    return index;
 }
 
-static LLVMValueRef load_const_buffer_desc_fast_path(struct si_shader_context *ctx)
-{
-   LLVMValueRef ptr = ac_get_arg(&ctx->ac, ctx->args->const_and_shader_buffers);
-   struct si_shader_selector *sel = ctx->shader->selector;
-
-   /* Do the bounds checking with a descriptor, because
-    * doing computation and manual bounds checking of 64-bit
-    * addresses generates horrible VALU code with very high
-    * VGPR usage and very low SIMD occupancy.
-    */
-   ptr = LLVMBuildPtrToInt(ctx->ac.builder, ptr, ctx->ac.intptr, "");
-
-   LLVMValueRef desc0, desc1;
-   desc0 = ptr;
-   desc1 = LLVMConstInt(ctx->ac.i32, S_008F04_BASE_ADDRESS_HI(ctx->screen->info.address32_hi), 0);
-
-   uint32_t rsrc3 = S_008F0C_DST_SEL_X(V_008F0C_SQ_SEL_X) | S_008F0C_DST_SEL_Y(V_008F0C_SQ_SEL_Y) |
-                    S_008F0C_DST_SEL_Z(V_008F0C_SQ_SEL_Z) | S_008F0C_DST_SEL_W(V_008F0C_SQ_SEL_W);
-
-   if (ctx->screen->info.gfx_level >= GFX11)
-      rsrc3 |= S_008F0C_FORMAT(V_008F0C_GFX11_FORMAT_32_FLOAT) |
-               S_008F0C_OOB_SELECT(V_008F0C_OOB_SELECT_RAW);
-   else if (ctx->screen->info.gfx_level >= GFX10)
-      rsrc3 |= S_008F0C_FORMAT(V_008F0C_GFX10_FORMAT_32_FLOAT) |
-               S_008F0C_OOB_SELECT(V_008F0C_OOB_SELECT_RAW) | S_008F0C_RESOURCE_LEVEL(1);
-   else
-      rsrc3 |= S_008F0C_NUM_FORMAT(V_008F0C_BUF_NUM_FORMAT_FLOAT) |
-               S_008F0C_DATA_FORMAT(V_008F0C_BUF_DATA_FORMAT_32);
-
-   LLVMValueRef desc_elems[] = {desc0, desc1,
-                                LLVMConstInt(ctx->ac.i32, sel->info.constbuf0_num_slots * 16, 0),
-                                LLVMConstInt(ctx->ac.i32, rsrc3, false)};
-
-   return ac_build_gather_values(&ctx->ac, desc_elems, 4);
-}
-
-static LLVMValueRef load_ubo(struct ac_shader_abi *abi, LLVMValueRef index)
-{
-   struct si_shader_context *ctx = si_shader_context_from_abi(abi);
-   struct si_shader_selector *sel = ctx->shader->selector;
-
-   if (sel->info.base.num_ubos == 1 && sel->info.base.num_ssbos == 0) {
-      return load_const_buffer_desc_fast_path(ctx);
-   }
-
-   index = si_llvm_bound_index(ctx, index, ctx->num_const_buffers);
-   index =
-      LLVMBuildAdd(ctx->ac.builder, index, LLVMConstInt(ctx->ac.i32, SI_NUM_SHADER_BUFFERS, 0), "");
-
-   return ac_build_load_to_sgpr(&ctx->ac,
-                                ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->const_and_shader_buffers),
-                                index);
-}
-
 static LLVMValueRef load_ssbo(struct ac_shader_abi *abi, LLVMValueRef index, bool write, bool non_uniform)
 {
    struct si_shader_context *ctx = si_shader_context_from_abi(abi);
@@ -335,7 +281,6 @@ static LLVMValueRef si_nir_load_sampler_desc(struct ac_shader_abi *abi, unsigned
 
 void si_llvm_init_resource_callbacks(struct si_shader_context *ctx)
 {
-   ctx->abi.load_ubo = load_ubo;
    ctx->abi.load_ssbo = load_ssbo;
    ctx->abi.load_sampler_desc = si_nir_load_sampler_desc;
 }
-- 
GitLab


From b67caeb16945b7321950fc592877568789e67a4d Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Tue, 16 Aug 2022 18:47:03 +0800
Subject: [PATCH 27/45] ac/llvm: check load_ssbo present before call it

radeonsi will remove it later.

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/llvm/ac_nir_to_llvm.c | 16 ++++++++++++----
 1 file changed, 12 insertions(+), 4 deletions(-)

diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index 53a080c545a4..59a7c540ea29 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -1750,7 +1750,11 @@ static LLVMValueRef visit_get_ssbo_size(struct ac_nir_context *ctx,
                                         const nir_intrinsic_instr *instr)
 {
    bool non_uniform = nir_intrinsic_access(instr) & ACCESS_NON_UNIFORM;
-   LLVMValueRef rsrc = ctx->abi->load_ssbo(ctx->abi, get_src(ctx, instr->src[0]), false, non_uniform);
+
+   LLVMValueRef rsrc = get_src(ctx, instr->src[0]);
+   if (ctx->abi->load_ssbo)
+      rsrc = ctx->abi->load_ssbo(ctx->abi, rsrc, false, non_uniform);
+
    return LLVMBuildExtractElement(ctx->ac.builder, rsrc, LLVMConstInt(ctx->ac.i32, 2, false), "");
 }
 
@@ -1824,7 +1828,9 @@ static void visit_store_ssbo(struct ac_nir_context *ctx, nir_intrinsic_instr *in
    struct waterfall_context wctx;
    LLVMValueRef rsrc_base = enter_waterfall_ssbo(ctx, &wctx, instr, instr->src[1]);
 
-   LLVMValueRef rsrc = ctx->abi->load_ssbo(ctx->abi, rsrc_base, true, false);
+   LLVMValueRef rsrc = ctx->abi->load_ssbo ?
+      ctx->abi->load_ssbo(ctx->abi, rsrc_base, true, false) : rsrc_base;
+
    LLVMValueRef base_data = src_data;
    base_data = ac_trim_vector(&ctx->ac, base_data, instr->num_components);
    LLVMValueRef base_offset = get_src(ctx, instr->src[2]);
@@ -2022,7 +2028,8 @@ static LLVMValueRef visit_atomic_ssbo(struct ac_nir_context *ctx, nir_intrinsic_
       abort();
    }
 
-   descriptor = ctx->abi->load_ssbo(ctx->abi, rsrc_base, true, false);
+   descriptor = ctx->abi->load_ssbo ?
+      ctx->abi->load_ssbo(ctx->abi, rsrc_base, true, false) : rsrc_base;
 
    if (instr->intrinsic == nir_intrinsic_ssbo_atomic_comp_swap && return_type == ctx->ac.i64) {
       result = emit_ssbo_comp_swap_64(ctx, descriptor, get_src(ctx, instr->src[1]),
@@ -2072,7 +2079,8 @@ static LLVMValueRef visit_load_buffer(struct ac_nir_context *ctx, nir_intrinsic_
    unsigned cache_policy = get_cache_policy(ctx, access, false, false);
 
    LLVMValueRef offset = get_src(ctx, instr->src[1]);
-   LLVMValueRef rsrc = ctx->abi->load_ssbo(ctx->abi, rsrc_base, false, false);
+   LLVMValueRef rsrc = ctx->abi->load_ssbo ?
+      ctx->abi->load_ssbo(ctx->abi, rsrc_base, false, false) : rsrc_base;
    LLVMValueRef vindex = ctx->ac.i32_0;
 
    LLVMTypeRef def_type = get_def_type(ctx, &instr->dest.ssa);
-- 
GitLab


From 21e74613610f26a756d16535759fe968ed9b2cbe Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Wed, 17 Aug 2022 17:40:59 +0800
Subject: [PATCH 28/45] radeonsi: replace llvm load_ssbo abi with nir lower

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 .../drivers/radeonsi/si_nir_lower_resource.c  | 55 +++++++++++++++++++
 .../radeonsi/si_shader_llvm_resources.c       | 19 -------
 2 files changed, 55 insertions(+), 19 deletions(-)

diff --git a/src/gallium/drivers/radeonsi/si_nir_lower_resource.c b/src/gallium/drivers/radeonsi/si_nir_lower_resource.c
index 02149d97016c..b61042511218 100644
--- a/src/gallium/drivers/radeonsi/si_nir_lower_resource.c
+++ b/src/gallium/drivers/radeonsi/si_nir_lower_resource.c
@@ -95,6 +95,26 @@ static nir_ssa_def *load_ubo_desc(nir_builder *b, nir_ssa_def *index,
    return nir_load_smem_amd(b, 4, addr, offset);
 }
 
+static nir_ssa_def *load_ssbo_desc(nir_builder *b, nir_src *index,
+                                   struct lower_resource_state *s)
+{
+   struct si_shader_selector *sel = s->shader->selector;
+
+   /* Fast path if the shader buffer is in user SGPRs. */
+   if (nir_src_is_const(*index)) {
+      unsigned slot = nir_src_as_uint(*index);
+      if (slot < sel->cs_num_shaderbufs_in_user_sgprs)
+         return ac_nir_load_arg(b, &s->args->ac, s->args->cs_shaderbuf[slot]);
+   }
+
+   nir_ssa_def *addr = ac_nir_load_arg(b, &s->args->ac, s->args->const_and_shader_buffers);
+   nir_ssa_def *slot = clamp_index(b, index->ssa, sel->info.base.num_ssbos);
+   slot = nir_isub(b, nir_imm_int(b, SI_NUM_SHADER_BUFFERS - 1), slot);
+
+   nir_ssa_def *offset = nir_ishl_imm(b, slot, 4);
+   return nir_load_smem_amd(b, 4, addr, offset);
+}
+
 static bool lower_resource_intrinsic(nir_builder *b, nir_intrinsic_instr *intrin,
                                      struct lower_resource_state *s)
 {
@@ -106,6 +126,41 @@ static bool lower_resource_intrinsic(nir_builder *b, nir_intrinsic_instr *intrin
       nir_instr_rewrite_src_ssa(&intrin->instr, &intrin->src[0], desc);
       break;
    }
+   case nir_intrinsic_load_ssbo:
+   case nir_intrinsic_ssbo_atomic_add:
+   case nir_intrinsic_ssbo_atomic_imin:
+   case nir_intrinsic_ssbo_atomic_umin:
+   case nir_intrinsic_ssbo_atomic_fmin:
+   case nir_intrinsic_ssbo_atomic_imax:
+   case nir_intrinsic_ssbo_atomic_umax:
+   case nir_intrinsic_ssbo_atomic_fmax:
+   case nir_intrinsic_ssbo_atomic_and:
+   case nir_intrinsic_ssbo_atomic_or:
+   case nir_intrinsic_ssbo_atomic_xor:
+   case nir_intrinsic_ssbo_atomic_exchange:
+   case nir_intrinsic_ssbo_atomic_comp_swap: {
+      assert(!(nir_intrinsic_access(intrin) & ACCESS_NON_UNIFORM));
+
+      nir_ssa_def *desc = load_ssbo_desc(b, &intrin->src[0], s);
+      nir_instr_rewrite_src_ssa(&intrin->instr, &intrin->src[0], desc);
+      break;
+   }
+   case nir_intrinsic_store_ssbo: {
+      assert(!(nir_intrinsic_access(intrin) & ACCESS_NON_UNIFORM));
+
+      nir_ssa_def *desc = load_ssbo_desc(b, &intrin->src[1], s);
+      nir_instr_rewrite_src_ssa(&intrin->instr, &intrin->src[1], desc);
+      break;
+   }
+   case nir_intrinsic_get_ssbo_size: {
+      assert(!(nir_intrinsic_access(intrin) & ACCESS_NON_UNIFORM));
+
+      nir_ssa_def *desc = load_ssbo_desc(b, &intrin->src[0], s);
+      nir_ssa_def *size = nir_channel(b, desc, 2);
+      nir_ssa_def_rewrite_uses(&intrin->dest.ssa, size);
+      nir_instr_remove(&intrin->instr);
+      break;
+   }
    default:
       return false;
    }
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm_resources.c b/src/gallium/drivers/radeonsi/si_shader_llvm_resources.c
index 2af7bea471b6..86daf419cc8f 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm_resources.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm_resources.c
@@ -53,24 +53,6 @@ static LLVMValueRef si_llvm_bound_index(struct si_shader_context *ctx, LLVMValue
    return index;
 }
 
-static LLVMValueRef load_ssbo(struct ac_shader_abi *abi, LLVMValueRef index, bool write, bool non_uniform)
-{
-   struct si_shader_context *ctx = si_shader_context_from_abi(abi);
-
-   /* Fast path if the shader buffer is in user SGPRs. */
-   if (LLVMIsConstant(index) &&
-       LLVMConstIntGetZExtValue(index) < ctx->shader->selector->cs_num_shaderbufs_in_user_sgprs)
-      return ac_get_arg(&ctx->ac, ctx->args->cs_shaderbuf[LLVMConstIntGetZExtValue(index)]);
-
-   index = si_llvm_bound_index(ctx, index, ctx->num_shader_buffers);
-   index = LLVMBuildSub(ctx->ac.builder, LLVMConstInt(ctx->ac.i32, SI_NUM_SHADER_BUFFERS - 1, 0),
-                        index, "");
-
-   return ac_build_load_to_sgpr(&ctx->ac,
-                                ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->const_and_shader_buffers),
-                                index);
-}
-
 /**
  * Given a 256-bit resource descriptor, force the DCC enable bit to off.
  *
@@ -281,6 +263,5 @@ static LLVMValueRef si_nir_load_sampler_desc(struct ac_shader_abi *abi, unsigned
 
 void si_llvm_init_resource_callbacks(struct si_shader_context *ctx)
 {
-   ctx->abi.load_ssbo = load_ssbo;
    ctx->abi.load_sampler_desc = si_nir_load_sampler_desc;
 }
-- 
GitLab


From 97322c1cf50a5e02d67ccf53e8533a52e0b5eae2 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Wed, 7 Sep 2022 17:53:33 +0800
Subject: [PATCH 29/45] nir: add image fragment load intrinsics

Like nir_texop_fragment_mask_fetch_amd and nir_texop_fragment_fetch_amd,
this is used to load multi sample image fmask and color data for AMD GPU.

We will lower multi sample image load and samples_identical intrisics
to these ones latter for radeonsi. RADV does not need this because it
always expand fmask images before dispatch compute shader.

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/compiler/nir/nir.c                        |  2 ++
 src/compiler/nir/nir.h                        | 22 ++++++++++++++-----
 src/compiler/nir/nir_divergence_analysis.c    |  6 +++++
 src/compiler/nir/nir_group_loads.c            |  6 +++++
 src/compiler/nir/nir_intrinsics.py            |  6 +++++
 src/compiler/nir/nir_lower_mediump.c          |  3 +++
 .../nir/nir_lower_non_uniform_access.c        |  6 +++++
 src/compiler/nir/nir_opt_non_uniform_access.c |  6 +++++
 8 files changed, 51 insertions(+), 6 deletions(-)

diff --git a/src/compiler/nir/nir.c b/src/compiler/nir/nir.c
index 5983274496c9..4d62a93d9020 100644
--- a/src/compiler/nir/nir.c
+++ b/src/compiler/nir/nir.c
@@ -2725,6 +2725,8 @@ nir_rewrite_image_intrinsic(nir_intrinsic_instr *intrin, nir_ssa_def *src,
    CASE(samples)
    CASE(load_raw_intel)
    CASE(store_raw_intel)
+   CASE(fragment_load_amd)
+   CASE(fragment_mask_load_amd)
 #undef CASE
    default:
       unreachable("Unhanded image intrinsic");
diff --git a/src/compiler/nir/nir.h b/src/compiler/nir/nir.h
index a8990f85dbae..39cea28dea1e 100644
--- a/src/compiler/nir/nir.h
+++ b/src/compiler/nir/nir.h
@@ -1992,21 +1992,31 @@ nir_intrinsic_can_reorder(nir_intrinsic_instr *instr)
    if (nir_intrinsic_has_access(instr) &&
        nir_intrinsic_access(instr) & ACCESS_VOLATILE)
       return false;
-   if (instr->intrinsic == nir_intrinsic_load_deref) {
+
+   switch (instr->intrinsic) {
+   case nir_intrinsic_load_deref: {
       nir_deref_instr *deref = nir_src_as_deref(instr->src[0]);
       return nir_deref_mode_is_in_set(deref, nir_var_read_only_modes) ||
              (nir_intrinsic_access(instr) & ACCESS_CAN_REORDER);
-   } else if (instr->intrinsic == nir_intrinsic_load_ssbo ||
-              instr->intrinsic == nir_intrinsic_bindless_image_load ||
-              instr->intrinsic == nir_intrinsic_image_deref_load ||
-              instr->intrinsic == nir_intrinsic_image_load) {
+   }
+   case nir_intrinsic_load_ssbo:
+   case nir_intrinsic_bindless_image_load:
+   case nir_intrinsic_image_deref_load:
+   case nir_intrinsic_image_load:
+   case nir_intrinsic_image_fragment_load_amd:
+   case nir_intrinsic_image_deref_fragment_load_amd:
+   case nir_intrinsic_bindless_image_fragment_load_amd:
+   case nir_intrinsic_image_fragment_mask_load_amd:
+   case nir_intrinsic_image_deref_fragment_mask_load_amd:
+   case nir_intrinsic_bindless_image_fragment_mask_load_amd:
       return nir_intrinsic_access(instr) & ACCESS_CAN_REORDER;
-   } else {
+   default: {
       const nir_intrinsic_info *info =
          &nir_intrinsic_infos[instr->intrinsic];
       return (info->flags & NIR_INTRINSIC_CAN_ELIMINATE) &&
              (info->flags & NIR_INTRINSIC_CAN_REORDER);
    }
+   }
 }
 
 bool nir_intrinsic_writes_external_memory(const nir_intrinsic_instr *instr);
diff --git a/src/compiler/nir/nir_divergence_analysis.c b/src/compiler/nir/nir_divergence_analysis.c
index dedc30eea7b4..bcc5ec2d0288 100644
--- a/src/compiler/nir/nir_divergence_analysis.c
+++ b/src/compiler/nir/nir_divergence_analysis.c
@@ -352,6 +352,9 @@ visit_intrinsic(nir_shader *shader, nir_intrinsic_instr *instr)
    case nir_intrinsic_image_samples_identical:
    case nir_intrinsic_image_deref_samples_identical:
    case nir_intrinsic_bindless_image_samples_identical:
+   case nir_intrinsic_image_fragment_mask_load_amd:
+   case nir_intrinsic_image_deref_fragment_mask_load_amd:
+   case nir_intrinsic_bindless_image_fragment_mask_load_amd:
       is_divergent = (instr->src[0].ssa->divergent && (nir_intrinsic_access(instr) & ACCESS_NON_UNIFORM)) ||
                      instr->src[1].ssa->divergent;
       break;
@@ -362,6 +365,9 @@ visit_intrinsic(nir_shader *shader, nir_intrinsic_instr *instr)
    case nir_intrinsic_image_sparse_load:
    case nir_intrinsic_image_deref_sparse_load:
    case nir_intrinsic_bindless_image_sparse_load:
+   case nir_intrinsic_image_fragment_load_amd:
+   case nir_intrinsic_image_deref_fragment_load_amd:
+   case nir_intrinsic_bindless_image_fragment_load_amd:
       is_divergent = (instr->src[0].ssa->divergent && (nir_intrinsic_access(instr) & ACCESS_NON_UNIFORM)) ||
                      instr->src[1].ssa->divergent || instr->src[2].ssa->divergent || instr->src[3].ssa->divergent;
       break;
diff --git a/src/compiler/nir/nir_group_loads.c b/src/compiler/nir/nir_group_loads.c
index b45f9f320bb1..89b042cabb71 100644
--- a/src/compiler/nir/nir_group_loads.c
+++ b/src/compiler/nir/nir_group_loads.c
@@ -91,6 +91,12 @@ get_intrinsic_resource(nir_intrinsic_instr *intr)
    case nir_intrinsic_bindless_image_load:
    case nir_intrinsic_bindless_image_sparse_load:
    case nir_intrinsic_load_ssbo:
+   case nir_intrinsic_image_fragment_load_amd:
+   case nir_intrinsic_image_deref_fragment_load_amd:
+   case nir_intrinsic_bindless_image_fragment_load_amd:
+   case nir_intrinsic_image_fragment_mask_load_amd:
+   case nir_intrinsic_image_deref_fragment_mask_load_amd:
+   case nir_intrinsic_bindless_image_fragment_mask_load_amd:
       return intr->src[0].ssa->parent_instr;
    default:
       return NULL;
diff --git a/src/compiler/nir/nir_intrinsics.py b/src/compiler/nir/nir_intrinsics.py
index 10a18fe24ad6..6fd68cce9667 100644
--- a/src/compiler/nir/nir_intrinsics.py
+++ b/src/compiler/nir/nir_intrinsics.py
@@ -668,6 +668,12 @@ image("descriptor_amd", dest_comp=0, src_comp=[], flags=[CAN_ELIMINATE, CAN_REOR
 # CL-specific format queries
 image("format", dest_comp=1, flags=[CAN_ELIMINATE, CAN_REORDER])
 image("order", dest_comp=1, flags=[CAN_ELIMINATE, CAN_REORDER])
+# Multisample fragment mask load
+# src_comp[0] is same as image load src_comp[0]
+image("fragment_mask_load_amd", src_comp=[4], dest_comp=1, bit_sizes=[32], flags=[CAN_ELIMINATE])
+# Multisample fragment color load
+# src_comp is same as image load src_comp
+image("fragment_load_amd", src_comp=[4, 1, 1], extra_indices=[DEST_TYPE], dest_comp=0, flags=[CAN_ELIMINATE])
 
 # Vulkan descriptor set intrinsics
 #
diff --git a/src/compiler/nir/nir_lower_mediump.c b/src/compiler/nir/nir_lower_mediump.c
index 7f870a6b3b11..8da6f5967c02 100644
--- a/src/compiler/nir/nir_lower_mediump.c
+++ b/src/compiler/nir/nir_lower_mediump.c
@@ -1019,6 +1019,9 @@ fold_16bit_tex_image(nir_builder *b, nir_instr *instr, void *params)
       case nir_intrinsic_bindless_image_load:
       case nir_intrinsic_image_deref_load:
       case nir_intrinsic_image_load:
+      case nir_intrinsic_image_fragment_load_amd:
+      case nir_intrinsic_image_deref_fragment_load_amd:
+      case nir_intrinsic_bindless_image_fragment_load_amd:
          if (options->fold_image_load_store_data)
             progress |= fold_16bit_load_data(b, intrinsic, exec_mode, options->rounding_mode);
          if (options->fold_image_srcs)
diff --git a/src/compiler/nir/nir_lower_non_uniform_access.c b/src/compiler/nir/nir_lower_non_uniform_access.c
index 02ad2a6ade15..068203e72ea7 100644
--- a/src/compiler/nir/nir_lower_non_uniform_access.c
+++ b/src/compiler/nir/nir_lower_non_uniform_access.c
@@ -311,6 +311,12 @@ nir_lower_non_uniform_access_impl(nir_function_impl *impl,
             case nir_intrinsic_image_deref_size:
             case nir_intrinsic_image_deref_samples:
             case nir_intrinsic_image_deref_samples_identical:
+            case nir_intrinsic_image_fragment_load_amd:
+            case nir_intrinsic_image_deref_fragment_load_amd:
+            case nir_intrinsic_bindless_image_fragment_load_amd:
+            case nir_intrinsic_image_fragment_mask_load_amd:
+            case nir_intrinsic_image_deref_fragment_mask_load_amd:
+            case nir_intrinsic_bindless_image_fragment_mask_load_amd:
                if ((options->types & nir_lower_non_uniform_image_access) &&
                    lower_non_uniform_access_intrin(options, &b, intrin, 0))
                   progress = true;
diff --git a/src/compiler/nir/nir_opt_non_uniform_access.c b/src/compiler/nir/nir_opt_non_uniform_access.c
index 72713306a535..8a1b674ee6db 100644
--- a/src/compiler/nir/nir_opt_non_uniform_access.c
+++ b/src/compiler/nir/nir_opt_non_uniform_access.c
@@ -115,6 +115,12 @@ is_image_intrinsic(nir_intrinsic_instr *intrin)
    case nir_intrinsic_image_deref_atomic_fmax:
    case nir_intrinsic_image_deref_size:
    case nir_intrinsic_image_deref_samples:
+   case nir_intrinsic_image_fragment_load_amd:
+   case nir_intrinsic_image_deref_fragment_load_amd:
+   case nir_intrinsic_bindless_image_fragment_load_amd:
+   case nir_intrinsic_image_fragment_mask_load_amd:
+   case nir_intrinsic_image_deref_fragment_mask_load_amd:
+   case nir_intrinsic_bindless_image_fragment_mask_load_amd:
       return true;
 
    default:
-- 
GitLab


From 002aa3760908e5d3d47bc5746b149e63dc0910d1 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Thu, 8 Sep 2022 14:43:39 +0800
Subject: [PATCH 30/45] nir: lower image add lower_to_fragment_load_amd option

Like lower_to_fragment_fetch_amd option in lower tex,
this is for radeonsi to lower MS image ops.

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/compiler/nir/nir.h             |   6 ++
 src/compiler/nir/nir_lower_image.c | 100 +++++++++++++++++++++++++++++
 2 files changed, 106 insertions(+)

diff --git a/src/compiler/nir/nir.h b/src/compiler/nir/nir.h
index 39cea28dea1e..5ace6a7f05b2 100644
--- a/src/compiler/nir/nir.h
+++ b/src/compiler/nir/nir.h
@@ -5281,6 +5281,12 @@ typedef struct nir_lower_image_options {
     * If true, lower cube size operations.
     */
    bool lower_cube_size;
+
+   /**
+    * Lower multi sample image load to fragment_mask_load and fragment_load and
+    * samples_identical to fragment_mask_fetch.
+    */
+   bool lower_to_fragment_load_amd;
 } nir_lower_image_options;
 
 bool nir_lower_image(nir_shader *nir,
diff --git a/src/compiler/nir/nir_lower_image.c b/src/compiler/nir/nir_lower_image.c
index 5dab8f5be9b5..548d7424a52e 100644
--- a/src/compiler/nir/nir_lower_image.c
+++ b/src/compiler/nir/nir_lower_image.c
@@ -61,6 +61,86 @@ lower_cube_size(nir_builder *b, nir_intrinsic_instr *intrin)
    nir_instr_free(&intrin->instr);
 }
 
+static void
+lower_image_to_fragment_load(nir_builder *b, nir_intrinsic_instr *intrin)
+{
+   b->cursor = nir_before_instr(&intrin->instr);
+
+   nir_intrinsic_op fmask_op, color_op;
+   switch (intrin->intrinsic) {
+   case nir_intrinsic_image_load:
+      fmask_op = nir_intrinsic_image_fragment_mask_load_amd;
+      color_op = nir_intrinsic_image_fragment_load_amd;
+      break;
+   case nir_intrinsic_image_deref_load:
+      fmask_op = nir_intrinsic_image_deref_fragment_mask_load_amd;
+      color_op = nir_intrinsic_image_deref_fragment_load_amd;
+      break;
+   case nir_intrinsic_bindless_image_load:
+      fmask_op = nir_intrinsic_bindless_image_fragment_mask_load_amd;
+      color_op = nir_intrinsic_bindless_image_fragment_load_amd;
+      break;
+   default:
+      unreachable("bad intrinsic");
+      break;
+   }
+
+   nir_ssa_def *fmask =
+      nir_image_fragment_mask_load_amd(b, intrin->src[0].ssa, intrin->src[1].ssa,
+                                       .image_dim = nir_intrinsic_image_dim(intrin),
+                                       .image_array = nir_intrinsic_image_array(intrin),
+                                       .format = nir_intrinsic_format(intrin),
+                                       .access = nir_intrinsic_access(intrin));
+
+   /* fix intrinsic op */
+   nir_intrinsic_instr *fmask_load = nir_instr_as_intrinsic(fmask->parent_instr);
+   fmask_load->intrinsic = fmask_op;
+
+   /* extract real color buffer index from fmask buffer */
+   nir_ssa_def *sample_index_old = intrin->src[2].ssa;
+   nir_ssa_def *fmask_offset = nir_ishl_imm(b, sample_index_old, 2);
+   nir_ssa_def *fmask_width = nir_imm_int(b, 4);
+   nir_ssa_def *sample_index_new =
+      nir_ubitfield_extract(b, fmask, fmask_offset, fmask_width);
+
+   /* fix color buffer load */
+   intrin->intrinsic = color_op;
+   nir_instr_rewrite_src_ssa(&intrin->instr, &intrin->src[2], sample_index_new);
+}
+
+static void
+lower_image_samples_identical_to_fragment_load(nir_builder *b, nir_intrinsic_instr *intrin)
+{
+   b->cursor = nir_before_instr(&intrin->instr);
+
+   nir_intrinsic_instr *fmask_load =
+      nir_instr_as_intrinsic(nir_instr_clone(b->shader, &intrin->instr));
+
+   switch (intrin->intrinsic) {
+   case nir_intrinsic_image_samples_identical:
+      fmask_load->intrinsic = nir_intrinsic_image_fragment_mask_load_amd;
+      break;
+   case nir_intrinsic_image_deref_samples_identical:
+      fmask_load->intrinsic = nir_intrinsic_image_deref_fragment_mask_load_amd;
+      break;
+   case nir_intrinsic_bindless_image_samples_identical:
+      fmask_load->intrinsic = nir_intrinsic_bindless_image_fragment_mask_load_amd;
+      break;
+   default:
+      unreachable("bad intrinsic");
+      break;
+   }
+
+   nir_ssa_dest_init(&fmask_load->instr, &fmask_load->dest, 1, 32, NULL);
+   nir_builder_instr_insert(b, &fmask_load->instr);
+
+   nir_ssa_def *samples_identical = nir_ieq_imm(b, &fmask_load->dest.ssa, 0);
+   nir_ssa_def_rewrite_uses(&intrin->dest.ssa, samples_identical);
+
+   nir_instr_remove(&intrin->instr);
+   nir_instr_free(&intrin->instr);
+}
+
 static bool
 lower_image_instr(nir_builder *b, nir_instr *instr, void *state)
 {
@@ -81,6 +161,26 @@ lower_image_instr(nir_builder *b, nir_instr *instr, void *state)
       }
       return false;
 
+   case nir_intrinsic_image_load:
+   case nir_intrinsic_image_deref_load:
+   case nir_intrinsic_bindless_image_load:
+      if (options->lower_to_fragment_load_amd &&
+          nir_intrinsic_image_dim(intrin) == GLSL_SAMPLER_DIM_MS) {
+         lower_image_to_fragment_load(b, intrin);
+         return true;
+      }
+      return false;
+
+   case nir_intrinsic_image_samples_identical:
+   case nir_intrinsic_image_deref_samples_identical:
+   case nir_intrinsic_bindless_image_samples_identical:
+      if (options->lower_to_fragment_load_amd &&
+          nir_intrinsic_image_dim(intrin) == GLSL_SAMPLER_DIM_MS) {
+         lower_image_samples_identical_to_fragment_load(b, intrin);
+         return true;
+      }
+      return false;
+
    default:
       return false;
    }
-- 
GitLab


From c33fb854423c0efa27d3020a730d38dca6d7afc9 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Fri, 16 Sep 2022 19:41:02 +0800
Subject: [PATCH 31/45] ac/llvm: add disable_aniso_single_level abi

RADV use dri option to enabled this for some apps, but it's
done in nir lower currently. I'm afraid it still needs this
option to handle the non-uniform case as desc is loaded in
llvm.

radeonsi always enable this for bind-textures.

radeonsi will lower all bind-textures to bindless-textures,
and only bind-textures use desc index, so add this abi for
bindless desc index path.

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/llvm/ac_nir_to_llvm.c | 7 ++++++-
 src/amd/llvm/ac_shader_abi.h  | 3 +++
 2 files changed, 9 insertions(+), 1 deletion(-)

diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index 59a7c540ea29..192db8c186bd 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -4712,9 +4712,14 @@ static void tex_fetch_ptrs(struct ac_nir_context *ctx, nir_tex_instr *instr,
          *res_ptr = ctx->abi->load_sampler_desc(ctx->abi, 0, 0, 0, texture_dynamic_handle,
                                                 main_descriptor, false, false, true);
 
-      if (samp_ptr && sampler_dynamic_handle)
+      if (samp_ptr && sampler_dynamic_handle) {
          *samp_ptr = ctx->abi->load_sampler_desc(ctx->abi, 0, 0, 0, sampler_dynamic_handle,
                                                  AC_DESC_SAMPLER, false, false, true);
+
+         if (ctx->abi->disable_aniso_single_level &&
+             instr->sampler_dim < GLSL_SAMPLER_DIM_RECT)
+            *samp_ptr = sici_fix_sampler_aniso(ctx, *res_ptr, *samp_ptr);
+      }
       return;
    }
 
diff --git a/src/amd/llvm/ac_shader_abi.h b/src/amd/llvm/ac_shader_abi.h
index 83aadfba544d..50305f93f1b5 100644
--- a/src/amd/llvm/ac_shader_abi.h
+++ b/src/amd/llvm/ac_shader_abi.h
@@ -135,6 +135,9 @@ struct ac_shader_abi {
     * waterfall to avoid incorrect rendering. */
    bool use_waterfall_for_divergent_tex_samplers;
 
+   /* Whether to disable anisotropic filtering. */
+   bool disable_aniso_single_level;
+
    /* Number of all interpolated inputs */
    unsigned num_interp;
 };
-- 
GitLab


From d17be46b4ea3e72c0c3b47d502d6c3738861bf3e Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Mon, 19 Sep 2022 12:24:51 +0800
Subject: [PATCH 32/45] nir: nir_rewrite_image_intrinsic set image dim and
 array

Should set intrinsic image dim and array from deref too.

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/compiler/nir/nir.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/src/compiler/nir/nir.c b/src/compiler/nir/nir.c
index 4d62a93d9020..aa0cccd324ca 100644
--- a/src/compiler/nir/nir.c
+++ b/src/compiler/nir/nir.c
@@ -2745,6 +2745,9 @@ nir_rewrite_image_intrinsic(nir_intrinsic_instr *intrin, nir_ssa_def *src,
    if (nir_intrinsic_has_dest_type(intrin))
       nir_intrinsic_set_dest_type(intrin, data_type);
 
+   nir_intrinsic_set_image_dim(intrin, glsl_get_sampler_dim(deref->type));
+   nir_intrinsic_set_image_array(intrin, glsl_sampler_type_is_array(deref->type));
+
    nir_instr_rewrite_src(&intrin->instr, &intrin->src[0],
                          nir_src_for_ssa(src));
 }
-- 
GitLab


From b376e5c757ca16191bb000eaedc8c6c99d9bf107 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Mon, 19 Sep 2022 15:33:10 +0800
Subject: [PATCH 33/45] nir/divergence_analysis: add missing intrinsics

Singed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/compiler/nir/nir_divergence_analysis.c | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/src/compiler/nir/nir_divergence_analysis.c b/src/compiler/nir/nir_divergence_analysis.c
index bcc5ec2d0288..8545a2c91e47 100644
--- a/src/compiler/nir/nir_divergence_analysis.c
+++ b/src/compiler/nir/nir_divergence_analysis.c
@@ -536,6 +536,8 @@ visit_intrinsic(nir_shader *shader, nir_intrinsic_instr *instr)
    case nir_intrinsic_image_atomic_fadd:
    case nir_intrinsic_image_atomic_fmin:
    case nir_intrinsic_image_atomic_fmax:
+   case nir_intrinsic_image_atomic_inc_wrap:
+   case nir_intrinsic_image_atomic_dec_wrap:
    case nir_intrinsic_bindless_image_atomic_add:
    case nir_intrinsic_bindless_image_atomic_imin:
    case nir_intrinsic_bindless_image_atomic_umin:
@@ -549,6 +551,8 @@ visit_intrinsic(nir_shader *shader, nir_intrinsic_instr *instr)
    case nir_intrinsic_bindless_image_atomic_fadd:
    case nir_intrinsic_bindless_image_atomic_fmin:
    case nir_intrinsic_bindless_image_atomic_fmax:
+   case nir_intrinsic_bindless_image_atomic_inc_wrap:
+   case nir_intrinsic_bindless_image_atomic_dec_wrap:
    case nir_intrinsic_shared_atomic_add:
    case nir_intrinsic_shared_atomic_imin:
    case nir_intrinsic_shared_atomic_umin:
-- 
GitLab


From 6b6149f055af4fa532b9ec5940294c6ddf9ed52e Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Thu, 8 Sep 2022 18:06:56 +0800
Subject: [PATCH 34/45] radeonsi: replace llvm resource code with nir lower

Port from ac_nir_to_llvm.c and si_shader_llvm_resource.c.

Due to need waterfall of llvm backend, we can't get bind-texture
descriptor directly in nir. So we keep load_sampler_desc abi only
for bind-texture index to desc.

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/llvm/ac_nir_to_llvm.c                 |  56 ++-
 src/gallium/drivers/radeonsi/meson.build      |   1 -
 .../drivers/radeonsi/si_nir_lower_resource.c  | 407 ++++++++++++++++++
 .../drivers/radeonsi/si_shader_internal.h     |   3 -
 src/gallium/drivers/radeonsi/si_shader_llvm.c |  61 ++-
 .../radeonsi/si_shader_llvm_resources.c       | 267 ------------
 src/gallium/drivers/radeonsi/si_shader_nir.c  |   6 +-
 7 files changed, 498 insertions(+), 303 deletions(-)
 delete mode 100644 src/gallium/drivers/radeonsi/si_shader_llvm_resources.c

diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index 192db8c186bd..7e9e8604fb24 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -2489,7 +2489,6 @@ static void get_image_coords(struct ac_nir_context *ctx, const nir_intrinsic_ins
       LLVMConstInt(ctx->ac.i32, 2, false),
       LLVMConstInt(ctx->ac.i32, 3, false),
    };
-   LLVMValueRef sample_index = NULL;
 
    int count;
    ASSERTED bool add_frag_pos =
@@ -2499,25 +2498,6 @@ static void get_image_coords(struct ac_nir_context *ctx, const nir_intrinsic_ins
    assert(!add_frag_pos && "Input attachments should be lowered by this point.");
    count = image_type_to_components_count(dim, is_array);
 
-   if (ctx->ac.gfx_level < GFX11 &&
-       is_ms && (instr->intrinsic == nir_intrinsic_image_deref_load ||
-                 instr->intrinsic == nir_intrinsic_bindless_image_load ||
-                 instr->intrinsic == nir_intrinsic_image_deref_sparse_load ||
-                 instr->intrinsic == nir_intrinsic_bindless_image_sparse_load)) {
-      LLVMValueRef fmask_load_address[3];
-
-      fmask_load_address[0] = LLVMBuildExtractElement(ctx->ac.builder, src0, masks[0], "");
-      fmask_load_address[1] = LLVMBuildExtractElement(ctx->ac.builder, src0, masks[1], "");
-      if (is_array)
-         fmask_load_address[2] = LLVMBuildExtractElement(ctx->ac.builder, src0, masks[2], "");
-      else
-         fmask_load_address[2] = NULL;
-
-      sample_index = ac_llvm_extract_elem(&ctx->ac, get_src(ctx, instr->src[2]), 0);
-      sample_index = adjust_sample_index_using_fmask(
-         &ctx->ac, fmask_load_address[0], fmask_load_address[1], fmask_load_address[2],
-         sample_index, get_image_descriptor(ctx, instr, dynamic_desc_index, AC_DESC_FMASK, false));
-   }
    if (count == 1 && !gfx9_1d) {
       if (instr->src[1].ssa->num_components)
          args->coords[0] = LLVMBuildExtractElement(ctx->ac.builder, src0, masks[0], "");
@@ -2581,8 +2561,8 @@ static void get_image_coords(struct ac_nir_context *ctx, const nir_intrinsic_ins
       }
 
       if (is_ms) {
-         if (!sample_index)
-            sample_index = ac_llvm_extract_elem(&ctx->ac, get_src(ctx, instr->src[2]), 0);
+         LLVMValueRef sample_index =
+            ac_llvm_extract_elem(&ctx->ac, get_src(ctx, instr->src[2]), 0);
          args->coords[count] = sample_index;
          count++;
       }
@@ -2651,7 +2631,7 @@ static LLVMValueRef visit_image_load(struct ac_nir_context *ctx, const nir_intri
 
       res = ac_trim_vector(&ctx->ac, res, instr->dest.ssa.num_components);
       res = ac_to_integer(&ctx->ac, res);
-   } else if (instr->intrinsic == nir_intrinsic_image_deref_samples_identical) {
+   } else if (instr->intrinsic == nir_intrinsic_bindless_image_fragment_mask_load_amd) {
       assert(ctx->ac.gfx_level < GFX11);
 
       args.opcode = ac_image_load;
@@ -2663,8 +2643,6 @@ static LLVMValueRef visit_image_load(struct ac_nir_context *ctx, const nir_intri
       args.a16 = ac_get_elem_bits(&ctx->ac, LLVMTypeOf(args.coords[0])) == 16;
 
       res = ac_build_image_opcode(&ctx->ac, &args);
-      res = LLVMBuildExtractElement(ctx->ac.builder, res, ctx->ac.i32_0, "");
-      res = LLVMBuildICmp(ctx->ac.builder, LLVMIntEQ, res, ctx->ac.i32_0, "");
    } else {
       bool level_zero = nir_src_is_const(instr->src[3]) && nir_src_as_uint(instr->src[3]) == 0;
 
@@ -3844,6 +3822,8 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
       break;
    case nir_intrinsic_bindless_image_load:
    case nir_intrinsic_bindless_image_sparse_load:
+   case nir_intrinsic_bindless_image_fragment_load_amd:
+   case nir_intrinsic_bindless_image_fragment_mask_load_amd:
       result = visit_image_load(ctx, instr, true);
       break;
    case nir_intrinsic_image_deref_load:
@@ -4641,6 +4621,8 @@ static void tex_fetch_ptrs(struct ac_nir_context *ctx, nir_tex_instr *instr,
                            LLVMValueRef *samp_ptr, LLVMValueRef *fmask_ptr,
                            bool divergent)
 {
+   bool texture_handle_divergent = false;
+   bool sampler_handle_divergent = false;
    LLVMValueRef texture_dynamic_handle = NULL;
    LLVMValueRef sampler_dynamic_handle = NULL;
    nir_deref_instr *texture_deref_instr = NULL;
@@ -4667,10 +4649,14 @@ static void tex_fetch_ptrs(struct ac_nir_context *ctx, nir_tex_instr *instr,
             else
                *samp_ptr = val;
          } else {
-            if (instr->src[i].src_type == nir_tex_src_texture_handle)
+            bool divergent = instr->src[i].src.ssa->divergent;
+            if (instr->src[i].src_type == nir_tex_src_texture_handle) {
                texture_dynamic_handle = val;
-            else
+               texture_handle_divergent = divergent;
+            } else {
                sampler_dynamic_handle = val;
+               sampler_handle_divergent = divergent;
+            }
          }
          break;
       }
@@ -4701,11 +4687,23 @@ static void tex_fetch_ptrs(struct ac_nir_context *ctx, nir_tex_instr *instr,
    }
 
    if (texture_dynamic_handle || sampler_dynamic_handle) {
+      /* instr->sampler_non_uniform and texture_non_uniform are always false in GLSL,
+       * but this can lead to unexpected behavior if texture/sampler index come from
+       * a vertex attribute.
+       * For instance, 2 consecutive draws using 2 different index values,
+       * could be squashed together by the hw - producing a single draw with
+       * non-dynamically uniform index.
+       * To avoid this, detect divergent indexing, and use enter_waterfall.
+       * See https://gitlab.freedesktop.org/mesa/mesa/-/issues/2253.
+       */
+
       /* descriptor handles given through nir_tex_src_{texture,sampler}_handle */
-      if (instr->texture_non_uniform)
+      if (instr->texture_non_uniform ||
+          (ctx->abi->use_waterfall_for_divergent_tex_samplers && texture_handle_divergent))
          texture_dynamic_handle = enter_waterfall(ctx, &wctx[0], texture_dynamic_handle, divergent);
 
-      if (instr->sampler_non_uniform)
+      if (instr->sampler_non_uniform ||
+         (ctx->abi->use_waterfall_for_divergent_tex_samplers && sampler_handle_divergent))
          sampler_dynamic_handle = enter_waterfall(ctx, &wctx[1], sampler_dynamic_handle, divergent);
 
       if (texture_dynamic_handle)
diff --git a/src/gallium/drivers/radeonsi/meson.build b/src/gallium/drivers/radeonsi/meson.build
index 40e9f2542231..2d37d56a7c05 100644
--- a/src/gallium/drivers/radeonsi/meson.build
+++ b/src/gallium/drivers/radeonsi/meson.build
@@ -56,7 +56,6 @@ files_libradeonsi = files(
   'si_shader_llvm.c',
   'si_shader_llvm_gs.c',
   'si_shader_llvm_ps.c',
-  'si_shader_llvm_resources.c',
   'si_shader_llvm_tess.c',
   'si_shader_llvm_vs.c',
   'si_shader_nir.c',
diff --git a/src/gallium/drivers/radeonsi/si_nir_lower_resource.c b/src/gallium/drivers/radeonsi/si_nir_lower_resource.c
index b61042511218..55135dc40a55 100644
--- a/src/gallium/drivers/radeonsi/si_nir_lower_resource.c
+++ b/src/gallium/drivers/radeonsi/si_nir_lower_resource.c
@@ -115,6 +115,168 @@ static nir_ssa_def *load_ssbo_desc(nir_builder *b, nir_src *index,
    return nir_load_smem_amd(b, 4, addr, offset);
 }
 
+static nir_ssa_def *fixup_image_desc(nir_builder *b, nir_ssa_def *rsrc, bool uses_store,
+                                     struct lower_resource_state *s)
+{
+   struct si_shader_selector *sel = s->shader->selector;
+   struct si_screen *screen = sel->screen;
+
+   /**
+    * Given a 256-bit resource descriptor, force the DCC enable bit to off.
+    *
+    * At least on Tonga, executing image stores on images with DCC enabled and
+    * non-trivial can eventually lead to lockups. This can occur when an
+    * application binds an image as read-only but then uses a shader that writes
+    * to it. The OpenGL spec allows almost arbitrarily bad behavior (including
+    * program termination) in this case, but it doesn't cost much to be a bit
+    * nicer: disabling DCC in the shader still leads to undefined results but
+    * avoids the lockup.
+    */
+   if (uses_store &&
+       screen->info.gfx_level <= GFX9 &&
+       screen->info.gfx_level >= GFX8) {
+      nir_ssa_def *tmp = nir_channel(b, rsrc, 6);
+      tmp = nir_iand_imm(b, tmp, C_008F28_COMPRESSION_EN);
+      rsrc = nir_vector_insert_imm(b, rsrc, tmp, 6);
+   }
+
+   if (!uses_store &&
+       screen->info.has_image_load_dcc_bug &&
+       screen->always_allow_dcc_stores) {
+      nir_ssa_def *tmp = nir_channel(b, rsrc, 6);
+      tmp = nir_iand_imm(b, tmp, C_00A018_WRITE_COMPRESS_ENABLE);
+      rsrc = nir_vector_insert_imm(b, rsrc, tmp, 6);
+   }
+
+   return rsrc;
+}
+
+/* AC_DESC_FMASK is handled exactly like AC_DESC_IMAGE. The caller should
+ * adjust "index" to point to FMASK.
+ */
+static nir_ssa_def *load_image_desc(nir_builder *b, nir_ssa_def *list, nir_ssa_def *index,
+                                    enum ac_descriptor_type desc_type, bool uses_store,
+                                    struct lower_resource_state *s)
+{
+   /* index is in uvec8 unit, convert to offset in bytes */
+   nir_ssa_def *offset = nir_ishl_imm(b, index, 5);
+
+   unsigned num_channels;
+   if (desc_type == AC_DESC_BUFFER) {
+      offset = nir_iadd_imm(b, offset, 16);
+      num_channels = 4;
+   } else {
+      assert(desc_type == AC_DESC_IMAGE || desc_type == AC_DESC_FMASK);
+      num_channels = 8;
+   }
+
+   nir_ssa_def *rsrc = nir_load_smem_amd(b, num_channels, list, offset);
+
+   if (desc_type == AC_DESC_IMAGE)
+      rsrc = fixup_image_desc(b, rsrc, uses_store, s);
+
+   return rsrc;
+}
+
+static nir_ssa_def *deref_to_index(nir_builder *b,
+                                   nir_deref_instr *deref,
+                                   unsigned max_slots,
+                                   nir_ssa_def **dynamic_index_ret,
+                                   unsigned *const_index_ret)
+{
+   unsigned const_index = 0;
+   nir_ssa_def *dynamic_index = NULL;
+   while (deref->deref_type != nir_deref_type_var) {
+      assert(deref->deref_type == nir_deref_type_array);
+      unsigned array_size = MAX2(glsl_get_aoa_size(deref->type), 1);
+
+      if (nir_src_is_const(deref->arr.index)) {
+         const_index += array_size * nir_src_as_uint(deref->arr.index);
+      } else {
+         nir_ssa_def *tmp = nir_imul_imm(b, deref->arr.index.ssa, array_size);
+         dynamic_index = dynamic_index ? nir_iadd(b, dynamic_index, tmp) : tmp;
+      }
+
+      deref = nir_deref_instr_parent(deref);
+   }
+
+   unsigned base_index = deref->var->data.binding;
+   const_index += base_index;
+
+   /* Redirect invalid resource indices to the first array element. */
+   if (const_index >= max_slots)
+      const_index = base_index;
+
+   nir_ssa_def *index = nir_imm_int(b, const_index);
+   if (dynamic_index) {
+      index = nir_iadd(b, dynamic_index, index);
+
+      /* From the GL_ARB_shader_image_load_store extension spec:
+       *
+       *    If a shader performs an image load, store, or atomic
+       *    operation using an image variable declared as an array,
+       *    and if the index used to select an individual element is
+       *    negative or greater than or equal to the size of the
+       *    array, the results of the operation are undefined but may
+       *    not lead to termination.
+       */
+      index = clamp_index(b, index, max_slots);
+   }
+
+   if (dynamic_index_ret)
+      *dynamic_index_ret = dynamic_index;
+   if (const_index_ret)
+      *const_index_ret = const_index;
+
+   return index;
+}
+
+static nir_ssa_def *load_deref_image_desc(nir_builder *b, nir_deref_instr *deref,
+                                          enum ac_descriptor_type desc_type, bool is_load,
+                                          struct lower_resource_state *s)
+{
+   unsigned const_index;
+   nir_ssa_def *dynamic_index;
+   nir_ssa_def *index = deref_to_index(b, deref, s->shader->selector->info.base.num_images,
+                                       &dynamic_index, &const_index);
+
+   nir_ssa_def *desc;
+   if (!dynamic_index && desc_type != AC_DESC_FMASK &&
+       const_index < s->shader->selector->cs_num_images_in_user_sgprs) {
+      /* Fast path if the image is in user SGPRs. */
+      desc = ac_nir_load_arg(b, &s->args->ac, s->args->cs_image[const_index]);
+
+      if (desc_type == AC_DESC_IMAGE)
+         desc = fixup_image_desc(b, desc, !is_load, s);
+   } else {
+      /* FMASKs are separate from images. */
+      if (desc_type == AC_DESC_FMASK)
+         index = nir_iadd_imm(b, index, SI_NUM_IMAGES);
+
+      index = nir_isub(b, nir_imm_int(b, SI_NUM_IMAGE_SLOTS - 1), index);
+
+      nir_ssa_def *list = ac_nir_load_arg(b, &s->args->ac, s->args->samplers_and_images);
+      desc = load_image_desc(b, list, index, desc_type, !is_load, s);
+   }
+
+   return desc;
+}
+
+static nir_ssa_def *load_bindless_image_desc(nir_builder *b, nir_ssa_def *index,
+                                             enum ac_descriptor_type desc_type, bool is_load,
+                                             struct lower_resource_state *s)
+{
+   /* Bindless image descriptors use 16-dword slots. */
+   index = nir_ishl_imm(b, index, 1);
+
+   /* FMASK is right after the image. */
+   if (desc_type == AC_DESC_FMASK)
+      index = nir_iadd_imm(b, index, 1);
+
+   nir_ssa_def *list = ac_nir_load_arg(b, &s->args->ac, s->args->bindless_samplers_and_images);
+   return load_image_desc(b, list, index, desc_type, !is_load, s);
+}
+
 static bool lower_resource_intrinsic(nir_builder *b, nir_intrinsic_instr *intrin,
                                      struct lower_resource_state *s)
 {
@@ -161,6 +323,105 @@ static bool lower_resource_intrinsic(nir_builder *b, nir_intrinsic_instr *intrin
       nir_instr_remove(&intrin->instr);
       break;
    }
+   case nir_intrinsic_image_deref_load:
+   case nir_intrinsic_image_deref_sparse_load:
+   case nir_intrinsic_image_deref_fragment_load_amd:
+   case nir_intrinsic_image_deref_fragment_mask_load_amd:
+   case nir_intrinsic_image_deref_store:
+   case nir_intrinsic_image_deref_atomic_add:
+   case nir_intrinsic_image_deref_atomic_imin:
+   case nir_intrinsic_image_deref_atomic_umin:
+   case nir_intrinsic_image_deref_atomic_fmin:
+   case nir_intrinsic_image_deref_atomic_imax:
+   case nir_intrinsic_image_deref_atomic_umax:
+   case nir_intrinsic_image_deref_atomic_fmax:
+   case nir_intrinsic_image_deref_atomic_and:
+   case nir_intrinsic_image_deref_atomic_or:
+   case nir_intrinsic_image_deref_atomic_xor:
+   case nir_intrinsic_image_deref_atomic_exchange:
+   case nir_intrinsic_image_deref_atomic_comp_swap:
+   case nir_intrinsic_image_deref_atomic_fadd:
+   case nir_intrinsic_image_deref_atomic_inc_wrap:
+   case nir_intrinsic_image_deref_atomic_dec_wrap:
+   case nir_intrinsic_image_deref_descriptor_amd: {
+      assert(!(nir_intrinsic_access(intrin) & ACCESS_NON_UNIFORM));
+
+      nir_deref_instr *deref = nir_src_as_deref(intrin->src[0]);
+
+      enum ac_descriptor_type desc_type;
+      if (intrin->intrinsic == nir_intrinsic_image_deref_fragment_mask_load_amd) {
+         desc_type = AC_DESC_FMASK;
+      } else {
+         enum glsl_sampler_dim dim = glsl_get_sampler_dim(deref->type);
+         desc_type = dim == GLSL_SAMPLER_DIM_BUF ? AC_DESC_BUFFER : AC_DESC_IMAGE;
+      }
+
+      bool is_load =
+         intrin->intrinsic == nir_intrinsic_image_deref_load ||
+         intrin->intrinsic == nir_intrinsic_image_deref_sparse_load ||
+         intrin->intrinsic == nir_intrinsic_image_deref_fragment_load_amd ||
+         intrin->intrinsic == nir_intrinsic_image_deref_fragment_mask_load_amd ||
+         intrin->intrinsic == nir_intrinsic_image_deref_descriptor_amd;
+
+      nir_ssa_def *desc = load_deref_image_desc(b, deref, desc_type, is_load, s);
+
+      if (intrin->intrinsic == nir_intrinsic_image_deref_descriptor_amd) {
+         nir_ssa_def_rewrite_uses(&intrin->dest.ssa, desc);
+         nir_instr_remove(&intrin->instr);
+      } else {
+         nir_rewrite_image_intrinsic(intrin, desc, true);
+      }
+      break;
+   }
+   case nir_intrinsic_bindless_image_load:
+   case nir_intrinsic_bindless_image_sparse_load:
+   case nir_intrinsic_bindless_image_fragment_load_amd:
+   case nir_intrinsic_bindless_image_fragment_mask_load_amd:
+   case nir_intrinsic_bindless_image_store:
+   case nir_intrinsic_bindless_image_atomic_add:
+   case nir_intrinsic_bindless_image_atomic_imin:
+   case nir_intrinsic_bindless_image_atomic_umin:
+   case nir_intrinsic_bindless_image_atomic_fmin:
+   case nir_intrinsic_bindless_image_atomic_imax:
+   case nir_intrinsic_bindless_image_atomic_umax:
+   case nir_intrinsic_bindless_image_atomic_fmax:
+   case nir_intrinsic_bindless_image_atomic_and:
+   case nir_intrinsic_bindless_image_atomic_or:
+   case nir_intrinsic_bindless_image_atomic_xor:
+   case nir_intrinsic_bindless_image_atomic_exchange:
+   case nir_intrinsic_bindless_image_atomic_comp_swap:
+   case nir_intrinsic_bindless_image_atomic_fadd:
+   case nir_intrinsic_bindless_image_atomic_inc_wrap:
+   case nir_intrinsic_bindless_image_atomic_dec_wrap: {
+      assert(!(nir_intrinsic_access(intrin) & ACCESS_NON_UNIFORM));
+
+      enum ac_descriptor_type desc_type;
+      if (intrin->intrinsic == nir_intrinsic_bindless_image_fragment_mask_load_amd) {
+         desc_type = AC_DESC_FMASK;
+      } else {
+         enum glsl_sampler_dim dim = nir_intrinsic_image_dim(intrin);
+         desc_type = dim == GLSL_SAMPLER_DIM_BUF ? AC_DESC_BUFFER : AC_DESC_IMAGE;
+      }
+
+      bool is_load =
+         intrin->intrinsic == nir_intrinsic_bindless_image_load ||
+         intrin->intrinsic == nir_intrinsic_bindless_image_sparse_load ||
+         intrin->intrinsic == nir_intrinsic_bindless_image_fragment_load_amd ||
+         intrin->intrinsic == nir_intrinsic_bindless_image_fragment_mask_load_amd ||
+         intrin->intrinsic == nir_intrinsic_bindless_image_descriptor_amd;
+
+      nir_ssa_def *index = nir_u2u32(b, intrin->src[0].ssa);
+
+      nir_ssa_def *desc = load_bindless_image_desc(b, index, desc_type, is_load, s);
+
+      if (intrin->intrinsic == nir_intrinsic_bindless_image_descriptor_amd) {
+         nir_ssa_def_rewrite_uses(&intrin->dest.ssa, desc);
+         nir_instr_remove(&intrin->instr);
+      } else {
+         nir_instr_rewrite_src(&intrin->instr, &intrin->src[0], nir_src_for_ssa(desc));
+      }
+      break;
+   }
    default:
       return false;
    }
@@ -168,6 +429,148 @@ static bool lower_resource_intrinsic(nir_builder *b, nir_intrinsic_instr *intrin
    return true;
 }
 
+static nir_ssa_def *load_sampler_desc(nir_builder *b, nir_ssa_def *list, nir_ssa_def *index,
+                                      enum ac_descriptor_type desc_type)
+{
+   /* index is in 16 dword unit, convert to offset in bytes */
+   nir_ssa_def *offset = nir_ishl_imm(b, index, 6);
+
+   unsigned num_channels = 0;
+   switch (desc_type) {
+   case AC_DESC_IMAGE:
+      /* The image is at [0:7]. */
+      num_channels = 8;
+      break;
+   case AC_DESC_BUFFER:
+      /* The buffer is in [4:7]. */
+      offset = nir_iadd_imm(b, offset, 16);
+      num_channels = 4;
+      break;
+   case AC_DESC_FMASK:
+      /* The FMASK is at [8:15]. */
+      offset = nir_iadd_imm(b, offset, 32);
+      num_channels = 8;
+      break;
+   case AC_DESC_SAMPLER:
+      /* The sampler state is at [12:15]. */
+      offset = nir_iadd_imm(b, offset, 48);
+      num_channels = 4;
+      break;
+   default:
+      unreachable("invalid desc type");
+      break;
+   }
+
+   return nir_load_smem_amd(b, num_channels, list, offset);
+}
+
+static nir_ssa_def *load_deref_sampler_desc(nir_builder *b, nir_deref_instr *deref,
+                                            enum ac_descriptor_type desc_type,
+                                            struct lower_resource_state *s,
+                                            bool return_descriptor)
+{
+   unsigned max_slots = BITSET_LAST_BIT(b->shader->info.textures_used);
+   nir_ssa_def *index = deref_to_index(b, deref, max_slots, NULL, NULL);
+   index = nir_iadd_imm(b, index, SI_NUM_IMAGE_SLOTS / 2);
+
+   /* return actual desc when required by caller */
+   if (return_descriptor) {
+      nir_ssa_def *list = ac_nir_load_arg(b, &s->args->ac, s->args->samplers_and_images);
+      return load_sampler_desc(b, list, index, desc_type);
+   }
+
+   /* Just use index here and let nir-to-llvm backend to translate to actual
+    * descriptor. This is because we need waterfall to handle none-dynamic-uniform
+    * index there.
+    */
+   return index;
+}
+
+static nir_ssa_def *load_bindless_sampler_desc(nir_builder *b, nir_ssa_def *index,
+                                               enum ac_descriptor_type desc_type,
+                                               struct lower_resource_state *s)
+{
+   nir_ssa_def *list = ac_nir_load_arg(b, &s->args->ac, s->args->bindless_samplers_and_images);
+
+   /* 64 bit to 32 bit */
+   index = nir_u2u32(b, index);
+
+   return load_sampler_desc(b, list, index, desc_type);
+}
+
+static bool lower_resource_tex(nir_builder *b, nir_tex_instr *tex,
+                               struct lower_resource_state *s)
+{
+   assert(!tex->texture_non_uniform && !tex->sampler_non_uniform);
+
+   nir_deref_instr *texture_deref = NULL;
+   nir_deref_instr *sampler_deref = NULL;
+   nir_ssa_def *texture_handle = NULL;
+   nir_ssa_def *sampler_handle = NULL;
+
+   for (unsigned i = 0; i < tex->num_srcs; i++) {
+      switch (tex->src[i].src_type) {
+      case nir_tex_src_texture_deref:
+         texture_deref = nir_src_as_deref(tex->src[i].src);
+         break;
+      case nir_tex_src_sampler_deref:
+         sampler_deref = nir_src_as_deref(tex->src[i].src);
+         break;
+      case nir_tex_src_texture_handle:
+         texture_handle = tex->src[i].src.ssa;
+         break;
+      case nir_tex_src_sampler_handle:
+         sampler_handle = tex->src[i].src.ssa;
+         break;
+      default:
+         break;
+      }
+   }
+
+   enum ac_descriptor_type desc_type;
+   if (tex->op == nir_texop_fragment_mask_fetch_amd)
+      desc_type = AC_DESC_FMASK;
+   else
+      desc_type = tex->sampler_dim == GLSL_SAMPLER_DIM_BUF ? AC_DESC_BUFFER : AC_DESC_IMAGE;
+
+   bool is_descriptor_op = tex->op == nir_texop_descriptor_amd;
+   nir_ssa_def *image = texture_deref ?
+      load_deref_sampler_desc(b, texture_deref, desc_type, s, is_descriptor_op) :
+      load_bindless_sampler_desc(b, texture_handle, desc_type, s);
+
+   nir_ssa_def *sampler = NULL;
+   if (sampler_deref)
+      sampler = load_deref_sampler_desc(b, sampler_deref, AC_DESC_SAMPLER, s, false);
+   else if (sampler_handle)
+      sampler = load_bindless_sampler_desc(b, sampler_handle, AC_DESC_SAMPLER, s);
+
+   if (is_descriptor_op) {
+      nir_ssa_def_rewrite_uses(&tex->dest.ssa, image);
+      nir_instr_remove(&tex->instr);
+   } else {
+      for (unsigned i = 0; i < tex->num_srcs; i++) {
+         switch (tex->src[i].src_type) {
+         case nir_tex_src_texture_deref:
+            tex->src[i].src_type = nir_tex_src_texture_handle;
+            FALLTHROUGH;
+         case nir_tex_src_texture_handle:
+            nir_instr_rewrite_src_ssa(&tex->instr, &tex->src[i].src, image);
+            break;
+         case nir_tex_src_sampler_deref:
+            tex->src[i].src_type = nir_tex_src_sampler_handle;
+            FALLTHROUGH;
+         case nir_tex_src_sampler_handle:
+            nir_instr_rewrite_src_ssa(&tex->instr, &tex->src[i].src, sampler);
+            break;
+         default:
+            break;
+         }
+      }
+   }
+
+   return true;
+}
+
 static bool lower_resource_instr(nir_builder *b, nir_instr *instr, void *state)
 {
    struct lower_resource_state *s = (struct lower_resource_state *)state;
@@ -179,6 +582,10 @@ static bool lower_resource_instr(nir_builder *b, nir_instr *instr, void *state)
       nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
       return lower_resource_intrinsic(b, intrin, s);
    }
+   case nir_instr_type_tex: {
+      nir_tex_instr *tex = nir_instr_as_tex(instr);
+      return lower_resource_tex(b, tex, s);
+   }
    default:
       return false;
    }
diff --git a/src/gallium/drivers/radeonsi/si_shader_internal.h b/src/gallium/drivers/radeonsi/si_shader_internal.h
index 3a0ad27b6c25..b3a78dc49849 100644
--- a/src/gallium/drivers/radeonsi/si_shader_internal.h
+++ b/src/gallium/drivers/radeonsi/si_shader_internal.h
@@ -258,9 +258,6 @@ void si_llvm_build_monolithic_ps(struct si_shader_context *ctx, struct si_shader
 void si_llvm_ps_build_end(struct si_shader_context *ctx);
 void si_llvm_init_ps_callbacks(struct si_shader_context *ctx);
 
-/* si_shader_llvm_resources.c */
-void si_llvm_init_resource_callbacks(struct si_shader_context *ctx);
-
 /* si_shader_llvm_vs.c */
 void si_llvm_clipvertex_to_clipdist(struct si_shader_context *ctx,
                                     struct ac_export_args clipdist[2], LLVMValueRef clipvertex[4]);
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index f669040da12d..5d9c655c6fe2 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -725,6 +725,64 @@ static LLVMValueRef si_llvm_load_intrinsic(struct ac_shader_abi *abi, nir_intrin
    }
 }
 
+static LLVMValueRef si_llvm_load_sampler_desc(struct ac_shader_abi *abi, unsigned descriptor_set,
+                                              unsigned base_index, unsigned constant_index,
+                                              LLVMValueRef dynamic_index,
+                                              enum ac_descriptor_type desc_type, bool image,
+                                              bool write, bool bindless)
+{
+   struct si_shader_context *ctx = si_shader_context_from_abi(abi);
+   LLVMBuilderRef builder = ctx->ac.builder;
+
+   /* always 0 for OpenGL */
+   assert(!descriptor_set);
+
+   /* all image and texture has been lowered to bindless one in nir */
+   assert(bindless);
+
+   if (dynamic_index && LLVMTypeOf(dynamic_index) == ctx->ac.i32) {
+      /* image desc has been lowered in nir, we only expect texture here */
+      assert(!image);
+
+      bool is_vec4 = false;
+      LLVMValueRef index = dynamic_index;
+
+      switch (desc_type) {
+      case AC_DESC_IMAGE:
+         /* The image is at [0:7]. */
+         index = LLVMBuildMul(builder, index, LLVMConstInt(ctx->ac.i32, 2, 0), "");
+         break;
+      case AC_DESC_BUFFER:
+         /* The buffer is in [4:7]. */
+         index = ac_build_imad(&ctx->ac, index, LLVMConstInt(ctx->ac.i32, 4, 0), ctx->ac.i32_1);
+         is_vec4 = true;
+         break;
+      case AC_DESC_FMASK:
+         /* The FMASK is at [8:15]. */
+         assert(ctx->screen->info.gfx_level < GFX11);
+         index = ac_build_imad(&ctx->ac, index, LLVMConstInt(ctx->ac.i32, 2, 0), ctx->ac.i32_1);
+         break;
+      case AC_DESC_SAMPLER:
+         /* The sampler state is at [12:15]. */
+         index = ac_build_imad(&ctx->ac, index, LLVMConstInt(ctx->ac.i32, 4, 0),
+                               LLVMConstInt(ctx->ac.i32, 3, 0));
+         is_vec4 = true;
+         break;
+      default:
+         unreachable("invalid desc");
+      }
+
+      struct ac_llvm_pointer list = {
+         .value = ac_get_arg(&ctx->ac, ctx->args->samplers_and_images),
+         .pointee_type = is_vec4 ? ctx->ac.v4i32 : ctx->ac.v8i32,
+      };
+
+      return ac_build_load_to_sgpr(&ctx->ac, list, index);
+   }
+
+   return dynamic_index;
+}
+
 bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shader,
                            struct nir_shader *nir, bool free_nir)
 {
@@ -742,8 +800,8 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
 
    ctx->abi.intrinsic_load = si_llvm_load_intrinsic;
    ctx->abi.export_vertex = gfx10_ngg_export_vertex;
+   ctx->abi.load_sampler_desc = si_llvm_load_sampler_desc;
 
-   si_llvm_init_resource_callbacks(ctx);
    si_llvm_create_main_func(ctx);
 
    if (ctx->stage <= MESA_SHADER_GEOMETRY &&
@@ -972,6 +1030,7 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
    ctx->abi.clamp_div_by_zero = ctx->screen->options.clamp_div_by_zero ||
                                 info->options & SI_PROFILE_CLAMP_DIV_BY_ZERO;
    ctx->abi.use_waterfall_for_divergent_tex_samplers = true;
+   ctx->abi.disable_aniso_single_level = true;
 
    unsigned num_outputs = info->num_outputs;
    /* need extra output to hold primitive id added by nir ngg lower */
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm_resources.c b/src/gallium/drivers/radeonsi/si_shader_llvm_resources.c
deleted file mode 100644
index 86daf419cc8f..000000000000
--- a/src/gallium/drivers/radeonsi/si_shader_llvm_resources.c
+++ /dev/null
@@ -1,267 +0,0 @@
-/*
- * Copyright 2020 Advanced Micro Devices, Inc.
- * All Rights Reserved.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * on the rights to use, copy, modify, merge, publish, distribute, sub
- * license, and/or sell copies of the Software, and to permit persons to whom
- * the Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice (including the next
- * paragraph) shall be included in all copies or substantial portions of the
- * Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
- * THE AUTHOR(S) AND/OR THEIR SUPPLIERS BE LIABLE FOR ANY CLAIM,
- * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
- * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
- * USE OR OTHER DEALINGS IN THE SOFTWARE.
- */
-
-#include "si_pipe.h"
-#include "si_shader_internal.h"
-#include "sid.h"
-
-/**
- * Return a value that is equal to the given i32 \p index if it lies in [0,num)
- * or an undefined value in the same interval otherwise.
- */
-static LLVMValueRef si_llvm_bound_index(struct si_shader_context *ctx, LLVMValueRef index,
-                                        unsigned num)
-{
-   LLVMBuilderRef builder = ctx->ac.builder;
-   LLVMValueRef c_max = LLVMConstInt(ctx->ac.i32, num - 1, 0);
-   LLVMValueRef cc;
-
-   if (util_is_power_of_two_or_zero(num)) {
-      index = LLVMBuildAnd(builder, index, c_max, "");
-   } else {
-      /* In theory, this MAX pattern should result in code that is
-       * as good as the bit-wise AND above.
-       *
-       * In practice, LLVM generates worse code (at the time of
-       * writing), because its value tracking is not strong enough.
-       */
-      cc = LLVMBuildICmp(builder, LLVMIntULE, index, c_max, "");
-      index = LLVMBuildSelect(builder, cc, index, c_max, "");
-   }
-
-   return index;
-}
-
-/**
- * Given a 256-bit resource descriptor, force the DCC enable bit to off.
- *
- * At least on Tonga, executing image stores on images with DCC enabled and
- * non-trivial can eventually lead to lockups. This can occur when an
- * application binds an image as read-only but then uses a shader that writes
- * to it. The OpenGL spec allows almost arbitrarily bad behavior (including
- * program termination) in this case, but it doesn't cost much to be a bit
- * nicer: disabling DCC in the shader still leads to undefined results but
- * avoids the lockup.
- */
-static LLVMValueRef force_dcc_off(struct si_shader_context *ctx, LLVMValueRef rsrc)
-{
-   if (ctx->screen->info.gfx_level <= GFX7) {
-      return rsrc;
-   } else {
-      LLVMValueRef i32_6 = LLVMConstInt(ctx->ac.i32, 6, 0);
-      LLVMValueRef i32_C = LLVMConstInt(ctx->ac.i32, C_008F28_COMPRESSION_EN, 0);
-      LLVMValueRef tmp;
-
-      tmp = LLVMBuildExtractElement(ctx->ac.builder, rsrc, i32_6, "");
-      tmp = LLVMBuildAnd(ctx->ac.builder, tmp, i32_C, "");
-      return LLVMBuildInsertElement(ctx->ac.builder, rsrc, tmp, i32_6, "");
-   }
-}
-
-static LLVMValueRef force_write_compress_off(struct si_shader_context *ctx, LLVMValueRef rsrc)
-{
-   LLVMValueRef i32_6 = LLVMConstInt(ctx->ac.i32, 6, 0);
-   LLVMValueRef i32_C = LLVMConstInt(ctx->ac.i32, C_00A018_WRITE_COMPRESS_ENABLE, 0);
-   LLVMValueRef tmp;
-
-   tmp = LLVMBuildExtractElement(ctx->ac.builder, rsrc, i32_6, "");
-   tmp = LLVMBuildAnd(ctx->ac.builder, tmp, i32_C, "");
-   return LLVMBuildInsertElement(ctx->ac.builder, rsrc, tmp, i32_6, "");
-}
-
-static LLVMValueRef fixup_image_desc(struct si_shader_context *ctx, LLVMValueRef rsrc,
-                                     bool uses_store)
-{
-   if (uses_store && ctx->ac.gfx_level <= GFX9)
-      rsrc = force_dcc_off(ctx, rsrc);
-
-   if (!uses_store && ctx->screen->info.has_image_load_dcc_bug &&
-       ctx->screen->always_allow_dcc_stores)
-      rsrc = force_write_compress_off(ctx, rsrc);
-
-   return rsrc;
-}
-
-/* AC_DESC_FMASK is handled exactly like AC_DESC_IMAGE. The caller should
- * adjust "index" to point to FMASK. */
-static LLVMValueRef si_load_image_desc(struct si_shader_context *ctx, struct ac_llvm_pointer list,
-                                       LLVMValueRef index, enum ac_descriptor_type desc_type,
-                                       bool uses_store, bool bindless)
-{
-   LLVMValueRef rsrc;
-
-   if (desc_type == AC_DESC_BUFFER) {
-      index = ac_build_imad(&ctx->ac, index, LLVMConstInt(ctx->ac.i32, 2, 0), ctx->ac.i32_1);
-      list.pointee_type = ctx->ac.v4i32;
-   } else {
-      assert(desc_type == AC_DESC_IMAGE || desc_type == AC_DESC_FMASK);
-   }
-
-   if (bindless)
-      rsrc = ac_build_load_to_sgpr_uint_wraparound(&ctx->ac, list, index);
-   else
-      rsrc = ac_build_load_to_sgpr(&ctx->ac, list, index);
-
-   if (desc_type == AC_DESC_IMAGE)
-      rsrc = fixup_image_desc(ctx, rsrc, uses_store);
-
-   return rsrc;
-}
-
-/**
- * Load an image view, fmask view. or sampler state descriptor.
- */
-static LLVMValueRef si_load_sampler_desc(struct si_shader_context *ctx, struct ac_llvm_pointer list,
-                                         LLVMValueRef index, enum ac_descriptor_type type)
-{
-   LLVMBuilderRef builder = ctx->ac.builder;
-
-   switch (type) {
-   case AC_DESC_IMAGE:
-      /* The image is at [0:7]. */
-      index = LLVMBuildMul(builder, index, LLVMConstInt(ctx->ac.i32, 2, 0), "");
-      break;
-   case AC_DESC_BUFFER:
-      /* The buffer is in [4:7]. */
-      index = ac_build_imad(&ctx->ac, index, LLVMConstInt(ctx->ac.i32, 4, 0), ctx->ac.i32_1);
-      list.pointee_type = ctx->ac.v4i32;
-      break;
-   case AC_DESC_FMASK:
-      /* The FMASK is at [8:15]. */
-      assert(ctx->screen->info.gfx_level < GFX11);
-      index = ac_build_imad(&ctx->ac, index, LLVMConstInt(ctx->ac.i32, 2, 0), ctx->ac.i32_1);
-      break;
-   case AC_DESC_SAMPLER:
-      /* The sampler state is at [12:15]. */
-      index = ac_build_imad(&ctx->ac, index, LLVMConstInt(ctx->ac.i32, 4, 0),
-                            LLVMConstInt(ctx->ac.i32, 3, 0));
-      list.pointee_type = ctx->ac.v4i32;
-      break;
-   case AC_DESC_PLANE_0:
-   case AC_DESC_PLANE_1:
-   case AC_DESC_PLANE_2:
-      /* Only used for the multiplane image support for Vulkan. Should
-       * never be reached in radeonsi.
-       */
-      unreachable("Plane descriptor requested in radeonsi.");
-   }
-
-   return ac_build_load_to_sgpr(&ctx->ac, list, index);
-}
-
-static LLVMValueRef si_nir_load_sampler_desc(struct ac_shader_abi *abi, unsigned descriptor_set,
-                                             unsigned base_index, unsigned constant_index,
-                                             LLVMValueRef dynamic_index,
-                                             enum ac_descriptor_type desc_type, bool image,
-                                             bool write, bool bindless)
-{
-   struct si_shader_context *ctx = si_shader_context_from_abi(abi);
-   LLVMBuilderRef builder = ctx->ac.builder;
-   unsigned const_index = base_index + constant_index;
-
-   assert(!descriptor_set);
-   assert(desc_type <= AC_DESC_BUFFER);
-
-   if (bindless) {
-      struct ac_llvm_pointer list = ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->bindless_samplers_and_images);
-
-      /* dynamic_index is the bindless handle */
-      if (image) {
-         /* Bindless image descriptors use 16-dword slots. */
-         dynamic_index =
-            LLVMBuildMul(ctx->ac.builder, dynamic_index, LLVMConstInt(ctx->ac.i64, 2, 0), "");
-         /* FMASK is right after the image. */
-         if (desc_type == AC_DESC_FMASK) {
-            dynamic_index = LLVMBuildAdd(ctx->ac.builder, dynamic_index, ctx->ac.i32_1, "");
-         }
-
-         return si_load_image_desc(ctx, list, dynamic_index, desc_type, write, true);
-      }
-
-      /* Since bindless handle arithmetic can contain an unsigned integer
-       * wraparound and si_load_sampler_desc assumes there isn't any,
-       * use GEP without "inbounds" (inside ac_build_pointer_add)
-       * to prevent incorrect code generation and hangs.
-       */
-      dynamic_index =
-         LLVMBuildMul(ctx->ac.builder, dynamic_index, LLVMConstInt(ctx->ac.i64, 2, 0), "");
-      list.v = ac_build_pointer_add(&ctx->ac, ctx->ac.v8i32, list.v, dynamic_index);
-      return si_load_sampler_desc(ctx, list, ctx->ac.i32_0, desc_type);
-   }
-
-   unsigned num_slots = image ? ctx->num_images : ctx->num_samplers;
-
-   /* Redirect invalid resource indices to the first array element. */
-   if (const_index >= num_slots)
-      const_index = base_index;
-
-   struct ac_llvm_pointer list = ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->samplers_and_images);
-   LLVMValueRef index = LLVMConstInt(ctx->ac.i32, const_index, false);
-
-   if (dynamic_index) {
-      index = LLVMBuildAdd(builder, index, dynamic_index, "");
-
-      /* From the GL_ARB_shader_image_load_store extension spec:
-       *
-       *    If a shader performs an image load, store, or atomic
-       *    operation using an image variable declared as an array,
-       *    and if the index used to select an individual element is
-       *    negative or greater than or equal to the size of the
-       *    array, the results of the operation are undefined but may
-       *    not lead to termination.
-       */
-      index = si_llvm_bound_index(ctx, index, num_slots);
-   }
-
-   if (image) {
-      /* Fast path if the image is in user SGPRs. */
-      if (!dynamic_index &&
-          const_index < ctx->shader->selector->cs_num_images_in_user_sgprs &&
-          (desc_type == AC_DESC_IMAGE || desc_type == AC_DESC_BUFFER)) {
-         LLVMValueRef rsrc = ac_get_arg(&ctx->ac, ctx->args->cs_image[const_index]);
-
-         if (desc_type == AC_DESC_IMAGE)
-            rsrc = fixup_image_desc(ctx, rsrc, write);
-         return rsrc;
-      }
-
-      /* FMASKs are separate from images. */
-      if (desc_type == AC_DESC_FMASK) {
-         index =
-            LLVMBuildAdd(ctx->ac.builder, index, LLVMConstInt(ctx->ac.i32, SI_NUM_IMAGES, 0), "");
-      }
-      index = LLVMBuildSub(ctx->ac.builder, LLVMConstInt(ctx->ac.i32, SI_NUM_IMAGE_SLOTS - 1, 0),
-                           index, "");
-      return si_load_image_desc(ctx, list, index, desc_type, write, false);
-   }
-
-   index = LLVMBuildAdd(ctx->ac.builder, index,
-                        LLVMConstInt(ctx->ac.i32, SI_NUM_IMAGE_SLOTS / 2, 0), "");
-   return si_load_sampler_desc(ctx, list, index, desc_type);
-}
-
-void si_llvm_init_resource_callbacks(struct si_shader_context *ctx)
-{
-   ctx->abi.load_sampler_desc = si_nir_load_sampler_desc;
-}
diff --git a/src/gallium/drivers/radeonsi/si_shader_nir.c b/src/gallium/drivers/radeonsi/si_shader_nir.c
index c704bfd312fe..50cb24a575dc 100644
--- a/src/gallium/drivers/radeonsi/si_shader_nir.c
+++ b/src/gallium/drivers/radeonsi/si_shader_nir.c
@@ -264,16 +264,18 @@ static void si_lower_nir(struct si_screen *sscreen, struct nir_shader *nir)
     *   and copy-propagated
     */
 
-   static const struct nir_lower_tex_options lower_tex_options = {
+   const struct nir_lower_tex_options lower_tex_options = {
       .lower_txp = ~0u,
       .lower_txs_cube_array = true,
       .lower_invalid_implicit_lod = true,
       .lower_tg4_offsets = true,
+      .lower_to_fragment_fetch_amd = sscreen->info.gfx_level < GFX11,
    };
    NIR_PASS_V(nir, nir_lower_tex, &lower_tex_options);
 
-   static const struct nir_lower_image_options lower_image_options = {
+   const struct nir_lower_image_options lower_image_options = {
       .lower_cube_size = true,
+      .lower_to_fragment_load_amd = sscreen->info.gfx_level < GFX11,
    };
    NIR_PASS_V(nir, nir_lower_image, &lower_image_options);
 
-- 
GitLab


From 71790904da57f966ae9edff44049359ac3d12cfe Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Fri, 16 Sep 2022 14:50:34 +0800
Subject: [PATCH 35/45] ac/llvm: remove image/texture descriptor_amd nir to
 llvm code

They have been lowered in nir.

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/llvm/ac_nir_to_llvm.c | 52 ++++-------------------------------
 1 file changed, 6 insertions(+), 46 deletions(-)

diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index 7e9e8604fb24..8a918480177c 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -2913,32 +2913,6 @@ static LLVMValueRef visit_image_atomic(struct ac_nir_context *ctx, const nir_int
    return result;
 }
 
-static LLVMValueRef visit_image_descriptor(struct ac_nir_context *ctx,
-                                           const nir_intrinsic_instr *instr,
-                                           bool bindless)
-{
-   enum glsl_sampler_dim dim;
-
-   if (bindless) {
-      dim = nir_intrinsic_image_dim(instr);
-   } else {
-      const struct glsl_type *type = get_image_deref(instr)->type;
-      dim = glsl_get_sampler_dim(type);
-   }
-
-   nir_deref_instr *deref_instr = NULL;
-   if (instr->src[0].ssa->parent_instr->type == nir_instr_type_deref)
-      deref_instr = nir_instr_as_deref(instr->src[0].ssa->parent_instr);
-
-   LLVMValueRef dynamic_index = get_sampler_desc_index(ctx, deref_instr, &instr->instr, true);
-
-   if (dim == GLSL_SAMPLER_DIM_BUF) {
-      return get_image_descriptor(ctx, instr, dynamic_index, AC_DESC_BUFFER, false);
-   } else {
-      return get_image_descriptor(ctx, instr, dynamic_index, AC_DESC_IMAGE, false);
-   }
-}
-
 static void emit_discard(struct ac_nir_context *ctx, const nir_intrinsic_instr *instr)
 {
    LLVMValueRef cond;
@@ -3869,12 +3843,6 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
    case nir_intrinsic_image_deref_atomic_fmax:
       result = visit_image_atomic(ctx, instr, false);
       break;
-   case nir_intrinsic_image_deref_descriptor_amd:
-      result = visit_image_descriptor(ctx, instr, false);
-      break;
-   case nir_intrinsic_bindless_image_descriptor_amd:
-      result = visit_image_descriptor(ctx, instr, true);
-      break;
    case nir_intrinsic_shader_clock:
       result = ac_build_shader_clock(&ctx->ac, nir_intrinsic_memory_scope(instr));
       break;
@@ -4618,8 +4586,7 @@ static LLVMValueRef sici_fix_sampler_aniso(struct ac_nir_context *ctx, LLVMValue
 
 static void tex_fetch_ptrs(struct ac_nir_context *ctx, nir_tex_instr *instr,
                            struct waterfall_context *wctx, LLVMValueRef *res_ptr,
-                           LLVMValueRef *samp_ptr, LLVMValueRef *fmask_ptr,
-                           bool divergent)
+                           LLVMValueRef *samp_ptr, LLVMValueRef *fmask_ptr)
 {
    bool texture_handle_divergent = false;
    bool sampler_handle_divergent = false;
@@ -4700,11 +4667,11 @@ static void tex_fetch_ptrs(struct ac_nir_context *ctx, nir_tex_instr *instr,
       /* descriptor handles given through nir_tex_src_{texture,sampler}_handle */
       if (instr->texture_non_uniform ||
           (ctx->abi->use_waterfall_for_divergent_tex_samplers && texture_handle_divergent))
-         texture_dynamic_handle = enter_waterfall(ctx, &wctx[0], texture_dynamic_handle, divergent);
+         texture_dynamic_handle = enter_waterfall(ctx, &wctx[0], texture_dynamic_handle, true);
 
       if (instr->sampler_non_uniform ||
          (ctx->abi->use_waterfall_for_divergent_tex_samplers && sampler_handle_divergent))
-         sampler_dynamic_handle = enter_waterfall(ctx, &wctx[1], sampler_dynamic_handle, divergent);
+         sampler_dynamic_handle = enter_waterfall(ctx, &wctx[1], sampler_dynamic_handle, true);
 
       if (texture_dynamic_handle)
          *res_ptr = ctx->abi->load_sampler_desc(ctx->abi, 0, 0, 0, texture_dynamic_handle,
@@ -4745,11 +4712,11 @@ static void tex_fetch_ptrs(struct ac_nir_context *ctx, nir_tex_instr *instr,
     */
    if (instr->texture_non_uniform ||
        (ctx->abi->use_waterfall_for_divergent_tex_samplers && texture_deref_instr->dest.ssa.divergent))
-      texture_dynamic_index = enter_waterfall(ctx, wctx + 0, texture_dynamic_index, divergent);
+      texture_dynamic_index = enter_waterfall(ctx, wctx + 0, texture_dynamic_index, true);
 
    if (instr->sampler_non_uniform ||
        (ctx->abi->use_waterfall_for_divergent_tex_samplers && sampler_deref_instr->dest.ssa.divergent))
-      sampler_dynamic_index = enter_waterfall(ctx, wctx + 1, sampler_dynamic_index, divergent);
+      sampler_dynamic_index = enter_waterfall(ctx, wctx + 1, sampler_dynamic_index, true);
 
    *res_ptr = get_sampler_desc(ctx, texture_deref_instr, main_descriptor, &instr->instr,
                                texture_dynamic_index, false, false);
@@ -4783,14 +4750,7 @@ static void visit_tex(struct ac_nir_context *ctx, nir_tex_instr *instr)
    unsigned offset_src = 0;
    struct waterfall_context wctx[2] = {{{0}}};
 
-   /* Don't use the waterfall loop when returning a descriptor. */
-   tex_fetch_ptrs(ctx, instr, wctx, &args.resource, &args.sampler, &fmask_ptr,
-                  instr->op != nir_texop_descriptor_amd);
-
-   if (instr->op == nir_texop_descriptor_amd) {
-      result = args.resource;
-      goto write_result;
-   }
+   tex_fetch_ptrs(ctx, instr, wctx, &args.resource, &args.sampler, &fmask_ptr);
 
    for (unsigned i = 0; i < instr->num_srcs; i++) {
       switch (instr->src[i].src_type) {
-- 
GitLab


From 3e0a5ef379cb37e626b9c7cbb910c45e0b7e4357 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Fri, 16 Sep 2022 14:57:01 +0800
Subject: [PATCH 36/45] ac/llvm: remove implicite ms texture fmask handling

It has been lowered to fragment_mask_load_amd in nir.

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/llvm/ac_nir_to_llvm.c | 38 +++--------------------------------
 1 file changed, 3 insertions(+), 35 deletions(-)

diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index 8a918480177c..589708b79391 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -2444,22 +2444,6 @@ static int image_type_to_components_count(enum glsl_sampler_dim dim, bool array)
    return 0;
 }
 
-static LLVMValueRef adjust_sample_index_using_fmask(struct ac_llvm_context *ctx,
-                                                    LLVMValueRef coord_x, LLVMValueRef coord_y,
-                                                    LLVMValueRef coord_z, LLVMValueRef sample_index,
-                                                    LLVMValueRef fmask_desc_ptr)
-{
-   if (!fmask_desc_ptr)
-      return sample_index;
-
-   unsigned sample_chan = coord_z ? 3 : 2;
-   LLVMValueRef addr[4] = {coord_x, coord_y, coord_z};
-   addr[sample_chan] = sample_index;
-
-   ac_apply_fmask_to_sample(ctx, fmask_desc_ptr, addr, coord_z != NULL);
-   return addr[sample_chan];
-}
-
 static nir_deref_instr *get_image_deref(const nir_intrinsic_instr *instr)
 {
    assert(instr->src[0].is_ssa);
@@ -4586,7 +4570,7 @@ static LLVMValueRef sici_fix_sampler_aniso(struct ac_nir_context *ctx, LLVMValue
 
 static void tex_fetch_ptrs(struct ac_nir_context *ctx, nir_tex_instr *instr,
                            struct waterfall_context *wctx, LLVMValueRef *res_ptr,
-                           LLVMValueRef *samp_ptr, LLVMValueRef *fmask_ptr)
+                           LLVMValueRef *samp_ptr)
 {
    bool texture_handle_divergent = false;
    bool sampler_handle_divergent = false;
@@ -4598,7 +4582,6 @@ static void tex_fetch_ptrs(struct ac_nir_context *ctx, nir_tex_instr *instr,
 
    *res_ptr = NULL;
    *samp_ptr = NULL;
-   *fmask_ptr = NULL;
    for (unsigned i = 0; i < instr->num_srcs; i++) {
       switch (instr->src[i].src_type) {
       case nir_tex_src_texture_deref:
@@ -4727,10 +4710,6 @@ static void tex_fetch_ptrs(struct ac_nir_context *ctx, nir_tex_instr *instr,
       if (instr->sampler_dim < GLSL_SAMPLER_DIM_RECT)
          *samp_ptr = sici_fix_sampler_aniso(ctx, *res_ptr, *samp_ptr);
    }
-   if (ctx->ac.gfx_level < GFX11 &&
-       fmask_ptr && (instr->op == nir_texop_txf_ms || instr->op == nir_texop_samples_identical))
-      *fmask_ptr = get_sampler_desc(ctx, texture_deref_instr, AC_DESC_FMASK, &instr->instr,
-                                    texture_dynamic_index, false, false);
 }
 
 static LLVMValueRef apply_round_slice(struct ac_llvm_context *ctx, LLVMValueRef coord)
@@ -4745,12 +4724,12 @@ static void visit_tex(struct ac_nir_context *ctx, nir_tex_instr *instr)
 {
    LLVMValueRef result = NULL;
    struct ac_image_args args = {0};
-   LLVMValueRef fmask_ptr = NULL, sample_index = NULL;
+   LLVMValueRef sample_index = NULL;
    LLVMValueRef ddx = NULL, ddy = NULL;
    unsigned offset_src = 0;
    struct waterfall_context wctx[2] = {{{0}}};
 
-   tex_fetch_ptrs(ctx, instr, wctx, &args.resource, &args.sampler, &fmask_ptr);
+   tex_fetch_ptrs(ctx, instr, wctx, &args.resource, &args.sampler);
 
    for (unsigned i = 0; i < instr->num_srcs; i++) {
       switch (instr->src[i].src_type) {
@@ -4943,17 +4922,6 @@ static void visit_tex(struct ac_nir_context *ctx, nir_tex_instr *instr)
       goto write_result;
    }
 
-   if (ctx->ac.gfx_level < GFX11 &&
-       (instr->sampler_dim == GLSL_SAMPLER_DIM_SUBPASS_MS ||
-        instr->sampler_dim == GLSL_SAMPLER_DIM_MS) &&
-       instr->op != nir_texop_fragment_fetch_amd &&
-       instr->op != nir_texop_fragment_mask_fetch_amd) {
-      unsigned sample_chan = instr->is_array ? 3 : 2;
-      args.coords[sample_chan] = adjust_sample_index_using_fmask(
-         &ctx->ac, args.coords[0], args.coords[1], instr->is_array ? args.coords[2] : NULL,
-         args.coords[sample_chan], fmask_ptr);
-   }
-
    if (args.offset && (instr->op == nir_texop_txf || instr->op == nir_texop_txf_ms)) {
       int num_offsets = instr->src[offset_src].src.ssa->num_components;
       num_offsets = MIN2(num_offsets, instr->coord_components);
-- 
GitLab


From 9eba99006ef1debbddd4e8c2d630099730ce5e91 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Fri, 16 Sep 2022 15:58:23 +0800
Subject: [PATCH 37/45] ac/llvm: remove samples_identical nir to llvm code

It has been lowered in nir.

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/llvm/ac_nir_to_llvm.c | 22 ++--------------------
 1 file changed, 2 insertions(+), 20 deletions(-)

diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index 589708b79391..317d08c6d5f8 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -1578,7 +1578,6 @@ static LLVMValueRef build_tex_intrinsic(struct ac_nir_context *ctx, const nir_te
    switch (instr->op) {
    case nir_texop_txf:
    case nir_texop_txf_ms:
-   case nir_texop_samples_identical:
       args->opcode = args->level_zero || instr->sampler_dim == GLSL_SAMPLER_DIM_MS
                         ? ac_image_load
                         : ac_image_load_mip;
@@ -3786,7 +3785,6 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
       break;
    case nir_intrinsic_image_deref_load:
    case nir_intrinsic_image_deref_sparse_load:
-   case nir_intrinsic_image_deref_samples_identical:
       result = visit_image_load(ctx, instr, false);
       break;
    case nir_intrinsic_bindless_image_store:
@@ -4622,13 +4620,13 @@ static void tex_fetch_ptrs(struct ac_nir_context *ctx, nir_tex_instr *instr,
       instr->sampler_dim == GLSL_SAMPLER_DIM_BUF ? AC_DESC_BUFFER : AC_DESC_IMAGE;
 
    if (plane >= 0) {
-      assert(instr->op != nir_texop_txf_ms && instr->op != nir_texop_samples_identical);
+      assert(instr->op != nir_texop_txf_ms);
       assert(instr->sampler_dim != GLSL_SAMPLER_DIM_BUF);
 
       main_descriptor = AC_DESC_PLANE_0 + plane;
    }
 
-   if (instr->op == nir_texop_fragment_mask_fetch_amd || instr->op == nir_texop_samples_identical) {
+   if (instr->op == nir_texop_fragment_mask_fetch_amd) {
       /* The fragment mask is fetched from the compressed
        * multisampled surface.
        */
@@ -4907,21 +4905,6 @@ static void visit_tex(struct ac_nir_context *ctx, nir_tex_instr *instr)
    if (sample_index && (instr->op == nir_texop_txf_ms || instr->op == nir_texop_fragment_fetch_amd))
       args.coords[instr->coord_components] = sample_index;
 
-   if (instr->op == nir_texop_samples_identical) {
-      assert(ctx->ac.gfx_level < GFX11);
-      struct ac_image_args txf_args = {0};
-      memcpy(txf_args.coords, args.coords, sizeof(txf_args.coords));
-
-      txf_args.dmask = 0xf;
-      txf_args.resource = args.resource;
-      txf_args.dim = instr->is_array ? ac_image_2darray : ac_image_2d;
-      result = build_tex_intrinsic(ctx, instr, &txf_args);
-
-      result = LLVMBuildExtractElement(ctx->ac.builder, result, ctx->ac.i32_0, "");
-      result = emit_int_cmp(&ctx->ac, LLVMIntEQ, result, ctx->ac.i32_0);
-      goto write_result;
-   }
-
    if (args.offset && (instr->op == nir_texop_txf || instr->op == nir_texop_txf_ms)) {
       int num_offsets = instr->src[offset_src].src.ssa->num_components;
       num_offsets = MIN2(num_offsets, instr->coord_components);
@@ -5003,7 +4986,6 @@ static void visit_tex(struct ac_nir_context *ctx, nir_tex_instr *instr)
    if (instr->is_sparse)
       result = ac_build_concat(&ctx->ac, result, code);
 
-write_result:
    if (result) {
       assert(instr->dest.is_ssa);
       result = ac_to_integer(&ctx->ac, result);
-- 
GitLab


From 84a50410dbdf385d5de536dfc19d2f04db9da3ff Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Fri, 16 Sep 2022 16:39:41 +0800
Subject: [PATCH 38/45] ac,radv,radeonsi: remove unused param of
 load_sampler_desc abi

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/llvm/ac_nir_to_llvm.c                 | 13 +++++-------
 src/amd/llvm/ac_shader_abi.h                  | 13 +++---------
 src/amd/vulkan/radv_nir_to_llvm.c             |  8 ++-----
 src/gallium/drivers/radeonsi/si_shader_llvm.c | 21 ++++---------------
 4 files changed, 14 insertions(+), 41 deletions(-)

diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index 317d08c6d5f8..a0729574a017 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -4534,10 +4534,7 @@ static LLVMValueRef get_sampler_desc(struct ac_nir_context *ctx, nir_deref_instr
                                      enum ac_descriptor_type desc_type, const nir_instr *instr,
                                      LLVMValueRef index, bool image, bool write)
 {
-   struct sampler_desc_address addr = get_sampler_desc_internal(ctx, deref_instr, instr, image);
-   return ctx->abi->load_sampler_desc(ctx->abi, addr.descriptor_set, addr.base_index,
-                                      addr.constant_index, index, desc_type, addr.image, write,
-                                      addr.bindless);
+   return ctx->abi->load_sampler_desc(ctx->abi, index, desc_type);
 }
 
 /* Disable anisotropic filtering if BASE_LEVEL == LAST_LEVEL.
@@ -4655,12 +4652,12 @@ static void tex_fetch_ptrs(struct ac_nir_context *ctx, nir_tex_instr *instr,
          sampler_dynamic_handle = enter_waterfall(ctx, &wctx[1], sampler_dynamic_handle, true);
 
       if (texture_dynamic_handle)
-         *res_ptr = ctx->abi->load_sampler_desc(ctx->abi, 0, 0, 0, texture_dynamic_handle,
-                                                main_descriptor, false, false, true);
+         *res_ptr = ctx->abi->load_sampler_desc(ctx->abi, texture_dynamic_handle,
+                                                main_descriptor);
 
       if (samp_ptr && sampler_dynamic_handle) {
-         *samp_ptr = ctx->abi->load_sampler_desc(ctx->abi, 0, 0, 0, sampler_dynamic_handle,
-                                                 AC_DESC_SAMPLER, false, false, true);
+         *samp_ptr = ctx->abi->load_sampler_desc(ctx->abi, sampler_dynamic_handle,
+                                                 AC_DESC_SAMPLER);
 
          if (ctx->abi->disable_aniso_single_level &&
              instr->sampler_dim < GLSL_SAMPLER_DIM_RECT)
diff --git a/src/amd/llvm/ac_shader_abi.h b/src/amd/llvm/ac_shader_abi.h
index 50305f93f1b5..5e6254c8e8e7 100644
--- a/src/amd/llvm/ac_shader_abi.h
+++ b/src/amd/llvm/ac_shader_abi.h
@@ -92,18 +92,11 @@ struct ac_shader_abi {
    /**
     * Load a descriptor associated to a sampler.
     *
-    * \param descriptor_set the descriptor set index (only for Vulkan)
-    * \param base_index the base index of the sampler variable
-    * \param constant_index constant part of an array index (or 0, if the
-    *                       sampler variable is not an array)
-    * \param index non-constant part of an array index (may be NULL)
+    * \param index of the descriptor
     * \param desc_type the type of descriptor to load
-    * \param image whether the descriptor is loaded for an image operation
     */
-   LLVMValueRef (*load_sampler_desc)(struct ac_shader_abi *abi, unsigned descriptor_set,
-                                     unsigned base_index, unsigned constant_index,
-                                     LLVMValueRef index, enum ac_descriptor_type desc_type,
-                                     bool image, bool write, bool bindless);
+   LLVMValueRef (*load_sampler_desc)(struct ac_shader_abi *abi, LLVMValueRef index,
+                                     enum ac_descriptor_type desc_type);
 
    LLVMValueRef (*load_sample_position)(struct ac_shader_abi *abi, LLVMValueRef sample_id);
 
diff --git a/src/amd/vulkan/radv_nir_to_llvm.c b/src/amd/vulkan/radv_nir_to_llvm.c
index 1ca2a31f0684..22f65e63a30a 100644
--- a/src/amd/vulkan/radv_nir_to_llvm.c
+++ b/src/amd/vulkan/radv_nir_to_llvm.c
@@ -284,15 +284,11 @@ radv_load_ssbo(struct ac_shader_abi *abi, LLVMValueRef buffer_ptr, bool write, b
 }
 
 static LLVMValueRef
-radv_get_sampler_desc(struct ac_shader_abi *abi, unsigned descriptor_set, unsigned base_index,
-                      unsigned constant_index, LLVMValueRef index,
-                      enum ac_descriptor_type desc_type, bool image, bool write, bool bindless)
+radv_get_sampler_desc(struct ac_shader_abi *abi, LLVMValueRef index,
+                      enum ac_descriptor_type desc_type)
 {
    struct radv_shader_context *ctx = radv_shader_context_from_abi(abi);
 
-   if (image && desc_type == AC_DESC_FMASK)
-      return NULL;
-
    /* 3 plane formats always have same size and format for plane 1 & 2, so
     * use the tail from plane 1 so that we can store only the first 16 bytes
     * of the last plane. */
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index 5d9c655c6fe2..ff354b197493 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -725,27 +725,14 @@ static LLVMValueRef si_llvm_load_intrinsic(struct ac_shader_abi *abi, nir_intrin
    }
 }
 
-static LLVMValueRef si_llvm_load_sampler_desc(struct ac_shader_abi *abi, unsigned descriptor_set,
-                                              unsigned base_index, unsigned constant_index,
-                                              LLVMValueRef dynamic_index,
-                                              enum ac_descriptor_type desc_type, bool image,
-                                              bool write, bool bindless)
+static LLVMValueRef si_llvm_load_sampler_desc(struct ac_shader_abi *abi, LLVMValueRef index,
+                                              enum ac_descriptor_type desc_type)
 {
    struct si_shader_context *ctx = si_shader_context_from_abi(abi);
    LLVMBuilderRef builder = ctx->ac.builder;
 
-   /* always 0 for OpenGL */
-   assert(!descriptor_set);
-
-   /* all image and texture has been lowered to bindless one in nir */
-   assert(bindless);
-
-   if (dynamic_index && LLVMTypeOf(dynamic_index) == ctx->ac.i32) {
-      /* image desc has been lowered in nir, we only expect texture here */
-      assert(!image);
-
+   if (index && LLVMTypeOf(index) == ctx->ac.i32) {
       bool is_vec4 = false;
-      LLVMValueRef index = dynamic_index;
 
       switch (desc_type) {
       case AC_DESC_IMAGE:
@@ -780,7 +767,7 @@ static LLVMValueRef si_llvm_load_sampler_desc(struct ac_shader_abi *abi, unsigne
       return ac_build_load_to_sgpr(&ctx->ac, list, index);
    }
 
-   return dynamic_index;
+   return index;
 }
 
 bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shader,
-- 
GitLab


From 1b4e7e853b8ea57142af34963a23a0bf46d66a4d Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Fri, 16 Sep 2022 17:01:11 +0800
Subject: [PATCH 39/45] ac/llvm: remove deref image/texture code

They have been lowered to bindless ones in nir.

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/llvm/ac_nir_to_llvm.c | 350 ++++------------------------------
 1 file changed, 37 insertions(+), 313 deletions(-)

diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index a0729574a017..dcd155e9a2a1 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -57,13 +57,6 @@ struct ac_nir_context {
    LLVMBasicBlockRef break_block;
 };
 
-static LLVMValueRef get_sampler_desc_index(struct ac_nir_context *ctx, nir_deref_instr *deref_instr,
-                                           const nir_instr *instr, bool image);
-
-static LLVMValueRef get_sampler_desc(struct ac_nir_context *ctx, nir_deref_instr *deref_instr,
-                                     enum ac_descriptor_type desc_type, const nir_instr *instr,
-                                     LLVMValueRef index, bool image, bool write);
-
 static LLVMTypeRef get_def_type(struct ac_nir_context *ctx, const nir_ssa_def *def)
 {
    LLVMTypeRef type = LLVMIntTypeInContext(ctx->ac.context, def->bit_size);
@@ -2443,24 +2436,6 @@ static int image_type_to_components_count(enum glsl_sampler_dim dim, bool array)
    return 0;
 }
 
-static nir_deref_instr *get_image_deref(const nir_intrinsic_instr *instr)
-{
-   assert(instr->src[0].is_ssa);
-   return nir_instr_as_deref(instr->src[0].ssa->parent_instr);
-}
-
-static LLVMValueRef get_image_descriptor(struct ac_nir_context *ctx,
-                                         const nir_intrinsic_instr *instr,
-                                         LLVMValueRef dynamic_index,
-                                         enum ac_descriptor_type desc_type, bool write)
-{
-   nir_deref_instr *deref_instr = instr->src[0].ssa->parent_instr->type == nir_instr_type_deref
-                                     ? nir_instr_as_deref(instr->src[0].ssa->parent_instr)
-                                     : NULL;
-
-   return get_sampler_desc(ctx, deref_instr, desc_type, &instr->instr, dynamic_index, true, write);
-}
-
 static void get_image_coords(struct ac_nir_context *ctx, const nir_intrinsic_instr *instr,
                              LLVMValueRef dynamic_desc_index, struct ac_image_args *args,
                              enum glsl_sampler_dim dim, bool is_array)
@@ -2556,34 +2531,19 @@ static LLVMValueRef enter_waterfall_image(struct ac_nir_context *ctx,
                                           struct waterfall_context *wctx,
                                           const nir_intrinsic_instr *instr)
 {
-   nir_deref_instr *deref_instr = NULL;
+   /* src0 is desc when uniform, desc index when non uniform */
+   LLVMValueRef value = get_src(ctx, instr->src[0]);
 
-   if (instr->src[0].ssa->parent_instr->type == nir_instr_type_deref)
-      deref_instr = nir_instr_as_deref(instr->src[0].ssa->parent_instr);
-
-   LLVMValueRef value = get_sampler_desc_index(ctx, deref_instr, &instr->instr, true);
    return enter_waterfall(ctx, wctx, value, nir_intrinsic_access(instr) & ACCESS_NON_UNIFORM);
 }
 
-static LLVMValueRef visit_image_load(struct ac_nir_context *ctx, const nir_intrinsic_instr *instr,
-                                     bool bindless)
+static LLVMValueRef visit_image_load(struct ac_nir_context *ctx, const nir_intrinsic_instr *instr)
 {
    LLVMValueRef res;
 
-   enum glsl_sampler_dim dim;
+   enum glsl_sampler_dim dim = nir_intrinsic_image_dim(instr);
    enum gl_access_qualifier access = nir_intrinsic_access(instr);
-   bool is_array;
-   if (bindless) {
-      dim = nir_intrinsic_image_dim(instr);
-      is_array = nir_intrinsic_image_array(instr);
-   } else {
-      const nir_deref_instr *image_deref = get_image_deref(instr);
-      const struct glsl_type *type = image_deref->type;
-      const nir_variable *var = nir_deref_instr_get_variable(image_deref);
-      dim = glsl_get_sampler_dim(type);
-      access |= var->data.access;
-      is_array = glsl_sampler_type_is_array(type);
-   }
+   bool is_array = nir_intrinsic_image_array(instr);
 
    struct waterfall_context wctx;
    LLVMValueRef dynamic_index = enter_waterfall_image(ctx, &wctx, instr);
@@ -2591,8 +2551,7 @@ static LLVMValueRef visit_image_load(struct ac_nir_context *ctx, const nir_intri
    struct ac_image_args args = {0};
 
    args.cache_policy = get_cache_policy(ctx, access, false, false);
-   args.tfe = instr->intrinsic == nir_intrinsic_image_deref_sparse_load ||
-              instr->intrinsic == nir_intrinsic_bindless_image_sparse_load;
+   args.tfe = instr->intrinsic == nir_intrinsic_bindless_image_sparse_load;
 
    if (dim == GLSL_SAMPLER_DIM_BUF) {
       unsigned num_channels = util_last_bit(nir_ssa_def_components_read(&instr->dest.ssa));
@@ -2600,7 +2559,7 @@ static LLVMValueRef visit_image_load(struct ac_nir_context *ctx, const nir_intri
          num_channels = num_channels < 4 ? 2 : 4;
       LLVMValueRef rsrc, vindex;
 
-      rsrc = get_image_descriptor(ctx, instr, dynamic_index, AC_DESC_BUFFER, false);
+      rsrc = ctx->abi->load_sampler_desc(ctx->abi, dynamic_index, AC_DESC_BUFFER);
       vindex =
          LLVMBuildExtractElement(ctx->ac.builder, get_src(ctx, instr->src[1]), ctx->ac.i32_0, "");
 
@@ -2618,7 +2577,7 @@ static LLVMValueRef visit_image_load(struct ac_nir_context *ctx, const nir_intri
       assert(ctx->ac.gfx_level < GFX11);
 
       args.opcode = ac_image_load;
-      args.resource = get_image_descriptor(ctx, instr, dynamic_index, AC_DESC_FMASK, false);
+      args.resource = ctx->abi->load_sampler_desc(ctx->abi, dynamic_index, AC_DESC_FMASK);
       get_image_coords(ctx, instr, dynamic_index, &args, GLSL_SAMPLER_DIM_2D, is_array);
       args.dmask = 0xf;
       args.dim = is_array ? ac_image_2darray : ac_image_2d;
@@ -2630,7 +2589,7 @@ static LLVMValueRef visit_image_load(struct ac_nir_context *ctx, const nir_intri
       bool level_zero = nir_src_is_const(instr->src[3]) && nir_src_as_uint(instr->src[3]) == 0;
 
       args.opcode = level_zero ? ac_image_load : ac_image_load_mip;
-      args.resource = get_image_descriptor(ctx, instr, dynamic_index, AC_DESC_IMAGE, false);
+      args.resource = ctx->abi->load_sampler_desc(ctx->abi, dynamic_index, AC_DESC_IMAGE);
       get_image_coords(ctx, instr, dynamic_index, &args, dim, is_array);
       args.dim = ac_get_image_dim(ctx->ac.gfx_level, dim, is_array);
       if (!level_zero)
@@ -2664,29 +2623,16 @@ static LLVMValueRef visit_image_load(struct ac_nir_context *ctx, const nir_intri
    return exit_waterfall(ctx, &wctx, res);
 }
 
-static void visit_image_store(struct ac_nir_context *ctx, const nir_intrinsic_instr *instr,
-                              bool bindless)
+static void visit_image_store(struct ac_nir_context *ctx, const nir_intrinsic_instr *instr)
 {
    if (ctx->ac.postponed_kill) {
       LLVMValueRef cond = LLVMBuildLoad2(ctx->ac.builder, ctx->ac.i1, ctx->ac.postponed_kill, "");
       ac_build_ifcc(&ctx->ac, cond, 7003);
    }
 
-   enum glsl_sampler_dim dim;
+   enum glsl_sampler_dim dim = nir_intrinsic_image_dim(instr);
    enum gl_access_qualifier access = nir_intrinsic_access(instr);
-   bool is_array;
-
-   if (bindless) {
-      dim = nir_intrinsic_image_dim(instr);
-      is_array = nir_intrinsic_image_array(instr);
-   } else {
-      const nir_deref_instr *image_deref = get_image_deref(instr);
-      const struct glsl_type *type = image_deref->type;
-      const nir_variable *var = nir_deref_instr_get_variable(image_deref);
-      dim = glsl_get_sampler_dim(type);
-      access |= var->data.access;
-      is_array = glsl_sampler_type_is_array(type);
-   }
+   bool is_array = nir_intrinsic_image_array(instr);
 
    struct waterfall_context wctx;
    LLVMValueRef dynamic_index = enter_waterfall_image(ctx, &wctx, instr);
@@ -2706,7 +2652,7 @@ static void visit_image_store(struct ac_nir_context *ctx, const nir_intrinsic_in
    }
 
    if (dim == GLSL_SAMPLER_DIM_BUF) {
-      LLVMValueRef rsrc = get_image_descriptor(ctx, instr, dynamic_index, AC_DESC_BUFFER, true);
+      LLVMValueRef rsrc = ctx->abi->load_sampler_desc(ctx->abi, dynamic_index, AC_DESC_BUFFER);
       unsigned src_channels = ac_get_llvm_num_components(src);
       LLVMValueRef vindex;
 
@@ -2722,7 +2668,7 @@ static void visit_image_store(struct ac_nir_context *ctx, const nir_intrinsic_in
 
       args.opcode = level_zero ? ac_image_store : ac_image_store_mip;
       args.data[0] = src;
-      args.resource = get_image_descriptor(ctx, instr, dynamic_index, AC_DESC_IMAGE, true);
+      args.resource = ctx->abi->load_sampler_desc(ctx->abi, dynamic_index, AC_DESC_IMAGE);
       get_image_coords(ctx, instr, dynamic_index, &args, dim, is_array);
       args.dim = ac_get_image_dim(ctx->ac.gfx_level, dim, is_array);
       if (!level_zero)
@@ -2738,8 +2684,7 @@ static void visit_image_store(struct ac_nir_context *ctx, const nir_intrinsic_in
       ac_build_endif(&ctx->ac, 7003);
 }
 
-static LLVMValueRef visit_image_atomic(struct ac_nir_context *ctx, const nir_intrinsic_instr *instr,
-                                       bool bindless)
+static LLVMValueRef visit_image_atomic(struct ac_nir_context *ctx, const nir_intrinsic_instr *instr)
 {
    if (ctx->ac.postponed_kill) {
       LLVMValueRef cond = LLVMBuildLoad2(ctx->ac.builder, ctx->ac.i1, ctx->ac.postponed_kill, "");
@@ -2749,96 +2694,73 @@ static LLVMValueRef visit_image_atomic(struct ac_nir_context *ctx, const nir_int
    LLVMValueRef params[7];
    int param_count = 0;
 
-   bool cmpswap = instr->intrinsic == nir_intrinsic_image_deref_atomic_comp_swap ||
-                  instr->intrinsic == nir_intrinsic_bindless_image_atomic_comp_swap;
+   bool cmpswap = instr->intrinsic == nir_intrinsic_bindless_image_atomic_comp_swap;
    const char *atomic_name;
    char intrinsic_name[64];
    enum ac_atomic_op atomic_subop;
    ASSERTED int length;
 
-   enum glsl_sampler_dim dim;
-   bool is_array;
-   if (bindless) {
-      dim = nir_intrinsic_image_dim(instr);
-      is_array = nir_intrinsic_image_array(instr);
-   } else {
-      const struct glsl_type *type = get_image_deref(instr)->type;
-      dim = glsl_get_sampler_dim(type);
-      is_array = glsl_sampler_type_is_array(type);
-   }
+   enum glsl_sampler_dim dim = nir_intrinsic_image_dim(instr);
+   bool is_array = nir_intrinsic_image_array(instr);
 
    struct waterfall_context wctx;
    LLVMValueRef dynamic_index = enter_waterfall_image(ctx, &wctx, instr);
 
    switch (instr->intrinsic) {
    case nir_intrinsic_bindless_image_atomic_add:
-   case nir_intrinsic_image_deref_atomic_add:
       atomic_name = "add";
       atomic_subop = ac_atomic_add;
       break;
    case nir_intrinsic_bindless_image_atomic_imin:
-   case nir_intrinsic_image_deref_atomic_imin:
       atomic_name = "smin";
       atomic_subop = ac_atomic_smin;
       break;
    case nir_intrinsic_bindless_image_atomic_umin:
-   case nir_intrinsic_image_deref_atomic_umin:
       atomic_name = "umin";
       atomic_subop = ac_atomic_umin;
       break;
    case nir_intrinsic_bindless_image_atomic_imax:
-   case nir_intrinsic_image_deref_atomic_imax:
       atomic_name = "smax";
       atomic_subop = ac_atomic_smax;
       break;
    case nir_intrinsic_bindless_image_atomic_umax:
-   case nir_intrinsic_image_deref_atomic_umax:
       atomic_name = "umax";
       atomic_subop = ac_atomic_umax;
       break;
    case nir_intrinsic_bindless_image_atomic_and:
-   case nir_intrinsic_image_deref_atomic_and:
       atomic_name = "and";
       atomic_subop = ac_atomic_and;
       break;
    case nir_intrinsic_bindless_image_atomic_or:
-   case nir_intrinsic_image_deref_atomic_or:
       atomic_name = "or";
       atomic_subop = ac_atomic_or;
       break;
    case nir_intrinsic_bindless_image_atomic_xor:
-   case nir_intrinsic_image_deref_atomic_xor:
       atomic_name = "xor";
       atomic_subop = ac_atomic_xor;
       break;
    case nir_intrinsic_bindless_image_atomic_exchange:
-   case nir_intrinsic_image_deref_atomic_exchange:
       atomic_name = "swap";
       atomic_subop = ac_atomic_swap;
       break;
    case nir_intrinsic_bindless_image_atomic_comp_swap:
-   case nir_intrinsic_image_deref_atomic_comp_swap:
       atomic_name = "cmpswap";
       atomic_subop = 0; /* not used */
       break;
-   case nir_intrinsic_bindless_image_atomic_inc_wrap:
-   case nir_intrinsic_image_deref_atomic_inc_wrap: {
+   case nir_intrinsic_bindless_image_atomic_inc_wrap: {
       atomic_name = "inc";
       atomic_subop = ac_atomic_inc_wrap;
       break;
    }
    case nir_intrinsic_bindless_image_atomic_dec_wrap:
-   case nir_intrinsic_image_deref_atomic_dec_wrap:
       atomic_name = "dec";
       atomic_subop = ac_atomic_dec_wrap;
       break;
    case nir_intrinsic_bindless_image_atomic_fmin:
-   case nir_intrinsic_image_deref_atomic_fmin:
       atomic_name = "fmin";
       atomic_subop = ac_atomic_fmin;
       break;
    case nir_intrinsic_bindless_image_atomic_fmax:
-   case nir_intrinsic_image_deref_atomic_fmax:
       atomic_name = "fmax";
       atomic_subop = ac_atomic_fmax;
       break;
@@ -2855,7 +2777,7 @@ static LLVMValueRef visit_image_atomic(struct ac_nir_context *ctx, const nir_int
 
    LLVMValueRef result;
    if (dim == GLSL_SAMPLER_DIM_BUF) {
-      params[param_count++] = get_image_descriptor(ctx, instr, dynamic_index, AC_DESC_BUFFER, true);
+      params[param_count++] = ctx->abi->load_sampler_desc(ctx->abi, dynamic_index, AC_DESC_BUFFER);
       params[param_count++] = LLVMBuildExtractElement(ctx->ac.builder, get_src(ctx, instr->src[1]),
                                                       ctx->ac.i32_0, ""); /* vindex */
       params[param_count++] = ctx->ac.i32_0;                              /* voffset */
@@ -2883,7 +2805,7 @@ static LLVMValueRef visit_image_atomic(struct ac_nir_context *ctx, const nir_int
       args.data[0] = params[0];
       if (cmpswap)
          args.data[1] = params[1];
-      args.resource = get_image_descriptor(ctx, instr, dynamic_index, AC_DESC_IMAGE, true);
+      args.resource = ctx->abi->load_sampler_desc(ctx->abi, dynamic_index, AC_DESC_IMAGE);
       get_image_coords(ctx, instr, dynamic_index, &args, dim, is_array);
       args.dim = ac_get_image_dim(ctx->ac.gfx_level, dim, is_array);
 
@@ -3781,17 +3703,10 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
    case nir_intrinsic_bindless_image_sparse_load:
    case nir_intrinsic_bindless_image_fragment_load_amd:
    case nir_intrinsic_bindless_image_fragment_mask_load_amd:
-      result = visit_image_load(ctx, instr, true);
-      break;
-   case nir_intrinsic_image_deref_load:
-   case nir_intrinsic_image_deref_sparse_load:
-      result = visit_image_load(ctx, instr, false);
+      result = visit_image_load(ctx, instr);
       break;
    case nir_intrinsic_bindless_image_store:
-      visit_image_store(ctx, instr, true);
-      break;
-   case nir_intrinsic_image_deref_store:
-      visit_image_store(ctx, instr, false);
+      visit_image_store(ctx, instr);
       break;
    case nir_intrinsic_bindless_image_atomic_add:
    case nir_intrinsic_bindless_image_atomic_imin:
@@ -3807,23 +3722,7 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
    case nir_intrinsic_bindless_image_atomic_dec_wrap:
    case nir_intrinsic_bindless_image_atomic_fmin:
    case nir_intrinsic_bindless_image_atomic_fmax:
-      result = visit_image_atomic(ctx, instr, true);
-      break;
-   case nir_intrinsic_image_deref_atomic_add:
-   case nir_intrinsic_image_deref_atomic_imin:
-   case nir_intrinsic_image_deref_atomic_umin:
-   case nir_intrinsic_image_deref_atomic_imax:
-   case nir_intrinsic_image_deref_atomic_umax:
-   case nir_intrinsic_image_deref_atomic_and:
-   case nir_intrinsic_image_deref_atomic_or:
-   case nir_intrinsic_image_deref_atomic_xor:
-   case nir_intrinsic_image_deref_atomic_exchange:
-   case nir_intrinsic_image_deref_atomic_comp_swap:
-   case nir_intrinsic_image_deref_atomic_inc_wrap:
-   case nir_intrinsic_image_deref_atomic_dec_wrap:
-   case nir_intrinsic_image_deref_atomic_fmin:
-   case nir_intrinsic_image_deref_atomic_fmax:
-      result = visit_image_atomic(ctx, instr, false);
+      result = visit_image_atomic(ctx, instr);
       break;
    case nir_intrinsic_shader_clock:
       result = ac_build_shader_clock(&ctx->ac, nir_intrinsic_memory_scope(instr));
@@ -4409,128 +4308,7 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
    return true;
 }
 
-static LLVMValueRef get_bindless_index_from_uniform(struct ac_nir_context *ctx, unsigned base_index,
-                                                    unsigned constant_index,
-                                                    LLVMValueRef dynamic_index)
-{
-   LLVMValueRef offset = LLVMConstInt(ctx->ac.i32, base_index * 4, 0);
-   LLVMValueRef index = LLVMBuildAdd(ctx->ac.builder, dynamic_index,
-                                     LLVMConstInt(ctx->ac.i32, constant_index, 0), "");
-
-   /* Bindless uniforms are 64bit so multiple index by 8 */
-   index = LLVMBuildMul(ctx->ac.builder, index, LLVMConstInt(ctx->ac.i32, 8, 0), "");
-   offset = LLVMBuildAdd(ctx->ac.builder, offset, index, "");
-
-   LLVMValueRef ubo_index = ctx->abi->load_ubo(ctx->abi, ctx->ac.i32_0);
-
-   LLVMValueRef ret =
-      ac_build_buffer_load(&ctx->ac, ubo_index, 1, NULL, offset, NULL, ctx->ac.f32, 0, true, true);
-
-   return LLVMBuildBitCast(ctx->ac.builder, ret, ctx->ac.i32, "");
-}
-
-struct sampler_desc_address {
-   unsigned descriptor_set;
-   unsigned base_index; /* binding in vulkan */
-   unsigned constant_index;
-   LLVMValueRef dynamic_index;
-   bool image;
-   bool bindless;
-};
-
-static struct sampler_desc_address get_sampler_desc_internal(struct ac_nir_context *ctx,
-                                                             nir_deref_instr *deref_instr,
-                                                             const nir_instr *instr, bool image)
-{
-   LLVMValueRef index = NULL;
-   unsigned constant_index = 0;
-   unsigned descriptor_set;
-   unsigned base_index;
-   bool bindless = false;
-
-   if (!deref_instr) {
-      descriptor_set = 0;
-      if (image) {
-         nir_intrinsic_instr *img_instr = nir_instr_as_intrinsic(instr);
-         base_index = 0;
-         bindless = true;
-         index = get_src(ctx, img_instr->src[0]);
-      } else {
-         nir_tex_instr *tex_instr = nir_instr_as_tex(instr);
-         int sampSrcIdx = nir_tex_instr_src_index(tex_instr, nir_tex_src_sampler_handle);
-         if (sampSrcIdx != -1) {
-            base_index = 0;
-            bindless = true;
-            index = get_src(ctx, tex_instr->src[sampSrcIdx].src);
-         } else {
-            assert(tex_instr && !image);
-            base_index = tex_instr->sampler_index;
-         }
-      }
-   } else {
-      while (deref_instr->deref_type != nir_deref_type_var) {
-         if (deref_instr->deref_type == nir_deref_type_array) {
-            unsigned array_size = glsl_get_aoa_size(deref_instr->type);
-            if (!array_size)
-               array_size = 1;
-
-            if (nir_src_is_const(deref_instr->arr.index)) {
-               constant_index += array_size * nir_src_as_uint(deref_instr->arr.index);
-            } else {
-               LLVMValueRef indirect = get_src(ctx, deref_instr->arr.index);
-
-               indirect = LLVMBuildMul(ctx->ac.builder, indirect,
-                                       LLVMConstInt(ctx->ac.i32, array_size, false), "");
-
-               if (!index)
-                  index = indirect;
-               else
-                  index = LLVMBuildAdd(ctx->ac.builder, index, indirect, "");
-            }
-
-            deref_instr = nir_src_as_deref(deref_instr->parent);
-         } else if (deref_instr->deref_type == nir_deref_type_struct) {
-            unsigned sidx = deref_instr->strct.index;
-            deref_instr = nir_src_as_deref(deref_instr->parent);
-            constant_index += glsl_get_struct_location_offset(deref_instr->type, sidx);
-         } else {
-            unreachable("Unsupported deref type");
-         }
-      }
-      descriptor_set = deref_instr->var->data.descriptor_set;
-
-      if (deref_instr->var->data.bindless) {
-         /* For now just assert on unhandled variable types */
-         assert(deref_instr->var->data.mode == nir_var_uniform);
-
-         base_index = deref_instr->var->data.driver_location;
-         bindless = true;
-
-         index = index ? index : ctx->ac.i32_0;
-         index = get_bindless_index_from_uniform(ctx, base_index, constant_index, index);
-      } else
-         base_index = deref_instr->var->data.binding;
-   }
-   return (struct sampler_desc_address){
-      .descriptor_set = descriptor_set,
-      .base_index = base_index,
-      .constant_index = constant_index,
-      .dynamic_index = index,
-      .image = image,
-      .bindless = bindless,
-   };
-}
-
-/* Extract any possibly divergent index into a separate value that can be fed
- * into get_sampler_desc with the same arguments. */
-static LLVMValueRef get_sampler_desc_index(struct ac_nir_context *ctx, nir_deref_instr *deref_instr,
-                                           const nir_instr *instr, bool image)
-{
-   struct sampler_desc_address addr = get_sampler_desc_internal(ctx, deref_instr, instr, image);
-   return addr.dynamic_index;
-}
-
-static LLVMValueRef get_sampler_desc(struct ac_nir_context *ctx, nir_deref_instr *deref_instr,
+static LLVMValueRef get_sampler_desc(struct ac_nir_context *ctx,
                                      enum ac_descriptor_type desc_type, const nir_instr *instr,
                                      LLVMValueRef index, bool image, bool write)
 {
@@ -4571,20 +4349,12 @@ static void tex_fetch_ptrs(struct ac_nir_context *ctx, nir_tex_instr *instr,
    bool sampler_handle_divergent = false;
    LLVMValueRef texture_dynamic_handle = NULL;
    LLVMValueRef sampler_dynamic_handle = NULL;
-   nir_deref_instr *texture_deref_instr = NULL;
-   nir_deref_instr *sampler_deref_instr = NULL;
    int plane = -1;
 
    *res_ptr = NULL;
    *samp_ptr = NULL;
    for (unsigned i = 0; i < instr->num_srcs; i++) {
       switch (instr->src[i].src_type) {
-      case nir_tex_src_texture_deref:
-         texture_deref_instr = nir_src_as_deref(instr->src[i].src);
-         break;
-      case nir_tex_src_sampler_deref:
-         sampler_deref_instr = nir_src_as_deref(instr->src[i].src);
-         break;
       case nir_tex_src_texture_handle:
       case nir_tex_src_sampler_handle: {
          LLVMValueRef val = get_src(ctx, instr->src[i].src);
@@ -4631,54 +4401,6 @@ static void tex_fetch_ptrs(struct ac_nir_context *ctx, nir_tex_instr *instr,
       main_descriptor = AC_DESC_FMASK;
    }
 
-   if (texture_dynamic_handle || sampler_dynamic_handle) {
-      /* instr->sampler_non_uniform and texture_non_uniform are always false in GLSL,
-       * but this can lead to unexpected behavior if texture/sampler index come from
-       * a vertex attribute.
-       * For instance, 2 consecutive draws using 2 different index values,
-       * could be squashed together by the hw - producing a single draw with
-       * non-dynamically uniform index.
-       * To avoid this, detect divergent indexing, and use enter_waterfall.
-       * See https://gitlab.freedesktop.org/mesa/mesa/-/issues/2253.
-       */
-
-      /* descriptor handles given through nir_tex_src_{texture,sampler}_handle */
-      if (instr->texture_non_uniform ||
-          (ctx->abi->use_waterfall_for_divergent_tex_samplers && texture_handle_divergent))
-         texture_dynamic_handle = enter_waterfall(ctx, &wctx[0], texture_dynamic_handle, true);
-
-      if (instr->sampler_non_uniform ||
-         (ctx->abi->use_waterfall_for_divergent_tex_samplers && sampler_handle_divergent))
-         sampler_dynamic_handle = enter_waterfall(ctx, &wctx[1], sampler_dynamic_handle, true);
-
-      if (texture_dynamic_handle)
-         *res_ptr = ctx->abi->load_sampler_desc(ctx->abi, texture_dynamic_handle,
-                                                main_descriptor);
-
-      if (samp_ptr && sampler_dynamic_handle) {
-         *samp_ptr = ctx->abi->load_sampler_desc(ctx->abi, sampler_dynamic_handle,
-                                                 AC_DESC_SAMPLER);
-
-         if (ctx->abi->disable_aniso_single_level &&
-             instr->sampler_dim < GLSL_SAMPLER_DIM_RECT)
-            *samp_ptr = sici_fix_sampler_aniso(ctx, *res_ptr, *samp_ptr);
-      }
-      return;
-   }
-
-   if (*res_ptr) {
-      /* descriptors given through nir_tex_src_{texture,sampler}_handle */
-      return;
-   }
-
-   LLVMValueRef texture_dynamic_index =
-      get_sampler_desc_index(ctx, texture_deref_instr, &instr->instr, false);
-   if (!sampler_deref_instr)
-      sampler_deref_instr = texture_deref_instr;
-
-   LLVMValueRef sampler_dynamic_index =
-      get_sampler_desc_index(ctx, sampler_deref_instr, &instr->instr, false);
-
    /* instr->sampler_non_uniform and texture_non_uniform are always false in GLSL,
     * but this can lead to unexpected behavior if texture/sampler index come from
     * a vertex attribute.
@@ -4688,21 +4410,23 @@ static void tex_fetch_ptrs(struct ac_nir_context *ctx, nir_tex_instr *instr,
     * To avoid this, detect divergent indexing, and use enter_waterfall.
     * See https://gitlab.freedesktop.org/mesa/mesa/-/issues/2253.
     */
+
+   /* descriptor handles given through nir_tex_src_{texture,sampler}_handle */
    if (instr->texture_non_uniform ||
-       (ctx->abi->use_waterfall_for_divergent_tex_samplers && texture_deref_instr->dest.ssa.divergent))
-      texture_dynamic_index = enter_waterfall(ctx, wctx + 0, texture_dynamic_index, true);
+       (ctx->abi->use_waterfall_for_divergent_tex_samplers && texture_handle_divergent))
+      texture_dynamic_handle = enter_waterfall(ctx, &wctx[0], texture_dynamic_handle, true);
 
    if (instr->sampler_non_uniform ||
-       (ctx->abi->use_waterfall_for_divergent_tex_samplers && sampler_deref_instr->dest.ssa.divergent))
-      sampler_dynamic_index = enter_waterfall(ctx, wctx + 1, sampler_dynamic_index, true);
+       (ctx->abi->use_waterfall_for_divergent_tex_samplers && sampler_handle_divergent))
+      sampler_dynamic_handle = enter_waterfall(ctx, &wctx[1], sampler_dynamic_handle, true);
+
+   if (texture_dynamic_handle)
+      *res_ptr = ctx->abi->load_sampler_desc(ctx->abi, texture_dynamic_handle, main_descriptor);
 
-   *res_ptr = get_sampler_desc(ctx, texture_deref_instr, main_descriptor, &instr->instr,
-                               texture_dynamic_index, false, false);
+   if (sampler_dynamic_handle) {
+      *samp_ptr = ctx->abi->load_sampler_desc(ctx->abi, sampler_dynamic_handle, AC_DESC_SAMPLER);
 
-   if (samp_ptr) {
-      *samp_ptr = get_sampler_desc(ctx, sampler_deref_instr, AC_DESC_SAMPLER, &instr->instr,
-                                   sampler_dynamic_index, false, false);
-      if (instr->sampler_dim < GLSL_SAMPLER_DIM_RECT)
+      if (ctx->abi->disable_aniso_single_level && instr->sampler_dim < GLSL_SAMPLER_DIM_RECT)
          *samp_ptr = sici_fix_sampler_aniso(ctx, *res_ptr, *samp_ptr);
    }
 }
-- 
GitLab


From 465c17adc99657693d92a9a195d419c91f0abb39 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Mon, 17 Oct 2022 17:01:35 +0800
Subject: [PATCH 41/45] radeonsi: implement nir_load_ring_gsvs_amd

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/gallium/drivers/radeonsi/si_shader_llvm.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index ff354b197493..afa1b25567ff 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -714,6 +714,9 @@ static LLVMValueRef si_llvm_load_intrinsic(struct ac_shader_abi *abi, nir_intrin
    case nir_intrinsic_load_ring_esgs_amd:
       return ctx->esgs_ring;
 
+   case nir_intrinsic_load_ring_gsvs_amd:
+      return ctx->gsvs_ring[0];
+
    case nir_intrinsic_load_lds_ngg_scratch_base_amd:
       return LLVMBuildBitCast(ctx->ac.builder, ctx->gs_ngg_scratch.value, ctx->ac.i32, "");
 
-- 
GitLab


From a7b112dce9301001c0f11eff20570405086ad0b4 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Mon, 17 Oct 2022 17:16:40 +0800
Subject: [PATCH 42/45] radeonsi: lower nir streamout intrinsics in abi

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/gallium/drivers/radeonsi/si_nir_lower_abi.c | 10 ++++++++++
 1 file changed, 10 insertions(+)

diff --git a/src/gallium/drivers/radeonsi/si_nir_lower_abi.c b/src/gallium/drivers/radeonsi/si_nir_lower_abi.c
index 455ea09cbfdb..a2c150de2268 100644
--- a/src/gallium/drivers/radeonsi/si_nir_lower_abi.c
+++ b/src/gallium/drivers/radeonsi/si_nir_lower_abi.c
@@ -289,6 +289,16 @@ static bool lower_abi_instr(nir_builder *b, nir_instr *instr, struct lower_abi_s
       replacement = nir_ishl_imm(b, offset, 9);
       break;
    }
+   case nir_intrinsic_load_streamout_config_amd:
+      replacement = ac_nir_load_arg(b, &args->ac, args->ac.streamout_config);
+      break;
+   case nir_intrinsic_load_streamout_write_index_amd:
+      replacement = ac_nir_load_arg(b, &args->ac, args->ac.streamout_write_index);
+      break;
+   case nir_intrinsic_load_streamout_offset_amd:
+      replacement =
+         ac_nir_load_arg(b, &args->ac, args->ac.streamout_offset[nir_intrinsic_base(intrin)]);
+      break;
    default:
       return false;
    }
-- 
GitLab


From 756f152682621833f0aa44da6787940aeef58d9f Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Tue, 18 Oct 2022 14:04:12 +0800
Subject: [PATCH 43/45] radeonsi: use ac_nir_lower_legacy_vs to replace
 si_llvm_vs_build_end

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/gallium/drivers/radeonsi/si_shader.c      | 53 +++++++++++++------
 .../drivers/radeonsi/si_shader_internal.h     |  5 +-
 src/gallium/drivers/radeonsi/si_shader_llvm.c | 19 +++----
 .../drivers/radeonsi/si_shader_llvm_vs.c      | 38 -------------
 4 files changed, 47 insertions(+), 68 deletions(-)

diff --git a/src/gallium/drivers/radeonsi/si_shader.c b/src/gallium/drivers/radeonsi/si_shader.c
index 7679b8cbfc41..93cf728e68e9 100644
--- a/src/gallium/drivers/radeonsi/si_shader.c
+++ b/src/gallium/drivers/radeonsi/si_shader.c
@@ -173,12 +173,14 @@ unsigned si_shader_io_get_unique_index(unsigned semantic, bool is_varying)
    }
 }
 
-static void si_dump_streamout(struct pipe_stream_output_info *so)
+static void si_dump_streamout(nir_shader *nir, struct pipe_stream_output_info *so)
 {
    unsigned i;
 
    if (so->num_outputs) {
-      fprintf(stderr, "STREAMOUT\n");
+      fprintf(stderr, "STREAMOUT-FOR-SOURCE: {");
+      _mesa_sha1_print(stderr, nir->info.source_sha1);
+      fprintf(stderr, "}\n");
 
       fprintf(stderr, "  STRIDES: {");
       for (i = 0; i < PIPE_MAX_SO_BUFFERS; i++)
@@ -1920,14 +1922,29 @@ struct nir_shader *si_get_nir_shader(struct si_shader *shader, struct si_shader_
 
    bool opt_offsets = si_lower_io_to_mem(shader, nir, tcs_vgpr_only_inputs);
 
-   /* Assign param export indices. */
-   if (is_last_vgt_stage)
+   if (is_last_vgt_stage) {
+      /* Assign param export indices. */
       si_assign_param_offsets(nir, shader);
 
-   /* Only lower last VGT NGG shader stage. */
-   if (sel->stage <= MESA_SHADER_GEOMETRY && key->ge.as_ngg && !key->ge.as_es) {
-      si_lower_ngg(shader, nir);
-      opt_offsets = true;
+      if (key->ge.as_ngg) {
+         /* Lower last VGT NGG shader stage. */
+         si_lower_ngg(shader, nir);
+         opt_offsets = true;
+      } else if (sel->stage == MESA_SHADER_VERTEX || sel->stage == MESA_SHADER_TESS_EVAL) {
+         /* Lower last VGT none-NGG VS/TES shader stage. */
+         struct pipe_stream_output_info so = {};
+         if (si_shader_uses_streamout(shader))
+            nir_gather_stream_output_info(nir, &so);
+
+         if (si_can_dump_shader(sel->screen, sel->stage) &&
+             !(sel->screen->debug_flags & DBG(NO_NIR)))
+            si_dump_streamout(nir, &so);
+
+         int primitive_id_location =
+            shader->key.ge.mono.u.vs_export_prim_id ? sel->info.num_outputs : -1;
+
+         NIR_PASS_V(nir, ac_nir_lower_legacy_vs, primitive_id_location, &so);
+      }
    }
 
    NIR_PASS(progress2, nir, si_nir_lower_abi, shader, args);
@@ -1982,17 +1999,11 @@ bool si_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *compi
    bool free_nir;
    struct nir_shader *nir = si_get_nir_shader(shader, &args, &free_nir, 0);
 
-   struct pipe_stream_output_info so = {};
-   /* NGG streamout has been lowered to buffer store in nir. */
-   if (!sscreen->use_ngg_streamout && si_shader_uses_streamout(shader))
-      nir_gather_stream_output_info(nir, &so);
-
    /* Dump NIR before doing NIR->LLVM conversion in case the
     * conversion fails. */
    if (si_can_dump_shader(sscreen, sel->stage) &&
        !(sscreen->debug_flags & DBG(NO_NIR))) {
       nir_print_shader(nir, stderr);
-      si_dump_streamout(&so);
    }
 
    /* Initialize vs_output_ps_input_cntl to default. */
@@ -2036,13 +2047,22 @@ bool si_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *compi
     * with PS and NGG VS), but monolithic shaders should be compiled
     * by LLVM due to more complicated compilation.
     */
-   if (!si_llvm_compile_shader(sscreen, compiler, shader, &args, &so, debug, nir, free_nir))
+   if (!si_llvm_compile_shader(sscreen, compiler, shader, &args, debug, nir))
       return false;
 
    shader->config.float_mode = float_mode;
 
    /* The GS copy shader is compiled next. */
    if (sel->stage == MESA_SHADER_GEOMETRY && !shader->key.ge.as_ngg) {
+      struct pipe_stream_output_info so = {};
+      if (si_shader_uses_streamout(shader))
+         nir_gather_stream_output_info(nir, &so);
+
+      if (si_can_dump_shader(sscreen, sel->stage) &&
+          !(sscreen->debug_flags & DBG(NO_NIR))) {
+         si_dump_streamout(nir, &so);
+      }
+
       shader->gs_copy_shader = si_generate_gs_copy_shader(sscreen, compiler, sel, &so, debug);
       if (!shader->gs_copy_shader) {
          fprintf(stderr, "radeonsi: can't create GS copy shader\n");
@@ -2050,6 +2070,9 @@ bool si_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *compi
       }
    }
 
+   if (free_nir)
+      ralloc_free(nir);
+
    /* Compute vs_output_ps_input_cntl. */
    if ((sel->stage == MESA_SHADER_VERTEX ||
         sel->stage == MESA_SHADER_TESS_EVAL ||
diff --git a/src/gallium/drivers/radeonsi/si_shader_internal.h b/src/gallium/drivers/radeonsi/si_shader_internal.h
index b3a78dc49849..6e203c018469 100644
--- a/src/gallium/drivers/radeonsi/si_shader_internal.h
+++ b/src/gallium/drivers/radeonsi/si_shader_internal.h
@@ -229,9 +229,7 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
                            struct nir_shader *nir, bool free_nir);
 bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *compiler,
                             struct si_shader *shader, struct si_shader_args *args,
-                            const struct pipe_stream_output_info *so,
-                            struct util_debug_callback *debug, struct nir_shader *nir,
-                            bool free_nir);
+                            struct util_debug_callback *debug, struct nir_shader *nir);
 
 /* si_shader_llvm_gs.c */
 LLVMValueRef si_is_es_thread(struct si_shader_context *ctx);
@@ -269,7 +267,6 @@ void si_llvm_emit_streamout(struct si_shader_context *ctx, struct si_shader_outp
                             unsigned noutput, unsigned stream);
 void si_llvm_build_vs_exports(struct si_shader_context *ctx,
                               struct si_shader_output_values *outputs, unsigned noutput);
-void si_llvm_vs_build_end(struct si_shader_context *ctx);
 void si_llvm_build_vs_prolog(struct si_shader_context *ctx, union si_shader_part_key *key);
 void si_llvm_init_vs_callbacks(struct si_shader_context *ctx);
 
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index afa1b25567ff..8ed4d30b22c9 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -1023,8 +1023,8 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
    ctx->abi.disable_aniso_single_level = true;
 
    unsigned num_outputs = info->num_outputs;
-   /* need extra output to hold primitive id added by nir ngg lower */
-   if (ctx->stage <= MESA_SHADER_GEOMETRY && shader->key.ge.as_ngg &&
+   /* need extra output to hold primitive id added by nir lower */
+   if (ctx->stage <= MESA_SHADER_GEOMETRY &&
        ctx->shader->key.ge.mono.u.vs_export_prim_id)
       num_outputs++;
 
@@ -1051,8 +1051,6 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
          si_llvm_ls_build_end(ctx);
       else if (shader->key.ge.as_es)
          si_llvm_es_build_end(ctx);
-      else if (!shader->key.ge.as_ngg)
-         si_llvm_vs_build_end(ctx);
       break;
 
    case MESA_SHADER_TESS_CTRL:
@@ -1062,8 +1060,6 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
    case MESA_SHADER_TESS_EVAL:
       if (ctx->shader->key.ge.as_es)
          si_llvm_es_build_end(ctx);
-      else if (!ctx->shader->key.ge.as_ngg)
-         si_llvm_vs_build_end(ctx);
       break;
 
    case MESA_SHADER_GEOMETRY:
@@ -1102,18 +1098,15 @@ static bool si_should_optimize_less(struct ac_llvm_compiler *compiler,
 
 bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *compiler,
                             struct si_shader *shader, struct si_shader_args *args,
-                            const struct pipe_stream_output_info *so,
-                            struct util_debug_callback *debug, struct nir_shader *nir,
-                            bool free_nir)
+                            struct util_debug_callback *debug, struct nir_shader *nir)
 {
    struct si_shader_selector *sel = shader->selector;
    struct si_shader_context ctx;
 
    si_llvm_context_init(&ctx, sscreen, compiler, shader->wave_size);
-   ctx.so = *so;
    ctx.args = args;
 
-   if (!si_llvm_translate_nir(&ctx, shader, nir, free_nir)) {
+   if (!si_llvm_translate_nir(&ctx, shader, nir, false)) {
       si_llvm_dispose(&ctx);
       return false;
    }
@@ -1166,6 +1159,8 @@ bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *
          shader_ls.is_monolithic = true;
 
          si_init_shader_args(&shader_ls, ctx.args);
+
+         bool free_nir;
          nir = si_get_nir_shader(&shader_ls, ctx.args, &free_nir, sel->info.tcs_vgpr_only_inputs);
          si_update_shader_binary_info(shader, nir);
 
@@ -1238,6 +1233,8 @@ bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *
          shader_es.is_monolithic = true;
 
          si_init_shader_args(&shader_es, ctx.args);
+
+         bool free_nir;
          nir = si_get_nir_shader(&shader_es, ctx.args, &free_nir, 0);
          si_update_shader_binary_info(shader, nir);
 
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c b/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c
index 734a15504b89..1aadd00b360d 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c
@@ -724,44 +724,6 @@ void si_llvm_build_vs_exports(struct si_shader_context *ctx,
       ac_build_export(&ctx->ac, &param_exports[i]);
 }
 
-void si_llvm_vs_build_end(struct si_shader_context *ctx)
-{
-   struct si_shader_info *info = &ctx->shader->selector->info;
-   struct si_shader_output_values *outputs = NULL;
-   LLVMValueRef *addrs = ctx->abi.outputs;
-   int i, j;
-
-   assert(!ctx->shader->is_gs_copy_shader);
-   assert(info->num_outputs <= AC_LLVM_MAX_OUTPUTS);
-
-   outputs = MALLOC((info->num_outputs + 1) * sizeof(outputs[0]));
-
-   for (i = 0; i < info->num_outputs; i++) {
-      outputs[i].semantic = info->output_semantic[i];
-
-      for (j = 0; j < 4; j++) {
-         outputs[i].values[j] = LLVMBuildLoad2(ctx->ac.builder, ctx->ac.f32, addrs[4 * i + j], "");
-         outputs[i].vertex_streams = info->output_streams[i];
-      }
-   }
-
-   if (!ctx->screen->use_ngg_streamout && ctx->so.num_outputs)
-      si_llvm_emit_streamout(ctx, outputs, i, 0);
-
-   /* Export PrimitiveID. */
-   if (ctx->shader->key.ge.mono.u.vs_export_prim_id) {
-      outputs[i].semantic = VARYING_SLOT_PRIMITIVE_ID;
-      outputs[i].vertex_streams = 0;
-      outputs[i].values[0] = ac_to_float(&ctx->ac, si_get_primitive_id(ctx, 0));
-      for (j = 1; j < 4; j++)
-         outputs[i].values[j] = LLVMConstReal(ctx->ac.f32, 0);
-      i++;
-   }
-
-   si_llvm_build_vs_exports(ctx, outputs, i);
-   FREE(outputs);
-}
-
 /**
  * Build the vertex shader prolog function.
  *
-- 
GitLab


From 04afc6ba26a18d7addfdafe0f587e1db0178146c Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Tue, 18 Oct 2022 17:09:26 +0800
Subject: [PATCH 44/45] radeonsi: replace llvm gs copy shader generation with
 nir

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/gallium/drivers/radeonsi/si_shader.c      |  97 +++++++-
 src/gallium/drivers/radeonsi/si_shader.h      |   7 -
 .../drivers/radeonsi/si_shader_internal.h     |   7 -
 src/gallium/drivers/radeonsi/si_shader_llvm.c |  13 +-
 .../drivers/radeonsi/si_shader_llvm_gs.c      | 228 ------------------
 .../drivers/radeonsi/si_shader_llvm_vs.c      | 114 ---------
 6 files changed, 97 insertions(+), 369 deletions(-)

diff --git a/src/gallium/drivers/radeonsi/si_shader.c b/src/gallium/drivers/radeonsi/si_shader.c
index 93cf728e68e9..1678f78917ae 100644
--- a/src/gallium/drivers/radeonsi/si_shader.c
+++ b/src/gallium/drivers/radeonsi/si_shader.c
@@ -70,7 +70,7 @@ bool si_is_multi_part_shader(struct si_shader *shader)
 /** Whether the shader runs on a merged HW stage (LSHS or ESGS) */
 bool si_is_merged_shader(struct si_shader *shader)
 {
-   if (shader->selector->stage > MESA_SHADER_GEOMETRY)
+   if (shader->selector->stage > MESA_SHADER_GEOMETRY || shader->is_gs_copy_shader)
       return false;
 
    return shader->key.ge.as_ngg || si_is_multi_part_shader(shader);
@@ -1988,6 +1988,90 @@ void si_update_shader_binary_info(struct si_shader *shader, nir_shader *nir)
    shader->info.uses_vmem_sampler_or_bvh |= info.uses_vmem_sampler_or_bvh;
 }
 
+/* Generate code for the hardware VS shader stage to go with a geometry shader */
+static struct si_shader *si_generate_gs_copy_shader(struct si_screen *sscreen,
+                                                    struct ac_llvm_compiler *compiler,
+                                                    struct si_shader *gs_shader,
+                                                    nir_shader *gs_nir,
+                                                    struct util_debug_callback *debug)
+{
+   struct si_shader *shader;
+   struct si_shader_selector *gs_selector = gs_shader->selector;
+   struct si_shader_info *gsinfo = &gs_selector->info;
+
+   shader = CALLOC_STRUCT(si_shader);
+   if (!shader)
+      return NULL;
+
+   /* We can leave the fence as permanently signaled because the GS copy
+    * shader only becomes visible globally after it has been compiled. */
+   util_queue_fence_init(&shader->ready);
+
+   shader->selector = gs_selector;
+   shader->is_gs_copy_shader = true;
+   shader->wave_size = si_determine_wave_size(sscreen, shader);
+
+   STATIC_ASSERT(sizeof(shader->info.vs_output_param_offset[0]) == 1);
+   memset(shader->info.vs_output_param_offset, AC_EXP_PARAM_DEFAULT_VAL_0000,
+          sizeof(shader->info.vs_output_param_offset));
+
+   for (unsigned i = 0; i < gsinfo->num_outputs; i++) {
+      unsigned semantic = gsinfo->output_semantic[i];
+
+      /* Skip if no channel writes to stream 0. */
+      if (!nir_slot_is_varying(semantic) ||
+          (gsinfo->output_streams[i] & 0x03 &&
+           gsinfo->output_streams[i] & 0x0c &&
+           gsinfo->output_streams[i] & 0x30 &&
+           gsinfo->output_streams[i] & 0xc0))
+         continue;
+
+      shader->info.vs_output_param_offset[semantic] = shader->info.nr_param_exports++;
+      shader->info.vs_output_param_mask |= BITFIELD64_BIT(i);
+   }
+
+   struct pipe_stream_output_info so = {};
+   if (si_shader_uses_streamout(gs_shader))
+      nir_gather_stream_output_info(gs_nir, &so);
+
+   nir_shader *nir = ac_nir_create_gs_copy_shader(
+      gs_nir, &so, gsinfo->num_outputs, gsinfo->output_usagemask, gsinfo->output_streams,
+      gsinfo->output_semantic, gsinfo->num_stream_output_components);
+
+   /* used in si_nir_clamp_vertex_color */
+   nir->info.outputs_written = gsinfo->base.outputs_written;
+   NIR_PASS_V(nir, si_nir_clamp_vertex_color);
+
+   struct si_shader_args args;
+   si_init_shader_args(shader, &args);
+
+   NIR_PASS_V(nir, si_nir_lower_abi, shader, &args);
+
+   if (si_can_dump_shader(sscreen, MESA_SHADER_GEOMETRY)) {
+      fprintf(stderr, "GS Copy Shader:\n");
+      if (!(sscreen->debug_flags & DBG(NO_NIR))) {
+         nir_print_shader(nir, stderr);
+         si_dump_streamout(gs_nir, &so);
+      }
+   }
+
+   bool ok = false;
+   if (si_llvm_compile_shader(sscreen, compiler, shader, &args, debug, nir)) {
+      assert(!shader->config.scratch_bytes_per_wave);
+      ok = si_shader_binary_upload(sscreen, shader, 0);
+      si_shader_dump(sscreen, shader, debug, stderr, true);
+   }
+   ralloc_free(nir);
+
+   if (!ok) {
+      FREE(shader);
+      shader = NULL;
+   } else {
+      si_fix_resource_usage(sscreen, shader);
+   }
+   return shader;
+}
+
 bool si_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *compiler,
                        struct si_shader *shader, struct util_debug_callback *debug)
 {
@@ -2054,16 +2138,7 @@ bool si_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *compi
 
    /* The GS copy shader is compiled next. */
    if (sel->stage == MESA_SHADER_GEOMETRY && !shader->key.ge.as_ngg) {
-      struct pipe_stream_output_info so = {};
-      if (si_shader_uses_streamout(shader))
-         nir_gather_stream_output_info(nir, &so);
-
-      if (si_can_dump_shader(sscreen, sel->stage) &&
-          !(sscreen->debug_flags & DBG(NO_NIR))) {
-         si_dump_streamout(nir, &so);
-      }
-
-      shader->gs_copy_shader = si_generate_gs_copy_shader(sscreen, compiler, sel, &so, debug);
+      shader->gs_copy_shader = si_generate_gs_copy_shader(sscreen, compiler, shader, nir, debug);
       if (!shader->gs_copy_shader) {
          fprintf(stderr, "radeonsi: can't create GS copy shader\n");
          return false;
diff --git a/src/gallium/drivers/radeonsi/si_shader.h b/src/gallium/drivers/radeonsi/si_shader.h
index 53c589f2e8b1..3937c7bbb3b5 100644
--- a/src/gallium/drivers/radeonsi/si_shader.h
+++ b/src/gallium/drivers/radeonsi/si_shader.h
@@ -993,13 +993,6 @@ bool si_get_external_symbol(enum amd_gfx_level gfx_level, void *data, const char
 void si_nir_scan_shader(struct si_screen *sscreen,  const struct nir_shader *nir,
                         struct si_shader_info *info);
 
-/* si_shader_llvm_gs.c */
-struct si_shader *si_generate_gs_copy_shader(struct si_screen *sscreen,
-                                             struct ac_llvm_compiler *compiler,
-                                             struct si_shader_selector *gs_selector,
-                                             const struct pipe_stream_output_info *so,
-                                             struct util_debug_callback *debug);
-
 /* si_shader_nir.c */
 extern const nir_lower_subgroups_options si_nir_subgroups_options;
 
diff --git a/src/gallium/drivers/radeonsi/si_shader_internal.h b/src/gallium/drivers/radeonsi/si_shader_internal.h
index 6e203c018469..02bde35af694 100644
--- a/src/gallium/drivers/radeonsi/si_shader_internal.h
+++ b/src/gallium/drivers/radeonsi/si_shader_internal.h
@@ -118,7 +118,6 @@ struct si_shader_context {
    struct ac_llvm_context ac;
    struct si_shader *shader;
    struct si_screen *screen;
-   struct pipe_stream_output_info so;
 
    gl_shader_stage stage;
 
@@ -259,12 +258,6 @@ void si_llvm_init_ps_callbacks(struct si_shader_context *ctx);
 /* si_shader_llvm_vs.c */
 void si_llvm_clipvertex_to_clipdist(struct si_shader_context *ctx,
                                     struct ac_export_args clipdist[2], LLVMValueRef clipvertex[4]);
-void si_llvm_streamout_store_output(struct si_shader_context *ctx, LLVMValueRef const *so_buffers,
-                                    LLVMValueRef const *so_write_offsets,
-                                    struct pipe_stream_output *stream_out,
-                                    struct si_shader_output_values *shader_out);
-void si_llvm_emit_streamout(struct si_shader_context *ctx, struct si_shader_output_values *outputs,
-                            unsigned noutput, unsigned stream);
 void si_llvm_build_vs_exports(struct si_shader_context *ctx,
                               struct si_shader_output_values *outputs, unsigned noutput);
 void si_llvm_build_vs_prolog(struct si_shader_context *ctx, union si_shader_part_key *key);
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index 8ed4d30b22c9..910024e15117 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -780,7 +780,7 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
    const struct si_shader_info *info = &sel->info;
 
    ctx->shader = shader;
-   ctx->stage = sel->stage;
+   ctx->stage = shader->is_gs_copy_shader ? MESA_SHADER_VERTEX : sel->stage;
 
    ctx->num_const_buffers = info->base.num_ubos;
    ctx->num_shader_buffers = info->base.num_ssbos;
@@ -810,6 +810,15 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
             ac_build_load_to_sgpr(
                &ctx->ac, buf, LLVMConstInt(ctx->ac.i32, SI_VS_CONST_INSTANCE_DIVISORS, 0));
       }
+
+      /* preload GSVS ring for GS copy shader */
+      if (shader->is_gs_copy_shader) {
+         ctx->gsvs_ring[0] =
+            ac_build_load_to_sgpr(
+               &ctx->ac,
+               ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->internal_bindings),
+               LLVMConstInt(ctx->ac.i32, SI_RING_GSVS, 0));
+      }
       break;
 
    case MESA_SHADER_TESS_CTRL:
@@ -1045,7 +1054,7 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
    if (!ac_nir_translate(&ctx->ac, &ctx->abi, &ctx->args->ac, nir))
       return false;
 
-   switch (sel->stage) {
+   switch (ctx->stage) {
    case MESA_SHADER_VERTEX:
       if (shader->key.ge.as_ls)
          si_llvm_ls_build_end(ctx);
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c b/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c
index f3c4cb0aff6c..36c07d21a7e1 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c
@@ -339,234 +339,6 @@ void si_preload_gs_rings(struct si_shader_context *ctx)
    }
 }
 
-/**
- * Vertex color clamping.
- *
- * This uses a state constant loaded in a user data SGPR and
- * an IF statement is added that clamps all colors if the constant
- * is true.
- */
-static void si_vertex_color_clamping(struct si_shader_context *ctx,
-                                     struct si_shader_output_values *outputs, unsigned noutput)
-{
-   LLVMValueRef addr[SI_MAX_VS_OUTPUTS][4];
-   bool has_colors = false;
-
-   /* Store original colors to alloca variables. */
-   for (unsigned i = 0; i < noutput; i++) {
-      if (outputs[i].semantic != VARYING_SLOT_COL0 &&
-          outputs[i].semantic != VARYING_SLOT_COL1 &&
-          outputs[i].semantic != VARYING_SLOT_BFC0 &&
-          outputs[i].semantic != VARYING_SLOT_BFC1)
-         continue;
-
-      for (unsigned j = 0; j < 4; j++)
-         addr[i][j] = ac_build_alloca_init(&ctx->ac, outputs[i].values[j], "");
-
-      has_colors = true;
-   }
-
-   if (!has_colors)
-      return;
-
-   /* The state is in the first bit of the user SGPR. */
-   LLVMValueRef cond = GET_FIELD(ctx, VS_STATE_CLAMP_VERTEX_COLOR);
-   cond = LLVMBuildTrunc(ctx->ac.builder, cond, ctx->ac.i1, "");
-
-   ac_build_ifcc(&ctx->ac, cond, 6502);
-
-   /* Store clamped colors to alloca variables within the conditional block. */
-   for (unsigned i = 0; i < noutput; i++) {
-      if (outputs[i].semantic != VARYING_SLOT_COL0 &&
-          outputs[i].semantic != VARYING_SLOT_COL1 &&
-          outputs[i].semantic != VARYING_SLOT_BFC0 &&
-          outputs[i].semantic != VARYING_SLOT_BFC1)
-         continue;
-
-      for (unsigned j = 0; j < 4; j++) {
-         LLVMBuildStore(ctx->ac.builder, ac_build_clamp(&ctx->ac, outputs[i].values[j]),
-                        addr[i][j]);
-      }
-   }
-   ac_build_endif(&ctx->ac, 6502);
-
-   /* Load clamped colors */
-   for (unsigned i = 0; i < noutput; i++) {
-      if (outputs[i].semantic != VARYING_SLOT_COL0 &&
-          outputs[i].semantic != VARYING_SLOT_COL1 &&
-          outputs[i].semantic != VARYING_SLOT_BFC0 &&
-          outputs[i].semantic != VARYING_SLOT_BFC1)
-         continue;
-
-      for (unsigned j = 0; j < 4; j++) {
-         outputs[i].values[j] = LLVMBuildLoad2(ctx->ac.builder, ctx->ac.f32, addr[i][j], "");
-      }
-   }
-}
-
-/* Generate code for the hardware VS shader stage to go with a geometry shader */
-struct si_shader *si_generate_gs_copy_shader(struct si_screen *sscreen,
-                                             struct ac_llvm_compiler *compiler,
-                                             struct si_shader_selector *gs_selector,
-                                             const struct pipe_stream_output_info *so,
-                                             struct util_debug_callback *debug)
-{
-   struct si_shader_context ctx;
-   struct si_shader *shader;
-   LLVMBuilderRef builder;
-   struct si_shader_output_values outputs[SI_MAX_VS_OUTPUTS];
-   struct si_shader_info *gsinfo = &gs_selector->info;
-   int i;
-
-   shader = CALLOC_STRUCT(si_shader);
-   if (!shader)
-      return NULL;
-
-   /* We can leave the fence as permanently signaled because the GS copy
-    * shader only becomes visible globally after it has been compiled. */
-   util_queue_fence_init(&shader->ready);
-
-   shader->selector = gs_selector;
-   shader->is_gs_copy_shader = true;
-   shader->wave_size = si_determine_wave_size(sscreen, shader);
-
-   STATIC_ASSERT(sizeof(shader->info.vs_output_param_offset[0]) == 1);
-   memset(shader->info.vs_output_param_offset, AC_EXP_PARAM_DEFAULT_VAL_0000,
-          sizeof(shader->info.vs_output_param_offset));
-
-   for (unsigned i = 0; i < gsinfo->num_outputs; i++) {
-      unsigned semantic = gsinfo->output_semantic[i];
-
-      /* Skip if no channel writes to stream 0. */
-      if (!nir_slot_is_varying(semantic) ||
-          (gsinfo->output_streams[i] & 0x03 &&
-           gsinfo->output_streams[i] & 0x0c &&
-           gsinfo->output_streams[i] & 0x30 &&
-           gsinfo->output_streams[i] & 0xc0))
-         continue;
-
-      shader->info.vs_output_param_offset[semantic] = shader->info.nr_param_exports++;
-      shader->info.vs_output_param_mask |= BITFIELD64_BIT(i);
-   }
-
-   si_llvm_context_init(&ctx, sscreen, compiler, shader->wave_size);
-   ctx.shader = shader;
-   ctx.stage = MESA_SHADER_VERTEX;
-   ctx.so = *so;
-
-   struct si_shader_args args;
-   si_init_shader_args(shader, &args);
-   ctx.args = &args;
-
-   builder = ctx.ac.builder;
-
-   /* Build the main function. */
-   si_llvm_create_main_func(&ctx);
-
-   ctx.gsvs_ring[0] =
-      ac_build_load_to_sgpr(&ctx.ac,
-         ac_get_ptr_arg(&ctx.ac, &ctx.args->ac, ctx.args->internal_bindings), LLVMConstInt(ctx.ac.i32, SI_RING_GSVS, 0));
-
-   LLVMValueRef voffset =
-      LLVMBuildMul(ctx.ac.builder, ctx.abi.vertex_id, LLVMConstInt(ctx.ac.i32, 4, 0), "");
-
-   /* Fetch the vertex stream ID.*/
-   LLVMValueRef stream_id;
-
-   if (!sscreen->use_ngg_streamout && ctx.so.num_outputs)
-      stream_id = si_unpack_param(&ctx, ctx.args->ac.streamout_config, 24, 2);
-   else
-      stream_id = ctx.ac.i32_0;
-
-   /* Fill in output information. */
-   for (i = 0; i < gsinfo->num_outputs; ++i) {
-      outputs[i].semantic = gsinfo->output_semantic[i];
-      outputs[i].vertex_streams = gsinfo->output_streams[i];
-   }
-
-   LLVMBasicBlockRef end_bb;
-   LLVMValueRef switch_inst;
-
-   end_bb = LLVMAppendBasicBlockInContext(ctx.ac.context, ctx.main_fn.value, "end");
-   switch_inst = LLVMBuildSwitch(builder, stream_id, end_bb, 4);
-
-   for (int stream = 0; stream < 4; stream++) {
-      LLVMBasicBlockRef bb;
-      unsigned offset;
-
-      if (!gsinfo->num_stream_output_components[stream])
-         continue;
-
-      if (stream > 0 && !ctx.so.num_outputs)
-         continue;
-
-      bb = LLVMInsertBasicBlockInContext(ctx.ac.context, end_bb, "out");
-      LLVMAddCase(switch_inst, LLVMConstInt(ctx.ac.i32, stream, 0), bb);
-      LLVMPositionBuilderAtEnd(builder, bb);
-
-      /* Fetch vertex data from GSVS ring */
-      offset = 0;
-      for (i = 0; i < gsinfo->num_outputs; ++i) {
-         for (unsigned chan = 0; chan < 4; chan++) {
-            if (!(gsinfo->output_usagemask[i] & (1 << chan)) ||
-                ((outputs[i].vertex_streams >> (chan * 2)) & 0x3) != stream) {
-               outputs[i].values[chan] = LLVMGetUndef(ctx.ac.f32);
-               continue;
-            }
-
-            LLVMValueRef soffset =
-               LLVMConstInt(ctx.ac.i32, offset * gs_selector->info.base.gs.vertices_out * 16 * 4, 0);
-            offset++;
-
-            outputs[i].values[chan] =
-               ac_build_buffer_load(&ctx.ac, ctx.gsvs_ring[0], 1, ctx.ac.i32_0, voffset, soffset,
-                                    ctx.ac.f32, ac_glc | ac_slc, true, false);
-         }
-      }
-
-      /* Streamout and exports. */
-      if (!sscreen->use_ngg_streamout && ctx.so.num_outputs) {
-         si_llvm_emit_streamout(&ctx, outputs, gsinfo->num_outputs, stream);
-      }
-
-      if (stream == 0) {
-         si_vertex_color_clamping(&ctx, outputs, gsinfo->num_outputs);
-         si_llvm_build_vs_exports(&ctx, outputs, gsinfo->num_outputs);
-      }
-
-      LLVMBuildBr(builder, end_bb);
-   }
-
-   LLVMPositionBuilderAtEnd(builder, end_bb);
-
-   LLVMBuildRetVoid(ctx.ac.builder);
-
-   ctx.stage = MESA_SHADER_GEOMETRY; /* override for shader dumping */
-   si_llvm_optimize_module(&ctx);
-
-   bool ok = false;
-   if (si_compile_llvm(sscreen, &ctx.shader->binary, &ctx.shader->config, ctx.compiler, &ctx.ac,
-                       debug, MESA_SHADER_GEOMETRY, "GS Copy Shader", false)) {
-      assert(!ctx.shader->config.scratch_bytes_per_wave);
-      if (!ctx.shader->config.scratch_bytes_per_wave)
-         ok = si_shader_binary_upload(sscreen, ctx.shader, 0);
-
-      if (si_can_dump_shader(sscreen, MESA_SHADER_GEOMETRY))
-         fprintf(stderr, "GS Copy Shader:\n");
-      si_shader_dump(sscreen, ctx.shader, debug, stderr, true);
-   }
-
-   si_llvm_dispose(&ctx);
-
-   if (!ok) {
-      FREE(shader);
-      shader = NULL;
-   } else {
-      si_fix_resource_usage(sscreen, shader);
-   }
-   return shader;
-}
-
 void si_llvm_init_gs_callbacks(struct si_shader_context *ctx)
 {
    ctx->abi.emit_vertex_with_counter = si_llvm_emit_vertex;
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c b/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c
index 1aadd00b360d..1679f6863eb2 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm_vs.c
@@ -330,120 +330,6 @@ static LLVMValueRef si_load_vs_input(struct ac_shader_abi *abi, unsigned driver_
    return ac_build_varying_gather_values(&ctx->ac, values, num_components, component);
 }
 
-void si_llvm_streamout_store_output(struct si_shader_context *ctx, LLVMValueRef const *so_buffers,
-                                    LLVMValueRef const *so_write_offsets,
-                                    struct pipe_stream_output *stream_out,
-                                    struct si_shader_output_values *shader_out)
-{
-   unsigned buf_idx = stream_out->output_buffer;
-   unsigned start = stream_out->start_component;
-   unsigned num_comps = stream_out->num_components;
-   LLVMValueRef out[4];
-
-   assert(num_comps && num_comps <= 4);
-   if (!num_comps || num_comps > 4)
-      return;
-
-   /* Load the output as int. */
-   for (int j = 0; j < num_comps; j++) {
-      assert(stream_out->stream == ((shader_out->vertex_streams >> ((start + j) * 2)) & 0x3));
-
-      out[j] = ac_to_integer(&ctx->ac, shader_out->values[start + j]);
-   }
-
-   /* Pack the output. */
-   LLVMValueRef vdata = NULL;
-
-   switch (num_comps) {
-   case 1: /* as i32 */
-      vdata = out[0];
-      break;
-   case 2: /* as v2i32 */
-   case 3: /* as v3i32 */
-   case 4: /* as v4i32 */
-      vdata = ac_build_gather_values(&ctx->ac, out, num_comps);
-      break;
-   }
-
-   ac_build_buffer_store_dword(&ctx->ac, so_buffers[buf_idx], vdata, NULL,
-                               LLVMBuildAdd(ctx->ac.builder, so_write_offsets[buf_idx],
-                                            LLVMConstInt(ctx->ac.i32, stream_out->dst_offset * 4, 0), ""),
-                               ctx->ac.i32_0, ac_glc | ac_slc);
-}
-
-/**
- * Write streamout data to buffers for vertex stream @p stream (different
- * vertex streams can occur for GS copy shaders).
- */
-void si_llvm_emit_streamout(struct si_shader_context *ctx, struct si_shader_output_values *outputs,
-                            unsigned noutput, unsigned stream)
-{
-   struct pipe_stream_output_info *so = &ctx->so;
-   LLVMBuilderRef builder = ctx->ac.builder;
-   int i;
-
-   /* Get bits [22:16], i.e. (so_param >> 16) & 127; */
-   LLVMValueRef so_vtx_count = si_unpack_param(ctx, ctx->args->ac.streamout_config, 16, 7);
-
-   LLVMValueRef tid = ac_get_thread_id(&ctx->ac);
-
-   /* can_emit = tid < so_vtx_count; */
-   LLVMValueRef can_emit = LLVMBuildICmp(builder, LLVMIntULT, tid, so_vtx_count, "");
-
-   /* Emit the streamout code conditionally. This actually avoids
-    * out-of-bounds buffer access. The hw tells us via the SGPR
-    * (so_vtx_count) which threads are allowed to emit streamout data. */
-   ac_build_ifcc(&ctx->ac, can_emit, 6501);
-   {
-      /* The buffer offset is computed as follows:
-       *   ByteOffset = streamout_offset[buffer_id]*4 +
-       *                (streamout_write_index + thread_id)*stride[buffer_id] +
-       *                attrib_offset
-       */
-
-      LLVMValueRef so_write_index = ac_get_arg(&ctx->ac, ctx->args->ac.streamout_write_index);
-
-      /* Compute (streamout_write_index + thread_id). */
-      so_write_index = LLVMBuildAdd(builder, so_write_index, tid, "");
-
-      /* Load the descriptor and compute the write offset for each
-       * enabled buffer. */
-      LLVMValueRef so_write_offset[4] = {};
-      LLVMValueRef so_buffers[4];
-      struct ac_llvm_pointer arg = ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->internal_bindings);
-
-      for (i = 0; i < 4; i++) {
-         if (!so->stride[i])
-            continue;
-
-         LLVMValueRef offset = LLVMConstInt(ctx->ac.i32, SI_VS_STREAMOUT_BUF0 + i, 0);
-
-         so_buffers[i] = ac_build_load_to_sgpr(&ctx->ac, arg, offset);
-
-         LLVMValueRef so_offset = ac_get_arg(&ctx->ac, ctx->args->ac.streamout_offset[i]);
-         so_offset = LLVMBuildMul(builder, so_offset, LLVMConstInt(ctx->ac.i32, 4, 0), "");
-
-         so_write_offset[i] = ac_build_imad(
-            &ctx->ac, so_write_index, LLVMConstInt(ctx->ac.i32, so->stride[i] * 4, 0), so_offset);
-      }
-
-      /* Write streamout data. */
-      for (i = 0; i < so->num_outputs; i++) {
-         unsigned reg = so->output[i].register_index;
-
-         if (reg >= noutput)
-            continue;
-
-         if (stream != so->output[i].stream)
-            continue;
-
-         si_llvm_streamout_store_output(ctx, so_buffers, so_write_offset, &so->output[i],
-                                        &outputs[reg]);
-      }
-   }
-   ac_build_endif(&ctx->ac, 6501);
-}
-
 void si_llvm_clipvertex_to_clipdist(struct si_shader_context *ctx,
                                     struct ac_export_args clipdist[2], LLVMValueRef clipvertex[4])
 {
-- 
GitLab


From c20b0de2304a74b8827d8a4421f705c7640b33b1 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Wed, 19 Oct 2022 10:26:14 +0800
Subject: [PATCH 45/45] radeonsi: move gfx10_ngg_export_vertex to
 si_shader_llvm.c

It's now also used by none-ngg pipeline and older GPUs.

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 .../drivers/radeonsi/gfx10_shader_ngg.c       | 29 -----------------
 .../drivers/radeonsi/si_shader_internal.h     |  1 -
 src/gallium/drivers/radeonsi/si_shader_llvm.c | 31 ++++++++++++++++++-
 3 files changed, 30 insertions(+), 31 deletions(-)

diff --git a/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c b/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
index 17d0bea74418..7616af5b9313 100644
--- a/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
+++ b/src/gallium/drivers/radeonsi/gfx10_shader_ngg.c
@@ -95,35 +95,6 @@ bool gfx10_ngg_export_prim_early(struct si_shader *shader)
           !gfx10_ngg_writes_user_edgeflags(shader);
 }
 
-void gfx10_ngg_export_vertex(struct ac_shader_abi *abi)
-{
-   struct si_shader_context *ctx = si_shader_context_from_abi(abi);
-   struct si_shader_info *info = &ctx->shader->selector->info;
-   struct si_shader_output_values outputs[PIPE_MAX_SHADER_OUTPUTS];
-   LLVMValueRef *addrs = ctx->abi.outputs;
-
-   unsigned num_outputs = info->num_outputs;
-   /* if needed, nir ngg lower will append primitive id export at last */
-   if (ctx->shader->key.ge.mono.u.vs_export_prim_id)
-      num_outputs++;
-
-   for (unsigned i = 0; i < num_outputs; i++) {
-      if (i < info->num_outputs) {
-         outputs[i].semantic = info->output_semantic[i];
-         outputs[i].vertex_streams = info->output_streams[i];
-      } else {
-         outputs[i].semantic = VARYING_SLOT_PRIMITIVE_ID;
-         outputs[i].vertex_streams = 0;
-      }
-
-      for (unsigned j = 0; j < 4; j++)
-         outputs[i].values[j] =
-            LLVMBuildLoad2(ctx->ac.builder, ctx->ac.f32, addrs[4 * i + j], "");
-   }
-
-   si_llvm_build_vs_exports(ctx, outputs, num_outputs);
-}
-
 void gfx10_ngg_gs_emit_begin(struct si_shader_context *ctx)
 {
    LLVMBuilderRef builder = ctx->ac.builder;
diff --git a/src/gallium/drivers/radeonsi/si_shader_internal.h b/src/gallium/drivers/radeonsi/si_shader_internal.h
index 02bde35af694..a4bd77bb2a56 100644
--- a/src/gallium/drivers/radeonsi/si_shader_internal.h
+++ b/src/gallium/drivers/radeonsi/si_shader_internal.h
@@ -181,7 +181,6 @@ void si_fix_resource_usage(struct si_screen *sscreen, struct si_shader *shader);
 LLVMValueRef gfx10_get_thread_id_in_tg(struct si_shader_context *ctx);
 unsigned gfx10_ngg_get_vertices_per_prim(struct si_shader *shader);
 bool gfx10_ngg_export_prim_early(struct si_shader *shader);
-void gfx10_ngg_export_vertex(struct ac_shader_abi *abi);
 void gfx10_ngg_gs_emit_begin(struct si_shader_context *ctx);
 unsigned gfx10_ngg_get_scratch_dw_size(struct si_shader *shader);
 bool gfx10_ngg_calculate_subgroup_info(struct si_shader *shader);
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index 910024e15117..629201b9df83 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -773,6 +773,35 @@ static LLVMValueRef si_llvm_load_sampler_desc(struct ac_shader_abi *abi, LLVMVal
    return index;
 }
 
+static void si_llvm_export_vertex(struct ac_shader_abi *abi)
+{
+   struct si_shader_context *ctx = si_shader_context_from_abi(abi);
+   struct si_shader_info *info = &ctx->shader->selector->info;
+   struct si_shader_output_values outputs[PIPE_MAX_SHADER_OUTPUTS];
+   LLVMValueRef *addrs = ctx->abi.outputs;
+
+   unsigned num_outputs = info->num_outputs;
+   /* if needed, nir lower will append primitive id export at last */
+   if (ctx->shader->key.ge.mono.u.vs_export_prim_id)
+      num_outputs++;
+
+   for (unsigned i = 0; i < num_outputs; i++) {
+      if (i < info->num_outputs) {
+         outputs[i].semantic = info->output_semantic[i];
+         outputs[i].vertex_streams = info->output_streams[i];
+      } else {
+         outputs[i].semantic = VARYING_SLOT_PRIMITIVE_ID;
+         outputs[i].vertex_streams = 0;
+      }
+
+      for (unsigned j = 0; j < 4; j++)
+         outputs[i].values[j] =
+            LLVMBuildLoad2(ctx->ac.builder, ctx->ac.f32, addrs[4 * i + j], "");
+   }
+
+   si_llvm_build_vs_exports(ctx, outputs, num_outputs);
+}
+
 bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shader,
                            struct nir_shader *nir, bool free_nir)
 {
@@ -789,7 +818,7 @@ bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shader *shad
    ctx->num_images = info->base.num_images;
 
    ctx->abi.intrinsic_load = si_llvm_load_intrinsic;
-   ctx->abi.export_vertex = gfx10_ngg_export_vertex;
+   ctx->abi.export_vertex = si_llvm_export_vertex;
    ctx->abi.load_sampler_desc = si_llvm_load_sampler_desc;
 
    si_llvm_create_main_func(ctx);
-- 
GitLab

