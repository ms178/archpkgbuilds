From 0ca81084b82dc479a49e4f0837cabc691a751219 Mon Sep 17 00:00:00 2001
From: Ian Romanick <ian.d.romanick@intel.com>
Date: Tue, 21 May 2024 19:02:46 -0700
Subject: [PATCH] nir/algebraic: Optimize many patterns involving isnan or
 isfinite

People sure are creative with ways to open-code isfinite. This doesn't
cover all the possible patterns, but it does capture everything that I
have observed "in the wild." With these patterns added, nothing in my
fossil-db generates any `X cmp X`.

I was originally looking to see if there was a use for Intel's CMPN
instruction. This instruction is `(x cmp y) || isnan(y)`.

shader-db:

All Intel platforms had similar results. (Meteor Lake shown)
total instructions in shared programs: 19874221 -> 19873885 (<.01%)
instructions in affected programs: 106530 -> 106194 (-0.32%)
helped: 103 / HURT: 0

total cycles in shared programs: 936154303 -> 936141744 (<.01%)
cycles in affected programs: 14866221 -> 14853662 (-0.08%)
helped: 87 / HURT: 16

fossil-db:

All Intel platforms had similar results. (Meteor Lake shown)
Totals:
Instrs: 153527262 -> 153442540 (-0.06%); split: -0.06%, +0.00%
Cycle count: 16426316190 -> 16424941342 (-0.01%); split: -0.01%, +0.00%
Spill count: 130979 -> 130981 (+0.00%)
Fill count: 236742 -> 236740 (-0.00%); split: -0.00%, +0.00%
Max live registers: 32569234 -> 32587354 (+0.06%); split: -0.01%, +0.06%
Max dispatch width: 5545768 -> 5542440 (-0.06%); split: +0.06%, -0.12%

Totals from 63496 (10.05% of 631598) affected shaders:
Instrs: 13874124 -> 13789402 (-0.61%); split: -0.61%, +0.00%
Cycle count: 326654593 -> 325279745 (-0.42%); split: -0.64%, +0.22%
Spill count: 3014 -> 3016 (+0.07%)
Fill count: 7873 -> 7871 (-0.03%); split: -0.05%, +0.03%
Max live registers: 3686647 -> 3704767 (+0.49%); split: -0.06%, +0.55%
Max dispatch width: 650080 -> 646752 (-0.51%); split: +0.51%, -1.02%
---
 src/compiler/nir/nir_opt_algebraic.py | 65 +++++++++++++++++++++++++--
 1 file changed, 61 insertions(+), 4 deletions(-)

diff --git a/src/compiler/nir/nir_opt_algebraic.py b/src/compiler/nir/nir_opt_algebraic.py
index dfee4ea3b9398..7c8d2c121c60d 100644
--- a/src/compiler/nir/nir_opt_algebraic.py
+++ b/src/compiler/nir/nir_opt_algebraic.py
@@ -2724,6 +2724,13 @@ for op in ['flt', 'fge', 'feq']:
    optimizations += [
       (('iand', ('feq', a, a), (op, a, b)), ('!' + op, a, b)),
       (('iand', ('feq', a, a), (op, b, a)), ('!' + op, b, a)),
+      (('iand', ('fge', a, a), (op, a, b)), ('!' + op, a, b)),
+      (('iand', ('fge', a, a), (op, b, a)), ('!' + op, b, a)),
+
+      (('iand', ('feq', a, a), ('iand', (op, a, b), c)), ('iand', ('!' + op, a, b), c)),
+      (('iand', ('feq', a, a), ('iand', (op, b, a), c)), ('iand', ('!' + op, b, a), c)),
+      (('iand', ('fge', a, a), ('iand', (op, a, b), c)), ('iand', ('!' + op, a, b), c)),
+      (('iand', ('fge', a, a), ('iand', (op, b, a), c)), ('iand', ('!' + op, b, a), c)),
    ]
 
 # Add optimizations to handle the case where the result of a ternary is
@@ -3018,15 +3025,60 @@ late_optimizations = [
    (('feq',  ('fadd(is_used_once)', 'a(is_finite)', b), 0.0), ('feq',  a, ('fneg', b))),
    (('fneu', ('fadd(is_used_once)', 'a(is_finite)', b), 0.0), ('fneu', a, ('fneg', b))),
 
+   (('iand', ('feq',  'a@32', a), ('feq',  'b@32', b)),          ('!fge', ('fabs', a), ('fneg', ('fabs', b))) ),
+   (('ior',  ('fneu', 'a@32', a), ('fneu', 'b@32', b)), ('inot', ('!fge', ('fabs', a), ('fneg', ('fabs', b))))),
+
+   # These patterns try to catch the cases where a tree for
+   # all(!isnan(some_vec4)) is not well balanced.
+   (('iand', ('feq', 'a@32', a),
+             ('iand', ('feq', 'b@32', b), c)),
+    ('iand',         ('!fge', ('fabs', a), ('fneg', ('fabs', b))) , c)),
+
+   (('ior',  ('fneu', 'a@32', a),
+             ('ior', ('fneu', 'b@32', b), c)),
+    ('ior', ('inot', ('!fge', ('fabs', a), ('fneg', ('fabs', b)))), c)),
+
+   # These are open-coded versions of isfinite(x) and !isfinite(x). These are
+   # variations that have been "observed in the wild." Others are possible,
+   # and they may well exist.
+   (('iand', ('feq', a, a), ('ine', ('iand', 'a@32', 0x7f800000), 0x7f800000)),
+    ('fisfinite', a)),
+
+   (('ior', ('fneu', a, a), ('ieq', ('fabs', 'a@32'), 0x7f800000)),
+    ('inot', ('fisfinite', a))),
+
+   (('ior', ('fneu', a, a), ('ieq', ('iand', 'a@32', 0x7fffffff), 0x7f800000)),
+    ('inot', ('fisfinite', a))),
+
+   (('ior', ('fneu', a, a), ('ieq', 'a@32(is_not_negative)', 0x7f800000)),
+    ('inot', ('fisfinite', a))),
+
+   (('ior(many-comm-expr)',
+       ('ior', ('inot', ('!fge', ('fabs', a), ('fneg', ('fabs', b)))),
+               ('inot', ('!fge', ('fabs', c), ('fneg', ('fabs', d))))),
+       ('ior', ('ieq', ('fabs', a), 0x7f800000),
+               ('ior', ('ieq', ('fabs', b), 0x7f800000),
+                       ('ior', ('ieq', ('fabs', c), 0x7f800000),
+                               ('ieq', ('fabs', d), 0x7f800000))))
+    ),
+    ('inot', ('iand', ('iand', ('fisfinite', a), ('fisfinite', b)),
+                      ('iand', ('fisfinite', c), ('fisfinite', d)))
+    )
+   ),
+
    # This is how SpvOpFOrdNotEqual might be implemented.  Replace it with
    # SpvOpLessOrGreater.
-   (('iand', ('fneu', a, b),   ('iand', ('feq', a, a), ('feq', b, b))), ('ior', ('!flt', a, b), ('!flt', b, a))),
-   (('iand', ('fneu', a, 0.0),          ('feq', a, a)                ), ('!flt', 0.0, ('fabs', a))),
+   (('iand', ('fneu', a, b), ('!fge', ('fabs', a), ('fneg', ('fabs', b)))), ('ior', ('!flt', a, b), ('!flt', b, a))),
+   (('iand', ('fneu', a, 0.0), ('feq', a, a)), ('!flt', 0.0, ('fabs', a))),
+   (('iand', ('fneu', a, 'b(is_finite)'), ('feq', a, a)), ('!flt', 0.0, ('fabs', ('fadd', ('fneg', a), b)))),
+
+   (('iand', ('iand', ('fneu', a, 0.0), ('fneu', a, b)), ('feq', a, a)), ('iand', ('!flt', 0.0, ('fabs', a)), ('fneu', a, b))),
 
    # This is how SpvOpFUnordEqual might be implemented.  Replace it with
    # !SpvOpLessOrGreater.
-   (('ior', ('feq', a, b),   ('ior', ('fneu', a, a), ('fneu', b, b))), ('inot', ('ior', ('!flt', a, b), ('!flt', b, a)))),
-   (('ior', ('feq', a, 0.0),         ('fneu', a, a),                ), ('inot', ('!flt', 0.0, ('fabs', a)))),
+   (('ior', ('feq', a, b), ('inot', ('!fge', ('fabs', a), ('fneg', ('fabs', b))))), ('inot', ('ior', ('!flt', a, b), ('!flt', b, a)))),
+   (('ior', ('feq', a, 0.0), ('fneu', a, a)), ('inot', ('!flt', 0.0, ('fabs', a)))),
+   (('ior', ('feq', a, 'b(is_finite)'), ('fneu', a, a)), ('inot', ('!flt', 0.0, ('fabs', ('fadd', ('fneg', a), b))))),
 
    # nir_lower_to_source_mods will collapse this, but its existence during the
    # optimization loop can prevent other optimizations.
@@ -3374,6 +3426,11 @@ late_optimizations += [
   (('fcsel', ('sge', a, 0), b, c), ('fcsel_ge', a, b, c), "options->has_fused_comp_and_csel"),
   (('fcsel', ('sge', 0, a), b, c), ('fcsel_ge', ('fneg', a), b, c), "options->has_fused_comp_and_csel"),
 
+  # Several isnan related patterns in late_optimizations insert inot
+  # instructions. These case block generation of fcsel / icsel in addition to
+  # just being extra instructions for the backend to try to optimize.
+  (('bcsel', ('inot', a), b, c), ('bcsel', a, c, b)),
+
   (('bcsel', ('ilt', 0, 'a@32'), 'b@32', 'c@32'), ('i32csel_gt', a, b, c), "options->has_fused_comp_and_csel && !options->no_integers"),
   (('bcsel', ('ilt', 'a@32', 0), 'b@32', 'c@32'), ('i32csel_ge', a, c, b), "options->has_fused_comp_and_csel && !options->no_integers"),
   (('bcsel', ('ige', 'a@32', 0), 'b@32', 'c@32'), ('i32csel_ge', a, b, c), "options->has_fused_comp_and_csel && !options->no_integers"),
-- 
GitLab

