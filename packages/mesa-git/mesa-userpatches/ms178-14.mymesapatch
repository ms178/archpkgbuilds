--- a/src/amd/compiler/aco_scheduler_ilp.cpp	2025-06-28 01:06:51.021200224 +0200
+++ b/src/amd/compiler/aco_scheduler_ilp.cpp	2025-06-28 01:20:12.987784951 +0200
@@ -355,32 +355,72 @@ can_use_vopd(const SchedILPContext& ctx,
    return compat;
 }
 
+static inline unsigned
+ctz32(mask_t v) noexcept
+{
+      return static_cast<unsigned>(__builtin_ctz(static_cast<unsigned>(v)));
+}
+
 Instruction_cycle_info
-get_cycle_info_with_mem_latency(const SchedILPContext& ctx, const Instruction* const instr)
+get_cycle_info_with_mem_latency(const SchedILPContext& ctx,
+                                const Instruction* const instr)
 {
-   Instruction_cycle_info cycle_info = get_cycle_info(*ctx.program, *instr);
+      /* Start with the architecture-agnostic pipe information. */
+      Instruction_cycle_info cycle_info = get_cycle_info(*ctx.program, *instr);
 
-   /* Based on get_wait_counter_info in aco_statistics.cpp. */
-   if (instr->isVMEM() || instr->isFlatLike()) {
-      cycle_info.latency = 320;
-   } else if (instr->isSMEM()) {
-      if (instr->operands.empty()) {
-         cycle_info.latency = 1;
-      } else if (instr->operands[0].size() == 2 ||
-                 (instr->operands[1].isConstant() &&
-                  (instr->operands.size() < 3 || instr->operands[2].isConstant()))) {
-         /* Likely cached. */
-         cycle_info.latency = 30;
-      } else {
-         cycle_info.latency = 200;
+      const amd_gfx_level gfx = ctx.program->gfx_level;
+
+      /* ------------------------------------------------------------------ *
+       * Dedicated latency table for GFX9 (Vega)                            *
+       * ------------------------------------------------------------------ */
+      if (gfx == GFX9) {
+            if (instr->isVMEM() || instr->isFlatLike()) {
+                  cycle_info.latency = 140;                       /* L2 hit          */
+            } else if (instr->isSMEM()) {
+                  if (instr->operands.empty()) {
+                        cycle_info.latency = 1;                      /* s_load_*_imm    */
+                  } else if (instr->operands[0].size() == 2) {    /* 64-bit address  */
+                        cycle_info.latency = 18;                     /* cached constant */
+                  } else {
+                        /* “Likely cached” heuristic – protect every index access. */
+                        const bool cached_const =
+                        instr->operands.size() >= 2 && instr->operands[1].isConstant() &&
+                        (instr->operands.size() < 3 ||
+                        (instr->operands.size() >= 3 && instr->operands[2].isConstant()));
+                        cycle_info.latency = cached_const ? 18 : 120;
+                  }
+            } else if (instr->isLDSDIR()) {
+                  cycle_info.latency = 9;
+            } else if (instr->isDS()) {
+                  cycle_info.latency = 14;
+            }
+            return cycle_info;                                /* Early exit      */
       }
-   } else if (instr->isLDSDIR()) {
-      cycle_info.latency = 13;
-   } else if (instr->isDS()) {
-      cycle_info.latency = 20;
-   }
 
-   return cycle_info;
+      /* ------------------------------------------------------------------ *
+       * Default RDNA(+) latency model – unchanged                          *
+       * ------------------------------------------------------------------ */
+      if (instr->isVMEM() || instr->isFlatLike()) {
+            cycle_info.latency = 320;
+      } else if (instr->isSMEM()) {
+            if (instr->operands.empty()) {
+                  cycle_info.latency = 1;
+            } else if (instr->operands[0].size() == 2) {
+                  cycle_info.latency = 30;
+            } else {
+                  const bool cached_const =
+                  instr->operands.size() >= 2 && instr->operands[1].isConstant() &&
+                  (instr->operands.size() < 3 ||
+                  (instr->operands.size() >= 3 && instr->operands[2].isConstant()));
+                  cycle_info.latency = cached_const ? 30 : 200;
+            }
+      } else if (instr->isLDSDIR()) {
+            cycle_info.latency = 13;
+      } else if (instr->isDS()) {
+            cycle_info.latency = 20;
+      }
+
+      return cycle_info;
 }
 
 bool
@@ -641,52 +681,104 @@ collect_clause_dependencies(const SchedI
 unsigned
 select_instruction_ilp(const SchedILPContext& ctx)
 {
-   mask_t mask = ctx.active_mask;
-
-   /* First, continue the currently open clause.
-    * Otherwise collect all dependencies of the next non-reorderable instruction(s).
-    * These make up the list of possible candidates.
-    */
-   if (ctx.next_non_reorderable != UINT8_MAX) {
-      if (ctx.prev_info.instr && ctx.nodes[ctx.next_non_reorderable].dependency_mask == 0 &&
-          should_form_clause(ctx.prev_info.instr, ctx.nodes[ctx.next_non_reorderable].instr))
-         return ctx.next_non_reorderable;
-      mask = collect_clause_dependencies(ctx, ctx.next_non_reorderable, 0);
-   }
-
-   /* VINTRP(gfx6-10.3) can be handled like alu, but switching between VINTRP and other
-    * alu has a cost. So if the previous instr was VINTRP, try to keep selecting VINTRP.
-    */
-   bool prefer_vintrp = ctx.prev_info.instr && ctx.prev_info.instr->isVINTRP();
-
-   /* Select the instruction with lowest wait_cycles of all candidates. */
-   unsigned idx = -1u;
-   bool idx_vintrp = false;
-   int32_t wait_cycles = INT32_MAX;
-   u_foreach_bit (i, mask) {
-      const InstrInfo& candidate = ctx.nodes[i];
-
-      /* Check if the candidate has pending dependencies. */
-      if (candidate.dependency_mask)
-         continue;
-
-      bool is_vintrp = prefer_vintrp && candidate.instr->isVINTRP();
-
-      if (idx == -1u || (is_vintrp && !idx_vintrp) ||
-          (is_vintrp == idx_vintrp && candidate.wait_cycles < wait_cycles)) {
-         idx = i;
-         idx_vintrp = is_vintrp;
-         wait_cycles = candidate.wait_cycles;
+      /* ------------------------------------------------------------- *
+       * 1.  Determine the mask of candidates we are allowed to pick   *
+       * ------------------------------------------------------------- */
+      mask_t mask = ctx.active_mask;
+
+      if (ctx.next_non_reorderable != UINT8_MAX) {
+            /* If we are inside a memory clause try to keep it alive. */
+            if (ctx.prev_info.instr &&
+                  ctx.nodes[ctx.next_non_reorderable].dependency_mask == 0 &&
+                  should_form_clause(ctx.prev_info.instr,
+                                     ctx.nodes[ctx.next_non_reorderable].instr)) {
+                  return ctx.next_non_reorderable;
+                                     }
+                                     mask = collect_clause_dependencies(ctx, ctx.next_non_reorderable, 0);
+      }
+
+      /* Early out if only the barrier-like instruction is available. */
+      if (mask == 0) {
+            assert(ctx.next_non_reorderable != UINT8_MAX);
+            return ctx.next_non_reorderable;
+      }
+
+      /* ------------------------------------------------------------- *
+       * 2.  Strategy flags                                             *
+       * ------------------------------------------------------------- */
+      const bool prefer_vintrp = ctx.prev_info.instr && ctx.prev_info.instr->isVINTRP();
+      const bool prev_is_vmem  = ctx.prev_info.instr && ctx.prev_info.instr->isVMEM();
+
+      /* ------------------------------------------------------------- *
+       * 3.  Vega VMEM-clause continuation heuristic                    *
+       * ------------------------------------------------------------- */
+      if (ctx.program->gfx_level == GFX9 && prev_is_vmem) {
+            mask_t vm_scan = mask;
+            while (vm_scan) {
+                  const unsigned i = ctz32(vm_scan);
+                  vm_scan &= vm_scan - 1;                   /* clear bit */
+
+                  const InstrInfo& cand = ctx.nodes[i];
+                  if (cand.dependency_mask)
+                        continue;                             /* not ready      */
+
+                        if (cand.instr->isVMEM() && cand.wait_cycles < 80) {
+                              return i;                             /* extend clause  */
+                        }
+            }
       }
-   }
-
-   if (idx != -1u)
-      return idx;
 
-   /* Select the next non-reorderable instruction. (it must have no dependencies) */
-   assert(ctx.next_non_reorderable != UINT8_MAX);
-   assert(ctx.nodes[ctx.next_non_reorderable].dependency_mask == 0);
-   return ctx.next_non_reorderable;
+      /* ------------------------------------------------------------- *
+       * 4.  General ready-list scan                                    *
+       * ------------------------------------------------------------- */
+      unsigned best_idx          = UINT32_MAX;
+      bool     best_is_vintrp    = false;
+      bool     best_is_salu      = false;
+      int32_t  best_wait_cycles  = INT32_MAX;
+
+      mask_t scan = mask;
+      while (scan) {
+            const unsigned i = ctz32(scan);
+            scan &= scan - 1;
+
+            const InstrInfo& cand = ctx.nodes[i];
+            if (cand.dependency_mask)
+                  continue;
+
+            const Instruction* inst     = cand.instr;
+            const bool cand_is_vintrp   = prefer_vintrp && inst->isVINTRP();
+            const bool cand_is_salu     = inst->isSALU();
+            const int32_t wait_cycles   = cand.wait_cycles;
+
+            bool take = false;
+            if (best_idx == UINT32_MAX) {
+                  take = true;                                     /* first ready node  */
+            } else if (cand_is_vintrp != best_is_vintrp) {
+                  take = cand_is_vintrp;                           /* VINTRP priority   */
+            } else if (!best_is_salu && cand_is_salu &&
+                  best_wait_cycles >= 8) {                 /* SALU latency hide */
+                        take = true;
+                  } else if (wait_cycles < best_wait_cycles) {        /* shortest stall    */
+                        take = true;
+                  }
+
+                  if (take) {
+                        best_idx          = i;
+                        best_is_vintrp    = cand_is_vintrp;
+                        best_is_salu      = cand_is_salu;
+                        best_wait_cycles  = wait_cycles;
+                  }
+      }
+
+      /* There must always be at least one ready candidate. */
+      if (best_idx != UINT32_MAX)
+            return best_idx;
+
+      /* Fallback – only triggered if logic above proves none ready, which is
+       * impossible unless caller violated invariants; still keep the original
+       * behaviour to avoid UB in release builds. */
+      assert(ctx.next_non_reorderable != UINT8_MAX);
+      return ctx.next_non_reorderable;
 }
 
 bool
@@ -736,36 +828,46 @@ compare_nodes_vopd(const SchedILPContext
 unsigned
 select_instruction_vopd(const SchedILPContext& ctx, unsigned* vopd_compat)
 {
-   *vopd_compat = 0;
-
-   mask_t mask = ctx.active_mask;
-   if (ctx.next_non_reorderable != UINT8_MAX)
-      mask = ctx.nodes[ctx.next_non_reorderable].dependency_mask;
+      *vopd_compat = 0;
 
-   if (mask == 0)
-      return ctx.next_non_reorderable;
-
-   int num_vopd_odd_minus_even =
-      (int)util_bitcount(ctx.vopd_odd_mask & mask) - (int)util_bitcount(ctx.vopd_even_mask & mask);
-
-   unsigned cur = -1u;
-   u_foreach_bit (i, mask) {
-      const InstrInfo& candidate = ctx.nodes[i];
-
-      /* Check if the candidate has pending dependencies. */
-      if (candidate.dependency_mask)
-         continue;
-
-      if (cur == -1u) {
-         cur = i;
-         *vopd_compat = can_use_vopd(ctx, i);
-      } else if (compare_nodes_vopd(ctx, num_vopd_odd_minus_even, vopd_compat, cur, i)) {
-         cur = i;
+      mask_t mask = ctx.active_mask;
+      if (ctx.next_non_reorderable != UINT8_MAX)
+            mask = ctx.nodes[ctx.next_non_reorderable].dependency_mask;
+
+      if (mask == 0)
+            return ctx.next_non_reorderable;            /* only the barrier is ready */
+
+            /* Track the odd/even dst balance to improve future pairing chances. */
+            const int odd_minus_even =
+            static_cast<int>(util_bitcount(ctx.vopd_odd_mask  & mask)) -
+            static_cast<int>(util_bitcount(ctx.vopd_even_mask & mask));
+
+      unsigned cur = UINT32_MAX;
+
+      mask_t scan = mask;
+      while (scan) {
+            const unsigned i = ctz32(scan);
+            scan &= scan - 1;
+
+            const InstrInfo& cand = ctx.nodes[i];
+            if (cand.dependency_mask)
+                  continue;                                 /* not ready */
+
+                  if (cur == UINT32_MAX) {
+                        cur          = i;
+                        *vopd_compat = can_use_vopd(ctx, i);
+                        continue;
+                  }
+
+                  unsigned tmp_compat = *vopd_compat;
+            if (compare_nodes_vopd(ctx, odd_minus_even, &tmp_compat, cur, i)) {
+                  cur          = i;
+                  *vopd_compat = tmp_compat;
+            }
       }
-   }
 
-   assert(cur != -1u);
-   return cur;
+      assert(cur != UINT32_MAX);                      /* at least one node ready */
+      return cur;
 }
 
 void
