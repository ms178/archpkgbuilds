From 49adb9edc7a4753ba777ab1e1eb4730014cf7b89 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Fri, 20 Jan 2023 17:04:20 +0100
Subject: [PATCH 1/5] radv: Get rid of app_shaders_internal.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

This will make sure the internal field is set to true for internal
shaders which are initialized outside of radv_device_init_meta.

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
---
 src/amd/vulkan/radv_meta.c      | 4 ----
 src/amd/vulkan/radv_pipeline.c  | 5 ++++-
 src/amd/vulkan/radv_private.h   | 4 +---
 src/amd/vulkan/radv_rt_shader.c | 2 +-
 src/amd/vulkan/radv_shader.c    | 6 +++---
 src/amd/vulkan/radv_shader.h    | 3 ++-
 6 files changed, 11 insertions(+), 13 deletions(-)

diff --git a/src/amd/vulkan/radv_meta.c b/src/amd/vulkan/radv_meta.c
index 009fb3fe6bf6..9c482e5da082 100644
--- a/src/amd/vulkan/radv_meta.c
+++ b/src/amd/vulkan/radv_meta.c
@@ -428,8 +428,6 @@ radv_device_init_meta(struct radv_device *device)
 
    mtx_init(&device->meta_state.mtx, mtx_plain);
 
-   device->app_shaders_internal = true;
-
    result = radv_device_init_meta_clear_state(device, on_demand);
    if (result != VK_SUCCESS)
       goto fail_clear;
@@ -501,8 +499,6 @@ radv_device_init_meta(struct radv_device *device)
          goto fail_accel_struct;
    }
 
-   device->app_shaders_internal = false;
-
    return VK_SUCCESS;
 
 fail_accel_struct:
diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index 1ea50c2dc1fd..c12da38c5500 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -3121,7 +3121,8 @@ radv_pipeline_get_nir(struct radv_pipeline *pipeline, struct radv_pipeline_stage
       if (pipeline->retained_shaders[s].nir) {
          stages[s].nir = nir_shader_clone(NULL, pipeline->retained_shaders[s].nir);
       } else {
-         stages[s].nir = radv_shader_spirv_to_nir(device, &stages[s], pipeline_key);
+         stages[s].nir = radv_shader_spirv_to_nir(device, &stages[s], pipeline_key,
+                                                  pipeline->is_internal);
       }
 
       if (retain_shaders)
@@ -5083,6 +5084,7 @@ radv_graphics_pipeline_create(VkDevice _device, VkPipelineCache _cache,
       return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
 
    radv_pipeline_init(device, &pipeline->base, RADV_PIPELINE_GRAPHICS);
+   pipeline->base.is_internal = is_internal;
 
    result = radv_graphics_pipeline_init(pipeline, device, cache, pCreateInfo, extra);
    if (result != VK_SUCCESS) {
@@ -5373,6 +5375,7 @@ radv_compute_pipeline_create(VkDevice _device, VkPipelineCache _cache,
    }
 
    radv_pipeline_init(device, &pipeline->base, RADV_PIPELINE_COMPUTE);
+   pipeline->base.is_internal = is_internal;
 
    const VkPipelineCreationFeedbackCreateInfo *creation_feedback =
       vk_find_struct_const(pCreateInfo->pNext, PIPELINE_CREATION_FEEDBACK_CREATE_INFO);
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index 559582831e4e..411ee4598fca 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -997,9 +997,6 @@ struct radv_device {
    /* Whether per-vertex VRS is forced. */
    bool force_vrs_enabled;
 
-   /* Whether shaders created through application entrypoints are considered internal. */
-   bool app_shaders_internal;
-
    simple_mtx_t pstate_mtx;
    unsigned pstate_cnt;
 
@@ -2023,6 +2020,7 @@ struct radv_pipeline {
    struct radv_pipeline_slab *slab;
    struct radeon_winsys_bo *slab_bo;
 
+   bool is_internal;
    bool need_indirect_descriptor_sets;
    struct radv_shader *shaders[MESA_VULKAN_SHADER_STAGES];
    struct radv_shader *gs_copy_shader;
diff --git a/src/amd/vulkan/radv_rt_shader.c b/src/amd/vulkan/radv_rt_shader.c
index 90efab76a22c..b51b93e73104 100644
--- a/src/amd/vulkan/radv_rt_shader.c
+++ b/src/amd/vulkan/radv_rt_shader.c
@@ -802,7 +802,7 @@ parse_rt_stage(struct radv_device *device, const VkPipelineShaderStageCreateInfo
 
    radv_pipeline_stage_init(sinfo, &rt_stage, vk_to_mesa_shader_stage(sinfo->stage));
 
-   nir_shader *shader = radv_shader_spirv_to_nir(device, &rt_stage, key);
+   nir_shader *shader = radv_shader_spirv_to_nir(device, &rt_stage, key, false);
 
    if (shader->info.stage == MESA_SHADER_RAYGEN || shader->info.stage == MESA_SHADER_CLOSEST_HIT ||
        shader->info.stage == MESA_SHADER_CALLABLE || shader->info.stage == MESA_SHADER_MISS) {
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 661b4288f877..b2bcc853f0d7 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -716,7 +716,7 @@ is_not_xfb_output(nir_variable *var, void *data)
 
 nir_shader *
 radv_shader_spirv_to_nir(struct radv_device *device, const struct radv_pipeline_stage *stage,
-                         const struct radv_pipeline_key *key)
+                         const struct radv_pipeline_key *key, bool is_internal)
 {
    unsigned subgroup_size = 64, ballot_bit_size = 64;
    if (key->cs.compute_subgroup_size) {
@@ -745,7 +745,7 @@ radv_shader_spirv_to_nir(struct radv_device *device, const struct radv_pipeline_
 
       bool dump_meta = device->instance->debug_flags & RADV_DEBUG_DUMP_META_SHADERS;
       if ((device->instance->debug_flags & RADV_DEBUG_DUMP_SPIRV) &&
-          (!device->app_shaders_internal || dump_meta))
+          (!is_internal || dump_meta))
          radv_print_spirv(stage->spirv.data, stage->spirv.size, stderr);
 
       uint32_t num_spec_entries = 0;
@@ -839,7 +839,7 @@ radv_shader_spirv_to_nir(struct radv_device *device, const struct radv_pipeline_
       nir = spirv_to_nir(spirv, stage->spirv.size / 4, spec_entries, num_spec_entries, stage->stage,
                          stage->entrypoint, &spirv_options,
                          &device->physical_device->nir_options[stage->stage]);
-      nir->info.internal |= device->app_shaders_internal;
+      nir->info.internal |= is_internal;
       assert(nir->info.stage == stage->stage);
       nir_validate_shader(nir, "after spirv_to_nir");
 
diff --git a/src/amd/vulkan/radv_shader.h b/src/amd/vulkan/radv_shader.h
index 871384a59a65..7d70544eeb02 100644
--- a/src/amd/vulkan/radv_shader.h
+++ b/src/amd/vulkan/radv_shader.h
@@ -542,7 +542,8 @@ struct radv_pipeline_stage;
 
 nir_shader *radv_shader_spirv_to_nir(struct radv_device *device,
                                      const struct radv_pipeline_stage *stage,
-                                     const struct radv_pipeline_key *key);
+                                     const struct radv_pipeline_key *key,
+                                     bool is_internal);
 
 void radv_nir_lower_abi(nir_shader *shader, enum amd_gfx_level gfx_level,
                         const struct radv_shader_info *info, const struct radv_shader_args *args,
-- 
GitLab


From 4588fd866a80f13e5e4ffb1b5d40edfe0a1f91f1 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Tue, 10 Jan 2023 12:58:52 +0100
Subject: [PATCH 2/5] radv, aco: Add uses_full_subgroups to compute shader
 info.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Allow the compiler to assume that the shader always has full subgroups,
meaning that the initial EXEC mask is -1 in all waves (all lanes enabled).
This assumption is incorrect for ray tracing and internal (meta) shaders
because they can use unaligned dispatch.

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
Reviewed-by: Daniel Schürmann <daniel@schuermann.dev>
---
 src/amd/compiler/aco_shader_info.h    |  1 +
 src/amd/vulkan/radv_aco_shader_info.h |  1 +
 src/amd/vulkan/radv_pipeline.c        |  3 ++-
 src/amd/vulkan/radv_private.h         |  1 +
 src/amd/vulkan/radv_shader.h          |  1 +
 src/amd/vulkan/radv_shader_info.c     | 11 +++++++++++
 6 files changed, 17 insertions(+), 1 deletion(-)

diff --git a/src/amd/compiler/aco_shader_info.h b/src/amd/compiler/aco_shader_info.h
index e292262261d8..a3f1872f4759 100644
--- a/src/amd/compiler/aco_shader_info.h
+++ b/src/amd/compiler/aco_shader_info.h
@@ -117,6 +117,7 @@ struct aco_shader_info {
    } ps;
    struct {
       uint8_t subgroup_size;
+      bool uses_full_subgroups;
    } cs;
 
    uint32_t gfx9_gs_ring_lds_size;
diff --git a/src/amd/vulkan/radv_aco_shader_info.h b/src/amd/vulkan/radv_aco_shader_info.h
index 8394b5ec71f6..02ca932e6b54 100644
--- a/src/amd/vulkan/radv_aco_shader_info.h
+++ b/src/amd/vulkan/radv_aco_shader_info.h
@@ -87,6 +87,7 @@ radv_aco_convert_shader_info(struct aco_shader_info *aco_info,
    ASSIGN_FIELD(ps.num_interp);
    ASSIGN_FIELD(ps.spi_ps_input);
    ASSIGN_FIELD(cs.subgroup_size);
+   ASSIGN_FIELD(cs.uses_full_subgroups);
    aco_info->gfx9_gs_ring_lds_size = radv->gs_ring_info.lds_size;
 }
 
diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index c12da38c5500..e9257c647470 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -2471,6 +2471,7 @@ radv_fill_shader_info(struct radv_pipeline *pipeline,
 
       radv_nir_shader_info_init(&stages[i].info);
       radv_nir_shader_info_pass(device, stages[i].nir, pipeline_layout, pipeline_key,
+                                pipeline->type,
                                 &stages[i].info);
    }
 
@@ -2998,7 +2999,7 @@ radv_pipeline_create_gs_copy_shader(struct radv_pipeline *pipeline,
    nir_shader_gather_info(nir, nir_shader_get_entrypoint(nir));
 
    struct radv_shader_info info = {0};
-   radv_nir_shader_info_pass(device, nir, pipeline_layout, pipeline_key, &info);
+   radv_nir_shader_info_pass(device, nir, pipeline_layout, pipeline_key, pipeline->type, &info);
    info.wave_size = 64; /* Wave32 not supported. */
    info.workgroup_size = 64; /* HW VS: separate waves, no workgroups */
    info.so = gs_info->so;
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index 411ee4598fca..3729c171312c 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -2829,6 +2829,7 @@ struct radv_shader_info;
 void radv_nir_shader_info_pass(struct radv_device *device, const struct nir_shader *nir,
                                const struct radv_pipeline_layout *layout,
                                const struct radv_pipeline_key *pipeline_key,
+                               const enum radv_pipeline_type pipeline_type,
                                struct radv_shader_info *info);
 
 void radv_nir_shader_info_init(struct radv_shader_info *info);
diff --git a/src/amd/vulkan/radv_shader.h b/src/amd/vulkan/radv_shader.h
index 7d70544eeb02..fec4b8c1774a 100644
--- a/src/amd/vulkan/radv_shader.h
+++ b/src/amd/vulkan/radv_shader.h
@@ -359,6 +359,7 @@ struct radv_shader_info {
       bool uses_ray_launch_size;
       bool uses_dynamic_rt_callable_stack;
       bool uses_rt;
+      bool uses_full_subgroups;
    } cs;
    struct {
       uint64_t tes_inputs_read;
diff --git a/src/amd/vulkan/radv_shader_info.c b/src/amd/vulkan/radv_shader_info.c
index 6c7ccf24a8b5..3e25671552bf 100644
--- a/src/amd/vulkan/radv_shader_info.c
+++ b/src/amd/vulkan/radv_shader_info.c
@@ -683,6 +683,7 @@ void
 radv_nir_shader_info_pass(struct radv_device *device, const struct nir_shader *nir,
                           const struct radv_pipeline_layout *layout,
                           const struct radv_pipeline_key *pipeline_key,
+                          const enum radv_pipeline_type pipeline_type,
                           struct radv_shader_info *info)
 {
    struct nir_function *func = (struct nir_function *)exec_list_get_head_const(&nir->functions);
@@ -823,6 +824,16 @@ radv_nir_shader_info_pass(struct radv_device *device, const struct nir_shader *n
    case MESA_SHADER_TASK:
       info->workgroup_size =
          ac_compute_cs_workgroup_size(nir->info.workgroup_size, false, UINT32_MAX);
+
+      /* Allow the compiler to assume that the shader always has full subgroups,
+       * meaning that the initial EXEC mask is -1 in all waves (all lanes enabled).
+       * This assumption is incorrect for ray tracing and internal (meta) shaders
+       * because they can use unaligned dispatch.
+       */
+      info->cs.uses_full_subgroups =
+         pipeline_type != RADV_PIPELINE_RAY_TRACING &&
+         !nir->info.internal &&
+         (info->workgroup_size % info->wave_size) == 0;
       break;
    case MESA_SHADER_MESH:
       /* Already computed in gather_shader_info_mesh(). */
-- 
GitLab


From 1efa79eaa8a6f0b2d7c8a8fa5faa4b94c2a6960a Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Tue, 10 Jan 2023 19:15:25 +0100
Subject: [PATCH 3/5] aco: Enable constant exec mask based optimization on
 compute shaders.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

We know for sure exec is initially -1 when the shader always has full subgroups.

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
Reviewed-by: Daniel Schürmann <daniel@schuermann.dev>
---
 src/amd/compiler/aco_insert_exec_mask.cpp | 6 ++++++
 1 file changed, 6 insertions(+)

diff --git a/src/amd/compiler/aco_insert_exec_mask.cpp b/src/amd/compiler/aco_insert_exec_mask.cpp
index feffd5acd714..63560cb6d301 100644
--- a/src/amd/compiler/aco_insert_exec_mask.cpp
+++ b/src/amd/compiler/aco_insert_exec_mask.cpp
@@ -268,6 +268,12 @@ add_coupling_code(exec_ctx& ctx, Block* block, std::vector<aco_ptr<Instruction>>
          bld.copy(Definition(exec, bld.lm), start_exec);
       }
 
+      /* EXEC is automatically initialized by the HW for compute shaders.
+       * We know for sure exec is initially -1 when the shader always has full subgroups.
+       */
+      if (ctx.program->stage == compute_cs && ctx.program->info.cs.uses_full_subgroups)
+         start_exec = Operand::c32_or_c64(-1u, bld.lm == s2);
+
       if (ctx.handle_wqm) {
          ctx.info[0].exec.emplace_back(start_exec, mask_type_global | mask_type_exact);
          /* if this block needs WQM, initialize already */
-- 
GitLab


From 4c439a0ce1fcd2c39c749ddd840d3d977b6b8a3f Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Sun, 22 Jan 2023 19:50:46 +0100
Subject: [PATCH 4/5] aco/optimizer: Change v_cmp with subgroup invocation to
 constant.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

When a shader has a comparison with the subgroup invocation id,
we can use a constant instead, saving a VALU instruction.
When the constant can't be represented as a 64-bit literal,
use the s_bfm_b64 instruction to generate it instead.

Fossil DB stats on GFX11:
Totals from 300 (0.22% of 134913) affected shaders:
CodeSize: 2223052 -> 2214336 (-0.39%); split: -0.43%, +0.04%
Instrs: 430216 -> 429882 (-0.08%); split: -0.14%, +0.06%
Latency: 5881180 -> 5878181 (-0.05%); split: -0.05%, +0.00%
InvThroughput: 731846 -> 729293 (-0.35%)
Copies: 31662 -> 31847 (+0.58%); split: -0.03%, +0.61%
Branches: 8241 -> 8100 (-1.71%)
PreVGPRs: 15788 -> 15786 (-0.01%)

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
---
 src/amd/compiler/aco_ir.cpp        |   7 ++
 src/amd/compiler/aco_ir.h          |   1 +
 src/amd/compiler/aco_optimizer.cpp | 103 +++++++++++++++++++++++++++++
 3 files changed, 111 insertions(+)

diff --git a/src/amd/compiler/aco_ir.cpp b/src/amd/compiler/aco_ir.cpp
index 34c9ea535c87..bb8a01136958 100644
--- a/src/amd/compiler/aco_ir.cpp
+++ b/src/amd/compiler/aco_ir.cpp
@@ -861,6 +861,13 @@ get_inverse(aco_opcode op)
    return get_cmp_info(op, &info) ? info.inverse : aco_opcode::num_opcodes;
 }
 
+aco_opcode
+get_swapped(aco_opcode op)
+{
+   CmpInfo info;
+   return get_cmp_info(op, &info) ? info.swapped : aco_opcode::num_opcodes;
+}
+
 aco_opcode
 get_f32_cmp(aco_opcode op)
 {
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index baf64b04267c..f37bf04a1b9a 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -1883,6 +1883,7 @@ bool needs_exec_mask(const Instruction* instr);
 aco_opcode get_ordered(aco_opcode op);
 aco_opcode get_unordered(aco_opcode op);
 aco_opcode get_inverse(aco_opcode op);
+aco_opcode get_swapped(aco_opcode op);
 aco_opcode get_f32_cmp(aco_opcode op);
 aco_opcode get_vcmpx(aco_opcode op);
 unsigned get_cmp_bitsize(aco_opcode op);
diff --git a/src/amd/compiler/aco_optimizer.cpp b/src/amd/compiler/aco_optimizer.cpp
index 62df72faf1c5..6e205eae9b5a 100644
--- a/src/amd/compiler/aco_optimizer.cpp
+++ b/src/amd/compiler/aco_optimizer.cpp
@@ -123,6 +123,7 @@ enum Label {
    label_f2f32 = 1ull << 37,
    label_f2f16 = 1ull << 38,
    label_split = 1ull << 39,
+   label_subgroup_invocation = 1ull << 40,
 };
 
 static constexpr uint64_t instr_usedef_labels =
@@ -494,6 +495,14 @@ struct ssa_info {
    }
 
    bool is_split() { return label & label_split; }
+
+   void set_subgroup_invocation(Instruction* label_instr)
+   {
+      add_label(label_subgroup_invocation);
+      instr = label_instr;
+   }
+
+   bool is_subgroup_invocation() { return label & label_subgroup_invocation; }
 };
 
 struct opt_ctx {
@@ -2108,6 +2117,26 @@ label_instruction(opt_ctx& ctx, aco_ptr<Instruction>& instr)
       ctx.info[instr->definitions[0].tempId()].set_usedef(instr.get());
       break;
    }
+   case aco_opcode::v_mbcnt_lo_u32_b32: {
+      if (instr->operands[0].constantEquals(-1) && instr->operands[1].constantEquals(0)) {
+         if (ctx.program->wave_size == 32)
+            ctx.info[instr->definitions[0].tempId()].set_subgroup_invocation(instr.get());
+         else
+            ctx.info[instr->definitions[0].tempId()].set_usedef(instr.get());
+      }
+      break;
+   }
+   case aco_opcode::v_mbcnt_hi_u32_b32:
+   case aco_opcode::v_mbcnt_hi_u32_b32_e64: {
+      if (instr->operands[0].constantEquals(-1) && instr->operands[1].isTemp() &&
+          ctx.info[instr->operands[1].tempId()].is_usedef()) {
+         Instruction *usedef_instr = ctx.info[instr->operands[1].tempId()].instr;
+         if (usedef_instr->opcode == aco_opcode::v_mbcnt_lo_u32_b32 &&
+             usedef_instr->operands[0].constantEquals(-1) && usedef_instr->operands[1].constantEquals(0))
+            ctx.info[instr->definitions[0].tempId()].set_subgroup_invocation(instr.get());
+      }
+      break;
+   }
    case aco_opcode::v_cvt_f16_f32: {
       if (instr->operands[0].isTemp())
          ctx.info[instr->operands[0].tempId()].set_f2f16(instr.get());
@@ -2353,6 +2382,75 @@ combine_comparison_ordering(opt_ctx& ctx, aco_ptr<Instruction>& instr)
    return true;
 }
 
+bool
+optimize_cmp_subgroup_invocation(opt_ctx& ctx, aco_ptr<Instruction>& instr)
+{
+   assert(instr->operands.size() == 2);
+
+   const int const_op_idx = instr->operands[0].isConstant() ? 0 : instr->operands[1].isConstant() ? 1 : -1;
+   if (const_op_idx == -1)
+      return false;
+
+   const int mbcnt_op_idx = 1 - const_op_idx;
+   const Operand mbcnt_op = instr->operands[mbcnt_op_idx];
+   if (!mbcnt_op.isTemp() || !ctx.info[mbcnt_op.tempId()].is_subgroup_invocation())
+      return false;
+
+   const unsigned wave_size = ctx.program->wave_size;
+   const unsigned val = instr->operands[const_op_idx].constantValue();
+   const aco_opcode op = const_op_idx == 0 ? get_swapped(instr->opcode) : instr->opcode;
+   unsigned first_bit = 0, num_bits = 0;
+
+   switch (op) {
+   case aco_opcode::v_cmp_eq_u32:
+   case aco_opcode::v_cmp_eq_i32:
+      first_bit = val;
+      num_bits = 1;
+      break;
+   case aco_opcode::v_cmp_le_u32:
+   case aco_opcode::v_cmp_le_i32:
+      first_bit = 0;
+      num_bits = val + 1;
+      break;
+   case aco_opcode::v_cmp_lt_u32:
+   case aco_opcode::v_cmp_lt_i32:
+      first_bit = 0;
+      num_bits = val;
+      break;
+   case aco_opcode::v_cmp_ge_u32:
+   case aco_opcode::v_cmp_ge_i32:
+      first_bit = val;
+      num_bits = wave_size - val;
+      break;
+   case aco_opcode::v_cmp_gt_u32:
+   case aco_opcode::v_cmp_gt_i32:
+      first_bit = val + 1;
+      num_bits = wave_size - val - 1;
+      break;
+   default:
+      unreachable("Unsupported opcode.");
+   }
+
+   Instruction *cpy = NULL;
+   const uint64_t mask = BITFIELD64_RANGE(first_bit, num_bits);
+   if (wave_size == 64 && mask > 0x7fffffff && mask != -1ull) {
+      /* Mask can't be represented as a 64-bit constant or literal, use s_bfm_b64. */
+      cpy = create_instruction<SOP2_instruction>(aco_opcode::s_bfm_b64, Format::SOP2, 2, 1);
+      cpy->operands[0] = Operand::c32(num_bits);
+      cpy->operands[1] = Operand::c32(first_bit);
+   } else {
+      /* Copy mask as a literal constant. */
+      cpy = create_instruction<Pseudo_instruction>(aco_opcode::p_parallelcopy, Format::PSEUDO, 1, 1);
+      cpy->operands[0] = wave_size == 32 ? Operand::c32((uint32_t)mask) : Operand::c64(mask);
+   }
+
+   ctx.uses[mbcnt_op.tempId()]--;
+   cpy->definitions[0] = instr->definitions[0];
+   instr.reset(cpy);
+
+   return true;
+}
+
 bool
 is_operand_constant(opt_ctx& ctx, Operand op, unsigned bit_size, uint64_t* value)
 {
@@ -4037,6 +4135,11 @@ combine_instruction(opt_ctx& ctx, aco_ptr<Instruction>& instr)
       apply_ds_extract(ctx, instr);
    }
 
+   if (instr->isVOPC()) {
+      if (optimize_cmp_subgroup_invocation(ctx, instr))
+         return;
+   }
+
    /* TODO: There are still some peephole optimizations that could be done:
     * - abs(a - b) -> s_absdiff_i32
     * - various patterns for s_bitcmp{0,1}_b32 and s_bitset{0,1}_b32
-- 
GitLab


From ce77336de43b6391c957df85f9ef9b6779ff5e6e Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Mon, 23 Jan 2023 00:12:28 +0100
Subject: [PATCH 5/5] aco: Propagate constant exec mask to pseudo branch
 instructions.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Fossil DB stats on GFX11:
Totals from 59 (0.04% of 134913) affected shaders:
CodeSize: 474168 -> 473116 (-0.22%); split: -0.22%, +0.00%
Instrs: 93776 -> 93509 (-0.28%); split: -0.29%, +0.00%
Latency: 1206267 -> 1206169 (-0.01%); split: -0.01%, +0.00%
InvThroughput: 182223 -> 182222 (-0.00%); split: -0.00%, +0.00%
Copies: 4967 -> 4823 (-2.90%)
Branches: 1551 -> 1396 (-9.99%); split: -10.38%, +0.39%
PreSGPRs: 2362 -> 2336 (-1.10%)

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
---
 src/amd/compiler/aco_insert_exec_mask.cpp |  9 ++++++---
 src/amd/compiler/aco_optimizer.cpp        | 13 +++++++++++++
 2 files changed, 19 insertions(+), 3 deletions(-)

diff --git a/src/amd/compiler/aco_insert_exec_mask.cpp b/src/amd/compiler/aco_insert_exec_mask.cpp
index 63560cb6d301..0e02af51f841 100644
--- a/src/amd/compiler/aco_insert_exec_mask.cpp
+++ b/src/amd/compiler/aco_insert_exec_mask.cpp
@@ -803,12 +803,15 @@ add_branch_code(exec_ctx& ctx, Block* block)
       // orig = s_and_saveexec_b64
       assert(block->linear_succs.size() == 2);
       assert(block->instructions.back()->opcode == aco_opcode::p_cbranch_z);
-      Temp cond = block->instructions.back()->operands[0].getTemp();
-      nir_selection_control sel_ctrl = block->instructions.back()->branch().selection_control;
+      const Operand cond = block->instructions.back()->operands[0];
+      const nir_selection_control sel_ctrl =
+         (cond.isConstant() && cond.constantValue64() > 0)
+         ? nir_selection_control_divergent_always_taken
+         : block->instructions.back()->branch().selection_control;
       block->instructions.pop_back();
 
       uint8_t mask_type = ctx.info[idx].exec.back().second & (mask_type_wqm | mask_type_exact);
-      if (ctx.info[idx].exec.back().first.constantEquals(-1u)) {
+      if (ctx.info[idx].exec.back().first.isConstant()) {
          bld.copy(Definition(exec, bld.lm), cond);
       } else {
          Temp old_exec = bld.sop1(Builder::s_and_saveexec, bld.def(bld.lm), bld.def(s1, scc),
diff --git a/src/amd/compiler/aco_optimizer.cpp b/src/amd/compiler/aco_optimizer.cpp
index 6e205eae9b5a..fdb72d946360 100644
--- a/src/amd/compiler/aco_optimizer.cpp
+++ b/src/amd/compiler/aco_optimizer.cpp
@@ -2442,6 +2442,9 @@ optimize_cmp_subgroup_invocation(opt_ctx& ctx, aco_ptr<Instruction>& instr)
       /* Copy mask as a literal constant. */
       cpy = create_instruction<Pseudo_instruction>(aco_opcode::p_parallelcopy, Format::PSEUDO, 1, 1);
       cpy->operands[0] = wave_size == 32 ? Operand::c32((uint32_t)mask) : Operand::c64(mask);
+
+      /* Mark mask as constant so it can be propagated to p_cbranch later. */
+      ctx.info[instr->definitions[0].tempId()].set_constant(ctx.program->gfx_level, mask);
    }
 
    ctx.uses[mbcnt_op.tempId()]--;
@@ -4766,6 +4769,16 @@ select_instruction(opt_ctx& ctx, aco_ptr<Instruction>& instr)
       instr->definitions[0].setFixed(scc);
    }
 
+   if (instr->isBranch() && !instr->operands.empty() && instr->operands[0].isTemp() &&
+       (ctx.info[instr->operands[0].tempId()].is_constant_or_literal(32) ||
+        ctx.info[instr->operands[0].tempId()].is_constant_or_literal(ctx.program->wave_size))) {
+      uint32_t val = ctx.info[instr->operands[0].tempId()].val;
+      if (ctx.program->wave_size == 32)
+         instr->operands[0] = Operand::c32(val);
+      else
+         instr->operands[0] = Operand::c64(val);
+   }
+
    /* check for literals */
    if (!instr->isSALU() && !instr->isVALU())
       return;
-- 
GitLab

