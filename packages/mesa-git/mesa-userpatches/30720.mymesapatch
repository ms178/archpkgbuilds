From 3290b8c99c902daa4c8e081bfacc4b23586c86b6 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Daniel=20Sch=C3=BCrmann?= <daniel@schuermann.dev>
Date: Wed, 24 Jul 2024 11:51:31 +0200
Subject: [PATCH] aco/scheduler: improve scheduling heuristic

The heuristic we are currently using still stems from the GCN era
with the only adjustments being made for RDNA was to double (or triple)
the wave count.

This rewrite aims to detangle some concepts and provide more consistent results.

 - wave_factor: The purpose of this value is to reflect that RDNA SIMDs can
                accomodate twice as many waves as GCN SIMDs.
 - reg_file_multiple: This value accounts for the larger register file of wave32
                      and some RDNA3 families.
 - wave_minimum: Below this value, we don't sacrifice any waves. It corresponds
                 to a register demand of 64 VGPRs in wave64.
 - occupancy_factor: Depending on target_waves and wave_factor, this controls
                     the scheduling window sizes and number of moves.

The main differences from the previous heuristic is a lower wave minimum and
a slightly less aggressive reduction of waves.
It also increases SMEM_MAX_MOVES in order to mitigate some of the changes
from targeting less waves.

Totals from 44431 (71.79% of 61893) affected shaders: (Bonaire)
MaxWaves: 299687 -> 296182 (-1.17%); split: +0.86%, -2.03%
Instrs: 23410339 -> 23368977 (-0.18%); split: -0.30%, +0.12%
CodeSize: 115049968 -> 114880136 (-0.15%); split: -0.26%, +0.11%
SGPRs: 2700164 -> 2806962 (+3.96%); split: -0.73%, +4.69%
VGPRs: 1865124 -> 1905684 (+2.17%); split: -0.89%, +3.06%
Latency: 240133437 -> 238304789 (-0.76%); split: -1.24%, +0.48%
InvThroughput: 119764699 -> 119737568 (-0.02%); split: -0.17%, +0.15%
VClause: 499780 -> 491675 (-1.62%); split: -2.95%, +1.33%
SClause: 825657 -> 760647 (-7.87%); split: -8.64%, +0.76%
Copies: 2246063 -> 2242450 (-0.16%); split: -1.50%, +1.34%
Branches: 404473 -> 404420 (-0.01%); split: -0.02%, +0.01%
VALU: 16604748 -> 16591001 (-0.08%); split: -0.17%, +0.09%
SALU: 2457813 -> 2468097 (+0.42%); split: -0.37%, +0.79%

Totals from 44780 (71.02% of 63053) affected shaders: (Vega10)
MaxWaves: 313712 -> 312190 (-0.49%); split: +0.80%, -1.29%
Instrs: 23273350 -> 23230586 (-0.18%); split: -0.26%, +0.07%
CodeSize: 121245220 -> 121070144 (-0.14%); split: -0.21%, +0.07%
SGPRs: 2959216 -> 3082704 (+4.17%); split: -0.49%, +4.66%
VGPRs: 1841648 -> 1874076 (+1.76%); split: -0.87%, +2.63%
Latency: 240051899 -> 238177669 (-0.78%); split: -1.24%, +0.46%
InvThroughput: 119588600 -> 119564015 (-0.02%); split: -0.17%, +0.15%
VClause: 502636 -> 494056 (-1.71%); split: -2.91%, +1.21%
SClause: 820644 -> 751743 (-8.40%); split: -9.05%, +0.65%
Copies: 2162195 -> 2157140 (-0.23%); split: -1.18%, +0.94%
Branches: 415160 -> 415204 (+0.01%); split: -0.01%, +0.02%
VALU: 16774463 -> 16763882 (-0.06%); split: -0.14%, +0.07%
SALU: 2178564 -> 2183698 (+0.24%); split: -0.26%, +0.50%

Totals from 57582 (72.53% of 79395) affected shaders: (Navi21)
MaxWaves: 1443566 -> 1456212 (+0.88%); split: +1.80%, -0.93%
Instrs: 41917962 -> 41877492 (-0.10%); split: -0.18%, +0.09%
CodeSize: 225082248 -> 224919760 (-0.07%); split: -0.16%, +0.09%
VGPRs: 2499408 -> 2507712 (+0.33%); split: -1.67%, +2.00%
Latency: 302996694 -> 302368297 (-0.21%); split: -1.18%, +0.98%
InvThroughput: 69006703 -> 68992092 (-0.02%); split: -0.16%, +0.14%
VClause: 875091 -> 874829 (-0.03%); split: -1.53%, +1.50%
SClause: 1280029 -> 1204760 (-5.88%); split: -6.32%, +0.44%
Copies: 3054533 -> 3059980 (+0.18%); split: -0.83%, +1.01%
Branches: 1062242 -> 1062266 (+0.00%); split: -0.01%, +0.01%
VALU: 26823737 -> 26822770 (-0.00%); split: -0.08%, +0.08%
SALU: 5597050 -> 5603988 (+0.12%); split: -0.11%, +0.23%

Totals from 61849 (77.90% of 79395) affected shaders: (Navi31)
MaxWaves: 1859477 -> 1836577 (-1.23%); split: +0.01%, -1.24%
Instrs: 42398309 -> 42314711 (-0.20%); split: -0.33%, +0.13%
CodeSize: 221689252 -> 221357656 (-0.15%); split: -0.27%, +0.12%
VGPRs: 3011296 -> 3043264 (+1.06%); split: -0.17%, +1.23%
Latency: 291512507 -> 289861706 (-0.57%); split: -0.93%, +0.36%
InvThroughput: 45439059 -> 45436322 (-0.01%); split: -0.17%, +0.17%
VClause: 815792 -> 802713 (-1.60%); split: -2.42%, +0.81%
SClause: 1264060 -> 1197622 (-5.26%); split: -5.68%, +0.42%
Copies: 2745760 -> 2737480 (-0.30%); split: -1.19%, +0.89%
Branches: 836523 -> 836584 (+0.01%); split: -0.00%, +0.01%
VALU: 24242929 -> 24230364 (-0.05%); split: -0.12%, +0.07%
SALU: 4044994 -> 4049032 (+0.10%); split: -0.17%, +0.27%
VOPD: 8107 -> 8112 (+0.06%); split: +1.04%, -0.97%
---
 src/amd/compiler/aco_scheduler.cpp | 53 ++++++++++++++----------------
 1 file changed, 24 insertions(+), 29 deletions(-)

diff --git a/src/amd/compiler/aco_scheduler.cpp b/src/amd/compiler/aco_scheduler.cpp
index 0bfce41101e5a..f3c1555c1e749 100644
--- a/src/amd/compiler/aco_scheduler.cpp
+++ b/src/amd/compiler/aco_scheduler.cpp
@@ -12,17 +12,17 @@
 #include <algorithm>
 #include <vector>
 
-#define SMEM_WINDOW_SIZE    (350 - ctx.num_waves * 35)
-#define VMEM_WINDOW_SIZE    (1024 - ctx.num_waves * 64)
+#define SMEM_WINDOW_SIZE    (256 - ctx.occupancy_factor * 16)
+#define VMEM_WINDOW_SIZE    (1024 - ctx.occupancy_factor * 64)
 #define LDS_WINDOW_SIZE     64
 #define POS_EXP_WINDOW_SIZE 512
-#define SMEM_MAX_MOVES      (64 - ctx.num_waves * 4)
-#define VMEM_MAX_MOVES      (256 - ctx.num_waves * 16)
+#define SMEM_MAX_MOVES      (128 - ctx.occupancy_factor * 8)
+#define VMEM_MAX_MOVES      (256 - ctx.occupancy_factor * 16)
 #define LDSDIR_MAX_MOVES    10
 #define LDS_MAX_MOVES       32
 /* creating clauses decreases def-use distances, so make it less aggressive the lower num_waves is */
-#define VMEM_CLAUSE_MAX_GRAB_DIST (ctx.num_waves * 2)
-#define VMEM_STORE_CLAUSE_MAX_GRAB_DIST (ctx.num_waves * 4)
+#define VMEM_CLAUSE_MAX_GRAB_DIST       (ctx.occupancy_factor * 2)
+#define VMEM_STORE_CLAUSE_MAX_GRAB_DIST (ctx.occupancy_factor * 4)
 #define POS_EXP_MAX_MOVES         512
 
 namespace aco {
@@ -115,7 +115,7 @@ struct MoveState {
 
 struct sched_ctx {
    amd_gfx_level gfx_level;
-   int16_t num_waves;
+   int16_t occupancy_factor;
    int16_t last_SMEM_stall;
    int last_SMEM_dep_idx;
    MoveState mv;
@@ -745,7 +745,7 @@ schedule_SMEM(sched_ctx& ctx, Block* block, Instruction* current, int idx)
       /* only move VMEM instructions below descriptor loads. be more aggressive at higher num_waves
        * to help create more vmem clauses */
       if ((candidate->isVMEM() || candidate->isFlatLike()) &&
-          (cursor.insert_idx - cursor.source_idx > (ctx.num_waves * 4) ||
+          (cursor.insert_idx - cursor.source_idx > (ctx.occupancy_factor * 4) ||
            current->operands[0].size() == 4))
          break;
       /* don't move descriptor loads below buffer loads */
@@ -847,7 +847,7 @@ schedule_SMEM(sched_ctx& ctx, Block* block, Instruction* current, int idx)
    }
 
    ctx.last_SMEM_dep_idx = found_dependency ? up_cursor.insert_idx : 0;
-   ctx.last_SMEM_stall = 10 - ctx.num_waves - k;
+   ctx.last_SMEM_stall = 10 - ctx.occupancy_factor - k;
 }
 
 void
@@ -1247,35 +1247,30 @@ schedule_program(Program* program)
    RegisterDemand demand;
    for (Block& block : program->blocks)
       demand.update(block.register_demand);
-   demand.vgpr += program->config->num_shared_vgprs / 2;
 
    sched_ctx ctx;
    ctx.gfx_level = program->gfx_level;
    ctx.mv.depends_on.resize(program->peekAllocationId());
    ctx.mv.RAR_dependencies.resize(program->peekAllocationId());
    ctx.mv.RAR_dependencies_clause.resize(program->peekAllocationId());
-   /* Allowing the scheduler to reduce the number of waves to as low as 5
-    * improves performance of Thrones of Britannia significantly and doesn't
-    * seem to hurt anything else. */
-   unsigned wave_fac = program->dev.physical_vgprs / 256;
-   if (program->num_waves <= 5 * wave_fac)
-      ctx.num_waves = program->num_waves;
-   else if (demand.vgpr >= 29)
-      ctx.num_waves = 5 * wave_fac;
-   else if (demand.vgpr >= 25)
-      ctx.num_waves = 6 * wave_fac;
-   else
-      ctx.num_waves = 7 * wave_fac;
-   ctx.num_waves = std::max<uint16_t>(ctx.num_waves, program->min_waves);
-   ctx.num_waves = std::min<uint16_t>(ctx.num_waves, program->num_waves);
-   ctx.num_waves = max_suitable_waves(program, ctx.num_waves);
-
-   assert(ctx.num_waves >= program->min_waves);
-   ctx.mv.max_registers = get_addr_regs_from_waves(program, ctx.num_waves);
+
+   const int wave_factor = program->gfx_level >= GFX10 ? 2 : 1;
+   const float reg_file_multiple = program->dev.physical_vgprs / (256.0 * wave_factor);
+   const int wave_minimum = std::max<int>(program->min_waves, 4 * wave_factor * reg_file_multiple);
+
+   /* If we already have less waves than the minimum, don't reduce them further.
+    * Otherwise, sacrifice some waves and use more VGPRs, in order to improve scheduling.
+    */
+   int vgpr_demand = std::max<int>(24, demand.vgpr) + 12 * reg_file_multiple;
+   int target_waves = std::max(wave_minimum, program->dev.physical_vgprs / vgpr_demand);
+   target_waves = max_suitable_waves(program, std::min<int>(program->num_waves, target_waves));
+   assert(target_waves >= program->min_waves);
+
+   ctx.mv.max_registers = get_addr_regs_from_waves(program, target_waves);
    ctx.mv.max_registers.vgpr -= 2;
 
    /* VMEM_MAX_MOVES and such assume pre-GFX10 wave count */
-   ctx.num_waves = std::max<uint16_t>(ctx.num_waves / wave_fac, 1);
+   ctx.occupancy_factor = target_waves / wave_factor;
 
    /* NGG culling shaders are very sensitive to position export scheduling.
     * Schedule less aggressively when early primitive export is used, and
-- 
GitLab

