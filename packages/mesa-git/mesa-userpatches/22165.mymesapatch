From 494001ba4e45119d4ce5921c907f76e3c120ab09 Mon Sep 17 00:00:00 2001
From: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
Date: Tue, 28 Mar 2023 20:18:43 +0900
Subject: [PATCH 1/5] radv: Rename shader_dma fields to better reflect upload /
 download.

There's no download yet, but in preparation of adding download helpers,
make it clearer which fields are shared by upload / download and which are
upload only.
---
 src/amd/vulkan/layers/radv_sqtt_layer.c |   5 +-
 src/amd/vulkan/radv_device.c            |   6 +-
 src/amd/vulkan/radv_pipeline_cache.c    |   2 +-
 src/amd/vulkan/radv_private.h           |  14 +--
 src/amd/vulkan/radv_queue.c             |   2 +-
 src/amd/vulkan/radv_shader.c            | 116 ++++++++++++------------
 src/amd/vulkan/radv_shader.h            |  27 +++---
 7 files changed, 86 insertions(+), 86 deletions(-)

diff --git a/src/amd/vulkan/layers/radv_sqtt_layer.c b/src/amd/vulkan/layers/radv_sqtt_layer.c
index ab0e331869c5..d7bfa2cc84a0 100644
--- a/src/amd/vulkan/layers/radv_sqtt_layer.c
+++ b/src/amd/vulkan/layers/radv_sqtt_layer.c
@@ -176,8 +176,7 @@ radv_sqtt_reloc_graphics_shaders(struct radv_device *device,
    uint64_t offset = 0;
 
    if (device->shader_use_invisible_vram) {
-       submission =
-         radv_shader_dma_get_submission(device, reloc->bo, slab_va, code_size);
+       submission = radv_shader_upload_get_submission(device, reloc->bo, slab_va, code_size);
       if (!submission)
          return VK_ERROR_UNKNOWN;
    }
@@ -201,7 +200,7 @@ radv_sqtt_reloc_graphics_shaders(struct radv_device *device,
    }
 
    if (device->shader_use_invisible_vram) {
-      if (!radv_shader_dma_submit(device, submission, &pipeline->base.shader_upload_seq))
+      if (!radv_shader_upload_submit(device, submission, &pipeline->base.shader_upload_seq))
          return VK_ERROR_UNKNOWN;
    }
 
diff --git a/src/amd/vulkan/radv_device.c b/src/amd/vulkan/radv_device.c
index 9d44b143fc3f..c5ee729dd1fc 100644
--- a/src/amd/vulkan/radv_device.c
+++ b/src/amd/vulkan/radv_device.c
@@ -864,7 +864,7 @@ radv_CreateDevice(VkPhysicalDevice physicalDevice, const VkDeviceCreateInfo *pCr
       (device->instance->perftest_flags & RADV_PERFTEST_DMA_SHADERS) &&
       /* SDMA buffer copy is only implemented for GFX7+. */
       device->physical_device->rad_info.gfx_level >= GFX7;
-   result = radv_init_shader_upload_queue(device);
+   result = radv_init_shader_dma(device);
    if (result != VK_SUCCESS)
       goto fail;
 
@@ -1107,7 +1107,7 @@ fail:
    radv_device_finish_ps_epilogs(device);
    radv_device_finish_border_color(device);
 
-   radv_destroy_shader_upload_queue(device);
+   radv_finish_shader_dma(device);
 
 fail_queue:
    for (unsigned i = 0; i < RADV_MAX_QUEUE_FAMILIES; i++) {
@@ -1184,7 +1184,7 @@ radv_DestroyDevice(VkDevice _device, const VkAllocationCallbacks *pAllocator)
 
    vk_pipeline_cache_destroy(device->mem_cache, NULL);
 
-   radv_destroy_shader_upload_queue(device);
+   radv_finish_shader_dma(device);
 
    radv_trap_handler_finish(device);
    radv_finish_trace(device);
diff --git a/src/amd/vulkan/radv_pipeline_cache.c b/src/amd/vulkan/radv_pipeline_cache.c
index 9391ff83ed16..69f6a17f2ce7 100644
--- a/src/amd/vulkan/radv_pipeline_cache.c
+++ b/src/amd/vulkan/radv_pipeline_cache.c
@@ -128,7 +128,7 @@ radv_shader_destroy(struct vk_device *_device, struct vk_pipeline_cache_object *
 
    if (device->shader_use_invisible_vram) {
       /* Wait for any pending upload to complete, or we'll be writing into freed shader memory. */
-      radv_shader_wait_for_upload(device, shader->upload_seq);
+      radv_shader_dma_wait(device, shader->upload_seq);
    }
 
    radv_free_shader_memory(device, shader->alloc);
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index 12de11d288db..09ba82db50d7 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -975,13 +975,13 @@ struct radv_device {
    struct list_head shader_block_obj_pool;
    mtx_t shader_arena_mutex;
 
-   mtx_t shader_upload_hw_ctx_mutex;
-   struct radeon_winsys_ctx *shader_upload_hw_ctx;
-   VkSemaphore shader_upload_sem;
-   uint64_t shader_upload_seq;
-   struct list_head shader_dma_submissions;
-   mtx_t shader_dma_submission_list_mutex;
-   cnd_t shader_dma_submission_list_cond;
+   mtx_t shader_dma_hw_ctx_mutex;
+   struct radeon_winsys_ctx *shader_dma_hw_ctx;
+   VkSemaphore shader_dma_sem;
+   uint64_t shader_dma_seq;
+   struct list_head shader_upload_submission_list;
+   mtx_t shader_upload_submission_list_mutex;
+   cnd_t shader_upload_submission_list_cond;
 
    /* Whether to DMA shaders to invisible VRAM or to upload directly through BAR. */
    bool shader_use_invisible_vram;
diff --git a/src/amd/vulkan/radv_queue.c b/src/amd/vulkan/radv_queue.c
index 0823e545a190..7db51e93ddb5 100644
--- a/src/amd/vulkan/radv_queue.c
+++ b/src/amd/vulkan/radv_queue.c
@@ -1549,7 +1549,7 @@ static void
 radv_get_shader_upload_sync_wait(struct radv_device *device, uint64_t shader_upload_seq,
                                  struct vk_sync_wait *out_sync_wait)
 {
-   struct vk_semaphore *semaphore = vk_semaphore_from_handle(device->shader_upload_sem);
+   struct vk_semaphore *semaphore = vk_semaphore_from_handle(device->shader_dma_sem);
    struct vk_sync *sync = vk_semaphore_get_active_sync(semaphore);
    *out_sync_wait = (struct vk_sync_wait){
       .sync = sync,
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 0129516cee59..4226aaa93b2a 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -1001,14 +1001,14 @@ free_block_obj(struct radv_device *device, union radv_shader_arena_block *block)
 }
 
 VkResult
-radv_shader_wait_for_upload(struct radv_device *device, uint64_t seq)
+radv_shader_dma_wait(struct radv_device *device, uint64_t seq)
 {
    if (!seq)
       return VK_SUCCESS;
 
    const VkSemaphoreWaitInfo wait_info = {
       .sType = VK_STRUCTURE_TYPE_SEMAPHORE_WAIT_INFO,
-      .pSemaphores = &device->shader_upload_sem,
+      .pSemaphores = &device->shader_dma_sem,
       .semaphoreCount = 1,
       .pValues = &seq,
    };
@@ -1240,7 +1240,7 @@ radv_destroy_shader_arenas(struct radv_device *device)
 }
 
 VkResult
-radv_init_shader_upload_queue(struct radv_device *device)
+radv_init_shader_dma(struct radv_device *device)
 {
    if (!device->shader_use_invisible_vram)
       return VK_SUCCESS;
@@ -1251,21 +1251,21 @@ radv_init_shader_upload_queue(struct radv_device *device)
    const struct vk_device_dispatch_table *disp = &device->vk.dispatch_table;
    VkResult result = VK_SUCCESS;
 
-   result = ws->ctx_create(ws, RADEON_CTX_PRIORITY_MEDIUM, &device->shader_upload_hw_ctx);
+   result = ws->ctx_create(ws, RADEON_CTX_PRIORITY_MEDIUM, &device->shader_dma_hw_ctx);
    if (result != VK_SUCCESS)
       return result;
-   mtx_init(&device->shader_upload_hw_ctx_mutex, mtx_plain);
+   mtx_init(&device->shader_dma_hw_ctx_mutex, mtx_plain);
 
-   mtx_init(&device->shader_dma_submission_list_mutex, mtx_plain);
-   cnd_init(&device->shader_dma_submission_list_cond);
-   list_inithead(&device->shader_dma_submissions);
+   mtx_init(&device->shader_upload_submission_list_mutex, mtx_plain);
+   cnd_init(&device->shader_upload_submission_list_cond);
+   list_inithead(&device->shader_upload_submission_list);
 
    for (unsigned i = 0; i < RADV_SHADER_UPLOAD_CS_COUNT; i++) {
       struct radv_shader_dma_submission *submission = calloc(1, sizeof(struct radv_shader_dma_submission));
       submission->cs = ws->cs_create(ws, AMD_IP_SDMA, false);
       if (!submission->cs)
          return VK_ERROR_OUT_OF_HOST_MEMORY;
-      list_addtail(&submission->list, &device->shader_dma_submissions);
+      list_addtail(&submission->list, &device->shader_upload_submission_list);
    }
 
    const VkSemaphoreTypeCreateInfo sem_type = {
@@ -1277,7 +1277,7 @@ radv_init_shader_upload_queue(struct radv_device *device)
       .sType = VK_STRUCTURE_TYPE_SEMAPHORE_CREATE_INFO,
       .pNext = &sem_type,
    };
-   result = disp->CreateSemaphore(vk_device, &sem_create, NULL, &device->shader_upload_sem);
+   result = disp->CreateSemaphore(vk_device, &sem_create, NULL, &device->shader_dma_sem);
    if (result != VK_SUCCESS)
       return result;
 
@@ -1285,7 +1285,7 @@ radv_init_shader_upload_queue(struct radv_device *device)
 }
 
 void
-radv_destroy_shader_upload_queue(struct radv_device *device)
+radv_finish_shader_dma(struct radv_device *device)
 {
    if (!device->shader_use_invisible_vram)
       return;
@@ -1294,11 +1294,11 @@ radv_destroy_shader_upload_queue(struct radv_device *device)
    struct radeon_winsys *ws = device->ws;
 
    /* Upload queue should be idle assuming that pipelines are not leaked */
-   if (device->shader_upload_sem)
-      disp->DestroySemaphore(radv_device_to_handle(device), device->shader_upload_sem, NULL);
+   if (device->shader_dma_sem)
+      disp->DestroySemaphore(radv_device_to_handle(device), device->shader_dma_sem, NULL);
 
    list_for_each_entry_safe(struct radv_shader_dma_submission, submission,
-                            &device->shader_dma_submissions, list)
+                            &device->shader_upload_submission_list, list)
    {
       if (submission->cs)
          ws->cs_destroy(submission->cs);
@@ -1308,12 +1308,12 @@ radv_destroy_shader_upload_queue(struct radv_device *device)
       free(submission);
    }
 
-   cnd_destroy(&device->shader_dma_submission_list_cond);
-   mtx_destroy(&device->shader_dma_submission_list_mutex);
+   cnd_destroy(&device->shader_upload_submission_list_cond);
+   mtx_destroy(&device->shader_upload_submission_list_mutex);
 
-   if (device->shader_upload_hw_ctx) {
-      mtx_destroy(&device->shader_upload_hw_ctx_mutex);
-      ws->ctx_destroy(device->shader_upload_hw_ctx);
+   if (device->shader_dma_hw_ctx) {
+      mtx_destroy(&device->shader_dma_hw_ctx_mutex);
+      ws->ctx_destroy(device->shader_dma_hw_ctx);
    }
 }
 
@@ -1752,7 +1752,7 @@ radv_shader_binary_upload(struct radv_device *device, const struct radv_shader_b
 }
 
 static VkResult
-radv_shader_dma_resize_upload_buf(struct radv_shader_dma_submission *submission,
+radv_shader_dma_resize_staging_buf(struct radv_shader_dma_submission *submission,
                                   struct radeon_winsys *ws, uint64_t size)
 {
    if (submission->bo)
@@ -1773,56 +1773,57 @@ radv_shader_dma_resize_upload_buf(struct radv_shader_dma_submission *submission,
 }
 
 struct radv_shader_dma_submission *
-radv_shader_dma_pop_submission(struct radv_device *device)
+radv_shader_upload_pop_submission(struct radv_device *device)
 {
    struct radv_shader_dma_submission *submission;
 
-   mtx_lock(&device->shader_dma_submission_list_mutex);
+   mtx_lock(&device->shader_upload_submission_list_mutex);
 
-   while (list_is_empty(&device->shader_dma_submissions))
-      cnd_wait(&device->shader_dma_submission_list_cond, &device->shader_dma_submission_list_mutex);
+   while (list_is_empty(&device->shader_upload_submission_list))
+      cnd_wait(&device->shader_upload_submission_list_cond,
+               &device->shader_upload_submission_list_mutex);
 
-   submission =
-      list_first_entry(&device->shader_dma_submissions, struct radv_shader_dma_submission, list);
+   submission = list_first_entry(&device->shader_upload_submission_list,
+                                 struct radv_shader_dma_submission, list);
    list_del(&submission->list);
 
-   mtx_unlock(&device->shader_dma_submission_list_mutex);
+   mtx_unlock(&device->shader_upload_submission_list_mutex);
 
    return submission;
 }
 
 void
-radv_shader_dma_push_submission(struct radv_device *device,
-                                struct radv_shader_dma_submission *submission, uint64_t seq)
+radv_shader_upload_push_submission(struct radv_device *device,
+                                   struct radv_shader_dma_submission *submission, uint64_t seq)
 {
    submission->seq = seq;
 
-   mtx_lock(&device->shader_dma_submission_list_mutex);
+   mtx_lock(&device->shader_upload_submission_list_mutex);
 
-   list_addtail(&submission->list, &device->shader_dma_submissions);
-   cnd_signal(&device->shader_dma_submission_list_cond);
+   list_addtail(&submission->list, &device->shader_upload_submission_list);
+   cnd_signal(&device->shader_upload_submission_list_cond);
 
-   mtx_unlock(&device->shader_dma_submission_list_mutex);
+   mtx_unlock(&device->shader_upload_submission_list_mutex);
 }
 
 struct radv_shader_dma_submission *
-radv_shader_dma_get_submission(struct radv_device *device, struct radeon_winsys_bo *bo, uint64_t va,
-                               uint64_t size)
+radv_shader_upload_get_submission(struct radv_device *device, struct radeon_winsys_bo *bo,
+                                  uint64_t va, uint64_t size)
 {
-   struct radv_shader_dma_submission *submission = radv_shader_dma_pop_submission(device);
+   struct radv_shader_dma_submission *submission = radv_shader_upload_pop_submission(device);
    struct radeon_cmdbuf *cs = submission->cs;
    struct radeon_winsys *ws = device->ws;
    VkResult result;
 
    /* Wait for potentially in-flight submission to settle */
-   result = radv_shader_wait_for_upload(device, submission->seq);
+   result = radv_shader_dma_wait(device, submission->seq);
    if (result != VK_SUCCESS)
       goto fail;
 
    ws->cs_reset(cs);
 
    if (submission->bo_size < size) {
-      result = radv_shader_dma_resize_upload_buf(submission, ws, size);
+      result = radv_shader_dma_resize_staging_buf(submission, ws, size);
       if (result != VK_SUCCESS)
          goto fail;
    }
@@ -1838,7 +1839,7 @@ radv_shader_dma_get_submission(struct radv_device *device, struct radeon_winsys_
    return submission;
 
 fail:
-   radv_shader_dma_push_submission(device, submission, 0);
+   radv_shader_upload_push_submission(device, submission, 0);
 
    return NULL;
 }
@@ -1848,18 +1849,19 @@ fail:
  * semaphore value to wait on device->shader_upload_sem is stored in *upload_seq_out.
  */
 bool
-radv_shader_dma_submit(struct radv_device *device, struct radv_shader_dma_submission *submission,
-                       uint64_t *upload_seq_out)
+radv_shader_upload_submit(struct radv_device *device,
+                          struct radv_shader_dma_submission *submission,
+                          uint64_t *upload_seq_out)
 {
    struct radeon_cmdbuf *cs = submission->cs;
    struct radeon_winsys *ws = device->ws;
    VkResult result;
 
-   mtx_lock(&device->shader_upload_hw_ctx_mutex);
+   mtx_lock(&device->shader_dma_hw_ctx_mutex);
 
-   uint64_t upload_seq = device->shader_upload_seq + 1;
+   uint64_t upload_seq = device->shader_dma_seq + 1;
 
-   struct vk_semaphore *semaphore = vk_semaphore_from_handle(device->shader_upload_sem);
+   struct vk_semaphore *semaphore = vk_semaphore_from_handle(device->shader_dma_sem);
    struct vk_sync *sync = vk_semaphore_get_active_sync(semaphore);
    const struct vk_sync_signal signal_info = {
       .sync = sync,
@@ -1874,22 +1876,22 @@ radv_shader_dma_submit(struct radv_device *device, struct radv_shader_dma_submis
       .cs_count = 1,
    };
 
-   result = ws->cs_submit(device->shader_upload_hw_ctx, &submit, 0, NULL, 1, &signal_info);
+   result = ws->cs_submit(device->shader_dma_hw_ctx, &submit, 0, NULL, 1, &signal_info);
    if (result != VK_SUCCESS)
    {
-      mtx_unlock(&device->shader_upload_hw_ctx_mutex);
-      radv_shader_dma_push_submission(device, submission, 0);
+      mtx_unlock(&device->shader_dma_hw_ctx_mutex);
+      radv_shader_upload_push_submission(device, submission, 0);
       return false;
    }
-   device->shader_upload_seq = upload_seq;
-   mtx_unlock(&device->shader_upload_hw_ctx_mutex);
+   device->shader_dma_seq = upload_seq;
+   mtx_unlock(&device->shader_dma_hw_ctx_mutex);
 
-   radv_shader_dma_push_submission(device, submission, upload_seq);
+   radv_shader_upload_push_submission(device, submission, upload_seq);
 
    if (upload_seq_out) {
       *upload_seq_out = upload_seq;
    } else {
-      result = radv_shader_wait_for_upload(device, upload_seq);
+      result = radv_shader_dma_wait(device, upload_seq);
       if (result != VK_SUCCESS)
          return false;
    }
@@ -1950,16 +1952,16 @@ radv_shader_create(struct radv_device *device, const struct radv_shader_binary *
 
    if (device->shader_use_invisible_vram) {
       struct radv_shader_dma_submission *submission =
-         radv_shader_dma_get_submission(device, shader->bo, shader->va, shader->code_size);
+         radv_shader_upload_get_submission(device, shader->bo, shader->va, shader->code_size);
       if (!submission)
          return NULL;
 
       if (!radv_shader_binary_upload(device, binary, shader, submission->ptr)) {
-         radv_shader_dma_push_submission(device, submission, 0);
+         radv_shader_upload_push_submission(device, submission, 0);
          return NULL;
       }
 
-      if (!radv_shader_dma_submit(device, submission, &shader->upload_seq))
+      if (!radv_shader_upload_submit(device, submission, &shader->upload_seq))
          return NULL;
    } else {
       void *dest_ptr = shader->alloc->arena->ptr + shader->alloc->offset;
@@ -1982,7 +1984,7 @@ radv_shader_part_binary_upload(struct radv_device *device, const struct radv_sha
    if (device->shader_use_invisible_vram) {
       uint64_t va = radv_buffer_get_va(shader_part->alloc->arena->bo) + shader_part->alloc->offset;
       submission =
-         radv_shader_dma_get_submission(device, shader_part->alloc->arena->bo, va, code_size);
+         radv_shader_upload_get_submission(device, shader_part->alloc->arena->bo, va, code_size);
       if (!submission)
          return false;
 
@@ -1998,7 +2000,7 @@ radv_shader_part_binary_upload(struct radv_device *device, const struct radv_sha
       ptr32[i] = DEBUGGER_END_OF_CODE_MARKER;
 
    if (device->shader_use_invisible_vram) {
-      if (!radv_shader_dma_submit(device, submission, &shader_part->upload_seq))
+      if (!radv_shader_upload_submit(device, submission, &shader_part->upload_seq))
          return false;
    }
 
@@ -2531,7 +2533,7 @@ radv_shader_part_destroy(struct radv_device *device, struct radv_shader_part *sh
 
    if (device->shader_use_invisible_vram) {
       /* Wait for any pending upload to complete, or we'll be writing into freed shader memory. */
-      radv_shader_wait_for_upload(device, shader_part->upload_seq);
+      radv_shader_dma_wait(device, shader_part->upload_seq);
    }
 
    if (shader_part->alloc)
diff --git a/src/amd/vulkan/radv_shader.h b/src/amd/vulkan/radv_shader.h
index 21554d3dd300..bfc73a966896 100644
--- a/src/amd/vulkan/radv_shader.h
+++ b/src/amd/vulkan/radv_shader.h
@@ -564,8 +564,8 @@ nir_shader *radv_shader_spirv_to_nir(struct radv_device *device,
 
 void radv_init_shader_arenas(struct radv_device *device);
 void radv_destroy_shader_arenas(struct radv_device *device);
-VkResult radv_init_shader_upload_queue(struct radv_device *device);
-void radv_destroy_shader_upload_queue(struct radv_device *device);
+VkResult radv_init_shader_dma(struct radv_device *device);
+void radv_finish_shader_dma(struct radv_device *device);
 
 struct radv_shader_args;
 
@@ -582,22 +582,21 @@ radv_shader_nir_to_asm(struct radv_device *device, struct vk_pipeline_cache *cac
                        int shader_count, const struct radv_pipeline_key *key, bool keep_shader_info,
                        bool keep_statistic_info, struct radv_shader_binary **binary_out);
 
-VkResult radv_shader_wait_for_upload(struct radv_device *device, uint64_t seq);
+VkResult radv_shader_dma_wait(struct radv_device *device, uint64_t seq);
 
-struct radv_shader_dma_submission *
-radv_shader_dma_pop_submission(struct radv_device *device);
+struct radv_shader_dma_submission *radv_shader_upload_pop_submission(struct radv_device *device);
 
-void radv_shader_dma_push_submission(struct radv_device *device,
-                                     struct radv_shader_dma_submission *submission,
-                                     uint64_t seq);
+void radv_shader_upload_push_submission(struct radv_device *device,
+                                        struct radv_shader_dma_submission *submission,
+                                        uint64_t seq);
 
-struct radv_shader_dma_submission *radv_shader_dma_get_submission(struct radv_device *device,
-                                                                  struct radeon_winsys_bo *bo,
-                                                                  uint64_t va, uint64_t size);
+struct radv_shader_dma_submission *radv_shader_upload_get_submission(struct radv_device *device,
+                                                                        struct radeon_winsys_bo *bo,
+                                                                        uint64_t va, uint64_t size);
 
-bool radv_shader_dma_submit(struct radv_device *device,
-                            struct radv_shader_dma_submission *submission,
-                            uint64_t *upload_seq_out);
+bool radv_shader_upload_submit(struct radv_device *device,
+                               struct radv_shader_dma_submission *submission,
+                               uint64_t *upload_seq_out);
 
 union radv_shader_arena_block *radv_alloc_shader_memory(struct radv_device *device, uint32_t size,
                                                         void *ptr);
-- 
GitLab


From 3364c2851f2e44d0a72a30b74a7cdde1e1bc256a Mon Sep 17 00:00:00 2001
From: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
Date: Tue, 28 Mar 2023 20:55:29 +0900
Subject: [PATCH 2/5] radv: Move dma submission init and finish to separate
 helpers.

---
 src/amd/vulkan/radv_shader.c | 34 +++++++++++++++++++++++++++-------
 1 file changed, 27 insertions(+), 7 deletions(-)

diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 4226aaa93b2a..66746a708568 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -1239,6 +1239,29 @@ radv_destroy_shader_arenas(struct radv_device *device)
    mtx_destroy(&device->shader_arena_mutex);
 }
 
+static VkResult
+radv_shader_dma_submission_init(struct radv_device *device, struct radv_shader_dma_submission* submission)
+{
+   struct radeon_winsys *ws = device->ws;
+
+   submission->cs = ws->cs_create(ws, AMD_IP_SDMA, false);
+   if (!submission->cs)
+      return VK_ERROR_OUT_OF_HOST_MEMORY;
+
+   return VK_SUCCESS;
+}
+
+static void
+radv_shader_dma_submission_finish(struct radv_device *device, struct radv_shader_dma_submission* submission)
+{
+   struct radeon_winsys *ws = device->ws;
+
+   if (submission->cs)
+      ws->cs_destroy(submission->cs);
+   if (submission->bo)
+      ws->buffer_destroy(ws, submission->bo);
+}
+
 VkResult
 radv_init_shader_dma(struct radv_device *device)
 {
@@ -1262,9 +1285,9 @@ radv_init_shader_dma(struct radv_device *device)
 
    for (unsigned i = 0; i < RADV_SHADER_UPLOAD_CS_COUNT; i++) {
       struct radv_shader_dma_submission *submission = calloc(1, sizeof(struct radv_shader_dma_submission));
-      submission->cs = ws->cs_create(ws, AMD_IP_SDMA, false);
-      if (!submission->cs)
-         return VK_ERROR_OUT_OF_HOST_MEMORY;
+      result = radv_shader_dma_submission_init(device, submission);
+      if (result != VK_SUCCESS)
+         return result;
       list_addtail(&submission->list, &device->shader_upload_submission_list);
    }
 
@@ -1300,10 +1323,7 @@ radv_finish_shader_dma(struct radv_device *device)
    list_for_each_entry_safe(struct radv_shader_dma_submission, submission,
                             &device->shader_upload_submission_list, list)
    {
-      if (submission->cs)
-         ws->cs_destroy(submission->cs);
-      if (submission->bo)
-         ws->buffer_destroy(ws, submission->bo);
+      radv_shader_dma_submission_finish(device, submission);
       list_del(&submission->list);
       free(submission);
    }
-- 
GitLab


From 3035254a5bedb5b10adb783121b5af4296fc6797 Mon Sep 17 00:00:00 2001
From: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
Date: Tue, 28 Mar 2023 21:09:51 +0900
Subject: [PATCH 3/5] radv: Move common shader DMA submission code to
 radv_shader_dma_submit.

---
 src/amd/vulkan/radv_shader.c | 43 ++++++++++++++++++++++++------------
 1 file changed, 29 insertions(+), 14 deletions(-)

diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 66746a708568..1845a69f9089 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -1240,7 +1240,7 @@ radv_destroy_shader_arenas(struct radv_device *device)
 }
 
 static VkResult
-radv_shader_dma_submission_init(struct radv_device *device, struct radv_shader_dma_submission* submission)
+radv_shader_dma_submission_init(struct radv_device *device, struct radv_shader_dma_submission *submission)
 {
    struct radeon_winsys *ws = device->ws;
 
@@ -1252,7 +1252,7 @@ radv_shader_dma_submission_init(struct radv_device *device, struct radv_shader_d
 }
 
 static void
-radv_shader_dma_submission_finish(struct radv_device *device, struct radv_shader_dma_submission* submission)
+radv_shader_dma_submission_finish(struct radv_device *device, struct radv_shader_dma_submission *submission)
 {
    struct radeon_winsys *ws = device->ws;
 
@@ -1864,14 +1864,9 @@ fail:
    return NULL;
 }
 
-/*
- * If upload_seq_out is NULL, this function blocks until the DMA is complete. Otherwise, the
- * semaphore value to wait on device->shader_upload_sem is stored in *upload_seq_out.
- */
-bool
-radv_shader_upload_submit(struct radv_device *device,
-                          struct radv_shader_dma_submission *submission,
-                          uint64_t *upload_seq_out)
+static VkResult
+radv_shader_dma_submit(struct radv_device *device, struct radv_shader_dma_submission *submission,
+                       uint64_t *upload_seq_out)
 {
    struct radeon_cmdbuf *cs = submission->cs;
    struct radeon_winsys *ws = device->ws;
@@ -1897,16 +1892,36 @@ radv_shader_upload_submit(struct radv_device *device,
    };
 
    result = ws->cs_submit(device->shader_dma_hw_ctx, &submit, 0, NULL, 1, &signal_info);
-   if (result != VK_SUCCESS)
-   {
+   if (result != VK_SUCCESS) {
+      *upload_seq_out = 0;
       mtx_unlock(&device->shader_dma_hw_ctx_mutex);
-      radv_shader_upload_push_submission(device, submission, 0);
-      return false;
+      return result;
    }
+
+   *upload_seq_out = upload_seq;
    device->shader_dma_seq = upload_seq;
    mtx_unlock(&device->shader_dma_hw_ctx_mutex);
+   return result;
+}
 
+/*
+ * If upload_seq_out is NULL, this function blocks until the DMA is complete. Otherwise, the
+ * semaphore value to wait on device->shader_upload_sem is stored in *upload_seq_out.
+ */
+bool
+radv_shader_upload_submit(struct radv_device *device,
+                          struct radv_shader_dma_submission *submission,
+                          uint64_t *upload_seq_out)
+{
+   struct radeon_cmdbuf *cs = submission->cs;
+   struct radeon_winsys *ws = device->ws;
+   uint64_t upload_seq;
+   VkResult result;
+
+   result = radv_shader_dma_submit(device, submission, &upload_seq);
    radv_shader_upload_push_submission(device, submission, upload_seq);
+   if (result != VK_SUCCESS)
+      return false;
 
    if (upload_seq_out) {
       *upload_seq_out = upload_seq;
-- 
GitLab


From 4c87f86fc04e24c2ed42de19297ae6f374e23aa0 Mon Sep 17 00:00:00 2001
From: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
Date: Fri, 31 Mar 2023 19:05:16 +0900
Subject: [PATCH 4/5] radv: Move shader DMA CS building to
 radv_shader_dma_build_submission.

This will be later shared with the download path.
---
 src/amd/vulkan/radv_shader.c | 37 +++++++++++++++++++++++++-----------
 1 file changed, 26 insertions(+), 11 deletions(-)

diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 1845a69f9089..bfd0443e0a8c 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -1826,6 +1826,31 @@ radv_shader_upload_push_submission(struct radv_device *device,
    mtx_unlock(&device->shader_upload_submission_list_mutex);
 }
 
+static VkResult
+radv_shader_dma_build_submission(struct radv_device *device,
+                                 struct radv_shader_dma_submission *submission,
+                                 struct radeon_winsys_bo *bo, uint64_t va, uint64_t size)
+{
+   struct radeon_cmdbuf *cs = submission->cs;
+   struct radeon_winsys *ws = device->ws;
+   uint64_t staging_va;
+   VkResult result;
+
+   if (submission->bo_size < size) {
+      result = radv_shader_dma_resize_staging_buf(submission, ws, size);
+      if (result != VK_SUCCESS)
+         return result;
+   }
+   staging_va = radv_buffer_get_va(submission->bo);
+
+   radv_sdma_copy_buffer(device, cs, staging_va, va, size);
+   radv_cs_add_buffer(ws, cs, submission->bo);
+   radv_cs_add_buffer(ws, cs, bo);
+
+   result = ws->cs_finalize(cs);
+   return result;
+}
+
 struct radv_shader_dma_submission *
 radv_shader_upload_get_submission(struct radv_device *device, struct radeon_winsys_bo *bo,
                                   uint64_t va, uint64_t size)
@@ -1842,17 +1867,7 @@ radv_shader_upload_get_submission(struct radv_device *device, struct radeon_wins
 
    ws->cs_reset(cs);
 
-   if (submission->bo_size < size) {
-      result = radv_shader_dma_resize_staging_buf(submission, ws, size);
-      if (result != VK_SUCCESS)
-         goto fail;
-   }
-
-   radv_sdma_copy_buffer(device, cs, radv_buffer_get_va(submission->bo), va, size);
-   radv_cs_add_buffer(ws, cs, submission->bo);
-   radv_cs_add_buffer(ws, cs, bo);
-
-   result = ws->cs_finalize(cs);
+   result = radv_shader_dma_build_submission(device, submission, bo, va, size);
    if (result != VK_SUCCESS)
       goto fail;
 
-- 
GitLab


From 64b7b618a45b74d0dfa15a38cd7c6d331e77d71d Mon Sep 17 00:00:00 2001
From: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
Date: Thu, 30 Mar 2023 21:49:57 +0900
Subject: [PATCH 5/5] radv: Add a helper to download shaders from invisible
 VRAM.

---
 src/amd/vulkan/radv_private.h |  2 ++
 src/amd/vulkan/radv_shader.c  | 62 +++++++++++++++++++++++++++++------
 src/amd/vulkan/radv_shader.h  |  3 ++
 3 files changed, 57 insertions(+), 10 deletions(-)

diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index 09ba82db50d7..9f2874a81aa2 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -982,6 +982,8 @@ struct radv_device {
    struct list_head shader_upload_submission_list;
    mtx_t shader_upload_submission_list_mutex;
    cnd_t shader_upload_submission_list_cond;
+   struct radv_shader_dma_submission shader_download_submission;
+   mtx_t shader_download_mutex;
 
    /* Whether to DMA shaders to invisible VRAM or to upload directly through BAR. */
    bool shader_use_invisible_vram;
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index bfd0443e0a8c..f1bc7521d11e 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -1291,6 +1291,10 @@ radv_init_shader_dma(struct radv_device *device)
       list_addtail(&submission->list, &device->shader_upload_submission_list);
    }
 
+   result = radv_shader_dma_submission_init(device, &device->shader_download_submission);
+   if (result != VK_SUCCESS)
+      return result;
+
    const VkSemaphoreTypeCreateInfo sem_type = {
       .sType = VK_STRUCTURE_TYPE_SEMAPHORE_TYPE_CREATE_INFO,
       .semaphoreType = VK_SEMAPHORE_TYPE_TIMELINE,
@@ -1320,6 +1324,8 @@ radv_finish_shader_dma(struct radv_device *device)
    if (device->shader_dma_sem)
       disp->DestroySemaphore(radv_device_to_handle(device), device->shader_dma_sem, NULL);
 
+   radv_shader_dma_submission_finish(device, &device->shader_download_submission);
+
    list_for_each_entry_safe(struct radv_shader_dma_submission, submission,
                             &device->shader_upload_submission_list, list)
    {
@@ -1773,16 +1779,18 @@ radv_shader_binary_upload(struct radv_device *device, const struct radv_shader_b
 
 static VkResult
 radv_shader_dma_resize_staging_buf(struct radv_shader_dma_submission *submission,
-                                  struct radeon_winsys *ws, uint64_t size)
+                                   struct radeon_winsys *ws, uint64_t size, bool is_upload)
 {
    if (submission->bo)
       ws->buffer_destroy(ws, submission->bo);
 
-   VkResult result =
-      ws->buffer_create(ws, size, RADV_SHADER_ALLOC_ALIGNMENT, RADEON_DOMAIN_GTT,
-                        RADEON_FLAG_CPU_ACCESS | RADEON_FLAG_NO_INTERPROCESS_SHARING |
-                           RADEON_FLAG_32BIT | RADEON_FLAG_GTT_WC,
-                        RADV_BO_PRIORITY_UPLOAD_BUFFER, 0, &submission->bo);
+   enum radeon_bo_flag flags =
+      RADEON_FLAG_CPU_ACCESS | RADEON_FLAG_NO_INTERPROCESS_SHARING | RADEON_FLAG_32BIT;
+   if (is_upload)
+      flags |= RADEON_FLAG_GTT_WC;
+
+   VkResult result = ws->buffer_create(ws, size, RADV_SHADER_ALLOC_ALIGNMENT, RADEON_DOMAIN_GTT,
+                                       flags, RADV_BO_PRIORITY_UPLOAD_BUFFER, 0, &submission->bo);
    if (result != VK_SUCCESS)
       return result;
 
@@ -1829,7 +1837,8 @@ radv_shader_upload_push_submission(struct radv_device *device,
 static VkResult
 radv_shader_dma_build_submission(struct radv_device *device,
                                  struct radv_shader_dma_submission *submission,
-                                 struct radeon_winsys_bo *bo, uint64_t va, uint64_t size)
+                                 struct radeon_winsys_bo *bo, uint64_t va, uint64_t size,
+                                 bool is_upload)
 {
    struct radeon_cmdbuf *cs = submission->cs;
    struct radeon_winsys *ws = device->ws;
@@ -1837,13 +1846,16 @@ radv_shader_dma_build_submission(struct radv_device *device,
    VkResult result;
 
    if (submission->bo_size < size) {
-      result = radv_shader_dma_resize_staging_buf(submission, ws, size);
+      result = radv_shader_dma_resize_staging_buf(submission, ws, size, is_upload);
       if (result != VK_SUCCESS)
          return result;
    }
    staging_va = radv_buffer_get_va(submission->bo);
 
-   radv_sdma_copy_buffer(device, cs, staging_va, va, size);
+   if (is_upload)
+      radv_sdma_copy_buffer(device, cs, staging_va, va, size);
+   else
+      radv_sdma_copy_buffer(device, cs, va, staging_va, size);
    radv_cs_add_buffer(ws, cs, submission->bo);
    radv_cs_add_buffer(ws, cs, bo);
 
@@ -1867,7 +1879,7 @@ radv_shader_upload_get_submission(struct radv_device *device, struct radeon_wins
 
    ws->cs_reset(cs);
 
-   result = radv_shader_dma_build_submission(device, submission, bo, va, size);
+   result = radv_shader_dma_build_submission(device, submission, bo, va, size, true);
    if (result != VK_SUCCESS)
       goto fail;
 
@@ -1949,6 +1961,36 @@ radv_shader_upload_submit(struct radv_device *device,
    return true;
 }
 
+bool
+radv_shader_dma_download(struct radv_device *device, struct radv_shader *shader, void *buf,
+                         uint32_t size)
+{
+   struct radv_shader_dma_submission *submission = &device->shader_download_submission;
+   struct radeon_cmdbuf *cs = submission->cs;
+   struct radeon_winsys *ws = device->ws;
+   uint64_t download_seq;
+   VkResult result;
+
+   mtx_lock(&device->shader_download_mutex);
+
+   result =
+      radv_shader_dma_build_submission(device, submission, shader->bo, shader->va, size, false);
+   if (result != VK_SUCCESS)
+      goto out;
+   result = radv_shader_dma_submit(device, submission, &download_seq);
+   if (result != VK_SUCCESS)
+      goto out;
+   result = radv_shader_dma_wait(device, download_seq);
+   if (result != VK_SUCCESS)
+      goto out;
+
+   memcpy(buf, submission->ptr, size);
+
+out:
+   ws->cs_reset(cs);
+   mtx_unlock(&device->shader_download_mutex);
+   return result == VK_SUCCESS;
+}
 
 struct radv_shader *
 radv_shader_create(struct radv_device *device, const struct radv_shader_binary *binary)
diff --git a/src/amd/vulkan/radv_shader.h b/src/amd/vulkan/radv_shader.h
index bfc73a966896..eb538ff49305 100644
--- a/src/amd/vulkan/radv_shader.h
+++ b/src/amd/vulkan/radv_shader.h
@@ -598,6 +598,9 @@ bool radv_shader_upload_submit(struct radv_device *device,
                                struct radv_shader_dma_submission *submission,
                                uint64_t *upload_seq_out);
 
+bool radv_shader_dma_download(struct radv_device *device, struct radv_shader *shader, void *buf,
+                              uint32_t size);
+
 union radv_shader_arena_block *radv_alloc_shader_memory(struct radv_device *device, uint32_t size,
                                                         void *ptr);
 void radv_free_shader_memory(struct radv_device *device, union radv_shader_arena_block *alloc);
-- 
GitLab

