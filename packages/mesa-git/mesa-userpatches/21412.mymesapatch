From a1b07e1c70577624d6a13445a1c3c9b7e78c707b Mon Sep 17 00:00:00 2001
From: Georg Lehmann <dadschoorse@gmail.com>
Date: Sun, 19 Feb 2023 16:49:02 +0100
Subject: [PATCH 1/6] aco: use v_permlane(x)16_b32 for masked swizzle

Should be cheaper than ds_swizzle.

Totals from 8 (0.01% of 134913) affected shaders:
CodeSize: 16316 -> 16388 (+0.44%)
Instrs: 3088 -> 3086 (-0.06%)
Latency: 49558 -> 49508 (-0.10%)
InvThroughput: 9180 -> 9198 (+0.20%)
Copies: 376 -> 384 (+2.13%)
---
 src/amd/compiler/aco_instruction_selection.cpp | 16 +++++++++++++++-
 1 file changed, 15 insertions(+), 1 deletion(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 9ea40736d178..27fed9cb21c4 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -249,6 +249,10 @@ emit_masked_swizzle(isel_context* ctx, Builder& bld, Temp src, unsigned mask)
 
       uint16_t dpp_ctrl = 0xffff;
 
+      /* DPP16 before DPP8 before v_permlane(x)16_b32
+       * because DPP16 support modifiers and v_permlane
+       * can't be folded into valu instructions.
+       */
       if (and_mask == 0x1f && or_mask < 4 && xor_mask < 4) {
          unsigned res[4] = {0, 1, 2, 3};
          for (unsigned i = 0; i < 4; i++)
@@ -262,12 +266,22 @@ emit_masked_swizzle(isel_context* ctx, Builder& bld, Temp src, unsigned mask)
          dpp_ctrl = dpp_row_half_mirror;
       } else if (ctx->options->gfx_level >= GFX10 && (and_mask & 0x18) == 0x18 && or_mask < 8 &&
                  xor_mask < 8) {
-         // DPP8 comes last, as it does not allow several modifiers like `abs` that are available with DPP16
          Builder::Result ret = bld.vop1_dpp8(aco_opcode::v_mov_b32, bld.def(v1), src);
          for (unsigned i = 0; i < 8; i++) {
             ret->dpp8().lane_sel[i] = (((i & and_mask) | or_mask) ^ xor_mask) & 0x7;
          }
          return ret;
+      } else if (ctx->options->gfx_level >= GFX10 && (and_mask & 0x10) == 0x10 && or_mask < 0x10) {
+         uint64_t lane_mask = 0;
+         for (unsigned i = 0; i < 16; i++)
+            lane_mask |= uint64_t(((i & and_mask) | or_mask) ^ (xor_mask & 0xf)) << i * 4;
+         aco_opcode opcode =
+            xor_mask & 0x10 ? aco_opcode::v_permlanex16_b32 : aco_opcode::v_permlane16_b32;
+         Temp op1 = bld.copy(bld.def(s1), Operand::c32(lane_mask & 0xffffffff));
+         Temp op2 = bld.copy(bld.def(s1), Operand::c32(lane_mask >> 32));
+         Builder::Result ret = bld.vop3(opcode, bld.def(v1), src, op1, op2);
+         ret->vop3().opsel = 0x3; /* set BOUND_CTRL/FETCH_INACTIVE */
+         return ret;
       }
 
       if (dpp_ctrl != 0xffff)
-- 
GitLab


From d256232083aa7316457082e551b32f0d7bb40b1a Mon Sep 17 00:00:00 2001
From: Georg Lehmann <dadschoorse@gmail.com>
Date: Sun, 19 Feb 2023 17:58:45 +0100
Subject: [PATCH 2/6] nir: add shuffle_xor_32_amd intrinsic

---
 src/compiler/nir/nir_divergence_analysis.c | 1 +
 src/compiler/nir/nir_intrinsics.py         | 2 ++
 2 files changed, 3 insertions(+)

diff --git a/src/compiler/nir/nir_divergence_analysis.c b/src/compiler/nir/nir_divergence_analysis.c
index 6b359e61b663..cb9326f7f599 100644
--- a/src/compiler/nir/nir_divergence_analysis.c
+++ b/src/compiler/nir/nir_divergence_analysis.c
@@ -423,6 +423,7 @@ visit_intrinsic(nir_shader *shader, nir_intrinsic_instr *instr)
    case nir_intrinsic_atomic_counter_read_deref:
    case nir_intrinsic_quad_swizzle_amd:
    case nir_intrinsic_masked_swizzle_amd:
+   case nir_intrinsic_shuffle_xor_32_amd:
    case nir_intrinsic_is_sparse_texels_resident:
    case nir_intrinsic_sparse_residency_code_and:
    case nir_intrinsic_bvh64_intersect_ray_amd:
diff --git a/src/compiler/nir/nir_intrinsics.py b/src/compiler/nir/nir_intrinsics.py
index 25cafc872532..d25925ee8860 100644
--- a/src/compiler/nir/nir_intrinsics.py
+++ b/src/compiler/nir/nir_intrinsics.py
@@ -469,6 +469,8 @@ intrinsic("mbcnt_amd", src_comp=[1, 1], dest_comp=1, bit_sizes=[32], flags=[CAN_
 intrinsic("byte_permute_amd", src_comp=[1, 1, 1], dest_comp=1, bit_sizes=[32], flags=[CAN_ELIMINATE, CAN_REORDER])
 # Compiled to v_permlane16_b32. src = [ value, lanesel_lo, lanesel_hi ]
 intrinsic("lane_permute_16_amd", src_comp=[1, 1, 1], dest_comp=1, bit_sizes=[32], flags=[CAN_ELIMINATE])
+# Compiled to v_permlane64_b32, subgroupShuffleXor(x, 32);
+intrinsic("shuffle_xor_32_amd", src_comp=[0], dest_comp=0, bit_sizes=src0, flags=[CAN_ELIMINATE])
 
 # Basic Geometry Shader intrinsics.
 #
-- 
GitLab


From 44add1027f61a7912c18873fa6b2e041a00b8275 Mon Sep 17 00:00:00 2001
From: Georg Lehmann <dadschoorse@gmail.com>
Date: Sun, 19 Feb 2023 20:36:28 +0100
Subject: [PATCH 3/6] nir: add option to use shuffle_xor_32_amd

---
 src/compiler/nir/nir.h                 |  1 +
 src/compiler/nir/nir_lower_subgroups.c | 13 ++++++++-----
 2 files changed, 9 insertions(+), 5 deletions(-)

diff --git a/src/compiler/nir/nir.h b/src/compiler/nir/nir.h
index 344d604352c6..5967f19f62a8 100644
--- a/src/compiler/nir/nir.h
+++ b/src/compiler/nir/nir.h
@@ -5053,6 +5053,7 @@ typedef struct nir_lower_subgroups_options {
    bool lower_relative_shuffle:1;
    bool lower_shuffle_to_32bit:1;
    bool lower_shuffle_to_swizzle_amd:1;
+   bool lower_shuffle_to_shuffle_xor_32_amd:1;
    bool lower_shuffle:1;
    bool lower_quad:1;
    bool lower_quad_broadcast_dynamic:1;
diff --git a/src/compiler/nir/nir_lower_subgroups.c b/src/compiler/nir/nir_lower_subgroups.c
index 0c6f3a2b28c0..6035f99baeda 100644
--- a/src/compiler/nir/nir_lower_subgroups.c
+++ b/src/compiler/nir/nir_lower_subgroups.c
@@ -203,14 +203,16 @@ lower_shuffle_to_swizzle(nir_builder *b, nir_intrinsic_instr *intrin,
 {
    unsigned mask = nir_src_as_uint(intrin->src[1]);
 
-   if (mask >= 32)
+   if (!(mask < 32 && options->lower_shuffle_to_swizzle_amd) &&
+       !(mask == 32 && options->lower_shuffle_to_shuffle_xor_32_amd))
       return NULL;
 
-   nir_intrinsic_instr *swizzle = nir_intrinsic_instr_create(
-      b->shader, nir_intrinsic_masked_swizzle_amd);
+   nir_intrinsic_op op = mask == 32 ? nir_intrinsic_shuffle_xor_32_amd : nir_intrinsic_masked_swizzle_amd;
+   nir_intrinsic_instr *swizzle = nir_intrinsic_instr_create(b->shader, op);
    swizzle->num_components = intrin->num_components;
    nir_src_copy(&swizzle->src[0], &intrin->src[0], &swizzle->instr);
-   nir_intrinsic_set_swizzle_mask(swizzle, (mask << 10) | 0x1f);
+   if (mask != 32)
+      nir_intrinsic_set_swizzle_mask(swizzle, (mask << 10) | 0x1f);
    nir_ssa_dest_init(&swizzle->instr, &swizzle->dest,
                      intrin->dest.ssa.num_components,
                      intrin->dest.ssa.bit_size, NULL);
@@ -232,7 +234,8 @@ lower_to_shuffle(nir_builder *b, nir_intrinsic_instr *intrin,
                  const nir_lower_subgroups_options *options)
 {
    if (intrin->intrinsic == nir_intrinsic_shuffle_xor &&
-       options->lower_shuffle_to_swizzle_amd &&
+       (options->lower_shuffle_to_swizzle_amd ||
+        options->lower_shuffle_to_shuffle_xor_32_amd) &&
        nir_src_is_const(intrin->src[1])) {
       nir_ssa_def *result =
          lower_shuffle_to_swizzle(b, intrin, options);
-- 
GitLab


From 8a5b0393bdc699c3940be8dd8fa05a943eebd589 Mon Sep 17 00:00:00 2001
From: Georg Lehmann <dadschoorse@gmail.com>
Date: Sun, 19 Feb 2023 20:37:03 +0100
Subject: [PATCH 4/6] aco: implement shuffle_xor_32_amd

We could also implement it on gfx10 with shared vgprs,
but I'm not sure if that's a good idea.
---
 .../compiler/aco_instruction_selection.cpp    | 37 +++++++++++++++++++
 .../aco_instruction_selection_setup.cpp       |  1 +
 2 files changed, 38 insertions(+)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 27fed9cb21c4..810f3301806d 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -8835,6 +8835,43 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
       }
       break;
    }
+   case nir_intrinsic_shuffle_xor_32_amd: {
+      if (bld.lm == s1) {
+         Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
+         /* Result is undefined. */
+         bld.copy(Definition(dst), Operand::zero());
+         break;
+      }
+      Temp src = get_ssa_temp(ctx, instr->src[0].ssa);
+      if (!nir_dest_is_divergent(instr->dest)) {
+         emit_uniform_subgroup(ctx, instr, src);
+         break;
+      }
+      Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
+
+      if (instr->dest.ssa.bit_size != 1)
+         src = as_vgpr(ctx, src);
+
+      if (instr->dest.ssa.bit_size == 1) {
+         assert(src.regClass() == s2);
+         assert(dst.regClass() == s2);
+         Temp lo = bld.tmp(s1), hi = bld.tmp(s1);
+         bld.pseudo(aco_opcode::p_split_vector, Definition(lo), Definition(hi), src);
+         bld.pseudo(aco_opcode::p_create_vector, Definition(dst), hi, lo);
+      } else if (dst.size() == 1) {
+         emit_wqm(bld, bld.vop1(aco_opcode::v_permlane64_b32, bld.def(v1), src), dst);
+      } else if (dst.regClass() == v2) {
+         Temp lo = bld.tmp(v1), hi = bld.tmp(v1);
+         bld.pseudo(aco_opcode::p_split_vector, Definition(lo), Definition(hi), src);
+         lo = emit_wqm(bld, bld.vop1(aco_opcode::v_permlane64_b32, bld.def(v1), lo));
+         hi = emit_wqm(bld, bld.vop1(aco_opcode::v_permlane64_b32, bld.def(v1), hi));
+         bld.pseudo(aco_opcode::p_create_vector, Definition(dst), lo, hi);
+         emit_split_vector(ctx, dst, 2);
+      } else {
+         isel_err(&instr->instr, "Unimplemented NIR instr bit size");
+      }
+      break;
+   }
    case nir_intrinsic_load_helper_invocation:
    case nir_intrinsic_is_helper_invocation: {
       /* load_helper() after demote() get lowered to is_helper().
diff --git a/src/amd/compiler/aco_instruction_selection_setup.cpp b/src/amd/compiler/aco_instruction_selection_setup.cpp
index d43c5966ed53..befdff739598 100644
--- a/src/amd/compiler/aco_instruction_selection_setup.cpp
+++ b/src/amd/compiler/aco_instruction_selection_setup.cpp
@@ -695,6 +695,7 @@ init_context(isel_context* ctx, nir_shader* shader)
                   }
                   FALLTHROUGH;
                case nir_intrinsic_shuffle:
+               case nir_intrinsic_shuffle_xor_32_amd:
                case nir_intrinsic_quad_broadcast:
                case nir_intrinsic_quad_swap_horizontal:
                case nir_intrinsic_quad_swap_vertical:
-- 
GitLab


From e19a45e1225d71134a4297366b7505c064f10313 Mon Sep 17 00:00:00 2001
From: Georg Lehmann <dadschoorse@gmail.com>
Date: Sun, 19 Feb 2023 20:42:13 +0100
Subject: [PATCH 5/6] radv: use lower_shuffle_to_shuffle_xor_32_amd on gfx11

Foz-DB GFX1100:
Totals from 2 (0.00% of 134913) affected shaders:
CodeSize: 2344 -> 2156 (-8.02%)
Instrs: 451 -> 417 (-7.54%)
Latency: 7299 -> 7227 (-0.99%)
InvThroughput: 913 -> 901 (-1.31%)
PreVGPRs: 18 -> 16 (-11.11%)
---
 src/amd/vulkan/radv_shader.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 3908a03baa97..68cadd557ea6 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -952,6 +952,7 @@ radv_shader_spirv_to_nir(struct radv_device *device, const struct radv_pipeline_
    NIR_PASS(_, nir, nir_lower_global_vars_to_local);
    NIR_PASS(_, nir, nir_remove_dead_variables, nir_var_function_temp, NULL);
    bool gfx7minus = device->physical_device->rad_info.gfx_level <= GFX7;
+   bool gfx11plus = device->physical_device->rad_info.gfx_level >= GFX11;
    NIR_PASS(_, nir, nir_lower_subgroups,
             &(struct nir_lower_subgroups_options){
                .subgroup_size = subgroup_size,
@@ -965,6 +966,8 @@ radv_shader_spirv_to_nir(struct radv_device *device, const struct radv_pipeline_
                .lower_quad_broadcast_dynamic = 1,
                .lower_quad_broadcast_dynamic_to_const = gfx7minus,
                .lower_shuffle_to_swizzle_amd = 1,
+               .lower_shuffle_to_shuffle_xor_32_amd =
+                  gfx11plus && !radv_use_llvm_for_stage(device, stage->stage),
             });
 
    NIR_PASS(_, nir, nir_lower_load_const_to_scalar);
-- 
GitLab


From 3e7e9043284072419d865b35cba48fcc506fa8cb Mon Sep 17 00:00:00 2001
From: Georg Lehmann <dadschoorse@gmail.com>
Date: Sun, 19 Feb 2023 22:09:01 +0100
Subject: [PATCH 6/6] aco/gfx11: use dpp_row_xmask and dpp_row_share

---
 src/amd/compiler/aco_builder_h.py              | 18 +++++++++++++++++-
 src/amd/compiler/aco_instruction_selection.cpp |  6 ++++++
 src/amd/compiler/aco_print_ir.cpp              |  4 ++++
 3 files changed, 27 insertions(+), 1 deletion(-)

diff --git a/src/amd/compiler/aco_builder_h.py b/src/amd/compiler/aco_builder_h.py
index 816bc33aa02d..0cc869b1073d 100644
--- a/src/amd/compiler/aco_builder_h.py
+++ b/src/amd/compiler/aco_builder_h.py
@@ -43,7 +43,9 @@ enum dpp_ctrl {
     dpp_row_mirror = 0x140,
     dpp_row_half_mirror = 0x141,
     dpp_row_bcast15 = 0x142,
-    dpp_row_bcast31 = 0x143
+    dpp_row_bcast31 = 0x143,
+    _dpp_row_share = 0x150,
+    _dpp_row_xmask = 0x160,
 };
 
 inline dpp_ctrl
@@ -74,6 +76,20 @@ dpp_row_rr(unsigned amount)
     return (dpp_ctrl)(((unsigned) _dpp_row_rr) | amount);
 }
 
+inline dpp_ctrl
+dpp_row_share(unsigned lane)
+{
+    assert(lane < 16);
+    return (dpp_ctrl)(((unsigned) _dpp_row_share) | lane);
+}
+
+inline dpp_ctrl
+dpp_row_xmask(unsigned mask)
+{
+    assert(mask < 16);
+    return (dpp_ctrl)(((unsigned) _dpp_row_xmask) | mask);
+}
+
 inline unsigned
 ds_pattern_bitmode(unsigned and_mask, unsigned or_mask, unsigned xor_mask)
 {
diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 810f3301806d..b7f23a99380a 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -264,6 +264,12 @@ emit_masked_swizzle(isel_context* ctx, Builder& bld, Temp src, unsigned mask)
          dpp_ctrl = dpp_row_mirror;
       } else if (and_mask == 0x1f && !or_mask && xor_mask == 0x7) {
          dpp_ctrl = dpp_row_half_mirror;
+      } else if (ctx->options->gfx_level >= GFX11 && and_mask == 0x10 && or_mask < 0x10 &&
+                 !xor_mask) {
+         dpp_ctrl = dpp_row_share(or_mask);
+      } else if (ctx->options->gfx_level >= GFX11 && and_mask == 0x1f && !or_mask &&
+                 xor_mask < 0x10) {
+         dpp_ctrl = dpp_row_xmask(xor_mask);
       } else if (ctx->options->gfx_level >= GFX10 && (and_mask & 0x18) == 0x18 && or_mask < 8 &&
                  xor_mask < 8) {
          Builder::Result ret = bld.vop1_dpp8(aco_opcode::v_mov_b32, bld.def(v1), src);
diff --git a/src/amd/compiler/aco_print_ir.cpp b/src/amd/compiler/aco_print_ir.cpp
index b0f4ae471c12..c1f5a11bf067 100644
--- a/src/amd/compiler/aco_print_ir.cpp
+++ b/src/amd/compiler/aco_print_ir.cpp
@@ -667,6 +667,10 @@ print_instr_format_specific(enum amd_gfx_level gfx_level, const Instruction* ins
          fprintf(output, " row_bcast:15");
       } else if (dpp.dpp_ctrl == dpp_row_bcast31) {
          fprintf(output, " row_bcast:31");
+      } else if (dpp.dpp_ctrl >= dpp_row_share(0) && dpp.dpp_ctrl <= dpp_row_share(15)) {
+         fprintf(output, " row_share:%d", dpp.dpp_ctrl & 0xf);
+      } else if (dpp.dpp_ctrl >= dpp_row_xmask(0) && dpp.dpp_ctrl <= dpp_row_xmask(15)) {
+         fprintf(output, " row_xmask:%d", dpp.dpp_ctrl & 0xf);
       } else {
          fprintf(output, " dpp_ctrl:0x%.3x", dpp.dpp_ctrl);
       }
-- 
GitLab

