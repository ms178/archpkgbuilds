From 3863b9fa316e28af35474c2de0704bcb6251a6dd Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Fri, 8 Dec 2023 22:34:00 -0500
Subject: [PATCH 01/36] winsys/amdgpu: reduce wasted memory due to the size
 tolerance in pb_cache

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c b/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c
index f7a06c8c25f3d..f271fbcc74f2c 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c
@@ -445,7 +445,7 @@ amdgpu_winsys_create(int fd, const struct pipe_screen_config *config,
 
       /* Create managers. */
       pb_cache_init(&aws->bo_cache, RADEON_NUM_HEAPS,
-                    500000, aws->check_vm ? 1.0f : 2.0f, 0,
+                    500000, aws->check_vm ? 1.0f : 1.5f, 0,
                     ((uint64_t)aws->info.vram_size_kb + aws->info.gart_size_kb) * 1024 / 8, aws,
                     /* Cast to void* because one of the function parameters
                      * is a struct pointer instead of void*. */
-- 
GitLab


From b0b915675f5a98b301e2ac248c30ee2c71275067 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Fri, 8 Dec 2023 23:39:36 -0500
Subject: [PATCH 02/36] gallium/pb_slab: move group_index and entry_size from
 pb_slab_entry to pb_slab

This removes 8 bytes from every slab entry, and thus amdgpu_bo_slab.

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/auxiliary/pipebuffer/pb_slab.c    |  2 +-
 src/gallium/auxiliary/pipebuffer/pb_slab.h    |  4 ++--
 src/gallium/drivers/iris/iris_bufmgr.c        |  4 ++--
 src/gallium/drivers/zink/zink_bo.c            |  4 ++--
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c     | 10 +++++-----
 src/gallium/winsys/radeon/drm/radeon_drm_bo.c |  4 ++--
 6 files changed, 14 insertions(+), 14 deletions(-)

diff --git a/src/gallium/auxiliary/pipebuffer/pb_slab.c b/src/gallium/auxiliary/pipebuffer/pb_slab.c
index 8b79dfa1f96ac..24f4d3411377f 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_slab.c
+++ b/src/gallium/auxiliary/pipebuffer/pb_slab.c
@@ -61,7 +61,7 @@ pb_slab_reclaim(struct pb_slabs *slabs, struct pb_slab_entry *entry)
 
    /* Add slab to the group's list if it isn't already linked. */
    if (!list_is_linked(&slab->head)) {
-      struct pb_slab_group *group = &slabs->groups[entry->group_index];
+      struct pb_slab_group *group = &slabs->groups[entry->slab->group_index];
       list_addtail(&slab->head, &group->slabs);
    }
 
diff --git a/src/gallium/auxiliary/pipebuffer/pb_slab.h b/src/gallium/auxiliary/pipebuffer/pb_slab.h
index b59c7b0796a2e..885483e44154c 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_slab.h
+++ b/src/gallium/auxiliary/pipebuffer/pb_slab.h
@@ -62,8 +62,6 @@ struct pb_slab_entry
 {
    struct list_head head;
    struct pb_slab *slab; /* the slab that contains this buffer */
-   unsigned group_index; /* index into pb_slabs::groups */
-   unsigned entry_size;
 };
 
 /* Descriptor of a slab from which many entries are carved out.
@@ -78,6 +76,8 @@ struct pb_slab
    struct list_head free; /* list of free pb_slab_entry structures */
    unsigned num_free; /* number of entries in free list */
    unsigned num_entries; /* total number of entries */
+   unsigned group_index; /* index into pb_slabs::groups */
+   unsigned entry_size;
 };
 
 /* Callback function that is called when a new slab needs to be allocated
diff --git a/src/gallium/drivers/iris/iris_bufmgr.c b/src/gallium/drivers/iris/iris_bufmgr.c
index f0e483289fca4..0d9bbee1f839f 100644
--- a/src/gallium/drivers/iris/iris_bufmgr.c
+++ b/src/gallium/drivers/iris/iris_bufmgr.c
@@ -793,6 +793,8 @@ iris_slab_alloc(void *priv,
 
    slab->base.num_entries = slab_size / entry_size;
    slab->base.num_free = slab->base.num_entries;
+   slab->base.group_index = group_index;
+   slab->base.entry_size = entry_size;
    slab->entry_size = entry_size;
    slab->entries = calloc(slab->base.num_entries, sizeof(*slab->entries));
    if (!slab->entries)
@@ -815,8 +817,6 @@ iris_slab_alloc(void *priv,
       bo->zeroed = slab->bo->zeroed;
 
       bo->slab.entry.slab = &slab->base;
-      bo->slab.entry.group_index = group_index;
-      bo->slab.entry.entry_size = entry_size;
 
       bo->slab.real = iris_get_backing_bo(slab->bo);
 
diff --git a/src/gallium/drivers/zink/zink_bo.c b/src/gallium/drivers/zink/zink_bo.c
index 0ba1e196a0f87..280f6a6080ada 100644
--- a/src/gallium/drivers/zink/zink_bo.c
+++ b/src/gallium/drivers/zink/zink_bo.c
@@ -1242,6 +1242,8 @@ bo_slab_alloc(void *priv, unsigned mem_type_idx, unsigned entry_size, unsigned g
 
    slab->base.num_entries = slab_size / entry_size;
    slab->base.num_free = slab->base.num_entries;
+   slab->base.group_index = group_index;
+   slab->base.entry_size = entry_size;
    slab->entry_size = entry_size;
    slab->entries = CALLOC(slab->base.num_entries, sizeof(*slab->entries));
    if (!slab->entries)
@@ -1260,8 +1262,6 @@ bo_slab_alloc(void *priv, unsigned mem_type_idx, unsigned entry_size, unsigned g
       bo->offset = slab->buffer->offset + i * entry_size;
       bo->unique_id = base_id + i;
       bo->u.slab.entry.slab = &slab->base;
-      bo->u.slab.entry.group_index = group_index;
-      bo->u.slab.entry.entry_size = entry_size;
 
       if (slab->buffer->mem) {
          /* The slab is not suballocated. */
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index ecaea7b161d3b..7807980f15424 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -638,11 +638,11 @@ static struct pb_slabs *get_slabs(struct amdgpu_winsys *ws, uint64_t size)
 
 static unsigned get_slab_wasted_size(struct amdgpu_winsys *ws, struct amdgpu_bo_slab *bo)
 {
-   assert(bo->b.base.size <= bo->entry.entry_size);
+   assert(bo->b.base.size <= bo->entry.slab->entry_size);
    assert(bo->b.base.size < (1 << bo->b.base.alignment_log2) ||
           bo->b.base.size < 1 << ws->bo_slabs[0].min_order ||
-          bo->b.base.size > bo->entry.entry_size / 2);
-   return bo->entry.entry_size - bo->b.base.size;
+          bo->b.base.size > bo->entry.slab->entry_size / 2);
+   return bo->entry.slab->entry_size - bo->b.base.size;
 }
 
 static void amdgpu_bo_slab_destroy(struct radeon_winsys *rws, struct pb_buffer *_buf)
@@ -744,6 +744,8 @@ struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_s
 
    slab->base.num_entries = slab_size / entry_size;
    slab->base.num_free = slab->base.num_entries;
+   slab->base.group_index = group_index;
+   slab->base.entry_size = entry_size;
    slab->entry_size = entry_size;
    slab->entries = CALLOC(slab->base.num_entries, sizeof(*slab->entries));
    if (!slab->entries)
@@ -773,8 +775,6 @@ struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_s
       }
 
       bo->entry.slab = &slab->base;
-      bo->entry.group_index = group_index;
-      bo->entry.entry_size = entry_size;
       list_addtail(&bo->entry.head, &slab->base.free);
    }
 
diff --git a/src/gallium/winsys/radeon/drm/radeon_drm_bo.c b/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
index c44811edb51ad..172062f6c4c91 100644
--- a/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
+++ b/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
@@ -774,6 +774,8 @@ struct pb_slab *radeon_bo_slab_alloc(void *priv, unsigned heap,
 
    slab->base.num_entries = slab->buffer->base.size / entry_size;
    slab->base.num_free = slab->base.num_entries;
+   slab->base.group_index = group_index;
+   slab->base.entry_size = entry_size;
    slab->entries = CALLOC(slab->base.num_entries, sizeof(*slab->entries));
    if (!slab->entries)
       goto fail_buffer;
@@ -794,8 +796,6 @@ struct pb_slab *radeon_bo_slab_alloc(void *priv, unsigned heap,
       bo->initial_domain = domains;
       bo->hash = base_hash + i;
       bo->u.slab.entry.slab = &slab->base;
-      bo->u.slab.entry.group_index = group_index;
-      bo->u.slab.entry.entry_size = entry_size;
       bo->u.slab.real = slab->buffer;
 
       list_addtail(&bo->u.slab.entry.head, &slab->base.free);
-- 
GitLab


From 0897da16388bb6a1265256714966b4e17158b6e3 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Fri, 8 Dec 2023 23:45:04 -0500
Subject: [PATCH 03/36] iris,zink,winsys/amdgpu: remove unused/redundant
 slab->entry_size

slab->base has the same field.

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/drivers/iris/iris_bufmgr.c    | 3 ---
 src/gallium/drivers/zink/zink_bo.c        | 4 +---
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c | 7 +++----
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.h | 1 -
 4 files changed, 4 insertions(+), 11 deletions(-)

diff --git a/src/gallium/drivers/iris/iris_bufmgr.c b/src/gallium/drivers/iris/iris_bufmgr.c
index 0d9bbee1f839f..455d7f7bbaf4c 100644
--- a/src/gallium/drivers/iris/iris_bufmgr.c
+++ b/src/gallium/drivers/iris/iris_bufmgr.c
@@ -179,8 +179,6 @@ struct iris_memregion {
 struct iris_slab {
    struct pb_slab base;
 
-   unsigned entry_size;
-
    /** The BO representing the entire slab */
    struct iris_bo *bo;
 
@@ -795,7 +793,6 @@ iris_slab_alloc(void *priv,
    slab->base.num_free = slab->base.num_entries;
    slab->base.group_index = group_index;
    slab->base.entry_size = entry_size;
-   slab->entry_size = entry_size;
    slab->entries = calloc(slab->base.num_entries, sizeof(*slab->entries));
    if (!slab->entries)
       goto fail_bo;
diff --git a/src/gallium/drivers/zink/zink_bo.c b/src/gallium/drivers/zink/zink_bo.c
index 280f6a6080ada..856dd2439c5db 100644
--- a/src/gallium/drivers/zink/zink_bo.c
+++ b/src/gallium/drivers/zink/zink_bo.c
@@ -69,7 +69,6 @@ struct zink_sparse_commitment {
 
 struct zink_slab {
    struct pb_slab base;
-   unsigned entry_size;
    struct zink_bo *buffer;
    struct zink_bo *entries;
 };
@@ -174,7 +173,7 @@ bo_slab_free(struct zink_screen *screen, struct pb_slab *pslab)
    struct zink_slab *slab = zink_slab(pslab);
    ASSERTED unsigned slab_size = slab->buffer->base.size;
 
-   assert(slab->base.num_entries * slab->entry_size <= slab_size);
+   assert(slab->base.num_entries * slab->base.entry_size <= slab_size);
    FREE(slab->entries);
    zink_bo_unref(screen, slab->buffer);
    FREE(slab);
@@ -1244,7 +1243,6 @@ bo_slab_alloc(void *priv, unsigned mem_type_idx, unsigned entry_size, unsigned g
    slab->base.num_free = slab->base.num_entries;
    slab->base.group_index = group_index;
    slab->base.entry_size = entry_size;
-   slab->entry_size = entry_size;
    slab->entries = CALLOC(slab->base.num_entries, sizeof(*slab->entries));
    if (!slab->entries)
       goto fail_buffer;
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index 7807980f15424..e7412c3215260 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -746,7 +746,6 @@ struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_s
    slab->base.num_free = slab->base.num_entries;
    slab->base.group_index = group_index;
    slab->base.entry_size = entry_size;
-   slab->entry_size = entry_size;
    slab->entries = CALLOC(slab->base.num_entries, sizeof(*slab->entries));
    if (!slab->entries)
       goto fail_buffer;
@@ -799,11 +798,11 @@ void amdgpu_bo_slab_free(struct amdgpu_winsys *ws, struct pb_slab *pslab)
    struct amdgpu_slab *slab = amdgpu_slab(pslab);
    unsigned slab_size = slab->buffer->base.size;
 
-   assert(slab->base.num_entries * slab->entry_size <= slab_size);
+   assert(slab->base.num_entries * slab->base.entry_size <= slab_size);
    if (slab->buffer->base.placement & RADEON_DOMAIN_VRAM)
-      ws->slab_wasted_vram -= slab_size - slab->base.num_entries * slab->entry_size;
+      ws->slab_wasted_vram -= slab_size - slab->base.num_entries * slab->base.entry_size;
    else
-      ws->slab_wasted_gtt -= slab_size - slab->base.num_entries * slab->entry_size;
+      ws->slab_wasted_gtt -= slab_size - slab->base.num_entries * slab->base.entry_size;
 
    for (unsigned i = 0; i < slab->base.num_entries; ++i)
       amdgpu_bo_remove_fences(&slab->entries[i].b);
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
index 5b23b89b9d3ba..e0959e03da01e 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
@@ -126,7 +126,6 @@ struct amdgpu_bo_slab {
 
 struct amdgpu_slab {
    struct pb_slab base;
-   unsigned entry_size;
    struct amdgpu_winsys_bo *buffer;
    struct amdgpu_bo_slab *entries;
 };
-- 
GitLab


From 7e7480479c637150f3fd9afabc9cdef9673558b7 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 9 Dec 2023 00:26:29 -0500
Subject: [PATCH 04/36] winsys/amdgpu: rename to amdgpu_bo_slab to
 amdgpu_bo_slab_entry

It's a slab entry. "Slab" is the whole buffer, which is AMDGPU_BO_REAL
if we want to be precise.

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c | 20 ++++++++++----------
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.h | 12 ++++++------
 src/gallium/winsys/amdgpu/drm/amdgpu_cs.c | 10 +++++-----
 3 files changed, 21 insertions(+), 21 deletions(-)

diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index e7412c3215260..b138361a3bb0f 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -361,7 +361,7 @@ void *amdgpu_bo_map(struct radeon_winsys *rws,
    if (is_real_bo(bo)) {
       real = get_real_bo(bo);
    } else {
-      real = get_slab_bo(bo)->real;
+      real = get_slab_entry_bo(bo)->real;
       offset = bo->va - real->b.va;
    }
 
@@ -401,7 +401,7 @@ void amdgpu_bo_unmap(struct radeon_winsys *rws, struct pb_buffer *buf)
 
    assert(bo->type != AMDGPU_BO_SPARSE);
 
-   real = is_real_bo(bo) ? get_real_bo(bo) : get_slab_bo(bo)->real;
+   real = is_real_bo(bo) ? get_real_bo(bo) : get_slab_entry_bo(bo)->real;
 
    if (real->is_user_ptr)
       return;
@@ -617,7 +617,7 @@ bool amdgpu_bo_can_reclaim(struct amdgpu_winsys *ws, struct pb_buffer *_buf)
 
 bool amdgpu_bo_can_reclaim_slab(void *priv, struct pb_slab_entry *entry)
 {
-   struct amdgpu_bo_slab *bo = container_of(entry, struct amdgpu_bo_slab, entry);
+   struct amdgpu_bo_slab_entry *bo = container_of(entry, struct amdgpu_bo_slab_entry, entry);
 
    return amdgpu_bo_can_reclaim(priv, &bo->b.base);
 }
@@ -636,7 +636,7 @@ static struct pb_slabs *get_slabs(struct amdgpu_winsys *ws, uint64_t size)
    return NULL;
 }
 
-static unsigned get_slab_wasted_size(struct amdgpu_winsys *ws, struct amdgpu_bo_slab *bo)
+static unsigned get_slab_wasted_size(struct amdgpu_winsys *ws, struct amdgpu_bo_slab_entry *bo)
 {
    assert(bo->b.base.size <= bo->entry.slab->entry_size);
    assert(bo->b.base.size < (1 << bo->b.base.alignment_log2) ||
@@ -648,7 +648,7 @@ static unsigned get_slab_wasted_size(struct amdgpu_winsys *ws, struct amdgpu_bo_
 static void amdgpu_bo_slab_destroy(struct radeon_winsys *rws, struct pb_buffer *_buf)
 {
    struct amdgpu_winsys *ws = amdgpu_winsys(rws);
-   struct amdgpu_bo_slab *bo = get_slab_bo(amdgpu_winsys_bo(_buf));
+   struct amdgpu_bo_slab_entry *bo = get_slab_entry_bo(amdgpu_winsys_bo(_buf));
    struct pb_slabs *slabs;
 
    slabs = get_slabs(ws, bo->b.base.size);
@@ -755,13 +755,13 @@ struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_s
    base_id = __sync_fetch_and_add(&ws->next_bo_unique_id, slab->base.num_entries);
 
    for (unsigned i = 0; i < slab->base.num_entries; ++i) {
-      struct amdgpu_bo_slab *bo = &slab->entries[i];
+      struct amdgpu_bo_slab_entry *bo = &slab->entries[i];
 
       bo->b.base.placement = domains;
       bo->b.base.alignment_log2 = util_logbase2(get_slab_entry_alignment(ws, entry_size));
       bo->b.base.size = entry_size;
       bo->b.base.vtbl = &amdgpu_winsys_bo_slab_vtbl;
-      bo->b.type = AMDGPU_BO_SLAB;
+      bo->b.type = AMDGPU_BO_SLAB_ENTRY;
       bo->b.va = slab->buffer->va + i * entry_size;
       bo->b.unique_id = base_id + i;
 
@@ -770,7 +770,7 @@ struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_s
          bo->real = get_real_bo(slab->buffer);
       } else {
          /* The slab is allocated out of a bigger slab. */
-         bo->real = get_slab_bo(slab->buffer)->real;
+         bo->real = get_slab_entry_bo(slab->buffer)->real;
       }
 
       bo->entry.slab = &slab->base;
@@ -1419,7 +1419,7 @@ amdgpu_bo_create(struct amdgpu_winsys *ws,
       if (!entry)
          return NULL;
 
-      struct amdgpu_bo_slab *slab_bo = container_of(entry, struct amdgpu_bo_slab, entry);
+      struct amdgpu_bo_slab_entry *slab_bo = container_of(entry, struct amdgpu_bo_slab_entry, entry);
       pipe_reference_init(&slab_bo->b.base.reference, 1);
       slab_bo->b.base.size = size;
       assert(alignment <= 1 << slab_bo->b.base.alignment_log2);
@@ -1775,7 +1775,7 @@ static bool amdgpu_bo_is_suballocated(struct pb_buffer *buf)
 {
    struct amdgpu_winsys_bo *bo = (struct amdgpu_winsys_bo*)buf;
 
-   return bo->type == AMDGPU_BO_SLAB;
+   return bo->type == AMDGPU_BO_SLAB_ENTRY;
 }
 
 static uint64_t amdgpu_bo_get_va(struct pb_buffer *buf)
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
index e0959e03da01e..833caae1e05f5 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
@@ -36,7 +36,7 @@ struct amdgpu_sparse_commitment {
 };
 
 enum amdgpu_bo_type {
-   AMDGPU_BO_SLAB,
+   AMDGPU_BO_SLAB_ENTRY,
    AMDGPU_BO_SPARSE,
    AMDGPU_BO_REAL, /* only REAL enums can be present after this */
    AMDGPU_BO_REAL_REUSABLE,
@@ -118,7 +118,7 @@ struct amdgpu_bo_sparse {
 /* Suballocated buffer using the slab allocator. This BO is only 1 piece of a larger buffer
  * called slab, which is a buffer that's divided into smaller equal-sized buffers.
  */
-struct amdgpu_bo_slab {
+struct amdgpu_bo_slab_entry {
    struct amdgpu_winsys_bo b;
    struct amdgpu_bo_real *real;
    struct pb_slab_entry entry;
@@ -127,7 +127,7 @@ struct amdgpu_bo_slab {
 struct amdgpu_slab {
    struct pb_slab base;
    struct amdgpu_winsys_bo *buffer;
-   struct amdgpu_bo_slab *entries;
+   struct amdgpu_bo_slab_entry *entries;
 };
 
 static inline bool is_real_bo(struct amdgpu_winsys_bo *bo)
@@ -153,10 +153,10 @@ static struct amdgpu_bo_sparse *get_sparse_bo(struct amdgpu_winsys_bo *bo)
    return (struct amdgpu_bo_sparse*)bo;
 }
 
-static struct amdgpu_bo_slab *get_slab_bo(struct amdgpu_winsys_bo *bo)
+static struct amdgpu_bo_slab_entry *get_slab_entry_bo(struct amdgpu_winsys_bo *bo)
 {
-   assert(bo->type == AMDGPU_BO_SLAB);
-   return (struct amdgpu_bo_slab*)bo;
+   assert(bo->type == AMDGPU_BO_SLAB_ENTRY);
+   return (struct amdgpu_bo_slab_entry*)bo;
 }
 
 bool amdgpu_bo_can_reclaim(struct amdgpu_winsys *ws, struct pb_buffer *_buf);
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
index 87c5dbf7a552d..8a6851f882e58 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
@@ -634,14 +634,14 @@ amdgpu_lookup_or_add_buffer(struct amdgpu_cs_context *cs, struct amdgpu_winsys_b
 static struct amdgpu_cs_buffer *
 amdgpu_lookup_or_add_slab_buffer(struct amdgpu_cs_context *cs, struct amdgpu_winsys_bo *bo)
 {
-   struct amdgpu_buffer_list *list = &cs->buffer_lists[AMDGPU_BO_SLAB];
+   struct amdgpu_buffer_list *list = &cs->buffer_lists[AMDGPU_BO_SLAB_ENTRY];
    struct amdgpu_cs_buffer *buffer = amdgpu_lookup_buffer(cs, bo, list);
 
    if (buffer)
       return buffer;
 
    struct amdgpu_cs_buffer *real_buffer =
-      amdgpu_lookup_or_add_buffer(cs, &get_slab_bo(bo)->real->b, AMDGPU_BO_REAL);
+      amdgpu_lookup_or_add_buffer(cs, &get_slab_entry_bo(bo)->real->b, AMDGPU_BO_REAL);
    if (!real_buffer)
       return NULL;
 
@@ -671,7 +671,7 @@ static unsigned amdgpu_cs_add_buffer(struct radeon_cmdbuf *rcs,
        (usage & cs->last_added_bo_usage) == usage)
       return 0;
 
-   if (bo->type == AMDGPU_BO_SLAB) {
+   if (bo->type == AMDGPU_BO_SLAB_ENTRY) {
       buffer = amdgpu_lookup_or_add_slab_buffer(cs, bo);
       if (!buffer)
          return 0;
@@ -1619,9 +1619,9 @@ cleanup:
    for (i = 0; i < initial_num_real_buffers; i++)
       p_atomic_dec(&cs->buffer_lists[AMDGPU_BO_REAL].buffers[i].bo->num_active_ioctls);
 
-   unsigned num_slab_buffers = cs->buffer_lists[AMDGPU_BO_SLAB].num_buffers;
+   unsigned num_slab_buffers = cs->buffer_lists[AMDGPU_BO_SLAB_ENTRY].num_buffers;
    for (i = 0; i < num_slab_buffers; i++)
-      p_atomic_dec(&cs->buffer_lists[AMDGPU_BO_SLAB].buffers[i].bo->num_active_ioctls);
+      p_atomic_dec(&cs->buffer_lists[AMDGPU_BO_SLAB_ENTRY].buffers[i].bo->num_active_ioctls);
 
    unsigned num_sparse_buffers = cs->buffer_lists[AMDGPU_BO_SPARSE].num_buffers;
    for (i = 0; i < num_sparse_buffers; i++)
-- 
GitLab


From 0aeb2706d7d068b40cc1cf727b3dc236e9197bf6 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 9 Dec 2023 14:49:30 -0500
Subject: [PATCH 05/36] winsys/amdgpu: stop using pb_buffer::vtbl

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/include/winsys/radeon_winsys.h    | 11 +++++-
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c     | 36 +++++++------------
 src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c |  5 ++-
 src/gallium/winsys/radeon/drm/radeon_drm_bo.c |  6 ++++
 4 files changed, 33 insertions(+), 25 deletions(-)

diff --git a/src/gallium/include/winsys/radeon_winsys.h b/src/gallium/include/winsys/radeon_winsys.h
index ce254a92745c9..b79f8861d5fe5 100644
--- a/src/gallium/include/winsys/radeon_winsys.h
+++ b/src/gallium/include/winsys/radeon_winsys.h
@@ -336,6 +336,11 @@ struct radeon_winsys {
    struct pb_buffer *(*buffer_create)(struct radeon_winsys *ws, uint64_t size, unsigned alignment,
                                       enum radeon_bo_domain domain, enum radeon_bo_flag flags);
 
+   /**
+    * Don't use directly. Use radeon_bo_reference.
+    */
+   void (*buffer_destroy)(struct radeon_winsys *ws, struct pb_buffer *buf);
+
    /**
     * Map the entire data store of a buffer object into the client's address
     * space.
@@ -776,7 +781,11 @@ static inline bool radeon_uses_secure_bos(struct radeon_winsys* ws)
 static inline void
 radeon_bo_reference(struct radeon_winsys *rws, struct pb_buffer **dst, struct pb_buffer *src)
 {
-   pb_reference_with_winsys(rws, dst, src);
+   struct pb_buffer *old = *dst;
+
+   if (pipe_reference(&(*dst)->reference, &src->reference))
+      rws->buffer_destroy(rws, old);
+   *dst = src;
 }
 
 /* The following bits describe the heaps managed by slab allocators (pb_slab) and
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index b138361a3bb0f..86d57cf2eaa50 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -421,12 +421,6 @@ void amdgpu_bo_unmap(struct radeon_winsys *rws, struct pb_buffer *buf)
    amdgpu_bo_cpu_unmap(real->bo);
 }
 
-static const struct pb_vtbl amdgpu_winsys_bo_vtbl = {
-   /* Cast to void* because one of the function parameters is a struct pointer instead of void*. */
-   (void*)amdgpu_bo_destroy_or_cache
-   /* other functions are never called */
-};
-
 static void amdgpu_add_buffer_to_global_list(struct amdgpu_winsys *ws, struct amdgpu_bo_real *bo)
 {
 #if DEBUG
@@ -583,7 +577,6 @@ static struct amdgpu_winsys_bo *amdgpu_create_bo(struct amdgpu_winsys *ws,
    bo->b.base.alignment_log2 = util_logbase2(alignment);
    bo->b.base.usage = flags;
    bo->b.base.size = size;
-   bo->b.base.vtbl = &amdgpu_winsys_bo_vtbl;
    bo->b.va = va;
    bo->b.unique_id = __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
    bo->bo = buf_handle;
@@ -661,12 +654,6 @@ static void amdgpu_bo_slab_destroy(struct radeon_winsys *rws, struct pb_buffer *
    pb_slab_free(slabs, &bo->entry);
 }
 
-static const struct pb_vtbl amdgpu_winsys_bo_slab_vtbl = {
-   /* Cast to void* because one of the function parameters is a struct pointer instead of void*. */
-   (void*)amdgpu_bo_slab_destroy
-   /* other functions are never called */
-};
-
 /* Return the power of two size of a slab entry matching the input size. */
 static unsigned get_slab_pot_entry_size(struct amdgpu_winsys *ws, unsigned size)
 {
@@ -760,7 +747,6 @@ struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_s
       bo->b.base.placement = domains;
       bo->b.base.alignment_log2 = util_logbase2(get_slab_entry_alignment(ws, entry_size));
       bo->b.base.size = entry_size;
-      bo->b.base.vtbl = &amdgpu_winsys_bo_slab_vtbl;
       bo->b.type = AMDGPU_BO_SLAB_ENTRY;
       bo->b.va = slab->buffer->va + i * entry_size;
       bo->b.unique_id = base_id + i;
@@ -1069,12 +1055,6 @@ static void amdgpu_bo_sparse_destroy(struct radeon_winsys *rws, struct pb_buffer
    FREE(bo);
 }
 
-static const struct pb_vtbl amdgpu_winsys_bo_sparse_vtbl = {
-   /* Cast to void* because one of the function parameters is a struct pointer instead of void*. */
-   (void*)amdgpu_bo_sparse_destroy
-   /* other functions are never called */
-};
-
 static struct pb_buffer *
 amdgpu_bo_sparse_create(struct amdgpu_winsys *ws, uint64_t size,
                         enum radeon_bo_domain domain,
@@ -1102,7 +1082,6 @@ amdgpu_bo_sparse_create(struct amdgpu_winsys *ws, uint64_t size,
    bo->b.base.alignment_log2 = util_logbase2(RADEON_SPARSE_PAGE_SIZE);
    bo->b.base.usage = flags;
    bo->b.base.size = size;
-   bo->b.base.vtbl = &amdgpu_winsys_bo_sparse_vtbl;
    bo->b.unique_id =  __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
    bo->b.type = AMDGPU_BO_SPARSE;
 
@@ -1581,7 +1560,6 @@ static struct pb_buffer *amdgpu_bo_from_handle(struct radeon_winsys *rws,
 				info.phys_alignment : ws->info.gart_page_size);
    bo->b.base.usage = flags;
    bo->b.base.size = result.alloc_size;
-   bo->b.base.vtbl = &amdgpu_winsys_bo_vtbl;
    bo->b.type = AMDGPU_BO_REAL;
    bo->b.va = va;
    bo->b.unique_id = __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
@@ -1736,7 +1714,6 @@ static struct pb_buffer *amdgpu_bo_from_ptr(struct radeon_winsys *rws,
     bo->b.base.placement = RADEON_DOMAIN_GTT;
     bo->b.base.alignment_log2 = 0;
     bo->b.base.size = size;
-    bo->b.base.vtbl = &amdgpu_winsys_bo_vtbl;
     bo->b.type = AMDGPU_BO_REAL;
     bo->b.va = va;
     bo->b.unique_id = __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
@@ -1783,6 +1760,18 @@ static uint64_t amdgpu_bo_get_va(struct pb_buffer *buf)
    return ((struct amdgpu_winsys_bo*)buf)->va;
 }
 
+static void amdgpu_buffer_destroy(struct radeon_winsys *ws, struct pb_buffer *buf)
+{
+   struct amdgpu_winsys_bo *bo = amdgpu_winsys_bo(buf);
+
+   if (bo->type == AMDGPU_BO_SLAB_ENTRY)
+      amdgpu_bo_slab_destroy(ws, buf);
+   else if (bo->type == AMDGPU_BO_SPARSE)
+      amdgpu_bo_sparse_destroy(ws, buf);
+   else
+      amdgpu_bo_destroy_or_cache(ws, buf);
+}
+
 void amdgpu_bo_init_functions(struct amdgpu_screen_winsys *ws)
 {
    ws->base.buffer_set_metadata = amdgpu_buffer_set_metadata;
@@ -1791,6 +1780,7 @@ void amdgpu_bo_init_functions(struct amdgpu_screen_winsys *ws)
    ws->base.buffer_unmap = amdgpu_bo_unmap;
    ws->base.buffer_wait = amdgpu_bo_wait;
    ws->base.buffer_create = amdgpu_buffer_create;
+   ws->base.buffer_destroy = amdgpu_buffer_destroy;
    ws->base.buffer_from_handle = amdgpu_bo_from_handle;
    ws->base.buffer_from_ptr = amdgpu_bo_from_ptr;
    ws->base.buffer_is_user_ptr = amdgpu_bo_is_user_ptr;
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c b/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c
index f271fbcc74f2c..abf4c46407176 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c
@@ -438,7 +438,10 @@ amdgpu_winsys_create(int fd, const struct pipe_screen_config *config,
       }
       aws->info.drm_major = drm_major;
       aws->info.drm_minor = drm_minor;
-      aws->dummy_ws.aws = aws; /* only the pointer is used */
+
+      /* Only aws and buffer functions are used. */
+      aws->dummy_ws.aws = aws;
+      amdgpu_bo_init_functions(&aws->dummy_ws);
 
       if (!do_winsys_init(aws, config, fd))
          goto fail_alloc;
diff --git a/src/gallium/winsys/radeon/drm/radeon_drm_bo.c b/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
index 172062f6c4c91..28a12b6132de3 100644
--- a/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
+++ b/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
@@ -1070,6 +1070,11 @@ radeon_winsys_bo_create(struct radeon_winsys *rws,
    return &bo->base;
 }
 
+static void radeon_winsys_bo_destroy(struct radeon_winsys *ws, struct pb_buffer *buf)
+{
+   buf->vtbl->destroy(ws, buf);
+}
+
 static struct pb_buffer *radeon_winsys_bo_from_ptr(struct radeon_winsys *rws,
                                                    void *pointer, uint64_t size,
                                                    enum radeon_bo_flag flags)
@@ -1375,6 +1380,7 @@ void radeon_drm_bo_init_functions(struct radeon_drm_winsys *ws)
    ws->base.buffer_unmap = radeon_bo_unmap;
    ws->base.buffer_wait = radeon_bo_wait;
    ws->base.buffer_create = radeon_winsys_bo_create;
+   ws->base.buffer_destroy = radeon_winsys_bo_destroy;
    ws->base.buffer_from_handle = radeon_winsys_bo_from_handle;
    ws->base.buffer_from_ptr = radeon_winsys_bo_from_ptr;
    ws->base.buffer_is_user_ptr = radeon_winsys_bo_is_user_ptr;
-- 
GitLab


From 48e186517a9190be8ecc12a36824432d18f59ff8 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 9 Dec 2023 15:39:15 -0500
Subject: [PATCH 06/36] gallium/pb_cache: remove pb_cache_entry::end to save
 space

just compute it at each use

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/auxiliary/pipebuffer/pb_cache.c | 7 ++++---
 src/gallium/auxiliary/pipebuffer/pb_cache.h | 2 +-
 2 files changed, 5 insertions(+), 4 deletions(-)

diff --git a/src/gallium/auxiliary/pipebuffer/pb_cache.c b/src/gallium/auxiliary/pipebuffer/pb_cache.c
index 201655b8149b7..6e4dba0a57542 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_cache.c
+++ b/src/gallium/auxiliary/pipebuffer/pb_cache.c
@@ -65,7 +65,8 @@ release_expired_buffers_locked(struct list_head *cache,
    while (curr != cache) {
       entry = list_entry(curr, struct pb_cache_entry, head);
 
-      if (!os_time_timeout(entry->start, entry->end, current_time))
+      if (!os_time_timeout(entry->start, entry->start + entry->mgr->usecs,
+                           current_time))
          break;
 
       destroy_buffer_locked(entry);
@@ -103,7 +104,6 @@ pb_cache_add_buffer(struct pb_cache_entry *entry)
    }
 
    entry->start = os_time_get();
-   entry->end = entry->start + mgr->usecs;
    list_addtail(&entry->head, cache);
    ++mgr->num_buffers;
    mgr->cache_size += buf->size;
@@ -171,7 +171,8 @@ pb_cache_reclaim_buffer(struct pb_cache *mgr, pb_size size,
       if (!entry && (ret = pb_cache_is_buffer_compat(cur_entry, size,
                                                      alignment, usage)) > 0)
          entry = cur_entry;
-      else if (os_time_timeout(cur_entry->start, cur_entry->end, now))
+      else if (os_time_timeout(cur_entry->start,
+                               cur_entry->start + mgr->usecs, now))
          destroy_buffer_locked(cur_entry);
       else
          /* This buffer (and all hereafter) are still hot in cache */
diff --git a/src/gallium/auxiliary/pipebuffer/pb_cache.h b/src/gallium/auxiliary/pipebuffer/pb_cache.h
index 4e80669425c6a..9e4cabfd8a85f 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_cache.h
+++ b/src/gallium/auxiliary/pipebuffer/pb_cache.h
@@ -42,7 +42,7 @@ struct pb_cache_entry
    struct list_head head;
    struct pb_buffer *buffer; /**< Pointer to the structure this is part of. */
    struct pb_cache *mgr;
-   int64_t start, end; /**< Caching time interval */
+   int64_t start; /**< Cached start time */
    unsigned bucket_index;
 };
 
-- 
GitLab


From 212754ae30fd56dcd67c270e94b35b0449bc12dd Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 9 Dec 2023 15:47:15 -0500
Subject: [PATCH 07/36] gallium/pb_cache: switch time variables to milliseconds
 and 32-bit type

to decrease pb_cache_entry by 8 bytes.

Add msecs_base_time to offset time == 0 to the creation of pb_cache.

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/auxiliary/pipebuffer/pb_cache.c | 43 +++++++++++++++------
 src/gallium/auxiliary/pipebuffer/pb_cache.h |  5 ++-
 2 files changed, 35 insertions(+), 13 deletions(-)

diff --git a/src/gallium/auxiliary/pipebuffer/pb_cache.c b/src/gallium/auxiliary/pipebuffer/pb_cache.c
index 6e4dba0a57542..44af0f630971f 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_cache.c
+++ b/src/gallium/auxiliary/pipebuffer/pb_cache.c
@@ -30,6 +30,28 @@
 #include "util/u_memory.h"
 #include "util/os_time.h"
 
+/*
+ * Helper function for detecting time outs, taking in account overflow.
+ *
+ * Returns true if the current time has elapsed beyond the specified interval.
+ */
+static inline bool
+time_timeout_ms(unsigned start, unsigned interval, unsigned curr)
+{
+   unsigned end = start + interval;
+
+   if (start <= end)
+      return !(start <= curr && curr < end);
+   else
+      return !((start <= curr) || (curr < end));
+}
+
+static unsigned
+time_get_ms(struct pb_cache *mgr)
+{
+   /* Return the time relative to msecs_base_time. */
+   return os_time_get() / 1000 - mgr->msecs_base_time;
+}
 
 /**
  * Actually destroy the buffer.
@@ -55,7 +77,7 @@ destroy_buffer_locked(struct pb_cache_entry *entry)
  */
 static void
 release_expired_buffers_locked(struct list_head *cache,
-                               int64_t current_time)
+                               unsigned current_time_ms)
 {
    struct list_head *curr, *next;
    struct pb_cache_entry *entry;
@@ -65,8 +87,8 @@ release_expired_buffers_locked(struct list_head *cache,
    while (curr != cache) {
       entry = list_entry(curr, struct pb_cache_entry, head);
 
-      if (!os_time_timeout(entry->start, entry->start + entry->mgr->usecs,
-                           current_time))
+      if (!time_timeout_ms(entry->start_ms, entry->mgr->msecs,
+                           current_time_ms))
          break;
 
       destroy_buffer_locked(entry);
@@ -91,10 +113,10 @@ pb_cache_add_buffer(struct pb_cache_entry *entry)
    simple_mtx_lock(&mgr->mutex);
    assert(!pipe_is_referenced(&buf->reference));
 
-   int64_t current_time = os_time_get();
+   unsigned current_time_ms = time_get_ms(mgr);
 
    for (i = 0; i < mgr->num_heaps; i++)
-      release_expired_buffers_locked(&mgr->buckets[i], current_time);
+      release_expired_buffers_locked(&mgr->buckets[i], current_time_ms);
 
    /* Directly release any buffer that exceeds the limit. */
    if (mgr->cache_size + buf->size > mgr->max_cache_size) {
@@ -103,7 +125,7 @@ pb_cache_add_buffer(struct pb_cache_entry *entry)
       return;
    }
 
-   entry->start = os_time_get();
+   entry->start_ms = time_get_ms(mgr);
    list_addtail(&entry->head, cache);
    ++mgr->num_buffers;
    mgr->cache_size += buf->size;
@@ -151,7 +173,6 @@ pb_cache_reclaim_buffer(struct pb_cache *mgr, pb_size size,
    struct pb_cache_entry *entry;
    struct pb_cache_entry *cur_entry;
    struct list_head *cur, *next;
-   int64_t now;
    int ret = 0;
 
    assert(bucket_index < mgr->num_heaps);
@@ -164,15 +185,14 @@ pb_cache_reclaim_buffer(struct pb_cache *mgr, pb_size size,
    next = cur->next;
 
    /* search in the expired buffers, freeing them in the process */
-   now = os_time_get();
+   unsigned now = time_get_ms(mgr);
    while (cur != cache) {
       cur_entry = list_entry(cur, struct pb_cache_entry, head);
 
       if (!entry && (ret = pb_cache_is_buffer_compat(cur_entry, size,
                                                      alignment, usage)) > 0)
          entry = cur_entry;
-      else if (os_time_timeout(cur_entry->start,
-                               cur_entry->start + mgr->usecs, now))
+      else if (time_timeout_ms(cur_entry->start_ms, mgr->msecs, now))
          destroy_buffer_locked(cur_entry);
       else
          /* This buffer (and all hereafter) are still hot in cache */
@@ -302,7 +322,8 @@ pb_cache_init(struct pb_cache *mgr, unsigned num_heaps,
    mgr->cache_size = 0;
    mgr->max_cache_size = maximum_cache_size;
    mgr->num_heaps = num_heaps;
-   mgr->usecs = usecs;
+   mgr->msecs = usecs / 1000;
+   mgr->msecs_base_time = os_time_get() / 1000;
    mgr->num_buffers = 0;
    mgr->bypass_usage = bypass_usage;
    mgr->size_factor = size_factor;
diff --git a/src/gallium/auxiliary/pipebuffer/pb_cache.h b/src/gallium/auxiliary/pipebuffer/pb_cache.h
index 9e4cabfd8a85f..cf61873b72d62 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_cache.h
+++ b/src/gallium/auxiliary/pipebuffer/pb_cache.h
@@ -42,7 +42,7 @@ struct pb_cache_entry
    struct list_head head;
    struct pb_buffer *buffer; /**< Pointer to the structure this is part of. */
    struct pb_cache *mgr;
-   int64_t start; /**< Cached start time */
+   unsigned start_ms; /**< Cached start time */
    unsigned bucket_index;
 };
 
@@ -58,7 +58,8 @@ struct pb_cache
    uint64_t cache_size;
    uint64_t max_cache_size;
    unsigned num_heaps;
-   unsigned usecs;
+   unsigned msecs;
+   int64_t msecs_base_time;
    unsigned num_buffers;
    unsigned bypass_usage;
    float size_factor;
-- 
GitLab


From 687a6f4fb86dc3cf74d73a24383f366dc87eba4f Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 9 Dec 2023 16:25:20 -0500
Subject: [PATCH 08/36] radeon_winsys: add struct radeon_winsys* parameter into
 fence_reference

Since the radeon winsys implements fences as buffers, we need radeon_winsys*
to destroy them. This will enable the removal of pb_cache_entry::mgr.

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/drivers/r300/r300_flush.c         |  2 +-
 src/gallium/drivers/r300/r300_screen.c        |  2 +-
 src/gallium/drivers/r600/r600_hw_context.c    |  2 +-
 src/gallium/drivers/r600/r600_pipe_common.c   | 18 +++++++++---------
 src/gallium/drivers/radeonsi/radeon_uvd.c     |  2 +-
 src/gallium/drivers/radeonsi/radeon_uvd_enc.c |  2 +-
 src/gallium/drivers/radeonsi/radeon_vce.c     |  2 +-
 src/gallium/drivers/radeonsi/radeon_vcn_dec.c |  8 ++++----
 src/gallium/drivers/radeonsi/radeon_vcn_enc.c |  2 +-
 src/gallium/drivers/radeonsi/si_fence.c       |  8 ++++----
 src/gallium/drivers/radeonsi/si_gfx_cs.c      |  2 +-
 src/gallium/drivers/radeonsi/si_pipe.c        |  2 +-
 src/gallium/include/winsys/radeon_winsys.h    |  3 ++-
 src/gallium/winsys/amdgpu/drm/amdgpu_cs.c     |  9 ++++++++-
 src/gallium/winsys/radeon/drm/radeon_drm_cs.c | 18 ++++++++++--------
 15 files changed, 46 insertions(+), 36 deletions(-)

diff --git a/src/gallium/drivers/r300/r300_flush.c b/src/gallium/drivers/r300/r300_flush.c
index 2c813e5fad0ab..45045cea3a5fd 100644
--- a/src/gallium/drivers/r300/r300_flush.c
+++ b/src/gallium/drivers/r300/r300_flush.c
@@ -112,7 +112,7 @@ void r300_flush(struct pipe_context *pipe,
                 }
 
                 if (fence && *fence)
-                    r300->rws->fence_reference(fence, NULL);
+                    r300->rws->fence_reference(r300->rws, fence, NULL);
                 r300_flush_and_cleanup(r300, flags, fence);
             }
 
diff --git a/src/gallium/drivers/r300/r300_screen.c b/src/gallium/drivers/r300/r300_screen.c
index 776a5aca4108e..2a0bb599f55e3 100644
--- a/src/gallium/drivers/r300/r300_screen.c
+++ b/src/gallium/drivers/r300/r300_screen.c
@@ -796,7 +796,7 @@ static void r300_fence_reference(struct pipe_screen *screen,
 {
     struct radeon_winsys *rws = r300_screen(screen)->rws;
 
-    rws->fence_reference(ptr, fence);
+    rws->fence_reference(rws, ptr, fence);
 }
 
 static bool r300_fence_finish(struct pipe_screen *screen,
diff --git a/src/gallium/drivers/r600/r600_hw_context.c b/src/gallium/drivers/r600/r600_hw_context.c
index d1db316964dc8..7178a1369d6c6 100644
--- a/src/gallium/drivers/r600/r600_hw_context.c
+++ b/src/gallium/drivers/r600/r600_hw_context.c
@@ -299,7 +299,7 @@ void r600_context_gfx_flush(void *context, unsigned flags,
 	/* Flush the CS. */
 	ws->cs_flush(cs, flags, &ctx->b.last_gfx_fence);
 	if (fence)
-		ws->fence_reference(fence, ctx->b.last_gfx_fence);
+		ws->fence_reference(ws, fence, ctx->b.last_gfx_fence);
 	ctx->b.num_gfx_cs_flushes++;
 
 	if (ctx->is_debug) {
diff --git a/src/gallium/drivers/r600/r600_pipe_common.c b/src/gallium/drivers/r600/r600_pipe_common.c
index 783d7badc3bf2..6664c284c3e8f 100644
--- a/src/gallium/drivers/r600/r600_pipe_common.c
+++ b/src/gallium/drivers/r600/r600_pipe_common.c
@@ -352,7 +352,7 @@ static void r600_flush_from_st(struct pipe_context *ctx,
 
 	if (!radeon_emitted(&rctx->gfx.cs, rctx->initial_gfx_cs_size)) {
 		if (fence)
-			ws->fence_reference(&gfx_fence, rctx->last_gfx_fence);
+			ws->fence_reference(ws, &gfx_fence, rctx->last_gfx_fence);
 		if (!(flags & PIPE_FLUSH_DEFERRED))
 			ws->cs_sync_flush(&rctx->gfx.cs);
 	} else {
@@ -374,8 +374,8 @@ static void r600_flush_from_st(struct pipe_context *ctx,
 		struct r600_multi_fence *multi_fence =
 			CALLOC_STRUCT(r600_multi_fence);
 		if (!multi_fence) {
-			ws->fence_reference(&sdma_fence, NULL);
-			ws->fence_reference(&gfx_fence, NULL);
+			ws->fence_reference(ws, &sdma_fence, NULL);
+			ws->fence_reference(ws, &gfx_fence, NULL);
 			goto finish;
 		}
 
@@ -412,7 +412,7 @@ static void r600_flush_dma_ring(void *ctx, unsigned flags,
 
 	if (!radeon_emitted(cs, 0)) {
 		if (fence)
-			rctx->ws->fence_reference(fence, rctx->last_sdma_fence);
+			rctx->ws->fence_reference(rctx->ws, fence, rctx->last_sdma_fence);
 		return;
 	}
 
@@ -421,7 +421,7 @@ static void r600_flush_dma_ring(void *ctx, unsigned flags,
 
 	rctx->ws->cs_flush(cs, flags, &rctx->last_sdma_fence);
 	if (fence)
-		rctx->ws->fence_reference(fence, rctx->last_sdma_fence);
+		rctx->ws->fence_reference(rctx->ws, fence, rctx->last_sdma_fence);
 
 	if (check_vm) {
 		/* Use conservative timeout 800ms, after which we won't wait any
@@ -666,8 +666,8 @@ void r600_common_context_cleanup(struct r600_common_context *rctx)
 	slab_destroy_child(&rctx->pool_transfers_unsync);
 
 	u_suballocator_destroy(&rctx->allocator_zeroed_memory);
-	rctx->ws->fence_reference(&rctx->last_gfx_fence, NULL);
-	rctx->ws->fence_reference(&rctx->last_sdma_fence, NULL);
+	rctx->ws->fence_reference(rctx->ws, &rctx->last_gfx_fence, NULL);
+	rctx->ws->fence_reference(rctx->ws, &rctx->last_sdma_fence, NULL);
 	r600_resource_reference(&rctx->eop_bug_scratch, NULL);
 }
 
@@ -1072,8 +1072,8 @@ static void r600_fence_reference(struct pipe_screen *screen,
 	struct r600_multi_fence *rsrc = (struct r600_multi_fence *)src;
 
 	if (pipe_reference(&(*rdst)->reference, &rsrc->reference)) {
-		ws->fence_reference(&(*rdst)->gfx, NULL);
-		ws->fence_reference(&(*rdst)->sdma, NULL);
+		ws->fence_reference(ws, &(*rdst)->gfx, NULL);
+		ws->fence_reference(ws, &(*rdst)->sdma, NULL);
 		FREE(*rdst);
 	}
         *rdst = rsrc;
diff --git a/src/gallium/drivers/radeonsi/radeon_uvd.c b/src/gallium/drivers/radeonsi/radeon_uvd.c
index d51b1b6c54565..5cccfb28fe998 100644
--- a/src/gallium/drivers/radeonsi/radeon_uvd.c
+++ b/src/gallium/drivers/radeonsi/radeon_uvd.c
@@ -92,7 +92,7 @@ static void ruvd_dec_destroy_fence(struct pipe_video_codec *decoder,
 {
    struct ruvd_decoder *dec = (struct ruvd_decoder *)decoder;
 
-   dec->ws->fence_reference(&fence, NULL);
+   dec->ws->fence_reference(dec->ws, &fence, NULL);
 }
 
 /* add a new set register command to the IB */
diff --git a/src/gallium/drivers/radeonsi/radeon_uvd_enc.c b/src/gallium/drivers/radeonsi/radeon_uvd_enc.c
index 9c287a0c4ce16..c6daee96366ad 100644
--- a/src/gallium/drivers/radeonsi/radeon_uvd_enc.c
+++ b/src/gallium/drivers/radeonsi/radeon_uvd_enc.c
@@ -282,7 +282,7 @@ static void radeon_uvd_enc_destroy_fence(struct pipe_video_codec *encoder,
 {
    struct radeon_uvd_encoder *enc = (struct radeon_uvd_encoder *)encoder;
 
-   enc->ws->fence_reference(&fence, NULL);
+   enc->ws->fence_reference(enc->ws, &fence, NULL);
 }
 
 struct pipe_video_codec *radeon_uvd_create_encoder(struct pipe_context *context,
diff --git a/src/gallium/drivers/radeonsi/radeon_vce.c b/src/gallium/drivers/radeonsi/radeon_vce.c
index 1b5178c8a83cb..9a35e1d39afa3 100644
--- a/src/gallium/drivers/radeonsi/radeon_vce.c
+++ b/src/gallium/drivers/radeonsi/radeon_vce.c
@@ -350,7 +350,7 @@ static void rvce_destroy_fence(struct pipe_video_codec *encoder,
 {
    struct rvce_encoder *enc = (struct rvce_encoder *)encoder;
 
-   enc->ws->fence_reference(&fence, NULL);
+   enc->ws->fence_reference(enc->ws, &fence, NULL);
 }
 
 /**
diff --git a/src/gallium/drivers/radeonsi/radeon_vcn_dec.c b/src/gallium/drivers/radeonsi/radeon_vcn_dec.c
index b873958609ebc..6e42ff16e75a5 100644
--- a/src/gallium/drivers/radeonsi/radeon_vcn_dec.c
+++ b/src/gallium/drivers/radeonsi/radeon_vcn_dec.c
@@ -2781,10 +2781,10 @@ static void radeon_dec_destroy(struct pipe_video_codec *decoder)
       send_msg_buf(dec);
       flush(dec, 0, &dec->destroy_fence);
       dec->ws->fence_wait(dec->ws, dec->destroy_fence, PIPE_DEFAULT_DECODER_FEEDBACK_TIMEOUT_NS);
-      dec->ws->fence_reference(&dec->destroy_fence, NULL);
+      dec->ws->fence_reference(dec->ws, &dec->destroy_fence, NULL);
    }
 
-   dec->ws->fence_reference(&dec->prev_fence, NULL);
+   dec->ws->fence_reference(dec->ws, &dec->prev_fence, NULL);
    dec->ws->cs_destroy(&dec->cs);
 
    if (dec->stream_type == RDECODE_CODEC_JPEG) {
@@ -2964,7 +2964,7 @@ static void radeon_dec_end_frame(struct pipe_video_codec *decoder, struct pipe_v
    dec->send_cmd(dec, target, picture);
    flush(dec, PIPE_FLUSH_ASYNC, picture->fence);
    if (picture->fence)
-      dec->ws->fence_reference(&dec->prev_fence, *picture->fence);
+      dec->ws->fence_reference(dec->ws, &dec->prev_fence, *picture->fence);
    next_buffer(dec);
 }
 
@@ -3017,7 +3017,7 @@ static void radeon_dec_destroy_fence(struct pipe_video_codec *decoder,
 {
    struct radeon_decoder *dec = (struct radeon_decoder *)decoder;
 
-   dec->ws->fence_reference(&fence, NULL);
+   dec->ws->fence_reference(dec->ws, &fence, NULL);
 }
 
 /**
diff --git a/src/gallium/drivers/radeonsi/radeon_vcn_enc.c b/src/gallium/drivers/radeonsi/radeon_vcn_enc.c
index 52d3e9d65d185..78c415f26c493 100644
--- a/src/gallium/drivers/radeonsi/radeon_vcn_enc.c
+++ b/src/gallium/drivers/radeonsi/radeon_vcn_enc.c
@@ -1229,7 +1229,7 @@ static void radeon_enc_destroy_fence(struct pipe_video_codec *encoder,
 {
    struct radeon_encoder *enc = (struct radeon_encoder *)encoder;
 
-   enc->ws->fence_reference(&fence, NULL);
+   enc->ws->fence_reference(enc->ws, &fence, NULL);
 }
 
 struct pipe_video_codec *radeon_create_encoder(struct pipe_context *context,
diff --git a/src/gallium/drivers/radeonsi/si_fence.c b/src/gallium/drivers/radeonsi/si_fence.c
index 5f55f94d43684..3913737e3130a 100644
--- a/src/gallium/drivers/radeonsi/si_fence.c
+++ b/src/gallium/drivers/radeonsi/si_fence.c
@@ -186,7 +186,7 @@ static void si_fence_reference(struct pipe_screen *screen, struct pipe_fence_han
    struct si_fence *ssrc = (struct si_fence *)src;
 
    if (pipe_reference(&(*sdst)->reference, &ssrc->reference)) {
-      ws->fence_reference(&(*sdst)->gfx, NULL);
+      ws->fence_reference(ws, &(*sdst)->gfx, NULL);
       tc_unflushed_batch_token_reference(&(*sdst)->tc_token, NULL);
       si_resource_reference(&(*sdst)->fine.buf, NULL);
       FREE(*sdst);
@@ -304,7 +304,7 @@ static bool si_fence_finish(struct pipe_screen *screen, struct pipe_context *ctx
       return true;
 
    if (sfence->fine.buf && si_fine_fence_signaled(rws, &sfence->fine)) {
-      rws->fence_reference(&sfence->gfx, NULL);
+      rws->fence_reference(rws, &sfence->gfx, NULL);
       si_resource_reference(&sfence->fine.buf, NULL);
       return true;
    }
@@ -464,7 +464,7 @@ static void si_flush_all_queues(struct pipe_context *ctx,
 
    if (!radeon_emitted(&sctx->gfx_cs, sctx->initial_gfx_cs_size)) {
       if (fence)
-         ws->fence_reference(&gfx_fence, sctx->last_gfx_fence);
+         ws->fence_reference(ws, &gfx_fence, sctx->last_gfx_fence);
       if (!(flags & PIPE_FLUSH_DEFERRED))
          ws->cs_sync_flush(&sctx->gfx_cs);
 
@@ -502,7 +502,7 @@ static void si_flush_all_queues(struct pipe_context *ctx,
       } else {
          new_fence = si_alloc_fence();
          if (!new_fence) {
-            ws->fence_reference(&gfx_fence, NULL);
+            ws->fence_reference(ws, &gfx_fence, NULL);
             goto finish;
          }
 
diff --git a/src/gallium/drivers/radeonsi/si_gfx_cs.c b/src/gallium/drivers/radeonsi/si_gfx_cs.c
index d1d477a625c5c..40276919c983a 100644
--- a/src/gallium/drivers/radeonsi/si_gfx_cs.c
+++ b/src/gallium/drivers/radeonsi/si_gfx_cs.c
@@ -141,7 +141,7 @@ void si_flush_gfx_cs(struct si_context *ctx, unsigned flags, struct pipe_fence_h
 
    tc_driver_internal_flush_notify(ctx->tc);
    if (fence)
-      ws->fence_reference(fence, ctx->last_gfx_fence);
+      ws->fence_reference(ws, fence, ctx->last_gfx_fence);
 
    ctx->num_gfx_cs_flushes++;
 
diff --git a/src/gallium/drivers/radeonsi/si_pipe.c b/src/gallium/drivers/radeonsi/si_pipe.c
index c00fb1e923bfc..77a4a8803ad07 100644
--- a/src/gallium/drivers/radeonsi/si_pipe.c
+++ b/src/gallium/drivers/radeonsi/si_pipe.c
@@ -351,7 +351,7 @@ static void si_destroy_context(struct pipe_context *context)
 
    u_suballocator_destroy(&sctx->allocator_zeroed_memory);
 
-   sctx->ws->fence_reference(&sctx->last_gfx_fence, NULL);
+   sctx->ws->fence_reference(sctx->ws, &sctx->last_gfx_fence, NULL);
    si_resource_reference(&sctx->eop_bug_scratch, NULL);
    si_resource_reference(&sctx->eop_bug_scratch_tmz, NULL);
    si_resource_reference(&sctx->shadowing.registers, NULL);
diff --git a/src/gallium/include/winsys/radeon_winsys.h b/src/gallium/include/winsys/radeon_winsys.h
index b79f8861d5fe5..9ac02e058e9ea 100644
--- a/src/gallium/include/winsys/radeon_winsys.h
+++ b/src/gallium/include/winsys/radeon_winsys.h
@@ -697,7 +697,8 @@ struct radeon_winsys {
    /**
     * Reference counting for fences.
     */
-   void (*fence_reference)(struct pipe_fence_handle **dst, struct pipe_fence_handle *src);
+   void (*fence_reference)(struct radeon_winsys *ws, struct pipe_fence_handle **dst,
+                           struct pipe_fence_handle *src);
 
    /**
     * Create a new fence object corresponding to the given syncobj fd.
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
index 8a6851f882e58..819c65d6c3053 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
@@ -1803,6 +1803,13 @@ static void amdgpu_cs_set_mcbp_reg_shadowing_va(struct radeon_cmdbuf *rcs,uint64
    cs->mcbp_fw_shadow_chunk.flags = AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW;
 }
 
+static void amdgpu_winsys_fence_reference(struct radeon_winsys *rws,
+                                          struct pipe_fence_handle **dst,
+                                          struct pipe_fence_handle *src)
+{
+   amdgpu_fence_reference(dst, src);
+}
+
 void amdgpu_cs_init_functions(struct amdgpu_screen_winsys *ws)
 {
    ws->base.ctx_create = amdgpu_ctx_create;
@@ -1824,7 +1831,7 @@ void amdgpu_cs_init_functions(struct amdgpu_screen_winsys *ws)
    ws->base.cs_add_syncobj_signal = amdgpu_cs_add_syncobj_signal;
    ws->base.cs_get_ip_type = amdgpu_cs_get_ip_type;
    ws->base.fence_wait = amdgpu_fence_wait_rel_timeout;
-   ws->base.fence_reference = amdgpu_fence_reference;
+   ws->base.fence_reference = amdgpu_winsys_fence_reference;
    ws->base.fence_import_syncobj = amdgpu_fence_import_syncobj;
    ws->base.fence_import_sync_file = amdgpu_fence_import_sync_file;
    ws->base.fence_export_sync_file = amdgpu_fence_export_sync_file;
diff --git a/src/gallium/winsys/radeon/drm/radeon_drm_cs.c b/src/gallium/winsys/radeon/drm/radeon_drm_cs.c
index 2193be10281c3..75fd32a3aa254 100644
--- a/src/gallium/winsys/radeon/drm/radeon_drm_cs.c
+++ b/src/gallium/winsys/radeon/drm/radeon_drm_cs.c
@@ -47,7 +47,8 @@
 #define RELOC_DWORDS (sizeof(struct drm_radeon_cs_reloc) / sizeof(uint32_t))
 
 static struct pipe_fence_handle *radeon_cs_create_fence(struct radeon_cmdbuf *rcs);
-static void radeon_fence_reference(struct pipe_fence_handle **dst,
+static void radeon_fence_reference(struct radeon_winsys *ws,
+                                   struct pipe_fence_handle **dst,
                                    struct pipe_fence_handle *src);
 
 static struct radeon_winsys_ctx *radeon_drm_ctx_create(struct radeon_winsys *ws,
@@ -636,7 +637,7 @@ static int radeon_drm_cs_flush(struct radeon_cmdbuf *rcs,
 
       if (fence) {
          if (pfence)
-            radeon_fence_reference(pfence, fence);
+            radeon_fence_reference(&cs->ws->base, pfence, fence);
 
          mtx_lock(&cs->ws->bo_fence_lock);
          for (unsigned i = 0; i < cs->csc->num_slab_buffers; ++i) {
@@ -646,10 +647,10 @@ static int radeon_drm_cs_flush(struct radeon_cmdbuf *rcs,
          }
          mtx_unlock(&cs->ws->bo_fence_lock);
 
-         radeon_fence_reference(&fence, NULL);
+         radeon_fence_reference(&cs->ws->base, &fence, NULL);
       }
    } else {
-      radeon_fence_reference(&cs->next_fence, NULL);
+      radeon_fence_reference(&cs->ws->base, &cs->next_fence, NULL);
    }
 
    radeon_drm_cs_sync_flush(rcs);
@@ -756,7 +757,7 @@ static void radeon_drm_cs_destroy(struct radeon_cmdbuf *rcs)
    p_atomic_dec(&cs->ws->num_cs);
    radeon_destroy_cs_context(&cs->csc1);
    radeon_destroy_cs_context(&cs->csc2);
-   radeon_fence_reference(&cs->next_fence, NULL);
+   radeon_fence_reference(&cs->ws->base, &cs->next_fence, NULL);
    FREE(cs);
 }
 
@@ -815,7 +816,8 @@ static bool radeon_fence_wait(struct radeon_winsys *ws,
                           RADEON_USAGE_READWRITE);
 }
 
-static void radeon_fence_reference(struct pipe_fence_handle **dst,
+static void radeon_fence_reference(struct radeon_winsys *ws,
+                                   struct pipe_fence_handle **dst,
                                    struct pipe_fence_handle *src)
 {
    pb_reference((struct pb_buffer**)dst, (struct pb_buffer*)src);
@@ -827,7 +829,7 @@ static struct pipe_fence_handle *radeon_drm_cs_get_next_fence(struct radeon_cmdb
    struct pipe_fence_handle *fence = NULL;
 
    if (cs->next_fence) {
-      radeon_fence_reference(&fence, cs->next_fence);
+      radeon_fence_reference(&cs->ws->base, &fence, cs->next_fence);
       return fence;
    }
 
@@ -835,7 +837,7 @@ static struct pipe_fence_handle *radeon_drm_cs_get_next_fence(struct radeon_cmdb
    if (!fence)
       return NULL;
 
-   radeon_fence_reference(&cs->next_fence, fence);
+   radeon_fence_reference(&cs->ws->base, &cs->next_fence, fence);
    return fence;
 }
 
-- 
GitLab


From f0ebd137a37a2e700dfe53e78abaab5c24489d4c Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 9 Dec 2023 16:10:21 -0500
Subject: [PATCH 09/36] r300,r600,radeon/winsys: always pass the winsys to
 radeon_bo_reference

This will allow the removal of pb_cache_entry::mgr.

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/drivers/r300/r300_context.c       |  2 +-
 src/gallium/drivers/r300/r300_query.c         |  5 +-
 src/gallium/drivers/r300/r300_render.c        |  2 +-
 src/gallium/drivers/r300/r300_screen_buffer.c |  9 +--
 src/gallium/drivers/r300/r300_texture.c       |  2 +-
 src/gallium/drivers/r600/r600_buffer_common.c |  7 ++-
 src/gallium/drivers/r600/r600_texture.c       | 10 ++--
 src/gallium/drivers/r600/radeon_video.c       |  4 +-
 src/gallium/winsys/radeon/drm/radeon_drm_bo.c | 31 +++++-----
 src/gallium/winsys/radeon/drm/radeon_drm_bo.h |  7 ++-
 src/gallium/winsys/radeon/drm/radeon_drm_cs.c | 58 ++++++++++---------
 src/gallium/winsys/radeon/drm/radeon_drm_cs.h |  7 ++-
 12 files changed, 78 insertions(+), 66 deletions(-)

diff --git a/src/gallium/drivers/r300/r300_context.c b/src/gallium/drivers/r300/r300_context.c
index 3df744fada897..edad0071a5a64 100644
--- a/src/gallium/drivers/r300/r300_context.c
+++ b/src/gallium/drivers/r300/r300_context.c
@@ -64,7 +64,7 @@ static void r300_release_referenced_objects(struct r300_context *r300)
 
     /* Manually-created vertex buffers. */
     pipe_vertex_buffer_unreference(&r300->dummy_vb);
-    pb_reference(&r300->vbo, NULL);
+    radeon_bo_reference(r300->rws, &r300->vbo, NULL);
 
     r300->context.delete_depth_stencil_alpha_state(&r300->context,
                                                    r300->dsa_decompress_zmask);
diff --git a/src/gallium/drivers/r300/r300_query.c b/src/gallium/drivers/r300/r300_query.c
index 00514d0e1e877..5ef921889f861 100644
--- a/src/gallium/drivers/r300/r300_query.c
+++ b/src/gallium/drivers/r300/r300_query.c
@@ -73,9 +73,10 @@ static struct pipe_query *r300_create_query(struct pipe_context *pipe,
 static void r300_destroy_query(struct pipe_context* pipe,
                                struct pipe_query* query)
 {
+    struct r300_context *r300 = r300_context(pipe);
     struct r300_query* q = r300_query(query);
 
-    pb_reference(&q->buf, NULL);
+    radeon_bo_reference(r300->rws, &q->buf, NULL);
     FREE(query);
 }
 
@@ -120,7 +121,7 @@ static bool r300_end_query(struct pipe_context* pipe,
     struct r300_query *q = r300_query(query);
 
     if (q->type == PIPE_QUERY_GPU_FINISHED) {
-        pb_reference(&q->buf, NULL);
+        radeon_bo_reference(r300->rws, &q->buf, NULL);
         r300_flush(pipe, PIPE_FLUSH_ASYNC,
                    (struct pipe_fence_handle**)&q->buf);
         return true;
diff --git a/src/gallium/drivers/r300/r300_render.c b/src/gallium/drivers/r300/r300_render.c
index 59dc0bc4c1834..858d1798b480a 100644
--- a/src/gallium/drivers/r300/r300_render.c
+++ b/src/gallium/drivers/r300/r300_render.c
@@ -951,7 +951,7 @@ static bool r300_render_allocate_vertices(struct vbuf_render* render,
     DBG(r300, DBG_DRAW, "r300: render_allocate_vertices (size: %d)\n", size);
 
     if (!r300->vbo || size + r300->draw_vbo_offset > r300->vbo->size) {
-	pb_reference(&r300->vbo, NULL);
+	radeon_bo_reference(r300->rws, &r300->vbo, NULL);
         r300->vbo = NULL;
         r300render->vbo_ptr = NULL;
 
diff --git a/src/gallium/drivers/r300/r300_screen_buffer.c b/src/gallium/drivers/r300/r300_screen_buffer.c
index 2b119a83e7964..884284bd86388 100644
--- a/src/gallium/drivers/r300/r300_screen_buffer.c
+++ b/src/gallium/drivers/r300/r300_screen_buffer.c
@@ -53,17 +53,18 @@ void r300_upload_index_buffer(struct r300_context *r300,
 void r300_resource_destroy(struct pipe_screen *screen,
                            struct pipe_resource *buf)
 {
+   struct r300_screen *rscreen = r300_screen(screen);
+
    if (buf->target == PIPE_BUFFER) {
       struct r300_resource *rbuf = r300_resource(buf);
 
       align_free(rbuf->malloced_buffer);
 
       if (rbuf->buf)
-         pb_reference(&rbuf->buf, NULL);
+         radeon_bo_reference(rscreen->rws, &rbuf->buf, NULL);
 
       FREE(rbuf);
    } else {
-      struct r300_screen *rscreen = r300_screen(screen);
       struct r300_resource* tex = (struct r300_resource*)buf;
 
       if (tex->tex.cmask_dwords) {
@@ -73,7 +74,7 @@ void r300_resource_destroy(struct pipe_screen *screen,
           }
           mtx_unlock(&rscreen->cmask_mutex);
       }
-      pb_reference(&tex->buf, NULL);
+      radeon_bo_reference(rscreen->rws, &tex->buf, NULL);
       FREE(tex);
    }
 }
@@ -122,7 +123,7 @@ r300_buffer_transfer_map( struct pipe_context *context,
                                                RADEON_FLAG_NO_INTERPROCESS_SHARING);
             if (new_buf) {
                 /* Discard the old buffer. */
-                pb_reference(&rbuf->buf, NULL);
+                radeon_bo_reference(r300->rws, &rbuf->buf, NULL);
                 rbuf->buf = new_buf;
 
                 /* We changed the buffer, now we need to bind it where the old one was bound. */
diff --git a/src/gallium/drivers/r300/r300_texture.c b/src/gallium/drivers/r300/r300_texture.c
index 40f63a8468b92..73aa8cf194e0f 100644
--- a/src/gallium/drivers/r300/r300_texture.c
+++ b/src/gallium/drivers/r300/r300_texture.c
@@ -1109,7 +1109,7 @@ r300_texture_create_object(struct r300_screen *rscreen,
 fail:
     FREE(tex);
     if (buffer)
-        pb_reference(&buffer, NULL);
+        radeon_bo_reference(rscreen->rws, &buffer, NULL);
     return NULL;
 }
 
diff --git a/src/gallium/drivers/r600/r600_buffer_common.c b/src/gallium/drivers/r600/r600_buffer_common.c
index 974d44247b8e9..beb6b4a6e382e 100644
--- a/src/gallium/drivers/r600/r600_buffer_common.c
+++ b/src/gallium/drivers/r600/r600_buffer_common.c
@@ -188,7 +188,7 @@ bool r600_alloc_resource(struct r600_common_screen *rscreen,
 	else
 		res->gpu_address = 0;
 
-	pb_reference(&old_buf, NULL);
+	radeon_bo_reference(rscreen->ws, &old_buf, NULL);
 
 	util_range_set_empty(&res->valid_buffer_range);
 
@@ -203,12 +203,13 @@ bool r600_alloc_resource(struct r600_common_screen *rscreen,
 
 void r600_buffer_destroy(struct pipe_screen *screen, struct pipe_resource *buf)
 {
+	struct r600_screen *rscreen = (struct r600_screen*)screen;
 	struct r600_resource *rbuffer = r600_resource(buf);
 
 	threaded_resource_deinit(buf);
 	util_range_destroy(&rbuffer->valid_buffer_range);
 	pipe_resource_reference((struct pipe_resource**)&rbuffer->immed_buffer, NULL);
-	pb_reference(&rbuffer->buf, NULL);
+	radeon_bo_reference(rscreen->b.ws, &rbuffer->buf, NULL);
 	FREE(rbuffer);
 }
 
@@ -251,7 +252,7 @@ void r600_replace_buffer_storage(struct pipe_context *ctx,
 	struct r600_resource *rsrc = r600_resource(src);
 	uint64_t old_gpu_address = rdst->gpu_address;
 
-	pb_reference(&rdst->buf, rsrc->buf);
+	radeon_bo_reference(rctx->ws, &rdst->buf, rsrc->buf);
 	rdst->gpu_address = rsrc->gpu_address;
 	rdst->b.b.bind = rsrc->b.b.bind;
 	rdst->flags = rsrc->flags;
diff --git a/src/gallium/drivers/r600/r600_texture.c b/src/gallium/drivers/r600/r600_texture.c
index 88666c00bdb22..536b50ecc7861 100644
--- a/src/gallium/drivers/r600/r600_texture.c
+++ b/src/gallium/drivers/r600/r600_texture.c
@@ -405,7 +405,7 @@ static void r600_reallocate_texture_inplace(struct r600_common_context *rctx,
 
 	/* Replace the structure fields of rtex. */
 	rtex->resource.b.b.bind = templ.bind;
-	pb_reference(&rtex->resource.buf, new_tex->resource.buf);
+	radeon_bo_reference(rctx->ws, &rtex->resource.buf, new_tex->resource.buf);
 	rtex->resource.gpu_address = new_tex->resource.gpu_address;
 	rtex->resource.vram_usage = new_tex->resource.vram_usage;
 	rtex->resource.gart_usage = new_tex->resource.gart_usage;
@@ -576,6 +576,7 @@ static bool r600_texture_get_handle(struct pipe_screen* screen,
 
 void r600_texture_destroy(struct pipe_screen *screen, struct pipe_resource *ptex)
 {
+	struct r600_common_screen *rscreen = (struct r600_common_screen*)screen;
 	struct r600_texture *rtex = (struct r600_texture*)ptex;
 	struct r600_resource *resource = &rtex->resource;
 
@@ -585,7 +586,7 @@ void r600_texture_destroy(struct pipe_screen *screen, struct pipe_resource *ptex
 	if (rtex->cmask_buffer != &rtex->resource) {
 	    r600_resource_reference(&rtex->cmask_buffer, NULL);
 	}
-	pb_reference(&resource->buf, NULL);
+	radeon_bo_reference(rscreen->ws, &resource->buf, NULL);
 	FREE(rtex);
 }
 
@@ -1802,9 +1803,10 @@ static void
 r600_memobj_destroy(struct pipe_screen *screen,
 		    struct pipe_memory_object *_memobj)
 {
+	struct r600_common_screen *rscreen = (struct r600_common_screen*)screen;
 	struct r600_memory_object *memobj = (struct r600_memory_object *)_memobj;
 
-	pb_reference(&memobj->buf, NULL);
+	radeon_bo_reference(rscreen->ws, &memobj->buf, NULL);
 	free(memobj);
 }
 
@@ -1870,7 +1872,7 @@ r600_texture_from_memobj(struct pipe_screen *screen,
 	/* r600_texture_create_object doesn't increment refcount of
 	 * memobj->buf, so increment it here.
 	 */
-	pb_reference(&buf, memobj->buf);
+	radeon_bo_reference(rscreen->ws, &buf, memobj->buf);
 
 	rtex->resource.b.is_shared = true;
 	rtex->resource.external_usage = PIPE_HANDLE_USAGE_FRAMEBUFFER_WRITE;
diff --git a/src/gallium/drivers/r600/radeon_video.c b/src/gallium/drivers/r600/radeon_video.c
index 16a522f7a1d98..c094fc08f4895 100644
--- a/src/gallium/drivers/r600/radeon_video.c
+++ b/src/gallium/drivers/r600/radeon_video.c
@@ -209,10 +209,10 @@ void rvid_join_surfaces(struct r600_common_context *rctx,
 		if (!buffers[i] || !*buffers[i])
 			continue;
 
-		pb_reference(buffers[i], pb);
+		radeon_bo_reference(rctx->ws, buffers[i], pb);
 	}
 
-	pb_reference(&pb, NULL);
+	radeon_bo_reference(rctx->ws, &pb, NULL);
 }
 
 int rvid_get_video_param(struct pipe_screen *screen,
diff --git a/src/gallium/winsys/radeon/drm/radeon_drm_bo.c b/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
index 28a12b6132de3..034380ac9e7fe 100644
--- a/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
+++ b/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
@@ -48,7 +48,7 @@ static bool radeon_real_bo_is_busy(struct radeon_bo *bo)
                               &args, sizeof(args)) != 0;
 }
 
-static bool radeon_bo_is_busy(struct radeon_bo *bo)
+static bool radeon_bo_is_busy(struct radeon_winsys *rws, struct radeon_bo *bo)
 {
    unsigned num_idle;
    bool busy = false;
@@ -62,7 +62,7 @@ static bool radeon_bo_is_busy(struct radeon_bo *bo)
          busy = true;
          break;
       }
-      radeon_ws_bo_reference(&bo->u.slab.fences[num_idle], NULL);
+      radeon_ws_bo_reference(rws, &bo->u.slab.fences[num_idle], NULL);
    }
    memmove(&bo->u.slab.fences[0], &bo->u.slab.fences[num_idle],
          (bo->u.slab.num_fences - num_idle) * sizeof(bo->u.slab.fences[0]));
@@ -81,7 +81,7 @@ static void radeon_real_bo_wait_idle(struct radeon_bo *bo)
                           &args, sizeof(args)) == -EBUSY);
 }
 
-static void radeon_bo_wait_idle(struct radeon_bo *bo)
+static void radeon_bo_wait_idle(struct radeon_winsys *rws, struct radeon_bo *bo)
 {
    if (bo->handle) {
       radeon_real_bo_wait_idle(bo);
@@ -89,7 +89,7 @@ static void radeon_bo_wait_idle(struct radeon_bo *bo)
       mtx_lock(&bo->rws->bo_fence_lock);
       while (bo->u.slab.num_fences) {
          struct radeon_bo *fence = NULL;
-         radeon_ws_bo_reference(&fence, bo->u.slab.fences[0]);
+         radeon_ws_bo_reference(rws, &fence, bo->u.slab.fences[0]);
          mtx_unlock(&bo->rws->bo_fence_lock);
 
          /* Wait without holding the fence lock. */
@@ -97,12 +97,12 @@ static void radeon_bo_wait_idle(struct radeon_bo *bo)
 
          mtx_lock(&bo->rws->bo_fence_lock);
          if (bo->u.slab.num_fences && fence == bo->u.slab.fences[0]) {
-            radeon_ws_bo_reference(&bo->u.slab.fences[0], NULL);
+            radeon_ws_bo_reference(rws, &bo->u.slab.fences[0], NULL);
             memmove(&bo->u.slab.fences[0], &bo->u.slab.fences[1],
                   (bo->u.slab.num_fences - 1) * sizeof(bo->u.slab.fences[0]));
             bo->u.slab.num_fences--;
          }
-         radeon_ws_bo_reference(&fence, NULL);
+         radeon_ws_bo_reference(rws, &fence, NULL);
       }
       mtx_unlock(&bo->rws->bo_fence_lock);
    }
@@ -117,7 +117,7 @@ static bool radeon_bo_wait(struct radeon_winsys *rws,
 
    /* No timeout. Just query. */
    if (timeout == 0)
-      return !bo->num_active_ioctls && !radeon_bo_is_busy(bo);
+      return !bo->num_active_ioctls && !radeon_bo_is_busy(rws, bo);
 
    abs_timeout = os_time_get_absolute_timeout(timeout);
 
@@ -127,12 +127,12 @@ static bool radeon_bo_wait(struct radeon_winsys *rws,
 
    /* Infinite timeout. */
    if (abs_timeout == OS_TIMEOUT_INFINITE) {
-      radeon_bo_wait_idle(bo);
+      radeon_bo_wait_idle(rws, bo);
       return true;
    }
 
    /* Other timeouts need to be emulated with a loop. */
-   while (radeon_bo_is_busy(bo)) {
+   while (radeon_bo_is_busy(rws, bo)) {
       if (os_time_get_nano() >= abs_timeout)
          return false;
       os_time_sleep(10);
@@ -704,7 +704,7 @@ static struct radeon_bo *radeon_create_bo(struct radeon_drm_winsys *rws,
                _mesa_hash_table_u64_search(rws->bo_vas, va.offset);
 
          mtx_unlock(&rws->bo_handles_mutex);
-         pb_reference(&b, &old_bo->base);
+         radeon_bo_reference(&rws->base, &b, &old_bo->base);
          return radeon_bo(b);
       }
 
@@ -804,7 +804,7 @@ struct pb_slab *radeon_bo_slab_alloc(void *priv, unsigned heap,
    return &slab->base;
 
 fail_buffer:
-   radeon_ws_bo_reference(&slab->buffer, NULL);
+   radeon_ws_bo_reference(&ws->base, &slab->buffer, NULL);
 fail:
    FREE(slab);
    return NULL;
@@ -812,17 +812,18 @@ fail:
 
 void radeon_bo_slab_free(void *priv, struct pb_slab *pslab)
 {
+   struct radeon_winsys *rws = (struct radeon_winsys *)priv;
    struct radeon_slab *slab = (struct radeon_slab *)pslab;
 
    for (unsigned i = 0; i < slab->base.num_entries; ++i) {
       struct radeon_bo *bo = &slab->entries[i];
       for (unsigned j = 0; j < bo->u.slab.num_fences; ++j)
-         radeon_ws_bo_reference(&bo->u.slab.fences[j], NULL);
+         radeon_ws_bo_reference(rws, &bo->u.slab.fences[j], NULL);
       FREE(bo->u.slab.fences);
    }
 
    FREE(slab->entries);
-   radeon_ws_bo_reference(&slab->buffer, NULL);
+   radeon_ws_bo_reference(rws, &slab->buffer, NULL);
    FREE(slab);
 }
 
@@ -1153,7 +1154,7 @@ static struct pb_buffer *radeon_winsys_bo_from_ptr(struct radeon_winsys *rws,
                _mesa_hash_table_u64_search(ws->bo_vas, va.offset);
 
          mtx_unlock(&ws->bo_handles_mutex);
-         pb_reference(&b, &old_bo->base);
+         radeon_bo_reference(rws, &b, &old_bo->base);
          return b;
       }
 
@@ -1284,7 +1285,7 @@ done:
                _mesa_hash_table_u64_search(ws->bo_vas, va.offset);
 
          mtx_unlock(&ws->bo_handles_mutex);
-         pb_reference(&b, &old_bo->base);
+         radeon_bo_reference(rws, &b, &old_bo->base);
          return b;
       }
 
diff --git a/src/gallium/winsys/radeon/drm/radeon_drm_bo.h b/src/gallium/winsys/radeon/drm/radeon_drm_bo.h
index 999f0acd47304..078f97568a7af 100644
--- a/src/gallium/winsys/radeon/drm/radeon_drm_bo.h
+++ b/src/gallium/winsys/radeon/drm/radeon_drm_bo.h
@@ -66,10 +66,11 @@ struct pb_slab *radeon_bo_slab_alloc(void *priv, unsigned heap,
                                      unsigned group_index);
 void radeon_bo_slab_free(void *priv, struct pb_slab *slab);
 
-static inline
-void radeon_ws_bo_reference(struct radeon_bo **dst, struct radeon_bo *src)
+static inline void
+radeon_ws_bo_reference(struct radeon_winsys *rws, struct radeon_bo **dst,
+                       struct radeon_bo *src)
 {
-   pb_reference((struct pb_buffer**)dst, (struct pb_buffer*)src);
+   radeon_bo_reference(rws, (struct pb_buffer**)dst, (struct pb_buffer*)src);
 }
 
 void *radeon_bo_do_map(struct radeon_bo *bo);
diff --git a/src/gallium/winsys/radeon/drm/radeon_drm_cs.c b/src/gallium/winsys/radeon/drm/radeon_drm_cs.c
index 75fd32a3aa254..fa324f4c7d71f 100644
--- a/src/gallium/winsys/radeon/drm/radeon_drm_cs.c
+++ b/src/gallium/winsys/radeon/drm/radeon_drm_cs.c
@@ -135,17 +135,18 @@ static bool radeon_init_cs_context(struct radeon_cs_context *csc,
    return true;
 }
 
-static void radeon_cs_context_cleanup(struct radeon_cs_context *csc)
+static void radeon_cs_context_cleanup(struct radeon_winsys *rws,
+                                      struct radeon_cs_context *csc)
 {
    unsigned i;
 
    for (i = 0; i < csc->num_relocs; i++) {
       p_atomic_dec(&csc->relocs_bo[i].bo->num_cs_references);
-      radeon_ws_bo_reference(&csc->relocs_bo[i].bo, NULL);
+      radeon_ws_bo_reference(rws, &csc->relocs_bo[i].bo, NULL);
    }
    for (i = 0; i < csc->num_slab_buffers; ++i) {
       p_atomic_dec(&csc->slab_buffers[i].bo->num_cs_references);
-      radeon_ws_bo_reference(&csc->slab_buffers[i].bo, NULL);
+      radeon_ws_bo_reference(rws, &csc->slab_buffers[i].bo, NULL);
    }
 
    csc->num_relocs = 0;
@@ -159,9 +160,9 @@ static void radeon_cs_context_cleanup(struct radeon_cs_context *csc)
    }
 }
 
-static void radeon_destroy_cs_context(struct radeon_cs_context *csc)
+static void radeon_destroy_cs_context(struct radeon_winsys *rws, struct radeon_cs_context *csc)
 {
-   radeon_cs_context_cleanup(csc);
+   radeon_cs_context_cleanup(rws, csc);
    FREE(csc->slab_buffers);
    FREE(csc->relocs_bo);
    FREE(csc->relocs);
@@ -201,7 +202,7 @@ radeon_drm_cs_create(struct radeon_cmdbuf *rcs,
       return false;
    }
    if (!radeon_init_cs_context(&cs->csc2, cs->ws)) {
-      radeon_destroy_cs_context(&cs->csc1);
+      radeon_destroy_cs_context(&ws->base, &cs->csc1);
       FREE(cs);
       return false;
    }
@@ -227,7 +228,8 @@ static void radeon_drm_cs_set_preamble(struct radeon_cmdbuf *cs, const uint32_t
    radeon_emit_array(cs, preamble_ib, preamble_num_dw);
 }
 
-int radeon_lookup_buffer(struct radeon_cs_context *csc, struct radeon_bo *bo)
+int radeon_lookup_buffer(struct radeon_winsys *rws, struct radeon_cs_context *csc,
+                         struct radeon_bo *bo)
 {
    unsigned hash = bo->hash & (ARRAY_SIZE(csc->reloc_indices_hashlist)-1);
    struct radeon_bo_item *buffers;
@@ -273,7 +275,7 @@ static unsigned radeon_lookup_or_add_real_buffer(struct radeon_drm_cs *cs,
    unsigned hash = bo->hash & (ARRAY_SIZE(csc->reloc_indices_hashlist)-1);
    int i = -1;
 
-   i = radeon_lookup_buffer(csc, bo);
+   i = radeon_lookup_buffer(&cs->ws->base, csc, bo);
 
    if (i >= 0) {
       /* For async DMA, every add_buffer call must add a buffer to the list
@@ -308,7 +310,7 @@ static unsigned radeon_lookup_or_add_real_buffer(struct radeon_drm_cs *cs,
    /* Initialize the new relocation. */
    csc->relocs_bo[csc->num_relocs].bo = NULL;
    csc->relocs_bo[csc->num_relocs].u.real.priority_usage = 0;
-   radeon_ws_bo_reference(&csc->relocs_bo[csc->num_relocs].bo, bo);
+   radeon_ws_bo_reference(&cs->ws->base, &csc->relocs_bo[csc->num_relocs].bo, bo);
    p_atomic_inc(&bo->num_cs_references);
    reloc = &csc->relocs[csc->num_relocs];
    reloc->handle = bo->handle;
@@ -332,7 +334,7 @@ static int radeon_lookup_or_add_slab_buffer(struct radeon_drm_cs *cs,
    int idx;
    int real_idx;
 
-   idx = radeon_lookup_buffer(csc, bo);
+   idx = radeon_lookup_buffer(&cs->ws->base, csc, bo);
    if (idx >= 0)
       return idx;
 
@@ -361,7 +363,7 @@ static int radeon_lookup_or_add_slab_buffer(struct radeon_drm_cs *cs,
 
    item->bo = NULL;
    item->u.slab.real_idx = real_idx;
-   radeon_ws_bo_reference(&item->bo, bo);
+   radeon_ws_bo_reference(&cs->ws->base, &item->bo, bo);
    p_atomic_inc(&bo->num_cs_references);
 
    hash = bo->hash & (ARRAY_SIZE(csc->reloc_indices_hashlist)-1);
@@ -425,7 +427,7 @@ static int radeon_drm_cs_lookup_buffer(struct radeon_cmdbuf *rcs,
 {
    struct radeon_drm_cs *cs = radeon_drm_cs(rcs);
 
-   return radeon_lookup_buffer(cs->csc, (struct radeon_bo*)buf);
+   return radeon_lookup_buffer(&cs->ws->base, cs->csc, (struct radeon_bo*)buf);
 }
 
 static bool radeon_drm_cs_validate(struct radeon_cmdbuf *rcs)
@@ -445,7 +447,7 @@ static bool radeon_drm_cs_validate(struct radeon_cmdbuf *rcs)
 
       for (i = cs->csc->num_validated_relocs; i < cs->csc->num_relocs; i++) {
          p_atomic_dec(&cs->csc->relocs_bo[i].bo->num_cs_references);
-         radeon_ws_bo_reference(&cs->csc->relocs_bo[i].bo, NULL);
+         radeon_ws_bo_reference(&cs->ws->base, &cs->csc->relocs_bo[i].bo, NULL);
       }
       cs->csc->num_relocs = cs->csc->num_validated_relocs;
 
@@ -454,7 +456,7 @@ static bool radeon_drm_cs_validate(struct radeon_cmdbuf *rcs)
          cs->flush_cs(cs->flush_data,
                       RADEON_FLUSH_ASYNC_START_NEXT_GFX_IB_NOW, NULL);
       } else {
-         radeon_cs_context_cleanup(cs->csc);
+         radeon_cs_context_cleanup(&cs->ws->base, cs->csc);
          rcs->used_vram_kb = 0;
          rcs->used_gart_kb = 0;
 
@@ -491,7 +493,8 @@ static unsigned radeon_drm_cs_get_buffer_list(struct radeon_cmdbuf *rcs,
 
 void radeon_drm_cs_emit_ioctl_oneshot(void *job, void *gdata, int thread_index)
 {
-   struct radeon_cs_context *csc = ((struct radeon_drm_cs*)job)->cst;
+   struct radeon_drm_cs *cs = (struct radeon_drm_cs*)job;
+   struct radeon_cs_context *csc = cs->cst;
    unsigned i;
    int r;
 
@@ -518,7 +521,7 @@ void radeon_drm_cs_emit_ioctl_oneshot(void *job, void *gdata, int thread_index)
    for (i = 0; i < csc->num_slab_buffers; i++)
       p_atomic_dec(&csc->slab_buffers[i].bo->num_active_ioctls);
 
-   radeon_cs_context_cleanup(csc);
+   radeon_cs_context_cleanup(&cs->ws->base, csc);
 }
 
 /*
@@ -544,7 +547,8 @@ void radeon_drm_cs_sync_flush(struct radeon_cmdbuf *rcs)
  * their respective ioctl do not have to be kept, because we know that they
  * will signal earlier.
  */
-static void radeon_bo_slab_fence(struct radeon_bo *bo, struct radeon_bo *fence)
+static void radeon_bo_slab_fence(struct radeon_winsys *rws, struct radeon_bo *bo,
+                                 struct radeon_bo *fence)
 {
    unsigned dst;
 
@@ -557,7 +561,7 @@ static void radeon_bo_slab_fence(struct radeon_bo *bo, struct radeon_bo *fence)
          bo->u.slab.fences[dst] = bo->u.slab.fences[src];
          dst++;
       } else {
-         radeon_ws_bo_reference(&bo->u.slab.fences[src], NULL);
+         radeon_ws_bo_reference(rws, &bo->u.slab.fences[src], NULL);
       }
    }
    bo->u.slab.num_fences = dst;
@@ -579,7 +583,7 @@ static void radeon_bo_slab_fence(struct radeon_bo *bo, struct radeon_bo *fence)
 
    /* Add the new fence */
    bo->u.slab.fences[bo->u.slab.num_fences] = NULL;
-   radeon_ws_bo_reference(&bo->u.slab.fences[bo->u.slab.num_fences], fence);
+   radeon_ws_bo_reference(rws, &bo->u.slab.fences[bo->u.slab.num_fences], fence);
    bo->u.slab.num_fences++;
 }
 
@@ -643,7 +647,7 @@ static int radeon_drm_cs_flush(struct radeon_cmdbuf *rcs,
          for (unsigned i = 0; i < cs->csc->num_slab_buffers; ++i) {
             struct radeon_bo *bo = cs->csc->slab_buffers[i].bo;
             p_atomic_inc(&bo->num_active_ioctls);
-            radeon_bo_slab_fence(bo, (struct radeon_bo *)fence);
+            radeon_bo_slab_fence(&cs->ws->base, bo, (struct radeon_bo *)fence);
          }
          mtx_unlock(&cs->ws->bo_fence_lock);
 
@@ -727,7 +731,7 @@ static int radeon_drm_cs_flush(struct radeon_cmdbuf *rcs,
          radeon_drm_cs_emit_ioctl_oneshot(cs, NULL, 0);
       }
    } else {
-      radeon_cs_context_cleanup(cs->cst);
+      radeon_cs_context_cleanup(&cs->ws->base, cs->cst);
    }
 
    /* Prepare a new CS. */
@@ -752,11 +756,11 @@ static void radeon_drm_cs_destroy(struct radeon_cmdbuf *rcs)
 
    radeon_drm_cs_sync_flush(rcs);
    util_queue_fence_destroy(&cs->flush_completed);
-   radeon_cs_context_cleanup(&cs->csc1);
-   radeon_cs_context_cleanup(&cs->csc2);
+   radeon_cs_context_cleanup(&cs->ws->base, &cs->csc1);
+   radeon_cs_context_cleanup(&cs->ws->base, &cs->csc2);
    p_atomic_dec(&cs->ws->num_cs);
-   radeon_destroy_cs_context(&cs->csc1);
-   radeon_destroy_cs_context(&cs->csc2);
+   radeon_destroy_cs_context(&cs->ws->base, &cs->csc1);
+   radeon_destroy_cs_context(&cs->ws->base, &cs->csc2);
    radeon_fence_reference(&cs->ws->base, &cs->next_fence, NULL);
    FREE(cs);
 }
@@ -772,7 +776,7 @@ static bool radeon_bo_is_referenced(struct radeon_cmdbuf *rcs,
    if (!bo->num_cs_references)
       return false;
 
-   index = radeon_lookup_buffer(cs->csc, bo);
+   index = radeon_lookup_buffer(&cs->ws->base, cs->csc, bo);
    if (index == -1)
       return false;
 
@@ -820,7 +824,7 @@ static void radeon_fence_reference(struct radeon_winsys *ws,
                                    struct pipe_fence_handle **dst,
                                    struct pipe_fence_handle *src)
 {
-   pb_reference((struct pb_buffer**)dst, (struct pb_buffer*)src);
+   radeon_bo_reference(ws, (struct pb_buffer**)dst, (struct pb_buffer*)src);
 }
 
 static struct pipe_fence_handle *radeon_drm_cs_get_next_fence(struct radeon_cmdbuf *rcs)
diff --git a/src/gallium/winsys/radeon/drm/radeon_drm_cs.h b/src/gallium/winsys/radeon/drm/radeon_drm_cs.h
index 1ea6cb075f104..637c70f8b80dd 100644
--- a/src/gallium/winsys/radeon/drm/radeon_drm_cs.h
+++ b/src/gallium/winsys/radeon/drm/radeon_drm_cs.h
@@ -73,7 +73,8 @@ struct radeon_drm_cs {
    struct pipe_fence_handle *next_fence;
 };
 
-int radeon_lookup_buffer(struct radeon_cs_context *csc, struct radeon_bo *bo);
+int radeon_lookup_buffer(struct radeon_winsys *rws, struct radeon_cs_context *csc,
+                         struct radeon_bo *bo);
 
 static inline struct radeon_drm_cs *
 radeon_drm_cs(struct radeon_cmdbuf *rcs)
@@ -87,7 +88,7 @@ radeon_bo_is_referenced_by_cs(struct radeon_drm_cs *cs,
 {
    int num_refs = bo->num_cs_references;
    return num_refs == bo->rws->num_cs ||
-         (num_refs && radeon_lookup_buffer(cs->csc, bo) != -1);
+         (num_refs && radeon_lookup_buffer(&cs->ws->base, cs->csc, bo) != -1);
 }
 
 static inline bool
@@ -99,7 +100,7 @@ radeon_bo_is_referenced_by_cs_for_write(struct radeon_drm_cs *cs,
    if (!bo->num_cs_references)
       return false;
 
-   index = radeon_lookup_buffer(cs->csc, bo);
+   index = radeon_lookup_buffer(&cs->ws->base, cs->csc, bo);
    if (index == -1)
       return false;
 
-- 
GitLab


From 3e83c54c84a82edbc145d5ea690b54406222ee67 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Fri, 8 Dec 2023 20:10:11 -0500
Subject: [PATCH 10/36] winsys/amdgpu: don't layer slabs, use only 1 level of
 slabs, it improves perf

This increases FPS in VP2020/Catia1 by 10-18%!!!!!!!!!!!!!!!!!!!!!!!

I have no rational explanation for this.

In the most extreme case, 8192 256B slab BOs (smallest size) are now
allocated from a single 2MB slab.

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c     | 95 +++++++------------
 src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c | 47 ++++-----
 src/gallium/winsys/amdgpu/drm/amdgpu_winsys.h |  8 +-
 3 files changed, 49 insertions(+), 101 deletions(-)

diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index 86d57cf2eaa50..a999d7e2882c4 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -229,9 +229,7 @@ static void amdgpu_bo_destroy_or_cache(struct radeon_winsys *rws, struct pb_buff
 
 static void amdgpu_clean_up_buffer_managers(struct amdgpu_winsys *ws)
 {
-   for (unsigned i = 0; i < NUM_SLAB_ALLOCATORS; i++)
-      pb_slabs_reclaim(&ws->bo_slabs[i]);
-
+   pb_slabs_reclaim(&ws->bo_slabs);
    pb_cache_release_all_buffers(&ws->bo_cache);
 }
 
@@ -615,25 +613,11 @@ bool amdgpu_bo_can_reclaim_slab(void *priv, struct pb_slab_entry *entry)
    return amdgpu_bo_can_reclaim(priv, &bo->b.base);
 }
 
-static struct pb_slabs *get_slabs(struct amdgpu_winsys *ws, uint64_t size)
-{
-   /* Find the correct slab allocator for the given size. */
-   for (unsigned i = 0; i < NUM_SLAB_ALLOCATORS; i++) {
-      struct pb_slabs *slabs = &ws->bo_slabs[i];
-
-      if (size <= 1 << (slabs->min_order + slabs->num_orders - 1))
-         return slabs;
-   }
-
-   assert(0);
-   return NULL;
-}
-
 static unsigned get_slab_wasted_size(struct amdgpu_winsys *ws, struct amdgpu_bo_slab_entry *bo)
 {
    assert(bo->b.base.size <= bo->entry.slab->entry_size);
    assert(bo->b.base.size < (1 << bo->b.base.alignment_log2) ||
-          bo->b.base.size < 1 << ws->bo_slabs[0].min_order ||
+          bo->b.base.size < 1 << ws->bo_slabs.min_order ||
           bo->b.base.size > bo->entry.slab->entry_size / 2);
    return bo->entry.slab->entry_size - bo->b.base.size;
 }
@@ -642,23 +626,20 @@ static void amdgpu_bo_slab_destroy(struct radeon_winsys *rws, struct pb_buffer *
 {
    struct amdgpu_winsys *ws = amdgpu_winsys(rws);
    struct amdgpu_bo_slab_entry *bo = get_slab_entry_bo(amdgpu_winsys_bo(_buf));
-   struct pb_slabs *slabs;
-
-   slabs = get_slabs(ws, bo->b.base.size);
 
    if (bo->b.base.placement & RADEON_DOMAIN_VRAM)
       ws->slab_wasted_vram -= get_slab_wasted_size(ws, bo);
    else
       ws->slab_wasted_gtt -= get_slab_wasted_size(ws, bo);
 
-   pb_slab_free(slabs, &bo->entry);
+   pb_slab_free(&ws->bo_slabs, &bo->entry);
 }
 
 /* Return the power of two size of a slab entry matching the input size. */
 static unsigned get_slab_pot_entry_size(struct amdgpu_winsys *ws, unsigned size)
 {
    unsigned entry_size = util_next_power_of_two(size);
-   unsigned min_entry_size = 1 << ws->bo_slabs[0].min_order;
+   unsigned min_entry_size = 1 << ws->bo_slabs.min_order;
 
    return MAX2(entry_size, min_entry_size);
 }
@@ -682,44 +663,37 @@ struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_s
    enum radeon_bo_domain domains = radeon_domain_from_heap(heap);
    enum radeon_bo_flag flags = radeon_flags_from_heap(heap);
    uint32_t base_id;
-   unsigned slab_size = 0;
 
    if (!slab)
       return NULL;
 
    /* Determine the slab buffer size. */
-   for (unsigned i = 0; i < NUM_SLAB_ALLOCATORS; i++) {
-      unsigned max_entry_size = 1 << (ws->bo_slabs[i].min_order + ws->bo_slabs[i].num_orders - 1);
+   unsigned max_entry_size = 1 << (ws->bo_slabs.min_order + ws->bo_slabs.num_orders - 1);
 
-      if (entry_size <= max_entry_size) {
-         /* The slab size is twice the size of the largest possible entry. */
-         slab_size = max_entry_size * 2;
+   assert(entry_size <= max_entry_size);
 
-         if (!util_is_power_of_two_nonzero(entry_size)) {
-            assert(util_is_power_of_two_nonzero(entry_size * 4 / 3));
+   /* The slab size is twice the size of the largest possible entry. */
+   unsigned slab_size = max_entry_size * 2;
 
-            /* If the entry size is 3/4 of a power of two, we would waste space and not gain
-             * anything if we allocated only twice the power of two for the backing buffer:
-             *   2 * 3/4 = 1.5 usable with buffer size 2
-             *
-             * Allocating 5 times the entry size leads us to the next power of two and results
-             * in a much better memory utilization:
-             *   5 * 3/4 = 3.75 usable with buffer size 4
-             */
-            if (entry_size * 5 > slab_size)
-               slab_size = util_next_power_of_two(entry_size * 5);
-         }
+   if (!util_is_power_of_two_nonzero(entry_size)) {
+      assert(util_is_power_of_two_nonzero(entry_size * 4 / 3));
 
-         /* The largest slab should have the same size as the PTE fragment
-          * size to get faster address translation.
-          */
-         if (i == NUM_SLAB_ALLOCATORS - 1 &&
-             slab_size < ws->info.pte_fragment_size)
-            slab_size = ws->info.pte_fragment_size;
-         break;
-      }
+      /* If the entry size is 3/4 of a power of two, we would waste space and not gain
+       * anything if we allocated only twice the power of two for the backing buffer:
+       *   2 * 3/4 = 1.5 usable with buffer size 2
+       *
+       * Allocating 5 times the entry size leads us to the next power of two and results
+       * in a much better memory utilization:
+       *   5 * 3/4 = 3.75 usable with buffer size 4
+       */
+      if (entry_size * 5 > slab_size)
+         slab_size = util_next_power_of_two(entry_size * 5);
    }
-   assert(slab_size != 0);
+
+   /* The largest slab should have the same size as the PTE fragment
+    * size to get faster address translation.
+    */
+   slab_size = MAX2(slab_size, ws->info.pte_fragment_size);
 
    slab->buffer = amdgpu_winsys_bo(amdgpu_bo_create(ws,
                                                     slab_size, slab_size,
@@ -727,6 +701,7 @@ struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_s
    if (!slab->buffer)
       goto fail;
 
+   /* We can get a buffer from pb_cache that is slightly larger. */
    slab_size = slab->buffer->base.size;
 
    slab->base.num_entries = slab_size / entry_size;
@@ -751,13 +726,9 @@ struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_s
       bo->b.va = slab->buffer->va + i * entry_size;
       bo->b.unique_id = base_id + i;
 
-      if (is_real_bo(slab->buffer)) {
-         /* The slab is not suballocated. */
-         bo->real = get_real_bo(slab->buffer);
-      } else {
-         /* The slab is allocated out of a bigger slab. */
-         bo->real = get_slab_entry_bo(slab->buffer)->real;
-      }
+      /* The slab is not suballocated. */
+      assert(is_real_bo(slab->buffer));
+      bo->real = get_real_bo(slab->buffer);
 
       bo->entry.slab = &slab->base;
       list_addtail(&bo->entry.head, &slab->base.free);
@@ -1358,8 +1329,7 @@ amdgpu_bo_create(struct amdgpu_winsys *ws,
       return amdgpu_bo_sparse_create(ws, size, domain, flags);
    }
 
-   struct pb_slabs *last_slab = &ws->bo_slabs[NUM_SLAB_ALLOCATORS - 1];
-   unsigned max_slab_entry_size = 1 << (last_slab->min_order + last_slab->num_orders - 1);
+   unsigned max_slab_entry_size = 1 << (ws->bo_slabs.min_order + ws->bo_slabs.num_orders - 1);
    int heap = radeon_get_heap_index(domain, flags);
 
    /* Sub-allocate small buffers from slabs. */
@@ -1387,13 +1357,12 @@ amdgpu_bo_create(struct amdgpu_winsys *ws,
          }
       }
 
-      struct pb_slabs *slabs = get_slabs(ws, alloc_size);
-      entry = pb_slab_alloc(slabs, alloc_size, heap);
+      entry = pb_slab_alloc(&ws->bo_slabs, alloc_size, heap);
       if (!entry) {
          /* Clean up buffer managers and try again. */
          amdgpu_clean_up_buffer_managers(ws);
 
-         entry = pb_slab_alloc(slabs, alloc_size, heap);
+         entry = pb_slab_alloc(&ws->bo_slabs, alloc_size, heap);
       }
       if (!entry)
          return NULL;
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c b/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c
index abf4c46407176..d54f229c36341 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c
@@ -76,10 +76,8 @@ static void do_winsys_deinit(struct amdgpu_winsys *ws)
       util_queue_destroy(&ws->cs_queue);
 
    simple_mtx_destroy(&ws->bo_fence_lock);
-   for (unsigned i = 0; i < NUM_SLAB_ALLOCATORS; i++) {
-      if (ws->bo_slabs[i].groups)
-         pb_slabs_deinit(&ws->bo_slabs[i]);
-   }
+   if (ws->bo_slabs.groups)
+      pb_slabs_deinit(&ws->bo_slabs);
    pb_cache_deinit(&ws->bo_cache);
    _mesa_hash_table_destroy(ws->bo_export_table, NULL);
    simple_mtx_destroy(&ws->sws_list_lock);
@@ -454,35 +452,22 @@ amdgpu_winsys_create(int fd, const struct pipe_screen_config *config,
                      * is a struct pointer instead of void*. */
                     (void*)amdgpu_bo_destroy, (void*)amdgpu_bo_can_reclaim);
 
-      unsigned min_slab_order = 8;  /* 256 bytes */
-      unsigned max_slab_order = 20; /* 1 MB (slab size = 2 MB) */
-      unsigned num_slab_orders_per_allocator = (max_slab_order - min_slab_order) /
-                                               NUM_SLAB_ALLOCATORS;
-
-      /* Divide the size order range among slab managers. */
-      for (unsigned i = 0; i < NUM_SLAB_ALLOCATORS; i++) {
-         unsigned min_order = min_slab_order;
-         unsigned max_order = MIN2(min_order + num_slab_orders_per_allocator,
-                                   max_slab_order);
-
-         if (!pb_slabs_init(&aws->bo_slabs[i],
-                            min_order, max_order,
-                            RADEON_NUM_HEAPS, true,
-                            aws,
-                            amdgpu_bo_can_reclaim_slab,
-                            amdgpu_bo_slab_alloc,
-                            /* Cast to void* because one of the function parameters
-                             * is a struct pointer instead of void*. */
-                            (void*)amdgpu_bo_slab_free)) {
-            amdgpu_winsys_destroy(&ws->base);
-            simple_mtx_unlock(&dev_tab_mutex);
-            return NULL;
-         }
-
-         min_slab_order = max_order + 1;
+      if (!pb_slabs_init(&aws->bo_slabs,
+                         8,  /* min slab entry size: 256 bytes */
+                         20, /* max slab entry size: 1 MB (slab size = 2 MB) */
+                         RADEON_NUM_HEAPS, true,
+                         aws,
+                         amdgpu_bo_can_reclaim_slab,
+                         amdgpu_bo_slab_alloc,
+                         /* Cast to void* because one of the function parameters
+                          * is a struct pointer instead of void*. */
+                         (void*)amdgpu_bo_slab_free)) {
+         amdgpu_winsys_destroy(&ws->base);
+         simple_mtx_unlock(&dev_tab_mutex);
+         return NULL;
       }
 
-      aws->info.min_alloc_size = 1 << aws->bo_slabs[0].min_order;
+      aws->info.min_alloc_size = 1 << aws->bo_slabs.min_order;
 
       /* init reference */
       pipe_reference_init(&aws->reference, 1);
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.h b/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.h
index 62b583a5d551a..4c552461bca2b 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.h
@@ -17,8 +17,6 @@
 
 struct amdgpu_cs;
 
-#define NUM_SLAB_ALLOCATORS 3
-
 /* DRM file descriptors, file descriptions and buffer sharing.
  *
  * amdgpu_device_initialize first argument is a file descriptor (fd)
@@ -70,11 +68,7 @@ struct amdgpu_winsys {
    int fd;
 
    struct pb_cache bo_cache;
-
-   /* Each slab buffer can only contain suballocations of equal sizes, so we
-    * need to layer the allocators, so that we don't waste too much memory.
-    */
-   struct pb_slabs bo_slabs[NUM_SLAB_ALLOCATORS];
+   struct pb_slabs bo_slabs;  /* Slab allocator. */
 
    amdgpu_device_handle dev;
 
-- 
GitLab


From 6746660ce09bd222893d830fcc50a669b89b1f44 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 9 Dec 2023 01:28:20 -0500
Subject: [PATCH 11/36] winsys/amdgpu: add amdgpu_bo_real_reusable slab for the
 backing buffer

Add contents of amdgpu_bo_slab into it. This will allow removing the "real"
pointer from amdgpu_bo_slab_entry because "(char*)entry.slab" is now
pointing next to it.

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/include/winsys/radeon_winsys.h |   1 +
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c  | 131 +++++++++++++--------
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.h  |  27 +++--
 src/gallium/winsys/amdgpu/drm/amdgpu_cs.h  |   2 +-
 4 files changed, 97 insertions(+), 64 deletions(-)

diff --git a/src/gallium/include/winsys/radeon_winsys.h b/src/gallium/include/winsys/radeon_winsys.h
index 9ac02e058e9ea..b9d76c52061c0 100644
--- a/src/gallium/include/winsys/radeon_winsys.h
+++ b/src/gallium/include/winsys/radeon_winsys.h
@@ -63,6 +63,7 @@ enum radeon_bo_flag
     * This guarantees that this buffer will never be moved to GTT.
     */
   RADEON_FLAG_DISCARDABLE = (1 << 10),
+  RADEON_FLAG_WINSYS_SLAB_BACKING = (1 << 11), /* only used by the winsys */
 };
 
 static inline void
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index a999d7e2882c4..e6db491873add 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -221,7 +221,7 @@ static void amdgpu_bo_destroy_or_cache(struct radeon_winsys *rws, struct pb_buff
 
    assert(is_real_bo(bo)); /* slab buffers have a separate vtbl */
 
-   if (bo->type == AMDGPU_BO_REAL_REUSABLE)
+   if (bo->type >= AMDGPU_BO_REAL_REUSABLE)
       pb_cache_add_buffer(&((struct amdgpu_bo_real_reusable*)bo)->cache_entry);
    else
       amdgpu_bo_destroy(ws, _buf);
@@ -469,13 +469,20 @@ static struct amdgpu_winsys_bo *amdgpu_create_bo(struct amdgpu_winsys *ws,
    alignment = amdgpu_get_optimal_alignment(ws, size, alignment);
 
    if (heap >= 0 && flags & RADEON_FLAG_NO_INTERPROCESS_SHARING) {
-      struct amdgpu_bo_real_reusable *new_bo = CALLOC_STRUCT(amdgpu_bo_real_reusable);
+      struct amdgpu_bo_real_reusable *new_bo;
+      bool slab_backing = flags & RADEON_FLAG_WINSYS_SLAB_BACKING;
+
+      if (slab_backing)
+         new_bo = (struct amdgpu_bo_real_reusable *)CALLOC_STRUCT(amdgpu_bo_real_reusable_slab);
+      else
+         new_bo = CALLOC_STRUCT(amdgpu_bo_real_reusable);
+
       if (!new_bo)
          return NULL;
 
       bo = &new_bo->b;
       pb_cache_init_entry(&ws->bo_cache, &new_bo->cache_entry, &bo->b.base, heap);
-      bo->b.type = AMDGPU_BO_REAL_REUSABLE;
+      bo->b.type = slab_backing ? AMDGPU_BO_REAL_REUSABLE_SLAB : AMDGPU_BO_REAL_REUSABLE;
    } else {
       bo = CALLOC_STRUCT(amdgpu_bo_real);
       if (!bo)
@@ -659,14 +666,10 @@ struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_s
                                      unsigned group_index)
 {
    struct amdgpu_winsys *ws = priv;
-   struct amdgpu_slab *slab = CALLOC_STRUCT(amdgpu_slab);
    enum radeon_bo_domain domains = radeon_domain_from_heap(heap);
    enum radeon_bo_flag flags = radeon_flags_from_heap(heap);
    uint32_t base_id;
 
-   if (!slab)
-      return NULL;
-
    /* Determine the slab buffer size. */
    unsigned max_entry_size = 1 << (ws->bo_slabs.min_order + ws->bo_slabs.num_orders - 1);
 
@@ -695,78 +698,81 @@ struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_s
     */
    slab_size = MAX2(slab_size, ws->info.pte_fragment_size);
 
-   slab->buffer = amdgpu_winsys_bo(amdgpu_bo_create(ws,
-                                                    slab_size, slab_size,
-                                                    domains, flags));
-   if (!slab->buffer)
-      goto fail;
+   flags |= RADEON_FLAG_NO_INTERPROCESS_SHARING |
+            RADEON_FLAG_NO_SUBALLOC |
+            RADEON_FLAG_WINSYS_SLAB_BACKING;
 
-   /* We can get a buffer from pb_cache that is slightly larger. */
-   slab_size = slab->buffer->base.size;
+   struct amdgpu_bo_real_reusable_slab *slab_bo =
+      (struct amdgpu_bo_real_reusable_slab*)amdgpu_bo_create(ws, slab_size, slab_size,
+                                                             domains, flags);
+   if (!slab_bo)
+      return NULL;
+
+   /* The slab is not suballocated. */
+   assert(is_real_bo(&slab_bo->b.b.b));
+   assert(slab_bo->b.b.b.type == AMDGPU_BO_REAL_REUSABLE_SLAB);
 
-   slab->base.num_entries = slab_size / entry_size;
-   slab->base.num_free = slab->base.num_entries;
-   slab->base.group_index = group_index;
-   slab->base.entry_size = entry_size;
-   slab->entries = CALLOC(slab->base.num_entries, sizeof(*slab->entries));
-   if (!slab->entries)
-      goto fail_buffer;
+   /* We can get a buffer from pb_cache that is slightly larger. */
+   slab_size = slab_bo->b.b.b.base.size;
+
+   slab_bo->slab.num_entries = slab_size / entry_size;
+   slab_bo->slab.num_free = slab_bo->slab.num_entries;
+   slab_bo->slab.group_index = group_index;
+   slab_bo->slab.entry_size = entry_size;
+   slab_bo->entries = CALLOC(slab_bo->slab.num_entries, sizeof(*slab_bo->entries));
+   if (!slab_bo->entries)
+      goto fail;
 
-   list_inithead(&slab->base.free);
+   list_inithead(&slab_bo->slab.free);
 
-   base_id = __sync_fetch_and_add(&ws->next_bo_unique_id, slab->base.num_entries);
+   base_id = __sync_fetch_and_add(&ws->next_bo_unique_id, slab_bo->slab.num_entries);
 
-   for (unsigned i = 0; i < slab->base.num_entries; ++i) {
-      struct amdgpu_bo_slab_entry *bo = &slab->entries[i];
+   for (unsigned i = 0; i < slab_bo->slab.num_entries; ++i) {
+      struct amdgpu_bo_slab_entry *bo = &slab_bo->entries[i];
 
       bo->b.base.placement = domains;
       bo->b.base.alignment_log2 = util_logbase2(get_slab_entry_alignment(ws, entry_size));
       bo->b.base.size = entry_size;
       bo->b.type = AMDGPU_BO_SLAB_ENTRY;
-      bo->b.va = slab->buffer->va + i * entry_size;
+      bo->b.va = slab_bo->b.b.b.va + i * entry_size;
       bo->b.unique_id = base_id + i;
 
-      /* The slab is not suballocated. */
-      assert(is_real_bo(slab->buffer));
-      bo->real = get_real_bo(slab->buffer);
+      bo->real = &slab_bo->b.b;
 
-      bo->entry.slab = &slab->base;
-      list_addtail(&bo->entry.head, &slab->base.free);
+      bo->entry.slab = &slab_bo->slab;
+      list_addtail(&bo->entry.head, &slab_bo->slab.free);
    }
 
    /* Wasted alignment due to slabs with 3/4 allocations being aligned to a power of two. */
-   assert(slab->base.num_entries * entry_size <= slab_size);
+   assert(slab_bo->slab.num_entries * entry_size <= slab_size);
    if (domains & RADEON_DOMAIN_VRAM)
-      ws->slab_wasted_vram += slab_size - slab->base.num_entries * entry_size;
+      ws->slab_wasted_vram += slab_size - slab_bo->slab.num_entries * entry_size;
    else
-      ws->slab_wasted_gtt += slab_size - slab->base.num_entries * entry_size;
+      ws->slab_wasted_gtt += slab_size - slab_bo->slab.num_entries * entry_size;
 
-   return &slab->base;
+   return &slab_bo->slab;
 
-fail_buffer:
-   amdgpu_winsys_bo_reference(ws, &slab->buffer, NULL);
 fail:
-   FREE(slab);
+   amdgpu_winsys_bo_reference(ws, (struct amdgpu_winsys_bo**)&slab_bo, NULL);
    return NULL;
 }
 
-void amdgpu_bo_slab_free(struct amdgpu_winsys *ws, struct pb_slab *pslab)
+void amdgpu_bo_slab_free(struct amdgpu_winsys *ws, struct pb_slab *slab)
 {
-   struct amdgpu_slab *slab = amdgpu_slab(pslab);
-   unsigned slab_size = slab->buffer->base.size;
+   struct amdgpu_bo_real_reusable_slab *bo = get_bo_from_slab(slab);
+   unsigned slab_size = bo->b.b.b.base.size;
 
-   assert(slab->base.num_entries * slab->base.entry_size <= slab_size);
-   if (slab->buffer->base.placement & RADEON_DOMAIN_VRAM)
-      ws->slab_wasted_vram -= slab_size - slab->base.num_entries * slab->base.entry_size;
+   assert(bo->slab.num_entries * bo->slab.entry_size <= slab_size);
+   if (bo->b.b.b.base.placement & RADEON_DOMAIN_VRAM)
+      ws->slab_wasted_vram -= slab_size - bo->slab.num_entries * bo->slab.entry_size;
    else
-      ws->slab_wasted_gtt -= slab_size - slab->base.num_entries * slab->base.entry_size;
+      ws->slab_wasted_gtt -= slab_size - bo->slab.num_entries * bo->slab.entry_size;
 
-   for (unsigned i = 0; i < slab->base.num_entries; ++i)
-      amdgpu_bo_remove_fences(&slab->entries[i].b);
+   for (unsigned i = 0; i < bo->slab.num_entries; ++i)
+      amdgpu_bo_remove_fences(&bo->entries[i].b);
 
-   FREE(slab->entries);
-   amdgpu_winsys_bo_reference(ws, &slab->buffer, NULL);
-   FREE(slab);
+   FREE(bo->entries);
+   amdgpu_winsys_bo_reference(ws, (struct amdgpu_winsys_bo**)&bo, NULL);
 }
 
 #if DEBUG_SPARSE_COMMITS
@@ -1401,8 +1407,31 @@ no_slab:
        /* Get a buffer from the cache. */
        bo = (struct amdgpu_winsys_bo*)
             pb_cache_reclaim_buffer(&ws->bo_cache, size, alignment, 0, heap);
-       if (bo)
+       if (bo) {
+          /* If the buffer is amdgpu_bo_real_reusable, but we need amdgpu_bo_real_reusable_slab,
+           * keep the allocation but make the structure bigger.
+           */
+          if (flags & RADEON_FLAG_WINSYS_SLAB_BACKING && bo->type == AMDGPU_BO_REAL_REUSABLE) {
+             const unsigned orig_size = sizeof(struct amdgpu_bo_real_reusable);
+             const unsigned new_size = sizeof(struct amdgpu_bo_real_reusable_slab);
+             struct amdgpu_winsys_bo *new_bo =
+                (struct amdgpu_winsys_bo*)REALLOC(bo, orig_size, new_size);
+
+             if (!new_bo) {
+                amdgpu_winsys_bo_reference(ws, &bo, NULL);
+                return NULL;
+             }
+
+             memset((uint8_t*)new_bo + orig_size, 0, new_size - orig_size);
+             bo = new_bo;
+             bo->type = AMDGPU_BO_REAL_REUSABLE_SLAB;
+
+             /* Re-set pointers after realloc. */
+             struct amdgpu_bo_real_reusable *real_bo = get_real_bo_reusable(bo);
+             real_bo->cache_entry.buffer = &bo->base;
+          }
           return &bo->base;
+       }
    }
 
    /* Create a new one. */
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
index 833caae1e05f5..1d3777b2573d9 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
@@ -38,8 +38,9 @@ struct amdgpu_sparse_commitment {
 enum amdgpu_bo_type {
    AMDGPU_BO_SLAB_ENTRY,
    AMDGPU_BO_SPARSE,
-   AMDGPU_BO_REAL, /* only REAL enums can be present after this */
-   AMDGPU_BO_REAL_REUSABLE,
+   AMDGPU_BO_REAL,               /* only REAL enums can be present after this */
+   AMDGPU_BO_REAL_REUSABLE,      /* only REAL_REUSABLE enums can be present after this */
+   AMDGPU_BO_REAL_REUSABLE_SLAB,
 };
 
 /* Anything above REAL will use the BO list for REAL. */
@@ -124,9 +125,12 @@ struct amdgpu_bo_slab_entry {
    struct pb_slab_entry entry;
 };
 
-struct amdgpu_slab {
-   struct pb_slab base;
-   struct amdgpu_winsys_bo *buffer;
+/* The slab buffer, which is the big backing buffer out of which smaller BOs are suballocated and
+ * represented by amdgpu_bo_slab_entry. It's always a real and reusable buffer.
+ */
+struct amdgpu_bo_real_reusable_slab {
+   struct amdgpu_bo_real_reusable b;
+   struct pb_slab slab;
    struct amdgpu_bo_slab_entry *entries;
 };
 
@@ -143,7 +147,7 @@ static struct amdgpu_bo_real *get_real_bo(struct amdgpu_winsys_bo *bo)
 
 static struct amdgpu_bo_real_reusable *get_real_bo_reusable(struct amdgpu_winsys_bo *bo)
 {
-   assert(bo->type == AMDGPU_BO_REAL_REUSABLE);
+   assert(bo->type >= AMDGPU_BO_REAL_REUSABLE);
    return (struct amdgpu_bo_real_reusable*)bo;
 }
 
@@ -159,6 +163,11 @@ static struct amdgpu_bo_slab_entry *get_slab_entry_bo(struct amdgpu_winsys_bo *b
    return (struct amdgpu_bo_slab_entry*)bo;
 }
 
+static inline struct amdgpu_bo_real_reusable_slab *get_bo_from_slab(struct pb_slab *slab)
+{
+   return container_of(slab, struct amdgpu_bo_real_reusable_slab, slab);
+}
+
 bool amdgpu_bo_can_reclaim(struct amdgpu_winsys *ws, struct pb_buffer *_buf);
 struct pb_buffer *amdgpu_bo_create(struct amdgpu_winsys *ws,
                                    uint64_t size,
@@ -184,12 +193,6 @@ struct amdgpu_winsys_bo *amdgpu_winsys_bo(struct pb_buffer *bo)
    return (struct amdgpu_winsys_bo *)bo;
 }
 
-static inline
-struct amdgpu_slab *amdgpu_slab(struct pb_slab *slab)
-{
-   return (struct amdgpu_slab *)slab;
-}
-
 static inline
 void amdgpu_winsys_bo_reference(struct amdgpu_winsys *ws,
                                 struct amdgpu_winsys_bo **dst,
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h
index 81ab171deaf12..df222d6fecab2 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h
@@ -224,7 +224,7 @@ amdgpu_bo_is_referenced_by_cs(struct amdgpu_cs *cs,
 
 static inline unsigned get_buf_list_idx(struct amdgpu_winsys_bo *bo)
 {
-   /* AMDGPU_BO_REAL_REUSABLE maps to AMDGPU_BO_REAL. */
+   /* AMDGPU_BO_REAL_REUSABLE* maps to AMDGPU_BO_REAL. */
    static_assert(ARRAY_SIZE(((struct amdgpu_cs_context*)NULL)->buffer_lists) == NUM_BO_LIST_TYPES, "");
    return MIN2(bo->type, AMDGPU_BO_REAL);
 }
-- 
GitLab


From 70ddcd556efb54a9564313c2717291422fa3ba37 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 9 Dec 2023 01:46:28 -0500
Subject: [PATCH 12/36] winsys/amdgpu: remove now-redundant
 amdgpu_bo_slab_entry::real

The pb_slab pointer can be used to get the BO pointer because pb_slab is
inside the BO structure now.

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c | 6 ++----
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.h | 7 ++++++-
 src/gallium/winsys/amdgpu/drm/amdgpu_cs.c | 2 +-
 3 files changed, 9 insertions(+), 6 deletions(-)

diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index e6db491873add..c5204278f6f13 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -359,7 +359,7 @@ void *amdgpu_bo_map(struct radeon_winsys *rws,
    if (is_real_bo(bo)) {
       real = get_real_bo(bo);
    } else {
-      real = get_slab_entry_bo(bo)->real;
+      real = get_slab_entry_real_bo(bo);
       offset = bo->va - real->b.va;
    }
 
@@ -399,7 +399,7 @@ void amdgpu_bo_unmap(struct radeon_winsys *rws, struct pb_buffer *buf)
 
    assert(bo->type != AMDGPU_BO_SPARSE);
 
-   real = is_real_bo(bo) ? get_real_bo(bo) : get_slab_entry_bo(bo)->real;
+   real = is_real_bo(bo) ? get_real_bo(bo) : get_slab_entry_real_bo(bo);
 
    if (real->is_user_ptr)
       return;
@@ -737,8 +737,6 @@ struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_s
       bo->b.va = slab_bo->b.b.b.va + i * entry_size;
       bo->b.unique_id = base_id + i;
 
-      bo->real = &slab_bo->b.b;
-
       bo->entry.slab = &slab_bo->slab;
       list_addtail(&bo->entry.head, &slab_bo->slab.free);
    }
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
index 1d3777b2573d9..be7c3d1bbe28c 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
@@ -121,7 +121,6 @@ struct amdgpu_bo_sparse {
  */
 struct amdgpu_bo_slab_entry {
    struct amdgpu_winsys_bo b;
-   struct amdgpu_bo_real *real;
    struct pb_slab_entry entry;
 };
 
@@ -168,6 +167,12 @@ static inline struct amdgpu_bo_real_reusable_slab *get_bo_from_slab(struct pb_sl
    return container_of(slab, struct amdgpu_bo_real_reusable_slab, slab);
 }
 
+static struct amdgpu_bo_real *get_slab_entry_real_bo(struct amdgpu_winsys_bo *bo)
+{
+   assert(bo->type == AMDGPU_BO_SLAB_ENTRY);
+   return &get_bo_from_slab(((struct amdgpu_bo_slab_entry*)bo)->entry.slab)->b.b;
+}
+
 bool amdgpu_bo_can_reclaim(struct amdgpu_winsys *ws, struct pb_buffer *_buf);
 struct pb_buffer *amdgpu_bo_create(struct amdgpu_winsys *ws,
                                    uint64_t size,
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
index 819c65d6c3053..05c9082fad563 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
@@ -641,7 +641,7 @@ amdgpu_lookup_or_add_slab_buffer(struct amdgpu_cs_context *cs, struct amdgpu_win
       return buffer;
 
    struct amdgpu_cs_buffer *real_buffer =
-      amdgpu_lookup_or_add_buffer(cs, &get_slab_entry_bo(bo)->real->b, AMDGPU_BO_REAL);
+      amdgpu_lookup_or_add_buffer(cs, &get_slab_entry_real_bo(bo)->b, AMDGPU_BO_REAL);
    if (!real_buffer)
       return NULL;
 
-- 
GitLab


From d4d2dd9d66d9763f32e2a1589bcb2c2f65938a9c Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 9 Dec 2023 14:23:49 -0500
Subject: [PATCH 13/36] winsys/amdgpu: remove va (gpu_address) from
 amdgpu_bo_slab_entry

Keep it only in amdgpu_bo_real and amdgpu_bo_sparse. Slab entries can
compute it from the slab BO and adding their entry index.

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c | 38 +++++++++++++++--------
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.h |  4 ++-
 src/gallium/winsys/amdgpu/drm/amdgpu_cs.c |  9 +++---
 src/gallium/winsys/amdgpu/drm/amdgpu_cs.h |  1 +
 4 files changed, 34 insertions(+), 18 deletions(-)

diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index c5204278f6f13..08c0ca198f601 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -162,7 +162,7 @@ void amdgpu_bo_destroy(struct amdgpu_winsys *ws, struct pb_buffer *_buf)
    _mesa_hash_table_remove_key(ws->bo_export_table, bo->bo);
 
    if (bo->b.base.placement & RADEON_DOMAIN_VRAM_GTT) {
-      amdgpu_bo_va_op(bo->bo, 0, bo->b.base.size, bo->b.va, 0, AMDGPU_VA_OP_UNMAP);
+      amdgpu_bo_va_op(bo->bo, 0, bo->b.base.size, bo->gpu_address, 0, AMDGPU_VA_OP_UNMAP);
       amdgpu_va_range_free(bo->va_handle);
    }
 
@@ -360,7 +360,7 @@ void *amdgpu_bo_map(struct radeon_winsys *rws,
       real = get_real_bo(bo);
    } else {
       real = get_slab_entry_real_bo(bo);
-      offset = bo->va - real->b.va;
+      offset = amdgpu_bo_get_va(buf) - real->gpu_address;
    }
 
    if (usage & RADEON_MAP_TEMPORARY) {
@@ -582,7 +582,7 @@ static struct amdgpu_winsys_bo *amdgpu_create_bo(struct amdgpu_winsys *ws,
    bo->b.base.alignment_log2 = util_logbase2(alignment);
    bo->b.base.usage = flags;
    bo->b.base.size = size;
-   bo->b.va = va;
+   bo->gpu_address = va;
    bo->b.unique_id = __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
    bo->bo = buf_handle;
    bo->va_handle = va_handle;
@@ -734,7 +734,6 @@ struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_s
       bo->b.base.alignment_log2 = util_logbase2(get_slab_entry_alignment(ws, entry_size));
       bo->b.base.size = entry_size;
       bo->b.type = AMDGPU_BO_SLAB_ENTRY;
-      bo->b.va = slab_bo->b.b.b.va + i * entry_size;
       bo->b.unique_id = base_id + i;
 
       bo->entry.slab = &slab_bo->slab;
@@ -1013,7 +1012,7 @@ static void amdgpu_bo_sparse_destroy(struct radeon_winsys *rws, struct pb_buffer
 
    r = amdgpu_bo_va_op_raw(ws->dev, NULL, 0,
                            (uint64_t)bo->num_va_pages * RADEON_SPARSE_PAGE_SIZE,
-                           bo->b.va, 0, AMDGPU_VA_OP_CLEAR);
+                           bo->gpu_address, 0, AMDGPU_VA_OP_CLEAR);
    if (r) {
       fprintf(stderr, "amdgpu: clearing PRT VA region on destroy failed (%d)\n", r);
    }
@@ -1072,12 +1071,12 @@ amdgpu_bo_sparse_create(struct amdgpu_winsys *ws, uint64_t size,
    va_gap_size = ws->check_vm ? 4 * RADEON_SPARSE_PAGE_SIZE : 0;
    r = amdgpu_va_range_alloc(ws->dev, amdgpu_gpu_va_range_general,
                              map_size + va_gap_size, RADEON_SPARSE_PAGE_SIZE,
-                             0, &bo->b.va, &bo->va_handle,
+                             0, &bo->gpu_address, &bo->va_handle,
 			     AMDGPU_VA_RANGE_HIGH);
    if (r)
       goto error_va_alloc;
 
-   r = amdgpu_bo_va_op_raw(ws->dev, NULL, 0, map_size, bo->b.va,
+   r = amdgpu_bo_va_op_raw(ws->dev, NULL, 0, map_size, bo->gpu_address,
                            AMDGPU_VM_PAGE_PRT, AMDGPU_VA_OP_MAP);
    if (r)
       goto error_va_map;
@@ -1150,7 +1149,7 @@ amdgpu_bo_sparse_commit(struct radeon_winsys *rws, struct pb_buffer *buf,
             r = amdgpu_bo_va_op_raw(ws->dev, backing->bo->bo,
                                     (uint64_t)backing_start * RADEON_SPARSE_PAGE_SIZE,
                                     (uint64_t)backing_size * RADEON_SPARSE_PAGE_SIZE,
-                                    bo->b.va + (uint64_t)span_va_page * RADEON_SPARSE_PAGE_SIZE,
+                                    bo->gpu_address + (uint64_t)span_va_page * RADEON_SPARSE_PAGE_SIZE,
                                     AMDGPU_VM_PAGE_READABLE |
                                     AMDGPU_VM_PAGE_WRITEABLE |
                                     AMDGPU_VM_PAGE_EXECUTABLE,
@@ -1175,7 +1174,7 @@ amdgpu_bo_sparse_commit(struct radeon_winsys *rws, struct pb_buffer *buf,
    } else {
       r = amdgpu_bo_va_op_raw(ws->dev, NULL, 0,
                               (uint64_t)(end_va_page - va_page) * RADEON_SPARSE_PAGE_SIZE,
-                              bo->b.va + (uint64_t)va_page * RADEON_SPARSE_PAGE_SIZE,
+                              bo->gpu_address + (uint64_t)va_page * RADEON_SPARSE_PAGE_SIZE,
                               AMDGPU_VM_PAGE_PRT, AMDGPU_VA_OP_REPLACE);
       if (r) {
          ok = false;
@@ -1557,7 +1556,7 @@ static struct pb_buffer *amdgpu_bo_from_handle(struct radeon_winsys *rws,
    bo->b.base.usage = flags;
    bo->b.base.size = result.alloc_size;
    bo->b.type = AMDGPU_BO_REAL;
-   bo->b.va = va;
+   bo->gpu_address = va;
    bo->b.unique_id = __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
    simple_mtx_init(&bo->lock, mtx_plain);
    bo->bo = result.buf_handle;
@@ -1711,7 +1710,7 @@ static struct pb_buffer *amdgpu_bo_from_ptr(struct radeon_winsys *rws,
     bo->b.base.alignment_log2 = 0;
     bo->b.base.size = size;
     bo->b.type = AMDGPU_BO_REAL;
-    bo->b.va = va;
+    bo->gpu_address = va;
     bo->b.unique_id = __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
     simple_mtx_init(&bo->lock, mtx_plain);
     bo->bo = buf_handle;
@@ -1751,9 +1750,22 @@ static bool amdgpu_bo_is_suballocated(struct pb_buffer *buf)
    return bo->type == AMDGPU_BO_SLAB_ENTRY;
 }
 
-static uint64_t amdgpu_bo_get_va(struct pb_buffer *buf)
+uint64_t amdgpu_bo_get_va(struct pb_buffer *buf)
 {
-   return ((struct amdgpu_winsys_bo*)buf)->va;
+   struct amdgpu_winsys_bo *bo = amdgpu_winsys_bo(buf);
+
+   if (bo->type == AMDGPU_BO_SLAB_ENTRY) {
+      struct amdgpu_bo_slab_entry *slab_entry_bo = get_slab_entry_bo(bo);
+      struct amdgpu_bo_real_reusable_slab *slab_bo =
+         (struct amdgpu_bo_real_reusable_slab *)get_slab_entry_real_bo(bo);
+      unsigned entry_index = slab_entry_bo - slab_bo->entries;
+
+      return slab_bo->b.b.gpu_address + slab_bo->slab.entry_size * entry_index;
+   } else if (bo->type == AMDGPU_BO_SPARSE) {
+      return get_sparse_bo(bo)->gpu_address;
+   } else {
+      return get_real_bo(bo)->gpu_address;
+   }
 }
 
 static void amdgpu_buffer_destroy(struct radeon_winsys *ws, struct pb_buffer *buf)
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
index be7c3d1bbe28c..4b3ae7b0bcbd6 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
@@ -52,7 +52,6 @@ struct amdgpu_winsys_bo {
    enum amdgpu_bo_type type;
 
    uint32_t unique_id;
-   uint64_t va;
 
    /* how many command streams, which are being emitted in a separate
     * thread, is this bo referenced in? */
@@ -75,6 +74,7 @@ struct amdgpu_bo_real {
 
    amdgpu_bo_handle bo;
    amdgpu_va_handle va_handle;
+   uint64_t gpu_address;
    void *cpu_ptr; /* for user_ptr and permanent maps */
    int map_count;
    uint32_t kms_handle;
@@ -105,6 +105,7 @@ struct amdgpu_bo_real_reusable {
 struct amdgpu_bo_sparse {
    struct amdgpu_winsys_bo b;
    amdgpu_va_handle va_handle;
+   uint64_t gpu_address;
 
    uint32_t num_va_pages;
    uint32_t num_backing_pages;
@@ -191,6 +192,7 @@ bool amdgpu_bo_can_reclaim_slab(void *priv, struct pb_slab_entry *entry);
 struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_size,
                                      unsigned group_index);
 void amdgpu_bo_slab_free(struct amdgpu_winsys *ws, struct pb_slab *slab);
+uint64_t amdgpu_bo_get_va(struct pb_buffer *buf);
 
 static inline
 struct amdgpu_winsys_bo *amdgpu_winsys_bo(struct pb_buffer *bo)
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
index 05c9082fad563..de856ad799c1a 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
@@ -755,6 +755,7 @@ static bool amdgpu_ib_new_buffer(struct amdgpu_winsys *ws,
    radeon_bo_reference(&ws->dummy_ws.base, &main_ib->big_buffer, pb);
    radeon_bo_reference(&ws->dummy_ws.base, &pb, NULL);
 
+   main_ib->gpu_address = amdgpu_bo_get_va(main_ib->big_buffer);
    main_ib->big_buffer_cpu_ptr = mapped;
    main_ib->used_ib_space = 0;
 
@@ -799,7 +800,7 @@ static bool amdgpu_get_new_ib(struct amdgpu_winsys *ws,
          return false;
    }
 
-   chunk_ib->va_start = amdgpu_winsys_bo(main_ib->big_buffer)->va + main_ib->used_ib_space;
+   chunk_ib->va_start = main_ib->gpu_address + main_ib->used_ib_space;
    chunk_ib->ib_bytes = 0;
    /* ib_bytes is in dwords and the conversion to bytes will be done before
     * the CS ioctl. */
@@ -1052,7 +1053,7 @@ amdgpu_cs_setup_preemption(struct radeon_cmdbuf *rcs, const uint32_t *preamble_i
    amdgpu_bo_unmap(&ws->dummy_ws.base, preamble_bo);
 
    for (unsigned i = 0; i < 2; i++) {
-      csc[i]->chunk_ib[IB_PREAMBLE].va_start = amdgpu_winsys_bo(preamble_bo)->va;
+      csc[i]->chunk_ib[IB_PREAMBLE].va_start = amdgpu_bo_get_va(preamble_bo);
       csc[i]->chunk_ib[IB_PREAMBLE].ib_bytes = preamble_num_dw * 4;
 
       csc[i]->chunk_ib[IB_MAIN].flags |= AMDGPU_IB_FLAG_PREEMPT;
@@ -1115,7 +1116,7 @@ static bool amdgpu_cs_check_space(struct radeon_cmdbuf *rcs, unsigned dw)
       return false;
 
    assert(main_ib->used_ib_space == 0);
-   uint64_t va = amdgpu_winsys_bo(main_ib->big_buffer)->va;
+   uint64_t va = main_ib->gpu_address;
 
    /* This space was originally reserved. */
    rcs->current.max_dw += cs_epilog_dw;
@@ -1163,7 +1164,7 @@ static unsigned amdgpu_cs_get_buffer_list(struct radeon_cmdbuf *rcs,
     if (list) {
         for (unsigned i = 0; i < num_real_buffers; i++) {
             list[i].bo_size = real_buffers->buffers[i].bo->base.size;
-            list[i].vm_address = real_buffers->buffers[i].bo->va;
+            list[i].vm_address = get_real_bo(real_buffers->buffers[i].bo)->gpu_address;
             list[i].priority_usage = real_buffers->buffers[i].usage;
         }
     }
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h
index df222d6fecab2..67d7d97dd7c08 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h
@@ -50,6 +50,7 @@ struct amdgpu_ib {
    /* A buffer out of which new IBs are allocated. */
    struct pb_buffer        *big_buffer;
    uint8_t                 *big_buffer_cpu_ptr;
+   uint64_t                gpu_address;
    unsigned                used_ib_space;
 
    /* The maximum seen size from cs_check_space. If the driver does
-- 
GitLab


From 570c9f4e1af8481ec06d50963246f1273c93a063 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Thu, 21 Dec 2023 00:58:19 -0500
Subject: [PATCH 14/36] winsys/amdgpu: don't use gpu_address to compute slab
 entry offset in bo_map

use the code we have in amdgpu_bo_get_va

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c | 16 ++++++++++++----
 1 file changed, 12 insertions(+), 4 deletions(-)

diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index 08c0ca198f601..7cb9b3cfb93de 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -124,6 +124,16 @@ static bool amdgpu_bo_wait(struct radeon_winsys *rws,
    }
 }
 
+static inline unsigned get_slab_entry_offset(struct amdgpu_winsys_bo *bo)
+{
+   struct amdgpu_bo_slab_entry *slab_entry_bo = get_slab_entry_bo(bo);
+   struct amdgpu_bo_real_reusable_slab *slab_bo =
+      (struct amdgpu_bo_real_reusable_slab *)get_slab_entry_real_bo(bo);
+   unsigned entry_index = slab_entry_bo - slab_bo->entries;
+
+   return slab_bo->slab.entry_size * entry_index;
+}
+
 static enum radeon_bo_domain amdgpu_bo_get_initial_domain(
       struct pb_buffer *buf)
 {
@@ -360,7 +370,7 @@ void *amdgpu_bo_map(struct radeon_winsys *rws,
       real = get_real_bo(bo);
    } else {
       real = get_slab_entry_real_bo(bo);
-      offset = amdgpu_bo_get_va(buf) - real->gpu_address;
+      offset = get_slab_entry_offset(bo);
    }
 
    if (usage & RADEON_MAP_TEMPORARY) {
@@ -1755,12 +1765,10 @@ uint64_t amdgpu_bo_get_va(struct pb_buffer *buf)
    struct amdgpu_winsys_bo *bo = amdgpu_winsys_bo(buf);
 
    if (bo->type == AMDGPU_BO_SLAB_ENTRY) {
-      struct amdgpu_bo_slab_entry *slab_entry_bo = get_slab_entry_bo(bo);
       struct amdgpu_bo_real_reusable_slab *slab_bo =
          (struct amdgpu_bo_real_reusable_slab *)get_slab_entry_real_bo(bo);
-      unsigned entry_index = slab_entry_bo - slab_bo->entries;
 
-      return slab_bo->b.b.gpu_address + slab_bo->slab.entry_size * entry_index;
+      return slab_bo->b.b.gpu_address + get_slab_entry_offset(bo);
    } else if (bo->type == AMDGPU_BO_SPARSE) {
       return get_sparse_bo(bo)->gpu_address;
    } else {
-- 
GitLab

From 77124d5241785eebd9deb217d506ae494facc346 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Mon, 25 Dec 2023 04:41:01 -0500
Subject: [PATCH 16/36] meson: require libdrm_amdgpu 2.4.119

---
 meson.build | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/meson.build b/meson.build
index 309e7efb8e7f4..55f1854c67d23 100644
--- a/meson.build
+++ b/meson.build
@@ -1625,7 +1625,7 @@ dep_libdrm_radeon = null_dep
 dep_libdrm_nouveau = null_dep
 dep_libdrm_intel = null_dep
 
-_drm_amdgpu_ver = '2.4.110'
+_drm_amdgpu_ver = '2.4.119'
 _drm_radeon_ver = '2.4.71'
 _drm_nouveau_ver = '2.4.102'
 _drm_intel_ver = '2.4.75'
-- 
GitLab


From 589178c1daf296d028c25c21f588c87cdd254900 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Thu, 21 Dec 2023 01:00:07 -0500
Subject: [PATCH 17/36] winsys/amdgpu: remove amdgpu_bo_real::gpu_address, use
 amdgpu_va_get_start_addr

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c | 10 ++++------
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.h |  1 -
 src/gallium/winsys/amdgpu/drm/amdgpu_cs.c |  3 ++-
 3 files changed, 6 insertions(+), 8 deletions(-)

diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index 7cb9b3cfb93de..efade38c6a4ae 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -172,7 +172,8 @@ void amdgpu_bo_destroy(struct amdgpu_winsys *ws, struct pb_buffer *_buf)
    _mesa_hash_table_remove_key(ws->bo_export_table, bo->bo);
 
    if (bo->b.base.placement & RADEON_DOMAIN_VRAM_GTT) {
-      amdgpu_bo_va_op(bo->bo, 0, bo->b.base.size, bo->gpu_address, 0, AMDGPU_VA_OP_UNMAP);
+      amdgpu_bo_va_op(bo->bo, 0, bo->b.base.size,
+                      amdgpu_va_get_start_addr(bo->va_handle), 0, AMDGPU_VA_OP_UNMAP);
       amdgpu_va_range_free(bo->va_handle);
    }
 
@@ -592,7 +593,6 @@ static struct amdgpu_winsys_bo *amdgpu_create_bo(struct amdgpu_winsys *ws,
    bo->b.base.alignment_log2 = util_logbase2(alignment);
    bo->b.base.usage = flags;
    bo->b.base.size = size;
-   bo->gpu_address = va;
    bo->b.unique_id = __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
    bo->bo = buf_handle;
    bo->va_handle = va_handle;
@@ -1566,7 +1566,6 @@ static struct pb_buffer *amdgpu_bo_from_handle(struct radeon_winsys *rws,
    bo->b.base.usage = flags;
    bo->b.base.size = result.alloc_size;
    bo->b.type = AMDGPU_BO_REAL;
-   bo->gpu_address = va;
    bo->b.unique_id = __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
    simple_mtx_init(&bo->lock, mtx_plain);
    bo->bo = result.buf_handle;
@@ -1720,7 +1719,6 @@ static struct pb_buffer *amdgpu_bo_from_ptr(struct radeon_winsys *rws,
     bo->b.base.alignment_log2 = 0;
     bo->b.base.size = size;
     bo->b.type = AMDGPU_BO_REAL;
-    bo->gpu_address = va;
     bo->b.unique_id = __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
     simple_mtx_init(&bo->lock, mtx_plain);
     bo->bo = buf_handle;
@@ -1768,11 +1766,11 @@ uint64_t amdgpu_bo_get_va(struct pb_buffer *buf)
       struct amdgpu_bo_real_reusable_slab *slab_bo =
          (struct amdgpu_bo_real_reusable_slab *)get_slab_entry_real_bo(bo);
 
-      return slab_bo->b.b.gpu_address + get_slab_entry_offset(bo);
+      return amdgpu_va_get_start_addr(slab_bo->b.b.va_handle) + get_slab_entry_offset(bo);
    } else if (bo->type == AMDGPU_BO_SPARSE) {
       return get_sparse_bo(bo)->gpu_address;
    } else {
-      return get_real_bo(bo)->gpu_address;
+      return amdgpu_va_get_start_addr(get_real_bo(bo)->va_handle);
    }
 }
 
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
index 4b3ae7b0bcbd6..0e268af0d7313 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
@@ -74,7 +74,6 @@ struct amdgpu_bo_real {
 
    amdgpu_bo_handle bo;
    amdgpu_va_handle va_handle;
-   uint64_t gpu_address;
    void *cpu_ptr; /* for user_ptr and permanent maps */
    int map_count;
    uint32_t kms_handle;
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
index de856ad799c1a..5625c7963d97a 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
@@ -1164,7 +1164,8 @@ static unsigned amdgpu_cs_get_buffer_list(struct radeon_cmdbuf *rcs,
     if (list) {
         for (unsigned i = 0; i < num_real_buffers; i++) {
             list[i].bo_size = real_buffers->buffers[i].bo->base.size;
-            list[i].vm_address = get_real_bo(real_buffers->buffers[i].bo)->gpu_address;
+            list[i].vm_address =
+               amdgpu_va_get_start_addr(get_real_bo(real_buffers->buffers[i].bo)->va_handle);
             list[i].priority_usage = real_buffers->buffers[i].usage;
         }
     }
-- 
GitLab


From 9be10653ecc94a4e8555ae30e7734f5c605da830 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Thu, 21 Dec 2023 01:00:07 -0500
Subject: [PATCH 18/36] winsys/amdgpu: remove amdgpu_bo_sparse::gpu_address,
 use amdgpu_va_get_start_addr

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c | 17 ++++++++++-------
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.h |  1 -
 2 files changed, 10 insertions(+), 8 deletions(-)

diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index efade38c6a4ae..1ca0cbfed1436 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -1022,7 +1022,7 @@ static void amdgpu_bo_sparse_destroy(struct radeon_winsys *rws, struct pb_buffer
 
    r = amdgpu_bo_va_op_raw(ws->dev, NULL, 0,
                            (uint64_t)bo->num_va_pages * RADEON_SPARSE_PAGE_SIZE,
-                           bo->gpu_address, 0, AMDGPU_VA_OP_CLEAR);
+                           amdgpu_va_get_start_addr(bo->va_handle), 0, AMDGPU_VA_OP_CLEAR);
    if (r) {
       fprintf(stderr, "amdgpu: clearing PRT VA region on destroy failed (%d)\n", r);
    }
@@ -1079,14 +1079,15 @@ amdgpu_bo_sparse_create(struct amdgpu_winsys *ws, uint64_t size,
    /* For simplicity, we always map a multiple of the page size. */
    map_size = align64(size, RADEON_SPARSE_PAGE_SIZE);
    va_gap_size = ws->check_vm ? 4 * RADEON_SPARSE_PAGE_SIZE : 0;
+
+   uint64_t gpu_address;
    r = amdgpu_va_range_alloc(ws->dev, amdgpu_gpu_va_range_general,
                              map_size + va_gap_size, RADEON_SPARSE_PAGE_SIZE,
-                             0, &bo->gpu_address, &bo->va_handle,
-			     AMDGPU_VA_RANGE_HIGH);
+                             0, &gpu_address, &bo->va_handle, AMDGPU_VA_RANGE_HIGH);
    if (r)
       goto error_va_alloc;
 
-   r = amdgpu_bo_va_op_raw(ws->dev, NULL, 0, map_size, bo->gpu_address,
+   r = amdgpu_bo_va_op_raw(ws->dev, NULL, 0, map_size, gpu_address,
                            AMDGPU_VM_PAGE_PRT, AMDGPU_VA_OP_MAP);
    if (r)
       goto error_va_map;
@@ -1159,7 +1160,8 @@ amdgpu_bo_sparse_commit(struct radeon_winsys *rws, struct pb_buffer *buf,
             r = amdgpu_bo_va_op_raw(ws->dev, backing->bo->bo,
                                     (uint64_t)backing_start * RADEON_SPARSE_PAGE_SIZE,
                                     (uint64_t)backing_size * RADEON_SPARSE_PAGE_SIZE,
-                                    bo->gpu_address + (uint64_t)span_va_page * RADEON_SPARSE_PAGE_SIZE,
+                                    amdgpu_va_get_start_addr(bo->va_handle) +
+                                    (uint64_t)span_va_page * RADEON_SPARSE_PAGE_SIZE,
                                     AMDGPU_VM_PAGE_READABLE |
                                     AMDGPU_VM_PAGE_WRITEABLE |
                                     AMDGPU_VM_PAGE_EXECUTABLE,
@@ -1184,7 +1186,8 @@ amdgpu_bo_sparse_commit(struct radeon_winsys *rws, struct pb_buffer *buf,
    } else {
       r = amdgpu_bo_va_op_raw(ws->dev, NULL, 0,
                               (uint64_t)(end_va_page - va_page) * RADEON_SPARSE_PAGE_SIZE,
-                              bo->gpu_address + (uint64_t)va_page * RADEON_SPARSE_PAGE_SIZE,
+                              amdgpu_va_get_start_addr(bo->va_handle) +
+                              (uint64_t)va_page * RADEON_SPARSE_PAGE_SIZE,
                               AMDGPU_VM_PAGE_PRT, AMDGPU_VA_OP_REPLACE);
       if (r) {
          ok = false;
@@ -1768,7 +1771,7 @@ uint64_t amdgpu_bo_get_va(struct pb_buffer *buf)
 
       return amdgpu_va_get_start_addr(slab_bo->b.b.va_handle) + get_slab_entry_offset(bo);
    } else if (bo->type == AMDGPU_BO_SPARSE) {
-      return get_sparse_bo(bo)->gpu_address;
+      return amdgpu_va_get_start_addr(get_sparse_bo(bo)->va_handle);
    } else {
       return amdgpu_va_get_start_addr(get_real_bo(bo)->va_handle);
    }
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
index 0e268af0d7313..af7213e7fa253 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
@@ -104,7 +104,6 @@ struct amdgpu_bo_real_reusable {
 struct amdgpu_bo_sparse {
    struct amdgpu_winsys_bo b;
    amdgpu_va_handle va_handle;
-   uint64_t gpu_address;
 
    uint32_t num_va_pages;
    uint32_t num_backing_pages;
-- 
GitLab


From b08ffab7f5608c45a34960d6fefaa52475668ecc Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 9 Dec 2023 15:13:38 -0500
Subject: [PATCH 19/36] gallium/pb_buffer: define pb_buffer_lean without vtbl,
 inherit it by pb_buffer

amdgpu doesn't need vtbl.

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/auxiliary/pipebuffer/pb_buffer.h  |  25 ++--
 .../auxiliary/pipebuffer/pb_buffer_fenced.c   |  32 ++---
 .../auxiliary/pipebuffer/pb_bufmgr_cache.c    |  16 +--
 .../auxiliary/pipebuffer/pb_bufmgr_debug.c    |  30 ++--
 .../auxiliary/pipebuffer/pb_bufmgr_mm.c       |  10 +-
 .../auxiliary/pipebuffer/pb_bufmgr_slab.c     |  18 +--
 src/gallium/auxiliary/pipebuffer/pb_cache.c   |  22 +--
 src/gallium/drivers/d3d12/d3d12_bufmgr.cpp    |  10 +-
 src/gallium/drivers/d3d12/d3d12_bufmgr.h      |   2 +-
 src/gallium/drivers/r300/r300_emit.c          |   4 +-
 src/gallium/drivers/r300/r300_render.c        |   4 +-
 src/gallium/drivers/r300/r300_texture_desc.c  |   6 +-
 src/gallium/drivers/r600/r600_buffer_common.c |   4 +-
 src/gallium/drivers/r600/r600_state.c         |   4 +-
 src/gallium/drivers/r600/r600_texture.c       |  12 +-
 src/gallium/drivers/r600/radeon_uvd.c         |   4 +-
 src/gallium/drivers/r600/radeon_video.c       |  10 +-
 src/gallium/drivers/radeonsi/radeon_uvd.c     |   8 +-
 src/gallium/drivers/radeonsi/radeon_vce_50.c  |   2 +-
 src/gallium/drivers/radeonsi/radeon_vce_52.c  |   2 +-
 src/gallium/drivers/radeonsi/radeon_vcn_dec.c |  12 +-
 src/gallium/drivers/radeonsi/radeon_vcn_enc.c |   2 +-
 src/gallium/drivers/radeonsi/radeon_video.c   |   2 +-
 src/gallium/drivers/radeonsi/si_buffer.c      |   8 +-
 src/gallium/drivers/radeonsi/si_texture.c     |  10 +-
 src/gallium/drivers/zink/zink_bo.c            |  76 +++++-----
 src/gallium/drivers/zink/zink_bo.h            |   2 +-
 src/gallium/drivers/zink/zink_resource.c      |   6 +-
 src/gallium/include/winsys/radeon_winsys.h    |   2 +-
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c     | 136 +++++++++---------
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.h     |   2 +-
 src/gallium/winsys/amdgpu/drm/amdgpu_cs.c     |   8 +-
 src/gallium/winsys/radeon/drm/radeon_drm_bo.c |  70 ++++-----
 src/gallium/winsys/radeon/drm/radeon_drm_cs.c |   6 +-
 .../winsys/svga/drm/pb_buffer_simple_fenced.c |  32 ++---
 src/gallium/winsys/svga/drm/vmw_buffer.c      |  12 +-
 src/gallium/winsys/svga/drm/vmw_context.c     |   4 +-
 37 files changed, 312 insertions(+), 303 deletions(-)

diff --git a/src/gallium/auxiliary/pipebuffer/pb_buffer.h b/src/gallium/auxiliary/pipebuffer/pb_buffer.h
index ea153f219a4e2..e9aeb51890450 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_buffer.h
+++ b/src/gallium/auxiliary/pipebuffer/pb_buffer.h
@@ -106,9 +106,9 @@ typedef uint64_t pb_size;
 
 
 /**
- * Base class for all pb_* buffers.
+ * Base class for all pb_* buffers without the vtbl pointer.
  */
-struct pb_buffer
+struct pb_buffer_lean
 {
    struct pipe_reference  reference;
 
@@ -129,6 +129,15 @@ struct pb_buffer
    uint16_t               usage;
 
    pb_size                size;
+};
+
+
+/**
+ * Base class for all pb_* buffers with the vtbl pointer.
+ */
+struct pb_buffer
+{
+   struct pb_buffer_lean base;
 
    /**
     * Pointer to the virtual function table.
@@ -191,7 +200,7 @@ pb_map(struct pb_buffer *buf, enum pb_usage_flags flags, void *flush_ctx)
    assert(buf);
    if (!buf)
       return NULL;
-   assert(pipe_is_referenced(&buf->reference));
+   assert(pipe_is_referenced(&buf->base.reference));
    return buf->vtbl->map(buf, flags, flush_ctx);
 }
 
@@ -202,7 +211,7 @@ pb_unmap(struct pb_buffer *buf)
    assert(buf);
    if (!buf)
       return;
-   assert(pipe_is_referenced(&buf->reference));
+   assert(pipe_is_referenced(&buf->base.reference));
    buf->vtbl->unmap(buf);
 }
 
@@ -218,11 +227,11 @@ pb_get_base_buffer(struct pb_buffer *buf,
       offset = NULL;
       return;
    }
-   assert(pipe_is_referenced(&buf->reference));
+   assert(pipe_is_referenced(&buf->base.reference));
    assert(buf->vtbl->get_base_buffer);
    buf->vtbl->get_base_buffer(buf, base_buf, offset);
    assert(*base_buf);
-   assert(*offset < (*base_buf)->size);
+   assert(*offset < (*base_buf)->base.size);
 }
 
 
@@ -270,7 +279,7 @@ pb_reference(struct pb_buffer **dst,
 {
    struct pb_buffer *old = *dst;
 
-   if (pipe_reference(&(*dst)->reference, &src->reference))
+   if (pipe_reference(&(*dst)->base.reference, &src->base.reference))
       pb_destroy(NULL, old);
    *dst = src;
 }
@@ -282,7 +291,7 @@ pb_reference_with_winsys(void *winsys,
 {
    struct pb_buffer *old = *dst;
 
-   if (pipe_reference(&(*dst)->reference, &src->reference))
+   if (pipe_reference(&(*dst)->base.reference, &src->base.reference))
       pb_destroy(winsys, old);
    *dst = src;
 }
diff --git a/src/gallium/auxiliary/pipebuffer/pb_buffer_fenced.c b/src/gallium/auxiliary/pipebuffer/pb_buffer_fenced.c
index 8e8c861a300f9..055fb3f9cbc9d 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_buffer_fenced.c
+++ b/src/gallium/auxiliary/pipebuffer/pb_buffer_fenced.c
@@ -211,8 +211,8 @@ fenced_manager_dump_locked(struct fenced_manager *fenced_mgr)
       assert(!fenced_buf->fence);
       debug_printf("%10p %"PRIu64" %8u %7s\n",
                    (void *) fenced_buf,
-                   fenced_buf->base.size,
-                   p_atomic_read(&fenced_buf->base.reference.count),
+                   fenced_buf->base.base.size,
+                   p_atomic_read(&fenced_buf->base.base.reference.count),
                    fenced_buf->buffer ? "gpu" : (fenced_buf->data ? "cpu" : "none"));
       curr = next;
       next = curr->next;
@@ -227,8 +227,8 @@ fenced_manager_dump_locked(struct fenced_manager *fenced_mgr)
       signaled = ops->fence_signalled(ops, fenced_buf->fence, 0);
       debug_printf("%10p %"PRIu64" %8u %7s %10p %s\n",
                    (void *) fenced_buf,
-                   fenced_buf->base.size,
-                   p_atomic_read(&fenced_buf->base.reference.count),
+                   fenced_buf->base.base.size,
+                   p_atomic_read(&fenced_buf->base.base.reference.count),
                    "gpu",
                    (void *) fenced_buf->fence,
                    signaled == 0 ? "y" : "n");
@@ -245,7 +245,7 @@ static inline void
 fenced_buffer_destroy_locked(struct fenced_manager *fenced_mgr,
                              struct fenced_buffer *fenced_buf)
 {
-   assert(!pipe_is_referenced(&fenced_buf->base.reference));
+   assert(!pipe_is_referenced(&fenced_buf->base.base.reference));
 
    assert(!fenced_buf->fence);
    assert(fenced_buf->head.prev);
@@ -270,11 +270,11 @@ static inline void
 fenced_buffer_add_locked(struct fenced_manager *fenced_mgr,
                          struct fenced_buffer *fenced_buf)
 {
-   assert(pipe_is_referenced(&fenced_buf->base.reference));
+   assert(pipe_is_referenced(&fenced_buf->base.base.reference));
    assert(fenced_buf->flags & PB_USAGE_GPU_READ_WRITE);
    assert(fenced_buf->fence);
 
-   p_atomic_inc(&fenced_buf->base.reference.count);
+   p_atomic_inc(&fenced_buf->base.base.reference.count);
 
    list_del(&fenced_buf->head);
    assert(fenced_mgr->num_unfenced);
@@ -312,7 +312,7 @@ fenced_buffer_remove_locked(struct fenced_manager *fenced_mgr,
    list_addtail(&fenced_buf->head, &fenced_mgr->unfenced);
    ++fenced_mgr->num_unfenced;
 
-   if (p_atomic_dec_zero(&fenced_buf->base.reference.count)) {
+   if (p_atomic_dec_zero(&fenced_buf->base.base.reference.count)) {
       fenced_buffer_destroy_locked(fenced_mgr, fenced_buf);
       return true;
    }
@@ -338,7 +338,7 @@ fenced_buffer_finish_locked(struct fenced_manager *fenced_mgr,
    debug_warning("waiting for GPU");
 #endif
 
-   assert(pipe_is_referenced(&fenced_buf->base.reference));
+   assert(pipe_is_referenced(&fenced_buf->base.base.reference));
    assert(fenced_buf->fence);
 
    if (fenced_buf->fence) {
@@ -354,7 +354,7 @@ fenced_buffer_finish_locked(struct fenced_manager *fenced_mgr,
 
       mtx_lock(&fenced_mgr->mutex);
 
-      assert(pipe_is_referenced(&fenced_buf->base.reference));
+      assert(pipe_is_referenced(&fenced_buf->base.base.reference));
 
       /* Only proceed if the fence object didn't change in the meanwhile.
        * Otherwise assume the work has been already carried out by another
@@ -650,7 +650,7 @@ fenced_buffer_destroy(void *winsys, struct pb_buffer *buf)
    struct fenced_buffer *fenced_buf = fenced_buffer(buf);
    struct fenced_manager *fenced_mgr = fenced_buf->mgr;
 
-   assert(!pipe_is_referenced(&fenced_buf->base.reference));
+   assert(!pipe_is_referenced(&fenced_buf->base.base.reference));
 
    mtx_lock(&fenced_mgr->mutex);
 
@@ -818,7 +818,7 @@ fenced_buffer_fence(struct pb_buffer *buf,
 
    mtx_lock(&fenced_mgr->mutex);
 
-   assert(pipe_is_referenced(&fenced_buf->base.reference));
+   assert(pipe_is_referenced(&fenced_buf->base.base.reference));
    assert(fenced_buf->buffer);
 
    if (fence != fenced_buf->fence) {
@@ -907,10 +907,10 @@ fenced_bufmgr_create_buffer(struct pb_manager *mgr,
    if (!fenced_buf)
       goto no_buffer;
 
-   pipe_reference_init(&fenced_buf->base.reference, 1);
-   fenced_buf->base.alignment_log2 = util_logbase2(desc->alignment);
-   fenced_buf->base.usage = desc->usage;
-   fenced_buf->base.size = size;
+   pipe_reference_init(&fenced_buf->base.base.reference, 1);
+   fenced_buf->base.base.alignment_log2 = util_logbase2(desc->alignment);
+   fenced_buf->base.base.usage = desc->usage;
+   fenced_buf->base.base.size = size;
    fenced_buf->size = size;
    fenced_buf->desc = *desc;
 
diff --git a/src/gallium/auxiliary/pipebuffer/pb_bufmgr_cache.c b/src/gallium/auxiliary/pipebuffer/pb_bufmgr_cache.c
index 040f1c5b52feb..8bf121f284228 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_bufmgr_cache.c
+++ b/src/gallium/auxiliary/pipebuffer/pb_bufmgr_cache.c
@@ -98,7 +98,7 @@ _pb_cache_buffer_destroy(void *winsys, struct pb_buffer *pb_buf)
 {
    struct pb_cache_buffer *buf = pb_cache_buffer(pb_buf);
 
-   assert(!pipe_is_referenced(&buf->base.reference));
+   assert(!pipe_is_referenced(&buf->base.base.reference));
    pb_reference(&buf->buffer, NULL);
    FREE(buf);
 }
@@ -233,14 +233,14 @@ pb_cache_manager_create_buffer(struct pb_manager *_mgr,
       return NULL;
    }
    
-   assert(pipe_is_referenced(&buf->buffer->reference));
-   assert(pb_check_alignment(desc->alignment, 1u << buf->buffer->alignment_log2));
-   assert(buf->buffer->size >= aligned_size);
+   assert(pipe_is_referenced(&buf->buffer->base.reference));
+   assert(pb_check_alignment(desc->alignment, 1u << buf->buffer->base.alignment_log2));
+   assert(buf->buffer->base.size >= aligned_size);
    
-   pipe_reference_init(&buf->base.reference, 1);
-   buf->base.alignment_log2 = buf->buffer->alignment_log2;
-   buf->base.usage = buf->buffer->usage;
-   buf->base.size = buf->buffer->size;
+   pipe_reference_init(&buf->base.base.reference, 1);
+   buf->base.base.alignment_log2 = buf->buffer->base.alignment_log2;
+   buf->base.base.usage = buf->buffer->base.usage;
+   buf->base.base.size = buf->buffer->base.size;
    
    buf->base.vtbl = &pb_cache_buffer_vtbl;
    buf->mgr = mgr;
diff --git a/src/gallium/auxiliary/pipebuffer/pb_bufmgr_debug.c b/src/gallium/auxiliary/pipebuffer/pb_bufmgr_debug.c
index 61b80f5a96c19..75d8af2b7a893 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_bufmgr_debug.c
+++ b/src/gallium/auxiliary/pipebuffer/pb_bufmgr_debug.c
@@ -162,7 +162,7 @@ pb_debug_buffer_fill(struct pb_debug_buffer *buf)
    assert(map);
    if (map) {
       fill_random_pattern(map, buf->underflow_size);
-      fill_random_pattern(map + buf->underflow_size + buf->base.size,
+      fill_random_pattern(map + buf->underflow_size + buf->base.base.size,
                           buf->overflow_size);
       pb_unmap(buf->buffer);
    }
@@ -196,12 +196,12 @@ pb_debug_buffer_check(struct pb_debug_buffer *buf)
                       buf->underflow_size - max_ofs);
       }
       
-      overflow = !check_random_pattern(map + buf->underflow_size + buf->base.size,
+      overflow = !check_random_pattern(map + buf->underflow_size + buf->base.base.size,
                                        buf->overflow_size, 
                                        &min_ofs, &max_ofs);
       if(overflow) {
          debug_printf("buffer overflow (size %"PRIu64" plus offset %"PRIu64" to %"PRIu64"%s bytes) detected\n",
-                      buf->base.size,
+                      buf->base.base.size,
                       min_ofs,
                       max_ofs,
                       max_ofs == buf->overflow_size - 1 ? "+" : "");
@@ -217,7 +217,7 @@ pb_debug_buffer_check(struct pb_debug_buffer *buf)
       if(underflow)
          fill_random_pattern(map, buf->underflow_size);
       if(overflow)
-         fill_random_pattern(map + buf->underflow_size + buf->base.size,
+         fill_random_pattern(map + buf->underflow_size + buf->base.base.size,
                              buf->overflow_size);
 
       pb_unmap(buf->buffer);
@@ -231,7 +231,7 @@ pb_debug_buffer_destroy(void *winsys, struct pb_buffer *_buf)
    struct pb_debug_buffer *buf = pb_debug_buffer(_buf);
    struct pb_debug_manager *mgr = buf->mgr;
    
-   assert(!pipe_is_referenced(&buf->base.reference));
+   assert(!pipe_is_referenced(&buf->base.base.reference));
    
    pb_debug_buffer_check(buf);
 
@@ -351,7 +351,7 @@ pb_debug_manager_dump_locked(struct pb_debug_manager *mgr)
       buf = list_entry(curr, struct pb_debug_buffer, head);
 
       debug_printf("buffer = %p\n", (void *) buf);
-      debug_printf("    .size = 0x%"PRIx64"\n", buf->base.size);
+      debug_printf("    .size = 0x%"PRIx64"\n", buf->base.base.size);
       debug_backtrace_dump(buf->create_backtrace, PB_DEBUG_CREATE_BACKTRACE);
       
       curr = next; 
@@ -398,21 +398,21 @@ pb_debug_manager_create_buffer(struct pb_manager *_mgr,
       return NULL;
    }
    
-   assert(pipe_is_referenced(&buf->buffer->reference));
-   assert(pb_check_alignment(real_desc.alignment, 1u << buf->buffer->alignment_log2));
-   assert(pb_check_usage(real_desc.usage, buf->buffer->usage));
-   assert(buf->buffer->size >= real_size);
+   assert(pipe_is_referenced(&buf->buffer->base.reference));
+   assert(pb_check_alignment(real_desc.alignment, 1u << buf->buffer->base.alignment_log2));
+   assert(pb_check_usage(real_desc.usage, buf->buffer->base.usage));
+   assert(buf->buffer->base.size >= real_size);
    
-   pipe_reference_init(&buf->base.reference, 1);
-   buf->base.alignment_log2 = util_logbase2(desc->alignment);
-   buf->base.usage = desc->usage;
-   buf->base.size = size;
+   pipe_reference_init(&buf->base.base.reference, 1);
+   buf->base.base.alignment_log2 = util_logbase2(desc->alignment);
+   buf->base.base.usage = desc->usage;
+   buf->base.base.size = size;
    
    buf->base.vtbl = &pb_debug_buffer_vtbl;
    buf->mgr = mgr;
 
    buf->underflow_size = mgr->underflow_size;
-   buf->overflow_size = buf->buffer->size - buf->underflow_size - size;
+   buf->overflow_size = buf->buffer->base.size - buf->underflow_size - size;
    
    debug_backtrace_capture(buf->create_backtrace, 1, PB_DEBUG_CREATE_BACKTRACE);
 
diff --git a/src/gallium/auxiliary/pipebuffer/pb_bufmgr_mm.c b/src/gallium/auxiliary/pipebuffer/pb_bufmgr_mm.c
index 397e42eed3e30..d97e9da6efbb2 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_bufmgr_mm.c
+++ b/src/gallium/auxiliary/pipebuffer/pb_bufmgr_mm.c
@@ -97,7 +97,7 @@ mm_buffer_destroy(void *winsys, struct pb_buffer *buf)
    struct mm_buffer *mm_buf = mm_buffer(buf);
    struct mm_pb_manager *mm = mm_buf->mgr;
    
-   assert(!pipe_is_referenced(&mm_buf->base.reference));
+   assert(!pipe_is_referenced(&mm_buf->base.base.reference));
    
    mtx_lock(&mm->mutex);
    u_mmFreeMem(mm_buf->block);
@@ -192,10 +192,10 @@ mm_bufmgr_create_buffer(struct pb_manager *mgr,
       return NULL;
    }
 
-   pipe_reference_init(&mm_buf->base.reference, 1);
-   mm_buf->base.alignment_log2 = util_logbase2(desc->alignment);
-   mm_buf->base.usage = desc->usage;
-   mm_buf->base.size = size;
+   pipe_reference_init(&mm_buf->base.base.reference, 1);
+   mm_buf->base.base.alignment_log2 = util_logbase2(desc->alignment);
+   mm_buf->base.base.usage = desc->usage;
+   mm_buf->base.base.size = size;
    
    mm_buf->base.vtbl = &mm_buffer_vtbl;
    
diff --git a/src/gallium/auxiliary/pipebuffer/pb_bufmgr_slab.c b/src/gallium/auxiliary/pipebuffer/pb_bufmgr_slab.c
index a986895ad0572..7496a804d0bff 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_bufmgr_slab.c
+++ b/src/gallium/auxiliary/pipebuffer/pb_bufmgr_slab.c
@@ -196,7 +196,7 @@ pb_slab_buffer_destroy(void *winsys, struct pb_buffer *_buf)
 
    mtx_lock(&mgr->mutex);
    
-   assert(!pipe_is_referenced(&buf->base.reference));
+   assert(!pipe_is_referenced(&buf->base.base.reference));
    
    buf->mapCount = 0;
 
@@ -321,7 +321,7 @@ pb_slab_create(struct pb_slab_manager *mgr)
       goto out_err1;
    }
 
-   numBuffers = slab->bo->size / mgr->bufSize;
+   numBuffers = slab->bo->base.size / mgr->bufSize;
 
    slab->buffers = CALLOC(numBuffers, sizeof(*slab->buffers));
    if (!slab->buffers) {
@@ -337,10 +337,10 @@ pb_slab_create(struct pb_slab_manager *mgr)
 
    buf = slab->buffers;
    for (i=0; i < numBuffers; ++i) {
-      pipe_reference_init(&buf->base.reference, 0);
-      buf->base.size = mgr->bufSize;
-      buf->base.alignment_log2 = 0;
-      buf->base.usage = 0;
+      pipe_reference_init(&buf->base.base.reference, 0);
+      buf->base.base.size = mgr->bufSize;
+      buf->base.base.alignment_log2 = 0;
+      buf->base.base.usage = 0;
       buf->base.vtbl = &pb_slab_buffer_vtbl;
       buf->slab = slab;
       buf->start = i* mgr->bufSize;
@@ -415,9 +415,9 @@ pb_slab_manager_create_buffer(struct pb_manager *_mgr,
    mtx_unlock(&mgr->mutex);
    buf = list_entry(list, struct pb_slab_buffer, head);
    
-   pipe_reference_init(&buf->base.reference, 1);
-   buf->base.alignment_log2 = util_logbase2(desc->alignment);
-   buf->base.usage = desc->usage;
+   pipe_reference_init(&buf->base.base.reference, 1);
+   buf->base.base.alignment_log2 = util_logbase2(desc->alignment);
+   buf->base.base.usage = desc->usage;
    
    return &buf->base;
 }
diff --git a/src/gallium/auxiliary/pipebuffer/pb_cache.c b/src/gallium/auxiliary/pipebuffer/pb_cache.c
index 44af0f630971f..fc6a17a36b3a7 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_cache.c
+++ b/src/gallium/auxiliary/pipebuffer/pb_cache.c
@@ -62,12 +62,12 @@ destroy_buffer_locked(struct pb_cache_entry *entry)
    struct pb_cache *mgr = entry->mgr;
    struct pb_buffer *buf = entry->buffer;
 
-   assert(!pipe_is_referenced(&buf->reference));
+   assert(!pipe_is_referenced(&buf->base.reference));
    if (list_is_linked(&entry->head)) {
       list_del(&entry->head);
       assert(mgr->num_buffers);
       --mgr->num_buffers;
-      mgr->cache_size -= buf->size;
+      mgr->cache_size -= buf->base.size;
    }
    mgr->destroy_buffer(mgr->winsys, buf);
 }
@@ -111,7 +111,7 @@ pb_cache_add_buffer(struct pb_cache_entry *entry)
    unsigned i;
 
    simple_mtx_lock(&mgr->mutex);
-   assert(!pipe_is_referenced(&buf->reference));
+   assert(!pipe_is_referenced(&buf->base.reference));
 
    unsigned current_time_ms = time_get_ms(mgr);
 
@@ -119,7 +119,7 @@ pb_cache_add_buffer(struct pb_cache_entry *entry)
       release_expired_buffers_locked(&mgr->buckets[i], current_time_ms);
 
    /* Directly release any buffer that exceeds the limit. */
-   if (mgr->cache_size + buf->size > mgr->max_cache_size) {
+   if (mgr->cache_size + buf->base.size > mgr->max_cache_size) {
       mgr->destroy_buffer(mgr->winsys, buf);
       simple_mtx_unlock(&mgr->mutex);
       return;
@@ -128,7 +128,7 @@ pb_cache_add_buffer(struct pb_cache_entry *entry)
    entry->start_ms = time_get_ms(mgr);
    list_addtail(&entry->head, cache);
    ++mgr->num_buffers;
-   mgr->cache_size += buf->size;
+   mgr->cache_size += buf->base.size;
    simple_mtx_unlock(&mgr->mutex);
 }
 
@@ -144,18 +144,18 @@ pb_cache_is_buffer_compat(struct pb_cache_entry *entry,
    struct pb_cache *mgr = entry->mgr;
    struct pb_buffer *buf = entry->buffer;
 
-   if (!pb_check_usage(usage, buf->usage))
+   if (!pb_check_usage(usage, buf->base.usage))
       return 0;
 
    /* be lenient with size */
-   if (buf->size < size ||
-       buf->size > (unsigned) (mgr->size_factor * size))
+   if (buf->base.size < size ||
+       buf->base.size > (unsigned) (mgr->size_factor * size))
       return 0;
 
    if (usage & mgr->bypass_usage)
       return 0;
 
-   if (!pb_check_alignment(alignment, 1u << buf->alignment_log2))
+   if (!pb_check_alignment(alignment, 1u << buf->base.alignment_log2))
       return 0;
 
    return mgr->can_reclaim(mgr->winsys, buf) ? 1 : -1;
@@ -228,12 +228,12 @@ pb_cache_reclaim_buffer(struct pb_cache *mgr, pb_size size,
    if (entry) {
       struct pb_buffer *buf = entry->buffer;
 
-      mgr->cache_size -= buf->size;
+      mgr->cache_size -= buf->base.size;
       list_del(&entry->head);
       --mgr->num_buffers;
       simple_mtx_unlock(&mgr->mutex);
       /* Increase refcount */
-      pipe_reference_init(&buf->reference, 1);
+      pipe_reference_init(&buf->base.reference, 1);
       return buf;
    }
 
diff --git a/src/gallium/drivers/d3d12/d3d12_bufmgr.cpp b/src/gallium/drivers/d3d12/d3d12_bufmgr.cpp
index 8f4530e79fdd1..6e021e428a1bb 100644
--- a/src/gallium/drivers/d3d12/d3d12_bufmgr.cpp
+++ b/src/gallium/drivers/d3d12/d3d12_bufmgr.cpp
@@ -64,7 +64,7 @@ describe_suballoc_bo(char *buf, struct d3d12_bo *ptr)
    d3d12_bo *base = d3d12_bo_get_base(ptr, &offset);
    describe_direct_bo(res, base);
    sprintf(buf, "d3d12_bo<suballoc<%s>,0x%x,0x%x>", res,
-           (unsigned)ptr->buffer->size, (unsigned)offset);
+           (unsigned)ptr->buffer->base.size, (unsigned)offset);
 }
 
 void
@@ -332,11 +332,11 @@ d3d12_bufmgr_create_buffer(struct pb_manager *pmgr,
    if (!buf)
       return NULL;
 
-   pipe_reference_init(&buf->base.reference, 1);
-   buf->base.alignment_log2 = util_logbase2(pb_desc->alignment);
-   buf->base.usage = pb_desc->usage;
+   pipe_reference_init(&buf->base.base.reference, 1);
+   buf->base.base.alignment_log2 = util_logbase2(pb_desc->alignment);
+   buf->base.base.usage = pb_desc->usage;
    buf->base.vtbl = &d3d12_buffer_vtbl;
-   buf->base.size = size;
+   buf->base.base.size = size;
    buf->range.Begin = 0;
    buf->range.End = size;
 
diff --git a/src/gallium/drivers/d3d12/d3d12_bufmgr.h b/src/gallium/drivers/d3d12/d3d12_bufmgr.h
index 78f716fb50892..68147185bbef7 100644
--- a/src/gallium/drivers/d3d12/d3d12_bufmgr.h
+++ b/src/gallium/drivers/d3d12/d3d12_bufmgr.h
@@ -106,7 +106,7 @@ static inline uint64_t
 d3d12_bo_get_size(struct d3d12_bo *bo)
 {
    if (bo->buffer)
-      return bo->buffer->size;
+      return bo->buffer->base.size;
    else
       return GetDesc(bo->res).Width;
 }
diff --git a/src/gallium/drivers/r300/r300_emit.c b/src/gallium/drivers/r300/r300_emit.c
index ad0e4c0eb3dc5..5b2cb3ad4ea5f 100644
--- a/src/gallium/drivers/r300/r300_emit.c
+++ b/src/gallium/drivers/r300/r300_emit.c
@@ -770,8 +770,8 @@ void r300_emit_query_end(struct r300_context* r300)
     query->num_results += query->num_pipes;
 
     /* XXX grab all the results and reset the counter. */
-    if (query->num_results >= query->buf->size / 4 - 4) {
-        query->num_results = (query->buf->size / 4) / 2;
+    if (query->num_results >= query->buf->base.size / 4 - 4) {
+        query->num_results = (query->buf->base.size / 4) / 2;
         fprintf(stderr, "r300: Rewinding OQBO...\n");
     }
 }
diff --git a/src/gallium/drivers/r300/r300_render.c b/src/gallium/drivers/r300/r300_render.c
index 858d1798b480a..19c213766f5cb 100644
--- a/src/gallium/drivers/r300/r300_render.c
+++ b/src/gallium/drivers/r300/r300_render.c
@@ -950,7 +950,7 @@ static bool r300_render_allocate_vertices(struct vbuf_render* render,
 
     DBG(r300, DBG_DRAW, "r300: render_allocate_vertices (size: %d)\n", size);
 
-    if (!r300->vbo || size + r300->draw_vbo_offset > r300->vbo->size) {
+    if (!r300->vbo || size + r300->draw_vbo_offset > r300->vbo->base.size) {
 	radeon_bo_reference(r300->rws, &r300->vbo, NULL);
         r300->vbo = NULL;
         r300render->vbo_ptr = NULL;
@@ -1056,7 +1056,7 @@ static void r300_render_draw_elements(struct vbuf_render* render,
 {
     struct r300_render* r300render = r300_render(render);
     struct r300_context* r300 = r300render->r300;
-    unsigned max_index = (r300->vbo->size - r300->draw_vbo_offset) /
+    unsigned max_index = (r300->vbo->base.size - r300->draw_vbo_offset) /
                          (r300render->r300->vertex_info.size * 4) - 1;
     struct pipe_resource *index_buffer = NULL;
     unsigned index_buffer_offset;
diff --git a/src/gallium/drivers/r300/r300_texture_desc.c b/src/gallium/drivers/r300/r300_texture_desc.c
index 7b1b2e21d2ae2..c98e29db55580 100644
--- a/src/gallium/drivers/r300/r300_texture_desc.c
+++ b/src/gallium/drivers/r300/r300_texture_desc.c
@@ -606,17 +606,17 @@ void r300_texture_desc_init(struct r300_screen *rscreen,
     r300_setup_miptree(rscreen, tex, true);
     /* If the required buffer size is larger than the given max size,
      * try again without the alignment for the CBZB clear. */
-    if (tex->buf && tex->tex.size_in_bytes > tex->buf->size) {
+    if (tex->buf && tex->tex.size_in_bytes > tex->buf->base.size) {
         r300_setup_miptree(rscreen, tex, false);
 
         /* Make sure the buffer we got is large enough. */
-        if (tex->tex.size_in_bytes > tex->buf->size) {
+        if (tex->tex.size_in_bytes > tex->buf->base.size) {
             fprintf(stderr,
                 "r300: I got a pre-allocated buffer to use it as a texture "
                 "storage, but the buffer is too small. I'll use the buffer "
                 "anyway, because I can't crash here, but it's dangerous. "
                 "This can be a DDX bug. Got: %"PRIu64"B, Need: %uB, Info:\n",
-                tex->buf->size, tex->tex.size_in_bytes);
+                tex->buf->base.size, tex->tex.size_in_bytes);
             r300_tex_print_info(tex, "texture_desc_init");
             /* Oops, what now. Apps will break if we fail this,
              * so just pretend everything's okay. */
diff --git a/src/gallium/drivers/r600/r600_buffer_common.c b/src/gallium/drivers/r600/r600_buffer_common.c
index beb6b4a6e382e..20b182e2aed32 100644
--- a/src/gallium/drivers/r600/r600_buffer_common.c
+++ b/src/gallium/drivers/r600/r600_buffer_common.c
@@ -195,8 +195,8 @@ bool r600_alloc_resource(struct r600_common_screen *rscreen,
 	/* Print debug information. */
 	if (rscreen->debug_flags & DBG_VM && res->b.b.target == PIPE_BUFFER) {
 		fprintf(stderr, "VM start=0x%"PRIX64"  end=0x%"PRIX64" | Buffer %"PRIu64" bytes\n",
-			res->gpu_address, res->gpu_address + res->buf->size,
-			res->buf->size);
+			res->gpu_address, res->gpu_address + res->buf->base.size,
+			res->buf->base.size);
 	}
 	return true;
 }
diff --git a/src/gallium/drivers/r600/r600_state.c b/src/gallium/drivers/r600/r600_state.c
index a3746cc771a2e..e90d8430348fc 100644
--- a/src/gallium/drivers/r600/r600_state.c
+++ b/src/gallium/drivers/r600/r600_state.c
@@ -990,7 +990,7 @@ static void r600_init_color_surface(struct r600_context *rctx,
 		/* CMASK. */
 		if (!rctx->dummy_cmask ||
 		    rctx->dummy_cmask->b.b.width0 < cmask.size ||
-		    (1 << rctx->dummy_cmask->buf->alignment_log2) % cmask.alignment != 0) {
+		    (1 << rctx->dummy_cmask->buf->base.alignment_log2) % cmask.alignment != 0) {
 			struct pipe_transfer *transfer;
 			void *ptr;
 
@@ -1015,7 +1015,7 @@ static void r600_init_color_surface(struct r600_context *rctx,
 		/* FMASK. */
 		if (!rctx->dummy_fmask ||
 		    rctx->dummy_fmask->b.b.width0 < fmask.size ||
-		    (1 << rctx->dummy_fmask->buf->alignment_log2) % fmask.alignment != 0) {
+		    (1 << rctx->dummy_fmask->buf->base.alignment_log2) % fmask.alignment != 0) {
 			r600_resource_reference(&rctx->dummy_fmask, NULL);
 			rctx->dummy_fmask = (struct r600_resource*)
 				r600_aligned_buffer_create(&rscreen->b.b, 0,
diff --git a/src/gallium/drivers/r600/r600_texture.c b/src/gallium/drivers/r600/r600_texture.c
index 536b50ecc7861..347be5e4a8f4e 100644
--- a/src/gallium/drivers/r600/r600_texture.c
+++ b/src/gallium/drivers/r600/r600_texture.c
@@ -973,13 +973,13 @@ r600_texture_create_object(struct pipe_screen *screen,
 	} else {
 		resource->buf = buf;
 		resource->gpu_address = rscreen->ws->buffer_get_virtual_address(resource->buf);
-		resource->bo_size = buf->size;
-		resource->bo_alignment = 1 << buf->alignment_log2;
+		resource->bo_size = buf->base.size;
+		resource->bo_alignment = 1 << buf->base.alignment_log2;
 		resource->domains = rscreen->ws->buffer_get_initial_domain(resource->buf);
 		if (resource->domains & RADEON_DOMAIN_VRAM)
-			resource->vram_usage = buf->size;
+			resource->vram_usage = buf->base.size;
 		else if (resource->domains & RADEON_DOMAIN_GTT)
-			resource->gart_usage = buf->size;
+			resource->gart_usage = buf->base.size;
 	}
 
 	if (rtex->cmask.size) {
@@ -1004,7 +1004,7 @@ r600_texture_create_object(struct pipe_screen *screen,
 	if (rscreen->debug_flags & DBG_VM) {
 		fprintf(stderr, "VM start=0x%"PRIX64"  end=0x%"PRIX64" | Texture %ix%ix%i, %i levels, %i samples, %s\n",
 			rtex->resource.gpu_address,
-			rtex->resource.gpu_address + rtex->resource.buf->size,
+			rtex->resource.gpu_address + rtex->resource.buf->base.size,
 			base->width0, base->height0, util_num_layers(base, 0), base->last_level+1,
 			base->nr_samples ? base->nr_samples : 1, util_format_short_name(base->format));
 	}
@@ -1486,7 +1486,7 @@ void r600_texture_transfer_unmap(struct pipe_context *ctx,
 	}
 
 	if (rtransfer->staging) {
-		rctx->num_alloc_tex_transfer_bytes += rtransfer->staging->buf->size;
+		rctx->num_alloc_tex_transfer_bytes += rtransfer->staging->buf->base.size;
 		r600_resource_reference(&rtransfer->staging, NULL);
 	}
 
diff --git a/src/gallium/drivers/r600/radeon_uvd.c b/src/gallium/drivers/r600/radeon_uvd.c
index 569fe42a6a936..0dc16cf3e8492 100644
--- a/src/gallium/drivers/r600/radeon_uvd.c
+++ b/src/gallium/drivers/r600/radeon_uvd.c
@@ -887,7 +887,7 @@ static void ruvd_decode_bitstream(struct pipe_video_codec *decoder,
 		if (format == PIPE_VIDEO_FORMAT_JPEG)
 			new_size += 2; /* save for EOI */
 
-		if (new_size > buf->res->buf->size) {
+		if (new_size > buf->res->buf->base.size) {
 			dec->ws->buffer_unmap(dec->ws, buf->res->buf);
 			dec->bs_ptr = NULL;
 			if (!rvid_resize_buffer(dec->screen, &dec->cs, buf, new_size)) {
@@ -960,7 +960,7 @@ static void ruvd_end_frame(struct pipe_video_codec *decoder,
 	}
 
 	if (dec->dpb.res)
-		dec->msg->body.decode.dpb_size = dec->dpb.res->buf->size;
+		dec->msg->body.decode.dpb_size = dec->dpb.res->buf->base.size;
 	dec->msg->body.decode.bsd_size = bs_size;
 	dec->msg->body.decode.db_pitch = align(dec->base.width, get_db_pitch_alignment(dec));
 
diff --git a/src/gallium/drivers/r600/radeon_video.c b/src/gallium/drivers/r600/radeon_video.c
index c094fc08f4895..54325e2957853 100644
--- a/src/gallium/drivers/r600/radeon_video.c
+++ b/src/gallium/drivers/r600/radeon_video.c
@@ -90,7 +90,7 @@ bool rvid_resize_buffer(struct pipe_screen *screen, struct radeon_cmdbuf *cs,
 {
 	struct r600_common_screen *rscreen = (struct r600_common_screen *)screen;
 	struct radeon_winsys* ws = rscreen->ws;
-	unsigned bytes = MIN2(new_buf->res->buf->size, new_size);
+	unsigned bytes = MIN2(new_buf->res->buf->base.size, new_size);
 	struct rvid_buffer old_buf = *new_buf;
 	void *src = NULL, *dst = NULL;
 
@@ -132,7 +132,7 @@ void rvid_clear_buffer(struct pipe_context *context, struct rvid_buffer* buffer)
 	struct r600_common_context *rctx = (struct r600_common_context*)context;
 
 	rctx->dma_clear_buffer(context, &buffer->res->b.b, 0,
-			       buffer->res->buf->size, 0);
+			       buffer->res->buf->base.size, 0);
 	context->flush(context, NULL, 0);
 }
 
@@ -189,9 +189,9 @@ void rvid_join_surfaces(struct r600_common_context *rctx,
 		if (!buffers[i] || !*buffers[i])
 			continue;
 
-		size = align(size, 1 << (*buffers[i])->alignment_log2);
-		size += (*buffers[i])->size;
-		alignment = MAX2(alignment, 1 << (*buffers[i])->alignment_log2);
+		size = align(size, 1 << (*buffers[i])->base.alignment_log2);
+		size += (*buffers[i])->base.size;
+		alignment = MAX2(alignment, 1 << (*buffers[i])->base.alignment_log2);
 	}
 
 	if (!size)
diff --git a/src/gallium/drivers/radeonsi/radeon_uvd.c b/src/gallium/drivers/radeonsi/radeon_uvd.c
index 5cccfb28fe998..f7602429a0905 100644
--- a/src/gallium/drivers/radeonsi/radeon_uvd.c
+++ b/src/gallium/drivers/radeonsi/radeon_uvd.c
@@ -1046,7 +1046,7 @@ static void ruvd_decode_bitstream(struct pipe_video_codec *decoder,
       struct rvid_buffer *buf = &dec->bs_buffers[dec->cur_buffer];
       unsigned new_size = dec->bs_size + sizes[i];
 
-      if (new_size > buf->res->buf->size) {
+      if (new_size > buf->res->buf->base.size) {
          dec->ws->buffer_unmap(dec->ws, buf->res->buf);
          if (!si_vid_resize_buffer(dec->screen, &dec->cs, buf, new_size, NULL)) {
             RVID_ERR("Can't resize bitstream buffer!");
@@ -1110,13 +1110,13 @@ static void ruvd_end_frame(struct pipe_video_codec *decoder, struct pipe_video_b
    }
 
    if (dec->dpb.res)
-      dec->msg->body.decode.dpb_size = dec->dpb.res->buf->size;
+      dec->msg->body.decode.dpb_size = dec->dpb.res->buf->base.size;
    dec->msg->body.decode.bsd_size = bs_size;
    dec->msg->body.decode.db_pitch = align(dec->base.width, get_db_pitch_alignment(dec));
 
    if (dec->stream_type == RUVD_CODEC_H264_PERF &&
        ((struct si_screen *)dec->screen)->info.family >= CHIP_POLARIS10)
-      dec->msg->body.decode.dpb_reserved = dec->ctx.res->buf->size;
+      dec->msg->body.decode.dpb_reserved = dec->ctx.res->buf->base.size;
 
    dt = dec->set_dtb(dec->msg, (struct vl_video_buffer *)target);
    if (((struct si_screen *)dec->screen)->info.family >= CHIP_STONEY)
@@ -1144,7 +1144,7 @@ static void ruvd_end_frame(struct pipe_video_codec *decoder, struct pipe_video_b
       }
 
       if (dec->ctx.res)
-         dec->msg->body.decode.dpb_reserved = dec->ctx.res->buf->size;
+         dec->msg->body.decode.dpb_reserved = dec->ctx.res->buf->base.size;
       break;
 
    case PIPE_VIDEO_FORMAT_VC1:
diff --git a/src/gallium/drivers/radeonsi/radeon_vce_50.c b/src/gallium/drivers/radeonsi/radeon_vce_50.c
index d555be36441c7..e89ab4974dd70 100644
--- a/src/gallium/drivers/radeonsi/radeon_vce_50.c
+++ b/src/gallium/drivers/radeonsi/radeon_vce_50.c
@@ -79,7 +79,7 @@ static void encode(struct rvce_encoder *enc)
 
    if (enc->dual_pipe) {
       unsigned aux_offset =
-         enc->cpb.res->buf->size - RVCE_MAX_AUX_BUFFER_NUM * RVCE_MAX_BITSTREAM_OUTPUT_ROW_SIZE * 2;
+         enc->cpb.res->buf->base.size - RVCE_MAX_AUX_BUFFER_NUM * RVCE_MAX_BITSTREAM_OUTPUT_ROW_SIZE * 2;
       RVCE_BEGIN(0x05000002); // auxiliary buffer
       for (i = 0; i < 8; ++i) {
          RVCE_CS(aux_offset);
diff --git a/src/gallium/drivers/radeonsi/radeon_vce_52.c b/src/gallium/drivers/radeonsi/radeon_vce_52.c
index f2400cd749671..19fa18a65a4c4 100644
--- a/src/gallium/drivers/radeonsi/radeon_vce_52.c
+++ b/src/gallium/drivers/radeonsi/radeon_vce_52.c
@@ -226,7 +226,7 @@ static void encode(struct rvce_encoder *enc)
 
    if (enc->dual_pipe) {
       unsigned aux_offset =
-         enc->cpb.res->buf->size - RVCE_MAX_AUX_BUFFER_NUM * RVCE_MAX_BITSTREAM_OUTPUT_ROW_SIZE * 2;
+         enc->cpb.res->buf->base.size - RVCE_MAX_AUX_BUFFER_NUM * RVCE_MAX_BITSTREAM_OUTPUT_ROW_SIZE * 2;
       RVCE_BEGIN(0x05000002); // auxiliary buffer
       for (i = 0; i < 8; ++i) {
          RVCE_CS(aux_offset);
diff --git a/src/gallium/drivers/radeonsi/radeon_vcn_dec.c b/src/gallium/drivers/radeonsi/radeon_vcn_dec.c
index 6e42ff16e75a5..fe40358c5acbd 100644
--- a/src/gallium/drivers/radeonsi/radeon_vcn_dec.c
+++ b/src/gallium/drivers/radeonsi/radeon_vcn_dec.c
@@ -2164,14 +2164,14 @@ static struct pb_buffer *rvcn_dec_message_decode(struct radeon_decoder *dec,
    luma   = (struct si_texture *)((struct vl_video_buffer *)out_surf)->resources[0];
    chroma = (struct si_texture *)((struct vl_video_buffer *)out_surf)->resources[1];
 
-   decode->dpb_size = (dec->dpb_type != DPB_DYNAMIC_TIER_2) ? dec->dpb.res->buf->size : 0;
+   decode->dpb_size = (dec->dpb_type != DPB_DYNAMIC_TIER_2) ? dec->dpb.res->buf->base.size : 0;
 
    /* When texture being created, the bo will be created with total size of planes,
     * and all planes point to the same buffer */
-   assert(si_resource(((struct vl_video_buffer *)out_surf)->resources[0])->buf->size ==
-      si_resource(((struct vl_video_buffer *)out_surf)->resources[1])->buf->size);
+   assert(si_resource(((struct vl_video_buffer *)out_surf)->resources[0])->buf->base.size ==
+      si_resource(((struct vl_video_buffer *)out_surf)->resources[1])->buf->base.size);
 
-   decode->dt_size = si_resource(((struct vl_video_buffer *)out_surf)->resources[0])->buf->size;
+   decode->dt_size = si_resource(((struct vl_video_buffer *)out_surf)->resources[0])->buf->base.size;
 
    decode->sct_size = 0;
    decode->sc_coeff_size = 0;
@@ -2364,7 +2364,7 @@ static struct pb_buffer *rvcn_dec_message_decode(struct radeon_decoder *dec,
    }
 
    if (dec->ctx.res)
-      decode->hw_ctxt_size = dec->ctx.res->buf->size;
+      decode->hw_ctxt_size = dec->ctx.res->buf->base.size;
 
    if (dec->dpb_type == DPB_DYNAMIC_TIER_2)
       if (rvcn_dec_dynamic_dpb_t2_message(dec, decode, dynamic_dpb_t2, encrypted))
@@ -2882,7 +2882,7 @@ static void radeon_dec_decode_bitstream(struct pipe_video_codec *decoder,
 
    struct rvid_buffer *buf = &dec->bs_buffers[dec->cur_buffer];
 
-   if (total_bs_size > buf->res->buf->size) {
+   if (total_bs_size > buf->res->buf->base.size) {
       dec->ws->buffer_unmap(dec->ws, buf->res->buf);
       dec->bs_ptr = NULL;
       if (!si_vid_resize_buffer(dec->screen, &dec->cs, buf, total_bs_size, NULL)) {
diff --git a/src/gallium/drivers/radeonsi/radeon_vcn_enc.c b/src/gallium/drivers/radeonsi/radeon_vcn_enc.c
index 78c415f26c493..ef0c7c33ee7f5 100644
--- a/src/gallium/drivers/radeonsi/radeon_vcn_enc.c
+++ b/src/gallium/drivers/radeonsi/radeon_vcn_enc.c
@@ -1160,7 +1160,7 @@ static void radeon_enc_encode_bitstream(struct pipe_video_codec *encoder,
 
    if (vid_buf->base.statistics_data) {
       enc->get_buffer(vid_buf->base.statistics_data, &enc->stats, NULL);
-      if (enc->stats->size < sizeof(rvcn_encode_stats_type_0_t)) {
+      if (enc->stats->base.size < sizeof(rvcn_encode_stats_type_0_t)) {
          RVID_ERR("Encoder statistics output buffer is too small.\n");
          enc->stats = NULL;
       }
diff --git a/src/gallium/drivers/radeonsi/radeon_video.c b/src/gallium/drivers/radeonsi/radeon_video.c
index a5e2b626abc7c..298eb54b59f13 100644
--- a/src/gallium/drivers/radeonsi/radeon_video.c
+++ b/src/gallium/drivers/radeonsi/radeon_video.c
@@ -73,7 +73,7 @@ bool si_vid_resize_buffer(struct pipe_screen *screen, struct radeon_cmdbuf *cs,
 {
    struct si_screen *sscreen = (struct si_screen *)screen;
    struct radeon_winsys *ws = sscreen->ws;
-   unsigned bytes = MIN2(new_buf->res->buf->size, new_size);
+   unsigned bytes = MIN2(new_buf->res->buf->base.size, new_size);
    struct rvid_buffer old_buf = *new_buf;
    void *src = NULL, *dst = NULL;
 
diff --git a/src/gallium/drivers/radeonsi/si_buffer.c b/src/gallium/drivers/radeonsi/si_buffer.c
index 27d227cdbbb5e..92aa5aeed83fe 100644
--- a/src/gallium/drivers/radeonsi/si_buffer.c
+++ b/src/gallium/drivers/radeonsi/si_buffer.c
@@ -179,7 +179,7 @@ bool si_alloc_resource(struct si_screen *sscreen, struct si_resource *res)
    /* Print debug information. */
    if (sscreen->debug_flags & DBG(VM) && res->b.b.target == PIPE_BUFFER) {
       fprintf(stderr, "VM start=0x%" PRIX64 "  end=0x%" PRIX64 " | Buffer %" PRIu64 " bytes | Flags: ",
-              res->gpu_address, res->gpu_address + res->buf->size, res->buf->size);
+              res->gpu_address, res->gpu_address + res->buf->base.size, res->buf->base.size);
       si_res_print_flags(res->flags);
       fprintf(stderr, "\n");
    }
@@ -643,7 +643,7 @@ struct pipe_resource *si_buffer_from_winsys_buffer(struct pipe_screen *screen,
                                                    struct pb_buffer *imported_buf,
                                                    uint64_t offset)
 {
-   if (offset + templ->width0 > imported_buf->size)
+   if (offset + templ->width0 > imported_buf->base.size)
       return NULL;
 
    struct si_screen *sscreen = (struct si_screen *)screen;
@@ -679,8 +679,8 @@ struct pipe_resource *si_buffer_from_winsys_buffer(struct pipe_screen *screen,
          res->b.b.usage = PIPE_USAGE_STAGING;
    }
 
-   si_init_resource_fields(sscreen, res, imported_buf->size,
-                           1 << imported_buf->alignment_log2);
+   si_init_resource_fields(sscreen, res, imported_buf->base.size,
+                           1 << imported_buf->base.alignment_log2);
 
    res->b.is_shared = true;
    res->b.buffer_id_unique = util_idalloc_mt_alloc(&sscreen->buffer_ids);
diff --git a/src/gallium/drivers/radeonsi/si_texture.c b/src/gallium/drivers/radeonsi/si_texture.c
index 1c27dd3bd7a89..f55ba96870a0e 100644
--- a/src/gallium/drivers/radeonsi/si_texture.c
+++ b/src/gallium/drivers/radeonsi/si_texture.c
@@ -1007,8 +1007,8 @@ static struct si_texture *si_texture_create_object(struct pipe_screen *screen,
    } else {
       resource->buf = imported_buf;
       resource->gpu_address = sscreen->ws->buffer_get_virtual_address(resource->buf);
-      resource->bo_size = imported_buf->size;
-      resource->bo_alignment_log2 = imported_buf->alignment_log2;
+      resource->bo_size = imported_buf->base.size;
+      resource->bo_alignment_log2 = imported_buf->base.alignment_log2;
       resource->domains = sscreen->ws->buffer_get_initial_domain(resource->buf);
       if (sscreen->ws->buffer_get_flags)
          resource->flags = sscreen->ws->buffer_get_flags(resource->buf);
@@ -1018,7 +1018,7 @@ static struct si_texture *si_texture_create_object(struct pipe_screen *screen,
       fprintf(stderr,
               "VM start=0x%" PRIX64 "  end=0x%" PRIX64
               " | Texture %ix%ix%i, %i levels, %i samples, %s | Flags: ",
-              tex->buffer.gpu_address, tex->buffer.gpu_address + tex->buffer.buf->size,
+              tex->buffer.gpu_address, tex->buffer.gpu_address + tex->buffer.buf->base.size,
               base->width0, base->height0, util_num_layers(base, 0), base->last_level + 1,
               base->nr_samples ? base->nr_samples : 1, util_format_short_name(base->format));
       si_res_print_flags(tex->buffer.flags);
@@ -1681,7 +1681,7 @@ static struct pipe_resource *si_texture_from_winsys_buffer(struct si_screen *ssc
    }
 
    if (ac_surface_get_plane_offset(sscreen->info.gfx_level, &tex->surface, 0, 0) +
-        tex->surface.total_size > buf->size) {
+        tex->surface.total_size > buf->base.size) {
       si_texture_reference(&tex, NULL);
       return NULL;
    }
@@ -2025,7 +2025,7 @@ static void si_texture_transfer_unmap(struct pipe_context *ctx, struct pipe_tran
       si_copy_from_staging_texture(ctx, stransfer);
 
    if (stransfer->staging) {
-      sctx->num_alloc_tex_transfer_bytes += stransfer->staging->buf->size;
+      sctx->num_alloc_tex_transfer_bytes += stransfer->staging->buf->base.size;
       si_resource_reference(&stransfer->staging, NULL);
    }
 
diff --git a/src/gallium/drivers/zink/zink_bo.c b/src/gallium/drivers/zink/zink_bo.c
index 856dd2439c5db..42d902fdd1393 100644
--- a/src/gallium/drivers/zink/zink_bo.c
+++ b/src/gallium/drivers/zink/zink_bo.c
@@ -171,7 +171,7 @@ static void
 bo_slab_free(struct zink_screen *screen, struct pb_slab *pslab)
 {
    struct zink_slab *slab = zink_slab(pslab);
-   ASSERTED unsigned slab_size = slab->buffer->base.size;
+   ASSERTED unsigned slab_size = slab->buffer->base.base.size;
 
    assert(slab->base.num_entries * slab->base.entry_size <= slab_size);
    FREE(slab->entries);
@@ -189,7 +189,7 @@ bo_slab_destroy(struct zink_screen *screen, struct pb_buffer *pbuf)
    //if (bo->base.usage & RADEON_FLAG_ENCRYPTED)
       //pb_slab_free(get_slabs(screen, bo->base.size, RADEON_FLAG_ENCRYPTED), &bo->u.slab.entry);
    //else
-      pb_slab_free(get_slabs(screen, bo->base.size, 0), &bo->u.slab.entry);
+      pb_slab_free(get_slabs(screen, bo->base.base.size, 0), &bo->u.slab.entry);
 }
 
 static bool
@@ -320,12 +320,12 @@ bo_create_internal(struct zink_screen *screen,
 
 
    simple_mtx_init(&bo->lock, mtx_plain);
-   pipe_reference_init(&bo->base.reference, 1);
-   bo->base.alignment_log2 = util_logbase2(alignment);
-   bo->base.size = mai.allocationSize;
+   pipe_reference_init(&bo->base.base.reference, 1);
+   bo->base.base.alignment_log2 = util_logbase2(alignment);
+   bo->base.base.size = mai.allocationSize;
    bo->base.vtbl = &bo_vtbl;
-   bo->base.placement = mem_type_idx;
-   bo->base.usage = flags;
+   bo->base.base.placement = mem_type_idx;
+   bo->base.base.usage = flags;
    bo->unique_id = p_atomic_inc_return(&screen->pb.next_bo_unique_id);
 
    return bo;
@@ -383,11 +383,11 @@ sparse_backing_alloc(struct zink_screen *screen, struct zink_bo *bo,
          return NULL;
       }
 
-      assert(bo->u.sparse.num_backing_pages < DIV_ROUND_UP(bo->base.size, ZINK_SPARSE_BUFFER_PAGE_SIZE));
+      assert(bo->u.sparse.num_backing_pages < DIV_ROUND_UP(bo->base.base.size, ZINK_SPARSE_BUFFER_PAGE_SIZE));
 
-      size = MIN3(bo->base.size / 16,
+      size = MIN3(bo->base.base.size / 16,
                   8 * 1024 * 1024,
-                  bo->base.size - (uint64_t)bo->u.sparse.num_backing_pages * ZINK_SPARSE_BUFFER_PAGE_SIZE);
+                  bo->base.base.size - (uint64_t)bo->u.sparse.num_backing_pages * ZINK_SPARSE_BUFFER_PAGE_SIZE);
       size = MAX2(size, ZINK_SPARSE_BUFFER_PAGE_SIZE);
 
       buf = zink_bo_create(screen, size, ZINK_SPARSE_BUFFER_PAGE_SIZE,
@@ -399,7 +399,7 @@ sparse_backing_alloc(struct zink_screen *screen, struct zink_bo *bo,
       }
 
       /* We might have gotten a bigger buffer than requested via caching. */
-      pages = buf->size / ZINK_SPARSE_BUFFER_PAGE_SIZE;
+      pages = buf->base.size / ZINK_SPARSE_BUFFER_PAGE_SIZE;
 
       best_backing->bo = zink_bo(buf);
       best_backing->num_chunks = 1;
@@ -430,7 +430,7 @@ static void
 sparse_free_backing_buffer(struct zink_screen *screen, struct zink_bo *bo,
                            struct zink_sparse_backing *backing)
 {
-   bo->u.sparse.num_backing_pages -= backing->bo->base.size / ZINK_SPARSE_BUFFER_PAGE_SIZE;
+   bo->u.sparse.num_backing_pages -= backing->bo->base.base.size / ZINK_SPARSE_BUFFER_PAGE_SIZE;
 
    list_del(&backing->list);
    zink_bo_unref(screen, backing->bo);
@@ -497,7 +497,7 @@ sparse_backing_free(struct zink_screen *screen, struct zink_bo *bo,
    }
 
    if (backing->num_chunks == 1 && backing->chunks[0].begin == 0 &&
-       backing->chunks[0].end == backing->bo->base.size / ZINK_SPARSE_BUFFER_PAGE_SIZE)
+       backing->chunks[0].end == backing->bo->base.base.size / ZINK_SPARSE_BUFFER_PAGE_SIZE)
       sparse_free_backing_buffer(screen, bo, backing);
 
    return true;
@@ -508,7 +508,7 @@ bo_sparse_destroy(struct zink_screen *screen, struct pb_buffer *pbuf)
 {
    struct zink_bo *bo = zink_bo(pbuf);
 
-   assert(!bo->mem && bo->base.usage & ZINK_ALLOC_SPARSE);
+   assert(!bo->mem && bo->base.base.usage & ZINK_ALLOC_SPARSE);
 
    while (!list_is_empty(&bo->u.sparse.backing)) {
       sparse_free_backing_buffer(screen, bo,
@@ -544,15 +544,15 @@ bo_sparse_create(struct zink_screen *screen, uint64_t size)
       return NULL;
 
    simple_mtx_init(&bo->lock, mtx_plain);
-   pipe_reference_init(&bo->base.reference, 1);
-   bo->base.alignment_log2 = util_logbase2(ZINK_SPARSE_BUFFER_PAGE_SIZE);
-   bo->base.size = size;
+   pipe_reference_init(&bo->base.base.reference, 1);
+   bo->base.base.alignment_log2 = util_logbase2(ZINK_SPARSE_BUFFER_PAGE_SIZE);
+   bo->base.base.size = size;
    bo->base.vtbl = &bo_sparse_vtbl;
    unsigned placement = zink_mem_type_idx_from_bits(screen, ZINK_HEAP_DEVICE_LOCAL_SPARSE, VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT);
    assert(placement != UINT32_MAX);
-   bo->base.placement = placement;
+   bo->base.base.placement = placement;
    bo->unique_id = p_atomic_inc_return(&screen->pb.next_bo_unique_id);
-   bo->base.usage = ZINK_ALLOC_SPARSE;
+   bo->base.base.usage = ZINK_ALLOC_SPARSE;
 
    bo->u.sparse.num_va_pages = DIV_ROUND_UP(size, ZINK_SPARSE_BUFFER_PAGE_SIZE);
    bo->u.sparse.commitments = CALLOC(bo->u.sparse.num_va_pages,
@@ -633,10 +633,10 @@ zink_bo_create(struct zink_screen *screen, uint64_t size, unsigned alignment, en
          return NULL;
 
       bo = container_of(entry, struct zink_bo, u.slab.entry);
-      assert(bo->base.placement == mem_type_idx);
-      pipe_reference_init(&bo->base.reference, 1);
-      bo->base.size = size;
-      assert(alignment <= 1 << bo->base.alignment_log2);
+      assert(bo->base.base.placement == mem_type_idx);
+      pipe_reference_init(&bo->base.base.reference, 1);
+      bo->base.base.size = size;
+      assert(alignment <= 1 << bo->base.base.alignment_log2);
 
       return &bo->base;
    }
@@ -663,7 +663,7 @@ no_slab:
        /* Get a buffer from the cache. */
        bo = (struct zink_bo*)
             pb_cache_reclaim_buffer(&screen->pb.bo_cache, size, alignment, 0, mem_type_idx);
-       assert(!bo || bo->base.placement == mem_type_idx);
+       assert(!bo || bo->base.base.placement == mem_type_idx);
        if (bo)
           return &bo->base;
    }
@@ -677,7 +677,7 @@ no_slab:
       if (!bo)
          return NULL;
    }
-   assert(bo->base.placement == mem_type_idx);
+   assert(bo->base.base.placement == mem_type_idx);
 
    return &bo->base;
 }
@@ -703,15 +703,15 @@ zink_bo_map(struct zink_screen *screen, struct zink_bo *bo)
        * be atomic thanks to the lock. */
       cpu = real->u.real.cpu_ptr;
       if (!cpu) {
-         VkResult result = VKSCR(MapMemory)(screen->dev, real->mem, 0, real->base.size, 0, &cpu);
+         VkResult result = VKSCR(MapMemory)(screen->dev, real->mem, 0, real->base.base.size, 0, &cpu);
          if (result != VK_SUCCESS) {
             mesa_loge("ZINK: vkMapMemory failed (%s)", vk_Result_to_str(result));
             simple_mtx_unlock(&real->lock);
             return NULL;
          }
          if (unlikely(zink_debug & ZINK_DEBUG_MAP)) {
-            p_atomic_add(&screen->mapped_vram, real->base.size);
-            mesa_loge("NEW MAP(%"PRIu64") TOTAL(%"PRIu64")", real->base.size, screen->mapped_vram);
+            p_atomic_add(&screen->mapped_vram, real->base.base.size);
+            mesa_loge("NEW MAP(%"PRIu64") TOTAL(%"PRIu64")", real->base.base.size, screen->mapped_vram);
          }
          p_atomic_set(&real->u.real.cpu_ptr, cpu);
       }
@@ -732,8 +732,8 @@ zink_bo_unmap(struct zink_screen *screen, struct zink_bo *bo)
    if (p_atomic_dec_zero(&real->u.real.map_count)) {
       p_atomic_set(&real->u.real.cpu_ptr, NULL);
       if (unlikely(zink_debug & ZINK_DEBUG_MAP)) {
-         p_atomic_add(&screen->mapped_vram, -real->base.size);
-         mesa_loge("UNMAP(%"PRIu64") TOTAL(%"PRIu64")", real->base.size, screen->mapped_vram);
+         p_atomic_add(&screen->mapped_vram, -real->base.base.size);
+         mesa_loge("UNMAP(%"PRIu64") TOTAL(%"PRIu64")", real->base.base.size, screen->mapped_vram);
       }
       VKSCR(UnmapMemory)(screen->dev, real->mem);
    }
@@ -743,7 +743,7 @@ zink_bo_unmap(struct zink_screen *screen, struct zink_bo *bo)
 static void
 track_freed_sparse_bo(struct zink_context *ctx, struct zink_sparse_backing *backing)
 {
-   pipe_reference(NULL, &backing->bo->base.reference);
+   pipe_reference(NULL, &backing->bo->base.base.reference);
    util_dynarray_append(&ctx->batch.state->freed_sparse_backing_bos, struct zink_bo*, backing->bo);
 }
 
@@ -789,9 +789,9 @@ buffer_bo_commit(struct zink_context *ctx, struct zink_resource *res, uint32_t o
    struct zink_screen *screen = zink_screen(ctx->base.screen);
    struct zink_bo *bo = res->obj->bo;
    assert(offset % ZINK_SPARSE_BUFFER_PAGE_SIZE == 0);
-   assert(offset <= bo->base.size);
-   assert(size <= bo->base.size - offset);
-   assert(size % ZINK_SPARSE_BUFFER_PAGE_SIZE == 0 || offset + size == bo->base.size);
+   assert(offset <= bo->base.base.size);
+   assert(size <= bo->base.base.size - offset);
+   assert(size % ZINK_SPARSE_BUFFER_PAGE_SIZE == 0 || offset + size == bo->base.base.size);
 
    struct zink_sparse_commitment *comm = bo->u.sparse.commitments;
 
@@ -1237,7 +1237,7 @@ bo_slab_alloc(void *priv, unsigned mem_type_idx, unsigned entry_size, unsigned g
    if (!slab->buffer)
       goto fail;
 
-   slab_size = slab->buffer->base.size;
+   slab_size = slab->buffer->base.base.size;
 
    slab->base.num_entries = slab_size / entry_size;
    slab->base.num_free = slab->base.num_entries;
@@ -1254,8 +1254,8 @@ bo_slab_alloc(void *priv, unsigned mem_type_idx, unsigned entry_size, unsigned g
       struct zink_bo *bo = &slab->entries[i];
 
       simple_mtx_init(&bo->lock, mtx_plain);
-      bo->base.alignment_log2 = util_logbase2(get_slab_entry_alignment(screen, entry_size));
-      bo->base.size = entry_size;
+      bo->base.base.alignment_log2 = util_logbase2(get_slab_entry_alignment(screen, entry_size));
+      bo->base.base.size = entry_size;
       bo->base.vtbl = &bo_slab_vtbl;
       bo->offset = slab->buffer->offset + i * entry_size;
       bo->unique_id = base_id + i;
@@ -1269,7 +1269,7 @@ bo_slab_alloc(void *priv, unsigned mem_type_idx, unsigned entry_size, unsigned g
          bo->u.slab.real = slab->buffer->u.slab.real;
          assert(bo->u.slab.real->mem);
       }
-      bo->base.placement = bo->u.slab.real->base.placement;
+      bo->base.base.placement = bo->u.slab.real->base.base.placement;
 
       list_addtail(&bo->u.slab.entry.head, &slab->base.free);
    }
diff --git a/src/gallium/drivers/zink/zink_bo.h b/src/gallium/drivers/zink/zink_bo.h
index dc457905f0027..f747df6d869c7 100644
--- a/src/gallium/drivers/zink/zink_bo.h
+++ b/src/gallium/drivers/zink/zink_bo.h
@@ -131,7 +131,7 @@ zink_bo_get_mem(const struct zink_bo *bo)
 static ALWAYS_INLINE VkDeviceSize
 zink_bo_get_size(const struct zink_bo *bo)
 {
-   return bo->mem ? bo->base.size : bo->u.slab.real->base.size;
+   return bo->mem ? bo->base.base.size : bo->u.slab.real->base.base.size;
 }
 
 void *
diff --git a/src/gallium/drivers/zink/zink_resource.c b/src/gallium/drivers/zink/zink_resource.c
index e4d7e4ab9ad0c..2568f557fb4ff 100644
--- a/src/gallium/drivers/zink/zink_resource.c
+++ b/src/gallium/drivers/zink/zink_resource.c
@@ -1337,9 +1337,9 @@ retry:
       obj->bo->name = zink_debug_mem_add(screen, obj->size, buf);
    }
 
-   obj->coherent = screen->info.mem_props.memoryTypes[obj->bo->base.placement].propertyFlags & VK_MEMORY_PROPERTY_HOST_COHERENT_BIT;
+   obj->coherent = screen->info.mem_props.memoryTypes[obj->bo->base.base.placement].propertyFlags & VK_MEMORY_PROPERTY_HOST_COHERENT_BIT;
    if (!(templ->flags & PIPE_RESOURCE_FLAG_SPARSE)) {
-      obj->host_visible = screen->info.mem_props.memoryTypes[obj->bo->base.placement].propertyFlags & VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT;
+      obj->host_visible = screen->info.mem_props.memoryTypes[obj->bo->base.base.placement].propertyFlags & VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT;
    }
 
    if (templ->target == PIPE_BUFFER) {
@@ -2201,7 +2201,7 @@ zink_buffer_map(struct pipe_context *pctx,
       usage |= PIPE_MAP_UNSYNCHRONIZED;
    } else if (!(usage & PIPE_MAP_UNSYNCHRONIZED) &&
               (((usage & PIPE_MAP_READ) && !(usage & PIPE_MAP_PERSISTENT) &&
-               ((screen->info.mem_props.memoryTypes[res->obj->bo->base.placement].propertyFlags & VK_STAGING_RAM) != VK_STAGING_RAM)) ||
+               ((screen->info.mem_props.memoryTypes[res->obj->bo->base.base.placement].propertyFlags & VK_STAGING_RAM) != VK_STAGING_RAM)) ||
               !res->obj->host_visible)) {
       /* the above conditional catches uncached reads and non-HV writes */
       assert(!(usage & (TC_TRANSFER_MAP_THREADED_UNSYNC)));
diff --git a/src/gallium/include/winsys/radeon_winsys.h b/src/gallium/include/winsys/radeon_winsys.h
index b9d76c52061c0..4a9ae07a7e840 100644
--- a/src/gallium/include/winsys/radeon_winsys.h
+++ b/src/gallium/include/winsys/radeon_winsys.h
@@ -785,7 +785,7 @@ radeon_bo_reference(struct radeon_winsys *rws, struct pb_buffer **dst, struct pb
 {
    struct pb_buffer *old = *dst;
 
-   if (pipe_reference(&(*dst)->reference, &src->reference))
+   if (pipe_reference(&(*dst)->base.reference, &src->base.reference))
       rws->buffer_destroy(rws, old);
    *dst = src;
 }
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index 1ca0cbfed1436..4811874b17ee5 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -137,13 +137,13 @@ static inline unsigned get_slab_entry_offset(struct amdgpu_winsys_bo *bo)
 static enum radeon_bo_domain amdgpu_bo_get_initial_domain(
       struct pb_buffer *buf)
 {
-   return ((struct amdgpu_winsys_bo*)buf)->base.placement;
+   return ((struct amdgpu_winsys_bo*)buf)->base.base.placement;
 }
 
 static enum radeon_bo_flag amdgpu_bo_get_flags(
       struct pb_buffer *buf)
 {
-   return ((struct amdgpu_winsys_bo*)buf)->base.usage;
+   return ((struct amdgpu_winsys_bo*)buf)->base.base.usage;
 }
 
 static void amdgpu_bo_remove_fences(struct amdgpu_winsys_bo *bo)
@@ -164,15 +164,15 @@ void amdgpu_bo_destroy(struct amdgpu_winsys *ws, struct pb_buffer *_buf)
    simple_mtx_lock(&ws->bo_export_table_lock);
 
    /* amdgpu_bo_from_handle might have revived the bo */
-   if (p_atomic_read(&bo->b.base.reference.count)) {
+   if (p_atomic_read(&bo->b.base.base.reference.count)) {
       simple_mtx_unlock(&ws->bo_export_table_lock);
       return;
    }
 
    _mesa_hash_table_remove_key(ws->bo_export_table, bo->bo);
 
-   if (bo->b.base.placement & RADEON_DOMAIN_VRAM_GTT) {
-      amdgpu_bo_va_op(bo->bo, 0, bo->b.base.size,
+   if (bo->b.base.base.placement & RADEON_DOMAIN_VRAM_GTT) {
+      amdgpu_bo_va_op(bo->bo, 0, bo->b.base.base.size,
                       amdgpu_va_get_start_addr(bo->va_handle), 0, AMDGPU_VA_OP_UNMAP);
       amdgpu_va_range_free(bo->va_handle);
    }
@@ -216,10 +216,10 @@ void amdgpu_bo_destroy(struct amdgpu_winsys *ws, struct pb_buffer *_buf)
 
    amdgpu_bo_remove_fences(&bo->b);
 
-   if (bo->b.base.placement & RADEON_DOMAIN_VRAM)
-      ws->allocated_vram -= align64(bo->b.base.size, ws->info.gart_page_size);
-   else if (bo->b.base.placement & RADEON_DOMAIN_GTT)
-      ws->allocated_gtt -= align64(bo->b.base.size, ws->info.gart_page_size);
+   if (bo->b.base.base.placement & RADEON_DOMAIN_VRAM)
+      ws->allocated_vram -= align64(bo->b.base.base.size, ws->info.gart_page_size);
+   else if (bo->b.base.base.placement & RADEON_DOMAIN_GTT)
+      ws->allocated_gtt -= align64(bo->b.base.base.size, ws->info.gart_page_size);
 
    simple_mtx_destroy(&bo->lock);
    FREE(bo);
@@ -260,10 +260,10 @@ static bool amdgpu_bo_do_map(struct radeon_winsys *rws, struct amdgpu_bo_real *b
    }
 
    if (p_atomic_inc_return(&bo->map_count) == 1) {
-      if (bo->b.base.placement & RADEON_DOMAIN_VRAM)
-         ws->mapped_vram += bo->b.base.size;
-      else if (bo->b.base.placement & RADEON_DOMAIN_GTT)
-         ws->mapped_gtt += bo->b.base.size;
+      if (bo->b.base.base.placement & RADEON_DOMAIN_VRAM)
+         ws->mapped_vram += bo->b.base.base.size;
+      else if (bo->b.base.base.placement & RADEON_DOMAIN_GTT)
+         ws->mapped_gtt += bo->b.base.base.size;
       ws->num_mapped_buffers++;
    }
 
@@ -420,10 +420,10 @@ void amdgpu_bo_unmap(struct radeon_winsys *rws, struct pb_buffer *buf)
       assert(!real->cpu_ptr &&
              "too many unmaps or forgot RADEON_MAP_TEMPORARY flag");
 
-      if (real->b.base.placement & RADEON_DOMAIN_VRAM)
-         ws->mapped_vram -= real->b.base.size;
-      else if (real->b.base.placement & RADEON_DOMAIN_GTT)
-         ws->mapped_gtt -= real->b.base.size;
+      if (real->b.base.base.placement & RADEON_DOMAIN_VRAM)
+         ws->mapped_vram -= real->b.base.base.size;
+      else if (real->b.base.base.placement & RADEON_DOMAIN_GTT)
+         ws->mapped_gtt -= real->b.base.base.size;
       ws->num_mapped_buffers--;
    }
 
@@ -588,11 +588,11 @@ static struct amdgpu_winsys_bo *amdgpu_create_bo(struct amdgpu_winsys *ws,
    }
 
    simple_mtx_init(&bo->lock, mtx_plain);
-   pipe_reference_init(&bo->b.base.reference, 1);
-   bo->b.base.placement = initial_domain;
-   bo->b.base.alignment_log2 = util_logbase2(alignment);
-   bo->b.base.usage = flags;
-   bo->b.base.size = size;
+   pipe_reference_init(&bo->b.base.base.reference, 1);
+   bo->b.base.base.placement = initial_domain;
+   bo->b.base.base.alignment_log2 = util_logbase2(alignment);
+   bo->b.base.base.usage = flags;
+   bo->b.base.base.size = size;
    bo->b.unique_id = __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
    bo->bo = buf_handle;
    bo->va_handle = va_handle;
@@ -632,11 +632,11 @@ bool amdgpu_bo_can_reclaim_slab(void *priv, struct pb_slab_entry *entry)
 
 static unsigned get_slab_wasted_size(struct amdgpu_winsys *ws, struct amdgpu_bo_slab_entry *bo)
 {
-   assert(bo->b.base.size <= bo->entry.slab->entry_size);
-   assert(bo->b.base.size < (1 << bo->b.base.alignment_log2) ||
-          bo->b.base.size < 1 << ws->bo_slabs.min_order ||
-          bo->b.base.size > bo->entry.slab->entry_size / 2);
-   return bo->entry.slab->entry_size - bo->b.base.size;
+   assert(bo->b.base.base.size <= bo->entry.slab->entry_size);
+   assert(bo->b.base.base.size < (1 << bo->b.base.base.alignment_log2) ||
+          bo->b.base.base.size < 1 << ws->bo_slabs.min_order ||
+          bo->b.base.base.size > bo->entry.slab->entry_size / 2);
+   return bo->entry.slab->entry_size - bo->b.base.base.size;
 }
 
 static void amdgpu_bo_slab_destroy(struct radeon_winsys *rws, struct pb_buffer *_buf)
@@ -644,7 +644,7 @@ static void amdgpu_bo_slab_destroy(struct radeon_winsys *rws, struct pb_buffer *
    struct amdgpu_winsys *ws = amdgpu_winsys(rws);
    struct amdgpu_bo_slab_entry *bo = get_slab_entry_bo(amdgpu_winsys_bo(_buf));
 
-   if (bo->b.base.placement & RADEON_DOMAIN_VRAM)
+   if (bo->b.base.base.placement & RADEON_DOMAIN_VRAM)
       ws->slab_wasted_vram -= get_slab_wasted_size(ws, bo);
    else
       ws->slab_wasted_gtt -= get_slab_wasted_size(ws, bo);
@@ -723,7 +723,7 @@ struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_s
    assert(slab_bo->b.b.b.type == AMDGPU_BO_REAL_REUSABLE_SLAB);
 
    /* We can get a buffer from pb_cache that is slightly larger. */
-   slab_size = slab_bo->b.b.b.base.size;
+   slab_size = slab_bo->b.b.b.base.base.size;
 
    slab_bo->slab.num_entries = slab_size / entry_size;
    slab_bo->slab.num_free = slab_bo->slab.num_entries;
@@ -740,9 +740,9 @@ struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_s
    for (unsigned i = 0; i < slab_bo->slab.num_entries; ++i) {
       struct amdgpu_bo_slab_entry *bo = &slab_bo->entries[i];
 
-      bo->b.base.placement = domains;
-      bo->b.base.alignment_log2 = util_logbase2(get_slab_entry_alignment(ws, entry_size));
-      bo->b.base.size = entry_size;
+      bo->b.base.base.placement = domains;
+      bo->b.base.base.alignment_log2 = util_logbase2(get_slab_entry_alignment(ws, entry_size));
+      bo->b.base.base.size = entry_size;
       bo->b.type = AMDGPU_BO_SLAB_ENTRY;
       bo->b.unique_id = base_id + i;
 
@@ -767,10 +767,10 @@ fail:
 void amdgpu_bo_slab_free(struct amdgpu_winsys *ws, struct pb_slab *slab)
 {
    struct amdgpu_bo_real_reusable_slab *bo = get_bo_from_slab(slab);
-   unsigned slab_size = bo->b.b.b.base.size;
+   unsigned slab_size = bo->b.b.b.base.base.size;
 
    assert(bo->slab.num_entries * bo->slab.entry_size <= slab_size);
-   if (bo->b.b.b.base.placement & RADEON_DOMAIN_VRAM)
+   if (bo->b.b.b.base.base.placement & RADEON_DOMAIN_VRAM)
       ws->slab_wasted_vram -= slab_size - bo->slab.num_entries * bo->slab.entry_size;
    else
       ws->slab_wasted_gtt -= slab_size - bo->slab.num_entries * bo->slab.entry_size;
@@ -885,16 +885,16 @@ sparse_backing_alloc(struct amdgpu_winsys *ws, struct amdgpu_bo_sparse *bo,
          return NULL;
       }
 
-      assert(bo->num_backing_pages < DIV_ROUND_UP(bo->b.base.size, RADEON_SPARSE_PAGE_SIZE));
+      assert(bo->num_backing_pages < DIV_ROUND_UP(bo->b.base.base.size, RADEON_SPARSE_PAGE_SIZE));
 
-      size = MIN3(bo->b.base.size / 16,
+      size = MIN3(bo->b.base.base.size / 16,
                   8 * 1024 * 1024,
-                  bo->b.base.size - (uint64_t)bo->num_backing_pages * RADEON_SPARSE_PAGE_SIZE);
+                  bo->b.base.base.size - (uint64_t)bo->num_backing_pages * RADEON_SPARSE_PAGE_SIZE);
       size = MAX2(size, RADEON_SPARSE_PAGE_SIZE);
 
       buf = amdgpu_bo_create(ws, size, RADEON_SPARSE_PAGE_SIZE,
-                             bo->b.base.placement,
-                             (bo->b.base.usage & ~RADEON_FLAG_SPARSE &
+                             bo->b.base.base.placement,
+                             (bo->b.base.base.usage & ~RADEON_FLAG_SPARSE &
                               /* Set the interprocess sharing flag to disable pb_cache because
                                * amdgpu_bo_wait doesn't wait for active CS jobs.
                                */
@@ -906,7 +906,7 @@ sparse_backing_alloc(struct amdgpu_winsys *ws, struct amdgpu_bo_sparse *bo,
       }
 
       /* We might have gotten a bigger buffer than requested via caching. */
-      pages = buf->size / RADEON_SPARSE_PAGE_SIZE;
+      pages = buf->base.size / RADEON_SPARSE_PAGE_SIZE;
 
       best_backing->bo = get_real_bo(amdgpu_winsys_bo(buf));
       best_backing->num_chunks = 1;
@@ -937,7 +937,7 @@ static void
 sparse_free_backing_buffer(struct amdgpu_winsys *ws, struct amdgpu_bo_sparse *bo,
                            struct amdgpu_sparse_backing *backing)
 {
-   bo->num_backing_pages -= backing->bo->b.base.size / RADEON_SPARSE_PAGE_SIZE;
+   bo->num_backing_pages -= backing->bo->b.base.base.size / RADEON_SPARSE_PAGE_SIZE;
 
    simple_mtx_lock(&ws->bo_fence_lock);
    amdgpu_add_fences(&backing->bo->b, bo->b.num_fences, bo->b.fences);
@@ -1008,7 +1008,7 @@ sparse_backing_free(struct amdgpu_winsys *ws, struct amdgpu_bo_sparse *bo,
    }
 
    if (backing->num_chunks == 1 && backing->chunks[0].begin == 0 &&
-       backing->chunks[0].end == backing->bo->b.base.size / RADEON_SPARSE_PAGE_SIZE)
+       backing->chunks[0].end == backing->bo->b.base.base.size / RADEON_SPARSE_PAGE_SIZE)
       sparse_free_backing_buffer(ws, bo, backing);
 
    return true;
@@ -1061,11 +1061,11 @@ amdgpu_bo_sparse_create(struct amdgpu_winsys *ws, uint64_t size,
       return NULL;
 
    simple_mtx_init(&bo->lock, mtx_plain);
-   pipe_reference_init(&bo->b.base.reference, 1);
-   bo->b.base.placement = domain;
-   bo->b.base.alignment_log2 = util_logbase2(RADEON_SPARSE_PAGE_SIZE);
-   bo->b.base.usage = flags;
-   bo->b.base.size = size;
+   pipe_reference_init(&bo->b.base.base.reference, 1);
+   bo->b.base.base.placement = domain;
+   bo->b.base.base.alignment_log2 = util_logbase2(RADEON_SPARSE_PAGE_SIZE);
+   bo->b.base.base.usage = flags;
+   bo->b.base.base.size = size;
    bo->b.unique_id =  __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
    bo->b.type = AMDGPU_BO_SPARSE;
 
@@ -1116,9 +1116,9 @@ amdgpu_bo_sparse_commit(struct radeon_winsys *rws, struct pb_buffer *buf,
    int r;
 
    assert(offset % RADEON_SPARSE_PAGE_SIZE == 0);
-   assert(offset <= bo->b.base.size);
-   assert(size <= bo->b.base.size - offset);
-   assert(size % RADEON_SPARSE_PAGE_SIZE == 0 || offset + size == bo->b.base.size);
+   assert(offset <= bo->b.base.base.size);
+   assert(size <= bo->b.base.base.size - offset);
+   assert(size % RADEON_SPARSE_PAGE_SIZE == 0 || offset + size == bo->b.base.base.size);
 
    comm = bo->commitments;
    va_page = offset / RADEON_SPARSE_PAGE_SIZE;
@@ -1248,7 +1248,7 @@ amdgpu_bo_find_next_committed_memory(struct pb_buffer *buf,
    if (*range_size == 0)
       return 0;
 
-   assert(*range_size + range_offset <= bo->b.base.size);
+   assert(*range_size + range_offset <= bo->b.base.base.size);
 
    uncommitted_range_prev = uncommitted_range_next = 0;
    comm = bo->commitments;
@@ -1384,9 +1384,9 @@ amdgpu_bo_create(struct amdgpu_winsys *ws,
          return NULL;
 
       struct amdgpu_bo_slab_entry *slab_bo = container_of(entry, struct amdgpu_bo_slab_entry, entry);
-      pipe_reference_init(&slab_bo->b.base.reference, 1);
-      slab_bo->b.base.size = size;
-      assert(alignment <= 1 << slab_bo->b.base.alignment_log2);
+      pipe_reference_init(&slab_bo->b.base.base.reference, 1);
+      slab_bo->b.base.base.size = size;
+      assert(alignment <= 1 << slab_bo->b.base.base.alignment_log2);
 
       if (domain & RADEON_DOMAIN_VRAM)
          ws->slab_wasted_vram += get_slab_wasted_size(ws, slab_bo);
@@ -1508,7 +1508,7 @@ static struct pb_buffer *amdgpu_bo_from_handle(struct radeon_winsys *rws,
     * counter and return it.
     */
    if (bo) {
-      p_atomic_inc(&bo->b.base.reference.count);
+      p_atomic_inc(&bo->b.base.base.reference.count);
       simple_mtx_unlock(&ws->bo_export_table_lock);
 
       /* Release the buffer handle, because we don't need it anymore.
@@ -1562,12 +1562,12 @@ static struct pb_buffer *amdgpu_bo_from_handle(struct radeon_winsys *rws,
    }
 
    /* Initialize the structure. */
-   pipe_reference_init(&bo->b.base.reference, 1);
-   bo->b.base.placement = initial;
-   bo->b.base.alignment_log2 = util_logbase2(info.phys_alignment ?
+   pipe_reference_init(&bo->b.base.base.reference, 1);
+   bo->b.base.base.placement = initial;
+   bo->b.base.base.alignment_log2 = util_logbase2(info.phys_alignment ?
 				info.phys_alignment : ws->info.gart_page_size);
-   bo->b.base.usage = flags;
-   bo->b.base.size = result.alloc_size;
+   bo->b.base.base.usage = flags;
+   bo->b.base.base.size = result.alloc_size;
    bo->b.type = AMDGPU_BO_REAL;
    bo->b.unique_id = __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
    simple_mtx_init(&bo->lock, mtx_plain);
@@ -1575,10 +1575,10 @@ static struct pb_buffer *amdgpu_bo_from_handle(struct radeon_winsys *rws,
    bo->va_handle = va_handle;
    bo->is_shared = true;
 
-   if (bo->b.base.placement & RADEON_DOMAIN_VRAM)
-      ws->allocated_vram += align64(bo->b.base.size, ws->info.gart_page_size);
-   else if (bo->b.base.placement & RADEON_DOMAIN_GTT)
-      ws->allocated_gtt += align64(bo->b.base.size, ws->info.gart_page_size);
+   if (bo->b.base.base.placement & RADEON_DOMAIN_VRAM)
+      ws->allocated_vram += align64(bo->b.base.base.size, ws->info.gart_page_size);
+   else if (bo->b.base.base.placement & RADEON_DOMAIN_GTT)
+      ws->allocated_gtt += align64(bo->b.base.base.size, ws->info.gart_page_size);
 
    amdgpu_bo_export(bo->bo, amdgpu_bo_handle_type_kms, &bo->kms_handle);
 
@@ -1717,10 +1717,10 @@ static struct pb_buffer *amdgpu_bo_from_ptr(struct radeon_winsys *rws,
 
     /* Initialize it. */
     bo->is_user_ptr = true;
-    pipe_reference_init(&bo->b.base.reference, 1);
-    bo->b.base.placement = RADEON_DOMAIN_GTT;
-    bo->b.base.alignment_log2 = 0;
-    bo->b.base.size = size;
+    pipe_reference_init(&bo->b.base.base.reference, 1);
+    bo->b.base.base.placement = RADEON_DOMAIN_GTT;
+    bo->b.base.base.alignment_log2 = 0;
+    bo->b.base.base.size = size;
     bo->b.type = AMDGPU_BO_REAL;
     bo->b.unique_id = __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
     simple_mtx_init(&bo->lock, mtx_plain);
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
index af7213e7fa253..9a51a54c07674 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
@@ -151,7 +151,7 @@ static struct amdgpu_bo_real_reusable *get_real_bo_reusable(struct amdgpu_winsys
 
 static struct amdgpu_bo_sparse *get_sparse_bo(struct amdgpu_winsys_bo *bo)
 {
-   assert(bo->type == AMDGPU_BO_SPARSE && bo->base.usage & RADEON_FLAG_SPARSE);
+   assert(bo->type == AMDGPU_BO_SPARSE && bo->base.base.usage & RADEON_FLAG_SPARSE);
    return (struct amdgpu_bo_sparse*)bo;
 }
 
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
index 5625c7963d97a..766cc3df7c501 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
@@ -795,7 +795,7 @@ static bool amdgpu_get_new_ib(struct amdgpu_winsys *ws,
 
    /* Allocate a new buffer for IBs if the current buffer is all used. */
    if (!main_ib->big_buffer ||
-       main_ib->used_ib_space + ib_size > main_ib->big_buffer->size) {
+       main_ib->used_ib_space + ib_size > main_ib->big_buffer->base.size) {
       if (!amdgpu_ib_new_buffer(ws, main_ib, cs))
          return false;
    }
@@ -814,7 +814,7 @@ static bool amdgpu_get_new_ib(struct amdgpu_winsys *ws,
 
    cs->csc->ib_main_addr = rcs->current.buf;
 
-   ib_size = main_ib->big_buffer->size - main_ib->used_ib_space;
+   ib_size = main_ib->big_buffer->base.size - main_ib->used_ib_space;
    rcs->current.max_dw = ib_size / 4 - amdgpu_cs_epilog_dws(cs);
    return true;
 }
@@ -1146,7 +1146,7 @@ static bool amdgpu_cs_check_space(struct radeon_cmdbuf *rcs, unsigned dw)
    rcs->current.cdw = 0;
 
    rcs->current.buf = (uint32_t*)(main_ib->big_buffer_cpu_ptr + main_ib->used_ib_space);
-   rcs->current.max_dw = main_ib->big_buffer->size / 4 - cs_epilog_dw;
+   rcs->current.max_dw = main_ib->big_buffer->base.size / 4 - cs_epilog_dw;
 
    amdgpu_cs_add_buffer(rcs, main_ib->big_buffer,
                         RADEON_USAGE_READ | RADEON_PRIO_IB, 0);
@@ -1163,7 +1163,7 @@ static unsigned amdgpu_cs_get_buffer_list(struct radeon_cmdbuf *rcs,
 
     if (list) {
         for (unsigned i = 0; i < num_real_buffers; i++) {
-            list[i].bo_size = real_buffers->buffers[i].bo->base.size;
+            list[i].bo_size = real_buffers->buffers[i].bo->base.base.size;
             list[i].vm_address =
                amdgpu_va_get_start_addr(get_real_bo(real_buffers->buffers[i].bo)->va_handle);
             list[i].priority_usage = real_buffers->buffers[i].usage;
diff --git a/src/gallium/winsys/radeon/drm/radeon_drm_bo.c b/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
index 034380ac9e7fe..5c73f202c6791 100644
--- a/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
+++ b/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
@@ -339,7 +339,7 @@ void radeon_bo_destroy(void *winsys, struct pb_buffer *_buf)
 
    mtx_lock(&rws->bo_handles_mutex);
    /* radeon_winsys_bo_from_handle might have revived the bo */
-   if (pipe_is_referenced(&bo->base.reference)) {
+   if (pipe_is_referenced(&bo->base.base.reference)) {
       mtx_unlock(&rws->bo_handles_mutex);
       return;
    }
@@ -351,7 +351,7 @@ void radeon_bo_destroy(void *winsys, struct pb_buffer *_buf)
    mtx_unlock(&rws->bo_handles_mutex);
 
    if (bo->u.real.ptr)
-      os_munmap(bo->u.real.ptr, bo->base.size);
+      os_munmap(bo->u.real.ptr, bo->base.base.size);
 
    if (rws->info.r600_has_virtual_memory) {
       if (rws->va_unmap_working) {
@@ -369,14 +369,14 @@ void radeon_bo_destroy(void *winsys, struct pb_buffer *_buf)
                                  sizeof(va)) != 0 &&
              va.operation == RADEON_VA_RESULT_ERROR) {
             fprintf(stderr, "radeon: Failed to deallocate virtual address for buffer:\n");
-            fprintf(stderr, "radeon:    size      : %"PRIu64" bytes\n", bo->base.size);
+            fprintf(stderr, "radeon:    size      : %"PRIu64" bytes\n", bo->base.base.size);
             fprintf(stderr, "radeon:    va        : 0x%"PRIx64"\n", bo->va);
          }
       }
 
       radeon_bomgr_free_va(&rws->info,
                            bo->va < rws->vm32.end ? &rws->vm32 : &rws->vm64,
-                           bo->va, bo->base.size);
+                           bo->va, bo->base.base.size);
    }
 
    /* Close object. */
@@ -386,15 +386,15 @@ void radeon_bo_destroy(void *winsys, struct pb_buffer *_buf)
    mtx_destroy(&bo->u.real.map_mutex);
 
    if (bo->initial_domain & RADEON_DOMAIN_VRAM)
-      rws->allocated_vram -= align(bo->base.size, rws->info.gart_page_size);
+      rws->allocated_vram -= align(bo->base.base.size, rws->info.gart_page_size);
    else if (bo->initial_domain & RADEON_DOMAIN_GTT)
-      rws->allocated_gtt -= align(bo->base.size, rws->info.gart_page_size);
+      rws->allocated_gtt -= align(bo->base.base.size, rws->info.gart_page_size);
 
    if (bo->u.real.map_count >= 1) {
       if (bo->initial_domain & RADEON_DOMAIN_VRAM)
-         bo->rws->mapped_vram -= bo->base.size;
+         bo->rws->mapped_vram -= bo->base.base.size;
       else
-         bo->rws->mapped_gtt -= bo->base.size;
+         bo->rws->mapped_gtt -= bo->base.base.size;
       bo->rws->num_mapped_buffers--;
    }
 
@@ -440,7 +440,7 @@ void *radeon_bo_do_map(struct radeon_bo *bo)
    }
    args.handle = bo->handle;
    args.offset = 0;
-   args.size = (uint64_t)bo->base.size;
+   args.size = (uint64_t)bo->base.base.size;
    if (drmCommandWriteRead(bo->rws->fd,
                            DRM_RADEON_GEM_MMAP,
                            &args,
@@ -469,9 +469,9 @@ void *radeon_bo_do_map(struct radeon_bo *bo)
    bo->u.real.map_count = 1;
 
    if (bo->initial_domain & RADEON_DOMAIN_VRAM)
-      bo->rws->mapped_vram += bo->base.size;
+      bo->rws->mapped_vram += bo->base.base.size;
    else
-      bo->rws->mapped_gtt += bo->base.size;
+      bo->rws->mapped_gtt += bo->base.base.size;
    bo->rws->num_mapped_buffers++;
 
    mtx_unlock(&bo->u.real.map_mutex);
@@ -583,13 +583,13 @@ static void radeon_bo_unmap(struct radeon_winsys *rws, struct pb_buffer *_buf)
       return; /* it's been mapped multiple times */
    }
 
-   os_munmap(bo->u.real.ptr, bo->base.size);
+   os_munmap(bo->u.real.ptr, bo->base.base.size);
    bo->u.real.ptr = NULL;
 
    if (bo->initial_domain & RADEON_DOMAIN_VRAM)
-      bo->rws->mapped_vram -= bo->base.size;
+      bo->rws->mapped_vram -= bo->base.base.size;
    else
-      bo->rws->mapped_gtt -= bo->base.size;
+      bo->rws->mapped_gtt -= bo->base.base.size;
    bo->rws->num_mapped_buffers--;
 
    mtx_unlock(&bo->u.real.map_mutex);
@@ -649,10 +649,10 @@ static struct radeon_bo *radeon_create_bo(struct radeon_drm_winsys *rws,
    if (!bo)
       return NULL;
 
-   pipe_reference_init(&bo->base.reference, 1);
-   bo->base.alignment_log2 = util_logbase2(alignment);
-   bo->base.usage = 0;
-   bo->base.size = size;
+   pipe_reference_init(&bo->base.base.reference, 1);
+   bo->base.base.alignment_log2 = util_logbase2(alignment);
+   bo->base.base.usage = 0;
+   bo->base.base.size = size;
    bo->base.vtbl = &radeon_bo_vtbl;
    bo->rws = rws;
    bo->handle = args.handle;
@@ -772,7 +772,7 @@ struct pb_slab *radeon_bo_slab_alloc(void *priv, unsigned heap,
 
    assert(slab->buffer->handle);
 
-   slab->base.num_entries = slab->buffer->base.size / entry_size;
+   slab->base.num_entries = slab->buffer->base.base.size / entry_size;
    slab->base.num_free = slab->base.num_entries;
    slab->base.group_index = group_index;
    slab->base.entry_size = entry_size;
@@ -787,9 +787,9 @@ struct pb_slab *radeon_bo_slab_alloc(void *priv, unsigned heap,
    for (unsigned i = 0; i < slab->base.num_entries; ++i) {
       struct radeon_bo *bo = &slab->entries[i];
 
-      bo->base.alignment_log2 = util_logbase2(entry_size);
-      bo->base.usage = slab->buffer->base.usage;
-      bo->base.size = entry_size;
+      bo->base.base.alignment_log2 = util_logbase2(entry_size);
+      bo->base.base.usage = slab->buffer->base.base.usage;
+      bo->base.base.size = entry_size;
       bo->base.vtbl = &radeon_winsys_bo_slab_vtbl;
       bo->rws = ws;
       bo->va = slab->buffer->va + i * entry_size;
@@ -1024,7 +1024,7 @@ radeon_winsys_bo_create(struct radeon_winsys *rws,
 
       bo = container_of(entry, struct radeon_bo, u.slab.entry);
 
-      pipe_reference_init(&bo->base.reference, 1);
+      pipe_reference_init(&bo->base.base.reference, 1);
 
       return &bo->base;
    }
@@ -1112,10 +1112,10 @@ static struct pb_buffer *radeon_winsys_bo_from_ptr(struct radeon_winsys *rws,
    mtx_lock(&ws->bo_handles_mutex);
 
    /* Initialize it. */
-   pipe_reference_init(&bo->base.reference, 1);
+   pipe_reference_init(&bo->base.base.reference, 1);
    bo->handle = args.handle;
-   bo->base.alignment_log2 = 0;
-   bo->base.size = size;
+   bo->base.base.alignment_log2 = 0;
+   bo->base.base.size = size;
    bo->base.vtbl = &radeon_bo_vtbl;
    bo->rws = ws;
    bo->user_ptr = pointer;
@@ -1131,7 +1131,7 @@ static struct pb_buffer *radeon_winsys_bo_from_ptr(struct radeon_winsys *rws,
    if (ws->info.r600_has_virtual_memory) {
       struct drm_radeon_gem_va va;
 
-      bo->va = radeon_bomgr_find_va64(ws, bo->base.size, 1 << 20);
+      bo->va = radeon_bomgr_find_va64(ws, bo->base.base.size, 1 << 20);
 
       va.handle = bo->handle;
       va.operation = RADEON_VA_MAP;
@@ -1162,7 +1162,7 @@ static struct pb_buffer *radeon_winsys_bo_from_ptr(struct radeon_winsys *rws,
       mtx_unlock(&ws->bo_handles_mutex);
    }
 
-   ws->allocated_gtt += align(bo->base.size, ws->info.gart_page_size);
+   ws->allocated_gtt += align(bo->base.base.size, ws->info.gart_page_size);
 
    return (struct pb_buffer*)bo;
 }
@@ -1202,7 +1202,7 @@ static struct pb_buffer *radeon_winsys_bo_from_handle(struct radeon_winsys *rws,
 
    if (bo) {
       /* Increase the refcount. */
-      p_atomic_inc(&bo->base.reference.count);
+      p_atomic_inc(&bo->base.base.reference.count);
       goto done;
    }
 
@@ -1242,9 +1242,9 @@ static struct pb_buffer *radeon_winsys_bo_from_handle(struct radeon_winsys *rws,
    bo->handle = handle;
 
    /* Initialize it. */
-   pipe_reference_init(&bo->base.reference, 1);
-   bo->base.alignment_log2 = 0;
-   bo->base.size = (unsigned) size;
+   pipe_reference_init(&bo->base.base.reference, 1);
+   bo->base.base.alignment_log2 = 0;
+   bo->base.base.size = (unsigned) size;
    bo->base.vtbl = &radeon_bo_vtbl;
    bo->rws = ws;
    bo->va = 0;
@@ -1262,7 +1262,7 @@ done:
    if (ws->info.r600_has_virtual_memory && !bo->va) {
       struct drm_radeon_gem_va va;
 
-      bo->va = radeon_bomgr_find_va64(ws, bo->base.size, vm_alignment);
+      bo->va = radeon_bomgr_find_va64(ws, bo->base.base.size, vm_alignment);
 
       va.handle = bo->handle;
       va.operation = RADEON_VA_MAP;
@@ -1296,9 +1296,9 @@ done:
    bo->initial_domain = radeon_bo_get_initial_domain((void*)bo);
 
    if (bo->initial_domain & RADEON_DOMAIN_VRAM)
-      ws->allocated_vram += align(bo->base.size, ws->info.gart_page_size);
+      ws->allocated_vram += align(bo->base.base.size, ws->info.gart_page_size);
    else if (bo->initial_domain & RADEON_DOMAIN_GTT)
-      ws->allocated_gtt += align(bo->base.size, ws->info.gart_page_size);
+      ws->allocated_gtt += align(bo->base.base.size, ws->info.gart_page_size);
 
    return (struct pb_buffer*)bo;
 
diff --git a/src/gallium/winsys/radeon/drm/radeon_drm_cs.c b/src/gallium/winsys/radeon/drm/radeon_drm_cs.c
index fa324f4c7d71f..2cdd5780eefd4 100644
--- a/src/gallium/winsys/radeon/drm/radeon_drm_cs.c
+++ b/src/gallium/winsys/radeon/drm/radeon_drm_cs.c
@@ -415,9 +415,9 @@ static unsigned radeon_drm_cs_add_buffer(struct radeon_cmdbuf *rcs,
    cs->csc->relocs_bo[index].u.real.priority_usage |= priority;
 
    if (added_domains & RADEON_DOMAIN_VRAM)
-      rcs->used_vram_kb += bo->base.size / 1024;
+      rcs->used_vram_kb += bo->base.base.size / 1024;
    else if (added_domains & RADEON_DOMAIN_GTT)
-      rcs->used_gart_kb += bo->base.size / 1024;
+      rcs->used_gart_kb += bo->base.base.size / 1024;
 
    return index;
 }
@@ -483,7 +483,7 @@ static unsigned radeon_drm_cs_get_buffer_list(struct radeon_cmdbuf *rcs,
 
    if (list) {
       for (i = 0; i < cs->csc->num_relocs; i++) {
-         list[i].bo_size = cs->csc->relocs_bo[i].bo->base.size;
+         list[i].bo_size = cs->csc->relocs_bo[i].bo->base.base.size;
          list[i].vm_address = cs->csc->relocs_bo[i].bo->va;
          list[i].priority_usage = cs->csc->relocs_bo[i].u.real.priority_usage;
       }
diff --git a/src/gallium/winsys/svga/drm/pb_buffer_simple_fenced.c b/src/gallium/winsys/svga/drm/pb_buffer_simple_fenced.c
index 5315fd7a95487..321b8cb444bb8 100644
--- a/src/gallium/winsys/svga/drm/pb_buffer_simple_fenced.c
+++ b/src/gallium/winsys/svga/drm/pb_buffer_simple_fenced.c
@@ -175,8 +175,8 @@ fenced_manager_dump_locked(struct fenced_manager *fenced_mgr)
       assert(!fenced_buf->fence);
       debug_printf("%10p %"PRIu64" %8u %7s\n",
                    (void *) fenced_buf,
-                   fenced_buf->base.size,
-                   p_atomic_read(&fenced_buf->base.reference.count),
+                   fenced_buf->base.base.size,
+                   p_atomic_read(&fenced_buf->base.base.reference.count),
                    fenced_buf->buffer ? "gpu" : "none");
       curr = next;
       next = curr->next;
@@ -191,8 +191,8 @@ fenced_manager_dump_locked(struct fenced_manager *fenced_mgr)
       signaled = ops->fence_signalled(ops, fenced_buf->fence, 0);
       debug_printf("%10p %"PRIu64" %8u %7s %10p %s\n",
                    (void *) fenced_buf,
-                   fenced_buf->base.size,
-                   p_atomic_read(&fenced_buf->base.reference.count),
+                   fenced_buf->base.base.size,
+                   p_atomic_read(&fenced_buf->base.base.reference.count),
                    "gpu",
                    (void *) fenced_buf->fence,
                    signaled == 0 ? "y" : "n");
@@ -209,7 +209,7 @@ static inline void
 fenced_buffer_destroy_locked(struct fenced_manager *fenced_mgr,
                              struct fenced_buffer *fenced_buf)
 {
-   assert(!pipe_is_referenced(&fenced_buf->base.reference));
+   assert(!pipe_is_referenced(&fenced_buf->base.base.reference));
 
    assert(!fenced_buf->fence);
    assert(fenced_buf->head.prev);
@@ -233,11 +233,11 @@ static inline void
 fenced_buffer_add_locked(struct fenced_manager *fenced_mgr,
                          struct fenced_buffer *fenced_buf)
 {
-   assert(pipe_is_referenced(&fenced_buf->base.reference));
+   assert(pipe_is_referenced(&fenced_buf->base.base.reference));
    assert(fenced_buf->flags & PB_USAGE_GPU_READ_WRITE);
    assert(fenced_buf->fence);
 
-   p_atomic_inc(&fenced_buf->base.reference.count);
+   p_atomic_inc(&fenced_buf->base.base.reference.count);
 
    list_del(&fenced_buf->head);
    assert(fenced_mgr->num_unfenced);
@@ -275,7 +275,7 @@ fenced_buffer_remove_locked(struct fenced_manager *fenced_mgr,
    list_addtail(&fenced_buf->head, &fenced_mgr->unfenced);
    ++fenced_mgr->num_unfenced;
 
-   if (p_atomic_dec_zero(&fenced_buf->base.reference.count)) {
+   if (p_atomic_dec_zero(&fenced_buf->base.base.reference.count)) {
       fenced_buffer_destroy_locked(fenced_mgr, fenced_buf);
       return true;
    }
@@ -301,7 +301,7 @@ fenced_buffer_finish_locked(struct fenced_manager *fenced_mgr,
    debug_warning("waiting for GPU");
 #endif
 
-   assert(pipe_is_referenced(&fenced_buf->base.reference));
+   assert(pipe_is_referenced(&fenced_buf->base.base.reference));
    assert(fenced_buf->fence);
 
    if(fenced_buf->fence) {
@@ -317,7 +317,7 @@ fenced_buffer_finish_locked(struct fenced_manager *fenced_mgr,
 
       mtx_lock(&fenced_mgr->mutex);
 
-      assert(pipe_is_referenced(&fenced_buf->base.reference));
+      assert(pipe_is_referenced(&fenced_buf->base.base.reference));
 
       /*
        * Only proceed if the fence object didn't change in the meanwhile.
@@ -506,7 +506,7 @@ fenced_buffer_destroy(void *winsys, struct pb_buffer *buf)
    struct fenced_buffer *fenced_buf = fenced_buffer(buf);
    struct fenced_manager *fenced_mgr = fenced_buf->mgr;
 
-   assert(!pipe_is_referenced(&fenced_buf->base.reference));
+   assert(!pipe_is_referenced(&fenced_buf->base.base.reference));
 
    mtx_lock(&fenced_mgr->mutex);
 
@@ -651,7 +651,7 @@ fenced_buffer_fence(struct pb_buffer *buf,
 
    mtx_lock(&fenced_mgr->mutex);
 
-   assert(pipe_is_referenced(&fenced_buf->base.reference));
+   assert(pipe_is_referenced(&fenced_buf->base.base.reference));
    assert(fenced_buf->buffer);
 
    if(fence != fenced_buf->fence) {
@@ -730,10 +730,10 @@ fenced_bufmgr_create_buffer(struct pb_manager *mgr,
    if(!fenced_buf)
       goto no_buffer;
 
-   pipe_reference_init(&fenced_buf->base.reference, 1);
-   fenced_buf->base.alignment_log2 = util_logbase2(desc->alignment);
-   fenced_buf->base.usage = desc->usage;
-   fenced_buf->base.size = size;
+   pipe_reference_init(&fenced_buf->base.base.reference, 1);
+   fenced_buf->base.base.alignment_log2 = util_logbase2(desc->alignment);
+   fenced_buf->base.base.usage = desc->usage;
+   fenced_buf->base.base.size = size;
    fenced_buf->size = size;
 
    fenced_buf->base.vtbl = &fenced_buffer_vtbl;
diff --git a/src/gallium/winsys/svga/drm/vmw_buffer.c b/src/gallium/winsys/svga/drm/vmw_buffer.c
index 618225d2ae009..20c45694c16de 100644
--- a/src/gallium/winsys/svga/drm/vmw_buffer.c
+++ b/src/gallium/winsys/svga/drm/vmw_buffer.c
@@ -130,7 +130,7 @@ vmw_dma_buffer_map(struct pb_buffer *_buf,
    if (!buf->map)
       return NULL;
 
-   if ((_buf->usage & VMW_BUFFER_USAGE_SYNC) &&
+   if ((_buf->base.usage & VMW_BUFFER_USAGE_SYNC) &&
        !(flags & PB_USAGE_UNSYNCHRONIZED)) {
       ret = vmw_ioctl_syncforcpu(buf->region,
                                  !!(flags & PB_USAGE_DONTBLOCK),
@@ -151,7 +151,7 @@ vmw_dma_buffer_unmap(struct pb_buffer *_buf)
    struct vmw_dma_buffer *buf = vmw_pb_to_dma_buffer(_buf);
    enum pb_usage_flags flags = buf->map_flags;
 
-   if ((_buf->usage & VMW_BUFFER_USAGE_SYNC) &&
+   if ((_buf->base.usage & VMW_BUFFER_USAGE_SYNC) &&
        !(flags & PB_USAGE_UNSYNCHRONIZED)) {
       vmw_ioctl_releasefromcpu(buf->region,
                                !(flags & PB_USAGE_CPU_WRITE),
@@ -220,12 +220,12 @@ vmw_dma_bufmgr_create_buffer(struct pb_manager *_mgr,
    if(!buf)
       goto error1;
 
-   pipe_reference_init(&buf->base.reference, 1);
-   buf->base.alignment_log2 = util_logbase2(pb_desc->alignment);
-   buf->base.usage = pb_desc->usage & ~VMW_BUFFER_USAGE_SHARED;
+   pipe_reference_init(&buf->base.base.reference, 1);
+   buf->base.base.alignment_log2 = util_logbase2(pb_desc->alignment);
+   buf->base.base.usage = pb_desc->usage & ~VMW_BUFFER_USAGE_SHARED;
    buf->base.vtbl = &vmw_dma_buffer_vtbl;
    buf->mgr = mgr;
-   buf->base.size = size;
+   buf->base.base.size = size;
    if ((pb_desc->usage & VMW_BUFFER_USAGE_SHARED) && desc->region) {
       buf->region = desc->region;
    } else {
diff --git a/src/gallium/winsys/svga/drm/vmw_context.c b/src/gallium/winsys/svga/drm/vmw_context.c
index dcdab125d0a76..4dde2e5404e70 100644
--- a/src/gallium/winsys/svga/drm/vmw_context.c
+++ b/src/gallium/winsys/svga/drm/vmw_context.c
@@ -407,7 +407,7 @@ vmw_swc_region_relocation(struct svga_winsys_context *swc,
    ++vswc->region.staged;
 
    if (vmw_swc_add_validate_buffer(vswc, reloc->buffer, flags)) {
-      vswc->seen_regions += reloc->buffer->size;
+      vswc->seen_regions += reloc->buffer->base.size;
       if ((swc->hints & SVGA_HINT_FLAG_CAN_PRE_FLUSH) &&
           vswc->seen_regions >= VMW_GMR_POOL_SIZE/5)
          vswc->preemptive_flush = true;
@@ -449,7 +449,7 @@ vmw_swc_mob_relocation(struct svga_winsys_context *swc,
    }
 
    if (vmw_swc_add_validate_buffer(vswc, pb_buffer, flags)) {
-      vswc->seen_mobs += pb_buffer->size;
+      vswc->seen_mobs += pb_buffer->base.size;
 
       if ((swc->hints & SVGA_HINT_FLAG_CAN_PRE_FLUSH) &&
           vswc->seen_mobs >=
-- 
GitLab


From 5d1df822c442f2ff3a884e5aa7d4b9df4fbfe0ab Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 9 Dec 2023 15:31:27 -0500
Subject: [PATCH 20/36] gallium/pb_cache: switch to pb_buffer_lean

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 .../auxiliary/pipebuffer/pb_bufmgr_cache.c    | 10 ++---
 src/gallium/auxiliary/pipebuffer/pb_cache.c   | 38 +++++++++----------
 src/gallium/auxiliary/pipebuffer/pb_cache.h   | 14 +++----
 src/gallium/drivers/zink/zink_bo.c            |  2 +-
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c     |  4 +-
 src/gallium/winsys/radeon/drm/radeon_drm_bo.c | 26 ++++++-------
 src/gallium/winsys/radeon/drm/radeon_drm_bo.h |  4 +-
 7 files changed, 49 insertions(+), 49 deletions(-)

diff --git a/src/gallium/auxiliary/pipebuffer/pb_bufmgr_cache.c b/src/gallium/auxiliary/pipebuffer/pb_bufmgr_cache.c
index 8bf121f284228..0f62fc60d2cf3 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_bufmgr_cache.c
+++ b/src/gallium/auxiliary/pipebuffer/pb_bufmgr_cache.c
@@ -94,9 +94,9 @@ pb_cache_manager_remove_buffer(struct pb_buffer *pb_buf)
  * Actually destroy the buffer.
  */
 static void
-_pb_cache_buffer_destroy(void *winsys, struct pb_buffer *pb_buf)
+_pb_cache_buffer_destroy(void *winsys, struct pb_buffer_lean *pb_buf)
 {
-   struct pb_cache_buffer *buf = pb_cache_buffer(pb_buf);
+   struct pb_cache_buffer *buf = pb_cache_buffer((struct pb_buffer*)pb_buf);
 
    assert(!pipe_is_referenced(&buf->base.base.reference));
    pb_reference(&buf->buffer, NULL);
@@ -178,9 +178,9 @@ pb_cache_buffer_vtbl = {
 
 
 static bool
-pb_cache_can_reclaim_buffer(void *winsys, struct pb_buffer *_buf)
+pb_cache_can_reclaim_buffer(void *winsys, struct pb_buffer_lean *_buf)
 {
-   struct pb_cache_buffer *buf = pb_cache_buffer(_buf);
+   struct pb_cache_buffer *buf = pb_cache_buffer((struct pb_buffer*)_buf);
 
    if (buf->mgr->provider->is_buffer_busy) {
       if (buf->mgr->provider->is_buffer_busy(buf->mgr->provider, buf->buffer))
@@ -244,7 +244,7 @@ pb_cache_manager_create_buffer(struct pb_manager *_mgr,
    
    buf->base.vtbl = &pb_cache_buffer_vtbl;
    buf->mgr = mgr;
-   pb_cache_init_entry(&mgr->cache, &buf->cache_entry, &buf->base, 0);
+   pb_cache_init_entry(&mgr->cache, &buf->cache_entry, &buf->base.base, 0);
    
    return &buf->base;
 }
diff --git a/src/gallium/auxiliary/pipebuffer/pb_cache.c b/src/gallium/auxiliary/pipebuffer/pb_cache.c
index fc6a17a36b3a7..e8986d59c616d 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_cache.c
+++ b/src/gallium/auxiliary/pipebuffer/pb_cache.c
@@ -60,14 +60,14 @@ static void
 destroy_buffer_locked(struct pb_cache_entry *entry)
 {
    struct pb_cache *mgr = entry->mgr;
-   struct pb_buffer *buf = entry->buffer;
+   struct pb_buffer_lean *buf = entry->buffer;
 
-   assert(!pipe_is_referenced(&buf->base.reference));
+   assert(!pipe_is_referenced(&buf->reference));
    if (list_is_linked(&entry->head)) {
       list_del(&entry->head);
       assert(mgr->num_buffers);
       --mgr->num_buffers;
-      mgr->cache_size -= buf->base.size;
+      mgr->cache_size -= buf->size;
    }
    mgr->destroy_buffer(mgr->winsys, buf);
 }
@@ -107,11 +107,11 @@ pb_cache_add_buffer(struct pb_cache_entry *entry)
 {
    struct pb_cache *mgr = entry->mgr;
    struct list_head *cache = &mgr->buckets[entry->bucket_index];
-   struct pb_buffer *buf = entry->buffer;
+   struct pb_buffer_lean *buf = entry->buffer;
    unsigned i;
 
    simple_mtx_lock(&mgr->mutex);
-   assert(!pipe_is_referenced(&buf->base.reference));
+   assert(!pipe_is_referenced(&buf->reference));
 
    unsigned current_time_ms = time_get_ms(mgr);
 
@@ -119,7 +119,7 @@ pb_cache_add_buffer(struct pb_cache_entry *entry)
       release_expired_buffers_locked(&mgr->buckets[i], current_time_ms);
 
    /* Directly release any buffer that exceeds the limit. */
-   if (mgr->cache_size + buf->base.size > mgr->max_cache_size) {
+   if (mgr->cache_size + buf->size > mgr->max_cache_size) {
       mgr->destroy_buffer(mgr->winsys, buf);
       simple_mtx_unlock(&mgr->mutex);
       return;
@@ -128,7 +128,7 @@ pb_cache_add_buffer(struct pb_cache_entry *entry)
    entry->start_ms = time_get_ms(mgr);
    list_addtail(&entry->head, cache);
    ++mgr->num_buffers;
-   mgr->cache_size += buf->base.size;
+   mgr->cache_size += buf->size;
    simple_mtx_unlock(&mgr->mutex);
 }
 
@@ -142,20 +142,20 @@ pb_cache_is_buffer_compat(struct pb_cache_entry *entry,
                           pb_size size, unsigned alignment, unsigned usage)
 {
    struct pb_cache *mgr = entry->mgr;
-   struct pb_buffer *buf = entry->buffer;
+   struct pb_buffer_lean *buf = entry->buffer;
 
-   if (!pb_check_usage(usage, buf->base.usage))
+   if (!pb_check_usage(usage, buf->usage))
       return 0;
 
    /* be lenient with size */
-   if (buf->base.size < size ||
-       buf->base.size > (unsigned) (mgr->size_factor * size))
+   if (buf->size < size ||
+       buf->size > (unsigned) (mgr->size_factor * size))
       return 0;
 
    if (usage & mgr->bypass_usage)
       return 0;
 
-   if (!pb_check_alignment(alignment, 1u << buf->base.alignment_log2))
+   if (!pb_check_alignment(alignment, 1u << buf->alignment_log2))
       return 0;
 
    return mgr->can_reclaim(mgr->winsys, buf) ? 1 : -1;
@@ -165,7 +165,7 @@ pb_cache_is_buffer_compat(struct pb_cache_entry *entry,
  * Find a compatible buffer in the cache, return it, and remove it
  * from the cache.
  */
-struct pb_buffer *
+struct pb_buffer_lean *
 pb_cache_reclaim_buffer(struct pb_cache *mgr, pb_size size,
                         unsigned alignment, unsigned usage,
                         unsigned bucket_index)
@@ -226,14 +226,14 @@ pb_cache_reclaim_buffer(struct pb_cache *mgr, pb_size size,
 
    /* found a compatible buffer, return it */
    if (entry) {
-      struct pb_buffer *buf = entry->buffer;
+      struct pb_buffer_lean *buf = entry->buffer;
 
-      mgr->cache_size -= buf->base.size;
+      mgr->cache_size -= buf->size;
       list_del(&entry->head);
       --mgr->num_buffers;
       simple_mtx_unlock(&mgr->mutex);
       /* Increase refcount */
-      pipe_reference_init(&buf->base.reference, 1);
+      pipe_reference_init(&buf->reference, 1);
       return buf;
    }
 
@@ -272,7 +272,7 @@ pb_cache_release_all_buffers(struct pb_cache *mgr)
 
 void
 pb_cache_init_entry(struct pb_cache *mgr, struct pb_cache_entry *entry,
-                    struct pb_buffer *buf, unsigned bucket_index)
+                    struct pb_buffer_lean *buf, unsigned bucket_index)
 {
    assert(bucket_index < mgr->num_heaps);
 
@@ -305,8 +305,8 @@ pb_cache_init(struct pb_cache *mgr, unsigned num_heaps,
               unsigned usecs, float size_factor,
               unsigned bypass_usage, uint64_t maximum_cache_size,
               void *winsys,
-              void (*destroy_buffer)(void *winsys, struct pb_buffer *buf),
-              bool (*can_reclaim)(void *winsys, struct pb_buffer *buf))
+              void (*destroy_buffer)(void *winsys, struct pb_buffer_lean *buf),
+              bool (*can_reclaim)(void *winsys, struct pb_buffer_lean *buf))
 {
    unsigned i;
 
diff --git a/src/gallium/auxiliary/pipebuffer/pb_cache.h b/src/gallium/auxiliary/pipebuffer/pb_cache.h
index cf61873b72d62..7eb923fab3e62 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_cache.h
+++ b/src/gallium/auxiliary/pipebuffer/pb_cache.h
@@ -40,7 +40,7 @@
 struct pb_cache_entry
 {
    struct list_head head;
-   struct pb_buffer *buffer; /**< Pointer to the structure this is part of. */
+   struct pb_buffer_lean *buffer; /**< Pointer to the structure this is part of. */
    struct pb_cache *mgr;
    unsigned start_ms; /**< Cached start time */
    unsigned bucket_index;
@@ -64,23 +64,23 @@ struct pb_cache
    unsigned bypass_usage;
    float size_factor;
 
-   void (*destroy_buffer)(void *winsys, struct pb_buffer *buf);
-   bool (*can_reclaim)(void *winsys, struct pb_buffer *buf);
+   void (*destroy_buffer)(void *winsys, struct pb_buffer_lean *buf);
+   bool (*can_reclaim)(void *winsys, struct pb_buffer_lean *buf);
 };
 
 void pb_cache_add_buffer(struct pb_cache_entry *entry);
-struct pb_buffer *pb_cache_reclaim_buffer(struct pb_cache *mgr, pb_size size,
+struct pb_buffer_lean *pb_cache_reclaim_buffer(struct pb_cache *mgr, pb_size size,
                                           unsigned alignment, unsigned usage,
                                           unsigned bucket_index);
 unsigned pb_cache_release_all_buffers(struct pb_cache *mgr);
 void pb_cache_init_entry(struct pb_cache *mgr, struct pb_cache_entry *entry,
-                         struct pb_buffer *buf, unsigned bucket_index);
+                         struct pb_buffer_lean *buf, unsigned bucket_index);
 void pb_cache_init(struct pb_cache *mgr, unsigned num_heaps,
                    unsigned usecs, float size_factor,
                    unsigned bypass_usage, uint64_t maximum_cache_size,
                    void *winsys,
-                   void (*destroy_buffer)(void *winsys, struct pb_buffer *buf),
-                   bool (*can_reclaim)(void *winsys, struct pb_buffer *buf));
+                   void (*destroy_buffer)(void *winsys, struct pb_buffer_lean *buf),
+                   bool (*can_reclaim)(void *winsys, struct pb_buffer_lean *buf));
 void pb_cache_deinit(struct pb_cache *mgr);
 
 #endif
diff --git a/src/gallium/drivers/zink/zink_bo.c b/src/gallium/drivers/zink/zink_bo.c
index 42d902fdd1393..a0b6ef8a3475c 100644
--- a/src/gallium/drivers/zink/zink_bo.c
+++ b/src/gallium/drivers/zink/zink_bo.c
@@ -310,7 +310,7 @@ bo_create_internal(struct zink_screen *screen,
 
    if (init_pb_cache) {
       bo->u.real.use_reusable_pool = true;
-      pb_cache_init_entry(&screen->pb.bo_cache, bo->cache_entry, &bo->base, mem_type_idx);
+      pb_cache_init_entry(&screen->pb.bo_cache, bo->cache_entry, &bo->base.base, mem_type_idx);
    } else {
 #ifdef ZINK_USE_DMABUF
       list_inithead(&bo->u.real.exports);
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index 4811874b17ee5..a88b75f1cfaa8 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -492,7 +492,7 @@ static struct amdgpu_winsys_bo *amdgpu_create_bo(struct amdgpu_winsys *ws,
          return NULL;
 
       bo = &new_bo->b;
-      pb_cache_init_entry(&ws->bo_cache, &new_bo->cache_entry, &bo->b.base, heap);
+      pb_cache_init_entry(&ws->bo_cache, &new_bo->cache_entry, &bo->b.base.base, heap);
       bo->b.type = slab_backing ? AMDGPU_BO_REAL_REUSABLE_SLAB : AMDGPU_BO_REAL_REUSABLE;
    } else {
       bo = CALLOC_STRUCT(amdgpu_bo_real);
@@ -1438,7 +1438,7 @@ no_slab:
 
              /* Re-set pointers after realloc. */
              struct amdgpu_bo_real_reusable *real_bo = get_real_bo_reusable(bo);
-             real_bo->cache_entry.buffer = &bo->base;
+             real_bo->cache_entry.buffer = &bo->base.base;
           }
           return &bo->base;
        }
diff --git a/src/gallium/winsys/radeon/drm/radeon_drm_bo.c b/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
index 5c73f202c6791..90d792c993088 100644
--- a/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
+++ b/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
@@ -327,9 +327,9 @@ out:
    mtx_unlock(&heap->mutex);
 }
 
-void radeon_bo_destroy(void *winsys, struct pb_buffer *_buf)
+void radeon_bo_destroy(void *winsys, struct pb_buffer_lean *_buf)
 {
-   struct radeon_bo *bo = radeon_bo(_buf);
+   struct radeon_bo *bo = radeon_bo((struct pb_buffer*)_buf);
    struct radeon_drm_winsys *rws = bo->rws;
    struct drm_gem_close args;
 
@@ -410,7 +410,7 @@ static void radeon_bo_destroy_or_cache(void *winsys, struct pb_buffer *_buf)
    if (bo->u.real.use_reusable_pool)
       pb_cache_add_buffer(&bo->u.real.cache_entry);
    else
-      radeon_bo_destroy(NULL, _buf);
+      radeon_bo_destroy(NULL, &_buf->base);
 }
 
 void *radeon_bo_do_map(struct radeon_bo *bo)
@@ -662,7 +662,7 @@ static struct radeon_bo *radeon_create_bo(struct radeon_drm_winsys *rws,
    (void) mtx_init(&bo->u.real.map_mutex, mtx_plain);
 
    if (heap >= 0) {
-      pb_cache_init_entry(&rws->bo_cache, &bo->u.real.cache_entry, &bo->base,
+      pb_cache_init_entry(&rws->bo_cache, &bo->u.real.cache_entry, &bo->base.base,
                           heap);
    }
 
@@ -694,7 +694,7 @@ static struct radeon_bo *radeon_create_bo(struct radeon_drm_winsys *rws,
          fprintf(stderr, "radeon:    alignment : %d bytes\n", alignment);
          fprintf(stderr, "radeon:    domains   : %d\n", args.initial_domain);
          fprintf(stderr, "radeon:    va        : 0x%016llx\n", (unsigned long long)bo->va);
-         radeon_bo_destroy(NULL, &bo->base);
+         radeon_bo_destroy(NULL, &bo->base.base);
          return NULL;
       }
       mtx_lock(&rws->bo_handles_mutex);
@@ -720,21 +720,21 @@ static struct radeon_bo *radeon_create_bo(struct radeon_drm_winsys *rws,
    return bo;
 }
 
-bool radeon_bo_can_reclaim(void *winsys, struct pb_buffer *_buf)
+bool radeon_bo_can_reclaim(void *winsys, struct pb_buffer_lean *_buf)
 {
-   struct radeon_bo *bo = radeon_bo(_buf);
+   struct radeon_bo *bo = radeon_bo((struct pb_buffer*)_buf);
 
    if (radeon_bo_is_referenced_by_any_cs(bo))
       return false;
 
-   return radeon_bo_wait(winsys, _buf, 0, RADEON_USAGE_READWRITE);
+   return radeon_bo_wait(winsys, (struct pb_buffer*)_buf, 0, RADEON_USAGE_READWRITE);
 }
 
 bool radeon_bo_can_reclaim_slab(void *priv, struct pb_slab_entry *entry)
 {
    struct radeon_bo *bo = container_of(entry, struct radeon_bo, u.slab.entry);
 
-   return radeon_bo_can_reclaim(NULL, &bo->base);
+   return radeon_bo_can_reclaim(NULL, &bo->base.base);
 }
 
 static void radeon_bo_slab_destroy(void *winsys, struct pb_buffer *_buf)
@@ -1045,8 +1045,8 @@ radeon_winsys_bo_create(struct radeon_winsys *rws,
       heap = radeon_get_heap_index(domain, flags & ~RADEON_FLAG_NO_SUBALLOC);
       assert(heap >= 0 && heap < RADEON_NUM_HEAPS);
 
-      bo = radeon_bo(pb_cache_reclaim_buffer(&ws->bo_cache, size, alignment,
-                                             0, heap));
+      bo = radeon_bo((struct pb_buffer*)pb_cache_reclaim_buffer(&ws->bo_cache, size,
+                                                                alignment, 0, heap));
       if (bo)
          return &bo->base;
    }
@@ -1144,7 +1144,7 @@ static struct pb_buffer *radeon_winsys_bo_from_ptr(struct radeon_winsys *rws,
       r = drmCommandWriteRead(ws->fd, DRM_RADEON_GEM_VA, &va, sizeof(va));
       if (r && va.operation == RADEON_VA_RESULT_ERROR) {
          fprintf(stderr, "radeon: Failed to assign virtual address space\n");
-         radeon_bo_destroy(NULL, &bo->base);
+         radeon_bo_destroy(NULL, &bo->base.base);
          return NULL;
       }
       mtx_lock(&ws->bo_handles_mutex);
@@ -1275,7 +1275,7 @@ done:
       r = drmCommandWriteRead(ws->fd, DRM_RADEON_GEM_VA, &va, sizeof(va));
       if (r && va.operation == RADEON_VA_RESULT_ERROR) {
          fprintf(stderr, "radeon: Failed to assign virtual address space\n");
-         radeon_bo_destroy(NULL, &bo->base);
+         radeon_bo_destroy(NULL, &bo->base.base);
          return NULL;
       }
       mtx_lock(&ws->bo_handles_mutex);
diff --git a/src/gallium/winsys/radeon/drm/radeon_drm_bo.h b/src/gallium/winsys/radeon/drm/radeon_drm_bo.h
index 078f97568a7af..8c7a87afc0f01 100644
--- a/src/gallium/winsys/radeon/drm/radeon_drm_bo.h
+++ b/src/gallium/winsys/radeon/drm/radeon_drm_bo.h
@@ -56,8 +56,8 @@ struct radeon_slab {
    struct radeon_bo *entries;
 };
 
-void radeon_bo_destroy(void *winsys, struct pb_buffer *_buf);
-bool radeon_bo_can_reclaim(void *winsys, struct pb_buffer *_buf);
+void radeon_bo_destroy(void *winsys, struct pb_buffer_lean *_buf);
+bool radeon_bo_can_reclaim(void *winsys, struct pb_buffer_lean *_buf);
 void radeon_drm_bo_init_functions(struct radeon_drm_winsys *ws);
 
 bool radeon_bo_can_reclaim_slab(void *priv, struct pb_slab_entry *entry);
-- 
GitLab


From bfc7d2cbe6bb213edac3b13a091d8fac0ad73a50 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 9 Dec 2023 15:59:56 -0500
Subject: [PATCH 21/36] gallium/pb_cache: remove pb_cache_entry::mgr

We can just pass it via functions.

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 .../auxiliary/pipebuffer/pb_bufmgr_cache.c    |  2 +-
 src/gallium/auxiliary/pipebuffer/pb_cache.c   | 26 ++++++++-----------
 src/gallium/auxiliary/pipebuffer/pb_cache.h   |  3 +--
 src/gallium/drivers/zink/zink_bo.c            |  2 +-
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c     |  2 +-
 src/gallium/winsys/radeon/drm/radeon_drm_bo.c |  3 ++-
 6 files changed, 17 insertions(+), 21 deletions(-)

diff --git a/src/gallium/auxiliary/pipebuffer/pb_bufmgr_cache.c b/src/gallium/auxiliary/pipebuffer/pb_bufmgr_cache.c
index 0f62fc60d2cf3..bc15dd6353ad5 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_bufmgr_cache.c
+++ b/src/gallium/auxiliary/pipebuffer/pb_bufmgr_cache.c
@@ -116,7 +116,7 @@ pb_cache_buffer_destroy(void *winsys, struct pb_buffer *_buf)
       return;
    }
 
-   pb_cache_add_buffer(&buf->cache_entry);
+   pb_cache_add_buffer(&mgr->cache, &buf->cache_entry);
 }
 
 
diff --git a/src/gallium/auxiliary/pipebuffer/pb_cache.c b/src/gallium/auxiliary/pipebuffer/pb_cache.c
index e8986d59c616d..a982e9572f2ad 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_cache.c
+++ b/src/gallium/auxiliary/pipebuffer/pb_cache.c
@@ -57,9 +57,8 @@ time_get_ms(struct pb_cache *mgr)
  * Actually destroy the buffer.
  */
 static void
-destroy_buffer_locked(struct pb_cache_entry *entry)
+destroy_buffer_locked(struct pb_cache *mgr, struct pb_cache_entry *entry)
 {
-   struct pb_cache *mgr = entry->mgr;
    struct pb_buffer_lean *buf = entry->buffer;
 
    assert(!pipe_is_referenced(&buf->reference));
@@ -76,7 +75,7 @@ destroy_buffer_locked(struct pb_cache_entry *entry)
  * Free as many cache buffers from the list head as possible.
  */
 static void
-release_expired_buffers_locked(struct list_head *cache,
+release_expired_buffers_locked(struct pb_cache *mgr, struct list_head *cache,
                                unsigned current_time_ms)
 {
    struct list_head *curr, *next;
@@ -87,11 +86,11 @@ release_expired_buffers_locked(struct list_head *cache,
    while (curr != cache) {
       entry = list_entry(curr, struct pb_cache_entry, head);
 
-      if (!time_timeout_ms(entry->start_ms, entry->mgr->msecs,
+      if (!time_timeout_ms(entry->start_ms, mgr->msecs,
                            current_time_ms))
          break;
 
-      destroy_buffer_locked(entry);
+      destroy_buffer_locked(mgr, entry);
 
       curr = next;
       next = curr->next;
@@ -103,9 +102,8 @@ release_expired_buffers_locked(struct list_head *cache,
  * being released.
  */
 void
-pb_cache_add_buffer(struct pb_cache_entry *entry)
+pb_cache_add_buffer(struct pb_cache *mgr, struct pb_cache_entry *entry)
 {
-   struct pb_cache *mgr = entry->mgr;
    struct list_head *cache = &mgr->buckets[entry->bucket_index];
    struct pb_buffer_lean *buf = entry->buffer;
    unsigned i;
@@ -116,7 +114,7 @@ pb_cache_add_buffer(struct pb_cache_entry *entry)
    unsigned current_time_ms = time_get_ms(mgr);
 
    for (i = 0; i < mgr->num_heaps; i++)
-      release_expired_buffers_locked(&mgr->buckets[i], current_time_ms);
+      release_expired_buffers_locked(mgr, &mgr->buckets[i], current_time_ms);
 
    /* Directly release any buffer that exceeds the limit. */
    if (mgr->cache_size + buf->size > mgr->max_cache_size) {
@@ -138,10 +136,9 @@ pb_cache_add_buffer(struct pb_cache_entry *entry)
  *        -1   if compatible and can't be reclaimed
  */
 static int
-pb_cache_is_buffer_compat(struct pb_cache_entry *entry,
+pb_cache_is_buffer_compat(struct pb_cache *mgr, struct pb_cache_entry *entry,
                           pb_size size, unsigned alignment, unsigned usage)
 {
-   struct pb_cache *mgr = entry->mgr;
    struct pb_buffer_lean *buf = entry->buffer;
 
    if (!pb_check_usage(usage, buf->usage))
@@ -189,11 +186,11 @@ pb_cache_reclaim_buffer(struct pb_cache *mgr, pb_size size,
    while (cur != cache) {
       cur_entry = list_entry(cur, struct pb_cache_entry, head);
 
-      if (!entry && (ret = pb_cache_is_buffer_compat(cur_entry, size,
+      if (!entry && (ret = pb_cache_is_buffer_compat(mgr, cur_entry, size,
                                                      alignment, usage)) > 0)
          entry = cur_entry;
       else if (time_timeout_ms(cur_entry->start_ms, mgr->msecs, now))
-         destroy_buffer_locked(cur_entry);
+         destroy_buffer_locked(mgr, cur_entry);
       else
          /* This buffer (and all hereafter) are still hot in cache */
          break;
@@ -210,7 +207,7 @@ pb_cache_reclaim_buffer(struct pb_cache *mgr, pb_size size,
    if (!entry && ret != -1) {
       while (cur != cache) {
          cur_entry = list_entry(cur, struct pb_cache_entry, head);
-         ret = pb_cache_is_buffer_compat(cur_entry, size, alignment, usage);
+         ret = pb_cache_is_buffer_compat(mgr, cur_entry, size, alignment, usage);
 
          if (ret > 0) {
             entry = cur_entry;
@@ -260,7 +257,7 @@ pb_cache_release_all_buffers(struct pb_cache *mgr)
       next = curr->next;
       while (curr != cache) {
          buf = list_entry(curr, struct pb_cache_entry, head);
-         destroy_buffer_locked(buf);
+         destroy_buffer_locked(mgr, buf);
          num_reclaims++;
          curr = next;
          next = curr->next;
@@ -278,7 +275,6 @@ pb_cache_init_entry(struct pb_cache *mgr, struct pb_cache_entry *entry,
 
    memset(entry, 0, sizeof(*entry));
    entry->buffer = buf;
-   entry->mgr = mgr;
    entry->bucket_index = bucket_index;
 }
 
diff --git a/src/gallium/auxiliary/pipebuffer/pb_cache.h b/src/gallium/auxiliary/pipebuffer/pb_cache.h
index 7eb923fab3e62..c83a4d89054d1 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_cache.h
+++ b/src/gallium/auxiliary/pipebuffer/pb_cache.h
@@ -41,7 +41,6 @@ struct pb_cache_entry
 {
    struct list_head head;
    struct pb_buffer_lean *buffer; /**< Pointer to the structure this is part of. */
-   struct pb_cache *mgr;
    unsigned start_ms; /**< Cached start time */
    unsigned bucket_index;
 };
@@ -68,7 +67,7 @@ struct pb_cache
    bool (*can_reclaim)(void *winsys, struct pb_buffer_lean *buf);
 };
 
-void pb_cache_add_buffer(struct pb_cache_entry *entry);
+void pb_cache_add_buffer(struct pb_cache *mgr, struct pb_cache_entry *entry);
 struct pb_buffer_lean *pb_cache_reclaim_buffer(struct pb_cache *mgr, pb_size size,
                                           unsigned alignment, unsigned usage,
                                           unsigned bucket_index);
diff --git a/src/gallium/drivers/zink/zink_bo.c b/src/gallium/drivers/zink/zink_bo.c
index a0b6ef8a3475c..75e3c061bccba 100644
--- a/src/gallium/drivers/zink/zink_bo.c
+++ b/src/gallium/drivers/zink/zink_bo.c
@@ -232,7 +232,7 @@ bo_destroy_or_cache(struct zink_screen *screen, struct pb_buffer *pbuf)
    bo->writes.u = NULL;
 
    if (bo->u.real.use_reusable_pool)
-      pb_cache_add_buffer(bo->cache_entry);
+      pb_cache_add_buffer(&screen->pb.bo_cache, bo->cache_entry);
    else
       bo_destroy(screen, pbuf);
 }
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index a88b75f1cfaa8..563ef5624c35c 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -233,7 +233,7 @@ static void amdgpu_bo_destroy_or_cache(struct radeon_winsys *rws, struct pb_buff
    assert(is_real_bo(bo)); /* slab buffers have a separate vtbl */
 
    if (bo->type >= AMDGPU_BO_REAL_REUSABLE)
-      pb_cache_add_buffer(&((struct amdgpu_bo_real_reusable*)bo)->cache_entry);
+      pb_cache_add_buffer(&ws->bo_cache, &((struct amdgpu_bo_real_reusable*)bo)->cache_entry);
    else
       amdgpu_bo_destroy(ws, _buf);
 }
diff --git a/src/gallium/winsys/radeon/drm/radeon_drm_bo.c b/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
index 90d792c993088..3cc282584764b 100644
--- a/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
+++ b/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
@@ -403,12 +403,13 @@ void radeon_bo_destroy(void *winsys, struct pb_buffer_lean *_buf)
 
 static void radeon_bo_destroy_or_cache(void *winsys, struct pb_buffer *_buf)
 {
+   struct radeon_drm_winsys *rws = (struct radeon_drm_winsys *)winsys;
    struct radeon_bo *bo = radeon_bo(_buf);
 
    assert(bo->handle && "must not be called for slab entries");
 
    if (bo->u.real.use_reusable_pool)
-      pb_cache_add_buffer(&bo->u.real.cache_entry);
+      pb_cache_add_buffer(&rws->bo_cache, &bo->u.real.cache_entry);
    else
       radeon_bo_destroy(NULL, &_buf->base);
 }
-- 
GitLab


From 243413cdd23d86cf094785d56825be30bd038083 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 9 Dec 2023 16:54:33 -0500
Subject: [PATCH 22/36] gallium/pb_cache: remove pb_cache_entry::buffer

The buffer pointer is always at a constant offset from pb_cache_entry,
so just pass the "offsetof" value to pb_cache and use that to get
the pointer.

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 .../auxiliary/pipebuffer/pb_bufmgr_cache.c    |  3 ++-
 src/gallium/auxiliary/pipebuffer/pb_cache.c   | 19 +++++++++++++------
 src/gallium/auxiliary/pipebuffer/pb_cache.h   |  4 ++--
 src/gallium/drivers/zink/zink_bo.c            |  2 +-
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c     |  4 ----
 src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c |  3 ++-
 .../winsys/radeon/drm/radeon_drm_winsys.c     |  4 +++-
 7 files changed, 23 insertions(+), 16 deletions(-)

diff --git a/src/gallium/auxiliary/pipebuffer/pb_bufmgr_cache.c b/src/gallium/auxiliary/pipebuffer/pb_bufmgr_cache.c
index bc15dd6353ad5..61ca808068cc3 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_bufmgr_cache.c
+++ b/src/gallium/auxiliary/pipebuffer/pb_bufmgr_cache.c
@@ -307,7 +307,8 @@ pb_cache_manager_create(struct pb_manager *provider,
    mgr->base.flush = pb_cache_manager_flush;
    mgr->provider = provider;
    pb_cache_init(&mgr->cache, 1, usecs, size_factor, bypass_usage,
-                 maximum_cache_size, NULL,
+                 maximum_cache_size,
+                 offsetof(struct pb_cache_buffer, cache_entry), NULL,
                  _pb_cache_buffer_destroy,
                  pb_cache_can_reclaim_buffer);
    return &mgr->base;
diff --git a/src/gallium/auxiliary/pipebuffer/pb_cache.c b/src/gallium/auxiliary/pipebuffer/pb_cache.c
index a982e9572f2ad..7fcb13b709916 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_cache.c
+++ b/src/gallium/auxiliary/pipebuffer/pb_cache.c
@@ -53,13 +53,19 @@ time_get_ms(struct pb_cache *mgr)
    return os_time_get() / 1000 - mgr->msecs_base_time;
 }
 
+static struct pb_buffer_lean *
+get_buffer(struct pb_cache *mgr, struct pb_cache_entry *entry)
+{
+   return (struct pb_buffer_lean*)((char*)entry - mgr->offsetof_pb_cache_entry);
+}
+
 /**
  * Actually destroy the buffer.
  */
 static void
 destroy_buffer_locked(struct pb_cache *mgr, struct pb_cache_entry *entry)
 {
-   struct pb_buffer_lean *buf = entry->buffer;
+   struct pb_buffer_lean *buf = get_buffer(mgr, entry);
 
    assert(!pipe_is_referenced(&buf->reference));
    if (list_is_linked(&entry->head)) {
@@ -105,7 +111,7 @@ void
 pb_cache_add_buffer(struct pb_cache *mgr, struct pb_cache_entry *entry)
 {
    struct list_head *cache = &mgr->buckets[entry->bucket_index];
-   struct pb_buffer_lean *buf = entry->buffer;
+   struct pb_buffer_lean *buf = get_buffer(mgr, entry);
    unsigned i;
 
    simple_mtx_lock(&mgr->mutex);
@@ -139,7 +145,7 @@ static int
 pb_cache_is_buffer_compat(struct pb_cache *mgr, struct pb_cache_entry *entry,
                           pb_size size, unsigned alignment, unsigned usage)
 {
-   struct pb_buffer_lean *buf = entry->buffer;
+   struct pb_buffer_lean *buf = get_buffer(mgr, entry);
 
    if (!pb_check_usage(usage, buf->usage))
       return 0;
@@ -223,7 +229,7 @@ pb_cache_reclaim_buffer(struct pb_cache *mgr, pb_size size,
 
    /* found a compatible buffer, return it */
    if (entry) {
-      struct pb_buffer_lean *buf = entry->buffer;
+      struct pb_buffer_lean *buf = get_buffer(mgr, entry);
 
       mgr->cache_size -= buf->size;
       list_del(&entry->head);
@@ -274,7 +280,6 @@ pb_cache_init_entry(struct pb_cache *mgr, struct pb_cache_entry *entry,
    assert(bucket_index < mgr->num_heaps);
 
    memset(entry, 0, sizeof(*entry));
-   entry->buffer = buf;
    entry->bucket_index = bucket_index;
 }
 
@@ -293,6 +298,7 @@ pb_cache_init_entry(struct pb_cache *mgr, struct pb_cache_entry *entry,
  *                      buffer allocation requests are rejected.
  * @param maximum_cache_size  Maximum size of all unused buffers the cache can
  *                            hold.
+ * @param offsetof_pb_cache_entry  offsetof(driver_bo, pb_cache_entry)
  * @param destroy_buffer  Function that destroys a buffer for good.
  * @param can_reclaim     Whether a buffer can be reclaimed (e.g. is not busy)
  */
@@ -300,7 +306,7 @@ void
 pb_cache_init(struct pb_cache *mgr, unsigned num_heaps,
               unsigned usecs, float size_factor,
               unsigned bypass_usage, uint64_t maximum_cache_size,
-              void *winsys,
+              unsigned offsetof_pb_cache_entry, void *winsys,
               void (*destroy_buffer)(void *winsys, struct pb_buffer_lean *buf),
               bool (*can_reclaim)(void *winsys, struct pb_buffer_lean *buf))
 {
@@ -323,6 +329,7 @@ pb_cache_init(struct pb_cache *mgr, unsigned num_heaps,
    mgr->num_buffers = 0;
    mgr->bypass_usage = bypass_usage;
    mgr->size_factor = size_factor;
+   mgr->offsetof_pb_cache_entry = offsetof_pb_cache_entry;
    mgr->destroy_buffer = destroy_buffer;
    mgr->can_reclaim = can_reclaim;
 }
diff --git a/src/gallium/auxiliary/pipebuffer/pb_cache.h b/src/gallium/auxiliary/pipebuffer/pb_cache.h
index c83a4d89054d1..2fa50e1a20f53 100644
--- a/src/gallium/auxiliary/pipebuffer/pb_cache.h
+++ b/src/gallium/auxiliary/pipebuffer/pb_cache.h
@@ -40,7 +40,6 @@
 struct pb_cache_entry
 {
    struct list_head head;
-   struct pb_buffer_lean *buffer; /**< Pointer to the structure this is part of. */
    unsigned start_ms; /**< Cached start time */
    unsigned bucket_index;
 };
@@ -62,6 +61,7 @@ struct pb_cache
    unsigned num_buffers;
    unsigned bypass_usage;
    float size_factor;
+   unsigned offsetof_pb_cache_entry; /* offsetof(driver_bo, pb_cache_entry) */
 
    void (*destroy_buffer)(void *winsys, struct pb_buffer_lean *buf);
    bool (*can_reclaim)(void *winsys, struct pb_buffer_lean *buf);
@@ -77,7 +77,7 @@ void pb_cache_init_entry(struct pb_cache *mgr, struct pb_cache_entry *entry,
 void pb_cache_init(struct pb_cache *mgr, unsigned num_heaps,
                    unsigned usecs, float size_factor,
                    unsigned bypass_usage, uint64_t maximum_cache_size,
-                   void *winsys,
+                   unsigned offsetof_pb_cache_entry, void *winsys,
                    void (*destroy_buffer)(void *winsys, struct pb_buffer_lean *buf),
                    bool (*can_reclaim)(void *winsys, struct pb_buffer_lean *buf));
 void pb_cache_deinit(struct pb_cache *mgr);
diff --git a/src/gallium/drivers/zink/zink_bo.c b/src/gallium/drivers/zink/zink_bo.c
index 75e3c061bccba..ef3fda4afb314 100644
--- a/src/gallium/drivers/zink/zink_bo.c
+++ b/src/gallium/drivers/zink/zink_bo.c
@@ -1301,7 +1301,7 @@ zink_bo_init(struct zink_screen *screen)
    /* Create managers. */
    pb_cache_init(&screen->pb.bo_cache, screen->info.mem_props.memoryTypeCount,
                  500000, 2.0f, 0,
-                 total_mem / 8, screen,
+                 total_mem / 8, offsetof(struct zink_bo, cache_entry), screen,
                  (void*)bo_destroy, (void*)bo_can_reclaim);
 
    unsigned min_slab_order = MIN_SLAB_ORDER;  /* 256 bytes */
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index 563ef5624c35c..acc7c20635c18 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -1435,10 +1435,6 @@ no_slab:
              memset((uint8_t*)new_bo + orig_size, 0, new_size - orig_size);
              bo = new_bo;
              bo->type = AMDGPU_BO_REAL_REUSABLE_SLAB;
-
-             /* Re-set pointers after realloc. */
-             struct amdgpu_bo_real_reusable *real_bo = get_real_bo_reusable(bo);
-             real_bo->cache_entry.buffer = &bo->base.base;
           }
           return &bo->base;
        }
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c b/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c
index d54f229c36341..efd9c18c32fcc 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c
@@ -447,7 +447,8 @@ amdgpu_winsys_create(int fd, const struct pipe_screen_config *config,
       /* Create managers. */
       pb_cache_init(&aws->bo_cache, RADEON_NUM_HEAPS,
                     500000, aws->check_vm ? 1.0f : 1.5f, 0,
-                    ((uint64_t)aws->info.vram_size_kb + aws->info.gart_size_kb) * 1024 / 8, aws,
+                    ((uint64_t)aws->info.vram_size_kb + aws->info.gart_size_kb) * 1024 / 8,
+                    offsetof(struct amdgpu_bo_real_reusable, cache_entry), aws,
                     /* Cast to void* because one of the function parameters
                      * is a struct pointer instead of void*. */
                     (void*)amdgpu_bo_destroy, (void*)amdgpu_bo_can_reclaim);
diff --git a/src/gallium/winsys/radeon/drm/radeon_drm_winsys.c b/src/gallium/winsys/radeon/drm/radeon_drm_winsys.c
index 00a0847f2a370..864434944005e 100644
--- a/src/gallium/winsys/radeon/drm/radeon_drm_winsys.c
+++ b/src/gallium/winsys/radeon/drm/radeon_drm_winsys.c
@@ -885,7 +885,9 @@ radeon_drm_winsys_create(int fd, const struct pipe_screen_config *config,
 
    pb_cache_init(&ws->bo_cache, RADEON_NUM_HEAPS,
                  500000, ws->check_vm ? 1.0f : 2.0f, 0,
-                 (uint64_t)MIN2(ws->info.vram_size_kb, ws->info.gart_size_kb) * 1024, NULL,
+                 (uint64_t)MIN2(ws->info.vram_size_kb, ws->info.gart_size_kb) * 1024,
+                 offsetof(struct radeon_bo, u.real.cache_entry),
+                 NULL,
                  radeon_bo_destroy,
                  radeon_bo_can_reclaim);
 
-- 
GitLab


From 93df7af7223010fb0d0f70e230b5fb39a803e30a Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 9 Dec 2023 17:11:35 -0500
Subject: [PATCH 23/36] winsys/radeon: stop using pb_buffer::vtbl

Only the destroy function used it.

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/winsys/radeon/drm/radeon_drm_bo.c | 21 ++++++-------------
 1 file changed, 6 insertions(+), 15 deletions(-)

diff --git a/src/gallium/winsys/radeon/drm/radeon_drm_bo.c b/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
index 3cc282584764b..0491ec04399e0 100644
--- a/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
+++ b/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
@@ -596,11 +596,6 @@ static void radeon_bo_unmap(struct radeon_winsys *rws, struct pb_buffer *_buf)
    mtx_unlock(&bo->u.real.map_mutex);
 }
 
-static const struct pb_vtbl radeon_bo_vtbl = {
-   radeon_bo_destroy_or_cache
-   /* other functions are never called */
-};
-
 static struct radeon_bo *radeon_create_bo(struct radeon_drm_winsys *rws,
                                           unsigned size, unsigned alignment,
                                           unsigned initial_domains,
@@ -654,7 +649,6 @@ static struct radeon_bo *radeon_create_bo(struct radeon_drm_winsys *rws,
    bo->base.base.alignment_log2 = util_logbase2(alignment);
    bo->base.base.usage = 0;
    bo->base.base.size = size;
-   bo->base.vtbl = &radeon_bo_vtbl;
    bo->rws = rws;
    bo->handle = args.handle;
    bo->va = 0;
@@ -747,11 +741,6 @@ static void radeon_bo_slab_destroy(void *winsys, struct pb_buffer *_buf)
    pb_slab_free(&bo->rws->bo_slabs, &bo->u.slab.entry);
 }
 
-static const struct pb_vtbl radeon_winsys_bo_slab_vtbl = {
-   radeon_bo_slab_destroy
-   /* other functions are never called */
-};
-
 struct pb_slab *radeon_bo_slab_alloc(void *priv, unsigned heap,
                                      unsigned entry_size,
                                      unsigned group_index)
@@ -791,7 +780,6 @@ struct pb_slab *radeon_bo_slab_alloc(void *priv, unsigned heap,
       bo->base.base.alignment_log2 = util_logbase2(entry_size);
       bo->base.base.usage = slab->buffer->base.base.usage;
       bo->base.base.size = entry_size;
-      bo->base.vtbl = &radeon_winsys_bo_slab_vtbl;
       bo->rws = ws;
       bo->va = slab->buffer->va + i * entry_size;
       bo->initial_domain = domains;
@@ -1074,7 +1062,12 @@ radeon_winsys_bo_create(struct radeon_winsys *rws,
 
 static void radeon_winsys_bo_destroy(struct radeon_winsys *ws, struct pb_buffer *buf)
 {
-   buf->vtbl->destroy(ws, buf);
+   struct radeon_bo *bo = radeon_bo(buf);
+
+   if (bo->handle)
+      radeon_bo_destroy_or_cache(ws, buf);
+   else
+      radeon_bo_slab_destroy(ws, buf);
 }
 
 static struct pb_buffer *radeon_winsys_bo_from_ptr(struct radeon_winsys *rws,
@@ -1117,7 +1110,6 @@ static struct pb_buffer *radeon_winsys_bo_from_ptr(struct radeon_winsys *rws,
    bo->handle = args.handle;
    bo->base.base.alignment_log2 = 0;
    bo->base.base.size = size;
-   bo->base.vtbl = &radeon_bo_vtbl;
    bo->rws = ws;
    bo->user_ptr = pointer;
    bo->va = 0;
@@ -1246,7 +1238,6 @@ static struct pb_buffer *radeon_winsys_bo_from_handle(struct radeon_winsys *rws,
    pipe_reference_init(&bo->base.base.reference, 1);
    bo->base.base.alignment_log2 = 0;
    bo->base.base.size = (unsigned) size;
-   bo->base.vtbl = &radeon_bo_vtbl;
    bo->rws = ws;
    bo->va = 0;
    bo->hash = __sync_fetch_and_add(&ws->next_bo_hash, 1);
-- 
GitLab


From 439bb4b4891217f70ad046c5920e7cdccc92976f Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 9 Dec 2023 17:07:14 -0500
Subject: [PATCH 24/36] r300,r600,radeonsi: switch to pb_buffer_lean

to remove pb_buffer::vtbl from all buffer structures

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/drivers/r300/r300_context.h       |   8 +-
 src/gallium/drivers/r300/r300_emit.c          |   4 +-
 src/gallium/drivers/r300/r300_render.c        |   4 +-
 src/gallium/drivers/r300/r300_screen_buffer.c |   2 +-
 src/gallium/drivers/r300/r300_texture.c       |   4 +-
 src/gallium/drivers/r300/r300_texture_desc.c  |   6 +-
 src/gallium/drivers/r600/r600_buffer_common.c |   8 +-
 src/gallium/drivers/r600/r600_pipe_common.h   |   6 +-
 src/gallium/drivers/r600/r600_state.c         |   4 +-
 src/gallium/drivers/r600/r600_texture.c       |  20 +-
 src/gallium/drivers/r600/r600_uvd.c           |   6 +-
 src/gallium/drivers/r600/radeon_uvd.c         |  30 +--
 src/gallium/drivers/r600/radeon_uvd.h         |   2 +-
 src/gallium/drivers/r600/radeon_vce.c         |   2 +-
 src/gallium/drivers/r600/radeon_vce.h         |   8 +-
 src/gallium/drivers/r600/radeon_video.c       |  14 +-
 src/gallium/drivers/r600/radeon_video.h       |   2 +-
 src/gallium/drivers/radeonsi/radeon_uvd.c     |  56 ++---
 src/gallium/drivers/radeonsi/radeon_uvd.h     |   2 +-
 src/gallium/drivers/radeonsi/radeon_uvd_enc.h |   6 +-
 .../drivers/radeonsi/radeon_uvd_enc_1_1.c     |   2 +-
 src/gallium/drivers/radeonsi/radeon_vce.c     |   2 +-
 src/gallium/drivers/radeonsi/radeon_vce.h     |   8 +-
 src/gallium/drivers/radeonsi/radeon_vce_50.c  |   2 +-
 src/gallium/drivers/radeonsi/radeon_vce_52.c  |   2 +-
 src/gallium/drivers/radeonsi/radeon_vcn_dec.c |  66 +++---
 .../drivers/radeonsi/radeon_vcn_dec_jpeg.c    |  16 +-
 src/gallium/drivers/radeonsi/radeon_vcn_enc.c |   4 +-
 src/gallium/drivers/radeonsi/radeon_vcn_enc.h |  10 +-
 src/gallium/drivers/radeonsi/radeon_video.c   |   2 +-
 src/gallium/drivers/radeonsi/si_buffer.c      |  14 +-
 src/gallium/drivers/radeonsi/si_perfcounter.c |   2 +-
 src/gallium/drivers/radeonsi/si_pipe.c        |   2 +-
 src/gallium/drivers/radeonsi/si_pipe.h        |  12 +-
 src/gallium/drivers/radeonsi/si_sqtt.c        |   2 +-
 src/gallium/drivers/radeonsi/si_texture.c     |  20 +-
 src/gallium/drivers/radeonsi/si_uvd.c         |   4 +-
 src/gallium/include/winsys/radeon_winsys.h    |  57 ++---
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c     | 200 +++++++++---------
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.h     |  20 +-
 src/gallium/winsys/amdgpu/drm/amdgpu_cs.c     |  16 +-
 src/gallium/winsys/amdgpu/drm/amdgpu_cs.h     |   4 +-
 src/gallium/winsys/radeon/drm/radeon_drm_bo.c | 146 ++++++-------
 src/gallium/winsys/radeon/drm/radeon_drm_bo.h |   4 +-
 src/gallium/winsys/radeon/drm/radeon_drm_cs.c |  18 +-
 45 files changed, 417 insertions(+), 412 deletions(-)

diff --git a/src/gallium/drivers/r300/r300_context.h b/src/gallium/drivers/r300/r300_context.h
index adce1d772e5a3..d7a784ac0835a 100644
--- a/src/gallium/drivers/r300/r300_context.h
+++ b/src/gallium/drivers/r300/r300_context.h
@@ -294,14 +294,14 @@ struct r300_query {
     bool begin_emitted;
 
     /* The buffer where query results are stored. */
-    struct pb_buffer *buf;
+    struct pb_buffer_lean *buf;
 };
 
 struct r300_surface {
     struct pipe_surface base;
 
     /* Winsys buffer backing the texture. */
-    struct pb_buffer *buf;
+    struct pb_buffer_lean *buf;
 
     enum radeon_bo_domain domain;
 
@@ -392,7 +392,7 @@ struct r300_resource
     struct pipe_resource b;
 
     /* Winsys buffer backing this resource. */
-    struct pb_buffer *buf;
+    struct pb_buffer_lean *buf;
     enum radeon_bo_domain domain;
 
     /* Constant buffers and SWTCL vertex and index buffers are in user
@@ -456,7 +456,7 @@ struct r300_context {
     /* Draw module. Used mostly for SW TCL. */
     struct draw_context* draw;
     /* Vertex buffer for SW TCL. */
-    struct pb_buffer *vbo;
+    struct pb_buffer_lean *vbo;
     /* Offset and size into the SW TCL VBO. */
     size_t draw_vbo_offset;
 
diff --git a/src/gallium/drivers/r300/r300_emit.c b/src/gallium/drivers/r300/r300_emit.c
index 5b2cb3ad4ea5f..ad0e4c0eb3dc5 100644
--- a/src/gallium/drivers/r300/r300_emit.c
+++ b/src/gallium/drivers/r300/r300_emit.c
@@ -770,8 +770,8 @@ void r300_emit_query_end(struct r300_context* r300)
     query->num_results += query->num_pipes;
 
     /* XXX grab all the results and reset the counter. */
-    if (query->num_results >= query->buf->base.size / 4 - 4) {
-        query->num_results = (query->buf->base.size / 4) / 2;
+    if (query->num_results >= query->buf->size / 4 - 4) {
+        query->num_results = (query->buf->size / 4) / 2;
         fprintf(stderr, "r300: Rewinding OQBO...\n");
     }
 }
diff --git a/src/gallium/drivers/r300/r300_render.c b/src/gallium/drivers/r300/r300_render.c
index 19c213766f5cb..858d1798b480a 100644
--- a/src/gallium/drivers/r300/r300_render.c
+++ b/src/gallium/drivers/r300/r300_render.c
@@ -950,7 +950,7 @@ static bool r300_render_allocate_vertices(struct vbuf_render* render,
 
     DBG(r300, DBG_DRAW, "r300: render_allocate_vertices (size: %d)\n", size);
 
-    if (!r300->vbo || size + r300->draw_vbo_offset > r300->vbo->base.size) {
+    if (!r300->vbo || size + r300->draw_vbo_offset > r300->vbo->size) {
 	radeon_bo_reference(r300->rws, &r300->vbo, NULL);
         r300->vbo = NULL;
         r300render->vbo_ptr = NULL;
@@ -1056,7 +1056,7 @@ static void r300_render_draw_elements(struct vbuf_render* render,
 {
     struct r300_render* r300render = r300_render(render);
     struct r300_context* r300 = r300render->r300;
-    unsigned max_index = (r300->vbo->base.size - r300->draw_vbo_offset) /
+    unsigned max_index = (r300->vbo->size - r300->draw_vbo_offset) /
                          (r300render->r300->vertex_info.size * 4) - 1;
     struct pipe_resource *index_buffer = NULL;
     unsigned index_buffer_offset;
diff --git a/src/gallium/drivers/r300/r300_screen_buffer.c b/src/gallium/drivers/r300/r300_screen_buffer.c
index 884284bd86388..9dd23f256eb1b 100644
--- a/src/gallium/drivers/r300/r300_screen_buffer.c
+++ b/src/gallium/drivers/r300/r300_screen_buffer.c
@@ -114,7 +114,7 @@ r300_buffer_transfer_map( struct pipe_context *context,
         if (r300->rws->cs_is_buffer_referenced(&r300->cs, rbuf->buf, RADEON_USAGE_READWRITE) ||
             !r300->rws->buffer_wait(r300->rws, rbuf->buf, 0, RADEON_USAGE_READWRITE)) {
             unsigned i;
-            struct pb_buffer *new_buf;
+            struct pb_buffer_lean *new_buf;
 
             /* Create a new one in the same pipe_resource. */
             new_buf = r300->rws->buffer_create(r300->rws, rbuf->b.width0,
diff --git a/src/gallium/drivers/r300/r300_texture.c b/src/gallium/drivers/r300/r300_texture.c
index 73aa8cf194e0f..23b38490d45bc 100644
--- a/src/gallium/drivers/r300/r300_texture.c
+++ b/src/gallium/drivers/r300/r300_texture.c
@@ -1033,7 +1033,7 @@ r300_texture_create_object(struct r300_screen *rscreen,
                            enum radeon_bo_layout microtile,
                            enum radeon_bo_layout macrotile,
                            unsigned stride_in_bytes_override,
-                           struct pb_buffer *buffer)
+                           struct pb_buffer_lean *buffer)
 {
     struct radeon_winsys *rws = rscreen->rws;
     struct r300_resource *tex = NULL;
@@ -1142,7 +1142,7 @@ struct pipe_resource *r300_texture_from_handle(struct pipe_screen *screen,
 {
     struct r300_screen *rscreen = r300_screen(screen);
     struct radeon_winsys *rws = rscreen->rws;
-    struct pb_buffer *buffer;
+    struct pb_buffer_lean *buffer;
     struct radeon_bo_metadata tiling = {};
 
     /* Support only 2D textures without mipmaps */
diff --git a/src/gallium/drivers/r300/r300_texture_desc.c b/src/gallium/drivers/r300/r300_texture_desc.c
index c98e29db55580..7b1b2e21d2ae2 100644
--- a/src/gallium/drivers/r300/r300_texture_desc.c
+++ b/src/gallium/drivers/r300/r300_texture_desc.c
@@ -606,17 +606,17 @@ void r300_texture_desc_init(struct r300_screen *rscreen,
     r300_setup_miptree(rscreen, tex, true);
     /* If the required buffer size is larger than the given max size,
      * try again without the alignment for the CBZB clear. */
-    if (tex->buf && tex->tex.size_in_bytes > tex->buf->base.size) {
+    if (tex->buf && tex->tex.size_in_bytes > tex->buf->size) {
         r300_setup_miptree(rscreen, tex, false);
 
         /* Make sure the buffer we got is large enough. */
-        if (tex->tex.size_in_bytes > tex->buf->base.size) {
+        if (tex->tex.size_in_bytes > tex->buf->size) {
             fprintf(stderr,
                 "r300: I got a pre-allocated buffer to use it as a texture "
                 "storage, but the buffer is too small. I'll use the buffer "
                 "anyway, because I can't crash here, but it's dangerous. "
                 "This can be a DDX bug. Got: %"PRIu64"B, Need: %uB, Info:\n",
-                tex->buf->base.size, tex->tex.size_in_bytes);
+                tex->buf->size, tex->tex.size_in_bytes);
             r300_tex_print_info(tex, "texture_desc_init");
             /* Oops, what now. Apps will break if we fail this,
              * so just pretend everything's okay. */
diff --git a/src/gallium/drivers/r600/r600_buffer_common.c b/src/gallium/drivers/r600/r600_buffer_common.c
index 20b182e2aed32..a072ed935c4a6 100644
--- a/src/gallium/drivers/r600/r600_buffer_common.c
+++ b/src/gallium/drivers/r600/r600_buffer_common.c
@@ -34,7 +34,7 @@
 #include <stdio.h>
 
 bool r600_rings_is_buffer_referenced(struct r600_common_context *ctx,
-				     struct pb_buffer *buf,
+				     struct pb_buffer_lean *buf,
 				     unsigned usage)
 {
 	if (ctx->ws->cs_is_buffer_referenced(&ctx->gfx.cs, buf, usage)) {
@@ -166,7 +166,7 @@ void r600_init_resource_fields(struct r600_common_screen *rscreen,
 bool r600_alloc_resource(struct r600_common_screen *rscreen,
 			 struct r600_resource *res)
 {
-	struct pb_buffer *old_buf, *new_buf;
+	struct pb_buffer_lean *old_buf, *new_buf;
 
 	/* Allocate a new resource. */
 	new_buf = rscreen->ws->buffer_create(rscreen->ws, res->bo_size,
@@ -195,8 +195,8 @@ bool r600_alloc_resource(struct r600_common_screen *rscreen,
 	/* Print debug information. */
 	if (rscreen->debug_flags & DBG_VM && res->b.b.target == PIPE_BUFFER) {
 		fprintf(stderr, "VM start=0x%"PRIX64"  end=0x%"PRIX64" | Buffer %"PRIu64" bytes\n",
-			res->gpu_address, res->gpu_address + res->buf->base.size,
-			res->buf->base.size);
+			res->gpu_address, res->gpu_address + res->buf->size,
+			res->buf->size);
 	}
 	return true;
 }
diff --git a/src/gallium/drivers/r600/r600_pipe_common.h b/src/gallium/drivers/r600/r600_pipe_common.h
index 7d07c3db029fc..707c29703ca73 100644
--- a/src/gallium/drivers/r600/r600_pipe_common.h
+++ b/src/gallium/drivers/r600/r600_pipe_common.h
@@ -125,7 +125,7 @@ struct r600_resource {
 	struct threaded_resource	b;
 
 	/* Winsys objects. */
-	struct pb_buffer		*buf;
+	struct pb_buffer_lean		*buf;
 	uint64_t			gpu_address;
 	/* Memory usage if the buffer placement is optimal. */
 	uint64_t			vram_usage;
@@ -314,7 +314,7 @@ union r600_mmio_counters {
 
 struct r600_memory_object {
 	struct pipe_memory_object	b;
-	struct pb_buffer		*buf;
+	struct pb_buffer_lean		*buf;
 	uint32_t			stride;
 	uint32_t			offset;
 };
@@ -616,7 +616,7 @@ struct r600_common_context {
 
 /* r600_buffer_common.c */
 bool r600_rings_is_buffer_referenced(struct r600_common_context *ctx,
-				     struct pb_buffer *buf,
+				     struct pb_buffer_lean *buf,
 				     unsigned usage);
 void *r600_buffer_map_sync_with_rings(struct r600_common_context *ctx,
                                       struct r600_resource *resource,
diff --git a/src/gallium/drivers/r600/r600_state.c b/src/gallium/drivers/r600/r600_state.c
index e90d8430348fc..a3746cc771a2e 100644
--- a/src/gallium/drivers/r600/r600_state.c
+++ b/src/gallium/drivers/r600/r600_state.c
@@ -990,7 +990,7 @@ static void r600_init_color_surface(struct r600_context *rctx,
 		/* CMASK. */
 		if (!rctx->dummy_cmask ||
 		    rctx->dummy_cmask->b.b.width0 < cmask.size ||
-		    (1 << rctx->dummy_cmask->buf->base.alignment_log2) % cmask.alignment != 0) {
+		    (1 << rctx->dummy_cmask->buf->alignment_log2) % cmask.alignment != 0) {
 			struct pipe_transfer *transfer;
 			void *ptr;
 
@@ -1015,7 +1015,7 @@ static void r600_init_color_surface(struct r600_context *rctx,
 		/* FMASK. */
 		if (!rctx->dummy_fmask ||
 		    rctx->dummy_fmask->b.b.width0 < fmask.size ||
-		    (1 << rctx->dummy_fmask->buf->base.alignment_log2) % fmask.alignment != 0) {
+		    (1 << rctx->dummy_fmask->buf->alignment_log2) % fmask.alignment != 0) {
 			r600_resource_reference(&rctx->dummy_fmask, NULL);
 			rctx->dummy_fmask = (struct r600_resource*)
 				r600_aligned_buffer_create(&rscreen->b.b, 0,
diff --git a/src/gallium/drivers/r600/r600_texture.c b/src/gallium/drivers/r600/r600_texture.c
index 347be5e4a8f4e..3f2c8c8be1994 100644
--- a/src/gallium/drivers/r600/r600_texture.c
+++ b/src/gallium/drivers/r600/r600_texture.c
@@ -897,7 +897,7 @@ void r600_print_texture_info(struct r600_common_screen *rscreen,
 static struct r600_texture *
 r600_texture_create_object(struct pipe_screen *screen,
 			   const struct pipe_resource *base,
-			   struct pb_buffer *buf,
+			   struct pb_buffer_lean *buf,
 			   struct radeon_surf *surface)
 {
 	struct r600_texture *rtex;
@@ -973,13 +973,13 @@ r600_texture_create_object(struct pipe_screen *screen,
 	} else {
 		resource->buf = buf;
 		resource->gpu_address = rscreen->ws->buffer_get_virtual_address(resource->buf);
-		resource->bo_size = buf->base.size;
-		resource->bo_alignment = 1 << buf->base.alignment_log2;
+		resource->bo_size = buf->size;
+		resource->bo_alignment = 1 << buf->alignment_log2;
 		resource->domains = rscreen->ws->buffer_get_initial_domain(resource->buf);
 		if (resource->domains & RADEON_DOMAIN_VRAM)
-			resource->vram_usage = buf->base.size;
+			resource->vram_usage = buf->size;
 		else if (resource->domains & RADEON_DOMAIN_GTT)
-			resource->gart_usage = buf->base.size;
+			resource->gart_usage = buf->size;
 	}
 
 	if (rtex->cmask.size) {
@@ -1004,7 +1004,7 @@ r600_texture_create_object(struct pipe_screen *screen,
 	if (rscreen->debug_flags & DBG_VM) {
 		fprintf(stderr, "VM start=0x%"PRIX64"  end=0x%"PRIX64" | Texture %ix%ix%i, %i levels, %i samples, %s\n",
 			rtex->resource.gpu_address,
-			rtex->resource.gpu_address + rtex->resource.buf->base.size,
+			rtex->resource.gpu_address + rtex->resource.buf->size,
 			base->width0, base->height0, util_num_layers(base, 0), base->last_level+1,
 			base->nr_samples ? base->nr_samples : 1, util_format_short_name(base->format));
 	}
@@ -1107,7 +1107,7 @@ static struct pipe_resource *r600_texture_from_handle(struct pipe_screen *screen
                                                       unsigned usage)
 {
 	struct r600_common_screen *rscreen = (struct r600_common_screen*)screen;
-	struct pb_buffer *buf = NULL;
+	struct pb_buffer_lean *buf = NULL;
 	enum radeon_surf_mode array_mode;
 	struct radeon_surf surface = {};
 	int r;
@@ -1486,7 +1486,7 @@ void r600_texture_transfer_unmap(struct pipe_context *ctx,
 	}
 
 	if (rtransfer->staging) {
-		rctx->num_alloc_tex_transfer_bytes += rtransfer->staging->buf->base.size;
+		rctx->num_alloc_tex_transfer_bytes += rtransfer->staging->buf->size;
 		r600_resource_reference(&rtransfer->staging, NULL);
 	}
 
@@ -1778,7 +1778,7 @@ r600_memobj_from_handle(struct pipe_screen *screen,
 {
 	struct r600_common_screen *rscreen = (struct r600_common_screen*)screen;
 	struct r600_memory_object *memobj = CALLOC_STRUCT(r600_memory_object);
-	struct pb_buffer *buf = NULL;
+	struct pb_buffer_lean *buf = NULL;
 
 	if (!memobj)
 		return NULL;
@@ -1824,7 +1824,7 @@ r600_texture_from_memobj(struct pipe_screen *screen,
 	struct radeon_bo_metadata metadata = {};
 	enum radeon_surf_mode array_mode;
 	bool is_scanout;
-	struct pb_buffer *buf = NULL;
+	struct pb_buffer_lean *buf = NULL;
 
 	if (memobj->b.dedicated) {
 		rscreen->ws->buffer_get_metadata(rscreen->ws, memobj->buf, &metadata, NULL);
diff --git a/src/gallium/drivers/r600/r600_uvd.c b/src/gallium/drivers/r600/r600_uvd.c
index 9c84b6380ba68..6f893bdb339db 100644
--- a/src/gallium/drivers/r600/r600_uvd.c
+++ b/src/gallium/drivers/r600/r600_uvd.c
@@ -61,7 +61,7 @@ struct pipe_video_buffer *r600_video_buffer_create(struct pipe_context *pipe,
 	struct r600_context *ctx = (struct r600_context *)pipe;
 	struct r600_texture *resources[VL_NUM_COMPONENTS] = {};
 	struct radeon_surf* surfaces[VL_NUM_COMPONENTS] = {};
-	struct pb_buffer **pbs[VL_NUM_COMPONENTS] = {};
+	struct pb_buffer_lean **pbs[VL_NUM_COMPONENTS] = {};
 	enum pipe_format resource_formats[3];
 	struct pipe_video_buffer template;
 	struct pipe_resource templ;
@@ -156,7 +156,7 @@ static uint32_t eg_num_banks(uint32_t nbanks)
 }
 
 /* set the decoding target buffer offsets */
-static struct pb_buffer* r600_uvd_set_dtb(struct ruvd_msg *msg, struct vl_video_buffer *buf)
+static struct pb_buffer_lean* r600_uvd_set_dtb(struct ruvd_msg *msg, struct vl_video_buffer *buf)
 {
 	struct r600_screen *rscreen = (struct r600_screen*)buf->base.context->screen;
 	struct r600_texture *luma = (struct r600_texture *)buf->resources[0];
@@ -172,7 +172,7 @@ static struct pb_buffer* r600_uvd_set_dtb(struct ruvd_msg *msg, struct vl_video_
 
 /* get the radeon resources for VCE */
 static void r600_vce_get_buffer(struct pipe_resource *resource,
-				struct pb_buffer **handle,
+				struct pb_buffer_lean **handle,
 				struct radeon_surf **surface)
 {
 	struct r600_texture *res = (struct r600_texture *)resource;
diff --git a/src/gallium/drivers/r600/radeon_uvd.c b/src/gallium/drivers/r600/radeon_uvd.c
index 0dc16cf3e8492..b589ee88b1dec 100644
--- a/src/gallium/drivers/r600/radeon_uvd.c
+++ b/src/gallium/drivers/r600/radeon_uvd.c
@@ -114,7 +114,7 @@ static void set_reg(struct ruvd_decoder *dec, unsigned reg, uint32_t val)
 
 /* send a command to the VCPU through the GPCOM registers */
 static void send_cmd(struct ruvd_decoder *dec, unsigned cmd,
-		     struct pb_buffer* buf, uint32_t off,
+		     struct pb_buffer_lean* buf, uint32_t off,
 		     unsigned usage, enum radeon_bo_domain domain)
 {
 	int reloc_idx;
@@ -257,38 +257,38 @@ static unsigned calc_dpb_size(struct ruvd_decoder *dec)
 	case PIPE_VIDEO_FORMAT_MPEG4_AVC: {
 		if (!dec->use_legacy) {
 			unsigned fs_in_mb = width_in_mb * height_in_mb;
-			unsigned alignment = 64, num_dpb_buffer;
+			unsigned alignment = 64, num_dpb_buffer_lean;
 
 			if (dec->stream_type == RUVD_CODEC_H264_PERF)
 				alignment = 256;
 			switch(dec->base.level) {
 			case 30:
-				num_dpb_buffer = 8100 / fs_in_mb;
+				num_dpb_buffer_lean = 8100 / fs_in_mb;
 				break;
 			case 31:
-				num_dpb_buffer = 18000 / fs_in_mb;
+				num_dpb_buffer_lean = 18000 / fs_in_mb;
 				break;
 			case 32:
-				num_dpb_buffer = 20480 / fs_in_mb;
+				num_dpb_buffer_lean = 20480 / fs_in_mb;
 				break;
 			case 41:
-				num_dpb_buffer = 32768 / fs_in_mb;
+				num_dpb_buffer_lean = 32768 / fs_in_mb;
 				break;
 			case 42:
-				num_dpb_buffer = 34816 / fs_in_mb;
+				num_dpb_buffer_lean = 34816 / fs_in_mb;
 				break;
 			case 50:
-				num_dpb_buffer = 110400 / fs_in_mb;
+				num_dpb_buffer_lean = 110400 / fs_in_mb;
 				break;
 			case 51:
-				num_dpb_buffer = 184320 / fs_in_mb;
+				num_dpb_buffer_lean = 184320 / fs_in_mb;
 				break;
 			default:
-				num_dpb_buffer = 184320 / fs_in_mb;
+				num_dpb_buffer_lean = 184320 / fs_in_mb;
 				break;
 			}
-			num_dpb_buffer++;
-			max_references = MAX2(MIN2(NUM_H264_REFS, num_dpb_buffer), max_references);
+			num_dpb_buffer_lean++;
+			max_references = MAX2(MIN2(NUM_H264_REFS, num_dpb_buffer_lean), max_references);
 			dpb_size = image_size * max_references;
 			if ((dec->stream_type != RUVD_CODEC_H264_PERF)) {
 				dpb_size += max_references * align(width_in_mb * height_in_mb  * 192, alignment);
@@ -887,7 +887,7 @@ static void ruvd_decode_bitstream(struct pipe_video_codec *decoder,
 		if (format == PIPE_VIDEO_FORMAT_JPEG)
 			new_size += 2; /* save for EOI */
 
-		if (new_size > buf->res->buf->base.size) {
+		if (new_size > buf->res->buf->size) {
 			dec->ws->buffer_unmap(dec->ws, buf->res->buf);
 			dec->bs_ptr = NULL;
 			if (!rvid_resize_buffer(dec->screen, &dec->cs, buf, new_size)) {
@@ -925,7 +925,7 @@ static void ruvd_end_frame(struct pipe_video_codec *decoder,
 			   struct pipe_picture_desc *picture)
 {
 	struct ruvd_decoder *dec = (struct ruvd_decoder*)decoder;
-	struct pb_buffer *dt;
+	struct pb_buffer_lean *dt;
 	struct rvid_buffer *msg_fb_it_buf, *bs_buf;
 	unsigned bs_size;
 
@@ -960,7 +960,7 @@ static void ruvd_end_frame(struct pipe_video_codec *decoder,
 	}
 
 	if (dec->dpb.res)
-		dec->msg->body.decode.dpb_size = dec->dpb.res->buf->base.size;
+		dec->msg->body.decode.dpb_size = dec->dpb.res->buf->size;
 	dec->msg->body.decode.bsd_size = bs_size;
 	dec->msg->body.decode.db_pitch = align(dec->base.width, get_db_pitch_alignment(dec));
 
diff --git a/src/gallium/drivers/r600/radeon_uvd.h b/src/gallium/drivers/r600/radeon_uvd.h
index ac4dd1555d004..c7c05a6ef29a2 100644
--- a/src/gallium/drivers/r600/radeon_uvd.h
+++ b/src/gallium/drivers/r600/radeon_uvd.h
@@ -428,7 +428,7 @@ struct ruvd_msg {
 };
 
 /* driver dependent callback */
-typedef struct pb_buffer* (*ruvd_set_dtb)
+typedef struct pb_buffer_lean* (*ruvd_set_dtb)
 (struct ruvd_msg* msg, struct vl_video_buffer *vb);
 
 /* create an UVD decode */
diff --git a/src/gallium/drivers/r600/radeon_vce.c b/src/gallium/drivers/r600/radeon_vce.c
index d0dddb5f7fbb3..b65cab665c0b4 100644
--- a/src/gallium/drivers/r600/radeon_vce.c
+++ b/src/gallium/drivers/r600/radeon_vce.c
@@ -513,7 +513,7 @@ bool rvce_is_fw_version_supported(struct r600_common_screen *rscreen)
 /**
  * Add the buffer as relocation to the current command submission
  */
-void rvce_add_buffer(struct rvce_encoder *enc, struct pb_buffer *buf,
+void rvce_add_buffer(struct rvce_encoder *enc, struct pb_buffer_lean *buf,
                      unsigned usage, enum radeon_bo_domain domain,
                      signed offset)
 {
diff --git a/src/gallium/drivers/r600/radeon_vce.h b/src/gallium/drivers/r600/radeon_vce.h
index f570d04980249..150749a199520 100644
--- a/src/gallium/drivers/r600/radeon_vce.h
+++ b/src/gallium/drivers/r600/radeon_vce.h
@@ -52,7 +52,7 @@ struct r600_common_screen;
 
 /* driver dependent callback */
 typedef void (*rvce_get_buffer)(struct pipe_resource *resource,
-				struct pb_buffer **handle,
+				struct pb_buffer_lean **handle,
 				struct radeon_surf **surface);
 
 /* Coded picture buffer slot */
@@ -391,11 +391,11 @@ struct rvce_encoder {
 
 	rvce_get_buffer			get_buffer;
 
-	struct pb_buffer*	handle;
+	struct pb_buffer_lean*	handle;
 	struct radeon_surf*		luma;
 	struct radeon_surf*		chroma;
 
-	struct pb_buffer*	bs_handle;
+	struct pb_buffer_lean*	bs_handle;
 	unsigned			bs_size;
 
 	struct rvce_cpb_slot		*cpb_array;
@@ -430,7 +430,7 @@ struct pipe_video_codec *rvce_create_encoder(struct pipe_context *context,
 
 bool rvce_is_fw_version_supported(struct r600_common_screen *rscreen);
 
-void rvce_add_buffer(struct rvce_encoder *enc, struct pb_buffer *buf,
+void rvce_add_buffer(struct rvce_encoder *enc, struct pb_buffer_lean *buf,
 		     unsigned usage, enum radeon_bo_domain domain,
 		     signed offset);
 
diff --git a/src/gallium/drivers/r600/radeon_video.c b/src/gallium/drivers/r600/radeon_video.c
index 54325e2957853..46d4e1e2a85dd 100644
--- a/src/gallium/drivers/r600/radeon_video.c
+++ b/src/gallium/drivers/r600/radeon_video.c
@@ -90,7 +90,7 @@ bool rvid_resize_buffer(struct pipe_screen *screen, struct radeon_cmdbuf *cs,
 {
 	struct r600_common_screen *rscreen = (struct r600_common_screen *)screen;
 	struct radeon_winsys* ws = rscreen->ws;
-	unsigned bytes = MIN2(new_buf->res->buf->base.size, new_size);
+	unsigned bytes = MIN2(new_buf->res->buf->size, new_size);
 	struct rvid_buffer old_buf = *new_buf;
 	void *src = NULL, *dst = NULL;
 
@@ -132,7 +132,7 @@ void rvid_clear_buffer(struct pipe_context *context, struct rvid_buffer* buffer)
 	struct r600_common_context *rctx = (struct r600_common_context*)context;
 
 	rctx->dma_clear_buffer(context, &buffer->res->b.b, 0,
-			       buffer->res->buf->base.size, 0);
+			       buffer->res->buf->size, 0);
 	context->flush(context, NULL, 0);
 }
 
@@ -141,13 +141,13 @@ void rvid_clear_buffer(struct pipe_context *context, struct rvid_buffer* buffer)
  * sumup their sizes and replace the backend buffers with a single bo
  */
 void rvid_join_surfaces(struct r600_common_context *rctx,
-			struct pb_buffer** buffers[VL_NUM_COMPONENTS],
+			struct pb_buffer_lean** buffers[VL_NUM_COMPONENTS],
 			struct radeon_surf *surfaces[VL_NUM_COMPONENTS])
 {
 	struct radeon_winsys* ws;
 	unsigned best_tiling, best_wh, off;
 	unsigned size, alignment;
-	struct pb_buffer *pb;
+	struct pb_buffer_lean *pb;
 	unsigned i, j;
 
 	ws = rctx->ws;
@@ -189,9 +189,9 @@ void rvid_join_surfaces(struct r600_common_context *rctx,
 		if (!buffers[i] || !*buffers[i])
 			continue;
 
-		size = align(size, 1 << (*buffers[i])->base.alignment_log2);
-		size += (*buffers[i])->base.size;
-		alignment = MAX2(alignment, 1 << (*buffers[i])->base.alignment_log2);
+		size = align(size, 1 << (*buffers[i])->alignment_log2);
+		size += (*buffers[i])->size;
+		alignment = MAX2(alignment, 1 << (*buffers[i])->alignment_log2);
 	}
 
 	if (!size)
diff --git a/src/gallium/drivers/r600/radeon_video.h b/src/gallium/drivers/r600/radeon_video.h
index 5345cfb28383f..0ab52d3d5ebef 100644
--- a/src/gallium/drivers/r600/radeon_video.h
+++ b/src/gallium/drivers/r600/radeon_video.h
@@ -67,7 +67,7 @@ void rvid_clear_buffer(struct pipe_context *context, struct rvid_buffer* buffer)
 /* join surfaces into the same buffer with identical tiling params
    sum up their sizes and replace the backend buffers with a single bo */
 void rvid_join_surfaces(struct r600_common_context *rctx,
-                        struct pb_buffer** buffers[VL_NUM_COMPONENTS],
+                        struct pb_buffer_lean** buffers[VL_NUM_COMPONENTS],
                         struct radeon_surf *surfaces[VL_NUM_COMPONENTS]);
 
 /* returns supported codecs and other parameters */
diff --git a/src/gallium/drivers/radeonsi/radeon_uvd.c b/src/gallium/drivers/radeonsi/radeon_uvd.c
index f7602429a0905..3a8361f750853 100644
--- a/src/gallium/drivers/radeonsi/radeon_uvd.c
+++ b/src/gallium/drivers/radeonsi/radeon_uvd.c
@@ -103,7 +103,7 @@ static void set_reg(struct ruvd_decoder *dec, unsigned reg, uint32_t val)
 }
 
 /* send a command to the VCPU through the GPCOM registers */
-static void send_cmd(struct ruvd_decoder *dec, unsigned cmd, struct pb_buffer *buf, uint32_t off,
+static void send_cmd(struct ruvd_decoder *dec, unsigned cmd, struct pb_buffer_lean *buf, uint32_t off,
                      unsigned usage, enum radeon_bo_domain domain)
 {
    int reloc_idx;
@@ -226,35 +226,35 @@ static unsigned calc_ctx_size_h264_perf(struct ruvd_decoder *dec)
 
    if (!dec->use_legacy) {
       unsigned fs_in_mb = width_in_mb * height_in_mb;
-      unsigned num_dpb_buffer;
+      unsigned num_dpb_buffer_lean;
       switch (dec->base.level) {
       case 30:
-         num_dpb_buffer = 8100 / fs_in_mb;
+         num_dpb_buffer_lean = 8100 / fs_in_mb;
          break;
       case 31:
-         num_dpb_buffer = 18000 / fs_in_mb;
+         num_dpb_buffer_lean = 18000 / fs_in_mb;
          break;
       case 32:
-         num_dpb_buffer = 20480 / fs_in_mb;
+         num_dpb_buffer_lean = 20480 / fs_in_mb;
          break;
       case 41:
-         num_dpb_buffer = 32768 / fs_in_mb;
+         num_dpb_buffer_lean = 32768 / fs_in_mb;
          break;
       case 42:
-         num_dpb_buffer = 34816 / fs_in_mb;
+         num_dpb_buffer_lean = 34816 / fs_in_mb;
          break;
       case 50:
-         num_dpb_buffer = 110400 / fs_in_mb;
+         num_dpb_buffer_lean = 110400 / fs_in_mb;
          break;
       case 51:
-         num_dpb_buffer = 184320 / fs_in_mb;
+         num_dpb_buffer_lean = 184320 / fs_in_mb;
          break;
       default:
-         num_dpb_buffer = 184320 / fs_in_mb;
+         num_dpb_buffer_lean = 184320 / fs_in_mb;
          break;
       }
-      num_dpb_buffer++;
-      max_references = MAX2(MIN2(NUM_H264_REFS, num_dpb_buffer), max_references);
+      num_dpb_buffer_lean++;
+      max_references = MAX2(MIN2(NUM_H264_REFS, num_dpb_buffer_lean), max_references);
       ctx_size = max_references * align(width_in_mb * height_in_mb * 192, 256);
    } else {
       // the firmware seems to always assume a minimum of ref frames
@@ -351,38 +351,38 @@ static unsigned calc_dpb_size(struct ruvd_decoder *dec)
    case PIPE_VIDEO_FORMAT_MPEG4_AVC: {
       if (!dec->use_legacy) {
          unsigned fs_in_mb = width_in_mb * height_in_mb;
-         unsigned alignment = 64, num_dpb_buffer;
+         unsigned alignment = 64, num_dpb_buffer_lean;
 
          if (dec->stream_type == RUVD_CODEC_H264_PERF)
             alignment = 256;
          switch (dec->base.level) {
          case 30:
-            num_dpb_buffer = 8100 / fs_in_mb;
+            num_dpb_buffer_lean = 8100 / fs_in_mb;
             break;
          case 31:
-            num_dpb_buffer = 18000 / fs_in_mb;
+            num_dpb_buffer_lean = 18000 / fs_in_mb;
             break;
          case 32:
-            num_dpb_buffer = 20480 / fs_in_mb;
+            num_dpb_buffer_lean = 20480 / fs_in_mb;
             break;
          case 41:
-            num_dpb_buffer = 32768 / fs_in_mb;
+            num_dpb_buffer_lean = 32768 / fs_in_mb;
             break;
          case 42:
-            num_dpb_buffer = 34816 / fs_in_mb;
+            num_dpb_buffer_lean = 34816 / fs_in_mb;
             break;
          case 50:
-            num_dpb_buffer = 110400 / fs_in_mb;
+            num_dpb_buffer_lean = 110400 / fs_in_mb;
             break;
          case 51:
-            num_dpb_buffer = 184320 / fs_in_mb;
+            num_dpb_buffer_lean = 184320 / fs_in_mb;
             break;
          default:
-            num_dpb_buffer = 184320 / fs_in_mb;
+            num_dpb_buffer_lean = 184320 / fs_in_mb;
             break;
          }
-         num_dpb_buffer++;
-         max_references = MAX2(MIN2(NUM_H264_REFS, num_dpb_buffer), max_references);
+         num_dpb_buffer_lean++;
+         max_references = MAX2(MIN2(NUM_H264_REFS, num_dpb_buffer_lean), max_references);
          dpb_size = image_size * max_references;
          if ((dec->stream_type != RUVD_CODEC_H264_PERF) ||
              (((struct si_screen *)dec->screen)->info.family < CHIP_POLARIS10)) {
@@ -1046,7 +1046,7 @@ static void ruvd_decode_bitstream(struct pipe_video_codec *decoder,
       struct rvid_buffer *buf = &dec->bs_buffers[dec->cur_buffer];
       unsigned new_size = dec->bs_size + sizes[i];
 
-      if (new_size > buf->res->buf->base.size) {
+      if (new_size > buf->res->buf->size) {
          dec->ws->buffer_unmap(dec->ws, buf->res->buf);
          if (!si_vid_resize_buffer(dec->screen, &dec->cs, buf, new_size, NULL)) {
             RVID_ERR("Can't resize bitstream buffer!");
@@ -1074,7 +1074,7 @@ static void ruvd_end_frame(struct pipe_video_codec *decoder, struct pipe_video_b
                            struct pipe_picture_desc *picture)
 {
    struct ruvd_decoder *dec = (struct ruvd_decoder *)decoder;
-   struct pb_buffer *dt;
+   struct pb_buffer_lean *dt;
    struct rvid_buffer *msg_fb_it_buf, *bs_buf;
    unsigned bs_size;
 
@@ -1110,13 +1110,13 @@ static void ruvd_end_frame(struct pipe_video_codec *decoder, struct pipe_video_b
    }
 
    if (dec->dpb.res)
-      dec->msg->body.decode.dpb_size = dec->dpb.res->buf->base.size;
+      dec->msg->body.decode.dpb_size = dec->dpb.res->buf->size;
    dec->msg->body.decode.bsd_size = bs_size;
    dec->msg->body.decode.db_pitch = align(dec->base.width, get_db_pitch_alignment(dec));
 
    if (dec->stream_type == RUVD_CODEC_H264_PERF &&
        ((struct si_screen *)dec->screen)->info.family >= CHIP_POLARIS10)
-      dec->msg->body.decode.dpb_reserved = dec->ctx.res->buf->base.size;
+      dec->msg->body.decode.dpb_reserved = dec->ctx.res->buf->size;
 
    dt = dec->set_dtb(dec->msg, (struct vl_video_buffer *)target);
    if (((struct si_screen *)dec->screen)->info.family >= CHIP_STONEY)
@@ -1144,7 +1144,7 @@ static void ruvd_end_frame(struct pipe_video_codec *decoder, struct pipe_video_b
       }
 
       if (dec->ctx.res)
-         dec->msg->body.decode.dpb_reserved = dec->ctx.res->buf->base.size;
+         dec->msg->body.decode.dpb_reserved = dec->ctx.res->buf->size;
       break;
 
    case PIPE_VIDEO_FORMAT_VC1:
diff --git a/src/gallium/drivers/radeonsi/radeon_uvd.h b/src/gallium/drivers/radeonsi/radeon_uvd.h
index a59d2db698f42..d535314d822a8 100644
--- a/src/gallium/drivers/radeonsi/radeon_uvd.h
+++ b/src/gallium/drivers/radeonsi/radeon_uvd.h
@@ -15,7 +15,7 @@
 #include "ac_uvd_dec.h"
 
 /* driver dependent callback */
-typedef struct pb_buffer *(*ruvd_set_dtb)(struct ruvd_msg *msg, struct vl_video_buffer *vb);
+typedef struct pb_buffer_lean *(*ruvd_set_dtb)(struct ruvd_msg *msg, struct vl_video_buffer *vb);
 
 /* create an UVD decode */
 struct pipe_video_codec *si_common_uvd_create_decoder(struct pipe_context *context,
diff --git a/src/gallium/drivers/radeonsi/radeon_uvd_enc.h b/src/gallium/drivers/radeonsi/radeon_uvd_enc.h
index 2797dfa01bf6d..43e902913c1ed 100644
--- a/src/gallium/drivers/radeonsi/radeon_uvd_enc.h
+++ b/src/gallium/drivers/radeonsi/radeon_uvd_enc.h
@@ -328,7 +328,7 @@ typedef struct ruvd_enc_vui_info_s
    uint32_t max_num_reorder_frames;
 } ruvd_enc_vui_info;
 
-typedef void (*radeon_uvd_enc_get_buffer)(struct pipe_resource *resource, struct pb_buffer **handle,
+typedef void (*radeon_uvd_enc_get_buffer)(struct pipe_resource *resource, struct pb_buffer_lean **handle,
                                           struct radeon_surf **surface);
 
 struct pipe_video_codec *radeon_uvd_create_encoder(struct pipe_context *context,
@@ -406,11 +406,11 @@ struct radeon_uvd_encoder {
 
    radeon_uvd_enc_get_buffer get_buffer;
 
-   struct pb_buffer *handle;
+   struct pb_buffer_lean *handle;
    struct radeon_surf *luma;
    struct radeon_surf *chroma;
 
-   struct pb_buffer *bs_handle;
+   struct pb_buffer_lean *bs_handle;
    unsigned bs_size;
 
    unsigned cpb_num;
diff --git a/src/gallium/drivers/radeonsi/radeon_uvd_enc_1_1.c b/src/gallium/drivers/radeonsi/radeon_uvd_enc_1_1.c
index d1068c1c2adc5..4ecaa4405c85e 100644
--- a/src/gallium/drivers/radeonsi/radeon_uvd_enc_1_1.c
+++ b/src/gallium/drivers/radeonsi/radeon_uvd_enc_1_1.c
@@ -34,7 +34,7 @@
 
 static const unsigned index_to_shifts[4] = {24, 16, 8, 0};
 
-static void radeon_uvd_enc_add_buffer(struct radeon_uvd_encoder *enc, struct pb_buffer *buf,
+static void radeon_uvd_enc_add_buffer(struct radeon_uvd_encoder *enc, struct pb_buffer_lean *buf,
                                       unsigned usage, enum radeon_bo_domain domain,
                                       signed offset)
 {
diff --git a/src/gallium/drivers/radeonsi/radeon_vce.c b/src/gallium/drivers/radeonsi/radeon_vce.c
index 9a35e1d39afa3..06e4eb76cb701 100644
--- a/src/gallium/drivers/radeonsi/radeon_vce.c
+++ b/src/gallium/drivers/radeonsi/radeon_vce.c
@@ -528,7 +528,7 @@ bool si_vce_is_fw_version_supported(struct si_screen *sscreen)
 /**
  * Add the buffer as relocation to the current command submission
  */
-void si_vce_add_buffer(struct rvce_encoder *enc, struct pb_buffer *buf, unsigned usage,
+void si_vce_add_buffer(struct rvce_encoder *enc, struct pb_buffer_lean *buf, unsigned usage,
                        enum radeon_bo_domain domain, signed offset)
 {
    int reloc_idx;
diff --git a/src/gallium/drivers/radeonsi/radeon_vce.h b/src/gallium/drivers/radeonsi/radeon_vce.h
index 8de161a87199f..f1813a2ce670d 100644
--- a/src/gallium/drivers/radeonsi/radeon_vce.h
+++ b/src/gallium/drivers/radeonsi/radeon_vce.h
@@ -33,7 +33,7 @@
 struct si_screen;
 
 /* driver dependent callback */
-typedef void (*rvce_get_buffer)(struct pipe_resource *resource, struct pb_buffer **handle,
+typedef void (*rvce_get_buffer)(struct pipe_resource *resource, struct pb_buffer_lean **handle,
                                 struct radeon_surf **surface);
 
 /* Coded picture buffer slot */
@@ -374,11 +374,11 @@ struct rvce_encoder {
 
    rvce_get_buffer get_buffer;
 
-   struct pb_buffer *handle;
+   struct pb_buffer_lean *handle;
    struct radeon_surf *luma;
    struct radeon_surf *chroma;
 
-   struct pb_buffer *bs_handle;
+   struct pb_buffer_lean *bs_handle;
    unsigned bs_size;
 
    struct rvce_cpb_slot *cpb_array;
@@ -413,7 +413,7 @@ struct pipe_video_codec *si_vce_create_encoder(struct pipe_context *context,
 
 bool si_vce_is_fw_version_supported(struct si_screen *sscreen);
 
-void si_vce_add_buffer(struct rvce_encoder *enc, struct pb_buffer *buf, unsigned usage,
+void si_vce_add_buffer(struct rvce_encoder *enc, struct pb_buffer_lean *buf, unsigned usage,
                        enum radeon_bo_domain domain, signed offset);
 
 /* init vce fw 40.2.2 specific callbacks */
diff --git a/src/gallium/drivers/radeonsi/radeon_vce_50.c b/src/gallium/drivers/radeonsi/radeon_vce_50.c
index e89ab4974dd70..d555be36441c7 100644
--- a/src/gallium/drivers/radeonsi/radeon_vce_50.c
+++ b/src/gallium/drivers/radeonsi/radeon_vce_50.c
@@ -79,7 +79,7 @@ static void encode(struct rvce_encoder *enc)
 
    if (enc->dual_pipe) {
       unsigned aux_offset =
-         enc->cpb.res->buf->base.size - RVCE_MAX_AUX_BUFFER_NUM * RVCE_MAX_BITSTREAM_OUTPUT_ROW_SIZE * 2;
+         enc->cpb.res->buf->size - RVCE_MAX_AUX_BUFFER_NUM * RVCE_MAX_BITSTREAM_OUTPUT_ROW_SIZE * 2;
       RVCE_BEGIN(0x05000002); // auxiliary buffer
       for (i = 0; i < 8; ++i) {
          RVCE_CS(aux_offset);
diff --git a/src/gallium/drivers/radeonsi/radeon_vce_52.c b/src/gallium/drivers/radeonsi/radeon_vce_52.c
index 19fa18a65a4c4..f2400cd749671 100644
--- a/src/gallium/drivers/radeonsi/radeon_vce_52.c
+++ b/src/gallium/drivers/radeonsi/radeon_vce_52.c
@@ -226,7 +226,7 @@ static void encode(struct rvce_encoder *enc)
 
    if (enc->dual_pipe) {
       unsigned aux_offset =
-         enc->cpb.res->buf->base.size - RVCE_MAX_AUX_BUFFER_NUM * RVCE_MAX_BITSTREAM_OUTPUT_ROW_SIZE * 2;
+         enc->cpb.res->buf->size - RVCE_MAX_AUX_BUFFER_NUM * RVCE_MAX_BITSTREAM_OUTPUT_ROW_SIZE * 2;
       RVCE_BEGIN(0x05000002); // auxiliary buffer
       for (i = 0; i < 8; ++i) {
          RVCE_CS(aux_offset);
diff --git a/src/gallium/drivers/radeonsi/radeon_vcn_dec.c b/src/gallium/drivers/radeonsi/radeon_vcn_dec.c
index fe40358c5acbd..c6fa84af78a9f 100644
--- a/src/gallium/drivers/radeonsi/radeon_vcn_dec.c
+++ b/src/gallium/drivers/radeonsi/radeon_vcn_dec.c
@@ -1912,9 +1912,9 @@ static unsigned rvcn_dec_dynamic_dpb_t2_message(struct radeon_decoder *dec, rvcn
    return 0;
 }
 
-static struct pb_buffer *rvcn_dec_message_decode(struct radeon_decoder *dec,
-                                                 struct pipe_video_buffer *target,
-                                                 struct pipe_picture_desc *picture)
+static struct pb_buffer_lean *rvcn_dec_message_decode(struct radeon_decoder *dec,
+                                                      struct pipe_video_buffer *target,
+                                                      struct pipe_picture_desc *picture)
 {
    DECRYPT_PARAMETERS *decrypt = (DECRYPT_PARAMETERS *)picture->decrypt_key;
    bool encrypted = picture->protected_playback;
@@ -2164,14 +2164,14 @@ static struct pb_buffer *rvcn_dec_message_decode(struct radeon_decoder *dec,
    luma   = (struct si_texture *)((struct vl_video_buffer *)out_surf)->resources[0];
    chroma = (struct si_texture *)((struct vl_video_buffer *)out_surf)->resources[1];
 
-   decode->dpb_size = (dec->dpb_type != DPB_DYNAMIC_TIER_2) ? dec->dpb.res->buf->base.size : 0;
+   decode->dpb_size = (dec->dpb_type != DPB_DYNAMIC_TIER_2) ? dec->dpb.res->buf->size : 0;
 
    /* When texture being created, the bo will be created with total size of planes,
     * and all planes point to the same buffer */
-   assert(si_resource(((struct vl_video_buffer *)out_surf)->resources[0])->buf->base.size ==
-      si_resource(((struct vl_video_buffer *)out_surf)->resources[1])->buf->base.size);
+   assert(si_resource(((struct vl_video_buffer *)out_surf)->resources[0])->buf->size ==
+      si_resource(((struct vl_video_buffer *)out_surf)->resources[1])->buf->size);
 
-   decode->dt_size = si_resource(((struct vl_video_buffer *)out_surf)->resources[0])->buf->base.size;
+   decode->dt_size = si_resource(((struct vl_video_buffer *)out_surf)->resources[0])->buf->size;
 
    decode->sct_size = 0;
    decode->sc_coeff_size = 0;
@@ -2364,7 +2364,7 @@ static struct pb_buffer *rvcn_dec_message_decode(struct radeon_decoder *dec,
    }
 
    if (dec->ctx.res)
-      decode->hw_ctxt_size = dec->ctx.res->buf->base.size;
+      decode->hw_ctxt_size = dec->ctx.res->buf->size;
 
    if (dec->dpb_type == DPB_DYNAMIC_TIER_2)
       if (rvcn_dec_dynamic_dpb_t2_message(dec, decode, dynamic_dpb_t2, encrypted))
@@ -2418,7 +2418,7 @@ static void set_reg(struct radeon_decoder *dec, unsigned reg, uint32_t val)
 }
 
 /* send a command to the VCPU through the GPCOM registers */
-static void send_cmd(struct radeon_decoder *dec, unsigned cmd, struct pb_buffer *buf, uint32_t off,
+static void send_cmd(struct radeon_decoder *dec, unsigned cmd, struct pb_buffer_lean *buf, uint32_t off,
                      unsigned usage, enum radeon_bo_domain domain)
 {
    uint64_t addr;
@@ -2586,35 +2586,35 @@ static unsigned calc_ctx_size_h264_perf(struct radeon_decoder *dec)
    height_in_mb = align(height / VL_MACROBLOCK_HEIGHT, 2);
 
    unsigned fs_in_mb = width_in_mb * height_in_mb;
-   unsigned num_dpb_buffer;
+   unsigned num_dpb_buffer_lean;
    switch (dec->base.level) {
    case 30:
-      num_dpb_buffer = 8100 / fs_in_mb;
+      num_dpb_buffer_lean = 8100 / fs_in_mb;
       break;
    case 31:
-      num_dpb_buffer = 18000 / fs_in_mb;
+      num_dpb_buffer_lean = 18000 / fs_in_mb;
       break;
    case 32:
-      num_dpb_buffer = 20480 / fs_in_mb;
+      num_dpb_buffer_lean = 20480 / fs_in_mb;
       break;
    case 41:
-      num_dpb_buffer = 32768 / fs_in_mb;
+      num_dpb_buffer_lean = 32768 / fs_in_mb;
       break;
    case 42:
-      num_dpb_buffer = 34816 / fs_in_mb;
+      num_dpb_buffer_lean = 34816 / fs_in_mb;
       break;
    case 50:
-      num_dpb_buffer = 110400 / fs_in_mb;
+      num_dpb_buffer_lean = 110400 / fs_in_mb;
       break;
    case 51:
-      num_dpb_buffer = 184320 / fs_in_mb;
+      num_dpb_buffer_lean = 184320 / fs_in_mb;
       break;
    default:
-      num_dpb_buffer = 184320 / fs_in_mb;
+      num_dpb_buffer_lean = 184320 / fs_in_mb;
       break;
    }
-   num_dpb_buffer++;
-   max_references = MAX2(MIN2(NUM_H264_REFS, num_dpb_buffer), max_references);
+   num_dpb_buffer_lean++;
+   max_references = MAX2(MIN2(NUM_H264_REFS, num_dpb_buffer_lean), max_references);
    ctx_size = max_references * align(width_in_mb * height_in_mb * 192, 256);
 
    return ctx_size;
@@ -2644,36 +2644,36 @@ static unsigned calc_dpb_size(struct radeon_decoder *dec)
    switch (u_reduce_video_profile(dec->base.profile)) {
    case PIPE_VIDEO_FORMAT_MPEG4_AVC: {
       unsigned fs_in_mb = width_in_mb * height_in_mb;
-      unsigned num_dpb_buffer;
+      unsigned num_dpb_buffer_lean;
 
       switch (dec->base.level) {
       case 30:
-         num_dpb_buffer = 8100 / fs_in_mb;
+         num_dpb_buffer_lean = 8100 / fs_in_mb;
          break;
       case 31:
-         num_dpb_buffer = 18000 / fs_in_mb;
+         num_dpb_buffer_lean = 18000 / fs_in_mb;
          break;
       case 32:
-         num_dpb_buffer = 20480 / fs_in_mb;
+         num_dpb_buffer_lean = 20480 / fs_in_mb;
          break;
       case 41:
-         num_dpb_buffer = 32768 / fs_in_mb;
+         num_dpb_buffer_lean = 32768 / fs_in_mb;
          break;
       case 42:
-         num_dpb_buffer = 34816 / fs_in_mb;
+         num_dpb_buffer_lean = 34816 / fs_in_mb;
          break;
       case 50:
-         num_dpb_buffer = 110400 / fs_in_mb;
+         num_dpb_buffer_lean = 110400 / fs_in_mb;
          break;
       case 51:
-         num_dpb_buffer = 184320 / fs_in_mb;
+         num_dpb_buffer_lean = 184320 / fs_in_mb;
          break;
       default:
-         num_dpb_buffer = 184320 / fs_in_mb;
+         num_dpb_buffer_lean = 184320 / fs_in_mb;
          break;
       }
-      num_dpb_buffer++;
-      max_references = MAX2(MIN2(NUM_H264_REFS, num_dpb_buffer), max_references);
+      num_dpb_buffer_lean++;
+      max_references = MAX2(MIN2(NUM_H264_REFS, num_dpb_buffer_lean), max_references);
       dpb_size = image_size * max_references;
       break;
    }
@@ -2882,7 +2882,7 @@ static void radeon_dec_decode_bitstream(struct pipe_video_codec *decoder,
 
    struct rvid_buffer *buf = &dec->bs_buffers[dec->cur_buffer];
 
-   if (total_bs_size > buf->res->buf->base.size) {
+   if (total_bs_size > buf->res->buf->size) {
       dec->ws->buffer_unmap(dec->ws, buf->res->buf);
       dec->bs_ptr = NULL;
       if (!si_vid_resize_buffer(dec->screen, &dec->cs, buf, total_bs_size, NULL)) {
@@ -2911,7 +2911,7 @@ static void radeon_dec_decode_bitstream(struct pipe_video_codec *decoder,
 void send_cmd_dec(struct radeon_decoder *dec, struct pipe_video_buffer *target,
                   struct pipe_picture_desc *picture)
 {
-   struct pb_buffer *dt;
+   struct pb_buffer_lean *dt;
    struct rvid_buffer *msg_fb_it_probs_buf, *bs_buf;
 
    msg_fb_it_probs_buf = &dec->msg_fb_it_probs_buffers[dec->cur_buffer];
diff --git a/src/gallium/drivers/radeonsi/radeon_vcn_dec_jpeg.c b/src/gallium/drivers/radeonsi/radeon_vcn_dec_jpeg.c
index 14c2e857f041e..11c24f6a19836 100644
--- a/src/gallium/drivers/radeonsi/radeon_vcn_dec_jpeg.c
+++ b/src/gallium/drivers/radeonsi/radeon_vcn_dec_jpeg.c
@@ -16,9 +16,9 @@
 #include <assert.h>
 #include <stdio.h>
 
-static struct pb_buffer *radeon_jpeg_get_decode_param(struct radeon_decoder *dec,
-                                                      struct pipe_video_buffer *target,
-                                                      struct pipe_picture_desc *picture)
+static struct pb_buffer_lean *radeon_jpeg_get_decode_param(struct radeon_decoder *dec,
+                                                           struct pipe_video_buffer *target,
+                                                           struct pipe_picture_desc *picture)
 {
    struct si_texture *luma = (struct si_texture *)((struct vl_video_buffer *)target)->resources[0];
    struct si_texture *chroma, *chromav;
@@ -62,7 +62,7 @@ static void set_reg_jpeg(struct radeon_decoder *dec, unsigned reg, unsigned cond
 }
 
 /* send a bitstream buffer command */
-static void send_cmd_bitstream(struct radeon_decoder *dec, struct pb_buffer *buf, uint32_t off,
+static void send_cmd_bitstream(struct radeon_decoder *dec, struct pb_buffer_lean *buf, uint32_t off,
                                unsigned usage, enum radeon_bo_domain domain)
 {
    uint64_t addr;
@@ -105,7 +105,7 @@ static void send_cmd_bitstream(struct radeon_decoder *dec, struct pb_buffer *buf
 }
 
 /* send a target buffer command */
-static void send_cmd_target(struct radeon_decoder *dec, struct pb_buffer *buf, uint32_t off,
+static void send_cmd_target(struct radeon_decoder *dec, struct pb_buffer_lean *buf, uint32_t off,
                             unsigned usage, enum radeon_bo_domain domain)
 {
    uint64_t addr;
@@ -184,7 +184,7 @@ static void send_cmd_target(struct radeon_decoder *dec, struct pb_buffer *buf, u
 }
 
 /* send a bitstream buffer command */
-static void send_cmd_bitstream_direct(struct radeon_decoder *dec, struct pb_buffer *buf,
+static void send_cmd_bitstream_direct(struct radeon_decoder *dec, struct pb_buffer_lean *buf,
                                       uint32_t off, unsigned usage,
                                       enum radeon_bo_domain domain)
 {
@@ -224,7 +224,7 @@ static void send_cmd_bitstream_direct(struct radeon_decoder *dec, struct pb_buff
 }
 
 /* send a target buffer command */
-static void send_cmd_target_direct(struct radeon_decoder *dec, struct pb_buffer *buf, uint32_t off,
+static void send_cmd_target_direct(struct radeon_decoder *dec, struct pb_buffer_lean *buf, uint32_t off,
                                    unsigned usage, enum radeon_bo_domain domain,
                                    enum pipe_format buffer_format)
 {
@@ -360,7 +360,7 @@ static void send_cmd_target_direct(struct radeon_decoder *dec, struct pb_buffer
 void send_cmd_jpeg(struct radeon_decoder *dec, struct pipe_video_buffer *target,
                    struct pipe_picture_desc *picture)
 {
-   struct pb_buffer *dt;
+   struct pb_buffer_lean *dt;
    struct rvid_buffer *bs_buf;
 
    bs_buf = &dec->bs_buffers[dec->cur_buffer];
diff --git a/src/gallium/drivers/radeonsi/radeon_vcn_enc.c b/src/gallium/drivers/radeonsi/radeon_vcn_enc.c
index ef0c7c33ee7f5..4bd1f9b468bb4 100644
--- a/src/gallium/drivers/radeonsi/radeon_vcn_enc.c
+++ b/src/gallium/drivers/radeonsi/radeon_vcn_enc.c
@@ -1160,7 +1160,7 @@ static void radeon_enc_encode_bitstream(struct pipe_video_codec *encoder,
 
    if (vid_buf->base.statistics_data) {
       enc->get_buffer(vid_buf->base.statistics_data, &enc->stats, NULL);
-      if (enc->stats->base.size < sizeof(rvcn_encode_stats_type_0_t)) {
+      if (enc->stats->size < sizeof(rvcn_encode_stats_type_0_t)) {
          RVID_ERR("Encoder statistics output buffer is too small.\n");
          enc->stats = NULL;
       }
@@ -1291,7 +1291,7 @@ error:
    return NULL;
 }
 
-void radeon_enc_add_buffer(struct radeon_encoder *enc, struct pb_buffer *buf,
+void radeon_enc_add_buffer(struct radeon_encoder *enc, struct pb_buffer_lean *buf,
                            unsigned usage, enum radeon_bo_domain domain, signed offset)
 {
    enc->ws->cs_add_buffer(&enc->cs, buf, usage | RADEON_USAGE_SYNCHRONIZED, domain);
diff --git a/src/gallium/drivers/radeonsi/radeon_vcn_enc.h b/src/gallium/drivers/radeonsi/radeon_vcn_enc.h
index 5287090654036..e75479d7106a2 100644
--- a/src/gallium/drivers/radeonsi/radeon_vcn_enc.h
+++ b/src/gallium/drivers/radeonsi/radeon_vcn_enc.h
@@ -49,7 +49,7 @@
       }                                                                                          \
    } while(0)
 
-typedef void (*radeon_enc_get_buffer)(struct pipe_resource *resource, struct pb_buffer **handle,
+typedef void (*radeon_enc_get_buffer)(struct pipe_resource *resource, struct pb_buffer_lean **handle,
                                       struct radeon_surf **surface);
 
 struct pipe_video_codec *radeon_create_encoder(struct pipe_context *context,
@@ -233,11 +233,11 @@ struct radeon_encoder {
 
    radeon_enc_get_buffer get_buffer;
 
-   struct pb_buffer *handle;
+   struct pb_buffer_lean *handle;
    struct radeon_surf *luma;
    struct radeon_surf *chroma;
 
-   struct pb_buffer *bs_handle;
+   struct pb_buffer_lean *bs_handle;
    unsigned bs_size;
 
    struct rvid_buffer *si;
@@ -246,7 +246,7 @@ struct radeon_encoder {
    struct rvid_buffer *cdf;
    struct rvid_buffer *roi;
    struct radeon_enc_pic enc_pic;
-   struct pb_buffer *stats;
+   struct pb_buffer_lean *stats;
    rvcn_enc_cmd_t cmd;
 
    unsigned alignment;
@@ -270,7 +270,7 @@ struct radeon_encoder {
    struct pipe_context *ectx;
 };
 
-void radeon_enc_add_buffer(struct radeon_encoder *enc, struct pb_buffer *buf,
+void radeon_enc_add_buffer(struct radeon_encoder *enc, struct pb_buffer_lean *buf,
                            unsigned usage, enum radeon_bo_domain domain, signed offset);
 
 void radeon_enc_dummy(struct radeon_encoder *enc);
diff --git a/src/gallium/drivers/radeonsi/radeon_video.c b/src/gallium/drivers/radeonsi/radeon_video.c
index 298eb54b59f13..a5e2b626abc7c 100644
--- a/src/gallium/drivers/radeonsi/radeon_video.c
+++ b/src/gallium/drivers/radeonsi/radeon_video.c
@@ -73,7 +73,7 @@ bool si_vid_resize_buffer(struct pipe_screen *screen, struct radeon_cmdbuf *cs,
 {
    struct si_screen *sscreen = (struct si_screen *)screen;
    struct radeon_winsys *ws = sscreen->ws;
-   unsigned bytes = MIN2(new_buf->res->buf->base.size, new_size);
+   unsigned bytes = MIN2(new_buf->res->buf->size, new_size);
    struct rvid_buffer old_buf = *new_buf;
    void *src = NULL, *dst = NULL;
 
diff --git a/src/gallium/drivers/radeonsi/si_buffer.c b/src/gallium/drivers/radeonsi/si_buffer.c
index 92aa5aeed83fe..85d0ea5dffc1e 100644
--- a/src/gallium/drivers/radeonsi/si_buffer.c
+++ b/src/gallium/drivers/radeonsi/si_buffer.c
@@ -12,7 +12,7 @@
 #include <inttypes.h>
 #include <stdio.h>
 
-bool si_cs_is_buffer_referenced(struct si_context *sctx, struct pb_buffer *buf,
+bool si_cs_is_buffer_referenced(struct si_context *sctx, struct pb_buffer_lean *buf,
                                 unsigned usage)
 {
    return sctx->ws->cs_is_buffer_referenced(&sctx->gfx_cs, buf, usage);
@@ -144,7 +144,7 @@ void si_init_resource_fields(struct si_screen *sscreen, struct si_resource *res,
 
 bool si_alloc_resource(struct si_screen *sscreen, struct si_resource *res)
 {
-   struct pb_buffer *old_buf, *new_buf;
+   struct pb_buffer_lean *old_buf, *new_buf;
 
    /* Allocate a new resource. */
    new_buf = sscreen->ws->buffer_create(sscreen->ws, res->bo_size, 1 << res->bo_alignment_log2,
@@ -179,7 +179,7 @@ bool si_alloc_resource(struct si_screen *sscreen, struct si_resource *res)
    /* Print debug information. */
    if (sscreen->debug_flags & DBG(VM) && res->b.b.target == PIPE_BUFFER) {
       fprintf(stderr, "VM start=0x%" PRIX64 "  end=0x%" PRIX64 " | Buffer %" PRIu64 " bytes | Flags: ",
-              res->gpu_address, res->gpu_address + res->buf->base.size, res->buf->base.size);
+              res->gpu_address, res->gpu_address + res->buf->size, res->buf->size);
       si_res_print_flags(res->flags);
       fprintf(stderr, "\n");
    }
@@ -640,10 +640,10 @@ static struct pipe_resource *si_buffer_from_user_memory(struct pipe_screen *scre
 
 struct pipe_resource *si_buffer_from_winsys_buffer(struct pipe_screen *screen,
                                                    const struct pipe_resource *templ,
-                                                   struct pb_buffer *imported_buf,
+                                                   struct pb_buffer_lean *imported_buf,
                                                    uint64_t offset)
 {
-   if (offset + templ->width0 > imported_buf->base.size)
+   if (offset + templ->width0 > imported_buf->size)
       return NULL;
 
    struct si_screen *sscreen = (struct si_screen *)screen;
@@ -679,8 +679,8 @@ struct pipe_resource *si_buffer_from_winsys_buffer(struct pipe_screen *screen,
          res->b.b.usage = PIPE_USAGE_STAGING;
    }
 
-   si_init_resource_fields(sscreen, res, imported_buf->base.size,
-                           1 << imported_buf->base.alignment_log2);
+   si_init_resource_fields(sscreen, res, imported_buf->size,
+                           1 << imported_buf->alignment_log2);
 
    res->b.is_shared = true;
    res->b.buffer_id_unique = util_idalloc_mt_alloc(&sscreen->buffer_ids);
diff --git a/src/gallium/drivers/radeonsi/si_perfcounter.c b/src/gallium/drivers/radeonsi/si_perfcounter.c
index 821d3a436bddc..c54854bd9a7e5 100644
--- a/src/gallium/drivers/radeonsi/si_perfcounter.c
+++ b/src/gallium/drivers/radeonsi/si_perfcounter.c
@@ -899,7 +899,7 @@ si_spm_init(struct si_context *sctx)
 void
 si_spm_finish(struct si_context *sctx)
 {
-   struct pb_buffer *bo = sctx->spm.bo;
+   struct pb_buffer_lean *bo = sctx->spm.bo;
    radeon_bo_reference(sctx->screen->ws, &bo, NULL);
 
    ac_destroy_spm(&sctx->spm);
diff --git a/src/gallium/drivers/radeonsi/si_pipe.c b/src/gallium/drivers/radeonsi/si_pipe.c
index 77a4a8803ad07..aea75511cc159 100644
--- a/src/gallium/drivers/radeonsi/si_pipe.c
+++ b/src/gallium/drivers/radeonsi/si_pipe.c
@@ -1087,7 +1087,7 @@ static void si_test_gds_memory_management(struct si_context *sctx, unsigned allo
 {
    struct radeon_winsys *ws = sctx->ws;
    struct radeon_cmdbuf cs[8];
-   struct pb_buffer *gds_bo[ARRAY_SIZE(cs)];
+   struct pb_buffer_lean *gds_bo[ARRAY_SIZE(cs)];
 
    for (unsigned i = 0; i < ARRAY_SIZE(cs); i++) {
       ws->cs_create(&cs[i], sctx->ctx, AMD_IP_COMPUTE, NULL, NULL);
diff --git a/src/gallium/drivers/radeonsi/si_pipe.h b/src/gallium/drivers/radeonsi/si_pipe.h
index 237b234978a3d..6a771e6d3e88a 100644
--- a/src/gallium/drivers/radeonsi/si_pipe.h
+++ b/src/gallium/drivers/radeonsi/si_pipe.h
@@ -322,7 +322,7 @@ struct si_resource {
    uint32_t _pad;
 
    /* Winsys objects. */
-   struct pb_buffer *buf;
+   struct pb_buffer_lean *buf;
    uint64_t gpu_address;
 
    /* Resource properties. */
@@ -438,7 +438,7 @@ struct si_texture {
  */
 struct si_auxiliary_texture {
    struct threaded_resource b;
-   struct pb_buffer *buffer;
+   struct pb_buffer_lean *buffer;
    uint32_t offset;
    uint32_t stride;
 };
@@ -529,7 +529,7 @@ union si_mmio_counters {
 
 struct si_memory_object {
    struct pipe_memory_object b;
-   struct pb_buffer *buf;
+   struct pb_buffer_lean *buf;
    uint32_t stride;
 };
 
@@ -715,7 +715,7 @@ struct si_screen {
 
    /* NGG streamout. */
    simple_mtx_t gds_mutex;
-   struct pb_buffer *gds_oa;
+   struct pb_buffer_lean *gds_oa;
 };
 
 struct si_compute {
@@ -1420,7 +1420,7 @@ void si_gfx_blit(struct pipe_context *ctx, const struct pipe_blit_info *info);
 bool si_nir_is_output_const_if_tex_is_const(struct nir_shader *shader, float *in, float *out, int *texunit);
 
 /* si_buffer.c */
-bool si_cs_is_buffer_referenced(struct si_context *sctx, struct pb_buffer *buf,
+bool si_cs_is_buffer_referenced(struct si_context *sctx, struct pb_buffer_lean *buf,
                                 unsigned usage);
 void *si_buffer_map(struct si_context *sctx, struct si_resource *resource,
                     unsigned usage);
@@ -1433,7 +1433,7 @@ struct si_resource *si_aligned_buffer_create(struct pipe_screen *screen, unsigne
                                              unsigned usage, unsigned size, unsigned alignment);
 struct pipe_resource *si_buffer_from_winsys_buffer(struct pipe_screen *screen,
                                                    const struct pipe_resource *templ,
-                                                   struct pb_buffer *imported_buf,
+                                                   struct pb_buffer_lean *imported_buf,
                                                    uint64_t offset);
 void si_replace_buffer_storage(struct pipe_context *ctx, struct pipe_resource *dst,
                                struct pipe_resource *src, unsigned num_rebinds,
diff --git a/src/gallium/drivers/radeonsi/si_sqtt.c b/src/gallium/drivers/radeonsi/si_sqtt.c
index 48b5b98147462..335107dff17ee 100644
--- a/src/gallium/drivers/radeonsi/si_sqtt.c
+++ b/src/gallium/drivers/radeonsi/si_sqtt.c
@@ -669,7 +669,7 @@ bool si_init_sqtt(struct si_context *sctx)
 void si_destroy_sqtt(struct si_context *sctx)
 {
    struct si_screen *sscreen = sctx->screen;
-   struct pb_buffer *bo = sctx->sqtt->bo;
+   struct pb_buffer_lean *bo = sctx->sqtt->bo;
    radeon_bo_reference(sctx->screen->ws, &bo, NULL);
 
    if (sctx->sqtt->trigger_file)
diff --git a/src/gallium/drivers/radeonsi/si_texture.c b/src/gallium/drivers/radeonsi/si_texture.c
index f55ba96870a0e..070e568bc3298 100644
--- a/src/gallium/drivers/radeonsi/si_texture.c
+++ b/src/gallium/drivers/radeonsi/si_texture.c
@@ -949,7 +949,7 @@ static struct si_texture *si_texture_create_object(struct pipe_screen *screen,
                                                    const struct pipe_resource *base,
                                                    const struct radeon_surf *surface,
                                                    const struct si_texture *plane0,
-                                                   struct pb_buffer *imported_buf,
+                                                   struct pb_buffer_lean *imported_buf,
                                                    uint64_t offset, unsigned pitch_in_bytes,
                                                    uint64_t alloc_size, unsigned alignment)
 {
@@ -1007,8 +1007,8 @@ static struct si_texture *si_texture_create_object(struct pipe_screen *screen,
    } else {
       resource->buf = imported_buf;
       resource->gpu_address = sscreen->ws->buffer_get_virtual_address(resource->buf);
-      resource->bo_size = imported_buf->base.size;
-      resource->bo_alignment_log2 = imported_buf->base.alignment_log2;
+      resource->bo_size = imported_buf->size;
+      resource->bo_alignment_log2 = imported_buf->alignment_log2;
       resource->domains = sscreen->ws->buffer_get_initial_domain(resource->buf);
       if (sscreen->ws->buffer_get_flags)
          resource->flags = sscreen->ws->buffer_get_flags(resource->buf);
@@ -1018,7 +1018,7 @@ static struct si_texture *si_texture_create_object(struct pipe_screen *screen,
       fprintf(stderr,
               "VM start=0x%" PRIX64 "  end=0x%" PRIX64
               " | Texture %ix%ix%i, %i levels, %i samples, %s | Flags: ",
-              tex->buffer.gpu_address, tex->buffer.gpu_address + tex->buffer.buf->base.size,
+              tex->buffer.gpu_address, tex->buffer.gpu_address + tex->buffer.buf->size,
               base->width0, base->height0, util_num_layers(base, 0), base->last_level + 1,
               base->nr_samples ? base->nr_samples : 1, util_format_short_name(base->format));
       si_res_print_flags(tex->buffer.flags);
@@ -1580,7 +1580,7 @@ static bool si_texture_is_aux_plane(const struct pipe_resource *resource)
 
 static struct pipe_resource *si_texture_from_winsys_buffer(struct si_screen *sscreen,
                                                            const struct pipe_resource *templ,
-                                                           struct pb_buffer *buf, unsigned stride,
+                                                           struct pb_buffer_lean *buf, unsigned stride,
                                                            uint64_t offset, uint64_t modifier,
                                                            unsigned usage, bool dedicated)
 {
@@ -1681,7 +1681,7 @@ static struct pipe_resource *si_texture_from_winsys_buffer(struct si_screen *ssc
    }
 
    if (ac_surface_get_plane_offset(sscreen->info.gfx_level, &tex->surface, 0, 0) +
-        tex->surface.total_size > buf->base.size) {
+        tex->surface.total_size > buf->size) {
       si_texture_reference(&tex, NULL);
       return NULL;
    }
@@ -1705,7 +1705,7 @@ static struct pipe_resource *si_texture_from_handle(struct pipe_screen *screen,
                                                     struct winsys_handle *whandle, unsigned usage)
 {
    struct si_screen *sscreen = (struct si_screen *)screen;
-   struct pb_buffer *buf = NULL;
+   struct pb_buffer_lean *buf = NULL;
 
    buf = sscreen->ws->buffer_from_handle(sscreen->ws, whandle,
                                          sscreen->info.max_alignment,
@@ -2025,7 +2025,7 @@ static void si_texture_transfer_unmap(struct pipe_context *ctx, struct pipe_tran
       si_copy_from_staging_texture(ctx, stransfer);
 
    if (stransfer->staging) {
-      sctx->num_alloc_tex_transfer_bytes += stransfer->staging->buf->base.size;
+      sctx->num_alloc_tex_transfer_bytes += stransfer->staging->buf->size;
       si_resource_reference(&stransfer->staging, NULL);
    }
 
@@ -2262,7 +2262,7 @@ si_memobj_from_handle(struct pipe_screen *screen, struct winsys_handle *whandle,
 {
    struct si_screen *sscreen = (struct si_screen *)screen;
    struct si_memory_object *memobj = CALLOC_STRUCT(si_memory_object);
-   struct pb_buffer *buf = NULL;
+   struct pb_buffer_lean *buf = NULL;
 
    if (!memobj)
       return NULL;
@@ -2312,7 +2312,7 @@ static struct pipe_resource *si_resource_from_memobj(struct pipe_screen *screen,
    /* si_texture_from_winsys_buffer doesn't increment refcount of
     * memobj->buf, so increment it here.
     */
-   struct pb_buffer *buf = NULL;
+   struct pb_buffer_lean *buf = NULL;
    radeon_bo_reference(sscreen->ws, &buf, memobj->buf);
    return res;
 }
diff --git a/src/gallium/drivers/radeonsi/si_uvd.c b/src/gallium/drivers/radeonsi/si_uvd.c
index 1013f22ab8374..d7f944c706921 100644
--- a/src/gallium/drivers/radeonsi/si_uvd.c
+++ b/src/gallium/drivers/radeonsi/si_uvd.c
@@ -69,7 +69,7 @@ struct pipe_video_buffer *si_video_buffer_create_with_modifiers(struct pipe_cont
 }
 
 /* set the decoding target buffer offsets */
-static struct pb_buffer *si_uvd_set_dtb(struct ruvd_msg *msg, struct vl_video_buffer *buf)
+static struct pb_buffer_lean *si_uvd_set_dtb(struct ruvd_msg *msg, struct vl_video_buffer *buf)
 {
    struct si_screen *sscreen = (struct si_screen *)buf->base.context->screen;
    struct si_texture *luma = (struct si_texture *)buf->resources[0];
@@ -85,7 +85,7 @@ static struct pb_buffer *si_uvd_set_dtb(struct ruvd_msg *msg, struct vl_video_bu
 }
 
 /* get the radeon resources for VCE */
-static void si_vce_get_buffer(struct pipe_resource *resource, struct pb_buffer **handle,
+static void si_vce_get_buffer(struct pipe_resource *resource, struct pb_buffer_lean **handle,
                               struct radeon_surf **surface)
 {
    struct si_texture *res = (struct si_texture *)resource;
diff --git a/src/gallium/include/winsys/radeon_winsys.h b/src/gallium/include/winsys/radeon_winsys.h
index 4a9ae07a7e840..7c61be2c3c762 100644
--- a/src/gallium/include/winsys/radeon_winsys.h
+++ b/src/gallium/include/winsys/radeon_winsys.h
@@ -334,13 +334,14 @@ struct radeon_winsys {
     * \param domain    A bitmask of the RADEON_DOMAIN_* flags.
     * \return          The created buffer object.
     */
-   struct pb_buffer *(*buffer_create)(struct radeon_winsys *ws, uint64_t size, unsigned alignment,
-                                      enum radeon_bo_domain domain, enum radeon_bo_flag flags);
+   struct pb_buffer_lean *(*buffer_create)(struct radeon_winsys *ws, uint64_t size,
+                                           unsigned alignment, enum radeon_bo_domain domain,
+                                           enum radeon_bo_flag flags);
 
    /**
     * Don't use directly. Use radeon_bo_reference.
     */
-   void (*buffer_destroy)(struct radeon_winsys *ws, struct pb_buffer *buf);
+   void (*buffer_destroy)(struct radeon_winsys *ws, struct pb_buffer_lean *buf);
 
    /**
     * Map the entire data store of a buffer object into the client's address
@@ -354,7 +355,7 @@ struct radeon_winsys {
     * \param usage     A bitmask of the PIPE_MAP_* and RADEON_MAP_* flags.
     * \return          The pointer at the beginning of the buffer.
     */
-   void *(*buffer_map)(struct radeon_winsys *ws, struct pb_buffer *buf,
+   void *(*buffer_map)(struct radeon_winsys *ws, struct pb_buffer_lean *buf,
                        struct radeon_cmdbuf *cs, enum pipe_map_flags usage);
 
    /**
@@ -362,7 +363,7 @@ struct radeon_winsys {
     *
     * \param buf       A winsys buffer object to unmap.
     */
-   void (*buffer_unmap)(struct radeon_winsys *ws, struct pb_buffer *buf);
+   void (*buffer_unmap)(struct radeon_winsys *ws, struct pb_buffer_lean *buf);
 
    /**
     * Wait for the buffer and return true if the buffer is not used
@@ -372,7 +373,7 @@ struct radeon_winsys {
     * The timeout of OS_TIMEOUT_INFINITE will always wait until the buffer
     * is idle.
     */
-   bool (*buffer_wait)(struct radeon_winsys *ws, struct pb_buffer *buf,
+   bool (*buffer_wait)(struct radeon_winsys *ws, struct pb_buffer_lean *buf,
                        uint64_t timeout, unsigned usage);
 
    /**
@@ -382,7 +383,7 @@ struct radeon_winsys {
     * \param buf       A winsys buffer object to get the flags from.
     * \param md        Metadata
     */
-   void (*buffer_get_metadata)(struct radeon_winsys *ws, struct pb_buffer *buf,
+   void (*buffer_get_metadata)(struct radeon_winsys *ws, struct pb_buffer_lean *buf,
                                struct radeon_bo_metadata *md, struct radeon_surf *surf);
 
    /**
@@ -392,7 +393,7 @@ struct radeon_winsys {
     * \param buf       A winsys buffer object to set the flags for.
     * \param md        Metadata
     */
-   void (*buffer_set_metadata)(struct radeon_winsys *ws, struct pb_buffer *buf,
+   void (*buffer_set_metadata)(struct radeon_winsys *ws, struct pb_buffer_lean *buf,
                                struct radeon_bo_metadata *md, struct radeon_surf *surf);
 
    /**
@@ -403,8 +404,10 @@ struct radeon_winsys {
     * \param whandle   A winsys handle pointer as was received from a state
     *                  tracker.
     */
-   struct pb_buffer *(*buffer_from_handle)(struct radeon_winsys *ws, struct winsys_handle *whandle,
-                                           unsigned vm_alignment, bool is_prime_linear_buffer);
+   struct pb_buffer_lean *(*buffer_from_handle)(struct radeon_winsys *ws,
+                                                struct winsys_handle *whandle,
+                                                unsigned vm_alignment,
+                                                bool is_prime_linear_buffer);
 
    /**
     * Get a winsys buffer from a user pointer. The resulting buffer can't
@@ -414,7 +417,8 @@ struct radeon_winsys {
     * \param pointer   User pointer to turn into a buffer object.
     * \param Size      Size in bytes for the new buffer.
     */
-   struct pb_buffer *(*buffer_from_ptr)(struct radeon_winsys *ws, void *pointer, uint64_t size, enum radeon_bo_flag flags);
+   struct pb_buffer_lean *(*buffer_from_ptr)(struct radeon_winsys *ws, void *pointer,
+                                             uint64_t size, enum radeon_bo_flag flags);
 
    /**
     * Whether the buffer was created from a user pointer.
@@ -422,10 +426,10 @@ struct radeon_winsys {
     * \param buf       A winsys buffer object
     * \return          whether \p buf was created via buffer_from_ptr
     */
-   bool (*buffer_is_user_ptr)(struct pb_buffer *buf);
+   bool (*buffer_is_user_ptr)(struct pb_buffer_lean *buf);
 
    /** Whether the buffer was suballocated. */
-   bool (*buffer_is_suballocated)(struct pb_buffer *buf);
+   bool (*buffer_is_suballocated)(struct pb_buffer_lean *buf);
 
    /**
     * Get a winsys handle from a winsys buffer. The internal structure
@@ -436,7 +440,7 @@ struct radeon_winsys {
     * \param whandle   A winsys handle pointer.
     * \return          true on success.
     */
-   bool (*buffer_get_handle)(struct radeon_winsys *ws, struct pb_buffer *buf,
+   bool (*buffer_get_handle)(struct radeon_winsys *ws, struct pb_buffer_lean *buf,
                              struct winsys_handle *whandle);
 
    /**
@@ -449,7 +453,7 @@ struct radeon_winsys {
     *
     * \return false on out of memory or other failure, true on success.
     */
-   bool (*buffer_commit)(struct radeon_winsys *ws, struct pb_buffer *buf,
+   bool (*buffer_commit)(struct radeon_winsys *ws, struct pb_buffer_lean *buf,
                          uint64_t offset, uint64_t size, bool commit);
 
    /**
@@ -457,7 +461,7 @@ struct radeon_winsys {
     * \note Only implemented by the amdgpu winsys.
     * \return the skipped count if the range_offset fall into a hole.
     */
-   unsigned (*buffer_find_next_committed_memory)(struct pb_buffer *buf,
+   unsigned (*buffer_find_next_committed_memory)(struct pb_buffer_lean *buf,
                         uint64_t range_offset, unsigned *range_size);
    /**
     * Return the virtual address of a buffer.
@@ -468,7 +472,7 @@ struct radeon_winsys {
     * \param buf       A winsys buffer object
     * \return          virtual address
     */
-   uint64_t (*buffer_get_virtual_address)(struct pb_buffer *buf);
+   uint64_t (*buffer_get_virtual_address)(struct pb_buffer_lean *buf);
 
    /**
     * Return the offset of this buffer relative to the relocation base.
@@ -480,12 +484,12 @@ struct radeon_winsys {
     * \param buf      A winsys buffer object
     * \return         the offset for relocations
     */
-   unsigned (*buffer_get_reloc_offset)(struct pb_buffer *buf);
+   unsigned (*buffer_get_reloc_offset)(struct pb_buffer_lean *buf);
 
    /**
     * Query the initial placement of the buffer from the kernel driver.
     */
-   enum radeon_bo_domain (*buffer_get_initial_domain)(struct pb_buffer *buf);
+   enum radeon_bo_domain (*buffer_get_initial_domain)(struct pb_buffer_lean *buf);
 
    /**
     * Query the flags used for creation of this buffer.
@@ -493,7 +497,7 @@ struct radeon_winsys {
     * Note that for imported buffer this may be lossy since not all flags
     * are passed 1:1.
     */
-   enum radeon_bo_flag (*buffer_get_flags)(struct pb_buffer *buf);
+   enum radeon_bo_flag (*buffer_get_flags)(struct pb_buffer_lean *buf);
 
    /**************************************************************************
     * Command submission.
@@ -576,7 +580,7 @@ struct radeon_winsys {
     * \param domain  Bitmask of the RADEON_DOMAIN_* flags.
     * \return Buffer index.
     */
-   unsigned (*cs_add_buffer)(struct radeon_cmdbuf *cs, struct pb_buffer *buf,
+   unsigned (*cs_add_buffer)(struct radeon_cmdbuf *cs, struct pb_buffer_lean *buf,
                              unsigned usage, enum radeon_bo_domain domain);
 
    /**
@@ -589,7 +593,7 @@ struct radeon_winsys {
     * \param buf       Buffer
     * \return          The buffer index, or -1 if the buffer has not been added.
     */
-   int (*cs_lookup_buffer)(struct radeon_cmdbuf *cs, struct pb_buffer *buf);
+   int (*cs_lookup_buffer)(struct radeon_cmdbuf *cs, struct pb_buffer_lean *buf);
 
    /**
     * Return true if there is enough memory in VRAM and GTT for the buffers
@@ -650,7 +654,7 @@ struct radeon_winsys {
     * \param cs        A command stream.
     * \param buf       A winsys buffer.
     */
-   bool (*cs_is_buffer_referenced)(struct radeon_cmdbuf *cs, struct pb_buffer *buf,
+   bool (*cs_is_buffer_referenced)(struct radeon_cmdbuf *cs, struct pb_buffer_lean *buf,
                                    unsigned usage);
 
    /**
@@ -781,11 +785,12 @@ static inline bool radeon_uses_secure_bos(struct radeon_winsys* ws)
 }
 
 static inline void
-radeon_bo_reference(struct radeon_winsys *rws, struct pb_buffer **dst, struct pb_buffer *src)
+radeon_bo_reference(struct radeon_winsys *rws, struct pb_buffer_lean **dst,
+                    struct pb_buffer_lean *src)
 {
-   struct pb_buffer *old = *dst;
+   struct pb_buffer_lean *old = *dst;
 
-   if (pipe_reference(&(*dst)->base.reference, &src->base.reference))
+   if (pipe_reference(&(*dst)->reference, &src->reference))
       rws->buffer_destroy(rws, old);
    *dst = src;
 }
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index acc7c20635c18..2e5e13cb177f6 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -32,7 +32,7 @@ struct amdgpu_sparse_backing_chunk {
 };
 
 static bool amdgpu_bo_wait(struct radeon_winsys *rws,
-                           struct pb_buffer *_buf, uint64_t timeout,
+                           struct pb_buffer_lean *_buf, uint64_t timeout,
                            unsigned usage)
 {
    struct amdgpu_winsys *ws = amdgpu_winsys(rws);
@@ -135,15 +135,15 @@ static inline unsigned get_slab_entry_offset(struct amdgpu_winsys_bo *bo)
 }
 
 static enum radeon_bo_domain amdgpu_bo_get_initial_domain(
-      struct pb_buffer *buf)
+      struct pb_buffer_lean *buf)
 {
-   return ((struct amdgpu_winsys_bo*)buf)->base.base.placement;
+   return ((struct amdgpu_winsys_bo*)buf)->base.placement;
 }
 
 static enum radeon_bo_flag amdgpu_bo_get_flags(
-      struct pb_buffer *buf)
+      struct pb_buffer_lean *buf)
 {
-   return ((struct amdgpu_winsys_bo*)buf)->base.base.usage;
+   return ((struct amdgpu_winsys_bo*)buf)->base.usage;
 }
 
 static void amdgpu_bo_remove_fences(struct amdgpu_winsys_bo *bo)
@@ -156,7 +156,7 @@ static void amdgpu_bo_remove_fences(struct amdgpu_winsys_bo *bo)
    bo->max_fences = 0;
 }
 
-void amdgpu_bo_destroy(struct amdgpu_winsys *ws, struct pb_buffer *_buf)
+void amdgpu_bo_destroy(struct amdgpu_winsys *ws, struct pb_buffer_lean *_buf)
 {
    struct amdgpu_bo_real *bo = get_real_bo(amdgpu_winsys_bo(_buf));
    struct amdgpu_screen_winsys *sws_iter;
@@ -164,15 +164,15 @@ void amdgpu_bo_destroy(struct amdgpu_winsys *ws, struct pb_buffer *_buf)
    simple_mtx_lock(&ws->bo_export_table_lock);
 
    /* amdgpu_bo_from_handle might have revived the bo */
-   if (p_atomic_read(&bo->b.base.base.reference.count)) {
+   if (p_atomic_read(&bo->b.base.reference.count)) {
       simple_mtx_unlock(&ws->bo_export_table_lock);
       return;
    }
 
    _mesa_hash_table_remove_key(ws->bo_export_table, bo->bo);
 
-   if (bo->b.base.base.placement & RADEON_DOMAIN_VRAM_GTT) {
-      amdgpu_bo_va_op(bo->bo, 0, bo->b.base.base.size,
+   if (bo->b.base.placement & RADEON_DOMAIN_VRAM_GTT) {
+      amdgpu_bo_va_op(bo->bo, 0, bo->b.base.size,
                       amdgpu_va_get_start_addr(bo->va_handle), 0, AMDGPU_VA_OP_UNMAP);
       amdgpu_va_range_free(bo->va_handle);
    }
@@ -216,16 +216,16 @@ void amdgpu_bo_destroy(struct amdgpu_winsys *ws, struct pb_buffer *_buf)
 
    amdgpu_bo_remove_fences(&bo->b);
 
-   if (bo->b.base.base.placement & RADEON_DOMAIN_VRAM)
-      ws->allocated_vram -= align64(bo->b.base.base.size, ws->info.gart_page_size);
-   else if (bo->b.base.base.placement & RADEON_DOMAIN_GTT)
-      ws->allocated_gtt -= align64(bo->b.base.base.size, ws->info.gart_page_size);
+   if (bo->b.base.placement & RADEON_DOMAIN_VRAM)
+      ws->allocated_vram -= align64(bo->b.base.size, ws->info.gart_page_size);
+   else if (bo->b.base.placement & RADEON_DOMAIN_GTT)
+      ws->allocated_gtt -= align64(bo->b.base.size, ws->info.gart_page_size);
 
    simple_mtx_destroy(&bo->lock);
    FREE(bo);
 }
 
-static void amdgpu_bo_destroy_or_cache(struct radeon_winsys *rws, struct pb_buffer *_buf)
+static void amdgpu_bo_destroy_or_cache(struct radeon_winsys *rws, struct pb_buffer_lean *_buf)
 {
    struct amdgpu_winsys *ws = amdgpu_winsys(rws);
    struct amdgpu_winsys_bo *bo = amdgpu_winsys_bo(_buf);
@@ -260,10 +260,10 @@ static bool amdgpu_bo_do_map(struct radeon_winsys *rws, struct amdgpu_bo_real *b
    }
 
    if (p_atomic_inc_return(&bo->map_count) == 1) {
-      if (bo->b.base.base.placement & RADEON_DOMAIN_VRAM)
-         ws->mapped_vram += bo->b.base.base.size;
-      else if (bo->b.base.base.placement & RADEON_DOMAIN_GTT)
-         ws->mapped_gtt += bo->b.base.base.size;
+      if (bo->b.base.placement & RADEON_DOMAIN_VRAM)
+         ws->mapped_vram += bo->b.base.size;
+      else if (bo->b.base.placement & RADEON_DOMAIN_GTT)
+         ws->mapped_gtt += bo->b.base.size;
       ws->num_mapped_buffers++;
    }
 
@@ -271,7 +271,7 @@ static bool amdgpu_bo_do_map(struct radeon_winsys *rws, struct amdgpu_bo_real *b
 }
 
 void *amdgpu_bo_map(struct radeon_winsys *rws,
-                    struct pb_buffer *buf,
+                    struct pb_buffer_lean *buf,
                     struct radeon_cmdbuf *rcs,
                     enum pipe_map_flags usage)
 {
@@ -301,7 +301,7 @@ void *amdgpu_bo_map(struct radeon_winsys *rws,
                return NULL;
             }
 
-            if (!amdgpu_bo_wait(rws, (struct pb_buffer*)bo, 0,
+            if (!amdgpu_bo_wait(rws, (struct pb_buffer_lean*)bo, 0,
                                 RADEON_USAGE_WRITE)) {
                return NULL;
             }
@@ -312,7 +312,7 @@ void *amdgpu_bo_map(struct radeon_winsys *rws,
                return NULL;
             }
 
-            if (!amdgpu_bo_wait(rws, (struct pb_buffer*)bo, 0,
+            if (!amdgpu_bo_wait(rws, (struct pb_buffer_lean*)bo, 0,
                                 RADEON_USAGE_READWRITE)) {
                return NULL;
             }
@@ -340,7 +340,7 @@ void *amdgpu_bo_map(struct radeon_winsys *rws,
                }
             }
 
-            amdgpu_bo_wait(rws, (struct pb_buffer*)bo, OS_TIMEOUT_INFINITE,
+            amdgpu_bo_wait(rws, (struct pb_buffer_lean*)bo, OS_TIMEOUT_INFINITE,
                            RADEON_USAGE_WRITE);
          } else {
             /* Mapping for write. */
@@ -355,7 +355,7 @@ void *amdgpu_bo_map(struct radeon_winsys *rws,
                }
             }
 
-            amdgpu_bo_wait(rws, (struct pb_buffer*)bo, OS_TIMEOUT_INFINITE,
+            amdgpu_bo_wait(rws, (struct pb_buffer_lean*)bo, OS_TIMEOUT_INFINITE,
                            RADEON_USAGE_READWRITE);
          }
 
@@ -402,7 +402,7 @@ void *amdgpu_bo_map(struct radeon_winsys *rws,
    return (uint8_t*)cpu + offset;
 }
 
-void amdgpu_bo_unmap(struct radeon_winsys *rws, struct pb_buffer *buf)
+void amdgpu_bo_unmap(struct radeon_winsys *rws, struct pb_buffer_lean *buf)
 {
    struct amdgpu_winsys *ws = amdgpu_winsys(rws);
    struct amdgpu_winsys_bo *bo = (struct amdgpu_winsys_bo*)buf;
@@ -420,10 +420,10 @@ void amdgpu_bo_unmap(struct radeon_winsys *rws, struct pb_buffer *buf)
       assert(!real->cpu_ptr &&
              "too many unmaps or forgot RADEON_MAP_TEMPORARY flag");
 
-      if (real->b.base.base.placement & RADEON_DOMAIN_VRAM)
-         ws->mapped_vram -= real->b.base.base.size;
-      else if (real->b.base.base.placement & RADEON_DOMAIN_GTT)
-         ws->mapped_gtt -= real->b.base.base.size;
+      if (real->b.base.placement & RADEON_DOMAIN_VRAM)
+         ws->mapped_vram -= real->b.base.size;
+      else if (real->b.base.placement & RADEON_DOMAIN_GTT)
+         ws->mapped_gtt -= real->b.base.size;
       ws->num_mapped_buffers--;
    }
 
@@ -492,7 +492,7 @@ static struct amdgpu_winsys_bo *amdgpu_create_bo(struct amdgpu_winsys *ws,
          return NULL;
 
       bo = &new_bo->b;
-      pb_cache_init_entry(&ws->bo_cache, &new_bo->cache_entry, &bo->b.base.base, heap);
+      pb_cache_init_entry(&ws->bo_cache, &new_bo->cache_entry, &bo->b.base, heap);
       bo->b.type = slab_backing ? AMDGPU_BO_REAL_REUSABLE_SLAB : AMDGPU_BO_REAL_REUSABLE;
    } else {
       bo = CALLOC_STRUCT(amdgpu_bo_real);
@@ -588,11 +588,11 @@ static struct amdgpu_winsys_bo *amdgpu_create_bo(struct amdgpu_winsys *ws,
    }
 
    simple_mtx_init(&bo->lock, mtx_plain);
-   pipe_reference_init(&bo->b.base.base.reference, 1);
-   bo->b.base.base.placement = initial_domain;
-   bo->b.base.base.alignment_log2 = util_logbase2(alignment);
-   bo->b.base.base.usage = flags;
-   bo->b.base.base.size = size;
+   pipe_reference_init(&bo->b.base.reference, 1);
+   bo->b.base.placement = initial_domain;
+   bo->b.base.alignment_log2 = util_logbase2(alignment);
+   bo->b.base.usage = flags;
+   bo->b.base.size = size;
    bo->b.unique_id = __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
    bo->bo = buf_handle;
    bo->va_handle = va_handle;
@@ -618,7 +618,7 @@ error_bo_alloc:
    return NULL;
 }
 
-bool amdgpu_bo_can_reclaim(struct amdgpu_winsys *ws, struct pb_buffer *_buf)
+bool amdgpu_bo_can_reclaim(struct amdgpu_winsys *ws, struct pb_buffer_lean *_buf)
 {
    return amdgpu_bo_wait(&ws->dummy_ws.base, _buf, 0, RADEON_USAGE_READWRITE);
 }
@@ -632,19 +632,19 @@ bool amdgpu_bo_can_reclaim_slab(void *priv, struct pb_slab_entry *entry)
 
 static unsigned get_slab_wasted_size(struct amdgpu_winsys *ws, struct amdgpu_bo_slab_entry *bo)
 {
-   assert(bo->b.base.base.size <= bo->entry.slab->entry_size);
-   assert(bo->b.base.base.size < (1 << bo->b.base.base.alignment_log2) ||
-          bo->b.base.base.size < 1 << ws->bo_slabs.min_order ||
-          bo->b.base.base.size > bo->entry.slab->entry_size / 2);
-   return bo->entry.slab->entry_size - bo->b.base.base.size;
+   assert(bo->b.base.size <= bo->entry.slab->entry_size);
+   assert(bo->b.base.size < (1 << bo->b.base.alignment_log2) ||
+          bo->b.base.size < 1 << ws->bo_slabs.min_order ||
+          bo->b.base.size > bo->entry.slab->entry_size / 2);
+   return bo->entry.slab->entry_size - bo->b.base.size;
 }
 
-static void amdgpu_bo_slab_destroy(struct radeon_winsys *rws, struct pb_buffer *_buf)
+static void amdgpu_bo_slab_destroy(struct radeon_winsys *rws, struct pb_buffer_lean *_buf)
 {
    struct amdgpu_winsys *ws = amdgpu_winsys(rws);
    struct amdgpu_bo_slab_entry *bo = get_slab_entry_bo(amdgpu_winsys_bo(_buf));
 
-   if (bo->b.base.base.placement & RADEON_DOMAIN_VRAM)
+   if (bo->b.base.placement & RADEON_DOMAIN_VRAM)
       ws->slab_wasted_vram -= get_slab_wasted_size(ws, bo);
    else
       ws->slab_wasted_gtt -= get_slab_wasted_size(ws, bo);
@@ -723,7 +723,7 @@ struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_s
    assert(slab_bo->b.b.b.type == AMDGPU_BO_REAL_REUSABLE_SLAB);
 
    /* We can get a buffer from pb_cache that is slightly larger. */
-   slab_size = slab_bo->b.b.b.base.base.size;
+   slab_size = slab_bo->b.b.b.base.size;
 
    slab_bo->slab.num_entries = slab_size / entry_size;
    slab_bo->slab.num_free = slab_bo->slab.num_entries;
@@ -740,9 +740,9 @@ struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_s
    for (unsigned i = 0; i < slab_bo->slab.num_entries; ++i) {
       struct amdgpu_bo_slab_entry *bo = &slab_bo->entries[i];
 
-      bo->b.base.base.placement = domains;
-      bo->b.base.base.alignment_log2 = util_logbase2(get_slab_entry_alignment(ws, entry_size));
-      bo->b.base.base.size = entry_size;
+      bo->b.base.placement = domains;
+      bo->b.base.alignment_log2 = util_logbase2(get_slab_entry_alignment(ws, entry_size));
+      bo->b.base.size = entry_size;
       bo->b.type = AMDGPU_BO_SLAB_ENTRY;
       bo->b.unique_id = base_id + i;
 
@@ -767,10 +767,10 @@ fail:
 void amdgpu_bo_slab_free(struct amdgpu_winsys *ws, struct pb_slab *slab)
 {
    struct amdgpu_bo_real_reusable_slab *bo = get_bo_from_slab(slab);
-   unsigned slab_size = bo->b.b.b.base.base.size;
+   unsigned slab_size = bo->b.b.b.base.size;
 
    assert(bo->slab.num_entries * bo->slab.entry_size <= slab_size);
-   if (bo->b.b.b.base.base.placement & RADEON_DOMAIN_VRAM)
+   if (bo->b.b.b.base.placement & RADEON_DOMAIN_VRAM)
       ws->slab_wasted_vram -= slab_size - bo->slab.num_entries * bo->slab.entry_size;
    else
       ws->slab_wasted_gtt -= slab_size - bo->slab.num_entries * bo->slab.entry_size;
@@ -869,7 +869,7 @@ sparse_backing_alloc(struct amdgpu_winsys *ws, struct amdgpu_bo_sparse *bo,
 
    /* Allocate a new backing buffer if necessary. */
    if (!best_backing) {
-      struct pb_buffer *buf;
+      struct pb_buffer_lean *buf;
       uint64_t size;
       uint32_t pages;
 
@@ -885,16 +885,16 @@ sparse_backing_alloc(struct amdgpu_winsys *ws, struct amdgpu_bo_sparse *bo,
          return NULL;
       }
 
-      assert(bo->num_backing_pages < DIV_ROUND_UP(bo->b.base.base.size, RADEON_SPARSE_PAGE_SIZE));
+      assert(bo->num_backing_pages < DIV_ROUND_UP(bo->b.base.size, RADEON_SPARSE_PAGE_SIZE));
 
-      size = MIN3(bo->b.base.base.size / 16,
+      size = MIN3(bo->b.base.size / 16,
                   8 * 1024 * 1024,
-                  bo->b.base.base.size - (uint64_t)bo->num_backing_pages * RADEON_SPARSE_PAGE_SIZE);
+                  bo->b.base.size - (uint64_t)bo->num_backing_pages * RADEON_SPARSE_PAGE_SIZE);
       size = MAX2(size, RADEON_SPARSE_PAGE_SIZE);
 
       buf = amdgpu_bo_create(ws, size, RADEON_SPARSE_PAGE_SIZE,
-                             bo->b.base.base.placement,
-                             (bo->b.base.base.usage & ~RADEON_FLAG_SPARSE &
+                             bo->b.base.placement,
+                             (bo->b.base.usage & ~RADEON_FLAG_SPARSE &
                               /* Set the interprocess sharing flag to disable pb_cache because
                                * amdgpu_bo_wait doesn't wait for active CS jobs.
                                */
@@ -906,7 +906,7 @@ sparse_backing_alloc(struct amdgpu_winsys *ws, struct amdgpu_bo_sparse *bo,
       }
 
       /* We might have gotten a bigger buffer than requested via caching. */
-      pages = buf->base.size / RADEON_SPARSE_PAGE_SIZE;
+      pages = buf->size / RADEON_SPARSE_PAGE_SIZE;
 
       best_backing->bo = get_real_bo(amdgpu_winsys_bo(buf));
       best_backing->num_chunks = 1;
@@ -937,7 +937,7 @@ static void
 sparse_free_backing_buffer(struct amdgpu_winsys *ws, struct amdgpu_bo_sparse *bo,
                            struct amdgpu_sparse_backing *backing)
 {
-   bo->num_backing_pages -= backing->bo->b.base.base.size / RADEON_SPARSE_PAGE_SIZE;
+   bo->num_backing_pages -= backing->bo->b.base.size / RADEON_SPARSE_PAGE_SIZE;
 
    simple_mtx_lock(&ws->bo_fence_lock);
    amdgpu_add_fences(&backing->bo->b, bo->b.num_fences, bo->b.fences);
@@ -1008,13 +1008,13 @@ sparse_backing_free(struct amdgpu_winsys *ws, struct amdgpu_bo_sparse *bo,
    }
 
    if (backing->num_chunks == 1 && backing->chunks[0].begin == 0 &&
-       backing->chunks[0].end == backing->bo->b.base.base.size / RADEON_SPARSE_PAGE_SIZE)
+       backing->chunks[0].end == backing->bo->b.base.size / RADEON_SPARSE_PAGE_SIZE)
       sparse_free_backing_buffer(ws, bo, backing);
 
    return true;
 }
 
-static void amdgpu_bo_sparse_destroy(struct radeon_winsys *rws, struct pb_buffer *_buf)
+static void amdgpu_bo_sparse_destroy(struct radeon_winsys *rws, struct pb_buffer_lean *_buf)
 {
    struct amdgpu_winsys *ws = amdgpu_winsys(rws);
    struct amdgpu_bo_sparse *bo = get_sparse_bo(amdgpu_winsys_bo(_buf));
@@ -1039,7 +1039,7 @@ static void amdgpu_bo_sparse_destroy(struct radeon_winsys *rws, struct pb_buffer
    FREE(bo);
 }
 
-static struct pb_buffer *
+static struct pb_buffer_lean *
 amdgpu_bo_sparse_create(struct amdgpu_winsys *ws, uint64_t size,
                         enum radeon_bo_domain domain,
                         enum radeon_bo_flag flags)
@@ -1061,11 +1061,11 @@ amdgpu_bo_sparse_create(struct amdgpu_winsys *ws, uint64_t size,
       return NULL;
 
    simple_mtx_init(&bo->lock, mtx_plain);
-   pipe_reference_init(&bo->b.base.base.reference, 1);
-   bo->b.base.base.placement = domain;
-   bo->b.base.base.alignment_log2 = util_logbase2(RADEON_SPARSE_PAGE_SIZE);
-   bo->b.base.base.usage = flags;
-   bo->b.base.base.size = size;
+   pipe_reference_init(&bo->b.base.reference, 1);
+   bo->b.base.placement = domain;
+   bo->b.base.alignment_log2 = util_logbase2(RADEON_SPARSE_PAGE_SIZE);
+   bo->b.base.usage = flags;
+   bo->b.base.size = size;
    bo->b.unique_id =  __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
    bo->b.type = AMDGPU_BO_SPARSE;
 
@@ -1105,7 +1105,7 @@ error_alloc_commitments:
 }
 
 static bool
-amdgpu_bo_sparse_commit(struct radeon_winsys *rws, struct pb_buffer *buf,
+amdgpu_bo_sparse_commit(struct radeon_winsys *rws, struct pb_buffer_lean *buf,
                         uint64_t offset, uint64_t size, bool commit)
 {
    struct amdgpu_winsys *ws = amdgpu_winsys(rws);
@@ -1116,9 +1116,9 @@ amdgpu_bo_sparse_commit(struct radeon_winsys *rws, struct pb_buffer *buf,
    int r;
 
    assert(offset % RADEON_SPARSE_PAGE_SIZE == 0);
-   assert(offset <= bo->b.base.base.size);
-   assert(size <= bo->b.base.base.size - offset);
-   assert(size % RADEON_SPARSE_PAGE_SIZE == 0 || offset + size == bo->b.base.base.size);
+   assert(offset <= bo->b.base.size);
+   assert(size <= bo->b.base.size - offset);
+   assert(size % RADEON_SPARSE_PAGE_SIZE == 0 || offset + size == bo->b.base.size);
 
    comm = bo->commitments;
    va_page = offset / RADEON_SPARSE_PAGE_SIZE;
@@ -1236,7 +1236,7 @@ out:
 }
 
 static unsigned
-amdgpu_bo_find_next_committed_memory(struct pb_buffer *buf,
+amdgpu_bo_find_next_committed_memory(struct pb_buffer_lean *buf,
                                      uint64_t range_offset, unsigned *range_size)
 {
    struct amdgpu_bo_sparse *bo = get_sparse_bo(amdgpu_winsys_bo(buf));
@@ -1248,7 +1248,7 @@ amdgpu_bo_find_next_committed_memory(struct pb_buffer *buf,
    if (*range_size == 0)
       return 0;
 
-   assert(*range_size + range_offset <= bo->b.base.base.size);
+   assert(*range_size + range_offset <= bo->b.base.size);
 
    uncommitted_range_prev = uncommitted_range_next = 0;
    comm = bo->commitments;
@@ -1290,7 +1290,7 @@ amdgpu_bo_find_next_committed_memory(struct pb_buffer *buf,
 }
 
 static void amdgpu_buffer_get_metadata(struct radeon_winsys *rws,
-                                       struct pb_buffer *_buf,
+                                       struct pb_buffer_lean *_buf,
                                        struct radeon_bo_metadata *md,
                                        struct radeon_surf *surf)
 {
@@ -1311,7 +1311,7 @@ static void amdgpu_buffer_get_metadata(struct radeon_winsys *rws,
 }
 
 static void amdgpu_buffer_set_metadata(struct radeon_winsys *rws,
-                                       struct pb_buffer *_buf,
+                                       struct pb_buffer_lean *_buf,
                                        struct radeon_bo_metadata *md,
                                        struct radeon_surf *surf)
 {
@@ -1327,7 +1327,7 @@ static void amdgpu_buffer_set_metadata(struct radeon_winsys *rws,
    amdgpu_bo_set_metadata(bo->bo, &metadata);
 }
 
-struct pb_buffer *
+struct pb_buffer_lean *
 amdgpu_bo_create(struct amdgpu_winsys *ws,
                  uint64_t size,
                  unsigned alignment,
@@ -1384,9 +1384,9 @@ amdgpu_bo_create(struct amdgpu_winsys *ws,
          return NULL;
 
       struct amdgpu_bo_slab_entry *slab_bo = container_of(entry, struct amdgpu_bo_slab_entry, entry);
-      pipe_reference_init(&slab_bo->b.base.base.reference, 1);
-      slab_bo->b.base.base.size = size;
-      assert(alignment <= 1 << slab_bo->b.base.base.alignment_log2);
+      pipe_reference_init(&slab_bo->b.base.reference, 1);
+      slab_bo->b.base.size = size;
+      assert(alignment <= 1 << slab_bo->b.base.alignment_log2);
 
       if (domain & RADEON_DOMAIN_VRAM)
          ws->slab_wasted_vram += get_slab_wasted_size(ws, slab_bo);
@@ -1454,19 +1454,19 @@ no_slab:
    return &bo->base;
 }
 
-static struct pb_buffer *
+static struct pb_buffer_lean *
 amdgpu_buffer_create(struct radeon_winsys *ws,
                      uint64_t size,
                      unsigned alignment,
                      enum radeon_bo_domain domain,
                      enum radeon_bo_flag flags)
 {
-   struct pb_buffer * res = amdgpu_bo_create(amdgpu_winsys(ws), size, alignment, domain,
+   struct pb_buffer_lean * res = amdgpu_bo_create(amdgpu_winsys(ws), size, alignment, domain,
                            flags);
    return res;
 }
 
-static struct pb_buffer *amdgpu_bo_from_handle(struct radeon_winsys *rws,
+static struct pb_buffer_lean *amdgpu_bo_from_handle(struct radeon_winsys *rws,
                                                struct winsys_handle *whandle,
                                                unsigned vm_alignment,
                                                bool is_prime_linear_buffer)
@@ -1504,7 +1504,7 @@ static struct pb_buffer *amdgpu_bo_from_handle(struct radeon_winsys *rws,
     * counter and return it.
     */
    if (bo) {
-      p_atomic_inc(&bo->b.base.base.reference.count);
+      p_atomic_inc(&bo->b.base.reference.count);
       simple_mtx_unlock(&ws->bo_export_table_lock);
 
       /* Release the buffer handle, because we don't need it anymore.
@@ -1558,12 +1558,12 @@ static struct pb_buffer *amdgpu_bo_from_handle(struct radeon_winsys *rws,
    }
 
    /* Initialize the structure. */
-   pipe_reference_init(&bo->b.base.base.reference, 1);
-   bo->b.base.base.placement = initial;
-   bo->b.base.base.alignment_log2 = util_logbase2(info.phys_alignment ?
+   pipe_reference_init(&bo->b.base.reference, 1);
+   bo->b.base.placement = initial;
+   bo->b.base.alignment_log2 = util_logbase2(info.phys_alignment ?
 				info.phys_alignment : ws->info.gart_page_size);
-   bo->b.base.base.usage = flags;
-   bo->b.base.base.size = result.alloc_size;
+   bo->b.base.usage = flags;
+   bo->b.base.size = result.alloc_size;
    bo->b.type = AMDGPU_BO_REAL;
    bo->b.unique_id = __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
    simple_mtx_init(&bo->lock, mtx_plain);
@@ -1571,10 +1571,10 @@ static struct pb_buffer *amdgpu_bo_from_handle(struct radeon_winsys *rws,
    bo->va_handle = va_handle;
    bo->is_shared = true;
 
-   if (bo->b.base.base.placement & RADEON_DOMAIN_VRAM)
-      ws->allocated_vram += align64(bo->b.base.base.size, ws->info.gart_page_size);
-   else if (bo->b.base.base.placement & RADEON_DOMAIN_GTT)
-      ws->allocated_gtt += align64(bo->b.base.base.size, ws->info.gart_page_size);
+   if (bo->b.base.placement & RADEON_DOMAIN_VRAM)
+      ws->allocated_vram += align64(bo->b.base.size, ws->info.gart_page_size);
+   else if (bo->b.base.placement & RADEON_DOMAIN_GTT)
+      ws->allocated_gtt += align64(bo->b.base.size, ws->info.gart_page_size);
 
    amdgpu_bo_export(bo->bo, amdgpu_bo_handle_type_kms, &bo->kms_handle);
 
@@ -1596,7 +1596,7 @@ error:
 }
 
 static bool amdgpu_bo_get_handle(struct radeon_winsys *rws,
-                                 struct pb_buffer *buffer,
+                                 struct pb_buffer_lean *buffer,
                                  struct winsys_handle *whandle)
 {
    struct amdgpu_screen_winsys *sws = amdgpu_screen_winsys(rws);
@@ -1681,7 +1681,7 @@ static bool amdgpu_bo_get_handle(struct radeon_winsys *rws,
    return true;
 }
 
-static struct pb_buffer *amdgpu_bo_from_ptr(struct radeon_winsys *rws,
+static struct pb_buffer_lean *amdgpu_bo_from_ptr(struct radeon_winsys *rws,
 					    void *pointer, uint64_t size,
 					    enum radeon_bo_flag flags)
 {
@@ -1713,10 +1713,10 @@ static struct pb_buffer *amdgpu_bo_from_ptr(struct radeon_winsys *rws,
 
     /* Initialize it. */
     bo->is_user_ptr = true;
-    pipe_reference_init(&bo->b.base.base.reference, 1);
-    bo->b.base.base.placement = RADEON_DOMAIN_GTT;
-    bo->b.base.base.alignment_log2 = 0;
-    bo->b.base.base.size = size;
+    pipe_reference_init(&bo->b.base.reference, 1);
+    bo->b.base.placement = RADEON_DOMAIN_GTT;
+    bo->b.base.alignment_log2 = 0;
+    bo->b.base.size = size;
     bo->b.type = AMDGPU_BO_REAL;
     bo->b.unique_id = __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
     simple_mtx_init(&bo->lock, mtx_plain);
@@ -1730,7 +1730,7 @@ static struct pb_buffer *amdgpu_bo_from_ptr(struct radeon_winsys *rws,
 
     amdgpu_bo_export(bo->bo, amdgpu_bo_handle_type_kms, &bo->kms_handle);
 
-    return (struct pb_buffer*)bo;
+    return (struct pb_buffer_lean*)bo;
 
 error_va_map:
     amdgpu_va_range_free(va_handle);
@@ -1743,21 +1743,21 @@ error:
     return NULL;
 }
 
-static bool amdgpu_bo_is_user_ptr(struct pb_buffer *buf)
+static bool amdgpu_bo_is_user_ptr(struct pb_buffer_lean *buf)
 {
    struct amdgpu_winsys_bo *bo = (struct amdgpu_winsys_bo*)buf;
 
    return is_real_bo(bo) ? get_real_bo(bo)->is_user_ptr : false;
 }
 
-static bool amdgpu_bo_is_suballocated(struct pb_buffer *buf)
+static bool amdgpu_bo_is_suballocated(struct pb_buffer_lean *buf)
 {
    struct amdgpu_winsys_bo *bo = (struct amdgpu_winsys_bo*)buf;
 
    return bo->type == AMDGPU_BO_SLAB_ENTRY;
 }
 
-uint64_t amdgpu_bo_get_va(struct pb_buffer *buf)
+uint64_t amdgpu_bo_get_va(struct pb_buffer_lean *buf)
 {
    struct amdgpu_winsys_bo *bo = amdgpu_winsys_bo(buf);
 
@@ -1773,7 +1773,7 @@ uint64_t amdgpu_bo_get_va(struct pb_buffer *buf)
    }
 }
 
-static void amdgpu_buffer_destroy(struct radeon_winsys *ws, struct pb_buffer *buf)
+static void amdgpu_buffer_destroy(struct radeon_winsys *ws, struct pb_buffer_lean *buf)
 {
    struct amdgpu_winsys_bo *bo = amdgpu_winsys_bo(buf);
 
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
index 9a51a54c07674..8f99d68259b67 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
@@ -48,7 +48,7 @@ enum amdgpu_bo_type {
 
 /* Base class of the buffer object that other structures inherit. */
 struct amdgpu_winsys_bo {
-   struct pb_buffer base;
+   struct pb_buffer_lean base;
    enum amdgpu_bo_type type;
 
    uint32_t unique_id;
@@ -151,7 +151,7 @@ static struct amdgpu_bo_real_reusable *get_real_bo_reusable(struct amdgpu_winsys
 
 static struct amdgpu_bo_sparse *get_sparse_bo(struct amdgpu_winsys_bo *bo)
 {
-   assert(bo->type == AMDGPU_BO_SPARSE && bo->base.base.usage & RADEON_FLAG_SPARSE);
+   assert(bo->type == AMDGPU_BO_SPARSE && bo->base.usage & RADEON_FLAG_SPARSE);
    return (struct amdgpu_bo_sparse*)bo;
 }
 
@@ -172,28 +172,28 @@ static struct amdgpu_bo_real *get_slab_entry_real_bo(struct amdgpu_winsys_bo *bo
    return &get_bo_from_slab(((struct amdgpu_bo_slab_entry*)bo)->entry.slab)->b.b;
 }
 
-bool amdgpu_bo_can_reclaim(struct amdgpu_winsys *ws, struct pb_buffer *_buf);
-struct pb_buffer *amdgpu_bo_create(struct amdgpu_winsys *ws,
+bool amdgpu_bo_can_reclaim(struct amdgpu_winsys *ws, struct pb_buffer_lean *_buf);
+struct pb_buffer_lean *amdgpu_bo_create(struct amdgpu_winsys *ws,
                                    uint64_t size,
                                    unsigned alignment,
                                    enum radeon_bo_domain domain,
                                    enum radeon_bo_flag flags);
-void amdgpu_bo_destroy(struct amdgpu_winsys *ws, struct pb_buffer *_buf);
+void amdgpu_bo_destroy(struct amdgpu_winsys *ws, struct pb_buffer_lean *_buf);
 void *amdgpu_bo_map(struct radeon_winsys *rws,
-                    struct pb_buffer *buf,
+                    struct pb_buffer_lean *buf,
                     struct radeon_cmdbuf *rcs,
                     enum pipe_map_flags usage);
-void amdgpu_bo_unmap(struct radeon_winsys *rws, struct pb_buffer *buf);
+void amdgpu_bo_unmap(struct radeon_winsys *rws, struct pb_buffer_lean *buf);
 void amdgpu_bo_init_functions(struct amdgpu_screen_winsys *ws);
 
 bool amdgpu_bo_can_reclaim_slab(void *priv, struct pb_slab_entry *entry);
 struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_size,
                                      unsigned group_index);
 void amdgpu_bo_slab_free(struct amdgpu_winsys *ws, struct pb_slab *slab);
-uint64_t amdgpu_bo_get_va(struct pb_buffer *buf);
+uint64_t amdgpu_bo_get_va(struct pb_buffer_lean *buf);
 
 static inline
-struct amdgpu_winsys_bo *amdgpu_winsys_bo(struct pb_buffer *bo)
+struct amdgpu_winsys_bo *amdgpu_winsys_bo(struct pb_buffer_lean *bo)
 {
    return (struct amdgpu_winsys_bo *)bo;
 }
@@ -204,7 +204,7 @@ void amdgpu_winsys_bo_reference(struct amdgpu_winsys *ws,
                                 struct amdgpu_winsys_bo *src)
 {
    radeon_bo_reference(&ws->dummy_ws.base,
-                       (struct pb_buffer**)dst, (struct pb_buffer*)src);
+                       (struct pb_buffer_lean**)dst, (struct pb_buffer_lean*)src);
 }
 
 #endif
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
index 766cc3df7c501..da8f92316d029 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
@@ -652,7 +652,7 @@ amdgpu_lookup_or_add_slab_buffer(struct amdgpu_cs_context *cs, struct amdgpu_win
 }
 
 static unsigned amdgpu_cs_add_buffer(struct radeon_cmdbuf *rcs,
-                                    struct pb_buffer *buf,
+                                    struct pb_buffer_lean *buf,
                                     unsigned usage,
                                     enum radeon_bo_domain domains)
 {
@@ -699,7 +699,7 @@ static bool amdgpu_ib_new_buffer(struct amdgpu_winsys *ws,
                                  struct amdgpu_ib *main_ib,
                                  struct amdgpu_cs *cs)
 {
-   struct pb_buffer *pb;
+   struct pb_buffer_lean *pb;
    uint8_t *mapped;
    unsigned buffer_size;
 
@@ -795,7 +795,7 @@ static bool amdgpu_get_new_ib(struct amdgpu_winsys *ws,
 
    /* Allocate a new buffer for IBs if the current buffer is all used. */
    if (!main_ib->big_buffer ||
-       main_ib->used_ib_space + ib_size > main_ib->big_buffer->base.size) {
+       main_ib->used_ib_space + ib_size > main_ib->big_buffer->size) {
       if (!amdgpu_ib_new_buffer(ws, main_ib, cs))
          return false;
    }
@@ -814,7 +814,7 @@ static bool amdgpu_get_new_ib(struct amdgpu_winsys *ws,
 
    cs->csc->ib_main_addr = rcs->current.buf;
 
-   ib_size = main_ib->big_buffer->base.size - main_ib->used_ib_space;
+   ib_size = main_ib->big_buffer->size - main_ib->used_ib_space;
    rcs->current.max_dw = ib_size / 4 - amdgpu_cs_epilog_dws(cs);
    return true;
 }
@@ -1026,7 +1026,7 @@ amdgpu_cs_setup_preemption(struct radeon_cmdbuf *rcs, const uint32_t *preamble_i
    struct amdgpu_winsys *ws = cs->ws;
    struct amdgpu_cs_context *csc[2] = {&cs->csc1, &cs->csc2};
    unsigned size = align(preamble_num_dw * 4, ws->info.ip[AMD_IP_GFX].ib_alignment);
-   struct pb_buffer *preamble_bo;
+   struct pb_buffer_lean *preamble_bo;
    uint32_t *map;
 
    /* Create the preamble IB buffer. */
@@ -1146,7 +1146,7 @@ static bool amdgpu_cs_check_space(struct radeon_cmdbuf *rcs, unsigned dw)
    rcs->current.cdw = 0;
 
    rcs->current.buf = (uint32_t*)(main_ib->big_buffer_cpu_ptr + main_ib->used_ib_space);
-   rcs->current.max_dw = main_ib->big_buffer->base.size / 4 - cs_epilog_dw;
+   rcs->current.max_dw = main_ib->big_buffer->size / 4 - cs_epilog_dw;
 
    amdgpu_cs_add_buffer(rcs, main_ib->big_buffer,
                         RADEON_USAGE_READ | RADEON_PRIO_IB, 0);
@@ -1163,7 +1163,7 @@ static unsigned amdgpu_cs_get_buffer_list(struct radeon_cmdbuf *rcs,
 
     if (list) {
         for (unsigned i = 0; i < num_real_buffers; i++) {
-            list[i].bo_size = real_buffers->buffers[i].bo->base.base.size;
+            list[i].bo_size = real_buffers->buffers[i].bo->base.size;
             list[i].vm_address =
                amdgpu_va_get_start_addr(get_real_bo(real_buffers->buffers[i].bo)->va_handle);
             list[i].priority_usage = real_buffers->buffers[i].usage;
@@ -1786,7 +1786,7 @@ static void amdgpu_cs_destroy(struct radeon_cmdbuf *rcs)
 }
 
 static bool amdgpu_bo_is_referenced(struct radeon_cmdbuf *rcs,
-                                    struct pb_buffer *_buf,
+                                    struct pb_buffer_lean *_buf,
                                     unsigned usage)
 {
    struct amdgpu_cs *cs = amdgpu_cs(rcs);
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h
index 67d7d97dd7c08..c922efc596f26 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h
@@ -48,7 +48,7 @@ enum ib_type {
 
 struct amdgpu_ib {
    /* A buffer out of which new IBs are allocated. */
-   struct pb_buffer        *big_buffer;
+   struct pb_buffer_lean   *big_buffer;
    uint8_t                 *big_buffer_cpu_ptr;
    uint64_t                gpu_address;
    unsigned                used_ib_space;
@@ -145,7 +145,7 @@ struct amdgpu_cs {
 
    struct util_queue_fence flush_completed;
    struct pipe_fence_handle *next_fence;
-   struct pb_buffer *preamble_ib_bo;
+   struct pb_buffer_lean *preamble_ib_bo;
 
    struct drm_amdgpu_cs_chunk_cp_gfx_shadow mcbp_fw_shadow_chunk;
 };
diff --git a/src/gallium/winsys/radeon/drm/radeon_drm_bo.c b/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
index 0491ec04399e0..2e1d9c488e2ca 100644
--- a/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
+++ b/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
@@ -21,14 +21,14 @@
 #include <stdio.h>
 #include <inttypes.h>
 
-static struct pb_buffer *
+static struct pb_buffer_lean *
 radeon_winsys_bo_create(struct radeon_winsys *rws,
                         uint64_t size,
                         unsigned alignment,
                         enum radeon_bo_domain domain,
                         enum radeon_bo_flag flags);
 
-static inline struct radeon_bo *radeon_bo(struct pb_buffer *bo)
+static inline struct radeon_bo *radeon_bo(struct pb_buffer_lean *bo)
 {
    return (struct radeon_bo *)bo;
 }
@@ -109,7 +109,7 @@ static void radeon_bo_wait_idle(struct radeon_winsys *rws, struct radeon_bo *bo)
 }
 
 static bool radeon_bo_wait(struct radeon_winsys *rws,
-                           struct pb_buffer *_buf, uint64_t timeout,
+                           struct pb_buffer_lean *_buf, uint64_t timeout,
                            unsigned usage)
 {
    struct radeon_bo *bo = radeon_bo(_buf);
@@ -154,7 +154,7 @@ static enum radeon_bo_domain get_valid_domain(enum radeon_bo_domain domain)
 }
 
 static enum radeon_bo_domain radeon_bo_get_initial_domain(
-      struct pb_buffer *buf)
+      struct pb_buffer_lean *buf)
 {
    struct radeon_bo *bo = (struct radeon_bo*)buf;
    struct drm_radeon_gem_op args;
@@ -329,7 +329,7 @@ out:
 
 void radeon_bo_destroy(void *winsys, struct pb_buffer_lean *_buf)
 {
-   struct radeon_bo *bo = radeon_bo((struct pb_buffer*)_buf);
+   struct radeon_bo *bo = radeon_bo((struct pb_buffer_lean*)_buf);
    struct radeon_drm_winsys *rws = bo->rws;
    struct drm_gem_close args;
 
@@ -339,7 +339,7 @@ void radeon_bo_destroy(void *winsys, struct pb_buffer_lean *_buf)
 
    mtx_lock(&rws->bo_handles_mutex);
    /* radeon_winsys_bo_from_handle might have revived the bo */
-   if (pipe_is_referenced(&bo->base.base.reference)) {
+   if (pipe_is_referenced(&bo->base.reference)) {
       mtx_unlock(&rws->bo_handles_mutex);
       return;
    }
@@ -351,7 +351,7 @@ void radeon_bo_destroy(void *winsys, struct pb_buffer_lean *_buf)
    mtx_unlock(&rws->bo_handles_mutex);
 
    if (bo->u.real.ptr)
-      os_munmap(bo->u.real.ptr, bo->base.base.size);
+      os_munmap(bo->u.real.ptr, bo->base.size);
 
    if (rws->info.r600_has_virtual_memory) {
       if (rws->va_unmap_working) {
@@ -369,14 +369,14 @@ void radeon_bo_destroy(void *winsys, struct pb_buffer_lean *_buf)
                                  sizeof(va)) != 0 &&
              va.operation == RADEON_VA_RESULT_ERROR) {
             fprintf(stderr, "radeon: Failed to deallocate virtual address for buffer:\n");
-            fprintf(stderr, "radeon:    size      : %"PRIu64" bytes\n", bo->base.base.size);
+            fprintf(stderr, "radeon:    size      : %"PRIu64" bytes\n", bo->base.size);
             fprintf(stderr, "radeon:    va        : 0x%"PRIx64"\n", bo->va);
          }
       }
 
       radeon_bomgr_free_va(&rws->info,
                            bo->va < rws->vm32.end ? &rws->vm32 : &rws->vm64,
-                           bo->va, bo->base.base.size);
+                           bo->va, bo->base.size);
    }
 
    /* Close object. */
@@ -386,22 +386,22 @@ void radeon_bo_destroy(void *winsys, struct pb_buffer_lean *_buf)
    mtx_destroy(&bo->u.real.map_mutex);
 
    if (bo->initial_domain & RADEON_DOMAIN_VRAM)
-      rws->allocated_vram -= align(bo->base.base.size, rws->info.gart_page_size);
+      rws->allocated_vram -= align(bo->base.size, rws->info.gart_page_size);
    else if (bo->initial_domain & RADEON_DOMAIN_GTT)
-      rws->allocated_gtt -= align(bo->base.base.size, rws->info.gart_page_size);
+      rws->allocated_gtt -= align(bo->base.size, rws->info.gart_page_size);
 
    if (bo->u.real.map_count >= 1) {
       if (bo->initial_domain & RADEON_DOMAIN_VRAM)
-         bo->rws->mapped_vram -= bo->base.base.size;
+         bo->rws->mapped_vram -= bo->base.size;
       else
-         bo->rws->mapped_gtt -= bo->base.base.size;
+         bo->rws->mapped_gtt -= bo->base.size;
       bo->rws->num_mapped_buffers--;
    }
 
    FREE(bo);
 }
 
-static void radeon_bo_destroy_or_cache(void *winsys, struct pb_buffer *_buf)
+static void radeon_bo_destroy_or_cache(void *winsys, struct pb_buffer_lean *_buf)
 {
    struct radeon_drm_winsys *rws = (struct radeon_drm_winsys *)winsys;
    struct radeon_bo *bo = radeon_bo(_buf);
@@ -411,7 +411,7 @@ static void radeon_bo_destroy_or_cache(void *winsys, struct pb_buffer *_buf)
    if (bo->u.real.use_reusable_pool)
       pb_cache_add_buffer(&rws->bo_cache, &bo->u.real.cache_entry);
    else
-      radeon_bo_destroy(NULL, &_buf->base);
+      radeon_bo_destroy(NULL, _buf);
 }
 
 void *radeon_bo_do_map(struct radeon_bo *bo)
@@ -441,7 +441,7 @@ void *radeon_bo_do_map(struct radeon_bo *bo)
    }
    args.handle = bo->handle;
    args.offset = 0;
-   args.size = (uint64_t)bo->base.base.size;
+   args.size = (uint64_t)bo->base.size;
    if (drmCommandWriteRead(bo->rws->fd,
                            DRM_RADEON_GEM_MMAP,
                            &args,
@@ -470,9 +470,9 @@ void *radeon_bo_do_map(struct radeon_bo *bo)
    bo->u.real.map_count = 1;
 
    if (bo->initial_domain & RADEON_DOMAIN_VRAM)
-      bo->rws->mapped_vram += bo->base.base.size;
+      bo->rws->mapped_vram += bo->base.size;
    else
-      bo->rws->mapped_gtt += bo->base.base.size;
+      bo->rws->mapped_gtt += bo->base.size;
    bo->rws->num_mapped_buffers++;
 
    mtx_unlock(&bo->u.real.map_mutex);
@@ -480,7 +480,7 @@ void *radeon_bo_do_map(struct radeon_bo *bo)
 }
 
 static void *radeon_bo_map(struct radeon_winsys *rws,
-                           struct pb_buffer *buf,
+                           struct pb_buffer_lean *buf,
                            struct radeon_cmdbuf *rcs,
                            enum pipe_map_flags usage)
 {
@@ -505,7 +505,7 @@ static void *radeon_bo_map(struct radeon_winsys *rws,
                return NULL;
             }
 
-            if (!radeon_bo_wait(rws, (struct pb_buffer*)bo, 0,
+            if (!radeon_bo_wait(rws, (struct pb_buffer_lean*)bo, 0,
                                 RADEON_USAGE_WRITE)) {
                return NULL;
             }
@@ -516,7 +516,7 @@ static void *radeon_bo_map(struct radeon_winsys *rws,
                return NULL;
             }
 
-            if (!radeon_bo_wait(rws, (struct pb_buffer*)bo, 0,
+            if (!radeon_bo_wait(rws, (struct pb_buffer_lean*)bo, 0,
                                 RADEON_USAGE_READWRITE)) {
                return NULL;
             }
@@ -536,7 +536,7 @@ static void *radeon_bo_map(struct radeon_winsys *rws,
                cs->flush_cs(cs->flush_data,
                             RADEON_FLUSH_START_NEXT_GFX_IB_NOW, NULL);
             }
-            radeon_bo_wait(rws, (struct pb_buffer*)bo, OS_TIMEOUT_INFINITE,
+            radeon_bo_wait(rws, (struct pb_buffer_lean*)bo, OS_TIMEOUT_INFINITE,
                            RADEON_USAGE_WRITE);
          } else {
             /* Mapping for write. */
@@ -551,7 +551,7 @@ static void *radeon_bo_map(struct radeon_winsys *rws,
                }
             }
 
-            radeon_bo_wait(rws, (struct pb_buffer*)bo, OS_TIMEOUT_INFINITE,
+            radeon_bo_wait(rws, (struct pb_buffer_lean*)bo, OS_TIMEOUT_INFINITE,
                            RADEON_USAGE_READWRITE);
          }
 
@@ -562,7 +562,7 @@ static void *radeon_bo_map(struct radeon_winsys *rws,
    return radeon_bo_do_map(bo);
 }
 
-static void radeon_bo_unmap(struct radeon_winsys *rws, struct pb_buffer *_buf)
+static void radeon_bo_unmap(struct radeon_winsys *rws, struct pb_buffer_lean *_buf)
 {
    struct radeon_bo *bo = (struct radeon_bo*)_buf;
 
@@ -584,13 +584,13 @@ static void radeon_bo_unmap(struct radeon_winsys *rws, struct pb_buffer *_buf)
       return; /* it's been mapped multiple times */
    }
 
-   os_munmap(bo->u.real.ptr, bo->base.base.size);
+   os_munmap(bo->u.real.ptr, bo->base.size);
    bo->u.real.ptr = NULL;
 
    if (bo->initial_domain & RADEON_DOMAIN_VRAM)
-      bo->rws->mapped_vram -= bo->base.base.size;
+      bo->rws->mapped_vram -= bo->base.size;
    else
-      bo->rws->mapped_gtt -= bo->base.base.size;
+      bo->rws->mapped_gtt -= bo->base.size;
    bo->rws->num_mapped_buffers--;
 
    mtx_unlock(&bo->u.real.map_mutex);
@@ -645,10 +645,10 @@ static struct radeon_bo *radeon_create_bo(struct radeon_drm_winsys *rws,
    if (!bo)
       return NULL;
 
-   pipe_reference_init(&bo->base.base.reference, 1);
-   bo->base.base.alignment_log2 = util_logbase2(alignment);
-   bo->base.base.usage = 0;
-   bo->base.base.size = size;
+   pipe_reference_init(&bo->base.reference, 1);
+   bo->base.alignment_log2 = util_logbase2(alignment);
+   bo->base.usage = 0;
+   bo->base.size = size;
    bo->rws = rws;
    bo->handle = args.handle;
    bo->va = 0;
@@ -657,7 +657,7 @@ static struct radeon_bo *radeon_create_bo(struct radeon_drm_winsys *rws,
    (void) mtx_init(&bo->u.real.map_mutex, mtx_plain);
 
    if (heap >= 0) {
-      pb_cache_init_entry(&rws->bo_cache, &bo->u.real.cache_entry, &bo->base.base,
+      pb_cache_init_entry(&rws->bo_cache, &bo->u.real.cache_entry, &bo->base,
                           heap);
    }
 
@@ -689,12 +689,12 @@ static struct radeon_bo *radeon_create_bo(struct radeon_drm_winsys *rws,
          fprintf(stderr, "radeon:    alignment : %d bytes\n", alignment);
          fprintf(stderr, "radeon:    domains   : %d\n", args.initial_domain);
          fprintf(stderr, "radeon:    va        : 0x%016llx\n", (unsigned long long)bo->va);
-         radeon_bo_destroy(NULL, &bo->base.base);
+         radeon_bo_destroy(NULL, &bo->base);
          return NULL;
       }
       mtx_lock(&rws->bo_handles_mutex);
       if (va.operation == RADEON_VA_RESULT_VA_EXIST) {
-         struct pb_buffer *b = &bo->base;
+         struct pb_buffer_lean *b = &bo->base;
          struct radeon_bo *old_bo =
                _mesa_hash_table_u64_search(rws->bo_vas, va.offset);
 
@@ -717,22 +717,22 @@ static struct radeon_bo *radeon_create_bo(struct radeon_drm_winsys *rws,
 
 bool radeon_bo_can_reclaim(void *winsys, struct pb_buffer_lean *_buf)
 {
-   struct radeon_bo *bo = radeon_bo((struct pb_buffer*)_buf);
+   struct radeon_bo *bo = radeon_bo((struct pb_buffer_lean*)_buf);
 
    if (radeon_bo_is_referenced_by_any_cs(bo))
       return false;
 
-   return radeon_bo_wait(winsys, (struct pb_buffer*)_buf, 0, RADEON_USAGE_READWRITE);
+   return radeon_bo_wait(winsys, (struct pb_buffer_lean*)_buf, 0, RADEON_USAGE_READWRITE);
 }
 
 bool radeon_bo_can_reclaim_slab(void *priv, struct pb_slab_entry *entry)
 {
    struct radeon_bo *bo = container_of(entry, struct radeon_bo, u.slab.entry);
 
-   return radeon_bo_can_reclaim(NULL, &bo->base.base);
+   return radeon_bo_can_reclaim(NULL, &bo->base);
 }
 
-static void radeon_bo_slab_destroy(void *winsys, struct pb_buffer *_buf)
+static void radeon_bo_slab_destroy(void *winsys, struct pb_buffer_lean *_buf)
 {
    struct radeon_bo *bo = radeon_bo(_buf);
 
@@ -762,7 +762,7 @@ struct pb_slab *radeon_bo_slab_alloc(void *priv, unsigned heap,
 
    assert(slab->buffer->handle);
 
-   slab->base.num_entries = slab->buffer->base.base.size / entry_size;
+   slab->base.num_entries = slab->buffer->base.size / entry_size;
    slab->base.num_free = slab->base.num_entries;
    slab->base.group_index = group_index;
    slab->base.entry_size = entry_size;
@@ -777,9 +777,9 @@ struct pb_slab *radeon_bo_slab_alloc(void *priv, unsigned heap,
    for (unsigned i = 0; i < slab->base.num_entries; ++i) {
       struct radeon_bo *bo = &slab->entries[i];
 
-      bo->base.base.alignment_log2 = util_logbase2(entry_size);
-      bo->base.base.usage = slab->buffer->base.base.usage;
-      bo->base.base.size = entry_size;
+      bo->base.alignment_log2 = util_logbase2(entry_size);
+      bo->base.usage = slab->buffer->base.usage;
+      bo->base.size = entry_size;
       bo->rws = ws;
       bo->va = slab->buffer->va + i * entry_size;
       bo->initial_domain = domains;
@@ -846,7 +846,7 @@ static unsigned eg_tile_split_rev(unsigned eg_tile_split)
 }
 
 static void radeon_bo_get_metadata(struct radeon_winsys *rws,
-                                   struct pb_buffer *_buf,
+                                   struct pb_buffer_lean *_buf,
                                    struct radeon_bo_metadata *md,
                                    struct radeon_surf *surf)
 {
@@ -904,7 +904,7 @@ static void radeon_bo_get_metadata(struct radeon_winsys *rws,
 }
 
 static void radeon_bo_set_metadata(struct radeon_winsys *rws,
-                                   struct pb_buffer *_buf,
+                                   struct pb_buffer_lean *_buf,
                                    struct radeon_bo_metadata *md,
                                    struct radeon_surf *surf)
 {
@@ -974,7 +974,7 @@ static void radeon_bo_set_metadata(struct radeon_winsys *rws,
                        sizeof(args));
 }
 
-static struct pb_buffer *
+static struct pb_buffer_lean *
 radeon_winsys_bo_create(struct radeon_winsys *rws,
                         uint64_t size,
                         unsigned alignment,
@@ -1013,7 +1013,7 @@ radeon_winsys_bo_create(struct radeon_winsys *rws,
 
       bo = container_of(entry, struct radeon_bo, u.slab.entry);
 
-      pipe_reference_init(&bo->base.base.reference, 1);
+      pipe_reference_init(&bo->base.reference, 1);
 
       return &bo->base;
    }
@@ -1034,7 +1034,7 @@ radeon_winsys_bo_create(struct radeon_winsys *rws,
       heap = radeon_get_heap_index(domain, flags & ~RADEON_FLAG_NO_SUBALLOC);
       assert(heap >= 0 && heap < RADEON_NUM_HEAPS);
 
-      bo = radeon_bo((struct pb_buffer*)pb_cache_reclaim_buffer(&ws->bo_cache, size,
+      bo = radeon_bo((struct pb_buffer_lean*)pb_cache_reclaim_buffer(&ws->bo_cache, size,
                                                                 alignment, 0, heap));
       if (bo)
          return &bo->base;
@@ -1060,7 +1060,7 @@ radeon_winsys_bo_create(struct radeon_winsys *rws,
    return &bo->base;
 }
 
-static void radeon_winsys_bo_destroy(struct radeon_winsys *ws, struct pb_buffer *buf)
+static void radeon_winsys_bo_destroy(struct radeon_winsys *ws, struct pb_buffer_lean *buf)
 {
    struct radeon_bo *bo = radeon_bo(buf);
 
@@ -1070,7 +1070,7 @@ static void radeon_winsys_bo_destroy(struct radeon_winsys *ws, struct pb_buffer
       radeon_bo_slab_destroy(ws, buf);
 }
 
-static struct pb_buffer *radeon_winsys_bo_from_ptr(struct radeon_winsys *rws,
+static struct pb_buffer_lean *radeon_winsys_bo_from_ptr(struct radeon_winsys *rws,
                                                    void *pointer, uint64_t size,
                                                    enum radeon_bo_flag flags)
 {
@@ -1106,10 +1106,10 @@ static struct pb_buffer *radeon_winsys_bo_from_ptr(struct radeon_winsys *rws,
    mtx_lock(&ws->bo_handles_mutex);
 
    /* Initialize it. */
-   pipe_reference_init(&bo->base.base.reference, 1);
+   pipe_reference_init(&bo->base.reference, 1);
    bo->handle = args.handle;
-   bo->base.base.alignment_log2 = 0;
-   bo->base.base.size = size;
+   bo->base.alignment_log2 = 0;
+   bo->base.size = size;
    bo->rws = ws;
    bo->user_ptr = pointer;
    bo->va = 0;
@@ -1124,7 +1124,7 @@ static struct pb_buffer *radeon_winsys_bo_from_ptr(struct radeon_winsys *rws,
    if (ws->info.r600_has_virtual_memory) {
       struct drm_radeon_gem_va va;
 
-      bo->va = radeon_bomgr_find_va64(ws, bo->base.base.size, 1 << 20);
+      bo->va = radeon_bomgr_find_va64(ws, bo->base.size, 1 << 20);
 
       va.handle = bo->handle;
       va.operation = RADEON_VA_MAP;
@@ -1137,12 +1137,12 @@ static struct pb_buffer *radeon_winsys_bo_from_ptr(struct radeon_winsys *rws,
       r = drmCommandWriteRead(ws->fd, DRM_RADEON_GEM_VA, &va, sizeof(va));
       if (r && va.operation == RADEON_VA_RESULT_ERROR) {
          fprintf(stderr, "radeon: Failed to assign virtual address space\n");
-         radeon_bo_destroy(NULL, &bo->base.base);
+         radeon_bo_destroy(NULL, &bo->base);
          return NULL;
       }
       mtx_lock(&ws->bo_handles_mutex);
       if (va.operation == RADEON_VA_RESULT_VA_EXIST) {
-         struct pb_buffer *b = &bo->base;
+         struct pb_buffer_lean *b = &bo->base;
          struct radeon_bo *old_bo =
                _mesa_hash_table_u64_search(ws->bo_vas, va.offset);
 
@@ -1155,12 +1155,12 @@ static struct pb_buffer *radeon_winsys_bo_from_ptr(struct radeon_winsys *rws,
       mtx_unlock(&ws->bo_handles_mutex);
    }
 
-   ws->allocated_gtt += align(bo->base.base.size, ws->info.gart_page_size);
+   ws->allocated_gtt += align(bo->base.size, ws->info.gart_page_size);
 
-   return (struct pb_buffer*)bo;
+   return (struct pb_buffer_lean*)bo;
 }
 
-static struct pb_buffer *radeon_winsys_bo_from_handle(struct radeon_winsys *rws,
+static struct pb_buffer_lean *radeon_winsys_bo_from_handle(struct radeon_winsys *rws,
                                                       struct winsys_handle *whandle,
                                                       unsigned vm_alignment,
                                                       bool is_dri_prime_linear_buffer)
@@ -1195,7 +1195,7 @@ static struct pb_buffer *radeon_winsys_bo_from_handle(struct radeon_winsys *rws,
 
    if (bo) {
       /* Increase the refcount. */
-      p_atomic_inc(&bo->base.base.reference.count);
+      p_atomic_inc(&bo->base.reference.count);
       goto done;
    }
 
@@ -1235,9 +1235,9 @@ static struct pb_buffer *radeon_winsys_bo_from_handle(struct radeon_winsys *rws,
    bo->handle = handle;
 
    /* Initialize it. */
-   pipe_reference_init(&bo->base.base.reference, 1);
-   bo->base.base.alignment_log2 = 0;
-   bo->base.base.size = (unsigned) size;
+   pipe_reference_init(&bo->base.reference, 1);
+   bo->base.alignment_log2 = 0;
+   bo->base.size = (unsigned) size;
    bo->rws = ws;
    bo->va = 0;
    bo->hash = __sync_fetch_and_add(&ws->next_bo_hash, 1);
@@ -1254,7 +1254,7 @@ done:
    if (ws->info.r600_has_virtual_memory && !bo->va) {
       struct drm_radeon_gem_va va;
 
-      bo->va = radeon_bomgr_find_va64(ws, bo->base.base.size, vm_alignment);
+      bo->va = radeon_bomgr_find_va64(ws, bo->base.size, vm_alignment);
 
       va.handle = bo->handle;
       va.operation = RADEON_VA_MAP;
@@ -1267,12 +1267,12 @@ done:
       r = drmCommandWriteRead(ws->fd, DRM_RADEON_GEM_VA, &va, sizeof(va));
       if (r && va.operation == RADEON_VA_RESULT_ERROR) {
          fprintf(stderr, "radeon: Failed to assign virtual address space\n");
-         radeon_bo_destroy(NULL, &bo->base.base);
+         radeon_bo_destroy(NULL, &bo->base);
          return NULL;
       }
       mtx_lock(&ws->bo_handles_mutex);
       if (va.operation == RADEON_VA_RESULT_VA_EXIST) {
-         struct pb_buffer *b = &bo->base;
+         struct pb_buffer_lean *b = &bo->base;
          struct radeon_bo *old_bo =
                _mesa_hash_table_u64_search(ws->bo_vas, va.offset);
 
@@ -1288,11 +1288,11 @@ done:
    bo->initial_domain = radeon_bo_get_initial_domain((void*)bo);
 
    if (bo->initial_domain & RADEON_DOMAIN_VRAM)
-      ws->allocated_vram += align(bo->base.base.size, ws->info.gart_page_size);
+      ws->allocated_vram += align(bo->base.size, ws->info.gart_page_size);
    else if (bo->initial_domain & RADEON_DOMAIN_GTT)
-      ws->allocated_gtt += align(bo->base.base.size, ws->info.gart_page_size);
+      ws->allocated_gtt += align(bo->base.size, ws->info.gart_page_size);
 
-   return (struct pb_buffer*)bo;
+   return (struct pb_buffer_lean*)bo;
 
 fail:
    mtx_unlock(&ws->bo_handles_mutex);
@@ -1300,7 +1300,7 @@ fail:
 }
 
 static bool radeon_winsys_bo_get_handle(struct radeon_winsys *rws,
-                                        struct pb_buffer *buffer,
+                                        struct pb_buffer_lean *buffer,
                                         struct winsys_handle *whandle)
 {
    struct drm_gem_flink flink;
@@ -1340,22 +1340,22 @@ static bool radeon_winsys_bo_get_handle(struct radeon_winsys *rws,
    return true;
 }
 
-static bool radeon_winsys_bo_is_user_ptr(struct pb_buffer *buf)
+static bool radeon_winsys_bo_is_user_ptr(struct pb_buffer_lean *buf)
 {
    return ((struct radeon_bo*)buf)->user_ptr != NULL;
 }
 
-static bool radeon_winsys_bo_is_suballocated(struct pb_buffer *buf)
+static bool radeon_winsys_bo_is_suballocated(struct pb_buffer_lean *buf)
 {
    return !((struct radeon_bo*)buf)->handle;
 }
 
-static uint64_t radeon_winsys_bo_va(struct pb_buffer *buf)
+static uint64_t radeon_winsys_bo_va(struct pb_buffer_lean *buf)
 {
    return ((struct radeon_bo*)buf)->va;
 }
 
-static unsigned radeon_winsys_bo_get_reloc_offset(struct pb_buffer *buf)
+static unsigned radeon_winsys_bo_get_reloc_offset(struct pb_buffer_lean *buf)
 {
    struct radeon_bo *bo = radeon_bo(buf);
 
diff --git a/src/gallium/winsys/radeon/drm/radeon_drm_bo.h b/src/gallium/winsys/radeon/drm/radeon_drm_bo.h
index 8c7a87afc0f01..28619185e1a77 100644
--- a/src/gallium/winsys/radeon/drm/radeon_drm_bo.h
+++ b/src/gallium/winsys/radeon/drm/radeon_drm_bo.h
@@ -13,7 +13,7 @@
 #include "pipebuffer/pb_slab.h"
 
 struct radeon_bo {
-   struct pb_buffer base;
+   struct pb_buffer_lean base;
    union {
       struct {
          struct pb_cache_entry cache_entry;
@@ -70,7 +70,7 @@ static inline void
 radeon_ws_bo_reference(struct radeon_winsys *rws, struct radeon_bo **dst,
                        struct radeon_bo *src)
 {
-   radeon_bo_reference(rws, (struct pb_buffer**)dst, (struct pb_buffer*)src);
+   radeon_bo_reference(rws, (struct pb_buffer_lean**)dst, (struct pb_buffer_lean*)src);
 }
 
 void *radeon_bo_do_map(struct radeon_bo *bo);
diff --git a/src/gallium/winsys/radeon/drm/radeon_drm_cs.c b/src/gallium/winsys/radeon/drm/radeon_drm_cs.c
index 2cdd5780eefd4..0d5bd68b12e2f 100644
--- a/src/gallium/winsys/radeon/drm/radeon_drm_cs.c
+++ b/src/gallium/winsys/radeon/drm/radeon_drm_cs.c
@@ -373,7 +373,7 @@ static int radeon_lookup_or_add_slab_buffer(struct radeon_drm_cs *cs,
 }
 
 static unsigned radeon_drm_cs_add_buffer(struct radeon_cmdbuf *rcs,
-                                         struct pb_buffer *buf,
+                                         struct pb_buffer_lean *buf,
                                          unsigned usage,
                                          enum radeon_bo_domain domains)
 {
@@ -415,15 +415,15 @@ static unsigned radeon_drm_cs_add_buffer(struct radeon_cmdbuf *rcs,
    cs->csc->relocs_bo[index].u.real.priority_usage |= priority;
 
    if (added_domains & RADEON_DOMAIN_VRAM)
-      rcs->used_vram_kb += bo->base.base.size / 1024;
+      rcs->used_vram_kb += bo->base.size / 1024;
    else if (added_domains & RADEON_DOMAIN_GTT)
-      rcs->used_gart_kb += bo->base.base.size / 1024;
+      rcs->used_gart_kb += bo->base.size / 1024;
 
    return index;
 }
 
 static int radeon_drm_cs_lookup_buffer(struct radeon_cmdbuf *rcs,
-                                       struct pb_buffer *buf)
+                                       struct pb_buffer_lean *buf)
 {
    struct radeon_drm_cs *cs = radeon_drm_cs(rcs);
 
@@ -483,7 +483,7 @@ static unsigned radeon_drm_cs_get_buffer_list(struct radeon_cmdbuf *rcs,
 
    if (list) {
       for (i = 0; i < cs->csc->num_relocs; i++) {
-         list[i].bo_size = cs->csc->relocs_bo[i].bo->base.base.size;
+         list[i].bo_size = cs->csc->relocs_bo[i].bo->base.size;
          list[i].vm_address = cs->csc->relocs_bo[i].bo->va;
          list[i].priority_usage = cs->csc->relocs_bo[i].u.real.priority_usage;
       }
@@ -766,7 +766,7 @@ static void radeon_drm_cs_destroy(struct radeon_cmdbuf *rcs)
 }
 
 static bool radeon_bo_is_referenced(struct radeon_cmdbuf *rcs,
-                                    struct pb_buffer *_buf,
+                                    struct pb_buffer_lean *_buf,
                                     unsigned usage)
 {
    struct radeon_drm_cs *cs = radeon_drm_cs(rcs);
@@ -796,7 +796,7 @@ static bool radeon_bo_is_referenced(struct radeon_cmdbuf *rcs,
 static struct pipe_fence_handle *radeon_cs_create_fence(struct radeon_cmdbuf *rcs)
 {
    struct radeon_drm_cs *cs = radeon_drm_cs(rcs);
-   struct pb_buffer *fence;
+   struct pb_buffer_lean *fence;
 
    /* Create a fence, which is a dummy BO. */
    fence = cs->ws->base.buffer_create(&cs->ws->base, 1, 1,
@@ -816,7 +816,7 @@ static bool radeon_fence_wait(struct radeon_winsys *ws,
                               struct pipe_fence_handle *fence,
                               uint64_t timeout)
 {
-   return ws->buffer_wait(ws, (struct pb_buffer*)fence, timeout,
+   return ws->buffer_wait(ws, (struct pb_buffer_lean*)fence, timeout,
                           RADEON_USAGE_READWRITE);
 }
 
@@ -824,7 +824,7 @@ static void radeon_fence_reference(struct radeon_winsys *ws,
                                    struct pipe_fence_handle **dst,
                                    struct pipe_fence_handle *src)
 {
-   radeon_bo_reference(ws, (struct pb_buffer**)dst, (struct pb_buffer*)src);
+   radeon_bo_reference(ws, (struct pb_buffer_lean**)dst, (struct pb_buffer_lean*)src);
 }
 
 static struct pipe_fence_handle *radeon_drm_cs_get_next_fence(struct radeon_cmdbuf *rcs)
-- 
GitLab


From c0a6ad0e66367e51e1c5b9e617b088b2e994f8ce Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 9 Dec 2023 17:23:26 -0500
Subject: [PATCH 25/36] winsys/amdgpu: allocate 1 amdgpu_bo_slab_entry per
 cache line

The structure size is exactly 64 bytes, so every entry occupies exactly
1 cache line.

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c | 6 ++++--
 1 file changed, 4 insertions(+), 2 deletions(-)

diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index 2e5e13cb177f6..a3acf4fa7b787 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -729,10 +729,12 @@ struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_s
    slab_bo->slab.num_free = slab_bo->slab.num_entries;
    slab_bo->slab.group_index = group_index;
    slab_bo->slab.entry_size = entry_size;
-   slab_bo->entries = CALLOC(slab_bo->slab.num_entries, sizeof(*slab_bo->entries));
+   slab_bo->entries = os_malloc_aligned(slab_bo->slab.num_entries * sizeof(*slab_bo->entries),
+                                        CACHE_LINE_SIZE);
    if (!slab_bo->entries)
       goto fail;
 
+   memset(slab_bo->entries, 0, slab_bo->slab.num_entries * sizeof(*slab_bo->entries));
    list_inithead(&slab_bo->slab.free);
 
    base_id = __sync_fetch_and_add(&ws->next_bo_unique_id, slab_bo->slab.num_entries);
@@ -778,7 +780,7 @@ void amdgpu_bo_slab_free(struct amdgpu_winsys *ws, struct pb_slab *slab)
    for (unsigned i = 0; i < bo->slab.num_entries; ++i)
       amdgpu_bo_remove_fences(&bo->entries[i].b);
 
-   FREE(bo->entries);
+   os_free_aligned(bo->entries);
    amdgpu_winsys_bo_reference(ws, (struct amdgpu_winsys_bo**)&bo, NULL);
 }
 
-- 
GitLab


From ab7a4b79bfbd6165b81aa35731844939fcad2b55 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sun, 10 Dec 2023 02:39:17 -0500
Subject: [PATCH 26/36] winsys/amdgpu: compute bo->unique_id at pb_slab_alloc,
 not at memory allocation

We would compute the unique IDs for 1000 slab entries and then only use
a few, wasting the IDs. Assign the IDs only when we actually need to
return a new buffer.

This decreases the number of collisions we get in amdgpu_lookup_buffer,
and thus the number of times we have to search in the BO list.

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c | 5 +----
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.h | 3 +++
 2 files changed, 4 insertions(+), 4 deletions(-)

diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index a3acf4fa7b787..c254cba535e70 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -678,7 +678,6 @@ struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_s
    struct amdgpu_winsys *ws = priv;
    enum radeon_bo_domain domains = radeon_domain_from_heap(heap);
    enum radeon_bo_flag flags = radeon_flags_from_heap(heap);
-   uint32_t base_id;
 
    /* Determine the slab buffer size. */
    unsigned max_entry_size = 1 << (ws->bo_slabs.min_order + ws->bo_slabs.num_orders - 1);
@@ -737,8 +736,6 @@ struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_s
    memset(slab_bo->entries, 0, slab_bo->slab.num_entries * sizeof(*slab_bo->entries));
    list_inithead(&slab_bo->slab.free);
 
-   base_id = __sync_fetch_and_add(&ws->next_bo_unique_id, slab_bo->slab.num_entries);
-
    for (unsigned i = 0; i < slab_bo->slab.num_entries; ++i) {
       struct amdgpu_bo_slab_entry *bo = &slab_bo->entries[i];
 
@@ -746,7 +743,6 @@ struct pb_slab *amdgpu_bo_slab_alloc(void *priv, unsigned heap, unsigned entry_s
       bo->b.base.alignment_log2 = util_logbase2(get_slab_entry_alignment(ws, entry_size));
       bo->b.base.size = entry_size;
       bo->b.type = AMDGPU_BO_SLAB_ENTRY;
-      bo->b.unique_id = base_id + i;
 
       bo->entry.slab = &slab_bo->slab;
       list_addtail(&bo->entry.head, &slab_bo->slab.free);
@@ -1388,6 +1384,7 @@ amdgpu_bo_create(struct amdgpu_winsys *ws,
       struct amdgpu_bo_slab_entry *slab_bo = container_of(entry, struct amdgpu_bo_slab_entry, entry);
       pipe_reference_init(&slab_bo->b.base.reference, 1);
       slab_bo->b.base.size = size;
+      slab_bo->b.unique_id = __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
       assert(alignment <= 1 << slab_bo->b.base.alignment_log2);
 
       if (domain & RADEON_DOMAIN_VRAM)
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
index 8f99d68259b67..9f145c749c6a6 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
@@ -51,6 +51,9 @@ struct amdgpu_winsys_bo {
    struct pb_buffer_lean base;
    enum amdgpu_bo_type type;
 
+   /* This is set when a buffer is returned by buffer_create(), not when the memory is allocated
+    * as part of slab BO.
+    */
    uint32_t unique_id;
 
    /* how many command streams, which are being emitted in a separate
-- 
GitLab


From 48782e24e53cdd80bbde3a194f1311bd5bc96578 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Fri, 1 Dec 2023 22:16:34 -0500
Subject: [PATCH 27/36] winsys/amdgpu: rewrite BO fence tracking by adding a
 new queue fence system

This decreases the time spent in amdgpu_cs_submit_ib from 15.4% to 8.3%
in VP2020/Catia1, which is a decrease of CPU load for that thread by 46%.
Overall, it increases performance by a small number in CPU-bound benchmarks.
The biggest improvement I have seen is VP2020/Catia2, where it increases
FPS by 12%.

It no longer stores pipe_fence_handle references inside amdgpu_winsys_bo.

The idea is to have a global fixed list of queues (only 1 queue per IP
for now) where each queue generates its own sequence numbers (generated
by the winsys, not the kernel). Each queue also has a ring of fences.
The sequence numbers are used as indices into the ring of fences, which
is how sequence numbers are converted to fences.

With that, each BO only has to keep a list of sequence numbers, 1 for each
queue. The maximum number of queues is set to 6. Since the system can
handle integer wraparounds of sequence numbers correctly, we only need
16-bit sequence numbers in BOs to have accurate busyness tracking. Thus,
each BO uses only 12 bytes to represent all its fences for all queues.
There is also a 1-byte bitmask saying which sequence numbers are
initialized.

amdgpu_winsys.h contains the complete description. It has several
limitations that exist to minimize the memory footprint and updating of
BO fences.

Acked-by: Yogesh Mohan Marimuthu <yogesh.mohanmarimuthu@amd.com>
Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c     |  89 +++----
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.h     |  68 ++++-
 src/gallium/winsys/amdgpu/drm/amdgpu_cs.c     | 244 ++++++++++--------
 src/gallium/winsys/amdgpu/drm/amdgpu_cs.h     |   5 +-
 src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c |   5 +
 src/gallium/winsys/amdgpu/drm/amdgpu_winsys.h |  77 ++++++
 6 files changed, 322 insertions(+), 166 deletions(-)

diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index c254cba535e70..cd1c5fa26ab20 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -65,63 +65,48 @@ static bool amdgpu_bo_wait(struct radeon_winsys *rws,
       return !buffer_busy;
    }
 
-   if (timeout == 0) {
-      unsigned idle_fences;
-      bool buffer_idle;
-
-      simple_mtx_lock(&ws->bo_fence_lock);
-
-      for (idle_fences = 0; idle_fences < bo->num_fences; ++idle_fences) {
-         if (!amdgpu_fence_wait(bo->fences[idle_fences], 0, false))
-            break;
-      }
-
-      /* Release the idle fences to avoid checking them again later. */
-      for (unsigned i = 0; i < idle_fences; ++i)
-         amdgpu_fence_reference(&bo->fences[i], NULL);
+   simple_mtx_lock(&ws->bo_fence_lock);
 
-      memmove(&bo->fences[0], &bo->fences[idle_fences],
-              (bo->num_fences - idle_fences) * sizeof(*bo->fences));
-      bo->num_fences -= idle_fences;
+   u_foreach_bit(i, bo->fences.valid_fence_mask) {
+      struct pipe_fence_handle **fence = get_fence_from_ring(ws, &bo->fences, i);
 
-      buffer_idle = !bo->num_fences;
-      simple_mtx_unlock(&ws->bo_fence_lock);
+      if (fence) {
+         if (timeout == 0) {
+            bool idle = amdgpu_fence_wait(*fence, 0, false);
 
-      return buffer_idle;
-   } else {
-      bool buffer_idle = true;
+            if (!idle) {
+               simple_mtx_unlock(&ws->bo_fence_lock);
+               return false; /* busy */
+            }
 
-      simple_mtx_lock(&ws->bo_fence_lock);
-      while (bo->num_fences && buffer_idle) {
-         struct pipe_fence_handle *fence = NULL;
-         bool fence_idle = false;
+            /* It's idle. Remove it from the ring to skip checking it again later. */
+            amdgpu_fence_reference(fence, NULL);
+         } else {
+            struct pipe_fence_handle *tmp_fence = NULL;
+            amdgpu_fence_reference(&tmp_fence, *fence);
 
-         amdgpu_fence_reference(&fence, bo->fences[0]);
+            /* While waiting, unlock the mutex. */
+            simple_mtx_unlock(&ws->bo_fence_lock);
 
-         /* Wait for the fence. */
-         simple_mtx_unlock(&ws->bo_fence_lock);
-         if (amdgpu_fence_wait(fence, abs_timeout, true))
-            fence_idle = true;
-         else
-            buffer_idle = false;
-         simple_mtx_lock(&ws->bo_fence_lock);
+            bool idle = amdgpu_fence_wait(tmp_fence, abs_timeout, true);
+            if (!idle) {
+               amdgpu_fence_reference(&tmp_fence, NULL);
+               return false; /* busy */
+            }
 
-         /* Release an idle fence to avoid checking it again later, keeping in
-          * mind that the fence array may have been modified by other threads.
-          */
-         if (fence_idle && bo->num_fences && bo->fences[0] == fence) {
-            amdgpu_fence_reference(&bo->fences[0], NULL);
-            memmove(&bo->fences[0], &bo->fences[1],
-                    (bo->num_fences - 1) * sizeof(*bo->fences));
-            bo->num_fences--;
+            simple_mtx_lock(&ws->bo_fence_lock);
+            /* It's idle. Remove it from the ring to skip checking it again later. */
+            if (tmp_fence == *fence)
+               amdgpu_fence_reference(fence, NULL);
+            amdgpu_fence_reference(&tmp_fence, NULL);
          }
-
-         amdgpu_fence_reference(&fence, NULL);
       }
-      simple_mtx_unlock(&ws->bo_fence_lock);
 
-      return buffer_idle;
+      bo->fences.valid_fence_mask &= ~BITFIELD_BIT(i); /* remove the fence from the BO */
    }
+
+   simple_mtx_unlock(&ws->bo_fence_lock);
+   return true; /* idle */
 }
 
 static inline unsigned get_slab_entry_offset(struct amdgpu_winsys_bo *bo)
@@ -148,12 +133,7 @@ static enum radeon_bo_flag amdgpu_bo_get_flags(
 
 static void amdgpu_bo_remove_fences(struct amdgpu_winsys_bo *bo)
 {
-   for (unsigned i = 0; i < bo->num_fences; ++i)
-      amdgpu_fence_reference(&bo->fences[i], NULL);
-
-   FREE(bo->fences);
-   bo->num_fences = 0;
-   bo->max_fences = 0;
+   bo->fences.valid_fence_mask = 0;
 }
 
 void amdgpu_bo_destroy(struct amdgpu_winsys *ws, struct pb_buffer_lean *_buf)
@@ -937,8 +917,11 @@ sparse_free_backing_buffer(struct amdgpu_winsys *ws, struct amdgpu_bo_sparse *bo
 {
    bo->num_backing_pages -= backing->bo->b.base.size / RADEON_SPARSE_PAGE_SIZE;
 
+   /* Add fences from bo to backing->bo. */
    simple_mtx_lock(&ws->bo_fence_lock);
-   amdgpu_add_fences(&backing->bo->b, bo->b.num_fences, bo->b.fences);
+   u_foreach_bit(i, bo->b.fences.valid_fence_mask) {
+      add_seq_no_to_list(ws, &backing->bo->b.fences, i, bo->b.fences.seq_no[i]);
+   }
    simple_mtx_unlock(&ws->bo_fence_lock);
 
    list_del(&backing->list);
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
index 9f145c749c6a6..50e311233d084 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
@@ -49,7 +49,8 @@ enum amdgpu_bo_type {
 /* Base class of the buffer object that other structures inherit. */
 struct amdgpu_winsys_bo {
    struct pb_buffer_lean base;
-   enum amdgpu_bo_type type;
+   enum amdgpu_bo_type type:8;
+   struct amdgpu_seq_no_fences fences;
 
    /* This is set when a buffer is returned by buffer_create(), not when the memory is allocated
     * as part of slab BO.
@@ -59,11 +60,6 @@ struct amdgpu_winsys_bo {
    /* how many command streams, which are being emitted in a separate
     * thread, is this bo referenced in? */
    volatile int num_active_ioctls;
-
-   /* Fences for buffer synchronization. */
-   uint16_t num_fences;
-   uint16_t max_fences;
-   struct pipe_fence_handle **fences;
 };
 
 /* Real GPU memory allocation managed by the amdgpu kernel driver.
@@ -175,6 +171,66 @@ static struct amdgpu_bo_real *get_slab_entry_real_bo(struct amdgpu_winsys_bo *bo
    return &get_bo_from_slab(((struct amdgpu_bo_slab_entry*)bo)->entry.slab)->b.b;
 }
 
+/* Given a sequence number "fences->seq_no[queue_index]", return a pointer to a non-NULL fence
+ * pointer in the queue ring corresponding to that sequence number if the fence is non-NULL.
+ * If the fence is not present in the ring (= is idle), return NULL. If it returns a non-NULL
+ * pointer and the caller finds the fence to be idle, it's recommended to use the returned pointer
+ * to set the fence to NULL in the ring, which is why we return a pointer to a pointer.
+ */
+static inline struct pipe_fence_handle **
+get_fence_from_ring(struct amdgpu_winsys *ws, struct amdgpu_seq_no_fences *fences,
+                    unsigned queue_index)
+{
+   /* The caller should check if the BO has a fence. */
+   assert(queue_index < AMDGPU_MAX_QUEUES);
+   assert(fences->valid_fence_mask & BITFIELD_BIT(queue_index));
+
+   uint_seq_no buffer_seq_no = fences->seq_no[queue_index];
+   uint_seq_no latest_seq_no = ws->queues[queue_index].latest_seq_no;
+   bool fence_present = latest_seq_no - buffer_seq_no < AMDGPU_FENCE_RING_SIZE;
+
+   if (fence_present) {
+      struct pipe_fence_handle **fence =
+         &ws->queues[queue_index].fences[buffer_seq_no % AMDGPU_FENCE_RING_SIZE];
+
+      if (*fence)
+         return fence;
+   }
+
+   /* If the sequence number references a fence that is not present, it's guaranteed to be idle
+    * because the winsys always waits for the oldest fence when it removes it from the ring.
+    */
+   fences->valid_fence_mask &= ~BITFIELD_BIT(queue_index);
+   return NULL;
+}
+
+static inline uint_seq_no pick_latest_seq_no(struct amdgpu_winsys *ws, unsigned queue_index,
+                                             uint_seq_no n1, uint_seq_no n2)
+{
+   uint_seq_no latest = ws->queues[queue_index].latest_seq_no;
+
+   /* Since sequence numbers can wrap around, we need to pick the later number that's logically
+    * before "latest". The trick is to subtract "latest + 1" to underflow integer such
+    * that "latest" becomes UINT*_MAX, and then just return the maximum.
+    */
+   uint_seq_no s1 = n1 - latest - 1;
+   uint_seq_no s2 = n2 - latest - 1;
+
+   return s1 >= s2 ? n1 : n2;
+}
+
+static inline void add_seq_no_to_list(struct amdgpu_winsys *ws, struct amdgpu_seq_no_fences *fences,
+                                      unsigned queue_index, uint_seq_no seq_no)
+{
+   if (fences->valid_fence_mask & BITFIELD_BIT(queue_index)) {
+      fences->seq_no[queue_index] = pick_latest_seq_no(ws, queue_index, seq_no,
+                                                       fences->seq_no[queue_index]);
+   } else {
+      fences->seq_no[queue_index] = seq_no;
+      fences->valid_fence_mask |= BITFIELD_BIT(queue_index);
+   }
+}
+
 bool amdgpu_bo_can_reclaim(struct amdgpu_winsys *ws, struct pb_buffer_lean *_buf);
 struct pb_buffer_lean *amdgpu_bo_create(struct amdgpu_winsys *ws,
                                    uint64_t size,
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
index da8f92316d029..74cdc4a29a447 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
@@ -51,6 +51,7 @@ amdgpu_fence_import_syncobj(struct radeon_winsys *rws, int fd)
    }
 
    util_queue_fence_init(&fence->submitted);
+   fence->imported = true;
 
    assert(amdgpu_fence_is_syncobj(fence));
    return (struct pipe_fence_handle*)fence;
@@ -84,6 +85,7 @@ amdgpu_fence_import_sync_file(struct radeon_winsys *rws, int fd)
    }
 
    util_queue_fence_init(&fence->submitted);
+   fence->imported = true;
 
    return (struct pipe_fence_handle*)fence;
 }
@@ -975,6 +977,23 @@ amdgpu_cs_create(struct radeon_cmdbuf *rcs,
    cs->has_chaining = ctx->ws->info.gfx_level >= GFX7 &&
                       (ip_type == AMD_IP_GFX || ip_type == AMD_IP_COMPUTE);
 
+   /* Compute the queue index by counting the IPs that have queues. */
+   assert(ip_type < ARRAY_SIZE(ctx->ws->info.ip));
+   assert(ctx->ws->info.ip[ip_type].num_queues);
+   cs->queue_index = 0;
+
+   for (unsigned i = 0; i < ARRAY_SIZE(ctx->ws->info.ip); i++) {
+      if (!ctx->ws->info.ip[i].num_queues)
+         continue;
+
+      if (i == ip_type)
+         break;
+
+      cs->queue_index++;
+   }
+
+   assert(cs->queue_index < AMDGPU_MAX_QUEUES);
+
    struct amdgpu_cs_fence_info fence_info;
    fence_info.handle = cs->ctx->user_fence_bo;
    fence_info.offset = cs->ip_type * 4;
@@ -1191,27 +1210,6 @@ static void add_fence_to_list(struct amdgpu_fence_list *fences,
    amdgpu_fence_reference(&fences->list[idx], (struct pipe_fence_handle*)fence);
 }
 
-static bool is_noop_fence_dependency(struct amdgpu_cs *acs,
-                                     struct amdgpu_fence *fence)
-{
-   struct amdgpu_cs_context *cs = acs->csc;
-
-   /* Detect no-op dependencies only when there is only 1 ring,
-    * because IBs on one ring are always executed one at a time.
-    *
-    * We always want no dependency between back-to-back gfx IBs, because
-    * we need the parallelism between IBs for good performance.
-    */
-   if ((acs->ip_type == AMD_IP_GFX ||
-        acs->ws->info.ip[acs->ip_type].num_queues == 1) &&
-       !amdgpu_fence_is_syncobj(fence) &&
-       fence->ctx == acs->ctx &&
-       fence->fence.ip_type == cs->chunk_ib[IB_MAIN].ip_type)
-      return true;
-
-   return amdgpu_fence_wait((void *)fence, 0, false);
-}
-
 static void amdgpu_cs_add_fence_dependency(struct radeon_cmdbuf *rws,
                                            struct pipe_fence_handle *pfence,
                                            unsigned dependency_flags)
@@ -1222,7 +1220,8 @@ static void amdgpu_cs_add_fence_dependency(struct radeon_cmdbuf *rws,
 
    util_queue_fence_wait(&fence->submitted);
 
-   if (is_noop_fence_dependency(acs, fence))
+   /* Ignore non-imported idle fences. This will only check the user fence in memory. */
+   if (!fence->imported && amdgpu_fence_wait((void *)fence, 0, false))
       return;
 
    if (amdgpu_fence_is_syncobj(fence))
@@ -1231,94 +1230,30 @@ static void amdgpu_cs_add_fence_dependency(struct radeon_cmdbuf *rws,
       add_fence_to_list(&cs->fence_dependencies, fence);
 }
 
-static void amdgpu_add_bo_fence_dependencies(struct amdgpu_cs *acs,
-                                             struct amdgpu_cs_context *cs,
-                                             struct amdgpu_cs_buffer *buffer)
-{
-   struct amdgpu_winsys_bo *bo = buffer->bo;
-   unsigned new_num_fences = 0;
-   const unsigned num_fences = bo->num_fences;
-
-   for (unsigned j = 0; j < num_fences; ++j) {
-      struct amdgpu_fence *bo_fence = (void *)bo->fences[j];
-
-      if (is_noop_fence_dependency(acs, bo_fence))
-         continue;
-
-      amdgpu_fence_reference(&bo->fences[new_num_fences], bo->fences[j]);
-      new_num_fences++;
-
-      if (!(buffer->usage & RADEON_USAGE_SYNCHRONIZED))
-         continue;
-
-      add_fence_to_list(&cs->fence_dependencies, bo_fence);
-   }
-
-   for (unsigned j = new_num_fences; j < num_fences; ++j)
-      amdgpu_fence_reference(&bo->fences[j], NULL);
-
-   bo->num_fences = new_num_fences;
-}
-
-/* Add the given list of fences to the buffer's fence list.
- *
- * Must be called with the winsys bo_fence_lock held.
- */
-void amdgpu_add_fences(struct amdgpu_winsys_bo *bo,
-                       unsigned num_fences,
-                       struct pipe_fence_handle **fences)
-{
-   if (bo->num_fences + num_fences > bo->max_fences) {
-      unsigned new_max_fences = MAX2(bo->num_fences + num_fences, bo->max_fences * 2);
-      struct pipe_fence_handle **new_fences =
-         REALLOC(bo->fences,
-                 bo->num_fences * sizeof(*new_fences),
-                 new_max_fences * sizeof(*new_fences));
-      if (likely(new_fences && new_max_fences < UINT16_MAX)) {
-         bo->fences = new_fences;
-         bo->max_fences = new_max_fences;
-      } else {
-         unsigned drop;
-
-         fprintf(stderr, new_fences ? "amdgpu_add_fences: too many fences, dropping some\n"
-                                    : "amdgpu_add_fences: allocation failure, dropping fence(s)\n");
-         free(new_fences);
-
-         if (!bo->num_fences)
-            return;
-
-         bo->num_fences--; /* prefer to keep the most recent fence if possible */
-         amdgpu_fence_reference(&bo->fences[bo->num_fences], NULL);
-
-         drop = bo->num_fences + num_fences - bo->max_fences;
-         num_fences -= drop;
-         fences += drop;
-      }
-   }
-
-   unsigned bo_num_fences = bo->num_fences;
-
-   for (unsigned i = 0; i < num_fences; ++i) {
-      bo->fences[bo_num_fences] = NULL;
-      amdgpu_fence_reference(&bo->fences[bo_num_fences], fences[i]);
-      bo_num_fences++;
-   }
-   bo->num_fences = bo_num_fences;
-}
-
 static void amdgpu_add_bo_fences_to_dependencies(struct amdgpu_cs *acs,
-                                                 struct amdgpu_cs_context *cs,
-                                                 struct pipe_fence_handle *fence,
+                                                 struct amdgpu_seq_no_fences *dependencies,
+                                                 uint_seq_no new_queue_seq_no,
                                                  struct amdgpu_buffer_list *list)
 {
+   struct amdgpu_winsys *ws = acs->ws;
+   unsigned queue_index = acs->queue_index;
    unsigned num_buffers = list->num_buffers;
 
    for (unsigned i = 0; i < num_buffers; i++) {
       struct amdgpu_cs_buffer *buffer = &list->buffers[i];
       struct amdgpu_winsys_bo *bo = buffer->bo;
 
-      amdgpu_add_bo_fence_dependencies(acs, cs, buffer);
-      amdgpu_add_fences(bo, 1, &fence);
+      /* Add BO fences from queues other than 'queue_index' to dependencies. */
+      if (buffer->usage & RADEON_USAGE_SYNCHRONIZED) {
+         u_foreach_bit(other_queue_idx, bo->fences.valid_fence_mask & ~BITFIELD_BIT(queue_index)) {
+            add_seq_no_to_list(ws, dependencies, other_queue_idx,
+                               bo->fences.seq_no[other_queue_idx]);
+         }
+      }
+
+      /* Also set the fence in the BO. */
+      bo->fences.seq_no[queue_index] = new_queue_seq_no;
+      bo->fences.valid_fence_mask |= BITFIELD_BIT(queue_index);
    }
 }
 
@@ -1379,11 +1314,112 @@ static void amdgpu_cs_submit_ib(void *job, void *gdata, int thread_index)
    bool has_user_fence = amdgpu_cs_has_user_fence(cs);
 
    simple_mtx_lock(&ws->bo_fence_lock);
+   struct amdgpu_queue *queue = &ws->queues[acs->queue_index];
+   uint_seq_no prev_seq_no = queue->latest_seq_no;
+
+   /* Generate a per queue sequence number. The logic is similar to the kernel side amdgpu seqno,
+    * but the values aren't related.
+    */
+   uint_seq_no next_seq_no = prev_seq_no + 1;
+
+   /* Wait for the oldest fence to signal. This should always check the user fence, then wait
+    * via the ioctl. We have to do this because we are going to release the oldest fence and
+    * replace it with the latest fence in the ring.
+    */
+   struct pipe_fence_handle **oldest_fence =
+      &queue->fences[next_seq_no % AMDGPU_FENCE_RING_SIZE];
+
+   if (*oldest_fence) {
+      if (!amdgpu_fence_wait(*oldest_fence, 0, false)) {
+         /* Take the reference because the fence can be released by other threads after we
+          * unlock the mutex.
+          */
+         struct pipe_fence_handle *tmp_fence = NULL;
+         amdgpu_fence_reference(&tmp_fence, *oldest_fence);
+
+         /* Unlock the mutex before waiting. */
+         simple_mtx_unlock(&ws->bo_fence_lock);
+         amdgpu_fence_wait(tmp_fence, OS_TIMEOUT_INFINITE, false);
+         amdgpu_fence_reference(&tmp_fence, NULL);
+         simple_mtx_lock(&ws->bo_fence_lock);
+      }
+
+      /* Remove the idle fence from the ring. */
+      amdgpu_fence_reference(oldest_fence, NULL);
+   }
+
+   /* We'll accumulate sequence numbers in this structure. It automatically keeps only the latest
+    * sequence number per queue and removes all older ones.
+    */
+   struct amdgpu_seq_no_fences seq_no_dependencies;
+   seq_no_dependencies.valid_fence_mask = 0;
+
+   /* Add a fence dependency on the previous IB if the IP has multiple physical queues to
+    * make it appear as if it had only 1 queue, or if the previous IB comes from a different
+    * context. The reasons are:
+    * - Our BO fence tracking only supports 1 queue per IP.
+    * - IBs from different contexts must wait for each other and can't execute in a random order.
+    */
+   struct amdgpu_fence *prev_fence =
+      (struct amdgpu_fence*)queue->fences[prev_seq_no % AMDGPU_FENCE_RING_SIZE];
+
+   if (prev_fence && (ws->info.ip[acs->ip_type].num_queues > 1 || prev_fence->ctx != acs->ctx))
+      add_seq_no_to_list(ws, &seq_no_dependencies, acs->queue_index, prev_seq_no);
+
    /* Since the kernel driver doesn't synchronize execution between different
-    * rings automatically, we have to add fence dependencies manually.
+    * rings automatically, we have to add fence dependencies manually. This gathers sequence
+    * numbers from BOs and sets the next sequence number in the BOs.
     */
-   for (unsigned i = 0; i < ARRAY_SIZE(cs->buffer_lists); i++)
-      amdgpu_add_bo_fences_to_dependencies(acs, cs, cs->fence, &cs->buffer_lists[i]);
+   for (unsigned i = 0; i < ARRAY_SIZE(cs->buffer_lists); i++) {
+      amdgpu_add_bo_fences_to_dependencies(acs, &seq_no_dependencies, next_seq_no,
+                                           &cs->buffer_lists[i]);
+   }
+
+#if 0 /* Debug code. */
+   printf("submit queue=%u, seq_no=%u\n", acs->queue_index, next_seq_no);
+
+   /* Wait for all previous fences. This can be used when BO fence tracking doesn't work. */
+   for (unsigned i = 0; i < AMDGPU_MAX_QUEUES; i++) {
+      if (i == acs->queue_index)
+         continue;
+
+      struct pipe_fence_handle *fence = queue->fences[ws->queues[i].latest_seq_no % AMDGPU_FENCE_RING_SIZE];
+      if (!fence) {
+         if (i <= 1)
+            printf("      queue %u doesn't have any fence at seq_no %u\n", i, ws->queues[i].latest_seq_no);
+         continue;
+      }
+
+      bool valid = seq_no_dependencies.valid_fence_mask & BITFIELD_BIT(i);
+      uint_seq_no old = seq_no_dependencies.seq_no[i];
+      add_seq_no_to_list(ws, &seq_no_dependencies, i, ws->queues[i].latest_seq_no);
+      uint_seq_no new = seq_no_dependencies.seq_no[i];
+
+      if (!valid)
+         printf("   missing dependency on queue=%u, seq_no=%u\n", i, new);
+      else if (old != new)
+         printf("   too old dependency on queue=%u, old=%u, new=%u\n", i, old, new);
+      else
+         printf("   has dependency on queue=%u, seq_no=%u\n", i, old);
+   }
+#endif
+
+   /* Convert the sequence numbers we gathered to fence dependencies. */
+   u_foreach_bit(i, seq_no_dependencies.valid_fence_mask) {
+      struct pipe_fence_handle **fence = get_fence_from_ring(ws, &seq_no_dependencies, i);
+
+      if (fence) {
+         /* If it's idle, don't add it to the list of dependencies. */
+         if (amdgpu_fence_wait(*fence, 0, false))
+            amdgpu_fence_reference(fence, NULL);
+         else
+            add_fence_to_list(&cs->fence_dependencies, (struct amdgpu_fence*)*fence);
+      }
+   }
+
+   /* Finally, add the IB fence into the winsys queue. */
+   amdgpu_fence_reference(&queue->fences[next_seq_no % AMDGPU_FENCE_RING_SIZE], cs->fence);
+   queue->latest_seq_no = next_seq_no;
    simple_mtx_unlock(&ws->bo_fence_lock);
 
    struct drm_amdgpu_bo_list_entry *bo_list = NULL;
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h
index c922efc596f26..5b505af00bce9 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h
@@ -119,6 +119,7 @@ struct amdgpu_cs {
     */
    struct drm_amdgpu_cs_chunk_fence fence_chunk;
    enum amd_ip_type ip_type;
+   unsigned queue_index;
 
    /* We flip between these two CS. While one is being consumed
     * by the kernel in another thread, the other one is being filled
@@ -166,6 +167,7 @@ struct amdgpu_fence {
    struct util_queue_fence submitted;
 
    volatile int signalled;              /* bool (int for atomicity) */
+   bool imported;
 };
 
 static inline bool amdgpu_fence_is_syncobj(struct amdgpu_fence *fence)
@@ -242,9 +244,6 @@ amdgpu_bo_is_referenced_by_cs_with_usage(struct amdgpu_cs *cs,
 
 bool amdgpu_fence_wait(struct pipe_fence_handle *fence, uint64_t timeout,
                        bool absolute);
-void amdgpu_add_fences(struct amdgpu_winsys_bo *bo,
-                       unsigned num_fences,
-                       struct pipe_fence_handle **fences);
 void amdgpu_cs_sync_flush(struct radeon_cmdbuf *rcs);
 void amdgpu_cs_init_functions(struct amdgpu_screen_winsys *ws);
 
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c b/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c
index efd9c18c32fcc..8d3ef782a251c 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c
@@ -72,6 +72,11 @@ static void do_winsys_deinit(struct amdgpu_winsys *ws)
    if (ws->reserve_vmid)
       amdgpu_vm_unreserve_vmid(ws->dev, 0);
 
+   for (unsigned i = 0; i < ARRAY_SIZE(ws->queues); i++) {
+      for (unsigned j = 0; j < ARRAY_SIZE(ws->queues[i].fences); j++)
+         amdgpu_fence_reference(&ws->queues[i].fences[j], NULL);
+   }
+
    if (util_queue_is_initialized(&ws->cs_queue))
       util_queue_destroy(&ws->cs_queue);
 
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.h b/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.h
index 4c552461bca2b..70564e41bbc02 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.h
@@ -62,11 +62,88 @@ struct amdgpu_screen_winsys {
    struct hash_table *kms_handles;
 };
 
+/* Maximum this number of IBs can be busy per queue. When submitting a new IB and the oldest IB
+ * ("AMDGPU_FENCE_RING_SIZE" IBs ago) is still busy, the CS thread will wait for it and will
+ * also block all queues from submitting new IBs.
+ */
+#define AMDGPU_FENCE_RING_SIZE 32
+
+/* The maximum number of queues that can be present. */
+#define AMDGPU_MAX_QUEUES 6
+
+/* This can use any integer type because the logic handles integer wraparounds robustly, but
+ * uint8_t wraps around so quickly that some BOs might never become idle because we don't
+ * remove idle fences from BOs, so they become "busy" again after a queue sequence number wraps
+ * around and they may stay "busy" in pb_cache long enough that we run out of memory.
+ */
+typedef uint16_t uint_seq_no;
+
+struct amdgpu_queue {
+   /* Ring buffer of fences.
+    *
+    * We only remember a certain number of the most recent fences per queue. When we add a new
+    * fence, we wait for the oldest one, which implies that all older fences not present
+    * in the ring are idle. This way we don't have to keep track of a million fence references
+    * for a million BOs.
+    *
+    * We only support 1 queue per IP. If an IP has multiple queues, we always add a fence
+    * dependency on the previous fence to make it behave like there is only 1 queue.
+    *
+    * amdgpu_winsys_bo doesn't have a list of fences. It only remembers the last sequence number
+    * for every queue where it was used. We then use the BO's sequence number to look up a fence
+    * in this ring.
+    */
+   struct pipe_fence_handle *fences[AMDGPU_FENCE_RING_SIZE];
+
+   /* The sequence number of the latest fence.
+    *
+    * This sequence number is global per queue per device, shared by all contexts, and generated
+    * by the winsys, not the kernel.
+    *
+    * The latest fence is: fences[latest_seq_no % AMDGPU_FENCE_RING_SIZE]
+    * The oldest fence is: fences([latest_seq_no + 1) % AMDGPU_FENCE_RING_SIZE]
+    * The oldest sequence number in the ring: latest_seq_no - AMDGPU_FENCE_RING_SIZE + 1
+    *
+    * The sequence number is in the ring if:
+    *    latest_seq_no - buffer_seq_no < AMDGPU_FENCE_RING_SIZE
+    * If the sequence number is not in the ring, it's idle.
+    *
+    * Integer wraparounds of the sequence number behave as follows:
+    *
+    * The comparison above gives the correct answer if buffer_seq_no isn't older than UINT*_MAX.
+    * If it's older than UINT*_MAX but not older than UINT*_MAX + AMDGPU_FENCE_RING_SIZE, we
+    * incorrectly pick and wait for one of the fences in the ring. That's only a problem when
+    * the type is so small (uint8_t) that seq_no wraps around very frequently, causing BOs to
+    * never become idle in certain very unlucky scenarios and running out of memory.
+    */
+   uint_seq_no latest_seq_no;
+};
+
+/* This is part of every BO. */
+struct amdgpu_seq_no_fences {
+   /* A fence sequence number per queue. This number is used to look up the fence from
+    * struct amdgpu_queue.
+    *
+    * This sequence number is global per queue per device, shared by all contexts, and generated
+    * by the winsys, not the kernel.
+    */
+   uint_seq_no seq_no[AMDGPU_MAX_QUEUES];
+
+   /* The mask of queues where seq_no[i] is valid. */
+   uint8_t valid_fence_mask;
+};
+
+/* valid_fence_mask should have 1 bit for each queue. */
+static_assert(sizeof(((struct amdgpu_seq_no_fences*)NULL)->valid_fence_mask) * 8 >= AMDGPU_MAX_QUEUES, "");
+
 struct amdgpu_winsys {
    struct pipe_reference reference;
    /* See comment above */
    int fd;
 
+   /* Protected by bo_fence_lock. */
+   struct amdgpu_queue queues[AMDGPU_MAX_QUEUES];
+
    struct pb_cache bo_cache;
    struct pb_slabs bo_slabs;  /* Slab allocator. */
 
-- 
GitLab


From 291d0652d01ad162e3789fad3df7267d66e39f1c Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Fri, 15 Dec 2023 07:34:12 -0500
Subject: [PATCH 28/36] winsys/amdgpu: rename amdgpu_winsys_bo::bo -> bo_handle

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c | 38 +++++++++++------------
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.h |  2 +-
 2 files changed, 20 insertions(+), 20 deletions(-)

diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index cd1c5fa26ab20..df1a13f8b4ca4 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -59,7 +59,7 @@ static bool amdgpu_bo_wait(struct radeon_winsys *rws,
       bool buffer_busy = true;
       int r;
 
-      r = amdgpu_bo_wait_for_idle(get_real_bo(bo)->bo, timeout, &buffer_busy);
+      r = amdgpu_bo_wait_for_idle(get_real_bo(bo)->bo_handle, timeout, &buffer_busy);
       if (r)
          fprintf(stderr, "%s: amdgpu_bo_wait_for_idle failed %i\n", __func__, r);
       return !buffer_busy;
@@ -149,10 +149,10 @@ void amdgpu_bo_destroy(struct amdgpu_winsys *ws, struct pb_buffer_lean *_buf)
       return;
    }
 
-   _mesa_hash_table_remove_key(ws->bo_export_table, bo->bo);
+   _mesa_hash_table_remove_key(ws->bo_export_table, bo->bo_handle);
 
    if (bo->b.base.placement & RADEON_DOMAIN_VRAM_GTT) {
-      amdgpu_bo_va_op(bo->bo, 0, bo->b.base.size,
+      amdgpu_bo_va_op(bo->bo_handle, 0, bo->b.base.size,
                       amdgpu_va_get_start_addr(bo->va_handle), 0, AMDGPU_VA_OP_UNMAP);
       amdgpu_va_range_free(bo->va_handle);
    }
@@ -165,7 +165,7 @@ void amdgpu_bo_destroy(struct amdgpu_winsys *ws, struct pb_buffer_lean *_buf)
    }
    assert(bo->is_user_ptr || bo->map_count == 0);
 
-   amdgpu_bo_free(bo->bo);
+   amdgpu_bo_free(bo->bo_handle);
 
 #if DEBUG
    if (ws->debug_all_bos) {
@@ -230,11 +230,11 @@ static bool amdgpu_bo_do_map(struct radeon_winsys *rws, struct amdgpu_bo_real *b
 
    assert(!bo->is_user_ptr);
 
-   int r = amdgpu_bo_cpu_map(bo->bo, cpu);
+   int r = amdgpu_bo_cpu_map(bo->bo_handle, cpu);
    if (r) {
       /* Clean up buffer managers and try again. */
       amdgpu_clean_up_buffer_managers(ws);
-      r = amdgpu_bo_cpu_map(bo->bo, cpu);
+      r = amdgpu_bo_cpu_map(bo->bo_handle, cpu);
       if (r)
          return false;
    }
@@ -407,7 +407,7 @@ void amdgpu_bo_unmap(struct radeon_winsys *rws, struct pb_buffer_lean *buf)
       ws->num_mapped_buffers--;
    }
 
-   amdgpu_bo_cpu_unmap(real->bo);
+   amdgpu_bo_cpu_unmap(real->bo_handle);
 }
 
 static void amdgpu_add_buffer_to_global_list(struct amdgpu_winsys *ws, struct amdgpu_bo_real *bo)
@@ -574,7 +574,7 @@ static struct amdgpu_winsys_bo *amdgpu_create_bo(struct amdgpu_winsys *ws,
    bo->b.base.usage = flags;
    bo->b.base.size = size;
    bo->b.unique_id = __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
-   bo->bo = buf_handle;
+   bo->bo_handle = buf_handle;
    bo->va_handle = va_handle;
 
    if (initial_domain & RADEON_DOMAIN_VRAM)
@@ -582,7 +582,7 @@ static struct amdgpu_winsys_bo *amdgpu_create_bo(struct amdgpu_winsys *ws,
    else if (initial_domain & RADEON_DOMAIN_GTT)
       ws->allocated_gtt += align64(size, ws->info.gart_page_size);
 
-   amdgpu_bo_export(bo->bo, amdgpu_bo_handle_type_kms, &bo->kms_handle);
+   amdgpu_bo_export(bo->bo_handle, amdgpu_bo_handle_type_kms, &bo->kms_handle);
    amdgpu_add_buffer_to_global_list(ws, bo);
 
    return &bo->b;
@@ -1138,7 +1138,7 @@ amdgpu_bo_sparse_commit(struct radeon_winsys *rws, struct pb_buffer_lean *buf,
                goto out;
             }
 
-            r = amdgpu_bo_va_op_raw(ws->dev, backing->bo->bo,
+            r = amdgpu_bo_va_op_raw(ws->dev, backing->bo->bo_handle,
                                     (uint64_t)backing_start * RADEON_SPARSE_PAGE_SIZE,
                                     (uint64_t)backing_size * RADEON_SPARSE_PAGE_SIZE,
                                     amdgpu_va_get_start_addr(bo->va_handle) +
@@ -1280,7 +1280,7 @@ static void amdgpu_buffer_get_metadata(struct radeon_winsys *rws,
    struct amdgpu_bo_info info = {0};
    int r;
 
-   r = amdgpu_bo_query_info(bo->bo, &info);
+   r = amdgpu_bo_query_info(bo->bo_handle, &info);
    if (r)
       return;
 
@@ -1305,7 +1305,7 @@ static void amdgpu_buffer_set_metadata(struct radeon_winsys *rws,
    metadata.size_metadata = md->size_metadata;
    memcpy(metadata.umd_metadata, md->metadata, sizeof(md->metadata));
 
-   amdgpu_bo_set_metadata(bo->bo, &metadata);
+   amdgpu_bo_set_metadata(bo->bo_handle, &metadata);
 }
 
 struct pb_buffer_lean *
@@ -1549,7 +1549,7 @@ static struct pb_buffer_lean *amdgpu_bo_from_handle(struct radeon_winsys *rws,
    bo->b.type = AMDGPU_BO_REAL;
    bo->b.unique_id = __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
    simple_mtx_init(&bo->lock, mtx_plain);
-   bo->bo = result.buf_handle;
+   bo->bo_handle = result.buf_handle;
    bo->va_handle = va_handle;
    bo->is_shared = true;
 
@@ -1558,11 +1558,11 @@ static struct pb_buffer_lean *amdgpu_bo_from_handle(struct radeon_winsys *rws,
    else if (bo->b.base.placement & RADEON_DOMAIN_GTT)
       ws->allocated_gtt += align64(bo->b.base.size, ws->info.gart_page_size);
 
-   amdgpu_bo_export(bo->bo, amdgpu_bo_handle_type_kms, &bo->kms_handle);
+   amdgpu_bo_export(bo->bo_handle, amdgpu_bo_handle_type_kms, &bo->kms_handle);
 
    amdgpu_add_buffer_to_global_list(ws, bo);
 
-   _mesa_hash_table_insert(ws->bo_export_table, bo->bo, bo);
+   _mesa_hash_table_insert(ws->bo_export_table, bo->bo_handle, bo);
    simple_mtx_unlock(&ws->bo_export_table_lock);
 
    return &bo->b.base;
@@ -1625,7 +1625,7 @@ static bool amdgpu_bo_get_handle(struct radeon_winsys *rws,
       return false;
    }
 
-   r = amdgpu_bo_export(bo->bo, type, &whandle->handle);
+   r = amdgpu_bo_export(bo->bo_handle, type, &whandle->handle);
    if (r)
       return false;
 
@@ -1656,7 +1656,7 @@ static bool amdgpu_bo_get_handle(struct radeon_winsys *rws,
 
  hash_table_set:
    simple_mtx_lock(&ws->bo_export_table_lock);
-   _mesa_hash_table_insert(ws->bo_export_table, bo->bo, bo);
+   _mesa_hash_table_insert(ws->bo_export_table, bo->bo_handle, bo);
    simple_mtx_unlock(&ws->bo_export_table_lock);
 
    bo->is_shared = true;
@@ -1702,7 +1702,7 @@ static struct pb_buffer_lean *amdgpu_bo_from_ptr(struct radeon_winsys *rws,
     bo->b.type = AMDGPU_BO_REAL;
     bo->b.unique_id = __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
     simple_mtx_init(&bo->lock, mtx_plain);
-    bo->bo = buf_handle;
+    bo->bo_handle = buf_handle;
     bo->cpu_ptr = pointer;
     bo->va_handle = va_handle;
 
@@ -1710,7 +1710,7 @@ static struct pb_buffer_lean *amdgpu_bo_from_ptr(struct radeon_winsys *rws,
 
     amdgpu_add_buffer_to_global_list(ws, bo);
 
-    amdgpu_bo_export(bo->bo, amdgpu_bo_handle_type_kms, &bo->kms_handle);
+    amdgpu_bo_export(bo->bo_handle, amdgpu_bo_handle_type_kms, &bo->kms_handle);
 
     return (struct pb_buffer_lean*)bo;
 
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
index 50e311233d084..e80ce9afe4341 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
@@ -71,7 +71,7 @@ struct amdgpu_winsys_bo {
 struct amdgpu_bo_real {
    struct amdgpu_winsys_bo b;
 
-   amdgpu_bo_handle bo;
+   amdgpu_bo_handle bo_handle;
    amdgpu_va_handle va_handle;
    void *cpu_ptr; /* for user_ptr and permanent maps */
    int map_count;
-- 
GitLab


From 89c379f7f01a0c6ed13a1e594cfaabd4aacead99 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Fri, 15 Dec 2023 07:40:17 -0500
Subject: [PATCH 29/36] winsys/amdgpu: rename amdgpu_bo_sparse::lock ->
 commit_lock

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c | 16 ++++++++--------
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.h |  2 +-
 src/gallium/winsys/amdgpu/drm/amdgpu_cs.c |  6 +++---
 3 files changed, 12 insertions(+), 12 deletions(-)

diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index df1a13f8b4ca4..520b458fc8d20 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -1016,7 +1016,7 @@ static void amdgpu_bo_sparse_destroy(struct radeon_winsys *rws, struct pb_buffer
 
    amdgpu_va_range_free(bo->va_handle);
    FREE(bo->commitments);
-   simple_mtx_destroy(&bo->lock);
+   simple_mtx_destroy(&bo->commit_lock);
    FREE(bo);
 }
 
@@ -1041,7 +1041,7 @@ amdgpu_bo_sparse_create(struct amdgpu_winsys *ws, uint64_t size,
    if (!bo)
       return NULL;
 
-   simple_mtx_init(&bo->lock, mtx_plain);
+   simple_mtx_init(&bo->commit_lock, mtx_plain);
    pipe_reference_init(&bo->b.base.reference, 1);
    bo->b.base.placement = domain;
    bo->b.base.alignment_log2 = util_logbase2(RADEON_SPARSE_PAGE_SIZE);
@@ -1080,7 +1080,7 @@ error_va_map:
 error_va_alloc:
    FREE(bo->commitments);
 error_alloc_commitments:
-   simple_mtx_destroy(&bo->lock);
+   simple_mtx_destroy(&bo->commit_lock);
    FREE(bo);
    return NULL;
 }
@@ -1105,7 +1105,7 @@ amdgpu_bo_sparse_commit(struct radeon_winsys *rws, struct pb_buffer_lean *buf,
    va_page = offset / RADEON_SPARSE_PAGE_SIZE;
    end_va_page = va_page + DIV_ROUND_UP(size, RADEON_SPARSE_PAGE_SIZE);
 
-   simple_mtx_lock(&bo->lock);
+   simple_mtx_lock(&bo->commit_lock);
 
 #if DEBUG_SPARSE_COMMITS
    sparse_dump(bo, __func__);
@@ -1211,7 +1211,7 @@ amdgpu_bo_sparse_commit(struct radeon_winsys *rws, struct pb_buffer_lean *buf,
    }
 out:
 
-   simple_mtx_unlock(&bo->lock);
+   simple_mtx_unlock(&bo->commit_lock);
 
    return ok;
 }
@@ -1236,7 +1236,7 @@ amdgpu_bo_find_next_committed_memory(struct pb_buffer_lean *buf,
    start_va_page = va_page = range_offset / RADEON_SPARSE_PAGE_SIZE;
    end_va_page = (*range_size + range_offset) / RADEON_SPARSE_PAGE_SIZE;
 
-   simple_mtx_lock(&bo->lock);
+   simple_mtx_lock(&bo->commit_lock);
    /* Lookup the first committed page with backing physical storage */
    while (va_page < end_va_page && !comm[va_page].backing)
       va_page++;
@@ -1245,7 +1245,7 @@ amdgpu_bo_find_next_committed_memory(struct pb_buffer_lean *buf,
    if (va_page == end_va_page && !comm[va_page].backing) {
       uncommitted_range_prev = *range_size;
       *range_size = 0;
-      simple_mtx_unlock(&bo->lock);
+      simple_mtx_unlock(&bo->commit_lock);
       return uncommitted_range_prev;
    }
 
@@ -1253,7 +1253,7 @@ amdgpu_bo_find_next_committed_memory(struct pb_buffer_lean *buf,
    span_va_page = va_page;
    while (va_page < end_va_page && comm[va_page].backing)
       va_page++;
-   simple_mtx_unlock(&bo->lock);
+   simple_mtx_unlock(&bo->commit_lock);
 
    /* Calc byte count that need to skip before committed range */
    if (span_va_page != start_va_page)
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
index e80ce9afe4341..5ab91c80856fb 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
@@ -106,7 +106,7 @@ struct amdgpu_bo_sparse {
 
    uint32_t num_va_pages;
    uint32_t num_backing_pages;
-   simple_mtx_t lock;
+   simple_mtx_t commit_lock;
 
    struct list_head backing;
 
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
index 74cdc4a29a447..60cdff7755a99 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
@@ -1281,7 +1281,7 @@ static bool amdgpu_add_sparse_backing_buffers(struct amdgpu_cs_context *cs)
       struct amdgpu_cs_buffer *buffer = &cs->buffer_lists[AMDGPU_BO_SPARSE].buffers[i];
       struct amdgpu_bo_sparse *bo = get_sparse_bo(buffer->bo);
 
-      simple_mtx_lock(&bo->lock);
+      simple_mtx_lock(&bo->commit_lock);
 
       list_for_each_entry(struct amdgpu_sparse_backing, backing, &bo->backing, list) {
          /* We can directly add the buffer here, because we know that each
@@ -1291,14 +1291,14 @@ static bool amdgpu_add_sparse_backing_buffers(struct amdgpu_cs_context *cs)
             amdgpu_do_add_buffer(cs, &backing->bo->b, &cs->buffer_lists[AMDGPU_BO_REAL]);
          if (!real_buffer) {
             fprintf(stderr, "%s: failed to add buffer\n", __func__);
-            simple_mtx_unlock(&bo->lock);
+            simple_mtx_unlock(&bo->commit_lock);
             return false;
          }
 
          real_buffer->usage = buffer->usage;
       }
 
-      simple_mtx_unlock(&bo->lock);
+      simple_mtx_unlock(&bo->commit_lock);
    }
 
    return true;
-- 
GitLab


From 4201840af8f9f59feca92b5da679d19d359ba8c6 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Fri, 15 Dec 2023 07:42:34 -0500
Subject: [PATCH 30/36] winsys/amdgpu: rename amdgpu_bo_real::lock to map_lock

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.c | 14 +++++++-------
 src/gallium/winsys/amdgpu/drm/amdgpu_bo.h |  2 +-
 2 files changed, 8 insertions(+), 8 deletions(-)

diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
index 520b458fc8d20..fea6026500fc8 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.c
@@ -201,7 +201,7 @@ void amdgpu_bo_destroy(struct amdgpu_winsys *ws, struct pb_buffer_lean *_buf)
    else if (bo->b.base.placement & RADEON_DOMAIN_GTT)
       ws->allocated_gtt -= align64(bo->b.base.size, ws->info.gart_page_size);
 
-   simple_mtx_destroy(&bo->lock);
+   simple_mtx_destroy(&bo->map_lock);
    FREE(bo);
 }
 
@@ -364,18 +364,18 @@ void *amdgpu_bo_map(struct radeon_winsys *rws,
    } else {
       cpu = p_atomic_read(&real->cpu_ptr);
       if (!cpu) {
-         simple_mtx_lock(&real->lock);
+         simple_mtx_lock(&real->map_lock);
          /* Must re-check due to the possibility of a race. Re-check need not
           * be atomic thanks to the lock. */
          cpu = real->cpu_ptr;
          if (!cpu) {
             if (!amdgpu_bo_do_map(rws, real, &cpu)) {
-               simple_mtx_unlock(&real->lock);
+               simple_mtx_unlock(&real->map_lock);
                return NULL;
             }
             p_atomic_set(&real->cpu_ptr, cpu);
          }
-         simple_mtx_unlock(&real->lock);
+         simple_mtx_unlock(&real->map_lock);
       }
    }
 
@@ -567,7 +567,7 @@ static struct amdgpu_winsys_bo *amdgpu_create_bo(struct amdgpu_winsys *ws,
          goto error_va_map;
    }
 
-   simple_mtx_init(&bo->lock, mtx_plain);
+   simple_mtx_init(&bo->map_lock, mtx_plain);
    pipe_reference_init(&bo->b.base.reference, 1);
    bo->b.base.placement = initial_domain;
    bo->b.base.alignment_log2 = util_logbase2(alignment);
@@ -1548,7 +1548,7 @@ static struct pb_buffer_lean *amdgpu_bo_from_handle(struct radeon_winsys *rws,
    bo->b.base.size = result.alloc_size;
    bo->b.type = AMDGPU_BO_REAL;
    bo->b.unique_id = __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
-   simple_mtx_init(&bo->lock, mtx_plain);
+   simple_mtx_init(&bo->map_lock, mtx_plain);
    bo->bo_handle = result.buf_handle;
    bo->va_handle = va_handle;
    bo->is_shared = true;
@@ -1701,7 +1701,7 @@ static struct pb_buffer_lean *amdgpu_bo_from_ptr(struct radeon_winsys *rws,
     bo->b.base.size = size;
     bo->b.type = AMDGPU_BO_REAL;
     bo->b.unique_id = __sync_fetch_and_add(&ws->next_bo_unique_id, 1);
-    simple_mtx_init(&bo->lock, mtx_plain);
+    simple_mtx_init(&bo->map_lock, mtx_plain);
     bo->bo_handle = buf_handle;
     bo->cpu_ptr = pointer;
     bo->va_handle = va_handle;
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
index 5ab91c80856fb..4da7b20fa7253 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_bo.h
@@ -79,7 +79,7 @@ struct amdgpu_bo_real {
 #if DEBUG
    struct list_head global_list_item;
 #endif
-   simple_mtx_t lock;
+   simple_mtx_t map_lock;
 
    bool is_user_ptr;
 
-- 
GitLab


From 1badc97672951dc869e47f5c8186776307352de0 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Thu, 21 Dec 2023 01:24:49 -0500
Subject: [PATCH 31/36] winsys/amdgpu: remove dependency_flags parameter from
 cs_add_fence_dependency

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/drivers/radeonsi/si_fence.c       | 2 +-
 src/gallium/include/winsys/radeon_winsys.h    | 5 +----
 src/gallium/winsys/amdgpu/drm/amdgpu_cs.c     | 3 +--
 src/gallium/winsys/radeon/drm/radeon_drm_cs.c | 3 +--
 4 files changed, 4 insertions(+), 9 deletions(-)

diff --git a/src/gallium/drivers/radeonsi/si_fence.c b/src/gallium/drivers/radeonsi/si_fence.c
index 3913737e3130a..cb0035a8316d8 100644
--- a/src/gallium/drivers/radeonsi/si_fence.c
+++ b/src/gallium/drivers/radeonsi/si_fence.c
@@ -170,7 +170,7 @@ static void si_add_fence_dependency(struct si_context *sctx, struct pipe_fence_h
 {
    struct radeon_winsys *ws = sctx->ws;
 
-   ws->cs_add_fence_dependency(&sctx->gfx_cs, fence, 0);
+   ws->cs_add_fence_dependency(&sctx->gfx_cs, fence);
 }
 
 static void si_add_syncobj_signal(struct si_context *sctx, struct pipe_fence_handle *fence)
diff --git a/src/gallium/include/winsys/radeon_winsys.h b/src/gallium/include/winsys/radeon_winsys.h
index 7c61be2c3c762..22d10cbe37029 100644
--- a/src/gallium/include/winsys/radeon_winsys.h
+++ b/src/gallium/include/winsys/radeon_winsys.h
@@ -675,11 +675,8 @@ struct radeon_winsys {
    /**
     * Add a fence dependency to the CS, so that the CS will wait for
     * the fence before execution.
-    *
-    * \param dependency_flags  Bitmask of RADEON_DEPENDENCY_*
     */
-   void (*cs_add_fence_dependency)(struct radeon_cmdbuf *cs, struct pipe_fence_handle *fence,
-                                   unsigned dependency_flags);
+   void (*cs_add_fence_dependency)(struct radeon_cmdbuf *cs, struct pipe_fence_handle *fence);
 
    /**
     * Signal a syncobj when the CS finishes execution.
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
index 60cdff7755a99..94e809f77494e 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
@@ -1211,8 +1211,7 @@ static void add_fence_to_list(struct amdgpu_fence_list *fences,
 }
 
 static void amdgpu_cs_add_fence_dependency(struct radeon_cmdbuf *rws,
-                                           struct pipe_fence_handle *pfence,
-                                           unsigned dependency_flags)
+                                           struct pipe_fence_handle *pfence)
 {
    struct amdgpu_cs *acs = amdgpu_cs(rws);
    struct amdgpu_cs_context *cs = acs->csc;
diff --git a/src/gallium/winsys/radeon/drm/radeon_drm_cs.c b/src/gallium/winsys/radeon/drm/radeon_drm_cs.c
index 0d5bd68b12e2f..243a819c58a25 100644
--- a/src/gallium/winsys/radeon/drm/radeon_drm_cs.c
+++ b/src/gallium/winsys/radeon/drm/radeon_drm_cs.c
@@ -847,8 +847,7 @@ static struct pipe_fence_handle *radeon_drm_cs_get_next_fence(struct radeon_cmdb
 
 static void
 radeon_drm_cs_add_fence_dependency(struct radeon_cmdbuf *cs,
-                                   struct pipe_fence_handle *fence,
-                                   unsigned dependency_flags)
+                                   struct pipe_fence_handle *fence)
 {
    /* TODO: Handle the following unlikely multi-threaded scenario:
     *
-- 
GitLab


From 22e245adffb6ac25643101858fc9d3ef6ddcbcc2 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Thu, 21 Dec 2023 02:33:27 -0500
Subject: [PATCH 32/36] winsys/amdgpu: implement explicit fence dependencies as
 sequence numbers

This eliminates redundant fence dependencies if BOs add the same ones.

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/winsys/amdgpu/drm/amdgpu_cs.c | 32 +++++++++++++----------
 src/gallium/winsys/amdgpu/drm/amdgpu_cs.h |  3 +++
 2 files changed, 21 insertions(+), 14 deletions(-)

diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
index 94e809f77494e..aaac9b01a75d6 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
@@ -16,17 +16,19 @@
 /* FENCES */
 
 static struct pipe_fence_handle *
-amdgpu_fence_create(struct amdgpu_ctx *ctx, unsigned ip_type)
+amdgpu_fence_create(struct amdgpu_cs *cs)
 {
    struct amdgpu_fence *fence = CALLOC_STRUCT(amdgpu_fence);
+   struct amdgpu_ctx *ctx = cs->ctx;
 
    fence->reference.count = 1;
    fence->ws = ctx->ws;
    fence->ctx = ctx;
    fence->fence.context = ctx->ctx;
-   fence->fence.ip_type = ip_type;
+   fence->fence.ip_type = cs->ip_type;
    util_queue_fence_init(&fence->submitted);
    util_queue_fence_reset(&fence->submitted);
+   fence->queue_index = cs->queue_index;
    p_atomic_inc(&ctx->refcount);
    return (struct pipe_fence_handle *)fence;
 }
@@ -244,8 +246,7 @@ amdgpu_cs_get_next_fence(struct radeon_cmdbuf *rcs)
       return fence;
    }
 
-   fence = amdgpu_fence_create(cs->ctx,
-                               cs->csc->chunk_ib[IB_MAIN].ip_type);
+   fence = amdgpu_fence_create(cs);
    if (!fence)
       return NULL;
 
@@ -925,6 +926,7 @@ static void amdgpu_cs_context_cleanup(struct amdgpu_winsys *ws, struct amdgpu_cs
       cs->buffer_lists[i].num_buffers = 0;
    }
 
+   cs->seq_no_dependencies.valid_fence_mask = 0;
    cleanup_fence_list(&cs->fence_dependencies);
    cleanup_fence_list(&cs->syncobj_dependencies);
    cleanup_fence_list(&cs->syncobj_to_signal);
@@ -1210,20 +1212,22 @@ static void add_fence_to_list(struct amdgpu_fence_list *fences,
    amdgpu_fence_reference(&fences->list[idx], (struct pipe_fence_handle*)fence);
 }
 
-static void amdgpu_cs_add_fence_dependency(struct radeon_cmdbuf *rws,
+static void amdgpu_cs_add_fence_dependency(struct radeon_cmdbuf *rcs,
                                            struct pipe_fence_handle *pfence)
 {
-   struct amdgpu_cs *acs = amdgpu_cs(rws);
+   struct amdgpu_cs *acs = amdgpu_cs(rcs);
    struct amdgpu_cs_context *cs = acs->csc;
    struct amdgpu_fence *fence = (struct amdgpu_fence*)pfence;
 
    util_queue_fence_wait(&fence->submitted);
 
-   /* Ignore non-imported idle fences. This will only check the user fence in memory. */
-   if (!fence->imported && amdgpu_fence_wait((void *)fence, 0, false))
-      return;
-
-   if (amdgpu_fence_is_syncobj(fence))
+   if (!fence->imported) {
+      /* Ignore idle fences. This will only check the user fence in memory. */
+      if (!amdgpu_fence_wait((void *)fence, 0, false)) {
+         add_seq_no_to_list(acs->ws, &cs->seq_no_dependencies, fence->queue_index,
+                            fence->queue_seq_no);
+      }
+   } else if (amdgpu_fence_is_syncobj(fence))
       add_fence_to_list(&cs->syncobj_dependencies, fence);
    else
       add_fence_to_list(&cs->fence_dependencies, fence);
@@ -1351,7 +1355,7 @@ static void amdgpu_cs_submit_ib(void *job, void *gdata, int thread_index)
     * sequence number per queue and removes all older ones.
     */
    struct amdgpu_seq_no_fences seq_no_dependencies;
-   seq_no_dependencies.valid_fence_mask = 0;
+   memcpy(&seq_no_dependencies, &cs->seq_no_dependencies, sizeof(seq_no_dependencies));
 
    /* Add a fence dependency on the previous IB if the IP has multiple physical queues to
     * make it appear as if it had only 1 queue, or if the previous IB comes from a different
@@ -1419,6 +1423,7 @@ static void amdgpu_cs_submit_ib(void *job, void *gdata, int thread_index)
    /* Finally, add the IB fence into the winsys queue. */
    amdgpu_fence_reference(&queue->fences[next_seq_no % AMDGPU_FENCE_RING_SIZE], cs->fence);
    queue->latest_seq_no = next_seq_no;
+   ((struct amdgpu_fence*)cs->fence)->queue_seq_no = next_seq_no;
    simple_mtx_unlock(&ws->bo_fence_lock);
 
    struct drm_amdgpu_bo_list_entry *bo_list = NULL;
@@ -1745,8 +1750,7 @@ static int amdgpu_cs_flush(struct radeon_cmdbuf *rcs,
          cur->fence = cs->next_fence;
          cs->next_fence = NULL;
       } else {
-         cur->fence = amdgpu_fence_create(cs->ctx,
-                                          cur->chunk_ib[IB_MAIN].ip_type);
+         cur->fence = amdgpu_fence_create(cs);
       }
       if (fence)
          amdgpu_fence_reference(fence, cur->fence);
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h
index 5b505af00bce9..044b3d8888aef 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h
@@ -93,6 +93,7 @@ struct amdgpu_cs_context {
    struct amdgpu_winsys_bo     *last_added_bo;
    unsigned                    last_added_bo_usage;
 
+   struct amdgpu_seq_no_fences seq_no_dependencies;
    struct amdgpu_fence_list    fence_dependencies;
    struct amdgpu_fence_list    syncobj_dependencies;
    struct amdgpu_fence_list    syncobj_to_signal;
@@ -168,6 +169,8 @@ struct amdgpu_fence {
 
    volatile int signalled;              /* bool (int for atomicity) */
    bool imported;
+   uint8_t queue_index;       /* for non-imported fences */
+   uint_seq_no queue_seq_no;  /* winsys-generated sequence number */
 };
 
 static inline bool amdgpu_fence_is_syncobj(struct amdgpu_fence *fence)
-- 
GitLab


From 3020f36fb8b4dabcbff0a27a6f6c5a7c165f8cbe Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Thu, 21 Dec 2023 03:05:17 -0500
Subject: [PATCH 33/36] winsys/amdgpu: use pipe_reference for amdgpu_ctx
 refcounting

this is the standard utility for refcounting

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/winsys/amdgpu/drm/amdgpu_cs.c |  9 +++++----
 src/gallium/winsys/amdgpu/drm/amdgpu_cs.h | 18 +++++++++++-------
 2 files changed, 16 insertions(+), 11 deletions(-)

diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
index aaac9b01a75d6..883374a744fc9 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
@@ -23,13 +23,12 @@ amdgpu_fence_create(struct amdgpu_cs *cs)
 
    fence->reference.count = 1;
    fence->ws = ctx->ws;
-   fence->ctx = ctx;
+   amdgpu_ctx_reference(&fence->ctx, ctx);
    fence->fence.context = ctx->ctx;
    fence->fence.ip_type = cs->ip_type;
    util_queue_fence_init(&fence->submitted);
    util_queue_fence_reset(&fence->submitted);
    fence->queue_index = cs->queue_index;
-   p_atomic_inc(&ctx->refcount);
    return (struct pipe_fence_handle *)fence;
 }
 
@@ -287,7 +286,7 @@ static struct radeon_winsys_ctx *amdgpu_ctx_create(struct radeon_winsys *ws,
       return NULL;
 
    ctx->ws = amdgpu_winsys(ws);
-   ctx->refcount = 1;
+   ctx->reference.count = 1;
    ctx->allow_context_lost = allow_context_lost;
 
    r = amdgpu_cs_ctx_create2(ctx->ws->dev, amdgpu_priority, &ctx->ctx);
@@ -328,7 +327,9 @@ error_create:
 
 static void amdgpu_ctx_destroy(struct radeon_winsys_ctx *rwctx)
 {
-   amdgpu_ctx_unref((struct amdgpu_ctx*)rwctx);
+   struct amdgpu_ctx *ctx = (struct amdgpu_ctx*)rwctx;
+
+   amdgpu_ctx_reference(&ctx, NULL);
 }
 
 static void amdgpu_pad_gfx_compute_ib(struct amdgpu_winsys *ws, enum amd_ip_type ip_type,
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h
index 044b3d8888aef..2476c2f7fc297 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.h
@@ -19,11 +19,11 @@
 #define IB_MAX_SUBMIT_DWORDS (20 * 1024)
 
 struct amdgpu_ctx {
+   struct pipe_reference reference;
    struct amdgpu_winsys *ws;
    amdgpu_context_handle ctx;
    amdgpu_bo_handle user_fence_bo;
    uint64_t *user_fence_cpu_address_base;
-   int refcount;
 
    /* If true, report lost contexts and skip command submission.
     * If false, terminate the process.
@@ -178,13 +178,17 @@ static inline bool amdgpu_fence_is_syncobj(struct amdgpu_fence *fence)
    return fence->ctx == NULL;
 }
 
-static inline void amdgpu_ctx_unref(struct amdgpu_ctx *ctx)
+static inline void amdgpu_ctx_reference(struct amdgpu_ctx **dst, struct amdgpu_ctx *src)
 {
-   if (p_atomic_dec_zero(&ctx->refcount)) {
-      amdgpu_cs_ctx_free(ctx->ctx);
-      amdgpu_bo_free(ctx->user_fence_bo);
-      FREE(ctx);
+   struct amdgpu_ctx *old_dst = *dst;
+
+   if (pipe_reference(old_dst ? &old_dst->reference : NULL,
+                      src ? &src->reference : NULL)) {
+      amdgpu_cs_ctx_free(old_dst->ctx);
+      amdgpu_bo_free(old_dst->user_fence_bo);
+      FREE(old_dst);
    }
+   *dst = src;
 }
 
 static inline void amdgpu_fence_reference(struct pipe_fence_handle **dst,
@@ -199,7 +203,7 @@ static inline void amdgpu_fence_reference(struct pipe_fence_handle **dst,
       if (amdgpu_fence_is_syncobj(fence))
          amdgpu_cs_destroy_syncobj(fence->ws->dev, fence->syncobj);
       else
-         amdgpu_ctx_unref(fence->ctx);
+         amdgpu_ctx_reference(&fence->ctx, NULL);
 
       util_queue_fence_destroy(&fence->submitted);
       FREE(fence);
-- 
GitLab


From 033c88c73b0f7ea8389d84c3dc13b0b802bc9011 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Thu, 21 Dec 2023 03:09:13 -0500
Subject: [PATCH 34/36] winsys/amdgpu: don't use amdgpu_fence::ctx for fence
 dependencies

The only remaining use of ctx is amdgpu_fence_is_syncobj.

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/winsys/amdgpu/drm/amdgpu_cs.c     | 5 ++++-
 src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c | 2 ++
 src/gallium/winsys/amdgpu/drm/amdgpu_winsys.h | 3 +++
 3 files changed, 9 insertions(+), 1 deletion(-)

diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
index 883374a744fc9..f0ec2b932fde0 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
@@ -1367,7 +1367,7 @@ static void amdgpu_cs_submit_ib(void *job, void *gdata, int thread_index)
    struct amdgpu_fence *prev_fence =
       (struct amdgpu_fence*)queue->fences[prev_seq_no % AMDGPU_FENCE_RING_SIZE];
 
-   if (prev_fence && (ws->info.ip[acs->ip_type].num_queues > 1 || prev_fence->ctx != acs->ctx))
+   if (prev_fence && (ws->info.ip[acs->ip_type].num_queues > 1 || queue->last_ctx != acs->ctx))
       add_seq_no_to_list(ws, &seq_no_dependencies, acs->queue_index, prev_seq_no);
 
    /* Since the kernel driver doesn't synchronize execution between different
@@ -1425,6 +1425,9 @@ static void amdgpu_cs_submit_ib(void *job, void *gdata, int thread_index)
    amdgpu_fence_reference(&queue->fences[next_seq_no % AMDGPU_FENCE_RING_SIZE], cs->fence);
    queue->latest_seq_no = next_seq_no;
    ((struct amdgpu_fence*)cs->fence)->queue_seq_no = next_seq_no;
+
+   /* Update the last used context in the queue. */
+   amdgpu_ctx_reference(&queue->last_ctx, acs->ctx);
    simple_mtx_unlock(&ws->bo_fence_lock);
 
    struct drm_amdgpu_bo_list_entry *bo_list = NULL;
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c b/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c
index 8d3ef782a251c..5673ecf228b33 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.c
@@ -75,6 +75,8 @@ static void do_winsys_deinit(struct amdgpu_winsys *ws)
    for (unsigned i = 0; i < ARRAY_SIZE(ws->queues); i++) {
       for (unsigned j = 0; j < ARRAY_SIZE(ws->queues[i].fences); j++)
          amdgpu_fence_reference(&ws->queues[i].fences[j], NULL);
+
+      amdgpu_ctx_reference(&ws->queues[i].last_ctx, NULL);
    }
 
    if (util_queue_is_initialized(&ws->cs_queue))
diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.h b/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.h
index 70564e41bbc02..f1abe14dc8cff 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.h
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_winsys.h
@@ -117,6 +117,9 @@ struct amdgpu_queue {
     * never become idle in certain very unlucky scenarios and running out of memory.
     */
    uint_seq_no latest_seq_no;
+
+   /* The last context using this queue. */
+   struct amdgpu_ctx *last_ctx;
 };
 
 /* This is part of every BO. */
-- 
GitLab


From ea46bce2498b6d8e74f775254f393e4b05370362 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Thu, 21 Dec 2023 03:28:27 -0500
Subject: [PATCH 35/36] winsys/amdgpu: simplify code using
 amdgpu_cs_context::chunk_ib

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/winsys/amdgpu/drm/amdgpu_cs.c | 78 ++++++-----------------
 1 file changed, 19 insertions(+), 59 deletions(-)

diff --git a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
index f0ec2b932fde0..0831874296c19 100644
--- a/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
+++ b/src/gallium/winsys/amdgpu/drm/amdgpu_cs.c
@@ -536,11 +536,11 @@ amdgpu_ctx_query_reset_status(struct radeon_winsys_ctx *rwctx, bool full_reset_o
 
 /* COMMAND SUBMISSION */
 
-static bool amdgpu_cs_has_user_fence(struct amdgpu_cs_context *cs)
+static bool amdgpu_cs_has_user_fence(struct amdgpu_cs *acs)
 {
-   return cs->chunk_ib[IB_MAIN].ip_type == AMD_IP_GFX ||
-          cs->chunk_ib[IB_MAIN].ip_type == AMD_IP_COMPUTE ||
-          cs->chunk_ib[IB_MAIN].ip_type == AMD_IP_SDMA;
+   return acs->ip_type == AMD_IP_GFX ||
+          acs->ip_type == AMD_IP_COMPUTE ||
+          acs->ip_type == AMD_IP_SDMA;
 }
 
 static inline unsigned amdgpu_cs_epilog_dws(struct amdgpu_cs *cs)
@@ -847,63 +847,23 @@ static bool amdgpu_init_cs_context(struct amdgpu_winsys *ws,
                                    struct amdgpu_cs_context *cs,
                                    enum amd_ip_type ip_type)
 {
-   switch (ip_type) {
-   case AMD_IP_SDMA:
-      cs->chunk_ib[IB_MAIN].ip_type = AMDGPU_HW_IP_DMA;
-      break;
-
-   case AMD_IP_UVD:
-      cs->chunk_ib[IB_MAIN].ip_type = AMDGPU_HW_IP_UVD;
-      break;
-
-   case AMD_IP_UVD_ENC:
-      cs->chunk_ib[IB_MAIN].ip_type = AMDGPU_HW_IP_UVD_ENC;
-      break;
-
-   case AMD_IP_VCE:
-      cs->chunk_ib[IB_MAIN].ip_type = AMDGPU_HW_IP_VCE;
-      break;
-
-   case AMD_IP_VCN_DEC:
-      cs->chunk_ib[IB_MAIN].ip_type = AMDGPU_HW_IP_VCN_DEC;
-      break;
-
-   case AMD_IP_VCN_ENC:
-      cs->chunk_ib[IB_MAIN].ip_type = AMDGPU_HW_IP_VCN_ENC;
-      break;
-
-   case AMD_IP_VCN_JPEG:
-      cs->chunk_ib[IB_MAIN].ip_type = AMDGPU_HW_IP_VCN_JPEG;
-      break;
-
-   case AMD_IP_VPE:
-      cs->chunk_ib[IB_MAIN].ip_type = AMDGPU_HW_IP_VPE;
-      break;
-
-   case AMD_IP_COMPUTE:
-   case AMD_IP_GFX:
-      cs->chunk_ib[IB_MAIN].ip_type = ip_type == AMD_IP_GFX ? AMDGPU_HW_IP_GFX :
-                                                              AMDGPU_HW_IP_COMPUTE;
-
-      /* The kernel shouldn't invalidate L2 and vL1. The proper place for cache
-       * invalidation is the beginning of IBs (the previous commit does that),
-       * because completion of an IB doesn't care about the state of GPU caches,
-       * but the beginning of an IB does. Draw calls from multiple IBs can be
-       * executed in parallel, so draw calls from the current IB can finish after
-       * the next IB starts drawing, and so the cache flush at the end of IB
-       * is always late.
-       */
-      cs->chunk_ib[IB_PREAMBLE].flags = AMDGPU_IB_FLAG_TC_WB_NOT_INVALIDATE;
-      cs->chunk_ib[IB_MAIN].flags = AMDGPU_IB_FLAG_TC_WB_NOT_INVALIDATE;
-      break;
-
-   default:
-      assert(0);
+   for (unsigned i = 0; i < ARRAY_SIZE(cs->chunk_ib); i++) {
+      cs->chunk_ib[i].ip_type = ip_type;
+      cs->chunk_ib[i].flags = 0;
+
+      if (ip_type == AMD_IP_GFX || ip_type == AMD_IP_COMPUTE) {
+         /* The kernel shouldn't invalidate L2 and vL1. The proper place for cache invalidation
+          * is the beginning of IBs because completion of an IB doesn't care about the state of
+          * GPU caches, only the beginning of an IB does. Draw calls from multiple IBs can be
+          * executed in parallel, so draw calls from the current IB can finish after the next IB
+          * starts drawing, and so the cache flush at the end of IBs is usually late and thus
+          * useless.
+          */
+         cs->chunk_ib[i].flags |= AMDGPU_IB_FLAG_TC_WB_NOT_INVALIDATE;
+      }
    }
 
    cs->chunk_ib[IB_PREAMBLE].flags |= AMDGPU_IB_FLAG_PREAMBLE;
-   cs->chunk_ib[IB_PREAMBLE].ip_type = cs->chunk_ib[IB_MAIN].ip_type;
-
    cs->last_added_bo = NULL;
    return true;
 }
@@ -1315,7 +1275,7 @@ static void amdgpu_cs_submit_ib(void *job, void *gdata, int thread_index)
    struct amdgpu_cs_context *cs = acs->cst;
    int i, r;
    uint64_t seq_no = 0;
-   bool has_user_fence = amdgpu_cs_has_user_fence(cs);
+   bool has_user_fence = amdgpu_cs_has_user_fence(acs);
 
    simple_mtx_lock(&ws->bo_fence_lock);
    struct amdgpu_queue *queue = &ws->queues[acs->queue_index];
-- 
GitLab
