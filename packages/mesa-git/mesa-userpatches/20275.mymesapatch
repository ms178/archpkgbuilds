From 8e5325c27ea357da5f9d5507f8ca20dd9a95f160 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Tue, 13 Dec 2022 19:13:35 -0500
Subject: [PATCH 2/5] util: add util_bitcount(64)_default

to make it reusable in follow-up commits where the behavior of
util_bitcount(64) will be changed.
---
 src/util/bitscan.h | 32 ++++++++++++++++++++++----------
 1 file changed, 22 insertions(+), 10 deletions(-)

diff --git a/src/util/bitscan.h b/src/util/bitscan.h
index 53cbb91e98c9..f0c283019762 100644
--- a/src/util/bitscan.h
+++ b/src/util/bitscan.h
@@ -294,11 +294,8 @@ u_bit_consecutive64(unsigned start, unsigned count)
    return (((uint64_t)1 << count) - 1) << start;
 }
 
-/**
- * Return number of bits set in n.
- */
 static inline unsigned
-util_bitcount(unsigned n)
+util_bitcount_default(unsigned n)
 {
 #if defined(HAVE___BUILTIN_POPCOUNT)
    return __builtin_popcount(n);
@@ -317,6 +314,16 @@ util_bitcount(unsigned n)
 #endif
 }
 
+static inline unsigned
+util_bitcount64_default(unsigned n)
+{
+#ifdef HAVE___BUILTIN_POPCOUNTLL
+   return __builtin_popcountll(n);
+#else
+   return util_bitcount_default(n) + util_bitcount_default(n >> 32);
+#endif
+}
+
 /**
  * Return the number of bits set in n using the native popcnt instruction.
  * The caller is responsible for ensuring that popcnt is supported by the CPU.
@@ -333,18 +340,23 @@ util_popcnt_inline_asm(unsigned n)
    return out;
 #else
    /* We should never get here by accident, but I'm sure it'll happen. */
-   return util_bitcount(n);
+   return util_bitcount_default(n);
 #endif
 }
 
+/**
+ * Return number of bits set in n.
+ */
+static inline unsigned
+util_bitcount(unsigned n)
+{
+   return util_bitcount_default(n);
+}
+
 static inline unsigned
 util_bitcount64(uint64_t n)
 {
-#ifdef HAVE___BUILTIN_POPCOUNTLL
-   return __builtin_popcountll(n);
-#else
-   return util_bitcount(n) + util_bitcount(n >> 32);
-#endif
+   return util_bitcount64_default(n);
 }
 
 /**
-- 
GitLab


From ff972ee5ed421018204bcfb5bd841489dfe38b23 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Tue, 13 Dec 2022 17:17:04 -0500
Subject: [PATCH 3/5] util: add all variants of util_bitcount_native using
 POPCNT on x86

split from a bigger commit

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
Reviewed-by: Erik Faye-Lund <erik.faye-lund@collabora.com>
---
 src/util/bitscan.h | 44 ++++++++++++++++++++++++++++++++++++++------
 1 file changed, 38 insertions(+), 6 deletions(-)

diff --git a/src/util/bitscan.h b/src/util/bitscan.h
index f0c283019762..c3ae88951581 100644
--- a/src/util/bitscan.h
+++ b/src/util/bitscan.h
@@ -42,6 +42,9 @@
 #include <popcntintrin.h>
 #endif
 
+#include "detect_arch.h"
+#include "u_cpu_detect.h"
+
 #ifdef __cplusplus
 extern "C" {
 #endif
@@ -328,22 +331,51 @@ util_bitcount64_default(unsigned n)
  * Return the number of bits set in n using the native popcnt instruction.
  * The caller is responsible for ensuring that popcnt is supported by the CPU.
  *
- * gcc doesn't use it if -mpopcnt or -march= that has popcnt is missing.
- *
+ * The gcc builtin doesn't use it if -mpopcnt is missing or -march= with
+ * popcnt is missing.
  */
 static inline unsigned
-util_popcnt_inline_asm(unsigned n)
+util_bitcount_native(unsigned n)
 {
 #if defined(USE_X86_64_ASM) || defined(USE_X86_ASM)
    uint32_t out;
-   __asm volatile("popcnt %1, %0" : "=r"(out) : "r"(n));
+   /* popcnt = instruction, l = 32bit, %1 = src (n), %0 = dst (out) */
+   /* '=' = write only, no prefix = read only, r = use any register */
+   __asm volatile("popcntl %1, %0" : "=r"(out) : "r"(n));
    return out;
+#elif defined(_MSC_VER) && (DETECT_ARCH_X86_64 || DETECT_ARCH_X86)
+   return __popcnt(n);
 #else
-   /* We should never get here by accident, but I'm sure it'll happen. */
+   /* Some code only checks has_popcnt and not definitions, so we need this. */
    return util_bitcount_default(n);
 #endif
 }
 
+static inline unsigned
+util_bitcount64_native(uint64_t n)
+{
+#if defined(USE_X86_64_ASM)
+   uint64_t out;
+   /* popcnt = instruction, q = 64bit, %1 = src (n), %0 = dst (out) */
+   /* '=' = write only, no prefix = read only, r = use any register */
+   __asm volatile("popcntq %1, %0" : "=r"(out) : "r"(n));
+   return out;
+#elif defined(USE_X86_ASM)
+   /* 64-bit popcnt can't be used on i386. Use 32-bit popcnt instead. */
+   uint32_t out0, out1, in0 = n, in1 = n >> 32;
+   __asm volatile("popcntl %1, %0" : "=r"(out0) : "r"(in0));
+   __asm volatile("popcntl %1, %0" : "=r"(out1) : "r"(in1));
+   return out0 + out1;
+#elif defined(_MSC_VER) && DETECT_ARCH_X86_64
+   return __popcnt64(n);
+#elif defined(_MSC_VER) && DETECT_ARCH_X86
+   return __popcnt(n) + __popcnt(n >> 32);
+#else
+   /* Some code only checks has_popcnt and not definitions, so we need this. */
+   return util_bitcount64_default(n);
+#endif
+}
+
 /**
  * Return number of bits set in n.
  */
@@ -397,7 +429,7 @@ template<util_popcnt POPCNT> inline unsigned
 util_bitcount_fast(unsigned n)
 {
    if (POPCNT == POPCNT_YES)
-      return util_popcnt_inline_asm(n);
+      return util_bitcount_native(n);
    else
       return util_bitcount(n);
 }
-- 
GitLab


From 3eb083e0888b86c38c972ba46e0f8e6087b178ae Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sun, 11 Dec 2022 21:45:14 -0500
Subject: [PATCH 4/5] util: always use POPCNT in util_bitcount if it's
 supported

The native popcnt instruction is used on x86 if it's available.

_mesa_marshal_DrawElements uses util_bitcount once to bind uploaded user
buffers. The CPU time spent in there is decreased from 11% to 10.6%.

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
Reviewed-by: Erik Faye-Lund <erik.faye-lund@collabora.com>
---
 src/util/bitscan.h | 37 +++++++++++++++++++++++++++++++++++++
 1 file changed, 37 insertions(+)

diff --git a/src/util/bitscan.h b/src/util/bitscan.h
index c3ae88951581..288bd079516e 100644
--- a/src/util/bitscan.h
+++ b/src/util/bitscan.h
@@ -378,17 +378,54 @@ util_bitcount64_native(uint64_t n)
 
 /**
  * Return number of bits set in n.
+ *
+ * Note that this shouldn't be used in loops for the best performance.
+ * Loops should call util_bitcount_native directly when it's supported.
  */
 static inline unsigned
 util_bitcount(unsigned n)
 {
+#if defined(HAVE___BUILTIN_POPCOUNT) && defined(__POPCNT__) /* -mpopcnt */
+   /* GCC generates popcnt with -mpopcnt, but that's not set by default,
+    * so this is basically a dead path. It's only here for when you use -march.
+    */
+   return __builtin_popcount(n);
+#else /* no __POPCNT__ */
+
+#if defined(USE_X86_64_ASM) || defined(USE_X86_ASM) || \
+    (defined(_MSC_VER) && (DETECT_ARCH_X86_64 || DETECT_ARCH_X86))
+   /* The best we can do on x86 is this runtime check, which is faster than
+    * the slow path even if it's conditional.
+    */
+   if (util_get_cpu_caps()->has_popcnt)
+      return util_bitcount_native(n);
+#endif
+
    return util_bitcount_default(n);
+#endif
 }
 
 static inline unsigned
 util_bitcount64(uint64_t n)
 {
+#if defined(HAVE___BUILTIN_POPCOUNTLL) && defined(__POPCNT__) /* -mpopcnt */
+   /* GCC generates popcnt with -mpopcnt, but that's not set by default,
+    * so this is basically a dead path. It's only here for when you use -march.
+    */
+   return __builtin_popcountll(n);
+#else /* no __POPCNT__ */
+
+#if defined(USE_X86_64_ASM) || defined(USE_X86_ASM) || \
+    (defined(_MSC_VER) && (DETECT_ARCH_X86_64 || DETECT_ARCH_X86))
+   /* The best we can do on x86 is this runtime check, which is faster than
+    * the slow path even if it's conditional.
+    */
+   if (util_get_cpu_caps()->has_popcnt)
+      return util_bitcount64_native(n);
+#endif
+
    return util_bitcount64_default(n);
+#endif
 }
 
 /**
-- 
GitLab


From fb72cb912d31326d4f168f46b0cb8deec98b2c39 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Tue, 13 Dec 2022 18:02:30 -0500
Subject: [PATCH 5/5] util: replace the slow bitcount algorithms with better
 ones

split from a bigger commit

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
Reviewed-by: Erik Faye-Lund <erik.faye-lund@collabora.com>
---
 src/util/bitscan.h | 22 +++++++++++-----------
 1 file changed, 11 insertions(+), 11 deletions(-)

diff --git a/src/util/bitscan.h b/src/util/bitscan.h
index 288bd079516e..cc1504fbbc75 100644
--- a/src/util/bitscan.h
+++ b/src/util/bitscan.h
@@ -303,17 +303,12 @@ util_bitcount_default(unsigned n)
 #if defined(HAVE___BUILTIN_POPCOUNT)
    return __builtin_popcount(n);
 #else
-   /* K&R classic bitcount.
-    *
-    * For each iteration, clear the LSB from the bitfield.
-    * Requires only one iteration per set bit, instead of
-    * one iteration per bit less than highest set bit.
+   /* The slow version of __builtin_popcount() is identical to this.
+    * Google "popcount 0x55555555" for explanations.
     */
-   unsigned bits;
-   for (bits = 0; n; bits++) {
-      n &= n - 1;
-   }
-   return bits;
+   n = n - ((n >> 1) & 0x55555555);
+   n = (n & 0x33333333) + ((n >> 2) & 0x33333333);
+   return (((n + (n >> 4)) & 0x0F0F0F0F) * 0x01010101) >> 24;
 #endif
 }
 
@@ -323,7 +318,12 @@ util_bitcount64_default(unsigned n)
 #ifdef HAVE___BUILTIN_POPCOUNTLL
    return __builtin_popcountll(n);
 #else
-   return util_bitcount_default(n) + util_bitcount_default(n >> 32);
+   /* The slow version of __builtin_popcountll() is identical to this.
+    * Google "popcount 0x5555555555555555" for explanations.
+    */
+   n = n - ((n >> 1) & 0x5555555555555555);
+   n = (n & 0x3333333333333333) + ((n >> 2) & 0x3333333333333333);
+   return (((n + (n >> 4)) & 0x0F0F0F0F0F0F0F0F) * 0x0101010101010101) >> 56;
 #endif
 }
 
-- 
GitLab

