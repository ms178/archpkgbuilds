--- a/src/compiler/nir/nir_opt_copy_prop_vars.c	2025-10-02 23:36:01.188395340 +0200
+++ b/src/compiler/nir/nir_opt_copy_prop_vars.c	2025-10-02 23:39:43.185616745 +0200
@@ -56,10 +56,8 @@ static const bool debug = false;
 struct copies {
    struct list_head node;
 
-   /* Hash table of copies referenced by variables */
    struct hash_table ht;
 
-   /* Array of derefs that can't be chased back to a variable */
    struct util_dynarray arr;
 };
 
@@ -67,14 +65,12 @@ struct copies_dynarray {
    struct list_head node;
    struct util_dynarray arr;
 
-   /* The copies structure this dynarray was cloned or created for */
    struct copies *owner;
 };
 
 struct vars_written {
    nir_variable_mode modes;
 
-   /* Key is deref and value is the uintptr_t with the write mask. */
    struct hash_table derefs;
 };
 
@@ -89,12 +85,13 @@ struct value {
    };
 };
 
-static void
+static inline void
 value_set_ssa_components(struct value *value, nir_def *def,
                          unsigned num_components)
 {
+   assert(num_components <= NIR_MAX_VEC_COMPONENTS);
    value->is_ssa = true;
-   for (unsigned i = 0; i < num_components; i++) {
+   for (unsigned i = 0; i < num_components && i < NIR_MAX_VEC_COMPONENTS; i++) {
       value->ssa.def[i] = def;
       value->ssa.component[i] = i;
    }
@@ -112,12 +109,8 @@ struct copy_prop_var_state {
    void *mem_ctx;
    linear_ctx *lin_ctx;
 
-   /* Maps nodes to vars_written.  Used to invalidate copy entries when
-    * visiting each node.
-    */
    struct hash_table vars_written_map;
 
-   /* List of copy structures ready for reuse */
    struct list_head unused_copy_structs_list;
 
    bool progress;
@@ -127,15 +120,16 @@ static nir_component_mask_t
 value_equals_store_src(struct value *value, nir_intrinsic_instr *intrin)
 {
    assert(intrin->intrinsic == nir_intrinsic_store_deref);
-   nir_component_mask_t write_mask = nir_intrinsic_write_mask(intrin);
+   const nir_component_mask_t write_mask = nir_intrinsic_write_mask(intrin);
    nir_component_mask_t equals_mask = 0;
 
    for (unsigned i = 0; i < intrin->num_components; i++) {
-      nir_scalar src = nir_scalar_resolved(intrin->src[1].ssa, i);
-      if ((write_mask & (1 << i)) &&
-          (value->ssa.def[i] == src.def &&
-           value->ssa.component[i] == src.comp))
-         equals_mask |= 1 << i;
+      if (write_mask & (1 << i)) {
+         nir_scalar src = nir_scalar_resolved(intrin->src[1].ssa, i);
+         if (value->ssa.def[i] == src.def &&
+             value->ssa.component[i] == src.comp)
+            equals_mask |= 1 << i;
+      }
    }
 
    return equals_mask;
@@ -235,7 +229,6 @@ gather_vars_written(struct copy_prop_var
          case nir_intrinsic_store_deref:
          case nir_intrinsic_copy_deref:
          case nir_intrinsic_memcpy_deref: {
-            /* Destination in all of store_deref, copy_deref and the atomics is src[0]. */
             nir_deref_instr *dst = nir_src_as_deref(intrin->src[0]);
 
             uintptr_t mask = intrin->intrinsic == nir_intrinsic_store_deref ? nir_intrinsic_write_mask(intrin) : (1 << glsl_get_vector_elements(dst->type)) - 1;
@@ -288,7 +281,6 @@ gather_vars_written(struct copy_prop_var
    }
 
    if (new_written) {
-      /* Merge new information to the parent control flow node. */
       if (written) {
          written->modes |= new_written->modes;
          hash_table_foreach(&new_written->derefs, new_entry) {
@@ -309,20 +301,20 @@ gather_vars_written(struct copy_prop_var
    }
 }
 
-/* Creates a fresh dynarray */
 static struct copies_dynarray *
 get_copies_dynarray(struct copy_prop_var_state *state)
 {
    struct copies_dynarray *cp_arr =
       ralloc(state->mem_ctx, struct copies_dynarray);
+
+   if (unlikely(cp_arr == NULL))
+      return NULL;
+
    util_dynarray_init(&cp_arr->arr, state->mem_ctx);
    return cp_arr;
 }
 
-/* Checks if the pointer leads to a cloned copy of the array for this hash
- * table or if the pointer was inherited from when the hash table was cloned.
- */
-static bool
+static inline bool
 copies_owns_ht_entry(struct copies *copies,
                      struct hash_entry *ht_entry)
 {
@@ -338,11 +330,6 @@ clone_copies_dynarray_from_src(struct co
    util_dynarray_append_dynarray(&dst->arr, &src->arr);
 }
 
-/* Gets copies array from the hash table entry or clones the source array if
- * the hash entry contains NULL. The values are not cloned when the hash table
- * is created because its expensive to clone everything and most value will
- * never actually be accessed.
- */
 static struct copies_dynarray *
 get_copies_array_from_ht_entry(struct copy_prop_var_state *state,
                                struct copies *copies,
@@ -350,11 +337,11 @@ get_copies_array_from_ht_entry(struct co
 {
    struct copies_dynarray *copies_array;
    if (copies_owns_ht_entry(copies, ht_entry)) {
-      /* The array already exists so just return it */
       copies_array = (struct copies_dynarray *)ht_entry->data;
    } else {
-      /* Clone the array and set the data value for future access */
       copies_array = get_copies_dynarray(state);
+      if (unlikely(copies_array == NULL))
+         return NULL;
       copies_array->owner = copies;
       clone_copies_dynarray_from_src(copies_array, ht_entry->data);
       ht_entry->data = copies_array;
@@ -372,6 +359,8 @@ copies_array_for_var(struct copy_prop_va
       return get_copies_array_from_ht_entry(state, copies, entry);
 
    struct copies_dynarray *copies_array = get_copies_dynarray(state);
+   if (unlikely(copies_array == NULL))
+      return NULL;
    copies_array->owner = copies;
    _mesa_hash_table_insert(&copies->ht, var, copies_array);
 
@@ -390,6 +379,8 @@ copies_array_for_deref(struct copy_prop_
    } else {
       struct copies_dynarray *cpda =
          copies_array_for_var(state, copies, deref->_path->path[0]->var);
+      if (unlikely(cpda == NULL))
+         return NULL;
       copies_array = &cpda->arr;
    }
 
@@ -403,6 +394,9 @@ copy_entry_create(struct copy_prop_var_s
    struct util_dynarray *copies_array =
       copies_array_for_deref(state, copies, deref);
 
+   if (unlikely(copies_array == NULL))
+      return NULL;
+
    struct copy_entry new_entry = {
       .dst = *deref,
    };
@@ -410,12 +404,7 @@ copy_entry_create(struct copy_prop_var_s
    return util_dynarray_top_ptr(copies_array, struct copy_entry);
 }
 
-/* Remove copy entry by swapping it with the last element and reducing the
- * size.  If used inside an iteration on copies, it must be a reverse
- * (backwards) iteration.  It is safe to use in those cases because the swap
- * will not affect the rest of the iteration.
- */
-static void
+static inline void
 copy_entry_remove(struct util_dynarray *copies,
                   struct copy_entry *entry,
                   struct copy_entry **relocated_entry)
@@ -423,11 +412,6 @@ copy_entry_remove(struct util_dynarray *
    const struct copy_entry *src =
       util_dynarray_pop_ptr(copies, struct copy_entry);
 
-   /* Because we're removing elements from an array, pointers to those
-    * elements are not stable as we modify the array.
-    * If relocated_entry != NULL, it's points to an entry we saved off earlier
-    * and want to keep pointing to the right spot.
-    */
    if (relocated_entry && *relocated_entry == src)
       *relocated_entry = entry;
 
@@ -435,7 +419,7 @@ copy_entry_remove(struct util_dynarray *
       *entry = *src;
 }
 
-static bool
+static inline bool
 is_array_deref_of_vector(const nir_deref_and_path *deref)
 {
    if (deref->instr->deref_type != nir_deref_type_array)
@@ -454,6 +438,9 @@ lookup_entry_for_deref(struct copy_prop_
    struct util_dynarray *copies_array =
       copies_array_for_deref(state, copies, deref);
 
+   if (unlikely(copies_array == NULL))
+      return NULL;
+
    struct copy_entry *entry = NULL;
    util_dynarray_foreach(copies_array, struct copy_entry, iter) {
       nir_deref_compare_result result =
@@ -465,7 +452,6 @@ lookup_entry_for_deref(struct copy_prop_
                *equal = true;
             break;
          }
-         /* Keep looking in case we have an equal match later in the array. */
       }
    }
 
@@ -486,7 +472,6 @@ lookup_entry_and_kill_aliases_copy_array
          nir_compare_derefs_and_paths(state->mem_ctx, &iter->dst, deref);
 
       if (comp & nir_derefs_equal_bit) {
-         /* Make sure it is unique. */
          assert(!*entry && !*entry_removed);
          if (remove_entry) {
             copy_entry_remove(copies_array, iter, NULL);
@@ -507,17 +492,11 @@ lookup_entry_and_kill_aliases(struct cop
                               unsigned write_mask,
                               bool remove_entry)
 {
-   /* TODO: Take into account the write_mask. */
-
-   bool UNUSED entry_removed = false;
+   bool entry_removed = false;
    struct copy_entry *entry = NULL;
 
    nir_get_deref_path(state->mem_ctx, deref);
 
-   /* For any other variable types if the variables are different,
-    * they don't alias. So we only need to compare different vars and loop
-    * over the hash table for ssbos and shared vars.
-    */
    if (deref->_path->path[0]->deref_type != nir_deref_type_var ||
        deref->_path->path[0]->var->data.mode == nir_var_mem_ssbo ||
        deref->_path->path[0]->var->data.mode == nir_var_mem_shared) {
@@ -531,6 +510,9 @@ lookup_entry_and_kill_aliases(struct cop
          struct copies_dynarray *copies_array =
             get_copies_array_from_ht_entry(state, copies, ht_entry);
 
+         if (unlikely(copies_array == NULL))
+            continue;
+
          lookup_entry_and_kill_aliases_copy_array(state, &copies_array->arr,
                                                   deref, write_mask,
                                                   remove_entry, &entry,
@@ -547,6 +529,8 @@ lookup_entry_and_kill_aliases(struct cop
    } else {
       struct copies_dynarray *cpda =
          copies_array_for_var(state, copies, deref->_path->path[0]->var);
+      if (unlikely(cpda == NULL))
+         return NULL;
       struct util_dynarray *copies_array = &cpda->arr;
 
       lookup_entry_and_kill_aliases_copy_array(state, copies_array, deref,
@@ -567,8 +551,6 @@ kill_aliases(struct copy_prop_var_state
              nir_deref_and_path *deref,
              unsigned write_mask)
 {
-   /* TODO: Take into account the write_mask. */
-
    lookup_entry_and_kill_aliases(state, copies, deref, write_mask, true);
 }
 
@@ -578,8 +560,6 @@ get_entry_and_kill_aliases(struct copy_p
                            nir_deref_and_path *deref,
                            unsigned write_mask)
 {
-   /* TODO: Take into account the write_mask. */
-
    struct copy_entry *entry =
       lookup_entry_and_kill_aliases(state, copies, deref, write_mask, false);
    if (entry == NULL)
@@ -607,6 +587,9 @@ apply_barrier_for_modes(struct copy_prop
       struct copies_dynarray *copies_array =
          get_copies_array_from_ht_entry(state, copies, ht_entry);
 
+      if (unlikely(copies_array == NULL))
+         continue;
+
       apply_barrier_for_modes_to_dynarr(&copies_array->arr, modes);
    }
 
@@ -617,29 +600,24 @@ static void
 value_set_from_value(struct value *value, const struct value *from,
                      unsigned base_index, unsigned write_mask)
 {
-   /* We can't have non-zero indexes with non-trivial write masks */
    assert(base_index == 0 || write_mask == 1);
+   assert(base_index < NIR_MAX_VEC_COMPONENTS);
 
    if (from->is_ssa) {
-      /* Clear value if it was being used as non-SSA. */
       value->is_ssa = true;
-      /* Only overwrite the written components */
       for (unsigned i = 0; i < NIR_MAX_VEC_COMPONENTS; i++) {
          if (write_mask & (1 << i)) {
+            assert(base_index + i < NIR_MAX_VEC_COMPONENTS);
             value->ssa.def[base_index + i] = from->ssa.def[i];
             value->ssa.component[base_index + i] = from->ssa.component[i];
          }
       }
    } else {
-      /* Non-ssa stores always write everything */
       value->is_ssa = false;
       value->deref = from->deref;
    }
 }
 
-/* Try to load a single element of a vector from the copy_entry.  If the data
- * isn't available, just let the original intrinsic do the work.
- */
 static bool
 load_element_from_ssa_entry_value(struct copy_prop_var_state *state,
                                   struct copy_entry *entry,
@@ -648,7 +626,6 @@ load_element_from_ssa_entry_value(struct
 {
    assert(index < glsl_get_vector_elements(entry->dst.instr->type));
 
-   /* We don't have the element available, so let the instruction do the work. */
    if (!entry->src.ssa.def[index])
       return false;
 
@@ -673,14 +650,6 @@ load_element_from_ssa_entry_value(struct
    return true;
 }
 
-/* Do a "load" from an SSA-based entry return it in "value" as a value with a
- * single SSA def.  Because an entry could reference multiple different SSA
- * defs, a vecN operation may be inserted to combine them into a single SSA
- * def before handing it back to the caller.  If the load instruction is no
- * longer needed, it is removed and nir_instr::block is set to NULL.  (It is
- * possible, in some cases, for the load to be used in the vecN operation in
- * which case it isn't deleted.)
- */
 static bool
 load_from_ssa_entry_value(struct copy_prop_var_state *state,
                           struct copy_entry *entry,
@@ -694,10 +663,8 @@ load_from_ssa_entry_value(struct copy_pr
                                                   value, index);
       }
 
-      /* An SSA copy_entry for the vector won't help indirect load. */
       if (glsl_type_is_vector(entry->dst.instr->type)) {
          assert(entry->dst.instr->type == nir_deref_instr_parent(src->instr)->type);
-         /* TODO: If all SSA entries are there, try an if-ladder. */
          return false;
       }
    }
@@ -721,7 +688,6 @@ load_from_ssa_entry_value(struct copy_pr
    }
 
    if (all_same) {
-      /* Our work here is done */
       b->cursor = nir_instr_remove(&intrin->instr);
       intrin->instr.block = NULL;
       return true;
@@ -730,10 +696,6 @@ load_from_ssa_entry_value(struct copy_pr
    if (available != (1 << num_components) - 1 &&
        intrin->intrinsic == nir_intrinsic_load_deref &&
        (available & nir_def_components_read(&intrin->def)) == 0) {
-      /* If none of the components read are available as SSA values, then we
-       * should just bail.  Otherwise, we would end up replacing the uses of
-       * the load_deref a vecN() that just gathers up its components.
-       */
       return false;
    }
 
@@ -748,9 +710,6 @@ load_from_ssa_entry_value(struct copy_pr
       if (value->ssa.def[i]) {
          comps[i] = nir_get_scalar(value->ssa.def[i], value->ssa.component[i]);
       } else {
-         /* We don't have anything for this component in our
-          * list.  Just re-use a channel from the load.
-          */
          if (load_def == NULL)
             load_def = nir_load_deref(b, entry->dst.instr);
 
@@ -765,10 +724,6 @@ load_from_ssa_entry_value(struct copy_pr
    value_set_ssa_components(value, vec, num_components);
 
    if (!keep_intrin) {
-      /* Removing this instruction should not touch the cursor because we
-       * created the cursor after the intrinsic and have added at least one
-       * instruction (the vec) since then.
-       */
       assert(b->cursor.instr != &intrin->instr);
       nir_instr_remove(&intrin->instr);
       intrin->instr.block = NULL;
@@ -777,14 +732,6 @@ load_from_ssa_entry_value(struct copy_pr
    return true;
 }
 
-/**
- * Specialize the wildcards in a deref chain
- *
- * This function returns a deref chain identical to \param deref except that
- * some of its wildcards are replaced with indices from \param specific.  The
- * process is guided by \param guide which references the same type as \param
- * specific but has the same wildcard array lengths as \param deref.
- */
 static nir_deref_instr *
 specialize_wildcards(nir_builder *b,
                      nir_deref_path *deref,
@@ -803,10 +750,6 @@ specialize_wildcards(nir_builder *b,
    nir_deref_instr **spec_p = &specific->path[1];
    for (; *deref_p; deref_p++) {
       if ((*deref_p)->deref_type == nir_deref_type_array_wildcard) {
-         /* This is where things get tricky.  We have to search through
-          * the entry deref to find its corresponding wildcard and fill
-          * this slot in with the value from the src.
-          */
          while (*guide_p &&
                 (*guide_p)->deref_type != nir_deref_type_array_wildcard) {
             guide_p++;
@@ -826,11 +769,6 @@ specialize_wildcards(nir_builder *b,
    return ret_tail;
 }
 
-/* Do a "load" from an deref-based entry return it in "value" as a value.  The
- * deref returned in "value" will always be a fresh copy so the caller can
- * steal it and assign it to the instruction directly without copying it
- * again.
- */
 static bool
 load_from_deref_entry_value(struct copy_prop_var_state *state,
                             struct copy_entry *entry,
@@ -856,27 +794,17 @@ load_from_deref_entry_value(struct copy_
          need_to_specialize_wildcards = true;
    }
 
-   /* If the entry deref is longer than the source deref then it refers to a
-    * smaller type and we can't source from it.
-    */
    assert(*entry_p == NULL);
 
    value->deref._path = NULL;
 
    if (need_to_specialize_wildcards) {
-      /* The entry has some wildcards that are not in src.  This means we need
-       * to construct a new deref based on the entry but using the wildcards
-       * from the source and guided by the entry dst.  Oof.
-       */
       nir_deref_path *entry_src_path =
          nir_get_deref_path(state->mem_ctx, &entry->src.deref);
       value->deref.instr = specialize_wildcards(b, entry_src_path,
                                                 entry_dst_path, src_path);
    }
 
-   /* If our source deref is longer than the entry deref, that's ok because
-    * it just means the entry deref needs to be extended a bit.
-    */
    while (*src_p) {
       nir_deref_instr *src_tail = *src_p++;
       value->deref.instr = nir_build_deref_follower(b, value->deref.instr, src_tail);
@@ -914,6 +842,9 @@ invalidate_copies_for_cf_node(struct cop
          struct copies_dynarray *copies_array =
             get_copies_array_from_ht_entry(state, copies, ht_entry);
 
+         if (unlikely(copies_array == NULL))
+            continue;
+
          util_dynarray_foreach_reverse(&copies_array->arr, struct copy_entry, entry) {
             if (nir_deref_mode_may_be(entry->dst.instr, written->modes))
                copy_entry_remove(&copies_array->arr, entry, NULL);
@@ -951,7 +882,7 @@ print_value(struct value *value, unsigne
    if (same_ssa) {
       printf(" ssa_%d", value->ssa.def[0]->index);
    } else {
-      for (int i = 0; i < num_components; i++) {
+      for (unsigned i = 0; i < num_components; i++) {
          if (value->ssa.def[i])
             printf(" ssa_%d[%u]", value->ssa.def[i]->index, value->ssa.component[i]);
          else
@@ -1059,23 +990,14 @@ copy_prop_vars_block(struct copy_prop_va
 
          nir_deref_and_path src = { nir_src_as_deref(intrin->src[0]), NULL };
 
-         /* If this is a load from a read-only mode, then all this pass would
-          * do is combine redundant loads and CSE should be more efficient for
-          * that.
-          */
          nir_variable_mode ignore = nir_var_read_only_modes & ~nir_var_vec_indexable_modes;
          if (nir_deref_mode_must_be(src.instr, ignore))
             break;
 
-         /* Ignore trivial casts. If trivial casts are applied to array derefs of vectors,
-          * not doing this causes is_array_deref_of_vector to (wrongly) return false. */
          while (src.instr->deref_type == nir_deref_type_cast &&
                 nir_deref_instr_parent(src.instr) && nir_deref_cast_is_trivial(src.instr))
             src.instr = nir_deref_instr_parent(src.instr);
 
-         /* Direct array_derefs of vectors operate on the vectors (the parent
-          * deref).  Indirects will be handled like other derefs.
-          */
          int vec_index = 0;
          nir_deref_and_path vec_src = src;
          if (is_array_deref_of_vector(&src) && nir_src_is_const(src.instr->arr.index)) {
@@ -1083,7 +1005,6 @@ copy_prop_vars_block(struct copy_prop_va
             unsigned vec_comps = glsl_get_vector_elements(vec_src.instr->type);
             vec_index = nir_src_as_uint(src.instr->arr.index);
 
-            /* Loading from an invalid index yields an undef */
             if (vec_index >= vec_comps) {
                b->cursor = nir_instr_remove(instr);
                nir_def *u = nir_undef(b, 1, intrin->def.bit_size);
@@ -1100,17 +1021,7 @@ copy_prop_vars_block(struct copy_prop_va
          struct value value = { 0 };
          if (try_load_from_entry(state, src_entry, b, intrin, &src, &value)) {
             if (value.is_ssa) {
-               /* lookup_load has already ensured that we get a single SSA
-                * value that has all of the channels.  We just have to do the
-                * rewrite operation.  Note for array derefs of vectors, the
-                * channel 0 is used.
-                */
                if (intrin->instr.block) {
-                  /* The lookup left our instruction in-place.  This means it
-                   * must have used it to vec up a bunch of different sources.
-                   * We need to be careful when rewriting uses so we don't
-                   * rewrite the vecN itself.
-                   */
                   nir_def_rewrite_uses_after(&intrin->def,
                                              value.ssa.def[0]);
                } else {
@@ -1118,10 +1029,8 @@ copy_prop_vars_block(struct copy_prop_va
                                        value.ssa.def[0]);
                }
             } else {
-               /* We're turning it into a load of a different variable */
                intrin->src[0] = nir_src_for_ssa(&value.deref.instr->def);
 
-               /* Put it back in again. */
                nir_builder_instr_insert(b, instr);
                value_set_ssa_components(&value, &intrin->def,
                                         intrin->num_components);
@@ -1132,13 +1041,6 @@ copy_prop_vars_block(struct copy_prop_va
                                      intrin->num_components);
          }
 
-         /* Now that we have a value, we're going to store it back so that we
-          * have the right value next time we come looking for it.  In order
-          * to do this, we need an exact match, not just something that
-          * contains what we're looking for.
-          *
-          * We avoid doing another lookup if src.instr == vec_src.instr.
-          */
          struct copy_entry *entry = src_entry;
          if (src.instr != vec_src.instr)
             entry = lookup_entry_for_deref(state, copies, &vec_src,
@@ -1149,11 +1051,10 @@ copy_prop_vars_block(struct copy_prop_va
          if (!entry)
             entry = copy_entry_create(state, copies, &vec_src);
 
-         /* Update the entry with the value of the load.  This way
-          * we can potentially remove subsequent loads.
-          */
-         value_set_from_value(&entry->src, &value, vec_index,
-                              (1 << intrin->num_components) - 1);
+         if (likely(entry != NULL)) {
+            value_set_from_value(&entry->src, &value, vec_index,
+                                 (1 << intrin->num_components) - 1);
+         }
          break;
       }
 
@@ -1164,15 +1065,10 @@ copy_prop_vars_block(struct copy_prop_va
          nir_deref_and_path dst = { nir_src_as_deref(intrin->src[0]), NULL };
          assert(glsl_type_is_vector_or_scalar(dst.instr->type));
 
-         /* Ignore trivial casts. If trivial casts are applied to array derefs of vectors,
-          * not doing this causes is_array_deref_of_vector to (wrongly) return false. */
          while (dst.instr->deref_type == nir_deref_type_cast &&
                 nir_deref_instr_parent(dst.instr) && nir_deref_cast_is_trivial(dst.instr))
             dst.instr = nir_deref_instr_parent(dst.instr);
 
-         /* Direct array_derefs of vectors operate on the vectors (the parent
-          * deref).  Indirects will be handled like other derefs.
-          */
          int vec_index = 0;
          nir_deref_and_path vec_dst = dst;
          if (is_array_deref_of_vector(&dst) && nir_src_is_const(dst.instr->arr.index)) {
@@ -1181,7 +1077,6 @@ copy_prop_vars_block(struct copy_prop_va
 
             vec_index = nir_src_as_uint(dst.instr->arr.index);
 
-            /* Storing to an invalid index is a no-op. */
             if (vec_index >= vec_comps) {
                nir_instr_remove(instr);
                state->progress = true;
@@ -1199,34 +1094,19 @@ copy_prop_vars_block(struct copy_prop_va
             lookup_entry_for_deref(state, copies, &dst, nir_derefs_equal_bit, NULL);
          nir_component_mask_t equals_mask = entry ? value_equals_store_src(&entry->src, intrin) : 0;
          if (equals_mask == nir_intrinsic_write_mask(intrin)) {
-            /* If we are storing the value from a load of the same var the
-             * store is redundant so remove it.
-             */
             nir_instr_remove(instr);
             state->progress = true;
          } else {
             if (!(b->shader->info.stage == MESA_SHADER_FRAGMENT &&
                   (nir_deref_mode_may_be(dst.instr, nir_var_shader_out)))) {
-               /* If any channels we wrote were already the dst's value, mask them
-                * off (which can lead to other dead code elimination).  Apparently
-                * glslang does write masking with load-vec-store.
-                *
-                * Skip this for FS outputs, where multiple drivers don't like color
-                * writes getting channels writemasked out based on copying the
-                * fbfetched data through.
-                */
                nir_component_mask_t remove_mask = equals_mask & nir_intrinsic_write_mask(intrin);
                if (remove_mask) {
                   nir_intrinsic_set_write_mask(intrin, nir_intrinsic_write_mask(intrin) & ~equals_mask);
 
-                  /* For any channels we're trimming off the write mask, replace
-                   * them with undefs.  This lets them be dead-code eliminated,
-                   * which no other pass would do on its own.
-                   */
                   b->cursor = nir_before_instr(instr);
                   nir_def *channels[NIR_MAX_VEC_COMPONENTS];
                   nir_def *undef = nir_undef(b, 1, intrin->src[1].ssa->bit_size);
-                  for (int i = 0; i < intrin->num_components; i++) {
+                  for (unsigned i = 0; i < intrin->num_components; i++) {
                      if (remove_mask & (1 << i)) {
                         channels[i] = undef;
                      } else {
@@ -1243,7 +1123,8 @@ copy_prop_vars_block(struct copy_prop_va
             unsigned wrmask = nir_intrinsic_write_mask(intrin);
             struct copy_entry *entry =
                get_entry_and_kill_aliases(state, copies, &vec_dst, wrmask);
-            value_set_from_value(&entry->src, &value, vec_index, wrmask);
+            if (likely(entry != NULL))
+               value_set_from_value(&entry->src, &value, vec_index, wrmask);
          }
 
          break;
@@ -1256,9 +1137,6 @@ copy_prop_vars_block(struct copy_prop_va
          nir_deref_and_path dst = { nir_src_as_deref(intrin->src[0]), NULL };
          nir_deref_and_path src = { nir_src_as_deref(intrin->src[1]), NULL };
 
-         /* The copy_deref intrinsic doesn't keep track of num_components, so
-          * get it ourselves.
-          */
          unsigned num_components = glsl_get_vector_elements(dst.instr->type);
          unsigned full_mask = (1 << num_components) - 1;
 
@@ -1271,15 +1149,11 @@ copy_prop_vars_block(struct copy_prop_va
          nir_deref_compare_result comp =
             nir_compare_derefs_and_paths(state->mem_ctx, &src, &dst);
          if (comp & nir_derefs_equal_bit) {
-            /* This is a no-op self-copy.  Get rid of it */
             nir_instr_remove(instr);
             state->progress = true;
             continue;
          }
 
-         /* Copy of direct array derefs of vectors are not handled.  Just
-          * invalidate what's written and bail.
-          */
          if ((is_array_deref_of_vector(&src) && nir_src_is_const(src.instr->arr.index)) ||
              (is_array_deref_of_vector(&dst) && nir_src_is_const(dst.instr->arr.index))) {
             kill_aliases(state, copies, &dst, full_mask);
@@ -1290,19 +1164,15 @@ copy_prop_vars_block(struct copy_prop_va
             lookup_entry_for_deref(state, copies, &src, nir_derefs_a_contains_b_bit, NULL);
          struct value value;
          if (try_load_from_entry(state, src_entry, b, intrin, &src, &value)) {
-            /* If load works, intrin (the copy_deref) is removed. */
             if (value.is_ssa) {
                nir_store_deref(b, dst.instr, value.ssa.def[0], full_mask);
             } else {
-               /* If this would be a no-op self-copy, don't bother. */
                comp = nir_compare_derefs_and_paths(state->mem_ctx, &value.deref, &dst);
                if (comp & nir_derefs_equal_bit)
                   continue;
 
-               /* Just turn it into a copy of a different deref */
                intrin->src[1] = nir_src_for_ssa(&value.deref.instr->def);
 
-               /* Put it back in again. */
                nir_builder_instr_insert(b, instr);
             }
 
@@ -1316,15 +1186,13 @@ copy_prop_vars_block(struct copy_prop_va
 
          nir_variable *src_var = nir_deref_instr_get_variable(src.instr);
          if (src_var && src_var->data.cannot_coalesce) {
-            /* The source cannot be coaleseced, which means we can't propagate
-             * this copy.
-             */
             break;
          }
 
          struct copy_entry *dst_entry =
             get_entry_and_kill_aliases(state, copies, &dst, full_mask);
-         value_set_from_value(&dst_entry->src, &value, 0, full_mask);
+         if (likely(dst_entry != NULL))
+            value_set_from_value(&dst_entry->src, &value, 0, full_mask);
          break;
       }
 
@@ -1345,7 +1213,7 @@ copy_prop_vars_block(struct copy_prop_va
 
       case nir_intrinsic_memcpy_deref:
       case nir_intrinsic_deref_atomic:
-      case nir_intrinsic_deref_atomic_swap:
+      case nir_intrinsic_deref_atomic_swap: {
          if (debug)
             dump_instr(instr);
 
@@ -1354,14 +1222,12 @@ copy_prop_vars_block(struct copy_prop_va
          unsigned full_mask = (1 << num_components) - 1;
          kill_aliases(state, copies, &dst, full_mask);
          break;
+      }
 
       case nir_intrinsic_store_deref_block_intel: {
          if (debug)
             dump_instr(instr);
 
-         /* Invalidate the whole variable (or cast) and anything that alias
-          * with it.
-          */
          nir_deref_and_path dst = { nir_src_as_deref(intrin->src[0]), NULL };
          while (nir_deref_instr_parent(dst.instr))
             dst.instr = nir_deref_instr_parent(dst.instr);
@@ -1375,7 +1241,7 @@ copy_prop_vars_block(struct copy_prop_va
       }
 
       default:
-         continue; /* To skip the debug below. */
+         continue;
       }
 
       if (debug)
@@ -1387,26 +1253,21 @@ static void
 clone_copies(struct copy_prop_var_state *state, struct copies *clones,
              struct copies *copies)
 {
-   /* Simply clone the entire hash table. This is much faster than trying to
-    * rebuild it and is needed to avoid slow compilation of very large shaders.
-    * If needed we will clone the data later if it is ever looked up.
-    */
    assert(clones->ht.table == NULL);
+
    _mesa_hash_table_copy(&clones->ht, &copies->ht, state->mem_ctx);
 
    util_dynarray_clone(&clones->arr, state->mem_ctx, &copies->arr);
 }
 
-/* Returns an existing struct for reuse or creates a new on if they are
- * all in use. This greatly reduces the time spent allocating memory if we
- * were to just creating a fresh one each time.
- */
 static struct copies *
 get_copies_structure(struct copy_prop_var_state *state)
 {
    struct copies *copies;
    if (list_is_empty(&state->unused_copy_structs_list)) {
       copies = ralloc(state->mem_ctx, struct copies);
+      if (unlikely(copies == NULL))
+         return NULL;
       copies->ht.table = NULL;
       util_dynarray_init(&copies->arr, state->mem_ctx);
    } else {
@@ -1436,6 +1297,8 @@ copy_prop_vars_cf_node(struct copy_prop_
       nir_function_impl *impl = nir_cf_node_as_function(cf_node);
 
       struct copies *impl_copies = get_copies_structure(state);
+      if (unlikely(impl_copies == NULL))
+         return;
       _mesa_hash_table_init(&impl_copies->ht, state->mem_ctx,
                             _mesa_hash_pointer, _mesa_key_pointer_equal);
 
@@ -1457,37 +1320,30 @@ copy_prop_vars_cf_node(struct copy_prop_
    case nir_cf_node_if: {
       nir_if *if_stmt = nir_cf_node_as_if(cf_node);
 
-      /* Create new hash tables for tracking vars and fill it with clones of
-       * the copy arrays for each variable we are tracking.
-       *
-       * We clone the copies for each branch of the if statement.  The idea is
-       * that they both see the same state of available copies, but do not
-       * interfere to each other.
-       */
       if (!exec_list_is_empty(&if_stmt->then_list)) {
          struct copies *then_copies = get_copies_structure(state);
-         clone_copies(state, then_copies, copies);
+         if (likely(then_copies != NULL)) {
+            clone_copies(state, then_copies, copies);
 
-         foreach_list_typed_safe(nir_cf_node, cf_node, node, &if_stmt->then_list)
-            copy_prop_vars_cf_node(state, then_copies, cf_node);
+            foreach_list_typed_safe(nir_cf_node, cf_node, node, &if_stmt->then_list)
+               copy_prop_vars_cf_node(state, then_copies, cf_node);
 
-         clear_copies_structure(state, then_copies);
+            clear_copies_structure(state, then_copies);
+         }
       }
 
       if (!exec_list_is_empty(&if_stmt->else_list)) {
          struct copies *else_copies = get_copies_structure(state);
-         clone_copies(state, else_copies, copies);
+         if (likely(else_copies != NULL)) {
+            clone_copies(state, else_copies, copies);
 
-         foreach_list_typed_safe(nir_cf_node, cf_node, node, &if_stmt->else_list)
-            copy_prop_vars_cf_node(state, else_copies, cf_node);
+            foreach_list_typed_safe(nir_cf_node, cf_node, node, &if_stmt->else_list)
+               copy_prop_vars_cf_node(state, else_copies, cf_node);
 
-         clear_copies_structure(state, else_copies);
+            clear_copies_structure(state, else_copies);
+         }
       }
 
-      /* Both branches copies can be ignored, since the effect of running both
-       * branches was captured in the first pass that collects vars_written.
-       */
-
       invalidate_copies_for_cf_node(state, copies, cf_node);
 
       break;
@@ -1497,19 +1353,17 @@ copy_prop_vars_cf_node(struct copy_prop_
       nir_loop *loop = nir_cf_node_as_loop(cf_node);
       assert(!nir_loop_has_continue_construct(loop));
 
-      /* Invalidate before cloning the copies for the loop, since the loop
-       * body can be executed more than once.
-       */
-
       invalidate_copies_for_cf_node(state, copies, cf_node);
 
       struct copies *loop_copies = get_copies_structure(state);
-      clone_copies(state, loop_copies, copies);
+      if (likely(loop_copies != NULL)) {
+         clone_copies(state, loop_copies, copies);
 
-      foreach_list_typed_safe(nir_cf_node, cf_node, node, &loop->body)
-         copy_prop_vars_cf_node(state, loop_copies, cf_node);
+         foreach_list_typed_safe(nir_cf_node, cf_node, node, &loop->body)
+            copy_prop_vars_cf_node(state, loop_copies, cf_node);
 
-      clear_copies_structure(state, loop_copies);
+         clear_copies_structure(state, loop_copies);
+      }
 
       break;
    }

--- a/src/mesa/main/matrix.c	2025-10-02 20:44:54.337186391 +0200
+++ b/src/mesa/main/matrix.c	2025-10-02 20:45:45.884745099 +0200
@@ -376,6 +376,11 @@ pop_matrix( struct gl_context *ctx, stru
    if (stack->Depth == 0)
       return GL_FALSE;
 
+   /* Sync if in glthread to avoid racing with main-thread attribute calls */
+   if (ctx->GLThread.enabled) {
+      _mesa_glthread_finish_before(ctx, "pop_matrix");
+   }
+
    stack->Depth--;
 
    /* If the popped matrix is the same as the current one, treat it as
@@ -1008,8 +1013,13 @@ init_matrix_stack(struct gl_matrix_stack
    stack->Depth = 0;
    stack->MaxDepth = maxDepth;
    stack->DirtyFlag = dirtyFlag;
-   /* The stack will be dynamically resized at glPushMatrix() time */
    stack->Stack = os_malloc_aligned(sizeof(GLmatrix), 16);
+   if (stack->Stack == NULL) {
+      _mesa_error(NULL, GL_OUT_OF_MEMORY, "init_matrix_stack allocation failed");
+      stack->StackSize = 0;
+      stack->Top = NULL;
+      return;
+   }
    stack->StackSize = 1;
    _math_matrix_ctr(&stack->Stack[0]);
    stack->Top = stack->Stack;
@@ -1024,6 +1034,9 @@ init_matrix_stack(struct gl_matrix_stack
 static void
 free_matrix_stack( struct gl_matrix_stack *stack )
 {
+   if (stack->Stack == NULL) {
+      return;
+   }
    os_free_aligned(stack->Stack);
    stack->Stack = stack->Top = NULL;
    stack->StackSize = 0;
@@ -1049,21 +1062,43 @@ void _mesa_init_matrix( struct gl_contex
 {
    GLuint i;
 
-   /* Initialize matrix stacks */
+   // Initialize matrix stacks with OOM checks (holistic: Ensure all succeed or fail cleanly)
    init_matrix_stack(&ctx->ModelviewMatrixStack, MAX_MODELVIEW_STACK_DEPTH,
                      _NEW_MODELVIEW);
+   if (ctx->ModelviewMatrixStack.Stack == NULL) goto init_fail;  // Propagate OOM
+
    init_matrix_stack(&ctx->ProjectionMatrixStack, MAX_PROJECTION_STACK_DEPTH,
                      _NEW_PROJECTION);
-   for (i = 0; i < ARRAY_SIZE(ctx->TextureMatrixStack); i++)
+   if (ctx->ProjectionMatrixStack.Stack == NULL) goto init_fail;
+
+   for (i = 0; i < ARRAY_SIZE(ctx->TextureMatrixStack); i++) {
       init_matrix_stack(&ctx->TextureMatrixStack[i], MAX_TEXTURE_STACK_DEPTH,
                         _NEW_TEXTURE_MATRIX);
-   for (i = 0; i < ARRAY_SIZE(ctx->ProgramMatrixStack); i++)
+      if (ctx->TextureMatrixStack[i].Stack == NULL) goto init_fail;
+   }
+
+   for (i = 0; i < ARRAY_SIZE(ctx->ProgramMatrixStack); i++) {
       init_matrix_stack(&ctx->ProgramMatrixStack[i],
-		        MAX_PROGRAM_MATRIX_STACK_DEPTH, _NEW_TRACK_MATRIX);
+                        MAX_PROGRAM_MATRIX_STACK_DEPTH, _NEW_TRACK_MATRIX);
+      if (ctx->ProgramMatrixStack[i].Stack == NULL) goto init_fail;
+   }
+
    ctx->CurrentStack = &ctx->ModelviewMatrixStack;
 
-   /* Init combined Modelview*Projection matrix */
+   // Init combined Modelview*Projection matrix (safe after checks)
    _math_matrix_ctr( &ctx->_ModelProjectMatrix );
+
+   return;  // Success
+
+init_fail:
+   // Cleanup partial allocations to avoid leaks (memory-safe)
+   free_matrix_stack(&ctx->ModelviewMatrixStack);
+   free_matrix_stack(&ctx->ProjectionMatrixStack);
+   for (i = 0; i < ARRAY_SIZE(ctx->TextureMatrixStack); i++)
+      free_matrix_stack(&ctx->TextureMatrixStack[i]);
+   for (i = 0; i < ARRAY_SIZE(ctx->ProgramMatrixStack); i++)
+      free_matrix_stack(&ctx->ProgramMatrixStack[i]);
+   // Caller (init_attrib_groups) will handle failed init (e.g., context creation fails)
 }
