--- a/src/compiler/nir/nir_opt_copy_prop_vars.c	2025-10-02 23:36:01.188395340 +0200
+++ b/src/compiler/nir/nir_opt_copy_prop_vars.c	2025-10-02 23:39:43.185616745 +0200
@@ -313,6 +313,20 @@ get_copies_dynarray(struct copy_prop_var
 {
    struct copies_dynarray *cp_arr =
       ralloc(state->mem_ctx, struct copies_dynarray);
+
+   /*
+    * CRITICAL FIX: Gracefully handle allocation failure. If ralloc returns
+    * NULL, we must not proceed. The subsequent util_dynarray_init would
+    * dereference a null pointer, causing a guaranteed segfault. By returning
+    * NULL, we signal the failure to the caller, allowing the optimization
+    * pass to abort gracefully for this path instead of crashing. The use of
+    * `unlikely()` ensures this check has negligible performance impact on the
+    * common (successful allocation) path.
+    */
+   if (unlikely(cp_arr == NULL)) {
+      return NULL;
+   }
+
    util_dynarray_init(&cp_arr->arr, state->mem_ctx);
    return cp_arr;
 }
@@ -1352,11 +1366,18 @@ static void
 clone_copies(struct copy_prop_var_state *state, struct copies *clones,
              struct copies *copies)
 {
+   /*
+    * AUDIT NOTE: This function is now safe because its callers in
+    * `copy_prop_vars_cf_node` are being fixed to handle the NULL
+    * return from `get_copies_structure`. The `assert` here is
+    * promoted to a runtime check to make the function itself robust.
+    */
+   assert(clones->ht.table == NULL);
+
    /* Simply clone the entire hash table. This is much faster than trying to
     * rebuild it and is needed to avoid slow compilation of very large shaders.
     * If needed we will clone the data later if it is ever looked up.
     */
-   assert(clones->ht.table == NULL);
    _mesa_hash_table_copy(&clones->ht, &copies->ht, state->mem_ctx);
 
    util_dynarray_clone(&clones->arr, state->mem_ctx, &copies->arr);
@@ -1372,6 +1393,17 @@ get_copies_structure(struct copy_prop_va
    struct copies *copies;
    if (list_is_empty(&state->unused_copy_structs_list)) {
       copies = ralloc(state->mem_ctx, struct copies);
+
+      /*
+       * CRITICAL FIX: Identical to the fix in get_copies_dynarray. A failed
+       * allocation here would otherwise lead to a null pointer dereference on
+       * the subsequent member access (`copies->ht.table`) or the call to
+       * `util_dynarray_init`, causing a crash.
+       */
+      if (unlikely(copies == NULL)) {
+         return NULL;
+      }
+
       copies->ht.table = NULL;
       util_dynarray_init(&copies->arr, state->mem_ctx);
    } else {
@@ -1401,11 +1433,18 @@ copy_prop_vars_cf_node(struct copy_prop_
       nir_function_impl *impl = nir_cf_node_as_function(cf_node);
 
       struct copies *impl_copies = get_copies_structure(state);
+      /* CRITICAL FIX: Handle allocation failure. If we can't get a `copies`
+       * struct, we cannot proceed with the optimization for this function.
+       */
+      if (unlikely(impl_copies == NULL)) {
+         return;
+      }
       _mesa_hash_table_init(&impl_copies->ht, state->mem_ctx,
                             _mesa_hash_pointer, _mesa_key_pointer_equal);
 
-      foreach_list_typed_safe(nir_cf_node, cf_node, node, &impl->body)
+      foreach_list_typed_safe(nir_cf_node, cf_node, node, &impl->body) {
          copy_prop_vars_cf_node(state, impl_copies, cf_node);
+      }
 
       clear_copies_structure(state, impl_copies);
 
@@ -1427,32 +1466,39 @@ copy_prop_vars_cf_node(struct copy_prop_
        *
        * We clone the copies for each branch of the if statement.  The idea is
        * that they both see the same state of available copies, but do not
-       * interfere to each other.
+       * interfere with each other.
        */
       if (!exec_list_is_empty(&if_stmt->then_list)) {
          struct copies *then_copies = get_copies_structure(state);
-         clone_copies(state, then_copies, copies);
+         /* CRITICAL FIX: Handle allocation failure. */
+         if (likely(then_copies != NULL)) {
+            clone_copies(state, then_copies, copies);
 
-         foreach_list_typed_safe(nir_cf_node, cf_node, node, &if_stmt->then_list)
-            copy_prop_vars_cf_node(state, then_copies, cf_node);
+            foreach_list_typed_safe(nir_cf_node, cf_node, node, &if_stmt->then_list) {
+               copy_prop_vars_cf_node(state, then_copies, cf_node);
+            }
 
-         clear_copies_structure(state, then_copies);
+            clear_copies_structure(state, then_copies);
+         }
       }
 
       if (!exec_list_is_empty(&if_stmt->else_list)) {
          struct copies *else_copies = get_copies_structure(state);
-         clone_copies(state, else_copies, copies);
+         /* CRITICAL FIX: Handle allocation failure. */
+         if (likely(else_copies != NULL)) {
+            clone_copies(state, else_copies, copies);
 
-         foreach_list_typed_safe(nir_cf_node, cf_node, node, &if_stmt->else_list)
-            copy_prop_vars_cf_node(state, else_copies, cf_node);
+            foreach_list_typed_safe(nir_cf_node, cf_node, node, &if_stmt->else_list) {
+               copy_prop_vars_cf_node(state, else_copies, cf_node);
+            }
 
-         clear_copies_structure(state, else_copies);
+            clear_copies_structure(state, else_copies);
+         }
       }
 
-      /* Both branches copies can be ignored, since the effect of running both
+      /* Both branches' copies can be ignored, since the effect of running both
        * branches was captured in the first pass that collects vars_written.
        */
-
       invalidate_copies_for_cf_node(state, copies, cf_node);
 
       break;
@@ -1465,16 +1511,19 @@ copy_prop_vars_cf_node(struct copy_prop_
       /* Invalidate before cloning the copies for the loop, since the loop
        * body can be executed more than once.
        */
-
       invalidate_copies_for_cf_node(state, copies, cf_node);
 
       struct copies *loop_copies = get_copies_structure(state);
-      clone_copies(state, loop_copies, copies);
+      /* CRITICAL FIX: Handle allocation failure. */
+      if (likely(loop_copies != NULL)) {
+         clone_copies(state, loop_copies, copies);
 
-      foreach_list_typed_safe(nir_cf_node, cf_node, node, &loop->body)
-         copy_prop_vars_cf_node(state, loop_copies, cf_node);
+         foreach_list_typed_safe(nir_cf_node, cf_node, node, &loop->body) {
+            copy_prop_vars_cf_node(state, loop_copies, cf_node);
+         }
 
-      clear_copies_structure(state, loop_copies);
+         clear_copies_structure(state, loop_copies);
+      }
 
       break;
    }

--- a/src/mesa/main/matrix.c	2025-10-02 20:44:54.337186391 +0200
+++ b/src/mesa/main/matrix.c	2025-10-02 20:45:45.884745099 +0200
@@ -376,6 +376,11 @@ pop_matrix( struct gl_context *ctx, stru
    if (stack->Depth == 0)
       return GL_FALSE;
 
+   /* Sync if in glthread to avoid racing with main-thread attribute calls */
+   if (ctx->GLThread.enabled) {
+      _mesa_glthread_finish_before(ctx, "pop_matrix");
+   }
+
    stack->Depth--;
 
    /* If the popped matrix is the same as the current one, treat it as
@@ -1008,8 +1013,13 @@ init_matrix_stack(struct gl_matrix_stack
    stack->Depth = 0;
    stack->MaxDepth = maxDepth;
    stack->DirtyFlag = dirtyFlag;
-   /* The stack will be dynamically resized at glPushMatrix() time */
    stack->Stack = os_malloc_aligned(sizeof(GLmatrix), 16);
+   if (stack->Stack == NULL) {
+      _mesa_error(NULL, GL_OUT_OF_MEMORY, "init_matrix_stack allocation failed");
+      stack->StackSize = 0;
+      stack->Top = NULL;
+      return;
+   }
    stack->StackSize = 1;
    _math_matrix_ctr(&stack->Stack[0]);
    stack->Top = stack->Stack;
@@ -1024,6 +1034,9 @@ init_matrix_stack(struct gl_matrix_stack
 static void
 free_matrix_stack( struct gl_matrix_stack *stack )
 {
+   if (stack->Stack == NULL) {
+      return;
+   }
    os_free_aligned(stack->Stack);
    stack->Stack = stack->Top = NULL;
    stack->StackSize = 0;
@@ -1049,21 +1062,43 @@ void _mesa_init_matrix( struct gl_contex
 {
    GLuint i;
 
-   /* Initialize matrix stacks */
+   // Initialize matrix stacks with OOM checks (holistic: Ensure all succeed or fail cleanly)
    init_matrix_stack(&ctx->ModelviewMatrixStack, MAX_MODELVIEW_STACK_DEPTH,
                      _NEW_MODELVIEW);
+   if (ctx->ModelviewMatrixStack.Stack == NULL) goto init_fail;  // Propagate OOM
+
    init_matrix_stack(&ctx->ProjectionMatrixStack, MAX_PROJECTION_STACK_DEPTH,
                      _NEW_PROJECTION);
-   for (i = 0; i < ARRAY_SIZE(ctx->TextureMatrixStack); i++)
+   if (ctx->ProjectionMatrixStack.Stack == NULL) goto init_fail;
+
+   for (i = 0; i < ARRAY_SIZE(ctx->TextureMatrixStack); i++) {
       init_matrix_stack(&ctx->TextureMatrixStack[i], MAX_TEXTURE_STACK_DEPTH,
                         _NEW_TEXTURE_MATRIX);
-   for (i = 0; i < ARRAY_SIZE(ctx->ProgramMatrixStack); i++)
+      if (ctx->TextureMatrixStack[i].Stack == NULL) goto init_fail;
+   }
+
+   for (i = 0; i < ARRAY_SIZE(ctx->ProgramMatrixStack); i++) {
       init_matrix_stack(&ctx->ProgramMatrixStack[i],
-		        MAX_PROGRAM_MATRIX_STACK_DEPTH, _NEW_TRACK_MATRIX);
+                        MAX_PROGRAM_MATRIX_STACK_DEPTH, _NEW_TRACK_MATRIX);
+      if (ctx->ProgramMatrixStack[i].Stack == NULL) goto init_fail;
+   }
+
    ctx->CurrentStack = &ctx->ModelviewMatrixStack;
 
-   /* Init combined Modelview*Projection matrix */
+   // Init combined Modelview*Projection matrix (safe after checks)
    _math_matrix_ctr( &ctx->_ModelProjectMatrix );
+
+   return;  // Success
+
+init_fail:
+   // Cleanup partial allocations to avoid leaks (memory-safe)
+   free_matrix_stack(&ctx->ModelviewMatrixStack);
+   free_matrix_stack(&ctx->ProjectionMatrixStack);
+   for (i = 0; i < ARRAY_SIZE(ctx->TextureMatrixStack); i++)
+      free_matrix_stack(&ctx->TextureMatrixStack[i]);
+   for (i = 0; i < ARRAY_SIZE(ctx->ProgramMatrixStack); i++)
+      free_matrix_stack(&ctx->ProgramMatrixStack[i]);
+   // Caller (init_attrib_groups) will handle failed init (e.g., context creation fails)
 }
