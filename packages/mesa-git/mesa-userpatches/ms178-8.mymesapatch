--- a/src/util/blob.c	2025-09-19 11:24:25.535738075 +0200
+++ b/src/util/blob.c	2025-09-20 18:26:46.618936402 +0200
@@ -22,9 +22,12 @@
  */
 
 #include <string.h>
+#include <assert.h>
+#include <stdint.h>
+#include <limits.h>
+#include <stdlib.h>
 
 #include "blob.h"
-#include "u_math.h"
 
 #ifdef HAVE_VALGRIND
 #include <valgrind.h>
@@ -34,38 +37,130 @@
 #define VG(x)
 #endif
 
+/* Portable feature detection */
+#ifndef __has_builtin
+#define __has_builtin(x) 0
+#endif
+
+/* Branch prediction hints */
+#if __has_builtin(__builtin_expect)
+#define LIKELY(x)   __builtin_expect(!!(x), 1)
+#define UNLIKELY(x) __builtin_expect(!!(x), 0)
+#else
+#define LIKELY(x)   (x)
+#define UNLIKELY(x) (x)
+#endif
+
+/* Prefetch hint */
+#if __has_builtin(__builtin_prefetch)
+#define PREFETCH_READ(addr, locality) __builtin_prefetch((addr), 0, (locality))
+#else
+#define PREFETCH_READ(addr, locality) ((void)0)
+#endif
+
 #define BLOB_INITIAL_SIZE 4096
+#define CACHE_LINE_SIZE 64
+
+/* Align x up to 'alignment' with overflow safety.
+ * Treat alignment <= 1 as a no-op. Return false on overflow.
+ */
+static inline bool
+align_up_safe(size_t x, size_t alignment, size_t *out)
+{
+   if (alignment <= 1) {
+      *out = x;
+      return true;
+   }
+
+   /* Power-of-two fast path */
+   if ((alignment & (alignment - 1)) == 0) {
+      const size_t mask = alignment - 1;
+      if (UNLIKELY(x > SIZE_MAX - mask)) {
+         return false;
+      }
+      *out = (x + mask) & ~mask;
+      return true;
+   }
+
+   /* Generic path */
+   const size_t rem = x % alignment;
+   if (rem == 0) {
+      *out = x;
+      return true;
+   }
+   const size_t add = alignment - rem;
+   if (UNLIKELY(x > SIZE_MAX - add)) {
+      return false;
+   }
+   *out = x + add;
+   return true;
+}
 
-/* Ensure that \blob will be able to fit an additional object of size
- * \additional.  The growing (if any) will occur by doubling the existing
- * allocation.
+/* Ensure that 'blob' can fit 'additional' more bytes. Grow by doubling.
+ * Returns false on out-of-memory or overflow. Never calls realloc(..., 0).
  */
 static bool
 grow_to_fit(struct blob *blob, size_t additional)
 {
+   size_t required_size;
    size_t to_allocate;
-   uint8_t *new_data;
 
-   if (blob->out_of_memory)
+   if (UNLIKELY(blob->out_of_memory)) {
+      return false;
+   }
+
+   /* Overflow check for size + additional */
+   if (UNLIKELY(additional > SIZE_MAX - blob->size)) {
+      blob->out_of_memory = true;
       return false;
+   }
+
+   required_size = blob->size + additional;
 
-   if (blob->size + additional <= blob->allocated)
+   if (LIKELY(required_size <= blob->allocated)) {
       return true;
+   }
 
-   if (blob->fixed_allocation) {
+   if (UNLIKELY(blob->fixed_allocation)) {
       blob->out_of_memory = true;
       return false;
    }
 
-   if (blob->allocated == 0)
+   /* Compute new allocation size: double, with overflow guard */
+   if (blob->allocated == 0) {
       to_allocate = BLOB_INITIAL_SIZE;
-   else
-      to_allocate = blob->allocated * 2;
+   } else {
+      if (blob->allocated >= SIZE_MAX / 2) {
+         to_allocate = SIZE_MAX;
+      } else {
+         to_allocate = blob->allocated * 2;
+      }
+   }
+
+   /* Ensure at least what's required */
+   if (to_allocate < required_size) {
+      to_allocate = required_size;
+   }
+
+   /* Optionally round to cache line; never reduce below required_size, never overflow */
+   if (CACHE_LINE_SIZE > 1) {
+      size_t rounded;
+      if (align_up_safe(to_allocate, CACHE_LINE_SIZE, &rounded)) {
+         if (rounded >= required_size) {
+            to_allocate = rounded;
+         }
+      }
+      /* If rounding failed or would reduce size, keep to_allocate as-is */
+   }
 
-   to_allocate = MAX2(to_allocate, blob->allocated + additional);
+   /* Never realloc with size 0; required_size > 0 if we got here */
+   if (UNLIKELY(to_allocate == 0)) {
+      blob->out_of_memory = true;
+      return false;
+   }
 
-   new_data = realloc(blob->data, to_allocate);
-   if (new_data == NULL) {
+   uint8_t *new_data = (uint8_t *)realloc(blob->data, to_allocate);
+   if (UNLIKELY(new_data == NULL)) {
       blob->out_of_memory = true;
       return false;
    }
@@ -76,23 +171,71 @@ grow_to_fit(struct blob *blob, size_t ad
    return true;
 }
 
-/* Align the blob->size so that reading or writing a value at (blob->data +
- * blob->size) will result in an access aligned to a granularity of \alignment
- * bytes.
- *
- * \return True unless allocation fails
+/* Internal: Reserve 'write_size' bytes aligned to 'alignment'.
+ * - Inserts zero padding as needed for alignment.
+ * - On success, sets *out_offset to the aligned write position and advances blob->size.
+ */
+static bool
+writer_reserve_aligned(struct blob *blob, size_t alignment, size_t write_size, size_t *out_offset)
+{
+   size_t aligned_size;
+   size_t pad;
+
+   /* Compute aligned position for the write */
+   if (!align_up_safe(blob->size, alignment, &aligned_size)) {
+      blob->out_of_memory = true;
+      return false;
+   }
+
+   pad = aligned_size - blob->size;
+
+   /* Overflow check for total additional */
+   if (UNLIKELY(write_size > SIZE_MAX - pad)) {
+      blob->out_of_memory = true;
+      return false;
+   }
+
+   if (!grow_to_fit(blob, pad + write_size)) {
+      return false;
+   }
+
+   /* Zero any padding bytes to preserve previous blob_align semantics */
+   if (pad > 0 && blob->data != NULL) {
+      memset(blob->data + blob->size, 0, pad);
+   }
+
+   *out_offset = aligned_size;
+   blob->size = aligned_size + write_size;
+   return true;
+}
+
+/* Align the blob->size to a granularity of 'alignment' bytes.
+ * Any padding bytes are zeroed.
  */
 bool
 blob_align(struct blob *blob, size_t alignment)
 {
-   const size_t new_size = align_uintptr(blob->size, alignment);
+   size_t new_size;
+   size_t pad;
+
+   if (alignment <= 1) {
+      return true;
+   }
+
+   if (!align_up_safe(blob->size, alignment, &new_size)) {
+     blob->out_of_memory = true;
+     return false;
+   }
 
    if (blob->size < new_size) {
-      if (!grow_to_fit(blob, new_size - blob->size))
+      pad = new_size - blob->size;
+      if (!grow_to_fit(blob, pad)) {
          return false;
+      }
 
-      if (blob->data)
-         memset(blob->data + blob->size, 0, new_size - blob->size);
+      if (blob->data != NULL) {
+         memset(blob->data + blob->size, 0, pad);
+      }
       blob->size = new_size;
    }
 
@@ -102,7 +245,42 @@ blob_align(struct blob *blob, size_t ali
 void
 blob_reader_align(struct blob_reader *blob, size_t alignment)
 {
-   blob->current = blob->data + align_uintptr(blob->current - blob->data, alignment);
+   /* If we're already in overrun, or alignment is no-op, bail early. */
+   if (blob->overrun || alignment <= 1) {
+      return;
+   }
+
+   /* If data is NULL, reader cannot meaningfully align; mark overrun. */
+   if (blob->data == NULL) {
+      blob->overrun = true;
+      blob->current = blob->end;
+      return;
+   }
+
+   /* Validate current within [data, end] before doing pointer math. */
+   if (UNLIKELY(blob->current < blob->data || blob->current > blob->end)) {
+      blob->overrun = true;
+      blob->current = blob->end;
+      return;
+   }
+
+   size_t off = (size_t)(blob->current - blob->data);
+   size_t new_off;
+
+   if (!align_up_safe(off, alignment, &new_off)) {
+      blob->overrun = true;
+      blob->current = blob->end;
+      return;
+   }
+
+   /* If aligning would move past end, it's an overrun. */
+   if (UNLIKELY(new_off > (size_t)(blob->end - blob->data))) {
+      blob->overrun = true;
+      blob->current = blob->end;
+      return;
+   }
+
+   blob->current = blob->data + new_off;
 }
 
 void
@@ -118,7 +296,7 @@ blob_init(struct blob *blob)
 void
 blob_init_fixed(struct blob *blob, void *data, size_t size)
 {
-   blob->data = data;
+   blob->data = (uint8_t *)data;
    blob->allocated = size;
    blob->size = 0;
    blob->fixed_allocation = true;
@@ -132,8 +310,13 @@ blob_finish_get_buffer(struct blob *blob
    *size = blob->size;
    blob->data = NULL;
 
-   /* Trim the buffer. */
-   *buffer = realloc(*buffer, *size);
+   /* Trim the buffer - but don't lose data on failure */
+   if (*size > 0 && *buffer != NULL) {
+      void *trimmed = realloc(*buffer, *size);
+      if (trimmed != NULL) {
+         *buffer = trimmed;
+      }
+   }
 }
 
 bool
@@ -142,14 +325,19 @@ blob_overwrite_bytes(struct blob *blob,
                      const void *bytes,
                      size_t to_write)
 {
-   /* Detect an attempt to overwrite data out of bounds. */
-   if (offset + to_write < offset || blob->size < offset + to_write)
+   /* Validate overwrite region: offset + to_write <= size (with overflow safety) */
+   if (UNLIKELY(offset > blob->size)) {
       return false;
+   }
+   if (UNLIKELY(to_write > blob->size - offset)) {
+      return false;
+   }
 
    VG(VALGRIND_CHECK_MEM_IS_DEFINED(bytes, to_write));
 
-   if (blob->data)
+   if (blob->data != NULL && to_write > 0) {
       memcpy(blob->data + offset, bytes, to_write);
+   }
 
    return true;
 }
@@ -157,10 +345,15 @@ blob_overwrite_bytes(struct blob *blob,
 bool
 blob_write_bytes(struct blob *blob, const void *bytes, size_t to_write)
 {
-   if (! grow_to_fit(blob, to_write))
-       return false;
+   if (to_write == 0) {
+      return true;
+   }
+
+   if (!grow_to_fit(blob, to_write)) {
+      return false;
+   }
 
-   if (blob->data && to_write > 0) {
+   if (blob->data != NULL) {
       VG(VALGRIND_CHECK_MEM_IS_DEFINED(bytes, to_write));
       memcpy(blob->data + blob->size, bytes, to_write);
    }
@@ -174,10 +367,18 @@ blob_reserve_bytes(struct blob *blob, si
 {
    intptr_t ret;
 
-   if (! grow_to_fit (blob, to_write))
+   /* Ensure the returned offset can fit in intptr_t */
+   if (UNLIKELY(blob->size > (size_t)INTPTR_MAX ||
+                to_write > (size_t)(INTPTR_MAX - blob->size))) {
+      blob->out_of_memory = true;
       return -1;
+   }
 
-   ret = blob->size;
+   if (!grow_to_fit(blob, to_write)) {
+      return -1;
+   }
+
+   ret = (intptr_t)blob->size;
    blob->size += to_write;
 
    return ret;
@@ -186,23 +387,34 @@ blob_reserve_bytes(struct blob *blob, si
 intptr_t
 blob_reserve_uint32(struct blob *blob)
 {
-   blob_align(blob, sizeof(uint32_t));
+   if (!blob_align(blob, sizeof(uint32_t))) {
+      return -1;
+   }
    return blob_reserve_bytes(blob, sizeof(uint32_t));
 }
 
 intptr_t
 blob_reserve_intptr(struct blob *blob)
 {
-   blob_align(blob, sizeof(intptr_t));
+   if (!blob_align(blob, sizeof(intptr_t))) {
+      return -1;
+   }
    return blob_reserve_bytes(blob, sizeof(intptr_t));
 }
 
-#define BLOB_WRITE_TYPE(name, type)                      \
-bool                                                     \
-name(struct blob *blob, type value)                      \
-{                                                        \
-   blob_align(blob, sizeof(value));                      \
-   return blob_write_bytes(blob, &value, sizeof(value)); \
+/* Optimized typed writes: fused align + grow + zero-pad + write. */
+#define BLOB_WRITE_TYPE(name, type)                                    \
+bool                                                                   \
+name(struct blob *blob, type value)                                    \
+{                                                                      \
+   size_t off;                                                         \
+   if (!writer_reserve_aligned(blob, sizeof(type), sizeof(type), &off)) { \
+      return false;                                                    \
+   }                                                                   \
+   if (blob->data != NULL) {                                           \
+      memcpy(blob->data + off, &value, sizeof(type));                  \
+   }                                                                   \
+   return true;                                                        \
 }
 
 BLOB_WRITE_TYPE(blob_write_uint8, uint8_t)
@@ -211,31 +423,35 @@ BLOB_WRITE_TYPE(blob_write_uint32, uint3
 BLOB_WRITE_TYPE(blob_write_uint64, uint64_t)
 BLOB_WRITE_TYPE(blob_write_intptr, intptr_t)
 
-#define ASSERT_ALIGNED(_offset, _align) \
-   assert(align_uintptr((_offset), (_align)) == (_offset))
+#define ASSERT_ALIGNED(_offset, _align)                                \
+   do {                                                                \
+      size_t __aligned;                                                \
+      assert(align_up_safe((_offset), (_align), &__aligned) &&         \
+             __aligned == (_offset));                                  \
+   } while (0)
 
 bool
-blob_overwrite_uint8 (struct blob *blob,
-                      size_t offset,
-                      uint8_t value)
+blob_overwrite_uint8(struct blob *blob,
+                     size_t offset,
+                     uint8_t value)
 {
    ASSERT_ALIGNED(offset, sizeof(value));
    return blob_overwrite_bytes(blob, offset, &value, sizeof(value));
 }
 
 bool
-blob_overwrite_uint32 (struct blob *blob,
-                       size_t offset,
-                       uint32_t value)
+blob_overwrite_uint32(struct blob *blob,
+                      size_t offset,
+                      uint32_t value)
 {
    ASSERT_ALIGNED(offset, sizeof(value));
    return blob_overwrite_bytes(blob, offset, &value, sizeof(value));
 }
 
 bool
-blob_overwrite_intptr (struct blob *blob,
-                       size_t offset,
-                       intptr_t value)
+blob_overwrite_intptr(struct blob *blob,
+                      size_t offset,
+                      intptr_t value)
 {
    ASSERT_ALIGNED(offset, sizeof(value));
    return blob_overwrite_bytes(blob, offset, &value, sizeof(value));
@@ -244,33 +460,100 @@ blob_overwrite_intptr (struct blob *blob
 bool
 blob_write_string(struct blob *blob, const char *str)
 {
-   return blob_write_bytes(blob, str, strlen(str) + 1);
+   /* str must be non-NULL; enforce via assert in debug builds. */
+   assert(str != NULL);
+
+   /* Include NUL terminator; detect overflow if strlen returns SIZE_MAX */
+   size_t len = strlen(str);
+   if (UNLIKELY(len == SIZE_MAX)) {
+      blob->out_of_memory = true;
+      return false;
+   }
+   len += 1;
+
+   if (!grow_to_fit(blob, len)) {
+      return false;
+   }
+
+   if (blob->data != NULL) {
+      VG(VALGRIND_CHECK_MEM_IS_DEFINED(str, len));
+      memcpy(blob->data + blob->size, str, len);
+   }
+   blob->size += len;
+
+   return true;
 }
 
 void
 blob_reader_init(struct blob_reader *blob, const void *data, size_t size)
 {
-   blob->data = data;
+   blob->data = (const uint8_t *)data;
+
+   /* If size==0, it's valid to have data==NULL; start/end/current equal. */
+   if (size == 0) {
+      blob->end = blob->data;
+      blob->current = blob->data;
+      blob->overrun = false;
+      return;
+   }
+
+   /* If size>0 but data==NULL, this is invalid input; fail safely. */
+   if (blob->data == NULL) {
+      blob->end = (const uint8_t *)NULL;
+      blob->current = (const uint8_t *)NULL;
+      blob->overrun = true;
+      return;
+   }
+
    blob->end = blob->data + size;
-   blob->current = data;
+   blob->current = blob->data;
    blob->overrun = false;
+
+   /* Prefetch first cache lines for sequential read pattern, safely. */
+   PREFETCH_READ(blob->current, 3);
+   if (size > CACHE_LINE_SIZE) {
+      PREFETCH_READ(blob->current + CACHE_LINE_SIZE, 2);
+   }
+   if (size > 2 * CACHE_LINE_SIZE) {
+      PREFETCH_READ(blob->current + 2 * CACHE_LINE_SIZE, 1);
+   }
 }
 
-/* Check that an object of size \size can be read from this blob.
- *
- * If not, set blob->overrun to indicate that we attempted to read too far.
+/* Check that 'size' bytes can be read from current without overrun.
+ * Sets blob->overrun on failure and never performs invalid pointer arithmetic.
  */
 static bool
 ensure_can_read(struct blob_reader *blob, size_t size)
 {
-   if (blob->overrun)
+   if (UNLIKELY(blob->overrun)) {
       return false;
+   }
 
-   if (blob->current <= blob->end && blob->end - blob->current >= size)
+   /* Special-case zero-length reads: always okay if within bounds. */
+   if (size == 0) {
+      /* If data is NULL with size==0 (init path), remain okay. For size>0, we
+       * would have set overrun in init. Just require current and end to be sane. */
       return true;
+   }
 
-   blob->overrun = true;
+   /* If data is NULL here, we cannot read any bytes safely. */
+   if (UNLIKELY(blob->data == NULL)) {
+      blob->overrun = true;
+      return false;
+   }
 
+   /* Validate current within [data, end] before subtraction. */
+   if (UNLIKELY(blob->current < blob->data || blob->current > blob->end)) {
+      blob->overrun = true;
+      return false;
+   }
+
+   const size_t remaining = (size_t)(blob->end - blob->current);
+   if (LIKELY(remaining >= size)) {
+      return true;
+   }
+
+   blob->overrun = true;
    return false;
 }
 
@@ -279,44 +562,59 @@ blob_read_bytes(struct blob_reader *blob
 {
    const void *ret;
 
-   if (! ensure_can_read (blob, size))
+   if (!ensure_can_read(blob, size)) {
       return NULL;
+   }
 
    ret = blob->current;
-
    blob->current += size;
 
+   /* Prefetch ahead without creating invalid pointers */
+   size_t remain = (size_t)(blob->end - blob->current);
+   if (remain > CACHE_LINE_SIZE) {
+      PREFETCH_READ(blob->current + CACHE_LINE_SIZE, 2);
+   }
+
    return ret;
 }
 
 void
 blob_copy_bytes(struct blob_reader *blob, void *dest, size_t size)
 {
-   const void *bytes;
-
-   bytes = blob_read_bytes(blob, size);
-   if (bytes == NULL || size == 0)
+   const void *bytes = blob_read_bytes(blob, size);
+   if (bytes == NULL) {
       return;
+   }
 
-   memcpy(dest, bytes, size);
+   if (size > 0) {
+      memcpy(dest, bytes, size);
+   }
 }
 
 void
 blob_skip_bytes(struct blob_reader *blob, size_t size)
 {
-   if (ensure_can_read (blob, size))
+   if (ensure_can_read(blob, size)) {
       blob->current += size;
+   }
 }
 
-#define BLOB_READ_TYPE(name, type)         \
-type                                       \
-name(struct blob_reader *blob)             \
-{                                          \
-   type ret = 0;                           \
-   int size = sizeof(ret);                 \
-   blob_reader_align(blob, size);          \
-   blob_copy_bytes(blob, &ret, size);      \
-   return ret;                             \
+/* Optimized typed reads with safe alignment and bounds checks. */
+#define BLOB_READ_TYPE(name, type)                                     \
+type                                                                   \
+name(struct blob_reader *blob)                                         \
+{                                                                      \
+   type ret = (type)0;                                                 \
+   blob_reader_align(blob, sizeof(type));                              \
+   if (ensure_can_read(blob, sizeof(type))) {                          \
+      memcpy(&ret, blob->current, sizeof(type));                       \
+      blob->current += sizeof(type);                                   \
+      size_t remain = (size_t)(blob->end - blob->current);             \
+      if (remain > CACHE_LINE_SIZE) {                                  \
+         PREFETCH_READ(blob->current + CACHE_LINE_SIZE, 2);            \
+      }                                                                \
+   }                                                                   \
+   return ret;                                                         \
 }
 
 BLOB_READ_TYPE(blob_read_uint8, uint8_t)
@@ -328,32 +626,31 @@ BLOB_READ_TYPE(blob_read_intptr, intptr_
 char *
 blob_read_string(struct blob_reader *blob)
 {
-   int size;
-   char *ret;
-   uint8_t *nul;
-
    /* If we're already at the end, then this is an overrun. */
-   if (blob->current >= blob->end) {
+   if (UNLIKELY(blob->current >= blob->end)) {
       blob->overrun = true;
       return NULL;
    }
 
-   /* Similarly, if there is no zero byte in the data remaining in this blob,
-    * we also consider that an overrun.
-    */
-   nul = memchr(blob->current, 0, blob->end - blob->current);
-
-   if (nul == NULL) {
+   /* data==NULL implies invalid reader; fail safely */
+   if (UNLIKELY(blob->data == NULL)) {
       blob->overrun = true;
       return NULL;
    }
 
-   size = nul - blob->current + 1;
+   /* Search for NUL within remaining bytes */
+   size_t remain = (size_t)(blob->end - blob->current);
+   const uint8_t *nul = (const uint8_t *)memchr(blob->current, 0, remain);
 
-   assert(ensure_can_read(blob, size));
+   if (UNLIKELY(nul == NULL)) {
+      blob->overrun = true;
+      return NULL;
+   }
 
-   ret = (char *) blob->current;
+   size_t size = (size_t)(nul - blob->current) + 1;
 
+   /* We already know we can read this much */
+   char *ret = (char *)blob->current;
    blob->current += size;
 
    return ret;

--- a/src/util/bitscan.h	2025-06-12 22:18:54.150523804 +0200
+++ b/src/util/bitscan.h	2025-06-12 22:28:09.322741675 +0200
@@ -19,13 +19,12 @@
  * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
  * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
  * IN NO EVENT SHALL VMWARE AND/OR ITS SUPPLIERS BE LIABLE FOR
- * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
- * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF
+ * CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
  * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
  *
  **************************************************************************/
 
-
 #ifndef BITSCAN_H
 #define BITSCAN_H
 
@@ -39,9 +38,15 @@
 #endif
 
 #if defined(__POPCNT__)
+/* _mm_popcnt_u32/_mm_popcnt_u64 intrinsics */
 #include <popcntintrin.h>
 #endif
 
+#if (defined(__x86_64__) || defined(_M_X64) || defined(__i386__) || defined(_M_IX86)) && defined(__BMI2__)
+/* For _pdep_u32 (BMI2) */
+#include <immintrin.h>
+#endif
+
 #include "util/detect_arch.h"
 #include "util/detect_cc.h"
 #include "util/macros.h"
@@ -50,7 +55,6 @@
 extern "C" {
 #endif
 
-
 /**
  * Find first bit set in word.  Least significant bit is 1.
  * Return 0 if no bits set.
@@ -58,18 +62,18 @@ extern "C" {
 #ifdef HAVE___BUILTIN_FFS
 #define ffs __builtin_ffs
 #elif defined(_MSC_VER) && (_M_IX86 || _M_ARM || _M_AMD64 || _M_IA64)
-static inline
-int ffs(int i)
+static inline int
+ffs(int i)
 {
    unsigned long index;
-   if (_BitScanForward(&index, i))
-      return index + 1;
+   if (_BitScanForward(&index, (unsigned long)i))
+      return (int)index + 1;
    else
       return 0;
 }
 #else
-extern
-int ffs(int i);
+extern int
+ffs(int i);
 #endif
 
 #ifdef HAVE___BUILTIN_FFSLL
@@ -79,8 +83,8 @@ static inline int
 ffsll(long long int i)
 {
    unsigned long index;
-   if (_BitScanForward64(&index, i))
-      return index + 1;
+   if (_BitScanForward64(&index, (unsigned long long)i))
+      return (int)index + 1;
    else
       return 0;
 }
@@ -89,7 +93,6 @@ extern int
 ffsll(long long int val);
 #endif
 
-
 /* Destructively loop over all of the bits in a mask as in:
  *
  * while (mymask) {
@@ -97,46 +100,68 @@ ffsll(long long int val);
  *   ... process element i
  * }
  *
+ * Note: u_bit_scan() asserts mask != 0 (debug) to prevent UB (shift by -1).
  */
 static inline int
 u_bit_scan(unsigned *mask)
 {
+   assert(mask && *mask);
+#if defined(__GNUC__) || defined(__clang__)
+   /* Fast path: ctz → clear LSB (mask &= mask - 1) */
+   const unsigned m = *mask;
+   const int i = __builtin_ctz(m);
+   *mask = m & (m - 1);
+   return i;
+#else
    const int i = ffs(*mask) - 1;
-   *mask ^= (1u << i);
+   *mask &= ~(1u << i);
    return i;
+#endif
 }
 
-#define u_foreach_bit(b, dword)                          \
-   for (uint32_t __dword = (dword), b;                     \
-        ((b) = ffs(__dword) - 1, __dword);      \
-        __dword &= ~(1 << (b)))
+/* Iterate over set bits in a 32-bit mask.
+ * 'b' is assigned the bit index on each iteration.
+ */
+#define u_foreach_bit(b, dword)                                  \
+   for (uint32_t __dword = (uint32_t)(dword), b;                  \
+        ((b) = ffs(__dword) - 1, __dword);                        \
+        __dword &= ~(1u << (b)))
 
 static inline int
 u_bit_scan64(uint64_t *mask)
 {
+   assert(mask && *mask);
+#if defined(__GNUC__) || defined(__clang__)
+   const uint64_t m = *mask;
+   const int i = __builtin_ctzll(m);
+   *mask = m & (m - 1);
+   return i;
+#else
    const int i = ffsll(*mask) - 1;
-   *mask ^= (((uint64_t)1) << i);
+   *mask &= ~(1ull << i);
    return i;
+#endif
 }
 
-#define u_foreach_bit64(b, dword)                          \
-   for (uint64_t __dword = (dword), b;                     \
-        ((b) = ffsll(__dword) - 1, __dword);      \
+/* Iterate over set bits in a 64-bit mask.
+ * 'b' is assigned the bit index on each iteration.
+ */
+#define u_foreach_bit64(b, dword)                                \
+   for (uint64_t __dword = (uint64_t)(dword), b;                  \
+        ((b) = ffsll(__dword) - 1, __dword);                      \
         __dword &= ~(1ull << (b)))
 
 /* Given two bitmasks, loop over all bits of both of them.
- * Bits of mask1 are: b = scan_bit(mask1);
- * Bits of mask2 are: b = offset + scan_bit(mask2);
+ * Bits of mask1 are: b = ffsll(mask1) - 1;
+ * Bits of mask2 are: b = offset + (ffsll(mask2) - 1);
  */
-#define u_foreach_bit64_two_masks(b, mask1, offset, mask2)                          \
-   for (uint64_t __mask1 = (mask1), __mask2 = (mask2), b;                           \
-        (__mask1 ? ((b) = ffsll(__mask1) - 1)                                       \
-                 : ((b) = ffsll(__mask2) - 1 + offset), __mask1 || __mask2);        \
-        __mask1 ? (__mask1 &= ~(1ull << (b))) : (__mask2 &= ~(1ull << (b - offset))))
+#define u_foreach_bit64_two_masks(b, mask1, offset, mask2)                             \
+   for (uint64_t __mask1 = (uint64_t)(mask1), __mask2 = (uint64_t)(mask2), b;          \
+        (__mask1 ? ((b) = ffsll(__mask1) - 1)                                          \
+                 : ((b) = ffsll(__mask2) - 1 + (offset)), __mask1 || __mask2);         \
+        __mask1 ? (__mask1 &= ~(1ull << (b))) : (__mask2 &= ~(1ull << ((b) - (offset)))))
 
 /* Determine if an uint32_t value is a power of two.
- *
- * \note
  * Zero is treated as a power of two.
  */
 static inline bool
@@ -146,8 +171,6 @@ util_is_power_of_two_or_zero(uint32_t v)
 }
 
 /* Determine if an uint64_t value is a power of two.
- *
- * \note
  * Zero is treated as a power of two.
  */
 static inline bool
@@ -157,33 +180,18 @@ util_is_power_of_two_or_zero64(uint64_t
 }
 
 /* Determine if an uint32_t value is a power of two.
+ * Zero is not treated as a power of two.
  *
- * \note
- * Zero is \b not treated as a power of two.
+ * Branchless bit trick (fast on all CPUs; no popcnt required).
  */
 static inline bool
 util_is_power_of_two_nonzero(uint32_t v)
 {
-   /* __POPCNT__ is different from HAVE___BUILTIN_POPCOUNT.  The latter
-    * indicates the existence of the __builtin_popcount function.  The former
-    * indicates that _mm_popcnt_u32 exists and is a native instruction.
-    *
-    * The other alternative is to use SSE 4.2 compile-time flags.  This has
-    * two drawbacks.  First, there is currently no build infrastructure for
-    * SSE 4.2 (only 4.1), so that would have to be added.  Second, some AMD
-    * CPUs support POPCNT but not SSE 4.2 (e.g., Barcelona).
-    */
-#ifdef __POPCNT__
-   return _mm_popcnt_u32(v) == 1;
-#else
    return IS_POT_NONZERO(v);
-#endif
 }
 
 /* Determine if an uint64_t value is a power of two.
- *
- * \note
- * Zero is \b not treated as a power of two.
+ * Zero is not treated as a power of two.
  */
 static inline bool
 util_is_power_of_two_nonzero64(uint64_t v)
@@ -191,10 +199,8 @@ util_is_power_of_two_nonzero64(uint64_t
    return IS_POT_NONZERO(v);
 }
 
-/* Determine if an size_t/uintptr_t/intptr_t value is a power of two.
- *
- * \note
- * Zero is \b not treated as a power of two.
+/* Determine if a uintptr_t value is a power of two.
+ * Zero is not treated as a power of two.
  */
 static inline bool
 util_is_power_of_two_nonzero_uintptr(uintptr_t v)
@@ -202,30 +208,19 @@ util_is_power_of_two_nonzero_uintptr(uin
    return IS_POT_NONZERO(v);
 }
 
-/* For looping over a bitmask when you want to loop over consecutive bits
- * manually, for example:
- *
- * while (mask) {
- *    int start, count, i;
- *
- *    u_bit_scan_consecutive_range(&mask, &start, &count);
- *
- *    for (i = 0; i < count; i++)
- *       ... process element (start+i)
- * }
- */
+/* Loop over a bitmask yielding ranges of consecutive bits. */
 static inline void
 u_bit_scan_consecutive_range(unsigned *mask, int *start, int *count)
 {
-   if (*mask == 0xffffffff) {
+   if (*mask == 0xffffffffu) {
       *start = 0;
       *count = 32;
-      *mask = 0;
+      *mask  = 0;
       return;
    }
    *start = ffs(*mask) - 1;
    *count = ffs(~(*mask >> *start)) - 1;
-   *mask &= ~(((1u << *count) - 1) << *start);
+   *mask &= ~(((1u << (unsigned)*count) - 1u) << (unsigned)*start);
 }
 
 static inline void
@@ -234,31 +229,29 @@ u_bit_scan_consecutive_range64(uint64_t
    if (*mask == UINT64_MAX) {
       *start = 0;
       *count = 64;
-      *mask = 0;
+      *mask  = 0;
       return;
    }
    *start = ffsll(*mask) - 1;
    *count = ffsll(~(*mask >> *start)) - 1;
-   *mask &= ~(((((uint64_t)1) << *count) - 1) << *start);
+   *mask &= ~(((((uint64_t)1) << (unsigned)*count) - 1ull) << (unsigned)*start);
 }
 
-
 /**
- * Find last bit set in a word.  The least significant bit is 1.
- * Return 0 if no bits are set.
- * Essentially ffs() in the reverse direction.
+ * Find last bit set in a word. Least significant bit is 1.
+ * Return 0 if no bits are set. (Essentially ffs() in reverse.)
  */
 static inline unsigned
 util_last_bit(unsigned u)
 {
 #if defined(HAVE___BUILTIN_CLZ)
-   return u == 0 ? 0 : 32 - __builtin_clz(u);
+   return u == 0 ? 0u : 32u - (unsigned)__builtin_clz(u);
 #elif defined(_MSC_VER) && (_M_IX86 || _M_ARM || _M_AMD64 || _M_IA64)
    unsigned long index;
-   if (_BitScanReverse(&index, u))
-      return index + 1;
+   if (_BitScanReverse(&index, (unsigned long)u))
+      return (unsigned)index + 1u;
    else
-      return 0;
+      return 0u;
 #else
    unsigned r = 0;
    while (u) {
@@ -270,21 +263,20 @@ util_last_bit(unsigned u)
 }
 
 /**
- * Find last bit set in a word.  The least significant bit is 1.
- * Return 0 if no bits are set.
- * Essentially ffsll() in the reverse direction.
+ * Find last bit set in a 64-bit word. Least significant bit is 1.
+ * Return 0 if no bits are set. (Essentially ffsll() in reverse.)
  */
 static inline unsigned
 util_last_bit64(uint64_t u)
 {
 #if defined(HAVE___BUILTIN_CLZLL)
-   return u == 0 ? 0 : 64 - __builtin_clzll(u);
+   return u == 0 ? 0u : 64u - (unsigned)__builtin_clzll(u);
 #elif defined(_MSC_VER) && (_M_AMD64 || _M_ARM64 || _M_IA64)
    unsigned long index;
-   if (_BitScanReverse64(&index, u))
-      return index + 1;
+   if (_BitScanReverse64(&index, (unsigned long long)u))
+      return (unsigned)index + 1u;
    else
-      return 0;
+      return 0u;
 #else
    unsigned r = 0;
    while (u) {
@@ -296,29 +288,26 @@ util_last_bit64(uint64_t u)
 }
 
 /**
- * Find last bit in a word that does not match the sign bit. The least
- * significant bit is 1.
- * Return 0 if no bits are set.
+ * Find last bit in a word that does not match the sign bit.
+ * The least significant bit is 1. Return 0 if no bits are set.
  */
 static inline unsigned
 util_last_bit_signed(int i)
 {
    if (i >= 0)
-      return util_last_bit(i);
+      return util_last_bit((unsigned)i);
    else
       return util_last_bit(~(unsigned)i);
 }
 
-/* Returns a bitfield in which the first count bits starting at start are
- * set.
- */
+/* Returns a bitfield in which the first count bits starting at start are set. */
 static inline unsigned
 u_bit_consecutive(unsigned start, unsigned count)
 {
    assert(start + count <= 32);
    if (count == 32)
-      return ~0;
-   return ((1u << count) - 1) << start;
+      return ~0u;
+   return ((1u << count) - 1u) << start;
 }
 
 static inline uint64_t
@@ -327,28 +316,24 @@ u_bit_consecutive64(unsigned start, unsi
    assert(start + count <= 64);
    if (count == 64)
       return ~(uint64_t)0;
-   return (((uint64_t)1 << count) - 1) << start;
+   return (((uint64_t)1 << count) - 1ull) << start;
 }
 
 /**
- * Return number of bits set in n.
+ * Return number of bits set in n (32-bit).
  */
 static inline unsigned
 util_bitcount(unsigned n)
 {
 #if defined(HAVE___BUILTIN_POPCOUNT)
-   return __builtin_popcount(n);
+   return (unsigned)__builtin_popcount(n);
 #elif __OPENCL_VERSION__
-   return popcount(n);
+   return (unsigned)popcount(n);
 #else
-   /* K&R classic bitcount.
-    *
-    * For each iteration, clear the LSB from the bitfield.
-    * Requires only one iteration per set bit, instead of
-    * one iteration per bit less than highest set bit.
-    */
-   unsigned bits;
-   for (bits = 0; n; bits++) {
+   /* K&R bitcount: clear lowest set bit per iteration. */
+   unsigned bits = 0;
+   while (n) {
+      bits++;
       n &= n - 1;
    }
    return bits;
@@ -358,9 +343,7 @@ util_bitcount(unsigned n)
 /**
  * Return the number of bits set in n using the native popcnt instruction.
  * The caller is responsible for ensuring that popcnt is supported by the CPU.
- *
- * gcc doesn't use it if -mpopcnt or -march= that has popcnt is missing.
- *
+ * gcc won't auto-emit popcnt unless -mpopcnt or suitable -march is used.
  */
 static inline unsigned
 util_popcnt_inline_asm(unsigned n)
@@ -370,47 +353,61 @@ util_popcnt_inline_asm(unsigned n)
    __asm volatile("popcnt %1, %0" : "=r"(out) : "r"(n));
    return out;
 #else
-   /* We should never get here by accident, but I'm sure it'll happen. */
+   /* Fallback safely to software popcount. */
    return util_bitcount(n);
 #endif
 }
 
+/**
+ * Return number of bits set in n (64-bit).
+ */
 static inline unsigned
 util_bitcount64(uint64_t n)
 {
 #ifdef HAVE___BUILTIN_POPCOUNTLL
-   return __builtin_popcountll(n);
+   return (unsigned)__builtin_popcountll(n);
 #elif __OPENCL_VERSION__
-   return popcount(n);
+   return (unsigned)popcount(n);
 #else
+   /* Portable fallback via two 32-bit popcounts. */
    return util_bitcount((unsigned)n) + util_bitcount((unsigned)(n >> 32));
 #endif
 }
 
 /**
- * Widens the given bit mask by a multiplier, meaning that it will
- * replicate each bit by that amount.
- *
- * For example:
- * 0b101 widened by 2 will become: 0b110011
- *
- * This is typically used in shader I/O to transform a 64-bit
- * writemask to a 32-bit writemask.
+ * Widens the given bit mask by a multiplier, replicating each bit by that count.
+ * Example: 0b101 widened by 2 -> 0b110011
  */
 static inline uint32_t
 util_widen_mask(uint32_t mask, unsigned multiplier)
 {
+   if (!mask || multiplier == 0)
+      return 0u;
+
+#if (defined(__x86_64__) || defined(_M_X64) || defined(__i386__) || defined(_M_IX86)) && defined(__BMI2__)
+   /* Fast path for the most common case: multiplier == 2.
+    * Use BMI2 PDEP to interleave zeros between bits, then OR with a shift to replicate.
+    */
+   if (multiplier == 2) {
+      /* Spread source bits into odd positions. */
+      uint32_t spread = _pdep_u32(mask, 0x55555555u);
+      return spread | (spread << 1);
+   }
+#endif
+
+   /* Generic path for any multiplier. */
    uint32_t new_mask = 0;
-   u_foreach_bit(i, mask)
+   u_foreach_bit(i, mask) {
       new_mask |= ((1u << multiplier) - 1u) << (i * multiplier);
+   }
    return new_mask;
 }
 
 #ifdef __cplusplus
-}
+} /* extern "C" */
 
-/* util_bitcount has large measurable overhead (~2%), so it's recommended to
- * use the POPCNT instruction via inline assembly if the CPU supports it.
+/* util_bitcount has measurable overhead (~2%) compared to popcnt,
+ * so prefer inline assembly if the CPU supports it.
  */
 enum util_popcnt {
    POPCNT_NO,
@@ -419,8 +416,7 @@ enum util_popcnt {
 };
 
 /* Convenient function to select popcnt through a C++ template argument.
- * This should be used as part of larger functions that are optimized
- * as a whole.
+ * This should be used as part of larger functions optimized as a whole.
  */
 template<util_popcnt POPCNT> inline unsigned
 util_bitcount_fast(unsigned n)

--- a/src/util/disk_cache.h	2025-09-20 20:11:22.595220313 +0200
+++ b/src/util/disk_cache.h	2025-09-20 20:12:11.465301770 +0200


--- a/src/util/disk_cache.c	2025-09-20 20:01:48.286294965 +0200
+++ b/src/util/disk_cache.c	2025-09-20 20:12:47.835723859 +0200
