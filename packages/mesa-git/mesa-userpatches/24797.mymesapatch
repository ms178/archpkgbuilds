From c3fb0b03e91c259bf052010cb7dd7cdd9b3e6604 Mon Sep 17 00:00:00 2001
From: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
Date: Sun, 20 Aug 2023 17:29:19 +0900
Subject: [PATCH 3/5] radv/amdgpu: Separate the concept of residency from
 use_global_list.

A BO can be always resident by two ways:
1. Through kernel bookkeeping. The BO is created with
   AMDGPU_GEM_CREATE_VM_ALWAYS_VALID and bo->is_local gets set to true.
2. Through the driver global BO list. On every submission, the global
   BO list is added to the CS's BO list.

Until now, use_global_list reflected either 1. or 2. . This commit
changes it to reflect 2. only, and update callsites that checks for
residency to use a new helper.
---
 src/amd/vulkan/radv_radeon_winsys.h           | 10 +++++++++-
 src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c |  4 ++--
 2 files changed, 11 insertions(+), 3 deletions(-)

diff --git a/src/amd/vulkan/radv_radeon_winsys.h b/src/amd/vulkan/radv_radeon_winsys.h
index 0e729b131d872..fdab5457df3e0 100644
--- a/src/amd/vulkan/radv_radeon_winsys.h
+++ b/src/amd/vulkan/radv_radeon_winsys.h
@@ -184,8 +184,10 @@ struct radeon_winsys_ctx;
 
 struct radeon_winsys_bo {
    uint64_t va;
+   /* buffer is created with AMDGPU_GEM_CREATE_VM_ALWAYS_VALID */
    bool is_local;
    bool vram_no_cpu_access;
+   /* buffer is added to the BO list of all submissions */
    bool use_global_list;
    enum radeon_bo_domain initial_domain;
 };
@@ -337,10 +339,16 @@ radv_buffer_get_va(const struct radeon_winsys_bo *bo)
    return bo->va;
 }
 
+static inline bool
+radv_buffer_is_resident(const struct radeon_winsys_bo *bo)
+{
+   return bo->use_global_list || bo->is_local;
+}
+
 static inline void
 radv_cs_add_buffer(struct radeon_winsys *ws, struct radeon_cmdbuf *cs, struct radeon_winsys_bo *bo)
 {
-   if (bo->use_global_list)
+   if (radv_buffer_is_resident(bo))
       return;
 
    ws->cs_add_buffer(cs, bo);
diff --git a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c
index 8c26c9fbcf123..33cd8162067b7 100644
--- a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c
+++ b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c
@@ -146,7 +146,7 @@ radv_amdgpu_winsys_bo_virtual_bind(struct radeon_winsys *_ws, struct radeon_wins
     * The issue still exists for non-global BO but it will be addressed later, once we are 100% it's
     * RADV fault (mostly because the solution looks more complicated).
     */
-   if (bo && bo->base.use_global_list) {
+   if (bo && radv_buffer_is_resident(&bo->base)) {
       bo = NULL;
       bo_offset = 0;
    }
@@ -506,7 +506,7 @@ radv_amdgpu_winsys_bo_create(struct radeon_winsys *_ws, uint64_t size, unsigned
 
    bo->bo = buf_handle;
    bo->base.initial_domain = initial_domain;
-   bo->base.use_global_list = bo->base.is_local;
+   bo->base.use_global_list = false;
    bo->priority = priority;
 
    r = amdgpu_bo_export(buf_handle, amdgpu_bo_handle_type_kms, &bo->bo_handle);
-- 
GitLab


From 5198fb497b4efab8585052ac9528e64d35e13f49 Mon Sep 17 00:00:00 2001
From: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
Date: Sun, 20 Aug 2023 18:05:03 +0900
Subject: [PATCH 4/5] radv/amdgpu: Use rbtree for the global BO list.

The main motivation is to deduplicate global BO list entries, while
maintaining references to all duplicates, as we cannot know which
duplicate might be destroyed and removed from the list first. rbtree
fits well for this purpose since duplicates will come adjacent in
iteration order, which we exploit to quickly deduplicate while copying
the BO list to submission.

Additional benefits include removal without full scan, and VA-sorted
iteration order which simplifies debug dump functions.
---
 src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c | 69 ++++++++-----------
 src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.h |  2 +
 src/amd/vulkan/winsys/amdgpu/radv_amdgpu_cs.c | 19 +++--
 .../vulkan/winsys/amdgpu/radv_amdgpu_winsys.c |  2 +-
 .../vulkan/winsys/amdgpu/radv_amdgpu_winsys.h |  4 +-
 5 files changed, 46 insertions(+), 50 deletions(-)

diff --git a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c
index 33cd8162067b7..db46da19b57e6 100644
--- a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c
+++ b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c
@@ -282,22 +282,30 @@ radv_amdgpu_log_bo(struct radv_amdgpu_winsys *ws, struct radv_amdgpu_winsys_bo *
 }
 
 static int
-radv_amdgpu_global_bo_list_add(struct radv_amdgpu_winsys *ws, struct radv_amdgpu_winsys_bo *bo)
+radv_amdgpu_bo_cmp(const struct rb_node *_a, const struct rb_node *_b)
 {
-   u_rwlock_wrlock(&ws->global_bo_list.lock);
-   if (ws->global_bo_list.count == ws->global_bo_list.capacity) {
-      unsigned capacity = MAX2(4, ws->global_bo_list.capacity * 2);
-      void *data = realloc(ws->global_bo_list.bos, capacity * sizeof(struct radv_amdgpu_winsys_bo *));
-      if (!data) {
-         u_rwlock_wrunlock(&ws->global_bo_list.lock);
-         return VK_ERROR_OUT_OF_HOST_MEMORY;
-      }
+   const struct radv_amdgpu_winsys_bo *a = rb_node_data(struct radv_amdgpu_winsys_bo, _a, global_list_node);
+   const struct radv_amdgpu_winsys_bo *b = rb_node_data(struct radv_amdgpu_winsys_bo, _b, global_list_node);
+   assert(!a->is_virtual && !b->is_virtual);
 
-      ws->global_bo_list.bos = (struct radv_amdgpu_winsys_bo **)data;
-      ws->global_bo_list.capacity = capacity;
-   }
+   if (a->base.va < b->base.va)
+      return -1;
+   if (a->base.va > b->base.va)
+      return 1;
 
-   ws->global_bo_list.bos[ws->global_bo_list.count++] = bo;
+   if (a->bo_handle > b->bo_handle)
+      return -1;
+   if (a->bo_handle < b->bo_handle)
+      return 1;
+   return 0;
+}
+
+static int
+radv_amdgpu_global_bo_list_add(struct radv_amdgpu_winsys *ws, struct radv_amdgpu_winsys_bo *bo)
+{
+   u_rwlock_wrlock(&ws->global_bo_list.lock);
+   rb_tree_insert(&ws->global_bo_list.bos, &bo->global_list_node, radv_amdgpu_bo_cmp);
+   ws->global_bo_list.count++;
    bo->base.use_global_list = true;
    u_rwlock_wrunlock(&ws->global_bo_list.lock);
    return VK_SUCCESS;
@@ -307,13 +315,10 @@ static void
 radv_amdgpu_global_bo_list_del(struct radv_amdgpu_winsys *ws, struct radv_amdgpu_winsys_bo *bo)
 {
    u_rwlock_wrlock(&ws->global_bo_list.lock);
-   for (unsigned i = ws->global_bo_list.count; i-- > 0;) {
-      if (ws->global_bo_list.bos[i] == bo) {
-         ws->global_bo_list.bos[i] = ws->global_bo_list.bos[ws->global_bo_list.count - 1];
-         --ws->global_bo_list.count;
-         bo->base.use_global_list = false;
-         break;
-      }
+   if (bo->base.use_global_list) {
+      rb_tree_remove(&ws->global_bo_list.bos, &bo->global_list_node);
+      ws->global_bo_list.count--;
+      bo->base.use_global_list = false;
    }
    u_rwlock_wrunlock(&ws->global_bo_list.lock);
 }
@@ -1022,28 +1027,12 @@ radv_amdgpu_dump_bo_ranges(struct radeon_winsys *_ws, FILE *file)
 {
    struct radv_amdgpu_winsys *ws = radv_amdgpu_winsys(_ws);
    if (ws->debug_all_bos) {
-      struct radv_amdgpu_winsys_bo **bos = NULL;
-      int i = 0;
-
       u_rwlock_rdlock(&ws->global_bo_list.lock);
-      bos = malloc(sizeof(*bos) * ws->global_bo_list.count);
-      if (!bos) {
-         u_rwlock_rdunlock(&ws->global_bo_list.lock);
-         fprintf(file, "  Failed to allocate memory to sort VA ranges for dumping\n");
-         return;
-      }
-
-      for (i = 0; i < ws->global_bo_list.count; i++) {
-         bos[i] = ws->global_bo_list.bos[i];
-      }
-      qsort(bos, ws->global_bo_list.count, sizeof(bos[0]), radv_amdgpu_bo_va_compare);
-
-      for (i = 0; i < ws->global_bo_list.count; ++i) {
-         fprintf(file, "  VA=%.16llx-%.16llx, handle=%d%s\n", (long long)radv_amdgpu_canonicalize_va(bos[i]->base.va),
-                 (long long)radv_amdgpu_canonicalize_va(bos[i]->base.va + bos[i]->size), bos[i]->bo_handle,
-                 bos[i]->is_virtual ? " sparse" : "");
+      rb_tree_foreach (struct radv_amdgpu_winsys_bo, bo, &ws->global_bo_list.bos, global_list_node) {
+         fprintf(file, "  VA=%.16llx-%.16llx, handle=%d%s\n", (long long)radv_amdgpu_canonicalize_va(bo->base.va),
+                 (long long)radv_amdgpu_canonicalize_va(bo->base.va + bo->size), bo->bo_handle,
+                 bo->is_virtual ? " sparse" : "");
       }
-      free(bos);
       u_rwlock_rdunlock(&ws->global_bo_list.lock);
    } else
       fprintf(file, "  To get BO VA ranges, please specify RADV_DEBUG=allbos\n");
diff --git a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.h b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.h
index ffe4e27077e23..e82b2bdbd5961 100644
--- a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.h
+++ b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.h
@@ -29,6 +29,7 @@
 #ifndef RADV_AMDGPU_BO_H
 #define RADV_AMDGPU_BO_H
 
+#include "util/rb_tree.h"
 #include "radv_amdgpu_winsys.h"
 
 struct radv_amdgpu_map_range {
@@ -50,6 +51,7 @@ struct radv_amdgpu_winsys_bo {
       struct {
          amdgpu_bo_handle bo;
          uint32_t bo_handle;
+         struct rb_node global_list_node;
       };
       /* virtual bo */
       struct {
diff --git a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_cs.c b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_cs.c
index 6e17a9ba10903..ac64ec5eac70b 100644
--- a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_cs.c
+++ b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_cs.c
@@ -860,12 +860,17 @@ radv_amdgpu_add_cs_array_to_bo_list(struct radeon_cmdbuf **cs_array, unsigned nu
 static unsigned
 radv_amdgpu_copy_global_bo_list(struct radv_amdgpu_winsys *ws, struct drm_amdgpu_bo_list_entry *handles)
 {
-   for (uint32_t i = 0; i < ws->global_bo_list.count; i++) {
-      handles[i].bo_handle = ws->global_bo_list.bos[i]->bo_handle;
-      handles[i].bo_priority = ws->global_bo_list.bos[i]->priority;
-   }
+   unsigned i = 0;
+   rb_tree_foreach (struct radv_amdgpu_winsys_bo, bo, &ws->global_bo_list.bos, global_list_node) {
+      /* Global BO list can contain duplicate entries if exported then imported. */
+      if (i != 0 && handles[i - 1].bo_handle == bo->bo_handle)
+         continue;
 
-   return ws->global_bo_list.count;
+      handles[i].bo_handle = bo->bo_handle;
+      handles[i].bo_priority = bo->priority;
+      ++i;
+   }
+   return i;
 }
 
 static VkResult
@@ -1282,8 +1287,8 @@ radv_amdgpu_winsys_get_cpu_addr(void *_cs, uint64_t addr)
       }
    }
    u_rwlock_rdlock(&cs->ws->global_bo_list.lock);
-   for (uint32_t i = 0; i < cs->ws->global_bo_list.count; i++) {
-      struct radv_amdgpu_winsys_bo *bo = cs->ws->global_bo_list.bos[i];
+   /* TODO: Use rb_tree search instead of iteration. */
+   rb_tree_foreach (struct radv_amdgpu_winsys_bo, bo, &cs->ws->global_bo_list.bos, global_list_node) {
       if (addr >= bo->base.va && addr - bo->base.va < bo->size) {
          if (amdgpu_bo_cpu_map(bo->bo, &ret) == 0) {
             u_rwlock_rdunlock(&cs->ws->global_bo_list.lock);
diff --git a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_winsys.c b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_winsys.c
index 8884c13d9fe33..3fe25301326a3 100644
--- a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_winsys.c
+++ b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_winsys.c
@@ -178,7 +178,6 @@ radv_amdgpu_winsys_destroy(struct radeon_winsys *rws)
       return;
 
    u_rwlock_destroy(&ws->global_bo_list.lock);
-   free(ws->global_bo_list.bos);
 
    if (ws->reserve_vmid)
       amdgpu_vm_unreserve_vmid(ws->dev, 0);
@@ -291,6 +290,7 @@ radv_amdgpu_winsys_create(int fd, uint64_t debug_flags, uint64_t perftest_flags,
    ws->perftest = perftest_flags;
    ws->zero_all_vram_allocs = debug_flags & RADV_DEBUG_ZERO_VRAM;
    u_rwlock_init(&ws->global_bo_list.lock);
+   rb_tree_init(&ws->global_bo_list.bos);
    list_inithead(&ws->log_bo_list);
    u_rwlock_init(&ws->log_bo_list_lock);
    ws->base.query_info = radv_amdgpu_winsys_query_info;
diff --git a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_winsys.h b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_winsys.h
index 5ebe34129baba..43b2482d2dbe6 100644
--- a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_winsys.h
+++ b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_winsys.h
@@ -35,6 +35,7 @@
 #include "ac_gpu_info.h"
 #include "radv_radeon_winsys.h"
 
+#include "util/rb_tree.h"
 #include "vk_sync.h"
 #include "vk_sync_timeline.h"
 
@@ -58,9 +59,8 @@ struct radv_amdgpu_winsys {
 
    /* Global BO list */
    struct {
-      struct radv_amdgpu_winsys_bo **bos;
+      struct rb_tree bos;
       uint32_t count;
-      uint32_t capacity;
       struct u_rwlock lock;
    } global_bo_list;
 
-- 
GitLab


From 81f19dc0055f8c912ce521873e2901c69db5f985 Mon Sep 17 00:00:00 2001
From: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
Date: Sun, 20 Aug 2023 18:09:50 +0900
Subject: [PATCH 5/5] radv/amdgpu: Remove virtual bo dump logic.

Virtual BOs cannot go into the global bo list. Accessing bo_handle is
invalid in such cases anyway.
---
 src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c | 5 ++---
 1 file changed, 2 insertions(+), 3 deletions(-)

diff --git a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c
index db46da19b57e6..042d4634fb481 100644
--- a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c
+++ b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c
@@ -1029,9 +1029,8 @@ radv_amdgpu_dump_bo_ranges(struct radeon_winsys *_ws, FILE *file)
    if (ws->debug_all_bos) {
       u_rwlock_rdlock(&ws->global_bo_list.lock);
       rb_tree_foreach (struct radv_amdgpu_winsys_bo, bo, &ws->global_bo_list.bos, global_list_node) {
-         fprintf(file, "  VA=%.16llx-%.16llx, handle=%d%s\n", (long long)radv_amdgpu_canonicalize_va(bo->base.va),
-                 (long long)radv_amdgpu_canonicalize_va(bo->base.va + bo->size), bo->bo_handle,
-                 bo->is_virtual ? " sparse" : "");
+         fprintf(file, "  VA=%.16llx-%.16llx, handle=%d\n", (long long)radv_amdgpu_canonicalize_va(bo->base.va),
+                 (long long)radv_amdgpu_canonicalize_va(bo->base.va + bo->size), bo->bo_handle);
       }
       u_rwlock_rdunlock(&ws->global_bo_list.lock);
    } else
-- 
GitLab

