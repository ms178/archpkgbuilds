--- a/src/amd/vulkan/radv_cp_dma.c	2025-10-12 22:28:18.734783670 +0200
+++ b/src/amd/vulkan/radv_cp_dma.c	2025-10-12 22:28:43.623278061 +0200
@@ -30,16 +27,20 @@
 /* Alignment for optimal performance. */
 #define SI_CPDMA_ALIGNMENT 32
 
+/* Ensure alignment is a power of two for bit-masking operations. */
+_Static_assert((SI_CPDMA_ALIGNMENT & (SI_CPDMA_ALIGNMENT - 1)) == 0,
+               "SI_CPDMA_ALIGNMENT must be power of 2");
+
 /* The max number of bytes that can be copied per packet. */
 static inline unsigned
 cp_dma_max_byte_count(enum amd_gfx_level gfx_level)
 {
-   unsigned max = gfx_level >= GFX11  ? 32767
+   unsigned max = gfx_level >= GFX11  ? 32767u
                   : gfx_level >= GFX9 ? S_415_BYTE_COUNT_GFX9(~0u)
                                       : S_415_BYTE_COUNT_GFX6(~0u);
 
    /* make it aligned for optimal performance */
-   return max & ~(SI_CPDMA_ALIGNMENT - 1);
+   return max & ~(SI_CPDMA_ALIGNMENT - 1u);
 }
 
 /* Emit a CP DMA packet to do a copy from one buffer to another, or to clear
@@ -51,53 +52,52 @@ radv_cs_emit_cp_dma(struct radv_device *
                     uint64_t src_va, unsigned size, unsigned flags)
 {
    const struct radv_physical_device *pdev = radv_device_physical(device);
-   const bool cp_dma_use_L2 = (flags & CP_DMA_USE_L2) && pdev->info.cp_dma_use_L2;
-   const bool cp_dma_use_mall = pdev->info.gfx_level == GFX12;
-   /* GFX12: TC_L2 means MALL, which should always be set. */
-   const bool cp_dma_tc_l2_flag = cp_dma_use_L2 || cp_dma_use_mall;
+   const enum amd_gfx_level gfx_level = pdev->info.gfx_level;
+   const bool cp_dma_use_L2 = __builtin_expect((flags & CP_DMA_USE_L2) && pdev->info.cp_dma_use_L2, 1);
    uint32_t header = 0, command = 0;
 
-   assert(size <= cp_dma_max_byte_count(pdev->info.gfx_level));
+   assert(size <= cp_dma_max_byte_count(gfx_level));
 
    radeon_check_space(device->ws, cs->b, 9);
-   if (pdev->info.gfx_level >= GFX9)
+
+   if (__builtin_expect(gfx_level >= GFX9, 1))
       command |= S_415_BYTE_COUNT_GFX9(size);
    else
       command |= S_415_BYTE_COUNT_GFX6(size);
 
    /* Sync flags. */
-   if (flags & CP_DMA_SYNC)
+   if (__builtin_expect(flags & CP_DMA_SYNC, 0))
       header |= S_411_CP_SYNC(1);
 
-   if (flags & CP_DMA_RAW_WAIT)
+   if (__builtin_expect(flags & CP_DMA_RAW_WAIT, 0))
       command |= S_415_RAW_WAIT(1);
 
    /* Src and dst flags. */
-   if (cp_dma_tc_l2_flag)
+   if (cp_dma_use_L2)
       header |= S_411_DST_SEL(V_411_DST_ADDR_TC_L2);
 
-   if (flags & CP_DMA_CLEAR)
+   if (__builtin_expect(flags & CP_DMA_CLEAR, 0))
       header |= S_411_SRC_SEL(V_411_DATA);
-   else if (cp_dma_tc_l2_flag)
+   else if (cp_dma_use_L2)
       header |= S_411_SRC_SEL(V_411_SRC_ADDR_TC_L2);
 
    radeon_begin(cs);
-   if (pdev->info.gfx_level >= GFX7) {
+   if (__builtin_expect(gfx_level >= GFX7, 1)) {
       radeon_emit(PKT3(PKT3_DMA_DATA, 5, predicating));
       radeon_emit(header);
-      radeon_emit(src_va);       /* SRC_ADDR_LO [31:0] */
-      radeon_emit(src_va >> 32); /* SRC_ADDR_HI [31:0] */
-      radeon_emit(dst_va);       /* DST_ADDR_LO [31:0] */
-      radeon_emit(dst_va >> 32); /* DST_ADDR_HI [31:0] */
+      radeon_emit((uint32_t)src_va);       /* SRC_ADDR_LO [31:0] */
+      radeon_emit((uint32_t)(src_va >> 32)); /* SRC_ADDR_HI [31:0] */
+      radeon_emit((uint32_t)dst_va);       /* DST_ADDR_LO [31:0] */
+      radeon_emit((uint32_t)(dst_va >> 32)); /* DST_ADDR_HI [31:0] */
       radeon_emit(command);
    } else {
-      assert(!cp_dma_tc_l2_flag);
+      assert(!cp_dma_use_L2);
       header |= S_411_SRC_ADDR_HI(src_va >> 32);
       radeon_emit(PKT3(PKT3_CP_DMA, 4, predicating));
-      radeon_emit(src_va);                  /* SRC_ADDR_LO [31:0] */
-      radeon_emit(header);                  /* SRC_ADDR_HI [15:0] + flags. */
-      radeon_emit(dst_va);                  /* DST_ADDR_LO [31:0] */
-      radeon_emit((dst_va >> 32) & 0xffff); /* DST_ADDR_HI [15:0] */
+      radeon_emit((uint32_t)src_va);                  /* SRC_ADDR_LO [31:0] */
+      radeon_emit(header);                            /* SRC_ADDR_HI [15:0] + flags. */
+      radeon_emit((uint32_t)dst_va);                  /* DST_ADDR_LO [31:0] */
+      radeon_emit((uint32_t)((dst_va >> 32) & 0xffffu)); /* DST_ADDR_HI [15:0] */
       radeon_emit(command);
    }
    radeon_end();
@@ -119,7 +119,10 @@ radv_emit_cp_dma(struct radv_cmd_buffer
     */
    if (flags & CP_DMA_SYNC) {
       if (cmd_buffer->qf == RADV_QUEUE_GENERAL) {
-         ac_emit_cp_pfp_sync_me(cs->b, cmd_buffer->state.predicating);
+         radeon_begin(cs);
+         radeon_emit(PKT3(PKT3_PFP_SYNC_ME, 0, cmd_buffer->state.predicating));
+         radeon_emit(0);
+         radeon_end();
       }
 
       /* CP will see the sync flag and wait for all DMAs to complete. */
@@ -153,37 +156,54 @@ radv_cs_cp_dma_prefetch(const struct rad
                         bool predicating)
 {
    const struct radv_physical_device *pdev = radv_device_physical(device);
-   struct radeon_winsys *ws = device->ws;
-   enum amd_gfx_level gfx_level = pdev->info.gfx_level;
-   uint32_t header = 0, command = 0;
+   const enum amd_gfx_level gfx_level = pdev->info.gfx_level;
 
-   if (gfx_level >= GFX11)
-      size = MIN2(size, 32768 - SI_CPDMA_ALIGNMENT);
+   if (__builtin_expect(gfx_level >= GFX11, 0)) {
+      /* For GFX11+, clamp to safe maximum to prevent overflow after alignment.
+       * Original uses 32768 - 32, but worst-case alignment can add 31 bytes,
+       * potentially exceeding the 32767 hardware limit. Use 32736 and rely on
+       * defensive clamping below. */
+      const unsigned gfx11_max = 32768u - SI_CPDMA_ALIGNMENT;
+      size = (size < gfx11_max) ? size : gfx11_max;
+   }
 
    assert(size <= cp_dma_max_byte_count(gfx_level));
 
-   radeon_check_space(ws, cs->b, 9);
+   radeon_check_space(device->ws, cs->b, 9);
 
-   uint64_t aligned_va = va & ~(SI_CPDMA_ALIGNMENT - 1);
-   uint64_t aligned_size = ((va + size + SI_CPDMA_ALIGNMENT - 1) & ~(SI_CPDMA_ALIGNMENT - 1)) - aligned_va;
+   /* Align VA down to 32-byte boundary and compute aligned region size.
+    * Note: For va near UINT64_MAX, (va + size) wraps, but unsigned arithmetic
+    * is well-defined and the alignment logic remains correct modulo 2^64. */
+   const uint64_t aligned_va = va & ~(uint64_t)(SI_CPDMA_ALIGNMENT - 1);
+   const uint64_t end_va = va + size;
+   const uint64_t aligned_end = (end_va + SI_CPDMA_ALIGNMENT - 1u) & ~(uint64_t)(SI_CPDMA_ALIGNMENT - 1);
+   uint64_t aligned_size = aligned_end - aligned_va;
+
+   /* Defense in depth: Clamp aligned_size to hardware maximum.
+    * Worst case: va=1, size=32736 â†’ aligned_size=32768, which exceeds GFX11's 32767 limit.
+    * Clamping ensures we never overflow the packet byte_count field. */
+   const unsigned hw_max = cp_dma_max_byte_count(gfx_level);
+   if (__builtin_expect(aligned_size > hw_max, 0)) {
+      aligned_size = hw_max;
+   }
 
-   if (gfx_level >= GFX9) {
-      command |= S_415_BYTE_COUNT_GFX9(aligned_size) | S_415_DISABLE_WR_CONFIRM_GFX9(1);
-      header |= S_411_DST_SEL(V_411_NOWHERE);
+   uint32_t header, command;
+
+   if (__builtin_expect(gfx_level >= GFX9, 1)) {
+      command = S_415_BYTE_COUNT_GFX9(aligned_size) | S_415_DISABLE_WR_CONFIRM_GFX9(1);
+      header = S_411_DST_SEL(V_411_NOWHERE) | S_411_SRC_SEL(V_411_SRC_ADDR_TC_L2);
    } else {
-      command |= S_415_BYTE_COUNT_GFX6(aligned_size) | S_415_DISABLE_WR_CONFIRM_GFX6(1);
-      header |= S_411_DST_SEL(V_411_DST_ADDR_TC_L2);
+      command = S_415_BYTE_COUNT_GFX6(aligned_size) | S_415_DISABLE_WR_CONFIRM_GFX6(1);
+      header = S_411_DST_SEL(V_411_DST_ADDR_TC_L2) | S_411_SRC_SEL(V_411_SRC_ADDR_TC_L2);
    }
 
-   header |= S_411_SRC_SEL(V_411_SRC_ADDR_TC_L2);
-
    radeon_begin(cs);
    radeon_emit(PKT3(PKT3_DMA_DATA, 5, predicating));
    radeon_emit(header);
-   radeon_emit(aligned_va);       /* SRC_ADDR_LO [31:0] */
-   radeon_emit(aligned_va >> 32); /* SRC_ADDR_HI [31:0] */
-   radeon_emit(aligned_va);       /* DST_ADDR_LO [31:0] */
-   radeon_emit(aligned_va >> 32); /* DST_ADDR_HI [31:0] */
+   radeon_emit((uint32_t)aligned_va);       /* SRC_ADDR_LO [31:0] */
+   radeon_emit((uint32_t)(aligned_va >> 32)); /* SRC_ADDR_HI [31:0] */
+   radeon_emit((uint32_t)aligned_va);       /* DST_ADDR_LO [31:0] */
+   radeon_emit((uint32_t)(aligned_va >> 32)); /* DST_ADDR_HI [31:0] */
    radeon_emit(command);
    radeon_end();
 }
@@ -244,11 +264,10 @@ radv_cp_dma_copy_memory(struct radv_cmd_
 {
    struct radv_device *device = radv_cmd_buffer_device(cmd_buffer);
    const struct radv_physical_device *pdev = radv_device_physical(device);
-   enum amd_gfx_level gfx_level = pdev->info.gfx_level;
-   uint64_t main_src_va, main_dest_va;
-   uint64_t skipped_size = 0, realign_size = 0;
+   const enum amd_gfx_level gfx_level = pdev->info.gfx_level;
+   const unsigned max_byte_count = cp_dma_max_byte_count(gfx_level);
 
-   if (!(pdev->info.cp_dma_use_L2 && pdev->info.gfx_level >= GFX9)) {
+   if (__builtin_expect(!(pdev->info.cp_dma_use_L2 && gfx_level >= GFX9), 0)) {
       /* Invalidate L2 in case "src_va" or "dest_va" were previously written through L2. */
       cmd_buffer->state.flush_bits |= RADV_CMD_FLAG_INV_L2;
    }
@@ -256,6 +275,34 @@ radv_cp_dma_copy_memory(struct radv_cmd_
    /* Assume that we are not going to sync after the last DMA operation. */
    cmd_buffer->state.dma_is_busy = true;
 
+   /* Fast path for GFX9+ (Vega, Navi, RDNA, etc.): No alignment workarounds needed.
+    * Eliminates all skipped_size/realign_size logic for modern GPUs. */
+   if (__builtin_expect(gfx_level >= GFX9, 1)) {
+      while (size) {
+         /* Use ternary instead of MIN2 macro to enable better constant propagation.
+          * Cast to unsigned is safe: size < max_byte_count, and max is <64KiB. */
+         const unsigned byte_count = (size < max_byte_count) ? (unsigned)size : max_byte_count;
+         unsigned dma_flags = CP_DMA_USE_L2;
+
+         radv_cp_dma_prepare(cmd_buffer, byte_count, size, &dma_flags);
+
+         /* CRITICAL: Do NOT clear CP_DMA_SYNC here!
+          * radv_cp_dma_prepare sets SYNC on the last iteration (byte_count == size),
+          * and we must preserve it to properly synchronize the final packet. */
+
+         radv_emit_cp_dma(cmd_buffer, dest_va, src_va, byte_count, dma_flags);
+
+         size -= byte_count;
+         src_va += byte_count;
+         dest_va += byte_count;
+      }
+      return;
+   }
+
+   /* Legacy path for GFX6-GFX8 (pre-Vega): Alignment workarounds required for Carrizo/Stoney. */
+   uint64_t main_src_va, main_dest_va;
+   uint64_t skipped_size = 0, realign_size = 0;
+
    if (pdev->info.family <= CHIP_CARRIZO || pdev->info.family == CHIP_STONEY) {
       /* If the size is not aligned, we must add a dummy copy at the end
        * just to align the internal counter. Otherwise, the DMA engine
@@ -275,30 +322,17 @@ radv_cp_dma_copy_memory(struct radv_cmd_
          size -= skipped_size;
       }
    }
+
    main_src_va = src_va + skipped_size;
    main_dest_va = dest_va + skipped_size;
 
    while (size) {
+      const unsigned byte_count = (size < max_byte_count) ? (unsigned)size : max_byte_count;
       unsigned dma_flags = 0;
-      unsigned byte_count = MIN2(size, cp_dma_max_byte_count(gfx_level));
-
-      if (pdev->info.gfx_level >= GFX9) {
-         /* DMA operations via L2 are coherent and faster.
-          * TODO: GFX7-GFX8 should also support this but it
-          * requires tests/benchmarks.
-          *
-          * Also enable on GFX9 so we can use L2 at rest on GFX9+. On Raven
-          * this didn't seem to be worse.
-          *
-          * Note that we only use CP DMA for sizes < RADV_BUFFER_OPS_CS_THRESHOLD,
-          * which is 4k at the moment, so this is really unlikely to cause
-          * significant thrashing.
-          */
-         dma_flags |= CP_DMA_USE_L2;
-      }
 
       radv_cp_dma_prepare(cmd_buffer, byte_count, size + skipped_size + realign_size, &dma_flags);
 
+      /* Clear SYNC in main loop; the tail operations (skipped_size or realign_size) will sync. */
       dma_flags &= ~CP_DMA_SYNC;
 
       radv_emit_cp_dma(cmd_buffer, main_dest_va, main_src_va, byte_count, dma_flags);
@@ -311,7 +345,7 @@ radv_cp_dma_copy_memory(struct radv_cmd_
    if (skipped_size) {
       unsigned dma_flags = 0;
 
-      radv_cp_dma_prepare(cmd_buffer, skipped_size, size + skipped_size + realign_size, &dma_flags);
+      radv_cp_dma_prepare(cmd_buffer, skipped_size, skipped_size + realign_size, &dma_flags);
 
       radv_emit_cp_dma(cmd_buffer, dest_va, src_va, skipped_size, dma_flags);
    }
@@ -322,41 +356,33 @@ radv_cp_dma_copy_memory(struct radv_cmd_
 void
 radv_cp_dma_fill_memory(struct radv_cmd_buffer *cmd_buffer, uint64_t va, uint64_t size, unsigned value)
 {
+   /* Early return for zero-size fills. Hint to compiler that this is rare. */
+   if (__builtin_expect(!size, 0))
+      return;
+
    struct radv_device *device = radv_cmd_buffer_device(cmd_buffer);
    const struct radv_physical_device *pdev = radv_device_physical(device);
+   const enum amd_gfx_level gfx_level = pdev->info.gfx_level;
+   const unsigned max_byte_count = cp_dma_max_byte_count(gfx_level);
+   const bool use_L2 = __builtin_expect(gfx_level >= GFX9, 1);
 
-   if (!size)
-      return;
-
-   if (!(pdev->info.cp_dma_use_L2 && pdev->info.gfx_level >= GFX9)) {
+   if (__builtin_expect(!use_L2, 0)) {
       /* Invalidate L2 in case "va" was previously written through L2. */
       cmd_buffer->state.flush_bits |= RADV_CMD_FLAG_INV_L2;
    }
 
    assert(va % 4 == 0 && size % 4 == 0);
 
-   enum amd_gfx_level gfx_level = pdev->info.gfx_level;
-
    /* Assume that we are not going to sync after the last DMA operation. */
    cmd_buffer->state.dma_is_busy = true;
 
    while (size) {
-      unsigned byte_count = MIN2(size, cp_dma_max_byte_count(gfx_level));
-      unsigned dma_flags = CP_DMA_CLEAR;
-
-      if (pdev->info.gfx_level >= GFX9) {
-         /* DMA operations via L2 are coherent and faster.
-          * TODO: GFX7-GFX8 should also support this but it
-          * requires tests/benchmarks.
-          *
-          * Also enable on GFX9 so we can use L2 at rest on GFX9+.
-          */
-         dma_flags |= CP_DMA_USE_L2;
-      }
+      const unsigned byte_count = (size < max_byte_count) ? (unsigned)size : max_byte_count;
+      unsigned dma_flags = CP_DMA_CLEAR | (use_L2 ? CP_DMA_USE_L2 : 0u);
 
       radv_cp_dma_prepare(cmd_buffer, byte_count, size, &dma_flags);
 
-      /* Emit the clear packet. */
+      /* Emit the clear packet. radv_cp_dma_prepare sets SYNC on the last iteration. */
       radv_emit_cp_dma(cmd_buffer, va, value, byte_count, dma_flags);
 
       size -= byte_count;


--- a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c	2025-10-12 21:36:53.990079978 +0200
+++ b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c	2025-10-12 21:48:00.888697690 +0200
@@ -45,7 +45,15 @@ radv_amdgpu_bo_va_op(struct radv_amdgpu_
          flags |= AMDGPU_VM_PAGE_WRITEABLE;
    }
 
-   size = align64(size, getpagesize());
+   /* Cache page size - getpagesize() returns a constant at runtime but may involve overhead */
+   static long cached_page_size = 0;
+   long page_sz = cached_page_size;
+   if (__builtin_expect(page_sz == 0, 0)) {
+      page_sz = getpagesize();
+      cached_page_size = page_sz; /* Benign race if multiple threads initialize; all write same value */
+   }
+
+   size = align64(size, (uint64_t)page_sz);
 
    return ac_drm_bo_va_op_raw(ws->dev, bo_handle, offset, size, addr, flags, ops);
 }
@@ -53,9 +61,10 @@ radv_amdgpu_bo_va_op(struct radv_amdgpu_
 static int
 bo_comparator(const void *ap, const void *bp)
 {
-   struct radv_amdgpu_bo *a = *(struct radv_amdgpu_bo *const *)ap;
-   struct radv_amdgpu_bo *b = *(struct radv_amdgpu_bo *const *)bp;
-   return (a > b) ? 1 : (a < b) ? -1 : 0;
+   const struct radv_amdgpu_winsys_bo *a = *(struct radv_amdgpu_winsys_bo *const *)ap;
+   const struct radv_amdgpu_winsys_bo *b = *(struct radv_amdgpu_winsys_bo *const *)bp;
+   /* Branchless comparison: (a > b) yields 1 or 0, (a < b) yields 1 or 0; subtract to get -1, 0, or 1 */
+   return (a > b) - (a < b);
 }
 
 static VkResult
@@ -75,20 +84,25 @@ radv_amdgpu_winsys_rebuild_bo_list(struc
    }
 
    uint32_t temp_bo_count = 0;
-   for (uint32_t i = 0; i < bo->range_count; ++i)
+   for (uint32_t i = 0; i < bo->range_count; ++i) {
       if (bo->ranges[i].bo)
          bo->bos[temp_bo_count++] = bo->ranges[i].bo;
+   }
 
-   qsort(bo->bos, temp_bo_count, sizeof(struct radv_amdgpu_winsys_bo *), &bo_comparator);
-
-   if (!temp_bo_count) {
+   if (temp_bo_count == 0) {
       bo->bo_count = 0;
+   } else if (temp_bo_count == 1) {
+      /* Fast path: single BO, no need to sort or deduplicate */
+      bo->bo_count = 1;
    } else {
+      qsort(bo->bos, temp_bo_count, sizeof(struct radv_amdgpu_winsys_bo *), &bo_comparator);
+
+      /* Deduplicate by comparing with last written position (better cache locality) */
       uint32_t final_bo_count = 1;
-      for (uint32_t i = 1; i < temp_bo_count; ++i)
-         if (bo->bos[i] != bo->bos[i - 1])
+      for (uint32_t i = 1; i < temp_bo_count; ++i) {
+         if (bo->bos[i] != bo->bos[final_bo_count - 1])
             bo->bos[final_bo_count++] = bo->bos[i];
-
+      }
       bo->bo_count = final_bo_count;
    }
 
@@ -109,13 +123,15 @@ static void
 radv_amdgpu_log_va_op(struct radv_amdgpu_winsys *ws, struct radv_amdgpu_winsys_bo *bo, uint64_t offset, uint64_t size,
                       uint64_t virtual_va)
 {
+   /* Early exit if no logging is enabled - this is the common case in release builds */
+   if (!ws->debug_log_bos && !ws->bo_history_logfile)
+      return;
+
    const uint64_t timestamp = os_time_get_nano();
-   uint64_t mapped_va = bo ? (bo->base.va + offset) : 0;
+   const uint64_t mapped_va = bo ? (bo->base.va + offset) : 0;
 
    if (ws->debug_log_bos) {
-      struct radv_amdgpu_winsys_bo_log *bo_log = NULL;
-
-      bo_log = calloc(1, sizeof(*bo_log));
+      struct radv_amdgpu_winsys_bo_log *bo_log = calloc(1, sizeof(*bo_log));
       if (!bo_log)
          return;
 
@@ -328,12 +344,14 @@ radv_amdgpu_winsys_bo_virtual_bind(struc
 static void
 radv_amdgpu_log_bo(struct radv_amdgpu_winsys *ws, struct radv_amdgpu_winsys_bo *bo, bool destroyed)
 {
+   /* Early exit if no logging is enabled - this is the common case in release builds */
+   if (!ws->debug_log_bos && !ws->bo_history_logfile)
+      return;
+
    const uint64_t timestamp = os_time_get_nano();
 
    if (ws->debug_log_bos) {
-      struct radv_amdgpu_winsys_bo_log *bo_log = NULL;
-
-      bo_log = calloc(1, sizeof(*bo_log));
+      struct radv_amdgpu_winsys_bo_log *bo_log = calloc(1, sizeof(*bo_log));
       if (!bo_log)
          return;
 
