--- a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c	2025-10-12 21:36:53.990079978 +0200
+++ b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_bo.c	2025-10-12 21:48:00.888697690 +0200
@@ -45,7 +45,15 @@ radv_amdgpu_bo_va_op(struct radv_amdgpu_
          flags |= AMDGPU_VM_PAGE_WRITEABLE;
    }
 
-   size = align64(size, getpagesize());
+   /* Cache page size - getpagesize() returns a constant at runtime but may involve overhead */
+   static long cached_page_size = 0;
+   long page_sz = cached_page_size;
+   if (__builtin_expect(page_sz == 0, 0)) {
+      page_sz = getpagesize();
+      cached_page_size = page_sz; /* Benign race if multiple threads initialize; all write same value */
+   }
+
+   size = align64(size, (uint64_t)page_sz);
 
    return ac_drm_bo_va_op_raw(ws->dev, bo_handle, offset, size, addr, flags, ops);
 }
@@ -53,9 +61,10 @@ radv_amdgpu_bo_va_op(struct radv_amdgpu_
 static int
 bo_comparator(const void *ap, const void *bp)
 {
-   struct radv_amdgpu_bo *a = *(struct radv_amdgpu_bo *const *)ap;
-   struct radv_amdgpu_bo *b = *(struct radv_amdgpu_bo *const *)bp;
-   return (a > b) ? 1 : (a < b) ? -1 : 0;
+   const struct radv_amdgpu_winsys_bo *a = *(struct radv_amdgpu_winsys_bo *const *)ap;
+   const struct radv_amdgpu_winsys_bo *b = *(struct radv_amdgpu_winsys_bo *const *)bp;
+   /* Branchless comparison: (a > b) yields 1 or 0, (a < b) yields 1 or 0; subtract to get -1, 0, or 1 */
+   return (a > b) - (a < b);
 }
 
 static VkResult
@@ -75,20 +84,25 @@ radv_amdgpu_winsys_rebuild_bo_list(struc
    }
 
    uint32_t temp_bo_count = 0;
-   for (uint32_t i = 0; i < bo->range_count; ++i)
+   for (uint32_t i = 0; i < bo->range_count; ++i) {
       if (bo->ranges[i].bo)
          bo->bos[temp_bo_count++] = bo->ranges[i].bo;
+   }
 
-   qsort(bo->bos, temp_bo_count, sizeof(struct radv_amdgpu_winsys_bo *), &bo_comparator);
-
-   if (!temp_bo_count) {
+   if (temp_bo_count == 0) {
       bo->bo_count = 0;
+   } else if (temp_bo_count == 1) {
+      /* Fast path: single BO, no need to sort or deduplicate */
+      bo->bo_count = 1;
    } else {
+      qsort(bo->bos, temp_bo_count, sizeof(struct radv_amdgpu_winsys_bo *), &bo_comparator);
+
+      /* Deduplicate by comparing with last written position (better cache locality) */
       uint32_t final_bo_count = 1;
-      for (uint32_t i = 1; i < temp_bo_count; ++i)
-         if (bo->bos[i] != bo->bos[i - 1])
+      for (uint32_t i = 1; i < temp_bo_count; ++i) {
+         if (bo->bos[i] != bo->bos[final_bo_count - 1])
             bo->bos[final_bo_count++] = bo->bos[i];
-
+      }
       bo->bo_count = final_bo_count;
    }
 
@@ -109,13 +123,15 @@ static void
 radv_amdgpu_log_va_op(struct radv_amdgpu_winsys *ws, struct radv_amdgpu_winsys_bo *bo, uint64_t offset, uint64_t size,
                       uint64_t virtual_va)
 {
+   /* Early exit if no logging is enabled - this is the common case in release builds */
+   if (!ws->debug_log_bos && !ws->bo_history_logfile)
+      return;
+
    const uint64_t timestamp = os_time_get_nano();
-   uint64_t mapped_va = bo ? (bo->base.va + offset) : 0;
+   const uint64_t mapped_va = bo ? (bo->base.va + offset) : 0;
 
    if (ws->debug_log_bos) {
-      struct radv_amdgpu_winsys_bo_log *bo_log = NULL;
-
-      bo_log = calloc(1, sizeof(*bo_log));
+      struct radv_amdgpu_winsys_bo_log *bo_log = calloc(1, sizeof(*bo_log));
       if (!bo_log)
          return;
 
@@ -328,12 +344,14 @@ radv_amdgpu_winsys_bo_virtual_bind(struc
 static void
 radv_amdgpu_log_bo(struct radv_amdgpu_winsys *ws, struct radv_amdgpu_winsys_bo *bo, bool destroyed)
 {
+   /* Early exit if no logging is enabled - this is the common case in release builds */
+   if (!ws->debug_log_bos && !ws->bo_history_logfile)
+      return;
+
    const uint64_t timestamp = os_time_get_nano();
 
    if (ws->debug_log_bos) {
-      struct radv_amdgpu_winsys_bo_log *bo_log = NULL;
-
-      bo_log = calloc(1, sizeof(*bo_log));
+      struct radv_amdgpu_winsys_bo_log *bo_log = calloc(1, sizeof(*bo_log));
       if (!bo_log)
          return;
 
