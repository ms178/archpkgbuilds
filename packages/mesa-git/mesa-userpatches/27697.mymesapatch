From 315c2285e1079625899fe8231293fcc56f3cbd6f Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Fri, 16 Feb 2024 11:09:22 +0000
Subject: [PATCH 01/12] aco: don't combine linear and normal VGPR copies

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
Cc: mesa-stable
---
 src/amd/compiler/aco_lower_to_hw_instr.cpp  |  3 ++
 src/amd/compiler/tests/test_to_hw_instr.cpp | 50 +++++++++++++++++++++
 2 files changed, 53 insertions(+)

diff --git a/src/amd/compiler/aco_lower_to_hw_instr.cpp b/src/amd/compiler/aco_lower_to_hw_instr.cpp
index 27787c7322f97..03282ea28571c 100644
--- a/src/amd/compiler/aco_lower_to_hw_instr.cpp
+++ b/src/amd/compiler/aco_lower_to_hw_instr.cpp
@@ -1767,6 +1767,9 @@ try_coalesce_copies(lower_context* ctx, std::map<PhysReg, copy_operation>& copy_
        copy.op.isConstant() != other->second.op.isConstant())
       return;
 
+   if (other->second.def.regClass().is_linear_vgpr() != copy.def.regClass().is_linear_vgpr())
+      return;
+
    /* don't create 64-bit copies before GFX10 */
    if (copy.bytes >= 4 && copy.def.regClass().type() == RegType::vgpr &&
        ctx->program->gfx_level < GFX10)
diff --git a/src/amd/compiler/tests/test_to_hw_instr.cpp b/src/amd/compiler/tests/test_to_hw_instr.cpp
index e14ff7e56f4f7..4feb610b3e42e 100644
--- a/src/amd/compiler/tests/test_to_hw_instr.cpp
+++ b/src/amd/compiler/tests/test_to_hw_instr.cpp
@@ -839,6 +839,56 @@ BEGIN_TEST(to_hw_instr.copy_linear_vgpr_v3)
    finish_to_hw_instr_test();
 END_TEST
 
+BEGIN_TEST(to_hw_instr.copy_linear_vgpr_coalesce)
+   if (!setup_cs(NULL, GFX10))
+      return;
+
+   PhysReg reg_v0{256};
+   PhysReg reg_v1{256 + 1};
+   PhysReg reg_v4{256 + 4};
+   PhysReg reg_v5{256 + 5};
+   RegClass v1_linear = v1.as_linear();
+
+   //>> p_unit_test 0
+   //! lv2: %0:v[0-1] = v_lshrrev_b64 0, %0:v[4-5]
+   //! s2: %0:exec,  s1: %0:scc = s_not_b64 %0:exec
+   //! lv2: %0:v[0-1] = v_lshrrev_b64 0, %0:v[4-5]
+   //! s2: %0:exec,  s1: %0:scc = s_not_b64 %0:exec
+   bld.pseudo(aco_opcode::p_unit_test, Operand::zero());
+
+   Instruction* instr = bld.pseudo(aco_opcode::p_parallelcopy, Definition(reg_v0, v1_linear),
+                                   Definition(reg_v1, v1_linear), Operand(reg_v4, v1_linear),
+                                   Operand(reg_v5, v1_linear));
+   instr->pseudo().scratch_sgpr = m0;
+
+   //! p_unit_test 1
+   //! lv1: %0:v[0] = v_mov_b32 %0:v[4]
+   //! s2: %0:exec,  s1: %0:scc = s_not_b64 %0:exec
+   //! lv1: %0:v[0] = v_mov_b32 %0:v[4]
+   //! s2: %0:exec,  s1: %0:scc = s_not_b64 %0:exec
+   //! v1: %0:v[1] = v_mov_b32 %0:v[5]
+   bld.pseudo(aco_opcode::p_unit_test, Operand::c32(1));
+
+   instr = bld.pseudo(aco_opcode::p_parallelcopy, Definition(reg_v0, v1_linear),
+                      Definition(reg_v1, v1), Operand(reg_v4, v1_linear), Operand(reg_v5, v1));
+   instr->pseudo().scratch_sgpr = m0;
+
+   //! p_unit_test 2
+   //! v1: %0:v[0] = v_mov_b32 %0:v[4]
+   //! lv1: %0:v[1] = v_mov_b32 %0:v[5]
+   //! s2: %0:exec,  s1: %0:scc = s_not_b64 %0:exec
+   //! lv1: %0:v[1] = v_mov_b32 %0:v[5]
+   //! s2: %0:exec,  s1: %0:scc = s_not_b64 %0:exec
+   bld.pseudo(aco_opcode::p_unit_test, Operand::c32(2));
+
+   instr =
+      bld.pseudo(aco_opcode::p_parallelcopy, Definition(reg_v0, v1), Definition(reg_v1, v1_linear),
+                 Operand(reg_v4, v1), Operand(reg_v5, v1_linear));
+   instr->pseudo().scratch_sgpr = m0;
+
+   finish_to_hw_instr_test();
+END_TEST
+
 BEGIN_TEST(to_hw_instr.pack2x16_constant)
    PhysReg v0_lo{256};
    PhysReg v0_hi{256};
-- 
GitLab


From 3d1dda223a3337452a6bd1c48e7aac5f276a453a Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Fri, 2 Feb 2024 15:39:36 +0000
Subject: [PATCH 02/12] aco: optimize purely linear VGPR copies

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_lower_to_hw_instr.cpp  | 45 ++++++++++++++++++++-
 src/amd/compiler/tests/test_to_hw_instr.cpp | 20 +++++----
 2 files changed, 57 insertions(+), 8 deletions(-)

diff --git a/src/amd/compiler/aco_lower_to_hw_instr.cpp b/src/amd/compiler/aco_lower_to_hw_instr.cpp
index 03282ea28571c..263f40a29c0f3 100644
--- a/src/amd/compiler/aco_lower_to_hw_instr.cpp
+++ b/src/amd/compiler/aco_lower_to_hw_instr.cpp
@@ -2169,6 +2169,44 @@ handle_operands(std::map<PhysReg, copy_operation>& copy_map, lower_context* ctx,
       ctx->instructions.size() - num_instructions_before;
 }
 
+/* If all copies are linear VGPR copies, we can do a special path which only has two s_not instead
+ * of two for each copy.
+ */
+void
+handle_operands_linear_vgpr(std::map<PhysReg, copy_operation>& copy_map, lower_context* ctx,
+                            amd_gfx_level gfx_level, Pseudo_instruction* pi)
+{
+   Builder bld(ctx->program, &ctx->instructions);
+
+   for (auto& copy : copy_map) {
+      copy.second.op =
+         Operand(copy.second.op.physReg(), RegClass::get(RegType::vgpr, copy.second.op.bytes()));
+      copy.second.def = Definition(copy.second.def.physReg(),
+                                   RegClass::get(RegType::vgpr, copy.second.def.bytes()));
+   }
+
+   std::map<PhysReg, copy_operation> second_map(copy_map);
+   handle_operands(second_map, ctx, gfx_level, pi);
+
+   bool tmp_in_scc = pi->tmp_in_scc;
+   if (tmp_in_scc) {
+      bld.sop1(aco_opcode::s_mov_b32, Definition(pi->scratch_sgpr, s1), Operand(scc, s1));
+      pi->tmp_in_scc = false;
+   }
+   bld.sop1(Builder::s_not, Definition(exec, bld.lm), Definition(scc, s1), Operand(exec, bld.lm));
+
+   handle_operands(copy_map, ctx, gfx_level, pi);
+
+   bld.sop1(Builder::s_not, Definition(exec, bld.lm), Definition(scc, s1), Operand(exec, bld.lm));
+   if (tmp_in_scc) {
+      bld.sopc(aco_opcode::s_cmp_lg_i32, Definition(scc, s1), Operand(pi->scratch_sgpr, s1),
+               Operand::zero());
+      pi->tmp_in_scc = true;
+   }
+
+   ctx->program->statistics[aco_statistic_copies] += tmp_in_scc ? 4 : 2;
+}
+
 void
 emit_set_mode(Builder& bld, float_mode new_mode, bool set_round, bool set_denorm)
 {
@@ -2403,12 +2441,17 @@ lower_to_hw_instr(Program* program)
             }
             case aco_opcode::p_parallelcopy: {
                std::map<PhysReg, copy_operation> copy_operations;
+               bool all_linear_vgpr = true;
                for (unsigned j = 0; j < instr->operands.size(); j++) {
                   assert(instr->definitions[j].bytes() == instr->operands[j].bytes());
                   copy_operations[instr->definitions[j].physReg()] = {
                      instr->operands[j], instr->definitions[j], instr->operands[j].bytes()};
+                  all_linear_vgpr &= instr->definitions[j].regClass().is_linear_vgpr();
                }
-               handle_operands(copy_operations, &ctx, program->gfx_level, pi);
+               if (all_linear_vgpr)
+                  handle_operands_linear_vgpr(copy_operations, &ctx, program->gfx_level, pi);
+               else
+                  handle_operands(copy_operations, &ctx, program->gfx_level, pi);
                break;
             }
             case aco_opcode::p_start_linear_vgpr: {
diff --git a/src/amd/compiler/tests/test_to_hw_instr.cpp b/src/amd/compiler/tests/test_to_hw_instr.cpp
index 4feb610b3e42e..01251c5dfce22 100644
--- a/src/amd/compiler/tests/test_to_hw_instr.cpp
+++ b/src/amd/compiler/tests/test_to_hw_instr.cpp
@@ -796,18 +796,21 @@ BEGIN_TEST(to_hw_instr.swap_linear_vgpr)
 
    PhysReg reg_v0{256};
    PhysReg reg_v1{257};
+   PhysReg reg_v8{256 + 8};
    RegClass v1_linear = v1.as_linear();
 
    //>> p_unit_test 0
    bld.pseudo(aco_opcode::p_unit_test, Operand::zero());
 
+   //! v1: %0:v[8] = v_mov_b32 0
    //! lv1: %0:v[0],  lv1: %0:v[1] = v_swap_b32 %0:v[1], %0:v[0]
    //! s2: %0:exec,  s1: %0:scc = s_not_b64 %0:exec
    //! lv1: %0:v[0],  lv1: %0:v[1] = v_swap_b32 %0:v[1], %0:v[0]
    //! s2: %0:exec,  s1: %0:scc = s_not_b64 %0:exec
-   Instruction* instr = bld.pseudo(aco_opcode::p_parallelcopy, Definition(reg_v0, v1_linear),
-                                   Definition(reg_v1, v1_linear), Operand(reg_v1, v1_linear),
-                                   Operand(reg_v0, v1_linear));
+   Instruction* instr =
+      bld.pseudo(aco_opcode::p_parallelcopy, Definition(reg_v0, v1_linear),
+                 Definition(reg_v1, v1_linear), Definition(reg_v8, v1), Operand(reg_v1, v1_linear),
+                 Operand(reg_v0, v1_linear), Operand::zero());
    instr->pseudo().scratch_sgpr = m0;
 
    finish_to_hw_instr_test();
@@ -819,6 +822,7 @@ BEGIN_TEST(to_hw_instr.copy_linear_vgpr_v3)
 
    PhysReg reg_v0{256};
    PhysReg reg_v4{256 + 4};
+   PhysReg reg_v8{256 + 8};
    RegClass v3_linear = v3.as_linear();
 
    //>> p_unit_test 0
@@ -832,8 +836,10 @@ BEGIN_TEST(to_hw_instr.copy_linear_vgpr_v3)
    //! s2: %0:exec,  s1: %0:scc = s_not_b64 %0:exec
    //! lv1: %0:v[2] = v_mov_b32 %0:v[6]
    //! s2: %0:exec,  s1: %0:scc = s_not_b64 %0:exec
-   Instruction* instr = bld.pseudo(aco_opcode::p_parallelcopy, Definition(reg_v0, v3_linear),
-                                   Operand(reg_v4, v3_linear));
+   //! v1: %0:v[8] = v_mov_b32 0
+   Instruction* instr =
+      bld.pseudo(aco_opcode::p_parallelcopy, Definition(reg_v0, v3_linear), Definition(reg_v8, v1),
+                 Operand(reg_v4, v3_linear), Operand::zero());
    instr->pseudo().scratch_sgpr = m0;
 
    finish_to_hw_instr_test();
@@ -850,9 +856,9 @@ BEGIN_TEST(to_hw_instr.copy_linear_vgpr_coalesce)
    RegClass v1_linear = v1.as_linear();
 
    //>> p_unit_test 0
-   //! lv2: %0:v[0-1] = v_lshrrev_b64 0, %0:v[4-5]
+   //! v2: %0:v[0-1] = v_lshrrev_b64 0, %0:v[4-5]
    //! s2: %0:exec,  s1: %0:scc = s_not_b64 %0:exec
-   //! lv2: %0:v[0-1] = v_lshrrev_b64 0, %0:v[4-5]
+   //! v2: %0:v[0-1] = v_lshrrev_b64 0, %0:v[4-5]
    //! s2: %0:exec,  s1: %0:scc = s_not_b64 %0:exec
    bld.pseudo(aco_opcode::p_unit_test, Operand::zero());
 
-- 
GitLab


From 55efdec05d83c5933e0359462570c0ca417f860c Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Mon, 29 Jan 2024 17:54:34 +0000
Subject: [PATCH 03/12] aco/ra: disable p_start_linear_vgpr allocation hint

As this is, this will become useless soon.

fossil-db (navi31):
Totals from 176 (0.22% of 79242) affected shaders:
Instrs: 101932 -> 102413 (+0.47%); split: -0.01%, +0.49%
CodeSize: 541352 -> 543256 (+0.35%); split: -0.01%, +0.36%
VGPRs: 7884 -> 7896 (+0.15%)
Latency: 588129 -> 588559 (+0.07%); split: -0.07%, +0.15%
InvThroughput: 83349 -> 83689 (+0.41%); split: -0.01%, +0.42%
Copies: 4324 -> 4691 (+8.49%)
VALU: 61431 -> 61798 (+0.60%)

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_register_allocation.cpp |  4 +---
 src/amd/compiler/tests/test_d3d11_derivs.cpp | 22 +++++++++++++-------
 2 files changed, 15 insertions(+), 11 deletions(-)

diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index 989e566fa26a6..8e1a601a76f5f 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -2944,9 +2944,7 @@ register_allocation(Program* program, std::vector<IDSet>& live_out_per_block, ra
                   if (get_reg_specified(ctx, register_file, rc, instr, reg))
                      definition->setFixed(reg);
                }
-            } else if (instr->opcode == aco_opcode::p_parallelcopy ||
-                       (instr->opcode == aco_opcode::p_start_linear_vgpr &&
-                        !instr->operands.empty())) {
+            } else if (instr->opcode == aco_opcode::p_parallelcopy) {
                PhysReg reg = instr->operands[i].physReg();
                if (instr->operands[i].isTemp() &&
                    instr->operands[i].getTemp().type() == definition->getTemp().type() &&
diff --git a/src/amd/compiler/tests/test_d3d11_derivs.cpp b/src/amd/compiler/tests/test_d3d11_derivs.cpp
index 4bff19aa7d21f..17370714e2ee9 100644
--- a/src/amd/compiler/tests/test_d3d11_derivs.cpp
+++ b/src/amd/compiler/tests/test_d3d11_derivs.cpp
@@ -61,9 +61,10 @@ BEGIN_TEST(d3d11_derivs.simple)
    //>> p_end_linear_vgpr (kill)%wqm
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
 
-   //>> v_interp_p2_f32_e32 v#rx, v#_, attr0.x                                             ; $_
+   //>> v_interp_p2_f32_e32 v#rx_tmp, v#_, attr0.x                                         ; $_
    //>> v_interp_p2_f32_e32 v#ry_tmp, v#_, attr0.y                                         ; $_
-   //>> v_mov_b32_e32 v#ry, v#ry_tmp                                                       ; $_
+   //>> v_mov_b32_e32 v#ry_tmp2, v#ry_tmp                                                  ; $_
+   //>> v_lshrrev_b64 v[#rx:#ry], 0, v[#rx_tmp:#ry_tmp2]                                   ; $_ $_
    //>> image_sample v[#_:#_], v[#rx:#ry], s[#_:#_], s[#_:#_] dmask:0xf dim:SQ_RSRC_IMG_2D ; $_ $_
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
 END_TEST
@@ -444,9 +445,11 @@ BEGIN_TEST(d3d11_derivs.cube)
    //>> p_end_linear_vgpr (kill)%wqm
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
 
-   //>> v_cubeid_f32 v#rf, v#_, v#_, v#_                                                     ; $_ $_
-   //>> v_fmaak_f32 v#rx, v#_, v#_, 0x3fc00000                                               ; $_ $_
-   //>> v_fmaak_f32 v#ry, v#_, v#_, 0x3fc00000                                               ; $_ $_
+   //>> v_cubeid_f32 v#rf_tmp, v#_, v#_, v#_                                                 ; $_ $_
+   //>> v_fmaak_f32 v#rx_tmp, v#_, v#_, 0x3fc00000                                           ; $_ $_
+   //>> v_fmaak_f32 v#ry_tmp, v#_, v#_, 0x3fc00000                                           ; $_ $_
+   //>> v_mov_b32_e32 v#rf, v#rf_tmp                                                         ; $_
+   //>> v_lshrrev_b64 v[#rx:#ry], 0, v[#rx_tmp:#ry_tmp]                                      ; $_ $_
    //; success = rx+1 == ry and rx+2 == rf
    //>> image_sample v[#_:#_], v[#rx:#rf], s[#_:#_], s[#_:#_] dmask:0xf dim:SQ_RSRC_IMG_CUBE ; $_ $_
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
@@ -492,9 +495,12 @@ BEGIN_TEST(d3d11_derivs.cube_array)
 
    //>> v_rndne_f32_e32 v#rl, v#_                                                             ; $_
    //>> v_cubeid_f32 v#rf, v#_, v#_, v#_                                                      ; $_ $_
-   //>> v_fmaak_f32 v#rx, v#_, v#_, 0x3fc00000                                                ; $_ $_
-   //>> v_fmaak_f32 v#ry, v#_, v#_, 0x3fc00000                                                ; $_ $_
-   //>> v_fmamk_f32 v#rlf, v#rl, 0x41000000, v#rf                                             ; $_ $_
+   //>> v_fmaak_f32 v#rx_tmp, v#_, v#_, 0x3fc00000                                            ; $_ $_
+   //>> v_fmaak_f32 v#ry_tmp, v#_, v#_, 0x3fc00000                                            ; $_ $_
+   //>> v_fmamk_f32 v#rlf_tmp, v#rl, 0x41000000, v#rf                                         ; $_ $_
+   //>> v_mov_b32_e32 v#rx, v#rx_tmp                                                          ; $_
+   //>> v_mov_b32_e32 v#ry, v#ry_tmp                                                          ; $_
+   //>> v_mov_b32_e32 v#rlf, v#rlf_tmp                                                        ; $_
    //>> BB1:
    //; success = rx+1 == ry and rx+2 == rlf
    //>> image_sample v[#_:#_], v[#rx:#rlf], s[#_:#_], s[#_:#_] dmask:0xf dim:SQ_RSRC_IMG_CUBE ; $_ $_
-- 
GitLab


From fe6868f09419f88353de7773a2b2524abca0e0a7 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Mon, 19 Feb 2024 17:00:19 +0000
Subject: [PATCH 04/12] aco: allow p_start_linear_vgpr to use multiple operands

Merging the p_create_vector into the p_start_linear_vgpr is useful since
we stopped attempting to place the p_start_linear_vgpr definition in the
same registers as the operand.

fossil-db (navi31):
Totals from 927 (1.17% of 79242) affected shaders:
MaxWaves: 26412 -> 26442 (+0.11%)
Instrs: 938328 -> 938181 (-0.02%); split: -0.14%, +0.13%
CodeSize: 4891448 -> 4890820 (-0.01%); split: -0.11%, +0.10%
VGPRs: 47016 -> 47004 (-0.03%); split: -0.13%, +0.10%
SpillSGPRs: 222 -> 226 (+1.80%)
Latency: 5076065 -> 5075191 (-0.02%); split: -0.12%, +0.10%
InvThroughput: 712316 -> 712421 (+0.01%); split: -0.09%, +0.10%
SClause: 27992 -> 27972 (-0.07%); split: -0.09%, +0.02%
Copies: 38042 -> 38104 (+0.16%); split: -1.95%, +2.12%
PreVGPRs: 39448 -> 39369 (-0.20%)
VALU: 570157 -> 570224 (+0.01%); split: -0.13%, +0.14%
SALU: 51672 -> 51678 (+0.01%); split: -0.01%, +0.02%

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 .../compiler/aco_instruction_selection.cpp    |  7 +-
 src/amd/compiler/aco_lower_to_hw_instr.cpp    | 28 ++-----
 src/amd/compiler/aco_optimizer.cpp            |  1 +
 src/amd/compiler/aco_validate.cpp             | 22 +++---
 src/amd/compiler/tests/test_d3d11_derivs.cpp  | 75 +++++++++----------
 5 files changed, 55 insertions(+), 78 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 356d7aadce741..2b714bbdf03a6 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -9272,7 +9272,6 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
    case nir_intrinsic_strict_wqm_coord_amd: {
       Temp dst = get_ssa_temp(ctx, &instr->def);
       Temp src = get_ssa_temp(ctx, instr->src[0].ssa);
-      Temp tmp = bld.tmp(RegClass::get(RegType::vgpr, dst.bytes()));
       unsigned begin_size = nir_intrinsic_base(instr);
 
       unsigned num_src = 1;
@@ -9281,7 +9280,7 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
          num_src = src.bytes() / it->second[0].bytes();
 
       aco_ptr<Pseudo_instruction> vec{create_instruction<Pseudo_instruction>(
-         aco_opcode::p_create_vector, Format::PSEUDO, num_src + !!begin_size, 1)};
+         aco_opcode::p_start_linear_vgpr, Format::PSEUDO, num_src + !!begin_size, 1)};
 
       if (begin_size)
          vec->operands[0] = Operand(RegClass::get(RegType::vgpr, begin_size));
@@ -9290,10 +9289,8 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
          vec->operands[i + !!begin_size] = Operand(comp);
       }
 
-      vec->definitions[0] = Definition(tmp);
+      vec->definitions[0] = Definition(dst);
       ctx->block->instructions.emplace_back(std::move(vec));
-
-      bld.pseudo(aco_opcode::p_start_linear_vgpr, Definition(dst), tmp);
       break;
    }
    case nir_intrinsic_load_lds_ngg_scratch_base_amd: {
diff --git a/src/amd/compiler/aco_lower_to_hw_instr.cpp b/src/amd/compiler/aco_lower_to_hw_instr.cpp
index 263f40a29c0f3..d31ed70ec27fa 100644
--- a/src/amd/compiler/aco_lower_to_hw_instr.cpp
+++ b/src/amd/compiler/aco_lower_to_hw_instr.cpp
@@ -2395,14 +2395,18 @@ lower_to_hw_instr(Program* program)
                handle_operands(copy_operations, &ctx, program->gfx_level, pi);
                break;
             }
-            case aco_opcode::p_create_vector: {
+            case aco_opcode::p_create_vector:
+            case aco_opcode::p_start_linear_vgpr: {
+               if (instr->operands.empty())
+                  break;
+
                std::map<PhysReg, copy_operation> copy_operations;
                PhysReg reg = instr->definitions[0].physReg();
 
                for (const Operand& op : instr->operands) {
+                  RegClass rc = RegClass::get(instr->definitions[0].regClass().type(), op.bytes());
                   if (op.isConstant()) {
-                     const Definition def = Definition(
-                        reg, instr->definitions[0].getTemp().regClass().resize(op.bytes()));
+                     const Definition def = Definition(reg, rc);
                      copy_operations[reg] = {op, def, op.bytes()};
                      reg.reg_b += op.bytes();
                      continue;
@@ -2413,10 +2417,7 @@ lower_to_hw_instr(Program* program)
                      continue;
                   }
 
-                  RegClass rc_def =
-                     op.regClass().is_subdword()
-                        ? op.regClass()
-                        : instr->definitions[0].getTemp().regClass().resize(op.bytes());
+                  RegClass rc_def = op.regClass().is_subdword() ? op.regClass() : rc;
                   const Definition def = Definition(reg, rc_def);
                   copy_operations[def.physReg()] = {op, def, op.bytes()};
                   reg.reg_b += op.bytes();
@@ -2454,19 +2455,6 @@ lower_to_hw_instr(Program* program)
                   handle_operands(copy_operations, &ctx, program->gfx_level, pi);
                break;
             }
-            case aco_opcode::p_start_linear_vgpr: {
-               if (instr->operands.empty())
-                  break;
-
-               Definition def(instr->definitions[0].physReg(),
-                              RegClass::get(RegType::vgpr, instr->definitions[0].bytes()));
-
-               std::map<PhysReg, copy_operation> copy_operations;
-               copy_operations[def.physReg()] = {instr->operands[0], def,
-                                                 instr->operands[0].bytes()};
-               handle_operands(copy_operations, &ctx, program->gfx_level, pi);
-               break;
-            }
             case aco_opcode::p_exit_early_if: {
                /* don't bother with an early exit near the end of the program */
                if ((block->instructions.size() - 1 - instr_idx) <= 4 &&
diff --git a/src/amd/compiler/aco_optimizer.cpp b/src/amd/compiler/aco_optimizer.cpp
index 73530876c4be3..dc7e003b6ab65 100644
--- a/src/amd/compiler/aco_optimizer.cpp
+++ b/src/amd/compiler/aco_optimizer.cpp
@@ -580,6 +580,7 @@ pseudo_propagate_temp(opt_ctx& ctx, aco_ptr<Instruction>& instr, Temp temp, unsi
    case aco_opcode::p_linear_phi:
    case aco_opcode::p_parallelcopy:
    case aco_opcode::p_create_vector:
+   case aco_opcode::p_start_linear_vgpr:
       if (temp.bytes() != instr->operands[index].bytes())
          return false;
       break;
diff --git a/src/amd/compiler/aco_validate.cpp b/src/amd/compiler/aco_validate.cpp
index b571e8fd09f8d..5d59b0af56575 100644
--- a/src/amd/compiler/aco_validate.cpp
+++ b/src/amd/compiler/aco_validate.cpp
@@ -366,6 +366,7 @@ validate_ir(Program* program)
                bool flat = instr->isFlatLike();
                bool can_be_undef = is_phi(instr) || instr->isEXP() || instr->isReduction() ||
                                    instr->opcode == aco_opcode::p_create_vector ||
+                                   instr->opcode == aco_opcode::p_start_linear_vgpr ||
                                    instr->opcode == aco_opcode::p_jump_to_epilog ||
                                    instr->opcode == aco_opcode::p_dual_src_export_gfx11 ||
                                    instr->opcode == aco_opcode::p_end_with_regs ||
@@ -527,20 +528,26 @@ validate_ir(Program* program)
 
          switch (instr->format) {
          case Format::PSEUDO: {
-            if (instr->opcode == aco_opcode::p_create_vector) {
+            if (instr->opcode == aco_opcode::p_create_vector ||
+                instr->opcode == aco_opcode::p_start_linear_vgpr) {
                unsigned size = 0;
                for (const Operand& op : instr->operands) {
                   check(op.bytes() < 4 || size % 4 == 0, "Operand is not aligned", instr.get());
                   size += op.bytes();
                }
-               check(size == instr->definitions[0].bytes(),
-                     "Definition size does not match operand sizes", instr.get());
+               if (!instr->operands.empty() || instr->opcode == aco_opcode::p_create_vector) {
+                  check(size == instr->definitions[0].bytes(),
+                        "Definition size does not match operand sizes", instr.get());
+               }
                if (instr->definitions[0].regClass().type() == RegType::sgpr) {
                   for (const Operand& op : instr->operands) {
                      check(op.isConstant() || op.regClass().type() == RegType::sgpr,
                            "Wrong Operand type for scalar vector", instr.get());
                   }
                }
+               if (instr->opcode == aco_opcode::p_start_linear_vgpr)
+                  check(instr->definitions[0].regClass().is_linear_vgpr(),
+                        "Definition must be linear VGPR", instr.get());
             } else if (instr->opcode == aco_opcode::p_extract_vector) {
                check(!instr->operands[0].isConstant() && instr->operands[1].isConstant(),
                      "Wrong Operand types", instr.get());
@@ -680,15 +687,6 @@ validate_ir(Program* program)
                      instr->operands[i].isOfType(RegType::vgpr) || instr->operands[i].isUndefined(),
                      "Operands of p_dual_src_export_gfx11 must be VGPRs or undef", instr.get());
                }
-            } else if (instr->opcode == aco_opcode::p_start_linear_vgpr) {
-               check(instr->definitions.size() == 1, "Must have one definition", instr.get());
-               check(instr->operands.size() <= 1, "Must have one or zero operands", instr.get());
-               if (!instr->definitions.empty())
-                  check(instr->definitions[0].regClass().is_linear_vgpr(),
-                        "Definition must be linear VGPR", instr.get());
-               if (!instr->definitions.empty() && !instr->operands.empty())
-                  check(instr->definitions[0].bytes() == instr->operands[0].bytes(),
-                        "Operand size must match definition", instr.get());
             }
             break;
          }
diff --git a/src/amd/compiler/tests/test_d3d11_derivs.cpp b/src/amd/compiler/tests/test_d3d11_derivs.cpp
index 17370714e2ee9..f180de4a1574b 100644
--- a/src/amd/compiler/tests/test_d3d11_derivs.cpp
+++ b/src/amd/compiler/tests/test_d3d11_derivs.cpp
@@ -52,8 +52,7 @@ BEGIN_TEST(d3d11_derivs.simple)
 
    //>> v1: %x = v_interp_p2_f32 %_, %_:m0, (kill)%_ attr0.x
    //>> v1: %y = v_interp_p2_f32 (kill)%_, (kill)%_:m0, (kill)%_ attr0.y
-   //>> v2: %vec = p_create_vector (kill)%x, (kill)%y
-   //>> lv2: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> lv2: %wqm = p_start_linear_vgpr (kill)%x, (kill)%y
    //>> BB1
    //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm 2d
    //>> BB2
@@ -63,8 +62,8 @@ BEGIN_TEST(d3d11_derivs.simple)
 
    //>> v_interp_p2_f32_e32 v#rx_tmp, v#_, attr0.x                                         ; $_
    //>> v_interp_p2_f32_e32 v#ry_tmp, v#_, attr0.y                                         ; $_
-   //>> v_mov_b32_e32 v#ry_tmp2, v#ry_tmp                                                  ; $_
-   //>> v_lshrrev_b64 v[#rx:#ry], 0, v[#rx_tmp:#ry_tmp2]                                   ; $_ $_
+   //>> v_mov_b32_e32 v#ry, v#ry_tmp                                                       ; $_
+   //>> v_mov_b32_e32 v#rx, v#rx_tmp                                                       ; $_
    //>> image_sample v[#_:#_], v[#rx:#ry], s[#_:#_], s[#_:#_] dmask:0xf dim:SQ_RSRC_IMG_2D ; $_ $_
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
 END_TEST
@@ -94,8 +93,7 @@ BEGIN_TEST(d3d11_derivs.constant)
    pbld.add_vsfs(vs, fs);
 
    //>> v1: %x = v_interp_p2_f32 (kill)%_, (kill)%_:m0, (kill)%_ attr0.x
-   //>> v2: %vec = p_create_vector (kill)%x, -0.5
-   //>> lv2: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> lv2: %wqm = p_start_linear_vgpr (kill)%x, -0.5
    //>> BB1
    //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm 2d
    //>> BB2
@@ -134,7 +132,7 @@ BEGIN_TEST(d3d11_derivs.discard)
    pbld.add_vsfs(vs, fs);
 
    /* The interpolation must be done before the discard_if. */
-   //>> lv2: %wqm = p_start_linear_vgpr (kill)%_
+   //>> lv2: %wqm = p_start_linear_vgpr (kill)%_, (kill)%_
    //>> s2: %_:exec, s1: (kill)%_:scc = s_andn2_b64 %_:exec, %_
    //>> s2: %_, s1: %_:scc = s_andn2_b64 (kill)%_, (kill)%_
    //>> p_exit_early_if (kill)%_:scc
@@ -167,8 +165,7 @@ BEGIN_TEST(d3d11_derivs.bias)
    pbld.add_vsfs(vs, fs);
 
    //>> s2: %_:s[0-1], s1: %_:s[2], s1: %_:s[3], s1: %_:s[4], v2: %_:v[0-1], v1: %bias:v[2] = p_startpgm
-   //>> v3: %vec = p_create_vector v1: undef, (kill)%_, (kill)%_
-   //>> lv3: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> lv3: %wqm = p_start_linear_vgpr v1: undef, (kill)%_, (kill)%_
    //>> BB1
    //>> v4: %_ = image_sample_b (kill)%_, (kill)%_, v1: undef, %wqm, (kill)%bias 2d
    //>> BB2
@@ -176,12 +173,12 @@ BEGIN_TEST(d3d11_derivs.bias)
    //>> p_end_linear_vgpr (kill)%wqm
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
 
-   //>> v_interp_p2_f32_e32 v#rx, v#_, attr0.x                                                       ; $_
-   //>> v_interp_p2_f32_e32 v#ry_tmp, v#_, attr0.y                                                   ; $_
-   //>> v_mov_b32_e32 v#rb, v2                                                                       ; $_
-   //>> v_mov_b32_e32 v#ry, v#ry_tmp                                                                 ; $_
+   //>> v_interp_p2_f32_e32 v#rx_tmp, v#_, attr0.x                                                 ; $_
+   //>> v_interp_p2_f32_e32 v#ry_tmp, v#_, attr0.y                                                 ; $_
+   //>> v_mov_b32_e32 v#rx, v#rx_tmp                                                               ; $_
+   //>> v_mov_b32_e32 v#ry, v#ry_tmp                                                               ; $_
    //>> BB1:
-   //>> image_sample_b v[#_:#_], [v#rb, v#rx, v#ry], s[#_:#_], s[#_:#_] dmask:0xf dim:SQ_RSRC_IMG_2D ; $_ $_ $_
+   //>> image_sample_b v[#_:#_], [v2, v#rx, v#ry], s[#_:#_], s[#_:#_] dmask:0xf dim:SQ_RSRC_IMG_2D ; $_ $_ $_
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
 END_TEST
 
@@ -210,8 +207,7 @@ BEGIN_TEST(d3d11_derivs.offset)
    PipelineBuilder pbld(get_vk_device(GFX9));
    pbld.add_vsfs(vs, fs);
 
-   //>> v3: %vec = p_create_vector v1: undef, (kill)%_, (kill)%_
-   //>> lv3: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> lv3: %wqm = p_start_linear_vgpr v1: undef, (kill)%_, (kill)%_
    //>> BB1
    //>> v1: %offset = p_parallelcopy 0x201
    //>> v4: %_ = image_sample_o (kill)%_, (kill)%_, v1: undef, %wqm, (kill)%offset 2d
@@ -220,8 +216,9 @@ BEGIN_TEST(d3d11_derivs.offset)
    //>> p_end_linear_vgpr (kill)%wqm
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
 
-   //>> v_interp_p2_f32_e32 v#rx, v#_, attr0.x                            ; $_
+   //>> v_interp_p2_f32_e32 v#rx_tmp, v#_, attr0.x                        ; $_
    //>> v_interp_p2_f32_e32 v#ry_tmp, v#_, attr0.y                        ; $_
+   //>> v_mov_b32_e32 v#rx, v#rx_tmp                                      ; $_
    //>> v_mov_b32_e32 v#ry, v#ry_tmp                                      ; $_
    //>> BB1:
    //>> v_mov_b32_e32 v#ro_tmp, 0x201                                     ; $_ $_
@@ -256,8 +253,7 @@ BEGIN_TEST(d3d11_derivs.array)
    pbld.add_vsfs(vs, fs);
 
    //>> v1: %layer = v_rndne_f32 (kill)%_
-   //>> v3: %vec = p_create_vector (kill)%_, (kill)%_, (kill)%layer
-   //>> lv3: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> lv3: %wqm = p_start_linear_vgpr (kill)%_, (kill)%_, (kill)%layer
    //>> BB1
    //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm 2darray da
    //>> BB2
@@ -266,9 +262,11 @@ BEGIN_TEST(d3d11_derivs.array)
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
 
    //>> v_interp_p2_f32_e32 v#rl_tmp, v#_, attr0.z                                               ; $_
-   //>> v_rndne_f32_e32 v#rl, v#rl_tmp                                                           ; $_
-   //>> v_interp_p2_f32_e32 v#rx, v#_, attr0.x                                                   ; $_
+   //>> v_interp_p2_f32_e32 v#rx_tmp, v#_, attr0.x                                               ; $_
    //>> v_interp_p2_f32_e32 v#ry_tmp, v#_, attr0.y                                               ; $_
+   //>> v_rndne_f32_e32 v#rl_tmp, v#rl_tmp                                                       ; $_
+   //>> v_mov_b32_e32 v#rl, v#rl_tmp                                                             ; $_
+   //>> v_mov_b32_e32 v#rx, v#rx_tmp                                                             ; $_
    //>> v_mov_b32_e32 v#ry, v#ry_tmp                                                             ; $_
    //>> BB1:
    //; success = rx+1 == ry and rx+2 == rl
@@ -302,8 +300,7 @@ BEGIN_TEST(d3d11_derivs.bias_array)
 
    //>> s2: %_:s[0-1], s1: %_:s[2], s1: %_:s[3], s1: %_:s[4], v2: %_:v[0-1], v1: %bias:v[2] = p_startpgm
    //>> v1: %layer = v_rndne_f32 (kill)%_
-   //>> v4: %vec = p_create_vector v1: undef, (kill)%_, (kill)%_, (kill)%layer
-   //>> lv4: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> lv4: %wqm = p_start_linear_vgpr v1: undef, (kill)%_, (kill)%_, (kill)%layer
    //>> BB1
    //>> v4: %_ = image_sample_b (kill)%_, (kill)%_, v1: undef, %wqm, (kill)%bias 2darray da
    //>> BB2
@@ -312,11 +309,12 @@ BEGIN_TEST(d3d11_derivs.bias_array)
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
 
    //>> v_interp_p2_f32_e32 v#rl_tmp, v#_, attr0.z                                                             ; $_
-   //>> v_rndne_f32_e32 v#rl, v#rl_tmp                                                                         ; $_
    //>> v_interp_p2_f32_e32 v#rx_tmp, v#_, attr0.x                                                             ; $_
    //>> v_interp_p2_f32_e32 v#ry_tmp, v#_, attr0.y                                                             ; $_
+   //>> v_rndne_f32_e32 v#rl_tmp, v#rl_tmp                                                                     ; $_
    //>> v_mov_b32_e32 v#rx, v#rx_tmp                                                                           ; $_
    //>> v_mov_b32_e32 v#ry, v#ry_tmp                                                                           ; $_
+   //>> v_mov_b32_e32 v#rl, v#rl_tmp                                                                           ; $_
    //>> BB1:
    //>> image_sample_b v[#_:#_], [v2, v#rx, v#ry, v#rl], s[#_:#_], s[#_:#_] dmask:0xf dim:SQ_RSRC_IMG_2D_ARRAY ; $_ $_ $_
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
@@ -347,8 +345,7 @@ BEGIN_TEST(d3d11_derivs._1d_gfx9)
    pbld.add_vsfs(vs, fs);
 
    //>> v1: %x = v_interp_p2_f32 (kill)%_, (kill)%_:m0, (kill)%_ attr0.x
-   //>> v2: %vec = p_create_vector (kill)%x, 0.5
-   //>> lv2: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> lv2: %wqm = p_start_linear_vgpr (kill)%x, 0.5
    //>> BB1
    //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm 2d
    //>> BB2
@@ -389,8 +386,7 @@ BEGIN_TEST(d3d11_derivs._1d_array_gfx9)
 
    //>> v1: %layer = v_rndne_f32 (kill)%_
    //>> v1: %x = v_interp_p2_f32 (kill)%_, (kill)%_:m0, (kill)%_ attr0.x
-   //>> v3: %vec = p_create_vector (kill)%x, 0.5, (kill)%layer
-   //>> lv3: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> lv3: %wqm = p_start_linear_vgpr (kill)%x, 0.5, (kill)%layer
    //>> BB1
    //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm 2darray da
    //>> BB2
@@ -400,8 +396,9 @@ BEGIN_TEST(d3d11_derivs._1d_array_gfx9)
 
    //>> v_interp_p2_f32_e32 v#rl_tmp, v#_, attr0.y                   ; $_
    //>> v_interp_p2_f32_e32 v#rx_tmp, v#_, attr0.x                   ; $_
-   //>> v_rndne_f32_e32 v#rl, v#rl_tmp                               ; $_
+   //>> v_rndne_f32_e32 v#rl_tmp, v#rl_tmp                           ; $_
    //>> v_mov_b32_e32 v#ry, 0.5                                      ; $_
+   //>> v_mov_b32_e32 v#rl, v#rl_tmp                                 ; $_
    //>> v_mov_b32_e32 v#rx, v#rx_tmp                                 ; $_
    //>> BB1:
    //; success = rx+1 == ry and rx+2 == rl
@@ -436,8 +433,7 @@ BEGIN_TEST(d3d11_derivs.cube)
    //>> v1: %face = v_cubeid_f32 (kill)%_, (kill)%_, (kill)%_
    //>> v1: %x = v_fmaak_f32 (kill)%_, %_, 0x3fc00000
    //>> v1: %y = v_fmaak_f32 (kill)%_, (kill)%_, 0x3fc00000
-   //>> v3: %vec = p_create_vector (kill)%x, (kill)%y, (kill)%face
-   //>> lv3: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> lv3: %wqm = p_start_linear_vgpr (kill)%x, (kill)%y, (kill)%face
    //>> BB1
    //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm cube da
    //>> BB2
@@ -446,10 +442,10 @@ BEGIN_TEST(d3d11_derivs.cube)
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
 
    //>> v_cubeid_f32 v#rf_tmp, v#_, v#_, v#_                                                 ; $_ $_
-   //>> v_fmaak_f32 v#rx_tmp, v#_, v#_, 0x3fc00000                                           ; $_ $_
+   //>> v_fmaak_f32 v#rx, v#_, v#_, 0x3fc00000                                               ; $_ $_
    //>> v_fmaak_f32 v#ry_tmp, v#_, v#_, 0x3fc00000                                           ; $_ $_
    //>> v_mov_b32_e32 v#rf, v#rf_tmp                                                         ; $_
-   //>> v_lshrrev_b64 v[#rx:#ry], 0, v[#rx_tmp:#ry_tmp]                                      ; $_ $_
+   //>> v_mov_b32_e32 v#ry, v#ry_tmp                                                         ; $_
    //; success = rx+1 == ry and rx+2 == rf
    //>> image_sample v[#_:#_], v[#rx:#rf], s[#_:#_], s[#_:#_] dmask:0xf dim:SQ_RSRC_IMG_CUBE ; $_ $_
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
@@ -484,8 +480,7 @@ BEGIN_TEST(d3d11_derivs.cube_array)
    //>> v1: %x = v_fmaak_f32 (kill)%_, %_, 0x3fc00000
    //>> v1: %y = v_fmaak_f32 (kill)%_, (kill)%_, 0x3fc00000
    //>> v1: %face_layer = v_fmamk_f32 (kill)%layer, (kill)%face, 0x41000000
-   //>> v3: %vec = p_create_vector (kill)%x, (kill)%y, (kill)%face_layer
-   //>> lv3: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> lv3: %wqm = p_start_linear_vgpr (kill)%x, (kill)%y, (kill)%face_layer
    //>> BB1
    //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm cube da
    //>> BB2
@@ -495,12 +490,12 @@ BEGIN_TEST(d3d11_derivs.cube_array)
 
    //>> v_rndne_f32_e32 v#rl, v#_                                                             ; $_
    //>> v_cubeid_f32 v#rf, v#_, v#_, v#_                                                      ; $_ $_
+   //>> v_fmamk_f32 v#rlf_tmp, v#rl, 0x41000000, v#rf                                         ; $_ $_
    //>> v_fmaak_f32 v#rx_tmp, v#_, v#_, 0x3fc00000                                            ; $_ $_
    //>> v_fmaak_f32 v#ry_tmp, v#_, v#_, 0x3fc00000                                            ; $_ $_
-   //>> v_fmamk_f32 v#rlf_tmp, v#rl, 0x41000000, v#rf                                         ; $_ $_
-   //>> v_mov_b32_e32 v#rx, v#rx_tmp                                                          ; $_
    //>> v_mov_b32_e32 v#ry, v#ry_tmp                                                          ; $_
    //>> v_mov_b32_e32 v#rlf, v#rlf_tmp                                                        ; $_
+   //>> v_mov_b32_e32 v#rx, v#rx_tmp                                                          ; $_
    //>> BB1:
    //; success = rx+1 == ry and rx+2 == rlf
    //>> image_sample v[#_:#_], v[#rx:#rlf], s[#_:#_], s[#_:#_] dmask:0xf dim:SQ_RSRC_IMG_CUBE ; $_ $_
@@ -566,8 +561,7 @@ BEGIN_TEST(d3d11_derivs.bc_optimize)
    //>> v1: %y_coord2 = v_cndmask_b32 (kill)%_, %_, (kill)%_
    //>> v1: %x = v_interp_p2_f32 (kill)%_, %_:m0, (kill)%_ attr0.x
    //>> v1: %y = v_interp_p2_f32 (kill)%y_coord2, (kill)%_:m0, (kill)%_ attr0.y
-   //>> v2: %vec = p_create_vector (kill)%x, (kill)%y
-   //>> lv2: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> lv2: %wqm = p_start_linear_vgpr (kill)%x, (kill)%y
    //>> BB1
    //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm 2d
    //>> BB2
@@ -602,8 +596,7 @@ BEGIN_TEST(d3d11_derivs.get_lod)
 
    //>> v1: %x = v_interp_p2_f32 %_, %_:m0, (kill)%_ attr0.x
    //>> v1: %y = v_interp_p2_f32 (kill)%_, (kill)%_:m0, (kill)%_ attr0.y
-   //>> v2: %vec = p_create_vector %x, %y
-   //>> lv2: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> lv2: %wqm = p_start_linear_vgpr %x, %y
    //>> v1: %x0 = v_mov_b32 %x quad_perm:[0,0,0,0] bound_ctrl:1 fi
    //>> v1: %x1_m_x0 = v_sub_f32 %x, %x0 quad_perm:[1,1,1,1] bound_ctrl:1 fi
    //>> v1: %x2_m_x0 = v_sub_f32 (kill)%x, (kill)%x0 quad_perm:[2,2,2,2] bound_ctrl:1 fi
-- 
GitLab


From 1460e5e3dca1fa5880ffb1a3c9d9d0f20d123450 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Wed, 31 Jan 2024 18:44:21 +0000
Subject: [PATCH 05/12] aco: require linear vgpr uses to be late kill

This also removes some linear VGPR tests which will be replaced soon
anyway.

fossil-db (navi31):
Totals from 107 (0.14% of 79242) affected shaders:
Instrs: 66203 -> 66211 (+0.01%); split: -0.09%, +0.10%
CodeSize: 354644 -> 354588 (-0.02%); split: -0.08%, +0.07%
VGPRs: 4476 -> 4452 (-0.54%); split: -0.80%, +0.27%
Latency: 513863 -> 513877 (+0.00%); split: -0.08%, +0.08%
InvThroughput: 68871 -> 68870 (-0.00%); split: -0.02%, +0.02%
SClause: 1589 -> 1590 (+0.06%)
PreVGPRs: 3404 -> 3415 (+0.32%)

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 .../compiler/aco_instruction_selection.cpp    |  12 +-
 src/amd/compiler/aco_reduce_assign.cpp        |  10 +-
 src/amd/compiler/aco_spill.cpp                |   6 +-
 src/amd/compiler/aco_validate.cpp             |  10 ++
 src/amd/compiler/tests/test_d3d11_derivs.cpp  |  50 ++++-----
 src/amd/compiler/tests/test_reduce_assign.cpp |   4 +-
 src/amd/compiler/tests/test_regalloc.cpp      | 103 ------------------
 7 files changed, 59 insertions(+), 136 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 2b714bbdf03a6..154bb4e9fe7af 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -6095,8 +6095,11 @@ emit_mimg(Builder& bld, aco_opcode op, Temp dst, Temp rsrc, Operand samp, std::v
    mimg->operands[0] = Operand(rsrc);
    mimg->operands[1] = samp;
    mimg->operands[2] = vdata;
-   for (unsigned i = 0; i < coords.size(); i++)
+   for (unsigned i = 0; i < coords.size(); i++) {
       mimg->operands[3 + i] = Operand(coords[i]);
+      if (coords[i].regClass().is_linear_vgpr())
+         mimg->operands[3 + i].setLateKill(true);
+   }
    mimg->strict_wqm = strict_wqm;
 
    MIMG_instruction* res = mimg.get();
@@ -10321,8 +10324,11 @@ visit_block(isel_context* ctx, nir_block* block)
 {
    if (ctx->block->kind & block_kind_top_level) {
       Builder bld(ctx->program, ctx->block);
-      for (Temp tmp : ctx->unended_linear_vgprs)
-         bld.pseudo(aco_opcode::p_end_linear_vgpr, tmp);
+      for (Temp tmp : ctx->unended_linear_vgprs) {
+         Operand op(tmp);
+         op.setLateKill(true);
+         bld.pseudo(aco_opcode::p_end_linear_vgpr, op);
+      }
       ctx->unended_linear_vgprs.clear();
    }
 
diff --git a/src/amd/compiler/aco_reduce_assign.cpp b/src/amd/compiler/aco_reduce_assign.cpp
index 83514206d4652..5e7cf400920dd 100644
--- a/src/amd/compiler/aco_reduce_assign.cpp
+++ b/src/amd/compiler/aco_reduce_assign.cpp
@@ -82,8 +82,11 @@ setup_reduce_temp(Program* program)
             aco_ptr<Instruction> end{create_instruction<Instruction>(
                aco_opcode::p_end_linear_vgpr, Format::PSEUDO, vtmp_inserted_at >= 0 ? 2 : 1, 0)};
             end->operands[0] = Operand(reduceTmp);
-            if (vtmp_inserted_at >= 0)
+            end->operands[0].setLateKill(true);
+            if (vtmp_inserted_at >= 0) {
                end->operands[1] = Operand(vtmp);
+               end->operands[1].setLateKill(true);
+            }
             /* insert after the phis of the block */
             std::vector<aco_ptr<Instruction>>::iterator it = block.instructions.begin();
             while ((*it)->opcode == aco_opcode::p_linear_phi || (*it)->opcode == aco_opcode::p_phi)
@@ -169,8 +172,11 @@ setup_reduce_temp(Program* program)
 
          if (instr->isReduction()) {
             instr->operands[1] = Operand(reduceTmp);
-            if (need_vtmp)
+            instr->operands[1].setLateKill(true);
+            if (need_vtmp) {
                instr->operands[2] = Operand(vtmp);
+               instr->operands[2].setLateKill(true);
+            }
          } else {
             assert(instr->opcode == aco_opcode::p_interp_gfx11 ||
                    instr->opcode == aco_opcode::p_bpermute_permlane);
diff --git a/src/amd/compiler/aco_spill.cpp b/src/amd/compiler/aco_spill.cpp
index 8da3d49202bfc..2f9e4b0421f67 100644
--- a/src/amd/compiler/aco_spill.cpp
+++ b/src/amd/compiler/aco_spill.cpp
@@ -1742,8 +1742,10 @@ end_unused_spill_vgprs(spill_ctx& ctx, Block& block, std::vector<Temp>& vgpr_spi
 
    aco_ptr<Instruction> destr{create_instruction<Pseudo_instruction>(
       aco_opcode::p_end_linear_vgpr, Format::PSEUDO, temps.size(), 0)};
-   for (unsigned i = 0; i < temps.size(); i++)
+   for (unsigned i = 0; i < temps.size(); i++) {
       destr->operands[i] = Operand(temps[i]);
+      destr->operands[i].setLateKill(true);
+   }
 
    std::vector<aco_ptr<Instruction>>::iterator it = block.instructions.begin();
    while (is_phi(*it))
@@ -1856,6 +1858,7 @@ assign_spill_slots(spill_ctx& ctx, unsigned spills_to_vgpr)
                Pseudo_instruction* spill =
                   create_instruction<Pseudo_instruction>(aco_opcode::p_spill, Format::PSEUDO, 3, 0);
                spill->operands[0] = Operand(vgpr_spill_temps[spill_slot / ctx.wave_size]);
+               spill->operands[0].setLateKill(true);
                spill->operands[1] = Operand::c32(spill_slot % ctx.wave_size);
                spill->operands[2] = (*it)->operands[0];
                instructions.emplace_back(aco_ptr<Instruction>(spill));
@@ -1896,6 +1899,7 @@ assign_spill_slots(spill_ctx& ctx, unsigned spills_to_vgpr)
                Pseudo_instruction* reload = create_instruction<Pseudo_instruction>(
                   aco_opcode::p_reload, Format::PSEUDO, 2, 1);
                reload->operands[0] = Operand(vgpr_spill_temps[spill_slot / ctx.wave_size]);
+               reload->operands[0].setLateKill(true);
                reload->operands[1] = Operand::c32(spill_slot % ctx.wave_size);
                reload->definitions[0] = (*it)->definitions[0];
                instructions.emplace_back(aco_ptr<Instruction>(reload));
diff --git a/src/amd/compiler/aco_validate.cpp b/src/amd/compiler/aco_validate.cpp
index 5d59b0af56575..d8d88fbdee668 100644
--- a/src/amd/compiler/aco_validate.cpp
+++ b/src/amd/compiler/aco_validate.cpp
@@ -384,6 +384,16 @@ validate_ir(Program* program)
             }
          }
 
+         /* Check that linear vgprs are late kill: this is to ensure linear VGPR operands and
+          * normal VGPR definitions don't try to use the same register, which is problematic
+          * because of assignment restrictions.
+          */
+         for (Operand& op : instr->operands) {
+            if (!op.isUndefined() && !op.isFixed() && op.hasRegClass() &&
+                op.regClass().is_linear_vgpr())
+               check(op.isLateKill(), "Linear VGPR operands must be late kill", instr.get());
+         }
+
          /* check subdword definitions */
          for (unsigned i = 0; i < instr->definitions.size(); i++) {
             if (instr->definitions[i].regClass().is_subdword())
diff --git a/src/amd/compiler/tests/test_d3d11_derivs.cpp b/src/amd/compiler/tests/test_d3d11_derivs.cpp
index f180de4a1574b..0b68803f979f9 100644
--- a/src/amd/compiler/tests/test_d3d11_derivs.cpp
+++ b/src/amd/compiler/tests/test_d3d11_derivs.cpp
@@ -54,10 +54,10 @@ BEGIN_TEST(d3d11_derivs.simple)
    //>> v1: %y = v_interp_p2_f32 (kill)%_, (kill)%_:m0, (kill)%_ attr0.y
    //>> lv2: %wqm = p_start_linear_vgpr (kill)%x, (kill)%y
    //>> BB1
-   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm 2d
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, (latekill)%wqm 2d
    //>> BB2
    //>> BB6
-   //>> p_end_linear_vgpr (kill)%wqm
+   //>> p_end_linear_vgpr (latekill)(kill)%wqm
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
 
    //>> v_interp_p2_f32_e32 v#rx_tmp, v#_, attr0.x                                         ; $_
@@ -95,10 +95,10 @@ BEGIN_TEST(d3d11_derivs.constant)
    //>> v1: %x = v_interp_p2_f32 (kill)%_, (kill)%_:m0, (kill)%_ attr0.x
    //>> lv2: %wqm = p_start_linear_vgpr (kill)%x, -0.5
    //>> BB1
-   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm 2d
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, (latekill)%wqm 2d
    //>> BB2
    //>> BB6
-   //>> p_end_linear_vgpr (kill)%wqm
+   //>> p_end_linear_vgpr (latekill)(kill)%wqm
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
 
    //>> v_interp_p2_f32_e32 v#rx, v#_, attr0.x                                             ; $_
@@ -136,7 +136,7 @@ BEGIN_TEST(d3d11_derivs.discard)
    //>> s2: %_:exec, s1: (kill)%_:scc = s_andn2_b64 %_:exec, %_
    //>> s2: %_, s1: %_:scc = s_andn2_b64 (kill)%_, (kill)%_
    //>> p_exit_early_if (kill)%_:scc
-   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, (kill)%wqm 2d
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, (latekill)(kill)%wqm 2d
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
 END_TEST
 
@@ -167,10 +167,10 @@ BEGIN_TEST(d3d11_derivs.bias)
    //>> s2: %_:s[0-1], s1: %_:s[2], s1: %_:s[3], s1: %_:s[4], v2: %_:v[0-1], v1: %bias:v[2] = p_startpgm
    //>> lv3: %wqm = p_start_linear_vgpr v1: undef, (kill)%_, (kill)%_
    //>> BB1
-   //>> v4: %_ = image_sample_b (kill)%_, (kill)%_, v1: undef, %wqm, (kill)%bias 2d
+   //>> v4: %_ = image_sample_b (kill)%_, (kill)%_, v1: undef, (latekill)%wqm, (kill)%bias 2d
    //>> BB2
    //>> BB6
-   //>> p_end_linear_vgpr (kill)%wqm
+   //>> p_end_linear_vgpr (latekill)(kill)%wqm
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
 
    //>> v_interp_p2_f32_e32 v#rx_tmp, v#_, attr0.x                                                 ; $_
@@ -210,10 +210,10 @@ BEGIN_TEST(d3d11_derivs.offset)
    //>> lv3: %wqm = p_start_linear_vgpr v1: undef, (kill)%_, (kill)%_
    //>> BB1
    //>> v1: %offset = p_parallelcopy 0x201
-   //>> v4: %_ = image_sample_o (kill)%_, (kill)%_, v1: undef, %wqm, (kill)%offset 2d
+   //>> v4: %_ = image_sample_o (kill)%_, (kill)%_, v1: undef, (latekill)%wqm, (kill)%offset 2d
    //>> BB2
    //>> BB6
-   //>> p_end_linear_vgpr (kill)%wqm
+   //>> p_end_linear_vgpr (latekill)(kill)%wqm
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
 
    //>> v_interp_p2_f32_e32 v#rx_tmp, v#_, attr0.x                        ; $_
@@ -255,10 +255,10 @@ BEGIN_TEST(d3d11_derivs.array)
    //>> v1: %layer = v_rndne_f32 (kill)%_
    //>> lv3: %wqm = p_start_linear_vgpr (kill)%_, (kill)%_, (kill)%layer
    //>> BB1
-   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm 2darray da
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, (latekill)%wqm 2darray da
    //>> BB2
    //>> BB6
-   //>> p_end_linear_vgpr (kill)%wqm
+   //>> p_end_linear_vgpr (latekill)(kill)%wqm
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
 
    //>> v_interp_p2_f32_e32 v#rl_tmp, v#_, attr0.z                                               ; $_
@@ -302,10 +302,10 @@ BEGIN_TEST(d3d11_derivs.bias_array)
    //>> v1: %layer = v_rndne_f32 (kill)%_
    //>> lv4: %wqm = p_start_linear_vgpr v1: undef, (kill)%_, (kill)%_, (kill)%layer
    //>> BB1
-   //>> v4: %_ = image_sample_b (kill)%_, (kill)%_, v1: undef, %wqm, (kill)%bias 2darray da
+   //>> v4: %_ = image_sample_b (kill)%_, (kill)%_, v1: undef, (latekill)%wqm, (kill)%bias 2darray da
    //>> BB2
    //>> BB6
-   //>> p_end_linear_vgpr (kill)%wqm
+   //>> p_end_linear_vgpr (latekill)(kill)%wqm
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
 
    //>> v_interp_p2_f32_e32 v#rl_tmp, v#_, attr0.z                                                             ; $_
@@ -347,10 +347,10 @@ BEGIN_TEST(d3d11_derivs._1d_gfx9)
    //>> v1: %x = v_interp_p2_f32 (kill)%_, (kill)%_:m0, (kill)%_ attr0.x
    //>> lv2: %wqm = p_start_linear_vgpr (kill)%x, 0.5
    //>> BB1
-   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm 2d
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, (latekill)%wqm 2d
    //>> BB2
    //>> BB6
-   //>> p_end_linear_vgpr (kill)%wqm
+   //>> p_end_linear_vgpr (latekill)(kill)%wqm
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
 
    //>> v_interp_p2_f32_e32 v#rx, v#_, attr0.x                    ; $_
@@ -388,10 +388,10 @@ BEGIN_TEST(d3d11_derivs._1d_array_gfx9)
    //>> v1: %x = v_interp_p2_f32 (kill)%_, (kill)%_:m0, (kill)%_ attr0.x
    //>> lv3: %wqm = p_start_linear_vgpr (kill)%x, 0.5, (kill)%layer
    //>> BB1
-   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm 2darray da
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, (latekill)%wqm 2darray da
    //>> BB2
    //>> BB6
-   //>> p_end_linear_vgpr (kill)%wqm
+   //>> p_end_linear_vgpr (latekill)(kill)%wqm
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
 
    //>> v_interp_p2_f32_e32 v#rl_tmp, v#_, attr0.y                   ; $_
@@ -435,10 +435,10 @@ BEGIN_TEST(d3d11_derivs.cube)
    //>> v1: %y = v_fmaak_f32 (kill)%_, (kill)%_, 0x3fc00000
    //>> lv3: %wqm = p_start_linear_vgpr (kill)%x, (kill)%y, (kill)%face
    //>> BB1
-   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm cube da
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, (latekill)%wqm cube da
    //>> BB2
    //>> BB6
-   //>> p_end_linear_vgpr (kill)%wqm
+   //>> p_end_linear_vgpr (latekill)(kill)%wqm
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
 
    //>> v_cubeid_f32 v#rf_tmp, v#_, v#_, v#_                                                 ; $_ $_
@@ -482,10 +482,10 @@ BEGIN_TEST(d3d11_derivs.cube_array)
    //>> v1: %face_layer = v_fmamk_f32 (kill)%layer, (kill)%face, 0x41000000
    //>> lv3: %wqm = p_start_linear_vgpr (kill)%x, (kill)%y, (kill)%face_layer
    //>> BB1
-   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm cube da
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, (latekill)%wqm cube da
    //>> BB2
    //>> BB6
-   //>> p_end_linear_vgpr (kill)%wqm
+   //>> p_end_linear_vgpr (latekill)(kill)%wqm
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
 
    //>> v_rndne_f32_e32 v#rl, v#_                                                             ; $_
@@ -563,10 +563,10 @@ BEGIN_TEST(d3d11_derivs.bc_optimize)
    //>> v1: %y = v_interp_p2_f32 (kill)%y_coord2, (kill)%_:m0, (kill)%_ attr0.y
    //>> lv2: %wqm = p_start_linear_vgpr (kill)%x, (kill)%y
    //>> BB1
-   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm 2d
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, (latekill)%wqm 2d
    //>> BB2
    //>> BB6
-   //>> p_end_linear_vgpr (kill)%wqm
+   //>> p_end_linear_vgpr (latekill)(kill)%wqm
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
 END_TEST
 
@@ -604,10 +604,10 @@ BEGIN_TEST(d3d11_derivs.get_lod)
    //>> v1: %y1_m_y0 = v_sub_f32 %y, %y0 quad_perm:[1,1,1,1] bound_ctrl:1 fi
    //>> v1: %y2_m_y0 = v_sub_f32 (kill)%y, (kill)%y0 quad_perm:[2,2,2,2] bound_ctrl:1 fi
    //>> BB1
-   //>> v2: %_ = image_get_lod (kill)%_, (kill)%_, v1: undef, %wqm 2d
+   //>> v2: %_ = image_get_lod (kill)%_, (kill)%_, v1: undef, (latekill)%wqm 2d
    //>> BB2
    //>> BB6
-   //>> p_end_linear_vgpr (kill)%wqm
+   //>> p_end_linear_vgpr (latekill)(kill)%wqm
    pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
 END_TEST
 
diff --git a/src/amd/compiler/tests/test_reduce_assign.cpp b/src/amd/compiler/tests/test_reduce_assign.cpp
index 7f44e55486f16..7f6b4644803cb 100644
--- a/src/amd/compiler/tests/test_reduce_assign.cpp
+++ b/src/amd/compiler/tests/test_reduce_assign.cpp
@@ -46,7 +46,7 @@ BEGIN_TEST(setup_reduce_temp.divergent_if_phi)
       program.get(), bld, Operand(inputs[0]),
       [&]() -> void
       {
-         //>> s1: %_, s2: %_, s1: %_:scc = p_reduce %a, %lv, lv1: undef op:umin32 cluster_size:64
+         //>> s1: %_, s2: %_, s1: %_:scc = p_reduce %a, (latekill)%lv, lv1: undef op:umin32 cluster_size:64
          Instruction* reduce =
             bld.reduction(aco_opcode::p_reduce, bld.def(s1), bld.def(bld.lm), bld.def(s1, scc),
                           inputs[1], Operand(v1.as_linear()), Operand(v1.as_linear()), umin32);
@@ -58,7 +58,7 @@ BEGIN_TEST(setup_reduce_temp.divergent_if_phi)
       });
    bld.pseudo(aco_opcode::p_phi, bld.def(v1), Operand::c32(1), Operand::zero());
    //>> /* logical preds: BB1, BB4, / linear preds: BB4, BB5, / kind: uniform, top-level, merge, */
-   //! p_end_linear_vgpr %lv
+   //! p_end_linear_vgpr (latekill)%lv
 
    finish_setup_reduce_temp_test();
 END_TEST
diff --git a/src/amd/compiler/tests/test_regalloc.cpp b/src/amd/compiler/tests/test_regalloc.cpp
index 2a8ac922fc6c1..60f33d630283a 100644
--- a/src/amd/compiler/tests/test_regalloc.cpp
+++ b/src/amd/compiler/tests/test_regalloc.cpp
@@ -227,109 +227,6 @@ BEGIN_TEST(regalloc.scratch_sgpr.create_vector_sgpr_operand)
    finish_ra_test(ra_test_policy(), true);
 END_TEST
 
-BEGIN_TEST(regalloc.linear_vgpr.live_range_split.fixed_def)
-   //>> p_startpgm
-   if (!setup_cs("", GFX10))
-      return;
-
-   PhysReg reg_v0{256};
-
-   //! lv1: %tmp1:v[0] = p_unit_test
-   Temp tmp = bld.pseudo(aco_opcode::p_unit_test, bld.def(v1.as_linear(), reg_v0));
-
-   //! lv1: %tmp2:v[1] = p_parallelcopy %tmp1:v[0]
-   //! v1: %_:v[0] = p_unit_test
-   bld.pseudo(aco_opcode::p_unit_test, Definition(reg_v0, v1));
-
-   //! p_unit_test %tmp2:v[1]
-   bld.pseudo(aco_opcode::p_unit_test, tmp);
-
-   finish_ra_test(ra_test_policy());
-END_TEST
-
-BEGIN_TEST(regalloc.linear_vgpr.live_range_split.get_reg_impl)
-   //>> p_startpgm
-   if (!setup_cs("", GFX10))
-      return;
-
-   program->dev.vgpr_limit = 3;
-
-   PhysReg reg_v1{257};
-
-   //! s1: %scc_tmp:scc, s1: %1:s[0] = p_unit_test
-   Temp s0_tmp = bld.tmp(s1);
-   Temp scc_tmp = bld.pseudo(aco_opcode::p_unit_test, bld.def(s1, scc),
-                             Definition(s0_tmp.id(), PhysReg{0}, s1));
-
-   //! lv1: %tmp1:v[1] = p_unit_test
-   Temp tmp = bld.pseudo(aco_opcode::p_unit_test, bld.def(v1.as_linear(), reg_v1));
-
-   //! lv1: %tmp2:v[2] = p_parallelcopy %tmp1:v[1]
-   //! v2: %_:v[0-1] = p_unit_test
-   bld.pseudo(aco_opcode::p_unit_test, bld.def(v2));
-
-   //! p_unit_test %tmp2:v[2], %scc_tmp:scc, %1:s[0]
-   bld.pseudo(aco_opcode::p_unit_test, tmp, scc_tmp, s0_tmp);
-
-   finish_ra_test(ra_test_policy());
-
-   //>> lv1: %5:v[2] = p_parallelcopy %3:v[1] scc:1 scratch:s1
-   Pseudo_instruction& parallelcopy = program->blocks[0].instructions[3]->pseudo();
-   aco_print_instr(program->gfx_level, &parallelcopy, output);
-   fprintf(output, " scc:%u scratch:s%u\n", parallelcopy.tmp_in_scc,
-           parallelcopy.scratch_sgpr.reg());
-END_TEST
-
-BEGIN_TEST(regalloc.linear_vgpr.live_range_split.get_regs_for_copies)
-   //>> p_startpgm
-   if (!setup_cs("", GFX10))
-      return;
-
-   program->dev.vgpr_limit = 6;
-
-   PhysReg reg_v2{258};
-   PhysReg reg_v4{260};
-
-   //! lv1: %lin_tmp1:v[4] = p_unit_test
-   Temp lin_tmp = bld.pseudo(aco_opcode::p_unit_test, bld.def(v1.as_linear(), reg_v4));
-   //! v2: %log_tmp1:v[2-3] = p_unit_test
-   Temp log_tmp = bld.pseudo(aco_opcode::p_unit_test, bld.def(v2, reg_v2));
-
-   //! lv1: %lin_tmp2:v[0], v2: %log_tmp2:v[4-5] = p_parallelcopy %lin_tmp1:v[4], %log_tmp1:v[2-3]
-   //! v3: %_:v[1-3] = p_unit_test
-   bld.pseudo(aco_opcode::p_unit_test, bld.def(v3));
-
-   //! p_unit_test %log_tmp2:v[4-5], %lin_tmp2:v[0]
-   bld.pseudo(aco_opcode::p_unit_test, log_tmp, lin_tmp);
-
-   finish_ra_test(ra_test_policy());
-END_TEST
-
-BEGIN_TEST(regalloc.linear_vgpr.live_range_split.get_reg_create_vector)
-   //>> p_startpgm
-   if (!setup_cs("", GFX10))
-      return;
-
-   program->dev.vgpr_limit = 4;
-
-   PhysReg reg_v0{256};
-   PhysReg reg_v1{257};
-
-   //! lv1: %lin_tmp1:v[0] = p_unit_test
-   Temp lin_tmp = bld.pseudo(aco_opcode::p_unit_test, bld.def(v1.as_linear(), reg_v0));
-   //! v1: %log_tmp:v[1] = p_unit_test
-   Temp log_tmp = bld.pseudo(aco_opcode::p_unit_test, bld.def(v1, reg_v1));
-
-   //! lv1: %lin_tmp2:v[2] = p_parallelcopy %lin_tmp1:v[0]
-   //! v2: %_:v[0-1] = p_create_vector v1: undef, %log_tmp:v[1]
-   bld.pseudo(aco_opcode::p_create_vector, bld.def(v2), Operand(v1), log_tmp);
-
-   //! p_unit_test %lin_tmp2:v[2]
-   bld.pseudo(aco_opcode::p_unit_test, lin_tmp);
-
-   finish_ra_test(ra_test_policy());
-END_TEST
-
 BEGIN_TEST(regalloc.branch_def_phis_at_merge_block)
    //>> p_startpgm
    if (!setup_cs("", GFX10))
-- 
GitLab


From f47ae16a5957817411ea88015e3ffce00376ce75 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Wed, 31 Jan 2024 19:39:27 +0000
Subject: [PATCH 06/12] aco: only allow linear vgpr kills in top-level blocks

This is already the case, and requiring it will be useful in the future.

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_validate.cpp | 22 +++++++++++++++-------
 1 file changed, 15 insertions(+), 7 deletions(-)

diff --git a/src/amd/compiler/aco_validate.cpp b/src/amd/compiler/aco_validate.cpp
index d8d88fbdee668..b952c56d4d4cb 100644
--- a/src/amd/compiler/aco_validate.cpp
+++ b/src/amd/compiler/aco_validate.cpp
@@ -384,14 +384,22 @@ validate_ir(Program* program)
             }
          }
 
-         /* Check that linear vgprs are late kill: this is to ensure linear VGPR operands and
-          * normal VGPR definitions don't try to use the same register, which is problematic
-          * because of assignment restrictions.
-          */
          for (Operand& op : instr->operands) {
-            if (!op.isUndefined() && !op.isFixed() && op.hasRegClass() &&
-                op.regClass().is_linear_vgpr())
-               check(op.isLateKill(), "Linear VGPR operands must be late kill", instr.get());
+            if (op.isFixed() || !op.hasRegClass() || !op.regClass().is_linear_vgpr() ||
+                op.isUndefined())
+               continue;
+
+            /* Check that linear vgprs are late kill: this is to ensure linear VGPR operands and
+             * normal VGPR definitions don't try to use the same register, which is problematic
+             * because of assignment restrictions. */
+            check(op.isLateKill(), "Linear VGPR operands must be late kill", instr.get());
+
+            /* Only kill linear VGPRs in top-level blocks. Otherwise, we might have to move linear
+             * VGPRs to make space for normal ones and that isn't possible inside control flow. */
+            if (op.isKill()) {
+               check(block.kind & block_kind_top_level,
+                     "Linear VGPR operands must only be killed at top-level blocks", instr.get());
+            }
          }
 
          /* check subdword definitions */
-- 
GitLab


From 2ce5015df58b428c93b685d0bf30424d28416258 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Wed, 14 Feb 2024 15:30:44 +0000
Subject: [PATCH 07/12] aco/ra: constify various RegisterFile

This makes it more obvious that these functions don't change it.

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_register_allocation.cpp | 48 +++++++++++---------
 1 file changed, 27 insertions(+), 21 deletions(-)

diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index 8e1a601a76f5f..e1ff4fe9a1449 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -262,7 +262,7 @@ public:
 
    uint32_t& operator[](PhysReg index) { return regs[index]; }
 
-   unsigned count_zero(PhysRegInterval reg_interval)
+   unsigned count_zero(PhysRegInterval reg_interval) const
    {
       unsigned res = 0;
       for (PhysReg reg : reg_interval)
@@ -271,16 +271,17 @@ public:
    }
 
    /* Returns true if any of the bytes in the given range are allocated or blocked */
-   bool test(PhysReg start, unsigned num_bytes)
+   bool test(PhysReg start, unsigned num_bytes) const
    {
       for (PhysReg i = start; i.reg_b < start.reg_b + num_bytes; i = PhysReg(i + 1)) {
          assert(i <= 511);
          if (regs[i] & 0x0FFFFFFF)
             return true;
          if (regs[i] == 0xF0000000) {
-            assert(subdword_regs.find(i) != subdword_regs.end());
+            auto it = subdword_regs.find(i);
+            assert(it != subdword_regs.end());
             for (unsigned j = i.byte(); i * 4 + j < start.reg_b + num_bytes && j < 4; j++) {
-               if (subdword_regs[i][j])
+               if (it->second[j])
                   return true;
             }
          }
@@ -296,24 +297,28 @@ public:
          fill(start, rc.size(), 0xFFFFFFFF);
    }
 
-   bool is_blocked(PhysReg start)
+   bool is_blocked(PhysReg start) const
    {
       if (regs[start] == 0xFFFFFFFF)
          return true;
       if (regs[start] == 0xF0000000) {
+         auto it = subdword_regs.find(start);
+         assert(it != subdword_regs.end());
          for (unsigned i = start.byte(); i < 4; i++)
-            if (subdword_regs[start][i] == 0xFFFFFFFF)
+            if (it->second[i] == 0xFFFFFFFF)
                return true;
       }
       return false;
    }
 
-   bool is_empty_or_blocked(PhysReg start)
+   bool is_empty_or_blocked(PhysReg start) const
    {
       /* Empty is 0, blocked is 0xFFFFFFFF, so to check both we compare the
        * incremented value to 1 */
       if (regs[start] == 0xF0000000) {
-         return subdword_regs[start][start.byte()] + 1 <= 1;
+         auto it = subdword_regs.find(start);
+         assert(it != subdword_regs.end());
+         return it->second[start.byte()] + 1 <= 1;
       }
       return regs[start] + 1 <= 1;
    }
@@ -346,9 +351,9 @@ public:
 
    void clear(Definition def) { clear(def.physReg(), def.regClass()); }
 
-   unsigned get_id(PhysReg reg)
+   unsigned get_id(PhysReg reg) const
    {
-      return regs[reg] == 0xF0000000 ? subdword_regs[reg][reg.byte()] : regs[reg];
+      return regs[reg] == 0xF0000000 ? subdword_regs.at(reg)[reg.byte()] : regs[reg];
    }
 
 private:
@@ -376,7 +381,7 @@ private:
    }
 };
 
-std::vector<unsigned> find_vars(ra_ctx& ctx, RegisterFile& reg_file,
+std::vector<unsigned> find_vars(ra_ctx& ctx, const RegisterFile& reg_file,
                                 const PhysRegInterval reg_interval);
 
 /* helper function for debugging */
@@ -419,7 +424,7 @@ print_reg(const RegisterFile& reg_file, PhysReg reg, bool has_adjacent_variable)
 
 /* helper function for debugging */
 UNUSED void
-print_regs(ra_ctx& ctx, bool vgprs, RegisterFile& reg_file)
+print_regs(ra_ctx& ctx, bool vgprs, const RegisterFile& reg_file)
 {
    PhysRegInterval regs = get_reg_bounds(ctx.program, vgprs ? RegType::vgpr : RegType::sgpr);
    char reg_char = vgprs ? 'v' : 's';
@@ -880,7 +885,7 @@ update_renames(ra_ctx& ctx, RegisterFile& reg_file,
 }
 
 std::optional<PhysReg>
-get_reg_simple(ra_ctx& ctx, RegisterFile& reg_file, DefInfo info)
+get_reg_simple(ra_ctx& ctx, const RegisterFile& reg_file, DefInfo info)
 {
    const PhysRegInterval& bounds = info.bounds;
    uint32_t size = info.size;
@@ -973,7 +978,8 @@ get_reg_simple(ra_ctx& ctx, RegisterFile& reg_file, DefInfo info)
     * larger instruction encodings or copies
     * TODO: don't do this in situations where it doesn't benefit */
    if (rc.is_subdword()) {
-      for (std::pair<const uint32_t, std::array<uint32_t, 4>>& entry : reg_file.subdword_regs) {
+      for (const std::pair<const uint32_t, std::array<uint32_t, 4>>& entry :
+           reg_file.subdword_regs) {
          assert(reg_file[PhysReg{entry.first}] == 0xF0000000);
          if (!bounds.contains({PhysReg{entry.first}, rc.size()}))
             continue;
@@ -1003,7 +1009,7 @@ get_reg_simple(ra_ctx& ctx, RegisterFile& reg_file, DefInfo info)
 
 /* collect variables from a register area */
 std::vector<unsigned>
-find_vars(ra_ctx& ctx, RegisterFile& reg_file, const PhysRegInterval reg_interval)
+find_vars(ra_ctx& ctx, const RegisterFile& reg_file, const PhysRegInterval reg_interval)
 {
    std::vector<unsigned> vars;
    for (PhysReg j : reg_interval) {
@@ -1011,7 +1017,7 @@ find_vars(ra_ctx& ctx, RegisterFile& reg_file, const PhysRegInterval reg_interva
          continue;
       if (reg_file[j] == 0xF0000000) {
          for (unsigned k = 0; k < 4; k++) {
-            unsigned id = reg_file.subdword_regs[j][k];
+            unsigned id = reg_file.subdword_regs.at(j)[k];
             if (id && (vars.empty() || id != vars.back()))
                vars.emplace_back(id);
          }
@@ -1254,7 +1260,7 @@ get_regs_for_copies(ra_ctx& ctx, RegisterFile& reg_file,
 }
 
 std::optional<PhysReg>
-get_reg_impl(ra_ctx& ctx, RegisterFile& reg_file,
+get_reg_impl(ra_ctx& ctx, const RegisterFile& reg_file,
              std::vector<std::pair<Operand, Definition>>& parallelcopies, const DefInfo& info,
              aco_ptr<Instruction>& instr)
 {
@@ -1402,8 +1408,8 @@ get_reg_impl(ra_ctx& ctx, RegisterFile& reg_file,
 }
 
 bool
-get_reg_specified(ra_ctx& ctx, RegisterFile& reg_file, RegClass rc, aco_ptr<Instruction>& instr,
-                  PhysReg reg)
+get_reg_specified(ra_ctx& ctx, const RegisterFile& reg_file, RegClass rc,
+                  aco_ptr<Instruction>& instr, PhysReg reg)
 {
    /* catch out-of-range registers */
    if (reg >= PhysReg{512})
@@ -1538,7 +1544,7 @@ compact_relocate_vars(ra_ctx& ctx, const std::vector<IDAndRegClass>& vars,
 }
 
 bool
-is_mimg_vaddr_intact(ra_ctx& ctx, RegisterFile& reg_file, Instruction* instr)
+is_mimg_vaddr_intact(ra_ctx& ctx, const RegisterFile& reg_file, Instruction* instr)
 {
    PhysReg first{512};
    for (unsigned i = 0; i < instr->operands.size() - 3u; i++) {
@@ -1570,7 +1576,7 @@ is_mimg_vaddr_intact(ra_ctx& ctx, RegisterFile& reg_file, Instruction* instr)
 }
 
 std::optional<PhysReg>
-get_reg_vector(ra_ctx& ctx, RegisterFile& reg_file, Temp temp, aco_ptr<Instruction>& instr)
+get_reg_vector(ra_ctx& ctx, const RegisterFile& reg_file, Temp temp, aco_ptr<Instruction>& instr)
 {
    Instruction* vec = ctx.vectors[temp.id()];
    unsigned first_operand = vec->format == Format::MIMG ? 3 : 0;
-- 
GitLab


From 844c711f051243d765659d9f792bb291f11ff9b1 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Wed, 14 Feb 2024 15:32:13 +0000
Subject: [PATCH 08/12] aco/ra: move parallelcopy creation into helper

This is almost a direct copy+paste into it's own function.

This is useful both for future work and the make the caller smaller.

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_register_allocation.cpp | 125 ++++++++++---------
 1 file changed, 67 insertions(+), 58 deletions(-)

diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index e1ff4fe9a1449..74b751ebca2b2 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -2727,6 +2727,72 @@ optimize_encoding(Program* program, ra_ctx& ctx, RegisterFile& register_file,
       optimize_encoding_sopk(program, ctx, register_file, instr);
 }
 
+void
+emit_parallel_copy(ra_ctx& ctx, std::vector<std::pair<Operand, Definition>>& parallelcopy,
+                   aco_ptr<Instruction>& instr, std::vector<aco_ptr<Instruction>>& instructions,
+                   bool temp_in_scc, RegisterFile& register_file)
+{
+   if (parallelcopy.empty())
+      return;
+
+   aco_ptr<Pseudo_instruction> pc;
+   pc.reset(create_instruction<Pseudo_instruction>(aco_opcode::p_parallelcopy, Format::PSEUDO,
+                                                   parallelcopy.size(), parallelcopy.size()));
+   bool linear_vgpr = false;
+   bool sgpr_operands_alias_defs = false;
+   uint64_t sgpr_operands[4] = {0, 0, 0, 0};
+   for (unsigned i = 0; i < parallelcopy.size(); i++) {
+      linear_vgpr |= parallelcopy[i].first.regClass().is_linear_vgpr();
+
+      if (temp_in_scc && parallelcopy[i].first.isTemp() &&
+          parallelcopy[i].first.getTemp().type() == RegType::sgpr) {
+         if (!sgpr_operands_alias_defs) {
+            unsigned reg = parallelcopy[i].first.physReg().reg();
+            unsigned size = parallelcopy[i].first.getTemp().size();
+            sgpr_operands[reg / 64u] |= u_bit_consecutive64(reg % 64u, size);
+
+            reg = parallelcopy[i].second.physReg().reg();
+            size = parallelcopy[i].second.getTemp().size();
+            if (sgpr_operands[reg / 64u] & u_bit_consecutive64(reg % 64u, size))
+               sgpr_operands_alias_defs = true;
+         }
+      }
+
+      pc->operands[i] = parallelcopy[i].first;
+      pc->definitions[i] = parallelcopy[i].second;
+      assert(pc->operands[i].size() == pc->definitions[i].size());
+
+      /* it might happen that the operand is already renamed. we have to restore the
+       * original name. */
+      std::unordered_map<unsigned, Temp>::iterator it =
+         ctx.orig_names.find(pc->operands[i].tempId());
+      Temp orig = it != ctx.orig_names.end() ? it->second : pc->operands[i].getTemp();
+      ctx.orig_names[pc->definitions[i].tempId()] = orig;
+      ctx.renames[ctx.block->index][orig.id()] = pc->definitions[i].getTemp();
+   }
+
+   if (temp_in_scc && (sgpr_operands_alias_defs || linear_vgpr)) {
+      /* disable definitions and re-enable operands */
+      RegisterFile tmp_file(register_file);
+      for (const Definition& def : instr->definitions) {
+         if (def.isTemp() && !def.isKill())
+            tmp_file.clear(def);
+      }
+      for (const Operand& op : instr->operands) {
+         if (op.isTemp() && op.isFirstKill())
+            tmp_file.block(op.physReg(), op.regClass());
+      }
+
+      handle_pseudo(ctx, tmp_file, pc.get());
+   } else {
+      pc->tmp_in_scc = false;
+   }
+
+   instructions.emplace_back(std::move(pc));
+
+   parallelcopy.clear();
+}
+
 } /* end namespace */
 
 void
@@ -3018,64 +3084,7 @@ register_allocation(Program* program, std::vector<IDSet>& live_out_per_block, ra
                add_subdword_operand(ctx, instr, i, op.physReg().byte(), op.regClass());
          }
 
-         /* emit parallelcopy */
-         if (!parallelcopy.empty()) {
-            aco_ptr<Pseudo_instruction> pc;
-            pc.reset(create_instruction<Pseudo_instruction>(aco_opcode::p_parallelcopy,
-                                                            Format::PSEUDO, parallelcopy.size(),
-                                                            parallelcopy.size()));
-            bool linear_vgpr = false;
-            bool sgpr_operands_alias_defs = false;
-            uint64_t sgpr_operands[4] = {0, 0, 0, 0};
-            for (unsigned i = 0; i < parallelcopy.size(); i++) {
-               linear_vgpr |= parallelcopy[i].first.regClass().is_linear_vgpr();
-
-               if (temp_in_scc && parallelcopy[i].first.isTemp() &&
-                   parallelcopy[i].first.getTemp().type() == RegType::sgpr) {
-                  if (!sgpr_operands_alias_defs) {
-                     unsigned reg = parallelcopy[i].first.physReg().reg();
-                     unsigned size = parallelcopy[i].first.getTemp().size();
-                     sgpr_operands[reg / 64u] |= u_bit_consecutive64(reg % 64u, size);
-
-                     reg = parallelcopy[i].second.physReg().reg();
-                     size = parallelcopy[i].second.getTemp().size();
-                     if (sgpr_operands[reg / 64u] & u_bit_consecutive64(reg % 64u, size))
-                        sgpr_operands_alias_defs = true;
-                  }
-               }
-
-               pc->operands[i] = parallelcopy[i].first;
-               pc->definitions[i] = parallelcopy[i].second;
-               assert(pc->operands[i].size() == pc->definitions[i].size());
-
-               /* it might happen that the operand is already renamed. we have to restore the
-                * original name. */
-               std::unordered_map<unsigned, Temp>::iterator it =
-                  ctx.orig_names.find(pc->operands[i].tempId());
-               Temp orig = it != ctx.orig_names.end() ? it->second : pc->operands[i].getTemp();
-               ctx.orig_names[pc->definitions[i].tempId()] = orig;
-               ctx.renames[block.index][orig.id()] = pc->definitions[i].getTemp();
-            }
-
-            if (temp_in_scc && (sgpr_operands_alias_defs || linear_vgpr)) {
-               /* disable definitions and re-enable operands */
-               RegisterFile tmp_file(register_file);
-               for (const Definition& def : instr->definitions) {
-                  if (def.isTemp() && !def.isKill())
-                     tmp_file.clear(def);
-               }
-               for (const Operand& op : instr->operands) {
-                  if (op.isTemp() && op.isFirstKill())
-                     tmp_file.block(op.physReg(), op.regClass());
-               }
-
-               handle_pseudo(ctx, tmp_file, pc.get());
-            } else {
-               pc->tmp_in_scc = false;
-            }
-
-            instructions.emplace_back(std::move(pc));
-         }
+         emit_parallel_copy(ctx, parallelcopy, instr, instructions, temp_in_scc, register_file);
 
          /* some instructions need VOP3 encoding if operand/definition is not assigned to VCC */
          bool instr_needs_vop3 =
-- 
GitLab


From a1171f46b226efd1826d6d925a1a523df5284aff Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Tue, 13 Feb 2024 18:07:13 +0000
Subject: [PATCH 09/12] aco/ra: change get_reg_bounds() helper

We will have a separate bounds for linear VGPRs.

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_register_allocation.cpp | 43 +++++++++++---------
 1 file changed, 24 insertions(+), 19 deletions(-)

diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index 74b751ebca2b2..7576e645bed43 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -194,15 +194,21 @@ get_stride(RegClass rc)
 }
 
 PhysRegInterval
-get_reg_bounds(Program* program, RegType type)
+get_reg_bounds(ra_ctx& ctx, RegType type, bool linear_vgpr)
 {
    if (type == RegType::vgpr) {
-      return {PhysReg{256}, (unsigned)program->max_reg_demand.vgpr};
+      return {PhysReg{256}, (unsigned)ctx.program->max_reg_demand.vgpr};
    } else {
-      return {PhysReg{0}, (unsigned)program->max_reg_demand.sgpr};
+      return {PhysReg{0}, (unsigned)ctx.program->max_reg_demand.sgpr};
    }
 }
 
+PhysRegInterval
+get_reg_bounds(ra_ctx& ctx, RegClass rc)
+{
+   return get_reg_bounds(ctx, rc.type(), rc.is_linear_vgpr());
+}
+
 struct DefInfo {
    PhysRegInterval bounds;
    uint8_t size;
@@ -214,7 +220,7 @@ struct DefInfo {
       size = rc.size();
       stride = get_stride(rc);
 
-      bounds = get_reg_bounds(ctx.program, rc.type());
+      bounds = get_reg_bounds(ctx, rc);
 
       if (rc.is_subdword() && operand >= 0) {
          /* stride in bytes */
@@ -424,10 +430,9 @@ print_reg(const RegisterFile& reg_file, PhysReg reg, bool has_adjacent_variable)
 
 /* helper function for debugging */
 UNUSED void
-print_regs(ra_ctx& ctx, bool vgprs, const RegisterFile& reg_file)
+print_regs(ra_ctx& ctx, PhysRegInterval regs, const RegisterFile& reg_file)
 {
-   PhysRegInterval regs = get_reg_bounds(ctx.program, vgprs ? RegType::vgpr : RegType::sgpr);
-   char reg_char = vgprs ? 'v' : 's';
+   char reg_char = regs.lo().reg() >= 256 ? 'v' : 's';
    const int max_regs_per_line = 64;
 
    /* print markers */
@@ -483,11 +488,11 @@ print_regs(ra_ctx& ctx, bool vgprs, const RegisterFile& reg_file)
           ctx.orig_names[size_id.second].id() != size_id.second) {
          printf("(was %%%d) ", ctx.orig_names[size_id.second].id());
       }
-      printf("= %c[%d", reg_char, first_reg.reg() - regs.lo());
+      printf("= %c[%d", reg_char, first_reg.reg() % 256);
       PhysReg last_reg = first_reg.advance(size_id.first - 1);
       if (first_reg.reg() != last_reg.reg()) {
          assert(first_reg.byte() == 0 && last_reg.byte() == 3);
-         printf("-%d", last_reg.reg() - regs.lo());
+         printf("-%d", last_reg.reg() % 256);
       }
       printf("]");
       if (first_reg.byte() != 0 || last_reg.byte() != 3) {
@@ -1116,7 +1121,7 @@ get_regs_for_copies(ra_ctx& ctx, RegisterFile& reg_file,
    /* Variables are sorted from large to small and with increasing assigned register */
    for (unsigned id : vars) {
       assignment& var = ctx.assignments[id];
-      PhysRegInterval bounds = get_reg_bounds(ctx.program, var.rc.type());
+      PhysRegInterval bounds = get_reg_bounds(ctx, var.rc);
       DefInfo info = DefInfo(ctx, ctx.pseudo_dummy, var.rc, -1);
       uint32_t size = info.size;
 
@@ -1428,7 +1433,7 @@ get_reg_specified(ra_ctx& ctx, const RegisterFile& reg_file, RegClass rc,
       return false;
 
    PhysRegInterval reg_win = {reg, rc.size()};
-   PhysRegInterval bounds = get_reg_bounds(ctx.program, rc.type());
+   PhysRegInterval bounds = get_reg_bounds(ctx, rc);
    PhysRegInterval vcc_win = {vcc, 2};
    /* VCC is outside the bounds */
    bool is_vcc = rc.type() == RegType::sgpr && vcc_win.contains(reg_win) && ctx.program->needs_vcc;
@@ -1451,12 +1456,12 @@ get_reg_specified(ra_ctx& ctx, const RegisterFile& reg_file, RegClass rc,
 }
 
 bool
-increase_register_file(ra_ctx& ctx, RegType type)
+increase_register_file(ra_ctx& ctx, RegClass rc)
 {
-   if (type == RegType::vgpr && ctx.program->max_reg_demand.vgpr < ctx.vgpr_limit) {
+   if (rc.type() == RegType::vgpr && ctx.program->max_reg_demand.vgpr < ctx.vgpr_limit) {
       update_vgpr_sgpr_demand(ctx.program, RegisterDemand(ctx.program->max_reg_demand.vgpr + 1,
                                                           ctx.program->max_reg_demand.sgpr));
-   } else if (type == RegType::sgpr && ctx.program->max_reg_demand.sgpr < ctx.sgpr_limit) {
+   } else if (rc.type() == RegType::sgpr && ctx.program->max_reg_demand.sgpr < ctx.sgpr_limit) {
       update_vgpr_sgpr_demand(ctx.program, RegisterDemand(ctx.program->max_reg_demand.vgpr,
                                                           ctx.program->max_reg_demand.sgpr + 1));
    } else {
@@ -1554,7 +1559,7 @@ is_mimg_vaddr_intact(ra_ctx& ctx, const RegisterFile& reg_file, Instruction* ins
          PhysReg reg = ctx.assignments[op.tempId()].reg;
 
          if (first.reg() == 512) {
-            PhysRegInterval bounds = get_reg_bounds(ctx.program, RegType::vgpr);
+            PhysRegInterval bounds = get_reg_bounds(ctx, RegType::vgpr, false);
             first = reg.advance(i * -4);
             PhysRegInterval vec = PhysRegInterval{first, instr->operands.size() - 3u};
             if (!bounds.contains(vec)) /* not enough space for other operands */
@@ -1695,7 +1700,7 @@ get_reg(ra_ctx& ctx, RegisterFile& reg_file, Temp temp,
     * too many moves. */
    assert(reg_file.count_zero(info.bounds) >= info.size);
 
-   if (!increase_register_file(ctx, info.rc.type())) {
+   if (!increase_register_file(ctx, info.rc)) {
       /* fallback algorithm: reallocate all variables at once */
       unsigned def_size = info.rc.size();
       for (Definition def : instr->definitions) {
@@ -1709,7 +1714,7 @@ get_reg(ra_ctx& ctx, RegisterFile& reg_file, Temp temp,
             killed_op_size += op.regClass().size();
       }
 
-      const PhysRegInterval regs = get_reg_bounds(ctx.program, info.rc.type());
+      const PhysRegInterval regs = get_reg_bounds(ctx, info.rc);
 
       /* reallocate passthrough variables and non-killed operands */
       std::vector<IDAndRegClass> vars;
@@ -1750,7 +1755,7 @@ get_reg_create_vector(ra_ctx& ctx, RegisterFile& reg_file, Temp temp,
    uint32_t size = rc.size();
    uint32_t bytes = rc.bytes();
    uint32_t stride = get_stride(rc);
-   PhysRegInterval bounds = get_reg_bounds(ctx.program, rc.type());
+   PhysRegInterval bounds = get_reg_bounds(ctx, rc);
 
    // TODO: improve p_create_vector for sub-dword vectors
 
@@ -1870,7 +1875,7 @@ get_reg_create_vector(ra_ctx& ctx, RegisterFile& reg_file, Temp temp,
    success = get_regs_for_copies(ctx, tmp_file, pc, vars, instr, PhysRegInterval{best_pos, size});
 
    if (!success) {
-      if (!increase_register_file(ctx, temp.type())) {
+      if (!increase_register_file(ctx, temp.regClass())) {
          /* use the fallback algorithm in get_reg() */
          return get_reg(ctx, reg_file, temp, parallelcopies, instr);
       }
-- 
GitLab


From 4bec99ff3f65c418cb73106a44395acd470852fc Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Wed, 14 Feb 2024 19:55:59 +0000
Subject: [PATCH 10/12] aco/ra: rework linear VGPR allocation

We allocate them at the end of the register file and keep them separate
from normal VGPRs. This is for two reasons:
- Because we only ever move linear VGPRs into an empty space or a space
  previously occupied by a linear one, we never have to swap a normal VGPR
  and a linear one. This simplifies copy lowering.
- As linear VGPR's live ranges only start and end on top-level blocks, we
  never have to move a linear VGPR in control flow.

fossil-db (navi31):
Totals from 5493 (6.93% of 79242) affected shaders:
MaxWaves: 150365 -> 150343 (-0.01%)
Instrs: 7974740 -> 7976069 (+0.02%); split: -0.06%, +0.08%
CodeSize: 41296024 -> 41299008 (+0.01%); split: -0.06%, +0.06%
VGPRs: 283192 -> 329560 (+16.37%)
Latency: 64267936 -> 64268410 (+0.00%); split: -0.17%, +0.17%
InvThroughput: 10954037 -> 10951737 (-0.02%); split: -0.09%, +0.07%
VClause: 132792 -> 132956 (+0.12%); split: -0.06%, +0.18%
SClause: 223854 -> 223841 (-0.01%); split: -0.01%, +0.01%
Copies: 559574 -> 561391 (+0.32%); split: -0.24%, +0.56%
Branches: 179630 -> 179636 (+0.00%); split: -0.02%, +0.02%
VALU: 4572683 -> 4574487 (+0.04%); split: -0.03%, +0.07%
SALU: 772076 -> 772107 (+0.00%); split: -0.01%, +0.01%
VOPD: 1095 -> 1099 (+0.37%); split: +0.73%, -0.37%

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_interface.cpp           |   2 +-
 src/amd/compiler/aco_ir.h                    |   3 +-
 src/amd/compiler/aco_register_allocation.cpp | 217 +++++++++++++++++--
 src/amd/compiler/tests/helpers.cpp           |   2 +-
 src/amd/compiler/tests/test_d3d11_derivs.cpp |  27 ++-
 src/amd/compiler/tests/test_regalloc.cpp     |   3 +
 6 files changed, 217 insertions(+), 37 deletions(-)

diff --git a/src/amd/compiler/aco_interface.cpp b/src/amd/compiler/aco_interface.cpp
index c42bcb8724ab3..7e418caaa55f2 100644
--- a/src/amd/compiler/aco_interface.cpp
+++ b/src/amd/compiler/aco_interface.cpp
@@ -176,7 +176,7 @@ aco_postprocess_shader(const struct aco_compiler_options* options,
       validate(program.get());
 
       /* Register Allocation */
-      aco::register_allocation(program.get(), live_vars.live_out);
+      aco::register_allocation(program.get(), live_vars);
 
       if (aco::validate_ra(program.get())) {
          aco_print_program(program.get(), stderr);
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index 184b4605da982..54dc2ec790ee2 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -2221,8 +2221,7 @@ void optimize(Program* program);
 void optimize_postRA(Program* program);
 void setup_reduce_temp(Program* program);
 void lower_to_cssa(Program* program, live& live_vars);
-void register_allocation(Program* program, std::vector<IDSet>& live_out_per_block,
-                         ra_test_policy = {});
+void register_allocation(Program* program, live& live_vars, ra_test_policy = {});
 void ssa_elimination(Program* program);
 void lower_to_hw_instr(Program* program);
 void schedule_program(Program* program, live& live_vars);
diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index 7576e645bed43..7dad828f7fe5f 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -89,6 +89,10 @@ struct ra_ctx {
    uint16_t vgpr_limit;
    std::bitset<512> war_hint;
 
+   uint16_t sgpr_bounds;
+   uint16_t vgpr_bounds;
+   uint16_t linear_vgpr_start;
+
    ra_test_policy policy;
 
    ra_ctx(Program* program_, ra_test_policy policy_)
@@ -101,6 +105,10 @@ struct ra_ctx {
          create_instruction<Instruction>(aco_opcode::p_linear_phi, Format::PSEUDO, 0, 0));
       sgpr_limit = get_addr_sgpr_from_waves(program, program->min_waves);
       vgpr_limit = get_addr_vgpr_from_waves(program, program->min_waves);
+
+      sgpr_bounds = program->max_reg_demand.sgpr;
+      vgpr_bounds = program->max_reg_demand.vgpr;
+      linear_vgpr_start = vgpr_bounds;
    }
 };
 
@@ -196,10 +204,13 @@ get_stride(RegClass rc)
 PhysRegInterval
 get_reg_bounds(ra_ctx& ctx, RegType type, bool linear_vgpr)
 {
-   if (type == RegType::vgpr) {
-      return {PhysReg{256}, (unsigned)ctx.program->max_reg_demand.vgpr};
+   if (type == RegType::vgpr && linear_vgpr) {
+      int size = ctx.vgpr_bounds - ctx.linear_vgpr_start;
+      return PhysRegInterval{PhysReg(256 + ctx.linear_vgpr_start), (unsigned)size};
+   } else if (type == RegType::vgpr) {
+      return PhysRegInterval{PhysReg(256), ctx.linear_vgpr_start};
    } else {
-      return {PhysReg{0}, (unsigned)ctx.program->max_reg_demand.sgpr};
+      return PhysRegInterval{PhysReg(0), ctx.sgpr_bounds};
    }
 }
 
@@ -251,8 +262,10 @@ struct DefInfo {
          bool imageGather4D16Bug = operand == -1 && rc == v2 && instr->mimg().dmask != 0xF;
          assert(ctx.program->gfx_level == GFX9 && "Image D16 on GFX8 not supported.");
 
-         if (imageGather4D16Bug)
-            bounds.size -= rc.bytes() / 4;
+         if (imageGather4D16Bug) {
+            int num_linear_vgprs = ctx.vgpr_bounds - ctx.linear_vgpr_start;
+            bounds.size -= MAX2(rc.bytes() / 4 - num_linear_vgprs, 0);
+         }
       }
    }
 };
@@ -1294,7 +1307,6 @@ get_reg_impl(ra_ctx& ctx, const RegisterFile& reg_file,
       }
    }
 
-   assert(regs_free >= size);
    /* we might have to move dead operands to dst in order to make space */
    unsigned op_moves = 0;
 
@@ -1458,15 +1470,26 @@ get_reg_specified(ra_ctx& ctx, const RegisterFile& reg_file, RegClass rc,
 bool
 increase_register_file(ra_ctx& ctx, RegClass rc)
 {
-   if (rc.type() == RegType::vgpr && ctx.program->max_reg_demand.vgpr < ctx.vgpr_limit) {
-      update_vgpr_sgpr_demand(ctx.program, RegisterDemand(ctx.program->max_reg_demand.vgpr + 1,
-                                                          ctx.program->max_reg_demand.sgpr));
+   if (rc.type() == RegType::vgpr && !rc.is_linear_vgpr() &&
+       ctx.linear_vgpr_start == ctx.vgpr_bounds && ctx.vgpr_bounds < ctx.vgpr_limit) {
+      /* vgpr_bounds might be smaller than max_reg_demand because we might not have increased it
+       * when increasing SGPR demand earler because there were linear VGPRs.
+       */
+      update_vgpr_sgpr_demand(
+         ctx.program, RegisterDemand(ctx.vgpr_bounds + 1, ctx.program->max_reg_demand.sgpr));
    } else if (rc.type() == RegType::sgpr && ctx.program->max_reg_demand.sgpr < ctx.sgpr_limit) {
       update_vgpr_sgpr_demand(ctx.program, RegisterDemand(ctx.program->max_reg_demand.vgpr,
                                                           ctx.program->max_reg_demand.sgpr + 1));
    } else {
       return false;
    }
+
+   if (ctx.linear_vgpr_start == ctx.vgpr_bounds) {
+      ctx.vgpr_bounds = ctx.program->max_reg_demand.vgpr;
+      ctx.linear_vgpr_start = ctx.vgpr_bounds;
+   }
+   ctx.sgpr_bounds = ctx.program->max_reg_demand.sgpr;
+
    return true;
 }
 
@@ -1632,6 +1655,124 @@ get_reg_vector(ra_ctx& ctx, const RegisterFile& reg_file, Temp temp, aco_ptr<Ins
    return {};
 }
 
+bool
+compact_linear_vgprs(ra_ctx& ctx, const RegisterFile& reg_file,
+                     std::vector<std::pair<Operand, Definition>>& parallelcopies)
+{
+   PhysRegInterval linear_vgpr_bounds = get_reg_bounds(ctx, RegType::vgpr, true);
+   int zeros = reg_file.count_zero(linear_vgpr_bounds);
+   if (zeros == 0)
+      return false;
+
+   std::vector<IDAndRegClass> vars;
+   for (unsigned id : find_vars(ctx, reg_file, linear_vgpr_bounds))
+      vars.emplace_back(id, ctx.assignments[id].rc);
+
+   ctx.linear_vgpr_start += zeros;
+   compact_relocate_vars(ctx, vars, parallelcopies, PhysReg(256 + ctx.linear_vgpr_start));
+
+   return true;
+}
+
+/* Allocates a linear VGPR. We allocate them at the end of the register file and keep them separate
+ * from normal VGPRs. This is for two reasons:
+ * - Because we only ever move linear VGPRs into an empty space or a space previously occupied by a
+ *   linear one, we never have to swap a normal VGPR and a linear one.
+ * - As linear VGPR's live ranges only start and end on top-level blocks, we never have to move a
+ *   linear VGPR in control flow.
+ */
+PhysReg
+alloc_linear_vgpr(ra_ctx& ctx, const RegisterFile& reg_file, aco_ptr<Instruction>& instr,
+                  std::vector<std::pair<Operand, Definition>>& parallelcopies)
+{
+   assert(instr->opcode == aco_opcode::p_start_linear_vgpr);
+   assert(instr->definitions.size() == 1 && instr->definitions[0].bytes() % 4 == 0);
+
+   RegClass rc = instr->definitions[0].regClass();
+
+   /* Try to choose an unused space in the linear VGPR bounds. */
+   for (int i = (int)ctx.vgpr_bounds - (int)rc.size(); i >= (int)ctx.linear_vgpr_start; i--) {
+      PhysReg reg(256 + i);
+      if (!reg_file.test(reg, rc.bytes())) {
+         adjust_max_used_regs(ctx, rc, reg);
+         return reg;
+      }
+   }
+
+   PhysRegInterval old_normal_bounds = get_reg_bounds(ctx, RegType::vgpr, false);
+
+   /* Compact linear VGPRs, grow the bounds if necessary, and choose a space at the beginning: */
+   compact_linear_vgprs(ctx, reg_file, parallelcopies);
+
+   PhysReg reg(256 + ctx.linear_vgpr_start - rc.size());
+   /* Space that was for normal VGPRs, but is now for linear VGPRs. */
+   PhysRegInterval new_win = PhysRegInterval::from_until(reg, MAX2(old_normal_bounds.hi(), reg));
+
+   RegisterFile tmp_file(reg_file);
+   PhysRegInterval reg_win{reg, rc.size()};
+   std::vector<unsigned> blocking_vars = collect_vars(ctx, tmp_file, new_win);
+
+   /* Re-enable killed operands */
+   for (Operand& op : instr->operands) {
+      if (op.isTemp() && op.isFirstKillBeforeDef())
+         tmp_file.fill(op);
+   }
+
+   /* Find new assignments for blocking vars. */
+   std::vector<std::pair<Operand, Definition>> pc;
+   if (!ctx.policy.skip_optimistic_path &&
+       get_regs_for_copies(ctx, tmp_file, pc, blocking_vars, instr, reg_win)) {
+      parallelcopies.insert(parallelcopies.end(), pc.begin(), pc.end());
+   } else {
+      /* Fallback algorithm: reallocate all variables at once. */
+      std::vector<IDAndRegClass> vars;
+      for (unsigned id : find_vars(ctx, reg_file, old_normal_bounds))
+         vars.emplace_back(id, ctx.assignments[id].rc);
+      compact_relocate_vars(ctx, vars, parallelcopies, PhysReg(256));
+
+      std::vector<IDAndRegClass> killed_op_vars;
+      for (Operand& op : instr->operands) {
+         if (op.isTemp() && op.isFirstKillBeforeDef() && op.regClass().type() == RegType::vgpr)
+            killed_op_vars.emplace_back(op.tempId(), op.regClass());
+      }
+      compact_relocate_vars(ctx, killed_op_vars, parallelcopies, reg_win.lo());
+   }
+
+   /* If this is decreased earlier, a killed operand can't be placed inside the definition. */
+   ctx.linear_vgpr_start -= rc.size();
+
+   adjust_max_used_regs(ctx, rc, reg);
+   return reg;
+}
+
+bool
+should_compact_linear_vgprs(ra_ctx& ctx, live& live_vars, const RegisterFile& reg_file)
+{
+   if (!(ctx.block->kind & block_kind_top_level) || ctx.block->linear_succs.empty())
+      return false;
+
+   /* Since we won't be able to copy linear VGPRs to make space when in control flow, we have to
+    * ensure in advance that there is enough space for normal VGPRs. */
+   unsigned max_vgpr_usage = 0;
+   unsigned next_toplevel = ctx.block->index + 1;
+   for (; !(ctx.program->blocks[next_toplevel].kind & block_kind_top_level); next_toplevel++) {
+      max_vgpr_usage =
+         MAX2(max_vgpr_usage, (unsigned)ctx.program->blocks[next_toplevel].register_demand.vgpr);
+   }
+
+   std::vector<aco_ptr<Instruction>>& instructions =
+      ctx.program->blocks[next_toplevel].instructions;
+   if (!instructions.empty() && is_phi(instructions[0])) {
+      max_vgpr_usage =
+         MAX2(max_vgpr_usage, (unsigned)live_vars.register_demand[next_toplevel][0].vgpr);
+   }
+
+   for (unsigned tmp : find_vars(ctx, reg_file, get_reg_bounds(ctx, RegType::vgpr, true)))
+      max_vgpr_usage -= ctx.assignments[tmp].rc.size();
+
+   return max_vgpr_usage > get_reg_bounds(ctx, RegType::vgpr, false).size;
+}
+
 PhysReg
 get_reg(ra_ctx& ctx, RegisterFile& reg_file, Temp temp,
         std::vector<std::pair<Operand, Definition>>& parallelcopies, aco_ptr<Instruction>& instr,
@@ -1694,14 +1835,25 @@ get_reg(ra_ctx& ctx, RegisterFile& reg_file, Temp temp,
    if (res)
       return *res;
 
-   /* try using more registers */
+   /* try compacting the linear vgprs to make more space */
+   std::vector<std::pair<Operand, Definition>> pc;
+   if (info.rc.type() == RegType::vgpr && (ctx.block->kind & block_kind_top_level) &&
+       compact_linear_vgprs(ctx, reg_file, pc)) {
+      parallelcopies.insert(parallelcopies.end(), pc.begin(), pc.end());
 
-   /* We should only fail here because keeping under the limit would require
-    * too many moves. */
-   assert(reg_file.count_zero(info.bounds) >= info.size);
+      /* We don't need to fill the copy definitions in because we don't care about the linear VGPR
+       * space here. */
+      RegisterFile tmp_file(reg_file);
+      for (std::pair<Operand, Definition>& copy : pc)
+         tmp_file.clear(copy.first);
 
+      return get_reg(ctx, tmp_file, temp, parallelcopies, instr, operand_index);
+   }
+
+   /* try using more registers */
    if (!increase_register_file(ctx, info.rc)) {
-      /* fallback algorithm: reallocate all variables at once */
+      /* fallback algorithm: reallocate all variables at once (linear VGPRs should already be
+       * compact at the end) */
       unsigned def_size = info.rc.size();
       for (Definition def : instr->definitions) {
          if (ctx.assignments[def.tempId()].assigned && def.regClass().type() == info.rc.type())
@@ -2801,8 +2953,9 @@ emit_parallel_copy(ra_ctx& ctx, std::vector<std::pair<Operand, Definition>>& par
 } /* end namespace */
 
 void
-register_allocation(Program* program, std::vector<IDSet>& live_out_per_block, ra_test_policy policy)
+register_allocation(Program* program, live& live_vars, ra_test_policy policy)
 {
+   std::vector<IDSet>& live_out_per_block = live_vars.live_out;
    ra_ctx ctx(program, policy);
    get_affinities(ctx, live_out_per_block);
 
@@ -2880,19 +3033,25 @@ register_allocation(Program* program, std::vector<IDSet>& live_out_per_block, ra
             continue;
          }
 
-         /* unconditional branches are handled after phis of the target */
+         std::vector<std::pair<Operand, Definition>> parallelcopy;
+         bool temp_in_scc = register_file[scc];
+
          if (instr->opcode == aco_opcode::p_branch) {
             /* last instruction of the block */
+            if (should_compact_linear_vgprs(ctx, live_vars, register_file)) {
+               compact_linear_vgprs(ctx, register_file, parallelcopy);
+               update_renames(ctx, register_file, parallelcopy, instr, (UpdateRenames)0);
+               emit_parallel_copy(ctx, parallelcopy, instr, instructions, temp_in_scc,
+                                  register_file);
+            }
+
+            /* unconditional branches are handled after phis of the target */
             instructions.emplace_back(std::move(instr));
             break;
          }
 
-         std::vector<std::pair<Operand, Definition>> parallelcopy;
-
          assert(!is_phi(instr));
 
-         bool temp_in_scc = register_file[scc];
-
          /* handle operands */
          bool fixed = false;
          for (unsigned i = 0; i < instr->operands.size(); ++i) {
@@ -3002,7 +3161,11 @@ register_allocation(Program* program, std::vector<IDSet>& live_out_per_block, ra
                continue;
 
             /* find free reg */
-            if (instr->opcode == aco_opcode::p_split_vector) {
+            if (instr->opcode == aco_opcode::p_start_linear_vgpr) {
+               /* Allocation of linear VGPRs is special. */
+               definition->setFixed(alloc_linear_vgpr(ctx, register_file, instr, parallelcopy));
+               update_renames(ctx, register_file, parallelcopy, instr, rename_not_killed_ops);
+            } else if (instr->opcode == aco_opcode::p_split_vector) {
                PhysReg reg = instr->operands[0].physReg();
                RegClass rc = definition->regClass();
                for (unsigned j = 0; j < i; j++)
@@ -3073,6 +3236,11 @@ register_allocation(Program* program, std::vector<IDSet>& live_out_per_block, ra
             register_file.fill(*definition);
          }
 
+         if (instr->isBranch() && should_compact_linear_vgprs(ctx, live_vars, register_file)) {
+            compact_linear_vgprs(ctx, register_file, parallelcopy);
+            update_renames(ctx, register_file, parallelcopy, instr, rename_not_killed_ops);
+         }
+
          handle_pseudo(ctx, register_file, instr.get());
 
          /* kill definitions and late-kill operands and ensure that sub-dword operands can actually
@@ -3160,6 +3328,13 @@ register_allocation(Program* program, std::vector<IDSet>& live_out_per_block, ra
 
       } /* end for Instr */
 
+      if ((block.kind & block_kind_top_level) && block.linear_succs.empty()) {
+         /* Reset this for block_kind_resume. */
+         ASSERTED PhysRegInterval linear_vgpr_bounds = get_reg_bounds(ctx, RegType::vgpr, true);
+         assert(register_file.count_zero(linear_vgpr_bounds) == linear_vgpr_bounds.size);
+         ctx.linear_vgpr_start = ctx.vgpr_bounds;
+      }
+
       block.instructions = std::move(instructions);
    } /* end for BB */
 
-- 
GitLab


From 7b16ea8b22657580e7869df8eb3fbb7ca6d643d3 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Mon, 29 Jan 2024 19:58:32 +0000
Subject: [PATCH 11/12] aco/ra: disable live range splitting of linear vgprs

These shouldn't happen anymore.

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_register_allocation.cpp | 21 +++++++-------------
 1 file changed, 7 insertions(+), 14 deletions(-)

diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index 7dad828f7fe5f..9aed9801fcf29 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -1216,9 +1216,8 @@ get_regs_for_copies(ra_ctx& ctx, RegisterFile& reg_file,
                n++;
                continue;
             }
-            /* we cannot split live ranges of linear vgprs inside control flow */
-            if (!(ctx.block->kind & block_kind_top_level) &&
-                ctx.assignments[reg_file[j]].rc.is_linear_vgpr()) {
+            /* we cannot split live ranges of linear vgprs */
+            if (ctx.assignments[reg_file[j]].rc.is_linear_vgpr()) {
                found = false;
                break;
             }
@@ -1363,10 +1362,8 @@ get_reg_impl(ra_ctx& ctx, const RegisterFile& reg_file,
             break;
          }
 
-         /* we cannot split live ranges of linear vgprs inside control flow */
-         // TODO: ensure that live range splits inside control flow are never necessary
-         if (!(ctx.block->kind & block_kind_top_level) &&
-             ctx.assignments[reg_file[j]].rc.is_linear_vgpr()) {
+         /* we cannot split live ranges of linear vgprs */
+         if (ctx.assignments[reg_file[j]].rc.is_linear_vgpr()) {
             found = false;
             break;
          }
@@ -1967,13 +1964,9 @@ get_reg_create_vector(ra_ctx& ctx, RegisterFile& reg_file, Temp temp,
          avoid |= ctx.war_hint[j];
       }
 
-      if (linear_vgpr) {
-         /* we cannot split live ranges of linear vgprs inside control flow */
-         if (ctx.block->kind & block_kind_top_level)
-            avoid = true;
-         else
-            continue;
-      }
+      /* we cannot split live ranges of linear vgprs */
+      if (linear_vgpr)
+         continue;
 
       if (avoid && !best_avoid)
          continue;
-- 
GitLab


From ef1e0d81988c9bce07fad5e1ef23806f719057b2 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Mon, 19 Feb 2024 16:01:48 +0000
Subject: [PATCH 12/12] aco/tests: add tests for linear VGPR register
 allocation

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/tests/test_regalloc.cpp | 314 +++++++++++++++++++++++
 1 file changed, 314 insertions(+)

diff --git a/src/amd/compiler/tests/test_regalloc.cpp b/src/amd/compiler/tests/test_regalloc.cpp
index 27558940177f4..2778ca026939a 100644
--- a/src/amd/compiler/tests/test_regalloc.cpp
+++ b/src/amd/compiler/tests/test_regalloc.cpp
@@ -328,3 +328,317 @@ BEGIN_TEST(regalloc.writelane)
 
    finish_ra_test(ra_test_policy());
 END_TEST
+
+static void
+end_linear_vgpr(Temp tmp)
+{
+   Operand op(tmp);
+   op.setLateKill(true);
+   bld.pseudo(aco_opcode::p_end_linear_vgpr, op);
+}
+
+BEGIN_TEST(regalloc.linear_vgpr.alloc.basic)
+   if (!setup_cs("", GFX8))
+      return;
+
+   //>> lv1: %ltmp0:v[31] = p_start_linear_vgpr
+   //! lv1: %ltmp1:v[30] = p_start_linear_vgpr
+   //! p_end_linear_vgpr (latekill)%ltmp0:v[31]
+   //! lv1: %ltmp2:v[31] = p_start_linear_vgpr
+   //! p_end_linear_vgpr (latekill)%ltmp1:v[30]
+   //! p_end_linear_vgpr (latekill)%ltmp2:v[31]
+   Temp ltmp0 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v1.as_linear()));
+   Temp ltmp1 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v1.as_linear()));
+   end_linear_vgpr(ltmp0);
+   Temp ltmp2 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v1.as_linear()));
+   end_linear_vgpr(ltmp1);
+   end_linear_vgpr(ltmp2);
+
+   finish_ra_test(ra_test_policy());
+END_TEST
+
+BEGIN_TEST(regalloc.linear_vgpr.alloc.compact_grow)
+   for (bool pessimistic : {false, true}) {
+      const char* subvariant = pessimistic ? "_pessimistic" : "_optimistic";
+      //>> v1: %in0:v[0] = p_startpgm
+      if (!setup_cs("v1", GFX8, CHIP_UNKNOWN, subvariant))
+         continue;
+
+      //! lv1: %ltmp0:v[31] = p_start_linear_vgpr
+      //! lv1: %ltmp1:v[30] = p_start_linear_vgpr
+      //! p_end_linear_vgpr (latekill)%ltmp0:v[31]
+      Temp ltmp0 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v1.as_linear()));
+      Temp ltmp1 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v1.as_linear()));
+      end_linear_vgpr(ltmp0);
+
+      //! v1: %tmp:v[29] = p_parallelcopy %in0:v[0]
+      Temp tmp = bld.pseudo(aco_opcode::p_parallelcopy, bld.def(v1, PhysReg(256 + 29)), inputs[0]);
+
+      /* When there's not enough space in the linear VGPR area for a new one, the area is compacted
+       * and the beginning is chosen. Any variables which are in the way, are moved.
+       */
+      //! lv1: %ltmp1_2:v[31], v1: %tmp_2:v[#_] = p_parallelcopy %ltmp1:v[30], %tmp:v[29]
+      //! lv2: %ltmp2:v[29-30] = p_start_linear_vgpr
+      Temp ltmp2 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v2.as_linear()));
+
+      //! p_end_linear_vgpr (latekill)%ltmp1_2:v[31]
+      //! p_end_linear_vgpr (latekill)%ltmp2:v[29-30]
+      end_linear_vgpr(ltmp1);
+      end_linear_vgpr(ltmp2);
+
+      //! p_unit_test %tmp_2:v[#_]
+      bld.pseudo(aco_opcode::p_unit_test, tmp);
+
+      finish_ra_test(ra_test_policy{pessimistic});
+   }
+END_TEST
+
+BEGIN_TEST(regalloc.linear_vgpr.alloc.compact_shrink)
+   for (bool pessimistic : {false, true}) {
+      const char* subvariant = pessimistic ? "_pessimistic" : "_optimistic";
+      //>> v1: %in0:v[0] = p_startpgm
+      if (!setup_cs("v1", GFX8, CHIP_UNKNOWN, subvariant))
+         continue;
+
+      //! lv1: %ltmp0:v[31] = p_start_linear_vgpr
+      //! lv1: %ltmp1:v[30] = p_start_linear_vgpr
+      //! lv1: %ltmp2:v[29] = p_start_linear_vgpr
+      //! lv1: %ltmp3:v[28] = p_start_linear_vgpr
+      //! lv1: %ltmp4:v[27] = p_start_linear_vgpr
+      //! p_end_linear_vgpr (latekill)%ltmp0:v[31]
+      //! p_end_linear_vgpr (latekill)%ltmp2:v[29]
+      //! p_end_linear_vgpr (latekill)%ltmp4:v[27]
+      Temp ltmp0 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v1.as_linear()));
+      Temp ltmp1 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v1.as_linear()));
+      Temp ltmp2 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v1.as_linear()));
+      Temp ltmp3 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v1.as_linear()));
+      Temp ltmp4 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v1.as_linear()));
+      end_linear_vgpr(ltmp0);
+      end_linear_vgpr(ltmp2);
+      end_linear_vgpr(ltmp4);
+
+      /* Unlike regalloc.linear_vgpr.alloc.compact_grow, this shrinks the linear VGPR area. */
+      //! lv1: %ltmp3_2:v[30], lv1: %ltmp1_2:v[31] = p_parallelcopy %ltmp3:v[28], %ltmp1:v[30]
+      //! lv2: %ltmp5:v[28-29] = p_start_linear_vgpr
+      Temp ltmp5 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v2.as_linear()));
+
+      /* There should be enough space for 28 normal VGPRs. */
+      //! v28: %_:v[0-27] = p_unit_test
+      bld.pseudo(aco_opcode::p_unit_test, bld.def(RegClass::get(RegType::vgpr, 28 * 4)));
+
+      //! p_end_linear_vgpr (latekill)%ltmp1_2:v[31]
+      //! p_end_linear_vgpr (latekill)%ltmp3_2:v[30]
+      //! p_end_linear_vgpr (latekill)%ltmp5:v[28-29]
+      end_linear_vgpr(ltmp1);
+      end_linear_vgpr(ltmp3);
+      end_linear_vgpr(ltmp5);
+
+      finish_ra_test(ra_test_policy{pessimistic});
+   }
+END_TEST
+
+BEGIN_TEST(regalloc.linear_vgpr.alloc.compact_for_normal)
+   for (bool pessimistic : {false, true}) {
+      const char* subvariant = pessimistic ? "_pessimistic" : "_optimistic";
+      //>> v1: %in0:v[0] = p_startpgm
+      if (!setup_cs("v1", GFX8, CHIP_UNKNOWN, subvariant))
+         continue;
+
+      //! lv1: %ltmp0:v[31] = p_start_linear_vgpr
+      //! lv1: %ltmp1:v[30] = p_start_linear_vgpr
+      //! p_end_linear_vgpr (latekill)%ltmp0:v[31]
+      Temp ltmp0 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v1.as_linear()));
+      Temp ltmp1 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v1.as_linear()));
+      end_linear_vgpr(ltmp0);
+
+      //! lv1: %ltmp1_2:v[31] = p_parallelcopy %ltmp1:v[30]
+      //! v31: %_:v[0-30] = p_unit_test
+      bld.pseudo(aco_opcode::p_unit_test, bld.def(RegClass::get(RegType::vgpr, 31 * 4)));
+
+      //! p_end_linear_vgpr (latekill)%ltmp1_2:v[31]
+      end_linear_vgpr(ltmp1);
+
+      finish_ra_test(ra_test_policy{pessimistic});
+   }
+END_TEST
+
+BEGIN_TEST(regalloc.linear_vgpr.alloc.compact_for_vec)
+   for (bool pessimistic : {false, true}) {
+      const char* subvariant = pessimistic ? "_pessimistic" : "_optimistic";
+      //>> v1: %in0:v[0] = p_startpgm
+      if (!setup_cs("v1", GFX8, CHIP_UNKNOWN, subvariant))
+         continue;
+
+      //! lv1: %ltmp0:v[31] = p_start_linear_vgpr
+      //! lv1: %ltmp1:v[30] = p_start_linear_vgpr
+      //! p_end_linear_vgpr (latekill)%ltmp0:v[31]
+      Temp ltmp0 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v1.as_linear()));
+      Temp ltmp1 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v1.as_linear()));
+      end_linear_vgpr(ltmp0);
+
+      //! lv1: %ltmp1_2:v[31] = p_parallelcopy %ltmp1:v[30]
+      //! v31: %_:v[0-30] = p_create_vector v31: undef
+      RegClass v31 = RegClass::get(RegType::vgpr, 31 * 4);
+      bld.pseudo(aco_opcode::p_create_vector, bld.def(v31), Operand(v31));
+
+      //! p_end_linear_vgpr (latekill)%ltmp1_2:v[31]
+      end_linear_vgpr(ltmp1);
+
+      finish_ra_test(ra_test_policy{pessimistic});
+   }
+END_TEST
+
+BEGIN_TEST(regalloc.linear_vgpr.alloc.killed_op)
+   for (bool pessimistic : {false, true}) {
+      const char* subvariant = pessimistic ? "_pessimistic" : "_optimistic";
+      if (!setup_cs("", GFX8, CHIP_UNKNOWN, subvariant))
+         continue;
+
+      //>> v31: %tmp0:v[0-30] = p_unit_test
+      //! v1: %tmp1:v[31] = p_unit_test
+      Temp tmp0 =
+         bld.pseudo(aco_opcode::p_unit_test, bld.def(RegClass::get(RegType::vgpr, 31 * 4)));
+      Temp tmp1 = bld.pseudo(aco_opcode::p_unit_test, bld.def(v1));
+
+      //! lv1: %ltmp0:v[31] = p_start_linear_vgpr %tmp1:v[31]
+      //! p_end_linear_vgpr (latekill)%ltmp0:v[31]
+      Temp ltmp0 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v1.as_linear()), tmp1);
+      end_linear_vgpr(ltmp0);
+
+      bld.pseudo(aco_opcode::p_unit_test, tmp0);
+
+      finish_ra_test(ra_test_policy{pessimistic});
+   }
+END_TEST
+
+BEGIN_TEST(regalloc.linear_vgpr.alloc.move_killed_op)
+   for (bool pessimistic : {false, true}) {
+      const char* subvariant = pessimistic ? "_pessimistic" : "_optimistic";
+      if (!setup_cs("", GFX8, CHIP_UNKNOWN, subvariant))
+         continue;
+
+      //>> v30: %tmp0:v[0-29] = p_unit_test
+      //! v1: %tmp1:v[30] = p_unit_test
+      //! v1: %tmp2:v[31] = p_unit_test
+      Temp tmp0 =
+         bld.pseudo(aco_opcode::p_unit_test, bld.def(RegClass::get(RegType::vgpr, 30 * 4)));
+      Temp tmp1 = bld.pseudo(aco_opcode::p_unit_test, bld.def(v1));
+      Temp tmp2 = bld.pseudo(aco_opcode::p_unit_test, bld.def(v1));
+
+      //~gfx8_optimistic! v1: %tmp1_2:v[31], v1: %tmp2_2:v[30] = p_parallelcopy %tmp1:v[30], %tmp2:v[31]
+      //~gfx8_pessimistic! v1: %tmp2_2:v[30], v1: %tmp1_2:v[31] = p_parallelcopy %tmp2:v[31], %tmp1:v[30]
+      //! lv1: %ltmp0:v[31] = p_start_linear_vgpr %tmp1_2:v[31]
+      //! p_end_linear_vgpr (latekill)%ltmp0:v[31]
+      Temp ltmp0 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v1.as_linear()), tmp1);
+      end_linear_vgpr(ltmp0);
+
+      //! p_unit_test %tmp0:v[0-29], %tmp2_2:v[30]
+      bld.pseudo(aco_opcode::p_unit_test, tmp0, tmp2);
+
+      finish_ra_test(ra_test_policy{pessimistic});
+   }
+END_TEST
+
+BEGIN_TEST(regalloc.linear_vgpr.compact_for_future_def)
+   for (bool cbr : {false, true}) {
+      const char* subvariant = cbr ? "_cbranch" : "_branch";
+      if (!setup_cs("", GFX8, CHIP_UNKNOWN, subvariant))
+         continue;
+
+      //>> lv2: %ltmp0:v[30-31] = p_start_linear_vgpr
+      //! lv1: %ltmp1:v[29] = p_start_linear_vgpr
+      //! lv1: %ltmp2:v[28] = p_start_linear_vgpr
+      //! p_end_linear_vgpr (latekill)%ltmp1:v[29]
+      Temp ltmp0 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v2.as_linear()));
+      Temp ltmp1 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v1.as_linear()));
+      Temp ltmp2 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v1.as_linear()));
+      end_linear_vgpr(ltmp1);
+
+      //! lv1: %ltmp2_2:v[29] = p_parallelcopy %ltmp2:v[28]
+      //~gfx8_cbranch! s2: %_:s[0-1] = p_cbranch_z %_:scc
+      //~gfx8_branch! s2: %_:s[0-1] = p_branch
+      if (cbr)
+         bld.branch(aco_opcode::p_cbranch_z, bld.def(s2), Operand(scc, s1));
+      else
+         bld.branch(aco_opcode::p_branch, bld.def(s2));
+
+      //! BB1
+      //! /* logical preds: BB0, / linear preds: BB0, / kind: */
+      bld.reset(program->create_and_insert_block());
+      program->blocks[1].linear_preds.push_back(0);
+      program->blocks[1].logical_preds.push_back(0);
+
+      //! v29: %_:v[0-28] = p_unit_test
+      //! s2: %_:s[0-1] = p_branch
+      bld.pseudo(aco_opcode::p_unit_test, bld.def(RegClass::get(RegType::vgpr, 29 * 4)));
+      bld.branch(aco_opcode::p_branch, bld.def(s2));
+
+      //! BB2
+      //! /* logical preds: BB1, / linear preds: BB1, / kind: uniform, top-level, */
+      bld.reset(program->create_and_insert_block());
+      program->blocks[2].linear_preds.push_back(1);
+      program->blocks[2].logical_preds.push_back(1);
+      program->blocks[2].kind |= block_kind_top_level;
+
+      //! p_end_linear_vgpr (latekill)%ltmp0_2:v[30-31]
+      //! p_end_linear_vgpr (latekill)%ltmp2_2:v[29]
+      end_linear_vgpr(ltmp0);
+      end_linear_vgpr(ltmp2);
+
+      finish_ra_test(ra_test_policy());
+   }
+END_TEST
+
+BEGIN_TEST(regalloc.linear_vgpr.compact_for_future_phis)
+   for (bool cbr : {false, true}) {
+      const char* subvariant = cbr ? "_cbranch" : "_branch";
+      if (!setup_cs("", GFX8, CHIP_UNKNOWN, subvariant))
+         continue;
+
+      //>> lv1: %ltmp0:v[31] = p_start_linear_vgpr
+      //! lv1: %ltmp1:v[30] = p_start_linear_vgpr
+      //! lv1: %ltmp2:v[29] = p_start_linear_vgpr
+      //! p_end_linear_vgpr (latekill)%ltmp1:v[30]
+      Temp ltmp0 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v1.as_linear()));
+      Temp ltmp1 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v1.as_linear()));
+      Temp ltmp2 = bld.pseudo(aco_opcode::p_start_linear_vgpr, bld.def(v1.as_linear()));
+      end_linear_vgpr(ltmp1);
+
+      //! lv1: %ltmp2_2:v[30] = p_parallelcopy %ltmp2:v[29]
+      //~gfx8_cbranch! s2: %_:s[0-1] = p_cbranch_z %_:scc
+      //~gfx8_branch! s2: %_:s[0-1] = p_branch
+      if (cbr)
+         bld.branch(aco_opcode::p_cbranch_z, bld.def(s2), Operand(scc, s1));
+      else
+         bld.branch(aco_opcode::p_branch, bld.def(s2));
+
+      //! BB1
+      //! /* logical preds: BB0, / linear preds: BB0, / kind: */
+      bld.reset(program->create_and_insert_block());
+      program->blocks[1].linear_preds.push_back(0);
+      program->blocks[1].logical_preds.push_back(0);
+
+      //! s2: %_:s[0-1] = p_branch
+      bld.branch(aco_opcode::p_branch, bld.def(s2));
+
+      //! BB2
+      //! /* logical preds: BB1, / linear preds: BB1, / kind: uniform, top-level, */
+      bld.reset(program->create_and_insert_block());
+      program->blocks[2].linear_preds.push_back(1);
+      program->blocks[2].logical_preds.push_back(1);
+      program->blocks[2].kind |= block_kind_top_level;
+
+      RegClass v30 = RegClass::get(RegType::vgpr, 30 * 4);
+      //! v30: %tmp:v[0-29] = p_phi v30: undef
+      //! p_unit_test %tmp:v[0-29]
+      Temp tmp = bld.pseudo(aco_opcode::p_phi, bld.def(v30), Operand(v30));
+      bld.pseudo(aco_opcode::p_unit_test, tmp);
+
+      //! p_end_linear_vgpr (latekill)%ltmp0_2:v[31]
+      //! p_end_linear_vgpr (latekill)%ltmp2_2:v[30]
+      end_linear_vgpr(ltmp0);
+      end_linear_vgpr(ltmp2);
+
+      finish_ra_test(ra_test_policy());
+   }
+END_TEST
-- 
GitLab

