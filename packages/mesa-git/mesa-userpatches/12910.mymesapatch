From a8b50c8835f4bca93f1a379db9273a3e4691da95 Mon Sep 17 00:00:00 2001
From: Connor Abbott <cwabbott0@gmail.com>
Date: Tue, 19 Mar 2019 14:16:41 +0100
Subject: [PATCH 1/7] nir: Free instructions more often

Soon we'll be allocating instructions out of a per-shader pool, which
means that if we don't free too many instructions during the main
optimization loop, the final nir_sweep() call will create holes which
can't be filled. By freeing instructions more aggressively, we can
allocate more instructions from the freelist which will reduce the final
memory usage.

Modified from Connor Abbott's original patch to rebase on top of
refactored DCE and so that the use-after-free in nir_algebraic_impl() is
fixed.

Co-authored-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/compiler/nir/nir_opt_dce.c | 24 ++++++++++++++++--------
 src/compiler/nir/nir_search.c  | 20 +++++++++++++++-----
 src/compiler/nir/nir_search.h  |  3 ++-
 3 files changed, 33 insertions(+), 14 deletions(-)

diff --git a/src/compiler/nir/nir_opt_dce.c b/src/compiler/nir/nir_opt_dce.c
index 49e7ac015c27..1f4b0b33fb4e 100644
--- a/src/compiler/nir/nir_opt_dce.c
+++ b/src/compiler/nir/nir_opt_dce.c
@@ -107,7 +107,8 @@ struct loop_state {
 };
 
 static bool
-dce_block(nir_block *block, BITSET_WORD *defs_live, struct loop_state *loop)
+dce_block(nir_block *block, BITSET_WORD *defs_live, struct loop_state *loop,
+          struct exec_list *dead_instrs)
 {
    bool progress = false;
    bool phis_changed = false;
@@ -131,6 +132,7 @@ dce_block(nir_block *block, BITSET_WORD *defs_live, struct loop_state *loop)
          instr->pass_flags = live;
       } else if (!live) {
          nir_instr_remove(instr);
+         exec_list_push_tail(dead_instrs, &instr->node);
          progress = true;
       }
    }
@@ -146,20 +148,20 @@ dce_block(nir_block *block, BITSET_WORD *defs_live, struct loop_state *loop)
 
 static bool
 dce_cf_list(struct exec_list *cf_list, BITSET_WORD *defs_live,
-            struct loop_state *parent_loop)
+            struct loop_state *parent_loop, struct exec_list *dead_instrs)
 {
    bool progress = false;
    foreach_list_typed_reverse(nir_cf_node, cf_node, node, cf_list) {
       switch (cf_node->type) {
       case nir_cf_node_block: {
          nir_block *block = nir_cf_node_as_block(cf_node);
-         progress |= dce_block(block, defs_live, parent_loop);
+         progress |= dce_block(block, defs_live, parent_loop, dead_instrs);
          break;
       }
       case nir_cf_node_if: {
          nir_if *nif = nir_cf_node_as_if(cf_node);
-         progress |= dce_cf_list(&nif->else_list, defs_live, parent_loop);
-         progress |= dce_cf_list(&nif->then_list, defs_live, parent_loop);
+         progress |= dce_cf_list(&nif->else_list, defs_live, parent_loop, dead_instrs);
+         progress |= dce_cf_list(&nif->then_list, defs_live, parent_loop, dead_instrs);
          mark_src_live(&nif->condition, defs_live);
          break;
       }
@@ -176,7 +178,7 @@ dce_cf_list(struct exec_list *cf_list, BITSET_WORD *defs_live,
          struct set *predecessors = nir_loop_first_block(loop)->predecessors;
          if (predecessors->entries == 1 &&
              _mesa_set_next_entry(predecessors, NULL)->key == inner_state.preheader) {
-            progress |= dce_cf_list(&loop->body, defs_live, parent_loop);
+            progress |= dce_cf_list(&loop->body, defs_live, parent_loop, dead_instrs);
             break;
          }
 
@@ -185,7 +187,7 @@ dce_cf_list(struct exec_list *cf_list, BITSET_WORD *defs_live,
             /* dce_cf_list() resets inner_state.header_phis_changed itself, so
              * it doesn't have to be done here.
              */
-            dce_cf_list(&loop->body, defs_live, &inner_state);
+            dce_cf_list(&loop->body, defs_live, &inner_state, dead_instrs);
          } while (inner_state.header_phis_changed);
 
          /* We don't know how many times mark_cf_list() will repeat, so
@@ -199,6 +201,7 @@ dce_cf_list(struct exec_list *cf_list, BITSET_WORD *defs_live,
                nir_foreach_instr_safe(instr, block) {
                   if (!instr->pass_flags) {
                      nir_instr_remove(instr);
+                     exec_list_push_tail(dead_instrs, &instr->node);
                      progress = true;
                   }
                }
@@ -222,12 +225,17 @@ nir_opt_dce_impl(nir_function_impl *impl)
    BITSET_WORD *defs_live = rzalloc_array(NULL, BITSET_WORD,
                                           BITSET_WORDS(impl->ssa_alloc));
 
+   struct exec_list dead_instrs;
+   exec_list_make_empty(&dead_instrs);
+
    struct loop_state loop;
    loop.preheader = NULL;
-   bool progress = dce_cf_list(&impl->body, defs_live, &loop);
+   bool progress = dce_cf_list(&impl->body, defs_live, &loop, &dead_instrs);
 
    ralloc_free(defs_live);
 
+   nir_instr_free_list(&dead_instrs);
+
    if (progress) {
       nir_metadata_preserve(impl, nir_metadata_block_index |
                                   nir_metadata_dominance);
diff --git a/src/compiler/nir/nir_search.c b/src/compiler/nir/nir_search.c
index 4fd7ad661430..1ee3ce4ddcc4 100644
--- a/src/compiler/nir/nir_search.c
+++ b/src/compiler/nir/nir_search.c
@@ -692,7 +692,8 @@ nir_replace_instr(nir_builder *build, nir_alu_instr *instr,
                   const nir_algebraic_table *table,
                   const nir_search_expression *search,
                   const nir_search_value *replace,
-                  nir_instr_worklist *algebraic_worklist)
+                  nir_instr_worklist *algebraic_worklist,
+                  struct exec_list *dead_instrs)
 {
    uint8_t swizzle[NIR_MAX_VEC_COMPONENTS] = { 0 };
 
@@ -803,7 +804,9 @@ nir_replace_instr(nir_builder *build, nir_alu_instr *instr,
     * that the instr may be in the worklist still, so we can't free it
     * directly.
     */
+   instr->instr.pass_flags = 1;
    nir_instr_remove(&instr->instr);
+   exec_list_push_tail(dead_instrs, &instr->instr.node);
 
    return ssa_val;
 }
@@ -865,7 +868,8 @@ nir_algebraic_instr(nir_builder *build, nir_instr *instr,
                     const bool *condition_flags,
                     const nir_algebraic_table *table,
                     struct util_dynarray *states,
-                    nir_instr_worklist *worklist)
+                    nir_instr_worklist *worklist,
+                    struct exec_list *dead_instrs)
 {
 
    if (instr->type != nir_instr_type_alu)
@@ -891,7 +895,7 @@ nir_algebraic_instr(nir_builder *build, nir_instr *instr,
           !(table->values[xform->search].expression.inexact && ignore_inexact) &&
           nir_replace_instr(build, alu, range_ht, states, table,
                             &table->values[xform->search].expression,
-                            &table->values[xform->replace].value, worklist)) {
+                            &table->values[xform->replace].value, worklist, dead_instrs)) {
          _mesa_hash_table_clear(range_ht, NULL);
          return true;
       }
@@ -938,25 +942,31 @@ nir_algebraic_impl(nir_function_impl *impl,
     */
    nir_foreach_block_reverse(block, impl) {
       nir_foreach_instr_reverse(instr, block) {
+         instr->pass_flags = 0;
          if (instr->type == nir_instr_type_alu)
             nir_instr_worklist_push_tail(worklist, instr);
       }
    }
 
+   struct exec_list dead_instrs;
+   exec_list_make_empty(&dead_instrs);
+
    nir_instr *instr;
    while ((instr = nir_instr_worklist_pop_head(worklist))) {
       /* The worklist can have an instr pushed to it multiple times if it was
        * the src of multiple instrs that also got optimized, so make sure that
        * we don't try to re-optimize an instr we already handled.
        */
-      if (exec_node_is_tail_sentinel(&instr->node))
+      if (instr->pass_flags)
          continue;
 
       progress |= nir_algebraic_instr(&build, instr,
                                       range_ht, condition_flags,
-                                      table, &states, worklist);
+                                      table, &states, worklist, &dead_instrs);
    }
 
+   nir_instr_free_list(&dead_instrs);
+
    nir_instr_worklist_destroy(worklist);
    ralloc_free(range_ht);
    util_dynarray_fini(&states);
diff --git a/src/compiler/nir/nir_search.h b/src/compiler/nir/nir_search.h
index ba113f15ffb0..4e3a8ef9c5d4 100644
--- a/src/compiler/nir/nir_search.h
+++ b/src/compiler/nir/nir_search.h
@@ -244,7 +244,8 @@ nir_replace_instr(struct nir_builder *b, nir_alu_instr *instr,
                   const nir_algebraic_table *table,
                   const nir_search_expression *search,
                   const nir_search_value *replace,
-                  nir_instr_worklist *algebraic_worklist);
+                  nir_instr_worklist *algebraic_worklist,
+                  struct exec_list *dead_instrs);
 bool
 nir_algebraic_impl(nir_function_impl *impl,
                    const bool *condition_flags,
-- 
GitLab


From 31c28e161cbb0f17abd0bceaaee5937acb8d2408 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Wed, 8 Sep 2021 16:13:01 +0100
Subject: [PATCH 2/7] util/ralloc: add HEADER_ALIGN macro

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/util/ralloc.c | 19 ++++++++-----------
 1 file changed, 8 insertions(+), 11 deletions(-)

diff --git a/src/util/ralloc.c b/src/util/ralloc.c
index 357406556136..0d57a6045bd6 100644
--- a/src/util/ralloc.c
+++ b/src/util/ralloc.c
@@ -36,6 +36,12 @@
 
 #define CANARY 0x5A1106
 
+#if defined(__LP64__) || defined(_WIN64)
+#define HEADER_ALIGN alignas(16)
+#else
+#define HEADER_ALIGN alignas(8)
+#endif
+
 /* Align the header's size so that ralloc() allocations will return with the
  * same alignment as a libc malloc would have (8 on 32-bit GLIBC, 16 on
  * 64-bit), avoiding performance penalities on x86 and alignment faults on
@@ -43,11 +49,7 @@
  */
 struct ralloc_header
 {
-#if defined(__LP64__) || defined(_WIN64)
-   alignas(16)
-#else
-   alignas(8)
-#endif
+   HEADER_ALIGN
 
 #ifndef NDEBUG
    /* A canary value used to determine whether a pointer is ralloc'd. */
@@ -550,12 +552,7 @@ ralloc_vasprintf_rewrite_tail(char **str, size_t *start, const char *fmt,
 
 struct linear_header {
 
-   /* align first member to align struct */
-#if defined(__LP64__) || defined(_WIN64)
-   alignas(16)
-#else
-   alignas(8)
-#endif
+   HEADER_ALIGN
 
 #ifndef NDEBUG
    unsigned magic;   /* for debugging */
-- 
GitLab


From cd16dcdb0e89fbf74dfa0e56d5ecc525f1359451 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Wed, 8 Sep 2021 15:21:57 +0100
Subject: [PATCH 3/7] util: add freelist allocator with mark/sweep

Based on the allocator https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/524,
but modified a bit (in particular, it's now separate from ralloc).

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/util/ralloc.c | 334 ++++++++++++++++++++++++++++++++++++++++++++++
 src/util/ralloc.h |  29 ++++
 2 files changed, 363 insertions(+)

diff --git a/src/util/ralloc.c b/src/util/ralloc.c
index 0d57a6045bd6..5b7f043dab33 100644
--- a/src/util/ralloc.c
+++ b/src/util/ralloc.c
@@ -28,6 +28,7 @@
 #include <stdlib.h>
 #include <string.h>
 
+#include "util/list.h"
 #include "util/macros.h"
 #include "util/u_math.h"
 #include "util/u_printf.h"
@@ -528,6 +529,339 @@ ralloc_vasprintf_rewrite_tail(char **str, size_t *start, const char *fmt,
    return true;
 }
 
+/***************************************************************************
+ * GC context.
+ ***************************************************************************
+ */
+
+/* The maximum size of an object that will be allocated specially.
+ */
+#define MAX_FREELIST_SIZE 512
+
+/* Allocations small enough to be allocated from a freelist will be aligned up
+ * to this size.
+ */
+#define FREELIST_ALIGNMENT 32
+
+#define NUM_FREELIST_BUCKETS (MAX_FREELIST_SIZE / FREELIST_ALIGNMENT)
+
+/* The size of a slab. */
+#define SLAB_SIZE (32 * 1024)
+
+#define GC_CANARY 0xAF6B5B72
+
+enum gc_flags {
+   IS_USED = (1 << 0),
+   CURRENT_GENERATION = (1 << 1),
+};
+
+typedef struct
+{
+#ifndef NDEBUG
+   /* A canary value used to determine whether a pointer is allocated using gc_alloc. */
+   unsigned canary;
+#endif
+
+   unsigned short slab_offset;
+   unsigned char bucket;
+   unsigned char flags;
+} gc_block_header;
+
+/* This structure is at the start of the slab. Objects inside a slab are
+ * allocated using a freelist backed by a simple linear allocator.
+ */
+typedef struct gc_slab {
+   gc_ctx *ctx;
+
+   /* Objects are allocated using either linear or freelist allocation. "next_available" is the
+    * pointer used for linear allocation, while "freelist" is the next free object for freelist
+    allocation.
+    */
+   char *next_available;
+   gc_block_header *freelist;
+
+   /* Slabs that handle the same-sized objects. */
+   struct list_head link;
+
+   /* Free slabs that handle the same-sized objects. */
+   struct list_head free_link;
+
+   /* Number of allocated and free objects, recorded so that we can free the slab if it
+    * becomes empty or add one to the freelist if it's no longer full.
+    */
+   unsigned num_allocated;
+   unsigned num_free;
+
+   char data[];
+} gc_slab;
+
+struct gc_ctx {
+   /* Array of slabs for fixed-size allocations. Each slab tracks allocations
+    * of specific sized blocks. User allocations are rounded up to the nearest
+    * fixed size. slabs[N] contains allocations of size
+    * FREELIST_ALIGNMENT * (N + 1).
+    */
+   struct {
+      /* List of slabs in this bucket. */
+      struct list_head slabs;
+
+      /* List of slabs with free space in this bucket, so we can quickly choose one when
+       * allocating.
+       */
+      struct list_head free_slabs;
+   } slabs[NUM_FREELIST_BUCKETS];
+
+   unsigned char current_gen;
+   void *rubbish;
+};
+
+static gc_block_header *
+get_gc_header(const void *ptr)
+{
+   gc_block_header *info = (gc_block_header *) (((char *) ptr) -
+					    sizeof(gc_block_header));
+   assert(info->canary == GC_CANARY);
+   return info;
+}
+
+static gc_block_header **
+get_gc_freelist_next(gc_block_header *ptr)
+{
+   return (gc_block_header **)(ptr+1);
+}
+
+gc_ctx *
+gc_context(const void *parent)
+{
+   gc_ctx *ctx = rzalloc(parent, gc_ctx);
+   for (unsigned i = 0; i < NUM_FREELIST_BUCKETS; i++) {
+      list_inithead(&ctx->slabs[i].slabs);
+      list_inithead(&ctx->slabs[i].free_slabs);
+   }
+   return ctx;
+}
+
+static gc_block_header *
+alloc_from_slab(gc_slab *slab, unsigned bucket)
+{
+   size_t size = (bucket + 1) * FREELIST_ALIGNMENT;
+   gc_block_header *header;
+   if (slab->freelist) {
+      /* Prioritize already-allocated chunks, since they probably have a page
+       * backing them.
+       */
+      header = slab->freelist;
+      slab->freelist = *get_gc_freelist_next(slab->freelist);
+   } else if (slab->next_available + size <= ((char *) slab) + SLAB_SIZE) {
+      header = (gc_block_header *) slab->next_available;
+      header->slab_offset = (char *) header - (char *) slab;
+      header->bucket = bucket;
+      slab->next_available += size;
+   } else {
+      return NULL;
+   }
+
+   slab->num_allocated++;
+   slab->num_free--;
+   if (!slab->num_free)
+      list_del(&slab->free_link);
+   return header;
+}
+
+static void
+free_from_slab(gc_block_header *header)
+{
+   gc_slab *slab =
+      (gc_slab *)((char *) header - header->slab_offset);
+
+   if (slab->num_allocated == 1) {
+      /* Free the slab if this is the last object. */
+      if (list_is_linked(&slab->free_link))
+         list_del(&slab->free_link);
+      list_del(&slab->link);
+      ralloc_free(slab);
+      return;
+   } else if (slab->num_free == 0) {
+      list_add(&slab->free_link, &slab->ctx->slabs[header->bucket].free_slabs);
+   } else {
+      /* Keep the free list sorted by the number of free objects in ascending order. By prefering to
+       * allocate from the slab with the fewest free objects, we help free the slabs with many free
+       * objects.
+       */
+      while (slab->free_link.next != &slab->ctx->slabs[header->bucket].free_slabs &&
+             slab->num_free > LIST_ENTRY(gc_slab, slab->free_link.next, free_link)->num_free) {
+         gc_slab *next = LIST_ENTRY(gc_slab, slab->free_link.next, free_link);
+
+         list_del(&slab->free_link);
+
+         /* Insert "slab" after "next". */
+         slab->free_link.prev = &next->free_link;
+         slab->free_link.next = next->free_link.next;
+         slab->free_link.prev->next = &slab->free_link;
+         slab->free_link.next->prev = &slab->free_link;
+      }
+   }
+
+   *get_gc_freelist_next(header) = slab->freelist;
+   slab->freelist = header;
+
+   slab->num_allocated--;
+   slab->num_free++;
+}
+
+static unsigned
+get_slab_size(unsigned bucket)
+{
+   unsigned obj_size = (bucket + 1) * FREELIST_ALIGNMENT;
+   unsigned num_objs = (SLAB_SIZE - sizeof(gc_slab)) / obj_size;
+   return align64(sizeof(gc_slab) + num_objs * obj_size, alignof(gc_slab));
+}
+
+static gc_slab *
+create_slab(gc_ctx *ctx, unsigned bucket)
+{
+   gc_slab *slab = ralloc_size(ctx, get_slab_size(bucket));
+   if (unlikely(!slab))
+      return NULL;
+
+   slab->ctx = ctx;
+   slab->freelist = NULL;
+   slab->next_available = slab->data;
+   slab->num_allocated = 0;
+   slab->num_free = (SLAB_SIZE - sizeof(gc_slab)) / ((bucket + 1) * FREELIST_ALIGNMENT);
+
+   list_addtail(&slab->link, &ctx->slabs[bucket].slabs);
+   list_addtail(&slab->free_link, &ctx->slabs[bucket].free_slabs);
+
+   return slab;
+}
+
+void *
+gc_alloc_size(gc_ctx *ctx, size_t size, size_t align)
+{
+   assert(ctx);
+
+   align = MAX2(align, alignof(gc_block_header));
+
+   size = align64(size, align);
+   size += align64(sizeof(gc_block_header), align);
+
+   gc_block_header *header = NULL;
+   if (size <= MAX_FREELIST_SIZE) {
+      unsigned bucket = (size - 1) / FREELIST_ALIGNMENT;
+      if (list_is_empty(&ctx->slabs[bucket].free_slabs) && !create_slab(ctx, bucket))
+         return NULL;
+      gc_slab *slab = list_first_entry(&ctx->slabs[bucket].free_slabs, gc_slab, free_link);
+      header = alloc_from_slab(slab, bucket);
+   } else {
+      header = ralloc_size(ctx, size);
+      if (unlikely(!header))
+         return NULL;
+      /* Mark the header as allocated directly, so we know to actually free it. */
+      header->bucket = NUM_FREELIST_BUCKETS;
+   }
+
+   header->flags = ctx->current_gen | IS_USED;
+#ifndef NDEBUG
+   header->canary = GC_CANARY;
+#endif
+
+   return (char *)header + sizeof(gc_block_header);
+}
+
+void *
+gc_zalloc_size(gc_ctx *ctx, size_t size, size_t align)
+{
+   void *ptr = gc_alloc_size(ctx, size, align);
+
+   if (likely(ptr))
+      memset(ptr, 0, size);
+
+   return ptr;
+}
+
+void
+gc_free(void *ptr)
+{
+   if (!ptr)
+      return;
+
+   gc_block_header *header = get_gc_header(ptr);
+   header->flags &= ~IS_USED;
+
+   if (header->bucket < NUM_FREELIST_BUCKETS)
+      free_from_slab(header);
+   else
+      ralloc_free(header);
+}
+
+gc_ctx *gc_get_context(void *ptr)
+{
+   gc_block_header *header = get_gc_header(ptr);
+
+   if (header->bucket < NUM_FREELIST_BUCKETS) {
+      gc_slab *slab =
+         (gc_slab *)((char *) header - header->slab_offset);
+      return slab->ctx;
+   } else {
+      return ralloc_parent(header);
+   }
+}
+
+void
+gc_sweep_start(gc_ctx *ctx)
+{
+   ctx->current_gen ^= CURRENT_GENERATION;
+
+   ctx->rubbish = ralloc_context(NULL);
+   ralloc_adopt(ctx->rubbish, ctx);
+}
+
+void
+gc_mark_live(gc_ctx *ctx, const void *mem)
+{
+   gc_block_header *header = get_gc_header(mem);
+   if (header->bucket >= NUM_FREELIST_BUCKETS) {
+      ralloc_steal(ctx, header);
+   } else {
+      header->flags ^= CURRENT_GENERATION;
+   }
+}
+
+void
+gc_sweep_end(gc_ctx *ctx)
+{
+   assert(ctx->rubbish);
+
+   for (unsigned i = 0; i < NUM_FREELIST_BUCKETS; i++) {
+      unsigned obj_size = (i + 1) * FREELIST_ALIGNMENT;
+      list_for_each_entry_safe(gc_slab, slab, &ctx->slabs[i].slabs, link) {
+         for (char *ptr = slab->data; ptr != slab->next_available; ptr += obj_size) {
+            gc_block_header *header = (gc_block_header *)ptr;
+            if (!(header->flags & IS_USED))
+               continue;
+            if ((header->flags & CURRENT_GENERATION) == ctx->current_gen)
+               continue;
+
+            bool last = slab->num_allocated == 1;
+            gc_free(header + 1);
+            if (last)
+               break;
+         }
+      }
+   }
+
+   for (unsigned i = 0; i < NUM_FREELIST_BUCKETS; i++) {
+      list_for_each_entry(gc_slab, slab, &ctx->slabs[i].slabs, link) {
+         assert(slab->num_allocated > 0); /* gc_free() should free it otherwise */
+         ralloc_steal(ctx, slab);
+      }
+   }
+
+   ralloc_free(ctx->rubbish);
+   ctx->rubbish = NULL;
+}
+
 /***************************************************************************
  * Linear allocator for short-lived allocations.
  ***************************************************************************
diff --git a/src/util/ralloc.h b/src/util/ralloc.h
index 857ca5f797f0..0fdcb3c309c8 100644
--- a/src/util/ralloc.h
+++ b/src/util/ralloc.h
@@ -478,6 +478,35 @@ bool ralloc_asprintf_append (char **str, const char *fmt, ...)
 bool ralloc_vasprintf_append(char **str, const char *fmt, va_list args);
 /// @}
 
+typedef struct gc_ctx gc_ctx;
+
+/**
+ * Allocate a new garbage collection context. The children of the
+ * context are not necessarily ralloc'd pointers and cannot be stolen to a ralloc context. Instead,
+ * The user should use the mark-and-sweep interface below to free any unused children. Under the
+ * hood, this restriction lets us manage allocations ourselves, using a freelist. This means that
+ * GC contexts should be used for scenarios where there are many allocations and frees, most of
+ * which use only a few different sizes.
+ */
+gc_ctx *gc_context(const void *parent);
+
+#define gc_alloc(ctx, type, count) gc_alloc_size(ctx, sizeof(type) * (count), alignof(type))
+#define gc_zalloc(ctx, type, count) gc_zalloc_size(ctx, sizeof(type) * (count), alignof(type))
+
+#define gc_alloc_zla(ctx, type, type2, count) \
+   gc_alloc_size(ctx, sizeof(type) + sizeof(type2) * (count), MAX2(alignof(type), alignof(type2)))
+#define gc_zalloc_zla(ctx, type, type2, count) \
+   gc_zalloc_size(ctx, sizeof(type) + sizeof(type2) * (count), MAX2(alignof(type), alignof(type2)))
+
+void *gc_alloc_size(gc_ctx *ctx, size_t size, size_t align) MALLOCLIKE;
+void *gc_zalloc_size(gc_ctx *ctx, size_t size, size_t align) MALLOCLIKE;
+void gc_free(void *ptr);
+gc_ctx *gc_get_context(void *ptr);
+
+void gc_sweep_start(gc_ctx *ctx);
+void gc_mark_live(gc_ctx *ctx, const void *mem);
+void gc_sweep_end(gc_ctx *ctx);
+
 /**
  * Declare C++ new and delete operators which use ralloc.
  *
-- 
GitLab


From c528c6641b8d4e3ccd099f3181eebbb1b7186bad Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Wed, 8 Sep 2021 15:28:53 +0100
Subject: [PATCH 4/7] nir/serialize: remove unused parameter from read_src()

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/compiler/nir/nir_serialize.c | 26 +++++++++++++-------------
 1 file changed, 13 insertions(+), 13 deletions(-)

diff --git a/src/compiler/nir/nir_serialize.c b/src/compiler/nir/nir_serialize.c
index bd69225dd190..215a4160fc6c 100644
--- a/src/compiler/nir/nir_serialize.c
+++ b/src/compiler/nir/nir_serialize.c
@@ -541,7 +541,7 @@ write_src(write_ctx *ctx, const nir_src *src)
 }
 
 static union packed_src
-read_src(read_ctx *ctx, nir_src *src, void *mem_ctx)
+read_src(read_ctx *ctx, nir_src *src)
 {
    STATIC_ASSERT(sizeof(union packed_src) == 4);
    union packed_src header;
@@ -555,7 +555,7 @@ read_src(read_ctx *ctx, nir_src *src, void *mem_ctx)
       src->reg.base_offset = blob_read_uint32(ctx->blob);
       if (header.any.is_indirect) {
          src->reg.indirect = malloc(sizeof(nir_src));
-         read_src(ctx, src->reg.indirect, mem_ctx);
+         read_src(ctx, src->reg.indirect);
       } else {
          src->reg.indirect = NULL;
       }
@@ -776,7 +776,7 @@ read_dest(read_ctx *ctx, nir_dest *dst, nir_instr *instr,
       dst->reg.base_offset = blob_read_uint32(ctx->blob);
       if (dest.reg.is_indirect) {
          dst->reg.indirect = malloc(sizeof(nir_src));
-         read_src(ctx, dst->reg.indirect, instr);
+         read_src(ctx, dst->reg.indirect);
       }
    }
 }
@@ -932,7 +932,7 @@ read_alu(read_ctx *ctx, union packed_instr header)
       }
    } else {
       for (unsigned i = 0; i < num_srcs; i++) {
-         union packed_src src = read_src(ctx, &alu->src[i].src, &alu->instr);
+         union packed_src src = read_src(ctx, &alu->src[i].src);
          unsigned src_channels = nir_ssa_alu_instr_src_components(alu, i);
          unsigned src_components = nir_src_num_components(alu->src[i].src);
          bool packed = src_components <= 4 && src_channels <= 4;
@@ -1111,7 +1111,7 @@ read_deref(read_ctx *ctx, union packed_instr header)
       break;
 
    case nir_deref_type_struct:
-      read_src(ctx, &deref->parent, &deref->instr);
+      read_src(ctx, &deref->parent);
       parent = nir_src_as_deref(deref->parent);
       deref->strct.index = blob_read_uint32(ctx->blob);
       deref->type = glsl_get_struct_field(parent->type, deref->strct.index);
@@ -1125,8 +1125,8 @@ read_deref(read_ctx *ctx, union packed_instr header)
          deref->arr.index.is_ssa = true;
          deref->arr.index.ssa = read_lookup_object(ctx, blob_read_uint16(ctx->blob));
       } else {
-         read_src(ctx, &deref->parent, &deref->instr);
-         read_src(ctx, &deref->arr.index, &deref->instr);
+         read_src(ctx, &deref->parent);
+         read_src(ctx, &deref->arr.index);
       }
 
       deref->arr.in_bounds = header.deref.in_bounds;
@@ -1139,7 +1139,7 @@ read_deref(read_ctx *ctx, union packed_instr header)
       break;
 
    case nir_deref_type_cast:
-      read_src(ctx, &deref->parent, &deref->instr);
+      read_src(ctx, &deref->parent);
       deref->cast.ptr_stride = blob_read_uint32(ctx->blob);
       deref->cast.align_mul = blob_read_uint32(ctx->blob);
       deref->cast.align_offset = blob_read_uint32(ctx->blob);
@@ -1152,7 +1152,7 @@ read_deref(read_ctx *ctx, union packed_instr header)
       break;
 
    case nir_deref_type_array_wildcard:
-      read_src(ctx, &deref->parent, &deref->instr);
+      read_src(ctx, &deref->parent);
       parent = nir_src_as_deref(deref->parent);
       deref->type = glsl_get_array_element(parent->type);
       break;
@@ -1252,7 +1252,7 @@ read_intrinsic(read_ctx *ctx, union packed_instr header)
       read_dest(ctx, &intrin->dest, &intrin->instr, header);
 
    for (unsigned i = 0; i < num_srcs; i++)
-      read_src(ctx, &intrin->src[i], &intrin->instr);
+      read_src(ctx, &intrin->src[i]);
 
    /* Vectorized instrinsics have num_components same as dst or src that has
     * 0 components in the info. Find it.
@@ -1572,7 +1572,7 @@ read_tex(read_ctx *ctx, union packed_instr header)
    tex->array_is_lowered_cube = packed.u.array_is_lowered_cube;
 
    for (unsigned i = 0; i < tex->num_srcs; i++) {
-      union packed_src src = read_src(ctx, &tex->src[i].src, &tex->instr);
+      union packed_src src = read_src(ctx, &tex->src[i].src);
       tex->src[i].src_type = src.tex.src_type;
    }
 
@@ -1718,7 +1718,7 @@ read_call(read_ctx *ctx)
    nir_call_instr *call = nir_call_instr_create(ctx->nir, callee);
 
    for (unsigned i = 0; i < call->num_params; i++)
-      read_src(ctx, &call->params[i], call);
+      read_src(ctx, &call->params[i]);
 
    return call;
 }
@@ -1871,7 +1871,7 @@ read_if(read_ctx *ctx, struct exec_list *cf_list)
 {
    nir_if *nif = nir_if_create(ctx->nir);
 
-   read_src(ctx, &nif->condition, nif);
+   read_src(ctx, &nif->condition);
    nif->control = blob_read_uint8(ctx->blob);
 
    nir_cf_node_insert_end(cf_list, &nif->cf_node);
-- 
GitLab


From b063b7c7bba8bfe7b3e6c027e0a06a0a111f478e Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Wed, 8 Sep 2021 11:37:07 +0100
Subject: [PATCH 5/7] Revert "nir: Drop the unused instr arg for src/dest copy
 functions."

This reverts commit c3a01841184ee8303c0c5ebe58491301622c5ad6.
---
 .../vulkan/radv_nir_lower_ycbcr_textures.c    |  2 +-
 src/compiler/glsl/glsl_to_nir.cpp             |  2 +-
 src/compiler/nir/nir.c                        | 24 ++++++++++---------
 src/compiler/nir/nir.h                        | 10 ++++----
 src/compiler/nir/nir_builtin_builder.c        |  4 ++--
 src/compiler/nir/nir_deref.c                  |  4 ++--
 src/compiler/nir/nir_from_ssa.c               |  2 +-
 src/compiler/nir/nir_lower_alu_width.c        |  8 +++----
 src/compiler/nir/nir_lower_atomics_to_ssbo.c  | 12 +++++-----
 src/compiler/nir/nir_lower_bit_size.c         |  2 +-
 src/compiler/nir/nir_lower_indirect_derefs.c  |  2 +-
 src/compiler/nir/nir_lower_io.c               |  2 +-
 .../nir/nir_lower_io_arrays_to_elements.c     |  6 +++--
 src/compiler/nir/nir_lower_io_to_scalar.c     | 10 ++++----
 src/compiler/nir/nir_lower_locals_to_regs.c   |  4 ++--
 src/compiler/nir/nir_lower_phis_to_scalar.c   |  2 +-
 src/compiler/nir/nir_lower_ssbo.c             |  8 +++----
 src/compiler/nir/nir_lower_subgroups.c        | 10 ++++----
 src/compiler/nir/nir_lower_tex.c              |  8 +++----
 src/compiler/nir/nir_lower_vec_to_movs.c      |  4 ++--
 src/compiler/nir/nir_opt_peephole_select.c    |  4 ++--
 src/compiler/nir/nir_opt_undef.c              |  3 ++-
 src/compiler/nir/nir_search.c                 |  3 ++-
 .../drivers/etnaviv/etnaviv_compiler_nir.c    |  2 +-
 .../lima/ir/lima_nir_split_load_input.c       |  2 +-
 .../r600/sfn/sfn_nir_vectorize_vs_inputs.c    |  2 +-
 .../compiler/brw_nir_opt_peephole_ffma.c      |  2 +-
 .../vulkan/anv_nir_lower_ycbcr_textures.c     |  2 +-
 .../compiler/dxil_nir_lower_int_cubemaps.c    |  2 +-
 .../compiler/dxil_nir_lower_int_samplers.c    |  6 ++---
 src/panfrost/midgard/midgard_errata_lod.c     |  2 +-
 31 files changed, 82 insertions(+), 74 deletions(-)

diff --git a/src/amd/vulkan/radv_nir_lower_ycbcr_textures.c b/src/amd/vulkan/radv_nir_lower_ycbcr_textures.c
index e5a38b23f3ea..996cc36cc5b1 100644
--- a/src/amd/vulkan/radv_nir_lower_ycbcr_textures.c
+++ b/src/amd/vulkan/radv_nir_lower_ycbcr_textures.c
@@ -132,7 +132,7 @@ create_plane_tex_instr_implicit(struct ycbcr_state *state, uint32_t plane)
          }
       FALLTHROUGH;
       default:
-         nir_src_copy(&tex->src[i].src, &old_tex->src[i].src);
+         nir_src_copy(&tex->src[i].src, &old_tex->src[i].src, tex);
          break;
       }
    }
diff --git a/src/compiler/glsl/glsl_to_nir.cpp b/src/compiler/glsl/glsl_to_nir.cpp
index 985dce858476..0d833c80d585 100644
--- a/src/compiler/glsl/glsl_to_nir.cpp
+++ b/src/compiler/glsl/glsl_to_nir.cpp
@@ -1686,7 +1686,7 @@ nir_visitor::visit(ir_call *ir)
          nir_ssa_def *val = evaluate_rvalue(param_rvalue);
          nir_src src = nir_src_for_ssa(val);
 
-         nir_src_copy(&call->params[i], &src);
+         nir_src_copy(&call->params[i], &src, call);
       } else if (sig_param->data.mode == ir_var_function_inout) {
          unreachable("unimplemented: inout parameters");
       }
diff --git a/src/compiler/nir/nir.c b/src/compiler/nir/nir.c
index 0f181e891206..f3dc93620e3f 100644
--- a/src/compiler/nir/nir.c
+++ b/src/compiler/nir/nir.c
@@ -454,7 +454,7 @@ static void dest_free_indirects(nir_dest *dest)
 /* NOTE: if the instruction you are copying a src to is already added
  * to the IR, use nir_instr_rewrite_src() instead.
  */
-void nir_src_copy(nir_src *dest, const nir_src *src)
+void nir_src_copy(nir_src *dest, const nir_src *src, void *mem_ctx)
 {
    src_free_indirects(dest);
 
@@ -466,14 +466,14 @@ void nir_src_copy(nir_src *dest, const nir_src *src)
       dest->reg.reg = src->reg.reg;
       if (src->reg.indirect) {
          dest->reg.indirect = calloc(1, sizeof(nir_src));
-         nir_src_copy(dest->reg.indirect, src->reg.indirect);
+         nir_src_copy(dest->reg.indirect, src->reg.indirect, mem_ctx);
       } else {
          dest->reg.indirect = NULL;
       }
    }
 }
 
-void nir_dest_copy(nir_dest *dest, const nir_dest *src)
+void nir_dest_copy(nir_dest *dest, const nir_dest *src, nir_instr *instr)
 {
    /* Copying an SSA definition makes no sense whatsoever. */
    assert(!src->is_ssa);
@@ -486,16 +486,17 @@ void nir_dest_copy(nir_dest *dest, const nir_dest *src)
    dest->reg.reg = src->reg.reg;
    if (src->reg.indirect) {
       dest->reg.indirect = calloc(1, sizeof(nir_src));
-      nir_src_copy(dest->reg.indirect, src->reg.indirect);
+      nir_src_copy(dest->reg.indirect, src->reg.indirect, instr);
    } else {
       dest->reg.indirect = NULL;
    }
 }
 
 void
-nir_alu_src_copy(nir_alu_src *dest, const nir_alu_src *src)
+nir_alu_src_copy(nir_alu_src *dest, const nir_alu_src *src,
+                 nir_alu_instr *instr)
 {
-   nir_src_copy(&dest->src, &src->src);
+   nir_src_copy(&dest->src, &src->src, &instr->instr);
    dest->abs = src->abs;
    dest->negate = src->negate;
    for (unsigned i = 0; i < NIR_MAX_VEC_COMPONENTS; i++)
@@ -503,9 +504,10 @@ nir_alu_src_copy(nir_alu_src *dest, const nir_alu_src *src)
 }
 
 void
-nir_alu_dest_copy(nir_alu_dest *dest, const nir_alu_dest *src)
+nir_alu_dest_copy(nir_alu_dest *dest, const nir_alu_dest *src,
+                  nir_alu_instr *instr)
 {
-   nir_dest_copy(&dest->dest, &src->dest);
+   nir_dest_copy(&dest->dest, &src->dest, &instr->instr);
    dest->write_mask = src->write_mask;
    dest->saturate = src->saturate;
 }
@@ -1680,7 +1682,7 @@ nir_instr_rewrite_src(nir_instr *instr, nir_src *src, nir_src new_src)
    assert(!src_is_valid(src) || src->parent_instr == instr);
 
    src_remove_all_uses(src);
-   nir_src_copy(src, &new_src);
+   nir_src_copy(src, &new_src, instr);
    src_add_all_uses(src, instr, NULL);
 }
 
@@ -1704,7 +1706,7 @@ nir_if_rewrite_condition(nir_if *if_stmt, nir_src new_src)
    assert(!src_is_valid(src) || src->parent_if == if_stmt);
 
    src_remove_all_uses(src);
-   nir_src_copy(src, &new_src);
+   nir_src_copy(src, &new_src, if_stmt);
    src_add_all_uses(src, NULL, if_stmt);
 }
 
@@ -1723,7 +1725,7 @@ nir_instr_rewrite_dest(nir_instr *instr, nir_dest *dest, nir_dest new_dest)
    /* We can't re-write with an SSA def */
    assert(!new_dest.is_ssa);
 
-   nir_dest_copy(dest, &new_dest);
+   nir_dest_copy(dest, &new_dest, instr);
 
    dest->reg.parent_instr = instr;
    list_addtail(&dest->reg.def_link, &new_dest.reg.reg->defs);
diff --git a/src/compiler/nir/nir.h b/src/compiler/nir/nir.h
index b6387fd398e9..a3fef9a2bd5b 100644
--- a/src/compiler/nir/nir.h
+++ b/src/compiler/nir/nir.h
@@ -1136,8 +1136,8 @@ nir_is_sequential_comp_swizzle(uint8_t *swiz, unsigned nr_comp)
    return true;
 }
 
-void nir_src_copy(nir_src *dest, const nir_src *src);
-void nir_dest_copy(nir_dest *dest, const nir_dest *src);
+void nir_src_copy(nir_src *dest, const nir_src *src, void *instr_or_if);
+void nir_dest_copy(nir_dest *dest, const nir_dest *src, nir_instr *instr);
 
 typedef struct {
    /** Base source */
@@ -1458,8 +1458,10 @@ typedef struct nir_alu_instr {
    nir_alu_src src[];
 } nir_alu_instr;
 
-void nir_alu_src_copy(nir_alu_src *dest, const nir_alu_src *src);
-void nir_alu_dest_copy(nir_alu_dest *dest, const nir_alu_dest *src);
+void nir_alu_src_copy(nir_alu_src *dest, const nir_alu_src *src,
+                      nir_alu_instr *instr);
+void nir_alu_dest_copy(nir_alu_dest *dest, const nir_alu_dest *src,
+                       nir_alu_instr *instr);
 
 bool nir_alu_instr_is_copy(nir_alu_instr *instr);
 
diff --git a/src/compiler/nir/nir_builtin_builder.c b/src/compiler/nir/nir_builtin_builder.c
index b975157cfb17..26b524122238 100644
--- a/src/compiler/nir/nir_builtin_builder.c
+++ b/src/compiler/nir/nir_builtin_builder.c
@@ -366,7 +366,7 @@ nir_get_texture_size(nir_builder *b, nir_tex_instr *tex)
           tex->src[i].src_type == nir_tex_src_sampler_offset ||
           tex->src[i].src_type == nir_tex_src_texture_handle ||
           tex->src[i].src_type == nir_tex_src_sampler_handle) {
-         nir_src_copy(&txs->src[idx].src, &tex->src[i].src);
+         nir_src_copy(&txs->src[idx].src, &tex->src[i].src, txs);
          txs->src[idx].src_type = tex->src[i].src_type;
          idx++;
       }
@@ -421,7 +421,7 @@ nir_get_texture_lod(nir_builder *b, nir_tex_instr *tex)
           tex->src[i].src_type == nir_tex_src_sampler_offset ||
           tex->src[i].src_type == nir_tex_src_texture_handle ||
           tex->src[i].src_type == nir_tex_src_sampler_handle) {
-         nir_src_copy(&tql->src[idx].src, &tex->src[i].src);
+         nir_src_copy(&tql->src[idx].src, &tex->src[i].src, tql);
          tql->src[idx].src_type = tex->src[i].src_type;
          idx++;
       }
diff --git a/src/compiler/nir/nir_deref.c b/src/compiler/nir/nir_deref.c
index f9d599c02d56..9908f4998f13 100644
--- a/src/compiler/nir/nir_deref.c
+++ b/src/compiler/nir/nir_deref.c
@@ -747,7 +747,7 @@ rematerialize_deref_in_block(nir_deref_instr *deref,
          parent = rematerialize_deref_in_block(parent, state);
          new_deref->parent = nir_src_for_ssa(&parent->dest.ssa);
       } else {
-         nir_src_copy(&new_deref->parent, &deref->parent);
+         nir_src_copy(&new_deref->parent, &deref->parent, new_deref);
       }
    }
 
@@ -764,7 +764,7 @@ rematerialize_deref_in_block(nir_deref_instr *deref,
    case nir_deref_type_array:
    case nir_deref_type_ptr_as_array:
       assert(!nir_src_as_deref(deref->arr.index));
-      nir_src_copy(&new_deref->arr.index, &deref->arr.index);
+      nir_src_copy(&new_deref->arr.index, &deref->arr.index, new_deref);
       break;
 
    case nir_deref_type_struct:
diff --git a/src/compiler/nir/nir_from_ssa.c b/src/compiler/nir/nir_from_ssa.c
index 3bcac93302c2..38dcc9a67031 100644
--- a/src/compiler/nir/nir_from_ssa.c
+++ b/src/compiler/nir/nir_from_ssa.c
@@ -638,7 +638,7 @@ emit_copy(nir_builder *b, nir_src src, nir_src dest_src)
       assert(src.reg.reg->num_components >= dest_src.reg.reg->num_components);
 
    nir_alu_instr *mov = nir_alu_instr_create(b->shader, nir_op_mov);
-   nir_src_copy(&mov->src[0].src, &src);
+   nir_src_copy(&mov->src[0].src, &src, mov);
    mov->dest.dest = nir_dest_for_reg(dest_src.reg.reg);
    mov->dest.write_mask = (1 << dest_src.reg.reg->num_components) - 1;
 
diff --git a/src/compiler/nir/nir_lower_alu_width.c b/src/compiler/nir/nir_lower_alu_width.c
index 039e2bd796ba..c75fc75f5748 100644
--- a/src/compiler/nir/nir_lower_alu_width.c
+++ b/src/compiler/nir/nir_lower_alu_width.c
@@ -106,11 +106,11 @@ lower_reduction(nir_alu_instr *alu, nir_op chan_op, nir_op merge_op,
    for (int i = num_components - 1; i >= 0; i--) {
       nir_alu_instr *chan = nir_alu_instr_create(builder->shader, chan_op);
       nir_alu_ssa_dest_init(chan, 1, alu->dest.dest.ssa.bit_size);
-      nir_alu_src_copy(&chan->src[0], &alu->src[0]);
+      nir_alu_src_copy(&chan->src[0], &alu->src[0], chan);
       chan->src[0].swizzle[0] = chan->src[0].swizzle[i];
       if (nir_op_infos[chan_op].num_inputs > 1) {
          assert(nir_op_infos[chan_op].num_inputs == 2);
-         nir_alu_src_copy(&chan->src[1], &alu->src[1]);
+         nir_alu_src_copy(&chan->src[1], &alu->src[1], chan);
          chan->src[1].swizzle[0] = chan->src[1].swizzle[i];
       }
       chan->exact = alu->exact;
@@ -159,7 +159,7 @@ lower_fdot(nir_alu_instr *alu, nir_builder *builder)
          builder->shader, prev ? nir_op_ffma : nir_op_fmul);
       nir_alu_ssa_dest_init(instr, 1, alu->dest.dest.ssa.bit_size);
       for (unsigned j = 0; j < 2; j++) {
-         nir_alu_src_copy(&instr->src[j], &alu->src[j]);
+         nir_alu_src_copy(&instr->src[j], &alu->src[j], instr);
          instr->src[j].swizzle[0] = alu->src[j].swizzle[i];
       }
       if (i != num_components - 1)
@@ -381,7 +381,7 @@ lower_alu_instr_width(nir_builder *b, nir_instr *instr, void *_data)
       nir_alu_instr *lower = nir_alu_instr_create(b->shader, alu->op);
 
       for (i = 0; i < num_src; i++) {
-         nir_alu_src_copy(&lower->src[i], &alu->src[i]);
+         nir_alu_src_copy(&lower->src[i], &alu->src[i], lower);
 
          /* We only handle same-size-as-dest (input_sizes[] == 0) or scalar
           * args (input_sizes[] == 1).
diff --git a/src/compiler/nir/nir_lower_atomics_to_ssbo.c b/src/compiler/nir/nir_lower_atomics_to_ssbo.c
index a7e10c6a0636..621938fb0cf3 100644
--- a/src/compiler/nir/nir_lower_atomics_to_ssbo.c
+++ b/src/compiler/nir/nir_lower_atomics_to_ssbo.c
@@ -122,7 +122,7 @@ lower_instr(nir_intrinsic_instr *instr, unsigned ssbo_offset, nir_builder *b, un
       /* remapped to ssbo_atomic_add: { buffer_idx, offset, +1 } */
       temp = nir_imm_int(b, +1);
       new_instr->src[0] = nir_src_for_ssa(buffer);
-      nir_src_copy(&new_instr->src[1], &instr->src[0]);
+      nir_src_copy(&new_instr->src[1], &instr->src[0], new_instr);
       new_instr->src[2] = nir_src_for_ssa(temp);
       break;
    case nir_intrinsic_atomic_counter_pre_dec:
@@ -131,22 +131,22 @@ lower_instr(nir_intrinsic_instr *instr, unsigned ssbo_offset, nir_builder *b, un
       /* NOTE semantic difference so we adjust the return value below */
       temp = nir_imm_int(b, -1);
       new_instr->src[0] = nir_src_for_ssa(buffer);
-      nir_src_copy(&new_instr->src[1], &instr->src[0]);
+      nir_src_copy(&new_instr->src[1], &instr->src[0], new_instr);
       new_instr->src[2] = nir_src_for_ssa(temp);
       break;
    case nir_intrinsic_atomic_counter_read:
       /* remapped to load_ssbo: { buffer_idx, offset } */
       new_instr->src[0] = nir_src_for_ssa(buffer);
-      nir_src_copy(&new_instr->src[1], &instr->src[0]);
+      nir_src_copy(&new_instr->src[1], &instr->src[0], new_instr);
       break;
    default:
       /* remapped to ssbo_atomic_x: { buffer_idx, offset, data, (compare)? } */
       new_instr->src[0] = nir_src_for_ssa(buffer);
-      nir_src_copy(&new_instr->src[1], &instr->src[0]);
-      nir_src_copy(&new_instr->src[2], &instr->src[1]);
+      nir_src_copy(&new_instr->src[1], &instr->src[0], new_instr);
+      nir_src_copy(&new_instr->src[2], &instr->src[1], new_instr);
       if (op == nir_intrinsic_ssbo_atomic_comp_swap ||
           op == nir_intrinsic_ssbo_atomic_fcomp_swap)
-         nir_src_copy(&new_instr->src[3], &instr->src[2]);
+         nir_src_copy(&new_instr->src[3], &instr->src[2], new_instr);
       break;
    }
 
diff --git a/src/compiler/nir/nir_lower_bit_size.c b/src/compiler/nir/nir_lower_bit_size.c
index 4c76d8d56c39..eb4d9cdc048b 100644
--- a/src/compiler/nir/nir_lower_bit_size.c
+++ b/src/compiler/nir/nir_lower_bit_size.c
@@ -38,7 +38,7 @@ static nir_ssa_def *convert_to_bit_size(nir_builder *bld, nir_ssa_def *src,
    if ((type & (nir_type_uint | nir_type_int)) && bit_size == 32 &&
        alu && (alu->op == nir_op_b2i8 || alu->op == nir_op_b2i16)) {
       nir_alu_instr *instr = nir_alu_instr_create(bld->shader, nir_op_b2i32);
-      nir_alu_src_copy(&instr->src[0], &alu->src[0]);
+      nir_alu_src_copy(&instr->src[0], &alu->src[0], instr);
       return nir_builder_alu_instr_finish_and_insert(bld, instr);
    }
 
diff --git a/src/compiler/nir/nir_lower_indirect_derefs.c b/src/compiler/nir/nir_lower_indirect_derefs.c
index 93be73725280..e30ccad9ebc5 100644
--- a/src/compiler/nir/nir_lower_indirect_derefs.c
+++ b/src/compiler/nir/nir_lower_indirect_derefs.c
@@ -98,7 +98,7 @@ emit_load_store_deref(nir_builder *b, nir_intrinsic_instr *orig_instr,
       /* Copy over any other sources.  This is needed for interp_deref_at */
       for (unsigned i = 1;
            i < nir_intrinsic_infos[orig_instr->intrinsic].num_srcs; i++)
-         nir_src_copy(&load->src[i], &orig_instr->src[i]);
+         nir_src_copy(&load->src[i], &orig_instr->src[i], load);
 
       nir_ssa_dest_init(&load->instr, &load->dest,
                         orig_instr->dest.ssa.num_components,
diff --git a/src/compiler/nir/nir_lower_io.c b/src/compiler/nir/nir_lower_io.c
index 9cb400b5a6e4..bb568ed4975d 100644
--- a/src/compiler/nir/nir_lower_io.c
+++ b/src/compiler/nir/nir_lower_io.c
@@ -606,7 +606,7 @@ lower_interpolate_at(nir_intrinsic_instr *intrin, struct lower_io_state *state,
    if (intrin->intrinsic == nir_intrinsic_interp_deref_at_sample ||
        intrin->intrinsic == nir_intrinsic_interp_deref_at_offset ||
        intrin->intrinsic == nir_intrinsic_interp_deref_at_vertex)
-      nir_src_copy(&bary_setup->src[0], &intrin->src[1]);
+      nir_src_copy(&bary_setup->src[0], &intrin->src[1], bary_setup);
 
    nir_builder_instr_insert(b, &bary_setup->instr);
 
diff --git a/src/compiler/nir/nir_lower_io_arrays_to_elements.c b/src/compiler/nir/nir_lower_io_arrays_to_elements.c
index 6383b1d0022a..901da66962bd 100644
--- a/src/compiler/nir/nir_lower_io_arrays_to_elements.c
+++ b/src/compiler/nir/nir_lower_io_arrays_to_elements.c
@@ -181,7 +181,8 @@ lower_array(nir_builder *b, nir_intrinsic_instr *intr, nir_variable *var,
       if (intr->intrinsic == nir_intrinsic_interp_deref_at_offset ||
           intr->intrinsic == nir_intrinsic_interp_deref_at_sample ||
           intr->intrinsic == nir_intrinsic_interp_deref_at_vertex) {
-         nir_src_copy(&element_intr->src[1], &intr->src[1]);
+         nir_src_copy(&element_intr->src[1], &intr->src[1],
+                      &element_intr->instr);
       }
 
       nir_ssa_def_rewrite_uses(&intr->dest.ssa,
@@ -189,7 +190,8 @@ lower_array(nir_builder *b, nir_intrinsic_instr *intr, nir_variable *var,
    } else {
       nir_intrinsic_set_write_mask(element_intr,
                                    nir_intrinsic_write_mask(intr));
-      nir_src_copy(&element_intr->src[1], &intr->src[1]);
+      nir_src_copy(&element_intr->src[1], &intr->src[1],
+                   &element_intr->instr);
    }
 
    nir_builder_instr_insert(b, &element_intr->instr);
diff --git a/src/compiler/nir/nir_lower_io_to_scalar.c b/src/compiler/nir/nir_lower_io_to_scalar.c
index 7f0d1332d65d..a4fa671976fc 100644
--- a/src/compiler/nir/nir_lower_io_to_scalar.c
+++ b/src/compiler/nir/nir_lower_io_to_scalar.c
@@ -61,7 +61,7 @@ lower_load_input_to_scalar(nir_builder *b, nir_intrinsic_instr *intr)
       nir_intrinsic_set_dest_type(chan_intr, nir_intrinsic_dest_type(intr));
       set_io_semantics(chan_intr, intr, i);
       /* offset */
-      nir_src_copy(&chan_intr->src[0], &intr->src[0]);
+      nir_src_copy(&chan_intr->src[0], &intr->src[0], chan_intr);
 
       nir_builder_instr_insert(b, &chan_intr->instr);
 
@@ -104,7 +104,7 @@ lower_load_to_scalar(nir_builder *b, nir_intrinsic_instr *intr)
       if (nir_intrinsic_has_base(intr))
          nir_intrinsic_set_base(chan_intr, nir_intrinsic_base(intr));
       for (unsigned j = 0; j < nir_intrinsic_infos[intr->intrinsic].num_srcs - 1; j++)
-         nir_src_copy(&chan_intr->src[j], &intr->src[j]);
+         nir_src_copy(&chan_intr->src[j], &intr->src[j], &chan_intr->instr);
 
       /* increment offset per component */
       nir_ssa_def *offset = nir_iadd_imm(b, base_offset, i * (intr->dest.ssa.bit_size / 8));
@@ -167,7 +167,7 @@ lower_store_output_to_scalar(nir_builder *b, nir_intrinsic_instr *intr)
       /* value */
       chan_intr->src[0] = nir_src_for_ssa(nir_channel(b, value, i));
       /* offset */
-      nir_src_copy(&chan_intr->src[1], &intr->src[1]);
+      nir_src_copy(&chan_intr->src[1], &intr->src[1], chan_intr);
 
       nir_builder_instr_insert(b, &chan_intr->instr);
    }
@@ -203,7 +203,7 @@ lower_store_to_scalar(nir_builder *b, nir_intrinsic_instr *intr)
       /* value */
       chan_intr->src[0] = nir_src_for_ssa(nir_channel(b, value, i));
       for (unsigned j = 1; j < nir_intrinsic_infos[intr->intrinsic].num_srcs - 1; j++)
-         nir_src_copy(&chan_intr->src[j], &intr->src[j]);
+         nir_src_copy(&chan_intr->src[j], &intr->src[j], &chan_intr->instr);
 
       /* increment offset per component */
       nir_ssa_def *offset = nir_iadd_imm(b, base_offset, i * (value->bit_size / 8));
@@ -354,7 +354,7 @@ lower_load_to_scalar_early(nir_builder *b, nir_intrinsic_instr *intr,
       if (intr->intrinsic == nir_intrinsic_interp_deref_at_offset ||
           intr->intrinsic == nir_intrinsic_interp_deref_at_sample ||
           intr->intrinsic == nir_intrinsic_interp_deref_at_vertex)
-         nir_src_copy(&chan_intr->src[1], &intr->src[1]);
+         nir_src_copy(&chan_intr->src[1], &intr->src[1], &chan_intr->instr);
 
       nir_builder_instr_insert(b, &chan_intr->instr);
 
diff --git a/src/compiler/nir/nir_lower_locals_to_regs.c b/src/compiler/nir/nir_lower_locals_to_regs.c
index 40e10cadb0a0..d4d4c8899bf4 100644
--- a/src/compiler/nir/nir_lower_locals_to_regs.c
+++ b/src/compiler/nir/nir_lower_locals_to_regs.c
@@ -218,7 +218,7 @@ lower_locals_to_regs_block(nir_block *block,
             nir_ssa_def_rewrite_uses(&intrin->dest.ssa,
                                      &mov->dest.dest.ssa);
          } else {
-            nir_dest_copy(&mov->dest.dest, &intrin->dest);
+            nir_dest_copy(&mov->dest.dest, &intrin->dest, &mov->instr);
          }
          nir_builder_instr_insert(b, &mov->instr);
 
@@ -246,7 +246,7 @@ lower_locals_to_regs_block(nir_block *block,
 
          nir_alu_instr *mov = nir_alu_instr_create(b->shader, nir_op_mov);
 
-         nir_src_copy(&mov->src[0].src, &intrin->src[1]);
+         nir_src_copy(&mov->src[0].src, &intrin->src[1], mov);
 
          /* The normal NIR SSA copy propagate pass can't happen after this pass,
           * so do an ad-hoc copy propagate since this ALU op can do swizzles
diff --git a/src/compiler/nir/nir_lower_phis_to_scalar.c b/src/compiler/nir/nir_lower_phis_to_scalar.c
index 9abaf24bae5d..9ba119a19380 100644
--- a/src/compiler/nir/nir_lower_phis_to_scalar.c
+++ b/src/compiler/nir/nir_lower_phis_to_scalar.c
@@ -239,7 +239,7 @@ lower_phis_to_scalar_block(nir_block *block,
                                                       nir_op_mov);
             nir_ssa_dest_init(&mov->instr, &mov->dest.dest, 1, bit_size, NULL);
             mov->dest.write_mask = 1;
-            nir_src_copy(&mov->src[0].src, &src->src);
+            nir_src_copy(&mov->src[0].src, &src->src, &mov->instr);
             mov->src[0].swizzle[0] = i;
 
             /* Insert at the end of the predecessor but before the jump */
diff --git a/src/compiler/nir/nir_lower_ssbo.c b/src/compiler/nir/nir_lower_ssbo.c
index 408b03b35005..19a040a68d86 100644
--- a/src/compiler/nir/nir_lower_ssbo.c
+++ b/src/compiler/nir/nir_lower_ssbo.c
@@ -90,7 +90,7 @@ nir_load_ssbo_prop(nir_builder *b, nir_intrinsic_op op,
 {
    nir_intrinsic_instr *load = nir_intrinsic_instr_create(b->shader, op);
    load->num_components = 1;
-   nir_src_copy(&load->src[0], idx);
+   nir_src_copy(&load->src[0], idx, load);
    nir_ssa_dest_init(&load->instr, &load->dest, 1, bitsize, NULL);
    nir_builder_instr_insert(b, &load->instr);
    return &load->dest.ssa;
@@ -134,7 +134,7 @@ lower_ssbo_instr(nir_builder *b, nir_intrinsic_instr *intr)
    }
 
    if (is_store) {
-      nir_src_copy(&global->src[0], &intr->src[0]);
+      nir_src_copy(&global->src[0], &intr->src[0], global);
       nir_intrinsic_set_write_mask(global, nir_intrinsic_write_mask(intr));
    } else {
       nir_ssa_dest_init(&global->instr, &global->dest,
@@ -142,9 +142,9 @@ lower_ssbo_instr(nir_builder *b, nir_intrinsic_instr *intr)
                         intr->dest.ssa.bit_size, NULL);
 
       if (is_atomic) {
-         nir_src_copy(&global->src[1], &intr->src[2]);
+         nir_src_copy(&global->src[1], &intr->src[2], global);
          if (nir_intrinsic_infos[op].num_srcs > 2)
-            nir_src_copy(&global->src[2], &intr->src[3]);
+            nir_src_copy(&global->src[2], &intr->src[3], global);
       }
    }
 
diff --git a/src/compiler/nir/nir_lower_subgroups.c b/src/compiler/nir/nir_lower_subgroups.c
index a83a17107046..737663ec6892 100644
--- a/src/compiler/nir/nir_lower_subgroups.c
+++ b/src/compiler/nir/nir_lower_subgroups.c
@@ -45,7 +45,7 @@ lower_subgroups_64bit_split_intrinsic(nir_builder *b, nir_intrinsic_instr *intri
    intr->const_index[1] = intrin->const_index[1];
    intr->src[0] = nir_src_for_ssa(comp);
    if (nir_intrinsic_infos[intrin->intrinsic].num_srcs == 2)
-      nir_src_copy(&intr->src[1], &intrin->src[1]);
+      nir_src_copy(&intr->src[1], &intrin->src[1], intr);
 
    intr->num_components = 1;
    nir_builder_instr_insert(b, &intr->instr);
@@ -126,7 +126,7 @@ lower_subgroup_op_to_scalar(nir_builder *b, nir_intrinsic_instr *intrin,
       /* invocation */
       if (nir_intrinsic_infos[intrin->intrinsic].num_srcs > 1) {
          assert(nir_intrinsic_infos[intrin->intrinsic].num_srcs == 2);
-         nir_src_copy(&chan_intrin->src[1], &intrin->src[1]);
+         nir_src_copy(&chan_intrin->src[1], &intrin->src[1], chan_intrin);
       }
 
       chan_intrin->const_index[0] = intrin->const_index[0];
@@ -209,7 +209,7 @@ lower_shuffle_to_swizzle(nir_builder *b, nir_intrinsic_instr *intrin,
    nir_intrinsic_instr *swizzle = nir_intrinsic_instr_create(
       b->shader, nir_intrinsic_masked_swizzle_amd);
    swizzle->num_components = intrin->num_components;
-   nir_src_copy(&swizzle->src[0], &intrin->src[0]);
+   nir_src_copy(&swizzle->src[0], &intrin->src[0], swizzle);
    nir_intrinsic_set_swizzle_mask(swizzle, (mask << 10) | 0x1f);
    nir_ssa_dest_init(&swizzle->instr, &swizzle->dest,
                      intrin->dest.ssa.num_components,
@@ -288,7 +288,7 @@ lower_to_shuffle(nir_builder *b, nir_intrinsic_instr *intrin,
    nir_intrinsic_instr *shuffle =
       nir_intrinsic_instr_create(b->shader, nir_intrinsic_shuffle);
    shuffle->num_components = intrin->num_components;
-   nir_src_copy(&shuffle->src[0], &intrin->src[0]);
+   nir_src_copy(&shuffle->src[0], &intrin->src[0], shuffle);
    shuffle->src[1] = nir_src_for_ssa(index);
    nir_ssa_dest_init(&shuffle->instr, &shuffle->dest,
                      intrin->dest.ssa.num_components,
@@ -568,7 +568,7 @@ lower_dynamic_quad_broadcast(nir_builder *b, nir_intrinsic_instr *intrin,
 
       qbcst->num_components = intrin->num_components;
       qbcst->src[1] = nir_src_for_ssa(nir_imm_int(b, i));
-      nir_src_copy(&qbcst->src[0], &intrin->src[0]);
+      nir_src_copy(&qbcst->src[0], &intrin->src[0], qbcst);
       nir_ssa_dest_init(&qbcst->instr, &qbcst->dest,
                         intrin->dest.ssa.num_components,
                         intrin->dest.ssa.bit_size, NULL);
diff --git a/src/compiler/nir/nir_lower_tex.c b/src/compiler/nir/nir_lower_tex.c
index ec48d9a4ac9a..31da4cb2bd99 100644
--- a/src/compiler/nir/nir_lower_tex.c
+++ b/src/compiler/nir/nir_lower_tex.c
@@ -289,7 +289,7 @@ sample_plane(nir_builder *b, nir_tex_instr *tex, int plane,
    nir_tex_instr *plane_tex =
       nir_tex_instr_create(b->shader, tex->num_srcs + 1);
    for (unsigned i = 0; i < tex->num_srcs; i++) {
-      nir_src_copy(&plane_tex->src[i].src, &tex->src[i].src);
+      nir_src_copy(&plane_tex->src[i].src, &tex->src[i].src, plane_tex);
       plane_tex->src[i].src_type = tex->src[i].src_type;
    }
    plane_tex->src[tex->num_srcs].src = nir_src_for_ssa(nir_imm_int(b, plane));
@@ -773,7 +773,7 @@ lower_tex_to_txd(nir_builder *b, nir_tex_instr *tex)
 
    /* reuse existing srcs */
    for (unsigned i = 0; i < tex->num_srcs; i++) {
-      nir_src_copy(&txd->src[i].src, &tex->src[i].src);
+      nir_src_copy(&txd->src[i].src, &tex->src[i].src, txd);
       txd->src[i].src_type = tex->src[i].src_type;
    }
    int coord = nir_tex_instr_src_index(tex, nir_tex_src_coord);
@@ -813,7 +813,7 @@ lower_txb_to_txl(nir_builder *b, nir_tex_instr *tex)
    /* reuse all but bias src */
    for (int i = 0; i < 2; i++) {
       if (tex->src[i].src_type != nir_tex_src_bias) {
-         nir_src_copy(&txl->src[i].src, &tex->src[i].src);
+         nir_src_copy(&txl->src[i].src, &tex->src[i].src, txl);
          txl->src[i].src_type = tex->src[i].src_type;
       }
    }
@@ -1125,7 +1125,7 @@ lower_tg4_offsets(nir_builder *b, nir_tex_instr *tex)
       tex_copy->dest_type = tex->dest_type;
 
       for (unsigned j = 0; j < tex->num_srcs; ++j) {
-         nir_src_copy(&tex_copy->src[j].src, &tex->src[j].src);
+         nir_src_copy(&tex_copy->src[j].src, &tex->src[j].src, tex_copy);
          tex_copy->src[j].src_type = tex->src[j].src_type;
       }
 
diff --git a/src/compiler/nir/nir_lower_vec_to_movs.c b/src/compiler/nir/nir_lower_vec_to_movs.c
index dd3827688716..afebc1364527 100644
--- a/src/compiler/nir/nir_lower_vec_to_movs.c
+++ b/src/compiler/nir/nir_lower_vec_to_movs.c
@@ -68,8 +68,8 @@ insert_mov(nir_alu_instr *vec, unsigned start_idx, nir_shader *shader)
       return 1 << start_idx;
 
    nir_alu_instr *mov = nir_alu_instr_create(shader, nir_op_mov);
-   nir_alu_src_copy(&mov->src[0], &vec->src[start_idx]);
-   nir_alu_dest_copy(&mov->dest, &vec->dest);
+   nir_alu_src_copy(&mov->src[0], &vec->src[start_idx], mov);
+   nir_alu_dest_copy(&mov->dest, &vec->dest, mov);
 
    mov->dest.write_mask = (1u << start_idx);
    mov->src[0].swizzle[start_idx] = vec->src[start_idx].swizzle[0];
diff --git a/src/compiler/nir/nir_opt_peephole_select.c b/src/compiler/nir/nir_opt_peephole_select.c
index fc0a32b2bc68..cc5fb5463ef6 100644
--- a/src/compiler/nir/nir_opt_peephole_select.c
+++ b/src/compiler/nir/nir_opt_peephole_select.c
@@ -457,7 +457,7 @@ nir_opt_peephole_select_block(nir_block *block, nir_shader *shader,
 
       nir_phi_instr *phi = nir_instr_as_phi(instr);
       nir_alu_instr *sel = nir_alu_instr_create(shader, nir_op_bcsel);
-      nir_src_copy(&sel->src[0].src, &if_stmt->condition);
+      nir_src_copy(&sel->src[0].src, &if_stmt->condition, sel);
       /* Splat the condition to all channels */
       memset(sel->src[0].swizzle, 0, sizeof sel->src[0].swizzle);
 
@@ -467,7 +467,7 @@ nir_opt_peephole_select_block(nir_block *block, nir_shader *shader,
          assert(src->src.is_ssa);
 
          unsigned idx = src->pred == then_block ? 1 : 2;
-         nir_src_copy(&sel->src[idx].src, &src->src);
+         nir_src_copy(&sel->src[idx].src, &src->src, sel);
       }
 
       nir_ssa_dest_init(&sel->instr, &sel->dest.dest,
diff --git a/src/compiler/nir/nir_opt_undef.c b/src/compiler/nir/nir_opt_undef.c
index 5b19393c8a8e..27182ef3d5b5 100644
--- a/src/compiler/nir/nir_opt_undef.c
+++ b/src/compiler/nir/nir_opt_undef.c
@@ -56,7 +56,8 @@ opt_undef_csel(nir_alu_instr *instr)
        */
       nir_instr_rewrite_src(&instr->instr, &instr->src[0].src,
                             instr->src[i == 1 ? 2 : 1].src);
-      nir_alu_src_copy(&instr->src[0], &instr->src[i == 1 ? 2 : 1]);
+      nir_alu_src_copy(&instr->src[0], &instr->src[i == 1 ? 2 : 1],
+                       instr);
 
       nir_src empty_src;
       memset(&empty_src, 0, sizeof(empty_src));
diff --git a/src/compiler/nir/nir_search.c b/src/compiler/nir/nir_search.c
index 1ee3ce4ddcc4..e5e00ecd13f8 100644
--- a/src/compiler/nir/nir_search.c
+++ b/src/compiler/nir/nir_search.c
@@ -526,7 +526,8 @@ construct_value(nir_builder *build,
       assert(state->variables_seen & (1 << var->variable));
 
       nir_alu_src val = { NIR_SRC_INIT };
-      nir_alu_src_copy(&val, &state->variables[var->variable]);
+      nir_alu_src_copy(&val, &state->variables[var->variable],
+                       (void *)build->shader);
       assert(!var->is_constant);
 
       for (unsigned i = 0; i < NIR_MAX_VEC_COMPONENTS; i++)
diff --git a/src/gallium/drivers/etnaviv/etnaviv_compiler_nir.c b/src/gallium/drivers/etnaviv/etnaviv_compiler_nir.c
index 7f50fbeae4f9..4642d4a7019a 100644
--- a/src/gallium/drivers/etnaviv/etnaviv_compiler_nir.c
+++ b/src/gallium/drivers/etnaviv/etnaviv_compiler_nir.c
@@ -689,7 +689,7 @@ insert_vec_mov(nir_alu_instr *vec, unsigned start_idx, nir_shader *shader)
    unsigned write_mask = (1u << start_idx);
 
    nir_alu_instr *mov = nir_alu_instr_create(shader, nir_op_mov);
-   nir_alu_src_copy(&mov->src[0], &vec->src[start_idx]);
+   nir_alu_src_copy(&mov->src[0], &vec->src[start_idx], mov);
 
    mov->src[0].swizzle[0] = vec->src[start_idx].swizzle[0];
    mov->src[0].negate = vec->src[start_idx].negate;
diff --git a/src/gallium/drivers/lima/ir/lima_nir_split_load_input.c b/src/gallium/drivers/lima/ir/lima_nir_split_load_input.c
index 4cd37bdd2430..7506c0f90be5 100644
--- a/src/gallium/drivers/lima/ir/lima_nir_split_load_input.c
+++ b/src/gallium/drivers/lima/ir/lima_nir_split_load_input.c
@@ -85,7 +85,7 @@ lima_nir_split_load_input_instr(nir_builder *b,
    nir_intrinsic_set_dest_type(new_intrin, nir_intrinsic_dest_type(intrin));
 
    /* offset */
-   nir_src_copy(&new_intrin->src[0], &intrin->src[0]);
+   nir_src_copy(&new_intrin->src[0], &intrin->src[0], new_intrin);
 
    nir_builder_instr_insert(b, &new_intrin->instr);
    nir_ssa_def_rewrite_uses(&alu->dest.dest.ssa,
diff --git a/src/gallium/drivers/r600/sfn/sfn_nir_vectorize_vs_inputs.c b/src/gallium/drivers/r600/sfn/sfn_nir_vectorize_vs_inputs.c
index 583eb12d89ee..35fd5cfffa19 100644
--- a/src/gallium/drivers/r600/sfn/sfn_nir_vectorize_vs_inputs.c
+++ b/src/gallium/drivers/r600/sfn/sfn_nir_vectorize_vs_inputs.c
@@ -159,7 +159,7 @@ r600_create_new_load(nir_builder *b, nir_intrinsic_instr *intr, nir_variable *va
 
    if (intr->intrinsic == nir_intrinsic_interp_deref_at_offset ||
        intr->intrinsic == nir_intrinsic_interp_deref_at_sample)
-      nir_src_copy(&new_intr->src[1], &intr->src[1]);
+      nir_src_copy(&new_intr->src[1], &intr->src[1], &new_intr->instr);
 
    nir_builder_instr_insert(b, &new_intr->instr);
 
diff --git a/src/intel/compiler/brw_nir_opt_peephole_ffma.c b/src/intel/compiler/brw_nir_opt_peephole_ffma.c
index fb5ff5c61f1b..eab22819f2ba 100644
--- a/src/intel/compiler/brw_nir_opt_peephole_ffma.c
+++ b/src/intel/compiler/brw_nir_opt_peephole_ffma.c
@@ -243,7 +243,7 @@ brw_nir_opt_peephole_ffma_instr(nir_builder *b,
       for (unsigned j = 0; j < add->dest.dest.ssa.num_components; j++)
          ffma->src[i].swizzle[j] = mul->src[i].swizzle[swizzle[j]];
    }
-   nir_alu_src_copy(&ffma->src[2], &add->src[1 - add_mul_src]);
+   nir_alu_src_copy(&ffma->src[2], &add->src[1 - add_mul_src], ffma);
 
    assert(add->dest.dest.is_ssa);
 
diff --git a/src/intel/vulkan/anv_nir_lower_ycbcr_textures.c b/src/intel/vulkan/anv_nir_lower_ycbcr_textures.c
index fdc77d879883..58f81c208038 100644
--- a/src/intel/vulkan/anv_nir_lower_ycbcr_textures.c
+++ b/src/intel/vulkan/anv_nir_lower_ycbcr_textures.c
@@ -135,7 +135,7 @@ create_plane_tex_instr_implicit(struct ycbcr_state *state,
          }
          FALLTHROUGH;
       default:
-         nir_src_copy(&tex->src[i].src, &old_tex->src[i].src);
+         nir_src_copy(&tex->src[i].src, &old_tex->src[i].src, tex);
          break;
       }
    }
diff --git a/src/microsoft/compiler/dxil_nir_lower_int_cubemaps.c b/src/microsoft/compiler/dxil_nir_lower_int_cubemaps.c
index be4a1d2b7ffe..e315faf8d494 100644
--- a/src/microsoft/compiler/dxil_nir_lower_int_cubemaps.c
+++ b/src/microsoft/compiler/dxil_nir_lower_int_cubemaps.c
@@ -210,7 +210,7 @@ create_array_tex_from_cube_tex(nir_builder *b, nir_tex_instr *tex, nir_ssa_def *
       nir_src *psrc = (tex->src[i].src_type == nir_tex_src_coord) ?
                          &coord_src : &tex->src[i].src;
 
-      nir_src_copy(&array_tex->src[i].src, psrc);
+      nir_src_copy(&array_tex->src[i].src, psrc, array_tex);
       array_tex->src[i].src_type = tex->src[i].src_type;
    }
 
diff --git a/src/microsoft/compiler/dxil_nir_lower_int_samplers.c b/src/microsoft/compiler/dxil_nir_lower_int_samplers.c
index 4811fa370525..4546a79a1cf0 100644
--- a/src/microsoft/compiler/dxil_nir_lower_int_samplers.c
+++ b/src/microsoft/compiler/dxil_nir_lower_int_samplers.c
@@ -81,7 +81,7 @@ dx_get_texture_lod(nir_builder *b, nir_tex_instr *tex)
    nir_ssa_def *ssa_src = nir_channels(b, tex->src[coord_index].src.ssa,
                                        (1 << coord_components) - 1);
    nir_src src = nir_src_for_ssa(ssa_src);
-   nir_src_copy(&tql->src[0].src, &src);
+   nir_src_copy(&tql->src[0].src, &src, tql);
    tql->src[0].src_type = nir_tex_src_coord;
 
    unsigned idx = 1;
@@ -92,7 +92,7 @@ dx_get_texture_lod(nir_builder *b, nir_tex_instr *tex)
           tex->src[i].src_type == nir_tex_src_sampler_offset ||
           tex->src[i].src_type == nir_tex_src_texture_handle ||
           tex->src[i].src_type == nir_tex_src_sampler_handle) {
-         nir_src_copy(&tql->src[idx].src, &tex->src[i].src);
+         nir_src_copy(&tql->src[idx].src, &tex->src[i].src, tql);
          tql->src[idx].src_type = tex->src[i].src_type;
          idx++;
       }
@@ -278,7 +278,7 @@ create_txf_from_tex(nir_builder *b, nir_tex_instr *tex)
       if (tex->src[i].src_type == nir_tex_src_texture_deref ||
           tex->src[i].src_type == nir_tex_src_texture_offset ||
           tex->src[i].src_type == nir_tex_src_texture_handle) {
-         nir_src_copy(&txf->src[idx].src, &tex->src[i].src);
+         nir_src_copy(&txf->src[idx].src, &tex->src[i].src, txf);
          txf->src[idx].src_type = tex->src[i].src_type;
          idx++;
       }
diff --git a/src/panfrost/midgard/midgard_errata_lod.c b/src/panfrost/midgard/midgard_errata_lod.c
index be5800ed2d39..7ceea1c23c6a 100644
--- a/src/panfrost/midgard/midgard_errata_lod.c
+++ b/src/panfrost/midgard/midgard_errata_lod.c
@@ -53,7 +53,7 @@ nir_lod_errata_instr(nir_builder *b, nir_instr *instr, void *data)
 
         /* TODO: Indirect samplers, separate sampler objects XXX */
         nir_src idx = nir_src_for_ssa(nir_imm_int(b, tex->texture_index));
-        nir_src_copy(&l->src[0], &idx);
+        nir_src_copy(&l->src[0], &idx, l);
 
         nir_builder_instr_insert(b, &l->instr);
         nir_ssa_def *params = &l->dest.ssa;
-- 
GitLab


From e57b7af2b25308012b209bdc2161bb05639651d5 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Wed, 8 Sep 2021 16:41:00 +0100
Subject: [PATCH 6/7] nir: adjust nir_src_copy signature to take a nir_instr *

This is almost always a nir_instr and updating the src of a nir_if will
have to work slightly differently in the future.

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 .../vulkan/radv_nir_lower_ycbcr_textures.c    |  2 +-
 src/compiler/glsl/glsl_to_nir.cpp             |  2 +-
 src/compiler/nir/nir.c                        | 20 ++++++++++++-------
 src/compiler/nir/nir.h                        |  2 +-
 src/compiler/nir/nir_builtin_builder.c        |  4 ++--
 src/compiler/nir/nir_deref.c                  |  4 ++--
 src/compiler/nir/nir_from_ssa.c               |  2 +-
 src/compiler/nir/nir_lower_atomics_to_ssbo.c  | 12 +++++------
 src/compiler/nir/nir_lower_indirect_derefs.c  |  2 +-
 src/compiler/nir/nir_lower_io.c               |  2 +-
 src/compiler/nir/nir_lower_io_to_scalar.c     |  4 ++--
 src/compiler/nir/nir_lower_locals_to_regs.c   |  2 +-
 src/compiler/nir/nir_lower_ssbo.c             |  8 ++++----
 src/compiler/nir/nir_lower_subgroups.c        | 10 +++++-----
 src/compiler/nir/nir_lower_tex.c              |  8 ++++----
 src/compiler/nir/nir_opt_peephole_select.c    |  4 ++--
 src/compiler/nir/nir_search.c                 |  3 +--
 .../lima/ir/lima_nir_split_load_input.c       |  2 +-
 .../vulkan/anv_nir_lower_ycbcr_textures.c     |  2 +-
 .../compiler/dxil_nir_lower_int_cubemaps.c    |  2 +-
 .../compiler/dxil_nir_lower_int_samplers.c    |  6 +++---
 src/panfrost/midgard/midgard_errata_lod.c     |  2 +-
 22 files changed, 55 insertions(+), 50 deletions(-)

diff --git a/src/amd/vulkan/radv_nir_lower_ycbcr_textures.c b/src/amd/vulkan/radv_nir_lower_ycbcr_textures.c
index 996cc36cc5b1..80c1f2f4d18d 100644
--- a/src/amd/vulkan/radv_nir_lower_ycbcr_textures.c
+++ b/src/amd/vulkan/radv_nir_lower_ycbcr_textures.c
@@ -132,7 +132,7 @@ create_plane_tex_instr_implicit(struct ycbcr_state *state, uint32_t plane)
          }
       FALLTHROUGH;
       default:
-         nir_src_copy(&tex->src[i].src, &old_tex->src[i].src, tex);
+         nir_src_copy(&tex->src[i].src, &old_tex->src[i].src, &tex->instr);
          break;
       }
    }
diff --git a/src/compiler/glsl/glsl_to_nir.cpp b/src/compiler/glsl/glsl_to_nir.cpp
index 0d833c80d585..3310f5dca079 100644
--- a/src/compiler/glsl/glsl_to_nir.cpp
+++ b/src/compiler/glsl/glsl_to_nir.cpp
@@ -1686,7 +1686,7 @@ nir_visitor::visit(ir_call *ir)
          nir_ssa_def *val = evaluate_rvalue(param_rvalue);
          nir_src src = nir_src_for_ssa(val);
 
-         nir_src_copy(&call->params[i], &src, call);
+         nir_src_copy(&call->params[i], &src, &call->instr);
       } else if (sig_param->data.mode == ir_var_function_inout) {
          unreachable("unimplemented: inout parameters");
       }
diff --git a/src/compiler/nir/nir.c b/src/compiler/nir/nir.c
index f3dc93620e3f..f1da1eb50d30 100644
--- a/src/compiler/nir/nir.c
+++ b/src/compiler/nir/nir.c
@@ -451,10 +451,8 @@ static void dest_free_indirects(nir_dest *dest)
    }
 }
 
-/* NOTE: if the instruction you are copying a src to is already added
- * to the IR, use nir_instr_rewrite_src() instead.
- */
-void nir_src_copy(nir_src *dest, const nir_src *src, void *mem_ctx)
+static void
+src_copy(nir_src *dest, const nir_src *src)
 {
    src_free_indirects(dest);
 
@@ -466,13 +464,21 @@ void nir_src_copy(nir_src *dest, const nir_src *src, void *mem_ctx)
       dest->reg.reg = src->reg.reg;
       if (src->reg.indirect) {
          dest->reg.indirect = calloc(1, sizeof(nir_src));
-         nir_src_copy(dest->reg.indirect, src->reg.indirect, mem_ctx);
+         src_copy(dest->reg.indirect, src->reg.indirect);
       } else {
          dest->reg.indirect = NULL;
       }
    }
 }
 
+/* NOTE: if the instruction you are copying a src to is already added
+ * to the IR, use nir_instr_rewrite_src() instead.
+ */
+void nir_src_copy(nir_src *dest, const nir_src *src, nir_instr *instr)
+{
+   src_copy(dest, src);
+}
+
 void nir_dest_copy(nir_dest *dest, const nir_dest *src, nir_instr *instr)
 {
    /* Copying an SSA definition makes no sense whatsoever. */
@@ -496,7 +502,7 @@ void
 nir_alu_src_copy(nir_alu_src *dest, const nir_alu_src *src,
                  nir_alu_instr *instr)
 {
-   nir_src_copy(&dest->src, &src->src, &instr->instr);
+   nir_src_copy(&dest->src, &src->src, instr ? &instr->instr : NULL);
    dest->abs = src->abs;
    dest->negate = src->negate;
    for (unsigned i = 0; i < NIR_MAX_VEC_COMPONENTS; i++)
@@ -1706,7 +1712,7 @@ nir_if_rewrite_condition(nir_if *if_stmt, nir_src new_src)
    assert(!src_is_valid(src) || src->parent_if == if_stmt);
 
    src_remove_all_uses(src);
-   nir_src_copy(src, &new_src, if_stmt);
+   src_copy(src, &new_src);
    src_add_all_uses(src, NULL, if_stmt);
 }
 
diff --git a/src/compiler/nir/nir.h b/src/compiler/nir/nir.h
index a3fef9a2bd5b..39f002b54fb3 100644
--- a/src/compiler/nir/nir.h
+++ b/src/compiler/nir/nir.h
@@ -1136,7 +1136,7 @@ nir_is_sequential_comp_swizzle(uint8_t *swiz, unsigned nr_comp)
    return true;
 }
 
-void nir_src_copy(nir_src *dest, const nir_src *src, void *instr_or_if);
+void nir_src_copy(nir_src *dest, const nir_src *src, nir_instr *instr);
 void nir_dest_copy(nir_dest *dest, const nir_dest *src, nir_instr *instr);
 
 typedef struct {
diff --git a/src/compiler/nir/nir_builtin_builder.c b/src/compiler/nir/nir_builtin_builder.c
index 26b524122238..bd11159f9fe9 100644
--- a/src/compiler/nir/nir_builtin_builder.c
+++ b/src/compiler/nir/nir_builtin_builder.c
@@ -366,7 +366,7 @@ nir_get_texture_size(nir_builder *b, nir_tex_instr *tex)
           tex->src[i].src_type == nir_tex_src_sampler_offset ||
           tex->src[i].src_type == nir_tex_src_texture_handle ||
           tex->src[i].src_type == nir_tex_src_sampler_handle) {
-         nir_src_copy(&txs->src[idx].src, &tex->src[i].src, txs);
+         nir_src_copy(&txs->src[idx].src, &tex->src[i].src, &txs->instr);
          txs->src[idx].src_type = tex->src[i].src_type;
          idx++;
       }
@@ -421,7 +421,7 @@ nir_get_texture_lod(nir_builder *b, nir_tex_instr *tex)
           tex->src[i].src_type == nir_tex_src_sampler_offset ||
           tex->src[i].src_type == nir_tex_src_texture_handle ||
           tex->src[i].src_type == nir_tex_src_sampler_handle) {
-         nir_src_copy(&tql->src[idx].src, &tex->src[i].src, tql);
+         nir_src_copy(&tql->src[idx].src, &tex->src[i].src, &tql->instr);
          tql->src[idx].src_type = tex->src[i].src_type;
          idx++;
       }
diff --git a/src/compiler/nir/nir_deref.c b/src/compiler/nir/nir_deref.c
index 9908f4998f13..2c9f8613aecf 100644
--- a/src/compiler/nir/nir_deref.c
+++ b/src/compiler/nir/nir_deref.c
@@ -747,7 +747,7 @@ rematerialize_deref_in_block(nir_deref_instr *deref,
          parent = rematerialize_deref_in_block(parent, state);
          new_deref->parent = nir_src_for_ssa(&parent->dest.ssa);
       } else {
-         nir_src_copy(&new_deref->parent, &deref->parent, new_deref);
+         nir_src_copy(&new_deref->parent, &deref->parent, &new_deref->instr);
       }
    }
 
@@ -764,7 +764,7 @@ rematerialize_deref_in_block(nir_deref_instr *deref,
    case nir_deref_type_array:
    case nir_deref_type_ptr_as_array:
       assert(!nir_src_as_deref(deref->arr.index));
-      nir_src_copy(&new_deref->arr.index, &deref->arr.index, new_deref);
+      nir_src_copy(&new_deref->arr.index, &deref->arr.index, &new_deref->instr);
       break;
 
    case nir_deref_type_struct:
diff --git a/src/compiler/nir/nir_from_ssa.c b/src/compiler/nir/nir_from_ssa.c
index 38dcc9a67031..c0feeeb76598 100644
--- a/src/compiler/nir/nir_from_ssa.c
+++ b/src/compiler/nir/nir_from_ssa.c
@@ -638,7 +638,7 @@ emit_copy(nir_builder *b, nir_src src, nir_src dest_src)
       assert(src.reg.reg->num_components >= dest_src.reg.reg->num_components);
 
    nir_alu_instr *mov = nir_alu_instr_create(b->shader, nir_op_mov);
-   nir_src_copy(&mov->src[0].src, &src, mov);
+   nir_src_copy(&mov->src[0].src, &src, &mov->instr);
    mov->dest.dest = nir_dest_for_reg(dest_src.reg.reg);
    mov->dest.write_mask = (1 << dest_src.reg.reg->num_components) - 1;
 
diff --git a/src/compiler/nir/nir_lower_atomics_to_ssbo.c b/src/compiler/nir/nir_lower_atomics_to_ssbo.c
index 621938fb0cf3..678418096096 100644
--- a/src/compiler/nir/nir_lower_atomics_to_ssbo.c
+++ b/src/compiler/nir/nir_lower_atomics_to_ssbo.c
@@ -122,7 +122,7 @@ lower_instr(nir_intrinsic_instr *instr, unsigned ssbo_offset, nir_builder *b, un
       /* remapped to ssbo_atomic_add: { buffer_idx, offset, +1 } */
       temp = nir_imm_int(b, +1);
       new_instr->src[0] = nir_src_for_ssa(buffer);
-      nir_src_copy(&new_instr->src[1], &instr->src[0], new_instr);
+      nir_src_copy(&new_instr->src[1], &instr->src[0], &new_instr->instr);
       new_instr->src[2] = nir_src_for_ssa(temp);
       break;
    case nir_intrinsic_atomic_counter_pre_dec:
@@ -131,22 +131,22 @@ lower_instr(nir_intrinsic_instr *instr, unsigned ssbo_offset, nir_builder *b, un
       /* NOTE semantic difference so we adjust the return value below */
       temp = nir_imm_int(b, -1);
       new_instr->src[0] = nir_src_for_ssa(buffer);
-      nir_src_copy(&new_instr->src[1], &instr->src[0], new_instr);
+      nir_src_copy(&new_instr->src[1], &instr->src[0], &new_instr->instr);
       new_instr->src[2] = nir_src_for_ssa(temp);
       break;
    case nir_intrinsic_atomic_counter_read:
       /* remapped to load_ssbo: { buffer_idx, offset } */
       new_instr->src[0] = nir_src_for_ssa(buffer);
-      nir_src_copy(&new_instr->src[1], &instr->src[0], new_instr);
+      nir_src_copy(&new_instr->src[1], &instr->src[0], &new_instr->instr);
       break;
    default:
       /* remapped to ssbo_atomic_x: { buffer_idx, offset, data, (compare)? } */
       new_instr->src[0] = nir_src_for_ssa(buffer);
-      nir_src_copy(&new_instr->src[1], &instr->src[0], new_instr);
-      nir_src_copy(&new_instr->src[2], &instr->src[1], new_instr);
+      nir_src_copy(&new_instr->src[1], &instr->src[0], &new_instr->instr);
+      nir_src_copy(&new_instr->src[2], &instr->src[1], &new_instr->instr);
       if (op == nir_intrinsic_ssbo_atomic_comp_swap ||
           op == nir_intrinsic_ssbo_atomic_fcomp_swap)
-         nir_src_copy(&new_instr->src[3], &instr->src[2], new_instr);
+         nir_src_copy(&new_instr->src[3], &instr->src[2], &new_instr->instr);
       break;
    }
 
diff --git a/src/compiler/nir/nir_lower_indirect_derefs.c b/src/compiler/nir/nir_lower_indirect_derefs.c
index e30ccad9ebc5..1ffb990ea1c9 100644
--- a/src/compiler/nir/nir_lower_indirect_derefs.c
+++ b/src/compiler/nir/nir_lower_indirect_derefs.c
@@ -98,7 +98,7 @@ emit_load_store_deref(nir_builder *b, nir_intrinsic_instr *orig_instr,
       /* Copy over any other sources.  This is needed for interp_deref_at */
       for (unsigned i = 1;
            i < nir_intrinsic_infos[orig_instr->intrinsic].num_srcs; i++)
-         nir_src_copy(&load->src[i], &orig_instr->src[i], load);
+         nir_src_copy(&load->src[i], &orig_instr->src[i], &load->instr);
 
       nir_ssa_dest_init(&load->instr, &load->dest,
                         orig_instr->dest.ssa.num_components,
diff --git a/src/compiler/nir/nir_lower_io.c b/src/compiler/nir/nir_lower_io.c
index bb568ed4975d..d07828b9efc5 100644
--- a/src/compiler/nir/nir_lower_io.c
+++ b/src/compiler/nir/nir_lower_io.c
@@ -606,7 +606,7 @@ lower_interpolate_at(nir_intrinsic_instr *intrin, struct lower_io_state *state,
    if (intrin->intrinsic == nir_intrinsic_interp_deref_at_sample ||
        intrin->intrinsic == nir_intrinsic_interp_deref_at_offset ||
        intrin->intrinsic == nir_intrinsic_interp_deref_at_vertex)
-      nir_src_copy(&bary_setup->src[0], &intrin->src[1], bary_setup);
+      nir_src_copy(&bary_setup->src[0], &intrin->src[1], &bary_setup->instr);
 
    nir_builder_instr_insert(b, &bary_setup->instr);
 
diff --git a/src/compiler/nir/nir_lower_io_to_scalar.c b/src/compiler/nir/nir_lower_io_to_scalar.c
index a4fa671976fc..52ca0a61f179 100644
--- a/src/compiler/nir/nir_lower_io_to_scalar.c
+++ b/src/compiler/nir/nir_lower_io_to_scalar.c
@@ -61,7 +61,7 @@ lower_load_input_to_scalar(nir_builder *b, nir_intrinsic_instr *intr)
       nir_intrinsic_set_dest_type(chan_intr, nir_intrinsic_dest_type(intr));
       set_io_semantics(chan_intr, intr, i);
       /* offset */
-      nir_src_copy(&chan_intr->src[0], &intr->src[0], chan_intr);
+      nir_src_copy(&chan_intr->src[0], &intr->src[0], &chan_intr->instr);
 
       nir_builder_instr_insert(b, &chan_intr->instr);
 
@@ -167,7 +167,7 @@ lower_store_output_to_scalar(nir_builder *b, nir_intrinsic_instr *intr)
       /* value */
       chan_intr->src[0] = nir_src_for_ssa(nir_channel(b, value, i));
       /* offset */
-      nir_src_copy(&chan_intr->src[1], &intr->src[1], chan_intr);
+      nir_src_copy(&chan_intr->src[1], &intr->src[1], &chan_intr->instr);
 
       nir_builder_instr_insert(b, &chan_intr->instr);
    }
diff --git a/src/compiler/nir/nir_lower_locals_to_regs.c b/src/compiler/nir/nir_lower_locals_to_regs.c
index d4d4c8899bf4..9860d5f10207 100644
--- a/src/compiler/nir/nir_lower_locals_to_regs.c
+++ b/src/compiler/nir/nir_lower_locals_to_regs.c
@@ -246,7 +246,7 @@ lower_locals_to_regs_block(nir_block *block,
 
          nir_alu_instr *mov = nir_alu_instr_create(b->shader, nir_op_mov);
 
-         nir_src_copy(&mov->src[0].src, &intrin->src[1], mov);
+         nir_src_copy(&mov->src[0].src, &intrin->src[1], &mov->instr);
 
          /* The normal NIR SSA copy propagate pass can't happen after this pass,
           * so do an ad-hoc copy propagate since this ALU op can do swizzles
diff --git a/src/compiler/nir/nir_lower_ssbo.c b/src/compiler/nir/nir_lower_ssbo.c
index 19a040a68d86..7535717b98de 100644
--- a/src/compiler/nir/nir_lower_ssbo.c
+++ b/src/compiler/nir/nir_lower_ssbo.c
@@ -90,7 +90,7 @@ nir_load_ssbo_prop(nir_builder *b, nir_intrinsic_op op,
 {
    nir_intrinsic_instr *load = nir_intrinsic_instr_create(b->shader, op);
    load->num_components = 1;
-   nir_src_copy(&load->src[0], idx, load);
+   nir_src_copy(&load->src[0], idx, &load->instr);
    nir_ssa_dest_init(&load->instr, &load->dest, 1, bitsize, NULL);
    nir_builder_instr_insert(b, &load->instr);
    return &load->dest.ssa;
@@ -134,7 +134,7 @@ lower_ssbo_instr(nir_builder *b, nir_intrinsic_instr *intr)
    }
 
    if (is_store) {
-      nir_src_copy(&global->src[0], &intr->src[0], global);
+      nir_src_copy(&global->src[0], &intr->src[0], &global->instr);
       nir_intrinsic_set_write_mask(global, nir_intrinsic_write_mask(intr));
    } else {
       nir_ssa_dest_init(&global->instr, &global->dest,
@@ -142,9 +142,9 @@ lower_ssbo_instr(nir_builder *b, nir_intrinsic_instr *intr)
                         intr->dest.ssa.bit_size, NULL);
 
       if (is_atomic) {
-         nir_src_copy(&global->src[1], &intr->src[2], global);
+         nir_src_copy(&global->src[1], &intr->src[2], &global->instr);
          if (nir_intrinsic_infos[op].num_srcs > 2)
-            nir_src_copy(&global->src[2], &intr->src[3], global);
+            nir_src_copy(&global->src[2], &intr->src[3], &global->instr);
       }
    }
 
diff --git a/src/compiler/nir/nir_lower_subgroups.c b/src/compiler/nir/nir_lower_subgroups.c
index 737663ec6892..bbd1550d8950 100644
--- a/src/compiler/nir/nir_lower_subgroups.c
+++ b/src/compiler/nir/nir_lower_subgroups.c
@@ -45,7 +45,7 @@ lower_subgroups_64bit_split_intrinsic(nir_builder *b, nir_intrinsic_instr *intri
    intr->const_index[1] = intrin->const_index[1];
    intr->src[0] = nir_src_for_ssa(comp);
    if (nir_intrinsic_infos[intrin->intrinsic].num_srcs == 2)
-      nir_src_copy(&intr->src[1], &intrin->src[1], intr);
+      nir_src_copy(&intr->src[1], &intrin->src[1], &intr->instr);
 
    intr->num_components = 1;
    nir_builder_instr_insert(b, &intr->instr);
@@ -126,7 +126,7 @@ lower_subgroup_op_to_scalar(nir_builder *b, nir_intrinsic_instr *intrin,
       /* invocation */
       if (nir_intrinsic_infos[intrin->intrinsic].num_srcs > 1) {
          assert(nir_intrinsic_infos[intrin->intrinsic].num_srcs == 2);
-         nir_src_copy(&chan_intrin->src[1], &intrin->src[1], chan_intrin);
+         nir_src_copy(&chan_intrin->src[1], &intrin->src[1], &chan_intrin->instr);
       }
 
       chan_intrin->const_index[0] = intrin->const_index[0];
@@ -209,7 +209,7 @@ lower_shuffle_to_swizzle(nir_builder *b, nir_intrinsic_instr *intrin,
    nir_intrinsic_instr *swizzle = nir_intrinsic_instr_create(
       b->shader, nir_intrinsic_masked_swizzle_amd);
    swizzle->num_components = intrin->num_components;
-   nir_src_copy(&swizzle->src[0], &intrin->src[0], swizzle);
+   nir_src_copy(&swizzle->src[0], &intrin->src[0], &swizzle->instr);
    nir_intrinsic_set_swizzle_mask(swizzle, (mask << 10) | 0x1f);
    nir_ssa_dest_init(&swizzle->instr, &swizzle->dest,
                      intrin->dest.ssa.num_components,
@@ -288,7 +288,7 @@ lower_to_shuffle(nir_builder *b, nir_intrinsic_instr *intrin,
    nir_intrinsic_instr *shuffle =
       nir_intrinsic_instr_create(b->shader, nir_intrinsic_shuffle);
    shuffle->num_components = intrin->num_components;
-   nir_src_copy(&shuffle->src[0], &intrin->src[0], shuffle);
+   nir_src_copy(&shuffle->src[0], &intrin->src[0], &shuffle->instr);
    shuffle->src[1] = nir_src_for_ssa(index);
    nir_ssa_dest_init(&shuffle->instr, &shuffle->dest,
                      intrin->dest.ssa.num_components,
@@ -568,7 +568,7 @@ lower_dynamic_quad_broadcast(nir_builder *b, nir_intrinsic_instr *intrin,
 
       qbcst->num_components = intrin->num_components;
       qbcst->src[1] = nir_src_for_ssa(nir_imm_int(b, i));
-      nir_src_copy(&qbcst->src[0], &intrin->src[0], qbcst);
+      nir_src_copy(&qbcst->src[0], &intrin->src[0], &qbcst->instr);
       nir_ssa_dest_init(&qbcst->instr, &qbcst->dest,
                         intrin->dest.ssa.num_components,
                         intrin->dest.ssa.bit_size, NULL);
diff --git a/src/compiler/nir/nir_lower_tex.c b/src/compiler/nir/nir_lower_tex.c
index 31da4cb2bd99..7c2aec458132 100644
--- a/src/compiler/nir/nir_lower_tex.c
+++ b/src/compiler/nir/nir_lower_tex.c
@@ -289,7 +289,7 @@ sample_plane(nir_builder *b, nir_tex_instr *tex, int plane,
    nir_tex_instr *plane_tex =
       nir_tex_instr_create(b->shader, tex->num_srcs + 1);
    for (unsigned i = 0; i < tex->num_srcs; i++) {
-      nir_src_copy(&plane_tex->src[i].src, &tex->src[i].src, plane_tex);
+      nir_src_copy(&plane_tex->src[i].src, &tex->src[i].src, &plane_tex->instr);
       plane_tex->src[i].src_type = tex->src[i].src_type;
    }
    plane_tex->src[tex->num_srcs].src = nir_src_for_ssa(nir_imm_int(b, plane));
@@ -773,7 +773,7 @@ lower_tex_to_txd(nir_builder *b, nir_tex_instr *tex)
 
    /* reuse existing srcs */
    for (unsigned i = 0; i < tex->num_srcs; i++) {
-      nir_src_copy(&txd->src[i].src, &tex->src[i].src, txd);
+      nir_src_copy(&txd->src[i].src, &tex->src[i].src, &txd->instr);
       txd->src[i].src_type = tex->src[i].src_type;
    }
    int coord = nir_tex_instr_src_index(tex, nir_tex_src_coord);
@@ -813,7 +813,7 @@ lower_txb_to_txl(nir_builder *b, nir_tex_instr *tex)
    /* reuse all but bias src */
    for (int i = 0; i < 2; i++) {
       if (tex->src[i].src_type != nir_tex_src_bias) {
-         nir_src_copy(&txl->src[i].src, &tex->src[i].src, txl);
+         nir_src_copy(&txl->src[i].src, &tex->src[i].src, &txl->instr);
          txl->src[i].src_type = tex->src[i].src_type;
       }
    }
@@ -1125,7 +1125,7 @@ lower_tg4_offsets(nir_builder *b, nir_tex_instr *tex)
       tex_copy->dest_type = tex->dest_type;
 
       for (unsigned j = 0; j < tex->num_srcs; ++j) {
-         nir_src_copy(&tex_copy->src[j].src, &tex->src[j].src, tex_copy);
+         nir_src_copy(&tex_copy->src[j].src, &tex->src[j].src, &tex_copy->instr);
          tex_copy->src[j].src_type = tex->src[j].src_type;
       }
 
diff --git a/src/compiler/nir/nir_opt_peephole_select.c b/src/compiler/nir/nir_opt_peephole_select.c
index cc5fb5463ef6..03d656dd8ebb 100644
--- a/src/compiler/nir/nir_opt_peephole_select.c
+++ b/src/compiler/nir/nir_opt_peephole_select.c
@@ -457,7 +457,7 @@ nir_opt_peephole_select_block(nir_block *block, nir_shader *shader,
 
       nir_phi_instr *phi = nir_instr_as_phi(instr);
       nir_alu_instr *sel = nir_alu_instr_create(shader, nir_op_bcsel);
-      nir_src_copy(&sel->src[0].src, &if_stmt->condition, sel);
+      nir_src_copy(&sel->src[0].src, &if_stmt->condition, &sel->instr);
       /* Splat the condition to all channels */
       memset(sel->src[0].swizzle, 0, sizeof sel->src[0].swizzle);
 
@@ -467,7 +467,7 @@ nir_opt_peephole_select_block(nir_block *block, nir_shader *shader,
          assert(src->src.is_ssa);
 
          unsigned idx = src->pred == then_block ? 1 : 2;
-         nir_src_copy(&sel->src[idx].src, &src->src, sel);
+         nir_src_copy(&sel->src[idx].src, &src->src, &sel->instr);
       }
 
       nir_ssa_dest_init(&sel->instr, &sel->dest.dest,
diff --git a/src/compiler/nir/nir_search.c b/src/compiler/nir/nir_search.c
index e5e00ecd13f8..31053751d848 100644
--- a/src/compiler/nir/nir_search.c
+++ b/src/compiler/nir/nir_search.c
@@ -526,8 +526,7 @@ construct_value(nir_builder *build,
       assert(state->variables_seen & (1 << var->variable));
 
       nir_alu_src val = { NIR_SRC_INIT };
-      nir_alu_src_copy(&val, &state->variables[var->variable],
-                       (void *)build->shader);
+      nir_alu_src_copy(&val, &state->variables[var->variable], NULL);
       assert(!var->is_constant);
 
       for (unsigned i = 0; i < NIR_MAX_VEC_COMPONENTS; i++)
diff --git a/src/gallium/drivers/lima/ir/lima_nir_split_load_input.c b/src/gallium/drivers/lima/ir/lima_nir_split_load_input.c
index 7506c0f90be5..802a083882f3 100644
--- a/src/gallium/drivers/lima/ir/lima_nir_split_load_input.c
+++ b/src/gallium/drivers/lima/ir/lima_nir_split_load_input.c
@@ -85,7 +85,7 @@ lima_nir_split_load_input_instr(nir_builder *b,
    nir_intrinsic_set_dest_type(new_intrin, nir_intrinsic_dest_type(intrin));
 
    /* offset */
-   nir_src_copy(&new_intrin->src[0], &intrin->src[0], new_intrin);
+   nir_src_copy(&new_intrin->src[0], &intrin->src[0], &new_intrin->instr);
 
    nir_builder_instr_insert(b, &new_intrin->instr);
    nir_ssa_def_rewrite_uses(&alu->dest.dest.ssa,
diff --git a/src/intel/vulkan/anv_nir_lower_ycbcr_textures.c b/src/intel/vulkan/anv_nir_lower_ycbcr_textures.c
index 58f81c208038..e82cd032e20d 100644
--- a/src/intel/vulkan/anv_nir_lower_ycbcr_textures.c
+++ b/src/intel/vulkan/anv_nir_lower_ycbcr_textures.c
@@ -135,7 +135,7 @@ create_plane_tex_instr_implicit(struct ycbcr_state *state,
          }
          FALLTHROUGH;
       default:
-         nir_src_copy(&tex->src[i].src, &old_tex->src[i].src, tex);
+         nir_src_copy(&tex->src[i].src, &old_tex->src[i].src, &tex->instr);
          break;
       }
    }
diff --git a/src/microsoft/compiler/dxil_nir_lower_int_cubemaps.c b/src/microsoft/compiler/dxil_nir_lower_int_cubemaps.c
index e315faf8d494..b2d0efcde4db 100644
--- a/src/microsoft/compiler/dxil_nir_lower_int_cubemaps.c
+++ b/src/microsoft/compiler/dxil_nir_lower_int_cubemaps.c
@@ -210,7 +210,7 @@ create_array_tex_from_cube_tex(nir_builder *b, nir_tex_instr *tex, nir_ssa_def *
       nir_src *psrc = (tex->src[i].src_type == nir_tex_src_coord) ?
                          &coord_src : &tex->src[i].src;
 
-      nir_src_copy(&array_tex->src[i].src, psrc, array_tex);
+      nir_src_copy(&array_tex->src[i].src, psrc, &array_tex->instr);
       array_tex->src[i].src_type = tex->src[i].src_type;
    }
 
diff --git a/src/microsoft/compiler/dxil_nir_lower_int_samplers.c b/src/microsoft/compiler/dxil_nir_lower_int_samplers.c
index 4546a79a1cf0..2faae5a07d21 100644
--- a/src/microsoft/compiler/dxil_nir_lower_int_samplers.c
+++ b/src/microsoft/compiler/dxil_nir_lower_int_samplers.c
@@ -81,7 +81,7 @@ dx_get_texture_lod(nir_builder *b, nir_tex_instr *tex)
    nir_ssa_def *ssa_src = nir_channels(b, tex->src[coord_index].src.ssa,
                                        (1 << coord_components) - 1);
    nir_src src = nir_src_for_ssa(ssa_src);
-   nir_src_copy(&tql->src[0].src, &src, tql);
+   nir_src_copy(&tql->src[0].src, &src, &tql->instr);
    tql->src[0].src_type = nir_tex_src_coord;
 
    unsigned idx = 1;
@@ -92,7 +92,7 @@ dx_get_texture_lod(nir_builder *b, nir_tex_instr *tex)
           tex->src[i].src_type == nir_tex_src_sampler_offset ||
           tex->src[i].src_type == nir_tex_src_texture_handle ||
           tex->src[i].src_type == nir_tex_src_sampler_handle) {
-         nir_src_copy(&tql->src[idx].src, &tex->src[i].src, tql);
+         nir_src_copy(&tql->src[idx].src, &tex->src[i].src, &tql->instr);
          tql->src[idx].src_type = tex->src[i].src_type;
          idx++;
       }
@@ -278,7 +278,7 @@ create_txf_from_tex(nir_builder *b, nir_tex_instr *tex)
       if (tex->src[i].src_type == nir_tex_src_texture_deref ||
           tex->src[i].src_type == nir_tex_src_texture_offset ||
           tex->src[i].src_type == nir_tex_src_texture_handle) {
-         nir_src_copy(&txf->src[idx].src, &tex->src[i].src, txf);
+         nir_src_copy(&txf->src[idx].src, &tex->src[i].src, &txf->instr);
          txf->src[idx].src_type = tex->src[i].src_type;
          idx++;
       }
diff --git a/src/panfrost/midgard/midgard_errata_lod.c b/src/panfrost/midgard/midgard_errata_lod.c
index 7ceea1c23c6a..395d8c1b3888 100644
--- a/src/panfrost/midgard/midgard_errata_lod.c
+++ b/src/panfrost/midgard/midgard_errata_lod.c
@@ -53,7 +53,7 @@ nir_lod_errata_instr(nir_builder *b, nir_instr *instr, void *data)
 
         /* TODO: Indirect samplers, separate sampler objects XXX */
         nir_src idx = nir_src_for_ssa(nir_imm_int(b, tex->texture_index));
-        nir_src_copy(&l->src[0], &idx, l);
+        nir_src_copy(&l->src[0], &idx, &l->instr);
 
         nir_builder_instr_insert(b, &l->instr);
         nir_ssa_def *params = &l->dest.ssa;
-- 
GitLab


From bc88f8e265e3e4a6119fc6e8dbcf2cfc6b5a99d2 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Wed, 8 Sep 2021 15:24:10 +0100
Subject: [PATCH 7/7] nir: use a GC context for instructions

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/compiler/nir/nir.c                      | 94 +++++++--------------
 src/compiler/nir/nir.h                      |  6 +-
 src/compiler/nir/nir_clone.c                | 10 +--
 src/compiler/nir/nir_control_flow.c         |  2 +-
 src/compiler/nir/nir_lower_locals_to_regs.c |  2 +-
 src/compiler/nir/nir_serialize.c            |  4 +-
 src/compiler/nir/nir_sweep.c                | 50 ++++++++---
 src/compiler/nir/nir_validate.c             | 14 ---
 8 files changed, 76 insertions(+), 106 deletions(-)

diff --git a/src/compiler/nir/nir.c b/src/compiler/nir/nir.c
index f1da1eb50d30..b657ed8a8975 100644
--- a/src/compiler/nir/nir.c
+++ b/src/compiler/nir/nir.c
@@ -51,8 +51,6 @@ static const struct debug_named_value nir_debug_control[] = {
      "Disable shader validation at each successful lowering/optimization call" },
    { "validate_ssa_dominance", NIR_DEBUG_VALIDATE_SSA_DOMINANCE,
      "Validate SSA dominance in shader at each successful lowering/optimization call" },
-   { "validate_gc_list", NIR_DEBUG_VALIDATE_GC_LIST,
-     "Validate the instruction GC list at each successful lowering/optimization call" },
    { "tgsi", NIR_DEBUG_TGSI,
      "Dump NIR/TGSI shaders when doing a NIR<->TGSI translation" },
    { "print", NIR_DEBUG_PRINT,
@@ -184,17 +182,6 @@ nir_component_mask_reinterpret(nir_component_mask_t mask,
    return new_mask;
 }
 
-static void
-nir_shader_destructor(void *ptr)
-{
-   nir_shader *shader = ptr;
-
-   /* Free all instrs from the shader, since they're not ralloced. */
-   list_for_each_entry_safe(nir_instr, instr, &shader->gc_list, gc_node) {
-      nir_instr_free(instr);
-   }
-}
-
 nir_shader *
 nir_shader_create(void *mem_ctx,
                   gl_shader_stage stage,
@@ -202,7 +189,8 @@ nir_shader_create(void *mem_ctx,
                   shader_info *si)
 {
    nir_shader *shader = rzalloc(mem_ctx, nir_shader);
-   ralloc_set_destructor(shader, nir_shader_destructor);
+
+   shader->gctx = gc_context(shader);
 
 #ifndef NDEBUG
    nir_process_debug_variable();
@@ -221,8 +209,6 @@ nir_shader_create(void *mem_ctx,
 
    exec_list_make_empty(&shader->functions);
 
-   list_inithead(&shader->gc_list);
-
    shader->num_inputs = 0;
    shader->num_outputs = 0;
    shader->num_uniforms = 0;
@@ -437,7 +423,7 @@ static void src_free_indirects(nir_src *src)
 {
    if (src_has_indirect(src)) {
       assert(src->reg.indirect->is_ssa || !src->reg.indirect->reg.indirect);
-      free(src->reg.indirect);
+      gc_free(src->reg.indirect);
       src->reg.indirect = NULL;
    }
 }
@@ -446,13 +432,13 @@ static void dest_free_indirects(nir_dest *dest)
 {
    if (!dest->is_ssa && dest->reg.indirect) {
       assert(dest->reg.indirect->is_ssa || !dest->reg.indirect->reg.indirect);
-      free(dest->reg.indirect);
+      gc_free(dest->reg.indirect);
       dest->reg.indirect = NULL;
    }
 }
 
 static void
-src_copy(nir_src *dest, const nir_src *src)
+src_copy(nir_src *dest, const nir_src *src, gc_ctx *ctx)
 {
    src_free_indirects(dest);
 
@@ -463,8 +449,8 @@ src_copy(nir_src *dest, const nir_src *src)
       dest->reg.base_offset = src->reg.base_offset;
       dest->reg.reg = src->reg.reg;
       if (src->reg.indirect) {
-         dest->reg.indirect = calloc(1, sizeof(nir_src));
-         src_copy(dest->reg.indirect, src->reg.indirect);
+         dest->reg.indirect = gc_zalloc(ctx, nir_src, 1);
+         src_copy(dest->reg.indirect, src->reg.indirect, ctx);
       } else {
          dest->reg.indirect = NULL;
       }
@@ -476,7 +462,7 @@ src_copy(nir_src *dest, const nir_src *src)
  */
 void nir_src_copy(nir_src *dest, const nir_src *src, nir_instr *instr)
 {
-   src_copy(dest, src);
+   src_copy(dest, src, instr ? gc_get_context(instr) : NULL);
 }
 
 void nir_dest_copy(nir_dest *dest, const nir_dest *src, nir_instr *instr)
@@ -491,7 +477,7 @@ void nir_dest_copy(nir_dest *dest, const nir_dest *src, nir_instr *instr)
    dest->reg.base_offset = src->reg.base_offset;
    dest->reg.reg = src->reg.reg;
    if (src->reg.indirect) {
-      dest->reg.indirect = calloc(1, sizeof(nir_src));
+      dest->reg.indirect = gc_zalloc(gc_get_context(instr), nir_src, 1);
       nir_src_copy(dest->reg.indirect, src->reg.indirect, instr);
    } else {
       dest->reg.indirect = NULL;
@@ -703,7 +689,7 @@ nir_alu_instr_create(nir_shader *shader, nir_op op)
 {
    unsigned num_srcs = nir_op_infos[op].num_inputs;
    /* TODO: don't use calloc */
-   nir_alu_instr *instr = calloc(1, sizeof(nir_alu_instr) + num_srcs * sizeof(nir_alu_src));
+   nir_alu_instr *instr = gc_zalloc_zla(shader->gctx, nir_alu_instr, nir_alu_src, num_srcs);
 
    instr_init(&instr->instr, nir_instr_type_alu);
    instr->op = op;
@@ -711,15 +697,13 @@ nir_alu_instr_create(nir_shader *shader, nir_op op)
    for (unsigned i = 0; i < num_srcs; i++)
       alu_src_init(&instr->src[i]);
 
-   list_add(&instr->instr.gc_node, &shader->gc_list);
-
    return instr;
 }
 
 nir_deref_instr *
 nir_deref_instr_create(nir_shader *shader, nir_deref_type deref_type)
 {
-   nir_deref_instr *instr = calloc(1, sizeof(*instr));
+   nir_deref_instr *instr = gc_zalloc(shader->gctx, nir_deref_instr, 1);
 
    instr_init(&instr->instr, nir_instr_type_deref);
 
@@ -733,23 +717,19 @@ nir_deref_instr_create(nir_shader *shader, nir_deref_type deref_type)
 
    dest_init(&instr->dest);
 
-   list_add(&instr->instr.gc_node, &shader->gc_list);
-
    return instr;
 }
 
 nir_jump_instr *
 nir_jump_instr_create(nir_shader *shader, nir_jump_type type)
 {
-   nir_jump_instr *instr = malloc(sizeof(*instr));
+   nir_jump_instr *instr = gc_alloc(shader->gctx, nir_jump_instr, 1);
    instr_init(&instr->instr, nir_instr_type_jump);
    src_init(&instr->condition);
    instr->type = type;
    instr->target = NULL;
    instr->else_target = NULL;
 
-   list_add(&instr->instr.gc_node, &shader->gc_list);
-
    return instr;
 }
 
@@ -758,13 +738,11 @@ nir_load_const_instr_create(nir_shader *shader, unsigned num_components,
                             unsigned bit_size)
 {
    nir_load_const_instr *instr =
-      calloc(1, sizeof(*instr) + num_components * sizeof(*instr->value));
+      gc_zalloc_zla(shader->gctx, nir_load_const_instr, nir_const_value, num_components);
    instr_init(&instr->instr, nir_instr_type_load_const);
 
    nir_ssa_def_init(&instr->instr, &instr->def, num_components, bit_size);
 
-   list_add(&instr->instr.gc_node, &shader->gc_list);
-
    return instr;
 }
 
@@ -774,7 +752,7 @@ nir_intrinsic_instr_create(nir_shader *shader, nir_intrinsic_op op)
    unsigned num_srcs = nir_intrinsic_infos[op].num_srcs;
    /* TODO: don't use calloc */
    nir_intrinsic_instr *instr =
-      calloc(1, sizeof(nir_intrinsic_instr) + num_srcs * sizeof(nir_src));
+      gc_zalloc_zla(shader->gctx, nir_intrinsic_instr, nir_src, num_srcs);
 
    instr_init(&instr->instr, nir_instr_type_intrinsic);
    instr->intrinsic = op;
@@ -785,8 +763,6 @@ nir_intrinsic_instr_create(nir_shader *shader, nir_intrinsic_op op)
    for (unsigned i = 0; i < num_srcs; i++)
       src_init(&instr->src[i]);
 
-   list_add(&instr->instr.gc_node, &shader->gc_list);
-
    return instr;
 }
 
@@ -795,7 +771,7 @@ nir_call_instr_create(nir_shader *shader, nir_function *callee)
 {
    const unsigned num_params = callee->num_params;
    nir_call_instr *instr =
-      calloc(1, sizeof(*instr) + num_params * sizeof(instr->params[0]));
+      gc_zalloc_zla(shader->gctx, nir_call_instr, nir_src, num_params);
 
    instr_init(&instr->instr, nir_instr_type_call);
    instr->callee = callee;
@@ -803,8 +779,6 @@ nir_call_instr_create(nir_shader *shader, nir_function *callee)
    for (unsigned i = 0; i < num_params; i++)
       src_init(&instr->params[i]);
 
-   list_add(&instr->instr.gc_node, &shader->gc_list);
-
    return instr;
 }
 
@@ -819,13 +793,13 @@ static int8_t default_tg4_offsets[4][2] =
 nir_tex_instr *
 nir_tex_instr_create(nir_shader *shader, unsigned num_srcs)
 {
-   nir_tex_instr *instr = calloc(1, sizeof(*instr));
+   nir_tex_instr *instr = gc_zalloc(shader->gctx, nir_tex_instr, 1);
    instr_init(&instr->instr, nir_instr_type_tex);
 
    dest_init(&instr->dest);
 
    instr->num_srcs = num_srcs;
-   instr->src = malloc(sizeof(nir_tex_src) * num_srcs);
+   instr->src = gc_alloc(shader->gctx, nir_tex_src, num_srcs);
    for (unsigned i = 0; i < num_srcs; i++)
       src_init(&instr->src[i].src);
 
@@ -833,8 +807,6 @@ nir_tex_instr_create(nir_shader *shader, unsigned num_srcs)
    instr->sampler_index = 0;
    memcpy(instr->tg4_offsets, default_tg4_offsets, sizeof(instr->tg4_offsets));
 
-   list_add(&instr->instr.gc_node, &shader->gc_list);
-
    return instr;
 }
 
@@ -843,8 +815,7 @@ nir_tex_instr_add_src(nir_tex_instr *tex,
                       nir_tex_src_type src_type,
                       nir_src src)
 {
-   nir_tex_src *new_srcs = calloc(sizeof(*new_srcs),
-                                         tex->num_srcs + 1);
+   nir_tex_src *new_srcs = gc_zalloc(gc_get_context(tex), nir_tex_src, tex->num_srcs + 1);
 
    for (unsigned i = 0; i < tex->num_srcs; i++) {
       new_srcs[i].src_type = tex->src[i].src_type;
@@ -852,7 +823,7 @@ nir_tex_instr_add_src(nir_tex_instr *tex,
                          &tex->src[i].src);
    }
 
-   free(tex->src);
+   gc_free(tex->src);
    tex->src = new_srcs;
 
    tex->src[tex->num_srcs].src_type = src_type;
@@ -888,14 +859,12 @@ nir_tex_instr_has_explicit_tg4_offsets(nir_tex_instr *tex)
 nir_phi_instr *
 nir_phi_instr_create(nir_shader *shader)
 {
-   nir_phi_instr *instr = malloc(sizeof(*instr));
+   nir_phi_instr *instr = gc_alloc(shader->gctx, nir_phi_instr, 1);
    instr_init(&instr->instr, nir_instr_type_phi);
 
    dest_init(&instr->dest);
    exec_list_make_empty(&instr->srcs);
 
-   list_add(&instr->instr.gc_node, &shader->gc_list);
-
    return instr;
 }
 
@@ -912,7 +881,7 @@ nir_phi_instr_add_src(nir_phi_instr *instr, nir_block *pred, nir_src src)
 {
    nir_phi_src *phi_src;
 
-   phi_src = calloc(1, sizeof(nir_phi_src));
+   phi_src = gc_zalloc(gc_get_context(instr), nir_phi_src, 1);
    phi_src->pred = pred;
    phi_src->src = src;
    phi_src->src.parent_instr = &instr->instr;
@@ -924,13 +893,11 @@ nir_phi_instr_add_src(nir_phi_instr *instr, nir_block *pred, nir_src src)
 nir_parallel_copy_instr *
 nir_parallel_copy_instr_create(nir_shader *shader)
 {
-   nir_parallel_copy_instr *instr = malloc(sizeof(*instr));
+   nir_parallel_copy_instr *instr = gc_alloc(shader->gctx, nir_parallel_copy_instr, 1);
    instr_init(&instr->instr, nir_instr_type_parallel_copy);
 
    exec_list_make_empty(&instr->entries);
 
-   list_add(&instr->instr.gc_node, &shader->gc_list);
-
    return instr;
 }
 
@@ -939,13 +906,11 @@ nir_ssa_undef_instr_create(nir_shader *shader,
                            unsigned num_components,
                            unsigned bit_size)
 {
-   nir_ssa_undef_instr *instr = malloc(sizeof(*instr));
+   nir_ssa_undef_instr *instr = gc_alloc(shader->gctx, nir_ssa_undef_instr, 1);
    instr_init(&instr->instr, nir_instr_type_ssa_undef);
 
    nir_ssa_def_init(&instr->instr, &instr->def, num_components, bit_size);
 
-   list_add(&instr->instr.gc_node, &shader->gc_list);
-
    return instr;
 }
 
@@ -1264,14 +1229,13 @@ void nir_instr_free(nir_instr *instr)
 
    switch (instr->type) {
    case nir_instr_type_tex:
-      free(nir_instr_as_tex(instr)->src);
+      gc_free(nir_instr_as_tex(instr)->src);
       break;
 
    case nir_instr_type_phi: {
       nir_phi_instr *phi = nir_instr_as_phi(instr);
-      nir_foreach_phi_src_safe(phi_src, phi) {
-         free(phi_src);
-      }
+      nir_foreach_phi_src_safe(phi_src, phi)
+         gc_free(phi_src);
       break;
    }
 
@@ -1279,8 +1243,7 @@ void nir_instr_free(nir_instr *instr)
       break;
    }
 
-   list_del(&instr->gc_node);
-   free(instr);
+   gc_free(instr);
 }
 
 void
@@ -1708,11 +1671,12 @@ nir_instr_move_src(nir_instr *dest_instr, nir_src *dest, nir_src *src)
 void
 nir_if_rewrite_condition(nir_if *if_stmt, nir_src new_src)
 {
+   nir_shader *shader = ralloc_parent(if_stmt);
    nir_src *src = &if_stmt->condition;
    assert(!src_is_valid(src) || src->parent_if == if_stmt);
 
    src_remove_all_uses(src);
-   src_copy(src, &new_src);
+   src_copy(src, &new_src, shader->gctx);
    src_add_all_uses(src, NULL, if_stmt);
 }
 
diff --git a/src/compiler/nir/nir.h b/src/compiler/nir/nir.h
index 39f002b54fb3..b2d882c2bdc4 100644
--- a/src/compiler/nir/nir.h
+++ b/src/compiler/nir/nir.h
@@ -91,7 +91,6 @@ extern bool nir_debug_print_shader[MESA_SHADER_KERNEL + 1];
 #define NIR_DEBUG_PRINT_CBS              (1u << 18)
 #define NIR_DEBUG_PRINT_KS               (1u << 19)
 #define NIR_DEBUG_PRINT_CONSTS           (1u << 20)
-#define NIR_DEBUG_VALIDATE_GC_LIST       (1u << 21)
 
 #define NIR_DEBUG_PRINT (NIR_DEBUG_PRINT_VS  | \
                          NIR_DEBUG_PRINT_TCS | \
@@ -877,7 +876,6 @@ typedef enum PACKED {
 
 typedef struct nir_instr {
    struct exec_node node;
-   struct list_head gc_node;
    struct nir_block *block;
    nir_instr_type type;
 
@@ -3661,6 +3659,8 @@ typedef struct nir_shader_compiler_options {
 } nir_shader_compiler_options;
 
 typedef struct nir_shader {
+   gc_ctx *gctx;
+
    /** list of uniforms (nir_variable) */
    struct exec_list variables;
 
@@ -3676,8 +3676,6 @@ typedef struct nir_shader {
 
    struct exec_list functions; /** < list of nir_function */
 
-   struct list_head gc_list; /** < list of all nir_instrs allocated on the shader but not yet freed. */
-
    /**
     * The size of the variable space for load_input_*, load_uniform_*, etc.
     * intrinsics.  This is in back-end specific units which is likely one of
diff --git a/src/compiler/nir/nir_clone.c b/src/compiler/nir/nir_clone.c
index ee509da06e25..86ab29437e7d 100644
--- a/src/compiler/nir/nir_clone.c
+++ b/src/compiler/nir/nir_clone.c
@@ -244,7 +244,7 @@ __clone_src(clone_state *state, void *ninstr_or_if,
    } else {
       nsrc->reg.reg = remap_reg(state, src->reg.reg);
       if (src->reg.indirect) {
-         nsrc->reg.indirect = malloc(sizeof(nir_src));
+         nsrc->reg.indirect = gc_alloc(state->ns->gctx, nir_src, 1);
          __clone_src(state, ninstr_or_if, nsrc->reg.indirect, src->reg.indirect);
       }
       nsrc->reg.base_offset = src->reg.base_offset;
@@ -264,7 +264,7 @@ __clone_dst(clone_state *state, nir_instr *ninstr,
    } else {
       ndst->reg.reg = remap_reg(state, dst->reg.reg);
       if (dst->reg.indirect) {
-         ndst->reg.indirect = malloc(sizeof(nir_src));
+         ndst->reg.indirect = gc_alloc(state->ns->gctx, nir_src, 1);
          __clone_src(state, ninstr, ndst->reg.indirect, dst->reg.indirect);
       }
       ndst->reg.base_offset = dst->reg.base_offset;
@@ -814,10 +814,6 @@ nir_shader_replace(nir_shader *dst, nir_shader *src)
    ralloc_adopt(dead_ctx, dst);
    ralloc_free(dead_ctx);
 
-   list_for_each_entry_safe(nir_instr, instr, &dst->gc_list, gc_node) {
-      nir_instr_free(instr);
-   }
-
    /* Re-parent all of src's ralloc children to dst */
    ralloc_adopt(dst, src);
 
@@ -826,8 +822,6 @@ nir_shader_replace(nir_shader *dst, nir_shader *src)
    /* We have to move all the linked lists over separately because we need the
     * pointers in the list elements to point to the lists in dst and not src.
     */
-   list_replace(&src->gc_list, &dst->gc_list);
-   list_inithead(&src->gc_list);
    exec_list_move_nodes_to(&src->variables, &dst->variables);
 
    /* Now move the functions over.  This takes a tiny bit more work */
diff --git a/src/compiler/nir/nir_control_flow.c b/src/compiler/nir/nir_control_flow.c
index 842723ab049e..4973c60de5c7 100644
--- a/src/compiler/nir/nir_control_flow.c
+++ b/src/compiler/nir/nir_control_flow.c
@@ -441,7 +441,7 @@ remove_phi_src(nir_block *block, nir_block *pred)
          if (src->pred == pred) {
             list_del(&src->src.use_link);
             exec_node_remove(&src->node);
-            free(src);
+            gc_free(src);
          }
       }
    }
diff --git a/src/compiler/nir/nir_lower_locals_to_regs.c b/src/compiler/nir/nir_lower_locals_to_regs.c
index 9860d5f10207..b260fda3f436 100644
--- a/src/compiler/nir/nir_lower_locals_to_regs.c
+++ b/src/compiler/nir/nir_lower_locals_to_regs.c
@@ -159,7 +159,7 @@ get_deref_reg_src(nir_deref_instr *deref, struct locals_to_regs_state *state)
          if (src.reg.indirect) {
             assert(src.reg.base_offset == 0);
          } else {
-            src.reg.indirect = malloc(sizeof(nir_src));
+            src.reg.indirect = gc_alloc(gc_get_context(deref), nir_src, 1);
             *src.reg.indirect =
                nir_src_for_ssa(nir_imm_int(b, src.reg.base_offset));
             src.reg.base_offset = 0;
diff --git a/src/compiler/nir/nir_serialize.c b/src/compiler/nir/nir_serialize.c
index 215a4160fc6c..d729dc6067de 100644
--- a/src/compiler/nir/nir_serialize.c
+++ b/src/compiler/nir/nir_serialize.c
@@ -554,7 +554,7 @@ read_src(read_ctx *ctx, nir_src *src)
       src->reg.reg = read_lookup_object(ctx, header.any.object_idx);
       src->reg.base_offset = blob_read_uint32(ctx->blob);
       if (header.any.is_indirect) {
-         src->reg.indirect = malloc(sizeof(nir_src));
+         src->reg.indirect = gc_alloc(ctx->nir->gctx, nir_src, 1);
          read_src(ctx, src->reg.indirect);
       } else {
          src->reg.indirect = NULL;
@@ -775,7 +775,7 @@ read_dest(read_ctx *ctx, nir_dest *dst, nir_instr *instr,
       dst->reg.reg = read_object(ctx);
       dst->reg.base_offset = blob_read_uint32(ctx->blob);
       if (dest.reg.is_indirect) {
-         dst->reg.indirect = malloc(sizeof(nir_src));
+         dst->reg.indirect = gc_alloc(ctx->nir->gctx, nir_src, 1);
          read_src(ctx, dst->reg.indirect);
       }
    }
diff --git a/src/compiler/nir/nir_sweep.c b/src/compiler/nir/nir_sweep.c
index 8be959b1b6e7..ca3ec1dbf603 100644
--- a/src/compiler/nir/nir_sweep.c
+++ b/src/compiler/nir/nir_sweep.c
@@ -40,6 +40,24 @@
 
 static void sweep_cf_node(nir_shader *nir, nir_cf_node *cf_node);
 
+static bool
+sweep_src_indirect(nir_src *src, void *nir)
+{
+   if (!src->is_ssa && src->reg.indirect)
+      gc_mark_live(((nir_shader*)nir)->gctx, src->reg.indirect);
+
+   return true;
+}
+
+static bool
+sweep_dest_indirect(nir_dest *dest, void *nir)
+{
+   if (!dest->is_ssa && dest->reg.indirect)
+      gc_mark_live(((nir_shader*)nir)->gctx, dest->reg.indirect);
+
+   return true;
+}
+
 static void
 sweep_block(nir_shader *nir, nir_block *block)
 {
@@ -55,8 +73,22 @@ sweep_block(nir_shader *nir, nir_block *block)
    block->live_out = NULL;
 
    nir_foreach_instr(instr, block) {
-      list_del(&instr->gc_node);
-      list_add(&instr->gc_node, &nir->gc_list);
+      gc_mark_live(nir->gctx, instr);
+
+      switch (instr->type) {
+      case nir_instr_type_tex:
+         gc_mark_live(nir->gctx, nir_instr_as_tex(instr)->src);
+         break;
+      case nir_instr_type_phi:
+         nir_foreach_phi_src(src, nir_instr_as_phi(instr))
+            gc_mark_live(nir->gctx, src);
+         break;
+      default:
+         break;
+      }
+
+      nir_foreach_src(instr, sweep_src_indirect, nir);
+      nir_foreach_dest(instr, sweep_dest_indirect, nir);
    }
 }
 
@@ -138,12 +170,13 @@ nir_sweep(nir_shader *nir)
    struct list_head instr_gc_list;
    list_inithead(&instr_gc_list);
 
-   list_replace(&nir->gc_list, &instr_gc_list);
-   list_inithead(&nir->gc_list);
-
    /* First, move ownership of all the memory to a temporary context; assume dead. */
    ralloc_adopt(rubbish, nir);
 
+   /* Start sweeping */
+   gc_sweep_start(nir->gctx);
+
+   ralloc_steal(nir, nir->gctx);
    ralloc_steal(nir, (char *)nir->info.name);
    if (nir->info.label)
       ralloc_steal(nir, (char *)nir->info.label);
@@ -156,12 +189,6 @@ nir_sweep(nir_shader *nir)
       sweep_function(nir, func);
    }
 
-   /* Sweep instrs not found while walking the shader. */
-   list_for_each_entry_safe(nir_instr, instr, &instr_gc_list, gc_node) {
-      nir_instr_free(instr);
-   }
-   assert(list_is_empty(&instr_gc_list));
-
    ralloc_steal(nir, nir->constant_data);
    ralloc_steal(nir, nir->xfb_info);
    ralloc_steal(nir, nir->printf_info);
@@ -171,5 +198,6 @@ nir_sweep(nir_shader *nir)
    }
 
    /* Free everything we didn't steal back. */
+   gc_sweep_end(nir->gctx);
    ralloc_free(rubbish);
 }
diff --git a/src/compiler/nir/nir_validate.c b/src/compiler/nir/nir_validate.c
index 3216e6ca066b..e7fa19c8b0b4 100644
--- a/src/compiler/nir/nir_validate.c
+++ b/src/compiler/nir/nir_validate.c
@@ -101,8 +101,6 @@ typedef struct {
 
    /* map of instruction/var/etc to failed assert string */
    struct hash_table *errors;
-
-   struct set *shader_gc_list;
 } validate_state;
 
 static void
@@ -1086,9 +1084,6 @@ validate_instr(nir_instr *instr, validate_state *state)
 
    state->instr = instr;
 
-   if (state->shader_gc_list)
-      validate_assert(state, _mesa_set_search(state->shader_gc_list, instr));
-
    switch (instr->type) {
    case nir_instr_type_alu:
       validate_alu_instr(nir_instr_as_alu(instr), state);
@@ -1697,8 +1692,6 @@ init_validate_state(validate_state *state)
    state->blocks = _mesa_pointer_set_create(state->mem_ctx);
    state->var_defs = _mesa_pointer_hash_table_create(state->mem_ctx);
    state->errors = _mesa_pointer_hash_table_create(state->mem_ctx);
-   state->shader_gc_list = NIR_DEBUG(VALIDATE_GC_LIST) ?
-                           _mesa_pointer_set_create(state->mem_ctx) : NULL;
 
    state->loop = NULL;
    state->instr = NULL;
@@ -1755,13 +1748,6 @@ nir_validate_shader(nir_shader *shader, const char *when)
    validate_state state;
    init_validate_state(&state);
 
-   if (state.shader_gc_list) {
-      list_for_each_entry(nir_instr, instr, &shader->gc_list, gc_node) {
-         if (instr->node.prev || instr->node.next)
-            _mesa_set_add(state.shader_gc_list, instr);
-      }
-   }
-
    state.shader = shader;
 
    nir_variable_mode valid_modes =
-- 
GitLab

