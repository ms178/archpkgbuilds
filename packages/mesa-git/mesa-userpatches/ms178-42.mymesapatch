--- a/src/amd/vulkan/radv_pipeline_cache.c	2026-01-08 21:43:17.881698849 +0100
+++ b/src/amd/vulkan/radv_pipeline_cache.c	2026-01-14 21:43:42.762829339 +0100
@@ -5,14 +5,17 @@
  */
 
 #include "radv_pipeline_cache.h"
+
 #include "util/disk_cache.h"
 #include "util/macros.h"
 #include "util/mesa-blake3.h"
 #include "util/mesa-sha1.h"
 #include "util/u_atomic.h"
 #include "util/u_debug.h"
+
 #include "nir.h"
 #include "nir_serialize.h"
+
 #include "radv_debug.h"
 #include "radv_descriptor_set.h"
 #include "radv_pipeline.h"
@@ -21,11 +24,19 @@
 #include "radv_pipeline_graphics.h"
 #include "radv_pipeline_rt.h"
 #include "radv_shader.h"
+
 #include "vk_pipeline.h"
 #include "vk_util.h"
 
 #include "aco_interface.h"
 
+#include <assert.h>
+#include <stdbool.h>
+#include <stddef.h>
+#include <stdint.h>
+#include <stdlib.h>
+#include <string.h>
+
 void
 radv_hash_graphics_spirv_to_nir(blake3_hash hash, const struct radv_shader_stage *stage,
                                 const struct radv_spirv_to_nir_options *options)
@@ -68,14 +79,34 @@ radv_shader_deserialize(struct radv_devi
 {
    const struct radv_shader_binary *binary = blob_read_bytes(blob, sizeof(struct radv_shader_binary));
 
-   struct radv_shader *shader;
+   if (unlikely(!binary))
+      return NULL;
+
+   /* Prevent underflow and obviously invalid binaries. */
+   if (unlikely(binary->total_size < sizeof(struct radv_shader_binary)))
+      return NULL;
+
+   /* Ensure the blob contains the full binary before passing the pointer onward.
+    * blob_read_bytes() advanced blob->current by sizeof(struct radv_shader_binary) already.
+    */
+   const size_t remaining = (size_t)(blob->end - blob->current);
+   const size_t needed = (size_t)binary->total_size - sizeof(struct radv_shader_binary);
+   if (unlikely(needed > remaining))
+      return NULL;
+
+   struct radv_shader *shader = NULL;
    radv_shader_create_uncached(device, binary, false, NULL, &shader);
-   if (!shader)
+
+   /* Advance past the rest of the shader binary regardless of success, to keep the reader state
+    * consistent if the caller attempts to continue parsing.
+    */
+   blob_skip_bytes(blob, (unsigned)needed);
+
+   if (unlikely(!shader))
       return NULL;
 
    assert(key_size == sizeof(shader->hash));
    memcpy(shader->hash, key_data, key_size);
-   blob_skip_bytes(blob, binary->total_size - sizeof(struct radv_shader_binary));
 
    return shader;
 }
@@ -85,9 +116,8 @@ radv_shader_cache_deserialize(struct vk_
                               struct blob_reader *blob)
 {
    struct radv_device *device = container_of(cache->base.device, struct radv_device, vk);
-   struct radv_shader *shader;
 
-   shader = radv_shader_deserialize(device, key_data, key_size, blob);
+   struct radv_shader *shader = radv_shader_deserialize(device, key_data, key_size, blob);
 
    return shader ? &shader->base : NULL;
 }
@@ -95,11 +125,13 @@ radv_shader_cache_deserialize(struct vk_
 void
 radv_shader_serialize(struct radv_shader *shader, struct blob *blob)
 {
-   size_t stats_size = shader->statistics ? sizeof(struct amd_stats) : 0;
-   size_t code_size = shader->code_size;
-   uint32_t total_size = sizeof(struct radv_shader_binary_legacy) + code_size + stats_size;
+   const size_t stats_size = shader->statistics ? sizeof(struct amd_stats) : 0;
+   const size_t code_size = shader->code_size;
+
+   /* Legacy format stores sizes as 32-bit. Shader code sizes are bounded well below 4 GiB. */
+   const uint32_t total_size = (uint32_t)(sizeof(struct radv_shader_binary_legacy) + code_size + stats_size);
 
-   struct radv_shader_binary_legacy binary = {
+   const struct radv_shader_binary_legacy binary = {
       .base =
          {
             .type = RADV_BINARY_TYPE_LEGACY,
@@ -107,11 +139,11 @@ radv_shader_serialize(struct radv_shader
             .info = shader->info,
             .total_size = total_size,
          },
-      .code_size = code_size,
+      .code_size = (uint32_t)code_size,
       .exec_size = shader->exec_size,
       .ir_size = 0,
       .disasm_size = 0,
-      .stats_size = stats_size,
+      .stats_size = (uint32_t)stats_size,
    };
 
    blob_write_bytes(blob, &binary, sizeof(struct radv_shader_binary_legacy));
@@ -131,21 +163,24 @@ radv_shader_cache_serialize(struct vk_pi
 static bool
 radv_is_cache_disabled(const struct radv_device *device, const struct vk_pipeline_cache *cache)
 {
-   const struct radv_physical_device *pdev = radv_device_physical(device);
-   const struct radv_instance *instance = radv_physical_device_instance(pdev);
-
    /* The buffer address used for debug printf is hardcoded. */
-   if (device->printf.buffer_addr)
+   if (unlikely(device->printf.buffer_addr))
       return true;
 
    /* The buffer address used for validating VAs is hardcoded. */
-   if (device->valid_vas_addr)
+   if (unlikely(device->valid_vas_addr))
       return true;
 
+   const struct radv_physical_device *pdev = radv_device_physical(device);
+
    /* Pipeline caches can be disabled with RADV_DEBUG=nocache, with MESA_GLSL_CACHE_DISABLE=1 and
     * when ACO_DEBUG is used. MESA_GLSL_CACHE_DISABLE is done elsewhere.
     */
-   if ((instance->debug_flags & RADV_DEBUG_NO_CACHE) || (pdev->use_llvm ? 0 : aco_get_codegen_flags()))
+   if (unlikely(!pdev->use_llvm && aco_get_codegen_flags()))
+      return true;
+
+   const struct radv_instance *instance = radv_physical_device_instance(pdev);
+   if (unlikely(instance->debug_flags & RADV_DEBUG_NO_CACHE))
       return true;
 
    if (!cache) {
@@ -165,7 +200,7 @@ radv_shader_create(struct radv_device *d
                    bool skip_cache)
 {
    if (radv_is_cache_disabled(device, cache) || skip_cache) {
-      struct radv_shader *shader;
+      struct radv_shader *shader = NULL;
       radv_shader_create_uncached(device, binary, false, NULL, &shader);
       return shader;
    }
@@ -176,9 +211,8 @@ radv_shader_create(struct radv_device *d
    blake3_hash hash;
    _mesa_blake3_compute(binary, binary->total_size, hash);
 
-   struct vk_pipeline_cache_object *shader_obj;
-   shader_obj = vk_pipeline_cache_create_and_insert_object(cache, hash, sizeof(hash), binary, binary->total_size,
-                                                           &radv_shader_ops);
+   struct vk_pipeline_cache_object *shader_obj =
+      vk_pipeline_cache_create_and_insert_object(cache, hash, sizeof(hash), binary, binary->total_size, &radv_shader_ops);
 
    return shader_obj ? container_of(shader_obj, struct radv_shader, base) : NULL;
 }
@@ -203,8 +237,23 @@ const struct vk_pipeline_cache_object_op
 static struct radv_pipeline_cache_object *
 radv_pipeline_cache_object_create(struct vk_device *device, unsigned num_shaders, const void *hash, unsigned data_size)
 {
-   const size_t size =
-      sizeof(struct radv_pipeline_cache_object) + (num_shaders * sizeof(struct radv_shader *)) + data_size;
+   /* Overflow-safe size computation for:
+    * sizeof(struct radv_pipeline_cache_object) + num_shaders * sizeof(ptr) + data_size
+    */
+   const size_t base_size = sizeof(struct radv_pipeline_cache_object);
+   const size_t shaders_size = (size_t)num_shaders * sizeof(struct radv_shader *);
+
+   if (unlikely(num_shaders != 0 && shaders_size / sizeof(struct radv_shader *) != (size_t)num_shaders))
+      return NULL;
+
+   if (unlikely(shaders_size > SIZE_MAX - base_size))
+      return NULL;
+
+   const size_t size_without_data = base_size + shaders_size;
+   if (unlikely((size_t)data_size > SIZE_MAX - size_without_data))
+      return NULL;
+
+   const size_t size = size_without_data + (size_t)data_size;
 
    struct radv_pipeline_cache_object *object = vk_alloc(&device->alloc, size, 8, VK_SYSTEM_ALLOCATION_SCOPE_CACHE);
    if (!object)
@@ -215,8 +264,10 @@ radv_pipeline_cache_object_create(struct
    object->data = &object->shaders[num_shaders];
    object->data_size = data_size;
    memcpy(object->sha1, hash, SHA1_DIGEST_LENGTH);
+
    memset(object->shaders, 0, sizeof(object->shaders[0]) * num_shaders);
-   memset(object->data, 0, data_size);
+   if (data_size)
+      memset(object->data, 0, data_size);
 
    return object;
 }
@@ -241,13 +292,31 @@ radv_pipeline_cache_object_deserialize(s
                                        struct blob_reader *blob)
 {
    struct radv_device *device = container_of(cache->base.device, struct radv_device, vk);
+
    assert(key_size == SHA1_DIGEST_LENGTH);
-   unsigned total_size = blob->end - blob->current;
-   unsigned num_shaders = blob_read_uint32(blob);
-   unsigned data_size = blob_read_uint32(blob);
 
-   struct radv_pipeline_cache_object *object;
-   object = radv_pipeline_cache_object_create(&device->vk, num_shaders, key_data, data_size);
+   const unsigned total_size = (unsigned)(blob->end - blob->current);
+   const unsigned num_shaders = blob_read_uint32(blob);
+   const unsigned data_size = blob_read_uint32(blob);
+
+   if (unlikely(blob->overrun))
+      return NULL;
+
+   /* Make sure the blob contains enough bytes for:
+    *   num_shaders * sizeof(blake3_hash) + data_size
+    */
+   const size_t remaining = (size_t)(blob->end - blob->current);
+   if (unlikely(num_shaders > (SIZE_MAX / sizeof(blake3_hash))))
+      return NULL;
+
+   const size_t needed_hashes = (size_t)num_shaders * sizeof(blake3_hash);
+   const size_t needed_total = needed_hashes + (size_t)data_size;
+
+   if (unlikely(needed_total > remaining))
+      return NULL;
+
+   struct radv_pipeline_cache_object *object =
+      radv_pipeline_cache_object_create(&device->vk, num_shaders, key_data, data_size);
    if (!object)
       return NULL;
 
@@ -255,6 +324,11 @@ radv_pipeline_cache_object_deserialize(s
 
    for (unsigned i = 0; i < num_shaders; i++) {
       const uint8_t *hash = blob_read_bytes(blob, sizeof(blake3_hash));
+      if (unlikely(!hash)) {
+         vk_pipeline_cache_object_unref(&device->vk, &object->base);
+         return NULL;
+      }
+
       struct vk_pipeline_cache_object *shader =
          vk_pipeline_cache_lookup_object(cache, hash, sizeof(blake3_hash), &radv_shader_ops, NULL);
 
@@ -270,6 +344,10 @@ radv_pipeline_cache_object_deserialize(s
    }
 
    blob_copy_bytes(blob, object->data, data_size);
+   if (unlikely(blob->overrun)) {
+      vk_pipeline_cache_object_unref(&device->vk, &object->base);
+      return NULL;
+   }
 
    return &object->base;
 }
@@ -299,14 +377,14 @@ const struct vk_pipeline_cache_object_op
 static void
 radv_report_pso_cache_stats(struct radv_device *device, const struct radv_pipeline *pipeline, bool cache_hit)
 {
+   /* Only gather PSO cache stats for application pipelines. */
+   if (pipeline->is_internal)
+      return;
+
    const struct radv_physical_device *pdev = radv_device_physical(device);
    const struct radv_instance *instance = radv_physical_device_instance(pdev);
 
-   if (!(instance->debug_flags & RADV_DEBUG_PSO_CACHE_STATS))
-      return;
-
-   /* Only gather PSO cache stats for application pipelines. */
-   if (pipeline->is_internal)
+   if (likely(!(instance->debug_flags & RADV_DEBUG_PSO_CACHE_STATS)))
       return;
 
    assert(pipeline->type < ARRAY_SIZE(device->pso_cache_stats));
@@ -533,6 +611,11 @@ radv_ray_tracing_pipeline_cache_insert(s
 
    struct radv_pipeline_cache_object *pipeline_obj =
       radv_pipeline_cache_object_create(&device->vk, num_shaders, pipeline->base.base.sha1, data_size);
+
+   /* OOM safety: avoid dereferencing a NULL pipeline_obj. */
+   if (unlikely(!pipeline_obj))
+      return;
+
    struct radv_ray_tracing_pipeline_cache_data *data = pipeline_obj->data;
    struct radv_ray_tracing_group_cache_data *group_data = (void *)&data->stages[num_stages];
 
@@ -667,7 +750,9 @@ radv_pipeline_cache_get_binaries(struct
                                  uint32_t *num_binaries, bool *found_in_internal_cache)
 {
    struct vk_pipeline_cache *cache = device->mem_cache;
-   VkResult result;
+
+   /* Ensure defined return value even if no binaries are produced (e.g. corrupt cache object). */
+   VkResult result = VK_SUCCESS;
 
    *found_in_internal_cache = false;
 
@@ -719,7 +804,7 @@ radv_pipeline_cache_get_binaries(struct
          if (nir)
             vk_pipeline_cache_object_unref(&device->vk, nir);
 
-         if (result != VK_SUCCESS)
+         if (unlikely(result != VK_SUCCESS))
             goto fail;
       }
 
@@ -727,7 +812,7 @@ radv_pipeline_cache_get_binaries(struct
          result = radv_create_pipeline_binary_from_rt_shader(device, pAllocator, traversal_shader, true,
                                                              traversal_shader->hash, NULL, 0, NULL, pipeline_binaries,
                                                              num_binaries);
-         if (result != VK_SUCCESS)
+         if (unlikely(result != VK_SUCCESS))
             goto fail;
       }
    } else {
@@ -743,7 +828,7 @@ radv_pipeline_cache_get_binaries(struct
          } else {
             result =
                radv_create_pipeline_binary_from_shader(device, pAllocator, shader, pipeline_binaries, num_binaries);
-            if (result != VK_SUCCESS)
+            if (unlikely(result != VK_SUCCESS))
                goto fail;
          }
       }
@@ -751,7 +836,7 @@ radv_pipeline_cache_get_binaries(struct
       if (gs_copy_shader) {
          result = radv_create_pipeline_binary_from_shader(device, pAllocator, gs_copy_shader, pipeline_binaries,
                                                           num_binaries);
-         if (result != VK_SUCCESS)
+         if (unlikely(result != VK_SUCCESS))
             goto fail;
       }
    }
