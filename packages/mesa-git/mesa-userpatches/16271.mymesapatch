From b79c01491eb2d5fb7216cd661a31844e8670c5ef Mon Sep 17 00:00:00 2001
From: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
Date: Sun, 10 Jul 2022 18:04:12 +0900
Subject: [PATCH 1/5] radv: Add a radeon_cmdbuf variant for CP DMA copy helper.

For use in shader uploads. We want to avoid radv_cmd_buffer due to the
heavyweight state initialization as well as more verbose object management.
---
 src/amd/vulkan/radv_private.h  |  2 ++
 src/amd/vulkan/si_cmd_buffer.c | 36 ++++++++++++++++++++++++++++++++++
 2 files changed, 38 insertions(+)

diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index 5ab7de9daa9d..8a307a57cc9f 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -1718,6 +1718,8 @@ void si_emit_set_predication_state(struct radv_cmd_buffer *cmd_buffer, bool draw
                                    unsigned pred_op, uint64_t va);
 void si_cp_dma_buffer_copy(struct radv_cmd_buffer *cmd_buffer, uint64_t src_va, uint64_t dest_va,
                            uint64_t size);
+void si_cs_cp_dma_buffer_copy_internal(struct radv_device *device, struct radeon_cmdbuf *cs,
+                                       uint64_t src_va, uint64_t dest_va, uint64_t size);
 void si_cs_cp_dma_prefetch(const struct radv_device *device, struct radeon_cmdbuf *cs, uint64_t va,
                            unsigned size, bool predicating);
 void si_cp_dma_prefetch(struct radv_cmd_buffer *cmd_buffer, uint64_t va, unsigned size);
diff --git a/src/amd/vulkan/si_cmd_buffer.c b/src/amd/vulkan/si_cmd_buffer.c
index 9d8724ab7e15..d4fa5c32acfd 100644
--- a/src/amd/vulkan/si_cmd_buffer.c
+++ b/src/amd/vulkan/si_cmd_buffer.c
@@ -1883,6 +1883,42 @@ si_cp_dma_buffer_copy(struct radv_cmd_buffer *cmd_buffer, uint64_t src_va, uint6
       si_cp_dma_realign_engine(cmd_buffer, realign_size);
 }
 
+/* A simplified version of si_cp_dma_buffer_copy for internal copies.
+ * Differences:
+ * - src_va, dest_va, size must be aligned to SI_CPDMA_ALIGNMENT.
+ * - Automatically syncs after the copy. Manual syncs tend to be more error-prone with radeon_cmdbuf
+ *   which does not track states, and we don't have any internal use cases yet where explicit sync
+ *   would benefit performance.
+ */
+void
+si_cs_cp_dma_buffer_copy_internal(struct radv_device *device, struct radeon_cmdbuf *cs,
+                                  uint64_t src_va, uint64_t dest_va, uint64_t size)
+{
+   enum amd_gfx_level gfx_level = device->physical_device->rad_info.gfx_level;
+
+   assert(size % SI_CPDMA_ALIGNMENT == 0 && src_va % SI_CPDMA_ALIGNMENT == 0 &&
+          dest_va % SI_CPDMA_ALIGNMENT == 0);
+
+   while (size) {
+      unsigned dma_flags = 0;
+      unsigned byte_count = MIN2(size, cp_dma_max_byte_count(gfx_level));
+
+      if (device->physical_device->rad_info.gfx_level >= GFX9) {
+         /* See si_cp_dma_buffer_copy above for reason to use L2. */
+         dma_flags |= CP_DMA_USE_L2;
+      }
+
+      if (byte_count == size)
+         dma_flags |= CP_DMA_SYNC;
+
+      si_cs_emit_cp_dma(device, cs, false, dest_va, src_va, byte_count, dma_flags);
+
+      size -= byte_count;
+      src_va += byte_count;
+      dest_va += byte_count;
+   }
+}
+
 void
 si_cp_dma_clear_buffer(struct radv_cmd_buffer *cmd_buffer, uint64_t va, uint64_t size,
                        unsigned value)
-- 
GitLab


From 92b70eceae8bb2468dcfcfb87ce4fbee552ff95c Mon Sep 17 00:00:00 2001
From: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
Date: Sun, 10 Jul 2022 22:36:07 +0900
Subject: [PATCH 2/5] radv: Make radv_upload_shaders return bool instead of
 VkResult.

In a following patch we're going to call radv_queue_internal_submit which
returns a bool that can map to multiple VkResult.

The error codes were synthetic and the caller didn't care about the kind of
the error anyway, so simplify it to return a boolean instead.
---
 src/amd/vulkan/radv_pipeline.c       | 10 +++++-----
 src/amd/vulkan/radv_pipeline_cache.c |  5 ++---
 src/amd/vulkan/radv_private.h        |  2 +-
 3 files changed, 8 insertions(+), 9 deletions(-)

diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index 8e1f1a461578..43b464e322ef 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -2944,7 +2944,7 @@ non_uniform_access_callback(const nir_src *src, void *_)
 }
 
 
-VkResult
+bool
 radv_upload_shaders(struct radv_device *device, struct radv_pipeline *pipeline,
                     struct radv_shader_binary **binaries, struct radv_shader_binary *gs_copy_binary)
 {
@@ -2969,7 +2969,7 @@ radv_upload_shaders(struct radv_device *device, struct radv_pipeline *pipeline,
    /* Allocate memory for all shader binaries. */
    pipeline->slab = radv_pipeline_slab_create(device, pipeline, code_size);
    if (!pipeline->slab)
-      return VK_ERROR_OUT_OF_DEVICE_MEMORY;
+      return false;
 
    pipeline->slab_bo = pipeline->slab->alloc->arena->bo;
 
@@ -2990,7 +2990,7 @@ radv_upload_shaders(struct radv_device *device, struct radv_pipeline *pipeline,
 
       void *dest_ptr = slab_ptr + slab_offset;
       if (!radv_shader_binary_upload(device, binaries[i], shader, dest_ptr))
-         return VK_ERROR_OUT_OF_HOST_MEMORY;
+         return false;
 
       slab_offset += align(shader->code_size, RADV_SHADER_ALLOC_ALIGNMENT);
    }
@@ -3000,10 +3000,10 @@ radv_upload_shaders(struct radv_device *device, struct radv_pipeline *pipeline,
 
       void *dest_ptr = slab_ptr + slab_offset;
       if (!radv_shader_binary_upload(device, gs_copy_binary, pipeline->gs_copy_shader, dest_ptr))
-         return VK_ERROR_OUT_OF_HOST_MEMORY;
+         return false;
    }
 
-   return VK_SUCCESS;
+   return true;
 }
 
 static bool
diff --git a/src/amd/vulkan/radv_pipeline_cache.c b/src/amd/vulkan/radv_pipeline_cache.c
index 14acb697713c..9c6597097a08 100644
--- a/src/amd/vulkan/radv_pipeline_cache.c
+++ b/src/amd/vulkan/radv_pipeline_cache.c
@@ -324,7 +324,6 @@ radv_create_shaders_from_pipeline_cache(
    uint32_t *num_stack_sizes, bool *found_in_application_cache)
 {
    struct cache_entry *entry;
-   VkResult result;
 
    if (!cache) {
       cache = device->mem_cache;
@@ -404,7 +403,7 @@ radv_create_shaders_from_pipeline_cache(
    }
 
    if (needs_upload) {
-      result = radv_upload_shaders(device, pipeline, binaries, gs_copy_binary);
+      bool upload_result = radv_upload_shaders(device, pipeline, binaries, gs_copy_binary);
 
       for (int i = 0; i < MESA_VULKAN_SHADER_STAGES; ++i) {
          if (pipeline->shaders[i])
@@ -412,7 +411,7 @@ radv_create_shaders_from_pipeline_cache(
       }
       free(gs_copy_binary);
 
-      if (result != VK_SUCCESS) {
+      if (!upload_result) {
          radv_pipeline_cache_unlock(cache);
          return false;
       }
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index 8a307a57cc9f..b8fc71408813 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -409,7 +409,7 @@ void radv_pipeline_cache_insert_shaders(
    struct radv_pipeline *pipeline, struct radv_shader_binary *const *binaries,
    const struct radv_pipeline_shader_stack_size *stack_sizes, uint32_t num_stack_sizes);
 
-VkResult radv_upload_shaders(struct radv_device *device, struct radv_pipeline *pipeline,
+bool radv_upload_shaders(struct radv_device *device, struct radv_pipeline *pipeline,
                              struct radv_shader_binary **binaries,
                              struct radv_shader_binary *gs_copy_binary);
 
-- 
GitLab


From 61b826c1391d68fa94cf4df4f31999c124eb5615 Mon Sep 17 00:00:00 2001
From: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
Date: Tue, 12 Jul 2022 17:21:15 +0900
Subject: [PATCH 3/5] radv: Upload shaders to invisible VRAM on small BAR
 systems.

Following PAL's implementation, this patch avoids allocating shader code
buffers in BAR and use compute DMA to upload them to invisible VRAM
directly.

For some games like HZD, shaders can take as much as 400MB, which exceeds
the non-resizable BAR size (256MB) and cause inconsistent spilling
behavior. The kernel will normally move these to invisible VRAM on its own,
but there are a few cases that it does not reliably happen. This patch does
the moving explicitly in the driver to ensure predictable results.

In this patch, we upload the shaders synchronously; so the shader will be
ready as soon as vkCreate*Pipeline returns. A following patch will make
this asynchronous and don't block until we see a use of the pipeline.

As a side effect, when SQTT is used we now store the shaders on a cacheable
buffer which would speed up writing the trace to the disk.
---
 docs/envvars.rst                |   2 +
 src/amd/vulkan/radv_constants.h |   2 +
 src/amd/vulkan/radv_debug.h     |   1 +
 src/amd/vulkan/radv_device.c    |  21 ++-
 src/amd/vulkan/radv_pipeline.c  |  52 +++++--
 src/amd/vulkan/radv_private.h   |  20 +++
 src/amd/vulkan/radv_shader.c    | 256 +++++++++++++++++++++++++++++---
 src/amd/vulkan/radv_shader.h    |  14 +-
 8 files changed, 324 insertions(+), 44 deletions(-)

diff --git a/docs/envvars.rst b/docs/envvars.rst
index 26186cc789b7..438db6a2344b 100644
--- a/docs/envvars.rst
+++ b/docs/envvars.rst
@@ -1061,6 +1061,8 @@ RADV driver environment variables
       enable wave32 for compute shaders (GFX10+)
    ``dccmsaa``
       enable DCC for MSAA images
+   ``dmashaders``
+      upload shaders to invisible VRAM (might be useful for non-resizable BAR systems)
    ``emulate_rt``
       forces ray-tracing to be emulated in software on GFX10_3+ and enables
       rt extensions with older hardware.
diff --git a/src/amd/vulkan/radv_constants.h b/src/amd/vulkan/radv_constants.h
index 2cb211a7b4ae..ee1aef1867e8 100644
--- a/src/amd/vulkan/radv_constants.h
+++ b/src/amd/vulkan/radv_constants.h
@@ -137,6 +137,8 @@
 #define PERF_CTR_BO_LOCK_OFFSET  0
 #define PERF_CTR_BO_FENCE_OFFSET 8
 
+#define RADV_SHADER_UPLOAD_CS_COUNT 16
+
 /* NGG GDS counters:
  *   offset  0| 4| 8|12  - reserved for NGG streamout counters
  *   offset 16           - pipeline statistics counter for all streams
diff --git a/src/amd/vulkan/radv_debug.h b/src/amd/vulkan/radv_debug.h
index 8e062908ba09..234f4fbfc283 100644
--- a/src/amd/vulkan/radv_debug.h
+++ b/src/amd/vulkan/radv_debug.h
@@ -85,6 +85,7 @@ enum {
    RADV_PERFTEST_RT_WAVE_64 = 1u << 12,
    RADV_PERFTEST_GPL = 1u << 13,
    RADV_PERFTEST_NGG_STREAMOUT = 1u << 14,
+   RADV_PERFTEST_DMA_SHADERS = 1u << 15,
 };
 
 bool radv_init_trace(struct radv_device *device);
diff --git a/src/amd/vulkan/radv_device.c b/src/amd/vulkan/radv_device.c
index dd2bb5d4aedc..ab7282cd4ba6 100644
--- a/src/amd/vulkan/radv_device.c
+++ b/src/amd/vulkan/radv_device.c
@@ -50,6 +50,8 @@
 #include "radv_private.h"
 #include "radv_shader.h"
 #include "vk_util.h"
+#include "vk_common_entrypoints.h"
+#include "vk_semaphore.h"
 #ifdef _WIN32
 typedef void *drmDevicePtr;
 #include <io.h>
@@ -1110,6 +1112,7 @@ static const struct debug_control radv_perftest_options[] = {{"localbos", RADV_P
                                                              {"rtwave64", RADV_PERFTEST_RT_WAVE_64},
                                                              {"gpl", RADV_PERFTEST_GPL},
                                                              {"ngg_streamout", RADV_PERFTEST_NGG_STREAMOUT},
+                                                             {"dmashaders", RADV_PERFTEST_DMA_SHADERS},
                                                              {NULL, 0}};
 
 const char *
@@ -3861,8 +3864,6 @@ radv_CreateDevice(VkPhysicalDevice physicalDevice, const VkDeviceCreateInfo *pCr
    device->primitives_generated_query = primitives_generated_query;
    device->uses_device_generated_commands = use_dgc;
 
-   radv_init_shader_arenas(device);
-
    device->overallocation_disallowed = overallocation_disallowed;
    mtx_init(&device->overallocation_mutex, mtx_plain);
 
@@ -3878,7 +3879,7 @@ radv_CreateDevice(VkPhysicalDevice physicalDevice, const VkDeviceCreateInfo *pCr
 
       result = device->ws->ctx_create(device->ws, priority, &device->hw_ctx[priority]);
       if (result != VK_SUCCESS)
-         goto fail;
+         goto fail_queue;
    }
 
    for (unsigned i = 0; i < pCreateInfo->queueCreateInfoCount; i++) {
@@ -3892,7 +3893,7 @@ radv_CreateDevice(VkPhysicalDevice physicalDevice, const VkDeviceCreateInfo *pCr
                   VK_SYSTEM_ALLOCATION_SCOPE_DEVICE);
       if (!device->queues[qfi]) {
          result = VK_ERROR_OUT_OF_HOST_MEMORY;
-         goto fail;
+         goto fail_queue;
       }
 
       memset(device->queues[qfi], 0, queue_create->queueCount * sizeof(struct radv_queue));
@@ -3902,11 +3903,19 @@ radv_CreateDevice(VkPhysicalDevice physicalDevice, const VkDeviceCreateInfo *pCr
       for (unsigned q = 0; q < queue_create->queueCount; q++) {
          result = radv_queue_init(device, &device->queues[qfi][q], q, queue_create, global_priority);
          if (result != VK_SUCCESS)
-            goto fail;
+            goto fail_queue;
       }
    }
    device->private_sdma_queue = VK_NULL_HANDLE;
 
+   device->shader_use_invisible_vram =
+      (device->instance->perftest_flags & RADV_PERFTEST_DMA_SHADERS) &&
+      /* dmashaders requires ACE queue for uploads. */
+      !(device->instance->debug_flags & RADV_DEBUG_NO_COMPUTE_QUEUE);
+   device->keep_shader_staging_buf = radv_thread_trace_enabled();
+   if ((result = radv_init_shader_arenas(device)) != VK_SUCCESS)
+      goto fail;
+
    device->pbb_allowed = device->physical_device->rad_info.gfx_level >= GFX9 &&
                          !(device->instance->debug_flags & RADV_DEBUG_NOBINNING);
 
@@ -4152,6 +4161,8 @@ fail:
    radv_device_finish_ps_epilogs(device);
    radv_device_finish_border_color(device);
 
+   radv_destroy_shader_arenas(device);
+fail_queue:
    for (unsigned i = 0; i < RADV_MAX_QUEUE_FAMILIES; i++) {
       for (unsigned q = 0; q < device->queue_count[i]; q++)
          radv_queue_finish(&device->queues[i][q]);
diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index 43b464e322ef..ae2cbb869bc7 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -198,6 +198,9 @@ radv_pipeline_destroy(struct radv_device *device, struct radv_pipeline *pipeline
    if (pipeline->gs_copy_shader)
       radv_shader_unref(device, pipeline->gs_copy_shader);
 
+   if (pipeline->shader_upload_buf)
+      free(pipeline->shader_upload_buf);
+
    if (pipeline->cs.buf)
       free(pipeline->cs.buf);
 
@@ -2949,6 +2952,7 @@ radv_upload_shaders(struct radv_device *device, struct radv_pipeline *pipeline,
                     struct radv_shader_binary **binaries, struct radv_shader_binary *gs_copy_binary)
 {
    uint32_t code_size = 0;
+   char *staging = NULL;
 
    /* Compute the total code size. */
    for (int i = 0; i < MESA_VULKAN_SHADER_STAGES; i++) {
@@ -2969,14 +2973,16 @@ radv_upload_shaders(struct radv_device *device, struct radv_pipeline *pipeline,
    /* Allocate memory for all shader binaries. */
    pipeline->slab = radv_pipeline_slab_create(device, pipeline, code_size);
    if (!pipeline->slab)
-      return false;
+      goto fail;
+
+   staging = calloc(1, code_size);
 
    pipeline->slab_bo = pipeline->slab->alloc->arena->bo;
 
    /* Upload shader binaries. */
-   uint64_t slab_va = radv_buffer_get_va(pipeline->slab_bo);
-   uint32_t slab_offset = pipeline->slab->alloc->offset;
-   char *slab_ptr = pipeline->slab->alloc->arena->ptr;
+   uint64_t slab_va = radv_buffer_get_va(pipeline->slab_bo) + pipeline->slab->alloc->offset;
+   uint32_t offset = 0;
+   char *slab_ptr = pipeline->slab->alloc->arena->ptr + pipeline->slab->alloc->offset;
 
    for (int i = 0; i < MESA_VULKAN_SHADER_STAGES; ++i) {
       struct radv_shader *shader = pipeline->shaders[i];
@@ -2986,24 +2992,44 @@ radv_upload_shaders(struct radv_device *device, struct radv_pipeline *pipeline,
       if (shader->bo)
          continue;
 
-      shader->va = slab_va + slab_offset;
+      shader->va = slab_va + offset;
 
-      void *dest_ptr = slab_ptr + slab_offset;
-      if (!radv_shader_binary_upload(device, binaries[i], shader, dest_ptr))
-         return false;
+      if (!radv_shader_binary_reloc(device, binaries[i], shader, staging + offset))
+         goto fail;
 
-      slab_offset += align(shader->code_size, RADV_SHADER_ALLOC_ALIGNMENT);
+      if (device->keep_shader_staging_buf) {
+         shader->code_ptr = (uint8_t *)staging + offset;
+      }
+
+      offset += align(shader->code_size, RADV_SHADER_ALLOC_ALIGNMENT);
    }
 
    if (pipeline->gs_copy_shader && !pipeline->gs_copy_shader->bo) {
-      pipeline->gs_copy_shader->va = slab_va + slab_offset;
+      struct radv_shader *shader = pipeline->gs_copy_shader;
+      shader->va = slab_va + offset;
 
-      void *dest_ptr = slab_ptr + slab_offset;
-      if (!radv_shader_binary_upload(device, gs_copy_binary, pipeline->gs_copy_shader, dest_ptr))
-         return false;
+      if (!radv_shader_binary_reloc(device, gs_copy_binary, shader, staging + offset))
+         goto fail;
+
+      if (device->keep_shader_staging_buf) {
+         shader->code_ptr = (uint8_t *)staging + offset;
+      }
+   }
+
+   if (radv_shader_binary_upload(device, pipeline->slab->alloc->arena->bo, staging, code_size,
+                                 slab_ptr, slab_va, NULL) != VK_SUCCESS)
+      goto fail;
+
+   if (device->keep_shader_staging_buf) {
+      pipeline->shader_upload_buf = staging;
+   } else {
+      free(staging);
    }
 
    return true;
+fail:
+   free(staging);
+   return false;
 }
 
 static bool
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index b8fc71408813..85c603636bfb 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -785,6 +785,13 @@ struct radv_queue {
    struct radeon_winsys_bo *gang_sem_bo;
 };
 
+struct radv_shader_upload_cs {
+   struct radeon_cmdbuf *cs;
+   struct radeon_winsys_bo *bo;
+   uint64_t bo_size;
+   void *bo_ptr;
+};
+
 #define RADV_BORDER_COLOR_COUNT       4096
 #define RADV_BORDER_COLOR_BUFFER_SIZE (sizeof(VkClearColorValue) * RADV_BORDER_COLOR_COUNT)
 
@@ -915,6 +922,17 @@ struct radv_device {
    struct list_head shader_block_obj_pool;
    mtx_t shader_arena_mutex;
 
+   mtx_t shader_upload_queue_mutex;
+   struct radeon_winsys_ctx *shader_upload_hw_ctx;
+   uint64_t shader_upload_seq;
+   VkSemaphore shader_upload_sem;
+   struct radv_shader_upload_cs shader_upload_cs[RADV_SHADER_UPLOAD_CS_COUNT];
+
+   /* Whether to DMA shaders to invisible VRAM or to upload directly through BAR. */
+   bool shader_use_invisible_vram;
+   /* If true, retain shaders in system RAM to allow dumping to SQTT traces. */
+   bool keep_shader_staging_buf;
+
    /* For detecting VM faults reported by dmesg. */
    uint64_t dmesg_timestamp;
 
@@ -2025,6 +2043,8 @@ struct radv_pipeline {
    struct radv_shader *shaders[MESA_VULKAN_SHADER_STAGES];
    struct radv_shader *gs_copy_shader;
 
+   char *shader_upload_buf;
+
    struct radeon_cmdbuf cs;
    uint32_t ctx_cs_hash;
    struct radeon_cmdbuf ctx_cs;
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 661b4288f877..6902f051176f 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -33,6 +33,7 @@
 #include "util/memstream.h"
 #include "util/mesa-sha1.h"
 #include "util/u_atomic.h"
+#include "radv_cs.h"
 #include "radv_debug.h"
 #include "radv_meta.h"
 #include "radv_private.h"
@@ -47,6 +48,8 @@
 #include "aco_interface.h"
 #include "sid.h"
 #include "vk_format.h"
+#include "vk_sync.h"
+#include "vk_semaphore.h"
 
 #include "aco_shader_info.h"
 #include "radv_aco_shader_info.h"
@@ -1529,6 +1532,19 @@ free_block_obj(struct radv_device *device, union radv_shader_arena_block *block)
    list_add(&block->pool, &device->shader_block_obj_pool);
 }
 
+VkResult
+radv_shader_wait_for_upload(struct radv_device *device, uint64_t upload_seq)
+{
+   const VkSemaphoreWaitInfo wait_info = {
+      .sType = VK_STRUCTURE_TYPE_SEMAPHORE_WAIT_INFO,
+      .pSemaphores = &device->shader_upload_sem,
+      .semaphoreCount = 1,
+      .pValues = &upload_seq,
+   };
+   return device->vk.dispatch_table.WaitSemaphores(radv_device_to_handle(device), &wait_info,
+                                                   UINT64_MAX);
+}
+
 /* Segregated fit allocator, implementing a good-fit allocation policy.
  *
  * This is an variation of sequential fit allocation with several lists of free blocks ("holes")
@@ -1604,21 +1620,30 @@ radv_alloc_shader_memory(struct radv_device *device, uint32_t size, void *ptr)
       MAX2(RADV_SHADER_ALLOC_MIN_ARENA_SIZE
               << MIN2(RADV_SHADER_ALLOC_MAX_ARENA_SIZE_SHIFT, device->shader_arena_shift),
            size);
-   VkResult result = device->ws->buffer_create(
-      device->ws, arena_size, RADV_SHADER_ALLOC_ALIGNMENT, RADEON_DOMAIN_VRAM,
-      RADEON_FLAG_NO_INTERPROCESS_SHARING | RADEON_FLAG_32BIT |
+   enum radeon_bo_flag flags = RADEON_FLAG_NO_INTERPROCESS_SHARING | RADEON_FLAG_32BIT;
+   if (device->shader_use_invisible_vram)
+      flags |= RADEON_FLAG_NO_CPU_ACCESS;
+   else
+      flags |=
+         RADEON_FLAG_CPU_ACCESS |
          (device->physical_device->rad_info.cpdma_prefetch_writes_memory ? 0
-                                                                         : RADEON_FLAG_READ_ONLY),
-      RADV_BO_PRIORITY_SHADER, 0, &arena->bo);
+                                                                         : RADEON_FLAG_READ_ONLY);
+
+   VkResult result;
+   result =
+      device->ws->buffer_create(device->ws, arena_size, RADV_SHADER_ALLOC_ALIGNMENT,
+                                RADEON_DOMAIN_VRAM, flags, RADV_BO_PRIORITY_SHADER, 0, &arena->bo);
    if (result != VK_SUCCESS)
       goto fail;
    radv_rmv_log_bo_allocate(device, arena->bo, arena_size, true);
 
    list_inithead(&arena->entries);
 
-   arena->ptr = (char *)device->ws->buffer_map(arena->bo);
-   if (!arena->ptr)
-      goto fail;
+   if (flags & RADEON_FLAG_CPU_ACCESS) {
+      arena->ptr = (char *)device->ws->buffer_map(arena->bo);
+      if (!arena->ptr)
+         goto fail;
+   }
 
    alloc = alloc_block_obj(device);
    hole = arena_size - size > 0 ? alloc_block_obj(device) : alloc;
@@ -1716,9 +1741,15 @@ radv_free_shader_memory(struct radv_device *device, union radv_shader_arena_bloc
    mtx_unlock(&device->shader_arena_mutex);
 }
 
-void
+VkResult
 radv_init_shader_arenas(struct radv_device *device)
 {
+   VkDevice vk_device = radv_device_to_handle(device);
+   struct radeon_winsys *ws = device->ws;
+
+   const struct vk_device_dispatch_table *disp = &device->vk.dispatch_table;
+   VkResult result = VK_SUCCESS;
+
    mtx_init(&device->shader_arena_mutex, mtx_plain);
 
    device->shader_free_list_mask = 0;
@@ -1727,11 +1758,59 @@ radv_init_shader_arenas(struct radv_device *device)
    list_inithead(&device->shader_block_obj_pool);
    for (unsigned i = 0; i < RADV_SHADER_ALLOC_NUM_FREE_LISTS; i++)
       list_inithead(&device->shader_free_lists[i]);
+
+   if (device->shader_use_invisible_vram) {
+      enum radv_queue_family qf = RADV_QUEUE_COMPUTE;
+      result = ws->ctx_create(ws, RADEON_CTX_PRIORITY_MEDIUM, &device->shader_upload_hw_ctx);
+      if (result != VK_SUCCESS)
+         return result;
+      mtx_init(&device->shader_upload_queue_mutex, mtx_plain);
+
+      for (unsigned i = 0; i < RADV_SHADER_UPLOAD_CS_COUNT; i++) {
+         device->shader_upload_cs[i].cs =
+            ws->cs_create(ws, radv_queue_family_to_ring(device->physical_device, qf));
+         if (!device->shader_upload_cs[i].cs) {
+            return VK_ERROR_OUT_OF_HOST_MEMORY;
+         }
+      }
+
+      const VkSemaphoreTypeCreateInfo sem_type = {
+         .sType = VK_STRUCTURE_TYPE_SEMAPHORE_TYPE_CREATE_INFO,
+         .semaphoreType = VK_SEMAPHORE_TYPE_TIMELINE,
+         .initialValue = 0,
+      };
+      const VkSemaphoreCreateInfo sem_create = {
+         .sType = VK_STRUCTURE_TYPE_SEMAPHORE_CREATE_INFO,
+         .pNext = &sem_type,
+      };
+      result = disp->CreateSemaphore(vk_device, &sem_create, NULL, &device->shader_upload_sem);
+      if (result != VK_SUCCESS)
+         return result;
+   }
+
+   return VK_SUCCESS;
 }
 
 void
 radv_destroy_shader_arenas(struct radv_device *device)
 {
+   struct vk_device_dispatch_table *disp = &device->vk.dispatch_table;
+   struct radeon_winsys *ws = device->ws;
+
+   /* Upload queue should be idle assuming that pipelines are not leaked */
+   if (device->shader_upload_sem)
+      disp->DestroySemaphore(radv_device_to_handle(device), device->shader_upload_sem, NULL);
+   for (unsigned i = 0; i < RADV_SHADER_UPLOAD_CS_COUNT; i++) {
+      if (device->shader_upload_cs[i].cs)
+         ws->cs_destroy(device->shader_upload_cs[i].cs);
+      if (device->shader_upload_cs[i].bo)
+         ws->buffer_destroy(ws, device->shader_upload_cs[i].bo);
+   }
+   if (device->shader_upload_hw_ctx) {
+      mtx_destroy(&device->shader_upload_queue_mutex);
+      ws->ctx_destroy(device->shader_upload_hw_ctx);
+   }
+
    list_for_each_entry_safe(union radv_shader_arena_block, block, &device->shader_block_obj_pool,
                             pool) free(block);
 
@@ -2086,7 +2165,7 @@ radv_open_rtld_binary(struct radv_device *device, const struct radv_shader *shad
 #endif
 
 bool
-radv_shader_binary_upload(struct radv_device *device, const struct radv_shader_binary *binary,
+radv_shader_binary_reloc(struct radv_device *device, const struct radv_shader_binary *binary,
                           struct radv_shader *shader, void *dest_ptr)
 {
    if (binary->type == RADV_BINARY_TYPE_RTLD) {
@@ -2112,7 +2191,6 @@ radv_shader_binary_upload(struct radv_device *device, const struct radv_shader_b
          return false;
       }
 
-      shader->code_ptr = dest_ptr;
       ac_rtld_close(&rtld_binary);
 #endif
    } else {
@@ -2123,13 +2201,125 @@ radv_shader_binary_upload(struct radv_device *device, const struct radv_shader_b
       uint32_t *ptr32 = (uint32_t *)dest_ptr + bin->code_size / 4;
       for (unsigned i = 0; i < DEBUGGER_NUM_MARKERS; i++)
          ptr32[i] = DEBUGGER_END_OF_CODE_MARKER;
-
-      shader->code_ptr = dest_ptr;
    }
 
    return true;
 }
 
+static VkResult
+radv_shader_resize_upload_buf(struct radv_shader_upload_cs *upload_cs, struct radeon_winsys *ws,
+                              uint64_t size)
+{
+   if (upload_cs->bo)
+      ws->buffer_destroy(ws, upload_cs->bo);
+
+   VkResult result =
+      ws->buffer_create(ws, size, RADV_SHADER_ALLOC_ALIGNMENT, ws->cs_domain(ws),
+                        RADEON_FLAG_CPU_ACCESS | RADEON_FLAG_NO_INTERPROCESS_SHARING |
+                           RADEON_FLAG_32BIT | RADEON_FLAG_GTT_WC,
+                        RADV_BO_PRIORITY_UPLOAD_BUFFER, 0, &upload_cs->bo);
+   if (unlikely(result != VK_SUCCESS))
+      return result;
+
+   upload_cs->bo_ptr = ws->buffer_map(upload_cs->bo);
+   upload_cs->bo_size = size;
+
+   return VK_SUCCESS;
+}
+
+/**
+ * When using invisible VRAM for shaders, schedule the upload using DMA. Otherwise, memcpy code
+ * to ptr.
+ * If upload_seq_out is NULL, this function blocks until the DMA is complete. Otherwise, the
+ * semaphore value to wait on device->shader_upload_sem is stored in *upload_seq_out.
+ */
+VkResult
+radv_shader_binary_upload(struct radv_device *device, struct radeon_winsys_bo *bo, char *code,
+                          unsigned size, char *ptr, uint64_t va, uint64_t *upload_seq_out)
+{
+   if (!device->shader_use_invisible_vram) {
+      memcpy(ptr, code, size);
+      return VK_SUCCESS;
+   }
+
+   uint64_t upload_seq = p_atomic_inc_return(&device->shader_upload_seq);
+   uint64_t cs_idx = upload_seq % RADV_SHADER_UPLOAD_CS_COUNT;
+
+   struct radv_shader_upload_cs *upload_cs = &device->shader_upload_cs[cs_idx];
+   struct radeon_cmdbuf *cs = upload_cs->cs;
+   struct radeon_winsys *ws = device->ws;
+
+   VkResult result;
+
+   /* Make sure we don't reset an in-flight command buffer */
+   uint64_t wait_val = MAX2(RADV_SHADER_UPLOAD_CS_COUNT, upload_seq) - RADV_SHADER_UPLOAD_CS_COUNT;
+   result = radv_shader_wait_for_upload(device, wait_val);
+   if (unlikely(result != VK_SUCCESS))
+      return result;
+
+   ws->cs_reset(cs);
+
+   /* Emit a stripped down version of CS preamble.
+    * A L2 invalidation is required because CP DMA uses L2 on gfx9+.
+    */
+   si_emit_compute(device, cs);
+
+   const enum amd_gfx_level gfx_level = device->physical_device->rad_info.gfx_level;
+   const bool is_mec = gfx_level >= GFX7; /* Shader upload always uses compute */
+   enum rgp_flush_bits sqtt_flush_bits = 0;
+   enum radv_cmd_flush_bits flush_bits = RADV_CMD_FLAG_INV_L2;
+   si_cs_emit_cache_flush(cs, gfx_level, NULL, 0, is_mec, flush_bits, &sqtt_flush_bits, 0);
+
+   if (upload_cs->bo_size < size) {
+      result = radv_shader_resize_upload_buf(upload_cs, ws, size);
+      if (unlikely(result != VK_SUCCESS))
+         return result;
+   }
+
+   memcpy(upload_cs->bo_ptr, code, size);
+   radv_cs_add_buffer(ws, cs, upload_cs->bo);
+   radv_cs_add_buffer(ws, cs, bo);
+   /* Compute-based copy requires shaders which might not have been uploaded yet, so use CP DMA
+    * even if it might be slightly slower.
+    */
+   si_cs_cp_dma_buffer_copy_internal(device, cs, radv_buffer_get_va(upload_cs->bo), va, size);
+
+   result = ws->cs_finalize(cs);
+   if (unlikely(result != VK_SUCCESS))
+      return result;
+
+   struct vk_semaphore *semaphore = vk_semaphore_from_handle(device->shader_upload_sem);
+   struct vk_sync *sync = vk_semaphore_get_active_sync(semaphore);
+   const struct vk_sync_signal signal_info = {
+      .sync = sync,
+      .signal_value = upload_seq,
+      .stage_mask = VK_PIPELINE_STAGE_2_ALL_COMMANDS_BIT,
+   };
+
+   struct radv_winsys_submit_info submit = {
+      .ip_type = AMD_IP_COMPUTE,
+      .queue_index = 0,
+      .cs_array = &cs,
+      .cs_count = 1,
+   };
+
+   mtx_lock(&device->shader_upload_queue_mutex);
+   result = ws->cs_submit(device->shader_upload_hw_ctx, &submit, 0, NULL, 1, &signal_info, false);
+   mtx_unlock(&device->shader_upload_queue_mutex);
+   if (unlikely(result != VK_SUCCESS))
+      return VK_ERROR_OUT_OF_HOST_MEMORY;
+
+   if (upload_seq_out) {
+      *upload_seq_out = upload_seq;
+   } else {
+      result = radv_shader_wait_for_upload(device, upload_seq);
+      if (unlikely(result != VK_SUCCESS))
+         return result;
+   }
+
+   return VK_SUCCESS;
+}
+
 struct radv_shader *
 radv_shader_create(struct radv_device *device, const struct radv_shader_binary *binary,
                    bool keep_shader_info, bool from_cache, const struct radv_shader_args *args)
@@ -2263,17 +2453,32 @@ radv_shader_part_create(struct radv_shader_part_binary *binary, unsigned wave_si
    return shader_part;
 }
 
-void
-radv_shader_part_binary_upload(const struct radv_shader_part_binary *binary, void *dest_ptr)
+bool
+radv_shader_part_binary_upload(struct radv_device *device,
+                               struct radv_shader_part *shader_part)
 {
-   memcpy(dest_ptr, binary->data, binary->code_size);
+   char *dest_ptr = shader_part->alloc->arena->ptr + shader_part->alloc->offset;
+   const struct radv_shader_part_binary *bin = shader_part->binary;
+   uint32_t code_size = radv_get_shader_binary_size(bin->code_size);
+
+   char *staging = calloc(1, code_size);
+   memcpy(staging, bin->data, bin->code_size);
 
    /* Add end-of-code markers for the UMR disassembler. */
-   uint32_t *ptr32 = (uint32_t *)dest_ptr + binary->code_size / 4;
+   uint32_t *ptr32 = (uint32_t *)staging + bin->code_size / 4;
    for (unsigned i = 0; i < DEBUGGER_NUM_MARKERS; i++)
       ptr32[i] = DEBUGGER_END_OF_CODE_MARKER;
+
+   if (radv_shader_binary_upload(device, shader_part->bo, staging, bin->code_size, dest_ptr,
+                                 shader_part->va, NULL) != VK_SUCCESS) {
+      free(staging);
+      return false;
+   }
+   free(staging);
+   return true;
 }
 
+
 static char *
 radv_dump_nir_shaders(struct nir_shader *const *shaders, int shader_count)
 {
@@ -2504,10 +2709,17 @@ radv_create_trap_handler_shader(struct radv_device *device)
    trap->alloc = radv_alloc_shader_memory(device, shader->code_size, NULL);
 
    trap->bo = trap->alloc->arena->bo;
+   uint64_t dest_va = radv_buffer_get_va(trap->alloc->arena->bo) + trap->alloc->offset;
    char *dest_ptr = trap->alloc->arena->ptr + trap->alloc->offset;
 
    struct radv_shader_binary_legacy *bin = (struct radv_shader_binary_legacy *)binary;
-   memcpy(dest_ptr, bin->data, bin->code_size);
+   if (radv_shader_binary_upload(device, trap->bo, (char *)bin->data, bin->code_size, dest_ptr,
+                                 dest_va, NULL) != VK_SUCCESS) {
+      radv_free_shader_memory(device, trap->alloc);
+      ralloc_free(b.shader);
+      free(shader);
+      return NULL;
+   }
 
    ralloc_free(b.shader);
    free(shader);
@@ -2619,8 +2831,8 @@ radv_create_vs_prolog(struct radv_device *device, const struct radv_vs_prolog_ke
    prolog->bo = prolog->alloc->arena->bo;
    prolog->va = radv_buffer_get_va(prolog->bo) + prolog->alloc->offset;
 
-   void *dest_ptr = prolog->alloc->arena->ptr + prolog->alloc->offset;
-   radv_shader_part_binary_upload(binary, dest_ptr);
+   if (!radv_shader_part_binary_upload(device, prolog))
+      goto fail_alloc;
 
    if (options.dump_shader) {
       fprintf(stderr, "Vertex prolog");
@@ -2684,8 +2896,8 @@ radv_create_ps_epilog(struct radv_device *device, const struct radv_ps_epilog_ke
    epilog->bo = epilog->alloc->arena->bo;
    epilog->va = radv_buffer_get_va(epilog->bo) + epilog->alloc->offset;
 
-   void *dest_ptr = epilog->alloc->arena->ptr + epilog->alloc->offset;
-   radv_shader_part_binary_upload(binary, dest_ptr);
+   if (!radv_shader_part_binary_upload(device, epilog))
+      goto fail_alloc;
 
    if (options.dump_shader) {
       fprintf(stderr, "Fragment epilog");
diff --git a/src/amd/vulkan/radv_shader.h b/src/amd/vulkan/radv_shader.h
index 50bc984bf001..3802c14cbd68 100644
--- a/src/amd/vulkan/radv_shader.h
+++ b/src/amd/vulkan/radv_shader.h
@@ -551,7 +551,7 @@ void radv_nir_lower_abi(nir_shader *shader, enum amd_gfx_level gfx_level,
                         const struct radv_pipeline_key *pl_key, bool use_llvm,
                         uint32_t address32_hi);
 
-void radv_init_shader_arenas(struct radv_device *device);
+VkResult radv_init_shader_arenas(struct radv_device *device);
 void radv_destroy_shader_arenas(struct radv_device *device);
 
 struct radv_pipeline_shader_stack_size;
@@ -579,10 +579,16 @@ struct radv_shader *radv_shader_nir_to_asm(
    int shader_count, const struct radv_pipeline_key *key, bool keep_shader_info, bool keep_statistic_info,
    struct radv_shader_binary **binary_out);
 
-bool radv_shader_binary_upload(struct radv_device *device, const struct radv_shader_binary *binary,
-                               struct radv_shader *shader, void *dest_ptr);
+bool radv_shader_binary_reloc(struct radv_device *device, const struct radv_shader_binary *binary,
+                              struct radv_shader *shader, void *dest_ptr);
+VkResult radv_shader_binary_upload(struct radv_device *device, struct radeon_winsys_bo *bo,
+                                   char *code, unsigned size, char *ptr, uint64_t va,
+                                   uint64_t *upload_seq_out);
 
-void radv_shader_part_binary_upload(const struct radv_shader_part_binary *binary, void *dest_ptr);
+VkResult radv_shader_wait_for_upload(struct radv_device *device, uint64_t upload_seq);
+
+bool radv_shader_part_binary_upload(struct radv_device *device,
+                                    struct radv_shader_part *shader_part);
 
 union radv_shader_arena_block *radv_alloc_shader_memory(struct radv_device *device, uint32_t size,
                                                         void *ptr);
-- 
GitLab


From fa3b6ff1b86e0c997172ce0154f50108e5d3bce7 Mon Sep 17 00:00:00 2001
From: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
Date: Tue, 12 Jul 2022 17:25:00 +0900
Subject: [PATCH 4/5] radv: Wait for shader uploads asynchronously.

This introduces tracking of the required semaphore values in pipelines,
which is then propagated to cmd_buffers on bind. Each queue also keeps
track the maximum count it has waited for, so that we can avoid the waiting
overhead once all the shaders are loaded and referenced.
---
 src/amd/vulkan/radv_cmd_buffer.c | 11 +++++++++
 src/amd/vulkan/radv_device.c     | 39 ++++++++++++++++++++++++++++++--
 src/amd/vulkan/radv_pipeline.c   |  7 +++++-
 src/amd/vulkan/radv_private.h    |  5 ++++
 src/amd/vulkan/radv_shader.c     |  7 +++++-
 src/amd/vulkan/radv_shader.h     |  1 +
 6 files changed, 66 insertions(+), 4 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index b61dc02277fc..d88fe9849437 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -512,6 +512,7 @@ radv_reset_cmd_buffer(struct vk_command_buffer *vk_cmd_buffer,
    cmd_buffer->ace_internal.sem.gfx2ace_value = 0;
    cmd_buffer->ace_internal.sem.emitted_gfx2ace_value = 0;
    cmd_buffer->ace_internal.sem.va = 0;
+   cmd_buffer->shader_upload_seq = 0;
 
    if (cmd_buffer->upload.upload_bo)
       radv_cs_add_buffer(cmd_buffer->device->ws, cmd_buffer->cs, cmd_buffer->upload.upload_bo);
@@ -1921,6 +1922,8 @@ radv_emit_ps_epilog_state(struct radv_cmd_buffer *cmd_buffer, struct radv_shader
    radv_emit_shader_pointer(cmd_buffer->device, cmd_buffer->cs, base_reg + loc->sgpr_idx * 4,
                             ps_epilog->va, false);
 
+   cmd_buffer->shader_upload_seq = MAX2(cmd_buffer->shader_upload_seq, ps_epilog->upload_seq);
+
    cmd_buffer->state.emitted_ps_epilog = ps_epilog;
 }
 
@@ -3949,6 +3952,8 @@ radv_emit_vertex_input(struct radv_cmd_buffer *cmd_buffer, bool pipeline_is_dirt
    emit_prolog_regs(cmd_buffer, vs_shader, prolog, pipeline_is_dirty);
    emit_prolog_inputs(cmd_buffer, vs_shader, nontrivial_divisors, pipeline_is_dirty);
 
+   cmd_buffer->shader_upload_seq = MAX2(cmd_buffer->shader_upload_seq, prolog->upload_seq);
+
    cmd_buffer->state.emitted_vs_prolog = prolog;
 
    if (unlikely(cmd_buffer->device->trace_bo))
@@ -6131,6 +6136,10 @@ radv_CmdBindPipeline(VkCommandBuffer commandBuffer, VkPipelineBindPoint pipeline
    RADV_FROM_HANDLE(radv_cmd_buffer, cmd_buffer, commandBuffer);
    RADV_FROM_HANDLE(radv_pipeline, pipeline, _pipeline);
 
+   if (pipeline)
+      cmd_buffer->shader_upload_seq =
+         MAX2(cmd_buffer->shader_upload_seq, pipeline->shader_upload_seq);
+
    switch (pipelineBindPoint) {
    case VK_PIPELINE_BIND_POINT_COMPUTE: {
       struct radv_compute_pipeline *compute_pipeline = radv_pipeline_to_compute(pipeline);
@@ -7043,6 +7052,8 @@ radv_CmdExecuteCommands(VkCommandBuffer commandBuffer, uint32_t commandBufferCou
       if (secondary->gds_oa_needed)
          primary->gds_oa_needed = true;
 
+      primary->shader_upload_seq = MAX2(primary->shader_upload_seq, secondary->shader_upload_seq);
+
       if (!secondary->state.render.has_image_views && primary->state.render.active &&
           (primary->state.dirty & RADV_CMD_DIRTY_FRAMEBUFFER)) {
          /* Emit the framebuffer state from primary if secondary
diff --git a/src/amd/vulkan/radv_device.c b/src/amd/vulkan/radv_device.c
index ab7282cd4ba6..94936d393fd6 100644
--- a/src/amd/vulkan/radv_device.c
+++ b/src/amd/vulkan/radv_device.c
@@ -5746,6 +5746,19 @@ radv_queue_submit_empty(struct radv_queue *queue, struct vk_queue_submit *submis
                                        submission->signal_count, submission->signals, false);
 }
 
+static void
+radv_get_shader_upload_sync_wait(struct radv_device *device, uint64_t shader_upload_seq,
+                                 struct vk_sync_wait *out_sync_wait)
+{
+   struct vk_semaphore *semaphore = vk_semaphore_from_handle(device->shader_upload_sem);
+   struct vk_sync *sync = vk_semaphore_get_active_sync(semaphore);
+   *out_sync_wait = (struct vk_sync_wait){
+      .sync = sync,
+      .wait_value = shader_upload_seq,
+      .stage_mask = VK_PIPELINE_STAGE_2_ALL_COMMANDS_BIT,
+   };
+}
+
 static VkResult
 radv_queue_submit_normal(struct radv_queue *queue, struct vk_queue_submit *submission)
 {
@@ -5754,6 +5767,9 @@ radv_queue_submit_normal(struct radv_queue *queue, struct vk_queue_submit *submi
    bool use_ace = false;
    bool use_perf_counters = false;
    VkResult result;
+   uint64_t shader_upload_seq = 0;
+   uint32_t wait_count = submission->wait_count;
+   struct vk_sync_wait *waits = submission->waits;
 
    result = radv_update_preambles(&queue->state, queue->device, submission->command_buffers,
                                   submission->command_buffer_count, &use_perf_counters, &use_ace);
@@ -5783,6 +5799,20 @@ radv_queue_submit_normal(struct radv_queue *queue, struct vk_queue_submit *submi
    if (queue->device->trace_bo)
       simple_mtx_lock(&queue->device->trace_mtx);
 
+   for (uint32_t j = 0; j < submission->command_buffer_count; j++) {
+      struct radv_cmd_buffer *cmd_buffer = (struct radv_cmd_buffer *)submission->command_buffers[j];
+      shader_upload_seq = MAX2(shader_upload_seq, cmd_buffer->shader_upload_seq);
+   }
+
+   if (shader_upload_seq > queue->last_shader_upload_seq) {
+      /* Patch the wait array to add waiting for referenced shaders to upload. */
+      waits = malloc(sizeof(struct vk_sync_wait) * (wait_count + 1));
+      wait_count += 1;
+      memcpy(waits, submission->waits, sizeof(struct vk_sync_wait) * submission->wait_count);
+      radv_get_shader_upload_sync_wait(queue->device, shader_upload_seq,
+                                       &waits[submission->wait_count]);
+   }
+
    struct radeon_cmdbuf *perf_ctr_lock_cs = NULL;
    struct radeon_cmdbuf *perf_ctr_unlock_cs = NULL;
 
@@ -5808,7 +5838,7 @@ radv_queue_submit_normal(struct radv_queue *queue, struct vk_queue_submit *submi
    /* For fences on the same queue/vm amdgpu doesn't wait till all processing is finished
     * before starting the next cmdbuffer, so we need to do it here.
     */
-   const bool need_wait = submission->wait_count > 0;
+   const bool need_wait = wait_count > 0;
    unsigned num_preambles = 0;
    struct radeon_cmdbuf *preambles[4] = {0};
 
@@ -5880,7 +5910,7 @@ radv_queue_submit_normal(struct radv_queue *queue, struct vk_queue_submit *submi
       submit.preamble_count = submit_ace ? num_preambles : num_1q_preambles;
 
       result = queue->device->ws->cs_submit(
-         ctx, &submit, j == 0 ? submission->wait_count : 0, submission->waits,
+         ctx, &submit, j == 0 ? wait_count : 0, waits,
          last_submit ? submission->signal_count : 0, submission->signals, can_patch);
 
       if (result != VK_SUCCESS)
@@ -5898,8 +5928,13 @@ radv_queue_submit_normal(struct radv_queue *queue, struct vk_queue_submit *submi
       preambles[1] = !use_ace ? NULL : queue->ace_internal_state->initial_preamble_cs;
    }
 
+   queue->last_shader_upload_seq =
+      MAX2(queue->last_shader_upload_seq, shader_upload_seq);
+
 fail:
    free(cs_array);
+   if (waits != submission->waits)
+      free(waits);
    if (queue->device->trace_bo)
       simple_mtx_unlock(&queue->device->trace_mtx);
 
diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index ae2cbb869bc7..f72558cafb96 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -156,6 +156,11 @@ void
 radv_pipeline_destroy(struct radv_device *device, struct radv_pipeline *pipeline,
                       const VkAllocationCallbacks *allocator)
 {
+   if (device->shader_use_invisible_vram) {
+      /* Wait for any pending upload to complete, or we'll be writing into freed shader memory. */
+      radv_shader_wait_for_upload(device, pipeline->shader_upload_seq);
+   }
+
    if (pipeline->type == RADV_PIPELINE_GRAPHICS) {
       struct radv_graphics_pipeline *graphics_pipeline = radv_pipeline_to_graphics(pipeline);
 
@@ -3017,7 +3022,7 @@ radv_upload_shaders(struct radv_device *device, struct radv_pipeline *pipeline,
    }
 
    if (radv_shader_binary_upload(device, pipeline->slab->alloc->arena->bo, staging, code_size,
-                                 slab_ptr, slab_va, NULL) != VK_SUCCESS)
+                                 slab_ptr, slab_va, &pipeline->shader_upload_seq) != VK_SUCCESS)
       goto fail;
 
    if (device->keep_shader_staging_buf) {
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index 85c603636bfb..c24b5be21ef9 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -783,6 +783,8 @@ struct radv_queue {
    struct radv_queue_state state;
    struct radv_queue_state *ace_internal_state;
    struct radeon_winsys_bo *gang_sem_bo;
+
+   uint64_t last_shader_upload_seq;
 };
 
 struct radv_shader_upload_cs {
@@ -1693,6 +1695,8 @@ struct radv_cmd_buffer {
     * Bitmask of pending active query flushes.
     */
    enum radv_cmd_flush_bits active_query_flush_bits;
+
+   uint64_t shader_upload_seq;
 };
 
 extern const struct vk_command_buffer_ops radv_cmd_buffer_ops;
@@ -2044,6 +2048,7 @@ struct radv_pipeline {
    struct radv_shader *gs_copy_shader;
 
    char *shader_upload_buf;
+   uint64_t shader_upload_seq;
 
    struct radeon_cmdbuf cs;
    uint32_t ctx_cs_hash;
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 6902f051176f..36baeeb0b877 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -2470,7 +2470,7 @@ radv_shader_part_binary_upload(struct radv_device *device,
       ptr32[i] = DEBUGGER_END_OF_CODE_MARKER;
 
    if (radv_shader_binary_upload(device, shader_part->bo, staging, bin->code_size, dest_ptr,
-                                 shader_part->va, NULL) != VK_SUCCESS) {
+                                 shader_part->va, &shader_part->upload_seq) != VK_SUCCESS) {
       free(staging);
       return false;
    }
@@ -2934,6 +2934,11 @@ radv_shader_part_destroy(struct radv_device *device, struct radv_shader_part *sh
 {
    assert(shader_part->ref_count == 0);
 
+   if (device->shader_use_invisible_vram) {
+      /* Wait for any pending upload to complete, or we'll be writing into freed shader memory. */
+      radv_shader_wait_for_upload(device, shader_part->upload_seq);
+   }
+
    if (shader_part->alloc)
       radv_free_shader_memory(device, shader_part->alloc);
    free(shader_part->binary);
diff --git a/src/amd/vulkan/radv_shader.h b/src/amd/vulkan/radv_shader.h
index 3802c14cbd68..a6a0bd735b30 100644
--- a/src/amd/vulkan/radv_shader.h
+++ b/src/amd/vulkan/radv_shader.h
@@ -520,6 +520,7 @@ struct radv_shader_part {
    uint8_t num_preserved_sgprs;
    bool nontrivial_divisors;
    uint32_t spi_shader_col_format;
+   uint64_t upload_seq;
 
    struct radv_shader_part_binary *binary;
 
-- 
GitLab


From f145581c93180112dee9ea6090465ad2279f8a6c Mon Sep 17 00:00:00 2001
From: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
Date: Sun, 10 Jul 2022 23:16:39 +0900
Subject: [PATCH 5/5] radv: Assume all_vram_vis in null winsys.

To disable the staged-upload mechanism which requires GPU command execution
to operate.
---
 src/amd/vulkan/winsys/null/radv_null_winsys.c | 2 ++
 1 file changed, 2 insertions(+)

diff --git a/src/amd/vulkan/winsys/null/radv_null_winsys.c b/src/amd/vulkan/winsys/null/radv_null_winsys.c
index 87178dbc820f..314f7bf96ad2 100644
--- a/src/amd/vulkan/winsys/null/radv_null_winsys.c
+++ b/src/amd/vulkan/winsys/null/radv_null_winsys.c
@@ -143,6 +143,8 @@ radv_null_winsys_query_info(struct radeon_winsys *rws, struct radeon_info *info)
    info->max_render_backends = gpu_info[info->family].num_render_backends;
 
    info->has_dedicated_vram = gpu_info[info->family].has_dedicated_vram;
+   /* Avoid use of staged shader uploads which requires CS submissions */
+   info->all_vram_visible = true;
    info->has_packed_math_16bit = info->gfx_level >= GFX9;
 
    info->has_image_load_dcc_bug =
-- 
GitLab

