From 5ed55fabf34e3d5df52054216f27fcb1f08e1752 Mon Sep 17 00:00:00 2001
From: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
Date: Tue, 28 Feb 2023 22:02:30 +0900
Subject: [PATCH 1/4] radv: Use radeon_cmdbuf for sdma_copy_image.

For consistency with the sdma_copy_buffer helper that will be added next.

As a general justification, SDMA commands require little state tracking and
using radeon_cmdbuf makes it more suitable for driver internal use.
---
 src/amd/vulkan/radv_meta_copy.c       | 13 ++---
 src/amd/vulkan/radv_private.h         |  5 +-
 src/amd/vulkan/radv_sdma_copy_image.c | 69 +++++++++++++--------------
 3 files changed, 43 insertions(+), 44 deletions(-)

diff --git a/src/amd/vulkan/radv_meta_copy.c b/src/amd/vulkan/radv_meta_copy.c
index 25f88b9df6d3..4f598c59a2c3 100644
--- a/src/amd/vulkan/radv_meta_copy.c
+++ b/src/amd/vulkan/radv_meta_copy.c
@@ -230,16 +230,18 @@ copy_image_to_buffer(struct radv_cmd_buffer *cmd_buffer, struct radv_buffer *buf
                      struct radv_image *image, VkImageLayout layout,
                      const VkBufferImageCopy2 *region)
 {
+   struct radv_device *device = cmd_buffer->device;
    if (cmd_buffer->qf == RADV_QUEUE_TRANSFER) {
+      struct radeon_cmdbuf *cs = cmd_buffer->cs;
       /* RADV_QUEUE_TRANSFER should only be used for the prime blit */
       assert(!region->imageOffset.x && !region->imageOffset.y && !region->imageOffset.z);
       assert(image->vk.image_type == VK_IMAGE_TYPE_2D);
       assert(image->info.width == region->imageExtent.width);
       assert(image->info.height == region->imageExtent.height);
-      ASSERTED bool res = radv_sdma_copy_image(cmd_buffer, image, buffer, region);
+      ASSERTED bool res = radv_sdma_copy_image(device, cs, image, buffer, region);
       assert(res);
-      radv_cs_add_buffer(cmd_buffer->device->ws, cmd_buffer->cs, image->bindings[0].bo);
-      radv_cs_add_buffer(cmd_buffer->device->ws, cmd_buffer->cs, buffer->bo);
+      radv_cs_add_buffer(device->ws, cs, image->bindings[0].bo);
+      radv_cs_add_buffer(device->ws, cs, buffer->bo);
       return;
    }
 
@@ -283,9 +285,8 @@ copy_image_to_buffer(struct radv_cmd_buffer *cmd_buffer, struct radv_buffer *buf
    if (!radv_is_buffer_format_supported(img_info.format, NULL)) {
       uint32_t queue_mask = radv_image_queue_family_mask(image, cmd_buffer->qf,
                                                          cmd_buffer->qf);
-      bool compressed =
-         radv_layout_dcc_compressed(cmd_buffer->device, image, region->imageSubresource.mipLevel,
-                                    layout, queue_mask);
+      bool compressed = radv_layout_dcc_compressed(device, image, region->imageSubresource.mipLevel,
+                                                   layout, queue_mask);
       if (compressed) {
          radv_decompress_dcc(cmd_buffer, image,
                              &(VkImageSubresourceRange){
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index c0d8c9282a45..64cc2277eb4c 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -2971,8 +2971,9 @@ void radv_rra_trace_init(struct radv_device *device);
 VkResult radv_rra_dump_trace(VkQueue vk_queue, char *filename);
 void radv_rra_trace_finish(VkDevice vk_device, struct radv_rra_trace_data *data);
 
-bool radv_sdma_copy_image(struct radv_cmd_buffer *cmd_buffer, struct radv_image *image,
-                          struct radv_buffer *buffer, const VkBufferImageCopy2 *region);
+bool radv_sdma_copy_image(struct radv_device *device, struct radeon_cmdbuf *cs,
+                          struct radv_image *image, struct radv_buffer *buffer,
+                          const VkBufferImageCopy2 *region);
 
 void radv_memory_trace_init(struct radv_device *device);
 void radv_rmv_log_bo_allocate(struct radv_device *device, struct radeon_winsys_bo *bo,
diff --git a/src/amd/vulkan/radv_sdma_copy_image.c b/src/amd/vulkan/radv_sdma_copy_image.c
index 2f9d48ec73d4..0ada01116af1 100644
--- a/src/amd/vulkan/radv_sdma_copy_image.c
+++ b/src/amd/vulkan/radv_sdma_copy_image.c
@@ -27,12 +27,11 @@
 #include "radv_private.h"
 
 static bool
-radv_sdma_v4_v5_copy_image_to_buffer(struct radv_cmd_buffer *cmd_buffer, struct radv_image *image,
-                                     struct radv_buffer *buffer,
+radv_sdma_v4_v5_copy_image_to_buffer(struct radv_device *device, struct radeon_cmdbuf *cs,
+                                     struct radv_image *image, struct radv_buffer *buffer,
                                      const VkBufferImageCopy2 *region)
 {
    assert(image->plane_count == 1);
-   struct radv_device *device = cmd_buffer->device;
    unsigned bpp = image->planes[0].surface.bpe;
    uint64_t dst_address = buffer->bo->va;
    uint64_t src_address = image->bindings[0].bo->va + image->planes[0].surface.u.gfx9.surf_offset;
@@ -43,7 +42,7 @@ radv_sdma_v4_v5_copy_image_to_buffer(struct radv_cmd_buffer *cmd_buffer, struct
 
    /* Linear -> linear sub-window copy. */
    if (image->planes[0].surface.is_linear) {
-      ASSERTED unsigned cdw_max = radeon_check_space(cmd_buffer->device->ws, cmd_buffer->cs, 7);
+      ASSERTED unsigned cdw_max = radeon_check_space(device->ws, cs, 7);
       unsigned bytes = src_pitch * copy_height * bpp;
 
       if (!(bytes < (1u << 22)))
@@ -51,16 +50,16 @@ radv_sdma_v4_v5_copy_image_to_buffer(struct radv_cmd_buffer *cmd_buffer, struct
 
       src_address += image->planes[0].surface.u.gfx9.offset[0];
 
-      radeon_emit(cmd_buffer->cs, CIK_SDMA_PACKET(CIK_SDMA_OPCODE_COPY,
+      radeon_emit(cs, CIK_SDMA_PACKET(CIK_SDMA_OPCODE_COPY,
                                                   CIK_SDMA_COPY_SUB_OPCODE_LINEAR, (tmz ? 4 : 0)));
-      radeon_emit(cmd_buffer->cs, bytes - 1);
-      radeon_emit(cmd_buffer->cs, 0);
-      radeon_emit(cmd_buffer->cs, src_address);
-      radeon_emit(cmd_buffer->cs, src_address >> 32);
-      radeon_emit(cmd_buffer->cs, dst_address);
-      radeon_emit(cmd_buffer->cs, dst_address >> 32);
+      radeon_emit(cs, bytes - 1);
+      radeon_emit(cs, 0);
+      radeon_emit(cs, src_address);
+      radeon_emit(cs, src_address >> 32);
+      radeon_emit(cs, dst_address);
+      radeon_emit(cs, dst_address >> 32);
 
-      assert(cmd_buffer->cs->cdw <= cdw_max);
+      assert(cs->cdw <= cdw_max);
 
       return true;
    }
@@ -81,34 +80,32 @@ radv_sdma_v4_v5_copy_image_to_buffer(struct radv_cmd_buffer *cmd_buffer, struct
             linear_slice_pitch < (1 << 28) && copy_width < (1 << 14) && copy_height < (1 << 14)))
          return false;
 
-      ASSERTED unsigned cdw_max =
-         radeon_check_space(cmd_buffer->device->ws, cmd_buffer->cs, 14 + (dcc ? 3 : 0));
+      ASSERTED unsigned cdw_max = radeon_check_space(device->ws, cs, 14 + (dcc ? 3 : 0));
 
-      radeon_emit(cmd_buffer->cs,
+      radeon_emit(cs,
                   CIK_SDMA_PACKET(CIK_SDMA_OPCODE_COPY, CIK_SDMA_COPY_SUB_OPCODE_TILED_SUB_WINDOW,
                                   (tmz ? 4 : 0)) |
                      dcc << 19 | (is_v5 ? 0 : 0 /* tiled->buffer.b.b.last_level */) << 20 |
                      1u << 31);
-      radeon_emit(
-         cmd_buffer->cs,
+      radeon_emit(cs,
          (uint32_t)tiled_address | (image->planes[0].surface.tile_swizzle << 8));
-      radeon_emit(cmd_buffer->cs, (uint32_t)(tiled_address >> 32));
-      radeon_emit(cmd_buffer->cs, 0);
-      radeon_emit(cmd_buffer->cs, ((tiled_width - 1) << 16));
-      radeon_emit(cmd_buffer->cs, (tiled_height - 1));
+      radeon_emit(cs, (uint32_t)(tiled_address >> 32));
+      radeon_emit(cs, 0);
+      radeon_emit(cs, ((tiled_width - 1) << 16));
+      radeon_emit(cs, (tiled_height - 1));
       radeon_emit(
-         cmd_buffer->cs,
+         cs,
          util_logbase2(bpp) | image->planes[0].surface.u.gfx9.swizzle_mode << 3 |
             image->planes[0].surface.u.gfx9.resource_type << 9 |
             (is_v5 ? 0 /* tiled->buffer.b.b.last_level */ : image->planes[0].surface.u.gfx9.epitch)
                << 16);
-      radeon_emit(cmd_buffer->cs, (uint32_t)linear_address);
-      radeon_emit(cmd_buffer->cs, (uint32_t)(linear_address >> 32));
-      radeon_emit(cmd_buffer->cs, 0);
-      radeon_emit(cmd_buffer->cs, ((linear_pitch - 1) << 16));
-      radeon_emit(cmd_buffer->cs, linear_slice_pitch - 1);
-      radeon_emit(cmd_buffer->cs, (copy_width - 1) | ((copy_height - 1) << 16));
-      radeon_emit(cmd_buffer->cs, 0);
+      radeon_emit(cs, (uint32_t)linear_address);
+      radeon_emit(cs, (uint32_t)(linear_address >> 32));
+      radeon_emit(cs, 0);
+      radeon_emit(cs, ((linear_pitch - 1) << 16));
+      radeon_emit(cs, linear_slice_pitch - 1);
+      radeon_emit(cs, (copy_width - 1) | ((copy_height - 1) << 16));
+      radeon_emit(cs, 0);
 
       if (dcc) {
          uint64_t md_address = tiled_address + image->planes[0].surface.meta_offset;
@@ -121,16 +118,16 @@ radv_sdma_v4_v5_copy_image_to_buffer(struct radv_cmd_buffer *cmd_buffer, struct
          hw_type = radv_translate_buffer_numformat(desc, vk_format_get_first_non_void_channel(format));
 
          /* Add metadata */
-         radeon_emit(cmd_buffer->cs, (uint32_t)md_address);
-         radeon_emit(cmd_buffer->cs, (uint32_t)(md_address >> 32));
-         radeon_emit(cmd_buffer->cs,
+         radeon_emit(cs, (uint32_t)md_address);
+         radeon_emit(cs, (uint32_t)(md_address >> 32));
+         radeon_emit(cs,
                      hw_fmt | vi_alpha_is_on_msb(device, format) << 8 | hw_type << 9 |
                         image->planes[0].surface.u.gfx9.color.dcc.max_compressed_block_size << 24 |
                         V_028C78_MAX_BLOCK_SIZE_256B << 26 | tmz << 29 |
                         image->planes[0].surface.u.gfx9.color.dcc.pipe_aligned << 31);
       }
 
-      assert(cmd_buffer->cs->cdw <= cdw_max);
+      assert(cs->cdw <= cdw_max);
 
       return true;
    }
@@ -139,9 +136,9 @@ radv_sdma_v4_v5_copy_image_to_buffer(struct radv_cmd_buffer *cmd_buffer, struct
 }
 
 bool
-radv_sdma_copy_image(struct radv_cmd_buffer *cmd_buffer, struct radv_image *image,
+radv_sdma_copy_image(struct radv_device *device, struct radeon_cmdbuf *cs, struct radv_image *image,
                      struct radv_buffer *buffer, const VkBufferImageCopy2 *region)
 {
-   assert(cmd_buffer->device->physical_device->rad_info.gfx_level >= GFX9);
-   return radv_sdma_v4_v5_copy_image_to_buffer(cmd_buffer, image, buffer, region);
+   assert(device->physical_device->rad_info.gfx_level >= GFX9);
+   return radv_sdma_v4_v5_copy_image_to_buffer(device, cs, image, buffer, region);
 }
-- 
GitLab


From 44bd7e28986d871085491b5046326917461eb7eb Mon Sep 17 00:00:00 2001
From: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
Date: Tue, 28 Feb 2023 22:09:18 +0900
Subject: [PATCH 2/4] radv: Introduce sdma_copy_buffer for GFX7+.

Helper salvaged from radeonsi (before SDMA removal).

This will be used for driver internal submissions to DMA shaders from GTT
to invisible VRAM.
---
 src/amd/vulkan/radv_private.h         |  2 ++
 src/amd/vulkan/radv_sdma_copy_image.c | 40 +++++++++++++++++++++++++++
 2 files changed, 42 insertions(+)

diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index 64cc2277eb4c..76cbba5d4295 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -2974,6 +2974,8 @@ void radv_rra_trace_finish(VkDevice vk_device, struct radv_rra_trace_data *data)
 bool radv_sdma_copy_image(struct radv_device *device, struct radeon_cmdbuf *cs,
                           struct radv_image *image, struct radv_buffer *buffer,
                           const VkBufferImageCopy2 *region);
+void radv_sdma_copy_buffer(struct radv_device *device, struct radeon_cmdbuf *cs, uint64_t src_va,
+                           uint64_t dst_va, uint64_t size);
 
 void radv_memory_trace_init(struct radv_device *device);
 void radv_rmv_log_bo_allocate(struct radv_device *device, struct radeon_winsys_bo *bo,
diff --git a/src/amd/vulkan/radv_sdma_copy_image.c b/src/amd/vulkan/radv_sdma_copy_image.c
index 0ada01116af1..20cc2c06c35d 100644
--- a/src/amd/vulkan/radv_sdma_copy_image.c
+++ b/src/amd/vulkan/radv_sdma_copy_image.c
@@ -142,3 +142,43 @@ radv_sdma_copy_image(struct radv_device *device, struct radeon_cmdbuf *cs, struc
    assert(device->physical_device->rad_info.gfx_level >= GFX9);
    return radv_sdma_v4_v5_copy_image_to_buffer(device, cs, image, buffer, region);
 }
+
+void
+radv_sdma_copy_buffer(struct radv_device *device, struct radeon_cmdbuf *cs, uint64_t src_va,
+                      uint64_t dst_va, uint64_t size)
+{
+   if (size == 0)
+      return;
+
+   enum amd_gfx_level gfx_level = device->physical_device->rad_info.gfx_level;
+   unsigned max_size_per_packet =
+      gfx_level >= GFX10_3 ? GFX103_SDMA_COPY_MAX_SIZE : CIK_SDMA_COPY_MAX_SIZE;
+   unsigned align = ~0u;
+   unsigned ncopy = DIV_ROUND_UP(size, max_size_per_packet);
+   bool tmz = false;
+
+   assert(gfx_level >= GFX7);
+
+   /* Align copy size to dw if src/dst address are dw aligned */
+   if ((src_va & 0x3) == 0 && (src_va & 0x3) == 0 && size > 4 && (size & 3) != 0) {
+      align = ~0x3u;
+      ncopy++;
+   }
+
+   radeon_check_space(device->ws, cs, ncopy * 7);
+
+   for (unsigned i = 0; i < ncopy; i++) {
+      unsigned csize = size >= 4 ? MIN2(size & align, max_size_per_packet) : size;
+      radeon_emit(cs, CIK_SDMA_PACKET(CIK_SDMA_OPCODE_COPY, CIK_SDMA_COPY_SUB_OPCODE_LINEAR,
+                                      (tmz ? 1u : 0) << 2));
+      radeon_emit(cs, gfx_level >= GFX9 ? csize - 1 : csize);
+      radeon_emit(cs, 0); /* src/dst endian swap */
+      radeon_emit(cs, src_va);
+      radeon_emit(cs, src_va >> 32);
+      radeon_emit(cs, dst_va);
+      radeon_emit(cs, dst_va >> 32);
+      dst_va += csize;
+      src_va += csize;
+      size -= csize;
+   }
+}
\ No newline at end of file
-- 
GitLab


From 851b07a79a53c32fab5dbff5c3c76f14bae2fec9 Mon Sep 17 00:00:00 2001
From: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
Date: Fri, 24 Feb 2023 14:56:20 +0900
Subject: [PATCH 3/4] radv: Upload shaders to invisible VRAM on small BAR
 systems.

Following PAL's implementation, this patch avoids allocating shader code
buffers in BAR and use compute DMA to upload them to invisible VRAM
directly.

For some games like HZD, shaders can take as much as 400MB, which exceeds
the non-resizable BAR size (256MB) and cause inconsistent spilling
behavior. The kernel will normally move these to invisible VRAM on its own,
but there are a few cases that it does not reliably happen. This patch does
the moving explicitly in the driver to ensure predictable results.

In this patch, we upload the shaders synchronously; so the shader will be
ready as soon as vkCreate*Pipeline returns. A following patch will make
this asynchronous and don't block until we see a use of the pipeline.

As a side effect, when SQTT is used we now store the shaders on a cacheable
buffer which would speed up writing the trace to the disk.
---
 docs/envvars.rst                        |   2 +
 src/amd/vulkan/layers/radv_sqtt_layer.c |  30 +-
 src/amd/vulkan/radv_constants.h         |   2 +
 src/amd/vulkan/radv_debug.h             |   1 +
 src/amd/vulkan/radv_device.c            |  24 +-
 src/amd/vulkan/radv_private.h           |  23 ++
 src/amd/vulkan/radv_shader.c            | 346 ++++++++++++++++++++++--
 src/amd/vulkan/radv_shader.h            |  23 +-
 8 files changed, 412 insertions(+), 39 deletions(-)

diff --git a/docs/envvars.rst b/docs/envvars.rst
index 8c2ce3a6f7c9..a0414ef24243 100644
--- a/docs/envvars.rst
+++ b/docs/envvars.rst
@@ -1145,6 +1145,8 @@ RADV driver environment variables
       enable wave32 for compute shaders (GFX10+)
    ``dccmsaa``
       enable DCC for MSAA images
+   ``dmashaders``
+      upload shaders to invisible VRAM (might be useful for non-resizable BAR systems)
    ``emulate_rt``
       forces ray-tracing to be emulated in software on GFX10_3+ and enables
       rt extensions with older hardware.
diff --git a/src/amd/vulkan/layers/radv_sqtt_layer.c b/src/amd/vulkan/layers/radv_sqtt_layer.c
index a6c6eb6da8d6..f39afed6c828 100644
--- a/src/amd/vulkan/layers/radv_sqtt_layer.c
+++ b/src/amd/vulkan/layers/radv_sqtt_layer.c
@@ -144,6 +144,7 @@ static VkResult
 radv_sqtt_reloc_graphics_shaders(struct radv_device *device,
                                  struct radv_graphics_pipeline *pipeline)
 {
+   struct radv_shader_dma_submission *submission;
    struct radv_sqtt_shaders_reloc *reloc;
    uint32_t code_size = 0;
 
@@ -170,21 +171,38 @@ radv_sqtt_reloc_graphics_shaders(struct radv_device *device,
    reloc->bo = reloc->alloc->arena->bo;
 
    /* Relocate shader binaries to be contiguous in memory as requested by RGP. */
-   uint64_t slab_va = radv_buffer_get_va(reloc->bo);
-   uint32_t slab_offset = reloc->alloc->offset;
-   char *slab_ptr = reloc->alloc->arena->ptr;
+   uint64_t slab_va = radv_buffer_get_va(reloc->bo) + reloc->alloc->offset;
+   char *slab_ptr = reloc->alloc->arena->ptr + reloc->alloc->offset;
+   uint64_t offset = 0;
+
+   if (device->shader_use_invisible_vram) {
+       submission =
+         radv_shader_dma_get_submission(device, reloc->bo, slab_va, code_size);
+      if (!submission)
+         return VK_ERROR_UNKNOWN;
+   }
 
    for (int i = 0; i < MESA_VULKAN_SHADER_STAGES; ++i) {
       const struct radv_shader *shader = pipeline->base.shaders[i];
+      void *dest_ptr;
       if (!shader)
          continue;
 
-      reloc->va[i] = slab_va + slab_offset;
+      reloc->va[i] = slab_va + offset;
+
+      if (device->shader_use_invisible_vram)
+         dest_ptr = submission->ptr + offset;
+      else
+         dest_ptr = slab_ptr + offset;
 
-      void *dest_ptr = slab_ptr + slab_offset;
       memcpy(dest_ptr, shader->code, shader->code_size);
 
-      slab_offset += align(shader->code_size, RADV_SHADER_ALLOC_ALIGNMENT);
+      offset += align(shader->code_size, RADV_SHADER_ALLOC_ALIGNMENT);
+   }
+
+   if (device->shader_use_invisible_vram) {
+      if (!radv_shader_dma_submit(device, submission, NULL))
+         return VK_ERROR_UNKNOWN;
    }
 
    pipeline->sqtt_shaders_reloc = reloc;
diff --git a/src/amd/vulkan/radv_constants.h b/src/amd/vulkan/radv_constants.h
index 2cb211a7b4ae..7bc41054f56a 100644
--- a/src/amd/vulkan/radv_constants.h
+++ b/src/amd/vulkan/radv_constants.h
@@ -137,6 +137,8 @@
 #define PERF_CTR_BO_LOCK_OFFSET  0
 #define PERF_CTR_BO_FENCE_OFFSET 8
 
+#define RADV_SHADER_UPLOAD_CS_COUNT 32
+
 /* NGG GDS counters:
  *   offset  0| 4| 8|12  - reserved for NGG streamout counters
  *   offset 16           - pipeline statistics counter for all streams
diff --git a/src/amd/vulkan/radv_debug.h b/src/amd/vulkan/radv_debug.h
index bd0e53c4a2dc..07d3033adbe8 100644
--- a/src/amd/vulkan/radv_debug.h
+++ b/src/amd/vulkan/radv_debug.h
@@ -87,6 +87,7 @@ enum {
    RADV_PERFTEST_GPL = 1u << 13,
    RADV_PERFTEST_NGG_STREAMOUT = 1u << 14,
    RADV_PERFTEST_VIDEO_DECODE = 1u << 15,
+   RADV_PERFTEST_DMA_SHADERS = 1u << 16,
 };
 
 bool radv_init_trace(struct radv_device *device);
diff --git a/src/amd/vulkan/radv_device.c b/src/amd/vulkan/radv_device.c
index 536b2155d809..9a77c93bbdd9 100644
--- a/src/amd/vulkan/radv_device.c
+++ b/src/amd/vulkan/radv_device.c
@@ -50,6 +50,8 @@
 #include "radv_private.h"
 #include "radv_shader.h"
 #include "vk_util.h"
+#include "vk_common_entrypoints.h"
+#include "vk_semaphore.h"
 #ifdef _WIN32
 typedef void *drmDevicePtr;
 #include <io.h>
@@ -1128,6 +1130,7 @@ static const struct debug_control radv_perftest_options[] = {{"localbos", RADV_P
                                                              {"gpl", RADV_PERFTEST_GPL},
                                                              {"ngg_streamout", RADV_PERFTEST_NGG_STREAMOUT},
                                                              {"video_decode", RADV_PERFTEST_VIDEO_DECODE},
+                                                             {"dmashaders", RADV_PERFTEST_DMA_SHADERS},
                                                              {NULL, 0}};
 
 const char *
@@ -3980,7 +3983,7 @@ radv_CreateDevice(VkPhysicalDevice physicalDevice, const VkDeviceCreateInfo *pCr
 
       result = device->ws->ctx_create(device->ws, priority, &device->hw_ctx[priority]);
       if (result != VK_SUCCESS)
-         goto fail;
+         goto fail_queue;
    }
 
    for (unsigned i = 0; i < pCreateInfo->queueCreateInfoCount; i++) {
@@ -3994,7 +3997,7 @@ radv_CreateDevice(VkPhysicalDevice physicalDevice, const VkDeviceCreateInfo *pCr
                   VK_SYSTEM_ALLOCATION_SCOPE_DEVICE);
       if (!device->queues[qfi]) {
          result = VK_ERROR_OUT_OF_HOST_MEMORY;
-         goto fail;
+         goto fail_queue;
       }
 
       memset(device->queues[qfi], 0, queue_create->queueCount * sizeof(struct radv_queue));
@@ -4004,11 +4007,19 @@ radv_CreateDevice(VkPhysicalDevice physicalDevice, const VkDeviceCreateInfo *pCr
       for (unsigned q = 0; q < queue_create->queueCount; q++) {
          result = radv_queue_init(device, &device->queues[qfi][q], q, queue_create, global_priority);
          if (result != VK_SUCCESS)
-            goto fail;
+            goto fail_queue;
       }
    }
    device->private_sdma_queue = VK_NULL_HANDLE;
 
+   device->shader_use_invisible_vram =
+      (device->instance->perftest_flags & RADV_PERFTEST_DMA_SHADERS) &&
+      /* SDMA buffer copy is only implemented for GFX7+. */
+      !(device->physical_device->rad_info.gfx_level >= GFX7);
+   result = radv_init_shader_upload_queue(device);
+   if (result != VK_SUCCESS)
+      goto fail;
+
    device->pbb_allowed = device->physical_device->rad_info.gfx_level >= GFX9 &&
                          !(device->instance->debug_flags & RADV_DEBUG_NOBINNING);
 
@@ -4256,6 +4267,9 @@ fail:
    radv_device_finish_ps_epilogs(device);
    radv_device_finish_border_color(device);
 
+   radv_destroy_shader_upload_queue(device);
+
+fail_queue:
    for (unsigned i = 0; i < RADV_MAX_QUEUE_FAMILIES; i++) {
       for (unsigned q = 0; q < device->queue_count[i]; q++)
          radv_queue_finish(&device->queues[i][q]);
@@ -4268,6 +4282,8 @@ fail:
          device->ws->ctx_destroy(device->hw_ctx[i]);
    }
 
+   radv_destroy_shader_arenas(device);
+
    _mesa_hash_table_destroy(device->rt_handles, NULL);
 
    simple_mtx_destroy(&device->pstate_mtx);
@@ -4329,6 +4345,8 @@ radv_DestroyDevice(VkDevice _device, const VkAllocationCallbacks *pAllocator)
    VkPipelineCache pc = radv_pipeline_cache_to_handle(device->mem_cache);
    radv_DestroyPipelineCache(radv_device_to_handle(device), pc, NULL);
 
+   radv_destroy_shader_upload_queue(device);
+
    radv_trap_handler_finish(device);
    radv_finish_trace(device);
 
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index 76cbba5d4295..ac0b7a1a56c2 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -814,6 +814,18 @@ struct radv_queue {
    struct radeon_winsys_bo *gang_sem_bo;
 };
 
+struct radv_shader_dma_submission {
+   struct list_head list;
+
+   struct radeon_cmdbuf *cs;
+   struct radeon_winsys_bo *bo;
+   uint64_t bo_size;
+   char *ptr;
+
+   /* The semaphore value to wait for before reusing this submission. */
+   uint64_t seq;
+};
+
 #define RADV_BORDER_COLOR_COUNT       4096
 #define RADV_BORDER_COLOR_BUFFER_SIZE (sizeof(VkClearColorValue) * RADV_BORDER_COLOR_COUNT)
 
@@ -946,6 +958,17 @@ struct radv_device {
    struct list_head shader_block_obj_pool;
    mtx_t shader_arena_mutex;
 
+   mtx_t shader_upload_hw_ctx_mutex;
+   struct radeon_winsys_ctx *shader_upload_hw_ctx;
+   VkSemaphore shader_upload_sem;
+   uint64_t shader_upload_seq;
+   struct list_head shader_dma_submissions;
+   mtx_t shader_dma_submission_list_mutex;
+   cnd_t shader_dma_submission_list_cond;
+
+   /* Whether to DMA shaders to invisible VRAM or to upload directly through BAR. */
+   bool shader_use_invisible_vram;
+
    /* For detecting VM faults reported by dmesg. */
    uint64_t dmesg_timestamp;
 
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 4fd2895e1101..385aae79f037 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -34,6 +34,7 @@
 #include "util/mesa-sha1.h"
 #include "util/u_atomic.h"
 #include "util/streaming-load-memcpy.h"
+#include "radv_cs.h"
 #include "radv_debug.h"
 #include "radv_meta.h"
 #include "radv_private.h"
@@ -48,6 +49,8 @@
 #include "aco_interface.h"
 #include "sid.h"
 #include "vk_format.h"
+#include "vk_sync.h"
+#include "vk_semaphore.h"
 
 #include "aco_shader_info.h"
 #include "radv_aco_shader_info.h"
@@ -1470,6 +1473,22 @@ free_block_obj(struct radv_device *device, union radv_shader_arena_block *block)
    list_add(&block->pool, &device->shader_block_obj_pool);
 }
 
+VkResult
+radv_shader_wait_for_upload(struct radv_device *device, uint64_t seq)
+{
+   if (!seq)
+      return VK_SUCCESS;
+
+   const VkSemaphoreWaitInfo wait_info = {
+      .sType = VK_STRUCTURE_TYPE_SEMAPHORE_WAIT_INFO,
+      .pSemaphores = &device->shader_upload_sem,
+      .semaphoreCount = 1,
+      .pValues = &seq,
+   };
+   return device->vk.dispatch_table.WaitSemaphores(radv_device_to_handle(device), &wait_info,
+                                                   UINT64_MAX);
+}
+
 /* Segregated fit allocator, implementing a good-fit allocation policy.
  *
  * This is an variation of sequential fit allocation with several lists of free blocks ("holes")
@@ -1545,21 +1564,29 @@ radv_alloc_shader_memory(struct radv_device *device, uint32_t size, void *ptr)
       MAX2(RADV_SHADER_ALLOC_MIN_ARENA_SIZE
               << MIN2(RADV_SHADER_ALLOC_MAX_ARENA_SIZE_SHIFT, device->shader_arena_shift),
            size);
-   VkResult result = device->ws->buffer_create(
-      device->ws, arena_size, RADV_SHADER_ALLOC_ALIGNMENT, RADEON_DOMAIN_VRAM,
-      RADEON_FLAG_NO_INTERPROCESS_SHARING | RADEON_FLAG_32BIT |
+   enum radeon_bo_flag flags = RADEON_FLAG_NO_INTERPROCESS_SHARING | RADEON_FLAG_32BIT;
+   if (device->shader_use_invisible_vram)
+      flags |= RADEON_FLAG_NO_CPU_ACCESS;
+   else
+      flags |=
          (device->physical_device->rad_info.cpdma_prefetch_writes_memory ? 0
-                                                                         : RADEON_FLAG_READ_ONLY),
-      RADV_BO_PRIORITY_SHADER, 0, &arena->bo);
+                                                                         : RADEON_FLAG_READ_ONLY);
+
+   VkResult result;
+   result =
+      device->ws->buffer_create(device->ws, arena_size, RADV_SHADER_ALLOC_ALIGNMENT,
+                                RADEON_DOMAIN_VRAM, flags, RADV_BO_PRIORITY_SHADER, 0, &arena->bo);
    if (result != VK_SUCCESS)
       goto fail;
    radv_rmv_log_bo_allocate(device, arena->bo, arena_size, true);
 
    list_inithead(&arena->entries);
 
-   arena->ptr = (char *)device->ws->buffer_map(arena->bo);
-   if (!arena->ptr)
-      goto fail;
+   if (!(flags & RADEON_FLAG_NO_CPU_ACCESS)) {
+      arena->ptr = (char *)device->ws->buffer_map(arena->bo);
+      if (!arena->ptr)
+         goto fail;
+   }
 
    alloc = alloc_block_obj(device);
    hole = arena_size - size > 0 ? alloc_block_obj(device) : alloc;
@@ -1685,6 +1712,85 @@ radv_destroy_shader_arenas(struct radv_device *device)
    mtx_destroy(&device->shader_arena_mutex);
 }
 
+VkResult
+radv_init_shader_upload_queue(struct radv_device *device)
+{
+   if (!device->shader_use_invisible_vram)
+      return VK_SUCCESS;
+
+   VkDevice vk_device = radv_device_to_handle(device);
+   struct radeon_winsys *ws = device->ws;
+
+   const struct vk_device_dispatch_table *disp = &device->vk.dispatch_table;
+   VkResult result = VK_SUCCESS;
+
+   enum radv_queue_family qf = RADV_QUEUE_COMPUTE;
+   result = ws->ctx_create(ws, RADEON_CTX_PRIORITY_MEDIUM, &device->shader_upload_hw_ctx);
+   if (result != VK_SUCCESS)
+      return result;
+   mtx_init(&device->shader_upload_hw_ctx_mutex, mtx_plain);
+
+   mtx_init(&device->shader_dma_submission_list_mutex, mtx_plain);
+   cnd_init(&device->shader_dma_submission_list_cond);
+   list_inithead(&device->shader_dma_submissions);
+
+   for (unsigned i = 0; i < RADV_SHADER_UPLOAD_CS_COUNT; i++) {
+      struct radv_shader_dma_submission *submission = calloc(1, sizeof(struct radv_shader_dma_submission));
+      submission->cs = ws->cs_create(ws, radv_queue_family_to_ring(device->physical_device, qf));
+      if (!submission->cs)
+         return VK_ERROR_OUT_OF_HOST_MEMORY;
+      list_addtail(&submission->list, &device->shader_dma_submissions);
+   }
+
+   const VkSemaphoreTypeCreateInfo sem_type = {
+      .sType = VK_STRUCTURE_TYPE_SEMAPHORE_TYPE_CREATE_INFO,
+      .semaphoreType = VK_SEMAPHORE_TYPE_TIMELINE,
+      .initialValue = 0,
+   };
+   const VkSemaphoreCreateInfo sem_create = {
+      .sType = VK_STRUCTURE_TYPE_SEMAPHORE_CREATE_INFO,
+      .pNext = &sem_type,
+   };
+   result = disp->CreateSemaphore(vk_device, &sem_create, NULL, &device->shader_upload_sem);
+   if (result != VK_SUCCESS)
+      return result;
+
+   return VK_SUCCESS;
+}
+
+void
+radv_destroy_shader_upload_queue(struct radv_device *device)
+{
+   if (!device->shader_use_invisible_vram)
+      return;
+
+   struct vk_device_dispatch_table *disp = &device->vk.dispatch_table;
+   struct radeon_winsys *ws = device->ws;
+
+   /* Upload queue should be idle assuming that pipelines are not leaked */
+   if (device->shader_upload_sem)
+      disp->DestroySemaphore(radv_device_to_handle(device), device->shader_upload_sem, NULL);
+
+   list_for_each_entry_safe(struct radv_shader_dma_submission, submission,
+                            &device->shader_dma_submissions, list)
+   {
+      if (submission->cs)
+         ws->cs_destroy(submission->cs);
+      if (submission->bo)
+         ws->buffer_destroy(ws, submission->bo);
+      list_del(&submission->list);
+      free(submission);
+   }
+
+   cnd_destroy(&device->shader_dma_submission_list_cond);
+   mtx_destroy(&device->shader_dma_submission_list_mutex);
+
+   if (device->shader_upload_hw_ctx) {
+      mtx_destroy(&device->shader_upload_hw_ctx_mutex);
+      ws->ctx_destroy(device->shader_upload_hw_ctx);
+   }
+}
+
 /* For the UMR disassembler. */
 #define DEBUGGER_END_OF_CODE_MARKER 0xbf9f0000 /* invalid instruction */
 #define DEBUGGER_NUM_MARKERS        5
@@ -2028,19 +2134,8 @@ radv_open_rtld_binary(struct radv_device *device, const struct radv_shader *shad
 
 static bool
 radv_shader_binary_upload(struct radv_device *device, const struct radv_shader_binary *binary,
-                          struct radv_shader *shader)
+                          struct radv_shader *shader, void *dest_ptr)
 {
-   void *dest_ptr;
-
-   shader->alloc = radv_alloc_shader_memory(device, shader->code_size, shader);
-   if (!shader->alloc)
-      return false;
-
-   shader->bo = shader->alloc->arena->bo;
-   shader->va = radv_buffer_get_va(shader->bo) + shader->alloc->offset;
-
-   dest_ptr = shader->alloc->arena->ptr + shader->alloc->offset;
-
    if (device->thread_trace.bo) {
       shader->code = calloc(shader->code_size, 1);
       if (!shader->code) {
@@ -2098,6 +2193,152 @@ radv_shader_binary_upload(struct radv_device *device, const struct radv_shader_b
    return true;
 }
 
+static VkResult
+radv_shader_dma_resize_upload_buf(struct radv_shader_dma_submission *submission,
+                                  struct radeon_winsys *ws, uint64_t size)
+{
+   if (submission->bo)
+      ws->buffer_destroy(ws, submission->bo);
+
+   VkResult result =
+      ws->buffer_create(ws, size, RADV_SHADER_ALLOC_ALIGNMENT, RADEON_DOMAIN_GTT,
+                        RADEON_FLAG_CPU_ACCESS | RADEON_FLAG_NO_INTERPROCESS_SHARING |
+                           RADEON_FLAG_32BIT | RADEON_FLAG_GTT_WC,
+                        RADV_BO_PRIORITY_UPLOAD_BUFFER, 0, &submission->bo);
+   if (unlikely(result != VK_SUCCESS))
+      return result;
+
+   submission->ptr = ws->buffer_map(submission->bo);
+   submission->bo_size = size;
+
+   return VK_SUCCESS;
+}
+
+struct radv_shader_dma_submission *
+radv_shader_dma_pop_submission(struct radv_device *device)
+{
+   struct radv_shader_dma_submission *submission;
+
+   mtx_lock(&device->shader_dma_submission_list_mutex);
+
+   while (list_is_empty(&device->shader_dma_submissions))
+      cnd_wait(&device->shader_dma_submission_list_cond, &device->shader_dma_submission_list_mutex);
+
+   submission =
+      list_first_entry(&device->shader_dma_submissions, struct radv_shader_dma_submission, list);
+   list_del(&submission->list);
+
+   mtx_unlock(&device->shader_dma_submission_list_mutex);
+
+   return submission;
+}
+
+void
+radv_shader_dma_push_submission(struct radv_device *device,
+                                struct radv_shader_dma_submission *submission, uint64_t seq)
+{
+   submission->seq = seq;
+
+   mtx_lock(&device->shader_dma_submission_list_mutex);
+
+   list_addtail(&submission->list, &device->shader_dma_submissions);
+   cnd_signal(&device->shader_dma_submission_list_cond);
+
+   mtx_unlock(&device->shader_dma_submission_list_mutex);
+}
+
+struct radv_shader_dma_submission *
+radv_shader_dma_get_submission(struct radv_device *device, struct radeon_winsys_bo *bo, uint64_t va,
+                               uint64_t size)
+{
+   struct radv_shader_dma_submission *submission = radv_shader_dma_pop_submission(device);
+   struct radeon_cmdbuf *cs = submission->cs;
+   struct radeon_winsys *ws = device->ws;
+   VkResult result;
+
+   /* Wait for potentially in-flight submission to settle */
+   result = radv_shader_wait_for_upload(device, submission->seq);
+   if (unlikely(result != VK_SUCCESS))
+      goto fail;
+
+   ws->cs_reset(cs);
+
+   if (submission->bo_size < size) {
+      result = radv_shader_dma_resize_upload_buf(submission, ws, size);
+      if (unlikely(result != VK_SUCCESS))
+         goto fail;
+   }
+
+   radv_sdma_copy_buffer(device, cs, radv_buffer_get_va(submission->bo), va, size);
+   radv_cs_add_buffer(ws, cs, submission->bo);
+   radv_cs_add_buffer(ws, cs, bo);
+
+   result = ws->cs_finalize(cs);
+   if (unlikely(result != VK_SUCCESS))
+      goto fail;
+
+   return submission;
+
+fail:
+   radv_shader_dma_push_submission(device, submission, 0);
+
+   return NULL;
+}
+
+/*
+ * If upload_seq_out is NULL, this function blocks until the DMA is complete. Otherwise, the
+ * semaphore value to wait on device->shader_upload_sem is stored in *upload_seq_out.
+ */
+bool
+radv_shader_dma_submit(struct radv_device *device, struct radv_shader_dma_submission *submission,
+                       uint64_t *upload_seq_out)
+{
+   struct radeon_cmdbuf *cs = submission->cs;
+   struct radeon_winsys *ws = device->ws;
+   VkResult result;
+
+   mtx_lock(&device->shader_upload_hw_ctx_mutex);
+
+   uint64_t upload_seq = device->shader_upload_seq + 1;
+
+   struct vk_semaphore *semaphore = vk_semaphore_from_handle(device->shader_upload_sem);
+   struct vk_sync *sync = vk_semaphore_get_active_sync(semaphore);
+   const struct vk_sync_signal signal_info = {
+      .sync = sync,
+      .signal_value = upload_seq,
+      .stage_mask = VK_PIPELINE_STAGE_2_ALL_COMMANDS_BIT,
+   };
+
+   struct radv_winsys_submit_info submit = {
+      .ip_type = AMD_IP_COMPUTE,
+      .queue_index = 0,
+      .cs_array = &cs,
+      .cs_count = 1,
+   };
+
+   result = ws->cs_submit(device->shader_upload_hw_ctx, &submit, 0, NULL, 1, &signal_info, false);
+   mtx_unlock(&device->shader_upload_hw_ctx_mutex);
+   if (unlikely(result != VK_SUCCESS))
+   {
+      radv_shader_dma_push_submission(device, submission, 0);
+      return false;
+   }
+   device->shader_upload_seq = upload_seq;
+
+   radv_shader_dma_push_submission(device, submission, upload_seq);
+
+   if (upload_seq_out) {
+      *upload_seq_out = upload_seq;
+   } else {
+      result = radv_shader_wait_for_upload(device, upload_seq);
+      if (unlikely(result != VK_SUCCESS))
+         return false;
+   }
+
+   return true;
+}
+
+
 struct radv_shader *
 radv_shader_create(struct radv_device *device, const struct radv_shader_binary *binary,
                    bool keep_shader_info, bool from_cache, const struct radv_shader_args *args)
@@ -2207,9 +2448,33 @@ radv_shader_create(struct radv_device *device, const struct radv_shader_binary *
       }
    }
 
-   if (!radv_shader_binary_upload(device, binary, shader))
+   shader->alloc = radv_alloc_shader_memory(device, shader->code_size, shader);
+   if (!shader->alloc)
       return NULL;
 
+   shader->bo = shader->alloc->arena->bo;
+   shader->va = radv_buffer_get_va(shader->bo) + shader->alloc->offset;
+
+   if (device->shader_use_invisible_vram) {
+      struct radv_shader_dma_submission *submission =
+         radv_shader_dma_get_submission(device, shader->bo, shader->va, shader->code_size);
+      if (!submission)
+         return NULL;
+
+      if (!radv_shader_binary_upload(device, binary, shader, submission->ptr)) {
+         radv_shader_dma_push_submission(device, submission, 0);
+         return NULL;
+      }
+
+      if (!radv_shader_dma_submit(device, submission, NULL))
+         return NULL;
+   } else {
+      void *dest_ptr = shader->alloc->arena->ptr + shader->alloc->offset;
+
+      if (!radv_shader_binary_upload(device, binary, shader, dest_ptr))
+         return NULL;
+   }
+
    return shader;
 }
 
@@ -2235,15 +2500,38 @@ radv_shader_part_create(struct radv_shader_part_binary *binary, unsigned wave_si
    return shader_part;
 }
 
-void
-radv_shader_part_binary_upload(const struct radv_shader_part_binary *binary, void *dest_ptr)
+bool
+radv_shader_part_binary_upload(struct radv_device *device, struct radv_shader_part *shader_part)
 {
-   memcpy(dest_ptr, binary->data, binary->code_size);
+   const struct radv_shader_part_binary *bin = shader_part->binary;
+   uint32_t code_size = radv_get_shader_binary_size(bin->code_size);
+   struct radv_shader_dma_submission *submission;
+   void *dest_ptr;
+
+   if (device->shader_use_invisible_vram) {
+      uint64_t va = radv_buffer_get_va(shader_part->alloc->arena->bo) + shader_part->alloc->offset;
+      submission =
+         radv_shader_dma_get_submission(device, shader_part->alloc->arena->bo, va, code_size);
+      if (!submission)
+         return false;
+
+      dest_ptr = submission->ptr;
+   } else {
+      dest_ptr = shader_part->alloc->arena->ptr + shader_part->alloc->offset;
+   }
 
+   memcpy(dest_ptr, bin->data, bin->code_size);
    /* Add end-of-code markers for the UMR disassembler. */
-   uint32_t *ptr32 = (uint32_t *)dest_ptr + binary->code_size / 4;
+   uint32_t *ptr32 = (uint32_t *)dest_ptr + code_size / 4;
    for (unsigned i = 0; i < DEBUGGER_NUM_MARKERS; i++)
       ptr32[i] = DEBUGGER_END_OF_CODE_MARKER;
+
+   if (device->shader_use_invisible_vram) {
+      if (!radv_shader_dma_submit(device, submission, NULL))
+         return false;
+   }
+
+   return true;
 }
 
 static char *
@@ -2562,8 +2850,8 @@ radv_create_vs_prolog(struct radv_device *device, const struct radv_vs_prolog_ke
    prolog->bo = prolog->alloc->arena->bo;
    prolog->va = radv_buffer_get_va(prolog->bo) + prolog->alloc->offset;
 
-   void *dest_ptr = prolog->alloc->arena->ptr + prolog->alloc->offset;
-   radv_shader_part_binary_upload(binary, dest_ptr);
+   if (!radv_shader_part_binary_upload(device, prolog))
+      goto fail_alloc;
 
    if (options.dump_shader) {
       fprintf(stderr, "Vertex prolog");
@@ -2627,8 +2915,8 @@ radv_create_ps_epilog(struct radv_device *device, const struct radv_ps_epilog_ke
    epilog->bo = epilog->alloc->arena->bo;
    epilog->va = radv_buffer_get_va(epilog->bo) + epilog->alloc->offset;
 
-   void *dest_ptr = epilog->alloc->arena->ptr + epilog->alloc->offset;
-   radv_shader_part_binary_upload(binary, dest_ptr);
+   if (!radv_shader_part_binary_upload(device, epilog))
+      goto fail_alloc;
 
    if (options.dump_shader) {
       fprintf(stderr, "Fragment epilog");
diff --git a/src/amd/vulkan/radv_shader.h b/src/amd/vulkan/radv_shader.h
index dee8a42ab7e3..65da28503765 100644
--- a/src/amd/vulkan/radv_shader.h
+++ b/src/amd/vulkan/radv_shader.h
@@ -555,6 +555,9 @@ bool radv_nir_lower_vs_inputs(nir_shader *shader, const struct radv_pipeline_sta
 
 void radv_init_shader_arenas(struct radv_device *device);
 void radv_destroy_shader_arenas(struct radv_device *device);
+VkResult
+radv_init_shader_upload_queue(struct radv_device *device);
+void radv_destroy_shader_upload_queue(struct radv_device *device);
 
 struct radv_pipeline_shader_stack_size;
 struct radv_compute_pipeline;
@@ -578,7 +581,25 @@ struct radv_shader *radv_shader_nir_to_asm(
    int shader_count, const struct radv_pipeline_key *key, bool keep_shader_info, bool keep_statistic_info,
    struct radv_shader_binary **binary_out);
 
-void radv_shader_part_binary_upload(const struct radv_shader_part_binary *binary, void *dest_ptr);
+VkResult radv_shader_wait_for_upload(struct radv_device *device, uint64_t seq);
+
+bool radv_shader_part_binary_upload(struct radv_device *device,
+                                    struct radv_shader_part *shader_part);
+
+struct radv_shader_dma_submission *
+radv_shader_dma_pop_submission(struct radv_device *device);
+
+void radv_shader_dma_push_submission(struct radv_device *device,
+                                     struct radv_shader_dma_submission *submission,
+                                     uint64_t seq);
+
+struct radv_shader_dma_submission *radv_shader_dma_get_submission(struct radv_device *device,
+                                                                  struct radeon_winsys_bo *bo,
+                                                                  uint64_t va, uint64_t size);
+
+bool radv_shader_dma_submit(struct radv_device *device,
+                            struct radv_shader_dma_submission *submission,
+                            uint64_t *upload_seq_out);
 
 union radv_shader_arena_block *radv_alloc_shader_memory(struct radv_device *device, uint32_t size,
                                                         void *ptr);
-- 
GitLab


From f268cab953118c87765b7c9a828eb67b85177c6c Mon Sep 17 00:00:00 2001
From: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
Date: Tue, 12 Jul 2022 17:25:00 +0900
Subject: [PATCH 4/4] radv: Wait for shader uploads asynchronously.

This introduces tracking of the required semaphore values in pipelines,
which is then propagated to cmd_buffers on bind. Each queue also keeps
track the maximum count it has waited for, so that we can avoid the waiting
overhead once all the shaders are loaded and referenced.
---
 src/amd/vulkan/layers/radv_sqtt_layer.c |  2 +-
 src/amd/vulkan/radv_cmd_buffer.c        | 11 +++++++
 src/amd/vulkan/radv_device.c            | 39 +++++++++++++++++++++++--
 src/amd/vulkan/radv_pipeline.c          |  9 ++++++
 src/amd/vulkan/radv_private.h           |  6 ++++
 src/amd/vulkan/radv_shader.c            | 14 +++++++--
 src/amd/vulkan/radv_shader.h            |  3 ++
 7 files changed, 79 insertions(+), 5 deletions(-)

diff --git a/src/amd/vulkan/layers/radv_sqtt_layer.c b/src/amd/vulkan/layers/radv_sqtt_layer.c
index f39afed6c828..8a3f7da2504a 100644
--- a/src/amd/vulkan/layers/radv_sqtt_layer.c
+++ b/src/amd/vulkan/layers/radv_sqtt_layer.c
@@ -201,7 +201,7 @@ radv_sqtt_reloc_graphics_shaders(struct radv_device *device,
    }
 
    if (device->shader_use_invisible_vram) {
-      if (!radv_shader_dma_submit(device, submission, NULL))
+      if (!radv_shader_dma_submit(device, submission, &pipeline->base.shader_upload_seq))
          return VK_ERROR_UNKNOWN;
    }
 
diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index acd1d3931f76..8f7d1b80ba1d 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -420,6 +420,7 @@ radv_reset_cmd_buffer(struct vk_command_buffer *vk_cmd_buffer,
    cmd_buffer->ace_internal.sem.gfx2ace_value = 0;
    cmd_buffer->ace_internal.sem.emitted_gfx2ace_value = 0;
    cmd_buffer->ace_internal.sem.va = 0;
+   cmd_buffer->shader_upload_seq = 0;
 
    if (cmd_buffer->upload.upload_bo)
       radv_cs_add_buffer(cmd_buffer->device->ws, cmd_buffer->cs, cmd_buffer->upload.upload_bo);
@@ -1844,6 +1845,8 @@ radv_emit_ps_epilog_state(struct radv_cmd_buffer *cmd_buffer, struct radv_shader
    radv_emit_shader_pointer(cmd_buffer->device, cmd_buffer->cs, base_reg + loc->sgpr_idx * 4,
                             ps_epilog->va, false);
 
+   cmd_buffer->shader_upload_seq = MAX2(cmd_buffer->shader_upload_seq, ps_epilog->upload_seq);
+
    cmd_buffer->state.emitted_ps_epilog = ps_epilog;
 }
 
@@ -3877,6 +3880,8 @@ radv_emit_vertex_input(struct radv_cmd_buffer *cmd_buffer, bool pipeline_is_dirt
    emit_prolog_regs(cmd_buffer, vs_shader, prolog, pipeline_is_dirty);
    emit_prolog_inputs(cmd_buffer, vs_shader, nontrivial_divisors, pipeline_is_dirty);
 
+   cmd_buffer->shader_upload_seq = MAX2(cmd_buffer->shader_upload_seq, prolog->upload_seq);
+
    cmd_buffer->state.emitted_vs_prolog = prolog;
 
    if (unlikely(cmd_buffer->device->trace_bo))
@@ -6346,6 +6351,10 @@ radv_CmdBindPipeline(VkCommandBuffer commandBuffer, VkPipelineBindPoint pipeline
       assert(!"invalid bind point");
       break;
    }
+
+   if (pipeline && cmd_buffer->device->shader_use_invisible_vram)
+      cmd_buffer->shader_upload_seq =
+         MAX2(cmd_buffer->shader_upload_seq, pipeline->shader_upload_seq);
 }
 
 VKAPI_ATTR void VKAPI_CALL
@@ -7125,6 +7134,8 @@ radv_CmdExecuteCommands(VkCommandBuffer commandBuffer, uint32_t commandBufferCou
       if (secondary->gds_oa_needed)
          primary->gds_oa_needed = true;
 
+      primary->shader_upload_seq = MAX2(primary->shader_upload_seq, secondary->shader_upload_seq);
+
       if (!secondary->state.render.has_image_views && primary->state.render.active &&
           (primary->state.dirty & RADV_CMD_DIRTY_FRAMEBUFFER)) {
          /* Emit the framebuffer state from primary if secondary
diff --git a/src/amd/vulkan/radv_device.c b/src/amd/vulkan/radv_device.c
index 9a77c93bbdd9..2fec78401ccd 100644
--- a/src/amd/vulkan/radv_device.c
+++ b/src/amd/vulkan/radv_device.c
@@ -5863,6 +5863,19 @@ radv_queue_submit_empty(struct radv_queue *queue, struct vk_queue_submit *submis
                                        submission->signal_count, submission->signals, false);
 }
 
+static void
+radv_get_shader_upload_sync_wait(struct radv_device *device, uint64_t shader_upload_seq,
+                                 struct vk_sync_wait *out_sync_wait)
+{
+   struct vk_semaphore *semaphore = vk_semaphore_from_handle(device->shader_upload_sem);
+   struct vk_sync *sync = vk_semaphore_get_active_sync(semaphore);
+   *out_sync_wait = (struct vk_sync_wait){
+      .sync = sync,
+      .wait_value = shader_upload_seq,
+      .stage_mask = VK_PIPELINE_STAGE_2_ALL_COMMANDS_BIT,
+   };
+}
+
 static VkResult
 radv_queue_submit_normal(struct radv_queue *queue, struct vk_queue_submit *submission)
 {
@@ -5871,6 +5884,9 @@ radv_queue_submit_normal(struct radv_queue *queue, struct vk_queue_submit *submi
    bool use_ace = false;
    bool use_perf_counters = false;
    VkResult result;
+   uint64_t shader_upload_seq = 0;
+   uint32_t wait_count = submission->wait_count;
+   struct vk_sync_wait *waits = submission->waits;
 
    result = radv_update_preambles(&queue->state, queue->device, submission->command_buffers,
                                   submission->command_buffer_count, &use_perf_counters, &use_ace);
@@ -5900,6 +5916,20 @@ radv_queue_submit_normal(struct radv_queue *queue, struct vk_queue_submit *submi
    if (queue->device->trace_bo)
       simple_mtx_lock(&queue->device->trace_mtx);
 
+   for (uint32_t j = 0; j < submission->command_buffer_count; j++) {
+      struct radv_cmd_buffer *cmd_buffer = (struct radv_cmd_buffer *)submission->command_buffers[j];
+      shader_upload_seq = MAX2(shader_upload_seq, cmd_buffer->shader_upload_seq);
+   }
+
+   if (shader_upload_seq > queue->last_shader_upload_seq) {
+      /* Patch the wait array to add waiting for referenced shaders to upload. */
+      waits = malloc(sizeof(struct vk_sync_wait) * (wait_count + 1));
+      wait_count += 1;
+      memcpy(waits, submission->waits, sizeof(struct vk_sync_wait) * submission->wait_count);
+      radv_get_shader_upload_sync_wait(queue->device, shader_upload_seq,
+                                       &waits[submission->wait_count]);
+   }
+
    struct radeon_cmdbuf *perf_ctr_lock_cs = NULL;
    struct radeon_cmdbuf *perf_ctr_unlock_cs = NULL;
 
@@ -5925,7 +5955,7 @@ radv_queue_submit_normal(struct radv_queue *queue, struct vk_queue_submit *submi
    /* For fences on the same queue/vm amdgpu doesn't wait till all processing is finished
     * before starting the next cmdbuffer, so we need to do it here.
     */
-   const bool need_wait = submission->wait_count > 0;
+   const bool need_wait = wait_count > 0;
    unsigned num_preambles = 0;
    struct radeon_cmdbuf *preambles[4] = {0};
 
@@ -6000,7 +6030,7 @@ radv_queue_submit_normal(struct radv_queue *queue, struct vk_queue_submit *submi
       submit.preamble_count = submit_ace ? num_preambles : num_1q_preambles;
 
       result = queue->device->ws->cs_submit(
-         ctx, &submit, j == 0 ? submission->wait_count : 0, submission->waits,
+         ctx, &submit, j == 0 ? wait_count : 0, waits,
          last_submit ? submission->signal_count : 0, submission->signals, can_patch);
 
       if (result != VK_SUCCESS)
@@ -6018,8 +6048,13 @@ radv_queue_submit_normal(struct radv_queue *queue, struct vk_queue_submit *submi
       preambles[1] = !use_ace ? NULL : queue->ace_internal_state->initial_preamble_cs;
    }
 
+   queue->last_shader_upload_seq =
+      MAX2(queue->last_shader_upload_seq, shader_upload_seq);
+
 fail:
    free(cs_array);
+   if (waits != submission->waits)
+      free(waits);
    if (queue->device->trace_bo)
       simple_mtx_unlock(&queue->device->trace_mtx);
 
diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index 0f13744f021f..8fdb4dd12adc 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -5112,6 +5112,13 @@ radv_graphics_pipeline_init(struct radv_graphics_pipeline *pipeline, struct radv
    pipeline->base.push_constant_size = pipeline_layout.push_constant_size;
    pipeline->base.dynamic_offset_count = pipeline_layout.dynamic_offset_count;
 
+   for (unsigned i = 0; i < MESA_SHADER_COMPUTE; i++) {
+      if (pipeline->base.shaders[i]) {
+         pipeline->base.shader_upload_seq = MAX2(pipeline->base.shader_upload_seq,
+                                                 pipeline->base.shaders[i]->upload_seq);
+      }
+   }
+
    if (extra) {
       radv_pipeline_init_extra(pipeline, extra, &blend, &state, &vgt_gs_out_prim_type);
    }
@@ -5378,6 +5385,8 @@ radv_compute_pipeline_init(struct radv_compute_pipeline *pipeline,
    pipeline->base.push_constant_size = layout->push_constant_size;
    pipeline->base.dynamic_offset_count = layout->dynamic_offset_count;
 
+   pipeline->base.shader_upload_seq = pipeline->base.shaders[MESA_SHADER_COMPUTE]->upload_seq;
+
    if (device->physical_device->rad_info.has_cs_regalloc_hang_bug) {
       struct radv_shader *compute_shader = pipeline->base.shaders[MESA_SHADER_COMPUTE];
       unsigned *cs_block_size = compute_shader->info.cs.block_size;
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index ac0b7a1a56c2..f0099e23c1f1 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -812,6 +812,8 @@ struct radv_queue {
    struct radv_queue_state state;
    struct radv_queue_state *ace_internal_state;
    struct radeon_winsys_bo *gang_sem_bo;
+
+   uint64_t last_shader_upload_seq;
 };
 
 struct radv_shader_dma_submission {
@@ -1735,6 +1737,8 @@ struct radv_cmd_buffer {
       struct radv_video_session *vid;
       struct radv_video_session_params *params;
    } video;
+
+   uint64_t shader_upload_seq;
 };
 
 extern const struct vk_command_buffer_ops radv_cmd_buffer_ops;
@@ -2118,6 +2122,8 @@ struct radv_pipeline {
    struct radv_shader *shaders[MESA_VULKAN_SHADER_STAGES];
    struct radv_shader *gs_copy_shader;
 
+   uint64_t shader_upload_seq;
+
    struct radeon_cmdbuf cs;
    uint32_t ctx_cs_hash;
    struct radeon_cmdbuf ctx_cs;
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 385aae79f037..8d9e9a136781 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -2466,7 +2466,7 @@ radv_shader_create(struct radv_device *device, const struct radv_shader_binary *
          return NULL;
       }
 
-      if (!radv_shader_dma_submit(device, submission, NULL))
+      if (!radv_shader_dma_submit(device, submission, &shader->upload_seq))
          return NULL;
    } else {
       void *dest_ptr = shader->alloc->arena->ptr + shader->alloc->offset;
@@ -2527,7 +2527,7 @@ radv_shader_part_binary_upload(struct radv_device *device, struct radv_shader_pa
       ptr32[i] = DEBUGGER_END_OF_CODE_MARKER;
 
    if (device->shader_use_invisible_vram) {
-      if (!radv_shader_dma_submit(device, submission, NULL))
+      if (!radv_shader_dma_submit(device, submission, &shader_part->upload_seq))
          return false;
    }
 
@@ -2940,6 +2940,11 @@ radv_shader_destroy(struct radv_device *device, struct radv_shader *shader)
 {
    assert(shader->ref_count == 0);
 
+   if (device->shader_use_invisible_vram) {
+      /* Wait for any pending upload to complete, or we'll be writing into freed shader memory. */
+      radv_shader_wait_for_upload(device, shader->upload_seq);
+   }
+
    radv_free_shader_memory(device, shader->alloc);
 
    free(shader->code);
@@ -2956,6 +2961,11 @@ radv_shader_part_destroy(struct radv_device *device, struct radv_shader_part *sh
 {
    assert(shader_part->ref_count == 0);
 
+   if (device->shader_use_invisible_vram) {
+      /* Wait for any pending upload to complete, or we'll be writing into freed shader memory. */
+      radv_shader_wait_for_upload(device, shader_part->upload_seq);
+   }
+
    if (shader_part->alloc)
       radv_free_shader_memory(device, shader_part->alloc);
    free(shader_part->binary);
diff --git a/src/amd/vulkan/radv_shader.h b/src/amd/vulkan/radv_shader.h
index 65da28503765..4bf34a3c5340 100644
--- a/src/amd/vulkan/radv_shader.h
+++ b/src/amd/vulkan/radv_shader.h
@@ -491,6 +491,8 @@ struct radv_shader {
    union radv_shader_arena_block *alloc;
    uint64_t va;
 
+   uint64_t upload_seq;
+
    struct ac_shader_config config;
    uint32_t code_size;
    uint32_t exec_size;
@@ -520,6 +522,7 @@ struct radv_shader_part {
    uint8_t num_preserved_sgprs;
    bool nontrivial_divisors;
    uint32_t spi_shader_col_format;
+   uint64_t upload_seq;
 
    struct radv_shader_part_binary *binary;
 
-- 
GitLab

