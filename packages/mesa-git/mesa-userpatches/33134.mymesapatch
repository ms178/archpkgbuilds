From 445a71389f8a9f5206e8173982218d4ec31853ce Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sun, 19 Jan 2025 00:03:01 -0500
Subject: [PATCH 01/10] nir: silence a warning in nir_opt_shrink_vectors

gcc assumes the array is a string, so it treats it as having 15 elements
instead of 16.
---
 src/compiler/nir/nir_opt_shrink_vectors.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/src/compiler/nir/nir_opt_shrink_vectors.c b/src/compiler/nir/nir_opt_shrink_vectors.c
index f420fa6285c60..879897696eb51 100644
--- a/src/compiler/nir/nir_opt_shrink_vectors.c
+++ b/src/compiler/nir/nir_opt_shrink_vectors.c
@@ -135,7 +135,7 @@ shrink_dest_to_read_mask(nir_def *def, bool shrink_start)
 
          /* Reswizzle sources, which must be ALU since they have swizzle */
          assert(first_bit + comps <= NIR_MAX_VEC_COMPONENTS);
-         uint8_t swizzle[NIR_MAX_VEC_COMPONENTS] = { 0 };
+         uint8_t swizzle[NIR_MAX_VEC_COMPONENTS + 1] = { 0 };
          for (unsigned i = 0; i < comps; ++i) {
             swizzle[first_bit + i] = i;
          }
-- 
GitLab


From f6c16934624ffe7c6f1251a6e9ff2e0fb5dac479 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 18 Jan 2025 22:20:30 -0500
Subject: [PATCH 02/10] nir: handle store_buffer_amd in
 nir_intrinsic_writes_external_memory

---
 src/compiler/nir/nir_gather_info.c | 1 +
 1 file changed, 1 insertion(+)

diff --git a/src/compiler/nir/nir_gather_info.c b/src/compiler/nir/nir_gather_info.c
index 0d03981e6ff50..76688989b020b 100644
--- a/src/compiler/nir/nir_gather_info.c
+++ b/src/compiler/nir/nir_gather_info.c
@@ -372,6 +372,7 @@ nir_intrinsic_writes_external_memory(const nir_intrinsic_instr *instr)
    case nir_intrinsic_store_global_etna:
    case nir_intrinsic_store_global_ir3:
    case nir_intrinsic_store_global_amd:
+   case nir_intrinsic_store_buffer_amd:
    case nir_intrinsic_store_ssbo:
    case nir_intrinsic_store_ssbo_ir3:
       return true;
-- 
GitLab


From b22a5f31b61a7a7e88e334153f6e5b0e61032376 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 18 Jan 2025 17:56:03 -0500
Subject: [PATCH 03/10] nir/group_loads: handle more loads

---
 src/compiler/nir/nir_group_loads.c | 25 ++++++++++++++++++-------
 1 file changed, 18 insertions(+), 7 deletions(-)

diff --git a/src/compiler/nir/nir_group_loads.c b/src/compiler/nir/nir_group_loads.c
index 867c7601a84fa..ebafcfa3271cf 100644
--- a/src/compiler/nir/nir_group_loads.c
+++ b/src/compiler/nir/nir_group_loads.c
@@ -76,24 +76,35 @@ static nir_instr *
 get_intrinsic_resource(nir_intrinsic_instr *intr)
 {
    /* This is also the list of intrinsics that are grouped. */
-   /* load_ubo is ignored because it's usually cheap. */
    switch (intr->intrinsic) {
+   /* Image loads. */
    case nir_intrinsic_image_load:
    case nir_intrinsic_image_deref_load:
+   case nir_intrinsic_bindless_image_load:
    case nir_intrinsic_image_sparse_load:
    case nir_intrinsic_image_deref_sparse_load:
-   /* Group image_size too because it has the same latency as cache hits. */
+   case nir_intrinsic_bindless_image_sparse_load:
+   /* Fragment mask loads. (samples_identical also loads it) */
+   case nir_intrinsic_image_fragment_mask_load_amd:
+   case nir_intrinsic_image_deref_fragment_mask_load_amd:
+   case nir_intrinsic_bindless_image_fragment_mask_load_amd:
    case nir_intrinsic_image_samples_identical:
    case nir_intrinsic_image_deref_samples_identical:
    case nir_intrinsic_bindless_image_samples_identical:
+   /* Image queries. (group them too since they can have cache hit latency) */
    case nir_intrinsic_image_size:
    case nir_intrinsic_image_deref_size:
-   case nir_intrinsic_bindless_image_load:
-   case nir_intrinsic_bindless_image_sparse_load:
+   case nir_intrinsic_bindless_image_size:
+   case nir_intrinsic_image_levels:
+   case nir_intrinsic_image_deref_levels:
+   case nir_intrinsic_bindless_image_levels:
+   case nir_intrinsic_image_samples:
+   case nir_intrinsic_image_deref_samples:
+   case nir_intrinsic_bindless_image_samples:
+   /* Other loads. */
+   /* load_ubo is ignored because it's usually cheap. */
    case nir_intrinsic_load_ssbo:
-   case nir_intrinsic_image_fragment_mask_load_amd:
-   case nir_intrinsic_image_deref_fragment_mask_load_amd:
-   case nir_intrinsic_bindless_image_fragment_mask_load_amd:
+   case nir_intrinsic_load_global:
       return intr->src[0].ssa->parent_instr;
    default:
       return NULL;
-- 
GitLab


From efa03cb57b7843c15549c3e668c3039735d5a5c4 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 18 Jan 2025 22:22:26 -0500
Subject: [PATCH 04/10] nir/group_loads: be more robust around non-reorderable
 intrinsics

This is more easily provable to be correct.
---
 src/compiler/nir/nir_group_loads.c | 42 ++++++++++++++++--------------
 1 file changed, 23 insertions(+), 19 deletions(-)

diff --git a/src/compiler/nir/nir_group_loads.c b/src/compiler/nir/nir_group_loads.c
index ebafcfa3271cf..063785b284b0b 100644
--- a/src/compiler/nir/nir_group_loads.c
+++ b/src/compiler/nir/nir_group_loads.c
@@ -311,23 +311,6 @@ handle_load_range(nir_instr **first, nir_instr **last,
    }
 }
 
-static bool
-is_barrier(nir_instr *instr)
-{
-   if (instr->type == nir_instr_type_intrinsic) {
-      nir_intrinsic_instr *intr = nir_instr_as_intrinsic(instr);
-      const char *name = nir_intrinsic_infos[intr->intrinsic].name;
-
-      if (intr->intrinsic == nir_intrinsic_terminate ||
-          intr->intrinsic == nir_intrinsic_terminate_if ||
-          /* TODO: nir_intrinsics.py could do this */
-          strstr(name, "barrier"))
-         return true;
-   }
-
-   return false;
-}
-
 struct indirection_state {
    nir_block *block;
    unsigned indirections;
@@ -431,8 +414,29 @@ process_block(nir_block *block, nir_load_grouping grouping,
        * between them out.
        */
       nir_foreach_instr(current, block) {
-         /* Don't group across barriers. */
-         if (is_barrier(current)) {
+         /* Don't group across non-reorderable instructions.
+          *
+          * TODO: This is overly conservative. There is a better way to do it
+          * because what we really do here is grouping reorderable instructions
+          * that can be around non-reorderable instructions, and doing that is
+          * perfectly legal.
+          *
+          * If you think about it, non-reorderable memory intrinsics have
+          * a hidden "use" on the previous non-reorderable memory intrinsic,
+          * and all non-reorderable memory intrinsics are chained that way.
+          * If we just added a chain src and a chain def to non-reorderable
+          * memory intrinsics and chained all such intrinsics that way,
+          * accidental reordering would be impossible because it would be
+          * def-after-use for the chain. Having just that would be enough
+          * to skip checking can_reorder because this algorithm would
+          * automatically do the correct thing if foreach_src and foreach_use
+          * included chain defs and srcs.
+          *
+          * Non-reorderable non-memory intrinsics that affect or are affected
+          * by e.g. load_helper_invocation could be another chain.
+          */
+         if (current->type == nir_instr_type_intrinsic &&
+             !nir_intrinsic_can_reorder(nir_instr_as_intrinsic(current))) {
             /* Group unconditionally.  */
             handle_load_range(&first_load, &last_load, NULL, 0);
             first_load = NULL;
-- 
GitLab


From 4f5c62990e5d0fd8453fdb336badbc2fcfd43491 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 18 Jan 2025 22:29:25 -0500
Subject: [PATCH 05/10] nir/group_loads: invert the return value of can_move to
 reflect its true meaning

The previous commit handles non-reorderable instructions better, so this is
no longer needed.
---
 src/compiler/nir/nir_group_loads.c | 28 +++++++---------------------
 1 file changed, 7 insertions(+), 21 deletions(-)

diff --git a/src/compiler/nir/nir_group_loads.c b/src/compiler/nir/nir_group_loads.c
index 063785b284b0b..08a4d6dd12ddb 100644
--- a/src/compiler/nir/nir_group_loads.c
+++ b/src/compiler/nir/nir_group_loads.c
@@ -126,26 +126,13 @@ is_grouped_load(nir_instr *instr)
 }
 
 static bool
-can_move(nir_instr *instr, uint8_t current_indirection_level)
+is_part_of_group(nir_instr *instr, uint8_t current_indirection_level)
 {
-   /* Grouping is done by moving everything else out of the first/last
-    * instruction range of the indirection level.
+   /* Grouping is done by moving everything else out of the first..last
+    * instruction range of the load group corresponding to the given
+    * indirection level.
     */
-   if (is_grouped_load(instr) && instr->pass_flags == current_indirection_level)
-      return false;
-
-   if (instr->type == nir_instr_type_alu ||
-       instr->type == nir_instr_type_deref ||
-       instr->type == nir_instr_type_tex ||
-       instr->type == nir_instr_type_load_const ||
-       instr->type == nir_instr_type_undef)
-      return true;
-
-   if (instr->type == nir_instr_type_intrinsic &&
-       nir_intrinsic_can_reorder(nir_instr_as_intrinsic(instr)))
-      return true;
-
-   return false;
+   return is_grouped_load(instr) && instr->pass_flags == current_indirection_level;
 }
 
 static nir_instr *
@@ -200,8 +187,7 @@ group_loads(nir_instr *first, nir_instr *last)
                                                    last->node.prev, node);
         instr != first;
         instr = exec_node_data_backward(nir_instr, instr->node.prev, node)) {
-      /* Only move instructions without side effects. */
-      if (!can_move(instr, first->pass_flags))
+      if (is_part_of_group(instr, first->pass_flags))
          continue;
 
       nir_def *def = nir_instr_def(instr);
@@ -242,7 +228,7 @@ group_loads(nir_instr *first, nir_instr *last)
         instr != last;
         instr = exec_node_data_forward(nir_instr, instr->node.next, node)) {
       /* Only move instructions without side effects. */
-      if (!can_move(instr, first->pass_flags))
+      if (is_part_of_group(instr, first->pass_flags))
          continue;
 
       if (nir_foreach_src(instr, has_only_sources_less_than, &state)) {
-- 
GitLab


From 82731e47ebd54bfcd63775c79064350f516eec90 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 18 Jan 2025 22:50:39 -0500
Subject: [PATCH 06/10] nir/group_loads: remove mostly duplicated function
 is_memory_load

is_grouped_load does the same thing
---
 src/compiler/nir/nir_group_loads.c | 27 +++------------------------
 1 file changed, 3 insertions(+), 24 deletions(-)

diff --git a/src/compiler/nir/nir_group_loads.c b/src/compiler/nir/nir_group_loads.c
index 08a4d6dd12ddb..068d7da72c6e8 100644
--- a/src/compiler/nir/nir_group_loads.c
+++ b/src/compiler/nir/nir_group_loads.c
@@ -50,28 +50,6 @@
 
 #include "nir.h"
 
-static bool
-is_memory_load(nir_instr *instr)
-{
-   /* Count texture_size too because it has the same latency as cache hits. */
-   if (instr->type == nir_instr_type_tex)
-      return true;
-
-   if (instr->type == nir_instr_type_intrinsic) {
-      nir_intrinsic_instr *intr = nir_instr_as_intrinsic(instr);
-      const char *name = nir_intrinsic_infos[intr->intrinsic].name;
-
-      /* TODO: nir_intrinsics.py could do this */
-      /* load_ubo is ignored because it's usually cheap. */
-      if (!nir_intrinsic_writes_external_memory(intr) &&
-          !strstr(name, "shared") &&
-          (strstr(name, "ssbo") || strstr(name, "image")))
-         return true;
-   }
-
-   return false;
-}
-
 static nir_instr *
 get_intrinsic_resource(nir_intrinsic_instr *intr)
 {
@@ -119,7 +97,8 @@ is_grouped_load(nir_instr *instr)
    if (instr->type == nir_instr_type_tex)
       return true;
 
-   if (instr->type == nir_instr_type_intrinsic)
+   if (instr->type == nir_instr_type_intrinsic &&
+       nir_intrinsic_can_reorder(nir_instr_as_intrinsic(instr)))
       return get_intrinsic_resource(nir_instr_as_intrinsic(instr)) != NULL;
 
    return false;
@@ -315,7 +294,7 @@ gather_indirections(nir_src *src, void *data)
    if (instr->block == state->block) {
       unsigned indirections = get_num_indirections(src->ssa->parent_instr);
 
-      if (instr->type == nir_instr_type_tex || is_memory_load(instr))
+      if (instr->type == nir_instr_type_tex || is_grouped_load(instr))
          indirections++;
 
       state->indirections = MAX2(state->indirections, indirections);
-- 
GitLab


From 97e7f2e45358814692b08b91535f8ec5dbd71225 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Wed, 25 Jun 2025 14:17:27 -0400
Subject: [PATCH 07/10] nir/group_loads: make is_grouped_load use
 get_load_resource

and unify get_intrinsic_resource & get_uniform_inst_resource into the new
helper get_load_resource.
---
 src/compiler/nir/nir_group_loads.c | 126 +++++++++++++----------------
 1 file changed, 57 insertions(+), 69 deletions(-)

diff --git a/src/compiler/nir/nir_group_loads.c b/src/compiler/nir/nir_group_loads.c
index 068d7da72c6e8..73258f7ee4603 100644
--- a/src/compiler/nir/nir_group_loads.c
+++ b/src/compiler/nir/nir_group_loads.c
@@ -51,57 +51,72 @@
 #include "nir.h"
 
 static nir_instr *
-get_intrinsic_resource(nir_intrinsic_instr *intr)
+get_load_resource(nir_instr *instr)
 {
-   /* This is also the list of intrinsics that are grouped. */
-   switch (intr->intrinsic) {
-   /* Image loads. */
-   case nir_intrinsic_image_load:
-   case nir_intrinsic_image_deref_load:
-   case nir_intrinsic_bindless_image_load:
-   case nir_intrinsic_image_sparse_load:
-   case nir_intrinsic_image_deref_sparse_load:
-   case nir_intrinsic_bindless_image_sparse_load:
-   /* Fragment mask loads. (samples_identical also loads it) */
-   case nir_intrinsic_image_fragment_mask_load_amd:
-   case nir_intrinsic_image_deref_fragment_mask_load_amd:
-   case nir_intrinsic_bindless_image_fragment_mask_load_amd:
-   case nir_intrinsic_image_samples_identical:
-   case nir_intrinsic_image_deref_samples_identical:
-   case nir_intrinsic_bindless_image_samples_identical:
-   /* Image queries. (group them too since they can have cache hit latency) */
-   case nir_intrinsic_image_size:
-   case nir_intrinsic_image_deref_size:
-   case nir_intrinsic_bindless_image_size:
-   case nir_intrinsic_image_levels:
-   case nir_intrinsic_image_deref_levels:
-   case nir_intrinsic_bindless_image_levels:
-   case nir_intrinsic_image_samples:
-   case nir_intrinsic_image_deref_samples:
-   case nir_intrinsic_bindless_image_samples:
-   /* Other loads. */
-   /* load_ubo is ignored because it's usually cheap. */
-   case nir_intrinsic_load_ssbo:
-   case nir_intrinsic_load_global:
-      return intr->src[0].ssa->parent_instr;
-   default:
-      return NULL;
+   if (instr->type == nir_instr_type_tex) {
+      nir_tex_instr *tex = nir_instr_as_tex(instr);
+
+      for (unsigned i = 0; i < tex->num_srcs; i++) {
+         switch (tex->src[i].src_type) {
+         case nir_tex_src_texture_deref:
+         case nir_tex_src_texture_handle:
+            return tex->src[i].src.ssa->parent_instr;
+         default:
+            break;
+         }
+      }
+      unreachable("tex instr should have a resource");
+   }
+
+   if (instr->type == nir_instr_type_intrinsic) {
+      /* This is also the list of intrinsics that are grouped. */
+      switch (nir_instr_as_intrinsic(instr)->intrinsic) {
+      /* Image loads. */
+      case nir_intrinsic_image_load:
+      case nir_intrinsic_image_deref_load:
+      case nir_intrinsic_bindless_image_load:
+      case nir_intrinsic_image_sparse_load:
+      case nir_intrinsic_image_deref_sparse_load:
+      case nir_intrinsic_bindless_image_sparse_load:
+      /* Fragment mask loads. (samples_identical also loads it) */
+      case nir_intrinsic_image_fragment_mask_load_amd:
+      case nir_intrinsic_image_deref_fragment_mask_load_amd:
+      case nir_intrinsic_bindless_image_fragment_mask_load_amd:
+      case nir_intrinsic_image_samples_identical:
+      case nir_intrinsic_image_deref_samples_identical:
+      case nir_intrinsic_bindless_image_samples_identical:
+      /* Image queries. (group them too since they can have cache hit latency) */
+      case nir_intrinsic_image_size:
+      case nir_intrinsic_image_deref_size:
+      case nir_intrinsic_bindless_image_size:
+      case nir_intrinsic_image_levels:
+      case nir_intrinsic_image_deref_levels:
+      case nir_intrinsic_bindless_image_levels:
+      case nir_intrinsic_image_samples:
+      case nir_intrinsic_image_deref_samples:
+      case nir_intrinsic_bindless_image_samples:
+      /* Other loads. */
+      /* load_ubo is ignored because it's usually cheap. */
+      case nir_intrinsic_load_ssbo:
+      case nir_intrinsic_load_global:
+         return nir_instr_as_intrinsic(instr)->src[0].ssa->parent_instr;
+      default:
+         return NULL;
+      }
    }
+
+   return NULL;
 }
 
 /* Track only those that we want to group. */
 static bool
 is_grouped_load(nir_instr *instr)
 {
-   /* Count texture_size too because it has the same latency as cache hits. */
-   if (instr->type == nir_instr_type_tex)
-      return true;
-
    if (instr->type == nir_instr_type_intrinsic &&
-       nir_intrinsic_can_reorder(nir_instr_as_intrinsic(instr)))
-      return get_intrinsic_resource(nir_instr_as_intrinsic(instr)) != NULL;
+       !nir_intrinsic_can_reorder(nir_instr_as_intrinsic(instr)))
+      return false;
 
-   return false;
+   return get_load_resource(instr) != NULL;
 }
 
 static bool
@@ -114,33 +129,6 @@ is_part_of_group(nir_instr *instr, uint8_t current_indirection_level)
    return is_grouped_load(instr) && instr->pass_flags == current_indirection_level;
 }
 
-static nir_instr *
-get_uniform_inst_resource(nir_instr *instr)
-{
-   if (instr->type == nir_instr_type_tex) {
-      nir_tex_instr *tex = nir_instr_as_tex(instr);
-
-      if (tex->texture_non_uniform)
-         return NULL;
-
-      for (unsigned i = 0; i < tex->num_srcs; i++) {
-         switch (tex->src[i].src_type) {
-         case nir_tex_src_texture_deref:
-         case nir_tex_src_texture_handle:
-            return tex->src[i].src.ssa->parent_instr;
-         default:
-            break;
-         }
-      }
-      return NULL;
-   }
-
-   if (instr->type == nir_instr_type_intrinsic)
-      return get_intrinsic_resource(nir_instr_as_intrinsic(instr));
-
-   return NULL;
-}
-
 struct check_sources_state {
    nir_block *block;
    uint32_t first_index;
@@ -422,7 +410,7 @@ process_block(nir_block *block, nir_load_grouping grouping,
                break;
 
             case nir_group_same_resource_only:
-               current_resource = get_uniform_inst_resource(current);
+               current_resource = get_load_resource(current);
 
                if (current_resource) {
                   if (!first_load) {
-- 
GitLab


From 78265ed6a06ae55897f49e79c4ee2e8ae978b59d Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sun, 19 Jan 2025 02:16:11 -0500
Subject: [PATCH 08/10] nir/group_loads: expose nir_is_grouped_load &
 nir_get_load_resource

---
 src/compiler/nir/nir.h             |  2 ++
 src/compiler/nir/nir_group_loads.c | 22 +++++++++++-----------
 2 files changed, 13 insertions(+), 11 deletions(-)

diff --git a/src/compiler/nir/nir.h b/src/compiler/nir/nir.h
index 04476ecc932e5..57ea47fcb13c9 100644
--- a/src/compiler/nir/nir.h
+++ b/src/compiler/nir/nir.h
@@ -4866,6 +4866,8 @@ typedef enum {
    nir_group_same_resource_only,
 } nir_load_grouping;
 
+bool nir_is_grouped_load(nir_instr *instr);
+nir_instr *nir_get_load_resource(nir_instr *instr);
 bool nir_group_loads(nir_shader *shader, nir_load_grouping grouping,
                      unsigned max_distance);
 
diff --git a/src/compiler/nir/nir_group_loads.c b/src/compiler/nir/nir_group_loads.c
index 73258f7ee4603..2e1d99910391c 100644
--- a/src/compiler/nir/nir_group_loads.c
+++ b/src/compiler/nir/nir_group_loads.c
@@ -50,8 +50,8 @@
 
 #include "nir.h"
 
-static nir_instr *
-get_load_resource(nir_instr *instr)
+nir_instr *
+nir_get_load_resource(nir_instr *instr)
 {
    if (instr->type == nir_instr_type_tex) {
       nir_tex_instr *tex = nir_instr_as_tex(instr);
@@ -109,14 +109,14 @@ get_load_resource(nir_instr *instr)
 }
 
 /* Track only those that we want to group. */
-static bool
-is_grouped_load(nir_instr *instr)
+bool
+nir_is_grouped_load(nir_instr *instr)
 {
    if (instr->type == nir_instr_type_intrinsic &&
        !nir_intrinsic_can_reorder(nir_instr_as_intrinsic(instr)))
       return false;
 
-   return get_load_resource(instr) != NULL;
+   return nir_get_load_resource(instr) != NULL;
 }
 
 static bool
@@ -126,7 +126,7 @@ is_part_of_group(nir_instr *instr, uint8_t current_indirection_level)
     * instruction range of the load group corresponding to the given
     * indirection level.
     */
-   return is_grouped_load(instr) && instr->pass_flags == current_indirection_level;
+   return nir_is_grouped_load(instr) && instr->pass_flags == current_indirection_level;
 }
 
 struct check_sources_state {
@@ -235,7 +235,7 @@ set_instr_indices(nir_block *block)
       /* Make sure grouped instructions don't have the same index as pseudo
        * instructions.
        */
-      if (last && is_pseudo_inst(last) && is_grouped_load(instr))
+      if (last && is_pseudo_inst(last) && nir_is_grouped_load(instr))
          counter++;
 
       /* Set each instruction's index within the block. */
@@ -282,7 +282,7 @@ gather_indirections(nir_src *src, void *data)
    if (instr->block == state->block) {
       unsigned indirections = get_num_indirections(src->ssa->parent_instr);
 
-      if (instr->type == nir_instr_type_tex || is_grouped_load(instr))
+      if (instr->type == nir_instr_type_tex || nir_is_grouped_load(instr))
          indirections++;
 
       state->indirections = MAX2(state->indirections, indirections);
@@ -336,7 +336,7 @@ process_block(nir_block *block, nir_load_grouping grouping,
     * within this block. Store it in pass_flags.
     */
    nir_foreach_instr(instr, block) {
-      if (is_grouped_load(instr)) {
+      if (nir_is_grouped_load(instr)) {
          unsigned indirections = get_num_indirections(instr);
 
          /* pass_flags has only 8 bits */
@@ -398,7 +398,7 @@ process_block(nir_block *block, nir_load_grouping grouping,
          }
 
          /* Only group load instructions with the same indirection level. */
-         if (is_grouped_load(current) && current->pass_flags == level) {
+         if (nir_is_grouped_load(current) && current->pass_flags == level) {
             nir_instr *current_resource;
 
             switch (grouping) {
@@ -410,7 +410,7 @@ process_block(nir_block *block, nir_load_grouping grouping,
                break;
 
             case nir_group_same_resource_only:
-               current_resource = get_load_resource(current);
+               current_resource = nir_get_load_resource(current);
 
                if (current_resource) {
                   if (!first_load) {
-- 
GitLab


From 83930b42cf9fab44f730743e59a681155392aa93 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 18 Jan 2025 22:47:37 -0500
Subject: [PATCH 09/10] nir: add a new pass that reorders adjacent loads for
 performance

Adjacent loads are sorted by the distance of their closest use and whether
they use the same binding. This is a scheduling improvement for ACO.
---
 src/compiler/nir/meson.build                  |   1 +
 src/compiler/nir/nir.c                        |  64 +++++
 src/compiler/nir/nir.h                        |   6 +
 src/compiler/nir/nir_reorder_adjacent_loads.c | 219 ++++++++++++++++++
 4 files changed, 290 insertions(+)
 create mode 100644 src/compiler/nir/nir_reorder_adjacent_loads.c

diff --git a/src/compiler/nir/meson.build b/src/compiler/nir/meson.build
index 50acc601ecac8..4ba252cb3aa98 100644
--- a/src/compiler/nir/meson.build
+++ b/src/compiler/nir/meson.build
@@ -304,6 +304,7 @@ else
   'nir_range_analysis.h',
   'nir_remove_dead_variables.c',
   'nir_remove_tex_shadow.c',
+  'nir_reorder_adjacent_loads.c',
   'nir_repair_ssa.c',
   'nir_scale_fdiv.c',
   'nir_schedule.c',
diff --git a/src/compiler/nir/nir.c b/src/compiler/nir/nir.c
index 80f3dc5a80fff..497c6df445fe0 100644
--- a/src/compiler/nir/nir.c
+++ b/src/compiler/nir/nir.c
@@ -31,6 +31,7 @@
 #include <math.h>
 #include "util/half_float.h"
 #include "util/macros.h"
+#include "util/u_dynarray.h"
 #include "util/u_math.h"
 #include "util/u_qsort.h"
 #include "nir_builder.h"
@@ -3646,3 +3647,66 @@ nir_atomic_op_to_alu(nir_atomic_op op)
 
    unreachable("Invalid nir_atomic_op");
 }
+
+/* Sort a range of instructions starting with *first_instr and ending with
+ * *last_instr (inclusive) within a shader. After it's done, *first_instr
+ * and *last_instr will be set to the first and last instructions of the sorted
+ * range, respectively.
+ *
+ * "scratch" is temporary storage for the sort that the caller can reuse for
+ * multiple invocations of this function, so that it doesn't have to allocate
+ * the scratch every time.
+ *
+ * Return the number of instructions in the range (for convenience if needed).
+ */
+unsigned
+nir_sort_instr(nir_instr **first_instr, nir_instr **last_instr,
+               int (*compare)(nir_instr **, nir_instr **),
+               struct util_dynarray *scratch)
+{
+   nir_instr *first = *first_instr;
+   nir_instr *last = *last_instr;
+   assert(first->block == last->block);
+   assert(first != last);
+
+   /* Verify that first is before last. */
+   ASSERTED nir_instr *instr = nir_instr_next(first);
+   while (instr && instr != last)
+      instr = nir_instr_next(instr);
+   assert(instr == last);
+
+   /* Determine where we will re-insert sorted instructions. */
+   nir_instr *after_last = nir_instr_next(last);
+   nir_cursor cursor;
+   if (after_last)
+      cursor = nir_before_instr(after_last);
+   else
+      cursor = nir_after_block(last->block);
+
+   /* Move the instructions into a temporary array. */
+   util_dynarray_clear(scratch);
+   for (nir_instr *instr = first;; instr = nir_instr_next(instr)) {
+      util_dynarray_append(scratch, nir_instr *, instr);
+
+      if (instr == last)
+         break;
+   }
+
+   util_dynarray_foreach(scratch, nir_instr *, instr) {
+      nir_instr_remove(*instr);
+   }
+
+   unsigned num_elements = util_dynarray_num_elements(scratch, nir_instr *);
+   qsort(scratch->data, num_elements, sizeof(nir_instr *),
+         (int (*)(const void *, const void *))compare);
+
+   /* Move the sorted instructions back into the shader. */
+   util_dynarray_foreach(scratch, nir_instr *, instr) {
+      nir_instr_insert(cursor, *instr);
+   }
+
+   *first_instr = *util_dynarray_element(scratch, nir_instr *, 0);
+   *last_instr = *util_dynarray_element(scratch, nir_instr *, num_elements - 1);
+   util_dynarray_clear(scratch);
+   return num_elements;
+}
diff --git a/src/compiler/nir/nir.h b/src/compiler/nir/nir.h
index 57ea47fcb13c9..52710b6c94e91 100644
--- a/src/compiler/nir/nir.h
+++ b/src/compiler/nir/nir.h
@@ -58,6 +58,7 @@ extern "C" {
 #endif
 
 typedef struct u_printf_info u_printf_info;
+struct util_dynarray;
 extern uint32_t nir_debug;
 extern bool nir_debug_print_shader[MESA_SHADER_KERNEL + 1];
 
@@ -4870,6 +4871,7 @@ bool nir_is_grouped_load(nir_instr *instr);
 nir_instr *nir_get_load_resource(nir_instr *instr);
 bool nir_group_loads(nir_shader *shader, nir_load_grouping grouping,
                      unsigned max_distance);
+bool nir_reorder_adjacent_loads(nir_shader *shader);
 
 bool nir_shrink_vec_array_vars(nir_shader *shader, nir_variable_mode modes);
 bool nir_split_array_vars(nir_shader *shader, nir_variable_mode modes);
@@ -6564,6 +6566,10 @@ typedef struct {
 void nir_gather_output_clipper_var_groups(nir_shader *nir,
                                           nir_output_clipper_var_groups *groups);
 
+unsigned nir_sort_instr(nir_instr **first, nir_instr **last,
+                        int (*compare)(nir_instr **, nir_instr **),
+                        struct util_dynarray *scratch);
+
 #include "nir_inline_helpers.h"
 
 #ifdef __cplusplus
diff --git a/src/compiler/nir/nir_reorder_adjacent_loads.c b/src/compiler/nir/nir_reorder_adjacent_loads.c
new file mode 100644
index 0000000000000..cc4a29f97d737
--- /dev/null
+++ b/src/compiler/nir/nir_reorder_adjacent_loads.c
@@ -0,0 +1,219 @@
+/*
+ * Copyright Â© 2025 Advanced Micro Devices, Inc.
+ * SPDX-License-Identifier: MIT
+ */
+
+/* All groups of adjacent independent loads are sorted as follows:
+ * 1. If 2 or more loads use the same binding, they must be next to each other.
+ * 2. If a use of load A is before a use of load B and both loads use different
+ *    bindings, load A must be before load B. If load group A and load group B
+ *    are groups of loads using the same binding, then the closest use of all
+ *    loads from each group is used to determine the ordering of the groups.
+ *
+ * The shader is walked from the end to the beginning, so that loads are more
+ * likely to match the order of stores.
+ *
+ * It's recommended to run nir_group_loads(nir, nir_group_all) to move
+ * independent loads next to each other.
+ *
+ * This improves performance on certain GPU architectures.
+ */
+
+#include "nir.h"
+#include "util/u_dynarray.h"
+
+static unsigned
+get_closest_use(nir_instr *instr)
+{
+   uint32_t min = UINT32_MAX;
+
+   nir_foreach_use_including_if(src, nir_instr_def(instr)) {
+      /* TODO: Which instr->index should we use for ifs? */
+      assert(!nir_src_is_if(src));
+      min = MIN2(min, nir_src_parent_instr(src)->index);
+   }
+   return min;
+}
+
+static int
+compare_binding(nir_instr **a, nir_instr **b)
+{
+   nir_instr *ia = nir_get_load_resource(*a);
+   nir_instr *ib = nir_get_load_resource(*b);
+
+   if (ia != ib)
+      return ia->index > ib->index ? 1 : -1;
+
+   return 0;
+}
+
+static int
+compare_uses(nir_instr **a, nir_instr **b)
+{
+   return nir_instr_def(*a)->index - nir_instr_def(*b)->index;
+}
+
+static bool
+reorder_loads(nir_instr *first, nir_instr *last, struct util_dynarray *scratch)
+{
+   unsigned first_instr_index = first->index;
+   /* Sort loads by binding. */
+   unsigned num_loads = nir_sort_instr(&first, &last, compare_binding, scratch);
+
+   /* Gather information about the loads. */
+   struct load_info {
+      nir_instr *binding;
+      unsigned closest_use;
+      unsigned same_binding_group_index;
+   } *info = (struct load_info*)alloca(sizeof(info[0]) * num_loads);
+   unsigned i, group_index;
+   nir_instr *instr;
+
+   for (i = 0, group_index = 0, instr = first; i < num_loads;
+        i++, instr = nir_instr_next(instr)) {
+      info[i].binding = nir_get_load_resource(instr);
+      info[i].closest_use = get_closest_use(instr);
+
+      if (i && info[i].binding != info[i - 1].binding)
+         group_index++;
+
+      info[i].same_binding_group_index = group_index;
+   }
+
+   /* For a group of loads using the same binding, set their closest use to
+    * the minimum of all loads in the group. This will keep those loads together.
+    */
+   for (i = 0, instr = first; i < num_loads; i++, instr = nir_instr_next(instr)) {
+      unsigned min = info[i].closest_use;
+      unsigned num = 1;
+
+      for (unsigned j = i + 1;
+           j < num_loads &&
+           info[j].same_binding_group_index == info[i].same_binding_group_index; j++) {
+         min = MIN2(min, info[j].closest_use);
+         num++;
+      }
+
+      if (num > 1) {
+         info[i].closest_use = min;
+         for (unsigned j = i + 1;
+              j < num_loads &&
+              info[j].same_binding_group_index == info[i].same_binding_group_index; j++)
+            info[j].closest_use = min;
+
+         /* We processed all instructions in this group. Move to the next group. */
+         i += num - 1;
+         for (unsigned j = 0; j < num - 1; j++)
+            instr = nir_instr_next(instr);
+      }
+   }
+
+   /* Use nir_def::index to store the index of the closest use for the comparison. */
+   for (i = 0, instr = first; i < num_loads; i++, instr = nir_instr_next(instr))
+      nir_instr_def(instr)->index = info[i].closest_use;
+
+   /* Sort loads by the distance of their use.
+    *
+    * TODO: This relies on qsort being stable to keep loads that use the same
+    * resource together. qsort is only stable with glibc because it's implemented
+    * as a merge sort there, so it's OK to use qsort with that. We should add
+    * our own merge sort into Mesa to have a stable sort on all systems.
+    */
+   nir_sort_instr(&first, &last, compare_uses, scratch);
+
+   /* Recompute nir_instr::index of loads after reordering. */
+   for (i = 0, instr = first; i < num_loads; i++, instr = nir_instr_next(instr))
+      instr->index = first_instr_index + i;
+
+   return true;
+}
+
+static bool
+process_block(nir_block *block, struct util_dynarray *scratch)
+{
+   /* Find a group of adjacent loads that will be reordered. */
+   nir_instr *last = NULL, *first = NULL;
+   bool progress = false;
+
+   nir_foreach_instr_reverse(instr, block) {
+      if (nir_is_grouped_load(instr)) {
+         if (!last) {
+            /* Start a new group (in the reverse order). */
+            assert(!first);
+            last = instr;
+            continue;
+         }
+
+         /* If any use isn't after "last", we have to break the group. */
+         bool break_group = false;
+         nir_foreach_use(src, nir_instr_def(instr)) {
+            nir_instr *use = nir_src_parent_instr(src);
+
+            if (use->index < instr->index) {
+               /* "use" is a phi at the beginning of a loop. Accept it because
+                * it's equivalent to being used at the end of the loop.
+                */
+               assert(use->type == nir_instr_type_phi);
+               assert(use->block->cf_node.type == nir_cf_node_loop);
+            } else if (use->index <= last->index) {
+               /* The use isn't after "last", so this is a different group. */
+               break_group = true;
+               break;
+            }
+         }
+
+         if (break_group) {
+            if (first && last)
+               progress |= reorder_loads(first, last, scratch);
+
+            /* Start a new group. */
+            first = NULL;
+            last = instr;
+            continue;
+         }
+
+         /* Add the load into the group. */
+         first = instr;
+         continue;
+      }
+
+      /* We are outside a group. Reorder the group we found. */
+      if (first && last)
+         progress |= reorder_loads(first, last, scratch);
+
+      first = NULL;
+      last = NULL;
+   }
+
+   if (first && last)
+      progress |= reorder_loads(first, last, scratch);
+
+   return progress;
+}
+
+bool
+nir_reorder_adjacent_loads(nir_shader *shader)
+{
+   struct util_dynarray scratch;
+   util_dynarray_init(&scratch, NULL);
+   bool any_progress = false;
+
+   nir_foreach_function_impl(impl, shader) {
+      nir_metadata_require(impl, nir_metadata_instr_index);
+      bool progress = false;
+
+      nir_foreach_block_reverse(block, impl) {
+         progress |= process_block(block, &scratch);
+      }
+
+      any_progress |= nir_progress(progress, impl,
+                                   nir_metadata_control_flow |
+                                   nir_metadata_loop_analysis);
+      /* The pass overwrites def indices. Recompute them. */
+      if (progress)
+         nir_index_ssa_defs(impl);
+   }
+
+   util_dynarray_fini(&scratch);
+   return any_progress;
+}
-- 
GitLab


From f0d6bb445d2a9ca2efc90365718bdbd9bf5ed027 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sun, 19 Jan 2025 03:17:02 -0500
Subject: [PATCH 10/10] radeonsi: use nir_reorder_adjacent_loads

---
 src/gallium/drivers/radeonsi/si_shader.c | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/src/gallium/drivers/radeonsi/si_shader.c b/src/gallium/drivers/radeonsi/si_shader.c
index 70704c87b46f0..43066bd34e85c 100644
--- a/src/gallium/drivers/radeonsi/si_shader.c
+++ b/src/gallium/drivers/radeonsi/si_shader.c
@@ -1776,7 +1776,8 @@ static void run_late_optimization_and_lowering_passes(struct si_nir_shader_ctx *
    /* This helps LLVM form VMEM clauses and thus get more GPU cache hits.
     * 200 is tuned for Viewperf. It should be done last.
     */
-   NIR_PASS_V(nir, nir_group_loads, nir_group_same_resource_only, 200);
+   NIR_PASS_V(nir, nir_group_loads, nir_group_all, 200);
+   NIR_PASS(_, nir, nir_reorder_adjacent_loads);
 }
 
 static void get_input_nir(struct si_shader *shader, struct si_nir_shader_ctx *ctx)
-- 
GitLab

