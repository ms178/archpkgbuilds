From a2d3c3e6450cfb2405eee521832431e9ffcdc01a Mon Sep 17 00:00:00 2001
From: Konstantin Seurer <konstantin.seurer@gmail.com>
Date: Sat, 23 Jul 2022 23:23:55 +0200
Subject: [PATCH 1/3] radv: Use instructions_pass for lowering the ABI

nir_shader_instructions_pass is sufficient and it should be a bit faster
than nir_shader_lower_instructions.

Signed-off-by: Konstantin Seurer <konstantin.seurer@gmail.com>
---
 src/amd/vulkan/radv_nir_lower_abi.c | 214 +++++++++++++---------------
 1 file changed, 100 insertions(+), 114 deletions(-)

diff --git a/src/amd/vulkan/radv_nir_lower_abi.c b/src/amd/vulkan/radv_nir_lower_abi.c
index f23c70b72132..51f79a2a4542 100644
--- a/src/amd/vulkan/radv_nir_lower_abi.c
+++ b/src/amd/vulkan/radv_nir_lower_abi.c
@@ -57,38 +57,56 @@ nggc_bool_setting(nir_builder *b, unsigned mask, lower_abi_state *s)
    return nir_test_mask(b, settings, mask);
 }
 
-static nir_ssa_def *
+static bool
 lower_abi_instr(nir_builder *b, nir_instr *instr, void *state)
 {
-   lower_abi_state *s = (lower_abi_state *) state;
+   if (instr->type != nir_instr_type_intrinsic)
+      return false;
+
    nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
+
+   lower_abi_state *s = (lower_abi_state *)state;
    gl_shader_stage stage = b->shader->info.stage;
 
+   b->cursor = nir_before_instr(instr);
+
+   nir_ssa_def *replacement = NULL;
+
    switch (intrin->intrinsic) {
    case nir_intrinsic_load_ring_tess_factors_amd:
-      return load_ring(b, RING_HS_TESS_FACTOR, s);
+      if (s->use_llvm)
+         break;
 
+      replacement = load_ring(b, RING_HS_TESS_FACTOR, s);
+      break;
    case nir_intrinsic_load_ring_tess_factors_offset_amd:
-      return ac_nir_load_arg(b, &s->args->ac, s->args->ac.tcs_factor_offset);
-
+      replacement = ac_nir_load_arg(b, &s->args->ac, s->args->ac.tcs_factor_offset);
+      break;
    case nir_intrinsic_load_ring_tess_offchip_amd:
-      return load_ring(b, RING_HS_TESS_OFFCHIP, s);
+      if (s->use_llvm)
+         break;
 
+      replacement = load_ring(b, RING_HS_TESS_OFFCHIP, s);
+      break;
    case nir_intrinsic_load_ring_tess_offchip_offset_amd:
-      return ac_nir_load_arg(b, &s->args->ac, s->args->ac.tess_offchip_offset);
+      if (s->use_llvm)
+         break;
 
+      replacement = ac_nir_load_arg(b, &s->args->ac, s->args->ac.tess_offchip_offset);
+      break;
    case nir_intrinsic_load_tcs_num_patches_amd:
-      return nir_imm_int(b, s->info->num_tess_patches);
-
+      replacement = nir_imm_int(b, s->info->num_tess_patches);
+      break;
    case nir_intrinsic_load_ring_esgs_amd:
-      return load_ring(b, stage == MESA_SHADER_GEOMETRY ? RING_ESGS_GS : RING_ESGS_VS, s);
-
+      replacement = load_ring(b, stage == MESA_SHADER_GEOMETRY ? RING_ESGS_GS : RING_ESGS_VS, s);
+      break;
    case nir_intrinsic_load_ring_es2gs_offset_amd:
-      return ac_nir_load_arg(b, &s->args->ac, s->args->ac.es2gs_offset);
-
+      replacement = ac_nir_load_arg(b, &s->args->ac, s->args->ac.es2gs_offset);
+      break;
    case nir_intrinsic_load_tess_rel_patch_id_amd:
       if (stage == MESA_SHADER_TESS_CTRL) {
-         return nir_extract_u8(b, ac_nir_load_arg(b, &s->args->ac, s->args->ac.tcs_rel_ids), nir_imm_int(b, 0));
+         replacement = nir_extract_u8(b, ac_nir_load_arg(b, &s->args->ac, s->args->ac.tcs_rel_ids),
+                                      nir_imm_int(b, 0));
       } else if (stage == MESA_SHADER_TESS_EVAL) {
          /* Setting an upper bound like this will actually make it possible
           * to optimize some multiplications (in address calculations) so that
@@ -97,52 +115,54 @@ lower_abi_instr(nir_builder *b, nir_instr *instr, void *state)
          nir_ssa_def *arg = ac_nir_load_arg(b, &s->args->ac, s->args->ac.tes_rel_patch_id);
          nir_intrinsic_instr *load_arg = nir_instr_as_intrinsic(arg->parent_instr);
          nir_intrinsic_set_arg_upper_bound_u32_amd(load_arg, 2048 / MAX2(b->shader->info.tess.tcs_vertices_out, 1));
-         return arg;
+         replacement = arg;
       } else {
          unreachable("invalid tessellation shader stage");
       }
-
+      break;
    case nir_intrinsic_load_patch_vertices_in:
       if (stage == MESA_SHADER_TESS_CTRL)
-         return nir_imm_int(b, s->pl_key->tcs.tess_input_vertices);
+         replacement = nir_imm_int(b, s->pl_key->tcs.tess_input_vertices);
       else if (stage == MESA_SHADER_TESS_EVAL)
-         return nir_imm_int(b, b->shader->info.tess.tcs_vertices_out);
+         replacement = nir_imm_int(b, b->shader->info.tess.tcs_vertices_out);
       else
          unreachable("invalid tessellation shader stage");
-
+      break;
    case nir_intrinsic_load_gs_vertex_offset_amd:
-      return ac_nir_load_arg(b, &s->args->ac, s->args->ac.gs_vtx_offset[nir_intrinsic_base(intrin)]);
-
+      replacement =
+         ac_nir_load_arg(b, &s->args->ac, s->args->ac.gs_vtx_offset[nir_intrinsic_base(intrin)]);
+      break;
    case nir_intrinsic_load_workgroup_num_input_vertices_amd:
-      return nir_ubfe(b, ac_nir_load_arg(b, &s->args->ac, s->args->ac.gs_tg_info),
-                         nir_imm_int(b, 12), nir_imm_int(b, 9));
-
+      replacement = nir_ubfe(b, ac_nir_load_arg(b, &s->args->ac, s->args->ac.gs_tg_info),
+                             nir_imm_int(b, 12), nir_imm_int(b, 9));
+      break;
    case nir_intrinsic_load_workgroup_num_input_primitives_amd:
-      return nir_ubfe(b, ac_nir_load_arg(b, &s->args->ac, s->args->ac.gs_tg_info),
-                         nir_imm_int(b, 22), nir_imm_int(b, 9));
-
+      replacement = nir_ubfe(b, ac_nir_load_arg(b, &s->args->ac, s->args->ac.gs_tg_info),
+                             nir_imm_int(b, 22), nir_imm_int(b, 9));
+      break;
    case nir_intrinsic_load_packed_passthrough_primitive_amd:
       /* NGG passthrough mode: the HW already packs the primitive export value to a single register. */
-      return ac_nir_load_arg(b, &s->args->ac, s->args->ac.gs_vtx_offset[0]);
-
+      replacement = ac_nir_load_arg(b, &s->args->ac, s->args->ac.gs_vtx_offset[0]);
+      break;
    case nir_intrinsic_load_shader_query_enabled_amd:
-      return nir_ieq_imm(b, ac_nir_load_arg(b, &s->args->ac, s->args->ngg_query_state), 1);
-
+      replacement = nir_ieq_imm(b, ac_nir_load_arg(b, &s->args->ac, s->args->ngg_query_state), 1);
+      break;
    case nir_intrinsic_load_cull_any_enabled_amd:
-      return nggc_bool_setting(b, radv_nggc_front_face | radv_nggc_back_face | radv_nggc_small_primitives, s);
-
+      replacement = nggc_bool_setting(
+         b, radv_nggc_front_face | radv_nggc_back_face | radv_nggc_small_primitives, s);
+      break;
    case nir_intrinsic_load_cull_front_face_enabled_amd:
-      return nggc_bool_setting(b, radv_nggc_front_face, s);
-
+      replacement = nggc_bool_setting(b, radv_nggc_front_face, s);
+      break;
    case nir_intrinsic_load_cull_back_face_enabled_amd:
-      return nggc_bool_setting(b, radv_nggc_back_face, s);
-
+      replacement = nggc_bool_setting(b, radv_nggc_back_face, s);
+      break;
    case nir_intrinsic_load_cull_ccw_amd:
-      return nggc_bool_setting(b, radv_nggc_face_is_ccw, s);
-
+      replacement = nggc_bool_setting(b, radv_nggc_face_is_ccw, s);
+      break;
    case nir_intrinsic_load_cull_small_primitives_enabled_amd:
-      return nggc_bool_setting(b, radv_nggc_small_primitives, s);
-
+      replacement = nggc_bool_setting(b, radv_nggc_small_primitives, s);
+      break;
    case nir_intrinsic_load_cull_small_prim_precision_amd: {
       /* To save space, only the exponent is stored in the high 8 bits.
        * We calculate the precision from those 8 bits:
@@ -151,108 +171,73 @@ lower_abi_instr(nir_builder *b, nir_instr *instr, void *state)
        */
       nir_ssa_def *settings = ac_nir_load_arg(b, &s->args->ac, s->args->ngg_culling_settings);
       nir_ssa_def *exponent = nir_ishr_imm(b, settings, 24u);
-      return nir_ldexp(b, nir_imm_float(b, 1.0f), exponent);
+      replacement = nir_ldexp(b, nir_imm_float(b, 1.0f), exponent);
+      break;
    }
-
    case nir_intrinsic_load_viewport_x_scale:
-      return ac_nir_load_arg(b, &s->args->ac, s->args->ngg_viewport_scale[0]);
-
+      replacement = ac_nir_load_arg(b, &s->args->ac, s->args->ngg_viewport_scale[0]);
+      break;
    case nir_intrinsic_load_viewport_x_offset:
-      return ac_nir_load_arg(b, &s->args->ac, s->args->ngg_viewport_translate[0]);
-
+      replacement = ac_nir_load_arg(b, &s->args->ac, s->args->ngg_viewport_translate[0]);
+      break;
    case nir_intrinsic_load_viewport_y_scale:
-      return ac_nir_load_arg(b, &s->args->ac, s->args->ngg_viewport_scale[1]);
-
+      replacement = ac_nir_load_arg(b, &s->args->ac, s->args->ngg_viewport_scale[1]);
+      break;
    case nir_intrinsic_load_viewport_y_offset:
-      return ac_nir_load_arg(b, &s->args->ac, s->args->ngg_viewport_translate[1]);
-
+      replacement = ac_nir_load_arg(b, &s->args->ac, s->args->ngg_viewport_translate[1]);
+      break;
    case nir_intrinsic_load_ring_task_draw_amd:
-      return load_ring(b, RING_TS_DRAW, s);
-
+      replacement = load_ring(b, RING_TS_DRAW, s);
+      break;
    case nir_intrinsic_load_ring_task_payload_amd:
-      return load_ring(b, RING_TS_PAYLOAD, s);
-
+      replacement = load_ring(b, RING_TS_PAYLOAD, s);
+      break;
    case nir_intrinsic_load_ring_mesh_scratch_amd:
-      return load_ring(b, RING_MS_SCRATCH, s);
-
+      replacement = load_ring(b, RING_MS_SCRATCH, s);
+      break;
    case nir_intrinsic_load_ring_mesh_scratch_offset_amd:
       /* gs_tg_info[0:11] is ordered_wave_id. Multiply by the ring entry size. */
-      return nir_imul_imm(b, nir_iand_imm(b, ac_nir_load_arg(b, &s->args->ac, s->args->ac.gs_tg_info), 0xfff),
-                                          RADV_MESH_SCRATCH_ENTRY_BYTES);
-
+      replacement = nir_imul_imm(
+         b, nir_iand_imm(b, ac_nir_load_arg(b, &s->args->ac, s->args->ac.gs_tg_info), 0xfff),
+         RADV_MESH_SCRATCH_ENTRY_BYTES);
+      break;
    case nir_intrinsic_load_task_ring_entry_amd:
-      return ac_nir_load_arg(b, &s->args->ac, s->args->ac.task_ring_entry);
-
+      replacement = ac_nir_load_arg(b, &s->args->ac, s->args->ac.task_ring_entry);
+      break;
    case nir_intrinsic_load_task_ib_addr:
-      return ac_nir_load_arg(b, &s->args->ac, s->args->task_ib_addr);
-
+      replacement = ac_nir_load_arg(b, &s->args->ac, s->args->task_ib_addr);
+      break;
    case nir_intrinsic_load_task_ib_stride:
-      return ac_nir_load_arg(b, &s->args->ac, s->args->task_ib_stride);
-
+      replacement = ac_nir_load_arg(b, &s->args->ac, s->args->task_ib_stride);
+      break;
    case nir_intrinsic_load_lshs_vertex_stride_amd: {
       unsigned io_num = stage == MESA_SHADER_VERTEX ?
          s->info->vs.num_linked_outputs :
          s->info->tcs.num_linked_inputs;
-      return nir_imm_int(b, io_num * 16);
+      replacement = nir_imm_int(b, io_num * 16);
+      break;
    }
-
    case nir_intrinsic_load_hs_out_patch_data_offset_amd: {
       unsigned num_patches = s->info->num_tess_patches;
       unsigned out_vertices_per_patch = b->shader->info.tess.tcs_vertices_out;
       unsigned num_tcs_outputs = stage == MESA_SHADER_TESS_CTRL ?
          s->info->tcs.num_linked_outputs : s->info->tes.num_linked_inputs;
       int per_vertex_output_patch_size = out_vertices_per_patch * num_tcs_outputs * 16u;
-      return nir_imm_int(b, num_patches * per_vertex_output_patch_size);
+      replacement = nir_imm_int(b, num_patches * per_vertex_output_patch_size);
+      break;
    }
-
    default:
-      unreachable("invalid NIR RADV ABI intrinsic.");
+      break;
    }
-}
 
-static bool
-filter_abi_instr(const nir_instr *instr,
-                 UNUSED const void *state)
-{
-   lower_abi_state *s = (lower_abi_state *) state;
-
-   if (instr->type != nir_instr_type_intrinsic)
+   if (!replacement)
       return false;
 
-   nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
-   return (intrin->intrinsic == nir_intrinsic_load_ring_tess_factors_amd && !s->use_llvm) ||
-          (intrin->intrinsic == nir_intrinsic_load_ring_tess_offchip_amd && !s->use_llvm) ||
-          (intrin->intrinsic == nir_intrinsic_load_ring_esgs_amd && !s->use_llvm) ||
-          intrin->intrinsic == nir_intrinsic_load_ring_tess_factors_offset_amd ||
-          intrin->intrinsic == nir_intrinsic_load_ring_tess_offchip_offset_amd ||
-          intrin->intrinsic == nir_intrinsic_load_patch_vertices_in ||
-          intrin->intrinsic == nir_intrinsic_load_tcs_num_patches_amd ||
-          intrin->intrinsic == nir_intrinsic_load_ring_es2gs_offset_amd ||
-          intrin->intrinsic == nir_intrinsic_load_tess_rel_patch_id_amd ||
-          intrin->intrinsic == nir_intrinsic_load_gs_vertex_offset_amd ||
-          intrin->intrinsic == nir_intrinsic_load_workgroup_num_input_vertices_amd ||
-          intrin->intrinsic == nir_intrinsic_load_workgroup_num_input_primitives_amd ||
-          intrin->intrinsic == nir_intrinsic_load_packed_passthrough_primitive_amd ||
-          intrin->intrinsic == nir_intrinsic_load_shader_query_enabled_amd ||
-          intrin->intrinsic == nir_intrinsic_load_cull_any_enabled_amd ||
-          intrin->intrinsic == nir_intrinsic_load_cull_front_face_enabled_amd ||
-          intrin->intrinsic == nir_intrinsic_load_cull_back_face_enabled_amd ||
-          intrin->intrinsic == nir_intrinsic_load_cull_ccw_amd ||
-          intrin->intrinsic == nir_intrinsic_load_cull_small_primitives_enabled_amd ||
-          intrin->intrinsic == nir_intrinsic_load_cull_small_prim_precision_amd ||
-          intrin->intrinsic == nir_intrinsic_load_viewport_x_scale ||
-          intrin->intrinsic == nir_intrinsic_load_viewport_x_offset ||
-          intrin->intrinsic == nir_intrinsic_load_viewport_y_scale ||
-          intrin->intrinsic == nir_intrinsic_load_viewport_y_offset ||
-          intrin->intrinsic == nir_intrinsic_load_ring_task_draw_amd ||
-          intrin->intrinsic == nir_intrinsic_load_ring_task_payload_amd ||
-          intrin->intrinsic == nir_intrinsic_load_ring_mesh_scratch_amd ||
-          intrin->intrinsic == nir_intrinsic_load_ring_mesh_scratch_offset_amd ||
-          intrin->intrinsic == nir_intrinsic_load_task_ring_entry_amd ||
-          intrin->intrinsic == nir_intrinsic_load_task_ib_addr ||
-          intrin->intrinsic == nir_intrinsic_load_task_ib_stride ||
-          intrin->intrinsic == nir_intrinsic_load_lshs_vertex_stride_amd ||
-          intrin->intrinsic == nir_intrinsic_load_hs_out_patch_data_offset_amd;
+   nir_ssa_def_rewrite_uses(&intrin->dest.ssa, replacement);
+   nir_instr_remove(instr);
+   nir_instr_free(instr);
+
+   return true;
 }
 
 void
@@ -268,5 +253,6 @@ radv_nir_lower_abi(nir_shader *shader, enum amd_gfx_level gfx_level,
       .use_llvm = use_llvm,
    };
 
-   nir_shader_lower_instructions(shader, filter_abi_instr, lower_abi_instr, &state);
+   nir_shader_instructions_pass(shader, lower_abi_instr,
+                                nir_metadata_dominance | nir_metadata_block_index, &state);
 }
-- 
GitLab


From 521c21f1fad2cbe84602795ed61c581fa7f45303 Mon Sep 17 00:00:00 2001
From: Konstantin Seurer <konstantin.seurer@gmail.com>
Date: Sun, 24 Jul 2022 11:06:20 +0200
Subject: [PATCH 2/3] radv: Lower more intrinsics in NIR

Totals from 1117 (15.93% of 7013) affected shaders:
VGPRs: 88576 -> 88592 (+0.02%)
SpillSGPRs: 16385 -> 16133 (-1.54%)
CodeSize: 18104168 -> 18081760 (-0.12%); split: -0.15%, +0.03%
MaxWaves: 18998 -> 18994 (-0.02%)
Instrs: 3483794 -> 3479941 (-0.11%); split: -0.15%, +0.04%
Latency: 87309236 -> 87166046 (-0.16%); split: -0.18%, +0.01%
InvThroughput: 40132068 -> 40120277 (-0.03%); split: -0.04%, +0.01%
VClause: 61708 -> 61699 (-0.01%); split: -0.06%, +0.05%
SClause: 137452 -> 137618 (+0.12%); split: -0.08%, +0.20%
Copies: 559548 -> 555991 (-0.64%); split: -0.79%, +0.15%
Branches: 143310 -> 143243 (-0.05%); split: -0.07%, +0.03%
PreSGPRs: 58472 -> 58252 (-0.38%); split: -0.38%, +0.00%
PreVGPRs: 81997 -> 82034 (+0.05%)

Signed-off-by: Konstantin Seurer <konstantin.seurer@gmail.com>
---
 src/amd/vulkan/radv_nir_lower_abi.c | 64 +++++++++++++++++++++++++++++
 1 file changed, 64 insertions(+)

diff --git a/src/amd/vulkan/radv_nir_lower_abi.c b/src/amd/vulkan/radv_nir_lower_abi.c
index 51f79a2a4542..43db35e37d67 100644
--- a/src/amd/vulkan/radv_nir_lower_abi.c
+++ b/src/amd/vulkan/radv_nir_lower_abi.c
@@ -226,6 +226,70 @@ lower_abi_instr(nir_builder *b, nir_instr *instr, void *state)
       replacement = nir_imm_int(b, num_patches * per_vertex_output_patch_size);
       break;
    }
+   case nir_intrinsic_load_first_vertex: {
+      if (s->use_llvm)
+         break;
+
+      replacement = ac_nir_load_arg(b, &s->args->ac, s->args->ac.base_vertex);
+      break;
+   }
+   case nir_intrinsic_load_base_instance: {
+      replacement = ac_nir_load_arg(b, &s->args->ac, s->args->ac.start_instance);
+      break;
+   }
+   case nir_intrinsic_load_draw_id: {
+      replacement = ac_nir_load_arg(b, &s->args->ac, s->args->ac.draw_id);
+      break;
+   }
+   case nir_intrinsic_load_instance_id: {
+      if (s->use_llvm)
+         break;
+
+      replacement = ac_nir_load_arg(b, &s->args->ac, s->args->ac.instance_id);
+      break;
+   }
+   case nir_intrinsic_load_sample_pos: {
+      nir_ssa_def *posx = ac_nir_load_arg(b, &s->args->ac, s->args->ac.frag_pos[0]);
+      nir_ssa_def *posy = ac_nir_load_arg(b, &s->args->ac, s->args->ac.frag_pos[1]);
+
+      replacement = nir_vec2(b, nir_ffract(b, posx), nir_ffract(b, posy));
+      break;
+   }
+   case nir_intrinsic_load_front_face: {
+      replacement = nir_ine_imm(b, ac_nir_load_arg(b, &s->args->ac, s->args->ac.front_face), 0);
+      break;
+   }
+   case nir_intrinsic_load_sample_id: {
+      replacement = nir_ubfe_imm(b, ac_nir_load_arg(b, &s->args->ac, s->args->ac.ancillary), 8, 4);
+      break;
+   }
+   case nir_intrinsic_load_local_invocation_id: {
+      if (s->gfx_level >= GFX11) {
+         nir_ssa_def *packed_id =
+            ac_nir_load_arg(b, &s->args->ac, s->args->ac.local_invocation_ids);
+
+         nir_ssa_def *x = nir_iand_imm(b, packed_id, 0x3FF);
+         nir_ssa_def *y = nir_ubfe_imm(b, packed_id, 10, 10);
+         nir_ssa_def *z = nir_ubfe_imm(b, packed_id, 20, 10);
+
+         replacement = nir_vec3(b, x, y, z);
+      } else {
+         replacement = ac_nir_load_arg(b, &s->args->ac, s->args->ac.local_invocation_ids);
+      }
+      break;
+   }
+   case nir_intrinsic_load_workgroup_id: {
+      nir_ssa_def *components[3];
+
+      for (uint32_t i = 0; i < 3; i++) {
+         components[i] = s->args->ac.workgroup_ids[i].used
+                            ? ac_nir_load_arg(b, &s->args->ac, s->args->ac.workgroup_ids[i])
+                            : nir_imm_int(b, 0);
+      }
+
+      replacement = nir_vec(b, components, 3);
+      break;
+   }
    default:
       break;
    }
-- 
GitLab


From aafb3202dce865f07a6bbef94aa1478578c957eb Mon Sep 17 00:00:00 2001
From: Konstantin Seurer <konstantin.seurer@gmail.com>
Date: Sun, 24 Jul 2022 11:35:19 +0200
Subject: [PATCH 3/3] aco: Remove dead ABI handling

Signed-off-by: Konstantin Seurer <konstantin.seurer@gmail.com>
---
 .../compiler/aco_instruction_selection.cpp    | 73 -------------------
 .../aco_instruction_selection_setup.cpp       |  7 --
 2 files changed, 80 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 1b557b436a27..a90057df1db3 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -8340,11 +8340,6 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
       emit_interp_center(ctx, get_ssa_temp(ctx, &instr->dest.ssa), bary, pos1, pos2);
       break;
    }
-   case nir_intrinsic_load_front_face: {
-      bld.vopc(aco_opcode::v_cmp_lg_u32, Definition(get_ssa_temp(ctx, &instr->dest.ssa)),
-               Operand::zero(), get_arg(ctx, ctx->args->ac.front_face));
-      break;
-   }
    case nir_intrinsic_load_view_index: {
       Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
       bld.copy(Definition(dst), Operand(get_arg(ctx, ctx->args->ac.view_index)));
@@ -8357,15 +8352,6 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
    case nir_intrinsic_load_frag_shading_rate:
       emit_load_frag_shading_rate(ctx, get_ssa_temp(ctx, &instr->dest.ssa));
       break;
-   case nir_intrinsic_load_sample_pos: {
-      Temp posx = get_arg(ctx, ctx->args->ac.frag_pos[0]);
-      Temp posy = get_arg(ctx, ctx->args->ac.frag_pos[1]);
-      bld.pseudo(
-         aco_opcode::p_create_vector, Definition(get_ssa_temp(ctx, &instr->dest.ssa)),
-         posx.id() ? bld.vop1(aco_opcode::v_fract_f32, bld.def(v1), posx) : Operand::zero(),
-         posy.id() ? bld.vop1(aco_opcode::v_fract_f32, bld.def(v1), posy) : Operand::zero());
-      break;
-   }
    case nir_intrinsic_load_tess_coord: visit_load_tess_coord(ctx, instr); break;
    case nir_intrinsic_load_interpolated_input: visit_load_interpolated_input(ctx, instr); break;
    case nir_intrinsic_store_output: visit_store_output(ctx, instr); break;
@@ -8464,40 +8450,6 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
       bld.copy(Definition(dst), Operand(addr));
       break;
    }
-   case nir_intrinsic_load_local_invocation_id: {
-      Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
-      if (ctx->options->gfx_level >= GFX11) {
-         Temp local_ids[3];
-
-         /* Thread IDs are packed in VGPR0, 10 bits per component. */
-         for (uint32_t i = 0; i < 3; i++) {
-            local_ids[i] = bld.vop3(aco_opcode::v_bfe_u32, bld.def(v1),
-                                    get_arg(ctx, ctx->args->ac.local_invocation_ids),
-                                    Operand::c32(i * 10u), Operand::c32(10u));
-         }
-
-         bld.pseudo(aco_opcode::p_create_vector, Definition(dst), local_ids[0], local_ids[1],
-                    local_ids[2]);
-      } else {
-         bld.copy(Definition(dst), Operand(get_arg(ctx, ctx->args->ac.local_invocation_ids)));
-      }
-      emit_split_vector(ctx, dst, 3);
-      break;
-   }
-   case nir_intrinsic_load_workgroup_id: {
-      Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
-      if (ctx->stage.hw == HWStage::CS) {
-         const struct ac_arg* ids = ctx->args->ac.workgroup_ids;
-         bld.pseudo(aco_opcode::p_create_vector, Definition(dst),
-                    ids[0].used ? Operand(get_arg(ctx, ids[0])) : Operand::zero(),
-                    ids[1].used ? Operand(get_arg(ctx, ids[1])) : Operand::zero(),
-                    ids[2].used ? Operand(get_arg(ctx, ids[2])) : Operand::zero());
-         emit_split_vector(ctx, dst, 3);
-      } else {
-         isel_err(&instr->instr, "Unsupported stage for load_workgroup_id");
-      }
-      break;
-   }
    case nir_intrinsic_load_local_invocation_index: {
       if (ctx->stage.hw == HWStage::LS || ctx->stage.hw == HWStage::HS) {
          if (ctx->options->gfx_level >= GFX11) {
@@ -8658,11 +8610,6 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
       }
       break;
    }
-   case nir_intrinsic_load_sample_id: {
-      bld.vop3(aco_opcode::v_bfe_u32, Definition(get_ssa_temp(ctx, &instr->dest.ssa)),
-               get_arg(ctx, ctx->args->ac.ancillary), Operand::c32(8u), Operand::c32(4u));
-      break;
-   }
    case nir_intrinsic_read_first_invocation: {
       Temp src = get_ssa_temp(ctx, instr->src[0].ssa);
       Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
@@ -9074,26 +9021,6 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
       bld.copy(Definition(dst), get_arg(ctx, ctx->args->ac.vertex_id));
       break;
    }
-   case nir_intrinsic_load_first_vertex: {
-      Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
-      bld.copy(Definition(dst), get_arg(ctx, ctx->args->ac.base_vertex));
-      break;
-   }
-   case nir_intrinsic_load_base_instance: {
-      Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
-      bld.copy(Definition(dst), get_arg(ctx, ctx->args->ac.start_instance));
-      break;
-   }
-   case nir_intrinsic_load_instance_id: {
-      Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
-      bld.copy(Definition(dst), get_arg(ctx, ctx->args->ac.instance_id));
-      break;
-   }
-   case nir_intrinsic_load_draw_id: {
-      Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
-      bld.copy(Definition(dst), get_arg(ctx, ctx->args->ac.draw_id));
-      break;
-   }
    case nir_intrinsic_load_invocation_id: {
       Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
 
diff --git a/src/amd/compiler/aco_instruction_selection_setup.cpp b/src/amd/compiler/aco_instruction_selection_setup.cpp
index 44cfef4022d9..7aa9f4b4b227 100644
--- a/src/amd/compiler/aco_instruction_selection_setup.cpp
+++ b/src/amd/compiler/aco_instruction_selection_setup.cpp
@@ -601,14 +601,11 @@ init_context(isel_context* ctx, nir_shader* shader)
                RegType type = RegType::sgpr;
                switch (intrinsic->intrinsic) {
                case nir_intrinsic_load_push_constant:
-               case nir_intrinsic_load_workgroup_id:
                case nir_intrinsic_load_num_workgroups:
                case nir_intrinsic_load_ray_launch_size_addr_amd:
                case nir_intrinsic_load_sbt_base_amd:
                case nir_intrinsic_load_subgroup_id:
                case nir_intrinsic_load_num_subgroups:
-               case nir_intrinsic_load_first_vertex:
-               case nir_intrinsic_load_base_instance:
                case nir_intrinsic_vote_all:
                case nir_intrinsic_vote_any:
                case nir_intrinsic_read_first_invocation:
@@ -621,7 +618,6 @@ init_context(isel_context* ctx, nir_shader* shader)
                case nir_intrinsic_load_force_vrs_rates_amd:
                case nir_intrinsic_load_scalar_arg_amd:
                case nir_intrinsic_load_smem_amd: type = RegType::sgpr; break;
-               case nir_intrinsic_load_sample_id:
                case nir_intrinsic_load_input:
                case nir_intrinsic_load_output:
                case nir_intrinsic_load_input_vertex:
@@ -637,8 +633,6 @@ init_context(isel_context* ctx, nir_shader* shader)
                case nir_intrinsic_load_interpolated_input:
                case nir_intrinsic_load_frag_coord:
                case nir_intrinsic_load_frag_shading_rate:
-               case nir_intrinsic_load_sample_pos:
-               case nir_intrinsic_load_local_invocation_id:
                case nir_intrinsic_load_local_invocation_index:
                case nir_intrinsic_load_subgroup_invocation:
                case nir_intrinsic_load_tess_coord:
@@ -646,7 +640,6 @@ init_context(isel_context* ctx, nir_shader* shader)
                case nir_intrinsic_mbcnt_amd:
                case nir_intrinsic_byte_permute_amd:
                case nir_intrinsic_lane_permute_16_amd:
-               case nir_intrinsic_load_instance_id:
                case nir_intrinsic_ssbo_atomic_add:
                case nir_intrinsic_ssbo_atomic_imin:
                case nir_intrinsic_ssbo_atomic_umin:
-- 
GitLab

