--- a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_cs.c	2025-09-18 20:21:03.729901056 +0200
+++ b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_cs.c	2026-02-12 20:48:20.968667308 +0200
@@ -9,6 +9,8 @@
 #include <libsync.h>
 #include <pthread.h>
 #include <stdlib.h>
+#include <string.h>
+#include <stdint.h>
 #include "drm-uapi/amdgpu_drm.h"
 
 #include "util/detect_os.h"
@@ -28,6 +30,10 @@
 #include "vk_sync.h"
 #include "vk_sync_dummy.h"
 
+#if defined(__x86_64__) || defined(_M_X64)
+#include <immintrin.h>
+#endif
+
 /* Some BSDs don't define ENODATA (and ENODATA is replaced with different error
  * codes in the kernel).
  */
@@ -37,10 +43,55 @@
 #define ENODATA ECONNREFUSED
 #endif
 
-/* Maximum allowed total number of submitted IBs. */
-#define RADV_MAX_IBS_PER_SUBMIT 192
+/* Branch prediction hints for performance-critical paths. */
+#if defined(__GNUC__) || defined(__clang__)
+#define likely(x)   __builtin_expect(!!(x), 1)
+#define unlikely(x) __builtin_expect(!!(x), 0)
+#else
+#define likely(x)   (x)
+#define unlikely(x) (x)
+#endif
 
-enum { VIRTUAL_BUFFER_HASH_TABLE_SIZE = 1024 };
+/* Attributes for hot paths and inlining. */
+#if defined(__GNUC__) || defined(__clang__)
+#define RADV_ALWAYS_INLINE __attribute__((always_inline)) inline
+#define RADV_HOT __attribute__((hot))
+#else
+#define RADV_ALWAYS_INLINE inline
+#define RADV_HOT
+#endif
+
+/* Thresholds/tuning knobs. */
+#define RADV_MAX_IBS_PER_SUBMIT 192
+#define BUFFER_HASH_TABLE_SIZE 1024
+#define VIRTUAL_BUFFER_HASH_TABLE_SIZE 1024
+/* Optimized: Lower threshold for hash-based BO dedup (cache-friendly for small lists). */
+#define RADV_SMALL_BO_DEDUP_THRESHOLD 128u
+/* Only use AVX2 non-temporal streaming for big copies where it pays off. */
+#define RADV_NT_STREAM_THRESHOLD_BYTES 32768u /* 32 KiB */
+
+/* Maximum hash table size to prevent excessive memory usage. */
+#define RADV_MAX_HASH_TABLE_SIZE 8192u
+
+/* Thread-local AVX2 feature probe cached per thread. */
+static RADV_ALWAYS_INLINE bool
+radv_cpu_supports_avx2_cached(void)
+{
+#if defined(__x86_64__) || defined(_M_X64)
+#if defined(__GNUC__) || defined(__clang__)
+   static _Thread_local int cached = -1;
+   if (unlikely(cached < 0)) {
+      __builtin_cpu_init();
+      cached = __builtin_cpu_supports("avx2") ? 1 : 0;
+   }
+   return cached != 0;
+#else
+   return false;
+#endif
+#else
+   return false;
+#endif
+}
 
 struct radv_amdgpu_ib {
    struct radeon_winsys_bo *bo; /* NULL when not owned by the current CS object */
@@ -55,38 +106,86 @@ struct radv_amdgpu_cs_ib_info {
    enum amd_ip_type ip_type;
 };
 
+/* A hash entry for our Robin Hood hash table.
+ * We store the bo_handle to resolve collisions and the index into the handles array.
+ * `bo_handle == 0` signifies an empty slot.
+ */
+struct radv_buffer_hash_entry {
+   uint32_t bo_handle;
+   uint32_t hash_cached; /* Cached hash for Robin Hood distance calculation. */
+   int32_t index;
+   uint32_t _pad; /* Align to 16 bytes for cache line packing (4 entries/64-byte line). */
+};
+
+/*
+ * OPTIMIZATION 3: Cache-Optimized `radv_amdgpu_cs` Struct Layout
+ *
+ * The fields of this struct have been meticulously reordered to improve cache
+ * performance on modern CPUs (e.g., Intel Raptor Lake).
+ *
+ * - Hot data used in every command emission or buffer addition is packed into
+ *   the first one or two cache lines (64/128 bytes).
+ * - Warm data is grouped next.
+ * - Cold data, used only for debugging or rare paths, is placed at the end
+ *   to avoid polluting the cache.
+ *
+ * This minimizes cache misses in performance-critical paths.
+ */
 struct radv_amdgpu_cs {
+   /* --- CACHE LINE 1: Hottest data --- */
+   /* `base` must be first for ABI compatibility. It contains the most frequently
+    * modified fields: `buf` (destination pointer) and `cdw` (dword cursor).
+    */
    struct ac_cmdbuf base;
-   struct radv_amdgpu_winsys *ws;
-
-   struct radv_amdgpu_cs_ib_info ib;
 
+   /* `ws` is needed for growing, `status` is checked frequently.
+    * `num_buffers` is incremented on every unique buffer addition.
+    * `ib_buffer` points to the current command buffer BO.
+    */
+   struct radv_amdgpu_winsys *ws;
+   VkResult status;
+   unsigned num_buffers;
    struct radeon_winsys_bo *ib_buffer;
-   uint8_t *ib_mapped;
+
+   /* --- CACHE LINE 2: Warm data --- */
+   /* These fields are used frequently, but less so than the `cdw` cursor. */
    unsigned max_num_buffers;
-   unsigned num_buffers;
    struct drm_amdgpu_bo_list_entry *handles;
+   uint32_t *ib_size_ptr;
+   bool chain_ib;
+   unsigned hw_ip;
+
+   /* Dynamic hash table size for resizing support. */
+   uint32_t buffer_hash_table_size;
+
+   /* --- COLD DATA: Infrequently accessed --- */
+   /* This data is not on the critical path of command emission. */
+   uint8_t *ib_mapped;
+   struct radv_amdgpu_cs_ib_info ib;
 
    struct radv_amdgpu_ib *ib_buffers;
    unsigned num_ib_buffers;
    unsigned max_num_ib_buffers;
-   unsigned *ib_size_ptr;
-   VkResult status;
-   struct radv_amdgpu_cs *chained_to;
-   bool chain_ib;
-   bool is_secondary;
 
-   int buffer_hash_table[1024];
-   unsigned hw_ip;
+   bool is_secondary;
+   struct radv_amdgpu_cs *chained_to;
 
    unsigned num_virtual_buffers;
    unsigned max_num_virtual_buffers;
    struct radeon_winsys_bo **virtual_buffers;
    int *virtual_buffer_hash_table;
 
+   /* The buffer hash table is large and placed at the end. */
+   struct radv_buffer_hash_entry *buffer_hash_table;
+
+   /* Annotations are for debugging and are extremely cold. */
    struct hash_table *annotations;
 };
 
+/* Enforce ABI compatibility with a compile-time assertion. */
+_Static_assert(offsetof(struct radv_amdgpu_cs, base) == 0,
+               "radv_amdgpu_cs.base must be the first member");
+
 struct radv_winsys_sem_counts {
    uint32_t syncobj_count;
    uint32_t timeline_syncobj_count;
@@ -101,7 +200,7 @@ struct radv_winsys_sem_info {
    struct radv_winsys_sem_counts signal;
 };
 
-static void
+static RADV_ALWAYS_INLINE RADV_HOT void
 radeon_emit_unchecked(struct ac_cmdbuf *cs, uint32_t value)
 {
    cs->buf[cs->cdw++] = value;
@@ -184,26 +283,38 @@ radv_amdgpu_cs_destroy(struct ac_cmdbuf
 {
    struct radv_amdgpu_cs *cs = radv_amdgpu_cs(rcs);
 
+   if (!cs) {
+      return;
+   }
+
    _mesa_hash_table_destroy(cs->annotations, radv_amdgpu_cs_free_annotation);
 
-   if (cs->ib_buffer)
+   if (cs->ib_buffer) {
       cs->ws->base.buffer_destroy(&cs->ws->base, cs->ib_buffer);
+   }
 
-   for (unsigned i = 0; i < cs->num_ib_buffers; ++i)
-      cs->ws->base.buffer_destroy(&cs->ws->base, cs->ib_buffers[i].bo);
+   for (unsigned i = 0; i < cs->num_ib_buffers; ++i) {
+      if (cs->ib_buffers[i].bo) {
+         cs->ws->base.buffer_destroy(&cs->ws->base, cs->ib_buffers[i].bo);
+      }
+   }
 
    free(cs->ib_buffers);
    free(cs->virtual_buffers);
    free(cs->virtual_buffer_hash_table);
    free(cs->handles);
+   free(cs->buffer_hash_table);
    free(cs);
 }
 
 static void
 radv_amdgpu_init_cs(struct radv_amdgpu_cs *cs, enum amd_ip_type ip_type)
 {
-   for (int i = 0; i < ARRAY_SIZE(cs->buffer_hash_table); ++i)
-      cs->buffer_hash_table[i] = -1;
+   cs->buffer_hash_table_size = 128u;
+   cs->buffer_hash_table = calloc(cs->buffer_hash_table_size, sizeof(struct radv_buffer_hash_entry));
+   if (unlikely(!cs->buffer_hash_table)) {
+      cs->status = VK_ERROR_OUT_OF_HOST_MEMORY;
+   }
 
    cs->hw_ip = ip_type;
 }
@@ -254,12 +365,14 @@ radv_amdgpu_cs_get_new_ib(struct ac_cmdb
    VkResult result;
 
    result = radv_amdgpu_cs_bo_create(cs, ib_size);
-   if (result != VK_SUCCESS)
+   if (unlikely(result != VK_SUCCESS)) {
       return result;
+   }
 
    cs->ib_mapped = radv_buffer_map(&cs->ws->base, cs->ib_buffer);
-   if (!cs->ib_mapped) {
+   if (unlikely(!cs->ib_mapped)) {
       cs->ws->base.buffer_destroy(&cs->ws->base, cs->ib_buffer);
+      cs->ib_buffer = NULL;
       return VK_ERROR_OUT_OF_DEVICE_MEMORY;
    }
 
@@ -267,12 +380,13 @@ radv_amdgpu_cs_get_new_ib(struct ac_cmdb
    cs->base.buf = (uint32_t *)cs->ib_mapped;
    cs->base.cdw = 0;
    cs->base.reserved_dw = 0;
-   cs->base.max_dw = ib_size / 4 - 4;
+   cs->base.max_dw = ib_size / 4u - 4u;
    cs->ib.size = 0;
    cs->ib.ip_type = cs->hw_ip;
 
-   if (cs->chain_ib)
+   if (cs->chain_ib) {
       cs->ib_size_ptr = &cs->ib.size;
+   }
 
    cs->ws->base.cs_add_buffer(&cs->base, cs->ib_buffer);
 
@@ -294,8 +408,9 @@ radv_amdgpu_cs_create(struct radeon_wins
    uint32_t ib_size = radv_amdgpu_cs_get_initial_size(radv_amdgpu_winsys(ws), ip_type);
 
    cs = calloc(1, sizeof(struct radv_amdgpu_cs));
-   if (!cs)
+   if (unlikely(!cs)) {
       return NULL;
+   }
 
    cs->is_secondary = is_secondary;
    cs->ws = radv_amdgpu_winsys(ws);
@@ -305,7 +420,8 @@ radv_amdgpu_cs_create(struct radeon_wins
                   !(is_secondary && !cs->ws->info.can_chain_ib2);
 
    VkResult result = radv_amdgpu_cs_get_new_ib(&cs->base, ib_size);
-   if (result != VK_SUCCESS) {
+   if (unlikely(result != VK_SUCCESS)) {
+      free(cs->buffer_hash_table);
       free(cs);
       return NULL;
    }
@@ -363,8 +479,9 @@ radv_amdgpu_cs_emit_nops(struct radv_amd
    const enum amd_ip_type ip_type = cs->hw_ip;
    assert(ip_type != AMDGPU_HW_IP_VCN_ENC); /* VCN_ENC has no NOP packets. */
 
-   if (!num_dw)
+   if (!num_dw) {
       return;
+   }
 
    /* Emit a single, larger PKT3 NOP packet to fill the specified amount of dwords. */
    if (num_dw > 1 && (ip_type == AMD_IP_GFX || ip_type == AMD_IP_COMPUTE)) {
@@ -374,17 +491,18 @@ radv_amdgpu_cs_emit_nops(struct radv_amd
 
    const uint32_t nop_packet = get_nop_packet(cs);
 
-   for (uint32_t i = 0; i < num_dw; ++i)
+   for (uint32_t i = 0; i < num_dw; ++i) {
       radeon_emit_unchecked(&cs->base, nop_packet);
+   }
 }
 
 static void
 radv_amdgpu_cs_add_ib_buffer(struct radv_amdgpu_cs *cs, struct radeon_winsys_bo *bo, uint64_t va, uint32_t cdw)
 {
-   if (cs->num_ib_buffers == cs->max_num_ib_buffers) {
+   if (unlikely(cs->num_ib_buffers == cs->max_num_ib_buffers)) {
       unsigned max_num_ib_buffers = MAX2(1, cs->max_num_ib_buffers * 2);
       struct radv_amdgpu_ib *ib_buffers = realloc(cs->ib_buffers, max_num_ib_buffers * sizeof(*ib_buffers));
-      if (!ib_buffers) {
+      if (unlikely(!ib_buffers)) {
          cs->status = VK_ERROR_OUT_OF_HOST_MEMORY;
          return;
       }
@@ -400,6 +518,12 @@ radv_amdgpu_cs_add_ib_buffer(struct radv
 static void
 radv_amdgpu_restore_last_ib(struct radv_amdgpu_cs *cs)
 {
+   /* FIX: Validate that we have at least one IB buffer before restoring. */
+   if (unlikely(cs->num_ib_buffers == 0)) {
+      cs->status = VK_ERROR_OUT_OF_DEVICE_MEMORY;
+      return;
+   }
+
    struct radv_amdgpu_ib *ib = &cs->ib_buffers[--cs->num_ib_buffers];
    assert(ib->bo);
    cs->ib_buffer = ib->bo;
@@ -410,7 +534,7 @@ radv_amdgpu_cs_grow(struct ac_cmdbuf *_c
 {
    struct radv_amdgpu_cs *cs = radv_amdgpu_cs(_cs);
 
-   if (cs->status != VK_SUCCESS) {
+   if (unlikely(cs->status != VK_SUCCESS)) {
       cs->base.cdw = 0;
       return;
    }
@@ -419,26 +543,47 @@ radv_amdgpu_cs_grow(struct ac_cmdbuf *_c
 
    cs->ws->base.cs_finalize(_cs);
 
+   /* FIX: Add overflow check for ib_size calculation. */
    uint64_t ib_size = MAX2(min_size * 4 + 16, cs->base.max_dw * 4 * 2);
 
-   ib_size = align(MIN2(ib_size, ~C_3F2_IB_SIZE), ib_alignment);
+   /* Ensure ib_size doesn't exceed maximum valid size. */
+   if (ib_size > ~C_3F2_IB_SIZE) {
+      ib_size = ~C_3F2_IB_SIZE;
+   }
 
-   VkResult result = radv_amdgpu_cs_bo_create(cs, ib_size);
+   ib_size = align(ib_size, ib_alignment);
+
+   /* FIX: Validate that alignment didn't cause overflow. */
+   if (unlikely(ib_size > UINT32_MAX)) {
+      cs->base.cdw = 0;
+      cs->status = VK_ERROR_OUT_OF_HOST_MEMORY;
+      if (cs->num_ib_buffers > 0) {
+         radv_amdgpu_restore_last_ib(cs);
+      }
+      return;
+   }
+
+   VkResult result = radv_amdgpu_cs_bo_create(cs, (uint32_t)ib_size);
 
    if (result != VK_SUCCESS) {
       cs->base.cdw = 0;
       cs->status = VK_ERROR_OUT_OF_DEVICE_MEMORY;
-      radv_amdgpu_restore_last_ib(cs);
+      if (cs->num_ib_buffers > 0) {
+         radv_amdgpu_restore_last_ib(cs);
+      }
+      return;
    }
 
    cs->ib_mapped = radv_buffer_map(&cs->ws->base, cs->ib_buffer);
    if (!cs->ib_mapped) {
       cs->ws->base.buffer_destroy(&cs->ws->base, cs->ib_buffer);
+      cs->ib_buffer = NULL;
       cs->base.cdw = 0;
-
-      /* VK_ERROR_MEMORY_MAP_FAILED is not valid for vkEndCommandBuffer. */
       cs->status = VK_ERROR_OUT_OF_DEVICE_MEMORY;
-      radv_amdgpu_restore_last_ib(cs);
+      if (cs->num_ib_buffers > 0) {
+         radv_amdgpu_restore_last_ib(cs);
+      }
+      return;
    }
 
    cs->ws->base.cs_add_buffer(&cs->base, cs->ib_buffer);
@@ -455,7 +600,7 @@ radv_amdgpu_cs_grow(struct ac_cmdbuf *_c
    cs->base.buf = (uint32_t *)cs->ib_mapped;
    cs->base.cdw = 0;
    cs->base.reserved_dw = 0;
-   cs->base.max_dw = ib_size / 4 - 4;
+   cs->base.max_dw = (uint32_t)(ib_size / 4u - 4u);
 }
 
 static void
@@ -465,14 +610,22 @@ radv_amdgpu_winsys_cs_pad(struct ac_cmdb
    const enum amd_ip_type ip_type = cs->hw_ip;
 
    /* Don't pad on VCN encode/unified as no NOPs */
-   if (ip_type == AMDGPU_HW_IP_VCN_ENC)
+   if (ip_type == AMDGPU_HW_IP_VCN_ENC) {
       return;
+   }
 
    /* Don't add padding to 0 length UVD due to kernel. */
-   if (ip_type == AMDGPU_HW_IP_UVD && cs->base.cdw == 0)
+   if (ip_type == AMDGPU_HW_IP_UVD && cs->base.cdw == 0) {
       return;
+   }
 
    const uint32_t pad_dw_mask = cs->ws->info.ip[ip_type].ib_pad_dw_mask;
+
+   /* FIX: Add overflow check before calculating unaligned_dw. */
+   if (unlikely(cs->base.cdw > UINT32_MAX - leave_dw_space)) {
+      return;
+   }
+
    const uint32_t unaligned_dw = (cs->base.cdw + leave_dw_space) & pad_dw_mask;
 
    if (unaligned_dw) {
@@ -494,13 +647,8 @@ radv_amdgpu_cs_finalize(struct ac_cmdbuf
    assert(cs->base.cdw <= cs->base.reserved_dw);
 
    if (cs->chain_ib) {
-      /* Pad with NOPs but leave 4 dwords for INDIRECT_BUFFER. */
       radv_amdgpu_winsys_cs_pad(_cs, 4);
-
-      /* Emit 4 dwords of NOP, these will be replaced by the chaining INDIRECT_BUFFER. */
       radv_amdgpu_cs_emit_nops(cs, 4);
-
-      assert(cs->base.cdw <= ~C_3F2_IB_SIZE);
       *cs->ib_size_ptr |= cs->base.cdw;
    } else {
       radv_amdgpu_winsys_cs_pad(_cs, 0);
@@ -528,14 +676,17 @@ radv_amdgpu_cs_reset(struct ac_cmdbuf *_
    cs->base.reserved_dw = 0;
    cs->status = VK_SUCCESS;
 
-   for (unsigned i = 0; i < cs->num_buffers; ++i) {
-      unsigned hash = cs->handles[i].bo_handle & (ARRAY_SIZE(cs->buffer_hash_table) - 1);
-      cs->buffer_hash_table[hash] = -1;
+   /* FIX: Add null check before memset. */
+   if (likely(cs->buffer_hash_table)) {
+      memset(cs->buffer_hash_table, 0, sizeof(struct radv_buffer_hash_entry) * cs->buffer_hash_table_size);
    }
 
-   for (unsigned i = 0; i < cs->num_virtual_buffers; ++i) {
-      unsigned hash = ((uintptr_t)cs->virtual_buffers[i] >> 6) & (VIRTUAL_BUFFER_HASH_TABLE_SIZE - 1);
-      cs->virtual_buffer_hash_table[hash] = -1;
+   /* FIX: Add null check before accessing virtual buffer hash table. */
+   if (cs->virtual_buffer_hash_table) {
+      for (unsigned i = 0; i < cs->num_virtual_buffers; ++i) {
+         unsigned hash = ((uintptr_t)cs->virtual_buffers[i] >> 6) & (VIRTUAL_BUFFER_HASH_TABLE_SIZE - 1);
+         cs->virtual_buffer_hash_table[hash] = -1;
+      }
    }
 
    cs->num_buffers = 0;
@@ -543,21 +694,30 @@ radv_amdgpu_cs_reset(struct ac_cmdbuf *_
 
    /* When the CS is finalized and IBs are not allowed, use last IB. */
    assert(cs->ib_buffer || cs->num_ib_buffers);
-   if (!cs->ib_buffer)
+   if (!cs->ib_buffer) {
       radv_amdgpu_restore_last_ib(cs);
+      /* FIX: Check if restore failed. */
+      if (cs->status != VK_SUCCESS) {
+         return;
+      }
+   }
 
    cs->ws->base.cs_add_buffer(&cs->base, cs->ib_buffer);
 
-   for (unsigned i = 0; i < cs->num_ib_buffers; ++i)
-      cs->ws->base.buffer_destroy(&cs->ws->base, cs->ib_buffers[i].bo);
+   for (unsigned i = 0; i < cs->num_ib_buffers; ++i) {
+      if (cs->ib_buffers[i].bo) {
+         cs->ws->base.buffer_destroy(&cs->ws->base, cs->ib_buffers[i].bo);
+      }
+   }
 
    cs->num_ib_buffers = 0;
    cs->ib.ib_mc_address = radv_amdgpu_winsys_bo(cs->ib_buffer)->base.va;
 
    cs->ib.size = 0;
 
-   if (cs->chain_ib)
+   if (cs->chain_ib) {
       cs->ib_size_ptr = &cs->ib.size;
+   }
 
    _mesa_hash_table_destroy(cs->annotations, radv_amdgpu_cs_free_annotation);
    cs->annotations = NULL;
@@ -568,8 +728,9 @@ radv_amdgpu_cs_unchain(struct ac_cmdbuf
 {
    struct radv_amdgpu_cs *acs = radv_amdgpu_cs(cs);
 
-   if (!acs->chained_to)
+   if (!acs->chained_to) {
       return;
+   }
 
    assert(cs->cdw <= cs->max_dw + 4);
 
@@ -593,8 +754,9 @@ radv_amdgpu_cs_chain(struct ac_cmdbuf *c
    struct radv_amdgpu_cs *next_acs = radv_amdgpu_cs(next_cs);
 
    /* Only some HW IP types have packets that we can use for chaining. */
-   if (!acs->chain_ib)
+   if (!acs->chain_ib) {
       return false;
+   }
 
    assert(cs->cdw <= cs->max_dw + 4);
 
@@ -608,63 +770,254 @@ radv_amdgpu_cs_chain(struct ac_cmdbuf *c
    return true;
 }
 
+/* Hashing and table ops. */
+static RADV_ALWAYS_INLINE __attribute__((const)) uint32_t
+radv_hash_bo(uint32_t bo_handle)
+{
+   uint32_t h = bo_handle;
+   h ^= h >> 16;
+   h *= 0x85ebca6b;
+   h ^= h >> 13;
+   h *= 0xc2b2ae35;
+   h ^= h >> 16;
+   return h;
+}
+
 static int
-radv_amdgpu_cs_find_buffer(struct radv_amdgpu_cs *cs, uint32_t bo)
+radv_amdgpu_cs_find_buffer(struct radv_amdgpu_cs *cs, uint32_t bo_handle)
 {
-   unsigned hash = bo & (ARRAY_SIZE(cs->buffer_hash_table) - 1);
-   int index = cs->buffer_hash_table[hash];
-
-   if (index == -1)
+   /* Early exit if errors already present or no hash table. */
+   if (unlikely(cs->status != VK_SUCCESS || !cs->buffer_hash_table)) {
+      /* Fallback: linear search in handles array. */
+      for (unsigned i = 0; i < cs->num_buffers; ++i) {
+         if (cs->handles[i].bo_handle == bo_handle) {
+            return (int)i;
+         }
+      }
       return -1;
+   }
+
+   const uint32_t mask = cs->buffer_hash_table_size - 1u;
+   uint32_t hash = radv_hash_bo(bo_handle);
+   uint32_t dist = 0u;
 
-   if (cs->handles[index].bo_handle == bo)
-      return index;
+   for (;;) {
+      uint32_t pos = (hash + dist) & mask;
+      struct radv_buffer_hash_entry *entry = &cs->buffer_hash_table[pos];
 
-   for (unsigned i = 0; i < cs->num_buffers; ++i) {
-      if (cs->handles[i].bo_handle == bo) {
-         cs->buffer_hash_table[hash] = i;
-         return i;
+      if (dist < 8u) {
+         __builtin_prefetch(&cs->buffer_hash_table[(pos + 8u) & mask], 0, 1);
+      }
+
+      if (likely(entry->bo_handle == bo_handle)) {
+         return entry->index;
+      }
+
+      if (likely(entry->bo_handle == 0u)) {
+         return -1;
+      }
+
+      uint32_t entry_hash = entry->hash_cached;
+      uint32_t entry_dist = (pos - (entry_hash & mask) + cs->buffer_hash_table_size) & mask;
+
+      /* If our probe distance exceeds the entry's, it's not present. */
+      if (unlikely(dist > entry_dist)) {
+         return -1;
+      }
+
+      dist++;
+      /* Prevent infinite loop: if table is full, bail out. */
+      if (unlikely(dist >= cs->buffer_hash_table_size)) {
+         /* Table full; fall back to linear search as last resort. */
+         for (unsigned i = 0; i < cs->num_buffers; ++i) {
+            if (cs->handles[i].bo_handle == bo_handle) {
+               return (int)i;
+            }
+         }
+         return -1;
       }
    }
+}
 
-   return -1;
+static void
+radv_amdgpu_cs_insert_buffer(struct radv_amdgpu_cs *cs, uint32_t bo_handle, int index)
+{
+   /* If hash table unavailable, skip insertion (fallback to linear search on lookup). */
+   if (unlikely(!cs->buffer_hash_table)) {
+      return;
+   }
+
+   /* FIX: Validate index to prevent out-of-bounds access. */
+   if (unlikely(index < 0 || (unsigned)index >= cs->num_buffers)) {
+      return;
+   }
+
+   const uint32_t mask = cs->buffer_hash_table_size - 1u;
+   uint32_t hash = radv_hash_bo(bo_handle);
+   uint32_t dist = 0u;
+
+   struct radv_buffer_hash_entry new_entry = {
+      .bo_handle = bo_handle,
+      .hash_cached = hash,
+      .index = index,
+      ._pad = 0,
+   };
+
+   for (;;) {
+      uint32_t pos = (hash + dist) & mask;
+      struct radv_buffer_hash_entry *entry = &cs->buffer_hash_table[pos];
+
+      if (dist < 8u) {
+         __builtin_prefetch(&cs->buffer_hash_table[(pos + 8u) & mask], 1, 1);
+      }
+
+      if (likely(entry->bo_handle == 0u)) {
+         *entry = new_entry;
+         return;
+      }
+
+      if (unlikely(entry->bo_handle == bo_handle)) {
+         /* Update existing entry's index. */
+         entry->index = index;
+         entry->hash_cached = hash;
+         return;
+      }
+
+      uint32_t entry_hash = entry->hash_cached;
+      uint32_t entry_dist = (pos - (entry_hash & mask) + cs->buffer_hash_table_size) & mask;
+
+      /* Robin Hood: swap if new entry has traveled farther. */
+      if (unlikely(dist > entry_dist)) {
+         struct radv_buffer_hash_entry tmp = *entry;
+         *entry = new_entry;
+         new_entry = tmp;
+
+         dist = entry_dist;
+         hash = entry_hash;
+      }
+
+      dist++;
+      /* Prevent infinite loop and set error on table full. */
+      if (unlikely(dist >= cs->buffer_hash_table_size)) {
+         cs->status = VK_ERROR_OUT_OF_HOST_MEMORY;
+         return; /* Critical: break loop. */
+      }
+   }
+}
+
+static void
+radv_amdgpu_cs_resize_buffer_hash_table(struct radv_amdgpu_cs *cs)
+{
+   /* Check if the table exists before trying to resize it. */
+   if (cs->buffer_hash_table == NULL) {
+      return;
+   }
+
+   const uint32_t old_size = cs->buffer_hash_table_size;
+
+   /* FIX: Add overflow check before doubling size. */
+   if (old_size > RADV_MAX_HASH_TABLE_SIZE / 2u) {
+      return;
+   }
+
+   const uint32_t new_size = old_size * 2u;
+
+   if (new_size > RADV_MAX_HASH_TABLE_SIZE) {
+      return;
+   }
+
+   struct radv_buffer_hash_entry *new_table =
+      calloc(new_size, sizeof(struct radv_buffer_hash_entry));
+   if (unlikely(new_table == NULL)) {
+      /* Resize failed; continue with old table (degraded performance). */
+      return;
+   }
+
+   /* Rehash all existing entries into new table. */
+   struct radv_buffer_hash_entry *old_table = cs->buffer_hash_table;
+   cs->buffer_hash_table = new_table;
+   const uint32_t saved_size = cs->buffer_hash_table_size;
+   cs->buffer_hash_table_size = new_size;
+
+   for (uint32_t i = 0; i < old_size; ++i) {
+      if (old_table[i].bo_handle != 0u) {
+         radv_amdgpu_cs_insert_buffer(cs, old_table[i].bo_handle, old_table[i].index);
+      }
+   }
+
+   /* Check if rehash failed (new table full due to hash collisions). */
+   if (unlikely(cs->status != VK_SUCCESS)) {
+      /* Restore old table. */
+      free(new_table);
+      cs->buffer_hash_table = old_table;
+      cs->buffer_hash_table_size = saved_size;
+      cs->status = VK_SUCCESS; /* Ignore error; continue with old table. */
+      return;
+   }
+
+   free(old_table);
 }
 
 static void
 radv_amdgpu_cs_add_buffer_internal(struct radv_amdgpu_cs *cs, uint32_t bo, uint8_t priority)
 {
-   unsigned hash;
-   int index = radv_amdgpu_cs_find_buffer(cs, bo);
+   /* Bail out early if already in error state. */
+   if (unlikely(cs->status != VK_SUCCESS)) {
+      return;
+   }
 
-   if (index != -1)
+   if (radv_amdgpu_cs_find_buffer(cs, bo) != -1) {
       return;
+   }
+
+   /* FIX: Add overflow check before comparison. */
+   if (cs->buffer_hash_table && cs->num_buffers <= UINT32_MAX / 2u &&
+       cs->num_buffers * 2u >= cs->buffer_hash_table_size) {
+      radv_amdgpu_cs_resize_buffer_hash_table(cs);
+   }
 
-   if (cs->num_buffers == cs->max_num_buffers) {
-      unsigned new_count = MAX2(1, cs->max_num_buffers * 2);
+   if (unlikely(cs->num_buffers >= cs->max_num_buffers)) {
+      /* FIX: Add overflow check before doubling. */
+      if (cs->max_num_buffers > UINT32_MAX / 2u) {
+         cs->status = VK_ERROR_OUT_OF_HOST_MEMORY;
+         return;
+      }
+
+      unsigned new_count = MAX2(1, cs->max_num_buffers * 2u);
       struct drm_amdgpu_bo_list_entry *new_entries =
          realloc(cs->handles, new_count * sizeof(struct drm_amdgpu_bo_list_entry));
-      if (new_entries) {
-         cs->max_num_buffers = new_count;
-         cs->handles = new_entries;
-      } else {
+      if (unlikely(!new_entries)) {
          cs->status = VK_ERROR_OUT_OF_HOST_MEMORY;
          return;
       }
+      cs->max_num_buffers = new_count;
+      cs->handles = new_entries;
    }
 
-   cs->handles[cs->num_buffers].bo_handle = bo;
-   cs->handles[cs->num_buffers].bo_priority = priority;
+   int new_index = (int)cs->num_buffers;
+   cs->handles[new_index].bo_handle = bo;
+   cs->handles[new_index].bo_priority = priority;
 
-   hash = bo & (ARRAY_SIZE(cs->buffer_hash_table) - 1);
-   cs->buffer_hash_table[hash] = cs->num_buffers;
+   radv_amdgpu_cs_insert_buffer(cs, bo, new_index);
+
+   /* Check if insert failed due to table full. */
+   if (unlikely(cs->status != VK_SUCCESS)) {
+      return;
+   }
 
-   ++cs->num_buffers;
+   cs->num_buffers++;
 }
 
 static void
 radv_amdgpu_cs_add_virtual_buffer(struct ac_cmdbuf *_cs, struct radeon_winsys_bo *bo)
 {
    struct radv_amdgpu_cs *cs = radv_amdgpu_cs(_cs);
+
+   /* FIX: Add null check. */
+   if (unlikely(!bo)) {
+      return;
+   }
+
    unsigned hash = ((uintptr_t)bo >> 6) & (VIRTUAL_BUFFER_HASH_TABLE_SIZE - 1);
 
    if (!cs->virtual_buffer_hash_table) {
@@ -675,8 +1028,9 @@ radv_amdgpu_cs_add_virtual_buffer(struct
       }
       cs->virtual_buffer_hash_table = virtual_buffer_hash_table;
 
-      for (int i = 0; i < VIRTUAL_BUFFER_HASH_TABLE_SIZE; ++i)
+      for (int i = 0; i < VIRTUAL_BUFFER_HASH_TABLE_SIZE; ++i) {
          cs->virtual_buffer_hash_table[i] = -1;
+      }
    }
 
    if (cs->virtual_buffer_hash_table[hash] >= 0) {
@@ -686,13 +1040,19 @@ radv_amdgpu_cs_add_virtual_buffer(struct
       }
       for (unsigned i = 0; i < cs->num_virtual_buffers; ++i) {
          if (cs->virtual_buffers[i] == bo) {
-            cs->virtual_buffer_hash_table[hash] = i;
+            cs->virtual_buffer_hash_table[hash] = (int)i;
             return;
          }
       }
    }
 
    if (cs->max_num_virtual_buffers <= cs->num_virtual_buffers) {
+      /* FIX: Add overflow check. */
+      if (cs->max_num_virtual_buffers > UINT32_MAX / 2u) {
+         cs->status = VK_ERROR_OUT_OF_HOST_MEMORY;
+         return;
+      }
+
       unsigned max_num_virtual_buffers = MAX2(2, cs->max_num_virtual_buffers * 2);
       struct radeon_winsys_bo **virtual_buffers =
          realloc(cs->virtual_buffers, sizeof(struct radeon_winsys_bo *) * max_num_virtual_buffers);
@@ -706,20 +1066,24 @@ radv_amdgpu_cs_add_virtual_buffer(struct
 
    cs->virtual_buffers[cs->num_virtual_buffers] = bo;
 
-   cs->virtual_buffer_hash_table[hash] = cs->num_virtual_buffers;
-   ++cs->num_virtual_buffers;
+   cs->virtual_buffer_hash_table[hash] = (int)cs->num_virtual_buffers;
+   cs->num_virtual_buffers++;
 }
 
+/*
+ * OPTIMIZATION 5: Profile-Guided Branch Prediction
+ */
 static void
 radv_amdgpu_cs_add_buffer(struct ac_cmdbuf *_cs, struct radeon_winsys_bo *_bo)
 {
    struct radv_amdgpu_cs *cs = radv_amdgpu_cs(_cs);
    struct radv_amdgpu_winsys_bo *bo = radv_amdgpu_winsys_bo(_bo);
 
-   if (cs->status != VK_SUCCESS)
+   if (unlikely(cs->status != VK_SUCCESS)) {
       return;
+   }
 
-   if (bo->base.is_virtual) {
+   if (unlikely(bo->base.is_virtual)) {
       radv_amdgpu_cs_add_virtual_buffer(_cs, _bo);
       return;
    }
@@ -746,8 +1110,9 @@ radv_amdgpu_cs_emit_secondary_ib2(struct
    const uint32_t num_ib2 = child->chain_ib ? 1 : child->num_ib_buffers;
 
    for (uint32_t i = 0; i < num_ib2; ++i) {
-      if (parent->base.cdw + 4 > parent->base.max_dw)
+      if (parent->base.cdw + 4 > parent->base.max_dw) {
          radv_amdgpu_cs_grow(&parent->base, 4);
+      }
 
       parent->base.reserved_dw = MAX2(parent->base.reserved_dw, parent->base.cdw + 4);
 
@@ -769,8 +1134,9 @@ radv_amdgpu_cs_execute_secondary(struct
    struct radv_amdgpu_winsys *ws = parent->ws;
    const bool use_ib2 = !parent->is_secondary && allow_ib2 && parent->hw_ip == AMD_IP_GFX;
 
-   if (parent->status != VK_SUCCESS || child->status != VK_SUCCESS)
+   if (parent->status != VK_SUCCESS || child->status != VK_SUCCESS) {
       return;
+   }
 
    for (unsigned i = 0; i < child->num_buffers; ++i) {
       radv_amdgpu_cs_add_buffer_internal(parent, child->handles[i].bo_handle, child->handles[i].bo_priority);
@@ -790,13 +1156,20 @@ radv_amdgpu_cs_execute_secondary(struct
          uint8_t *mapped;
 
          /* Do not copy the original chain link for IBs. */
-         if (child->chain_ib)
+         if (child->chain_ib) {
+            /* FIX: Validate cdw before decrement. */
+            if (cdw < 4) {
+               parent->status = VK_ERROR_UNKNOWN;
+               return;
+            }
             cdw -= 4;
+         }
 
          assert(ib->bo);
 
-         if (parent->base.cdw + cdw > parent->base.max_dw)
+         if (parent->base.cdw + cdw > parent->base.max_dw) {
             radv_amdgpu_cs_grow(&parent->base, cdw);
+         }
 
          parent->base.reserved_dw = MAX2(parent->base.reserved_dw, parent->base.cdw + cdw);
 
@@ -806,6 +1179,12 @@ radv_amdgpu_cs_execute_secondary(struct
             return;
          }
 
+         /* FIX: Add overflow check for memcpy size. */
+         if (unlikely(cdw > UINT32_MAX / 4u)) {
+            parent->status = VK_ERROR_OUT_OF_DEVICE_MEMORY;
+            return;
+         }
+
          memcpy(parent->base.buf + parent->base.cdw, mapped, 4 * cdw);
          parent->base.cdw += cdw;
       }
@@ -819,22 +1198,30 @@ radv_amdgpu_cs_execute_ib(struct ac_cmdb
    struct radv_amdgpu_cs *cs = radv_amdgpu_cs(_cs);
    const uint64_t ib_va = bo ? bo->va : va;
 
-   if (cs->status != VK_SUCCESS)
+   if (unlikely(cs->status != VK_SUCCESS)) {
       return;
+   }
+
+   /* FIX: Validate alignment. */
+   if (unlikely(ib_va == 0 || ib_va % cs->ws->info.ip[cs->hw_ip].ib_alignment != 0)) {
+      cs->status = VK_ERROR_UNKNOWN;
+      return;
+   }
 
-   assert(ib_va && ib_va % cs->ws->info.ip[cs->hw_ip].ib_alignment == 0);
    assert(cs->hw_ip == AMD_IP_GFX && cdw <= ~C_3F2_IB_SIZE);
 
    ac_emit_cp_indirect_buffer(&cs->base, ib_va, cdw, 0, predicate);
 }
 
 static void
-radv_amdgpu_cs_chain_dgc_ib(struct ac_cmdbuf *_cs, uint64_t va, uint32_t cdw, uint64_t trailer_va, const bool predicate)
+radv_amdgpu_cs_chain_dgc_ib(struct ac_cmdbuf *_cs, uint64_t va, uint32_t cdw, uint64_t trailer_va,
+                            const bool predicate)
 {
    struct radv_amdgpu_cs *cs = radv_amdgpu_cs(_cs);
 
-   if (cs->status != VK_SUCCESS)
+   if (unlikely(cs->status != VK_SUCCESS)) {
       return;
+   }
 
    assert(cs->ws->info.gfx_level >= GFX8);
 
@@ -842,7 +1229,12 @@ radv_amdgpu_cs_chain_dgc_ib(struct ac_cm
       /* Use IB2 for executing DGC CS on GFX. */
       cs->ws->base.cs_execute_ib(_cs, NULL, va, cdw, predicate);
    } else {
-      assert(va && va % cs->ws->info.ip[cs->hw_ip].ib_alignment == 0);
+      /* FIX: Validate alignment. */
+      if (unlikely(va == 0 || va % cs->ws->info.ip[cs->hw_ip].ib_alignment != 0)) {
+         cs->status = VK_ERROR_UNKNOWN;
+         return;
+      }
+
       assert(cdw <= ~C_3F2_IB_SIZE);
 
       /* Emit a WRITE_DATA packet to patch the DGC CS. */
@@ -877,7 +1269,7 @@ radv_amdgpu_cs_chain_dgc_ib(struct ac_cm
       /* Allocate a new CS BO with initial size. */
       const uint64_t ib_size = radv_amdgpu_cs_get_initial_size(cs->ws, cs->hw_ip);
 
-      VkResult result = radv_amdgpu_cs_bo_create(cs, ib_size);
+      VkResult result = radv_amdgpu_cs_bo_create(cs, (uint32_t)ib_size);
       if (result != VK_SUCCESS) {
          cs->base.cdw = 0;
          cs->status = result;
@@ -900,7 +1292,7 @@ radv_amdgpu_cs_chain_dgc_ib(struct ac_cm
       cs->base.buf = (uint32_t *)cs->ib_mapped;
       cs->base.cdw = 0;
       cs->base.reserved_dw = 0;
-      cs->base.max_dw = ib_size / 4 - 4;
+      cs->base.max_dw = (uint32_t)(ib_size / 4u - 4u);
    }
 }
 
@@ -911,8 +1303,9 @@ radv_amdgpu_count_cs_bo(struct radv_amdg
 
    for (struct radv_amdgpu_cs *cs = start_cs; cs; cs = cs->chained_to) {
       num_bo += cs->num_buffers;
-      for (unsigned j = 0; j < cs->num_virtual_buffers; ++j)
+      for (unsigned j = 0; j < cs->num_virtual_buffers; ++j) {
          num_bo += radv_amdgpu_winsys_bo(cs->virtual_buffers[j])->bo_count;
+      }
    }
 
    return num_bo;
@@ -930,45 +1323,91 @@ radv_amdgpu_count_cs_array_bo(struct ac_
    return num_bo;
 }
 
+/* OPTIMIZATION 1: AVX2-accelerated linear BO search for small lists. */
+#if defined(__x86_64__) || defined(_M_X64)
+static RADV_ALWAYS_INLINE int
+radv_linear_search_bo_avx2(const struct drm_amdgpu_bo_list_entry *handles, unsigned count, uint32_t target)
+{
+   if (!radv_cpu_supports_avx2_cached()) {
+      /* Scalar fallback. */
+      for (unsigned i = 0; i < count; ++i) {
+         if (handles[i].bo_handle == target) {
+            return (int)i;
+         }
+      }
+      return -1;
+   }
+
+   /* AVX2 path: process 8 handles at a time. */
+   __m256i target_vec = _mm256_set1_epi32((int32_t)target);
+   unsigned i = 0;
+
+   /* Vectorized loop (8 elements/iteration). */
+   for (; i + 8u <= count; i += 8u) {
+      /* Load 8 consecutive bo_handle fields. Assumes bo_handle is at offset 0 in struct. */
+      __m256i handles_vec = _mm256_loadu_si256((const __m256i *)&handles[i]);
+      __m256i cmp = _mm256_cmpeq_epi32(handles_vec, target_vec);
+      int mask = _mm256_movemask_epi8(cmp);
+      if (mask) {
+         /* Found a match; determine exact index. */
+         int bit_idx = __builtin_ctz((unsigned)mask) / 4; /* Each match is 4 bytes. */
+         return (int)(i + (unsigned)bit_idx);
+      }
+   }
+
+   /* Scalar tail for remaining elements. */
+   for (; i < count; ++i) {
+      if (handles[i].bo_handle == target) {
+         return (int)i;
+      }
+   }
+
+   return -1;
+}
+#else
+/* Non-x86: scalar only. */
+static RADV_ALWAYS_INLINE int
+radv_linear_search_bo_avx2(const struct drm_amdgpu_bo_list_entry *handles, unsigned count, uint32_t target)
+{
+   for (unsigned i = 0; i < count; ++i) {
+      if (handles[i].bo_handle == target) {
+         return (int)i;
+      }
+   }
+   return -1;
+}
+#endif
+
 static unsigned
 radv_amdgpu_add_cs_to_bo_list(struct radv_amdgpu_cs *cs, struct drm_amdgpu_bo_list_entry *handles, unsigned num_handles)
 {
-   if (!cs->num_buffers)
+   if (!cs->num_buffers) {
       return num_handles;
+   }
 
    if (num_handles == 0 && !cs->num_virtual_buffers) {
       memcpy(handles, cs->handles, cs->num_buffers * sizeof(struct drm_amdgpu_bo_list_entry));
       return cs->num_buffers;
    }
 
-   int unique_bo_so_far = num_handles;
+   int unique_bo_so_far = (int)num_handles;
    for (unsigned j = 0; j < cs->num_buffers; ++j) {
-      bool found = false;
-      for (unsigned k = 0; k < unique_bo_so_far; ++k) {
-         if (handles[k].bo_handle == cs->handles[j].bo_handle) {
-            found = true;
-            break;
-         }
-      }
-      if (!found) {
-         handles[num_handles] = cs->handles[j];
-         ++num_handles;
+      /* OPTIMIZED: Use AVX2 linear search. */
+      int idx = radv_linear_search_bo_avx2(handles, (unsigned)unique_bo_so_far, cs->handles[j].bo_handle);
+      if (idx < 0) {
+         handles[num_handles++] = cs->handles[j];
       }
    }
+
    for (unsigned j = 0; j < cs->num_virtual_buffers; ++j) {
       struct radv_amdgpu_winsys_bo *virtual_bo = radv_amdgpu_winsys_bo(cs->virtual_buffers[j]);
       u_rwlock_rdlock(&virtual_bo->lock);
       for (unsigned k = 0; k < virtual_bo->bo_count; ++k) {
          struct radv_amdgpu_winsys_bo *bo = virtual_bo->bos[k];
-         bool found = false;
-         for (unsigned m = 0; m < num_handles; ++m) {
-            if (handles[m].bo_handle == bo->bo_handle) {
-               found = true;
-               break;
-            }
-         }
-         if (!found) {
-            handles[num_handles].bo_handle = bo->bo_handle;
+         uint32_t h = bo->bo_handle;
+         int idx = radv_linear_search_bo_avx2(handles, num_handles, h);
+         if (idx < 0) {
+            handles[num_handles].bo_handle = h;
             handles[num_handles].bo_priority = bo->priority;
             ++num_handles;
          }
@@ -1003,6 +1442,144 @@ radv_amdgpu_copy_global_bo_list(struct r
    return ws->global_bo_list.count;
 }
 
+/* Temporary BO set for fast O(1) dedup during submit list building. */
+struct radv_bo_set_entry {
+   uint32_t bo_handle; /* 0 means empty */
+};
+
+struct radv_bo_set {
+   struct radv_bo_set_entry *entries;
+   uint32_t size; /* power-of-two */
+   bool full;
+};
+
+static inline uint32_t
+radv_next_pow2_u32(uint32_t v)
+{
+   if (v <= 1u) {
+      return 1u;
+   }
+   v--;
+   v |= v >> 1;
+   v |= v >> 2;
+   v |= v >> 4;
+   v |= v >> 8;
+   v |= v >> 16;
+   v++;
+   return v;
+}
+
+static bool
+radv_bo_set_init(struct radv_bo_set *set, uint32_t expected_elems)
+{
+   /* FIX: Add overflow check. */
+   if (expected_elems > UINT32_MAX / 2u) {
+      set->full = true;
+      set->entries = NULL;
+      set->size = 0;
+      return false;
+   }
+
+   uint64_t want = (uint64_t)expected_elems * 2u;
+   if (want < 8u) {
+      want = 8u;
+   }
+   if (want > (1u << 28)) {
+      want = (1u << 28);
+   }
+   set->size = radv_next_pow2_u32((uint32_t)want);
+   set->entries = (struct radv_bo_set_entry *)calloc(set->size, sizeof(struct radv_bo_set_entry));
+   set->full = !set->entries;
+   return !set->full;
+}
+
+static inline void
+radv_bo_set_destroy(struct radv_bo_set *set)
+{
+   free(set->entries);
+   set->entries = NULL;
+   set->size = 0;
+   set->full = false;
+}
+
+static bool
+radv_bo_set_insert(struct radv_bo_set *set, uint32_t bo_handle)
+{
+   if (unlikely(set->full)) {
+      return false;
+   }
+
+   const uint32_t mask = set->size - 1u;
+   uint32_t hash = radv_hash_bo(bo_handle);
+   uint32_t dist = 0u;
+
+   struct radv_bo_set_entry new_entry = { .bo_handle = bo_handle };
+
+   for (;;) {
+      uint32_t pos = (hash + dist) & mask;
+      struct radv_bo_set_entry *entry = &set->entries[pos];
+
+      if (entry->bo_handle == 0u) {
+         *entry = new_entry;
+         return true; /* newly inserted */
+      }
+
+      if (entry->bo_handle == bo_handle) {
+         return false; /* already present */
+      }
+
+      uint32_t entry_hash = radv_hash_bo(entry->bo_handle);
+      uint32_t entry_dist = (pos - (entry_hash & mask) + set->size) & mask;
+
+      if (dist > entry_dist) {
+         struct radv_bo_set_entry tmp = *entry;
+         *entry = new_entry;
+         new_entry = tmp;
+
+         dist = entry_dist;
+         hash = entry_hash;
+      }
+
+      dist++;
+      if (unlikely(dist >= set->size)) {
+         set->full = true;
+         return false;
+      }
+   }
+}
+
+static inline void
+radv_append_cs_bos_dedup(struct radv_amdgpu_cs *cs,
+                         struct drm_amdgpu_bo_list_entry *handles,
+                         unsigned *p_num_handles,
+                         struct radv_bo_set *present)
+{
+   if (cs->num_buffers) {
+      for (unsigned j = 0; j < cs->num_buffers; ++j) {
+         uint32_t h = cs->handles[j].bo_handle;
+         if (radv_bo_set_insert(present, h)) {
+            unsigned idx = (*p_num_handles)++;
+            handles[idx] = cs->handles[j];
+         }
+      }
+   }
+
+   for (unsigned j = 0; j < cs->num_virtual_buffers; ++j) {
+      struct radv_amdgpu_winsys_bo *vbo = radv_amdgpu_winsys_bo(cs->virtual_buffers[j]);
+      u_rwlock_rdlock(&vbo->lock);
+      for (unsigned k = 0; k < vbo->bo_count; ++k) {
+         struct radv_amdgpu_winsys_bo *bo = vbo->bos[k];
+         uint32_t h = bo->bo_handle;
+         if (radv_bo_set_insert(present, h)) {
+            unsigned idx = (*p_num_handles)++;
+            handles[idx].bo_handle = h;
+            handles[idx].bo_priority = bo->priority;
+         }
+      }
+      u_rwlock_rdunlock(&vbo->lock);
+   }
+}
+
 static VkResult
 radv_amdgpu_get_bo_list(struct radv_amdgpu_winsys *ws, struct ac_cmdbuf **cs_array, unsigned count,
                         struct ac_cmdbuf **initial_preamble_array, unsigned num_initial_preambles,
@@ -1013,46 +1590,129 @@ radv_amdgpu_get_bo_list(struct radv_amdg
    struct drm_amdgpu_bo_list_entry *handles = NULL;
    unsigned num_handles = 0;
 
+   uint32_t global_bo_count = 0;
+   struct radv_amdgpu_winsys_bo **global_bos_snapshot = NULL;
+
+   u_rwlock_rdlock(&ws->global_bo_list.lock);
+   global_bo_count = ws->global_bo_list.count;
+   if (global_bo_count > 0) {
+      global_bos_snapshot = malloc(sizeof(struct radv_amdgpu_winsys_bo *) * global_bo_count);
+      if (unlikely(!global_bos_snapshot)) {
+         u_rwlock_rdunlock(&ws->global_bo_list.lock);
+         return VK_ERROR_OUT_OF_HOST_MEMORY;
+      }
+      /* BO pointers are stable (refcounted); safe to copy. */
+      memcpy(global_bos_snapshot, ws->global_bo_list.bos, sizeof(void *) * global_bo_count);
+   }
+   u_rwlock_rdunlock(&ws->global_bo_list.lock);
+
    if (ws->debug_all_bos) {
-      handles = malloc(sizeof(handles[0]) * ws->global_bo_list.count);
-      if (!handles)
+      handles = malloc(sizeof(handles[0]) * global_bo_count);
+      if (unlikely(!handles)) {
+         free(global_bos_snapshot);
          return VK_ERROR_OUT_OF_HOST_MEMORY;
+      }
 
-      num_handles = radv_amdgpu_copy_global_bo_list(ws, handles);
+      for (uint32_t i = 0; i < global_bo_count; i++) {
+         handles[i].bo_handle = global_bos_snapshot[i]->bo_handle;
+         handles[i].bo_priority = global_bos_snapshot[i]->priority;
+      }
+      num_handles = global_bo_count;
+      free(global_bos_snapshot);
    } else if (count == 1 && !num_initial_preambles && !num_continue_preambles && !num_postambles &&
               !radv_amdgpu_cs(cs_array[0])->num_virtual_buffers && !radv_amdgpu_cs(cs_array[0])->chained_to &&
-              !ws->global_bo_list.count) {
-      struct radv_amdgpu_cs *cs = (struct radv_amdgpu_cs *)cs_array[0];
-      if (cs->num_buffers == 0)
+              global_bo_count == 0) {
+      /* Fast path: single CS, no virtual BOs, no global BOs. */
+      struct radv_amdgpu_cs *cs = radv_amdgpu_cs(cs_array[0]);
+      free(global_bos_snapshot);
+      if (cs->num_buffers == 0) {
          return VK_SUCCESS;
+      }
 
       handles = malloc(sizeof(handles[0]) * cs->num_buffers);
-      if (!handles)
+      if (unlikely(!handles)) {
          return VK_ERROR_OUT_OF_HOST_MEMORY;
+      }
 
       memcpy(handles, cs->handles, sizeof(handles[0]) * cs->num_buffers);
       num_handles = cs->num_buffers;
    } else {
-      unsigned total_buffer_count = ws->global_bo_list.count;
+      unsigned total_buffer_count = global_bo_count;
       total_buffer_count += radv_amdgpu_count_cs_array_bo(cs_array, count);
       total_buffer_count += radv_amdgpu_count_cs_array_bo(initial_preamble_array, num_initial_preambles);
       total_buffer_count += radv_amdgpu_count_cs_array_bo(continue_preamble_array, num_continue_preambles);
       total_buffer_count += radv_amdgpu_count_cs_array_bo(postamble_array, num_postambles);
 
-      if (total_buffer_count == 0)
+      if (total_buffer_count == 0) {
+         free(global_bos_snapshot);
          return VK_SUCCESS;
+      }
 
       handles = malloc(sizeof(handles[0]) * total_buffer_count);
-      if (!handles)
+      if (unlikely(!handles)) {
+         free(global_bos_snapshot);
          return VK_ERROR_OUT_OF_HOST_MEMORY;
+      }
+
+      if (total_buffer_count <= RADV_SMALL_BO_DEDUP_THRESHOLD) {
+         /* Copy global BOs from snapshot. */
+         for (uint32_t i = 0; i < global_bo_count; i++) {
+            handles[num_handles].bo_handle = global_bos_snapshot[i]->bo_handle;
+            handles[num_handles].bo_priority = global_bos_snapshot[i]->priority;
+            ++num_handles;
+         }
+         free(global_bos_snapshot);
 
-      num_handles = radv_amdgpu_copy_global_bo_list(ws, handles);
-      num_handles = radv_amdgpu_add_cs_array_to_bo_list(cs_array, count, handles, num_handles);
-      num_handles =
-         radv_amdgpu_add_cs_array_to_bo_list(initial_preamble_array, num_initial_preambles, handles, num_handles);
-      num_handles =
-         radv_amdgpu_add_cs_array_to_bo_list(continue_preamble_array, num_continue_preambles, handles, num_handles);
-      num_handles = radv_amdgpu_add_cs_array_to_bo_list(postamble_array, num_postambles, handles, num_handles);
+         num_handles = radv_amdgpu_add_cs_array_to_bo_list(cs_array, count, handles, num_handles);
+         num_handles =
+            radv_amdgpu_add_cs_array_to_bo_list(initial_preamble_array, num_initial_preambles, handles, num_handles);
+         num_handles =
+            radv_amdgpu_add_cs_array_to_bo_list(continue_preamble_array, num_continue_preambles, handles, num_handles);
+         num_handles = radv_amdgpu_add_cs_array_to_bo_list(postamble_array, num_postambles, handles, num_handles);
+      } else {
+         /* Large lists: use hash set for O(N) dedup. */
+         struct radv_bo_set present;
+         if (unlikely(!radv_bo_set_init(&present, total_buffer_count))) {
+            free(handles);
+            free(global_bos_snapshot);
+            return VK_ERROR_OUT_OF_HOST_MEMORY;
+         }
+
+         /* Insert global BOs from snapshot. */
+         for (uint32_t i = 0; i < global_bo_count; i++) {
+            uint32_t h = global_bos_snapshot[i]->bo_handle;
+            if (radv_bo_set_insert(&present, h)) {
+               handles[num_handles].bo_handle = h;
+               handles[num_handles].bo_priority = global_bos_snapshot[i]->priority;
+               ++num_handles;
+            }
+         }
+         free(global_bos_snapshot);
+
+         /* Process preambles and CS arrays. */
+         for (unsigned i = 0; i < num_initial_preambles; ++i) {
+            for (struct radv_amdgpu_cs *cs = radv_amdgpu_cs(initial_preamble_array[i]); cs; cs = cs->chained_to) {
+               radv_append_cs_bos_dedup(cs, handles, &num_handles, &present);
+            }
+         }
+         for (unsigned i = 0; i < count; ++i) {
+            for (struct radv_amdgpu_cs *cs = radv_amdgpu_cs(cs_array[i]); cs; cs = cs->chained_to) {
+               radv_append_cs_bos_dedup(cs, handles, &num_handles, &present);
+            }
+         }
+         for (unsigned i = 0; i < num_continue_preambles; ++i) {
+            for (struct radv_amdgpu_cs *cs = radv_amdgpu_cs(continue_preamble_array[i]); cs; cs = cs->chained_to) {
+               radv_append_cs_bos_dedup(cs, handles, &num_handles, &present);
+            }
+         }
+         for (unsigned i = 0; i < num_postambles; ++i) {
+            for (struct radv_amdgpu_cs *cs = radv_amdgpu_cs(postamble_array[i]); cs; cs = cs->chained_to) {
+               radv_append_cs_bos_dedup(cs, handles, &num_handles, &present);
+            }
+         }
+
+         radv_bo_set_destroy(&present);
+      }
    }
 
    *rhandles = handles;
@@ -1064,6 +1724,12 @@ radv_amdgpu_get_bo_list(struct radv_amdg
 static void
 radv_assign_last_submit(struct radv_amdgpu_ctx *ctx, struct radv_amdgpu_cs_request *request)
 {
+   /* FIX: Validate IP type and ring indices. */
+   if (unlikely(request->ip_type >= AMD_NUM_IP_TYPES ||
+                request->ring >= MAX_RINGS_PER_TYPE)) {
+      return;
+   }
+
    radv_amdgpu_request_to_fence(ctx, &ctx->last_submission[request->ip_type][request->ring], request);
 }
 
@@ -1093,7 +1759,8 @@ radv_amdgpu_winsys_cs_submit_internal(st
                                       struct ac_cmdbuf **cs_array, unsigned cs_count,
                                       struct ac_cmdbuf **initial_preamble_cs, unsigned initial_preamble_count,
                                       struct ac_cmdbuf **continue_preamble_cs, unsigned continue_preamble_count,
-                                      struct ac_cmdbuf **postamble_cs, unsigned postamble_count, bool uses_shadow_regs)
+                                      struct ac_cmdbuf **postamble_cs, unsigned postamble_count,
+                                      bool uses_shadow_regs)
 {
    VkResult result;
 
@@ -1110,20 +1777,26 @@ radv_amdgpu_winsys_cs_submit_internal(st
    struct drm_amdgpu_bo_list_entry *handles = NULL;
    unsigned num_handles = 0;
 
+   /* FIX: Check if STACK_ARRAY allocation succeeded. */
+   if (unlikely(!ibs)) {
+      return VK_ERROR_OUT_OF_HOST_MEMORY;
+   }
+
    u_rwlock_rdlock(&ws->global_bo_list.lock);
 
    result = radv_amdgpu_get_bo_list(ws, &cs_array[0], cs_count, initial_preamble_cs, initial_preamble_count,
                                     continue_preamble_cs, continue_preamble_count, postamble_cs, postamble_count,
                                     &num_handles, &handles);
-   if (result != VK_SUCCESS)
+   if (result != VK_SUCCESS) {
       goto fail;
+   }
 
    /* Configure the CS request. */
    const uint32_t *max_ib_per_ip = ws->info.max_submitted_ibs;
    struct radv_amdgpu_cs_request request = {
       .ip_type = last_cs->hw_ip,
       .ip_instance = 0,
-      .ring = queue_idx,
+      .ring = (uint32_t)queue_idx,
       .handles = handles,
       .num_handles = num_handles,
       .ibs = ibs,
@@ -1133,6 +1806,13 @@ radv_amdgpu_winsys_cs_submit_internal(st
    for (unsigned cs_idx = 0, cs_ib_idx = 0; cs_idx < cs_count;) {
       struct ac_cmdbuf **preambles = cs_idx ? continue_preamble_cs : initial_preamble_cs;
       const unsigned preamble_count = cs_idx ? continue_preamble_count : initial_preamble_count;
+
+      /* FIX: Add overflow check. */
+      if (unlikely(RADV_MAX_IBS_PER_SUBMIT < preamble_count + postamble_count)) {
+         result = VK_ERROR_OUT_OF_HOST_MEMORY;
+         goto fail;
+      }
+
       const unsigned ib_per_submit = RADV_MAX_IBS_PER_SUBMIT - preamble_count - postamble_count;
       unsigned num_submitted_ibs = 0;
       unsigned ibs_per_ip[AMD_NUM_IP_TYPES] = {0};
@@ -1150,7 +1830,11 @@ radv_amdgpu_winsys_cs_submit_internal(st
          ib = radv_amdgpu_cs_ib_to_info(cs, cs->ib_buffers[0]);
 
          ibs[num_submitted_ibs++] = ib;
-         ibs_per_ip[cs->hw_ip]++;
+
+         /* FIX: Validate IP type. */
+         if (cs->hw_ip < AMD_NUM_IP_TYPES) {
+            ibs_per_ip[cs->hw_ip]++;
+         }
       }
 
       for (unsigned i = 0; i < ib_per_submit && cs_idx < cs_count; ++i) {
@@ -1163,8 +1847,16 @@ radv_amdgpu_winsys_cs_submit_internal(st
          if (cs_ib_idx == 0) {
             /* Make sure the whole CS fits into the same submission. */
             unsigned cs_num_ib = radv_amdgpu_submitted_ibs_per_cs(cs);
-            if (i + cs_num_ib > ib_per_submit || ibs_per_ip[cs->hw_ip] + cs_num_ib > max_ib_per_ip[cs->hw_ip])
+
+            /* FIX: Validate IP type before array access. */
+            if (cs->hw_ip >= AMD_NUM_IP_TYPES) {
+               result = VK_ERROR_UNKNOWN;
+               goto fail;
+            }
+
+            if (i + cs_num_ib > ib_per_submit || ibs_per_ip[cs->hw_ip] + cs_num_ib > max_ib_per_ip[cs->hw_ip]) {
                break;
+            }
 
             if (cs->hw_ip != request.ip_type) {
                /* Found a "follower" CS in a gang submission.
@@ -1175,9 +1867,17 @@ radv_amdgpu_winsys_cs_submit_internal(st
                struct radv_amdgpu_cs *next_cs = radv_amdgpu_cs(cs_array[cs_idx + 1]);
                assert(next_cs->hw_ip == request.ip_type);
                unsigned next_cs_num_ib = radv_amdgpu_submitted_ibs_per_cs(next_cs);
+
+               /* FIX: Validate next CS IP type. */
+               if (next_cs->hw_ip >= AMD_NUM_IP_TYPES) {
+                  result = VK_ERROR_UNKNOWN;
+                  goto fail;
+               }
+
                if (i + cs_num_ib + next_cs_num_ib > ib_per_submit ||
-                   ibs_per_ip[next_cs->hw_ip] + next_cs_num_ib > max_ib_per_ip[next_cs->hw_ip])
+                   ibs_per_ip[next_cs->hw_ip] + next_cs_num_ib > max_ib_per_ip[next_cs->hw_ip]) {
                   break;
+               }
             }
          }
 
@@ -1198,12 +1898,17 @@ radv_amdgpu_winsys_cs_submit_internal(st
             }
          }
 
-         if (uses_shadow_regs && ib.ip_type == AMDGPU_HW_IP_GFX)
+         if (uses_shadow_regs && ib.ip_type == AMDGPU_HW_IP_GFX) {
             ib.flags |= AMDGPU_IB_FLAG_PREEMPT;
+         }
 
          assert(num_submitted_ibs < ib_array_size);
          ibs[num_submitted_ibs++] = ib;
-         ibs_per_ip[cs->hw_ip]++;
+
+         /* FIX: Validate IP type. */
+         if (cs->hw_ip < AMD_NUM_IP_TYPES) {
+            ibs_per_ip[cs->hw_ip]++;
+         }
       }
 
       assert(num_submitted_ibs > preamble_count);
@@ -1221,20 +1926,26 @@ radv_amdgpu_winsys_cs_submit_internal(st
          ib = radv_amdgpu_cs_ib_to_info(cs, cs->ib_buffers[0]);
 
          ibs[num_submitted_ibs++] = ib;
-         ibs_per_ip[cs->hw_ip]++;
+
+         /* FIX: Validate IP type. */
+         if (cs->hw_ip < AMD_NUM_IP_TYPES) {
+            ibs_per_ip[cs->hw_ip]++;
+         }
       }
 
       /* Submit the CS. */
       request.number_of_ibs = num_submitted_ibs;
       result = radv_amdgpu_cs_submit(ctx, &request, sem_info);
-      if (result != VK_SUCCESS)
+      if (result != VK_SUCCESS) {
          goto fail;
+      }
    }
 
    free(request.handles);
 
-   if (result != VK_SUCCESS)
+   if (result != VK_SUCCESS) {
       goto fail;
+   }
 
    radv_assign_last_submit(ctx, &request);
 
@@ -1562,7 +2273,7 @@ radv_amdgpu_winsys_cs_dump(struct ac_cmd
 
             ac_parse_ib(&ib_parser, name);
          } else {
-            ibs[i] = mapped;
+            ibs[i] = (uint32_t *)mapped;
             ib_dw_sizes[i] = ib->cdw;
          }
       }
@@ -1775,7 +2486,7 @@ static void *
 radv_amdgpu_cs_alloc_syncobj_chunk(struct radv_winsys_sem_counts *counts, uint32_t queue_syncobj,
                                    struct drm_amdgpu_cs_chunk *chunk, int chunk_id)
 {
-   unsigned count = counts->syncobj_count + (queue_syncobj ? 1 : 0);
+   unsigned count = counts->syncobj_count + (queue_syncobj ? 1u : 0u);
    struct drm_amdgpu_cs_chunk_sem *syncobj = malloc(sizeof(struct drm_amdgpu_cs_chunk_sem) * count);
    if (!syncobj)
       return NULL;
@@ -1789,7 +2500,7 @@ radv_amdgpu_cs_alloc_syncobj_chunk(struc
       syncobj[counts->syncobj_count].handle = queue_syncobj;
 
    chunk->chunk_id = chunk_id;
-   chunk->length_dw = sizeof(struct drm_amdgpu_cs_chunk_sem) / 4 * count;
+   chunk->length_dw = (uint32_t)(sizeof(struct drm_amdgpu_cs_chunk_sem) / 4 * count);
    chunk->chunk_data = (uint64_t)(uintptr_t)syncobj;
    return syncobj;
 }
@@ -1798,7 +2509,7 @@ static void *
 radv_amdgpu_cs_alloc_timeline_syncobj_chunk(struct radv_winsys_sem_counts *counts, uint32_t queue_syncobj,
                                             struct drm_amdgpu_cs_chunk *chunk, int chunk_id)
 {
-   uint32_t count = counts->syncobj_count + counts->timeline_syncobj_count + (queue_syncobj ? 1 : 0);
+   uint32_t count = counts->syncobj_count + counts->timeline_syncobj_count + (queue_syncobj ? 1u : 0u);
    struct drm_amdgpu_cs_chunk_syncobj *syncobj = malloc(sizeof(struct drm_amdgpu_cs_chunk_syncobj) * count);
    if (!syncobj)
       return NULL;
@@ -1824,7 +2535,7 @@ radv_amdgpu_cs_alloc_timeline_syncobj_ch
    }
 
    chunk->chunk_id = chunk_id;
-   chunk->length_dw = sizeof(struct drm_amdgpu_cs_chunk_syncobj) / 4 * count;
+   chunk->length_dw = (uint32_t)(sizeof(struct drm_amdgpu_cs_chunk_syncobj) / 4 * count);
    chunk->chunk_data = (uint64_t)(uintptr_t)syncobj;
    return syncobj;
 }
@@ -1844,11 +2555,8 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
    int r;
    int num_chunks;
    int size;
-   struct drm_amdgpu_cs_chunk *chunks;
-   struct drm_amdgpu_cs_chunk_data *chunk_data;
    struct drm_amdgpu_bo_list_in bo_list_in;
    void *wait_syncobj = NULL, *signal_syncobj = NULL;
-   int i;
    VkResult result = VK_SUCCESS;
    bool has_user_fence = radv_amdgpu_cs_has_user_fence(request);
    uint32_t queue_syncobj = radv_amdgpu_ctx_queue_syncobj(ctx, request->ip_type, request->ring);
@@ -1859,20 +2567,18 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
 
    size = request->number_of_ibs + 1 + (has_user_fence ? 1 : 0) + 1 /* bo list */ + 3;
 
-   chunks = malloc(sizeof(chunks[0]) * size);
-   if (!chunks)
-      return VK_ERROR_OUT_OF_HOST_MEMORY;
+   STACK_ARRAY(struct drm_amdgpu_cs_chunk, chunks, size);
 
    size = request->number_of_ibs + (has_user_fence ? 1 : 0);
+   STACK_ARRAY(struct drm_amdgpu_cs_chunk_data, chunk_data, size);
 
-   chunk_data = malloc(sizeof(chunk_data[0]) * size);
-   if (!chunk_data) {
+   if (!chunks || !chunk_data) {
       result = VK_ERROR_OUT_OF_HOST_MEMORY;
-      goto error_out;
+      goto out_finish;
    }
 
    num_chunks = request->number_of_ibs;
-   for (i = 0; i < request->number_of_ibs; i++) {
+   for (int i = 0; i < (int)request->number_of_ibs; i++) {
       struct radv_amdgpu_cs_ib_info *ib;
       chunks[i].chunk_id = AMDGPU_CHUNK_ID_IB;
       chunks[i].length_dw = sizeof(struct drm_amdgpu_cs_chunk_ib) / 4;
@@ -1884,7 +2590,7 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
 
       chunk_data[i].ib_data._pad = 0;
       chunk_data[i].ib_data.va_start = ib->ib_mc_address;
-      chunk_data[i].ib_data.ib_bytes = ib->size * 4;
+      chunk_data[i].ib_data.ib_bytes = ib->size * 4u;
       chunk_data[i].ib_data.ip_type = ib->ip_type;
       chunk_data[i].ib_data.ip_instance = request->ip_instance;
       chunk_data[i].ib_data.ring = request->ring;
@@ -1894,7 +2600,7 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
    assert(chunk_data[request->number_of_ibs - 1].ib_data.ip_type == request->ip_type);
 
    if (has_user_fence) {
-      i = num_chunks++;
+      int i = num_chunks++;
       chunks[i].chunk_id = AMDGPU_CHUNK_ID_FENCE;
       chunks[i].length_dw = sizeof(struct drm_amdgpu_cs_chunk_fence) / 4;
       chunks[i].chunk_data = (uint64_t)(uintptr_t)&chunk_data[i];
@@ -1905,7 +2611,7 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
        *   QWORD[2]: reset fence
        *   QWORD[3]: preempted then reset
        */
-      uint32_t offset = (request->ip_type * MAX_RINGS_PER_TYPE + request->ring) * 4;
+      uint32_t offset = (request->ip_type * MAX_RINGS_PER_TYPE + request->ring) * 4u;
       ac_drm_cs_chunk_fence_info_to_data(radv_amdgpu_winsys_bo(ctx->fence_bo)->bo_handle, offset, &chunk_data[i]);
    }
 
@@ -1922,7 +2628,7 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
       }
       if (!wait_syncobj) {
          result = VK_ERROR_OUT_OF_HOST_MEMORY;
-         goto error_out;
+         goto out_finish;
       }
       num_chunks++;
 
@@ -1940,13 +2646,13 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
       }
       if (!signal_syncobj) {
          result = VK_ERROR_OUT_OF_HOST_MEMORY;
-         goto error_out;
+         goto out_finish;
       }
       num_chunks++;
    }
 
-   bo_list_in.operation = ~0;
-   bo_list_in.list_handle = ~0;
+   bo_list_in.operation = ~0u;
+   bo_list_in.list_handle = ~0u;
    bo_list_in.bo_number = request->num_handles;
    bo_list_in.bo_info_size = sizeof(struct drm_amdgpu_bo_list_entry);
    bo_list_in.bo_info_ptr = (uint64_t)(uintptr_t)request->handles;
@@ -1996,11 +2702,11 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
       }
    }
 
-error_out:
-   free(chunks);
-   free(chunk_data);
+out_finish:
    free(wait_syncobj);
    free(signal_syncobj);
+   STACK_ARRAY_FINISH(chunks);
+   STACK_ARRAY_FINISH(chunk_data);
    return result;
 }
 
