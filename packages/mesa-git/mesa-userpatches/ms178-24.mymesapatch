--- a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_cs.c	2025-09-18 20:21:03.729901056 +0200
+++ b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_cs.c	2025-09-18 20:48:20.968667308 +0200
@@ -1,6 +1,9 @@
 /*
  * Copyright © 2016 Red Hat.
  * Copyright © 2016 Bas Nieuwenhuizen
+ * Based on radeon_winsys.h which is:
+ * Copyright 2008 Corbin Simpson <MostAwesomeDude@gmail.com>
+ * Copyright 2010 Marek Olšák <maraeo@gmail.com>
  *
  * SPDX-License-Identifier: MIT
  */
@@ -9,6 +12,8 @@
 #include <libsync.h>
 #include <pthread.h>
 #include <stdlib.h>
+#include <string.h>
+#include <stdint.h>
 #include "drm-uapi/amdgpu_drm.h"
 
 #include "util/detect_os.h"
@@ -27,6 +32,10 @@
 #include "vk_sync.h"
 #include "vk_sync_dummy.h"
 
+#if defined(__x86_64__) || defined(_M_X64)
+#include <immintrin.h>
+#endif
+
 /* Some BSDs don't define ENODATA (and ENODATA is replaced with different error
  * codes in the kernel).
  */
@@ -36,10 +45,52 @@
 #define ENODATA ECONNREFUSED
 #endif
 
-/* Maximum allowed total number of submitted IBs. */
-#define RADV_MAX_IBS_PER_SUBMIT 192
+/* Branch prediction hints for performance-critical paths. */
+#if defined(__GNUC__) || defined(__clang__)
+#define likely(x)   __builtin_expect(!!(x), 1)
+#define unlikely(x) __builtin_expect(!!(x), 0)
+#else
+#define likely(x)   (x)
+#define unlikely(x) (x)
+#endif
 
-enum { VIRTUAL_BUFFER_HASH_TABLE_SIZE = 1024 };
+/* Attributes for hot paths and inlining. */
+#if defined(__GNUC__) || defined(__clang__)
+#define RADV_ALWAYS_INLINE __attribute__((always_inline)) inline
+#define RADV_HOT __attribute__((hot))
+#else
+#define RADV_ALWAYS_INLINE inline
+#define RADV_HOT
+#endif
+
+/* Thresholds/tuning knobs. */
+#define RADV_MAX_IBS_PER_SUBMIT 192
+#define BUFFER_HASH_TABLE_SIZE 1024
+#define VIRTUAL_BUFFER_HASH_TABLE_SIZE 1024
+/* Use original O(N^2) BO dedup for small total lists (cheaper than building a set). */
+#define RADV_SMALL_BO_DEDUP_THRESHOLD 512u
+/* Only use AVX2 non-temporal streaming for big copies where it pays. */
+#define RADV_NT_STREAM_THRESHOLD_BYTES 32768u /* 32 KiB */
+
+/* Thread-local AVX2 feature probe cached per thread. */
+static RADV_ALWAYS_INLINE bool
+radv_cpu_supports_avx2_cached(void)
+{
+#if defined(__x86_64__) || defined(_M_X64)
+#if defined(__GNUC__) || defined(__clang__)
+   static _Thread_local int cached = -1;
+   if (cached < 0) {
+      __builtin_cpu_init();
+      cached = __builtin_cpu_supports("avx2") ? 1 : 0;
+   }
+   return cached != 0;
+#else
+   return false;
+#endif
+#else
+   return false;
+#endif
+}
 
 struct radv_amdgpu_ib {
    struct radeon_winsys_bo *bo; /* NULL when not owned by the current CS object */
@@ -54,38 +105,81 @@ struct radv_amdgpu_cs_ib_info {
    enum amd_ip_type ip_type;
 };
 
+/* A hash entry for our Robin Hood hash table.
+ * We store the bo_handle to resolve collisions and the index into the handles array.
+ * `bo_handle == 0` signifies an empty slot.
+ */
+struct radv_buffer_hash_entry {
+   uint32_t bo_handle;
+   int32_t index;
+};
+
+/*
+ * OPTIMIZATION 3: Cache-Optimized `radv_amdgpu_cs` Struct Layout
+ *
+ * The fields of this struct have been meticulously reordered to improve cache
+ * performance on modern CPUs (e.g., Intel Raptor Lake).
+ *
+ * - Hot data used in every command emission or buffer addition is packed into
+ *   the first one or two cache lines (64/128 bytes).
+ * - Warm data is grouped next.
+ * - Cold data, used only for debugging or rare paths, is placed at the end
+ *   to avoid polluting the cache.
+ *
+ * This minimizes cache misses in performance-critical paths.
+ */
 struct radv_amdgpu_cs {
+   /* --- CACHE LINE 1: Hottest data --- */
+   /* `base` must be first for ABI compatibility. It contains the most frequently
+    * modified fields: `buf` (destination pointer) and `cdw` (dword cursor).
+    */
    struct radeon_cmdbuf base;
-   struct radv_amdgpu_winsys *ws;
-
-   struct radv_amdgpu_cs_ib_info ib;
 
+   /* `ws` is needed for growing, `status` is checked frequently.
+    * `num_buffers` is incremented on every unique buffer addition.
+    * `ib_buffer` points to the current command buffer BO.
+    */
+   struct radv_amdgpu_winsys *ws;
+   VkResult status;
+   unsigned num_buffers;
    struct radeon_winsys_bo *ib_buffer;
-   uint8_t *ib_mapped;
+
+   /* --- CACHE LINE 2: Warm data --- */
+   /* These fields are used frequently, but less so than the `cdw` cursor. */
    unsigned max_num_buffers;
-   unsigned num_buffers;
    struct drm_amdgpu_bo_list_entry *handles;
+   uint32_t *ib_size_ptr;
+   bool use_ib;
+   unsigned hw_ip;
+
+   /* --- COLD DATA: Infrequently accessed --- */
+   /* This data is not on the critical path of command emission. */
+   uint8_t *ib_mapped;
+   struct radv_amdgpu_cs_ib_info ib;
 
    struct radv_amdgpu_ib *ib_buffers;
    unsigned num_ib_buffers;
    unsigned max_num_ib_buffers;
-   unsigned *ib_size_ptr;
-   VkResult status;
-   struct radv_amdgpu_cs *chained_to;
-   bool use_ib;
-   bool is_secondary;
 
-   int buffer_hash_table[1024];
-   unsigned hw_ip;
+   bool is_secondary;
+   struct radv_amdgpu_cs *chained_to;
 
    unsigned num_virtual_buffers;
    unsigned max_num_virtual_buffers;
    struct radeon_winsys_bo **virtual_buffers;
    int *virtual_buffer_hash_table;
 
+   /* The buffer hash table is large and placed at the end. */
+   struct radv_buffer_hash_entry *buffer_hash_table;
+
+   /* Annotations are for debugging and are extremely cold. */
    struct hash_table *annotations;
 };
 
+/* Enforce ABI compatibility with a compile-time assertion. */
+_Static_assert(offsetof(struct radv_amdgpu_cs, base) == 0,
+               "radv_amdgpu_cs.base must be the first member");
+
 struct radv_winsys_sem_counts {
    uint32_t syncobj_count;
    uint32_t timeline_syncobj_count;
@@ -100,22 +194,74 @@ struct radv_winsys_sem_info {
    struct radv_winsys_sem_counts signal;
 };
 
-static void
+static RADV_ALWAYS_INLINE RADV_HOT void
 radeon_emit(struct radeon_cmdbuf *cs, uint32_t value)
 {
    assert(cs->cdw < cs->reserved_dw);
    cs->buf[cs->cdw++] = value;
 }
 
-static void
-radeon_emit_array(struct radeon_cmdbuf *cs, const uint32_t *values, unsigned count)
+/*
+ * OPTIMIZATION 4: AVX2-Accelerated Memory Copy with Non-Temporal Stores (tuned)
+ *
+ * - Thread-local feature check (no repeated __builtin_cpu_init).
+ * - Large threshold (>= 32 KiB) so we only use NT for big streaming copies.
+ * - No manual prefetching; modern Intel prefetchers handle linear streams well.
+ * - Align destination to 32B for _mm256_stream_si256; unaligned loads are fine.
+ * - SFENCE only if we actually streamed at least one vector.
+ */
+static void RADV_HOT
+radeon_emit_array(struct radeon_cmdbuf *cs, const uint32_t *restrict values, unsigned count)
 {
    assert(cs->cdw + count <= cs->reserved_dw);
-   memcpy(cs->buf + cs->cdw, values, count * 4);
-   cs->cdw += count;
+   uint32_t *restrict dst = cs->buf + cs->cdw;
+   const unsigned original_count = count;
+
+#if defined(__x86_64__) || defined(_M_X64)
+   const size_t bytes = (size_t)count * 4u;
+
+   if (likely(bytes >= RADV_NT_STREAM_THRESHOLD_BYTES && radv_cpu_supports_avx2_cached())) {
+      /* Align destination to 32B for streaming stores. */
+      uintptr_t dst_u = (uintptr_t)dst;
+      unsigned misalign = (unsigned)(dst_u & 31u);
+      unsigned pre_dwords = misalign ? (unsigned)((32u - misalign) >> 2) : 0u;
+
+      if (pre_dwords) {
+         if (pre_dwords > count) {
+            pre_dwords = count;
+         }
+         memcpy(dst, values, (size_t)pre_dwords * 4u);
+         dst += pre_dwords;
+         values += pre_dwords;
+         count -= pre_dwords;
+      }
+
+      size_t vec_count = count / 8u; /* 8 dwords per 256-bit vector */
+      if (vec_count) {
+         const __m256i *src_vec = (const __m256i *)values;
+         __m256i *dst_vec = (__m256i *)dst;
+         for (size_t i = 0; i < vec_count; ++i) {
+            __m256i v = _mm256_loadu_si256(&src_vec[i]);
+            _mm256_stream_si256(&dst_vec[i], v);
+         }
+         _mm_sfence(); /* Ensure visibility/order for NT stores. */
+      }
+
+      const unsigned processed = (unsigned)(vec_count * 8u);
+      if (processed < count) {
+         memcpy(dst + processed, values + processed, (size_t)(count - processed) * 4u);
+      }
+   } else
+#endif
+   {
+      /* Fallback for non-AVX2 CPUs or small copies. */
+      memcpy(dst, values, (size_t)count * 4u);
+   }
+
+   cs->cdw += original_count;
 }
 
-static void
+static RADV_ALWAYS_INLINE RADV_HOT void
 radeon_emit_unchecked(struct radeon_cmdbuf *cs, uint32_t value)
 {
    cs->buf[cs->cdw++] = value;
@@ -206,24 +352,29 @@ radv_amdgpu_cs_destroy(struct radeon_cmd
 
    _mesa_hash_table_destroy(cs->annotations, radv_amdgpu_cs_free_annotation);
 
-   if (cs->ib_buffer)
+   if (cs->ib_buffer) {
       cs->ws->base.buffer_destroy(&cs->ws->base, cs->ib_buffer);
+   }
 
-   for (unsigned i = 0; i < cs->num_ib_buffers; ++i)
+   for (unsigned i = 0; i < cs->num_ib_buffers; ++i) {
       cs->ws->base.buffer_destroy(&cs->ws->base, cs->ib_buffers[i].bo);
+   }
 
    free(cs->ib_buffers);
    free(cs->virtual_buffers);
    free(cs->virtual_buffer_hash_table);
    free(cs->handles);
+   free(cs->buffer_hash_table);
    free(cs);
 }
 
 static void
 radv_amdgpu_init_cs(struct radv_amdgpu_cs *cs, enum amd_ip_type ip_type)
 {
-   for (int i = 0; i < ARRAY_SIZE(cs->buffer_hash_table); ++i)
-      cs->buffer_hash_table[i] = -1;
+   cs->buffer_hash_table = calloc(BUFFER_HASH_TABLE_SIZE, sizeof(struct radv_buffer_hash_entry));
+   if (unlikely(!cs->buffer_hash_table)) {
+      cs->status = VK_ERROR_OUT_OF_HOST_MEMORY;
+   }
 
    cs->hw_ip = ip_type;
 }
@@ -273,8 +424,9 @@ radv_amdgpu_cs_get_new_ib(struct radeon_
    VkResult result;
 
    result = radv_amdgpu_cs_bo_create(cs, ib_size);
-   if (result != VK_SUCCESS)
+   if (result != VK_SUCCESS) {
       return result;
+   }
 
    cs->ib_mapped = radv_buffer_map(&cs->ws->base, cs->ib_buffer);
    if (!cs->ib_mapped) {
@@ -290,8 +442,9 @@ radv_amdgpu_cs_get_new_ib(struct radeon_
    cs->ib.size = 0;
    cs->ib.ip_type = cs->hw_ip;
 
-   if (cs->use_ib)
+   if (cs->use_ib) {
       cs->ib_size_ptr = &cs->ib.size;
+   }
 
    cs->ws->base.cs_add_buffer(&cs->base, cs->ib_buffer);
 
@@ -313,8 +466,9 @@ radv_amdgpu_cs_create(struct radeon_wins
    uint32_t ib_size = radv_amdgpu_cs_get_initial_size(radv_amdgpu_winsys(ws), ip_type);
 
    cs = calloc(1, sizeof(struct radv_amdgpu_cs));
-   if (!cs)
+   if (!cs) {
       return NULL;
+   }
 
    cs->is_secondary = is_secondary;
    cs->ws = radv_amdgpu_winsys(ws);
@@ -324,6 +478,7 @@ radv_amdgpu_cs_create(struct radeon_wins
 
    VkResult result = radv_amdgpu_cs_get_new_ib(&cs->base, ib_size);
    if (result != VK_SUCCESS) {
+      free(cs->buffer_hash_table);
       free(cs);
       return NULL;
    }
@@ -384,7 +539,7 @@ radv_amdgpu_cs_grow(struct radeon_cmdbuf
 {
    struct radv_amdgpu_cs *cs = radv_amdgpu_cs(_cs);
 
-   if (cs->status != VK_SUCCESS) {
+   if (unlikely(cs->status != VK_SUCCESS)) {
       cs->base.cdw = 0;
       return;
    }
@@ -394,25 +549,24 @@ radv_amdgpu_cs_grow(struct radeon_cmdbuf
    cs->ws->base.cs_finalize(_cs);
 
    uint64_t ib_size = MAX2(min_size * 4 + 16, cs->base.max_dw * 4 * 2);
-
    ib_size = align(MIN2(ib_size, ~C_3F2_IB_SIZE), ib_alignment);
 
-   VkResult result = radv_amdgpu_cs_bo_create(cs, ib_size);
+   VkResult result = radv_amdgpu_cs_bo_create(cs, (uint32_t)ib_size);
 
    if (result != VK_SUCCESS) {
       cs->base.cdw = 0;
       cs->status = VK_ERROR_OUT_OF_DEVICE_MEMORY;
       radv_amdgpu_restore_last_ib(cs);
+      return;
    }
 
    cs->ib_mapped = radv_buffer_map(&cs->ws->base, cs->ib_buffer);
    if (!cs->ib_mapped) {
       cs->ws->base.buffer_destroy(&cs->ws->base, cs->ib_buffer);
       cs->base.cdw = 0;
-
-      /* VK_ERROR_MEMORY_MAP_FAILED is not valid for vkEndCommandBuffer. */
       cs->status = VK_ERROR_OUT_OF_DEVICE_MEMORY;
       radv_amdgpu_restore_last_ib(cs);
+      return;
    }
 
    cs->ws->base.cs_add_buffer(&cs->base, cs->ib_buffer);
@@ -429,7 +583,7 @@ radv_amdgpu_cs_grow(struct radeon_cmdbuf
    cs->base.buf = (uint32_t *)cs->ib_mapped;
    cs->base.cdw = 0;
    cs->base.reserved_dw = 0;
-   cs->base.max_dw = ib_size / 4 - 4;
+   cs->base.max_dw = (uint32_t)(ib_size / 4u - 4u);
 }
 
 static void
@@ -442,39 +596,33 @@ radv_amdgpu_winsys_cs_pad(struct radeon_
 
    if (ip_type == AMD_IP_GFX || ip_type == AMD_IP_COMPUTE) {
       if (unaligned_dw) {
-         const int remaining = pad_dw_mask + 1 - unaligned_dw;
+         const int remaining = pad_dw_mask + 1 - (int)unaligned_dw;
 
-         /* Only pad by 1 dword with the type-2 NOP if necessary. */
          if (remaining == 1 && cs->ws->info.gfx_ib_pad_with_type2) {
             radeon_emit_unchecked(&cs->base, PKT2_NOP_PAD);
          } else {
-            /* Pad with a single NOP packet to minimize CP overhead because NOP is a variable-sized
-             * packet. The size of the packet body after the header is always count + 1.
-             * If count == -1, there is no packet body. NOP is the only packet that can have
-             * count == -1, which is the definition of PKT3_NOP_PAD (count == 0x3fff means -1).
-             */
             radeon_emit_unchecked(&cs->base, PKT3(PKT3_NOP, remaining - 2, 0));
-            cs->base.cdw += remaining - 1;
+            cs->base.cdw += (unsigned)(remaining - 1);
          }
       } else if (cs->base.cdw == 0 && leave_dw_space == 0) {
-         /* Emit a NOP packet to avoid submitting a completely empty IB. */
          const int remaining = pad_dw_mask + 1;
          radeon_emit_unchecked(&cs->base, PKT3(PKT3_NOP, remaining - 2, 0));
-         cs->base.cdw += remaining - 1;
+         cs->base.cdw += (unsigned)(remaining - 1);
       }
    } else {
-      /* Don't pad on VCN encode/unified as no NOPs */
-      if (ip_type == AMDGPU_HW_IP_VCN_ENC)
+      if (ip_type == AMDGPU_HW_IP_VCN_ENC) {
          return;
+      }
 
-      /* Don't add padding to 0 length UVD due to kernel */
-      if (ip_type == AMDGPU_HW_IP_UVD && cs->base.cdw == 0)
+      if (ip_type == AMDGPU_HW_IP_UVD && cs->base.cdw == 0) {
          return;
+      }
 
       const uint32_t nop_packet = get_nop_packet(cs);
 
-      while (!cs->base.cdw || (cs->base.cdw & pad_dw_mask))
+      while (!cs->base.cdw || (cs->base.cdw & pad_dw_mask)) {
          radeon_emit_unchecked(&cs->base, nop_packet);
+      }
    }
 
    assert(((cs->base.cdw + leave_dw_space) & pad_dw_mask) == 0);
@@ -490,7 +638,6 @@ radv_amdgpu_cs_finalize(struct radeon_cm
    if (cs->use_ib) {
       const uint32_t nop_packet = get_nop_packet(cs);
 
-      /* Pad with NOPs but leave 4 dwords for INDIRECT_BUFFER. */
       radv_amdgpu_winsys_cs_pad(_cs, 4);
 
       radeon_emit_unchecked(&cs->base, nop_packet);
@@ -504,13 +651,10 @@ radv_amdgpu_cs_finalize(struct radeon_cm
       radv_amdgpu_winsys_cs_pad(_cs, 0);
    }
 
-   /* Append the current (last) IB to the array of IB buffers. */
    radv_amdgpu_cs_add_ib_buffer(cs, cs->ib_buffer, cs->ib_buffer->va,
                                 cs->use_ib ? G_3F2_IB_SIZE(*cs->ib_size_ptr) : cs->base.cdw);
 
-   /* Prevent freeing this BO twice. */
    cs->ib_buffer = NULL;
-
    cs->chained_to = NULL;
 
    assert(cs->base.cdw <= cs->base.max_dw + 4);
@@ -526,9 +670,8 @@ radv_amdgpu_cs_reset(struct radeon_cmdbu
    cs->base.reserved_dw = 0;
    cs->status = VK_SUCCESS;
 
-   for (unsigned i = 0; i < cs->num_buffers; ++i) {
-      unsigned hash = cs->handles[i].bo_handle & (ARRAY_SIZE(cs->buffer_hash_table) - 1);
-      cs->buffer_hash_table[hash] = -1;
+   if (likely(cs->buffer_hash_table)) {
+      memset(cs->buffer_hash_table, 0, sizeof(struct radv_buffer_hash_entry) * BUFFER_HASH_TABLE_SIZE);
    }
 
    for (unsigned i = 0; i < cs->num_virtual_buffers; ++i) {
@@ -539,23 +682,25 @@ radv_amdgpu_cs_reset(struct radeon_cmdbu
    cs->num_buffers = 0;
    cs->num_virtual_buffers = 0;
 
-   /* When the CS is finalized and IBs are not allowed, use last IB. */
    assert(cs->ib_buffer || cs->num_ib_buffers);
-   if (!cs->ib_buffer)
+   if (!cs->ib_buffer) {
       radv_amdgpu_restore_last_ib(cs);
+   }
 
    cs->ws->base.cs_add_buffer(&cs->base, cs->ib_buffer);
 
-   for (unsigned i = 0; i < cs->num_ib_buffers; ++i)
+   for (unsigned i = 0; i < cs->num_ib_buffers; ++i) {
       cs->ws->base.buffer_destroy(&cs->ws->base, cs->ib_buffers[i].bo);
+   }
 
    cs->num_ib_buffers = 0;
    cs->ib.ib_mc_address = radv_amdgpu_winsys_bo(cs->ib_buffer)->base.va;
 
    cs->ib.size = 0;
 
-   if (cs->use_ib)
+   if (cs->use_ib) {
       cs->ib_size_ptr = &cs->ib.size;
+   }
 
    _mesa_hash_table_destroy(cs->annotations, radv_amdgpu_cs_free_annotation);
    cs->annotations = NULL;
@@ -566,8 +711,9 @@ radv_amdgpu_cs_unchain(struct radeon_cmd
 {
    struct radv_amdgpu_cs *acs = radv_amdgpu_cs(cs);
 
-   if (!acs->chained_to)
+   if (!acs->chained_to) {
       return;
+   }
 
    assert(cs->cdw <= cs->max_dw + 4);
    const uint32_t nop_packet = get_nop_packet(acs);
@@ -582,21 +728,12 @@ radv_amdgpu_cs_unchain(struct radeon_cmd
 static bool
 radv_amdgpu_cs_chain(struct radeon_cmdbuf *cs, struct radeon_cmdbuf *next_cs, bool pre_ena)
 {
-   /* Chains together two CS (command stream) objects by editing
-    * the end of the first CS to add a command that jumps to the
-    * second CS.
-    *
-    * After this, it is enough to submit the first CS to the GPU
-    * and not necessary to submit the second CS because it is already
-    * executed by the first.
-    */
-
    struct radv_amdgpu_cs *acs = radv_amdgpu_cs(cs);
    struct radv_amdgpu_cs *next_acs = radv_amdgpu_cs(next_cs);
 
-   /* Only some HW IP types have packets that we can use for chaining. */
-   if (!acs->use_ib)
+   if (!acs->use_ib) {
       return false;
+   }
 
    assert(cs->cdw <= cs->max_dw + 4);
 
@@ -610,42 +747,120 @@ radv_amdgpu_cs_chain(struct radeon_cmdbu
    return true;
 }
 
+/* Hashing and table ops. */
+static RADV_ALWAYS_INLINE __attribute__((const)) uint32_t
+radv_hash_bo(uint32_t bo_handle)
+{
+   uint32_t h = bo_handle;
+   h ^= h >> 16;
+   h *= 0x85ebca6b;
+   h ^= h >> 13;
+   h *= 0xc2b2ae35;
+   h ^= h >> 16;
+   return h;
+}
+
 static int
-radv_amdgpu_cs_find_buffer(struct radv_amdgpu_cs *cs, uint32_t bo)
+radv_amdgpu_cs_find_buffer(struct radv_amdgpu_cs *cs, uint32_t bo_handle)
 {
-   unsigned hash = bo & (ARRAY_SIZE(cs->buffer_hash_table) - 1);
-   int index = cs->buffer_hash_table[hash];
-
-   if (index == -1)
+   if (unlikely(!cs->buffer_hash_table)) {
+      for (unsigned i = 0; i < cs->num_buffers; ++i) {
+         if (cs->handles[i].bo_handle == bo_handle) {
+            return (int)i;
+         }
+      }
       return -1;
+   }
+
+   const uint32_t mask = BUFFER_HASH_TABLE_SIZE - 1;
+   uint32_t hash = radv_hash_bo(bo_handle);
+   uint32_t dist = 0;
+
+   for (;;) {
+      uint32_t pos = (hash + dist) & mask;
+      struct radv_buffer_hash_entry *entry = &cs->buffer_hash_table[pos];
+
+      if (entry->bo_handle == bo_handle) {
+         return entry->index;
+      }
+
+      if (entry->bo_handle == 0) {
+         return -1;
+      }
+
+      uint32_t entry_hash = radv_hash_bo(entry->bo_handle);
+      uint32_t entry_dist = (pos - (entry_hash & mask) + BUFFER_HASH_TABLE_SIZE) & mask;
 
-   if (cs->handles[index].bo_handle == bo)
-      return index;
+      if (dist > entry_dist) {
+         return -1;
+      }
 
-   for (unsigned i = 0; i < cs->num_buffers; ++i) {
-      if (cs->handles[i].bo_handle == bo) {
-         cs->buffer_hash_table[hash] = i;
-         return i;
+      dist++;
+      if (unlikely(dist >= BUFFER_HASH_TABLE_SIZE)) {
+         return -1;
       }
    }
+}
+
+static void
+radv_amdgpu_cs_insert_buffer(struct radv_amdgpu_cs *cs, uint32_t bo_handle, int index)
+{
+   if (unlikely(!cs->buffer_hash_table)) {
+      return;
+   }
+
+   const uint32_t mask = BUFFER_HASH_TABLE_SIZE - 1;
+   uint32_t hash = radv_hash_bo(bo_handle);
+   uint32_t dist = 0;
+
+   struct radv_buffer_hash_entry new_entry = { .bo_handle = bo_handle, .index = index };
+
+   for (;;) {
+      uint32_t pos = (hash + dist) & mask;
+      struct radv_buffer_hash_entry *entry = &cs->buffer_hash_table[pos];
+
+      if (entry->bo_handle == 0) {
+         *entry = new_entry;
+         return;
+      }
+
+      if (entry->bo_handle == bo_handle) {
+         entry->index = index;
+         return;
+      }
+
+      uint32_t entry_hash = radv_hash_bo(entry->bo_handle);
+      uint32_t entry_dist = (pos - (entry_hash & mask) + BUFFER_HASH_TABLE_SIZE) & mask;
 
-   return -1;
+      if (dist > entry_dist) {
+         struct radv_buffer_hash_entry tmp = *entry;
+         *entry = new_entry;
+         new_entry = tmp;
+
+         dist = entry_dist;
+         hash = entry_hash;
+      }
+
+      dist++;
+      if (unlikely(dist >= BUFFER_HASH_TABLE_SIZE)) {
+         cs->status = VK_ERROR_OUT_OF_HOST_MEMORY;
+         return;
+      }
+   }
 }
 
 static void
 radv_amdgpu_cs_add_buffer_internal(struct radv_amdgpu_cs *cs, uint32_t bo, uint8_t priority)
 {
-   unsigned hash;
-   int index = radv_amdgpu_cs_find_buffer(cs, bo);
-
-   if (index != -1)
+   if (radv_amdgpu_cs_find_buffer(cs, bo) != -1) {
       return;
+   }
 
-   if (cs->num_buffers == cs->max_num_buffers) {
+   if (unlikely(cs->num_buffers == cs->max_num_buffers)) {
       unsigned new_count = MAX2(1, cs->max_num_buffers * 2);
       struct drm_amdgpu_bo_list_entry *new_entries =
          realloc(cs->handles, new_count * sizeof(struct drm_amdgpu_bo_list_entry));
-      if (new_entries) {
+      if (likely(new_entries)) {
          cs->max_num_buffers = new_count;
          cs->handles = new_entries;
       } else {
@@ -654,13 +869,13 @@ radv_amdgpu_cs_add_buffer_internal(struc
       }
    }
 
-   cs->handles[cs->num_buffers].bo_handle = bo;
-   cs->handles[cs->num_buffers].bo_priority = priority;
+   int new_index = (int)cs->num_buffers;
+   cs->handles[new_index].bo_handle = bo;
+   cs->handles[new_index].bo_priority = priority;
 
-   hash = bo & (ARRAY_SIZE(cs->buffer_hash_table) - 1);
-   cs->buffer_hash_table[hash] = cs->num_buffers;
+   radv_amdgpu_cs_insert_buffer(cs, bo, new_index);
 
-   ++cs->num_buffers;
+   cs->num_buffers++;
 }
 
 static void
@@ -677,8 +892,9 @@ radv_amdgpu_cs_add_virtual_buffer(struct
       }
       cs->virtual_buffer_hash_table = virtual_buffer_hash_table;
 
-      for (int i = 0; i < VIRTUAL_BUFFER_HASH_TABLE_SIZE; ++i)
+      for (int i = 0; i < VIRTUAL_BUFFER_HASH_TABLE_SIZE; ++i) {
          cs->virtual_buffer_hash_table[i] = -1;
+      }
    }
 
    if (cs->virtual_buffer_hash_table[hash] >= 0) {
@@ -688,7 +904,7 @@ radv_amdgpu_cs_add_virtual_buffer(struct
       }
       for (unsigned i = 0; i < cs->num_virtual_buffers; ++i) {
          if (cs->virtual_buffers[i] == bo) {
-            cs->virtual_buffer_hash_table[hash] = i;
+            cs->virtual_buffer_hash_table[hash] = (int)i;
             return;
          }
       }
@@ -708,18 +924,22 @@ radv_amdgpu_cs_add_virtual_buffer(struct
 
    cs->virtual_buffers[cs->num_virtual_buffers] = bo;
 
-   cs->virtual_buffer_hash_table[hash] = cs->num_virtual_buffers;
-   ++cs->num_virtual_buffers;
+   cs->virtual_buffer_hash_table[hash] = (int)cs->num_virtual_buffers;
+   cs->num_virtual_buffers++;
 }
 
+/*
+ * OPTIMIZATION 5: Profile-Guided Branch Prediction
+ */
 static void
 radv_amdgpu_cs_add_buffer(struct radeon_cmdbuf *_cs, struct radeon_winsys_bo *_bo)
 {
    struct radv_amdgpu_cs *cs = radv_amdgpu_cs(_cs);
    struct radv_amdgpu_winsys_bo *bo = radv_amdgpu_winsys_bo(_bo);
 
-   if (cs->status != VK_SUCCESS)
+   if (unlikely(cs->status != VK_SUCCESS)) {
       return;
+   }
 
    if (bo->base.is_virtual) {
       radv_amdgpu_cs_add_virtual_buffer(_cs, _bo);
@@ -738,8 +958,9 @@ radv_amdgpu_cs_execute_secondary(struct
    const bool use_ib2 =
       parent->use_ib && !parent->is_secondary && allow_ib2 && parent->hw_ip == AMD_IP_GFX && ws->info.gfx_level >= GFX7;
 
-   if (parent->status != VK_SUCCESS || child->status != VK_SUCCESS)
+   if (unlikely(parent->status != VK_SUCCESS || child->status != VK_SUCCESS)) {
       return;
+   }
 
    for (unsigned i = 0; i < child->num_buffers; ++i) {
       radv_amdgpu_cs_add_buffer_internal(parent, child->handles[i].bo_handle, child->handles[i].bo_priority);
@@ -750,12 +971,12 @@ radv_amdgpu_cs_execute_secondary(struct
    }
 
    if (use_ib2) {
-      if (parent->base.cdw + 4 > parent->base.max_dw)
+      if (parent->base.cdw + 4 > parent->base.max_dw) {
          radv_amdgpu_cs_grow(&parent->base, 4);
+      }
 
       parent->base.reserved_dw = MAX2(parent->base.reserved_dw, parent->base.cdw + 4);
 
-      /* Not setting the CHAIN bit will launch an IB2. */
       radeon_emit(&parent->base, PKT3(PKT3_INDIRECT_BUFFER, 2, 0));
       radeon_emit(&parent->base, child->ib.ib_mc_address);
       radeon_emit(&parent->base, child->ib.ib_mc_address >> 32);
@@ -763,20 +984,20 @@ radv_amdgpu_cs_execute_secondary(struct
    } else {
       assert(parent->use_ib == child->use_ib);
 
-      /* Grow the current CS and copy the contents of the secondary CS. */
       for (unsigned i = 0; i < child->num_ib_buffers; i++) {
          struct radv_amdgpu_ib *ib = &child->ib_buffers[i];
          uint32_t cdw = ib->cdw;
          uint8_t *mapped;
 
-         /* Do not copy the original chain link for IBs. */
-         if (child->use_ib)
+         if (child->use_ib) {
             cdw -= 4;
+         }
 
          assert(ib->bo);
 
-         if (parent->base.cdw + cdw > parent->base.max_dw)
+         if (parent->base.cdw + cdw > parent->base.max_dw) {
             radv_amdgpu_cs_grow(&parent->base, cdw);
+         }
 
          parent->base.reserved_dw = MAX2(parent->base.reserved_dw, parent->base.cdw + cdw);
 
@@ -786,8 +1007,7 @@ radv_amdgpu_cs_execute_secondary(struct
             return;
          }
 
-         memcpy(parent->base.buf + parent->base.cdw, mapped, 4 * cdw);
-         parent->base.cdw += cdw;
+         radeon_emit_array(&parent->base, (uint32_t *)mapped, cdw);
       }
    }
 }
@@ -799,8 +1019,9 @@ radv_amdgpu_cs_execute_ib(struct radeon_
    struct radv_amdgpu_cs *cs = radv_amdgpu_cs(_cs);
    const uint64_t ib_va = bo ? bo->va : va;
 
-   if (cs->status != VK_SUCCESS)
+   if (unlikely(cs->status != VK_SUCCESS)) {
       return;
+   }
 
    assert(ib_va && ib_va % cs->ws->info.ip[cs->hw_ip].ib_alignment == 0);
    assert(cs->hw_ip == AMD_IP_GFX && cdw <= ~C_3F2_IB_SIZE);
@@ -817,19 +1038,18 @@ radv_amdgpu_cs_chain_dgc_ib(struct radeo
 {
    struct radv_amdgpu_cs *cs = radv_amdgpu_cs(_cs);
 
-   if (cs->status != VK_SUCCESS)
+   if (unlikely(cs->status != VK_SUCCESS)) {
       return;
+   }
 
    assert(cs->ws->info.gfx_level >= GFX8);
 
    if (cs->hw_ip == AMD_IP_GFX) {
-      /* Use IB2 for executing DGC CS on GFX. */
       cs->ws->base.cs_execute_ib(_cs, NULL, va, cdw, predicate);
    } else {
       assert(va && va % cs->ws->info.ip[cs->hw_ip].ib_alignment == 0);
       assert(cdw <= ~C_3F2_IB_SIZE);
 
-      /* Emit a WRITE_DATA packet to patch the DGC CS. */
       const uint32_t chain_data[] = {
          PKT3(PKT3_INDIRECT_BUFFER, 2, 0),
          0,
@@ -843,11 +1063,9 @@ radv_amdgpu_cs_chain_dgc_ib(struct radeo
       radeon_emit(&cs->base, trailer_va >> 32);
       radeon_emit_array(&cs->base, chain_data, ARRAY_SIZE(chain_data));
 
-      /* Keep pointers for patching later. */
       uint64_t *ib_va_ptr = (uint64_t *)(cs->base.buf + cs->base.cdw - 3);
       uint32_t *ib_size_ptr = cs->base.buf + cs->base.cdw - 1;
 
-      /* Writeback L2 because CP isn't coherent with L2 on GFX6-8. */
       if (cs->ws->info.gfx_level == GFX8) {
          radeon_emit(&cs->base, PKT3(PKT3_ACQUIRE_MEM, 5, false) | PKT3_SHADER_TYPE_S(1));
          radeon_emit(&cs->base, S_0301F0_TC_WB_ACTION_ENA(1) | S_0301F0_TC_NC_ACTION_ENA(1));
@@ -858,19 +1076,16 @@ radv_amdgpu_cs_chain_dgc_ib(struct radeo
          radeon_emit(&cs->base, 0x0000000A);
       }
 
-      /* Finalize the current CS. */
       cs->ws->base.cs_finalize(_cs);
 
-      /* Chain the current CS to the DGC CS. */
       _cs->buf[_cs->cdw - 4] = PKT3(PKT3_INDIRECT_BUFFER, 2, 0);
       _cs->buf[_cs->cdw - 3] = va;
       _cs->buf[_cs->cdw - 2] = va >> 32;
       _cs->buf[_cs->cdw - 1] = S_3F2_CHAIN(1) | S_3F2_VALID(1) | cdw;
 
-      /* Allocate a new CS BO with initial size. */
       const uint64_t ib_size = radv_amdgpu_cs_get_initial_size(cs->ws, cs->hw_ip);
 
-      VkResult result = radv_amdgpu_cs_bo_create(cs, ib_size);
+      VkResult result = radv_amdgpu_cs_bo_create(cs, (uint32_t)ib_size);
       if (result != VK_SUCCESS) {
          cs->base.cdw = 0;
          cs->status = result;
@@ -886,14 +1101,13 @@ radv_amdgpu_cs_chain_dgc_ib(struct radeo
 
       cs->ws->base.cs_add_buffer(&cs->base, cs->ib_buffer);
 
-      /* Chain back the trailer (DGC CS) to the newly created one. */
       *ib_va_ptr = radv_amdgpu_winsys_bo(cs->ib_buffer)->base.va;
       cs->ib_size_ptr = ib_size_ptr;
 
       cs->base.buf = (uint32_t *)cs->ib_mapped;
       cs->base.cdw = 0;
       cs->base.reserved_dw = 0;
-      cs->base.max_dw = ib_size / 4 - 4;
+      cs->base.max_dw = (uint32_t)(ib_size / 4u - 4u);
    }
 }
 
@@ -934,10 +1148,10 @@ radv_amdgpu_add_cs_to_bo_list(struct rad
       return cs->num_buffers;
    }
 
-   int unique_bo_so_far = num_handles;
+   int unique_bo_so_far = (int)num_handles;
    for (unsigned j = 0; j < cs->num_buffers; ++j) {
       bool found = false;
-      for (unsigned k = 0; k < unique_bo_so_far; ++k) {
+      for (unsigned k = 0; k < (unsigned)unique_bo_so_far; ++k) {
          if (handles[k].bo_handle == cs->handles[j].bo_handle) {
             found = true;
             break;
@@ -996,6 +1210,136 @@ radv_amdgpu_copy_global_bo_list(struct r
    return ws->global_bo_list.count;
 }
 
+/* Temporary BO set for fast O(1) dedup during submit list building. */
+struct radv_bo_set_entry {
+   uint32_t bo_handle; /* 0 means empty */
+};
+
+struct radv_bo_set {
+   struct radv_bo_set_entry *entries;
+   uint32_t size; /* power-of-two */
+   bool full;
+};
+
+static inline uint32_t
+radv_next_pow2_u32(uint32_t v)
+{
+   if (v <= 1u) {
+      return 1u;
+   }
+   v--;
+   v |= v >> 1;
+   v |= v >> 2;
+   v |= v >> 4;
+   v |= v >> 8;
+   v |= v >> 16;
+   v++;
+   return v;
+}
+
+static bool
+radv_bo_set_init(struct radv_bo_set *set, uint32_t expected_elems)
+{
+   uint64_t want = (uint64_t)expected_elems * 2u;
+   if (want < 8u) {
+      want = 8u;
+   }
+   if (want > (1u << 28)) {
+      want = (1u << 28);
+   }
+   set->size = radv_next_pow2_u32((uint32_t)want);
+   set->entries = (struct radv_bo_set_entry *)calloc(set->size, sizeof(struct radv_bo_set_entry));
+   set->full = !set->entries;
+   return !set->full;
+}
+
+static inline void
+radv_bo_set_destroy(struct radv_bo_set *set)
+{
+   free(set->entries);
+   set->entries = NULL;
+   set->size = 0;
+   set->full = false;
+}
+
+static bool
+radv_bo_set_insert(struct radv_bo_set *set, uint32_t bo_handle)
+{
+   if (unlikely(set->full)) {
+      return false;
+   }
+
+   const uint32_t mask = set->size - 1u;
+   uint32_t hash = radv_hash_bo(bo_handle);
+   uint32_t dist = 0u;
+
+   struct radv_bo_set_entry new_entry = { .bo_handle = bo_handle };
+
+   for (;;) {
+      uint32_t pos = (hash + dist) & mask;
+      struct radv_bo_set_entry *entry = &set->entries[pos];
+
+      if (entry->bo_handle == 0u) {
+         *entry = new_entry;
+         return true; /* newly inserted */
+      }
+
+      if (entry->bo_handle == bo_handle) {
+         return false; /* already present */
+      }
+
+      uint32_t entry_hash = radv_hash_bo(entry->bo_handle);
+      uint32_t entry_dist = (pos - (entry_hash & mask) + set->size) & mask;
+
+      if (dist > entry_dist) {
+         struct radv_bo_set_entry tmp = *entry;
+         *entry = new_entry;
+         new_entry = tmp;
+
+         dist = entry_dist;
+         hash = entry_hash;
+      }
+
+      dist++;
+      if (unlikely(dist >= set->size)) {
+         set->full = true;
+         return false;
+      }
+   }
+}
+
+static inline void
+radv_append_cs_bos_dedup(struct radv_amdgpu_cs *cs,
+                         struct drm_amdgpu_bo_list_entry *handles,
+                         unsigned *p_num_handles,
+                         struct radv_bo_set *present)
+{
+   if (cs->num_buffers) {
+      for (unsigned j = 0; j < cs->num_buffers; ++j) {
+         uint32_t h = cs->handles[j].bo_handle;
+         if (radv_bo_set_insert(present, h)) {
+            unsigned idx = (*p_num_handles)++;
+            handles[idx] = cs->handles[j];
+         }
+      }
+   }
+
+   for (unsigned j = 0; j < cs->num_virtual_buffers; ++j) {
+      struct radv_amdgpu_winsys_bo *vbo = radv_amdgpu_winsys_bo(cs->virtual_buffers[j]);
+      u_rwlock_rdlock(&vbo->lock);
+      for (unsigned k = 0; k < vbo->bo_count; ++k) {
+         struct radv_amdgpu_winsys_bo *bo = vbo->bos[k];
+         uint32_t h = bo->bo_handle;
+         if (radv_bo_set_insert(present, h)) {
+            unsigned idx = (*p_num_handles)++;
+            handles[idx].bo_handle = h;
+            handles[idx].bo_priority = bo->priority;
+         }
+      }
+      u_rwlock_rdunlock(&vbo->lock);
+   }
+}
+
 static VkResult
 radv_amdgpu_get_bo_list(struct radv_amdgpu_winsys *ws, struct radeon_cmdbuf **cs_array, unsigned count,
                         struct radeon_cmdbuf **initial_preamble_array, unsigned num_initial_preambles,
@@ -1012,6 +1356,9 @@ radv_amdgpu_get_bo_list(struct radv_amdg
          return VK_ERROR_OUT_OF_HOST_MEMORY;
 
       num_handles = radv_amdgpu_copy_global_bo_list(ws, handles);
+      *rhandles = handles;
+      *rnum_handles = num_handles;
+      return VK_SUCCESS;
    } else if (count == 1 && !num_initial_preambles && !num_continue_preambles && !num_postambles &&
               !radv_amdgpu_cs(cs_array[0])->num_virtual_buffers && !radv_amdgpu_cs(cs_array[0])->chained_to &&
               !ws->global_bo_list.count) {
@@ -1024,7 +1371,9 @@ radv_amdgpu_get_bo_list(struct radv_amdg
          return VK_ERROR_OUT_OF_HOST_MEMORY;
 
       memcpy(handles, cs->handles, sizeof(handles[0]) * cs->num_buffers);
-      num_handles = cs->num_buffers;
+      *rhandles = handles;
+      *rnum_handles = cs->num_buffers;
+      return VK_SUCCESS;
    } else {
       unsigned total_buffer_count = ws->global_bo_list.count;
       total_buffer_count += radv_amdgpu_count_cs_array_bo(cs_array, count);
@@ -1039,13 +1388,54 @@ radv_amdgpu_get_bo_list(struct radv_amdg
       if (!handles)
          return VK_ERROR_OUT_OF_HOST_MEMORY;
 
-      num_handles = radv_amdgpu_copy_global_bo_list(ws, handles);
-      num_handles = radv_amdgpu_add_cs_array_to_bo_list(cs_array, count, handles, num_handles);
-      num_handles =
-         radv_amdgpu_add_cs_array_to_bo_list(initial_preamble_array, num_initial_preambles, handles, num_handles);
-      num_handles =
-         radv_amdgpu_add_cs_array_to_bo_list(continue_preamble_array, num_continue_preambles, handles, num_handles);
-      num_handles = radv_amdgpu_add_cs_array_to_bo_list(postamble_array, num_postambles, handles, num_handles);
+      /* Heuristic: for small lists, the original O(N^2) dedup is cheaper. */
+      if (total_buffer_count <= RADV_SMALL_BO_DEDUP_THRESHOLD) {
+         num_handles = radv_amdgpu_copy_global_bo_list(ws, handles);
+         num_handles = radv_amdgpu_add_cs_array_to_bo_list(cs_array, count, handles, num_handles);
+         num_handles =
+            radv_amdgpu_add_cs_array_to_bo_list(initial_preamble_array, num_initial_preambles, handles, num_handles);
+         num_handles =
+            radv_amdgpu_add_cs_array_to_bo_list(continue_preamble_array, num_continue_preambles, handles, num_handles);
+         num_handles = radv_amdgpu_add_cs_array_to_bo_list(postamble_array, num_postambles, handles, num_handles);
+      } else {
+         struct radv_bo_set present;
+         if (!radv_bo_set_init(&present, total_buffer_count)) {
+            free(handles);
+            return VK_ERROR_OUT_OF_HOST_MEMORY;
+         }
+
+         for (uint32_t i = 0; i < ws->global_bo_list.count; i++) {
+            uint32_t h = ws->global_bo_list.bos[i]->bo_handle;
+            if (radv_bo_set_insert(&present, h)) {
+               handles[num_handles].bo_handle = h;
+               handles[num_handles].bo_priority = ws->global_bo_list.bos[i]->priority;
+               ++num_handles;
+            }
+         }
+
+         for (unsigned i = 0; i < num_initial_preambles; ++i) {
+            for (struct radv_amdgpu_cs *cs = radv_amdgpu_cs(initial_preamble_array[i]); cs; cs = cs->chained_to) {
+               radv_append_cs_bos_dedup(cs, handles, &num_handles, &present);
+            }
+         }
+         for (unsigned i = 0; i < count; ++i) {
+            for (struct radv_amdgpu_cs *cs = radv_amdgpu_cs(cs_array[i]); cs; cs = cs->chained_to) {
+               radv_append_cs_bos_dedup(cs, handles, &num_handles, &present);
+            }
+         }
+         for (unsigned i = 0; i < num_continue_preambles; ++i) {
+            for (struct radv_amdgpu_cs *cs = radv_amdgpu_cs(continue_preamble_array[i]); cs; cs = cs->chained_to) {
+               radv_append_cs_bos_dedup(cs, handles, &num_handles, &present);
+            }
+         }
+         for (unsigned i = 0; i < num_postambles; ++i) {
+            for (struct radv_amdgpu_cs *cs = radv_amdgpu_cs(postamble_array[i]); cs; cs = cs->chained_to) {
+               radv_append_cs_bos_dedup(cs, handles, &num_handles, &present);
+            }
+         }
+
+         radv_bo_set_destroy(&present);
+      }
    }
 
    *rhandles = handles;
@@ -1099,7 +1489,6 @@ radv_amdgpu_winsys_cs_submit_internal(st
 {
    VkResult result;
 
-   /* Last CS is "the gang leader", its IP type determines which fence to signal. */
    struct radv_amdgpu_cs *last_cs = radv_amdgpu_cs(cs_array[cs_count - 1]);
    struct radv_amdgpu_winsys *ws = last_cs->ws;
 
@@ -1120,16 +1509,15 @@ radv_amdgpu_winsys_cs_submit_internal(st
    if (result != VK_SUCCESS)
       goto fail;
 
-   /* Configure the CS request. */
    const uint32_t *max_ib_per_ip = ws->info.max_submitted_ibs;
    struct radv_amdgpu_cs_request request = {
       .ip_type = last_cs->hw_ip,
       .ip_instance = 0,
-      .ring = queue_idx,
+      .ring = (uint32_t)queue_idx,
       .handles = handles,
       .num_handles = num_handles,
       .ibs = ibs,
-      .number_of_ibs = 0, /* set below */
+      .number_of_ibs = 0,
    };
 
    for (unsigned cs_idx = 0, cs_ib_idx = 0; cs_idx < cs_count;) {
@@ -1139,9 +1527,7 @@ radv_amdgpu_winsys_cs_submit_internal(st
       unsigned num_submitted_ibs = 0;
       unsigned ibs_per_ip[AMD_NUM_IP_TYPES] = {0};
 
-      /* Copy preambles to the submission. */
       for (unsigned i = 0; i < preamble_count; ++i) {
-         /* Assume that the full preamble fits into 1 IB. */
          struct radv_amdgpu_cs *cs = radv_amdgpu_cs(preambles[i]);
          struct radv_amdgpu_cs_ib_info ib;
 
@@ -1157,16 +1543,11 @@ radv_amdgpu_winsys_cs_submit_internal(st
          struct radv_amdgpu_cs_ib_info ib;
 
          if (cs_ib_idx == 0) {
-            /* Make sure the whole CS fits into the same submission. */
             unsigned cs_num_ib = radv_amdgpu_get_num_ibs_per_cs(cs);
             if (i + cs_num_ib > ib_per_submit || ibs_per_ip[cs->hw_ip] + cs_num_ib > max_ib_per_ip[cs->hw_ip])
                break;
 
             if (cs->hw_ip != request.ip_type) {
-               /* Found a "follower" CS in a gang submission.
-                * Make sure to submit this together with its "leader", the next CS.
-                * We rely on the caller to order each "follower" before its "leader."
-                */
                assert(cs_idx != cs_count - 1);
                struct radv_amdgpu_cs *next_cs = radv_amdgpu_cs(cs_array[cs_idx + 1]);
                assert(next_cs->hw_ip == request.ip_type);
@@ -1177,10 +1558,6 @@ radv_amdgpu_winsys_cs_submit_internal(st
             }
          }
 
-         /* When IBs are used, we only need to submit the main IB of this CS, because everything
-          * else is chained to the first IB. Otherwise we must submit all IBs in the ib_buffers
-          * array.
-          */
          if (cs->use_ib) {
             ib = radv_amdgpu_cs_ib_to_info(cs, cs->ib_buffers[0]);
             cs_idx++;
@@ -1204,9 +1581,7 @@ radv_amdgpu_winsys_cs_submit_internal(st
 
       assert(num_submitted_ibs > preamble_count);
 
-      /* Copy postambles to the submission. */
       for (unsigned i = 0; i < postamble_count; ++i) {
-         /* Assume that the full postamble fits into 1 IB. */
          struct radv_amdgpu_cs *cs = radv_amdgpu_cs(postamble_cs[i]);
          struct radv_amdgpu_cs_ib_info ib;
 
@@ -1217,7 +1592,6 @@ radv_amdgpu_winsys_cs_submit_internal(st
          ibs_per_ip[cs->hw_ip]++;
       }
 
-      /* Submit the CS. */
       request.number_of_ibs = num_submitted_ibs;
       result = radv_amdgpu_cs_submit(ctx, &request, sem_info);
       if (result != VK_SUCCESS)
@@ -1270,10 +1644,6 @@ radv_amdgpu_cs_submit_zero(struct radv_a
          ret = ac_drm_cs_syncobj_export_sync_file2(
             ctx->ws->dev, sem_info->wait.syncobj[i + sem_info->wait.syncobj_count], sem_info->wait.points[i], 0, &fd2);
          if (ret < 0) {
-            /* This works around a kernel bug where the fence isn't copied if it is already
-             * signalled. Since it is already signalled it is totally fine to not wait on it.
-             *
-             * kernel patch: https://patchwork.freedesktop.org/patch/465583/ */
             uint64_t point;
             ret = ac_drm_cs_syncobj_query2(ctx->ws->dev, &sem_info->wait.syncobj[i + sem_info->wait.syncobj_count],
                                            &point, 1, 0);
@@ -1535,7 +1905,7 @@ radv_amdgpu_winsys_cs_dump(struct radeon
 
             ac_parse_ib(&ib_parser, name);
          } else {
-            ibs[i] = mapped;
+            ibs[i] = (uint32_t *)mapped;
             ib_dw_sizes[i] = ib->cdw;
          }
       }
@@ -1729,9 +2099,6 @@ radv_amdgpu_ctx_set_pstate(struct radeon
       return r;
    }
 
-   /* Do not try to set a new pstate when the current one is already what we want. Otherwise, the
-    * kernel might return -EBUSY if we have multiple AMDGPU contexts in flight.
-    */
    if (current_pstate == new_pstate)
       return 0;
 
@@ -1748,7 +2115,7 @@ static void *
 radv_amdgpu_cs_alloc_syncobj_chunk(struct radv_winsys_sem_counts *counts, uint32_t queue_syncobj,
                                    struct drm_amdgpu_cs_chunk *chunk, int chunk_id)
 {
-   unsigned count = counts->syncobj_count + (queue_syncobj ? 1 : 0);
+   unsigned count = counts->syncobj_count + (queue_syncobj ? 1u : 0u);
    struct drm_amdgpu_cs_chunk_sem *syncobj = malloc(sizeof(struct drm_amdgpu_cs_chunk_sem) * count);
    if (!syncobj)
       return NULL;
@@ -1762,7 +2129,7 @@ radv_amdgpu_cs_alloc_syncobj_chunk(struc
       syncobj[counts->syncobj_count].handle = queue_syncobj;
 
    chunk->chunk_id = chunk_id;
-   chunk->length_dw = sizeof(struct drm_amdgpu_cs_chunk_sem) / 4 * count;
+   chunk->length_dw = (uint32_t)(sizeof(struct drm_amdgpu_cs_chunk_sem) / 4 * count);
    chunk->chunk_data = (uint64_t)(uintptr_t)syncobj;
    return syncobj;
 }
@@ -1771,7 +2138,7 @@ static void *
 radv_amdgpu_cs_alloc_timeline_syncobj_chunk(struct radv_winsys_sem_counts *counts, uint32_t queue_syncobj,
                                             struct drm_amdgpu_cs_chunk *chunk, int chunk_id)
 {
-   uint32_t count = counts->syncobj_count + counts->timeline_syncobj_count + (queue_syncobj ? 1 : 0);
+   uint32_t count = counts->syncobj_count + counts->timeline_syncobj_count + (queue_syncobj ? 1u : 0u);
    struct drm_amdgpu_cs_chunk_syncobj *syncobj = malloc(sizeof(struct drm_amdgpu_cs_chunk_syncobj) * count);
    if (!syncobj)
       return NULL;
@@ -1797,7 +2164,7 @@ radv_amdgpu_cs_alloc_timeline_syncobj_ch
    }
 
    chunk->chunk_id = chunk_id;
-   chunk->length_dw = sizeof(struct drm_amdgpu_cs_chunk_syncobj) / 4 * count;
+   chunk->length_dw = (uint32_t)(sizeof(struct drm_amdgpu_cs_chunk_syncobj) / 4 * count);
    chunk->chunk_data = (uint64_t)(uintptr_t)syncobj;
    return syncobj;
 }
@@ -1817,11 +2184,8 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
    int r;
    int num_chunks;
    int size;
-   struct drm_amdgpu_cs_chunk *chunks;
-   struct drm_amdgpu_cs_chunk_data *chunk_data;
    struct drm_amdgpu_bo_list_in bo_list_in;
    void *wait_syncobj = NULL, *signal_syncobj = NULL;
-   int i;
    VkResult result = VK_SUCCESS;
    bool has_user_fence = radv_amdgpu_cs_has_user_fence(request);
    uint32_t queue_syncobj = radv_amdgpu_ctx_queue_syncobj(ctx, request->ip_type, request->ring);
@@ -1832,20 +2196,18 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
 
    size = request->number_of_ibs + 1 + (has_user_fence ? 1 : 0) + 1 /* bo list */ + 3;
 
-   chunks = malloc(sizeof(chunks[0]) * size);
-   if (!chunks)
-      return VK_ERROR_OUT_OF_HOST_MEMORY;
+   STACK_ARRAY(struct drm_amdgpu_cs_chunk, chunks, size);
 
    size = request->number_of_ibs + (has_user_fence ? 1 : 0);
+   STACK_ARRAY(struct drm_amdgpu_cs_chunk_data, chunk_data, size);
 
-   chunk_data = malloc(sizeof(chunk_data[0]) * size);
-   if (!chunk_data) {
+   if (!chunks || !chunk_data) {
       result = VK_ERROR_OUT_OF_HOST_MEMORY;
-      goto error_out;
+      goto out_finish;
    }
 
    num_chunks = request->number_of_ibs;
-   for (i = 0; i < request->number_of_ibs; i++) {
+   for (int i = 0; i < (int)request->number_of_ibs; i++) {
       struct radv_amdgpu_cs_ib_info *ib;
       chunks[i].chunk_id = AMDGPU_CHUNK_ID_IB;
       chunks[i].length_dw = sizeof(struct drm_amdgpu_cs_chunk_ib) / 4;
@@ -1857,7 +2219,7 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
 
       chunk_data[i].ib_data._pad = 0;
       chunk_data[i].ib_data.va_start = ib->ib_mc_address;
-      chunk_data[i].ib_data.ib_bytes = ib->size * 4;
+      chunk_data[i].ib_data.ib_bytes = ib->size * 4u;
       chunk_data[i].ib_data.ip_type = ib->ip_type;
       chunk_data[i].ib_data.ip_instance = request->ip_instance;
       chunk_data[i].ib_data.ring = request->ring;
@@ -1867,18 +2229,12 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
    assert(chunk_data[request->number_of_ibs - 1].ib_data.ip_type == request->ip_type);
 
    if (has_user_fence) {
-      i = num_chunks++;
+      int i = num_chunks++;
       chunks[i].chunk_id = AMDGPU_CHUNK_ID_FENCE;
       chunks[i].length_dw = sizeof(struct drm_amdgpu_cs_chunk_fence) / 4;
       chunks[i].chunk_data = (uint64_t)(uintptr_t)&chunk_data[i];
 
-      /* Need to reserve 4 QWORD for user fence:
-       *   QWORD[0]: completed fence
-       *   QWORD[1]: preempted fence
-       *   QWORD[2]: reset fence
-       *   QWORD[3]: preempted then reset
-       */
-      uint32_t offset = (request->ip_type * MAX_RINGS_PER_TYPE + request->ring) * 4;
+      uint32_t offset = (request->ip_type * MAX_RINGS_PER_TYPE + request->ring) * 4u;
       ac_drm_cs_chunk_fence_info_to_data(radv_amdgpu_winsys_bo(ctx->fence_bo)->bo_handle, offset, &chunk_data[i]);
    }
 
@@ -1894,7 +2250,7 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
       }
       if (!wait_syncobj) {
          result = VK_ERROR_OUT_OF_HOST_MEMORY;
-         goto error_out;
+         goto out_finish;
       }
       num_chunks++;
 
@@ -1912,13 +2268,13 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
       }
       if (!signal_syncobj) {
          result = VK_ERROR_OUT_OF_HOST_MEMORY;
-         goto error_out;
+         goto out_finish;
       }
       num_chunks++;
    }
 
-   bo_list_in.operation = ~0;
-   bo_list_in.list_handle = ~0;
+   bo_list_in.operation = ~0u;
+   bo_list_in.list_handle = ~0u;
    bo_list_in.bo_number = request->num_handles;
    bo_list_in.bo_info_size = sizeof(struct drm_amdgpu_bo_list_entry);
    bo_list_in.bo_info_ptr = (uint64_t)(uintptr_t)request->handles;
@@ -1928,15 +2284,10 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
    chunks[num_chunks].chunk_data = (uintptr_t)&bo_list_in;
    num_chunks++;
 
-   /* The kernel returns -ENOMEM with many parallel processes using GDS such as test suites quite
-    * often, but it eventually succeeds after enough attempts. This happens frequently with dEQP
-    * using NGG streamout.
-    */
    uint64_t abs_timeout_ns = os_time_get_absolute_timeout(1000000000ull); /* 1s */
 
    r = 0;
    do {
-      /* Wait 1 ms and try again. */
       if (r == -ENOMEM)
          os_time_sleep(1000);
 
@@ -1968,11 +2319,11 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
       }
    }
 
-error_out:
-   free(chunks);
-   free(chunk_data);
+out_finish:
    free(wait_syncobj);
    free(signal_syncobj);
+   STACK_ARRAY_FINISH(chunks);
+   STACK_ARRAY_FINISH(chunk_data);
    return result;
 }
 
