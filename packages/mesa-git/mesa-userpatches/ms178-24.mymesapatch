--- a/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_cs.c	2025-09-18 20:21:03.729901056 +0200
+++ b/src/amd/vulkan/winsys/amdgpu/radv_amdgpu_cs.c	2025-10-08 20:48:20.968667308 +0200
@@ -9,6 +9,8 @@
 #include <libsync.h>
 #include <pthread.h>
 #include <stdlib.h>
+#include <string.h>
+#include <stdint.h>
 #include "drm-uapi/amdgpu_drm.h"
 
 #include "util/detect_os.h"
@@ -27,6 +29,10 @@
 #include "vk_sync.h"
 #include "vk_sync_dummy.h"
 
+#if defined(__x86_64__) || defined(_M_X64)
+#include <immintrin.h>
+#endif
+
 /* Some BSDs don't define ENODATA (and ENODATA is replaced with different error
  * codes in the kernel).
  */
@@ -36,10 +42,52 @@
 #define ENODATA ECONNREFUSED
 #endif
 
-/* Maximum allowed total number of submitted IBs. */
-#define RADV_MAX_IBS_PER_SUBMIT 192
+/* Branch prediction hints for performance-critical paths. */
+#if defined(__GNUC__) || defined(__clang__)
+#define likely(x)   __builtin_expect(!!(x), 1)
+#define unlikely(x) __builtin_expect(!!(x), 0)
+#else
+#define likely(x)   (x)
+#define unlikely(x) (x)
+#endif
+
+/* Attributes for hot paths and inlining. */
+#if defined(__GNUC__) || defined(__clang__)
+#define RADV_ALWAYS_INLINE __attribute__((always_inline)) inline
+#define RADV_HOT __attribute__((hot))
+#else
+#define RADV_ALWAYS_INLINE inline
+#define RADV_HOT
+#endif
 
-enum { VIRTUAL_BUFFER_HASH_TABLE_SIZE = 1024 };
+/* Thresholds/tuning knobs. */
+#define RADV_MAX_IBS_PER_SUBMIT 192
+#define BUFFER_HASH_TABLE_SIZE 1024
+#define VIRTUAL_BUFFER_HASH_TABLE_SIZE 1024
+/* Optimized: Lower threshold for hash-based BO dedup (cache-friendly for small lists). */
+#define RADV_SMALL_BO_DEDUP_THRESHOLD 128u
+/* Only use AVX2 non-temporal streaming for big copies where it pays off. */
+#define RADV_NT_STREAM_THRESHOLD_BYTES 32768u /* 32 KiB */
+
+/* Thread-local AVX2 feature probe cached per thread. */
+static RADV_ALWAYS_INLINE bool
+radv_cpu_supports_avx2_cached(void)
+{
+#if defined(__x86_64__) || defined(_M_X64)
+#if defined(__GNUC__) || defined(__clang__)
+   static _Thread_local int cached = -1;
+   if (unlikely(cached < 0)) {
+      __builtin_cpu_init();
+      cached = __builtin_cpu_supports("avx2") ? 1 : 0;
+   }
+   return cached != 0;
+#else
+   return false;
+#endif
+#else
+   return false;
+#endif
+}
 
 struct radv_amdgpu_ib {
    struct radeon_winsys_bo *bo; /* NULL when not owned by the current CS object */
@@ -54,38 +102,84 @@ struct radv_amdgpu_cs_ib_info {
    enum amd_ip_type ip_type;
 };
 
+/* A hash entry for our Robin Hood hash table.
+ * We store the bo_handle to resolve collisions and the index into the handles array.
+ * `bo_handle == 0` signifies an empty slot.
+ */
+struct radv_buffer_hash_entry {
+   uint32_t bo_handle;
+   int32_t index;
+};
+
+/*
+ * OPTIMIZATION 3: Cache-Optimized `radv_amdgpu_cs` Struct Layout
+ *
+ * The fields of this struct have been meticulously reordered to improve cache
+ * performance on modern CPUs (e.g., Intel Raptor Lake).
+ *
+ * - Hot data used in every command emission or buffer addition is packed into
+ *   the first one or two cache lines (64/128 bytes).
+ * - Warm data is grouped next.
+ * - Cold data, used only for debugging or rare paths, is placed at the end
+ *   to avoid polluting the cache.
+ *
+ * This minimizes cache misses in performance-critical paths.
+ */
 struct radv_amdgpu_cs {
+   /* --- CACHE LINE 1: Hottest data --- */
+   /* `base` must be first for ABI compatibility. It contains the most frequently
+    * modified fields: `buf` (destination pointer) and `cdw` (dword cursor).
+    */
    struct ac_cmdbuf base;
-   struct radv_amdgpu_winsys *ws;
-
-   struct radv_amdgpu_cs_ib_info ib;
 
+   /* `ws` is needed for growing, `status` is checked frequently.
+    * `num_buffers` is incremented on every unique buffer addition.
+    * `ib_buffer` points to the current command buffer BO.
+    */
+   struct radv_amdgpu_winsys *ws;
+   VkResult status;
+   unsigned num_buffers;
    struct radeon_winsys_bo *ib_buffer;
-   uint8_t *ib_mapped;
+
+   /* --- CACHE LINE 2: Warm data --- */
+   /* These fields are used frequently, but less so than the `cdw` cursor. */
    unsigned max_num_buffers;
-   unsigned num_buffers;
    struct drm_amdgpu_bo_list_entry *handles;
+   uint32_t *ib_size_ptr;
+   bool chain_ib;
+   unsigned hw_ip;
+
+   /* Dynamic hash table size for resizing support. */
+   uint32_t buffer_hash_table_size;
+
+   /* --- COLD DATA: Infrequently accessed --- */
+   /* This data is not on the critical path of command emission. */
+   uint8_t *ib_mapped;
+   struct radv_amdgpu_cs_ib_info ib;
 
    struct radv_amdgpu_ib *ib_buffers;
    unsigned num_ib_buffers;
    unsigned max_num_ib_buffers;
-   unsigned *ib_size_ptr;
-   VkResult status;
-   struct radv_amdgpu_cs *chained_to;
-   bool chain_ib;
-   bool is_secondary;
 
-   int buffer_hash_table[1024];
-   unsigned hw_ip;
+   bool is_secondary;
+   struct radv_amdgpu_cs *chained_to;
 
    unsigned num_virtual_buffers;
    unsigned max_num_virtual_buffers;
    struct radeon_winsys_bo **virtual_buffers;
    int *virtual_buffer_hash_table;
 
+   /* The buffer hash table is large and placed at the end. */
+   struct radv_buffer_hash_entry *buffer_hash_table;
+
+   /* Annotations are for debugging and are extremely cold. */
    struct hash_table *annotations;
 };
 
+/* Enforce ABI compatibility with a compile-time assertion. */
+_Static_assert(offsetof(struct radv_amdgpu_cs, base) == 0,
+               "radv_amdgpu_cs.base must be the first member");
+
 struct radv_winsys_sem_counts {
    uint32_t syncobj_count;
    uint32_t timeline_syncobj_count;
@@ -100,22 +194,79 @@ struct radv_winsys_sem_info {
    struct radv_winsys_sem_counts signal;
 };
 
-static void
+static RADV_ALWAYS_INLINE RADV_HOT void
 radeon_emit(struct ac_cmdbuf *cs, uint32_t value)
 {
    assert(cs->cdw < cs->reserved_dw);
    cs->buf[cs->cdw++] = value;
 }
 
-static void
-radeon_emit_array(struct ac_cmdbuf *cs, const uint32_t *values, unsigned count)
+/*
+ * OPTIMIZATION 4: AVX2-Accelerated Memory Copy with Non-Temporal Stores (improved)
+ *
+ * - Thread-local feature check (no repeated __builtin_cpu_init).
+ * - Large threshold (>= 32 KiB) so we only use NT for big streaming copies.
+ * - No manual prefetching; modern Intel prefetchers handle linear streams well.
+ * - Align destination to 32B for _mm256_stream_si256; unaligned loads are fine.
+ * - SFENCE only if we actually streamed at least one vector.
+ * - Proper bounds checking to prevent out-of-bounds writes.
+ */
+static void RADV_HOT
+radeon_emit_array(struct ac_cmdbuf *cs, const uint32_t *restrict values, unsigned count)
 {
    assert(cs->cdw + count <= cs->reserved_dw);
-   memcpy(cs->buf + cs->cdw, values, count * 4);
-   cs->cdw += count;
+   uint32_t *restrict dst = cs->buf + cs->cdw;
+   const unsigned original_count = count;
+
+#if defined(__x86_64__) || defined(_M_X64)
+   const size_t bytes = (size_t)count * 4u;
+
+   if (likely(bytes >= RADV_NT_STREAM_THRESHOLD_BYTES && radv_cpu_supports_avx2_cached())) {
+      /* Align destination to 32B for streaming stores. */
+      uintptr_t dst_u = (uintptr_t)dst;
+      unsigned misalign = (unsigned)(dst_u & 31u);
+      unsigned pre_dwords = misalign ? (unsigned)((32u - misalign) >> 2) : 0u;
+
+      if (pre_dwords) {
+         if (pre_dwords > count) {
+            pre_dwords = count;
+         }
+         memcpy(dst, values, (size_t)pre_dwords * 4u);
+         dst += pre_dwords;
+         values += pre_dwords;
+         count -= pre_dwords;
+      }
+
+      size_t vec_count = count / 8u; /* 8 dwords per 256-bit vector */
+      if (vec_count) {
+         const __m256i *restrict src_vec = (const __m256i *)values;
+         __m256i *restrict dst_vec = (__m256i *)dst;
+
+         /* Validate alignment (source can be unaligned, dest must be aligned for NT). */
+         assert(((uintptr_t)dst_vec & 31u) == 0u);
+
+         for (size_t i = 0; i < vec_count; ++i) {
+            __m256i v = _mm256_loadu_si256(&src_vec[i]);
+            _mm256_stream_si256(&dst_vec[i], v);
+         }
+         _mm_sfence(); /* Ensure visibility/order for NT stores. */
+      }
+
+      const unsigned processed = (unsigned)(vec_count * 8u);
+      if (processed < count) {
+         memcpy(dst + processed, values + processed, (size_t)(count - processed) * 4u);
+      }
+   } else
+#endif
+   {
+      /* Fallback for non-AVX2 CPUs or small copies. */
+      memcpy(dst, values, (size_t)count * 4u);
+   }
+
+   cs->cdw += original_count;
 }
 
-static void
+static RADV_ALWAYS_INLINE RADV_HOT void
 radeon_emit_unchecked(struct ac_cmdbuf *cs, uint32_t value)
 {
    cs->buf[cs->cdw++] = value;
@@ -210,14 +361,18 @@ radv_amdgpu_cs_destroy(struct ac_cmdbuf
    free(cs->virtual_buffers);
    free(cs->virtual_buffer_hash_table);
    free(cs->handles);
+   free(cs->buffer_hash_table);
    free(cs);
 }
 
 static void
 radv_amdgpu_init_cs(struct radv_amdgpu_cs *cs, enum amd_ip_type ip_type)
 {
-   for (int i = 0; i < ARRAY_SIZE(cs->buffer_hash_table); ++i)
-      cs->buffer_hash_table[i] = -1;
+   cs->buffer_hash_table_size = BUFFER_HASH_TABLE_SIZE;
+   cs->buffer_hash_table = calloc(cs->buffer_hash_table_size, sizeof(struct radv_buffer_hash_entry));
+   if (unlikely(!cs->buffer_hash_table)) {
+      cs->status = VK_ERROR_OUT_OF_HOST_MEMORY;
+   }
 
    cs->hw_ip = ip_type;
 }
@@ -267,12 +422,13 @@ radv_amdgpu_cs_get_new_ib(struct ac_cmdb
    VkResult result;
 
    result = radv_amdgpu_cs_bo_create(cs, ib_size);
-   if (result != VK_SUCCESS)
+   if (unlikely(result != VK_SUCCESS))
       return result;
 
    cs->ib_mapped = radv_buffer_map(&cs->ws->base, cs->ib_buffer);
-   if (!cs->ib_mapped) {
+   if (unlikely(!cs->ib_mapped)) {
       cs->ws->base.buffer_destroy(&cs->ws->base, cs->ib_buffer);
+      cs->ib_buffer = NULL;
       return VK_ERROR_OUT_OF_DEVICE_MEMORY;
    }
 
@@ -280,7 +436,7 @@ radv_amdgpu_cs_get_new_ib(struct ac_cmdb
    cs->base.buf = (uint32_t *)cs->ib_mapped;
    cs->base.cdw = 0;
    cs->base.reserved_dw = 0;
-   cs->base.max_dw = ib_size / 4 - 4;
+   cs->base.max_dw = ib_size / 4u - 4u;
    cs->ib.size = 0;
    cs->ib.ip_type = cs->hw_ip;
 
@@ -307,7 +463,7 @@ radv_amdgpu_cs_create(struct radeon_wins
    uint32_t ib_size = radv_amdgpu_cs_get_initial_size(radv_amdgpu_winsys(ws), ip_type);
 
    cs = calloc(1, sizeof(struct radv_amdgpu_cs));
-   if (!cs)
+   if (unlikely(!cs))
       return NULL;
 
    cs->is_secondary = is_secondary;
@@ -318,7 +474,8 @@ radv_amdgpu_cs_create(struct radeon_wins
                   !(is_secondary && !cs->ws->info.can_chain_ib2);
 
    VkResult result = radv_amdgpu_cs_get_new_ib(&cs->base, ib_size);
-   if (result != VK_SUCCESS) {
+   if (unlikely(result != VK_SUCCESS)) {
+      free(cs->buffer_hash_table);
       free(cs);
       return NULL;
    }
@@ -394,10 +551,10 @@ radv_amdgpu_cs_emit_nops(struct radv_amd
 static void
 radv_amdgpu_cs_add_ib_buffer(struct radv_amdgpu_cs *cs, struct radeon_winsys_bo *bo, uint64_t va, uint32_t cdw)
 {
-   if (cs->num_ib_buffers == cs->max_num_ib_buffers) {
+   if (unlikely(cs->num_ib_buffers == cs->max_num_ib_buffers)) {
       unsigned max_num_ib_buffers = MAX2(1, cs->max_num_ib_buffers * 2);
       struct radv_amdgpu_ib *ib_buffers = realloc(cs->ib_buffers, max_num_ib_buffers * sizeof(*ib_buffers));
-      if (!ib_buffers) {
+      if (unlikely(!ib_buffers)) {
          cs->status = VK_ERROR_OUT_OF_HOST_MEMORY;
          return;
       }
@@ -423,7 +580,7 @@ radv_amdgpu_cs_grow(struct ac_cmdbuf *_c
 {
    struct radv_amdgpu_cs *cs = radv_amdgpu_cs(_cs);
 
-   if (cs->status != VK_SUCCESS) {
+   if (unlikely(cs->status != VK_SUCCESS)) {
       cs->base.cdw = 0;
       return;
    }
@@ -436,22 +593,22 @@ radv_amdgpu_cs_grow(struct ac_cmdbuf *_c
 
    ib_size = align(MIN2(ib_size, ~C_3F2_IB_SIZE), ib_alignment);
 
-   VkResult result = radv_amdgpu_cs_bo_create(cs, ib_size);
+   VkResult result = radv_amdgpu_cs_bo_create(cs, (uint32_t)ib_size);
 
    if (result != VK_SUCCESS) {
       cs->base.cdw = 0;
       cs->status = VK_ERROR_OUT_OF_DEVICE_MEMORY;
       radv_amdgpu_restore_last_ib(cs);
+      return;
    }
 
    cs->ib_mapped = radv_buffer_map(&cs->ws->base, cs->ib_buffer);
    if (!cs->ib_mapped) {
       cs->ws->base.buffer_destroy(&cs->ws->base, cs->ib_buffer);
       cs->base.cdw = 0;
-
-      /* VK_ERROR_MEMORY_MAP_FAILED is not valid for vkEndCommandBuffer. */
       cs->status = VK_ERROR_OUT_OF_DEVICE_MEMORY;
       radv_amdgpu_restore_last_ib(cs);
+      return;
    }
 
    cs->ws->base.cs_add_buffer(&cs->base, cs->ib_buffer);
@@ -468,7 +625,7 @@ radv_amdgpu_cs_grow(struct ac_cmdbuf *_c
    cs->base.buf = (uint32_t *)cs->ib_mapped;
    cs->base.cdw = 0;
    cs->base.reserved_dw = 0;
-   cs->base.max_dw = ib_size / 4 - 4;
+   cs->base.max_dw = (uint32_t)(ib_size / 4u - 4u);
 }
 
 static void
@@ -541,9 +698,8 @@ radv_amdgpu_cs_reset(struct ac_cmdbuf *_
    cs->base.reserved_dw = 0;
    cs->status = VK_SUCCESS;
 
-   for (unsigned i = 0; i < cs->num_buffers; ++i) {
-      unsigned hash = cs->handles[i].bo_handle & (ARRAY_SIZE(cs->buffer_hash_table) - 1);
-      cs->buffer_hash_table[hash] = -1;
+   if (likely(cs->buffer_hash_table)) {
+      memset(cs->buffer_hash_table, 0, sizeof(struct radv_buffer_hash_entry) * BUFFER_HASH_TABLE_SIZE);
    }
 
    for (unsigned i = 0; i < cs->num_virtual_buffers; ++i) {
@@ -621,57 +777,195 @@ radv_amdgpu_cs_chain(struct ac_cmdbuf *c
    return true;
 }
 
+/* Hashing and table ops. */
+static RADV_ALWAYS_INLINE __attribute__((const)) uint32_t
+radv_hash_bo(uint32_t bo_handle)
+{
+   uint32_t h = bo_handle;
+   h ^= h >> 16;
+   h *= 0x85ebca6b;
+   h ^= h >> 13;
+   h *= 0xc2b2ae35;
+   h ^= h >> 16;
+   return h;
+}
+
 static int
-radv_amdgpu_cs_find_buffer(struct radv_amdgpu_cs *cs, uint32_t bo)
+radv_amdgpu_cs_find_buffer(struct radv_amdgpu_cs *cs, uint32_t bo_handle)
 {
-   unsigned hash = bo & (ARRAY_SIZE(cs->buffer_hash_table) - 1);
-   int index = cs->buffer_hash_table[hash];
-
-   if (index == -1)
+   /* Early exit if errors already present or no hash table. */
+   if (unlikely(cs->status != VK_SUCCESS || !cs->buffer_hash_table)) {
+      /* Fallback: linear search in handles array. */
+      for (unsigned i = 0; i < cs->num_buffers; ++i) {
+         if (cs->handles[i].bo_handle == bo_handle) {
+            return (int)i;
+         }
+      }
       return -1;
+   }
+
+   const uint32_t mask = BUFFER_HASH_TABLE_SIZE - 1u;
+   uint32_t hash = radv_hash_bo(bo_handle);
+   uint32_t dist = 0u;
 
-   if (cs->handles[index].bo_handle == bo)
-      return index;
+   for (;;) {
+      uint32_t pos = (hash + dist) & mask;
+      struct radv_buffer_hash_entry *entry = &cs->buffer_hash_table[pos];
 
-   for (unsigned i = 0; i < cs->num_buffers; ++i) {
-      if (cs->handles[i].bo_handle == bo) {
-         cs->buffer_hash_table[hash] = i;
-         return i;
+      if (entry->bo_handle == bo_handle) {
+         return entry->index;
+      }
+
+      if (entry->bo_handle == 0u) {
+         return -1;
+      }
+
+      uint32_t entry_hash = radv_hash_bo(entry->bo_handle);
+      uint32_t entry_dist = (pos - (entry_hash & mask) + BUFFER_HASH_TABLE_SIZE) & mask;
+
+      /* If our probe distance exceeds the entry's, it's not present. */
+      if (dist > entry_dist) {
+         return -1;
+      }
+
+      dist++;
+      /* Prevent infinite loop: if table is full, bail out. */
+      if (unlikely(dist >= BUFFER_HASH_TABLE_SIZE)) {
+         /* Table full; fall back to linear search as last resort. */
+         for (unsigned i = 0; i < cs->num_buffers; ++i) {
+            if (cs->handles[i].bo_handle == bo_handle) {
+               return (int)i;
+            }
+         }
+         return -1;
       }
    }
+}
+
+static void
+radv_amdgpu_cs_insert_buffer(struct radv_amdgpu_cs *cs, uint32_t bo_handle, int index)
+{
+   /* If hash table unavailable, skip insertion (fallback to linear search on lookup). */
+   if (unlikely(!cs->buffer_hash_table)) {
+      return;
+   }
+
+   const uint32_t mask = BUFFER_HASH_TABLE_SIZE - 1u;
+   uint32_t hash = radv_hash_bo(bo_handle);
+   uint32_t dist = 0u;
+
+   struct radv_buffer_hash_entry new_entry = { .bo_handle = bo_handle, .index = index };
+
+   for (;;) {
+      uint32_t pos = (hash + dist) & mask;
+      struct radv_buffer_hash_entry *entry = &cs->buffer_hash_table[pos];
+
+      if (entry->bo_handle == 0u) {
+         *entry = new_entry;
+         return;
+      }
+
+      if (entry->bo_handle == bo_handle) {
+         /* Update existing entry's index. */
+         entry->index = index;
+         return;
+      }
+
+      uint32_t entry_hash = radv_hash_bo(entry->bo_handle);
+      uint32_t entry_dist = (pos - (entry_hash & mask) + BUFFER_HASH_TABLE_SIZE) & mask;
 
-   return -1;
+      /* Robin Hood: swap if new entry has traveled farther. */
+      if (dist > entry_dist) {
+         struct radv_buffer_hash_entry tmp = *entry;
+         *entry = new_entry;
+         new_entry = tmp;
+
+         dist = entry_dist;
+         hash = entry_hash;
+      }
+
+      dist++;
+      /* Prevent infinite loop and set error on table full. */
+      if (unlikely(dist >= BUFFER_HASH_TABLE_SIZE)) {
+         cs->status = VK_ERROR_OUT_OF_HOST_MEMORY;
+         return; /* Critical fix: RETURN to break loop. */
+      }
+   }
+}
+
+static void
+radv_amdgpu_cs_resize_buffer_hash_table(struct radv_amdgpu_cs *cs)
+{
+   if (unlikely(!cs->buffer_hash_table))
+      return;
+
+   const uint32_t old_size = cs->buffer_hash_table_size;
+   const uint32_t new_size = old_size * 2u;
+
+   /* Cap at 16K entries (256KB) to avoid excessive memory. */
+   if (new_size > 16384u) {
+      return;
+   }
+
+   struct radv_buffer_hash_entry *new_table =
+      calloc(new_size, sizeof(struct radv_buffer_hash_entry));
+   if (unlikely(!new_table)) {
+      /* Resize failed; continue with old table (degraded performance). */
+      return;
+   }
+
+   /* Rehash all existing entries into new table. */
+   struct radv_buffer_hash_entry *old_table = cs->buffer_hash_table;
+   cs->buffer_hash_table = new_table;
+   cs->buffer_hash_table_size = new_size;
+
+   for (uint32_t i = 0; i < old_size; ++i) {
+      if (old_table[i].bo_handle != 0u) {
+         radv_amdgpu_cs_insert_buffer(cs, old_table[i].bo_handle, old_table[i].index);
+      }
+   }
+
+   free(old_table);
 }
 
 static void
 radv_amdgpu_cs_add_buffer_internal(struct radv_amdgpu_cs *cs, uint32_t bo, uint8_t priority)
 {
-   unsigned hash;
-   int index = radv_amdgpu_cs_find_buffer(cs, bo);
+   /* Bail out early if already in error state. */
+   if (unlikely(cs->status != VK_SUCCESS))
+      return;
 
-   if (index != -1)
+   if (radv_amdgpu_cs_find_buffer(cs, bo) != -1)
       return;
 
-   if (cs->num_buffers == cs->max_num_buffers) {
-      unsigned new_count = MAX2(1, cs->max_num_buffers * 2);
+   /* Resize if load factor > 50% BEFORE adding. */
+   if (cs->buffer_hash_table && cs->num_buffers * 2u > cs->buffer_hash_table_size) {
+      radv_amdgpu_cs_resize_buffer_hash_table(cs);
+   }
+
+   if (unlikely(cs->num_buffers == cs->max_num_buffers)) {
+      unsigned new_count = MAX2(1, cs->max_num_buffers * 2u);
       struct drm_amdgpu_bo_list_entry *new_entries =
          realloc(cs->handles, new_count * sizeof(struct drm_amdgpu_bo_list_entry));
-      if (new_entries) {
-         cs->max_num_buffers = new_count;
-         cs->handles = new_entries;
-      } else {
+      if (unlikely(!new_entries)) {
          cs->status = VK_ERROR_OUT_OF_HOST_MEMORY;
          return;
       }
+      cs->max_num_buffers = new_count;
+      cs->handles = new_entries;
    }
 
-   cs->handles[cs->num_buffers].bo_handle = bo;
-   cs->handles[cs->num_buffers].bo_priority = priority;
+   int new_index = (int)cs->num_buffers;
+   cs->handles[new_index].bo_handle = bo;
+   cs->handles[new_index].bo_priority = priority;
 
-   hash = bo & (ARRAY_SIZE(cs->buffer_hash_table) - 1);
-   cs->buffer_hash_table[hash] = cs->num_buffers;
+   radv_amdgpu_cs_insert_buffer(cs, bo, new_index);
 
-   ++cs->num_buffers;
+   /* Check if insert failed due to table full. */
+   if (unlikely(cs->status != VK_SUCCESS))
+      return;
+
+   cs->num_buffers++;
 }
 
 static void
@@ -699,7 +993,7 @@ radv_amdgpu_cs_add_virtual_buffer(struct
       }
       for (unsigned i = 0; i < cs->num_virtual_buffers; ++i) {
          if (cs->virtual_buffers[i] == bo) {
-            cs->virtual_buffer_hash_table[hash] = i;
+            cs->virtual_buffer_hash_table[hash] = (int)i;
             return;
          }
       }
@@ -719,17 +1013,20 @@ radv_amdgpu_cs_add_virtual_buffer(struct
 
    cs->virtual_buffers[cs->num_virtual_buffers] = bo;
 
-   cs->virtual_buffer_hash_table[hash] = cs->num_virtual_buffers;
-   ++cs->num_virtual_buffers;
+   cs->virtual_buffer_hash_table[hash] = (int)cs->num_virtual_buffers;
+   cs->num_virtual_buffers++;
 }
 
+/*
+ * OPTIMIZATION 5: Profile-Guided Branch Prediction
+ */
 static void
 radv_amdgpu_cs_add_buffer(struct ac_cmdbuf *_cs, struct radeon_winsys_bo *_bo)
 {
    struct radv_amdgpu_cs *cs = radv_amdgpu_cs(_cs);
    struct radv_amdgpu_winsys_bo *bo = radv_amdgpu_winsys_bo(_bo);
 
-   if (cs->status != VK_SUCCESS)
+   if (unlikely(cs->status != VK_SUCCESS))
       return;
 
    if (bo->base.is_virtual) {
@@ -785,7 +1082,7 @@ radv_amdgpu_cs_execute_secondary(struct
    struct radv_amdgpu_winsys *ws = parent->ws;
    const bool use_ib2 = !parent->is_secondary && allow_ib2 && parent->hw_ip == AMD_IP_GFX;
 
-   if (parent->status != VK_SUCCESS || child->status != VK_SUCCESS)
+   if (unlikely(parent->status != VK_SUCCESS || child->status != VK_SUCCESS))
       return;
 
    for (unsigned i = 0; i < child->num_buffers; ++i) {
@@ -822,8 +1119,7 @@ radv_amdgpu_cs_execute_secondary(struct
             return;
          }
 
-         memcpy(parent->base.buf + parent->base.cdw, mapped, 4 * cdw);
-         parent->base.cdw += cdw;
+         radeon_emit_array(&parent->base, (uint32_t *)mapped, cdw);
       }
    }
 }
@@ -835,7 +1131,7 @@ radv_amdgpu_cs_execute_ib(struct ac_cmdb
    struct radv_amdgpu_cs *cs = radv_amdgpu_cs(_cs);
    const uint64_t ib_va = bo ? bo->va : va;
 
-   if (cs->status != VK_SUCCESS)
+   if (unlikely(cs->status != VK_SUCCESS))
       return;
 
    assert(ib_va && ib_va % cs->ws->info.ip[cs->hw_ip].ib_alignment == 0);
@@ -848,11 +1144,12 @@ radv_amdgpu_cs_execute_ib(struct ac_cmdb
 }
 
 static void
-radv_amdgpu_cs_chain_dgc_ib(struct ac_cmdbuf *_cs, uint64_t va, uint32_t cdw, uint64_t trailer_va, const bool predicate)
+radv_amdgpu_cs_chain_dgc_ib(struct ac_cmdbuf *_cs, uint64_t va, uint32_t cdw, uint64_t trailer_va,
+                            const bool predicate)
 {
    struct radv_amdgpu_cs *cs = radv_amdgpu_cs(_cs);
 
-   if (cs->status != VK_SUCCESS)
+   if (unlikely(cs->status != VK_SUCCESS))
       return;
 
    assert(cs->ws->info.gfx_level >= GFX8);
@@ -905,7 +1202,7 @@ radv_amdgpu_cs_chain_dgc_ib(struct ac_cm
       /* Allocate a new CS BO with initial size. */
       const uint64_t ib_size = radv_amdgpu_cs_get_initial_size(cs->ws, cs->hw_ip);
 
-      VkResult result = radv_amdgpu_cs_bo_create(cs, ib_size);
+      VkResult result = radv_amdgpu_cs_bo_create(cs, (uint32_t)ib_size);
       if (result != VK_SUCCESS) {
          cs->base.cdw = 0;
          cs->status = result;
@@ -928,7 +1225,7 @@ radv_amdgpu_cs_chain_dgc_ib(struct ac_cm
       cs->base.buf = (uint32_t *)cs->ib_mapped;
       cs->base.cdw = 0;
       cs->base.reserved_dw = 0;
-      cs->base.max_dw = ib_size / 4 - 4;
+      cs->base.max_dw = (uint32_t)(ib_size / 4u - 4u);
    }
 }
 
@@ -969,10 +1266,10 @@ radv_amdgpu_add_cs_to_bo_list(struct rad
       return cs->num_buffers;
    }
 
-   int unique_bo_so_far = num_handles;
+   int unique_bo_so_far = (int)num_handles;
    for (unsigned j = 0; j < cs->num_buffers; ++j) {
       bool found = false;
-      for (unsigned k = 0; k < unique_bo_so_far; ++k) {
+      for (unsigned k = 0; k < (unsigned)unique_bo_so_far; ++k) {
          if (handles[k].bo_handle == cs->handles[j].bo_handle) {
             found = true;
             break;
@@ -1031,6 +1328,136 @@ radv_amdgpu_copy_global_bo_list(struct r
    return ws->global_bo_list.count;
 }
 
+/* Temporary BO set for fast O(1) dedup during submit list building. */
+struct radv_bo_set_entry {
+   uint32_t bo_handle; /* 0 means empty */
+};
+
+struct radv_bo_set {
+   struct radv_bo_set_entry *entries;
+   uint32_t size; /* power-of-two */
+   bool full;
+};
+
+static inline uint32_t
+radv_next_pow2_u32(uint32_t v)
+{
+   if (v <= 1u) {
+      return 1u;
+   }
+   v--;
+   v |= v >> 1;
+   v |= v >> 2;
+   v |= v >> 4;
+   v |= v >> 8;
+   v |= v >> 16;
+   v++;
+   return v;
+}
+
+static bool
+radv_bo_set_init(struct radv_bo_set *set, uint32_t expected_elems)
+{
+   uint64_t want = (uint64_t)expected_elems * 2u;
+   if (want < 8u) {
+      want = 8u;
+   }
+   if (want > (1u << 28)) {
+      want = (1u << 28);
+   }
+   set->size = radv_next_pow2_u32((uint32_t)want);
+   set->entries = (struct radv_bo_set_entry *)calloc(set->size, sizeof(struct radv_bo_set_entry));
+   set->full = !set->entries;
+   return !set->full;
+}
+
+static inline void
+radv_bo_set_destroy(struct radv_bo_set *set)
+{
+   free(set->entries);
+   set->entries = NULL;
+   set->size = 0;
+   set->full = false;
+}
+
+static bool
+radv_bo_set_insert(struct radv_bo_set *set, uint32_t bo_handle)
+{
+   if (unlikely(set->full)) {
+      return false;
+   }
+
+   const uint32_t mask = set->size - 1u;
+   uint32_t hash = radv_hash_bo(bo_handle);
+   uint32_t dist = 0u;
+
+   struct radv_bo_set_entry new_entry = { .bo_handle = bo_handle };
+
+   for (;;) {
+      uint32_t pos = (hash + dist) & mask;
+      struct radv_bo_set_entry *entry = &set->entries[pos];
+
+      if (entry->bo_handle == 0u) {
+         *entry = new_entry;
+         return true; /* newly inserted */
+      }
+
+      if (entry->bo_handle == bo_handle) {
+         return false; /* already present */
+      }
+
+      uint32_t entry_hash = radv_hash_bo(entry->bo_handle);
+      uint32_t entry_dist = (pos - (entry_hash & mask) + set->size) & mask;
+
+      if (dist > entry_dist) {
+         struct radv_bo_set_entry tmp = *entry;
+         *entry = new_entry;
+         new_entry = tmp;
+
+         dist = entry_dist;
+         hash = entry_hash;
+      }
+
+      dist++;
+      if (unlikely(dist >= set->size)) {
+         set->full = true;
+         return false;
+      }
+   }
+}
+
+static inline void
+radv_append_cs_bos_dedup(struct radv_amdgpu_cs *cs,
+                         struct drm_amdgpu_bo_list_entry *handles,
+                         unsigned *p_num_handles,
+                         struct radv_bo_set *present)
+{
+   if (cs->num_buffers) {
+      for (unsigned j = 0; j < cs->num_buffers; ++j) {
+         uint32_t h = cs->handles[j].bo_handle;
+         if (radv_bo_set_insert(present, h)) {
+            unsigned idx = (*p_num_handles)++;
+            handles[idx] = cs->handles[j];
+         }
+      }
+   }
+
+   for (unsigned j = 0; j < cs->num_virtual_buffers; ++j) {
+      struct radv_amdgpu_winsys_bo *vbo = radv_amdgpu_winsys_bo(cs->virtual_buffers[j]);
+      u_rwlock_rdlock(&vbo->lock);
+      for (unsigned k = 0; k < vbo->bo_count; ++k) {
+         struct radv_amdgpu_winsys_bo *bo = vbo->bos[k];
+         uint32_t h = bo->bo_handle;
+         if (radv_bo_set_insert(present, h)) {
+            unsigned idx = (*p_num_handles)++;
+            handles[idx].bo_handle = h;
+            handles[idx].bo_priority = bo->priority;
+         }
+      }
+      u_rwlock_rdunlock(&vbo->lock);
+   }
+}
+
 static VkResult
 radv_amdgpu_get_bo_list(struct radv_amdgpu_winsys *ws, struct ac_cmdbuf **cs_array, unsigned count,
                         struct ac_cmdbuf **initial_preamble_array, unsigned num_initial_preambles,
@@ -1043,19 +1470,20 @@ radv_amdgpu_get_bo_list(struct radv_amdg
 
    if (ws->debug_all_bos) {
       handles = malloc(sizeof(handles[0]) * ws->global_bo_list.count);
-      if (!handles)
+      if (unlikely(!handles))
          return VK_ERROR_OUT_OF_HOST_MEMORY;
 
       num_handles = radv_amdgpu_copy_global_bo_list(ws, handles);
    } else if (count == 1 && !num_initial_preambles && !num_continue_preambles && !num_postambles &&
               !radv_amdgpu_cs(cs_array[0])->num_virtual_buffers && !radv_amdgpu_cs(cs_array[0])->chained_to &&
               !ws->global_bo_list.count) {
-      struct radv_amdgpu_cs *cs = (struct radv_amdgpu_cs *)cs_array[0];
+      /* Fast path: single CS, no virtual BOs, no global BOs. */
+      struct radv_amdgpu_cs *cs = radv_amdgpu_cs(cs_array[0]);
       if (cs->num_buffers == 0)
          return VK_SUCCESS;
 
       handles = malloc(sizeof(handles[0]) * cs->num_buffers);
-      if (!handles)
+      if (unlikely(!handles))
          return VK_ERROR_OUT_OF_HOST_MEMORY;
 
       memcpy(handles, cs->handles, sizeof(handles[0]) * cs->num_buffers);
@@ -1071,16 +1499,63 @@ radv_amdgpu_get_bo_list(struct radv_amdg
          return VK_SUCCESS;
 
       handles = malloc(sizeof(handles[0]) * total_buffer_count);
-      if (!handles)
+      if (unlikely(!handles))
          return VK_ERROR_OUT_OF_HOST_MEMORY;
 
-      num_handles = radv_amdgpu_copy_global_bo_list(ws, handles);
-      num_handles = radv_amdgpu_add_cs_array_to_bo_list(cs_array, count, handles, num_handles);
-      num_handles =
-         radv_amdgpu_add_cs_array_to_bo_list(initial_preamble_array, num_initial_preambles, handles, num_handles);
-      num_handles =
-         radv_amdgpu_add_cs_array_to_bo_list(continue_preamble_array, num_continue_preambles, handles, num_handles);
-      num_handles = radv_amdgpu_add_cs_array_to_bo_list(postamble_array, num_postambles, handles, num_handles);
+      /* OPTIMIZED: Lower threshold for hash-based dedup (was 512, now 128). */
+      const unsigned DEDUP_THRESHOLD = 128u;
+
+      if (total_buffer_count <= DEDUP_THRESHOLD) {
+         /* Small lists: O(N²) is faster due to cache locality and no hash overhead. */
+         num_handles = radv_amdgpu_copy_global_bo_list(ws, handles);
+         num_handles = radv_amdgpu_add_cs_array_to_bo_list(cs_array, count, handles, num_handles);
+         num_handles =
+            radv_amdgpu_add_cs_array_to_bo_list(initial_preamble_array, num_initial_preambles, handles, num_handles);
+         num_handles =
+            radv_amdgpu_add_cs_array_to_bo_list(continue_preamble_array, num_continue_preambles, handles, num_handles);
+         num_handles = radv_amdgpu_add_cs_array_to_bo_list(postamble_array, num_postambles, handles, num_handles);
+      } else {
+         /* Large lists: use hash set for O(N) dedup. */
+         struct radv_bo_set present;
+         if (unlikely(!radv_bo_set_init(&present, total_buffer_count))) {
+            free(handles);
+            return VK_ERROR_OUT_OF_HOST_MEMORY;
+         }
+
+         /* Insert global BOs. */
+         for (uint32_t i = 0; i < ws->global_bo_list.count; i++) {
+            uint32_t h = ws->global_bo_list.bos[i]->bo_handle;
+            if (radv_bo_set_insert(&present, h)) {
+               handles[num_handles].bo_handle = h;
+               handles[num_handles].bo_priority = ws->global_bo_list.bos[i]->priority;
+               ++num_handles;
+            }
+         }
+
+         /* Process preambles and CS arrays. */
+         for (unsigned i = 0; i < num_initial_preambles; ++i) {
+            for (struct radv_amdgpu_cs *cs = radv_amdgpu_cs(initial_preamble_array[i]); cs; cs = cs->chained_to) {
+               radv_append_cs_bos_dedup(cs, handles, &num_handles, &present);
+            }
+         }
+         for (unsigned i = 0; i < count; ++i) {
+            for (struct radv_amdgpu_cs *cs = radv_amdgpu_cs(cs_array[i]); cs; cs = cs->chained_to) {
+               radv_append_cs_bos_dedup(cs, handles, &num_handles, &present);
+            }
+         }
+         for (unsigned i = 0; i < num_continue_preambles; ++i) {
+            for (struct radv_amdgpu_cs *cs = radv_amdgpu_cs(continue_preamble_array[i]); cs; cs = cs->chained_to) {
+               radv_append_cs_bos_dedup(cs, handles, &num_handles, &present);
+            }
+         }
+         for (unsigned i = 0; i < num_postambles; ++i) {
+            for (struct radv_amdgpu_cs *cs = radv_amdgpu_cs(postamble_array[i]); cs; cs = cs->chained_to) {
+               radv_append_cs_bos_dedup(cs, handles, &num_handles, &present);
+            }
+         }
+
+         radv_bo_set_destroy(&present);
+      }
    }
 
    *rhandles = handles;
@@ -1121,7 +1596,8 @@ radv_amdgpu_winsys_cs_submit_internal(st
                                       struct ac_cmdbuf **cs_array, unsigned cs_count,
                                       struct ac_cmdbuf **initial_preamble_cs, unsigned initial_preamble_count,
                                       struct ac_cmdbuf **continue_preamble_cs, unsigned continue_preamble_count,
-                                      struct ac_cmdbuf **postamble_cs, unsigned postamble_count, bool uses_shadow_regs)
+                                      struct ac_cmdbuf **postamble_cs, unsigned postamble_count,
+                                      bool uses_shadow_regs)
 {
    VkResult result;
 
@@ -1151,7 +1627,7 @@ radv_amdgpu_winsys_cs_submit_internal(st
    struct radv_amdgpu_cs_request request = {
       .ip_type = last_cs->hw_ip,
       .ip_instance = 0,
-      .ring = queue_idx,
+      .ring = (uint32_t)queue_idx,
       .handles = handles,
       .num_handles = num_handles,
       .ibs = ibs,
@@ -1561,7 +2037,7 @@ radv_amdgpu_winsys_cs_dump(struct ac_cmd
 
             ac_parse_ib(&ib_parser, name);
          } else {
-            ibs[i] = mapped;
+            ibs[i] = (uint32_t *)mapped;
             ib_dw_sizes[i] = ib->cdw;
          }
       }
@@ -1774,7 +2250,7 @@ static void *
 radv_amdgpu_cs_alloc_syncobj_chunk(struct radv_winsys_sem_counts *counts, uint32_t queue_syncobj,
                                    struct drm_amdgpu_cs_chunk *chunk, int chunk_id)
 {
-   unsigned count = counts->syncobj_count + (queue_syncobj ? 1 : 0);
+   unsigned count = counts->syncobj_count + (queue_syncobj ? 1u : 0u);
    struct drm_amdgpu_cs_chunk_sem *syncobj = malloc(sizeof(struct drm_amdgpu_cs_chunk_sem) * count);
    if (!syncobj)
       return NULL;
@@ -1788,7 +2264,7 @@ radv_amdgpu_cs_alloc_syncobj_chunk(struc
       syncobj[counts->syncobj_count].handle = queue_syncobj;
 
    chunk->chunk_id = chunk_id;
-   chunk->length_dw = sizeof(struct drm_amdgpu_cs_chunk_sem) / 4 * count;
+   chunk->length_dw = (uint32_t)(sizeof(struct drm_amdgpu_cs_chunk_sem) / 4 * count);
    chunk->chunk_data = (uint64_t)(uintptr_t)syncobj;
    return syncobj;
 }
@@ -1797,7 +2273,7 @@ static void *
 radv_amdgpu_cs_alloc_timeline_syncobj_chunk(struct radv_winsys_sem_counts *counts, uint32_t queue_syncobj,
                                             struct drm_amdgpu_cs_chunk *chunk, int chunk_id)
 {
-   uint32_t count = counts->syncobj_count + counts->timeline_syncobj_count + (queue_syncobj ? 1 : 0);
+   uint32_t count = counts->syncobj_count + counts->timeline_syncobj_count + (queue_syncobj ? 1u : 0u);
    struct drm_amdgpu_cs_chunk_syncobj *syncobj = malloc(sizeof(struct drm_amdgpu_cs_chunk_syncobj) * count);
    if (!syncobj)
       return NULL;
@@ -1823,7 +2299,7 @@ radv_amdgpu_cs_alloc_timeline_syncobj_ch
    }
 
    chunk->chunk_id = chunk_id;
-   chunk->length_dw = sizeof(struct drm_amdgpu_cs_chunk_syncobj) / 4 * count;
+   chunk->length_dw = (uint32_t)(sizeof(struct drm_amdgpu_cs_chunk_syncobj) / 4 * count);
    chunk->chunk_data = (uint64_t)(uintptr_t)syncobj;
    return syncobj;
 }
@@ -1843,11 +2319,8 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
    int r;
    int num_chunks;
    int size;
-   struct drm_amdgpu_cs_chunk *chunks;
-   struct drm_amdgpu_cs_chunk_data *chunk_data;
    struct drm_amdgpu_bo_list_in bo_list_in;
    void *wait_syncobj = NULL, *signal_syncobj = NULL;
-   int i;
    VkResult result = VK_SUCCESS;
    bool has_user_fence = radv_amdgpu_cs_has_user_fence(request);
    uint32_t queue_syncobj = radv_amdgpu_ctx_queue_syncobj(ctx, request->ip_type, request->ring);
@@ -1858,20 +2331,18 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
 
    size = request->number_of_ibs + 1 + (has_user_fence ? 1 : 0) + 1 /* bo list */ + 3;
 
-   chunks = malloc(sizeof(chunks[0]) * size);
-   if (!chunks)
-      return VK_ERROR_OUT_OF_HOST_MEMORY;
+   STACK_ARRAY(struct drm_amdgpu_cs_chunk, chunks, size);
 
    size = request->number_of_ibs + (has_user_fence ? 1 : 0);
+   STACK_ARRAY(struct drm_amdgpu_cs_chunk_data, chunk_data, size);
 
-   chunk_data = malloc(sizeof(chunk_data[0]) * size);
-   if (!chunk_data) {
+   if (!chunks || !chunk_data) {
       result = VK_ERROR_OUT_OF_HOST_MEMORY;
-      goto error_out;
+      goto out_finish;
    }
 
    num_chunks = request->number_of_ibs;
-   for (i = 0; i < request->number_of_ibs; i++) {
+   for (int i = 0; i < (int)request->number_of_ibs; i++) {
       struct radv_amdgpu_cs_ib_info *ib;
       chunks[i].chunk_id = AMDGPU_CHUNK_ID_IB;
       chunks[i].length_dw = sizeof(struct drm_amdgpu_cs_chunk_ib) / 4;
@@ -1883,7 +2354,7 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
 
       chunk_data[i].ib_data._pad = 0;
       chunk_data[i].ib_data.va_start = ib->ib_mc_address;
-      chunk_data[i].ib_data.ib_bytes = ib->size * 4;
+      chunk_data[i].ib_data.ib_bytes = ib->size * 4u;
       chunk_data[i].ib_data.ip_type = ib->ip_type;
       chunk_data[i].ib_data.ip_instance = request->ip_instance;
       chunk_data[i].ib_data.ring = request->ring;
@@ -1893,7 +2364,7 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
    assert(chunk_data[request->number_of_ibs - 1].ib_data.ip_type == request->ip_type);
 
    if (has_user_fence) {
-      i = num_chunks++;
+      int i = num_chunks++;
       chunks[i].chunk_id = AMDGPU_CHUNK_ID_FENCE;
       chunks[i].length_dw = sizeof(struct drm_amdgpu_cs_chunk_fence) / 4;
       chunks[i].chunk_data = (uint64_t)(uintptr_t)&chunk_data[i];
@@ -1904,7 +2375,7 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
        *   QWORD[2]: reset fence
        *   QWORD[3]: preempted then reset
        */
-      uint32_t offset = (request->ip_type * MAX_RINGS_PER_TYPE + request->ring) * 4;
+      uint32_t offset = (request->ip_type * MAX_RINGS_PER_TYPE + request->ring) * 4u;
       ac_drm_cs_chunk_fence_info_to_data(radv_amdgpu_winsys_bo(ctx->fence_bo)->bo_handle, offset, &chunk_data[i]);
    }
 
@@ -1920,7 +2391,7 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
       }
       if (!wait_syncobj) {
          result = VK_ERROR_OUT_OF_HOST_MEMORY;
-         goto error_out;
+         goto out_finish;
       }
       num_chunks++;
 
@@ -1938,13 +2409,13 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
       }
       if (!signal_syncobj) {
          result = VK_ERROR_OUT_OF_HOST_MEMORY;
-         goto error_out;
+         goto out_finish;
       }
       num_chunks++;
    }
 
-   bo_list_in.operation = ~0;
-   bo_list_in.list_handle = ~0;
+   bo_list_in.operation = ~0u;
+   bo_list_in.list_handle = ~0u;
    bo_list_in.bo_number = request->num_handles;
    bo_list_in.bo_info_size = sizeof(struct drm_amdgpu_bo_list_entry);
    bo_list_in.bo_info_ptr = (uint64_t)(uintptr_t)request->handles;
@@ -1994,11 +2465,11 @@ radv_amdgpu_cs_submit(struct radv_amdgpu
       }
    }
 
-error_out:
-   free(chunks);
-   free(chunk_data);
+out_finish:
    free(wait_syncobj);
    free(signal_syncobj);
+   STACK_ARRAY_FINISH(chunks);
+   STACK_ARRAY_FINISH(chunk_data);
    return result;
 }
 
