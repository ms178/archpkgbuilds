From bff2121151dba5d6bd9c2cd8aee5b05fa429008b Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Fri, 17 Feb 2023 20:14:00 +0100
Subject: [PATCH 01/10] util: Add util_format_get_array.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

This is a poor man's version of MESA_ARRAY_FORMAT.
Implemented based on a gigantic switch-case
with some help from the C preprocessor.
Thank you, preprocessor!

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
---
 src/util/format/u_format.c | 62 ++++++++++++++++++++++++++++++++++++++
 src/util/format/u_format.h |  5 +++
 2 files changed, 67 insertions(+)

diff --git a/src/util/format/u_format.c b/src/util/format/u_format.c
index a1edafe2d102..3cd97076e65e 100644
--- a/src/util/format/u_format.c
+++ b/src/util/format/u_format.c
@@ -1367,3 +1367,65 @@ util_format_rgbx_to_rgba(enum pipe_format format)
    }
    }
 }
+
+enum pipe_format
+util_format_get_array(const enum util_format_type type, const unsigned bits,
+                      const unsigned nr_components, const bool normalized)
+{
+#define CASE_BY_BIT_SWITCH_BY_NR_COMPONENTS_1TO4(TYPE, BITS, NR_VAR) \
+   case BITS: \
+      switch (NR_VAR) { \
+      case 1: \
+         return PIPE_FORMAT_R##BITS##_##TYPE; \
+      case 2: \
+         return PIPE_FORMAT_R##BITS##G##BITS##_##TYPE; \
+      case 3: \
+         return PIPE_FORMAT_R##BITS##G##BITS##B##BITS##_##TYPE; \
+      case 4: \
+         return PIPE_FORMAT_R##BITS##G##BITS##B##BITS##A##BITS##_##TYPE; \
+      default: \
+         return PIPE_FORMAT_NONE; \
+      }
+
+#define SWITCH_BY_BITS_CASEX3(TYPE, BITS_VAR, BITS1, BITS2, BITS3, NR_VAR) \
+   switch (BITS_VAR) { \
+   CASE_BY_BIT_SWITCH_BY_NR_COMPONENTS_1TO4(TYPE, BITS1, NR_VAR) \
+   CASE_BY_BIT_SWITCH_BY_NR_COMPONENTS_1TO4(TYPE, BITS2, NR_VAR) \
+   CASE_BY_BIT_SWITCH_BY_NR_COMPONENTS_1TO4(TYPE, BITS3, NR_VAR) \
+   default: \
+      return PIPE_FORMAT_NONE; \
+   }
+
+#define SWITCH_BY_BITS_CASEX4(TYPE, BITS_VAR, BITS1, BITS2, BITS3, BITS4, NR_VAR) \
+   switch (BITS_VAR) { \
+   CASE_BY_BIT_SWITCH_BY_NR_COMPONENTS_1TO4(TYPE, BITS1, NR_VAR) \
+   CASE_BY_BIT_SWITCH_BY_NR_COMPONENTS_1TO4(TYPE, BITS2, NR_VAR) \
+   CASE_BY_BIT_SWITCH_BY_NR_COMPONENTS_1TO4(TYPE, BITS3, NR_VAR) \
+   CASE_BY_BIT_SWITCH_BY_NR_COMPONENTS_1TO4(TYPE, BITS4, NR_VAR) \
+   default: \
+      return PIPE_FORMAT_NONE; \
+   }
+
+   switch (type) {
+   case UTIL_FORMAT_TYPE_UNSIGNED:
+      if (normalized)
+         SWITCH_BY_BITS_CASEX3(UNORM, bits, 8, 16, 32, nr_components)
+      else
+         SWITCH_BY_BITS_CASEX4(UINT, bits, 8, 16, 32, 64, nr_components)
+   case UTIL_FORMAT_TYPE_SIGNED:
+      if (normalized)
+         SWITCH_BY_BITS_CASEX3(SNORM, bits, 8, 16, 32, nr_components)
+      else
+         SWITCH_BY_BITS_CASEX4(SINT, bits, 8, 16, 32, 64, nr_components)
+   case UTIL_FORMAT_TYPE_FLOAT:
+      SWITCH_BY_BITS_CASEX3(FLOAT, bits, 16, 32, 64, nr_components)
+   default:
+      return PIPE_FORMAT_NONE;
+   }
+
+#undef CASE_BY_BIT_SWITCH_BY_NR_COMPONENTS_1TO4
+#undef SWITCH_BY_BITS_CASEX3
+#undef SWITCH_BY_BITS_CASEX4
+
+   return PIPE_FORMAT_NONE;
+}
diff --git a/src/util/format/u_format.h b/src/util/format/u_format.h
index 035a7e0bba80..b45b628f6df0 100644
--- a/src/util/format/u_format.h
+++ b/src/util/format/u_format.h
@@ -1706,6 +1706,11 @@ util_format_snorm_to_unorm(enum pipe_format format);
 enum pipe_format
 util_format_rgbx_to_rgba(enum pipe_format format);
 
+/* Returns the pipe format for the given array type, bitsize and component count. */
+enum pipe_format
+util_format_get_array(const enum util_format_type type, const unsigned bits,
+                      const unsigned nr_components, const bool normalized);
+
 #ifdef __cplusplus
 } // extern "C" {
 #endif
-- 
GitLab


From ff54652284c9de7ced2f89e8bfcce08088bc6039 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Thu, 2 Feb 2023 10:47:58 +0100
Subject: [PATCH 02/10] nir: Add load_typed_buffer_amd intrinsic.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

This new intrinsic maps to the MTBUF instruction format on AMD GPUs
and represents a typed buffer load in NIR.

Also add an unsigned upper bound for the new intrinsic.
Code for that ported from aco_instruction_selection_setup.

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
Reviewed-by: Konstantin Seurer <konstantin.seurer@gmail.com>
Reviewed-by: Marek Olšák <marek.olsak@amd.com>
---
 src/compiler/nir/nir_divergence_analysis.c |  1 +
 src/compiler/nir/nir_intrinsics.py         | 13 +++++++++++++
 src/compiler/nir/nir_range_analysis.c      | 19 +++++++++++++++++++
 3 files changed, 33 insertions(+)

diff --git a/src/compiler/nir/nir_divergence_analysis.c b/src/compiler/nir/nir_divergence_analysis.c
index 6b359e61b663..d943b284be59 100644
--- a/src/compiler/nir/nir_divergence_analysis.c
+++ b/src/compiler/nir/nir_divergence_analysis.c
@@ -406,6 +406,7 @@ visit_intrinsic(nir_shader *shader, nir_intrinsic_instr *instr)
    case nir_intrinsic_load_kernel_input:
    case nir_intrinsic_load_task_payload:
    case nir_intrinsic_load_buffer_amd:
+   case nir_intrinsic_load_typed_buffer_amd:
    case nir_intrinsic_image_samples:
    case nir_intrinsic_image_deref_samples:
    case nir_intrinsic_bindless_image_samples:
diff --git a/src/compiler/nir/nir_intrinsics.py b/src/compiler/nir/nir_intrinsics.py
index 25cafc872532..cb461fba8067 100644
--- a/src/compiler/nir/nir_intrinsics.py
+++ b/src/compiler/nir/nir_intrinsics.py
@@ -182,6 +182,7 @@ index("enum glsl_sampler_dim", "image_dim")
 index("bool", "image_array")
 
 # Image format for image intrinsics
+# Vertex buffer format for load_typed_buffer_amd
 index("enum pipe_format", "format")
 
 # Access qualifiers for image and memory access intrinsics. ACCESS_RESTRICT is
@@ -1329,6 +1330,18 @@ intrinsic("load_buffer_amd", src_comp=[4, 1, 1, 1], dest_comp=0, indices=[BASE,
 # src[] = { store value, descriptor, vector byte offset, scalar byte offset, index offset }
 intrinsic("store_buffer_amd", src_comp=[0, 4, 1, 1, 1], indices=[BASE, WRITE_MASK, MEMORY_MODES, ACCESS])
 
+# Typed buffer load of arbitrary length, using a specified format.
+# src[] = { descriptor, vector byte offset, scalar byte offset, index offset }
+#
+# The compiler backend is responsible for emitting correct HW instructions according to alignment, range etc.
+# Users of this intrinsic must ensure that the first component being loaded is really the first component
+# of the specified format, because range analysis assumes this.
+# The size of the specified format also determines the memory range that this instruction is allowed to access.
+#
+# The index offset is multiplied by the stride in the descriptor, if any.
+# The vector/scalar offsets are in bytes, BASE is a constant byte offset.
+intrinsic("load_typed_buffer_amd", src_comp=[4, 1, 1, 1], dest_comp=0, indices=[BASE, MEMORY_MODES, ACCESS, FORMAT, ALIGN_MUL, ALIGN_OFFSET], flags=[CAN_ELIMINATE])
+
 # src[] = { address, unsigned 32-bit offset }.
 load("global_amd", [1, 1], indices=[BASE, ACCESS, ALIGN_MUL, ALIGN_OFFSET], flags=[CAN_ELIMINATE])
 # src[] = { value, address, unsigned 32-bit offset }.
diff --git a/src/compiler/nir/nir_range_analysis.c b/src/compiler/nir/nir_range_analysis.c
index 9b102e89f6a8..0c853d07d582 100644
--- a/src/compiler/nir/nir_range_analysis.c
+++ b/src/compiler/nir/nir_range_analysis.c
@@ -25,6 +25,7 @@
 #include "nir.h"
 #include "nir_range_analysis.h"
 #include "util/hash_table.h"
+#include "util/u_math.h"
 
 /**
  * Analyzes a sequence of operations to determine some aspects of the range of
@@ -1469,6 +1470,24 @@ nir_unsigned_upper_bound_impl(nir_shader *shader, struct hash_table *range_ht,
          /* Very generous maximum: TCS/TES executed by largest possible workgroup */
          res = config->max_workgroup_invocations / MAX2(shader->info.tess.tcs_vertices_out, 1u);
          break;
+      case nir_intrinsic_load_typed_buffer_amd: {
+         const enum pipe_format format = nir_intrinsic_format(intrin);
+         if (format == PIPE_FORMAT_NONE)
+            break;
+
+         const struct util_format_description* desc = util_format_description(format);
+         if (desc->channel[scalar.comp].type != UTIL_FORMAT_TYPE_UNSIGNED)
+            break;
+
+         if (desc->channel[scalar.comp].normalized) {
+            res = fui(1.0);
+            break;
+         }
+
+         const uint32_t chan_max = u_uintN_max(desc->channel[scalar.comp].size);
+         res = desc->channel[scalar.comp].pure_integer ? chan_max : fui(chan_max);
+         break;
+      }
       case nir_intrinsic_load_scalar_arg_amd:
       case nir_intrinsic_load_vector_arg_amd: {
          uint32_t upper_bound = nir_intrinsic_arg_upper_bound_u32_amd(intrin);
-- 
GitLab


From 7ff5770ebd55e781849c95d7fa07cac09c732655 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Fri, 3 Feb 2023 01:03:22 +0100
Subject: [PATCH 03/10] aco: Implement load_typed_buffer_amd.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
---
 .../compiler/aco_instruction_selection.cpp    | 157 ++++++++++++++++--
 .../aco_instruction_selection_setup.cpp       |   1 +
 2 files changed, 145 insertions(+), 13 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 9ea40736d178..759ad24658c2 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -4028,9 +4028,11 @@ struct LoadEmitInfo {
    unsigned const_offset = 0;
    unsigned align_mul = 0;
    unsigned align_offset = 0;
+   pipe_format format;
 
    bool glc = false;
    bool slc = false;
+   bool split_by_component_stride = true;
    unsigned swizzle_component_size = 0;
    memory_sync_info sync;
    Temp soffset = Temp(0, s1);
@@ -4088,10 +4090,12 @@ emit_load(isel_context* ctx, Builder& bld, const LoadEmitInfo& info,
          }
       }
 
-      if (info.swizzle_component_size)
-         bytes_needed = MIN2(bytes_needed, info.swizzle_component_size);
-      if (info.component_stride)
-         bytes_needed = MIN2(bytes_needed, info.component_size);
+      if (info.split_by_component_stride) {
+         if (info.swizzle_component_size)
+            bytes_needed = MIN2(bytes_needed, info.swizzle_component_size);
+         if (info.component_stride)
+            bytes_needed = MIN2(bytes_needed, info.component_size);
+      }
 
       bool need_to_align_offset = byte_align && (align_mul % 4 || align_offset % 4);
 
@@ -4198,9 +4202,11 @@ emit_load(isel_context* ctx, Builder& bld, const LoadEmitInfo& info,
 
       /* add result to list and advance */
       if (info.component_stride) {
-         assert(val.bytes() == info.component_size && "unimplemented");
-         const_offset += info.component_stride;
-         align_offset = (align_offset + info.component_stride) % align_mul;
+         assert(val.bytes() % info.component_size == 0);
+         unsigned num_loaded_components = val.bytes() / info.component_size;
+         unsigned advance_bytes = info.component_stride * num_loaded_components;
+         const_offset += advance_bytes;
+         align_offset = (align_offset + advance_bytes) % align_mul;
       } else {
          const_offset += val.bytes();
          align_offset = (align_offset + val.bytes()) % align_mul;
@@ -5492,6 +5498,104 @@ visit_load_interpolated_input(isel_context* ctx, nir_intrinsic_instr* instr)
    }
 }
 
+Temp
+mtbuf_load_callback(Builder& bld, const LoadEmitInfo& info, Temp offset, unsigned bytes_needed,
+                    unsigned alignment, unsigned const_offset, Temp dst_hint)
+{
+   Operand vaddr = offset.type() == RegType::vgpr ? Operand(offset) : Operand(v1);
+   Operand soffset = offset.type() == RegType::sgpr ? Operand(offset) : Operand::c32(0);
+
+   if (info.soffset.id()) {
+      if (soffset.isTemp())
+         vaddr = bld.copy(bld.def(v1), soffset);
+      soffset = Operand(info.soffset);
+   }
+
+   const bool offen = !vaddr.isUndefined();
+   const bool idxen = info.idx.id();
+
+   if (offen && idxen)
+      vaddr = bld.pseudo(aco_opcode::p_create_vector, bld.def(v2), info.idx, vaddr);
+   else if (idxen)
+      vaddr = Operand(info.idx);
+
+   /* Determine number of fetched components.
+    * Note, ACO IR works with GFX6-8 nfmt + dfmt fields, these are later converted for GFX10+.
+    */
+   const struct ac_vtx_format_info* vtx_info =
+      ac_get_vtx_format_info(GFX8, CHIP_POLARIS10, info.format);
+   /* The number of channels in the format determines the memory range. */
+   const unsigned max_components = vtx_info->num_channels;
+   /* Calculate number of components loaded, which may be lower than the needed bytes. */
+   unsigned num_fetched_components = bytes_needed / info.component_size;
+   num_fetched_components =
+      ac_get_safe_fetch_size(bld.program->gfx_level, vtx_info, const_offset, max_components,
+                             alignment, num_fetched_components);
+   const unsigned fetch_fmt = vtx_info->hw_format[num_fetched_components - 1];
+   /* Adjust bytes needed. */
+   bytes_needed = num_fetched_components * info.component_size;
+   unsigned bytes_size = 0;
+   const unsigned bit_size = info.component_size * 8;
+   aco_opcode op = aco_opcode::num_opcodes;
+
+   if (bytes_needed == 2) {
+      bytes_size = 2;
+      op = aco_opcode::tbuffer_load_format_d16_x;
+   } else if (bytes_needed <= 4) {
+      bytes_size = 4;
+      if (bit_size == 16)
+         op = aco_opcode::tbuffer_load_format_d16_xy;
+      else
+         op = aco_opcode::tbuffer_load_format_x;
+   } else if (bytes_needed <= 6) {
+      bytes_size = 6;
+      if (bit_size == 16)
+         op = aco_opcode::tbuffer_load_format_d16_xyz;
+      else
+         op = aco_opcode::tbuffer_load_format_xy;
+   } else if (bytes_needed <= 8) {
+      bytes_size = 8;
+      if (bit_size == 16)
+         op = aco_opcode::tbuffer_load_format_d16_xyzw;
+      else
+         op = aco_opcode::tbuffer_load_format_xy;
+   } else if (bytes_needed <= 12) {
+      bytes_size = 12;
+      op = aco_opcode::tbuffer_load_format_xyz;
+   } else {
+      bytes_size = 16;
+      op = aco_opcode::tbuffer_load_format_xyzw;
+   }
+
+   /* Abort when suitable opcode wasn't found so we don't compile buggy shaders. */
+   if (op == aco_opcode::num_opcodes) {
+      aco_err(bld.program, "unsupported bit size for typed buffer load");
+      abort();
+   }
+
+   aco_ptr<MTBUF_instruction> mtbuf{create_instruction<MTBUF_instruction>(op, Format::MTBUF, 3, 1)};
+   mtbuf->operands[0] = Operand(info.resource);
+   mtbuf->operands[1] = vaddr;
+   mtbuf->operands[2] = soffset;
+   mtbuf->offen = offen;
+   mtbuf->idxen = idxen;
+   mtbuf->glc = info.glc;
+   mtbuf->dlc = info.glc && (bld.program->gfx_level == GFX10 || bld.program->gfx_level == GFX10_3);
+   mtbuf->slc = info.slc;
+   mtbuf->sync = info.sync;
+   mtbuf->offset = const_offset;
+   mtbuf->dfmt = fetch_fmt & 0xf;
+   mtbuf->nfmt = fetch_fmt >> 4;
+   RegClass rc = RegClass::get(RegType::vgpr, bytes_size);
+   Temp val = dst_hint.id() && rc == dst_hint.regClass() ? dst_hint : bld.tmp(rc);
+   mtbuf->definitions[0] = Definition(val);
+   bld.insert(std::move(mtbuf));
+
+   return val;
+}
+
+const EmitLoadParameters mtbuf_load_params{mtbuf_load_callback, false, true, 4096};
+
 void
 visit_load_input(isel_context* ctx, nir_intrinsic_instr* instr)
 {
@@ -7180,24 +7284,50 @@ visit_load_buffer(isel_context* ctx, nir_intrinsic_instr* intrin)
    unsigned const_offset = nir_intrinsic_base(intrin);
    unsigned elem_size_bytes = intrin->dest.ssa.bit_size / 8u;
    unsigned num_components = intrin->dest.ssa.num_components;
-   unsigned swizzle_element_size = swizzled ? (ctx->program->gfx_level <= GFX8 ? 4 : 16) : 0;
 
    nir_variable_mode mem_mode = nir_intrinsic_memory_modes(intrin);
    memory_sync_info sync(aco_storage_mode_from_nir_mem_mode(mem_mode));
 
    LoadEmitInfo info = {Operand(v_offset), dst, num_components, elem_size_bytes, descriptor};
    info.idx = idx;
-   info.component_stride = swizzle_element_size;
    info.glc = glc;
    info.slc = slc;
-   info.swizzle_component_size = swizzle_element_size ? 4 : 0;
-   info.align_mul = MIN2(elem_size_bytes, 4);
-   info.align_offset = 0;
    info.soffset = s_offset;
    info.const_offset = const_offset;
    info.sync = sync;
 
-   emit_load(ctx, bld, info, mubuf_load_params);
+   if (intrin->intrinsic == nir_intrinsic_load_typed_buffer_amd) {
+      const pipe_format format = nir_intrinsic_format(intrin);
+      const struct ac_vtx_format_info* vtx_info =
+         ac_get_vtx_format_info(ctx->program->gfx_level, ctx->program->family, format);
+      const struct util_format_description* f = util_format_description(format);
+      const unsigned align_mul = nir_intrinsic_align_mul(intrin);
+      const unsigned align_offset = nir_intrinsic_align_offset(intrin);
+
+      /* Avoid splitting:
+       * - non-array formats because that would result in incorrect code
+       * - when element size is same as component size (to reduce instruction count)
+       */
+      const bool can_split = f->is_array && elem_size_bytes != vtx_info->chan_byte_size;
+
+      info.align_mul = align_mul;
+      info.align_offset = align_offset;
+      info.format = format;
+      info.component_stride = can_split ? vtx_info->chan_byte_size : 0;
+      info.split_by_component_stride = false;
+
+      emit_load(ctx, bld, info, mtbuf_load_params);
+   } else {
+      const unsigned swizzle_element_size =
+         swizzled ? (ctx->program->gfx_level <= GFX8 ? 4 : 16) : 0;
+
+      info.component_stride = swizzle_element_size;
+      info.swizzle_component_size = swizzle_element_size ? 4 : 0;
+      info.align_mul = MIN2(elem_size_bytes, 4);
+      info.align_offset = 0;
+
+      emit_load(ctx, bld, info, mubuf_load_params);
+   }
 }
 
 void
@@ -8253,6 +8383,7 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
    case nir_intrinsic_bindless_image_atomic_fmax: visit_image_atomic(ctx, instr); break;
    case nir_intrinsic_load_ssbo: visit_load_ssbo(ctx, instr); break;
    case nir_intrinsic_store_ssbo: visit_store_ssbo(ctx, instr); break;
+   case nir_intrinsic_load_typed_buffer_amd:
    case nir_intrinsic_load_buffer_amd: visit_load_buffer(ctx, instr); break;
    case nir_intrinsic_store_buffer_amd: visit_store_buffer(ctx, instr); break;
    case nir_intrinsic_load_smem_amd: visit_load_smem(ctx, instr); break;
diff --git a/src/amd/compiler/aco_instruction_selection_setup.cpp b/src/amd/compiler/aco_instruction_selection_setup.cpp
index d43c5966ed53..439f807dfff4 100644
--- a/src/amd/compiler/aco_instruction_selection_setup.cpp
+++ b/src/amd/compiler/aco_instruction_selection_setup.cpp
@@ -677,6 +677,7 @@ init_context(isel_context* ctx, nir_shader* shader)
                case nir_intrinsic_load_scratch:
                case nir_intrinsic_load_invocation_id:
                case nir_intrinsic_load_primitive_id:
+               case nir_intrinsic_load_typed_buffer_amd:
                case nir_intrinsic_load_buffer_amd:
                case nir_intrinsic_load_initial_edgeflags_amd:
                case nir_intrinsic_gds_atomic_add_amd:
-- 
GitLab


From ac667b72eafb332d1117949f6748f168b87b89a6 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Fri, 10 Feb 2023 23:44:05 +0100
Subject: [PATCH 04/10] ac/llvm: Implement typed buffer load intrinsic.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
Reviewed-by: Qiang Yu <yuq825@gmail.com>
Acked-by: Konstantin Seurer <konstantin.seurer@gmail.com>
---
 src/amd/llvm/ac_llvm_build.c  | 42 +++++++++++++++++++++++++++++++++++
 src/amd/llvm/ac_llvm_build.h  | 11 +++++++++
 src/amd/llvm/ac_nir_to_llvm.c | 21 +++++++++++++++---
 3 files changed, 71 insertions(+), 3 deletions(-)

diff --git a/src/amd/llvm/ac_llvm_build.c b/src/amd/llvm/ac_llvm_build.c
index 179a1d5136f5..e620ae28cdcf 100644
--- a/src/amd/llvm/ac_llvm_build.c
+++ b/src/amd/llvm/ac_llvm_build.c
@@ -1472,6 +1472,48 @@ LLVMValueRef ac_build_struct_tbuffer_load(struct ac_llvm_context *ctx, LLVMValue
                                 ctx->i32, cache_policy, can_speculate);
 }
 
+LLVMValueRef ac_build_safe_tbuffer_load(struct ac_llvm_context *ctx, LLVMValueRef rsrc,
+                                        LLVMValueRef vidx, LLVMValueRef base_voffset,
+                                        LLVMValueRef soffset, LLVMTypeRef channel_type,
+                                        const struct ac_vtx_format_info *vtx_info,
+                                        unsigned const_offset,
+                                        unsigned align_offset,
+                                        unsigned align_mul,
+                                        unsigned num_channels,
+                                        unsigned cache_policy,
+                                        bool can_speculate)
+{
+   const unsigned max_channels = vtx_info->num_channels;
+   LLVMValueRef voffset_plus_const =
+      LLVMBuildAdd(ctx->builder, base_voffset, LLVMConstInt(ctx->i32, const_offset, 0), "");
+
+   /* Split the specified load into several MTBUF instructions,
+    * according to a safe fetch size determined by aligmnent information.
+    */
+   LLVMValueRef result = NULL;
+   for (unsigned i = 0, fetch_num_channels; i < num_channels; i += fetch_num_channels) {
+      const unsigned fetch_const_offset = const_offset + i * vtx_info->chan_byte_size;
+      const unsigned fetch_align_offset = (align_offset + i * vtx_info->chan_byte_size) % align_mul;
+      const unsigned fetch_alignment = fetch_align_offset ? 1 << (ffs(fetch_align_offset) - 1) : align_mul;
+
+      fetch_num_channels =
+         ac_get_safe_fetch_size(ctx->gfx_level, vtx_info, fetch_const_offset,
+                                max_channels - i, fetch_alignment, num_channels - i);
+      const unsigned fetch_format = vtx_info->hw_format[fetch_num_channels - 1];
+      LLVMValueRef fetch_voffset =
+            LLVMBuildAdd(ctx->builder, voffset_plus_const,
+                         LLVMConstInt(ctx->i32, i * vtx_info->chan_byte_size, 0), "");
+      LLVMValueRef item =
+         ac_build_tbuffer_load(ctx, rsrc, vidx, fetch_voffset, soffset,
+                               fetch_num_channels, fetch_format, channel_type,
+                               cache_policy, can_speculate);
+      result = ac_build_concat(ctx, result, item);
+   }
+
+   return result;
+}
+
+
 LLVMValueRef ac_build_buffer_load_short(struct ac_llvm_context *ctx, LLVMValueRef rsrc,
                                         LLVMValueRef voffset, LLVMValueRef soffset,
                                         unsigned cache_policy)
diff --git a/src/amd/llvm/ac_llvm_build.h b/src/amd/llvm/ac_llvm_build.h
index 49d89d60f095..accdc636de59 100644
--- a/src/amd/llvm/ac_llvm_build.h
+++ b/src/amd/llvm/ac_llvm_build.h
@@ -309,6 +309,17 @@ LLVMValueRef ac_build_struct_tbuffer_load(struct ac_llvm_context *ctx, LLVMValue
                                           unsigned dfmt, unsigned nfmt, unsigned cache_policy,
                                           bool can_speculate);
 
+LLVMValueRef ac_build_safe_tbuffer_load(struct ac_llvm_context *ctx, LLVMValueRef rsrc,
+                                        LLVMValueRef vindex, LLVMValueRef voffset,
+                                        LLVMValueRef soffset, LLVMTypeRef channel_type,
+                                        const struct ac_vtx_format_info *vtx_info,
+                                        unsigned const_offset,
+                                        unsigned align_offset,
+                                        unsigned align_mul,
+                                        unsigned num_channels,
+                                        unsigned cache_policy,
+                                        bool can_speculate);
+
 LLVMValueRef ac_build_opencoded_load_format(struct ac_llvm_context *ctx, unsigned log_size,
                                             unsigned num_channels, unsigned format, bool reverse,
                                             bool known_aligned, LLVMValueRef rsrc,
diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index 30e5dc4c6aa2..c360437570bd 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -4015,6 +4015,7 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
    case nir_intrinsic_set_vertex_and_primitive_count:
       /* Currently ignored. */
       break;
+   case nir_intrinsic_load_typed_buffer_amd:
    case nir_intrinsic_load_buffer_amd:
    case nir_intrinsic_store_buffer_amd: {
       unsigned src_base = instr->intrinsic == nir_intrinsic_store_buffer_amd ? 1 : 0;
@@ -4053,7 +4054,8 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
       } else if (instr->intrinsic == nir_intrinsic_store_buffer_amd && uses_format) {
          assert(instr->src[0].ssa->bit_size == 16 || instr->src[0].ssa->bit_size == 32);
          ac_build_buffer_store_format(&ctx->ac, descriptor, store_data, vidx, voffset, cache_policy);
-      } else if (instr->intrinsic == nir_intrinsic_load_buffer_amd) {
+      } else if (instr->intrinsic == nir_intrinsic_load_buffer_amd ||
+                 instr->intrinsic == nir_intrinsic_load_typed_buffer_amd) {
          /* LLVM is unable to select instructions for larger than 32-bit channel types.
           * Workaround by using i32 and casting to the correct type later.
           */
@@ -4062,8 +4064,21 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
          LLVMTypeRef channel_type =
             LLVMIntTypeInContext(ctx->ac.context, MIN2(32, instr->dest.ssa.bit_size));
 
-         result = ac_build_buffer_load(&ctx->ac, descriptor, fetch_num_components, vidx, voffset,
-                                       addr_soffset, channel_type, cache_policy, reorder, false);
+         if (instr->intrinsic == nir_intrinsic_load_buffer_amd) {
+            result = ac_build_buffer_load(&ctx->ac, descriptor, fetch_num_components, vidx, voffset,
+                                          addr_soffset, channel_type, cache_policy, reorder, false);
+         } else {
+            const unsigned align_offset = nir_intrinsic_align_offset(instr);
+            const unsigned align_mul = nir_intrinsic_align_mul(instr);
+            const enum pipe_format format = nir_intrinsic_format(instr);
+            const struct ac_vtx_format_info *vtx_info =
+               ac_get_vtx_format_info(ctx->ac.gfx_level, ctx->ac.family, format);
+
+            result =
+               ac_build_safe_tbuffer_load(&ctx->ac, descriptor, vidx, addr_voffset, addr_soffset,
+                                          channel_type, vtx_info, const_offset, align_offset,
+                                          align_mul, fetch_num_components, cache_policy, reorder);
+         }
 
          /* Trim to needed vector components. */
          result = ac_trim_vector(&ctx->ac, result, fetch_num_components);
-- 
GitLab


From ce2d5780faa16ded1371fd5bd7f84f16cfb6fee3 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Fri, 10 Feb 2023 08:37:06 +0100
Subject: [PATCH 05/10] radv: Lower non-dynamic VS inputs in NIR.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
Reviewed-by: Konstantin Seurer <konstantin.seurer@gmail.com>
---
 src/amd/vulkan/radv_nir_lower_vs_inputs.c | 199 +++++++++++++++++++++-
 1 file changed, 197 insertions(+), 2 deletions(-)

diff --git a/src/amd/vulkan/radv_nir_lower_vs_inputs.c b/src/amd/vulkan/radv_nir_lower_vs_inputs.c
index fbed239856b3..fb29b01a4a49 100644
--- a/src/amd/vulkan/radv_nir_lower_vs_inputs.c
+++ b/src/amd/vulkan/radv_nir_lower_vs_inputs.c
@@ -80,6 +80,202 @@ lower_load_vs_input_from_prolog(nir_builder *b, nir_intrinsic_instr *intrin,
    return extracted;
 }
 
+static nir_ssa_def *
+calc_vs_input_index_instance_rate(nir_builder *b, unsigned location, lower_vs_inputs_state *s)
+{
+   const uint32_t divisor = s->pl_key->vs.instance_rate_divisors[location];
+   nir_ssa_def *start_instance = nir_load_base_instance(b);
+
+   if (divisor == 0)
+      return start_instance;
+
+   nir_ssa_def *instance_id = nir_udiv_imm(b, nir_load_instance_id(b), divisor);
+   return nir_iadd(b, start_instance, instance_id);
+}
+
+static nir_ssa_def *
+calc_vs_input_index(nir_builder *b, unsigned location, lower_vs_inputs_state *s)
+{
+   if (s->pl_key->vs.instance_rate_inputs & BITFIELD_BIT(location))
+      return calc_vs_input_index_instance_rate(b, location, s);
+
+   return nir_iadd(b, nir_load_first_vertex(b), nir_load_vertex_id_zero_base(b));
+}
+
+static bool
+can_use_untyped_load(const struct util_format_description *f, const unsigned bit_size)
+{
+   /* All components must have same size and type. */
+   if (!f->is_array)
+      return false;
+
+   const struct util_format_channel_description *c = &f->channel[0];
+   return c->size == bit_size && bit_size >= 32;
+}
+
+static nir_ssa_def *
+oob_input_load_value(nir_builder *b, const unsigned channel_idx, const unsigned bit_size,
+                     const bool is_float)
+{
+   /* 22.1.1. Attribute Location and Component Assignment of Vulkan 1.3 specification:
+    * For 64-bit data types, no default attribute values are provided. Input variables
+    * must not use more components than provided by the attribute.
+    */
+   if (bit_size == 64)
+      return nir_ssa_undef(b, 1, bit_size);
+
+   if (channel_idx == 3) {
+      if (is_float)
+         return nir_imm_floatN_t(b, 1.0, bit_size);
+      else
+         return nir_imm_intN_t(b, 1, bit_size);
+   }
+
+   return nir_imm_intN_t(b, 0, bit_size);
+}
+
+static enum pipe_format
+format_skip_first_n_components(const struct util_format_description *desc, const unsigned n)
+{
+   if (!n)
+      return desc->format;
+
+   /* We can't shrink non-array formats. */
+   if (!desc->is_array || desc->is_mixed || n >= desc->nr_channels)
+      return PIPE_FORMAT_NONE;
+
+   return util_format_get_array(desc->channel[0].type, desc->channel[0].size, desc->nr_channels - n,
+                                desc->is_unorm || desc->is_snorm);
+}
+
+static nir_ssa_def *
+lower_load_vs_input(nir_builder *b, nir_intrinsic_instr *intrin, lower_vs_inputs_state *s)
+{
+   nir_src *offset_src = nir_get_io_offset_src(intrin);
+   assert(nir_src_is_const(*offset_src));
+
+   const unsigned base = nir_intrinsic_base(intrin);
+   const unsigned base_offset = nir_src_as_uint(*offset_src);
+   const unsigned location = base + base_offset - VERT_ATTRIB_GENERIC0;
+   const unsigned bit_size = intrin->dest.ssa.bit_size;
+   const unsigned num_components = intrin->dest.ssa.num_components;
+
+   /* Convert the component offset to bit_size units.
+    * (Intrinsic component offset is in 32-bit units.)
+    *
+    * Small bitsize inputs consume the same space as 32-bit inputs,
+    * but 64-bit inputs consume twice as many.
+    * 64-bit variables must not have a component of 1 or 3.
+    * (See VK spec 15.1.5 "Component Assignment")
+    */
+   const unsigned component = nir_intrinsic_component(intrin) * MAX2(32, bit_size) / 32;
+
+   /* Bitmask of components in bit_size units
+    * of the current input load that are actually used.
+    */
+   const unsigned mask = nir_ssa_def_components_read(&intrin->dest.ssa) << component;
+
+   /* If the input is entirely unused, just replace it with undef.
+    * This is just in case we debug this pass without running DCE first.
+    */
+   if (!mask)
+      return nir_ssa_undef(b, num_components, bit_size);
+
+   const uint32_t attrib_binding = s->pl_key->vs.vertex_attribute_bindings[location];
+   const uint32_t attrib_offset = s->pl_key->vs.vertex_attribute_offsets[location];
+   const uint32_t attrib_stride = s->pl_key->vs.vertex_attribute_strides[location];
+   const enum pipe_format attrib_format = s->pl_key->vs.vertex_attribute_formats[location];
+   const struct util_format_description *f = util_format_description(attrib_format);
+   const unsigned binding_index =
+      s->info->vs.use_per_attribute_vb_descs ? location : attrib_binding;
+   const unsigned desc_index =
+      util_bitcount(s->info->vs.vb_desc_usage_mask & u_bit_consecutive(0, binding_index));
+
+   nir_ssa_def *vertex_buffers_arg = ac_nir_load_arg(b, &s->args->ac, s->args->ac.vertex_buffers);
+   nir_ssa_def *vertex_buffers =
+      nir_pack_64_2x32_split(b, vertex_buffers_arg, nir_imm_int(b, s->address32_hi));
+   nir_ssa_def *descriptor =
+      nir_load_smem_amd(b, 4, vertex_buffers, nir_imm_int(b, desc_index * 16));
+   nir_ssa_def *index = calc_vs_input_index(b, location, s);
+   nir_ssa_def *zero = nir_imm_int(b, 0);
+
+   /* Try to shrink the load format by skipping unused components from the start.
+    * Beneficial because the backend may be able to emit fewer HW instructions.
+    */
+   const unsigned first_used_channel = ffs(mask) - 1;
+   const enum pipe_format shrinked_format = format_skip_first_n_components(f, first_used_channel);
+   const enum pipe_format fetch_format = shrinked_format ? shrinked_format : attrib_format;
+   const unsigned skipped_start = shrinked_format ? first_used_channel : 0;
+
+   /* We should be able to shrink all supported array formats. */
+   assert(!f->is_array || f->is_mixed || shrinked_format);
+
+   /* Number of channels we actually use and load.
+    * Don't shrink the format here because this might allow the backend to
+    * emit fewer (but larger than needed) HW instructions.
+    */
+   const unsigned first_trailing_unused_channel = util_last_bit(mask);
+   const unsigned max_loaded_channels = MIN2(first_trailing_unused_channel, f->nr_channels);
+
+   /* Add excess constant offset to the index. */
+   unsigned const_off = attrib_offset + f->channel[0].size * skipped_start / 8;
+   if (attrib_stride && const_off > attrib_stride) {
+      index = nir_iadd_imm(b, index, const_off / attrib_stride);
+      const_off %= attrib_stride;
+   }
+
+   /* Load VS inputs from VRAM.
+    * Prefer using untyped buffer loads if possible, to avoid potential alignment issues.
+    */
+   nir_ssa_def *load = NULL;
+   if (can_use_untyped_load(f, bit_size)) {
+      load = nir_load_buffer_amd(b, max_loaded_channels - skipped_start, bit_size, descriptor, zero,
+                                 zero, index, .base = const_off, .memory_modes = nir_var_shader_in);
+   } else {
+      const unsigned align_mul = MAX2(1, s->pl_key->vs.vertex_binding_align[attrib_binding]);
+      const unsigned align_offset = const_off % align_mul;
+
+      load = nir_load_typed_buffer_amd(b, max_loaded_channels - skipped_start, bit_size, descriptor,
+                                       zero, zero, index, .base = const_off, .format = fetch_format,
+                                       .align_mul = align_mul, .align_offset = align_offset,
+                                       .memory_modes = nir_var_shader_in);
+   }
+
+   /* When we couldn't skip starting components (eg. due to the format),
+    * extract the channels we actually need.
+    */
+   if (first_used_channel > skipped_start)
+      load = nir_extract_bits(b, &load, 1, (first_used_channel - skipped_start) * bit_size,
+                              max_loaded_channels - first_used_channel, bit_size);
+
+   /* Return early if possible to avoid generating unnecessary IR. */
+   if (first_used_channel == component && load->num_components == num_components)
+      return load;
+
+   /* Fill unused components at the beginning with undefs.
+    * Fill OOB components at the end with their respective values.
+    */
+   const nir_alu_type dst_type = nir_alu_type_get_base_type(nir_intrinsic_dest_type(intrin));
+   nir_ssa_def *channels[NIR_MAX_VEC_COMPONENTS] = {0};
+   for (unsigned i = 0; i < num_components; ++i) {
+      const unsigned c = i + component;
+      if (c < first_used_channel)
+         /* Fill unused channels at the start with undef. */
+         channels[i] = nir_ssa_undef(b, 1, bit_size);
+      else if (c < max_loaded_channels)
+         /* Use channels that were loaded from VRAM. */
+         channels[i] = nir_channel(b, load, c - first_used_channel);
+      else if (c < first_trailing_unused_channel)
+         /* Handle input loads that are larger than their format. */
+         channels[i] = oob_input_load_value(b, c, bit_size, dst_type == nir_type_float);
+      else
+         /* Fill unused channels at the end with undef. */
+         channels[i] = nir_ssa_undef(b, 1, bit_size);
+   }
+
+   return nir_vec(b, channels, num_components);
+}
+
 static bool
 lower_vs_input_instr(nir_builder *b, nir_instr *instr, void *state)
 {
@@ -99,8 +295,7 @@ lower_vs_input_instr(nir_builder *b, nir_instr *instr, void *state)
    if (s->info->vs.dynamic_inputs) {
       replacement = lower_load_vs_input_from_prolog(b, intrin, s);
    } else {
-      /* TODO: lower non-dynamic inputs */
-      return false;
+      replacement = lower_load_vs_input(b, intrin, s);
    }
 
    nir_ssa_def_rewrite_uses(&intrin->dest.ssa, replacement);
-- 
GitLab


From ec4f26f128224e322db8993e16e2cfa99517bc1e Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Thu, 2 Feb 2023 17:55:06 +0100
Subject: [PATCH 06/10] aco: Remove VS inputs from visit_load_input.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
Acked-by: Konstantin Seurer <konstantin.seurer@gmail.com>
---
 .../compiler/aco_instruction_selection.cpp    | 267 +-----------------
 1 file changed, 1 insertion(+), 266 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 759ad24658c2..e6dd0be4359c 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -293,58 +293,6 @@ as_vgpr(isel_context* ctx, Temp val)
    return as_vgpr(bld, val);
 }
 
-// assumes a != 0xffffffff
-void
-emit_v_div_u32(isel_context* ctx, Temp dst, Temp a, uint32_t b)
-{
-   assert(b != 0);
-   Builder bld(ctx->program, ctx->block);
-
-   if (util_is_power_of_two_or_zero(b)) {
-      bld.vop2(aco_opcode::v_lshrrev_b32, Definition(dst), Operand::c32(util_logbase2(b)), a);
-      return;
-   }
-
-   util_fast_udiv_info info = util_compute_fast_udiv_info(b, 32, 32);
-
-   assert(info.multiplier <= 0xffffffff);
-
-   bool pre_shift = info.pre_shift != 0;
-   bool increment = info.increment != 0;
-   bool multiply = true;
-   bool post_shift = info.post_shift != 0;
-
-   if (!pre_shift && !increment && !multiply && !post_shift) {
-      bld.copy(Definition(dst), a);
-      return;
-   }
-
-   Temp pre_shift_dst = a;
-   if (pre_shift) {
-      pre_shift_dst = (increment || multiply || post_shift) ? bld.tmp(v1) : dst;
-      bld.vop2(aco_opcode::v_lshrrev_b32, Definition(pre_shift_dst), Operand::c32(info.pre_shift),
-               a);
-   }
-
-   Temp increment_dst = pre_shift_dst;
-   if (increment) {
-      increment_dst = (post_shift || multiply) ? bld.tmp(v1) : dst;
-      bld.vadd32(Definition(increment_dst), Operand::c32(info.increment), pre_shift_dst);
-   }
-
-   Temp multiply_dst = increment_dst;
-   if (multiply) {
-      multiply_dst = post_shift ? bld.tmp(v1) : dst;
-      bld.vop3(aco_opcode::v_mul_hi_u32, Definition(multiply_dst), increment_dst,
-               bld.copy(bld.def(v1), Operand::c32(info.multiplier)));
-   }
-
-   if (post_shift) {
-      bld.vop2(aco_opcode::v_lshrrev_b32, Definition(dst), Operand::c32(info.post_shift),
-               multiply_dst);
-   }
-}
-
 void
 emit_extract_vector(isel_context* ctx, Temp src, uint32_t idx, Temp dst)
 {
@@ -5603,220 +5551,7 @@ visit_load_input(isel_context* ctx, nir_intrinsic_instr* instr)
    Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
    nir_src offset = *nir_get_io_offset_src(instr);
 
-   if (ctx->shader->info.stage == MESA_SHADER_VERTEX) {
-      if (!nir_src_is_const(offset) || nir_src_as_uint(offset))
-         isel_err(offset.ssa->parent_instr,
-                  "Unimplemented non-zero nir_intrinsic_load_input offset");
-
-      Temp vertex_buffers =
-         convert_pointer_to_64_bit(ctx, get_arg(ctx, ctx->args->ac.vertex_buffers));
-
-      unsigned location = nir_intrinsic_base(instr) - VERT_ATTRIB_GENERIC0;
-      unsigned bitsize = instr->dest.ssa.bit_size;
-      unsigned component = nir_intrinsic_component(instr) >> (bitsize == 64 ? 1 : 0);
-      unsigned attrib_binding = ctx->options->key.vs.vertex_attribute_bindings[location];
-      uint32_t attrib_offset = ctx->options->key.vs.vertex_attribute_offsets[location];
-      uint32_t attrib_stride = ctx->options->key.vs.vertex_attribute_strides[location];
-      enum pipe_format attrib_format =
-         (enum pipe_format)ctx->options->key.vs.vertex_attribute_formats[location];
-      unsigned binding_align = ctx->options->key.vs.vertex_binding_align[attrib_binding];
-
-      const struct ac_vtx_format_info* vtx_info =
-         ac_get_vtx_format_info(GFX8, CHIP_POLARIS10, attrib_format);
-
-      unsigned mask = nir_ssa_def_components_read(&instr->dest.ssa) << component;
-      unsigned num_channels = MIN2(util_last_bit(mask), vtx_info->num_channels);
-
-      unsigned desc_index =
-         ctx->program->info.vs.use_per_attribute_vb_descs ? location : attrib_binding;
-      desc_index = util_bitcount(ctx->program->info.vs.vb_desc_usage_mask &
-                                 u_bit_consecutive(0, desc_index));
-      Operand off = bld.copy(bld.def(s1), Operand::c32(desc_index * 16u));
-      Temp list = bld.smem(aco_opcode::s_load_dwordx4, bld.def(s4), vertex_buffers, off);
-
-      Temp index;
-      if (ctx->options->key.vs.instance_rate_inputs & (1u << location)) {
-         uint32_t divisor = ctx->options->key.vs.instance_rate_divisors[location];
-         Temp start_instance = get_arg(ctx, ctx->args->ac.start_instance);
-         if (divisor) {
-            Temp instance_id = get_arg(ctx, ctx->args->ac.instance_id);
-            if (divisor != 1) {
-               Temp divided = bld.tmp(v1);
-               emit_v_div_u32(ctx, divided, as_vgpr(ctx, instance_id), divisor);
-               index = bld.vadd32(bld.def(v1), start_instance, divided);
-            } else {
-               index = bld.vadd32(bld.def(v1), start_instance, instance_id);
-            }
-         } else {
-            index = bld.copy(bld.def(v1), start_instance);
-         }
-      } else {
-         index = bld.vadd32(bld.def(v1), get_arg(ctx, ctx->args->ac.base_vertex),
-                            get_arg(ctx, ctx->args->ac.vertex_id));
-      }
-
-      Temp* const channels = (Temp*)alloca(num_channels * sizeof(Temp));
-      unsigned channel_start = 0;
-      bool direct_fetch = false;
-
-      /* skip unused channels at the start */
-      if (vtx_info->chan_byte_size) {
-         channel_start = ffs(mask) - 1;
-         for (unsigned i = 0; i < MIN2(channel_start, num_channels); i++)
-            channels[i] = Temp(0, s1);
-      }
-
-      /* load channels */
-      while (channel_start < num_channels) {
-         unsigned fetch_component = num_channels - channel_start;
-         unsigned fetch_offset = attrib_offset + channel_start * vtx_info->chan_byte_size;
-
-         /* use MUBUF when possible to avoid possible alignment issues */
-         /* TODO: we could use SDWA to unpack 8/16-bit attributes without extra instructions */
-         bool use_mubuf = vtx_info->chan_byte_size == 4 && bitsize != 16;
-         unsigned fetch_fmt = V_008F0C_BUF_DATA_FORMAT_INVALID;
-         if (!use_mubuf) {
-            fetch_component = ac_get_safe_fetch_size(ctx->program->gfx_level, vtx_info, fetch_offset,
-                                                     vtx_info->num_channels - channel_start, binding_align,
-                                                     fetch_component);
-            fetch_fmt = vtx_info->hw_format[fetch_component - 1];
-         } else {
-            /* GFX6 only supports loading vec3 with MTBUF, split to vec2,scalar. */
-            if (fetch_component == 3 && ctx->options->gfx_level == GFX6)
-               fetch_component = 2;
-         }
-
-         unsigned fetch_bytes = fetch_component * bitsize / 8;
-
-         Temp fetch_index = index;
-         if (attrib_stride != 0 && fetch_offset > attrib_stride) {
-            fetch_index =
-               bld.vadd32(bld.def(v1), Operand::c32(fetch_offset / attrib_stride), fetch_index);
-            fetch_offset = fetch_offset % attrib_stride;
-         }
-
-         Operand soffset = Operand::zero();
-         if (fetch_offset >= 4096) {
-            soffset = bld.copy(bld.def(s1), Operand::c32(fetch_offset / 4096 * 4096));
-            fetch_offset %= 4096;
-         }
-
-         aco_opcode opcode;
-         switch (fetch_bytes) {
-         case 2:
-            assert(!use_mubuf && bitsize == 16);
-            opcode = aco_opcode::tbuffer_load_format_d16_x;
-            break;
-         case 4:
-            if (bitsize == 16) {
-               assert(!use_mubuf);
-               opcode = aco_opcode::tbuffer_load_format_d16_xy;
-            } else {
-               opcode =
-                  use_mubuf ? aco_opcode::buffer_load_dword : aco_opcode::tbuffer_load_format_x;
-            }
-            break;
-         case 6:
-            assert(!use_mubuf && bitsize == 16);
-            opcode = aco_opcode::tbuffer_load_format_d16_xyz;
-            break;
-         case 8:
-            if (bitsize == 16) {
-               assert(!use_mubuf);
-               opcode = aco_opcode::tbuffer_load_format_d16_xyzw;
-            } else {
-               opcode =
-                  use_mubuf ? aco_opcode::buffer_load_dwordx2 : aco_opcode::tbuffer_load_format_xy;
-            }
-            break;
-         case 12:
-            assert(ctx->options->gfx_level >= GFX7 ||
-                   (!use_mubuf && ctx->options->gfx_level == GFX6));
-            opcode =
-               use_mubuf ? aco_opcode::buffer_load_dwordx3 : aco_opcode::tbuffer_load_format_xyz;
-            break;
-         case 16:
-            opcode =
-               use_mubuf ? aco_opcode::buffer_load_dwordx4 : aco_opcode::tbuffer_load_format_xyzw;
-            break;
-         default: unreachable("Unimplemented load_input vector size");
-         }
-
-         Temp fetch_dst;
-         if (channel_start == 0 && fetch_bytes == dst.bytes()) {
-            direct_fetch = true;
-            fetch_dst = dst;
-         } else {
-            fetch_dst = bld.tmp(RegClass::get(RegType::vgpr, fetch_bytes));
-         }
-
-         if (use_mubuf) {
-            Instruction* mubuf = bld.mubuf(opcode, Definition(fetch_dst), list, fetch_index,
-                                           soffset, fetch_offset, false, false, true)
-                                    .instr;
-            mubuf->mubuf().vtx_binding = attrib_binding + 1;
-         } else {
-            unsigned dfmt = fetch_fmt & 0xf;
-            unsigned nfmt = fetch_fmt >> 4;
-            Instruction* mtbuf = bld.mtbuf(opcode, Definition(fetch_dst), list, fetch_index,
-                                           soffset, dfmt, nfmt, fetch_offset, false, true)
-                                    .instr;
-            mtbuf->mtbuf().vtx_binding = attrib_binding + 1;
-         }
-
-         emit_split_vector(ctx, fetch_dst, fetch_dst.bytes() * 8 / bitsize);
-
-         if (fetch_component == 1) {
-            channels[channel_start] = fetch_dst;
-         } else {
-            for (unsigned i = 0; i < MIN2(fetch_component, num_channels - channel_start); i++)
-               channels[channel_start + i] = emit_extract_vector(
-                  ctx, fetch_dst, i, RegClass::get(RegType::vgpr, bitsize / 8u));
-         }
-
-         channel_start += fetch_component;
-      }
-
-      if (!direct_fetch) {
-         bool is_float =
-            nir_alu_type_get_base_type(nir_intrinsic_dest_type(instr)) == nir_type_float;
-
-         unsigned num_components = instr->dest.ssa.num_components;
-
-         aco_ptr<Instruction> vec{create_instruction<Pseudo_instruction>(
-            aco_opcode::p_create_vector, Format::PSEUDO, num_components, 1)};
-         std::array<Temp, NIR_MAX_VEC_COMPONENTS> elems;
-         unsigned num_temp = 0;
-         for (unsigned i = 0; i < num_components; i++) {
-            unsigned idx = i + component;
-            if (idx < num_channels && channels[idx].id()) {
-               Temp channel = channels[idx];
-               vec->operands[i] = Operand(channel);
-
-               num_temp++;
-               elems[i] = channel;
-            } else if (bitsize == 64) {
-               /* 22.1.1. Attribute Location and Component Assignment of Vulkan 1.3 specification:
-                * For 64-bit data types, no default attribute values are provided. Input variables
-                * must not use more components than provided by the attribute.
-                */
-               vec->operands[i] = Operand(v2);
-            } else if (is_float && idx == 3) {
-               vec->operands[i] = bitsize == 16 ? Operand::c16(0x3c00u) : Operand::c32(0x3f800000u);
-            } else if (!is_float && idx == 3) {
-               vec->operands[i] = Operand::get_const(ctx->options->gfx_level, 1u, bitsize / 8u);
-            } else {
-               vec->operands[i] = Operand::zero(bitsize / 8u);
-            }
-         }
-         vec->definitions[0] = Definition(dst);
-         ctx->block->instructions.emplace_back(std::move(vec));
-         emit_split_vector(ctx, dst, num_components);
-
-         if (num_temp == num_components)
-            ctx->allocated_vec.emplace(dst.id(), elems);
-      }
-   } else if (ctx->shader->info.stage == MESA_SHADER_FRAGMENT) {
+   if (ctx->shader->info.stage == MESA_SHADER_FRAGMENT) {
       if (!nir_src_is_const(offset) || nir_src_as_uint(offset))
          isel_err(offset.ssa->parent_instr,
                   "Unimplemented non-zero nir_intrinsic_load_input offset");
-- 
GitLab


From b95fc1611104452a6a02a6467ee0a2adb5e8dd1e Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Thu, 2 Feb 2023 17:57:25 +0100
Subject: [PATCH 07/10] aco: Rename visit_load_input to visit_load_fs_input.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
Acked-by: Konstantin Seurer <konstantin.seurer@gmail.com>
---
 .../compiler/aco_instruction_selection.cpp    | 63 +++++++++----------
 1 file changed, 31 insertions(+), 32 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index e6dd0be4359c..80cdb18af4c3 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -5545,47 +5545,41 @@ mtbuf_load_callback(Builder& bld, const LoadEmitInfo& info, Temp offset, unsigne
 const EmitLoadParameters mtbuf_load_params{mtbuf_load_callback, false, true, 4096};
 
 void
-visit_load_input(isel_context* ctx, nir_intrinsic_instr* instr)
+visit_load_fs_input(isel_context* ctx, nir_intrinsic_instr* instr)
 {
    Builder bld(ctx->program, ctx->block);
    Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
    nir_src offset = *nir_get_io_offset_src(instr);
 
-   if (ctx->shader->info.stage == MESA_SHADER_FRAGMENT) {
-      if (!nir_src_is_const(offset) || nir_src_as_uint(offset))
-         isel_err(offset.ssa->parent_instr,
-                  "Unimplemented non-zero nir_intrinsic_load_input offset");
+   if (!nir_src_is_const(offset) || nir_src_as_uint(offset))
+      isel_err(offset.ssa->parent_instr, "Unimplemented non-zero nir_intrinsic_load_input offset");
 
-      Temp prim_mask = get_arg(ctx, ctx->args->ac.prim_mask);
+   Temp prim_mask = get_arg(ctx, ctx->args->ac.prim_mask);
 
-      unsigned idx = nir_intrinsic_base(instr);
-      unsigned component = nir_intrinsic_component(instr);
-      unsigned vertex_id = 0; /* P0 */
+   unsigned idx = nir_intrinsic_base(instr);
+   unsigned component = nir_intrinsic_component(instr);
+   unsigned vertex_id = 0; /* P0 */
 
-      if (instr->intrinsic == nir_intrinsic_load_input_vertex)
-         vertex_id = nir_src_as_uint(instr->src[0]);
+   if (instr->intrinsic == nir_intrinsic_load_input_vertex)
+      vertex_id = nir_src_as_uint(instr->src[0]);
 
-      if (instr->dest.ssa.num_components == 1 &&
-          instr->dest.ssa.bit_size != 64) {
-         emit_interp_mov_instr(ctx, idx, component, vertex_id, dst, prim_mask);
-      } else {
-         unsigned num_components = instr->dest.ssa.num_components;
-         if (instr->dest.ssa.bit_size == 64)
-            num_components *= 2;
-         aco_ptr<Pseudo_instruction> vec{create_instruction<Pseudo_instruction>(
-            aco_opcode::p_create_vector, Format::PSEUDO, num_components, 1)};
-         for (unsigned i = 0; i < num_components; i++) {
-            unsigned chan_component = (component + i) % 4;
-            unsigned chan_idx = idx + (component + i) / 4;
-            vec->operands[i] = Operand(bld.tmp(instr->dest.ssa.bit_size == 16 ? v2b : v1));
-            emit_interp_mov_instr(ctx, chan_idx, chan_component, vertex_id,
-                                  vec->operands[i].getTemp(), prim_mask);
-         }
-         vec->definitions[0] = Definition(dst);
-         bld.insert(std::move(vec));
-      }
+   if (instr->dest.ssa.num_components == 1 && instr->dest.ssa.bit_size != 64) {
+      emit_interp_mov_instr(ctx, idx, component, vertex_id, dst, prim_mask);
    } else {
-      unreachable("Shader stage not implemented");
+      unsigned num_components = instr->dest.ssa.num_components;
+      if (instr->dest.ssa.bit_size == 64)
+         num_components *= 2;
+      aco_ptr<Pseudo_instruction> vec{create_instruction<Pseudo_instruction>(
+         aco_opcode::p_create_vector, Format::PSEUDO, num_components, 1)};
+      for (unsigned i = 0; i < num_components; i++) {
+         unsigned chan_component = (component + i) % 4;
+         unsigned chan_idx = idx + (component + i) / 4;
+         vec->operands[i] = Operand(bld.tmp(instr->dest.ssa.bit_size == 16 ? v2b : v1));
+         emit_interp_mov_instr(ctx, chan_idx, chan_component, vertex_id, vec->operands[i].getTemp(),
+                               prim_mask);
+      }
+      vec->definitions[0] = Definition(dst);
+      bld.insert(std::move(vec));
    }
 }
 
@@ -8078,7 +8072,12 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
    case nir_intrinsic_load_interpolated_input: visit_load_interpolated_input(ctx, instr); break;
    case nir_intrinsic_store_output: visit_store_output(ctx, instr); break;
    case nir_intrinsic_load_input:
-   case nir_intrinsic_load_input_vertex: visit_load_input(ctx, instr); break;
+   case nir_intrinsic_load_input_vertex:
+      if (ctx->program->stage == fragment_fs)
+         visit_load_fs_input(ctx, instr);
+      else
+         isel_err(&instr->instr, "Shader inputs should have been lowered in NIR.");
+      break;
    case nir_intrinsic_load_per_vertex_input: visit_load_per_vertex_input(ctx, instr); break;
    case nir_intrinsic_load_ubo: visit_load_ubo(ctx, instr); break;
    case nir_intrinsic_load_push_constant: visit_load_push_constant(ctx, instr); break;
-- 
GitLab


From 4ca744d7b1a2d241fa87e6c994304abddeb3f0eb Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Thu, 2 Feb 2023 18:06:16 +0100
Subject: [PATCH 08/10] aco, radv: Remove VS input information from ACO shader
 info.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
Acked-by: Konstantin Seurer <konstantin.seurer@gmail.com>
---
 .../aco_instruction_selection_setup.cpp        | 18 ------------------
 src/amd/compiler/aco_shader_info.h             |  9 ---------
 src/amd/vulkan/radv_aco_shader_info.h          |  7 -------
 3 files changed, 34 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection_setup.cpp b/src/amd/compiler/aco_instruction_selection_setup.cpp
index 439f807dfff4..dd1e27d05bc8 100644
--- a/src/amd/compiler/aco_instruction_selection_setup.cpp
+++ b/src/amd/compiler/aco_instruction_selection_setup.cpp
@@ -398,24 +398,6 @@ init_context(isel_context* ctx, nir_shader* shader)
    ctx->ub_config.max_workgroup_size[0] = 2048;
    ctx->ub_config.max_workgroup_size[1] = 2048;
    ctx->ub_config.max_workgroup_size[2] = 2048;
-   for (unsigned i = 0; i < MAX_VERTEX_ATTRIBS; i++) {
-      pipe_format format = (pipe_format)ctx->options->key.vs.vertex_attribute_formats[i];
-      const struct util_format_description* desc = util_format_description(format);
-
-      uint32_t max;
-      if (desc->channel[0].type != UTIL_FORMAT_TYPE_UNSIGNED) {
-         max = UINT32_MAX;
-      } else if (desc->channel[0].normalized) {
-         max = 0x3f800000u;
-      } else {
-         max = 0;
-         for (unsigned j = 0; j < desc->nr_channels; j++) {
-            uint32_t chan_max = u_uintN_max(desc->channel[0].size);
-            max = MAX2(max, desc->channel[j].pure_integer ? chan_max : fui(chan_max));
-         }
-      }
-      ctx->ub_config.vertex_attrib_max[i] = max;
-   }
 
    nir_divergence_analysis(shader);
    nir_opt_uniform_atomics(shader);
diff --git a/src/amd/compiler/aco_shader_info.h b/src/amd/compiler/aco_shader_info.h
index a3f1872f4759..bc3a4c200463 100644
--- a/src/amd/compiler/aco_shader_info.h
+++ b/src/amd/compiler/aco_shader_info.h
@@ -142,15 +142,6 @@ struct aco_ps_epilog_key {
 struct aco_stage_input {
    uint32_t optimisations_disabled : 1;
    uint32_t image_2d_view_of_3d : 1;
-   struct {
-      uint32_t instance_rate_inputs;
-      uint32_t instance_rate_divisors[ACO_MAX_VERTEX_ATTRIBS];
-      uint8_t vertex_attribute_formats[ACO_MAX_VERTEX_ATTRIBS];
-      uint32_t vertex_attribute_bindings[ACO_MAX_VERTEX_ATTRIBS];
-      uint32_t vertex_attribute_offsets[ACO_MAX_VERTEX_ATTRIBS];
-      uint32_t vertex_attribute_strides[ACO_MAX_VERTEX_ATTRIBS];
-      uint8_t vertex_binding_align[ACO_MAX_VBS];
-   } vs;
 
    struct {
       unsigned tess_input_vertices;
diff --git a/src/amd/vulkan/radv_aco_shader_info.h b/src/amd/vulkan/radv_aco_shader_info.h
index 02ca932e6b54..0959e7a98d85 100644
--- a/src/amd/vulkan/radv_aco_shader_info.h
+++ b/src/amd/vulkan/radv_aco_shader_info.h
@@ -128,13 +128,6 @@ radv_aco_convert_pipe_key(struct aco_stage_input *aco_info,
    radv_aco_convert_ps_epilog_key(&aco_info->ps.epilog, &radv->ps.epilog);
    ASSIGN_FIELD(optimisations_disabled);
    ASSIGN_FIELD(image_2d_view_of_3d);
-   ASSIGN_FIELD(vs.instance_rate_inputs);
-   ASSIGN_FIELD_CP(vs.instance_rate_divisors);
-   ASSIGN_FIELD_CP(vs.vertex_attribute_formats);
-   ASSIGN_FIELD_CP(vs.vertex_attribute_bindings);
-   ASSIGN_FIELD_CP(vs.vertex_attribute_offsets);
-   ASSIGN_FIELD_CP(vs.vertex_attribute_strides);
-   ASSIGN_FIELD_CP(vs.vertex_binding_align);
    ASSIGN_FIELD(tcs.tess_input_vertices);
    ASSIGN_FIELD(ps.alpha_to_coverage_via_mrtz);
 }
-- 
GitLab


From 93f0c101fa3bafc21b4dcbb4d75888f2ac65546d Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Fri, 10 Feb 2023 23:48:33 +0100
Subject: [PATCH 09/10] radv: Remove VS inputs code from LLVM backend.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
Acked-by: Konstantin Seurer <konstantin.seurer@gmail.com>
---
 src/amd/vulkan/radv_nir_to_llvm.c | 174 ------------------------------
 1 file changed, 174 deletions(-)

diff --git a/src/amd/vulkan/radv_nir_to_llvm.c b/src/amd/vulkan/radv_nir_to_llvm.c
index c017bc87098e..ed4b5ad0e105 100644
--- a/src/amd/vulkan/radv_nir_to_llvm.c
+++ b/src/amd/vulkan/radv_nir_to_llvm.c
@@ -255,178 +255,6 @@ radv_get_sampler_desc(struct ac_shader_abi *abi, LLVMValueRef index,
    return radv_load_rsrc(ctx, index, v4 ? ctx->ac.v4i32 : ctx->ac.v8i32);
 }
 
-static LLVMValueRef
-radv_fixup_vertex_input_fetches(struct radv_shader_context *ctx, LLVMValueRef value,
-                                unsigned num_channels, bool is_float, bool is_64bit)
-{
-   LLVMValueRef zero = is_64bit ? ctx->ac.i64_0 : (is_float ? ctx->ac.f32_0 : ctx->ac.i32_0);
-   LLVMValueRef one = is_64bit ? ctx->ac.i64_0 : (is_float ? ctx->ac.f32_1 : ctx->ac.i32_1);
-   LLVMValueRef chan[4];
-
-   if (LLVMGetTypeKind(LLVMTypeOf(value)) == LLVMVectorTypeKind) {
-      unsigned vec_size = LLVMGetVectorSize(LLVMTypeOf(value));
-
-      if (num_channels == 4 && num_channels == vec_size)
-         return value;
-
-      num_channels = MIN2(num_channels, vec_size);
-
-      for (unsigned i = 0; i < num_channels; i++)
-         chan[i] = ac_llvm_extract_elem(&ctx->ac, value, i);
-   } else {
-      assert(num_channels == 1);
-      chan[0] = value;
-   }
-
-   for (unsigned i = num_channels; i < 4; i++) {
-      chan[i] = i == 3 ? one : zero;
-      chan[i] = ac_to_integer(&ctx->ac, chan[i]);
-   }
-
-   return ac_build_gather_values(&ctx->ac, chan, 4);
-}
-
-static void
-load_vs_input(struct radv_shader_context *ctx, unsigned driver_location, LLVMTypeRef dest_type,
-              LLVMValueRef out[4])
-{
-   struct ac_llvm_pointer t_list_ptr = ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->ac.vertex_buffers);
-   LLVMValueRef t_offset;
-   LLVMValueRef t_list;
-   LLVMValueRef input;
-   LLVMValueRef buffer_index;
-   unsigned attrib_index = driver_location - VERT_ATTRIB_GENERIC0;
-   enum pipe_format attrib_format = ctx->options->key.vs.vertex_attribute_formats[attrib_index];
-   const struct util_format_description *desc = util_format_description(attrib_format);
-   bool is_float = !desc->channel[0].pure_integer;
-   uint8_t input_usage_mask =
-      ctx->shader_info->vs.input_usage_mask[driver_location];
-   unsigned num_input_channels = util_last_bit(input_usage_mask);
-
-   if (ctx->options->key.vs.instance_rate_inputs & (1u << attrib_index)) {
-      uint32_t divisor = ctx->options->key.vs.instance_rate_divisors[attrib_index];
-
-      if (divisor) {
-         buffer_index = ctx->abi.instance_id;
-
-         if (divisor != 1) {
-            buffer_index = LLVMBuildUDiv(ctx->ac.builder, buffer_index,
-                                         LLVMConstInt(ctx->ac.i32, divisor, 0), "");
-         }
-      } else {
-         buffer_index = ctx->ac.i32_0;
-      }
-
-      buffer_index = LLVMBuildAdd(
-         ctx->ac.builder, ac_get_arg(&ctx->ac, ctx->args->ac.start_instance), buffer_index, "");
-   } else {
-      buffer_index = LLVMBuildAdd(ctx->ac.builder, ctx->abi.vertex_id,
-                                  ac_get_arg(&ctx->ac, ctx->args->ac.base_vertex), "");
-   }
-
-   const struct ac_vtx_format_info *vtx_info =
-      ac_get_vtx_format_info(GFX8, CHIP_POLARIS10, attrib_format);
-
-   /* Adjust the number of channels to load based on the vertex attribute format. */
-   unsigned num_channels = MIN2(num_input_channels, vtx_info->num_channels);
-   unsigned attrib_binding = ctx->options->key.vs.vertex_attribute_bindings[attrib_index];
-   unsigned attrib_offset = ctx->options->key.vs.vertex_attribute_offsets[attrib_index];
-   unsigned attrib_stride = ctx->options->key.vs.vertex_attribute_strides[attrib_index];
-
-   unsigned data_format = vtx_info->hw_format[num_channels - 1] & 0xf;
-   unsigned num_format = vtx_info->hw_format[0] >> 4;
-
-   unsigned desc_index =
-      ctx->shader_info->vs.use_per_attribute_vb_descs ? attrib_index : attrib_binding;
-   desc_index = util_bitcount(ctx->shader_info->vs.vb_desc_usage_mask &
-                              u_bit_consecutive(0, desc_index));
-   t_offset = LLVMConstInt(ctx->ac.i32, desc_index, false);
-   t_list = ac_build_load_to_sgpr(&ctx->ac, t_list_ptr, t_offset);
-
-   /* Always split typed vertex buffer loads on GFX6 and GFX10+ to avoid any alignment issues that
-    * triggers memory violations and eventually a GPU hang. This can happen if the stride (static or
-    * dynamic) is unaligned and also if the VBO offset is aligned to a scalar (eg. stride is 8 and
-    * VBO offset is 2 for R16G16B16A16_SNORM).
-    */
-   unsigned chan_dwords = vtx_info->chan_byte_size == 8 ? 2 : 1;
-   if (((ctx->ac.gfx_level == GFX6 || ctx->ac.gfx_level >= GFX10) && vtx_info->chan_byte_size) ||
-       !(vtx_info->has_hw_format & BITFIELD_BIT(vtx_info->num_channels - 1)) ||
-       vtx_info->element_size > 16) {
-      unsigned chan_format = vtx_info->hw_format[0] & 0xf;
-      LLVMValueRef values[4];
-
-      for (unsigned chan = 0; chan < num_channels; chan++) {
-         unsigned chan_offset = attrib_offset + chan * vtx_info->chan_byte_size;
-         LLVMValueRef chan_index = buffer_index;
-
-         if (attrib_stride != 0 && chan_offset > attrib_stride) {
-            LLVMValueRef buffer_offset =
-               LLVMConstInt(ctx->ac.i32, chan_offset / attrib_stride, false);
-
-            chan_index = LLVMBuildAdd(ctx->ac.builder, buffer_index, buffer_offset, "");
-
-            chan_offset = chan_offset % attrib_stride;
-         }
-
-         values[chan] = ac_build_struct_tbuffer_load(
-            &ctx->ac, t_list, chan_index, LLVMConstInt(ctx->ac.i32, chan_offset, false),
-            ctx->ac.i32_0, chan_dwords, chan_format, num_format, 0, true);
-      }
-
-      input = ac_build_gather_values(&ctx->ac, values, num_channels);
-   } else {
-      if (attrib_stride != 0 && attrib_offset > attrib_stride) {
-         LLVMValueRef buffer_offset =
-            LLVMConstInt(ctx->ac.i32, attrib_offset / attrib_stride, false);
-
-         buffer_index = LLVMBuildAdd(ctx->ac.builder, buffer_index, buffer_offset, "");
-
-         attrib_offset = attrib_offset % attrib_stride;
-      }
-
-      input = ac_build_struct_tbuffer_load(
-         &ctx->ac, t_list, buffer_index, LLVMConstInt(ctx->ac.i32, attrib_offset, false),
-         ctx->ac.i32_0, num_channels * chan_dwords, data_format, num_format, 0, true);
-   }
-
-   if (vtx_info->chan_byte_size == 8)
-      input =
-         LLVMBuildBitCast(ctx->ac.builder, input, LLVMVectorType(ctx->ac.i64, num_channels), "");
-
-   input = radv_fixup_vertex_input_fetches(ctx, input, num_channels, is_float,
-                                           vtx_info->chan_byte_size == 8);
-
-   for (unsigned chan = 0; chan < 4; chan++) {
-      LLVMValueRef llvm_chan = LLVMConstInt(ctx->ac.i32, chan, false);
-      out[chan] = LLVMBuildExtractElement(ctx->ac.builder, input, llvm_chan, "");
-      if (dest_type == ctx->ac.i16 && is_float) {
-         out[chan] = LLVMBuildBitCast(ctx->ac.builder, out[chan], ctx->ac.f32, "");
-         out[chan] = LLVMBuildFPTrunc(ctx->ac.builder, out[chan], ctx->ac.f16, "");
-      }
-   }
-
-   for (unsigned chan = 0; chan < 4; chan++) {
-      out[chan] = ac_to_integer(&ctx->ac, out[chan]);
-      if (dest_type == ctx->ac.i16 && !is_float)
-         out[chan] = LLVMBuildTrunc(ctx->ac.builder, out[chan], ctx->ac.i16, "");
-   }
-}
-
-static LLVMValueRef
-radv_load_vs_inputs(struct ac_shader_abi *abi, unsigned driver_location, unsigned component,
-                    unsigned num_components, unsigned vertex_index, LLVMTypeRef type)
-{
-   struct radv_shader_context *ctx = radv_shader_context_from_abi(abi);
-   LLVMValueRef values[4];
-
-   load_vs_input(ctx, driver_location, type, values);
-
-   for (unsigned i = 0; i < 4; i++)
-      values[i] = LLVMBuildBitCast(ctx->ac.builder, values[i], type, "");
-
-   return ac_build_varying_gather_values(&ctx->ac, values, num_components, component);
-}
-
 static void
 prepare_interp_optimize(struct radv_shader_context *ctx, struct nir_shader *nir)
 {
@@ -957,8 +785,6 @@ ac_translate_nir_to_llvm(struct ac_llvm_compiler *ac_llvm,
       if (shaders[shader_idx]->info.stage == MESA_SHADER_GEOMETRY && !ctx.shader_info->is_ngg) {
          ctx.abi.emit_vertex_with_counter = visit_emit_vertex_with_counter;
          ctx.abi.emit_primitive = visit_end_primitive;
-      } else if (shaders[shader_idx]->info.stage == MESA_SHADER_VERTEX) {
-         ctx.abi.load_inputs = radv_load_vs_inputs;
       }
 
       if (shader_idx && !(shaders[shader_idx]->info.stage == MESA_SHADER_GEOMETRY && info->is_ngg)) {
-- 
GitLab


From 616164219e4d57d970db5c6a3dafabe9cd15e320 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Mon, 13 Feb 2023 11:35:02 +0100
Subject: [PATCH 10/10] ac/llvm: Remove unused function
 ac_build_struct_tbuffer_load.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
Reviewed-by: Qiang Yu <yuq825@gmail.com>
Reviewed-by: Marek Olšák <marek.olsak@amd.com>
Acked-by: Konstantin Seurer <konstantin.seurer@gmail.com>
---
 src/amd/llvm/ac_llvm_build.c | 11 -----------
 src/amd/llvm/ac_llvm_build.h |  6 ------
 2 files changed, 17 deletions(-)

diff --git a/src/amd/llvm/ac_llvm_build.c b/src/amd/llvm/ac_llvm_build.c
index e620ae28cdcf..61740e3a7f17 100644
--- a/src/amd/llvm/ac_llvm_build.c
+++ b/src/amd/llvm/ac_llvm_build.c
@@ -1461,17 +1461,6 @@ static LLVMValueRef ac_build_tbuffer_load(struct ac_llvm_context *ctx, LLVMValue
                              can_speculate ? AC_ATTR_INVARIANT_LOAD : 0);
 }
 
-LLVMValueRef ac_build_struct_tbuffer_load(struct ac_llvm_context *ctx, LLVMValueRef rsrc,
-                                          LLVMValueRef vindex, LLVMValueRef voffset,
-                                          LLVMValueRef soffset, unsigned num_channels,
-                                          unsigned dfmt, unsigned nfmt, unsigned cache_policy,
-                                          bool can_speculate)
-{
-   unsigned fmt = ac_get_tbuffer_format(ctx->gfx_level, dfmt, nfmt);
-   return ac_build_tbuffer_load(ctx, rsrc, vindex, voffset, soffset, num_channels, fmt,
-                                ctx->i32, cache_policy, can_speculate);
-}
-
 LLVMValueRef ac_build_safe_tbuffer_load(struct ac_llvm_context *ctx, LLVMValueRef rsrc,
                                         LLVMValueRef vidx, LLVMValueRef base_voffset,
                                         LLVMValueRef soffset, LLVMTypeRef channel_type,
diff --git a/src/amd/llvm/ac_llvm_build.h b/src/amd/llvm/ac_llvm_build.h
index accdc636de59..76fe0278edfa 100644
--- a/src/amd/llvm/ac_llvm_build.h
+++ b/src/amd/llvm/ac_llvm_build.h
@@ -303,12 +303,6 @@ LLVMValueRef ac_build_buffer_load_byte(struct ac_llvm_context *ctx, LLVMValueRef
                                        LLVMValueRef voffset, LLVMValueRef soffset,
                                        unsigned cache_policy);
 
-LLVMValueRef ac_build_struct_tbuffer_load(struct ac_llvm_context *ctx, LLVMValueRef rsrc,
-                                          LLVMValueRef vindex, LLVMValueRef voffset,
-                                          LLVMValueRef soffset, unsigned num_channels,
-                                          unsigned dfmt, unsigned nfmt, unsigned cache_policy,
-                                          bool can_speculate);
-
 LLVMValueRef ac_build_safe_tbuffer_load(struct ac_llvm_context *ctx, LLVMValueRef rsrc,
                                         LLVMValueRef vindex, LLVMValueRef voffset,
                                         LLVMValueRef soffset, LLVMTypeRef channel_type,
-- 
GitLab

