From 8591c9239c07a667b9f675800f0eab9ab1d35c71 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Thu, 2 Feb 2023 10:47:58 +0100
Subject: [PATCH 01/13] nir: Add load_typed_buffer_amd intrinsic.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

This new intrinsic maps to the MTBUF instruction format on AMD GPUs
and represents a typed buffer load in NIR.

Also add an unsigned upper bound for the new intrinsic.
Code for that ported from aco_instruction_selection_setup.

Signed-off-by: Timur Krist칩f <timur.kristof@gmail.com>
Reviewed-by: Konstantin Seurer <konstantin.seurer@gmail.com>
Reviewed-by: Marek Ol코치k <marek.olsak@amd.com>
---
 src/compiler/nir/nir_divergence_analysis.c |  1 +
 src/compiler/nir/nir_intrinsics.py         | 13 +++++++++++++
 src/compiler/nir/nir_range_analysis.c      | 19 +++++++++++++++++++
 3 files changed, 33 insertions(+)

diff --git a/src/compiler/nir/nir_divergence_analysis.c b/src/compiler/nir/nir_divergence_analysis.c
index e57d2bc008cd..c1c132665b33 100644
--- a/src/compiler/nir/nir_divergence_analysis.c
+++ b/src/compiler/nir/nir_divergence_analysis.c
@@ -407,6 +407,7 @@ visit_intrinsic(nir_shader *shader, nir_intrinsic_instr *instr)
    case nir_intrinsic_load_kernel_input:
    case nir_intrinsic_load_task_payload:
    case nir_intrinsic_load_buffer_amd:
+   case nir_intrinsic_load_typed_buffer_amd:
    case nir_intrinsic_image_samples:
    case nir_intrinsic_image_deref_samples:
    case nir_intrinsic_bindless_image_samples:
diff --git a/src/compiler/nir/nir_intrinsics.py b/src/compiler/nir/nir_intrinsics.py
index 30ee3818d9fd..cb6671e8548d 100644
--- a/src/compiler/nir/nir_intrinsics.py
+++ b/src/compiler/nir/nir_intrinsics.py
@@ -182,6 +182,7 @@ index("enum glsl_sampler_dim", "image_dim")
 index("bool", "image_array")
 
 # Image format for image intrinsics
+# Vertex buffer format for load_typed_buffer_amd
 index("enum pipe_format", "format")
 
 # Access qualifiers for image and memory access intrinsics. ACCESS_RESTRICT is
@@ -1331,6 +1332,18 @@ intrinsic("load_buffer_amd", src_comp=[4, 1, 1, 1], dest_comp=0, indices=[BASE,
 # src[] = { store value, descriptor, vector byte offset, scalar byte offset, index offset }
 intrinsic("store_buffer_amd", src_comp=[0, 4, 1, 1, 1], indices=[BASE, WRITE_MASK, MEMORY_MODES, ACCESS])
 
+# Typed buffer load of arbitrary length, using a specified format.
+# src[] = { descriptor, vector byte offset, scalar byte offset, index offset }
+#
+# The compiler backend is responsible for emitting correct HW instructions according to alignment, range etc.
+# Users of this intrinsic must ensure that the first component being loaded is really the first component
+# of the specified format, because range analysis assumes this.
+# The size of the specified format also determines the memory range that this instruction is allowed to access.
+#
+# The index offset is multiplied by the stride in the descriptor, if any.
+# The vector/scalar offsets are in bytes, BASE is a constant byte offset.
+intrinsic("load_typed_buffer_amd", src_comp=[4, 1, 1, 1], dest_comp=0, indices=[BASE, MEMORY_MODES, ACCESS, FORMAT, ALIGN_MUL, ALIGN_OFFSET], flags=[CAN_ELIMINATE])
+
 # src[] = { address, unsigned 32-bit offset }.
 load("global_amd", [1, 1], indices=[BASE, ACCESS, ALIGN_MUL, ALIGN_OFFSET], flags=[CAN_ELIMINATE])
 # src[] = { value, address, unsigned 32-bit offset }.
diff --git a/src/compiler/nir/nir_range_analysis.c b/src/compiler/nir/nir_range_analysis.c
index 9b102e89f6a8..0c853d07d582 100644
--- a/src/compiler/nir/nir_range_analysis.c
+++ b/src/compiler/nir/nir_range_analysis.c
@@ -25,6 +25,7 @@
 #include "nir.h"
 #include "nir_range_analysis.h"
 #include "util/hash_table.h"
+#include "util/u_math.h"
 
 /**
  * Analyzes a sequence of operations to determine some aspects of the range of
@@ -1469,6 +1470,24 @@ nir_unsigned_upper_bound_impl(nir_shader *shader, struct hash_table *range_ht,
          /* Very generous maximum: TCS/TES executed by largest possible workgroup */
          res = config->max_workgroup_invocations / MAX2(shader->info.tess.tcs_vertices_out, 1u);
          break;
+      case nir_intrinsic_load_typed_buffer_amd: {
+         const enum pipe_format format = nir_intrinsic_format(intrin);
+         if (format == PIPE_FORMAT_NONE)
+            break;
+
+         const struct util_format_description* desc = util_format_description(format);
+         if (desc->channel[scalar.comp].type != UTIL_FORMAT_TYPE_UNSIGNED)
+            break;
+
+         if (desc->channel[scalar.comp].normalized) {
+            res = fui(1.0);
+            break;
+         }
+
+         const uint32_t chan_max = u_uintN_max(desc->channel[scalar.comp].size);
+         res = desc->channel[scalar.comp].pure_integer ? chan_max : fui(chan_max);
+         break;
+      }
       case nir_intrinsic_load_scalar_arg_amd:
       case nir_intrinsic_load_vector_arg_amd: {
          uint32_t upper_bound = nir_intrinsic_arg_upper_bound_u32_amd(intrin);
-- 
GitLab


From a3747695860927c097073776462e1fae914430ac Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Fri, 3 Feb 2023 01:03:22 +0100
Subject: [PATCH 02/13] aco: Implement load_typed_buffer_amd.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Timur Krist칩f <timur.kristof@gmail.com>
---
 .../compiler/aco_instruction_selection.cpp    | 157 ++++++++++++++++--
 .../aco_instruction_selection_setup.cpp       |   1 +
 2 files changed, 145 insertions(+), 13 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 7e606caff952..2ac0ca95cad6 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -4051,9 +4051,11 @@ struct LoadEmitInfo {
    unsigned const_offset = 0;
    unsigned align_mul = 0;
    unsigned align_offset = 0;
+   pipe_format format;
 
    bool glc = false;
    bool slc = false;
+   bool split_by_component_stride = true;
    unsigned swizzle_component_size = 0;
    memory_sync_info sync;
    Temp soffset = Temp(0, s1);
@@ -4111,10 +4113,12 @@ emit_load(isel_context* ctx, Builder& bld, const LoadEmitInfo& info,
          }
       }
 
-      if (info.swizzle_component_size)
-         bytes_needed = MIN2(bytes_needed, info.swizzle_component_size);
-      if (info.component_stride)
-         bytes_needed = MIN2(bytes_needed, info.component_size);
+      if (info.split_by_component_stride) {
+         if (info.swizzle_component_size)
+            bytes_needed = MIN2(bytes_needed, info.swizzle_component_size);
+         if (info.component_stride)
+            bytes_needed = MIN2(bytes_needed, info.component_size);
+      }
 
       bool need_to_align_offset = byte_align && (align_mul % 4 || align_offset % 4);
 
@@ -4221,9 +4225,11 @@ emit_load(isel_context* ctx, Builder& bld, const LoadEmitInfo& info,
 
       /* add result to list and advance */
       if (info.component_stride) {
-         assert(val.bytes() == info.component_size && "unimplemented");
-         const_offset += info.component_stride;
-         align_offset = (align_offset + info.component_stride) % align_mul;
+         assert(val.bytes() % info.component_size == 0);
+         unsigned num_loaded_components = val.bytes() / info.component_size;
+         unsigned advance_bytes = info.component_stride * num_loaded_components;
+         const_offset += advance_bytes;
+         align_offset = (align_offset + advance_bytes) % align_mul;
       } else {
          const_offset += val.bytes();
          align_offset = (align_offset + val.bytes()) % align_mul;
@@ -5518,6 +5524,104 @@ visit_load_interpolated_input(isel_context* ctx, nir_intrinsic_instr* instr)
    }
 }
 
+Temp
+mtbuf_load_callback(Builder& bld, const LoadEmitInfo& info, Temp offset, unsigned bytes_needed,
+                    unsigned alignment, unsigned const_offset, Temp dst_hint)
+{
+   Operand vaddr = offset.type() == RegType::vgpr ? Operand(offset) : Operand(v1);
+   Operand soffset = offset.type() == RegType::sgpr ? Operand(offset) : Operand::c32(0);
+
+   if (info.soffset.id()) {
+      if (soffset.isTemp())
+         vaddr = bld.copy(bld.def(v1), soffset);
+      soffset = Operand(info.soffset);
+   }
+
+   const bool offen = !vaddr.isUndefined();
+   const bool idxen = info.idx.id();
+
+   if (offen && idxen)
+      vaddr = bld.pseudo(aco_opcode::p_create_vector, bld.def(v2), info.idx, vaddr);
+   else if (idxen)
+      vaddr = Operand(info.idx);
+
+   /* Determine number of fetched components.
+    * Note, ACO IR works with GFX6-8 nfmt + dfmt fields, these are later converted for GFX10+.
+    */
+   const struct ac_vtx_format_info* vtx_info =
+      ac_get_vtx_format_info(GFX8, CHIP_POLARIS10, info.format);
+   /* The number of channels in the format determines the memory range. */
+   const unsigned max_components = vtx_info->num_channels;
+   /* Calculate number of components loaded, which may be lower than the needed bytes. */
+   unsigned num_fetched_components = bytes_needed / info.component_size;
+   num_fetched_components =
+      ac_get_safe_fetch_size(bld.program->gfx_level, vtx_info, const_offset, max_components,
+                             alignment, num_fetched_components);
+   const unsigned fetch_fmt = vtx_info->hw_format[num_fetched_components - 1];
+   /* Adjust bytes needed. */
+   bytes_needed = num_fetched_components * info.component_size;
+   unsigned bytes_size = 0;
+   const unsigned bit_size = info.component_size * 8;
+   aco_opcode op = aco_opcode::num_opcodes;
+
+   if (bytes_needed == 2) {
+      bytes_size = 2;
+      op = aco_opcode::tbuffer_load_format_d16_x;
+   } else if (bytes_needed <= 4) {
+      bytes_size = 4;
+      if (bit_size == 16)
+         op = aco_opcode::tbuffer_load_format_d16_xy;
+      else
+         op = aco_opcode::tbuffer_load_format_x;
+   } else if (bytes_needed <= 6) {
+      bytes_size = 6;
+      if (bit_size == 16)
+         op = aco_opcode::tbuffer_load_format_d16_xyz;
+      else
+         op = aco_opcode::tbuffer_load_format_xy;
+   } else if (bytes_needed <= 8) {
+      bytes_size = 8;
+      if (bit_size == 16)
+         op = aco_opcode::tbuffer_load_format_d16_xyzw;
+      else
+         op = aco_opcode::tbuffer_load_format_xy;
+   } else if (bytes_needed <= 12) {
+      bytes_size = 12;
+      op = aco_opcode::tbuffer_load_format_xyz;
+   } else {
+      bytes_size = 16;
+      op = aco_opcode::tbuffer_load_format_xyzw;
+   }
+
+   /* Abort when suitable opcode wasn't found so we don't compile buggy shaders. */
+   if (op == aco_opcode::num_opcodes) {
+      aco_err(bld.program, "unsupported bit size for typed buffer load");
+      abort();
+   }
+
+   aco_ptr<MTBUF_instruction> mtbuf{create_instruction<MTBUF_instruction>(op, Format::MTBUF, 3, 1)};
+   mtbuf->operands[0] = Operand(info.resource);
+   mtbuf->operands[1] = vaddr;
+   mtbuf->operands[2] = soffset;
+   mtbuf->offen = offen;
+   mtbuf->idxen = idxen;
+   mtbuf->glc = info.glc;
+   mtbuf->dlc = info.glc && (bld.program->gfx_level == GFX10 || bld.program->gfx_level == GFX10_3);
+   mtbuf->slc = info.slc;
+   mtbuf->sync = info.sync;
+   mtbuf->offset = const_offset;
+   mtbuf->dfmt = fetch_fmt & 0xf;
+   mtbuf->nfmt = fetch_fmt >> 4;
+   RegClass rc = RegClass::get(RegType::vgpr, bytes_size);
+   Temp val = dst_hint.id() && rc == dst_hint.regClass() ? dst_hint : bld.tmp(rc);
+   mtbuf->definitions[0] = Definition(val);
+   bld.insert(std::move(mtbuf));
+
+   return val;
+}
+
+const EmitLoadParameters mtbuf_load_params{mtbuf_load_callback, false, true, 4096};
+
 void
 visit_load_input(isel_context* ctx, nir_intrinsic_instr* instr)
 {
@@ -7206,24 +7310,50 @@ visit_load_buffer(isel_context* ctx, nir_intrinsic_instr* intrin)
    unsigned const_offset = nir_intrinsic_base(intrin);
    unsigned elem_size_bytes = intrin->dest.ssa.bit_size / 8u;
    unsigned num_components = intrin->dest.ssa.num_components;
-   unsigned swizzle_element_size = swizzled ? (ctx->program->gfx_level <= GFX8 ? 4 : 16) : 0;
 
    nir_variable_mode mem_mode = nir_intrinsic_memory_modes(intrin);
    memory_sync_info sync(aco_storage_mode_from_nir_mem_mode(mem_mode));
 
    LoadEmitInfo info = {Operand(v_offset), dst, num_components, elem_size_bytes, descriptor};
    info.idx = idx;
-   info.component_stride = swizzle_element_size;
    info.glc = glc;
    info.slc = slc;
-   info.swizzle_component_size = swizzle_element_size ? 4 : 0;
-   info.align_mul = MIN2(elem_size_bytes, 4);
-   info.align_offset = 0;
    info.soffset = s_offset;
    info.const_offset = const_offset;
    info.sync = sync;
 
-   emit_load(ctx, bld, info, mubuf_load_params);
+   if (intrin->intrinsic == nir_intrinsic_load_typed_buffer_amd) {
+      const pipe_format format = nir_intrinsic_format(intrin);
+      const struct ac_vtx_format_info* vtx_info =
+         ac_get_vtx_format_info(ctx->program->gfx_level, ctx->program->family, format);
+      const struct util_format_description* f = util_format_description(format);
+      const unsigned align_mul = nir_intrinsic_align_mul(intrin);
+      const unsigned align_offset = nir_intrinsic_align_offset(intrin);
+
+      /* Avoid splitting:
+       * - non-array formats because that would result in incorrect code
+       * - when element size is same as component size (to reduce instruction count)
+       */
+      const bool can_split = f->is_array && elem_size_bytes != vtx_info->chan_byte_size;
+
+      info.align_mul = align_mul;
+      info.align_offset = align_offset;
+      info.format = format;
+      info.component_stride = can_split ? vtx_info->chan_byte_size : 0;
+      info.split_by_component_stride = false;
+
+      emit_load(ctx, bld, info, mtbuf_load_params);
+   } else {
+      const unsigned swizzle_element_size =
+         swizzled ? (ctx->program->gfx_level <= GFX8 ? 4 : 16) : 0;
+
+      info.component_stride = swizzle_element_size;
+      info.swizzle_component_size = swizzle_element_size ? 4 : 0;
+      info.align_mul = MIN2(elem_size_bytes, 4);
+      info.align_offset = 0;
+
+      emit_load(ctx, bld, info, mubuf_load_params);
+   }
 }
 
 void
@@ -8279,6 +8409,7 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
    case nir_intrinsic_bindless_image_atomic_fmax: visit_image_atomic(ctx, instr); break;
    case nir_intrinsic_load_ssbo: visit_load_ssbo(ctx, instr); break;
    case nir_intrinsic_store_ssbo: visit_store_ssbo(ctx, instr); break;
+   case nir_intrinsic_load_typed_buffer_amd:
    case nir_intrinsic_load_buffer_amd: visit_load_buffer(ctx, instr); break;
    case nir_intrinsic_store_buffer_amd: visit_store_buffer(ctx, instr); break;
    case nir_intrinsic_load_smem_amd: visit_load_smem(ctx, instr); break;
diff --git a/src/amd/compiler/aco_instruction_selection_setup.cpp b/src/amd/compiler/aco_instruction_selection_setup.cpp
index dfb53de7ed2e..ca43726a2d95 100644
--- a/src/amd/compiler/aco_instruction_selection_setup.cpp
+++ b/src/amd/compiler/aco_instruction_selection_setup.cpp
@@ -677,6 +677,7 @@ init_context(isel_context* ctx, nir_shader* shader)
                case nir_intrinsic_load_scratch:
                case nir_intrinsic_load_invocation_id:
                case nir_intrinsic_load_primitive_id:
+               case nir_intrinsic_load_typed_buffer_amd:
                case nir_intrinsic_load_buffer_amd:
                case nir_intrinsic_load_initial_edgeflags_amd:
                case nir_intrinsic_gds_atomic_add_amd:
-- 
GitLab


From 07be021a27bc6c22e9e1075dd9d73abc54cdb1ea Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Fri, 10 Feb 2023 23:44:05 +0100
Subject: [PATCH 03/13] ac/llvm: Implement typed buffer load intrinsic.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Timur Krist칩f <timur.kristof@gmail.com>
Reviewed-by: Qiang Yu <yuq825@gmail.com>
Acked-by: Konstantin Seurer <konstantin.seurer@gmail.com>
---
 src/amd/llvm/ac_llvm_build.c  | 42 +++++++++++++++++++++++++++++++++++
 src/amd/llvm/ac_llvm_build.h  | 11 +++++++++
 src/amd/llvm/ac_nir_to_llvm.c | 21 +++++++++++++++---
 3 files changed, 71 insertions(+), 3 deletions(-)

diff --git a/src/amd/llvm/ac_llvm_build.c b/src/amd/llvm/ac_llvm_build.c
index b30cea10a12e..6e51da7802f8 100644
--- a/src/amd/llvm/ac_llvm_build.c
+++ b/src/amd/llvm/ac_llvm_build.c
@@ -1473,6 +1473,48 @@ LLVMValueRef ac_build_struct_tbuffer_load(struct ac_llvm_context *ctx, LLVMValue
                                 ctx->i32, cache_policy, can_speculate);
 }
 
+LLVMValueRef ac_build_safe_tbuffer_load(struct ac_llvm_context *ctx, LLVMValueRef rsrc,
+                                        LLVMValueRef vidx, LLVMValueRef base_voffset,
+                                        LLVMValueRef soffset, LLVMTypeRef channel_type,
+                                        const struct ac_vtx_format_info *vtx_info,
+                                        unsigned const_offset,
+                                        unsigned align_offset,
+                                        unsigned align_mul,
+                                        unsigned num_channels,
+                                        unsigned cache_policy,
+                                        bool can_speculate)
+{
+   const unsigned max_channels = vtx_info->num_channels;
+   LLVMValueRef voffset_plus_const =
+      LLVMBuildAdd(ctx->builder, base_voffset, LLVMConstInt(ctx->i32, const_offset, 0), "");
+
+   /* Split the specified load into several MTBUF instructions,
+    * according to a safe fetch size determined by aligmnent information.
+    */
+   LLVMValueRef result = NULL;
+   for (unsigned i = 0, fetch_num_channels; i < num_channels; i += fetch_num_channels) {
+      const unsigned fetch_const_offset = const_offset + i * vtx_info->chan_byte_size;
+      const unsigned fetch_align_offset = (align_offset + i * vtx_info->chan_byte_size) % align_mul;
+      const unsigned fetch_alignment = fetch_align_offset ? 1 << (ffs(fetch_align_offset) - 1) : align_mul;
+
+      fetch_num_channels =
+         ac_get_safe_fetch_size(ctx->gfx_level, vtx_info, fetch_const_offset,
+                                max_channels - i, fetch_alignment, num_channels - i);
+      const unsigned fetch_format = vtx_info->hw_format[fetch_num_channels - 1];
+      LLVMValueRef fetch_voffset =
+            LLVMBuildAdd(ctx->builder, voffset_plus_const,
+                         LLVMConstInt(ctx->i32, i * vtx_info->chan_byte_size, 0), "");
+      LLVMValueRef item =
+         ac_build_tbuffer_load(ctx, rsrc, vidx, fetch_voffset, soffset,
+                               fetch_num_channels, fetch_format, channel_type,
+                               cache_policy, can_speculate);
+      result = ac_build_concat(ctx, result, item);
+   }
+
+   return result;
+}
+
+
 LLVMValueRef ac_build_buffer_load_short(struct ac_llvm_context *ctx, LLVMValueRef rsrc,
                                         LLVMValueRef voffset, LLVMValueRef soffset,
                                         unsigned cache_policy)
diff --git a/src/amd/llvm/ac_llvm_build.h b/src/amd/llvm/ac_llvm_build.h
index 0c11f4942a9e..5b1c1bbcd413 100644
--- a/src/amd/llvm/ac_llvm_build.h
+++ b/src/amd/llvm/ac_llvm_build.h
@@ -310,6 +310,17 @@ LLVMValueRef ac_build_struct_tbuffer_load(struct ac_llvm_context *ctx, LLVMValue
                                           unsigned dfmt, unsigned nfmt, unsigned cache_policy,
                                           bool can_speculate);
 
+LLVMValueRef ac_build_safe_tbuffer_load(struct ac_llvm_context *ctx, LLVMValueRef rsrc,
+                                        LLVMValueRef vindex, LLVMValueRef voffset,
+                                        LLVMValueRef soffset, LLVMTypeRef channel_type,
+                                        const struct ac_vtx_format_info *vtx_info,
+                                        unsigned const_offset,
+                                        unsigned align_offset,
+                                        unsigned align_mul,
+                                        unsigned num_channels,
+                                        unsigned cache_policy,
+                                        bool can_speculate);
+
 LLVMValueRef ac_build_opencoded_load_format(struct ac_llvm_context *ctx, unsigned log_size,
                                             unsigned num_channels, unsigned format, bool reverse,
                                             bool known_aligned, LLVMValueRef rsrc,
diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index 0389682ff0f7..8bb4008a9ce9 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -3966,6 +3966,7 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
    case nir_intrinsic_set_vertex_and_primitive_count:
       /* Currently ignored. */
       break;
+   case nir_intrinsic_load_typed_buffer_amd:
    case nir_intrinsic_load_buffer_amd:
    case nir_intrinsic_store_buffer_amd: {
       unsigned src_base = instr->intrinsic == nir_intrinsic_store_buffer_amd ? 1 : 0;
@@ -4004,7 +4005,8 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
       } else if (instr->intrinsic == nir_intrinsic_store_buffer_amd && uses_format) {
          assert(instr->src[0].ssa->bit_size == 16 || instr->src[0].ssa->bit_size == 32);
          ac_build_buffer_store_format(&ctx->ac, descriptor, store_data, vidx, voffset, cache_policy);
-      } else if (instr->intrinsic == nir_intrinsic_load_buffer_amd) {
+      } else if (instr->intrinsic == nir_intrinsic_load_buffer_amd ||
+                 instr->intrinsic == nir_intrinsic_load_typed_buffer_amd) {
          /* LLVM is unable to select instructions for larger than 32-bit channel types.
           * Workaround by using i32 and casting to the correct type later.
           */
@@ -4013,8 +4015,21 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
          LLVMTypeRef channel_type =
             LLVMIntTypeInContext(ctx->ac.context, MIN2(32, instr->dest.ssa.bit_size));
 
-         result = ac_build_buffer_load(&ctx->ac, descriptor, fetch_num_components, vidx, voffset,
-                                       addr_soffset, channel_type, cache_policy, reorder, false);
+         if (instr->intrinsic == nir_intrinsic_load_buffer_amd) {
+            result = ac_build_buffer_load(&ctx->ac, descriptor, fetch_num_components, vidx, voffset,
+                                          addr_soffset, channel_type, cache_policy, reorder, false);
+         } else {
+            const unsigned align_offset = nir_intrinsic_align_offset(instr);
+            const unsigned align_mul = nir_intrinsic_align_mul(instr);
+            const enum pipe_format format = nir_intrinsic_format(instr);
+            const struct ac_vtx_format_info *vtx_info =
+               ac_get_vtx_format_info(ctx->ac.gfx_level, ctx->ac.family, format);
+
+            result =
+               ac_build_safe_tbuffer_load(&ctx->ac, descriptor, vidx, addr_voffset, addr_soffset,
+                                          channel_type, vtx_info, const_offset, align_offset,
+                                          align_mul, fetch_num_components, cache_policy, reorder);
+         }
 
          /* Trim to needed vector components. */
          result = ac_trim_vector(&ctx->ac, result, fetch_num_components);
-- 
GitLab


From 0d6e8614a08978692619f8b655ee0a99621c3cb2 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Fri, 10 Feb 2023 08:37:06 +0100
Subject: [PATCH 04/13] radv: Lower non-dynamic VS inputs in NIR.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Timur Krist칩f <timur.kristof@gmail.com>
Reviewed-by: Konstantin Seurer <konstantin.seurer@gmail.com>
---
 src/amd/vulkan/radv_nir_lower_vs_inputs.c | 236 +++++++++++++++++++++-
 1 file changed, 234 insertions(+), 2 deletions(-)

diff --git a/src/amd/vulkan/radv_nir_lower_vs_inputs.c b/src/amd/vulkan/radv_nir_lower_vs_inputs.c
index fbed239856b3..0dfb9f3779f0 100644
--- a/src/amd/vulkan/radv_nir_lower_vs_inputs.c
+++ b/src/amd/vulkan/radv_nir_lower_vs_inputs.c
@@ -80,6 +80,239 @@ lower_load_vs_input_from_prolog(nir_builder *b, nir_intrinsic_instr *intrin,
    return extracted;
 }
 
+static nir_ssa_def *
+calc_vs_input_index_instance_rate(nir_builder *b, unsigned location, lower_vs_inputs_state *s)
+{
+   const uint32_t divisor = s->pl_key->vs.instance_rate_divisors[location];
+   nir_ssa_def *start_instance = nir_load_base_instance(b);
+
+   if (divisor == 0)
+      return start_instance;
+
+   nir_ssa_def *instance_id = nir_udiv_imm(b, nir_load_instance_id(b), divisor);
+   return nir_iadd(b, start_instance, instance_id);
+}
+
+static nir_ssa_def *
+calc_vs_input_index(nir_builder *b, unsigned location, lower_vs_inputs_state *s)
+{
+   if (s->pl_key->vs.instance_rate_inputs & BITFIELD_BIT(location))
+      return calc_vs_input_index_instance_rate(b, location, s);
+
+   return nir_iadd(b, nir_load_first_vertex(b), nir_load_vertex_id_zero_base(b));
+}
+
+static bool
+can_use_untyped_load(const struct util_format_description *f, const unsigned bit_size)
+{
+   /* All components must have same size and type. */
+   if (!f->is_array)
+      return false;
+
+   const struct util_format_channel_description *c = &f->channel[0];
+   return c->size == bit_size && bit_size >= 32;
+}
+
+static nir_ssa_def *
+oob_input_load_value(nir_builder *b, const unsigned channel_idx, const unsigned bit_size,
+                     const bool is_float)
+{
+   /* 22.1.1. Attribute Location and Component Assignment of Vulkan 1.3 specification:
+    * For 64-bit data types, no default attribute values are provided. Input variables
+    * must not use more components than provided by the attribute.
+    */
+   if (bit_size == 64)
+      return nir_ssa_undef(b, 1, bit_size);
+
+   if (channel_idx == 3) {
+      if (is_float)
+         return nir_imm_floatN_t(b, 1.0, bit_size);
+      else
+         return nir_imm_intN_t(b, 1, bit_size);
+   }
+
+   return nir_imm_intN_t(b, 0, bit_size);
+}
+
+static unsigned
+count_format_bytes(const struct util_format_description *f, const unsigned first_channel,
+                   const unsigned num_channels)
+{
+   if (!num_channels)
+      return 0;
+
+   const unsigned last_channel = first_channel + num_channels - 1;
+   assert(last_channel < f->nr_channels);
+   unsigned bits = 0;
+   for (unsigned i = first_channel; i <= last_channel; ++i) {
+      bits += f->channel[i].size;
+   }
+
+   assert(bits % 8 == 0);
+   return bits / 8;
+}
+
+static nir_ssa_def *
+lower_load_vs_input(nir_builder *b, nir_intrinsic_instr *intrin, lower_vs_inputs_state *s)
+{
+   nir_src *offset_src = nir_get_io_offset_src(intrin);
+   assert(nir_src_is_const(*offset_src));
+
+   const unsigned base = nir_intrinsic_base(intrin);
+   const unsigned base_offset = nir_src_as_uint(*offset_src);
+   const unsigned location = base + base_offset - VERT_ATTRIB_GENERIC0;
+   const unsigned bit_size = intrin->dest.ssa.bit_size;
+   const unsigned num_components = intrin->dest.ssa.num_components;
+
+   /* Convert the component offset to bit_size units.
+    * (Intrinsic component offset is in 32-bit units.)
+    *
+    * Small bitsize inputs consume the same space as 32-bit inputs,
+    * but 64-bit inputs consume twice as many.
+    * 64-bit variables must not have a component of 1 or 3.
+    * (See VK spec 15.1.5 "Component Assignment")
+    */
+   const unsigned component = nir_intrinsic_component(intrin) / (MAX2(32, bit_size) / 32);
+
+   /* Bitmask of components in bit_size units
+    * of the current input load that are actually used.
+    */
+   const unsigned mask = nir_ssa_def_components_read(&intrin->dest.ssa) << component;
+
+   /* If the input is entirely unused, just replace it with undef.
+    * This is just in case we debug this pass without running DCE first.
+    */
+   if (!mask)
+      return nir_ssa_undef(b, num_components, bit_size);
+
+   const uint32_t attrib_binding = s->pl_key->vs.vertex_attribute_bindings[location];
+   const uint32_t attrib_offset = s->pl_key->vs.vertex_attribute_offsets[location];
+   const uint32_t attrib_stride = s->pl_key->vs.vertex_attribute_strides[location];
+   const enum pipe_format attrib_format = s->pl_key->vs.vertex_attribute_formats[location];
+   const struct util_format_description *f = util_format_description(attrib_format);
+   const unsigned binding_index =
+      s->info->vs.use_per_attribute_vb_descs ? location : attrib_binding;
+   const unsigned desc_index =
+      util_bitcount(s->info->vs.vb_desc_usage_mask & u_bit_consecutive(0, binding_index));
+
+   nir_ssa_def *vertex_buffers_arg = ac_nir_load_arg(b, &s->args->ac, s->args->ac.vertex_buffers);
+   nir_ssa_def *vertex_buffers =
+      nir_pack_64_2x32_split(b, vertex_buffers_arg, nir_imm_int(b, s->address32_hi));
+   nir_ssa_def *descriptor =
+      nir_load_smem_amd(b, 4, vertex_buffers, nir_imm_int(b, desc_index * 16));
+   nir_ssa_def *base_index = calc_vs_input_index(b, location, s);
+   nir_ssa_def *zero = nir_imm_int(b, 0);
+
+   /* Try to shrink the load format by skipping unused components from the start.
+    * Beneficial because the backend may be able to emit fewer HW instructions.
+    * Only possible with array formats.
+    */
+   const unsigned first_used_channel = ffs(mask) - 1;
+   const unsigned skipped_start = f->is_array ? first_used_channel : 0;
+
+   /* Number of channels we actually use and load.
+    * Don't shrink the format here because this might allow the backend to
+    * emit fewer (but larger than needed) HW instructions.
+    */
+   const unsigned first_trailing_unused_channel = util_last_bit(mask);
+   const unsigned max_loaded_channels = MIN2(first_trailing_unused_channel, f->nr_channels);
+   const unsigned fetch_num_channels = max_loaded_channels - skipped_start;
+
+   /* Load VS inputs from VRAM.
+    *
+    * For the vast majority of cases this will only create 1x load_(typed)_buffer_amd
+    * intrinsic and the backend is responsible for further splitting that
+    * to as many HW instructions as needed based on alignment.
+    *
+    * Take care to prevent loaded components from failing the range check,
+    * by emitting several load intrinsics with different index sources.
+    * This is necessary because the backend can't further roll the const offset
+    * into the index source of MUBUF / MTBUF instructions.
+    */
+   nir_ssa_def *loads[NIR_MAX_VEC_COMPONENTS] = {0};
+   unsigned num_loads = 0;
+   for (unsigned x = 0, channels = fetch_num_channels; x < fetch_num_channels; x += channels) {
+      const unsigned start = skipped_start + x;
+      enum pipe_format fetch_format = attrib_format;
+      nir_ssa_def *index = base_index;
+
+      /* Add excess constant offset to the index. */
+      unsigned const_off = attrib_offset + count_format_bytes(f, 0, start);
+      if (attrib_stride && const_off > attrib_stride) {
+         index = nir_iadd_imm(b, base_index, const_off / attrib_stride);
+         const_off %= attrib_stride;
+      }
+
+      /* Reduce the number of loaded channels until we can pass the range check.
+       * Only for array formats. VK spec mandates proper alignment for packed formats.
+       */
+      if (f->is_array) {
+         while (channels > 1 &&
+                (const_off + count_format_bytes(f, start, channels)) > attrib_stride) {
+            channels--;
+         }
+         if (channels != fetch_num_channels) {
+            fetch_format =
+               util_format_get_array(f->channel[0].type, f->channel[0].size, channels,
+                                     f->is_unorm || f->is_snorm, f->channel[0].pure_integer);
+         }
+      }
+
+      assert(f->is_array || channels == fetch_num_channels);
+
+      /* Prefer using untyped buffer loads if possible, to avoid potential alignment issues. */
+      if (can_use_untyped_load(f, bit_size)) {
+         loads[num_loads++] =
+            nir_load_buffer_amd(b, channels, bit_size, descriptor, zero, zero, index,
+                                .base = const_off, .memory_modes = nir_var_shader_in);
+      } else {
+         const unsigned align_mul = MAX2(1, s->pl_key->vs.vertex_binding_align[attrib_binding]);
+         const unsigned align_offset = const_off % align_mul;
+
+         loads[num_loads++] = nir_load_typed_buffer_amd(
+            b, channels, bit_size, descriptor, zero, zero, index, .base = const_off,
+            .format = fetch_format, .align_mul = align_mul, .align_offset = align_offset,
+            .memory_modes = nir_var_shader_in);
+      }
+   }
+
+   nir_ssa_def *load = loads[0];
+
+   /* Extract the channels we actually need when we couldn't skip starting
+    * components or had to emit more than one load instrinsic.
+    */
+   if (first_used_channel > skipped_start || num_loads != 1)
+      load = nir_extract_bits(b, loads, num_loads, (first_used_channel - skipped_start) * bit_size,
+                              max_loaded_channels - first_used_channel, bit_size);
+
+   /* Return early if possible to avoid generating unnecessary IR. */
+   if (first_used_channel == component && load->num_components == num_components)
+      return load;
+
+   /* Fill unused components at the beginning with undefs.
+    * Fill OOB components at the end with their respective values.
+    */
+   const nir_alu_type dst_type = nir_alu_type_get_base_type(nir_intrinsic_dest_type(intrin));
+   nir_ssa_def *channels[NIR_MAX_VEC_COMPONENTS] = {0};
+   for (unsigned i = 0; i < num_components; ++i) {
+      const unsigned c = i + component;
+      if (c < first_used_channel)
+         /* Fill unused channels at the start with undef. */
+         channels[i] = nir_ssa_undef(b, 1, bit_size);
+      else if (c < max_loaded_channels)
+         /* Use channels that were loaded from VRAM. */
+         channels[i] = nir_channel(b, load, c - first_used_channel);
+      else if (c < first_trailing_unused_channel)
+         /* Handle input loads that are larger than their format. */
+         channels[i] = oob_input_load_value(b, c, bit_size, dst_type == nir_type_float);
+      else
+         /* Fill unused channels at the end with undef. */
+         channels[i] = nir_ssa_undef(b, 1, bit_size);
+   }
+
+   return nir_vec(b, channels, num_components);
+}
+
 static bool
 lower_vs_input_instr(nir_builder *b, nir_instr *instr, void *state)
 {
@@ -99,8 +332,7 @@ lower_vs_input_instr(nir_builder *b, nir_instr *instr, void *state)
    if (s->info->vs.dynamic_inputs) {
       replacement = lower_load_vs_input_from_prolog(b, intrin, s);
    } else {
-      /* TODO: lower non-dynamic inputs */
-      return false;
+      replacement = lower_load_vs_input(b, intrin, s);
    }
 
    nir_ssa_def_rewrite_uses(&intrin->dest.ssa, replacement);
-- 
GitLab


From 10257157e578ee18dd6f5f8f5e60bd3faaafbb70 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Thu, 2 Feb 2023 17:55:06 +0100
Subject: [PATCH 05/13] aco: Remove VS inputs from visit_load_input.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Timur Krist칩f <timur.kristof@gmail.com>
Acked-by: Konstantin Seurer <konstantin.seurer@gmail.com>
---
 .../compiler/aco_instruction_selection.cpp    | 267 +-----------------
 1 file changed, 1 insertion(+), 266 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 2ac0ca95cad6..551134d8d801 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -313,58 +313,6 @@ as_vgpr(isel_context* ctx, Temp val)
    return as_vgpr(bld, val);
 }
 
-// assumes a != 0xffffffff
-void
-emit_v_div_u32(isel_context* ctx, Temp dst, Temp a, uint32_t b)
-{
-   assert(b != 0);
-   Builder bld(ctx->program, ctx->block);
-
-   if (util_is_power_of_two_or_zero(b)) {
-      bld.vop2(aco_opcode::v_lshrrev_b32, Definition(dst), Operand::c32(util_logbase2(b)), a);
-      return;
-   }
-
-   util_fast_udiv_info info = util_compute_fast_udiv_info(b, 32, 32);
-
-   assert(info.multiplier <= 0xffffffff);
-
-   bool pre_shift = info.pre_shift != 0;
-   bool increment = info.increment != 0;
-   bool multiply = true;
-   bool post_shift = info.post_shift != 0;
-
-   if (!pre_shift && !increment && !multiply && !post_shift) {
-      bld.copy(Definition(dst), a);
-      return;
-   }
-
-   Temp pre_shift_dst = a;
-   if (pre_shift) {
-      pre_shift_dst = (increment || multiply || post_shift) ? bld.tmp(v1) : dst;
-      bld.vop2(aco_opcode::v_lshrrev_b32, Definition(pre_shift_dst), Operand::c32(info.pre_shift),
-               a);
-   }
-
-   Temp increment_dst = pre_shift_dst;
-   if (increment) {
-      increment_dst = (post_shift || multiply) ? bld.tmp(v1) : dst;
-      bld.vadd32(Definition(increment_dst), Operand::c32(info.increment), pre_shift_dst);
-   }
-
-   Temp multiply_dst = increment_dst;
-   if (multiply) {
-      multiply_dst = post_shift ? bld.tmp(v1) : dst;
-      bld.vop3(aco_opcode::v_mul_hi_u32, Definition(multiply_dst), increment_dst,
-               bld.copy(bld.def(v1), Operand::c32(info.multiplier)));
-   }
-
-   if (post_shift) {
-      bld.vop2(aco_opcode::v_lshrrev_b32, Definition(dst), Operand::c32(info.post_shift),
-               multiply_dst);
-   }
-}
-
 void
 emit_extract_vector(isel_context* ctx, Temp src, uint32_t idx, Temp dst)
 {
@@ -5629,220 +5577,7 @@ visit_load_input(isel_context* ctx, nir_intrinsic_instr* instr)
    Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
    nir_src offset = *nir_get_io_offset_src(instr);
 
-   if (ctx->shader->info.stage == MESA_SHADER_VERTEX) {
-      if (!nir_src_is_const(offset) || nir_src_as_uint(offset))
-         isel_err(offset.ssa->parent_instr,
-                  "Unimplemented non-zero nir_intrinsic_load_input offset");
-
-      Temp vertex_buffers =
-         convert_pointer_to_64_bit(ctx, get_arg(ctx, ctx->args->ac.vertex_buffers));
-
-      unsigned location = nir_intrinsic_base(instr) - VERT_ATTRIB_GENERIC0;
-      unsigned bitsize = instr->dest.ssa.bit_size;
-      unsigned component = nir_intrinsic_component(instr) >> (bitsize == 64 ? 1 : 0);
-      unsigned attrib_binding = ctx->options->key.vs.vertex_attribute_bindings[location];
-      uint32_t attrib_offset = ctx->options->key.vs.vertex_attribute_offsets[location];
-      uint32_t attrib_stride = ctx->options->key.vs.vertex_attribute_strides[location];
-      enum pipe_format attrib_format =
-         (enum pipe_format)ctx->options->key.vs.vertex_attribute_formats[location];
-      unsigned binding_align = ctx->options->key.vs.vertex_binding_align[attrib_binding];
-
-      const struct ac_vtx_format_info* vtx_info =
-         ac_get_vtx_format_info(GFX8, CHIP_POLARIS10, attrib_format);
-
-      unsigned mask = nir_ssa_def_components_read(&instr->dest.ssa) << component;
-      unsigned num_channels = MIN2(util_last_bit(mask), vtx_info->num_channels);
-
-      unsigned desc_index =
-         ctx->program->info.vs.use_per_attribute_vb_descs ? location : attrib_binding;
-      desc_index = util_bitcount(ctx->program->info.vs.vb_desc_usage_mask &
-                                 u_bit_consecutive(0, desc_index));
-      Operand off = bld.copy(bld.def(s1), Operand::c32(desc_index * 16u));
-      Temp list = bld.smem(aco_opcode::s_load_dwordx4, bld.def(s4), vertex_buffers, off);
-
-      Temp index;
-      if (ctx->options->key.vs.instance_rate_inputs & (1u << location)) {
-         uint32_t divisor = ctx->options->key.vs.instance_rate_divisors[location];
-         Temp start_instance = get_arg(ctx, ctx->args->ac.start_instance);
-         if (divisor) {
-            Temp instance_id = get_arg(ctx, ctx->args->ac.instance_id);
-            if (divisor != 1) {
-               Temp divided = bld.tmp(v1);
-               emit_v_div_u32(ctx, divided, as_vgpr(ctx, instance_id), divisor);
-               index = bld.vadd32(bld.def(v1), start_instance, divided);
-            } else {
-               index = bld.vadd32(bld.def(v1), start_instance, instance_id);
-            }
-         } else {
-            index = bld.copy(bld.def(v1), start_instance);
-         }
-      } else {
-         index = bld.vadd32(bld.def(v1), get_arg(ctx, ctx->args->ac.base_vertex),
-                            get_arg(ctx, ctx->args->ac.vertex_id));
-      }
-
-      Temp* const channels = (Temp*)alloca(num_channels * sizeof(Temp));
-      unsigned channel_start = 0;
-      bool direct_fetch = false;
-
-      /* skip unused channels at the start */
-      if (vtx_info->chan_byte_size) {
-         channel_start = ffs(mask) - 1;
-         for (unsigned i = 0; i < MIN2(channel_start, num_channels); i++)
-            channels[i] = Temp(0, s1);
-      }
-
-      /* load channels */
-      while (channel_start < num_channels) {
-         unsigned fetch_component = num_channels - channel_start;
-         unsigned fetch_offset = attrib_offset + channel_start * vtx_info->chan_byte_size;
-
-         /* use MUBUF when possible to avoid possible alignment issues */
-         /* TODO: we could use SDWA to unpack 8/16-bit attributes without extra instructions */
-         bool use_mubuf = vtx_info->chan_byte_size == 4 && bitsize != 16;
-         unsigned fetch_fmt = V_008F0C_BUF_DATA_FORMAT_INVALID;
-         if (!use_mubuf) {
-            fetch_component = ac_get_safe_fetch_size(ctx->program->gfx_level, vtx_info, fetch_offset,
-                                                     vtx_info->num_channels - channel_start, binding_align,
-                                                     fetch_component);
-            fetch_fmt = vtx_info->hw_format[fetch_component - 1];
-         } else {
-            /* GFX6 only supports loading vec3 with MTBUF, split to vec2,scalar. */
-            if (fetch_component == 3 && ctx->options->gfx_level == GFX6)
-               fetch_component = 2;
-         }
-
-         unsigned fetch_bytes = fetch_component * bitsize / 8;
-
-         Temp fetch_index = index;
-         if (attrib_stride != 0 && fetch_offset > attrib_stride) {
-            fetch_index =
-               bld.vadd32(bld.def(v1), Operand::c32(fetch_offset / attrib_stride), fetch_index);
-            fetch_offset = fetch_offset % attrib_stride;
-         }
-
-         Operand soffset = Operand::zero();
-         if (fetch_offset >= 4096) {
-            soffset = bld.copy(bld.def(s1), Operand::c32(fetch_offset / 4096 * 4096));
-            fetch_offset %= 4096;
-         }
-
-         aco_opcode opcode;
-         switch (fetch_bytes) {
-         case 2:
-            assert(!use_mubuf && bitsize == 16);
-            opcode = aco_opcode::tbuffer_load_format_d16_x;
-            break;
-         case 4:
-            if (bitsize == 16) {
-               assert(!use_mubuf);
-               opcode = aco_opcode::tbuffer_load_format_d16_xy;
-            } else {
-               opcode =
-                  use_mubuf ? aco_opcode::buffer_load_dword : aco_opcode::tbuffer_load_format_x;
-            }
-            break;
-         case 6:
-            assert(!use_mubuf && bitsize == 16);
-            opcode = aco_opcode::tbuffer_load_format_d16_xyz;
-            break;
-         case 8:
-            if (bitsize == 16) {
-               assert(!use_mubuf);
-               opcode = aco_opcode::tbuffer_load_format_d16_xyzw;
-            } else {
-               opcode =
-                  use_mubuf ? aco_opcode::buffer_load_dwordx2 : aco_opcode::tbuffer_load_format_xy;
-            }
-            break;
-         case 12:
-            assert(ctx->options->gfx_level >= GFX7 ||
-                   (!use_mubuf && ctx->options->gfx_level == GFX6));
-            opcode =
-               use_mubuf ? aco_opcode::buffer_load_dwordx3 : aco_opcode::tbuffer_load_format_xyz;
-            break;
-         case 16:
-            opcode =
-               use_mubuf ? aco_opcode::buffer_load_dwordx4 : aco_opcode::tbuffer_load_format_xyzw;
-            break;
-         default: unreachable("Unimplemented load_input vector size");
-         }
-
-         Temp fetch_dst;
-         if (channel_start == 0 && fetch_bytes == dst.bytes()) {
-            direct_fetch = true;
-            fetch_dst = dst;
-         } else {
-            fetch_dst = bld.tmp(RegClass::get(RegType::vgpr, fetch_bytes));
-         }
-
-         if (use_mubuf) {
-            Instruction* mubuf = bld.mubuf(opcode, Definition(fetch_dst), list, fetch_index,
-                                           soffset, fetch_offset, false, false, true)
-                                    .instr;
-            mubuf->mubuf().vtx_binding = attrib_binding + 1;
-         } else {
-            unsigned dfmt = fetch_fmt & 0xf;
-            unsigned nfmt = fetch_fmt >> 4;
-            Instruction* mtbuf = bld.mtbuf(opcode, Definition(fetch_dst), list, fetch_index,
-                                           soffset, dfmt, nfmt, fetch_offset, false, true)
-                                    .instr;
-            mtbuf->mtbuf().vtx_binding = attrib_binding + 1;
-         }
-
-         emit_split_vector(ctx, fetch_dst, fetch_dst.bytes() * 8 / bitsize);
-
-         if (fetch_component == 1) {
-            channels[channel_start] = fetch_dst;
-         } else {
-            for (unsigned i = 0; i < MIN2(fetch_component, num_channels - channel_start); i++)
-               channels[channel_start + i] = emit_extract_vector(
-                  ctx, fetch_dst, i, RegClass::get(RegType::vgpr, bitsize / 8u));
-         }
-
-         channel_start += fetch_component;
-      }
-
-      if (!direct_fetch) {
-         bool is_float =
-            nir_alu_type_get_base_type(nir_intrinsic_dest_type(instr)) == nir_type_float;
-
-         unsigned num_components = instr->dest.ssa.num_components;
-
-         aco_ptr<Instruction> vec{create_instruction<Pseudo_instruction>(
-            aco_opcode::p_create_vector, Format::PSEUDO, num_components, 1)};
-         std::array<Temp, NIR_MAX_VEC_COMPONENTS> elems;
-         unsigned num_temp = 0;
-         for (unsigned i = 0; i < num_components; i++) {
-            unsigned idx = i + component;
-            if (idx < num_channels && channels[idx].id()) {
-               Temp channel = channels[idx];
-               vec->operands[i] = Operand(channel);
-
-               num_temp++;
-               elems[i] = channel;
-            } else if (bitsize == 64) {
-               /* 22.1.1. Attribute Location and Component Assignment of Vulkan 1.3 specification:
-                * For 64-bit data types, no default attribute values are provided. Input variables
-                * must not use more components than provided by the attribute.
-                */
-               vec->operands[i] = Operand(v2);
-            } else if (is_float && idx == 3) {
-               vec->operands[i] = bitsize == 16 ? Operand::c16(0x3c00u) : Operand::c32(0x3f800000u);
-            } else if (!is_float && idx == 3) {
-               vec->operands[i] = Operand::get_const(ctx->options->gfx_level, 1u, bitsize / 8u);
-            } else {
-               vec->operands[i] = Operand::zero(bitsize / 8u);
-            }
-         }
-         vec->definitions[0] = Definition(dst);
-         ctx->block->instructions.emplace_back(std::move(vec));
-         emit_split_vector(ctx, dst, num_components);
-
-         if (num_temp == num_components)
-            ctx->allocated_vec.emplace(dst.id(), elems);
-      }
-   } else if (ctx->shader->info.stage == MESA_SHADER_FRAGMENT) {
+   if (ctx->shader->info.stage == MESA_SHADER_FRAGMENT) {
       if (!nir_src_is_const(offset) || nir_src_as_uint(offset))
          isel_err(offset.ssa->parent_instr,
                   "Unimplemented non-zero nir_intrinsic_load_input offset");
-- 
GitLab


From a41b4ecc5ca7c269815a4ae8cbb598624c20a9b4 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Thu, 2 Feb 2023 17:57:25 +0100
Subject: [PATCH 06/13] aco: Rename visit_load_input to visit_load_fs_input.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Timur Krist칩f <timur.kristof@gmail.com>
Acked-by: Konstantin Seurer <konstantin.seurer@gmail.com>
---
 .../compiler/aco_instruction_selection.cpp    | 63 +++++++++----------
 1 file changed, 31 insertions(+), 32 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 551134d8d801..8219cc14511c 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -5571,47 +5571,41 @@ mtbuf_load_callback(Builder& bld, const LoadEmitInfo& info, Temp offset, unsigne
 const EmitLoadParameters mtbuf_load_params{mtbuf_load_callback, false, true, 4096};
 
 void
-visit_load_input(isel_context* ctx, nir_intrinsic_instr* instr)
+visit_load_fs_input(isel_context* ctx, nir_intrinsic_instr* instr)
 {
    Builder bld(ctx->program, ctx->block);
    Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
    nir_src offset = *nir_get_io_offset_src(instr);
 
-   if (ctx->shader->info.stage == MESA_SHADER_FRAGMENT) {
-      if (!nir_src_is_const(offset) || nir_src_as_uint(offset))
-         isel_err(offset.ssa->parent_instr,
-                  "Unimplemented non-zero nir_intrinsic_load_input offset");
+   if (!nir_src_is_const(offset) || nir_src_as_uint(offset))
+      isel_err(offset.ssa->parent_instr, "Unimplemented non-zero nir_intrinsic_load_input offset");
 
-      Temp prim_mask = get_arg(ctx, ctx->args->ac.prim_mask);
+   Temp prim_mask = get_arg(ctx, ctx->args->ac.prim_mask);
 
-      unsigned idx = nir_intrinsic_base(instr);
-      unsigned component = nir_intrinsic_component(instr);
-      unsigned vertex_id = 0; /* P0 */
+   unsigned idx = nir_intrinsic_base(instr);
+   unsigned component = nir_intrinsic_component(instr);
+   unsigned vertex_id = 0; /* P0 */
 
-      if (instr->intrinsic == nir_intrinsic_load_input_vertex)
-         vertex_id = nir_src_as_uint(instr->src[0]);
+   if (instr->intrinsic == nir_intrinsic_load_input_vertex)
+      vertex_id = nir_src_as_uint(instr->src[0]);
 
-      if (instr->dest.ssa.num_components == 1 &&
-          instr->dest.ssa.bit_size != 64) {
-         emit_interp_mov_instr(ctx, idx, component, vertex_id, dst, prim_mask);
-      } else {
-         unsigned num_components = instr->dest.ssa.num_components;
-         if (instr->dest.ssa.bit_size == 64)
-            num_components *= 2;
-         aco_ptr<Pseudo_instruction> vec{create_instruction<Pseudo_instruction>(
-            aco_opcode::p_create_vector, Format::PSEUDO, num_components, 1)};
-         for (unsigned i = 0; i < num_components; i++) {
-            unsigned chan_component = (component + i) % 4;
-            unsigned chan_idx = idx + (component + i) / 4;
-            vec->operands[i] = Operand(bld.tmp(instr->dest.ssa.bit_size == 16 ? v2b : v1));
-            emit_interp_mov_instr(ctx, chan_idx, chan_component, vertex_id,
-                                  vec->operands[i].getTemp(), prim_mask);
-         }
-         vec->definitions[0] = Definition(dst);
-         bld.insert(std::move(vec));
-      }
+   if (instr->dest.ssa.num_components == 1 && instr->dest.ssa.bit_size != 64) {
+      emit_interp_mov_instr(ctx, idx, component, vertex_id, dst, prim_mask);
    } else {
-      unreachable("Shader stage not implemented");
+      unsigned num_components = instr->dest.ssa.num_components;
+      if (instr->dest.ssa.bit_size == 64)
+         num_components *= 2;
+      aco_ptr<Pseudo_instruction> vec{create_instruction<Pseudo_instruction>(
+         aco_opcode::p_create_vector, Format::PSEUDO, num_components, 1)};
+      for (unsigned i = 0; i < num_components; i++) {
+         unsigned chan_component = (component + i) % 4;
+         unsigned chan_idx = idx + (component + i) / 4;
+         vec->operands[i] = Operand(bld.tmp(instr->dest.ssa.bit_size == 16 ? v2b : v1));
+         emit_interp_mov_instr(ctx, chan_idx, chan_component, vertex_id, vec->operands[i].getTemp(),
+                               prim_mask);
+      }
+      vec->definitions[0] = Definition(dst);
+      bld.insert(std::move(vec));
    }
 }
 
@@ -8104,7 +8098,12 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
    case nir_intrinsic_load_interpolated_input: visit_load_interpolated_input(ctx, instr); break;
    case nir_intrinsic_store_output: visit_store_output(ctx, instr); break;
    case nir_intrinsic_load_input:
-   case nir_intrinsic_load_input_vertex: visit_load_input(ctx, instr); break;
+   case nir_intrinsic_load_input_vertex:
+      if (ctx->program->stage == fragment_fs)
+         visit_load_fs_input(ctx, instr);
+      else
+         isel_err(&instr->instr, "Shader inputs should have been lowered in NIR.");
+      break;
    case nir_intrinsic_load_per_vertex_input: visit_load_per_vertex_input(ctx, instr); break;
    case nir_intrinsic_load_ubo: visit_load_ubo(ctx, instr); break;
    case nir_intrinsic_load_push_constant: visit_load_push_constant(ctx, instr); break;
-- 
GitLab


From 5cab0697b105df175a27e3e13fcfc7aa24fe919e Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Thu, 2 Mar 2023 17:09:14 -0800
Subject: [PATCH 07/13] aco: Remove vtx_binding from MUBUF/MTBUF instructions.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

They are not used anymore.

Signed-off-by: Timur Krist칩f <timur.kristof@gmail.com>
---
 src/amd/compiler/aco_ir.cpp                 |  8 -------
 src/amd/compiler/aco_ir.h                   |  2 --
 src/amd/compiler/tests/test_hard_clause.cpp | 26 ++++-----------------
 3 files changed, 4 insertions(+), 32 deletions(-)

diff --git a/src/amd/compiler/aco_ir.cpp b/src/amd/compiler/aco_ir.cpp
index 4652af8ddb92..9013bcf7acc6 100644
--- a/src/amd/compiler/aco_ir.cpp
+++ b/src/amd/compiler/aco_ir.cpp
@@ -1050,14 +1050,6 @@ wait_imm::empty() const
 bool
 should_form_clause(const Instruction* a, const Instruction* b)
 {
-   /* Vertex attribute loads from the same binding likely load from similar addresses */
-   unsigned a_vtx_binding =
-      a->isMUBUF() ? a->mubuf().vtx_binding : (a->isMTBUF() ? a->mtbuf().vtx_binding : 0);
-   unsigned b_vtx_binding =
-      b->isMUBUF() ? b->mubuf().vtx_binding : (b->isMTBUF() ? b->mtbuf().vtx_binding : 0);
-   if (a_vtx_binding && a_vtx_binding == b_vtx_binding)
-      return true;
-
    if (a->format != b->format)
       return false;
 
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index f57ba4a92688..d49fd0f10a25 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -1625,7 +1625,6 @@ struct MUBUF_instruction : public Instruction {
    uint16_t offset : 12;     /* Unsigned byte offset - 12 bit */
    uint16_t swizzled : 1;
    uint16_t padding0 : 2;
-   uint16_t vtx_binding : 6; /* 0 if this is not a vertex attribute load */
    uint16_t padding1 : 10;
 };
 static_assert(sizeof(MUBUF_instruction) == sizeof(Instruction) + 8, "Unexpected padding");
@@ -1649,7 +1648,6 @@ struct MTBUF_instruction : public Instruction {
    uint16_t slc : 1;         /* system level coherent */
    uint16_t tfe : 1;         /* texture fail enable */
    uint16_t disable_wqm : 1; /* Require an exec mask without helper invocations */
-   uint16_t vtx_binding : 6; /* 0 if this is not a vertex attribute load */
    uint16_t padding : 4;
    uint16_t offset; /* Unsigned byte offset - 12 bit */
 };
diff --git a/src/amd/compiler/tests/test_hard_clause.cpp b/src/amd/compiler/tests/test_hard_clause.cpp
index 2a2f1e56956f..a9eb4ee76fac 100644
--- a/src/amd/compiler/tests/test_hard_clause.cpp
+++ b/src/amd/compiler/tests/test_hard_clause.cpp
@@ -26,13 +26,12 @@
 
 using namespace aco;
 
-static void create_mubuf(Temp desc=Temp(0, s8), unsigned vtx_binding=0)
+static void create_mubuf(Temp desc=Temp(0, s8))
 {
    Operand desc_op(desc);
    desc_op.setFixed(PhysReg(0));
    bld.mubuf(aco_opcode::buffer_load_dword, Definition(PhysReg(256), v1), desc_op,
-             Operand(PhysReg(256), v1), Operand::zero(), 0, false)
-      ->mubuf().vtx_binding = vtx_binding;
+             Operand(PhysReg(256), v1), Operand::zero(), 0, false);
 }
 
 static void create_mubuf_store()
@@ -41,14 +40,13 @@ static void create_mubuf_store()
              Operand(PhysReg(256), v1), Operand::zero(), 0, false);
 }
 
-static void create_mtbuf(Temp desc=Temp(0, s8), unsigned vtx_binding=0)
+static void create_mtbuf(Temp desc=Temp(0, s8))
 {
    Operand desc_op(desc);
    desc_op.setFixed(PhysReg(0));
    bld.mtbuf(aco_opcode::tbuffer_load_format_x, Definition(PhysReg(256), v1), desc_op,
              Operand(PhysReg(256), v1), Operand::zero(), V_008F0C_BUF_DATA_FORMAT_32,
-             V_008F0C_BUF_NUM_FORMAT_FLOAT, 0, false)
-      ->mtbuf().vtx_binding = vtx_binding;
+             V_008F0C_BUF_NUM_FORMAT_FLOAT, 0, false);
 }
 
 static void create_flat()
@@ -305,22 +303,6 @@ BEGIN_TEST(form_hard_clauses.heuristic)
    create_mubuf(buf_desc0);
    create_mtbuf(buf_desc0);
 
-   //>> p_unit_test 8
-   //! s_clause imm:1
-   //; search_re('buffer_load_dword')
-   //; search_re('tbuffer_load_format_x')
-   bld.pseudo(aco_opcode::p_unit_test, Operand::c32(8u));
-   create_mubuf(buf_desc0, 1);
-   create_mtbuf(buf_desc0, 1);
-
-   //>> p_unit_test 9
-   //! s_clause imm:1
-   //; search_re('buffer_load_dword')
-   //; search_re('tbuffer_load_format_x')
-   bld.pseudo(aco_opcode::p_unit_test, Operand::c32(9u));
-   create_mubuf(buf_desc0, 1);
-   create_mtbuf(buf_desc1, 1);
-
    finish_form_hard_clause_test();
 END_TEST
 
-- 
GitLab


From 2b393549b92194e47b1b362a7a88ce050858b2d9 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Fri, 10 Feb 2023 23:48:33 +0100
Subject: [PATCH 08/13] radv: Remove VS inputs code from LLVM backend.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Timur Krist칩f <timur.kristof@gmail.com>
Acked-by: Konstantin Seurer <konstantin.seurer@gmail.com>
---
 src/amd/vulkan/radv_nir_to_llvm.c | 174 ------------------------------
 1 file changed, 174 deletions(-)

diff --git a/src/amd/vulkan/radv_nir_to_llvm.c b/src/amd/vulkan/radv_nir_to_llvm.c
index 7f162bc85d8e..502615d7ca67 100644
--- a/src/amd/vulkan/radv_nir_to_llvm.c
+++ b/src/amd/vulkan/radv_nir_to_llvm.c
@@ -255,178 +255,6 @@ radv_get_sampler_desc(struct ac_shader_abi *abi, LLVMValueRef index,
    return radv_load_rsrc(ctx, index, v4 ? ctx->ac.v4i32 : ctx->ac.v8i32);
 }
 
-static LLVMValueRef
-radv_fixup_vertex_input_fetches(struct radv_shader_context *ctx, LLVMValueRef value,
-                                unsigned num_channels, bool is_float, bool is_64bit)
-{
-   LLVMValueRef zero = is_64bit ? ctx->ac.i64_0 : (is_float ? ctx->ac.f32_0 : ctx->ac.i32_0);
-   LLVMValueRef one = is_64bit ? ctx->ac.i64_0 : (is_float ? ctx->ac.f32_1 : ctx->ac.i32_1);
-   LLVMValueRef chan[4];
-
-   if (LLVMGetTypeKind(LLVMTypeOf(value)) == LLVMVectorTypeKind) {
-      unsigned vec_size = LLVMGetVectorSize(LLVMTypeOf(value));
-
-      if (num_channels == 4 && num_channels == vec_size)
-         return value;
-
-      num_channels = MIN2(num_channels, vec_size);
-
-      for (unsigned i = 0; i < num_channels; i++)
-         chan[i] = ac_llvm_extract_elem(&ctx->ac, value, i);
-   } else {
-      assert(num_channels == 1);
-      chan[0] = value;
-   }
-
-   for (unsigned i = num_channels; i < 4; i++) {
-      chan[i] = i == 3 ? one : zero;
-      chan[i] = ac_to_integer(&ctx->ac, chan[i]);
-   }
-
-   return ac_build_gather_values(&ctx->ac, chan, 4);
-}
-
-static void
-load_vs_input(struct radv_shader_context *ctx, unsigned driver_location, LLVMTypeRef dest_type,
-              LLVMValueRef out[4])
-{
-   struct ac_llvm_pointer t_list_ptr = ac_get_ptr_arg(&ctx->ac, &ctx->args->ac, ctx->args->ac.vertex_buffers);
-   LLVMValueRef t_offset;
-   LLVMValueRef t_list;
-   LLVMValueRef input;
-   LLVMValueRef buffer_index;
-   unsigned attrib_index = driver_location - VERT_ATTRIB_GENERIC0;
-   enum pipe_format attrib_format = ctx->options->key.vs.vertex_attribute_formats[attrib_index];
-   const struct util_format_description *desc = util_format_description(attrib_format);
-   bool is_float = !desc->channel[0].pure_integer;
-   uint8_t input_usage_mask =
-      ctx->shader_info->vs.input_usage_mask[driver_location];
-   unsigned num_input_channels = util_last_bit(input_usage_mask);
-
-   if (ctx->options->key.vs.instance_rate_inputs & (1u << attrib_index)) {
-      uint32_t divisor = ctx->options->key.vs.instance_rate_divisors[attrib_index];
-
-      if (divisor) {
-         buffer_index = ctx->abi.instance_id;
-
-         if (divisor != 1) {
-            buffer_index = LLVMBuildUDiv(ctx->ac.builder, buffer_index,
-                                         LLVMConstInt(ctx->ac.i32, divisor, 0), "");
-         }
-      } else {
-         buffer_index = ctx->ac.i32_0;
-      }
-
-      buffer_index = LLVMBuildAdd(
-         ctx->ac.builder, ac_get_arg(&ctx->ac, ctx->args->ac.start_instance), buffer_index, "");
-   } else {
-      buffer_index = LLVMBuildAdd(ctx->ac.builder, ctx->abi.vertex_id,
-                                  ac_get_arg(&ctx->ac, ctx->args->ac.base_vertex), "");
-   }
-
-   const struct ac_vtx_format_info *vtx_info =
-      ac_get_vtx_format_info(GFX8, CHIP_POLARIS10, attrib_format);
-
-   /* Adjust the number of channels to load based on the vertex attribute format. */
-   unsigned num_channels = MIN2(num_input_channels, vtx_info->num_channels);
-   unsigned attrib_binding = ctx->options->key.vs.vertex_attribute_bindings[attrib_index];
-   unsigned attrib_offset = ctx->options->key.vs.vertex_attribute_offsets[attrib_index];
-   unsigned attrib_stride = ctx->options->key.vs.vertex_attribute_strides[attrib_index];
-
-   unsigned data_format = vtx_info->hw_format[num_channels - 1] & 0xf;
-   unsigned num_format = vtx_info->hw_format[0] >> 4;
-
-   unsigned desc_index =
-      ctx->shader_info->vs.use_per_attribute_vb_descs ? attrib_index : attrib_binding;
-   desc_index = util_bitcount(ctx->shader_info->vs.vb_desc_usage_mask &
-                              u_bit_consecutive(0, desc_index));
-   t_offset = LLVMConstInt(ctx->ac.i32, desc_index, false);
-   t_list = ac_build_load_to_sgpr(&ctx->ac, t_list_ptr, t_offset);
-
-   /* Always split typed vertex buffer loads on GFX6 and GFX10+ to avoid any alignment issues that
-    * triggers memory violations and eventually a GPU hang. This can happen if the stride (static or
-    * dynamic) is unaligned and also if the VBO offset is aligned to a scalar (eg. stride is 8 and
-    * VBO offset is 2 for R16G16B16A16_SNORM).
-    */
-   unsigned chan_dwords = vtx_info->chan_byte_size == 8 ? 2 : 1;
-   if (((ctx->ac.gfx_level == GFX6 || ctx->ac.gfx_level >= GFX10) && vtx_info->chan_byte_size) ||
-       !(vtx_info->has_hw_format & BITFIELD_BIT(vtx_info->num_channels - 1)) ||
-       vtx_info->element_size > 16) {
-      unsigned chan_format = vtx_info->hw_format[0] & 0xf;
-      LLVMValueRef values[4];
-
-      for (unsigned chan = 0; chan < num_channels; chan++) {
-         unsigned chan_offset = attrib_offset + chan * vtx_info->chan_byte_size;
-         LLVMValueRef chan_index = buffer_index;
-
-         if (attrib_stride != 0 && chan_offset > attrib_stride) {
-            LLVMValueRef buffer_offset =
-               LLVMConstInt(ctx->ac.i32, chan_offset / attrib_stride, false);
-
-            chan_index = LLVMBuildAdd(ctx->ac.builder, buffer_index, buffer_offset, "");
-
-            chan_offset = chan_offset % attrib_stride;
-         }
-
-         values[chan] = ac_build_struct_tbuffer_load(
-            &ctx->ac, t_list, chan_index, LLVMConstInt(ctx->ac.i32, chan_offset, false),
-            ctx->ac.i32_0, chan_dwords, chan_format, num_format, 0, true);
-      }
-
-      input = ac_build_gather_values(&ctx->ac, values, num_channels);
-   } else {
-      if (attrib_stride != 0 && attrib_offset > attrib_stride) {
-         LLVMValueRef buffer_offset =
-            LLVMConstInt(ctx->ac.i32, attrib_offset / attrib_stride, false);
-
-         buffer_index = LLVMBuildAdd(ctx->ac.builder, buffer_index, buffer_offset, "");
-
-         attrib_offset = attrib_offset % attrib_stride;
-      }
-
-      input = ac_build_struct_tbuffer_load(
-         &ctx->ac, t_list, buffer_index, LLVMConstInt(ctx->ac.i32, attrib_offset, false),
-         ctx->ac.i32_0, num_channels * chan_dwords, data_format, num_format, 0, true);
-   }
-
-   if (vtx_info->chan_byte_size == 8)
-      input =
-         LLVMBuildBitCast(ctx->ac.builder, input, LLVMVectorType(ctx->ac.i64, num_channels), "");
-
-   input = radv_fixup_vertex_input_fetches(ctx, input, num_channels, is_float,
-                                           vtx_info->chan_byte_size == 8);
-
-   for (unsigned chan = 0; chan < 4; chan++) {
-      LLVMValueRef llvm_chan = LLVMConstInt(ctx->ac.i32, chan, false);
-      out[chan] = LLVMBuildExtractElement(ctx->ac.builder, input, llvm_chan, "");
-      if (dest_type == ctx->ac.i16 && is_float) {
-         out[chan] = LLVMBuildBitCast(ctx->ac.builder, out[chan], ctx->ac.f32, "");
-         out[chan] = LLVMBuildFPTrunc(ctx->ac.builder, out[chan], ctx->ac.f16, "");
-      }
-   }
-
-   for (unsigned chan = 0; chan < 4; chan++) {
-      out[chan] = ac_to_integer(&ctx->ac, out[chan]);
-      if (dest_type == ctx->ac.i16 && !is_float)
-         out[chan] = LLVMBuildTrunc(ctx->ac.builder, out[chan], ctx->ac.i16, "");
-   }
-}
-
-static LLVMValueRef
-radv_load_vs_inputs(struct ac_shader_abi *abi, unsigned driver_location, unsigned component,
-                    unsigned num_components, unsigned vertex_index, LLVMTypeRef type)
-{
-   struct radv_shader_context *ctx = radv_shader_context_from_abi(abi);
-   LLVMValueRef values[4];
-
-   load_vs_input(ctx, driver_location, type, values);
-
-   for (unsigned i = 0; i < 4; i++)
-      values[i] = LLVMBuildBitCast(ctx->ac.builder, values[i], type, "");
-
-   return ac_build_varying_gather_values(&ctx->ac, values, num_components, component);
-}
-
 static void
 prepare_interp_optimize(struct radv_shader_context *ctx, struct nir_shader *nir)
 {
@@ -958,8 +786,6 @@ ac_translate_nir_to_llvm(struct ac_llvm_compiler *ac_llvm,
       if (shaders[shader_idx]->info.stage == MESA_SHADER_GEOMETRY && !ctx.shader_info->is_ngg) {
          ctx.abi.emit_vertex_with_counter = visit_emit_vertex_with_counter;
          ctx.abi.emit_primitive = visit_end_primitive;
-      } else if (shaders[shader_idx]->info.stage == MESA_SHADER_VERTEX) {
-         ctx.abi.load_inputs = radv_load_vs_inputs;
       }
 
       if (shader_idx && !(shaders[shader_idx]->info.stage == MESA_SHADER_GEOMETRY && info->is_ngg)) {
-- 
GitLab


From 599e07e7f0066172d2ea9d8f4306c5c9db2af20f Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Mon, 13 Feb 2023 11:35:02 +0100
Subject: [PATCH 09/13] ac/llvm: Remove unused function
 ac_build_struct_tbuffer_load.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Timur Krist칩f <timur.kristof@gmail.com>
Reviewed-by: Qiang Yu <yuq825@gmail.com>
Reviewed-by: Marek Ol코치k <marek.olsak@amd.com>
Acked-by: Konstantin Seurer <konstantin.seurer@gmail.com>
---
 src/amd/llvm/ac_llvm_build.c | 11 -----------
 src/amd/llvm/ac_llvm_build.h |  6 ------
 2 files changed, 17 deletions(-)

diff --git a/src/amd/llvm/ac_llvm_build.c b/src/amd/llvm/ac_llvm_build.c
index 6e51da7802f8..39c7e9621612 100644
--- a/src/amd/llvm/ac_llvm_build.c
+++ b/src/amd/llvm/ac_llvm_build.c
@@ -1462,17 +1462,6 @@ static LLVMValueRef ac_build_tbuffer_load(struct ac_llvm_context *ctx, LLVMValue
                              can_speculate ? AC_ATTR_INVARIANT_LOAD : 0);
 }
 
-LLVMValueRef ac_build_struct_tbuffer_load(struct ac_llvm_context *ctx, LLVMValueRef rsrc,
-                                          LLVMValueRef vindex, LLVMValueRef voffset,
-                                          LLVMValueRef soffset, unsigned num_channels,
-                                          unsigned dfmt, unsigned nfmt, unsigned cache_policy,
-                                          bool can_speculate)
-{
-   unsigned fmt = ac_get_tbuffer_format(ctx->gfx_level, dfmt, nfmt);
-   return ac_build_tbuffer_load(ctx, rsrc, vindex, voffset, soffset, num_channels, fmt,
-                                ctx->i32, cache_policy, can_speculate);
-}
-
 LLVMValueRef ac_build_safe_tbuffer_load(struct ac_llvm_context *ctx, LLVMValueRef rsrc,
                                         LLVMValueRef vidx, LLVMValueRef base_voffset,
                                         LLVMValueRef soffset, LLVMTypeRef channel_type,
diff --git a/src/amd/llvm/ac_llvm_build.h b/src/amd/llvm/ac_llvm_build.h
index 5b1c1bbcd413..197a30bb1543 100644
--- a/src/amd/llvm/ac_llvm_build.h
+++ b/src/amd/llvm/ac_llvm_build.h
@@ -304,12 +304,6 @@ LLVMValueRef ac_build_buffer_load_byte(struct ac_llvm_context *ctx, LLVMValueRef
                                        LLVMValueRef voffset, LLVMValueRef soffset,
                                        unsigned cache_policy);
 
-LLVMValueRef ac_build_struct_tbuffer_load(struct ac_llvm_context *ctx, LLVMValueRef rsrc,
-                                          LLVMValueRef vindex, LLVMValueRef voffset,
-                                          LLVMValueRef soffset, unsigned num_channels,
-                                          unsigned dfmt, unsigned nfmt, unsigned cache_policy,
-                                          bool can_speculate);
-
 LLVMValueRef ac_build_safe_tbuffer_load(struct ac_llvm_context *ctx, LLVMValueRef rsrc,
                                         LLVMValueRef vindex, LLVMValueRef voffset,
                                         LLVMValueRef soffset, LLVMTypeRef channel_type,
-- 
GitLab


From a54c3cd0b83b6ae8b1db32abe74078e769468d78 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Thu, 2 Mar 2023 17:29:26 -0800
Subject: [PATCH 10/13] ac: Add needs_vmcnt field to args.

---
 src/amd/common/ac_shader_args.h | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/src/amd/common/ac_shader_args.h b/src/amd/common/ac_shader_args.h
index eed7dc340b6d..6ccc8176a2ad 100644
--- a/src/amd/common/ac_shader_args.h
+++ b/src/amd/common/ac_shader_args.h
@@ -64,7 +64,8 @@ struct ac_shader_args {
       enum ac_arg_regfile file;
       uint8_t offset;
       uint8_t size;
-      bool skip;
+      bool skip : 1;
+      bool needs_vmcnt : 1;
    } args[AC_MAX_ARGS];
 
    uint16_t arg_count;
-- 
GitLab


From cfaa99ec11f7d1cb7c542b5d13dda062c4bd2f65 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Thu, 2 Mar 2023 17:29:53 -0800
Subject: [PATCH 11/13] radv: Set needs_vmcnt on dynamic VS input args.

---
 src/amd/vulkan/radv_shader_args.c | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/src/amd/vulkan/radv_shader_args.c b/src/amd/vulkan/radv_shader_args.c
index a969f9d00be8..049283786031 100644
--- a/src/amd/vulkan/radv_shader_args.c
+++ b/src/amd/vulkan/radv_shader_args.c
@@ -364,8 +364,10 @@ declare_vs_input_vgprs(enum amd_gfx_level gfx_level, const struct radv_shader_in
    if (info->vs.dynamic_inputs) {
       assert(info->vs.use_per_attribute_vb_descs);
       unsigned num_attributes = util_last_bit(info->vs.input_slot_usage_mask);
-      for (unsigned i = 0; i < num_attributes; i++)
+      for (unsigned i = 0; i < num_attributes; i++) {
          ac_add_arg(&args->ac, AC_ARG_VGPR, 4, AC_ARG_INT, &args->vs_inputs[i]);
+         args->ac.args[args->vs_inputs[i].arg_index].needs_vmcnt = true;
+      }
       /* Ensure the main shader doesn't use less vgprs than the prolog. The prolog requires one
        * VGPR more than the number of shader arguments in the case of non-trivial divisors on GFX8.
        */
-- 
GitLab


From afc6ec7f5edd841e502c4b70af3bd15c14dde0b9 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Thu, 2 Mar 2023 17:30:49 -0800
Subject: [PATCH 12/13] aco: Generalize vs_inputs to vmcnt_args.

---
 src/amd/compiler/aco_insert_waitcnt.cpp       |  2 +-
 .../compiler/aco_instruction_selection.cpp    | 21 ++++++-------------
 src/amd/compiler/aco_ir.h                     |  2 +-
 src/amd/compiler/aco_statistics.cpp           | 10 ++++-----
 4 files changed, 13 insertions(+), 22 deletions(-)

diff --git a/src/amd/compiler/aco_insert_waitcnt.cpp b/src/amd/compiler/aco_insert_waitcnt.cpp
index b7edf0446294..e0d019c9cb4f 100644
--- a/src/amd/compiler/aco_insert_waitcnt.cpp
+++ b/src/amd/compiler/aco_insert_waitcnt.cpp
@@ -1033,7 +1033,7 @@ insert_wait_states(Program* program)
    unsigned loop_progress = 0;
 
    if (program->stage.has(SWStage::VS) && program->info.vs.dynamic_inputs) {
-      for (Definition def : program->vs_inputs) {
+      for (Definition def : program->vmcnt_args) {
          update_counters(in_ctx[0], event_vmem);
          insert_wait_entry(in_ctx[0], def, event_vmem);
       }
diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 8219cc14511c..abecbae3b71f 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -11131,10 +11131,13 @@ add_startpgm(struct isel_context* ctx)
          ctx->arg_temps[i] = create_vec_from_array(ctx, elems, size, RegType::sgpr, 4);
       } else {
          Temp dst = ctx->program->allocateTmp(type);
+         Definition def(dst);
+         def.setFixed(PhysReg{file == AC_ARG_SGPR ? reg : reg + 256});
          ctx->arg_temps[i] = dst;
-         startpgm->definitions[arg] = Definition(dst);
-         startpgm->definitions[arg].setFixed(PhysReg{file == AC_ARG_SGPR ? reg : reg + 256});
-         arg++;
+         startpgm->definitions[arg++] = def;
+
+         if (ctx->args->ac.args[i].needs_vmcnt)
+            ctx->program->vmcnt_args.push_back(def);
       }
    }
 
@@ -11154,18 +11157,6 @@ add_startpgm(struct isel_context* ctx)
       }
    }
 
-   if (ctx->stage.has(SWStage::VS) && ctx->program->info.vs.dynamic_inputs) {
-      unsigned num_attributes = util_last_bit(ctx->program->info.vs.input_slot_usage_mask);
-      for (unsigned i = 0; i < num_attributes; i++) {
-         Definition def(get_arg(ctx, ctx->args->vs_inputs[i]));
-
-         unsigned idx = ctx->args->vs_inputs[i].arg_index;
-         def.setFixed(PhysReg(256 + ctx->args->ac.args[idx].offset));
-
-         ctx->program->vs_inputs.push_back(def);
-      }
-   }
-
    return startpgm;
 }
 
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index d49fd0f10a25..7c4d63e5b02e 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -2181,7 +2181,7 @@ public:
    unsigned next_divergent_if_logical_depth = 0;
    unsigned next_uniform_if_depth = 0;
 
-   std::vector<Definition> vs_inputs;
+   std::vector<Definition> vmcnt_args;
 
    struct {
       FILE* output = stderr;
diff --git a/src/amd/compiler/aco_statistics.cpp b/src/amd/compiler/aco_statistics.cpp
index 5662bc7af4c8..80beadfba1a9 100644
--- a/src/amd/compiler/aco_statistics.cpp
+++ b/src/amd/compiler/aco_statistics.cpp
@@ -544,12 +544,12 @@ collect_preasm_stats(Program* program)
    double usage[(int)BlockCycleEstimator::resource_count] = {0};
    std::vector<BlockCycleEstimator> blocks(program->blocks.size(), program);
 
-   if (program->stage.has(SWStage::VS) && program->info.vs.has_prolog) {
-      unsigned vs_input_latency = 320;
-      for (Definition def : program->vs_inputs) {
-         blocks[0].vm.push_back(vs_input_latency);
+   if (program->vmcnt_args.size()) {
+      const unsigned vmem_latency = 320;
+      for (const Definition def : program->vmcnt_args) {
+         blocks[0].vm.push_back(vmem_latency);
          for (unsigned i = 0; i < def.size(); i++)
-            blocks[0].reg_available[def.physReg().reg() + i] = vs_input_latency;
+            blocks[0].reg_available[def.physReg().reg() + i] = vmem_latency;
       }
    }
 
-- 
GitLab


From 621c8a45eac8eada041bd4f086022089ee8f2add Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Thu, 2 Feb 2023 18:06:16 +0100
Subject: [PATCH 13/13] aco, radv: Remove VS IO information from ACO.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Timur Krist칩f <timur.kristof@gmail.com>
Acked-by: Konstantin Seurer <konstantin.seurer@gmail.com>
---
 .../aco_instruction_selection_setup.cpp       | 42 -------------------
 src/amd/compiler/aco_shader_info.h            | 27 ------------
 src/amd/vulkan/radv_aco_shader_info.h         | 29 -------------
 3 files changed, 98 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection_setup.cpp b/src/amd/compiler/aco_instruction_selection_setup.cpp
index ca43726a2d95..860c8bd056f0 100644
--- a/src/amd/compiler/aco_instruction_selection_setup.cpp
+++ b/src/amd/compiler/aco_instruction_selection_setup.cpp
@@ -249,25 +249,9 @@ get_reg_class(isel_context* ctx, RegType type, unsigned components, unsigned bit
       return RegClass::get(type, components * bitsize / 8u);
 }
 
-void
-setup_vs_output_info(isel_context* ctx, nir_shader* nir)
-{
-   const aco_vp_output_info* outinfo = &ctx->program->info.outinfo;
-
-   ctx->export_clip_dists = outinfo->export_clip_dists;
-   ctx->num_clip_distances = util_bitcount(outinfo->clip_dist_mask);
-   ctx->num_cull_distances = util_bitcount(outinfo->cull_dist_mask);
-
-   assert(ctx->num_clip_distances + ctx->num_cull_distances <= 8);
-}
-
 void
 setup_vs_variables(isel_context* ctx, nir_shader* nir)
 {
-   if (ctx->stage == vertex_vs || ctx->stage == vertex_ngg) {
-      setup_vs_output_info(ctx, nir);
-   }
-
    if (ctx->stage == vertex_ngg) {
       ctx->program->config->lds_size =
          DIV_ROUND_UP(nir->info.shared_size, ctx->program->dev.lds_encoding_granule);
@@ -283,8 +267,6 @@ setup_gs_variables(isel_context* ctx, nir_shader* nir)
       ctx->program->config->lds_size =
          ctx->program->info.gfx9_gs_ring_lds_size; /* Already in units of the alloc granularity */
    } else if (ctx->stage == vertex_geometry_ngg || ctx->stage == tess_eval_geometry_ngg) {
-      setup_vs_output_info(ctx, nir);
-
       ctx->program->config->lds_size =
          DIV_ROUND_UP(nir->info.shared_size, ctx->program->dev.lds_encoding_granule);
    }
@@ -301,10 +283,6 @@ setup_tcs_info(isel_context* ctx, nir_shader* nir, nir_shader* vs)
 void
 setup_tes_variables(isel_context* ctx, nir_shader* nir)
 {
-   if (ctx->stage == tess_eval_vs || ctx->stage == tess_eval_ngg) {
-      setup_vs_output_info(ctx, nir);
-   }
-
    if (ctx->stage == tess_eval_ngg) {
       ctx->program->config->lds_size =
          DIV_ROUND_UP(nir->info.shared_size, ctx->program->dev.lds_encoding_granule);
@@ -316,8 +294,6 @@ setup_tes_variables(isel_context* ctx, nir_shader* nir)
 void
 setup_ms_variables(isel_context* ctx, nir_shader* nir)
 {
-   setup_vs_output_info(ctx, nir);
-
    ctx->program->config->lds_size =
       DIV_ROUND_UP(nir->info.shared_size, ctx->program->dev.lds_encoding_granule);
    assert((ctx->program->config->lds_size * ctx->program->dev.lds_encoding_granule) < (32 * 1024));
@@ -399,24 +375,6 @@ init_context(isel_context* ctx, nir_shader* shader)
    ctx->ub_config.max_workgroup_size[0] = 2048;
    ctx->ub_config.max_workgroup_size[1] = 2048;
    ctx->ub_config.max_workgroup_size[2] = 2048;
-   for (unsigned i = 0; i < MAX_VERTEX_ATTRIBS; i++) {
-      pipe_format format = (pipe_format)ctx->options->key.vs.vertex_attribute_formats[i];
-      const struct util_format_description* desc = util_format_description(format);
-
-      uint32_t max;
-      if (desc->channel[0].type != UTIL_FORMAT_TYPE_UNSIGNED) {
-         max = UINT32_MAX;
-      } else if (desc->channel[0].normalized) {
-         max = 0x3f800000u;
-      } else {
-         max = 0;
-         for (unsigned j = 0; j < desc->nr_channels; j++) {
-            uint32_t chan_max = u_uintN_max(desc->channel[0].size);
-            max = MAX2(max, desc->channel[j].pure_integer ? chan_max : fui(chan_max));
-         }
-      }
-      ctx->ub_config.vertex_attrib_max[i] = max;
-   }
 
    nir_divergence_analysis(shader);
    nir_opt_uniform_atomics(shader);
diff --git a/src/amd/compiler/aco_shader_info.h b/src/amd/compiler/aco_shader_info.h
index a3f1872f4759..6aef9c8f41d0 100644
--- a/src/amd/compiler/aco_shader_info.h
+++ b/src/amd/compiler/aco_shader_info.h
@@ -60,30 +60,12 @@ struct aco_vs_prolog_key {
    gl_shader_stage next_stage;
 };
 
-struct aco_vp_output_info {
-   uint8_t vs_output_param_offset[VARYING_SLOT_MAX];
-   uint8_t clip_dist_mask;
-   uint8_t cull_dist_mask;
-   uint8_t param_exports;
-   uint8_t prim_param_exports;
-   bool writes_pointsize;
-   bool writes_layer;
-   bool writes_layer_per_primitive;
-   bool writes_viewport_index;
-   bool writes_viewport_index_per_primitive;
-   bool writes_primitive_shading_rate;
-   bool writes_primitive_shading_rate_per_primitive;
-   bool export_prim_id;
-   bool export_clip_dists;
-};
-
 struct aco_shader_info {
    uint8_t wave_size;
    bool is_ngg;
    bool has_ngg_culling;
    bool has_ngg_early_prim_export;
    unsigned workgroup_size;
-   struct aco_vp_output_info outinfo;
    struct {
       bool as_es;
       bool as_ls;
@@ -142,15 +124,6 @@ struct aco_ps_epilog_key {
 struct aco_stage_input {
    uint32_t optimisations_disabled : 1;
    uint32_t image_2d_view_of_3d : 1;
-   struct {
-      uint32_t instance_rate_inputs;
-      uint32_t instance_rate_divisors[ACO_MAX_VERTEX_ATTRIBS];
-      uint8_t vertex_attribute_formats[ACO_MAX_VERTEX_ATTRIBS];
-      uint32_t vertex_attribute_bindings[ACO_MAX_VERTEX_ATTRIBS];
-      uint32_t vertex_attribute_offsets[ACO_MAX_VERTEX_ATTRIBS];
-      uint32_t vertex_attribute_strides[ACO_MAX_VERTEX_ATTRIBS];
-      uint8_t vertex_binding_align[ACO_MAX_VBS];
-   } vs;
 
    struct {
       unsigned tess_input_vertices;
diff --git a/src/amd/vulkan/radv_aco_shader_info.h b/src/amd/vulkan/radv_aco_shader_info.h
index 02ca932e6b54..80203c591485 100644
--- a/src/amd/vulkan/radv_aco_shader_info.h
+++ b/src/amd/vulkan/radv_aco_shader_info.h
@@ -34,27 +34,6 @@
 #define ASSIGN_FIELD(x) aco_info->x = radv->x
 #define ASSIGN_FIELD_CP(x) memcpy(&aco_info->x, &radv->x, sizeof(radv->x))
 
-static inline void
-radv_aco_convert_shader_vp_info(struct aco_vp_output_info *aco_info,
-				const struct radv_vs_output_info *radv)
-{
-   ASSIGN_FIELD_CP(vs_output_param_offset);
-   ASSIGN_FIELD(clip_dist_mask);
-   ASSIGN_FIELD(cull_dist_mask);
-   ASSIGN_FIELD(param_exports);
-   ASSIGN_FIELD(prim_param_exports);
-   ASSIGN_FIELD(writes_pointsize);
-   ASSIGN_FIELD(writes_layer);
-   ASSIGN_FIELD(writes_layer_per_primitive);
-   ASSIGN_FIELD(writes_viewport_index);
-   ASSIGN_FIELD(writes_viewport_index_per_primitive);
-   ASSIGN_FIELD(writes_primitive_shading_rate);
-   ASSIGN_FIELD(writes_primitive_shading_rate_per_primitive);
-   ASSIGN_FIELD(export_prim_id);
-   ASSIGN_FIELD(export_clip_dists);
-   /* don't use export params */
-}
-
 static inline void
 radv_aco_convert_shader_info(struct aco_shader_info *aco_info,
 			     const struct radv_shader_info *radv)
@@ -64,7 +43,6 @@ radv_aco_convert_shader_info(struct aco_shader_info *aco_info,
    ASSIGN_FIELD(has_ngg_culling);
    ASSIGN_FIELD(has_ngg_early_prim_export);
    ASSIGN_FIELD(workgroup_size);
-   radv_aco_convert_shader_vp_info(&aco_info->outinfo, &radv->outinfo);
    ASSIGN_FIELD(vs.as_es);
    ASSIGN_FIELD(vs.as_ls);
    ASSIGN_FIELD(vs.tcs_in_out_eq);
@@ -128,13 +106,6 @@ radv_aco_convert_pipe_key(struct aco_stage_input *aco_info,
    radv_aco_convert_ps_epilog_key(&aco_info->ps.epilog, &radv->ps.epilog);
    ASSIGN_FIELD(optimisations_disabled);
    ASSIGN_FIELD(image_2d_view_of_3d);
-   ASSIGN_FIELD(vs.instance_rate_inputs);
-   ASSIGN_FIELD_CP(vs.instance_rate_divisors);
-   ASSIGN_FIELD_CP(vs.vertex_attribute_formats);
-   ASSIGN_FIELD_CP(vs.vertex_attribute_bindings);
-   ASSIGN_FIELD_CP(vs.vertex_attribute_offsets);
-   ASSIGN_FIELD_CP(vs.vertex_attribute_strides);
-   ASSIGN_FIELD_CP(vs.vertex_binding_align);
    ASSIGN_FIELD(tcs.tess_input_vertices);
    ASSIGN_FIELD(ps.alpha_to_coverage_via_mrtz);
 }
-- 
GitLab

