From fd4f74591a560263aeee742eda5c898755ddf07d Mon Sep 17 00:00:00 2001
From: Bas Nieuwenhuizen <bas@basnieuwenhuizen.nl>
Date: Sat, 15 Jul 2023 19:24:07 +0200
Subject: [PATCH 1/6] nir: Add AMD cooperative matrix intrinsics.

---
 src/compiler/nir/nir_divergence_analysis.c | 1 +
 src/compiler/nir/nir_intrinsics.py         | 3 +++
 2 files changed, 4 insertions(+)

diff --git a/src/compiler/nir/nir_divergence_analysis.c b/src/compiler/nir/nir_divergence_analysis.c
index c5954f59fcb2..eb6f64050995 100644
--- a/src/compiler/nir/nir_divergence_analysis.c
+++ b/src/compiler/nir/nir_divergence_analysis.c
@@ -610,6 +610,7 @@ visit_intrinsic(nir_shader *shader, nir_intrinsic_instr *instr)
    case nir_intrinsic_rq_load:
    case nir_intrinsic_load_ray_triangle_vertex_positions:
    case nir_intrinsic_cmat_extract:
+   case nir_intrinsic_cmat_muladd_amd:
       is_divergent = true;
       break;
 
diff --git a/src/compiler/nir/nir_intrinsics.py b/src/compiler/nir/nir_intrinsics.py
index 50b25ed85b29..49f59ccf0c9b 100644
--- a/src/compiler/nir/nir_intrinsics.py
+++ b/src/compiler/nir/nir_intrinsics.py
@@ -1664,6 +1664,9 @@ system_value("barycentric_optimize_amd", dest_comp=1, bit_sizes=[1])
 intrinsic("strict_wqm_coord_amd", src_comp=[0], dest_comp=0, bit_sizes=[32], indices=[BASE],
           flags=[CAN_ELIMINATE])
 
+intrinsic("cmat_muladd_amd", src_comp=[16, 16, 0], dest_comp=0, bit_sizes=src2,
+          indices=[SATURATE, CMAT_SIGNED_MASK], flags=[CAN_ELIMINATE])
+
 # V3D-specific instrinc for tile buffer color reads.
 #
 # The hardware requires that we read the samples and components of a pixel
-- 
GitLab


From 64953d7873ee6d4c2b9c51ac6b658f82f67939a3 Mon Sep 17 00:00:00 2001
From: Bas Nieuwenhuizen <bas@basnieuwenhuizen.nl>
Date: Sat, 15 Jul 2023 19:49:49 +0200
Subject: [PATCH 2/6] aco: Add WMMA instructions.

---
 .../compiler/aco_instruction_selection.cpp    | 43 +++++++++++++++++++
 .../aco_instruction_selection_setup.cpp       |  3 +-
 src/amd/compiler/aco_ir.h                     | 11 +++++
 src/amd/compiler/aco_opcodes.py               |  6 +++
 src/amd/compiler/aco_optimizer.cpp            | 16 ++++++-
 src/amd/compiler/aco_validate.cpp             |  4 +-
 6 files changed, 78 insertions(+), 5 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 287669f1fb6d..d2ac0c3f56ac 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -8059,6 +8059,48 @@ create_fs_dual_src_export_gfx11(isel_context* ctx, const struct aco_export_mrt*
    ctx->program->has_color_exports = true;
 }
 
+static void
+visit_cmat_muladd(isel_context* ctx, nir_intrinsic_instr* instr)
+{
+   aco_opcode opcode = aco_opcode::num_opcodes;
+   unsigned signed_mask = 0;
+   bool clamp = false;
+
+   switch (instr->src[0].ssa->bit_size) {
+   case 16:
+      switch (instr->def.bit_size) {
+      case 32: opcode = aco_opcode::v_wmma_f32_16x16x16_f16; break;
+      case 16: opcode = aco_opcode::v_wmma_f16_16x16x16_f16; break;
+      }
+      break;
+   case 8:
+      opcode = aco_opcode::v_wmma_i32_16x16x16_iu8;
+      signed_mask = nir_intrinsic_cmat_signed_mask(instr);
+      clamp = nir_intrinsic_saturate(instr);
+      break;
+   }
+
+   if (opcode == aco_opcode::num_opcodes)
+      unreachable("visit_cmat_muladd: invalid bit size combination");
+
+   Builder bld(ctx->program, ctx->block);
+
+   Temp dst = get_ssa_temp(ctx, &instr->def);
+   Operand A(as_vgpr(ctx, get_ssa_temp(ctx, instr->src[0].ssa)));
+   Operand B(as_vgpr(ctx, get_ssa_temp(ctx, instr->src[1].ssa)));
+   Operand C(as_vgpr(ctx, get_ssa_temp(ctx, instr->src[2].ssa)));
+
+   A.setLateKill(true);
+   B.setLateKill(true);
+
+   VALU_instruction& vop3p = bld.vop3p(opcode, Definition(dst), A, B, C, 0, 0)->valu();
+   vop3p.neg_lo[0] = (signed_mask & 0x1) != 0;
+   vop3p.neg_lo[1] = (signed_mask & 0x2) != 0;
+   vop3p.clamp = clamp;
+
+   emit_split_vector(ctx, dst, instr->def.num_components);
+}
+
 void
 visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
 {
@@ -9139,6 +9181,7 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
          bld.pseudo(aco_opcode::p_pops_gfx9_ordered_section_done);
       break;
    }
+   case nir_intrinsic_cmat_muladd_amd: visit_cmat_muladd(ctx, instr); break;
    default:
       isel_err(&instr->instr, "Unimplemented intrinsic instr");
       abort();
diff --git a/src/amd/compiler/aco_instruction_selection_setup.cpp b/src/amd/compiler/aco_instruction_selection_setup.cpp
index 27b318451d79..a94cec7b89df 100644
--- a/src/amd/compiler/aco_instruction_selection_setup.cpp
+++ b/src/amd/compiler/aco_instruction_selection_setup.cpp
@@ -535,7 +535,8 @@ init_context(isel_context* ctx, nir_shader* shader)
                case nir_intrinsic_bvh64_intersect_ray_amd:
                case nir_intrinsic_load_vector_arg_amd:
                case nir_intrinsic_load_rt_dynamic_callable_stack_base_amd:
-               case nir_intrinsic_ordered_xfb_counter_add_amd: type = RegType::vgpr; break;
+               case nir_intrinsic_ordered_xfb_counter_add_amd:
+               case nir_intrinsic_cmat_muladd_amd: type = RegType::vgpr; break;
                case nir_intrinsic_load_shared:
                case nir_intrinsic_load_shared2_amd:
                   /* When the result of these loads is only used by cross-lane instructions,
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index 6d9389eb0ee6..a212572941c8 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -1820,6 +1820,17 @@ is_phi(aco_ptr<Instruction>& instr)
    return is_phi(instr.get());
 }
 
+static inline bool
+is_wmma(aco_ptr<Instruction>& instr)
+{
+   return instr->opcode == aco_opcode::v_wmma_bf16_16x16x16_bf16 ||
+          instr->opcode == aco_opcode::v_wmma_f16_16x16x16_f16 ||
+          instr->opcode == aco_opcode::v_wmma_f32_16x16x16_f16 ||
+          instr->opcode == aco_opcode::v_wmma_f32_16x16x16_bf16 ||
+          instr->opcode == aco_opcode::v_wmma_i32_16x16x16_iu8 ||
+          instr->opcode == aco_opcode::v_wmma_i32_16x16x16_iu4;
+}
+
 memory_sync_info get_sync_info(const Instruction* instr);
 
 inline bool
diff --git a/src/amd/compiler/aco_opcodes.py b/src/amd/compiler/aco_opcodes.py
index dc559198a66e..61a7f1af27e3 100644
--- a/src/amd/compiler/aco_opcodes.py
+++ b/src/amd/compiler/aco_opcodes.py
@@ -1045,6 +1045,12 @@ opcode("v_dot8_i32_iu4", -1, -1, -1, 0x18, Format.VOP3P, InstrClass.Valu32)
 opcode("v_dot8_u32_u4", -1, 0x2b, 0x19, 0x19, Format.VOP3P, InstrClass.Valu32)
 opcode("v_dot2_f32_f16", -1, 0x23, 0x13, 0x13, Format.VOP3P, InstrClass.Valu32)
 opcode("v_dot2_f32_bf16", -1, -1, -1, 0x1a, Format.VOP3P, InstrClass.Valu32)
+opcode("v_wmma_f32_16x16x16_f16", -1, -1, -1, 0x40, Format.VOP3P, InstrClass.Valu32, False, False)
+opcode("v_wmma_f32_16x16x16_bf16", -1, -1, -1, 0x41, Format.VOP3P, InstrClass.Valu32, False, False)
+opcode("v_wmma_f16_16x16x16_f16", -1, -1, -1, 0x42, Format.VOP3P, InstrClass.Valu32, False, False)
+opcode("v_wmma_bf16_16x16x16_bf16", -1, -1, -1, 0x43, Format.VOP3P, InstrClass.Valu32, False, False)
+opcode("v_wmma_i32_16x16x16_iu8", -1, -1, -1, 0x44, Format.VOP3P, InstrClass.Valu32, False, False)
+opcode("v_wmma_i32_16x16x16_iu4", -1, -1, -1, 0x45, Format.VOP3P, InstrClass.Valu32, False, False)
 
 
 # VINTRP (GFX6 - GFX10.3) instructions:
diff --git a/src/amd/compiler/aco_optimizer.cpp b/src/amd/compiler/aco_optimizer.cpp
index dfe7b5c4a789..3320a6e7dd01 100644
--- a/src/amd/compiler/aco_optimizer.cpp
+++ b/src/amd/compiler/aco_optimizer.cpp
@@ -631,7 +631,13 @@ can_apply_sgprs(opt_ctx& ctx, aco_ptr<Instruction>& instr)
           instr->opcode != aco_opcode::v_interp_p10_f16_f32_inreg &&
           instr->opcode != aco_opcode::v_interp_p2_f16_f32_inreg &&
           instr->opcode != aco_opcode::v_interp_p10_rtz_f16_f32_inreg &&
-          instr->opcode != aco_opcode::v_interp_p2_rtz_f16_f32_inreg;
+          instr->opcode != aco_opcode::v_interp_p2_rtz_f16_f32_inreg &&
+          instr->opcode != aco_opcode::v_wmma_f32_16x16x16_f16 &&
+          instr->opcode != aco_opcode::v_wmma_f32_16x16x16_bf16 &&
+          instr->opcode != aco_opcode::v_wmma_f16_16x16x16_f16 &&
+          instr->opcode != aco_opcode::v_wmma_bf16_16x16x16_bf16 &&
+          instr->opcode != aco_opcode::v_wmma_i32_16x16x16_iu8 &&
+          instr->opcode != aco_opcode::v_wmma_i32_16x16x16_iu4;
 }
 
 bool
@@ -685,7 +691,13 @@ alu_can_accept_constant(const aco_ptr<Instruction>& instr, unsigned operand)
    case aco_opcode::v_interp_p10_f16_f32_inreg:
    case aco_opcode::v_interp_p2_f16_f32_inreg:
    case aco_opcode::v_interp_p10_rtz_f16_f32_inreg:
-   case aco_opcode::v_interp_p2_rtz_f16_f32_inreg: return false;
+   case aco_opcode::v_interp_p2_rtz_f16_f32_inreg:
+   case aco_opcode::v_wmma_f32_16x16x16_f16:
+   case aco_opcode::v_wmma_f32_16x16x16_bf16:
+   case aco_opcode::v_wmma_f16_16x16x16_f16:
+   case aco_opcode::v_wmma_bf16_16x16x16_bf16:
+   case aco_opcode::v_wmma_i32_16x16x16_iu8:
+   case aco_opcode::v_wmma_i32_16x16x16_iu4: return false;
    default: return true;
    }
 }
diff --git a/src/amd/compiler/aco_validate.cpp b/src/amd/compiler/aco_validate.cpp
index 32c3d798dea1..289815a0b5df 100644
--- a/src/amd/compiler/aco_validate.cpp
+++ b/src/amd/compiler/aco_validate.cpp
@@ -254,8 +254,8 @@ validate_ir(Program* program)
                   check(!vop3p.opsel_lo[i] && !vop3p.opsel_hi[i],
                         "Unexpected opsel for subdword operand", instr.get());
             }
-            check(instr->definitions[0].regClass() == v1, "VOP3P must have v1 definition",
-                  instr.get());
+            check(instr->definitions[0].regClass() == v1 || is_wmma(instr),
+                  "VOP3P must have v1 definition", instr.get());
          }
 
          /* check for undefs */
-- 
GitLab


From 5ccd9b8636795da181fec567809ab6b7cb4eaafe Mon Sep 17 00:00:00 2001
From: Bas Nieuwenhuizen <bas@basnieuwenhuizen.nl>
Date: Sat, 15 Jul 2023 19:43:08 +0200
Subject: [PATCH 3/6] aco: Make RA understand WMMA instructions.

---
 src/amd/compiler/aco_register_allocation.cpp | 8 ++++++++
 1 file changed, 8 insertions(+)

diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index 1acd899e6c56..1d63211d8783 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -2959,6 +2959,14 @@ register_allocation(Program* program, std::vector<IDSet>& live_out_per_block, ra
                                                    parallelcopy, instr);
                update_renames(ctx, register_file, parallelcopy, instr, (UpdateRenames)0);
                definition->setFixed(reg);
+            } else if (is_wmma(instr) && instr->operands[2].isTemp() &&
+                       instr->operands[2].isKill() &&
+                       instr->operands[2].regClass() == definition->regClass()) {
+               /* For WMMA, the dest needs to either be equal to operands[2], or not overlap it.
+                * Here we set a policy of forcing them the same if operands[2] gets killed (and
+                * otherwise they don't overlap). This may not be optimal if RA would select a
+                * different location due to affinity, but that gets complicated very quickly. */
+               definition->setFixed(instr->operands[2].physReg());
             }
 
             if (!definition->isFixed()) {
-- 
GitLab


From 0f8b40e39f347deb4a4d5f21517c18096a1d7ba3 Mon Sep 17 00:00:00 2001
From: Bas Nieuwenhuizen <bas@basnieuwenhuizen.nl>
Date: Sun, 23 Jul 2023 22:14:04 +0200
Subject: [PATCH 4/6] radv: Don't transparently use wave32 with cooperative
 matrices.

The instruction has different regsizes for wave32 vs. wave64.

To ensure cases with cooperative matrix load/store without any
actual wmma instructions get handled correctly, also require
full subgroups if subgroup invocation/id are used. Not sure
those could be transparently changed anyway.
---
 src/amd/vulkan/radv_shader_info.c | 5 +++--
 1 file changed, 3 insertions(+), 2 deletions(-)

diff --git a/src/amd/vulkan/radv_shader_info.c b/src/amd/vulkan/radv_shader_info.c
index 4a3e250d91f6..c54c6d1118fb 100644
--- a/src/amd/vulkan/radv_shader_info.c
+++ b/src/amd/vulkan/radv_shader_info.c
@@ -911,10 +911,11 @@ gather_shader_info_cs(struct radv_device *device, const nir_shader *nir, const s
    unsigned local_size = nir->info.workgroup_size[0] * nir->info.workgroup_size[1] * nir->info.workgroup_size[2];
 
    /* Games don't always request full subgroups when they should, which can cause bugs if cswave32
-    * is enabled.
+    * is enabled. Furthermore, if cooperative matrices or subgroup info are used, we can't transparently change
+    * the subgroup size.
     */
    const bool require_full_subgroups =
-      pipeline_key->stage_info[MESA_SHADER_COMPUTE].subgroup_require_full ||
+      pipeline_key->stage_info[MESA_SHADER_COMPUTE].subgroup_require_full || nir->info.cs.has_cooperative_matrix ||
       (default_wave_size == 32 && nir->info.uses_wide_subgroup_intrinsics && local_size % RADV_SUBGROUP_SIZE == 0);
 
    const unsigned required_subgroup_size = pipeline_key->stage_info[MESA_SHADER_COMPUTE].subgroup_required_size * 32;
-- 
GitLab


From bd2338596617bfd30b3986d78c329b8431c30e1e Mon Sep 17 00:00:00 2001
From: Bas Nieuwenhuizen <bas@basnieuwenhuizen.nl>
Date: Sun, 16 Jul 2023 01:01:55 +0200
Subject: [PATCH 5/6] radv: Add cooperative matrix lowering.

---
 src/amd/vulkan/meson.build                    |   1 +
 src/amd/vulkan/nir/radv_nir.h                 |   2 +
 .../nir/radv_nir_lower_cooperative_matrix.c   | 435 ++++++++++++++++++
 src/amd/vulkan/radv_shader.c                  |   4 +
 4 files changed, 442 insertions(+)
 create mode 100644 src/amd/vulkan/nir/radv_nir_lower_cooperative_matrix.c

diff --git a/src/amd/vulkan/meson.build b/src/amd/vulkan/meson.build
index e9d2f7e0a30e..a43624d11756 100644
--- a/src/amd/vulkan/meson.build
+++ b/src/amd/vulkan/meson.build
@@ -75,6 +75,7 @@ libradv_files = files(
   'nir/radv_nir_apply_pipeline_layout.c',
   'nir/radv_nir_export_multiview.c',
   'nir/radv_nir_lower_abi.c',
+  'nir/radv_nir_lower_cooperative_matrix.c',
   'nir/radv_nir_lower_fs_barycentric.c',
   'nir/radv_nir_lower_fs_intrinsics.c',
   'nir/radv_nir_lower_intrinsics_early.c',
diff --git a/src/amd/vulkan/nir/radv_nir.h b/src/amd/vulkan/nir/radv_nir.h
index 279f161e71eb..44a51d747e1e 100644
--- a/src/amd/vulkan/nir/radv_nir.h
+++ b/src/amd/vulkan/nir/radv_nir.h
@@ -78,6 +78,8 @@ bool radv_nir_lower_io_to_mem(struct radv_device *device, struct radv_shader_sta
 
 void radv_nir_lower_poly_line_smooth(nir_shader *nir, const struct radv_pipeline_key *key);
 
+bool radv_nir_lower_cooperative_matrix(nir_shader *shader, unsigned wave_size);
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/src/amd/vulkan/nir/radv_nir_lower_cooperative_matrix.c b/src/amd/vulkan/nir/radv_nir_lower_cooperative_matrix.c
new file mode 100644
index 000000000000..d81231b0137b
--- /dev/null
+++ b/src/amd/vulkan/nir/radv_nir_lower_cooperative_matrix.c
@@ -0,0 +1,435 @@
+/*
+ * Copyright Â© 2023 Bas Nieuwenhuizen
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ */
+
+#include "nir_builder.h"
+#include "radv_nir.h"
+
+static unsigned
+radv_nir_cmat_length(struct glsl_cmat_description desc, unsigned wave_size)
+{
+   return desc.use != GLSL_CMAT_USE_ACCUMULATOR
+             ? 16
+             : (desc.cols * desc.rows / wave_size * 32 / glsl_base_type_bit_size(desc.element_type));
+}
+
+/* for C matrices we have 1 VGPR per element even if the element type is < 32 bits. So with 8 fp16 elements we implement
+ * that with a f16vec16. We then use the coefficient generated by this function to figure out how many elements we
+ * really have.
+ */
+static unsigned
+radv_nir_cmat_length_mul(struct glsl_cmat_description desc)
+{
+   return desc.use == GLSL_CMAT_USE_ACCUMULATOR ? (32 / glsl_base_type_bit_size(desc.element_type)) : 1;
+}
+
+static unsigned
+radv_nir_cmat_bits(struct glsl_cmat_description desc)
+{
+   return glsl_base_type_bit_size(desc.element_type);
+}
+
+static nir_def *
+radv_nir_load_cmat(nir_builder *b, unsigned wave_size, nir_def *src)
+{
+   nir_deref_instr *deref = nir_instr_as_deref(src->parent_instr);
+   struct glsl_cmat_description desc = *glsl_get_cmat_description(deref->type);
+   return nir_build_load_deref(b, radv_nir_cmat_length(desc, wave_size), glsl_base_type_bit_size(desc.element_type),
+                               src, 0);
+}
+
+static const struct glsl_type *
+radv_nir_translate_matrix_type(const struct glsl_type *orig_type, struct hash_table *type_map, unsigned wave_size)
+{
+   struct hash_entry *entry = _mesa_hash_table_search(type_map, orig_type);
+   if (entry) {
+      return entry->data;
+   } else if (glsl_type_is_cmat(orig_type)) {
+      struct glsl_cmat_description desc = *glsl_get_cmat_description(orig_type);
+      unsigned length = radv_nir_cmat_length(desc, wave_size);
+
+      return glsl_vector_type(desc.element_type, length);
+   } else if (glsl_type_is_array(orig_type)) {
+      const struct glsl_type *elem_type = glsl_get_array_element(orig_type);
+      const struct glsl_type *new_elem_type = radv_nir_translate_matrix_type(elem_type, type_map, wave_size);
+
+      if (elem_type == new_elem_type)
+         return orig_type;
+
+      return glsl_array_type(new_elem_type, glsl_get_length(orig_type), glsl_get_explicit_stride(orig_type));
+   } else if (glsl_type_is_struct(orig_type)) {
+      unsigned num_fields = glsl_get_length(orig_type);
+
+      bool change = false;
+      for (unsigned i = 0; i < num_fields; ++i) {
+         const struct glsl_type *field_type = glsl_get_struct_field(orig_type, i);
+         const struct glsl_type *new_field_type = radv_nir_translate_matrix_type(field_type, type_map, wave_size);
+
+         if (field_type != new_field_type) {
+            change = true;
+            break;
+         }
+      }
+
+      if (!change)
+         return orig_type;
+
+      struct glsl_struct_field *fields = malloc(sizeof(struct glsl_struct_field) * num_fields);
+
+      for (unsigned i = 0; i < num_fields; ++i) {
+         fields[i] = *glsl_get_struct_field_data(orig_type, i);
+
+         fields[i].type = radv_nir_translate_matrix_type(fields[i].type, type_map, wave_size);
+      }
+
+      const struct glsl_type *ret =
+         glsl_struct_type(fields, num_fields, glsl_get_type_name(orig_type), glsl_struct_type_is_packed(orig_type));
+      free(fields);
+
+      _mesa_hash_table_insert(type_map, orig_type, (void *)ret);
+      return ret;
+   } else
+      return orig_type;
+}
+
+bool
+radv_nir_lower_cooperative_matrix(nir_shader *shader, unsigned wave_size)
+{
+   bool progress = false;
+
+   if (!shader->info.cs.has_cooperative_matrix)
+      return false;
+
+   struct nir_function *func = (struct nir_function *)exec_list_get_head_const(&shader->functions);
+   struct hash_table *type_map = _mesa_pointer_hash_table_create(NULL);
+
+   nir_foreach_variable_with_modes (var, shader, nir_var_shader_temp) {
+      const struct glsl_type *new_type = radv_nir_translate_matrix_type(var->type, type_map, wave_size);
+      if (new_type != var->type) {
+         var->type = new_type;
+         progress = true;
+      }
+   }
+
+   nir_foreach_function_temp_variable (var, func->impl) {
+      const struct glsl_type *new_type = radv_nir_translate_matrix_type(var->type, type_map, wave_size);
+      if (new_type != var->type) {
+         var->type = new_type;
+         progress = true;
+      }
+   }
+
+   nir_builder b = nir_builder_create(func->impl);
+
+   /* Iterate in reverse order so that lowering can still use the matrix types from the derefs before we change it. */
+   nir_foreach_block_reverse (block, func->impl) {
+      nir_foreach_instr_reverse_safe (instr, block) {
+         b.cursor = nir_before_instr(instr);
+
+         switch (instr->type) {
+         case nir_instr_type_intrinsic: {
+            nir_intrinsic_instr *intr = nir_instr_as_intrinsic(instr);
+            switch (intr->intrinsic) {
+            case nir_intrinsic_cmat_length: {
+               struct glsl_cmat_description desc = nir_intrinsic_cmat_desc(intr);
+               unsigned len = radv_nir_cmat_length(desc, wave_size) / radv_nir_cmat_length_mul(desc);
+               nir_def_rewrite_uses(&intr->def, nir_imm_int(&b, len));
+               nir_instr_remove(instr);
+               progress = true;
+               break;
+            }
+            case nir_intrinsic_cmat_extract: {
+               nir_deref_instr *src_deref = nir_instr_as_deref(intr->src[0].ssa->parent_instr);
+               struct glsl_cmat_description desc = *glsl_get_cmat_description(src_deref->type);
+               nir_def *src0 = radv_nir_load_cmat(&b, wave_size, intr->src[0].ssa);
+
+               nir_def *index = intr->src[1].ssa;
+               index = nir_imul_imm(&b, index, radv_nir_cmat_length_mul(desc));
+
+               nir_def *elem = nir_vector_extract(&b, src0, index);
+
+               nir_def_rewrite_uses(&intr->def, elem);
+               nir_instr_remove(instr);
+               progress = true;
+               break;
+            }
+            case nir_intrinsic_cmat_insert: {
+               nir_def *src1 = radv_nir_load_cmat(&b, wave_size, intr->src[2].ssa);
+               nir_deref_instr *dst_deref = nir_instr_as_deref(intr->src[0].ssa->parent_instr);
+               struct glsl_cmat_description desc = *glsl_get_cmat_description(dst_deref->type);
+               nir_def *index = intr->src[3].ssa;
+               index = nir_imul_imm(&b, index, radv_nir_cmat_length_mul(desc));
+
+               nir_def *elem = intr->src[1].ssa;
+               nir_def *r = nir_vector_insert(&b, src1, elem, index);
+               nir_store_deref(&b, dst_deref, r, 0xffff);
+               nir_instr_remove(instr);
+               progress = true;
+               break;
+            }
+            case nir_intrinsic_cmat_construct: {
+               nir_deref_instr *dst_deref = nir_instr_as_deref(intr->src[0].ssa->parent_instr);
+               struct glsl_cmat_description desc = *glsl_get_cmat_description(dst_deref->type);
+               nir_def *elem = intr->src[1].ssa;
+
+               nir_def *r = nir_replicate(&b, elem, radv_nir_cmat_length(desc, wave_size));
+
+               nir_store_deref(&b, dst_deref, r, 0xffff);
+               nir_instr_remove(instr);
+               progress = true;
+               break;
+            }
+            case nir_intrinsic_cmat_load: {
+               nir_deref_instr *dst_deref = nir_instr_as_deref(intr->src[0].ssa->parent_instr);
+               struct glsl_cmat_description desc = *glsl_get_cmat_description(dst_deref->type);
+               enum glsl_matrix_layout layout = nir_intrinsic_matrix_layout(intr);
+
+               nir_deref_instr *deref = nir_instr_as_deref(intr->src[1].ssa->parent_instr);
+               nir_def *stride = intr->src[2].ssa;
+
+               nir_def *local_idx = nir_load_subgroup_invocation(&b);
+               nir_def *inner_idx = nir_iand_imm(&b, local_idx, 15);
+
+               /* A input is transposed */
+               if (desc.use == GLSL_CMAT_USE_A)
+                  layout = layout == GLSL_MATRIX_LAYOUT_COLUMN_MAJOR ? GLSL_MATRIX_LAYOUT_ROW_MAJOR
+                                                                     : GLSL_MATRIX_LAYOUT_COLUMN_MAJOR;
+
+               unsigned length = radv_nir_cmat_length(desc, wave_size);
+               unsigned mul = radv_nir_cmat_length_mul(desc);
+               unsigned lanes_per_iter = desc.use == GLSL_CMAT_USE_ACCUMULATOR ? wave_size : 16;
+               nir_def *vars[16];
+               if (mul > 1) {
+                  for (unsigned i = 0; i < length; ++i)
+                     if (i % mul != 0)
+                        vars[i] = nir_undef(&b, 1, glsl_base_type_bit_size(desc.element_type));
+               }
+
+               unsigned idx_bits = deref->def.bit_size;
+               nir_def *base_row =
+                  desc.use == GLSL_CMAT_USE_ACCUMULATOR ? nir_udiv_imm(&b, local_idx, 16) : nir_imm_int(&b, 0);
+
+               for (unsigned i = 0; i < length / mul; ++i) {
+                  nir_def *col_offset = inner_idx;
+                  nir_def *row_offset = nir_iadd_imm(&b, base_row, i * lanes_per_iter / 16);
+
+                  if (layout == GLSL_MATRIX_LAYOUT_ROW_MAJOR) {
+                     nir_def *tmp = col_offset;
+                     col_offset = row_offset;
+                     row_offset = tmp;
+                  }
+
+                  col_offset = nir_imul(&b, col_offset, stride);
+
+                  col_offset = nir_u2uN(&b, col_offset, idx_bits);
+                  row_offset = nir_u2uN(&b, row_offset, idx_bits);
+
+                  nir_deref_instr *iter_deref = nir_build_deref_ptr_as_array(&b, deref, col_offset);
+                  iter_deref =
+                     nir_build_deref_cast(&b, &iter_deref->def, deref->modes, glsl_scalar_type(desc.element_type),
+                                          glsl_base_type_bit_size(desc.element_type) / 8);
+                  iter_deref = nir_build_deref_ptr_as_array(&b, iter_deref, row_offset);
+
+                  vars[i * mul] = nir_load_deref(&b, iter_deref);
+               }
+
+               nir_def *mat = nir_vec(&b, vars, length);
+               nir_store_deref(&b, dst_deref, mat, 0xffff);
+               nir_instr_remove(instr);
+               progress = true;
+               break;
+            }
+            case nir_intrinsic_cmat_store: {
+               enum glsl_matrix_layout layout = nir_intrinsic_matrix_layout(intr);
+
+               nir_deref_instr *deref = nir_instr_as_deref(intr->src[0].ssa->parent_instr);
+               nir_def *src = intr->src[1].ssa;
+               nir_def *stride = intr->src[2].ssa;
+
+               nir_deref_instr *src_deref = nir_instr_as_deref(src->parent_instr);
+               struct glsl_cmat_description desc = *glsl_get_cmat_description(src_deref->type);
+               src = radv_nir_load_cmat(&b, wave_size, src);
+
+               nir_def *local_idx = nir_load_subgroup_invocation(&b);
+
+               if (desc.use != GLSL_CMAT_USE_ACCUMULATOR)
+                  nir_push_if(&b, nir_ilt_imm(&b, local_idx, 16));
+
+               nir_def *inner_idx = nir_iand_imm(&b, local_idx, 15);
+
+               /* A input is transposed */
+               if (desc.use == GLSL_CMAT_USE_A)
+                  layout = layout == GLSL_MATRIX_LAYOUT_COLUMN_MAJOR ? GLSL_MATRIX_LAYOUT_ROW_MAJOR
+                                                                     : GLSL_MATRIX_LAYOUT_COLUMN_MAJOR;
+
+               unsigned length = radv_nir_cmat_length(desc, wave_size);
+               unsigned mul = radv_nir_cmat_length_mul(desc);
+               unsigned lanes_per_iter = desc.use == GLSL_CMAT_USE_ACCUMULATOR ? wave_size : 16;
+               nir_def *vars[16];
+               for (unsigned i = 0; i < length; ++i)
+                  vars[i] = nir_channel(&b, src, i);
+
+               unsigned idx_bits = deref->def.bit_size;
+               nir_def *base_row =
+                  desc.use == GLSL_CMAT_USE_ACCUMULATOR ? nir_udiv_imm(&b, local_idx, 16) : nir_imm_int(&b, 0);
+
+               for (unsigned i = 0; i < length / mul; ++i) {
+                  nir_def *col_offset = inner_idx;
+                  nir_def *row_offset = nir_iadd_imm(&b, base_row, i * lanes_per_iter / 16);
+
+                  if (layout == GLSL_MATRIX_LAYOUT_ROW_MAJOR) {
+                     nir_def *tmp = col_offset;
+                     col_offset = row_offset;
+                     row_offset = tmp;
+                  }
+
+                  col_offset = nir_imul(&b, col_offset, stride);
+
+                  col_offset = nir_u2uN(&b, col_offset, idx_bits);
+                  row_offset = nir_u2uN(&b, row_offset, idx_bits);
+
+                  nir_deref_instr *iter_deref = nir_build_deref_ptr_as_array(&b, deref, col_offset);
+                  iter_deref =
+                     nir_build_deref_cast(&b, &iter_deref->def, deref->modes, glsl_scalar_type(desc.element_type),
+                                          glsl_base_type_bit_size(desc.element_type) / 8);
+                  iter_deref = nir_build_deref_ptr_as_array(&b, iter_deref, row_offset);
+
+                  nir_store_deref(&b, iter_deref, vars[i * mul], 1);
+               }
+
+               if (desc.use != GLSL_CMAT_USE_ACCUMULATOR)
+                  nir_pop_if(&b, NULL);
+
+               nir_instr_remove(instr);
+               progress = true;
+               break;
+            }
+            case nir_intrinsic_cmat_muladd: {
+               nir_def *A = radv_nir_load_cmat(&b, wave_size, intr->src[1].ssa);
+               nir_def *B = radv_nir_load_cmat(&b, wave_size, intr->src[2].ssa);
+               nir_def *C = radv_nir_load_cmat(&b, wave_size, intr->src[3].ssa);
+               nir_def *ret;
+
+               ret = nir_cmat_muladd_amd(&b, A, B, C, .saturate = nir_intrinsic_saturate(intr),
+                                         .cmat_signed_mask = nir_intrinsic_cmat_signed_mask(intr));
+
+               nir_store_deref(&b, nir_instr_as_deref(intr->src[0].ssa->parent_instr), ret, 0xffff);
+               nir_instr_remove(instr);
+               progress = true;
+               break;
+            }
+            case nir_intrinsic_cmat_unary_op: {
+               nir_deref_instr *dst_deref = nir_instr_as_deref(intr->src[0].ssa->parent_instr);
+               nir_deref_instr *src_deref = nir_instr_as_deref(intr->src[1].ssa->parent_instr);
+               struct glsl_cmat_description desc = *glsl_get_cmat_description(dst_deref->type);
+               struct glsl_cmat_description src_desc = *glsl_get_cmat_description(src_deref->type);
+               nir_def *src = radv_nir_load_cmat(&b, wave_size, intr->src[1].ssa);
+               nir_op op = nir_intrinsic_alu_op(intr);
+
+               if (glsl_base_type_bit_size(src_desc.element_type) == 16 &&
+                   glsl_base_type_bit_size(desc.element_type) == 32 && desc.use == GLSL_CMAT_USE_ACCUMULATOR) {
+                  nir_def *components[NIR_MAX_VEC_COMPONENTS];
+                  for (unsigned i = 0; i * 2 < src->num_components; ++i) {
+                     components[i] = nir_channel(&b, src, i * 2);
+                  }
+                  src = nir_vec(&b, components, src->num_components / 2);
+               }
+
+               nir_def *ret = nir_build_alu1(&b, op, src);
+
+               if (glsl_base_type_bit_size(src_desc.element_type) == 32 &&
+                   glsl_base_type_bit_size(desc.element_type) == 16 && desc.use == GLSL_CMAT_USE_ACCUMULATOR) {
+                  nir_def *components[NIR_MAX_VEC_COMPONENTS];
+                  for (unsigned i = 0; i < ret->num_components; ++i) {
+                     components[i * 2] = nir_channel(&b, ret, i);
+                     components[i * 2 + 1] = nir_undef(&b, 1, 16);
+                  }
+                  ret = nir_vec(&b, components, ret->num_components * 2);
+               }
+
+               nir_store_deref(&b, dst_deref, ret, 0xffff);
+               nir_instr_remove(instr);
+               progress = true;
+               break;
+            }
+            case nir_intrinsic_cmat_scalar_op: {
+               nir_def *src1 = radv_nir_load_cmat(&b, wave_size, intr->src[1].ssa);
+               nir_op op = nir_intrinsic_alu_op(intr);
+               nir_def *ret = nir_build_alu2(&b, op, src1, intr->src[2].ssa);
+               nir_store_deref(&b, nir_instr_as_deref(intr->src[0].ssa->parent_instr), ret, 0xffff);
+               nir_instr_remove(instr);
+               progress = true;
+               break;
+            }
+            case nir_intrinsic_cmat_binary_op: {
+               nir_def *src1 = radv_nir_load_cmat(&b, wave_size, intr->src[1].ssa);
+               nir_def *src2 = radv_nir_load_cmat(&b, wave_size, intr->src[2].ssa);
+               nir_op op = nir_intrinsic_alu_op(intr);
+               nir_def *ret = nir_build_alu2(&b, op, src1, src2);
+               nir_store_deref(&b, nir_instr_as_deref(intr->src[0].ssa->parent_instr), ret, 0xffff);
+               nir_instr_remove(instr);
+               progress = true;
+               break;
+            }
+            case nir_intrinsic_cmat_bitcast: {
+               nir_def *src1 = radv_nir_load_cmat(&b, wave_size, intr->src[1].ssa);
+               nir_store_deref(&b, nir_instr_as_deref(intr->src[0].ssa->parent_instr), src1, 0xffff);
+               nir_instr_remove(instr);
+               progress = true;
+               break;
+            }
+            case nir_intrinsic_cmat_copy: {
+               nir_build_copy_deref(&b, intr->src[0].ssa, intr->src[1].ssa);
+               nir_instr_remove(instr);
+               progress = true;
+               break;
+            }
+            default:
+               continue;
+            }
+            break;
+         }
+         case nir_instr_type_deref: {
+            nir_deref_instr *deref = nir_instr_as_deref(instr);
+            const struct glsl_type *new_type = radv_nir_translate_matrix_type(deref->type, type_map, wave_size);
+            if (new_type != deref->type) {
+               deref->type = new_type;
+               progress = true;
+            }
+            break;
+         }
+         default:
+            continue;
+         }
+      }
+   }
+
+   _mesa_hash_table_destroy(type_map, NULL);
+
+   if (progress) {
+      nir_metadata_preserve(func->impl, 0);
+   } else {
+      nir_metadata_preserve(func->impl, nir_metadata_all);
+   }
+
+   return progress;
+}
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 7636a66181f3..ef1ff9b079db 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -447,6 +447,7 @@ radv_shader_spirv_to_nir(struct radv_device *device, const struct radv_shader_st
                .vk_memory_model_device_scope = true,
                .fragment_shading_rate = device->physical_device->rad_info.gfx_level >= GFX10_3,
                .workgroup_memory_explicit_layout = true,
+               .cooperative_matrix = true,
             },
          .ubo_addr_format = nir_address_format_vec2_index_32bit_offset,
          .ssbo_addr_format = nir_address_format_vec2_index_32bit_offset,
@@ -504,6 +505,8 @@ radv_shader_spirv_to_nir(struct radv_device *device, const struct radv_shader_st
        */
       NIR_PASS(_, nir, nir_lower_variable_initializers, ~0);
 
+      NIR_PASS(_, nir, radv_nir_lower_cooperative_matrix, subgroup_size);
+
       /* Split member structs.  We do this before lower_io_to_temporaries so that
        * it doesn't lower system values to temporaries by accident.
        */
@@ -533,6 +536,7 @@ radv_shader_spirv_to_nir(struct radv_device *device, const struct radv_shader_st
        * than it needs to be.
        */
       NIR_PASS(_, nir, nir_lower_global_vars_to_local);
+
       NIR_PASS(_, nir, nir_lower_vars_to_ssa);
 
       NIR_PASS(_, nir, nir_propagate_invariant, key->invariant_geom);
-- 
GitLab


From fbaf47526a8e8b14eb821dbe73769ac0147bfbe8 Mon Sep 17 00:00:00 2001
From: Bas Nieuwenhuizen <bas@basnieuwenhuizen.nl>
Date: Sun, 16 Jul 2023 01:34:29 +0200
Subject: [PATCH 6/6] radv: Expose VK_KHR_cooperative_matrix.

---
 docs/relnotes/new_features.txt        |  1 +
 src/amd/vulkan/radv_physical_device.c | 70 +++++++++++++++++++++++++++
 2 files changed, 71 insertions(+)

diff --git a/docs/relnotes/new_features.txt b/docs/relnotes/new_features.txt
index 62f583879a1c..ca79cf2a7459 100644
--- a/docs/relnotes/new_features.txt
+++ b/docs/relnotes/new_features.txt
@@ -18,3 +18,4 @@ GL_OES_sample_variables on Asahi
 GL_OES_shader_multisample_interpolation on Asahi
 GL_OES_gpu_shader5 on Asahi
 EGL_ANDROID_blob_cache works when disk caching is disabled
+VK_KHR_cooperative_matrix on RADV/GFX11+
diff --git a/src/amd/vulkan/radv_physical_device.c b/src/amd/vulkan/radv_physical_device.c
index 7dab2d8a8dae..ed1a1ffd5d7c 100644
--- a/src/amd/vulkan/radv_physical_device.c
+++ b/src/amd/vulkan/radv_physical_device.c
@@ -380,6 +380,7 @@ radv_physical_device_get_supported_extensions(const struct radv_physical_device
       .KHR_8bit_storage = true,
       .KHR_16bit_storage = true,
       .KHR_acceleration_structure = radv_enable_rt(device, false),
+      .KHR_cooperative_matrix = device->rad_info.gfx_level >= GFX11 && !device->use_llvm,
       .KHR_bind_memory2 = true,
       .KHR_buffer_device_address = true,
       .KHR_copy_commands2 = true,
@@ -1038,6 +1039,10 @@ radv_physical_device_get_features(const struct radv_physical_device *pdevice, st
       .deviceGeneratedCompute = true,
       .deviceGeneratedComputePipelines = false,
       .deviceGeneratedComputeCaptureReplay = false,
+
+      /* VK_KHR_cooperative_matrix */
+      .cooperativeMatrix = pdevice->rad_info.gfx_level >= GFX11 && !pdevice->use_llvm,
+      .cooperativeMatrixRobustBufferAccess = pdevice->rad_info.gfx_level >= GFX11 && !pdevice->use_llvm,
    };
 }
 
@@ -1710,6 +1715,9 @@ radv_get_physical_device_properties(struct radv_physical_device *pdevice)
    p->polygonModePointSize = true;
    p->nonStrictSinglePixelWideLinesUseParallelogram = false;
    p->nonStrictWideLinesUseParallelogram = false;
+
+   /* VK_KHR_cooperative_matrix */
+   p->cooperativeMatrixSupportedStages = VK_SHADER_STAGE_COMPUTE_BIT;
 }
 
 static VkResult
@@ -2434,3 +2442,65 @@ radv_GetPhysicalDeviceToolProperties(VkPhysicalDevice physicalDevice, uint32_t *
 
    return vk_outarray_status(&out);
 }
+
+VKAPI_ATTR VkResult VKAPI_CALL
+radv_GetPhysicalDeviceCooperativeMatrixPropertiesKHR(VkPhysicalDevice physicalDevice, uint32_t *pPropertyCount,
+                                                     VkCooperativeMatrixPropertiesKHR *pProperties)
+{
+   VK_OUTARRAY_MAKE_TYPED(VkCooperativeMatrixPropertiesKHR, out, pProperties, pPropertyCount);
+
+   vk_outarray_append_typed(VkCooperativeMatrixPropertiesKHR, &out, p)
+   {
+      *p = (struct VkCooperativeMatrixPropertiesKHR){.sType = VK_STRUCTURE_TYPE_COOPERATIVE_MATRIX_PROPERTIES_KHR,
+                                                     .MSize = 16,
+                                                     .NSize = 16,
+                                                     .KSize = 16,
+                                                     .AType = VK_COMPONENT_TYPE_FLOAT16_KHR,
+                                                     .BType = VK_COMPONENT_TYPE_FLOAT16_KHR,
+                                                     .CType = VK_COMPONENT_TYPE_FLOAT16_KHR,
+                                                     .ResultType = VK_COMPONENT_TYPE_FLOAT16_KHR,
+                                                     .saturatingAccumulation = false,
+                                                     .scope = VK_SCOPE_SUBGROUP_KHR};
+   }
+
+   vk_outarray_append_typed(VkCooperativeMatrixPropertiesKHR, &out, p)
+   {
+      *p = (struct VkCooperativeMatrixPropertiesKHR){.sType = VK_STRUCTURE_TYPE_COOPERATIVE_MATRIX_PROPERTIES_KHR,
+                                                     .MSize = 16,
+                                                     .NSize = 16,
+                                                     .KSize = 16,
+                                                     .AType = VK_COMPONENT_TYPE_FLOAT16_KHR,
+                                                     .BType = VK_COMPONENT_TYPE_FLOAT16_KHR,
+                                                     .CType = VK_COMPONENT_TYPE_FLOAT32_KHR,
+                                                     .ResultType = VK_COMPONENT_TYPE_FLOAT32_KHR,
+                                                     .saturatingAccumulation = false,
+                                                     .scope = VK_SCOPE_SUBGROUP_KHR};
+   }
+
+   for (unsigned asigned = 0; asigned < 2; asigned++) {
+      for (unsigned bsigned = 0; bsigned < 2; bsigned++) {
+         for (unsigned csigned = 0; csigned < 2; csigned++) {
+            for (unsigned saturate = 0; saturate < 2; saturate++) {
+               if (!csigned && saturate)
+                  continue; /* The HW only supports signed acc. */
+               vk_outarray_append_typed(VkCooperativeMatrixPropertiesKHR, &out, p)
+               {
+                  *p = (struct VkCooperativeMatrixPropertiesKHR){
+                     .sType = VK_STRUCTURE_TYPE_COOPERATIVE_MATRIX_PROPERTIES_KHR,
+                     .MSize = 16,
+                     .NSize = 16,
+                     .KSize = 16,
+                     .AType = asigned ? VK_COMPONENT_TYPE_SINT8_KHR : VK_COMPONENT_TYPE_UINT8_KHR,
+                     .BType = bsigned ? VK_COMPONENT_TYPE_SINT8_KHR : VK_COMPONENT_TYPE_UINT8_KHR,
+                     .CType = csigned ? VK_COMPONENT_TYPE_SINT32_KHR : VK_COMPONENT_TYPE_UINT32_KHR,
+                     .ResultType = csigned ? VK_COMPONENT_TYPE_SINT32_KHR : VK_COMPONENT_TYPE_UINT32_KHR,
+                     .saturatingAccumulation = saturate,
+                     .scope = VK_SCOPE_SUBGROUP_KHR};
+               }
+            }
+         }
+      }
+   }
+
+   return vk_outarray_status(&out);
+}
-- 
GitLab

