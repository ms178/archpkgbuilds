From 7e4a77f1a91d7cfdb52df8d5ae4d976f40b35c56 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Thu, 22 Feb 2024 17:06:16 +0100
Subject: [PATCH 1/9] radv: Add function to determine if SDMA supports an
 image.

---
 src/amd/vulkan/meta/radv_meta_copy.c |  8 ++++++++
 src/amd/vulkan/radv_sdma.c           | 17 +++++++++++++++++
 src/amd/vulkan/radv_sdma.h           |  1 +
 3 files changed, 26 insertions(+)

diff --git a/src/amd/vulkan/meta/radv_meta_copy.c b/src/amd/vulkan/meta/radv_meta_copy.c
index cc70b615b4b97..e637f5deb61e1 100644
--- a/src/amd/vulkan/meta/radv_meta_copy.c
+++ b/src/amd/vulkan/meta/radv_meta_copy.c
@@ -101,6 +101,10 @@ transfer_copy_buffer_image(struct radv_cmd_buffer *cmd_buffer, struct radv_buffe
    radv_cs_add_buffer(device->ws, cs, image->bindings[binding_idx].bo);
    radv_cs_add_buffer(device->ws, cs, buffer->bo);
 
+   if (!radv_sdma_supports_image(device, image)) {
+      return;
+   }
+
    struct radv_sdma_surf buf = radv_sdma_get_buf_surf(buffer, image, region, aspect_mask);
    const struct radv_sdma_surf img =
       radv_sdma_get_surf(device, image, region->imageSubresource, region->imageOffset, aspect_mask);
@@ -391,6 +395,10 @@ transfer_copy_image(struct radv_cmd_buffer *cmd_buffer, struct radv_image *src_i
    struct radeon_cmdbuf *cs = cmd_buffer->cs;
    unsigned int dst_aspect_mask_remaining = region->dstSubresource.aspectMask;
 
+   if (!radv_sdma_supports_image(device, src_image) || !radv_sdma_supports_image(device, dst_image)) {
+      return;
+   }
+
    u_foreach_bit (b, region->srcSubresource.aspectMask) {
       const VkImageAspectFlags src_aspect_mask = BITFIELD_BIT(b);
       const VkImageAspectFlags dst_aspect_mask = BITFIELD_BIT(u_bit_scan(&dst_aspect_mask_remaining));
diff --git a/src/amd/vulkan/radv_sdma.c b/src/amd/vulkan/radv_sdma.c
index c086104d673c2..d509b8d7a6407 100644
--- a/src/amd/vulkan/radv_sdma.c
+++ b/src/amd/vulkan/radv_sdma.c
@@ -796,3 +796,20 @@ radv_sdma_copy_image_t2t_scanline(const struct radv_device *device, struct radeo
       }
    }
 }
+
+bool
+radv_sdma_supports_image(const struct radv_device *device, const struct radv_image *image)
+{
+   if (radv_is_format_emulated(device->physical_device, image->vk.format))
+      return false;
+
+   if (!device->physical_device->rad_info.sdma_supports_sparse &&
+       (image->vk.create_flags & (VK_IMAGE_CREATE_SPARSE_BINDING_BIT | VK_IMAGE_CREATE_SPARSE_RESIDENCY_BIT |
+                                  VK_IMAGE_CREATE_SPARSE_ALIASED_BIT)))
+      return false;
+
+   if (image->vk.samples != VK_SAMPLE_COUNT_1_BIT)
+      return false;
+
+   return true;
+}
diff --git a/src/amd/vulkan/radv_sdma.h b/src/amd/vulkan/radv_sdma.h
index bcc95919c97bf..1ced4ca9d36a0 100644
--- a/src/amd/vulkan/radv_sdma.h
+++ b/src/amd/vulkan/radv_sdma.h
@@ -95,6 +95,7 @@ void radv_sdma_copy_buffer(const struct radv_device *device, struct radeon_cmdbu
                            uint64_t size);
 void radv_sdma_fill_buffer(const struct radv_device *device, struct radeon_cmdbuf *cs, const uint64_t va,
                            const uint64_t size, const uint32_t value);
+bool radv_sdma_supports_image(const struct radv_device *device, const struct radv_image *image);
 
 #ifdef __cplusplus
 }
-- 
GitLab


From c31987ea87e423791db7e3683e2cfb17e00e1c5b Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Thu, 22 Feb 2024 17:07:21 +0100
Subject: [PATCH 2/9] radv: Disable transfer queues when gang submit or compute
 is unavailable.

---
 src/amd/vulkan/radv_physical_device.c | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/src/amd/vulkan/radv_physical_device.c b/src/amd/vulkan/radv_physical_device.c
index 0959d843fc63d..fb57c1134c44f 100644
--- a/src/amd/vulkan/radv_physical_device.c
+++ b/src/amd/vulkan/radv_physical_device.c
@@ -79,6 +79,10 @@ radv_transfer_queue_enabled(const struct radv_physical_device *pdevice)
        !(pdevice->instance->perftest_flags & RADV_PERFTEST_TRANSFER_QUEUE))
       return false;
 
+   if (!pdevice->rad_info.has_gang_submit || !pdevice->rad_info.ip[AMD_IP_COMPUTE].num_queues ||
+       (pdevice->instance->debug_flags & RADV_DEBUG_NO_COMPUTE_QUEUE))
+      return false;
+
    return pdevice->rad_info.gfx_level >= GFX9;
 }
 
-- 
GitLab


From 4a19a05f0cbd59330bd97c5cce8d235247afbb7c Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Thu, 22 Feb 2024 17:07:45 +0100
Subject: [PATCH 3/9] radv: Implement gang semaphores for transfer queues.

---
 src/amd/vulkan/radv_cmd_buffer.c | 26 +++++++++++++++++++++++---
 1 file changed, 23 insertions(+), 3 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index 70aa1dd93a5ca..0e8af464fda2d 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -625,10 +625,30 @@ radv_gang_barrier(struct radv_cmd_buffer *cmd_buffer, VkPipelineStageFlags2 src_
         VK_PIPELINE_STAGE_2_BOTTOM_OF_PIPE_BIT | VK_PIPELINE_STAGE_2_ALL_COMMANDS_BIT))
       dst_stage_mask |= cmd_buffer->state.dma_is_busy ? VK_PIPELINE_STAGE_2_TASK_SHADER_BIT_EXT : 0;
 
-   /* Increment the GFX/ACE semaphore when task shaders are blocked. */
-   if (dst_stage_mask & (VK_PIPELINE_STAGE_2_TOP_OF_PIPE_BIT | VK_PIPELINE_STAGE_2_DRAW_INDIRECT_BIT |
-                         VK_PIPELINE_STAGE_2_TASK_SHADER_BIT_EXT))
+   /* Increment the leader to follower semaphore when the leader wants to block the follower:
+    * - graphics command buffer: task shader execution needs to wait for something
+    * - transfer command buffer: a transfer operation on ACE needs to wait for a previous operation on SDMA
+    */
+   const VkPipelineStageFlags2 gang_leader_flags =
+      cmd_buffer->qf == RADV_QUEUE_TRANSFER
+         ? (VK_PIPELINE_STAGE_2_TOP_OF_PIPE_BIT_KHR | VK_PIPELINE_STAGE_2_TRANSFER_BIT_KHR |
+            VK_PIPELINE_STAGE_2_ALL_TRANSFER_BIT)
+         : (VK_PIPELINE_STAGE_2_TOP_OF_PIPE_BIT_KHR | VK_PIPELINE_STAGE_2_DRAW_INDIRECT_BIT |
+            VK_PIPELINE_STAGE_2_TASK_SHADER_BIT_EXT);
+   if (dst_stage_mask & gang_leader_flags)
       cmd_buffer->gang.sem.leader_value++;
+
+   /* Increment the follower to leader semaphore when the follower wants to block the leader:
+    * - graphics command buffer: not necessary yet
+    * - transfer command buffer: a transfer operation on SDMA needs to wait for a previous operation on ACE
+    */
+   const VkPipelineStageFlags2 gang_follower_flags =
+      cmd_buffer->qf == RADV_QUEUE_TRANSFER
+         ? (VK_PIPELINE_STAGE_2_BOTTOM_OF_PIPE_BIT_KHR | VK_PIPELINE_STAGE_2_TRANSFER_BIT_KHR |
+            VK_PIPELINE_STAGE_2_ALL_TRANSFER_BIT)
+         : 0;
+   if (src_stage_mask & gang_follower_flags)
+      cmd_buffer->gang.sem.follower_value++;
 }
 
 void
-- 
GitLab


From e4d355c8fce130e01ba515576278f392c346ec61 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Thu, 22 Feb 2024 17:14:38 +0100
Subject: [PATCH 4/9] radv: Add layout argument to transfer_copy_buffer_image.

---
 src/amd/vulkan/meta/radv_meta_copy.c | 6 +++---
 1 file changed, 3 insertions(+), 3 deletions(-)

diff --git a/src/amd/vulkan/meta/radv_meta_copy.c b/src/amd/vulkan/meta/radv_meta_copy.c
index e637f5deb61e1..c799f7a151d86 100644
--- a/src/amd/vulkan/meta/radv_meta_copy.c
+++ b/src/amd/vulkan/meta/radv_meta_copy.c
@@ -91,7 +91,7 @@ alloc_transfer_temp_bo(struct radv_cmd_buffer *cmd_buffer)
 
 static void
 transfer_copy_buffer_image(struct radv_cmd_buffer *cmd_buffer, struct radv_buffer *buffer, struct radv_image *image,
-                           const VkBufferImageCopy2 *region, bool to_image)
+                           const VkImageLayout layout, const VkBufferImageCopy2 *region, bool to_image)
 {
    const struct radv_device *device = cmd_buffer->device;
    struct radeon_cmdbuf *cs = cmd_buffer->cs;
@@ -126,7 +126,7 @@ copy_buffer_to_image(struct radv_cmd_buffer *cmd_buffer, struct radv_buffer *buf
                      VkImageLayout layout, const VkBufferImageCopy2 *region)
 {
    if (cmd_buffer->qf == RADV_QUEUE_TRANSFER) {
-      transfer_copy_buffer_image(cmd_buffer, buffer, image, region, true);
+      transfer_copy_buffer_image(cmd_buffer, buffer, image, layout, region, true);
       return;
    }
 
@@ -277,7 +277,7 @@ copy_image_to_buffer(struct radv_cmd_buffer *cmd_buffer, struct radv_buffer *buf
 {
    struct radv_device *device = cmd_buffer->device;
    if (cmd_buffer->qf == RADV_QUEUE_TRANSFER) {
-      transfer_copy_buffer_image(cmd_buffer, buffer, image, region, false);
+      transfer_copy_buffer_image(cmd_buffer, buffer, image, layout, region, false);
       return;
    }
 
-- 
GitLab


From eeaa10e071ed53f48f76fa113d9f432d7a5a317e Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Sat, 21 Oct 2023 14:13:38 +0200
Subject: [PATCH 5/9] radv: Declare some gang submit functions in radv private
 header.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

They will be called from the transfer copy functions.

Signed-off-by: Timur Krist√≥f <timur.kristof@gmail.com>
---
 src/amd/vulkan/radv_cmd_buffer.c | 8 ++++----
 src/amd/vulkan/radv_private.h    | 6 ++++++
 2 files changed, 10 insertions(+), 4 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index 0e8af464fda2d..3eb257efe5a8e 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -713,7 +713,7 @@ radv_flush_gang_semaphore(struct radv_cmd_buffer *cmd_buffer, struct radeon_cmdb
    return true;
 }
 
-ALWAYS_INLINE static bool
+bool
 radv_flush_gang_leader_semaphore(struct radv_cmd_buffer *cmd_buffer)
 {
    if (!radv_gang_leader_sem_dirty(cmd_buffer))
@@ -724,7 +724,7 @@ radv_flush_gang_leader_semaphore(struct radv_cmd_buffer *cmd_buffer)
    return radv_flush_gang_semaphore(cmd_buffer, cmd_buffer->cs, cmd_buffer->qf, 0, cmd_buffer->gang.sem.leader_value);
 }
 
-ALWAYS_INLINE static bool
+bool
 radv_flush_gang_follower_semaphore(struct radv_cmd_buffer *cmd_buffer)
 {
    if (!radv_gang_follower_sem_dirty(cmd_buffer))
@@ -745,14 +745,14 @@ radv_wait_gang_semaphore(struct radv_cmd_buffer *cmd_buffer, struct radeon_cmdbu
    radv_cp_wait_mem(cs, qf, WAIT_REG_MEM_GREATER_OR_EQUAL, cmd_buffer->gang.sem.va + va_off, value, 0xffffffff);
 }
 
-ALWAYS_INLINE static void
+void
 radv_wait_gang_leader(struct radv_cmd_buffer *cmd_buffer)
 {
    /* Follower waits for the semaphore which the gang leader wrote. */
    radv_wait_gang_semaphore(cmd_buffer, cmd_buffer->gang.cs, RADV_QUEUE_COMPUTE, 0, cmd_buffer->gang.sem.leader_value);
 }
 
-ALWAYS_INLINE static void
+void
 radv_wait_gang_follower(struct radv_cmd_buffer *cmd_buffer)
 {
    /* Gang leader waits for the semaphore which the follower wrote. */
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index f35e1a35e03ce..def405f07064e 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -3148,6 +3148,12 @@ VkResult radv_rra_dump_trace(VkQueue vk_queue, char *filename);
 void radv_rra_trace_clear_ray_history(VkDevice _device, struct radv_rra_trace_data *data);
 void radv_rra_trace_finish(VkDevice vk_device, struct radv_rra_trace_data *data);
 
+bool radv_flush_gang_leader_semaphore(struct radv_cmd_buffer *cmd_buffer);
+bool radv_flush_gang_follower_semaphore(struct radv_cmd_buffer *cmd_buffer);
+void radv_wait_gang_leader(struct radv_cmd_buffer *cmd_buffer);
+void radv_wait_gang_follower(struct radv_cmd_buffer *cmd_buffer);
+bool radv_gang_init(struct radv_cmd_buffer *cmd_buffer);
+
 void radv_memory_trace_init(struct radv_device *device);
 void radv_rmv_log_bo_allocate(struct radv_device *device, struct radeon_winsys_bo *bo, uint32_t size, bool is_internal);
 void radv_rmv_log_bo_destroy(struct radv_device *device, struct radeon_winsys_bo *bo);
-- 
GitLab


From 047abe748a891b170b61e0ce0c6a73e98f620dd6 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Fri, 23 Feb 2024 12:27:58 +0100
Subject: [PATCH 6/9] radv: Slightly refactor command buffer initialization.

---
 src/amd/vulkan/radv_cmd_buffer.c | 55 ++++++++++++++++++++------------
 1 file changed, 35 insertions(+), 20 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index 3eb257efe5a8e..de755df6d3d3d 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -321,10 +321,8 @@ radv_cmd_buffer_init_shader_part_cache(struct radv_device *device, struct radv_c
 }
 
 static void
-radv_destroy_cmd_buffer(struct vk_command_buffer *vk_cmd_buffer)
+radv_free_cmd_buffer_resources(struct radv_cmd_buffer *cmd_buffer)
 {
-   struct radv_cmd_buffer *cmd_buffer = container_of(vk_cmd_buffer, struct radv_cmd_buffer, vk);
-
    if (cmd_buffer->qf != RADV_QUEUE_SPARSE) {
       util_dynarray_fini(&cmd_buffer->ray_history);
 
@@ -361,44 +359,42 @@ radv_destroy_cmd_buffer(struct vk_command_buffer *vk_cmd_buffer)
    }
 
    vk_command_buffer_finish(&cmd_buffer->vk);
+}
+
+static void
+radv_destroy_cmd_buffer(struct vk_command_buffer *vk_cmd_buffer)
+{
+   struct radv_cmd_buffer *cmd_buffer = container_of(vk_cmd_buffer, struct radv_cmd_buffer, vk);
+   radv_free_cmd_buffer_resources(cmd_buffer);
    vk_free(&cmd_buffer->vk.pool->alloc, cmd_buffer);
 }
 
 static VkResult
-radv_create_cmd_buffer(struct vk_command_pool *pool, struct vk_command_buffer **cmd_buffer_out)
+radv_init_cmd_buffer(struct radv_cmd_buffer *cmd_buffer, struct vk_command_pool *pool, const enum radv_queue_family qf)
 {
    struct radv_device *device = container_of(pool->base.device, struct radv_device, vk);
 
-   struct radv_cmd_buffer *cmd_buffer;
-   unsigned ring;
-   cmd_buffer = vk_zalloc(&pool->alloc, sizeof(*cmd_buffer), 8, VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
-   if (cmd_buffer == NULL)
-      return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
-
    VkResult result = vk_command_buffer_init(pool, &cmd_buffer->vk, &radv_cmd_buffer_ops, 0);
-   if (result != VK_SUCCESS) {
-      vk_free(&cmd_buffer->vk.pool->alloc, cmd_buffer);
+   if (result != VK_SUCCESS)
       return result;
-   }
 
    cmd_buffer->device = device;
+   cmd_buffer->qf = qf;
 
-   cmd_buffer->qf = vk_queue_to_radv(device->physical_device, pool->queue_family_index);
-
-   if (cmd_buffer->qf != RADV_QUEUE_SPARSE) {
+   if (qf != RADV_QUEUE_SPARSE) {
       list_inithead(&cmd_buffer->upload.list);
 
       if (!radv_cmd_buffer_init_shader_part_cache(device, cmd_buffer)) {
-         radv_destroy_cmd_buffer(&cmd_buffer->vk);
+         radv_free_cmd_buffer_resources(cmd_buffer);
          return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
       }
 
-      ring = radv_queue_family_to_ring(device->physical_device, cmd_buffer->qf);
+      const unsigned ring = radv_queue_family_to_ring(device->physical_device, cmd_buffer->qf);
 
       cmd_buffer->cs =
          device->ws->cs_create(device->ws, ring, cmd_buffer->vk.level == VK_COMMAND_BUFFER_LEVEL_SECONDARY);
       if (!cmd_buffer->cs) {
-         radv_destroy_cmd_buffer(&cmd_buffer->vk);
+         radv_free_cmd_buffer_resources(cmd_buffer);
          return vk_error(device, VK_ERROR_OUT_OF_DEVICE_MEMORY);
       }
 
@@ -410,8 +406,27 @@ radv_create_cmd_buffer(struct vk_command_pool *pool, struct vk_command_buffer **
       util_dynarray_init(&cmd_buffer->ray_history, NULL);
    }
 
-   *cmd_buffer_out = &cmd_buffer->vk;
+   return VK_SUCCESS;
+}
+
+static VkResult
+radv_create_cmd_buffer(struct vk_command_pool *pool, struct vk_command_buffer **cmd_buffer_out)
+{
+   struct radv_device *device = container_of(pool->base.device, struct radv_device, vk);
+
+   struct radv_cmd_buffer *cmd_buffer;
+   cmd_buffer = vk_zalloc(&pool->alloc, sizeof(*cmd_buffer), 8, VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
+   if (cmd_buffer == NULL)
+      return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
+
+   const enum radv_queue_family qf = vk_queue_to_radv(device->physical_device, pool->queue_family_index);
+   VkResult result = radv_init_cmd_buffer(cmd_buffer, pool, qf);
+   if (result != VK_SUCCESS) {
+      vk_free(&cmd_buffer->vk.pool->alloc, cmd_buffer);
+      return result;
+   }
 
+   *cmd_buffer_out = &cmd_buffer->vk;
    return VK_SUCCESS;
 }
 
-- 
GitLab


From 20818253e93c22c80200870ca56f8706a0af1902 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Fri, 23 Feb 2024 15:51:24 +0100
Subject: [PATCH 7/9] radv: Extract radv_cmd_buffer_prepend_upload_buf helper
 function.

---
 src/amd/vulkan/radv_cmd_buffer.c | 33 ++++++++++++++++++++------------
 1 file changed, 21 insertions(+), 12 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index de755df6d3d3d..ba33e210b7d24 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -498,12 +498,29 @@ const struct vk_command_buffer_ops radv_cmd_buffer_ops = {
    .destroy = radv_destroy_cmd_buffer,
 };
 
+static bool
+radv_cmd_buffer_prepend_upload_buf(struct radv_cmd_buffer *cmd_buffer, struct radv_cmd_buffer_upload *data)
+{
+   if (!data->upload_bo)
+      return true;
+
+   struct radv_cmd_buffer_upload *upload = malloc(sizeof(struct radv_cmd_buffer_upload));
+
+   if (!upload) {
+      vk_command_buffer_set_error(&cmd_buffer->vk, VK_ERROR_OUT_OF_HOST_MEMORY);
+      return false;
+   }
+
+   memcpy(upload, data, sizeof(struct radv_cmd_buffer_upload));
+   list_add(&upload->list, &cmd_buffer->upload.list);
+   return true;
+}
+
 static bool
 radv_cmd_buffer_resize_upload_buf(struct radv_cmd_buffer *cmd_buffer, uint64_t min_needed)
 {
    uint64_t new_size;
    struct radeon_winsys_bo *bo = NULL;
-   struct radv_cmd_buffer_upload *upload;
    struct radv_device *device = cmd_buffer->device;
 
    new_size = MAX2(min_needed, 16 * 1024);
@@ -520,17 +537,9 @@ radv_cmd_buffer_resize_upload_buf(struct radv_cmd_buffer *cmd_buffer, uint64_t m
    }
 
    radv_cs_add_buffer(device->ws, cmd_buffer->cs, bo);
-   if (cmd_buffer->upload.upload_bo) {
-      upload = malloc(sizeof(*upload));
-
-      if (!upload) {
-         vk_command_buffer_set_error(&cmd_buffer->vk, VK_ERROR_OUT_OF_HOST_MEMORY);
-         device->ws->buffer_destroy(device->ws, bo);
-         return false;
-      }
-
-      memcpy(upload, &cmd_buffer->upload, sizeof(*upload));
-      list_add(&upload->list, &cmd_buffer->upload.list);
+   if (!radv_cmd_buffer_prepend_upload_buf(cmd_buffer, &cmd_buffer->upload)) {
+      device->ws->buffer_destroy(device->ws, bo);
+      return false;
    }
 
    cmd_buffer->upload.upload_bo = bo;
-- 
GitLab


From 8d26541f8b45f33b42c0ae1b60317c3fc7a4c0da Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Thu, 22 Feb 2024 17:14:46 +0100
Subject: [PATCH 8/9] radv: Use gang submission for copying images unsupported
 by SDMA.

When the user tries to execute a transfer queue copy operation
that isn't supported by the SDMA engine, the driver must still
implement that copy somehow. To solve this, we use gang submission
and execute these copies on an async compute queue.

To simplify the implementation and avoid refactoring a large
portion of the code base, we create an internal command
buffer object on which we execute the already existing
compute based copy command implementations.
---
 src/amd/vulkan/meta/radv_meta_copy.c | 62 ++++++++++++++++++++++++++++
 src/amd/vulkan/radv_cmd_buffer.c     | 30 ++++++++++++++
 src/amd/vulkan/radv_private.h        |  2 +
 3 files changed, 94 insertions(+)

diff --git a/src/amd/vulkan/meta/radv_meta_copy.c b/src/amd/vulkan/meta/radv_meta_copy.c
index c799f7a151d86..2934dee1b1485 100644
--- a/src/amd/vulkan/meta/radv_meta_copy.c
+++ b/src/amd/vulkan/meta/radv_meta_copy.c
@@ -102,9 +102,46 @@ transfer_copy_buffer_image(struct radv_cmd_buffer *cmd_buffer, struct radv_buffe
    radv_cs_add_buffer(device->ws, cs, buffer->bo);
 
    if (!radv_sdma_supports_image(device, image)) {
+      struct radv_cmd_buffer follower = {0};
+      if (!radv_init_follower_temp_cmdbuf(cmd_buffer, &follower))
+         return;
+
+      if (radv_flush_gang_leader_semaphore(cmd_buffer))
+         radv_wait_gang_leader(cmd_buffer);
+
+      radv_gang_cache_flush(cmd_buffer);
+
+      if (to_image) {
+         VkCopyBufferToImageInfo2 info = {
+            .sType = VK_STRUCTURE_TYPE_COPY_BUFFER_TO_IMAGE_INFO_2,
+            .pNext = NULL,
+            .regionCount = 1,
+            .pRegions = region,
+            .dstImage = radv_image_to_handle(image),
+            .dstImageLayout = layout,
+            .srcBuffer = radv_buffer_to_handle(buffer),
+         };
+         radv_CmdCopyBufferToImage2(radv_cmd_buffer_to_handle(&follower), &info);
+      } else {
+         VkCopyImageToBufferInfo2 info = {
+            .sType = VK_STRUCTURE_TYPE_COPY_IMAGE_TO_BUFFER_INFO_2,
+            .pNext = NULL,
+            .regionCount = 1,
+            .pRegions = region,
+            .srcImage = radv_image_to_handle(image),
+            .srcImageLayout = layout,
+            .dstBuffer = radv_buffer_to_handle(buffer),
+         };
+         radv_CmdCopyImageToBuffer2(radv_cmd_buffer_to_handle(&follower), &info);
+      }
+
+      radv_finish_follower_temp_cmdbuf(cmd_buffer, &follower);
       return;
    }
 
+   if (cmd_buffer->gang.cs && radv_flush_gang_follower_semaphore(cmd_buffer))
+      radv_wait_gang_follower(cmd_buffer);
+
    struct radv_sdma_surf buf = radv_sdma_get_buf_surf(buffer, image, region, aspect_mask);
    const struct radv_sdma_surf img =
       radv_sdma_get_surf(device, image, region->imageSubresource, region->imageOffset, aspect_mask);
@@ -396,9 +433,34 @@ transfer_copy_image(struct radv_cmd_buffer *cmd_buffer, struct radv_image *src_i
    unsigned int dst_aspect_mask_remaining = region->dstSubresource.aspectMask;
 
    if (!radv_sdma_supports_image(device, src_image) || !radv_sdma_supports_image(device, dst_image)) {
+      struct radv_cmd_buffer follower = {0};
+      if (!radv_init_follower_temp_cmdbuf(cmd_buffer, &follower))
+         return;
+
+      if (radv_flush_gang_leader_semaphore(cmd_buffer))
+         radv_wait_gang_leader(cmd_buffer);
+
+      radv_gang_cache_flush(cmd_buffer);
+
+      VkCopyImageInfo2 info = {
+         .sType = VK_STRUCTURE_TYPE_COPY_IMAGE_INFO_2,
+         .pNext = NULL,
+         .regionCount = 1,
+         .pRegions = region,
+         .dstImage = radv_image_to_handle(dst_image),
+         .dstImageLayout = dst_image_layout,
+         .srcImage = radv_image_to_handle(src_image),
+         .srcImageLayout = src_image_layout,
+      };
+
+      radv_CmdCopyImage2(radv_cmd_buffer_to_handle(&follower), &info);
+      radv_finish_follower_temp_cmdbuf(cmd_buffer, &follower);
       return;
    }
 
+   if (cmd_buffer->gang.cs && radv_flush_gang_follower_semaphore(cmd_buffer))
+      radv_wait_gang_follower(cmd_buffer);
+
    u_foreach_bit (b, region->srcSubresource.aspectMask) {
       const VkImageAspectFlags src_aspect_mask = BITFIELD_BIT(b);
       const VkImageAspectFlags dst_aspect_mask = BITFIELD_BIT(u_bit_scan(&dst_aspect_mask_remaining));
diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index ba33e210b7d24..32340ae1577b8 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -830,6 +830,36 @@ radv_gang_finalize(struct radv_cmd_buffer *cmd_buffer)
    return device->ws->cs_finalize(ace_cs);
 }
 
+bool
+radv_init_follower_temp_cmdbuf(struct radv_cmd_buffer *cmd_buffer, struct radv_cmd_buffer *follower)
+{
+   if (!radv_gang_init(cmd_buffer))
+      return false;
+
+   if (radv_init_cmd_buffer(follower, cmd_buffer->vk.pool, RADV_QUEUE_COMPUTE) != VK_SUCCESS)
+      return false;
+
+   return true;
+}
+
+void
+radv_finish_follower_temp_cmdbuf(struct radv_cmd_buffer *cmd_buffer, struct radv_cmd_buffer *follower)
+{
+   /* Save the temp follower's upload BO to the original command buffer
+    * so that we keep until the original command buffer is used.
+    */
+   radv_cmd_buffer_prepend_upload_buf(cmd_buffer, &follower->upload);
+   follower->upload.upload_bo = NULL;
+
+   /* Copy the commands emitted to the temp follower's CS into
+    * the gang follower CS of the original cmd buffer.
+    */
+   cmd_buffer->device->ws->cs_finalize(follower->cs);
+   cmd_buffer->device->ws->cs_execute_secondary(cmd_buffer->gang.cs, follower->cs, false);
+
+   radv_free_cmd_buffer_resources(follower);
+}
+
 static void
 radv_cmd_buffer_after_draw(struct radv_cmd_buffer *cmd_buffer, enum radv_cmd_flush_bits flags, bool dgc)
 {
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index def405f07064e..2d2d5485bfe55 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -3153,6 +3153,8 @@ bool radv_flush_gang_follower_semaphore(struct radv_cmd_buffer *cmd_buffer);
 void radv_wait_gang_leader(struct radv_cmd_buffer *cmd_buffer);
 void radv_wait_gang_follower(struct radv_cmd_buffer *cmd_buffer);
 bool radv_gang_init(struct radv_cmd_buffer *cmd_buffer);
+bool radv_init_follower_temp_cmdbuf(struct radv_cmd_buffer *cmd_buffer, struct radv_cmd_buffer *follower);
+void radv_finish_follower_temp_cmdbuf(struct radv_cmd_buffer *cmd_buffer, struct radv_cmd_buffer *follower);
 
 void radv_memory_trace_init(struct radv_device *device);
 void radv_rmv_log_bo_allocate(struct radv_device *device, struct radeon_winsys_bo *bo, uint32_t size, bool is_internal);
-- 
GitLab
