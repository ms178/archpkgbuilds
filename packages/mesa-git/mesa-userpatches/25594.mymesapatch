From 1691d058776a4f7499098b73bc63885d30824ee6 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Sat, 21 Oct 2023 14:13:38 +0200
Subject: [PATCH 1/3] radv: Declare some gang submit functions in radv private
 header.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

They will be called from the transfer copy functions.

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
---
 src/amd/vulkan/radv_cmd_buffer.c | 8 ++++----
 src/amd/vulkan/radv_private.h    | 6 ++++++
 2 files changed, 10 insertions(+), 4 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index 2e329d448dea9..04c5909202569 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -687,7 +687,7 @@ radv_flush_gang_semaphore(struct radv_cmd_buffer *cmd_buffer, struct radeon_cmdb
    return true;
 }
 
-ALWAYS_INLINE static bool
+bool
 radv_flush_gang_leader_semaphore(struct radv_cmd_buffer *cmd_buffer)
 {
    if (!radv_gang_leader_sem_dirty(cmd_buffer))
@@ -698,7 +698,7 @@ radv_flush_gang_leader_semaphore(struct radv_cmd_buffer *cmd_buffer)
    return radv_flush_gang_semaphore(cmd_buffer, cmd_buffer->cs, cmd_buffer->qf, 0, cmd_buffer->gang.sem.leader_value);
 }
 
-ALWAYS_INLINE static bool
+bool
 radv_flush_gang_follower_semaphore(struct radv_cmd_buffer *cmd_buffer)
 {
    if (!radv_gang_follower_sem_dirty(cmd_buffer))
@@ -719,14 +719,14 @@ radv_wait_gang_semaphore(struct radv_cmd_buffer *cmd_buffer, struct radeon_cmdbu
    radv_cp_wait_mem(cs, qf, WAIT_REG_MEM_GREATER_OR_EQUAL, cmd_buffer->gang.sem.va + va_off, value, 0xffffffff);
 }
 
-ALWAYS_INLINE static void
+void
 radv_wait_gang_leader(struct radv_cmd_buffer *cmd_buffer)
 {
    /* Follower waits for the semaphore which the gang leader wrote. */
    radv_wait_gang_semaphore(cmd_buffer, cmd_buffer->gang.cs, RADV_QUEUE_COMPUTE, 0, cmd_buffer->gang.sem.leader_value);
 }
 
-ALWAYS_INLINE static void
+void
 radv_wait_gang_follower(struct radv_cmd_buffer *cmd_buffer)
 {
    /* Gang leader waits for the semaphore which the follower wrote. */
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index 48eaa68f6bba6..80ddbcf7d9bb4 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -2954,6 +2954,12 @@ void radv_rra_trace_init(struct radv_device *device);
 VkResult radv_rra_dump_trace(VkQueue vk_queue, char *filename);
 void radv_rra_trace_finish(VkDevice vk_device, struct radv_rra_trace_data *data);
 
+bool radv_flush_gang_leader_semaphore(struct radv_cmd_buffer *cmd_buffer);
+bool radv_flush_gang_follower_semaphore(struct radv_cmd_buffer *cmd_buffer);
+void radv_wait_gang_leader(struct radv_cmd_buffer *cmd_buffer);
+void radv_wait_gang_follower(struct radv_cmd_buffer *cmd_buffer);
+bool radv_gang_init(struct radv_cmd_buffer *cmd_buffer);
+
 void radv_memory_trace_init(struct radv_device *device);
 void radv_rmv_log_bo_allocate(struct radv_device *device, struct radeon_winsys_bo *bo, uint32_t size, bool is_internal);
 void radv_rmv_log_bo_destroy(struct radv_device *device, struct radeon_winsys_bo *bo);
-- 
GitLab


From 8cf2aa7d6c4b67540b02b1afeb1fa7a17346dcca Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Mon, 16 Oct 2023 12:05:21 +0200
Subject: [PATCH 2/3] radv: Use gang submission for copying images unsupported
 by SDMA.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

When the user tries to execute a transfer queue copy operation
that isn't supported by the SDMA engine, the driver must still
implement that copy somehow. To solve this, we use gang submission
and execute these copies on an async compute queue.

To simplify the implementation and avoid refactoring a large
portion of the code base, we create a full internal command
buffer object on which we execute the already existing
compute based copy command implementations.

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
---
 src/amd/vulkan/meta/radv_meta_copy.c  | 113 +++++++++++++++++++++++++-
 src/amd/vulkan/radv_cmd_buffer.c      |  36 +++++++-
 src/amd/vulkan/radv_physical_device.c |   8 +-
 src/amd/vulkan/radv_private.h         |   3 +
 src/amd/vulkan/radv_sdma.c            |  17 ++++
 src/amd/vulkan/radv_sdma.h            |   1 +
 6 files changed, 171 insertions(+), 7 deletions(-)

diff --git a/src/amd/vulkan/meta/radv_meta_copy.c b/src/amd/vulkan/meta/radv_meta_copy.c
index cc70b615b4b97..09b2b5322c34c 100644
--- a/src/amd/vulkan/meta/radv_meta_copy.c
+++ b/src/amd/vulkan/meta/radv_meta_copy.c
@@ -89,9 +89,60 @@ alloc_transfer_temp_bo(struct radv_cmd_buffer *cmd_buffer)
    return true;
 }
 
+static bool
+create_follower_cmdbuf(struct radv_cmd_buffer *cmd_buffer)
+{
+   if (cmd_buffer->gang.follower_cmdbuf)
+      return true;
+
+   if (!radv_gang_init(cmd_buffer))
+      return false;
+
+   if (radv_flush_gang_leader_semaphore(cmd_buffer))
+      radv_wait_gang_leader(cmd_buffer);
+
+   struct vk_command_pool *pool = calloc(1, sizeof(struct vk_command_pool));
+   if (!pool)
+      goto fail;
+
+   const VkCommandPoolCreateInfo pool_crinfo = {
+      .sType = VK_STRUCTURE_TYPE_COMMAND_POOL_CREATE_INFO,
+   };
+   if (vk_command_pool_init(&cmd_buffer->device->vk, pool, &pool_crinfo, NULL) != VK_SUCCESS)
+      goto fail;
+
+   struct vk_command_buffer *follower;
+   if (cmd_buffer->vk.ops->create(pool, &follower))
+      goto fail;
+
+   struct radv_cmd_buffer *f = (struct radv_cmd_buffer *)follower;
+   f->vk.ops->reset(follower, 0);
+
+   f->device->ws->cs_destroy(f->cs);
+   f->qf = RADV_QUEUE_COMPUTE;
+   f->cs = cmd_buffer->gang.cs;
+   f->state.flush_bits = cmd_buffer->gang.flush_bits;
+
+   cmd_buffer->gang.follower_cmdbuf = f;
+   return true;
+
+fail:
+   if (follower) {
+      cmd_buffer->vk.ops->destroy(follower);
+   }
+
+   if (pool) {
+      vk_command_pool_finish(pool);
+      free(pool);
+   }
+
+   vk_command_buffer_set_error(&cmd_buffer->vk, VK_ERROR_OUT_OF_HOST_MEMORY);
+   return false;
+}
+
 static void
 transfer_copy_buffer_image(struct radv_cmd_buffer *cmd_buffer, struct radv_buffer *buffer, struct radv_image *image,
-                           const VkBufferImageCopy2 *region, bool to_image)
+                           const VkImageLayout layout, const VkBufferImageCopy2 *region, bool to_image)
 {
    const struct radv_device *device = cmd_buffer->device;
    struct radeon_cmdbuf *cs = cmd_buffer->cs;
@@ -101,6 +152,40 @@ transfer_copy_buffer_image(struct radv_cmd_buffer *cmd_buffer, struct radv_buffe
    radv_cs_add_buffer(device->ws, cs, image->bindings[binding_idx].bo);
    radv_cs_add_buffer(device->ws, cs, buffer->bo);
 
+   if (!radv_sdma_supports_image(device, image)) {
+      if (!create_follower_cmdbuf(cmd_buffer))
+         return;
+
+      if (to_image) {
+         VkCopyBufferToImageInfo2 info = {
+            .sType = VK_STRUCTURE_TYPE_COPY_BUFFER_TO_IMAGE_INFO_2,
+            .pNext = NULL,
+            .regionCount = 1,
+            .pRegions = region,
+            .dstImage = radv_image_to_handle(image),
+            .dstImageLayout = layout,
+            .srcBuffer = radv_buffer_to_handle(buffer),
+         };
+         radv_CmdCopyBufferToImage2(radv_cmd_buffer_to_handle(cmd_buffer->gang.follower_cmdbuf), &info);
+      } else {
+         VkCopyImageToBufferInfo2 info = {
+            .sType = VK_STRUCTURE_TYPE_COPY_IMAGE_TO_BUFFER_INFO_2,
+            .pNext = NULL,
+            .regionCount = 1,
+            .pRegions = region,
+            .srcImage = radv_image_to_handle(image),
+            .srcImageLayout = layout,
+            .dstBuffer = radv_buffer_to_handle(buffer),
+         };
+         radv_CmdCopyImageToBuffer2(radv_cmd_buffer_to_handle(cmd_buffer->gang.follower_cmdbuf), &info);
+      }
+
+      return;
+   }
+
+   if (cmd_buffer->gang.cs && radv_flush_gang_follower_semaphore(cmd_buffer))
+      radv_wait_gang_follower(cmd_buffer);
+
    struct radv_sdma_surf buf = radv_sdma_get_buf_surf(buffer, image, region, aspect_mask);
    const struct radv_sdma_surf img =
       radv_sdma_get_surf(device, image, region->imageSubresource, region->imageOffset, aspect_mask);
@@ -122,7 +207,7 @@ copy_buffer_to_image(struct radv_cmd_buffer *cmd_buffer, struct radv_buffer *buf
                      VkImageLayout layout, const VkBufferImageCopy2 *region)
 {
    if (cmd_buffer->qf == RADV_QUEUE_TRANSFER) {
-      transfer_copy_buffer_image(cmd_buffer, buffer, image, region, true);
+      transfer_copy_buffer_image(cmd_buffer, buffer, image, layout, region, true);
       return;
    }
 
@@ -273,7 +358,7 @@ copy_image_to_buffer(struct radv_cmd_buffer *cmd_buffer, struct radv_buffer *buf
 {
    struct radv_device *device = cmd_buffer->device;
    if (cmd_buffer->qf == RADV_QUEUE_TRANSFER) {
-      transfer_copy_buffer_image(cmd_buffer, buffer, image, region, false);
+      transfer_copy_buffer_image(cmd_buffer, buffer, image, layout, region, false);
       return;
    }
 
@@ -391,6 +476,28 @@ transfer_copy_image(struct radv_cmd_buffer *cmd_buffer, struct radv_image *src_i
    struct radeon_cmdbuf *cs = cmd_buffer->cs;
    unsigned int dst_aspect_mask_remaining = region->dstSubresource.aspectMask;
 
+   if (!radv_sdma_supports_image(device, src_image) || !radv_sdma_supports_image(device, dst_image)) {
+      if (!create_follower_cmdbuf(cmd_buffer))
+         return;
+
+      VkCopyImageInfo2 info = {
+         .sType = VK_STRUCTURE_TYPE_COPY_IMAGE_INFO_2,
+         .pNext = NULL,
+         .regionCount = 1,
+         .pRegions = region,
+         .dstImage = radv_image_to_handle(dst_image),
+         .dstImageLayout = dst_image_layout,
+         .srcImage = radv_image_to_handle(src_image),
+         .srcImageLayout = src_image_layout,
+      };
+
+      radv_CmdCopyImage2(radv_cmd_buffer_to_handle(cmd_buffer->gang.follower_cmdbuf), &info);
+      return;
+   }
+
+   if (cmd_buffer->gang.cs && radv_flush_gang_follower_semaphore(cmd_buffer))
+      radv_wait_gang_follower(cmd_buffer);
+
    u_foreach_bit (b, region->srcSubresource.aspectMask) {
       const VkImageAspectFlags src_aspect_mask = BITFIELD_BIT(b);
       const VkImageAspectFlags dst_aspect_mask = BITFIELD_BIT(u_bit_scan(&dst_aspect_mask_remaining));
diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index 04c5909202569..206572a655e94 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -338,6 +338,16 @@ radv_destroy_cmd_buffer(struct vk_command_buffer *vk_cmd_buffer)
          cmd_buffer->device->ws->buffer_destroy(cmd_buffer->device->ws, cmd_buffer->upload.upload_bo);
       }
 
+      if (cmd_buffer->gang.follower_cmdbuf) {
+         /* Prevent double free of gang follower CS. */
+         cmd_buffer->gang.follower_cmdbuf->cs = NULL;
+
+         struct vk_command_pool *pool = cmd_buffer->gang.follower_cmdbuf->vk.pool;
+         radv_destroy_cmd_buffer(&cmd_buffer->gang.follower_cmdbuf->vk);
+         vk_command_pool_finish(pool);
+         free(pool);
+      }
+
       if (cmd_buffer->cs)
          cmd_buffer->device->ws->cs_destroy(cmd_buffer->cs);
       if (cmd_buffer->gang.cs)
@@ -619,10 +629,30 @@ radv_gang_barrier(struct radv_cmd_buffer *cmd_buffer, VkPipelineStageFlags2 src_
         VK_PIPELINE_STAGE_2_BOTTOM_OF_PIPE_BIT | VK_PIPELINE_STAGE_2_ALL_COMMANDS_BIT))
       dst_stage_mask |= cmd_buffer->state.dma_is_busy ? VK_PIPELINE_STAGE_2_TASK_SHADER_BIT_EXT : 0;
 
-   /* Increment the GFX/ACE semaphore when task shaders are blocked. */
-   if (dst_stage_mask & (VK_PIPELINE_STAGE_2_TOP_OF_PIPE_BIT | VK_PIPELINE_STAGE_2_DRAW_INDIRECT_BIT |
-                         VK_PIPELINE_STAGE_2_TASK_SHADER_BIT_EXT))
+   /* Increment the leader to follower semaphore when the leader wants to block the follower:
+    * - graphics command buffer: task shader execution needs to wait for something
+    * - transfer command buffer: a transfer operation on ACE needs to wait for a previous operation on SDMA
+    */
+   const VkPipelineStageFlags2 gang_leader_flags =
+      cmd_buffer->qf == RADV_QUEUE_TRANSFER
+         ? (VK_PIPELINE_STAGE_2_TOP_OF_PIPE_BIT_KHR | VK_PIPELINE_STAGE_2_TRANSFER_BIT_KHR |
+            VK_PIPELINE_STAGE_2_ALL_TRANSFER_BIT)
+         : (VK_PIPELINE_STAGE_2_TOP_OF_PIPE_BIT_KHR | VK_PIPELINE_STAGE_2_DRAW_INDIRECT_BIT |
+            VK_PIPELINE_STAGE_2_TASK_SHADER_BIT_EXT);
+   if (dst_stage_mask & gang_leader_flags)
       cmd_buffer->gang.sem.leader_value++;
+
+   /* Increment the follower to leader semaphore when the follower wants to block the leader:
+    * - graphics command buffer: not necessary yet
+    * - transfer command buffer: a transfer operation on SDMA needs to wait for a previous operation on ACE
+    */
+   const VkPipelineStageFlags2 gang_follower_flags =
+      cmd_buffer->qf == RADV_QUEUE_TRANSFER
+         ? (VK_PIPELINE_STAGE_2_BOTTOM_OF_PIPE_BIT_KHR | VK_PIPELINE_STAGE_2_TRANSFER_BIT_KHR |
+            VK_PIPELINE_STAGE_2_ALL_TRANSFER_BIT)
+         : 0;
+   if (src_stage_mask & gang_follower_flags)
+      cmd_buffer->gang.sem.follower_value++;
 }
 
 void
diff --git a/src/amd/vulkan/radv_physical_device.c b/src/amd/vulkan/radv_physical_device.c
index 867a0956e46e7..98632b1112a24 100644
--- a/src/amd/vulkan/radv_physical_device.c
+++ b/src/amd/vulkan/radv_physical_device.c
@@ -79,6 +79,11 @@ radv_transfer_queue_enabled(const struct radv_physical_device *pdevice)
        !(pdevice->instance->perftest_flags & RADV_PERFTEST_TRANSFER_QUEUE))
       return false;
 
+   /* Disable transfer queues when compue or gang submit is disabled. */
+   if (!pdevice->rad_info.has_gang_submit || !pdevice->rad_info.ip[AMD_IP_COMPUTE].num_queues ||
+       (pdevice->instance->debug_flags & RADV_DEBUG_NO_COMPUTE_QUEUE))
+      return false;
+
    return pdevice->rad_info.gfx_level >= GFX9;
 }
 
@@ -2187,7 +2192,8 @@ radv_get_physical_device_queue_family_properties(struct radv_physical_device *pd
       if (*pCount > idx) {
          *pQueueFamilyProperties[idx] = (VkQueueFamilyProperties){
             .queueFlags = VK_QUEUE_TRANSFER_BIT,
-            .queueCount = pdevice->rad_info.ip[AMD_IP_SDMA].num_queues,
+            .queueCount =
+               MIN2(pdevice->rad_info.ip[AMD_IP_SDMA].num_queues, pdevice->rad_info.ip[AMD_IP_COMPUTE].num_queues),
             .timestampValidBits = 64,
             .minImageTransferGranularity = (VkExtent3D){16, 16, 8},
          };
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index 80ddbcf7d9bb4..cd2f3be789ab9 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -1735,6 +1735,9 @@ struct radv_cmd_buffer {
       /** Follower command stream. */
       struct radeon_cmdbuf *cs;
 
+      /** Follower command buffer object (only initialized if necessary). */
+      struct radv_cmd_buffer *follower_cmdbuf;
+
       /** Flush bits for the follower cmdbuf. */
       enum radv_cmd_flush_bits flush_bits;
 
diff --git a/src/amd/vulkan/radv_sdma.c b/src/amd/vulkan/radv_sdma.c
index c086104d673c2..d509b8d7a6407 100644
--- a/src/amd/vulkan/radv_sdma.c
+++ b/src/amd/vulkan/radv_sdma.c
@@ -796,3 +796,20 @@ radv_sdma_copy_image_t2t_scanline(const struct radv_device *device, struct radeo
       }
    }
 }
+
+bool
+radv_sdma_supports_image(const struct radv_device *device, const struct radv_image *image)
+{
+   if (radv_is_format_emulated(device->physical_device, image->vk.format))
+      return false;
+
+   if (!device->physical_device->rad_info.sdma_supports_sparse &&
+       (image->vk.create_flags & (VK_IMAGE_CREATE_SPARSE_BINDING_BIT | VK_IMAGE_CREATE_SPARSE_RESIDENCY_BIT |
+                                  VK_IMAGE_CREATE_SPARSE_ALIASED_BIT)))
+      return false;
+
+   if (image->vk.samples != VK_SAMPLE_COUNT_1_BIT)
+      return false;
+
+   return true;
+}
diff --git a/src/amd/vulkan/radv_sdma.h b/src/amd/vulkan/radv_sdma.h
index bcc95919c97bf..1ced4ca9d36a0 100644
--- a/src/amd/vulkan/radv_sdma.h
+++ b/src/amd/vulkan/radv_sdma.h
@@ -95,6 +95,7 @@ void radv_sdma_copy_buffer(const struct radv_device *device, struct radeon_cmdbu
                            uint64_t size);
 void radv_sdma_fill_buffer(const struct radv_device *device, struct radeon_cmdbuf *cs, const uint64_t va,
                            const uint64_t size, const uint32_t value);
+bool radv_sdma_supports_image(const struct radv_device *device, const struct radv_image *image);
 
 #ifdef __cplusplus
 }
-- 
GitLab
