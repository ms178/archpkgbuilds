From 318578ff024f2d946a789b6b14528d9301bd535d Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Sat, 9 Dec 2023 22:15:24 +0100
Subject: [PATCH 1/7] radv: Implement image copies on transfer queues.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

When either of the images is linear then the imlementation can
use the same packets as used by the buffer/image copies.
However, tiled to tiled image copies use a separate packet.

Several variations of tiled to tiled copies are not supported
by the built-in packet and need a scanline copy as a workaround,
this will be implemented by an upcoming commit.

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
---
 src/amd/vulkan/meta/radv_meta_copy.c | 28 +++++++++
 src/amd/vulkan/radv_sdma.c           | 92 ++++++++++++++++++++++++++++
 src/amd/vulkan/radv_sdma.h           |  2 +
 3 files changed, 122 insertions(+)

diff --git a/src/amd/vulkan/meta/radv_meta_copy.c b/src/amd/vulkan/meta/radv_meta_copy.c
index 9c35ddb6b61f..31f4f7c9d826 100644
--- a/src/amd/vulkan/meta/radv_meta_copy.c
+++ b/src/amd/vulkan/meta/radv_meta_copy.c
@@ -382,10 +382,38 @@ radv_CmdCopyImageToBuffer2(VkCommandBuffer commandBuffer, const VkCopyImageToBuf
    }
 }
 
+static void
+transfer_copy_image(struct radv_cmd_buffer *cmd_buffer, struct radv_image *src_image, VkImageLayout src_image_layout,
+                    struct radv_image *dst_image, VkImageLayout dst_image_layout, const VkImageCopy2 *region)
+{
+   const struct radv_device *device = cmd_buffer->device;
+   struct radeon_cmdbuf *cs = cmd_buffer->cs;
+   radv_cs_add_buffer(device->ws, cs, src_image->bindings[0].bo);
+   radv_cs_add_buffer(device->ws, cs, dst_image->bindings[0].bo);
+   unsigned int dst_aspect_mask_remaining = region->dstSubresource.aspectMask;
+
+   u_foreach_bit (b, region->srcSubresource.aspectMask) {
+      const VkImageAspectFlags src_aspect_mask = BITFIELD_BIT(b);
+      const VkImageAspectFlags dst_aspect_mask = BITFIELD_BIT(u_bit_scan(&dst_aspect_mask_remaining));
+      const struct radv_sdma_surf src =
+         radv_sdma_get_surf(device, src_image, region->srcSubresource, region->srcOffset, src_aspect_mask);
+      const struct radv_sdma_surf dst =
+         radv_sdma_get_surf(device, dst_image, region->dstSubresource, region->dstOffset, dst_aspect_mask);
+      const VkExtent3D extent = radv_sdma_get_copy_extent(src_image, region->srcSubresource, region->extent);
+
+      radv_sdma_copy_image(device, cs, &src, &dst, extent);
+   }
+}
+
 static void
 copy_image(struct radv_cmd_buffer *cmd_buffer, struct radv_image *src_image, VkImageLayout src_image_layout,
            struct radv_image *dst_image, VkImageLayout dst_image_layout, const VkImageCopy2 *region)
 {
+   if (cmd_buffer->qf == RADV_QUEUE_TRANSFER) {
+      transfer_copy_image(cmd_buffer, src_image, src_image_layout, dst_image, dst_image_layout, region);
+      return;
+   }
+
    struct radv_meta_saved_state saved_state;
    bool cs;
 
diff --git a/src/amd/vulkan/radv_sdma.c b/src/amd/vulkan/radv_sdma.c
index f03cbb289788..00c69bd12cfb 100644
--- a/src/amd/vulkan/radv_sdma.c
+++ b/src/amd/vulkan/radv_sdma.c
@@ -474,6 +474,79 @@ radv_sdma_emit_copy_tiled_sub_window(const struct radv_device *device, struct ra
    assert(cs->cdw == cdw_end);
 }
 
+static void
+radv_sdma_emit_copy_t2t_sub_window(const struct radv_device *device, struct radeon_cmdbuf *cs,
+                                   const struct radv_sdma_surf *const src, const struct radv_sdma_surf *const dst,
+                                   const VkExtent3D px_extent)
+{
+   /* This packet has significant differences in its bit fields between SDMA v2.4, v4 and v5.
+    * Most notably, v5 (that is in GFX10) allows selecting a mip level.
+    * On GFX10+ this supports DCC, but cannot copy a compressed surface to another compressed surface.
+    *
+    * We currently only support the SDMA v4+ versions of this packet.
+    */
+
+   assert(device->physical_device->rad_info.gfx_level >= GFX9);
+   assert(!src->meta_va || !dst->meta_va);
+
+   if (device->physical_device->rad_info.gfx_level == GFX9) {
+      /* SDMA v4 doesn't support mip_id selection in the T2T copy packet. */
+      assert(src->header_dword >> 24 == 0);
+      assert(dst->header_dword >> 24 == 0);
+      /* SDMA v4 doesn't support any image metadata. */
+      assert(!src->meta_va);
+      assert(!dst->meta_va);
+   }
+
+   /* Despite the name, this can indicate DCC or HTILE metadata. */
+   const uint32_t dcc = src->meta_va || dst->meta_va;
+   /* 0 = compress (src is uncompressed), 1 = decompress (src is compressed). */
+   const uint32_t dcc_dir = src->meta_va && !dst->meta_va;
+   /* Trusted memory zone. */
+   const uint32_t tmz = 0;
+
+   const VkOffset3D src_off = radv_sdma_pixel_offset_to_blocks(src->offset, src->blk_w, src->blk_h);
+   const VkOffset3D dst_off = radv_sdma_pixel_offset_to_blocks(dst->offset, dst->blk_w, dst->blk_h);
+   const VkExtent3D src_ext = radv_sdma_pixel_extent_to_blocks(src->extent, src->blk_w, src->blk_h);
+   const VkExtent3D dst_ext = radv_sdma_pixel_extent_to_blocks(dst->extent, dst->blk_w, dst->blk_h);
+   const VkExtent3D ext = radv_sdma_pixel_extent_to_blocks(px_extent, src->blk_w, src->blk_h);
+
+   assert(util_is_power_of_two_nonzero(src->bpp));
+   assert(util_is_power_of_two_nonzero(dst->bpp));
+
+   ASSERTED unsigned cdw_end = radeon_check_space(device->ws, cs, 15 + (dcc ? 3 : 0));
+
+   radeon_emit(cs, SDMA_PACKET(SDMA_OPCODE_COPY, SDMA_COPY_SUB_OPCODE_T2T_SUB_WINDOW, 0) | tmz << 18 | dcc << 19 |
+                      dcc_dir << 31 | src->header_dword);
+   radeon_emit(cs, src->va);
+   radeon_emit(cs, src->va >> 32);
+   radeon_emit(cs, src_off.x | src_off.y << 16);
+   radeon_emit(cs, src_off.z | (src_ext.width - 1) << 16);
+   radeon_emit(cs, (src_ext.height - 1) | (src_ext.depth - 1) << 16);
+   radeon_emit(cs, src->info_dword);
+   radeon_emit(cs, dst->va);
+   radeon_emit(cs, dst->va >> 32);
+   radeon_emit(cs, dst_off.x | dst_off.y << 16);
+   radeon_emit(cs, dst_off.z | (dst_ext.width - 1) << 16);
+   radeon_emit(cs, (dst_ext.height - 1) | (dst_ext.depth - 1) << 16);
+   radeon_emit(cs, dst->info_dword);
+   radeon_emit(cs, (ext.width - 1) | (ext.height - 1) << 16);
+   radeon_emit(cs, (ext.depth - 1));
+
+   if (dst->meta_va) {
+      const uint32_t write_compress_enable = 1;
+      radeon_emit(cs, dst->meta_va);
+      radeon_emit(cs, dst->meta_va >> 32);
+      radeon_emit(cs, dst->meta_config | write_compress_enable << 28);
+   } else if (src->meta_va) {
+      radeon_emit(cs, src->meta_va);
+      radeon_emit(cs, src->meta_va >> 32);
+      radeon_emit(cs, src->meta_config);
+   }
+
+   assert(cs->cdw == cdw_end);
+}
+
 void
 radv_sdma_copy_buffer_image(const struct radv_device *device, struct radeon_cmdbuf *cs,
                             const struct radv_sdma_surf *buf, const struct radv_sdma_surf *img, const VkExtent3D extent,
@@ -577,3 +650,22 @@ radv_sdma_copy_buffer_image_unaligned(const struct radv_device *device, struct r
       }
    }
 }
+
+void
+radv_sdma_copy_image(const struct radv_device *device, struct radeon_cmdbuf *cs, const struct radv_sdma_surf *src,
+                     const struct radv_sdma_surf *dst, const VkExtent3D extent)
+{
+   if (src->is_linear) {
+      if (dst->is_linear) {
+         radv_sdma_emit_copy_linear_sub_window(device, cs, src, dst, extent);
+      } else {
+         radv_sdma_emit_copy_tiled_sub_window(device, cs, dst, src, extent, false);
+      }
+   } else {
+      if (dst->is_linear) {
+         radv_sdma_emit_copy_tiled_sub_window(device, cs, src, dst, extent, true);
+      } else {
+         radv_sdma_emit_copy_t2t_sub_window(device, cs, src, dst, extent);
+      }
+   }
+}
diff --git a/src/amd/vulkan/radv_sdma.h b/src/amd/vulkan/radv_sdma.h
index 5f5a701e6f13..e089618b4077 100644
--- a/src/amd/vulkan/radv_sdma.h
+++ b/src/amd/vulkan/radv_sdma.h
@@ -81,6 +81,8 @@ void radv_sdma_copy_buffer_image_unaligned(const struct radv_device *device, str
                                            const struct radv_sdma_surf *buf, const struct radv_sdma_surf *img_in,
                                            const VkExtent3D copy_extent, struct radeon_winsys_bo *temp_bo,
                                            bool to_image);
+void radv_sdma_copy_image(const struct radv_device *device, struct radeon_cmdbuf *cs, const struct radv_sdma_surf *src,
+                          const struct radv_sdma_surf *dst, const VkExtent3D extent);
 void radv_sdma_copy_buffer(const struct radv_device *device, struct radeon_cmdbuf *cs, uint64_t src_va, uint64_t dst_va,
                            uint64_t size);
 void radv_sdma_fill_buffer(const struct radv_device *device, struct radeon_cmdbuf *cs, const uint64_t va,
-- 
GitLab


From 757b49425e10bb8e13043b53e717a6f755e16a2a Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Sat, 9 Dec 2023 22:42:46 +0100
Subject: [PATCH 2/7] radv: Implement T2T scanline copy workaround.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

The built-in tiled-to-tiled copy packet doesn't support copying
between images that don't meet certain criteria such as alignment,
micro tile format, compression state etc.

To work around this, we copy the image piece by piece to a
temporary buffer that we know is supported,
and then copy it to the intended destination.

The implementation assumes that at least one pixel row of the
image fits into the temporary buffer, and will try to copy as
many rows as fit.

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
---
 src/amd/vulkan/meta/radv_meta_copy.c |   9 +-
 src/amd/vulkan/radv_sdma.c           | 132 +++++++++++++++++++++++++++
 src/amd/vulkan/radv_sdma.h           |  22 +++--
 3 files changed, 155 insertions(+), 8 deletions(-)

diff --git a/src/amd/vulkan/meta/radv_meta_copy.c b/src/amd/vulkan/meta/radv_meta_copy.c
index 31f4f7c9d826..03bde6f4ec29 100644
--- a/src/amd/vulkan/meta/radv_meta_copy.c
+++ b/src/amd/vulkan/meta/radv_meta_copy.c
@@ -401,7 +401,14 @@ transfer_copy_image(struct radv_cmd_buffer *cmd_buffer, struct radv_image *src_i
          radv_sdma_get_surf(device, dst_image, region->dstSubresource, region->dstOffset, dst_aspect_mask);
       const VkExtent3D extent = radv_sdma_get_copy_extent(src_image, region->srcSubresource, region->extent);
 
-      radv_sdma_copy_image(device, cs, &src, &dst, extent);
+      if (radv_sdma_use_t2t_scanline_copy(device, &src, &dst, extent)) {
+         if (!alloc_transfer_temp_bo(cmd_buffer))
+            return;
+
+         radv_sdma_copy_image_t2t_scanline(device, cs, &src, &dst, extent, cmd_buffer->transfer.copy_temp);
+      } else {
+         radv_sdma_copy_image(device, cs, &src, &dst, extent);
+      }
    }
 }
 
diff --git a/src/amd/vulkan/radv_sdma.c b/src/amd/vulkan/radv_sdma.c
index 00c69bd12cfb..f07ad8c71216 100644
--- a/src/amd/vulkan/radv_sdma.c
+++ b/src/amd/vulkan/radv_sdma.c
@@ -37,6 +37,22 @@ struct radv_sdma_chunked_copy_info {
    unsigned num_rows_per_copy;
 };
 
+static const VkExtent3D radv_sdma_t2t_alignment_2d_and_planar[] = {
+   {16, 16, 1}, /* 1 bpp */
+   {16, 8, 1},  /* 2 bpp */
+   {8, 8, 1},   /* 4 bpp */
+   {8, 4, 1},   /* 8 bpp */
+   {4, 4, 1},   /* 16 bpp */
+};
+
+static const VkExtent3D radv_sdma_t2t_alignment_3d[] = {
+   {8, 4, 8}, /* 1 bpp */
+   {4, 4, 8}, /* 2 bpp */
+   {4, 4, 4}, /* 4 bpp */
+   {4, 2, 4}, /* 8 bpp */
+   {2, 2, 4}, /* 16 bpp */
+};
+
 ALWAYS_INLINE static unsigned
 radv_sdma_pitch_alignment(const struct radv_device *device, const unsigned bpp)
 {
@@ -265,7 +281,10 @@ radv_sdma_get_surf(const struct radv_device *const device, const struct radv_ima
       .bpp = surf->bpe,
       .blk_w = surf->blk_w,
       .blk_h = surf->blk_h,
+      .mip_levels = image->vk.mip_levels,
+      .micro_tile_mode = surf->micro_tile_mode,
       .is_linear = surf->is_linear,
+      .is_3d = surf->u.gfx9.resource_type == RADEON_RESOURCE_3D,
    };
 
    if (surf->is_linear) {
@@ -669,3 +688,116 @@ radv_sdma_copy_image(const struct radv_device *device, struct radeon_cmdbuf *cs,
       }
    }
 }
+
+bool
+radv_sdma_use_t2t_scanline_copy(const struct radv_device *device, const struct radv_sdma_surf *src,
+                                const struct radv_sdma_surf *dst, const VkExtent3D extent)
+{
+   /* These need a linear-to-linear / linear-to-tiled copy. */
+   if (src->is_linear || dst->is_linear)
+      return false;
+
+   /* SDMA can't do format conversion. */
+   assert(src->bpp == dst->bpp);
+
+   const enum sdma_version ver = device->physical_device->rad_info.sdma_ip_version;
+   if (ver < SDMA_5_0) {
+      /* SDMA v4.x and older doesn't support proper mip level selection. */
+      if (src->mip_levels > 1 || dst->mip_levels > 1)
+         return true;
+   }
+
+   /* The two images can have a different block size,
+    * but must have the same swizzle mode.
+    */
+   if (src->micro_tile_mode != dst->micro_tile_mode)
+      return true;
+
+   /* The T2T subwindow copy packet only has fields for one metadata configuration.
+    * It can either compress or decompress, or copy uncompressed images, but it
+    * can't copy from a compressed image to another.
+    */
+   if (src->meta_va && dst->meta_va)
+      return true;
+
+   const bool needs_3d_alignment = src->is_3d && (src->micro_tile_mode == RADEON_MICRO_MODE_DISPLAY ||
+                                                  src->micro_tile_mode == RADEON_MICRO_MODE_STANDARD);
+   const unsigned log2bpp = util_logbase2(src->bpp);
+   const VkExtent3D *const alignment =
+      needs_3d_alignment ? &radv_sdma_t2t_alignment_3d[log2bpp] : &radv_sdma_t2t_alignment_2d_and_planar[log2bpp];
+
+   const VkExtent3D copy_extent_blk = radv_sdma_pixel_extent_to_blocks(extent, src->blk_w, src->blk_h);
+   const VkOffset3D src_offset_blk = radv_sdma_pixel_offset_to_blocks(src->offset, src->blk_w, src->blk_h);
+   const VkOffset3D dst_offset_blk = radv_sdma_pixel_offset_to_blocks(dst->offset, dst->blk_w, dst->blk_h);
+
+   if (!radv_is_aligned(copy_extent_blk.width, alignment->width) ||
+       !radv_is_aligned(copy_extent_blk.height, alignment->height) ||
+       !radv_is_aligned(copy_extent_blk.depth, alignment->depth))
+      return true;
+
+   if (!radv_is_aligned(src_offset_blk.x, alignment->width) || !radv_is_aligned(src_offset_blk.y, alignment->height) ||
+       !radv_is_aligned(src_offset_blk.z, alignment->depth))
+      return true;
+
+   if (!radv_is_aligned(dst_offset_blk.x, alignment->width) || !radv_is_aligned(dst_offset_blk.y, alignment->height) ||
+       !radv_is_aligned(dst_offset_blk.z, alignment->depth))
+      return true;
+
+   return false;
+}
+
+void
+radv_sdma_copy_image_t2t_scanline(const struct radv_device *device, struct radeon_cmdbuf *cs,
+                                  const struct radv_sdma_surf *src, const struct radv_sdma_surf *dst,
+                                  const VkExtent3D extent, struct radeon_winsys_bo *temp_bo)
+{
+   const struct radv_sdma_chunked_copy_info info = radv_sdma_get_chunked_copy_info(device, src, extent);
+   struct radv_sdma_surf t2l_src = *src;
+   struct radv_sdma_surf t2l_dst = {
+      .va = temp_bo->va,
+      .bpp = src->bpp,
+      .blk_w = src->blk_w,
+      .blk_h = src->blk_h,
+      .pitch = info.aligned_row_pitch * src->blk_w,
+   };
+   struct radv_sdma_surf l2t_dst = *dst;
+   struct radv_sdma_surf l2t_src = {
+      .va = temp_bo->va,
+      .bpp = src->bpp,
+      .blk_w = dst->blk_w,
+      .blk_h = dst->blk_h,
+      .pitch = info.aligned_row_pitch * dst->blk_w,
+   };
+
+   for (unsigned slice = 0; slice < extent.depth; ++slice) {
+      for (unsigned row = 0; row < info.extent_vertical_blocks; row += info.num_rows_per_copy) {
+         const unsigned rows = MIN2(info.extent_vertical_blocks - row, info.num_rows_per_copy);
+
+         const VkExtent3D t2l_extent = {
+            .width = info.extent_horizontal_blocks * src->blk_w,
+            .height = rows * src->blk_h,
+            .depth = 1,
+         };
+
+         t2l_src.offset.y = src->offset.y + row * src->blk_h;
+         t2l_src.offset.z = src->offset.z + slice;
+         t2l_dst.slice_pitch = t2l_dst.pitch * t2l_extent.height;
+
+         radv_sdma_emit_copy_tiled_sub_window(device, cs, &t2l_src, &t2l_dst, t2l_extent, true);
+         radv_sdma_emit_nop(device, cs);
+
+         const VkExtent3D l2t_extent = {
+            .width = info.extent_horizontal_blocks * dst->blk_w,
+            .height = rows * dst->blk_h,
+            .depth = 1,
+         };
+
+         l2t_dst.offset.y = dst->offset.y + row * dst->blk_h;
+         l2t_dst.offset.z = dst->offset.z + slice;
+         l2t_src.slice_pitch = l2t_src.pitch * l2t_extent.height;
+
+         radv_sdma_emit_copy_tiled_sub_window(device, cs, &l2t_dst, &l2t_src, l2t_extent, false);
+         radv_sdma_emit_nop(device, cs);
+      }
+   }
+}
diff --git a/src/amd/vulkan/radv_sdma.h b/src/amd/vulkan/radv_sdma.h
index e089618b4077..bcc95919c97b 100644
--- a/src/amd/vulkan/radv_sdma.h
+++ b/src/amd/vulkan/radv_sdma.h
@@ -31,13 +31,16 @@ extern "C" {
 #endif
 
 struct radv_sdma_surf {
-   VkExtent3D extent; /* Image extent. */
-   VkOffset3D offset; /* Image offset. */
-   uint64_t va;       /* Virtual address of image data. */
-   unsigned bpp;      /* Bytes per pixel. */
-   unsigned blk_w;    /* Image format block width in pixels. */
-   unsigned blk_h;    /* Image format block height in pixels. */
-   bool is_linear;    /* Whether the image is linear. */
+   VkExtent3D extent;       /* Image extent. */
+   VkOffset3D offset;       /* Image offset. */
+   uint64_t va;             /* Virtual address of image data. */
+   unsigned bpp;            /* Bytes per pixel. */
+   unsigned blk_w;          /* Image format block width in pixels. */
+   unsigned blk_h;          /* Image format block height in pixels. */
+   unsigned mip_levels;     /* Mip levels in the image. */
+   uint8_t micro_tile_mode; /* Micro tile mode of the image. */
+   bool is_linear;          /* Whether the image is linear. */
+   bool is_3d;              /* Whether the image is 3-dimensional. */
 
    union {
       /* linear images only */
@@ -83,6 +86,11 @@ void radv_sdma_copy_buffer_image_unaligned(const struct radv_device *device, str
                                            bool to_image);
 void radv_sdma_copy_image(const struct radv_device *device, struct radeon_cmdbuf *cs, const struct radv_sdma_surf *src,
                           const struct radv_sdma_surf *dst, const VkExtent3D extent);
+bool radv_sdma_use_t2t_scanline_copy(const struct radv_device *device, const struct radv_sdma_surf *src,
+                                     const struct radv_sdma_surf *dst, const VkExtent3D extent);
+void radv_sdma_copy_image_t2t_scanline(const struct radv_device *device, struct radeon_cmdbuf *cs,
+                                       const struct radv_sdma_surf *src, const struct radv_sdma_surf *dst,
+                                       const VkExtent3D extent, struct radeon_winsys_bo *temp_bo);
 void radv_sdma_copy_buffer(const struct radv_device *device, struct radeon_cmdbuf *cs, uint64_t src_va, uint64_t dst_va,
                            uint64_t size);
 void radv_sdma_fill_buffer(const struct radv_device *device, struct radeon_cmdbuf *cs, const uint64_t va,
-- 
GitLab


From ac403f42c75c254c76119e1b70aaf96d8d7dfc6a Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Fri, 13 Oct 2023 23:21:52 +0200
Subject: [PATCH 3/7] radv: Expose transfer queues, hidden behind a perftest
 flag.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Closes: https://gitlab.freedesktop.org/mesa/mesa/-/issues/850
Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
---
 docs/envvars.rst                      |  4 ++-
 src/amd/vulkan/radv_debug.h           |  1 +
 src/amd/vulkan/radv_instance.c        |  1 +
 src/amd/vulkan/radv_physical_device.c | 42 +++++++++++++++++++++++----
 4 files changed, 42 insertions(+), 6 deletions(-)

diff --git a/docs/envvars.rst b/docs/envvars.rst
index 084e37fa1fdd..1b9e190f70b3 100644
--- a/docs/envvars.rst
+++ b/docs/envvars.rst
@@ -995,7 +995,7 @@ Clover environment variables
    allows specifying additional linker options. Specified options are
    appended after the options set by the OpenCL program in
    ``clLinkProgram``.
-   
+
 .. _rusticl-env-var:
 
 .. envvar:: IRIS_ENABLE_CLOVER
@@ -1339,6 +1339,8 @@ RADV driver environment variables
       enable optimizations to move more driver internal objects to VRAM.
    ``rtwave64``
       enable wave64 for ray tracing shaders (GFX10+)
+   ``transfer_queue``
+      enable experimental transfer queue support
    ``video_decode``
       enable experimental video decoding support
    ``gsfastlaunch2``
diff --git a/src/amd/vulkan/radv_debug.h b/src/amd/vulkan/radv_debug.h
index d28c2de80438..73ccead2a679 100644
--- a/src/amd/vulkan/radv_debug.h
+++ b/src/amd/vulkan/radv_debug.h
@@ -89,6 +89,7 @@ enum {
    RADV_PERFTEST_VIDEO_DECODE = 1u << 11,
    RADV_PERFTEST_DMA_SHADERS = 1u << 12,
    RADV_PERFTEST_GS_FAST_LAUNCH_2 = 1u << 13,
+   RADV_PERFTEST_TRANSFER_QUEUE = 1u << 14,
 };
 
 bool radv_init_trace(struct radv_device *device);
diff --git a/src/amd/vulkan/radv_instance.c b/src/amd/vulkan/radv_instance.c
index 762b4b3383d7..c0877f53ce13 100644
--- a/src/amd/vulkan/radv_instance.c
+++ b/src/amd/vulkan/radv_instance.c
@@ -100,6 +100,7 @@ static const struct debug_control radv_perftest_options[] = {{"localbos", RADV_P
                                                              {"video_decode", RADV_PERFTEST_VIDEO_DECODE},
                                                              {"dmashaders", RADV_PERFTEST_DMA_SHADERS},
                                                              {"gsfastlaunch2", RADV_PERFTEST_GS_FAST_LAUNCH_2},
+                                                             {"transfer_queue", RADV_PERFTEST_TRANSFER_QUEUE},
                                                              {NULL, 0}};
 
 const char *
diff --git a/src/amd/vulkan/radv_physical_device.c b/src/amd/vulkan/radv_physical_device.c
index 17caa2ddcec9..5ef1feafc7c4 100644
--- a/src/amd/vulkan/radv_physical_device.c
+++ b/src/amd/vulkan/radv_physical_device.c
@@ -71,6 +71,18 @@ radv_taskmesh_enabled(const struct radv_physical_device *pdevice)
           !(pdevice->instance->debug_flags & RADV_DEBUG_NO_COMPUTE_QUEUE) && pdevice->rad_info.has_gang_submit;
 }
 
+static bool
+radv_transfer_queue_enabled(const struct radv_physical_device *pdevice)
+{
+   /* Check if the GPU has supported SDMA and transfer queues are allowed. */
+   if (pdevice->rad_info.sdma_ip_version == SDMA_UNKNOWN || !pdevice->rad_info.ip[AMD_IP_SDMA].num_queues ||
+       !(pdevice->instance->perftest_flags & RADV_PERFTEST_TRANSFER_QUEUE))
+      return false;
+
+   /* Enable transfer queues on GFX9+. */
+   return pdevice->rad_info.gfx_level >= GFX9;
+}
+
 static bool
 radv_vrs_attachment_enabled(const struct radv_physical_device *pdevice)
 {
@@ -181,6 +193,11 @@ radv_physical_device_init_queue_table(struct radv_physical_device *pdevice)
       }
    }
 
+   if (radv_transfer_queue_enabled(pdevice)) {
+      pdevice->vk_queue_to_radv[idx] = RADV_QUEUE_TRANSFER;
+      idx++;
+   }
+
    pdevice->vk_queue_to_radv[idx++] = RADV_QUEUE_SPARSE;
 
    pdevice->num_queues = idx;
@@ -2088,6 +2105,10 @@ radv_get_physical_device_queue_family_properties(struct radv_physical_device *pd
          num_queue_families++;
    }
 
+   if (radv_transfer_queue_enabled(pdevice)) {
+      num_queue_families++;
+   }
+
    if (pQueueFamilyProperties == NULL) {
       *pCount = num_queue_families;
       return;
@@ -2140,6 +2161,18 @@ radv_get_physical_device_queue_family_properties(struct radv_physical_device *pd
       }
    }
 
+   if (radv_transfer_queue_enabled(pdevice)) {
+      if (*pCount > idx) {
+         *pQueueFamilyProperties[idx] = (VkQueueFamilyProperties){
+            .queueFlags = VK_QUEUE_TRANSFER_BIT,
+            .queueCount = pdevice->rad_info.ip[AMD_IP_SDMA].num_queues,
+            .timestampValidBits = 64,
+            .minImageTransferGranularity = (VkExtent3D){16, 16, 8},
+         };
+         idx++;
+      }
+   }
+
    if (*pCount > idx) {
       *pQueueFamilyProperties[idx] = (VkQueueFamilyProperties){
          .queueFlags = VK_QUEUE_SPARSE_BINDING_BIT,
@@ -2170,13 +2203,12 @@ radv_GetPhysicalDeviceQueueFamilyProperties2(VkPhysicalDevice physicalDevice, ui
       return;
    }
    VkQueueFamilyProperties *properties[] = {
-      &pQueueFamilyProperties[0].queueFamilyProperties,
-      &pQueueFamilyProperties[1].queueFamilyProperties,
-      &pQueueFamilyProperties[2].queueFamilyProperties,
-      &pQueueFamilyProperties[3].queueFamilyProperties,
+      &pQueueFamilyProperties[0].queueFamilyProperties, &pQueueFamilyProperties[1].queueFamilyProperties,
+      &pQueueFamilyProperties[2].queueFamilyProperties, &pQueueFamilyProperties[3].queueFamilyProperties,
+      &pQueueFamilyProperties[4].queueFamilyProperties,
    };
    radv_get_physical_device_queue_family_properties(pdevice, pCount, properties);
-   assert(*pCount <= 4);
+   assert(*pCount <= 5);
 
    for (uint32_t i = 0; i < *pCount; i++) {
       vk_foreach_struct (ext, pQueueFamilyProperties[i].pNext) {
-- 
GitLab


From b231b03fc48865fca504ce1c5f4d4285e3abbeb6 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Sat, 21 Oct 2023 14:13:38 +0200
Subject: [PATCH 4/7] radv: Declare some gang submit functions in radv private
 header.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

They will be called from the transfer copy functions.

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
---
 src/amd/vulkan/radv_cmd_buffer.c | 8 ++++----
 src/amd/vulkan/radv_private.h    | 6 ++++++
 2 files changed, 10 insertions(+), 4 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index dcbab2471111..c8e42388d204 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -685,7 +685,7 @@ radv_flush_gang_semaphore(struct radv_cmd_buffer *cmd_buffer, struct radeon_cmdb
    return true;
 }
 
-ALWAYS_INLINE static bool
+bool
 radv_flush_gang_leader_semaphore(struct radv_cmd_buffer *cmd_buffer)
 {
    if (!radv_gang_leader_sem_dirty(cmd_buffer))
@@ -696,7 +696,7 @@ radv_flush_gang_leader_semaphore(struct radv_cmd_buffer *cmd_buffer)
    return radv_flush_gang_semaphore(cmd_buffer, cmd_buffer->cs, cmd_buffer->qf, 0, cmd_buffer->gang.sem.leader_value);
 }
 
-ALWAYS_INLINE static bool
+bool
 radv_flush_gang_follower_semaphore(struct radv_cmd_buffer *cmd_buffer)
 {
    if (!radv_gang_follower_sem_dirty(cmd_buffer))
@@ -717,14 +717,14 @@ radv_wait_gang_semaphore(struct radv_cmd_buffer *cmd_buffer, struct radeon_cmdbu
    radv_cp_wait_mem(cs, qf, WAIT_REG_MEM_GREATER_OR_EQUAL, cmd_buffer->gang.sem.va + va_off, value, 0xffffffff);
 }
 
-ALWAYS_INLINE static void
+void
 radv_wait_gang_leader(struct radv_cmd_buffer *cmd_buffer)
 {
    /* Follower waits for the semaphore which the gang leader wrote. */
    radv_wait_gang_semaphore(cmd_buffer, cmd_buffer->gang.cs, RADV_QUEUE_COMPUTE, 0, cmd_buffer->gang.sem.leader_value);
 }
 
-ALWAYS_INLINE static void
+void
 radv_wait_gang_follower(struct radv_cmd_buffer *cmd_buffer)
 {
    /* Gang leader waits for the semaphore which the follower wrote. */
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index 2af4a0e2cfbe..f04bfe343da8 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -2883,6 +2883,12 @@ void radv_rra_trace_init(struct radv_device *device);
 VkResult radv_rra_dump_trace(VkQueue vk_queue, char *filename);
 void radv_rra_trace_finish(VkDevice vk_device, struct radv_rra_trace_data *data);
 
+bool radv_flush_gang_leader_semaphore(struct radv_cmd_buffer *cmd_buffer);
+bool radv_flush_gang_follower_semaphore(struct radv_cmd_buffer *cmd_buffer);
+void radv_wait_gang_leader(struct radv_cmd_buffer *cmd_buffer);
+void radv_wait_gang_follower(struct radv_cmd_buffer *cmd_buffer);
+bool radv_gang_init(struct radv_cmd_buffer *cmd_buffer);
+
 void radv_memory_trace_init(struct radv_device *device);
 void radv_rmv_log_bo_allocate(struct radv_device *device, struct radeon_winsys_bo *bo, uint32_t size, bool is_internal);
 void radv_rmv_log_bo_destroy(struct radv_device *device, struct radeon_winsys_bo *bo);
-- 
GitLab


From d55bd5a6eb19d5c801c6bd01d8218752a69c1ccb Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Mon, 16 Oct 2023 12:05:21 +0200
Subject: [PATCH 5/7] radv: Use gang submission for copying images unsupported
 by SDMA.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

When the user tries to execute a transfer queue copy operation
that isn't supported by the SDMA engine, the driver must still
implement that copy somehow. To solve this, we use gang submission
and execute these copies on an async compute queue.

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
---
 src/amd/vulkan/meta/radv_meta_copy.c  | 79 ++++++++++++++++++++++++++-
 src/amd/vulkan/radv_physical_device.c |  8 ++-
 src/amd/vulkan/radv_sdma.c            | 17 ++++++
 src/amd/vulkan/radv_sdma.h            |  1 +
 4 files changed, 101 insertions(+), 4 deletions(-)

diff --git a/src/amd/vulkan/meta/radv_meta_copy.c b/src/amd/vulkan/meta/radv_meta_copy.c
index 03bde6f4ec29..6e06eff259c4 100644
--- a/src/amd/vulkan/meta/radv_meta_copy.c
+++ b/src/amd/vulkan/meta/radv_meta_copy.c
@@ -89,9 +89,40 @@ alloc_transfer_temp_bo(struct radv_cmd_buffer *cmd_buffer)
    return true;
 }
 
+typedef void (*transfer_copy_func)(VkCommandBuffer c, void *info);
+
+static void
+transfer_gang_exec(struct radv_cmd_buffer *cmd_buffer, transfer_copy_func func, void *info)
+{
+   if (!radv_gang_init(cmd_buffer))
+      return;
+
+   /* Save original command buffer CS. */
+   const enum radv_queue_family qf = cmd_buffer->qf;
+   struct radeon_cmdbuf *sdma_cs = cmd_buffer->cs;
+
+   /* Set gang follower as the main CS. */
+   cmd_buffer->qf = RADV_QUEUE_COMPUTE;
+   cmd_buffer->cs = cmd_buffer->gang.cs;
+
+   /* Assume that func() will emit a flush, so add gang flush bits to main bits. */
+   cmd_buffer->state.flush_bits |= cmd_buffer->gang.flush_bits;
+
+   /* Execute a Vulkan command on the gang follower CS. */
+   VkCommandBuffer vk_cmd_buf = radv_cmd_buffer_to_handle(cmd_buffer);
+   func(vk_cmd_buf, info);
+
+   /* Restore original CS. */
+   cmd_buffer->qf = qf;
+   cmd_buffer->cs = sdma_cs;
+
+   /* Assume that func() emitted a flush on the follower CS. */
+   cmd_buffer->gang.flush_bits = 0;
+}
+
 static void
 transfer_copy_buffer_image(struct radv_cmd_buffer *cmd_buffer, struct radv_buffer *buffer, struct radv_image *image,
-                           const VkBufferImageCopy2 *region, bool to_image)
+                           const VkImageLayout layout, const VkBufferImageCopy2 *region, bool to_image)
 {
    const struct radv_device *device = cmd_buffer->device;
    struct radeon_cmdbuf *cs = cmd_buffer->cs;
@@ -99,6 +130,33 @@ transfer_copy_buffer_image(struct radv_cmd_buffer *cmd_buffer, struct radv_buffe
    radv_cs_add_buffer(device->ws, cs, image->bindings[0].bo);
    radv_cs_add_buffer(device->ws, cs, buffer->bo);
 
+   if (!radv_sdma_supports_image(device, image)) {
+      if (to_image) {
+         VkCopyBufferToImageInfo2 info = {
+            .sType = VK_STRUCTURE_TYPE_COPY_BUFFER_TO_IMAGE_INFO_2,
+            .pNext = NULL,
+            .regionCount = 1,
+            .pRegions = region,
+            .dstImage = radv_image_to_handle(image),
+            .dstImageLayout = layout,
+            .srcBuffer = radv_buffer_to_handle(buffer),
+         };
+         transfer_gang_exec(cmd_buffer, (transfer_copy_func)radv_CmdCopyBufferToImage2, &info);
+      } else {
+         VkCopyImageToBufferInfo2 info = {
+            .sType = VK_STRUCTURE_TYPE_COPY_IMAGE_TO_BUFFER_INFO_2,
+            .pNext = NULL,
+            .regionCount = 1,
+            .pRegions = region,
+            .srcImage = radv_image_to_handle(image),
+            .srcImageLayout = layout,
+            .dstBuffer = radv_buffer_to_handle(buffer),
+         };
+         transfer_gang_exec(cmd_buffer, (transfer_copy_func)radv_CmdCopyImageToBuffer2, &info);
+      }
+      return;
+   }
+
    const VkImageAspectFlags aspect_mask = region->imageSubresource.aspectMask;
    struct radv_sdma_surf buf = radv_sdma_get_buf_surf(buffer, image, region, aspect_mask);
    const struct radv_sdma_surf img =
@@ -121,7 +179,7 @@ copy_buffer_to_image(struct radv_cmd_buffer *cmd_buffer, struct radv_buffer *buf
                      VkImageLayout layout, const VkBufferImageCopy2 *region)
 {
    if (cmd_buffer->qf == RADV_QUEUE_TRANSFER) {
-      transfer_copy_buffer_image(cmd_buffer, buffer, image, region, true);
+      transfer_copy_buffer_image(cmd_buffer, buffer, image, layout, region, true);
       return;
    }
 
@@ -272,7 +330,7 @@ copy_image_to_buffer(struct radv_cmd_buffer *cmd_buffer, struct radv_buffer *buf
 {
    struct radv_device *device = cmd_buffer->device;
    if (cmd_buffer->qf == RADV_QUEUE_TRANSFER) {
-      transfer_copy_buffer_image(cmd_buffer, buffer, image, region, false);
+      transfer_copy_buffer_image(cmd_buffer, buffer, image, layout, region, false);
       return;
    }
 
@@ -392,6 +450,21 @@ transfer_copy_image(struct radv_cmd_buffer *cmd_buffer, struct radv_image *src_i
    radv_cs_add_buffer(device->ws, cs, dst_image->bindings[0].bo);
    unsigned int dst_aspect_mask_remaining = region->dstSubresource.aspectMask;
 
+   if (!radv_sdma_supports_image(device, src_image) || !radv_sdma_supports_image(device, dst_image)) {
+      VkCopyImageInfo2 info = {
+         .sType = VK_STRUCTURE_TYPE_COPY_IMAGE_INFO_2,
+         .pNext = NULL,
+         .regionCount = 1,
+         .pRegions = region,
+         .dstImage = radv_image_to_handle(dst_image),
+         .dstImageLayout = dst_image_layout,
+         .srcImage = radv_image_to_handle(src_image),
+         .srcImageLayout = src_image_layout,
+      };
+      transfer_gang_exec(cmd_buffer, (transfer_copy_func)radv_CmdCopyImage2, &info);
+      return;
+   }
+
    u_foreach_bit (b, region->srcSubresource.aspectMask) {
       const VkImageAspectFlags src_aspect_mask = BITFIELD_BIT(b);
       const VkImageAspectFlags dst_aspect_mask = BITFIELD_BIT(u_bit_scan(&dst_aspect_mask_remaining));
diff --git a/src/amd/vulkan/radv_physical_device.c b/src/amd/vulkan/radv_physical_device.c
index 5ef1feafc7c4..1c91ecee46cb 100644
--- a/src/amd/vulkan/radv_physical_device.c
+++ b/src/amd/vulkan/radv_physical_device.c
@@ -79,6 +79,11 @@ radv_transfer_queue_enabled(const struct radv_physical_device *pdevice)
        !(pdevice->instance->perftest_flags & RADV_PERFTEST_TRANSFER_QUEUE))
       return false;
 
+   /* Also disable transfer queues when compue or gang submit is disabled. */
+   if (!pdevice->rad_info.has_gang_submit || !pdevice->rad_info.ip[AMD_IP_COMPUTE].num_queues ||
+       (pdevice->instance->debug_flags & RADV_DEBUG_NO_COMPUTE_QUEUE))
+      return false;
+
    /* Enable transfer queues on GFX9+. */
    return pdevice->rad_info.gfx_level >= GFX9;
 }
@@ -2165,7 +2170,8 @@ radv_get_physical_device_queue_family_properties(struct radv_physical_device *pd
       if (*pCount > idx) {
          *pQueueFamilyProperties[idx] = (VkQueueFamilyProperties){
             .queueFlags = VK_QUEUE_TRANSFER_BIT,
-            .queueCount = pdevice->rad_info.ip[AMD_IP_SDMA].num_queues,
+            .queueCount =
+               MIN2(pdevice->rad_info.ip[AMD_IP_SDMA].num_queues, pdevice->rad_info.ip[AMD_IP_COMPUTE].num_queues),
             .timestampValidBits = 64,
             .minImageTransferGranularity = (VkExtent3D){16, 16, 8},
          };
diff --git a/src/amd/vulkan/radv_sdma.c b/src/amd/vulkan/radv_sdma.c
index f07ad8c71216..1992569efb71 100644
--- a/src/amd/vulkan/radv_sdma.c
+++ b/src/amd/vulkan/radv_sdma.c
@@ -801,3 +801,20 @@ radv_sdma_copy_image_t2t_scanline(const struct radv_device *device, struct radeo
       }
    }
 }
+
+bool
+radv_sdma_supports_image(const struct radv_device *device, const struct radv_image *image)
+{
+   if (radv_is_format_emulated(device->physical_device, image->vk.format))
+      return false;
+
+   if (!device->physical_device->rad_info.sdma_supports_sparse &&
+       (image->vk.create_flags & (VK_IMAGE_CREATE_SPARSE_BINDING_BIT | VK_IMAGE_CREATE_SPARSE_RESIDENCY_BIT |
+                                  VK_IMAGE_CREATE_SPARSE_ALIASED_BIT)))
+      return false;
+
+   if (image->vk.samples != VK_SAMPLE_COUNT_1_BIT)
+      return false;
+
+   return true;
+}
diff --git a/src/amd/vulkan/radv_sdma.h b/src/amd/vulkan/radv_sdma.h
index bcc95919c97b..1ced4ca9d36a 100644
--- a/src/amd/vulkan/radv_sdma.h
+++ b/src/amd/vulkan/radv_sdma.h
@@ -95,6 +95,7 @@ void radv_sdma_copy_buffer(const struct radv_device *device, struct radeon_cmdbu
                            uint64_t size);
 void radv_sdma_fill_buffer(const struct radv_device *device, struct radeon_cmdbuf *cs, const uint64_t va,
                            const uint64_t size, const uint32_t value);
+bool radv_sdma_supports_image(const struct radv_device *device, const struct radv_image *image);
 
 #ifdef __cplusplus
 }
-- 
GitLab


From 603dfc2883a0e54f1be2afece516508300120c84 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Mon, 16 Oct 2023 11:57:58 +0200
Subject: [PATCH 6/7] radv: Use gang semaphores for transfer queues.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

When there is a series of copy operations on a transfer queue
command buffer and its follower compute command buffer which
have a barrier in-between, we must make sure these properly
wait for each other.

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
---
 src/amd/vulkan/meta/radv_meta_copy.c |  9 +++++++++
 src/amd/vulkan/radv_cmd_buffer.c     | 26 +++++++++++++++++++++++---
 2 files changed, 32 insertions(+), 3 deletions(-)

diff --git a/src/amd/vulkan/meta/radv_meta_copy.c b/src/amd/vulkan/meta/radv_meta_copy.c
index 6e06eff259c4..2f9799f9db1f 100644
--- a/src/amd/vulkan/meta/radv_meta_copy.c
+++ b/src/amd/vulkan/meta/radv_meta_copy.c
@@ -97,6 +97,9 @@ transfer_gang_exec(struct radv_cmd_buffer *cmd_buffer, transfer_copy_func func,
    if (!radv_gang_init(cmd_buffer))
       return;
 
+   if (radv_flush_gang_leader_semaphore(cmd_buffer))
+      radv_wait_gang_leader(cmd_buffer);
+
    /* Save original command buffer CS. */
    const enum radv_queue_family qf = cmd_buffer->qf;
    struct radeon_cmdbuf *sdma_cs = cmd_buffer->cs;
@@ -157,6 +160,9 @@ transfer_copy_buffer_image(struct radv_cmd_buffer *cmd_buffer, struct radv_buffe
       return;
    }
 
+   if (cmd_buffer->gang.cs && radv_flush_gang_follower_semaphore(cmd_buffer))
+      radv_wait_gang_follower(cmd_buffer);
+
    const VkImageAspectFlags aspect_mask = region->imageSubresource.aspectMask;
    struct radv_sdma_surf buf = radv_sdma_get_buf_surf(buffer, image, region, aspect_mask);
    const struct radv_sdma_surf img =
@@ -465,6 +471,9 @@ transfer_copy_image(struct radv_cmd_buffer *cmd_buffer, struct radv_image *src_i
       return;
    }
 
+   if (cmd_buffer->gang.cs && radv_flush_gang_follower_semaphore(cmd_buffer))
+      radv_wait_gang_follower(cmd_buffer);
+
    u_foreach_bit (b, region->srcSubresource.aspectMask) {
       const VkImageAspectFlags src_aspect_mask = BITFIELD_BIT(b);
       const VkImageAspectFlags dst_aspect_mask = BITFIELD_BIT(u_bit_scan(&dst_aspect_mask_remaining));
diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index c8e42388d204..2c333c6cddf9 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -617,10 +617,30 @@ radv_gang_barrier(struct radv_cmd_buffer *cmd_buffer, VkPipelineStageFlags2 src_
         VK_PIPELINE_STAGE_2_BOTTOM_OF_PIPE_BIT | VK_PIPELINE_STAGE_2_ALL_COMMANDS_BIT))
       dst_stage_mask |= cmd_buffer->state.dma_is_busy ? VK_PIPELINE_STAGE_2_TASK_SHADER_BIT_EXT : 0;
 
-   /* Increment the GFX/ACE semaphore when task shaders are blocked. */
-   if (dst_stage_mask & (VK_PIPELINE_STAGE_2_TOP_OF_PIPE_BIT | VK_PIPELINE_STAGE_2_DRAW_INDIRECT_BIT |
-                         VK_PIPELINE_STAGE_2_TASK_SHADER_BIT_EXT))
+   /* Increment the leader to follower semaphore when the leader wants to block the follower:
+    * 1. graphics command buffer: task shader execution needs to wait for something
+    * 2. transfer command buffer: a transfer operation on ACE needs to wait for a previous operation on SDMA
+    */
+   const VkPipelineStageFlags2 gang_leader_flags =
+      cmd_buffer->qf == RADV_QUEUE_TRANSFER
+         ? (VK_PIPELINE_STAGE_2_TOP_OF_PIPE_BIT_KHR | VK_PIPELINE_STAGE_2_TRANSFER_BIT_KHR |
+            VK_PIPELINE_STAGE_2_ALL_TRANSFER_BIT)
+         : (VK_PIPELINE_STAGE_2_TOP_OF_PIPE_BIT_KHR | VK_PIPELINE_STAGE_2_DRAW_INDIRECT_BIT |
+            VK_PIPELINE_STAGE_2_TASK_SHADER_BIT_EXT);
+   if (dst_stage_mask & gang_leader_flags)
       cmd_buffer->gang.sem.leader_value++;
+
+   /* Increment the follower to leader semaphore when the follower wants to block the leader:
+    * 1. graphics command buffer: not necessary yet
+    * 2. transfer command buffer: a transfer operation on SDMA needs to wait for a previous operation on ACE
+    */
+   const VkPipelineStageFlags2 gang_follower_flags =
+      cmd_buffer->qf == RADV_QUEUE_TRANSFER
+         ? (VK_PIPELINE_STAGE_2_BOTTOM_OF_PIPE_BIT_KHR | VK_PIPELINE_STAGE_2_TRANSFER_BIT_KHR |
+            VK_PIPELINE_STAGE_2_ALL_TRANSFER_BIT)
+         : 0;
+   if (src_stage_mask & gang_follower_flags)
+      cmd_buffer->gang.sem.follower_value++;
 }
 
 void
-- 
GitLab


From 8741000d888e71b5b114345bc492ad1c22e3df6e Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Wed, 18 Oct 2023 09:36:41 +0200
Subject: [PATCH 7/7] radv/ci: Enable transfer_queues for GFX9-11 CTS jobs.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
---
 src/amd/ci/gitlab-ci.yml | 5 +++++
 1 file changed, 5 insertions(+)

diff --git a/src/amd/ci/gitlab-ci.yml b/src/amd/ci/gitlab-ci.yml
index 5ee369986129..17bfc12d393f 100644
--- a/src/amd/ci/gitlab-ci.yml
+++ b/src/amd/ci/gitlab-ci.yml
@@ -181,6 +181,7 @@ vkcts-vega10-valve:
     GPU_VERSION: radv-vega10-aco
     B2C_TIMEOUT_BOOT_MINUTES: 70
     B2C_TIMEOUT_OVERALL_MINUTES: 70
+    RADV_PERFTEST: transfer_queue
 
 vkcts-renoir-valve:
   extends:
@@ -201,6 +202,7 @@ vkcts-navi10-valve:
     - .radv-valve-manual-rules
   variables:
     GPU_VERSION: radv-navi10-aco
+    RADV_PERFTEST: transfer_queue
 
 vkcts-navi21-valve:
   timeout: 35m
@@ -213,6 +215,7 @@ vkcts-navi21-valve:
     GPU_VERSION: radv-navi21-aco
     B2C_TIMEOUT_BOOT_MINUTES: 30
     B2C_TIMEOUT_OVERALL_MINUTES: 30
+    RADV_PERFTEST: transfer_queue
 
 # Disabled due to its extremelly-broken nature
 .vkcts-navi21-llvm-valve:
@@ -233,6 +236,7 @@ vkcts-vangogh-valve:
   timeout: 2h 10m
   variables:
     GPU_VERSION: radv-vangogh-aco
+    RADV_PERFTEST: transfer_queue
 
 glcts-vangogh-valve:
   extends:
@@ -253,6 +257,7 @@ vkcts-navi31-valve:
   variables:
     GPU_VERSION: radv-navi31-aco
     RADV_DEBUG: nomeshshader # Disable mesh shaders until task shaders stop hanging
+    RADV_PERFTEST: transfer_queue
 
 ############### Fossilize
 radv-fossils:
-- 
GitLab

