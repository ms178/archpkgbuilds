From cb7d68c3107286299b7b23ac0aedb324eb11c1ea Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Fri, 4 Jul 2025 19:26:26 -0400
Subject: [PATCH 01/17] ac/surface/gfx12: add addr_from_coord for sparse MSAA
 textures

Reviewed-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 src/amd/common/ac_surface.c | 48 +++++++++++++++++++++++++++++++++----
 1 file changed, 44 insertions(+), 4 deletions(-)

diff --git a/src/amd/common/ac_surface.c b/src/amd/common/ac_surface.c
index 165b4f1c18a67..a50ad8c5a9ef7 100644
--- a/src/amd/common/ac_surface.c
+++ b/src/amd/common/ac_surface.c
@@ -4203,10 +4203,10 @@ uint64_t ac_surface_get_plane_size(const struct radeon_surf *surf,
    }
 }
 
-uint64_t
-ac_surface_addr_from_coord(struct ac_addrlib *addrlib, const struct radeon_info *info,
-                           const struct radeon_surf *surf, const struct ac_surf_info *surf_info,
-                           unsigned level, unsigned x, unsigned y, unsigned layer, bool is_3d)
+static uint64_t
+gfx9_surface_addr_from_coord(struct ac_addrlib *addrlib, const struct radeon_info *info,
+                             const struct radeon_surf *surf, const struct ac_surf_info *surf_info,
+                             unsigned level, unsigned x, unsigned y, unsigned layer, bool is_3d)
 {
    /* Only implemented for GFX9+ */
    assert(info->gfx_level >= GFX9);
@@ -4234,6 +4234,46 @@ ac_surface_addr_from_coord(struct ac_addrlib *addrlib, const struct radeon_info
    return output.addr;
 }
 
+static uint64_t
+gfx12_surface_addr_from_coord(struct ac_addrlib *addrlib, const struct radeon_info *info,
+                              const struct radeon_surf *surf, const struct ac_surf_info *surf_info,
+                              unsigned level, unsigned x, unsigned y, unsigned layer, bool is_3d)
+{
+   ADDR3_COMPUTE_SURFACE_ADDRFROMCOORD_INPUT input = {0};
+   input.size = sizeof(ADDR3_COMPUTE_SURFACE_ADDRFROMCOORD_INPUT);
+   input.slice = layer;
+   input.mipId = level;
+   input.pitchInElement = surf->u.gfx9.pitch[level];
+   input.unAlignedDims.width = DIV_ROUND_UP(surf_info->width, surf->blk_w);
+   input.unAlignedDims.height = DIV_ROUND_UP(surf_info->height, surf->blk_h);
+   input.unAlignedDims.depth = is_3d ? surf_info->depth : surf_info->array_size;
+   input.numMipLevels = surf_info->levels;
+   input.numSamples = surf_info->samples;
+   input.swizzleMode = surf->u.gfx9.swizzle_mode;
+   input.resourceType = (AddrResourceType)surf->u.gfx9.resource_type;
+   input.bpp = surf->bpe * 8;
+   input.x = x;
+   input.y = y;
+
+   ADDR3_COMPUTE_SURFACE_ADDRFROMCOORD_OUTPUT output = {0};
+   output.size = sizeof(ADDR3_COMPUTE_SURFACE_ADDRFROMCOORD_OUTPUT);
+   Addr3ComputeSurfaceAddrFromCoord(addrlib->handle, &input, &output);
+   return output.addr;
+}
+
+uint64_t
+ac_surface_addr_from_coord(struct ac_addrlib *addrlib, const struct radeon_info *info,
+                           const struct radeon_surf *surf, const struct ac_surf_info *surf_info,
+                           unsigned level, unsigned x, unsigned y, unsigned layer, bool is_3d)
+{
+   if (info->gfx_level >= GFX12)
+      return gfx12_surface_addr_from_coord(addrlib, info, surf, surf_info, level, x, y, layer, is_3d);
+   else if (info->gfx_level >= GFX9)
+      return gfx9_surface_addr_from_coord(addrlib, info, surf, surf_info, level, x, y, layer, is_3d);
+   else
+      unreachable("invalid gfx_level");
+}
+
 static void
 gfx12_surface_compute_nbc_view(struct ac_addrlib *addrlib, const struct radeon_info *info,
                             const struct radeon_surf *surf, const struct ac_surf_info *surf_info,
-- 
GitLab


From aa6678f8860eb8337a0e77bc8c313a72a7f8f542 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Fri, 4 Jul 2025 19:29:14 -0400
Subject: [PATCH 02/17] ac/surface/gfx12: select 64K tiling for sparse MSAA
 textures

addrlib doesn't do it automatically for MSAA

Reviewed-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 src/amd/common/ac_surface.c | 5 +++++
 1 file changed, 5 insertions(+)

diff --git a/src/amd/common/ac_surface.c b/src/amd/common/ac_surface.c
index a50ad8c5a9ef7..f5ced1921b45f 100644
--- a/src/amd/common/ac_surface.c
+++ b/src/amd/common/ac_surface.c
@@ -3398,6 +3398,11 @@ static bool gfx12_compute_surface(struct ac_addrlib *addrlib, const struct radeo
       AddrSurfInfoIn.swizzleMode = ac_get_modifier_swizzle_mode(info->gfx_level, surf->modifier);
    } else if (surf->flags & (RADEON_SURF_IMPORTED | RADEON_SURF_FORCE_SWIZZLE_MODE)) {
       AddrSurfInfoIn.swizzleMode = surf->u.gfx9.swizzle_mode;
+   } else if (surf->flags & RADEON_SURF_PRT) {
+      if (config->is_3d && !AddrSurfInfoIn.flags.view3dAs2dArray)
+         AddrSurfInfoIn.swizzleMode = ADDR3_64KB_3D;
+      else
+         AddrSurfInfoIn.swizzleMode = ADDR3_64KB_2D;
    } else if (mode == RADEON_SURF_MODE_LINEAR_ALIGNED) {
       assert(config->info.samples <= 1 && !(surf->flags & RADEON_SURF_Z_OR_SBUFFER));
       AddrSurfInfoIn.swizzleMode = ADDR3_LINEAR;
-- 
GitLab


From 599d46a49f9c21b4049a6ca79a078dc46654be6e Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Fri, 4 Jul 2025 18:57:44 -0400
Subject: [PATCH 03/17] radeonsi/gfx12: enable sparse textures

---
 .../radeonsi/ci/gfx12-gfx1200-fail.csv        | 191 ++++++++++++++++++
 src/gallium/drivers/radeonsi/si_get.c         |   3 +-
 src/gallium/drivers/radeonsi/si_texture.c     |   5 +-
 3 files changed, 196 insertions(+), 3 deletions(-)

diff --git a/src/gallium/drivers/radeonsi/si_get.c b/src/gallium/drivers/radeonsi/si_get.c
index a027625c6ff12..b0c685de99c62 100644
--- a/src/gallium/drivers/radeonsi/si_get.c
+++ b/src/gallium/drivers/radeonsi/si_get.c
@@ -1040,8 +1040,7 @@ void si_init_screen_caps(struct si_screen *sscreen)
 
    /* Gfx8 (Polaris11) hangs, so don't enable this on Gfx8 and older chips. */
    bool enable_sparse =
-      sscreen->info.gfx_level >= GFX9 && sscreen->info.gfx_level < GFX12 &&
-      sscreen->info.has_sparse_vm_mappings;
+      sscreen->info.gfx_level >= GFX9 && sscreen->info.has_sparse_vm_mappings;
 
    /* Supported features (boolean caps). */
    caps->max_dual_source_render_targets = true;
diff --git a/src/gallium/drivers/radeonsi/si_texture.c b/src/gallium/drivers/radeonsi/si_texture.c
index 3deff75d44558..238653ef443b9 100644
--- a/src/gallium/drivers/radeonsi/si_texture.c
+++ b/src/gallium/drivers/radeonsi/si_texture.c
@@ -1121,8 +1121,11 @@ static struct si_texture *si_texture_create_object(struct pipe_screen *screen,
       /* Always set BO metadata - required for programming DCC fields for GFX12 SDMA in the kernel.
        * If the texture is suballocated, this will overwrite the metadata for all suballocations,
        * but there is nothing we can do about that.
+       *
+       * Sparse textures don't have any backing storage at this point.
        */
-      si_set_tex_bo_metadata(sscreen, tex);
+      if (!(base->flags & PIPE_RESOURCE_FLAG_SPARSE))
+         si_set_tex_bo_metadata(sscreen, tex);
       return tex;
    }
 
-- 
GitLab


From d1e661c3459937d1ba96fecd456d5017fbf88c93 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sun, 29 Jun 2025 21:45:40 -0400
Subject: [PATCH 04/17] ac/nir/meta: allow compute blits with R5G6B5 & R5G5B5A1
 formats on GFX9+

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/amd/common/nir/ac_nir_meta_cs_blit.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/src/amd/common/nir/ac_nir_meta_cs_blit.c b/src/amd/common/nir/ac_nir_meta_cs_blit.c
index 174ebde950483..4d1f5ddd29071 100644
--- a/src/amd/common/nir/ac_nir_meta_cs_blit.c
+++ b/src/amd/common/nir/ac_nir_meta_cs_blit.c
@@ -568,8 +568,8 @@ ac_prepare_compute_blit(const struct ac_cs_blit_options *options,
       return true;
 
    if (blit->dst.format == PIPE_FORMAT_A8R8_UNORM || /* This format fails AMD_TEST=imagecopy. */
-       max_dst_chan_size == 5 || /* PIPE_FORMAT_R5G5B5A1_UNORM has precision issues */
-       max_dst_chan_size == 6 || /* PIPE_FORMAT_R5G6B5_UNORM has precision issues */
+       (info->gfx_level <= GFX8 && max_dst_chan_size == 5) || /* image stores with R5G5B5A1_UNORM have precision issues */
+       (info->gfx_level <= GFX8 && max_dst_chan_size == 6) || /* image stores with R5G6B5_UNORM have precision issues */
        util_format_is_depth_or_stencil(blit->dst.format) ||
        dst_samples > SI_MAX_COMPUTE_BLIT_SAMPLES ||
        /* Image stores support DCC since GFX10. Fail only for gfx queues because compute queues
-- 
GitLab


From 261da68a1c1704e826a19047f1b71ebc22c9000e Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 12 Jul 2025 14:07:37 -0400
Subject: [PATCH 05/17] ac/nir: don't vectorize to 96-bit and 128-bit LDS loads
 (it's slower)

LLVM also generates better code with this.
(-0.51% code size in 153 shaders, less SGPR spilling)

Reviewed-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 src/amd/common/nir/ac_nir.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/src/amd/common/nir/ac_nir.c b/src/amd/common/nir/ac_nir.c
index caf527fe5d04a..9786111973efb 100644
--- a/src/amd/common/nir/ac_nir.c
+++ b/src/amd/common/nir/ac_nir.c
@@ -614,6 +614,9 @@ ac_nir_mem_vectorize_callback(unsigned align_mul, unsigned align_offset, unsigne
    if (!is_shared) {
       return (align % (bit_size / 8u)) == 0 && num_components <= NIR_MAX_VEC_COMPONENTS;
    } else {
+      /* 96-bit and 128-bit LDS loads are slow. Don't use them. */
+      if (!is_store && bit_size * num_components > 64)
+         return false;
       if (bit_size >= 32 && num_components == 3) {
          /* AMD hardware can't do 3-component loads except for 96-bit loads. */
          return bit_size == 32 && align % 16 == 0;
-- 
GitLab


From 40293b08149cb618031910b7ec130eeea2363063 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Tue, 18 Mar 2025 16:51:24 -0400
Subject: [PATCH 06/17] ac/nir: mark all input loads as reorderable and
 speculatable (for LICM)

These are only memory loads.

We could do the same for LDS loads, which are not truly speculatable
in merged shaders (can't be moved before the barrier), but that's fine
because LICM only moves code out of loops, which can't have barriers.

Reviewed-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 src/amd/common/nir/ac_nir_create_gs_copy_shader.c | 3 ++-
 src/amd/common/nir/ac_nir_lower_esgs_io_to_mem.c  | 6 ++++--
 src/amd/common/nir/ac_nir_lower_tess_io_to_mem.c  | 3 ++-
 3 files changed, 8 insertions(+), 4 deletions(-)

diff --git a/src/amd/common/nir/ac_nir_create_gs_copy_shader.c b/src/amd/common/nir/ac_nir_create_gs_copy_shader.c
index e80e7f09bd924..a7a34b8dceb09 100644
--- a/src/amd/common/nir/ac_nir_create_gs_copy_shader.c
+++ b/src/amd/common/nir/ac_nir_create_gs_copy_shader.c
@@ -54,7 +54,8 @@ ac_nir_create_gs_copy_shader(const nir_shader *gs_nir, ac_nir_lower_legacy_gs_op
             out->outputs[i][j] =
                nir_load_buffer_amd(&b, 1, 32, gsvs_ring, vtx_offset, zero, zero,
                                    .base = base,
-                                   .access = ACCESS_COHERENT | ACCESS_NON_TEMPORAL);
+                                   .access = ACCESS_COHERENT | ACCESS_NON_TEMPORAL |
+                                             ACCESS_CAN_REORDER | ACCESS_CAN_SPECULATE);
             offset += 4;
          }
       }
diff --git a/src/amd/common/nir/ac_nir_lower_esgs_io_to_mem.c b/src/amd/common/nir/ac_nir_lower_esgs_io_to_mem.c
index 1244889f4c61a..6e3d4135b2856 100644
--- a/src/amd/common/nir/ac_nir_lower_esgs_io_to_mem.c
+++ b/src/amd/common/nir/ac_nir_lower_esgs_io_to_mem.c
@@ -64,13 +64,15 @@ emit_split_buffer_load(nir_builder *b, unsigned num_components, unsigned bit_siz
    for (unsigned i = 0; i < full_dwords; ++i)
       comps[i] = nir_load_buffer_amd(b, 1, 32, desc, v_off, s_off, zero,
                                      .base = component_stride * i, .memory_modes = nir_var_shader_in,
-                                     .access = ACCESS_COHERENT);
+                                     .access = ACCESS_COHERENT | ACCESS_CAN_REORDER |
+                                               ACCESS_CAN_SPECULATE);
 
    if (remaining_bytes)
       comps[full_dwords] = nir_load_buffer_amd(b, 1, remaining_bytes * 8, desc, v_off, s_off, zero,
                                                .base = component_stride * full_dwords,
                                                .memory_modes = nir_var_shader_in,
-                                               .access = ACCESS_COHERENT);
+                                               .access = ACCESS_COHERENT | ACCESS_CAN_REORDER |
+                                                         ACCESS_CAN_SPECULATE);
 
    return nir_extract_bits(b, comps, full_dwords + !!remaining_bytes, 0, num_components, bit_size);
 }
diff --git a/src/amd/common/nir/ac_nir_lower_tess_io_to_mem.c b/src/amd/common/nir/ac_nir_lower_tess_io_to_mem.c
index a542cdecac208..19e7ef652aa98 100644
--- a/src/amd/common/nir/ac_nir_lower_tess_io_to_mem.c
+++ b/src/amd/common/nir/ac_nir_lower_tess_io_to_mem.c
@@ -1499,7 +1499,8 @@ lower_tes_input_load(nir_builder *b,
    nir_def *load = NULL;
 
    AC_NIR_LOAD_IO(load, b, intrin->def.num_components, intrin->def.bit_size, io_sem.high_16bits,
-                  nir_load_buffer_amd, offchip_ring, off, offchip_offset, zero, .access = ACCESS_COHERENT,
+                  nir_load_buffer_amd, offchip_ring, off, offchip_offset, zero,
+                  .access = ACCESS_COHERENT | ACCESS_CAN_REORDER | ACCESS_CAN_SPECULATE,
                   .memory_modes = nir_var_shader_in);
 
    return load;
-- 
GitLab


From c11d1926a0cbdc352cfc1dc3a4987958c057fbf2 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Fri, 4 Jul 2025 18:03:06 -0400
Subject: [PATCH 07/17] ac/llvm: rewrite global & shared stores to share code

The alignment change allows LLVM to select 32-bit stores when 8-bit and
16-bit stores are properly aligned.
---
 src/amd/llvm/ac_nir_to_llvm.c | 99 ++++++++++++++++++-----------------
 1 file changed, 51 insertions(+), 48 deletions(-)

diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index 2236a7026d756..79d2b8e30b505 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -1905,6 +1905,55 @@ static LLVMValueRef get_global_address(struct ac_nir_context *ctx,
    return LLVMBuildPointerCast(ctx->ac.builder, addr, ptr_type, "");
 }
 
+static LLVMValueRef get_memory_addr(struct ac_nir_context *ctx, nir_intrinsic_instr *intr,
+                               LLVMTypeRef type)
+{
+   switch (intr->intrinsic) {
+   case nir_intrinsic_store_global_amd:
+      return get_global_address(ctx, intr, type);
+   case nir_intrinsic_store_shared: {
+      LLVMValueRef ptr = get_shared_mem_ptr(ctx, intr->src[1], nir_intrinsic_base(intr));
+      /* Cast the pointer to the type of the value. */
+      return LLVMBuildPointerCast(ctx->ac.builder, ptr,
+                                  LLVMPointerType(type,
+                                                  LLVMGetPointerAddressSpace(LLVMTypeOf(ptr))), "");
+   }
+   default:
+      unreachable("unexpected store intrinsic");
+   }
+}
+
+static void set_mem_op_alignment(LLVMValueRef instr, nir_intrinsic_instr *intr, nir_def *type)
+{
+   unsigned align = nir_intrinsic_align(intr);
+   unsigned pot_size = (intr->src[0].ssa->bit_size / 8) *
+                       (1 << util_logbase2(intr->src[0].ssa->num_components));
+   /* The alignment must be at most the stored size. */
+   LLVMSetAlignment(instr, MIN2(align, pot_size));
+}
+
+static void set_coherent_volatile(LLVMValueRef instr, nir_intrinsic_instr *intr)
+{
+   if ((intr->intrinsic == nir_intrinsic_load_global ||
+        intr->intrinsic == nir_intrinsic_store_global) &&
+       nir_intrinsic_access(intr) & (ACCESS_COHERENT | ACCESS_VOLATILE))
+      LLVMSetOrdering(instr, LLVMAtomicOrderingMonotonic);
+}
+
+static void visit_store(struct ac_nir_context *ctx, nir_intrinsic_instr *intr)
+{
+   LLVMBuilderRef builder = ctx->ac.builder;
+   LLVMValueRef data = get_src(ctx, intr->src[0]);
+   LLVMValueRef ptr = get_memory_addr(ctx, intr, LLVMTypeOf(data));
+   unsigned writemask = nir_intrinsic_write_mask(intr);
+   /* nir_opt_shrink_stores should split stores with holes. */
+   assert(writemask == BITFIELD_MASK(intr->src[0].ssa->num_components));
+
+   LLVMValueRef store = LLVMBuildStore(builder, data, ptr);
+   set_mem_op_alignment(store, intr, intr->src[0].ssa);
+   set_coherent_volatile(store, intr);
+}
+
 static LLVMValueRef visit_load_global(struct ac_nir_context *ctx,
                                       nir_intrinsic_instr *instr)
 {
@@ -1929,28 +1978,6 @@ static LLVMValueRef visit_load_global(struct ac_nir_context *ctx,
    return val;
 }
 
-static void visit_store_global(struct ac_nir_context *ctx,
-				     nir_intrinsic_instr *instr)
-{
-   LLVMValueRef data = get_src(ctx, instr->src[0]);
-   LLVMTypeRef type = LLVMTypeOf(data);
-   LLVMValueRef addr = get_global_address(ctx, instr, type);
-   LLVMValueRef val;
-   /* nir_opt_shrink_stores should be enough to simplify the writemask. Store writemasks should
-    * have no holes.
-    */
-   assert(nir_intrinsic_write_mask(instr) == BITFIELD_MASK(instr->src[0].ssa->num_components));
-
-   val = LLVMBuildStore(ctx->ac.builder, data, addr);
-
-   uint32_t align = nir_intrinsic_align(instr);
-   uint32_t size = ac_get_type_size(type);
-   LLVMSetAlignment(val, MIN2(align, 1 << (ffs(size) - 1)));
-
-   if (nir_intrinsic_access(instr) & (ACCESS_COHERENT | ACCESS_VOLATILE))
-      LLVMSetOrdering(val, LLVMAtomicOrderingMonotonic);
-}
-
 static LLVMValueRef visit_global_atomic(struct ac_nir_context *ctx,
 					nir_intrinsic_instr *instr)
 {
@@ -2529,28 +2556,6 @@ static LLVMValueRef visit_load_shared(struct ac_nir_context *ctx, const nir_intr
    return LLVMBuildBitCast(ctx->ac.builder, ret, get_def_type(ctx, &instr->def), "");
 }
 
-static void visit_store_shared(struct ac_nir_context *ctx, const nir_intrinsic_instr *instr)
-{
-   LLVMValueRef derived_ptr, data, index;
-   LLVMBuilderRef builder = ctx->ac.builder;
-
-   unsigned const_off = nir_intrinsic_base(instr);
-   LLVMTypeRef elem_type = LLVMIntTypeInContext(ctx->ac.context, instr->src[0].ssa->bit_size);
-   LLVMValueRef ptr = get_shared_mem_ptr(ctx, instr->src[1], const_off);
-   LLVMValueRef src = get_src(ctx, instr->src[0]);
-
-   int writemask = nir_intrinsic_write_mask(instr);
-   for (int chan = 0; chan < 16; chan++) {
-      if (!(writemask & (1 << chan))) {
-         continue;
-      }
-      data = ac_llvm_extract_elem(&ctx->ac, src, chan);
-      index = LLVMConstInt(ctx->ac.i32, chan, 0);
-      derived_ptr = LLVMBuildGEP2(builder, elem_type, ptr, &index, 1, "");
-      LLVMBuildStore(builder, data, derived_ptr);
-   }
-}
-
 static LLVMValueRef visit_load_shared2_amd(struct ac_nir_context *ctx,
                                            const nir_intrinsic_instr *instr)
 {
@@ -2813,7 +2818,8 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
       result = visit_load_global(ctx, instr);
       break;
    case nir_intrinsic_store_global_amd:
-      visit_store_global(ctx, instr);
+   case nir_intrinsic_store_shared:
+      visit_store(ctx, instr);
       break;
    case nir_intrinsic_global_atomic_amd:
    case nir_intrinsic_global_atomic_swap_amd:
@@ -2838,9 +2844,6 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
    case nir_intrinsic_load_shared:
       result = visit_load_shared(ctx, instr);
       break;
-   case nir_intrinsic_store_shared:
-      visit_store_shared(ctx, instr);
-      break;
    case nir_intrinsic_load_shared2_amd:
       result = visit_load_shared2_amd(ctx, instr);
       break;
-- 
GitLab


From cc386d5f16bacdaeb6f3e96bd681f657f082280b Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 12 Jul 2025 12:30:51 -0400
Subject: [PATCH 08/17] ac/llvm: rewrite global & shared loads to share code

---
 src/amd/llvm/ac_nir_to_llvm.c | 66 ++++++++++-------------------------
 1 file changed, 18 insertions(+), 48 deletions(-)

diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index 79d2b8e30b505..b7025ca0cc3d9 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -1909,10 +1909,13 @@ static LLVMValueRef get_memory_addr(struct ac_nir_context *ctx, nir_intrinsic_in
                                LLVMTypeRef type)
 {
    switch (intr->intrinsic) {
+   case nir_intrinsic_load_global_amd:
    case nir_intrinsic_store_global_amd:
       return get_global_address(ctx, intr, type);
+   case nir_intrinsic_load_shared:
    case nir_intrinsic_store_shared: {
-      LLVMValueRef ptr = get_shared_mem_ptr(ctx, intr->src[1], nir_intrinsic_base(intr));
+      unsigned num_src = nir_intrinsic_infos[intr->intrinsic].num_srcs;
+      LLVMValueRef ptr = get_shared_mem_ptr(ctx, intr->src[num_src - 1], nir_intrinsic_base(intr));
       /* Cast the pointer to the type of the value. */
       return LLVMBuildPointerCast(ctx->ac.builder, ptr,
                                   LLVMPointerType(type,
@@ -1926,8 +1929,9 @@ static LLVMValueRef get_memory_addr(struct ac_nir_context *ctx, nir_intrinsic_in
 static void set_mem_op_alignment(LLVMValueRef instr, nir_intrinsic_instr *intr, nir_def *type)
 {
    unsigned align = nir_intrinsic_align(intr);
-   unsigned pot_size = (intr->src[0].ssa->bit_size / 8) *
-                       (1 << util_logbase2(intr->src[0].ssa->num_components));
+   bool has_dest = nir_intrinsic_infos[intr->intrinsic].has_dest;
+   nir_def *val = has_dest ? &intr->def : intr->src[0].ssa;
+   unsigned pot_size = (val->bit_size / 8) * (1 << util_logbase2(val->num_components));
    /* The alignment must be at most the stored size. */
    LLVMSetAlignment(instr, MIN2(align, pot_size));
 }
@@ -1954,28 +1958,15 @@ static void visit_store(struct ac_nir_context *ctx, nir_intrinsic_instr *intr)
    set_coherent_volatile(store, intr);
 }
 
-static LLVMValueRef visit_load_global(struct ac_nir_context *ctx,
-                                      nir_intrinsic_instr *instr)
+static LLVMValueRef visit_load(struct ac_nir_context *ctx, nir_intrinsic_instr *intr)
 {
-   LLVMTypeRef result_type = get_def_type(ctx, &instr->def);
-   LLVMValueRef val;
-   LLVMValueRef addr = get_global_address(ctx, instr, result_type);
-
-   val = LLVMBuildLoad2(ctx->ac.builder, result_type, addr, "");
-
-   /* From the LLVM 21.0.0 language reference:
-    * > An alignment value higher than the size of the loaded type implies memory up to the
-    * > alignment value bytes can be safely loaded without trapping in the default address space.
-    * So limit the alignment to the access size, since this isn't true in NIR.
-    */
-   uint32_t align = nir_intrinsic_align(instr);
-   uint32_t size = ac_get_type_size(result_type);
-   LLVMSetAlignment(val, MIN2(align, 1 << (ffs(size) - 1)));
-
-   if (nir_intrinsic_access(instr) & (ACCESS_COHERENT | ACCESS_VOLATILE))
-      LLVMSetOrdering(val, LLVMAtomicOrderingMonotonic);
+   LLVMTypeRef result_type = get_def_type(ctx, &intr->def);
+   LLVMValueRef ptr = get_memory_addr(ctx, intr, result_type);
 
-   return val;
+   LLVMValueRef value = LLVMBuildLoad2(ctx->ac.builder, result_type, ptr, "");
+   set_mem_op_alignment(value, intr, &intr->def);
+   set_coherent_volatile(value, intr);
+   return value;
 }
 
 static LLVMValueRef visit_global_atomic(struct ac_nir_context *ctx,
@@ -2537,25 +2528,6 @@ static LLVMValueRef visit_first_invocation(struct ac_nir_context *ctx)
    return LLVMBuildTrunc(ctx->ac.builder, result, ctx->ac.i32, "");
 }
 
-static LLVMValueRef visit_load_shared(struct ac_nir_context *ctx, const nir_intrinsic_instr *instr)
-{
-   LLVMValueRef values[16], derived_ptr, index, ret;
-   unsigned const_off = nir_intrinsic_base(instr);
-
-   LLVMTypeRef elem_type = LLVMIntTypeInContext(ctx->ac.context, instr->def.bit_size);
-   LLVMValueRef ptr = get_shared_mem_ptr(ctx, instr->src[0], const_off);
-
-   for (int chan = 0; chan < instr->num_components; chan++) {
-      index = LLVMConstInt(ctx->ac.i32, chan, 0);
-      derived_ptr = LLVMBuildGEP2(ctx->ac.builder, elem_type, ptr, &index, 1, "");
-      values[chan] = LLVMBuildLoad2(ctx->ac.builder, elem_type, derived_ptr, "");
-   }
-
-   ret = ac_build_gather_values(&ctx->ac, values, instr->num_components);
-
-   return LLVMBuildBitCast(ctx->ac.builder, ret, get_def_type(ctx, &instr->def), "");
-}
-
 static LLVMValueRef visit_load_shared2_amd(struct ac_nir_context *ctx,
                                            const nir_intrinsic_instr *instr)
 {
@@ -2680,7 +2652,7 @@ static LLVMValueRef load_interpolated_input(struct ac_nir_context *ctx, LLVMValu
    return ac_to_integer(&ctx->ac, ac_build_gather_values(&ctx->ac, values, num_components));
 }
 
-static LLVMValueRef visit_load(struct ac_nir_context *ctx, nir_intrinsic_instr *instr)
+static LLVMValueRef visit_load_input(struct ac_nir_context *ctx, nir_intrinsic_instr *instr)
 {
    LLVMValueRef values[8];
    LLVMTypeRef dest_type = get_def_type(ctx, &instr->def);
@@ -2815,7 +2787,8 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
       result = visit_load_buffer(ctx, instr);
       break;
    case nir_intrinsic_load_global_amd:
-      result = visit_load_global(ctx, instr);
+   case nir_intrinsic_load_shared:
+      result = visit_load(ctx, instr);
       break;
    case nir_intrinsic_store_global_amd:
    case nir_intrinsic_store_shared:
@@ -2836,14 +2809,11 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
    case nir_intrinsic_load_per_primitive_input:
    case nir_intrinsic_load_input_vertex:
    case nir_intrinsic_load_per_vertex_input:
-      result = visit_load(ctx, instr);
+      result = visit_load_input(ctx, instr);
       break;
    case nir_intrinsic_store_output:
       visit_store_output(ctx, instr);
       break;
-   case nir_intrinsic_load_shared:
-      result = visit_load_shared(ctx, instr);
-      break;
    case nir_intrinsic_load_shared2_amd:
       result = visit_load_shared2_amd(ctx, instr);
       break;
-- 
GitLab


From 52abc4d38aa456540f6a24cc0e3c6f5f77d4f112 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 12 Jul 2025 12:54:31 -0400
Subject: [PATCH 09/17] ac/llvm: always use opaque pointers

LLVM removed typed pointers a long time ago. These types and bitcasts
had no effect.
---
 src/amd/llvm/ac_llvm_build.c                  | 29 ++++---------
 src/amd/llvm/ac_llvm_build.h                  |  4 +-
 src/amd/llvm/ac_nir_to_llvm.c                 | 43 ++++++++-----------
 src/amd/vulkan/radv_nir_to_llvm.c             |  2 +-
 src/gallium/drivers/radeonsi/si_shader_llvm.c |  2 +-
 5 files changed, 29 insertions(+), 51 deletions(-)

diff --git a/src/amd/llvm/ac_llvm_build.c b/src/amd/llvm/ac_llvm_build.c
index 312bbd20b8ab8..a9ea8d85d0074 100644
--- a/src/amd/llvm/ac_llvm_build.c
+++ b/src/amd/llvm/ac_llvm_build.c
@@ -2313,7 +2313,7 @@ LLVMTypeRef ac_arg_type_to_pointee_type(struct ac_llvm_context *ctx, enum ac_arg
       return ctx->f32;
       break;
    case AC_ARG_CONST_PTR_PTR:
-      return ac_array_in_const32_addr_space(ctx->i8);
+      return ac_array_in_const32_addr_space(ctx);
       break;
    case AC_ARG_CONST_DESC_PTR:
       return ctx->v4i32;
@@ -2327,14 +2327,14 @@ LLVMTypeRef ac_arg_type_to_pointee_type(struct ac_llvm_context *ctx, enum ac_arg
    }
 }
 
-LLVMTypeRef ac_array_in_const_addr_space(LLVMTypeRef elem_type)
+LLVMTypeRef ac_array_in_const_addr_space(struct ac_llvm_context *ctx)
 {
-   return LLVMPointerType(elem_type, AC_ADDR_SPACE_CONST);
+   return LLVMPointerTypeInContext(ctx->context, AC_ADDR_SPACE_CONST);
 }
 
-LLVMTypeRef ac_array_in_const32_addr_space(LLVMTypeRef elem_type)
+LLVMTypeRef ac_array_in_const32_addr_space(struct ac_llvm_context *ctx)
 {
-   return LLVMPointerType(elem_type, AC_ADDR_SPACE_CONST_32BIT);
+   return LLVMPointerTypeInContext(ctx->context, AC_ADDR_SPACE_CONST_32BIT);
 }
 
 static struct ac_llvm_flow *get_current_flow(struct ac_llvm_context *ctx)
@@ -3600,38 +3600,27 @@ void ac_export_mrt_z(struct ac_llvm_context *ctx, LLVMValueRef depth, LLVMValueR
 
 static LLVMTypeRef arg_llvm_type(enum ac_arg_type type, unsigned size, struct ac_llvm_context *ctx)
 {
-   LLVMTypeRef base;
    switch (type) {
       case AC_ARG_FLOAT:
          return size == 1 ? ctx->f32 : LLVMVectorType(ctx->f32, size);
       case AC_ARG_INT:
          return size == 1 ? ctx->i32 : LLVMVectorType(ctx->i32, size);
       case AC_ARG_CONST_PTR:
-         base = ctx->i8;
-         break;
       case AC_ARG_CONST_FLOAT_PTR:
-         base = ctx->f32;
-         break;
       case AC_ARG_CONST_PTR_PTR:
-         base = ac_array_in_const32_addr_space(ctx->i8);
-         break;
       case AC_ARG_CONST_DESC_PTR:
-         base = ctx->v4i32;
-         break;
       case AC_ARG_CONST_IMAGE_PTR:
-         base = ctx->v8i32;
          break;
       default:
          assert(false);
          return NULL;
    }
 
-   assert(base);
    if (size == 1) {
-      return ac_array_in_const32_addr_space(base);
+      return ac_array_in_const32_addr_space(ctx);
    } else {
       assert(size == 2);
-      return ac_array_in_const_addr_space(base);
+      return ac_array_in_const_addr_space(ctx);
    }
 }
 
@@ -3681,9 +3670,7 @@ struct ac_llvm_pointer ac_build_main(const struct ac_shader_args *args, struct a
    if (args->ring_offsets.used) {
       ctx->ring_offsets =
          ac_build_intrinsic(ctx, "llvm.amdgcn.implicit.buffer.ptr",
-                            LLVMPointerType(ctx->i8, AC_ADDR_SPACE_CONST), NULL, 0, 0);
-      ctx->ring_offsets = LLVMBuildBitCast(ctx->builder, ctx->ring_offsets,
-                                           ac_array_in_const_addr_space(ctx->v4i32), "");
+                            LLVMPointerTypeInContext(ctx->context, AC_ADDR_SPACE_CONST), NULL, 0, 0);
    }
 
    ctx->main_function = (struct ac_llvm_pointer) {
diff --git a/src/amd/llvm/ac_llvm_build.h b/src/amd/llvm/ac_llvm_build.h
index fa354e8b0b28c..3e973dd745ae1 100644
--- a/src/amd/llvm/ac_llvm_build.h
+++ b/src/amd/llvm/ac_llvm_build.h
@@ -408,8 +408,8 @@ void ac_init_exec_full_mask(struct ac_llvm_context *ctx);
 
 LLVMValueRef ac_find_lsb(struct ac_llvm_context *ctx, LLVMTypeRef dst_type, LLVMValueRef src0);
 
-LLVMTypeRef ac_array_in_const_addr_space(LLVMTypeRef elem_type);
-LLVMTypeRef ac_array_in_const32_addr_space(LLVMTypeRef elem_type);
+LLVMTypeRef ac_array_in_const_addr_space(struct ac_llvm_context *ctx);
+LLVMTypeRef ac_array_in_const32_addr_space(struct ac_llvm_context *ctx);
 
 void ac_build_bgnloop(struct ac_llvm_context *ctx, int lable_id);
 void ac_build_break(struct ac_llvm_context *ctx);
diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index b7025ca0cc3d9..7df1732062cdd 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -63,7 +63,7 @@ static LLVMValueRef get_shared_mem_ptr(struct ac_nir_context *ctx, nir_src src,
       LLVMTypeRef type = LLVMArrayType(ctx->ac.i32, lds_size / 4);
       ctx->lds = (struct ac_llvm_pointer) {
          .value = LLVMBuildIntToPtr(ctx->ac.builder, ctx->ac.i32_0,
-                     LLVMPointerType(type, AC_ADDR_SPACE_LDS), "lds"),
+                     LLVMPointerTypeInContext(ctx->ac.context, AC_ADDR_SPACE_LDS), "lds"),
          .pointee_type = type,
       };
    }
@@ -1694,8 +1694,8 @@ static LLVMValueRef emit_ssbo_comp_swap_64(struct ac_nir_context *ctx, LLVMValue
    LLVMValueRef ptr = ac_build_gather_values(&ctx->ac, ptr_parts, 2);
    ptr = LLVMBuildBitCast(ctx->ac.builder, ptr, ctx->ac.i64, "");
    ptr = LLVMBuildAdd(ctx->ac.builder, ptr, offset, "");
-   ptr = LLVMBuildIntToPtr(ctx->ac.builder, ptr, LLVMPointerType(ctx->ac.i64, AC_ADDR_SPACE_GLOBAL),
-                           "");
+   ptr = LLVMBuildIntToPtr(ctx->ac.builder, ptr,
+                           LLVMPointerTypeInContext(ctx->ac.context, AC_ADDR_SPACE_GLOBAL), "");
 
    LLVMValueRef result =
       ac_build_atomic_cmp_xchg(&ctx->ac, ptr, compare, exchange, "singlethread-one-as");
@@ -1886,40 +1886,32 @@ static LLVMValueRef enter_waterfall_ubo(struct ac_nir_context *ctx, struct water
 }
 
 static LLVMValueRef get_global_address(struct ac_nir_context *ctx,
-                                       nir_intrinsic_instr *instr,
-                                       LLVMTypeRef type)
+                                       nir_intrinsic_instr *instr)
 {
    bool is_store = instr->intrinsic == nir_intrinsic_store_global_amd;
    LLVMValueRef addr = get_src(ctx, instr->src[is_store ? 1 : 0]);
 
-   LLVMTypeRef ptr_type = LLVMPointerType(type, AC_ADDR_SPACE_GLOBAL);
+   LLVMTypeRef ptr_type = LLVMPointerTypeInContext(ctx->ac.context, AC_ADDR_SPACE_GLOBAL);
 
    uint32_t base = nir_intrinsic_base(instr);
    unsigned num_src = nir_intrinsic_infos[instr->intrinsic].num_srcs;
    LLVMValueRef offset = get_src(ctx, instr->src[num_src - 1]);
    offset = LLVMBuildAdd(ctx->ac.builder, offset, LLVMConstInt(ctx->ac.i32, base, false), "");
 
-   LLVMTypeRef i8_ptr_type = LLVMPointerType(ctx->ac.i8, AC_ADDR_SPACE_GLOBAL);
-   addr = LLVMBuildIntToPtr(ctx->ac.builder, addr, i8_ptr_type, "");
-   addr = LLVMBuildGEP2(ctx->ac.builder, ctx->ac.i8, addr, &offset, 1, "");
-   return LLVMBuildPointerCast(ctx->ac.builder, addr, ptr_type, "");
+   addr = LLVMBuildIntToPtr(ctx->ac.builder, addr, ptr_type, "");
+   return LLVMBuildGEP2(ctx->ac.builder, ctx->ac.i8, addr, &offset, 1, "");
 }
 
-static LLVMValueRef get_memory_addr(struct ac_nir_context *ctx, nir_intrinsic_instr *intr,
-                               LLVMTypeRef type)
+static LLVMValueRef get_memory_addr(struct ac_nir_context *ctx, nir_intrinsic_instr *intr)
 {
    switch (intr->intrinsic) {
    case nir_intrinsic_load_global_amd:
    case nir_intrinsic_store_global_amd:
-      return get_global_address(ctx, intr, type);
+      return get_global_address(ctx, intr);
    case nir_intrinsic_load_shared:
    case nir_intrinsic_store_shared: {
       unsigned num_src = nir_intrinsic_infos[intr->intrinsic].num_srcs;
-      LLVMValueRef ptr = get_shared_mem_ptr(ctx, intr->src[num_src - 1], nir_intrinsic_base(intr));
-      /* Cast the pointer to the type of the value. */
-      return LLVMBuildPointerCast(ctx->ac.builder, ptr,
-                                  LLVMPointerType(type,
-                                                  LLVMGetPointerAddressSpace(LLVMTypeOf(ptr))), "");
+      return get_shared_mem_ptr(ctx, intr->src[num_src - 1], nir_intrinsic_base(intr));
    }
    default:
       unreachable("unexpected store intrinsic");
@@ -1948,7 +1940,7 @@ static void visit_store(struct ac_nir_context *ctx, nir_intrinsic_instr *intr)
 {
    LLVMBuilderRef builder = ctx->ac.builder;
    LLVMValueRef data = get_src(ctx, intr->src[0]);
-   LLVMValueRef ptr = get_memory_addr(ctx, intr, LLVMTypeOf(data));
+   LLVMValueRef ptr = get_memory_addr(ctx, intr);
    unsigned writemask = nir_intrinsic_write_mask(intr);
    /* nir_opt_shrink_stores should split stores with holes. */
    assert(writemask == BITFIELD_MASK(intr->src[0].ssa->num_components));
@@ -1961,7 +1953,7 @@ static void visit_store(struct ac_nir_context *ctx, nir_intrinsic_instr *intr)
 static LLVMValueRef visit_load(struct ac_nir_context *ctx, nir_intrinsic_instr *intr)
 {
    LLVMTypeRef result_type = get_def_type(ctx, &intr->def);
-   LLVMValueRef ptr = get_memory_addr(ctx, intr, result_type);
+   LLVMValueRef ptr = get_memory_addr(ctx, intr);
 
    LLVMValueRef value = LLVMBuildLoad2(ctx->ac.builder, result_type, ptr, "");
    set_mem_op_alignment(value, intr, &intr->def);
@@ -2000,7 +1992,7 @@ static LLVMValueRef visit_global_atomic(struct ac_nir_context *ctx,
       data = LLVMBuildBitCast(ctx->ac.builder, data, data_type, "");
    }
 
-   LLVMValueRef addr = get_global_address(ctx, instr, data_type);
+   LLVMValueRef addr = get_global_address(ctx, instr);
 
    if (instr->intrinsic == nir_intrinsic_global_atomic_swap_amd) {
       LLVMValueRef data1 = get_src(ctx, instr->src[2]);
@@ -3174,7 +3166,7 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
    case nir_intrinsic_gds_atomic_add_amd: {
       LLVMValueRef store_val = get_src(ctx, instr->src[0]);
       LLVMValueRef addr = get_src(ctx, instr->src[1]);
-      LLVMTypeRef gds_ptr_type = LLVMPointerType(ctx->ac.i32, AC_ADDR_SPACE_GDS);
+      LLVMTypeRef gds_ptr_type = LLVMPointerTypeInContext(ctx->ac.context, AC_ADDR_SPACE_GDS);
       LLVMValueRef gds_base = LLVMBuildIntToPtr(ctx->ac.builder, addr, gds_ptr_type, "");
       ac_build_atomic_rmw(&ctx->ac, LLVMAtomicRMWBinOpAdd, gds_base, store_val, "workgroup-one-as");
       break;
@@ -3213,9 +3205,8 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
       int addr_space = is_addr_32bit ? AC_ADDR_SPACE_CONST_32BIT : AC_ADDR_SPACE_CONST;
 
       LLVMTypeRef result_type = get_def_type(ctx, &instr->def);
-      LLVMTypeRef byte_ptr_type = LLVMPointerType(ctx->ac.i8, addr_space);
-
-      LLVMValueRef addr = LLVMBuildIntToPtr(ctx->ac.builder, base, byte_ptr_type, "");
+      LLVMValueRef addr = LLVMBuildIntToPtr(ctx->ac.builder, base,
+                                            LLVMPointerTypeInContext(ctx->ac.context, addr_space), "");
       /* see ac_build_load_custom() for 32bit/64bit addr GEP difference */
       addr = is_addr_32bit ?
          LLVMBuildInBoundsGEP2(ctx->ac.builder, ctx->ac.i8, addr, &offset, 1, "") :
@@ -3245,7 +3236,7 @@ static bool visit_intrinsic(struct ac_nir_context *ctx, nir_intrinsic_instr *ins
        */
       LLVMValueRef args[8] = {
          LLVMBuildIntToPtr(ctx->ac.builder, get_src(ctx, instr->src[0]),
-                           LLVMPointerType(ctx->ac.i32, AC_ADDR_SPACE_GDS), ""),
+                           LLVMPointerTypeInContext(ctx->ac.context, AC_ADDR_SPACE_GDS), ""),
          ctx->ac.i32_0,                             /* value to add */
          ctx->ac.i32_0,                             /* ordering */
          ctx->ac.i32_0,                             /* scope */
diff --git a/src/amd/vulkan/radv_nir_to_llvm.c b/src/amd/vulkan/radv_nir_to_llvm.c
index ac878a7131142..40f70c7435894 100644
--- a/src/amd/vulkan/radv_nir_to_llvm.c
+++ b/src/amd/vulkan/radv_nir_to_llvm.c
@@ -123,7 +123,7 @@ radv_load_rsrc(struct radv_shader_context *ctx, LLVMValueRef ptr, LLVMTypeRef ty
    if (ptr && LLVMTypeOf(ptr) == ctx->ac.i32) {
       LLVMValueRef result;
 
-      LLVMTypeRef ptr_type = LLVMPointerType(type, AC_ADDR_SPACE_CONST_32BIT);
+      LLVMTypeRef ptr_type = LLVMPointerTypeInContext(ctx->ac.context, AC_ADDR_SPACE_CONST_32BIT);
       ptr = LLVMBuildIntToPtr(ctx->ac.builder, ptr, ptr_type, "");
       LLVMSetMetadata(ptr, ctx->ac.uniform_md_kind, ctx->ac.empty_md);
 
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index b9cfe5f81516d..9a25724eb056b 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -261,7 +261,7 @@ LLVMValueRef si_prolog_get_internal_binding_slot(struct si_shader_context *ctx,
 {
    LLVMValueRef list = LLVMBuildIntToPtr(
       ctx->ac.builder, ac_get_arg(&ctx->ac, ctx->args->internal_bindings),
-      ac_array_in_const32_addr_space(ctx->ac.v4i32), "");
+      ac_array_in_const32_addr_space(&ctx->ac), "");
    LLVMValueRef index = LLVMConstInt(ctx->ac.i32, slot, 0);
 
    return ac_build_load_to_sgpr(&ctx->ac,
-- 
GitLab


From b667a3bb462127adaa71b0c62a3a277a9baa223d Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Tue, 18 Mar 2025 16:52:02 -0400
Subject: [PATCH 10/17] ac/llvm: fix readlane with vectors

It crashes in LLVM when the type is not scalar integer.

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/amd/llvm/ac_llvm_build.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/src/amd/llvm/ac_llvm_build.c b/src/amd/llvm/ac_llvm_build.c
index a9ea8d85d0074..4ccf1d8820041 100644
--- a/src/amd/llvm/ac_llvm_build.c
+++ b/src/amd/llvm/ac_llvm_build.c
@@ -2569,7 +2569,7 @@ static LLVMValueRef ac_build_readlane_common(struct ac_llvm_context *ctx, LLVMVa
 {
    LLVMTypeRef src_type = LLVMTypeOf(src);
    src = ac_to_integer(ctx, src);
-   unsigned bits = LLVMGetIntTypeWidth(LLVMTypeOf(src));
+   unsigned bits = ac_get_type_size(LLVMTypeOf(src)) * 8;
    LLVMValueRef ret;
 
    if (bits > 32) {
-- 
GitLab


From 1bacd6396a2775336b51e4974eb27a3a01115793 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sun, 29 Jun 2025 21:40:49 -0400
Subject: [PATCH 11/17] radeonsi: disallow the compute copy for Z/S

It doesn't work, and I think this path is actually never exercised.

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/drivers/radeonsi/si_compute_blit.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/src/gallium/drivers/radeonsi/si_compute_blit.c b/src/gallium/drivers/radeonsi/si_compute_blit.c
index e6ddfd597e629..e17c579fa1e99 100644
--- a/src/gallium/drivers/radeonsi/si_compute_blit.c
+++ b/src/gallium/drivers/radeonsi/si_compute_blit.c
@@ -599,6 +599,9 @@ bool si_compute_copy_image(struct si_context *sctx, struct pipe_resource *dst, u
 
    assert(util_format_is_subsampled_422(src_format) == util_format_is_subsampled_422(dst_format));
 
+   if (sdst->is_depth || ssrc->is_depth)
+      return false;
+
    /* Interpret as integer values to avoid NaN issues */
    if (!vi_dcc_enabled(ssrc, src_level) &&
        !vi_dcc_enabled(sdst, dst_level) &&
-- 
GitLab


From abeff5f126b9543997b3057ac595396239f89561 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 21 Jun 2025 11:21:09 -0400
Subject: [PATCH 12/17] radeonsi: add a workaround for gfx10.3-11 corruption
 with R9G9B9E5_FLOAT

Reviewed-by: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
---
 src/gallium/drivers/radeonsi/si_state.c | 26 +++++++++++++++++++++++--
 1 file changed, 24 insertions(+), 2 deletions(-)

diff --git a/src/gallium/drivers/radeonsi/si_state.c b/src/gallium/drivers/radeonsi/si_state.c
index 78109b6c8a497..9bbec335e6289 100644
--- a/src/gallium/drivers/radeonsi/si_state.c
+++ b/src/gallium/drivers/radeonsi/si_state.c
@@ -222,8 +222,30 @@ static void si_emit_cb_render_state(struct si_context *sctx, unsigned index)
             break;
 
          case V_028C70_COLOR_5_9_9_9:
-            if (spi_format == V_028714_SPI_SHADER_FP16_ABGR)
-               sx_ps_downconvert |= V_028754_SX_RT_EXPORT_9_9_9_E5 << (i * 4);
+            /* This only executes on GFX10.3+. */
+            if (spi_format == V_028714_SPI_SHADER_FP16_ABGR) {
+               if (sctx->gfx_level >= GFX12) {
+                  sx_ps_downconvert |= V_028754_SX_RT_EXPORT_9_9_9_E5 << (i * 4);
+               } else {
+                  /* GFX10.3-11 have a bug where R9G9B9E5 is broken with RB+ when the color mask is not
+                   * full or empty.
+                   *
+                   * If A is missing in the color mask, add it. If it's the only bit set, remove it.
+                   */
+                  if (colormask == BITFIELD_MASK(3))
+                     cb_target_mask |= BITFIELD_BIT(3) << (i * 4);
+                  else if (colormask == BITFIELD_BIT(3))
+                     cb_target_mask &= ~(BITFIELD_BIT(3) << (i * 4));
+
+                  colormask = (cb_target_mask >> (i * 4)) & 0xf;
+
+                  /* Don't enable RB+ if the color mask is not full or empty, which is done by not
+                   * setting SX_PS_DOWNCONVERT for that MRT.
+                   */
+                  if (colormask == 0xf || colormask == 0)
+                     sx_ps_downconvert |= V_028754_SX_RT_EXPORT_9_9_9_E5 << (i * 4);
+               }
+            }
             break;
          }
       }
-- 
GitLab


From dc89e4ad2b0d1171202878e90776a2c5a1988552 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sat, 12 Jul 2025 01:52:34 -0400
Subject: [PATCH 13/17] radeonsi: recompute FS output IO bases to prevent an
 LLVM crash

Fixes: 7a506d0a414 - glsl: remove gl_nir_opt_dead_builtin_varyings
---
 src/gallium/drivers/radeonsi/si_shader_nir.c | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/src/gallium/drivers/radeonsi/si_shader_nir.c b/src/gallium/drivers/radeonsi/si_shader_nir.c
index 9a11c15935620..56747b9621540 100644
--- a/src/gallium/drivers/radeonsi/si_shader_nir.c
+++ b/src/gallium/drivers/radeonsi/si_shader_nir.c
@@ -390,8 +390,10 @@ char *si_finalize_nir(struct pipe_screen *screen, struct nir_shader *nir)
       NIR_PASS(_, nir, nir_remove_dead_variables, nir_var_shader_in | nir_var_shader_out, NULL);
    }
 
-   if (nir->info.stage == MESA_SHADER_FRAGMENT)
+   if (nir->info.stage == MESA_SHADER_FRAGMENT) {
       NIR_PASS(_, nir, si_nir_lower_color_inputs_to_sysvals);
+      NIR_PASS(_, nir, nir_recompute_io_bases, nir_var_shader_out);
+   }
 
    NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_shared, nir_address_format_32bit_offset);
 
-- 
GitLab


From e843a43411aad3658c86b1710e7193ef9d8ae8ad Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sun, 13 Jul 2025 03:46:19 -0400
Subject: [PATCH 14/17] radeonsi: get si_shader_info::input::usage_mask from
 NIR

---
 src/gallium/drivers/radeonsi/si_nir_lower_vs_inputs.c | 8 +++++---
 src/gallium/drivers/radeonsi/si_shader_info.c         | 5 +----
 src/gallium/drivers/radeonsi/si_shader_info.h         | 1 -
 3 files changed, 6 insertions(+), 8 deletions(-)

diff --git a/src/gallium/drivers/radeonsi/si_nir_lower_vs_inputs.c b/src/gallium/drivers/radeonsi/si_nir_lower_vs_inputs.c
index e4545b3d3f4dc..3760cd1f09201 100644
--- a/src/gallium/drivers/radeonsi/si_nir_lower_vs_inputs.c
+++ b/src/gallium/drivers/radeonsi/si_nir_lower_vs_inputs.c
@@ -419,10 +419,12 @@ opencoded_load_format(nir_builder *b, nir_def *rsrc, nir_def *vindex,
 static void
 load_vs_input_from_vertex_buffer(nir_builder *b, unsigned input_index,
                                  struct lower_vs_inputs_state *s,
-                                 unsigned bit_size, nir_def *out[4])
+                                 nir_intrinsic_instr *intr, nir_def *out[4])
 {
    const struct si_shader_selector *sel = s->shader->selector;
    const union si_shader_key *key = &s->shader->key;
+   unsigned bit_size = intr->def.bit_size;
+   unsigned channels_read = nir_def_components_read(&intr->def) << nir_intrinsic_component(intr);
 
    nir_def *vb_desc;
    if (input_index < sel->info.num_vbos_in_user_sgprs) {
@@ -460,7 +462,7 @@ load_vs_input_from_vertex_buffer(nir_builder *b, unsigned input_index,
       return;
    }
 
-   unsigned required_channels = util_last_bit(sel->info.input[input_index].usage_mask);
+   unsigned required_channels = util_last_bit(channels_read);
    if (required_channels == 0) {
       for (unsigned i = 0; i < 4; ++i)
          out[i] = nir_undef(b, 1, bit_size);
@@ -566,7 +568,7 @@ lower_vs_input_instr(nir_builder *b, nir_intrinsic_instr *intrin, void *state)
    if (b->shader->info.vs.blit_sgprs_amd)
       load_vs_input_from_blit_sgpr(b, input_index, s, comp);
    else
-      load_vs_input_from_vertex_buffer(b, input_index, s, intrin->def.bit_size, comp);
+      load_vs_input_from_vertex_buffer(b, input_index, s, intrin, comp);
 
    nir_def *replacement = nir_vec(b, &comp[component], num_components);
 
diff --git a/src/gallium/drivers/radeonsi/si_shader_info.c b/src/gallium/drivers/radeonsi/si_shader_info.c
index 6b79b4843ca88..57527fe67df99 100644
--- a/src/gallium/drivers/radeonsi/si_shader_info.c
+++ b/src/gallium/drivers/radeonsi/si_shader_info.c
@@ -132,10 +132,8 @@ static void scan_io_usage(const nir_shader *nir, struct si_shader_info *info,
 
          info->input[loc].semantic = semantic + i;
 
-         if (mask) {
-            info->input[loc].usage_mask |= mask;
+         if (mask)
             info->num_inputs = MAX2(info->num_inputs, loc + 1);
-         }
       }
    } else {
       /* Outputs. */
@@ -592,7 +590,6 @@ void si_nir_scan_shader(struct si_screen *sscreen, struct nir_shader *nir,
                unsigned index = num_inputs_with_colors;
 
                info->input[index].semantic = (back ? VARYING_SLOT_BFC0 : VARYING_SLOT_COL0) + i;
-               info->input[index].usage_mask = info->colors_read >> (i * 4);
                num_inputs_with_colors++;
 
                /* Back-face color don't increment num_inputs. si_emit_spi_map will use
diff --git a/src/gallium/drivers/radeonsi/si_shader_info.h b/src/gallium/drivers/radeonsi/si_shader_info.h
index b935925189374..83cfc82fbfc0d 100644
--- a/src/gallium/drivers/radeonsi/si_shader_info.h
+++ b/src/gallium/drivers/radeonsi/si_shader_info.h
@@ -18,7 +18,6 @@ enum si_color_output_type {
 
 struct si_vs_tcs_input_info {
    uint8_t semantic;
-   uint8_t usage_mask;
 };
 
 /* Shader info from initial NIR before optimizations for shader variants. */
-- 
GitLab


From e8fb9111ae7cec7eab9de73ea2902f4357f0d527 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sun, 13 Jul 2025 03:51:42 -0400
Subject: [PATCH 15/17] radeonsi: flatten struct si_vs_tcs_input_info

---
 src/gallium/drivers/radeonsi/si_shader_info.c      | 12 ++++++------
 src/gallium/drivers/radeonsi/si_shader_info.h      |  6 +-----
 src/gallium/drivers/radeonsi/si_shader_llvm_tess.c |  2 +-
 3 files changed, 8 insertions(+), 12 deletions(-)

diff --git a/src/gallium/drivers/radeonsi/si_shader_info.c b/src/gallium/drivers/radeonsi/si_shader_info.c
index 57527fe67df99..ca1e25de901fe 100644
--- a/src/gallium/drivers/radeonsi/si_shader_info.c
+++ b/src/gallium/drivers/radeonsi/si_shader_info.c
@@ -125,12 +125,12 @@ static void scan_io_usage(const nir_shader *nir, struct si_shader_info *info,
    unsigned num_slots = indirect ? nir_intrinsic_io_semantics(intr).num_slots : 1;
 
    if (is_input) {
-      assert(driver_location + num_slots <= ARRAY_SIZE(info->input));
+      assert(driver_location + num_slots <= ARRAY_SIZE(info->input_semantic));
 
       for (unsigned i = 0; i < num_slots; i++) {
          unsigned loc = driver_location + i;
 
-         info->input[loc].semantic = semantic + i;
+         info->input_semantic[loc] = semantic + i;
 
          if (mask)
             info->num_inputs = MAX2(info->num_inputs, loc + 1);
@@ -589,7 +589,7 @@ void si_nir_scan_shader(struct si_screen *sscreen, struct nir_shader *nir,
             if ((info->colors_read >> (i * 4)) & 0xf) {
                unsigned index = num_inputs_with_colors;
 
-               info->input[index].semantic = (back ? VARYING_SLOT_BFC0 : VARYING_SLOT_COL0) + i;
+               info->input_semantic[index] = (back ? VARYING_SLOT_BFC0 : VARYING_SLOT_COL0) + i;
                num_inputs_with_colors++;
 
                /* Back-face color don't increment num_inputs. si_emit_spi_map will use
@@ -646,7 +646,7 @@ void si_nir_scan_shader(struct si_screen *sscreen, struct nir_shader *nir,
 
    if (nir->info.stage == MESA_SHADER_FRAGMENT) {
       for (unsigned i = 0; i < info->num_inputs; i++) {
-         unsigned semantic = info->input[i].semantic;
+         unsigned semantic = info->input_semantic[i];
 
          if ((semantic <= VARYING_SLOT_VAR31 || semantic >= VARYING_SLOT_VAR0_16BIT) &&
              semantic != VARYING_SLOT_PNTC) {
@@ -659,9 +659,9 @@ void si_nir_scan_shader(struct si_screen *sscreen, struct nir_shader *nir,
             info->colors_written_4bit |= 0xf << (4 * i);
 
       for (unsigned i = 0; i < info->num_inputs; i++) {
-         if (info->input[i].semantic == VARYING_SLOT_COL0)
+         if (info->input_semantic[i] == VARYING_SLOT_COL0)
             info->color_attr_index[0] = i;
-         else if (info->input[i].semantic == VARYING_SLOT_COL1)
+         else if (info->input_semantic[i] == VARYING_SLOT_COL1)
             info->color_attr_index[1] = i;
       }
    }
diff --git a/src/gallium/drivers/radeonsi/si_shader_info.h b/src/gallium/drivers/radeonsi/si_shader_info.h
index 83cfc82fbfc0d..cdd0dd3079b5f 100644
--- a/src/gallium/drivers/radeonsi/si_shader_info.h
+++ b/src/gallium/drivers/radeonsi/si_shader_info.h
@@ -16,10 +16,6 @@ enum si_color_output_type {
    SI_TYPE_UINT16,
 };
 
-struct si_vs_tcs_input_info {
-   uint8_t semantic;
-};
-
 /* Shader info from initial NIR before optimizations for shader variants. */
 struct si_shader_info {
    struct {
@@ -88,7 +84,7 @@ struct si_shader_info {
 
    uint8_t num_inputs;
    uint8_t num_outputs;
-   struct si_vs_tcs_input_info input[PIPE_MAX_SHADER_INPUTS];
+   uint8_t input_semantic[PIPE_MAX_SHADER_INPUTS];
    uint8_t output_semantic[PIPE_MAX_SHADER_OUTPUTS];
 
    uint8_t num_vs_inputs;
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm_tess.c b/src/gallium/drivers/radeonsi/si_shader_llvm_tess.c
index f9b02be3f632f..a31ceda2f33ba 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm_tess.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm_tess.c
@@ -18,7 +18,7 @@ static LLVMValueRef si_nir_load_tcs_varyings(struct ac_shader_abi *abi, LLVMType
 
    assert(ctx->shader->key.ge.opt.same_patch_vertices);
 
-   uint8_t semantic = info->input[driver_location].semantic;
+   uint8_t semantic = info->input_semantic[driver_location];
    /* Load the TCS input from a VGPR. */
    unsigned func_param = ctx->args->ac.tcs_rel_ids.arg_index + 1 +
       si_shader_io_get_unique_index(semantic) * 4;
-- 
GitLab


From 64ec1a196aad80098a082dc0ca6eb5d434373580 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sun, 13 Jul 2025 04:25:16 -0400
Subject: [PATCH 16/17] radv,radeonsi: mark VS input loads and poly stipple
 load speculatable

Reviewed-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 src/amd/vulkan/nir/radv_nir_lower_vs_inputs.c               | 6 ++++--
 src/gallium/drivers/radeonsi/si_nir_lower_polygon_stipple.c | 3 ++-
 src/gallium/drivers/radeonsi/si_nir_lower_vs_inputs.c       | 6 ++++--
 3 files changed, 10 insertions(+), 5 deletions(-)

diff --git a/src/amd/vulkan/nir/radv_nir_lower_vs_inputs.c b/src/amd/vulkan/nir/radv_nir_lower_vs_inputs.c
index 708debb93ef55..d5261137f284e 100644
--- a/src/amd/vulkan/nir/radv_nir_lower_vs_inputs.c
+++ b/src/amd/vulkan/nir/radv_nir_lower_vs_inputs.c
@@ -326,11 +326,13 @@ lower_load_vs_input(nir_builder *b, nir_intrinsic_instr *intrin, lower_vs_inputs
       if (can_use_untyped_load(f, bit_size)) {
          loads[num_loads++] = nir_load_buffer_amd(b, channels, bit_size, descriptor, zero, zero, index,
                                                   .base = const_off, .memory_modes = nir_var_shader_in,
-                                                  .align_mul = align_mul, .align_offset = align_offset);
+                                                  .align_mul = align_mul, .align_offset = align_offset,
+                                                  .access = ACCESS_CAN_REORDER | ACCESS_CAN_SPECULATE);
       } else {
          loads[num_loads++] = nir_load_typed_buffer_amd(
             b, channels, bit_size, descriptor, zero, zero, index, .base = const_off, .format = fetch_format,
-            .align_mul = align_mul, .align_offset = align_offset, .memory_modes = nir_var_shader_in);
+            .align_mul = align_mul, .align_offset = align_offset, .memory_modes = nir_var_shader_in,
+            .access = ACCESS_CAN_REORDER | ACCESS_CAN_SPECULATE);
       }
    }
 
diff --git a/src/gallium/drivers/radeonsi/si_nir_lower_polygon_stipple.c b/src/gallium/drivers/radeonsi/si_nir_lower_polygon_stipple.c
index be93fb2661240..64e021cd7323d 100644
--- a/src/gallium/drivers/radeonsi/si_nir_lower_polygon_stipple.c
+++ b/src/gallium/drivers/radeonsi/si_nir_lower_polygon_stipple.c
@@ -24,7 +24,8 @@ bool si_nir_lower_polygon_stipple(nir_shader *nir)
    nir_def *zero = nir_imm_int(b, 0);
    /* The stipple pattern is 32x32, each row has 32 bits. */
    nir_def *offset = nir_ishl_imm(b, nir_channel(b, pixel_coord, 1), 2);
-   nir_def *row = nir_load_buffer_amd(b, 1, 32, desc, offset, zero, zero);
+   nir_def *row = nir_load_buffer_amd(b, 1, 32, desc, offset, zero, zero,
+                                      .access = ACCESS_CAN_REORDER | ACCESS_CAN_SPECULATE);
    nir_def *bit = nir_ubfe(b, row, nir_channel(b, pixel_coord, 0), nir_imm_int(b, 1));
 
    nir_def *pass = nir_i2b(b, bit);
diff --git a/src/gallium/drivers/radeonsi/si_nir_lower_vs_inputs.c b/src/gallium/drivers/radeonsi/si_nir_lower_vs_inputs.c
index 3760cd1f09201..6cdc990f82651 100644
--- a/src/gallium/drivers/radeonsi/si_nir_lower_vs_inputs.c
+++ b/src/gallium/drivers/radeonsi/si_nir_lower_vs_inputs.c
@@ -238,7 +238,8 @@ opencoded_load_format(nir_builder *b, nir_def *rsrc, nir_def *vindex,
       unsigned bit_size = 8 << MIN2(load_log_size, 2);
       nir_def *zero = nir_imm_int(b, 0);
 
-      loads[i] = nir_load_buffer_amd(b, num_channels, bit_size, rsrc, zero, soffset, vindex);
+      loads[i] = nir_load_buffer_amd(b, num_channels, bit_size, rsrc, zero, soffset, vindex,
+                                     .access = ACCESS_CAN_REORDER | ACCESS_CAN_SPECULATE);
    }
 
    if (log_recombine > 0) {
@@ -490,7 +491,8 @@ load_vs_input_from_vertex_buffer(nir_builder *b, unsigned input_index,
       fetches[i] = nir_load_buffer_amd(b, channels_per_fetch, bit_size, vb_desc,
                                        zero, zero, vertex_index,
                                        .base = fetch_stride * i,
-                                       .access = ACCESS_USES_FORMAT_AMD);
+                                       .access = ACCESS_USES_FORMAT_AMD | ACCESS_CAN_REORDER |
+                                                 ACCESS_CAN_SPECULATE);
    }
 
    if (num_fetches == 1 && channels_per_fetch > 1) {
-- 
GitLab


From 6f089870fa575f1941bd3cf582df292d6d4e1a18 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Marek=20Ol=C5=A1=C3=A1k?= <marek.olsak@amd.com>
Date: Sun, 13 Jul 2025 04:52:30 -0400
Subject: [PATCH 17/17] radv: don't sink VS input loads and move them to the
 top

This is likely to be faster.

Reviewed-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 src/amd/vulkan/radv_pipeline.c | 18 ++++++++++++++----
 1 file changed, 14 insertions(+), 4 deletions(-)

diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index 542b358956f00..851deff74241a 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -466,11 +466,21 @@ radv_postprocess_nir(struct radv_device *device, const struct radv_graphics_stat
 
    if (!stage->key.optimisations_disabled) {
       NIR_PASS(_, stage->nir, nir_opt_licm);
-      if (stage->stage != MESA_SHADER_FRAGMENT || !pdev->cache_key.disable_sinking_load_input_fs)
-         sink_opts |= nir_move_load_input;
 
-      NIR_PASS(_, stage->nir, nir_opt_sink, sink_opts);
-      NIR_PASS(_, stage->nir, nir_opt_move, sink_opts | nir_move_load_input);
+      if (stage->stage == MESA_SHADER_VERTEX) {
+         /* Always load all VS inputs at the top to eliminate needless VMEM->s_wait->VMEM sequences.
+          * Each s_wait can cost 1000 cycles, so make sure all VS input loads are grouped.
+          */
+         NIR_PASS(_, stage->nir, nir_opt_move_to_top, nir_move_to_top_input_loads);
+         NIR_PASS(_, stage->nir, nir_opt_sink, sink_opts);
+         NIR_PASS(_, stage->nir, nir_opt_move, sink_opts);
+      } else {
+         if (stage->stage != MESA_SHADER_FRAGMENT || !pdev->cache_key.disable_sinking_load_input_fs)
+            sink_opts |= nir_move_load_input;
+
+         NIR_PASS(_, stage->nir, nir_opt_sink, sink_opts);
+         NIR_PASS(_, stage->nir, nir_opt_move, sink_opts | nir_move_load_input);
+      }
    }
 
    /* Lower VS inputs. We need to do this after nir_opt_sink, because
-- 
GitLab

