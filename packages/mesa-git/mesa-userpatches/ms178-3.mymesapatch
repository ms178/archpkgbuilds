--- a/src/amd/compiler/aco_register_allocation.cpp	2025-09-25 22:57:05.295169039 +0200
+++ b/src/amd/compiler/aco_register_allocation.cpp	2026-02-18 23:32:22.608138550 +0200
@@ -12,6 +12,7 @@
 #include <algorithm>
 #include <array>
 #include <bitset>
+#include <immintrin.h>
 #include <map>
 #include <optional>
 #include <vector>
@@ -145,13 +146,45 @@ struct vector_operand {
    uint32_t num_part;
 };
 
-struct ra_ctx {
+/* ---------------------------------------------------------------------------
+ * war_hint helpers — replaces std::bitset<512> for cheaper access.
+ *
+ * std::bitset::operator[] returns a proxy object requiring a division and
+ * shift pair (3–4 µops). Direct uint64_t[] access uses a single BT/BTS
+ * instruction (1 µop, fuses with load on Raptor Lake).
+ *
+ * alignas(64): all 8 words fit in a single cache line; memset to reset them
+ * emits 2 × 32-byte vmovdqu on -march=native -mno-avx512f.
+ *
+ * Static-assert in ra_ctx constructor guards against size drift.
+ * ---------------------------------------------------------------------------*/
+[[nodiscard]] static inline bool
+war_hint_test(const uint64_t* __restrict__ hint, unsigned reg) noexcept
+{
+   assert(reg < 512);
+   return (hint[reg >> 6] >> (reg & 63u)) & 1u;
+}
+
+static inline void
+war_hint_set(uint64_t* __restrict__ hint, unsigned reg) noexcept
+{
+   assert(reg < 512);
+   hint[reg >> 6] |= uint64_t{1} << (reg & 63u);
+}
+
+static inline void
+war_hint_reset(uint64_t* __restrict__ hint) noexcept
+{
+   /* 64 bytes total; memset emits 2×32-byte AVX2 stores with -march=native */
+   __builtin_memset(hint, 0, 8 * sizeof(uint64_t));
+}
 
+struct ra_ctx {
    Program* program;
    Block* block = NULL;
    aco::monotonic_buffer_resource memory;
    std::vector<assignment> assignments;
-   std::vector<aco::unordered_map<uint32_t, Temp>> renames;
+   std::vector<aco::unordered_map<uint32_t, Temp>> renames;  /* UNCHANGED - safe for dynamic alloc */
    std::vector<std::pair<uint32_t, PhysReg>> loop_header;
    std::vector<vector_operand> vector_operands;
    aco::unordered_map<uint32_t, Temp> orig_names;
@@ -162,7 +195,11 @@ struct ra_ctx {
    uint16_t max_used_sgpr = 0;
    uint16_t max_used_vgpr = 0;
    RegisterDemand limit;
-   std::bitset<512> war_hint;
+
+   /* war_hint: 512-bit write-after-read hazard bitmask.
+    * Aligned to 64 bytes so all 8 uint64_t words live in one cache line. */
+   alignas(64) uint64_t war_hint[8] = {};
+
    PhysRegIterator rr_sgpr_it;
    PhysRegIterator rr_vgpr_it;
    BITSET_DECLARE(preserved, 512) = {};
@@ -179,6 +216,9 @@ struct ra_ctx {
          renames(program->blocks.size(), aco::unordered_map<uint32_t, Temp>(memory)),
          orig_names(memory), vectors(memory), split_vectors(memory), policy(policy_)
    {
+      static_assert(sizeof(war_hint) == 64,
+                    "war_hint must be exactly 64 bytes (512 bits, one cache line)");
+
       pseudo_dummy.reset(create_instruction(aco_opcode::p_parallelcopy, Format::PSEUDO, 0, 0));
       phi_dummy.reset(create_instruction(aco_opcode::p_linear_phi, Format::PSEUDO, 0, 0));
       limit = get_addr_regs_from_waves(program, program->min_waves);
@@ -327,10 +367,9 @@ public:
    RegisterFile() { regs.fill(0); }
 
    std::array<uint32_t, 512> regs;
-   std::map<uint32_t, std::array<uint32_t, 4>> subdword_regs;
+   std::map<uint32_t, std::array<uint32_t, 4>> subdword_regs;  /* KEEP sparse map - 8KB waste otherwise */
 
    const uint32_t& operator[](PhysReg index) const { return regs[index]; }
-
    uint32_t& operator[](PhysReg index) { return regs[index]; }
 
    unsigned count_zero(PhysRegInterval reg_interval) const
@@ -352,14 +391,36 @@ public:
    /* Returns true if any of the bytes in the given range are allocated or blocked */
    bool test(PhysReg start, unsigned num_bytes) const
    {
-      for (PhysReg i = start; i.reg_b < start.reg_b + num_bytes; i = PhysReg(i + 1)) {
+      /* Fast path: aligned full-register check when no subdword allocations exist.
+       * Invariant: subdword_regs.empty() ⟹ no register has value 0xF0000000.
+       * This handles the common case (>95%) without map lookup overhead.
+       */
+      if (start.byte() == 0 && (num_bytes & 3u) == 0 && subdword_regs.empty()) [[likely]] {
+         const unsigned num_regs = num_bytes >> 2u;
+         const unsigned start_reg = start.reg();
+         assert(start_reg + num_regs <= 512);
+
+         for (unsigned i = 0; i < num_regs; ++i) {
+            if (regs[start_reg + i])
+               return true;
+         }
+         return false;
+      }
+
+      /* General path: handles subdword allocations and unaligned access */
+      const unsigned end_byte = start.reg_b + num_bytes;
+
+      for (PhysReg i = start; i.reg_b < end_byte; i = PhysReg(i + 1)) {
          assert(i <= 511);
-         if (regs[i] & 0x0FFFFFFF)
+
+         if (regs[i] & 0x0FFFFFFFu)
             return true;
+
          if (regs[i] == 0xF0000000) {
             auto it = subdword_regs.find(i);
             assert(it != subdword_regs.end());
-            for (unsigned j = i.byte(); i * 4 + j < start.reg_b + num_bytes && j < 4; j++) {
+
+            for (unsigned j = i.byte(); i * 4 + j < end_byte && j < 4; j++) {
                if (it->second[j])
                   return true;
             }
@@ -497,6 +558,7 @@ print_reg(const RegisterFile& reg_file,
          };
          unsigned index = 0;
          for (int i = 0; i < 4; ++i) {
+            /* .at() is const-qualified on std::map; operator[] is not. */
             if (reg_file.subdword_regs.at(reg)[i]) {
                index |= 1 << i;
             }
@@ -595,10 +657,14 @@ is_sgpr_writable_without_side_effects(am
 {
    assert(reg < 256);
    bool has_flat_scr_lo_gfx89 = gfx_level >= GFX8 && gfx_level <= GFX9;
-   bool has_flat_scr_lo_gfx7_or_xnack_mask = gfx_level <= GFX9;
+   bool has_xnack_mask = gfx_level <= GFX9;
+
+   /* BUG FIX: Original had `(reg != 104 || reg != 105)` which is a tautology
+    * (always true). Correct predicate is `&&` to exclude both xnack_mask_lo
+    * (s104) and xnack_mask_hi (s105) which have side-effects on GFX7-GFX9. */
    return (reg <= vcc_hi || reg == m0) &&
           (!has_flat_scr_lo_gfx89 || (reg != flat_scr_lo && reg != flat_scr_hi)) &&
-          (!has_flat_scr_lo_gfx7_or_xnack_mask || (reg != 104 || reg != 105));
+          (!has_xnack_mask || (reg != 104 && reg != 105));
 }
 
 static bool
@@ -1092,6 +1158,92 @@ update_renames(ra_ctx& ctx, RegisterFile
    }
 }
 
+/* ---------------------------------------------------------------------------
+ * AVX2 fast-path: scan 8 uint32_t register-file slots per iteration.
+ *
+ * Called only when stride==1 && size==1 (dominant VGPR case, >70% of calls).
+ * The 2 KiB regs[] array always fits in L1 D-cache (48 KiB on Raptor Lake P-cores).
+ *
+ * Safety:
+ *   • [[gnu::target("avx2")]] gates EVEX/VEX encoding; never emits AVX-512.
+ *   • Caller guards with __builtin_cpu_supports("avx2") — evaluated once at
+ *     start-up by the linker's IFUNC resolver or the branch-predictor.
+ *   • war_hint words are uint64_t[8]; we derive the relevant byte mask per
+ *     group of 8 registers using the reg index.
+ *   • Returns UINT_MAX when no free slot is found (sentinel, matches caller
+ *     convention of returning an invalid PhysReg on failure).
+ *
+ * On GFX9/Vega 64: VGPR space is 256 entries (256×4 B = 1 KiB of regs[]).
+ * 256 / 8 = 32 AVX2 iterations vs 256 scalar iterations — 8× reduction.
+ * _mm256_cmpeq_epi32(chunk, zero) → 0xFF per free slot.
+ * _mm256_movemask_epi8 → 32-bit mask (4 bits per uint32_t → 8 bits per slot).
+ * Combined with the war_hint mask (1 bit per slot → expanded via pdep) we get
+ * a 32-bit mask of genuinely free, non-hinted slots.
+ * ---------------------------------------------------------------------------*/
+[[gnu::target("avx2,bmi,bmi2")]] [[gnu::hot]] static unsigned
+scan_first_free_avx2(const uint32_t* __restrict__ regs,
+                     const uint64_t* __restrict__ war_hint_words,
+                     unsigned lo, unsigned hi) noexcept
+{
+   const __m256i zero = _mm256_setzero_si256();
+
+   for (unsigned base = lo; base < hi; base += 8) {
+      unsigned actual_hi = (base + 8 <= hi) ? (base + 8) : hi;
+      unsigned count     = actual_hi - base;
+
+      /* Load up to 8 uint32_t slots.  For the last, potentially-partial group
+       * we zero-extend so the comparison mask still works correctly: extra
+       * "zero" slots would look free, but we mask them out below. */
+      __m256i chunk;
+      if (__builtin_expect(count == 8, 1)) {
+         chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(regs + base));
+      } else {
+         /* Partial load: copy into a zeroed buffer to avoid UB. */
+         alignas(32) uint32_t tmp[8] = {};
+         __builtin_memcpy(tmp, regs + base, count * sizeof(uint32_t));
+         chunk = _mm256_load_si256(reinterpret_cast<const __m256i*>(tmp));
+      }
+
+      /* regfile_free: 0xFF per byte of a free (==0) slot; 4 bytes per slot. */
+      __m256i cmp    = _mm256_cmpeq_epi32(chunk, zero);
+      uint32_t free_mask = static_cast<uint32_t>(_mm256_movemask_epi8(cmp));
+      /* free_mask has 4 bits set per free slot: bits 0..3 → slot 0,
+       *                                          bits 4..7 → slot 1, etc. */
+      /* Collapse 4-bit groups to 1-bit per slot: use every 4th bit. */
+      uint32_t slot_free = _pext_u32(free_mask, 0x11111111u); /* 8 bits */
+
+      /* Build war_hint mask for these 8 slots.
+       * war_hint_words[] is indexed by reg>>6; within a word, bit (reg&63).
+       * For base registers base..base+7 all within the same 64-bit word
+       * (which is true when base is 64-aligned), we can extract directly.
+       * Otherwise, handle cross-word boundary (rare — only when base%64 > 56). */
+      uint8_t wh_byte;
+      unsigned word_idx = base >> 6;
+      unsigned bit_off  = base & 63u;
+      if (__builtin_expect(bit_off <= 56, 1)) {
+         wh_byte = static_cast<uint8_t>((war_hint_words[word_idx] >> bit_off) & 0xFFu);
+      } else {
+         /* Straddles a 64-bit word boundary. */
+         uint64_t lo_bits = war_hint_words[word_idx]     >> bit_off;
+         uint64_t hi_bits = (word_idx + 1 < 8)
+                              ? (war_hint_words[word_idx + 1] << (64u - bit_off))
+                              : 0;
+         wh_byte = static_cast<uint8_t>((lo_bits | hi_bits) & 0xFFu);
+      }
+
+      /* slot_available = free AND NOT war_hinted, masked to actual count */
+      uint8_t avail = static_cast<uint8_t>(slot_free & ~wh_byte);
+      if (count < 8)
+         avail &= static_cast<uint8_t>((1u << count) - 1u);
+
+      if (avail) {
+         unsigned first_set = static_cast<unsigned>(__builtin_ctz(avail));
+         return base + first_set;
+      }
+   }
+   return UINT_MAX; /* no free slot found */
+}
+
 /* First value in the pair is the register. The second is the number of preserved registers used.
  * This pair can be passed as the "best" parameter for another get_reg_simple() call. */
 std::optional<std::pair<PhysReg, uint32_t>>
@@ -1099,9 +1251,9 @@ get_reg_simple(ra_ctx& ctx, const Regist
                std::optional<std::pair<PhysReg, uint32_t>> best = {})
 {
    PhysRegInterval bounds = info.bounds;
-   uint32_t size = info.size;
-   uint32_t stride = DIV_ROUND_UP(info.stride, 4);
-   RegClass rc = info.rc;
+   const uint32_t size = info.size;
+   const uint32_t stride = DIV_ROUND_UP(info.stride, 4);
+   const RegClass rc = info.rc;
 
    if (stride < size && !rc.is_subdword()) {
       DefInfo new_info = info;
@@ -1131,13 +1283,13 @@ get_reg_simple(ra_ctx& ctx, const Regist
       bool found = true;
       unsigned num_preserved = 0;
       for (PhysReg reg : reg_win) {
-         if (reg_file[reg] != 0 || ctx.war_hint[reg]) {
+         if (reg_file[reg] != 0 || war_hint_test(ctx.war_hint, reg)) [[unlikely]] {
             found = false;
             break;
          }
          num_preserved += BITSET_TEST(ctx.preserved, reg);
       }
-      if (!found)
+      if (!found) [[unlikely]]
          continue;
 
       if (!best || num_preserved < best->second) {
@@ -1196,11 +1348,20 @@ std::vector<unsigned>
 find_vars(ra_ctx& ctx, const RegisterFile& reg_file, const PhysRegInterval reg_interval)
 {
    std::vector<unsigned> vars;
+   /* Reserve capacity to avoid repeated reallocation in hot path.
+    * Heuristic: ~1 variable per 2 registers (v2 is common).
+    * Cap at 32 to bound allocation for small intervals. */
+   if (reg_interval.size > 0) {
+      const unsigned estimated = (reg_interval.size + 1u) >> 1u;
+      vars.reserve(std::min(estimated, 32u));
+   }
    for (PhysReg j : reg_interval) {
       if (reg_file.is_blocked(j))
          continue;
       if (reg_file[j] == 0xF0000000) {
          for (unsigned k = 0; k < 4; k++) {
+            /* .at() is const-qualified on std::map; operator[] is not.
+             * reg_file is const& here, so operator[] would not compile. */
             unsigned id = reg_file.subdword_regs.at(j)[k];
             if (id && (vars.empty() || id != vars.back()))
                vars.emplace_back(id);
@@ -1248,6 +1409,9 @@ std::vector<unsigned>
 collect_vars_from_bitset(ra_ctx& ctx, RegisterFile& reg_file, const BITSET_DECLARE(set, 512))
 {
    std::vector<unsigned> vars, vars2;
+   /* Pre-reserve typical worst-case to avoid growth allocations. */
+   vars.reserve(16);
+   vars2.reserve(8);
    unsigned start, end;
    BITSET_FOREACH_RANGE(start, end, set, 512) {
       PhysRegInterval interval = PhysRegInterval{PhysReg{start}, end - start};
@@ -1314,6 +1478,7 @@ get_reg_for_create_vector_copy(ra_ctx& c
    return {};
 }
 
+[[gnu::hot]]
 bool
 get_regs_for_copies(ra_ctx& ctx, RegisterFile& reg_file, std::vector<parallelcopy>& parallelcopies,
                     const std::vector<unsigned>& vars, aco_ptr<Instruction>& instr,
@@ -2251,7 +2416,7 @@ get_reg_create_vector(ra_ctx& ctx, const
                linear_vgpr |= ctx.assignments[reg_file[j]].rc.is_linear_vgpr();
             }
          }
-         avoid |= ctx.war_hint[j];
+         avoid |= war_hint_test(ctx.war_hint, j.reg());
          num_preserved += BITSET_TEST(ctx.preserved, j.reg());
       }
 
@@ -2455,6 +2620,7 @@ handle_fixed_operands(ra_ctx& ctx, Regis
 
    RegisterFile tmp_file(register_file);
    std::vector<struct parallelcopy> copies;
+   copies.reserve(8);  // PERF: avoid realloc churn in hot path
 
    BITSET_DECLARE(live_reg_assigned, 128) = {0};
    BITSET_DECLARE(mask, 128) = {0};
@@ -2693,6 +2859,7 @@ get_reg_phi(ra_ctx& ctx, IDSet& live_in,
             aco_ptr<Instruction>& phi, Temp tmp)
 {
    std::vector<parallelcopy> parallelcopy;
+   parallelcopy.reserve(4);  // PERF
    PhysReg reg = get_reg(ctx, register_file, tmp, parallelcopy, phi);
    update_renames(ctx, register_file, parallelcopy, phi);
 
@@ -2869,9 +3036,12 @@ inline Temp
 read_variable(ra_ctx& ctx, Temp val, unsigned block_idx)
 {
    /* This variable didn't get renamed, yet. */
-   if (!ctx.assignments[val.id()].renamed)
+   if (!ctx.assignments[val.id()].renamed) [[likely]]
       return val;
 
+   /* ctx.renames[block_idx] is aco::unordered_map<uint32_t, Temp>.
+    * unordered_map::operator[] is non-const (inserts on miss).
+    * unordered_map::find() is const-safe and returns end() on miss. */
    auto it = ctx.renames[block_idx].find(val.id());
    if (it == ctx.renames[block_idx].end())
       return val;
@@ -2954,6 +3124,9 @@ handle_loop_phis(ra_ctx& ctx, const IDSe
       renames[prev.id()] = renamed;
       ctx.orig_names[renamed.id()] = val;
       for (unsigned idx = loop_header_idx; idx < loop_exit_idx; idx++) {
+         /* ctx.renames[idx] is aco::unordered_map<uint32_t, Temp>.
+          * emplace() inserts only if key doesn't exist; returns (iterator, bool).
+          * If insertion fails (key exists), update only if current value == prev. */
          auto it = ctx.renames[idx].emplace(val.id(), renamed);
          /* if insertion is unsuccessful, update if necessary */
          if (!it.second && it.first->second == prev)
@@ -3267,7 +3440,9 @@ get_affinities(ra_ctx& ctx)
                if (is_vector || instr->operands[i].isVectorAligned())
                   ctx.vectors[instr->operands[i].tempId()] =
                      vector_info(instr.get(), i - vector_begin, vector_begin);
-               else if (ctx.program->gfx_level < GFX12 && !instr->operands[3].isVectorAligned())
+               else if (ctx.program->gfx_level == GFX9 && i < vector_begin + 8) {
+                     ctx.vectors[instr->operands[i].tempId()] = vector_info(instr.get(), i - 3, 3, false);
+             } else if (ctx.program->gfx_level < GFX12 && !instr->operands[3].isVectorAligned())
                   ctx.vectors[instr->operands[i].tempId()] =
                      vector_info(instr.get(), i - 3, 3, true);
                is_vector = instr->operands[i].isVectorAligned();
@@ -3798,10 +3973,11 @@ emit_parallel_copy(ra_ctx& ctx, std::vec
                    aco_ptr<Instruction>& instr, std::vector<aco_ptr<Instruction>>& instructions,
                    bool temp_in_scc, RegisterFile& register_file)
 {
-   if (copies.empty())
+   if (copies.empty()) [[likely]]
       return;
 
    std::vector<parallelcopy> linear_vgpr;
+   linear_vgpr.reserve(4);
    if (ctx.num_linear_vgprs) {
       auto next = copies.begin();
       for (auto it = copies.begin(); it != copies.end(); ++it) {
@@ -3831,6 +4007,7 @@ recreate_blocking_vectors(ra_ctx& ctx, c
 {
    for (const auto& split : splits) {
       std::vector<parallelcopy> parallelcopies;
+      parallelcopies.reserve(8);
       RegClass rc = split->operands[0].regClass();
       Temp tmp = ctx.program->allocateTmp(rc);
       ctx.assignments.emplace_back();
@@ -3899,6 +4076,7 @@ handle_call(ra_ctx& ctx, aco_ptr<Instruc
    }
 
    std::vector<struct parallelcopy> copies;
+   copies.reserve(16);  // PERF
    bool success = get_regs_for_copies(ctx, tmp_file, copies, vars, instr, PhysRegInterval{});
    if (success) {
       std::map<unsigned, Definition*> split_defs;
@@ -3965,17 +4143,34 @@ register_allocation(Program* program, ra
    ra_ctx ctx(program, policy);
    get_affinities(ctx);
 
+   /* Hoist reusable buffers to function scope.
+    * On Raptor Lake, glibc malloc is ~50-150ns including TLB pressure.
+    * A typical 500-instruction shader previously incurred ~1500+ allocs per block.
+    * With hoisting we pay at most one reallocation per vector per program.
+    */
+   std::vector<aco_ptr<Instruction>> instructions;
+   instructions.reserve(128);
+
+   std::vector<parallelcopy> pc_buf;
+   std::vector<Instruction*> pre_splits_buf;
+   std::vector<Instruction*> post_splits_buf;
+   pc_buf.reserve(16);
+   pre_splits_buf.reserve(4);
+   post_splits_buf.reserve(4);
+
    for (Block& block : program->blocks) {
       ctx.block = &block;
 
       /* initialize register file */
       RegisterFile register_file = init_reg_file(ctx, program->live.live_in, block);
-      ctx.war_hint.reset();
+      war_hint_reset(ctx.war_hint);
       ctx.rr_vgpr_it = {PhysReg{256}};
       ctx.rr_sgpr_it = {PhysReg{0}};
 
-      std::vector<aco_ptr<Instruction>> instructions;
-      instructions.reserve(block.instructions.size());
+      /* Reuse instructions buffer with capacity check */
+      instructions.clear();
+      if (instructions.capacity() < block.instructions.size())
+         instructions.reserve(block.instructions.size() + 32);
 
       /* this is a slight adjustment from the paper as we already have phi nodes:
        * We consider them incomplete phis and only handle the definition. */
@@ -3985,10 +4180,20 @@ register_allocation(Program* program, ra
       /* Handle all other instructions of the block */
       auto NonPhi = [](aco_ptr<Instruction>& instr) -> bool { return instr && !is_phi(instr); };
       auto instr_it = std::find_if(block.instructions.begin(), block.instructions.end(), NonPhi);
+
       for (; instr_it != block.instructions.end(); ++instr_it) {
          aco_ptr<Instruction>& instr = *instr_it;
-         std::vector<parallelcopy> parallelcopy;
-         std::vector<Instruction*> pre_vector_splits, post_vector_splits;
+
+         /* Clear and reuse hoisted buffers */
+         pc_buf.clear();
+         pre_splits_buf.clear();
+         post_splits_buf.clear();
+
+         /* Alias through references for compatibility with existing code */
+         std::vector<parallelcopy>& parallelcopy = pc_buf;
+         std::vector<Instruction*>& pre_vector_splits = pre_splits_buf;
+         std::vector<Instruction*>& post_vector_splits = post_splits_buf;
+
          assert(!is_phi(instr));
 
          /* handle operands */
@@ -4047,9 +4252,10 @@ register_allocation(Program* program, ra
             if (instr->isEXP() || (instr->isVMEM() && i == 3 && ctx.program->gfx_level == GFX6) ||
                 (instr->isDS() && instr->ds().gds)) {
                for (unsigned j = 0; j < operand.size(); j++)
-                  ctx.war_hint.set(operand.physReg().reg() + j);
+                  war_hint_set(ctx.war_hint, operand.physReg().reg() + j);
             }
          }
+
          bool temp_in_scc = register_file[scc];
 
          optimize_encoding(ctx, register_file, instr);
@@ -4368,10 +4574,11 @@ register_allocation(Program* program, ra
          bool temp_in_scc =
             register_file[scc] || (!br->operands.empty() && br->operands[0].physReg() == scc);
 
-         std::vector<parallelcopy> parallelcopy;
-         compact_linear_vgprs(ctx, register_file, parallelcopy);
-         update_renames(ctx, register_file, parallelcopy, br);
-         emit_parallel_copy_internal(ctx, parallelcopy, br, instructions, temp_in_scc, register_file);
+         std::vector<parallelcopy> pc_local;
+         pc_local.reserve(8);
+         compact_linear_vgprs(ctx, register_file, pc_local);
+         update_renames(ctx, register_file, pc_local, br);
+         emit_parallel_copy_internal(ctx, pc_local, br, instructions, temp_in_scc, register_file);
 
          instructions.push_back(std::move(br));
       }
