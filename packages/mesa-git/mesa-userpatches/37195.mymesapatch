From dbea11a003d2235f94582131c4ae10cb3da62d8b Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Daniel=20Sch=C3=BCrmann?= <daniel@schuermann.dev>
Date: Fri, 5 Sep 2025 10:06:30 +0200
Subject: [PATCH 1/8] nir: add nir_imul_nuw() and nir_imul_imm_nuw() helpers

---
 .../nir/radv_nir_apply_pipeline_layout.c      | 25 ++++++-------------
 src/compiler/nir/nir_builder.h                | 18 +++++++++++++
 2 files changed, 25 insertions(+), 18 deletions(-)

diff --git a/src/amd/vulkan/nir/radv_nir_apply_pipeline_layout.c b/src/amd/vulkan/nir/radv_nir_apply_pipeline_layout.c
index df074c9233a9e..8ed12b41dfd3e 100644
--- a/src/amd/vulkan/nir/radv_nir_apply_pipeline_layout.c
+++ b/src/amd/vulkan/nir/radv_nir_apply_pipeline_layout.c
@@ -77,11 +77,8 @@ visit_vulkan_resource_index(nir_builder *b, apply_layout_state *state, nir_intri
       stride = layout->binding[binding].size;
    }
 
-   nir_def *binding_ptr = nir_imul_imm(b, intrin->src[0].ssa, stride);
-   nir_def_as_alu(binding_ptr)->no_unsigned_wrap = true;
-
-   binding_ptr = nir_iadd_imm(b, binding_ptr, offset);
-   nir_def_as_alu(binding_ptr)->no_unsigned_wrap = true;
+   nir_def *binding_ptr = nir_imul_imm_nuw(b, intrin->src[0].ssa, stride);
+   binding_ptr = nir_iadd_imm_nuw(b, binding_ptr, offset);
 
    if (layout->binding[binding].type == VK_DESCRIPTOR_TYPE_ACCELERATION_STRUCTURE_KHR) {
       assert(stride == 16);
@@ -100,8 +97,7 @@ visit_vulkan_resource_reindex(nir_builder *b, apply_layout_state *state, nir_int
       nir_def *set_ptr = nir_unpack_64_2x32_split_x(b, intrin->src[0].ssa);
       nir_def *binding_ptr = nir_unpack_64_2x32_split_y(b, intrin->src[0].ssa);
 
-      nir_def *index = nir_imul_imm(b, intrin->src[1].ssa, 16);
-      nir_def_as_alu(index)->no_unsigned_wrap = true;
+      nir_def *index = nir_imul_imm_nuw(b, intrin->src[1].ssa, 16);
 
       binding_ptr = nir_iadd_nuw(b, binding_ptr, index);
 
@@ -112,9 +108,7 @@ visit_vulkan_resource_reindex(nir_builder *b, apply_layout_state *state, nir_int
       nir_def *binding_ptr = nir_channel(b, intrin->src[0].ssa, 1);
       nir_def *stride = nir_channel(b, intrin->src[0].ssa, 2);
 
-      nir_def *index = nir_imul(b, intrin->src[1].ssa, stride);
-      nir_def_as_alu(index)->no_unsigned_wrap = true;
-
+      nir_def *index = nir_imul_nuw(b, intrin->src[1].ssa, stride);
       binding_ptr = nir_iadd_nuw(b, binding_ptr, index);
 
       nir_def_rewrite_uses(&intrin->def, nir_vector_insert_imm(b, intrin->src[0].ssa, binding_ptr, 1));
@@ -240,13 +234,10 @@ get_sampler_desc(nir_builder *b, apply_layout_state *state, nir_deref_instr *der
       unsigned array_size = MAX2(glsl_get_aoa_size(deref->type), 1);
       array_size *= binding->size;
 
-      nir_def *tmp = nir_imul_imm(b, deref->arr.index.ssa, array_size);
-      if (tmp != deref->arr.index.ssa)
-         nir_def_as_alu(tmp)->no_unsigned_wrap = true;
+      nir_def *tmp = nir_imul_imm_nuw(b, deref->arr.index.ssa, array_size);
 
       if (index) {
-         index = nir_iadd(b, tmp, index);
-         nir_def_as_alu(index)->no_unsigned_wrap = true;
+         index = nir_iadd_nuw(b, tmp, index);
       } else {
          index = tmp;
       }
@@ -254,9 +245,7 @@ get_sampler_desc(nir_builder *b, apply_layout_state *state, nir_deref_instr *der
       deref = nir_deref_instr_parent(deref);
    }
 
-   nir_def *index_offset = index ? nir_iadd_imm(b, index, offset) : nir_imm_int(b, offset);
-   if (index && index_offset != index)
-      nir_def_as_alu(index_offset)->no_unsigned_wrap = true;
+   nir_def *index_offset = index ? nir_iadd_imm_nuw(b, index, offset) : nir_imm_int(b, offset);
 
    if (non_uniform)
       return nir_iadd(b, load_desc_ptr(b, state, desc_set), index_offset);
diff --git a/src/compiler/nir/nir_builder.h b/src/compiler/nir/nir_builder.h
index fa203839adbee..0101fd7577cc2 100644
--- a/src/compiler/nir/nir_builder.h
+++ b/src/compiler/nir/nir_builder.h
@@ -1110,6 +1110,24 @@ nir_imul_imm(nir_builder *build, nir_def *x, uint64_t y)
    return _nir_mul_imm(build, x, y, false);
 }
 
+static inline nir_def *
+nir_imul_imm_nuw(nir_builder *build, nir_def *x, uint64_t y)
+{
+   nir_def *d = nir_imul_imm(build, x, y);
+   if (d != x && d->parent_instr->type == nir_instr_type_alu)
+      nir_def_as_alu(d)->no_unsigned_wrap = true;
+   return d;
+}
+
+static inline nir_def *
+nir_imul_nuw(nir_builder *build, nir_def *x, nir_def *y)
+{
+   nir_def *d = nir_imul(build, x, y);
+   if (d->parent_instr->type == nir_instr_type_alu)
+      nir_def_as_alu(d)->no_unsigned_wrap = true;
+   return d;
+}
+
 static inline nir_def *
 nir_amul_imm(nir_builder *build, nir_def *x, uint64_t y)
 {
-- 
GitLab


From e7697c84c95b5ba548d547c9688a2f5412a5c1f9 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Daniel=20Sch=C3=BCrmann?= <daniel@schuermann.dev>
Date: Fri, 5 Sep 2025 10:13:43 +0200
Subject: [PATCH 2/8] nir: don't use nir_build_alu() with incomplete sources

Ideally we'd have a version that takes nir_scalar arguments.
---
 src/compiler/nir/nir_lower_bool_to_bitsize.c | 11 +++++------
 src/compiler/nir/nir_opt_varyings.c          | 19 ++++++++-----------
 2 files changed, 13 insertions(+), 17 deletions(-)

diff --git a/src/compiler/nir/nir_lower_bool_to_bitsize.c b/src/compiler/nir/nir_lower_bool_to_bitsize.c
index 10de06437e2b7..e1c5acb89e884 100644
--- a/src/compiler/nir/nir_lower_bool_to_bitsize.c
+++ b/src/compiler/nir/nir_lower_bool_to_bitsize.c
@@ -69,18 +69,17 @@ make_sources_canonical(nir_builder *b, nir_alu_instr *alu, uint32_t start_idx)
       if (nir_src_bit_size(alu->src[i].src) != bit_size) {
          b->cursor = nir_before_instr(&alu->instr);
          nir_op convert_op = get_bool_convert_opcode(bit_size);
-         nir_def *new_src =
-            nir_build_alu(b, convert_op, alu->src[i].src.ssa, NULL, NULL, NULL);
+         nir_alu_instr *conv_instr = nir_alu_instr_create(b->shader, convert_op);
+         conv_instr->src[0].src = nir_src_for_ssa(alu->src[i].src.ssa);
          /* Retain the write mask and swizzle of the original instruction so
           * that we donâ€™t unnecessarily create a vectorized instruction.
           */
-         nir_alu_instr *conv_instr =
-            nir_instr_as_alu(nir_builder_last_instr(b));
-         conv_instr->def.num_components =
-            alu->def.num_components;
          memcpy(conv_instr->src[0].swizzle,
                 alu->src[i].swizzle,
                 sizeof(conv_instr->src[0].swizzle));
+
+         nir_def *new_src = nir_builder_alu_instr_finish_and_insert(b, conv_instr);
+
          nir_src_rewrite(&alu->src[i].src, new_src);
          /* The swizzle will have been handled by the conversion instruction
           * so we can reset it back to the default
diff --git a/src/compiler/nir/nir_opt_varyings.c b/src/compiler/nir/nir_opt_varyings.c
index 318aa457a9bba..9678b3aa872b3 100644
--- a/src/compiler/nir/nir_opt_varyings.c
+++ b/src/compiler/nir/nir_opt_varyings.c
@@ -2248,15 +2248,15 @@ clone_ssa_impl(struct linkage_info *linkage, nir_builder *b, nir_def *ssa)
          return get_stored_value_for_load(linkage, &alu->instr);
       }
 
-      nir_def *src[4] = { 0 };
       unsigned num_srcs = nir_op_infos[alu->op].num_inputs;
-      assert(num_srcs <= ARRAY_SIZE(src));
+      nir_alu_instr *alu_clone = nir_alu_instr_create(b->shader, alu->op);
 
-      for (unsigned i = 0; i < num_srcs; i++)
-         src[i] = clone_ssa_impl(linkage, b, alu->src[i].src.ssa);
-
-      clone = nir_build_alu(b, alu->op, src[0], src[1], src[2], src[3]);
-      nir_alu_instr *alu_clone = nir_def_as_alu(clone);
+      for (unsigned i = 0; i < num_srcs; i++) {
+         nir_def *src = clone_ssa_impl(linkage, b, alu->src[i].src.ssa);
+         alu_clone->src[i].src = nir_src_for_ssa(src);
+         memcpy(alu_clone->src[i].swizzle, alu->src[i].swizzle,
+                NIR_MAX_VEC_COMPONENTS);
+      }
 
       alu_clone->exact = alu->exact;
       alu_clone->no_signed_wrap = alu->no_signed_wrap;
@@ -2264,10 +2264,7 @@ clone_ssa_impl(struct linkage_info *linkage, nir_builder *b, nir_def *ssa)
       alu_clone->def.num_components = alu->def.num_components;
       alu_clone->def.bit_size = alu->def.bit_size;
 
-      for (unsigned i = 0; i < num_srcs; i++) {
-         memcpy(alu_clone->src[i].swizzle, alu->src[i].swizzle,
-                NIR_MAX_VEC_COMPONENTS);
-      }
+      clone = nir_builder_alu_instr_finish_and_insert(b, alu_clone);
       break;
    }
 
-- 
GitLab


From f7482bd920f657c6449cc9445a5295c4b428ae7f Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Daniel=20Sch=C3=BCrmann?= <daniel@schuermann.dev>
Date: Fri, 5 Sep 2025 10:57:27 +0200
Subject: [PATCH 3/8] nir: guard nir_def_as_alu()

We will potentially create load_const_instr instead of ALU.
---
 src/compiler/nir/nir_builder.h    |  3 +-
 src/compiler/nir/nir_lower_flrp.c | 57 ++-----------------------------
 src/compiler/spirv/vtn_alu.c      |  6 ++--
 3 files changed, 8 insertions(+), 58 deletions(-)

diff --git a/src/compiler/nir/nir_builder.h b/src/compiler/nir/nir_builder.h
index 0101fd7577cc2..3ee9ed3a3b740 100644
--- a/src/compiler/nir/nir_builder.h
+++ b/src/compiler/nir/nir_builder.h
@@ -1033,7 +1033,8 @@ static inline nir_def *
 nir_iadd_nuw(nir_builder *b, nir_def *x, nir_def *y)
 {
    nir_def *d = nir_iadd(b, x, y);
-   nir_def_as_alu(d)->no_unsigned_wrap = true;
+   if (d->parent_instr->type == nir_instr_type_alu)
+      nir_def_as_alu(d)->no_unsigned_wrap = true;
    return d;
 }
 
diff --git a/src/compiler/nir/nir_lower_flrp.c b/src/compiler/nir/nir_lower_flrp.c
index 6a03d1d03b87a..d84e75190deae 100644
--- a/src/compiler/nir/nir_lower_flrp.c
+++ b/src/compiler/nir/nir_lower_flrp.c
@@ -52,16 +52,8 @@ replace_with_strict_ffma(struct nir_builder *bld, struct u_vector *dead_flrp,
    nir_def *const c = nir_ssa_for_alu_src(bld, alu, 2);
 
    nir_def *const neg_a = nir_fneg(bld, a);
-   nir_def_as_alu(neg_a)->exact = alu->exact;
-   nir_def_as_alu(neg_a)->fp_fast_math = alu->fp_fast_math;
-
    nir_def *const inner_ffma = nir_ffma(bld, neg_a, c, a);
-   nir_def_as_alu(inner_ffma)->exact = alu->exact;
-   nir_def_as_alu(inner_ffma)->fp_fast_math = alu->fp_fast_math;
-
    nir_def *const outer_ffma = nir_ffma(bld, b, c, inner_ffma);
-   nir_def_as_alu(outer_ffma)->exact = alu->exact;
-   nir_def_as_alu(outer_ffma)->fp_fast_math = alu->fp_fast_math;
 
    nir_def_rewrite_uses(&alu->def, outer_ffma);
 
@@ -84,21 +76,10 @@ replace_with_single_ffma(struct nir_builder *bld, struct u_vector *dead_flrp,
    nir_def *const c = nir_ssa_for_alu_src(bld, alu, 2);
 
    nir_def *const neg_c = nir_fneg(bld, c);
-   nir_def_as_alu(neg_c)->exact = alu->exact;
-   nir_def_as_alu(neg_c)->fp_fast_math = alu->fp_fast_math;
-
    nir_def *const one_minus_c =
       nir_fadd(bld, nir_imm_floatN_t(bld, 1.0f, c->bit_size), neg_c);
-   nir_def_as_alu(one_minus_c)->exact = alu->exact;
-   nir_def_as_alu(one_minus_c)->fp_fast_math = alu->fp_fast_math;
-
    nir_def *const b_times_c = nir_fmul(bld, b, c);
-   nir_def_as_alu(b_times_c)->exact = alu->exact;
-   nir_def_as_alu(b_times_c)->fp_fast_math = alu->fp_fast_math;
-
    nir_def *const final_ffma = nir_ffma(bld, a, one_minus_c, b_times_c);
-   nir_def_as_alu(final_ffma)->exact = alu->exact;
-   nir_def_as_alu(final_ffma)->fp_fast_math = alu->fp_fast_math;
 
    nir_def_rewrite_uses(&alu->def, final_ffma);
 
@@ -121,25 +102,11 @@ replace_with_strict(struct nir_builder *bld, struct u_vector *dead_flrp,
    nir_def *const c = nir_ssa_for_alu_src(bld, alu, 2);
 
    nir_def *const neg_c = nir_fneg(bld, c);
-   nir_def_as_alu(neg_c)->exact = alu->exact;
-   nir_def_as_alu(neg_c)->fp_fast_math = alu->fp_fast_math;
-
    nir_def *const one_minus_c =
       nir_fadd(bld, nir_imm_floatN_t(bld, 1.0f, c->bit_size), neg_c);
-   nir_def_as_alu(one_minus_c)->exact = alu->exact;
-   nir_def_as_alu(one_minus_c)->fp_fast_math = alu->fp_fast_math;
-
    nir_def *const first_product = nir_fmul(bld, a, one_minus_c);
-   nir_def_as_alu(first_product)->exact = alu->exact;
-   nir_def_as_alu(first_product)->fp_fast_math = alu->fp_fast_math;
-
    nir_def *const second_product = nir_fmul(bld, b, c);
-   nir_def_as_alu(second_product)->exact = alu->exact;
-   nir_def_as_alu(second_product)->fp_fast_math = alu->fp_fast_math;
-
    nir_def *const sum = nir_fadd(bld, first_product, second_product);
-   nir_def_as_alu(sum)->exact = alu->exact;
-   nir_def_as_alu(sum)->fp_fast_math = alu->fp_fast_math;
 
    nir_def_rewrite_uses(&alu->def, sum);
 
@@ -162,20 +129,9 @@ replace_with_fast(struct nir_builder *bld, struct u_vector *dead_flrp,
    nir_def *const c = nir_ssa_for_alu_src(bld, alu, 2);
 
    nir_def *const neg_a = nir_fneg(bld, a);
-   nir_def_as_alu(neg_a)->exact = alu->exact;
-   nir_def_as_alu(neg_a)->fp_fast_math = alu->fp_fast_math;
-
    nir_def *const b_minus_a = nir_fadd(bld, b, neg_a);
-   nir_def_as_alu(b_minus_a)->exact = alu->exact;
-   nir_def_as_alu(b_minus_a)->fp_fast_math = alu->fp_fast_math;
-
    nir_def *const product = nir_fmul(bld, c, b_minus_a);
-   nir_def_as_alu(product)->exact = alu->exact;
-   nir_def_as_alu(product)->fp_fast_math = alu->fp_fast_math;
-
    nir_def *const sum = nir_fadd(bld, a, product);
-   nir_def_as_alu(sum)->exact = alu->exact;
-   nir_def_as_alu(sum)->fp_fast_math = alu->fp_fast_math;
 
    nir_def_rewrite_uses(&alu->def, sum);
 
@@ -201,27 +157,16 @@ replace_with_expanded_ffma_and_add(struct nir_builder *bld,
    nir_def *const c = nir_ssa_for_alu_src(bld, alu, 2);
 
    nir_def *const b_times_c = nir_fmul(bld, b, c);
-   nir_def_as_alu(b_times_c)->exact = alu->exact;
-   nir_def_as_alu(b_times_c)->fp_fast_math = alu->fp_fast_math;
 
    nir_def *inner_sum;
-
    if (subtract_c) {
       nir_def *const neg_c = nir_fneg(bld, c);
-      nir_def_as_alu(neg_c)->exact = alu->exact;
-      nir_def_as_alu(neg_c)->fp_fast_math = alu->fp_fast_math;
-
       inner_sum = nir_fadd(bld, a, neg_c);
    } else {
       inner_sum = nir_fadd(bld, a, c);
    }
 
-   nir_def_as_alu(inner_sum)->exact = alu->exact;
-   nir_def_as_alu(inner_sum)->fp_fast_math = alu->fp_fast_math;
-
    nir_def *const outer_sum = nir_fadd(bld, inner_sum, b_times_c);
-   nir_def_as_alu(outer_sum)->exact = alu->exact;
-   nir_def_as_alu(outer_sum)->fp_fast_math = alu->fp_fast_math;
 
    nir_def_rewrite_uses(&alu->def, outer_sum);
 
@@ -399,6 +344,8 @@ convert_flrp_instruction(nir_builder *bld,
       UNREACHABLE("invalid bit_size");
 
    bld->cursor = nir_before_instr(&alu->instr);
+   bld->exact = alu->exact;
+   bld->fp_fast_math = alu->fp_fast_math;
 
    /* There are two methods to implement flrp(x, y, t).  The strictly correct
     * implementation according to the GLSL spec is:
diff --git a/src/compiler/spirv/vtn_alu.c b/src/compiler/spirv/vtn_alu.c
index f3a6bf3c2debc..28d87a020e74a 100644
--- a/src/compiler/spirv/vtn_alu.c
+++ b/src/compiler/spirv/vtn_alu.c
@@ -1087,8 +1087,10 @@ vtn_handle_alu(struct vtn_builder *b, SpvOp opcode,
    case SpvOpShiftLeftLogical:
    case SpvOpSNegate: {
       nir_alu_instr *alu = nir_def_as_alu(dest->def);
-      alu->no_signed_wrap |= vtn_has_decoration(b, dest_val, SpvDecorationNoSignedWrap);
-      alu->no_unsigned_wrap |= vtn_has_decoration(b, dest_val, SpvDecorationNoUnsignedWrap);
+      if (alu) {
+         alu->no_signed_wrap |= vtn_has_decoration(b, dest_val, SpvDecorationNoSignedWrap);
+         alu->no_unsigned_wrap |= vtn_has_decoration(b, dest_val, SpvDecorationNoUnsignedWrap);
+      }
       break;
    }
    default:
-- 
GitLab


From ca5aefbbe04f6ba1c35778fb092fab14412d5ef4 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Daniel=20Sch=C3=BCrmann?= <daniel@schuermann.dev>
Date: Fri, 5 Sep 2025 11:14:22 +0200
Subject: [PATCH 4/8] nir/constant_folding: switch to
 nir_shader_lower_instructions()

Small differences due to implicit DCE.
Totals from 76 (0.10% of 79839) affected shaders: (Navi48)
Instrs: 168051 -> 168044 (-0.00%); split: -0.01%, +0.01%
CodeSize: 893284 -> 893256 (-0.00%); split: -0.01%, +0.01%
Latency: 1082007 -> 1082027 (+0.00%); split: -0.00%, +0.00%
InvThroughput: 155100 -> 155105 (+0.00%)
Copies: 9649 -> 9654 (+0.05%)
VALU: 92504 -> 92509 (+0.01%)
---
 src/compiler/nir/nir_opt_constant_folding.c | 81 ++++++++-------------
 1 file changed, 30 insertions(+), 51 deletions(-)

diff --git a/src/compiler/nir/nir_opt_constant_folding.c b/src/compiler/nir/nir_opt_constant_folding.c
index debc66f30d20f..869e932ba6bc8 100644
--- a/src/compiler/nir/nir_opt_constant_folding.c
+++ b/src/compiler/nir/nir_opt_constant_folding.c
@@ -33,10 +33,9 @@
 
 struct constant_fold_state {
    bool has_load_constant;
-   bool has_indirect_load_const;
 };
 
-static bool
+static nir_def *
 try_fold_alu(nir_builder *b, nir_alu_instr *alu)
 {
    nir_const_value src[NIR_ALU_MAX_INPUTS][NIR_MAX_VEC_COMPONENTS];
@@ -62,7 +61,7 @@ try_fold_alu(nir_builder *b, nir_alu_instr *alu)
       nir_instr *src_instr = alu->src[i].src.ssa->parent_instr;
 
       if (src_instr->type != nir_instr_type_load_const)
-         return false;
+         return NULL;
       nir_load_const_instr *load_const = nir_instr_as_load_const(src_instr);
 
       for (unsigned j = 0; j < nir_ssa_alu_instr_src_components(alu, i);
@@ -83,14 +82,9 @@ try_fold_alu(nir_builder *b, nir_alu_instr *alu)
                          bit_size, srcs,
                          b->shader->info.float_controls_execution_mode);
 
-   b->cursor = nir_before_instr(&alu->instr);
-   nir_def *imm = nir_build_imm(b, alu->def.num_components,
+   return nir_build_imm(b, alu->def.num_components,
                                 alu->def.bit_size,
                                 dest);
-   nir_def_replace(&alu->def, imm);
-   nir_instr_free(&alu->instr);
-
-   return true;
 }
 
 static nir_const_value *
@@ -168,7 +162,7 @@ fail:
    return NULL;
 }
 
-static bool
+static nir_def *
 try_fold_intrinsic(nir_builder *b, nir_intrinsic_instr *intrin,
                    struct constant_fold_state *state)
 {
@@ -193,30 +187,25 @@ try_fold_intrinsic(nir_builder *b, nir_intrinsic_instr *intrin,
                nir_intrinsic_instr_create(b->shader, op);
             nir_builder_instr_insert(b, &new_instr->instr);
          }
-         nir_instr_remove(&intrin->instr);
-         return true;
+         return NIR_LOWER_INSTR_PROGRESS_REPLACE;
       }
-      return false;
+      return NULL;
 
    case nir_intrinsic_load_deref: {
       nir_deref_instr *deref = nir_src_as_deref(intrin->src[0]);
       nir_const_value *v = const_value_for_deref(deref);
       if (v) {
          b->cursor = nir_before_instr(&intrin->instr);
-         nir_def *val = nir_build_imm(b, intrin->def.num_components,
-                                      intrin->def.bit_size, v);
-         nir_def_replace(&intrin->def, val);
-         return true;
+         return nir_build_imm(b, intrin->def.num_components,
+                              intrin->def.bit_size, v);
       }
-      return false;
+      return NULL;
    }
 
    case nir_intrinsic_load_constant: {
-      state->has_load_constant = true;
-
       if (!nir_src_is_const(intrin->src[0])) {
-         state->has_indirect_load_const = true;
-         return false;
+         state->has_load_constant = true;
+         return NULL;
       }
 
       unsigned offset = nir_src_as_uint(intrin->src[0]);
@@ -243,8 +232,7 @@ try_fold_intrinsic(nir_builder *b, nir_intrinsic_instr *intrin,
          val = nir_build_imm(b, intrin->def.num_components,
                              intrin->def.bit_size, imm);
       }
-      nir_def_replace(&intrin->def, val);
-      return true;
+      return val;
    }
 
    case nir_intrinsic_ddx:
@@ -254,7 +242,7 @@ try_fold_intrinsic(nir_builder *b, nir_intrinsic_instr *intrin,
    case nir_intrinsic_ddy_fine:
    case nir_intrinsic_ddy_coarse: {
       if (!nir_src_is_const(intrin->src[0]))
-         return false;
+         return NULL;
 
       /* Derivative of a constant is zero, except for NaNs and Infs */
       nir_const_value imm[NIR_MAX_VEC_COMPONENTS];
@@ -267,9 +255,7 @@ try_fold_intrinsic(nir_builder *b, nir_intrinsic_instr *intrin,
          imm[i] = nir_const_value_for_float(finite ? 0 : NAN, sz);
       }
 
-      nir_def_replace(&intrin->def,
-                      nir_build_imm(b, intrin->def.num_components, sz, imm));
-      return true;
+      return nir_build_imm(b, intrin->def.num_components, sz, imm);
    }
 
    case nir_intrinsic_vote_any:
@@ -292,23 +278,20 @@ try_fold_intrinsic(nir_builder *b, nir_intrinsic_instr *intrin,
        * the data is constant.
        */
       if (nir_src_is_const(intrin->src[0])) {
-         nir_def_replace(&intrin->def, intrin->src[0].ssa);
-         return true;
+         return intrin->src[0].ssa;
       }
-      return false;
+      return NULL;
 
    case nir_intrinsic_vote_feq:
    case nir_intrinsic_vote_ieq:
       if (nir_src_is_const(intrin->src[0])) {
-         b->cursor = nir_before_instr(&intrin->instr);
-         nir_def_replace(&intrin->def, nir_imm_true(b));
-         return true;
+         return nir_imm_true(b);
       }
-      return false;
+      return NULL;
 
    case nir_intrinsic_inverse_ballot: {
       if (!nir_src_is_const(intrin->src[0]))
-         return false;
+         return NULL;
       bool constant_true = true;
       bool constant_false = true;
       for (unsigned i = 0; i < nir_src_num_components(intrin->src[0]); i++) {
@@ -317,14 +300,13 @@ try_fold_intrinsic(nir_builder *b, nir_intrinsic_instr *intrin,
          constant_false &= value == 0;
       }
       if (!constant_true && !constant_false)
-         return false;
-      b->cursor = nir_before_instr(&intrin->instr);
-      nir_def_replace(&intrin->def, nir_imm_bool(b, constant_true));
-      return true;
+         return NULL;
+
+      return nir_imm_bool(b, constant_true);
    }
 
    default:
-      return false;
+      return NULL;
    }
 }
 
@@ -429,7 +411,7 @@ try_fold_texel_offset_src(nir_tex_instr *tex)
    return true;
 }
 
-static bool
+static nir_def *
 try_fold_tex(nir_builder *b, nir_tex_instr *tex)
 {
    bool progress = false;
@@ -449,10 +431,10 @@ try_fold_tex(nir_builder *b, nir_tex_instr *tex)
    /* tex with a zero offset is just tex. */
    progress |= try_fold_texel_offset_src(tex);
 
-   return progress;
+   return progress ? NIR_LOWER_INSTR_PROGRESS : NULL;
 }
 
-static bool
+static nir_def *
 try_fold_instr(nir_builder *b, nir_instr *instr, void *_state)
 {
    switch (instr->type) {
@@ -464,7 +446,7 @@ try_fold_instr(nir_builder *b, nir_instr *instr, void *_state)
       return try_fold_tex(b, nir_instr_as_tex(instr));
    default:
       /* Don't know how to constant fold */
-      return false;
+      return NULL;
    }
 }
 
@@ -473,17 +455,14 @@ nir_opt_constant_folding(nir_shader *shader)
 {
    struct constant_fold_state state;
    state.has_load_constant = false;
-   state.has_indirect_load_const = false;
 
-   bool progress = nir_shader_instructions_pass(shader, try_fold_instr,
-                                                nir_metadata_control_flow,
-                                                &state);
+   bool progress = nir_shader_lower_instructions(shader, NULL, try_fold_instr,
+                                                 &state);
 
    /* This doesn't free the constant data if there are no constant loads because
     * the data might still be used but the loads have been lowered to load_ubo
     */
-   if (state.has_load_constant && !state.has_indirect_load_const &&
-       shader->constant_data_size) {
+   if (!state.has_load_constant && shader->constant_data_size) {
       ralloc_free(shader->constant_data);
       shader->constant_data = NULL;
       shader->constant_data_size = 0;
-- 
GitLab


From 37fd889108d91fdfed7048a0f4823d5c37e02011 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Daniel=20Sch=C3=BCrmann?= <daniel@schuermann.dev>
Date: Tue, 14 Oct 2025 12:46:26 +0200
Subject: [PATCH 5/8] nir/builder: add option to immediately constant-fold ALU
 instructions upon insertion

---
 src/compiler/nir/nir.h                      | 2 ++
 src/compiler/nir/nir_builder.c              | 8 ++++++++
 src/compiler/nir/nir_builder.h              | 3 +++
 src/compiler/nir/nir_opt_constant_folding.c | 6 +++---
 4 files changed, 16 insertions(+), 3 deletions(-)

diff --git a/src/compiler/nir/nir.h b/src/compiler/nir/nir.h
index 73432c7a651e4..6c9daf2bfadb6 100644
--- a/src/compiler/nir/nir.h
+++ b/src/compiler/nir/nir.h
@@ -6228,6 +6228,8 @@ bool nir_opt_algebraic_integer_promotion(nir_shader *shader);
 bool nir_opt_reassociate_matrix_mul(nir_shader *shader);
 bool nir_opt_constant_folding(nir_shader *shader);
 
+nir_def *nir_try_constant_fold_alu(nir_builder *b, nir_alu_instr *alu);
+
 /* Try to combine a and b into a.  Return true if combination was possible,
  * which will result in b being removed by the pass.  Return false if
  * combination wasn't possible.
diff --git a/src/compiler/nir/nir_builder.c b/src/compiler/nir/nir_builder.c
index 2b89343ec888f..493aac67cdf84 100644
--- a/src/compiler/nir/nir_builder.c
+++ b/src/compiler/nir/nir_builder.c
@@ -123,6 +123,14 @@ nir_builder_alu_instr_finish_and_insert(nir_builder *build, nir_alu_instr *instr
    nir_def_init(&instr->instr, &instr->def, num_components,
                 bit_size);
 
+   if (build->constant_fold_alu) {
+      nir_def *new_def = nir_try_constant_fold_alu(build, instr);
+      if (new_def) {
+         nir_instr_free(&instr->instr);
+         return new_def;
+      }
+   }
+
    nir_builder_instr_insert(build, &instr->instr);
 
    return &instr->def;
diff --git a/src/compiler/nir/nir_builder.h b/src/compiler/nir/nir_builder.h
index 3ee9ed3a3b740..d2b898066c448 100644
--- a/src/compiler/nir/nir_builder.h
+++ b/src/compiler/nir/nir_builder.h
@@ -40,6 +40,9 @@ typedef struct nir_builder {
    /* Whether new ALU instructions will be marked "exact" */
    bool exact;
 
+   /* Whether new ALU instruction will be constanst-folded if possible. */
+   bool constant_fold_alu;
+
    /* Float_controls2 bits. See nir_alu_instr for details. */
    uint32_t fp_fast_math;
 
diff --git a/src/compiler/nir/nir_opt_constant_folding.c b/src/compiler/nir/nir_opt_constant_folding.c
index 869e932ba6bc8..dc8ee6cdb4aae 100644
--- a/src/compiler/nir/nir_opt_constant_folding.c
+++ b/src/compiler/nir/nir_opt_constant_folding.c
@@ -35,8 +35,8 @@ struct constant_fold_state {
    bool has_load_constant;
 };
 
-static nir_def *
-try_fold_alu(nir_builder *b, nir_alu_instr *alu)
+nir_def *
+nir_try_constant_fold_alu(nir_builder *b, nir_alu_instr *alu)
 {
    nir_const_value src[NIR_ALU_MAX_INPUTS][NIR_MAX_VEC_COMPONENTS];
 
@@ -439,7 +439,7 @@ try_fold_instr(nir_builder *b, nir_instr *instr, void *_state)
 {
    switch (instr->type) {
    case nir_instr_type_alu:
-      return try_fold_alu(b, nir_instr_as_alu(instr));
+      return nir_try_constant_fold_alu(b, nir_instr_as_alu(instr));
    case nir_instr_type_intrinsic:
       return try_fold_intrinsic(b, nir_instr_as_intrinsic(instr), _state);
    case nir_instr_type_tex:
-- 
GitLab


From 31b20bccae60b0009b0237080aa102abc88abda6 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Daniel=20Sch=C3=BCrmann?= <daniel@schuermann.dev>
Date: Tue, 14 Oct 2025 12:49:16 +0200
Subject: [PATCH 6/8] nir/lower_flrp: ad-hoc constant-fold ALU instructions

---
 src/compiler/nir/nir_lower_flrp.c | 1 +
 1 file changed, 1 insertion(+)

diff --git a/src/compiler/nir/nir_lower_flrp.c b/src/compiler/nir/nir_lower_flrp.c
index d84e75190deae..0bc1cd06554b0 100644
--- a/src/compiler/nir/nir_lower_flrp.c
+++ b/src/compiler/nir/nir_lower_flrp.c
@@ -567,6 +567,7 @@ lower_flrp_impl(nir_function_impl *impl,
                 bool always_precise)
 {
    nir_builder b = nir_builder_create(impl);
+   b.constant_fold_alu = true;
 
    nir_foreach_block(block, impl) {
       nir_foreach_instr_safe(instr, block) {
-- 
GitLab


From 0f774085e0a057e71d4b3f146704fa1a03a426a0 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Daniel=20Sch=C3=BCrmann?= <daniel@schuermann.dev>
Date: Wed, 15 Oct 2025 12:16:48 +0200
Subject: [PATCH 7/8] tree-wide: don't call nir_opt_constant_folding after
 nir_lower_flrp

---
 src/amd/vulkan/radv_shader.c                 |  6 +-----
 src/broadcom/compiler/nir_to_vir.c           | 11 ++---------
 src/compiler/glsl/gl_nir_linker.c            | 12 ++----------
 src/freedreno/ir3/ir3_nir.c                  |  5 +----
 src/gallium/auxiliary/nir/tgsi_to_nir.c      | 12 ++----------
 src/gallium/drivers/radeonsi/si_shader_nir.c |  8 +-------
 src/gallium/drivers/vc4/vc4_program.c        | 11 ++---------
 src/intel/compiler/brw/brw_nir.c             |  6 +-----
 src/intel/compiler/elk/elk_nir.c             |  6 +-----
 src/nouveau/compiler/nak_nir.c               |  3 +--
 10 files changed, 14 insertions(+), 66 deletions(-)

diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 459b089505937..dfff668da0dac 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -731,12 +731,8 @@ radv_shader_spirv_to_nir(struct radv_device *device, const struct radv_shader_st
 
    unsigned lower_flrp = (nir->options->lower_flrp16 ? 16 : 0) | (nir->options->lower_flrp32 ? 32 : 0) |
                          (nir->options->lower_flrp64 ? 64 : 0);
-   if (lower_flrp != 0) {
-      progress = false;
+   if (lower_flrp != 0)
       NIR_PASS(progress, nir, nir_lower_flrp, lower_flrp, false /* always precise */);
-      if (progress)
-         NIR_PASS(_, nir, nir_opt_constant_folding);
-   }
 
    NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_push_const, nir_address_format_32bit_offset);
 
diff --git a/src/broadcom/compiler/nir_to_vir.c b/src/broadcom/compiler/nir_to_vir.c
index ad359f38ca195..adff7bff2f575 100644
--- a/src/broadcom/compiler/nir_to_vir.c
+++ b/src/broadcom/compiler/nir_to_vir.c
@@ -2236,15 +2236,8 @@ v3d_optimize_nir(struct v3d_compile *c, struct nir_shader *s)
                 }
 
                 if (lower_flrp != 0) {
-                        bool lower_flrp_progress = false;
-
-                        NIR_PASS(lower_flrp_progress, s, nir_lower_flrp,
-                                 lower_flrp,
-                                 false /* always_precise */);
-                        if (lower_flrp_progress) {
-                                NIR_PASS(progress, s, nir_opt_constant_folding);
-                                progress = true;
-                        }
+                        NIR_PASS(progress, s, nir_lower_flrp,
+                                 lower_flrp, false /* always_precise */);
 
                         /* Nothing should rematerialize any flrps, so we only
                          * need to do this lowering once.
diff --git a/src/compiler/glsl/gl_nir_linker.c b/src/compiler/glsl/gl_nir_linker.c
index 4689f27534403..946ef39b4e974 100644
--- a/src/compiler/glsl/gl_nir_linker.c
+++ b/src/compiler/glsl/gl_nir_linker.c
@@ -111,16 +111,8 @@ gl_nir_opts(nir_shader *nir)
             (nir->options->lower_flrp64 ? 64 : 0);
 
          if (lower_flrp) {
-            bool lower_flrp_progress = false;
-
-            NIR_PASS(lower_flrp_progress, nir, nir_lower_flrp,
-                     lower_flrp,
-                     false /* always_precise */);
-            if (lower_flrp_progress) {
-               NIR_PASS(progress, nir,
-                        nir_opt_constant_folding);
-               progress = true;
-            }
+            NIR_PASS(progress, nir, nir_lower_flrp,
+                     lower_flrp, false /* always_precise */);
          }
 
          /* Nothing should rematerialize any flrps, so we only need to do this
diff --git a/src/freedreno/ir3/ir3_nir.c b/src/freedreno/ir3/ir3_nir.c
index 5e61b4f142d19..f4cc6fd78d3ac 100644
--- a/src/freedreno/ir3/ir3_nir.c
+++ b/src/freedreno/ir3/ir3_nir.c
@@ -397,10 +397,7 @@ ir3_optimize_loop(struct ir3_compiler *compiler,
       progress |= OPT(s, nir_opt_offsets, &offset_options);
 
       if (lower_flrp != 0) {
-         if (OPT(s, nir_lower_flrp, lower_flrp, false /* always_precise */)) {
-            OPT(s, nir_opt_constant_folding);
-            progress = true;
-         }
+         OPT(s, nir_lower_flrp, lower_flrp, false /* always_precise */);
 
          /* Nothing should rematerialize any flrps, so we only
           * need to do this lowering once.
diff --git a/src/gallium/auxiliary/nir/tgsi_to_nir.c b/src/gallium/auxiliary/nir/tgsi_to_nir.c
index 005830990477c..bb3bed79dbaf0 100644
--- a/src/gallium/auxiliary/nir/tgsi_to_nir.c
+++ b/src/gallium/auxiliary/nir/tgsi_to_nir.c
@@ -2440,16 +2440,8 @@ ttn_optimize_nir(nir_shader *nir)
             (nir->options->lower_flrp64 ? 64 : 0);
 
          if (lower_flrp) {
-            bool lower_flrp_progress = false;
-
-            NIR_PASS(lower_flrp_progress, nir, nir_lower_flrp,
-                     lower_flrp,
-                     false /* always_precise */);
-            if (lower_flrp_progress) {
-               NIR_PASS(progress, nir,
-                        nir_opt_constant_folding);
-               progress = true;
-            }
+            NIR_PASS(progress, nir, nir_lower_flrp,
+                     lower_flrp, false /* always_precise */);
          }
 
          /* Nothing should rematerialize any flrps, so we only need to do this
diff --git a/src/gallium/drivers/radeonsi/si_shader_nir.c b/src/gallium/drivers/radeonsi/si_shader_nir.c
index 45925168b0757..fedfed4aa5af2 100644
--- a/src/gallium/drivers/radeonsi/si_shader_nir.c
+++ b/src/gallium/drivers/radeonsi/si_shader_nir.c
@@ -99,13 +99,7 @@ void si_nir_opts(struct si_screen *sscreen, struct nir_shader *nir, bool has_arr
                                (nir->options->lower_flrp32 ? 32 : 0) |
                                (nir->options->lower_flrp64 ? 64 : 0);
          assert(lower_flrp);
-         bool lower_flrp_progress = false;
-
-         NIR_PASS(lower_flrp_progress, nir, nir_lower_flrp, lower_flrp, false /* always_precise */);
-         if (lower_flrp_progress) {
-            NIR_PASS(progress, nir, nir_opt_constant_folding);
-            progress = true;
-         }
+         NIR_PASS(progress, nir, nir_lower_flrp, lower_flrp, false /* always_precise */);
 
          /* Nothing should rematerialize any flrps, so we only
           * need to do this lowering once.
diff --git a/src/gallium/drivers/vc4/vc4_program.c b/src/gallium/drivers/vc4/vc4_program.c
index b744ada597b8c..5abc61812cd2c 100644
--- a/src/gallium/drivers/vc4/vc4_program.c
+++ b/src/gallium/drivers/vc4/vc4_program.c
@@ -1512,15 +1512,8 @@ vc4_optimize_nir(struct nir_shader *s)
                 NIR_PASS(progress, s, nir_opt_algebraic);
                 NIR_PASS(progress, s, nir_opt_constant_folding);
                 if (lower_flrp != 0) {
-                        bool lower_flrp_progress = false;
-
-                        NIR_PASS(lower_flrp_progress, s, nir_lower_flrp,
-                                 lower_flrp,
-                                 false /* always_precise */);
-                        if (lower_flrp_progress) {
-                                NIR_PASS(progress, s, nir_opt_constant_folding);
-                                progress = true;
-                        }
+                        NIR_PASS(progress, s, nir_lower_flrp,
+                                 lower_flrp, false /* always_precise */);
 
                         /* Nothing should rematerialize any flrps, so we only
                          * need to do this lowering once.
diff --git a/src/intel/compiler/brw/brw_nir.c b/src/intel/compiler/brw/brw_nir.c
index 78c1ccfed5688..5263f7945ca81 100644
--- a/src/intel/compiler/brw/brw_nir.c
+++ b/src/intel/compiler/brw/brw_nir.c
@@ -1100,11 +1100,7 @@ brw_nir_optimize(nir_shader *nir,
       LOOP_OPT(nir_opt_constant_folding);
 
       if (lower_flrp != 0) {
-         if (LOOP_OPT(nir_lower_flrp,
-                 lower_flrp,
-                 false /* always_precise */)) {
-            LOOP_OPT(nir_opt_constant_folding);
-         }
+         LOOP_OPT(nir_lower_flrp, lower_flrp, false /* always_precise */);
 
          /* Nothing should rematerialize any flrps, so we only need to do this
           * lowering once.
diff --git a/src/intel/compiler/elk/elk_nir.c b/src/intel/compiler/elk/elk_nir.c
index bc88cea9b385a..3bb5ec5c8374e 100644
--- a/src/intel/compiler/elk/elk_nir.c
+++ b/src/intel/compiler/elk/elk_nir.c
@@ -756,11 +756,7 @@ elk_nir_optimize(nir_shader *nir, bool is_scalar,
       OPT(nir_opt_constant_folding);
 
       if (lower_flrp != 0) {
-         if (OPT(nir_lower_flrp,
-                 lower_flrp,
-                 false /* always_precise */)) {
-            OPT(nir_opt_constant_folding);
-         }
+         OPT(nir_lower_flrp, lower_flrp, false /* always_precise */);
 
          /* Nothing should rematerialize any flrps, so we only need to do this
           * lowering once.
diff --git a/src/nouveau/compiler/nak_nir.c b/src/nouveau/compiler/nak_nir.c
index 84a7bc1272e30..dfb90244d8f27 100644
--- a/src/nouveau/compiler/nak_nir.c
+++ b/src/nouveau/compiler/nak_nir.c
@@ -188,8 +188,7 @@ optimize_nir(nir_shader *nir, const struct nak_compiler *nak, bool allow_copies)
       OPT(nir, nir_opt_constant_folding);
 
       if (lower_flrp != 0) {
-         if (OPT(nir, nir_lower_flrp, lower_flrp, false /* always_precise */))
-            OPT(nir, nir_opt_constant_folding);
+         OPT(nir, nir_lower_flrp, lower_flrp, false /* always_precise */);
          /* Nothing should rematerialize any flrps */
          lower_flrp = 0;
       }
-- 
GitLab


From c6ad73e49d550910263b0a78d266d2546216da81 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Daniel=20Sch=C3=BCrmann?= <daniel@schuermann.dev>
Date: Fri, 5 Sep 2025 10:57:45 +0200
Subject: [PATCH 8/8] nir/algebraic: ad-hoc constant-fold ALU instructions

Slight differences due to different optimization order.
Totals from 135 (0.17% of 79839) affected shaders: (Navi48)
Instrs: 287852 -> 287527 (-0.11%); split: -0.15%, +0.03%
CodeSize: 1522972 -> 1521764 (-0.08%); split: -0.12%, +0.04%
Latency: 1806803 -> 1825754 (+1.05%); split: -0.08%, +1.12%
InvThroughput: 242693 -> 244703 (+0.83%); split: -0.02%, +0.84%
VClause: 4092 -> 4084 (-0.20%)
SClause: 7462 -> 7478 (+0.21%)
Copies: 20509 -> 20401 (-0.53%); split: -0.74%, +0.21%
Branches: 6395 -> 6386 (-0.14%)
PreSGPRs: 7334 -> 7337 (+0.04%); split: -0.03%, +0.07%
PreVGPRs: 6375 -> 6382 (+0.11%)
VALU: 151787 -> 151595 (-0.13%); split: -0.15%, +0.02%
SALU: 52967 -> 52910 (-0.11%); split: -0.23%, +0.12%
VMEM: 6704 -> 6696 (-0.12%)
SMEM: 12099 -> 12129 (+0.25%)

Tested on a small collection of 2518 shaders from Dredge with callgrind using RADV:
baseline:
  nir_opt_algebraic was called 12917 times from radv_optimize_nir()
  nir_opt_cse was called 15204 times from radv_optimize_nir()
  relative time spent in radv_optimize_nir(): 31.48%
  total instruction fetch cost: 28,642,638,021

with nir/algebraic: ad-hoc constant-fold ALU instructions
  nir_opt_algebraic was called 12797 times from radv_optimize_nir()
  nir_opt_cse was called 12963 times from radv_optimize_nir()
  relative time spent in radv_optimize_nir(): 30.63%
  total instruction fetch cost: 28,284,386,123

=> ~1.27% improvement in total compile times
---
 src/compiler/nir/nir_search.c | 19 ++++++++++++++-----
 1 file changed, 14 insertions(+), 5 deletions(-)

diff --git a/src/compiler/nir/nir_search.c b/src/compiler/nir/nir_search.c
index 6c559be6f4c0f..5bff9259d7109 100644
--- a/src/compiler/nir/nir_search.c
+++ b/src/compiler/nir/nir_search.c
@@ -494,15 +494,24 @@ construct_value(nir_builder *build,
                                        state, instr);
       }
 
-      nir_builder_instr_insert(build, &alu->instr);
-
-      assert(alu->def.index ==
+      /* Immediately try to constant-fold the expression, in order to allow
+       * for more expressions to be matched within a single pass.
+       */
+      nir_def *def = &alu->def;
+      nir_def *const_expr = nir_try_constant_fold_alu(build, alu);
+      if (const_expr) {
+         nir_instr_free(&alu->instr);
+         def = const_expr;
+      } else {
+         nir_builder_instr_insert(build, &alu->instr);
+      }
+      assert(def->index ==
              util_dynarray_num_elements(state->states, uint16_t));
       util_dynarray_append(state->states, uint16_t, 0);
-      nir_algebraic_automaton(&alu->instr, state->states, state->pass_op_table);
+      nir_algebraic_automaton(def->parent_instr, state->states, state->pass_op_table);
 
       nir_alu_src val;
-      val.src = nir_src_for_ssa(&alu->def);
+      val.src = nir_src_for_ssa(def);
       if (expr->swizzle < 0)
          memcpy(val.swizzle, identity_swizzle, sizeof(val.swizzle));
       else
-- 
GitLab

