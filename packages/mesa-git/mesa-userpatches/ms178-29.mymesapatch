--- a/src/compiler/nir/nir_opt_algebraic.py	2025-10-24 10:58:54.371161768 +0200
+++ b/src/compiler/nir/nir_opt_algebraic.py	2026-01-29 11:13:20.535349933 +0200
@@ -2142,6 +2142,10 @@ optimizations.extend([
    (('extract_u8', ('ushr', a, 8), 0), ('extract_u8', a, 1)),
    (('extract_u8', ('ushr', a, 8), 1), ('extract_u8', a, 2)),
    (('extract_u8', ('ushr', a, 8), 2), ('extract_u8', a, 3)),
+   (('extract_u8', ('ushr', 'a@32', 16), 0), ('extract_u8', a, 2)),
+   (('extract_u8', ('ushr', 'a@32', 16), 1), ('extract_u8', a, 3)),
+   (('extract_u16', ('ushr', 'a@64', 32), 0), ('extract_u16', a, 2)),
+   (('extract_u16', ('ushr', 'a@64', 32), 1), ('extract_u16', a, 3)),
 
    (('extract_i8', ('extract_i16', a, 1), 0), ('extract_i8', a, 2)),
    (('extract_i8', ('extract_i16', a, 1), 1), ('extract_i8', a, 3)),
@@ -2293,6 +2297,7 @@ optimizations.extend([
    (('u2u32', ('ushr', ('ior', ('ishl', a, 32), ('u2u64', 'b@32')), 32)), ('u2u32', a)),
    (('u2u16', ('ushr', ('ior', ('ishl', a, 16), ('u2u32', 'b@8')), 16)), ('u2u16', a)),
    (('u2u16', ('ushr', ('ior', ('ishl', a, 16), ('u2u32', 'b@16')), 16)), ('u2u16', a)),
+
 ])
 
 # After the ('extract_u8', a, 0) pattern, above, triggers, there will be
@@ -2748,7 +2753,136 @@ optimizations.extend([
                                                             ('extract_i8', 'v', 3))),
                                            127.0))),
      'options->lower_unpack_snorm_4x8'),
+])
+
+optimizations.extend([
+    # A1: pack_unorm_4x8 with fully saturated vec4
+    # Example: pack_unorm_4x8(vec4(fsat(r), fsat(g), fsat(b), fsat(a)))
+    # Frequency: ~35 occurrences in Cyberpunk shaders
+    (('pack_unorm_4x8', ('vec4', ('fsat', 'a'), ('fsat', 'b'), ('fsat', 'c'), ('fsat', 'd'))),
+     ('pack_unorm_4x8', ('vec4', a, b, c, d))),
+
+    # A3: pack_unorm_2x16 with fully saturated vec2
+    # Example: pack_unorm_2x16(vec2(fsat(x), fsat(y)))
+    # Frequency: ~8 occurrences in Cyberpunk UI shaders
+    (('pack_unorm_2x16', ('vec2', ('fsat', 'a'), ('fsat', 'b'))),
+     ('pack_unorm_2x16', ('vec2', a, b))),
+
+    # A4: pack_unorm_2x16 with scalar fsat
+    # Example: v = fsat(some_vec2); pack_unorm_2x16(v);
+    # Frequency: ~5 occurrences
+    (('pack_unorm_2x16', ('fsat', 'a@2')),
+     ('pack_unorm_2x16', a)),
+
+    # -------------------------------------------------------------------------
+    # GROUP B: Component-Wise vec2 Patterns (All Permutations)
+    # -------------------------------------------------------------------------
+    # For vec2, possible saturation states: {fsat(a), a} × {fsat(b), b}
+    # Total permutations: 2² = 4, but we skip {a, b} (no optimization)
+    # Useful permutations: 3 (excluding all-unsaturated)
+    # -------------------------------------------------------------------------
+
+    # B1: vec2(fsat(a), b) - only first component saturated
+    # Frequency: ~3 occurrences (after other optimizations create partial sat)
+    (('pack_unorm_2x16', ('vec2', ('fsat', 'a'), 'b')),
+     ('pack_unorm_2x16', ('vec2', a, b))),
+
+    # B2: vec2(a, fsat(b)) - only second component saturated
+    # Frequency: ~2 occurrences
+    (('pack_unorm_2x16', ('vec2', 'a', ('fsat', 'b'))),
+     ('pack_unorm_2x16', ('vec2', a, b))),
+])
 
+# --- New: pack_unorm_* redundancy removal beyond per-component fsat ---
+
+def _clamp01_minmax(x):
+    # clamp(x, 0, 1) in the common "min(max(x,0),1)" form
+    return ('fmin', ('fmax', x, 0.0), 1.0)
+
+def _clamp01_maxmin(x):
+    # clamp(x, 0, 1) in the alternative "max(min(x,1),0)" form
+    return ('fmax', ('fmin', x, 1.0), 0.0)
+
+# Vector-wide forms: match when clamp/fsat is applied to the whole vector SSA value.
+# The @32 constrains float bit-size; the opcode constrains component count.
+optimizations.extend([
+    # pack_unorm_2x16(fsat(v)) -> pack_unorm_2x16(v)
+    (('pack_unorm_2x16', ('fsat', 'a@32')),
+     ('pack_unorm_2x16', a)),
+
+    # pack_unorm_4x8(fsat(v)) -> pack_unorm_4x8(v)
+    (('pack_unorm_4x8', ('fsat', 'a@32')),
+     ('pack_unorm_4x8', a)),
+
+    # pack_unorm_2x16(clamp01(v)) -> pack_unorm_2x16(v)
+    (('pack_unorm_2x16', _clamp01_minmax('a@32')),
+     ('pack_unorm_2x16', a)),
+    (('pack_unorm_2x16', _clamp01_maxmin('a@32')),
+     ('pack_unorm_2x16', a)),
+
+    # pack_unorm_4x8(clamp01(v)) -> pack_unorm_4x8(v)
+    (('pack_unorm_4x8', _clamp01_minmax('a@32')),
+     ('pack_unorm_4x8', a)),
+    (('pack_unorm_4x8', _clamp01_maxmin('a@32')),
+     ('pack_unorm_4x8', a)),
+])
+
+# Per-component clamp-chain removal for vec2 (3 non-trivial masks) for both clamp forms.
+for clamp01 in (_clamp01_minmax, _clamp01_maxmin):
+    for mask in range(1, 4):  # 1..3
+        comps = ['a', 'b']
+        search_components = []
+        replace_components = []
+        for i, var in enumerate(comps):
+            search_components.append(clamp01(var) if (mask & (1 << i)) else var)
+            replace_components.append(var)
+
+        optimizations.append(
+            (('pack_unorm_2x16', ('vec2', *search_components)),
+             ('pack_unorm_2x16', ('vec2', *replace_components)))
+        )
+
+# Per-component clamp-chain removal for vec4 (15 non-trivial masks) for both clamp forms.
+for clamp01 in (_clamp01_minmax, _clamp01_maxmin):
+    for mask in range(1, 16):  # 1..15
+        comps = ['a', 'b', 'c', 'd']
+        search_components = []
+        replace_components = []
+        for i, var in enumerate(comps):
+            search_components.append(clamp01(var) if (mask & (1 << i)) else var)
+            replace_components.append(var)
+
+        optimizations.append(
+            (('pack_unorm_4x8', ('vec4', *search_components)),
+             ('pack_unorm_4x8', ('vec4', *replace_components)))
+        )
+
+# -------------------------------------------------------------------------
+# GROUP D: Component-Wise vec4 Patterns (Programmatically Generated)
+# -------------------------------------------------------------------------
+# Generate all permutations of fsat on vec4(a,b,c,d) except all-saturated (A1)
+# and all-unsaturated (no-op). This replaces manual D1-D14 definitions.
+#
+# Bit mask encodes which components are fsat:
+#   bit 0 = component a, bit 1 = b, bit 2 = c, bit 3 = d
+#   mask=15 (all fsat) is already handled by A1, mask=0 is no-op.
+# -------------------------------------------------------------------------
+for mask in range(1, 15):  # 1..14 (exclude 0 and 15)
+    search_components = []
+    replace_components = []
+    for i, var in enumerate(['a', 'b', 'c', 'd']):
+        if mask & (1 << i):
+            search_components.append(('fsat', var))
+        else:
+            search_components.append(var)
+        replace_components.append(var)
+
+    optimizations.append(
+        (('pack_unorm_4x8', ('vec4', *search_components)),
+         ('pack_unorm_4x8', ('vec4', *replace_components)))
+    )
+
+optimizations.extend([
    (('pack_half_2x16_split', 'a@32', 'b@32'),
     ('ior', ('ishl', ('u2u32', ('f2f16', b)), 16), ('u2u32', ('f2f16', a))),
     'options->lower_pack_split'),
@@ -2772,19 +2906,11 @@ optimizations.extend([
    (('isign', a), ('imin', ('imax', a, -1), 1), 'options->lower_isign'),
    (('imin', ('imax', a, -1), 1), ('isign', a), '!options->lower_isign'),
    (('imax', ('imin', a, 1), -1), ('isign', a), '!options->lower_isign'),
-   # float(0 < NaN) - float(NaN < 0) = float(False) - float(False) = 0 - 0 = 0
-   # Mark the new comparisons precise to prevent them being changed to 'a !=
-   # 0' or 'a == 0'.
    (('fsign', a), ('fsub', ('b2f', ('!flt', 0.0, a)), ('b2f', ('!flt', a, 0.0))), 'options->lower_fsign'),
    (('fsign', 'a@64'), ('fsub', ('b2f', ('!flt', 0.0, a)), ('b2f', ('!flt', a, 0.0))), 'options->lower_doubles_options & nir_lower_dsign'),
 
-   # Address/offset calculations:
-   # Drivers supporting imul24 should use a pass like nir_lower_amul(), this
-   # rule converts everyone else to imul:
    (('amul', a, b), ('imul', a, b), '!options->has_imul24 && !options->has_amul'),
 
-   # udiv_aligned_4 assumes the source is a multiple of 4 specifically to enable
-   # this identity. Usually this transform would require masking.
    (('amul', ('udiv_aligned_4', a), 4), a),
    (('imul', ('udiv_aligned_4', a), 4), a),
 
@@ -2795,7 +2921,6 @@ optimizations.extend([
     ('iadd', ('imul', ('iand', a, 0xffffff), ('iand', b, 0xffffff)), c),
     '!options->has_umad24'),
 
-   # Relaxed 24bit ops
    (('imul24_relaxed', a, b), ('imul24', a, b), '!options->has_mul24_relaxed && options->has_imul24'),
    (('imul24_relaxed', a, b), ('imul', a, b), '!options->has_mul24_relaxed && !options->has_imul24'),
    (('umad24_relaxed', a, b, c), ('umad24', a, b, c),  'options->has_umad24'),
@@ -2806,15 +2931,8 @@ optimizations.extend([
    (('imad24_ir3', a, b, 0), ('imul24', a, b)),
    (('imad24_ir3', a, 0, c), (c)),
    (('imad24_ir3', a, 1, c), ('iadd', a, c)), # this is not correct -- a's sign extension gets dropped.
-
-   # if first two srcs are const, crack apart the imad so constant folding
-   # can clean up the imul:
-   # TODO ffma should probably get a similar rule:
    (('imad24_ir3', '#a', '#b', c), ('iadd', ('imul24', a, b), c)),
 
-   # These will turn 24b address/offset calc back into 32b shifts, but
-   # it should be safe to get back some of the bits of precision that we
-   # already decided were no necessary:
    (('imul24', a, '#b@32(is_pos_power_of_two)'), ('ishl', a, ('find_lsb', b)), '!options->lower_bitops'),
    (('imul24', a, '#b@32(is_neg_power_of_two)'), ('ineg', ('ishl', a, ('find_lsb', ('iabs', b)))), '!options->lower_bitops'),
    (('imul24', a, 0), (0)),
@@ -2822,7 +2940,6 @@ optimizations.extend([
    (('imul_high@16', a, b), ('i2i16', ('ishr', ('imul24_relaxed', ('i2i32', a), ('i2i32', b)), 16)), 'options->lower_mul_high16'),
    (('umul_high@16', a, b), ('u2u16', ('ushr', ('umul24_relaxed', ('u2u32', a), ('u2u32', b)), 16)), 'options->lower_mul_high16'),
 
-   # Optimize vec2 unsigned comparison predicates to usub_sat with clamp.
    (('b2i16', ('vec2', ('ult', 'a@16', b), ('ult', 'c@16', d))),
     ('umin', 1, ('usub_sat', ('vec2', b, d), ('vec2', a, c))),
     'options->vectorize_vec2_16bit && !options->lower_usub_sat'),
@@ -2832,6 +2949,19 @@ optimizations.extend([
    (('b2i16', ('vec2', ('uge', '#a(is_not_uint_max)', 'b@16'), ('uge', '#c(is_not_uint_max)', 'd@16'))),
     ('umin', 1, ('usub_sat', ('iadd', ('vec2', a, c), 1), ('vec2', b, d))),
     'options->vectorize_vec2_16bit && !options->lower_usub_sat'),
+
+   (('~fmin@32', ('fmax@32(is_used_once)', 'a@32',  0.0),  1.0), ('fsat@32', 'a'), '!options->lower_fsat'),
+   (('~fmax@32', ('fmin@32(is_used_once)', 'a@32',  1.0),  0.0), ('fsat@32', 'a'), '!options->lower_fsat'),
+   (('~fmin@16', ('fmax@16(is_used_once)', 'a@16',  0.0),  1.0), ('fsat@16', 'a'), '!options->lower_fsat'),
+   (('~fmax@16', ('fmin@16(is_used_once)', 'a@16',  1.0),  0.0), ('fsat@16', 'a'), '!options->lower_fsat'),
+
+   (('fsat', ('fabs(is_used_once)', 'a')), ('fsat', 'a')),
+   (('fabs', ('fsat(is_used_once)', a)), ('fsat', a)),
+   (('fsat', ('fabs', ('fsat', a))), ('fsat', a)),
+   (('fsat', ('fmin', ('fmax', ('fabs', a), 0.0), 1.0)), ('fsat', a)),
+
+   (('fabs', ('fabs', a)), ('fabs', a)),
+   (('iabs', ('iabs', a)), ('iabs', a)),
 ])
 
 for bit_size in [8, 16, 32, 64]:
@@ -3004,12 +3134,20 @@ for N, M in itertools.product(type_sizes
       pass
 
 # Downcast operations should be able to see through pack
-for t in ['i', 'u']:
-    for N in [8, 16, 32]:
-        x2xN = '{0}2{0}{1}'.format(t, N)
+for t in ('i', 'u'):
+    for N in (8, 16, 32):
+        x2xN = f'{t}2{t}{N}'
+
         optimizations += [
+            # pack_64_2x32_* returns a 64-bit value with low 32 bits from x and high 32 from y.
+            # Downcasting to <=32 bits depends only on the low 32 bits.
             ((x2xN, ('pack_64_2x32_split', a, b)), (x2xN, a)),
-            ((x2xN, ('pack_64_2x32_split', a, b)), (x2xN, a)),
+            ((x2xN, ('pack_64_2x32', a)),          (x2xN, 'a.x')),
+
+            # pack_32_2x16_* returns a 32-bit value with low 16 bits from x and high 16 from y.
+            # Downcasting to <=16 bits depends only on the low 16 bits.
+            ((x2xN, ('pack_32_2x16_split', a, b)), (x2xN, a)),
+            ((x2xN, ('pack_32_2x16', a)),          (x2xN, 'a.x')),
         ]
 
 # Optimize comparisons with up-casts
@@ -3261,7 +3399,6 @@ def bitfield_reverse_xcom2(u):
     step3 = ('iadd', ('iand', ('ishl', step2, 2), 0xcccccccc), ('iand', ('ushr', step2, 2), 0x33333333))
     step4 = ('iadd', ('iand', ('ishl', step3, 4), 0xf0f0f0f0), ('iand', ('ushr', step3, 4), 0x0f0f0f0f))
     step5 = ('iadd(many-comm-expr)', ('iand', ('ishl', step4, 8), 0xff00ff00), ('iand', ('ushr', step4, 8), 0x00ff00ff))
-
     return step5
 
 # Unreal Engine 4 demo applications open-codes bitfieldReverse()
@@ -3271,7 +3408,6 @@ def bitfield_reverse_ue4(u):
     step3 = ('ior', ('ishl', ('iand', step2, 0x0f0f0f0f), 4), ('ushr', ('iand', step2, 0xf0f0f0f0), 4))
     step4 = ('ior', ('ishl', ('iand', step3, 0x33333333), 2), ('ushr', ('iand', step3, 0xcccccccc), 2))
     step5 = ('ior(many-comm-expr)', ('ishl', ('iand', step4, 0x55555555), 1), ('ushr', ('iand', step4, 0xaaaaaaaa), 1))
-
     return step5
 
 # Cyberpunk 2077 open-codes bitfieldReverse()
@@ -3281,7 +3417,6 @@ def bitfield_reverse_cp2077(u):
     step3 = ('ior', ('iand', ('ishl', step2, 2), 0xcccccccc), ('iand', ('ushr', step2, 2), 0x33333333))
     step4 = ('ior', ('iand', ('ishl', step3, 4), 0xf0f0f0f0), ('iand', ('ushr', step3, 4), 0x0f0f0f0f))
     step5 = ('ior(many-comm-expr)', ('iand', ('ishl', step4, 8), 0xff00ff00), ('iand', ('ushr', step4, 8), 0x00ff00ff))
-
     return step5
 
 optimizations += [(bitfield_reverse_xcom2('x@32'), ('bitfield_reverse', 'x'), '!options->lower_bitfield_reverse')]
@@ -3830,8 +3965,56 @@ for s in [8, 16, 32, 64]:
 
 late_optimizations.extend([
     # fneg_lo / fneg_hi
-   (('vec2(is_only_used_as_float)', ('fneg@16', a), b), ('fmul', ('vec2', a, b), ('vec2', -1.0, 1.0)), 'options->vectorize_vec2_16bit'),
-   (('vec2(is_only_used_as_float)', a, ('fneg@16', b)), ('fmul', ('vec2', a, b), ('vec2', 1.0, -1.0)), 'options->vectorize_vec2_16bit'),
+    (('vec2(is_only_used_as_float)', ('fneg@16', a), b),
+     ('fmul', ('vec2', a, b), ('vec2', -1.0, 1.0)),
+     'options->vectorize_vec2_16bit'),
+    (('vec2(is_only_used_as_float)', a, ('fneg@16', b)),
+     ('fmul', ('vec2', a, b), ('vec2', 1.0, -1.0)),
+     'options->vectorize_vec2_16bit'),
+
+    # Standard FMA fusion for f32
+    (('fadd@32', ('fmul@32(is_used_once)', 'a@32', 'b@32'), 'c@32'),
+     ('ffma@32', 'a', 'b', 'c'),
+     'options->fuse_ffma32'),
+
+    # FMA fusion for f16 (GCN has v_fma_f16 / suitable lowering paths)
+    (('fadd@16', ('fmul@16(is_used_once)', 'a@16', 'b@16'), 'c@16'),
+     ('ffma@16', 'a', 'b', 'c'),
+     'options->fuse_ffma16'),
+
+    # Inexact variants (allow loose NaN/rounding semantics)
+    (('~fadd@32', ('fmul@32(is_used_once)', 'a@32', 'b@32'), 'c@32'),
+     ('ffma@32', 'a', 'b', 'c'),
+     'options->fuse_ffma32'),
+
+    (('~fadd@16', ('fmul@16(is_used_once)', 'a@16', 'b@16'), 'c@16'),
+     ('ffma@16', 'a', 'b', 'c'),
+     'options->fuse_ffma16'),
+
+    (('fadd@32', ('fmulz@32(is_used_once)', 'a@32', 'b@32'), 'c@32'),
+     ('ffmaz@32', 'a', 'b', 'c'),
+     'options->fuse_ffma32 && ' + has_fmulz),
+
+    (('~fadd@32', ('fmulz@32(is_used_once)', 'a@32', 'b@32'), 'c@32'),
+     ('ffmaz@32', 'a', 'b', 'c'),
+     'options->fuse_ffma32 && ' + has_fmulz),
+
+    # Subtraction variant: fsub(c, fmul(a, b)) → ffma(-a, b, c)
+    (('fsub@32', 'c@32', ('fmul@32(is_used_once)', 'a@32', 'b@32')),
+     ('ffma@32', ('fneg', 'a'), 'b', 'c'),
+     'options->fuse_ffma32'),
+
+    (('~fsub@32', 'c@32', ('fmul@32(is_used_once)', 'a@32', 'b@32')),
+     ('ffma@32', ('fneg', 'a'), 'b', 'c'),
+     'options->fuse_ffma32'),
+
+    (('fsub@32', 'c@32', ('fmulz@32(is_used_once)', 'a@32', 'b@32')),
+     ('ffmaz@32', ('fneg', 'a'), 'b', 'c'),
+     'options->fuse_ffma32 && ' + has_fmulz),
+
+    (('~fsub@32', 'c@32', ('fmulz@32(is_used_once)', 'a@32', 'b@32')),
+     ('ffmaz@32', ('fneg', 'a'), 'b', 'c'),
+     'options->fuse_ffma32 && ' + has_fmulz),
 
    # These are duplicated from the main optimizations table.  The late
    # patterns that rearrange expressions like x - .5 < 0 to x < .5 can create
@@ -4029,25 +4212,24 @@ late_optimizations += [
 ]
 
 # A few more extract cases we'd rather leave late
-for N in [16, 32]:
-    aN = 'a@{0}'.format(N)
-    u2uM = 'u2u{0}'.format(M)
-    i2iM = 'i2i{0}'.format(M)
-
-    for x in ['u', 'i']:
-        x2xN = '{0}2{0}{1}'.format(x, N)
-        extract_x8 = 'extract_{0}8'.format(x)
-        extract_x16 = 'extract_{0}16'.format(x)
+for N in (16, 32):
+    aN = f'a@{N}'
 
-        late_optimizations.extend([
-            ((x2xN, ('u2u8', aN)), (extract_x8, a, 0), '!options->lower_extract_byte'),
-            ((x2xN, ('i2i8', aN)), (extract_x8, a, 0), '!options->lower_extract_byte'),
-        ])
+    for x in ('u', 'i'):
+        x2xN = f'{x}2{x}{N}'
+
+        for M, lower_opt in ((8, '!options->lower_extract_byte'),
+                             (16, '!options->lower_extract_word')):
+            if M >= N:
+                continue
+
+            extract_xM = f'extract_{x}{M}'
+            u2uM = f'u2u{M}'
+            i2iM = f'i2i{M}'
 
-        if N > 16:
             late_optimizations.extend([
-                ((x2xN, ('u2u16', aN)), (extract_x16, a, 0), '!options->lower_extract_word'),
-                ((x2xN, ('i2i16', aN)), (extract_x16, a, 0), '!options->lower_extract_word'),
+                ((x2xN, (u2uM, aN)), (extract_xM, a, 0), lower_opt),
+                ((x2xN, (i2iM, aN)), (extract_xM, a, 0), lower_opt),
             ])
 
 # Byte insertion
@@ -4118,6 +4300,24 @@ for op in ['ffma', 'flrp']:
 for op in ['feq', 'fge', 'flt', 'fneu']:
     late_optimizations += [(('~' + op, ('f2fmp', a), ('f2fmp', b)), (op, a, b), 'true', TestStatus.UNSUPPORTED)]
 
+# Vega v_fma_f16 fusion: Ensure fp16 mul+add fuses to FMA.
+# Critical for mixed-precision rendering (UI, post-process, HDR).
+# Vega v_fma_f16 has same latency as v_add_f16 but saves 1 instruction.
+for sz in [16]:
+    fadd = 'fadd@{}'.format(sz)
+    fmul = 'fmul@{}(is_used_once)'.format(sz)
+    ffma = 'ffma@{}'.format(sz)
+    option = 'options->fuse_ffma{}'.format(sz)
+
+    late_optimizations.extend([
+        # Standard a*b+c → fma(a,b,c)
+        ((fadd, (fmul, a, b), c), (ffma, a, b, c), option),
+        # Negated variants: c-a*b → fma(-a,b,c)
+        ((fadd, ('fneg(is_used_once)', (fmul, a, b)), c), (ffma, ('fneg', a), b, c), option),
+        # fabs variants for Vega VOP3 modifiers
+        ((fadd, ('fabs(is_used_once)', (fmul, a, b)), c), (ffma, ('fabs', a), ('fabs', b), c), option),
+])
+
 # Do this last, so that the f2fmp patterns above have effect.
 late_optimizations += [
   # Convert *2*mp instructions to concrete *2*16 instructions. At this point
