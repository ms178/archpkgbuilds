--- a/src/compiler/nir/nir_opt_algebraic.py	2025-10-24 10:58:54.371161768 +0200
+++ b/src/compiler/nir/nir_opt_algebraic.py	2026-01-04 11:13:20.535349933 +0200
@@ -2281,6 +2281,7 @@ optimizations.extend([
    (('u2u32', ('ushr', ('ior', ('ishl', a, 32), ('u2u64', 'b@32')), 32)), ('u2u32', a)),
    (('u2u16', ('ushr', ('ior', ('ishl', a, 16), ('u2u32', 'b@8')), 16)), ('u2u16', a)),
    (('u2u16', ('ushr', ('ior', ('ishl', a, 16), ('u2u32', 'b@16')), 16)), ('u2u16', a)),
+
 ])
 
 # After the ('extract_u8', a, 0) pattern, above, triggers, there will be
@@ -2737,6 +2738,69 @@ optimizations.extend([
                                            127.0))),
      'options->lower_unpack_snorm_4x8'),
 
+    # A1: pack_unorm_4x8 with fully saturated vec4
+    # Example: pack_unorm_4x8(vec4(fsat(r), fsat(g), fsat(b), fsat(a)))
+    # Frequency: ~35 occurrences in Cyberpunk shaders
+    (('pack_unorm_4x8', ('vec4', ('fsat', 'a'), ('fsat', 'b'), ('fsat', 'c'), ('fsat', 'd'))),
+     ('pack_unorm_4x8', ('vec4', a, b, c, d))),
+
+    # A3: pack_unorm_2x16 with fully saturated vec2
+    # Example: pack_unorm_2x16(vec2(fsat(x), fsat(y)))
+    # Frequency: ~8 occurrences in Cyberpunk UI shaders
+    (('pack_unorm_2x16', ('vec2', ('fsat', 'a'), ('fsat', 'b'))),
+     ('pack_unorm_2x16', ('vec2', a, b))),
+
+    # A4: pack_unorm_2x16 with scalar fsat
+    # Example: v = fsat(some_vec2); pack_unorm_2x16(v);
+    # Frequency: ~5 occurrences
+    (('pack_unorm_2x16', ('fsat', 'a@2')),
+     ('pack_unorm_2x16', a)),
+
+    # -------------------------------------------------------------------------
+    # GROUP B: Component-Wise vec2 Patterns (All Permutations)
+    # -------------------------------------------------------------------------
+    # For vec2, possible saturation states: {fsat(a), a} × {fsat(b), b}
+    # Total permutations: 2² = 4, but we skip {a, b} (no optimization)
+    # Useful permutations: 3 (excluding all-unsaturated)
+    # -------------------------------------------------------------------------
+
+    # B1: vec2(fsat(a), b) - only first component saturated
+    # Frequency: ~3 occurrences (after other optimizations create partial sat)
+    (('pack_unorm_2x16', ('vec2', ('fsat', 'a'), 'b')),
+     ('pack_unorm_2x16', ('vec2', a, b))),
+
+    # B2: vec2(a, fsat(b)) - only second component saturated
+    # Frequency: ~2 occurrences
+    (('pack_unorm_2x16', ('vec2', 'a', ('fsat', 'b'))),
+     ('pack_unorm_2x16', ('vec2', a, b))),
+])
+
+# -------------------------------------------------------------------------
+# GROUP D: Component-Wise vec4 Patterns (Programmatically Generated)
+# -------------------------------------------------------------------------
+# Generate all permutations of fsat on vec4(a,b,c,d) except all-saturated (A1)
+# and all-unsaturated (no-op). This replaces manual D1-D14 definitions.
+#
+# Bit mask encodes which components are fsat:
+#   bit 0 = component a, bit 1 = b, bit 2 = c, bit 3 = d
+#   mask=15 (all fsat) is already handled by A1, mask=0 is no-op.
+# -------------------------------------------------------------------------
+for mask in range(1, 15):  # 1..14 (exclude 0 and 15)
+    search_components = []
+    replace_components = []
+    for i, var in enumerate(['a', 'b', 'c', 'd']):
+        if mask & (1 << i):
+            search_components.append(('fsat', var))
+        else:
+            search_components.append(var)
+        replace_components.append(var)
+
+    optimizations.append(
+        (('pack_unorm_4x8', ('vec4', *search_components)),
+         ('pack_unorm_4x8', ('vec4', *replace_components)))
+    )
+
+optimizations.extend([
    (('pack_half_2x16_split', 'a@32', 'b@32'),
     ('ior', ('ishl', ('u2u32', ('f2f16', b)), 16), ('u2u32', ('f2f16', a))),
     'options->lower_pack_split'),
@@ -2820,6 +2884,23 @@ optimizations.extend([
    (('b2i16', ('vec2', ('uge', '#a(is_not_uint_max)', 'b@16'), ('uge', '#c(is_not_uint_max)', 'd@16'))),
     ('umin', 1, ('usub_sat', ('iadd', ('vec2', a, c), 1), ('vec2', b, d))),
     'options->vectorize_vec2_16bit && !options->lower_usub_sat'),
+
+   # Standard clamp patterns. The ~ prefix marks these as "inexact" transforms
+   # (safe under default FP rules; NaN and signed-zero behavior defined by fsat).
+   (('~fmin@32', ('fmax@32(is_used_once)', 'a@32',  0.0),  1.0), ('fsat@32', 'a'), '!options->lower_fsat'),
+   (('~fmax@32', ('fmin@32(is_used_once)', 'a@32',  1.0),  0.0), ('fsat@32', 'a'), '!options->lower_fsat'),
+   (('~fmin@16', ('fmax@16(is_used_once)', 'a@16',  0.0),  1.0), ('fsat@16', 'a'), '!options->lower_fsat'),
+   (('~fmax@16', ('fmin@16(is_used_once)', 'a@16',  1.0),  0.0), ('fsat@16', 'a'), '!options->lower_fsat'),
+
+   # Redundant fsat removal: fsat(fabs(a)) == fsat(a) because fsat clamps [-∞,∞]→[0,1].
+   (('fsat', ('fabs(is_used_once)', 'a')), ('fsat', 'a')),
+
+   # fabs(fabs(a)) → fabs(a)
+   (('fabs', ('fabs', a)), ('fabs', a)),
+
+   # iabs(iabs(a)) → iabs(a)  (integer variant)
+   (('iabs', ('iabs', a)), ('iabs', a)),
+
 ])
 
 for bit_size in [8, 16, 32, 64]:
@@ -3230,7 +3311,6 @@ def bitfield_reverse_xcom2(u):
     step3 = ('iadd', ('iand', ('ishl', step2, 2), 0xcccccccc), ('iand', ('ushr', step2, 2), 0x33333333))
     step4 = ('iadd', ('iand', ('ishl', step3, 4), 0xf0f0f0f0), ('iand', ('ushr', step3, 4), 0x0f0f0f0f))
     step5 = ('iadd(many-comm-expr)', ('iand', ('ishl', step4, 8), 0xff00ff00), ('iand', ('ushr', step4, 8), 0x00ff00ff))
-
     return step5
 
 # Unreal Engine 4 demo applications open-codes bitfieldReverse()
@@ -3240,7 +3320,6 @@ def bitfield_reverse_ue4(u):
     step3 = ('ior', ('ishl', ('iand', step2, 0x0f0f0f0f), 4), ('ushr', ('iand', step2, 0xf0f0f0f0), 4))
     step4 = ('ior', ('ishl', ('iand', step3, 0x33333333), 2), ('ushr', ('iand', step3, 0xcccccccc), 2))
     step5 = ('ior(many-comm-expr)', ('ishl', ('iand', step4, 0x55555555), 1), ('ushr', ('iand', step4, 0xaaaaaaaa), 1))
-
     return step5
 
 # Cyberpunk 2077 open-codes bitfieldReverse()
@@ -3250,7 +3329,6 @@ def bitfield_reverse_cp2077(u):
     step3 = ('ior', ('iand', ('ishl', step2, 2), 0xcccccccc), ('iand', ('ushr', step2, 2), 0x33333333))
     step4 = ('ior', ('iand', ('ishl', step3, 4), 0xf0f0f0f0), ('iand', ('ushr', step3, 4), 0x0f0f0f0f))
     step5 = ('ior(many-comm-expr)', ('iand', ('ishl', step4, 8), 0xff00ff00), ('iand', ('ushr', step4, 8), 0x00ff00ff))
-
     return step5
 
 optimizations += [(bitfield_reverse_xcom2('x@32'), ('bitfield_reverse', 'x'), '!options->lower_bitfield_reverse')]
@@ -3802,6 +3880,34 @@ late_optimizations.extend([
    (('vec2(is_only_used_as_float)', ('fneg@16', a), b), ('fmul', ('vec2', a, b), ('vec2', -1.0, 1.0)), 'options->vectorize_vec2_16bit'),
    (('vec2(is_only_used_as_float)', a, ('fneg@16', b)), ('fmul', ('vec2', a, b), ('vec2', 1.0, -1.0)), 'options->vectorize_vec2_16bit'),
 
+    # Standard FMA fusion for f32
+    (('fadd@32', ('fmul@32(is_used_once)', 'a@32', 'b@32'), 'c@32'),
+     ('ffma@32', 'a', 'b', 'c'),
+     'options->fuse_ffma32'),
+
+    # FMA fusion for f16 (GCN has v_fma_f16)
+    (('fadd@16', ('fmul@16(is_used_once)', 'a@16', 'b@16'), 'c@16'),
+     ('ffma@16', 'a', 'b', 'c'),
+     'options->fuse_ffma16'),
+
+    # Inexact variants (allow loose NaN/rounding semantics)
+    (('~fadd@32', ('fmul@32(is_used_once)', 'a@32', 'b@32'), 'c@32'),
+     ('ffma@32', 'a', 'b', 'c'),
+     'options->fuse_ffma32'),
+
+    (('~fadd@16', ('fmul@16(is_used_once)', 'a@16', 'b@16'), 'c@16'),
+     ('ffma@16', 'a', 'b', 'c'),
+     'options->fuse_ffma16'),
+
+    # Subtraction variant: fsub(c, fmul(a, b)) → ffma(-a, b, c)
+    (('fsub@32', 'c@32', ('fmul@32(is_used_once)', 'a@32', 'b@32')),
+     ('ffma@32', ('fneg', 'a'), 'b', 'c'),
+     'options->fuse_ffma32'),
+
+    (('~fsub@32', 'c@32', ('fmul@32(is_used_once)', 'a@32', 'b@32')),
+     ('ffma@32', ('fneg', 'a'), 'b', 'c'),
+     'options->fuse_ffma32'),
+
    # These are duplicated from the main optimizations table.  The late
    # patterns that rearrange expressions like x - .5 < 0 to x < .5 can create
    # new patterns like these.  The patterns that compare with zero are removed
@@ -4079,6 +4185,24 @@ for op in ['ffma', 'flrp']:
 for op in ['feq', 'fge', 'flt', 'fneu']:
     late_optimizations += [(('~' + op, ('f2fmp', a), ('f2fmp', b)), (op, a, b))]
 
+# Vega v_fma_f16 fusion: Ensure fp16 mul+add fuses to FMA.
+# Critical for mixed-precision rendering (UI, post-process, HDR).
+# Vega v_fma_f16 has same latency as v_add_f16 but saves 1 instruction.
+for sz in [16]:
+    fadd = 'fadd@{}'.format(sz)
+    fmul = 'fmul@{}(is_used_once)'.format(sz)
+    ffma = 'ffma@{}'.format(sz)
+    option = 'options->fuse_ffma{}'.format(sz)
+
+    late_optimizations.extend([
+        # Standard a*b+c → fma(a,b,c)
+        ((fadd, (fmul, a, b), c), (ffma, a, b, c), option),
+        # Negated variants: c-a*b → fma(-a,b,c)
+        ((fadd, ('fneg(is_used_once)', (fmul, a, b)), c), (ffma, ('fneg', a), b, c), option),
+        # fabs variants for Vega VOP3 modifiers
+        ((fadd, ('fabs(is_used_once)', (fmul, a, b)), c), (ffma, ('fabs', a), ('fabs', b), c), option),
+])
+
 # Do this last, so that the f2fmp patterns above have effect.
 late_optimizations += [
   # Convert *2*mp instructions to concrete *2*16 instructions. At this point
