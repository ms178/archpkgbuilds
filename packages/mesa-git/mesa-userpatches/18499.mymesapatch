From 03743e2ff128289ec3476b650f8bc225e05e63ec Mon Sep 17 00:00:00 2001
From: Mike Blumenkrantz <michael.blumenkrantz@gmail.com>
Date: Wed, 7 Sep 2022 15:13:28 -0400
Subject: [PATCH 02/21] radv: remove some pipeline comparisons from draw
 emission

these help poorly-optimized apps and hurt well-optimized ones
---
 src/amd/vulkan/radv_cmd_buffer.c | 6 +-----
 1 file changed, 1 insertion(+), 5 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index 0b991f325841..c91831c6247f 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -1427,9 +1427,6 @@ radv_emit_graphics_pipeline(struct radv_cmd_buffer *cmd_buffer)
    struct radv_graphics_pipeline *pipeline = cmd_buffer->state.graphics_pipeline;
    const struct radv_device *device = cmd_buffer->device;
 
-   if (cmd_buffer->state.emitted_graphics_pipeline == pipeline)
-      return;
-
    radv_update_multisample_state(cmd_buffer, pipeline);
    radv_update_binning_state(cmd_buffer, pipeline);
 
@@ -7353,8 +7350,7 @@ ALWAYS_INLINE static bool
 radv_before_draw(struct radv_cmd_buffer *cmd_buffer, const struct radv_draw_info *info, uint32_t drawCount)
 {
    const bool has_prefetch = cmd_buffer->device->physical_device->rad_info.gfx_level >= GFX7;
-   const bool pipeline_is_dirty = (cmd_buffer->state.dirty & RADV_CMD_DIRTY_PIPELINE) &&
-                                  cmd_buffer->state.graphics_pipeline != cmd_buffer->state.emitted_graphics_pipeline;
+   const bool pipeline_is_dirty = cmd_buffer->state.dirty & RADV_CMD_DIRTY_PIPELINE;
 
    ASSERTED const unsigned cdw_max =
       radeon_check_space(cmd_buffer->device->ws, cmd_buffer->cs, 4096 + 128 * (drawCount - 1));
-- 
GitLab


From d2734caea2077d2789b3d5ef9f22fbb39a607958 Mon Sep 17 00:00:00 2001
From: Mike Blumenkrantz <michael.blumenkrantz@gmail.com>
Date: Wed, 7 Sep 2022 15:27:19 -0400
Subject: [PATCH 03/21] radv: avoid another pipeline comparison for emitting
 framebuffer state

this isn't technically the same check, but for a well-optimized app
it should result in no change except for one fewer memcmp
---
 src/amd/vulkan/radv_cmd_buffer.c | 3 +--
 1 file changed, 1 insertion(+), 2 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index c91831c6247f..a6a21e960373 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -7242,8 +7242,7 @@ radv_emit_all_graphics_states(struct radv_cmd_buffer *cmd_buffer, const struct r
    const struct radv_device *device = cmd_buffer->device;
    bool late_scissor_emission;
 
-   if ((cmd_buffer->state.dirty & RADV_CMD_DIRTY_FRAMEBUFFER) ||
-       cmd_buffer->state.emitted_graphics_pipeline != cmd_buffer->state.graphics_pipeline)
+   if (cmd_buffer->state.dirty & (RADV_CMD_DIRTY_PIPELINE | RADV_CMD_DIRTY_FRAMEBUFFER))
       radv_emit_rbplus_state(cmd_buffer);
 
    if (cmd_buffer->device->physical_device->use_ngg_culling &&
-- 
GitLab

From fd7f730711b3cf5eb335e55de9961d909344eabe Mon Sep 17 00:00:00 2001
From: Mike Blumenkrantz <michael.blumenkrantz@gmail.com>
Date: Mon, 12 Sep 2022 13:55:44 -0400
Subject: [PATCH 08/21] radv: hoist indirect check up from
 radv_emit_index_buffer()

avoid calling the function entirely when possible, as info->indirect
should be possible to inline as a nullable stack value
---
 src/amd/vulkan/radv_cmd_buffer.c | 13 +++++--------
 1 file changed, 5 insertions(+), 8 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index b8452dbc075a..90c480af8a39 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -2783,7 +2783,7 @@ radv_emit_guardband_state(struct radv_cmd_buffer *cmd_buffer)
 }
 
 static void
-radv_emit_index_buffer(struct radv_cmd_buffer *cmd_buffer, bool indirect)
+radv_emit_index_buffer(struct radv_cmd_buffer *cmd_buffer)
 {
    struct radeon_cmdbuf *cs = cmd_buffer->cs;
    struct radv_cmd_state *state = &cmd_buffer->state;
@@ -2793,11 +2793,6 @@ radv_emit_index_buffer(struct radv_cmd_buffer *cmd_buffer, bool indirect)
    if (state->index_type < 0)
       return;
 
-   /* For the direct indexed draws we use DRAW_INDEX_2, which includes
-    * the index_va and max_index_count already. */
-   if (!indirect)
-      return;
-
    if (state->max_index_count ||
        !cmd_buffer->device->physical_device->rad_info.has_zero_index_buffer_bug) {
       radeon_emit(cs, PKT3(PKT3_INDEX_BASE, 1, 0));
@@ -7274,8 +7269,10 @@ radv_emit_all_graphics_states(struct radv_cmd_buffer *cmd_buffer, const struct r
       radv_emit_guardband_state(cmd_buffer);
 
    if (info->indexed) {
-      if (cmd_buffer->state.dirty & RADV_CMD_DIRTY_INDEX_BUFFER)
-         radv_emit_index_buffer(cmd_buffer, info->indirect);
+      /* For the direct indexed draws we use DRAW_INDEX_2, which includes
+       * the index_va and max_index_count already. */
+      if (info->indirect && cmd_buffer->state.dirty & RADV_CMD_DIRTY_INDEX_BUFFER)
+         radv_emit_index_buffer(cmd_buffer);
    } else {
       /* On GFX7 and later, non-indexed draws overwrite VGT_INDEX_TYPE,
        * so the state must be re-emitted before the next indexed
-- 
GitLab

From c7c2cba20edf934ef637c23b4de9831077bd8f99 Mon Sep 17 00:00:00 2001
From: Mike Blumenkrantz <michael.blumenkrantz@gmail.com>
Date: Mon, 12 Sep 2022 15:38:35 -0400
Subject: [PATCH 15/21] radv: break out radv_flush_constants check

---
 src/amd/vulkan/radv_cmd_buffer.c | 9 +++++++--
 1 file changed, 7 insertions(+), 2 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index 41f88221556e..bf205e6b43fa 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -3461,6 +3461,12 @@ radv_emit_all_inline_push_consts(struct radv_device *device, struct radeon_cmdbu
    }
 }
 
+ALWAYS_INLINE static bool
+radv_must_flush_constants(const struct radv_cmd_buffer *cmd_buffer, const struct radv_pipeline *pipeline, VkShaderStageFlags stages)
+{
+   return (stages & cmd_buffer->push_constant_stages) && (pipeline->push_constant_size || pipeline->dynamic_offset_count);
+}
+
 static void
 radv_flush_constants(struct radv_cmd_buffer *cmd_buffer, VkShaderStageFlags stages,
                      struct radv_pipeline *pipeline, VkPipelineBindPoint bind_point)
@@ -3477,8 +3483,7 @@ radv_flush_constants(struct radv_cmd_buffer *cmd_buffer, VkShaderStageFlags stag
    uint32_t internal_stages;
    uint32_t dirty_stages = 0;
 
-   stages &= cmd_buffer->push_constant_stages;
-   if (!stages || (!pipeline->push_constant_size && !pipeline->dynamic_offset_count))
+   if (!radv_must_flush_constants(cmd_buffer, pipeline, stages))
       return;
 
    internal_stages = stages;
-- 
GitLab

From 54a9ef49b1116972d887b4c470e51f1e96e150e8 Mon Sep 17 00:00:00 2001
From: Mike Blumenkrantz <michael.blumenkrantz@gmail.com>
Date: Wed, 14 Sep 2022 16:40:33 -0400
Subject: [PATCH 20/21] radv: consolidate indexed draw paths

this reworks the existing code to behave identically, but with less code
by forcing inlined values
---
 src/amd/vulkan/radv_cmd_buffer.c | 184 +++++++++++++------------------
 1 file changed, 74 insertions(+), 110 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index bda7e1df9e3e..df28d35262d4 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -6610,82 +6610,21 @@ radv_emit_userdata_task(struct radv_cmd_buffer *cmd_buffer, uint32_t x, uint32_t
 }
 
 ALWAYS_INLINE static void
-radv_emit_draw_packets_indexed(struct radv_cmd_buffer *cmd_buffer,
-                               const struct radv_draw_info *info,
-                               uint32_t drawCount, const VkMultiDrawIndexedInfoEXT *minfo,
-                               uint32_t stride,
-                               const int32_t *vertexOffset)
-
+radv_emit_draw_packets_indexed_internal(struct radv_cmd_buffer *cmd_buffer,
+                                        const struct radv_draw_info *info,
+                                        uint32_t drawCount, const VkMultiDrawIndexedInfoEXT *minfo,
+                                        uint32_t stride,
+                                        const int32_t *vertexOffset,
+                                        const bool uses_drawid,
+                                        const bool can_eop)
 {
    struct radv_cmd_state *state = &cmd_buffer->state;
    struct radeon_cmdbuf *cs = cmd_buffer->cs;
    const int index_size = radv_get_vgt_index_size(state->index_type);
    unsigned i = 0;
-   const bool uses_drawid = state->graphics_pipeline->uses_drawid;
-   const bool can_eop =
-      !uses_drawid && cmd_buffer->device->physical_device->rad_info.gfx_level >= GFX10;
-
-   if (uses_drawid) {
-      if (vertexOffset) {
-         radv_emit_userdata_vertex(cmd_buffer, info, *vertexOffset);
-         vk_foreach_multi_draw_indexed(draw, i, minfo, drawCount, stride) {
-            const uint32_t remaining_indexes = MAX2(state->max_index_count, draw->firstIndex) - draw->firstIndex;
-
-            /* Skip draw calls with 0-sized index buffers if the GPU can't handle them */
-            if (!remaining_indexes &&
-                cmd_buffer->device->physical_device->rad_info.has_zero_index_buffer_bug)
-               continue;
-
-            if (i > 0)
-               radeon_set_sh_reg(cs, state->graphics_pipeline->vtx_base_sgpr + sizeof(uint32_t), i);
-
-            const uint64_t index_va = state->index_va + draw->firstIndex * index_size;
-
-            if (!state->render.view_mask) {
-               radv_cs_emit_draw_indexed_packet(cmd_buffer, index_va, remaining_indexes, draw->indexCount, false);
-            } else {
-               u_foreach_bit(view, state->render.view_mask) {
-                  radv_emit_view_index(cmd_buffer, view);
-
-                  radv_cs_emit_draw_indexed_packet(cmd_buffer, index_va, remaining_indexes, draw->indexCount, false);
-               }
-            }
-         }
-      } else {
-         vk_foreach_multi_draw_indexed(draw, i, minfo, drawCount, stride) {
-            const uint32_t remaining_indexes = MAX2(state->max_index_count, draw->firstIndex) - draw->firstIndex;
-
-            /* Skip draw calls with 0-sized index buffers if the GPU can't handle them */
-            if (!remaining_indexes &&
-                cmd_buffer->device->physical_device->rad_info.has_zero_index_buffer_bug)
-               continue;
-
-            if (i > 0) {
-               if (state->last_vertex_offset != draw->vertexOffset)
-                  radv_emit_userdata_vertex_drawid(cmd_buffer, draw->vertexOffset, i);
-               else
-                  radeon_set_sh_reg(cs, state->graphics_pipeline->vtx_base_sgpr + sizeof(uint32_t), i);
-            } else
-               radv_emit_userdata_vertex(cmd_buffer, info, draw->vertexOffset);
-
-            const uint64_t index_va = state->index_va + draw->firstIndex * index_size;
 
-            if (!state->render.view_mask) {
-               radv_cs_emit_draw_indexed_packet(cmd_buffer, index_va, remaining_indexes, draw->indexCount, false);
-            } else {
-               u_foreach_bit(view, state->render.view_mask) {
-                  radv_emit_view_index(cmd_buffer, view);
-
-                  radv_cs_emit_draw_indexed_packet(cmd_buffer, index_va, remaining_indexes, draw->indexCount, false);
-               }
-            }
-         }
-      }
-      if (drawCount > 1) {
-         state->last_drawid = drawCount - 1;
-      }
-   } else {
-      if (vertexOffset) {
+   if (vertexOffset) {
+      if (!uses_drawid) {
          if (cmd_buffer->device->physical_device->rad_info.gfx_level == GFX10) {
             /* GFX10 has a bug that consecutive draw packets with NOT_EOP must not have
              * count == 0 for the last draw that doesn't have NOT_EOP.
@@ -6698,56 +6637,81 @@ radv_emit_draw_packets_indexed(struct radv_cmd_buffer *cmd_buffer,
             }
          }
 
-         radv_emit_userdata_vertex(cmd_buffer, info, *vertexOffset);
-         vk_foreach_multi_draw_indexed(draw, i, minfo, drawCount, stride) {
-            const uint32_t remaining_indexes = MAX2(state->max_index_count, draw->firstIndex) - draw->firstIndex;
-
-            /* Skip draw calls with 0-sized index buffers if the GPU can't handle them */
-            if (!remaining_indexes &&
-                cmd_buffer->device->physical_device->rad_info.has_zero_index_buffer_bug)
-               continue;
-
-            const uint64_t index_va = state->index_va + draw->firstIndex * index_size;
+      }
+      radv_emit_userdata_vertex(cmd_buffer, info, *vertexOffset);
+   }
 
-            if (!state->render.view_mask) {
-               radv_cs_emit_draw_indexed_packet(cmd_buffer, index_va, remaining_indexes, draw->indexCount, can_eop && i < drawCount - 1);
-            } else {
-               u_foreach_bit(view, state->render.view_mask) {
-                  radv_emit_view_index(cmd_buffer, view);
+   vk_foreach_multi_draw_indexed(draw, i, minfo, drawCount, stride) {
+      const uint32_t remaining_indexes = MAX2(state->max_index_count, draw->firstIndex) - draw->firstIndex;
+      bool offset_changes = false;
 
-                  radv_cs_emit_draw_indexed_packet(cmd_buffer, index_va, remaining_indexes, draw->indexCount, false);
-               }
-            }
-         }
-      } else {
-         vk_foreach_multi_draw_indexed(draw, i, minfo, drawCount, stride) {
-            const uint32_t remaining_indexes = MAX2(state->max_index_count, draw->firstIndex) - draw->firstIndex;
-
-            /* Skip draw calls with 0-sized index buffers if the GPU can't handle them */
-            if (!remaining_indexes &&
-                cmd_buffer->device->physical_device->rad_info.has_zero_index_buffer_bug)
-               continue;
+      /* Skip draw calls with 0-sized index buffers if the GPU can't handle them */
+      if (!remaining_indexes &&
+          cmd_buffer->device->physical_device->rad_info.has_zero_index_buffer_bug)
+         continue;
 
+      if (!vertexOffset) {
+         if (uses_drawid) {
+            if (i > 0) {
+               if (state->last_vertex_offset != draw->vertexOffset)
+                  radv_emit_userdata_vertex_drawid(cmd_buffer, draw->vertexOffset, i);
+               else
+                  radeon_set_sh_reg(cs, state->graphics_pipeline->vtx_base_sgpr + sizeof(uint32_t), i);
+            } else
+               radv_emit_userdata_vertex(cmd_buffer, info, draw->vertexOffset);
+         } else {
             const VkMultiDrawIndexedInfoEXT *next = (const VkMultiDrawIndexedInfoEXT*)(i < drawCount - 1 ? ((uint8_t*)draw + stride) : NULL);
-            const bool offset_changes = next && next->vertexOffset != draw->vertexOffset;
+            offset_changes = next && next->vertexOffset != draw->vertexOffset;
             radv_emit_userdata_vertex(cmd_buffer, info, draw->vertexOffset);
+         }
+      }
 
-            const uint64_t index_va = state->index_va + draw->firstIndex * index_size;
+      if (vertexOffset && uses_drawid) {
+         if (i > 0)
+            radeon_set_sh_reg(cs, state->graphics_pipeline->vtx_base_sgpr + sizeof(uint32_t), i);
+      }
 
-            if (!state->render.view_mask) {
-               radv_cs_emit_draw_indexed_packet(cmd_buffer, index_va, remaining_indexes, draw->indexCount, can_eop && !offset_changes && i < drawCount - 1);
-            } else {
-               u_foreach_bit(view, state->render.view_mask) {
-                  radv_emit_view_index(cmd_buffer, view);
+      const uint64_t index_va = state->index_va + draw->firstIndex * index_size;
 
-                  radv_cs_emit_draw_indexed_packet(cmd_buffer, index_va, remaining_indexes, draw->indexCount, false);
-               }
-            }
+      if (!state->render.view_mask) {
+         radv_cs_emit_draw_indexed_packet(cmd_buffer, index_va, remaining_indexes, draw->indexCount, can_eop && !offset_changes && i < drawCount - 1);
+      } else {
+         u_foreach_bit(view, state->render.view_mask) {
+            radv_emit_view_index(cmd_buffer, view);
+
+            radv_cs_emit_draw_indexed_packet(cmd_buffer, index_va, remaining_indexes, draw->indexCount, false);
          }
       }
-      if (drawCount > 1) {
-         state->last_drawid = drawCount - 1;
-      }
+   }
+
+   if (drawCount > 1) {
+      state->last_drawid = drawCount - 1;
+   }
+}
+
+ALWAYS_INLINE static void
+radv_emit_draw_packets_indexed(struct radv_cmd_buffer *cmd_buffer,
+                               const struct radv_draw_info *info,
+                               uint32_t drawCount, const VkMultiDrawIndexedInfoEXT *minfo,
+                               uint32_t stride,
+                               const int32_t *vertexOffset)
+
+{
+   struct radv_cmd_state *state = &cmd_buffer->state;
+   const bool uses_drawid = state->graphics_pipeline->uses_drawid;
+   const bool can_eop =
+      !uses_drawid && cmd_buffer->device->physical_device->rad_info.gfx_level >= GFX10;
+
+   if (uses_drawid) {
+      if (vertexOffset)
+         radv_emit_draw_packets_indexed_internal(cmd_buffer, info, drawCount, minfo, stride, vertexOffset, true, can_eop);
+      else
+         radv_emit_draw_packets_indexed_internal(cmd_buffer, info, drawCount, minfo, stride, NULL, true, can_eop);
+   } else {
+      if (vertexOffset)
+         radv_emit_draw_packets_indexed_internal(cmd_buffer, info, drawCount, minfo, stride, vertexOffset, false, can_eop);
+      else
+         radv_emit_draw_packets_indexed_internal(cmd_buffer, info, drawCount, minfo, stride, NULL, false, can_eop);
    }
 }
 
-- 
GitLab


From 053d90b8f42f1a5aa522d65a16749408c678d962 Mon Sep 17 00:00:00 2001
From: Mike Blumenkrantz <michael.blumenkrantz@gmail.com>
Date: Wed, 14 Sep 2022 21:00:15 -0400
Subject: [PATCH 21/21] radv: use the vertexOffset path for indexed draws

this is slightly (~1%) faster
---
 src/amd/vulkan/radv_cmd_buffer.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index df28d35262d4..0c17d49d5ac0 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -7720,7 +7720,7 @@ radv_CmdDrawIndexed(VkCommandBuffer commandBuffer, uint32_t indexCount, uint32_t
    if (!radv_before_draw(cmd_buffer, &info, 1))
       return;
    const VkMultiDrawIndexedInfoEXT minfo = { firstIndex, indexCount, vertexOffset };
-   radv_emit_draw_packets_indexed(cmd_buffer, &info, 1, &minfo, 0, NULL);
+   radv_emit_draw_packets_indexed(cmd_buffer, &info, 1, &minfo, 0, &vertexOffset);
    radv_after_draw(cmd_buffer);
 }
 
-- 
GitLab

From 1c42c2fe68cd225b4cd96b859e21e2c1cfe50cb1 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Tue, 27 Sep 2022 17:38:49 +0200
Subject: [PATCH] radv: Properly implement radv_before_taskmesh_draw.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Instead of calling radv_before_draw with hacks, now create a proper
implementation that specializes on mesh + task shaders.

Signed-off-by: Timur Krist√≥f <timur.kristof@gmail.com>
---
 src/amd/vulkan/radv_cmd_buffer.c | 115 ++++++++++++++++++++-----------
 1 file changed, 73 insertions(+), 42 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index bbd28d9177bd..a6148a7d0ef8 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -3981,7 +3981,7 @@ radv_upload_graphics_shader_descriptors(struct radv_cmd_buffer *cmd_buffer, bool
    radv_flush_vertex_descriptors(cmd_buffer, pipeline_is_dirty);
    radv_flush_streamout_descriptors(cmd_buffer);

-   VkShaderStageFlags stages = VK_SHADER_STAGE_ALL_GRAPHICS | VK_SHADER_STAGE_MESH_BIT_EXT;
+   VkShaderStageFlags stages = VK_SHADER_STAGE_ALL_GRAPHICS;
    radv_flush_descriptors(cmd_buffer, stages, &pipeline->base, VK_PIPELINE_BIND_POINT_GRAPHICS);
    radv_flush_constants(cmd_buffer, stages, &pipeline->base, VK_PIPELINE_BIND_POINT_GRAPHICS);
    radv_flush_ngg_query_state(cmd_buffer);
@@ -7445,61 +7445,92 @@ ALWAYS_INLINE static bool
 radv_before_taskmesh_draw(struct radv_cmd_buffer *cmd_buffer, const struct radv_draw_info *info,
                           uint32_t drawCount)
 {
-   struct radv_descriptor_state *descriptors_state =
-      radv_get_descriptors_state(cmd_buffer, VK_PIPELINE_BIND_POINT_GRAPHICS);
+   /* For direct draws, this makes sure we don't draw anything.
+    * For indirect draws, this is necessary to prevent a GPU hang.
+    */
+   if (unlikely(!info->count))
+      return false;
+
+   struct radv_graphics_pipeline *pipeline = cmd_buffer->state.graphics_pipeline;
+   struct radv_physical_device *pdevice = cmd_buffer->device->physical_device;
+   struct radeon_cmdbuf *ace_cs = cmd_buffer->ace_internal.cs;
+   struct radv_shader *task_shader = radv_get_shader(&pipeline->base, MESA_SHADER_TASK);
+
+   assert(!task_shader || ace_cs);
+
+   const VkShaderStageFlags stages = VK_SHADER_STAGE_MESH_BIT_EXT | (task_shader ? VK_SHADER_STAGE_TASK_BIT_EXT : 0);
    const bool pipeline_is_dirty =
       cmd_buffer->state.dirty & RADV_CMD_DIRTY_PIPELINE &&
       cmd_buffer->state.graphics_pipeline != cmd_buffer->state.emitted_graphics_pipeline;
-   const bool push_dirty = descriptors_state->push_dirty;
-   const uint32_t desc_dirty = descriptors_state->dirty;
+   const bool need_task_semaphore = task_shader && radv_flush_gfx2ace_semaphore(cmd_buffer);

-   const bool gfx_result = radv_before_draw(cmd_buffer, info, drawCount);
-   struct radv_graphics_pipeline *pipeline = cmd_buffer->state.graphics_pipeline;
-   struct radv_shader *task_shader = radv_get_shader(&pipeline->base, MESA_SHADER_TASK);
+   ASSERTED const unsigned cdw_max =
+      radeon_check_space(cmd_buffer->device->ws, cmd_buffer->cs, 4096 + 128 * (drawCount - 1));
+   ASSERTED const unsigned ace_cdw_max = !ace_cs ? 0 :
+      radeon_check_space(cmd_buffer->device->ws, ace_cs, 4096 + 128 * (drawCount - 1));

-   /* If there is no task shader, no need to do anything special. */
-   if (!task_shader)
-      return gfx_result;
+   if (cmd_buffer->state.dirty & RADV_CMD_DIRTY_FRAMEBUFFER)
+      radv_emit_fb_mip_change_flush(cmd_buffer);

-   /* Need to check the count even for indirect draws to work around
-    * an issue with DISPATCH_TASKMESH_INDIRECT_MULTI_ACE.
-    */
-   if (!info->count || !gfx_result)
-      return false;
+   if (need_task_semaphore ||
+       (cmd_buffer->state.flush_bits &
+        (RADV_CMD_FLAG_FLUSH_AND_INV_CB | RADV_CMD_FLAG_FLUSH_AND_INV_DB |
+         RADV_CMD_FLAG_PS_PARTIAL_FLUSH | RADV_CMD_FLAG_CS_PARTIAL_FLUSH))) {
+      radv_emit_all_graphics_states(cmd_buffer, info, pipeline_is_dirty);
+      if (task_shader && pipeline_is_dirty) {
+         radv_pipeline_emit_hw_cs(pdevice, ace_cs, task_shader);
+         radv_pipeline_emit_compute_state(pdevice, ace_cs, task_shader);
+      }

-   const bool need_task_semaphore = radv_flush_gfx2ace_semaphore(cmd_buffer);
-   struct radv_physical_device *pdevice = cmd_buffer->device->physical_device;
-   struct radeon_cmdbuf *ace_cs = cmd_buffer->ace_internal.cs;
-   struct radeon_winsys *ws = cmd_buffer->device->ws;
+      si_emit_cache_flush(cmd_buffer);

-   assert(ace_cs);
-   ASSERTED const unsigned ace_cdw_max =
-      radeon_check_space(ws, ace_cs, 4096 + 128 * (drawCount - 1));
+      if (task_shader) {
+         radv_ace_internal_cache_flush(cmd_buffer);

-   if (need_task_semaphore)
-      radv_wait_gfx2ace_semaphore(cmd_buffer);
+         if (need_task_semaphore) {
+            radv_wait_gfx2ace_semaphore(cmd_buffer);
+         }
+      }

-   if (pipeline_is_dirty) {
-      radv_pipeline_emit_hw_cs(pdevice, ace_cs, task_shader);
-      radv_pipeline_emit_compute_state(pdevice, ace_cs, task_shader);
-   }
+      /* <-- CUs are idle here --> */

-   radv_ace_internal_cache_flush(cmd_buffer);
+      radv_flush_descriptors(cmd_buffer, stages, &pipeline->base, VK_PIPELINE_BIND_POINT_GRAPHICS);
+      radv_flush_constants(cmd_buffer, stages, &pipeline->base, VK_PIPELINE_BIND_POINT_GRAPHICS);
+   } else {
+      si_emit_cache_flush(cmd_buffer);

-   /* Restore dirty state of descriptors
-    * They were marked non-dirty in radv_before_draw,
-    * but they need to be re-emitted now to the ACE cmdbuf.
-    */
-   descriptors_state->push_dirty = push_dirty;
-   descriptors_state->dirty = desc_dirty;
+      if (task_shader) {
+         radv_ace_internal_cache_flush(cmd_buffer);
+      }
+
+      if (cmd_buffer->state.prefetch_L2_mask) {
+         radv_emit_prefetch_L2(cmd_buffer, cmd_buffer->state.graphics_pipeline, true);
+      }
+
+      radv_flush_descriptors(cmd_buffer, stages, &pipeline->base, VK_PIPELINE_BIND_POINT_GRAPHICS);
+      radv_flush_constants(cmd_buffer, stages, &pipeline->base, VK_PIPELINE_BIND_POINT_GRAPHICS);
+      radv_emit_all_graphics_states(cmd_buffer, info, pipeline_is_dirty);

-   /* Flush descriptors and push constants for task shaders. */
-   radv_flush_descriptors(cmd_buffer, VK_SHADER_STAGE_TASK_BIT_EXT, &pipeline->base,
-                          VK_PIPELINE_BIND_POINT_GRAPHICS);
-   radv_flush_constants(cmd_buffer, VK_SHADER_STAGE_TASK_BIT_EXT, &pipeline->base,
-                        VK_PIPELINE_BIND_POINT_GRAPHICS);
+      if (task_shader && pipeline_is_dirty) {
+         radv_pipeline_emit_hw_cs(pdevice, ace_cs, task_shader);
+         radv_pipeline_emit_compute_state(pdevice, ace_cs, task_shader);
+      }
+   }
+
+   radv_describe_draw(cmd_buffer);
+   if (likely(!info->indirect)) {
+      struct radv_cmd_state *state = &cmd_buffer->state;
+      if (unlikely(state->last_num_instances != 1)) {
+         struct radeon_cmdbuf *cs = cmd_buffer->cs;
+         radeon_emit(cs, PKT3(PKT3_NUM_INSTANCES, 0, false));
+         radeon_emit(cs, 1);
+         state->last_num_instances = 1;
+      }
+   }
+
+   assert(cmd_buffer->cs->cdw <= cdw_max);
+   assert(!ace_cs || ace_cs->cdw <= ace_cdw_max);

-   assert(ace_cs->cdw <= ace_cdw_max);
    return true;
 }

--
GitLab
