From 03743e2ff128289ec3476b650f8bc225e05e63ec Mon Sep 17 00:00:00 2001
From: Mike Blumenkrantz <michael.blumenkrantz@gmail.com>
Date: Wed, 7 Sep 2022 15:13:28 -0400
Subject: [PATCH 02/21] radv: remove some pipeline comparisons from draw
 emission

these help poorly-optimized apps and hurt well-optimized ones
---
 src/amd/vulkan/radv_cmd_buffer.c | 6 +-----
 1 file changed, 1 insertion(+), 5 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index 0b991f325841..c91831c6247f 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -1427,9 +1427,6 @@ radv_emit_graphics_pipeline(struct radv_cmd_buffer *cmd_buffer)
    struct radv_graphics_pipeline *pipeline = cmd_buffer->state.graphics_pipeline;
    const struct radv_device *device = cmd_buffer->device;
 
-   if (cmd_buffer->state.emitted_graphics_pipeline == pipeline)
-      return;
-
    radv_update_multisample_state(cmd_buffer, pipeline);
    radv_update_binning_state(cmd_buffer, pipeline);
 
@@ -7353,8 +7350,7 @@ ALWAYS_INLINE static bool
 radv_before_draw(struct radv_cmd_buffer *cmd_buffer, const struct radv_draw_info *info, uint32_t drawCount)
 {
    const bool has_prefetch = cmd_buffer->device->physical_device->rad_info.gfx_level >= GFX7;
-   const bool pipeline_is_dirty = (cmd_buffer->state.dirty & RADV_CMD_DIRTY_PIPELINE) &&
-                                  cmd_buffer->state.graphics_pipeline != cmd_buffer->state.emitted_graphics_pipeline;
+   const bool pipeline_is_dirty = cmd_buffer->state.dirty & RADV_CMD_DIRTY_PIPELINE;
 
    ASSERTED const unsigned cdw_max =
       radeon_check_space(cmd_buffer->device->ws, cmd_buffer->cs, 4096 + 128 * (drawCount - 1));
-- 
GitLab


From d2734caea2077d2789b3d5ef9f22fbb39a607958 Mon Sep 17 00:00:00 2001
From: Mike Blumenkrantz <michael.blumenkrantz@gmail.com>
Date: Wed, 7 Sep 2022 15:27:19 -0400
Subject: [PATCH 03/21] radv: avoid another pipeline comparison for emitting
 framebuffer state

this isn't technically the same check, but for a well-optimized app
it should result in no change except for one fewer memcmp
---
 src/amd/vulkan/radv_cmd_buffer.c | 3 +--
 1 file changed, 1 insertion(+), 2 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index c91831c6247f..a6a21e960373 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -7242,8 +7242,7 @@ radv_emit_all_graphics_states(struct radv_cmd_buffer *cmd_buffer, const struct r
    const struct radv_device *device = cmd_buffer->device;
    bool late_scissor_emission;
 
-   if ((cmd_buffer->state.dirty & RADV_CMD_DIRTY_FRAMEBUFFER) ||
-       cmd_buffer->state.emitted_graphics_pipeline != cmd_buffer->state.graphics_pipeline)
+   if (cmd_buffer->state.dirty & (RADV_CMD_DIRTY_PIPELINE | RADV_CMD_DIRTY_FRAMEBUFFER))
       radv_emit_rbplus_state(cmd_buffer);
 
    if (cmd_buffer->device->physical_device->use_ngg_culling &&
-- 
GitLab

From fd7f730711b3cf5eb335e55de9961d909344eabe Mon Sep 17 00:00:00 2001
From: Mike Blumenkrantz <michael.blumenkrantz@gmail.com>
Date: Mon, 12 Sep 2022 13:55:44 -0400
Subject: [PATCH 08/21] radv: hoist indirect check up from
 radv_emit_index_buffer()

avoid calling the function entirely when possible, as info->indirect
should be possible to inline as a nullable stack value
---
 src/amd/vulkan/radv_cmd_buffer.c | 13 +++++--------
 1 file changed, 5 insertions(+), 8 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index b8452dbc075a..90c480af8a39 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -2783,7 +2783,7 @@ radv_emit_guardband_state(struct radv_cmd_buffer *cmd_buffer)
 }
 
 static void
-radv_emit_index_buffer(struct radv_cmd_buffer *cmd_buffer, bool indirect)
+radv_emit_index_buffer(struct radv_cmd_buffer *cmd_buffer)
 {
    struct radeon_cmdbuf *cs = cmd_buffer->cs;
    struct radv_cmd_state *state = &cmd_buffer->state;
@@ -2793,11 +2793,6 @@ radv_emit_index_buffer(struct radv_cmd_buffer *cmd_buffer, bool indirect)
    if (state->index_type < 0)
       return;
 
-   /* For the direct indexed draws we use DRAW_INDEX_2, which includes
-    * the index_va and max_index_count already. */
-   if (!indirect)
-      return;
-
    if (state->max_index_count ||
        !cmd_buffer->device->physical_device->rad_info.has_zero_index_buffer_bug) {
       radeon_emit(cs, PKT3(PKT3_INDEX_BASE, 1, 0));
@@ -7274,8 +7269,10 @@ radv_emit_all_graphics_states(struct radv_cmd_buffer *cmd_buffer, const struct r
       radv_emit_guardband_state(cmd_buffer);
 
    if (info->indexed) {
-      if (cmd_buffer->state.dirty & RADV_CMD_DIRTY_INDEX_BUFFER)
-         radv_emit_index_buffer(cmd_buffer, info->indirect);
+      /* For the direct indexed draws we use DRAW_INDEX_2, which includes
+       * the index_va and max_index_count already. */
+      if (info->indirect && cmd_buffer->state.dirty & RADV_CMD_DIRTY_INDEX_BUFFER)
+         radv_emit_index_buffer(cmd_buffer);
    } else {
       /* On GFX7 and later, non-indexed draws overwrite VGT_INDEX_TYPE,
        * so the state must be re-emitted before the next indexed
-- 
GitLab

From 29f651016f111cc8317924873a94aec1b3c23151 Mon Sep 17 00:00:00 2001
From: Mike Blumenkrantz <michael.blumenkrantz@gmail.com>
Date: Mon, 12 Sep 2022 15:04:12 -0400
Subject: [PATCH 12/21] radv: rework VGT_INDEX_TYPE flagging on draw -> indexed
 draw cmdbuf

previously, this mechanic would force RADV_CMD_DIRTY_INDEX_BUFFER on
every non-indexed draw, ensuring that non-indexed draws always had at
least 1 bit set in cmd_buffer->state.dirty

instead, track whether the previous draw was indexed and set the flag
appropriately if it will be consumed

this enables successive non-indexed draws to execute without setting any
dirty flags
---
 src/amd/vulkan/radv_cmd_buffer.c | 36 +++++++++++++++-----------------
 src/amd/vulkan/radv_private.h    |  1 +
 2 files changed, 18 insertions(+), 19 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index 59d99b7745c7..f27f155cf1de 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -2802,8 +2802,6 @@ radv_emit_index_buffer(struct radv_cmd_buffer *cmd_buffer)
       radeon_emit(cs, PKT3(PKT3_INDEX_BUFFER_SIZE, 0, 0));
       radeon_emit(cs, state->max_index_count);
    }
-
-   cmd_buffer->state.dirty &= ~RADV_CMD_DIRTY_INDEX_BUFFER;
 }
 
 void
@@ -5877,6 +5875,7 @@ radv_CmdExecuteCommands(VkCommandBuffer commandBuffer, uint32_t commandBufferCou
       primary->state.last_sx_ps_downconvert = secondary->state.last_sx_ps_downconvert;
       primary->state.last_sx_blend_opt_epsilon = secondary->state.last_sx_blend_opt_epsilon;
       primary->state.last_sx_blend_opt_control = secondary->state.last_sx_blend_opt_control;
+      primary->state.last_draw_indexed = secondary->state.last_draw_indexed;
 
       if (secondary->state.last_index_type != -1) {
          primary->state.last_index_type = secondary->state.last_index_type;
@@ -7258,31 +7257,19 @@ radv_emit_all_graphics_states(struct radv_cmd_buffer *cmd_buffer, const struct r
    if (cmd_buffer->state.dirty & RADV_CMD_DIRTY_PIPELINE)
       radv_emit_graphics_pipeline(cmd_buffer);
 
-   /* This should be before the cmd_buffer->state.dirty is cleared
-    * (excluding RADV_CMD_DIRTY_PIPELINE) and after
-    * cmd_buffer->state.context_roll_without_scissor_emitted is set. */
-   late_scissor_emission = radv_need_late_scissor_emission(cmd_buffer, info);
-
    if (cmd_buffer->state.dirty & RADV_CMD_DIRTY_FRAMEBUFFER)
       radv_emit_framebuffer_state(cmd_buffer);
 
    if (cmd_buffer->state.dirty & RADV_CMD_DIRTY_GUARDBAND)
       radv_emit_guardband_state(cmd_buffer);
 
-   if (info->indexed) {
-      /* For the direct indexed draws we use DRAW_INDEX_2, which includes
-       * the index_va and max_index_count already. */
-      if (info->indirect && cmd_buffer->state.dirty & RADV_CMD_DIRTY_INDEX_BUFFER)
+   if (cmd_buffer->state.dirty & RADV_CMD_DIRTY_INDEX_BUFFER) {
+      if (info->indexed && info->indirect) {
+         /* For the direct indexed draws we use DRAW_INDEX_2, which includes
+          * the index_va and max_index_count already. */
          radv_emit_index_buffer(cmd_buffer);
-   } else {
-      /* On GFX7 and later, non-indexed draws overwrite VGT_INDEX_TYPE,
-       * so the state must be re-emitted before the next indexed
-       * draw.
-       */
-      if (cmd_buffer->device->physical_device->rad_info.gfx_level >= GFX7) {
-         cmd_buffer->state.last_index_type = -1;
-         cmd_buffer->state.dirty |= RADV_CMD_DIRTY_INDEX_BUFFER;
       }
+      cmd_buffer->state.dirty &= ~RADV_CMD_DIRTY_INDEX_BUFFER;
    }
 
    if (cmd_buffer->device->force_vrs != RADV_FORCE_VRS_1x1) {
@@ -7373,8 +7360,19 @@ radv_before_draw(struct radv_cmd_buffer *cmd_buffer, const struct radv_draw_info
       /* Handle count == 0. */
       if (unlikely(!info->count && !info->strmout_buffer))
          return false;
+   } else if (info->indexed && !cmd_buffer->state.last_draw_indexed) {
+      /* On GFX7 and later, non-indexed draws overwrite VGT_INDEX_TYPE,
+       * so the state must be re-emitted before the next indexed
+       * draw.
+       */
+      if (cmd_buffer->device->physical_device->rad_info.gfx_level >= GFX7) {
+         cmd_buffer->state.last_index_type = -1;
+         cmd_buffer->state.dirty |= RADV_CMD_DIRTY_INDEX_BUFFER;
+      }
    }
 
+   cmd_buffer->state.last_draw_indexed = info->indexed;
+
    /* Need to apply this workaround early as it can set flush flags. */
    if (cmd_buffer->state.dirty & RADV_CMD_DIRTY_FRAMEBUFFER)
       radv_emit_fb_mip_change_flush(cmd_buffer);
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index f417fb18f200..046454d26788 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -1457,6 +1457,7 @@ struct radv_cmd_state {
    uint64_t vb_va;
 
    bool predicating;
+   bool last_draw_indexed;
    uint64_t dirty;
 
    uint32_t prefetch_L2_mask;
-- 
GitLab


From 305c7aac0ad5752143b0eddd71db3048d41f9305 Mon Sep 17 00:00:00 2001
From: Mike Blumenkrantz <michael.blumenkrantz@gmail.com>
Date: Mon, 12 Sep 2022 15:07:05 -0400
Subject: [PATCH 13/21] radv: group flag checking in
 radv_emit_all_graphics_states()

this improves branch selection for successive draws in optimized builds
---
 src/amd/vulkan/radv_cmd_buffer.c | 144 ++++++++++++++++---------------
 1 file changed, 75 insertions(+), 69 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index f27f155cf1de..02f4fba6f865 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -7247,91 +7247,97 @@ radv_emit_all_graphics_states(struct radv_cmd_buffer *cmd_buffer, const struct r
    const struct radv_device *device = cmd_buffer->device;
    bool late_scissor_emission;
 
-   if (cmd_buffer->state.dirty & (RADV_CMD_DIRTY_PIPELINE | RADV_CMD_DIRTY_FRAMEBUFFER))
-      radv_emit_rbplus_state(cmd_buffer);
+   /* This should be before the cmd_buffer->state.dirty is cleared
+    * (excluding RADV_CMD_DIRTY_PIPELINE) and after
+    * cmd_buffer->state.context_roll_without_scissor_emitted is set. */
+   late_scissor_emission = radv_need_late_scissor_emission(cmd_buffer, info);
 
    if (cmd_buffer->device->physical_device->use_ngg_culling &&
        cmd_buffer->state.graphics_pipeline->is_ngg)
       radv_emit_ngg_culling_state(cmd_buffer, info);
 
-   if (cmd_buffer->state.dirty & RADV_CMD_DIRTY_PIPELINE)
-      radv_emit_graphics_pipeline(cmd_buffer);
+   if (cmd_buffer->state.dirty) {
+      if (cmd_buffer->state.dirty & (RADV_CMD_DIRTY_PIPELINE | RADV_CMD_DIRTY_FRAMEBUFFER))
+         radv_emit_rbplus_state(cmd_buffer);
 
-   if (cmd_buffer->state.dirty & RADV_CMD_DIRTY_FRAMEBUFFER)
-      radv_emit_framebuffer_state(cmd_buffer);
+      if (cmd_buffer->state.dirty & RADV_CMD_DIRTY_PIPELINE)
+         radv_emit_graphics_pipeline(cmd_buffer);
+
+      if (cmd_buffer->state.dirty & RADV_CMD_DIRTY_FRAMEBUFFER)
+         radv_emit_framebuffer_state(cmd_buffer);
 
-   if (cmd_buffer->state.dirty & RADV_CMD_DIRTY_GUARDBAND)
-      radv_emit_guardband_state(cmd_buffer);
+      if (cmd_buffer->state.dirty & RADV_CMD_DIRTY_GUARDBAND)
+         radv_emit_guardband_state(cmd_buffer);
 
-   if (cmd_buffer->state.dirty & RADV_CMD_DIRTY_INDEX_BUFFER) {
-      if (info->indexed && info->indirect) {
-         /* For the direct indexed draws we use DRAW_INDEX_2, which includes
-          * the index_va and max_index_count already. */
-         radv_emit_index_buffer(cmd_buffer);
+      if (cmd_buffer->state.dirty & RADV_CMD_DIRTY_INDEX_BUFFER) {
+         if (info->indexed && info->indirect) {
+            /* For the direct indexed draws we use DRAW_INDEX_2, which includes
+             * the index_va and max_index_count already. */
+            radv_emit_index_buffer(cmd_buffer);
+         }
+         cmd_buffer->state.dirty &= ~RADV_CMD_DIRTY_INDEX_BUFFER;
       }
-      cmd_buffer->state.dirty &= ~RADV_CMD_DIRTY_INDEX_BUFFER;
-   }
 
-   if (cmd_buffer->device->force_vrs != RADV_FORCE_VRS_1x1) {
-      struct radv_dynamic_state *d = &cmd_buffer->state.dynamic;
-      uint64_t dynamic_states =
-         cmd_buffer->state.dirty & cmd_buffer->state.emitted_graphics_pipeline->needed_dynamic_state;
+      if (cmd_buffer->device->force_vrs != RADV_FORCE_VRS_1x1) {
+         struct radv_dynamic_state *d = &cmd_buffer->state.dynamic;
+         uint64_t dynamic_states =
+            cmd_buffer->state.dirty & cmd_buffer->state.emitted_graphics_pipeline->needed_dynamic_state;
 
-      if ((dynamic_states & RADV_CMD_DIRTY_DYNAMIC_FRAGMENT_SHADING_RATE) &&
-          d->fragment_shading_rate.size.width == 1 &&
-          d->fragment_shading_rate.size.height == 1 &&
-          d->fragment_shading_rate.combiner_ops[0] == VK_FRAGMENT_SHADING_RATE_COMBINER_OP_KEEP_KHR &&
-          d->fragment_shading_rate.combiner_ops[1] == VK_FRAGMENT_SHADING_RATE_COMBINER_OP_KEEP_KHR) {
-         /* When per-vertex VRS is forced and the dynamic fragment shading rate is a no-op, ignore
-          * it. This is needed for vkd3d-proton because it always declares per-draw VRS as dynamic.
-          */
-         cmd_buffer->state.dirty &= ~RADV_CMD_DIRTY_DYNAMIC_FRAGMENT_SHADING_RATE;
+         if ((dynamic_states & RADV_CMD_DIRTY_DYNAMIC_FRAGMENT_SHADING_RATE) &&
+             d->fragment_shading_rate.size.width == 1 &&
+             d->fragment_shading_rate.size.height == 1 &&
+             d->fragment_shading_rate.combiner_ops[0] == VK_FRAGMENT_SHADING_RATE_COMBINER_OP_KEEP_KHR &&
+             d->fragment_shading_rate.combiner_ops[1] == VK_FRAGMENT_SHADING_RATE_COMBINER_OP_KEEP_KHR) {
+            /* When per-vertex VRS is forced and the dynamic fragment shading rate is a no-op, ignore
+             * it. This is needed for vkd3d-proton because it always declares per-draw VRS as dynamic.
+             */
+            cmd_buffer->state.dirty &= ~RADV_CMD_DIRTY_DYNAMIC_FRAGMENT_SHADING_RATE;
+         }
       }
-   }
 
-   if (device->pbb_allowed) {
-      struct radv_binning_settings *settings = &device->physical_device->binning_settings;
+      if (device->pbb_allowed) {
+         struct radv_binning_settings *settings = &device->physical_device->binning_settings;
 
-      if ((cmd_buffer->state.dirty & RADV_CMD_DIRTY_DYNAMIC_COLOR_WRITE_ENABLE) &&
-          settings->context_states_per_bin > 1) {
-         /* Break the batch on CB_TARGET_MASK changes. */
-         radeon_emit(cmd_buffer->cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
-         radeon_emit(cmd_buffer->cs, EVENT_TYPE(V_028A90_BREAK_BATCH) | EVENT_INDEX(0));
+         if ((cmd_buffer->state.dirty & RADV_CMD_DIRTY_DYNAMIC_COLOR_WRITE_ENABLE) &&
+             settings->context_states_per_bin > 1) {
+            /* Break the batch on CB_TARGET_MASK changes. */
+            radeon_emit(cmd_buffer->cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
+            radeon_emit(cmd_buffer->cs, EVENT_TYPE(V_028A90_BREAK_BATCH) | EVENT_INDEX(0));
+         }
       }
-   }
 
-   /* Pre-compute some tessellation info that depend on the number of patch control points when the
-    * bound pipeline declared this state as dynamic.
-    */
-   if (cmd_buffer->state.graphics_pipeline->dynamic_states & RADV_DYNAMIC_PATCH_CONTROL_POINTS) {
-      uint64_t dynamic_states =
-         cmd_buffer->state.dirty & cmd_buffer->state.emitted_graphics_pipeline->needed_dynamic_state;
-
-      if (dynamic_states & RADV_CMD_DIRTY_DYNAMIC_PATCH_CONTROL_POINTS) {
-         const struct radv_physical_device *pdevice = device->physical_device;
-         const struct radv_graphics_pipeline *pipeline = cmd_buffer->state.graphics_pipeline;
-         const struct radv_shader *tcs = pipeline->base.shaders[MESA_SHADER_TESS_CTRL];
-         const struct radv_dynamic_state *d = &cmd_buffer->state.dynamic;
-
-         /* Compute the number of patches and emit the context register. */
-         cmd_buffer->state.tess_num_patches =
-            get_tcs_num_patches(d->patch_control_points, tcs->info.tcs.tcs_vertices_out,
-                                tcs->info.tcs.num_linked_inputs, tcs->info.tcs.num_linked_outputs,
-                                tcs->info.tcs.num_linked_patch_outputs,
-                                pdevice->hs.tess_offchip_block_dw_size, pdevice->rad_info.gfx_level,
-                                pdevice->rad_info.family);
-
-         /* Compute the LDS size and emit the shader register. */
-         cmd_buffer->state.tess_lds_size =
-            calculate_tess_lds_size(pdevice->rad_info.gfx_level, d->patch_control_points,
-                                    tcs->info.tcs.tcs_vertices_out, tcs->info.tcs.num_linked_inputs,
-                                    cmd_buffer->state.tess_num_patches,
-                                    tcs->info.tcs.num_linked_outputs,
-                                    tcs->info.tcs.num_linked_patch_outputs);
-      }
-   }
-
-   radv_cmd_buffer_flush_dynamic_state(cmd_buffer, pipeline_is_dirty);
+      /* Pre-compute some tessellation info that depend on the number of patch control points when the
+       * bound pipeline declared this state as dynamic.
+       */
+      if (cmd_buffer->state.graphics_pipeline->dynamic_states & RADV_DYNAMIC_PATCH_CONTROL_POINTS) {
+         uint64_t dynamic_states =
+            cmd_buffer->state.dirty & cmd_buffer->state.emitted_graphics_pipeline->needed_dynamic_state;
+
+         if (dynamic_states & RADV_CMD_DIRTY_DYNAMIC_PATCH_CONTROL_POINTS) {
+            const struct radv_physical_device *pdevice = device->physical_device;
+            const struct radv_graphics_pipeline *pipeline = cmd_buffer->state.graphics_pipeline;
+            const struct radv_shader *tcs = pipeline->base.shaders[MESA_SHADER_TESS_CTRL];
+            const struct radv_dynamic_state *d = &cmd_buffer->state.dynamic;
+
+            /* Compute the number of patches and emit the context register. */
+            cmd_buffer->state.tess_num_patches =
+               get_tcs_num_patches(d->patch_control_points, tcs->info.tcs.tcs_vertices_out,
+                                   tcs->info.tcs.num_linked_inputs, tcs->info.tcs.num_linked_outputs,
+                                   tcs->info.tcs.num_linked_patch_outputs,
+                                   pdevice->hs.tess_offchip_block_dw_size, pdevice->rad_info.gfx_level,
+                                   pdevice->rad_info.family);
+
+            /* Compute the LDS size and emit the shader register. */
+            cmd_buffer->state.tess_lds_size =
+               calculate_tess_lds_size(pdevice->rad_info.gfx_level, d->patch_control_points,
+                                       tcs->info.tcs.tcs_vertices_out, tcs->info.tcs.num_linked_inputs,
+                                       cmd_buffer->state.tess_num_patches,
+                                       tcs->info.tcs.num_linked_outputs,
+                                       tcs->info.tcs.num_linked_patch_outputs);
+         }
+      }
+      radv_cmd_buffer_flush_dynamic_state(cmd_buffer, pipeline_is_dirty);
+   }
 
    radv_emit_draw_registers(cmd_buffer, info);
 
-- 
GitLab

From c7c2cba20edf934ef637c23b4de9831077bd8f99 Mon Sep 17 00:00:00 2001
From: Mike Blumenkrantz <michael.blumenkrantz@gmail.com>
Date: Mon, 12 Sep 2022 15:38:35 -0400
Subject: [PATCH 15/21] radv: break out radv_flush_constants check

---
 src/amd/vulkan/radv_cmd_buffer.c | 9 +++++++--
 1 file changed, 7 insertions(+), 2 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index 41f88221556e..bf205e6b43fa 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -3461,6 +3461,12 @@ radv_emit_all_inline_push_consts(struct radv_device *device, struct radeon_cmdbu
    }
 }
 
+ALWAYS_INLINE static bool
+radv_must_flush_constants(const struct radv_cmd_buffer *cmd_buffer, const struct radv_pipeline *pipeline, VkShaderStageFlags stages)
+{
+   return (stages & cmd_buffer->push_constant_stages) && (pipeline->push_constant_size || pipeline->dynamic_offset_count);
+}
+
 static void
 radv_flush_constants(struct radv_cmd_buffer *cmd_buffer, VkShaderStageFlags stages,
                      struct radv_pipeline *pipeline, VkPipelineBindPoint bind_point)
@@ -3477,8 +3483,7 @@ radv_flush_constants(struct radv_cmd_buffer *cmd_buffer, VkShaderStageFlags stag
    uint32_t internal_stages;
    uint32_t dirty_stages = 0;
 
-   stages &= cmd_buffer->push_constant_stages;
-   if (!stages || (!pipeline->push_constant_size && !pipeline->dynamic_offset_count))
+   if (!radv_must_flush_constants(cmd_buffer, pipeline, stages))
       return;
 
    internal_stages = stages;
-- 
GitLab

From 54a9ef49b1116972d887b4c470e51f1e96e150e8 Mon Sep 17 00:00:00 2001
From: Mike Blumenkrantz <michael.blumenkrantz@gmail.com>
Date: Wed, 14 Sep 2022 16:40:33 -0400
Subject: [PATCH 20/21] radv: consolidate indexed draw paths

this reworks the existing code to behave identically, but with less code
by forcing inlined values
---
 src/amd/vulkan/radv_cmd_buffer.c | 184 +++++++++++++------------------
 1 file changed, 74 insertions(+), 110 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index bda7e1df9e3e..df28d35262d4 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -6610,82 +6610,21 @@ radv_emit_userdata_task(struct radv_cmd_buffer *cmd_buffer, uint32_t x, uint32_t
 }
 
 ALWAYS_INLINE static void
-radv_emit_draw_packets_indexed(struct radv_cmd_buffer *cmd_buffer,
-                               const struct radv_draw_info *info,
-                               uint32_t drawCount, const VkMultiDrawIndexedInfoEXT *minfo,
-                               uint32_t stride,
-                               const int32_t *vertexOffset)
-
+radv_emit_draw_packets_indexed_internal(struct radv_cmd_buffer *cmd_buffer,
+                                        const struct radv_draw_info *info,
+                                        uint32_t drawCount, const VkMultiDrawIndexedInfoEXT *minfo,
+                                        uint32_t stride,
+                                        const int32_t *vertexOffset,
+                                        const bool uses_drawid,
+                                        const bool can_eop)
 {
    struct radv_cmd_state *state = &cmd_buffer->state;
    struct radeon_cmdbuf *cs = cmd_buffer->cs;
    const int index_size = radv_get_vgt_index_size(state->index_type);
    unsigned i = 0;
-   const bool uses_drawid = state->graphics_pipeline->uses_drawid;
-   const bool can_eop =
-      !uses_drawid && cmd_buffer->device->physical_device->rad_info.gfx_level >= GFX10;
-
-   if (uses_drawid) {
-      if (vertexOffset) {
-         radv_emit_userdata_vertex(cmd_buffer, info, *vertexOffset);
-         vk_foreach_multi_draw_indexed(draw, i, minfo, drawCount, stride) {
-            const uint32_t remaining_indexes = MAX2(state->max_index_count, draw->firstIndex) - draw->firstIndex;
-
-            /* Skip draw calls with 0-sized index buffers if the GPU can't handle them */
-            if (!remaining_indexes &&
-                cmd_buffer->device->physical_device->rad_info.has_zero_index_buffer_bug)
-               continue;
-
-            if (i > 0)
-               radeon_set_sh_reg(cs, state->graphics_pipeline->vtx_base_sgpr + sizeof(uint32_t), i);
-
-            const uint64_t index_va = state->index_va + draw->firstIndex * index_size;
-
-            if (!state->render.view_mask) {
-               radv_cs_emit_draw_indexed_packet(cmd_buffer, index_va, remaining_indexes, draw->indexCount, false);
-            } else {
-               u_foreach_bit(view, state->render.view_mask) {
-                  radv_emit_view_index(cmd_buffer, view);
-
-                  radv_cs_emit_draw_indexed_packet(cmd_buffer, index_va, remaining_indexes, draw->indexCount, false);
-               }
-            }
-         }
-      } else {
-         vk_foreach_multi_draw_indexed(draw, i, minfo, drawCount, stride) {
-            const uint32_t remaining_indexes = MAX2(state->max_index_count, draw->firstIndex) - draw->firstIndex;
-
-            /* Skip draw calls with 0-sized index buffers if the GPU can't handle them */
-            if (!remaining_indexes &&
-                cmd_buffer->device->physical_device->rad_info.has_zero_index_buffer_bug)
-               continue;
-
-            if (i > 0) {
-               if (state->last_vertex_offset != draw->vertexOffset)
-                  radv_emit_userdata_vertex_drawid(cmd_buffer, draw->vertexOffset, i);
-               else
-                  radeon_set_sh_reg(cs, state->graphics_pipeline->vtx_base_sgpr + sizeof(uint32_t), i);
-            } else
-               radv_emit_userdata_vertex(cmd_buffer, info, draw->vertexOffset);
-
-            const uint64_t index_va = state->index_va + draw->firstIndex * index_size;
 
-            if (!state->render.view_mask) {
-               radv_cs_emit_draw_indexed_packet(cmd_buffer, index_va, remaining_indexes, draw->indexCount, false);
-            } else {
-               u_foreach_bit(view, state->render.view_mask) {
-                  radv_emit_view_index(cmd_buffer, view);
-
-                  radv_cs_emit_draw_indexed_packet(cmd_buffer, index_va, remaining_indexes, draw->indexCount, false);
-               }
-            }
-         }
-      }
-      if (drawCount > 1) {
-         state->last_drawid = drawCount - 1;
-      }
-   } else {
-      if (vertexOffset) {
+   if (vertexOffset) {
+      if (!uses_drawid) {
          if (cmd_buffer->device->physical_device->rad_info.gfx_level == GFX10) {
             /* GFX10 has a bug that consecutive draw packets with NOT_EOP must not have
              * count == 0 for the last draw that doesn't have NOT_EOP.
@@ -6698,56 +6637,81 @@ radv_emit_draw_packets_indexed(struct radv_cmd_buffer *cmd_buffer,
             }
          }
 
-         radv_emit_userdata_vertex(cmd_buffer, info, *vertexOffset);
-         vk_foreach_multi_draw_indexed(draw, i, minfo, drawCount, stride) {
-            const uint32_t remaining_indexes = MAX2(state->max_index_count, draw->firstIndex) - draw->firstIndex;
-
-            /* Skip draw calls with 0-sized index buffers if the GPU can't handle them */
-            if (!remaining_indexes &&
-                cmd_buffer->device->physical_device->rad_info.has_zero_index_buffer_bug)
-               continue;
-
-            const uint64_t index_va = state->index_va + draw->firstIndex * index_size;
+      }
+      radv_emit_userdata_vertex(cmd_buffer, info, *vertexOffset);
+   }
 
-            if (!state->render.view_mask) {
-               radv_cs_emit_draw_indexed_packet(cmd_buffer, index_va, remaining_indexes, draw->indexCount, can_eop && i < drawCount - 1);
-            } else {
-               u_foreach_bit(view, state->render.view_mask) {
-                  radv_emit_view_index(cmd_buffer, view);
+   vk_foreach_multi_draw_indexed(draw, i, minfo, drawCount, stride) {
+      const uint32_t remaining_indexes = MAX2(state->max_index_count, draw->firstIndex) - draw->firstIndex;
+      bool offset_changes = false;
 
-                  radv_cs_emit_draw_indexed_packet(cmd_buffer, index_va, remaining_indexes, draw->indexCount, false);
-               }
-            }
-         }
-      } else {
-         vk_foreach_multi_draw_indexed(draw, i, minfo, drawCount, stride) {
-            const uint32_t remaining_indexes = MAX2(state->max_index_count, draw->firstIndex) - draw->firstIndex;
-
-            /* Skip draw calls with 0-sized index buffers if the GPU can't handle them */
-            if (!remaining_indexes &&
-                cmd_buffer->device->physical_device->rad_info.has_zero_index_buffer_bug)
-               continue;
+      /* Skip draw calls with 0-sized index buffers if the GPU can't handle them */
+      if (!remaining_indexes &&
+          cmd_buffer->device->physical_device->rad_info.has_zero_index_buffer_bug)
+         continue;
 
+      if (!vertexOffset) {
+         if (uses_drawid) {
+            if (i > 0) {
+               if (state->last_vertex_offset != draw->vertexOffset)
+                  radv_emit_userdata_vertex_drawid(cmd_buffer, draw->vertexOffset, i);
+               else
+                  radeon_set_sh_reg(cs, state->graphics_pipeline->vtx_base_sgpr + sizeof(uint32_t), i);
+            } else
+               radv_emit_userdata_vertex(cmd_buffer, info, draw->vertexOffset);
+         } else {
             const VkMultiDrawIndexedInfoEXT *next = (const VkMultiDrawIndexedInfoEXT*)(i < drawCount - 1 ? ((uint8_t*)draw + stride) : NULL);
-            const bool offset_changes = next && next->vertexOffset != draw->vertexOffset;
+            offset_changes = next && next->vertexOffset != draw->vertexOffset;
             radv_emit_userdata_vertex(cmd_buffer, info, draw->vertexOffset);
+         }
+      }
 
-            const uint64_t index_va = state->index_va + draw->firstIndex * index_size;
+      if (vertexOffset && uses_drawid) {
+         if (i > 0)
+            radeon_set_sh_reg(cs, state->graphics_pipeline->vtx_base_sgpr + sizeof(uint32_t), i);
+      }
 
-            if (!state->render.view_mask) {
-               radv_cs_emit_draw_indexed_packet(cmd_buffer, index_va, remaining_indexes, draw->indexCount, can_eop && !offset_changes && i < drawCount - 1);
-            } else {
-               u_foreach_bit(view, state->render.view_mask) {
-                  radv_emit_view_index(cmd_buffer, view);
+      const uint64_t index_va = state->index_va + draw->firstIndex * index_size;
 
-                  radv_cs_emit_draw_indexed_packet(cmd_buffer, index_va, remaining_indexes, draw->indexCount, false);
-               }
-            }
+      if (!state->render.view_mask) {
+         radv_cs_emit_draw_indexed_packet(cmd_buffer, index_va, remaining_indexes, draw->indexCount, can_eop && !offset_changes && i < drawCount - 1);
+      } else {
+         u_foreach_bit(view, state->render.view_mask) {
+            radv_emit_view_index(cmd_buffer, view);
+
+            radv_cs_emit_draw_indexed_packet(cmd_buffer, index_va, remaining_indexes, draw->indexCount, false);
          }
       }
-      if (drawCount > 1) {
-         state->last_drawid = drawCount - 1;
-      }
+   }
+
+   if (drawCount > 1) {
+      state->last_drawid = drawCount - 1;
+   }
+}
+
+ALWAYS_INLINE static void
+radv_emit_draw_packets_indexed(struct radv_cmd_buffer *cmd_buffer,
+                               const struct radv_draw_info *info,
+                               uint32_t drawCount, const VkMultiDrawIndexedInfoEXT *minfo,
+                               uint32_t stride,
+                               const int32_t *vertexOffset)
+
+{
+   struct radv_cmd_state *state = &cmd_buffer->state;
+   const bool uses_drawid = state->graphics_pipeline->uses_drawid;
+   const bool can_eop =
+      !uses_drawid && cmd_buffer->device->physical_device->rad_info.gfx_level >= GFX10;
+
+   if (uses_drawid) {
+      if (vertexOffset)
+         radv_emit_draw_packets_indexed_internal(cmd_buffer, info, drawCount, minfo, stride, vertexOffset, true, can_eop);
+      else
+         radv_emit_draw_packets_indexed_internal(cmd_buffer, info, drawCount, minfo, stride, NULL, true, can_eop);
+   } else {
+      if (vertexOffset)
+         radv_emit_draw_packets_indexed_internal(cmd_buffer, info, drawCount, minfo, stride, vertexOffset, false, can_eop);
+      else
+         radv_emit_draw_packets_indexed_internal(cmd_buffer, info, drawCount, minfo, stride, NULL, false, can_eop);
    }
 }
 
-- 
GitLab


From 053d90b8f42f1a5aa522d65a16749408c678d962 Mon Sep 17 00:00:00 2001
From: Mike Blumenkrantz <michael.blumenkrantz@gmail.com>
Date: Wed, 14 Sep 2022 21:00:15 -0400
Subject: [PATCH 21/21] radv: use the vertexOffset path for indexed draws

this is slightly (~1%) faster
---
 src/amd/vulkan/radv_cmd_buffer.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index df28d35262d4..0c17d49d5ac0 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -7720,7 +7720,7 @@ radv_CmdDrawIndexed(VkCommandBuffer commandBuffer, uint32_t indexCount, uint32_t
    if (!radv_before_draw(cmd_buffer, &info, 1))
       return;
    const VkMultiDrawIndexedInfoEXT minfo = { firstIndex, indexCount, vertexOffset };
-   radv_emit_draw_packets_indexed(cmd_buffer, &info, 1, &minfo, 0, NULL);
+   radv_emit_draw_packets_indexed(cmd_buffer, &info, 1, &minfo, 0, &vertexOffset);
    radv_after_draw(cmd_buffer);
 }
 
-- 
GitLab

