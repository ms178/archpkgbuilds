--- a/src/amd/compiler/instruction_selection/aco_select_nir.cpp	2025-12-02 12:40:13.429803805 +0100
+++ b/src/amd/compiler/instruction_selection/aco_select_nir.cpp	2025-12-02 12:50:03.401422921 +0100
@@ -11,6 +10,8 @@
 #include "aco_nir_call_attribs.h"
 
 #include "amdgfxregs.h"
+
+#include <algorithm>
 #include <array>
 #include <utility>
 #include <vector>
@@ -25,9 +26,10 @@ visit_load_const(isel_context* ctx, nir_
 {
    Temp dst = get_ssa_temp(ctx, &instr->def);
 
-   // TODO: we really want to have the resulting type as this would allow for 64bit literals
-   // which get truncated the lsb if double and msb if int
-   // for now, we only use s_mov_b64 with 64bit inline constants
+   /* TODO: we really want to have the resulting type as this would allow for 64bit literals
+    * which get truncated the lsb if double and msb if int
+    * for now, we only use s_mov_b64 with 64bit inline constants
+    */
    assert(instr->def.num_components == 1 && "Vector load_const should be lowered to scalar.");
    assert(dst.type() == RegType::sgpr);
 
@@ -41,7 +43,7 @@ visit_load_const(isel_context* ctx, nir_
    } else if (instr->def.bit_size == 8) {
       bld.copy(Definition(dst), Operand::c32(instr->value[0].u8));
    } else if (instr->def.bit_size == 16) {
-      /* sign-extend to use s_movk_i32 instead of a literal */
+      /* Sign-extend to use s_movk_i32 instead of a literal. */
       bld.copy(Definition(dst), Operand::c32(instr->value[0].i16));
    } else if (dst.size() == 1) {
       bld.copy(Definition(dst), Operand::c32(instr->value[0].u32));
@@ -49,10 +51,10 @@ visit_load_const(isel_context* ctx, nir_
       assert(dst.size() != 1);
       aco_ptr<Instruction> vec{
          create_instruction(aco_opcode::p_create_vector, Format::PSEUDO, dst.size(), 1)};
-      if (instr->def.bit_size == 64)
+      if (instr->def.bit_size == 64) {
          for (unsigned i = 0; i < dst.size(); i++)
-            vec->operands[i] = Operand::c32(instr->value[0].u64 >> i * 32);
-      else {
+            vec->operands[i] = Operand::c32(static_cast<uint32_t>(instr->value[0].u64 >> (i * 32)));
+      } else {
          for (unsigned i = 0; i < dst.size(); i++)
             vec->operands[i] = Operand::c32(instr->value[i].u32);
       }
@@ -276,7 +278,7 @@ visit_tex(isel_context* ctx, nir_tex_ins
 
    coords = emit_pack_v1(ctx, unpacked_coord);
 
-   /* pack derivatives */
+   /* Pack derivatives. */
    if (has_ddx || has_ddy) {
       assert(a16 == g16 || ctx->options->gfx_level >= GFX10);
       std::array<Temp, 2> ddxddy = {ddx, ddy};
@@ -294,10 +296,10 @@ visit_tex(isel_context* ctx, nir_tex_ins
    bool da = false;
    if (instr->sampler_dim != GLSL_SAMPLER_DIM_BUF) {
       dim = ac_get_sampler_dim(ctx->options->gfx_level, instr->sampler_dim, instr->is_array);
-      da = should_declare_array((ac_image_dim)dim);
+      da = should_declare_array(static_cast<ac_image_dim>(dim));
    }
 
-   /* Build tex instruction */
+   /* Build tex instruction. */
    unsigned dmask = nir_def_components_read(&instr->def);
    /* Mask out the bit set for the sparse info. */
    if (instr->is_sparse)
@@ -312,7 +314,7 @@ visit_tex(isel_context* ctx, nir_tex_ins
    Temp dst = get_ssa_temp(ctx, &instr->def);
    Temp tmp_dst = dst;
 
-   /* gather4 selects the component by dmask and always returns vec4 (vec5 if sparse) */
+   /* gather4 selects the component by dmask and always returns vec4 (vec5 if sparse). */
    if (instr->op == nir_texop_tg4) {
       assert(instr->def.num_components == (4 + instr->is_sparse));
       if (instr->is_shadow)
@@ -377,8 +379,8 @@ visit_tex(isel_context* ctx, nir_tex_ins
                             bld.vop2(aco_opcode::v_add_f32, bld.def(v1), coords[1], half_texel[1])};
 
       if (tg4_integer_cube_workaround) {
-         /* see comment in ac_nir_to_llvm.c's lower_gather4_integer() */
-         Temp* const desc = (Temp*)alloca(resource.size() * sizeof(Temp));
+         /* See comment in ac_nir_to_llvm.c's lower_gather4_integer(). */
+         Temp* const desc = static_cast<Temp*>(alloca(resource.size() * sizeof(Temp)));
          aco_ptr<Instruction> split{
             create_instruction(aco_opcode::p_split_vector, Format::PSEUDO, 1, resource.size())};
          split->operands[0] = Operand(resource);
@@ -431,8 +433,9 @@ visit_tex(isel_context* ctx, nir_tex_ins
    }
 
    if (instr->sampler_dim == GLSL_SAMPLER_DIM_BUF) {
-      // FIXME: if (ctx->abi->gfx9_stride_size_workaround) return
-      // ac_build_buffer_load_format_gfx9_safe()
+      /* FIXME: if (ctx->abi->gfx9_stride_size_workaround) return
+       * ac_build_buffer_load_format_gfx9_safe()
+       */
 
       assert(coords.size() == 1);
       aco_opcode op;
@@ -471,7 +474,7 @@ visit_tex(isel_context* ctx, nir_tex_ins
       return;
    }
 
-   /* gather MIMG address components */
+   /* Gather MIMG address components. */
    std::vector<Temp> args;
    if (has_wqm_coord) {
       args.emplace_back(wqm_coord);
@@ -534,7 +537,7 @@ visit_tex(isel_context* ctx, nir_tex_ins
 
    bool separate_g16 = ctx->options->gfx_level >= GFX10 && g16;
 
-   // TODO: would be better to do this by adding offsets, but needs the opcodes ordered.
+   /* TODO: would be better to do this by adding offsets, but needs the opcodes ordered. */
    aco_opcode opcode = aco_opcode::image_sample;
    if (has_offset) { /* image_sample_*_o */
       if (has_clamped_lod) {
@@ -745,17 +748,64 @@ visit_phi(isel_context* ctx, nir_phi_ins
    assert(instr->def.bit_size != 1 || dst.regClass() == ctx->program->lane_mask);
    aco_opcode opcode = instr->def.bit_size == 1 ? aco_opcode::p_boolean_phi : aco_opcode::p_phi;
 
-   /* we want a sorted list of sources, since the predecessor list is also sorted */
-   std::map<unsigned, nir_def*> phi_src;
-   nir_foreach_phi_src (src, instr)
-      phi_src[src->pred->index] = src->src.ssa;
-
-   Instruction* phi = create_instruction(opcode, Format::PSEUDO, phi_src.size(), 1);
-   unsigned i = 0;
-   for (std::pair<unsigned, nir_def*> src : phi_src)
-      phi->operands[i++] = get_phi_operand(ctx, src.second, dst.regClass());
+   /*
+    * OPTIMIZATION: Replace std::map with a small sorted array.
+    *
+    * Phi nodes typically have 2-4 sources. Using std::map incurs:
+    *   - Heap allocation per insertion
+    *   - O(log n) per insertion
+    *   - Poor cache locality (red-black tree)
+    *
+    * A simple array with insertion sort is O(n^2) but for n<=8, this is faster
+    * than the overhead of std::map. For larger phis (rare), fall back to std::map.
+    */
+   constexpr unsigned SMALL_PHI_THRESHOLD = 8;
+   unsigned num_srcs = exec_list_length(&instr->srcs);
+
+   Instruction* phi = create_instruction(opcode, Format::PSEUDO, num_srcs, 1);
+
+   if (num_srcs <= SMALL_PHI_THRESHOLD) {
+      /* Small phi: use stack-based sorted array. */
+      struct PhiSrc {
+         unsigned pred_index;
+         nir_def* ssa;
+      };
+      PhiSrc sorted_srcs[SMALL_PHI_THRESHOLD];
+      unsigned count = 0;
+
+      nir_foreach_phi_src(src, instr) {
+         unsigned pred_idx = src->pred->index;
+         nir_def* ssa = src->src.ssa;
+
+         /* Insertion sort: find position and shift. */
+         unsigned insert_pos = count;
+         while (insert_pos > 0 && sorted_srcs[insert_pos - 1].pred_index > pred_idx) {
+            sorted_srcs[insert_pos] = sorted_srcs[insert_pos - 1];
+            --insert_pos;
+         }
+         sorted_srcs[insert_pos].pred_index = pred_idx;
+         sorted_srcs[insert_pos].ssa = ssa;
+         ++count;
+      }
+
+      for (unsigned i = 0; i < count; i++) {
+         phi->operands[i] = get_phi_operand(ctx, sorted_srcs[i].ssa, dst.regClass());
+      }
+   } else {
+      /* Large phi (rare): fall back to std::map for correctness. */
+      std::map<unsigned, nir_def*> phi_src;
+      nir_foreach_phi_src(src, instr) {
+         phi_src[src->pred->index] = src->src.ssa;
+      }
+
+      unsigned i = 0;
+      for (const auto& src : phi_src) {
+         phi->operands[i++] = get_phi_operand(ctx, src.second, dst.regClass());
+      }
+   }
+
    phi->definitions[0] = Definition(dst);
-   ctx->block->instructions.emplace(ctx->block->instructions.begin(), std::move(phi));
+   ctx->block->instructions.emplace(ctx->block->instructions.begin(), phi);
 }
 
 void
@@ -954,15 +1004,21 @@ visit_block(isel_context* ctx, nir_block
       ctx->unended_linear_vgprs.clear();
    }
 
-   nir_foreach_phi (instr, block)
+   nir_foreach_phi(instr, block) {
       visit_phi(ctx, instr);
+   }
 
    nir_phi_instr* last_phi = nir_block_last_phi_instr(block);
    begin_empty_exec_skip(ctx, last_phi ? &last_phi->instr : NULL, block);
 
-   ctx->block->instructions.reserve(ctx->block->instructions.size() +
-                                    exec_list_length(&block->instr_list) * 2);
-   nir_foreach_instr (instr, block) {
+   /*
+    * Reserve space for instructions to reduce reallocations.
+    * Typical expansion factor is ~1.5x (some NIR instructions become multiple ACO instructions).
+    */
+   const unsigned instr_count = exec_list_length(&block->instr_list);
+   ctx->block->instructions.reserve(ctx->block->instructions.size() + instr_count + instr_count / 2);
+
+   nir_foreach_instr(instr, block) {
       if (ctx->shader->has_debug_info)
          visit_debug_info(ctx, nir_instr_get_debug_info(instr));
 
@@ -1006,7 +1062,7 @@ visit_if(isel_context* ctx, nir_if* if_s
 
    if (!nir_src_is_divergent(&if_stmt->condition)) { /* uniform condition */
       /**
-       * Uniform conditionals are represented in the following way*) :
+       * Uniform conditionals are represented in the following way*):
        *
        * The linear and logical CFG:
        *                        BB_IF
@@ -1015,11 +1071,11 @@ visit_if(isel_context* ctx, nir_if* if_s
        *                        \    /
        *                        BB_ENDIF
        *
-       * *) Exceptions may be due to break and continue statements within loops
+       * *) Exceptions may be due to break and continue statements within loops.
        *    If a break/continue happens within uniform control flow, it branches
        *    to the loop exit/entry block. Otherwise, it branches to the next
        *    merge block.
-       **/
+       */
 
       assert(cond.regClass() == ctx->program->lane_mask);
       cond = bool_to_scalar_condition(ctx, cond);
@@ -1034,7 +1090,7 @@ visit_if(isel_context* ctx, nir_if* if_s
    } else { /* non-uniform condition */
       /**
        * To maintain a logical and linear CFG without critical edges,
-       * non-uniform conditionals are represented in the following way*) :
+       * non-uniform conditionals are represented in the following way*):
        *
        * The linear CFG:
        *                        BB_IF
@@ -1054,8 +1110,8 @@ visit_if(isel_context* ctx, nir_if* if_s
        *                        \    /
        *                        BB_ENDIF
        *
-       * *) Exceptions may be due to break and continue statements within loops
-       **/
+       * *) Exceptions may be due to break and continue statements within loops.
+       */
 
       begin_divergent_if_then(ctx, &ic, cond, if_stmt->control);
       visit_cf_list(ctx, &if_stmt->then_list);
@@ -1077,7 +1133,7 @@ visit_cf_list(isel_context* ctx, struct
    if_context empty_exec_skip_old = std::move(ctx->empty_exec_skip);
    ctx->skipping_empty_exec = false;
 
-   foreach_list_typed (nir_cf_node, node, node, list) {
+   foreach_list_typed(nir_cf_node, node, node, list) {
       switch (node->type) {
       case nir_cf_node_block: visit_block(ctx, nir_cf_node_as_block(node)); break;
       case nir_cf_node_if: visit_if(ctx, nir_cf_node_as_if(node)); break;
@@ -1179,7 +1235,7 @@ create_fs_end_for_epilog(isel_context* c
          continue;
 
       if (type == ACO_TYPE_ANY32) {
-         u_foreach_bit (i, write_mask) {
+         u_foreach_bit(i, write_mask) {
             regs.emplace_back(Operand(ctx->outputs.temps[slot * 4 + i], PhysReg{vgpr + i}));
          }
       } else {
@@ -1220,7 +1276,7 @@ split_arguments(isel_context* ctx, Instr
    /* Split all arguments except for the first (ring_offsets) and the last
     * (exec) so that the dead channels don't stay live throughout the program.
     */
-   for (int i = 1; i < startpgm->definitions.size(); i++) {
+   for (unsigned i = 1; i < startpgm->definitions.size(); i++) {
       if (startpgm->definitions[i].regClass().size() > 1) {
          emit_split_vector(ctx, startpgm->definitions[i].getTemp(),
                            startpgm->definitions[i].regClass().size());
@@ -1250,14 +1306,15 @@ setup_fp_mode(isel_context* ctx, nir_sha
       (FLOAT_CONTROLS_ROUNDING_MODE_RTZ_FP16 | FLOAT_CONTROLS_ROUNDING_MODE_RTZ_FP64 |
        FLOAT_CONTROLS_ROUNDING_MODE_RTE_FP16 | FLOAT_CONTROLS_ROUNDING_MODE_RTE_FP64);
 
-   /* default to preserving fp16 and fp64 denorms, since it's free for fp64 and
-    * the precision seems needed for Wolfenstein: Youngblood to render correctly */
+   /* Default to preserving fp16 and fp64 denorms, since it's free for fp64 and
+    * the precision seems needed for Wolfenstein: Youngblood to render correctly.
+    */
    if (program->next_fp_mode.must_flush_denorms16_64)
       program->next_fp_mode.denorm16_64 = 0;
    else
       program->next_fp_mode.denorm16_64 = fp_denorm_keep;
 
-   /* preserving fp32 denorms is expensive, so only do it if asked */
+   /* Preserving fp32 denorms is expensive, so only do it if asked. */
    if (float_controls & FLOAT_CONTROLS_DENORM_PRESERVE_FP32)
       program->next_fp_mode.denorm32 = fp_denorm_keep;
    else
@@ -1301,7 +1358,7 @@ insert_return(isel_context& ctx)
    unsigned preserved_param_count = 1;
    if (ctx.callee_info.return_address.needs_explicit_preservation)
       ++preserved_param_count;
-   for (auto param_info : ctx.callee_info.param_infos) {
+   for (const auto& param_info : ctx.callee_info.param_infos) {
       if (!param_info.is_reg || !param_info.needs_explicit_preservation)
          continue;
       ++preserved_param_count;
@@ -1351,7 +1408,7 @@ select_program_rt(isel_context& ctx, uns
       init_context(&ctx, nir);
       setup_fp_mode(&ctx, nir);
 
-      nir_function_impl* impl = NULL;
+      nir_function_impl* impl = nullptr;
       nir_foreach_function_impl (func, nir) {
          impl = func;
          break;

--- a/src/amd/compiler/instruction_selection/aco_isel_setup.cpp	2025-07-21 22:07:21.084777262 +0200
+++ b/src/amd/compiler/instruction_selection/aco_isel_setup.cpp	2026-01-14 22:16:00.003821217 +0200
@@ -12,92 +12,133 @@
 #include "nir_control_flow.h"
 
 #include "ac_nir.h"
+
+#include <algorithm>
+#include <limits>
 #include <vector>
 
 namespace aco {
 
 namespace {
 
-/* Check whether the given SSA def is only used by cross-lane instructions. */
-bool
-only_used_by_cross_lane_instrs(nir_def* ssa, bool follow_phis = true)
+/*
+ * Check whether the given SSA def is only used by cross-lane instructions.
+ *
+ * This function determines if a value is exclusively consumed by operations
+ * that read from other lanes (read_invocation, read_first_invocation, etc.).
+ * When true, we can keep the value in VGPRs without needing uniformization,
+ * allowing better instruction scheduling and reduced register pressure.
+ *
+ * The function is recursive but bounded by follow_phis=false on second level
+ * to prevent infinite loops in cyclic phi chains.
+ */
+static bool
+only_used_by_cross_lane_instrs(const nir_def* ssa, bool follow_phis = true)
 {
-   nir_foreach_use (src, ssa) {
-      switch (nir_src_parent_instr(src)->type) {
-      case nir_instr_type_alu: {
-         nir_alu_instr* alu = nir_instr_as_alu(nir_src_parent_instr(src));
-         if (alu->op != nir_op_unpack_64_2x32_split_x && alu->op != nir_op_unpack_64_2x32_split_y)
-            return false;
-         if (!only_used_by_cross_lane_instrs(&alu->def, follow_phis))
-            return false;
+   nir_foreach_use(src, ssa) {
+      nir_instr* const parent = nir_src_parent_instr(src);
+      const nir_instr_type parent_type = parent->type;
 
-         continue;
-      }
+      switch (parent_type) {
       case nir_instr_type_intrinsic: {
-         nir_intrinsic_instr* intrin = nir_instr_as_intrinsic(nir_src_parent_instr(src));
-         if (intrin->intrinsic != nir_intrinsic_read_invocation &&
-             intrin->intrinsic != nir_intrinsic_read_first_invocation &&
-             intrin->intrinsic != nir_intrinsic_lane_permute_16_amd)
+         const nir_intrinsic_instr* intrin = nir_instr_as_intrinsic(parent);
+         switch (intrin->intrinsic) {
+         case nir_intrinsic_read_invocation:
+         case nir_intrinsic_read_first_invocation:
+         case nir_intrinsic_lane_permute_16_amd:
+            /* These are cross-lane ops - continue checking other uses. */
+            break;
+         default:
+            /* Any other intrinsic means this isn't cross-lane only. */
             return false;
+         }
+         break;
+      }
 
-         continue;
+      case nir_instr_type_alu: {
+         const nir_alu_instr* alu = nir_instr_as_alu(parent);
+         switch (alu->op) {
+         case nir_op_unpack_64_2x32_split_x:
+         case nir_op_unpack_64_2x32_split_y:
+            /* These unpack ops are transparent - check their uses recursively. */
+            if (!only_used_by_cross_lane_instrs(&alu->def, follow_phis)) {
+               return false;
+            }
+            break;
+         default:
+            /* Other ALU ops are not cross-lane. */
+            return false;
+         }
+         break;
       }
+
       case nir_instr_type_phi: {
-         /* Don't follow more than 1 phis, this avoids infinite loops. */
-         if (!follow_phis)
+         /* Don't follow more than 1 phi level to avoid infinite loops. */
+         if (!follow_phis) {
             return false;
-
-         nir_phi_instr* phi = nir_instr_as_phi(nir_src_parent_instr(src));
-         if (!only_used_by_cross_lane_instrs(&phi->def, false))
+         }
+         const nir_phi_instr* phi = nir_instr_as_phi(parent);
+         if (!only_used_by_cross_lane_instrs(&phi->def, false)) {
             return false;
-
-         continue;
+         }
+         break;
       }
-      default: return false;
+
+      default:
+         /* Anything else (tex, jump, etc.) is not cross-lane. */
+         return false;
       }
    }
 
    return true;
 }
 
-/* If one side of a divergent IF ends in a branch and the other doesn't, we
+/*
+ * Sanitize divergent if-statements for ACO's requirements.
+ *
+ * If one side of a divergent IF ends in a branch and the other doesn't, we
  * might have to emit the contents of the side without the branch at the merge
  * block instead. This is so that we can use any SGPR live-out of the side
  * without the branch without creating a linear phi in the invert or merge block.
  *
  * This also removes any unreachable merge blocks.
  */
-bool
+static bool
 sanitize_if(nir_function_impl* impl, nir_if* nif)
 {
-   nir_block* then_block = nir_if_last_then_block(nif);
-   nir_block* else_block = nir_if_last_else_block(nif);
-   bool then_jump = nir_block_ends_in_jump(then_block);
-   bool else_jump = nir_block_ends_in_jump(else_block);
-   if (!then_jump && !else_jump)
+   nir_block* const then_block = nir_if_last_then_block(nif);
+   nir_block* const else_block = nir_if_last_else_block(nif);
+   const bool then_jump = nir_block_ends_in_jump(then_block);
+   const bool else_jump = nir_block_ends_in_jump(else_block);
+
+   /* If neither side jumps, nothing to do. */
+   if (!then_jump && !else_jump) {
       return false;
+   }
 
-   /* If the continue from block is empty then return as there is nothing to
-    * move.
-    */
-   if (nir_cf_list_is_empty_block(then_jump ? &nif->else_list : &nif->then_list))
+   /* If the continue-from block is empty then return as there is nothing to move. */
+   if (nir_cf_list_is_empty_block(then_jump ? &nif->else_list : &nif->then_list)) {
       return false;
+   }
 
-   /* Even though this if statement has a jump on one side, we may still have
-    * phis afterwards.  Single-source phis can be produced by loop unrolling
-    * or dead control-flow passes and are perfectly legal.  Run a quick phi
+   /*
+    * Even though this if-statement has a jump on one side, we may still have
+    * phis afterwards. Single-source phis can be produced by loop unrolling
+    * or dead control-flow passes and are perfectly legal. Run a quick phi
     * removal on the block after the if to clean up any such phis.
     */
-   nir_remove_single_src_phis_block(nir_cf_node_as_block(nir_cf_node_next(&nif->cf_node)));
+   nir_block* const next_block = nir_cf_node_as_block(nir_cf_node_next(&nif->cf_node));
+   nir_remove_single_src_phis_block(next_block);
 
-   /* Finally, move the continue from branch after the if-statement. */
-   nir_block* last_continue_from_blk = then_jump ? else_block : then_block;
-   nir_block* first_continue_from_blk =
+   /* Finally, move the continue-from branch after the if-statement. */
+   nir_block* const last_continue_from_blk = then_jump ? else_block : then_block;
+   nir_block* const first_continue_from_blk =
       then_jump ? nir_if_first_else_block(nif) : nir_if_first_then_block(nif);
 
    /* We don't need to repair SSA. nir_remove_after_cf_node() replaces any uses with undef. */
-   if (then_jump && else_jump)
+   if (then_jump && else_jump) {
       nir_remove_after_cf_node(&nif->cf_node);
+   }
 
    nir_cf_list tmp;
    nir_cf_extract(&tmp, nir_before_block(first_continue_from_blk),
@@ -107,13 +148,20 @@ sanitize_if(nir_function_impl* impl, nir
    return true;
 }
 
-bool
+/*
+ * Recursively sanitize control flow for ACO requirements.
+ */
+static bool
 sanitize_cf_list(nir_function_impl* impl, struct exec_list* cf_list)
 {
    bool progress = false;
-   foreach_list_typed (nir_cf_node, cf_node, node, cf_list) {
+
+   foreach_list_typed(nir_cf_node, cf_node, node, cf_list) {
       switch (cf_node->type) {
-      case nir_cf_node_block: break;
+      case nir_cf_node_block:
+         /* Nothing to do for plain blocks. */
+         break;
+
       case nir_cf_node_if: {
          nir_if* nif = nir_cf_node_as_if(cf_node);
          progress |= sanitize_cf_list(impl, &nif->then_list);
@@ -121,16 +169,21 @@ sanitize_cf_list(nir_function_impl* impl
          progress |= sanitize_if(impl, nif);
          break;
       }
+
       case nir_cf_node_loop: {
          nir_loop* loop = nir_cf_node_as_loop(cf_node);
          assert(!nir_loop_has_continue_construct(loop));
          progress |= sanitize_cf_list(impl, &loop->body);
 
-         /* NIR seems to allow this, and even though the loop exit has no predecessors, SSA defs
-          * from the loop header are live. Handle this without complicating the ACO IR by creating a
-          * dummy break.
+         /*
+          * NIR seems to allow this, and even though the loop exit has no predecessors,
+          * SSA defs from the loop header are live. Handle this without complicating
+          * the ACO IR by creating a dummy break.
+          *
+          * nir_cf_node_cf_tree_next returns a nir_block*, which has the predecessors field.
           */
-         if (nir_cf_node_cf_tree_next(&loop->cf_node)->predecessors.entries == 0) {
+         nir_block* const next_block = nir_cf_node_cf_tree_next(&loop->cf_node);
+         if (next_block->predecessors.entries == 0) {
             nir_builder b = nir_builder_create(impl);
             b.cursor = nir_after_block_before_jump(nir_loop_last_block(loop));
 
@@ -144,93 +197,80 @@ sanitize_cf_list(nir_function_impl* impl
          }
          break;
       }
-      case nir_cf_node_function: UNREACHABLE("Invalid cf type");
+
+      case nir_cf_node_function:
+         UNREACHABLE("Invalid cf type");
       }
    }
 
    return progress;
 }
 
-void
+/*
+ * Attempt to mark an iadd instruction as no-unsigned-wrap (nuw).
+ *
+ * This uses NIR's upper-bound analysis to prove that the addition cannot
+ * overflow, enabling better optimization in later passes.
+ */
+static void
 apply_nuw_to_ssa(isel_context* ctx, nir_def* ssa)
 {
    nir_scalar scalar;
    scalar.def = ssa;
    scalar.comp = 0;
 
-   if (!nir_scalar_is_alu(scalar) || nir_scalar_alu_op(scalar) != nir_op_iadd)
+   if (!nir_scalar_is_alu(scalar) || nir_scalar_alu_op(scalar) != nir_op_iadd) {
       return;
+   }
 
+   /* Use nir_def_as_alu to convert from nir_def* to nir_alu_instr* */
    nir_alu_instr* add = nir_def_as_alu(ssa);
 
-   if (add->no_unsigned_wrap)
+   if (add->no_unsigned_wrap) {
       return;
+   }
 
    nir_scalar src0 = nir_scalar_chase_alu_src(scalar, 0);
    nir_scalar src1 = nir_scalar_chase_alu_src(scalar, 1);
 
    if (nir_scalar_is_const(src0)) {
-      std::swap(src0, src1);
+      nir_scalar tmp = src0;
+      src0 = src1;
+      src1 = tmp;
    }
 
-   uint32_t src1_ub = nir_unsigned_upper_bound(ctx->shader, ctx->range_ht, src1);
+   /* Use NIR's upper-bound analysis to prove iadd nuw. */
+   const uint32_t src1_ub = nir_unsigned_upper_bound(ctx->shader, ctx->range_ht, src1);
    add->no_unsigned_wrap = !nir_addition_might_overflow(ctx->shader, ctx->range_ht, src0, src1_ub);
 }
 
-void
-apply_nuw_to_offsets(isel_context* ctx, nir_function_impl* impl)
-{
-   nir_foreach_block (block, impl) {
-      nir_foreach_instr (instr, block) {
-         if (instr->type != nir_instr_type_intrinsic)
-            continue;
-         nir_intrinsic_instr* intrin = nir_instr_as_intrinsic(instr);
-
-         switch (intrin->intrinsic) {
-         case nir_intrinsic_load_constant:
-         case nir_intrinsic_load_uniform:
-         case nir_intrinsic_load_push_constant:
-            if (!nir_src_is_divergent(&intrin->src[0]))
-               apply_nuw_to_ssa(ctx, intrin->src[0].ssa);
-            break;
-         case nir_intrinsic_load_ubo:
-         case nir_intrinsic_load_ssbo:
-            if (!nir_src_is_divergent(&intrin->src[1]))
-               apply_nuw_to_ssa(ctx, intrin->src[1].ssa);
-            break;
-         case nir_intrinsic_store_ssbo:
-            if (!nir_src_is_divergent(&intrin->src[2]))
-               apply_nuw_to_ssa(ctx, intrin->src[2].ssa);
-            break;
-         case nir_intrinsic_load_scratch: apply_nuw_to_ssa(ctx, intrin->src[0].ssa); break;
-         case nir_intrinsic_store_scratch: apply_nuw_to_ssa(ctx, intrin->src[1].ssa); break;
-         case nir_intrinsic_load_global_amd:
-            if (nir_intrinsic_access(intrin) & ACCESS_SMEM_AMD)
-               apply_nuw_to_ssa(ctx, intrin->src[1].ssa);
-            break;
-         default: break;
-         }
-      }
-   }
-}
-
-RegClass
+/*
+ * Get the appropriate register class for a value with the given properties.
+ */
+static RegClass
 get_reg_class(isel_context* ctx, RegType type, unsigned components, unsigned bitsize)
 {
-   if (bitsize == 1)
+   if (bitsize == 1) {
       return RegClass(RegType::sgpr, ctx->program->lane_mask.size() * components);
-   else
+   } else {
       return RegClass::get(type, components * bitsize / 8u);
+   }
 }
 
-void
+/*
+ * Setup TCS (Tessellation Control Shader) related context info.
+ */
+static void
 setup_tcs_info(isel_context* ctx)
 {
    ctx->tcs_in_out_eq = ctx->program->info.vs.tcs_in_out_eq;
    ctx->any_tcs_inputs_via_lds = ctx->program->info.vs.any_tcs_inputs_via_lds;
 }
 
-void
+/*
+ * Prepare NIR shader for instruction selection.
+ */
+static void
 setup_nir(isel_context* ctx, nir_shader* nir)
 {
    nir_convert_to_lcssa(nir, true, false);
@@ -246,24 +286,34 @@ setup_nir(isel_context* ctx, nir_shader*
       nir_index_ssa_defs(func);
 }
 
-/* Returns true if we can skip uniformization of a merge phi. This makes the destination divergent,
- * and so is only safe if the inconsistency it introduces into the divergence analysis won't break
- * code generation. If we unsafely skip uniformization, later instructions (such as SSBO loads,
- * some subgroup intrinsics and certain conversions) can use divergence analysis information which
- * is no longer correct.
+/*
+ * Returns true if we can skip uniformization of a merge phi.
+ *
+ * This makes the destination divergent, and so is only safe if the
+ * inconsistency it introduces into the divergence analysis won't break
+ * code generation. If we unsafely skip uniformization, later instructions
+ * (such as SSBO loads, some subgroup intrinsics and certain conversions)
+ * can use divergence analysis information which is no longer correct.
  */
-bool
-skip_uniformize_merge_phi(nir_def* ssa, unsigned depth)
+static bool
+skip_uniformize_merge_phi(const nir_def* ssa, unsigned depth)
 {
-   if (depth >= 16)
+   /* Limit recursion depth to prevent stack overflow on pathological shaders. */
+   if (depth >= 16) {
       return false;
+   }
 
-   nir_foreach_use (src, ssa) {
-      switch (nir_src_parent_instr(src)->type) {
+   nir_foreach_use(src, ssa) {
+      nir_instr* const parent = nir_src_parent_instr(src);
+      const nir_instr_type parent_type = parent->type;
+
+      switch (parent_type) {
       case nir_instr_type_alu: {
-         nir_alu_instr* alu = nir_instr_as_alu(nir_src_parent_instr(src));
-         if (alu->def.divergent)
+         const nir_alu_instr* alu = nir_instr_as_alu(parent);
+         if (alu->def.divergent) {
+            /* Already divergent, no problem. */
             break;
+         }
 
          switch (alu->op) {
          case nir_op_f2i16:
@@ -278,58 +328,90 @@ skip_uniformize_merge_phi(nir_def* ssa,
          case nir_op_b2f32:
          case nir_op_b2f64:
          case nir_op_mov:
-            /* These opcodes p_as_uniform or vote_any() the source, so fail immediately. We don't
-             * need to do this for non-nir_op_b2 if we know we'll move it back into a VGPR,
-             * in which case the p_as_uniform would be eliminated. This would be way too fragile,
-             * though.
+            /*
+             * These opcodes use p_as_uniform or vote_any() on the source,
+             * so we cannot skip uniformization.
              */
             return false;
+
          default:
-            if (!skip_uniformize_merge_phi(&alu->def, depth + 1))
+            if (!skip_uniformize_merge_phi(&alu->def, depth + 1)) {
                return false;
+            }
             break;
          }
          break;
       }
+
       case nir_instr_type_intrinsic: {
-         nir_intrinsic_instr* intrin = nir_instr_as_intrinsic(nir_src_parent_instr(src));
-         unsigned src_idx = src - intrin->src;
-         /* nir_intrinsic_lane_permute_16_amd is only safe because we don't use divergence analysis
-          * for it's instruction selection. We use that intrinsic for NGG culling. All others are
-          * stores with VGPR sources.
+         const nir_intrinsic_instr* intrin = nir_instr_as_intrinsic(parent);
+         const unsigned num_srcs = nir_intrinsic_infos[intrin->intrinsic].num_srcs;
+
+         /*
+          * Calculate source index safely.
+          * The src pointer should be within the intrin->src array.
+          */
+         unsigned src_idx = 0;
+         for (unsigned i = 0; i < num_srcs; i++) {
+            if (&intrin->src[i] == src) {
+               src_idx = i;
+               break;
+            }
+         }
+
+         /*
+          * Safe intrinsics: either don't use divergence for ISel or use VGPR sources.
+          * These won't be affected by incorrect divergence info.
           */
-         if (intrin->intrinsic == nir_intrinsic_lane_permute_16_amd ||
-             intrin->intrinsic == nir_intrinsic_export_amd ||
-             intrin->intrinsic == nir_intrinsic_export_dual_src_blend_amd ||
-             (intrin->intrinsic == nir_intrinsic_export_row_amd && src_idx == 0) ||
-             (intrin->intrinsic == nir_intrinsic_store_buffer_amd && src_idx == 0) ||
-             (intrin->intrinsic == nir_intrinsic_store_ssbo && src_idx == 0) ||
-             (intrin->intrinsic == nir_intrinsic_store_global && src_idx == 0) ||
-             (intrin->intrinsic == nir_intrinsic_store_scratch && src_idx == 0) ||
-             (intrin->intrinsic == nir_intrinsic_store_shared && src_idx == 0))
+         switch (intrin->intrinsic) {
+         case nir_intrinsic_lane_permute_16_amd:
+         case nir_intrinsic_export_amd:
+         case nir_intrinsic_export_dual_src_blend_amd:
             break;
-         return false;
+         case nir_intrinsic_export_row_amd:
+         case nir_intrinsic_store_buffer_amd:
+         case nir_intrinsic_store_ssbo:
+         case nir_intrinsic_store_global:
+         case nir_intrinsic_store_scratch:
+         case nir_intrinsic_store_shared:
+            if (src_idx == 0) {
+               break;
+            }
+            return false;
+         default:
+            return false;
+         }
+         break;
       }
+
       case nir_instr_type_phi: {
-         nir_phi_instr* phi = nir_instr_as_phi(nir_src_parent_instr(src));
-         if (phi->def.divergent || skip_uniformize_merge_phi(&phi->def, depth + 1))
+         const nir_phi_instr* phi = nir_instr_as_phi(parent);
+         if (phi->def.divergent) {
+            /* Already divergent, no problem. */
             break;
-         return false;
-      }
-      case nir_instr_type_tex: {
-         /* This is either used as a VGPR source or it's a (potentially undef) descriptor. */
+         }
+         if (!skip_uniformize_merge_phi(&phi->def, depth + 1)) {
+            return false;
+         }
          break;
       }
-      default: {
+
+      case nir_instr_type_tex:
+         /* Either used as a VGPR source or it's a (potentially undef) descriptor. */
+         break;
+
+      default:
          return false;
       }
-      }
    }
 
    return true;
 }
 
-bool
+/*
+ * Callback for nir_opt_load_skip_helpers to determine which loads can skip helpers.
+ */
+static bool
 intrinsic_try_skip_helpers(nir_intrinsic_instr* intr, UNUSED void* data)
 {
    switch (intr->intrinsic) {
@@ -342,40 +424,266 @@ intrinsic_try_skip_helpers(nir_intrinsic
    case nir_intrinsic_bindless_image_fragment_mask_load_amd:
    case nir_intrinsic_bindless_image_sparse_load:
       return !(nir_intrinsic_access(intr) & ACCESS_SMEM_AMD);
-   default: return false;
+   default:
+      return false;
    }
 }
 
-} /* end namespace */
+/*
+ * Assign register class (SGPR vs VGPR) for an ALU instruction result.
+ *
+ * CRITICAL: This function must correctly identify which operations require VGPRs.
+ * On GFX9 (Vega), there are no scalar versions of:
+ *   - Transcendental functions (sin, cos, exp, log, rcp, rsq, sqrt)
+ *   - Float-to-int and int-to-float conversions
+ *   - Most floating-point operations
+ *
+ * Incorrect classification leads to "Unsupported opcode" errors in the assembler.
+ */
+static void
+assign_alu_regclass(isel_context* ctx, nir_alu_instr* alu_instr, RegClass* regclasses)
+{
+   RegType type = RegType::sgpr;
+   const unsigned num_components = alu_instr->def.num_components;
+   const unsigned bitsize = alu_instr->def.bit_size;
+
+   /* Packed 16-bit instructions have to be VGPR. */
+   if (num_components == 2 && ac_nir_op_supports_packed_math_16bit(alu_instr)) {
+      type = RegType::vgpr;
+   } else {
+      switch (alu_instr->op) {
+      /*
+       * These conversion ops can use p_as_uniform or vote_any() on their source,
+       * so they need special handling based on divergence AND source regclass.
+       */
+      case nir_op_f2i16:
+      case nir_op_f2u16:
+      case nir_op_f2i32:
+      case nir_op_f2u32:
+      case nir_op_mov:
+         if (alu_instr->def.divergent &&
+             regclasses[alu_instr->src[0].src.ssa->index].type() == RegType::vgpr) {
+            type = RegType::vgpr;
+         }
+         break;
+
+      /*
+       * These operations ALWAYS require VGPRs - there are no scalar equivalents
+       * on any AMD GPU generation.
+       */
+      case nir_op_f2e4m3fn:
+      case nir_op_f2e4m3fn_sat:
+      case nir_op_f2e4m3fn_satfn:
+      case nir_op_f2e5m2:
+      case nir_op_f2e5m2_sat:
+      case nir_op_e4m3fn2f:
+      case nir_op_e5m22f:
+      case nir_op_fmulz:
+      case nir_op_ffmaz:
+      case nir_op_f2f64:
+      case nir_op_u2f64:
+      case nir_op_i2f64:
+      case nir_op_pack_unorm_2x16:
+      case nir_op_pack_snorm_2x16:
+      case nir_op_pack_uint_2x16:
+      case nir_op_pack_sint_2x16:
+      case nir_op_ldexp:
+      case nir_op_frexp_sig:
+      case nir_op_frexp_exp:
+      case nir_op_cube_amd:
+      case nir_op_msad_4x8:
+      case nir_op_mqsad_4x8:
+      case nir_op_udot_4x8_uadd:
+      case nir_op_sdot_4x8_iadd:
+      case nir_op_sudot_4x8_iadd:
+      case nir_op_udot_4x8_uadd_sat:
+      case nir_op_sdot_4x8_iadd_sat:
+      case nir_op_sudot_4x8_iadd_sat:
+      case nir_op_udot_2x16_uadd:
+      case nir_op_sdot_2x16_iadd:
+      case nir_op_udot_2x16_uadd_sat:
+      case nir_op_sdot_2x16_iadd_sat:
+      case nir_op_bfdot2_bfadd:
+      case nir_op_byte_perm_amd:
+      case nir_op_alignbyte_amd:
+      case nir_op_f2f16_ru:
+      case nir_op_f2f16_rd: type = RegType::vgpr; break;
+
+      /*
+       * Floating-point operations: require VGPR on GFX9 through GFX11.
+       * On GFX11.5+, some of these have scalar equivalents for 32-bit or smaller.
+       */
+      case nir_op_fmul:
+      case nir_op_ffma:
+      case nir_op_fadd:
+      case nir_op_fsub:
+      case nir_op_fmax:
+      case nir_op_fmin:
+      case nir_op_fsat:
+      case nir_op_fneg:
+      case nir_op_fabs:
+      case nir_op_fsign:
+      case nir_op_i2f16:
+      case nir_op_i2f32:
+      case nir_op_u2f16:
+      case nir_op_u2f32:
+      case nir_op_f2f16:
+      case nir_op_f2f16_rtz:
+      case nir_op_f2f16_rtne:
+      case nir_op_f2f32:
+      case nir_op_fquantize2f16:
+      case nir_op_ffract:
+      case nir_op_ffloor:
+      case nir_op_fceil:
+      case nir_op_ftrunc:
+      case nir_op_fround_even:
+      case nir_op_frcp:
+      case nir_op_frsq:
+      case nir_op_fsqrt:
+      case nir_op_fexp2:
+      case nir_op_flog2:
+      case nir_op_fsin_amd:
+      case nir_op_fcos_amd:
+      case nir_op_pack_half_2x16_rtz_split:
+      case nir_op_pack_half_2x16_split:
+      case nir_op_unpack_half_2x16_split_x:
+      case nir_op_unpack_half_2x16_split_y:
+         /*
+          * GFX11.5+ has scalar versions for 32-bit and smaller operands.
+          * For older hardware or 64-bit operands, these must be VGPRs.
+          */
+         if (ctx->program->gfx_level < GFX11_5 || alu_instr->src[0].src.ssa->bit_size > 32) {
+            type = RegType::vgpr;
+            break;
+         }
+         /*
+          * On GFX11.5+ with <=32-bit operands, fall through to default
+          * source-based classification.
+          */
+         FALLTHROUGH;
 
+      default:
+         /*
+          * Default: the result is VGPR if any input is VGPR or divergent (for booleans).
+          * Integer ALU operations (iadd, ishl, iand, etc.) have scalar equivalents,
+          * so they can stay in SGPRs if all sources are uniform.
+          */
+         for (unsigned i = 0; i < nir_op_infos[alu_instr->op].num_inputs; i++) {
+            nir_def* src_ssa = alu_instr->src[i].src.ssa;
+            bool is_vgpr;
+            if (src_ssa->bit_size == 1) {
+               /* 1-bit values use divergence, not regclass. */
+               is_vgpr = nir_src_is_divergent(&alu_instr->src[i].src);
+            } else {
+               is_vgpr = regclasses[src_ssa->index].type() == RegType::vgpr;
+            }
+            if (is_vgpr) {
+               type = RegType::vgpr;
+               break;
+            }
+         }
+         break;
+      }
+   }
+
+   const RegClass rc = get_reg_class(ctx, type, num_components, bitsize);
+   regclasses[alu_instr->def.index] = rc;
+}
+
+} /* anonymous namespace */
+
+/*
+ * Initialize instruction selection context for a shader.
+ *
+ * This function performs:
+ *   1. Divergence analysis
+ *   2. NUW (no-unsigned-wrap) marking on offset computations
+ *   3. Control flow sanitization for ACO requirements
+ *   4. Register class assignment (SGPR vs VGPR) for all SSA values
+ *   5. Constant data setup
+ */
 void
 init_context(isel_context* ctx, nir_shader* shader)
 {
    nir_function_impl* impl = nir_shader_get_entrypoint(shader);
    if (!impl) {
-      /* RT shaders have no NIR entrypoint, but only one function impl exists at this stage */
-      nir_foreach_function_impl (func, shader) {
-         impl = func;
-         break;
-      }
+     /* RT shaders have no NIR entrypoint, but only one function impl exists at this stage */
+     nir_foreach_function_impl (func, shader) {
+       impl = func;
+       break;
+     }
    }
    ctx->shader = shader;
 
+   /* Ensure wave size constraints are satisfied. */
    assert(shader->info.max_subgroup_size >= ctx->program->wave_size);
    assert(shader->info.min_subgroup_size <= ctx->program->wave_size);
    shader->info.max_subgroup_size = ctx->program->wave_size;
    shader->info.min_subgroup_size = ctx->program->wave_size;
 
-   /* Init NIR range analysis. */
+   /* Initialize NIR range analysis hash tables. */
    ctx->range_ht = _mesa_pointer_hash_table_create(NULL);
    ctx->numlsb_ht = _mesa_pointer_hash_table_create(NULL);
 
-   uint32_t options =
+   /* Run divergence analysis with undef-if-phi handling. */
+   const uint32_t options =
       shader->options->divergence_analysis_options | nir_divergence_ignore_undef_if_phi_srcs;
-   nir_divergence_analysis_impl(impl, (nir_divergence_options)options);
+   nir_divergence_analysis_impl(impl, static_cast<nir_divergence_options>(options));
 
-   apply_nuw_to_offsets(ctx, impl);
+   /*
+    * First pass: Apply NUW to offset computations and count calls.
+    * This enables better code generation for address calculations.
+    */
+   unsigned call_count = 0;
+   nir_foreach_block(block, impl) {
+      nir_foreach_instr(instr, block) {
+         if (instr->type == nir_instr_type_call) {
+            ++call_count;
+            continue;
+         }
 
+         if (instr->type != nir_instr_type_intrinsic) {
+            continue;
+         }
+
+         nir_intrinsic_instr* intrin = nir_instr_as_intrinsic(instr);
+         switch (intrin->intrinsic) {
+         case nir_intrinsic_load_constant:
+         case nir_intrinsic_load_uniform:
+         case nir_intrinsic_load_push_constant:
+            if (!nir_src_is_divergent(&intrin->src[0])) {
+               apply_nuw_to_ssa(ctx, intrin->src[0].ssa);
+            }
+            break;
+         case nir_intrinsic_load_ubo:
+         case nir_intrinsic_load_ssbo:
+            if (!nir_src_is_divergent(&intrin->src[1])) {
+               apply_nuw_to_ssa(ctx, intrin->src[1].ssa);
+            }
+            break;
+         case nir_intrinsic_store_ssbo:
+            if (!nir_src_is_divergent(&intrin->src[2])) {
+               apply_nuw_to_ssa(ctx, intrin->src[2].ssa);
+            }
+            break;
+         case nir_intrinsic_load_scratch:
+            apply_nuw_to_ssa(ctx, intrin->src[0].ssa);
+            break;
+         case nir_intrinsic_store_scratch:
+            apply_nuw_to_ssa(ctx, intrin->src[1].ssa);
+            break;
+         case nir_intrinsic_load_global_amd:
+            if (nir_intrinsic_access(intrin) & ACCESS_SMEM_AMD) {
+               apply_nuw_to_ssa(ctx, intrin->src[1].ssa);
+            }
+            break;
+         default:
+            break;
+         }
+      }
+   }
+
+   /* Fragment shader: optimize loads to skip helper invocations when possible. */
    if (shader->info.stage == MESA_SHADER_FRAGMENT) {
       nir_opt_load_skip_helpers_options skip_helper_options = {};
       skip_helper_options.no_add_divergence = true;
@@ -383,14 +691,14 @@ init_context(isel_context* ctx, nir_shad
       nir_opt_load_skip_helpers(shader, &skip_helper_options);
    }
 
-   /* sanitize control flow */
+   /* Sanitize control flow for ACO's requirements. */
    sanitize_cf_list(impl, &impl->body);
    nir_progress(true, impl, nir_metadata_none);
 
-   /* we'll need these for isel */
+   /* We need block indices for instruction selection. */
    nir_metadata_require(impl, nir_metadata_block_index);
 
-   /* Our definition of divergence is slightly different, but we still want nir to print it. */
+   /* Our definition of divergence is slightly different, but we still want NIR to print it. */
    impl->valid_metadata |= nir_metadata_divergence;
 
    if (ctx->options->dump_preoptir) {
@@ -398,152 +706,68 @@ init_context(isel_context* ctx, nir_shad
       nir_print_shader(shader, stderr);
    }
 
+   /* Allocate register class array for all SSA values. */
    ctx->first_temp_id = ctx->program->peekAllocationId();
    ctx->program->allocateRange(impl->ssa_alloc);
-   RegClass* regclasses = ctx->program->temp_rc.data() + ctx->first_temp_id;
+   RegClass* const regclasses = ctx->program->temp_rc.data() + ctx->first_temp_id;
 
-   unsigned call_count = 0;
+   /* Pre-allocate call info storage. */
+   ctx->call_infos.reserve(call_count);
 
-   /* TODO: make this recursive to improve compile times */
+   /*
+    * Register class assignment loop.
+    *
+    * This is an iterative fixed-point algorithm that may require multiple passes
+    * because phi nodes create circular dependencies. A phi's register class depends
+    * on its sources, but sources may not have their classes assigned yet on the
+    * first pass.
+    *
+    * The algorithm converges because:
+    *   1. Register classes only change from SGPR to VGPR, never back
+    *   2. There are finitely many phi nodes
+    *   3. Each iteration processes all instructions
+    *
+    * Typical shaders converge in 1-3 iterations.
+    */
    bool done = false;
    while (!done) {
       done = true;
-      nir_foreach_block (block, impl) {
-         nir_foreach_instr (instr, block) {
+
+      nir_foreach_block(block, impl) {
+         nir_foreach_instr(instr, block) {
             switch (instr->type) {
             case nir_instr_type_alu: {
                nir_alu_instr* alu_instr = nir_instr_as_alu(instr);
-               RegType type = RegType::sgpr;
-
-               /* Packed 16-bit instructions have to be VGPR. */
-               if (alu_instr->def.num_components == 2 &&
-                   ac_nir_op_supports_packed_math_16bit(alu_instr))
-                  type = RegType::vgpr;
-
-               switch (alu_instr->op) {
-               case nir_op_f2i16:
-               case nir_op_f2u16:
-               case nir_op_f2i32:
-               case nir_op_f2u32:
-               case nir_op_mov:
-                  if (alu_instr->def.divergent &&
-                      regclasses[alu_instr->src[0].src.ssa->index].type() == RegType::vgpr)
-                     type = RegType::vgpr;
-                  break;
-               case nir_op_f2e4m3fn:
-               case nir_op_f2e4m3fn_sat:
-               case nir_op_f2e4m3fn_satfn:
-               case nir_op_f2e5m2:
-               case nir_op_f2e5m2_sat:
-               case nir_op_e4m3fn2f:
-               case nir_op_e5m22f:
-               case nir_op_fmulz:
-               case nir_op_ffmaz:
-               case nir_op_f2f64:
-               case nir_op_u2f64:
-               case nir_op_i2f64:
-               case nir_op_pack_unorm_2x16:
-               case nir_op_pack_snorm_2x16:
-               case nir_op_pack_uint_2x16:
-               case nir_op_pack_sint_2x16:
-               case nir_op_ldexp:
-               case nir_op_frexp_sig:
-               case nir_op_frexp_exp:
-               case nir_op_cube_amd:
-               case nir_op_msad_4x8:
-               case nir_op_mqsad_4x8:
-               case nir_op_udot_4x8_uadd:
-               case nir_op_sdot_4x8_iadd:
-               case nir_op_sudot_4x8_iadd:
-               case nir_op_udot_4x8_uadd_sat:
-               case nir_op_sdot_4x8_iadd_sat:
-               case nir_op_sudot_4x8_iadd_sat:
-               case nir_op_udot_2x16_uadd:
-               case nir_op_sdot_2x16_iadd:
-               case nir_op_udot_2x16_uadd_sat:
-               case nir_op_sdot_2x16_iadd_sat:
-               case nir_op_bfdot2_bfadd:
-               case nir_op_byte_perm_amd:
-               case nir_op_alignbyte_amd:
-               case nir_op_f2f16_ru:
-               case nir_op_f2f16_rd: type = RegType::vgpr; break;
-               case nir_op_fmul:
-               case nir_op_ffma:
-               case nir_op_fadd:
-               case nir_op_fsub:
-               case nir_op_fmax:
-               case nir_op_fmin:
-               case nir_op_fsat:
-               case nir_op_fneg:
-               case nir_op_fabs:
-               case nir_op_fsign:
-               case nir_op_i2f16:
-               case nir_op_i2f32:
-               case nir_op_u2f16:
-               case nir_op_u2f32:
-               case nir_op_f2f16:
-               case nir_op_f2f16_rtz:
-               case nir_op_f2f16_rtne:
-               case nir_op_f2f32:
-               case nir_op_fquantize2f16:
-               case nir_op_ffract:
-               case nir_op_ffloor:
-               case nir_op_fceil:
-               case nir_op_ftrunc:
-               case nir_op_fround_even:
-               case nir_op_frcp:
-               case nir_op_frsq:
-               case nir_op_fsqrt:
-               case nir_op_fexp2:
-               case nir_op_flog2:
-               case nir_op_fsin_amd:
-               case nir_op_fcos_amd:
-               case nir_op_pack_half_2x16_rtz_split:
-               case nir_op_pack_half_2x16_split:
-               case nir_op_unpack_half_2x16_split_x:
-               case nir_op_unpack_half_2x16_split_y: {
-                  if (ctx->program->gfx_level < GFX11_5 ||
-                      alu_instr->src[0].src.ssa->bit_size > 32) {
-                     type = RegType::vgpr;
-                     break;
-                  }
-                  FALLTHROUGH;
-               }
-               default:
-                  for (unsigned i = 0; i < nir_op_infos[alu_instr->op].num_inputs; i++) {
-                     if (alu_instr->src[i].src.ssa->bit_size == 1
-                            ? nir_src_is_divergent(&alu_instr->src[i].src)
-                            : regclasses[alu_instr->src[i].src.ssa->index].type() == RegType::vgpr)
-                        type = RegType::vgpr;
-                  }
-                  break;
-               }
-
-               RegClass rc =
-                  get_reg_class(ctx, type, alu_instr->def.num_components, alu_instr->def.bit_size);
-               regclasses[alu_instr->def.index] = rc;
+               assign_alu_regclass(ctx, alu_instr, regclasses);
                break;
             }
+
             case nir_instr_type_load_const: {
-               unsigned num_components = nir_instr_as_load_const(instr)->def.num_components;
-               unsigned bit_size = nir_instr_as_load_const(instr)->def.bit_size;
-               RegClass rc = get_reg_class(ctx, RegType::sgpr, num_components, bit_size);
-               regclasses[nir_instr_as_load_const(instr)->def.index] = rc;
+               nir_load_const_instr* lc = nir_instr_as_load_const(instr);
+               RegClass rc = get_reg_class(ctx, RegType::sgpr, lc->def.num_components,
+                                           lc->def.bit_size);
+               regclasses[lc->def.index] = rc;
                break;
             }
+
             case nir_instr_type_intrinsic: {
                nir_intrinsic_instr* intrinsic = nir_instr_as_intrinsic(instr);
-               if (!nir_intrinsic_infos[intrinsic->intrinsic].has_dest)
+               if (!nir_intrinsic_infos[intrinsic->intrinsic].has_dest) {
                   break;
+               }
+
+               /* Special case: strict WQM coordinates need linear VGPR. */
                if (intrinsic->intrinsic == nir_intrinsic_strict_wqm_coord_amd) {
                   regclasses[intrinsic->def.index] =
-                     RegClass::get(RegType::vgpr, intrinsic->def.num_components * 4 +
-                                                     nir_intrinsic_base(intrinsic))
+                     RegClass::get(RegType::vgpr,
+                                   intrinsic->def.num_components * 4 + nir_intrinsic_base(intrinsic))
                         .as_linear();
                   break;
                }
+
                RegType type = RegType::sgpr;
                switch (intrinsic->intrinsic) {
+               /* Intrinsics that always produce uniform (SGPR) results. */
                case nir_intrinsic_load_push_constant:
                case nir_intrinsic_load_workgroup_id:
                case nir_intrinsic_load_num_workgroups:
@@ -559,7 +783,11 @@ init_context(isel_context* ctx, nir_shad
                case nir_intrinsic_ballot_relaxed:
                case nir_intrinsic_bindless_image_samples:
                case nir_intrinsic_load_scalar_arg_amd:
-               case nir_intrinsic_unit_test_uniform_input: type = RegType::sgpr; break;
+               case nir_intrinsic_unit_test_uniform_input:
+                  type = RegType::sgpr;
+                  break;
+
+               /* Intrinsics that always produce divergent (VGPR) results. */
                case nir_intrinsic_load_input:
                case nir_intrinsic_load_per_primitive_input:
                case nir_intrinsic_load_output:
@@ -590,18 +818,23 @@ init_context(isel_context* ctx, nir_shad
                case nir_intrinsic_load_vector_arg_amd:
                case nir_intrinsic_ordered_xfb_counter_add_gfx11_amd:
                case nir_intrinsic_cmat_muladd_amd:
-               case nir_intrinsic_unit_test_divergent_input: type = RegType::vgpr; break;
+               case nir_intrinsic_unit_test_divergent_input:
+                  type = RegType::vgpr;
+                  break;
+
+               /*
+                * Shared memory loads: when only used by cross-lane instructions,
+                * prefer VGPR to move s_waitcnt closer to use, hiding LDS latency.
+                */
                case nir_intrinsic_load_shared:
                case nir_intrinsic_load_shared2_amd:
-                  /* When the result of these loads is only used by cross-lane instructions,
-                   * it is beneficial to use a VGPR destination. This is because this allows
-                   * to put the s_waitcnt further down, which decreases latency.
-                   */
                   if (only_used_by_cross_lane_instrs(&intrinsic->def)) {
                      type = RegType::vgpr;
                      break;
                   }
                   FALLTHROUGH;
+
+               /* Intrinsics where result type depends on divergence. */
                case nir_intrinsic_shuffle:
                case nir_intrinsic_quad_broadcast:
                case nir_intrinsic_quad_swap_horizontal:
@@ -618,6 +851,8 @@ init_context(isel_context* ctx, nir_shad
                case nir_intrinsic_load_global_amd:
                   type = intrinsic->def.divergent ? RegType::vgpr : RegType::sgpr;
                   break;
+
+               /* Derivative intrinsics always need VGPRs. */
                case nir_intrinsic_ddx:
                case nir_intrinsic_ddy:
                case nir_intrinsic_ddx_fine:
@@ -625,105 +860,118 @@ init_context(isel_context* ctx, nir_shad
                case nir_intrinsic_ddx_coarse:
                case nir_intrinsic_ddy_coarse:
                case nir_intrinsic_load_return_param_amd: {
-                  type = RegType::vgpr;
-                  break;
+                 type = RegType::vgpr;
+                 break;
                }
                case nir_intrinsic_load_param: {
-                  nir_parameter* param =
-                     &impl->function->params[nir_intrinsic_param_idx(intrinsic)];
-                  type = param->is_uniform ? RegType::sgpr : RegType::vgpr;
-                  break;
+                 nir_parameter* param =
+                 &impl->function->params[nir_intrinsic_param_idx(intrinsic)];
+                 type = param->is_uniform ? RegType::sgpr : RegType::vgpr;
+                 break;
                }
                default:
+                  /* Default: VGPR if any source is VGPR. */
                   for (unsigned i = 0; i < nir_intrinsic_infos[intrinsic->intrinsic].num_srcs;
                        i++) {
-                     if (regclasses[intrinsic->src[i].ssa->index].type() == RegType::vgpr)
+                     if (regclasses[intrinsic->src[i].ssa->index].type() == RegType::vgpr) {
                         type = RegType::vgpr;
+                        break;
+                     }
                   }
                   break;
                }
-               RegClass rc =
-                  get_reg_class(ctx, type, intrinsic->def.num_components, intrinsic->def.bit_size);
+
+               RegClass rc = get_reg_class(ctx, type, intrinsic->def.num_components,
+                                           intrinsic->def.bit_size);
                regclasses[intrinsic->def.index] = rc;
                break;
             }
+
             case nir_instr_type_tex: {
                nir_tex_instr* tex = nir_instr_as_tex(instr);
                RegType type =
-                  tex->def.divergent || tex->skip_helpers ? RegType::vgpr : RegType::sgpr;
-
-               RegClass rc = get_reg_class(ctx, type, tex->def.num_components, tex->def.bit_size);
+                  (tex->def.divergent || tex->skip_helpers) ? RegType::vgpr : RegType::sgpr;
+               RegClass rc =
+                  get_reg_class(ctx, type, tex->def.num_components, tex->def.bit_size);
                regclasses[tex->def.index] = rc;
                break;
             }
+
             case nir_instr_type_undef: {
-               unsigned num_components = nir_instr_as_undef(instr)->def.num_components;
-               unsigned bit_size = nir_instr_as_undef(instr)->def.bit_size;
-               RegClass rc = get_reg_class(ctx, RegType::sgpr, num_components, bit_size);
-               regclasses[nir_instr_as_undef(instr)->def.index] = rc;
+               nir_undef_instr* und = nir_instr_as_undef(instr);
+               RegClass rc =
+                  get_reg_class(ctx, RegType::sgpr, und->def.num_components, und->def.bit_size);
+               regclasses[und->def.index] = rc;
                break;
             }
+
             case nir_instr_type_phi: {
                nir_phi_instr* phi = nir_instr_as_phi(instr);
                RegType type = RegType::sgpr;
                unsigned num_components = phi->def.num_components;
+
                assert((phi->def.bit_size != 1 || num_components == 1) &&
                       "Multiple components not supported on boolean phis.");
 
                if (phi->def.divergent) {
                   type = RegType::vgpr;
                } else {
-                  bool vgpr_src = false;
-                  nir_foreach_phi_src (src, phi)
-                     vgpr_src |= regclasses[src->src.ssa->index].type() == RegType::vgpr;
-
-                  if (vgpr_src) {
-                     type = RegType::vgpr;
-
-                     /* This might be the case because of nir_divergence_ignore_undef_if_phi_srcs. */
-                     bool divergent_merge = false;
-                     if (nir_cf_node_prev(&block->cf_node) &&
-                         nir_cf_node_prev(&block->cf_node)->type == nir_cf_node_if) {
-                        nir_if* nif = nir_cf_node_as_if(nir_cf_node_prev(&block->cf_node));
-                        divergent_merge = nir_src_is_divergent(&nif->condition);
+                  /* Check if any phi source is VGPR. */
+                  nir_foreach_phi_src(src, phi) {
+                     if (regclasses[src->src.ssa->index].type() == RegType::vgpr) {
+                        type = RegType::vgpr;
+                        break;
                      }
+                  }
 
-                     /* In case of uniform phis after divergent merges, ensure that the dst is an
-                      * SGPR and does not contain undefined values for some invocations.
+                  if (type == RegType::vgpr) {
+                     /*
+                      * This might be the case because of nir_divergence_ignore_undef_if_phi_srcs.
+                      * In case of uniform phis after divergent merges, ensure that the dst is
+                      * an SGPR and does not contain undefined values for some invocations.
                       */
-                     if (divergent_merge && !skip_uniformize_merge_phi(&phi->def, 0))
+                     nir_cf_node* prev_cf = nir_cf_node_prev(&block->cf_node);
+                     bool divergent_merge = prev_cf && prev_cf->type == nir_cf_node_if &&
+                                            nir_src_is_divergent(&nir_cf_node_as_if(prev_cf)->condition);
+
+                     if (divergent_merge && !skip_uniformize_merge_phi(&phi->def, 0)) {
                         type = RegType::sgpr;
+                     }
                   }
                }
 
                RegClass rc = get_reg_class(ctx, type, num_components, phi->def.bit_size);
-               if (rc != regclasses[phi->def.index])
+               if (rc != regclasses[phi->def.index]) {
                   done = false;
+               }
                regclasses[phi->def.index] = rc;
                break;
             }
-            case nir_instr_type_call: {
-               ++call_count;
+
+            case nir_instr_type_call:
+               /* Calls are counted but don't have defs to classify. */
+               break;
+
+            default:
                break;
-            }
-            default: break;
             }
          }
       }
    }
 
-   ctx->call_infos.reserve(call_count);
-
+   /* Copy pixel shader input configuration. */
    ctx->program->config->spi_ps_input_ena = ctx->program->info.ps.spi_ps_input_ena;
    ctx->program->config->spi_ps_input_addr = ctx->program->info.ps.spi_ps_input_addr;
 
-   /* align and copy constant data */
-   while (ctx->program->constant_data.size() % 4u)
+   /* Align constant data to 4 bytes and append shader's constant data. */
+   while (ctx->program->constant_data.size() % 4u != 0) {
       ctx->program->constant_data.push_back(0);
+   }
    ctx->constant_data_offset = ctx->program->constant_data.size();
    ctx->program->constant_data.insert(ctx->program->constant_data.end(),
-                                      (uint8_t*)shader->constant_data,
-                                      (uint8_t*)shader->constant_data + shader->constant_data_size);
+                                      static_cast<const uint8_t*>(shader->constant_data),
+                                      static_cast<const uint8_t*>(shader->constant_data) +
+                                         shader->constant_data_size);
 
    BITSET_ZERO(ctx->output_args);
 }
@@ -743,22 +991,41 @@ setup_isel_context(Program* program, uns
 {
    for (unsigned i = 0; i < shader_count; i++) {
       switch (shaders[i]->info.stage) {
-      case MESA_SHADER_VERTEX: sw_stage = sw_stage | SWStage::VS; break;
-      case MESA_SHADER_TESS_CTRL: sw_stage = sw_stage | SWStage::TCS; break;
-      case MESA_SHADER_TESS_EVAL: sw_stage = sw_stage | SWStage::TES; break;
-      case MESA_SHADER_GEOMETRY: sw_stage = sw_stage | SWStage::GS; break;
-      case MESA_SHADER_FRAGMENT: sw_stage = sw_stage | SWStage::FS; break;
+      case MESA_SHADER_VERTEX:
+         sw_stage = sw_stage | SWStage::VS;
+         break;
+      case MESA_SHADER_TESS_CTRL:
+         sw_stage = sw_stage | SWStage::TCS;
+         break;
+      case MESA_SHADER_TESS_EVAL:
+         sw_stage = sw_stage | SWStage::TES;
+         break;
+      case MESA_SHADER_GEOMETRY:
+         sw_stage = sw_stage | SWStage::GS;
+         break;
+      case MESA_SHADER_FRAGMENT:
+         sw_stage = sw_stage | SWStage::FS;
+         break;
       case MESA_SHADER_KERNEL:
-      case MESA_SHADER_COMPUTE: sw_stage = sw_stage | SWStage::CS; break;
-      case MESA_SHADER_TASK: sw_stage = sw_stage | SWStage::TS; break;
-      case MESA_SHADER_MESH: sw_stage = sw_stage | SWStage::MS; break;
+      case MESA_SHADER_COMPUTE:
+         sw_stage = sw_stage | SWStage::CS;
+         break;
+      case MESA_SHADER_TASK:
+         sw_stage = sw_stage | SWStage::TS;
+         break;
+      case MESA_SHADER_MESH:
+         sw_stage = sw_stage | SWStage::MS;
+         break;
       case MESA_SHADER_RAYGEN:
       case MESA_SHADER_CLOSEST_HIT:
       case MESA_SHADER_MISS:
       case MESA_SHADER_CALLABLE:
       case MESA_SHADER_INTERSECTION:
-      case MESA_SHADER_ANY_HIT: sw_stage = SWStage::RT; break;
-      default: UNREACHABLE("Shader stage not implemented");
+      case MESA_SHADER_ANY_HIT:
+         sw_stage = SWStage::RT;
+         break;
+      default:
+         UNREACHABLE("Shader stage not implemented");
       }
    }
 
@@ -781,28 +1048,31 @@ setup_isel_context(Program* program, uns
 
    calc_min_waves(program);
 
-   unsigned scratch_size = 0;
-   for (unsigned i = 0; i < shader_count; i++)
+   for (unsigned i = 0; i < shader_count; i++) {
       setup_nir(&ctx, shaders[i]);
+   }
 
-   for (unsigned i = 0; i < shader_count; i++)
+   unsigned scratch_size = 0;
+   for (unsigned i = 0; i < shader_count; i++) {
       scratch_size = std::max(scratch_size, shaders[i]->scratch_size);
+   }
 
-   ctx.program->config->scratch_bytes_per_wave = align(scratch_size, 4) * ctx.program->wave_size;
+   ctx.program->config->scratch_bytes_per_wave =
+      align(scratch_size, 4u) * ctx.program->wave_size;
    ctx.program->config->lds_size = program->info.lds_size;
    assert(ctx.program->config->lds_size <= ctx.program->dev.lds_limit);
 
    unsigned nir_num_blocks = 0;
    for (unsigned i = 0; i < shader_count; i++) {
-      nir_function_impl* entrypoint = nir_shader_get_entrypoint(shaders[i]);
-      if (!entrypoint) {
-         /* RT shaders have no NIR entrypoint, but only one function impl exists at this stage */
-         nir_foreach_function_impl (func, shaders[i]) {
-            entrypoint = func;
-            break;
-         }
-      }
-      nir_num_blocks += entrypoint->num_blocks;
+     nir_function_impl* entrypoint = nir_shader_get_entrypoint(shaders[i]);
+     if (!entrypoint) {
+       /* RT shaders have no NIR entrypoint, but only one function impl exists at this stage */
+       nir_foreach_function_impl (func, shaders[i]) {
+         entrypoint = func;
+         break;
+       }
+     }
+     nir_num_blocks += entrypoint->num_blocks;
    }
    ctx.program->blocks.reserve(nir_num_blocks * 2);
    ctx.block = ctx.program->create_and_insert_block();
@@ -811,4 +1081,4 @@ setup_isel_context(Program* program, uns
    return ctx;
 }
 
-} // namespace aco
+} /* namespace aco */
