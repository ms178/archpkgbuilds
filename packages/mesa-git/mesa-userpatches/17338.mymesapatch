From dbb5998e7647f46a0c20061560a4c96446b1df00 Mon Sep 17 00:00:00 2001
From: Jonathan <jonathan.strobl@gmx.de>
Date: Fri, 1 Jul 2022 19:19:02 +0200
Subject: [PATCH] gallium/u_threaded: buffer sharedness tracking

TC is unable to determine if other TC instances are currently using a
buffer, which meant that TC would accidentally overwrite buffers that were
still being referenced by an unflushed batch in another TC.

Additionally, TC is unable to notify other TCs in case it replaces a
shared buffer's underlying storage, causing those other TCs to use an
outdated, possibly even freed buffer if they had the buffer bound during
a replace operation.

This patch addresses both of these issues by tracking if a buffer is
shared, and if it is, disabling all of the clever re-use / invalidation
tricks that TC does to improve performance (for the shared buffer only).
This is achieved by tracking which TC instance first accessed a certain
buffer. If a second instance then accesses it as well, it will realize
that it isn't the only one working on the buffer and mark the buffer
accordingly.

It might be possible to later detect that a buffer has become un-shared;
however, this is outside of the scope of this bugfix patch.

Fixes: e9c41b32148 ("gallium/u_threaded: add buffer lists - tracking of buffers referenced by tc")
---
 .../auxiliary/util/u_threaded_context.c       | 107 ++++++++++++------
 .../auxiliary/util/u_threaded_context.h       |  21 +++-
 2 files changed, 94 insertions(+), 34 deletions(-)

diff --git a/src/gallium/auxiliary/util/u_threaded_context.c b/src/gallium/auxiliary/util/u_threaded_context.c
index b532749e194a..00948fea4b13 100644
--- a/src/gallium/auxiliary/util/u_threaded_context.c
+++ b/src/gallium/auxiliary/util/u_threaded_context.c
@@ -406,18 +406,39 @@ threaded_context_flush(struct pipe_context *_pipe,
    }
 }
 
+/* Must be called before TC binds, maps, invalidates, or adds a buffer to a buffer list. */
+static void tc_touch_buffer(struct threaded_context *tc, struct threaded_resource *buf) {
+   const struct threaded_context *first_user = buf->first_user;
+
+   if (!first_user)
+      first_user = p_atomic_cmpxchg(&buf->first_user, NULL, tc);
+
+   if ((first_user != NULL) && (first_user != tc) && !buf->used_by_multiple_contexts)
+      buf->used_by_multiple_contexts = true;
+}
+
+static bool tc_is_buffer_shared(struct threaded_resource *buf) {
+   return buf->is_shared || buf->used_by_multiple_contexts;
+}
+
 static void
-tc_add_to_buffer_list(struct tc_buffer_list *next, struct pipe_resource *buf)
+tc_add_to_buffer_list(struct threaded_context *tc, struct tc_buffer_list *next, struct pipe_resource *buf)
 {
-   uint32_t id = threaded_resource(buf)->buffer_id_unique;
+   struct threaded_resource *tbuf = threaded_resource(buf);
+   tc_touch_buffer(tc, tbuf);
+
+   uint32_t id = tbuf->buffer_id_unique;
    BITSET_SET(next->buffer_list, id & TC_BUFFER_ID_MASK);
 }
 
 /* Set a buffer binding and add it to the buffer list. */
 static void
-tc_bind_buffer(uint32_t *binding, struct tc_buffer_list *next, struct pipe_resource *buf)
+tc_bind_buffer(struct threaded_context *tc, uint32_t *binding, struct tc_buffer_list *next, struct pipe_resource *buf)
 {
-   uint32_t id = threaded_resource(buf)->buffer_id_unique;
+   struct threaded_resource *tbuf = threaded_resource(buf);
+   tc_touch_buffer(tc, tbuf);
+
+   uint32_t id = tbuf->buffer_id_unique;
    *binding = id;
    BITSET_SET(next->buffer_list, id & TC_BUFFER_ID_MASK);
 }
@@ -647,7 +668,8 @@ static bool
 tc_is_buffer_busy(struct threaded_context *tc, struct threaded_resource *tbuf,
                   unsigned map_usage)
 {
-   if (!tc->options.is_resource_busy)
+   /* Buffer busyness tracking doesn't work for shared buffers. */
+   if (!tc->options.is_resource_busy || tc_is_buffer_shared(tbuf))
       return true;
 
    uint32_t id_hash = tbuf->buffer_id_unique & TC_BUFFER_ID_MASK;
@@ -675,6 +697,8 @@ threaded_resource_init(struct pipe_resource *res, bool allow_cpu_storage)
 {
    struct threaded_resource *tres = threaded_resource(res);
 
+   tres->first_user = NULL;
+   tres->used_by_multiple_contexts = false;
    tres->latest = &tres->b;
    tres->cpu_storage = NULL;
    util_range_init(&tres->valid_buffer_range);
@@ -933,7 +957,7 @@ tc_get_query_result_resource(struct pipe_context *_pipe,
    p->result_type = result_type;
    p->index = index;
    tc_set_resource_reference(&p->resource, resource);
-   tc_add_to_buffer_list(&tc->buffer_lists[tc->next_buf_list], resource);
+   tc_add_to_buffer_list(tc, &tc->buffer_lists[tc->next_buf_list], resource);
    p->offset = offset;
 }
 
@@ -1225,7 +1249,7 @@ tc_set_constant_buffer(struct pipe_context *_pipe,
       tc_set_resource_reference(&p->cb.buffer, buffer);
 
    if (buffer) {
-      tc_bind_buffer(&tc->const_buffers[shader][index],
+      tc_bind_buffer(tc, &tc->const_buffers[shader][index],
                      &tc->buffer_lists[tc->next_buf_list], buffer);
    } else {
       tc_unbind_buffer(&tc->const_buffers[shader][index]);
@@ -1425,7 +1449,7 @@ tc_set_sampler_views(struct pipe_context *_pipe,
 
          for (unsigned i = 0; i < count; i++) {
             if (views[i] && views[i]->target == PIPE_BUFFER) {
-               tc_bind_buffer(&tc->sampler_buffers[shader][start + i], next,
+               tc_bind_buffer(tc, &tc->sampler_buffers[shader][start + i], next,
                               views[i]->texture);
             } else {
                tc_unbind_buffer(&tc->sampler_buffers[shader][start + i]);
@@ -1437,7 +1461,7 @@ tc_set_sampler_views(struct pipe_context *_pipe,
             pipe_sampler_view_reference(&p->slot[i], views[i]);
 
             if (views[i] && views[i]->target == PIPE_BUFFER) {
-               tc_bind_buffer(&tc->sampler_buffers[shader][start + i], next,
+               tc_bind_buffer(tc, &tc->sampler_buffers[shader][start + i], next,
                               views[i]->texture);
             } else {
                tc_unbind_buffer(&tc->sampler_buffers[shader][start + i]);
@@ -1516,7 +1540,7 @@ tc_set_shader_images(struct pipe_context *_pipe,
          tc_set_resource_reference(&p->slot[i].resource, resource);
 
          if (resource && resource->target == PIPE_BUFFER) {
-            tc_bind_buffer(&tc->image_buffers[shader][start + i], next, resource);
+            tc_bind_buffer(tc, &tc->image_buffers[shader][start + i], next, resource);
 
             if (images[i].access & PIPE_IMAGE_ACCESS_WRITE) {
                struct threaded_resource *tres = threaded_resource(resource);
@@ -1611,7 +1635,7 @@ tc_set_shader_buffers(struct pipe_context *_pipe,
          if (src->buffer) {
             struct threaded_resource *tres = threaded_resource(src->buffer);
 
-            tc_bind_buffer(&tc->shader_buffers[shader][start + i], next, &tres->b);
+            tc_bind_buffer(tc, &tc->shader_buffers[shader][start + i], next, &tres->b);
 
             if (writable_bitmask & BITFIELD_BIT(i)) {
                tc_buffer_disable_cpu_storage(src->buffer);
@@ -1687,7 +1711,7 @@ tc_set_vertex_buffers(struct pipe_context *_pipe,
             struct pipe_resource *buf = buffers[i].buffer.resource;
 
             if (buf) {
-               tc_bind_buffer(&tc->vertex_buffers[start + i], next, buf);
+               tc_bind_buffer(tc, &tc->vertex_buffers[start + i], next, buf);
             } else {
                tc_unbind_buffer(&tc->vertex_buffers[start + i]);
             }
@@ -1705,7 +1729,7 @@ tc_set_vertex_buffers(struct pipe_context *_pipe,
             dst->buffer_offset = src->buffer_offset;
 
             if (buf) {
-               tc_bind_buffer(&tc->vertex_buffers[start + i], next, buf);
+               tc_bind_buffer(tc, &tc->vertex_buffers[start + i], next, buf);
             } else {
                tc_unbind_buffer(&tc->vertex_buffers[start + i]);
             }
@@ -1762,7 +1786,7 @@ tc_set_stream_output_targets(struct pipe_context *_pipe,
       pipe_so_target_reference(&p->targets[i], tgs[i]);
       if (tgs[i]) {
          tc_buffer_disable_cpu_storage(tgs[i]->buffer);
-         tc_bind_buffer(&tc->streamout_buffers[i], next, tgs[i]->buffer);
+         tc_bind_buffer(tc, &tc->streamout_buffers[i], next, tgs[i]->buffer);
       } else {
          tc_unbind_buffer(&tc->streamout_buffers[i]);
       }
@@ -2001,6 +2025,8 @@ static bool
 tc_invalidate_buffer(struct threaded_context *tc,
                      struct threaded_resource *tbuf)
 {
+   tc_touch_buffer(tc, tbuf);
+
    if (!tc_is_buffer_busy(tc, tbuf, PIPE_MAP_READ_WRITE)) {
       /* It's idle, so invalidation would be a no-op, but we can still clear
        * the valid range because we are technically doing invalidation, but
@@ -2017,7 +2043,7 @@ tc_invalidate_buffer(struct threaded_context *tc,
    struct pipe_resource *new_buf;
 
    /* Shared, pinned, and sparse buffers can't be reallocated. */
-   if (tbuf->is_shared ||
+   if (tc_is_buffer_shared(tbuf) ||
        tbuf->is_user_ptr ||
        tbuf->b.flags & (PIPE_RESOURCE_FLAG_SPARSE | PIPE_RESOURCE_FLAG_UNMAPPABLE))
       return false;
@@ -2120,7 +2146,7 @@ tc_improve_map_buffer_flags(struct threaded_context *tc,
    /* See if the buffer range being mapped has never been initialized or
     * the buffer is idle, in which case it can be mapped unsynchronized. */
    if (!(usage & PIPE_MAP_UNSYNCHRONIZED) &&
-       ((!tres->is_shared &&
+       ((!tc_is_buffer_shared(tres) &&
          !util_ranges_intersect(&tres->valid_buffer_range, offset, offset + size)) ||
         !tc_is_buffer_busy(tc, tres, usage)))
       usage |= PIPE_MAP_UNSYNCHRONIZED;
@@ -2176,6 +2202,8 @@ tc_buffer_map(struct pipe_context *_pipe,
    if (usage & PIPE_MAP_THREAD_SAFE)
       tc_buffer_disable_cpu_storage(resource);
 
+   tc_touch_buffer(tc, tres);
+
    usage = tc_improve_map_buffer_flags(tc, tres, usage, box->x, box->width);
 
    /* If the CPU storage is enabled, return it directly. */
@@ -2203,8 +2231,9 @@ tc_buffer_map(struct pipe_context *_pipe,
 
    /* Do a staging transfer within the threaded context. The driver should
     * only get resource_copy_region.
+    * Staging transfers can't be used for CPU storage uploads.
     */
-   if (usage & PIPE_MAP_DISCARD_RANGE) {
+   if ((usage & PIPE_MAP_DISCARD_RANGE) && !(usage & TC_TRANSFER_MAP_UPLOAD_CPU_STORAGE)) {
       struct threaded_transfer *ttrans = slab_zalloc(&tc->pool_transfers);
       uint8_t *map;
 
@@ -2450,11 +2479,21 @@ tc_buffer_unmap(struct pipe_context *_pipe, struct pipe_transfer *transfer)
       assert(tres->cpu_storage);
 
       if (tres->cpu_storage) {
-         tc_invalidate_buffer(tc, tres);
-         tc_buffer_subdata(&tc->base, &tres->b,
-                           PIPE_MAP_UNSYNCHRONIZED |
-                           TC_TRANSFER_MAP_UPLOAD_CPU_STORAGE,
-                           0, tres->b.width0, tres->cpu_storage);
+         /* Invalidation can fail for shared buffers, in which case we need to sync all
+          * the way down to the driver. Since we know that invalidation failed already,
+          * we can also forbid tc_buffer_subdata / tc_buffer_map to try improving the
+          * flags by invalidating again (it would just fail again).
+          */
+         bool invalidated = tc_invalidate_buffer(tc, tres);
+
+         unsigned storage_upload_usage = TC_TRANSFER_MAP_UPLOAD_CPU_STORAGE;
+         if (invalidated)
+            storage_upload_usage |= PIPE_MAP_UNSYNCHRONIZED;
+         else
+            storage_upload_usage |= TC_TRANSFER_MAP_NO_INVALIDATE | TC_TRANSFER_MAP_NO_INFER_UNSYNCHRONIZED;
+
+         tc_buffer_subdata(&tc->base, &tres->b, storage_upload_usage, 0, tres->b.width0, tres->cpu_storage);
+
          /* This shouldn't have been freed by buffer_subdata. */
          assert(tres->cpu_storage);
       } else {
@@ -2564,6 +2603,8 @@ tc_buffer_subdata(struct pipe_context *_pipe,
    if (!size)
       return;
 
+   tc_touch_buffer(tc, tres);
+
    usage |= PIPE_MAP_WRITE;
 
    /* PIPE_MAP_DIRECTLY supresses implicit DISCARD_RANGE. */
@@ -2603,7 +2644,7 @@ tc_buffer_subdata(struct pipe_context *_pipe,
    /* This is will always be busy because if it wasn't, tc_improve_map_buffer-
     * _flags would set UNSYNCHRONIZED and we wouldn't get here.
     */
-   tc_add_to_buffer_list(&tc->buffer_lists[tc->next_buf_list], resource);
+   tc_add_to_buffer_list(tc, &tc->buffer_lists[tc->next_buf_list], resource);
    p->usage = usage;
    p->offset = offset;
    p->size = size;
@@ -3246,7 +3287,7 @@ tc_draw_vbo(struct pipe_context *_pipe, const struct pipe_draw_info *info,
             tc_set_resource_reference(&p->info.index.resource,
                                       info->index.resource);
          }
-         tc_add_to_buffer_list(next, info->index.resource);
+         tc_add_to_buffer_list(tc, next, info->index.resource);
       }
       memcpy(&p->info, info, DRAW_INFO_SIZE_WITHOUT_MIN_MAX_INDEX);
 
@@ -3258,11 +3299,11 @@ tc_draw_vbo(struct pipe_context *_pipe, const struct pipe_draw_info *info,
                                indirect->count_from_stream_output);
 
       if (indirect->buffer)
-         tc_add_to_buffer_list(next, indirect->buffer);
+         tc_add_to_buffer_list(tc, next, indirect->buffer);
       if (indirect->indirect_draw_count)
-         tc_add_to_buffer_list(next, indirect->indirect_draw_count);
+         tc_add_to_buffer_list(tc, next, indirect->indirect_draw_count);
       if (indirect->count_from_stream_output)
-         tc_add_to_buffer_list(next, indirect->count_from_stream_output->buffer);
+         tc_add_to_buffer_list(tc, next, indirect->count_from_stream_output->buffer);
 
       memcpy(&p->indirect, indirect, sizeof(*indirect));
       p->draw.start = draws[0].start;
@@ -3314,7 +3355,7 @@ tc_draw_vbo(struct pipe_context *_pipe, const struct pipe_draw_info *info,
                tc_set_resource_reference(&p->info.index.resource,
                                          info->index.resource);
             }
-            tc_add_to_buffer_list(&tc->buffer_lists[tc->next_buf_list], info->index.resource);
+            tc_add_to_buffer_list(tc, &tc->buffer_lists[tc->next_buf_list], info->index.resource);
          }
          if (drawid_offset > 0)
             ((struct tc_draw_single_drawid*)p)->drawid_offset = drawid_offset;
@@ -3429,7 +3470,7 @@ tc_draw_vbo(struct pipe_context *_pipe, const struct pipe_draw_info *info,
                tc_set_resource_reference(&p->info.index.resource,
                                          info->index.resource);
             }
-            tc_add_to_buffer_list(&tc->buffer_lists[tc->next_buf_list], info->index.resource);
+            tc_add_to_buffer_list(tc, &tc->buffer_lists[tc->next_buf_list], info->index.resource);
          }
          take_index_buffer_ownership = false;
          memcpy(&p->info, info, DRAW_INFO_SIZE_WITHOUT_MIN_MAX_INDEX);
@@ -3641,7 +3682,7 @@ tc_launch_grid(struct pipe_context *_pipe,
    memcpy(&p->info, info, sizeof(*info));
 
    if (info->indirect)
-      tc_add_to_buffer_list(&tc->buffer_lists[tc->next_buf_list], info->indirect);
+      tc_add_to_buffer_list(tc, &tc->buffer_lists[tc->next_buf_list], info->indirect);
 
    /* This must be after tc_add_*call, which can flush the batch. */
    if (unlikely(tc->add_all_compute_bindings_to_buffer_list))
@@ -3688,8 +3729,8 @@ tc_resource_copy_region(struct pipe_context *_pipe,
    if (dst->target == PIPE_BUFFER) {
       struct tc_buffer_list *next = &tc->buffer_lists[tc->next_buf_list];
 
-      tc_add_to_buffer_list(next, src);
-      tc_add_to_buffer_list(next, dst);
+      tc_add_to_buffer_list(tc, next, src);
+      tc_add_to_buffer_list(tc, next, dst);
 
       util_range_add(&tdst->b, &tdst->valid_buffer_range,
                      dstx, dstx + src_box->width);
@@ -3993,7 +4034,7 @@ tc_clear_buffer(struct pipe_context *_pipe, struct pipe_resource *res,
    tc_buffer_disable_cpu_storage(res);
 
    tc_set_resource_reference(&p->res, res);
-   tc_add_to_buffer_list(&tc->buffer_lists[tc->next_buf_list], res);
+   tc_add_to_buffer_list(tc, &tc->buffer_lists[tc->next_buf_list], res);
    p->offset = offset;
    p->size = size;
    memcpy(p->clear_value, clear_value, clear_value_size);
diff --git a/src/gallium/auxiliary/util/u_threaded_context.h b/src/gallium/auxiliary/util/u_threaded_context.h
index 848c56abb896..7bb495d7c6f7 100644
--- a/src/gallium/auxiliary/util/u_threaded_context.h
+++ b/src/gallium/auxiliary/util/u_threaded_context.h
@@ -315,6 +315,24 @@ typedef bool (*tc_is_resource_busy)(struct pipe_screen *screen,
 struct threaded_resource {
    struct pipe_resource b;
 
+   /* Pointer to the TC that first used this threaded_resource (buffer). This is used to
+    * allow TCs to determine whether they have been given a buffer that was created by a
+    * different TC, in which case all TCs have to disable busyness tracking and buffer
+    * replacement for that particular buffer.
+    * DO NOT DEREFERENCE. The only operation allowed on this pointer is equality-checking
+    * since it might be dangling if a buffer has been shared and its first_user has
+    * already been destroyed. The pointer is const to discourage such disallowed usage.
+    * This is NULL if no TC has used this buffer yet.
+    */
+   const struct threaded_context *first_user;
+
+   /*
+    * True if multiple threaded contexts have accessed this buffer.
+    * Disables non-multicontext-safe optimizations in TC.
+    * We can't just re-use is_shared for that purpose as that would confuse drivers.
+    */
+   bool used_by_multiple_contexts;
+
    /* Since buffer invalidations are queued, we can't use the base resource
     * for unsychronized mappings. This points to the latest version of
     * the buffer after the latest invalidation. It's only used for unsychro-
@@ -343,7 +361,8 @@ struct threaded_resource {
    struct util_range valid_buffer_range;
 
    /* Drivers are required to update this for shared resources and user
-    * pointers. */
+    * pointers.
+    */
    bool is_shared;
    bool is_user_ptr;
    bool allow_cpu_storage;
-- 
GitLab

