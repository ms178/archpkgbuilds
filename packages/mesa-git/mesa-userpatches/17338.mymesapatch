From 6a38480c5d5170ca661213323681a63acfe4678c Mon Sep 17 00:00:00 2001
From: Jonathan <jonathan.strobl@gmx.de>
Date: Wed, 13 Jul 2022 21:00:28 +0200
Subject: [PATCH 1/2] gallium/u_threaded: Prepare for buffer sharedness
 tracking

tc_add_to_buffer_list and tc_bind_buffer need the tc instance too.
---
 .../auxiliary/util/u_threaded_context.c       | 44 +++++++++----------
 1 file changed, 22 insertions(+), 22 deletions(-)

diff --git a/src/gallium/auxiliary/util/u_threaded_context.c b/src/gallium/auxiliary/util/u_threaded_context.c
index a9f69b465802..e0e4e1339010 100644
--- a/src/gallium/auxiliary/util/u_threaded_context.c
+++ b/src/gallium/auxiliary/util/u_threaded_context.c
@@ -407,7 +407,7 @@ threaded_context_flush(struct pipe_context *_pipe,
 }
 
 static void
-tc_add_to_buffer_list(struct tc_buffer_list *next, struct pipe_resource *buf)
+tc_add_to_buffer_list(struct threaded_context *tc, struct tc_buffer_list *next, struct pipe_resource *buf)
 {
    uint32_t id = threaded_resource(buf)->buffer_id_unique;
    BITSET_SET(next->buffer_list, id & TC_BUFFER_ID_MASK);
@@ -415,7 +415,7 @@ tc_add_to_buffer_list(struct tc_buffer_list *next, struct pipe_resource *buf)
 
 /* Set a buffer binding and add it to the buffer list. */
 static void
-tc_bind_buffer(uint32_t *binding, struct tc_buffer_list *next, struct pipe_resource *buf)
+tc_bind_buffer(struct threaded_context *tc, uint32_t *binding, struct tc_buffer_list *next, struct pipe_resource *buf)
 {
    uint32_t id = threaded_resource(buf)->buffer_id_unique;
    *binding = id;
@@ -933,7 +933,7 @@ tc_get_query_result_resource(struct pipe_context *_pipe,
    p->result_type = result_type;
    p->index = index;
    tc_set_resource_reference(&p->resource, resource);
-   tc_add_to_buffer_list(&tc->buffer_lists[tc->next_buf_list], resource);
+   tc_add_to_buffer_list(tc, &tc->buffer_lists[tc->next_buf_list], resource);
    p->offset = offset;
 }
 
@@ -1225,7 +1225,7 @@ tc_set_constant_buffer(struct pipe_context *_pipe,
       tc_set_resource_reference(&p->cb.buffer, buffer);
 
    if (buffer) {
-      tc_bind_buffer(&tc->const_buffers[shader][index],
+      tc_bind_buffer(tc, &tc->const_buffers[shader][index],
                      &tc->buffer_lists[tc->next_buf_list], buffer);
    } else {
       tc_unbind_buffer(&tc->const_buffers[shader][index]);
@@ -1425,7 +1425,7 @@ tc_set_sampler_views(struct pipe_context *_pipe,
 
          for (unsigned i = 0; i < count; i++) {
             if (views[i] && views[i]->target == PIPE_BUFFER) {
-               tc_bind_buffer(&tc->sampler_buffers[shader][start + i], next,
+               tc_bind_buffer(tc, &tc->sampler_buffers[shader][start + i], next,
                               views[i]->texture);
             } else {
                tc_unbind_buffer(&tc->sampler_buffers[shader][start + i]);
@@ -1437,7 +1437,7 @@ tc_set_sampler_views(struct pipe_context *_pipe,
             pipe_sampler_view_reference(&p->slot[i], views[i]);
 
             if (views[i] && views[i]->target == PIPE_BUFFER) {
-               tc_bind_buffer(&tc->sampler_buffers[shader][start + i], next,
+               tc_bind_buffer(tc, &tc->sampler_buffers[shader][start + i], next,
                               views[i]->texture);
             } else {
                tc_unbind_buffer(&tc->sampler_buffers[shader][start + i]);
@@ -1516,7 +1516,7 @@ tc_set_shader_images(struct pipe_context *_pipe,
          tc_set_resource_reference(&p->slot[i].resource, resource);
 
          if (resource && resource->target == PIPE_BUFFER) {
-            tc_bind_buffer(&tc->image_buffers[shader][start + i], next, resource);
+            tc_bind_buffer(tc, &tc->image_buffers[shader][start + i], next, resource);
 
             if (images[i].access & PIPE_IMAGE_ACCESS_WRITE) {
                struct threaded_resource *tres = threaded_resource(resource);
@@ -1611,7 +1611,7 @@ tc_set_shader_buffers(struct pipe_context *_pipe,
          if (src->buffer) {
             struct threaded_resource *tres = threaded_resource(src->buffer);
 
-            tc_bind_buffer(&tc->shader_buffers[shader][start + i], next, &tres->b);
+            tc_bind_buffer(tc, &tc->shader_buffers[shader][start + i], next, &tres->b);
 
             if (writable_bitmask & BITFIELD_BIT(i)) {
                tc_buffer_disable_cpu_storage(src->buffer);
@@ -1687,7 +1687,7 @@ tc_set_vertex_buffers(struct pipe_context *_pipe,
             struct pipe_resource *buf = buffers[i].buffer.resource;
 
             if (buf) {
-               tc_bind_buffer(&tc->vertex_buffers[start + i], next, buf);
+               tc_bind_buffer(tc, &tc->vertex_buffers[start + i], next, buf);
             } else {
                tc_unbind_buffer(&tc->vertex_buffers[start + i]);
             }
@@ -1705,7 +1705,7 @@ tc_set_vertex_buffers(struct pipe_context *_pipe,
             dst->buffer_offset = src->buffer_offset;
 
             if (buf) {
-               tc_bind_buffer(&tc->vertex_buffers[start + i], next, buf);
+               tc_bind_buffer(tc, &tc->vertex_buffers[start + i], next, buf);
             } else {
                tc_unbind_buffer(&tc->vertex_buffers[start + i]);
             }
@@ -1762,7 +1762,7 @@ tc_set_stream_output_targets(struct pipe_context *_pipe,
       pipe_so_target_reference(&p->targets[i], tgs[i]);
       if (tgs[i]) {
          tc_buffer_disable_cpu_storage(tgs[i]->buffer);
-         tc_bind_buffer(&tc->streamout_buffers[i], next, tgs[i]->buffer);
+         tc_bind_buffer(tc, &tc->streamout_buffers[i], next, tgs[i]->buffer);
       } else {
          tc_unbind_buffer(&tc->streamout_buffers[i]);
       }
@@ -2603,7 +2603,7 @@ tc_buffer_subdata(struct pipe_context *_pipe,
    /* This is will always be busy because if it wasn't, tc_improve_map_buffer-
     * _flags would set UNSYNCHRONIZED and we wouldn't get here.
     */
-   tc_add_to_buffer_list(&tc->buffer_lists[tc->next_buf_list], resource);
+   tc_add_to_buffer_list(tc, &tc->buffer_lists[tc->next_buf_list], resource);
    p->usage = usage;
    p->offset = offset;
    p->size = size;
@@ -3233,7 +3233,7 @@ tc_draw_vbo(struct pipe_context *_pipe, const struct pipe_draw_info *info,
             tc_set_resource_reference(&p->info.index.resource,
                                       info->index.resource);
          }
-         tc_add_to_buffer_list(next, info->index.resource);
+         tc_add_to_buffer_list(tc, next, info->index.resource);
       }
       memcpy(&p->info, info, DRAW_INFO_SIZE_WITHOUT_MIN_MAX_INDEX);
 
@@ -3245,11 +3245,11 @@ tc_draw_vbo(struct pipe_context *_pipe, const struct pipe_draw_info *info,
                                indirect->count_from_stream_output);
 
       if (indirect->buffer)
-         tc_add_to_buffer_list(next, indirect->buffer);
+         tc_add_to_buffer_list(tc, next, indirect->buffer);
       if (indirect->indirect_draw_count)
-         tc_add_to_buffer_list(next, indirect->indirect_draw_count);
+         tc_add_to_buffer_list(tc, next, indirect->indirect_draw_count);
       if (indirect->count_from_stream_output)
-         tc_add_to_buffer_list(next, indirect->count_from_stream_output->buffer);
+         tc_add_to_buffer_list(tc, next, indirect->count_from_stream_output->buffer);
 
       memcpy(&p->indirect, indirect, sizeof(*indirect));
       p->draw.start = draws[0].start;
@@ -3301,7 +3301,7 @@ tc_draw_vbo(struct pipe_context *_pipe, const struct pipe_draw_info *info,
                tc_set_resource_reference(&p->info.index.resource,
                                          info->index.resource);
             }
-            tc_add_to_buffer_list(&tc->buffer_lists[tc->next_buf_list], info->index.resource);
+            tc_add_to_buffer_list(tc, &tc->buffer_lists[tc->next_buf_list], info->index.resource);
          }
          if (drawid_offset > 0)
             ((struct tc_draw_single_drawid*)p)->drawid_offset = drawid_offset;
@@ -3416,7 +3416,7 @@ tc_draw_vbo(struct pipe_context *_pipe, const struct pipe_draw_info *info,
                tc_set_resource_reference(&p->info.index.resource,
                                          info->index.resource);
             }
-            tc_add_to_buffer_list(&tc->buffer_lists[tc->next_buf_list], info->index.resource);
+            tc_add_to_buffer_list(tc, &tc->buffer_lists[tc->next_buf_list], info->index.resource);
          }
          take_index_buffer_ownership = false;
          memcpy(&p->info, info, DRAW_INFO_SIZE_WITHOUT_MIN_MAX_INDEX);
@@ -3628,7 +3628,7 @@ tc_launch_grid(struct pipe_context *_pipe,
    memcpy(&p->info, info, sizeof(*info));
 
    if (info->indirect)
-      tc_add_to_buffer_list(&tc->buffer_lists[tc->next_buf_list], info->indirect);
+      tc_add_to_buffer_list(tc, &tc->buffer_lists[tc->next_buf_list], info->indirect);
 
    /* This must be after tc_add_*call, which can flush the batch. */
    if (unlikely(tc->add_all_compute_bindings_to_buffer_list))
@@ -3675,8 +3675,8 @@ tc_resource_copy_region(struct pipe_context *_pipe,
    if (dst->target == PIPE_BUFFER) {
       struct tc_buffer_list *next = &tc->buffer_lists[tc->next_buf_list];
 
-      tc_add_to_buffer_list(next, src);
-      tc_add_to_buffer_list(next, dst);
+      tc_add_to_buffer_list(tc, next, src);
+      tc_add_to_buffer_list(tc, next, dst);
 
       util_range_add(&tdst->b, &tdst->valid_buffer_range,
                      dstx, dstx + src_box->width);
@@ -3980,7 +3980,7 @@ tc_clear_buffer(struct pipe_context *_pipe, struct pipe_resource *res,
    tc_buffer_disable_cpu_storage(res);
 
    tc_set_resource_reference(&p->res, res);
-   tc_add_to_buffer_list(&tc->buffer_lists[tc->next_buf_list], res);
+   tc_add_to_buffer_list(tc, &tc->buffer_lists[tc->next_buf_list], res);
    p->offset = offset;
    p->size = size;
    memcpy(p->clear_value, clear_value, clear_value_size);
-- 
GitLab


From ba71439db21a6b457fb166b265b0e467494ba36e Mon Sep 17 00:00:00 2001
From: Jonathan <jonathan.strobl@gmx.de>
Date: Wed, 13 Jul 2022 21:11:33 +0200
Subject: [PATCH 2/2] gallium/u_threaded: buffer sharedness tracking

This fixes TC's buffer invalidation code for buffers that are shared
between contexts.

TC is unable to notify other TCs in case it replaces a shared buffer's
underlying storage when invalidating, causing those other TCs to use
wrong buffer IDs for busyness tracking, which leads to corruption due
to invalidation fast-paths being triggered when they shouldn't be.

This patch addresses this issue by tracking if a buffer is shared, and
if it is, disabling buffer storage replacement for the affected buffer.
This is achieved by tracking which TC instance first accessed a certain
buffer. If a second instance then accesses it as well, it will realize
that it isn't the only one working on the buffer and mark the buffer
accordingly.

If TC needs to invalidate a buffer for the correctness of an operation
at any point, it will fall back to doing the operation in a synchronous
fashion with this patch if the buffer is shared and currently busy.

It might be possible to later detect that a buffer has become un-shared;
however, this is outside of the scope of this bugfix patch.

v2: Do not disable buffer busyness tracking for shared buffers.

Fixes: e9c41b32 ("gallium/u_threaded: add buffer lists - tracking of buffers referenced by tc")
---
 .../auxiliary/util/u_threaded_context.c       | 67 +++++++++++++++++--
 .../auxiliary/util/u_threaded_context.h       | 17 +++++
 2 files changed, 78 insertions(+), 6 deletions(-)

diff --git a/src/gallium/auxiliary/util/u_threaded_context.c b/src/gallium/auxiliary/util/u_threaded_context.c
index e0e4e1339010..96ee51123bfe 100644
--- a/src/gallium/auxiliary/util/u_threaded_context.c
+++ b/src/gallium/auxiliary/util/u_threaded_context.c
@@ -406,10 +406,40 @@ threaded_context_flush(struct pipe_context *_pipe,
    }
 }
 
+/* Must be called before TC binds, maps, invalidates, or adds a buffer to a buffer list. */
+static void tc_touch_buffer(struct threaded_context *tc, struct threaded_resource *buf)
+{
+   const struct threaded_context *first_user = buf->first_user;
+
+   /* Fast path exit to avoid additional branches */
+   if (likely(first_user == tc))
+      return;
+
+   if (!first_user)
+      first_user = p_atomic_cmpxchg(&buf->first_user, NULL, tc);
+
+   /* The NULL check might seem unnecessary here but it's actually critical:
+    * p_atomic_cmpxchg will return NULL if it succeeds, meaning that NULL is
+    * equivalent to "we're the first user" here. (It's equally important not
+    * to ignore the result of the cmpxchg above, since it might fail.)
+    * Without the NULL check, we'd set the flag unconditionally, which is bad.
+    */
+   if (first_user && first_user != tc && !buf->used_by_multiple_contexts)
+      buf->used_by_multiple_contexts = true;
+}
+
+static bool tc_is_buffer_shared(struct threaded_resource *buf)
+{
+   return buf->is_shared || buf->used_by_multiple_contexts;
+}
+
 static void
 tc_add_to_buffer_list(struct threaded_context *tc, struct tc_buffer_list *next, struct pipe_resource *buf)
 {
-   uint32_t id = threaded_resource(buf)->buffer_id_unique;
+   struct threaded_resource *tbuf = threaded_resource(buf);
+   tc_touch_buffer(tc, tbuf);
+
+   uint32_t id = tbuf->buffer_id_unique;
    BITSET_SET(next->buffer_list, id & TC_BUFFER_ID_MASK);
 }
 
@@ -417,7 +447,10 @@ tc_add_to_buffer_list(struct threaded_context *tc, struct tc_buffer_list *next,
 static void
 tc_bind_buffer(struct threaded_context *tc, uint32_t *binding, struct tc_buffer_list *next, struct pipe_resource *buf)
 {
-   uint32_t id = threaded_resource(buf)->buffer_id_unique;
+   struct threaded_resource *tbuf = threaded_resource(buf);
+   tc_touch_buffer(tc, tbuf);
+
+   uint32_t id = tbuf->buffer_id_unique;
    *binding = id;
    BITSET_SET(next->buffer_list, id & TC_BUFFER_ID_MASK);
 }
@@ -675,6 +708,8 @@ threaded_resource_init(struct pipe_resource *res, bool allow_cpu_storage)
 {
    struct threaded_resource *tres = threaded_resource(res);
 
+   tres->first_user = NULL;
+   tres->used_by_multiple_contexts = false;
    tres->latest = &tres->b;
    tres->cpu_storage = NULL;
    util_range_init(&tres->valid_buffer_range);
@@ -1996,7 +2031,9 @@ tc_call_replace_buffer_storage(struct pipe_context *pipe, void *call, uint64_t *
    return call_size(tc_replace_buffer_storage);
 }
 
-/* Return true if the buffer has been invalidated or is idle. */
+/* Return true if the buffer has been invalidated or is idle.
+ * Note that callers must've called tc_touch_buffer before calling
+ * this function. */
 static bool
 tc_invalidate_buffer(struct threaded_context *tc,
                      struct threaded_resource *tbuf)
@@ -2017,7 +2054,7 @@ tc_invalidate_buffer(struct threaded_context *tc,
    struct pipe_resource *new_buf;
 
    /* Shared, pinned, and sparse buffers can't be reallocated. */
-   if (tbuf->is_shared ||
+   if (tc_is_buffer_shared(tbuf) ||
        tbuf->is_user_ptr ||
        tbuf->b.flags & (PIPE_RESOURCE_FLAG_SPARSE | PIPE_RESOURCE_FLAG_UNMAPPABLE))
       return false;
@@ -2062,6 +2099,8 @@ tc_invalidate_buffer(struct threaded_context *tc,
    return true;
 }
 
+/* Note that callers must've called tc_touch_buffer first before
+ * calling tc_improve_map_buffer_flags. */
 static unsigned
 tc_improve_map_buffer_flags(struct threaded_context *tc,
                             struct threaded_resource *tres, unsigned usage,
@@ -2176,6 +2215,14 @@ tc_buffer_map(struct pipe_context *_pipe,
    if (usage & PIPE_MAP_THREAD_SAFE)
       tc_buffer_disable_cpu_storage(resource);
 
+   tc_touch_buffer(tc, tres);
+
+   /* CPU storage relies on buffer invalidation never failing. With shared buffers,
+    * invalidation might not always be possible, so CPU storage can't be used.
+    */
+   if (tc_is_buffer_shared(tres))
+      tc_buffer_disable_cpu_storage(resource);
+
    usage = tc_improve_map_buffer_flags(tc, tres, usage, box->x, box->width);
 
    /* If the CPU storage is enabled, return it directly. */
@@ -2450,7 +2497,10 @@ tc_buffer_unmap(struct pipe_context *_pipe, struct pipe_transfer *transfer)
       assert(tres->cpu_storage);
 
       if (tres->cpu_storage) {
-         tc_invalidate_buffer(tc, tres);
+         /* Invalidations shouldn't fail as long as CPU storage is allowed. */
+         ASSERTED bool invalidated = tc_invalidate_buffer(tc, tres);
+         assert(invalidated);
+
          tc_buffer_subdata(&tc->base, &tres->b,
                            PIPE_MAP_UNSYNCHRONIZED |
                            TC_TRANSFER_MAP_UPLOAD_CPU_STORAGE,
@@ -2564,6 +2614,8 @@ tc_buffer_subdata(struct pipe_context *_pipe,
    if (!size)
       return;
 
+   tc_touch_buffer(tc, tres);
+
    usage |= PIPE_MAP_WRITE;
 
    /* PIPE_MAP_DIRECTLY supresses implicit DISCARD_RANGE. */
@@ -3812,7 +3864,10 @@ tc_invalidate_resource(struct pipe_context *_pipe,
    struct threaded_context *tc = threaded_context(_pipe);
 
    if (resource->target == PIPE_BUFFER) {
-      tc_invalidate_buffer(tc, threaded_resource(resource));
+      /* This can fail, in which case we simply ignore the invalidation request. */
+      struct threaded_resource *tbuf = threaded_resource(resource);
+      tc_touch_buffer(tc, tbuf);
+      tc_invalidate_buffer(tc, tbuf);
       return;
    }
 
diff --git a/src/gallium/auxiliary/util/u_threaded_context.h b/src/gallium/auxiliary/util/u_threaded_context.h
index 848c56abb896..9f760b771c75 100644
--- a/src/gallium/auxiliary/util/u_threaded_context.h
+++ b/src/gallium/auxiliary/util/u_threaded_context.h
@@ -315,6 +315,23 @@ typedef bool (*tc_is_resource_busy)(struct pipe_screen *screen,
 struct threaded_resource {
    struct pipe_resource b;
 
+   /* Pointer to the TC that first used this threaded_resource (buffer). This is used to
+    * allow TCs to determine whether they have been given a buffer that was created by a
+    * different TC, in which case all TCs have to disable busyness tracking and buffer
+    * replacement for that particular buffer.
+    * DO NOT DEREFERENCE. The only operation allowed on this pointer is equality-checking
+    * since it might be dangling if a buffer has been shared and its first_user has
+    * already been destroyed. The pointer is const void to discourage such disallowed usage.
+    * This is NULL if no TC has used this buffer yet.
+    */
+   const void *first_user;
+
+   /* True if multiple threaded contexts have accessed this buffer.
+    * Disables non-multicontext-safe optimizations in TC.
+    * We can't just re-use is_shared for that purpose as that would confuse drivers.
+    */
+   bool used_by_multiple_contexts;
+
    /* Since buffer invalidations are queued, we can't use the base resource
     * for unsychronized mappings. This points to the latest version of
     * the buffer after the latest invalidation. It's only used for unsychro-
-- 
GitLab

