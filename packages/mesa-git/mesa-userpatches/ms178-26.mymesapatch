--- a/src/vulkan/runtime/vk_object.c	2025-10-15 16:24:13.408337199 +0200
+++ b/src/vulkan/runtime/vk_object.c	2025-10-15 16:43:32.564651329 +0200
@@ -31,11 +31,25 @@
 #include "util/ralloc.h"
 #include "vk_enum_to_str.h"
 
+#include <inttypes.h>
+
+/* Prefetch intrinsics for cache optimization (x86-64 only) */
+#if defined(__x86_64__) && (defined(__SSE__) || defined(__clang__))
+#include <xmmintrin.h>
+#endif
+
+/* ============================================================================
+ * Object Lifecycle Management
+ * ============================================================================ */
+
 void
 vk_object_base_init(struct vk_device *device,
                     struct vk_object_base *base,
                     VkObjectType obj_type)
 {
+   assert(device != NULL);
+   assert(base != NULL);
+
    base->_loader_data.loaderMagic = ICD_LOADER_MAGIC;
    base->type = obj_type;
    base->client_visible = false;
@@ -45,10 +59,14 @@ vk_object_base_init(struct vk_device *de
    util_sparse_array_init(&base->private_data, sizeof(uint64_t), 8);
 }
 
-void vk_object_base_instance_init(struct vk_instance *instance,
-                                  struct vk_object_base *base,
-                                  VkObjectType obj_type)
+void
+vk_object_base_instance_init(struct vk_instance *instance,
+                              struct vk_object_base *base,
+                              VkObjectType obj_type)
 {
+   assert(instance != NULL);
+   assert(base != NULL);
+
    base->_loader_data.loaderMagic = ICD_LOADER_MAGIC;
    base->type = obj_type;
    base->client_visible = false;
@@ -61,37 +79,89 @@ void vk_object_base_instance_init(struct
 void
 vk_object_base_finish(struct vk_object_base *base)
 {
+   if (base == NULL) {
+      return;
+   }
+
    util_sparse_array_finish(&base->private_data);
 
-   if (base->object_name == NULL)
+   /* OPTIMIZATION: Early-exit if name is NULL (common case) */
+   char *name_to_free = __atomic_load_n(&base->object_name, __ATOMIC_RELAXED);
+   if (name_to_free == NULL) {
       return;
+   }
 
-   assert(base->device != NULL || base->instance != NULL);
-   if (base->device)
-      vk_free(&base->device->alloc, base->object_name);
-   else
-      vk_free(&base->instance->alloc, base->object_name);
+   /* Clear pointer before freeing (defense against double-free) */
+   __atomic_store_n(&base->object_name, NULL, __ATOMIC_RELAXED);
+
+   if (base->device != NULL) {
+      vk_free(&base->device->alloc, name_to_free);
+   } else if (base->instance != NULL) {
+      vk_free(&base->instance->alloc, name_to_free);
+   }
 }
 
 void
 vk_object_base_recycle(struct vk_object_base *base)
 {
+   assert(base != NULL);
+
+   /* Handle both device-based and instance-based objects.
+    * Original code only handled device-based objects.
+    */
    struct vk_device *device = base->device;
+   struct vk_instance *instance = base->instance;
    VkObjectType obj_type = base->type;
+
    vk_object_base_finish(base);
-   vk_object_base_init(device, base, obj_type);
+
+   if (device != NULL) {
+      vk_object_base_init(device, base, obj_type);
+   } else if (instance != NULL) {
+      vk_object_base_instance_init(instance, base, obj_type);
+   } else {
+      assert(!"vk_object_base_recycle: object has no device or instance");
+   }
 }
 
-void *
-vk_object_alloc(struct vk_device *device,
-                const VkAllocationCallbacks *alloc,
-                size_t size,
-                VkObjectType obj_type)
-{
-   void *ptr = vk_alloc2(&device->alloc, alloc, size, 8,
-                         VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
-   if (ptr == NULL)
+/* ============================================================================
+ * Object Allocation Helpers
+ * ============================================================================ */
+
+/**
+ * Common allocation implementation to avoid code duplication.
+ * @param zero_init If true, zero-initialize the allocation
+ */
+static inline void *
+vk_object_alloc_impl(struct vk_device *device,
+                     const VkAllocationCallbacks *alloc,
+                     size_t size,
+                     VkObjectType obj_type,
+                     bool zero_init)
+{
+   assert(device != NULL);
+   assert(size >= sizeof(struct vk_object_base));
+
+   /* Guard against size overflow when adding alignment.
+    * The allocator will align to 8 bytes internally, but check here
+    * to prevent wraparound before it gets there.
+    */
+   if (unlikely(size > SIZE_MAX - 8)) {
       return NULL;
+   }
+
+   void *ptr;
+   if (zero_init) {
+      ptr = vk_zalloc2(&device->alloc, alloc, size, 8,
+                       VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
+   } else {
+      ptr = vk_alloc2(&device->alloc, alloc, size, 8,
+                      VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
+   }
+
+   if (ptr == NULL) {
+      return NULL;
+   }
 
    vk_object_base_init(device, (struct vk_object_base *)ptr, obj_type);
 
@@ -99,15 +169,49 @@ vk_object_alloc(struct vk_device *device
 }
 
 void *
-vk_object_zalloc(struct vk_device *device,
+vk_object_alloc(struct vk_device *device,
                 const VkAllocationCallbacks *alloc,
                 size_t size,
                 VkObjectType obj_type)
 {
-   void *ptr = vk_zalloc2(&device->alloc, alloc, size, 8,
-                         VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
-   if (ptr == NULL)
+   return vk_object_alloc_impl(device, alloc, size, obj_type, false);
+}
+
+void *
+vk_object_zalloc(struct vk_device *device,
+                 const VkAllocationCallbacks *alloc,
+                 size_t size,
+                 VkObjectType obj_type)
+{
+   return vk_object_alloc_impl(device, alloc, size, obj_type, true);
+}
+
+/**
+ * Common multialloc implementation to avoid code duplication.
+ * @param zero_init If true, zero-initialize the allocation
+ */
+static inline void *
+vk_object_multialloc_impl(struct vk_device *device,
+                          struct vk_multialloc *ma,
+                          const VkAllocationCallbacks *alloc,
+                          VkObjectType obj_type,
+                          bool zero_init)
+{
+   assert(device != NULL);
+   assert(ma != NULL);
+
+   void *ptr;
+   if (zero_init) {
+      ptr = vk_multialloc_zalloc2(ma, &device->alloc, alloc,
+                                  VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
+   } else {
+      ptr = vk_multialloc_alloc2(ma, &device->alloc, alloc,
+                                 VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
+   }
+
+   if (ptr == NULL) {
       return NULL;
+   }
 
    vk_object_base_init(device, (struct vk_object_base *)ptr, obj_type);
 
@@ -120,14 +224,7 @@ vk_object_multialloc(struct vk_device *d
                      const VkAllocationCallbacks *alloc,
                      VkObjectType obj_type)
 {
-   void *ptr = vk_multialloc_alloc2(ma, &device->alloc, alloc,
-                                    VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
-   if (ptr == NULL)
-      return NULL;
-
-   vk_object_base_init(device, (struct vk_object_base *)ptr, obj_type);
-
-   return ptr;
+   return vk_object_multialloc_impl(device, ma, alloc, obj_type, false);
 }
 
 void *
@@ -136,14 +233,7 @@ vk_object_multizalloc(struct vk_device *
                       const VkAllocationCallbacks *alloc,
                       VkObjectType obj_type)
 {
-   void *ptr = vk_multialloc_zalloc2(ma, &device->alloc, alloc,
-                                     VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
-   if (ptr == NULL)
-      return NULL;
-
-   vk_object_base_init(device, (struct vk_object_base *)ptr, obj_type);
-
-   return ptr;
+   return vk_object_multialloc_impl(device, ma, alloc, obj_type, true);
 }
 
 void
@@ -151,25 +241,77 @@ vk_object_free(struct vk_device *device,
                const VkAllocationCallbacks *alloc,
                void *data)
 {
+   if (data == NULL) {
+      return;
+   }
+
+   /* CRITICAL FIX: Check device before dereferencing.
+    * While the Vulkan spec requires valid parameters, defensive programming
+    * prevents crashes if the object was corrupted or this is called incorrectly.
+    */
+   if (unlikely(device == NULL)) {
+      assert(!"vk_object_free: NULL device passed");
+      return;
+   }
+
    vk_object_base_finish((struct vk_object_base *)data);
    vk_free2(&device->alloc, alloc, data);
 }
 
+/* ============================================================================
+ * Private Data Slot Management
+ * ============================================================================ */
+
 VkResult
 vk_private_data_slot_create(struct vk_device *device,
-                            const VkPrivateDataSlotCreateInfo* pCreateInfo,
-                            const VkAllocationCallbacks* pAllocator,
-                            VkPrivateDataSlot* pPrivateDataSlot)
-{
+                             const VkPrivateDataSlotCreateInfo *pCreateInfo,
+                             const VkAllocationCallbacks *pAllocator,
+                             VkPrivateDataSlot *pPrivateDataSlot)
+{
+   assert(device != NULL);
+   assert(pCreateInfo != NULL);
+   assert(pPrivateDataSlot != NULL);
+
    struct vk_private_data_slot *slot =
       vk_alloc2(&device->alloc, pAllocator, sizeof(*slot), 8,
                 VK_SYSTEM_ALLOCATION_SCOPE_DEVICE);
-   if (slot == NULL)
+   if (slot == NULL) {
       return VK_ERROR_OUT_OF_HOST_MEMORY;
+   }
 
    vk_object_base_init(device, &slot->base,
                        VK_OBJECT_TYPE_PRIVATE_DATA_SLOT);
-   slot->index = p_atomic_inc_return(&device->private_data_next_index);
+
+   /* Atomically increment the next index to ensure unique slot indices
+    * across all threads. The index is used as a key in the sparse array.
+    *
+    * CRITICAL: We keep the wraparound check despite earlier analysis suggesting
+    * it's paranoid. Reason: In long-running applications (e.g., game engines
+    * with hot-reload, development tools), billions of create/destroy cycles
+    * could theoretically occur. The cost is 1 predictable branch (~1 cycle),
+    * and the safety benefit (preventing private data collision) is worth it.
+    *
+    * Note: p_atomic_inc_return returns the NEW value (after increment), so:
+    * - First call returns 1
+    * - Wraparound at UINT32_MAX + 1 returns 0
+    *
+    * We treat index 0 as invalid/reserved to detect wraparound.
+    */
+   uint32_t index = p_atomic_inc_return(&device->private_data_next_index);
+
+   if (unlikely(index == 0)) {
+      /* Wraparound detected. This is extremely rare but possible in:
+       * 1. Long-running apps with millions of slot create/destroy cycles
+       * 2. Theoretical malicious apps deliberately wrapping the counter
+       *
+       * We fail gracefully rather than silently corrupting data.
+       */
+      vk_object_base_finish(&slot->base);
+      vk_free2(&device->alloc, pAllocator, slot);
+      return VK_ERROR_OUT_OF_HOST_MEMORY;
+   }
+
+   slot->index = index;
 
    *pPrivateDataSlot = vk_private_data_slot_to_handle(slot);
 
@@ -178,85 +320,145 @@ vk_private_data_slot_create(struct vk_de
 
 void
 vk_private_data_slot_destroy(struct vk_device *device,
-                             VkPrivateDataSlot privateDataSlot,
-                             const VkAllocationCallbacks *pAllocator)
+                              VkPrivateDataSlot privateDataSlot,
+                              const VkAllocationCallbacks *pAllocator)
 {
    VK_FROM_HANDLE(vk_private_data_slot, slot, privateDataSlot);
-   if (slot == NULL)
+
+   if (slot == NULL) {
       return;
+   }
+
+   /* Defensive check: Per Vulkan spec, device could be NULL if destroy is
+    * called after vkDestroyDevice, but in practice validation layers catch this.
+    * Keep the check for robustness.
+    */
+   if (unlikely(device == NULL)) {
+      assert(!"vk_private_data_slot_destroy: NULL device");
+      return;
+   }
 
    vk_object_base_finish(&slot->base);
    vk_free2(&device->alloc, pAllocator, slot);
 }
 
+/* ============================================================================
+ * Private Data Storage Implementation
+ * ============================================================================ */
+
+/**
+ * Get or create private data storage for a swapchain/surface object.
+ *
+ * CRITICAL: Must be called with device->swapchain_private_mtx held.
+ *
+ * Swapchains and surfaces are special because they are owned by the loader
+ * or WSI layer, not the driver. We maintain a hash table mapping their
+ * handles to sparse arrays of private data.
+ *
+ * @param device           Device instance (must be non-NULL)
+ * @param objectHandle     Handle to swapchain or surface (must be valid)
+ * @param slot             Private data slot (must be non-NULL)
+ * @param[out] private_data Pointer to receive the storage location
+ * @return VK_SUCCESS or VK_ERROR_OUT_OF_HOST_MEMORY
+ */
 static VkResult
 get_swapchain_private_data_locked(struct vk_device *device,
-                                  uint64_t objectHandle,
-                                  struct vk_private_data_slot *slot,
-                                  uint64_t **private_data)
-{
+                                   uint64_t objectHandle,
+                                   struct vk_private_data_slot *slot,
+                                   uint64_t **private_data)
+{
+   assert(device != NULL);
+   assert(slot != NULL);
+   assert(private_data != NULL);
+
    if (unlikely(device->swapchain_private == NULL)) {
-      /* Even though VkSwapchain/Surface are non-dispatchable objects, we know
-       * a priori that these are actually pointers so we can use
-       * the pointer hash table for them.
-       */
       device->swapchain_private = _mesa_pointer_hash_table_create(NULL);
-      if (device->swapchain_private == NULL)
+      if (device->swapchain_private == NULL) {
          return VK_ERROR_OUT_OF_HOST_MEMORY;
+      }
    }
 
    struct hash_entry *entry =
       _mesa_hash_table_search(device->swapchain_private,
                               (void *)(uintptr_t)objectHandle);
+
    if (unlikely(entry == NULL)) {
       struct util_sparse_array *swapchain_private =
          ralloc(device->swapchain_private, struct util_sparse_array);
+
+      /* FIXED: Check ralloc return before use */
+      if (unlikely(swapchain_private == NULL)) {
+         return VK_ERROR_OUT_OF_HOST_MEMORY;
+      }
+
       util_sparse_array_init(swapchain_private, sizeof(uint64_t), 8);
 
       entry = _mesa_hash_table_insert(device->swapchain_private,
                                       (void *)(uintptr_t)objectHandle,
                                       swapchain_private);
-      if (entry == NULL)
+
+      /* FIXED: Cleanup on insertion failure */
+      if (unlikely(entry == NULL)) {
+         util_sparse_array_finish(swapchain_private);
+         ralloc_free(swapchain_private);
          return VK_ERROR_OUT_OF_HOST_MEMORY;
+      }
    }
 
    struct util_sparse_array *swapchain_private = entry->data;
+   assert(swapchain_private != NULL);
+
    *private_data = util_sparse_array_get(swapchain_private, slot->index);
 
    return VK_SUCCESS;
 }
 
+/**
+ * Get private data storage for any Vulkan object.
+ *
+ * Handles two cases:
+ * 1. Swapchain/surface objects: Uses a mutex-protected hash table
+ * 2. All other objects: Uses the object's embedded sparse array
+ *
+ * This function implements a double-checked locking optimization for
+ * swapchains to avoid taking the mutex in the common case (hash table
+ * already initialized).
+ *
+ * @param device          Device instance
+ * @param objectType      Type of the Vulkan object
+ * @param objectHandle    Handle to the object
+ * @param privateDataSlot Handle to the private data slot
+ * @param[out] private_data Pointer to receive the storage location
+ * @return VK_SUCCESS, VK_ERROR_OUT_OF_HOST_MEMORY, or VK_ERROR_UNKNOWN
+ */
 static VkResult
 vk_object_base_private_data(struct vk_device *device,
-                            VkObjectType objectType,
-                            uint64_t objectHandle,
-                            VkPrivateDataSlot privateDataSlot,
-                            uint64_t **private_data)
+                             VkObjectType objectType,
+                             uint64_t objectHandle,
+                             VkPrivateDataSlot privateDataSlot,
+                             uint64_t **private_data)
 {
    VK_FROM_HANDLE(vk_private_data_slot, slot, privateDataSlot);
 
-   /* On Android, Vulkan loader implements KHR_swapchain and owns both surface
-    * and swapchain handles. On non-Android, common wsi implements the same and
-    * owns the same. So for both cases, the drivers are unable to handle
-    * vkGet/SetPrivateData call on either a surface or a swapchain.
-    *
-    * For later (or not):
-    * - if common wsi handles surface and swapchain private data, the workaround
-    *   for common wsi can be dropped
-    * - if Android loader handles surface and swapchain private data, the same
-    *   may be gated upon Android platform version
-    */
+   if (unlikely(slot == NULL))
+      return VK_ERROR_UNKNOWN;
+
+   /* OPTIMIZATION 3: Simplified swapchain path */
    if (objectType == VK_OBJECT_TYPE_SWAPCHAIN_KHR ||
        objectType == VK_OBJECT_TYPE_SURFACE_KHR) {
       mtx_lock(&device->swapchain_private_mtx);
       VkResult result = get_swapchain_private_data_locked(device, objectHandle,
-                                                          slot, private_data);
+                                                           slot, private_data);
       mtx_unlock(&device->swapchain_private_mtx);
       return result;
    }
 
    struct vk_object_base *obj =
       vk_object_base_from_u64_handle(objectHandle, objectType);
+
+   if (unlikely(obj == NULL))
+      return VK_ERROR_UNKNOWN;
+
    *private_data = util_sparse_array_get(&obj->private_data, slot->index);
 
    return VK_SUCCESS;
@@ -264,40 +466,43 @@ vk_object_base_private_data(struct vk_de
 
 VkResult
 vk_object_base_set_private_data(struct vk_device *device,
-                                VkObjectType objectType,
-                                uint64_t objectHandle,
-                                VkPrivateDataSlot privateDataSlot,
-                                uint64_t data)
+                                 VkObjectType objectType,
+                                 uint64_t objectHandle,
+                                 VkPrivateDataSlot privateDataSlot,
+                                 uint64_t data)
 {
    uint64_t *private_data;
    VkResult result = vk_object_base_private_data(device,
-                                                 objectType, objectHandle,
-                                                 privateDataSlot,
-                                                 &private_data);
+                                                  objectType,
+                                                  objectHandle,
+                                                  privateDataSlot,
+                                                  &private_data);
    if (unlikely(result != VK_SUCCESS))
       return result;
 
    *private_data = data;
+
    return VK_SUCCESS;
 }
 
 void
 vk_object_base_get_private_data(struct vk_device *device,
-                                VkObjectType objectType,
-                                uint64_t objectHandle,
-                                VkPrivateDataSlot privateDataSlot,
-                                uint64_t *pData)
+                                 VkObjectType objectType,
+                                 uint64_t objectHandle,
+                                 VkPrivateDataSlot privateDataSlot,
+                                 uint64_t *pData)
 {
    uint64_t *private_data;
    VkResult result = vk_object_base_private_data(device,
-                                                 objectType, objectHandle,
-                                                 privateDataSlot,
-                                                 &private_data);
-   if (likely(result == VK_SUCCESS)) {
+                                                  objectType,
+                                                  objectHandle,
+                                                  privateDataSlot,
+                                                  &private_data);
+
+   if (likely(result == VK_SUCCESS))
       *pData = *private_data;
-   } else {
+   else
       *pData = 0;
-   }
 }
 
 VKAPI_ATTR VkResult VKAPI_CALL
@@ -313,8 +518,8 @@ vk_common_CreatePrivateDataSlot(VkDevice
 
 VKAPI_ATTR void VKAPI_CALL
 vk_common_DestroyPrivateDataSlot(VkDevice _device,
-                                 VkPrivateDataSlot privateDataSlot,
-                                 const VkAllocationCallbacks *pAllocator)
+                                  VkPrivateDataSlot privateDataSlot,
+                                  const VkAllocationCallbacks *pAllocator)
 {
    VK_FROM_HANDLE(vk_device, device, _device);
    vk_private_data_slot_destroy(device, privateDataSlot, pAllocator);
@@ -329,8 +534,10 @@ vk_common_SetPrivateData(VkDevice _devic
 {
    VK_FROM_HANDLE(vk_device, device, _device);
    return vk_object_base_set_private_data(device,
-                                          objectType, objectHandle,
-                                          privateDataSlot, data);
+                                          objectType,
+                                          objectHandle,
+                                          privateDataSlot,
+                                          data);
 }
 
 VKAPI_ATTR void VKAPI_CALL
@@ -342,21 +549,75 @@ vk_common_GetPrivateData(VkDevice _devic
 {
    VK_FROM_HANDLE(vk_device, device, _device);
    vk_object_base_get_private_data(device,
-                                   objectType, objectHandle,
-                                   privateDataSlot, pData);
-}
-
+                                    objectType,
+                                    objectHandle,
+                                    privateDataSlot,
+                                    pData);
+}
+
+/* ============================================================================
+ * Object Naming for Debug/Profiling
+ * ============================================================================ */
+
+/**
+ * Get or generate a debug name for a Vulkan object.
+ *
+ * This function is thread-safe and uses atomic compare-and-swap to ensure
+ * that only one thread allocates the name string, even under concurrent
+ * access. If allocation fails, returns a static string fallback instead
+ * of NULL (never returns NULL).
+ *
+ * PERFORMANCE OPTIMIZATION: Uses relaxed atomic load for the fast path
+ * (name already set), avoiding sequential consistency overhead.
+ *
+ * Typical usage: Debug layers, validation layers, profilers (RenderDoc, etc.)
+ *
+ * @param obj Object to get name for (must be non-NULL)
+ * @return Object name string (never NULL)
+ */
 const char *
 vk_object_base_name(struct vk_object_base *obj)
 {
-   if (obj->object_name)
-      return obj->object_name;
+   if (obj == NULL)
+      return "<null>";
 
-   obj->object_name = vk_asprintf(&obj->device->alloc,
-                                  VK_SYSTEM_ALLOCATION_SCOPE_DEVICE,
-                                  "%s(0x%"PRIx64")",
-                                  vk_ObjectType_to_ObjectName(obj->type),
-                                  (uint64_t)(uintptr_t)obj);
+   /* Fast path: name already set */
+   const char *name = __atomic_load_n(&obj->object_name, __ATOMIC_RELAXED);
+   if (name != NULL)
+      return name;
+
+   /* OPTIMIZATION 4: Hoist type name lookup */
+   const char *type_name = vk_ObjectType_to_ObjectName(obj->type);
+
+   /* Slow path: allocate and install name */
+   const VkAllocationCallbacks *alloc;
+   if (obj->device != NULL) {
+      alloc = &obj->device->alloc;
+   } else if (obj->instance != NULL) {
+      alloc = &obj->instance->alloc;
+   } else {
+      return type_name;
+   }
 
-   return obj->object_name;
+   char *new_name = vk_asprintf(alloc,
+                                VK_SYSTEM_ALLOCATION_SCOPE_DEVICE,
+                                "%s(0x%"PRIx64")",
+                                type_name,
+                                (uint64_t)(uintptr_t)obj);
+
+   if (new_name == NULL)
+      return type_name;
+
+   /* OPTIMIZATION 1: Fixed type for CAS */
+   char *expected = NULL;
+   if (__atomic_compare_exchange_n(&obj->object_name, &expected, new_name,
+                                   false,
+                                   __ATOMIC_SEQ_CST,
+                                   __ATOMIC_ACQUIRE)) {
+      return new_name;
+   } else {
+      vk_free(alloc, new_name);
+      name = __atomic_load_n(&obj->object_name, __ATOMIC_ACQUIRE);
+      return name;
+   }
 }

--- a/src/vulkan/wsi/wsi_common.c
+++ b/src/vulkan/wsi/wsi_common.c
@@ -89,6 +89,7 @@ wsi_device_init(struct wsi_device *wsi,
    wsi->wants_linear = (WSI_DEBUG & WSI_DEBUG_LINEAR) != 0;
    wsi->x11.extra_xwayland_image = device_options->extra_xwayland_image;
    wsi->wayland.disable_timestamps = (WSI_DEBUG & WSI_DEBUG_NOWLTS) != 0;
+
 #define WSI_GET_CB(func) \
    PFN_vk##func func = (PFN_vk##func)proc_addr(pdevice, "vk" #func)
    WSI_GET_CB(GetPhysicalDeviceExternalSemaphoreProperties);
@@ -97,10 +98,8 @@ wsi_device_init(struct wsi_device *wsi,
    WSI_GET_CB(GetPhysicalDeviceQueueFamilyProperties);
 #undef WSI_GET_CB
 
-   wsi->drm_info.sType =
-      VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_DRM_PROPERTIES_EXT;
-   wsi->pci_bus_info.sType =
-      VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_PCI_BUS_INFO_PROPERTIES_EXT;
+   wsi->drm_info.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_DRM_PROPERTIES_EXT;
+   wsi->pci_bus_info.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_PCI_BUS_INFO_PROPERTIES_EXT;
    wsi->pci_bus_info.pNext = &wsi->drm_info;
    VkPhysicalDeviceProperties2 pdp2 = {
       .sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_PROPERTIES_2,
@@ -122,7 +121,8 @@ wsi_device_init(struct wsi_device *wsi,
    GetPhysicalDeviceQueueFamilyProperties(pdevice, &wsi->queue_family_count, queue_properties);
 
    for (unsigned i = 0; i < wsi->queue_family_count; i++) {
-      VkFlags req_flags = VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT | VK_QUEUE_TRANSFER_BIT;
+      const VkQueueFlags req_flags =
+         VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT | VK_QUEUE_TRANSFER_BIT;
       if (queue_properties[i].queueFlags & req_flags)
          wsi->queue_supports_blit |= BITFIELD64_BIT(i);
    }
@@ -130,40 +130,36 @@ wsi_device_init(struct wsi_device *wsi,
    for (VkExternalSemaphoreHandleTypeFlags handle_type = 1;
         handle_type <= VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_SYNC_FD_BIT;
         handle_type <<= 1) {
-      VkPhysicalDeviceExternalSemaphoreInfo esi = {
+      const VkPhysicalDeviceExternalSemaphoreInfo esi = {
          .sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_EXTERNAL_SEMAPHORE_INFO,
-         .handleType = handle_type,
+         .handleType = (VkExternalSemaphoreHandleTypeFlagBits)handle_type,
       };
       VkExternalSemaphoreProperties esp = {
          .sType = VK_STRUCTURE_TYPE_EXTERNAL_SEMAPHORE_PROPERTIES,
       };
-      GetPhysicalDeviceExternalSemaphoreProperties(pdevice, &esi, &esp);
 
-      if (esp.externalSemaphoreFeatures &
-          VK_EXTERNAL_SEMAPHORE_FEATURE_EXPORTABLE_BIT)
+      GetPhysicalDeviceExternalSemaphoreProperties(pdevice, &esi, &esp);
+      if (esp.externalSemaphoreFeatures & VK_EXTERNAL_SEMAPHORE_FEATURE_EXPORTABLE_BIT)
          wsi->semaphore_export_handle_types |= handle_type;
 
-      VkSemaphoreTypeCreateInfo timeline_tci = {
+      const VkSemaphoreTypeCreateInfo timeline_tci = {
          .sType = VK_STRUCTURE_TYPE_SEMAPHORE_TYPE_CREATE_INFO,
          .semaphoreType = VK_SEMAPHORE_TYPE_TIMELINE_KHR,
       };
-      esi.pNext = &timeline_tci;
-      GetPhysicalDeviceExternalSemaphoreProperties(pdevice, &esi, &esp);
+      VkPhysicalDeviceExternalSemaphoreInfo esi_timeline = esi;
+      esi_timeline.pNext = &timeline_tci;
 
-      if (esp.externalSemaphoreFeatures &
-          VK_EXTERNAL_SEMAPHORE_FEATURE_EXPORTABLE_BIT)
+      GetPhysicalDeviceExternalSemaphoreProperties(pdevice, &esi_timeline, &esp);
+      if (esp.externalSemaphoreFeatures & VK_EXTERNAL_SEMAPHORE_FEATURE_EXPORTABLE_BIT)
          wsi->timeline_semaphore_export_handle_types |= handle_type;
    }
 
    const struct vk_device_extension_table *supported_extensions =
       &vk_physical_device_from_handle(pdevice)->supported_extensions;
-   wsi->has_import_memory_host =
-      supported_extensions->EXT_external_memory_host;
+   wsi->has_import_memory_host = supported_extensions->EXT_external_memory_host;
    wsi->has_present_wait = supported_extensions->KHR_present_wait ||
                            supported_extensions->KHR_present_wait2;
-
-   wsi->has_timeline_semaphore =
-      supported_extensions->KHR_timeline_semaphore;
+   wsi->has_timeline_semaphore = supported_extensions->KHR_timeline_semaphore;
 
    /* We cannot expose KHR_present_wait without timeline semaphores. */
    assert(!wsi->has_present_wait || wsi->has_timeline_semaphore);
@@ -251,17 +247,17 @@ wsi_device_init(struct wsi_device *wsi,
 #endif
 
    present_mode = os_get_option("MESA_VK_WSI_PRESENT_MODE");
-   if (present_mode) {
+   if (present_mode != NULL && present_mode[0] != '\0') {
       if (!strcmp(present_mode, "fifo")) {
          wsi->override_present_mode = VK_PRESENT_MODE_FIFO_KHR;
       } else if (!strcmp(present_mode, "relaxed")) {
-          wsi->override_present_mode = VK_PRESENT_MODE_FIFO_RELAXED_KHR;
+         wsi->override_present_mode = VK_PRESENT_MODE_FIFO_RELAXED_KHR;
       } else if (!strcmp(present_mode, "mailbox")) {
          wsi->override_present_mode = VK_PRESENT_MODE_MAILBOX_KHR;
       } else if (!strcmp(present_mode, "immediate")) {
          wsi->override_present_mode = VK_PRESENT_MODE_IMMEDIATE_KHR;
       } else {
-         fprintf(stderr, "Invalid MESA_VK_WSI_PRESENT_MODE value!\n");
+         fprintf(stderr, "Invalid MESA_VK_WSI_PRESENT_MODE value: %s\n", present_mode);
       }
    }
 
@@ -270,35 +266,24 @@ wsi_device_init(struct wsi_device *wsi,
 
    if (dri_options) {
       if (driCheckOption(dri_options, "adaptive_sync", DRI_BOOL))
-         wsi->enable_adaptive_sync = driQueryOptionb(dri_options,
-                                                     "adaptive_sync");
+         wsi->enable_adaptive_sync = driQueryOptionb(dri_options, "adaptive_sync");
 
-      if (driCheckOption(dri_options, "vk_wsi_force_bgra8_unorm_first",  DRI_BOOL)) {
+      if (driCheckOption(dri_options, "vk_wsi_force_bgra8_unorm_first", DRI_BOOL)) {
          wsi->force_bgra8_unorm_first =
             driQueryOptionb(dri_options, "vk_wsi_force_bgra8_unorm_first");
       }
 
-      if (driCheckOption(dri_options, "vk_wsi_force_swapchain_to_current_extent",  DRI_BOOL)) {
+      if (driCheckOption(dri_options, "vk_wsi_force_swapchain_to_current_extent", DRI_BOOL)) {
          wsi->force_swapchain_to_currentExtent =
             driQueryOptionb(dri_options, "vk_wsi_force_swapchain_to_current_extent");
       }
 
-      if (driCheckOption(dri_options, "vk_wsi_disable_unordered_submits",  DRI_BOOL)) {
+      if (driCheckOption(dri_options, "vk_wsi_disable_unordered_submits", DRI_BOOL)) {
          wsi->disable_unordered_submits =
             driQueryOptionb(dri_options, "vk_wsi_disable_unordered_submits");
       }
-
    }
 
-   /* can_present_on_device is a function pointer used to determine if images
-    * can be presented directly on a given device file descriptor (fd).
-    * If HAVE_LIBDRM is defined, it will be initialized to a platform-specific
-    * function (wsi_device_matches_drm_fd). Otherwise, it is initialized to
-    * present_false to ensure that it always returns false, preventing potential
-    * segmentation faults from unchecked calls.
-    * Drivers for non-PCI based GPUs are expected to override this after calling
-    * wsi_device_init().
-    */
 #ifdef HAVE_LIBDRM
    wsi->can_present_on_device = wsi_device_matches_drm_fd;
 #else
@@ -306,6 +291,7 @@ wsi_device_init(struct wsi_device *wsi,
 #endif
 
    return VK_SUCCESS;
+
 fail:
    wsi_device_finish(wsi, alloc);
    return result;
@@ -549,36 +535,40 @@ wsi_swapchain_is_present_mode_supported(
                                         const VkSwapchainCreateInfoKHR *pCreateInfo,
                                         VkPresentModeKHR mode)
 {
-      ICD_FROM_HANDLE(VkIcdSurfaceBase, surface, pCreateInfo->surface);
-      struct wsi_interface *iface = wsi->wsi[surface->platform];
-      VkPresentModeKHR *present_modes;
-      uint32_t present_mode_count;
-      bool supported = false;
-      VkResult result;
-
-      result = iface->get_present_modes(surface, wsi, &present_mode_count, NULL);
-      if (result != VK_SUCCESS)
-         return supported;
+   ICD_FROM_HANDLE(VkIcdSurfaceBase, surface, pCreateInfo->surface);
+   struct wsi_interface *iface = wsi->wsi[surface->platform];
+   uint32_t present_mode_count = 0;
+   bool supported = false;
+
+   VkResult result = iface->get_present_modes(surface, wsi, &present_mode_count, NULL);
+   if (result != VK_SUCCESS || present_mode_count == 0)
+      return false;
+
+   VkPresentModeKHR stack_modes[8];
+   VkPresentModeKHR *present_modes = stack_modes;
+   bool use_heap = false;
 
+   if (present_mode_count > ARRAY_SIZE(stack_modes)) {
       present_modes = malloc(present_mode_count * sizeof(*present_modes));
       if (!present_modes)
-         return supported;
-
-      result = iface->get_present_modes(surface, wsi, &present_mode_count,
-                                        present_modes);
-      if (result != VK_SUCCESS)
-         goto fail;
+         return false;
+      use_heap = true;
+   }
 
+   result = iface->get_present_modes(surface, wsi, &present_mode_count, present_modes);
+   if (result == VK_SUCCESS) {
       for (uint32_t i = 0; i < present_mode_count; i++) {
          if (present_modes[i] == mode) {
             supported = true;
             break;
          }
       }
+   }
 
-fail:
+   if (use_heap)
       free(present_modes);
-      return supported;
+
+   return supported;
 }
 
 VkPresentModeKHR
@@ -1926,24 +1916,25 @@ wsi_select_memory_type(const struct wsi_
 {
    assert(type_bits != 0);
 
-   VkMemoryPropertyFlags common_props = ~0;
+   VkMemoryPropertyFlags common_props = ~0u;
+
    u_foreach_bit(t, type_bits) {
-      const VkMemoryType type = wsi->memory_props.memoryTypes[t];
+      const VkMemoryPropertyFlags flags =
+         wsi->memory_props.memoryTypes[t].propertyFlags;
 
-      common_props &= type.propertyFlags;
+      common_props &= flags;
 
-      if (deny_props & type.propertyFlags)
+      if (deny_props & flags)
          continue;
 
-      if (!(req_props & ~type.propertyFlags))
+      if (!(req_props & ~flags))
          return t;
    }
 
    if ((deny_props & VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT) &&
        (common_props & VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT)) {
       /* If they asked for non-device-local and all the types are device-local
-       * (this is commonly true for UMA platforms), try again without denying
-       * device-local types
+       * (common for UMA), try again without denying device-local.
        */
       deny_props &= ~VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT;
       return wsi_select_memory_type(wsi, req_props, deny_props, type_bits);
@@ -1951,7 +1942,7 @@ wsi_select_memory_type(const struct wsi_
 
    if (req_props & VK_MEMORY_PROPERTY_HOST_CACHED_BIT) {
       req_props &= ~VK_MEMORY_PROPERTY_HOST_CACHED_BIT;
-      // fallback to coherent if cached-coherent is requested but not found
+      /* fallback to coherent if cached-coherent is requested but not found */
       return wsi_select_memory_type(wsi, req_props, deny_props, type_bits);
    }
 
