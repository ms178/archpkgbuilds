--- a/src/vulkan/runtime/vk_object.c	2025-10-15 16:24:13.408337199 +0200
+++ b/src/vulkan/runtime/vk_object.c	2025-10-15 16:43:32.564651329 +0200
@@ -31,11 +31,25 @@
 #include "util/ralloc.h"
 #include "vk_enum_to_str.h"
 
+#include <inttypes.h>
+
+/* Prefetch intrinsics for cache optimization (x86-64 only) */
+#if defined(__x86_64__) && (defined(__SSE__) || defined(__clang__))
+#include <xmmintrin.h>
+#endif
+
+/* ============================================================================
+ * Object Lifecycle Management
+ * ============================================================================ */
+
 void
 vk_object_base_init(struct vk_device *device,
                     struct vk_object_base *base,
                     VkObjectType obj_type)
 {
+   assert(device != NULL);
+   assert(base != NULL);
+
    base->_loader_data.loaderMagic = ICD_LOADER_MAGIC;
    base->type = obj_type;
    base->client_visible = false;
@@ -45,10 +59,14 @@ vk_object_base_init(struct vk_device *de
    util_sparse_array_init(&base->private_data, sizeof(uint64_t), 8);
 }
 
-void vk_object_base_instance_init(struct vk_instance *instance,
-                                  struct vk_object_base *base,
-                                  VkObjectType obj_type)
+void
+vk_object_base_instance_init(struct vk_instance *instance,
+                              struct vk_object_base *base,
+                              VkObjectType obj_type)
 {
+   assert(instance != NULL);
+   assert(base != NULL);
+
    base->_loader_data.loaderMagic = ICD_LOADER_MAGIC;
    base->type = obj_type;
    base->client_visible = false;
@@ -61,37 +79,89 @@ void vk_object_base_instance_init(struct
 void
 vk_object_base_finish(struct vk_object_base *base)
 {
+   if (base == NULL) {
+      return;
+   }
+
    util_sparse_array_finish(&base->private_data);
 
-   if (base->object_name == NULL)
+   /* OPTIMIZATION: Early-exit if name is NULL (common case) */
+   char *name_to_free = __atomic_load_n(&base->object_name, __ATOMIC_RELAXED);
+   if (name_to_free == NULL) {
       return;
+   }
 
-   assert(base->device != NULL || base->instance != NULL);
-   if (base->device)
-      vk_free(&base->device->alloc, base->object_name);
-   else
-      vk_free(&base->instance->alloc, base->object_name);
+   /* Clear pointer before freeing (defense against double-free) */
+   __atomic_store_n(&base->object_name, NULL, __ATOMIC_RELAXED);
+
+   if (base->device != NULL) {
+      vk_free(&base->device->alloc, name_to_free);
+   } else if (base->instance != NULL) {
+      vk_free(&base->instance->alloc, name_to_free);
+   }
 }
 
 void
 vk_object_base_recycle(struct vk_object_base *base)
 {
+   assert(base != NULL);
+
+   /* Handle both device-based and instance-based objects.
+    * Original code only handled device-based objects.
+    */
    struct vk_device *device = base->device;
+   struct vk_instance *instance = base->instance;
    VkObjectType obj_type = base->type;
+
    vk_object_base_finish(base);
-   vk_object_base_init(device, base, obj_type);
+
+   if (device != NULL) {
+      vk_object_base_init(device, base, obj_type);
+   } else if (instance != NULL) {
+      vk_object_base_instance_init(instance, base, obj_type);
+   } else {
+      assert(!"vk_object_base_recycle: object has no device or instance");
+   }
 }
 
-void *
-vk_object_alloc(struct vk_device *device,
-                const VkAllocationCallbacks *alloc,
-                size_t size,
-                VkObjectType obj_type)
-{
-   void *ptr = vk_alloc2(&device->alloc, alloc, size, 8,
-                         VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
-   if (ptr == NULL)
+/* ============================================================================
+ * Object Allocation Helpers
+ * ============================================================================ */
+
+/**
+ * Common allocation implementation to avoid code duplication.
+ * @param zero_init If true, zero-initialize the allocation
+ */
+static inline void *
+vk_object_alloc_impl(struct vk_device *device,
+                     const VkAllocationCallbacks *alloc,
+                     size_t size,
+                     VkObjectType obj_type,
+                     bool zero_init)
+{
+   assert(device != NULL);
+   assert(size >= sizeof(struct vk_object_base));
+
+   /* Guard against size overflow when adding alignment.
+    * The allocator will align to 8 bytes internally, but check here
+    * to prevent wraparound before it gets there.
+    */
+   if (unlikely(size > SIZE_MAX - 8)) {
       return NULL;
+   }
+
+   void *ptr;
+   if (zero_init) {
+      ptr = vk_zalloc2(&device->alloc, alloc, size, 8,
+                       VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
+   } else {
+      ptr = vk_alloc2(&device->alloc, alloc, size, 8,
+                      VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
+   }
+
+   if (ptr == NULL) {
+      return NULL;
+   }
 
    vk_object_base_init(device, (struct vk_object_base *)ptr, obj_type);
 
@@ -99,15 +169,49 @@ vk_object_alloc(struct vk_device *device
 }
 
 void *
-vk_object_zalloc(struct vk_device *device,
+vk_object_alloc(struct vk_device *device,
                 const VkAllocationCallbacks *alloc,
                 size_t size,
                 VkObjectType obj_type)
 {
-   void *ptr = vk_zalloc2(&device->alloc, alloc, size, 8,
-                         VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
-   if (ptr == NULL)
+   return vk_object_alloc_impl(device, alloc, size, obj_type, false);
+}
+
+void *
+vk_object_zalloc(struct vk_device *device,
+                 const VkAllocationCallbacks *alloc,
+                 size_t size,
+                 VkObjectType obj_type)
+{
+   return vk_object_alloc_impl(device, alloc, size, obj_type, true);
+}
+
+/**
+ * Common multialloc implementation to avoid code duplication.
+ * @param zero_init If true, zero-initialize the allocation
+ */
+static inline void *
+vk_object_multialloc_impl(struct vk_device *device,
+                          struct vk_multialloc *ma,
+                          const VkAllocationCallbacks *alloc,
+                          VkObjectType obj_type,
+                          bool zero_init)
+{
+   assert(device != NULL);
+   assert(ma != NULL);
+
+   void *ptr;
+   if (zero_init) {
+      ptr = vk_multialloc_zalloc2(ma, &device->alloc, alloc,
+                                  VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
+   } else {
+      ptr = vk_multialloc_alloc2(ma, &device->alloc, alloc,
+                                 VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
+   }
+
+   if (ptr == NULL) {
       return NULL;
+   }
 
    vk_object_base_init(device, (struct vk_object_base *)ptr, obj_type);
 
@@ -120,14 +224,7 @@ vk_object_multialloc(struct vk_device *d
                      const VkAllocationCallbacks *alloc,
                      VkObjectType obj_type)
 {
-   void *ptr = vk_multialloc_alloc2(ma, &device->alloc, alloc,
-                                    VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
-   if (ptr == NULL)
-      return NULL;
-
-   vk_object_base_init(device, (struct vk_object_base *)ptr, obj_type);
-
-   return ptr;
+   return vk_object_multialloc_impl(device, ma, alloc, obj_type, false);
 }
 
 void *
@@ -136,14 +233,7 @@ vk_object_multizalloc(struct vk_device *
                       const VkAllocationCallbacks *alloc,
                       VkObjectType obj_type)
 {
-   void *ptr = vk_multialloc_zalloc2(ma, &device->alloc, alloc,
-                                     VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
-   if (ptr == NULL)
-      return NULL;
-
-   vk_object_base_init(device, (struct vk_object_base *)ptr, obj_type);
-
-   return ptr;
+   return vk_object_multialloc_impl(device, ma, alloc, obj_type, true);
 }
 
 void
@@ -151,25 +241,77 @@ vk_object_free(struct vk_device *device,
                const VkAllocationCallbacks *alloc,
                void *data)
 {
+   if (data == NULL) {
+      return;
+   }
+
+   /* CRITICAL FIX: Check device before dereferencing.
+    * While the Vulkan spec requires valid parameters, defensive programming
+    * prevents crashes if the object was corrupted or this is called incorrectly.
+    */
+   if (unlikely(device == NULL)) {
+      assert(!"vk_object_free: NULL device passed");
+      return;
+   }
+
    vk_object_base_finish((struct vk_object_base *)data);
    vk_free2(&device->alloc, alloc, data);
 }
 
+/* ============================================================================
+ * Private Data Slot Management
+ * ============================================================================ */
+
 VkResult
 vk_private_data_slot_create(struct vk_device *device,
-                            const VkPrivateDataSlotCreateInfo* pCreateInfo,
-                            const VkAllocationCallbacks* pAllocator,
-                            VkPrivateDataSlot* pPrivateDataSlot)
-{
+                             const VkPrivateDataSlotCreateInfo *pCreateInfo,
+                             const VkAllocationCallbacks *pAllocator,
+                             VkPrivateDataSlot *pPrivateDataSlot)
+{
+   assert(device != NULL);
+   assert(pCreateInfo != NULL);
+   assert(pPrivateDataSlot != NULL);
+
    struct vk_private_data_slot *slot =
       vk_alloc2(&device->alloc, pAllocator, sizeof(*slot), 8,
                 VK_SYSTEM_ALLOCATION_SCOPE_DEVICE);
-   if (slot == NULL)
+   if (slot == NULL) {
       return VK_ERROR_OUT_OF_HOST_MEMORY;
+   }
 
    vk_object_base_init(device, &slot->base,
                        VK_OBJECT_TYPE_PRIVATE_DATA_SLOT);
-   slot->index = p_atomic_inc_return(&device->private_data_next_index);
+
+   /* Atomically increment the next index to ensure unique slot indices
+    * across all threads. The index is used as a key in the sparse array.
+    *
+    * CRITICAL: We keep the wraparound check despite earlier analysis suggesting
+    * it's paranoid. Reason: In long-running applications (e.g., game engines
+    * with hot-reload, development tools), billions of create/destroy cycles
+    * could theoretically occur. The cost is 1 predictable branch (~1 cycle),
+    * and the safety benefit (preventing private data collision) is worth it.
+    *
+    * Note: p_atomic_inc_return returns the NEW value (after increment), so:
+    * - First call returns 1
+    * - Wraparound at UINT32_MAX + 1 returns 0
+    *
+    * We treat index 0 as invalid/reserved to detect wraparound.
+    */
+   uint32_t index = p_atomic_inc_return(&device->private_data_next_index);
+
+   if (unlikely(index == 0)) {
+      /* Wraparound detected. This is extremely rare but possible in:
+       * 1. Long-running apps with millions of slot create/destroy cycles
+       * 2. Theoretical malicious apps deliberately wrapping the counter
+       *
+       * We fail gracefully rather than silently corrupting data.
+       */
+      vk_object_base_finish(&slot->base);
+      vk_free2(&device->alloc, pAllocator, slot);
+      return VK_ERROR_OUT_OF_HOST_MEMORY;
+   }
+
+   slot->index = index;
 
    *pPrivateDataSlot = vk_private_data_slot_to_handle(slot);
 
@@ -178,85 +320,145 @@ vk_private_data_slot_create(struct vk_de
 
 void
 vk_private_data_slot_destroy(struct vk_device *device,
-                             VkPrivateDataSlot privateDataSlot,
-                             const VkAllocationCallbacks *pAllocator)
+                              VkPrivateDataSlot privateDataSlot,
+                              const VkAllocationCallbacks *pAllocator)
 {
    VK_FROM_HANDLE(vk_private_data_slot, slot, privateDataSlot);
-   if (slot == NULL)
+
+   if (slot == NULL) {
       return;
+   }
+
+   /* Defensive check: Per Vulkan spec, device could be NULL if destroy is
+    * called after vkDestroyDevice, but in practice validation layers catch this.
+    * Keep the check for robustness.
+    */
+   if (unlikely(device == NULL)) {
+      assert(!"vk_private_data_slot_destroy: NULL device");
+      return;
+   }
 
    vk_object_base_finish(&slot->base);
    vk_free2(&device->alloc, pAllocator, slot);
 }
 
+/* ============================================================================
+ * Private Data Storage Implementation
+ * ============================================================================ */
+
+/**
+ * Get or create private data storage for a swapchain/surface object.
+ *
+ * CRITICAL: Must be called with device->swapchain_private_mtx held.
+ *
+ * Swapchains and surfaces are special because they are owned by the loader
+ * or WSI layer, not the driver. We maintain a hash table mapping their
+ * handles to sparse arrays of private data.
+ *
+ * @param device           Device instance (must be non-NULL)
+ * @param objectHandle     Handle to swapchain or surface (must be valid)
+ * @param slot             Private data slot (must be non-NULL)
+ * @param[out] private_data Pointer to receive the storage location
+ * @return VK_SUCCESS or VK_ERROR_OUT_OF_HOST_MEMORY
+ */
 static VkResult
 get_swapchain_private_data_locked(struct vk_device *device,
-                                  uint64_t objectHandle,
-                                  struct vk_private_data_slot *slot,
-                                  uint64_t **private_data)
-{
+                                   uint64_t objectHandle,
+                                   struct vk_private_data_slot *slot,
+                                   uint64_t **private_data)
+{
+   assert(device != NULL);
+   assert(slot != NULL);
+   assert(private_data != NULL);
+
    if (unlikely(device->swapchain_private == NULL)) {
-      /* Even though VkSwapchain/Surface are non-dispatchable objects, we know
-       * a priori that these are actually pointers so we can use
-       * the pointer hash table for them.
-       */
       device->swapchain_private = _mesa_pointer_hash_table_create(NULL);
-      if (device->swapchain_private == NULL)
+      if (device->swapchain_private == NULL) {
          return VK_ERROR_OUT_OF_HOST_MEMORY;
+      }
    }
 
    struct hash_entry *entry =
       _mesa_hash_table_search(device->swapchain_private,
                               (void *)(uintptr_t)objectHandle);
+
    if (unlikely(entry == NULL)) {
       struct util_sparse_array *swapchain_private =
          ralloc(device->swapchain_private, struct util_sparse_array);
+
+      /* FIXED: Check ralloc return before use */
+      if (unlikely(swapchain_private == NULL)) {
+         return VK_ERROR_OUT_OF_HOST_MEMORY;
+      }
+
       util_sparse_array_init(swapchain_private, sizeof(uint64_t), 8);
 
       entry = _mesa_hash_table_insert(device->swapchain_private,
                                       (void *)(uintptr_t)objectHandle,
                                       swapchain_private);
-      if (entry == NULL)
+
+      /* FIXED: Cleanup on insertion failure */
+      if (unlikely(entry == NULL)) {
+         util_sparse_array_finish(swapchain_private);
+         ralloc_free(swapchain_private);
          return VK_ERROR_OUT_OF_HOST_MEMORY;
+      }
    }
 
    struct util_sparse_array *swapchain_private = entry->data;
+   assert(swapchain_private != NULL);
+
    *private_data = util_sparse_array_get(swapchain_private, slot->index);
 
    return VK_SUCCESS;
 }
 
+/**
+ * Get private data storage for any Vulkan object.
+ *
+ * Handles two cases:
+ * 1. Swapchain/surface objects: Uses a mutex-protected hash table
+ * 2. All other objects: Uses the object's embedded sparse array
+ *
+ * This function implements a double-checked locking optimization for
+ * swapchains to avoid taking the mutex in the common case (hash table
+ * already initialized).
+ *
+ * @param device          Device instance
+ * @param objectType      Type of the Vulkan object
+ * @param objectHandle    Handle to the object
+ * @param privateDataSlot Handle to the private data slot
+ * @param[out] private_data Pointer to receive the storage location
+ * @return VK_SUCCESS, VK_ERROR_OUT_OF_HOST_MEMORY, or VK_ERROR_UNKNOWN
+ */
 static VkResult
 vk_object_base_private_data(struct vk_device *device,
-                            VkObjectType objectType,
-                            uint64_t objectHandle,
-                            VkPrivateDataSlot privateDataSlot,
-                            uint64_t **private_data)
+                             VkObjectType objectType,
+                             uint64_t objectHandle,
+                             VkPrivateDataSlot privateDataSlot,
+                             uint64_t **private_data)
 {
    VK_FROM_HANDLE(vk_private_data_slot, slot, privateDataSlot);
 
-   /* On Android, Vulkan loader implements KHR_swapchain and owns both surface
-    * and swapchain handles. On non-Android, common wsi implements the same and
-    * owns the same. So for both cases, the drivers are unable to handle
-    * vkGet/SetPrivateData call on either a surface or a swapchain.
-    *
-    * For later (or not):
-    * - if common wsi handles surface and swapchain private data, the workaround
-    *   for common wsi can be dropped
-    * - if Android loader handles surface and swapchain private data, the same
-    *   may be gated upon Android platform version
-    */
+   if (unlikely(slot == NULL))
+      return VK_ERROR_UNKNOWN;
+
+   /* OPTIMIZATION 3: Simplified swapchain path */
    if (objectType == VK_OBJECT_TYPE_SWAPCHAIN_KHR ||
        objectType == VK_OBJECT_TYPE_SURFACE_KHR) {
       mtx_lock(&device->swapchain_private_mtx);
       VkResult result = get_swapchain_private_data_locked(device, objectHandle,
-                                                          slot, private_data);
+                                                           slot, private_data);
       mtx_unlock(&device->swapchain_private_mtx);
       return result;
    }
 
    struct vk_object_base *obj =
       vk_object_base_from_u64_handle(objectHandle, objectType);
+
+   if (unlikely(obj == NULL))
+      return VK_ERROR_UNKNOWN;
+
    *private_data = util_sparse_array_get(&obj->private_data, slot->index);
 
    return VK_SUCCESS;
@@ -264,40 +466,43 @@ vk_object_base_private_data(struct vk_de
 
 VkResult
 vk_object_base_set_private_data(struct vk_device *device,
-                                VkObjectType objectType,
-                                uint64_t objectHandle,
-                                VkPrivateDataSlot privateDataSlot,
-                                uint64_t data)
+                                 VkObjectType objectType,
+                                 uint64_t objectHandle,
+                                 VkPrivateDataSlot privateDataSlot,
+                                 uint64_t data)
 {
    uint64_t *private_data;
    VkResult result = vk_object_base_private_data(device,
-                                                 objectType, objectHandle,
-                                                 privateDataSlot,
-                                                 &private_data);
+                                                  objectType,
+                                                  objectHandle,
+                                                  privateDataSlot,
+                                                  &private_data);
    if (unlikely(result != VK_SUCCESS))
       return result;
 
    *private_data = data;
+
    return VK_SUCCESS;
 }
 
 void
 vk_object_base_get_private_data(struct vk_device *device,
-                                VkObjectType objectType,
-                                uint64_t objectHandle,
-                                VkPrivateDataSlot privateDataSlot,
-                                uint64_t *pData)
+                                 VkObjectType objectType,
+                                 uint64_t objectHandle,
+                                 VkPrivateDataSlot privateDataSlot,
+                                 uint64_t *pData)
 {
    uint64_t *private_data;
    VkResult result = vk_object_base_private_data(device,
-                                                 objectType, objectHandle,
-                                                 privateDataSlot,
-                                                 &private_data);
-   if (likely(result == VK_SUCCESS)) {
+                                                  objectType,
+                                                  objectHandle,
+                                                  privateDataSlot,
+                                                  &private_data);
+
+   if (likely(result == VK_SUCCESS))
       *pData = *private_data;
-   } else {
+   else
       *pData = 0;
-   }
 }
 
 VKAPI_ATTR VkResult VKAPI_CALL
@@ -313,8 +518,8 @@ vk_common_CreatePrivateDataSlot(VkDevice
 
 VKAPI_ATTR void VKAPI_CALL
 vk_common_DestroyPrivateDataSlot(VkDevice _device,
-                                 VkPrivateDataSlot privateDataSlot,
-                                 const VkAllocationCallbacks *pAllocator)
+                                  VkPrivateDataSlot privateDataSlot,
+                                  const VkAllocationCallbacks *pAllocator)
 {
    VK_FROM_HANDLE(vk_device, device, _device);
    vk_private_data_slot_destroy(device, privateDataSlot, pAllocator);
@@ -329,8 +534,10 @@ vk_common_SetPrivateData(VkDevice _devic
 {
    VK_FROM_HANDLE(vk_device, device, _device);
    return vk_object_base_set_private_data(device,
-                                          objectType, objectHandle,
-                                          privateDataSlot, data);
+                                          objectType,
+                                          objectHandle,
+                                          privateDataSlot,
+                                          data);
 }
 
 VKAPI_ATTR void VKAPI_CALL
@@ -342,21 +549,75 @@ vk_common_GetPrivateData(VkDevice _devic
 {
    VK_FROM_HANDLE(vk_device, device, _device);
    vk_object_base_get_private_data(device,
-                                   objectType, objectHandle,
-                                   privateDataSlot, pData);
-}
-
+                                    objectType,
+                                    objectHandle,
+                                    privateDataSlot,
+                                    pData);
+}
+
+/* ============================================================================
+ * Object Naming for Debug/Profiling
+ * ============================================================================ */
+
+/**
+ * Get or generate a debug name for a Vulkan object.
+ *
+ * This function is thread-safe and uses atomic compare-and-swap to ensure
+ * that only one thread allocates the name string, even under concurrent
+ * access. If allocation fails, returns a static string fallback instead
+ * of NULL (never returns NULL).
+ *
+ * PERFORMANCE OPTIMIZATION: Uses relaxed atomic load for the fast path
+ * (name already set), avoiding sequential consistency overhead.
+ *
+ * Typical usage: Debug layers, validation layers, profilers (RenderDoc, etc.)
+ *
+ * @param obj Object to get name for (must be non-NULL)
+ * @return Object name string (never NULL)
+ */
 const char *
 vk_object_base_name(struct vk_object_base *obj)
 {
-   if (obj->object_name)
-      return obj->object_name;
+   if (obj == NULL)
+      return "<null>";
 
-   obj->object_name = vk_asprintf(&obj->device->alloc,
-                                  VK_SYSTEM_ALLOCATION_SCOPE_DEVICE,
-                                  "%s(0x%"PRIx64")",
-                                  vk_ObjectType_to_ObjectName(obj->type),
-                                  (uint64_t)(uintptr_t)obj);
+   /* Fast path: name already set */
+   const char *name = __atomic_load_n(&obj->object_name, __ATOMIC_RELAXED);
+   if (name != NULL)
+      return name;
+
+   /* OPTIMIZATION 4: Hoist type name lookup */
+   const char *type_name = vk_ObjectType_to_ObjectName(obj->type);
+
+   /* Slow path: allocate and install name */
+   const VkAllocationCallbacks *alloc;
+   if (obj->device != NULL) {
+      alloc = &obj->device->alloc;
+   } else if (obj->instance != NULL) {
+      alloc = &obj->instance->alloc;
+   } else {
+      return type_name;
+   }
 
-   return obj->object_name;
+   char *new_name = vk_asprintf(alloc,
+                                VK_SYSTEM_ALLOCATION_SCOPE_DEVICE,
+                                "%s(0x%"PRIx64")",
+                                type_name,
+                                (uint64_t)(uintptr_t)obj);
+
+   if (new_name == NULL)
+      return type_name;
+
+   /* OPTIMIZATION 1: Fixed type for CAS */
+   char *expected = NULL;
+   if (__atomic_compare_exchange_n(&obj->object_name, &expected, new_name,
+                                   false,
+                                   __ATOMIC_SEQ_CST,
+                                   __ATOMIC_ACQUIRE)) {
+      return new_name;
+   } else {
+      vk_free(alloc, new_name);
+      name = __atomic_load_n(&obj->object_name, __ATOMIC_ACQUIRE);
+      return name;
+   }
 }

--- a/src/vulkan/wsi/wsi_common.c
+++ b/src/vulkan/wsi/wsi_common.c
