From 8b898f40a53589c211663362e058850345a24766 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Fri, 11 Nov 2022 18:46:40 +0800
Subject: [PATCH 1/5] ac/nir/ngg: reduce nogs 16bit output gather space

Max slot number for 16bit output is 16, so no need to use
64 array size for them.

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/common/ac_nir_lower_ngg.c | 20 ++++++++++----------
 1 file changed, 10 insertions(+), 10 deletions(-)

diff --git a/src/amd/common/ac_nir_lower_ngg.c b/src/amd/common/ac_nir_lower_ngg.c
index 9e44b4a435a3..d7c04f685eab 100644
--- a/src/amd/common/ac_nir_lower_ngg.c
+++ b/src/amd/common/ac_nir_lower_ngg.c
@@ -1940,12 +1940,12 @@ gather_vs_outputs(nir_builder *b, struct exec_list *cf_list, vs_output *outputs,
                   const uint8_t *vs_output_param_offset)
 {
    uint64_t output_mask32 = 0;
-   nir_ssa_def *outputs32[64][4] = {0};
+   nir_ssa_def *outputs32[VARYING_SLOT_MAX][4] = {0};
 
-   uint64_t output_mask_lo = 0;
-   uint64_t output_mask_hi = 0;
-   nir_ssa_def *outputs_lo[64][4];
-   nir_ssa_def *outputs_hi[64][4];
+   unsigned output_mask_lo = 0;
+   unsigned output_mask_hi = 0;
+   nir_ssa_def *outputs_lo[16][4];
+   nir_ssa_def *outputs_hi[16][4];
 
    /* Assume:
     * - the shader used nir_lower_io_to_temporaries
@@ -1984,9 +1984,9 @@ gather_vs_outputs(nir_builder *b, struct exec_list *cf_list, vs_output *outputs,
          }
 
          if (is_16bit && is_hi)
-            output_mask_hi |= BITFIELD64_BIT(slot - VARYING_SLOT_VAR0_16BIT);
+            output_mask_hi |= BITFIELD_BIT(slot - VARYING_SLOT_VAR0_16BIT);
          else if (is_16bit)
-            output_mask_lo |= BITFIELD64_BIT(slot - VARYING_SLOT_VAR0_16BIT);
+            output_mask_lo |= BITFIELD_BIT(slot - VARYING_SLOT_VAR0_16BIT);
          else
             output_mask32 |= BITFIELD64_BIT(slot);
 
@@ -2008,13 +2008,13 @@ gather_vs_outputs(nir_builder *b, struct exec_list *cf_list, vs_output *outputs,
 
    if (output_mask_lo | output_mask_hi) {
       nir_ssa_def *undef = nir_ssa_undef(b, 1, 16);
-      u_foreach_bit64 (i, output_mask_lo | output_mask_hi) {
+      u_foreach_bit (i, output_mask_lo | output_mask_hi) {
          vs_output *output = &outputs[num_outputs++];
 
          output->slot = i + VARYING_SLOT_VAR0_16BIT;
          for (unsigned j = 0; j < 4; j++) {
-            nir_ssa_def *lo = output_mask_lo & BITFIELD64_BIT(i) ? outputs_lo[i][j] : NULL;
-            nir_ssa_def *hi = output_mask_hi & BITFIELD64_BIT(i) ? outputs_hi[i][j] : NULL;
+            nir_ssa_def *lo = output_mask_lo & BITFIELD_BIT(i) ? outputs_lo[i][j] : NULL;
+            nir_ssa_def *hi = output_mask_hi & BITFIELD_BIT(i) ? outputs_hi[i][j] : NULL;
             if (lo || hi)
                output->chan[j] = nir_pack_32_2x16_split(b, lo ? lo : undef, hi ? hi : undef);
             else
-- 
GitLab


From 9123af55d44b1bc4829dcfdbf979022a65feb939 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Sat, 12 Nov 2022 21:24:08 +0800
Subject: [PATCH 2/5] ac/nir/ngg: gs use u_foreach_bit64 to loop all output
 slots

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/common/ac_nir_lower_ngg.c | 7 ++-----
 1 file changed, 2 insertions(+), 5 deletions(-)

diff --git a/src/amd/common/ac_nir_lower_ngg.c b/src/amd/common/ac_nir_lower_ngg.c
index d7c04f685eab..23bbed1305da 100644
--- a/src/amd/common/ac_nir_lower_ngg.c
+++ b/src/amd/common/ac_nir_lower_ngg.c
@@ -2560,7 +2560,7 @@ lower_ngg_gs_emit_vertex_with_counter(nir_builder *b, nir_intrinsic_instr *intri
    nir_ssa_def *current_vtx_per_prim = intrin->src[1].ssa;
    nir_ssa_def *gs_emit_vtx_addr = ngg_gs_emit_vertex_addr(b, gs_emit_vtx_idx, s);
 
-   for (unsigned slot = 0; slot < VARYING_SLOT_MAX; ++slot) {
+   u_foreach_bit64(slot, b->shader->info.outputs_written) {
       unsigned packed_location = util_bitcount64((b->shader->info.outputs_written & BITFIELD64_MASK(slot)));
       gs_output_info *info = &s->output_info[slot];
 
@@ -2743,10 +2743,7 @@ ngg_gs_export_vertices(nir_builder *b, nir_ssa_def *max_num_out_vtx, nir_ssa_def
    unsigned num_outputs = 0;
    vs_output outputs[64];
 
-   for (unsigned slot = 0; slot < VARYING_SLOT_MAX; ++slot) {
-      if (!(b->shader->info.outputs_written & BITFIELD64_BIT(slot)))
-         continue;
-
+   u_foreach_bit64(slot, b->shader->info.outputs_written) {
       gs_output_info *info = &s->output_info[slot];
       unsigned mask = gs_output_component_mask_with_stream(info, 0);
       if (!mask)
-- 
GitLab


From 543e2ec072c707dc9fc309bd9e4ba10687a53b53 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Sun, 13 Nov 2022 16:30:39 +0800
Subject: [PATCH 3/5] ac/nir/ngg: gs store output use src_type index for type
 info

More precise type info, can be used for 16bit output streamout
to convert 16bit int/uint/float to 32bit one later.

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/common/ac_nir_lower_ngg.c | 8 +++++---
 1 file changed, 5 insertions(+), 3 deletions(-)

diff --git a/src/amd/common/ac_nir_lower_ngg.c b/src/amd/common/ac_nir_lower_ngg.c
index 23bbed1305da..0cf5cf400c8b 100644
--- a/src/amd/common/ac_nir_lower_ngg.c
+++ b/src/amd/common/ac_nir_lower_ngg.c
@@ -2477,6 +2477,8 @@ lower_ngg_gs_store_output(nir_builder *b, nir_intrinsic_instr *intrin, lower_ngg
    assert(base_index < VARYING_SLOT_MAX);
 
    nir_ssa_def *store_val = intrin->src[0].ssa;
+   nir_alu_type src_type = nir_intrinsic_src_type(intrin);
+   enum glsl_base_type val_type = nir_get_glsl_base_type_for_nir_type(src_type);
 
    /* Small bitsize components consume the same amount of space as 32-bit components,
     * but 64-bit ones consume twice as many. (Vulkan spec 15.1.5)
@@ -2484,6 +2486,7 @@ lower_ngg_gs_store_output(nir_builder *b, nir_intrinsic_instr *intrin, lower_ngg
     * 64-bit IO has been lowered to multi 32-bit IO.
     */
    assert(store_val->bit_size <= 32);
+   assert(glsl_base_type_get_bit_size(val_type) == store_val->bit_size);
 
    /* Save output usage info. */
    gs_output_info *info = &s->output_info[location];
@@ -2516,11 +2519,10 @@ lower_ngg_gs_store_output(nir_builder *b, nir_intrinsic_instr *intrin, lower_ngg
 
       nir_variable *var = s->output_vars[location][component];
       if (!var) {
-         var = nir_local_variable_create(
-            s->impl, glsl_uintN_t_type(store_val->bit_size), "output");
+         var = nir_local_variable_create(s->impl, glsl_scalar_type(val_type), "output");
          s->output_vars[location][component] = var;
       }
-      assert(glsl_base_type_bit_size(glsl_get_base_type(var->type)) == store_val->bit_size);
+      assert(glsl_get_base_type(var->type) == val_type);
 
       nir_store_var(b, var, nir_channel(b, store_val, comp), 0x1u);
    }
-- 
GitLab


From 4eae184736d7ec8b79b8ac4006a242d2f9a1e353 Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Sun, 13 Nov 2022 16:36:26 +0800
Subject: [PATCH 4/5] ac/nir/ngg: gs skip check bit size before nir_u2u

nir_u2u do for us.

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/common/ac_nir_lower_ngg.c | 8 +++-----
 1 file changed, 3 insertions(+), 5 deletions(-)

diff --git a/src/amd/common/ac_nir_lower_ngg.c b/src/amd/common/ac_nir_lower_ngg.c
index 0cf5cf400c8b..698e88bf2ed7 100644
--- a/src/amd/common/ac_nir_lower_ngg.c
+++ b/src/amd/common/ac_nir_lower_ngg.c
@@ -2586,11 +2586,10 @@ lower_ngg_gs_emit_vertex_with_counter(nir_builder *b, nir_intrinsic_instr *intri
             nir_ssa_def *val = nir_load_var(b, var);
 
             /* extend 8/16 bit to 32 bit, 64 bit has been lowered */
-            unsigned bit_size = glsl_base_type_bit_size(glsl_get_base_type(var->type));
-            values[c - start] = bit_size == 32 ? val : nir_u2u32(b, val);
+            values[c - start] = nir_u2u(b, val, 32);
 
             /* Clear the variable (it is undefined after emit_vertex) */
-            nir_store_var(b, s->output_vars[slot][c], nir_ssa_undef(b, 1, bit_size), 0x1);
+            nir_store_var(b, s->output_vars[slot][c], nir_ssa_undef(b, 1, val->bit_size), 0x1);
          }
 
          nir_ssa_def *store_val = nir_vec(b, values, (unsigned)count);
@@ -2793,8 +2792,7 @@ ngg_gs_export_vertices(nir_builder *b, nir_ssa_def *max_num_out_vtx, nir_ssa_def
             if (s->options->gfx_level < GFX11 || is_pos) {
                /* Convert to the expected bit size of the output variable. */
                unsigned bit_size = glsl_base_type_bit_size(glsl_get_base_type(var->type));
-               if (bit_size != 32)
-                  val = nir_u2u(b, val, bit_size);
+               val = nir_u2u(b, val, bit_size);
 
                nir_store_output(b, val, nir_imm_int(b, 0), .base = info->base,
                                 .io_semantics = io_sem, .component = start + i, .write_mask = 1);
-- 
GitLab


From 08fc4903bd590e89ac6d7eddd541765371f5046b Mon Sep 17 00:00:00 2001
From: Qiang Yu <yuq825@gmail.com>
Date: Sat, 12 Nov 2022 11:58:03 +0800
Subject: [PATCH 5/5] ac/nir/ngg: gs support 16bit outputs

radeonsi uses 16bit varying slots.

Signed-off-by: Qiang Yu <yuq825@gmail.com>
---
 src/amd/common/ac_nir_lower_ngg.c | 165 +++++++++++++++++++++++++++---
 1 file changed, 152 insertions(+), 13 deletions(-)

diff --git a/src/amd/common/ac_nir_lower_ngg.c b/src/amd/common/ac_nir_lower_ngg.c
index 698e88bf2ed7..879bff40854d 100644
--- a/src/amd/common/ac_nir_lower_ngg.c
+++ b/src/amd/common/ac_nir_lower_ngg.c
@@ -111,7 +111,6 @@ typedef struct
    const ac_nir_lower_ngg_options *options;
 
    nir_function_impl *impl;
-   nir_variable *output_vars[VARYING_SLOT_MAX][4];
    nir_variable *current_clear_primflag_idx_var;
    int const_out_vtxcnt[4];
    int const_out_prmcnt[4];
@@ -124,7 +123,14 @@ typedef struct
    bool found_out_vtxcnt[4];
    bool output_compile_time_known;
    bool streamout_enabled;
+   /* 32 bit outputs */
+   nir_variable *output_vars[VARYING_SLOT_MAX][4];
    gs_output_info output_info[VARYING_SLOT_MAX];
+   /* 16 bit outputs */
+   nir_variable *output_vars_hi[16][4];
+   nir_variable *output_vars_lo[16][4];
+   gs_output_info output_info_hi[16];
+   gs_output_info output_info_lo[16];
 } lower_ngg_gs_state;
 
 /* LDS layout of Mesh Shader workgroup info. */
@@ -2471,10 +2477,7 @@ lower_ngg_gs_store_output(nir_builder *b, nir_intrinsic_instr *intrin, lower_ngg
    nir_io_semantics io_sem = nir_intrinsic_io_semantics(intrin);
 
    unsigned location = io_sem.location + base_offset;
-   assert(location < VARYING_SLOT_MAX);
-
    unsigned base_index = base + base_offset;
-   assert(base_index < VARYING_SLOT_MAX);
 
    nir_ssa_def *store_val = intrin->src[0].ssa;
    nir_alu_type src_type = nir_intrinsic_src_type(intrin);
@@ -2488,8 +2491,25 @@ lower_ngg_gs_store_output(nir_builder *b, nir_intrinsic_instr *intrin, lower_ngg
    assert(store_val->bit_size <= 32);
    assert(glsl_base_type_get_bit_size(val_type) == store_val->bit_size);
 
-   /* Save output usage info. */
-   gs_output_info *info = &s->output_info[location];
+   /* Get corresponding output variable and usage info. */
+   nir_variable **var;
+   gs_output_info *info;
+   if (location >= VARYING_SLOT_VAR0_16BIT) {
+      unsigned index = location - VARYING_SLOT_VAR0_16BIT;
+      assert(index < 16);
+
+      if (io_sem.high_16bits) {
+         var = s->output_vars_hi[index];
+         info = s->output_info_hi + index;
+      } else {
+         var = s->output_vars_lo[index];
+         info = s->output_info_lo + index;
+      }
+   } else {
+      assert(location < VARYING_SLOT_MAX);
+      var = s->output_vars[location];
+      info = s->output_info + location;
+   }
 
    for (unsigned comp = 0; comp < store_val->num_components; ++comp) {
       if (!(writemask & (1 << comp)))
@@ -2517,14 +2537,13 @@ lower_ngg_gs_store_output(nir_builder *b, nir_intrinsic_instr *intrin, lower_ngg
       info->no_varying = io_sem.no_varying;
       info->no_sysval_output = io_sem.no_sysval_output;
 
-      nir_variable *var = s->output_vars[location][component];
-      if (!var) {
-         var = nir_local_variable_create(s->impl, glsl_scalar_type(val_type), "output");
-         s->output_vars[location][component] = var;
+      if (!var[component]) {
+         var[component] =
+            nir_local_variable_create(s->impl, glsl_scalar_type(val_type), "output");
       }
-      assert(glsl_get_base_type(var->type) == val_type);
+      assert(glsl_get_base_type(var[component]->type) == val_type);
 
-      nir_store_var(b, var, nir_channel(b, store_val, comp), 0x1u);
+      nir_store_var(b, var[component], nir_channel(b, store_val, comp), 0x1u);
    }
 
    nir_instr_remove(&intrin->instr);
@@ -2599,6 +2618,51 @@ lower_ngg_gs_emit_vertex_with_counter(nir_builder *b, nir_intrinsic_instr *intri
       }
    }
 
+   /* Store 16bit outputs to LDS. */
+   unsigned num_32bit_outputs = util_bitcount64(b->shader->info.outputs_written);
+   u_foreach_bit(slot, b->shader->info.outputs_written_16bit) {
+      unsigned packed_location = num_32bit_outputs +
+         util_bitcount(b->shader->info.outputs_written_16bit & BITFIELD_MASK(slot));
+
+      unsigned mask_lo = gs_output_component_mask_with_stream(s->output_info_lo + slot, stream);
+      unsigned mask_hi = gs_output_component_mask_with_stream(s->output_info_hi + slot, stream);
+      unsigned mask = mask_lo | mask_hi;
+      if (!mask)
+         continue;
+
+      nir_ssa_def *undef = nir_ssa_undef(b, 1, 16);
+
+      while (mask) {
+         int start, count;
+         u_bit_scan_consecutive_range(&mask, &start, &count);
+         nir_ssa_def *values[4] = {0};
+         for (int c = start; c < start + count; ++c) {
+            /* Load and reset the low half var. */
+            nir_ssa_def *lo = undef;
+            nir_variable *var_lo = s->output_vars_lo[slot][c];
+            if (var_lo) {
+               lo = nir_load_var(b, var_lo);
+               nir_store_var(b, var_lo, undef, 1);
+            }
+
+            /* Load and reset the high half var.*/
+            nir_ssa_def *hi = undef;
+            nir_variable *var_hi = s->output_vars_hi[slot][c];
+            if (var_hi) {
+               hi = nir_load_var(b, var_hi);
+               nir_store_var(b, var_hi, undef, 1);
+            }
+
+            values[c - start] = nir_pack_32_2x16_split(b, lo, hi);
+         }
+
+         nir_ssa_def *store_val = nir_vec(b, values, (unsigned)count);
+         nir_store_shared(b, store_val, gs_emit_vtx_addr,
+                          .base = packed_location * 16 + start * 4,
+                          .align_mul = 4);
+      }
+   }
+
    /* Calculate and store per-vertex primitive flags based on vertex counts:
     * - bit 0: whether this vertex finishes a primitive (a real primitive, not the strip)
     * - bit 1: whether the primitive index is odd (if we are emitting triangle strips, otherwise always 0)
@@ -2742,7 +2806,8 @@ ngg_gs_export_vertices(nir_builder *b, nir_ssa_def *max_num_out_vtx, nir_ssa_def
    }
 
    unsigned num_outputs = 0;
-   vs_output outputs[64];
+   /* 16 is for 16bit slots */
+   vs_output outputs[VARYING_SLOT_MAX + 16];
 
    u_foreach_bit64(slot, b->shader->info.outputs_written) {
       gs_output_info *info = &s->output_info[slot];
@@ -2803,6 +2868,80 @@ ngg_gs_export_vertices(nir_builder *b, nir_ssa_def *max_num_out_vtx, nir_ssa_def
       }
    }
 
+   /* 16bit outputs */
+   unsigned num_32bit_outputs = util_bitcount64(b->shader->info.outputs_written);
+   u_foreach_bit(i, b->shader->info.outputs_written_16bit) {
+      unsigned packed_location = num_32bit_outputs +
+         util_bitcount(b->shader->info.outputs_written_16bit & BITFIELD_MASK(i));
+      unsigned slot = VARYING_SLOT_VAR0_16BIT + i;
+
+      gs_output_info *info_lo = s->output_info_lo + i;
+      gs_output_info *info_hi = s->output_info_hi + i;
+      unsigned mask_lo = info_lo->no_varying ? 0 :
+         gs_output_component_mask_with_stream(info_lo, 0);
+      unsigned mask_hi = info_hi->no_varying ? 0 :
+         gs_output_component_mask_with_stream(info_hi, 0);
+      unsigned mask = mask_lo | mask_hi;
+      if (!mask)
+         continue;
+
+      nir_io_semantics io_sem_lo = {
+         .location = slot,
+         .num_slots = 1,
+         .no_varying = info_lo->no_varying,
+      };
+      nir_io_semantics io_sem_hi = {
+         .location = slot,
+         .num_slots = 1,
+         .no_varying = info_hi->no_varying,
+         .high_16bits = true,
+      };
+
+      vs_output *output = NULL;
+      if (s->options->gfx_level >= GFX11 &&
+          s->options->vs_output_param_offset[slot] <= AC_EXP_PARAM_OFFSET_31) {
+         output = &outputs[num_outputs++];
+         output->slot = slot;
+      }
+
+      while (mask) {
+         int start, count;
+         u_bit_scan_consecutive_range(&mask, &start, &count);
+         nir_ssa_def *load =
+            nir_load_shared(b, count, 32, exported_out_vtx_lds_addr,
+                            .base = packed_location * 16 + start * 4,
+                            .align_mul = 4);
+
+         for (int i = 0; i < count; i++) {
+            nir_ssa_def *val = nir_channel(b, load, i);
+            unsigned comp = start + i;
+
+            if (output) {
+               /* low and high varyings have been packed when LDS store */
+               output->chan[comp] = val;
+            } else {
+               if (mask_lo & BITFIELD_BIT(comp)) {
+                  nir_store_output(b, nir_unpack_32_2x16_split_x(b, val),
+                                   nir_imm_int(b, 0),
+                                   .base = info_lo->base,
+                                   .io_semantics = io_sem_lo,
+                                   .component = comp,
+                                   .write_mask = 1);
+               }
+
+               if (mask_hi & BITFIELD_BIT(comp)) {
+                  nir_store_output(b, nir_unpack_32_2x16_split_y(b, val),
+                                   nir_imm_int(b, 0),
+                                   .base = info_hi->base,
+                                   .io_semantics = io_sem_hi,
+                                   .component = comp,
+                                   .write_mask = 1);
+               }
+            }
+         }
+      }
+   }
+
    nir_export_vertex_amd(b);
    nir_pop_if(b, if_vtx_export_thread);
 
-- 
GitLab

