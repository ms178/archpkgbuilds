From 4f2c53ede17947161239d8b99b000c6bd4cbfec5 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Wed, 12 Apr 2023 20:12:26 +0100
Subject: [PATCH 01/15] nir/fold_16bit_tex_image: skip tex instructions with
 backend1
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

This will be used for RADV/ACO in the future, and I don't want to and
don't have to deal with 16-bit.

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
Reviewed-by: Marek Olšák <marek.olsak@amd.com>
---
 src/compiler/nir/nir_lower_mediump.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/src/compiler/nir/nir_lower_mediump.c b/src/compiler/nir/nir_lower_mediump.c
index e97eb5761461..168698c9f7dd 100644
--- a/src/compiler/nir/nir_lower_mediump.c
+++ b/src/compiler/nir/nir_lower_mediump.c
@@ -946,6 +946,9 @@ fold_16bit_tex_srcs(nir_builder *b, nir_tex_instr *tex,
    if (!(options->sampler_dims & BITFIELD_BIT(tex->sampler_dim)))
       return false;
 
+   if (nir_tex_instr_src_index(tex, nir_tex_src_backend1) >= 0)
+      return false;
+
    unsigned fold_srcs = 0;
    for (unsigned i = 0; i < tex->num_srcs; i++) {
       /* Filter out sources that should be ignored. */
-- 
GitLab


From 62f1d9d431616fc8c400ff1dc8f429aed1defc5b Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Wed, 5 Apr 2023 15:52:40 +0100
Subject: [PATCH 02/15] nir,vtn,aco,ac/llvm: make cube_face_coord_amd more
 direct
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
Reviewed-by: Marek Olšák <marek.olsak@amd.com>
---
 src/amd/compiler/aco_instruction_selection.cpp |  7 +------
 src/amd/llvm/ac_nir_to_llvm.c                  | 17 +++++------------
 src/compiler/nir/nir_opcodes.py                | 14 +++++---------
 src/compiler/spirv/vtn_amd.c                   |  6 +++++-
 4 files changed, 16 insertions(+), 28 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index b5d419535b79..b0f83eb9fecf 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -2503,14 +2503,9 @@ visit_alu_instr(isel_context* ctx, nir_alu_instr* instr)
       Temp src[3] = {emit_extract_vector(ctx, in, 0, v1), emit_extract_vector(ctx, in, 1, v1),
                      emit_extract_vector(ctx, in, 2, v1)};
       Temp ma = bld.vop3(aco_opcode::v_cubema_f32, bld.def(v1), src[0], src[1], src[2]);
-      ma = bld.vop1(aco_opcode::v_rcp_f32, bld.def(v1), ma);
       Temp sc = bld.vop3(aco_opcode::v_cubesc_f32, bld.def(v1), src[0], src[1], src[2]);
       Temp tc = bld.vop3(aco_opcode::v_cubetc_f32, bld.def(v1), src[0], src[1], src[2]);
-      sc = bld.vop2(aco_opcode::v_add_f32, bld.def(v1), Operand::c32(0x3f000000u /*0.5*/),
-                    bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), sc, ma));
-      tc = bld.vop2(aco_opcode::v_add_f32, bld.def(v1), Operand::c32(0x3f000000u /*0.5*/),
-                    bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), tc, ma));
-      bld.pseudo(aco_opcode::p_create_vector, Definition(dst), sc, tc);
+      bld.pseudo(aco_opcode::p_create_vector, Definition(dst), sc, tc, ma);
       break;
    }
    case nir_op_cube_face_index_amd: {
diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index caa5c3665462..5702cc7621d8 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -1200,21 +1200,14 @@ static bool visit_alu(struct ac_nir_context *ctx, const nir_alu_instr *instr)
 
    case nir_op_cube_face_coord_amd: {
       src[0] = ac_to_float(&ctx->ac, src[0]);
-      LLVMValueRef results[2];
+      LLVMValueRef results[3];
       LLVMValueRef in[3];
       for (unsigned chan = 0; chan < 3; chan++)
          in[chan] = ac_llvm_extract_elem(&ctx->ac, src[0], chan);
-      results[0] = ac_build_intrinsic(&ctx->ac, "llvm.amdgcn.cubesc", ctx->ac.f32, in, 3,
-                                      0);
-      results[1] = ac_build_intrinsic(&ctx->ac, "llvm.amdgcn.cubetc", ctx->ac.f32, in, 3,
-                                      0);
-      LLVMValueRef ma = ac_build_intrinsic(&ctx->ac, "llvm.amdgcn.cubema", ctx->ac.f32, in, 3, 0);
-      results[0] = ac_build_fdiv(&ctx->ac, results[0], ma);
-      results[1] = ac_build_fdiv(&ctx->ac, results[1], ma);
-      LLVMValueRef offset = LLVMConstReal(ctx->ac.f32, 0.5);
-      results[0] = LLVMBuildFAdd(ctx->ac.builder, results[0], offset, "");
-      results[1] = LLVMBuildFAdd(ctx->ac.builder, results[1], offset, "");
-      result = ac_build_gather_values(&ctx->ac, results, 2);
+      results[0] = ac_build_intrinsic(&ctx->ac, "llvm.amdgcn.cubesc", ctx->ac.f32, in, 3, 0);
+      results[1] = ac_build_intrinsic(&ctx->ac, "llvm.amdgcn.cubetc", ctx->ac.f32, in, 3, 0);
+      results[2] = ac_build_intrinsic(&ctx->ac, "llvm.amdgcn.cubema", ctx->ac.f32, in, 3, 0);
+      result = ac_build_gather_values(&ctx->ac, results, 3);
       break;
    }
 
diff --git a/src/compiler/nir/nir_opcodes.py b/src/compiler/nir/nir_opcodes.py
index f7e9df073f7a..ef4a73275754 100644
--- a/src/compiler/nir/nir_opcodes.py
+++ b/src/compiler/nir/nir_opcodes.py
@@ -537,16 +537,15 @@ for (unsigned bit = 0; bit < bit_size; bit++) {
 """)
 
 # AMD_gcn_shader extended instructions
-unop_horiz("cube_face_coord_amd", 2, tfloat32, 3, tfloat32, """
-dst.x = dst.y = 0.0;
+unop_horiz("cube_face_coord_amd", 3, tfloat32, 3, tfloat32, """
+dst.x = dst.y = dst.z = 0.0;
 float absX = fabsf(src0.x);
 float absY = fabsf(src0.y);
 float absZ = fabsf(src0.z);
 
-float ma = 0.0;
-if (absX >= absY && absX >= absZ) { ma = 2 * src0.x; }
-if (absY >= absX && absY >= absZ) { ma = 2 * src0.y; }
-if (absZ >= absX && absZ >= absY) { ma = 2 * src0.z; }
+if (absX >= absY && absX >= absZ) { dst.z = 2 * src0.x; }
+if (absY >= absX && absY >= absZ) { dst.z = 2 * src0.y; }
+if (absZ >= absX && absZ >= absY) { dst.z = 2 * src0.z; }
 
 if (src0.x >= 0 && absX >= absY && absX >= absZ) { dst.x = -src0.z; dst.y = -src0.y; }
 if (src0.x < 0 && absX >= absY && absX >= absZ) { dst.x = src0.z; dst.y = -src0.y; }
@@ -554,9 +553,6 @@ if (src0.y >= 0 && absY >= absX && absY >= absZ) { dst.x = src0.x; dst.y = src0.
 if (src0.y < 0 && absY >= absX && absY >= absZ) { dst.x = src0.x; dst.y = -src0.z; }
 if (src0.z >= 0 && absZ >= absX && absZ >= absY) { dst.x = src0.x; dst.y = -src0.y; }
 if (src0.z < 0 && absZ >= absX && absZ >= absY) { dst.x = -src0.x; dst.y = -src0.y; }
-
-dst.x = dst.x * (1.0f / ma) + 0.5f;
-dst.y = dst.y * (1.0f / ma) + 0.5f;
 """)
 
 unop_horiz("cube_face_index_amd", 1, tfloat32, 3, tfloat32, """
diff --git a/src/compiler/spirv/vtn_amd.c b/src/compiler/spirv/vtn_amd.c
index 85df179d91c8..5cf489247a0c 100644
--- a/src/compiler/spirv/vtn_amd.c
+++ b/src/compiler/spirv/vtn_amd.c
@@ -35,9 +35,13 @@ vtn_handle_amd_gcn_shader_instruction(struct vtn_builder *b, SpvOp ext_opcode,
    case CubeFaceIndexAMD:
       def = nir_cube_face_index_amd(&b->nb, vtn_get_nir_ssa(b, w[5]));
       break;
-   case CubeFaceCoordAMD:
+   case CubeFaceCoordAMD: {
       def = nir_cube_face_coord_amd(&b->nb, vtn_get_nir_ssa(b, w[5]));
+      nir_ssa_def *st = nir_channels(&b->nb, def, 0x3);
+      nir_ssa_def *invma = nir_frcp(&b->nb, nir_channel(&b->nb, def, 2));
+      def = nir_ffma_imm2(&b->nb, st, invma, 0.5);
       break;
+   }
    case TimeAMD: {
       def = nir_pack_64_2x32(&b->nb, nir_shader_clock(&b->nb, NIR_SCOPE_SUBGROUP));
       break;
-- 
GitLab


From e6db1fe8347f051a1028e4ae6d4beb68dbd81f6a Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Wed, 5 Apr 2023 16:58:43 +0100
Subject: [PATCH 03/15] ac/nir: add pass for lowering 1d/cube coordinates
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
Reviewed-by: Marek Olšák <marek.olsak@amd.com>
---
 src/amd/common/ac_nir.h           |   7 +
 src/amd/common/ac_nir_lower_tex.c | 231 ++++++++++++++++++++++++++++++
 src/amd/common/meson.build        |   1 +
 3 files changed, 239 insertions(+)
 create mode 100644 src/amd/common/ac_nir_lower_tex.c

diff --git a/src/amd/common/ac_nir.h b/src/amd/common/ac_nir.h
index e93790fa0833..4a170ffff586 100644
--- a/src/amd/common/ac_nir.h
+++ b/src/amd/common/ac_nir.h
@@ -327,6 +327,13 @@ typedef struct {
 void
 ac_nir_lower_ps(nir_shader *nir, const ac_nir_lower_ps_options *options);
 
+typedef struct {
+   enum amd_gfx_level gfx_level;
+} ac_nir_lower_tex_options;
+
+bool
+ac_nir_lower_tex(nir_shader *nir, const ac_nir_lower_tex_options *options);
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/src/amd/common/ac_nir_lower_tex.c b/src/amd/common/ac_nir_lower_tex.c
new file mode 100644
index 000000000000..5b091a4bc0c6
--- /dev/null
+++ b/src/amd/common/ac_nir_lower_tex.c
@@ -0,0 +1,231 @@
+/*
+ * Copyright © 2023 Valve Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ *
+ */
+
+#include "ac_nir.h"
+#include "nir_builder.h"
+
+/**
+ * Build a manual selection sequence for cube face sc/tc coordinates and
+ * major axis vector (multiplied by 2 for consistency) for the given
+ * vec3 \p coords, for the face implied by \p selcoords.
+ *
+ * For the major axis, we always adjust the sign to be in the direction of
+ * selcoords.ma; i.e., a positive out_ma means that coords is pointed towards
+ * the selcoords major axis.
+ */
+static void
+build_cube_select(nir_builder *b, nir_ssa_def *ma, nir_ssa_def *id, nir_ssa_def *deriv,
+                  nir_ssa_def **out_ma, nir_ssa_def **out_sc, nir_ssa_def **out_tc)
+{
+   nir_ssa_def *deriv_x = nir_channel(b, deriv, 0);
+   nir_ssa_def *deriv_y = nir_channel(b, deriv, 1);
+   nir_ssa_def *deriv_z = nir_channel(b, deriv, 2);
+
+   nir_ssa_def *is_ma_positive = nir_fge(b, ma, nir_imm_float(b, 0.0));
+   nir_ssa_def *sgn_ma =
+      nir_bcsel(b, is_ma_positive, nir_imm_float(b, 1.0), nir_imm_float(b, -1.0));
+   nir_ssa_def *neg_sgn_ma = nir_fneg(b, sgn_ma);
+
+   nir_ssa_def *is_ma_z = nir_fge(b, id, nir_imm_float(b, 4.0));
+   nir_ssa_def *is_ma_y = nir_fge(b, id, nir_imm_float(b, 2.0));
+   is_ma_y = nir_iand(b, is_ma_y, nir_inot(b, is_ma_z));
+   nir_ssa_def *is_not_ma_x = nir_ior(b, is_ma_z, is_ma_y);
+
+   /* Select sc */
+   nir_ssa_def *tmp = nir_bcsel(b, is_not_ma_x, deriv_x, deriv_z);
+   nir_ssa_def *sgn =
+      nir_bcsel(b, is_ma_y, nir_imm_float(b, 1.0), nir_bcsel(b, is_ma_z, sgn_ma, neg_sgn_ma));
+   *out_sc = nir_fmul(b, tmp, sgn);
+
+   /* Select tc */
+   tmp = nir_bcsel(b, is_ma_y, deriv_z, deriv_y);
+   sgn = nir_bcsel(b, is_ma_y, sgn_ma, nir_imm_float(b, 1.0));
+   *out_tc = nir_fmul(b, tmp, sgn);
+
+   /* Select ma */
+   tmp = nir_bcsel(b, is_ma_z, deriv_z, nir_bcsel(b, is_ma_y, deriv_y, deriv_x));
+   *out_ma = nir_fmul_imm(b, nir_fabs(b, tmp), 2.0);
+}
+
+static void
+prepare_cube_coords(nir_builder *b, nir_tex_instr *tex, nir_ssa_def **coord, nir_src *ddx,
+                    nir_src *ddy, const ac_nir_lower_tex_options *options)
+{
+   nir_ssa_def *coords[NIR_MAX_VEC_COMPONENTS] = {0};
+   for (unsigned i = 0; i < (*coord)->num_components; i++)
+      coords[i] = nir_channel(b, *coord, i);
+
+   /* Section 8.9 (Texture Functions) of the GLSL 4.50 spec says:
+    *
+    *    "For Array forms, the array layer used will be
+    *
+    *       max(0, min(d−1, floor(layer+0.5)))
+    *
+    *     where d is the depth of the texture array and layer
+    *     comes from the component indicated in the tables below.
+    *     Workaroudn for an issue where the layer is taken from a
+    *     helper invocation which happens to fall on a different
+    *     layer due to extrapolation."
+    *
+    * GFX8 and earlier attempt to implement this in hardware by
+    * clamping the value of coords[2] = (8 * layer) + face.
+    * Unfortunately, this means that the we end up with the wrong
+    * face when clamping occurs.
+    *
+    * Clamp the layer earlier to work around the issue.
+    */
+   if (tex->is_array && options->gfx_level <= GFX8)
+      coords[3] = nir_fmax(b, coords[3], nir_imm_float(b, 0.0));
+
+   nir_ssa_def *cube_coords = nir_cube_face_coord_amd(b, nir_vec(b, coords, 3));
+   nir_ssa_def *sc = nir_channel(b, cube_coords, 0);
+   nir_ssa_def *tc = nir_channel(b, cube_coords, 1);
+   nir_ssa_def *ma = nir_channel(b, cube_coords, 2);
+   nir_ssa_def *invma = nir_frcp(b, nir_fabs(b, ma));
+   nir_ssa_def *id = nir_cube_face_index_amd(b, nir_vec(b, coords, 3));
+
+   if (ddx || ddy) {
+      sc = nir_fmul(b, sc, invma);
+      tc = nir_fmul(b, tc, invma);
+
+      /* Convert cube derivatives to 2D derivatives. */
+      for (unsigned i = 0; i < 2; i++) {
+         /* Transform the derivative alongside the texture
+          * coordinate. Mathematically, the correct formula is
+          * as follows. Assume we're projecting onto the +Z face
+          * and denote by dx/dh the derivative of the (original)
+          * X texture coordinate with respect to horizontal
+          * window coordinates. The projection onto the +Z face
+          * plane is:
+          *
+          *   f(x,z) = x/z
+          *
+          * Then df/dh = df/dx * dx/dh + df/dz * dz/dh
+          *            = 1/z * dx/dh - x/z * 1/z * dz/dh.
+          *
+          * This motivatives the implementation below.
+          *
+          * Whether this actually gives the expected results for
+          * apps that might feed in derivatives obtained via
+          * finite differences is anyone's guess. The OpenGL spec
+          * seems awfully quiet about how textureGrad for cube
+          * maps should be handled.
+          */
+         nir_ssa_def *deriv_ma, *deriv_sc, *deriv_tc;
+         build_cube_select(b, ma, id, i ? ddy->ssa : ddx->ssa, &deriv_ma, &deriv_sc, &deriv_tc);
+
+         deriv_ma = nir_fmul(b, deriv_ma, invma);
+
+         nir_ssa_def *x = nir_fsub(b, nir_fmul(b, deriv_sc, invma), nir_fmul(b, deriv_ma, sc));
+         nir_ssa_def *y = nir_fsub(b, nir_fmul(b, deriv_tc, invma), nir_fmul(b, deriv_ma, tc));
+
+         nir_instr_rewrite_src_ssa(&tex->instr, i ? ddy : ddx, nir_vec2(b, x, y));
+      }
+
+      sc = nir_fadd_imm(b, sc, 1.5);
+      tc = nir_fadd_imm(b, tc, 1.5);
+   } else {
+      sc = nir_ffma_imm2(b, sc, invma, 1.5);
+      tc = nir_ffma_imm2(b, tc, invma, 1.5);
+   }
+
+   if (tex->is_array)
+      id = nir_ffma_imm1(b, coords[3], 8.0, id);
+
+   *coord = nir_vec3(b, sc, tc, id);
+}
+
+static bool
+lower_tex_coords(nir_builder *b, nir_tex_instr *tex, nir_ssa_def **coords,
+                 const ac_nir_lower_tex_options *options)
+{
+   if (tex->sampler_dim != GLSL_SAMPLER_DIM_CUBE &&
+       !(tex->sampler_dim == GLSL_SAMPLER_DIM_1D && options->gfx_level == GFX9))
+      return false;
+
+   int ddx_idx = nir_tex_instr_src_index(tex, nir_tex_src_ddx);
+   int ddy_idx = nir_tex_instr_src_index(tex, nir_tex_src_ddy);
+   nir_src *ddx = ddx_idx >= 0 ? &tex->src[ddx_idx].src : NULL;
+   nir_src *ddy = ddy_idx >= 0 ? &tex->src[ddy_idx].src : NULL;
+
+   if (tex->sampler_dim == GLSL_SAMPLER_DIM_1D) {
+      nir_ssa_def *y =
+         nir_imm_floatN_t(b, tex->op == nir_texop_txf ? 0.0 : 0.5, (*coords)->bit_size);
+      if (tex->is_array && (*coords)->num_components > 1) {
+         nir_ssa_def *x = nir_channel(b, *coords, 0);
+         nir_ssa_def *idx = nir_channel(b, *coords, 1);
+         *coords = nir_vec3(b, x, y, idx);
+      } else {
+         *coords = nir_vec2(b, *coords, y);
+      }
+
+      int offset_src = nir_tex_instr_src_index(tex, nir_tex_src_offset);
+      if (offset_src >= 0) {
+         nir_src *offset = &tex->src[offset_src].src;
+         nir_ssa_def *zero = nir_imm_intN_t(b, 0, offset->ssa->bit_size);
+         nir_instr_rewrite_src_ssa(&tex->instr, offset, nir_vec2(b, offset->ssa, zero));
+      }
+
+      if (ddx || ddy) {
+         nir_ssa_def *def = nir_vec2(b, ddx->ssa, nir_imm_floatN_t(b, 0.0, ddx->ssa->bit_size));
+         nir_instr_rewrite_src_ssa(&tex->instr, ddx, def);
+         def = nir_vec2(b, ddy->ssa, nir_imm_floatN_t(b, 0.0, ddy->ssa->bit_size));
+         nir_instr_rewrite_src_ssa(&tex->instr, ddy, def);
+      }
+   } else if (tex->sampler_dim == GLSL_SAMPLER_DIM_CUBE) {
+      prepare_cube_coords(b, tex, coords, ddx, ddy, options);
+   }
+
+   return true;
+}
+
+static bool
+lower_tex(nir_builder *b, nir_instr *instr, void *options_)
+{
+   const ac_nir_lower_tex_options *options = options_;
+   if (instr->type != nir_instr_type_tex)
+      return false;
+
+   nir_tex_instr *tex = nir_instr_as_tex(instr);
+   int coord_idx = nir_tex_instr_src_index(tex, nir_tex_src_coord);
+   if (coord_idx < 0 || nir_tex_instr_src_index(tex, nir_tex_src_backend1) >= 0)
+      return false;
+
+   b->cursor = nir_before_instr(instr);
+   nir_ssa_def *coords = tex->src[coord_idx].src.ssa;
+   if (lower_tex_coords(b, tex, &coords, options)) {
+      tex->coord_components = coords->num_components;
+      nir_instr_rewrite_src_ssa(&tex->instr, &tex->src[coord_idx].src, coords);
+      return true;
+   }
+
+   return false;
+}
+
+bool
+ac_nir_lower_tex(nir_shader *nir, const ac_nir_lower_tex_options *options)
+{
+   return nir_shader_instructions_pass(
+      nir, lower_tex, nir_metadata_block_index | nir_metadata_dominance, (void *)options);
+}
diff --git a/src/amd/common/meson.build b/src/amd/common/meson.build
index 21853a608cf4..eed8061bb14a 100644
--- a/src/amd/common/meson.build
+++ b/src/amd/common/meson.build
@@ -99,6 +99,7 @@ amd_common_files = files(
   'ac_nir_lower_subdword_loads.c',
   'ac_nir_lower_taskmesh_io_to_mem.c',
   'ac_nir_lower_tess_io_to_mem.c',
+  'ac_nir_lower_tex.c',
   'ac_nir_lower_ngg.c',
   'ac_nir_lower_ps.c',
   'amd_family.c',
-- 
GitLab


From 499338d7c193ba2d357d2dd1012f92f900b9af62 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Tue, 25 Apr 2023 15:37:02 +0100
Subject: [PATCH 04/15] ac/nir: round layer in ac_nir_lower_tex

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/common/ac_nir.h           |  5 +++++
 src/amd/common/ac_nir_lower_tex.c | 19 ++++++++++++++++++-
 2 files changed, 23 insertions(+), 1 deletion(-)

diff --git a/src/amd/common/ac_nir.h b/src/amd/common/ac_nir.h
index 4a170ffff586..a394eb80c376 100644
--- a/src/amd/common/ac_nir.h
+++ b/src/amd/common/ac_nir.h
@@ -329,6 +329,11 @@ ac_nir_lower_ps(nir_shader *nir, const ac_nir_lower_ps_options *options);
 
 typedef struct {
    enum amd_gfx_level gfx_level;
+
+   /* If true, round the layer component of the coordinates source to the nearest
+    * integer for all array ops.
+    */
+   bool lower_array_layer_round_even;
 } ac_nir_lower_tex_options;
 
 bool
diff --git a/src/amd/common/ac_nir_lower_tex.c b/src/amd/common/ac_nir_lower_tex.c
index 5b091a4bc0c6..79c2e6ad7862 100644
--- a/src/amd/common/ac_nir_lower_tex.c
+++ b/src/amd/common/ac_nir_lower_tex.c
@@ -156,13 +156,30 @@ prepare_cube_coords(nir_builder *b, nir_tex_instr *tex, nir_ssa_def **coord, nir
    *coord = nir_vec3(b, sc, tc, id);
 }
 
+static bool
+lower_array_layer_round_even(nir_builder *b, nir_tex_instr *tex, nir_ssa_def **coords)
+{
+   int coord_index = nir_tex_instr_src_index(tex, nir_tex_src_coord);
+   if (coord_index < 0 || nir_tex_instr_src_type(tex, coord_index) != nir_type_float)
+      return false;
+
+   unsigned layer = tex->coord_components - 1;
+   nir_ssa_def *rounded_layer = nir_fround_even(b, nir_channel(b, *coords, layer));
+   *coords = nir_vector_insert_imm(b, *coords, rounded_layer, layer);
+   return true;
+}
+
 static bool
 lower_tex_coords(nir_builder *b, nir_tex_instr *tex, nir_ssa_def **coords,
                  const ac_nir_lower_tex_options *options)
 {
+   bool progress = false;
+   if (options->lower_array_layer_round_even && tex->is_array && tex->op != nir_texop_lod)
+      progress |= lower_array_layer_round_even(b, tex, coords);
+
    if (tex->sampler_dim != GLSL_SAMPLER_DIM_CUBE &&
        !(tex->sampler_dim == GLSL_SAMPLER_DIM_1D && options->gfx_level == GFX9))
-      return false;
+      return progress;
 
    int ddx_idx = nir_tex_instr_src_index(tex, nir_tex_src_ddx);
    int ddy_idx = nir_tex_instr_src_index(tex, nir_tex_src_ddy);
-- 
GitLab


From f9304f26161bb78a52eaf1123d7535a21dfa556b Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Thu, 6 Apr 2023 11:43:29 +0100
Subject: [PATCH 05/15] radv,radeonsi: use ac_nir_lower_tex

fossil-db (navi21):
Totals from 17279 (12.74% of 135636) affected shaders:
MaxWaves: 270015 -> 269991 (-0.01%)
Instrs: 24847385 -> 24843807 (-0.01%); split: -0.02%, +0.00%
CodeSize: 133215364 -> 133198744 (-0.01%); split: -0.02%, +0.01%
VGPRs: 1217632 -> 1217872 (+0.02%); split: -0.00%, +0.02%
Latency: 405347021 -> 404971784 (-0.09%); split: -0.09%, +0.00%
InvThroughput: 75386590 -> 75350344 (-0.05%); split: -0.07%, +0.03%
VClause: 426986 -> 426821 (-0.04%); split: -0.04%, +0.01%
SClause: 966751 -> 966971 (+0.02%); split: -0.01%, +0.03%
Copies: 1738510 -> 1737970 (-0.03%); split: -0.08%, +0.05%
PreSGPRs: 1169070 -> 1169120 (+0.00%); split: -0.00%, +0.00%
PreVGPRs: 1136102 -> 1136183 (+0.01%); split: -0.00%, +0.01%

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 .../compiler/aco_instruction_selection.cpp    | 143 +-------------
 src/amd/llvm/ac_llvm_build.c                  | 184 ------------------
 src/amd/llvm/ac_llvm_build.h                  |   3 -
 src/amd/llvm/ac_nir_to_llvm.c                 |  54 +----
 src/amd/vulkan/radv_pipeline.c                |   7 +
 src/amd/vulkan/radv_shader.c                  |   1 -
 src/gallium/drivers/radeonsi/si_shader.c      |   7 +
 src/gallium/drivers/radeonsi/si_shader_nir.c  |   1 -
 8 files changed, 21 insertions(+), 379 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index b0f83eb9fecf..0f1c3afebe09 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -9059,119 +9059,6 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
    }
 }
 
-void
-build_cube_select(isel_context* ctx, Temp ma, Temp id, Temp deriv, Temp* out_ma, Temp* out_sc,
-                  Temp* out_tc)
-{
-   Builder bld(ctx->program, ctx->block);
-
-   Temp deriv_x = emit_extract_vector(ctx, deriv, 0, v1);
-   Temp deriv_y = emit_extract_vector(ctx, deriv, 1, v1);
-   Temp deriv_z = emit_extract_vector(ctx, deriv, 2, v1);
-
-   Operand neg_one = Operand::c32(0xbf800000u);
-   Operand one = Operand::c32(0x3f800000u);
-   Operand two = Operand::c32(0x40000000u);
-   Operand four = Operand::c32(0x40800000u);
-
-   Temp is_ma_positive = bld.vopc(aco_opcode::v_cmp_le_f32, bld.def(bld.lm), Operand::zero(), ma);
-   Temp sgn_ma = bld.vop2_e64(aco_opcode::v_cndmask_b32, bld.def(v1), neg_one, one, is_ma_positive);
-   Temp neg_sgn_ma = bld.vop2(aco_opcode::v_sub_f32, bld.def(v1), Operand::zero(), sgn_ma);
-
-   Temp is_ma_z = bld.vopc(aco_opcode::v_cmp_le_f32, bld.def(bld.lm), four, id);
-   Temp is_ma_y = bld.vopc(aco_opcode::v_cmp_le_f32, bld.def(bld.lm), two, id);
-   is_ma_y = bld.sop2(Builder::s_andn2, bld.def(bld.lm), bld.def(s1, scc), is_ma_y, is_ma_z);
-   Temp is_not_ma_x = bld.sop2(Builder::s_or, bld.def(bld.lm), bld.def(s1, scc), is_ma_z, is_ma_y);
-
-   /* select sc */
-   Temp tmp = bld.vop2(aco_opcode::v_cndmask_b32, bld.def(v1), deriv_z, deriv_x, is_not_ma_x);
-   Temp sgn = bld.vop2_e64(
-      aco_opcode::v_cndmask_b32, bld.def(v1),
-      bld.vop2(aco_opcode::v_cndmask_b32, bld.def(v1), neg_sgn_ma, sgn_ma, is_ma_z), one, is_ma_y);
-   *out_sc = bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), tmp, sgn);
-
-   /* select tc */
-   tmp = bld.vop2(aco_opcode::v_cndmask_b32, bld.def(v1), deriv_y, deriv_z, is_ma_y);
-   sgn = bld.vop2(aco_opcode::v_cndmask_b32, bld.def(v1), neg_one, sgn_ma, is_ma_y);
-   *out_tc = bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), tmp, sgn);
-
-   /* select ma */
-   tmp = bld.vop2(aco_opcode::v_cndmask_b32, bld.def(v1),
-                  bld.vop2(aco_opcode::v_cndmask_b32, bld.def(v1), deriv_x, deriv_y, is_ma_y),
-                  deriv_z, is_ma_z);
-   tmp = bld.vop2(aco_opcode::v_and_b32, bld.def(v1), Operand::c32(0x7fffffffu), tmp);
-   *out_ma = bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), two, tmp);
-}
-
-void
-prepare_cube_coords(isel_context* ctx, std::vector<Temp>& coords, Temp* ddx, Temp* ddy,
-                    bool is_deriv, bool is_array)
-{
-   Builder bld(ctx->program, ctx->block);
-   Temp ma, tc, sc, id;
-   aco_opcode madak =
-      ctx->program->gfx_level >= GFX10_3 ? aco_opcode::v_fmaak_f32 : aco_opcode::v_madak_f32;
-   aco_opcode madmk =
-      ctx->program->gfx_level >= GFX10_3 ? aco_opcode::v_fmamk_f32 : aco_opcode::v_madmk_f32;
-
-   /* see comment in ac_prepare_cube_coords() */
-   if (is_array && ctx->options->gfx_level <= GFX8)
-      coords[3] = bld.vop2(aco_opcode::v_max_f32, bld.def(v1), Operand::zero(), coords[3]);
-
-   ma = bld.vop3(aco_opcode::v_cubema_f32, bld.def(v1), coords[0], coords[1], coords[2]);
-
-   aco_ptr<VALU_instruction> vop3a{
-      create_instruction<VALU_instruction>(aco_opcode::v_rcp_f32, asVOP3(Format::VOP1), 1, 1)};
-   vop3a->operands[0] = Operand(ma);
-   vop3a->abs[0] = true;
-   Temp invma = bld.tmp(v1);
-   vop3a->definitions[0] = Definition(invma);
-   ctx->block->instructions.emplace_back(std::move(vop3a));
-
-   sc = bld.vop3(aco_opcode::v_cubesc_f32, bld.def(v1), coords[0], coords[1], coords[2]);
-   if (!is_deriv)
-      sc = bld.vop2(madak, bld.def(v1), sc, invma, Operand::c32(0x3fc00000u /*1.5*/));
-
-   tc = bld.vop3(aco_opcode::v_cubetc_f32, bld.def(v1), coords[0], coords[1], coords[2]);
-   if (!is_deriv)
-      tc = bld.vop2(madak, bld.def(v1), tc, invma, Operand::c32(0x3fc00000u /*1.5*/));
-
-   id = bld.vop3(aco_opcode::v_cubeid_f32, bld.def(v1), coords[0], coords[1], coords[2]);
-
-   if (is_deriv) {
-      sc = bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), sc, invma);
-      tc = bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), tc, invma);
-
-      for (unsigned i = 0; i < 2; i++) {
-         /* see comment in ac_prepare_cube_coords() */
-         Temp deriv_ma;
-         Temp deriv_sc, deriv_tc;
-         build_cube_select(ctx, ma, id, i ? *ddy : *ddx, &deriv_ma, &deriv_sc, &deriv_tc);
-
-         deriv_ma = bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), deriv_ma, invma);
-
-         Temp x = bld.vop2(aco_opcode::v_sub_f32, bld.def(v1),
-                           bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), deriv_sc, invma),
-                           bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), deriv_ma, sc));
-         Temp y = bld.vop2(aco_opcode::v_sub_f32, bld.def(v1),
-                           bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), deriv_tc, invma),
-                           bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), deriv_ma, tc));
-         *(i ? ddy : ddx) = bld.pseudo(aco_opcode::p_create_vector, bld.def(v2), x, y);
-      }
-
-      sc = bld.vop2(aco_opcode::v_add_f32, bld.def(v1), Operand::c32(0x3fc00000u /*1.5*/), sc);
-      tc = bld.vop2(aco_opcode::v_add_f32, bld.def(v1), Operand::c32(0x3fc00000u /*1.5*/), tc);
-   }
-
-   if (is_array) {
-      id = bld.vop2(madmk, bld.def(v1), coords[3], id, Operand::c32(0x41000000u /*8.0*/));
-      coords.erase(coords.begin() + 3);
-   }
-   coords[0] = sc;
-   coords[1] = tc;
-   coords[2] = id;
-}
-
 void
 get_const_vec(nir_ssa_def* vec, nir_const_value* cv[4])
 {
@@ -9363,25 +9250,8 @@ visit_tex(isel_context* ctx, nir_tex_instr* instr)
    }
 
    std::vector<Temp> unpacked_coord;
-   if (ctx->options->gfx_level == GFX9 && instr->sampler_dim == GLSL_SAMPLER_DIM_1D &&
-       instr->coord_components) {
-      RegClass rc = a16 ? v2b : v1;
-      for (unsigned i = 0; i < coord.bytes() / rc.bytes(); i++)
-         unpacked_coord.emplace_back(emit_extract_vector(ctx, coord, i, rc));
-
-      assert(unpacked_coord.size() > 0 && unpacked_coord.size() < 3);
-
-      Operand coord2d;
-      /* 0.5 for floating point coords, 0 for integer. */
-      if (a16)
-         coord2d = instr->op == nir_texop_txf ? Operand::c16(0) : Operand::c16(0x3800);
-      else
-         coord2d = instr->op == nir_texop_txf ? Operand::c32(0) : Operand::c32(0x3f000000);
-      unpacked_coord.insert(std::next(unpacked_coord.begin()), bld.copy(bld.def(rc), coord2d));
-   } else if (coord != Temp()) {
+   if (coord != Temp())
       unpacked_coord.push_back(coord);
-   }
-
    if (has_sample_index)
       unpacked_coord.push_back(sample_index);
    if (has_lod)
@@ -9391,25 +9261,14 @@ visit_tex(isel_context* ctx, nir_tex_instr* instr)
 
    coords = emit_pack_v1(ctx, unpacked_coord);
 
-   assert(instr->sampler_dim != GLSL_SAMPLER_DIM_CUBE || !a16);
-   if (instr->sampler_dim == GLSL_SAMPLER_DIM_CUBE && instr->coord_components)
-      prepare_cube_coords(ctx, coords, &ddx, &ddy, instr->op == nir_texop_txd,
-                          instr->is_array && instr->op != nir_texop_lod);
-
    /* pack derivatives */
    if (has_ddx || has_ddy) {
-      RegClass rc = g16 ? v2b : v1;
       assert(a16 == g16 || ctx->options->gfx_level >= GFX10);
       std::array<Temp, 2> ddxddy = {ddx, ddy};
       for (Temp tmp : ddxddy) {
          if (tmp == Temp())
             continue;
          std::vector<Temp> unpacked = {tmp};
-         if (instr->sampler_dim == GLSL_SAMPLER_DIM_1D && ctx->options->gfx_level == GFX9) {
-            assert(has_ddx && has_ddy);
-            Temp zero = bld.copy(bld.def(rc), Operand::zero(rc.bytes()));
-            unpacked.push_back(zero);
-         }
          for (Temp derv : emit_pack_v1(ctx, unpacked))
             derivs.push_back(derv);
       }
diff --git a/src/amd/llvm/ac_llvm_build.c b/src/amd/llvm/ac_llvm_build.c
index 2dd106965ac8..a9e2ff7816f4 100644
--- a/src/amd/llvm/ac_llvm_build.c
+++ b/src/amd/llvm/ac_llvm_build.c
@@ -773,190 +773,6 @@ LLVMValueRef ac_build_fast_udiv_u31_d_not_one(struct ac_llvm_context *ctx, LLVMV
    return LLVMBuildLShr(builder, num, post_shift, "");
 }
 
-/* Coordinates for cube map selection. sc, tc, and ma are as in Table 8.27
- * of the OpenGL 4.5 (Compatibility Profile) specification, except ma is
- * already multiplied by two. id is the cube face number.
- */
-struct cube_selection_coords {
-   LLVMValueRef stc[2];
-   LLVMValueRef ma;
-   LLVMValueRef id;
-};
-
-static void build_cube_intrinsic(struct ac_llvm_context *ctx, LLVMValueRef in[3],
-                                 struct cube_selection_coords *out)
-{
-   LLVMTypeRef f32 = ctx->f32;
-
-   out->stc[1] = ac_build_intrinsic(ctx, "llvm.amdgcn.cubetc", f32, in, 3, 0);
-   out->stc[0] = ac_build_intrinsic(ctx, "llvm.amdgcn.cubesc", f32, in, 3, 0);
-   out->ma = ac_build_intrinsic(ctx, "llvm.amdgcn.cubema", f32, in, 3, 0);
-   out->id = ac_build_intrinsic(ctx, "llvm.amdgcn.cubeid", f32, in, 3, 0);
-}
-
-/**
- * Build a manual selection sequence for cube face sc/tc coordinates and
- * major axis vector (multiplied by 2 for consistency) for the given
- * vec3 \p coords, for the face implied by \p selcoords.
- *
- * For the major axis, we always adjust the sign to be in the direction of
- * selcoords.ma; i.e., a positive out_ma means that coords is pointed towards
- * the selcoords major axis.
- */
-static void build_cube_select(struct ac_llvm_context *ctx,
-                              const struct cube_selection_coords *selcoords,
-                              const LLVMValueRef *coords, LLVMValueRef *out_st,
-                              LLVMValueRef *out_ma)
-{
-   LLVMBuilderRef builder = ctx->builder;
-   LLVMTypeRef f32 = LLVMTypeOf(coords[0]);
-   LLVMValueRef is_ma_positive;
-   LLVMValueRef sgn_ma;
-   LLVMValueRef is_ma_z, is_not_ma_z;
-   LLVMValueRef is_ma_y;
-   LLVMValueRef is_ma_x;
-   LLVMValueRef sgn;
-   LLVMValueRef tmp;
-
-   is_ma_positive = LLVMBuildFCmp(builder, LLVMRealUGE, selcoords->ma, LLVMConstReal(f32, 0.0), "");
-   sgn_ma = LLVMBuildSelect(builder, is_ma_positive, LLVMConstReal(f32, 1.0),
-                            LLVMConstReal(f32, -1.0), "");
-
-   is_ma_z = LLVMBuildFCmp(builder, LLVMRealUGE, selcoords->id, LLVMConstReal(f32, 4.0), "");
-   is_not_ma_z = LLVMBuildNot(builder, is_ma_z, "");
-   is_ma_y = LLVMBuildAnd(
-      builder, is_not_ma_z,
-      LLVMBuildFCmp(builder, LLVMRealUGE, selcoords->id, LLVMConstReal(f32, 2.0), ""), "");
-   is_ma_x = LLVMBuildAnd(builder, is_not_ma_z, LLVMBuildNot(builder, is_ma_y, ""), "");
-
-   /* Select sc */
-   tmp = LLVMBuildSelect(builder, is_ma_x, coords[2], coords[0], "");
-   sgn = LLVMBuildSelect(
-      builder, is_ma_y, LLVMConstReal(f32, 1.0),
-      LLVMBuildSelect(builder, is_ma_z, sgn_ma, LLVMBuildFNeg(builder, sgn_ma, ""), ""), "");
-   out_st[0] = LLVMBuildFMul(builder, tmp, sgn, "");
-
-   /* Select tc */
-   tmp = LLVMBuildSelect(builder, is_ma_y, coords[2], coords[1], "");
-   sgn = LLVMBuildSelect(builder, is_ma_y, sgn_ma, LLVMConstReal(f32, -1.0), "");
-   out_st[1] = LLVMBuildFMul(builder, tmp, sgn, "");
-
-   /* Select ma */
-   tmp = LLVMBuildSelect(builder, is_ma_z, coords[2],
-                         LLVMBuildSelect(builder, is_ma_y, coords[1], coords[0], ""), "");
-   tmp = ac_build_intrinsic(ctx, "llvm.fabs.f32", ctx->f32, &tmp, 1, 0);
-   *out_ma = LLVMBuildFMul(builder, tmp, LLVMConstReal(f32, 2.0), "");
-}
-
-void ac_prepare_cube_coords(struct ac_llvm_context *ctx, bool is_deriv, bool is_array, bool is_lod,
-                            LLVMValueRef *coords_arg, LLVMValueRef *derivs_arg)
-{
-
-   LLVMBuilderRef builder = ctx->builder;
-   struct cube_selection_coords selcoords;
-   LLVMValueRef coords[3];
-   LLVMValueRef invma;
-
-   if (is_array && !is_lod) {
-      LLVMValueRef tmp = ac_build_round(ctx, coords_arg[3]);
-
-      /* Section 8.9 (Texture Functions) of the GLSL 4.50 spec says:
-       *
-       *    "For Array forms, the array layer used will be
-       *
-       *       max(0, min(d−1, floor(layer+0.5)))
-       *
-       *     where d is the depth of the texture array and layer
-       *     comes from the component indicated in the tables below.
-       *     Workaround for an issue where the layer is taken from a
-       *     helper invocation which happens to fall on a different
-       *     layer due to extrapolation."
-       *
-       * GFX8 and earlier attempt to implement this in hardware by
-       * clamping the value of coords[2] = (8 * layer) + face.
-       * Unfortunately, this means that the we end up with the wrong
-       * face when clamping occurs.
-       *
-       * Clamp the layer earlier to work around the issue.
-       */
-      if (ctx->gfx_level <= GFX8) {
-         LLVMValueRef ge0;
-         ge0 = LLVMBuildFCmp(builder, LLVMRealOGE, tmp, ctx->f32_0, "");
-         tmp = LLVMBuildSelect(builder, ge0, tmp, ctx->f32_0, "");
-      }
-
-      coords_arg[3] = tmp;
-   }
-
-   build_cube_intrinsic(ctx, coords_arg, &selcoords);
-
-   invma =
-      ac_build_intrinsic(ctx, "llvm.fabs.f32", ctx->f32, &selcoords.ma, 1, 0);
-   invma = ac_build_fdiv(ctx, LLVMConstReal(ctx->f32, 1.0), invma);
-
-   for (int i = 0; i < 2; ++i)
-      coords[i] = LLVMBuildFMul(builder, selcoords.stc[i], invma, "");
-
-   coords[2] = selcoords.id;
-
-   if (is_deriv && derivs_arg) {
-      LLVMValueRef derivs[4];
-      int axis;
-
-      /* Convert cube derivatives to 2D derivatives. */
-      for (axis = 0; axis < 2; axis++) {
-         LLVMValueRef deriv_st[2];
-         LLVMValueRef deriv_ma;
-
-         /* Transform the derivative alongside the texture
-          * coordinate. Mathematically, the correct formula is
-          * as follows. Assume we're projecting onto the +Z face
-          * and denote by dx/dh the derivative of the (original)
-          * X texture coordinate with respect to horizontal
-          * window coordinates. The projection onto the +Z face
-          * plane is:
-          *
-          *   f(x,z) = x/z
-          *
-          * Then df/dh = df/dx * dx/dh + df/dz * dz/dh
-          *            = 1/z * dx/dh - x/z * 1/z * dz/dh.
-          *
-          * This motivatives the implementation below.
-          *
-          * Whether this actually gives the expected results for
-          * apps that might feed in derivatives obtained via
-          * finite differences is anyone's guess. The OpenGL spec
-          * seems awfully quiet about how textureGrad for cube
-          * maps should be handled.
-          */
-         build_cube_select(ctx, &selcoords, &derivs_arg[axis * 3], deriv_st, &deriv_ma);
-
-         deriv_ma = LLVMBuildFMul(builder, deriv_ma, invma, "");
-
-         for (int i = 0; i < 2; ++i)
-            derivs[axis * 2 + i] =
-               LLVMBuildFSub(builder, LLVMBuildFMul(builder, deriv_st[i], invma, ""),
-                             LLVMBuildFMul(builder, deriv_ma, coords[i], ""), "");
-      }
-
-      memcpy(derivs_arg, derivs, sizeof(derivs));
-   }
-
-   /* Shift the texture coordinate. This must be applied after the
-    * derivative calculation.
-    */
-   for (int i = 0; i < 2; ++i)
-      coords[i] = LLVMBuildFAdd(builder, coords[i], LLVMConstReal(ctx->f32, 1.5), "");
-
-   if (is_array) {
-      /* for cube arrays coord.z = coord.w(array_index) * 8 + face */
-      /* coords_arg.w component - array_index for cube arrays */
-      coords[2] = ac_build_fmad(ctx, coords_arg[3], LLVMConstReal(ctx->f32, 8.0), coords[2]);
-   }
-
-   memcpy(coords_arg, coords, sizeof(coords));
-}
-
 LLVMValueRef ac_build_fs_interp(struct ac_llvm_context *ctx, LLVMValueRef llvm_chan,
                                 LLVMValueRef attr_number, LLVMValueRef params, LLVMValueRef i,
                                 LLVMValueRef j)
diff --git a/src/amd/llvm/ac_llvm_build.h b/src/amd/llvm/ac_llvm_build.h
index 840ab74b172a..9f63098c0f5e 100644
--- a/src/amd/llvm/ac_llvm_build.h
+++ b/src/amd/llvm/ac_llvm_build.h
@@ -243,9 +243,6 @@ LLVMValueRef ac_build_fast_udiv_nuw(struct ac_llvm_context *ctx, LLVMValueRef nu
 LLVMValueRef ac_build_fast_udiv_u31_d_not_one(struct ac_llvm_context *ctx, LLVMValueRef num,
                                               LLVMValueRef multiplier, LLVMValueRef post_shift);
 
-void ac_prepare_cube_coords(struct ac_llvm_context *ctx, bool is_deriv, bool is_array, bool is_lod,
-                            LLVMValueRef *coords_arg, LLVMValueRef *derivs_arg);
-
 LLVMValueRef ac_build_fs_interp(struct ac_llvm_context *ctx, LLVMValueRef llvm_chan,
                                 LLVMValueRef attr_number, LLVMValueRef params, LLVMValueRef i,
                                 LLVMValueRef j);
diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index 5702cc7621d8..40f48c100005 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -1557,13 +1557,6 @@ static LLVMValueRef build_tex_intrinsic(struct ac_nir_context *ctx, const nir_te
       return lower_gather4_integer(&ctx->ac, args, instr);
    }
 
-   /* Fixup for GFX9 which allocates 1D textures as 2D. */
-   if (instr->op == nir_texop_lod && ctx->ac.gfx_level == GFX9) {
-      if ((args->dim == ac_image_2darray || args->dim == ac_image_2d) && !args->coords[1]) {
-         args->coords[1] = ctx->ac.i32_0;
-      }
-   }
-
    args->attributes = AC_ATTR_INVARIANT_LOAD;
    bool cs_derivs =
       ctx->stage == MESA_SHADER_COMPUTE && ctx->info->cs.derivative_group != DERIVATIVE_GROUP_NONE;
@@ -4259,61 +4252,26 @@ static void visit_tex(struct ac_nir_context *ctx, nir_tex_instr *instr)
 
    /* pack derivatives */
    if (ddx || ddy) {
-      int num_src_deriv_channels, num_dest_deriv_channels;
+      int num_deriv_channels;
       switch (instr->sampler_dim) {
       case GLSL_SAMPLER_DIM_3D:
       case GLSL_SAMPLER_DIM_CUBE:
-         num_src_deriv_channels = 3;
-         num_dest_deriv_channels = 3;
+         num_deriv_channels = 3;
          break;
       case GLSL_SAMPLER_DIM_2D:
       default:
-         num_src_deriv_channels = 2;
-         num_dest_deriv_channels = 2;
+         num_deriv_channels = 2;
          break;
       case GLSL_SAMPLER_DIM_1D:
-         num_src_deriv_channels = 1;
-         if (ctx->ac.gfx_level == GFX9) {
-            num_dest_deriv_channels = 2;
-         } else {
-            num_dest_deriv_channels = 1;
-         }
+         num_deriv_channels = ctx->ac.gfx_level == GFX9 ? 2 : 1;
          break;
       }
 
-      for (unsigned i = 0; i < num_src_deriv_channels; i++) {
+      for (unsigned i = 0; i < num_deriv_channels; i++) {
          args.derivs[i] = ac_to_float(&ctx->ac, ac_llvm_extract_elem(&ctx->ac, ddx, i));
-         args.derivs[num_dest_deriv_channels + i] =
+         args.derivs[num_deriv_channels + i] =
             ac_to_float(&ctx->ac, ac_llvm_extract_elem(&ctx->ac, ddy, i));
       }
-      for (unsigned i = num_src_deriv_channels; i < num_dest_deriv_channels; i++) {
-         LLVMValueRef zero = args.g16 ? ctx->ac.f16_0 : ctx->ac.f32_0;
-         args.derivs[i] = zero;
-         args.derivs[num_dest_deriv_channels + i] = zero;
-      }
-   }
-
-   if (instr->sampler_dim == GLSL_SAMPLER_DIM_CUBE && args.coords[0]) {
-      for (unsigned chan = 0; chan < instr->coord_components; chan++)
-         args.coords[chan] = ac_to_float(&ctx->ac, args.coords[chan]);
-      if (instr->coord_components == 3)
-         args.coords[3] = LLVMGetUndef(args.a16 ? ctx->ac.f16 : ctx->ac.f32);
-      ac_prepare_cube_coords(&ctx->ac, instr->op == nir_texop_txd, instr->is_array,
-                             instr->op == nir_texop_lod, args.coords, args.derivs);
-   }
-
-   /* Texture coordinates fixups */
-   if (ctx->ac.gfx_level == GFX9 && instr->sampler_dim == GLSL_SAMPLER_DIM_1D &&
-       instr->op != nir_texop_lod) {
-      LLVMValueRef filler;
-      if (instr->op == nir_texop_txf)
-         filler = args.a16 ? ctx->ac.i16_0 : ctx->ac.i32_0;
-      else
-         filler = LLVMConstReal(args.a16 ? ctx->ac.f16 : ctx->ac.f32, 0.5);
-
-      if (instr->is_array)
-         args.coords[2] = args.coords[1];
-      args.coords[1] = filler;
    }
 
    /* Pack sample index */
diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index a73dd8873275..275531f6b977 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -530,6 +530,13 @@ radv_postprocess_nir(struct radv_device *device, const struct radv_pipeline_layo
    if (progress)
       nir_shader_gather_info(stage->nir, nir_shader_get_entrypoint(stage->nir));
 
+   NIR_PASS(
+      _, stage->nir, ac_nir_lower_tex,
+      &(ac_nir_lower_tex_options){
+         .gfx_level = gfx_level,
+         .lower_array_layer_round_even = !device->physical_device->rad_info.conformant_trunc_coord,
+      });
+
    if (stage->nir->info.uses_resource_info_query)
       NIR_PASS(_, stage->nir, ac_nir_lower_resinfo, gfx_level);
 
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 7ca194eb46bd..68ee827fc043 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -613,7 +613,6 @@ radv_shader_spirv_to_nir(struct radv_device *device, const struct radv_pipeline_
       .lower_to_fragment_fetch_amd = device->physical_device->use_fmask,
       .lower_lod_zero_width = true,
       .lower_invalid_implicit_lod = true,
-      .lower_array_layer_round_even = !device->physical_device->rad_info.conformant_trunc_coord,
    };
 
    NIR_PASS(_, nir, nir_lower_tex, &tex_options);
diff --git a/src/gallium/drivers/radeonsi/si_shader.c b/src/gallium/drivers/radeonsi/si_shader.c
index a977bea43143..2f27b81b828f 100644
--- a/src/gallium/drivers/radeonsi/si_shader.c
+++ b/src/gallium/drivers/radeonsi/si_shader.c
@@ -2082,6 +2082,13 @@ struct nir_shader *si_get_nir_shader(struct si_shader *shader,
    if (sel->stage <= MESA_SHADER_GEOMETRY)
       NIR_PASS(progress, nir, si_nir_kill_outputs, key);
 
+   NIR_PASS(
+      _, nir, ac_nir_lower_tex,
+      &(ac_nir_lower_tex_options){
+         .gfx_level = sel->screen->info.gfx_level,
+         .lower_array_layer_round_even = !sel->screen->info.conformant_trunc_coord,
+      });
+
    if (nir->info.uses_resource_info_query)
       NIR_PASS(progress, nir, ac_nir_lower_resinfo, sel->screen->info.gfx_level);
 
diff --git a/src/gallium/drivers/radeonsi/si_shader_nir.c b/src/gallium/drivers/radeonsi/si_shader_nir.c
index 7ffd9ef57e33..4fd29b0d2bae 100644
--- a/src/gallium/drivers/radeonsi/si_shader_nir.c
+++ b/src/gallium/drivers/radeonsi/si_shader_nir.c
@@ -295,7 +295,6 @@ static void si_lower_nir(struct si_screen *sscreen, struct nir_shader *nir)
       .lower_invalid_implicit_lod = true,
       .lower_tg4_offsets = true,
       .lower_to_fragment_fetch_amd = sscreen->info.gfx_level < GFX11,
-      .lower_array_layer_round_even = !sscreen->info.conformant_trunc_coord,
    };
    NIR_PASS_V(nir, nir_lower_tex, &lower_tex_options);
 
-- 
GitLab


From 6ce64e7539b7321132dcf6b5b530d04f0d18ba4c Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Tue, 25 Apr 2023 15:37:32 +0100
Subject: [PATCH 06/15] nir/lower_tex: remove lower_array_layer_round_even

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/compiler/nir/nir.h           |  5 -----
 src/compiler/nir/nir_lower_tex.c | 26 --------------------------
 2 files changed, 31 deletions(-)

diff --git a/src/compiler/nir/nir.h b/src/compiler/nir/nir.h
index 8ac06bd5ed3b..8d37917cd5a3 100644
--- a/src/compiler/nir/nir.h
+++ b/src/compiler/nir/nir.h
@@ -5473,11 +5473,6 @@ typedef struct nir_lower_tex_options {
     */
    bool lower_invalid_implicit_lod;
 
-   /* If true, round the layer component of the coordinates source to the nearest
-    * integer for all array ops.
-    */
-   bool lower_array_layer_round_even;
-
    /* If true, texture_index (sampler_index) will be zero if a texture_offset
     * (sampler_offset) source is present. This is convenient for backends that
     * support indirect indexing of textures (samplers) but not offsetting it.
diff --git a/src/compiler/nir/nir_lower_tex.c b/src/compiler/nir/nir_lower_tex.c
index 639d8a256d50..403f5da7e31d 100644
--- a/src/compiler/nir/nir_lower_tex.c
+++ b/src/compiler/nir/nir_lower_tex.c
@@ -1100,27 +1100,6 @@ lower_tex_packing(nir_builder *b, nir_tex_instr *tex,
                                   color->parent_instr);
 }
 
-static bool
-lower_array_layer_round_even(nir_builder *b, nir_tex_instr *tex)
-{
-   int coord_index = nir_tex_instr_src_index(tex, nir_tex_src_coord);
-   if (coord_index < 0 || nir_tex_instr_src_type(tex, coord_index) != nir_type_float)
-      return false;
-
-   assert(tex->src[coord_index].src.is_ssa);
-   nir_ssa_def *coord = tex->src[coord_index].src.ssa;
-
-   b->cursor = nir_before_instr(&tex->instr);
-
-   unsigned layer = tex->coord_components - 1;
-   nir_ssa_def *rounded_layer = nir_fround_even(b, nir_channel(b, coord, layer));
-   nir_ssa_def *new_coord = nir_vector_insert_imm(b, coord, rounded_layer, layer);
-
-   nir_instr_rewrite_src_ssa(&tex->instr, &tex->src[coord_index].src, new_coord);
-
-   return true;
-}
-
 static bool
 sampler_index_lt(nir_tex_instr *tex, unsigned max)
 {
@@ -1559,11 +1538,6 @@ nir_lower_tex_block(nir_block *block, nir_builder *b,
          progress = true;
       }
 
-      if (options->lower_array_layer_round_even && tex->is_array &&
-          tex->op != nir_texop_lod) {
-         progress |= lower_array_layer_round_even(b, tex);
-      }
-
       if (tex->op == nir_texop_txd &&
           (options->lower_txd ||
            (options->lower_txd_shadow && tex->is_shadow) ||
-- 
GitLab


From b1a3ade8a79f7f42b494044f91a3953bb3f12906 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Mon, 24 Apr 2023 12:21:04 +0100
Subject: [PATCH 07/15] ac/nir: add fix_derivs_in_divergent_cf

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/common/ac_nir.h                    |  11 +
 src/amd/common/ac_nir_lower_tex.c          | 293 ++++++++++++++++++++-
 src/compiler/nir/nir_divergence_analysis.c |   1 +
 src/compiler/nir/nir_intrinsics.py         |   5 +
 4 files changed, 309 insertions(+), 1 deletion(-)

diff --git a/src/amd/common/ac_nir.h b/src/amd/common/ac_nir.h
index a394eb80c376..f4e794bee7a8 100644
--- a/src/amd/common/ac_nir.h
+++ b/src/amd/common/ac_nir.h
@@ -334,6 +334,17 @@ typedef struct {
     * integer for all array ops.
     */
    bool lower_array_layer_round_even;
+
+   /* Fix derivatives of constants and FS inputs in control flow.
+    *
+    * Ignores interpolateAtSample()/interpolateAtOffset(), dynamically indexed input loads,
+    * pervertexEXT input loads, textureGather() with implicit LOD and 16-bit derivatives and
+    * texture samples with nir_tex_src_min_lod.
+    *
+    * The layer must also be a constant or FS input.
+    */
+   bool fix_derivs_in_divergent_cf;
+   unsigned max_wqm_vgprs;
 } ac_nir_lower_tex_options;
 
 bool
diff --git a/src/amd/common/ac_nir_lower_tex.c b/src/amd/common/ac_nir_lower_tex.c
index 79c2e6ad7862..edc96e653b38 100644
--- a/src/amd/common/ac_nir_lower_tex.c
+++ b/src/amd/common/ac_nir_lower_tex.c
@@ -240,9 +240,300 @@ lower_tex(nir_builder *b, nir_instr *instr, void *options_)
    return false;
 }
 
+typedef struct {
+   nir_intrinsic_instr *bary;
+   nir_intrinsic_instr *load;
+} coord_info;
+
+static bool
+can_move_coord(nir_ssa_scalar scalar, coord_info *info)
+{
+   if (scalar.def->bit_size != 32)
+      return false;
+
+   if (nir_ssa_scalar_is_const(scalar))
+      return true;
+
+   if (scalar.def->parent_instr->type != nir_instr_type_intrinsic)
+      return false;
+
+   nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(scalar.def->parent_instr);
+   if (intrin->intrinsic == nir_intrinsic_load_input) {
+      info->bary = NULL;
+      info->load = intrin;
+      return true;
+   }
+
+   if (intrin->intrinsic != nir_intrinsic_load_interpolated_input)
+      return false;
+
+   nir_ssa_scalar coord_x = nir_ssa_scalar_resolved(intrin->src[0].ssa, 0);
+   nir_ssa_scalar coord_y = nir_ssa_scalar_resolved(intrin->src[0].ssa, 1);
+   if (coord_x.def->parent_instr->type != nir_instr_type_intrinsic || coord_x.comp != 0 ||
+       coord_y.def->parent_instr->type != nir_instr_type_intrinsic || coord_y.comp != 1)
+      return false;
+
+   nir_intrinsic_instr *intrin_x = nir_instr_as_intrinsic(coord_x.def->parent_instr);
+   nir_intrinsic_instr *intrin_y = nir_instr_as_intrinsic(coord_y.def->parent_instr);
+   if (intrin_x->intrinsic != intrin_y->intrinsic ||
+       (intrin_x->intrinsic != nir_intrinsic_load_barycentric_sample &&
+        intrin_x->intrinsic != nir_intrinsic_load_barycentric_pixel &&
+        intrin_x->intrinsic != nir_intrinsic_load_barycentric_centroid) ||
+       nir_intrinsic_interp_mode(intrin_x) != nir_intrinsic_interp_mode(intrin_y))
+      return false;
+
+   info->bary = intrin_x;
+   info->load = intrin;
+
+   return true;
+}
+
+struct move_tex_coords_state {
+   const ac_nir_lower_tex_options *options;
+   unsigned num_wqm_vgprs;
+   nir_builder toplevel_b;
+};
+
+static nir_ssa_def *
+build_coordinate(struct move_tex_coords_state *state, nir_ssa_scalar scalar, coord_info info)
+{
+   nir_builder *b = &state->toplevel_b;
+
+   if (nir_ssa_scalar_is_const(scalar))
+      return nir_imm_intN_t(b, nir_ssa_scalar_as_uint(scalar), scalar.def->bit_size);
+
+   ASSERTED nir_src offset = *nir_get_io_offset_src(info.load);
+   assert(nir_src_is_const(offset) && !nir_src_as_uint(offset));
+
+   nir_ssa_def *zero = nir_imm_int(b, 0);
+   nir_ssa_def *res;
+   if (info.bary) {
+      enum glsl_interp_mode interp_mode = nir_intrinsic_interp_mode(info.bary);
+      nir_ssa_def *bary = nir_load_system_value(b, info.bary->intrinsic, interp_mode, 2, 32);
+      res = nir_load_interpolated_input(b, 1, 32, bary, zero);
+   } else {
+      res = nir_load_input(b, 1, 32, zero);
+   }
+   nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(res->parent_instr);
+   nir_intrinsic_set_base(intrin, nir_intrinsic_base(info.load));
+   nir_intrinsic_set_component(intrin, nir_intrinsic_component(info.load) + scalar.comp);
+   nir_intrinsic_set_dest_type(intrin, nir_intrinsic_dest_type(info.load));
+   nir_intrinsic_set_io_semantics(intrin, nir_intrinsic_io_semantics(info.load));
+   return res;
+}
+
+static bool
+move_tex_coords(struct move_tex_coords_state *state, nir_function_impl *impl, nir_instr *instr)
+{
+   nir_tex_instr *tex = nir_instr_as_tex(instr);
+   if (tex->op != nir_texop_tex && tex->op != nir_texop_txb && tex->op != nir_texop_lod)
+      return false;
+
+   switch (tex->sampler_dim) {
+   case GLSL_SAMPLER_DIM_1D:
+   case GLSL_SAMPLER_DIM_2D:
+   case GLSL_SAMPLER_DIM_3D:
+   case GLSL_SAMPLER_DIM_CUBE:
+   case GLSL_SAMPLER_DIM_EXTERNAL:
+      break;
+   case GLSL_SAMPLER_DIM_RECT:
+   case GLSL_SAMPLER_DIM_BUF:
+   case GLSL_SAMPLER_DIM_MS:
+   case GLSL_SAMPLER_DIM_SUBPASS:
+   case GLSL_SAMPLER_DIM_SUBPASS_MS:
+      return false; /* No LOD or can't be sampled. */
+   }
+
+   if (nir_tex_instr_src_index(tex, nir_tex_src_min_lod) != -1)
+      return false;
+
+   nir_tex_src *src = &tex->src[nir_tex_instr_src_index(tex, nir_tex_src_coord)];
+   nir_ssa_scalar components[NIR_MAX_VEC_COMPONENTS];
+   coord_info infos[NIR_MAX_VEC_COMPONENTS];
+   bool can_move_all = true;
+   for (unsigned i = 0; i < tex->coord_components; i++) {
+      components[i] = nir_ssa_scalar_resolved(src->src.ssa, i);
+      can_move_all &= can_move_coord(components[i], &infos[i]);
+   }
+   if (!can_move_all)
+      return false;
+
+   int coord_base = 0;
+   unsigned linear_vgpr_size = tex->coord_components;
+   if (tex->sampler_dim == GLSL_SAMPLER_DIM_1D && state->options->gfx_level == GFX9)
+      linear_vgpr_size++;
+   if (tex->sampler_dim == GLSL_SAMPLER_DIM_CUBE && tex->is_array)
+      linear_vgpr_size--; /* cube array layer and face are combined */
+   for (unsigned i = 0; i < tex->num_srcs; i++) {
+      switch (tex->src[i].src_type) {
+      case nir_tex_src_offset:
+      case nir_tex_src_bias:
+      case nir_tex_src_comparator:
+         coord_base++;
+         linear_vgpr_size++;
+         break;
+      default:
+         break;
+      }
+   }
+
+   if (state->num_wqm_vgprs + linear_vgpr_size > state->options->max_wqm_vgprs)
+      return false;
+
+   for (unsigned i = 0; i < tex->coord_components; i++)
+      components[i] = nir_get_ssa_scalar(build_coordinate(state, components[i], infos[i]), 0);
+
+   nir_ssa_def *linear_vgpr = nir_vec_scalars(&state->toplevel_b, components, tex->coord_components);
+   lower_tex_coords(&state->toplevel_b, tex, &linear_vgpr, state->options);
+
+   linear_vgpr = nir_strict_wqm_coord_amd(&state->toplevel_b, linear_vgpr, coord_base * 4);
+
+   nir_tex_instr_remove_src(tex, nir_tex_instr_src_index(tex, nir_tex_src_coord));
+   tex->coord_components = 0;
+
+   nir_tex_instr_add_src(tex, nir_tex_src_backend1, nir_src_for_ssa(linear_vgpr));
+
+   int offset_src = nir_tex_instr_src_index(tex, nir_tex_src_offset);
+   if (offset_src >= 0) /* Workaround requirement in nir_tex_instr_src_size(). */
+      tex->src[offset_src].src_type = nir_tex_src_backend2;
+
+   state->num_wqm_vgprs += linear_vgpr_size;
+
+   return true;
+}
+
+static bool
+move_fddxy(struct move_tex_coords_state *state, nir_function_impl *impl, nir_alu_instr *instr)
+{
+   switch (instr->op) {
+   case nir_op_fddx:
+   case nir_op_fddy:
+   case nir_op_fddx_fine:
+   case nir_op_fddy_fine:
+   case nir_op_fddx_coarse:
+   case nir_op_fddy_coarse:
+      break;
+   default:
+      return false;
+   }
+
+   unsigned num_components = instr->dest.dest.ssa.num_components;
+   nir_ssa_scalar components[NIR_MAX_VEC_COMPONENTS];
+   coord_info infos[NIR_MAX_VEC_COMPONENTS];
+   bool can_move_all = true;
+   for (unsigned i = 0; i < num_components; i++) {
+      components[i] = nir_ssa_scalar_chase_alu_src(nir_get_ssa_scalar(&instr->dest.dest.ssa, i), 0);
+      components[i] = nir_ssa_scalar_chase_movs(components[i]);
+      can_move_all &= can_move_coord(components[i], &infos[i]);
+   }
+   if (!can_move_all || state->num_wqm_vgprs + num_components > state->options->max_wqm_vgprs)
+      return false;
+
+   for (unsigned i = 0; i < num_components; i++) {
+      nir_ssa_def *def = build_coordinate(state, components[i], infos[i]);
+      components[i] = nir_get_ssa_scalar(def, 0);
+   }
+
+   nir_ssa_def *def = nir_vec_scalars(&state->toplevel_b, components, num_components);
+   def = nir_build_alu1(&state->toplevel_b, instr->op, def);
+   nir_ssa_def_rewrite_uses(&instr->dest.dest.ssa, def);
+
+   state->num_wqm_vgprs += num_components;
+
+   return true;
+}
+
+static bool
+move_coords_from_divergent_cf(struct move_tex_coords_state *state, nir_function_impl *impl,
+                              struct exec_list *cf_list, bool *divergent_discard, bool divergent_cf)
+{
+   bool progress = false;
+   foreach_list_typed (nir_cf_node, cf_node, node, cf_list) {
+      switch (cf_node->type) {
+      case nir_cf_node_block: {
+         nir_block *block = nir_cf_node_as_block(cf_node);
+
+         bool top_level = cf_list == &impl->body;
+
+         nir_foreach_instr (instr, block) {
+            if (top_level && !*divergent_discard)
+               state->toplevel_b.cursor = nir_before_instr(instr);
+
+            if (instr->type == nir_instr_type_tex && (divergent_cf || *divergent_discard)) {
+               progress |= move_tex_coords(state, impl, instr);
+            } else if (instr->type == nir_instr_type_alu && (divergent_cf || *divergent_discard)) {
+               progress |= move_fddxy(state, impl, nir_instr_as_alu(instr));
+            } else if (instr->type == nir_instr_type_intrinsic) {
+               nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
+               switch (intrin->intrinsic) {
+               case nir_intrinsic_discard:
+               case nir_intrinsic_terminate:
+                  if (divergent_cf)
+                     *divergent_discard = true;
+                  break;
+               case nir_intrinsic_discard_if:
+               case nir_intrinsic_terminate_if:
+                  if (divergent_cf || nir_src_is_divergent(intrin->src[0]))
+                     *divergent_discard = true;
+                  break;
+               default:
+                  break;
+               }
+            }
+         }
+
+         if (top_level && !*divergent_discard)
+            state->toplevel_b.cursor = nir_after_block_before_jump(block);
+         break;
+      }
+      case nir_cf_node_if: {
+         nir_if *nif = nir_cf_node_as_if(cf_node);
+         bool divergent_discard_then = *divergent_discard;
+         bool divergent_discard_else = *divergent_discard;
+         bool then_else_divergent = divergent_cf || nir_src_is_divergent(nif->condition);
+         progress |= move_coords_from_divergent_cf(state, impl, &nif->then_list,
+                                                   &divergent_discard_then, then_else_divergent);
+         progress |= move_coords_from_divergent_cf(state, impl, &nif->else_list,
+                                                   &divergent_discard_else, then_else_divergent);
+         *divergent_discard |= divergent_discard_then || divergent_discard_else;
+         break;
+      }
+      case nir_cf_node_loop: {
+         nir_loop *loop = nir_cf_node_as_loop(cf_node);
+         assert(!nir_loop_has_continue_construct(loop));
+         progress |=
+            move_coords_from_divergent_cf(state, impl, &loop->body, divergent_discard, true);
+         break;
+      }
+      case nir_cf_node_function:
+         unreachable("Invalid cf type");
+      }
+   }
+
+   return progress;
+}
+
 bool
 ac_nir_lower_tex(nir_shader *nir, const ac_nir_lower_tex_options *options)
 {
-   return nir_shader_instructions_pass(
+   bool progress = false;
+   if (options->fix_derivs_in_divergent_cf) {
+      nir_function_impl *impl = nir_shader_get_entrypoint(nir);
+
+      struct move_tex_coords_state state;
+      nir_builder_init(&state.toplevel_b, impl);
+      state.options = options;
+      state.num_wqm_vgprs = 0;
+
+      bool divergent_discard = false;
+      if (move_coords_from_divergent_cf(&state, impl, &impl->body, &divergent_discard, false))
+         nir_metadata_preserve(impl, nir_metadata_block_index | nir_metadata_dominance);
+      else
+         nir_metadata_preserve(impl, nir_metadata_all);
+   }
+
+   progress |= nir_shader_instructions_pass(
       nir, lower_tex, nir_metadata_block_index | nir_metadata_dominance, (void *)options);
+
+   return progress;
 }
diff --git a/src/compiler/nir/nir_divergence_analysis.c b/src/compiler/nir/nir_divergence_analysis.c
index c10350583fd5..24136d4493e8 100644
--- a/src/compiler/nir/nir_divergence_analysis.c
+++ b/src/compiler/nir/nir_divergence_analysis.c
@@ -419,6 +419,7 @@ visit_intrinsic(nir_shader *shader, nir_intrinsic_instr *instr)
    case nir_intrinsic_image_descriptor_amd:
    case nir_intrinsic_image_deref_descriptor_amd:
    case nir_intrinsic_bindless_image_descriptor_amd:
+   case nir_intrinsic_strict_wqm_coord_amd:
    case nir_intrinsic_copy_deref:
    case nir_intrinsic_vulkan_resource_index:
    case nir_intrinsic_vulkan_resource_reindex:
diff --git a/src/compiler/nir/nir_intrinsics.py b/src/compiler/nir/nir_intrinsics.py
index 871ae5a2676a..421f454aa9a7 100644
--- a/src/compiler/nir/nir_intrinsics.py
+++ b/src/compiler/nir/nir_intrinsics.py
@@ -1582,6 +1582,11 @@ system_value("alpha_reference_amd", 1)
 # Whether to enable barycentric optimization
 system_value("barycentric_optimize_amd", dest_comp=1, bit_sizes=[1])
 
+# Copy the input into a register which will remain valid for entire quads, even in control flow.
+# This should only be used directly for texture sources.
+intrinsic("strict_wqm_coord_amd", src_comp=[0], dest_comp=0, bit_sizes=[32], indices=[BASE],
+          flags=[CAN_ELIMINATE])
+
 # V3D-specific instrinc for tile buffer color reads.
 #
 # The hardware requires that we read the samples and components of a pixel
-- 
GitLab


From c0760fce4f66a616c8416aa470247425f59da5d6 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Tue, 21 Mar 2023 14:47:45 +0000
Subject: [PATCH 08/15] aco: remove unused RegType

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_ir.h | 2 --
 1 file changed, 2 deletions(-)

diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index 04b93420f160..4323f4bbdcdf 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -304,10 +304,8 @@ withoutDPP(Format format)
 }
 
 enum class RegType {
-   none = 0,
    sgpr,
    vgpr,
-   linear_vgpr,
 };
 
 struct RegClass {
-- 
GitLab


From 0cb22c845a16b6f388716ad4713d762276f8f141 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Fri, 17 Mar 2023 16:44:25 +0000
Subject: [PATCH 09/15] aco: let p_start_linear_vgpr take an operand

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_lower_to_hw_instr.cpp   | 13 +++++++++++++
 src/amd/compiler/aco_opcodes.py              |  2 +-
 src/amd/compiler/aco_register_allocation.cpp |  7 +++++--
 src/amd/compiler/aco_validate.cpp            |  9 +++++++++
 4 files changed, 28 insertions(+), 3 deletions(-)

diff --git a/src/amd/compiler/aco_lower_to_hw_instr.cpp b/src/amd/compiler/aco_lower_to_hw_instr.cpp
index a008cc04d7dc..c048aa406b8c 100644
--- a/src/amd/compiler/aco_lower_to_hw_instr.cpp
+++ b/src/amd/compiler/aco_lower_to_hw_instr.cpp
@@ -2233,6 +2233,19 @@ lower_to_hw_instr(Program* program)
                handle_operands(copy_operations, &ctx, program->gfx_level, pi);
                break;
             }
+            case aco_opcode::p_start_linear_vgpr: {
+               if (instr->operands.empty())
+                  break;
+
+               Definition def(instr->definitions[0].physReg(),
+                              RegClass::get(RegType::vgpr, instr->definitions[0].bytes()));
+
+               std::map<PhysReg, copy_operation> copy_operations;
+               copy_operations[def.physReg()] = {instr->operands[0], def,
+                                                 instr->operands[0].bytes()};
+               handle_operands(copy_operations, &ctx, program->gfx_level, pi);
+               break;
+            }
             case aco_opcode::p_exit_early_if: {
                /* don't bother with an early exit near the end of the program */
                if ((block->instructions.size() - 1 - instr_idx) <= 4 &&
diff --git a/src/amd/compiler/aco_opcodes.py b/src/amd/compiler/aco_opcodes.py
index d656c0d7de9c..5fab419fe70f 100644
--- a/src/amd/compiler/aco_opcodes.py
+++ b/src/amd/compiler/aco_opcodes.py
@@ -305,7 +305,7 @@ opcode("p_barrier", format=Format.PSEUDO_BARRIER)
 opcode("p_spill")
 opcode("p_reload")
 
-# start/end linear vgprs
+# Start/end linear vgprs. p_start_linear_vgpr can take an operand to copy from, into the linear vgpr
 opcode("p_start_linear_vgpr")
 opcode("p_end_linear_vgpr")
 
diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index 2e98bb665b90..95fd684b222a 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -1890,7 +1890,8 @@ handle_pseudo(ra_ctx& ctx, const RegisterFile& reg_file, Instruction* instr)
    case aco_opcode::p_create_vector:
    case aco_opcode::p_split_vector:
    case aco_opcode::p_parallelcopy:
-   case aco_opcode::p_wqm: break;
+   case aco_opcode::p_wqm:
+   case aco_opcode::p_start_linear_vgpr: break;
    default: return;
    }
 
@@ -2950,7 +2951,9 @@ register_allocation(Program* program, std::vector<IDSet>& live_out_per_block, ra
                      definition->setFixed(reg);
                }
             } else if (instr->opcode == aco_opcode::p_wqm ||
-                       instr->opcode == aco_opcode::p_parallelcopy) {
+                       instr->opcode == aco_opcode::p_parallelcopy ||
+                       (instr->opcode == aco_opcode::p_start_linear_vgpr &&
+                        !instr->operands.empty())) {
                PhysReg reg = instr->operands[i].physReg();
                if (instr->operands[i].isTemp() &&
                    instr->operands[i].getTemp().type() == definition->getTemp().type() &&
diff --git a/src/amd/compiler/aco_validate.cpp b/src/amd/compiler/aco_validate.cpp
index b95e60be0b3e..7a21f90b00ff 100644
--- a/src/amd/compiler/aco_validate.cpp
+++ b/src/amd/compiler/aco_validate.cpp
@@ -585,6 +585,15 @@ validate_ir(Program* program)
                            instr->operands[i].isUndefined(),
                         "Operands of p_dual_src_export_gfx11 must be VGPRs or undef", instr.get());
                }
+            } else if (instr->opcode == aco_opcode::p_start_linear_vgpr) {
+               check(instr->definitions.size() == 1, "Must have one definition", instr.get());
+               check(instr->operands.size() <= 1, "Must have one or zero operands", instr.get());
+               if (!instr->definitions.empty())
+                  check(instr->definitions[0].regClass().is_linear_vgpr(),
+                        "Definition must be linear VGPR", instr.get());
+               if (!instr->definitions.empty() && !instr->operands.empty())
+                  check(instr->definitions[0].bytes() == instr->operands[0].bytes(),
+                        "Operand size must match definition", instr.get());
             }
             break;
          }
-- 
GitLab


From 9a92ccedd72ae7f842c03581d01afbb274d22818 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Tue, 21 Mar 2023 14:44:09 +0000
Subject: [PATCH 10/15] aco: add MIMG_instruction::strict_wqm

This lets us use linear VGPRs for part of the texture sample's address.

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 .../compiler/aco_instruction_selection.cpp    |  9 +--
 src/amd/compiler/aco_ir.cpp                   | 15 +++++
 src/amd/compiler/aco_ir.h                     |  4 +-
 src/amd/compiler/aco_lower_to_hw_instr.cpp    | 63 +++++++++++++++++++
 src/amd/compiler/aco_register_allocation.cpp  |  3 +-
 src/amd/compiler/aco_validate.cpp             | 43 ++++++++-----
 6 files changed, 113 insertions(+), 24 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 0f1c3afebe09..9f2b52e9bddc 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -5930,14 +5930,7 @@ static MIMG_instruction*
 emit_mimg(Builder& bld, aco_opcode op, Temp dst, Temp rsrc, Operand samp, std::vector<Temp> coords,
           bool needs_wqm = false, Operand vdata = Operand(v1))
 {
-   /* Limit NSA instructions to 3 dwords on GFX10 to avoid stability issues.
-    * On GFX11 the first 4 vaddr are single registers and the last contains the remaining
-    * vector.
-    */
-   size_t nsa_size = bld.program->gfx_level == GFX10     ? 5
-                     : bld.program->gfx_level == GFX10_3 ? 13
-                     : bld.program->gfx_level >= GFX11   ? 4
-                                                         : 0;
+   size_t nsa_size = bld.program->dev.max_nsa_vgprs;
    nsa_size = bld.program->gfx_level >= GFX11 || coords.size() <= nsa_size ? nsa_size : 0;
 
    for (unsigned i = 0; i < std::min(coords.size(), nsa_size); i++) {
diff --git a/src/amd/compiler/aco_ir.cpp b/src/amd/compiler/aco_ir.cpp
index c6cdec446a47..4bf647de9a86 100644
--- a/src/amd/compiler/aco_ir.cpp
+++ b/src/amd/compiler/aco_ir.cpp
@@ -188,6 +188,21 @@ init_program(Program* program, Stage stage, const struct aco_shader_info* info,
       program->dev.scratch_global_offset_max = 4095;
    }
 
+   if (program->gfx_level >= GFX11) {
+      /* GFX11 can have only 1 NSA dword. The last VGPR isn't included here because it contains the
+       * rest of the address.
+       */
+      program->dev.max_nsa_vgprs = 4;
+   } else if (program->gfx_level >= GFX10_3) {
+      /* GFX10.3 can have up to 3 NSA dwords. */
+      program->dev.max_nsa_vgprs = 13;
+   } else if (program->gfx_level >= GFX10) {
+      /* Limit NSA instructions to 1 NSA dword on GFX10 to avoid stability issues. */
+      program->dev.max_nsa_vgprs = 5;
+   } else {
+      program->dev.max_nsa_vgprs = 0;
+   }
+
    program->wgp_mode = wgp_mode;
 
    program->progress = CompilationProgress::after_isel;
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index 4323f4bbdcdf..c19e85c8ba77 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -1611,7 +1611,8 @@ struct MIMG_instruction : public Instruction {
    bool a16 : 1;         /* VEGA, NAVI: Address components are 16-bits */
    bool d16 : 1;         /* Convert 32-bit data to 16-bit data */
    bool disable_wqm : 1; /* Require an exec mask without helper invocations */
-   uint8_t padding0 : 2;
+   bool strict_wqm : 1;  /* VADDR is a linear VGPR and additional VGPRs may be copied into it */
+   uint8_t padding0 : 1;
    uint8_t padding1;
    uint8_t padding2;
 };
@@ -2068,6 +2069,7 @@ struct DeviceInfo {
 
    int16_t scratch_global_offset_min;
    int16_t scratch_global_offset_max;
+   unsigned max_nsa_vgprs;
 };
 
 enum class CompilationProgress {
diff --git a/src/amd/compiler/aco_lower_to_hw_instr.cpp b/src/amd/compiler/aco_lower_to_hw_instr.cpp
index c048aa406b8c..6da27e170d5c 100644
--- a/src/amd/compiler/aco_lower_to_hw_instr.cpp
+++ b/src/amd/compiler/aco_lower_to_hw_instr.cpp
@@ -2138,6 +2138,66 @@ hw_init_scratch(Builder& bld, Definition def, Operand scratch_addr, Operand scra
    }
 }
 
+void
+lower_image_sample(lower_context* ctx, aco_ptr<Instruction>& instr)
+{
+   Operand linear_vgpr = instr->operands[3];
+
+   unsigned nsa_size = ctx->program->dev.max_nsa_vgprs;
+   unsigned vaddr_size = linear_vgpr.size();
+   unsigned num_copied_vgprs = instr->operands.size() - 4;
+   nsa_size = num_copied_vgprs > 0 && (ctx->program->gfx_level >= GFX11 || vaddr_size <= nsa_size)
+                 ? nsa_size
+                 : 0;
+
+   Operand vaddr[16];
+   unsigned num_vaddr = 0;
+
+   if (nsa_size) {
+      assert(num_copied_vgprs <= nsa_size);
+      for (unsigned i = 0; i < num_copied_vgprs; i++)
+         vaddr[num_vaddr++] = instr->operands[4 + i];
+      for (unsigned i = num_copied_vgprs; i < std::min(vaddr_size, nsa_size); i++)
+         vaddr[num_vaddr++] = Operand(linear_vgpr.physReg().advance(i * 4), v1);
+      if (vaddr_size > nsa_size) {
+         RegClass rc = RegClass::get(RegType::vgpr, (vaddr_size - nsa_size) * 4);
+         vaddr[num_vaddr++] = Operand(PhysReg(linear_vgpr.physReg().advance(nsa_size * 4)), rc);
+      }
+   } else {
+      PhysReg reg = linear_vgpr.physReg();
+      std::map<PhysReg, copy_operation> copy_operations;
+      for (unsigned i = 4; i < instr->operands.size(); i++) {
+         Operand arg = instr->operands[i];
+         Definition def(reg, RegClass::get(RegType::vgpr, arg.bytes()));
+         copy_operations[def.physReg()] = {arg, def, def.bytes()};
+         reg = reg.advance(arg.bytes());
+      }
+      vaddr[num_vaddr++] = linear_vgpr;
+
+      Pseudo_instruction pi = {};
+      handle_operands(copy_operations, ctx, ctx->program->gfx_level, &pi);
+   }
+
+   instr->mimg().strict_wqm = false;
+
+   if ((3 + num_vaddr) > instr->operands.size()) {
+      MIMG_instruction *new_instr = create_instruction<MIMG_instruction>(
+         instr->opcode, Format::MIMG, 3 + num_vaddr, instr->definitions.size());
+      std::copy(instr->definitions.cbegin(), instr->definitions.cend(),
+                new_instr->definitions.begin());
+      new_instr->operands[0] = instr->operands[0];
+      new_instr->operands[1] = instr->operands[1];
+      new_instr->operands[2] = instr->operands[2];
+      memcpy((uint8_t*)new_instr + sizeof(Instruction), (uint8_t*)instr.get() + sizeof(Instruction),
+             sizeof(MIMG_instruction) - sizeof(Instruction));
+      instr.reset(new_instr);
+   } else {
+      while (instr->operands.size() > (3 + num_vaddr))
+         instr->operands.pop_back();
+   }
+   std::copy(vaddr, vaddr + num_vaddr, std::next(instr->operands.begin(), 3));
+}
+
 void
 lower_to_hw_instr(Program* program)
 {
@@ -2802,6 +2862,9 @@ lower_to_hw_instr(Program* program)
             ctx.instructions.emplace_back(std::move(instr));
 
             emit_set_mode(bld, block->fp_mode, set_round, false);
+         } else if (instr->isMIMG() && instr->mimg().strict_wqm) {
+            lower_image_sample(&ctx, instr);
+            ctx.instructions.emplace_back(std::move(instr));
          } else {
             ctx.instructions.emplace_back(std::move(instr));
          }
diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index 95fd684b222a..1e1a5551c508 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -2445,7 +2445,8 @@ get_affinities(ra_ctx& ctx, std::vector<IDSet>& live_out_per_block)
                    op.getTemp().type() == instr->definitions[0].getTemp().type())
                   ctx.vectors[op.tempId()] = instr.get();
             }
-         } else if (instr->format == Format::MIMG && instr->operands.size() > 4) {
+         } else if (instr->format == Format::MIMG && instr->operands.size() > 4 &&
+                    !instr->mimg().strict_wqm) {
             for (unsigned i = 3; i < instr->operands.size(); i++)
                ctx.vectors[instr->operands[i].tempId()] = instr.get();
          } else if (instr->opcode == aco_opcode::p_split_vector &&
diff --git a/src/amd/compiler/aco_validate.cpp b/src/amd/compiler/aco_validate.cpp
index 7a21f90b00ff..5e2ff297405f 100644
--- a/src/amd/compiler/aco_validate.cpp
+++ b/src/amd/compiler/aco_validate.cpp
@@ -696,21 +696,36 @@ validate_ir(Program* program)
                      "TFE/LWE loads",
                      instr.get());
             }
-            check(instr->operands.size() == 4 || program->gfx_level >= GFX10,
-                  "NSA is only supported on GFX10+", instr.get());
-            for (unsigned i = 3; i < instr->operands.size(); i++) {
-               check(instr->operands[i].hasRegClass() &&
-                        instr->operands[i].regClass().type() == RegType::vgpr,
-                     "MIMG operands[3+] (VADDR) must be VGPR", instr.get());
-               if (instr->operands.size() > 4) {
-                  if (program->gfx_level < GFX11) {
-                     check(instr->operands[i].regClass() == v1,
-                           "GFX10 MIMG VADDR must be v1 if NSA is used", instr.get());
-                  } else {
-                     if (instr->opcode != aco_opcode::image_bvh_intersect_ray &&
-                         instr->opcode != aco_opcode::image_bvh64_intersect_ray && i < 7) {
+
+            if (instr->mimg().strict_wqm) {
+               check(instr->operands[3].isTemp() && instr->operands[3].regClass().is_linear_vgpr(),
+                     "MIMG operands[3] must be temp linear VGPR.", instr.get());
+
+               unsigned total_size = 0;
+               for (unsigned i = 4; i < instr->operands.size(); i++) {
+                  check(instr->operands[i].isTemp() && instr->operands[i].regClass() == v1,
+                        "MIMG operands[4+] (VADDR) must be v1", instr.get());
+                  total_size += instr->operands[i].bytes();
+               }
+               check(total_size <= instr->operands[3].bytes(),
+                     "MIMG operands[4+] must fit within operands[3].", instr.get());
+            } else {
+               check(instr->operands.size() == 4 || program->gfx_level >= GFX10,
+                     "NSA is only supported on GFX10+", instr.get());
+               for (unsigned i = 3; i < instr->operands.size(); i++) {
+                  check(instr->operands[i].hasRegClass() &&
+                           instr->operands[i].regClass().type() == RegType::vgpr,
+                        "MIMG operands[3+] (VADDR) must be VGPR", instr.get());
+                  if (instr->operands.size() > 4) {
+                     if (program->gfx_level < GFX11) {
                         check(instr->operands[i].regClass() == v1,
-                              "first 4 GFX11 MIMG VADDR must be v1 if NSA is used", instr.get());
+                              "GFX10 MIMG VADDR must be v1 if NSA is used", instr.get());
+                     } else {
+                        if (instr->opcode != aco_opcode::image_bvh_intersect_ray &&
+                            instr->opcode != aco_opcode::image_bvh64_intersect_ray && i < 7) {
+                           check(instr->operands[i].regClass() == v1,
+                                 "first 4 GFX11 MIMG VADDR must be v1 if NSA is used", instr.get());
+                        }
                      }
                   }
                }
-- 
GitLab


From fc23065665c81f618ebbc1bc8eb96a3945fb8348 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Fri, 14 Apr 2023 15:44:43 +0100
Subject: [PATCH 11/15] aco: implement strict_wqm_coord_amd

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 .../compiler/aco_instruction_selection.cpp    | 27 +++++++++++++++++++
 .../aco_instruction_selection_setup.cpp       |  7 +++++
 2 files changed, 34 insertions(+)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 9f2b52e9bddc..1a9a29620c03 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -9044,6 +9044,33 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
       ctx->block->kind |= block_kind_export_end;
       break;
    }
+   case nir_intrinsic_strict_wqm_coord_amd: {
+      Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
+      Temp src = get_ssa_temp(ctx, instr->src[0].ssa);
+      Temp tmp = bld.tmp(RegClass::get(RegType::vgpr, dst.bytes()));
+      unsigned begin_size = nir_intrinsic_base(instr);
+
+      unsigned num_src = 1;
+      auto it = ctx->allocated_vec.find(src.id());
+      if (it != ctx->allocated_vec.end())
+         num_src = src.bytes() / it->second[0].bytes();
+
+      aco_ptr<Pseudo_instruction> vec{create_instruction<Pseudo_instruction>(
+         aco_opcode::p_create_vector, Format::PSEUDO, num_src + !!begin_size, 1)};
+
+      if (begin_size)
+         vec->operands[0] = Operand(RegClass::get(RegType::vgpr, begin_size));
+      for (unsigned i = 0; i < num_src; i++) {
+         Temp comp = it != ctx->allocated_vec.end() ? it->second[i] : src;
+         vec->operands[i + !!begin_size] = Operand(comp);
+      }
+
+      vec->definitions[0] = Definition(tmp);
+      ctx->block->instructions.emplace_back(std::move(vec));
+
+      bld.pseudo(aco_opcode::p_start_linear_vgpr, Definition(dst), tmp);
+      break;
+   }
    default:
       isel_err(&instr->instr, "Unimplemented intrinsic instr");
       abort();
diff --git a/src/amd/compiler/aco_instruction_selection_setup.cpp b/src/amd/compiler/aco_instruction_selection_setup.cpp
index 0a67a2f5c671..28184fa29190 100644
--- a/src/amd/compiler/aco_instruction_selection_setup.cpp
+++ b/src/amd/compiler/aco_instruction_selection_setup.cpp
@@ -464,6 +464,13 @@ init_context(isel_context* ctx, nir_shader* shader)
                nir_intrinsic_instr* intrinsic = nir_instr_as_intrinsic(instr);
                if (!nir_intrinsic_infos[intrinsic->intrinsic].has_dest)
                   break;
+               if (intrinsic->intrinsic == nir_intrinsic_strict_wqm_coord_amd) {
+                  regclasses[intrinsic->dest.ssa.index] =
+                     RegClass::get(RegType::vgpr, intrinsic->dest.ssa.num_components * 4 +
+                                                     nir_intrinsic_base(intrinsic))
+                        .as_linear();
+                  break;
+               }
                RegType type = RegType::sgpr;
                switch (intrinsic->intrinsic) {
                case nir_intrinsic_load_push_constant:
-- 
GitLab


From 9d0e1182cfae3a09f821bb7f2746f4b872b39b6e Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Fri, 14 Apr 2023 17:49:46 +0100
Subject: [PATCH 12/15] aco: implement texture samples with strict WQM
 coordinates

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 .../compiler/aco_instruction_selection.cpp    | 37 ++++++++++++++++++-
 src/amd/compiler/aco_instruction_selection.h  |  1 +
 2 files changed, 36 insertions(+), 2 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 1a9a29620c03..fb6abaa60afb 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -5933,7 +5933,14 @@ emit_mimg(Builder& bld, aco_opcode op, Temp dst, Temp rsrc, Operand samp, std::v
    size_t nsa_size = bld.program->dev.max_nsa_vgprs;
    nsa_size = bld.program->gfx_level >= GFX11 || coords.size() <= nsa_size ? nsa_size : 0;
 
+   const bool strict_wqm = coords[0].regClass().is_linear_vgpr();
+   if (strict_wqm)
+      nsa_size = coords.size();
+
    for (unsigned i = 0; i < std::min(coords.size(), nsa_size); i++) {
+      if (!coords[i].id())
+         continue;
+
       coords[i] = as_vgpr(bld, coords[i]);
    }
 
@@ -5973,6 +5980,7 @@ emit_mimg(Builder& bld, aco_opcode op, Temp dst, Temp rsrc, Operand samp, std::v
    mimg->operands[2] = vdata;
    for (unsigned i = 0; i < coords.size(); i++)
       mimg->operands[3 + i] = Operand(coords[i]);
+   mimg->strict_wqm = strict_wqm;
 
    MIMG_instruction* res = mimg.get();
    bld.insert(std::move(mimg));
@@ -9102,10 +9110,10 @@ visit_tex(isel_context* ctx, nir_tex_instr* instr)
    Builder bld(ctx->program, ctx->block);
    bool has_bias = false, has_lod = false, level_zero = false, has_compare = false,
         has_offset = false, has_ddx = false, has_ddy = false, has_derivs = false,
-        has_sample_index = false, has_clamped_lod = false;
+        has_sample_index = false, has_clamped_lod = false, has_wqm_coord = false;
    Temp resource, sampler, bias = Temp(), compare = Temp(), sample_index = Temp(), lod = Temp(),
                            offset = Temp(), ddx = Temp(), ddy = Temp(), clamped_lod = Temp(),
-                           coord = Temp();
+                           coord = Temp(), wqm_coord = Temp();
    std::vector<Temp> coords;
    std::vector<Temp> derivs;
    nir_const_value* const_offset[4] = {NULL, NULL, NULL, NULL};
@@ -9144,6 +9152,12 @@ visit_tex(isel_context* ctx, nir_tex_instr* instr)
          coord = get_ssa_temp_tex(ctx, instr->src[i].src.ssa, a16);
          break;
       }
+      case nir_tex_src_backend1: {
+         assert(instr->src[i].src.ssa->bit_size == 32);
+         wqm_coord = get_ssa_temp(ctx, instr->src[i].src.ssa);
+         has_wqm_coord = true;
+         break;
+      }
       case nir_tex_src_bias:
          assert(instr->src[i].src.ssa->bit_size == (a16 ? 16 : 32));
          /* Doesn't need get_ssa_temp_tex because we pack it into its own dword anyway. */
@@ -9173,6 +9187,7 @@ visit_tex(isel_context* ctx, nir_tex_instr* instr)
          }
          break;
       case nir_tex_src_offset:
+      case nir_tex_src_backend2:
          assert(instr->src[i].src.ssa->bit_size == 32);
          offset = get_ssa_temp(ctx, instr->src[i].src.ssa);
          get_const_vec(instr->src[i].src.ssa, const_offset);
@@ -9199,6 +9214,12 @@ visit_tex(isel_context* ctx, nir_tex_instr* instr)
       }
    }
 
+   if (has_wqm_coord) {
+      assert(instr->op == nir_texop_tex || instr->op == nir_texop_txb || instr->op == nir_texop_lod);
+      assert(wqm_coord.regClass().is_linear_vgpr());
+      assert(!a16 && !g16);
+   }
+
    if (instr->op == nir_texop_tg4 && !has_lod && !instr->is_gather_implicit_lod)
       level_zero = true;
 
@@ -9467,6 +9488,11 @@ visit_tex(isel_context* ctx, nir_tex_instr* instr)
 
    /* gather MIMG address components */
    std::vector<Temp> args;
+   if (has_wqm_coord) {
+      args.emplace_back(wqm_coord);
+      if (!(ctx->block->kind & block_kind_top_level))
+         ctx->unended_linear_vgprs.push_back(wqm_coord);
+   }
    if (has_offset)
       args.emplace_back(offset);
    if (has_bias)
@@ -10043,6 +10069,13 @@ visit_jump(isel_context* ctx, nir_jump_instr* instr)
 void
 visit_block(isel_context* ctx, nir_block* block)
 {
+   if (ctx->block->kind & block_kind_top_level) {
+      Builder bld(ctx->program, ctx->block);
+      for (Temp tmp : ctx->unended_linear_vgprs)
+         bld.pseudo(aco_opcode::p_end_linear_vgpr, tmp);
+      ctx->unended_linear_vgprs.clear();
+   }
+
    ctx->block->instructions.reserve(ctx->block->instructions.size() +
                                     exec_list_length(&block->instr_list) * 2);
    nir_foreach_instr (instr, block) {
diff --git a/src/amd/compiler/aco_instruction_selection.h b/src/amd/compiler/aco_instruction_selection.h
index 771c608d9ecd..f1b1b04ce8cc 100644
--- a/src/amd/compiler/aco_instruction_selection.h
+++ b/src/amd/compiler/aco_instruction_selection.h
@@ -62,6 +62,7 @@ struct isel_context {
    Block* block;
    uint32_t first_temp_id;
    std::unordered_map<unsigned, std::array<Temp, NIR_MAX_VEC_COMPONENTS>> allocated_vec;
+   std::vector<Temp> unended_linear_vgprs;
    Stage stage;
    struct {
       bool has_branch;
-- 
GitLab


From 08f0751e44828fc83948b8947c4d038b032ff756 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Fri, 14 Apr 2023 17:50:04 +0100
Subject: [PATCH 13/15] radv: use fix_derivs_in_divergent_cf

fossil-db (navi21):
Totals from 3256 (2.40% of 135636) affected shaders:
MaxWaves: 65430 -> 64366 (-1.63%)
Instrs: 3517336 -> 3517724 (+0.01%); split: -0.12%, +0.13%
CodeSize: 18963788 -> 18946904 (-0.09%); split: -0.14%, +0.05%
VGPRs: 172464 -> 175872 (+1.98%); split: -0.02%, +2.00%
Latency: 33643792 -> 33643179 (-0.00%); split: -0.12%, +0.12%
InvThroughput: 5912965 -> 5934404 (+0.36%); split: -0.09%, +0.46%
VClause: 60268 -> 60275 (+0.01%); split: -0.12%, +0.13%
SClause: 125227 -> 125241 (+0.01%); split: -0.09%, +0.10%
Copies: 253452 -> 254638 (+0.47%); split: -1.29%, +1.76%
Branches: 100951 -> 100953 (+0.00%); split: -0.00%, +0.00%
PreSGPRs: 186403 -> 185641 (-0.41%)
PreVGPRs: 153751 -> 156915 (+2.06%); split: -0.06%, +2.12%

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/vulkan/radv_pipeline.c | 21 +++++++++++++++------
 1 file changed, 15 insertions(+), 6 deletions(-)

diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index 275531f6b977..e9c58ef76dc3 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -530,12 +530,21 @@ radv_postprocess_nir(struct radv_device *device, const struct radv_pipeline_layo
    if (progress)
       nir_shader_gather_info(stage->nir, nir_shader_get_entrypoint(stage->nir));
 
-   NIR_PASS(
-      _, stage->nir, ac_nir_lower_tex,
-      &(ac_nir_lower_tex_options){
-         .gfx_level = gfx_level,
-         .lower_array_layer_round_even = !device->physical_device->rad_info.conformant_trunc_coord,
-      });
+   bool fix_derivs_in_divergent_cf =
+      stage->stage == MESA_SHADER_FRAGMENT && !radv_use_llvm_for_stage(device, stage->stage);
+   if (fix_derivs_in_divergent_cf) {
+      NIR_PASS(_, stage->nir, nir_convert_to_lcssa, true, true);
+      nir_divergence_analysis(stage->nir);
+   }
+   NIR_PASS(_, stage->nir, ac_nir_lower_tex,
+            &(ac_nir_lower_tex_options){
+               .gfx_level = gfx_level,
+               .lower_array_layer_round_even = !device->physical_device->rad_info.conformant_trunc_coord,
+               .fix_derivs_in_divergent_cf = fix_derivs_in_divergent_cf,
+               .max_wqm_vgprs = 64, // TODO: improve spiller and RA support for linear VGPRs
+            });
+   if (fix_derivs_in_divergent_cf)
+      NIR_PASS(_, stage->nir, nir_opt_remove_phis); /* cleanup LCSSA phis */
 
    if (stage->nir->info.uses_resource_info_query)
       NIR_PASS(_, stage->nir, ac_nir_lower_resinfo, gfx_level);
-- 
GitLab


From 4a0eeafe7948996db27bf3b34105ada58a9eb60d Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Tue, 9 May 2023 19:59:50 +0100
Subject: [PATCH 14/15] aco/tests: improve performance of declaration parsing

Unlike \S, \w only matches characters which are valid in identifiers. This
seems to be much faster, especially for longer identifier names.

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/tests/glsl_scraper.py | 12 ++++++------
 1 file changed, 6 insertions(+), 6 deletions(-)

diff --git a/src/amd/compiler/tests/glsl_scraper.py b/src/amd/compiler/tests/glsl_scraper.py
index c2ca6ebb93e5..291b33958f41 100644
--- a/src/amd/compiler/tests/glsl_scraper.py
+++ b/src/amd/compiler/tests/glsl_scraper.py
@@ -28,21 +28,21 @@ stage_to_glslang_stage = {
 }
 
 base_layout_qualifier_id_re = r'({0}\s*=\s*(?P<{0}>\d+))'
-id_re = '(?P<name_%d>[^(gl_)]\S+)'
-type_re = '(?P<dtype_%d>\S+)'
+id_re = '(?P<name_%d>[^(gl_)]\w+)'
+type_re = '(?P<dtype_%d>\w+)'
 location_re = base_layout_qualifier_id_re.format('location')
 component_re = base_layout_qualifier_id_re.format('component')
 binding_re = base_layout_qualifier_id_re.format('binding')
 set_re = base_layout_qualifier_id_re.format('set')
-unk_re = r'\S+(=\d+)?'
+unk_re = r'\w+(=\d+)?'
 layout_qualifier_re = r'layout\W*\((%s)+\)' % '|'.join([location_re, binding_re, set_re, unk_re, '[, ]+'])
 ubo_decl_re = 'uniform\W+%s(\W*{)?(?P<type_ubo>)' % (id_re%0)
 ssbo_decl_re = 'buffer\W+%s(\W*{)?(?P<type_ssbo>)' % (id_re%1)
 image_buffer_decl_re = r'uniform\W+imageBuffer\w+%s;(?P<type_img_buf>)' % (id_re%2)
-image_decl_re = r'uniform\W+image\S+\W+%s;(?P<type_img>)' % (id_re%3)
+image_decl_re = r'uniform\W+image\w+\W+%s;(?P<type_img>)' % (id_re%3)
 texture_buffer_decl_re = r'uniform\W+textureBuffer\w+%s;(?P<type_tex_buf>)' % (id_re%4)
-combined_texture_sampler_decl_re = r'uniform\W+sampler\S+\W+%s;(?P<type_combined>)' % (id_re%5)
-texture_decl_re = r'uniform\W+texture\S+\W+%s;(?P<type_tex>)' % (id_re%6)
+combined_texture_sampler_decl_re = r'uniform\W+sampler\w+\W+%s;(?P<type_combined>)' % (id_re%5)
+texture_decl_re = r'uniform\W+texture\w+\W+%s;(?P<type_tex>)' % (id_re%6)
 sampler_decl_re = r'uniform\W+sampler\w+%s;(?P<type_samp>)' % (id_re%7)
 input_re = r'in\W+%s\W+%s;(?P<type_in>)' % (type_re%0, id_re%8)
 output_re = r'out\W+%s\W+%s;(?P<type_out>)' % (type_re%1, id_re%9)
-- 
GitLab


From 5604b8c4f68853ba994ace3533f58fd468b60e24 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Fri, 14 Apr 2023 17:50:15 +0100
Subject: [PATCH 15/15] aco/tests: add fix_derivs_in_divergent_cf tests

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/tests/meson.build           |   2 +
 src/amd/compiler/tests/test_d3d11_derivs.cpp | 617 +++++++++++++++++++
 2 files changed, 619 insertions(+)
 create mode 100644 src/amd/compiler/tests/test_d3d11_derivs.cpp

diff --git a/src/amd/compiler/tests/meson.build b/src/amd/compiler/tests/meson.build
index d807426a3a8e..6d06d8bf0b5c 100644
--- a/src/amd/compiler/tests/meson.build
+++ b/src/amd/compiler/tests/meson.build
@@ -24,6 +24,7 @@ aco_tests_files = files(
   'main.cpp',
   'test_assembler.cpp',
   'test_builder.cpp',
+  'test_d3d11_derivs.cpp',
   'test_hard_clause.cpp',
   'test_insert_nops.cpp',
   'test_insert_waitcnt.cpp',
@@ -39,6 +40,7 @@ aco_tests_files = files(
 
 spirv_files = files(
   'test_isel.cpp',
+  'test_d3d11_derivs.cpp',
 )
 
 gen_spirv = generator(prog_python,
diff --git a/src/amd/compiler/tests/test_d3d11_derivs.cpp b/src/amd/compiler/tests/test_d3d11_derivs.cpp
new file mode 100644
index 000000000000..f2061c050ce4
--- /dev/null
+++ b/src/amd/compiler/tests/test_d3d11_derivs.cpp
@@ -0,0 +1,617 @@
+/*
+ * Copyright © 2023 Valve Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ *
+ */
+#include "helpers.h"
+#include "test_d3d11_derivs-spirv.h"
+
+using namespace aco;
+
+BEGIN_TEST(d3d11_derivs.simple)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec2 out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform sampler2D tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = texture(tex, in_coord);
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX10_3));
+   pbld.add_vsfs(vs, fs);
+
+   //>> v1: %x = v_interp_p2_f32 %_, %_:m0, (kill)%_ attr0.x
+   //>> v1: %y = v_interp_p2_f32 (kill)%_, (kill)%_:m0, (kill)%_ attr0.y
+   //>> v2: %vec = p_create_vector (kill)%x, (kill)%y
+   //>> lv2: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> BB1
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm 2d
+   //>> BB2
+   //>> BB6
+   //>> p_end_linear_vgpr (kill)%wqm
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+
+   //>> v_interp_p2_f32_e32 v#rx, v#_, attr0.x                                             ; $_
+   //>> v_interp_p2_f32_e32 v#ry_tmp, v#_, attr0.y                                         ; $_
+   //>> v_mov_b32_e32 v#ry, v#ry_tmp                                                       ; $_
+   //>> image_sample v[#_:#_], v[#rx:#ry], s[#_:#_], s[#_:#_] dmask:0xf dim:SQ_RSRC_IMG_2D ; $_ $_
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.constant)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in float in_coord;
+      layout(location = 0) out float out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in float in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform sampler2D tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = texture(tex, vec2(in_coord, -0.5));
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX10_3));
+   pbld.add_vsfs(vs, fs);
+
+   //>> v1: %x = v_interp_p2_f32 (kill)%_, (kill)%_:m0, (kill)%_ attr0.x
+   //>> v2: %vec = p_create_vector (kill)%x, -0.5
+   //>> lv2: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> BB1
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm 2d
+   //>> BB2
+   //>> BB6
+   //>> p_end_linear_vgpr (kill)%wqm
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+
+   //>> v_interp_p2_f32_e32 v#rx, v#_, attr0.x                                             ; $_
+   //>> v_mov_b32_e32 v#ry, -0.5                                                           ; $_
+   //>> image_sample v[#_:#_], v[#rx:#ry], s[#_:#_], s[#_:#_] dmask:0xf dim:SQ_RSRC_IMG_2D ; $_ $_
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.discard)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec2 out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform sampler2D tex;
+      void main() {
+         if (gl_FragCoord.y > 1.0)
+            discard;
+         out_color = texture(tex, in_coord);
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX10_3));
+   pbld.add_vsfs(vs, fs);
+
+   /* The interpolation must be done before the discard_if. */
+   //>> lv2: %wqm = p_start_linear_vgpr (kill)%_
+   //>> s2: %_:exec, s1: (kill)%_:scc = s_andn2_b64 %_:exec, %_
+   //>> s2: %_, s1: %_:scc = s_andn2_b64 (kill)%_, (kill)%_
+   //>> p_exit_early_if (kill)%_:scc
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, (kill)%wqm 2d
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.bias)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec2 out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform sampler2D tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = texture(tex, in_coord, gl_FragCoord.x);
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX10_3));
+   pbld.add_vsfs(vs, fs);
+
+   //>> s2: %_:s[0-1], s1: %_:s[2], s1: %_:s[3], s1: %_:s[4], v2: %_:v[0-1], v1: %bias:v[2] = p_startpgm
+   //>> v3: %vec = p_create_vector v1: undef, (kill)%_, (kill)%_
+   //>> lv3: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> BB1
+   //>> v4: %_ = image_sample_b (kill)%_, (kill)%_, v1: undef, %wqm, (kill)%bias 2d
+   //>> BB2
+   //>> BB6
+   //>> p_end_linear_vgpr (kill)%wqm
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+
+   //>> v_interp_p2_f32_e32 v#rx, v#_, attr0.x                                                       ; $_
+   //>> v_interp_p2_f32_e32 v#ry_tmp, v#_, attr0.y                                                   ; $_
+   //>> v_mov_b32_e32 v#rb, v2                                                                       ; $_
+   //>> v_mov_b32_e32 v#ry, v#ry_tmp                                                                 ; $_
+   //>> BB1:
+   //>> image_sample_b v[#_:#_], [v#rb, v#rx, v#ry], s[#_:#_], s[#_:#_] dmask:0xf dim:SQ_RSRC_IMG_2D ; $_ $_ $_
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.offset)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec2 out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform sampler2D tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = textureOffset(tex, in_coord, ivec2(1, 2));
+      }
+   );
+
+   /* Use GFX9 because we should have at least one test which doesn't use NSA. */
+   PipelineBuilder pbld(get_vk_device(GFX9));
+   pbld.add_vsfs(vs, fs);
+
+   //>> v3: %vec = p_create_vector v1: undef, (kill)%_, (kill)%_
+   //>> lv3: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> BB1
+   //>> v1: %offset = p_parallelcopy 0x201
+   //>> v4: %_ = image_sample_o (kill)%_, (kill)%_, v1: undef, %wqm, (kill)%offset 1d
+   //>> BB2
+   //>> BB6
+   //>> p_end_linear_vgpr (kill)%wqm
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+
+   //>> v_interp_p2_f32_e32 v#rx, v#_, attr0.x                            ; $_
+   //>> v_interp_p2_f32_e32 v#ry_tmp, v#_, attr0.y                        ; $_
+   //>> v_mov_b32_e32 v#ry, v#ry_tmp                                      ; $_
+   //>> BB1:
+   //>> v_mov_b32_e32 v#ro_tmp, 0x201                                     ; $_ $_
+   //>> v_mov_b32_e32 v#ro, v#r0_tmp                                      ; $_
+   //; success = ro+1 == rx and ro+2 == ry
+   //>> image_sample_o v[#_:#_], v[#ro:#rx], s[#_:#_], s[#_:#_] dmask:0xf ; $_ $_
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.array)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in vec3 in_coord;
+      layout(location = 0) out vec3 out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in vec3 in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform sampler2DArray tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = texture(tex, in_coord);
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX10_3));
+   pbld.add_vsfs(vs, fs);
+
+   //>> v1: %layer = v_rndne_f32 (kill)%_
+   //>> v3: %vec = p_create_vector (kill)%_, (kill)%_, (kill)%layer
+   //>> lv3: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> BB1
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm 2darray da
+   //>> BB2
+   //>> BB6
+   //>> p_end_linear_vgpr (kill)%wqm
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+
+   //>> v_interp_p2_f32_e32 v#rl_tmp, v#_, attr0.z                                               ; $_
+   //>> v_rndne_f32_e32 v#rl, v#rl_tmp                                                           ; $_
+   //>> v_interp_p2_f32_e32 v#rx, v#_, attr0.x                                                   ; $_
+   //>> v_interp_p2_f32_e32 v#ry_tmp, v#_, attr0.y                                               ; $_
+   //>> v_mov_b32_e32 v#ry, v#ry_tmp                                                             ; $_
+   //>> BB1:
+   //; success = rx+1 == ry and rx+2 == rl
+   //>> image_sample v[#_:#_], v[#rx:#rl], s[#_:#_], s[#_:#_] dmask:0xf dim:SQ_RSRC_IMG_2D_ARRAY ; $_ $_
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.bias_array)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in vec3 in_coord;
+      layout(location = 0) out vec3 out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in vec3 in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform sampler2DArray tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = texture(tex, in_coord, gl_FragCoord.x);
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX10_3));
+   pbld.add_vsfs(vs, fs);
+
+   //>> s2: %_:s[0-1], s1: %_:s[2], s1: %_:s[3], s1: %_:s[4], v2: %_:v[0-1], v1: %bias:v[2] = p_startpgm
+   //>> v1: %layer = v_rndne_f32 (kill)%_
+   //>> v4: %vec = p_create_vector v1: undef, (kill)%_, (kill)%_, (kill)%layer
+   //>> lv4: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> BB1
+   //>> v4: %_ = image_sample_b (kill)%_, (kill)%_, v1: undef, %wqm, (kill)%bias 2darray da
+   //>> BB2
+   //>> BB6
+   //>> p_end_linear_vgpr (kill)%wqm
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+
+   //>> v_interp_p2_f32_e32 v#rl_tmp, v#_, attr0.z                                                             ; $_
+   //>> v_rndne_f32_e32 v#rl, v#rl_tmp                                                                         ; $_
+   //>> v_interp_p2_f32_e32 v#rx_tmp, v#_, attr0.x                                                             ; $_
+   //>> v_interp_p2_f32_e32 v#ry_tmp, v#_, attr0.y                                                             ; $_
+   //>> v_mov_b32_e32 v#rx, v#rx_tmp                                                                           ; $_
+   //>> v_mov_b32_e32 v#ry, v#ry_tmp                                                                           ; $_
+   //>> BB1:
+   //>> image_sample_b v[#_:#_], [v2, v#rx, v#ry, v#rl], s[#_:#_], s[#_:#_] dmask:0xf dim:SQ_RSRC_IMG_2D_ARRAY ; $_ $_ $_
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.1d_gfx9)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in float in_coord;
+      layout(location = 0) out float out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in float in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform sampler1D tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = texture(tex, in_coord);
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX9));
+   pbld.add_vsfs(vs, fs);
+
+   //>> v1: %x = v_interp_p2_f32 (kill)%_, (kill)%_:m0, (kill)%_ attr0.x
+   //>> v2: %vec = p_create_vector (kill)%x, 0.5
+   //>> lv2: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> BB1
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm 1d
+   //>> BB2
+   //>> BB6
+   //>> p_end_linear_vgpr (kill)%wqm
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+
+   //>> v_interp_p2_f32_e32 v#rx, v#_, attr0.x                    ; $_
+   //>> v_mov_b32_e32 v#ry, 0.5                                   ; $_
+   //; success = rx+1 == ry
+   //>> image_sample v[#_:#_], v#rx, s[#_:#_], s[#_:#_] dmask:0xf ; $_ $_
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.1d_array_gfx9)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec2 out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform sampler1DArray tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = texture(tex, in_coord);
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX9));
+   pbld.add_vsfs(vs, fs);
+
+   //>> v1: %layer = v_rndne_f32 (kill)%_
+   //>> v1: %x = v_interp_p2_f32 (kill)%_, (kill)%_:m0, (kill)%_ attr0.x
+   //>> v3: %vec = p_create_vector (kill)%x, 0.5, (kill)%layer
+   //>> lv3: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> BB1
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm 1d da
+   //>> BB2
+   //>> BB6
+   //>> p_end_linear_vgpr (kill)%wqm
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+
+   //>> v_interp_p2_f32_e32 v#rl_tmp, v#_, attr0.y                   ; $_
+   //>> v_rndne_f32_e32 v#rl, v#rl_tmp                               ; $_
+   //>> v_interp_p2_f32_e32 v#rx_tmp, v#_, attr0.x                   ; $_
+   //>> v_mov_b32_e32 v#rx, v#rx_tmp                                 ; $_
+   //>> v_mov_b32_e32 v#ry, 0.5                                      ; $_
+   //>> BB1:
+   //; success = rx+1 == ry and rx+2 == rl
+   //>> image_sample v[#_:#_], v#rx, s[#_:#_], s[#_:#_] dmask:0xf da ; $_ $_
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.cube)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in vec3 in_coord;
+      layout(location = 0) out vec3 out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in vec3 in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform samplerCube tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = texture(tex, in_coord);
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX10_3));
+   pbld.add_vsfs(vs, fs);
+
+   //>> v1: %face = v_cubeid_f32 (kill)%_, (kill)%_, (kill)%_
+   //>> v1: %x = v_fmaak_f32 (kill)%_, %_, 0x3fc00000
+   //>> v1: %y = v_fmaak_f32 (kill)%_, (kill)%_, 0x3fc00000
+   //>> v3: %vec = p_create_vector (kill)%x, (kill)%y, (kill)%face
+   //>> lv3: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> BB1
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm cube da
+   //>> BB2
+   //>> BB6
+   //>> p_end_linear_vgpr (kill)%wqm
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+
+   //>> v_cubeid_f32 v#rf, v#_, v#_, v#_                                                     ; $_ $_
+   //>> v_fmaak_f32 v#rx, v#_, v#_, 0x3fc00000                                               ; $_ $_
+   //>> v_fmaak_f32 v#ry, v#_, v#_, 0x3fc00000                                               ; $_ $_
+   //; success = rx+1 == ry and rx+2 == rf
+   //>> image_sample v[#_:#_], v[#rx:#rf], s[#_:#_], s[#_:#_] dmask:0xf dim:SQ_RSRC_IMG_CUBE ; $_ $_
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.cube_array)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in vec4 in_coord;
+      layout(location = 0) out vec4 out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in vec4 in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform samplerCubeArray tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = texture(tex, in_coord);
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX10_3));
+   pbld.add_vsfs(vs, fs);
+
+   //>> v1: %layer = v_rndne_f32 (kill)%_
+   //>> v1: %face = v_cubeid_f32 (kill)%_, (kill)%_, (kill)%_
+   //>> v1: %x = v_fmaak_f32 (kill)%_, %_, 0x3fc00000
+   //>> v1: %y = v_fmaak_f32 (kill)%_, (kill)%_, 0x3fc00000
+   //>> v1: %face_layer = v_fmamk_f32 (kill)%layer, (kill)%face, 0x41000000
+   //>> v3: %vec = p_create_vector (kill)%x, (kill)%y, (kill)%face_layer
+   //>> lv3: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> BB1
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm cube da
+   //>> BB2
+   //>> BB6
+   //>> p_end_linear_vgpr (kill)%wqm
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+
+   //>> v_rndne_f32_e32 v#rl, v#_                                                             ; $_
+   //>> v_cubeid_f32 v#rf, v#_, v#_, v#_                                                      ; $_ $_
+   //>> v_fmaak_f32 v#rx, v#_, v#_, 0x3fc00000                                                ; $_ $_
+   //>> v_fmaak_f32 v#ry, v#_, v#_, 0x3fc00000                                                ; $_ $_
+   //>> v_fmamk_f32 v#rlf, v#rl, 0x41000000, v#rf                                             ; $_ $_
+   //>> BB1:
+   //; success = rx+1 == ry and rx+2 == rlf
+   //>> image_sample v[#_:#_], v[#rx:#rlf], s[#_:#_], s[#_:#_] dmask:0xf dim:SQ_RSRC_IMG_CUBE ; $_ $_
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.fddxy)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec2 out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform sampler2D tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = vec4(dFdxFine(in_coord.x), dFdyCoarse(in_coord.y), textureLod(tex, vec2(0.5), 0.0).xy);
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX10_3));
+   pbld.add_vsfs(vs, fs);
+
+   /* Must be before BB1 */
+   //>> v1: %_ = v_sub_f32 (kill)%_, (kill)%_ quad_perm:[1,1,3,3] bound_ctrl:1
+   //>> v1: %_ = v_sub_f32 (kill)%_, (kill)%_ quad_perm:[2,2,2,2] bound_ctrl:1
+   //>> BB1
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+END_TEST
+
+/* Ensure the BC optimize transform is done after ac_nir_lower_tex. */
+BEGIN_TEST(d3d11_derivs.bc_optimize)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec2 out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform sampler2D tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = texture(tex, vec2(in_coord.x, interpolateAtCentroid(in_coord.y)));
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX10_3));
+   pbld.add_vsfs(vs, fs);
+
+   //>> v1: %y_coord2 = v_cndmask_b32 (kill)%_, %_, (kill)%_
+   //>> v1: %x = v_interp_p2_f32 (kill)%_, %_:m0, (kill)%_ attr0.x
+   //>> v1: %y = v_interp_p2_f32 (kill)%y_coord2, (kill)%_:m0, (kill)%_ attr0.y
+   //>> v2: %vec = p_create_vector (kill)%x, (kill)%y
+   //>> lv2: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> BB1
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm 2d
+   //>> BB2
+   //>> BB6
+   //>> p_end_linear_vgpr (kill)%wqm
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.get_lod)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec2 out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec2 out_color;
+      layout(binding = 0) uniform sampler2D tex;
+      void main() {
+         out_color = vec2(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = textureQueryLod(tex, in_coord);
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX10_3));
+   pbld.add_vsfs(vs, fs);
+
+   //>> v1: %x = v_interp_p2_f32 %_, %_:m0, (kill)%_ attr0.x
+   //>> v1: %y = v_interp_p2_f32 (kill)%_, (kill)%_:m0, (kill)%_ attr0.y
+   //>> v2: %vec = p_create_vector %x, %y
+   //>> lv2: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> v1: %x0 = v_mov_b32 %x quad_perm:[0,0,0,0] bound_ctrl:1
+   //>> v1: %x1_m_x0 = v_sub_f32 %x, %x0 quad_perm:[1,1,1,1] bound_ctrl:1
+   //>> v1: %x2_m_x0 = v_sub_f32 (kill)%x, (kill)%x0 quad_perm:[2,2,2,2] bound_ctrl:1
+   //>> v1: %y0 = v_mov_b32 %y quad_perm:[0,0,0,0] bound_ctrl:1
+   //>> v1: %y1_m_y0 = v_sub_f32 %y, %y0 quad_perm:[1,1,1,1] bound_ctrl:1
+   //>> v1: %y2_m_y0 = v_sub_f32 (kill)%y, (kill)%y0 quad_perm:[2,2,2,2] bound_ctrl:1
+   //>> BB1
+   //>> v2: %_ = image_get_lod (kill)%_, (kill)%_, v1: undef, %wqm 2d
+   //>> BB2
+   //>> BB6
+   //>> p_end_linear_vgpr (kill)%wqm
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.nsa_max)
+   for (amd_gfx_level lvl : {GFX10, GFX10_3, GFX11}) {
+      if (!setup_cs(NULL, lvl))
+         continue;
+
+      PhysReg reg_v0{256};
+      PhysReg reg_v6{256 + 6};
+      PhysReg reg_v7{256 + 7};
+      PhysReg reg_v8{256 + 8};
+
+      //>> p_unit_test 0
+      bld.pseudo(aco_opcode::p_unit_test, Operand::zero());
+
+      //~gfx10! v2: %_:v[0-1] = v_lshrrev_b64 0, %_:v[6-7]
+      //~gfx10! v1: %_:v[2] = v_mov_b32 %_:v[8]
+      //~gfx10! v4: %_:v[0-3] = image_sample_c_b_o  s8: undef,  s4: undef,  v1: undef, %_:v[0-5] 2darray da
+
+      //~gfx10_3! v4: %_:v[0-3] = image_sample_c_b_o  s8: undef,  s4: undef,  v1: undef, %_:v[6], %_:v[7], %_:v[8], %_:v[3], %_:v[4], %_:v[5] 2darray da
+
+      //~gfx11! v4: %_:v[0-3] = image_sample_c_b_o  s8: undef,  s4: undef,  v1: undef, %_:v[6], %_:v[7], %_:v[8], %_:v[3], %_:v[4-5] 2darray da
+
+      Instruction *instr = bld.mimg(aco_opcode::image_sample_c_b_o, Definition(reg_v0, v4),
+                                    Operand(s8), Operand(s4), Operand(v1), Operand(reg_v0, v6.as_linear()),
+                                    Operand(reg_v6, v1), Operand(reg_v7, v1), Operand(reg_v8, v1));
+      instr->mimg().dim = ac_image_2darray;
+      instr->mimg().da = true;
+      instr->mimg().strict_wqm = true;
+
+      finish_to_hw_instr_test();
+   }
+END_TEST
-- 
GitLab

