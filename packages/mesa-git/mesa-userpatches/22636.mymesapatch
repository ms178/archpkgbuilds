From 070843a456f0306aac3195ac11bc072fec66c3fc Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Wed, 12 Apr 2023 20:12:26 +0100
Subject: [PATCH 01/13] nir/fold_16bit_tex_image: skip tex instructions with
 backend1

This will be used for RADV/ACO in the future, and I don't want to and
don't have to deal with 16-bit.

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/compiler/nir/nir_lower_mediump.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/src/compiler/nir/nir_lower_mediump.c b/src/compiler/nir/nir_lower_mediump.c
index 8698b0d9f928..c2cecd5536b1 100644
--- a/src/compiler/nir/nir_lower_mediump.c
+++ b/src/compiler/nir/nir_lower_mediump.c
@@ -958,6 +958,9 @@ fold_16bit_tex_srcs(nir_builder *b, nir_tex_instr *tex,
    if (!(options->sampler_dims & BITFIELD_BIT(tex->sampler_dim)))
       return false;
 
+   if (nir_tex_instr_src_index(tex, nir_tex_src_backend1) >= 0)
+      return false;
+
    unsigned fold_srcs = 0;
    for (unsigned i = 0; i < tex->num_srcs; i++) {
       /* Filter out sources that should be ignored. */
-- 
GitLab


From 16ea1d8bc991a04906dde002ae1e897b0e3d9900 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Wed, 5 Apr 2023 15:52:40 +0100
Subject: [PATCH 02/13] nir,vtn,aco,ac/llvm: make cube_face_coord_amd more
 direct

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_instruction_selection.cpp |  7 +------
 src/amd/llvm/ac_nir_to_llvm.c                  | 17 +++++------------
 src/compiler/nir/nir_opcodes.py                | 14 +++++---------
 src/compiler/spirv/vtn_amd.c                   |  6 +++++-
 4 files changed, 16 insertions(+), 28 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 2e09da8b00a0..1d247344f55b 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -2503,14 +2503,9 @@ visit_alu_instr(isel_context* ctx, nir_alu_instr* instr)
       Temp src[3] = {emit_extract_vector(ctx, in, 0, v1), emit_extract_vector(ctx, in, 1, v1),
                      emit_extract_vector(ctx, in, 2, v1)};
       Temp ma = bld.vop3(aco_opcode::v_cubema_f32, bld.def(v1), src[0], src[1], src[2]);
-      ma = bld.vop1(aco_opcode::v_rcp_f32, bld.def(v1), ma);
       Temp sc = bld.vop3(aco_opcode::v_cubesc_f32, bld.def(v1), src[0], src[1], src[2]);
       Temp tc = bld.vop3(aco_opcode::v_cubetc_f32, bld.def(v1), src[0], src[1], src[2]);
-      sc = bld.vop2(aco_opcode::v_add_f32, bld.def(v1), Operand::c32(0x3f000000u /*0.5*/),
-                    bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), sc, ma));
-      tc = bld.vop2(aco_opcode::v_add_f32, bld.def(v1), Operand::c32(0x3f000000u /*0.5*/),
-                    bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), tc, ma));
-      bld.pseudo(aco_opcode::p_create_vector, Definition(dst), sc, tc);
+      bld.pseudo(aco_opcode::p_create_vector, Definition(dst), sc, tc, ma);
       break;
    }
    case nir_op_cube_face_index_amd: {
diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index dadfd248f1f2..db0d092079a5 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -1263,21 +1263,14 @@ static bool visit_alu(struct ac_nir_context *ctx, const nir_alu_instr *instr)
 
    case nir_op_cube_face_coord_amd: {
       src[0] = ac_to_float(&ctx->ac, src[0]);
-      LLVMValueRef results[2];
+      LLVMValueRef results[3];
       LLVMValueRef in[3];
       for (unsigned chan = 0; chan < 3; chan++)
          in[chan] = ac_llvm_extract_elem(&ctx->ac, src[0], chan);
-      results[0] = ac_build_intrinsic(&ctx->ac, "llvm.amdgcn.cubesc", ctx->ac.f32, in, 3,
-                                      0);
-      results[1] = ac_build_intrinsic(&ctx->ac, "llvm.amdgcn.cubetc", ctx->ac.f32, in, 3,
-                                      0);
-      LLVMValueRef ma = ac_build_intrinsic(&ctx->ac, "llvm.amdgcn.cubema", ctx->ac.f32, in, 3, 0);
-      results[0] = ac_build_fdiv(&ctx->ac, results[0], ma);
-      results[1] = ac_build_fdiv(&ctx->ac, results[1], ma);
-      LLVMValueRef offset = LLVMConstReal(ctx->ac.f32, 0.5);
-      results[0] = LLVMBuildFAdd(ctx->ac.builder, results[0], offset, "");
-      results[1] = LLVMBuildFAdd(ctx->ac.builder, results[1], offset, "");
-      result = ac_build_gather_values(&ctx->ac, results, 2);
+      results[0] = ac_build_intrinsic(&ctx->ac, "llvm.amdgcn.cubesc", ctx->ac.f32, in, 3, 0);
+      results[1] = ac_build_intrinsic(&ctx->ac, "llvm.amdgcn.cubetc", ctx->ac.f32, in, 3, 0);
+      results[2] = ac_build_intrinsic(&ctx->ac, "llvm.amdgcn.cubema", ctx->ac.f32, in, 3, 0);
+      result = ac_build_gather_values(&ctx->ac, results, 3);
       break;
    }
 
diff --git a/src/compiler/nir/nir_opcodes.py b/src/compiler/nir/nir_opcodes.py
index bc263806c18f..fb9fa6ac9216 100644
--- a/src/compiler/nir/nir_opcodes.py
+++ b/src/compiler/nir/nir_opcodes.py
@@ -521,16 +521,15 @@ for (unsigned bit = 0; bit < bit_size; bit++) {
 """)
 
 # AMD_gcn_shader extended instructions
-unop_horiz("cube_face_coord_amd", 2, tfloat32, 3, tfloat32, """
-dst.x = dst.y = 0.0;
+unop_horiz("cube_face_coord_amd", 3, tfloat32, 3, tfloat32, """
+dst.x = dst.y = dst.z = 0.0;
 float absX = fabsf(src0.x);
 float absY = fabsf(src0.y);
 float absZ = fabsf(src0.z);
 
-float ma = 0.0;
-if (absX >= absY && absX >= absZ) { ma = 2 * src0.x; }
-if (absY >= absX && absY >= absZ) { ma = 2 * src0.y; }
-if (absZ >= absX && absZ >= absY) { ma = 2 * src0.z; }
+if (absX >= absY && absX >= absZ) { dst.z = 2 * src0.x; }
+if (absY >= absX && absY >= absZ) { dst.z = 2 * src0.y; }
+if (absZ >= absX && absZ >= absY) { dst.z = 2 * src0.z; }
 
 if (src0.x >= 0 && absX >= absY && absX >= absZ) { dst.x = -src0.z; dst.y = -src0.y; }
 if (src0.x < 0 && absX >= absY && absX >= absZ) { dst.x = src0.z; dst.y = -src0.y; }
@@ -538,9 +537,6 @@ if (src0.y >= 0 && absY >= absX && absY >= absZ) { dst.x = src0.x; dst.y = src0.
 if (src0.y < 0 && absY >= absX && absY >= absZ) { dst.x = src0.x; dst.y = -src0.z; }
 if (src0.z >= 0 && absZ >= absX && absZ >= absY) { dst.x = src0.x; dst.y = -src0.y; }
 if (src0.z < 0 && absZ >= absX && absZ >= absY) { dst.x = -src0.x; dst.y = -src0.y; }
-
-dst.x = dst.x * (1.0f / ma) + 0.5f;
-dst.y = dst.y * (1.0f / ma) + 0.5f;
 """)
 
 unop_horiz("cube_face_index_amd", 1, tfloat32, 3, tfloat32, """
diff --git a/src/compiler/spirv/vtn_amd.c b/src/compiler/spirv/vtn_amd.c
index 85df179d91c8..5cf489247a0c 100644
--- a/src/compiler/spirv/vtn_amd.c
+++ b/src/compiler/spirv/vtn_amd.c
@@ -35,9 +35,13 @@ vtn_handle_amd_gcn_shader_instruction(struct vtn_builder *b, SpvOp ext_opcode,
    case CubeFaceIndexAMD:
       def = nir_cube_face_index_amd(&b->nb, vtn_get_nir_ssa(b, w[5]));
       break;
-   case CubeFaceCoordAMD:
+   case CubeFaceCoordAMD: {
       def = nir_cube_face_coord_amd(&b->nb, vtn_get_nir_ssa(b, w[5]));
+      nir_ssa_def *st = nir_channels(&b->nb, def, 0x3);
+      nir_ssa_def *invma = nir_frcp(&b->nb, nir_channel(&b->nb, def, 2));
+      def = nir_ffma_imm2(&b->nb, st, invma, 0.5);
       break;
+   }
    case TimeAMD: {
       def = nir_pack_64_2x32(&b->nb, nir_shader_clock(&b->nb, NIR_SCOPE_SUBGROUP));
       break;
-- 
GitLab


From f7efa7f462ea5f0dc7a16842387009ec946fc724 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Wed, 5 Apr 2023 16:58:43 +0100
Subject: [PATCH 03/13] ac/nir: add pass for lowering 1d/cube coordinates

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/common/ac_nir.c | 199 ++++++++++++++++++++++++++++++++++++++++
 src/amd/common/ac_nir.h |   7 ++
 2 files changed, 206 insertions(+)

diff --git a/src/amd/common/ac_nir.c b/src/amd/common/ac_nir.c
index b4afb897ae98..e2cf8088d9bc 100644
--- a/src/amd/common/ac_nir.c
+++ b/src/amd/common/ac_nir.c
@@ -1083,3 +1083,202 @@ ac_nir_lower_legacy_gs(nir_shader *nir,
    if (progress)
       nir_metadata_preserve(impl, nir_metadata_none);
 }
+
+/**
+ * Build a manual selection sequence for cube face sc/tc coordinates and
+ * major axis vector (multiplied by 2 for consistency) for the given
+ * vec3 \p coords, for the face implied by \p selcoords.
+ *
+ * For the major axis, we always adjust the sign to be in the direction of
+ * selcoords.ma; i.e., a positive out_ma means that coords is pointed towards
+ * the selcoords major axis.
+ */
+static void
+build_cube_select(nir_builder *b, nir_ssa_def *ma, nir_ssa_def *id, nir_ssa_def *deriv,
+                  nir_ssa_def **out_ma, nir_ssa_def **out_sc, nir_ssa_def **out_tc)
+{
+   nir_ssa_def *deriv_x = nir_channel(b, deriv, 0);
+   nir_ssa_def *deriv_y = nir_channel(b, deriv, 1);
+   nir_ssa_def *deriv_z = nir_channel(b, deriv, 2);
+
+   nir_ssa_def *is_ma_positive = nir_fge(b, ma, nir_imm_float(b, 0.0));
+   nir_ssa_def *sgn_ma =
+      nir_bcsel(b, is_ma_positive, nir_imm_float(b, 1.0), nir_imm_float(b, -1.0));
+   nir_ssa_def *neg_sgn_ma = nir_fneg(b, sgn_ma);
+
+   nir_ssa_def *is_ma_z = nir_fge(b, id, nir_imm_float(b, 4.0));
+   nir_ssa_def *is_ma_y = nir_fge(b, id, nir_imm_float(b, 2.0));
+   is_ma_y = nir_iand(b, is_ma_y, nir_inot(b, is_ma_z));
+   nir_ssa_def *is_not_ma_x = nir_ior(b, is_ma_z, is_ma_y);
+
+   /* Select sc */
+   nir_ssa_def *tmp = nir_bcsel(b, is_not_ma_x, deriv_x, deriv_z);
+   nir_ssa_def *sgn =
+      nir_bcsel(b, is_ma_y, nir_imm_float(b, 1.0), nir_bcsel(b, is_ma_z, sgn_ma, neg_sgn_ma));
+   *out_sc = nir_fmul(b, tmp, sgn);
+
+   /* Select tc */
+   tmp = nir_bcsel(b, is_ma_y, deriv_z, deriv_y);
+   sgn = nir_bcsel(b, is_ma_y, sgn_ma, nir_imm_float(b, 1.0));
+   *out_tc = nir_fmul(b, tmp, sgn);
+
+   /* Select ma */
+   tmp = nir_bcsel(b, is_ma_z, deriv_z, nir_bcsel(b, is_ma_y, deriv_y, deriv_x));
+   *out_ma = nir_fmul_imm(b, nir_fabs(b, tmp), 2.0);
+}
+
+static void
+prepare_cube_coords(nir_builder *b, nir_tex_instr *tex, nir_ssa_def **coord, nir_src *ddx,
+                    nir_src *ddy, const ac_nir_lower_tex_options *options)
+{
+   nir_ssa_def *coords[NIR_MAX_VEC_COMPONENTS] = {0};
+   for (unsigned i = 0; i < (*coord)->num_components; i++)
+      coords[i] = nir_channel(b, *coord, i);
+
+   /* Section 8.9 (Texture Functions) of the GLSL 4.50 spec says:
+    *
+    *    "For Array forms, the array layer used will be
+    *
+    *       max(0, min(dâˆ’1, floor(layer+0.5)))
+    *
+    *     where d is the depth of the texture array and layer
+    *     comes from the component indicated in the tables below.
+    *     Workaroudn for an issue where the layer is taken from a
+    *     helper invocation which happens to fall on a different
+    *     layer due to extrapolation."
+    *
+    * GFX8 and earlier attempt to implement this in hardware by
+    * clamping the value of coords[2] = (8 * layer) + face.
+    * Unfortunately, this means that the we end up with the wrong
+    * face when clamping occurs.
+    *
+    * Clamp the layer earlier to work around the issue.
+    */
+   if (coords[3] && tex->is_array && options->gfx_level <= GFX8)
+      coords[3] = nir_fmax(b, coords[3], nir_imm_float(b, 0.0));
+
+   nir_ssa_def *cube_coords = nir_cube_face_coord_amd(b, nir_vec(b, coords, 3));
+   nir_ssa_def *sc = nir_channel(b, cube_coords, 0);
+   nir_ssa_def *tc = nir_channel(b, cube_coords, 1);
+   nir_ssa_def *ma = nir_channel(b, cube_coords, 2);
+   nir_ssa_def *invma = nir_frcp(b, nir_fabs(b, ma));
+   nir_ssa_def *id = nir_cube_face_index_amd(b, nir_vec(b, coords, 3));
+
+   if (ddx || ddy) {
+      sc = nir_fmul(b, sc, invma);
+      tc = nir_fmul(b, tc, invma);
+
+      /* Convert cube derivatives to 2D derivatives. */
+      for (unsigned i = 0; i < 2; i++) {
+         /* Transform the derivative alongside the texture
+          * coordinate. Mathematically, the correct formula is
+          * as follows. Assume we're projecting onto the +Z face
+          * and denote by dx/dh the derivative of the (original)
+          * X texture coordinate with respect to horizontal
+          * window coordinates. The projection onto the +Z face
+          * plane is:
+          *
+          *   f(x,z) = x/z
+          *
+          * Then df/dh = df/dx * dx/dh + df/dz * dz/dh
+          *            = 1/z * dx/dh - x/z * 1/z * dz/dh.
+          *
+          * This motivatives the implementation below.
+          *
+          * Whether this actually gives the expected results for
+          * apps that might feed in derivatives obtained via
+          * finite differences is anyone's guess. The OpenGL spec
+          * seems awfully quiet about how textureGrad for cube
+          * maps should be handled.
+          */
+         nir_ssa_def *deriv_ma, *deriv_sc, *deriv_tc;
+         build_cube_select(b, ma, id, i ? ddy->ssa : ddx->ssa, &deriv_ma, &deriv_sc, &deriv_tc);
+
+         deriv_ma = nir_fmul(b, deriv_ma, invma);
+
+         nir_ssa_def *x = nir_fsub(b, nir_fmul(b, deriv_sc, invma), nir_fmul(b, deriv_ma, sc));
+         nir_ssa_def *y = nir_fsub(b, nir_fmul(b, deriv_tc, invma), nir_fmul(b, deriv_ma, tc));
+
+         nir_instr_rewrite_src_ssa(&tex->instr, i ? ddy : ddx, nir_vec2(b, x, y));
+      }
+
+      sc = nir_fadd_imm(b, sc, 1.5);
+      tc = nir_fadd_imm(b, tc, 1.5);
+   } else {
+      sc = nir_ffma_imm2(b, sc, invma, 1.5);
+      tc = nir_ffma_imm2(b, tc, invma, 1.5);
+   }
+
+   if (coords[3] && tex->is_array)
+      id = nir_ffma_imm1(b, coords[3], 8.0, id);
+
+   *coord = nir_vec3(b, sc, tc, id);
+}
+
+static bool
+lower_tex_coords(nir_builder *b, nir_tex_instr *tex, nir_ssa_def **coords,
+                 const ac_nir_lower_tex_options *options)
+{
+   int ddx_idx = nir_tex_instr_src_index(tex, nir_tex_src_ddx);
+   int ddy_idx = nir_tex_instr_src_index(tex, nir_tex_src_ddy);
+
+   if (tex->sampler_dim != GLSL_SAMPLER_DIM_CUBE &&
+       !(tex->sampler_dim == GLSL_SAMPLER_DIM_1D && options->gfx_level == GFX9))
+      return false;
+
+   nir_src *ddx = ddx_idx >= 0 ? &tex->src[ddx_idx].src : NULL;
+   nir_src *ddy = ddy_idx >= 0 ? &tex->src[ddy_idx].src : NULL;
+
+   if (tex->sampler_dim == GLSL_SAMPLER_DIM_1D) {
+      nir_ssa_def *y =
+         nir_imm_floatN_t(b, tex->op == nir_texop_txf ? 0.0 : 0.5, (*coords)->bit_size);
+      if (tex->is_array && (*coords)->num_components > 1) {
+         nir_ssa_def *x = nir_channel(b, *coords, 0);
+         nir_ssa_def *idx = nir_channel(b, *coords, 1);
+         *coords = nir_vec3(b, x, y, idx);
+      } else {
+         *coords = nir_vec2(b, *coords, y);
+      }
+
+      if (ddx || ddy) {
+         nir_ssa_def *def = nir_vec2(b, ddx->ssa, nir_imm_floatN_t(b, 0.0, ddx->ssa->bit_size));
+         nir_instr_rewrite_src_ssa(&tex->instr, ddx, def);
+         def = nir_vec2(b, ddy->ssa, nir_imm_floatN_t(b, 0.0, ddy->ssa->bit_size));
+         nir_instr_rewrite_src_ssa(&tex->instr, ddy, def);
+      }
+   } else if (tex->sampler_dim == GLSL_SAMPLER_DIM_CUBE) {
+      prepare_cube_coords(b, tex, coords, ddx, ddy, options);
+   }
+
+   return true;
+}
+
+static bool
+lower_tex(nir_builder *b, nir_instr *instr, void *options_)
+{
+   const ac_nir_lower_tex_options *options = options_;
+   if (instr->type != nir_instr_type_tex)
+      return false;
+
+   nir_tex_instr *tex = nir_instr_as_tex(instr);
+   int coord_idx = nir_tex_instr_src_index(tex, nir_tex_src_coord);
+   if (coord_idx < 0 || nir_tex_instr_src_index(tex, nir_tex_src_backend1) >= 0)
+      return false;
+
+   b->cursor = nir_before_instr(instr);
+   nir_ssa_def *coords = tex->src[coord_idx].src.ssa;
+   if (lower_tex_coords(b, tex, &coords, options)) {
+      tex->coord_components = coords->num_components;
+      nir_instr_rewrite_src_ssa(&tex->instr, &tex->src[coord_idx].src, coords);
+      return true;
+   }
+
+   return false;
+}
+
+bool
+ac_nir_lower_tex(nir_shader *nir, const ac_nir_lower_tex_options *options)
+{
+   return nir_shader_instructions_pass(
+      nir, lower_tex, nir_metadata_block_index | nir_metadata_dominance, (void *)options);
+}
diff --git a/src/amd/common/ac_nir.h b/src/amd/common/ac_nir.h
index 07eb9f24f890..4c3b06335439 100644
--- a/src/amd/common/ac_nir.h
+++ b/src/amd/common/ac_nir.h
@@ -325,6 +325,13 @@ typedef struct {
 void
 ac_nir_lower_ps(nir_shader *nir, const ac_nir_lower_ps_options *options);
 
+typedef struct {
+   enum amd_gfx_level gfx_level;
+} ac_nir_lower_tex_options;
+
+bool
+ac_nir_lower_tex(nir_shader *nir, const ac_nir_lower_tex_options *options);
+
 #ifdef __cplusplus
 }
 #endif
-- 
GitLab


From 086969760bdae6ba038fd24c72d4ffedfa976d96 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Thu, 6 Apr 2023 11:43:29 +0100
Subject: [PATCH 04/13] ac/llvm: add ac_shader_abi::lower_1d_cube_coords

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/llvm/ac_nir_to_llvm.c                 | 9 +++++----
 src/amd/llvm/ac_shader_abi.h                  | 3 +++
 src/amd/vulkan/radv_nir_to_llvm.c             | 1 +
 src/gallium/drivers/radeonsi/si_shader_llvm.c | 1 +
 4 files changed, 10 insertions(+), 4 deletions(-)

diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index db0d092079a5..0f871a85132c 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -1621,7 +1621,7 @@ static LLVMValueRef build_tex_intrinsic(struct ac_nir_context *ctx, const nir_te
    }
 
    /* Fixup for GFX9 which allocates 1D textures as 2D. */
-   if (instr->op == nir_texop_lod && ctx->ac.gfx_level == GFX9) {
+   if (instr->op == nir_texop_lod && ctx->ac.gfx_level == GFX9 && ctx->abi->lower_1d_cube_coords) {
       if ((args->dim == ac_image_2darray || args->dim == ac_image_2d) && !args->coords[1]) {
          args->coords[1] = ctx->ac.i32_0;
       }
@@ -4576,12 +4576,12 @@ static void visit_tex(struct ac_nir_context *ctx, nir_tex_instr *instr)
          num_dest_deriv_channels = 2;
          break;
       case GLSL_SAMPLER_DIM_1D:
-         num_src_deriv_channels = 1;
          if (ctx->ac.gfx_level == GFX9) {
             num_dest_deriv_channels = 2;
          } else {
             num_dest_deriv_channels = 1;
          }
+         num_src_deriv_channels = ctx->abi->lower_1d_cube_coords ? 1 : num_dest_deriv_channels;
          break;
       }
 
@@ -4597,7 +4597,8 @@ static void visit_tex(struct ac_nir_context *ctx, nir_tex_instr *instr)
       }
    }
 
-   if (instr->sampler_dim == GLSL_SAMPLER_DIM_CUBE && args.coords[0]) {
+   if (instr->sampler_dim == GLSL_SAMPLER_DIM_CUBE && args.coords[0] &&
+       ctx->abi->lower_1d_cube_coords) {
       for (unsigned chan = 0; chan < instr->coord_components; chan++)
          args.coords[chan] = ac_to_float(&ctx->ac, args.coords[chan]);
       if (instr->coord_components == 3)
@@ -4624,7 +4625,7 @@ static void visit_tex(struct ac_nir_context *ctx, nir_tex_instr *instr)
    }
 
    if (ctx->ac.gfx_level == GFX9 && instr->sampler_dim == GLSL_SAMPLER_DIM_1D &&
-       instr->op != nir_texop_lod) {
+       instr->op != nir_texop_lod && ctx->abi->lower_1d_cube_coords) {
       LLVMValueRef filler;
       if (instr->op == nir_texop_txf)
          filler = args.a16 ? ctx->ac.i16_0 : ctx->ac.i32_0;
diff --git a/src/amd/llvm/ac_shader_abi.h b/src/amd/llvm/ac_shader_abi.h
index 0369d574a5fd..850a7dd6d198 100644
--- a/src/amd/llvm/ac_shader_abi.h
+++ b/src/amd/llvm/ac_shader_abi.h
@@ -121,6 +121,9 @@ struct ac_shader_abi {
 
    /* Equal to radeon_info::conformant_trunc_coord. */
    bool conformant_trunc_coord;
+
+   /* Whether to lower 1D/cube texture coordinates. */
+   bool lower_1d_cube_coords;
 };
 
 #endif /* AC_SHADER_ABI_H */
diff --git a/src/amd/vulkan/radv_nir_to_llvm.c b/src/amd/vulkan/radv_nir_to_llvm.c
index 0cde5b17b974..9565bbb693de 100644
--- a/src/amd/vulkan/radv_nir_to_llvm.c
+++ b/src/amd/vulkan/radv_nir_to_llvm.c
@@ -716,6 +716,7 @@ ac_translate_nir_to_llvm(struct ac_llvm_compiler *ac_llvm,
    ctx.abi.robust_buffer_access = options->robust_buffer_access;
    ctx.abi.load_grid_size_from_user_sgpr = args->load_grid_size_from_user_sgpr;
    ctx.abi.conformant_trunc_coord = options->conformant_trunc_coord;
+   ctx.abi.lower_1d_cube_coords = true;
 
    bool is_ngg = is_pre_gs_stage(shaders[0]->info.stage) && info->is_ngg;
    if (shader_count >= 2 || is_ngg)
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index 583abbd61e22..d573f1f06cbe 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -975,6 +975,7 @@ static bool si_llvm_translate_nir(struct si_shader_context *ctx, struct si_shade
                                 info->options & SI_PROFILE_CLAMP_DIV_BY_ZERO;
    ctx->abi.disable_aniso_single_level = true;
    ctx->abi.conformant_trunc_coord = ctx->screen->info.conformant_trunc_coord;
+   ctx->abi.lower_1d_cube_coords = true;
 
    bool ls_need_output =
       ctx->stage == MESA_SHADER_VERTEX && shader->key.ge.as_ls &&
-- 
GitLab


From d6cdd3557cf996ed64ab48d0bb2a96f93f7aac44 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Wed, 5 Apr 2023 17:03:52 +0100
Subject: [PATCH 05/13] radv: use pass for lowering 1d/cube coordinates

fossil-db (navi21):
Totals from 17077 (12.59% of 135636) affected shaders:
MaxWaves: 264173 -> 264149 (-0.01%)
Instrs: 24781649 -> 24778073 (-0.01%); split: -0.02%, +0.00%
CodeSize: 132873604 -> 132857192 (-0.01%); split: -0.02%, +0.01%
VGPRs: 1211936 -> 1212176 (+0.02%); split: -0.00%, +0.02%
Latency: 404496399 -> 404121404 (-0.09%); split: -0.09%, +0.00%
InvThroughput: 75223685 -> 75187440 (-0.05%); split: -0.07%, +0.03%
VClause: 425872 -> 425707 (-0.04%); split: -0.04%, +0.01%
SClause: 962984 -> 963202 (+0.02%); split: -0.01%, +0.03%
Copies: 1734980 -> 1734440 (-0.03%); split: -0.08%, +0.05%
PreSGPRs: 1162594 -> 1162644 (+0.00%); split: -0.00%, +0.00%
PreVGPRs: 1131429 -> 1131510 (+0.01%); split: -0.00%, +0.01%

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 .../compiler/aco_instruction_selection.cpp    | 142 +-----------------
 src/amd/vulkan/radv_nir_to_llvm.c             |   2 +-
 src/amd/vulkan/radv_pipeline.c                |   5 +
 3 files changed, 7 insertions(+), 142 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 1d247344f55b..ea9f84136c8b 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -9139,119 +9139,6 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
    }
 }
 
-void
-build_cube_select(isel_context* ctx, Temp ma, Temp id, Temp deriv, Temp* out_ma, Temp* out_sc,
-                  Temp* out_tc)
-{
-   Builder bld(ctx->program, ctx->block);
-
-   Temp deriv_x = emit_extract_vector(ctx, deriv, 0, v1);
-   Temp deriv_y = emit_extract_vector(ctx, deriv, 1, v1);
-   Temp deriv_z = emit_extract_vector(ctx, deriv, 2, v1);
-
-   Operand neg_one = Operand::c32(0xbf800000u);
-   Operand one = Operand::c32(0x3f800000u);
-   Operand two = Operand::c32(0x40000000u);
-   Operand four = Operand::c32(0x40800000u);
-
-   Temp is_ma_positive = bld.vopc(aco_opcode::v_cmp_le_f32, bld.def(bld.lm), Operand::zero(), ma);
-   Temp sgn_ma = bld.vop2_e64(aco_opcode::v_cndmask_b32, bld.def(v1), neg_one, one, is_ma_positive);
-   Temp neg_sgn_ma = bld.vop2(aco_opcode::v_sub_f32, bld.def(v1), Operand::zero(), sgn_ma);
-
-   Temp is_ma_z = bld.vopc(aco_opcode::v_cmp_le_f32, bld.def(bld.lm), four, id);
-   Temp is_ma_y = bld.vopc(aco_opcode::v_cmp_le_f32, bld.def(bld.lm), two, id);
-   is_ma_y = bld.sop2(Builder::s_andn2, bld.def(bld.lm), bld.def(s1, scc), is_ma_y, is_ma_z);
-   Temp is_not_ma_x = bld.sop2(Builder::s_or, bld.def(bld.lm), bld.def(s1, scc), is_ma_z, is_ma_y);
-
-   /* select sc */
-   Temp tmp = bld.vop2(aco_opcode::v_cndmask_b32, bld.def(v1), deriv_z, deriv_x, is_not_ma_x);
-   Temp sgn = bld.vop2_e64(
-      aco_opcode::v_cndmask_b32, bld.def(v1),
-      bld.vop2(aco_opcode::v_cndmask_b32, bld.def(v1), neg_sgn_ma, sgn_ma, is_ma_z), one, is_ma_y);
-   *out_sc = bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), tmp, sgn);
-
-   /* select tc */
-   tmp = bld.vop2(aco_opcode::v_cndmask_b32, bld.def(v1), deriv_y, deriv_z, is_ma_y);
-   sgn = bld.vop2(aco_opcode::v_cndmask_b32, bld.def(v1), neg_one, sgn_ma, is_ma_y);
-   *out_tc = bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), tmp, sgn);
-
-   /* select ma */
-   tmp = bld.vop2(aco_opcode::v_cndmask_b32, bld.def(v1),
-                  bld.vop2(aco_opcode::v_cndmask_b32, bld.def(v1), deriv_x, deriv_y, is_ma_y),
-                  deriv_z, is_ma_z);
-   tmp = bld.vop2(aco_opcode::v_and_b32, bld.def(v1), Operand::c32(0x7fffffffu), tmp);
-   *out_ma = bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), two, tmp);
-}
-
-void
-prepare_cube_coords(isel_context* ctx, std::vector<Temp>& coords, Temp* ddx, Temp* ddy,
-                    bool is_deriv, bool is_array)
-{
-   Builder bld(ctx->program, ctx->block);
-   Temp ma, tc, sc, id;
-   aco_opcode madak =
-      ctx->program->gfx_level >= GFX10_3 ? aco_opcode::v_fmaak_f32 : aco_opcode::v_madak_f32;
-   aco_opcode madmk =
-      ctx->program->gfx_level >= GFX10_3 ? aco_opcode::v_fmamk_f32 : aco_opcode::v_madmk_f32;
-
-   /* see comment in ac_prepare_cube_coords() */
-   if (is_array && ctx->options->gfx_level <= GFX8)
-      coords[3] = bld.vop2(aco_opcode::v_max_f32, bld.def(v1), Operand::zero(), coords[3]);
-
-   ma = bld.vop3(aco_opcode::v_cubema_f32, bld.def(v1), coords[0], coords[1], coords[2]);
-
-   aco_ptr<VALU_instruction> vop3a{
-      create_instruction<VALU_instruction>(aco_opcode::v_rcp_f32, asVOP3(Format::VOP1), 1, 1)};
-   vop3a->operands[0] = Operand(ma);
-   vop3a->abs[0] = true;
-   Temp invma = bld.tmp(v1);
-   vop3a->definitions[0] = Definition(invma);
-   ctx->block->instructions.emplace_back(std::move(vop3a));
-
-   sc = bld.vop3(aco_opcode::v_cubesc_f32, bld.def(v1), coords[0], coords[1], coords[2]);
-   if (!is_deriv)
-      sc = bld.vop2(madak, bld.def(v1), sc, invma, Operand::c32(0x3fc00000u /*1.5*/));
-
-   tc = bld.vop3(aco_opcode::v_cubetc_f32, bld.def(v1), coords[0], coords[1], coords[2]);
-   if (!is_deriv)
-      tc = bld.vop2(madak, bld.def(v1), tc, invma, Operand::c32(0x3fc00000u /*1.5*/));
-
-   id = bld.vop3(aco_opcode::v_cubeid_f32, bld.def(v1), coords[0], coords[1], coords[2]);
-
-   if (is_deriv) {
-      sc = bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), sc, invma);
-      tc = bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), tc, invma);
-
-      for (unsigned i = 0; i < 2; i++) {
-         /* see comment in ac_prepare_cube_coords() */
-         Temp deriv_ma;
-         Temp deriv_sc, deriv_tc;
-         build_cube_select(ctx, ma, id, i ? *ddy : *ddx, &deriv_ma, &deriv_sc, &deriv_tc);
-
-         deriv_ma = bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), deriv_ma, invma);
-
-         Temp x = bld.vop2(aco_opcode::v_sub_f32, bld.def(v1),
-                           bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), deriv_sc, invma),
-                           bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), deriv_ma, sc));
-         Temp y = bld.vop2(aco_opcode::v_sub_f32, bld.def(v1),
-                           bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), deriv_tc, invma),
-                           bld.vop2(aco_opcode::v_mul_f32, bld.def(v1), deriv_ma, tc));
-         *(i ? ddy : ddx) = bld.pseudo(aco_opcode::p_create_vector, bld.def(v2), x, y);
-      }
-
-      sc = bld.vop2(aco_opcode::v_add_f32, bld.def(v1), Operand::c32(0x3fc00000u /*1.5*/), sc);
-      tc = bld.vop2(aco_opcode::v_add_f32, bld.def(v1), Operand::c32(0x3fc00000u /*1.5*/), tc);
-   }
-
-   if (is_array) {
-      id = bld.vop2(madmk, bld.def(v1), coords[3], id, Operand::c32(0x41000000u /*8.0*/));
-      coords.erase(coords.begin() + 3);
-   }
-   coords[0] = sc;
-   coords[1] = tc;
-   coords[2] = id;
-}
-
 void
 get_const_vec(nir_ssa_def* vec, nir_const_value* cv[4])
 {
@@ -9444,23 +9331,7 @@ visit_tex(isel_context* ctx, nir_tex_instr* instr)
 
    unsigned wqm_coord_count = 0;
    std::vector<Temp> unpacked_coord;
-   if (ctx->options->gfx_level == GFX9 && instr->sampler_dim == GLSL_SAMPLER_DIM_1D &&
-       instr->coord_components) {
-      RegClass rc = a16 ? v2b : v1;
-      for (unsigned i = 0; i < coord.bytes() / rc.bytes(); i++)
-         unpacked_coord.emplace_back(emit_extract_vector(ctx, coord, i, rc));
-
-      assert(unpacked_coord.size() > 0 && unpacked_coord.size() < 3);
-
-      Operand coord2d;
-      /* 0.5 for floating point coords, 0 for integer. */
-      if (a16)
-         coord2d = instr->op == nir_texop_txf ? Operand::c16(0) : Operand::c16(0x3800);
-      else
-         coord2d = instr->op == nir_texop_txf ? Operand::c32(0) : Operand::c32(0x3f000000);
-      unpacked_coord.insert(std::next(unpacked_coord.begin()), bld.copy(bld.def(rc), coord2d));
-      wqm_coord_count = a16 ? DIV_ROUND_UP(unpacked_coord.size(), 2) : unpacked_coord.size();
-   } else if (coord != Temp()) {
+   if (coord != Temp()) {
       unpacked_coord.push_back(coord);
       wqm_coord_count = DIV_ROUND_UP(coord.bytes(), 4);
    }
@@ -9474,25 +9345,14 @@ visit_tex(isel_context* ctx, nir_tex_instr* instr)
 
    coords = emit_pack_v1(ctx, unpacked_coord);
 
-   assert(instr->sampler_dim != GLSL_SAMPLER_DIM_CUBE || !a16);
-   if (instr->sampler_dim == GLSL_SAMPLER_DIM_CUBE && instr->coord_components)
-      prepare_cube_coords(ctx, coords, &ddx, &ddy, instr->op == nir_texop_txd,
-                          instr->is_array && instr->op != nir_texop_lod);
-
    /* pack derivatives */
    if (has_ddx || has_ddy) {
-      RegClass rc = g16 ? v2b : v1;
       assert(a16 == g16 || ctx->options->gfx_level >= GFX10);
       std::array<Temp, 2> ddxddy = {ddx, ddy};
       for (Temp tmp : ddxddy) {
          if (tmp == Temp())
             continue;
          std::vector<Temp> unpacked = {tmp};
-         if (instr->sampler_dim == GLSL_SAMPLER_DIM_1D && ctx->options->gfx_level == GFX9) {
-            assert(has_ddx && has_ddy);
-            Temp zero = bld.copy(bld.def(rc), Operand::zero(rc.bytes()));
-            unpacked.push_back(zero);
-         }
          for (Temp derv : emit_pack_v1(ctx, unpacked))
             derivs.push_back(derv);
       }
diff --git a/src/amd/vulkan/radv_nir_to_llvm.c b/src/amd/vulkan/radv_nir_to_llvm.c
index 9565bbb693de..3e2e7a150097 100644
--- a/src/amd/vulkan/radv_nir_to_llvm.c
+++ b/src/amd/vulkan/radv_nir_to_llvm.c
@@ -716,7 +716,7 @@ ac_translate_nir_to_llvm(struct ac_llvm_compiler *ac_llvm,
    ctx.abi.robust_buffer_access = options->robust_buffer_access;
    ctx.abi.load_grid_size_from_user_sgpr = args->load_grid_size_from_user_sgpr;
    ctx.abi.conformant_trunc_coord = options->conformant_trunc_coord;
-   ctx.abi.lower_1d_cube_coords = true;
+   ctx.abi.lower_1d_cube_coords = false;
 
    bool is_ngg = is_pre_gs_stage(shaders[0]->info.stage) && info->is_ngg;
    if (shader_count >= 2 || is_ngg)
diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index 007e276252b6..86718183370f 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -533,6 +533,11 @@ radv_postprocess_nir(struct radv_device *device, const struct radv_pipeline_layo
    if (progress)
       nir_shader_gather_info(stage->nir, nir_shader_get_entrypoint(stage->nir));
 
+   NIR_PASS(_, stage->nir, ac_nir_lower_tex,
+            &(ac_nir_lower_tex_options){
+               .gfx_level = gfx_level,
+            });
+
    if (stage->nir->info.uses_resource_info_query)
       NIR_PASS(_, stage->nir, ac_nir_lower_resinfo, gfx_level);
 
-- 
GitLab


From fdccfe1346d5b662991f9db01e22c555e0986bd4 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Thu, 20 Apr 2023 17:33:46 +0100
Subject: [PATCH 06/13] ac/nir: add fix_derivs_in_divergent_cf

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/common/ac_nir.c                    | 314 ++++++++++++++++++++-
 src/amd/common/ac_nir.h                    |   8 +
 src/compiler/nir/nir_divergence_analysis.c |   1 +
 src/compiler/nir/nir_intrinsics.py         |   5 +
 4 files changed, 327 insertions(+), 1 deletion(-)

diff --git a/src/amd/common/ac_nir.c b/src/amd/common/ac_nir.c
index e2cf8088d9bc..ef8e1a676b7f 100644
--- a/src/amd/common/ac_nir.c
+++ b/src/amd/common/ac_nir.c
@@ -1276,9 +1276,321 @@ lower_tex(nir_builder *b, nir_instr *instr, void *options_)
    return false;
 }
 
+typedef struct {
+   nir_intrinsic_instr *bary;
+   nir_intrinsic_instr *load;
+} coord_info;
+
+static bool
+can_move_coord(nir_ssa_scalar scalar, coord_info *info)
+{
+   if (scalar.def->bit_size != 32)
+      return false;
+
+   if (nir_ssa_scalar_is_const(scalar))
+      return true;
+
+   if (scalar.def->parent_instr->type != nir_instr_type_intrinsic)
+      return false;
+
+   nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(scalar.def->parent_instr);
+   if (intrin->intrinsic == nir_intrinsic_load_input) {
+      info->bary = NULL;
+      info->load = intrin;
+      return true;
+   }
+
+   if (intrin->intrinsic != nir_intrinsic_load_interpolated_input)
+      return false;
+
+   nir_ssa_scalar coord_x = nir_ssa_scalar_resolved(intrin->src[0].ssa, 0);
+   nir_ssa_scalar coord_y = nir_ssa_scalar_resolved(intrin->src[0].ssa, 1);
+   if (coord_x.def->parent_instr->type != nir_instr_type_intrinsic || coord_x.comp != 0 ||
+       coord_y.def->parent_instr->type != nir_instr_type_intrinsic || coord_y.comp != 1)
+      return false;
+
+   nir_intrinsic_instr *intrin_x = nir_instr_as_intrinsic(coord_x.def->parent_instr);
+   nir_intrinsic_instr *intrin_y = nir_instr_as_intrinsic(coord_y.def->parent_instr);
+   if (intrin_x->intrinsic != intrin_y->intrinsic ||
+       (intrin_x->intrinsic != nir_intrinsic_load_barycentric_sample &&
+        intrin_x->intrinsic != nir_intrinsic_load_barycentric_pixel &&
+        intrin_x->intrinsic != nir_intrinsic_load_barycentric_centroid) ||
+       nir_intrinsic_interp_mode(intrin_x) != nir_intrinsic_interp_mode(intrin_y))
+      return false;
+
+   info->bary = intrin_x;
+   info->load = intrin;
+
+   return true;
+}
+
+struct move_tex_coords_state {
+   const ac_nir_lower_tex_options *options;
+   unsigned num_wqm_vgprs;
+   nir_builder toplevel_b;
+};
+
+static nir_ssa_def *
+build_coordinate(struct move_tex_coords_state *state, nir_ssa_scalar scalar, coord_info info)
+{
+   nir_builder *b = &state->toplevel_b;
+
+   if (nir_ssa_scalar_is_const(scalar))
+      return nir_imm_intN_t(b, nir_ssa_scalar_as_uint(scalar), scalar.def->bit_size);
+
+   ASSERTED nir_src offset = *nir_get_io_offset_src(info.load);
+   assert(nir_src_is_const(offset) && !nir_src_as_uint(offset));
+
+   nir_ssa_def *zero = nir_imm_int(b, 0);
+   nir_ssa_def *res;
+   if (info.bary) {
+      enum glsl_interp_mode interp_mode = nir_intrinsic_interp_mode(info.bary);
+      nir_ssa_def *bary = nir_load_system_value(b, info.bary->intrinsic, interp_mode, 2, 32);
+      res = nir_load_interpolated_input(b, 1, 32, bary, zero);
+   } else {
+      res = nir_load_input(b, 1, 32, zero);
+   }
+   nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(res->parent_instr);
+   nir_intrinsic_set_base(intrin, nir_intrinsic_base(info.load));
+   nir_intrinsic_set_component(intrin, nir_intrinsic_component(info.load) + scalar.comp);
+   nir_intrinsic_set_dest_type(intrin, nir_intrinsic_dest_type(info.load));
+   nir_intrinsic_set_io_semantics(intrin, nir_intrinsic_io_semantics(info.load));
+   return res;
+}
+
+static bool
+move_tex_coords(struct move_tex_coords_state *state, nir_function_impl *impl, nir_instr *instr)
+{
+   nir_tex_instr *tex = nir_instr_as_tex(instr);
+   if (tex->op != nir_texop_tex && tex->op != nir_texop_txb)
+      return false;
+
+   switch (tex->sampler_dim) {
+   case GLSL_SAMPLER_DIM_1D:
+   case GLSL_SAMPLER_DIM_2D:
+   case GLSL_SAMPLER_DIM_3D:
+   case GLSL_SAMPLER_DIM_CUBE:
+   case GLSL_SAMPLER_DIM_EXTERNAL:
+   case GLSL_SAMPLER_DIM_SUBPASS:
+      break;
+   case GLSL_SAMPLER_DIM_RECT:
+   case GLSL_SAMPLER_DIM_BUF:
+   case GLSL_SAMPLER_DIM_MS:
+   case GLSL_SAMPLER_DIM_SUBPASS_MS:
+      return false; /* No LOD. */
+   }
+
+   unsigned num_deriv_coords = tex->coord_components - tex->is_array;
+
+   nir_tex_src *src = &tex->src[nir_tex_instr_src_index(tex, nir_tex_src_coord)];
+   nir_ssa_scalar components[NIR_MAX_VEC_COMPONENTS];
+   coord_info infos[NIR_MAX_VEC_COMPONENTS];
+   bool can_move_all = true;
+   for (unsigned i = 0; i < tex->coord_components; i++) {
+      components[i] = nir_ssa_scalar_resolved(src->src.ssa, i);
+      can_move_all &= i >= num_deriv_coords || can_move_coord(components[i], &infos[i]);
+   }
+   if (!can_move_all)
+      return false;
+
+   bool cube_array = tex->sampler_dim == GLSL_SAMPLER_DIM_CUBE && tex->is_array;
+
+   int coord_base = 0;
+   unsigned linear_vgpr_size = num_deriv_coords;
+   if (tex->sampler_dim == GLSL_SAMPLER_DIM_1D && state->options->gfx_level == GFX9)
+      linear_vgpr_size++;
+   linear_vgpr_size += tex->is_array && !cube_array; /* cube array layer and face are combined */
+   for (unsigned i = 0; i < tex->num_srcs; i++) {
+      switch (tex->src[i].src_type) {
+      case nir_tex_src_offset:
+      case nir_tex_src_bias:
+      case nir_tex_src_comparator:
+         coord_base++;
+         FALLTHROUGH;
+      case nir_tex_src_ms_index:
+      case nir_tex_src_min_lod:
+         linear_vgpr_size++;
+         break;
+      default:
+         break;
+      }
+   }
+
+   if (state->num_wqm_vgprs + linear_vgpr_size + cube_array > state->options->max_wqm_vgprs)
+      return false;
+
+   for (unsigned i = 0; i < num_deriv_coords; i++)
+      components[i] = nir_get_ssa_scalar(build_coordinate(state, components[i], infos[i]), 0);
+
+   nir_ssa_def *linear_vgpr = nir_vec_scalars(&state->toplevel_b, components, num_deriv_coords);
+   lower_tex_coords(&state->toplevel_b, tex, &linear_vgpr, state->options);
+
+   /* Cube-array samples have the cube face in it's own linear vgpr, to be copied in later. */
+   nir_ssa_def *cube_face = NULL;
+   if (cube_array) {
+      cube_face = nir_strict_wqm_coord_amd(&state->toplevel_b, 1,
+                                           nir_channel(&state->toplevel_b, linear_vgpr, 2));
+      linear_vgpr = nir_channels(&state->toplevel_b, linear_vgpr, 0x3);
+   }
+
+   linear_vgpr = nir_strict_wqm_coord_amd(&state->toplevel_b, linear_vgpr_size, linear_vgpr,
+                                          .base = coord_base * 4);
+
+   if (tex->is_array) {
+      nir_builder b;
+      nir_builder_init(&b, impl);
+      b.cursor = nir_before_instr(instr);
+      nir_instr_rewrite_src_ssa(instr, &src->src,
+                                nir_vec_scalars(&b, &components[tex->coord_components - 1], 1));
+   } else {
+      nir_tex_instr_remove_src(tex, nir_tex_instr_src_index(tex, nir_tex_src_coord));
+   }
+
+   nir_tex_instr_add_src(tex, nir_tex_src_backend1, nir_src_for_ssa(linear_vgpr));
+   if (cube_array)
+      nir_tex_instr_add_src(tex, nir_tex_src_backend2, nir_src_for_ssa(cube_face));
+
+   tex->coord_components = tex->is_array ? 1 : 0;
+
+   state->num_wqm_vgprs += linear_vgpr_size + cube_array;
+
+   return true;
+}
+
+static bool
+move_fddxy(struct move_tex_coords_state *state, nir_function_impl *impl, nir_alu_instr *instr)
+{
+   switch (instr->op) {
+   case nir_op_fddx:
+   case nir_op_fddy:
+   case nir_op_fddx_fine:
+   case nir_op_fddy_fine:
+   case nir_op_fddx_coarse:
+   case nir_op_fddy_coarse:
+      break;
+   default:
+      return false;
+   }
+
+   unsigned num_components = instr->dest.dest.ssa.num_components;
+   nir_ssa_scalar components[NIR_MAX_VEC_COMPONENTS];
+   coord_info infos[NIR_MAX_VEC_COMPONENTS];
+   bool can_move_all = true;
+   for (unsigned i = 0; i < num_components; i++) {
+      components[i] = nir_ssa_scalar_chase_alu_src(nir_get_ssa_scalar(&instr->dest.dest.ssa, i), 0);
+      components[i] = nir_ssa_scalar_chase_movs(components[i]);
+      can_move_all &= can_move_coord(components[i], &infos[i]);
+   }
+   if (!can_move_all || state->num_wqm_vgprs + num_components > state->options->max_wqm_vgprs)
+      return false;
+
+   for (unsigned i = 0; i < num_components; i++) {
+      nir_ssa_def *def = build_coordinate(state, components[i], infos[i]);
+      components[i] = nir_get_ssa_scalar(def, 0);
+   }
+
+   nir_ssa_def *def = nir_vec_scalars(&state->toplevel_b, components, num_components);
+   def = nir_build_alu1(&state->toplevel_b, instr->op, def);
+   nir_ssa_def_rewrite_uses(&instr->dest.dest.ssa, def);
+
+   state->num_wqm_vgprs += num_components;
+
+   return true;
+}
+
+static bool
+move_coords_from_divergent_cf(struct move_tex_coords_state *state, nir_function_impl *impl,
+                              struct exec_list *cf_list, bool *divergent_discard, bool divergent_cf)
+{
+   bool progress = false;
+   foreach_list_typed (nir_cf_node, cf_node, node, cf_list) {
+      switch (cf_node->type) {
+      case nir_cf_node_block: {
+         nir_block *block = nir_cf_node_as_block(cf_node);
+
+         bool top_level = cf_list == &impl->body;
+
+         nir_foreach_instr (instr, block) {
+            if (top_level && !*divergent_discard)
+               state->toplevel_b.cursor = nir_before_instr(instr);
+
+            if (instr->type == nir_instr_type_tex && (divergent_cf || *divergent_discard)) {
+               progress |= move_tex_coords(state, impl, instr);
+            } else if (instr->type == nir_instr_type_alu && (divergent_cf || *divergent_discard)) {
+               progress |= move_fddxy(state, impl, nir_instr_as_alu(instr));
+            } else if (instr->type == nir_instr_type_intrinsic) {
+               nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
+               switch (intrin->intrinsic) {
+               case nir_intrinsic_discard:
+               case nir_intrinsic_demote:
+               case nir_intrinsic_terminate:
+                  if (divergent_cf)
+                     *divergent_discard = true;
+                  break;
+               case nir_intrinsic_discard_if:
+               case nir_intrinsic_demote_if:
+               case nir_intrinsic_terminate_if:
+                  if (divergent_cf || nir_src_is_divergent(intrin->src[0]))
+                     *divergent_discard = true;
+                  break;
+               default:
+                  break;
+               }
+            }
+         }
+
+         if (top_level && !*divergent_discard)
+            state->toplevel_b.cursor = nir_after_block_before_jump(block);
+         break;
+      }
+      case nir_cf_node_if: {
+         nir_if *nif = nir_cf_node_as_if(cf_node);
+         bool divergent_discard_then = *divergent_discard;
+         bool divergent_discard_else = *divergent_discard;
+         bool then_else_divergent = divergent_cf || nir_src_is_divergent(nif->condition);
+         progress |= move_coords_from_divergent_cf(state, impl, &nif->then_list,
+                                                   &divergent_discard_then, then_else_divergent);
+         progress |= move_coords_from_divergent_cf(state, impl, &nif->else_list,
+                                                   &divergent_discard_else, then_else_divergent);
+         *divergent_discard |= divergent_discard_then || divergent_discard_else;
+         break;
+      }
+      case nir_cf_node_loop: {
+         nir_loop *loop = nir_cf_node_as_loop(cf_node);
+         assert(!nir_loop_has_continue_construct(loop));
+         progress |=
+            move_coords_from_divergent_cf(state, impl, &loop->body, divergent_discard, true);
+         break;
+      }
+      case nir_cf_node_function:
+         unreachable("Invalid cf type");
+      }
+   }
+
+   return progress;
+}
+
 bool
 ac_nir_lower_tex(nir_shader *nir, const ac_nir_lower_tex_options *options)
 {
-   return nir_shader_instructions_pass(
+   bool progress = false;
+   if (options->fix_derivs_in_divergent_cf) {
+      nir_function_impl *impl = nir_shader_get_entrypoint(nir);
+
+      struct move_tex_coords_state state;
+      nir_builder_init(&state.toplevel_b, impl);
+      state.options = options;
+      state.num_wqm_vgprs = 0;
+
+      bool divergent_discard = false;
+      if (move_coords_from_divergent_cf(&state, impl, &impl->body, &divergent_discard, false))
+         nir_metadata_preserve(impl, nir_metadata_block_index | nir_metadata_dominance);
+      else
+         nir_metadata_preserve(impl, nir_metadata_all);
+   }
+
+   progress |= nir_shader_instructions_pass(
       nir, lower_tex, nir_metadata_block_index | nir_metadata_dominance, (void *)options);
+
+   return progress;
 }
diff --git a/src/amd/common/ac_nir.h b/src/amd/common/ac_nir.h
index 4c3b06335439..e444eee6ee71 100644
--- a/src/amd/common/ac_nir.h
+++ b/src/amd/common/ac_nir.h
@@ -327,6 +327,14 @@ ac_nir_lower_ps(nir_shader *nir, const ac_nir_lower_ps_options *options);
 
 typedef struct {
    enum amd_gfx_level gfx_level;
+
+   /* Fix derivatives of constants and FS inputs in control flow.
+    *
+    * Ignores interpolateAtSample()/interpolateAtOffset(), dynamically indexed input loads,
+    * pervertexEXT input loads, textureGather() with implicit LOD and 16-bit derivatives.
+    */
+   bool fix_derivs_in_divergent_cf;
+   unsigned max_wqm_vgprs;
 } ac_nir_lower_tex_options;
 
 bool
diff --git a/src/compiler/nir/nir_divergence_analysis.c b/src/compiler/nir/nir_divergence_analysis.c
index be833874c088..1fdbf80d003a 100644
--- a/src/compiler/nir/nir_divergence_analysis.c
+++ b/src/compiler/nir/nir_divergence_analysis.c
@@ -418,6 +418,7 @@ visit_intrinsic(nir_shader *shader, nir_intrinsic_instr *instr)
    case nir_intrinsic_image_descriptor_amd:
    case nir_intrinsic_image_deref_descriptor_amd:
    case nir_intrinsic_bindless_image_descriptor_amd:
+   case nir_intrinsic_strict_wqm_coord_amd:
    case nir_intrinsic_copy_deref:
    case nir_intrinsic_vulkan_resource_index:
    case nir_intrinsic_vulkan_resource_reindex:
diff --git a/src/compiler/nir/nir_intrinsics.py b/src/compiler/nir/nir_intrinsics.py
index 9cb01252e461..0d46bb0fb95e 100644
--- a/src/compiler/nir/nir_intrinsics.py
+++ b/src/compiler/nir/nir_intrinsics.py
@@ -1605,6 +1605,11 @@ system_value("alpha_reference_amd", 1)
 # Whether to enable barycentric optimization
 system_value("barycentric_optimize_amd", dest_comp=1, bit_sizes=[1])
 
+# Copy the input into a register which will remain valid for entire quads, even in control flow.
+# This should only be used directly for texture sources.
+intrinsic("strict_wqm_coord_amd", src_comp=[-1], dest_comp=0, bit_sizes=[32], indices=[BASE],
+          flags=[CAN_ELIMINATE])
+
 # V3D-specific instrinc for tile buffer color reads.
 #
 # The hardware requires that we read the samples and components of a pixel
-- 
GitLab


From 0855104644a01dd07452e72032cc3dfe10d174da Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Tue, 21 Mar 2023 14:47:45 +0000
Subject: [PATCH 07/13] aco: remove unused RegType

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_ir.h | 2 --
 1 file changed, 2 deletions(-)

diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index db2158e4ae82..08a47deb7da7 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -304,10 +304,8 @@ withoutDPP(Format format)
 }
 
 enum class RegType {
-   none = 0,
    sgpr,
    vgpr,
-   linear_vgpr,
 };
 
 struct RegClass {
-- 
GitLab


From 0a4533afad3f3b8288233fb99a5c6e1936bf4fb3 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Fri, 17 Mar 2023 16:44:25 +0000
Subject: [PATCH 08/13] aco: let p_start_linear_vgpr take an operand

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_lower_to_hw_instr.cpp   | 13 +++++++++++++
 src/amd/compiler/aco_register_allocation.cpp |  7 +++++--
 2 files changed, 18 insertions(+), 2 deletions(-)

diff --git a/src/amd/compiler/aco_lower_to_hw_instr.cpp b/src/amd/compiler/aco_lower_to_hw_instr.cpp
index 19e8ec706099..97d3a8979b7b 100644
--- a/src/amd/compiler/aco_lower_to_hw_instr.cpp
+++ b/src/amd/compiler/aco_lower_to_hw_instr.cpp
@@ -2233,6 +2233,19 @@ lower_to_hw_instr(Program* program)
                handle_operands(copy_operations, &ctx, program->gfx_level, pi);
                break;
             }
+            case aco_opcode::p_start_linear_vgpr: {
+               if (instr->operands.empty())
+                  break;
+
+               Definition def(instr->definitions[0].physReg(),
+                              RegClass::get(RegType::vgpr, instr->definitions[0].bytes()));
+
+               std::map<PhysReg, copy_operation> copy_operations;
+               copy_operations[def.physReg()] = {instr->operands[0], def,
+                                                 instr->operands[0].bytes()};
+               handle_operands(copy_operations, &ctx, program->gfx_level, pi);
+               break;
+            }
             case aco_opcode::p_exit_early_if: {
                /* don't bother with an early exit near the end of the program */
                if ((block->instructions.size() - 1 - instr_idx) <= 4 &&
diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index ca9b87024199..d0ffc8c73289 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -1884,7 +1884,8 @@ handle_pseudo(ra_ctx& ctx, const RegisterFile& reg_file, Instruction* instr)
    case aco_opcode::p_create_vector:
    case aco_opcode::p_split_vector:
    case aco_opcode::p_parallelcopy:
-   case aco_opcode::p_wqm: break;
+   case aco_opcode::p_wqm:
+   case aco_opcode::p_start_linear_vgpr: break;
    default: return;
    }
 
@@ -2942,7 +2943,9 @@ register_allocation(Program* program, std::vector<IDSet>& live_out_per_block, ra
                      definition->setFixed(reg);
                }
             } else if (instr->opcode == aco_opcode::p_wqm ||
-                       instr->opcode == aco_opcode::p_parallelcopy) {
+                       instr->opcode == aco_opcode::p_parallelcopy ||
+                       (instr->opcode == aco_opcode::p_start_linear_vgpr &&
+                        !instr->operands.empty())) {
                PhysReg reg = instr->operands[i].physReg();
                if (instr->operands[i].isTemp() &&
                    instr->operands[i].getTemp().type() == definition->getTemp().type() &&
-- 
GitLab


From dbe941f6595409dc08e1e9775956289af8471819 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Tue, 21 Mar 2023 14:44:09 +0000
Subject: [PATCH 09/13] aco: add MIMG_instruction::strict_wqm

This lets us use linear VGPRs for part of the texture sample's address.

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_ir.h                    |  3 +-
 src/amd/compiler/aco_lower_to_hw_instr.cpp   | 52 +++++++++++++++++++
 src/amd/compiler/aco_register_allocation.cpp |  3 +-
 src/amd/compiler/aco_validate.cpp            | 54 +++++++++++++++-----
 4 files changed, 96 insertions(+), 16 deletions(-)

diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index 08a47deb7da7..8cfca3f1f3de 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -1611,7 +1611,8 @@ struct MIMG_instruction : public Instruction {
    bool a16 : 1;         /* VEGA, NAVI: Address components are 16-bits */
    bool d16 : 1;         /* Convert 32-bit data to 16-bit data */
    bool disable_wqm : 1; /* Require an exec mask without helper invocations */
-   uint8_t padding0 : 2;
+   bool strict_wqm : 1;  /* VADDR is a linear VGPR and additional VGPRs may be copied into it */
+   uint8_t padding0 : 1;
    uint8_t padding1;
    uint8_t padding2;
 };
diff --git a/src/amd/compiler/aco_lower_to_hw_instr.cpp b/src/amd/compiler/aco_lower_to_hw_instr.cpp
index 97d3a8979b7b..69d43d28eed6 100644
--- a/src/amd/compiler/aco_lower_to_hw_instr.cpp
+++ b/src/amd/compiler/aco_lower_to_hw_instr.cpp
@@ -2138,6 +2138,55 @@ hw_init_scratch(Builder& bld, Definition def, Operand scratch_addr, Operand scra
    }
 }
 
+void
+lower_image_sample(lower_context* ctx, MIMG_instruction* instr)
+{
+   Operand linear_vgpr = instr->operands[3];
+   Operand wqm_cube_face = instr->operands[4];
+   bool has_wqm_cube_face = !instr->operands[4].isUndefined();
+   Operand layer;
+   PhysReg layer_reg;
+
+   PhysReg reg = linear_vgpr.physReg();
+   std::map<PhysReg, copy_operation> copy_operations;
+   for (unsigned i = 5; i < instr->operands.size(); i++) {
+      Operand arg = instr->operands[i];
+      if (i > 5 && instr->operands[i - 1].isUndefined() && has_wqm_cube_face) {
+         layer = arg;
+         layer_reg = reg;
+      } else if (!arg.isUndefined()) {
+         Definition def(reg, RegClass::get(RegType::vgpr, arg.bytes()));
+         copy_operations[def.physReg()] = {arg, def, def.bytes()};
+      }
+      reg = reg.advance(arg.bytes());
+   }
+
+   Pseudo_instruction pi = {};
+   handle_operands(copy_operations, ctx, ctx->program->gfx_level, &pi);
+
+   if (has_wqm_cube_face) {
+      Builder bld(ctx->program, &ctx->instructions);
+
+      assert(!layer.isUndefined());
+      assert(instr->definitions.size() >= 2 && instr->definitions[1].physReg() == scc);
+
+      aco_opcode madmk =
+         ctx->program->gfx_level >= GFX10_3 ? aco_opcode::v_fmamk_f32 : aco_opcode::v_madmk_f32;
+      bld.vop2(madmk, Definition(layer_reg, v1), layer, wqm_cube_face, Operand::c32(0x41000000u));
+
+      bld.sop1(Builder::s_not, Definition(exec, bld.lm), Definition(scc, s1),
+               Operand(exec, bld.lm));
+      bld.vop1(aco_opcode::v_mov_b32, Definition(layer_reg, v1), wqm_cube_face);
+      bld.sop1(Builder::s_not, Definition(exec, bld.lm), Definition(scc, s1),
+               Operand(exec, bld.lm));
+   }
+
+   instr->strict_wqm = false;
+
+   while (instr->operands.size() > 4)
+      instr->operands.pop_back();
+}
+
 void
 lower_to_hw_instr(Program* program)
 {
@@ -2795,6 +2844,9 @@ lower_to_hw_instr(Program* program)
             ctx.instructions.emplace_back(std::move(instr));
 
             emit_set_mode(bld, block->fp_mode, set_round, false);
+         } else if (instr->isMIMG() && instr->mimg().strict_wqm) {
+            lower_image_sample(&ctx, &instr->mimg());
+            ctx.instructions.emplace_back(std::move(instr));
          } else {
             ctx.instructions.emplace_back(std::move(instr));
          }
diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index d0ffc8c73289..ecfb01562b77 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -2439,7 +2439,8 @@ get_affinities(ra_ctx& ctx, std::vector<IDSet>& live_out_per_block)
                    op.getTemp().type() == instr->definitions[0].getTemp().type())
                   ctx.vectors[op.tempId()] = instr.get();
             }
-         } else if (instr->format == Format::MIMG && instr->operands.size() > 4) {
+         } else if (instr->format == Format::MIMG && instr->operands.size() > 4 &&
+                    !instr->mimg().strict_wqm) {
             for (unsigned i = 3; i < instr->operands.size(); i++)
                ctx.vectors[instr->operands[i].tempId()] = instr.get();
          } else if (instr->opcode == aco_opcode::p_split_vector &&
diff --git a/src/amd/compiler/aco_validate.cpp b/src/amd/compiler/aco_validate.cpp
index abf0a5925910..83e4f7ce188c 100644
--- a/src/amd/compiler/aco_validate.cpp
+++ b/src/amd/compiler/aco_validate.cpp
@@ -268,6 +268,7 @@ validate_ir(Program* program)
                                    instr->opcode == aco_opcode::p_dual_src_export_gfx11 ||
                                    (instr->opcode == aco_opcode::p_interp_gfx11 && i == 0) ||
                                    (instr->opcode == aco_opcode::p_bpermute_gfx11w64 && i == 0) ||
+                                   (instr->isMIMG() && instr->mimg().strict_wqm && i >= 4) ||
                                    (flat && i == 1) || (instr->isMIMG() && (i == 1 || i == 2)) ||
                                    ((instr->isMUBUF() || instr->isMTBUF()) && i == 1) ||
                                    (instr->isScratch() && i == 0);
@@ -677,21 +678,46 @@ validate_ir(Program* program)
                      "TFE/LWE loads",
                      instr.get());
             }
-            check(instr->operands.size() == 4 || program->gfx_level >= GFX10,
-                  "NSA is only supported on GFX10+", instr.get());
-            for (unsigned i = 3; i < instr->operands.size(); i++) {
-               check(instr->operands[i].hasRegClass() &&
-                        instr->operands[i].regClass().type() == RegType::vgpr,
-                     "MIMG operands[3+] (VADDR) must be VGPR", instr.get());
-               if (instr->operands.size() > 4) {
-                  if (program->gfx_level < GFX11) {
-                     check(instr->operands[i].regClass() == v1,
-                           "GFX10 MIMG VADDR must be v1 if NSA is used", instr.get());
-                  } else {
-                     if (instr->opcode != aco_opcode::image_bvh_intersect_ray &&
-                         instr->opcode != aco_opcode::image_bvh64_intersect_ray && i < 7) {
+
+            if (instr->mimg().strict_wqm) {
+               check(instr->operands[3].isTemp() && instr->operands[3].regClass().is_linear_vgpr(),
+                     "MIMG operands[3] must be temp linear VGPR.", instr.get());
+               check(instr->operands[4].hasRegClass() &&
+                        instr->operands[4].regClass() == v1.as_linear(),
+                     "MIMG operands[4] must be temp/undefined v1 linear VGPR.", instr.get());
+
+               if (!instr->operands[4].isUndefined()) {
+                  check(instr->definitions.size() >= 2 && instr->definitions[1].isFixed() &&
+                           instr->definitions[1].physReg() == scc,
+                        "Cubemap array MIMG needs an SCC definition", instr.get());
+               }
+
+               unsigned total_size = 0;
+               for (unsigned i = 5; i < instr->operands.size(); i++) {
+                  check(instr->operands[i].isUndefined() ||
+                           (instr->operands[i].isTemp() && instr->operands[i].regClass() == v1),
+                        "MIMG operands[5+] (VADDR) must be v1 or undef", instr.get());
+                  total_size += instr->operands[i].bytes();
+               }
+               check(total_size <= instr->operands[3].bytes(),
+                     "MIMG operands[5+] must fit within operands[3].", instr.get());
+            } else {
+               check(instr->operands.size() == 4 || program->gfx_level >= GFX10,
+                     "NSA is only supported on GFX10+", instr.get());
+               for (unsigned i = 3; i < instr->operands.size(); i++) {
+                  check(instr->operands[i].hasRegClass() &&
+                           instr->operands[i].regClass().type() == RegType::vgpr,
+                        "MIMG operands[3+] (VADDR) must be VGPR", instr.get());
+                  if (instr->operands.size() > 4) {
+                     if (program->gfx_level < GFX11) {
                         check(instr->operands[i].regClass() == v1,
-                              "first 4 GFX11 MIMG VADDR must be v1 if NSA is used", instr.get());
+                              "GFX10 MIMG VADDR must be v1 if NSA is used", instr.get());
+                     } else {
+                        if (instr->opcode != aco_opcode::image_bvh_intersect_ray &&
+                            instr->opcode != aco_opcode::image_bvh64_intersect_ray && i < 7) {
+                           check(instr->operands[i].regClass() == v1,
+                                 "first 4 GFX11 MIMG VADDR must be v1 if NSA is used", instr.get());
+                        }
                      }
                   }
                }
-- 
GitLab


From 4fcef12f37717bdb65c5b186613673093bea432f Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Fri, 14 Apr 2023 15:44:43 +0100
Subject: [PATCH 10/13] aco: implement strict_wqm_coord_amd

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 .../compiler/aco_instruction_selection.cpp    | 30 +++++++++++++++++++
 .../aco_instruction_selection_setup.cpp       |  7 ++++-
 2 files changed, 36 insertions(+), 1 deletion(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index ea9f84136c8b..fdb3c394a77c 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -9131,6 +9131,36 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
       ctx->block->instructions.emplace_back(std::move(exp));
       break;
    }
+   case nir_intrinsic_strict_wqm_coord_amd: {
+      Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
+      Temp src = get_ssa_temp(ctx, instr->src[0].ssa);
+      Temp tmp = bld.tmp(RegClass::get(RegType::vgpr, dst.bytes()));
+      unsigned begin_size = nir_intrinsic_base(instr);
+      unsigned end_size = dst.bytes() - begin_size - src.bytes();
+
+      unsigned num_src = 1;
+      auto it = ctx->allocated_vec.find(src.id());
+      if (it != ctx->allocated_vec.end())
+         num_src = src.bytes() / it->second[0].bytes();
+
+      aco_ptr<Pseudo_instruction> vec{create_instruction<Pseudo_instruction>(
+         aco_opcode::p_create_vector, Format::PSEUDO, num_src + !!begin_size + !!end_size, 1)};
+
+      if (begin_size)
+         vec->operands[0] = Operand(RegClass::get(RegType::vgpr, begin_size));
+      if (end_size)
+         vec->operands.back() = Operand(RegClass::get(RegType::vgpr, end_size));
+      for (unsigned i = 0; i < num_src; i++) {
+         Temp comp = it != ctx->allocated_vec.end() ? it->second[i] : src;
+         vec->operands[i + !!begin_size] = Operand(comp);
+      }
+
+      vec->definitions[0] = Definition(tmp);
+      ctx->block->instructions.emplace_back(std::move(vec));
+
+      bld.pseudo(aco_opcode::p_start_linear_vgpr, Definition(dst), tmp);
+      break;
+   }
    default:
       isel_err(&instr->instr, "Unimplemented intrinsic instr");
       abort();
diff --git a/src/amd/compiler/aco_instruction_selection_setup.cpp b/src/amd/compiler/aco_instruction_selection_setup.cpp
index d77266f4f66f..2a91842246ab 100644
--- a/src/amd/compiler/aco_instruction_selection_setup.cpp
+++ b/src/amd/compiler/aco_instruction_selection_setup.cpp
@@ -465,6 +465,7 @@ init_context(isel_context* ctx, nir_shader* shader)
                if (!nir_intrinsic_infos[intrinsic->intrinsic].has_dest)
                   break;
                RegType type = RegType::sgpr;
+               bool linear = false;
                switch (intrinsic->intrinsic) {
                case nir_intrinsic_load_push_constant:
                case nir_intrinsic_load_workgroup_id:
@@ -607,6 +608,10 @@ init_context(isel_context* ctx, nir_shader* shader)
                case nir_intrinsic_load_view_index:
                   type = ctx->stage == fragment_fs ? RegType::vgpr : RegType::sgpr;
                   break;
+               case nir_intrinsic_strict_wqm_coord_amd:
+                  type = RegType::vgpr;
+                  linear = true;
+                  break;
                default:
                   for (unsigned i = 0; i < nir_intrinsic_infos[intrinsic->intrinsic].num_srcs;
                        i++) {
@@ -617,7 +622,7 @@ init_context(isel_context* ctx, nir_shader* shader)
                }
                RegClass rc = get_reg_class(ctx, type, intrinsic->dest.ssa.num_components,
                                            intrinsic->dest.ssa.bit_size);
-               regclasses[intrinsic->dest.ssa.index] = rc;
+               regclasses[intrinsic->dest.ssa.index] = linear ? rc.as_linear() : rc;
                break;
             }
             case nir_instr_type_tex: {
-- 
GitLab


From 5864759b3dd9779a91d0e04d2123c253ffa1db0a Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Fri, 14 Apr 2023 17:49:46 +0100
Subject: [PATCH 11/13] aco: implement texture samples with strict WQM
 coordinates

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 .../compiler/aco_instruction_selection.cpp    | 65 +++++++++++++++++--
 src/amd/compiler/aco_instruction_selection.h  |  1 +
 2 files changed, 61 insertions(+), 5 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index fdb3c394a77c..398e32679e83 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -5940,7 +5940,14 @@ emit_mimg(Builder& bld, aco_opcode op, Definition dst, Temp rsrc, Operand samp,
                                                          : 0;
    nsa_size = bld.program->gfx_level >= GFX11 || coords.size() <= nsa_size ? nsa_size : 0;
 
+   bool strict_wqm = coords[0].regClass().is_linear_vgpr();
+   if (strict_wqm)
+      nsa_size = coords.size();
+
    for (unsigned i = 0; i < std::min(coords.size(), nsa_size); i++) {
+      if (!coords[i].id())
+         continue;
+
       coords[i] = as_vgpr(bld, coords[i]);
       if (wqm_mask & (1u << i))
          coords[i] = emit_wqm(bld, coords[i], bld.tmp(coords[i].regClass()), true);
@@ -5976,15 +5983,20 @@ emit_mimg(Builder& bld, aco_opcode op, Definition dst, Temp rsrc, Operand samp,
       coords.resize(nsa_size + 1);
    }
 
-   aco_ptr<MIMG_instruction> mimg{
-      create_instruction<MIMG_instruction>(op, Format::MIMG, 3 + coords.size(), dst.isTemp())};
+   bool scc_clobber = strict_wqm && coords[1].id();
+
+   aco_ptr<MIMG_instruction> mimg{create_instruction<MIMG_instruction>(
+      op, Format::MIMG, 3 + coords.size(), dst.isTemp() + scc_clobber)};
    if (dst.isTemp())
       mimg->definitions[0] = dst;
+   if (scc_clobber)
+      mimg->definitions.back() = bld.def(s1, scc);
    mimg->operands[0] = Operand(rsrc);
    mimg->operands[1] = samp;
    mimg->operands[2] = vdata;
    for (unsigned i = 0; i < coords.size(); i++)
       mimg->operands[3 + i] = Operand(coords[i]);
+   mimg->strict_wqm = strict_wqm;
 
    MIMG_instruction* res = mimg.get();
    bld.insert(std::move(mimg));
@@ -9192,10 +9204,11 @@ visit_tex(isel_context* ctx, nir_tex_instr* instr)
    Builder bld(ctx->program, ctx->block);
    bool has_bias = false, has_lod = false, level_zero = false, has_compare = false,
         has_offset = false, has_ddx = false, has_ddy = false, has_derivs = false,
-        has_sample_index = false, has_clamped_lod = false;
+        has_sample_index = false, has_clamped_lod = false, has_wqm_coord = false,
+        has_wqm_cube_face = false;
    Temp resource, sampler, bias = Temp(), compare = Temp(), sample_index = Temp(), lod = Temp(),
                            offset = Temp(), ddx = Temp(), ddy = Temp(), clamped_lod = Temp(),
-                           coord = Temp();
+                           coord = Temp(), wqm_coord = Temp(), wqm_cube_face = Temp();
    std::vector<Temp> coords;
    std::vector<Temp> derivs;
    nir_const_value* const_offset[4] = {NULL, NULL, NULL, NULL};
@@ -9234,6 +9247,18 @@ visit_tex(isel_context* ctx, nir_tex_instr* instr)
          coord = get_ssa_temp_tex(ctx, instr->src[i].src.ssa, a16);
          break;
       }
+      case nir_tex_src_backend1: {
+         assert(instr->src[i].src.ssa->bit_size == 32);
+         wqm_coord = get_ssa_temp(ctx, instr->src[i].src.ssa);
+         has_wqm_coord = true;
+         break;
+      }
+      case nir_tex_src_backend2: {
+         assert(instr->src[i].src.ssa->bit_size == 32);
+         wqm_cube_face = get_ssa_temp(ctx, instr->src[i].src.ssa);
+         has_wqm_cube_face = true;
+         break;
+      }
       case nir_tex_src_bias:
          assert(instr->src[i].src.ssa->bit_size == (a16 ? 16 : 32));
          /* Doesn't need get_ssa_temp_tex because we pack it into its own dword anyway. */
@@ -9289,6 +9314,13 @@ visit_tex(isel_context* ctx, nir_tex_instr* instr)
       }
    }
 
+   assert(!has_wqm_cube_face || (has_wqm_coord && wqm_cube_face.regClass().is_linear_vgpr()));
+   if (has_wqm_coord) {
+      assert(instr->op == nir_texop_tex || instr->op == nir_texop_txb);
+      assert(wqm_coord.regClass().is_linear_vgpr());
+      assert(!a16 && !g16);
+   }
+
    if (instr->op == nir_texop_tg4 && !has_lod && !instr->is_gather_implicit_lod)
       level_zero = true;
 
@@ -9562,6 +9594,16 @@ visit_tex(isel_context* ctx, nir_tex_instr* instr)
    /* gather MIMG address components */
    std::vector<Temp> args;
    unsigned wqm_mask = 0;
+   if (has_wqm_coord) {
+      args.emplace_back(wqm_coord);
+      args.emplace_back(has_wqm_cube_face ? wqm_cube_face : Temp(0, v1.as_linear()));
+
+      if (!(ctx->block->kind & block_kind_top_level)) {
+         ctx->unended_linear_vgprs.push_back(wqm_coord);
+         if (has_wqm_cube_face)
+            ctx->unended_linear_vgprs.push_back(wqm_cube_face);
+      }
+   }
    if (has_offset) {
       wqm_mask |= u_bit_consecutive(args.size(), 1);
       args.emplace_back(offset);
@@ -9573,7 +9615,13 @@ visit_tex(isel_context* ctx, nir_tex_instr* instr)
    if (has_derivs)
       args.insert(args.end(), derivs.begin(), derivs.end());
 
-   wqm_mask |= u_bit_consecutive(args.size(), wqm_coord_count);
+   if (has_wqm_coord) {
+      /* Add undef operand representing pre-initialized space in the strict WQM coord. */
+      unsigned copy_bytes = (args.size() - 2 + coords.size()) * 4;
+      args.emplace_back(Temp(0, RegClass::get(RegType::vgpr, wqm_coord.bytes() - copy_bytes)));
+   } else {
+      wqm_mask |= u_bit_consecutive(args.size(), wqm_coord_count);
+   }
    args.insert(args.end(), coords.begin(), coords.end());
 
    if (instr->op == nir_texop_txf || instr->op == nir_texop_fragment_fetch_amd ||
@@ -10141,6 +10189,13 @@ visit_jump(isel_context* ctx, nir_jump_instr* instr)
 void
 visit_block(isel_context* ctx, nir_block* block)
 {
+   if (ctx->block->kind & block_kind_top_level) {
+      Builder bld(ctx->program, ctx->block);
+      for (Temp tmp : ctx->unended_linear_vgprs)
+         bld.pseudo(aco_opcode::p_end_linear_vgpr, tmp);
+      ctx->unended_linear_vgprs.clear();
+   }
+
    ctx->block->instructions.reserve(ctx->block->instructions.size() +
                                     exec_list_length(&block->instr_list) * 2);
    nir_foreach_instr (instr, block) {
diff --git a/src/amd/compiler/aco_instruction_selection.h b/src/amd/compiler/aco_instruction_selection.h
index acb254111dda..53493abd5567 100644
--- a/src/amd/compiler/aco_instruction_selection.h
+++ b/src/amd/compiler/aco_instruction_selection.h
@@ -62,6 +62,7 @@ struct isel_context {
    Block* block;
    uint32_t first_temp_id;
    std::unordered_map<unsigned, std::array<Temp, NIR_MAX_VEC_COMPONENTS>> allocated_vec;
+   std::vector<Temp> unended_linear_vgprs;
    Stage stage;
    struct {
       bool has_branch;
-- 
GitLab


From f47563c52dcd1d5a2f2470ba2580c6eaad41412a Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Fri, 14 Apr 2023 17:50:04 +0100
Subject: [PATCH 12/13] radv: use fix_derivs_in_divergent_cf

fossil-db (navi21):
Totals from 5733 (4.23% of 135636) affected shaders:
MaxWaves: 125886 -> 124306 (-1.26%)
Instrs: 5062473 -> 5064119 (+0.03%); split: -0.14%, +0.17%
CodeSize: 27483324 -> 27460456 (-0.08%); split: -0.17%, +0.08%
VGPRs: 273552 -> 278240 (+1.71%); split: -0.03%, +1.75%
Latency: 64522875 -> 64531688 (+0.01%); split: -0.09%, +0.10%
InvThroughput: 10674953 -> 10706506 (+0.30%); split: -0.06%, +0.35%
VClause: 91443 -> 91589 (+0.16%); split: -0.10%, +0.26%
SClause: 213922 -> 213601 (-0.15%); split: -0.25%, +0.10%
Copies: 351438 -> 355223 (+1.08%); split: -1.40%, +2.47%
Branches: 134146 -> 134148 (+0.00%); split: -0.00%, +0.00%
PreSGPRs: 286970 -> 284855 (-0.74%); split: -0.74%, +0.00%
PreVGPRs: 239623 -> 243762 (+1.73%); split: -0.05%, +1.78%

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/vulkan/radv_pipeline.c | 10 ++++++++++
 1 file changed, 10 insertions(+)

diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index 86718183370f..37ca668b8d32 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -533,10 +533,20 @@ radv_postprocess_nir(struct radv_device *device, const struct radv_pipeline_layo
    if (progress)
       nir_shader_gather_info(stage->nir, nir_shader_get_entrypoint(stage->nir));
 
+   bool fix_derivs_in_divergent_cf =
+      stage->stage == MESA_SHADER_FRAGMENT && !radv_use_llvm_for_stage(device, stage->stage);
+   if (fix_derivs_in_divergent_cf) {
+      NIR_PASS(_, stage->nir, nir_convert_to_lcssa, true, true);
+      nir_divergence_analysis(stage->nir);
+   }
    NIR_PASS(_, stage->nir, ac_nir_lower_tex,
             &(ac_nir_lower_tex_options){
                .gfx_level = gfx_level,
+               .fix_derivs_in_divergent_cf = fix_derivs_in_divergent_cf,
+               .max_wqm_vgprs = 64, // TODO: improve spiller and RA support for linear VGPRs
             });
+   if (fix_derivs_in_divergent_cf)
+      NIR_PASS(_, stage->nir, nir_opt_remove_phis); /* cleanup LCSSA phis */
 
    if (stage->nir->info.uses_resource_info_query)
       NIR_PASS(_, stage->nir, ac_nir_lower_resinfo, gfx_level);
-- 
GitLab


From be1ab032f4722ad4f898851a63ed87f78a358dba Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Fri, 14 Apr 2023 17:50:15 +0100
Subject: [PATCH 13/13] aco/tests: add fix_derivs_in_divergent_cf tests

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/tests/meson.build           |   2 +
 src/amd/compiler/tests/test_d3d11_derivs.cpp | 475 +++++++++++++++++++
 2 files changed, 477 insertions(+)
 create mode 100644 src/amd/compiler/tests/test_d3d11_derivs.cpp

diff --git a/src/amd/compiler/tests/meson.build b/src/amd/compiler/tests/meson.build
index d807426a3a8e..6d06d8bf0b5c 100644
--- a/src/amd/compiler/tests/meson.build
+++ b/src/amd/compiler/tests/meson.build
@@ -24,6 +24,7 @@ aco_tests_files = files(
   'main.cpp',
   'test_assembler.cpp',
   'test_builder.cpp',
+  'test_d3d11_derivs.cpp',
   'test_hard_clause.cpp',
   'test_insert_nops.cpp',
   'test_insert_waitcnt.cpp',
@@ -39,6 +40,7 @@ aco_tests_files = files(
 
 spirv_files = files(
   'test_isel.cpp',
+  'test_d3d11_derivs.cpp',
 )
 
 gen_spirv = generator(prog_python,
diff --git a/src/amd/compiler/tests/test_d3d11_derivs.cpp b/src/amd/compiler/tests/test_d3d11_derivs.cpp
new file mode 100644
index 000000000000..7b3dc9b2a319
--- /dev/null
+++ b/src/amd/compiler/tests/test_d3d11_derivs.cpp
@@ -0,0 +1,475 @@
+/*
+ * Copyright Â© 2023 Valve Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ *
+ */
+#include "helpers.h"
+#include "test_d3d11_derivs-spirv.h"
+
+using namespace aco;
+
+BEGIN_TEST(d3d11_derivs.simple)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec2 out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform sampler2D tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = texture(tex, in_coord);
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX10_3));
+   pbld.add_vsfs(vs, fs);
+
+   //>> v1: %x = v_interp_p2_f32 %_, %_:m0, (kill)%_ attr0.x
+   //>> v1: %y = v_interp_p2_f32 (kill)%_, (kill)%_:m0, (kill)%_ attr0.y
+   //>> v2: %vec = p_create_vector (kill)%x, (kill)%y
+   //>> lv2: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> BB1
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm, lv1: undef, v2: undef 2d
+   //>> BB2
+   //>> BB6
+   //>> p_end_linear_vgpr (kill)%wqm
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+
+   //>> v_interp_p2_f32_e32 v#rx, v1, attr0.x                                          ; $_
+   //>> v_interp_p2_f32_e32 v#ry_tmp, v1, attr0.y                                      ; $_
+   //>> v_mov_b32_e32 v#ry, v#ry_tmp                                                   ; $_
+   //>> image_sample v[8:11], v[#rx:#ry], s[8:15], s[4:7] dmask:0xf dim:SQ_RSRC_IMG_2D ; $_ $_
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.constant)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in float in_coord;
+      layout(location = 0) out float out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in float in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform sampler2D tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = texture(tex, vec2(in_coord, -0.5));
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX10_3));
+   pbld.add_vsfs(vs, fs);
+
+   //>> v1: %x = v_interp_p2_f32 (kill)%_, (kill)%_:m0, (kill)%_ attr0.x
+   //>> v2: %vec = p_create_vector (kill)%x, -0.5
+   //>> lv2: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> BB1
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm, lv1: undef, v2: undef 2d
+   //>> BB2
+   //>> BB6
+   //>> p_end_linear_vgpr (kill)%wqm
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+
+   //>> v_interp_p2_f32_e32 v#rx, v1, attr0.x                                         ; $_
+   //>> v_mov_b32_e32 v#ry, -0.5                                                      ; $_
+   //>> image_sample v[4:7], v[#rx:#ry], s[8:15], s[4:7] dmask:0xf dim:SQ_RSRC_IMG_2D ; $_ $_
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.discard)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec2 out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform sampler2D tex;
+      void main() {
+         if (gl_FragCoord.y > 1.0)
+            discard;
+         out_color = texture(tex, in_coord);
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX10_3));
+   pbld.add_vsfs(vs, fs);
+
+   /* The interpolation must be done before the discard_if. */
+   //>> lv2: %wqm = p_start_linear_vgpr (kill)%_
+   //>> s2: %_:exec, s1: %_:scc = s_andn2_b64 %_:exec, (kill)%_
+   //>> p_exit_early_if (kill)%_:scc
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, (kill)%wqm, lv1: undef, v2: undef 2d
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.bias)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec2 out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform sampler2D tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = texture(tex, in_coord, gl_FragCoord.x);
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX10_3));
+   pbld.add_vsfs(vs, fs);
+
+   //>> s2: %_:s[0-1], s1: %_:s[2], s1: %_:s[3], s1: %_:s[4], v2: %_:v[0-1], v1: %bias:v[2] = p_startpgm
+   //>> v3: %vec = p_create_vector v1: undef, (kill)%_, (kill)%_
+   //>> lv3: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> BB1
+   //>> v4: %_ = image_sample_b (kill)%_, (kill)%_, v1: undef, %wqm, lv1: undef, (kill)%bias, v2: undef 2d
+   //>> BB2
+   //>> BB6
+   //>> p_end_linear_vgpr (kill)%wqm
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+
+   //>> v_interp_p2_f32_e32 v#rx, v1, attr0.x                                            ; $_
+   //>> v_interp_p2_f32_e32 v#ry_tmp, v1, attr0.y                                        ; $_
+   //>> v_mov_b32_e32 v#rb_tmp, v2                                                       ; $_
+   //>> v_mov_b32_e32 v#ry, v#ry_tmp                                                     ; $_
+   //>> BB1:
+   //>> v_mov_b32_e32 v#rb, v#rb_tmp                                                     ; $_
+   //; success = rb+1 == rx and rb+2 == ry
+   //>> image_sample_b v[8:11], v[#rb:#ry], s[8:15], s[4:7] dmask:0xf dim:SQ_RSRC_IMG_2D ; $_ $_
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.array)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec2 out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform sampler2DArray tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = texture(tex, vec3(in_coord, gl_FragCoord.x));
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX10_3));
+   pbld.add_vsfs(vs, fs);
+
+   //>> s2: %_:s[0-1], s1: %_:s[2], s1: %_:s[3], s1: %_:s[4], v2: %_:v[0-1], v1: %layer:v[2] = p_startpgm
+   //>> v3: %vec = p_create_vector (kill)%_, (kill)%_, v1: undef
+   //>> lv3: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> BB1
+   //>> v1: %layer2 = v_rndne_f32 (kill)%layer
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm, lv1: undef, v2: undef, (kill)%layer2 2darray da
+   //>> BB2
+   //>> BB6
+   //>> p_end_linear_vgpr (kill)%wqm
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+
+   //>> v_interp_p2_f32_e32 v#rx, v1, attr0.x                                                ; $_
+   //>> v_interp_p2_f32_e32 v#ry_tmp, v1, attr0.y                                            ; $_
+   //>> v_mov_b32_e32 v#ry, v#ry_tmp                                                         ; $_
+   //>> BB1:
+   //>> v_rndne_f32_e32 v#rl_tmp, v2                                                         ; $_
+   //>> v_mov_b32_e32 v#rl, v#rl_tmp                                                         ; $_
+   //; success = rx+1 == ry and rx+2 == rl
+   //>> image_sample v[8:11], v[#rx:#rl], s[8:15], s[4:7] dmask:0xf dim:SQ_RSRC_IMG_2D_ARRAY ; $_ $_
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.bias_array)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec2 out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform sampler2DArray tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = texture(tex, vec3(in_coord, gl_FragCoord.x), gl_FragCoord.x);
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX10_3));
+   pbld.add_vsfs(vs, fs);
+
+   //>> s2: %_:s[0-1], s1: %_:s[2], s1: %_:s[3], s1: %_:s[4], v2: %_:v[0-1], v1: %layer_bias:v[2] = p_startpgm
+   //>> v4: %vec = p_create_vector v1: undef, (kill)%_, (kill)%_, v1: undef
+   //>> lv4: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> BB1
+   //>> v1: %layer = v_rndne_f32 %layer_bias
+   //>> v4: %_ = image_sample_b (kill)%_, (kill)%_, v1: undef, %wqm, lv1: undef, (kill)%layer_bias, v2: undef, (kill)%layer 2darray da
+   //>> BB2
+   //>> BB6
+   //>> p_end_linear_vgpr (kill)%wqm
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+
+   //>> v_interp_p2_f32_e32 v#rx, v1, attr0.x                                                  ; $_
+   //>> v_interp_p2_f32_e32 v#ry_tmp, v1, attr0.y                                              ; $_
+   //>> v_mov_b32_e32 v#rbl, v2                                                                ; $_
+   //>> v_mov_b32_e32 v#ry, v#ry_tmp                                                           ; $_
+   //>> BB1:
+   //>> v_rndne_f32_e32 v#rl_tmp, v#rbl                                                        ; $_
+   //>> v_mov_b32_e32 v#rb, v#rbl                                                              ; $_
+   //>> v_mov_b32_e32 v#rl, v#rl_tmp                                                           ; $_
+   //; success = rb+1 == rx and rb+2 == ry and rb+3 == rl
+   //>> image_sample_b v[8:11], v[#rb:#rl], s[8:15], s[4:7] dmask:0xf dim:SQ_RSRC_IMG_2D_ARRAY ; $_ $_
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.1d_gfx9)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in float in_coord;
+      layout(location = 0) out float out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in float in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform sampler1D tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = texture(tex, in_coord);
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX9));
+   pbld.add_vsfs(vs, fs);
+
+   //>> v1: %x = v_interp_p2_f32 (kill)%_, (kill)%_:m0, (kill)%_ attr0.x
+   //>> v2: %vec = p_create_vector (kill)%x, 0.5
+   //>> lv2: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> BB1
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm, lv1: undef, v2: undef 1d
+   //>> BB2
+   //>> BB6
+   //>> p_end_linear_vgpr (kill)%wqm
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+
+   //>> v_interp_p2_f32_e32 v#rx, v1, attr0.x                ; $_
+   //>> v_mov_b32_e32 v#ry, 0.5                              ; $_
+   //; success = rx+1 == ry
+   //>> image_sample v[4:7], v#rx, s[8:15], s[4:7] dmask:0xf ; $_ $_
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.1d_array_gfx9)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in float in_coord;
+      layout(location = 0) out float out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in float in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform sampler1DArray tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = texture(tex, vec2(in_coord, gl_FragCoord.x));
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX9));
+   pbld.add_vsfs(vs, fs);
+
+   //>> s2: %_:s[0-1], s1: %_:s[2], s1: %_:s[3], s1: %_:s[4], v2: %_:v[0-1], v1: %layer:v[2] = p_startpgm
+   //>> v1: %x = v_interp_p2_f32 (kill)%_, (kill)%_:m0, (kill)%_ attr0.x
+   //>> v3: %vec = p_create_vector (kill)%x, 0.5, v1: undef
+   //>> lv3: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> BB1
+   //>> v1: %layer2 = v_rndne_f32 (kill)%layer
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm, lv1: undef, v2: undef, (kill)%layer2 1d da
+   //>> BB2
+   //>> BB6
+   //>> p_end_linear_vgpr (kill)%wqm
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+
+   //>> v_interp_p2_f32_e32 v#rx, v1, attr0.x                   ; $_
+   //>> v_mov_b32_e32 v#rl_tmp, v2                              ; $_
+   //>> v_mov_b32_e32 v#ry, 0.5                                 ; $_
+   //>> BB1:
+   //>> v_rndne_f32_e32 v#rl_tmp2, v#rl_tmp                     ; $_
+   //>> v_mov_b32_e32 v#rl, v#rl_tmp2                           ; $_
+   //; success = rx+1 == ry and rx+2 == rl
+   //>> image_sample v[4:7], v#rx, s[8:15], s[4:7] dmask:0xf da ; $_ $_
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.cube)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in vec3 in_coord;
+      layout(location = 0) out vec3 out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in vec3 in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform samplerCube tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = texture(tex, in_coord);
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX10_3));
+   pbld.add_vsfs(vs, fs);
+
+   //>> v1: %face = v_cubeid_f32 (kill)%_, (kill)%_, (kill)%_
+   //>> v1: %x = v_fmaak_f32 (kill)%_, %_, 0x3fc00000
+   //>> v1: %y = v_fmaak_f32 (kill)%_, (kill)%_, 0x3fc00000
+   //>> v3: %vec = p_create_vector (kill)%x, (kill)%y, (kill)%face
+   //>> lv3: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> BB1
+   //>> v4: %_ = image_sample (kill)%_, (kill)%_, v1: undef, %wqm, lv1: undef, v3: undef cube da
+   //>> BB2
+   //>> BB6
+   //>> p_end_linear_vgpr (kill)%wqm
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+
+   //>> v_cubeid_f32 v#rf, v#_, v#_, v#_                                                ; $_ $_
+   //>> v_fmaak_f32 v#rx, v#_, v#_, 0x3fc00000                                          ; $_ $_
+   //>> v_fmaak_f32 v#ry, v#_, v#_, 0x3fc00000                                          ; $_ $_
+   //; success = rx+1 == ry and rx+2 == rf
+   //>> image_sample v[0:3], v[#rx:#rf], s[8:15], s[4:7] dmask:0xf dim:SQ_RSRC_IMG_CUBE ; $_ $_
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.cube_array)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in vec3 in_coord;
+      layout(location = 0) out vec3 out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in vec3 in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform samplerCubeArray tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = texture(tex, vec4(in_coord, gl_FragCoord.x));
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX10_3));
+   pbld.add_vsfs(vs, fs);
+
+   //>> s2: %_:s[0-1], s1: %_:s[2], s1: %_:s[3], s1: %_:s[4], v2: %_:v[0-1], v1: %layer:v[2] = p_startpgm
+   //>> v1: %face = v_cubeid_f32 (kill)%_, (kill)%_, (kill)%_
+   //>> v1: %x = v_fmaak_f32 (kill)%_, %_, 0x3fc00000
+   //>> v1: %y = v_fmaak_f32 (kill)%_, (kill)%_, 0x3fc00000
+   //>> lv1: %wqm_face = p_start_linear_vgpr (kill)%face
+   //>> v3: %vec = p_create_vector (kill)%x, (kill)%y, v1: undef
+   //>> lv3: %wqm = p_start_linear_vgpr (kill)%vec
+   //>> BB1
+   //>> v1: %layer2 = v_rndne_f32 (kill)%layer
+   //>> v4: %_, s1: (kill)%_:scc = image_sample (kill)%_, (kill)%_, v1: undef, %wqm, %wqm_face, v2: undef, (kill)%layer2 cube da
+   //>> BB2
+   //>> BB6
+   //>> p_end_linear_vgpr (kill)%wqm
+   //>> p_end_linear_vgpr (kill)%wqm_face
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+
+   //>> v_cubeid_f32 v#rf, v#_, v#_, v#_                                                  ; $_ $_
+   //>> v_fmaak_f32 v#rx, v#_, v#_, 0x3fc00000                                            ; $_ $_
+   //>> v_fmaak_f32 v#ry, v#_, v#_, 0x3fc00000                                            ; $_ $_
+   //>> BB1:
+   //>> v_rndne_f32_e32 v#rl, v2                                                          ; $_
+   //>> v_fmamk_f32 v#rlf, v#rl, 0x41000000, v#rf                                         ; $_ $_
+   //>> s_not_b64 exec, exec                                                              ; $_
+   //>> v_mov_b32_e32 v#rlf, v#rf                                                         ; $_
+   //>> s_not_b64 exec, exec                                                              ; $_
+   //; success = rx+1 == ry and rx+2 == rlf
+   //>> image_sample v[8:11], v[#rx:#rlf], s[8:15], s[4:7] dmask:0xf dim:SQ_RSRC_IMG_CUBE ; $_ $_
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "Assembly");
+END_TEST
+
+BEGIN_TEST(d3d11_derivs.fddxy)
+   QoShaderModuleCreateInfo vs = qoShaderModuleCreateInfoGLSL(VERTEX,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec2 out_coord;
+      void main() {
+         out_coord = in_coord;
+      }
+   );
+   QoShaderModuleCreateInfo fs = qoShaderModuleCreateInfoGLSL(FRAGMENT,
+      layout(location = 0) in vec2 in_coord;
+      layout(location = 0) out vec4 out_color;
+      layout(binding = 0) uniform sampler2D tex;
+      void main() {
+         out_color = vec4(0.0);
+         if (gl_FragCoord.x > 1.0)
+            out_color = vec4(dFdxFine(in_coord.x), dFdyCoarse(in_coord.y), textureLod(tex, vec2(0.5), 0.0).xy);
+      }
+   );
+
+   PipelineBuilder pbld(get_vk_device(GFX10_3));
+   pbld.add_vsfs(vs, fs);
+
+   /* Must be before BB1 */
+   //>> v1: %_ = v_sub_f32 (kill)%_, (kill)%_ quad_perm:[1,1,3,3] bound_ctrl:1
+   //>> v1: %_ = v_sub_f32 (kill)%_, (kill)%_ quad_perm:[2,2,2,2] bound_ctrl:1
+   //>> BB1
+   pbld.print_ir(VK_SHADER_STAGE_FRAGMENT_BIT, "ACO IR");
+END_TEST
-- 
GitLab

