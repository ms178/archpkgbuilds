From 1dca4c713af2c655fb59ff396b315a9b718ef5a2 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Sat, 9 Apr 2022 22:28:28 +0200
Subject: [PATCH 1/2] nir: Add algebraic optimization for VKD3D-Proton
 fp32->fp16 conversion.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
---
 src/compiler/nir/nir_opt_algebraic.py | 33 +++++++++++++++++++++++++++
 1 file changed, 33 insertions(+)

diff --git a/src/compiler/nir/nir_opt_algebraic.py b/src/compiler/nir/nir_opt_algebraic.py
index 54be7f7195dd..121670b0d750 100644
--- a/src/compiler/nir/nir_opt_algebraic.py
+++ b/src/compiler/nir/nir_opt_algebraic.py
@@ -2292,6 +2292,39 @@ def bitfield_reverse_cp2077(u):
 optimizations += [(bitfield_reverse_ue4('x@32'), ('bitfield_reverse', 'x'), '!options->lower_bitfield_reverse')]
 optimizations += [(bitfield_reverse_cp2077('x@32'), ('bitfield_reverse', 'x'), '!options->lower_bitfield_reverse')]
 
+# VKD3D-Proton DXBC f32 to f16 conversion implements a float conversion
+# with round-to-zero semantics. There is no suitable SPIR-V opcode,
+# so the following sequence is used.
+
+# Input is f32, output is u32 that has the f16 packed into its low bits.
+def vkd3d_proton_packed_f2f16_rtz_lo(a):
+    packed_half = ('pack_half_2x16_split', a, 0)
+    packed_half_minus1 = ('iadd', packed_half, 0xffffffff)
+    f32_was_not_inf = ('ine', ('fabs', a), 0x7f800000)
+    f16_is_now_inf = ('ieq', ('iand', packed_half, 0x7fff), 0x7c00)
+    return ('bcsel', ('iand', f32_was_not_inf, f16_is_now_inf), packed_half_minus1, packed_half)
+
+# Same as the above, but with fneg.
+def vkd3d_proton_packed_f2f16_rtz_neg_lo(a):
+    packed_half = ('pack_half_2x16_split', ('fneg', a), 0)
+    packed_half_minus1 = ('iadd', packed_half, 0xffffffff)
+    f32_was_not_inf = ('ine', ('fabs', a), 0x7f800000)
+    f16_is_now_inf = ('ieq', ('iand', packed_half, 0x7fff), 0x7c00)
+    return ('bcsel', ('iand', f32_was_not_inf, f16_is_now_inf), packed_half_minus1, packed_half)
+
+# This is for when two of the above are packed.
+def repack_2x16_packed(a, b):
+    packed_a = ('pack_32_2x16', ('vec2', a, 0))
+    packed_b = ('pack_32_2x16', ('vec2', b, 0))
+    shifted_b = ('ishl', packed_b, 16)
+    return ('iadd', packed_a, shifted_b)
+
+optimizations += [
+   (vkd3d_proton_packed_f2f16_rtz_lo('x@32'), ('pack_32_2x16', ('vec2', ('f2f16_rtz', 'x'), 0))),
+   (vkd3d_proton_packed_f2f16_rtz_neg_lo('x@32'), ('pack_32_2x16', ('vec2', ('f2f16_rtz', ('fneg', 'x')), 0))),
+   (repack_2x16_packed('a@16', 'b@16'), ('pack_32_2x16', ('vec2', 'a', 'b'))),
+]
+
 # "all_equal(eq(a, b), vec(~0))" is the same as "all_equal(a, b)"
 # "any_nequal(neq(a, b), vec(0))" is the same as "any_nequal(a, b)"
 for ncomp in [2, 3, 4, 8, 16]:
-- 
GitLab


From a11469065dbc92425500735ab9e518c43396cef8 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Sat, 9 Apr 2022 22:29:22 +0200
Subject: [PATCH 2/2] aco: Add ability to combine two v_cvt_pkrtz into one.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

This pattern occours when VKD3D-Proton performs two fp32->fp16
conversions and wants to pack the results. We can use a single
instruction for that.

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
---
 src/amd/compiler/aco_optimizer.cpp | 38 ++++++++++++++++++++++++++++++
 1 file changed, 38 insertions(+)

diff --git a/src/amd/compiler/aco_optimizer.cpp b/src/amd/compiler/aco_optimizer.cpp
index b0d087743790..697f972518f1 100644
--- a/src/amd/compiler/aco_optimizer.cpp
+++ b/src/amd/compiler/aco_optimizer.cpp
@@ -1469,6 +1469,40 @@ label_instruction(opt_ctx& ctx, aco_ptr<Instruction>& instr)
          break;
       }
 
+      /* Special case for when a vector is created that repacks the outputs
+       * from two v_cvt_pkrtz instructions. This can be done with just a single
+       * v_cvt_pkrtz instruction then.
+       */
+      if (instr->operands.size() == 2 &&
+          instr->definitions[0].bytes() == 4 &&
+          instr->operands[0].isTemp() &&
+          instr->operands[1].isTemp() &&
+          ctx.info[instr->operands[0].tempId()].is_usedef() &&
+          ctx.info[instr->operands[1].tempId()].is_usedef()) {
+         Instruction *op0_instr = ctx.info[instr->operands[0].tempId()].instr;
+         Instruction *op1_instr = ctx.info[instr->operands[1].tempId()].instr;
+
+         if ((op0_instr->opcode == aco_opcode::v_cvt_pkrtz_f16_f32 ||
+              op0_instr->opcode == aco_opcode::v_cvt_pkrtz_f16_f32_e64) &&
+             (op1_instr->opcode == aco_opcode::v_cvt_pkrtz_f16_f32 ||
+              op1_instr->opcode == aco_opcode::v_cvt_pkrtz_f16_f32_e64)) {
+            Definition def = instr->definitions[0];
+
+            /* Prefer VOP2 when both operands are VGPRs. */
+            if (op0_instr->operands[0].regClass().type() == RegType::vgpr &&
+                op1_instr->operands[0].regClass().type() == RegType::vgpr)
+               instr.reset(create_instruction<VOP3_instruction>(aco_opcode::v_cvt_pkrtz_f16_f32,
+                                                               Format::VOP2, 2, 1));
+            else
+               instr.reset(create_instruction<VOP3_instruction>(aco_opcode::v_cvt_pkrtz_f16_f32_e64,
+                                                               Format::VOP3, 2, 1));
+            instr->operands[0] = op0_instr->operands[0];
+            instr->operands[1] = op1_instr->operands[0];
+            instr->definitions[0] = def;
+            break;
+         }
+      }
+
       /* expand vector operands */
       std::vector<Operand> ops;
       unsigned offset = 0;
@@ -1923,6 +1957,10 @@ label_instruction(opt_ctx& ctx, aco_ptr<Instruction>& instr)
          ctx.info[instr->definitions[0].tempId()].set_f2f32(instr.get());
       break;
    }
+   case aco_opcode::v_cvt_pkrtz_f16_f32:
+   case aco_opcode::v_cvt_pkrtz_f16_f32_e64: {
+      ctx.info[instr->definitions[0].tempId()].set_usedef(instr.get());
+   }
    default: break;
    }
 
-- 
GitLab

