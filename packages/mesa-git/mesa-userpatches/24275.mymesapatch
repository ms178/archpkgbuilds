From 26a1769bb0f9bd5cbcbc89137940d08410302e17 Mon Sep 17 00:00:00 2001
From: Samuel Pitoiset <samuel.pitoiset@gmail.com>
Date: Tue, 20 Jun 2023 08:38:57 +0200
Subject: [PATCH 1/6] radv: track the pipeline bind point for indirect commands
 layout

This will be used to implement DGC compute.

Signed-off-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 src/amd/vulkan/radv_device_generated_commands.c | 1 +
 src/amd/vulkan/radv_private.h                   | 2 ++
 2 files changed, 3 insertions(+)

diff --git a/src/amd/vulkan/radv_device_generated_commands.c b/src/amd/vulkan/radv_device_generated_commands.c
index bbc7c5858652..80a783b76504 100644
--- a/src/amd/vulkan/radv_device_generated_commands.c
+++ b/src/amd/vulkan/radv_device_generated_commands.c
@@ -934,6 +934,7 @@ radv_CreateIndirectCommandsLayoutNV(VkDevice _device, const VkIndirectCommandsLa
 
    vk_object_base_init(&device->vk, &layout->base, VK_OBJECT_TYPE_INDIRECT_COMMANDS_LAYOUT_NV);
 
+   layout->pipeline_bind_point = pCreateInfo->pipelineBindPoint;
    layout->input_stride = pCreateInfo->pStreamStrides[0];
    layout->token_count = pCreateInfo->tokenCount;
    typed_memcpy(layout->tokens, pCreateInfo->pTokens, pCreateInfo->tokenCount);
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index d51cbd3e173f..cf9542bf0047 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -3224,6 +3224,8 @@ void radv_sqtt_emit_relocated_shaders(struct radv_cmd_buffer *cmd_buffer, struct
 struct radv_indirect_command_layout {
    struct vk_object_base base;
 
+   VkPipelineBindPoint pipeline_bind_point;
+
    uint32_t input_stride;
    uint32_t token_count;
 
-- 
GitLab


From 9199a61bf163f16480dd1aa0509e132a44dd5702 Mon Sep 17 00:00:00 2001
From: Samuel Pitoiset <samuel.pitoiset@gmail.com>
Date: Tue, 20 Jun 2023 08:39:19 +0200
Subject: [PATCH 2/6] radv: prepare radv_get_sequence_size() for DGC compute

Signed-off-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 .../vulkan/radv_device_generated_commands.c   | 73 ++++++++++++-------
 1 file changed, 47 insertions(+), 26 deletions(-)

diff --git a/src/amd/vulkan/radv_device_generated_commands.c b/src/amd/vulkan/radv_device_generated_commands.c
index 80a783b76504..0fa7cdb67072 100644
--- a/src/amd/vulkan/radv_device_generated_commands.c
+++ b/src/amd/vulkan/radv_device_generated_commands.c
@@ -29,12 +29,18 @@
 #include "vk_common_entrypoints.h"
 
 static void
-radv_get_sequence_size(const struct radv_indirect_command_layout *layout, const struct radv_graphics_pipeline *pipeline,
-                       uint32_t *cmd_size, uint32_t *upload_size)
+radv_get_sequence_size_compute(const struct radv_indirect_command_layout *layout,
+                               const struct radv_compute_pipeline *pipeline, uint32_t *cmd_size)
+{
+   /* TODO */
+}
+
+static void
+radv_get_sequence_size_graphics(const struct radv_indirect_command_layout *layout,
+                                const struct radv_graphics_pipeline *pipeline, uint32_t *cmd_size,
+                                uint32_t *upload_size)
 {
    const struct radv_shader *vs = radv_get_shader(pipeline->base.shaders, MESA_SHADER_VERTEX);
-   *cmd_size = 0;
-   *upload_size = 0;
 
    if (layout->bind_vbo_mask) {
       *upload_size += 16 * util_bitcount(vs->info.vs.vb_desc_usage_mask);
@@ -43,14 +49,38 @@ radv_get_sequence_size(const struct radv_indirect_command_layout *layout, const
       *cmd_size += 3 * 4;
    }
 
+   if (layout->binds_index_buffer) {
+      /* Index type write (normal reg write) + index buffer base write (64-bits, but special packet
+       * so only 1 word overhead) + index buffer size (again, special packet so only 1 word
+       * overhead)
+       */
+      *cmd_size += (3 + 3 + 2) * 4;
+   }
+
+   if (layout->indexed) {
+      /* userdata writes + instance count + indexed draw */
+      *cmd_size += (5 + 2 + 5) * 4;
+   } else {
+      /* userdata writes + instance count + non-indexed draw */
+      *cmd_size += (5 + 2 + 3) * 4;
+   }
+}
+
+static void
+radv_get_sequence_size(const struct radv_indirect_command_layout *layout, struct radv_pipeline *pipeline,
+                       uint32_t *cmd_size, uint32_t *upload_size)
+{
+   *cmd_size = 0;
+   *upload_size = 0;
+
    if (layout->push_constant_mask) {
       bool need_copy = false;
 
-      for (unsigned i = 0; i < ARRAY_SIZE(pipeline->base.shaders); ++i) {
-         if (!pipeline->base.shaders[i])
+      for (unsigned i = 0; i < ARRAY_SIZE(pipeline->shaders); ++i) {
+         if (!pipeline->shaders[i])
             continue;
 
-         struct radv_userdata_locations *locs = &pipeline->base.shaders[i]->info.user_sgprs_locs;
+         struct radv_userdata_locations *locs = &pipeline->shaders[i]->info.user_sgprs_locs;
          if (locs->shader_data[AC_UD_PUSH_CONSTANTS].sgpr_idx >= 0) {
             /* One PKT3_SET_SH_REG for emitting push constants pointer (32-bit) */
             *cmd_size += 3 * 4;
@@ -61,23 +91,16 @@ radv_get_sequence_size(const struct radv_indirect_command_layout *layout, const
             *cmd_size += (2 + locs->shader_data[AC_UD_INLINE_PUSH_CONSTANTS].num_sgprs) * 4;
       }
       if (need_copy)
-         *upload_size += align(pipeline->base.push_constant_size + 16 * pipeline->base.dynamic_offset_count, 16);
-   }
-
-   if (layout->binds_index_buffer) {
-      /* Index type write (normal reg write) + index buffer base write (64-bits, but special packet
-       * so only 1 word overhead) + index buffer size (again, special packet so only 1 word
-       * overhead)
-       */
-      *cmd_size += (3 + 3 + 2) * 4;
+         *upload_size += align(pipeline->push_constant_size + 16 * pipeline->dynamic_offset_count, 16);
    }
 
-   if (layout->indexed) {
-      /* userdata writes + instance count + indexed draw */
-      *cmd_size += (5 + 2 + 5) * 4;
+   if (layout->pipeline_bind_point == VK_PIPELINE_BIND_POINT_GRAPHICS) {
+      struct radv_graphics_pipeline *graphics_pipeline = radv_pipeline_to_graphics(pipeline);
+      radv_get_sequence_size_graphics(layout, graphics_pipeline, cmd_size, upload_size);
    } else {
-      /* userdata writes + instance count + non-indexed draw */
-      *cmd_size += (5 + 2 + 3) * 4;
+      assert(layout->pipeline_bind_point == VK_PIPELINE_BIND_POINT_COMPUTE);
+      struct radv_compute_pipeline *compute_pipeline = radv_pipeline_to_compute(pipeline);
+      radv_get_sequence_size_compute(layout, compute_pipeline, cmd_size);
    }
 }
 
@@ -95,10 +118,9 @@ radv_get_indirect_cmdbuf_size(const VkGeneratedCommandsInfoNV *cmd_info)
    VK_FROM_HANDLE(radv_indirect_command_layout, layout, cmd_info->indirectCommandsLayout);
    VK_FROM_HANDLE(radv_pipeline, pipeline, cmd_info->pipeline);
    const struct radv_device *device = container_of(layout->base.device, struct radv_device, vk);
-   struct radv_graphics_pipeline *graphics_pipeline = radv_pipeline_to_graphics(pipeline);
 
    uint32_t cmd_size, upload_size;
-   radv_get_sequence_size(layout, graphics_pipeline, &cmd_size, &upload_size);
+   radv_get_sequence_size(layout, pipeline, &cmd_size, &upload_size);
    return radv_align_cmdbuf_size(device, cmd_size * cmd_info->sequencesCount);
 }
 
@@ -1008,10 +1030,9 @@ radv_GetGeneratedCommandsMemoryRequirementsNV(VkDevice _device,
    RADV_FROM_HANDLE(radv_device, device, _device);
    VK_FROM_HANDLE(radv_indirect_command_layout, layout, pInfo->indirectCommandsLayout);
    VK_FROM_HANDLE(radv_pipeline, pipeline, pInfo->pipeline);
-   struct radv_graphics_pipeline *graphics_pipeline = radv_pipeline_to_graphics(pipeline);
 
    uint32_t cmd_stride, upload_stride;
-   radv_get_sequence_size(layout, graphics_pipeline, &cmd_stride, &upload_stride);
+   radv_get_sequence_size(layout, pipeline, &cmd_stride, &upload_stride);
 
    VkDeviceSize cmd_buf_size = radv_align_cmdbuf_size(device, cmd_stride * pInfo->maxSequencesCount);
    VkDeviceSize upload_buf_size = upload_stride * pInfo->maxSequencesCount;
@@ -1043,7 +1064,7 @@ radv_prepare_dgc(struct radv_cmd_buffer *cmd_buffer, const VkGeneratedCommandsIn
    struct radv_buffer token_buffer;
 
    uint32_t cmd_stride, upload_stride;
-   radv_get_sequence_size(layout, graphics_pipeline, &cmd_stride, &upload_stride);
+   radv_get_sequence_size(layout, pipeline, &cmd_stride, &upload_stride);
 
    unsigned cmd_buf_size =
       radv_align_cmdbuf_size(cmd_buffer->device, cmd_stride * pGeneratedCommandsInfo->sequencesCount);
-- 
GitLab


From 1725ec4ad6f1b9d47cecf4fafacb5bc00623a401 Mon Sep 17 00:00:00 2001
From: Samuel Pitoiset <samuel.pitoiset@gmail.com>
Date: Mon, 24 Jul 2023 09:26:10 +0200
Subject: [PATCH 3/6] radv: prepare radv_prepare_dgc() for DGC compute

Signed-off-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 .../vulkan/radv_device_generated_commands.c   | 145 ++++++++++--------
 1 file changed, 85 insertions(+), 60 deletions(-)

diff --git a/src/amd/vulkan/radv_device_generated_commands.c b/src/amd/vulkan/radv_device_generated_commands.c
index 0fa7cdb67072..5ebaf740c626 100644
--- a/src/amd/vulkan/radv_device_generated_commands.c
+++ b/src/amd/vulkan/radv_device_generated_commands.c
@@ -1052,44 +1052,24 @@ radv_CmdPreprocessGeneratedCommandsNV(VkCommandBuffer commandBuffer,
 }
 
 /* Always need to call this directly before draw due to dependence on bound state. */
-void
-radv_prepare_dgc(struct radv_cmd_buffer *cmd_buffer, const VkGeneratedCommandsInfoNV *pGeneratedCommandsInfo)
+static void
+radv_prepare_dgc_graphics(struct radv_cmd_buffer *cmd_buffer, const VkGeneratedCommandsInfoNV *pGeneratedCommandsInfo,
+                          unsigned *upload_size, unsigned *upload_offset, void **upload_data,
+                          struct radv_dgc_params *params)
 {
    VK_FROM_HANDLE(radv_indirect_command_layout, layout, pGeneratedCommandsInfo->indirectCommandsLayout);
    VK_FROM_HANDLE(radv_pipeline, pipeline, pGeneratedCommandsInfo->pipeline);
-   VK_FROM_HANDLE(radv_buffer, prep_buffer, pGeneratedCommandsInfo->preprocessBuffer);
    struct radv_graphics_pipeline *graphics_pipeline = radv_pipeline_to_graphics(pipeline);
    struct radv_shader *vs = radv_get_shader(graphics_pipeline->base.shaders, MESA_SHADER_VERTEX);
-   struct radv_meta_saved_state saved_state;
-   struct radv_buffer token_buffer;
-
-   uint32_t cmd_stride, upload_stride;
-   radv_get_sequence_size(layout, pipeline, &cmd_stride, &upload_stride);
-
-   unsigned cmd_buf_size =
-      radv_align_cmdbuf_size(cmd_buffer->device, cmd_stride * pGeneratedCommandsInfo->sequencesCount);
-
    unsigned vb_size = layout->bind_vbo_mask ? util_bitcount(vs->info.vs.vb_desc_usage_mask) * 24 : 0;
-   unsigned const_size = graphics_pipeline->base.push_constant_size +
-                         16 * graphics_pipeline->base.dynamic_offset_count + sizeof(layout->push_constant_offsets) +
-                         ARRAY_SIZE(graphics_pipeline->base.shaders) * 12;
-   if (!layout->push_constant_mask)
-      const_size = 0;
 
-   unsigned upload_size = MAX2(vb_size + const_size, 16);
+   *upload_size = MAX2(*upload_size + vb_size, 16);
 
-   void *upload_data;
-   unsigned upload_offset;
-   if (!radv_cmd_buffer_upload_alloc(cmd_buffer, upload_size, &upload_offset, &upload_data)) {
+   if (!radv_cmd_buffer_upload_alloc(cmd_buffer, *upload_size, upload_offset, upload_data)) {
       vk_command_buffer_set_error(&cmd_buffer->vk, VK_ERROR_OUT_OF_HOST_MEMORY);
       return;
    }
 
-   radv_buffer_init(&token_buffer, cmd_buffer->device, cmd_buffer->upload.upload_bo, upload_size, upload_offset);
-
-   uint64_t upload_addr =
-      radv_buffer_get_va(prep_buffer->bo) + prep_buffer->offset + pGeneratedCommandsInfo->preprocessOffset;
-
    uint16_t vtx_base_sgpr = (cmd_buffer->state.graphics_pipeline->vtx_base_sgpr - SI_SH_REG_OFFSET) >> 2;
    if (cmd_buffer->state.graphics_pipeline->uses_drawid)
       vtx_base_sgpr |= DGC_USES_DRAWID;
@@ -1101,31 +1081,24 @@ radv_prepare_dgc(struct radv_cmd_buffer *cmd_buffer, const VkGeneratedCommandsIn
       ((radv_get_user_sgpr(vertex_shader, AC_UD_VS_VERTEX_BUFFERS)->sgpr_idx * 4 + vertex_shader->info.user_data_0) -
        SI_SH_REG_OFFSET) >>
       2;
-   struct radv_dgc_params params = {
-      .cmd_buf_stride = cmd_stride,
-      .cmd_buf_size = cmd_buf_size,
-      .upload_addr = (uint32_t)upload_addr,
-      .upload_stride = upload_stride,
-      .sequence_count = pGeneratedCommandsInfo->sequencesCount,
-      .stream_stride = layout->input_stride,
-      .draw_indexed = layout->indexed,
-      .draw_params_offset = layout->draw_params_offset,
-      .base_index_size = layout->binds_index_buffer ? 0 : radv_get_vgt_index_size(cmd_buffer->state.index_type),
-      .vtx_base_sgpr = vtx_base_sgpr,
-      .max_index_count = cmd_buffer->state.max_index_count,
-      .index_buffer_offset = layout->index_buffer_offset,
-      .vbo_reg = vbo_sgpr,
-      .ibo_type_32 = layout->ibo_type_32,
-      .ibo_type_8 = layout->ibo_type_8,
-   };
+
+   params->draw_indexed = layout->indexed;
+   params->draw_params_offset = layout->draw_params_offset;
+   params->base_index_size = layout->binds_index_buffer ? 0 : radv_get_vgt_index_size(cmd_buffer->state.index_type);
+   params->vtx_base_sgpr = vtx_base_sgpr;
+   params->max_index_count = cmd_buffer->state.max_index_count;
+   params->index_buffer_offset = layout->index_buffer_offset;
+   params->vbo_reg = vbo_sgpr;
+   params->ibo_type_32 = layout->ibo_type_32;
+   params->ibo_type_8 = layout->ibo_type_8;
 
    if (layout->bind_vbo_mask) {
       uint32_t mask = vertex_shader->info.vs.vb_desc_usage_mask;
       unsigned vb_desc_alloc_size = util_bitcount(mask) * 16;
 
-      radv_write_vertex_descriptors(cmd_buffer, graphics_pipeline, true, upload_data);
+      radv_write_vertex_descriptors(cmd_buffer, graphics_pipeline, true, *upload_data);
 
-      uint32_t *vbo_info = (uint32_t *)((char *)upload_data + vb_desc_alloc_size);
+      uint32_t *vbo_info = (uint32_t *)((char *)*upload_data + vb_desc_alloc_size);
 
       unsigned idx = 0;
       while (mask) {
@@ -1134,26 +1107,77 @@ radv_prepare_dgc(struct radv_cmd_buffer *cmd_buffer, const VkGeneratedCommandsIn
             vertex_shader->info.vs.use_per_attribute_vb_descs ? graphics_pipeline->attrib_bindings[i] : i;
          uint32_t attrib_end = graphics_pipeline->attrib_ends[i];
 
-         params.vbo_bind_mask |= ((layout->bind_vbo_mask >> binding) & 1u) << idx;
+         params->vbo_bind_mask |= ((layout->bind_vbo_mask >> binding) & 1u) << idx;
          vbo_info[2 * idx] =
             ((vertex_shader->info.vs.use_per_attribute_vb_descs ? 1u : 0u) << 31) | layout->vbo_offsets[binding];
          vbo_info[2 * idx + 1] = graphics_pipeline->attrib_index_offset[i] | (attrib_end << 16);
          ++idx;
       }
-      params.vbo_cnt = idx;
-      upload_data = (char *)upload_data + vb_size;
+      params->vbo_cnt = idx;
+      *upload_data = (char *)*upload_data + vb_size;
+   }
+}
+
+static void
+radv_prepare_dgc_compute(struct radv_cmd_buffer *cmd_buffer, const VkGeneratedCommandsInfoNV *pGeneratedCommandsInfo,
+                         unsigned *upload_size, unsigned *upload_offset, void **upload_data,
+                         struct radv_dgc_params *params)
+{
+   /* TODO */
+}
+
+void
+radv_prepare_dgc(struct radv_cmd_buffer *cmd_buffer, const VkGeneratedCommandsInfoNV *pGeneratedCommandsInfo)
+{
+   VK_FROM_HANDLE(radv_indirect_command_layout, layout, pGeneratedCommandsInfo->indirectCommandsLayout);
+   VK_FROM_HANDLE(radv_pipeline, pipeline, pGeneratedCommandsInfo->pipeline);
+   VK_FROM_HANDLE(radv_buffer, prep_buffer, pGeneratedCommandsInfo->preprocessBuffer);
+   struct radv_meta_saved_state saved_state;
+   unsigned upload_offset, upload_size;
+   struct radv_buffer token_buffer;
+   void *upload_data;
+
+   uint32_t cmd_stride, upload_stride;
+   radv_get_sequence_size(layout, pipeline, &cmd_stride, &upload_stride);
+
+   unsigned cmd_buf_size =
+      radv_align_cmdbuf_size(cmd_buffer->device, cmd_stride * pGeneratedCommandsInfo->sequencesCount);
+
+   uint64_t upload_addr =
+      radv_buffer_get_va(prep_buffer->bo) + prep_buffer->offset + pGeneratedCommandsInfo->preprocessOffset;
+
+   struct radv_dgc_params params = {
+      .cmd_buf_stride = cmd_stride,
+      .cmd_buf_size = cmd_buf_size,
+      .upload_addr = (uint32_t)upload_addr,
+      .upload_stride = upload_stride,
+      .sequence_count = pGeneratedCommandsInfo->sequencesCount,
+      .stream_stride = layout->input_stride,
+   };
+
+   upload_size = pipeline->push_constant_size + 16 * pipeline->dynamic_offset_count +
+                 sizeof(layout->push_constant_offsets) + ARRAY_SIZE(pipeline->shaders) * 12;
+   if (!layout->push_constant_mask)
+      upload_size = 0;
+
+   if (layout->pipeline_bind_point == VK_PIPELINE_BIND_POINT_GRAPHICS) {
+      radv_prepare_dgc_graphics(cmd_buffer, pGeneratedCommandsInfo, &upload_size, &upload_offset, &upload_data,
+                                &params);
+   } else {
+      assert(layout->pipeline_bind_point == VK_PIPELINE_BIND_POINT_COMPUTE);
+      radv_prepare_dgc_compute(cmd_buffer, pGeneratedCommandsInfo, &upload_size, &upload_offset, &upload_data, &params);
    }
 
    if (layout->push_constant_mask) {
       uint32_t *desc = upload_data;
-      upload_data = (char *)upload_data + ARRAY_SIZE(graphics_pipeline->base.shaders) * 12;
+      upload_data = (char *)upload_data + ARRAY_SIZE(pipeline->shaders) * 12;
 
       unsigned idx = 0;
-      for (unsigned i = 0; i < ARRAY_SIZE(graphics_pipeline->base.shaders); ++i) {
-         if (!graphics_pipeline->base.shaders[i])
+      for (unsigned i = 0; i < ARRAY_SIZE(pipeline->shaders); ++i) {
+         if (!pipeline->shaders[i])
             continue;
 
-         const struct radv_shader *shader = graphics_pipeline->base.shaders[i];
+         const struct radv_shader *shader = pipeline->shaders[i];
          const struct radv_userdata_locations *locs = &shader->info.user_sgprs_locs;
          if (locs->shader_data[AC_UD_PUSH_CONSTANTS].sgpr_idx >= 0)
             params.const_copy = 1;
@@ -1173,8 +1197,8 @@ radv_prepare_dgc(struct radv_cmd_buffer *cmd_buffer, const VkGeneratedCommandsIn
                inline_sgpr = (shader->info.user_data_0 + 4 * locs->shader_data[AC_UD_INLINE_PUSH_CONSTANTS].sgpr_idx -
                               SI_SH_REG_OFFSET) >>
                              2;
-               desc[idx * 3 + 1] = graphics_pipeline->base.shaders[i]->info.inline_push_constant_mask;
-               desc[idx * 3 + 2] = graphics_pipeline->base.shaders[i]->info.inline_push_constant_mask >> 32;
+               desc[idx * 3 + 1] = pipeline->shaders[i]->info.inline_push_constant_mask;
+               desc[idx * 3 + 2] = pipeline->shaders[i]->info.inline_push_constant_mask >> 32;
             }
             desc[idx * 3] = upload_sgpr | (inline_sgpr << 16);
             ++idx;
@@ -1183,22 +1207,23 @@ radv_prepare_dgc(struct radv_cmd_buffer *cmd_buffer, const VkGeneratedCommandsIn
 
       params.push_constant_shader_cnt = idx;
 
-      params.const_copy_size =
-         graphics_pipeline->base.push_constant_size + 16 * graphics_pipeline->base.dynamic_offset_count;
+      params.const_copy_size = pipeline->push_constant_size + 16 * pipeline->dynamic_offset_count;
       params.push_constant_mask = layout->push_constant_mask;
 
       memcpy(upload_data, layout->push_constant_offsets, sizeof(layout->push_constant_offsets));
       upload_data = (char *)upload_data + sizeof(layout->push_constant_offsets);
 
-      memcpy(upload_data, cmd_buffer->push_constants, graphics_pipeline->base.push_constant_size);
-      upload_data = (char *)upload_data + graphics_pipeline->base.push_constant_size;
+      memcpy(upload_data, cmd_buffer->push_constants, pipeline->push_constant_size);
+      upload_data = (char *)upload_data + pipeline->push_constant_size;
 
       struct radv_descriptor_state *descriptors_state =
          radv_get_descriptors_state(cmd_buffer, pGeneratedCommandsInfo->pipelineBindPoint);
-      memcpy(upload_data, descriptors_state->dynamic_buffers, 16 * graphics_pipeline->base.dynamic_offset_count);
-      upload_data = (char *)upload_data + 16 * graphics_pipeline->base.dynamic_offset_count;
+      memcpy(upload_data, descriptors_state->dynamic_buffers, 16 * pipeline->dynamic_offset_count);
+      upload_data = (char *)upload_data + 16 * pipeline->dynamic_offset_count;
    }
 
+   radv_buffer_init(&token_buffer, cmd_buffer->device, cmd_buffer->upload.upload_bo, upload_size, upload_offset);
+
    VkWriteDescriptorSet ds_writes[5];
    VkDescriptorBufferInfo buf_info[ARRAY_SIZE(ds_writes)];
    int ds_cnt = 0;
-- 
GitLab


From bd1fb423fd6c38cea7072dc005663e97906ed7f4 Mon Sep 17 00:00:00 2001
From: Samuel Pitoiset <samuel.pitoiset@gmail.com>
Date: Mon, 24 Jul 2023 09:48:43 +0200
Subject: [PATCH 4/6] radv: implement NV_device_generated_commands_compute

Signed-off-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 src/amd/vulkan/radv_cmd_buffer.c              | 140 +++++++++---
 .../vulkan/radv_device_generated_commands.c   | 202 +++++++++++++++---
 src/amd/vulkan/radv_private.h                 |   2 +
 3 files changed, 283 insertions(+), 61 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index 1634c2db5493..1dee0d82ad23 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -9453,6 +9453,10 @@ radv_CmdDrawMeshTasksIndirectCountEXT(VkCommandBuffer commandBuffer, VkBuffer _b
    radv_after_draw(cmd_buffer);
 }
 
+/* TODO: Use these functions with the normal dispatch path. */
+static void radv_dgc_before_dispatch(struct radv_cmd_buffer *cmd_buffer);
+static void radv_dgc_after_dispatch(struct radv_cmd_buffer *cmd_buffer);
+
 VKAPI_ATTR void VKAPI_CALL
 radv_CmdExecuteGeneratedCommandsNV(VkCommandBuffer commandBuffer, VkBool32 isPreprocessed,
                                    const VkGeneratedCommandsInfoNV *pGeneratedCommandsInfo)
@@ -9461,7 +9465,7 @@ radv_CmdExecuteGeneratedCommandsNV(VkCommandBuffer commandBuffer, VkBool32 isPre
    VK_FROM_HANDLE(radv_indirect_command_layout, layout, pGeneratedCommandsInfo->indirectCommandsLayout);
    VK_FROM_HANDLE(radv_pipeline, pipeline, pGeneratedCommandsInfo->pipeline);
    VK_FROM_HANDLE(radv_buffer, prep_buffer, pGeneratedCommandsInfo->preprocessBuffer);
-   struct radv_graphics_pipeline *graphics_pipeline = radv_pipeline_to_graphics(pipeline);
+   const bool compute = layout->pipeline_bind_point == VK_PIPELINE_BIND_POINT_COMPUTE;
    const struct radv_device *device = cmd_buffer->device;
 
    /* The only actions that can be done are draws, so skip on other queues. */
@@ -9475,20 +9479,24 @@ radv_CmdExecuteGeneratedCommandsNV(VkCommandBuffer commandBuffer, VkBool32 isPre
 
    radv_prepare_dgc(cmd_buffer, pGeneratedCommandsInfo);
 
-   struct radv_draw_info info;
+   if (compute) {
+      radv_dgc_before_dispatch(cmd_buffer);
+   } else {
+      struct radv_draw_info info;
 
-   info.count = pGeneratedCommandsInfo->sequencesCount;
-   info.indirect = prep_buffer; /* We're not really going use it this way, but a good signal
+      info.count = pGeneratedCommandsInfo->sequencesCount;
+      info.indirect = prep_buffer; /* We're not really going use it this way, but a good signal
                                    that this is not direct. */
-   info.indirect_offset = 0;
-   info.stride = 0;
-   info.strmout_buffer = NULL;
-   info.count_buffer = NULL;
-   info.indexed = layout->indexed;
-   info.instance_count = 0;
-
-   if (!radv_before_draw(cmd_buffer, &info, 1))
-      return;
+      info.indirect_offset = 0;
+      info.stride = 0;
+      info.strmout_buffer = NULL;
+      info.count_buffer = NULL;
+      info.indexed = layout->indexed;
+      info.instance_count = 0;
+
+      if (!radv_before_draw(cmd_buffer, &info, 1))
+         return;
+   }
 
    uint32_t cmdbuf_size = radv_get_indirect_cmdbuf_size(pGeneratedCommandsInfo);
    struct radeon_winsys_bo *ib_bo = prep_buffer->bo;
@@ -9498,7 +9506,7 @@ radv_CmdExecuteGeneratedCommandsNV(VkCommandBuffer commandBuffer, VkBool32 isPre
    radeon_emit(cmd_buffer->cs, PKT3(PKT3_PFP_SYNC_ME, 0, cmd_buffer->state.predicating));
    radeon_emit(cmd_buffer->cs, 0);
 
-   if (!view_mask) {
+   if (compute || !view_mask) {
       device->ws->cs_execute_ib(cmd_buffer->cs, ib_bo, ib_offset, cmdbuf_size >> 2);
    } else {
       u_foreach_bit (view, view_mask) {
@@ -9508,32 +9516,40 @@ radv_CmdExecuteGeneratedCommandsNV(VkCommandBuffer commandBuffer, VkBool32 isPre
       }
    }
 
-   if (layout->binds_index_buffer) {
-      cmd_buffer->state.last_index_type = -1;
-      cmd_buffer->state.dirty |= RADV_CMD_DIRTY_INDEX_BUFFER;
-   }
+   if (compute) {
+      cmd_buffer->push_constant_stages |= VK_SHADER_STAGE_COMPUTE_BIT;
 
-   if (layout->bind_vbo_mask)
-      cmd_buffer->state.dirty |= RADV_CMD_DIRTY_VERTEX_BUFFER;
+      radv_dgc_after_dispatch(cmd_buffer);
+   } else {
+      struct radv_graphics_pipeline *graphics_pipeline = radv_pipeline_to_graphics(pipeline);
 
-   if (layout->binds_state)
-      cmd_buffer->state.dirty |= RADV_CMD_DIRTY_DYNAMIC_FRONT_FACE;
+      if (layout->binds_index_buffer) {
+         cmd_buffer->state.last_index_type = -1;
+         cmd_buffer->state.dirty |= RADV_CMD_DIRTY_INDEX_BUFFER;
+      }
 
-   cmd_buffer->push_constant_stages |= graphics_pipeline->active_stages;
+      if (layout->bind_vbo_mask)
+         cmd_buffer->state.dirty |= RADV_CMD_DIRTY_VERTEX_BUFFER;
 
-   if (!layout->indexed && cmd_buffer->device->physical_device->rad_info.gfx_level >= GFX7) {
-      /* On GFX7 and later, non-indexed draws overwrite VGT_INDEX_TYPE, so the state must be
-       * re-emitted before the next indexed draw.
-       */
-      cmd_buffer->state.last_index_type = -1;
-   }
+      if (layout->binds_state)
+         cmd_buffer->state.dirty |= RADV_CMD_DIRTY_DYNAMIC_FRONT_FACE;
 
-   cmd_buffer->state.last_num_instances = -1;
-   cmd_buffer->state.last_vertex_offset_valid = false;
-   cmd_buffer->state.last_first_instance = -1;
-   cmd_buffer->state.last_drawid = -1;
+      cmd_buffer->push_constant_stages |= graphics_pipeline->active_stages;
 
-   radv_after_draw(cmd_buffer);
+      if (!layout->indexed && cmd_buffer->device->physical_device->rad_info.gfx_level >= GFX7) {
+         /* On GFX7 and later, non-indexed draws overwrite VGT_INDEX_TYPE, so the state must be
+          * re-emitted before the next indexed draw.
+          */
+         cmd_buffer->state.last_index_type = -1;
+      }
+
+      cmd_buffer->state.last_num_instances = -1;
+      cmd_buffer->state.last_vertex_offset_valid = false;
+      cmd_buffer->state.last_first_instance = -1;
+      cmd_buffer->state.last_drawid = -1;
+
+      radv_after_draw(cmd_buffer);
+   }
 }
 
 static void
@@ -9765,6 +9781,54 @@ radv_dispatch(struct radv_cmd_buffer *cmd_buffer, const struct radv_dispatch_inf
    radv_cmd_buffer_after_draw(cmd_buffer, RADV_CMD_FLAG_CS_PARTIAL_FLUSH);
 }
 
+static void
+radv_dgc_before_dispatch(struct radv_cmd_buffer *cmd_buffer)
+{
+   struct radv_compute_pipeline *pipeline = cmd_buffer->state.compute_pipeline;
+   struct radv_shader *compute_shader = cmd_buffer->state.shaders[MESA_SHADER_COMPUTE];
+
+   /* We will have run the DGC patch shaders before, so we can assume that there is something to
+    * flush. Otherwise, we just split radv_dispatch in two. One pre-dispatch and another one
+    * post-dispatch. */
+
+   if (compute_shader->info.cs.regalloc_hang_bug)
+      cmd_buffer->state.flush_bits |= RADV_CMD_FLAG_PS_PARTIAL_FLUSH | RADV_CMD_FLAG_CS_PARTIAL_FLUSH;
+
+   radv_emit_compute_pipeline(cmd_buffer, pipeline);
+   si_emit_cache_flush(cmd_buffer);
+
+   radv_upload_compute_shader_descriptors(cmd_buffer, VK_PIPELINE_BIND_POINT_COMPUTE);
+}
+
+static void
+radv_dgc_after_dispatch(struct radv_cmd_buffer *cmd_buffer)
+{
+   struct radv_compute_pipeline *pipeline = cmd_buffer->state.compute_pipeline;
+   struct radv_shader *compute_shader = cmd_buffer->state.shaders[MESA_SHADER_COMPUTE];
+   bool has_prefetch = cmd_buffer->device->physical_device->rad_info.gfx_level >= GFX7;
+   bool pipeline_is_dirty = pipeline != cmd_buffer->state.emitted_compute_pipeline;
+
+   if (has_prefetch && pipeline_is_dirty) {
+      radv_emit_shader_prefetch(cmd_buffer, compute_shader);
+   }
+
+   if (pipeline_is_dirty) {
+      /* Raytracing uses compute shaders but has separate bind points and pipelines.
+       * So if we set compute userdata & shader registers we should dirty the raytracing
+       * ones and the other way around.
+       *
+       * We only need to do this when the pipeline is dirty because when we switch between
+       * the two we always need to switch pipelines.
+       */
+      radv_mark_descriptor_sets_dirty(cmd_buffer, VK_PIPELINE_BIND_POINT_RAY_TRACING_KHR);
+   }
+
+   if (compute_shader->info.cs.regalloc_hang_bug)
+      cmd_buffer->state.flush_bits |= RADV_CMD_FLAG_CS_PARTIAL_FLUSH;
+
+   radv_cmd_buffer_after_draw(cmd_buffer, RADV_CMD_FLAG_CS_PARTIAL_FLUSH);
+}
+
 void
 radv_compute_dispatch(struct radv_cmd_buffer *cmd_buffer, const struct radv_dispatch_info *info)
 {
@@ -11053,6 +11117,14 @@ radv_CmdBindPipelineShaderGroupNV(VkCommandBuffer commandBuffer, VkPipelineBindP
    abort();
 }
 
+/* VK_NV_device_generated_commands_compute */
+VKAPI_ATTR void VKAPI_CALL
+radv_CmdUpdatePipelineIndirectBufferNV(VkCommandBuffer commandBuffer, VkPipelineBindPoint pipelineBindPoint,
+                                       VkPipeline pipeline)
+{
+   unreachable("radv: unimplemented vkCmdUpdatePipelineIndirectBufferNV");
+}
+
 /* VK_EXT_descriptor_buffer */
 VKAPI_ATTR void VKAPI_CALL
 radv_CmdBindDescriptorBuffersEXT(VkCommandBuffer commandBuffer, uint32_t bufferCount,
diff --git a/src/amd/vulkan/radv_device_generated_commands.c b/src/amd/vulkan/radv_device_generated_commands.c
index 5ebaf740c626..719b84a7be01 100644
--- a/src/amd/vulkan/radv_device_generated_commands.c
+++ b/src/amd/vulkan/radv_device_generated_commands.c
@@ -32,7 +32,22 @@ static void
 radv_get_sequence_size_compute(const struct radv_indirect_command_layout *layout,
                                const struct radv_compute_pipeline *pipeline, uint32_t *cmd_size)
 {
-   /* TODO */
+   const struct radv_device *device = container_of(layout->base.device, struct radv_device, vk);
+   struct radv_shader *cs = radv_get_shader(pipeline->base.shaders, MESA_SHADER_COMPUTE);
+
+   /* dispatch */
+   *cmd_size += 5 * 4;
+
+   const struct radv_userdata_info *loc = radv_get_user_sgpr(cs, AC_UD_CS_GRID_SIZE);
+   if (loc->sgpr_idx != -1) {
+      if (device->load_grid_size_from_user_sgpr) {
+         /* PKT3_SET_SH_REG for immediate values */
+         *cmd_size += 5 * 4;
+      } else {
+         /* PKT3_SET_SH_REG for pointer */
+         *cmd_size += 4 * 4;
+      }
+   }
 }
 
 static void
@@ -107,7 +122,8 @@ radv_get_sequence_size(const struct radv_indirect_command_layout *layout, struct
 static uint32_t
 radv_align_cmdbuf_size(const struct radv_device *device, uint32_t size)
 {
-   const uint32_t ib_pad_dw_mask = device->physical_device->rad_info.ib_pad_dw_mask[AMD_IP_GFX];
+   const uint32_t ib_pad_dw_mask = MAX2(device->physical_device->rad_info.ib_pad_dw_mask[AMD_IP_GFX],
+                                        device->physical_device->rad_info.ib_pad_dw_mask[AMD_IP_COMPUTE]);
 
    return align(size, ib_pad_dw_mask + 1);
 }
@@ -131,6 +147,7 @@ struct radv_dgc_params {
    uint32_t upload_addr;
    uint32_t sequence_count;
    uint32_t stream_stride;
+   uint64_t stream_addr;
 
    /* draw info */
    uint16_t draw_indexed;
@@ -139,6 +156,11 @@ struct radv_dgc_params {
    uint16_t vtx_base_sgpr;
    uint32_t max_index_count;
 
+   /* dispatch info */
+   uint32_t dispatch_initiator;
+   uint16_t dispatch_params_offset;
+   uint16_t grid_base_sgpr;
+
    /* bind index buffer info. Valid if base_index_size == 0 && draw_indexed */
    uint16_t index_buffer_offset;
 
@@ -158,6 +180,8 @@ struct radv_dgc_params {
    uint32_t ibo_type_8;
 
    uint16_t push_constant_shader_cnt;
+
+   uint8_t is_dispatch;
 };
 
 enum {
@@ -278,6 +302,41 @@ dgc_emit_draw_index_auto(nir_builder *b, struct dgc_cmdbuf *cs, nir_def *vertex_
    dgc_emit(b, cs, nir_vec(b, values, 3));
 }
 
+static void
+dgc_emit_dispatch_direct(nir_builder *b, struct dgc_cmdbuf *cs, nir_def *wg_x, nir_def *wg_y, nir_def *wg_z,
+                         nir_def *dispatch_initiator)
+{
+   nir_def *values[5] = {nir_imm_int(b, PKT3(PKT3_DISPATCH_DIRECT, 3, false) | PKT3_SHADER_TYPE_S(1)), wg_x, wg_y, wg_z,
+                         dispatch_initiator};
+
+   dgc_emit(b, cs, nir_vec(b, values, 5));
+}
+
+static void
+dgc_emit_grid_size_user_sgpr(nir_builder *b, struct dgc_cmdbuf *cs, nir_def *grid_base_sgpr, nir_def *wg_x,
+                             nir_def *wg_y, nir_def *wg_z)
+{
+   nir_def *values[5] = {
+      nir_imm_int(b, PKT3(PKT3_SET_SH_REG, 3, false)), grid_base_sgpr, wg_x, wg_y, wg_z,
+   };
+
+   dgc_emit(b, cs, nir_vec(b, values, 5));
+}
+
+static void
+dgc_emit_grid_size_pointer(nir_builder *b, struct dgc_cmdbuf *cs, nir_def *grid_base_sgpr, nir_def *stream_offset)
+{
+   nir_def *stream_addr = load_param64(b, stream_addr);
+   nir_def *va = nir_iadd(b, stream_addr, nir_u2u64(b, stream_offset));
+
+   nir_def *va_lo = nir_unpack_64_2x32_split_x(b, va);
+   nir_def *va_hi = nir_unpack_64_2x32_split_y(b, va);
+
+   nir_def *values[4] = {nir_imm_int(b, PKT3(PKT3_SET_SH_REG, 2, false)), grid_base_sgpr, va_lo, va_hi};
+
+   dgc_emit(b, cs, nir_vec(b, values, 4));
+}
+
 static void
 build_dgc_buffer_tail(nir_builder *b, nir_def *sequence_count, const struct radv_device *device)
 {
@@ -715,6 +774,38 @@ dgc_emit_vertex_buffer(nir_builder *b, struct dgc_cmdbuf *cs, nir_def *stream_bu
    nir_store_var(b, upload_offset, nir_iadd(b, nir_load_var(b, upload_offset), nir_imul_imm(b, vbo_cnt, 16)), 0x1);
 }
 
+/**
+ * For emitting VK_INDIRECT_COMMANDS_TOKEN_TYPE_DISPATCH_NV.
+ */
+static void
+dgc_emit_dispatch(nir_builder *b, struct dgc_cmdbuf *cs, nir_def *stream_buf, nir_def *stream_base,
+                  nir_def *dispatch_params_offset, const struct radv_device *device)
+{
+   nir_def *stream_offset = nir_iadd(b, dispatch_params_offset, stream_base);
+
+   nir_def *dispatch_data = nir_load_ssbo(b, 3, 32, stream_buf, stream_offset);
+   nir_def *wg_x = nir_channel(b, dispatch_data, 0);
+   nir_def *wg_y = nir_channel(b, dispatch_data, 1);
+   nir_def *wg_z = nir_channel(b, dispatch_data, 2);
+
+   nir_def *grid_sgpr = load_param16(b, grid_base_sgpr);
+   nir_push_if(b, nir_ine_imm(b, grid_sgpr, 0));
+   {
+      if (device->load_grid_size_from_user_sgpr) {
+         dgc_emit_grid_size_user_sgpr(b, cs, grid_sgpr, wg_x, wg_y, wg_z);
+      } else {
+         dgc_emit_grid_size_pointer(b, cs, grid_sgpr, stream_offset);
+      }
+   }
+   nir_pop_if(b, 0);
+
+   nir_push_if(b, nir_iand(b, nir_ine_imm(b, wg_x, 0), nir_iand(b, nir_ine_imm(b, wg_y, 0), nir_ine_imm(b, wg_z, 0))));
+   {
+      dgc_emit_dispatch_direct(b, cs, wg_x, wg_y, wg_z, load_param32(b, dispatch_initiator));
+   }
+   nir_pop_if(b, 0);
+}
+
 static nir_shader *
 build_dgc_prepare_shader(struct radv_device *dev)
 {
@@ -784,36 +875,45 @@ build_dgc_prepare_shader(struct radv_device *dev)
       }
       nir_pop_if(&b, 0);
 
-      nir_push_if(&b, nir_ieq_imm(&b, load_param16(&b, draw_indexed), 0));
-      {
-         dgc_emit_draw(&b, &cmd_buf, stream_buf, stream_base, load_param16(&b, draw_params_offset), sequence_id, dev);
-      }
-      nir_push_else(&b, NULL);
+      nir_push_if(&b, nir_ieq_imm(&b, load_param8(&b, is_dispatch), 0));
       {
-         nir_variable *index_size_var =
-            nir_variable_create(b.shader, nir_var_shader_temp, glsl_uint_type(), "index_size");
-         nir_store_var(&b, index_size_var, load_param16(&b, base_index_size), 0x1);
-         nir_variable *max_index_count_var =
-            nir_variable_create(b.shader, nir_var_shader_temp, glsl_uint_type(), "max_index_count");
-         nir_store_var(&b, max_index_count_var, load_param32(&b, max_index_count), 0x1);
-
-         nir_def *bind_index_buffer = nir_ieq_imm(&b, nir_load_var(&b, index_size_var), 0);
-         nir_push_if(&b, bind_index_buffer);
+         nir_push_if(&b, nir_ieq_imm(&b, load_param16(&b, draw_indexed), 0));
          {
-            dgc_emit_index_buffer(&b, &cmd_buf, stream_buf, stream_base, load_param16(&b, index_buffer_offset),
-                                  load_param32(&b, ibo_type_32), load_param32(&b, ibo_type_8), index_size_var,
-                                  max_index_count_var, dev);
+            dgc_emit_draw(&b, &cmd_buf, stream_buf, stream_base, load_param16(&b, draw_params_offset), sequence_id,
+                          dev);
          }
-         nir_pop_if(&b, NULL);
+         nir_push_else(&b, NULL);
+         {
+            nir_variable *index_size_var =
+               nir_variable_create(b.shader, nir_var_shader_temp, glsl_uint_type(), "index_size");
+            nir_store_var(&b, index_size_var, load_param16(&b, base_index_size), 0x1);
+            nir_variable *max_index_count_var =
+               nir_variable_create(b.shader, nir_var_shader_temp, glsl_uint_type(), "max_index_count");
+            nir_store_var(&b, max_index_count_var, load_param32(&b, max_index_count), 0x1);
+
+            nir_def *bind_index_buffer = nir_ieq_imm(&b, nir_load_var(&b, index_size_var), 0);
+            nir_push_if(&b, bind_index_buffer);
+            {
+               dgc_emit_index_buffer(&b, &cmd_buf, stream_buf, stream_base, load_param16(&b, index_buffer_offset),
+                                     load_param32(&b, ibo_type_32), load_param32(&b, ibo_type_8), index_size_var,
+                                     max_index_count_var, dev);
+            }
+            nir_pop_if(&b, NULL);
 
-         nir_def *index_size = nir_load_var(&b, index_size_var);
-         nir_def *max_index_count = nir_load_var(&b, max_index_count_var);
+            nir_def *index_size = nir_load_var(&b, index_size_var);
+            nir_def *max_index_count = nir_load_var(&b, max_index_count_var);
 
-         index_size = nir_bcsel(&b, bind_index_buffer, nir_load_var(&b, index_size_var), index_size);
-         max_index_count = nir_bcsel(&b, bind_index_buffer, nir_load_var(&b, max_index_count_var), max_index_count);
+            index_size = nir_bcsel(&b, bind_index_buffer, nir_load_var(&b, index_size_var), index_size);
+            max_index_count = nir_bcsel(&b, bind_index_buffer, nir_load_var(&b, max_index_count_var), max_index_count);
 
-         dgc_emit_draw_indexed(&b, &cmd_buf, stream_buf, stream_base, load_param16(&b, draw_params_offset), sequence_id,
-                               max_index_count, dev);
+            dgc_emit_draw_indexed(&b, &cmd_buf, stream_buf, stream_base, load_param16(&b, draw_params_offset),
+                                  sequence_id, max_index_count, dev);
+         }
+         nir_pop_if(&b, NULL);
+      }
+      nir_push_else(&b, NULL);
+      {
+         dgc_emit_dispatch(&b, &cmd_buf, stream_buf, stream_base, load_param16(&b, dispatch_params_offset), dev);
       }
       nir_pop_if(&b, NULL);
 
@@ -973,6 +1073,9 @@ radv_CreateIndirectCommandsLayoutNV(VkDevice _device, const VkIndirectCommandsLa
          layout->indexed = true;
          layout->draw_params_offset = pCreateInfo->pTokens[i].offset;
          break;
+      case VK_INDIRECT_COMMANDS_TOKEN_TYPE_DISPATCH_NV:
+         layout->dispatch_params_offset = pCreateInfo->pTokens[i].offset;
+         break;
       case VK_INDIRECT_COMMANDS_TOKEN_TYPE_INDEX_BUFFER_NV:
          layout->binds_index_buffer = true;
          layout->index_buffer_offset = pCreateInfo->pTokens[i].offset;
@@ -1123,7 +1226,38 @@ radv_prepare_dgc_compute(struct radv_cmd_buffer *cmd_buffer, const VkGeneratedCo
                          unsigned *upload_size, unsigned *upload_offset, void **upload_data,
                          struct radv_dgc_params *params)
 {
-   /* TODO */
+   VK_FROM_HANDLE(radv_indirect_command_layout, layout, pGeneratedCommandsInfo->indirectCommandsLayout);
+   VK_FROM_HANDLE(radv_pipeline, pipeline, pGeneratedCommandsInfo->pipeline);
+   VK_FROM_HANDLE(radv_buffer, stream_buffer, pGeneratedCommandsInfo->pStreams[0].buffer);
+   struct radv_compute_pipeline *compute_pipeline = radv_pipeline_to_compute(pipeline);
+   struct radv_shader *cs = radv_get_shader(compute_pipeline->base.shaders, MESA_SHADER_COMPUTE);
+
+   *upload_size = MAX2(*upload_size, 16);
+
+   if (!radv_cmd_buffer_upload_alloc(cmd_buffer, *upload_size, upload_offset, upload_data)) {
+      vk_command_buffer_set_error(&cmd_buffer->vk, VK_ERROR_OUT_OF_HOST_MEMORY);
+      return;
+   }
+
+   uint32_t dispatch_initiator = cmd_buffer->device->dispatch_initiator;
+   dispatch_initiator |= S_00B800_FORCE_START_AT_000(1);
+   if (cs->info.wave_size == 32) {
+      assert(cmd_buffer->device->physical_device->rad_info.gfx_level >= GFX10);
+      dispatch_initiator |= S_00B800_CS_W32_EN(1);
+   }
+
+   uint64_t stream_addr =
+      radv_buffer_get_va(stream_buffer->bo) + stream_buffer->offset + pGeneratedCommandsInfo->pStreams[0].offset;
+
+   params->dispatch_params_offset = layout->dispatch_params_offset;
+   params->dispatch_initiator = dispatch_initiator;
+   params->is_dispatch = 1;
+   params->stream_addr = stream_addr;
+
+   const struct radv_userdata_info *loc = radv_get_user_sgpr(cs, AC_UD_CS_GRID_SIZE);
+   if (loc->sgpr_idx != -1) {
+      params->grid_base_sgpr = (cs->info.user_data_0 + 4 * loc->sgpr_idx - SI_SH_REG_OFFSET) >> 2;
+   }
 }
 
 void
@@ -1295,3 +1429,17 @@ radv_prepare_dgc(struct radv_cmd_buffer *cmd_buffer, const VkGeneratedCommandsIn
 
    cmd_buffer->state.flush_bits |= RADV_CMD_FLAG_CS_PARTIAL_FLUSH | RADV_CMD_FLAG_INV_VCACHE | RADV_CMD_FLAG_INV_L2;
 }
+
+/* VK_NV_device_generated_commands_compute */
+VKAPI_ATTR void VKAPI_CALL
+radv_GetPipelineIndirectMemoryRequirementsNV(VkDevice device, const VkComputePipelineCreateInfo *pCreateInfo,
+                                             VkMemoryRequirements2 *pMemoryRequirements)
+{
+   unreachable("radv: unimplemented vkGetPipelineIndirectMemoryRequirementsNV");
+}
+
+VKAPI_ATTR VkDeviceAddress VKAPI_CALL
+radv_GetPipelineIndirectDeviceAddressNV(VkDevice device, const VkPipelineIndirectDeviceAddressInfoNV *pInfo)
+{
+   unreachable("radv: unimplemented vkGetPipelineIndirectDeviceAddressNV");
+}
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index cf9542bf0047..47e315488e9f 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -3235,6 +3235,8 @@ struct radv_indirect_command_layout {
    uint16_t draw_params_offset;
    uint16_t index_buffer_offset;
 
+   uint16_t dispatch_params_offset;
+
    uint16_t state_offset;
 
    uint32_t bind_vbo_mask;
-- 
GitLab


From ebccb6c96278be7f6be2cec5423ff2ea44328d79 Mon Sep 17 00:00:00 2001
From: Samuel Pitoiset <samuel.pitoiset@gmail.com>
Date: Tue, 20 Jun 2023 08:40:17 +0200
Subject: [PATCH 5/6] radv: allow DGC on the compute queue

DGC cmdbuf on ACE are executed as IB1 without chaining because IB2
isn't supported on ACE.

Signed-off-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 src/amd/vulkan/radv_cmd_buffer.c | 10 ++++------
 1 file changed, 4 insertions(+), 6 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index 1dee0d82ad23..684c466601da 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -9468,10 +9468,6 @@ radv_CmdExecuteGeneratedCommandsNV(VkCommandBuffer commandBuffer, VkBool32 isPre
    const bool compute = layout->pipeline_bind_point == VK_PIPELINE_BIND_POINT_COMPUTE;
    const struct radv_device *device = cmd_buffer->device;
 
-   /* The only actions that can be done are draws, so skip on other queues. */
-   if (cmd_buffer->qf != RADV_QUEUE_GENERAL)
-      return;
-
    /* Secondary command buffers are needed for the full extension but can't use
     * PKT3_INDIRECT_BUFFER.
     */
@@ -9503,8 +9499,10 @@ radv_CmdExecuteGeneratedCommandsNV(VkCommandBuffer commandBuffer, VkBool32 isPre
    const uint64_t ib_offset = prep_buffer->offset + pGeneratedCommandsInfo->preprocessOffset;
    const uint32_t view_mask = cmd_buffer->state.render.view_mask;
 
-   radeon_emit(cmd_buffer->cs, PKT3(PKT3_PFP_SYNC_ME, 0, cmd_buffer->state.predicating));
-   radeon_emit(cmd_buffer->cs, 0);
+   if (!radv_cmd_buffer_uses_mec(cmd_buffer)) {
+      radeon_emit(cmd_buffer->cs, PKT3(PKT3_PFP_SYNC_ME, 0, cmd_buffer->state.predicating));
+      radeon_emit(cmd_buffer->cs, 0);
+   }
 
    if (compute || !view_mask) {
       device->ws->cs_execute_ib(cmd_buffer->cs, ib_bo, ib_offset, cmdbuf_size >> 2);
-- 
GitLab


From c7be7da4a38073e85b8dde28f09ba3eee4e3bab2 Mon Sep 17 00:00:00 2001
From: Samuel Pitoiset <samuel.pitoiset@gmail.com>
Date: Tue, 20 Jun 2023 08:41:01 +0200
Subject: [PATCH 6/6] radv: advertise NV_device_generated_commands_compute

This extension introduces a token for implementing DGC compute, it's
only intended to be used by vkd3d-proton.

Signed-off-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 src/amd/vulkan/radv_physical_device.c | 6 ++++++
 1 file changed, 6 insertions(+)

diff --git a/src/amd/vulkan/radv_physical_device.c b/src/amd/vulkan/radv_physical_device.c
index 8e42c42a26aa..76e2764bf21a 100644
--- a/src/amd/vulkan/radv_physical_device.c
+++ b/src/amd/vulkan/radv_physical_device.c
@@ -577,6 +577,7 @@ radv_physical_device_get_supported_extensions(const struct radv_physical_device
       .INTEL_shader_integer_functions2 = true,
       .NV_compute_shader_derivatives = true,
       .NV_device_generated_commands = radv_NV_device_generated_commands_enabled(device),
+      .NV_device_generated_commands_compute = radv_NV_device_generated_commands_enabled(device),
       /* Undocumented extension purely for vkd3d-proton. This check is to prevent anyone else from
        * using it.
        */
@@ -1032,6 +1033,11 @@ radv_physical_device_get_features(const struct radv_physical_device *pdevice, st
 
       /* VK_KHR_maintenance5 */
       .maintenance5 = true,
+
+      /* VK_NV_device_generated_commands_compute */
+      .deviceGeneratedCompute = true,
+      .deviceGeneratedComputePipelines = false,
+      .deviceGeneratedComputeCaptureReplay = false,
    };
 }
 
-- 
GitLab

