From 52fd2e43fcdc677833b65977486b2d0028c7f8de Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Fri, 18 Jul 2025 10:33:32 +0100
Subject: [PATCH 01/12] aco: reduce cost of using values defined in
 predecessors

For code like:
   if (cond) {
      val = load()
   }
   use(val)
The "use(val)" now has a similar cost to a use inside the IF.

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_statistics.cpp | 74 +++++++++++++++++------------
 1 file changed, 44 insertions(+), 30 deletions(-)

diff --git a/src/amd/compiler/aco_statistics.cpp b/src/amd/compiler/aco_statistics.cpp
index 4100e5ee35922..6647003fefb4b 100644
--- a/src/amd/compiler/aco_statistics.cpp
+++ b/src/amd/compiler/aco_statistics.cpp
@@ -34,6 +34,7 @@ public:
    BlockCycleEstimator(Program* program_) : program(program_) {}
 
    Program* program;
+   Block* block;
 
    int32_t cur_cycle = 0;
    int32_t res_available[(int)BlockCycleEstimator::resource_count] = {0};
@@ -43,6 +44,7 @@ public:
 
    void add(aco_ptr<Instruction>& instr);
    void join(const BlockCycleEstimator& other);
+   double get_freq() const;
 
 private:
    unsigned get_waitcnt_cost(wait_imm imm);
@@ -459,24 +461,59 @@ BlockCycleEstimator::join(const BlockCycleEstimator& pred)
 {
    assert(cur_cycle == 0);
 
+   double mul = pred.get_freq() / get_freq();
+   mul = std::min(mul, 1.0);
+
    for (unsigned i = 0; i < (unsigned)resource_count; i++) {
       assert(res_usage[i] == 0);
-      res_available[i] = MAX2(res_available[i], pred.res_available[i] - pred.cur_cycle);
+      res_available[i] = MAX2(res_available[i], (pred.res_available[i] - pred.cur_cycle) * mul);
    }
 
    for (unsigned i = 0; i < 512; i++)
-      reg_available[i] = MAX2(reg_available[i], pred.reg_available[i] - pred.cur_cycle + cur_cycle);
+      reg_available[i] = MAX2(reg_available[i], (pred.reg_available[i] - pred.cur_cycle) * mul);
 
    for (unsigned i = 0; i < wait_type_num; i++) {
       std::deque<int32_t>& ops = mem_ops[i];
       const std::deque<int32_t>& pred_ops = pred.mem_ops[i];
       for (unsigned j = 0; j < MIN2(ops.size(), pred_ops.size()); j++)
-         ops.rbegin()[j] = MAX2(ops.rbegin()[j], pred_ops.rbegin()[j] - pred.cur_cycle);
+         ops.rbegin()[j] = MAX2(ops.rbegin()[j], (pred_ops.rbegin()[j] - pred.cur_cycle) * mul);
       for (int j = pred_ops.size() - ops.size() - 1; j >= 0; j--)
-         ops.push_front(pred_ops[j] - pred.cur_cycle);
+         ops.push_front((pred_ops[j] - pred.cur_cycle) * mul);
    }
 }
 
+double
+BlockCycleEstimator::get_freq() const
+{
+   /* TODO: it would be nice to be able to consider estimated loop trip
+    * counts used for loop unrolling.
+    */
+
+   /* TODO: estimate the trip_count of divergent loops (those which break
+    * divergent) higher than of uniform loops
+    */
+
+   /* Assume loops execute 8-2 times, uniform branches are taken 50% the time,
+    * and any lane in the wave takes a side of a divergent branch 75% of the
+    * time.
+    */
+   double iter = 1.0;
+   iter *= block->loop_nest_depth > 0 ? 8.0 : 1.0;
+   iter *= block->loop_nest_depth > 1 ? 4.0 : 1.0;
+   iter *= block->loop_nest_depth > 2 ? pow(2.0, block->loop_nest_depth - 2) : 1.0;
+   iter *= pow(0.5, block->uniform_if_depth);
+   iter *= pow(0.75, block->divergent_if_logical_depth);
+
+   bool divergent_if_linear_else =
+      block->logical_preds.empty() && block->linear_preds.size() == 1 &&
+      block->linear_succs.size() == 1 &&
+      program->blocks[block->linear_preds[0]].kind & (block_kind_branch | block_kind_invert);
+   if (divergent_if_linear_else)
+      iter *= 0.25;
+
+   return iter;
+}
+
 } /* end namespace */
 
 /* sgpr_presched/vgpr_presched */
@@ -543,6 +580,8 @@ collect_preasm_stats(Program* program)
    double latency = 0;
    double usage[(int)BlockCycleEstimator::resource_count] = {0};
    std::vector<BlockCycleEstimator> blocks(program->blocks.size(), program);
+   for (Block& block : program->blocks)
+      blocks[block.index].block = &block;
 
    constexpr const unsigned vmem_latency = 320;
    for (const Definition def : program->args_pending_vmem) {
@@ -562,32 +601,7 @@ collect_preasm_stats(Program* program)
          instr->pass_flags = block_est.cur_cycle - before;
       }
 
-      /* TODO: it would be nice to be able to consider estimated loop trip
-       * counts used for loop unrolling.
-       */
-
-      /* TODO: estimate the trip_count of divergent loops (those which break
-       * divergent) higher than of uniform loops
-       */
-
-      /* Assume loops execute 8-2 times, uniform branches are taken 50% the time,
-       * and any lane in the wave takes a side of a divergent branch 75% of the
-       * time.
-       */
-      double iter = 1.0;
-      iter *= block.loop_nest_depth > 0 ? 8.0 : 1.0;
-      iter *= block.loop_nest_depth > 1 ? 4.0 : 1.0;
-      iter *= block.loop_nest_depth > 2 ? pow(2.0, block.loop_nest_depth - 2) : 1.0;
-      iter *= pow(0.5, block.uniform_if_depth);
-      iter *= pow(0.75, block.divergent_if_logical_depth);
-
-      bool divergent_if_linear_else =
-         block.logical_preds.empty() && block.linear_preds.size() == 1 &&
-         block.linear_succs.size() == 1 &&
-         program->blocks[block.linear_preds[0]].kind & (block_kind_branch | block_kind_invert);
-      if (divergent_if_linear_else)
-         iter *= 0.25;
-
+      double iter = block_est.get_freq();
       latency += block_est.cur_cycle * iter;
       for (unsigned i = 0; i < (unsigned)BlockCycleEstimator::resource_count; i++)
          usage[i] += block_est.res_usage[i] * iter;
-- 
GitLab


From b05faa933e276c2707fc2e78b699e5dcefe9c5a5 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Tue, 22 Jul 2025 15:11:29 +0100
Subject: [PATCH 02/12] aco: add is_control_barrier helper

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_ir.cpp        | 27 +++++++++++++++++++++++++++
 src/amd/compiler/aco_ir.h          |  8 ++++++--
 src/amd/compiler/aco_scheduler.cpp | 24 ++----------------------
 3 files changed, 35 insertions(+), 24 deletions(-)

diff --git a/src/amd/compiler/aco_ir.cpp b/src/amd/compiler/aco_ir.cpp
index 772e0bc023c53..e6a11a7c0f10a 100644
--- a/src/amd/compiler/aco_ir.cpp
+++ b/src/amd/compiler/aco_ir.cpp
@@ -244,6 +244,33 @@ is_wait_export_ready(amd_gfx_level gfx_level, const Instruction* instr)
                               : !(instr->salu().imm & wait_event_imm_dont_wait_export_ready_gfx11));
 }
 
+static bool
+is_done_sendmsg(amd_gfx_level gfx_level, const Instruction* instr)
+{
+   if (gfx_level <= GFX10_3 && instr->opcode == aco_opcode::s_sendmsg)
+      return (instr->salu().imm & sendmsg_id_mask) == sendmsg_gs_done;
+   return false;
+}
+
+static bool
+is_pos_prim_export(amd_gfx_level gfx_level, const Instruction* instr)
+{
+   /* Because of NO_PC_EXPORT=1, a done=1 position or primitive export can launch PS waves before
+    * the NGG/VS wave finishes if there are no parameter exports.
+    */
+   return instr->opcode == aco_opcode::exp && instr->exp().dest >= V_008DFC_SQ_EXP_POS &&
+          instr->exp().dest <= V_008DFC_SQ_EXP_PRIM && gfx_level >= GFX10;
+}
+
+bool
+is_control_barrier(amd_gfx_level gfx_level, const Instruction* instr, unsigned semantic)
+{
+   bool is_release = semantic & semantic_release;
+   if (is_release && (is_done_sendmsg(gfx_level, instr) || is_pos_prim_export(gfx_level, instr)))
+      return true;
+   return (instr->isBarrier() && instr->barrier().exec_scope > scope_invocation);
+}
+
 memory_sync_info
 get_sync_info(const Instruction* instr)
 {
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index 90e1867c3a2d9..9a719f0a2e09b 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -64,11 +64,11 @@ enum memory_semantics : uint8_t {
    semantic_none = 0x0,
    /* for loads: don't move any access after this load to before this load (even other loads)
     * for barriers: don't move any access after the barrier to before any
-    * atomics/control_barriers/sendmsg_gs_done/position-primitive-export before the barrier */
+    * atomic_loads/control_barriers before the barrier */
    semantic_acquire = 0x1,
    /* for stores: don't move any access before this store to after this store
     * for barriers: don't move any access before the barrier to after any
-    * atomics/control_barriers/sendmsg_gs_done/position-primitive-export after the barrier */
+    * atomic_stores/control_barriers/sendmsg_gs_done/position-primitive-export after the barrier */
    semantic_release = 0x2,
 
    /* the rest are for load/stores/atomics only */
@@ -1875,6 +1875,10 @@ is_phi(aco_ptr<Instruction>& instr)
 }
 
 bool is_wait_export_ready(amd_gfx_level gfx_level, const Instruction* instr);
+
+/* Returns whether this is a control barrier for the purposes of acquire/release barriers. */
+bool is_control_barrier(amd_gfx_level gfx_level, const Instruction* instr, unsigned semantic);
+
 memory_sync_info get_sync_info(const Instruction* instr);
 
 inline bool
diff --git a/src/amd/compiler/aco_scheduler.cpp b/src/amd/compiler/aco_scheduler.cpp
index 52436cd9e2ddb..2fdfb263ac0c1 100644
--- a/src/amd/compiler/aco_scheduler.cpp
+++ b/src/amd/compiler/aco_scheduler.cpp
@@ -418,24 +418,6 @@ MoveState::upwards_skip(UpwardsCursor& cursor)
    cursor.verify_invariants(block);
 }
 
-bool
-is_done_sendmsg(amd_gfx_level gfx_level, const Instruction* instr)
-{
-   if (gfx_level <= GFX10_3 && instr->opcode == aco_opcode::s_sendmsg)
-      return (instr->salu().imm & sendmsg_id_mask) == sendmsg_gs_done;
-   return false;
-}
-
-bool
-is_pos_prim_export(amd_gfx_level gfx_level, const Instruction* instr)
-{
-   /* Because of NO_PC_EXPORT=1, a done=1 position or primitive export can launch PS waves before
-    * the NGG/VS wave finishes if there are no parameter exports.
-    */
-   return instr->opcode == aco_opcode::exp && instr->exp().dest >= V_008DFC_SQ_EXP_POS &&
-          instr->exp().dest <= V_008DFC_SQ_EXP_PRIM && gfx_level >= GFX10;
-}
-
 memory_sync_info
 get_sync_info_with_hack(const Instruction* instr)
 {
@@ -490,8 +472,8 @@ void
 add_memory_event(amd_gfx_level gfx_level, memory_event_set* set, Instruction* instr,
                  memory_sync_info* sync)
 {
-   set->has_control_barrier |= is_done_sendmsg(gfx_level, instr);
-   set->has_control_barrier |= is_pos_prim_export(gfx_level, instr);
+   set->has_control_barrier |=
+      is_control_barrier(gfx_level, instr, semantic_acquire | semantic_release);
    if (instr->opcode == aco_opcode::p_barrier) {
       Pseudo_barrier_instruction& bar = instr->barrier();
       if (bar.sync.semantics & semantic_acquire)
@@ -499,8 +481,6 @@ add_memory_event(amd_gfx_level gfx_level, memory_event_set* set, Instruction* in
       if (bar.sync.semantics & semantic_release)
          set->bar_release |= bar.sync.storage;
       set->bar_classes |= bar.sync.storage;
-
-      set->has_control_barrier |= bar.exec_scope > scope_invocation;
    }
 
    if (!sync->storage)
-- 
GitLab


From ac892c2d50ea969a80d7f9285c60634e4d41c4fb Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Fri, 1 Aug 2025 10:01:25 +0100
Subject: [PATCH 03/12] aco: don't move release barriers after interlock end

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_ir.cpp        | 26 +++++++++++++++++++++++---
 src/amd/compiler/aco_ir.h          |  7 +++++--
 src/amd/compiler/aco_scheduler.cpp | 12 ++++++++----
 3 files changed, 36 insertions(+), 9 deletions(-)

diff --git a/src/amd/compiler/aco_ir.cpp b/src/amd/compiler/aco_ir.cpp
index e6a11a7c0f10a..d86f62af966a7 100644
--- a/src/amd/compiler/aco_ir.cpp
+++ b/src/amd/compiler/aco_ir.cpp
@@ -262,12 +262,32 @@ is_pos_prim_export(amd_gfx_level gfx_level, const Instruction* instr)
           instr->exp().dest <= V_008DFC_SQ_EXP_PRIM && gfx_level >= GFX10;
 }
 
+static bool
+is_pops_end_export(Program* program, const Instruction* instr)
+{
+   return instr->opcode == aco_opcode::exp && instr->exp().dest <= V_008DFC_SQ_EXP_NULL &&
+          program->has_pops_overlapped_waves_wait && program->gfx_level >= GFX11;
+}
+
+static bool
+is_ordered_ps_done_sendmsg(const Instruction* instr)
+{
+   return instr->opcode == aco_opcode::s_sendmsg &&
+          (instr->salu().imm & sendmsg_id_mask) == sendmsg_ordered_ps_done;
+}
+
 bool
-is_control_barrier(amd_gfx_level gfx_level, const Instruction* instr, unsigned semantic)
+is_control_barrier(Program* program, const Instruction* instr, unsigned semantic)
 {
    bool is_release = semantic & semantic_release;
-   if (is_release && (is_done_sendmsg(gfx_level, instr) || is_pos_prim_export(gfx_level, instr)))
-      return true;
+   if (is_release) {
+      if (is_done_sendmsg(program->gfx_level, instr) || is_pos_prim_export(program->gfx_level, instr))
+         return true;
+
+      if (is_pops_end_export(program, instr) || is_ordered_ps_done_sendmsg(instr) ||
+          instr->opcode == aco_opcode::p_pops_gfx9_ordered_section_done)
+         return true;
+   }
    return (instr->isBarrier() && instr->barrier().exec_scope > scope_invocation);
 }
 
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index 9a719f0a2e09b..783604a91715a 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -68,7 +68,8 @@ enum memory_semantics : uint8_t {
    semantic_acquire = 0x1,
    /* for stores: don't move any access before this store to after this store
     * for barriers: don't move any access before the barrier to after any
-    * atomic_stores/control_barriers/sendmsg_gs_done/position-primitive-export after the barrier */
+    * atomic_stores/control_barriers/p_pops_gfx9_ordered_section_done or
+    * certain sendmsg/exports after the barrier */
    semantic_release = 0x2,
 
    /* the rest are for load/stores/atomics only */
@@ -1876,8 +1877,10 @@ is_phi(aco_ptr<Instruction>& instr)
 
 bool is_wait_export_ready(amd_gfx_level gfx_level, const Instruction* instr);
 
+class Program;
+
 /* Returns whether this is a control barrier for the purposes of acquire/release barriers. */
-bool is_control_barrier(amd_gfx_level gfx_level, const Instruction* instr, unsigned semantic);
+bool is_control_barrier(Program* program, const Instruction* instr, unsigned semantic);
 
 memory_sync_info get_sync_info(const Instruction* instr);
 
diff --git a/src/amd/compiler/aco_scheduler.cpp b/src/amd/compiler/aco_scheduler.cpp
index 2fdfb263ac0c1..5642599278f9a 100644
--- a/src/amd/compiler/aco_scheduler.cpp
+++ b/src/amd/compiler/aco_scheduler.cpp
@@ -115,6 +115,7 @@ struct MoveState {
 
 struct sched_ctx {
    amd_gfx_level gfx_level;
+   Program* program;
    int16_t occupancy_factor;
    int16_t last_SMEM_stall;
    int last_SMEM_dep_idx;
@@ -445,6 +446,7 @@ struct memory_event_set {
 };
 
 struct hazard_query {
+   Program* program;
    amd_gfx_level gfx_level;
    bool contains_spill;
    bool contains_sendmsg;
@@ -458,6 +460,7 @@ struct hazard_query {
 void
 init_hazard_query(const sched_ctx& ctx, hazard_query* query)
 {
+   query->program = ctx.program;
    query->gfx_level = ctx.gfx_level;
    query->contains_spill = false;
    query->contains_sendmsg = false;
@@ -469,11 +472,11 @@ init_hazard_query(const sched_ctx& ctx, hazard_query* query)
 }
 
 void
-add_memory_event(amd_gfx_level gfx_level, memory_event_set* set, Instruction* instr,
+add_memory_event(Program* program, memory_event_set* set, Instruction* instr,
                  memory_sync_info* sync)
 {
    set->has_control_barrier |=
-      is_control_barrier(gfx_level, instr, semantic_acquire | semantic_release);
+      is_control_barrier(program, instr, semantic_acquire | semantic_release);
    if (instr->opcode == aco_opcode::p_barrier) {
       Pseudo_barrier_instruction& bar = instr->barrier();
       if (bar.sync.semantics & semantic_acquire)
@@ -513,7 +516,7 @@ add_to_hazard_query(hazard_query* query, Instruction* instr)
 
    memory_sync_info sync = get_sync_info_with_hack(instr);
 
-   add_memory_event(query->gfx_level, &query->mem_events, instr, &sync);
+   add_memory_event(query->program, &query->mem_events, instr, &sync);
 
    if (!(sync.semantics & semantic_can_reorder)) {
       unsigned storage = sync.storage;
@@ -599,7 +602,7 @@ perform_hazard_query(hazard_query* query, Instruction* instr, bool upwards)
    memory_event_set instr_set;
    memset(&instr_set, 0, sizeof(instr_set));
    memory_sync_info sync = get_sync_info_with_hack(instr);
-   add_memory_event(query->gfx_level, &instr_set, instr, &sync);
+   add_memory_event(query->program, &instr_set, instr, &sync);
 
    memory_event_set* first = &instr_set;
    memory_event_set* second = &query->mem_events;
@@ -1231,6 +1234,7 @@ schedule_program(Program* program)
 
    sched_ctx ctx;
    ctx.gfx_level = program->gfx_level;
+   ctx.program = program;
    ctx.mv.depends_on.resize(program->peekAllocationId());
    ctx.mv.RAR_dependencies.resize(program->peekAllocationId());
    ctx.mv.RAR_dependencies_clause.resize(program->peekAllocationId());
-- 
GitLab


From c221199256bc711b49ecef9a247120823768e853 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Fri, 1 Aug 2025 11:09:08 +0100
Subject: [PATCH 04/12] aco: don't move acquire barriers before interlock begin

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_ir.cpp | 6 ++++++
 src/amd/compiler/aco_ir.h   | 3 ++-
 2 files changed, 8 insertions(+), 1 deletion(-)

diff --git a/src/amd/compiler/aco_ir.cpp b/src/amd/compiler/aco_ir.cpp
index d86f62af966a7..5ddc2d093eb4a 100644
--- a/src/amd/compiler/aco_ir.cpp
+++ b/src/amd/compiler/aco_ir.cpp
@@ -279,7 +279,13 @@ is_ordered_ps_done_sendmsg(const Instruction* instr)
 bool
 is_control_barrier(Program* program, const Instruction* instr, unsigned semantic)
 {
+   bool is_acquire = semantic & semantic_acquire;
    bool is_release = semantic & semantic_release;
+   if (is_acquire) {
+      if (is_wait_export_ready(program->gfx_level, instr) ||
+          instr->opcode == aco_opcode::p_pops_gfx9_add_exiting_wave_id)
+         return true;
+   }
    if (is_release) {
       if (is_done_sendmsg(program->gfx_level, instr) || is_pos_prim_export(program->gfx_level, instr))
          return true;
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index 783604a91715a..4b432f917b819 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -64,7 +64,8 @@ enum memory_semantics : uint8_t {
    semantic_none = 0x0,
    /* for loads: don't move any access after this load to before this load (even other loads)
     * for barriers: don't move any access after the barrier to before any
-    * atomic_loads/control_barriers before the barrier */
+    * atomic_loads/control_barriers/p_pops_gfx9_add_exiting_wave_id or
+    * certain s_wait_event before the barrier */
    semantic_acquire = 0x1,
    /* for stores: don't move any access before this store to after this store
     * for barriers: don't move any access before the barrier to after any
-- 
GitLab


From 3cf473cc5b5cf3629579d07d1bbf8f1a0a8b05c3 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Thu, 17 Jul 2025 12:29:41 +0100
Subject: [PATCH 05/12] aco: refactor waitcnt pass to use barrier_info

Currently there's just barrier_info_all, but more will be added later.

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_insert_waitcnt.cpp | 162 +++++++++++++++++-------
 1 file changed, 114 insertions(+), 48 deletions(-)

diff --git a/src/amd/compiler/aco_insert_waitcnt.cpp b/src/amd/compiler/aco_insert_waitcnt.cpp
index 1fb30571b8658..90d27c5be43c7 100644
--- a/src/amd/compiler/aco_insert_waitcnt.cpp
+++ b/src/amd/compiler/aco_insert_waitcnt.cpp
@@ -166,6 +166,41 @@ private:
    uint8_t counters[num_events] = {};
 };
 
+enum barrier_info_kind {
+   /* Waits for all non-private accesses and all scratch/vgpr-spill accesses */
+   barrier_info_all,
+   num_barrier_infos,
+};
+
+/* Used to keep track of wait imms that are yet to be emitted. */
+struct barrier_info {
+   wait_imm imm[storage_count];
+   uint16_t events[storage_count] = {}; /* use wait_event notion */
+   uint8_t storage = 0;
+
+   bool join(const barrier_info& other)
+   {
+      bool changed = false;
+      for (unsigned i = 0; i < storage_count; i++) {
+         changed |= imm[i].combine(other.imm[i]);
+         changed |= (other.events[i] & ~events[i]) != 0;
+         events[i] |= other.events[i];
+      }
+      storage |= other.storage;
+      return changed;
+   }
+
+   UNUSED void print(FILE* output) const
+   {
+      u_foreach_bit (i, storage) {
+         fprintf(output, "storage[%u] = {\n", i);
+         imm[i].print(output);
+         fprintf(output, "events: %u\n", events[i]);
+         fprintf(output, "}\n");
+      }
+   }
+};
+
 struct wait_ctx {
    Program* program;
    enum amd_gfx_level gfx_level;
@@ -176,8 +211,8 @@ struct wait_ctx {
    bool pending_flat_vm = false;
    bool pending_s_buffer_store = false; /* GFX10 workaround */
 
-   wait_imm barrier_imm[storage_count];
-   uint16_t barrier_events[storage_count] = {}; /* use wait_event notion */
+   barrier_info bar[num_barrier_infos];
+   uint8_t bar_nonempty = 0;
 
    std::map<PhysReg, wait_entry> gpr_map;
 
@@ -219,11 +254,9 @@ struct wait_ctx {
             }
          }
 
-         for (unsigned i = 0; i < storage_count; i++) {
-            changed |= barrier_imm[i].combine(other->barrier_imm[i]);
-            changed |= (other->barrier_events[i] & ~barrier_events[i]) != 0;
-            barrier_events[i] |= other->barrier_events[i];
-         }
+         u_foreach_bit (i, other->bar_nonempty)
+            changed |= bar[i].join(other->bar[i]);
+         bar_nonempty |= other->bar_nonempty;
       }
 
       return changed;
@@ -242,13 +275,10 @@ struct wait_ctx {
          fprintf(output, "}\n");
       }
 
-      for (unsigned i = 0; i < storage_count; i++) {
-         if (!barrier_imm[i].empty() || barrier_events[i]) {
-            fprintf(output, "barriers[%u] = {\n", i);
-            barrier_imm[i].print(output);
-            fprintf(output, "events: %u\n", barrier_events[i]);
-            fprintf(output, "}\n");
-         }
+      u_foreach_bit (i, bar_nonempty) {
+         fprintf(output, "barriers[%u] = {\n", i);
+         bar[i].print(output);
+         fprintf(output, "}\n");
       }
    }
 };
@@ -402,24 +432,22 @@ perform_barrier(wait_ctx& ctx, wait_imm& imm, memory_sync_info sync, unsigned se
    sync_scope subgroup_scope =
       ctx.program->workgroup_size <= ctx.program->wave_size ? scope_workgroup : scope_subgroup;
    if ((sync.semantics & semantics) && sync.scope > subgroup_scope) {
-      unsigned storage = sync.storage;
-      while (storage) {
-         unsigned idx = u_bit_scan(&storage);
+      barrier_info& bar = ctx.bar[barrier_info_all];
 
-         /* LDS is private to the workgroup */
-         sync_scope bar_scope_lds = MIN2(sync.scope, scope_workgroup);
+      u_foreach_bit (i, sync.storage & bar.storage) {
+         uint16_t events = bar.events[i];
 
-         uint16_t events = ctx.barrier_events[idx];
-         if (bar_scope_lds <= subgroup_scope)
+         /* LDS is private to the workgroup */
+         if (MIN2(sync.scope, scope_workgroup) <= subgroup_scope)
             events &= ~event_lds;
 
          /* Until GFX11, in non-WGP, the L1 (L0 on GFX10+) cache keeps all memory operations
           * in-order for the same workgroup */
          if (ctx.gfx_level < GFX11 && !ctx.program->wgp_mode && sync.scope <= scope_workgroup)
-            events &= ~(event_vmem | event_vmem_store | event_smem);
+            events &= ~(event_vmem | event_vmem_store);
 
          if (events)
-            imm.combine(ctx.barrier_imm[idx]);
+            imm.combine(bar.imm[i]);
       }
    }
 }
@@ -431,6 +459,31 @@ force_waitcnt(wait_ctx& ctx, wait_imm& imm)
       imm[i] = 0;
 }
 
+void
+update_barrier_info_for_wait(wait_ctx& ctx, unsigned idx, wait_imm imm)
+{
+   barrier_info& info = ctx.bar[idx];
+   for (unsigned i = 0; i < wait_type_num; i++) {
+      if (imm[i] == wait_imm::unset_counter)
+         continue;
+
+      u_foreach_bit (j, info.storage) {
+         wait_imm& bar = info.imm[j];
+         if (bar[i] != wait_imm::unset_counter && imm[i] <= bar[i]) {
+            /* Clear this counter */
+            bar[i] = wait_imm::unset_counter;
+            info.events[j] &= ~ctx.info->events[i];
+
+            if (!info.events[j]) {
+               info.storage &= ~(1 << j);
+               if (!info.storage)
+                  ctx.bar_nonempty &= ~(1 << idx);
+            }
+         }
+      }
+   }
+}
+
 void
 kill(wait_imm& imm, Instruction* instr, wait_ctx& ctx, memory_sync_info sync_info)
 {
@@ -447,8 +500,9 @@ kill(wait_imm& imm, Instruction* instr, wait_ctx& ctx, memory_sync_info sync_inf
     */
    if (ctx.gfx_level >= GFX11 && instr->opcode == aco_opcode::s_sendmsg &&
        instr->salu().imm == sendmsg_dealloc_vgprs) {
-      imm.combine(ctx.barrier_imm[ffs(storage_scratch) - 1]);
-      imm.combine(ctx.barrier_imm[ffs(storage_vgpr_spill) - 1]);
+      barrier_info& bar = ctx.bar[barrier_info_all];
+      imm.combine(bar.imm[ffs(storage_scratch) - 1]);
+      imm.combine(bar.imm[ffs(storage_vgpr_spill) - 1]);
    }
 
    /* Make sure POPS coherent memory accesses have reached the L2 cache before letting the
@@ -495,7 +549,8 @@ kill(wait_imm& imm, Instruction* instr, wait_ctx& ctx, memory_sync_info sync_inf
 
    if (instr->opcode == aco_opcode::ds_ordered_count &&
        ((instr->ds().offset1 | (instr->ds().offset0 >> 8)) & 0x1)) {
-      imm.combine(ctx.barrier_imm[ffs(storage_gds) - 1]);
+      barrier_info& bar = ctx.bar[barrier_info_all];
+      imm.combine(bar.imm[ffs(storage_gds) - 1]);
    }
 
    if (instr->opcode == aco_opcode::p_barrier)
@@ -513,17 +568,8 @@ kill(wait_imm& imm, Instruction* instr, wait_ctx& ctx, memory_sync_info sync_inf
       for (unsigned i = 0; i < wait_type_num; i++)
          ctx.nonzero &= imm[i] == 0 ? ~BITFIELD_BIT(i) : UINT32_MAX;
 
-      /* update barrier wait imms */
-      for (unsigned i = 0; i < storage_count; i++) {
-         wait_imm& bar = ctx.barrier_imm[i];
-         uint16_t& bar_ev = ctx.barrier_events[i];
-         for (unsigned j = 0; j < wait_type_num; j++) {
-            if (bar[j] != wait_imm::unset_counter && imm[j] <= bar[j]) {
-               bar[j] = wait_imm::unset_counter;
-               bar_ev &= ~ctx.info->events[j];
-            }
-         }
-      }
+      u_foreach_bit (i, ctx.bar_nonempty)
+         update_barrier_info_for_wait(ctx, i, imm);
 
       /* remove all gprs with higher counter from map */
       std::map<PhysReg, wait_entry>::iterator it = ctx.gpr_map.begin();
@@ -548,20 +594,28 @@ kill(wait_imm& imm, Instruction* instr, wait_ctx& ctx, memory_sync_info sync_inf
 }
 
 void
-update_barrier_imm(wait_ctx& ctx, uint8_t counters, wait_event event, memory_sync_info sync)
+update_barrier_info_for_event(wait_ctx& ctx, uint8_t counters, wait_event event,
+                              barrier_info_kind idx, uint16_t storage)
 {
-   for (unsigned i = 0; i < storage_count; i++) {
-      wait_imm& bar = ctx.barrier_imm[i];
-      uint16_t& bar_ev = ctx.barrier_events[i];
+   barrier_info& info = ctx.bar[idx];
+   if (storage) {
+      info.storage |= storage;
+      ctx.bar_nonempty |= 1 << idx;
+   }
 
-      /* We re-use barrier_imm/barrier_events to wait for all scratch stores to finish. */
-      bool ignore_private = i == (ffs(storage_scratch) - 1) || i == (ffs(storage_vgpr_spill) - 1);
+   unsigned storage_tmp = info.storage;
+   while (storage_tmp) {
+      unsigned i = u_bit_scan(&storage_tmp);
+      wait_imm& bar = info.imm[i];
+      uint16_t& bar_ev = info.events[i];
 
-      if (sync.storage & (1 << i) && (!(sync.semantics & semantic_private) || ignore_private)) {
+      if (storage & (1 << i)) {
+         /* Reset counters to zero so that this instruction is waited on. */
          bar_ev |= event;
          u_foreach_bit (j, counters)
             bar[j] = 0;
       } else if (!(bar_ev & ctx.info->unordered_events) && !(ctx.info->unordered_events & event)) {
+         /* Increase counters so that this instruction is ignored when waiting. */
          u_foreach_bit (j, counters) {
             if (bar[j] != wait_imm::unset_counter && (bar_ev & ctx.info->events[j]) == event)
                bar[j] = std::min<uint16_t>(bar[j] + 1, ctx.info->max_cnt[j]);
@@ -570,6 +624,18 @@ update_barrier_imm(wait_ctx& ctx, uint8_t counters, wait_event event, memory_syn
    }
 }
 
+/* This resets or increases the counters for the barrier infos in response to an instruction. */
+void
+update_barriers(wait_ctx& ctx, uint8_t counters, wait_event event, memory_sync_info sync)
+{
+   uint16_t storage_all = sync.storage;
+   /* We re-use barrier_info_all to wait for all scratch stores to finish, to track those even if
+    * they are private. */
+   if (sync.semantics & semantic_private)
+      storage_all &= storage_scratch | storage_vgpr_spill;
+   update_barrier_info_for_event(ctx, counters, event, barrier_info_all, storage_all);
+}
+
 void
 update_counters(wait_ctx& ctx, wait_event event, memory_sync_info sync = memory_sync_info())
 {
@@ -577,7 +643,7 @@ update_counters(wait_ctx& ctx, wait_event event, memory_sync_info sync = memory_
 
    ctx.nonzero |= counters;
 
-   update_barrier_imm(ctx, counters, event, sync);
+   update_barriers(ctx, counters, event, sync);
 
    if (ctx.info->unordered_events & event)
       return;
@@ -837,7 +903,7 @@ handle_block(Program* program, Block& block, wait_ctx& ctx)
          perform_barrier(ctx, queued_imm, sync_info, semantic_acquire);
 
          if (is_ordered_count_acquire)
-            queued_imm.combine(ctx.barrier_imm[ffs(storage_gds) - 1]);
+            queued_imm.combine(ctx.bar[barrier_info_all].imm[ffs(storage_gds) - 1]);
       }
    }
 
@@ -869,8 +935,8 @@ insert_waitcnt(Program* program)
    unsigned loop_progress = 0;
 
    if (program->pending_lds_access) {
-      update_barrier_imm(in_ctx[0], info.get_counters_for_event(event_lds), event_lds,
-                         memory_sync_info(storage_shared));
+      update_barriers(in_ctx[0], info.get_counters_for_event(event_lds), event_lds,
+                      memory_sync_info(storage_shared));
    }
 
    for (Definition def : program->args_pending_vmem) {
-- 
GitLab


From ae20218a6fb7faf13c56f646cd56b5b0dd4e8b69 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Tue, 22 Jul 2025 16:36:22 +0100
Subject: [PATCH 06/12] aco: add a separate barrier_info for release/acquire
 barriers

These can wait for different sets of accesses.

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_insert_waitcnt.cpp | 127 +++++++++++++++---------
 1 file changed, 81 insertions(+), 46 deletions(-)

diff --git a/src/amd/compiler/aco_insert_waitcnt.cpp b/src/amd/compiler/aco_insert_waitcnt.cpp
index 90d27c5be43c7..64cc2df7969a2 100644
--- a/src/amd/compiler/aco_insert_waitcnt.cpp
+++ b/src/amd/compiler/aco_insert_waitcnt.cpp
@@ -168,7 +168,9 @@ private:
 
 enum barrier_info_kind {
    /* Waits for all non-private accesses and all scratch/vgpr-spill accesses */
-   barrier_info_all,
+   barrier_info_release_dep,
+   /* Waits for all atomics */
+   barrier_info_acquire_dep,
    num_barrier_infos,
 };
 
@@ -426,29 +428,50 @@ check_instr(wait_ctx& ctx, wait_imm& wait, Instruction* instr)
    }
 }
 
+uint8_t
+is_atomic_or_control_instr(Program* program, Instruction* instr, memory_sync_info sync,
+                           bool is_acquire)
+{
+   bool is_atomic = sync.semantics & semantic_atomic;
+   // TODO: NIR doesn't have any atomic load/store, so we assume any load/store is atomic
+   is_atomic |= !(sync.semantics & semantic_private) && sync.storage;
+   if (is_atomic) {
+      bool is_load = !instr->definitions.empty() || (sync.semantics & semantic_rmw);
+      bool is_store = instr->definitions.empty() || (sync.semantics & semantic_rmw);
+      bool is_release = !is_acquire;
+      return ((is_release && is_store) || (is_acquire && is_load)) ? sync.storage : 0;
+   }
+
+   if (is_control_barrier(program, instr, is_acquire ? semantic_acquire : semantic_release))
+      return BITFIELD_MASK(storage_count);
+
+   return 0;
+}
+
 void
-perform_barrier(wait_ctx& ctx, wait_imm& imm, memory_sync_info sync, unsigned semantics)
+perform_barrier(wait_ctx& ctx, wait_imm& imm, memory_sync_info sync, bool is_acquire)
 {
    sync_scope subgroup_scope =
       ctx.program->workgroup_size <= ctx.program->wave_size ? scope_workgroup : scope_subgroup;
-   if ((sync.semantics & semantics) && sync.scope > subgroup_scope) {
-      barrier_info& bar = ctx.bar[barrier_info_all];
+   if (sync.scope <= subgroup_scope)
+      return;
 
-      u_foreach_bit (i, sync.storage & bar.storage) {
-         uint16_t events = bar.events[i];
+   barrier_info& bar = ctx.bar[is_acquire ? barrier_info_acquire_dep : barrier_info_release_dep];
 
-         /* LDS is private to the workgroup */
-         if (MIN2(sync.scope, scope_workgroup) <= subgroup_scope)
-            events &= ~event_lds;
+   u_foreach_bit (i, sync.storage & bar.storage) {
+      uint16_t events = bar.events[i];
 
-         /* Until GFX11, in non-WGP, the L1 (L0 on GFX10+) cache keeps all memory operations
-          * in-order for the same workgroup */
-         if (ctx.gfx_level < GFX11 && !ctx.program->wgp_mode && sync.scope <= scope_workgroup)
-            events &= ~(event_vmem | event_vmem_store);
+      /* LDS is private to the workgroup */
+      if (MIN2(sync.scope, scope_workgroup) <= subgroup_scope)
+         events &= ~event_lds;
 
-         if (events)
-            imm.combine(bar.imm[i]);
-      }
+      /* Until GFX11, in non-WGP, the L1 (L0 on GFX10+) cache keeps all memory operations
+       * in-order for the same workgroup */
+      if (ctx.gfx_level < GFX11 && !ctx.program->wgp_mode && sync.scope <= scope_workgroup)
+         events &= ~(event_vmem | event_vmem_store | event_smem);
+
+      if (events)
+         imm.combine(bar.imm[i]);
    }
 }
 
@@ -500,7 +523,7 @@ kill(wait_imm& imm, Instruction* instr, wait_ctx& ctx, memory_sync_info sync_inf
     */
    if (ctx.gfx_level >= GFX11 && instr->opcode == aco_opcode::s_sendmsg &&
        instr->salu().imm == sendmsg_dealloc_vgprs) {
-      barrier_info& bar = ctx.bar[barrier_info_all];
+      barrier_info& bar = ctx.bar[barrier_info_release_dep];
       imm.combine(bar.imm[ffs(storage_scratch) - 1]);
       imm.combine(bar.imm[ffs(storage_vgpr_spill) - 1]);
    }
@@ -549,14 +572,18 @@ kill(wait_imm& imm, Instruction* instr, wait_ctx& ctx, memory_sync_info sync_inf
 
    if (instr->opcode == aco_opcode::ds_ordered_count &&
        ((instr->ds().offset1 | (instr->ds().offset0 >> 8)) & 0x1)) {
-      barrier_info& bar = ctx.bar[barrier_info_all];
+      barrier_info& bar = ctx.bar[barrier_info_release_dep];
       imm.combine(bar.imm[ffs(storage_gds) - 1]);
    }
 
-   if (instr->opcode == aco_opcode::p_barrier)
-      perform_barrier(ctx, imm, instr->barrier().sync, semantic_acqrel);
-   else
-      perform_barrier(ctx, imm, sync_info, semantic_release);
+   if (instr->opcode == aco_opcode::p_barrier) {
+      if (instr->barrier().sync.semantics & semantic_release)
+         perform_barrier(ctx, imm, instr->barrier().sync, false);
+      if (instr->barrier().sync.semantics & semantic_acquire)
+         perform_barrier(ctx, imm, instr->barrier().sync, true);
+   } else if (sync_info.semantics & semantic_release) {
+      perform_barrier(ctx, imm, sync_info, false);
+   }
 
    if (!imm.empty()) {
       if (ctx.pending_flat_vm && imm.vm != wait_imm::unset_counter)
@@ -626,24 +653,31 @@ update_barrier_info_for_event(wait_ctx& ctx, uint8_t counters, wait_event event,
 
 /* This resets or increases the counters for the barrier infos in response to an instruction. */
 void
-update_barriers(wait_ctx& ctx, uint8_t counters, wait_event event, memory_sync_info sync)
+update_barriers(wait_ctx& ctx, uint8_t counters, wait_event event, Instruction* instr,
+                memory_sync_info sync)
 {
-   uint16_t storage_all = sync.storage;
-   /* We re-use barrier_info_all to wait for all scratch stores to finish, to track those even if
-    * they are private. */
+   uint16_t storage_rel = sync.storage;
+   /* We re-use barrier_info_release_dep to wait for all scratch stores to finish, so track those
+    * even if they are private. */
    if (sync.semantics & semantic_private)
-      storage_all &= storage_scratch | storage_vgpr_spill;
-   update_barrier_info_for_event(ctx, counters, event, barrier_info_all, storage_all);
+      storage_rel &= storage_scratch | storage_vgpr_spill;
+   update_barrier_info_for_event(ctx, counters, event, barrier_info_release_dep, storage_rel);
+
+   if (instr) {
+      uint16_t storage_acq = is_atomic_or_control_instr(ctx.program, instr, sync, true);
+      update_barrier_info_for_event(ctx, counters, event, barrier_info_acquire_dep, storage_acq);
+   }
 }
 
 void
-update_counters(wait_ctx& ctx, wait_event event, memory_sync_info sync = memory_sync_info())
+update_counters(wait_ctx& ctx, wait_event event, Instruction* instr,
+                memory_sync_info sync = memory_sync_info())
 {
    uint8_t counters = ctx.info->get_counters_for_event(event);
 
    ctx.nonzero |= counters;
 
-   update_barriers(ctx, counters, event, sync);
+   update_barriers(ctx, counters, event, instr, sync);
 
    if (ctx.info->unordered_events & event)
       return;
@@ -715,7 +749,7 @@ gen(Instruction* instr, wait_ctx& ctx)
          ev = event_exp_pos;
       else
          ev = event_exp_param;
-      update_counters(ctx, ev);
+      update_counters(ctx, ev, instr);
 
       /* insert new entries for exported vgprs */
       for (unsigned i = 0; i < 4; i++) {
@@ -731,8 +765,8 @@ gen(Instruction* instr, wait_ctx& ctx)
    case Format::FLAT: {
       FLAT_instruction& flat = instr->flat();
       wait_event vmem_ev = get_vmem_event(ctx, instr, vmem_nosampler);
-      update_counters(ctx, vmem_ev, flat.sync);
-      update_counters(ctx, event_lds, flat.sync);
+      update_counters(ctx, vmem_ev, instr, flat.sync);
+      update_counters(ctx, event_lds, instr, flat.sync);
 
       if (!instr->definitions.empty())
          insert_wait_entry(ctx, instr->definitions[0], vmem_ev, 0, get_vmem_mask(ctx, instr));
@@ -747,7 +781,7 @@ gen(Instruction* instr, wait_ctx& ctx)
    }
    case Format::SMEM: {
       SMEM_instruction& smem = instr->smem();
-      update_counters(ctx, event_smem, smem.sync);
+      update_counters(ctx, event_smem, instr, smem.sync);
 
       if (!instr->definitions.empty())
          insert_wait_entry(ctx, instr->definitions[0], event_smem);
@@ -758,9 +792,9 @@ gen(Instruction* instr, wait_ctx& ctx)
    }
    case Format::DS: {
       DS_instruction& ds = instr->ds();
-      update_counters(ctx, ds.gds ? event_gds : event_lds, ds.sync);
+      update_counters(ctx, ds.gds ? event_gds : event_lds, instr, ds.sync);
       if (ds.gds)
-         update_counters(ctx, event_gds_gpr_lock);
+         update_counters(ctx, event_gds_gpr_lock, instr);
 
       for (auto& definition : instr->definitions)
          insert_wait_entry(ctx, definition, ds.gds ? event_gds : event_lds);
@@ -774,7 +808,7 @@ gen(Instruction* instr, wait_ctx& ctx)
    }
    case Format::LDSDIR: {
       LDSDIR_instruction& ldsdir = instr->ldsdir();
-      update_counters(ctx, event_ldsdir, ldsdir.sync);
+      update_counters(ctx, event_ldsdir, instr, ldsdir.sync);
       insert_wait_entry(ctx, instr->definitions[0], event_ldsdir);
       break;
    }
@@ -787,16 +821,16 @@ gen(Instruction* instr, wait_ctx& ctx)
       wait_event ev = get_vmem_event(ctx, instr, type);
       uint32_t mask = ev == event_vmem ? get_vmem_mask(ctx, instr) : 0;
 
-      update_counters(ctx, ev, get_sync_info(instr));
+      update_counters(ctx, ev, instr, get_sync_info(instr));
 
       for (auto& definition : instr->definitions)
          insert_wait_entry(ctx, definition, ev, type, mask);
 
       if (ctx.gfx_level == GFX6 && instr->format != Format::MIMG && instr->operands.size() == 4) {
-         update_counters(ctx, event_vmem_gpr_lock);
+         update_counters(ctx, event_vmem_gpr_lock, instr);
          insert_wait_entry(ctx, instr->operands[3], event_vmem_gpr_lock);
       } else if (ctx.gfx_level == GFX6 && instr->isMIMG() && !instr->operands[2].isUndefined()) {
-         update_counters(ctx, event_vmem_gpr_lock);
+         update_counters(ctx, event_vmem_gpr_lock, instr);
          insert_wait_entry(ctx, instr->operands[2], event_vmem_gpr_lock);
       }
 
@@ -804,13 +838,13 @@ gen(Instruction* instr, wait_ctx& ctx)
    }
    case Format::SOPP: {
       if (instr->opcode == aco_opcode::s_sendmsg || instr->opcode == aco_opcode::s_sendmsghalt)
-         update_counters(ctx, event_sendmsg);
+         update_counters(ctx, event_sendmsg, instr);
       break;
    }
    case Format::SOP1: {
       if (instr->opcode == aco_opcode::s_sendmsg_rtn_b32 ||
           instr->opcode == aco_opcode::s_sendmsg_rtn_b64) {
-         update_counters(ctx, event_sendmsg);
+         update_counters(ctx, event_sendmsg, instr);
          insert_wait_entry(ctx, instr->definitions[0], event_sendmsg);
       }
       break;
@@ -900,10 +934,11 @@ handle_block(Program* program, Block& block, wait_ctx& ctx)
             !((instr->ds().offset1 | (instr->ds().offset0 >> 8)) & 0x1);
 
          new_instructions.emplace_back(std::move(instr));
-         perform_barrier(ctx, queued_imm, sync_info, semantic_acquire);
+         if (sync_info.semantics & semantic_acquire)
+            perform_barrier(ctx, queued_imm, sync_info, true);
 
          if (is_ordered_count_acquire)
-            queued_imm.combine(ctx.bar[barrier_info_all].imm[ffs(storage_gds) - 1]);
+            queued_imm.combine(ctx.bar[barrier_info_release_dep].imm[ffs(storage_gds) - 1]);
       }
    }
 
@@ -935,12 +970,12 @@ insert_waitcnt(Program* program)
    unsigned loop_progress = 0;
 
    if (program->pending_lds_access) {
-      update_barriers(in_ctx[0], info.get_counters_for_event(event_lds), event_lds,
+      update_barriers(in_ctx[0], info.get_counters_for_event(event_lds), event_lds, NULL,
                       memory_sync_info(storage_shared));
    }
 
    for (Definition def : program->args_pending_vmem) {
-      update_counters(in_ctx[0], event_vmem);
+      update_counters(in_ctx[0], event_vmem, NULL);
       insert_wait_entry(in_ctx[0], def, event_vmem, vmem_nosampler, 0xffffffff);
    }
 
-- 
GitLab


From 5595929ee9033c7e83234b3984122db104e0d629 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Thu, 31 Jul 2025 14:38:00 +0100
Subject: [PATCH 07/12] aco: delay barrier waitcnt until they are needed

fossil-db (navi21):
Totals from 44 (0.06% of 79825) affected shaders:
Instrs: 16001 -> 15932 (-0.43%); split: -0.46%, +0.02%
CodeSize: 85800 -> 85548 (-0.29%); split: -0.30%, +0.01%
Latency: 190124 -> 173458 (-8.77%)
InvThroughput: 23605 -> 22756 (-3.60%)

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_insert_waitcnt.cpp       | 66 +++++++++++++++----
 .../compiler/tests/test_insert_waitcnt.cpp    |  9 +--
 2 files changed, 60 insertions(+), 15 deletions(-)

diff --git a/src/amd/compiler/aco_insert_waitcnt.cpp b/src/amd/compiler/aco_insert_waitcnt.cpp
index 64cc2df7969a2..33c7b3c8ebd2f 100644
--- a/src/amd/compiler/aco_insert_waitcnt.cpp
+++ b/src/amd/compiler/aco_insert_waitcnt.cpp
@@ -171,6 +171,12 @@ enum barrier_info_kind {
    barrier_info_release_dep,
    /* Waits for all atomics */
    barrier_info_acquire_dep,
+   /* A wait that is to be emitted when an
+    * atomics/control_barriers/sendmsg_gs_done/position-primitive-export is encountered.
+    */
+   barrier_info_release,
+   /* A wait that is to be emitted when any non-private access is encountered. */
+   barrier_info_acquire,
    num_barrier_infos,
 };
 
@@ -448,18 +454,22 @@ is_atomic_or_control_instr(Program* program, Instruction* instr, memory_sync_inf
    return 0;
 }
 
+/* We delay the waitcnt for a barrier until it's needed. This can help hide the cost or let it be
+ * eliminated. */
 void
-perform_barrier(wait_ctx& ctx, wait_imm& imm, memory_sync_info sync, bool is_acquire)
+setup_barrier(wait_ctx& ctx, wait_imm& imm, memory_sync_info sync, bool is_acquire)
 {
    sync_scope subgroup_scope =
       ctx.program->workgroup_size <= ctx.program->wave_size ? scope_workgroup : scope_subgroup;
    if (sync.scope <= subgroup_scope)
       return;
 
-   barrier_info& bar = ctx.bar[is_acquire ? barrier_info_acquire_dep : barrier_info_release_dep];
+   barrier_info& src = ctx.bar[is_acquire ? barrier_info_acquire_dep : barrier_info_release_dep];
 
-   u_foreach_bit (i, sync.storage & bar.storage) {
-      uint16_t events = bar.events[i];
+   wait_imm dst_imm;
+   uint16_t dst_events = 0;
+   u_foreach_bit (i, sync.storage & src.storage) {
+      uint16_t events = src.events[i];
 
       /* LDS is private to the workgroup */
       if (MIN2(sync.scope, scope_workgroup) <= subgroup_scope)
@@ -468,10 +478,39 @@ perform_barrier(wait_ctx& ctx, wait_imm& imm, memory_sync_info sync, bool is_acq
       /* Until GFX11, in non-WGP, the L1 (L0 on GFX10+) cache keeps all memory operations
        * in-order for the same workgroup */
       if (ctx.gfx_level < GFX11 && !ctx.program->wgp_mode && sync.scope <= scope_workgroup)
-         events &= ~(event_vmem | event_vmem_store | event_smem);
+         events &= ~(event_vmem | event_vmem_store);
 
-      if (events)
-         imm.combine(bar.imm[i]);
+      if (events) {
+         dst_imm.combine(src.imm[i]);
+         dst_events |= src.events[i];
+      }
+   }
+   if (!dst_events)
+      return;
+
+   /* Copy over wait into barrier_info_acquire/barrier_info_release */
+   unsigned dst_index = is_acquire ? barrier_info_acquire : barrier_info_release;
+   barrier_info& dst = ctx.bar[dst_index];
+   u_foreach_bit (i, sync.storage) {
+      dst.imm[i].combine(dst_imm);
+      dst.events[i] |= dst_events;
+   }
+   dst.storage |= sync.storage;
+   ctx.bar_nonempty |= 1 << dst_index;
+}
+
+void
+finish_barriers(wait_ctx& ctx, wait_imm& imm, Instruction* instr, memory_sync_info sync)
+{
+   if (ctx.bar_nonempty & (1 << barrier_info_release)) {
+      uint16_t storage_release = is_atomic_or_control_instr(ctx.program, instr, sync, false);
+      u_foreach_bit (i, storage_release & ctx.bar[barrier_info_release].storage)
+         imm.combine(ctx.bar[barrier_info_release].imm[i]);
+   }
+   if (ctx.bar_nonempty & (1 << barrier_info_acquire)) {
+      uint16_t storage_acquire = (sync.semantics & semantic_private) ? 0 : sync.storage;
+      u_foreach_bit (i, storage_acquire & ctx.bar[barrier_info_acquire].storage)
+         imm.combine(ctx.bar[barrier_info_acquire].imm[i]);
    }
 }
 
@@ -578,13 +617,15 @@ kill(wait_imm& imm, Instruction* instr, wait_ctx& ctx, memory_sync_info sync_inf
 
    if (instr->opcode == aco_opcode::p_barrier) {
       if (instr->barrier().sync.semantics & semantic_release)
-         perform_barrier(ctx, imm, instr->barrier().sync, false);
+         setup_barrier(ctx, imm, instr->barrier().sync, false);
       if (instr->barrier().sync.semantics & semantic_acquire)
-         perform_barrier(ctx, imm, instr->barrier().sync, true);
+         setup_barrier(ctx, imm, instr->barrier().sync, true);
    } else if (sync_info.semantics & semantic_release) {
-      perform_barrier(ctx, imm, sync_info, false);
+      setup_barrier(ctx, imm, sync_info, false);
    }
 
+   finish_barriers(ctx, imm, instr, sync_info);
+
    if (!imm.empty()) {
       if (ctx.pending_flat_vm && imm.vm != wait_imm::unset_counter)
          imm.vm = 0;
@@ -667,6 +708,9 @@ update_barriers(wait_ctx& ctx, uint8_t counters, wait_event event, Instruction*
       uint16_t storage_acq = is_atomic_or_control_instr(ctx.program, instr, sync, true);
       update_barrier_info_for_event(ctx, counters, event, barrier_info_acquire_dep, storage_acq);
    }
+
+   update_barrier_info_for_event(ctx, counters, event, barrier_info_release, 0);
+   update_barrier_info_for_event(ctx, counters, event, barrier_info_acquire, 0);
 }
 
 void
@@ -935,7 +979,7 @@ handle_block(Program* program, Block& block, wait_ctx& ctx)
 
          new_instructions.emplace_back(std::move(instr));
          if (sync_info.semantics & semantic_acquire)
-            perform_barrier(ctx, queued_imm, sync_info, true);
+            setup_barrier(ctx, queued_imm, sync_info, true);
 
          if (is_ordered_count_acquire)
             queued_imm.combine(ctx.bar[barrier_info_release_dep].imm[ffs(storage_gds) - 1]);
diff --git a/src/amd/compiler/tests/test_insert_waitcnt.cpp b/src/amd/compiler/tests/test_insert_waitcnt.cpp
index 2049583f9545d..4bfdc97309f14 100644
--- a/src/amd/compiler/tests/test_insert_waitcnt.cpp
+++ b/src/amd/compiler/tests/test_insert_waitcnt.cpp
@@ -610,7 +610,8 @@ BEGIN_TEST(insert_waitcnt.vmem_ds)
 
    //! s_wait_storecnt_dscnt dscnt(0) storecnt(0)
    bld.barrier(aco_opcode::p_barrier,
-               memory_sync_info(storage_buffer | storage_shared, semantic_acqrel, scope_workgroup));
+               memory_sync_info(storage_buffer | storage_shared, semantic_acqrel, scope_workgroup),
+               scope_workgroup);
 
    finish_waitcnt_test();
 END_TEST
@@ -1119,7 +1120,7 @@ BEGIN_TEST(insert_waitcnt.flat.barrier)
                  memory_sync_info(storage_buffer));
       bld.flat(aco_opcode::flat_load_dword, dest1, addr, Operand(s1)).instr->flat().may_use_lds = true;
       bld.barrier(aco_opcode::p_barrier,
-                  memory_sync_info(storage_buffer, semantic_acqrel, scope_device));
+                  memory_sync_info(storage_buffer, semantic_acqrel, scope_device), scope_workgroup);
 
       //>> p_unit_test 1
       //! v1: %0:v[5] = flat_load_dword %0:v[0-1], s1: undef may_use_lds storage:buffer
@@ -1132,7 +1133,7 @@ BEGIN_TEST(insert_waitcnt.flat.barrier)
                memory_sync_info(storage_buffer)).instr->flat().may_use_lds = true;
       bld.global(aco_opcode::global_load_dword, dest0, addr, Operand(s1), 0);
       bld.barrier(aco_opcode::p_barrier,
-                  memory_sync_info(storage_buffer, semantic_acqrel, scope_device));
+                  memory_sync_info(storage_buffer, semantic_acqrel, scope_device), scope_workgroup);
 
       //>> p_unit_test 2
       //! flat_store_dword %0:v[0-1], s1: undef, %0:v[0] may_use_lds storage:buffer
@@ -1144,7 +1145,7 @@ BEGIN_TEST(insert_waitcnt.flat.barrier)
       bld.flat(aco_opcode::flat_store_dword, addr, Operand(s1), data, 0,
                memory_sync_info(storage_buffer)).instr->flat().may_use_lds = true;
       bld.barrier(aco_opcode::p_barrier,
-                  memory_sync_info(storage_buffer, semantic_acqrel, scope_device));
+                  memory_sync_info(storage_buffer, semantic_acqrel, scope_device), scope_workgroup);
 
       finish_waitcnt_test();
    }
-- 
GitLab


From bf3cb8e11d6dc54d292c23226d8dcab40e14fa6e Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Tue, 22 Jul 2025 16:05:13 +0100
Subject: [PATCH 08/12] aco: remove waitcnt code for SMEM stores

These were removed in GFX10.3 and we haven't used them in a while.

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_insert_waitcnt.cpp | 30 +------------------------
 1 file changed, 1 insertion(+), 29 deletions(-)

diff --git a/src/amd/compiler/aco_insert_waitcnt.cpp b/src/amd/compiler/aco_insert_waitcnt.cpp
index 33c7b3c8ebd2f..e65b36bca20a4 100644
--- a/src/amd/compiler/aco_insert_waitcnt.cpp
+++ b/src/amd/compiler/aco_insert_waitcnt.cpp
@@ -217,7 +217,6 @@ struct wait_ctx {
    uint32_t nonzero = 0;
    bool pending_flat_lgkm = false;
    bool pending_flat_vm = false;
-   bool pending_s_buffer_store = false; /* GFX10 workaround */
 
    barrier_info bar[num_barrier_infos];
    uint8_t bar_nonempty = 0;
@@ -237,7 +236,6 @@ struct wait_ctx {
       nonzero |= other->nonzero;
       pending_flat_lgkm |= other->pending_flat_lgkm;
       pending_flat_vm |= other->pending_flat_vm;
-      pending_s_buffer_store |= other->pending_s_buffer_store;
 
       using iterator = std::map<PhysReg, wait_entry>::iterator;
 
@@ -588,27 +586,6 @@ kill(wait_imm& imm, Instruction* instr, wait_ctx& ctx, memory_sync_info sync_inf
 
    check_instr(ctx, imm, instr);
 
-   /* It's required to wait for scalar stores before "writing back" data.
-    * It shouldn't cost anything anyways since we're about to do s_endpgm.
-    */
-   if ((ctx.nonzero & BITFIELD_BIT(wait_type_lgkm)) && instr->opcode == aco_opcode::s_dcache_wb) {
-      assert(ctx.gfx_level >= GFX8);
-      imm.lgkm = 0;
-   }
-
-   if (ctx.gfx_level >= GFX10 && instr->isSMEM()) {
-      /* GFX10: A store followed by a load at the same address causes a problem because
-       * the load doesn't load the correct values unless we wait for the store first.
-       * This is NOT mitigated by an s_nop.
-       *
-       * TODO: Refine this when we have proper alias analysis.
-       */
-      if (ctx.pending_s_buffer_store && !instr->smem().definitions.empty() &&
-          !instr->smem().sync.can_reorder()) {
-         imm.lgkm = 0;
-      }
-   }
-
    if (instr->opcode == aco_opcode::ds_ordered_count &&
        ((instr->ds().offset1 | (instr->ds().offset0 >> 8)) & 0x1)) {
       barrier_info& bar = ctx.bar[barrier_info_release_dep];
@@ -655,10 +632,8 @@ kill(wait_imm& imm, Instruction* instr, wait_ctx& ctx, memory_sync_info sync_inf
 
    if (imm.vm == 0)
       ctx.pending_flat_vm = false;
-   if (imm.lgkm == 0) {
+   if (imm.lgkm == 0)
       ctx.pending_flat_lgkm = false;
-      ctx.pending_s_buffer_store = false;
-   }
 }
 
 void
@@ -829,9 +804,6 @@ gen(Instruction* instr, wait_ctx& ctx)
 
       if (!instr->definitions.empty())
          insert_wait_entry(ctx, instr->definitions[0], event_smem);
-      else if (ctx.gfx_level >= GFX10 && !smem.sync.can_reorder())
-         ctx.pending_s_buffer_store = true;
-
       break;
    }
    case Format::DS: {
-- 
GitLab


From 063d50851c8881172fba37f167ca1d9b3cc8bf13 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Fri, 1 Aug 2025 11:11:27 +0100
Subject: [PATCH 09/12] aco: remove waitcnt code for POPS

We now insert barriers around these instead.

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_insert_waitcnt.cpp | 19 -------------------
 1 file changed, 19 deletions(-)

diff --git a/src/amd/compiler/aco_insert_waitcnt.cpp b/src/amd/compiler/aco_insert_waitcnt.cpp
index e65b36bca20a4..657fc44915be1 100644
--- a/src/amd/compiler/aco_insert_waitcnt.cpp
+++ b/src/amd/compiler/aco_insert_waitcnt.cpp
@@ -565,25 +565,6 @@ kill(wait_imm& imm, Instruction* instr, wait_ctx& ctx, memory_sync_info sync_inf
       imm.combine(bar.imm[ffs(storage_vgpr_spill) - 1]);
    }
 
-   /* Make sure POPS coherent memory accesses have reached the L2 cache before letting the
-    * overlapping waves proceed into the ordered section.
-    */
-   if (ctx.program->has_pops_overlapped_waves_wait &&
-       (ctx.gfx_level >= GFX11 ? instr->isEXP() && instr->exp().done
-                               : (instr->opcode == aco_opcode::s_sendmsg &&
-                                  instr->salu().imm == sendmsg_ordered_ps_done))) {
-      uint8_t c = counter_vm | counter_vs;
-      /* Await SMEM loads too, as it's possible for an application to create them, like using a
-       * scalarization loop - pointless and unoptimal for an inherently divergent address of
-       * per-pixel data, but still can be done at least synthetically and must be handled correctly.
-       */
-      if (ctx.program->has_smem_buffer_or_global_loads)
-         c |= counter_lgkm;
-
-      u_foreach_bit (i, c & ctx.nonzero)
-         imm[i] = 0;
-   }
-
    check_instr(ctx, imm, instr);
 
    if (instr->opcode == aco_opcode::ds_ordered_count &&
-- 
GitLab


From da44f08008f3289216c7e86891b7e7c256474476 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Tue, 22 Jul 2025 16:58:06 +0100
Subject: [PATCH 10/12] aco: update waitcnt events for exports

Include primitive, dual source blend and POS4 exports.

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_insert_waitcnt.cpp | 37 +++++++++++++++----------
 1 file changed, 23 insertions(+), 14 deletions(-)

diff --git a/src/amd/compiler/aco_insert_waitcnt.cpp b/src/amd/compiler/aco_insert_waitcnt.cpp
index 657fc44915be1..1e93bb907555f 100644
--- a/src/amd/compiler/aco_insert_waitcnt.cpp
+++ b/src/amd/compiler/aco_insert_waitcnt.cpp
@@ -45,16 +45,18 @@ enum wait_event : uint32_t {
    event_gds = 1 << 2,
    event_vmem = 1 << 3,
    event_vmem_store = 1 << 4, /* GFX10+ */
-   event_exp_pos = 1 << 6,
-   event_exp_param = 1 << 7,
-   event_exp_mrt_null = 1 << 8,
-   event_gds_gpr_lock = 1 << 9,
-   event_vmem_gpr_lock = 1 << 10,
-   event_sendmsg = 1 << 11,
-   event_ldsdir = 1 << 12,
-   event_vmem_sample = 1 << 13, /* GFX12+ */
-   event_vmem_bvh = 1 << 14,    /* GFX12+ */
-   num_events = 15,
+   event_exp_pos = 1 << 5,
+   event_exp_param = 1 << 6,
+   event_exp_mrt_null = 1 << 7,
+   event_exp_prim = 1 << 8,
+   event_exp_dual_src_blend = 1 << 9,
+   event_gds_gpr_lock = 1 << 10,
+   event_vmem_gpr_lock = 1 << 11,
+   event_sendmsg = 1 << 12,
+   event_ldsdir = 1 << 13,
+   event_vmem_sample = 1 << 14, /* GFX12+ */
+   event_vmem_bvh = 1 << 15,    /* GFX12+ */
+   num_events = 16,
 };
 
 enum counter_type : uint8_t {
@@ -140,7 +142,8 @@ struct target_info {
          max_cnt[i] = max_cnt[i] ? max_cnt[i] - 1 : 0;
 
       events[wait_type_exp] = event_exp_pos | event_exp_param | event_exp_mrt_null |
-                              event_gds_gpr_lock | event_vmem_gpr_lock | event_ldsdir;
+                              event_exp_prim | event_exp_dual_src_blend | event_gds_gpr_lock |
+                              event_vmem_gpr_lock | event_ldsdir;
       events[wait_type_lgkm] = event_smem | event_lds | event_gds | event_sendmsg;
       events[wait_type_vm] = event_vmem;
       events[wait_type_vs] = event_vmem_store;
@@ -743,12 +746,18 @@ gen(Instruction* instr, wait_ctx& ctx)
       Export_instruction& exp_instr = instr->exp();
 
       wait_event ev;
-      if (exp_instr.dest <= 9)
+      if (exp_instr.dest <= V_008DFC_SQ_EXP_NULL)
          ev = event_exp_mrt_null;
-      else if (exp_instr.dest <= 15)
+      else if (exp_instr.dest <= (V_008DFC_SQ_EXP_POS + 4))
          ev = event_exp_pos;
-      else
+      else if (exp_instr.dest == V_008DFC_SQ_EXP_PRIM)
+         ev = event_exp_prim;
+      else if (exp_instr.dest == 21 || exp_instr.dest == 22)
+         ev = event_exp_dual_src_blend;
+      else if (exp_instr.dest >= V_008DFC_SQ_EXP_PARAM)
          ev = event_exp_param;
+      else
+         UNREACHABLE("Invalid export destination");
       update_counters(ctx, ev, instr);
 
       /* insert new entries for exported vgprs */
-- 
GitLab


From 3b0def22e2790396e74c5d52d7e462690568f701 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Fri, 1 Aug 2025 15:38:31 +0100
Subject: [PATCH 11/12] aco: use a separate event for sendmsg_rtn

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_insert_waitcnt.cpp | 18 ++++++++++--------
 1 file changed, 10 insertions(+), 8 deletions(-)

diff --git a/src/amd/compiler/aco_insert_waitcnt.cpp b/src/amd/compiler/aco_insert_waitcnt.cpp
index 1e93bb907555f..07195641cd928 100644
--- a/src/amd/compiler/aco_insert_waitcnt.cpp
+++ b/src/amd/compiler/aco_insert_waitcnt.cpp
@@ -53,10 +53,11 @@ enum wait_event : uint32_t {
    event_gds_gpr_lock = 1 << 10,
    event_vmem_gpr_lock = 1 << 11,
    event_sendmsg = 1 << 12,
-   event_ldsdir = 1 << 13,
-   event_vmem_sample = 1 << 14, /* GFX12+ */
-   event_vmem_bvh = 1 << 15,    /* GFX12+ */
-   num_events = 16,
+   event_sendmsg_rtn = 1 << 13,
+   event_ldsdir = 1 << 14,
+   event_vmem_sample = 1 << 15, /* GFX12+ */
+   event_vmem_bvh = 1 << 16,    /* GFX12+ */
+   num_events = 17,
 };
 
 enum counter_type : uint8_t {
@@ -144,13 +145,14 @@ struct target_info {
       events[wait_type_exp] = event_exp_pos | event_exp_param | event_exp_mrt_null |
                               event_exp_prim | event_exp_dual_src_blend | event_gds_gpr_lock |
                               event_vmem_gpr_lock | event_ldsdir;
-      events[wait_type_lgkm] = event_smem | event_lds | event_gds | event_sendmsg;
+      events[wait_type_lgkm] =
+         event_smem | event_lds | event_gds | event_sendmsg | event_sendmsg_rtn;
       events[wait_type_vm] = event_vmem;
       events[wait_type_vs] = event_vmem_store;
       if (gfx_level >= GFX12) {
          events[wait_type_sample] = event_vmem_sample;
          events[wait_type_bvh] = event_vmem_bvh;
-         events[wait_type_km] = event_smem | event_sendmsg;
+         events[wait_type_km] = event_smem | event_sendmsg | event_sendmsg_rtn;
          events[wait_type_lgkm] &= ~events[wait_type_km];
       }
 
@@ -850,8 +852,8 @@ gen(Instruction* instr, wait_ctx& ctx)
    case Format::SOP1: {
       if (instr->opcode == aco_opcode::s_sendmsg_rtn_b32 ||
           instr->opcode == aco_opcode::s_sendmsg_rtn_b64) {
-         update_counters(ctx, event_sendmsg, instr);
-         insert_wait_entry(ctx, instr->definitions[0], event_sendmsg);
+         update_counters(ctx, event_sendmsg_rtn, instr);
+         insert_wait_entry(ctx, instr->definitions[0], event_sendmsg_rtn);
       }
       break;
    }
-- 
GitLab


From 1e8a53dce542f220f9355cb7e447c7553a8e15c1 Mon Sep 17 00:00:00 2001
From: Rhys Perry <pendingchaos02@gmail.com>
Date: Thu, 24 Jul 2025 14:44:29 +0100
Subject: [PATCH 12/12] aco/tests: add barrier-to-waitcnt tests

Signed-off-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/compiler/aco_builder_h.py             |   2 +-
 .../compiler/tests/test_insert_waitcnt.cpp    | 395 ++++++++++++++++++
 2 files changed, 396 insertions(+), 1 deletion(-)

diff --git a/src/amd/compiler/aco_builder_h.py b/src/amd/compiler/aco_builder_h.py
index cab0cf58ea72c..a941c6a6ac7cf 100644
--- a/src/amd/compiler/aco_builder_h.py
+++ b/src/amd/compiler/aco_builder_h.py
@@ -569,7 +569,7 @@ formats = [("pseudo", [Format.PSEUDO], list(itertools.product(range(5), range(7)
            ("sopp", [Format.SOPP], [(0, 0), (0, 1)]),
            ("sopc", [Format.SOPC], [(1, 2)]),
            ("smem", [Format.SMEM], [(0, 4), (0, 3), (1, 0), (1, 3), (1, 2), (1, 1), (0, 0)]),
-           ("ds", [Format.DS], [(1, 0), (1, 1), (1, 2), (1, 3), (0, 3), (0, 4), (2, 3)]),
+           ("ds", [Format.DS], [(1, 0), (1, 1), (1, 2), (1, 3), (0, 2), (0, 3), (0, 4), (2, 3)]),
            ("ldsdir", [Format.LDSDIR], [(1, 1)]),
            ("mubuf", [Format.MUBUF], [(0, 4), (1, 3), (1, 4)]),
            ("mtbuf", [Format.MTBUF], [(0, 4), (1, 3)]),
diff --git a/src/amd/compiler/tests/test_insert_waitcnt.cpp b/src/amd/compiler/tests/test_insert_waitcnt.cpp
index 4bfdc97309f14..1a610817fc281 100644
--- a/src/amd/compiler/tests/test_insert_waitcnt.cpp
+++ b/src/amd/compiler/tests/test_insert_waitcnt.cpp
@@ -1150,3 +1150,398 @@ BEGIN_TEST(insert_waitcnt.flat.barrier)
       finish_waitcnt_test();
    }
 END_TEST
+
+static void
+barrier(unsigned storage, unsigned semantics, sync_scope scope,
+        sync_scope exec_scope = scope_invocation)
+{
+   bld.barrier(aco_opcode::p_barrier, memory_sync_info(storage, semantics, scope), exec_scope);
+}
+
+static Definition&
+load_global(unsigned semantics = 0, sync_scope scope = scope_invocation)
+{
+   Definition dest0(PhysReg(260), v1);
+   Operand addr(PhysReg(256), v2);
+   return bld
+      .global(aco_opcode::global_load_dword, dest0, addr, Operand(s1), 0,
+              memory_sync_info(storage_buffer, semantics, scope))
+      .def(0);
+}
+
+static void
+store_global(unsigned semantics = 0, sync_scope scope = scope_invocation)
+{
+   Operand addr(PhysReg(256), v2);
+   Operand data(PhysReg(256), v1);
+   bld.global(aco_opcode::global_store_dword, addr, Operand(s1), data, 0,
+              memory_sync_info(storage_buffer, semantics, scope));
+}
+
+static Definition&
+load_shared(unsigned semantics = 0, sync_scope scope = scope_invocation)
+{
+   Definition dest0(PhysReg(260), v1);
+   Operand offset(PhysReg(256), v1);
+   Builder::Result res = bld.ds(aco_opcode::ds_read_b32, dest0, offset);
+   res.instr->ds().sync = memory_sync_info(storage_shared, semantics, scope);
+   return res.def(0);
+}
+
+static void
+store_shared(unsigned semantics = 0, sync_scope scope = scope_invocation)
+{
+   Operand offset(PhysReg(256), v1);
+   Operand data(PhysReg(256), v1);
+   bld.ds(aco_opcode::ds_write_b32, offset, data).instr->ds().sync =
+      memory_sync_info(storage_shared, semantics, scope);
+}
+
+struct barrier_test_variant {
+   unsigned workgroup_size;
+   bool wgp;
+   const char* name;
+};
+
+static const barrier_test_variant barrier_test_variants[] = {
+   {64, false, "_wg64cu"},
+   {128, false, "_wg128cu"},
+   {64, true, "_wg64wgp"},
+   {128, true, "_wg128wgp"},
+};
+
+BEGIN_TEST(insert_waitcnt.barrier.release)
+   for (barrier_test_variant var : barrier_test_variants) {
+      if (!setup_cs(NULL, GFX10, CHIP_UNKNOWN, var.name))
+         continue;
+
+      program->workgroup_size = var.workgroup_size;
+      program->wgp_mode = var.wgp;
+
+      Definition dest0(PhysReg(260), v1);
+      Definition dest1(PhysReg(261), v1);
+      Operand addr(PhysReg(256), v2);
+      Operand offset(PhysReg(256), v1);
+      Operand data(PhysReg(256), v1);
+
+      /* global->global, device scope */
+      //>> p_unit_test 0
+      //! global_store_dword %0:v[0-1], s1: undef, %0:v[0] storage:buffer
+      //! s_waitcnt_vscnt %0:null imm:0
+      //! global_store_dword %0:v[0-1], s1: undef, %0:v[0] storage:buffer semantics:atomic
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(0));
+      store_global();
+      barrier(storage_buffer, semantic_release, scope_device);
+      store_global(semantic_atomic);
+
+      //>> p_unit_test 1
+      //! v1: %0:v[4] = global_load_dword %0:v[0-1], s1: undef storage:buffer
+      //! s_waitcnt vmcnt(0)
+      //! global_store_dword %0:v[0-1], s1: undef, %0:v[0] storage:buffer semantics:atomic
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(1));
+      load_global();
+      barrier(storage_buffer, semantic_release, scope_device);
+      store_global(semantic_atomic);
+
+      /* global->global, workgroup scope */
+      //>> p_unit_test 2
+      //! global_store_dword %0:v[0-1], s1: undef, %0:v[0] storage:buffer
+      //~gfx10_wg128wgp! s_waitcnt_vscnt %0:null imm:0
+      //! global_store_dword %0:v[0-1], s1: undef, %0:v[0] storage:buffer semantics:atomic
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(2));
+      store_global();
+      barrier(storage_buffer, semantic_release, scope_workgroup);
+      store_global(semantic_atomic);
+
+      //>> p_unit_test 3
+      //! v1: %0:v[4] = global_load_dword %0:v[0-1], s1: undef storage:buffer
+      //~gfx10_wg128wgp! s_waitcnt vmcnt(0)
+      //! global_store_dword %0:v[0-1], s1: undef, %0:v[0] storage:buffer semantics:atomic
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(3));
+      load_global();
+      barrier(storage_buffer, semantic_release, scope_workgroup);
+      store_global(semantic_atomic);
+
+      /* shared->shared */
+      //>> p_unit_test 4
+      //! ds_write_b32 %0:v[0], %0:v[0] storage:shared
+      //~gfx10_wg128(cu|wgp)! s_waitcnt lgkmcnt(0)
+      //! ds_write_b32 %0:v[0], %0:v[0] storage:shared semantics:atomic
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(4));
+      store_shared();
+      barrier(storage_shared, semantic_release, scope_workgroup);
+      store_shared(semantic_atomic);
+
+      //>> p_unit_test 5
+      //! v1: %0:v[4] = ds_read_b32 %0:v[0] storage:shared
+      //~gfx10_wg128(cu|wgp)! s_waitcnt lgkmcnt(0)
+      //! ds_write_b32 %0:v[0], %0:v[0] storage:shared semantics:atomic
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(5));
+      load_shared();
+      barrier(storage_shared, semantic_release, scope_workgroup);
+      store_shared(semantic_atomic);
+
+      /* shared->global */
+      //>> p_unit_test 6
+      //! ds_write_b32 %0:v[0], %0:v[0] storage:shared
+      //~gfx10_wg128(cu|wgp)! s_waitcnt lgkmcnt(0)
+      //! global_store_dword %0:v[0-1], s1: undef, %0:v[0] storage:buffer semantics:atomic
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(6));
+      store_shared();
+      barrier(storage_buffer | storage_shared, semantic_release, scope_device);
+      store_global(semantic_atomic);
+
+      /* global->shared */
+      //>> p_unit_test 7
+      //! global_store_dword %0:v[0-1], s1: undef, %0:v[0] storage:buffer
+      //! s_waitcnt_vscnt %0:null imm:0
+      //! ds_write_b32 %0:v[0], %0:v[0] storage:shared semantics:atomic
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(7));
+      store_global();
+      barrier(storage_buffer | storage_shared, semantic_release, scope_device);
+      store_shared(semantic_atomic);
+
+      /* global->global, device scope, release in the atomic */
+      //>> p_unit_test 8
+      //! global_store_dword %0:v[0-1], s1: undef, %0:v[0] storage:buffer
+      //! s_waitcnt_vscnt %0:null imm:0
+      //! global_store_dword %0:v[0-1], s1: undef, %0:v[0] storage:buffer semantics:release,atomic scope:device
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(8));
+      store_global();
+      store_global(semantic_atomic | semantic_release, scope_device);
+
+      /* global->global, workgroup scope, control barrier */
+      //>> p_unit_test 9
+      //! global_store_dword %0:v[0-1], s1: undef, %0:v[0] storage:buffer
+      //~gfx10_wg128wgp! s_waitcnt_vscnt %0:null imm:0
+      //! s_barrier
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(9));
+      store_global();
+      barrier(storage_buffer, semantic_release, scope_workgroup);
+      barrier(0, 0, scope_invocation, scope_workgroup);
+      bld.sopp(aco_opcode::s_barrier);
+
+      /* global->global, device scope, delayed waitcnt */
+      //>> p_unit_test 10
+      //! global_store_dword %0:v[0-1], s1: undef, %0:v[0] storage:buffer
+      //! v_nop
+      //! s_waitcnt_vscnt %0:null imm:0
+      //! global_store_dword %0:v[0-1], s1: undef, %0:v[0] storage:buffer semantics:atomic
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(10));
+      store_global();
+      barrier(storage_buffer, semantic_release, scope_device);
+      bld.vop1(aco_opcode::v_nop);
+      store_global(semantic_atomic);
+
+      //>> p_unit_test 11
+      //! global_store_dword %0:v[0-1], s1: undef, %0:v[0] storage:buffer
+      //! v_nop
+      //! s_waitcnt_vscnt %0:null imm:0
+      //! v_nop
+      //! global_store_dword %0:v[0-1], s1: undef, %0:v[0] storage:buffer semantics:atomic
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(11));
+      store_global();
+      barrier(storage_buffer, semantic_release, scope_device);
+      bld.vop1(aco_opcode::v_nop);
+      wait_imm vs_imm;
+      vs_imm.vs = 0;
+      vs_imm.build_waitcnt(bld);
+      bld.vop1(aco_opcode::v_nop);
+      store_global(semantic_atomic);
+
+      //>> p_unit_test 12
+      //! global_store_dword %0:v[0-1], s1: undef, %0:v[0] storage:buffer
+      //! global_store_dword %0:v[0-1], s1: undef, %0:v[0] storage:buffer semantics:private
+      //! v_nop
+      //! s_waitcnt_vscnt %0:null imm:1
+      //! global_store_dword %0:v[0-1], s1: undef, %0:v[0] storage:buffer semantics:atomic
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(12));
+      store_global();
+      barrier(storage_buffer, semantic_release, scope_device);
+      store_global(semantic_private);
+      bld.vop1(aco_opcode::v_nop); /* break up clause */
+      store_global(semantic_atomic);
+
+      finish_waitcnt_test();
+   }
+END_TEST
+
+BEGIN_TEST(insert_waitcnt.barrier.acquire)
+   for (barrier_test_variant var : barrier_test_variants) {
+      if (!setup_cs(NULL, GFX10, CHIP_UNKNOWN, var.name))
+         continue;
+
+      program->workgroup_size = var.workgroup_size;
+      program->wgp_mode = var.wgp;
+
+      Definition dest0(PhysReg(260), v1);
+      Definition dest1(PhysReg(261), v1);
+      Definition dest2(PhysReg(262), v1);
+      Operand addr(PhysReg(256), v2);
+      Operand offset(PhysReg(256), v1);
+      Operand data(PhysReg(256), v1);
+
+      /* global->global, device scope */
+      //>> p_unit_test 0
+      //! v1: %0:v[4] = global_load_dword %0:v[0-1], s1: undef storage:buffer semantics:atomic
+      //! s_waitcnt vmcnt(0)
+      //! v1: %0:v[5] = global_load_dword %0:v[0-1], s1: undef storage:buffer
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(0));
+      load_global(semantic_atomic);
+      barrier(storage_buffer, semantic_acquire, scope_device);
+      load_global() = dest1;
+
+      //>> p_unit_test 1
+      //! v1: %0:v[4] = global_load_dword %0:v[0-1], s1: undef storage:buffer semantics:atomic
+      //! s_waitcnt vmcnt(0)
+      //! global_store_dword %0:v[0-1], s1: undef, %0:v[0] storage:buffer
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(1));
+      load_global(semantic_atomic);
+      barrier(storage_buffer, semantic_acquire, scope_device);
+      store_global();
+
+      /* global->global, workgroup scope */
+      //>> p_unit_test 2
+      //! v1: %0:v[4] = global_load_dword %0:v[0-1], s1: undef storage:buffer semantics:atomic
+      //~gfx10_wg128wgp! s_waitcnt vmcnt(0)
+      //! v1: %0:v[5] = global_load_dword %0:v[0-1], s1: undef storage:buffer
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(2));
+      load_global(semantic_atomic);
+      barrier(storage_buffer, semantic_acquire, scope_workgroup);
+      load_global() = dest1;
+
+      //>> p_unit_test 3
+      //! v1: %0:v[4] = global_load_dword %0:v[0-1], s1: undef storage:buffer semantics:atomic
+      //~gfx10_wg128wgp! s_waitcnt vmcnt(0)
+      //! global_store_dword %0:v[0-1], s1: undef, %0:v[0] storage:buffer
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(3));
+      load_global(semantic_atomic);
+      barrier(storage_buffer, semantic_acquire, scope_workgroup);
+      store_global();
+
+      /* shared->shared */
+      //>> p_unit_test 4
+      //! v1: %0:v[4] = ds_read_b32 %0:v[0] storage:shared semantics:atomic
+      //~gfx10_wg128(cu|wgp)! s_waitcnt lgkmcnt(0)
+      //! v1: %0:v[5] = ds_read_b32 %0:v[0] storage:shared
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(4));
+      load_shared(semantic_atomic);
+      barrier(storage_shared, semantic_acquire, scope_workgroup);
+      load_shared() = dest1;
+
+      //>> p_unit_test 5
+      //! v1: %0:v[4] = ds_read_b32 %0:v[0] storage:shared semantics:atomic
+      //~gfx10_wg128(cu|wgp)! s_waitcnt lgkmcnt(0)
+      //! ds_write_b32 %0:v[0], %0:v[0] storage:shared
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(5));
+      load_shared(semantic_atomic);
+      barrier(storage_shared, semantic_acquire, scope_workgroup);
+      store_shared();
+
+      /* shared->global */
+      //>> p_unit_test 6
+      //! v1: %0:v[4] = ds_read_b32 %0:v[0] storage:shared semantics:atomic
+      //~gfx10_wg128(cu|wgp)! s_waitcnt lgkmcnt(0)
+      //! v1: %0:v[5] = global_load_dword %0:v[0-1], s1: undef storage:buffer
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(6));
+      load_shared(semantic_atomic);
+      barrier(storage_buffer | storage_shared, semantic_acquire, scope_device);
+      load_global() = dest1;
+
+      /* global->shared */
+      //>> p_unit_test 7
+      //! v1: %0:v[4] = global_load_dword %0:v[0-1], s1: undef storage:buffer semantics:atomic
+      //! s_waitcnt vmcnt(0)
+      //! v1: %0:v[5] = ds_read_b32 %0:v[0] storage:shared
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(7));
+      load_global(semantic_atomic);
+      barrier(storage_buffer | storage_shared, semantic_acquire, scope_device);
+      load_shared() = dest1;
+
+      /* global->global, device scope, acquire in the atomic */
+      //>> p_unit_test 8
+      //! v1: %0:v[4] = global_load_dword %0:v[0-1], s1: undef storage:buffer semantics:acquire,atomic scope:device
+      //! s_waitcnt vmcnt(0)
+      //! v1: %0:v[5] = global_load_dword %0:v[0-1], s1: undef storage:buffer
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(8));
+      load_global(semantic_atomic | semantic_acquire, scope_device);
+      load_global() = dest1;
+
+      /* global->global, workgroup scope, control barrier */
+      //>> p_unit_test 9
+      //! s_barrier
+      //! v1: %0:v[4] = global_load_dword %0:v[0-1], s1: undef storage:buffer
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(9));
+      barrier(0, 0, scope_invocation, scope_workgroup);
+      bld.sopp(aco_opcode::s_barrier);
+      barrier(storage_buffer, semantic_acquire, scope_workgroup);
+      load_global();
+
+      /* global->global, device scope, delayed waitcnt */
+      //>> p_unit_test 10
+      //! v1: %0:v[4] = global_load_dword %0:v[0-1], s1: undef storage:buffer semantics:atomic
+      //! v_nop
+      //! s_waitcnt vmcnt(0)
+      //! v1: %0:v[5] = global_load_dword %0:v[0-1], s1: undef storage:buffer
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(10));
+      load_global(semantic_atomic);
+      barrier(storage_buffer, semantic_acquire, scope_device);
+      bld.vop1(aco_opcode::v_nop);
+      load_global() = dest1;
+
+      //>> p_unit_test 11
+      //! v1: %0:v[4] = global_load_dword %0:v[0-1], s1: undef storage:buffer semantics:atomic
+      //! v_nop
+      //! s_waitcnt vmcnt(0)
+      //! v_nop
+      //! v1: %0:v[5] = global_load_dword %0:v[0-1], s1: undef storage:buffer
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(11));
+      load_global(semantic_atomic);
+      barrier(storage_buffer, semantic_acquire, scope_device);
+      bld.vop1(aco_opcode::v_nop);
+      wait_imm vm_imm;
+      vm_imm.vm = 0;
+      vm_imm.build_waitcnt(bld);
+      bld.vop1(aco_opcode::v_nop);
+      load_global() = dest1;
+
+      //>> p_unit_test 12
+      //! v1: %0:v[4] = global_load_dword %0:v[0-1], s1: undef storage:buffer semantics:atomic
+      //! v1: %0:v[5] = global_load_dword %0:v[0-1], s1: undef storage:buffer semantics:private
+      //! v_nop
+      //! s_waitcnt vmcnt(1)
+      //! v1: %0:v[6] = global_load_dword %0:v[0-1], s1: undef storage:buffer
+      bld.reset(program->create_and_insert_block());
+      bld.pseudo(aco_opcode::p_unit_test, Operand::c32(12));
+      load_global(semantic_atomic);
+      barrier(storage_buffer, semantic_acquire, scope_device);
+      load_global(semantic_private) = dest1;
+      bld.vop1(aco_opcode::v_nop); /* break up clause */
+      load_global() = dest2;
+
+      finish_waitcnt_test();
+   }
+END_TEST
-- 
GitLab

