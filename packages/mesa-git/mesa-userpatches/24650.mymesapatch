From 56f8bd47e339071e33be37a10f997cde7b39ffbd Mon Sep 17 00:00:00 2001
From: Georg Lehmann <dadschoorse@gmail.com>
Date: Sat, 12 Aug 2023 13:32:16 +0200
Subject: [PATCH 1/5] nir: add cluster size and an option to fill in zeros to
 shuffle_up/down

This will be used to optimize open coded clustered inclusive scans.
---
 src/compiler/nir/nir_intrinsics.py | 11 +++++++++--
 1 file changed, 9 insertions(+), 2 deletions(-)

diff --git a/src/compiler/nir/nir_intrinsics.py b/src/compiler/nir/nir_intrinsics.py
index 576f9846ebb9..7cb7dec759fb 100644
--- a/src/compiler/nir/nir_intrinsics.py
+++ b/src/compiler/nir/nir_intrinsics.py
@@ -175,6 +175,10 @@ index("unsigned", "cluster_size")
 # Requires that the operation creates and includes helper invocations
 index("bool", "include_helpers")
 
+# Requires that the operation's result is 0 if the invocation
+# shift is outside of the cluster group
+index("bool", "zero_shuffle_in")
+
 # Parameter index for a load_param intrinsic
 index("unsigned", "param_idx")
 
@@ -481,8 +485,11 @@ intrinsic("ballot_find_msb", src_comp=[4], dest_comp=1, flags=[CAN_ELIMINATE])
 # Shuffle operations from SPIR-V.
 intrinsic("shuffle", src_comp=[0, 1], dest_comp=0, bit_sizes=src0, flags=[CAN_ELIMINATE])
 intrinsic("shuffle_xor", src_comp=[0, 1], dest_comp=0, bit_sizes=src0, flags=[CAN_ELIMINATE])
-intrinsic("shuffle_up", src_comp=[0, 1], dest_comp=0, bit_sizes=src0, flags=[CAN_ELIMINATE])
-intrinsic("shuffle_down", src_comp=[0, 1], dest_comp=0, bit_sizes=src0, flags=[CAN_ELIMINATE])
+
+intrinsic("shuffle_up", src_comp=[0, 1], dest_comp=0, bit_sizes=src0,
+          indices=[CLUSTER_SIZE, ZERO_SHUFFLE_IN], flags=[CAN_ELIMINATE])
+intrinsic("shuffle_down", src_comp=[0, 1], dest_comp=0, bit_sizes=src0,
+          indices=[CLUSTER_SIZE, ZERO_SHUFFLE_IN], flags=[CAN_ELIMINATE])
 
 # Quad operations from SPIR-V.
 intrinsic("quad_broadcast", src_comp=[0, 1], dest_comp=0, flags=[CAN_ELIMINATE])
-- 
GitLab


From ffe33dd42c767fbe2d606157379a5e3303620d78 Mon Sep 17 00:00:00 2001
From: Georg Lehmann <dadschoorse@gmail.com>
Date: Mon, 20 Feb 2023 16:31:06 +0100
Subject: [PATCH 2/5] nir: add a pass to optimize shuffle/booleans dependent
 only on tid/consts

This pass uses constant folding to determine which invocation is read by shuffle
for each invocation. Then, it detects patterns in the result and uses more
a specialized intrinsic if possible.

For booleans it creates inverse_ballot.
---
 src/compiler/nir/meson.build            |   1 +
 src/compiler/nir/nir.h                  |  14 +
 src/compiler/nir/nir_opt_tid_function.c | 596 ++++++++++++++++++++++++
 3 files changed, 611 insertions(+)
 create mode 100644 src/compiler/nir/nir_opt_tid_function.c

diff --git a/src/compiler/nir/meson.build b/src/compiler/nir/meson.build
index b4085789f93b..eda7e8d2e17d 100644
--- a/src/compiler/nir/meson.build
+++ b/src/compiler/nir/meson.build
@@ -269,6 +269,7 @@ files_libnir = files(
   'nir_opt_shrink_stores.c',
   'nir_opt_shrink_vectors.c',
   'nir_opt_sink.c',
+  'nir_opt_tid_function.c',
   'nir_opt_trivial_continues.c',
   'nir_opt_undef.c',
   'nir_opt_uniform_atomics.c',
diff --git a/src/compiler/nir/nir.h b/src/compiler/nir/nir.h
index 119b9efc105e..635c0e9d6f6e 100644
--- a/src/compiler/nir/nir.h
+++ b/src/compiler/nir/nir.h
@@ -5977,6 +5977,20 @@ bool nir_opt_algebraic_late(nir_shader *shader);
 bool nir_opt_algebraic_distribute_src_mods(nir_shader *shader);
 bool nir_opt_constant_folding(nir_shader *shader);
 
+typedef struct nir_opt_tid_function_options {
+   bool use_masked_swizzle_amd : 1;
+   bool use_shuffle_xor : 1;
+   uint8_t rotate_cluster_sizes;
+   uint8_t shuffle_zero_fill_cluster_sizes;
+   uint8_t subgroup_size;
+   uint8_t ballot_bit_size;
+   uint8_t ballot_num_comp;
+} nir_opt_tid_function_options;
+
+bool
+nir_opt_tid_function(nir_shader *shader,
+                     const nir_opt_tid_function_options *options);
+
 /* Try to combine a and b into a.  Return true if combination was possible,
  * which will result in b being removed by the pass.  Return false if
  * combination wasn't possible.
diff --git a/src/compiler/nir/nir_opt_tid_function.c b/src/compiler/nir/nir_opt_tid_function.c
new file mode 100644
index 000000000000..4252f6b24ed2
--- /dev/null
+++ b/src/compiler/nir/nir_opt_tid_function.c
@@ -0,0 +1,596 @@
+/*
+ * Copyright 2023 Valve Corporation
+ * SPDX-License-Identifier: MIT
+ */
+
+#include "nir.h"
+#include "nir_builder.h"
+#include "nir_constant_expressions.h"
+
+#define NIR_MAX_SUBGROUP_SIZE           128
+#define FOTID_MAX_RECURSION_DEPTH 16 /* totally arbitrary */
+
+/* This pass uses constant folding to determine which invocation is read by
+ * shuffle for each invocation. Then, it detects patterns in the result and
+ * uses more a specialized intrinsic if possible.
+ */
+
+static inline unsigned
+src_get_fotid_mask(nir_src src)
+{
+   return src.ssa->parent_instr->pass_flags;
+}
+
+static inline unsigned
+alu_src_get_fotid_mask(nir_alu_instr *instr, unsigned idx)
+{
+   unsigned unswizzled = src_get_fotid_mask(instr->src[idx].src);
+   unsigned result = 0;
+   for (unsigned i = 0; i < nir_ssa_alu_instr_src_components(instr, idx); i++) {
+      bool is_fotid = unswizzled & (1u << instr->src[idx].swizzle[i]);
+      result |= is_fotid << i;
+   }
+   return result;
+}
+
+static void
+update_fotid_alu(nir_builder *b, nir_alu_instr *instr,
+                 const nir_opt_tid_function_options *options)
+{
+   /* For legacy reasons these are ALU instructions
+    * when they should be intrinsics.
+    */
+   switch (instr->op) {
+   case nir_op_fddx:
+   case nir_op_fddy:
+   case nir_op_fddx_fine:
+   case nir_op_fddy_fine:
+   case nir_op_fddx_coarse:
+   case nir_op_fddy_coarse:
+      return;
+   default:
+      break;
+   }
+
+   const nir_op_info *info = &nir_op_infos[instr->op];
+
+   unsigned res = BITFIELD_MASK(instr->def.num_components);
+   for (unsigned i = 0; i < info->num_inputs; i++) {
+      unsigned src_mask = alu_src_get_fotid_mask(instr, i);
+      if (info->input_sizes[i] == 0)
+         res &= src_mask;
+      else if (src_mask != BITFIELD_MASK(info->input_sizes[i]))
+         res = 0;
+   }
+
+   instr->instr.pass_flags = res;
+}
+
+static void
+update_fotid_intrinsic(nir_builder *b, nir_intrinsic_instr *instr,
+                          const nir_opt_tid_function_options *options)
+{
+   switch (instr->intrinsic) {
+   case nir_intrinsic_load_subgroup_invocation: {
+      instr->instr.pass_flags = 1;
+      break;
+   }
+   case nir_intrinsic_load_local_invocation_id: {
+      /* TODO: is this always correct or do we need a callback? */
+      unsigned partial_size = 1;
+      for (unsigned i = 0; i < 3; i++) {
+         partial_size *= b->shader->info.workgroup_size[i];
+         if (partial_size == options->subgroup_size)
+            instr->instr.pass_flags = BITFIELD_MASK(i + 1);
+      }
+      if (partial_size <= options->subgroup_size)
+         instr->instr.pass_flags = 0x7;
+      break;
+   }
+   case nir_intrinsic_load_local_invocation_index: {
+      unsigned workgroup_size = b->shader->info.workgroup_size[0] *
+                                b->shader->info.workgroup_size[1] *
+                                b->shader->info.workgroup_size[2];
+      if (workgroup_size <= options->subgroup_size)
+         instr->instr.pass_flags = 0x1;
+      break;
+   }
+   case nir_intrinsic_inverse_ballot: {
+      if (src_get_fotid_mask(instr->src[0]) ==
+          BITFIELD_MASK(instr->src[0].ssa->num_components)) {
+         instr->instr.pass_flags = 0x1;
+      }
+      break;
+   }
+   default: {
+      break;
+   }
+   }
+}
+
+static void
+update_fotid_load_const(nir_load_const_instr *instr)
+{
+   instr->instr.pass_flags = BITFIELD_MASK(instr->def.num_components);
+}
+
+static bool
+update_fotid_instr(nir_builder *b, nir_instr *instr, void* params)
+{
+   const nir_opt_tid_function_options *options = params;
+
+   /* Gather a mask of components that are functions of tid. */
+   instr->pass_flags = 0;
+
+   switch (instr->type) {
+   case nir_instr_type_alu:
+      update_fotid_alu(b, nir_instr_as_alu(instr), options);
+      break;
+   case nir_instr_type_intrinsic:
+      update_fotid_intrinsic(b, nir_instr_as_intrinsic(instr), options);
+      break;
+   case nir_instr_type_load_const:
+      update_fotid_load_const(nir_instr_as_load_const(instr));
+      break;
+   default:
+      break;
+   }
+
+   return false;
+}
+
+struct ocs_context {
+   const nir_opt_tid_function_options *options;
+   uint8_t invocation_read[NIR_MAX_SUBGROUP_SIZE];
+   bool zero_invocations[NIR_MAX_SUBGROUP_SIZE];
+   nir_shader *shader;
+};
+
+static bool
+constant_fold_scalar(nir_scalar s, unsigned invocation_id,
+                     nir_shader *shader, nir_const_value *dest,
+                     unsigned depth)
+{
+   if (depth > FOTID_MAX_RECURSION_DEPTH)
+      return false;
+
+   memset(dest, 0, sizeof(*dest));
+
+   if (nir_scalar_is_alu(s)) {
+      nir_alu_instr *alu = nir_instr_as_alu(s.def->parent_instr);
+      nir_const_value sources[NIR_MAX_VEC_COMPONENTS][NIR_MAX_VEC_COMPONENTS];
+      const nir_op_info *op_info = &nir_op_infos[alu->op];
+
+      unsigned bit_size = 0;
+      if (!nir_alu_type_get_type_size(op_info->output_type))
+         bit_size = alu->def.bit_size;
+
+      for (unsigned i = 0; i < op_info->num_inputs; i++) {
+         if (!bit_size && !nir_alu_type_get_type_size(op_info->input_types[i]))
+            bit_size = alu->src[i].src.ssa->bit_size;
+
+         unsigned offset = 0;
+         unsigned num_comp = op_info->input_sizes[i];
+         if (num_comp == 0) {
+            num_comp = 1;
+            offset = s.comp;
+         }
+
+         for (unsigned j = 0; j < num_comp; j++) {
+            nir_scalar ss = nir_get_scalar(alu->src[i].src.ssa,
+                                           alu->src[i].swizzle[offset + j]);
+            if (!constant_fold_scalar(ss, invocation_id, shader,
+                                      &sources[i][j], depth + 1))
+               return false;
+         }
+      }
+
+      if (!bit_size)
+         bit_size = 32;
+
+      unsigned exec_mode = shader->info.float_controls_execution_mode;
+
+      nir_const_value *srcs[NIR_MAX_VEC_COMPONENTS];
+      for (unsigned i = 0; i < op_info->num_inputs; ++i)
+         srcs[i] = sources[i];
+      nir_const_value dests[NIR_MAX_VEC_COMPONENTS];
+      if (op_info->output_size == 0) {
+         nir_eval_const_opcode(alu->op, dests, 1, bit_size, srcs, exec_mode);
+         *dest = dests[0];
+      } else {
+         nir_eval_const_opcode(alu->op, dests, s.def->num_components, bit_size, srcs, exec_mode);
+         *dest = dests[s.comp];
+      }
+      return true;
+   } else if (nir_scalar_is_intrinsic(s)) {
+      switch (nir_scalar_intrinsic_op(s)) {
+      case nir_intrinsic_load_subgroup_invocation:
+      case nir_intrinsic_load_local_invocation_index: {
+         *dest = nir_const_value_for_uint(invocation_id, s.def->bit_size);
+         return true;
+      }
+      case nir_intrinsic_load_local_invocation_id: {
+         unsigned local_ids[3];
+         local_ids[2] = invocation_id / (shader->info.workgroup_size[0] * shader->info.workgroup_size[1]);
+         unsigned xy = invocation_id % (shader->info.workgroup_size[0] * shader->info.workgroup_size[1]);
+         local_ids[1] = xy / shader->info.workgroup_size[0];
+         local_ids[0] = xy % shader->info.workgroup_size[0];
+         *dest = nir_const_value_for_uint(local_ids[s.comp], s.def->bit_size);
+         return true;
+      }
+      case nir_intrinsic_inverse_ballot: {
+         nir_def *src = nir_instr_as_intrinsic(s.def->parent_instr)->src[0].ssa;
+         unsigned comp = invocation_id / src->bit_size;
+         unsigned bit = invocation_id % src->bit_size;
+         if (!constant_fold_scalar(nir_get_scalar(src, comp), invocation_id,
+                                   shader, dest, depth + 1))
+            return false;
+         uint64_t ballot = nir_const_value_as_uint(*dest, src->bit_size);
+         *dest = nir_const_value_for_bool(ballot & (1ull << bit), 1);
+         return true;
+      }
+      default:
+         return false;
+      }
+   } else if (nir_scalar_is_const(s)) {
+      *dest = nir_scalar_as_const_value(s);
+      return true;
+   }
+
+   return false;
+}
+
+static bool
+gather_read_invocation_shuffle(nir_def *src,
+                               struct ocs_context *ctx)
+{
+   nir_scalar s = { src, 0 };
+
+   /* Recursive constant folding for each lane */
+   for (unsigned invocation_id = 0;
+        invocation_id < ctx->options->subgroup_size; invocation_id++) {
+      nir_const_value value;
+      if (!constant_fold_scalar(s, invocation_id, ctx->shader, &value, 0))
+         return false;
+      ctx->invocation_read[invocation_id] =
+         MIN2(nir_const_value_as_uint(value, src->bit_size), UINT8_MAX);
+   }
+
+   return true;
+}
+
+static nir_alu_instr *
+gather_invocation_uses(nir_def *def, struct ocs_context *ctx)
+{
+   if (def->num_components != 1 || !list_is_singular(&def->uses))
+      return NULL;
+
+   nir_alu_instr *bcsel = NULL;
+   unsigned src_idx = 0;
+   nir_foreach_use_including_if_safe(src, def) {
+      if (src->is_if || src->parent_instr->type != nir_instr_type_alu)
+         return NULL;
+      bcsel = nir_instr_as_alu(src->parent_instr);
+      if (bcsel->op != nir_op_bcsel)
+         return false;
+      src_idx = list_entry(src, nir_alu_src, src) - bcsel->src;
+      break;
+   }
+
+   assert(src_idx < 3);
+
+   if (src_idx == 0 || !alu_src_get_fotid_mask(bcsel, 0))
+      return NULL;
+
+   nir_scalar s = { bcsel->src[0].src.ssa, bcsel->src[0].swizzle[0] };
+
+   bool return_bcsel = nir_src_is_const(bcsel->src[3 - src_idx].src) &&
+                       nir_src_as_uint(bcsel->src[3 - src_idx].src) == 0;
+
+   /* Recursive constant folding for each lane */
+   for (unsigned invocation_id = 0;
+        invocation_id < ctx->options->subgroup_size; invocation_id++) {
+      nir_const_value value;
+      if (!constant_fold_scalar(s, invocation_id, ctx->shader, &value, 0)) {
+         return_bcsel = false;
+         continue;
+      }
+
+      /* If this lane selects the other source, so we can read an undefined
+       * result. */
+      if (nir_const_value_as_bool(value, 1) == (src_idx != 1)) {
+         ctx->invocation_read[invocation_id] = UINT8_MAX;
+         ctx->zero_invocations[invocation_id] = return_bcsel;
+      }
+   }
+
+   if (return_bcsel) {
+      return bcsel;
+   } else {
+      memset(ctx->zero_invocations, 0, sizeof(ctx->zero_invocations));
+      return NULL;
+   }
+}
+
+static bool
+compute_bitmasks(struct ocs_context *ctx, unsigned *and_mask, unsigned *xor_mask)
+{
+   unsigned one = NIR_MAX_SUBGROUP_SIZE - 1;
+   unsigned zero = NIR_MAX_SUBGROUP_SIZE - 1;
+   unsigned copy = NIR_MAX_SUBGROUP_SIZE - 1;
+   unsigned invert = NIR_MAX_SUBGROUP_SIZE - 1;
+
+   for (unsigned i = 0; i < ctx->options->subgroup_size; i++) {
+      unsigned read = ctx->invocation_read[i];
+      if (read >= ctx->options->subgroup_size)
+         continue; /* undefined result */
+
+      copy &= ~(read ^ i);
+      invert &= read ^ i;
+      one &= read;
+      zero &= ~read;
+   }
+
+   if ((copy | zero | one | invert) != NIR_MAX_SUBGROUP_SIZE - 1) {
+      /* We didn't find valid masks for at least one bit. */
+      return false;
+   }
+
+   *and_mask = copy | invert;
+   *xor_mask = (one | invert) & ~copy;
+
+   return true;
+}
+
+static nir_def *
+try_opt_bitwise_mask(nir_builder *b, nir_def *src_def,
+                     struct ocs_context *ctx)
+{
+   unsigned and_mask, xor_mask;
+   if (!compute_bitmasks(ctx, &and_mask, &xor_mask))
+      return NULL;
+
+#if 0
+   fprintf(stderr, "and %x, xor %x \n", and_mask, xor_mask);
+
+   assert(false);
+#endif
+
+   if ((and_mask & (ctx->options->subgroup_size - 1)) == 0) {
+      return nir_read_invocation(b, src_def, nir_imm_int(b, xor_mask));
+   } else if (and_mask == 0x7f && xor_mask == 0) {
+      return src_def;
+   } else if (ctx->options->use_shuffle_xor && and_mask == 0x7f) {
+      return nir_shuffle_xor(b, src_def, nir_imm_int(b, xor_mask));
+   } else if (ctx->options->use_masked_swizzle_amd &&
+              (and_mask & 0x60) == 0x60 && xor_mask <= 0x1f) {
+      return nir_masked_swizzle_amd(b, src_def,
+                                   (xor_mask << 10) | (and_mask & 0x1f));
+   }
+
+   return NULL;
+}
+
+static nir_def *
+try_opt_rotate(nir_builder *b, nir_def *src_def, struct ocs_context *ctx)
+{
+   u_foreach_bit(i, ctx->options->rotate_cluster_sizes) {
+      unsigned csize = 1u << i;
+      unsigned cmask = csize - 1;
+
+      unsigned delta;
+      for (unsigned invocation = 0; invocation < ctx->options->subgroup_size;
+           invocation++) {
+         if (ctx->invocation_read[invocation] >= ctx->options->subgroup_size)
+            continue;
+
+         if (ctx->invocation_read[invocation] >= invocation)
+            delta = ctx->invocation_read[invocation] - invocation;
+         else
+            delta = csize - invocation + ctx->invocation_read[invocation];
+
+         if (delta < csize && delta != 0)
+            goto delta_found;
+      }
+
+      continue;
+
+   delta_found:
+      for (unsigned invocation = 0; invocation < ctx->options->subgroup_size;
+           invocation++) {
+         if (ctx->invocation_read[invocation] >= ctx->options->subgroup_size)
+            continue;
+         unsigned read =
+            ((invocation + delta) & cmask) + (invocation & ~cmask);
+         if (read != ctx->invocation_read[invocation])
+            goto continue_outerloop;
+      }
+
+      return nir_rotate(b, src_def, nir_imm_int(b, delta),
+                        .execution_scope = SCOPE_SUBGROUP,
+                        .cluster_size = csize);
+
+   continue_outerloop:
+   }
+
+   return NULL;
+}
+
+static nir_def *
+try_opt_shuffle_up_down(nir_builder *b, nir_def *src_def,
+                        struct ocs_context *ctx)
+{
+   u_foreach_bit(i, ctx->options->shuffle_zero_fill_cluster_sizes) {
+      int csize = 1 << i;
+      unsigned cmask = csize - 1;
+
+      int delta;
+      for (unsigned invocation = 0; invocation < ctx->options->subgroup_size;
+           invocation++) {
+         if (ctx->invocation_read[invocation] >= ctx->options->subgroup_size)
+            continue;
+
+         delta = ctx->invocation_read[invocation] - invocation;
+
+         if (delta < csize && delta > -csize && delta != 0)
+            goto delta_found;
+      }
+
+      continue;
+
+   delta_found:
+      for (unsigned invocation = 0; invocation < ctx->options->subgroup_size;
+           invocation++) {
+         int read = invocation + delta;
+         bool out_of_bounds = (read & ~cmask) != (invocation & ~cmask);
+         if (ctx->zero_invocations[invocation] && !out_of_bounds)
+            goto continue_outerloop;
+         if (ctx->invocation_read[invocation] >= ctx->options->subgroup_size)
+            continue;
+         if (read != ctx->invocation_read[invocation] || out_of_bounds)
+            goto continue_outerloop;
+      }
+
+      if (delta < 0)
+         return nir_shuffle_up(b, src_def, nir_imm_int(b, -delta),
+                               .cluster_size = csize,
+                               .zero_shuffle_in = true);
+      else
+         return nir_shuffle_down(b, src_def, nir_imm_int(b, delta),
+                                 .cluster_size = csize,
+                                 .zero_shuffle_in = true);
+
+   continue_outerloop:
+   }
+
+   return NULL;
+}
+
+static bool
+opt_fotid_shuffle(nir_builder *b, nir_intrinsic_instr *instr,
+                  const nir_opt_tid_function_options *options)
+{
+   struct ocs_context ctx = {
+      .options = options,
+      .zero_invocations = {},
+      .shader = b->shader,
+   };
+
+   memset(ctx.invocation_read, 0xff, sizeof(ctx.invocation_read));
+
+   if (!gather_read_invocation_shuffle(instr->src[1].ssa, &ctx))
+      return false;
+
+
+   /* Generalize invocation_read by taking into account which lanes
+    * do not use the shuffle result because of bcsel.
+    */
+   nir_alu_instr *bcsel = gather_invocation_uses(&instr->def, &ctx);
+
+#if 0
+   for (int i = 0; i < options->subgroup_size; i++) {
+      fprintf(stderr, "lane %d reads %d\n", i, ctx.invocation_read[i]);
+   }
+
+   for (int i = 0; i < options->subgroup_size; i++) {
+      fprintf(stderr, "lane %d zero %d\n", i, ctx.zero_invocations[i]);
+   }
+#endif
+
+   b->cursor = nir_after_instr(&instr->instr);
+
+   nir_def *res = NULL;
+
+   if (bcsel) {
+      res = try_opt_shuffle_up_down(b, instr->src[0].ssa, &ctx);
+      if (res) {
+         nir_def_rewrite_uses(&bcsel->def, res);
+         nir_instr_remove(&bcsel->instr);
+         nir_instr_remove(&instr->instr);
+         return true;
+      }
+   }
+
+   if (!res)
+      res = try_opt_bitwise_mask(b, instr->src[0].ssa, &ctx);
+   if (!res)
+      res = try_opt_rotate(b, instr->src[0].ssa, &ctx);
+
+   if (res) {
+      nir_def_rewrite_uses(&instr->def, res);
+      nir_instr_remove(&instr->instr);
+      return true;
+   } else {
+      return false;
+   }
+}
+
+static bool
+opt_fotid_bool(nir_builder *b, nir_alu_instr *instr,
+               const nir_opt_tid_function_options *options)
+{
+   nir_scalar s = { &instr->def, 0 };
+
+   b->cursor = nir_after_instr(&instr->instr);
+
+   nir_def *ballot_comp[NIR_MAX_VEC_COMPONENTS];
+
+   for (unsigned comp = 0; comp < options->ballot_num_comp; comp++) {
+      uint64_t cballot = 0;
+      for (unsigned invocation_id = comp * options->ballot_bit_size;
+           invocation_id < options->subgroup_size; invocation_id++) {
+         nir_const_value value;
+         if (!constant_fold_scalar(s, invocation_id, b->shader, &value, 0))
+            return false;
+         cballot |= nir_const_value_as_uint(value, 1) << invocation_id;
+      }
+      ballot_comp[comp] = nir_imm_intN_t(b, cballot, options->ballot_bit_size);
+   }
+
+   nir_def *ballot = nir_vec(b, ballot_comp, options->ballot_num_comp);
+   nir_def *res = nir_inverse_ballot(b, 1, ballot);
+   res->parent_instr->pass_flags = 1;
+
+   nir_def_rewrite_uses(&instr->def, res);
+   nir_instr_remove(&instr->instr);
+   return true;
+}
+
+static bool
+visit_instr(nir_builder *b, nir_instr *instr, void *params)
+{
+   const nir_opt_tid_function_options *options = params;
+
+   switch (instr->type) {
+   case nir_instr_type_alu: {
+      nir_alu_instr *alu = nir_instr_as_alu(instr);
+      if (!options->ballot_bit_size || !options->ballot_num_comp)
+         return false;
+      if (alu->def.bit_size != 1 || alu->def.num_components > 1 || !instr->pass_flags)
+         return false;
+      return opt_fotid_bool(b, alu, options);
+   }
+   case nir_instr_type_intrinsic: {
+      nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
+      if (intrin->intrinsic != nir_intrinsic_shuffle)
+         return false;
+      if (!intrin->src[1].ssa->parent_instr->pass_flags)
+         return false;
+      return opt_fotid_shuffle(b, intrin, options);
+   }
+   default:
+      return false;
+   }
+}
+
+bool
+nir_opt_tid_function(nir_shader *shader,
+                     const nir_opt_tid_function_options *options)
+{
+   nir_shader_instructions_pass(shader, update_fotid_instr,
+                                nir_metadata_none, (void *)options);
+
+   return nir_shader_instructions_pass(
+      shader, visit_instr,
+      nir_metadata_block_index | nir_metadata_dominance, (void *)options);
+}
-- 
GitLab


From cb1b20c5bcfa328c7e516b443210d2ff393f072f Mon Sep 17 00:00:00 2001
From: Georg Lehmann <dadschoorse@gmail.com>
Date: Sat, 12 Aug 2023 16:49:00 +0200
Subject: [PATCH 3/5] aco: implement constant rotate/shuffle up/down with
 cluster size 16

---
 .../compiler/aco_instruction_selection.cpp    | 86 +++++++++++++++++++
 1 file changed, 86 insertions(+)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 59ece2d037d3..baa97950f678 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -8504,6 +8504,92 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
       set_wqm(ctx, create_helpers);
       break;
    }
+   case nir_intrinsic_rotate: {
+      /* Only the special cases used by nir_opt_tid_function are implemented. */
+      Temp src = get_ssa_temp(ctx, instr->src[0].ssa);
+      Temp dst = get_ssa_temp(ctx, &instr->def);
+      assert(nir_intrinsic_execution_scope(instr) == SCOPE_SUBGROUP);
+      assert(nir_intrinsic_cluster_size(instr) == 16);
+      assert(nir_src_is_const(instr->src[1]));
+      assert(ctx->program->gfx_level >= GFX8);
+
+      uint64_t delta = nir_src_as_uint(instr->src[1]);
+      unsigned cluster_size = nir_intrinsic_cluster_size(instr);
+      delta &= cluster_size - 1;
+
+      if (delta == 0) {
+         bld.copy(Definition(dst), src);
+         break;
+      }
+
+      if (instr->def.bit_size == 1)
+         src = bld.vop2_e64(aco_opcode::v_cndmask_b32, bld.def(v1), Operand::zero(),
+                            Operand::c32(-1), src);
+
+      src = as_vgpr(ctx, src);
+
+      /* NIR rotates in the opposite direction compared to dpp_row_rr. */
+      uint16_t dpp_ctrl = dpp_row_rr(cluster_size - delta);
+      if (instr->def.bit_size == 1) {
+         Temp tmp = bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(src.regClass()), src, dpp_ctrl);
+         bld.vopc(aco_opcode::v_cmp_lg_u32, Definition(dst), Operand::zero(), tmp);
+      } else if (instr->def.bit_size == 64) {
+         Temp lo = bld.tmp(v1), hi = bld.tmp(v1);
+         bld.pseudo(aco_opcode::p_split_vector, Definition(lo), Definition(hi), src);
+         lo = bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), lo, dpp_ctrl);
+         hi = bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), hi, dpp_ctrl);
+         bld.pseudo(aco_opcode::p_create_vector, Definition(dst), lo, hi);
+         emit_split_vector(ctx, dst, 2);
+      } else {
+         bld.vop1_dpp(aco_opcode::v_mov_b32, Definition(dst), src, dpp_ctrl);
+      }
+      set_wqm(ctx);
+      break;
+   }
+   case nir_intrinsic_shuffle_up:
+   case nir_intrinsic_shuffle_down: {
+      /* Only the special cases used by nir_opt_tid_function are implemented. */
+      Temp src = get_ssa_temp(ctx, instr->src[0].ssa);
+      Temp dst = get_ssa_temp(ctx, &instr->def);
+      assert(nir_intrinsic_cluster_size(instr) == 16);
+      assert(nir_src_is_const(instr->src[1]));
+      assert(ctx->program->gfx_level >= GFX8);
+
+      uint64_t delta = nir_src_as_uint(instr->src[1]);
+
+      if (delta >= 16) {
+         bld.copy(Definition(dst), Operand::zero(dst.bytes()));
+         break;
+      } else if (delta == 0) {
+         bld.copy(Definition(dst), src);
+         break;
+      }
+
+      if (instr->def.bit_size == 1)
+         src = bld.vop2_e64(aco_opcode::v_cndmask_b32, bld.def(v1), Operand::zero(),
+                            Operand::c32(-1), src);
+
+      src = as_vgpr(ctx, src);
+
+      uint16_t dpp_ctrl =
+         instr->intrinsic == nir_intrinsic_shuffle_up ? dpp_row_sr(delta) : dpp_row_sl(delta);
+
+      if (instr->def.bit_size == 1) {
+         Temp tmp = bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(src.regClass()), src, dpp_ctrl);
+         bld.vopc(aco_opcode::v_cmp_lg_u32, Definition(dst), Operand::zero(), tmp);
+      } else if (instr->def.bit_size == 64) {
+         Temp lo = bld.tmp(v1), hi = bld.tmp(v1);
+         bld.pseudo(aco_opcode::p_split_vector, Definition(lo), Definition(hi), src);
+         lo = bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), lo, dpp_ctrl);
+         hi = bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), hi, dpp_ctrl);
+         bld.pseudo(aco_opcode::p_create_vector, Definition(dst), lo, hi);
+         emit_split_vector(ctx, dst, 2);
+      } else {
+         bld.vop1_dpp(aco_opcode::v_mov_b32, Definition(dst), src, dpp_ctrl);
+      }
+      set_wqm(ctx);
+      break;
+   }
    case nir_intrinsic_quad_broadcast:
    case nir_intrinsic_quad_swap_horizontal:
    case nir_intrinsic_quad_swap_vertical:
-- 
GitLab


From af9bc96655597a0db2a5444f8b49d932edea0818 Mon Sep 17 00:00:00 2001
From: Georg Lehmann <dadschoorse@gmail.com>
Date: Sat, 12 Aug 2023 16:51:21 +0200
Subject: [PATCH 4/5] radv: use nir_opt_tid_function for shuffles

The main motivation were open coded clustered inclusive scans
and clustered broadcasts in the gdeflate decompression shader used by
DirectStorage.

Foz-DB Navi21 (only the_last_of_us_part1 is affected):
Totals from 8 (0.01% of 76572) affected shaders:
Instrs: 6492 -> 5700 (-12.20%)
CodeSize: 34024 -> 29760 (-12.53%)
Latency: 81559 -> 77871 (-4.52%)
InvThroughput: 10037 -> 9131 (-9.03%); split: -9.04%, +0.01%
SClause: 324 -> 325 (+0.31%)
Copies: 773 -> 776 (+0.39%)
PreSGPRs: 553 -> 549 (-0.72%)
PreVGPRs: 239 -> 237 (-0.84%)
---
 src/amd/vulkan/radv_pipeline.c | 10 ++++++++++
 1 file changed, 10 insertions(+)

diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index 9766f322ec80..d555a2be7cb3 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -543,6 +543,16 @@ radv_postprocess_nir(struct radv_device *device, const struct radv_pipeline_key
       NIR_PASS(_, stage->nir, radv_nir_lower_fs_intrinsics, stage, pipeline_key);
    }
 
+   if (!radv_use_llvm_for_stage(device, stage->stage)) {
+      nir_opt_tid_function_options options = {
+         .use_masked_swizzle_amd = true,
+         .rotate_cluster_sizes = gfx_level >= GFX8 ? 16 : 0,
+         .shuffle_zero_fill_cluster_sizes = gfx_level >= GFX8 ? 16 : 0,
+         .subgroup_size = stage->info.wave_size,
+      };
+      NIR_PASS(_, stage->nir, nir_opt_tid_function, &options);
+   }
+
    enum nir_lower_non_uniform_access_type lower_non_uniform_access_types =
       nir_lower_non_uniform_ubo_access | nir_lower_non_uniform_ssbo_access | nir_lower_non_uniform_texture_access |
       nir_lower_non_uniform_image_access;
-- 
GitLab


From cd0eb2cb550bb00b7d44255d894a389bf2e7f4cc Mon Sep 17 00:00:00 2001
From: Georg Lehmann <dadschoorse@gmail.com>
Date: Wed, 13 Sep 2023 14:14:05 +0200
Subject: [PATCH 5/5] radv: use nir_opt_tid_function to create inverse_ballot

Foz-DB Navi21:
Totals from 564 (0.74% of 76572) affected shaders:
MaxWaves: 13921 -> 13923 (+0.01%)
Instrs: 622888 -> 624533 (+0.26%); split: -0.05%, +0.31%
CodeSize: 3317976 -> 3289844 (-0.85%); split: -1.11%, +0.27%
VGPRs: 22328 -> 22272 (-0.25%); split: -0.32%, +0.07%
SpillSGPRs: 149 -> 164 (+10.07%)
Latency: 5896948 -> 5898656 (+0.03%); split: -0.03%, +0.06%
InvThroughput: 1074693 -> 1071459 (-0.30%); split: -0.34%, +0.04%
VClause: 20584 -> 20588 (+0.02%)
SClause: 23372 -> 23297 (-0.32%); split: -0.38%, +0.06%
Copies: 64628 -> 66460 (+2.83%); split: -0.00%, +2.84%
Branches: 21731 -> 21927 (+0.90%); split: -0.03%, +0.93%
PreSGPRs: 23082 -> 23111 (+0.13%); split: -0.03%, +0.16%
PreVGPRs: 18658 -> 18564 (-0.50%)
---
 src/amd/vulkan/radv_pipeline.c | 2 ++
 1 file changed, 2 insertions(+)

diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index d555a2be7cb3..e618dec8f7f2 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -549,6 +549,8 @@ radv_postprocess_nir(struct radv_device *device, const struct radv_pipeline_key
          .rotate_cluster_sizes = gfx_level >= GFX8 ? 16 : 0,
          .shuffle_zero_fill_cluster_sizes = gfx_level >= GFX8 ? 16 : 0,
          .subgroup_size = stage->info.wave_size,
+         .ballot_bit_size = stage->info.wave_size,
+         .ballot_num_comp = 1,
       };
       NIR_PASS(_, stage->nir, nir_opt_tid_function, &options);
    }
-- 
GitLab

