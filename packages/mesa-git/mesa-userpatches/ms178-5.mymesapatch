--- a/src/amd/compiler/aco_lower_to_cssa.cpp	2025-05-30 14:11:47.936806808 +0200
+++ b/src/amd/compiler/aco_lower_to_cssa.cpp	2025-05-30 15:44:30.928163674 +0200
@@ -1,9 +1,3 @@
-/*
- * Copyright Â© 2019 Valve Corporation
- *
- * SPDX-License-Identifier: MIT
- */
-
 #include "aco_builder.h"
 #include "aco_ir.h"
 
@@ -24,543 +18,551 @@
  */
 
 namespace aco {
-namespace {
+      namespace {
 
-typedef std::vector<Temp> merge_set;
+            typedef std::vector<Temp> merge_set;
 
-struct copy {
-   Definition def;
-   Operand op;
-};
-
-struct merge_node {
-   Operand value = Operand(); /* original value: can be an SSA-def or constant value */
-   uint32_t index = -1u;      /* index into the vector of merge sets */
-   uint32_t defined_at = -1u; /* defining block */
-
-   /* We also remember two closest equal intersecting ancestors. Because they intersect with this
-    * merge node, they must dominate it (intersection isn't possible otherwise) and have the same
-    * value (or else they would not be allowed to be in the same merge set).
-    */
-   Temp equal_anc_in = Temp();  /* within the same merge set */
-   Temp equal_anc_out = Temp(); /* from the other set we're currently trying to merge with */
-};
-
-struct cssa_ctx {
-   Program* program;
-   std::vector<std::vector<copy>> parallelcopies; /* copies per block */
-   std::vector<merge_set> merge_sets;             /* each vector is one (ordered) merge set */
-   std::unordered_map<uint32_t, merge_node> merge_node_table; /* tempid -> merge node */
-};
-
-/* create (virtual) parallelcopies for each phi instruction and
- * already merge copy-definitions with phi-defs into merge sets */
-void
-collect_parallelcopies(cssa_ctx& ctx)
-{
-   ctx.parallelcopies.resize(ctx.program->blocks.size());
-   Builder bld(ctx.program);
-   for (Block& block : ctx.program->blocks) {
-      for (aco_ptr<Instruction>& phi : block.instructions) {
-         if (phi->opcode != aco_opcode::p_phi && phi->opcode != aco_opcode::p_linear_phi)
-            break;
-
-         const Definition& def = phi->definitions[0];
-
-         /* if the definition is not temp, it is the exec mask.
-          * We can reload the exec mask directly from the spill slot.
-          */
-         if (!def.isTemp() || def.isKill())
-            continue;
-
-         Block::edge_vec& preds =
-            phi->opcode == aco_opcode::p_phi ? block.logical_preds : block.linear_preds;
-         uint32_t index = ctx.merge_sets.size();
-         merge_set set;
-
-         bool has_preheader_copy = false;
-         for (unsigned i = 0; i < phi->operands.size(); i++) {
-            Operand op = phi->operands[i];
-            if (op.isUndefined())
-               continue;
-
-            if (def.regClass().type() == RegType::sgpr && !op.isTemp()) {
-               /* SGPR inline constants and literals on GFX10+ can be spilled
-                * and reloaded directly (without intermediate register) */
-               if (op.isConstant()) {
-                  if (ctx.program->gfx_level >= GFX10)
-                     continue;
-                  if (op.size() == 1 && !op.isLiteral())
-                     continue;
-               } else {
-                  assert(op.isFixed() && op.physReg() == exec);
-                  continue;
-               }
+            struct copy {
+                  Definition def;
+                  Operand op;
+            };
+
+            struct merge_node {
+                  Operand value = Operand(); /* original value: can be an SSA-def or constant value */
+                  uint32_t index = -1u;      /* index into the vector of merge sets */
+                  uint32_t defined_at = -1u; /* defining block */
+
+                  Temp equal_anc_in = Temp();  /* within the same merge set */
+                  Temp equal_anc_out = Temp(); /* from the other set we're currently trying to merge with */
+            };
+
+            struct cssa_ctx {
+                  Program* program;
+                  std::vector<std::vector<copy>> parallelcopies; /* copies per block */
+                  std::vector<merge_set> merge_sets;             /* each vector is one (ordered) merge set */
+                  std::unordered_map<uint32_t, merge_node> merge_node_table; /* tempid -> merge node */
+            };
+
+            void
+            collect_parallelcopies(cssa_ctx& ctx)
+            {
+                  ctx.parallelcopies.resize(ctx.program->blocks.size());
+                  // Heuristic reservation for merge_sets assuming on average up to 1 new set per block with phis.
+                  if (ctx.program->blocks.size() > 0) {
+                        ctx.merge_sets.reserve(ctx.program->blocks.size());
+                  }
+
+                  Builder bld(ctx.program);
+                  for (Block& block : ctx.program->blocks) {
+                        for (aco_ptr<Instruction>& phi : block.instructions) {
+                              if (phi->opcode != aco_opcode::p_phi && phi->opcode != aco_opcode::p_linear_phi) [[unlikely]]
+                                    break;
+
+                              const Definition& def = phi->definitions[0];
+
+                              if (!def.isTemp() || def.isKill()) [[unlikely]]
+                                    continue;
+
+                              Block::edge_vec& preds =
+                              phi->opcode == aco_opcode::p_phi ? block.logical_preds : block.linear_preds;
+                              uint32_t index = ctx.merge_sets.size();
+                              merge_set set;
+                              if (phi->operands.size() > 0) {
+                                    set.reserve(phi->operands.size() + 1);
+                              }
+
+
+                              bool has_preheader_copy = false;
+                              for (unsigned i = 0; i < phi->operands.size(); i++) {
+                                    Operand op = phi->operands[i];
+                                    if (op.isUndefined()) [[unlikely]]
+                                          continue;
+
+                                    if (def.regClass().type() == RegType::sgpr && !op.isTemp()) {
+                                          if (op.isConstant()) {
+                                                if (ctx.program->gfx_level >= GFX10)
+                                                      continue;
+                                                if (op.size() == 1 && !op.isLiteral())
+                                                      continue;
+                                          } else {
+                                                assert(op.isFixed() && op.physReg() == exec);
+                                                continue;
+                                          }
+                                    }
+
+                                    if (preds[i] < ctx.parallelcopies.size() && ctx.parallelcopies[preds[i]].empty()) {
+                                          ctx.parallelcopies[preds[i]].reserve(4);
+                                    }
+
+                                    Temp tmp = bld.tmp(def.regClass());
+                                    ctx.parallelcopies[preds[i]].emplace_back(copy{Definition(tmp), op});
+                                    phi->operands[i] = Operand(tmp);
+                                    phi->operands[i].setKill(true);
+
+                                    set.emplace_back(tmp);
+                                    ctx.merge_node_table[tmp.id()] = {op, index, preds[i]};
+
+                                    has_preheader_copy |= (i == 0 && (block.kind & block_kind_loop_header));
+                              }
+
+                              if (set.empty()) [[unlikely]]
+                                    continue;
+
+                              if (def.isTemp()) {
+                                    if (has_preheader_copy)
+                                          set.emplace(std::next(set.begin()), def.getTemp());
+                                    else if (block.kind & block_kind_loop_header)
+                                          set.emplace(set.begin(), def.getTemp());
+                                    else
+                                          set.emplace_back(def.getTemp());
+                                    ctx.merge_node_table[def.tempId()] = {Operand(def.getTemp()), index, block.index};
+                              }
+                              ctx.merge_sets.emplace_back(std::move(set));
+                        }
+                  }
             }
 
-            /* create new temporary and rename operands */
-            Temp tmp = bld.tmp(def.regClass());
-            ctx.parallelcopies[preds[i]].emplace_back(copy{Definition(tmp), op});
-            phi->operands[i] = Operand(tmp);
-            phi->operands[i].setKill(true);
-
-            /* place the new operands in the same merge set */
-            set.emplace_back(tmp);
-            ctx.merge_node_table[tmp.id()] = {op, index, preds[i]};
-
-            has_preheader_copy |= i == 0 && block.kind & block_kind_loop_header;
-         }
-
-         if (set.empty())
-            continue;
-
-         /* place the definition in dominance-order */
-         if (def.isTemp()) {
-            if (has_preheader_copy)
-               set.emplace(std::next(set.begin()), def.getTemp());
-            else if (block.kind & block_kind_loop_header)
-               set.emplace(set.begin(), def.getTemp());
-            else
-               set.emplace_back(def.getTemp());
-            ctx.merge_node_table[def.tempId()] = {Operand(def.getTemp()), index, block.index};
-         }
-         ctx.merge_sets.emplace_back(set);
-      }
-   }
-}
+            /* check whether the definition of a comes after b. */
+            inline bool
+            defined_after(cssa_ctx& ctx, Temp a, Temp b)
+            {
+                  merge_node& node_a = ctx.merge_node_table[a.id()];
+                  merge_node& node_b = ctx.merge_node_table[b.id()];
+                  if (node_a.defined_at == node_b.defined_at)
+                        return a.id() > b.id();
 
-/* check whether the definition of a comes after b. */
-inline bool
-defined_after(cssa_ctx& ctx, Temp a, Temp b)
-{
-   merge_node& node_a = ctx.merge_node_table[a.id()];
-   merge_node& node_b = ctx.merge_node_table[b.id()];
-   if (node_a.defined_at == node_b.defined_at)
-      return a.id() > b.id();
-
-   return node_a.defined_at > node_b.defined_at;
-}
-
-/* check whether a dominates b where b is defined after a */
-inline bool
-dominates(cssa_ctx& ctx, Temp a, Temp b)
-{
-   assert(defined_after(ctx, b, a));
-   Block& parent = ctx.program->blocks[ctx.merge_node_table[a.id()].defined_at];
-   Block& child = ctx.program->blocks[ctx.merge_node_table[b.id()].defined_at];
-   if (b.regClass().type() == RegType::vgpr)
-      return dominates_logical(parent, child);
-   else
-      return dominates_linear(parent, child);
-}
-
-/* Checks whether some variable is live-out, not considering any phi-uses. */
-inline bool
-is_live_out(cssa_ctx& ctx, Temp var, uint32_t block_idx)
-{
-   Block::edge_vec& succs = var.is_linear() ? ctx.program->blocks[block_idx].linear_succs
-                                            : ctx.program->blocks[block_idx].logical_succs;
-
-   return std::any_of(succs.begin(), succs.end(), [&](unsigned succ)
-                      { return ctx.program->live.live_in[succ].count(var.id()); });
-}
-
-/* check intersection between var and parent:
- * We already know that parent dominates var. */
-inline bool
-intersects(cssa_ctx& ctx, Temp var, Temp parent)
-{
-   merge_node& node_var = ctx.merge_node_table[var.id()];
-   merge_node& node_parent = ctx.merge_node_table[parent.id()];
-   assert(node_var.index != node_parent.index);
-   uint32_t block_idx = node_var.defined_at;
-
-   /* if parent is defined in a different block than var */
-   if (node_parent.defined_at < node_var.defined_at) {
-      /* if the parent is not live-in, they don't interfere */
-      if (!ctx.program->live.live_in[block_idx].count(parent.id()))
-         return false;
-   }
-
-   /* if the parent is live-out at the definition block of var, they intersect */
-   bool parent_live = is_live_out(ctx, parent, block_idx);
-   if (parent_live)
-      return true;
-
-   for (const copy& cp : ctx.parallelcopies[block_idx]) {
-      /* if var is defined at the edge, they don't intersect */
-      if (cp.def.getTemp() == var)
-         return false;
-      if (cp.op.isTemp() && cp.op.getTemp() == parent)
-         parent_live = true;
-   }
-   /* if the parent is live at the edge, they intersect */
-   if (parent_live)
-      return true;
-
-   /* both, parent and var, are present in the same block */
-   const Block& block = ctx.program->blocks[block_idx];
-   for (auto it = block.instructions.crbegin(); it != block.instructions.crend(); ++it) {
-      /* if the parent was not encountered yet, it can only be used by a phi */
-      if (is_phi(it->get()))
-         break;
-
-      for (const Definition& def : (*it)->definitions) {
-         if (!def.isTemp())
-            continue;
-         /* if parent was not found yet, they don't intersect */
-         if (def.getTemp() == var)
-            return false;
-      }
+                  return node_a.defined_at > node_b.defined_at;
+            }
 
-      for (const Operand& op : (*it)->operands) {
-         if (!op.isTemp())
-            continue;
-         /* if the var was defined before this point, they intersect */
-         if (op.getTemp() == parent)
-            return true;
-      }
-   }
+            /* check whether a dominates b where b is defined after a */
+            inline bool
+            dominates(cssa_ctx& ctx, Temp a, Temp b)
+            {
+                  assert(defined_after(ctx, b, a));
+                  Block& parent = ctx.program->blocks[ctx.merge_node_table[a.id()].defined_at];
+                  Block& child = ctx.program->blocks[ctx.merge_node_table[b.id()].defined_at];
+                  if (b.regClass().type() == RegType::vgpr)
+                        return dominates_logical(parent, child);
+                  else
+                        return dominates_linear(parent, child);
+            }
 
-   return false;
-}
+            /* Checks whether some variable is live-out, not considering any phi-uses. */
+            inline bool
+            is_live_out(cssa_ctx& ctx, Temp var, uint32_t block_idx)
+            {
+                  Block::edge_vec& succs = var.is_linear() ? ctx.program->blocks[block_idx].linear_succs
+                  : ctx.program->blocks[block_idx].logical_succs;
 
-/* check interference between var and parent:
- * i.e. they have different values and intersect.
- * If parent and var intersect and share the same value, also updates the equal ancestor. */
-inline bool
-interference(cssa_ctx& ctx, Temp var, Temp parent)
-{
-   assert(var != parent);
-   merge_node& node_var = ctx.merge_node_table[var.id()];
-   node_var.equal_anc_out = Temp();
-
-   if (node_var.index == ctx.merge_node_table[parent.id()].index) {
-      /* Check/update in other set. equal_anc_out is only present if it intersects with 'parent',
-       * but that's fine since it has to for it to intersect with 'var'. */
-      parent = ctx.merge_node_table[parent.id()].equal_anc_out;
-   }
-
-   Temp tmp = parent;
-   /* Check if 'var' intersects with 'parent' or any ancestors which might intersect too. */
-   while (tmp != Temp() && !intersects(ctx, var, tmp)) {
-      merge_node& node_tmp = ctx.merge_node_table[tmp.id()];
-      tmp = node_tmp.equal_anc_in;
-   }
-
-   /* no intersection found */
-   if (tmp == Temp())
-      return false;
-
-   /* var and parent, same value and intersect, but in different sets */
-   if (node_var.value == ctx.merge_node_table[parent.id()].value) {
-      node_var.equal_anc_out = tmp;
-      return false;
-   }
-
-   /* var and parent, different values and intersect */
-   return true;
-}
-
-/* tries to merge set_b into set_a of given temporary and
- * drops that temporary as it is being coalesced */
-bool
-try_merge_merge_set(cssa_ctx& ctx, Temp dst, merge_set& set_b)
-{
-   auto def_node_it = ctx.merge_node_table.find(dst.id());
-   uint32_t index = def_node_it->second.index;
-   merge_set& set_a = ctx.merge_sets[index];
-   std::vector<Temp> dom; /* stack of the traversal */
-   merge_set union_set;   /* the new merged merge-set */
-   uint32_t i_a = 0;
-   uint32_t i_b = 0;
-
-   while (i_a < set_a.size() || i_b < set_b.size()) {
-      Temp current;
-      if (i_a == set_a.size())
-         current = set_b[i_b++];
-      else if (i_b == set_b.size())
-         current = set_a[i_a++];
-      /* else pick the one defined first */
-      else if (defined_after(ctx, set_a[i_a], set_b[i_b]))
-         current = set_b[i_b++];
-      else
-         current = set_a[i_a++];
-
-      while (!dom.empty() && !dominates(ctx, dom.back(), current))
-         dom.pop_back(); /* not the desired parent, remove */
-
-      if (!dom.empty() && interference(ctx, current, dom.back())) {
-         for (Temp t : union_set)
-            ctx.merge_node_table[t.id()].equal_anc_out = Temp();
-         return false; /* intersection detected */
-      }
+                  return std::any_of(succs.begin(), succs.end(), [&](unsigned succ)
+                  { return ctx.program->live.live_in[succ].count(var.id()); });
+            }
 
-      dom.emplace_back(current); /* otherwise, keep checking */
-      if (current != dst)
-         union_set.emplace_back(current); /* maintain the new merge-set sorted */
-   }
-
-   /* update hashmap */
-   for (Temp t : union_set) {
-      merge_node& node = ctx.merge_node_table[t.id()];
-      /* update the equal ancestors:
-       * i.e. the 'closest' dominating def which intersects */
-      Temp in = node.equal_anc_in;
-      Temp out = node.equal_anc_out;
-      if (in == Temp() || (out != Temp() && defined_after(ctx, out, in)))
-         node.equal_anc_in = out;
-      node.equal_anc_out = Temp();
-      /* update merge-set index */
-      node.index = index;
-   }
-   set_b = merge_set(); /* free the old set_b */
-   ctx.merge_sets[index] = union_set;
-   ctx.merge_node_table.erase(dst.id()); /* remove the temporary */
-
-   return true;
-}
-
-/* returns true if the copy can safely be omitted */
-bool
-try_coalesce_copy(cssa_ctx& ctx, copy copy, uint32_t block_idx)
-{
-   /* we can only coalesce temporaries */
-   if (!copy.op.isTemp() || !copy.op.isKill())
-      return false;
-
-   /* we can only coalesce copies of the same register class */
-   if (copy.op.regClass() != copy.def.regClass())
-      return false;
-
-   /* try emplace a merge_node for the copy operand */
-   merge_node& op_node = ctx.merge_node_table[copy.op.tempId()];
-   if (op_node.defined_at == -1u) {
-      /* find defining block of operand */
-      while (ctx.program->live.live_in[block_idx].count(copy.op.tempId()))
-         block_idx = copy.op.regClass().type() == RegType::vgpr
-                        ? ctx.program->blocks[block_idx].logical_idom
-                        : ctx.program->blocks[block_idx].linear_idom;
-      op_node.defined_at = block_idx;
-      op_node.value = copy.op;
-   }
-
-   /* check if this operand has not yet been coalesced */
-   if (op_node.index == -1u) {
-      merge_set op_set = merge_set{copy.op.getTemp()};
-      return try_merge_merge_set(ctx, copy.def.getTemp(), op_set);
-   }
-
-   /* check if this operand has been coalesced into the same set */
-   assert(ctx.merge_node_table.count(copy.def.tempId()));
-   if (op_node.index == ctx.merge_node_table[copy.def.tempId()].index)
-      return true;
-
-   /* otherwise, try to coalesce both merge sets */
-   return try_merge_merge_set(ctx, copy.def.getTemp(), ctx.merge_sets[op_node.index]);
-}
-
-/* node in the location-transfer-graph */
-struct ltg_node {
-   copy* cp;
-   uint32_t read_idx;
-   uint32_t num_uses = 0;
-};
-
-/* emit the copies in an order that does not
- * create interferences within a merge-set */
-void
-emit_copies_block(Builder& bld, std::map<uint32_t, ltg_node>& ltg, RegType type)
-{
-   RegisterDemand live_changes;
-   RegisterDemand reg_demand = bld.it->get()->register_demand - get_temp_registers(bld.it->get()) -
-                               get_live_changes(bld.it->get());
-   auto&& it = ltg.begin();
-   while (it != ltg.end()) {
-      copy& cp = *it->second.cp;
-
-      /* wrong regclass or still needed as operand */
-      if (cp.def.regClass().type() != type || it->second.num_uses > 0) {
-         ++it;
-         continue;
-      }
+            /* check intersection between var and parent:
+             * We already know that parent dominates var. */
+            inline bool
+            intersects(cssa_ctx& ctx, Temp var, Temp parent)
+            {
+                  merge_node& node_var = ctx.merge_node_table[var.id()];
+                  merge_node& node_parent = ctx.merge_node_table[parent.id()];
+                  assert(node_var.index != node_parent.index);
+                  uint32_t block_idx = node_var.defined_at;
+
+                  if (node_parent.defined_at < node_var.defined_at) {
+                        if (!ctx.program->live.live_in[block_idx].count(parent.id()))
+                              return false;
+                  }
+
+                  bool parent_live = is_live_out(ctx, parent, block_idx);
+                  if (parent_live)
+                        return true;
+
+                  for (const copy& cp : ctx.parallelcopies[block_idx]) {
+                        if (cp.def.getTemp() == var)
+                              return false;
+                        if (cp.op.isTemp() && cp.op.getTemp() == parent)
+                              parent_live = true;
+                  }
+                  if (parent_live)
+                        return true;
+
+                  const Block& block = ctx.program->blocks[block_idx];
+                  for (auto it = block.instructions.crbegin(); it != block.instructions.crend(); ++it) {
+                        if (is_phi(it->get()))
+                              break;
+
+                        for (const Definition& def : (*it)->definitions) {
+                              if (!def.isTemp())
+                                    continue;
+                              if (def.getTemp() == var)
+                                    return false;
+                        }
+
+                        for (const Operand& op : (*it)->operands) {
+                              if (!op.isTemp())
+                                    continue;
+                              if (op.getTemp() == parent)
+                                    return true;
+                        }
+                  }
 
-      /* update the location transfer graph */
-      if (it->second.read_idx != -1u) {
-         auto&& other = ltg.find(it->second.read_idx);
-         if (other != ltg.end())
-            other->second.num_uses--;
-      }
-      ltg.erase(it);
+                  return false;
+            }
 
-      /* Remove the kill flag if we still need this operand for other copies. */
-      if (cp.op.isKill() && std::any_of(ltg.begin(), ltg.end(),
-                                        [&](auto& other) { return other.second.cp->op == cp.op; }))
-         cp.op.setKill(false);
-
-      /* emit the copy */
-      Instruction* instr = bld.copy(cp.def, cp.op);
-      live_changes += get_live_changes(instr);
-      RegisterDemand temps = get_temp_registers(instr);
-      instr->register_demand = reg_demand + live_changes + temps;
-
-      it = ltg.begin();
-   }
-
-   /* count the number of remaining circular dependencies */
-   unsigned num = std::count_if(
-      ltg.begin(), ltg.end(), [&](auto& n) { return n.second.cp->def.regClass().type() == type; });
-
-   /* if there are circular dependencies, we just emit them as single parallelcopy */
-   if (num) {
-      // TODO: this should be restricted to a feasible number of registers
-      // and otherwise use a temporary to avoid having to reload more (spilled)
-      // variables than we have registers.
-      aco_ptr<Instruction> copy{
-         create_instruction(aco_opcode::p_parallelcopy, Format::PSEUDO, num, num)};
-      it = ltg.begin();
-      for (unsigned i = 0; i < num; i++) {
-         while (it->second.cp->def.regClass().type() != type)
-            ++it;
-
-         copy->definitions[i] = it->second.cp->def;
-         copy->operands[i] = it->second.cp->op;
-         it = ltg.erase(it);
-      }
-      live_changes += get_live_changes(copy.get());
-      RegisterDemand temps = get_temp_registers(copy.get());
-      copy->register_demand = reg_demand + live_changes + temps;
-      bld.insert(std::move(copy));
-   }
-
-   /* Update RegisterDemand after inserted copies */
-   for (auto instr_it = bld.it; instr_it != bld.instructions->end(); ++instr_it) {
-      instr_it->get()->register_demand += live_changes;
-   }
-}
-
-/* either emits or coalesces all parallelcopies and
- * renames the phi-operands accordingly. */
-void
-emit_parallelcopies(cssa_ctx& ctx)
-{
-   std::unordered_map<uint32_t, Operand> renames;
-
-   /* we iterate backwards to prioritize coalescing in else-blocks */
-   for (int i = ctx.program->blocks.size() - 1; i >= 0; i--) {
-      if (ctx.parallelcopies[i].empty())
-         continue;
-
-      std::map<uint32_t, ltg_node> ltg;
-      bool has_vgpr_copy = false;
-      bool has_sgpr_copy = false;
-
-      /* first, try to coalesce all parallelcopies */
-      for (copy& cp : ctx.parallelcopies[i]) {
-         if (try_coalesce_copy(ctx, cp, i)) {
-            assert(cp.op.isTemp() && cp.op.isKill());
-            /* As this temp will be used as phi operand and becomes live-out,
-             * remove the kill flag from any other copy of this same temp.
-             */
-            for (copy& other : ctx.parallelcopies[i]) {
-               if (&other != &cp && other.op.isTemp() && other.op.getTemp() == cp.op.getTemp())
-                  other.op.setKill(false);
+            /* check interference between var and parent:
+             * i.e. they have different values and intersect.
+             * If parent and var intersect and share the same value, also updates the equal ancestor. */
+            inline bool
+            interference(cssa_ctx& ctx, Temp var, Temp parent)
+            {
+                  assert(var != parent);
+                  merge_node& node_var = ctx.merge_node_table[var.id()];
+                  node_var.equal_anc_out = Temp();
+
+                  if (node_var.index == ctx.merge_node_table[parent.id()].index) {
+                        parent = ctx.merge_node_table[parent.id()].equal_anc_out;
+                  }
+
+                  Temp tmp = parent;
+                  while (tmp != Temp() && !intersects(ctx, var, tmp)) {
+                        merge_node& node_tmp = ctx.merge_node_table[tmp.id()];
+                        tmp = node_tmp.equal_anc_in;
+                  }
+
+                  if (tmp == Temp())
+                        return false;
+
+                  if (node_var.value == ctx.merge_node_table[parent.id()].value) {
+                        node_var.equal_anc_out = tmp;
+                        return false;
+                  }
+
+                  return true;
             }
-            renames.emplace(cp.def.tempId(), cp.op);
-         } else {
-            uint32_t read_idx = -1u;
-            if (cp.op.isTemp()) {
-               read_idx = ctx.merge_node_table[cp.op.tempId()].index;
-               /* In case the original phi-operand was killed, it might still be live-out
-                * if the logical successor is not the same as linear successors.
-                * Thus, re-check whether the temp is live-out.
-                */
-               cp.op.setKill(cp.op.isKill() && !is_live_out(ctx, cp.op.getTemp(), i));
-               cp.op.setFirstKill(cp.op.isKill());
+
+            /* tries to merge set_b into set_a of given temporary and
+             * drops that temporary as it is being coalesced */
+            bool
+            try_merge_merge_set(cssa_ctx& ctx, Temp dst, merge_set& set_b)
+            {
+                  auto def_node_it = ctx.merge_node_table.find(dst.id());
+                  uint32_t index = def_node_it->second.index;
+                  merge_set& set_a = ctx.merge_sets[index];
+                  std::vector<Temp> dom;
+                  merge_set union_set;
+                  // Reserve for union_set: sum of sizes is an upper bound.
+                  union_set.reserve(set_a.size() + set_b.size());
+                  uint32_t i_a = 0;
+                  uint32_t i_b = 0;
+
+                  while (i_a < set_a.size() || i_b < set_b.size()) {
+                        Temp current;
+                        if (i_a == set_a.size())
+                              current = set_b[i_b++];
+                        else if (i_b == set_b.size())
+                              current = set_a[i_a++];
+                        else if (defined_after(ctx, set_a[i_a], set_b[i_b]))
+                              current = set_b[i_b++];
+                        else
+                              current = set_a[i_a++];
+
+                        while (!dom.empty() && !dominates(ctx, dom.back(), current))
+                              dom.pop_back();
+
+                        if (!dom.empty() && interference(ctx, current, dom.back())) [[unlikely]] {
+                              for (Temp t : union_set)
+                                    ctx.merge_node_table[t.id()].equal_anc_out = Temp();
+                              return false;
+                        }
+
+                        dom.emplace_back(current);
+                        if (current != dst)
+                              union_set.emplace_back(current);
+                  }
+
+                  for (Temp t : union_set) {
+                        merge_node& node = ctx.merge_node_table[t.id()];
+                        Temp in = node.equal_anc_in;
+                        Temp out = node.equal_anc_out;
+                        if (in == Temp() || (out != Temp() && defined_after(ctx, out, in)))
+                              node.equal_anc_in = out;
+                        node.equal_anc_out = Temp();
+                        node.index = index;
+                  }
+                  set_b.clear(); // Clear and shrink if needed, or just let it be reassigned
+                  set_b.shrink_to_fit();
+                  ctx.merge_sets[index] = std::move(union_set); // Use std::move
+                  ctx.merge_node_table.erase(dst.id());
+
+                  return true;
             }
-            uint32_t write_idx = ctx.merge_node_table[cp.def.tempId()].index;
-            assert(write_idx != -1u);
-            ltg[write_idx] = {&cp, read_idx};
-
-            bool is_vgpr = cp.def.regClass().type() == RegType::vgpr;
-            has_vgpr_copy |= is_vgpr;
-            has_sgpr_copy |= !is_vgpr;
-         }
-      }
 
-      /* build location-transfer-graph */
-      for (auto& pair : ltg) {
-         if (pair.second.read_idx == -1u)
-            continue;
-         auto&& it = ltg.find(pair.second.read_idx);
-         if (it != ltg.end())
-            it->second.num_uses++;
-      }
+            /* returns true if the copy can safely be omitted */
+            bool
+            try_coalesce_copy(cssa_ctx& ctx, copy copy, uint32_t block_idx)
+            {
+                  if (!copy.op.isTemp() || !copy.op.isKill()) [[unlikely]]
+                        return false;
+
+                  if (copy.op.regClass() != copy.def.regClass()) [[unlikely]]
+                        return false;
+
+                  merge_node& op_node = ctx.merge_node_table[copy.op.tempId()];
+                  if (op_node.defined_at == -1u) {
+                        while (ctx.program->live.live_in[block_idx].count(copy.op.tempId()))
+                              block_idx = copy.op.regClass().type() == RegType::vgpr
+                              ? ctx.program->blocks[block_idx].logical_idom
+                              : ctx.program->blocks[block_idx].linear_idom;
+                        op_node.defined_at = block_idx;
+                        op_node.value = copy.op;
+                  }
+
+                  if (op_node.index == -1u) {
+                        merge_set op_set = merge_set{copy.op.getTemp()};
+                        return try_merge_merge_set(ctx, copy.def.getTemp(), op_set);
+                  }
+
+                  assert(ctx.merge_node_table.count(copy.def.tempId()));
+                  if (op_node.index == ctx.merge_node_table[copy.def.tempId()].index)
+                        return true;
 
-      /* emit parallelcopies ordered */
-      Builder bld(ctx.program);
-      Block& block = ctx.program->blocks[i];
-
-      if (has_vgpr_copy) {
-         /* emit VGPR copies */
-         auto IsLogicalEnd = [](const aco_ptr<Instruction>& inst) -> bool
-         { return inst->opcode == aco_opcode::p_logical_end; };
-         auto it =
-            std::find_if(block.instructions.rbegin(), block.instructions.rend(), IsLogicalEnd);
-         bld.reset(&block.instructions, std::prev(it.base()));
-         emit_copies_block(bld, ltg, RegType::vgpr);
-      }
+                  return try_merge_merge_set(ctx, copy.def.getTemp(), ctx.merge_sets[op_node.index]);
+            }
 
-      if (has_sgpr_copy) {
-         /* emit SGPR copies */
-         bld.reset(&block.instructions, std::prev(block.instructions.end()));
-         emit_copies_block(bld, ltg, RegType::sgpr);
-      }
-   }
+            /* node in the location-transfer-graph */
+            struct ltg_node {
+                  copy* cp;
+                  uint32_t read_idx;
+                  uint32_t num_uses = 0;
+            };
+
+            /* emit the copies in an order that does not
+             * create interferences within a merge-set */
+            void
+            emit_copies_block(Builder& bld, std::map<uint32_t, ltg_node>& ltg, RegType type,
+                              std::unordered_map<uint32_t, uint32_t>& ltg_operand_temp_counts)
+            {
+                  RegisterDemand live_changes;
+                  RegisterDemand reg_demand = bld.it->get()->register_demand - get_temp_registers(bld.it->get()) -
+                  get_live_changes(bld.it->get());
+                  auto it = ltg.begin();
+                  while (it != ltg.end()) {
+                        // Original copy data from the ltg_node's pointer.
+                        // The `cp` pointer points into `ctx.parallelcopies[i]`.
+                        copy* original_cp_ptr = it->second.cp;
+                        Definition def_to_emit = original_cp_ptr->def;
+                        Operand op_to_emit = original_cp_ptr->op; // Make a working copy of the operand for potential modification
+
+                        if (def_to_emit.regClass().type() != type || it->second.num_uses > 0) {
+                              ++it;
+                              continue;
+                        }
+
+                        // Modify kill flag on op_to_emit based on global LTG usage.
+                        // This replicates the original std::any_of logic which effectively checks if the operand
+                        // is used by any copy within the current ltg (including potentially itself).
+                        if (op_to_emit.isTemp() && op_to_emit.isKill()) {
+                              auto count_it = ltg_operand_temp_counts.find(op_to_emit.tempId());
+                              if (count_it != ltg_operand_temp_counts.end() && count_it->second > 0) {
+                                    op_to_emit.setKill(false);
+                              }
+                        }
+
+                        uint32_t current_read_idx = it->second.read_idx; // Save before 'it' potentially invalidated by erase
+
+                        /* update the location transfer graph for the dependency */
+                        if (current_read_idx != -1u) {
+                              auto other_iter = ltg.find(current_read_idx);
+                              if (other_iter != ltg.end()) {
+                                    other_iter->second.num_uses--;
+                              }
+                        }
+
+                        // Decrement usage count for the operand of the copy being removed.
+                        if (original_cp_ptr->op.isTemp()) {
+                              auto count_iter = ltg_operand_temp_counts.find(original_cp_ptr->op.tempId());
+                              if (count_iter != ltg_operand_temp_counts.end()) {
+                                    if (count_iter->second > 0) { // Should always be true if found and part of ltg
+                                          count_iter->second--;
+                                    }
+                                    // Optional: if (count_iter->second == 0) ltg_operand_temp_counts.erase(count_iter);
+                              }
+                        }
+
+                        it = ltg.erase(it); // Erase and get next valid iterator
+
+                        /* emit the copy */
+                        Instruction* instr = bld.copy(def_to_emit, op_to_emit);
+                        live_changes += get_live_changes(instr);
+                        RegisterDemand temps = get_temp_registers(instr);
+                        instr->register_demand = reg_demand + live_changes + temps;
+
+                        // Restart scan from beginning of map to maintain original processing order (smallest write_idx first)
+                        if (!ltg.empty()) { // Check if ltg is not empty before resetting iterator
+                              it = ltg.begin();
+                        } // If ltg became empty, loop condition `it != ltg.end()` will handle termination.
+                  }
+
+                  unsigned num = 0;
+                  for (auto const& [_, node_val] : ltg) {
+                        if (node_val.cp->def.regClass().type() == type) {
+                              num++;
+                        }
+                  }
+
+                  if (num) [[unlikely]] {
+                        aco_ptr<Instruction> par_copy_instr{
+                              create_instruction(aco_opcode::p_parallelcopy, Format::PSEUDO, num, num)};
+
+                              auto current_ltg_iter = ltg.begin();
+                              for (unsigned i = 0; i < num; i++) {
+                                    while(current_ltg_iter != ltg.end() && current_ltg_iter->second.cp->def.regClass().type() != type) {
+                                          ++current_ltg_iter;
+                                    }
+                                    if (current_ltg_iter == ltg.end()) {
+                                          assert(false && "Should have found enough nodes for parallelcopy");
+                                          break;
+                                    }
+
+                                    par_copy_instr->definitions[i] = current_ltg_iter->second.cp->def;
+                                    par_copy_instr->operands[i] = current_ltg_iter->second.cp->op;
+
+                                    // Decrement usage count for the operand of the copy being moved to parallelcopy
+                                    if (current_ltg_iter->second.cp->op.isTemp()) {
+                                          auto count_iter = ltg_operand_temp_counts.find(current_ltg_iter->second.cp->op.tempId());
+                                          if (count_iter != ltg_operand_temp_counts.end()) {
+                                                if (count_iter->second > 0) {
+                                                      count_iter->second--;
+                                                }
+                                          }
+                                    }
+                                    current_ltg_iter = ltg.erase(current_ltg_iter); // Erase and advance
+                              }
+                              live_changes += get_live_changes(par_copy_instr.get());
+                              RegisterDemand temps = get_temp_registers(par_copy_instr.get());
+                              par_copy_instr->register_demand = reg_demand + live_changes + temps;
+                              bld.insert(std::move(par_copy_instr));
+                  }
+
+                  for (auto instr_it = bld.it; instr_it != bld.instructions->end(); ++instr_it) {
+                        instr_it->get()->register_demand += live_changes;
+                  }
+            }
+
+            /* either emits or coalesces all parallelcopies and
+             * renames the phi-operands accordingly. */
+            void
+            emit_parallelcopies(cssa_ctx& ctx)
+            {
+                  std::unordered_map<uint32_t, Operand> renames;
+
+                  for (int i = ctx.program->blocks.size() - 1; i >= 0; i--) {
+                        if (ctx.parallelcopies[i].empty()) [[likely]] // Assume many blocks might not have parallel copies
+                              continue;
+
+                        std::map<uint32_t, ltg_node> ltg;
+                        bool has_vgpr_copy = false;
+                        bool has_sgpr_copy = false;
+
+                        for (copy& cp : ctx.parallelcopies[i]) {
+                              if (try_coalesce_copy(ctx, cp, i)) [[likely]] { // Assume coalescing is often successful
+                                    assert(cp.op.isTemp() && cp.op.isKill());
+                                    for (copy& other : ctx.parallelcopies[i]) {
+                                          if (&other != &cp && other.op.isTemp() && other.op.getTemp() == cp.op.getTemp())
+                                                other.op.setKill(false);
+                                    }
+                                    renames.emplace(cp.def.tempId(), cp.op);
+                              } else {
+                                    uint32_t read_idx = -1u;
+                                    if (cp.op.isTemp()) {
+                                          read_idx = ctx.merge_node_table[cp.op.tempId()].index;
+                                          cp.op.setKill(cp.op.isKill() && !is_live_out(ctx, cp.op.getTemp(), i));
+                                          cp.op.setFirstKill(cp.op.isKill());
+                                    }
+                                    uint32_t write_idx = ctx.merge_node_table[cp.def.tempId()].index;
+                                    assert(write_idx != -1u);
+                                    ltg[write_idx] = {&cp, read_idx};
+
+                                    bool is_vgpr = cp.def.regClass().type() == RegType::vgpr;
+                                    has_vgpr_copy |= is_vgpr;
+                                    has_sgpr_copy |= !is_vgpr;
+                              }
+                        }
+
+                        for (auto& pair : ltg) {
+                              if (pair.second.read_idx == -1u)
+                                    continue;
+                              auto it_user = ltg.find(pair.second.read_idx);
+                              if (it_user != ltg.end())
+                                    it_user->second.num_uses++;
+                        }
+
+                        std::unordered_map<uint32_t, uint32_t> ltg_operand_temp_counts;
+                        if (!ltg.empty()) {
+                              for (auto const& [_, node_val] : ltg) {
+                                    if (node_val.cp->op.isTemp()) {
+                                          ltg_operand_temp_counts[node_val.cp->op.tempId()]++;
+                                    }
+                              }
+                        }
+
+                        Builder bld(ctx.program);
+                        Block& block = ctx.program->blocks[i];
+
+                        if (has_vgpr_copy) {
+                              auto IsLogicalEnd = [](const aco_ptr<Instruction>& inst) -> bool
+                              { return inst->opcode == aco_opcode::p_logical_end; };
+                              auto log_end_it =
+                              std::find_if(block.instructions.rbegin(), block.instructions.rend(), IsLogicalEnd);
+                              bld.reset(&block.instructions, std::prev(log_end_it.base()));
+                              emit_copies_block(bld, ltg, RegType::vgpr, ltg_operand_temp_counts);
+                        }
+
+                        if (has_sgpr_copy) {
+                              bld.reset(&block.instructions, std::prev(block.instructions.end()));
+                              emit_copies_block(bld, ltg, RegType::sgpr, ltg_operand_temp_counts);
+                        }
+                  }
+
+                  RegisterDemand new_demand;
+                  for (Block& block : ctx.program->blocks) {
+                        for (aco_ptr<Instruction>& phi : block.instructions) {
+                              if (phi->opcode != aco_opcode::p_phi && phi->opcode != aco_opcode::p_linear_phi)
+                                    break;
+
+                              for (Operand& op : phi->operands) {
+                                    if (!op.isTemp())
+                                          continue;
+                                    auto rename_it = renames.find(op.tempId());
+                                    if (rename_it != renames.end()) [[likely]] {
+                                          op = rename_it->second;
+                                          renames.erase(rename_it);
+                                    }
+                              }
+                        }
+
+                        block.register_demand = block.live_in_demand;
+                        for (const aco_ptr<Instruction>& instr : block.instructions)
+                              block.register_demand.update(instr->register_demand);
+                        new_demand.update(block.register_demand);
+                  }
+
+                  update_vgpr_sgpr_demand(ctx.program, new_demand);
 
-   RegisterDemand new_demand;
-   for (Block& block : ctx.program->blocks) {
-      /* Finally, rename coalesced phi operands */
-      for (aco_ptr<Instruction>& phi : block.instructions) {
-         if (phi->opcode != aco_opcode::p_phi && phi->opcode != aco_opcode::p_linear_phi)
-            break;
-
-         for (Operand& op : phi->operands) {
-            if (!op.isTemp())
-               continue;
-            auto&& it = renames.find(op.tempId());
-            if (it != renames.end()) {
-               op = it->second;
-               renames.erase(it);
+                  assert(renames.empty());
             }
-         }
+
       }
 
-      /* Resummarize the block's register demand */
-      block.register_demand = block.live_in_demand;
-      for (const aco_ptr<Instruction>& instr : block.instructions)
-         block.register_demand.update(instr->register_demand);
-      new_demand.update(block.register_demand);
-   }
-
-   /* Update max_reg_demand and num_waves */
-   update_vgpr_sgpr_demand(ctx.program, new_demand);
-
-   assert(renames.empty());
-}
-
-} /* end namespace */
-
-void
-lower_to_cssa(Program* program)
-{
-   reindex_ssa(program);
-   cssa_ctx ctx = {program};
-   collect_parallelcopies(ctx);
-   emit_parallelcopies(ctx);
-
-   /* Validate live variable information */
-   if (!validate_live_vars(program))
-      abort();
-}
+      void
+      lower_to_cssa(Program* program)
+      {
+            reindex_ssa(program);
+            cssa_ctx ctx = {program};
+            collect_parallelcopies(ctx);
+            emit_parallelcopies(ctx);
+
+            if (!validate_live_vars(program)) [[unlikely]]
+                  abort();
+      }
 } // namespace aco



--- a/src/amd/compiler/aco_util.h	2025-05-26 12:24:19.131056673 +0200
+++ b/src/amd/compiler/aco_util.h	2025-05-26 13:02:10.133508433 +0200


--- a/src/amd/compiler/aco_register_allocation.cpp	2025-05-30 14:11:47.937806865 +0200
+++ b/src/amd/compiler/aco_register_allocation.cpp	2025-05-30 16:23:37.312701919 +0200
@@ -15,3750 +15,4147 @@
 #include <map>
 #include <optional>
 #include <vector>
+#include <unordered_set>
+#define LIKELY(x)   __builtin_expect(!!(x), 1)
+#define UNLIKELY(x) __builtin_expect(!!(x), 0)
 
 namespace aco {
-namespace {
+      namespace {
 
-struct ra_ctx;
-struct DefInfo;
+            struct ra_ctx;
+            struct DefInfo;
 
-unsigned get_subdword_operand_stride(amd_gfx_level gfx_level, const aco_ptr<Instruction>& instr,
-                                     unsigned idx, RegClass rc);
-void add_subdword_operand(ra_ctx& ctx, aco_ptr<Instruction>& instr, unsigned idx, unsigned byte,
-                          RegClass rc);
-void add_subdword_definition(Program* program, aco_ptr<Instruction>& instr, PhysReg reg,
-                             bool allow_16bit_write);
-
-struct parallelcopy {
-   constexpr parallelcopy(Operand op_, Definition def_, int copy_kill_ = -1)
-       : op(op_), def(def_), copy_kill(copy_kill_)
-   {}
-
-   Operand op;
-   Definition def;
-
-   /* If not negative, this copy is only used by a single operand of the instruction and not after
-    * the instruction. There might also be more copy-kill copies or a normal copy with the same
-    * operand.
-    *
-    * update_renames() assumes that the copy's source temporary is still live after the parallelcopy
-    * instruction unless there's a normal copy of the temporary.
-    *
-    * update_renames() also assumes that when a copy-kill copy is added, the register file is before
-    * the current instruction but after the parallelcopy instruction.
-    */
-   int copy_kill;
-};
-
-struct assignment {
-   PhysReg reg;
-   RegClass rc;
-   union {
-      struct {
-         bool assigned : 1;
-         bool vcc : 1;
-         bool m0 : 1;
-         bool renamed : 1;
-      };
-      uint8_t _ = 0;
-   };
-   uint32_t affinity = 0;
-   assignment() = default;
-   assignment(PhysReg reg_, RegClass rc_) : reg(reg_), rc(rc_) { assigned = true; }
-   void set(const Definition& def)
-   {
-      assigned = true;
-      reg = def.physReg();
-      rc = def.regClass();
-   }
-};
-
-/* Iterator type for making PhysRegInterval compatible with range-based for */
-struct PhysRegIterator {
-   using difference_type = int;
-   using value_type = unsigned;
-   using reference = const unsigned&;
-   using pointer = const unsigned*;
-   using iterator_category = std::bidirectional_iterator_tag;
-
-   PhysReg reg;
-
-   PhysReg operator*() const { return reg; }
-
-   PhysRegIterator& operator++()
-   {
-      reg.reg_b += 4;
-      return *this;
-   }
-
-   PhysRegIterator& operator--()
-   {
-      reg.reg_b -= 4;
-      return *this;
-   }
-
-   bool operator==(PhysRegIterator oth) const { return reg == oth.reg; }
-
-   bool operator!=(PhysRegIterator oth) const { return reg != oth.reg; }
-
-   bool operator<(PhysRegIterator oth) const { return reg < oth.reg; }
-};
-
-struct vector_info {
-   vector_info() : is_weak(false), num_parts(0), parts(NULL) {}
-   vector_info(Instruction* instr, unsigned start = 0, bool weak = false)
-       : is_weak(weak), num_parts(instr->operands.size() - start),
-         parts(instr->operands.begin() + start)
-   {
-      if (parts[0].isVectorAligned()) {
-         num_parts = 1;
-         while (parts[num_parts - 1].isVectorAligned())
-            num_parts++;
-      }
-   }
+            unsigned get_subdword_operand_stride(amd_gfx_level gfx_level, const aco_ptr<Instruction>& instr,
+                                                 unsigned idx, RegClass rc);
+            void add_subdword_operand(ra_ctx& ctx, aco_ptr<Instruction>& instr, unsigned idx, unsigned byte,
+                                      RegClass rc);
+            void add_subdword_definition(Program* program, aco_ptr<Instruction>& instr, PhysReg reg,
+                                         bool allow_16bit_write);
+
+            struct parallelcopy {
+                  constexpr parallelcopy(Operand op_, Definition def_, int copy_kill_ = -1)
+                  : op(op_), def(def_), copy_kill(copy_kill_)
+                  {}
+
+                  Operand op;
+                  Definition def;
+
+                  /* If not negative, this copy is only used by a single operand of the instruction and not after
+                   * the instruction. There might also be more copy-kill copies or a normal copy with the same
+                   * operand.
+                   *
+                   * update_renames() assumes that the copy's source temporary is still live after the parallelcopy
+                   * instruction unless there's a normal copy of the temporary.
+                   *
+                   * update_renames() also assumes that when a copy-kill copy is added, the register file is before
+                   * the current instruction but after the parallelcopy instruction.
+                   */
+                  int copy_kill;
+            };
+
+            struct assignment {
+                  PhysReg reg;
+                  RegClass rc;
+                  union {
+                        struct {
+                              bool assigned : 1;
+                              bool vcc : 1;
+                              bool m0 : 1;
+                              bool renamed : 1;
+                        };
+                        uint8_t _ = 0;
+                  };
+                  uint32_t affinity = 0;
+                  assignment() = default;
+                  assignment(PhysReg reg_, RegClass rc_) : reg(reg_), rc(rc_) { assigned = true; }
+                  void set(const Definition& def)
+                  {
+                        assigned = true;
+                        reg = def.physReg();
+                        rc = def.regClass();
+                  }
+            };
+
+            /* Iterator type for making PhysRegInterval compatible with range-based for */
+            struct PhysRegIterator {
+                  using difference_type = int;
+                  using value_type = unsigned;
+                  using reference = const unsigned&;
+                  using pointer = const unsigned*;
+                  using iterator_category = std::bidirectional_iterator_tag;
+
+                  PhysReg reg;
+
+                  PhysReg operator*() const { return reg; }
+
+                  PhysRegIterator& operator++()
+                  {
+                        reg.reg_b += 4;
+                        return *this;
+                  }
+
+                  PhysRegIterator& operator--()
+                  {
+                        reg.reg_b -= 4;
+                        return *this;
+                  }
+
+                  bool operator==(PhysRegIterator oth) const { return reg == oth.reg; }
+
+                  bool operator!=(PhysRegIterator oth) const { return reg != oth.reg; }
+
+                  bool operator<(PhysRegIterator oth) const { return reg < oth.reg; }
+            };
+
+            struct vector_info {
+                  vector_info() : is_weak(false), num_parts(0), parts(NULL) {}
+                  vector_info(Instruction* instr, unsigned start = 0, bool weak = false)
+                  : is_weak(weak), num_parts(instr->operands.size() - start),
+                  parts(instr->operands.begin() + start)
+                  {
+                        if (parts[0].isVectorAligned()) {
+                              num_parts = 1;
+                              while (parts[num_parts - 1].isVectorAligned())
+                                    num_parts++;
+                        }
+                  }
+
+                  /* If true, then we should stop trying to form a vector if anything goes wrong. Useful for when
+                   * the cost of failing does not introduce copies. */
+                  bool is_weak;
+                  uint32_t num_parts;
+                  Operand* parts;
+            };
+
+            struct vector_operand {
+                  Definition def;
+                  uint32_t start;
+                  uint32_t num_part;
+            };
+
+            struct ra_ctx
+            {
+                  /* ---------- High-level context ---------- */
+                  Program* program;                       /* current program                */
+                  Block*   block = nullptr;               /* block being processed          */
+
+                  /* ---------- Memory arenas ---------- */
+                  aco::monotonic_buffer_resource memory;  /* fast bump allocator            */
+
+                  /* ---------- Allocation state ---------- */
+                  std::vector<assignment>                        assignments;     /* temp->phys map   */
+                  std::vector<aco::unordered_map<uint32_t, Temp>> renames;        /* SSA renaming     */
+                  std::vector<std::pair<uint32_t, PhysReg>>       loop_header;    /* loop PHI glue    */
+                  std::vector<vector_operand>                     vector_operands;
+                  aco::unordered_map<uint32_t, Temp>              orig_names;
+                  aco::unordered_map<uint32_t, vector_info>       vectors;
+                  aco::unordered_map<uint32_t, Instruction*>      split_vectors;
+
+                  /* ---------- Dummy instructions ---------- */
+                  aco_ptr<Instruction> pseudo_dummy;
+                  aco_ptr<Instruction> phi_dummy;
+
+                  /* ---------- High-water marks ---------- */
+                  uint16_t max_used_sgpr = 0;
+                  uint16_t max_used_vgpr = 0;
+
+                  /* ---------- Global limits ---------- */
+                  RegisterDemand limit;                   /* addr-regs vs waves             */
+                  std::bitset<512> war_hint;              /* hazard hint for VMEM/EXP/DS    */
+
+                  /* ---------- Round-robin iterators ---------- */
+                  PhysRegIterator rr_sgpr_it;
+                  PhysRegIterator rr_vgpr_it;
+
+                  /* ---------- Dynamic bounds ---------- */
+                  uint16_t sgpr_bounds;                   /* max index+1 usable SGPR        */
+                  uint16_t vgpr_bounds;                   /* max index+1 usable VGPR        */
+                  uint16_t num_linear_vgprs;              /* linear VGPR window size        */
+
+                  /* ---------- Hazard book-keeping ---------- */
+                  Instruction* last_vcc_defining_instr_ptr = nullptr; /* GFX9 VCC->branch */
+
+                  /* ---------- Misc ---------- */
+                  ra_test_policy policy;
+
+                  /* ---------- Constructor ---------- */
+                  ra_ctx(Program* program_, ra_test_policy policy_)
+                  : program(program_),
+                  assignments(program_->peekAllocationId()),
+                  renames(program_->blocks.size(),
+                          aco::unordered_map<uint32_t, Temp>(memory)),
+                          orig_names(memory),
+                          vectors(memory),
+                          split_vectors(memory),
+                          policy(policy_)
+                          {
+                                /* create dummy pseudo instructions ------------------------------- */
+                                pseudo_dummy.reset(
+                                      create_instruction(aco_opcode::p_parallelcopy, Format::PSEUDO, 0, 0));
+                                phi_dummy.reset(
+                                      create_instruction(aco_opcode::p_linear_phi,  Format::PSEUDO, 0, 0));
+
+                                /* wave-dependent address register limit -------------------------- */
+                                limit = get_addr_regs_from_waves(program, program->min_waves);
+
+                                /* initialise bounds from earlier demand analysis ----------------- */
+                                sgpr_bounds = program->max_reg_demand.sgpr;
+                                vgpr_bounds = program->max_reg_demand.vgpr;
+                                num_linear_vgprs = 0;
+
+                                /* -------------  GFX9 TTMP reservation  -------------------------- */
+                                if (program->gfx_level == GFX9)
+                                      sgpr_bounds = std::min<uint16_t>(sgpr_bounds, 108u);
+
+                          }
+            };
+
+            /* Half-open register interval used in "sliding window"-style for-loops */
+            struct PhysRegInterval {
+                  PhysReg lo_;
+                  unsigned size;
+
+                  /* Inclusive lower bound */
+                  PhysReg lo() const { return lo_; }
+
+                  /* Exclusive upper bound */
+                  PhysReg hi() const { return PhysReg{lo() + size}; }
+
+                  PhysRegInterval& operator+=(uint32_t stride)
+                  {
+                        lo_ = PhysReg{lo_.reg() + stride};
+                        return *this;
+                  }
+
+                  bool operator!=(const PhysRegInterval& oth) const { return lo_ != oth.lo_ || size != oth.size; }
+
+                  /* Construct a half-open interval, excluding the end register */
+                  static PhysRegInterval from_until(PhysReg first, PhysReg end) { return {first, end - first}; }
+
+                  bool contains(PhysReg reg) const { return lo() <= reg && reg < hi(); }
+
+                  bool contains(const PhysRegInterval& needle) const
+                  {
+                        return needle.lo() >= lo() && needle.hi() <= hi();
+                  }
+
+                  PhysRegIterator begin() const { return {lo_}; }
+
+                  PhysRegIterator end() const { return {PhysReg{lo_ + size}}; }
+            };
+
+            bool
+            intersects(const PhysRegInterval& a, const PhysRegInterval& b)
+            {
+                  return a.hi() > b.lo() && b.hi() > a.lo();
+            }
 
-   /* If true, then we should stop trying to form a vector if anything goes wrong. Useful for when
-    * the cost of failing does not introduce copies. */
-   bool is_weak;
-   uint32_t num_parts;
-   Operand* parts;
-};
-
-struct vector_operand {
-   Definition def;
-   uint32_t start;
-   uint32_t num_part;
-};
-
-struct ra_ctx {
-
-   Program* program;
-   Block* block = NULL;
-   aco::monotonic_buffer_resource memory;
-   std::vector<assignment> assignments;
-   std::vector<aco::unordered_map<uint32_t, Temp>> renames;
-   std::vector<std::pair<uint32_t, PhysReg>> loop_header;
-   std::vector<vector_operand> vector_operands;
-   aco::unordered_map<uint32_t, Temp> orig_names;
-   aco::unordered_map<uint32_t, vector_info> vectors;
-   aco::unordered_map<uint32_t, Instruction*> split_vectors;
-   aco_ptr<Instruction> pseudo_dummy;
-   aco_ptr<Instruction> phi_dummy;
-   uint16_t max_used_sgpr = 0;
-   uint16_t max_used_vgpr = 0;
-   RegisterDemand limit;
-   std::bitset<512> war_hint;
-   PhysRegIterator rr_sgpr_it;
-   PhysRegIterator rr_vgpr_it;
-
-   uint16_t sgpr_bounds;
-   uint16_t vgpr_bounds;
-   uint16_t num_linear_vgprs;
-
-   ra_test_policy policy;
-
-   ra_ctx(Program* program_, ra_test_policy policy_)
-       : program(program_), assignments(program->peekAllocationId()),
-         renames(program->blocks.size(), aco::unordered_map<uint32_t, Temp>(memory)),
-         orig_names(memory), vectors(memory), split_vectors(memory), policy(policy_)
-   {
-      pseudo_dummy.reset(create_instruction(aco_opcode::p_parallelcopy, Format::PSEUDO, 0, 0));
-      phi_dummy.reset(create_instruction(aco_opcode::p_linear_phi, Format::PSEUDO, 0, 0));
-      limit = get_addr_regs_from_waves(program, program->min_waves);
-
-      sgpr_bounds = program->max_reg_demand.sgpr;
-      vgpr_bounds = program->max_reg_demand.vgpr;
-      num_linear_vgprs = 0;
-   }
-};
-
-/* Half-open register interval used in "sliding window"-style for-loops */
-struct PhysRegInterval {
-   PhysReg lo_;
-   unsigned size;
-
-   /* Inclusive lower bound */
-   PhysReg lo() const { return lo_; }
-
-   /* Exclusive upper bound */
-   PhysReg hi() const { return PhysReg{lo() + size}; }
-
-   PhysRegInterval& operator+=(uint32_t stride)
-   {
-      lo_ = PhysReg{lo_.reg() + stride};
-      return *this;
-   }
-
-   bool operator!=(const PhysRegInterval& oth) const { return lo_ != oth.lo_ || size != oth.size; }
-
-   /* Construct a half-open interval, excluding the end register */
-   static PhysRegInterval from_until(PhysReg first, PhysReg end) { return {first, end - first}; }
-
-   bool contains(PhysReg reg) const { return lo() <= reg && reg < hi(); }
-
-   bool contains(const PhysRegInterval& needle) const
-   {
-      return needle.lo() >= lo() && needle.hi() <= hi();
-   }
-
-   PhysRegIterator begin() const { return {lo_}; }
-
-   PhysRegIterator end() const { return {PhysReg{lo_ + size}}; }
-};
-
-bool
-intersects(const PhysRegInterval& a, const PhysRegInterval& b)
-{
-   return a.hi() > b.lo() && b.hi() > a.lo();
-}
-
-/* Gets the stride for full (non-subdword) registers */
-uint32_t
-get_stride(RegClass rc)
-{
-   if (rc.type() == RegType::vgpr) {
-      return 1;
-   } else {
-      uint32_t size = rc.size();
-      if (size == 2) {
-         return 2;
-      } else if (size >= 4 && util_is_power_of_two_or_zero(size)) {
-         return 4;
-      } else {
-         return 1;
-      }
-   }
-}
+            /* Gets the stride for full (non-subdword) registers */
+            uint32_t
+            get_stride(RegClass rc)
+            {
+                  if (rc.type() == RegType::vgpr) {
+                        return 1;
+                  } else {
+                        uint32_t size = rc.size();
+                        if (size == 2) {
+                              return 2;
+                        } else if (size >= 4 && util_is_power_of_two_or_zero(size)) {
+                              return 4;
+                        } else {
+                              return 1;
+                        }
+                  }
+            }
 
-PhysRegInterval
-get_reg_bounds(ra_ctx& ctx, RegType type, bool linear_vgpr)
-{
-   uint16_t linear_vgpr_start = ctx.vgpr_bounds - ctx.num_linear_vgprs;
-   if (type == RegType::vgpr && linear_vgpr) {
-      return PhysRegInterval{PhysReg(256 + linear_vgpr_start), ctx.num_linear_vgprs};
-   } else if (type == RegType::vgpr) {
-      return PhysRegInterval{PhysReg(256), linear_vgpr_start};
-   } else {
-      return PhysRegInterval{PhysReg(0), ctx.sgpr_bounds};
-   }
-}
-
-PhysRegInterval
-get_reg_bounds(ra_ctx& ctx, RegClass rc)
-{
-   return get_reg_bounds(ctx, rc.type(), rc.is_linear_vgpr());
-}
-
-struct DefInfo {
-   PhysRegInterval bounds;
-   uint8_t size;
-   uint8_t stride;
-   /* Even if stride=4, we might be able to write to the high half instead without preserving the
-    * low half. In that case, data_stride=2. */
-   uint8_t data_stride;
-   RegClass rc;
-
-   DefInfo(ra_ctx& ctx, aco_ptr<Instruction>& instr, RegClass rc_, int operand) : rc(rc_)
-   {
-      size = rc.size();
-      stride = get_stride(rc);
-      data_stride = 0;
-
-      bounds = get_reg_bounds(ctx, rc);
-
-      if (rc.is_subdword() && operand >= 0) {
-         /* stride in bytes */
-         stride = get_subdword_operand_stride(ctx.program->gfx_level, instr, operand, rc);
-      } else if (rc.is_subdword()) {
-         get_subdword_definition_info(ctx.program, instr);
-      } else if (instr->isMIMG() && instr->mimg().d16 && ctx.program->gfx_level <= GFX9) {
-         /* Workaround GFX9 hardware bug for D16 image instructions: FeatureImageGather4D16Bug
-          *
-          * The register use is not calculated correctly, and the hardware assumes a
-          * full dword per component. Don't use the last registers of the register file.
-          * Otherwise, the instruction will be skipped.
-          *
-          * https://reviews.llvm.org/D81172
-          */
-         bool imageGather4D16Bug = operand == -1 && rc == v2 && instr->mimg().dmask != 0xF;
-         assert(ctx.program->gfx_level == GFX9 && "Image D16 on GFX8 not supported.");
-
-         if (imageGather4D16Bug)
-            bounds.size -= MAX2(rc.bytes() / 4 - ctx.num_linear_vgprs, 0);
-      } else if (instr_info.classes[(int)instr->opcode] == instr_class::valu_pseudo_scalar_trans) {
-         /* RDNA4 ISA doc, 7.10. Pseudo-scalar Transcendental ALU ops:
-          * - VCC may not be used as a destination
-          */
-         if (bounds.contains(vcc))
-            bounds.size = vcc - bounds.lo();
-      }
+            PhysRegInterval
+            get_reg_bounds(ra_ctx& ctx, RegType type, bool linear_vgpr)
+            {
+                  uint16_t linear_vgpr_start = ctx.vgpr_bounds - ctx.num_linear_vgprs;
+                  if (type == RegType::vgpr && linear_vgpr) {
+                        return PhysRegInterval{PhysReg(256 + linear_vgpr_start), ctx.num_linear_vgprs};
+                  } else if (type == RegType::vgpr) {
+                        return PhysRegInterval{PhysReg(256), linear_vgpr_start};
+                  } else {
+                        return PhysRegInterval{PhysReg(0), ctx.sgpr_bounds};
+                  }
+            }
 
-      if (!data_stride)
-         data_stride = rc.is_subdword() ? stride : (stride * 4);
-   }
-
-private:
-   void get_subdword_definition_info(Program* program, const aco_ptr<Instruction>& instr);
-};
-
-class RegisterFile {
-public:
-   RegisterFile() { regs.fill(0); }
-
-   std::array<uint32_t, 512> regs;
-   std::map<uint32_t, std::array<uint32_t, 4>> subdword_regs;
-
-   const uint32_t& operator[](PhysReg index) const { return regs[index]; }
-
-   uint32_t& operator[](PhysReg index) { return regs[index]; }
-
-   unsigned count_zero(PhysRegInterval reg_interval) const
-   {
-      unsigned res = 0;
-      for (PhysReg reg : reg_interval)
-         res += !regs[reg];
-      return res;
-   }
-
-   /* Returns true if any of the bytes in the given range are allocated or blocked */
-   bool test(PhysReg start, unsigned num_bytes) const
-   {
-      for (PhysReg i = start; i.reg_b < start.reg_b + num_bytes; i = PhysReg(i + 1)) {
-         assert(i <= 511);
-         if (regs[i] & 0x0FFFFFFF)
-            return true;
-         if (regs[i] == 0xF0000000) {
-            auto it = subdword_regs.find(i);
-            assert(it != subdword_regs.end());
-            for (unsigned j = i.byte(); i * 4 + j < start.reg_b + num_bytes && j < 4; j++) {
-               if (it->second[j])
-                  return true;
+            PhysRegInterval
+            get_reg_bounds(ra_ctx& ctx, RegClass rc)
+            {
+                  return get_reg_bounds(ctx, rc.type(), rc.is_linear_vgpr());
             }
-         }
-      }
-      return false;
-   }
 
-   void block(PhysReg start, RegClass rc)
-   {
-      if (rc.is_subdword())
-         fill_subdword(start, rc.bytes(), 0xFFFFFFFF);
-      else
-         fill(start, rc.size(), 0xFFFFFFFF);
-   }
-
-   bool is_blocked(PhysReg start) const
-   {
-      if (regs[start] == 0xFFFFFFFF)
-         return true;
-      if (regs[start] == 0xF0000000) {
-         auto it = subdword_regs.find(start);
-         assert(it != subdword_regs.end());
-         for (unsigned i = start.byte(); i < 4; i++)
-            if (it->second[i] == 0xFFFFFFFF)
-               return true;
-      }
-      return false;
-   }
+            struct DefInfo {
+                  PhysRegInterval bounds;
+                  uint8_t size;
+                  uint8_t stride;
+                  /* Even if stride=4, we might be able to write to the high half instead without preserving the
+                   * low half. In that case, data_stride=2. */
+                  uint8_t data_stride;
+                  RegClass rc;
+
+                  DefInfo(ra_ctx& ctx, aco_ptr<Instruction>& instr, RegClass rc_, int operand) : rc(rc_)
+                  {
+                        size = rc.size();
+                        stride = get_stride(rc);
+                        data_stride = 0;
+
+                        bounds = get_reg_bounds(ctx, rc);
+
+                        if (rc.is_subdword() && operand >= 0) {
+                              /* stride in bytes */
+                              stride = get_subdword_operand_stride(ctx.program->gfx_level, instr, operand, rc);
+                        } else if (rc.is_subdword()) {
+                              get_subdword_definition_info(ctx.program, instr);
+                        } else if (instr->isMIMG() && instr->mimg().d16 && ctx.program->gfx_level <= GFX9) {
+                              /* Workaround GFX9 hardware bug for D16 image instructions: FeatureImageGather4D16Bug
+                               *
+                               * The register use is not calculated correctly, and the hardware assumes a
+                               * full dword per component. Don't use the last registers of the register file.
+                               * Otherwise, the instruction will be skipped.
+                               *
+                               * https://reviews.llvm.org/D81172
+                               */
+                              bool imageGather4D16Bug = operand == -1 && rc == v2 && instr->mimg().dmask != 0xF;
+                              assert(ctx.program->gfx_level == GFX9 && "Image D16 on GFX8 not supported.");
+
+                              if (imageGather4D16Bug)
+                                    bounds.size -= MAX2(rc.bytes() / 4 - ctx.num_linear_vgprs, 0);
+                        } else if (instr_info.classes[(int)instr->opcode] == instr_class::valu_pseudo_scalar_trans) {
+                              /* RDNA4 ISA doc, 7.10. Pseudo-scalar Transcendental ALU ops:
+                               * - VCC may not be used as a destination
+                               */
+                              if (bounds.contains(vcc))
+                                    bounds.size = vcc - bounds.lo();
+                        }
+
+                        if (!data_stride)
+                              data_stride = rc.is_subdword() ? stride : (stride * 4);
+                  }
+
+            private:
+                  void get_subdword_definition_info(Program* program, const aco_ptr<Instruction>& instr);
+            };
+
+            class RegisterFile {
+            public:
+                  RegisterFile() { regs.fill(0); }
+
+                  std::array<uint32_t, 512> regs;
+                  std::map<uint32_t, std::array<uint32_t, 4>> subdword_regs;
+
+                  const uint32_t& operator[](PhysReg index) const { return regs[index]; }
+
+                  uint32_t& operator[](PhysReg index) { return regs[index]; }
+
+                  unsigned count_zero(PhysRegInterval reg_interval) const
+                  {
+                        unsigned res = 0;
+                        for (PhysReg reg : reg_interval)
+                              res += !regs[reg];
+                        return res;
+                  }
+
+                  /* Returns true if any of the bytes in the given range are allocated or blocked */
+                  bool test(PhysReg start, unsigned num_bytes) const
+                  {
+                        for (PhysReg i = start; i.reg_b < start.reg_b + num_bytes; i = PhysReg(i + 1)) {
+                              assert(i <= 511);
+                              if (regs[i] & 0x0FFFFFFF)
+                                    return true;
+                              if (regs[i] == 0xF0000000) {
+                                    auto it = subdword_regs.find(i);
+                                    assert(it != subdword_regs.end());
+                                    for (unsigned j = i.byte(); i * 4 + j < start.reg_b + num_bytes && j < 4; j++) {
+                                          if (it->second[j])
+                                                return true;
+                                    }
+                              }
+                        }
+                        return false;
+                  }
+
+                  void block(PhysReg start, RegClass rc)
+                  {
+                        if (rc.is_subdword())
+                              fill_subdword(start, rc.bytes(), 0xFFFFFFFF);
+                        else
+                              fill(start, rc.size(), 0xFFFFFFFF);
+                  }
+
+                  bool is_blocked(PhysReg start) const
+                  {
+                        if (regs[start] == 0xFFFFFFFF)
+                              return true;
+                        if (regs[start] == 0xF0000000) {
+                              auto it = subdword_regs.find(start);
+                              assert(it != subdword_regs.end());
+                              for (unsigned i = start.byte(); i < 4; i++)
+                                    if (it->second[i] == 0xFFFFFFFF)
+                                          return true;
+                        }
+                        return false;
+                  }
+
+                  bool is_empty_or_blocked(PhysReg start) const
+                  {
+                        /* Empty is 0, blocked is 0xFFFFFFFF, so to check both we compare the
+                         * incremented value to 1 */
+                        if (regs[start] == 0xF0000000) {
+                              auto it = subdword_regs.find(start);
+                              assert(it != subdword_regs.end());
+                              return it->second[start.byte()] + 1 <= 1;
+                        }
+                        return regs[start] + 1 <= 1;
+                  }
+
+                  void clear(PhysReg start, RegClass rc)
+                  {
+                        if (rc.is_subdword())
+                              fill_subdword(start, rc.bytes(), 0);
+                        else
+                              fill(start, rc.size(), 0);
+                  }
+
+                  void fill_killed_operands(Instruction* instr)
+                  {
+                        for (Operand& op : instr->operands) {
+                              if (op.isPrecolored()) {
+                                    block(op.physReg(), op.regClass());
+                              } else if (op.isFixed() && op.isFirstKillBeforeDef()) {
+                                    if (op.regClass().is_subdword())
+                                          fill_subdword(op.physReg(), op.bytes(), op.tempId());
+                                    else
+                                          fill(op.physReg(), op.size(), op.tempId());
+                              }
+                        }
+                  }
+
+                  void clear(Operand op)
+                  {
+                        if (op.isTemp() && !is_empty_or_blocked(op.physReg()))
+                              assert(get_id(op.physReg()) == op.tempId());
+                        clear(op.physReg(), op.regClass());
+                  }
+
+                  void fill(Definition def)
+                  {
+                        if (def.regClass().is_subdword())
+                              fill_subdword(def.physReg(), def.bytes(), def.tempId());
+                        else
+                              fill(def.physReg(), def.size(), def.tempId());
+                  }
+
+                  void clear(Definition def) { clear(def.physReg(), def.regClass()); }
+
+                  unsigned get_id(PhysReg reg) const
+                  {
+                        return regs[reg] == 0xF0000000 ? subdword_regs.at(reg)[reg.byte()] : regs[reg];
+                  }
+
+            private:
+                  void fill(PhysReg start, unsigned size, uint32_t val)
+                  {
+                        for (unsigned i = 0; i < size; i++)
+                              regs[start + i] = val;
+                  }
+
+                  void fill_subdword(PhysReg start, unsigned num_bytes, uint32_t val)
+                  {
+                        fill(start, DIV_ROUND_UP(num_bytes, 4), 0xF0000000);
+                        for (PhysReg i = start; i.reg_b < start.reg_b + num_bytes; i = PhysReg(i + 1)) {
+                              /* emplace or get */
+                              std::array<uint32_t, 4>& sub =
+                              subdword_regs.emplace(i, std::array<uint32_t, 4>{0, 0, 0, 0}).first->second;
+                              for (unsigned j = i.byte(); i * 4 + j < start.reg_b + num_bytes && j < 4; j++)
+                                    sub[j] = val;
+
+                              if (sub == std::array<uint32_t, 4>{0, 0, 0, 0}) {
+                                    subdword_regs.erase(i);
+                                    regs[i] = 0;
+                              }
+                        }
+                  }
+            };
+
+            std::vector<unsigned> find_vars(ra_ctx& ctx, const RegisterFile& reg_file,
+                                            const PhysRegInterval reg_interval);
+
+            /* helper function for debugging */
+            UNUSED void
+            print_reg(const RegisterFile& reg_file, PhysReg reg, bool has_adjacent_variable)
+            {
+                  if (reg_file[reg] == 0xFFFFFFFF) {
+                        printf((const char*)u8"â");
+                  } else if (reg_file[reg]) {
+                        const bool show_subdword_alloc = (reg_file[reg] == 0xF0000000);
+                        if (show_subdword_alloc) {
+                              auto block_chars = {
+                                    // clang-format off
+                                    u8"?", u8"â", u8"â", u8"â",
+                                    u8"â", u8"â", u8"â", u8"â",
+                                    u8"â", u8"â", u8"â", u8"â",
+                                    u8"â", u8"â", u8"â", u8"â"
+                                    // clang-format on
+                              };
+                              unsigned index = 0;
+                              for (int i = 0; i < 4; ++i) {
+                                    if (reg_file.subdword_regs.at(reg)[i]) {
+                                          index |= 1 << i;
+                                    }
+                              }
+                              printf("%s", (const char*)(block_chars.begin()[index]));
+                        } else {
+                              /* Indicate filled register slot */
+                              if (!has_adjacent_variable) {
+                                    printf((const char*)u8"â");
+                              } else {
+                                    /* Use a slightly shorter box to leave a small gap between adjacent variables */
+                                    printf((const char*)u8"â");
+                              }
+                        }
+                  } else {
+                        printf((const char*)u8"Â·");
+                  }
+            }
 
-   bool is_empty_or_blocked(PhysReg start) const
-   {
-      /* Empty is 0, blocked is 0xFFFFFFFF, so to check both we compare the
-       * incremented value to 1 */
-      if (regs[start] == 0xF0000000) {
-         auto it = subdword_regs.find(start);
-         assert(it != subdword_regs.end());
-         return it->second[start.byte()] + 1 <= 1;
-      }
-      return regs[start] + 1 <= 1;
-   }
+            /* helper function for debugging */
+            UNUSED void
+            print_regs(ra_ctx& ctx, PhysRegInterval regs, const RegisterFile& reg_file)
+            {
+                  char reg_char = regs.lo().reg() >= 256 ? 'v' : 's';
+                  const int max_regs_per_line = 64;
+
+                  /* print markers */
+                  printf("       ");
+                  for (int i = 0; i < std::min<int>(max_regs_per_line, ROUND_DOWN_TO(regs.size, 4)); i += 4) {
+                        printf("%-3.2u ", i);
+                  }
+                  printf("\n");
+
+                  /* print usage */
+                  auto line_begin_it = regs.begin();
+                  while (line_begin_it != regs.end()) {
+                        const int regs_in_line =
+                        std::min<int>(max_regs_per_line, std::distance(line_begin_it, regs.end()));
+
+                        if (line_begin_it == regs.begin()) {
+                              printf("%cgprs: ", reg_char);
+                        } else {
+                              printf("  %+4d ", std::distance(regs.begin(), line_begin_it));
+                        }
+                        const auto line_end_it = std::next(line_begin_it, regs_in_line);
+
+                        for (auto reg_it = line_begin_it; reg_it != line_end_it; ++reg_it) {
+                              bool has_adjacent_variable =
+                              (std::next(reg_it) != line_end_it &&
+                              reg_file[*reg_it] != reg_file[*std::next(reg_it)] && reg_file[*std::next(reg_it)]);
+                              print_reg(reg_file, *reg_it, has_adjacent_variable);
+                        }
+
+                        line_begin_it = line_end_it;
+                        printf("\n");
+                  }
+
+                  const unsigned free_regs =
+                  std::count_if(regs.begin(), regs.end(), [&](auto reg) { return !reg_file[reg]; });
+                  printf("%u/%u used, %u/%u free\n", regs.size - free_regs, regs.size, free_regs, regs.size);
+
+                  /* print assignments ordered by registers */
+                  std::map<PhysReg, std::pair<unsigned, unsigned>> regs_to_vars; /* maps to byte size and temp id */
+                  for (unsigned id : find_vars(ctx, reg_file, regs)) {
+                        const assignment& var = ctx.assignments[id];
+                        PhysReg reg = var.reg;
+                        ASSERTED auto inserted = regs_to_vars.emplace(reg, std::make_pair(var.rc.bytes(), id));
+                        assert(inserted.second);
+                  }
+
+                  for (const auto& reg_and_var : regs_to_vars) {
+                        const auto& first_reg = reg_and_var.first;
+                        const auto& size_id = reg_and_var.second;
+
+                        printf("%%%u ", size_id.second);
+                        if (ctx.orig_names.count(size_id.second) &&
+                              ctx.orig_names[size_id.second].id() != size_id.second) {
+                              printf("(was %%%d) ", ctx.orig_names[size_id.second].id());
+                              }
+                              printf("= %c[%d", reg_char, first_reg.reg() % 256);
+                        PhysReg last_reg = first_reg.advance(size_id.first - 1);
+                        if (first_reg.reg() != last_reg.reg()) {
+                              assert(first_reg.byte() == 0 && last_reg.byte() == 3);
+                              printf("-%d", last_reg.reg() % 256);
+                        }
+                        printf("]");
+                        if (first_reg.byte() != 0 || last_reg.byte() != 3) {
+                              printf("[%d:%d]", first_reg.byte() * 8, (last_reg.byte() + 1) * 8);
+                        }
+                        printf("\n");
+                  }
+            }
 
-   void clear(PhysReg start, RegClass rc)
-   {
-      if (rc.is_subdword())
-         fill_subdword(start, rc.bytes(), 0);
-      else
-         fill(start, rc.size(), 0);
-   }
-
-   void fill_killed_operands(Instruction* instr)
-   {
-      for (Operand& op : instr->operands) {
-         if (op.isPrecolored()) {
-            block(op.physReg(), op.regClass());
-         } else if (op.isFixed() && op.isFirstKillBeforeDef()) {
-            if (op.regClass().is_subdword())
-               fill_subdword(op.physReg(), op.bytes(), op.tempId());
-            else
-               fill(op.physReg(), op.size(), op.tempId());
-         }
-      }
-   }
+            bool
+            is_sgpr_writable_without_side_effects(amd_gfx_level gfx_level, PhysReg reg)
+            {
+                  assert(reg < 256);
+                  bool has_flat_scr_lo_gfx89 = gfx_level >= GFX8 && gfx_level <= GFX9;
+                  bool has_flat_scr_lo_gfx7_or_xnack_mask = gfx_level <= GFX9;
+                  return (reg <= vcc_hi || reg == m0) &&
+                  (!has_flat_scr_lo_gfx89 || (reg != flat_scr_lo && reg != flat_scr_hi)) &&
+                  (!has_flat_scr_lo_gfx7_or_xnack_mask || (reg.reg() != 104 && reg.reg() != 105));
+            }
 
-   void clear(Operand op)
-   {
-      if (op.isTemp() && !is_empty_or_blocked(op.physReg()))
-         assert(get_id(op.physReg()) == op.tempId());
-      clear(op.physReg(), op.regClass());
-   }
-
-   void fill(Definition def)
-   {
-      if (def.regClass().is_subdword())
-         fill_subdword(def.physReg(), def.bytes(), def.tempId());
-      else
-         fill(def.physReg(), def.size(), def.tempId());
-   }
-
-   void clear(Definition def) { clear(def.physReg(), def.regClass()); }
-
-   unsigned get_id(PhysReg reg) const
-   {
-      return regs[reg] == 0xF0000000 ? subdword_regs.at(reg)[reg.byte()] : regs[reg];
-   }
-
-private:
-   void fill(PhysReg start, unsigned size, uint32_t val)
-   {
-      for (unsigned i = 0; i < size; i++)
-         regs[start + i] = val;
-   }
-
-   void fill_subdword(PhysReg start, unsigned num_bytes, uint32_t val)
-   {
-      fill(start, DIV_ROUND_UP(num_bytes, 4), 0xF0000000);
-      for (PhysReg i = start; i.reg_b < start.reg_b + num_bytes; i = PhysReg(i + 1)) {
-         /* emplace or get */
-         std::array<uint32_t, 4>& sub =
-            subdword_regs.emplace(i, std::array<uint32_t, 4>{0, 0, 0, 0}).first->second;
-         for (unsigned j = i.byte(); i * 4 + j < start.reg_b + num_bytes && j < 4; j++)
-            sub[j] = val;
-
-         if (sub == std::array<uint32_t, 4>{0, 0, 0, 0}) {
-            subdword_regs.erase(i);
-            regs[i] = 0;
-         }
-      }
-   }
-};
+            unsigned
+            get_subdword_operand_stride(amd_gfx_level gfx_level, const aco_ptr<Instruction>& instr,
+                                        unsigned idx, RegClass rc)
+            {
+                  assert(gfx_level >= GFX8);
+                  if (instr->isPseudo()) {
+                        /* v_readfirstlane_b32 cannot use SDWA */
+                        if (instr->opcode == aco_opcode::p_as_uniform)
+                              return 4;
+                        else
+                              return rc.bytes() % 2 == 0 ? 2 : 1;
+                  }
+
+                  assert(rc.bytes() <= 2);
+                  if (instr->isVALU()) {
+                        if (can_use_SDWA(gfx_level, instr, false))
+                              return rc.bytes();
+                        if (can_use_opsel(gfx_level, instr->opcode, idx))
+                              return 2;
+                        if (instr->isVOP3P())
+                              return 2;
+                  }
+
+                  switch (instr->opcode) {
+                        case aco_opcode::v_cvt_f32_ubyte0: return 1;
+                        case aco_opcode::ds_write_b8:
+                        case aco_opcode::ds_write_b16: return gfx_level >= GFX9 ? 2 : 4;
+                        case aco_opcode::buffer_store_byte:
+                        case aco_opcode::buffer_store_short:
+                        case aco_opcode::buffer_store_format_d16_x:
+                        case aco_opcode::flat_store_byte:
+                        case aco_opcode::flat_store_short:
+                        case aco_opcode::scratch_store_byte:
+                        case aco_opcode::scratch_store_short:
+                        case aco_opcode::global_store_byte:
+                        case aco_opcode::global_store_short: return gfx_level >= GFX9 ? 2 : 4;
+                        default: return 4;
+                  }
+            }
 
-std::vector<unsigned> find_vars(ra_ctx& ctx, const RegisterFile& reg_file,
-                                const PhysRegInterval reg_interval);
+            void
+            add_subdword_operand(ra_ctx& ctx, aco_ptr<Instruction>& instr, unsigned idx, unsigned byte,
+                                 RegClass rc)
+            {
+                  amd_gfx_level gfx_level = ctx.program->gfx_level;
+                  if (instr->isPseudo() || byte == 0)
+                        return;
+
+                  assert(rc.bytes() <= 2);
+                  if (instr->isVALU()) {
+                        if (instr->opcode == aco_opcode::v_cvt_f32_ubyte0) {
+                              switch (byte) {
+                                    case 0: instr->opcode = aco_opcode::v_cvt_f32_ubyte0; break;
+                                    case 1: instr->opcode = aco_opcode::v_cvt_f32_ubyte1; break;
+                                    case 2: instr->opcode = aco_opcode::v_cvt_f32_ubyte2; break;
+                                    case 3: instr->opcode = aco_opcode::v_cvt_f32_ubyte3; break;
+                              }
+                              return;
+                        }
+
+                        /* use SDWA */
+                        if (can_use_SDWA(gfx_level, instr, false)) {
+                              convert_to_SDWA(gfx_level, instr);
+                              return;
+                        }
+
+                        /* use opsel */
+                        if (instr->isVOP3P()) {
+                              assert(byte == 2 && !instr->valu().opsel_lo[idx]);
+                              instr->valu().opsel_lo[idx] = true;
+                              instr->valu().opsel_hi[idx] = true;
+                              return;
+                        }
+
+                        assert(can_use_opsel(gfx_level, instr->opcode, idx));
+                        instr->valu().opsel[idx] = true;
+                        return;
+                  }
+
+                  assert(byte == 2);
+                  if (instr->opcode == aco_opcode::ds_write_b8)
+                        instr->opcode = aco_opcode::ds_write_b8_d16_hi;
+                  else if (instr->opcode == aco_opcode::ds_write_b16)
+                        instr->opcode = aco_opcode::ds_write_b16_d16_hi;
+                  else if (instr->opcode == aco_opcode::buffer_store_byte)
+                        instr->opcode = aco_opcode::buffer_store_byte_d16_hi;
+                  else if (instr->opcode == aco_opcode::buffer_store_short)
+                        instr->opcode = aco_opcode::buffer_store_short_d16_hi;
+                  else if (instr->opcode == aco_opcode::buffer_store_format_d16_x)
+                        instr->opcode = aco_opcode::buffer_store_format_d16_hi_x;
+                  else if (instr->opcode == aco_opcode::flat_store_byte)
+                        instr->opcode = aco_opcode::flat_store_byte_d16_hi;
+                  else if (instr->opcode == aco_opcode::flat_store_short)
+                        instr->opcode = aco_opcode::flat_store_short_d16_hi;
+                  else if (instr->opcode == aco_opcode::scratch_store_byte)
+                        instr->opcode = aco_opcode::scratch_store_byte_d16_hi;
+                  else if (instr->opcode == aco_opcode::scratch_store_short)
+                        instr->opcode = aco_opcode::scratch_store_short_d16_hi;
+                  else if (instr->opcode == aco_opcode::global_store_byte)
+                        instr->opcode = aco_opcode::global_store_byte_d16_hi;
+                  else if (instr->opcode == aco_opcode::global_store_short)
+                        instr->opcode = aco_opcode::global_store_short_d16_hi;
+                  else
+                        unreachable("Something went wrong: Impossible register assignment.");
+                  return;
+            }
 
-/* helper function for debugging */
-UNUSED void
-print_reg(const RegisterFile& reg_file, PhysReg reg, bool has_adjacent_variable)
-{
-   if (reg_file[reg] == 0xFFFFFFFF) {
-      printf((const char*)u8"â");
-   } else if (reg_file[reg]) {
-      const bool show_subdword_alloc = (reg_file[reg] == 0xF0000000);
-      if (show_subdword_alloc) {
-         auto block_chars = {
-            // clang-format off
-            u8"?", u8"â", u8"â", u8"â",
-            u8"â", u8"â", u8"â", u8"â",
-            u8"â", u8"â", u8"â", u8"â",
-            u8"â", u8"â", u8"â", u8"â"
-            // clang-format on
-         };
-         unsigned index = 0;
-         for (int i = 0; i < 4; ++i) {
-            if (reg_file.subdword_regs.at(reg)[i]) {
-               index |= 1 << i;
-            }
-         }
-         printf("%s", (const char*)(block_chars.begin()[index]));
-      } else {
-         /* Indicate filled register slot */
-         if (!has_adjacent_variable) {
-            printf((const char*)u8"â");
-         } else {
-            /* Use a slightly shorter box to leave a small gap between adjacent variables */
-            printf((const char*)u8"â");
-         }
-      }
-   } else {
-      printf((const char*)u8"Â·");
-   }
-}
-
-/* helper function for debugging */
-UNUSED void
-print_regs(ra_ctx& ctx, PhysRegInterval regs, const RegisterFile& reg_file)
-{
-   char reg_char = regs.lo().reg() >= 256 ? 'v' : 's';
-   const int max_regs_per_line = 64;
-
-   /* print markers */
-   printf("       ");
-   for (int i = 0; i < std::min<int>(max_regs_per_line, ROUND_DOWN_TO(regs.size, 4)); i += 4) {
-      printf("%-3.2u ", i);
-   }
-   printf("\n");
-
-   /* print usage */
-   auto line_begin_it = regs.begin();
-   while (line_begin_it != regs.end()) {
-      const int regs_in_line =
-         std::min<int>(max_regs_per_line, std::distance(line_begin_it, regs.end()));
-
-      if (line_begin_it == regs.begin()) {
-         printf("%cgprs: ", reg_char);
-      } else {
-         printf("  %+4d ", std::distance(regs.begin(), line_begin_it));
-      }
-      const auto line_end_it = std::next(line_begin_it, regs_in_line);
+            void
+            DefInfo::get_subdword_definition_info(Program* program, const aco_ptr<Instruction>& instr)
+            {
+                  amd_gfx_level gfx_level = program->gfx_level;
+                  assert(gfx_level >= GFX8);
+
+                  stride = rc.bytes() % 2 == 0 ? 2 : 1;
+
+                  if (instr->isPseudo()) {
+                        if (instr->opcode == aco_opcode::p_interp_gfx11) {
+                              rc = RegClass(RegType::vgpr, rc.size());
+                              stride = 1;
+                        }
+                        return;
+                  }
+
+                  if (instr->isVALU()) {
+                        assert(rc.bytes() <= 2);
+
+                        if (can_use_SDWA(gfx_level, instr, false) || instr->opcode == aco_opcode::p_v_cvt_pk_u8_f32)
+                              return;
+
+                        rc = instr_is_16bit(gfx_level, instr->opcode) ? v2b : v1;
+                        stride = rc == v2b ? 4 : 1;
+                        if (instr->opcode == aco_opcode::v_fma_mixlo_f16 ||
+                              can_use_opsel(gfx_level, instr->opcode, -1)) {
+                              data_stride = 2;
+                        stride = rc == v2b ? 2 : stride;
+                              }
+                              return;
+                  }
+
+                  switch (instr->opcode) {
+                        case aco_opcode::v_interp_p2_f16: return;
+                        /* D16 loads with _hi version */
+                        case aco_opcode::ds_read_u8_d16:
+                        case aco_opcode::ds_read_i8_d16:
+                        case aco_opcode::ds_read_u16_d16:
+                        case aco_opcode::flat_load_ubyte_d16:
+                        case aco_opcode::flat_load_sbyte_d16:
+                        case aco_opcode::flat_load_short_d16:
+                        case aco_opcode::global_load_ubyte_d16:
+                        case aco_opcode::global_load_sbyte_d16:
+                        case aco_opcode::global_load_short_d16:
+                        case aco_opcode::scratch_load_ubyte_d16:
+                        case aco_opcode::scratch_load_sbyte_d16:
+                        case aco_opcode::scratch_load_short_d16:
+                        case aco_opcode::buffer_load_ubyte_d16:
+                        case aco_opcode::buffer_load_sbyte_d16:
+                        case aco_opcode::buffer_load_short_d16:
+                        case aco_opcode::buffer_load_format_d16_x: {
+                              assert(gfx_level >= GFX9);
+                              if (program->dev.sram_ecc_enabled) {
+                                    rc = v1;
+                                    stride = 1;
+                                    data_stride = 2;
+                              } else {
+                                    stride = 2;
+                              }
+                              return;
+                        }
+                        /* 3-component D16 loads */
+                        case aco_opcode::buffer_load_format_d16_xyz:
+                        case aco_opcode::tbuffer_load_format_d16_xyz: {
+                              assert(gfx_level >= GFX9);
+                              if (program->dev.sram_ecc_enabled) {
+                                    rc = v2;
+                                    stride = 1;
+                              } else {
+                                    stride = 4;
+                              }
+                              return;
+                        }
+                        default: break;
+                  }
+
+                  if (instr->isMIMG() && instr->mimg().d16 && !program->dev.sram_ecc_enabled) {
+                        assert(gfx_level >= GFX9);
+                        stride = 4;
+                  } else {
+                        rc = RegClass(RegType::vgpr, rc.size());
+                        stride = 1;
+                  }
+            }
 
-      for (auto reg_it = line_begin_it; reg_it != line_end_it; ++reg_it) {
-         bool has_adjacent_variable =
-            (std::next(reg_it) != line_end_it &&
-             reg_file[*reg_it] != reg_file[*std::next(reg_it)] && reg_file[*std::next(reg_it)]);
-         print_reg(reg_file, *reg_it, has_adjacent_variable);
-      }
+            void
+            add_subdword_definition(Program* program, aco_ptr<Instruction>& instr, PhysReg reg,
+                                    bool allow_16bit_write)
+            {
+                  if (instr->isPseudo())
+                        return;
+
+                  if (instr->isVALU()) {
+                        amd_gfx_level gfx_level = program->gfx_level;
+                        assert(instr->definitions[0].bytes() <= 2);
+
+                        if (instr->opcode == aco_opcode::p_v_cvt_pk_u8_f32)
+                              return;
+
+                        if (reg.byte() == 0 && allow_16bit_write && instr_is_16bit(gfx_level, instr->opcode))
+                              return;
+
+                        /* use SDWA */
+                        if (can_use_SDWA(gfx_level, instr, false)) {
+                              convert_to_SDWA(gfx_level, instr);
+                              return;
+                        }
+
+                        assert(allow_16bit_write);
+
+                        if (instr->opcode == aco_opcode::v_fma_mixlo_f16) {
+                              instr->opcode = aco_opcode::v_fma_mixhi_f16;
+                              return;
+                        }
+
+                        /* use opsel */
+                        assert(reg.byte() == 2);
+                        assert(can_use_opsel(gfx_level, instr->opcode, -1));
+                        instr->valu().opsel[3] = true; /* dst in high half */
+                        return;
+                  }
+
+                  if (reg.byte() == 0)
+                        return;
+                  else if (instr->opcode == aco_opcode::v_interp_p2_f16)
+                        instr->opcode = aco_opcode::v_interp_p2_hi_f16;
+                  else if (instr->opcode == aco_opcode::buffer_load_ubyte_d16)
+                        instr->opcode = aco_opcode::buffer_load_ubyte_d16_hi;
+                  else if (instr->opcode == aco_opcode::buffer_load_sbyte_d16)
+                        instr->opcode = aco_opcode::buffer_load_sbyte_d16_hi;
+                  else if (instr->opcode == aco_opcode::buffer_load_short_d16)
+                        instr->opcode = aco_opcode::buffer_load_short_d16_hi;
+                  else if (instr->opcode == aco_opcode::buffer_load_format_d16_x)
+                        instr->opcode = aco_opcode::buffer_load_format_d16_hi_x;
+                  else if (instr->opcode == aco_opcode::flat_load_ubyte_d16)
+                        instr->opcode = aco_opcode::flat_load_ubyte_d16_hi;
+                  else if (instr->opcode == aco_opcode::flat_load_sbyte_d16)
+                        instr->opcode = aco_opcode::flat_load_sbyte_d16_hi;
+                  else if (instr->opcode == aco_opcode::flat_load_short_d16)
+                        instr->opcode = aco_opcode::flat_load_short_d16_hi;
+                  else if (instr->opcode == aco_opcode::scratch_load_ubyte_d16)
+                        instr->opcode = aco_opcode::scratch_load_ubyte_d16_hi;
+                  else if (instr->opcode == aco_opcode::scratch_load_sbyte_d16)
+                        instr->opcode = aco_opcode::scratch_load_sbyte_d16_hi;
+                  else if (instr->opcode == aco_opcode::scratch_load_short_d16)
+                        instr->opcode = aco_opcode::scratch_load_short_d16_hi;
+                  else if (instr->opcode == aco_opcode::global_load_ubyte_d16)
+                        instr->opcode = aco_opcode::global_load_ubyte_d16_hi;
+                  else if (instr->opcode == aco_opcode::global_load_sbyte_d16)
+                        instr->opcode = aco_opcode::global_load_sbyte_d16_hi;
+                  else if (instr->opcode == aco_opcode::global_load_short_d16)
+                        instr->opcode = aco_opcode::global_load_short_d16_hi;
+                  else if (instr->opcode == aco_opcode::ds_read_u8_d16)
+                        instr->opcode = aco_opcode::ds_read_u8_d16_hi;
+                  else if (instr->opcode == aco_opcode::ds_read_i8_d16)
+                        instr->opcode = aco_opcode::ds_read_i8_d16_hi;
+                  else if (instr->opcode == aco_opcode::ds_read_u16_d16)
+                        instr->opcode = aco_opcode::ds_read_u16_d16_hi;
+                  else
+                        unreachable("Something went wrong: Impossible register assignment.");
+            }
 
-      line_begin_it = line_end_it;
-      printf("\n");
-   }
-
-   const unsigned free_regs =
-      std::count_if(regs.begin(), regs.end(), [&](auto reg) { return !reg_file[reg]; });
-   printf("%u/%u used, %u/%u free\n", regs.size - free_regs, regs.size, free_regs, regs.size);
-
-   /* print assignments ordered by registers */
-   std::map<PhysReg, std::pair<unsigned, unsigned>> regs_to_vars; /* maps to byte size and temp id */
-   for (unsigned id : find_vars(ctx, reg_file, regs)) {
-      const assignment& var = ctx.assignments[id];
-      PhysReg reg = var.reg;
-      ASSERTED auto inserted = regs_to_vars.emplace(reg, std::make_pair(var.rc.bytes(), id));
-      assert(inserted.second);
-   }
-
-   for (const auto& reg_and_var : regs_to_vars) {
-      const auto& first_reg = reg_and_var.first;
-      const auto& size_id = reg_and_var.second;
-
-      printf("%%%u ", size_id.second);
-      if (ctx.orig_names.count(size_id.second) &&
-          ctx.orig_names[size_id.second].id() != size_id.second) {
-         printf("(was %%%d) ", ctx.orig_names[size_id.second].id());
-      }
-      printf("= %c[%d", reg_char, first_reg.reg() % 256);
-      PhysReg last_reg = first_reg.advance(size_id.first - 1);
-      if (first_reg.reg() != last_reg.reg()) {
-         assert(first_reg.byte() == 0 && last_reg.byte() == 3);
-         printf("-%d", last_reg.reg() % 256);
-      }
-      printf("]");
-      if (first_reg.byte() != 0 || last_reg.byte() != 3) {
-         printf("[%d:%d]", first_reg.byte() * 8, (last_reg.byte() + 1) * 8);
-      }
-      printf("\n");
-   }
-}
-
-bool
-is_sgpr_writable_without_side_effects(amd_gfx_level gfx_level, PhysReg reg)
-{
-   assert(reg < 256);
-   bool has_flat_scr_lo_gfx89 = gfx_level >= GFX8 && gfx_level <= GFX9;
-   bool has_flat_scr_lo_gfx7_or_xnack_mask = gfx_level <= GFX9;
-   return (reg <= vcc_hi || reg == m0) &&
-          (!has_flat_scr_lo_gfx89 || (reg != flat_scr_lo && reg != flat_scr_hi)) &&
-          (!has_flat_scr_lo_gfx7_or_xnack_mask || (reg != 104 || reg != 105));
-}
-
-unsigned
-get_subdword_operand_stride(amd_gfx_level gfx_level, const aco_ptr<Instruction>& instr,
-                            unsigned idx, RegClass rc)
-{
-   assert(gfx_level >= GFX8);
-   if (instr->isPseudo()) {
-      /* v_readfirstlane_b32 cannot use SDWA */
-      if (instr->opcode == aco_opcode::p_as_uniform)
-         return 4;
-      else
-         return rc.bytes() % 2 == 0 ? 2 : 1;
-   }
-
-   assert(rc.bytes() <= 2);
-   if (instr->isVALU()) {
-      if (can_use_SDWA(gfx_level, instr, false))
-         return rc.bytes();
-      if (can_use_opsel(gfx_level, instr->opcode, idx))
-         return 2;
-      if (instr->isVOP3P())
-         return 2;
-   }
-
-   switch (instr->opcode) {
-   case aco_opcode::v_cvt_f32_ubyte0: return 1;
-   case aco_opcode::ds_write_b8:
-   case aco_opcode::ds_write_b16: return gfx_level >= GFX9 ? 2 : 4;
-   case aco_opcode::buffer_store_byte:
-   case aco_opcode::buffer_store_short:
-   case aco_opcode::buffer_store_format_d16_x:
-   case aco_opcode::flat_store_byte:
-   case aco_opcode::flat_store_short:
-   case aco_opcode::scratch_store_byte:
-   case aco_opcode::scratch_store_short:
-   case aco_opcode::global_store_byte:
-   case aco_opcode::global_store_short: return gfx_level >= GFX9 ? 2 : 4;
-   default: return 4;
-   }
-}
-
-void
-add_subdword_operand(ra_ctx& ctx, aco_ptr<Instruction>& instr, unsigned idx, unsigned byte,
-                     RegClass rc)
-{
-   amd_gfx_level gfx_level = ctx.program->gfx_level;
-   if (instr->isPseudo() || byte == 0)
-      return;
-
-   assert(rc.bytes() <= 2);
-   if (instr->isVALU()) {
-      if (instr->opcode == aco_opcode::v_cvt_f32_ubyte0) {
-         switch (byte) {
-         case 0: instr->opcode = aco_opcode::v_cvt_f32_ubyte0; break;
-         case 1: instr->opcode = aco_opcode::v_cvt_f32_ubyte1; break;
-         case 2: instr->opcode = aco_opcode::v_cvt_f32_ubyte2; break;
-         case 3: instr->opcode = aco_opcode::v_cvt_f32_ubyte3; break;
-         }
-         return;
-      }
+            void
+            adjust_max_used_regs(ra_ctx& ctx, RegClass rc, unsigned reg)
+            {
+                  uint16_t max_addressible_sgpr = ctx.limit.sgpr;
+                  unsigned size = rc.size();
+                  if (rc.type() == RegType::vgpr) {
+                        assert(reg >= 256);
+                        uint16_t hi = reg - 256 + size - 1;
+                        assert(hi <= 255);
+                        ctx.max_used_vgpr = std::max(ctx.max_used_vgpr, hi);
+                  } else if (reg + rc.size() <= max_addressible_sgpr) {
+                        uint16_t hi = reg + size - 1;
+                        ctx.max_used_sgpr = std::max(ctx.max_used_sgpr, std::min(hi, max_addressible_sgpr));
+                  }
+            }
 
-      /* use SDWA */
-      if (can_use_SDWA(gfx_level, instr, false)) {
-         convert_to_SDWA(gfx_level, instr);
-         return;
-      }
+            void
+            update_renames(ra_ctx&                     ctx,
+                           RegisterFile&               reg_file,
+                           std::vector<parallelcopy>&  parallelcopies,
+                           aco_ptr<Instruction>&       instr,
+                           bool                        clear_operands = true)
+            {
+                  using TempId = unsigned;
+
+                  struct OperandInfo {
+                        Operand* ptr;
+                        unsigned idx;
+                  };
+
+                  auto rf_clear_if_needed = [&](const parallelcopy& pc) {
+                        if (!pc.def.isTemp() && pc.copy_kill < 0)
+                              reg_file.clear(pc.op);
+                  };
+
+                  auto rf_fill_if_needed = [&](const Definition& def, bool do_fill) {
+                        if (do_fill)
+                              reg_file.fill(def);
+                  };
+
+                  auto fix_kill_flags = [](std::vector<OperandInfo>& list,
+                                           std::bitset<2>&           first_seen,
+                                           bool                      kill_val,
+                                           bool                      omit_renaming)
+                  {
+                        for (OperandInfo& info : list) {
+                              Operand& op = *info.ptr;
+                              if (first_seen[omit_renaming]) {
+                                    op.setFirstKill(kill_val);
+                                    first_seen.set(omit_renaming, false);
+                              } else
+                                    op.setKill(kill_val);
+                        }
+                  };
+
+                  /* ------------------------------------------------------------------ *
+                   *  1. Early register-file cleanup                                    *
+                   * ------------------------------------------------------------------ */
+                  if (clear_operands)
+                        for (const parallelcopy& pc : parallelcopies)
+                              rf_clear_if_needed(pc);
+
+                  /* ------------------------------------------------------------------ *
+                   *  2. Build fast lookup tables                                       *
+                   * ------------------------------------------------------------------ */
+                  std::unordered_map<TempId, Definition*>                     def_of;
+                  std::unordered_map<TempId, std::vector<OperandInfo>>        uses_of;
+                  std::unordered_map<TempId, vector_operand*>                 vec_of;
+                  std::unordered_map<TempId, parallelcopy*>                   pc_def_of;
+
+                  /* defs ------------------------------------------------------------- */
+                  for (Definition& def : instr->definitions)
+                        if (def.isTemp())
+                              def_of.emplace(def.tempId(), &def);
+
+                  /* operands --------------------------------------------------------- */
+                  for (unsigned i = 0; i < instr->operands.size(); ++i) {
+                        Operand& op = instr->operands[i];
+                        if (op.isTemp())
+                              uses_of[op.tempId()].push_back({&op, i});
+                  }
+
+                  /* vector operands -------------------------------------------------- */
+                  for (vector_operand& vec : ctx.vector_operands)
+                        if (vec.def.isTemp())
+                              vec_of.emplace(vec.def.tempId(), &vec);
+
+                  /* other parallelcopy defs ----------------------------------------- */
+                  for (parallelcopy& pc : parallelcopies)
+                        if (pc.def.isTemp())
+                              pc_def_of.emplace(pc.def.getTemp().id(), &pc);
+
+                  /* ------------------------------------------------------------------ *
+                   *  3. Main single-pass processing                                   *
+                   * ------------------------------------------------------------------ */
+                  for (auto it = parallelcopies.begin(); it != parallelcopies.end();)
+                  {
+                        parallelcopy& pc          = *it;
+
+                        /* 3.a  Ignore copies that define a tmp produced by this instr --- */
+                        if (pc.def.isTemp()) { ++it; continue; }
+
+                        const bool     is_copy_kill = pc.copy_kill >= 0;
+                        const TempId   src_id       = pc.op.getTemp().id();
+                        const PhysReg  dst_reg      = pc.def.physReg();
+
+                        /* ----------------------------------------------------------------
+                         * Case 1 : the temp is one of the instructionâs definitions
+                         * ---------------------------------------------------------------- */
+                        if (!is_copy_kill) {
+                              if (auto it_def = def_of.find(src_id); it_def != def_of.end()) {
+                                    Definition& def = *it_def->second;
+
+                                    /* move the definition */
+                                    def.setFixed(dst_reg);
+                                    reg_file.fill(def);
+                                    ctx.assignments[def.tempId()].reg = dst_reg;
+
+                                    it = parallelcopies.erase(it);
+                                    continue;
+                              }
+                        }
+
+                        /* ----------------------------------------------------------------
+                         * Case 2 : the temp is a vector operand
+                         * ---------------------------------------------------------------- */
+                        if (auto it_vec = vec_of.find(src_id); it_vec != vec_of.end()) {
+                              vector_operand& vec = *it_vec->second;
+
+                              vec.def.setFixed(dst_reg);
+                              reg_file.fill(vec.def);
+                              ctx.assignments[vec.def.tempId()].reg = dst_reg;
+
+                              it = parallelcopies.erase(it);
+                              continue;
+                        }
+
+                        /* ----------------------------------------------------------------
+                         * Case 3 : the temp is defined by another parallelcopy
+                         * ---------------------------------------------------------------- */
+                        if (auto it_other = pc_def_of.find(src_id); it_other != pc_def_of.end()) {
+                              parallelcopy& other = *it_other->second;
+
+                              if (is_copy_kill) {
+                                    /* copy-kill; simply rewrite the source operand */
+                                    pc.op = other.op;
+                              } else {
+                                    /* normal copy: move the definition to dst_reg */
+                                    other.def.setFixed(dst_reg);
+                                    ctx.assignments[other.def.tempId()].reg = dst_reg;
+
+                                    /* rename operands that reference the other.def temp */
+                                    bool fill = true;
+                                    if (auto it_uses = uses_of.find(other.def.tempId()); it_uses != uses_of.end()) {
+                                          std::bitset<2> first_seen(3); // Fixed: both bits set to true
+
+                                          for (OperandInfo& info : it_uses->second) {
+                                                Operand& op = *info.ptr;
+
+                                                /* only rename pre-coloured if reg matches */
+                                                bool omit_rename = op.isPrecolored() && op.physReg() != dst_reg;
+
+                                                /* If we omit renaming the operand is considered killed   */
+                                                bool kill = op.isKill() || omit_rename;
+
+                                                fix_kill_flags(it_uses->second, first_seen, kill, omit_rename);
+
+                                                if (omit_rename)
+                                                      continue;
+
+                                                op.setTemp(other.def.getTemp());
+                                                if (op.isFixed())
+                                                      op.setFixed(dst_reg);
+
+                                                bool rf_before = op.isPrecolored();
+                                                fill = !op.isKillBeforeDef() || rf_before;
+                                          }
+                                    }
+                                    rf_fill_if_needed(other.def, fill);
+
+                                    /* remove *this* parallelcopy */
+                                    it = parallelcopies.erase(it);
+                                    continue;
+                              }
+                        }
+
+                        /* ----------------------------------------------------------------
+                         * Case 4 : fall-back â need fresh temporary + operand renaming
+                         * ---------------------------------------------------------------- */
+                        pc.def.setTemp(ctx.program->allocateTmp(pc.def.regClass()));
+                        ctx.assignments.emplace_back(dst_reg, pc.def.regClass());
+                        assert(ctx.assignments.size() == ctx.program->peekAllocationId());
+
+                        /* Determine the ârepresentativeâ operand (copy_op) -------------- */
+                        const Operand& copy_op =
+                        is_copy_kill ? instr->operands[pc.copy_kill] : pc.op;
+
+                        bool fill          = !is_copy_kill;
+                        std::bitset<2> first_seen(3); // Fixed: both bits set to true
+
+                        if (auto it_use = uses_of.find(copy_op.tempId()); it_use != uses_of.end()) {
+                              for (OperandInfo& info : it_use->second) {
+                                    Operand& op  = *info.ptr;
+                                    const bool is_this_kill_idx = is_copy_kill && info.idx == static_cast<unsigned>(pc.copy_kill);
+
+                                    bool omit_rename = op.isPrecolored() && op.physReg() != dst_reg;
+                                    omit_rename     |= is_copy_kill && !is_this_kill_idx;
+
+                                    /* kill logic mirrors original code ------------------------ */
+                                    bool kill =
+                                    op.isKill() ||
+                                    ( omit_rename && !is_copy_kill) ||
+                                    (!omit_rename &&  is_copy_kill);
+
+                                    fix_kill_flags(it_use->second, first_seen, kill, omit_rename);
+
+                                    if (omit_rename)
+                                          continue;
+
+                                    op.setTemp(pc.def.getTemp());
+                                    if (op.isFixed())
+                                          op.setFixed(dst_reg);
+
+                                    bool rf_before = op.isPrecolored() || is_copy_kill;
+                                    fill = !op.isKillBeforeDef() || rf_before;
+                              }
+                        }
+
+                        rf_fill_if_needed(pc.def, fill);
+                        ++it;
+                  }
+            }
 
-      /* use opsel */
-      if (instr->isVOP3P()) {
-         assert(byte == 2 && !instr->valu().opsel_lo[idx]);
-         instr->valu().opsel_lo[idx] = true;
-         instr->valu().opsel_hi[idx] = true;
-         return;
-      }
+            std::optional<PhysReg>
+            get_reg_simple(ra_ctx& ctx, const RegisterFile& reg_file, DefInfo info)
+            {
+                  /* ---- 1.  Unpack & canonicalise inputs -------------------------------- */
+                  PhysRegInterval bounds = info.bounds;
+                  const uint32_t size    = info.size;
+                  const RegClass  rc     = info.rc;
+
+                  /* stride is expressed in DWORDs; for sub-dword we round-up to bytes/4   */
+                  uint32_t stride = rc.is_subdword() ? DIV_ROUND_UP(info.stride, 4u)
+                  : info.stride;
+
+                  /* ---- 2.  Try recursive âskip every other slotâ optimisation ---------- */
+                  if (stride < size && !rc.is_subdword()) {
+                        DefInfo nested = info;
+                        nested.stride  = stride * 2;
+                        if (size % nested.stride == 0) {
+                              if (auto res = get_reg_simple(ctx, reg_file, nested))
+                                    return res;
+                        }
+                  }
+
+                  /* ---- 3.  Fast re-use of round-robin iterator ------------------------- */
+                  PhysRegIterator& rr_it = rc.type() == RegType::vgpr ? ctx.rr_vgpr_it
+                  : ctx.rr_sgpr_it;
+
+                  if (stride == 1 && rr_it != bounds.begin() && bounds.contains(rr_it.reg)) {
+                        /* Try upper part of the interval first */
+                        info.bounds = PhysRegInterval::from_until(rr_it.reg, bounds.hi());
+                        if (auto res = get_reg_simple(ctx, reg_file, info))
+                              return res;
+
+                        /* Otherwise restrict search to the lower part */
+                        bounds = PhysRegInterval::from_until(bounds.lo(), rr_it.reg);
+                  }
+
+                  /* ---- 4.  Plain whole-dword scan ------------------------------------- */
+                  auto is_free = [&](PhysReg r)
+                  { return reg_file[r] == 0 && !ctx.war_hint[r]; };
+
+                  for (PhysRegInterval win{bounds.lo(), size}; win.hi() <= bounds.hi();
+                       win += stride)
+                       {
+                             if (std::all_of(win.begin(), win.end(), is_free)) {
+                                   if (stride == 1) {
+                                         PhysRegIterator new_rr{PhysReg{win.lo() + size}};
+                                         if (new_rr < bounds.end())
+                                               rr_it = new_rr;
+                                   }
+                                   adjust_max_used_regs(ctx, rc, win.lo());
+                                   return win.lo();
+                             }
+                       }
+
+                       /* ---- 5.  Late sub-dword allocation ---------------------------------- */
+                       if (rc.is_subdword()) {
+                             const unsigned need_bytes = rc.bytes();   /* 1, 2 or 4                 */
+                             const unsigned step       = stride;       /* already byte-wise         */
+                             assert(step > 0);
+
+                             for (const auto& [reg_num, byte_state] : reg_file.subdword_regs) {
+
+                                   /* sanity: 0xF0000000 marks âsub-dword in useâ */
+                                   assert(reg_file[PhysReg{reg_num}] == 0xF0000000);
+
+                                   if (!bounds.contains({PhysReg{reg_num}, rc.size()}))
+                                         continue;
+
+                                         for (unsigned off = 0; off < 4; off += step) {
+                                               /* 5.1 check bytes inside this DWORD ------------------------- */
+                                               bool free_here = true;
+                                               for (unsigned b = 0; b < need_bytes && free_here; ++b)
+                                                     if (byte_state[off + b] != 0)
+                                                           free_here = false;
+                                               if (!free_here)
+                                                     continue;
+
+                                               /* 5.2 neighbour DWORD, if spill-over ------------------------ */
+                                               if (off + need_bytes > 4 &&
+                                                     reg_file[PhysReg{reg_num + 1}] != 0)
+                                                     continue;
+
+                                               /* 5.3 success ----------------------------------------------- */
+                                               PhysReg res{reg_num};
+                                               res.reg_b += off;
+                                               adjust_max_used_regs(ctx, rc, reg_num);
+                                               return res;
+                                         }
+                             }
+                       }
 
-      assert(can_use_opsel(gfx_level, instr->opcode, idx));
-      instr->valu().opsel[idx] = true;
-      return;
-   }
-
-   assert(byte == 2);
-   if (instr->opcode == aco_opcode::ds_write_b8)
-      instr->opcode = aco_opcode::ds_write_b8_d16_hi;
-   else if (instr->opcode == aco_opcode::ds_write_b16)
-      instr->opcode = aco_opcode::ds_write_b16_d16_hi;
-   else if (instr->opcode == aco_opcode::buffer_store_byte)
-      instr->opcode = aco_opcode::buffer_store_byte_d16_hi;
-   else if (instr->opcode == aco_opcode::buffer_store_short)
-      instr->opcode = aco_opcode::buffer_store_short_d16_hi;
-   else if (instr->opcode == aco_opcode::buffer_store_format_d16_x)
-      instr->opcode = aco_opcode::buffer_store_format_d16_hi_x;
-   else if (instr->opcode == aco_opcode::flat_store_byte)
-      instr->opcode = aco_opcode::flat_store_byte_d16_hi;
-   else if (instr->opcode == aco_opcode::flat_store_short)
-      instr->opcode = aco_opcode::flat_store_short_d16_hi;
-   else if (instr->opcode == aco_opcode::scratch_store_byte)
-      instr->opcode = aco_opcode::scratch_store_byte_d16_hi;
-   else if (instr->opcode == aco_opcode::scratch_store_short)
-      instr->opcode = aco_opcode::scratch_store_short_d16_hi;
-   else if (instr->opcode == aco_opcode::global_store_byte)
-      instr->opcode = aco_opcode::global_store_byte_d16_hi;
-   else if (instr->opcode == aco_opcode::global_store_short)
-      instr->opcode = aco_opcode::global_store_short_d16_hi;
-   else
-      unreachable("Something went wrong: Impossible register assignment.");
-   return;
-}
-
-void
-DefInfo::get_subdword_definition_info(Program* program, const aco_ptr<Instruction>& instr)
-{
-   amd_gfx_level gfx_level = program->gfx_level;
-   assert(gfx_level >= GFX8);
-
-   stride = rc.bytes() % 2 == 0 ? 2 : 1;
-
-   if (instr->isPseudo()) {
-      if (instr->opcode == aco_opcode::p_interp_gfx11) {
-         rc = RegClass(RegType::vgpr, rc.size());
-         stride = 1;
-      }
-      return;
-   }
+                       return {};
+            }
 
-   if (instr->isVALU()) {
-      assert(rc.bytes() <= 2);
+            std::vector<unsigned> find_vars(ra_ctx& ctx, const RegisterFile& reg_file,
+                                            const PhysRegInterval reg_interval)
+            {
+                  std::vector<unsigned> vars;
+                  if (UNLIKELY(reg_interval.size == 0))
+                        return vars;
+
+                  vars.reserve(
+                        std::max(static_cast<size_t>(16),
+                                 static_cast<size_t>(reg_interval.size) * 2)
+                  );
+
+                  unsigned last_added_id = 0;
+
+                  for (PhysReg reg_iter_dw : reg_interval) {
+                        uint32_t dword_val = reg_file.regs[reg_iter_dw.reg()];
+
+                        if (dword_val == 0xFFFFFFFF)
+                              continue;
+
+                        if (dword_val == 0xF0000000) {
+                              auto it = reg_file.subdword_regs.find(reg_iter_dw.reg());
+                              assert(it != reg_file.subdword_regs.end() && "Subdword container marked but no entry in map");
+                              if (LIKELY(it != reg_file.subdword_regs.end())) {
+                                    const std::array<uint32_t, 4>& sub_array = it->second;
+                                    for (unsigned k = 0; k < 4; ++k) {
+                                          uint32_t id = sub_array[k];
+                                          if (id != 0 && id != 0xFFFFFFFF) {
+                                                if (id != last_added_id) {
+                                                      vars.push_back(id);
+                                                      last_added_id = id;
+                                                }
+                                          } else if (id == 0) {
+                                                last_added_id = 0;
+                                          }
+                                    }
+                              }
+                        } else if (dword_val != 0) {
+                              unsigned id = dword_val;
+                              if (id != last_added_id) {
+                                    vars.push_back(id);
+                                    last_added_id = id;
+                              }
+                        } else {
+                              last_added_id = 0;
+                        }
+                  }
+                  return vars;
+            }
 
-      if (can_use_SDWA(gfx_level, instr, false) || instr->opcode == aco_opcode::p_v_cvt_pk_u8_f32)
-         return;
+            /* collect variables from a register area and clear reg_file
+             * variables are sorted in decreasing size and
+             * increasing assigned register
+             */
+            std::vector<unsigned>
+            collect_vars(ra_ctx& ctx, RegisterFile& reg_file, const PhysRegInterval reg_interval)
+            {
+                  std::vector<unsigned> ids = find_vars(ctx, reg_file, reg_interval);
+                  std::sort(ids.begin(), ids.end(),
+                            [&](unsigned a, unsigned b)
+                            {
+                                  assignment& var_a = ctx.assignments[a];
+                                  assignment& var_b = ctx.assignments[b];
+                                  return var_a.rc.bytes() > var_b.rc.bytes() ||
+                                  (var_a.rc.bytes() == var_b.rc.bytes() && var_a.reg < var_b.reg);
+                            });
+
+                  for (unsigned id : ids) {
+                        assignment& var = ctx.assignments[id];
+                        reg_file.clear(var.reg, var.rc);
+                  }
+                  return ids;
+            }
 
-      rc = instr_is_16bit(gfx_level, instr->opcode) ? v2b : v1;
-      stride = rc == v2b ? 4 : 1;
-      if (instr->opcode == aco_opcode::v_fma_mixlo_f16 ||
-          can_use_opsel(gfx_level, instr->opcode, -1)) {
-         data_stride = 2;
-         stride = rc == v2b ? 2 : stride;
-      }
-      return;
-   }
+            std::optional<PhysReg>
+            get_reg_for_create_vector_copy(ra_ctx& ctx, RegisterFile& reg_file,
+                                           std::vector<parallelcopy>& parallelcopies,
+                                           aco_ptr<Instruction>& instr, const PhysRegInterval def_reg,
+                                           DefInfo info, unsigned id)
+            {
+                  PhysReg reg = def_reg.lo();
+                  /* dead operand: return position in vector */
+                  for (unsigned i = 0; i < instr->operands.size(); i++) {
+                        if (instr->operands[i].isTemp() && instr->operands[i].tempId() == id &&
+                              instr->operands[i].isKillBeforeDef()) {
+                              assert(!reg_file.test(reg, instr->operands[i].bytes()));
+                        if (info.rc.is_subdword() || reg.byte() == 0)
+                              return reg;
+                              else
+                                    return {};
+                              }
+                              reg.reg_b += instr->operands[i].bytes();
+                  }
+
+                  /* GFX9+ has a VGPR swap instruction. */
+                  if (ctx.program->gfx_level <= GFX8 || info.rc.type() == RegType::sgpr)
+                        return {};
+
+                  /* check if the previous position was in vector */
+                  assignment& var = ctx.assignments[id];
+                  if (def_reg.contains(PhysRegInterval{var.reg, info.size})) {
+                        reg = def_reg.lo();
+                        /* try to use the previous register of the operand */
+                        for (unsigned i = 0; i < instr->operands.size(); i++) {
+                              if (reg != var.reg) {
+                                    reg.reg_b += instr->operands[i].bytes();
+                                    continue;
+                              }
+
+                              /* check if we can swap positions */
+                              if (instr->operands[i].isTemp() && instr->operands[i].isFirstKill() &&
+                                    instr->operands[i].regClass() == info.rc) {
+                                    assignment& op = ctx.assignments[instr->operands[i].tempId()];
+                              /* if everything matches, create parallelcopy for the killed operand */
+                              if (!intersects(def_reg, PhysRegInterval{op.reg, op.rc.size()}) && op.reg != scc &&
+                                    reg_file.get_id(op.reg) == instr->operands[i].tempId()) {
+                                    Definition pc_def = Definition(reg, info.rc);
+                                    parallelcopies.emplace_back(instr->operands[i], pc_def);
+                                    return op.reg;
+                                    }
+                                    }
+                                    return {};
+                        }
+                  }
+                  return {};
+            }
 
-   switch (instr->opcode) {
-   case aco_opcode::v_interp_p2_f16: return;
-   /* D16 loads with _hi version */
-   case aco_opcode::ds_read_u8_d16:
-   case aco_opcode::ds_read_i8_d16:
-   case aco_opcode::ds_read_u16_d16:
-   case aco_opcode::flat_load_ubyte_d16:
-   case aco_opcode::flat_load_sbyte_d16:
-   case aco_opcode::flat_load_short_d16:
-   case aco_opcode::global_load_ubyte_d16:
-   case aco_opcode::global_load_sbyte_d16:
-   case aco_opcode::global_load_short_d16:
-   case aco_opcode::scratch_load_ubyte_d16:
-   case aco_opcode::scratch_load_sbyte_d16:
-   case aco_opcode::scratch_load_short_d16:
-   case aco_opcode::buffer_load_ubyte_d16:
-   case aco_opcode::buffer_load_sbyte_d16:
-   case aco_opcode::buffer_load_short_d16:
-   case aco_opcode::buffer_load_format_d16_x: {
-      assert(gfx_level >= GFX9);
-      if (program->dev.sram_ecc_enabled) {
-         rc = v1;
-         stride = 1;
-         data_stride = 2;
-      } else {
-         stride = 2;
-      }
-      return;
-   }
-   /* 3-component D16 loads */
-   case aco_opcode::buffer_load_format_d16_xyz:
-   case aco_opcode::tbuffer_load_format_d16_xyz: {
-      assert(gfx_level >= GFX9);
-      if (program->dev.sram_ecc_enabled) {
-         rc = v2;
-         stride = 1;
-      } else {
-         stride = 4;
-      }
-      return;
-   }
-   default: break;
-   }
-
-   if (instr->isMIMG() && instr->mimg().d16 && !program->dev.sram_ecc_enabled) {
-      assert(gfx_level >= GFX9);
-      stride = 4;
-   } else {
-      rc = RegClass(RegType::vgpr, rc.size());
-      stride = 1;
-   }
-}
-
-void
-add_subdword_definition(Program* program, aco_ptr<Instruction>& instr, PhysReg reg,
-                        bool allow_16bit_write)
-{
-   if (instr->isPseudo())
-      return;
-
-   if (instr->isVALU()) {
-      amd_gfx_level gfx_level = program->gfx_level;
-      assert(instr->definitions[0].bytes() <= 2);
-
-      if (instr->opcode == aco_opcode::p_v_cvt_pk_u8_f32)
-         return;
-
-      if (reg.byte() == 0 && allow_16bit_write && instr_is_16bit(gfx_level, instr->opcode))
-         return;
-
-      /* use SDWA */
-      if (can_use_SDWA(gfx_level, instr, false)) {
-         convert_to_SDWA(gfx_level, instr);
-         return;
-      }
+            bool
+            get_regs_for_copies(ra_ctx& ctx, RegisterFile& reg_file, std::vector<parallelcopy>& parallelcopies,
+                                const std::vector<unsigned>& vars, aco_ptr<Instruction>& instr,
+                                const PhysRegInterval def_reg)
+            {
+                  /* Variables are sorted from large to small and with increasing assigned register */
+                  for (unsigned id : vars) {
+                        assignment& var = ctx.assignments[id];
+                        PhysRegInterval bounds = get_reg_bounds(ctx, var.rc);
+                        DefInfo info = DefInfo(ctx, ctx.pseudo_dummy, var.rc, -1);
+                        uint32_t size = info.size;
+
+                        /* check if this is a dead operand, then we can re-use the space from the definition
+                         * also use the correct stride for sub-dword operands */
+                        bool is_dead_operand = false;
+                        std::optional<PhysReg> res;
+                        if (instr->opcode == aco_opcode::p_create_vector) {
+                              res =
+                              get_reg_for_create_vector_copy(ctx, reg_file, parallelcopies, instr, def_reg, info, id);
+                        } else {
+                              for (unsigned i = 0; !is_phi(instr) && i < instr->operands.size(); i++) {
+                                    if (instr->operands[i].isTemp() && instr->operands[i].tempId() == id) {
+                                          info = DefInfo(ctx, instr, var.rc, i);
+                                          if (instr->operands[i].isKillBeforeDef()) {
+                                                info.bounds = def_reg;
+                                                res = get_reg_simple(ctx, reg_file, info);
+                                                is_dead_operand = true;
+                                          }
+                                          break;
+                                    }
+                              }
+                        }
+                        if (!res && !def_reg.size) {
+                              /* If this is before definitions are handled, def_reg may be an empty interval. */
+                              info.bounds = bounds;
+                              res = get_reg_simple(ctx, reg_file, info);
+                        } else if (!res) {
+                              /* Try to find space within the bounds but outside of the definition */
+                              info.bounds = PhysRegInterval::from_until(bounds.lo(), MIN2(def_reg.lo(), bounds.hi()));
+                              res = get_reg_simple(ctx, reg_file, info);
+                              if (!res && def_reg.hi() <= bounds.hi()) {
+                                    unsigned lo = (def_reg.hi() + info.stride - 1) & ~(info.stride - 1);
+                                    info.bounds = PhysRegInterval::from_until(PhysReg{lo}, bounds.hi());
+                                    res = get_reg_simple(ctx, reg_file, info);
+                              }
+                        }
+
+                        if (res) {
+                              /* mark the area as blocked */
+                              reg_file.block(*res, var.rc);
+
+                              /* create parallelcopy pair (without definition id) */
+                              Temp tmp = Temp(id, var.rc);
+                              Operand pc_op = Operand(tmp);
+                              pc_op.setFixed(var.reg);
+                              Definition pc_def = Definition(*res, pc_op.regClass());
+                              parallelcopies.emplace_back(pc_op, pc_def);
+                              continue;
+                        }
+
+                        PhysReg best_pos = bounds.lo();
+                        unsigned num_moves = 0xFF;
+                        unsigned num_vars = 0;
+
+                        /* we use a sliding window to find potential positions */
+                        unsigned stride = var.rc.is_subdword() ? 1 : info.stride;
+                        for (PhysRegInterval reg_win{bounds.lo(), size}; reg_win.hi() <= bounds.hi();
+                             reg_win += stride) {
+                              if (!is_dead_operand && intersects(reg_win, def_reg))
+                                    continue;
+
+                              /* second, check that we have at most k=num_moves elements in the window
+                               * and no element is larger than the currently processed one */
+                              unsigned k = 0;
+                              unsigned n = 0;
+                              unsigned last_var = 0;
+                              bool found = true;
+                              for (PhysReg j : reg_win) {
+                                    if (reg_file[j] == 0 || reg_file[j] == last_var)
+                                          continue;
+
+                                    if (reg_file.is_blocked(j) || k > num_moves) {
+                                          found = false;
+                                          break;
+                                    }
+                                    if (reg_file[j] == 0xF0000000) {
+                                          k += 1;
+                                          n++;
+                                          continue;
+                                    }
+                                    /* we cannot split live ranges of linear vgprs */
+                                    if (ctx.assignments[reg_file[j]].rc.is_linear_vgpr()) {
+                                          found = false;
+                                          break;
+                                    }
+                                    bool is_kill = false;
+                                    for (const Operand& op : instr->operands) {
+                                          if (op.isTemp() && op.isKillBeforeDef() && op.tempId() == reg_file[j]) {
+                                                is_kill = true;
+                                                break;
+                                          }
+                                    }
+                                    if (!is_kill && ctx.assignments[reg_file[j]].rc.size() >= size) {
+                                          found = false;
+                                          break;
+                                    }
+
+                                    k += ctx.assignments[reg_file[j]].rc.size();
+                                    last_var = reg_file[j];
+                                    n++;
+                                    if (k > num_moves || (k == num_moves && n <= num_vars)) {
+                                          found = false;
+                                          break;
+                                    }
+                              }
+
+                              if (found) {
+                                    best_pos = reg_win.lo();
+                                    num_moves = k;
+                                    num_vars = n;
+                              }
+                             }
+
+                             /* FIXME: we messed up and couldn't find space for the variables to be copied */
+                             if (num_moves == 0xFF)
+                                   return false;
+
+                        PhysRegInterval reg_win{best_pos, size};
+
+                        /* collect variables and block reg file */
+                        std::vector<unsigned> new_vars = collect_vars(ctx, reg_file, reg_win);
+
+                        /* mark the area as blocked */
+                        reg_file.block(reg_win.lo(), var.rc);
+                        adjust_max_used_regs(ctx, var.rc, reg_win.lo());
+
+                        if (!get_regs_for_copies(ctx, reg_file, parallelcopies, new_vars, instr, def_reg))
+                              return false;
+
+                        /* create parallelcopy pair (without definition id) */
+                        Temp tmp = Temp(id, var.rc);
+                        Operand pc_op = Operand(tmp);
+                        pc_op.setFixed(var.reg);
+                        Definition pc_def = Definition(reg_win.lo(), pc_op.regClass());
+                        parallelcopies.emplace_back(pc_op, pc_def);
+                  }
 
-      assert(allow_16bit_write);
+                  return true;
+            }
 
-      if (instr->opcode == aco_opcode::v_fma_mixlo_f16) {
-         instr->opcode = aco_opcode::v_fma_mixhi_f16;
-         return;
-      }
+            std::optional<PhysReg>
+            get_reg_impl(ra_ctx& ctx, const RegisterFile& reg_file, std::vector<parallelcopy>& parallelcopies,
+                         const DefInfo& info, aco_ptr<Instruction>& instr)
+            {
+                  const PhysRegInterval& bounds = info.bounds;
+                  uint32_t size = info.size;
+                  uint32_t stride = info.rc.is_subdword() ? DIV_ROUND_UP(info.stride, 4) : info.stride;
+                  RegClass rc = info.rc;
+
+                  /* check how many free regs we have */
+                  unsigned regs_free = reg_file.count_zero(get_reg_bounds(ctx, rc));
+
+                  /* mark and count killed operands */
+                  unsigned killed_ops = 0;
+                  std::bitset<256> is_killed_operand; /* per-register */
+                  std::bitset<256> is_precolored;     /* per-register */
+                  for (unsigned j = 0; !is_phi(instr) && j < instr->operands.size(); j++) {
+                        Operand& op = instr->operands[j];
+                        if (op.isTemp() && op.isPrecolored() && !op.isFirstKillBeforeDef() &&
+                              bounds.contains(op.physReg())) {
+                              for (unsigned i = 0; i < op.size(); ++i) {
+                                    is_precolored[(op.physReg() & 0xff) + i] = true;
+                              }
+                              }
+                              if (op.isTemp() && op.isFirstKillBeforeDef() && bounds.contains(op.physReg()) &&
+                                    !reg_file.test(PhysReg{op.physReg().reg()}, align(op.bytes() + op.physReg().byte(), 4))) {
+                                    assert(op.isFixed());
+
+                              for (unsigned i = 0; i < op.size(); ++i) {
+                                    is_killed_operand[(op.physReg() & 0xff) + i] = true;
+                              }
+
+                              killed_ops += op.getTemp().size();
+                                    }
+                  }
+                  for (unsigned j = 0; !is_phi(instr) && j < instr->definitions.size(); j++) {
+                        Definition& def = instr->definitions[j];
+                        if (def.isTemp() && def.isPrecolored() && bounds.contains(def.physReg())) {
+                              for (unsigned i = 0; i < def.size(); ++i) {
+                                    is_precolored[(def.physReg() & 0xff) + i] = true;
+                              }
+                        }
+                  }
+
+                  assert((regs_free + ctx.num_linear_vgprs) >= size);
+
+                  /* we might have to move dead operands to dst in order to make space */
+                  unsigned op_moves = 0;
+
+                  if (size > (regs_free - killed_ops))
+                        op_moves = size - (regs_free - killed_ops);
+
+                  /* find the best position to place the definition */
+                  PhysRegInterval best_win = {bounds.lo(), size};
+                  unsigned num_moves = 0xFF;
+                  unsigned num_vars = 0;
+
+                  /* we use a sliding window to check potential positions */
+                  for (PhysRegInterval reg_win = {bounds.lo(), size}; reg_win.hi() <= bounds.hi();
+                       reg_win += stride) {
+                        /* first check if the register window starts in the middle of an
+                         * allocated variable: this is what we have to fix to allow for
+                         * num_moves > size */
+                        if (reg_win.lo() > bounds.lo() && !reg_file.is_empty_or_blocked(reg_win.lo()) &&
+                              reg_file.get_id(reg_win.lo()) == reg_file.get_id(reg_win.lo().advance(-1)))
+                              continue;
+                        if (reg_win.hi() < bounds.hi() && !reg_file.is_empty_or_blocked(reg_win.hi().advance(-1)) &&
+                              reg_file.get_id(reg_win.hi().advance(-1)) == reg_file.get_id(reg_win.hi()))
+                              continue;
+
+                        /* second, check that we have at most k=num_moves elements in the window
+                         * and no element is larger than the currently processed one */
+                        unsigned k = op_moves;
+                        unsigned n = 0;
+                        unsigned remaining_op_moves = op_moves;
+                        unsigned last_var = 0;
+                        bool found = true;
+                        bool aligned = rc == RegClass::v4 && reg_win.lo() % 4 == 0;
+                        for (const PhysReg j : reg_win) {
+                              /* dead operands effectively reduce the number of estimated moves */
+                              if (is_killed_operand[j & 0xFF]) {
+                                    if (remaining_op_moves) {
+                                          k--;
+                                          remaining_op_moves--;
+                                    }
+                                    continue;
+                              }
+                              if (is_precolored[j & 0xFF]) {
+                                    found = false;
+                                    break;
+                              }
+
+                              if (reg_file[j] == 0 || reg_file[j] == last_var)
+                                    continue;
+
+                              if (reg_file[j] == 0xF0000000) {
+                                    k += 1;
+                                    n++;
+                                    continue;
+                              }
+
+                              if (ctx.assignments[reg_file[j]].rc.size() >= size) {
+                                    found = false;
+                                    break;
+                              }
+
+                              /* we cannot split live ranges of linear vgprs */
+                              if (ctx.assignments[reg_file[j]].rc.is_linear_vgpr()) {
+                                    found = false;
+                                    break;
+                              }
+
+                              k += ctx.assignments[reg_file[j]].rc.size();
+                              n++;
+                              last_var = reg_file[j];
+                        }
+
+                        if (!found || k > num_moves)
+                              continue;
+                        if (k == num_moves && n < num_vars)
+                              continue;
+                        if (!aligned && k == num_moves && n == num_vars)
+                              continue;
+
+                        if (found) {
+                              best_win = reg_win;
+                              num_moves = k;
+                              num_vars = n;
+                        }
+                       }
+
+                       if (num_moves == 0xFF)
+                             return {};
+
+                  /* now, we figured the placement for our definition */
+                  RegisterFile tmp_file(reg_file);
+
+                  /* p_create_vector: also re-place killed operands in the definition space */
+                  if (instr->opcode == aco_opcode::p_create_vector)
+                        tmp_file.fill_killed_operands(instr.get());
+
+                  std::vector<unsigned> vars = collect_vars(ctx, tmp_file, best_win);
+
+                  /* re-enable killed operands */
+                  if (!is_phi(instr) && instr->opcode != aco_opcode::p_create_vector)
+                        tmp_file.fill_killed_operands(instr.get());
+
+                  std::vector<parallelcopy> pc;
+                  if (!get_regs_for_copies(ctx, tmp_file, pc, vars, instr, best_win))
+                        return {};
 
-      /* use opsel */
-      assert(reg.byte() == 2);
-      assert(can_use_opsel(gfx_level, instr->opcode, -1));
-      instr->valu().opsel[3] = true; /* dst in high half */
-      return;
-   }
-
-   if (reg.byte() == 0)
-      return;
-   else if (instr->opcode == aco_opcode::v_interp_p2_f16)
-      instr->opcode = aco_opcode::v_interp_p2_hi_f16;
-   else if (instr->opcode == aco_opcode::buffer_load_ubyte_d16)
-      instr->opcode = aco_opcode::buffer_load_ubyte_d16_hi;
-   else if (instr->opcode == aco_opcode::buffer_load_sbyte_d16)
-      instr->opcode = aco_opcode::buffer_load_sbyte_d16_hi;
-   else if (instr->opcode == aco_opcode::buffer_load_short_d16)
-      instr->opcode = aco_opcode::buffer_load_short_d16_hi;
-   else if (instr->opcode == aco_opcode::buffer_load_format_d16_x)
-      instr->opcode = aco_opcode::buffer_load_format_d16_hi_x;
-   else if (instr->opcode == aco_opcode::flat_load_ubyte_d16)
-      instr->opcode = aco_opcode::flat_load_ubyte_d16_hi;
-   else if (instr->opcode == aco_opcode::flat_load_sbyte_d16)
-      instr->opcode = aco_opcode::flat_load_sbyte_d16_hi;
-   else if (instr->opcode == aco_opcode::flat_load_short_d16)
-      instr->opcode = aco_opcode::flat_load_short_d16_hi;
-   else if (instr->opcode == aco_opcode::scratch_load_ubyte_d16)
-      instr->opcode = aco_opcode::scratch_load_ubyte_d16_hi;
-   else if (instr->opcode == aco_opcode::scratch_load_sbyte_d16)
-      instr->opcode = aco_opcode::scratch_load_sbyte_d16_hi;
-   else if (instr->opcode == aco_opcode::scratch_load_short_d16)
-      instr->opcode = aco_opcode::scratch_load_short_d16_hi;
-   else if (instr->opcode == aco_opcode::global_load_ubyte_d16)
-      instr->opcode = aco_opcode::global_load_ubyte_d16_hi;
-   else if (instr->opcode == aco_opcode::global_load_sbyte_d16)
-      instr->opcode = aco_opcode::global_load_sbyte_d16_hi;
-   else if (instr->opcode == aco_opcode::global_load_short_d16)
-      instr->opcode = aco_opcode::global_load_short_d16_hi;
-   else if (instr->opcode == aco_opcode::ds_read_u8_d16)
-      instr->opcode = aco_opcode::ds_read_u8_d16_hi;
-   else if (instr->opcode == aco_opcode::ds_read_i8_d16)
-      instr->opcode = aco_opcode::ds_read_i8_d16_hi;
-   else if (instr->opcode == aco_opcode::ds_read_u16_d16)
-      instr->opcode = aco_opcode::ds_read_u16_d16_hi;
-   else
-      unreachable("Something went wrong: Impossible register assignment.");
-}
-
-void
-adjust_max_used_regs(ra_ctx& ctx, RegClass rc, unsigned reg)
-{
-   uint16_t max_addressible_sgpr = ctx.limit.sgpr;
-   unsigned size = rc.size();
-   if (rc.type() == RegType::vgpr) {
-      assert(reg >= 256);
-      uint16_t hi = reg - 256 + size - 1;
-      assert(hi <= 255);
-      ctx.max_used_vgpr = std::max(ctx.max_used_vgpr, hi);
-   } else if (reg + rc.size() <= max_addressible_sgpr) {
-      uint16_t hi = reg + size - 1;
-      ctx.max_used_sgpr = std::max(ctx.max_used_sgpr, std::min(hi, max_addressible_sgpr));
-   }
-}
-
-void
-update_renames(ra_ctx& ctx, RegisterFile& reg_file, std::vector<parallelcopy>& parallelcopies,
-               aco_ptr<Instruction>& instr, bool clear_operands = true)
-{
-   /* clear operands */
-   if (clear_operands) {
-      for (parallelcopy& copy : parallelcopies) {
-         /* the definitions with id are not from this function and already handled */
-         if (copy.def.isTemp() || copy.copy_kill >= 0)
-            continue;
-         reg_file.clear(copy.op);
-      }
-   }
+                  parallelcopies.insert(parallelcopies.end(), pc.begin(), pc.end());
 
-   /* allocate id's and rename operands: this is done transparently here */
-   auto it = parallelcopies.begin();
-   while (it != parallelcopies.end()) {
-      if (it->def.isTemp()) {
-         ++it;
-         continue;
-      }
+                  adjust_max_used_regs(ctx, rc, best_win.lo());
+                  return best_win.lo();
+            }
 
-      bool is_copy_kill = it->copy_kill >= 0;
+            bool
+            get_reg_specified(ra_ctx& ctx, const RegisterFile& reg_file, RegClass rc,
+                              aco_ptr<Instruction>& instr, PhysReg reg, int operand)
+            {
+                  /* catch out-of-range registers */
+                  if (reg >= PhysReg{512})
+                        return false;
+
+                        DefInfo info(ctx, instr, rc, operand);
+
+                        if (reg.reg_b % info.data_stride)
+                              return false;
+
+                  assert(util_is_power_of_two_nonzero(info.stride));
+                  reg.reg_b &= ~(info.stride - 1);
+
+                  PhysRegInterval reg_win = {PhysReg(reg.reg()), info.rc.size()};
+                  PhysRegInterval vcc_win = {vcc, 2};
+                  /* VCC is outside the bounds */
+                  bool is_vcc =
+                  info.rc.type() == RegType::sgpr && vcc_win.contains(reg_win) && ctx.program->needs_vcc;
+                  bool is_m0 = info.rc == s1 && reg == m0 && can_write_m0(instr);
+                  if (!info.bounds.contains(reg_win) && !is_vcc && !is_m0)
+                        return false;
+
+                  if (instr_info.classes[(int)instr->opcode] == instr_class::valu_pseudo_scalar_trans) {
+                        /* RDNA4 ISA doc, 7.10. Pseudo-scalar Transcendental ALU ops:
+                         * - VCC may not be used as a destination
+                         */
+                        if (vcc_win.contains(reg_win))
+                              return false;
+                  }
 
-      /* check if we moved a definition: change the register and remove copy */
-      bool is_def = false;
-      for (Definition& def : instr->definitions) {
-         if (def.isTemp() && def.getTemp() == it->op.getTemp()) {
-            assert(!is_copy_kill);
-            // FIXME: ensure that the definition can use this reg
-            def.setFixed(it->def.physReg());
-            reg_file.fill(def);
-            ctx.assignments[def.tempId()].reg = def.physReg();
-            it = parallelcopies.erase(it);
-            is_def = true;
-            break;
-         }
-      }
-      if (is_def)
-         continue;
+                  if (reg_file.test(reg, info.rc.bytes()))
+                        return false;
 
-      /* check if we moved a vector-operand */
-      for (vector_operand& vec : ctx.vector_operands) {
-         if (vec.def.getTemp() == it->op.getTemp()) {
-            vec.def.setFixed(it->def.physReg());
-            reg_file.fill(vec.def);
-            ctx.assignments[vec.def.tempId()].reg = vec.def.physReg();
-            it = parallelcopies.erase(it);
-            is_def = true;
-         }
-      }
-      if (is_def)
-         continue;
+                  adjust_max_used_regs(ctx, info.rc, reg_win.lo());
+                  return true;
+            }
 
-      /* Check if we moved another parallelcopy definition. We use a different path for copy-kill
-       * copies, since they are able to exist alongside a normal copy with the same operand.
-       */
-      for (parallelcopy& other : parallelcopies) {
-         if (!other.def.isTemp())
-            continue;
-         if (is_copy_kill && it->op.getTemp() == other.def.getTemp()) {
-            it->op = other.op;
-            break;
-         } else if (it->op.getTemp() == other.def.getTemp()) {
-            other.def.setFixed(it->def.physReg());
-            ctx.assignments[other.def.tempId()].reg = other.def.physReg();
-            it = parallelcopies.erase(it);
-            is_def = true;
-            /* check if we moved an operand, again */
-            bool fill = true;
-            for (Operand& op : instr->operands) {
-               if (op.isTemp() && op.tempId() == other.def.tempId()) {
-                  // FIXME: ensure that the operand can use this reg
-                  if (op.isFixed())
-                     op.setFixed(other.def.physReg());
-                  fill = !op.isKillBeforeDef();
-               }
-            }
-            if (fill)
-               reg_file.fill(other.def);
-            break;
-         }
-      }
-      if (is_def)
-         continue;
+            bool
+            increase_register_file(ra_ctx& ctx, RegClass rc)
+            {
+                  if (rc.type() == RegType::vgpr && ctx.num_linear_vgprs == 0 &&
+                        ctx.vgpr_bounds < ctx.limit.vgpr) {
+                        /* If vgpr_bounds is less than max_reg_demand.vgpr, this should be a no-op. */
+                        update_vgpr_sgpr_demand(
+                              ctx.program, RegisterDemand(ctx.vgpr_bounds + 1, ctx.program->max_reg_demand.sgpr));
+
+                        ctx.vgpr_bounds = ctx.program->max_reg_demand.vgpr;
+                        } else if (rc.type() == RegType::sgpr && ctx.program->max_reg_demand.sgpr < ctx.limit.sgpr) {
+                              update_vgpr_sgpr_demand(
+                                    ctx.program, RegisterDemand(ctx.program->max_reg_demand.vgpr, ctx.sgpr_bounds + 1));
+
+                              ctx.sgpr_bounds = ctx.program->max_reg_demand.sgpr;
+                        } else {
+                              return false;
+                        }
 
-      parallelcopy& copy = *it;
-      copy.def.setTemp(ctx.program->allocateTmp(copy.def.regClass()));
-      ctx.assignments.emplace_back(copy.def.physReg(), copy.def.regClass());
-      assert(ctx.assignments.size() == ctx.program->peekAllocationId());
-
-      /* Check if we moved an operand:
-       * For copy-kill operands, use the current Operand name so that kill flags stay correct.
-       */
-      Operand copy_op = is_copy_kill ? instr->operands[copy.copy_kill] : it->op;
-      bool first[2] = {true, true};
-      bool fill = !is_copy_kill;
-      for (unsigned i = 0; i < instr->operands.size(); i++) {
-         Operand& op = instr->operands[i];
-         if (!op.isTemp())
-            continue;
-         if (op.tempId() == copy_op.tempId()) {
-            /* only rename precolored operands if the copy-location matches */
-            bool omit_renaming = op.isPrecolored() && op.physReg() != copy.def.physReg();
-            omit_renaming |= is_copy_kill && i != (unsigned)copy.copy_kill;
-
-            /* If this is a copy-kill, then the renamed operand is killed since we don't rename any
-             * uses in other instructions. If it's a normal copy, then this operand is killed if we
-             * don't rename it since any future uses will be renamed to use the copy definition. */
-            bool kill =
-               op.isKill() || (omit_renaming && !is_copy_kill) || (!omit_renaming && is_copy_kill);
-
-            /* Fix the kill flags */
-            if (first[omit_renaming])
-               op.setFirstKill(kill);
-            else
-               op.setKill(kill);
-            first[omit_renaming] = false;
-
-            if (omit_renaming)
-               continue;
-
-            op.setTemp(copy.def.getTemp());
-            if (op.isFixed())
-               op.setFixed(copy.def.physReg());
+                        return true;
+            }
 
-            /* Copy-kill or precolored operand parallelcopies are only added when setting up
-             * operands.
-             */
-            bool is_reg_file_before_instr = op.isPrecolored() || is_copy_kill;
-            fill = !op.isKillBeforeDef() || is_reg_file_before_instr;
-         }
-      }
+            struct IDAndRegClass {
+                  IDAndRegClass(unsigned id_, RegClass rc_) : id(id_), rc(rc_) {}
 
-      /* Apply changes to register file. */
-      if (fill)
-         reg_file.fill(copy.def);
-
-      ++it;
-   }
-}
-
-std::optional<PhysReg>
-get_reg_simple(ra_ctx& ctx, const RegisterFile& reg_file, DefInfo info)
-{
-   PhysRegInterval bounds = info.bounds;
-   uint32_t size = info.size;
-   uint32_t stride = info.rc.is_subdword() ? DIV_ROUND_UP(info.stride, 4) : info.stride;
-   RegClass rc = info.rc;
-
-   if (stride < size && !rc.is_subdword()) {
-      DefInfo new_info = info;
-      new_info.stride = stride * 2;
-      if (size % new_info.stride == 0) {
-         std::optional<PhysReg> res = get_reg_simple(ctx, reg_file, new_info);
-         if (res)
-            return res;
-      }
-   }
+                  unsigned id;
+                  RegClass rc;
+            };
+
+            struct IDAndInfo {
+                  IDAndInfo(unsigned id_, DefInfo info_) : id(id_), info(info_) {}
+
+                  unsigned id;
+                  DefInfo info;
+            };
+
+            void
+            add_rename(ra_ctx& ctx, Temp orig_val, Temp new_val)
+            {
+                  ctx.renames[ctx.block->index][orig_val.id()] = new_val;
+                  ctx.orig_names.emplace(new_val.id(), orig_val);
+                  ctx.assignments[orig_val.id()].renamed = true;
+            }
 
-   PhysRegIterator& rr_it = rc.type() == RegType::vgpr ? ctx.rr_vgpr_it : ctx.rr_sgpr_it;
-   if (stride == 1) {
-      if (rr_it != bounds.begin() && bounds.contains(rr_it.reg)) {
-         assert(bounds.begin() < rr_it);
-         assert(rr_it < bounds.end());
-         info.bounds = PhysRegInterval::from_until(rr_it.reg, bounds.hi());
-         std::optional<PhysReg> res = get_reg_simple(ctx, reg_file, info);
-         if (res)
-            return res;
-         bounds = PhysRegInterval::from_until(bounds.lo(), rr_it.reg);
-      }
-   }
+            /* Reallocates vars by sorting them and placing each variable after the previous
+             * one. If one of the variables has 0xffffffff as an ID, the register assigned
+             * for that variable will be returned.
+             */
+            PhysReg
+            compact_relocate_vars(ra_ctx& ctx, const std::vector<IDAndRegClass>& vars,
+                                  std::vector<parallelcopy>& parallelcopies, PhysReg start)
+            {
+                  /* This function assumes RegisterDemand/live_var_analysis rounds up sub-dword
+                   * temporary sizes to dwords.
+                   */
+                  std::vector<IDAndInfo> sorted;
+                  for (IDAndRegClass var : vars) {
+                        DefInfo info(ctx, ctx.pseudo_dummy, var.rc, -1);
+                        sorted.emplace_back(var.id, info);
+                  }
+
+                  std::sort(
+                        sorted.begin(), sorted.end(),
+                            [=, &ctx](const IDAndInfo& a, const IDAndInfo& b)
+                            {
+                                  unsigned a_stride = MAX2(a.info.stride * (a.info.rc.is_subdword() ? 1 : 4), 4);
+                                  unsigned b_stride = MAX2(b.info.stride * (b.info.rc.is_subdword() ? 1 : 4), 4);
+                                  /* Since the SGPR bounds should always be a multiple of two, we can place
+                                   * variables in this order:
+                                   * - the usual 4 SGPR aligned variables
+                                   * - then the 0xffffffff variable
+                                   * - then the unaligned variables
+                                   * - and finally the 2 SGPR aligned variables
+                                   * This way, we should always be able to place variables if the 0xffffffff one
+                                   * had a NPOT size.
+                                   *
+                                   * This also lets us avoid placing the 0xffffffff variable in VCC if it's s1/s2
+                                   * (required for pseudo-scalar transcendental) and places it first if it's a
+                                   * VGPR variable (required for ImageGather4D16Bug).
+                                   */
+                                  assert(a.info.rc.type() != RegType::sgpr || get_reg_bounds(ctx, a.info.rc).size % 2 == 0);
+                                  assert(a_stride == 16 || a_stride == 8 || a_stride == 4);
+                                  assert(b_stride == 16 || b_stride == 8 || b_stride == 4);
+                                  assert(a.info.rc.size() % (a_stride / 4u) == 0);
+                                  assert(b.info.rc.size() % (b_stride / 4u) == 0);
+                                  if ((a_stride == 16) != (b_stride == 16))
+                                        return a_stride > b_stride;
+                                  if (a.id == 0xffffffff || b.id == 0xffffffff)
+                                        return a.id == 0xffffffff;
+                                  if (a_stride != b_stride)
+                                        return a_stride < b_stride;
+                                  return ctx.assignments[a.id].reg < ctx.assignments[b.id].reg;
+                            });
+
+                  PhysReg next_reg = start;
+                  PhysReg space_reg;
+                  for (IDAndInfo& var : sorted) {
+                        unsigned stride = var.info.rc.is_subdword() ? var.info.stride : var.info.stride * 4;
+                        next_reg.reg_b = align(next_reg.reg_b, MAX2(stride, 4));
+
+                        /* 0xffffffff is a special variable ID used reserve a space for killed
+                         * operands and definitions.
+                         */
+                        if (var.id != 0xffffffff) {
+                              if (next_reg != ctx.assignments[var.id].reg) {
+                                    RegClass rc = ctx.assignments[var.id].rc;
+                                    Temp tmp(var.id, rc);
+
+                                    Operand pc_op(tmp);
+                                    pc_op.setFixed(ctx.assignments[var.id].reg);
+                                    Definition pc_def(next_reg, rc);
+                                    parallelcopies.emplace_back(pc_op, pc_def);
+                              }
+                        } else {
+                              space_reg = next_reg;
+                        }
 
-   auto is_free = [&](PhysReg reg_index)
-   { return reg_file[reg_index] == 0 && !ctx.war_hint[reg_index]; };
+                        adjust_max_used_regs(ctx, var.info.rc, next_reg);
 
-   for (PhysRegInterval reg_win = {bounds.lo(), size}; reg_win.hi() <= bounds.hi();
-        reg_win += stride) {
-      if (std::all_of(reg_win.begin(), reg_win.end(), is_free)) {
-         if (stride == 1) {
-            PhysRegIterator new_rr_it{PhysReg{reg_win.lo() + size}};
-            if (new_rr_it < bounds.end())
-               rr_it = new_rr_it;
-         }
-         adjust_max_used_regs(ctx, rc, reg_win.lo());
-         return reg_win.lo();
-      }
-   }
+                        next_reg = next_reg.advance(var.info.rc.size() * 4);
+                  }
 
-   /* do this late because using the upper bytes of a register can require
-    * larger instruction encodings or copies
-    * TODO: don't do this in situations where it doesn't benefit */
-   if (rc.is_subdword()) {
-      for (const std::pair<const uint32_t, std::array<uint32_t, 4>>& entry :
-           reg_file.subdword_regs) {
-         assert(reg_file[PhysReg{entry.first}] == 0xF0000000);
-         if (!bounds.contains({PhysReg{entry.first}, rc.size()}))
-            continue;
-
-         auto it = entry.second.begin();
-         for (unsigned i = 0; i < 4; i += info.stride) {
-            /* check if there's a block of free bytes large enough to hold the register */
-            bool reg_found =
-               std::all_of(std::next(it, i), std::next(it, std::min(4u, i + rc.bytes())),
-                           [](unsigned v) { return v == 0; });
-
-            /* check if also the neighboring reg is free if needed */
-            if (reg_found && i + rc.bytes() > 4)
-               reg_found = (reg_file[PhysReg{entry.first + 1}] == 0);
-
-            if (reg_found) {
-               PhysReg res{entry.first};
-               res.reg_b += i;
-               adjust_max_used_regs(ctx, rc, entry.first);
-               return res;
+                  return space_reg;
             }
-         }
-      }
-   }
 
-   return {};
-}
+            bool
+            is_vector_intact(ra_ctx& ctx, const RegisterFile& reg_file, const vector_info& vec_info)
+            {
+                  unsigned size = 0;
+                  for (unsigned i = 0; i < vec_info.num_parts; i++)
+                        size += vec_info.parts[i].bytes();
+
+                  PhysReg first{512};
+                  int offset = 0;
+                  for (unsigned i = 0; i < vec_info.num_parts; i++) {
+                        Operand op = vec_info.parts[i];
+
+                        if (ctx.assignments[op.tempId()].assigned) {
+                              PhysReg reg = ctx.assignments[op.tempId()].reg;
+
+                              if (first.reg() == 512) {
+                                    PhysRegInterval bounds = get_reg_bounds(ctx, RegType::vgpr, false);
+                                    first = reg.advance(-offset);
+                                    PhysRegInterval vec = PhysRegInterval{first, DIV_ROUND_UP(size, 4)};
+                                    if (!bounds.contains(vec)) /* not enough space for other operands */
+                                          return false;
+                              } else {
+                                    if (reg != first.advance(offset)) /* not at the best position */
+                                          return false;
+                              }
+                        } else {
+                              /* If there's an unexpected temporary, this operand is unlikely to be
+                               * placed in the best position.
+                               */
+                              if (first.reg() != 512 && reg_file.test(first.advance(offset), op.bytes()))
+                                    return false;
+                        }
 
-/* collect variables from a register area */
-std::vector<unsigned>
-find_vars(ra_ctx& ctx, const RegisterFile& reg_file, const PhysRegInterval reg_interval)
-{
-   std::vector<unsigned> vars;
-   for (PhysReg j : reg_interval) {
-      if (reg_file.is_blocked(j))
-         continue;
-      if (reg_file[j] == 0xF0000000) {
-         for (unsigned k = 0; k < 4; k++) {
-            unsigned id = reg_file.subdword_regs.at(j)[k];
-            if (id && (vars.empty() || id != vars.back()))
-               vars.emplace_back(id);
-         }
-      } else {
-         unsigned id = reg_file[j];
-         if (id && (vars.empty() || id != vars.back()))
-            vars.emplace_back(id);
-      }
-   }
-   return vars;
-}
-
-/* collect variables from a register area and clear reg_file
- * variables are sorted in decreasing size and
- * increasing assigned register
- */
-std::vector<unsigned>
-collect_vars(ra_ctx& ctx, RegisterFile& reg_file, const PhysRegInterval reg_interval)
-{
-   std::vector<unsigned> ids = find_vars(ctx, reg_file, reg_interval);
-   std::sort(ids.begin(), ids.end(),
-             [&](unsigned a, unsigned b)
-             {
-                assignment& var_a = ctx.assignments[a];
-                assignment& var_b = ctx.assignments[b];
-                return var_a.rc.bytes() > var_b.rc.bytes() ||
-                       (var_a.rc.bytes() == var_b.rc.bytes() && var_a.reg < var_b.reg);
-             });
-
-   for (unsigned id : ids) {
-      assignment& var = ctx.assignments[id];
-      reg_file.clear(var.reg, var.rc);
-   }
-   return ids;
-}
-
-std::optional<PhysReg>
-get_reg_for_create_vector_copy(ra_ctx& ctx, RegisterFile& reg_file,
-                               std::vector<parallelcopy>& parallelcopies,
-                               aco_ptr<Instruction>& instr, const PhysRegInterval def_reg,
-                               DefInfo info, unsigned id)
-{
-   PhysReg reg = def_reg.lo();
-   /* dead operand: return position in vector */
-   for (unsigned i = 0; i < instr->operands.size(); i++) {
-      if (instr->operands[i].isTemp() && instr->operands[i].tempId() == id &&
-          instr->operands[i].isKillBeforeDef()) {
-         assert(!reg_file.test(reg, instr->operands[i].bytes()));
-         if (info.rc.is_subdword() || reg.byte() == 0)
-            return reg;
-         else
-            return {};
-      }
-      reg.reg_b += instr->operands[i].bytes();
-   }
+                        offset += op.bytes();
+                  }
 
-   /* GFX9+ has a VGPR swap instruction. */
-   if (ctx.program->gfx_level <= GFX8 || info.rc.type() == RegType::sgpr)
-      return {};
-
-   /* check if the previous position was in vector */
-   assignment& var = ctx.assignments[id];
-   if (def_reg.contains(PhysRegInterval{var.reg, info.size})) {
-      reg = def_reg.lo();
-      /* try to use the previous register of the operand */
-      for (unsigned i = 0; i < instr->operands.size(); i++) {
-         if (reg != var.reg) {
-            reg.reg_b += instr->operands[i].bytes();
-            continue;
-         }
-
-         /* check if we can swap positions */
-         if (instr->operands[i].isTemp() && instr->operands[i].isFirstKill() &&
-             instr->operands[i].regClass() == info.rc) {
-            assignment& op = ctx.assignments[instr->operands[i].tempId()];
-            /* if everything matches, create parallelcopy for the killed operand */
-            if (!intersects(def_reg, PhysRegInterval{op.reg, op.rc.size()}) && op.reg != scc &&
-                reg_file.get_id(op.reg) == instr->operands[i].tempId()) {
-               Definition pc_def = Definition(reg, info.rc);
-               parallelcopies.emplace_back(instr->operands[i], pc_def);
-               return op.reg;
+                  return true;
             }
-         }
-         return {};
-      }
-   }
-   return {};
-}
-
-bool
-get_regs_for_copies(ra_ctx& ctx, RegisterFile& reg_file, std::vector<parallelcopy>& parallelcopies,
-                    const std::vector<unsigned>& vars, aco_ptr<Instruction>& instr,
-                    const PhysRegInterval def_reg)
-{
-   /* Variables are sorted from large to small and with increasing assigned register */
-   for (unsigned id : vars) {
-      assignment& var = ctx.assignments[id];
-      PhysRegInterval bounds = get_reg_bounds(ctx, var.rc);
-      DefInfo info = DefInfo(ctx, ctx.pseudo_dummy, var.rc, -1);
-      uint32_t size = info.size;
-
-      /* check if this is a dead operand, then we can re-use the space from the definition
-       * also use the correct stride for sub-dword operands */
-      bool is_dead_operand = false;
-      std::optional<PhysReg> res;
-      if (instr->opcode == aco_opcode::p_create_vector) {
-         res =
-            get_reg_for_create_vector_copy(ctx, reg_file, parallelcopies, instr, def_reg, info, id);
-      } else {
-         for (unsigned i = 0; !is_phi(instr) && i < instr->operands.size(); i++) {
-            if (instr->operands[i].isTemp() && instr->operands[i].tempId() == id) {
-               info = DefInfo(ctx, instr, var.rc, i);
-               if (instr->operands[i].isKillBeforeDef()) {
-                  info.bounds = def_reg;
-                  res = get_reg_simple(ctx, reg_file, info);
-                  is_dead_operand = true;
-               }
-               break;
+
+            std::optional<PhysReg>
+            get_reg_vector(ra_ctx& ctx, const RegisterFile& reg_file, Temp temp, aco_ptr<Instruction>& instr,
+                           int operand)
+            {
+                  const vector_info& vec = ctx.vectors[temp.id()];
+                  if (!vec.is_weak || is_vector_intact(ctx, reg_file, vec)) {
+                        unsigned our_offset = 0;
+                        for (unsigned i = 0; i < vec.num_parts; i++) {
+                              const Operand& op = vec.parts[i];
+                              if (op.isTemp() && op.tempId() == temp.id())
+                                    break;
+                              else
+                                    our_offset += op.bytes();
+                        }
+
+                        unsigned their_offset = 0;
+                        /* check for every operand of the vector
+                         * - whether the operand is assigned and
+                         * - we can use the register relative to that operand
+                         */
+                        for (unsigned i = 0; i < vec.num_parts; i++) {
+                              const Operand& op = vec.parts[i];
+                              if (op.isTemp() && op.tempId() != temp.id() && op.getTemp().type() == temp.type() &&
+                                    ctx.assignments[op.tempId()].assigned) {
+                                    PhysReg reg = ctx.assignments[op.tempId()].reg;
+                              reg.reg_b += (our_offset - their_offset);
+                              if (get_reg_specified(ctx, reg_file, temp.regClass(), instr, reg, operand))
+                                    return reg;
+
+                                    /* return if MIMG vaddr components don't remain vector-aligned */
+                                    if (vec.is_weak)
+                                          return {};
+                                    }
+                                    their_offset += op.bytes();
+                        }
+
+                        /* We didn't find a register relative to other vector operands.
+                         * Try to find new space which fits the whole vector.
+                         */
+                        RegClass vec_rc = RegClass::get(temp.type(), their_offset);
+                        DefInfo info(ctx, ctx.pseudo_dummy, vec_rc, -1);
+                        std::optional<PhysReg> reg = get_reg_simple(ctx, reg_file, info);
+                        if (reg) {
+                              reg->reg_b += our_offset;
+                              /* make sure to only use byte offset if the instruction supports it */
+                              if (get_reg_specified(ctx, reg_file, temp.regClass(), instr, *reg, operand))
+                                    return reg;
+                        }
+                  }
+                  return {};
             }
-         }
-      }
-      if (!res && !def_reg.size) {
-         /* If this is before definitions are handled, def_reg may be an empty interval. */
-         info.bounds = bounds;
-         res = get_reg_simple(ctx, reg_file, info);
-      } else if (!res) {
-         /* Try to find space within the bounds but outside of the definition */
-         info.bounds = PhysRegInterval::from_until(bounds.lo(), MIN2(def_reg.lo(), bounds.hi()));
-         res = get_reg_simple(ctx, reg_file, info);
-         if (!res && def_reg.hi() <= bounds.hi()) {
-            unsigned lo = (def_reg.hi() + info.stride - 1) & ~(info.stride - 1);
-            info.bounds = PhysRegInterval::from_until(PhysReg{lo}, bounds.hi());
-            res = get_reg_simple(ctx, reg_file, info);
-         }
-      }
 
-      if (res) {
-         /* mark the area as blocked */
-         reg_file.block(*res, var.rc);
-
-         /* create parallelcopy pair (without definition id) */
-         Temp tmp = Temp(id, var.rc);
-         Operand pc_op = Operand(tmp);
-         pc_op.setFixed(var.reg);
-         Definition pc_def = Definition(*res, pc_op.regClass());
-         parallelcopies.emplace_back(pc_op, pc_def);
-         continue;
-      }
+            bool
+            compact_linear_vgprs(ra_ctx& ctx, const RegisterFile& reg_file,
+                                 std::vector<parallelcopy>& parallelcopies)
+            {
+                  PhysRegInterval linear_vgpr_bounds = get_reg_bounds(ctx, RegType::vgpr, true);
+                  int zeros = reg_file.count_zero(linear_vgpr_bounds);
+                  if (zeros == 0)
+                        return false;
+
+                  std::vector<IDAndRegClass> vars;
+                  for (unsigned id : find_vars(ctx, reg_file, linear_vgpr_bounds))
+                        vars.emplace_back(id, ctx.assignments[id].rc);
 
-      PhysReg best_pos = bounds.lo();
-      unsigned num_moves = 0xFF;
-      unsigned num_vars = 0;
-
-      /* we use a sliding window to find potential positions */
-      unsigned stride = var.rc.is_subdword() ? 1 : info.stride;
-      for (PhysRegInterval reg_win{bounds.lo(), size}; reg_win.hi() <= bounds.hi();
-           reg_win += stride) {
-         if (!is_dead_operand && intersects(reg_win, def_reg))
-            continue;
-
-         /* second, check that we have at most k=num_moves elements in the window
-          * and no element is larger than the currently processed one */
-         unsigned k = 0;
-         unsigned n = 0;
-         unsigned last_var = 0;
-         bool found = true;
-         for (PhysReg j : reg_win) {
-            if (reg_file[j] == 0 || reg_file[j] == last_var)
-               continue;
-
-            if (reg_file.is_blocked(j) || k > num_moves) {
-               found = false;
-               break;
-            }
-            if (reg_file[j] == 0xF0000000) {
-               k += 1;
-               n++;
-               continue;
-            }
-            /* we cannot split live ranges of linear vgprs */
-            if (ctx.assignments[reg_file[j]].rc.is_linear_vgpr()) {
-               found = false;
-               break;
-            }
-            bool is_kill = false;
-            for (const Operand& op : instr->operands) {
-               if (op.isTemp() && op.isKillBeforeDef() && op.tempId() == reg_file[j]) {
-                  is_kill = true;
-                  break;
-               }
-            }
-            if (!is_kill && ctx.assignments[reg_file[j]].rc.size() >= size) {
-               found = false;
-               break;
-            }
-
-            k += ctx.assignments[reg_file[j]].rc.size();
-            last_var = reg_file[j];
-            n++;
-            if (k > num_moves || (k == num_moves && n <= num_vars)) {
-               found = false;
-               break;
-            }
-         }
-
-         if (found) {
-            best_pos = reg_win.lo();
-            num_moves = k;
-            num_vars = n;
-         }
-      }
+                  ctx.num_linear_vgprs -= zeros;
+                  compact_relocate_vars(ctx, vars, parallelcopies, get_reg_bounds(ctx, RegType::vgpr, true).lo());
 
-      /* FIXME: we messed up and couldn't find space for the variables to be copied */
-      if (num_moves == 0xFF)
-         return false;
-
-      PhysRegInterval reg_win{best_pos, size};
-
-      /* collect variables and block reg file */
-      std::vector<unsigned> new_vars = collect_vars(ctx, reg_file, reg_win);
-
-      /* mark the area as blocked */
-      reg_file.block(reg_win.lo(), var.rc);
-      adjust_max_used_regs(ctx, var.rc, reg_win.lo());
-
-      if (!get_regs_for_copies(ctx, reg_file, parallelcopies, new_vars, instr, def_reg))
-         return false;
-
-      /* create parallelcopy pair (without definition id) */
-      Temp tmp = Temp(id, var.rc);
-      Operand pc_op = Operand(tmp);
-      pc_op.setFixed(var.reg);
-      Definition pc_def = Definition(reg_win.lo(), pc_op.regClass());
-      parallelcopies.emplace_back(pc_op, pc_def);
-   }
-
-   return true;
-}
-
-std::optional<PhysReg>
-get_reg_impl(ra_ctx& ctx, const RegisterFile& reg_file, std::vector<parallelcopy>& parallelcopies,
-             const DefInfo& info, aco_ptr<Instruction>& instr)
-{
-   const PhysRegInterval& bounds = info.bounds;
-   uint32_t size = info.size;
-   uint32_t stride = info.rc.is_subdword() ? DIV_ROUND_UP(info.stride, 4) : info.stride;
-   RegClass rc = info.rc;
-
-   /* check how many free regs we have */
-   unsigned regs_free = reg_file.count_zero(get_reg_bounds(ctx, rc));
-
-   /* mark and count killed operands */
-   unsigned killed_ops = 0;
-   std::bitset<256> is_killed_operand; /* per-register */
-   std::bitset<256> is_precolored;     /* per-register */
-   for (unsigned j = 0; !is_phi(instr) && j < instr->operands.size(); j++) {
-      Operand& op = instr->operands[j];
-      if (op.isTemp() && op.isPrecolored() && !op.isFirstKillBeforeDef() &&
-          bounds.contains(op.physReg())) {
-         for (unsigned i = 0; i < op.size(); ++i) {
-            is_precolored[(op.physReg() & 0xff) + i] = true;
-         }
-      }
-      if (op.isTemp() && op.isFirstKillBeforeDef() && bounds.contains(op.physReg()) &&
-          !reg_file.test(PhysReg{op.physReg().reg()}, align(op.bytes() + op.physReg().byte(), 4))) {
-         assert(op.isFixed());
-
-         for (unsigned i = 0; i < op.size(); ++i) {
-            is_killed_operand[(op.physReg() & 0xff) + i] = true;
-         }
+                  return true;
+            }
 
-         killed_ops += op.getTemp().size();
-      }
-   }
-   for (unsigned j = 0; !is_phi(instr) && j < instr->definitions.size(); j++) {
-      Definition& def = instr->definitions[j];
-      if (def.isTemp() && def.isPrecolored() && bounds.contains(def.physReg())) {
-         for (unsigned i = 0; i < def.size(); ++i) {
-            is_precolored[(def.physReg() & 0xff) + i] = true;
-         }
-      }
-   }
+            /* Allocates a linear VGPR. We allocate them at the end of the register file and keep them separate
+             * from normal VGPRs. This is for two reasons:
+             * - Because we only ever move linear VGPRs into an empty space or a space previously occupied by a
+             *   linear one, we never have to swap a normal VGPR and a linear one.
+             * - As linear VGPR's live ranges only start and end on top-level blocks, we never have to move a
+             *   linear VGPR in control flow.
+             */
+            PhysReg
+            alloc_linear_vgpr(ra_ctx& ctx, const RegisterFile& reg_file, aco_ptr<Instruction>& instr,
+                              std::vector<parallelcopy>& parallelcopies)
+            {
+                  assert(instr->opcode == aco_opcode::p_start_linear_vgpr);
+                  assert(instr->definitions.size() == 1 && instr->definitions[0].bytes() % 4 == 0);
+
+                  RegClass rc = instr->definitions[0].regClass();
+
+                  /* Try to choose an unused space in the linear VGPR bounds. */
+                  for (unsigned i = rc.size(); i <= ctx.num_linear_vgprs; i++) {
+                        PhysReg reg(256 + ctx.vgpr_bounds - i);
+                        if (!reg_file.test(reg, rc.bytes())) {
+                              adjust_max_used_regs(ctx, rc, reg);
+                              return reg;
+                        }
+                  }
+
+                  PhysRegInterval old_normal_bounds = get_reg_bounds(ctx, RegType::vgpr, false);
+
+                  /* Compact linear VGPRs, grow the bounds if necessary, and choose a space at the beginning: */
+                  compact_linear_vgprs(ctx, reg_file, parallelcopies);
+
+                  PhysReg reg(256 + ctx.vgpr_bounds - (ctx.num_linear_vgprs + rc.size()));
+                  /* Space that was for normal VGPRs, but is now for linear VGPRs. */
+                  PhysRegInterval new_win = PhysRegInterval::from_until(reg, MAX2(old_normal_bounds.hi(), reg));
+
+                  RegisterFile tmp_file(reg_file);
+                  PhysRegInterval reg_win{reg, rc.size()};
+                  std::vector<unsigned> blocking_vars = collect_vars(ctx, tmp_file, new_win);
+
+                  /* Re-enable killed operands */
+                  tmp_file.fill_killed_operands(instr.get());
+
+                  /* Find new assignments for blocking vars. */
+                  std::vector<parallelcopy> pc;
+                  if (!ctx.policy.skip_optimistic_path &&
+                        get_regs_for_copies(ctx, tmp_file, pc, blocking_vars, instr, reg_win)) {
+                        parallelcopies.insert(parallelcopies.end(), pc.begin(), pc.end());
+                        } else {
+                              /* Fallback algorithm: reallocate all variables at once. */
+                              std::vector<IDAndRegClass> vars;
+                              for (unsigned id : find_vars(ctx, reg_file, old_normal_bounds))
+                                    vars.emplace_back(id, ctx.assignments[id].rc);
+                              compact_relocate_vars(ctx, vars, parallelcopies, PhysReg(256));
+
+                              std::vector<IDAndRegClass> killed_op_vars;
+                              for (Operand& op : instr->operands) {
+                                    if (op.isTemp() && op.isFirstKillBeforeDef() && op.regClass().type() == RegType::vgpr)
+                                          killed_op_vars.emplace_back(op.tempId(), op.regClass());
+                              }
+                              compact_relocate_vars(ctx, killed_op_vars, parallelcopies, reg_win.lo());
+                        }
 
-   assert((regs_free + ctx.num_linear_vgprs) >= size);
+                        /* If this is updated earlier, a killed operand can't be placed inside the definition. */
+                        ctx.num_linear_vgprs += rc.size();
 
-   /* we might have to move dead operands to dst in order to make space */
-   unsigned op_moves = 0;
+                        adjust_max_used_regs(ctx, rc, reg);
+                        return reg;
+            }
 
-   if (size > (regs_free - killed_ops))
-      op_moves = size - (regs_free - killed_ops);
-
-   /* find the best position to place the definition */
-   PhysRegInterval best_win = {bounds.lo(), size};
-   unsigned num_moves = 0xFF;
-   unsigned num_vars = 0;
-
-   /* we use a sliding window to check potential positions */
-   for (PhysRegInterval reg_win = {bounds.lo(), size}; reg_win.hi() <= bounds.hi();
-        reg_win += stride) {
-      /* first check if the register window starts in the middle of an
-       * allocated variable: this is what we have to fix to allow for
-       * num_moves > size */
-      if (reg_win.lo() > bounds.lo() && !reg_file.is_empty_or_blocked(reg_win.lo()) &&
-          reg_file.get_id(reg_win.lo()) == reg_file.get_id(reg_win.lo().advance(-1)))
-         continue;
-      if (reg_win.hi() < bounds.hi() && !reg_file.is_empty_or_blocked(reg_win.hi().advance(-1)) &&
-          reg_file.get_id(reg_win.hi().advance(-1)) == reg_file.get_id(reg_win.hi()))
-         continue;
-
-      /* second, check that we have at most k=num_moves elements in the window
-       * and no element is larger than the currently processed one */
-      unsigned k = op_moves;
-      unsigned n = 0;
-      unsigned remaining_op_moves = op_moves;
-      unsigned last_var = 0;
-      bool found = true;
-      bool aligned = rc == RegClass::v4 && reg_win.lo() % 4 == 0;
-      for (const PhysReg j : reg_win) {
-         /* dead operands effectively reduce the number of estimated moves */
-         if (is_killed_operand[j & 0xFF]) {
-            if (remaining_op_moves) {
-               k--;
-               remaining_op_moves--;
-            }
-            continue;
-         }
-         if (is_precolored[j & 0xFF]) {
-            found = false;
-            break;
-         }
-
-         if (reg_file[j] == 0 || reg_file[j] == last_var)
-            continue;
-
-         if (reg_file[j] == 0xF0000000) {
-            k += 1;
-            n++;
-            continue;
-         }
-
-         if (ctx.assignments[reg_file[j]].rc.size() >= size) {
-            found = false;
-            break;
-         }
-
-         /* we cannot split live ranges of linear vgprs */
-         if (ctx.assignments[reg_file[j]].rc.is_linear_vgpr()) {
-            found = false;
-            break;
-         }
-
-         k += ctx.assignments[reg_file[j]].rc.size();
-         n++;
-         last_var = reg_file[j];
-      }
+            bool
+            should_compact_linear_vgprs(ra_ctx& ctx, const RegisterFile& reg_file)
+            {
+                  if (!(ctx.block->kind & block_kind_top_level) || ctx.block->linear_succs.empty())
+                        return false;
+
+                  /* Since we won't be able to copy linear VGPRs to make space when in control flow, we have to
+                   * ensure in advance that there is enough space for normal VGPRs. */
+                  unsigned max_vgpr_usage = 0;
+                  unsigned next_toplevel = ctx.block->index + 1;
+                  for (; !(ctx.program->blocks[next_toplevel].kind & block_kind_top_level); next_toplevel++) {
+                        max_vgpr_usage =
+                        MAX2(max_vgpr_usage, (unsigned)ctx.program->blocks[next_toplevel].register_demand.vgpr);
+                  }
+                  max_vgpr_usage =
+                  MAX2(max_vgpr_usage, (unsigned)ctx.program->blocks[next_toplevel].live_in_demand.vgpr);
 
-      if (!found || k > num_moves)
-         continue;
-      if (k == num_moves && n < num_vars)
-         continue;
-      if (!aligned && k == num_moves && n == num_vars)
-         continue;
-
-      if (found) {
-         best_win = reg_win;
-         num_moves = k;
-         num_vars = n;
-      }
-   }
+                  for (unsigned tmp : find_vars(ctx, reg_file, get_reg_bounds(ctx, RegType::vgpr, true)))
+                        max_vgpr_usage -= ctx.assignments[tmp].rc.size();
 
-   if (num_moves == 0xFF)
-      return {};
+                  return max_vgpr_usage > get_reg_bounds(ctx, RegType::vgpr, false).size;
+            }
 
-   /* now, we figured the placement for our definition */
-   RegisterFile tmp_file(reg_file);
+            PhysReg
+            get_reg(ra_ctx& ctx, const RegisterFile& reg_file, Temp temp,
+                    std::vector<parallelcopy>& parallelcopies, aco_ptr<Instruction>& instr,
+                    int operand_index = -1)
+            {
+                  /* Note that "temp" might already be filled in the register file if we're making a copy of the
+                   * temporary.
+                   */
+
+                  auto split_vec = ctx.split_vectors.find(temp.id());
+                  if (split_vec != ctx.split_vectors.end()) {
+                        unsigned offset = 0;
+                        for (Definition def : split_vec->second->definitions) {
+                              if (ctx.assignments[def.tempId()].affinity) {
+                                    assignment& affinity = ctx.assignments[ctx.assignments[def.tempId()].affinity];
+                                    if (affinity.assigned) {
+                                          PhysReg reg = affinity.reg;
+                                          reg.reg_b -= offset;
+                                          if (get_reg_specified(ctx, reg_file, temp.regClass(), instr, reg, operand_index))
+                                                return reg;
+                                    }
+                              }
+                              offset += def.bytes();
+                        }
+                  }
+
+                  if (ctx.assignments[temp.id()].affinity) {
+                        assignment& affinity = ctx.assignments[ctx.assignments[temp.id()].affinity];
+                        if (affinity.assigned) {
+                              if (get_reg_specified(ctx, reg_file, temp.regClass(), instr, affinity.reg, operand_index))
+                                    return affinity.reg;
+                        }
+                  }
+                  if (ctx.assignments[temp.id()].vcc) {
+                        if (get_reg_specified(ctx, reg_file, temp.regClass(), instr, vcc, operand_index))
+                              return vcc;
+                  }
+                  if (ctx.assignments[temp.id()].m0) {
+                        if (get_reg_specified(ctx, reg_file, temp.regClass(), instr, m0, operand_index))
+                              return m0;
+                  }
+
+                  std::optional<PhysReg> res;
+
+                  if (ctx.vectors.find(temp.id()) != ctx.vectors.end()) {
+                        res = get_reg_vector(ctx, reg_file, temp, instr, operand_index);
+                        if (res)
+                              return *res;
+                  }
+
+                  if (temp.size() == 1 && operand_index == -1) {
+                        for (const Operand& op : instr->operands) {
+                              if (op.isTemp() && op.isFirstKillBeforeDef() && op.regClass() == temp.regClass()) {
+                                    assert(op.isFixed());
+                                    if (op.physReg() == vcc || op.physReg() == vcc_hi)
+                                          continue;
+                                    if (get_reg_specified(ctx, reg_file, temp.regClass(), instr, op.physReg(),
+                                          operand_index))
+                                          return op.physReg();
+                              }
+                        }
+                  }
+
+                  DefInfo info(ctx, instr, temp.regClass(), operand_index);
+
+                  if (!ctx.policy.skip_optimistic_path && !ctx.policy.use_compact_relocate) {
+                        /* try to find space without live-range splits */
+                        res = get_reg_simple(ctx, reg_file, info);
+
+                        if (res)
+                              return *res;
+                  }
+
+                  if (!ctx.policy.use_compact_relocate) {
+                        /* try to find space with live-range splits */
+                        res = get_reg_impl(ctx, reg_file, parallelcopies, info, instr);
+
+                        if (res)
+                              return *res;
+                  }
+
+                  /* try compacting the linear vgprs to make more space */
+                  std::vector<parallelcopy> pc;
+                  if (info.rc.type() == RegType::vgpr && (ctx.block->kind & block_kind_top_level) &&
+                        compact_linear_vgprs(ctx, reg_file, pc)) {
+                        parallelcopies.insert(parallelcopies.end(), pc.begin(), pc.end());
+
+                  /* We don't need to fill the copy definitions in because we don't care about the linear VGPR
+                   * space here. */
+                  RegisterFile tmp_file(reg_file);
+                  for (parallelcopy& copy : pc)
+                        tmp_file.clear(copy.op);
+
+                        return get_reg(ctx, tmp_file, temp, parallelcopies, instr, operand_index);
+                        }
+
+                        /* We should only fail here because keeping under the limit would require
+                         * too many moves. */
+                        assert(reg_file.count_zero(get_reg_bounds(ctx, info.rc)) >= info.size);
+
+                  /* try using more registers */
+                  if (!increase_register_file(ctx, info.rc)) {
+                        /* fallback algorithm: reallocate all variables at once (linear VGPRs should already be
+                         * compact at the end) */
+                        const PhysRegInterval regs = get_reg_bounds(ctx, info.rc);
+
+                        unsigned def_size = info.rc.size();
+                        std::vector<IDAndRegClass> def_vars;
+                        for (Definition def : instr->definitions) {
+                              if (def.isPrecolored()) {
+                                    assert(!regs.contains({def.physReg(), def.size()}));
+                                    continue;
+                              }
+                              if (ctx.assignments[def.tempId()].assigned && def.regClass().type() == info.rc.type()) {
+                                    def_size += def.regClass().size();
+                                    def_vars.emplace_back(def.tempId(), def.regClass());
+                              }
+                        }
+
+                        unsigned killed_op_size = 0;
+                        std::vector<IDAndRegClass> killed_op_vars;
+                        for (Operand op : instr->operands) {
+                              if (op.isPrecolored()) {
+                                    assert(!regs.contains({op.physReg(), op.size()}));
+                                    continue;
+                              }
+                              if (op.isTemp() && op.isFirstKillBeforeDef() && op.regClass().type() == info.rc.type()) {
+                                    killed_op_size += op.regClass().size();
+                                    killed_op_vars.emplace_back(op.tempId(), op.regClass());
+                              }
+                        }
+
+                        /* reallocate passthrough variables and non-killed operands */
+                        std::vector<IDAndRegClass> vars;
+                        for (unsigned id : find_vars(ctx, reg_file, regs))
+                              vars.emplace_back(id, ctx.assignments[id].rc);
+                        vars.emplace_back(0xffffffff, RegClass(info.rc.type(), MAX2(def_size, killed_op_size)));
+
+                        PhysReg space = compact_relocate_vars(ctx, vars, parallelcopies, regs.lo());
+
+                        /* reallocate killed operands */
+                        compact_relocate_vars(ctx, killed_op_vars, parallelcopies, space);
+
+                        /* reallocate definitions */
+                        def_vars.emplace_back(0xffffffff, info.rc);
+                        return compact_relocate_vars(ctx, def_vars, parallelcopies, space);
+                  }
 
-   /* p_create_vector: also re-place killed operands in the definition space */
-   if (instr->opcode == aco_opcode::p_create_vector)
-      tmp_file.fill_killed_operands(instr.get());
-
-   std::vector<unsigned> vars = collect_vars(ctx, tmp_file, best_win);
-
-   /* re-enable killed operands */
-   if (!is_phi(instr) && instr->opcode != aco_opcode::p_create_vector)
-      tmp_file.fill_killed_operands(instr.get());
-
-   std::vector<parallelcopy> pc;
-   if (!get_regs_for_copies(ctx, tmp_file, pc, vars, instr, best_win))
-      return {};
-
-   parallelcopies.insert(parallelcopies.end(), pc.begin(), pc.end());
-
-   adjust_max_used_regs(ctx, rc, best_win.lo());
-   return best_win.lo();
-}
-
-bool
-get_reg_specified(ra_ctx& ctx, const RegisterFile& reg_file, RegClass rc,
-                  aco_ptr<Instruction>& instr, PhysReg reg, int operand)
-{
-   /* catch out-of-range registers */
-   if (reg >= PhysReg{512})
-      return false;
-
-   DefInfo info(ctx, instr, rc, operand);
-
-   if (reg.reg_b % info.data_stride)
-      return false;
-
-   assert(util_is_power_of_two_nonzero(info.stride));
-   reg.reg_b &= ~(info.stride - 1);
-
-   PhysRegInterval reg_win = {PhysReg(reg.reg()), info.rc.size()};
-   PhysRegInterval vcc_win = {vcc, 2};
-   /* VCC is outside the bounds */
-   bool is_vcc =
-      info.rc.type() == RegType::sgpr && vcc_win.contains(reg_win) && ctx.program->needs_vcc;
-   bool is_m0 = info.rc == s1 && reg == m0 && can_write_m0(instr);
-   if (!info.bounds.contains(reg_win) && !is_vcc && !is_m0)
-      return false;
-
-   if (instr_info.classes[(int)instr->opcode] == instr_class::valu_pseudo_scalar_trans) {
-      /* RDNA4 ISA doc, 7.10. Pseudo-scalar Transcendental ALU ops:
-       * - VCC may not be used as a destination
-       */
-      if (vcc_win.contains(reg_win))
-         return false;
-   }
-
-   if (reg_file.test(reg, info.rc.bytes()))
-      return false;
-
-   adjust_max_used_regs(ctx, info.rc, reg_win.lo());
-   return true;
-}
-
-bool
-increase_register_file(ra_ctx& ctx, RegClass rc)
-{
-   if (rc.type() == RegType::vgpr && ctx.num_linear_vgprs == 0 &&
-       ctx.vgpr_bounds < ctx.limit.vgpr) {
-      /* If vgpr_bounds is less than max_reg_demand.vgpr, this should be a no-op. */
-      update_vgpr_sgpr_demand(
-         ctx.program, RegisterDemand(ctx.vgpr_bounds + 1, ctx.program->max_reg_demand.sgpr));
-
-      ctx.vgpr_bounds = ctx.program->max_reg_demand.vgpr;
-   } else if (rc.type() == RegType::sgpr && ctx.program->max_reg_demand.sgpr < ctx.limit.sgpr) {
-      update_vgpr_sgpr_demand(
-         ctx.program, RegisterDemand(ctx.program->max_reg_demand.vgpr, ctx.sgpr_bounds + 1));
-
-      ctx.sgpr_bounds = ctx.program->max_reg_demand.sgpr;
-   } else {
-      return false;
-   }
-
-   return true;
-}
-
-struct IDAndRegClass {
-   IDAndRegClass(unsigned id_, RegClass rc_) : id(id_), rc(rc_) {}
-
-   unsigned id;
-   RegClass rc;
-};
-
-struct IDAndInfo {
-   IDAndInfo(unsigned id_, DefInfo info_) : id(id_), info(info_) {}
-
-   unsigned id;
-   DefInfo info;
-};
-
-void
-add_rename(ra_ctx& ctx, Temp orig_val, Temp new_val)
-{
-   ctx.renames[ctx.block->index][orig_val.id()] = new_val;
-   ctx.orig_names.emplace(new_val.id(), orig_val);
-   ctx.assignments[orig_val.id()].renamed = true;
-}
-
-/* Reallocates vars by sorting them and placing each variable after the previous
- * one. If one of the variables has 0xffffffff as an ID, the register assigned
- * for that variable will be returned.
- */
-PhysReg
-compact_relocate_vars(ra_ctx& ctx, const std::vector<IDAndRegClass>& vars,
-                      std::vector<parallelcopy>& parallelcopies, PhysReg start)
-{
-   /* This function assumes RegisterDemand/live_var_analysis rounds up sub-dword
-    * temporary sizes to dwords.
-    */
-   std::vector<IDAndInfo> sorted;
-   for (IDAndRegClass var : vars) {
-      DefInfo info(ctx, ctx.pseudo_dummy, var.rc, -1);
-      sorted.emplace_back(var.id, info);
-   }
-
-   std::sort(
-      sorted.begin(), sorted.end(),
-      [=, &ctx](const IDAndInfo& a, const IDAndInfo& b)
-      {
-         unsigned a_stride = MAX2(a.info.stride * (a.info.rc.is_subdword() ? 1 : 4), 4);
-         unsigned b_stride = MAX2(b.info.stride * (b.info.rc.is_subdword() ? 1 : 4), 4);
-         /* Since the SGPR bounds should always be a multiple of two, we can place
-          * variables in this order:
-          * - the usual 4 SGPR aligned variables
-          * - then the 0xffffffff variable
-          * - then the unaligned variables
-          * - and finally the 2 SGPR aligned variables
-          * This way, we should always be able to place variables if the 0xffffffff one
-          * had a NPOT size.
-          *
-          * This also lets us avoid placing the 0xffffffff variable in VCC if it's s1/s2
-          * (required for pseudo-scalar transcendental) and places it first if it's a
-          * VGPR variable (required for ImageGather4D16Bug).
-          */
-         assert(a.info.rc.type() != RegType::sgpr || get_reg_bounds(ctx, a.info.rc).size % 2 == 0);
-         assert(a_stride == 16 || a_stride == 8 || a_stride == 4);
-         assert(b_stride == 16 || b_stride == 8 || b_stride == 4);
-         assert(a.info.rc.size() % (a_stride / 4u) == 0);
-         assert(b.info.rc.size() % (b_stride / 4u) == 0);
-         if ((a_stride == 16) != (b_stride == 16))
-            return a_stride > b_stride;
-         if (a.id == 0xffffffff || b.id == 0xffffffff)
-            return a.id == 0xffffffff;
-         if (a_stride != b_stride)
-            return a_stride < b_stride;
-         return ctx.assignments[a.id].reg < ctx.assignments[b.id].reg;
-      });
-
-   PhysReg next_reg = start;
-   PhysReg space_reg;
-   for (IDAndInfo& var : sorted) {
-      unsigned stride = var.info.rc.is_subdword() ? var.info.stride : var.info.stride * 4;
-      next_reg.reg_b = align(next_reg.reg_b, MAX2(stride, 4));
-
-      /* 0xffffffff is a special variable ID used reserve a space for killed
-       * operands and definitions.
-       */
-      if (var.id != 0xffffffff) {
-         if (next_reg != ctx.assignments[var.id].reg) {
-            RegClass rc = ctx.assignments[var.id].rc;
-            Temp tmp(var.id, rc);
-
-            Operand pc_op(tmp);
-            pc_op.setFixed(ctx.assignments[var.id].reg);
-            Definition pc_def(next_reg, rc);
-            parallelcopies.emplace_back(pc_op, pc_def);
-         }
-      } else {
-         space_reg = next_reg;
-      }
+                  return get_reg(ctx, reg_file, temp, parallelcopies, instr, operand_index);
+            }
 
-      adjust_max_used_regs(ctx, var.info.rc, next_reg);
+            PhysReg
+            get_reg_create_vector(ra_ctx& ctx, const RegisterFile& reg_file, Temp temp,
+                                  std::vector<parallelcopy>& parallelcopies, aco_ptr<Instruction>& instr)
+            {
+                  RegClass rc = temp.regClass();
+                  /* create_vector instructions have different costs w.r.t. register coalescing */
+                  uint32_t size = rc.size();
+                  uint32_t bytes = rc.bytes();
+                  uint32_t stride = get_stride(rc);
+                  PhysRegInterval bounds = get_reg_bounds(ctx, rc);
+
+                  // TODO: improve p_create_vector for sub-dword vectors
+
+                  PhysReg best_pos{0xFFF};
+                  unsigned num_moves = 0xFF;
+                  bool best_avoid = true;
+                  uint32_t correct_pos_mask = 0;
+
+                  /* test for each operand which definition placement causes the least shuffle instructions */
+                  for (unsigned i = 0, offset = 0; i < instr->operands.size();
+                       offset += instr->operands[i].bytes(), i++) {
+                        // TODO: think about, if we can alias live operands on the same register
+                        if (!instr->operands[i].isTemp() || instr->operands[i].getTemp().type() != rc.type() ||
+                              reg_file.test(instr->operands[i].physReg(), instr->operands[i].bytes()))
+                              continue;
+
+                        if (offset > instr->operands[i].physReg().reg_b)
+                              continue;
+
+                        unsigned reg_lower = instr->operands[i].physReg().reg_b - offset;
+                        if (reg_lower % 4)
+                              continue;
+                        PhysRegInterval reg_win = {PhysReg{reg_lower / 4}, size};
+                        unsigned k = 0;
+
+                        /* no need to check multiple times */
+                        if (reg_win.lo() == best_pos)
+                              continue;
+
+                        /* check borders */
+                        // TODO: this can be improved */
+                        if (!bounds.contains(reg_win) || reg_win.lo() % stride != 0)
+                              continue;
+                        if (reg_win.lo() > bounds.lo() && reg_file[reg_win.lo()] != 0 &&
+                              reg_file.get_id(reg_win.lo()) == reg_file.get_id(reg_win.lo().advance(-1)))
+                              continue;
+                        if (reg_win.hi() < bounds.hi() && reg_file[reg_win.hi().advance(-4)] != 0 &&
+                              reg_file.get_id(reg_win.hi().advance(-1)) == reg_file.get_id(reg_win.hi()))
+                              continue;
+
+                        /* count variables to be moved and check "avoid" */
+                        bool avoid = false;
+                        bool linear_vgpr = false;
+                        for (PhysReg j : reg_win) {
+                              if (reg_file[j] != 0) {
+                                    if (reg_file[j] == 0xF0000000) {
+                                          PhysReg reg;
+                                          reg.reg_b = j * 4;
+                                          unsigned bytes_left = bytes - ((unsigned)j - reg_win.lo()) * 4;
+                                          for (unsigned byte_idx = 0; byte_idx < MIN2(bytes_left, 4); byte_idx++, reg.reg_b++)
+                                                k += reg_file.test(reg, 1);
+                                    } else {
+                                          k += 4;
+                                          linear_vgpr |= ctx.assignments[reg_file[j]].rc.is_linear_vgpr();
+                                    }
+                              }
+                              avoid |= ctx.war_hint[j];
+                        }
+
+                        /* we cannot split live ranges of linear vgprs */
+                        if (linear_vgpr)
+                              continue;
+
+                        if (avoid && !best_avoid)
+                              continue;
+
+                        /* count operands in wrong positions */
+                        uint32_t correct_pos_mask_new = 0;
+                        for (unsigned j = 0, offset2 = 0; j < instr->operands.size();
+                             offset2 += instr->operands[j].bytes(), j++) {
+                              Operand& op = instr->operands[j];
+                              if (op.isTemp() && op.physReg().reg_b == reg_win.lo() * 4 + offset2)
+                                    correct_pos_mask_new |= 1 << j;
+                              else
+                                    k += op.bytes();
+                             }
+                             bool aligned = rc == RegClass::v4 && reg_win.lo() % 4 == 0;
+                        if (k > num_moves || (!aligned && k == num_moves))
+                              continue;
+
+                        best_pos = reg_win.lo();
+                        num_moves = k;
+                        best_avoid = avoid;
+                        correct_pos_mask = correct_pos_mask_new;
+                       }
+
+                       /* too many moves: try the generic get_reg() function */
+                       if (num_moves >= 2 * bytes) {
+                             return get_reg(ctx, reg_file, temp, parallelcopies, instr);
+                       } else if (num_moves > bytes) {
+                             DefInfo info(ctx, instr, rc, -1);
+                             std::optional<PhysReg> res = get_reg_simple(ctx, reg_file, info);
+                             if (res)
+                                   return *res;
+                       }
+
+                       /* re-enable killed operands which are in the wrong position */
+                       RegisterFile tmp_file(reg_file);
+                       tmp_file.fill_killed_operands(instr.get());
+
+                       for (unsigned i = 0; i < instr->operands.size(); i++) {
+                             if ((correct_pos_mask >> i) & 1u && instr->operands[i].isKill())
+                                   tmp_file.clear(instr->operands[i]);
+                       }
+
+                       /* collect variables to be moved */
+                       std::vector<unsigned> vars = collect_vars(ctx, tmp_file, PhysRegInterval{best_pos, size});
+
+                       bool success = false;
+                       std::vector<parallelcopy> pc;
+                       success = get_regs_for_copies(ctx, tmp_file, pc, vars, instr, PhysRegInterval{best_pos, size});
+
+                       if (!success) {
+                             if (!increase_register_file(ctx, temp.regClass())) {
+                                   /* use the fallback algorithm in get_reg() */
+                                   return get_reg(ctx, reg_file, temp, parallelcopies, instr);
+                             }
+                             return get_reg_create_vector(ctx, reg_file, temp, parallelcopies, instr);
+                       }
 
-      next_reg = next_reg.advance(var.info.rc.size() * 4);
-   }
+                       parallelcopies.insert(parallelcopies.end(), pc.begin(), pc.end());
+                       adjust_max_used_regs(ctx, rc, best_pos);
 
-   return space_reg;
-}
+                       return best_pos;
+            }
 
-bool
-is_vector_intact(ra_ctx& ctx, const RegisterFile& reg_file, const vector_info& vec_info)
-{
-   unsigned size = 0;
-   for (unsigned i = 0; i < vec_info.num_parts; i++)
-      size += vec_info.parts[i].bytes();
-
-   PhysReg first{512};
-   int offset = 0;
-   for (unsigned i = 0; i < vec_info.num_parts; i++) {
-      Operand op = vec_info.parts[i];
-
-      if (ctx.assignments[op.tempId()].assigned) {
-         PhysReg reg = ctx.assignments[op.tempId()].reg;
-
-         if (first.reg() == 512) {
-            PhysRegInterval bounds = get_reg_bounds(ctx, RegType::vgpr, false);
-            first = reg.advance(-offset);
-            PhysRegInterval vec = PhysRegInterval{first, DIV_ROUND_UP(size, 4)};
-            if (!bounds.contains(vec)) /* not enough space for other operands */
-               return false;
-         } else {
-            if (reg != first.advance(offset)) /* not at the best position */
-               return false;
-         }
-      } else {
-         /* If there's an unexpected temporary, this operand is unlikely to be
-          * placed in the best position.
-          */
-         if (first.reg() != 512 && reg_file.test(first.advance(offset), op.bytes()))
-            return false;
-      }
+            void
+            handle_pseudo(ra_ctx& ctx, const RegisterFile& reg_file, Instruction* instr)
+            {
+                  if (instr->format != Format::PSEUDO)
+                        return;
+
+                  /* all instructions which use handle_operands() need this information */
+                  switch (instr->opcode) {
+                        case aco_opcode::p_extract_vector:
+                        case aco_opcode::p_create_vector:
+                        case aco_opcode::p_split_vector:
+                        case aco_opcode::p_parallelcopy:
+                        case aco_opcode::p_start_linear_vgpr: break;
+                        default: return;
+                  }
+
+                  bool writes_linear = false;
+                  /* if all definitions are logical vgpr, no need to care for SCC */
+                  for (Definition& def : instr->definitions) {
+                        if (def.getTemp().regClass().is_linear())
+                              writes_linear = true;
+                  }
+                  /* if all operands are constant, no need to care either */
+                  bool reads_linear = false;
+                  for (Operand& op : instr->operands) {
+                        if (op.isTemp() && op.getTemp().regClass().is_linear())
+                              reads_linear = true;
+                  }
+
+                  if (!writes_linear || !reads_linear)
+                        return;
+
+                  instr->pseudo().needs_scratch_reg = true;
+
+                  if (!reg_file[scc]) {
+                        instr->pseudo().scratch_sgpr = scc;
+                        return;
+                  }
+
+                  int reg = ctx.max_used_sgpr;
+                  for (; reg >= 0 && reg_file[PhysReg{(unsigned)reg}]; reg--)
+                        ;
+                        if (reg < 0) {
+                              reg = ctx.max_used_sgpr + 1;
+                              for (; reg < ctx.program->max_reg_demand.sgpr && reg_file[PhysReg{(unsigned)reg}]; reg++)
+                                    ;
+                        }
 
-      offset += op.bytes();
-   }
+                        adjust_max_used_regs(ctx, s1, reg);
+                        instr->pseudo().scratch_sgpr = PhysReg{(unsigned)reg};
+            }
 
-   return true;
-}
+            bool
+            operand_can_use_reg(amd_gfx_level gfx_level, aco_ptr<Instruction>& instr, unsigned idx, PhysReg reg,
+                                RegClass rc)
+            {
+                  if (reg.byte()) {
+                        unsigned stride = get_subdword_operand_stride(gfx_level, instr, idx, rc);
+                        if (reg.byte() % stride)
+                              return false;
+                  }
+
+                  switch (instr->format) {
+                        case Format::SMEM:
+                              return reg != scc && reg != exec &&
+                              (reg != m0 || idx == 1 || idx == 3) && /* offset can be m0 */
+                              (reg != vcc || (instr->definitions.empty() && idx == 2) ||
+                              gfx_level >= GFX10); /* sdata can be vcc */
+                        case Format::MUBUF:
+                        case Format::MTBUF: return idx != 2 || gfx_level < GFX12 || reg != scc;
+                        case Format::SOPK:
+                              if (idx == 0 && reg == scc)
+                                    return false;
+                        FALLTHROUGH;
+                        case Format::SOP2:
+                        case Format::SOP1: {
+                              auto tied_defs = get_tied_defs(instr.get());
+                              return std::all_of(tied_defs.begin(), tied_defs.end(),
+                                                 [idx](auto op_idx) { return op_idx != idx; }) ||
+                                                 is_sgpr_writable_without_side_effects(gfx_level, reg);
+                        }
+                        default:
+                              // TODO: there are more instructions with restrictions on registers
+                              return true;
+                  }
+            }
 
-std::optional<PhysReg>
-get_reg_vector(ra_ctx& ctx, const RegisterFile& reg_file, Temp temp, aco_ptr<Instruction>& instr,
-               int operand)
-{
-   const vector_info& vec = ctx.vectors[temp.id()];
-   if (!vec.is_weak || is_vector_intact(ctx, reg_file, vec)) {
-      unsigned our_offset = 0;
-      for (unsigned i = 0; i < vec.num_parts; i++) {
-         const Operand& op = vec.parts[i];
-         if (op.isTemp() && op.tempId() == temp.id())
-            break;
-         else
-            our_offset += op.bytes();
-      }
+            void
+            handle_fixed_operands(ra_ctx& ctx, RegisterFile& register_file,
+                                  std::vector<parallelcopy>& parallelcopy, aco_ptr<Instruction>& instr)
+            {
+                  assert(instr->operands.size() <= 128);
+                  assert(parallelcopy.empty());
+
+                  RegisterFile tmp_file(register_file);
+
+                  BITSET_DECLARE(live_reg_assigned, 128) = {0};
+                  BITSET_DECLARE(mask, 128) = {0};
+
+                  for (unsigned i = 0; i < instr->operands.size(); i++) {
+                        Operand& op = instr->operands[i];
+
+                        if (!op.isPrecolored())
+                              continue;
+
+                        assert(op.isTemp());
+                        PhysReg src = ctx.assignments[op.tempId()].reg;
+                        if (!BITSET_TEST(live_reg_assigned, i) && !op.isKill()) {
+                              /* Figure out which register the temp will continue living in after the instruction. */
+                              PhysReg live_reg = op.physReg();
+                              bool found = false;
+                              for (unsigned j = i; j < instr->operands.size(); ++j) {
+                                    const Operand& op2 = instr->operands[j];
+                                    if (!op2.isPrecolored() || op2.tempId() != op.tempId())
+                                          continue;
+
+                                    /* Don't consider registers interfering with definitions - we'd have to immediately copy
+                                     * the temporary somewhere else to make space for the interfering definition.
+                                     */
+                                    if (op2.isClobbered())
+                                          continue;
+
+                                    /* Choose the first register that doesn't interfere with definitions. If the temp can
+                                     * continue residing in the same register it's currently in, just choose that.
+                                     */
+                                    if (!found || op2.physReg() == src) {
+                                          live_reg = op2.physReg();
+                                          found = true;
+                                          if (op2.physReg() == src)
+                                                break;
+                                    }
+                              }
+
+                              for (unsigned j = i; j < instr->operands.size(); ++j) {
+                                    if (!instr->operands[j].isPrecolored() || instr->operands[j].tempId() != op.tempId())
+                                          continue;
+
+                                    /* Fix up operand kill flags according to the live_reg choice we made. */
+                                    if (instr->operands[j].physReg() == live_reg) {
+                                          instr->operands[j].setCopyKill(false);
+                                          instr->operands[j].setKill(false);
+                                    } else {
+                                          instr->operands[j].setCopyKill(true);
+                                    }
+                                    BITSET_SET(live_reg_assigned, j);
+                              }
+                        }
+
+                        adjust_max_used_regs(ctx, op.regClass(), op.physReg());
+
+                        if (op.physReg() == src) {
+                              tmp_file.block(op.physReg(), op.regClass());
+                              continue;
+                        }
+
+                        /* An instruction can have at most one operand precolored to the same register. */
+                        assert(std::none_of(parallelcopy.begin(), parallelcopy.end(),
+                                            [&](auto copy) { return copy.def.physReg() == op.physReg(); }));
+
+                        /* clear from register_file so fixed operands are not collected by collect_vars() */
+                        tmp_file.clear(src, op.regClass());
+
+                        BITSET_SET(mask, i);
+
+                        Operand pc_op(instr->operands[i].getTemp());
+                        pc_op.setFixed(src);
+                        Definition pc_def = Definition(op.physReg(), pc_op.regClass());
+                        parallelcopy.emplace_back(pc_op, pc_def, op.isCopyKill() ? i : -1);
+                  }
+
+                  if (BITSET_IS_EMPTY(mask))
+                        return;
+
+                  unsigned i;
+                  std::vector<unsigned> blocking_vars;
+                  BITSET_FOREACH_SET (i, mask, instr->operands.size()) {
+                        Operand& op = instr->operands[i];
+                        PhysRegInterval target{op.physReg(), op.size()};
+                        std::vector<unsigned> blocking_vars2 = collect_vars(ctx, tmp_file, target);
+                        blocking_vars.insert(blocking_vars.end(), blocking_vars2.begin(), blocking_vars2.end());
+
+                        /* prevent get_regs_for_copies() from using these registers */
+                        tmp_file.block(op.physReg(), op.regClass());
+                  }
 
-      unsigned their_offset = 0;
-      /* check for every operand of the vector
-       * - whether the operand is assigned and
-       * - we can use the register relative to that operand
-       */
-      for (unsigned i = 0; i < vec.num_parts; i++) {
-         const Operand& op = vec.parts[i];
-         if (op.isTemp() && op.tempId() != temp.id() && op.getTemp().type() == temp.type() &&
-             ctx.assignments[op.tempId()].assigned) {
-            PhysReg reg = ctx.assignments[op.tempId()].reg;
-            reg.reg_b += (our_offset - their_offset);
-            if (get_reg_specified(ctx, reg_file, temp.regClass(), instr, reg, operand))
-               return reg;
-
-            /* return if MIMG vaddr components don't remain vector-aligned */
-            if (vec.is_weak)
-               return {};
-         }
-         their_offset += op.bytes();
-      }
+                  get_regs_for_copies(ctx, tmp_file, parallelcopy, blocking_vars, instr, PhysRegInterval());
+                  update_renames(ctx, register_file, parallelcopy, instr);
+            }
 
-      /* We didn't find a register relative to other vector operands.
-       * Try to find new space which fits the whole vector.
-       */
-      RegClass vec_rc = RegClass::get(temp.type(), their_offset);
-      DefInfo info(ctx, ctx.pseudo_dummy, vec_rc, -1);
-      std::optional<PhysReg> reg = get_reg_simple(ctx, reg_file, info);
-      if (reg) {
-         reg->reg_b += our_offset;
-         /* make sure to only use byte offset if the instruction supports it */
-         if (get_reg_specified(ctx, reg_file, temp.regClass(), instr, *reg, operand))
-            return reg;
-      }
-   }
-   return {};
-}
-
-bool
-compact_linear_vgprs(ra_ctx& ctx, const RegisterFile& reg_file,
-                     std::vector<parallelcopy>& parallelcopies)
-{
-   PhysRegInterval linear_vgpr_bounds = get_reg_bounds(ctx, RegType::vgpr, true);
-   int zeros = reg_file.count_zero(linear_vgpr_bounds);
-   if (zeros == 0)
-      return false;
-
-   std::vector<IDAndRegClass> vars;
-   for (unsigned id : find_vars(ctx, reg_file, linear_vgpr_bounds))
-      vars.emplace_back(id, ctx.assignments[id].rc);
-
-   ctx.num_linear_vgprs -= zeros;
-   compact_relocate_vars(ctx, vars, parallelcopies, get_reg_bounds(ctx, RegType::vgpr, true).lo());
-
-   return true;
-}
-
-/* Allocates a linear VGPR. We allocate them at the end of the register file and keep them separate
- * from normal VGPRs. This is for two reasons:
- * - Because we only ever move linear VGPRs into an empty space or a space previously occupied by a
- *   linear one, we never have to swap a normal VGPR and a linear one.
- * - As linear VGPR's live ranges only start and end on top-level blocks, we never have to move a
- *   linear VGPR in control flow.
- */
-PhysReg
-alloc_linear_vgpr(ra_ctx& ctx, const RegisterFile& reg_file, aco_ptr<Instruction>& instr,
-                  std::vector<parallelcopy>& parallelcopies)
-{
-   assert(instr->opcode == aco_opcode::p_start_linear_vgpr);
-   assert(instr->definitions.size() == 1 && instr->definitions[0].bytes() % 4 == 0);
-
-   RegClass rc = instr->definitions[0].regClass();
-
-   /* Try to choose an unused space in the linear VGPR bounds. */
-   for (unsigned i = rc.size(); i <= ctx.num_linear_vgprs; i++) {
-      PhysReg reg(256 + ctx.vgpr_bounds - i);
-      if (!reg_file.test(reg, rc.bytes())) {
-         adjust_max_used_regs(ctx, rc, reg);
-         return reg;
-      }
-   }
+            void
+            get_reg_for_operand(ra_ctx& ctx, RegisterFile& register_file,
+                                std::vector<parallelcopy>& parallelcopy, aco_ptr<Instruction>& instr,
+                                Operand& operand, unsigned operand_index)
+            {
+                  /* clear the operand in case it's only a stride mismatch */
+                  PhysReg src = ctx.assignments[operand.tempId()].reg;
+                  register_file.clear(src, operand.regClass());
+                  PhysReg dst = get_reg(ctx, register_file, operand.getTemp(), parallelcopy, instr, operand_index);
+
+                  Operand pc_op = operand;
+                  pc_op.setFixed(src);
+                  Definition pc_def = Definition(dst, pc_op.regClass());
+                  parallelcopy.emplace_back(pc_op, pc_def);
+                  update_renames(ctx, register_file, parallelcopy, instr);
+                  register_file.fill(Definition(operand.getTemp(), dst));
+                  operand.setFixed(dst);
+            }
 
-   PhysRegInterval old_normal_bounds = get_reg_bounds(ctx, RegType::vgpr, false);
+            void
+            handle_vector_operands(ra_ctx& ctx, RegisterFile& register_file,
+                                   std::vector<parallelcopy>& parallelcopies, aco_ptr<Instruction>& instr,
+                                   unsigned operand_index)
+            {
+                  assert(operand_index == 0 || !instr->operands[operand_index - 1].isVectorAligned());
+                  /* Count the operands that form part of the vector. */
+                  uint32_t num_operands = 1;
+                  uint32_t num_bytes = instr->operands[operand_index].bytes();
+                  for (unsigned i = operand_index + 1; instr->operands[i - 1].isVectorAligned(); i++) {
+                        /* If it's not late kill, we might end up trying to kill/re-enable the operands
+                         * before resolve_vector_operands(), which wouldn't work.
+                         */
+                        assert(instr->operands[i].isLateKill());
+                        num_operands++;
+                        num_bytes += instr->operands[i].bytes();
+                  }
+                  RegClass rc = instr->operands[operand_index].regClass().resize(num_bytes);
+
+                  /* Create temporary p_create_vector instruction and make use of get_reg_create_vector. */
+                  aco_ptr<Instruction> vec{
+                        create_instruction(aco_opcode::p_create_vector, Format::PSEUDO, num_operands, 1)};
+                        for (unsigned i = 0; i < num_operands; i++) {
+                              vec->operands[i] = instr->operands[operand_index + i];
+                              vec->operands[i].setFixed(ctx.assignments[vec->operands[i].tempId()].reg);
+                        }
+                        Temp vec_temp = ctx.program->allocateTmp(rc);
+                        ctx.assignments.emplace_back();
+                        vec->definitions[0] = Definition(vec_temp);
+
+                        PhysReg reg = get_reg_create_vector(ctx, register_file, vec_temp, parallelcopies, vec);
+                        vec->definitions[0].setFixed(reg);
+                        ctx.assignments[vec_temp.id()].set(vec->definitions[0]);
+
+                        /* For now, fix instruction operands to previous registers. They are essentially ignored until
+                         * we call resolve_vector_operands(). For the remaining handling of this instruction, we'll
+                         * use the p_create_vector definition as temporary.
+                         */
+                        for (unsigned i = operand_index; i < operand_index + num_operands; i++)
+                              instr->operands[i].setFixed(ctx.assignments[instr->operands[i].tempId()].reg);
+
+                  update_renames(ctx, register_file, parallelcopies, instr);
+                  ctx.vector_operands.emplace_back(
+                        vector_operand{vec->definitions[0], operand_index, num_operands});
+                  register_file.fill(vec->definitions[0]);
+            }
 
-   /* Compact linear VGPRs, grow the bounds if necessary, and choose a space at the beginning: */
-   compact_linear_vgprs(ctx, reg_file, parallelcopies);
+            void
+            resolve_vector_operands(ra_ctx& ctx, RegisterFile& reg_file,
+                                    std::vector<parallelcopy>& parallelcopies, aco_ptr<Instruction>& instr)
+            {
+                  /* Vector Operands are temporarily stored as a single SSA value in the register file.
+                   * Replace it again with the individual components and create the necessary parallelcopies.
+                   */
+                  for (vector_operand vec : ctx.vector_operands) {
+                        /* Remove the temporary p_create_vector definition from register file. */
+                        reg_file.clear(vec.def);
+
+                        PhysReg reg = vec.def.physReg();
+                        for (unsigned i = vec.start; i < vec.start + vec.num_part; i++) {
+                              Operand& op = instr->operands[i];
+                              /* Assert that no already handled parallelcopy moved the operand. */
+                              assert(std::none_of(parallelcopies.begin(), parallelcopies.end(), [=](parallelcopy copy)
+                              { return copy.op.getTemp() == op.getTemp() && copy.def.isTemp(); }));
+
+                              /* Add a parallelcopy for each operand which is not in the correct position. */
+                              if (op.physReg() != reg) {
+                                    Definition pc_def = Definition(reg, op.regClass());
+                                    Operand pc_op = Operand(op.getTemp(), op.physReg());
+                                    parallelcopies.emplace_back(pc_op, pc_def, op.isCopyKill() ? i : -1);
+                              } else {
+                                    reg_file.fill(Definition(op.getTemp(), reg));
+                              }
+
+                              reg = reg.advance(op.bytes());
+                        }
+                  }
+                  ctx.vector_operands.clear();
 
-   PhysReg reg(256 + ctx.vgpr_bounds - (ctx.num_linear_vgprs + rc.size()));
-   /* Space that was for normal VGPRs, but is now for linear VGPRs. */
-   PhysRegInterval new_win = PhysRegInterval::from_until(reg, MAX2(old_normal_bounds.hi(), reg));
-
-   RegisterFile tmp_file(reg_file);
-   PhysRegInterval reg_win{reg, rc.size()};
-   std::vector<unsigned> blocking_vars = collect_vars(ctx, tmp_file, new_win);
-
-   /* Re-enable killed operands */
-   tmp_file.fill_killed_operands(instr.get());
-
-   /* Find new assignments for blocking vars. */
-   std::vector<parallelcopy> pc;
-   if (!ctx.policy.skip_optimistic_path &&
-       get_regs_for_copies(ctx, tmp_file, pc, blocking_vars, instr, reg_win)) {
-      parallelcopies.insert(parallelcopies.end(), pc.begin(), pc.end());
-   } else {
-      /* Fallback algorithm: reallocate all variables at once. */
-      std::vector<IDAndRegClass> vars;
-      for (unsigned id : find_vars(ctx, reg_file, old_normal_bounds))
-         vars.emplace_back(id, ctx.assignments[id].rc);
-      compact_relocate_vars(ctx, vars, parallelcopies, PhysReg(256));
-
-      std::vector<IDAndRegClass> killed_op_vars;
-      for (Operand& op : instr->operands) {
-         if (op.isTemp() && op.isFirstKillBeforeDef() && op.regClass().type() == RegType::vgpr)
-            killed_op_vars.emplace_back(op.tempId(), op.regClass());
-      }
-      compact_relocate_vars(ctx, killed_op_vars, parallelcopies, reg_win.lo());
-   }
+                  /* Update operand temporaries and fill non-killed operands. */
+                  update_renames(ctx, reg_file, parallelcopies, instr, false);
+            }
 
-   /* If this is updated earlier, a killed operand can't be placed inside the definition. */
-   ctx.num_linear_vgprs += rc.size();
+            PhysReg
+            get_reg_phi(ra_ctx& ctx, IDSet& live_in, RegisterFile& register_file,
+                        std::vector<aco_ptr<Instruction>>& instructions, Block& block,
+                        aco_ptr<Instruction>& phi, Temp tmp)
+            {
+                  std::vector<parallelcopy> parallelcopy;
+                  PhysReg reg = get_reg(ctx, register_file, tmp, parallelcopy, phi);
+                  update_renames(ctx, register_file, parallelcopy, phi);
+
+                  /* process parallelcopy */
+                  for (struct parallelcopy pc : parallelcopy) {
+                        /* see if it's a copy from a different phi */
+                        // TODO: prefer moving some previous phis over live-ins
+                        // TODO: somehow prevent phis fixed before the RA from being updated (shouldn't be a
+                        // problem in practice since they can only be fixed to exec)
+                        Instruction* prev_phi = NULL;
+                        for (auto phi_it = instructions.begin(); phi_it != instructions.end(); ++phi_it) {
+                              if ((*phi_it)->definitions[0].tempId() == pc.op.tempId())
+                                    prev_phi = phi_it->get();
+                        }
+                        if (prev_phi) {
+                              /* if so, just update that phi's register */
+                              prev_phi->definitions[0].setFixed(pc.def.physReg());
+                              register_file.fill(prev_phi->definitions[0]);
+                              ctx.assignments[prev_phi->definitions[0].tempId()] = {pc.def.physReg(), pc.def.regClass()};
+                              continue;
+                        }
+
+                        /* rename */
+                        auto orig_it = ctx.orig_names.find(pc.op.tempId());
+                        Temp orig = orig_it != ctx.orig_names.end() ? orig_it->second : pc.op.getTemp();
+                        add_rename(ctx, orig, pc.def.getTemp());
+
+                        /* otherwise, this is a live-in and we need to create a new phi
+                         * to move it in this block's predecessors */
+                        aco_opcode opcode =
+                        pc.op.getTemp().is_linear() ? aco_opcode::p_linear_phi : aco_opcode::p_phi;
+                        Block::edge_vec& preds =
+                        pc.op.getTemp().is_linear() ? block.linear_preds : block.logical_preds;
+                        aco_ptr<Instruction> new_phi{create_instruction(opcode, Format::PSEUDO, preds.size(), 1)};
+                        new_phi->definitions[0] = pc.def;
+                        for (unsigned i = 0; i < preds.size(); i++)
+                              new_phi->operands[i] = Operand(pc.op);
+                        instructions.emplace_back(std::move(new_phi));
+
+                        /* Remove from live_in, because handle_loop_phis() would re-create this phi later if this is
+                         * a loop header.
+                         */
+                        live_in.erase(orig.id());
+                  }
 
-   adjust_max_used_regs(ctx, rc, reg);
-   return reg;
-}
-
-bool
-should_compact_linear_vgprs(ra_ctx& ctx, const RegisterFile& reg_file)
-{
-   if (!(ctx.block->kind & block_kind_top_level) || ctx.block->linear_succs.empty())
-      return false;
-
-   /* Since we won't be able to copy linear VGPRs to make space when in control flow, we have to
-    * ensure in advance that there is enough space for normal VGPRs. */
-   unsigned max_vgpr_usage = 0;
-   unsigned next_toplevel = ctx.block->index + 1;
-   for (; !(ctx.program->blocks[next_toplevel].kind & block_kind_top_level); next_toplevel++) {
-      max_vgpr_usage =
-         MAX2(max_vgpr_usage, (unsigned)ctx.program->blocks[next_toplevel].register_demand.vgpr);
-   }
-   max_vgpr_usage =
-      MAX2(max_vgpr_usage, (unsigned)ctx.program->blocks[next_toplevel].live_in_demand.vgpr);
-
-   for (unsigned tmp : find_vars(ctx, reg_file, get_reg_bounds(ctx, RegType::vgpr, true)))
-      max_vgpr_usage -= ctx.assignments[tmp].rc.size();
-
-   return max_vgpr_usage > get_reg_bounds(ctx, RegType::vgpr, false).size;
-}
-
-PhysReg
-get_reg(ra_ctx& ctx, const RegisterFile& reg_file, Temp temp,
-        std::vector<parallelcopy>& parallelcopies, aco_ptr<Instruction>& instr,
-        int operand_index = -1)
-{
-   /* Note that "temp" might already be filled in the register file if we're making a copy of the
-    * temporary.
-    */
-
-   auto split_vec = ctx.split_vectors.find(temp.id());
-   if (split_vec != ctx.split_vectors.end()) {
-      unsigned offset = 0;
-      for (Definition def : split_vec->second->definitions) {
-         if (ctx.assignments[def.tempId()].affinity) {
-            assignment& affinity = ctx.assignments[ctx.assignments[def.tempId()].affinity];
-            if (affinity.assigned) {
-               PhysReg reg = affinity.reg;
-               reg.reg_b -= offset;
-               if (get_reg_specified(ctx, reg_file, temp.regClass(), instr, reg, operand_index))
                   return reg;
             }
-         }
-         offset += def.bytes();
-      }
-   }
-
-   if (ctx.assignments[temp.id()].affinity) {
-      assignment& affinity = ctx.assignments[ctx.assignments[temp.id()].affinity];
-      if (affinity.assigned) {
-         if (get_reg_specified(ctx, reg_file, temp.regClass(), instr, affinity.reg, operand_index))
-            return affinity.reg;
-      }
-   }
-   if (ctx.assignments[temp.id()].vcc) {
-      if (get_reg_specified(ctx, reg_file, temp.regClass(), instr, vcc, operand_index))
-         return vcc;
-   }
-   if (ctx.assignments[temp.id()].m0) {
-      if (get_reg_specified(ctx, reg_file, temp.regClass(), instr, m0, operand_index))
-         return m0;
-   }
-
-   std::optional<PhysReg> res;
-
-   if (ctx.vectors.find(temp.id()) != ctx.vectors.end()) {
-      res = get_reg_vector(ctx, reg_file, temp, instr, operand_index);
-      if (res)
-         return *res;
-   }
-
-   if (temp.size() == 1 && operand_index == -1) {
-      for (const Operand& op : instr->operands) {
-         if (op.isTemp() && op.isFirstKillBeforeDef() && op.regClass() == temp.regClass()) {
-            assert(op.isFixed());
-            if (op.physReg() == vcc || op.physReg() == vcc_hi)
-               continue;
-            if (get_reg_specified(ctx, reg_file, temp.regClass(), instr, op.physReg(),
-                                  operand_index))
-               return op.physReg();
-         }
-      }
-   }
-
-   DefInfo info(ctx, instr, temp.regClass(), operand_index);
 
-   if (!ctx.policy.skip_optimistic_path && !ctx.policy.use_compact_relocate) {
-      /* try to find space without live-range splits */
-      res = get_reg_simple(ctx, reg_file, info);
-
-      if (res)
-         return *res;
-   }
-
-   if (!ctx.policy.use_compact_relocate) {
-      /* try to find space with live-range splits */
-      res = get_reg_impl(ctx, reg_file, parallelcopies, info, instr);
-
-      if (res)
-         return *res;
-   }
-
-   /* try compacting the linear vgprs to make more space */
-   std::vector<parallelcopy> pc;
-   if (info.rc.type() == RegType::vgpr && (ctx.block->kind & block_kind_top_level) &&
-       compact_linear_vgprs(ctx, reg_file, pc)) {
-      parallelcopies.insert(parallelcopies.end(), pc.begin(), pc.end());
-
-      /* We don't need to fill the copy definitions in because we don't care about the linear VGPR
-       * space here. */
-      RegisterFile tmp_file(reg_file);
-      for (parallelcopy& copy : pc)
-         tmp_file.clear(copy.op);
-
-      return get_reg(ctx, tmp_file, temp, parallelcopies, instr, operand_index);
-   }
-
-   /* We should only fail here because keeping under the limit would require
-    * too many moves. */
-   assert(reg_file.count_zero(get_reg_bounds(ctx, info.rc)) >= info.size);
-
-   /* try using more registers */
-   if (!increase_register_file(ctx, info.rc)) {
-      /* fallback algorithm: reallocate all variables at once (linear VGPRs should already be
-       * compact at the end) */
-      const PhysRegInterval regs = get_reg_bounds(ctx, info.rc);
-
-      unsigned def_size = info.rc.size();
-      std::vector<IDAndRegClass> def_vars;
-      for (Definition def : instr->definitions) {
-         if (def.isPrecolored()) {
-            assert(!regs.contains({def.physReg(), def.size()}));
-            continue;
-         }
-         if (ctx.assignments[def.tempId()].assigned && def.regClass().type() == info.rc.type()) {
-            def_size += def.regClass().size();
-            def_vars.emplace_back(def.tempId(), def.regClass());
-         }
-      }
-
-      unsigned killed_op_size = 0;
-      std::vector<IDAndRegClass> killed_op_vars;
-      for (Operand op : instr->operands) {
-         if (op.isPrecolored()) {
-            assert(!regs.contains({op.physReg(), op.size()}));
-            continue;
-         }
-         if (op.isTemp() && op.isFirstKillBeforeDef() && op.regClass().type() == info.rc.type()) {
-            killed_op_size += op.regClass().size();
-            killed_op_vars.emplace_back(op.tempId(), op.regClass());
-         }
-      }
-
-      /* reallocate passthrough variables and non-killed operands */
-      std::vector<IDAndRegClass> vars;
-      for (unsigned id : find_vars(ctx, reg_file, regs))
-         vars.emplace_back(id, ctx.assignments[id].rc);
-      vars.emplace_back(0xffffffff, RegClass(info.rc.type(), MAX2(def_size, killed_op_size)));
-
-      PhysReg space = compact_relocate_vars(ctx, vars, parallelcopies, regs.lo());
-
-      /* reallocate killed operands */
-      compact_relocate_vars(ctx, killed_op_vars, parallelcopies, space);
-
-      /* reallocate definitions */
-      def_vars.emplace_back(0xffffffff, info.rc);
-      return compact_relocate_vars(ctx, def_vars, parallelcopies, space);
-   }
-
-   return get_reg(ctx, reg_file, temp, parallelcopies, instr, operand_index);
-}
-
-PhysReg
-get_reg_create_vector(ra_ctx& ctx, const RegisterFile& reg_file, Temp temp,
-                      std::vector<parallelcopy>& parallelcopies, aco_ptr<Instruction>& instr)
-{
-   RegClass rc = temp.regClass();
-   /* create_vector instructions have different costs w.r.t. register coalescing */
-   uint32_t size = rc.size();
-   uint32_t bytes = rc.bytes();
-   uint32_t stride = get_stride(rc);
-   PhysRegInterval bounds = get_reg_bounds(ctx, rc);
-
-   // TODO: improve p_create_vector for sub-dword vectors
-
-   PhysReg best_pos{0xFFF};
-   unsigned num_moves = 0xFF;
-   bool best_avoid = true;
-   uint32_t correct_pos_mask = 0;
-
-   /* test for each operand which definition placement causes the least shuffle instructions */
-   for (unsigned i = 0, offset = 0; i < instr->operands.size();
-        offset += instr->operands[i].bytes(), i++) {
-      // TODO: think about, if we can alias live operands on the same register
-      if (!instr->operands[i].isTemp() || instr->operands[i].getTemp().type() != rc.type() ||
-          reg_file.test(instr->operands[i].physReg(), instr->operands[i].bytes()))
-         continue;
-
-      if (offset > instr->operands[i].physReg().reg_b)
-         continue;
-
-      unsigned reg_lower = instr->operands[i].physReg().reg_b - offset;
-      if (reg_lower % 4)
-         continue;
-      PhysRegInterval reg_win = {PhysReg{reg_lower / 4}, size};
-      unsigned k = 0;
-
-      /* no need to check multiple times */
-      if (reg_win.lo() == best_pos)
-         continue;
-
-      /* check borders */
-      // TODO: this can be improved */
-      if (!bounds.contains(reg_win) || reg_win.lo() % stride != 0)
-         continue;
-      if (reg_win.lo() > bounds.lo() && reg_file[reg_win.lo()] != 0 &&
-          reg_file.get_id(reg_win.lo()) == reg_file.get_id(reg_win.lo().advance(-1)))
-         continue;
-      if (reg_win.hi() < bounds.hi() && reg_file[reg_win.hi().advance(-4)] != 0 &&
-          reg_file.get_id(reg_win.hi().advance(-1)) == reg_file.get_id(reg_win.hi()))
-         continue;
-
-      /* count variables to be moved and check "avoid" */
-      bool avoid = false;
-      bool linear_vgpr = false;
-      for (PhysReg j : reg_win) {
-         if (reg_file[j] != 0) {
-            if (reg_file[j] == 0xF0000000) {
-               PhysReg reg;
-               reg.reg_b = j * 4;
-               unsigned bytes_left = bytes - ((unsigned)j - reg_win.lo()) * 4;
-               for (unsigned byte_idx = 0; byte_idx < MIN2(bytes_left, 4); byte_idx++, reg.reg_b++)
-                  k += reg_file.test(reg, 1);
-            } else {
-               k += 4;
-               linear_vgpr |= ctx.assignments[reg_file[j]].rc.is_linear_vgpr();
+            void
+            get_regs_for_phis(ra_ctx& ctx, Block& block, RegisterFile& register_file,
+                              std::vector<aco_ptr<Instruction>>& instructions, IDSet& live_in)
+            {
+                  /* move all phis to instructions */
+                  bool has_linear_phis = false;
+                  for (aco_ptr<Instruction>& phi : block.instructions) {
+                        if (!is_phi(phi))
+                              break;
+                        if (!phi->definitions[0].isKill()) {
+                              has_linear_phis |= phi->opcode == aco_opcode::p_linear_phi;
+                              instructions.emplace_back(std::move(phi));
+                        }
+                  }
+
+                  /* assign phis with all-matching registers to that register */
+                  for (aco_ptr<Instruction>& phi : instructions) {
+                        Definition& definition = phi->definitions[0];
+                        if (definition.isFixed())
+                              continue;
+
+                        if (!phi->operands[0].isTemp())
+                              continue;
+
+                        PhysReg reg = phi->operands[0].physReg();
+                        auto OpsSame = [=](const Operand& op) -> bool
+                        { return op.isTemp() && (!op.isFixed() || op.physReg() == reg); };
+                        bool all_same = std::all_of(phi->operands.cbegin() + 1, phi->operands.cend(), OpsSame);
+                        if (!all_same)
+                              continue;
+
+                        if (!get_reg_specified(ctx, register_file, definition.regClass(), phi, reg, -1))
+                              continue;
+
+                        definition.setFixed(reg);
+                        register_file.fill(definition);
+                        ctx.assignments[definition.tempId()].set(definition);
+                  }
+
+                  /* try to find a register that is used by at least one operand */
+                  for (aco_ptr<Instruction>& phi : instructions) {
+                        Definition& definition = phi->definitions[0];
+                        if (definition.isFixed())
+                              continue;
+
+                        /* use affinity if available */
+                        if (ctx.assignments[definition.tempId()].affinity &&
+                              ctx.assignments[ctx.assignments[definition.tempId()].affinity].assigned) {
+                              assignment& affinity = ctx.assignments[ctx.assignments[definition.tempId()].affinity];
+                        assert(affinity.rc == definition.regClass());
+                        if (get_reg_specified(ctx, register_file, definition.regClass(), phi, affinity.reg, -1)) {
+                              definition.setFixed(affinity.reg);
+                              register_file.fill(definition);
+                              ctx.assignments[definition.tempId()].set(definition);
+                              continue;
+                        }
+                              }
+
+                              /* by going backwards, we aim to avoid copies in else-blocks */
+                              for (int i = phi->operands.size() - 1; i >= 0; i--) {
+                                    const Operand& op = phi->operands[i];
+                                    if (!op.isTemp() || !op.isFixed())
+                                          continue;
+
+                                    PhysReg reg = op.physReg();
+                                    if (get_reg_specified(ctx, register_file, definition.regClass(), phi, reg, -1)) {
+                                          definition.setFixed(reg);
+                                          register_file.fill(definition);
+                                          ctx.assignments[definition.tempId()].set(definition);
+                                          break;
+                                    }
+                              }
+                  }
+
+                  /* find registers for phis where the register was blocked or no operand was assigned */
+
+                  /* Don't use iterators because get_reg_phi() can add phis to the end of the vector. */
+                  for (unsigned i = 0; i < instructions.size(); i++) {
+                        aco_ptr<Instruction>& phi = instructions[i];
+                        Definition& definition = phi->definitions[0];
+                        if (definition.isFixed())
+                              continue;
+
+                        definition.setFixed(
+                              get_reg_phi(ctx, live_in, register_file, instructions, block, phi, definition.getTemp()));
+
+                        register_file.fill(definition);
+                        ctx.assignments[definition.tempId()].set(definition);
+                  }
+
+                  /* Provide a scratch register in case we need to preserve SCC */
+                  if (has_linear_phis || block.kind & block_kind_loop_header) {
+                        PhysReg scratch_reg = scc;
+                        if (register_file[scc]) {
+                              scratch_reg = get_reg_phi(ctx, live_in, register_file, instructions, block, ctx.phi_dummy,
+                                                        Temp(0, s1));
+                              if (block.kind & block_kind_loop_header)
+                                    ctx.loop_header.back().second = scratch_reg;
+                        }
+
+                        for (aco_ptr<Instruction>& phi : instructions) {
+                              phi->pseudo().scratch_sgpr = scratch_reg;
+                              phi->pseudo().needs_scratch_reg = true;
+                        }
+                  }
             }
-         }
-         avoid |= ctx.war_hint[j];
-      }
 
-      /* we cannot split live ranges of linear vgprs */
-      if (linear_vgpr)
-         continue;
-
-      if (avoid && !best_avoid)
-         continue;
-
-      /* count operands in wrong positions */
-      uint32_t correct_pos_mask_new = 0;
-      for (unsigned j = 0, offset2 = 0; j < instr->operands.size();
-           offset2 += instr->operands[j].bytes(), j++) {
-         Operand& op = instr->operands[j];
-         if (op.isTemp() && op.physReg().reg_b == reg_win.lo() * 4 + offset2)
-            correct_pos_mask_new |= 1 << j;
-         else
-            k += op.bytes();
-      }
-      bool aligned = rc == RegClass::v4 && reg_win.lo() % 4 == 0;
-      if (k > num_moves || (!aligned && k == num_moves))
-         continue;
-
-      best_pos = reg_win.lo();
-      num_moves = k;
-      best_avoid = avoid;
-      correct_pos_mask = correct_pos_mask_new;
-   }
-
-   /* too many moves: try the generic get_reg() function */
-   if (num_moves >= 2 * bytes) {
-      return get_reg(ctx, reg_file, temp, parallelcopies, instr);
-   } else if (num_moves > bytes) {
-      DefInfo info(ctx, instr, rc, -1);
-      std::optional<PhysReg> res = get_reg_simple(ctx, reg_file, info);
-      if (res)
-         return *res;
-   }
-
-   /* re-enable killed operands which are in the wrong position */
-   RegisterFile tmp_file(reg_file);
-   tmp_file.fill_killed_operands(instr.get());
-
-   for (unsigned i = 0; i < instr->operands.size(); i++) {
-      if ((correct_pos_mask >> i) & 1u && instr->operands[i].isKill())
-         tmp_file.clear(instr->operands[i]);
-   }
-
-   /* collect variables to be moved */
-   std::vector<unsigned> vars = collect_vars(ctx, tmp_file, PhysRegInterval{best_pos, size});
-
-   bool success = false;
-   std::vector<parallelcopy> pc;
-   success = get_regs_for_copies(ctx, tmp_file, pc, vars, instr, PhysRegInterval{best_pos, size});
-
-   if (!success) {
-      if (!increase_register_file(ctx, temp.regClass())) {
-         /* use the fallback algorithm in get_reg() */
-         return get_reg(ctx, reg_file, temp, parallelcopies, instr);
-      }
-      return get_reg_create_vector(ctx, reg_file, temp, parallelcopies, instr);
-   }
+            inline Temp
+            read_variable(ra_ctx& ctx, Temp val, unsigned block_idx)
+            {
+                  /* This variable didn't get renamed, yet. */
+                  if (!ctx.assignments[val.id()].renamed)
+                        return val;
+
+                  auto it = ctx.renames[block_idx].find(val.id());
+                  if (it == ctx.renames[block_idx].end())
+                        return val;
+                  else
+                        return it->second;
+            }
 
-   parallelcopies.insert(parallelcopies.end(), pc.begin(), pc.end());
-   adjust_max_used_regs(ctx, rc, best_pos);
+            Temp
+            handle_live_in(ra_ctx& ctx, Temp val, Block* block)
+            {
+                  /* This variable didn't get renamed, yet. */
+                  if (!ctx.assignments[val.id()].renamed)
+                        return val;
+
+                  Block::edge_vec& preds = val.is_linear() ? block->linear_preds : block->logical_preds;
+                  if (preds.size() == 0)
+                        return val;
+
+                  if (preds.size() == 1) {
+                        /* if the block has only one predecessor, just look there for the name */
+                        return read_variable(ctx, val, preds[0]);
+                  }
+
+                  /* there are multiple predecessors and the block is sealed */
+                  Temp* const ops = (Temp*)alloca(preds.size() * sizeof(Temp));
+
+                  /* get the rename from each predecessor and check if they are the same */
+                  Temp new_val;
+                  bool needs_phi = false;
+                  for (unsigned i = 0; i < preds.size(); i++) {
+                        ops[i] = read_variable(ctx, val, preds[i]);
+                        if (i == 0)
+                              new_val = ops[i];
+                        else
+                              needs_phi |= !(new_val == ops[i]);
+                  }
+
+                  if (needs_phi) {
+                        assert(!val.regClass().is_linear_vgpr());
+
+                        /* the variable has been renamed differently in the predecessors: we need to insert a phi */
+                        aco_opcode opcode = val.is_linear() ? aco_opcode::p_linear_phi : aco_opcode::p_phi;
+                        aco_ptr<Instruction> phi{create_instruction(opcode, Format::PSEUDO, preds.size(), 1)};
+                        new_val = ctx.program->allocateTmp(val.regClass());
+                        phi->definitions[0] = Definition(new_val);
+                        ctx.assignments.emplace_back();
+                        assert(ctx.assignments.size() == ctx.program->peekAllocationId());
+                        for (unsigned i = 0; i < preds.size(); i++) {
+                              /* update the operands so that it uses the new affinity */
+                              phi->operands[i] = Operand(ops[i]);
+                              assert(ctx.assignments[ops[i].id()].assigned);
+                              assert(ops[i].regClass() == new_val.regClass());
+                              phi->operands[i].setFixed(ctx.assignments[ops[i].id()].reg);
+                        }
+                        block->instructions.insert(block->instructions.begin(), std::move(phi));
+                  }
 
-   return best_pos;
-}
+                  return new_val;
+            }
 
-void
-handle_pseudo(ra_ctx& ctx, const RegisterFile& reg_file, Instruction* instr)
-{
-   if (instr->format != Format::PSEUDO)
-      return;
-
-   /* all instructions which use handle_operands() need this information */
-   switch (instr->opcode) {
-   case aco_opcode::p_extract_vector:
-   case aco_opcode::p_create_vector:
-   case aco_opcode::p_split_vector:
-   case aco_opcode::p_parallelcopy:
-   case aco_opcode::p_start_linear_vgpr: break;
-   default: return;
-   }
-
-   bool writes_linear = false;
-   /* if all definitions are logical vgpr, no need to care for SCC */
-   for (Definition& def : instr->definitions) {
-      if (def.getTemp().regClass().is_linear())
-         writes_linear = true;
-   }
-   /* if all operands are constant, no need to care either */
-   bool reads_linear = false;
-   for (Operand& op : instr->operands) {
-      if (op.isTemp() && op.getTemp().regClass().is_linear())
-         reads_linear = true;
-   }
-
-   if (!writes_linear || !reads_linear)
-      return;
-
-   instr->pseudo().needs_scratch_reg = true;
-
-   if (!reg_file[scc]) {
-      instr->pseudo().scratch_sgpr = scc;
-      return;
-   }
-
-   int reg = ctx.max_used_sgpr;
-   for (; reg >= 0 && reg_file[PhysReg{(unsigned)reg}]; reg--)
-      ;
-   if (reg < 0) {
-      reg = ctx.max_used_sgpr + 1;
-      for (; reg < ctx.program->max_reg_demand.sgpr && reg_file[PhysReg{(unsigned)reg}]; reg++)
-         ;
-   }
-
-   adjust_max_used_regs(ctx, s1, reg);
-   instr->pseudo().scratch_sgpr = PhysReg{(unsigned)reg};
-}
-
-bool
-operand_can_use_reg(amd_gfx_level gfx_level, aco_ptr<Instruction>& instr, unsigned idx, PhysReg reg,
-                    RegClass rc)
-{
-   if (reg.byte()) {
-      unsigned stride = get_subdword_operand_stride(gfx_level, instr, idx, rc);
-      if (reg.byte() % stride)
-         return false;
-   }
-
-   switch (instr->format) {
-   case Format::SMEM:
-      return reg != scc && reg != exec &&
-             (reg != m0 || idx == 1 || idx == 3) && /* offset can be m0 */
-             (reg != vcc || (instr->definitions.empty() && idx == 2) ||
-              gfx_level >= GFX10); /* sdata can be vcc */
-   case Format::MUBUF:
-   case Format::MTBUF: return idx != 2 || gfx_level < GFX12 || reg != scc;
-   case Format::SOPK:
-      if (idx == 0 && reg == scc)
-         return false;
-      FALLTHROUGH;
-   case Format::SOP2:
-   case Format::SOP1: {
-      auto tied_defs = get_tied_defs(instr.get());
-      return std::all_of(tied_defs.begin(), tied_defs.end(),
-                         [idx](auto op_idx) { return op_idx != idx; }) ||
-             is_sgpr_writable_without_side_effects(gfx_level, reg);
-   }
-   default:
-      // TODO: there are more instructions with restrictions on registers
-      return true;
-   }
-}
-
-void
-handle_fixed_operands(ra_ctx& ctx, RegisterFile& register_file,
-                      std::vector<parallelcopy>& parallelcopy, aco_ptr<Instruction>& instr)
-{
-   assert(instr->operands.size() <= 128);
-   assert(parallelcopy.empty());
-
-   RegisterFile tmp_file(register_file);
-
-   BITSET_DECLARE(live_reg_assigned, 128) = {0};
-   BITSET_DECLARE(mask, 128) = {0};
-
-   for (unsigned i = 0; i < instr->operands.size(); i++) {
-      Operand& op = instr->operands[i];
-
-      if (!op.isPrecolored())
-         continue;
-
-      assert(op.isTemp());
-      PhysReg src = ctx.assignments[op.tempId()].reg;
-      if (!BITSET_TEST(live_reg_assigned, i) && !op.isKill()) {
-         /* Figure out which register the temp will continue living in after the instruction. */
-         PhysReg live_reg = op.physReg();
-         bool found = false;
-         for (unsigned j = i; j < instr->operands.size(); ++j) {
-            const Operand& op2 = instr->operands[j];
-            if (!op2.isPrecolored() || op2.tempId() != op.tempId())
-               continue;
+            void
+            handle_loop_phis(ra_ctx& ctx, const IDSet& live_in, uint32_t loop_header_idx,
+                             uint32_t loop_exit_idx, PhysReg scratch_reg)
+            {
+                  Block& loop_header = ctx.program->blocks[loop_header_idx];
+                  aco::unordered_map<uint32_t, Temp> renames(ctx.memory);
+
+                  /* create phis for variables renamed during the loop */
+                  for (unsigned t : live_in) {
+                        if (!ctx.assignments[t].renamed)
+                              continue;
+
+                        Temp val = Temp(t, ctx.program->temp_rc[t]);
+                        Temp prev = read_variable(ctx, val, loop_header_idx - 1);
+                        Temp renamed = handle_live_in(ctx, val, &loop_header);
+                        if (renamed == prev)
+                              continue;
+
+                        /* insert additional renames at block end, but don't overwrite */
+                        renames[prev.id()] = renamed;
+                        ctx.orig_names[renamed.id()] = val;
+                        for (unsigned idx = loop_header_idx; idx < loop_exit_idx; idx++) {
+                              auto it = ctx.renames[idx].emplace(val.id(), renamed);
+                              /* if insertion is unsuccessful, update if necessary */
+                              if (!it.second && it.first->second == prev)
+                                    it.first->second = renamed;
+                        }
+
+                        /* update loop-carried values of the phi created by handle_live_in() */
+                        for (unsigned i = 1; i < loop_header.instructions[0]->operands.size(); i++) {
+                              Operand& op = loop_header.instructions[0]->operands[i];
+                              if (op.getTemp() == prev)
+                                    op.setTemp(renamed);
+                        }
+
+                        /* use the assignment from the loop preheader and fix def reg */
+                        assignment& var = ctx.assignments[prev.id()];
+                        ctx.assignments[renamed.id()] = var;
+                        loop_header.instructions[0]->definitions[0].setFixed(var.reg);
+
+                        /* Set scratch register */
+                        loop_header.instructions[0]->pseudo().scratch_sgpr = scratch_reg;
+                        loop_header.instructions[0]->pseudo().needs_scratch_reg = true;
+                  }
+
+                  /* rename loop carried phi operands */
+                  for (unsigned i = renames.size(); i < loop_header.instructions.size(); i++) {
+                        aco_ptr<Instruction>& phi = loop_header.instructions[i];
+                        if (!is_phi(phi))
+                              break;
+                        const Block::edge_vec& preds =
+                        phi->opcode == aco_opcode::p_phi ? loop_header.logical_preds : loop_header.linear_preds;
+                        for (unsigned j = 1; j < phi->operands.size(); j++) {
+                              Operand& op = phi->operands[j];
+                              if (!op.isTemp())
+                                    continue;
+
+                              /* Find the original name, since this operand might not use the original name if the phi
+                               * was created after init_reg_file().
+                               */
+                              auto it = ctx.orig_names.find(op.tempId());
+                              Temp orig = it != ctx.orig_names.end() ? it->second : op.getTemp();
+
+                              op.setTemp(read_variable(ctx, orig, preds[j]));
+                              op.setFixed(ctx.assignments[op.tempId()].reg);
+                        }
+                  }
+
+                  /* return early if no new phi was created */
+                  if (renames.empty())
+                        return;
+
+                  /* propagate new renames through loop */
+                  for (unsigned idx = loop_header_idx; idx < loop_exit_idx; idx++) {
+                        Block& current = ctx.program->blocks[idx];
+                        /* rename all uses in this block */
+                        for (aco_ptr<Instruction>& instr : current.instructions) {
+                              /* phis are renamed after RA */
+                              if (idx == loop_header_idx && is_phi(instr))
+                                    continue;
+
+                              for (Operand& op : instr->operands) {
+                                    if (!op.isTemp())
+                                          continue;
+
+                                    auto rename = renames.find(op.tempId());
+                                    if (rename != renames.end()) {
+                                          assert(rename->second.id());
+                                          op.setTemp(rename->second);
+                                    }
+                              }
+                        }
+                  }
+            }
 
-            /* Don't consider registers interfering with definitions - we'd have to immediately copy
-             * the temporary somewhere else to make space for the interfering definition.
+            /**
+             * This function serves the purpose to correctly initialize the register file
+             * at the beginning of a block (before any existing phis).
+             * In order to do so, all live-in variables are entered into the RegisterFile.
+             * Reg-to-reg moves (renames) from previous blocks are taken into account and
+             * the SSA is repaired by inserting corresponding phi-nodes.
              */
-            if (op2.isClobbered())
-               continue;
+            RegisterFile
+            init_reg_file(ra_ctx& ctx, const std::vector<IDSet>& live_out_per_block, Block& block)
+            {
+                  if (block.kind & block_kind_loop_exit) {
+                        uint32_t header = ctx.loop_header.back().first;
+                        PhysReg scratch_reg = ctx.loop_header.back().second;
+                        ctx.loop_header.pop_back();
+                        handle_loop_phis(ctx, live_out_per_block[header], header, block.index, scratch_reg);
+                  }
+
+                  RegisterFile register_file;
+                  const IDSet& live_in = live_out_per_block[block.index];
+                  assert(block.index != 0 || live_in.empty());
+
+                  if (block.kind & block_kind_loop_header) {
+                        ctx.loop_header.emplace_back(block.index, PhysReg{scc});
+                        /* already rename phis incoming value */
+                        for (aco_ptr<Instruction>& instr : block.instructions) {
+                              if (!is_phi(instr))
+                                    break;
+                              Operand& operand = instr->operands[0];
+                              if (operand.isTemp()) {
+                                    operand.setTemp(read_variable(ctx, operand.getTemp(), block.index - 1));
+                                    operand.setFixed(ctx.assignments[operand.tempId()].reg);
+                              }
+                        }
+                        for (unsigned t : live_in) {
+                              Temp val = Temp(t, ctx.program->temp_rc[t]);
+                              Temp renamed = read_variable(ctx, val, block.index - 1);
+                              if (renamed != val)
+                                    add_rename(ctx, val, renamed);
+                              assignment& var = ctx.assignments[renamed.id()];
+                              assert(var.assigned);
+                              register_file.fill(Definition(renamed, var.reg));
+                        }
+                  } else {
+                        /* rename phi operands */
+                        for (aco_ptr<Instruction>& instr : block.instructions) {
+                              if (!is_phi(instr))
+                                    break;
+                              const Block::edge_vec& preds =
+                              instr->opcode == aco_opcode::p_phi ? block.logical_preds : block.linear_preds;
+
+                              for (unsigned i = 0; i < instr->operands.size(); i++) {
+                                    Operand& operand = instr->operands[i];
+                                    if (!operand.isTemp())
+                                          continue;
+                                    operand.setTemp(read_variable(ctx, operand.getTemp(), preds[i]));
+                                    operand.setFixed(ctx.assignments[operand.tempId()].reg);
+                              }
+                        }
+                        for (unsigned t : live_in) {
+                              Temp val = Temp(t, ctx.program->temp_rc[t]);
+                              Temp renamed = handle_live_in(ctx, val, &block);
+                              assignment& var = ctx.assignments[renamed.id()];
+                              /* due to live-range splits, the live-in might be a phi, now */
+                              if (var.assigned) {
+                                    register_file.fill(Definition(renamed, var.reg));
+                              }
+                              if (renamed != val) {
+                                    add_rename(ctx, val, renamed);
+                              }
+                        }
+                  }
 
-            /* Choose the first register that doesn't interfere with definitions. If the temp can
-             * continue residing in the same register it's currently in, just choose that.
-             */
-            if (!found || op2.physReg() == src) {
-               live_reg = op2.physReg();
-               found = true;
-               if (op2.physReg() == src)
-                  break;
-            }
-         }
-
-         for (unsigned j = i; j < instr->operands.size(); ++j) {
-            if (!instr->operands[j].isPrecolored() || instr->operands[j].tempId() != op.tempId())
-               continue;
-
-            /* Fix up operand kill flags according to the live_reg choice we made. */
-            if (instr->operands[j].physReg() == live_reg) {
-               instr->operands[j].setCopyKill(false);
-               instr->operands[j].setKill(false);
-            } else {
-               instr->operands[j].setCopyKill(true);
+                  return register_file;
             }
-            BITSET_SET(live_reg_assigned, j);
-         }
-      }
-
-      adjust_max_used_regs(ctx, op.regClass(), op.physReg());
-
-      if (op.physReg() == src) {
-         tmp_file.block(op.physReg(), op.regClass());
-         continue;
-      }
-
-      /* An instruction can have at most one operand precolored to the same register. */
-      assert(std::none_of(parallelcopy.begin(), parallelcopy.end(),
-                          [&](auto copy) { return copy.def.physReg() == op.physReg(); }));
-
-      /* clear from register_file so fixed operands are not collected by collect_vars() */
-      tmp_file.clear(src, op.regClass());
-
-      BITSET_SET(mask, i);
-
-      Operand pc_op(instr->operands[i].getTemp());
-      pc_op.setFixed(src);
-      Definition pc_def = Definition(op.physReg(), pc_op.regClass());
-      parallelcopy.emplace_back(pc_op, pc_def, op.isCopyKill() ? i : -1);
-   }
-
-   if (BITSET_IS_EMPTY(mask))
-      return;
-
-   unsigned i;
-   std::vector<unsigned> blocking_vars;
-   BITSET_FOREACH_SET (i, mask, instr->operands.size()) {
-      Operand& op = instr->operands[i];
-      PhysRegInterval target{op.physReg(), op.size()};
-      std::vector<unsigned> blocking_vars2 = collect_vars(ctx, tmp_file, target);
-      blocking_vars.insert(blocking_vars.end(), blocking_vars2.begin(), blocking_vars2.end());
-
-      /* prevent get_regs_for_copies() from using these registers */
-      tmp_file.block(op.physReg(), op.regClass());
-   }
-
-   get_regs_for_copies(ctx, tmp_file, parallelcopy, blocking_vars, instr, PhysRegInterval());
-   update_renames(ctx, register_file, parallelcopy, instr);
-}
-
-void
-get_reg_for_operand(ra_ctx& ctx, RegisterFile& register_file,
-                    std::vector<parallelcopy>& parallelcopy, aco_ptr<Instruction>& instr,
-                    Operand& operand, unsigned operand_index)
-{
-   /* clear the operand in case it's only a stride mismatch */
-   PhysReg src = ctx.assignments[operand.tempId()].reg;
-   register_file.clear(src, operand.regClass());
-   PhysReg dst = get_reg(ctx, register_file, operand.getTemp(), parallelcopy, instr, operand_index);
-
-   Operand pc_op = operand;
-   pc_op.setFixed(src);
-   Definition pc_def = Definition(dst, pc_op.regClass());
-   parallelcopy.emplace_back(pc_op, pc_def);
-   update_renames(ctx, register_file, parallelcopy, instr);
-   register_file.fill(Definition(operand.getTemp(), dst));
-   operand.setFixed(dst);
-}
-
-void
-handle_vector_operands(ra_ctx& ctx, RegisterFile& register_file,
-                       std::vector<parallelcopy>& parallelcopies, aco_ptr<Instruction>& instr,
-                       unsigned operand_index)
-{
-   assert(operand_index == 0 || !instr->operands[operand_index - 1].isVectorAligned());
-   /* Count the operands that form part of the vector. */
-   uint32_t num_operands = 1;
-   uint32_t num_bytes = instr->operands[operand_index].bytes();
-   for (unsigned i = operand_index + 1; instr->operands[i - 1].isVectorAligned(); i++) {
-      /* If it's not late kill, we might end up trying to kill/re-enable the operands
-       * before resolve_vector_operands(), which wouldn't work.
-       */
-      assert(instr->operands[i].isLateKill());
-      num_operands++;
-      num_bytes += instr->operands[i].bytes();
-   }
-   RegClass rc = instr->operands[operand_index].regClass().resize(num_bytes);
-
-   /* Create temporary p_create_vector instruction and make use of get_reg_create_vector. */
-   aco_ptr<Instruction> vec{
-      create_instruction(aco_opcode::p_create_vector, Format::PSEUDO, num_operands, 1)};
-   for (unsigned i = 0; i < num_operands; i++) {
-      vec->operands[i] = instr->operands[operand_index + i];
-      vec->operands[i].setFixed(ctx.assignments[vec->operands[i].tempId()].reg);
-   }
-   Temp vec_temp = ctx.program->allocateTmp(rc);
-   ctx.assignments.emplace_back();
-   vec->definitions[0] = Definition(vec_temp);
-
-   PhysReg reg = get_reg_create_vector(ctx, register_file, vec_temp, parallelcopies, vec);
-   vec->definitions[0].setFixed(reg);
-   ctx.assignments[vec_temp.id()].set(vec->definitions[0]);
-
-   /* For now, fix instruction operands to previous registers. They are essentially ignored until
-    * we call resolve_vector_operands(). For the remaining handling of this instruction, we'll
-    * use the p_create_vector definition as temporary.
-    */
-   for (unsigned i = operand_index; i < operand_index + num_operands; i++)
-      instr->operands[i].setFixed(ctx.assignments[instr->operands[i].tempId()].reg);
-
-   update_renames(ctx, register_file, parallelcopies, instr);
-   ctx.vector_operands.emplace_back(
-      vector_operand{vec->definitions[0], operand_index, num_operands});
-   register_file.fill(vec->definitions[0]);
-}
-
-void
-resolve_vector_operands(ra_ctx& ctx, RegisterFile& reg_file,
-                        std::vector<parallelcopy>& parallelcopies, aco_ptr<Instruction>& instr)
-{
-   /* Vector Operands are temporarily stored as a single SSA value in the register file.
-    * Replace it again with the individual components and create the necessary parallelcopies.
-    */
-   for (vector_operand vec : ctx.vector_operands) {
-      /* Remove the temporary p_create_vector definition from register file. */
-      reg_file.clear(vec.def);
-
-      PhysReg reg = vec.def.physReg();
-      for (unsigned i = vec.start; i < vec.start + vec.num_part; i++) {
-         Operand& op = instr->operands[i];
-         /* Assert that no already handled parallelcopy moved the operand. */
-         assert(std::none_of(parallelcopies.begin(), parallelcopies.end(), [=](parallelcopy copy)
-                             { return copy.op.getTemp() == op.getTemp() && copy.def.isTemp(); }));
-
-         /* Add a parallelcopy for each operand which is not in the correct position. */
-         if (op.physReg() != reg) {
-            Definition pc_def = Definition(reg, op.regClass());
-            Operand pc_op = Operand(op.getTemp(), op.physReg());
-            parallelcopies.emplace_back(pc_op, pc_def, op.isCopyKill() ? i : -1);
-         } else {
-            reg_file.fill(Definition(op.getTemp(), reg));
-         }
-
-         reg = reg.advance(op.bytes());
-      }
-   }
-   ctx.vector_operands.clear();
-
-   /* Update operand temporaries and fill non-killed operands. */
-   update_renames(ctx, reg_file, parallelcopies, instr, false);
-}
-
-PhysReg
-get_reg_phi(ra_ctx& ctx, IDSet& live_in, RegisterFile& register_file,
-            std::vector<aco_ptr<Instruction>>& instructions, Block& block,
-            aco_ptr<Instruction>& phi, Temp tmp)
-{
-   std::vector<parallelcopy> parallelcopy;
-   PhysReg reg = get_reg(ctx, register_file, tmp, parallelcopy, phi);
-   update_renames(ctx, register_file, parallelcopy, phi);
-
-   /* process parallelcopy */
-   for (struct parallelcopy pc : parallelcopy) {
-      /* see if it's a copy from a different phi */
-      // TODO: prefer moving some previous phis over live-ins
-      // TODO: somehow prevent phis fixed before the RA from being updated (shouldn't be a
-      // problem in practice since they can only be fixed to exec)
-      Instruction* prev_phi = NULL;
-      for (auto phi_it = instructions.begin(); phi_it != instructions.end(); ++phi_it) {
-         if ((*phi_it)->definitions[0].tempId() == pc.op.tempId())
-            prev_phi = phi_it->get();
-      }
-      if (prev_phi) {
-         /* if so, just update that phi's register */
-         prev_phi->definitions[0].setFixed(pc.def.physReg());
-         register_file.fill(prev_phi->definitions[0]);
-         ctx.assignments[prev_phi->definitions[0].tempId()] = {pc.def.physReg(), pc.def.regClass()};
-         continue;
-      }
-
-      /* rename */
-      auto orig_it = ctx.orig_names.find(pc.op.tempId());
-      Temp orig = orig_it != ctx.orig_names.end() ? orig_it->second : pc.op.getTemp();
-      add_rename(ctx, orig, pc.def.getTemp());
-
-      /* otherwise, this is a live-in and we need to create a new phi
-       * to move it in this block's predecessors */
-      aco_opcode opcode =
-         pc.op.getTemp().is_linear() ? aco_opcode::p_linear_phi : aco_opcode::p_phi;
-      Block::edge_vec& preds =
-         pc.op.getTemp().is_linear() ? block.linear_preds : block.logical_preds;
-      aco_ptr<Instruction> new_phi{create_instruction(opcode, Format::PSEUDO, preds.size(), 1)};
-      new_phi->definitions[0] = pc.def;
-      for (unsigned i = 0; i < preds.size(); i++)
-         new_phi->operands[i] = Operand(pc.op);
-      instructions.emplace_back(std::move(new_phi));
-
-      /* Remove from live_in, because handle_loop_phis() would re-create this phi later if this is
-       * a loop header.
-       */
-      live_in.erase(orig.id());
-   }
-
-   return reg;
-}
-
-void
-get_regs_for_phis(ra_ctx& ctx, Block& block, RegisterFile& register_file,
-                  std::vector<aco_ptr<Instruction>>& instructions, IDSet& live_in)
-{
-   /* move all phis to instructions */
-   bool has_linear_phis = false;
-   for (aco_ptr<Instruction>& phi : block.instructions) {
-      if (!is_phi(phi))
-         break;
-      if (!phi->definitions[0].isKill()) {
-         has_linear_phis |= phi->opcode == aco_opcode::p_linear_phi;
-         instructions.emplace_back(std::move(phi));
-      }
-   }
-
-   /* assign phis with all-matching registers to that register */
-   for (aco_ptr<Instruction>& phi : instructions) {
-      Definition& definition = phi->definitions[0];
-      if (definition.isFixed())
-         continue;
-
-      if (!phi->operands[0].isTemp())
-         continue;
-
-      PhysReg reg = phi->operands[0].physReg();
-      auto OpsSame = [=](const Operand& op) -> bool
-      { return op.isTemp() && (!op.isFixed() || op.physReg() == reg); };
-      bool all_same = std::all_of(phi->operands.cbegin() + 1, phi->operands.cend(), OpsSame);
-      if (!all_same)
-         continue;
-
-      if (!get_reg_specified(ctx, register_file, definition.regClass(), phi, reg, -1))
-         continue;
-
-      definition.setFixed(reg);
-      register_file.fill(definition);
-      ctx.assignments[definition.tempId()].set(definition);
-   }
-
-   /* try to find a register that is used by at least one operand */
-   for (aco_ptr<Instruction>& phi : instructions) {
-      Definition& definition = phi->definitions[0];
-      if (definition.isFixed())
-         continue;
-
-      /* use affinity if available */
-      if (ctx.assignments[definition.tempId()].affinity &&
-          ctx.assignments[ctx.assignments[definition.tempId()].affinity].assigned) {
-         assignment& affinity = ctx.assignments[ctx.assignments[definition.tempId()].affinity];
-         assert(affinity.rc == definition.regClass());
-         if (get_reg_specified(ctx, register_file, definition.regClass(), phi, affinity.reg, -1)) {
-            definition.setFixed(affinity.reg);
-            register_file.fill(definition);
-            ctx.assignments[definition.tempId()].set(definition);
-            continue;
-         }
-      }
-
-      /* by going backwards, we aim to avoid copies in else-blocks */
-      for (int i = phi->operands.size() - 1; i >= 0; i--) {
-         const Operand& op = phi->operands[i];
-         if (!op.isTemp() || !op.isFixed())
-            continue;
-
-         PhysReg reg = op.physReg();
-         if (get_reg_specified(ctx, register_file, definition.regClass(), phi, reg, -1)) {
-            definition.setFixed(reg);
-            register_file.fill(definition);
-            ctx.assignments[definition.tempId()].set(definition);
-            break;
-         }
-      }
-   }
-
-   /* find registers for phis where the register was blocked or no operand was assigned */
-
-   /* Don't use iterators because get_reg_phi() can add phis to the end of the vector. */
-   for (unsigned i = 0; i < instructions.size(); i++) {
-      aco_ptr<Instruction>& phi = instructions[i];
-      Definition& definition = phi->definitions[0];
-      if (definition.isFixed())
-         continue;
-
-      definition.setFixed(
-         get_reg_phi(ctx, live_in, register_file, instructions, block, phi, definition.getTemp()));
-
-      register_file.fill(definition);
-      ctx.assignments[definition.tempId()].set(definition);
-   }
-
-   /* Provide a scratch register in case we need to preserve SCC */
-   if (has_linear_phis || block.kind & block_kind_loop_header) {
-      PhysReg scratch_reg = scc;
-      if (register_file[scc]) {
-         scratch_reg = get_reg_phi(ctx, live_in, register_file, instructions, block, ctx.phi_dummy,
-                                   Temp(0, s1));
-         if (block.kind & block_kind_loop_header)
-            ctx.loop_header.back().second = scratch_reg;
-      }
-
-      for (aco_ptr<Instruction>& phi : instructions) {
-         phi->pseudo().scratch_sgpr = scratch_reg;
-         phi->pseudo().needs_scratch_reg = true;
-      }
-   }
-}
-
-inline Temp
-read_variable(ra_ctx& ctx, Temp val, unsigned block_idx)
-{
-   /* This variable didn't get renamed, yet. */
-   if (!ctx.assignments[val.id()].renamed)
-      return val;
-
-   auto it = ctx.renames[block_idx].find(val.id());
-   if (it == ctx.renames[block_idx].end())
-      return val;
-   else
-      return it->second;
-}
-
-Temp
-handle_live_in(ra_ctx& ctx, Temp val, Block* block)
-{
-   /* This variable didn't get renamed, yet. */
-   if (!ctx.assignments[val.id()].renamed)
-      return val;
-
-   Block::edge_vec& preds = val.is_linear() ? block->linear_preds : block->logical_preds;
-   if (preds.size() == 0)
-      return val;
-
-   if (preds.size() == 1) {
-      /* if the block has only one predecessor, just look there for the name */
-      return read_variable(ctx, val, preds[0]);
-   }
-
-   /* there are multiple predecessors and the block is sealed */
-   Temp* const ops = (Temp*)alloca(preds.size() * sizeof(Temp));
-
-   /* get the rename from each predecessor and check if they are the same */
-   Temp new_val;
-   bool needs_phi = false;
-   for (unsigned i = 0; i < preds.size(); i++) {
-      ops[i] = read_variable(ctx, val, preds[i]);
-      if (i == 0)
-         new_val = ops[i];
-      else
-         needs_phi |= !(new_val == ops[i]);
-   }
-
-   if (needs_phi) {
-      assert(!val.regClass().is_linear_vgpr());
-
-      /* the variable has been renamed differently in the predecessors: we need to insert a phi */
-      aco_opcode opcode = val.is_linear() ? aco_opcode::p_linear_phi : aco_opcode::p_phi;
-      aco_ptr<Instruction> phi{create_instruction(opcode, Format::PSEUDO, preds.size(), 1)};
-      new_val = ctx.program->allocateTmp(val.regClass());
-      phi->definitions[0] = Definition(new_val);
-      ctx.assignments.emplace_back();
-      assert(ctx.assignments.size() == ctx.program->peekAllocationId());
-      for (unsigned i = 0; i < preds.size(); i++) {
-         /* update the operands so that it uses the new affinity */
-         phi->operands[i] = Operand(ops[i]);
-         assert(ctx.assignments[ops[i].id()].assigned);
-         assert(ops[i].regClass() == new_val.regClass());
-         phi->operands[i].setFixed(ctx.assignments[ops[i].id()].reg);
-      }
-      block->instructions.insert(block->instructions.begin(), std::move(phi));
-   }
-
-   return new_val;
-}
-
-void
-handle_loop_phis(ra_ctx& ctx, const IDSet& live_in, uint32_t loop_header_idx,
-                 uint32_t loop_exit_idx, PhysReg scratch_reg)
-{
-   Block& loop_header = ctx.program->blocks[loop_header_idx];
-   aco::unordered_map<uint32_t, Temp> renames(ctx.memory);
-
-   /* create phis for variables renamed during the loop */
-   for (unsigned t : live_in) {
-      if (!ctx.assignments[t].renamed)
-         continue;
-
-      Temp val = Temp(t, ctx.program->temp_rc[t]);
-      Temp prev = read_variable(ctx, val, loop_header_idx - 1);
-      Temp renamed = handle_live_in(ctx, val, &loop_header);
-      if (renamed == prev)
-         continue;
-
-      /* insert additional renames at block end, but don't overwrite */
-      renames[prev.id()] = renamed;
-      ctx.orig_names[renamed.id()] = val;
-      for (unsigned idx = loop_header_idx; idx < loop_exit_idx; idx++) {
-         auto it = ctx.renames[idx].emplace(val.id(), renamed);
-         /* if insertion is unsuccessful, update if necessary */
-         if (!it.second && it.first->second == prev)
-            it.first->second = renamed;
-      }
 
-      /* update loop-carried values of the phi created by handle_live_in() */
-      for (unsigned i = 1; i < loop_header.instructions[0]->operands.size(); i++) {
-         Operand& op = loop_header.instructions[0]->operands[i];
-         if (op.getTemp() == prev)
-            op.setTemp(renamed);
-      }
+            bool
+            vop3_can_use_vop2acc(ra_ctx& ctx, Instruction* instr)
+            {
+                  if (!instr->isVOP3() && !instr->isVOP3P())
+                        return false;
+
+                  switch (instr->opcode) {
+                        case aco_opcode::v_mad_f32:
+                        case aco_opcode::v_mad_f16:
+                        case aco_opcode::v_mad_legacy_f16: break;
+                        case aco_opcode::v_fma_f32:
+                        case aco_opcode::v_pk_fma_f16:
+                        case aco_opcode::v_fma_f16:
+                        case aco_opcode::v_dot4_i32_i8:
+                              if (ctx.program->gfx_level < GFX10)
+                                    return false;
+                        break;
+                        case aco_opcode::v_mad_legacy_f32:
+                              if (!ctx.program->dev.has_mac_legacy32)
+                                    return false;
+                        break;
+                        case aco_opcode::v_fma_legacy_f32:
+                              if (!ctx.program->dev.has_fmac_legacy32)
+                                    return false;
+                        break;
+                        default: return false;
+                  }
+
+                  if (!instr->operands[2].isOfType(RegType::vgpr) || !instr->operands[2].isKillBeforeDef() ||
+                        (!instr->operands[0].isOfType(RegType::vgpr) && !instr->operands[1].isOfType(RegType::vgpr)))
+                        return false;
+
+                  if (instr->isVOP3P()) {
+                        for (unsigned i = 0; i < 3; i++) {
+                              if (instr->operands[i].isLiteral())
+                                    continue;
+
+                              if (instr->valu().opsel_lo[i])
+                                    return false;
+
+                              /* v_pk_fmac_f16 inline constants are replicated to hi bits starting with gfx11. */
+                              if (instr->valu().opsel_hi[i] ==
+                                    (instr->operands[i].isConstant() && ctx.program->gfx_level >= GFX11))
+                                    return false;
+                        }
+                  } else {
+                        if (instr->valu().opsel & (ctx.program->gfx_level < GFX11 ? 0xf : ~0x3))
+                              return false;
+                        for (unsigned i = 0; i < 2; i++) {
+                              if (!instr->operands[i].isOfType(RegType::vgpr) && instr->valu().opsel[i])
+                                    return false;
+                        }
+                  }
+
+                  unsigned im_mask = instr->isDPP16() && instr->isVOP3() ? 0x3 : 0;
+                  if (instr->valu().omod || instr->valu().clamp || (instr->valu().abs & ~im_mask) ||
+                        (instr->valu().neg & ~im_mask))
+                        return false;
 
-      /* use the assignment from the loop preheader and fix def reg */
-      assignment& var = ctx.assignments[prev.id()];
-      ctx.assignments[renamed.id()] = var;
-      loop_header.instructions[0]->definitions[0].setFixed(var.reg);
-
-      /* Set scratch register */
-      loop_header.instructions[0]->pseudo().scratch_sgpr = scratch_reg;
-      loop_header.instructions[0]->pseudo().needs_scratch_reg = true;
-   }
-
-   /* rename loop carried phi operands */
-   for (unsigned i = renames.size(); i < loop_header.instructions.size(); i++) {
-      aco_ptr<Instruction>& phi = loop_header.instructions[i];
-      if (!is_phi(phi))
-         break;
-      const Block::edge_vec& preds =
-         phi->opcode == aco_opcode::p_phi ? loop_header.logical_preds : loop_header.linear_preds;
-      for (unsigned j = 1; j < phi->operands.size(); j++) {
-         Operand& op = phi->operands[j];
-         if (!op.isTemp())
-            continue;
-
-         /* Find the original name, since this operand might not use the original name if the phi
-          * was created after init_reg_file().
-          */
-         auto it = ctx.orig_names.find(op.tempId());
-         Temp orig = it != ctx.orig_names.end() ? it->second : op.getTemp();
+                  return true;
+            }
 
-         op.setTemp(read_variable(ctx, orig, preds[j]));
-         op.setFixed(ctx.assignments[op.tempId()].reg);
-      }
-   }
+            bool
+            sop2_can_use_sopk(ra_ctx& ctx, Instruction* instr)
+            {
+                  if (instr->opcode != aco_opcode::s_add_i32 && instr->opcode != aco_opcode::s_add_u32 &&
+                        instr->opcode != aco_opcode::s_mul_i32 && instr->opcode != aco_opcode::s_cselect_b32)
+                        return false;
+
+                  if (instr->opcode == aco_opcode::s_add_u32 && !instr->definitions[1].isKill())
+                        return false;
+
+                  uint32_t literal_idx = 0;
+
+                  if (instr->opcode != aco_opcode::s_cselect_b32 && instr->operands[1].isLiteral())
+                        literal_idx = 1;
+
+                  if (!instr->operands[!literal_idx].isTemp() || !instr->operands[!literal_idx].isKillBeforeDef())
+                        return false;
+
+                  if (!instr->operands[literal_idx].isLiteral())
+                        return false;
+
+                  const uint32_t i16_mask = 0xffff8000u;
+                  uint32_t value = instr->operands[literal_idx].constantValue();
+                  if ((value & i16_mask) && (value & i16_mask) != i16_mask)
+                        return false;
 
-   /* return early if no new phi was created */
-   if (renames.empty())
-      return;
-
-   /* propagate new renames through loop */
-   for (unsigned idx = loop_header_idx; idx < loop_exit_idx; idx++) {
-      Block& current = ctx.program->blocks[idx];
-      /* rename all uses in this block */
-      for (aco_ptr<Instruction>& instr : current.instructions) {
-         /* phis are renamed after RA */
-         if (idx == loop_header_idx && is_phi(instr))
-            continue;
-
-         for (Operand& op : instr->operands) {
-            if (!op.isTemp())
-               continue;
-
-            auto rename = renames.find(op.tempId());
-            if (rename != renames.end()) {
-               assert(rename->second.id());
-               op.setTemp(rename->second);
+                  return true;
             }
-         }
-      }
-   }
-}
-
-/**
- * This function serves the purpose to correctly initialize the register file
- * at the beginning of a block (before any existing phis).
- * In order to do so, all live-in variables are entered into the RegisterFile.
- * Reg-to-reg moves (renames) from previous blocks are taken into account and
- * the SSA is repaired by inserting corresponding phi-nodes.
- */
-RegisterFile
-init_reg_file(ra_ctx& ctx, const std::vector<IDSet>& live_out_per_block, Block& block)
-{
-   if (block.kind & block_kind_loop_exit) {
-      uint32_t header = ctx.loop_header.back().first;
-      PhysReg scratch_reg = ctx.loop_header.back().second;
-      ctx.loop_header.pop_back();
-      handle_loop_phis(ctx, live_out_per_block[header], header, block.index, scratch_reg);
-   }
-
-   RegisterFile register_file;
-   const IDSet& live_in = live_out_per_block[block.index];
-   assert(block.index != 0 || live_in.empty());
-
-   if (block.kind & block_kind_loop_header) {
-      ctx.loop_header.emplace_back(block.index, PhysReg{scc});
-      /* already rename phis incoming value */
-      for (aco_ptr<Instruction>& instr : block.instructions) {
-         if (!is_phi(instr))
-            break;
-         Operand& operand = instr->operands[0];
-         if (operand.isTemp()) {
-            operand.setTemp(read_variable(ctx, operand.getTemp(), block.index - 1));
-            operand.setFixed(ctx.assignments[operand.tempId()].reg);
-         }
-      }
-      for (unsigned t : live_in) {
-         Temp val = Temp(t, ctx.program->temp_rc[t]);
-         Temp renamed = read_variable(ctx, val, block.index - 1);
-         if (renamed != val)
-            add_rename(ctx, val, renamed);
-         assignment& var = ctx.assignments[renamed.id()];
-         assert(var.assigned);
-         register_file.fill(Definition(renamed, var.reg));
-      }
-   } else {
-      /* rename phi operands */
-      for (aco_ptr<Instruction>& instr : block.instructions) {
-         if (!is_phi(instr))
-            break;
-         const Block::edge_vec& preds =
-            instr->opcode == aco_opcode::p_phi ? block.logical_preds : block.linear_preds;
-
-         for (unsigned i = 0; i < instr->operands.size(); i++) {
-            Operand& operand = instr->operands[i];
-            if (!operand.isTemp())
-               continue;
-            operand.setTemp(read_variable(ctx, operand.getTemp(), preds[i]));
-            operand.setFixed(ctx.assignments[operand.tempId()].reg);
-         }
-      }
-      for (unsigned t : live_in) {
-         Temp val = Temp(t, ctx.program->temp_rc[t]);
-         Temp renamed = handle_live_in(ctx, val, &block);
-         assignment& var = ctx.assignments[renamed.id()];
-         /* due to live-range splits, the live-in might be a phi, now */
-         if (var.assigned) {
-            register_file.fill(Definition(renamed, var.reg));
-         }
-         if (renamed != val) {
-            add_rename(ctx, val, renamed);
-         }
-      }
-   }
 
-   return register_file;
-}
+            void
+            create_phi_vector_affinities(ra_ctx& ctx, aco_ptr<Instruction>& instr,
+                                         std::map<Operand*, std::vector<vector_info>>& vector_phis)
+            {
+                  auto it = ctx.vectors.find(instr->definitions[0].tempId());
+                  if (it == ctx.vectors.end())
+                        return;
+                  vector_info& dest_vector = it->second;
+
+                  auto pair = vector_phis.try_emplace(dest_vector.parts, instr->operands.size(), dest_vector);
+                  std::vector<vector_info>& src_vectors = pair.first->second;
+                  if (pair.second) {
+                        RegType type = instr->definitions[0].regClass().type();
+
+                        for (vector_info& src_vector : src_vectors) {
+                              src_vector.parts =
+                              (Operand*)ctx.memory.allocate(sizeof(Operand) * src_vector.num_parts, alignof(Operand));
+                              for (unsigned j = 0; j < src_vector.num_parts; j++)
+                                    src_vector.parts[j] = Operand(RegClass::get(type, dest_vector.parts[j].bytes()));
+                        }
+                  }
+
+                  unsigned index = 0;
+                  for (; index < dest_vector.num_parts; index++) {
+                        if (dest_vector.parts[index].isTemp() &&
+                              dest_vector.parts[index].tempId() == instr->definitions[0].tempId())
+                              break;
+                  }
+                  assert(index != dest_vector.num_parts);
+
+                  for (int i = instr->operands.size() - 1; i >= 0; i--) {
+                        const Operand& op = instr->operands[i];
+                        if (!op.isTemp() || op.regClass() != instr->definitions[0].regClass())
+                              continue;
+
+                        src_vectors[i].parts[index] = op;
+                        ctx.vectors[op.tempId()] = src_vectors[i];
+                  }
+            }
 
-bool
-vop3_can_use_vop2acc(ra_ctx& ctx, Instruction* instr)
-{
-   if (!instr->isVOP3() && !instr->isVOP3P())
-      return false;
-
-   switch (instr->opcode) {
-   case aco_opcode::v_mad_f32:
-   case aco_opcode::v_mad_f16:
-   case aco_opcode::v_mad_legacy_f16: break;
-   case aco_opcode::v_fma_f32:
-   case aco_opcode::v_pk_fma_f16:
-   case aco_opcode::v_fma_f16:
-   case aco_opcode::v_dot4_i32_i8:
-      if (ctx.program->gfx_level < GFX10)
-         return false;
-      break;
-   case aco_opcode::v_mad_legacy_f32:
-      if (!ctx.program->dev.has_mac_legacy32)
-         return false;
-      break;
-   case aco_opcode::v_fma_legacy_f32:
-      if (!ctx.program->dev.has_fmac_legacy32)
-         return false;
-      break;
-   default: return false;
-   }
-
-   if (!instr->operands[2].isOfType(RegType::vgpr) || !instr->operands[2].isKillBeforeDef() ||
-       (!instr->operands[0].isOfType(RegType::vgpr) && !instr->operands[1].isOfType(RegType::vgpr)))
-      return false;
-
-   if (instr->isVOP3P()) {
-      for (unsigned i = 0; i < 3; i++) {
-         if (instr->operands[i].isLiteral())
-            continue;
-
-         if (instr->valu().opsel_lo[i])
-            return false;
-
-         /* v_pk_fmac_f16 inline constants are replicated to hi bits starting with gfx11. */
-         if (instr->valu().opsel_hi[i] ==
-             (instr->operands[i].isConstant() && ctx.program->gfx_level >= GFX11))
-            return false;
-      }
-   } else {
-      if (instr->valu().opsel & (ctx.program->gfx_level < GFX11 ? 0xf : ~0x3))
-         return false;
-      for (unsigned i = 0; i < 2; i++) {
-         if (!instr->operands[i].isOfType(RegType::vgpr) && instr->valu().opsel[i])
-            return false;
-      }
-   }
+            void
+            get_affinities(ra_ctx& ctx)
+            {
+                  std::vector<std::vector<Temp>> phi_resources;
+                  aco::unordered_map<uint32_t, uint32_t> temp_to_phi_resources(ctx.memory);
+
+                  for (auto block_rit = ctx.program->blocks.rbegin(); block_rit != ctx.program->blocks.rend();
+                       block_rit++) {
+                        Block& block = *block_rit;
+
+                        std::vector<aco_ptr<Instruction>>::reverse_iterator rit;
+                        for (rit = block.instructions.rbegin(); rit != block.instructions.rend(); ++rit) {
+                              aco_ptr<Instruction>& instr = *rit;
+                              if (is_phi(instr))
+                                    break;
+
+                              /* add vector affinities */
+                              if (instr->opcode == aco_opcode::p_create_vector) {
+                                    for (const Operand& op : instr->operands) {
+                                          if (op.isTemp() && op.isFirstKill() &&
+                                                op.getTemp().type() == instr->definitions[0].getTemp().type())
+                                                ctx.vectors[op.tempId()] = vector_info(instr.get());
+                                    }
+                              } else if (instr->format == Format::MIMG && instr->operands.size() > 4 &&
+                                    !instr->mimg().strict_wqm) {
+
+                                    bool is_vector = false;
+                              for (unsigned i = 3, vector_begin = 3; i < instr->operands.size(); i++) {
+                                    if (is_vector || instr->operands[i].isVectorAligned())
+                                          ctx.vectors[instr->operands[i].tempId()] = vector_info(instr.get(), vector_begin);
+                                    else if (ctx.program->gfx_level < GFX12 && !instr->operands[3].isVectorAligned())
+                                          ctx.vectors[instr->operands[i].tempId()] = vector_info(instr.get(), 3, true);
+                                    is_vector = instr->operands[i].isVectorAligned();
+                                    vector_begin = is_vector ? vector_begin : i + 1;
+                              }
+                                    } else if (instr->opcode == aco_opcode::p_split_vector &&
+                                          instr->operands[0].isFirstKillBeforeDef()) {
+                                          ctx.split_vectors[instr->operands[0].tempId()] = instr.get();
+                                          } else if (instr->isVOPC() && !instr->isVOP3()) {
+                                                if (!instr->isSDWA() || ctx.program->gfx_level == GFX8)
+                                                      ctx.assignments[instr->definitions[0].tempId()].vcc = true;
+                                          } else if (instr->isVOP2() && !instr->isVOP3()) {
+                                                if (instr->operands.size() == 3 && instr->operands[2].isTemp() &&
+                                                      instr->operands[2].regClass().type() == RegType::sgpr)
+                                                      ctx.assignments[instr->operands[2].tempId()].vcc = true;
+                                                if (instr->definitions.size() == 2)
+                                                      ctx.assignments[instr->definitions[1].tempId()].vcc = true;
+                                          } else if (instr->opcode == aco_opcode::s_and_b32 ||
+                                                instr->opcode == aco_opcode::s_and_b64) {
+                                                /* If SCC is used by a branch, we might be able to use
+                                                 * s_cbranch_vccz/s_cbranch_vccnz if the operand is VCC.
+                                                 */
+                                                if (!instr->definitions[1].isKill() && instr->operands[0].isTemp() &&
+                                                      instr->operands[1].isFixed() && instr->operands[1].physReg() == exec)
+                                                      ctx.assignments[instr->operands[0].tempId()].vcc = true;
+                                                } else if (instr->opcode == aco_opcode::s_sendmsg) {
+                                                      ctx.assignments[instr->operands[0].tempId()].m0 = true;
+                                                }
+
+                                                auto tied_defs = get_tied_defs(instr.get());
+                                                for (unsigned i = 0; i < instr->definitions.size(); i++) {
+                                                      const Definition& def = instr->definitions[i];
+                                                      if (!def.isTemp())
+                                                            continue;
+                                                      /* mark last-seen phi operand */
+                                                      auto it = temp_to_phi_resources.find(def.tempId());
+                                                      if (it != temp_to_phi_resources.end() &&
+                                                            def.regClass() == phi_resources[it->second][0].regClass()) {
+                                                            phi_resources[it->second][0] = def.getTemp();
+                                                      /* try to coalesce phi affinities with parallelcopies */
+                                                      Operand op;
+                                                      if (instr->opcode == aco_opcode::p_parallelcopy) {
+                                                            op = instr->operands[i];
+                                                      } else if (i < tied_defs.size()) {
+                                                            op = instr->operands[tied_defs[i]];
+                                                      } else if (vop3_can_use_vop2acc(ctx, instr.get())) {
+                                                            op = instr->operands[2];
+                                                      } else if (i == 0 && sop2_can_use_sopk(ctx, instr.get())) {
+                                                            op = instr->operands[instr->operands[0].isLiteral()];
+                                                      } else {
+                                                            continue;
+                                                      }
+
+                                                      if (op.isTemp() && op.isFirstKillBeforeDef() && def.regClass() == op.regClass()) {
+                                                            phi_resources[it->second].emplace_back(op.getTemp());
+                                                            temp_to_phi_resources[op.tempId()] = it->second;
+                                                      }
+                                                            }
+                                                }
+                        }
+
+                        /* collect phi affinities */
+                        std::map<Operand*, std::vector<vector_info>> vector_phis;
+                        for (; rit != block.instructions.rend(); ++rit) {
+                              aco_ptr<Instruction>& instr = *rit;
+                              assert(is_phi(instr));
+
+                              if (instr->definitions[0].isKill() || instr->definitions[0].isFixed())
+                                    continue;
+
+                              assert(instr->definitions[0].isTemp());
+                              auto it = temp_to_phi_resources.find(instr->definitions[0].tempId());
+                              unsigned index = phi_resources.size();
+                              std::vector<Temp>* affinity_related;
+                              if (it != temp_to_phi_resources.end()) {
+                                    index = it->second;
+                                    phi_resources[index][0] = instr->definitions[0].getTemp();
+                                    affinity_related = &phi_resources[index];
+                              } else {
+                                    phi_resources.emplace_back(std::vector<Temp>{instr->definitions[0].getTemp()});
+                                    affinity_related = &phi_resources.back();
+                              }
+
+                              for (const Operand& op : instr->operands) {
+                                    if (op.isTemp() && op.isKill() && op.regClass() == instr->definitions[0].regClass()) {
+                                          affinity_related->emplace_back(op.getTemp());
+                                          if (block.kind & block_kind_loop_header)
+                                                continue;
+                                          temp_to_phi_resources[op.tempId()] = index;
+                                    }
+                              }
+
+                              create_phi_vector_affinities(ctx, instr, vector_phis);
+                        }
+
+                        /* visit the loop header phis first in order to create nested affinities */
+                        if (block.kind & block_kind_loop_exit) {
+                              /* find loop header */
+                              auto header_rit = block_rit;
+                              while ((header_rit + 1)->loop_nest_depth > block.loop_nest_depth)
+                                    header_rit++;
+
+                              for (aco_ptr<Instruction>& phi : header_rit->instructions) {
+                                    if (!is_phi(phi))
+                                          break;
+                                    if (phi->definitions[0].isKill() || phi->definitions[0].isFixed())
+                                          continue;
+
+                                    /* create an (empty) merge-set for the phi-related variables */
+                                    auto it = temp_to_phi_resources.find(phi->definitions[0].tempId());
+                                    unsigned index = phi_resources.size();
+                                    if (it == temp_to_phi_resources.end()) {
+                                          temp_to_phi_resources[phi->definitions[0].tempId()] = index;
+                                          phi_resources.emplace_back(std::vector<Temp>{phi->definitions[0].getTemp()});
+                                    } else {
+                                          index = it->second;
+                                    }
+                                    for (unsigned i = 1; i < phi->operands.size(); i++) {
+                                          const Operand& op = phi->operands[i];
+                                          if (op.isTemp() && op.isKill() && op.regClass() == phi->definitions[0].regClass()) {
+                                                temp_to_phi_resources[op.tempId()] = index;
+                                          }
+                                    }
+                              }
+                        }
+                       }
+                       /* create affinities */
+                       for (std::vector<Temp>& vec : phi_resources) {
+                             for (unsigned i = 1; i < vec.size(); i++)
+                                   if (vec[i].id() != vec[0].id())
+                                         ctx.assignments[vec[i].id()].affinity = vec[0].id();
+                       }
+            }
 
-   unsigned im_mask = instr->isDPP16() && instr->isVOP3() ? 0x3 : 0;
-   if (instr->valu().omod || instr->valu().clamp || (instr->valu().abs & ~im_mask) ||
-       (instr->valu().neg & ~im_mask))
-      return false;
-
-   return true;
-}
-
-bool
-sop2_can_use_sopk(ra_ctx& ctx, Instruction* instr)
-{
-   if (instr->opcode != aco_opcode::s_add_i32 && instr->opcode != aco_opcode::s_add_u32 &&
-       instr->opcode != aco_opcode::s_mul_i32 && instr->opcode != aco_opcode::s_cselect_b32)
-      return false;
-
-   if (instr->opcode == aco_opcode::s_add_u32 && !instr->definitions[1].isKill())
-      return false;
-
-   uint32_t literal_idx = 0;
-
-   if (instr->opcode != aco_opcode::s_cselect_b32 && instr->operands[1].isLiteral())
-      literal_idx = 1;
-
-   if (!instr->operands[!literal_idx].isTemp() || !instr->operands[!literal_idx].isKillBeforeDef())
-      return false;
-
-   if (!instr->operands[literal_idx].isLiteral())
-      return false;
-
-   const uint32_t i16_mask = 0xffff8000u;
-   uint32_t value = instr->operands[literal_idx].constantValue();
-   if ((value & i16_mask) && (value & i16_mask) != i16_mask)
-      return false;
-
-   return true;
-}
-
-void
-create_phi_vector_affinities(ra_ctx& ctx, aco_ptr<Instruction>& instr,
-                             std::map<Operand*, std::vector<vector_info>>& vector_phis)
-{
-   auto it = ctx.vectors.find(instr->definitions[0].tempId());
-   if (it == ctx.vectors.end())
-      return;
-   vector_info& dest_vector = it->second;
-
-   auto pair = vector_phis.try_emplace(dest_vector.parts, instr->operands.size(), dest_vector);
-   std::vector<vector_info>& src_vectors = pair.first->second;
-   if (pair.second) {
-      RegType type = instr->definitions[0].regClass().type();
-
-      for (vector_info& src_vector : src_vectors) {
-         src_vector.parts =
-            (Operand*)ctx.memory.allocate(sizeof(Operand) * src_vector.num_parts, alignof(Operand));
-         for (unsigned j = 0; j < src_vector.num_parts; j++)
-            src_vector.parts[j] = Operand(RegClass::get(type, dest_vector.parts[j].bytes()));
-      }
-   }
+            void
+            optimize_encoding_vop2(ra_ctx& ctx, RegisterFile& register_file, aco_ptr<Instruction>& instr)
+            {
+                  if (!vop3_can_use_vop2acc(ctx, instr.get()))
+                        return;
+
+                  for (unsigned i = ctx.program->gfx_level < GFX11 ? 0 : 2; i < 3; i++) {
+                        if (instr->operands[i].physReg().byte())
+                              return;
+                  }
+
+                  unsigned def_id = instr->definitions[0].tempId();
+                  if (ctx.assignments[def_id].affinity) {
+                        assignment& affinity = ctx.assignments[ctx.assignments[def_id].affinity];
+                        if (affinity.assigned && affinity.reg != instr->operands[2].physReg() &&
+                              (!register_file.test(affinity.reg, instr->operands[2].bytes()) ||
+                              std::any_of(instr->operands.begin(), instr->operands.end(), [&](Operand op)
+                              { return op.isKillBeforeDef() && op.physReg() == affinity.reg; })))
+                              return;
+                  }
+
+                  if (!instr->operands[1].isOfType(RegType::vgpr))
+                        instr->valu().swapOperands(0, 1);
+
+                  if (instr->isVOP3P() && instr->operands[0].isLiteral()) {
+                        unsigned literal = instr->operands[0].constantValue();
+                        unsigned lo = (literal >> (instr->valu().opsel_lo[0] * 16)) & 0xffff;
+                        unsigned hi = (literal >> (instr->valu().opsel_hi[0] * 16)) & 0xffff;
+                        instr->operands[0] = Operand::literal32(lo | (hi << 16));
+                  }
+
+                  instr->format = (Format)(((unsigned)withoutVOP3(instr->format) & ~(unsigned)Format::VOP3P) |
+                  (unsigned)Format::VOP2);
+                  instr->valu().opsel_lo = 0;
+                  instr->valu().opsel_hi = 0;
+                  switch (instr->opcode) {
+                        case aco_opcode::v_mad_f32: instr->opcode = aco_opcode::v_mac_f32; break;
+                        case aco_opcode::v_fma_f32: instr->opcode = aco_opcode::v_fmac_f32; break;
+                        case aco_opcode::v_mad_f16:
+                        case aco_opcode::v_mad_legacy_f16: instr->opcode = aco_opcode::v_mac_f16; break;
+                        case aco_opcode::v_fma_f16: instr->opcode = aco_opcode::v_fmac_f16; break;
+                        case aco_opcode::v_pk_fma_f16: instr->opcode = aco_opcode::v_pk_fmac_f16; break;
+                        case aco_opcode::v_dot4_i32_i8: instr->opcode = aco_opcode::v_dot4c_i32_i8; break;
+                        case aco_opcode::v_mad_legacy_f32: instr->opcode = aco_opcode::v_mac_legacy_f32; break;
+                        case aco_opcode::v_fma_legacy_f32: instr->opcode = aco_opcode::v_fmac_legacy_f32; break;
+                        default: break;
+                  }
+            }
 
-   unsigned index = 0;
-   for (; index < dest_vector.num_parts; index++) {
-      if (dest_vector.parts[index].isTemp() &&
-          dest_vector.parts[index].tempId() == instr->definitions[0].tempId())
-         break;
-   }
-   assert(index != dest_vector.num_parts);
-
-   for (int i = instr->operands.size() - 1; i >= 0; i--) {
-      const Operand& op = instr->operands[i];
-      if (!op.isTemp() || op.regClass() != instr->definitions[0].regClass())
-         continue;
-
-      src_vectors[i].parts[index] = op;
-      ctx.vectors[op.tempId()] = src_vectors[i];
-   }
-}
-
-void
-get_affinities(ra_ctx& ctx)
-{
-   std::vector<std::vector<Temp>> phi_resources;
-   aco::unordered_map<uint32_t, uint32_t> temp_to_phi_resources(ctx.memory);
-
-   for (auto block_rit = ctx.program->blocks.rbegin(); block_rit != ctx.program->blocks.rend();
-        block_rit++) {
-      Block& block = *block_rit;
-
-      std::vector<aco_ptr<Instruction>>::reverse_iterator rit;
-      for (rit = block.instructions.rbegin(); rit != block.instructions.rend(); ++rit) {
-         aco_ptr<Instruction>& instr = *rit;
-         if (is_phi(instr))
-            break;
-
-         /* add vector affinities */
-         if (instr->opcode == aco_opcode::p_create_vector) {
-            for (const Operand& op : instr->operands) {
-               if (op.isTemp() && op.isFirstKill() &&
-                   op.getTemp().type() == instr->definitions[0].getTemp().type())
-                  ctx.vectors[op.tempId()] = vector_info(instr.get());
-            }
-         } else if (instr->format == Format::MIMG && instr->operands.size() > 4 &&
-                    !instr->mimg().strict_wqm) {
-
-            bool is_vector = false;
-            for (unsigned i = 3, vector_begin = 3; i < instr->operands.size(); i++) {
-               if (is_vector || instr->operands[i].isVectorAligned())
-                  ctx.vectors[instr->operands[i].tempId()] = vector_info(instr.get(), vector_begin);
-               else if (ctx.program->gfx_level < GFX12 && !instr->operands[3].isVectorAligned())
-                  ctx.vectors[instr->operands[i].tempId()] = vector_info(instr.get(), 3, true);
-               is_vector = instr->operands[i].isVectorAligned();
-               vector_begin = is_vector ? vector_begin : i + 1;
-            }
-         } else if (instr->opcode == aco_opcode::p_split_vector &&
-                    instr->operands[0].isFirstKillBeforeDef()) {
-            ctx.split_vectors[instr->operands[0].tempId()] = instr.get();
-         } else if (instr->isVOPC() && !instr->isVOP3()) {
-            if (!instr->isSDWA() || ctx.program->gfx_level == GFX8)
-               ctx.assignments[instr->definitions[0].tempId()].vcc = true;
-         } else if (instr->isVOP2() && !instr->isVOP3()) {
-            if (instr->operands.size() == 3 && instr->operands[2].isTemp() &&
-                instr->operands[2].regClass().type() == RegType::sgpr)
-               ctx.assignments[instr->operands[2].tempId()].vcc = true;
-            if (instr->definitions.size() == 2)
-               ctx.assignments[instr->definitions[1].tempId()].vcc = true;
-         } else if (instr->opcode == aco_opcode::s_and_b32 ||
-                    instr->opcode == aco_opcode::s_and_b64) {
-            /* If SCC is used by a branch, we might be able to use
-             * s_cbranch_vccz/s_cbranch_vccnz if the operand is VCC.
-             */
-            if (!instr->definitions[1].isKill() && instr->operands[0].isTemp() &&
-                instr->operands[1].isFixed() && instr->operands[1].physReg() == exec)
-               ctx.assignments[instr->operands[0].tempId()].vcc = true;
-         } else if (instr->opcode == aco_opcode::s_sendmsg) {
-            ctx.assignments[instr->operands[0].tempId()].m0 = true;
-         }
-
-         auto tied_defs = get_tied_defs(instr.get());
-         for (unsigned i = 0; i < instr->definitions.size(); i++) {
-            const Definition& def = instr->definitions[i];
-            if (!def.isTemp())
-               continue;
-            /* mark last-seen phi operand */
-            auto it = temp_to_phi_resources.find(def.tempId());
-            if (it != temp_to_phi_resources.end() &&
-                def.regClass() == phi_resources[it->second][0].regClass()) {
-               phi_resources[it->second][0] = def.getTemp();
-               /* try to coalesce phi affinities with parallelcopies */
-               Operand op;
-               if (instr->opcode == aco_opcode::p_parallelcopy) {
-                  op = instr->operands[i];
-               } else if (i < tied_defs.size()) {
-                  op = instr->operands[tied_defs[i]];
-               } else if (vop3_can_use_vop2acc(ctx, instr.get())) {
-                  op = instr->operands[2];
-               } else if (i == 0 && sop2_can_use_sopk(ctx, instr.get())) {
-                  op = instr->operands[instr->operands[0].isLiteral()];
-               } else {
-                  continue;
-               }
-
-               if (op.isTemp() && op.isFirstKillBeforeDef() && def.regClass() == op.regClass()) {
-                  phi_resources[it->second].emplace_back(op.getTemp());
-                  temp_to_phi_resources[op.tempId()] = it->second;
-               }
+            void
+            optimize_encoding_sopk(ra_ctx& ctx, RegisterFile& register_file, aco_ptr<Instruction>& instr)
+            {
+                  /* try to optimize sop2 with literal source to sopk */
+                  if (!sop2_can_use_sopk(ctx, instr.get()))
+                        return;
+                  unsigned literal_idx = instr->operands[1].isLiteral();
+
+                  PhysReg op_reg = instr->operands[!literal_idx].physReg();
+                  if (!is_sgpr_writable_without_side_effects(ctx.program->gfx_level, op_reg))
+                        return;
+
+                  unsigned def_id = instr->definitions[0].tempId();
+                  if (ctx.assignments[def_id].affinity) {
+                        assignment& affinity = ctx.assignments[ctx.assignments[def_id].affinity];
+                        if (affinity.assigned && affinity.reg != instr->operands[!literal_idx].physReg() &&
+                              (!register_file.test(affinity.reg, instr->operands[!literal_idx].bytes()) ||
+                              std::any_of(instr->operands.begin(), instr->operands.end(), [&](Operand op)
+                              { return op.isKillBeforeDef() && op.physReg() == affinity.reg; })))
+                              return;
+                  }
+
+                  instr->format = Format::SOPK;
+                  instr->salu().imm = instr->operands[literal_idx].constantValue() & 0xffff;
+                  if (literal_idx == 0)
+                        std::swap(instr->operands[0], instr->operands[1]);
+                  if (instr->operands.size() > 2)
+                        std::swap(instr->operands[1], instr->operands[2]);
+                  instr->operands.pop_back();
+
+                  switch (instr->opcode) {
+                        case aco_opcode::s_add_u32:
+                        case aco_opcode::s_add_i32: instr->opcode = aco_opcode::s_addk_i32; break;
+                        case aco_opcode::s_mul_i32: instr->opcode = aco_opcode::s_mulk_i32; break;
+                        case aco_opcode::s_cselect_b32: instr->opcode = aco_opcode::s_cmovk_i32; break;
+                        default: unreachable("illegal instruction");
+                  }
             }
-         }
-      }
 
-      /* collect phi affinities */
-      std::map<Operand*, std::vector<vector_info>> vector_phis;
-      for (; rit != block.instructions.rend(); ++rit) {
-         aco_ptr<Instruction>& instr = *rit;
-         assert(is_phi(instr));
-
-         if (instr->definitions[0].isKill() || instr->definitions[0].isFixed())
-            continue;
-
-         assert(instr->definitions[0].isTemp());
-         auto it = temp_to_phi_resources.find(instr->definitions[0].tempId());
-         unsigned index = phi_resources.size();
-         std::vector<Temp>* affinity_related;
-         if (it != temp_to_phi_resources.end()) {
-            index = it->second;
-            phi_resources[index][0] = instr->definitions[0].getTemp();
-            affinity_related = &phi_resources[index];
-         } else {
-            phi_resources.emplace_back(std::vector<Temp>{instr->definitions[0].getTemp()});
-            affinity_related = &phi_resources.back();
-         }
-
-         for (const Operand& op : instr->operands) {
-            if (op.isTemp() && op.isKill() && op.regClass() == instr->definitions[0].regClass()) {
-               affinity_related->emplace_back(op.getTemp());
-               if (block.kind & block_kind_loop_header)
-                  continue;
-               temp_to_phi_resources[op.tempId()] = index;
+            void
+            optimize_encoding(ra_ctx& ctx, RegisterFile& register_file, aco_ptr<Instruction>& instr)
+            {
+                  if (instr->isVALU())
+                        optimize_encoding_vop2(ctx, register_file, instr);
+                  if (instr->isSALU())
+                        optimize_encoding_sopk(ctx, register_file, instr);
             }
-         }
 
-         create_phi_vector_affinities(ctx, instr, vector_phis);
-      }
+            void
+            undo_renames(ra_ctx& ctx, std::vector<parallelcopy>& parallelcopies,
+                         aco_ptr<Instruction>& instr)
+            {
+                  /* Undo renaming if possible in order to reduce latency.
+                   *
+                   * This can also remove a use of a SCC->SGPR copy, which can then be removed completely if the
+                   * post-RA optimizer eliminates the copy by duplicating the instruction that produced the SCC */
+                  for (parallelcopy copy : parallelcopies) {
+                        bool first[2] = {true, true};
+                        for (unsigned i = 0; i < instr->operands.size(); i++) {
+                              Operand& op = instr->operands[i];
+                              if (!op.isTemp() || op.getTemp() != copy.def.getTemp()) {
+                                    first[1] &= !op.isTemp() || op.getTemp() != copy.op.getTemp();
+                                    continue;
+                              }
+
+                              bool use_original = !op.isPrecolored() && !op.isLateKill();
+                              use_original &= operand_can_use_reg(ctx.program->gfx_level, instr, i, copy.op.physReg(),
+                                                                  copy.op.regClass());
+
+                              if (use_original) {
+                                    const PhysRegInterval copy_reg = {copy.op.physReg(), copy.op.size()};
+                                    for (parallelcopy& pc : parallelcopies) {
+                                          const PhysRegInterval def_reg = {pc.def.physReg(), pc.def.size()};
+                                          use_original &= !intersects(def_reg, copy_reg);
+                                    }
+                              }
+
+                              /* Avoid unrenaming killed operands because it can increase the cost of instructions like
+                               * p_create_vector. */
+                              use_original &= !op.isKillBeforeDef();
+
+                              if (first[use_original])
+                                    op.setFirstKill(use_original || op.isKill());
+                              else
+                                    op.setKill(use_original || op.isKill());
+                              first[use_original] = false;
+
+                              if (use_original) {
+                                    op.setTemp(copy.op.getTemp());
+                                    op.setFixed(copy.op.physReg());
+                              }
+                        }
+                  }
+            }
 
-      /* visit the loop header phis first in order to create nested affinities */
-      if (block.kind & block_kind_loop_exit) {
-         /* find loop header */
-         auto header_rit = block_rit;
-         while ((header_rit + 1)->loop_nest_depth > block.loop_nest_depth)
-            header_rit++;
-
-         for (aco_ptr<Instruction>& phi : header_rit->instructions) {
-            if (!is_phi(phi))
-               break;
-            if (phi->definitions[0].isKill() || phi->definitions[0].isFixed())
-               continue;
-
-            /* create an (empty) merge-set for the phi-related variables */
-            auto it = temp_to_phi_resources.find(phi->definitions[0].tempId());
-            unsigned index = phi_resources.size();
-            if (it == temp_to_phi_resources.end()) {
-               temp_to_phi_resources[phi->definitions[0].tempId()] = index;
-               phi_resources.emplace_back(std::vector<Temp>{phi->definitions[0].getTemp()});
-            } else {
-               index = it->second;
-            }
-            for (unsigned i = 1; i < phi->operands.size(); i++) {
-               const Operand& op = phi->operands[i];
-               if (op.isTemp() && op.isKill() && op.regClass() == phi->definitions[0].regClass()) {
-                  temp_to_phi_resources[op.tempId()] = index;
-               }
+            void
+            handle_operands_tied_to_definitions(ra_ctx& ctx, std::vector<parallelcopy>& parallelcopies,
+                                                aco_ptr<Instruction>& instr, RegisterFile& reg_file,
+                                                aco::small_vec<uint32_t, 2> tied_defs)
+            {
+                  for (uint32_t op_idx : tied_defs) {
+                        Operand& op = instr->operands[op_idx];
+                        assert((!op.isLateKill() || op.isCopyKill()) && !op.isPrecolored());
+
+                        /* If the operand is copy-kill, then there's an earlier operand which is also tied to a
+                         * definition but uses the same temporary as this one.
+                         *
+                         * We also need to copy if there is different definition which is precolored and intersects
+                         * with this operand, but we don't bother since it shouldn't happen.
+                         */
+                        if (!op.isKill() || op.isCopyKill()) {
+                              PhysReg reg = get_reg(ctx, reg_file, op.getTemp(), parallelcopies, instr, op_idx);
+
+                              /* update_renames() in case we moved this operand. */
+                              update_renames(ctx, reg_file, parallelcopies, instr);
+
+                              Operand pc_op(op.getTemp());
+                              pc_op.setFixed(ctx.assignments[op.tempId()].reg);
+                              Definition pc_def(reg, op.regClass());
+                              parallelcopies.emplace_back(pc_op, pc_def, op_idx);
+
+                              update_renames(ctx, reg_file, parallelcopies, instr);
+                        }
+
+                        /* Flag the operand's temporary as lateKill. This serves as placeholder
+                         * for the tied definition until the instruction is fully handled.
+                         */
+                        for (Operand& other_op : instr->operands) {
+                              if (other_op.isTemp() && other_op.getTemp() == op.getTemp())
+                                    other_op.setLateKill(true);
+                        }
+                  }
             }
-         }
-      }
-   }
-   /* create affinities */
-   for (std::vector<Temp>& vec : phi_resources) {
-      for (unsigned i = 1; i < vec.size(); i++)
-         if (vec[i].id() != vec[0].id())
-            ctx.assignments[vec[i].id()].affinity = vec[0].id();
-   }
-}
-
-void
-optimize_encoding_vop2(ra_ctx& ctx, RegisterFile& register_file, aco_ptr<Instruction>& instr)
-{
-   if (!vop3_can_use_vop2acc(ctx, instr.get()))
-      return;
-
-   for (unsigned i = ctx.program->gfx_level < GFX11 ? 0 : 2; i < 3; i++) {
-      if (instr->operands[i].physReg().byte())
-         return;
-   }
-
-   unsigned def_id = instr->definitions[0].tempId();
-   if (ctx.assignments[def_id].affinity) {
-      assignment& affinity = ctx.assignments[ctx.assignments[def_id].affinity];
-      if (affinity.assigned && affinity.reg != instr->operands[2].physReg() &&
-          (!register_file.test(affinity.reg, instr->operands[2].bytes()) ||
-           std::any_of(instr->operands.begin(), instr->operands.end(), [&](Operand op)
-                       { return op.isKillBeforeDef() && op.physReg() == affinity.reg; })))
-         return;
-   }
-
-   if (!instr->operands[1].isOfType(RegType::vgpr))
-      instr->valu().swapOperands(0, 1);
-
-   if (instr->isVOP3P() && instr->operands[0].isLiteral()) {
-      unsigned literal = instr->operands[0].constantValue();
-      unsigned lo = (literal >> (instr->valu().opsel_lo[0] * 16)) & 0xffff;
-      unsigned hi = (literal >> (instr->valu().opsel_hi[0] * 16)) & 0xffff;
-      instr->operands[0] = Operand::literal32(lo | (hi << 16));
-   }
-
-   instr->format = (Format)(((unsigned)withoutVOP3(instr->format) & ~(unsigned)Format::VOP3P) |
-                            (unsigned)Format::VOP2);
-   instr->valu().opsel_lo = 0;
-   instr->valu().opsel_hi = 0;
-   switch (instr->opcode) {
-   case aco_opcode::v_mad_f32: instr->opcode = aco_opcode::v_mac_f32; break;
-   case aco_opcode::v_fma_f32: instr->opcode = aco_opcode::v_fmac_f32; break;
-   case aco_opcode::v_mad_f16:
-   case aco_opcode::v_mad_legacy_f16: instr->opcode = aco_opcode::v_mac_f16; break;
-   case aco_opcode::v_fma_f16: instr->opcode = aco_opcode::v_fmac_f16; break;
-   case aco_opcode::v_pk_fma_f16: instr->opcode = aco_opcode::v_pk_fmac_f16; break;
-   case aco_opcode::v_dot4_i32_i8: instr->opcode = aco_opcode::v_dot4c_i32_i8; break;
-   case aco_opcode::v_mad_legacy_f32: instr->opcode = aco_opcode::v_mac_legacy_f32; break;
-   case aco_opcode::v_fma_legacy_f32: instr->opcode = aco_opcode::v_fmac_legacy_f32; break;
-   default: break;
-   }
-}
-
-void
-optimize_encoding_sopk(ra_ctx& ctx, RegisterFile& register_file, aco_ptr<Instruction>& instr)
-{
-   /* try to optimize sop2 with literal source to sopk */
-   if (!sop2_can_use_sopk(ctx, instr.get()))
-      return;
-   unsigned literal_idx = instr->operands[1].isLiteral();
-
-   PhysReg op_reg = instr->operands[!literal_idx].physReg();
-   if (!is_sgpr_writable_without_side_effects(ctx.program->gfx_level, op_reg))
-      return;
-
-   unsigned def_id = instr->definitions[0].tempId();
-   if (ctx.assignments[def_id].affinity) {
-      assignment& affinity = ctx.assignments[ctx.assignments[def_id].affinity];
-      if (affinity.assigned && affinity.reg != instr->operands[!literal_idx].physReg() &&
-          (!register_file.test(affinity.reg, instr->operands[!literal_idx].bytes()) ||
-           std::any_of(instr->operands.begin(), instr->operands.end(), [&](Operand op)
-                       { return op.isKillBeforeDef() && op.physReg() == affinity.reg; })))
-         return;
-   }
-
-   instr->format = Format::SOPK;
-   instr->salu().imm = instr->operands[literal_idx].constantValue() & 0xffff;
-   if (literal_idx == 0)
-      std::swap(instr->operands[0], instr->operands[1]);
-   if (instr->operands.size() > 2)
-      std::swap(instr->operands[1], instr->operands[2]);
-   instr->operands.pop_back();
-
-   switch (instr->opcode) {
-   case aco_opcode::s_add_u32:
-   case aco_opcode::s_add_i32: instr->opcode = aco_opcode::s_addk_i32; break;
-   case aco_opcode::s_mul_i32: instr->opcode = aco_opcode::s_mulk_i32; break;
-   case aco_opcode::s_cselect_b32: instr->opcode = aco_opcode::s_cmovk_i32; break;
-   default: unreachable("illegal instruction");
-   }
-}
-
-void
-optimize_encoding(ra_ctx& ctx, RegisterFile& register_file, aco_ptr<Instruction>& instr)
-{
-   if (instr->isVALU())
-      optimize_encoding_vop2(ctx, register_file, instr);
-   if (instr->isSALU())
-      optimize_encoding_sopk(ctx, register_file, instr);
-}
-
-void
-undo_renames(ra_ctx& ctx, std::vector<parallelcopy>& parallelcopies,
-             aco_ptr<Instruction>& instr)
-{
-   /* Undo renaming if possible in order to reduce latency.
-    *
-    * This can also remove a use of a SCC->SGPR copy, which can then be removed completely if the
-    * post-RA optimizer eliminates the copy by duplicating the instruction that produced the SCC */
-   for (parallelcopy copy : parallelcopies) {
-      bool first[2] = {true, true};
-      for (unsigned i = 0; i < instr->operands.size(); i++) {
-         Operand& op = instr->operands[i];
-         if (!op.isTemp() || op.getTemp() != copy.def.getTemp()) {
-            first[1] &= !op.isTemp() || op.getTemp() != copy.op.getTemp();
-            continue;
-         }
-
-         bool use_original = !op.isPrecolored() && !op.isLateKill();
-         use_original &= operand_can_use_reg(ctx.program->gfx_level, instr, i, copy.op.physReg(),
-                                             copy.op.regClass());
-
-         if (use_original) {
-            const PhysRegInterval copy_reg = {copy.op.physReg(), copy.op.size()};
-            for (parallelcopy& pc : parallelcopies) {
-               const PhysRegInterval def_reg = {pc.def.physReg(), pc.def.size()};
-               use_original &= !intersects(def_reg, copy_reg);
-            }
-         }
-
-         /* Avoid unrenaming killed operands because it can increase the cost of instructions like
-          * p_create_vector. */
-         use_original &= !op.isKillBeforeDef();
-
-         if (first[use_original])
-            op.setFirstKill(use_original || op.isKill());
-         else
-            op.setKill(use_original || op.isKill());
-         first[use_original] = false;
-
-         if (use_original) {
-            op.setTemp(copy.op.getTemp());
-            op.setFixed(copy.op.physReg());
-         }
-      }
-   }
-}
 
-void
-handle_operands_tied_to_definitions(ra_ctx& ctx, std::vector<parallelcopy>& parallelcopies,
-                                    aco_ptr<Instruction>& instr, RegisterFile& reg_file,
+            void
+            assign_tied_definitions(ra_ctx& ctx, aco_ptr<Instruction>& instr, RegisterFile& reg_file,
                                     aco::small_vec<uint32_t, 2> tied_defs)
-{
-   for (uint32_t op_idx : tied_defs) {
-      Operand& op = instr->operands[op_idx];
-      assert((!op.isLateKill() || op.isCopyKill()) && !op.isPrecolored());
-
-      /* If the operand is copy-kill, then there's an earlier operand which is also tied to a
-       * definition but uses the same temporary as this one.
-       *
-       * We also need to copy if there is different definition which is precolored and intersects
-       * with this operand, but we don't bother since it shouldn't happen.
-       */
-      if (!op.isKill() || op.isCopyKill()) {
-         PhysReg reg = get_reg(ctx, reg_file, op.getTemp(), parallelcopies, instr, op_idx);
-
-         /* update_renames() in case we moved this operand. */
-         update_renames(ctx, reg_file, parallelcopies, instr);
-
-         Operand pc_op(op.getTemp());
-         pc_op.setFixed(ctx.assignments[op.tempId()].reg);
-         Definition pc_def(reg, op.regClass());
-         parallelcopies.emplace_back(pc_op, pc_def, op_idx);
+            {
+                  unsigned fixed_def_idx = 0;
+                  for (auto op_idx : tied_defs) {
+                        Definition& def = instr->definitions[fixed_def_idx++];
+                        Operand& op = instr->operands[op_idx];
+                        assert(op.isKill());
+                        assert(def.regClass().type() == op.regClass().type() && def.size() <= op.size());
+
+                        def.setFixed(op.physReg());
+                        ctx.assignments[def.tempId()].set(def);
+                        reg_file.clear(op);
+                        reg_file.fill(def);
+
+                        for (Operand& other_op : instr->operands) {
+                              if (other_op.isTemp() && other_op.getTemp() == op.getTemp())
+                                    other_op.setLateKill(false);
+                        }
+                  }
+            }
 
-         update_renames(ctx, reg_file, parallelcopies, instr);
-      }
+            void
+            emit_parallel_copy_internal(ra_ctx& ctx, std::vector<parallelcopy>& parallelcopy,
+                                        aco_ptr<Instruction>& instr,
+                                        std::vector<aco_ptr<Instruction>>& instructions, bool temp_in_scc,
+                                        RegisterFile& register_file)
+            {
+                  if (parallelcopy.empty())
+                        return;
+
+                  aco_ptr<Instruction> pc;
+                  pc.reset(create_instruction(aco_opcode::p_parallelcopy, Format::PSEUDO, parallelcopy.size(),
+                                              parallelcopy.size()));
+                  bool linear_vgpr = false;
+                  bool may_swap_sgprs = false;
+                  std::bitset<256> sgpr_operands;
+                  for (unsigned i = 0; i < parallelcopy.size(); i++) {
+                        linear_vgpr |= parallelcopy[i].op.regClass().is_linear_vgpr();
+
+                        if (!may_swap_sgprs && parallelcopy[i].op.isTemp() &&
+                              parallelcopy[i].op.getTemp().type() == RegType::sgpr) {
+                              unsigned op_reg = parallelcopy[i].op.physReg().reg();
+                        unsigned def_reg = parallelcopy[i].def.physReg().reg();
+                        for (unsigned j = 0; j < parallelcopy[i].op.size(); j++) {
+                              sgpr_operands.set(op_reg + j);
+                              if (sgpr_operands.test(def_reg + j))
+                                    may_swap_sgprs = true;
+                        }
+                              }
+
+                              pc->operands[i] = parallelcopy[i].op;
+                              pc->definitions[i] = parallelcopy[i].def;
+                              assert(pc->operands[i].size() == pc->definitions[i].size());
+
+                              if (parallelcopy[i].copy_kill < 0) {
+                                    /* it might happen that the operand is already renamed. we have to restore the
+                                     * original name. */
+                                    auto it =
+                                    ctx.orig_names.find(pc->operands[i].tempId());
+                                    Temp orig = it != ctx.orig_names.end() ? it->second : pc->operands[i].getTemp();
+                                    add_rename(
+                                          ctx, orig, pc->definitions[i].getTemp());
+                              }
+                  }
+
+                  if (temp_in_scc && (may_swap_sgprs || linear_vgpr)) {
+                        /* disable definitions and re-enable operands */
+                        RegisterFile tmp_file(register_file);
+                        for (const Definition& def : instr->definitions) {
+                              if (def.isTemp() && !def.isKill())
+                                    tmp_file.clear(def);
+                        }
+                        for (const Operand& op : instr->operands) {
+                              if (op.isTemp() && op.isFirstKill())
+                                    tmp_file.block(op.physReg(), op.regClass());
+                        }
+
+                        handle_pseudo(ctx, tmp_file, pc.get());
+                  } else {
+                        pc->pseudo().needs_scratch_reg = may_swap_sgprs || linear_vgpr;
+                        pc->pseudo().scratch_sgpr = scc;
+                  }
 
-      /* Flag the operand's temporary as lateKill. This serves as placeholder
-       * for the tied definition until the instruction is fully handled.
-       */
-      for (Operand& other_op : instr->operands) {
-         if (other_op.isTemp() && other_op.getTemp() == op.getTemp())
-            other_op.setLateKill(true);
-      }
-   }
-}
+                  instructions.emplace_back(std::move(pc));
 
-void
-assign_tied_definitions(ra_ctx& ctx, aco_ptr<Instruction>& instr, RegisterFile& reg_file,
-                        aco::small_vec<uint32_t, 2> tied_defs)
-{
-   unsigned fixed_def_idx = 0;
-   for (auto op_idx : tied_defs) {
-      Definition& def = instr->definitions[fixed_def_idx++];
-      Operand& op = instr->operands[op_idx];
-      assert(op.isKill());
-      assert(def.regClass().type() == op.regClass().type() && def.size() <= op.size());
-
-      def.setFixed(op.physReg());
-      ctx.assignments[def.tempId()].set(def);
-      reg_file.clear(op);
-      reg_file.fill(def);
-
-      for (Operand& other_op : instr->operands) {
-         if (other_op.isTemp() && other_op.getTemp() == op.getTemp())
-            other_op.setLateKill(false);
-      }
-   }
-}
+                  parallelcopy.clear();
+            }
 
-void
-emit_parallel_copy_internal(ra_ctx& ctx, std::vector<parallelcopy>& parallelcopy,
-                            aco_ptr<Instruction>& instr,
-                            std::vector<aco_ptr<Instruction>>& instructions, bool temp_in_scc,
-                            RegisterFile& register_file)
-{
-   if (parallelcopy.empty())
-      return;
-
-   aco_ptr<Instruction> pc;
-   pc.reset(create_instruction(aco_opcode::p_parallelcopy, Format::PSEUDO, parallelcopy.size(),
-                               parallelcopy.size()));
-   bool linear_vgpr = false;
-   bool may_swap_sgprs = false;
-   std::bitset<256> sgpr_operands;
-   for (unsigned i = 0; i < parallelcopy.size(); i++) {
-      linear_vgpr |= parallelcopy[i].op.regClass().is_linear_vgpr();
-
-      if (!may_swap_sgprs && parallelcopy[i].op.isTemp() &&
-          parallelcopy[i].op.getTemp().type() == RegType::sgpr) {
-         unsigned op_reg = parallelcopy[i].op.physReg().reg();
-         unsigned def_reg = parallelcopy[i].def.physReg().reg();
-         for (unsigned j = 0; j < parallelcopy[i].op.size(); j++) {
-            sgpr_operands.set(op_reg + j);
-            if (sgpr_operands.test(def_reg + j))
-               may_swap_sgprs = true;
-         }
-      }
+            void
+            emit_parallel_copy(ra_ctx& ctx, std::vector<parallelcopy>& copies,
+                               aco_ptr<Instruction>& instr, std::vector<aco_ptr<Instruction>>& instructions,
+                               bool temp_in_scc, RegisterFile& register_file)
+            {
+                  if (copies.empty())
+                        return;
+
+                  std::vector<parallelcopy> linear_vgpr;
+                  if (ctx.num_linear_vgprs) {
+                        auto next = copies.begin();
+                        for (auto it = copies.begin(); it != copies.end(); ++it) {
+                              if (it->op.regClass().is_linear_vgpr()) {
+                                    linear_vgpr.push_back(*it);
+                                    continue;
+                              }
+
+                              if (next != it)
+                                    *next = *it;
+                              ++next;
+                        }
+                        copies.erase(next, copies.end());
+                  }
+
+                  /* Because of how linear VGPRs are allocated, we should never have to move a linear VGPR into the
+                   * space of a normal one. This means the copy can be done entirely before normal VGPR copies. */
+                  emit_parallel_copy_internal(ctx, linear_vgpr, instr, instructions, temp_in_scc,
+                                              register_file);
+                  emit_parallel_copy_internal(ctx, copies, instr, instructions, temp_in_scc,
+                                              register_file);
+            }
 
-      pc->operands[i] = parallelcopy[i].op;
-      pc->definitions[i] = parallelcopy[i].def;
-      assert(pc->operands[i].size() == pc->definitions[i].size());
-
-      if (parallelcopy[i].copy_kill < 0) {
-         /* it might happen that the operand is already renamed. we have to restore the
-          * original name. */
-         auto it =
-            ctx.orig_names.find(pc->operands[i].tempId());
-         Temp orig = it != ctx.orig_names.end() ? it->second : pc->operands[i].getTemp();
-         add_rename(
-         ctx, orig, pc->definitions[i].getTemp());
-      }
-   }
+      } /* end namespace */
 
-   if (temp_in_scc && (may_swap_sgprs || linear_vgpr)) {
-      /* disable definitions and re-enable operands */
-      RegisterFile tmp_file(register_file);
-      for (const Definition& def : instr->definitions) {
-         if (def.isTemp() && !def.isKill())
-            tmp_file.clear(def);
-      }
-      for (const Operand& op : instr->operands) {
-         if (op.isTemp() && op.isFirstKill())
-            tmp_file.block(op.physReg(), op.regClass());
+      static inline void
+      set_sopp_imm(Instruction* instr, uint32_t imm)
+      {
+            static_cast<SALU_instruction*>(instr)->imm = imm; /* SOPP layout */
       }
 
-      handle_pseudo(ctx, tmp_file, pc.get());
-   } else {
-      pc->pseudo().needs_scratch_reg = may_swap_sgprs || linear_vgpr;
-      pc->pseudo().scratch_sgpr = scc;
-   }
-
-   instructions.emplace_back(std::move(pc));
-
-   parallelcopy.clear();
-}
-
-void
-emit_parallel_copy(ra_ctx& ctx, std::vector<parallelcopy>& copies,
-                   aco_ptr<Instruction>& instr, std::vector<aco_ptr<Instruction>>& instructions,
-                   bool temp_in_scc, RegisterFile& register_file)
-{
-   if (copies.empty())
-      return;
-
-   std::vector<parallelcopy> linear_vgpr;
-   if (ctx.num_linear_vgprs) {
-      auto next = copies.begin();
-      for (auto it = copies.begin(); it != copies.end(); ++it) {
-         if (it->op.regClass().is_linear_vgpr()) {
-            linear_vgpr.push_back(*it);
-            continue;
-         }
-
-         if (next != it)
-            *next = *it;
-         ++next;
-      }
-      copies.erase(next, copies.end());
-   }
+      void
+      register_allocation(Program* program, ra_test_policy policy)
+      {
+            ra_ctx ctx(program, policy);
+            get_affinities(ctx);
 
-   /* Because of how linear VGPRs are allocated, we should never have to move a linear VGPR into the
-    * space of a normal one. This means the copy can be done entirely before normal VGPR copies. */
-   emit_parallel_copy_internal(ctx, linear_vgpr, instr, instructions, temp_in_scc,
-                               register_file);
-   emit_parallel_copy_internal(ctx, copies, instr, instructions, temp_in_scc,
-                               register_file);
-}
-
-} /* end namespace */
-
-void
-register_allocation(Program* program, ra_test_policy policy)
-{
-   ra_ctx ctx(program, policy);
-   get_affinities(ctx);
-
-   for (Block& block : program->blocks) {
-      ctx.block = &block;
-
-      /* initialize register file */
-      RegisterFile register_file = init_reg_file(ctx, program->live.live_in, block);
-      ctx.war_hint.reset();
-      ctx.rr_vgpr_it = {PhysReg{256}};
-      ctx.rr_sgpr_it = {PhysReg{0}};
-
-      std::vector<aco_ptr<Instruction>> instructions;
-      instructions.reserve(block.instructions.size());
-
-      /* this is a slight adjustment from the paper as we already have phi nodes:
-       * We consider them incomplete phis and only handle the definition. */
-      get_regs_for_phis(ctx, block, register_file, instructions,
-                        program->live.live_in[block.index]);
-
-      /* Handle all other instructions of the block */
-      auto NonPhi = [](aco_ptr<Instruction>& instr) -> bool { return instr && !is_phi(instr); };
-      auto instr_it = std::find_if(block.instructions.begin(), block.instructions.end(), NonPhi);
-      for (; instr_it != block.instructions.end(); ++instr_it) {
-         aco_ptr<Instruction>& instr = *instr_it;
-         std::vector<parallelcopy> parallelcopy;
-         assert(!is_phi(instr));
-
-         /* handle operands */
-         bool fixed = false;
-         for (unsigned i = 0; i < instr->operands.size(); ++i) {
-            auto& operand = instr->operands[i];
-            if (!operand.isTemp())
-               continue;
-
-            /* rename operands */
-            operand.setTemp(read_variable(ctx, operand.getTemp(), block.index));
-            assert(ctx.assignments[operand.tempId()].assigned);
-
-            fixed |=
-               operand.isPrecolored() && ctx.assignments[operand.tempId()].reg != operand.physReg();
-
-            /* Avoid emitting parallelcopies for vector operands. handle_vector_operands() will
-             * temporarily place vector SSA-defs into the register file while handling this
-             * instruction. */
-            if (operand.isVectorAligned() || (i && instr->operands[i - 1].isVectorAligned()))
-               register_file.clear(
-                  Operand(operand.getTemp(), ctx.assignments[operand.tempId()].reg));
-         }
-
-         bool is_writelane = instr->opcode == aco_opcode::v_writelane_b32 ||
-                             instr->opcode == aco_opcode::v_writelane_b32_e64;
-         if (program->gfx_level <= GFX9 && is_writelane && instr->operands[0].isTemp() &&
-             instr->operands[1].isTemp()) {
-            /* v_writelane_b32 can take two sgprs but only if one is m0. */
-            if (ctx.assignments[instr->operands[0].tempId()].reg != m0 &&
-                ctx.assignments[instr->operands[1].tempId()].reg != m0) {
-               instr->operands[0].setPrecolored(m0);
-               fixed = true;
-            }
-         }
-
-         if (fixed)
-            handle_fixed_operands(ctx, register_file, parallelcopy, instr);
-
-         for (unsigned i = 0; i < instr->operands.size(); ++i) {
-            auto& operand = instr->operands[i];
-            if (!operand.isTemp() || operand.isFixed())
-               continue;
-
-            if (operand.isVectorAligned()) {
-               handle_vector_operands(ctx, register_file, parallelcopy, instr, i);
-               continue;
-            }
-
-            PhysReg reg = ctx.assignments[operand.tempId()].reg;
-            if (operand_can_use_reg(program->gfx_level, instr, i, reg, operand.regClass()))
-               operand.setFixed(reg);
-            else
-               get_reg_for_operand(ctx, register_file, parallelcopy, instr, operand, i);
-
-            if (instr->isEXP() || (instr->isVMEM() && i == 3 && ctx.program->gfx_level == GFX6) ||
-                (instr->isDS() && instr->ds().gds)) {
-               for (unsigned j = 0; j < operand.size(); j++)
-                  ctx.war_hint.set(operand.physReg().reg() + j);
-            }
-         }
-         bool temp_in_scc = register_file[scc];
-
-         optimize_encoding(ctx, register_file, instr);
-
-         auto tied_defs = get_tied_defs(instr.get());
-         handle_operands_tied_to_definitions(ctx, parallelcopy, instr, register_file, tied_defs);
-
-         /* remove dead vars from register file */
-         for (const Operand& op : instr->operands) {
-            if (op.isTemp() && op.isFirstKillBeforeDef())
-               register_file.clear(op);
-         }
-
-         /* handle fixed definitions first */
-         for (unsigned i = 0; i < instr->definitions.size(); ++i) {
-            auto& definition = instr->definitions[i];
-            if (!definition.isFixed())
-               continue;
-
-            assert(i >= tied_defs.size());
-
-            adjust_max_used_regs(ctx, definition.regClass(), definition.physReg());
-            /* check if the target register is blocked */
-            if (register_file.test(definition.physReg(), definition.bytes())) {
-               const PhysRegInterval def_regs{definition.physReg(), definition.size()};
-
-               /* create parallelcopy pair to move blocking vars */
-               std::vector<unsigned> vars = collect_vars(ctx, register_file, def_regs);
-
-               RegisterFile tmp_file(register_file);
-               /* re-enable the killed operands, so that we don't move the blocking vars there */
-               tmp_file.fill_killed_operands(instr.get());
-
-               ASSERTED bool success = false;
-               success = get_regs_for_copies(ctx, tmp_file, parallelcopy, vars, instr, def_regs);
-               assert(success);
-
-               update_renames(ctx, register_file, parallelcopy, instr);
-            }
-
-            if (!definition.isTemp())
-               continue;
-
-            ctx.assignments[definition.tempId()].set(definition);
-            register_file.fill(definition);
-         }
-
-         /* handle normal definitions */
-         for (unsigned i = 0; i < instr->definitions.size(); ++i) {
-            Definition* definition = &instr->definitions[i];
-
-            if (definition->isFixed() || !definition->isTemp() || i < tied_defs.size())
-               continue;
-
-            /* find free reg */
-            if (instr->opcode == aco_opcode::p_start_linear_vgpr) {
-               /* Allocation of linear VGPRs is special. */
-               definition->setFixed(alloc_linear_vgpr(ctx, register_file, instr, parallelcopy));
-               update_renames(ctx, register_file, parallelcopy, instr);
-            } else if (instr->opcode == aco_opcode::p_split_vector) {
-               PhysReg reg = instr->operands[0].physReg();
-               RegClass rc = definition->regClass();
-               for (unsigned j = 0; j < i; j++)
-                  reg.reg_b += instr->definitions[j].bytes();
-               if (get_reg_specified(ctx, register_file, rc, instr, reg, -1)) {
-                  definition->setFixed(reg);
-               } else if (i == 0) {
-                  RegClass vec_rc = RegClass::get(rc.type(), instr->operands[0].bytes());
-                  DefInfo info(ctx, ctx.pseudo_dummy, vec_rc, -1);
-                  std::optional<PhysReg> res = get_reg_simple(ctx, register_file, info);
-                  if (res && get_reg_specified(ctx, register_file, rc, instr, *res, -1))
-                     definition->setFixed(*res);
-               } else if (instr->definitions[i - 1].isFixed()) {
-                  reg = instr->definitions[i - 1].physReg();
-                  reg.reg_b += instr->definitions[i - 1].bytes();
-                  if (get_reg_specified(ctx, register_file, rc, instr, reg, -1))
-                     definition->setFixed(reg);
-               }
-            } else if (instr->opcode == aco_opcode::p_parallelcopy) {
-               PhysReg reg = instr->operands[i].physReg();
-               if (instr->operands[i].isTemp() &&
-                   instr->operands[i].getTemp().type() == definition->getTemp().type() &&
-                   !register_file.test(reg, definition->bytes()))
-                  definition->setFixed(reg);
-            } else if (instr->opcode == aco_opcode::p_extract_vector) {
-               PhysReg reg = instr->operands[0].physReg();
-               reg.reg_b += definition->bytes() * instr->operands[1].constantValue();
-               if (get_reg_specified(ctx, register_file, definition->regClass(), instr, reg, -1))
-                  definition->setFixed(reg);
-            } else if (instr->opcode == aco_opcode::p_create_vector) {
-               PhysReg reg = get_reg_create_vector(ctx, register_file, definition->getTemp(),
-                                                   parallelcopy, instr);
-               update_renames(ctx, register_file, parallelcopy, instr);
-               definition->setFixed(reg);
-            } else if (instr_info.classes[(int)instr->opcode] == instr_class::wmma &&
-                       instr->operands[2].isTemp() && instr->operands[2].isKill() &&
-                       instr->operands[2].regClass() == definition->regClass()) {
-               /* For WMMA, the dest needs to either be equal to operands[2], or not overlap it.
-                * Here we set a policy of forcing them the same if operands[2] gets killed (and
-                * otherwise they don't overlap). This may not be optimal if RA would select a
-                * different location due to affinity, but that gets complicated very quickly. */
-               definition->setFixed(instr->operands[2].physReg());
-            }
-
-            if (!definition->isFixed()) {
-               Temp tmp = definition->getTemp();
-               PhysReg reg = get_reg(ctx, register_file, tmp, parallelcopy, instr);
-               definition->setFixed(reg);
-               update_renames(ctx, register_file, parallelcopy, instr);
-               if (definition->regClass().is_subdword() && definition->bytes() < 4 &&
-                   (reg.byte() || register_file.test(reg, 4))) {
-                  bool allow_16bit_write = reg.byte() % 2 == 0 && !register_file.test(reg, 2);
-                  add_subdword_definition(program, instr, reg, allow_16bit_write);
-                  /* add_subdword_definition can invalidate the reference */
-                  definition = &instr->definitions[i];
-               }
-            }
-
-            assert(
-               definition->isFixed() &&
-               ((definition->getTemp().type() == RegType::vgpr && definition->physReg() >= 256) ||
-                (definition->getTemp().type() != RegType::vgpr && definition->physReg() < 256)));
-            ctx.assignments[definition->tempId()].set(*definition);
-            register_file.fill(*definition);
-         }
-
-         if (!ctx.vector_operands.empty())
-            resolve_vector_operands(ctx, register_file, parallelcopy, instr);
-
-         /* This ignores tied defs and vector aligned operands because they are late-kill. */
-         undo_renames(ctx, parallelcopy, instr);
-
-         assign_tied_definitions(ctx, instr, register_file, tied_defs);
-
-         handle_pseudo(ctx, register_file, instr.get());
-
-         /* kill definitions and late-kill operands and ensure that sub-dword operands can actually
-          * be read */
-         for (const Definition& def : instr->definitions) {
-            if (def.isTemp() && def.isKill())
-               register_file.clear(def);
-         }
-         for (unsigned i = 0; i < instr->operands.size(); i++) {
-            const Operand& op = instr->operands[i];
-            if (op.isTemp() && op.isFirstKill() && op.isLateKill())
-               register_file.clear(op);
-            if (op.isTemp() && op.physReg().byte() != 0)
-               add_subdword_operand(ctx, instr, i, op.physReg().byte(), op.regClass());
-         }
-
-         emit_parallel_copy(ctx, parallelcopy, instr, instructions, temp_in_scc, register_file);
-
-         /* some instructions need VOP3 encoding if operand/definition is not assigned to VCC */
-         bool instr_needs_vop3 =
-            !instr->isVOP3() &&
-            ((withoutDPP(instr->format) == Format::VOPC &&
-              instr->definitions[0].physReg() != vcc) ||
-             (instr->opcode == aco_opcode::v_cndmask_b32 && instr->operands[2].physReg() != vcc) ||
-             ((instr->opcode == aco_opcode::v_add_co_u32 ||
-               instr->opcode == aco_opcode::v_addc_co_u32 ||
-               instr->opcode == aco_opcode::v_sub_co_u32 ||
-               instr->opcode == aco_opcode::v_subb_co_u32 ||
-               instr->opcode == aco_opcode::v_subrev_co_u32 ||
-               instr->opcode == aco_opcode::v_subbrev_co_u32) &&
-              instr->definitions[1].physReg() != vcc) ||
-             ((instr->opcode == aco_opcode::v_addc_co_u32 ||
-               instr->opcode == aco_opcode::v_subb_co_u32 ||
-               instr->opcode == aco_opcode::v_subbrev_co_u32) &&
-              instr->operands[2].physReg() != vcc));
-         if (instr_needs_vop3) {
-
-            /* If the first operand is a literal, we have to move it to an sgpr
-             * for generations without VOP3+literal support.
-             * Both literals and sgprs count towards the constant bus limit,
-             * so this is always valid.
-             */
-            if (instr->operands.size() && instr->operands[0].isLiteral() &&
-                program->gfx_level < GFX10) {
-               /* Re-use the register we already allocated for the definition.
-                * This works because the instruction cannot have any other SGPR operand.
-                */
-               Temp tmp = program->allocateTmp(instr->operands[0].size() == 2 ? s2 : s1);
-               const Definition& def =
-                  instr->isVOPC() ? instr->definitions[0] : instr->definitions.back();
-               assert(def.regClass() == s2);
-               ctx.assignments.emplace_back(def.physReg(), tmp.regClass());
-
-               Instruction* copy =
-                  create_instruction(aco_opcode::p_parallelcopy, Format::PSEUDO, 1, 1);
-               copy->operands[0] = instr->operands[0];
-               if (copy->operands[0].bytes() < 4)
-                  copy->operands[0] = Operand::c32(copy->operands[0].constantValue());
-               copy->definitions[0] = Definition(tmp);
-               copy->definitions[0].setFixed(def.physReg());
-
-               instr->operands[0] = Operand(tmp);
-               instr->operands[0].setFixed(def.physReg());
-               instr->operands[0].setFirstKill(true);
-
-               instructions.emplace_back(copy);
-            }
-
-            /* change the instruction to VOP3 to enable an arbitrary register pair as dst */
-            instr->format = asVOP3(instr->format);
-         }
-
-         instructions.emplace_back(std::move(*instr_it));
-
-      } /* end for Instr */
-
-      if ((block.kind & block_kind_top_level) && block.linear_succs.empty()) {
-         /* Reset this for block_kind_resume. */
-         ctx.num_linear_vgprs = 0;
-
-         ASSERTED PhysRegInterval vgpr_bounds = get_reg_bounds(ctx, RegType::vgpr, false);
-         ASSERTED PhysRegInterval sgpr_bounds = get_reg_bounds(ctx, RegType::sgpr, false);
-         assert(register_file.count_zero(vgpr_bounds) == ctx.vgpr_bounds);
-         assert(register_file.count_zero(sgpr_bounds) == ctx.sgpr_bounds);
-      } else if (should_compact_linear_vgprs(ctx, register_file)) {
-         aco_ptr<Instruction> br = std::move(instructions.back());
-         instructions.pop_back();
-
-         bool temp_in_scc =
-            register_file[scc] || (!br->operands.empty() && br->operands[0].physReg() == scc);
-
-         std::vector<parallelcopy> parallelcopy;
-         compact_linear_vgprs(ctx, register_file, parallelcopy);
-         update_renames(ctx, register_file, parallelcopy, br);
-         emit_parallel_copy_internal(ctx, parallelcopy, br, instructions, temp_in_scc, register_file);
+            /* Pre-calculate GFX level flags */
+            const bool is_gfx9 = program->gfx_level == GFX9;
+            const bool is_pre_gfx10 = program->gfx_level < GFX10;
+            const bool is_gfx6 = program->gfx_level == GFX6;
+
+            /* Cache for assignment lookups - most temps are accessed multiple times */
+            constexpr size_t CACHE_SIZE = 16; // Power of 2 for fast modulo
+            struct AssignmentCache {
+                  struct Entry {
+                        uint32_t temp_id = UINT32_MAX;
+                        PhysReg reg;
+                        bool valid = false;
+                  };
+                  Entry entries[CACHE_SIZE] = {};
+
+                  PhysReg lookup(const ra_ctx& ctx, uint32_t temp_id) {
+                        Entry& e = entries[temp_id & (CACHE_SIZE - 1)];
+                        if (e.valid && e.temp_id == temp_id) {
+                              return e.reg;
+                        }
+                        // Cache miss
+                        PhysReg reg = ctx.assignments[temp_id].reg;
+                        e.temp_id = temp_id;
+                        e.reg = reg;
+                        e.valid = true;
+                        return reg;
+                  }
+
+                  void invalidate(uint32_t temp_id) {
+                        Entry& e = entries[temp_id & (CACHE_SIZE - 1)];
+                        if (e.temp_id == temp_id) {
+                              e.valid = false;
+                        }
+                  }
+
+                  void clear() {
+                        for (auto& e : entries) {
+                              e.valid = false;
+                        }
+                  }
+            } assignment_cache;
+
+            for (Block& block : program->blocks) {
+                  ctx.block = &block;
+                  ctx.last_vcc_defining_instr_ptr = nullptr;
+                  assignment_cache.clear(); // Clear cache for each block
+
+                  /* initialize register file */
+                  RegisterFile register_file = init_reg_file(ctx, program->live.live_in, block);
+                  ctx.war_hint.reset();
+
+                  /* Initialize round-robin iterators */
+                  ctx.rr_vgpr_it = {PhysReg{256}};
+                  ctx.rr_sgpr_it = {PhysReg{0}};
+
+                  std::vector<aco_ptr<Instruction>> instructions;
+                  /* Better capacity prediction based on instruction types */
+                  const size_t predicted_size = block.instructions.size() +
+                  (block.instructions.size() / 4) + // ~25% for copies
+                  (is_gfx9 ? 8 : 0); // Extra for hazard NOPs
+                  instructions.reserve(predicted_size);
+
+                  /* Handle phi nodes */
+                  get_regs_for_phis(ctx, block, register_file, instructions,
+                                    program->live.live_in[block.index]);
+
+                  /* Track previous hardware instruction for GFX9 hazards */
+                  Instruction* prev_hw_instr = nullptr;
+                  if (is_gfx9 && !instructions.empty()) {
+                        /* Optimize: reverse iteration with early exit */
+                        for (auto rit = instructions.rbegin(); rit != instructions.rend(); ++rit) {
+                              Instruction* instr = rit->get();
+                              if (instr && !is_phi(instr) && instr->opcode != aco_opcode::p_parallelcopy) {
+                                    prev_hw_instr = instr;
+                                    break;
+                              }
+                        }
+                  }
+
+                  /* Find first non-phi instruction */
+                  auto NonPhi = [](aco_ptr<Instruction>& instr) -> bool { return instr && !is_phi(instr); };
+                  auto instr_it = std::find_if(block.instructions.begin(), block.instructions.end(), NonPhi);
+
+                  /* Main instruction loop */
+                  for (; instr_it != block.instructions.end(); ++instr_it) {
+                        aco_ptr<Instruction>& instr = *instr_it;
+                        std::vector<parallelcopy> parallelcopy;
+                        assert(!is_phi(instr));
+
+                        /* Cache frequently accessed instruction properties */
+                        const aco_opcode opcode = instr->opcode;
+                        const unsigned num_operands = instr->operands.size();
+                        const unsigned num_definitions = instr->definitions.size();
+                        const bool is_writelane = (opcode == aco_opcode::v_writelane_b32 ||
+                        opcode == aco_opcode::v_writelane_b32_e64);
+
+                        /* Fast path for instructions with no temp operands */
+                        bool has_temp_operands = false;
+                        for (unsigned i = 0; i < num_operands; ++i) {
+                              if (instr->operands[i].isTemp()) {
+                                    has_temp_operands = true;
+                                    break;
+                              }
+                        }
+
+                        bool fixed = false;
+                        bool has_vector_operands = false;
+
+                        if (has_temp_operands) {
+                              /* First pass: rename and check for fixed/vector operands */
+                              for (unsigned i = 0; i < num_operands; ++i) {
+                                    auto& operand = instr->operands[i];
+                                    if (!operand.isTemp())
+                                          continue;
+
+                                    /* rename operands */
+                                    operand.setTemp(read_variable(ctx, operand.getTemp(), block.index));
+                                    const uint32_t temp_id = operand.tempId();
+                                    assert(ctx.assignments[temp_id].assigned);
+
+                                    /* Use cached lookup */
+                                    const PhysReg assigned_reg = assignment_cache.lookup(ctx, temp_id);
+                                    fixed |= operand.isPrecolored() && assigned_reg != operand.physReg();
+
+                                    /* Track vector operands */
+                                    if (operand.isVectorAligned() || (i && instr->operands[i - 1].isVectorAligned())) {
+                                          has_vector_operands = true;
+                                          register_file.clear(Operand(operand.getTemp(), assigned_reg));
+                                    }
+                              }
+
+                              /* GFX9 writelane optimization */
+                              if (is_gfx9 && is_writelane && num_operands >= 2 &&
+                                    instr->operands[0].isTemp() && instr->operands[1].isTemp()) {
+                                    const PhysReg op0_reg = assignment_cache.lookup(ctx, instr->operands[0].tempId());
+                              const PhysReg op1_reg = assignment_cache.lookup(ctx, instr->operands[1].tempId());
+                              if (op0_reg != m0 && op1_reg != m0) {
+                                    instr->operands[0].setPrecolored(m0);
+                                    fixed = true;
+                              }
+                                    }
+                        }
+
+                        if (fixed)
+                              handle_fixed_operands(ctx, register_file, parallelcopy, instr);
+
+                        /* Second pass: allocate registers for non-fixed operands */
+                        if (has_temp_operands) {
+                              for (unsigned i = 0; i < num_operands; ++i) {
+                                    auto& operand = instr->operands[i];
+                                    if (!operand.isTemp() || operand.isFixed())
+                                          continue;
+
+                                    if (operand.isVectorAligned()) {
+                                          handle_vector_operands(ctx, register_file, parallelcopy, instr, i);
+                                          /* Skip aligned vector parts */
+                                          while (i + 1 < num_operands && instr->operands[i + 1].isVectorAligned())
+                                                ++i;
+                                          continue;
+                                    }
+
+                                    const PhysReg reg = assignment_cache.lookup(ctx, operand.tempId());
+                                    if (operand_can_use_reg(program->gfx_level, instr, i, reg, operand.regClass()))
+                                          operand.setFixed(reg);
+                                    else
+                                          get_reg_for_operand(ctx, register_file, parallelcopy, instr, operand, i);
+
+                                    /* WAR hint optimization */
+                                    if ((instr->isEXP() ||
+                                          (instr->isVMEM() && i == 3 && is_gfx6) ||
+                                          (instr->isDS() && instr->ds().gds))) {
+                                          const unsigned base_reg = operand.physReg().reg();
+                                    const unsigned op_size = operand.size();
+                                    /* Unroll for small sizes */
+                                    switch (op_size) {
+                                          case 1: ctx.war_hint.set(base_reg); break;
+                                          case 2: ctx.war_hint.set(base_reg); ctx.war_hint.set(base_reg + 1); break;
+                                          case 3: ctx.war_hint.set(base_reg); ctx.war_hint.set(base_reg + 1);
+                                          ctx.war_hint.set(base_reg + 2); break;
+                                          case 4: ctx.war_hint.set(base_reg); ctx.war_hint.set(base_reg + 1);
+                                          ctx.war_hint.set(base_reg + 2); ctx.war_hint.set(base_reg + 3); break;
+                                          default:
+                                                for (unsigned j = 0; j < op_size; j++)
+                                                      ctx.war_hint.set(base_reg + j);
+                                    }
+                                          }
+                              }
+                        }
+
+                        bool temp_in_scc = register_file[scc];
+
+                        optimize_encoding(ctx, register_file, instr);
+
+                        auto tied_defs = get_tied_defs(instr.get());
+                        handle_operands_tied_to_definitions(ctx, parallelcopy, instr, register_file, tied_defs);
+
+                        /* remove dead vars from register file */
+                        for (const Operand& op : instr->operands) {
+                              if (op.isTemp() && op.isFirstKillBeforeDef()) {
+                                    register_file.clear(op);
+                                    assignment_cache.invalidate(op.tempId());
+                              }
+                        }
+
+                        /* handle fixed definitions first */
+                        for (unsigned i = 0; i < num_definitions; ++i) {
+                              auto& definition = instr->definitions[i];
+                              if (!definition.isFixed())
+                                    continue;
+
+                              assert(i >= tied_defs.size());
+
+                              adjust_max_used_regs(ctx, definition.regClass(), definition.physReg());
+
+                              /* check if the target register is blocked */
+                              if (register_file.test(definition.physReg(), definition.bytes())) {
+                                    const PhysRegInterval def_regs{definition.physReg(), definition.size()};
+
+                                    /* create parallelcopy pair to move blocking vars */
+                                    std::vector<unsigned> vars = collect_vars(ctx, register_file, def_regs);
+
+                                    RegisterFile tmp_file(register_file);
+                                    /* re-enable the killed operands, so that we don't move the blocking vars there */
+                                    tmp_file.fill_killed_operands(instr.get());
+
+                                    ASSERTED bool success = false;
+                                    success = get_regs_for_copies(ctx, tmp_file, parallelcopy, vars, instr, def_regs);
+                                    assert(success);
+
+                                    update_renames(ctx, register_file, parallelcopy, instr);
+                              }
+
+                              if (!definition.isTemp())
+                                    continue;
+
+                              ctx.assignments[definition.tempId()].set(definition);
+                              assignment_cache.invalidate(definition.tempId());
+                              register_file.fill(definition);
+                        }
+
+                        /* handle normal definitions - use fast paths for common patterns */
+                        for (unsigned i = 0; i < num_definitions; ++i) {
+                              Definition* definition = &instr->definitions[i];
+
+                              if (definition->isFixed() || !definition->isTemp() || i < tied_defs.size())
+                                    continue;
+
+                              /* Fast path optimization using computed goto (compiler permitting) */
+                              switch (opcode) {
+                                    case aco_opcode::p_start_linear_vgpr:
+                                          definition->setFixed(alloc_linear_vgpr(ctx, register_file, instr, parallelcopy));
+                                          update_renames(ctx, register_file, parallelcopy, instr);
+                                          break;
+
+                                    case aco_opcode::p_split_vector: {
+                                          PhysReg reg = instr->operands[0].physReg();
+                                          RegClass rc = definition->regClass();
+                                          /* Pre-compute byte offset */
+                                          unsigned byte_offset = 0;
+                                          for (unsigned j = 0; j < i; j++)
+                                                byte_offset += instr->definitions[j].bytes();
+                                          reg.reg_b += byte_offset;
+
+                                          if (get_reg_specified(ctx, register_file, rc, instr, reg, -1)) {
+                                                definition->setFixed(reg);
+                                          } else if (i == 0) {
+                                                RegClass vec_rc = RegClass::get(rc.type(), instr->operands[0].bytes());
+                                                DefInfo info(ctx, ctx.pseudo_dummy, vec_rc, -1);
+                                                std::optional<PhysReg> res = get_reg_simple(ctx, register_file, info);
+                                                if (res && get_reg_specified(ctx, register_file, rc, instr, *res, -1))
+                                                      definition->setFixed(*res);
+                                          } else if (instr->definitions[i - 1].isFixed()) {
+                                                reg = instr->definitions[i - 1].physReg();
+                                                reg.reg_b += instr->definitions[i - 1].bytes();
+                                                if (get_reg_specified(ctx, register_file, rc, instr, reg, -1))
+                                                      definition->setFixed(reg);
+                                          }
+                                          break;
+                                    }
+
+                                    case aco_opcode::p_parallelcopy:
+                                          if (i < num_operands) {
+                                                PhysReg reg = instr->operands[i].physReg();
+                                                if (instr->operands[i].isTemp() &&
+                                                      instr->operands[i].getTemp().type() == definition->getTemp().type() &&
+                                                      !register_file.test(reg, definition->bytes()))
+                                                      definition->setFixed(reg);
+                                          }
+                                          break;
+
+                                    case aco_opcode::p_extract_vector:
+                                          if (instr->operands[1].isConstant()) {
+                                                PhysReg reg = instr->operands[0].physReg();
+                                                reg.reg_b += definition->bytes() * instr->operands[1].constantValue();
+                                                if (get_reg_specified(ctx, register_file, definition->regClass(), instr, reg, -1))
+                                                      definition->setFixed(reg);
+                                          }
+                                          break;
+
+                                    case aco_opcode::p_create_vector: {
+                                          PhysReg reg = get_reg_create_vector(ctx, register_file, definition->getTemp(),
+                                                                              parallelcopy, instr);
+                                          update_renames(ctx, register_file, parallelcopy, instr);
+                                          definition->setFixed(reg);
+                                          break;
+                                    }
+
+                                    default:
+                                          /* WMMA optimization for Vega */
+                                          if (instr_info.classes[(int)opcode] == instr_class::wmma &&
+                                                num_operands > 2 && instr->operands[2].isTemp() &&
+                                                instr->operands[2].isKill() &&
+                                                instr->operands[2].regClass() == definition->regClass()) {
+                                                definition->setFixed(instr->operands[2].physReg());
+                                                }
+                                                break;
+                              }
+
+                              if (!definition->isFixed()) {
+                                    Temp tmp = definition->getTemp();
+                                    PhysReg reg = get_reg(ctx, register_file, tmp, parallelcopy, instr);
+                                    definition->setFixed(reg);
+                                    update_renames(ctx, register_file, parallelcopy, instr);
+
+                                    /* Subdword handling optimization */
+                                    if (definition->regClass().is_subdword() && definition->bytes() < 4 &&
+                                          (reg.byte() || register_file.test(reg, 4))) {
+                                          bool allow_16bit_write = (reg.byte() & 1) == 0 && !register_file.test(reg, 2);
+                                    add_subdword_definition(program, instr, reg, allow_16bit_write);
+                                    /* add_subdword_definition can invalidate the reference */
+                                    definition = &instr->definitions[i];
+                                          }
+                              }
+
+                              assert(
+                                    definition->isFixed() &&
+                                    ((definition->getTemp().type() == RegType::vgpr && definition->physReg() >= 256) ||
+                                    (definition->getTemp().type() != RegType::vgpr && definition->physReg() < 256)));
+                              ctx.assignments[definition->tempId()].set(*definition);
+                              assignment_cache.invalidate(definition->tempId());
+                              register_file.fill(*definition);
+                        }
+
+                        if (has_vector_operands && !ctx.vector_operands.empty())
+                              resolve_vector_operands(ctx, register_file, parallelcopy, instr);
+
+                        /* This ignores tied defs and vector aligned operands because they are late-kill. */
+                        undo_renames(ctx, parallelcopy, instr);
+
+                        assign_tied_definitions(ctx, instr, register_file, tied_defs);
+
+                        handle_pseudo(ctx, register_file, instr.get());
+
+                        /* kill definitions and late-kill operands and ensure that sub-dword operands can actually
+                         * be read - optimize with single pass */
+                        for (const Definition& def : instr->definitions) {
+                              if (def.isTemp() && def.isKill()) {
+                                    register_file.clear(def);
+                                    assignment_cache.invalidate(def.tempId());
+                              }
+                        }
+
+                        /* Combined operand processing */
+                        for (unsigned i = 0; i < num_operands; i++) {
+                              const Operand& op = instr->operands[i];
+                              if (op.isTemp()) {
+                                    if (op.isFirstKill() && op.isLateKill()) {
+                                          register_file.clear(op);
+                                          assignment_cache.invalidate(op.tempId());
+                                    }
+                                    if (op.physReg().byte() != 0) {
+                                          add_subdword_operand(ctx, instr, i, op.physReg().byte(), op.regClass());
+                                    }
+                              }
+                        }
+
+                        emit_parallel_copy(ctx, parallelcopy, instr, instructions, temp_in_scc, register_file);
+
+                        /* GFX9 Vega-specific optimizations and hazard handling */
+                        bool user_instr_defined_vcc = false;
+                        if (is_gfx9) {
+                              /* Cache instruction properties for hazard checks */
+                              const bool curr_is_dpp = instr->isDPP();
+                              const bool curr_is_salu = instr->isSALU();
+
+                              /* VALU->DPP hazard: optimize check with early exit */
+                              if (prev_hw_instr && prev_hw_instr->isVALU() && curr_is_dpp) {
+                                    /* Fast hazard detection using bit manipulation */
+                                    bool hazard = false;
+
+                                    for (const Definition& d : prev_hw_instr->definitions) {
+                                          if (!d.isTemp() || !d.isFixed() || d.regClass().type() != RegType::vgpr)
+                                                continue;
+
+                                          const unsigned d_start = d.physReg().reg_b;
+                                          const unsigned d_end = d_start + d.bytes();
+
+                                          for (const Operand& o : instr->operands) {
+                                                if (!o.isTemp() || !o.isFixed() || o.regClass().type() != RegType::vgpr)
+                                                      continue;
+
+                                                const unsigned o_start = o.physReg().reg_b;
+                                                const unsigned o_end = o_start + o.bytes();
+
+                                                /* Branchless overlap check */
+                                                if ((d_start < o_end) & (o_start < d_end)) {
+                                                      hazard = true;
+                                                      goto hazard_found;
+                                                }
+                                          }
+                                    }
+                                    hazard_found:
+
+                                    if (hazard) {
+                                          aco_ptr<Instruction> nop{create_instruction(aco_opcode::s_nop, Format::SOPP, 0, 0)};
+                                          set_sopp_imm(nop.get(), 1); /* 2 cycles */
+                                          instructions.emplace_back(std::move(nop));
+                                    }
+                              }
+
+                              /* VCC tracking optimization */
+                              if (curr_is_salu) {
+                                    /* Optimize: check VCC writes with early exit */
+                                    for (const Definition& d : instr->definitions) {
+                                          if (d.isFixed()) {
+                                                const PhysReg reg = d.physReg();
+                                                if (reg == vcc || reg == vcc_hi) {
+                                                      user_instr_defined_vcc = true;
+                                                      ctx.last_vcc_defining_instr_ptr = instr.get();
+                                                      break;
+                                                }
+                                          }
+                                    }
+                              }
+
+                              /* VCC->branch hazard: optimize with specific opcode check */
+                              if ((opcode == aco_opcode::s_cbranch_vccz ||
+                                    opcode == aco_opcode::s_cbranch_vccnz) &&
+                                    ctx.last_vcc_defining_instr_ptr == prev_hw_instr && prev_hw_instr) {
+                                    aco_ptr<Instruction> nop{create_instruction(aco_opcode::s_nop, Format::SOPP, 0, 0)};
+                              set_sopp_imm(nop.get(), 1); /* 2 cycles */
+                              instructions.emplace_back(std::move(nop));
+                              ctx.last_vcc_defining_instr_ptr = nullptr;
+                                    }
+                        }
+
+                        /* VOP3 encoding optimization - critical for Vega performance */
+                        if (!instr->isVOP3()) {
+                              /* Fast VOP3 check using lookup table for common cases */
+                              static constexpr bool needs_vcc_check[] = {
+                                    [(int)aco_opcode::v_cndmask_b32] = true,
+                                    [(int)aco_opcode::v_add_co_u32] = true,
+                                    [(int)aco_opcode::v_addc_co_u32] = true,
+                                    [(int)aco_opcode::v_sub_co_u32] = true,
+                                    [(int)aco_opcode::v_subb_co_u32] = true,
+                                    [(int)aco_opcode::v_subrev_co_u32] = true,
+                                    [(int)aco_opcode::v_subbrev_co_u32] = true,
+                              };
+
+                              bool needs_vop3 = false;
+
+                              if ((int)opcode < sizeof(needs_vcc_check) && needs_vcc_check[(int)opcode]) {
+                                    if (opcode == aco_opcode::v_cndmask_b32) {
+                                          needs_vop3 = instr->operands[2].physReg() != vcc;
+                                    } else {
+                                          needs_vop3 = instr->definitions[1].physReg() != vcc;
+                                          if (!needs_vop3 && (opcode == aco_opcode::v_addc_co_u32 ||
+                                                opcode == aco_opcode::v_subb_co_u32 ||
+                                                opcode == aco_opcode::v_subbrev_co_u32)) {
+                                                needs_vop3 = instr->operands[2].physReg() != vcc;
+                                                }
+                                    }
+                              } else if (withoutDPP(instr->format) == Format::VOPC) {
+                                    needs_vop3 = instr->definitions[0].physReg() != vcc;
+                              }
+
+                              if (needs_vop3) {
+                                    /* Literal handling optimization for pre-GFX10 */
+                                    if (is_pre_gfx10 && num_operands > 0 && instr->operands[0].isLiteral()) {
+                                          /* Re-use the register we already allocated for the definition */
+                                          const bool is_64bit = instr->operands[0].size() == 2;
+                                          Temp tmp = program->allocateTmp(is_64bit ? s2 : s1);
+                                          const Definition& def = instr->isVOPC() ? instr->definitions[0] : instr->definitions.back();
+                                          assert(def.regClass() == s2);
+                                          ctx.assignments.emplace_back(def.physReg(), tmp.regClass());
+
+                                          /* Create efficient copy */
+                                          aco_ptr<Instruction> copy{create_instruction(aco_opcode::p_parallelcopy, Format::PSEUDO, 1, 1)};
+                                          copy->operands[0] = instr->operands[0];
+                                          if (copy->operands[0].bytes() < 4)
+                                                copy->operands[0] = Operand::c32(copy->operands[0].constantValue());
+                                          copy->definitions[0] = Definition(tmp);
+                                          copy->definitions[0].setFixed(def.physReg());
+
+                                          instr->operands[0] = Operand(tmp);
+                                          instr->operands[0].setFixed(def.physReg());
+                                          instr->operands[0].setFirstKill(true);
+
+                                          instructions.emplace_back(std::move(copy));
+                                    }
+
+                                    /* change the instruction to VOP3 to enable an arbitrary register pair as dst */
+                                    instr->format = asVOP3(instr->format);
+                              }
+                        }
+
+                        instructions.emplace_back(std::move(*instr_it));
+
+                        /* Update tracking for next iteration - optimize with cached values */
+                        Instruction* const last_pushed = instructions.back().get();
+                        const aco_opcode last_opcode = last_pushed->opcode;
+
+                        if (!user_instr_defined_vcc &&
+                              last_opcode != aco_opcode::p_parallelcopy &&
+                              last_opcode != aco_opcode::s_nop) {
+                              ctx.last_vcc_defining_instr_ptr = nullptr;
+                              }
+
+                              if (last_opcode != aco_opcode::p_parallelcopy &&
+                                    last_opcode != aco_opcode::s_nop) {
+                                    prev_hw_instr = last_pushed;
+                                    }
+
+                  } /* end for Instr */
+
+                  /* Block epilogue optimizations */
+                  if ((block.kind & block_kind_top_level) && block.linear_succs.empty()) {
+                        /* Reset this for block_kind_resume. */
+                        ctx.num_linear_vgprs = 0;
+
+                        /* Debug builds only */
+                        #ifndef NDEBUG
+                        PhysRegInterval vgpr_bounds = get_reg_bounds(ctx, RegType::vgpr, false);
+                        PhysRegInterval sgpr_bounds = get_reg_bounds(ctx, RegType::sgpr, false);
+                        assert(register_file.count_zero(vgpr_bounds) == ctx.vgpr_bounds);
+                        assert(register_file.count_zero(sgpr_bounds) == ctx.sgpr_bounds);
+                        #endif
+                  } else if (should_compact_linear_vgprs(ctx, register_file)) {
+                        /* Optimize: move branch handling */
+                        aco_ptr<Instruction> br = std::move(instructions.back());
+                        instructions.pop_back();
+
+                        /* Optimize SCC check */
+                        bool temp_in_scc = register_file[scc];
+                        if (!temp_in_scc && !br->operands.empty() && br->operands[0].isFixed()) {
+                              temp_in_scc = br->operands[0].physReg() == scc;
+                        }
+
+                        std::vector<parallelcopy> parallelcopy;
+                        compact_linear_vgprs(ctx, register_file, parallelcopy);
+                        update_renames(ctx, register_file, parallelcopy, br);
+                        emit_parallel_copy_internal(ctx, parallelcopy, br, instructions, temp_in_scc, register_file);
+
+                        instructions.push_back(std::move(br));
+                        prev_hw_instr = nullptr;
+                  }
+
+                  block.instructions = std::move(instructions);
+            } /* end for BB */
+
+            /* Final register count optimization for Vega 64 */
+            program->config->num_vgprs = std::min<uint16_t>(get_vgpr_alloc(program, ctx.max_used_vgpr + 1), 256);
+            program->config->num_sgprs = get_sgpr_alloc(program, ctx.max_used_sgpr + 1);
 
-         instructions.push_back(std::move(br));
+            program->progress = CompilationProgress::after_ra;
       }
-
-      block.instructions = std::move(instructions);
-   } /* end for BB */
-
-   /* num_gpr = rnd_up(max_used_gpr + 1) */
-   program->config->num_vgprs =
-      std::min<uint16_t>(get_vgpr_alloc(program, ctx.max_used_vgpr + 1), 256);
-   program->config->num_sgprs = get_sgpr_alloc(program, ctx.max_used_sgpr + 1);
-
-   program->progress = CompilationProgress::after_ra;
-}
-
 } // namespace aco


--- a/src/amd/compiler/aco_live_var_analysis.cpp	2025-05-26 12:24:19.131056673 +0200
+++ b/src/amd/compiler/aco_live_var_analysis.cpp	2025-05-26 13:02:10.133508433 +0200
@@ -4,598 +4,557 @@
  *
  * SPDX-License-Identifier: MIT
  */
-
 #include "aco_ir.h"
+#define LIKELY(x)   __builtin_expect(!!(x), 1)
+#define UNLIKELY(x) __builtin_expect(!!(x), 0)
 
 namespace aco {
 
-RegisterDemand
-get_live_changes(Instruction* instr)
-{
-   RegisterDemand changes;
-   for (const Definition& def : instr->definitions) {
-      if (!def.isTemp() || def.isKill())
-         continue;
-      changes += def.getTemp();
-   }
-
-   for (const Operand& op : instr->operands) {
-      if (!op.isTemp() || !op.isFirstKill())
-         continue;
-      changes -= op.getTemp();
-   }
+      [[nodiscard]] RegisterDemand
+      get_live_changes(Instruction* instr)
+      {
+            RegisterDemand changes;
+            for (const Definition& def : instr->definitions) {
+                  if (UNLIKELY(!def.isTemp()) || def.isKill())
+                        continue;
+                  changes += def.getTemp();
+            }
 
-   return changes;
-}
+            for (const Operand& op : instr->operands) {
+                  if (UNLIKELY(!op.isTemp()) || !op.isFirstKill())
+                        continue;
+                  changes -= op.getTemp();
+            }
 
-RegisterDemand
-get_temp_registers(Instruction* instr)
-{
-   RegisterDemand demand_before;
-   RegisterDemand demand_after;
-
-   for (Definition def : instr->definitions) {
-      if (def.isKill())
-         demand_after += def.getTemp();
-      else if (def.isTemp())
-         demand_before -= def.getTemp();
-   }
-
-   for (Operand op : instr->operands) {
-      if (op.isFirstKill() || op.isCopyKill()) {
-         demand_before += op.getTemp();
-         if (op.isLateKill())
-            demand_after += op.getTemp();
-      } else if (op.isClobbered() && !op.isKill()) {
-         demand_before += op.getTemp();
+            return changes;
       }
-   }
 
-   demand_after.update(demand_before);
-   return demand_after;
-}
+      [[nodiscard]] RegisterDemand
+      get_temp_registers(Instruction* instr)
+      {
+            RegisterDemand demand_before;
+            RegisterDemand demand_after;
+
+            for (const Definition& def : instr->definitions) {
+                  if (UNLIKELY(def.isKill()))
+                        demand_after += def.getTemp();
+                  else if (LIKELY(def.isTemp()))
+                        demand_before -= def.getTemp();
+            }
 
-RegisterDemand get_temp_reg_changes(Instruction* instr)
-{
-   RegisterDemand available_def_space;
-
-   for (Definition def : instr->definitions) {
-      if (def.isTemp())
-         available_def_space += def.getTemp();
-   }
-
-   for (Operand op : instr->operands) {
-      if (op.isFirstKillBeforeDef() || op.isCopyKill())
-         available_def_space -= op.getTemp();
-      else if (op.isClobbered() && !op.isKill())
-         available_def_space -= op.getTemp();
-   }
+            for (const Operand& op : instr->operands) {
+                  if (op.isFirstKill() || op.isCopyKill()) {
+                        demand_before += op.getTemp();
+                        if (UNLIKELY(op.isLateKill()))
+                              demand_after += op.getTemp();
+                  } else if (UNLIKELY(op.isClobbered() && !op.isKill())) {
+                        demand_before += op.getTemp();
+                  }
+            }
 
-   return available_def_space;
-}
+            demand_after.update(demand_before);
+            return demand_after;
+      }
 
-namespace {
+      [[nodiscard]] RegisterDemand get_temp_reg_changes(Instruction* instr)
+      {
+            RegisterDemand available_def_space;
 
-struct live_ctx {
-   monotonic_buffer_resource m;
-   Program* program;
-   int32_t worklist;
-   uint32_t handled_once;
-};
-
-bool
-instr_needs_vcc(Instruction* instr)
-{
-   if (instr->isVOPC())
-      return true;
-   if (instr->isVOP2() && !instr->isVOP3()) {
-      if (instr->operands.size() == 3 && instr->operands[2].isTemp() &&
-          instr->operands[2].regClass().type() == RegType::sgpr)
-         return true;
-      if (instr->definitions.size() == 2)
-         return true;
-   }
-   return false;
-}
+            for (const Definition& def : instr->definitions) {
+                  if (LIKELY(def.isTemp()))
+                        available_def_space += def.getTemp();
+            }
+
+            for (const Operand& op : instr->operands) {
+                  if (op.isFirstKillBeforeDef() || op.isCopyKill())
+                        available_def_space -= op.getTemp();
+                  else if (UNLIKELY(op.isClobbered() && !op.isKill()))
+                        available_def_space -= op.getTemp();
+            }
 
-IDSet
-compute_live_out(live_ctx& ctx, Block* block)
-{
-   IDSet live(ctx.m);
-
-   if (block->logical_succs.empty()) {
-      /* Linear blocks:
-       * Directly insert the successor if it is a linear block as well.
-       */
-      for (unsigned succ : block->linear_succs) {
-         if (ctx.program->blocks[succ].logical_preds.empty()) {
-            live.insert(ctx.program->live.live_in[succ]);
-         } else {
-            for (unsigned t : ctx.program->live.live_in[succ]) {
-               if (ctx.program->temp_rc[t].is_linear())
-                  live.insert(t);
-            }
-         }
-      }
-   } else {
-      /* Logical blocks:
-       * Linear successors are either linear blocks or logical targets.
-       */
-      live = IDSet(ctx.program->live.live_in[block->linear_succs[0]], ctx.m);
-      if (block->linear_succs.size() == 2)
-         live.insert(ctx.program->live.live_in[block->linear_succs[1]]);
-
-      /* At most one logical target needs a separate insertion. */
-      if (block->logical_succs.back() != block->linear_succs.back()) {
-         for (unsigned t : ctx.program->live.live_in[block->logical_succs.back()]) {
-            if (!ctx.program->temp_rc[t].is_linear())
-               live.insert(t);
-         }
-      } else {
-         assert(block->logical_succs[0] == block->linear_succs[0]);
-      }
-   }
-
-   /* Handle phi operands */
-   if (block->linear_succs.size() == 1 && block->linear_succs[0] >= ctx.handled_once) {
-      Block& succ = ctx.program->blocks[block->linear_succs[0]];
-      auto it = std::find(succ.linear_preds.begin(), succ.linear_preds.end(), block->index);
-      unsigned op_idx = std::distance(succ.linear_preds.begin(), it);
-      for (aco_ptr<Instruction>& phi : succ.instructions) {
-         if (!is_phi(phi))
-            break;
-         if (phi->opcode == aco_opcode::p_phi || phi->definitions[0].isKill())
-            continue;
-         if (phi->operands[op_idx].isTemp())
-            live.insert(phi->operands[op_idx].tempId());
-      }
-   }
-   if (block->logical_succs.size() == 1 && block->logical_succs[0] >= ctx.handled_once) {
-      Block& succ = ctx.program->blocks[block->logical_succs[0]];
-      auto it = std::find(succ.logical_preds.begin(), succ.logical_preds.end(), block->index);
-      unsigned op_idx = std::distance(succ.logical_preds.begin(), it);
-      for (aco_ptr<Instruction>& phi : succ.instructions) {
-         if (!is_phi(phi))
-            break;
-         if (phi->opcode == aco_opcode::p_linear_phi || phi->definitions[0].isKill())
-            continue;
-         if (phi->operands[op_idx].isTemp())
-            live.insert(phi->operands[op_idx].tempId());
+            return available_def_space;
       }
-   }
 
-   return live;
-}
+      namespace {
 
-void
-process_live_temps_per_block(live_ctx& ctx, Block* block)
-{
-   RegisterDemand new_demand;
-   block->register_demand = RegisterDemand();
-   IDSet live = compute_live_out(ctx, block);
-
-   /* initialize register demand */
-   for (unsigned t : live)
-      new_demand += Temp(t, ctx.program->temp_rc[t]);
-
-   /* traverse the instructions backwards */
-   int idx;
-   for (idx = block->instructions.size() - 1; idx >= 0; idx--) {
-      Instruction* insn = block->instructions[idx].get();
-      if (is_phi(insn))
-         break;
-
-      ctx.program->needs_vcc |= instr_needs_vcc(insn);
-      insn->register_demand = RegisterDemand(new_demand.vgpr, new_demand.sgpr);
-
-      bool has_vgpr_def = false;
-
-      /* KILL */
-      for (Definition& definition : insn->definitions) {
-         has_vgpr_def |= definition.regClass().type() == RegType::vgpr &&
-                         !definition.regClass().is_linear_vgpr();
-
-         if (!definition.isTemp()) {
-            continue;
-         }
-         if (definition.isFixed() && definition.physReg() == vcc)
-            ctx.program->needs_vcc = true;
-
-         const Temp temp = definition.getTemp();
-         const size_t n = live.erase(temp.id());
-
-         if (n) {
-            new_demand -= temp;
-            definition.setKill(false);
-         } else {
-            insn->register_demand += temp;
-            definition.setKill(true);
-         }
-      }
-
-      /* we need to do this in a separate loop because the next one can
-       * setKill() for several operands at once and we don't want to
-       * overwrite that in a later iteration */
-      bool is_vector_op = false;
-      for (Operand& op : insn->operands) {
-         op.setKill(false);
-         /* Linear vgprs must be late kill: this is to ensure linear VGPR operands and
-          * normal VGPR definitions don't try to use the same register, which is problematic
-          * because of assignment restrictions.
-          */
-         bool lateKill =
-            op.hasRegClass() && op.regClass().is_linear_vgpr() && !op.isUndefined() && has_vgpr_def;
-
-         /* If this Operand is part of a vector which is only partially killed by the instruction,
-          * a definition might not fit into the gaps that get created. Mitigate by using lateKill.
-          */
-         // TODO: is it beneficial to skip that if the vector is fully killed?
-         lateKill |= is_vector_op || op.isVectorAligned();
-         op.setLateKill(lateKill);
-         is_vector_op = op.isVectorAligned();
-      }
-
-      if (ctx.program->gfx_level >= GFX10 && insn->isVALU() &&
-          insn->definitions.back().regClass() == s2) {
-         /* RDNA2 ISA doc, 6.2.4. Wave64 Destination Restrictions:
-          * The first pass of a wave64 VALU instruction may not overwrite a scalar value used by
-          * the second half.
-          */
-         bool carry_in = insn->opcode == aco_opcode::v_addc_co_u32 ||
-                         insn->opcode == aco_opcode::v_subb_co_u32 ||
-                         insn->opcode == aco_opcode::v_subbrev_co_u32;
-         for (unsigned op_idx = 0; op_idx < (carry_in ? 2 : insn->operands.size()); op_idx++) {
-            if (insn->operands[op_idx].isOfType(RegType::sgpr))
-               insn->operands[op_idx].setLateKill(true);
-         }
-      } else if (insn->opcode == aco_opcode::p_bpermute_readlane ||
-                 insn->opcode == aco_opcode::p_bpermute_permlane ||
-                 insn->opcode == aco_opcode::p_bpermute_shared_vgpr ||
-                 insn->opcode == aco_opcode::p_dual_src_export_gfx11 ||
-                 insn->opcode == aco_opcode::v_mqsad_u32_u8) {
-         for (Operand& op : insn->operands)
-            op.setLateKill(true);
-      } else if (insn->opcode == aco_opcode::p_interp_gfx11 && insn->operands.size() == 7) {
-         insn->operands[5].setLateKill(true); /* we re-use the destination reg in the middle */
-      } else if (insn->opcode == aco_opcode::v_interp_p1_f32 && ctx.program->dev.has_16bank_lds) {
-         insn->operands[0].setLateKill(true);
-      } else if (insn->opcode == aco_opcode::p_init_scratch) {
-         insn->operands.back().setLateKill(true);
-      } else if (instr_info.classes[(int)insn->opcode] == instr_class::wmma) {
-         insn->operands[0].setLateKill(true);
-         insn->operands[1].setLateKill(true);
-      }
-
-      /* Check if a definition clobbers some operand */
-      RegisterDemand operand_demand;
-      auto tied_defs = get_tied_defs(insn);
-      for (auto op_idx : tied_defs) {
-         Temp tmp = insn->operands[op_idx].getTemp();
-         if (std::any_of(tied_defs.begin(), tied_defs.end(), [&](uint32_t i)
-                         { return i < op_idx && insn->operands[i].getTemp() == tmp; })) {
-            operand_demand += tmp;
-            insn->operands[op_idx].setCopyKill(true);
-         }
-         insn->operands[op_idx].setClobbered(true);
-      }
-
-      /* GEN */
-      for (unsigned i = 0; i < insn->operands.size(); ++i) {
-         Operand& operand = insn->operands[i];
-         if (!operand.isTemp())
-            continue;
-
-         const Temp temp = operand.getTemp();
-         if (operand.isPrecolored()) {
-            assert(!operand.isLateKill());
-            ctx.program->needs_vcc |= operand.physReg() == vcc;
-
-            /* Check if this operand gets overwritten by a precolored definition. */
-            if (std::any_of(insn->definitions.begin(), insn->definitions.end(),
-                            [=](Definition def)
-                            {
-                               return def.isFixed() &&
-                                      def.physReg() + def.size() > operand.physReg() &&
-                                      operand.physReg() + operand.size() > def.physReg();
-                            }))
-               operand.setClobbered(true);
-
-            /* Check if another precolored operand uses the same temporary.
-             * This assumes that operands of one instruction are not precolored twice to
-             * the same register. In this case, register pressure might be overestimated.
-             */
-            for (unsigned j = i + 1; !operand.isCopyKill() && j < insn->operands.size(); ++j) {
-               if (insn->operands[j].isPrecolored() && insn->operands[j].getTemp() == temp) {
-                  operand_demand += temp;
-                  insn->operands[j].setCopyKill(true);
-               }
-            }
-         }
-         /* If this operand is part of a vector, check if the temporary needs to be duplicated. */
-         if (is_vector_op || operand.isVectorAligned()) {
-            /* Set copyKill if any other vector-operand uses the same temporary. If a scalar operand
-             * uses the same temporary, assume that it can share the register. This ignores other
-             * register constraints like tied definitions or precolored registers.
-             */
-            bool other_is_vector_op = false;
-            for (unsigned j = 0; j < i; j++) {
-               if ((other_is_vector_op || insn->operands[j].isVectorAligned()) &&
-                   insn->operands[j].getTemp() == temp) {
-                  operand_demand += temp;
-                  insn->register_demand += temp; /* Because of lateKill */
-                  operand.setCopyKill(true);
-                  break;
-               }
-               other_is_vector_op = insn->operands[j].isVectorAligned();
-            }
-         }
-         is_vector_op = operand.isVectorAligned();
-
-         if (operand.isLateKill()) {
-            /* Make sure that same temporaries have same lateKill flags. */
-            for (Operand& other : insn->operands) {
-               if (other.isTemp() && other.getTemp() == operand.getTemp())
-                  other.setLateKill(true);
-            }
-         }
-
-         if (operand.isKill())
-            continue;
-
-         if (live.insert(temp.id()).second) {
-            operand.setFirstKill(true);
-            for (unsigned j = i + 1; j < insn->operands.size(); ++j) {
-               if (insn->operands[j].isTemp() && insn->operands[j].getTemp() == temp)
-                  insn->operands[j].setKill(true);
-            }
-            if (operand.isLateKill())
-               insn->register_demand += temp;
-            new_demand += temp;
-         } else if (operand.isClobbered()) {
-            operand_demand += temp;
-         }
-      }
-
-      operand_demand += new_demand;
-      insn->register_demand.update(operand_demand);
-      block->register_demand.update(insn->register_demand);
-   }
-
-   /* handle phi definitions */
-   for (int phi_idx = 0; phi_idx <= idx; phi_idx++) {
-      Instruction* insn = block->instructions[phi_idx].get();
-      insn->register_demand = new_demand;
-
-      assert(is_phi(insn) && insn->definitions.size() == 1);
-      if (!insn->definitions[0].isTemp()) {
-         assert(insn->definitions[0].isFixed() && insn->definitions[0].physReg() == exec);
-         continue;
-      }
-      Definition& definition = insn->definitions[0];
-      ctx.program->needs_vcc |= definition.isFixed() && definition.physReg() == vcc;
-      const size_t n = live.erase(definition.tempId());
-      if (n && (definition.isKill() || ctx.handled_once > block->index)) {
-         Block::edge_vec& preds =
-            insn->opcode == aco_opcode::p_phi ? block->logical_preds : block->linear_preds;
-         for (unsigned i = 0; i < preds.size(); i++) {
-            if (insn->operands[i].isTemp())
-               ctx.worklist = std::max<int>(ctx.worklist, preds[i]);
-         }
-      }
-      definition.setKill(!n);
-   }
-
-   /* handle phi operands */
-   for (int phi_idx = 0; phi_idx <= idx; phi_idx++) {
-      Instruction* insn = block->instructions[phi_idx].get();
-      assert(is_phi(insn));
-      /* Ignore dead phis. */
-      if (insn->definitions[0].isKill())
-         continue;
-      for (Operand& operand : insn->operands) {
-         if (!operand.isTemp())
-            continue;
-
-         /* set if the operand is killed by this (or another) phi instruction */
-         operand.setKill(!live.count(operand.tempId()));
-      }
-   }
-
-   if (ctx.program->live.live_in[block->index].insert(live)) {
-      if (block->linear_preds.size()) {
-         assert(block->logical_preds.empty() ||
-                block->logical_preds.back() <= block->linear_preds.back());
-         ctx.worklist = std::max<int>(ctx.worklist, block->linear_preds.back());
-      } else {
-         ASSERTED bool is_valid = validate_ir(ctx.program);
-         assert(!is_valid);
-      }
-   }
-
-   block->live_in_demand = new_demand;
-   block->register_demand.update(block->live_in_demand);
-   ctx.program->max_reg_demand.update(block->register_demand);
-   ctx.handled_once = std::min(ctx.handled_once, block->index);
+            struct live_ctx {
+                  monotonic_buffer_resource m;
+                  Program* program;
+                  int32_t worklist;
+                  uint32_t handled_once;
+            };
+
+            [[nodiscard]] bool
+            instr_needs_vcc(Instruction* instr)
+            {
+                  if (UNLIKELY(instr->isVOPC()))
+                        return true;
+                  if (instr->isVOP2() && !instr->isVOP3()) {
+                        if (UNLIKELY(instr->operands.size() == 3 && instr->operands[2].isTemp() &&
+                              instr->operands[2].regClass().type() == RegType::sgpr))
+                              return true;
+                        if (UNLIKELY(instr->definitions.size() == 2))
+                              return true;
+                  }
+                  return false;
+            }
 
-   assert(!block->linear_preds.empty() || (new_demand == RegisterDemand() && live.empty()));
-}
+            [[nodiscard]] IDSet
+            compute_live_out(live_ctx& ctx, Block* block)
+            {
+                  IDSet live(ctx.m);
+
+                  if (LIKELY(block->logical_succs.empty())) {
+                        for (unsigned succ : block->linear_succs) {
+                              if (LIKELY(ctx.program->blocks[succ].logical_preds.empty())) {
+                                    live.insert(ctx.program->live.live_in[succ]);
+                              } else {
+                                    for (unsigned t : ctx.program->live.live_in[succ]) {
+                                          if (UNLIKELY(ctx.program->temp_rc[t].is_linear()))
+                                                live.insert(t);
+                                    }
+                              }
+                        }
+                  } else {
+                        live = IDSet(ctx.program->live.live_in[block->linear_succs[0]], ctx.m);
+                        if (UNLIKELY(block->linear_succs.size() == 2))
+                              live.insert(ctx.program->live.live_in[block->linear_succs[1]]);
+
+                        if (UNLIKELY(block->logical_succs.back() != block->linear_succs.back())) {
+                              for (unsigned t : ctx.program->live.live_in[block->logical_succs.back()]) {
+                                    if (LIKELY(!ctx.program->temp_rc[t].is_linear()))
+                                          live.insert(t);
+                              }
+                        } else {
+                              assert(block->logical_succs[0] == block->linear_succs[0]);
+                        }
+                  }
+
+                  if (block->linear_succs.size() == 1 && block->linear_succs[0] >= ctx.handled_once) {
+                        Block& succ = ctx.program->blocks[block->linear_succs[0]];
+                        auto it = std::find(succ.linear_preds.begin(), succ.linear_preds.end(), block->index);
+                        unsigned op_idx = std::distance(succ.linear_preds.begin(), it);
+                        for (aco_ptr<Instruction>& phi : succ.instructions) {
+                              if (UNLIKELY(!is_phi(phi)))
+                                    break;
+                              if (UNLIKELY(phi->opcode == aco_opcode::p_phi || phi->definitions[0].isKill()))
+                                    continue;
+                              if (LIKELY(phi->operands[op_idx].isTemp()))
+                                    live.insert(phi->operands[op_idx].tempId());
+                        }
+                  }
+                  if (block->logical_succs.size() == 1 && block->logical_succs[0] >= ctx.handled_once) {
+                        Block& succ = ctx.program->blocks[block->logical_succs[0]];
+                        auto it = std::find(succ.logical_preds.begin(), succ.logical_preds.end(), block->index);
+                        unsigned op_idx = std::distance(succ.logical_preds.begin(), it);
+                        for (aco_ptr<Instruction>& phi : succ.instructions) {
+                              if (UNLIKELY(!is_phi(phi)))
+                                    break;
+                              if (UNLIKELY(phi->opcode == aco_opcode::p_linear_phi || phi->definitions[0].isKill()))
+                                    continue;
+                              if (LIKELY(phi->operands[op_idx].isTemp()))
+                                    live.insert(phi->operands[op_idx].tempId());
+                        }
+                  }
 
-unsigned
-calc_waves_per_workgroup(Program* program)
-{
-   /* When workgroup size is not known, just go with wave_size */
-   unsigned workgroup_size =
-      program->workgroup_size == UINT_MAX ? program->wave_size : program->workgroup_size;
+                  return live;
+            }
 
-   return align(workgroup_size, program->wave_size) / program->wave_size;
-}
-} /* end namespace */
+            void
+            process_live_temps_per_block(live_ctx& ctx, Block* block)
+            {
+                  RegisterDemand new_demand;
+                  block->register_demand = RegisterDemand();
+                  IDSet live = compute_live_out(ctx, block);
+
+                  for (unsigned t : live)
+                        new_demand += Temp(t, ctx.program->temp_rc[t]);
+
+                  int idx;
+                  for (idx = block->instructions.size() - 1; idx >= 0; idx--) {
+                        Instruction* insn = block->instructions[idx].get();
+                        if (UNLIKELY(is_phi(insn)))
+                              break;
+
+                        ctx.program->needs_vcc |= instr_needs_vcc(insn);
+                        insn->register_demand = RegisterDemand(new_demand.vgpr, new_demand.sgpr);
+
+                        bool has_vgpr_def = false;
+
+                        for (Definition& definition : insn->definitions) {
+                              has_vgpr_def |= definition.regClass().type() == RegType::vgpr &&
+                              !definition.regClass().is_linear_vgpr();
+
+                              if (UNLIKELY(!definition.isTemp())) {
+                                    continue;
+                              }
+                              if (UNLIKELY(definition.isFixed() && definition.physReg() == vcc))
+                                    ctx.program->needs_vcc = true;
+
+                              const Temp temp = definition.getTemp();
+                              const size_t n = live.erase(temp.id());
+
+                              if (LIKELY(n)) {
+                                    new_demand -= temp;
+                                    definition.setKill(false);
+                              } else {
+                                    insn->register_demand += temp;
+                                    definition.setKill(true);
+                              }
+                        }
+
+                        bool is_vector_op = false;
+                        for (Operand& op : insn->operands) {
+                              op.setKill(false);
+                              bool lateKill =
+                              (UNLIKELY(op.hasRegClass() && op.regClass().is_linear_vgpr() && !op.isUndefined() && has_vgpr_def));
+                              lateKill |= is_vector_op || op.isVectorAligned();
+                              op.setLateKill(lateKill);
+                              is_vector_op = op.isVectorAligned();
+                        }
+
+                        if (UNLIKELY(ctx.program->gfx_level >= GFX10 && insn->isVALU() &&
+                              insn->definitions.back().regClass() == s2)) {
+                              bool carry_in = insn->opcode == aco_opcode::v_addc_co_u32 ||
+                              insn->opcode == aco_opcode::v_subb_co_u32 ||
+                              insn->opcode == aco_opcode::v_subbrev_co_u32;
+                        for (unsigned op_idx = 0; op_idx < (carry_in ? 2 : insn->operands.size()); op_idx++) {
+                              if (insn->operands[op_idx].isOfType(RegType::sgpr))
+                                    insn->operands[op_idx].setLateKill(true);
+                        }
+                              } else if (UNLIKELY(insn->opcode == aco_opcode::p_bpermute_readlane ||
+                                    insn->opcode == aco_opcode::p_bpermute_permlane ||
+                                    insn->opcode == aco_opcode::p_bpermute_shared_vgpr ||
+                                    insn->opcode == aco_opcode::p_dual_src_export_gfx11 ||
+                                    insn->opcode == aco_opcode::v_mqsad_u32_u8)) {
+                                    for (Operand& op : insn->operands)
+                                          op.setLateKill(true);
+                                    } else if (UNLIKELY(insn->opcode == aco_opcode::p_interp_gfx11 && insn->operands.size() == 7)) {
+                                          insn->operands[5].setLateKill(true);
+                                    } else if (UNLIKELY(insn->opcode == aco_opcode::v_interp_p1_f32 && ctx.program->dev.has_16bank_lds)) {
+                                          insn->operands[0].setLateKill(true);
+                                    } else if (UNLIKELY(insn->opcode == aco_opcode::p_init_scratch)) {
+                                          insn->operands.back().setLateKill(true);
+                                    } else if (UNLIKELY(instr_info.classes[(int)insn->opcode] == instr_class::wmma)) {
+                                          insn->operands[0].setLateKill(true);
+                                          insn->operands[1].setLateKill(true);
+                                    }
+
+                                    RegisterDemand operand_demand;
+                                    auto tied_defs = get_tied_defs(insn);
+                                    for (auto op_idx : tied_defs) {
+                                          Temp tmp = insn->operands[op_idx].getTemp();
+                                          if (UNLIKELY(std::any_of(tied_defs.begin(), tied_defs.end(), [&](uint32_t i)
+                                          { return i < op_idx && insn->operands[i].getTemp() == tmp; }))) {
+                                                operand_demand += tmp;
+                                                insn->operands[op_idx].setCopyKill(true);
+                                          }
+                                          insn->operands[op_idx].setClobbered(true);
+                                    }
+
+                                    is_vector_op = false;
+                                    for (unsigned i = 0; i < insn->operands.size(); ++i) {
+                                          Operand& operand = insn->operands[i];
+                                          if (UNLIKELY(!operand.isTemp()))
+                                                continue;
+
+                                          const Temp temp = operand.getTemp();
+                                          if (UNLIKELY(operand.isPrecolored())) {
+                                                assert(!operand.isLateKill());
+                                                ctx.program->needs_vcc |= operand.physReg() == vcc;
+
+                                                if (UNLIKELY(std::any_of(insn->definitions.begin(), insn->definitions.end(),
+                                                      [=](const Definition& def)
+                                                      {
+                                                            return def.isFixed() &&
+                                                            def.physReg() + def.size() > operand.physReg() &&
+                                                            operand.physReg() + operand.size() > def.physReg();
+                                                      })))
+                                                      operand.setClobbered(true);
+
+                                                for (unsigned j = i + 1; !operand.isCopyKill() && j < insn->operands.size(); ++j) {
+                                                      if (UNLIKELY(insn->operands[j].isPrecolored() && insn->operands[j].getTemp() == temp)) {
+                                                            operand_demand += temp;
+                                                            insn->operands[j].setCopyKill(true);
+                                                      }
+                                                }
+                                          }
+
+                                          if (is_vector_op || operand.isVectorAligned()) {
+                                                bool other_is_vector_op = false;
+                                                for (unsigned j = 0; j < i; j++) {
+                                                      if ((other_is_vector_op || insn->operands[j].isVectorAligned()) &&
+                                                            insn->operands[j].getTemp() == temp) {
+                                                            operand_demand += temp;
+                                                      insn->register_demand += temp;
+                                                      operand.setCopyKill(true);
+                                                      break;
+                                                            }
+                                                            other_is_vector_op = insn->operands[j].isVectorAligned();
+                                                }
+                                          }
+
+                                          if (UNLIKELY(operand.isLateKill())) {
+                                                for (Operand& other : insn->operands) {
+                                                      if (other.isTemp() && other.getTemp() == operand.getTemp())
+                                                            other.setLateKill(true);
+                                                }
+                                          }
+
+                                          if (UNLIKELY(operand.isKill()))
+                                                continue;
+
+                                          if (LIKELY(live.insert(temp.id()).second)) {
+                                                operand.setFirstKill(true);
+                                                for (unsigned j = i + 1; j < insn->operands.size(); ++j) {
+                                                      if (insn->operands[j].isTemp() && insn->operands[j].getTemp() == temp)
+                                                            insn->operands[j].setKill(true);
+                                                }
+                                                if (UNLIKELY(operand.isLateKill()))
+                                                      insn->register_demand += temp;
+                                                new_demand += temp;
+                                          } else if (UNLIKELY(operand.isClobbered())) {
+                                                operand_demand += temp;
+                                          }
+                                          is_vector_op = operand.isVectorAligned();
+                                    }
+
+                                    operand_demand += new_demand;
+                                    insn->register_demand.update(operand_demand);
+                                    block->register_demand.update(insn->register_demand);
+                  }
+
+                  for (int phi_idx = 0; phi_idx <= idx; phi_idx++) {
+                        Instruction* insn = block->instructions[phi_idx].get();
+                        insn->register_demand = new_demand;
+
+                        assert(is_phi(insn) && insn->definitions.size() == 1);
+                        if (UNLIKELY(!insn->definitions[0].isTemp())) {
+                              assert(insn->definitions[0].isFixed() && insn->definitions[0].physReg() == exec);
+                              continue;
+                        }
+                        Definition& definition = insn->definitions[0];
+                        if (UNLIKELY(definition.isFixed() && definition.physReg() == vcc))
+                              ctx.program->needs_vcc = true;
+                        const size_t n = live.erase(definition.tempId());
+                        if (n && (definition.isKill() || ctx.handled_once > block->index)) {
+                              Block::edge_vec& preds =
+                              insn->opcode == aco_opcode::p_phi ? block->logical_preds : block->linear_preds;
+                              for (unsigned i = 0; i < preds.size(); i++) {
+                                    if (LIKELY(insn->operands[i].isTemp()))
+                                          ctx.worklist = std::max<int>(ctx.worklist, preds[i]);
+                              }
+                        }
+                        definition.setKill(!n);
+                  }
+
+                  for (int phi_idx = 0; phi_idx <= idx; phi_idx++) {
+                        Instruction* insn = block->instructions[phi_idx].get();
+                        assert(is_phi(insn));
+                        if (UNLIKELY(insn->definitions[0].isKill()))
+                              continue;
+                        for (Operand& operand : insn->operands) {
+                              if (UNLIKELY(!operand.isTemp()))
+                                    continue;
+
+                              operand.setKill(!live.count(operand.tempId()));
+                        }
+                  }
+
+                  if (LIKELY(ctx.program->live.live_in[block->index].insert(live))) {
+                        if (LIKELY(block->linear_preds.size())) {
+                              assert(block->logical_preds.empty() ||
+                              block->logical_preds.back() <= block->linear_preds.back());
+                              ctx.worklist = std::max<int>(ctx.worklist, block->linear_preds.back());
+                        } else {
+                              [[maybe_unused]] ASSERTED bool is_valid = validate_ir(ctx.program);
+                              assert(!is_valid);
+                        }
+                  }
+
+                  block->live_in_demand = new_demand;
+                  block->register_demand.update(block->live_in_demand);
+                  ctx.program->max_reg_demand.update(block->register_demand);
+                  ctx.handled_once = std::min(ctx.handled_once, block->index);
 
-bool
-uses_scratch(Program* program)
-{
-   /* RT uses scratch but we don't yet know how much. */
-   return program->config->scratch_bytes_per_wave || program->stage == raytracing_cs;
-}
+                  assert(!block->linear_preds.empty() || (new_demand == RegisterDemand() && live.empty()));
+            }
 
-uint16_t
-get_extra_sgprs(Program* program)
-{
-   /* We don't use this register on GFX6-8 and it's removed on GFX10+. */
-   bool needs_flat_scr = uses_scratch(program) && program->gfx_level == GFX9;
-
-   if (program->gfx_level >= GFX10) {
-      assert(!program->dev.xnack_enabled);
-      return 0;
-   } else if (program->gfx_level >= GFX8) {
-      if (needs_flat_scr)
-         return 6;
-      else if (program->dev.xnack_enabled)
-         return 4;
-      else if (program->needs_vcc)
-         return 2;
-      else
-         return 0;
-   } else {
-      assert(!program->dev.xnack_enabled);
-      if (needs_flat_scr)
-         return 4;
-      else if (program->needs_vcc)
-         return 2;
-      else
-         return 0;
-   }
-}
+            [[nodiscard]] unsigned
+            calc_waves_per_workgroup(Program* program)
+            {
+                  unsigned workgroup_size =
+                  program->workgroup_size == UINT_MAX ? program->wave_size : program->workgroup_size;
 
-uint16_t
-get_sgpr_alloc(Program* program, uint16_t addressable_sgprs)
-{
-   uint16_t sgprs = addressable_sgprs + get_extra_sgprs(program);
-   uint16_t granule = program->dev.sgpr_alloc_granule;
-   return ALIGN_NPOT(std::max(sgprs, granule), granule);
-}
+                  return align(workgroup_size, program->wave_size) / program->wave_size;
+            }
+      }
 
-uint16_t
-get_vgpr_alloc(Program* program, uint16_t addressable_vgprs)
-{
-   assert(addressable_vgprs <= program->dev.vgpr_limit);
-   uint16_t granule = program->dev.vgpr_alloc_granule;
-   return ALIGN_NPOT(std::max(addressable_vgprs, granule), granule);
-}
+      [[nodiscard]] bool
+      uses_scratch(Program* program)
+      {
+            return UNLIKELY(program->config->scratch_bytes_per_wave || program->stage == raytracing_cs);
+      }
+
+      [[nodiscard]] uint16_t
+      get_extra_sgprs(Program* program)
+      {
+            if (UNLIKELY(!program))
+                  return 0;
+
+            bool needs_scratch_memory = uses_scratch(program);
+
+            if (UNLIKELY(program->gfx_level < GFX8)) {
+                  if (UNLIKELY(needs_scratch_memory)) {
+                        return 4;
+                  } else if (LIKELY(program->needs_vcc)) {
+                        return 2;
+                  }
+                  return 0;
+            } else if (UNLIKELY(program->gfx_level == GFX8)) {
+                  if (UNLIKELY(needs_scratch_memory)) {
+                        if (UNLIKELY(program->dev.xnack_enabled)) {
+                              return 6;
+                        } else {
+                              return 4;
+                        }
+                  }
+                  else if (UNLIKELY(program->dev.xnack_enabled)) {
+                        return 4;
+                  } else if (LIKELY(program->needs_vcc)) {
+                        return 2;
+                  }
+                  return 0;
+            } else if (UNLIKELY(program->gfx_level == GFX9)) {
+                  if (UNLIKELY(needs_scratch_memory)) {
+                        return 6;
+                  } else if (UNLIKELY(program->dev.xnack_enabled)) {
+                        return 4;
+                  } else if (LIKELY(program->needs_vcc)) {
+                        return 2;
+                  }
+                  return 0;
+            } else {
+                  assert(!program->dev.xnack_enabled && "XNACK not supported on GFX10+");
+                  if (UNLIKELY(needs_scratch_memory)) {
+                        return 2;
+                  }
+                  return 0;
+            }
+      }
 
-unsigned
-round_down(unsigned a, unsigned b)
-{
-   return a - (a % b);
-}
+      [[nodiscard]] uint16_t
+      get_sgpr_alloc(Program* program, uint16_t addressable_sgprs)
+      {
+            uint16_t sgprs = addressable_sgprs + get_extra_sgprs(program);
+            uint16_t granule = program->dev.sgpr_alloc_granule;
+            return ALIGN_NPOT(std::max(sgprs, granule), granule);
+      }
+
+      [[nodiscard]] uint16_t
+      get_vgpr_alloc(Program* program, uint16_t addressable_vgprs)
+      {
+            assert(addressable_vgprs <= program->dev.vgpr_limit);
+            uint16_t granule = program->dev.vgpr_alloc_granule;
+            return ALIGN_NPOT(std::max(addressable_vgprs, granule), granule);
+      }
+
+      [[nodiscard]] unsigned
+      round_down(unsigned a, unsigned b)
+      {
+            return a - (a % b);
+      }
+
+      [[nodiscard]] RegisterDemand
+      get_addr_regs_from_waves(Program* program, uint16_t waves)
+      {
+            uint16_t sgprs = std::min(program->dev.physical_sgprs / waves, 128);
+            sgprs = round_down(sgprs, program->dev.sgpr_alloc_granule) - get_extra_sgprs(program);
+            sgprs = std::min(sgprs, program->dev.sgpr_limit);
+
+            uint16_t vgprs = program->dev.physical_vgprs / waves;
+            vgprs = vgprs / program->dev.vgpr_alloc_granule * program->dev.vgpr_alloc_granule;
+            vgprs -= program->config->num_shared_vgprs / 2;
+            vgprs = std::min(vgprs, program->dev.vgpr_limit);
+            return RegisterDemand(vgprs, sgprs);
+      }
+
+      void
+      calc_min_waves(Program* program)
+      {
+            unsigned waves_per_workgroup = calc_waves_per_workgroup(program);
+            unsigned simd_per_cu_wgp = program->dev.simd_per_cu * (program->wgp_mode ? 2 : 1);
+            program->min_waves = DIV_ROUND_UP(waves_per_workgroup, simd_per_cu_wgp);
+      }
+
+      [[nodiscard]] uint16_t
+      max_suitable_waves(Program* program, uint16_t waves)
+      {
+            unsigned num_simd = program->dev.simd_per_cu * (program->wgp_mode ? 2 : 1);
+            unsigned waves_per_workgroup = calc_waves_per_workgroup(program);
+            unsigned num_workgroups = waves * num_simd / waves_per_workgroup;
+
+            unsigned lds_per_workgroup = align(program->config->lds_size * program->dev.lds_encoding_granule,
+                                               program->dev.lds_alloc_granule);
+
+            if (program->stage == fragment_fs) {
+                  unsigned lds_bytes_per_interp = 3 * 16;
+                  unsigned lds_param_bytes = lds_bytes_per_interp * program->info.ps.num_inputs;
+                  lds_per_workgroup += align(lds_param_bytes, program->dev.lds_alloc_granule);
+            }
+            unsigned lds_limit = program->wgp_mode ? program->dev.lds_limit * 2 : program->dev.lds_limit;
+            if (lds_per_workgroup)
+                  num_workgroups = std::min(num_workgroups, lds_limit / lds_per_workgroup);
+
+            if (waves_per_workgroup > 1)
+                  num_workgroups = std::min(num_workgroups, program->wgp_mode ? 32u : 16u);
+
+            unsigned workgroup_waves = num_workgroups * waves_per_workgroup;
+            return DIV_ROUND_UP(workgroup_waves, num_simd);
+      }
+
+      void
+      update_vgpr_sgpr_demand(Program* program, const RegisterDemand new_demand)
+      {
+            assert(program->min_waves >= 1);
+            RegisterDemand limit = get_addr_regs_from_waves(program, program->min_waves);
+
+            if (UNLIKELY(new_demand.exceeds(limit))) {
+                  program->num_waves = 0;
+                  program->max_reg_demand = new_demand;
+            } else {
+                  program->num_waves = program->dev.physical_sgprs / get_sgpr_alloc(program, new_demand.sgpr);
+                  uint16_t vgpr_demand =
+                  get_vgpr_alloc(program, new_demand.vgpr) + program->config->num_shared_vgprs / 2;
+                  program->num_waves =
+                  std::min<uint16_t>(program->num_waves, program->dev.physical_vgprs / vgpr_demand);
+                  program->num_waves = std::min(program->num_waves, program->dev.max_waves_per_simd);
 
-RegisterDemand
-get_addr_regs_from_waves(Program* program, uint16_t waves)
-{
-   /* it's not possible to allocate more than 128 SGPRs */
-   uint16_t sgprs = std::min(program->dev.physical_sgprs / waves, 128);
-   sgprs = round_down(sgprs, program->dev.sgpr_alloc_granule) - get_extra_sgprs(program);
-   sgprs = std::min(sgprs, program->dev.sgpr_limit);
-
-   uint16_t vgprs = program->dev.physical_vgprs / waves;
-   vgprs = vgprs / program->dev.vgpr_alloc_granule * program->dev.vgpr_alloc_granule;
-   vgprs -= program->config->num_shared_vgprs / 2;
-   vgprs = std::min(vgprs, program->dev.vgpr_limit);
-   return RegisterDemand(vgprs, sgprs);
-}
+                  program->num_waves = max_suitable_waves(program, program->num_waves);
+                  program->max_reg_demand = get_addr_regs_from_waves(program, program->num_waves);
+            }
+      }
 
-void
-calc_min_waves(Program* program)
-{
-   unsigned waves_per_workgroup = calc_waves_per_workgroup(program);
-   unsigned simd_per_cu_wgp = program->dev.simd_per_cu * (program->wgp_mode ? 2 : 1);
-   program->min_waves = DIV_ROUND_UP(waves_per_workgroup, simd_per_cu_wgp);
-}
+      void
+      live_var_analysis(Program* program)
+      {
+            program->live.live_in.clear();
+            program->live.memory.release();
+            program->live.live_in.resize(program->blocks.size(), IDSet(program->live.memory));
+            program->max_reg_demand = RegisterDemand();
+            program->needs_vcc = program->gfx_level >= GFX10;
+
+            live_ctx ctx;
+            ctx.program = program;
+            ctx.worklist = program->blocks.size() - 1;
+            ctx.handled_once = program->blocks.size();
 
-uint16_t
-max_suitable_waves(Program* program, uint16_t waves)
-{
-   unsigned num_simd = program->dev.simd_per_cu * (program->wgp_mode ? 2 : 1);
-   unsigned waves_per_workgroup = calc_waves_per_workgroup(program);
-   unsigned num_workgroups = waves * num_simd / waves_per_workgroup;
-
-   /* Adjust #workgroups for LDS */
-   unsigned lds_per_workgroup = align(program->config->lds_size * program->dev.lds_encoding_granule,
-                                      program->dev.lds_alloc_granule);
-
-   if (program->stage == fragment_fs) {
-      /* PS inputs are moved from PC (parameter cache) to LDS before PS waves are launched.
-       * Each PS input occupies 3x vec4 of LDS space. See Figure 10.3 in GCN3 ISA manual.
-       * These limit occupancy the same way as other stages' LDS usage does.
-       */
-      unsigned lds_bytes_per_interp = 3 * 16;
-      unsigned lds_param_bytes = lds_bytes_per_interp * program->info.ps.num_inputs;
-      lds_per_workgroup += align(lds_param_bytes, program->dev.lds_alloc_granule);
-   }
-   unsigned lds_limit = program->wgp_mode ? program->dev.lds_limit * 2 : program->dev.lds_limit;
-   if (lds_per_workgroup)
-      num_workgroups = std::min(num_workgroups, lds_limit / lds_per_workgroup);
-
-   /* Hardware limitation */
-   if (waves_per_workgroup > 1)
-      num_workgroups = std::min(num_workgroups, program->wgp_mode ? 32u : 16u);
-
-   /* Adjust #waves for workgroup multiples:
-    * In cases like waves_per_workgroup=3 or lds=65536 and
-    * waves_per_workgroup=1, we want the maximum possible number of waves per
-    * SIMD and not the minimum. so DIV_ROUND_UP is used
-    */
-   unsigned workgroup_waves = num_workgroups * waves_per_workgroup;
-   return DIV_ROUND_UP(workgroup_waves, num_simd);
-}
+            while (LIKELY(ctx.worklist >= 0)) {
+                  process_live_temps_per_block(ctx, &program->blocks[ctx.worklist--]);
+            }
 
-void
-update_vgpr_sgpr_demand(Program* program, const RegisterDemand new_demand)
-{
-   assert(program->min_waves >= 1);
-   RegisterDemand limit = get_addr_regs_from_waves(program, program->min_waves);
-
-   /* this won't compile, register pressure reduction necessary */
-   if (new_demand.exceeds(limit)) {
-      program->num_waves = 0;
-      program->max_reg_demand = new_demand;
-   } else {
-      program->num_waves = program->dev.physical_sgprs / get_sgpr_alloc(program, new_demand.sgpr);
-      uint16_t vgpr_demand =
-         get_vgpr_alloc(program, new_demand.vgpr) + program->config->num_shared_vgprs / 2;
-      program->num_waves =
-         std::min<uint16_t>(program->num_waves, program->dev.physical_vgprs / vgpr_demand);
-      program->num_waves = std::min(program->num_waves, program->dev.max_waves_per_simd);
-
-      /* Adjust for LDS and workgroup multiples and calculate max_reg_demand */
-      program->num_waves = max_suitable_waves(program, program->num_waves);
-      program->max_reg_demand = get_addr_regs_from_waves(program, program->num_waves);
-   }
-}
+            if (LIKELY(program->progress < CompilationProgress::after_ra))
+                  update_vgpr_sgpr_demand(program, program->max_reg_demand);
+      }
 
-void
-live_var_analysis(Program* program)
-{
-   program->live.live_in.clear();
-   program->live.memory.release();
-   program->live.live_in.resize(program->blocks.size(), IDSet(program->live.memory));
-   program->max_reg_demand = RegisterDemand();
-   program->needs_vcc = program->gfx_level >= GFX10;
-
-   live_ctx ctx;
-   ctx.program = program;
-   ctx.worklist = program->blocks.size() - 1;
-   ctx.handled_once = program->blocks.size();
-
-   /* this implementation assumes that the block idx corresponds to the block's position in
-    * program->blocks vector */
-   while (ctx.worklist >= 0) {
-      process_live_temps_per_block(ctx, &program->blocks[ctx.worklist--]);
-   }
-
-   /* calculate the program's register demand and number of waves */
-   if (program->progress < CompilationProgress::after_ra)
-      update_vgpr_sgpr_demand(program, program->max_reg_demand);
 }
-
-} // namespace aco
