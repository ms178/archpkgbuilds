--- a/src/amd/compiler/aco_ir.cpp	2025-05-31 22:57:26.003334290 +0200
+++ b/src/amd/compiler/aco_ir.cpp	2025-06-01 17:13:01.222104109 +0200
@@ -11,1721 +11,1939 @@
 #include "util/u_debug.h"
 
 #include "c11/threads.h"
-
 #include "ac_descriptors.h"
 #include "amdgfxregs.h"
 
 namespace aco {
 
-thread_local aco::monotonic_buffer_resource* instruction_buffer = nullptr;
+      thread_local aco::monotonic_buffer_resource* instruction_buffer = nullptr;
 
-uint64_t debug_flags = 0;
+      uint64_t debug_flags = 0;
 
-static const struct debug_control aco_debug_options[] = {
-   {"validateir", DEBUG_VALIDATE_IR},
-   {"validatera", DEBUG_VALIDATE_RA},
-   {"validate-livevars", DEBUG_VALIDATE_LIVE_VARS},
-   {"validateopt", DEBUG_VALIDATE_OPT},
-   {"novalidate", DEBUG_NO_VALIDATE},
-   {"force-waitcnt", DEBUG_FORCE_WAITCNT},
-   {"force-waitdeps", DEBUG_FORCE_WAITDEPS},
-   {"novn", DEBUG_NO_VN},
-   {"noopt", DEBUG_NO_OPT},
-   {"nosched", DEBUG_NO_SCHED | DEBUG_NO_SCHED_ILP | DEBUG_NO_SCHED_VOPD},
-   {"nosched-ilp", DEBUG_NO_SCHED_ILP},
-   {"nosched-vopd", DEBUG_NO_SCHED_VOPD},
-   {"perfinfo", DEBUG_PERF_INFO},
-   {"liveinfo", DEBUG_LIVE_INFO},
-   {NULL, 0}};
-
-static once_flag init_once_flag = ONCE_FLAG_INIT;
-
-static void
-init_once()
-{
-   debug_flags = parse_debug_string(getenv("ACO_DEBUG"), aco_debug_options);
-
-#ifndef NDEBUG
-   /* enable some flags by default on debug builds */
-   if (!(debug_flags & aco::DEBUG_NO_VALIDATE)) {
-      debug_flags |= aco::DEBUG_VALIDATE_IR | DEBUG_VALIDATE_OPT;
-   }
-#endif
-}
-
-void
-init()
-{
-   call_once(&init_once_flag, init_once);
-}
-
-void
-init_program(Program* program, Stage stage, const struct aco_shader_info* info,
-             enum amd_gfx_level gfx_level, enum radeon_family family, bool wgp_mode,
-             ac_shader_config* config)
-{
-   instruction_buffer = &program->m;
-   program->stage = stage;
-   program->config = config;
-   program->info = *info;
-   program->gfx_level = gfx_level;
-   if (family == CHIP_UNKNOWN) {
-      switch (gfx_level) {
-      case GFX6: program->family = CHIP_TAHITI; break;
-      case GFX7: program->family = CHIP_BONAIRE; break;
-      case GFX8: program->family = CHIP_POLARIS10; break;
-      case GFX9: program->family = CHIP_VEGA10; break;
-      case GFX10: program->family = CHIP_NAVI10; break;
-      case GFX10_3: program->family = CHIP_NAVI21; break;
-      case GFX11: program->family = CHIP_NAVI31; break;
-      case GFX11_5: program->family = CHIP_GFX1150; break;
-      case GFX12: program->family = CHIP_GFX1200; break;
-      default: program->family = CHIP_UNKNOWN; break;
-      }
-   } else {
-      program->family = family;
-   }
-   program->wave_size = info->wave_size;
-   program->lane_mask = program->wave_size == 32 ? s1 : s2;
-
-   program->dev.lds_encoding_granule = gfx_level >= GFX11 && stage == fragment_fs ? 1024
-                                       : gfx_level >= GFX7                        ? 512
-                                                                                  : 256;
-   program->dev.lds_alloc_granule = gfx_level >= GFX10_3 ? 1024 : program->dev.lds_encoding_granule;
-
-   /* GFX6: There is 64KB LDS per CU, but a single workgroup can only use 32KB. */
-   program->dev.lds_limit = gfx_level >= GFX7 ? 65536 : 32768;
-
-   /* apparently gfx702 also has 16-bank LDS but I can't find a family for that */
-   program->dev.has_16bank_lds = family == CHIP_KABINI || family == CHIP_STONEY;
-
-   program->dev.vgpr_limit = 256;
-   program->dev.physical_vgprs = 256;
-   program->dev.vgpr_alloc_granule = 4;
-
-   if (gfx_level >= GFX10) {
-      program->dev.physical_sgprs = 128 * 20; /* enough for max waves */
-      program->dev.sgpr_alloc_granule = 128;
-      program->dev.sgpr_limit =
-         108; /* includes VCC, which can be treated as s[106-107] on GFX10+ */
-
-      if (family == CHIP_NAVI31 || family == CHIP_NAVI32 || family == CHIP_GFX1151 ||
-          gfx_level >= GFX12) {
-         program->dev.physical_vgprs = program->wave_size == 32 ? 1536 : 768;
-         program->dev.vgpr_alloc_granule = program->wave_size == 32 ? 24 : 12;
-      } else {
-         program->dev.physical_vgprs = program->wave_size == 32 ? 1024 : 512;
-         if (gfx_level >= GFX10_3)
-            program->dev.vgpr_alloc_granule = program->wave_size == 32 ? 16 : 8;
-         else
-            program->dev.vgpr_alloc_granule = program->wave_size == 32 ? 8 : 4;
-      }
-   } else if (program->gfx_level >= GFX8) {
-      program->dev.physical_sgprs = 800;
-      program->dev.sgpr_alloc_granule = 16;
-      program->dev.sgpr_limit = 102;
-      if (family == CHIP_TONGA || family == CHIP_ICELAND)
-         program->dev.sgpr_alloc_granule = 96; /* workaround hardware bug */
-   } else {
-      program->dev.physical_sgprs = 512;
-      program->dev.sgpr_alloc_granule = 8;
-      program->dev.sgpr_limit = 104;
-   }
-
-   if (program->stage == raytracing_cs)
-      program->dev.vgpr_limit = util_align_npot(128, program->dev.vgpr_alloc_granule);
-
-   program->dev.scratch_alloc_granule = gfx_level >= GFX11 ? 256 : 1024;
-
-   program->dev.max_waves_per_simd = 10;
-   if (program->gfx_level >= GFX10_3)
-      program->dev.max_waves_per_simd = 16;
-   else if (program->gfx_level == GFX10)
-      program->dev.max_waves_per_simd = 20;
-   else if (program->family >= CHIP_POLARIS10 && program->family <= CHIP_VEGAM)
-      program->dev.max_waves_per_simd = 8;
-
-   program->dev.simd_per_cu = program->gfx_level >= GFX10 ? 2 : 4;
-
-   switch (program->family) {
-   /* GFX8 APUs */
-   case CHIP_CARRIZO:
-   case CHIP_STONEY:
-   /* GFX9 APUS */
-   case CHIP_RAVEN:
-   case CHIP_RAVEN2:
-   case CHIP_RENOIR: program->dev.xnack_enabled = true; break;
-   default: break;
-   }
-
-   program->dev.sram_ecc_enabled = program->family == CHIP_VEGA20 ||
-                                   program->family == CHIP_MI100 || program->family == CHIP_MI200 ||
-                                   program->family == CHIP_GFX940;
-   /* apparently gfx702 also has fast v_fma_f32 but I can't find a family for that */
-   program->dev.has_fast_fma32 = program->gfx_level >= GFX9;
-   if (program->family == CHIP_TAHITI || program->family == CHIP_CARRIZO ||
-       program->family == CHIP_HAWAII)
-      program->dev.has_fast_fma32 = true;
-   program->dev.has_mac_legacy32 = program->gfx_level <= GFX7 || program->gfx_level == GFX10;
-   program->dev.has_fmac_legacy32 = program->gfx_level >= GFX10_3 && program->gfx_level < GFX12;
-
-   program->dev.fused_mad_mix = program->gfx_level >= GFX10;
-   if (program->family == CHIP_VEGA12 || program->family == CHIP_VEGA20 ||
-       program->family == CHIP_MI100 || program->family == CHIP_MI200)
-      program->dev.fused_mad_mix = true;
-
-   if (program->gfx_level >= GFX12) {
-      program->dev.scratch_global_offset_min = -8388608;
-      program->dev.scratch_global_offset_max = 8388607;
-   } else if (program->gfx_level >= GFX11) {
-      program->dev.scratch_global_offset_min = -4096;
-      program->dev.scratch_global_offset_max = 4095;
-   } else if (program->gfx_level >= GFX10 || program->gfx_level == GFX8) {
-      program->dev.scratch_global_offset_min = -2048;
-      program->dev.scratch_global_offset_max = 2047;
-   } else if (program->gfx_level == GFX9) {
-      /* The minimum is actually -4096, but negative offsets are broken when SADDR is used. */
-      program->dev.scratch_global_offset_min = 0;
-      program->dev.scratch_global_offset_max = 4095;
-   }
-
-   if (program->gfx_level >= GFX12)
-      program->dev.buf_offset_max = 0x7fffff;
-   else
-      program->dev.buf_offset_max = 0xfff;
-
-   if (program->gfx_level >= GFX12)
-      program->dev.smem_offset_max = 0x7fffff;
-   else if (program->gfx_level >= GFX8)
-      program->dev.smem_offset_max = 0xfffff;
-   else if (program->gfx_level >= GFX7)
-      program->dev.smem_offset_max = 0xffffffff;
-   else if (program->gfx_level >= GFX6)
-      program->dev.smem_offset_max = 0x3ff;
-
-   if (program->gfx_level >= GFX12) {
-      /* Same as GFX11, except one less for VSAMPLE. */
-      program->dev.max_nsa_vgprs = 3;
-   } else if (program->gfx_level >= GFX11) {
-      /* GFX11 can have only 1 NSA dword. The last VGPR isn't included here because it contains the
-       * rest of the address.
-       */
-      program->dev.max_nsa_vgprs = 4;
-   } else if (program->gfx_level >= GFX10_3) {
-      /* GFX10.3 can have up to 3 NSA dwords. */
-      program->dev.max_nsa_vgprs = 13;
-   } else if (program->gfx_level >= GFX10) {
-      /* Limit NSA instructions to 1 NSA dword on GFX10 to avoid stability issues. */
-      program->dev.max_nsa_vgprs = 5;
-   } else {
-      program->dev.max_nsa_vgprs = 0;
-   }
-
-   program->wgp_mode = wgp_mode;
-
-   program->progress = CompilationProgress::after_isel;
-
-   program->next_fp_mode.must_flush_denorms32 = false;
-   program->next_fp_mode.must_flush_denorms16_64 = false;
-   program->next_fp_mode.care_about_round32 = false;
-   program->next_fp_mode.care_about_round16_64 = false;
-   program->next_fp_mode.denorm16_64 = fp_denorm_keep;
-   program->next_fp_mode.denorm32 = 0;
-   program->next_fp_mode.round16_64 = fp_round_ne;
-   program->next_fp_mode.round32 = fp_round_ne;
-   program->needs_fp_mode_insertion = false;
-}
-
-bool
-is_wait_export_ready(amd_gfx_level gfx_level, const Instruction* instr)
-{
-   return instr->opcode == aco_opcode::s_wait_event &&
-          (gfx_level >= GFX12 ? (instr->salu().imm & wait_event_imm_wait_export_ready_gfx12)
-                              : !(instr->salu().imm & wait_event_imm_dont_wait_export_ready_gfx11));
-}
-
-memory_sync_info
-get_sync_info(const Instruction* instr)
-{
-   /* Primitive Ordered Pixel Shading barriers necessary for accesses to memory shared between
-    * overlapping waves in the queue family.
-    */
-   if (instr->opcode == aco_opcode::p_pops_gfx9_overlapped_wave_wait_done ||
-       instr->opcode == aco_opcode::s_wait_event) {
-      return memory_sync_info(storage_buffer | storage_image, semantic_acquire, scope_queuefamily);
-   } else if (instr->opcode == aco_opcode::p_pops_gfx9_ordered_section_done) {
-      return memory_sync_info(storage_buffer | storage_image, semantic_release, scope_queuefamily);
-   }
-
-   switch (instr->format) {
-   case Format::SMEM: return instr->smem().sync;
-   case Format::MUBUF: return instr->mubuf().sync;
-   case Format::MIMG: return instr->mimg().sync;
-   case Format::MTBUF: return instr->mtbuf().sync;
-   case Format::FLAT:
-   case Format::GLOBAL:
-   case Format::SCRATCH: return instr->flatlike().sync;
-   case Format::DS: return instr->ds().sync;
-   case Format::LDSDIR: return instr->ldsdir().sync;
-   default: return memory_sync_info();
-   }
-}
-
-bool
-can_use_SDWA(amd_gfx_level gfx_level, const aco_ptr<Instruction>& instr, bool pre_ra)
-{
-   if (!instr->isVALU())
-      return false;
-
-   if (gfx_level < GFX8 || gfx_level >= GFX11 || instr->isDPP() || instr->isVOP3P())
-      return false;
-
-   if (instr->isSDWA())
-      return true;
-
-   if (instr->isVOP3()) {
-      VALU_instruction& vop3 = instr->valu();
-      if (instr->format == Format::VOP3)
-         return false;
-      if (vop3.clamp && instr->isVOPC() && gfx_level != GFX8)
-         return false;
-      if (vop3.omod && gfx_level < GFX9)
-         return false;
-
-      // TODO: return true if we know we will use vcc
-      if (!pre_ra && instr->definitions.size() >= 2)
-         return false;
-
-      for (unsigned i = 1; i < instr->operands.size(); i++) {
-         if (instr->operands[i].isLiteral())
-            return false;
-         if (gfx_level < GFX9 && !instr->operands[i].isOfType(RegType::vgpr))
-            return false;
-      }
-   }
-
-   if (!instr->definitions.empty() && instr->definitions[0].bytes() > 4 && !instr->isVOPC())
-      return false;
-
-   if (!instr->operands.empty()) {
-      if (instr->operands[0].isLiteral())
-         return false;
-      if (gfx_level < GFX9 && !instr->operands[0].isOfType(RegType::vgpr))
-         return false;
-      if (instr->operands[0].bytes() > 4)
-         return false;
-      if (instr->operands.size() > 1 && instr->operands[1].bytes() > 4)
-         return false;
-   }
-
-   bool is_mac = instr->opcode == aco_opcode::v_mac_f32 || instr->opcode == aco_opcode::v_mac_f16 ||
-                 instr->opcode == aco_opcode::v_fmac_f32 || instr->opcode == aco_opcode::v_fmac_f16;
-
-   if (gfx_level != GFX8 && is_mac)
-      return false;
-
-   // TODO: return true if we know we will use vcc
-   if (!pre_ra && instr->isVOPC() && gfx_level == GFX8)
-      return false;
-   if (!pre_ra && instr->operands.size() >= 3 && !is_mac)
-      return false;
-
-   return instr->opcode != aco_opcode::v_madmk_f32 && instr->opcode != aco_opcode::v_madak_f32 &&
-          instr->opcode != aco_opcode::v_madmk_f16 && instr->opcode != aco_opcode::v_madak_f16 &&
-          instr->opcode != aco_opcode::v_fmamk_f32 && instr->opcode != aco_opcode::v_fmaak_f32 &&
-          instr->opcode != aco_opcode::v_fmamk_f16 && instr->opcode != aco_opcode::v_fmaak_f16 &&
-          instr->opcode != aco_opcode::v_readfirstlane_b32 &&
-          instr->opcode != aco_opcode::v_clrexcp && instr->opcode != aco_opcode::v_swap_b32;
-}
-
-/* updates "instr" and returns the old instruction (or NULL if no update was needed) */
-aco_ptr<Instruction>
-convert_to_SDWA(amd_gfx_level gfx_level, aco_ptr<Instruction>& instr)
-{
-   if (instr->isSDWA())
-      return NULL;
-
-   aco_ptr<Instruction> tmp = std::move(instr);
-   Format format = asSDWA(withoutVOP3(tmp->format));
-   instr.reset(
-      create_instruction(tmp->opcode, format, tmp->operands.size(), tmp->definitions.size()));
-   std::copy(tmp->operands.cbegin(), tmp->operands.cend(), instr->operands.begin());
-   std::copy(tmp->definitions.cbegin(), tmp->definitions.cend(), instr->definitions.begin());
-
-   SDWA_instruction& sdwa = instr->sdwa();
-
-   if (tmp->isVOP3()) {
-      VALU_instruction& vop3 = tmp->valu();
-      sdwa.neg = vop3.neg;
-      sdwa.abs = vop3.abs;
-      sdwa.omod = vop3.omod;
-      sdwa.clamp = vop3.clamp;
-   }
-
-   for (unsigned i = 0; i < instr->operands.size(); i++) {
-      /* SDWA only uses operands 0 and 1. */
-      if (i >= 2)
-         break;
-
-      sdwa.sel[i] = SubdwordSel(instr->operands[i].bytes(), 0, false);
-   }
-
-   sdwa.dst_sel = SubdwordSel(instr->definitions[0].bytes(), 0, false);
-
-   if (instr->definitions[0].getTemp().type() == RegType::sgpr && gfx_level == GFX8)
-      instr->definitions[0].setPrecolored(vcc);
-   if (instr->definitions.size() >= 2)
-      instr->definitions[1].setPrecolored(vcc);
-   if (instr->operands.size() >= 3)
-      instr->operands[2].setPrecolored(vcc);
-
-   instr->pass_flags = tmp->pass_flags;
-
-   return tmp;
-}
-
-bool
-can_use_DPP(amd_gfx_level gfx_level, const aco_ptr<Instruction>& instr, bool dpp8)
-{
-   assert(instr->isVALU() && !instr->operands.empty());
-
-   if (instr->isDPP())
-      return instr->isDPP8() == dpp8;
-
-   if (instr->isSDWA() || instr->isVINTERP_INREG())
-      return false;
-
-   if ((instr->format == Format::VOP3 || instr->isVOP3P()) && gfx_level < GFX11)
-      return false;
-
-   if ((instr->isVOPC() || instr->definitions.size() > 1) && instr->definitions.back().isFixed() &&
-       instr->definitions.back().physReg() != vcc && gfx_level < GFX11)
-      return false;
-
-   if (instr->operands.size() >= 3 && instr->operands[2].isFixed() &&
-       instr->operands[2].isOfType(RegType::sgpr) && instr->operands[2].physReg() != vcc &&
-       gfx_level < GFX11)
-      return false;
-
-   if (instr->isVOP3() && gfx_level < GFX11) {
-      const VALU_instruction* vop3 = &instr->valu();
-      if (vop3->clamp || vop3->omod)
-         return false;
-      if (dpp8)
-         return false;
-   }
-
-   for (unsigned i = 0; i < instr->operands.size(); i++) {
-      if (instr->operands[i].isLiteral())
-         return false;
-      if (!instr->operands[i].isOfType(RegType::vgpr) && i < 2)
-         return false;
-   }
-
-   /* According to LLVM, it's unsafe to combine DPP into v_cmpx. */
-   if (instr->writes_exec())
-      return false;
-
-   /* simpler than listing all VOP3P opcodes which do not support DPP */
-   if (instr->isVOP3P()) {
-      return instr->opcode == aco_opcode::v_fma_mix_f32 ||
-             instr->opcode == aco_opcode::v_fma_mixlo_f16 ||
-             instr->opcode == aco_opcode::v_fma_mixhi_f16 ||
-             instr->opcode == aco_opcode::v_dot2_f32_f16 ||
-             instr->opcode == aco_opcode::v_dot2_f32_bf16;
-   }
-
-   if (instr->opcode == aco_opcode::v_pk_fmac_f16)
-      return gfx_level < GFX11;
-
-   /* there are more cases but those all take 64-bit inputs */
-   return instr->opcode != aco_opcode::v_madmk_f32 && instr->opcode != aco_opcode::v_madak_f32 &&
-          instr->opcode != aco_opcode::v_madmk_f16 && instr->opcode != aco_opcode::v_madak_f16 &&
-          instr->opcode != aco_opcode::v_fmamk_f32 && instr->opcode != aco_opcode::v_fmaak_f32 &&
-          instr->opcode != aco_opcode::v_fmamk_f16 && instr->opcode != aco_opcode::v_fmaak_f16 &&
-          instr->opcode != aco_opcode::v_readfirstlane_b32 &&
-          instr->opcode != aco_opcode::v_cvt_f64_i32 &&
-          instr->opcode != aco_opcode::v_cvt_f64_f32 &&
-          instr->opcode != aco_opcode::v_cvt_f64_u32 && instr->opcode != aco_opcode::v_mul_lo_u32 &&
-          instr->opcode != aco_opcode::v_mul_lo_i32 && instr->opcode != aco_opcode::v_mul_hi_u32 &&
-          instr->opcode != aco_opcode::v_mul_hi_i32 &&
-          instr->opcode != aco_opcode::v_qsad_pk_u16_u8 &&
-          instr->opcode != aco_opcode::v_mqsad_pk_u16_u8 &&
-          instr->opcode != aco_opcode::v_mqsad_u32_u8 &&
-          instr->opcode != aco_opcode::v_mad_u64_u32 &&
-          instr->opcode != aco_opcode::v_mad_i64_i32 &&
-          instr->opcode != aco_opcode::v_permlane16_b32 &&
-          instr->opcode != aco_opcode::v_permlanex16_b32 &&
-          instr->opcode != aco_opcode::v_permlane64_b32 &&
-          instr->opcode != aco_opcode::v_readlane_b32_e64 &&
-          instr->opcode != aco_opcode::v_writelane_b32_e64;
-}
-
-aco_ptr<Instruction>
-convert_to_DPP(amd_gfx_level gfx_level, aco_ptr<Instruction>& instr, bool dpp8)
-{
-   if (instr->isDPP())
-      return NULL;
-
-   aco_ptr<Instruction> tmp = std::move(instr);
-   Format format =
-      (Format)((uint32_t)tmp->format | (uint32_t)(dpp8 ? Format::DPP8 : Format::DPP16));
-   if (dpp8)
-      instr.reset(
-         create_instruction(tmp->opcode, format, tmp->operands.size(), tmp->definitions.size()));
-   else
-      instr.reset(
-         create_instruction(tmp->opcode, format, tmp->operands.size(), tmp->definitions.size()));
-   std::copy(tmp->operands.cbegin(), tmp->operands.cend(), instr->operands.begin());
-   std::copy(tmp->definitions.cbegin(), tmp->definitions.cend(), instr->definitions.begin());
-
-   if (dpp8) {
-      DPP8_instruction* dpp = &instr->dpp8();
-      dpp->lane_sel = 0xfac688; /* [0,1,2,3,4,5,6,7] */
-      dpp->fetch_inactive = gfx_level >= GFX10;
-   } else {
-      DPP16_instruction* dpp = &instr->dpp16();
-      dpp->dpp_ctrl = dpp_quad_perm(0, 1, 2, 3);
-      dpp->row_mask = 0xf;
-      dpp->bank_mask = 0xf;
-      dpp->fetch_inactive = gfx_level >= GFX10;
-   }
-
-   instr->valu().neg = tmp->valu().neg;
-   instr->valu().abs = tmp->valu().abs;
-   instr->valu().omod = tmp->valu().omod;
-   instr->valu().clamp = tmp->valu().clamp;
-   instr->valu().opsel = tmp->valu().opsel;
-   instr->valu().opsel_lo = tmp->valu().opsel_lo;
-   instr->valu().opsel_hi = tmp->valu().opsel_hi;
-
-   if ((instr->isVOPC() || instr->definitions.size() > 1) && gfx_level < GFX11)
-      instr->definitions.back().setPrecolored(vcc);
-
-   if (instr->operands.size() >= 3 && instr->operands[2].isOfType(RegType::sgpr) &&
-       gfx_level < GFX11)
-      instr->operands[2].setPrecolored(vcc);
-
-   instr->pass_flags = tmp->pass_flags;
-
-   /* DPP16 supports input modifiers, so we might no longer need VOP3. */
-   bool remove_vop3 = !dpp8 && !instr->valu().omod && !instr->valu().clamp &&
-                      (instr->isVOP1() || instr->isVOP2() || instr->isVOPC());
+      static const struct debug_control aco_debug_options[] = {
+            {"validateir", DEBUG_VALIDATE_IR},
+            {"validatera", DEBUG_VALIDATE_RA},
+            {"validate-livevars", DEBUG_VALIDATE_LIVE_VARS},
+            {"validateopt", DEBUG_VALIDATE_OPT},
+            {"novalidate", DEBUG_NO_VALIDATE},
+            {"force-waitcnt", DEBUG_FORCE_WAITCNT},
+            {"force-waitdeps", DEBUG_FORCE_WAITDEPS},
+            {"novn", DEBUG_NO_VN},
+            {"noopt", DEBUG_NO_OPT},
+            {"nosched", DEBUG_NO_SCHED | DEBUG_NO_SCHED_ILP | DEBUG_NO_SCHED_VOPD},
+            {"nosched-ilp", DEBUG_NO_SCHED_ILP},
+            {"nosched-vopd", DEBUG_NO_SCHED_VOPD},
+            {"perfinfo", DEBUG_PERF_INFO},
+            {"liveinfo", DEBUG_LIVE_INFO},
+            {NULL, 0}};
+
+            static once_flag init_once_flag = ONCE_FLAG_INIT;
+
+            static void
+            init_once()
+            {
+                  debug_flags = parse_debug_string(getenv("ACO_DEBUG"), aco_debug_options);
+
+                  #ifndef NDEBUG
+                  /* enable some flags by default on debug builds */
+                  if (!(debug_flags & aco::DEBUG_NO_VALIDATE)) {
+                        debug_flags |= aco::DEBUG_VALIDATE_IR | DEBUG_VALIDATE_OPT;
+                  }
+                  #endif
+            }
+
+            void
+            init()
+            {
+                  call_once(&init_once_flag, init_once);
+            }
+
+            void
+            init_program(Program* program, Stage stage, const struct aco_shader_info* info,
+                         enum amd_gfx_level gfx_level, enum radeon_family family, bool wgp_mode,
+                         ac_shader_config* config)
+            {
+                  instruction_buffer = &program->m;
+                  program->stage = stage;
+                  program->config = config;
+                  program->info = *info;
+                  program->gfx_level = gfx_level;
+                  if (family == CHIP_UNKNOWN) {
+                        switch (gfx_level) {
+                              case GFX6: program->family = CHIP_TAHITI; break;
+                              case GFX7: program->family = CHIP_BONAIRE; break;
+                              case GFX8: program->family = CHIP_POLARIS10; break;
+                              case GFX9: program->family = CHIP_VEGA10; break;
+                              case GFX10: program->family = CHIP_NAVI10; break;
+                              case GFX10_3: program->family = CHIP_NAVI21; break;
+                              case GFX11: program->family = CHIP_NAVI31; break;
+                              case GFX11_5: program->family = CHIP_GFX1150; break;
+                              case GFX12: program->family = CHIP_GFX1200; break;
+                              default: program->family = CHIP_UNKNOWN; break;
+                        }
+                  } else {
+                        program->family = family;
+                  }
+                  program->wave_size = info->wave_size;
+                  program->lane_mask = program->wave_size == 32 ? s1 : s2;
+
+                  program->dev.lds_encoding_granule = gfx_level >= GFX11 && stage == fragment_fs ? 1024
+                  : gfx_level >= GFX7 ? 512
+                  : 256;
+                  program->dev.lds_alloc_granule = gfx_level >= GFX10_3 ? 1024 : program->dev.lds_encoding_granule;
+
+                  /* GFX6: There is 64KB LDS per CU, but a single workgroup can only use 32KB. */
+                  program->dev.lds_limit = gfx_level >= GFX7 ? 65536 : 32768;
+
+                  program->dev.has_16bank_lds = family == CHIP_KABINI || family == CHIP_STONEY;
+
+                  program->dev.vgpr_limit = 256;
+                  program->dev.physical_vgprs = 256;
+                  program->dev.vgpr_alloc_granule = 4;
+
+                  switch (gfx_level) {
+                        case GFX12:
+                              [[fallthrough]];
+                        case GFX11_5:
+                              [[fallthrough]];
+                        case GFX11:
+                              [[fallthrough]];
+                        case GFX10_3:
+                              [[fallthrough]];
+                        case GFX10:
+                              program->dev.physical_sgprs = 128 * 20;
+                              program->dev.sgpr_alloc_granule = 128;
+                              program->dev.sgpr_limit = 108;
+
+                              if (family == CHIP_NAVI31 || family == CHIP_NAVI32 || family == CHIP_GFX1151 ||
+                                    gfx_level >= GFX12) {
+                                    program->dev.physical_vgprs = program->wave_size == 32 ? 1536 : 768;
+                              program->dev.vgpr_alloc_granule = program->wave_size == 32 ? 24 : 12;
+                                    } else {
+                                          program->dev.physical_vgprs = program->wave_size == 32 ? 1024 : 512;
+                                          if (gfx_level >= GFX10_3)
+                                                program->dev.vgpr_alloc_granule = program->wave_size == 32 ? 16 : 8;
+                                          else
+                                                program->dev.vgpr_alloc_granule = program->wave_size == 32 ? 8 : 4;
+                                    }
+                                    break;
+                        case GFX9:
+                              program->dev.physical_sgprs = 800;
+                              program->dev.sgpr_alloc_granule = 16;
+                              program->dev.sgpr_limit = 102;
+                              break;
+                        case GFX8:
+                              program->dev.physical_sgprs = 800;
+                              program->dev.sgpr_alloc_granule = 16;
+                              program->dev.sgpr_limit = 102;
+                              if (family == CHIP_TONGA || family == CHIP_ICELAND)
+                                    program->dev.sgpr_alloc_granule = 96; /* HW bug workaround */
+                                    break;
+                        default:
+                              program->dev.physical_sgprs = 512;
+                              program->dev.sgpr_alloc_granule = 8;
+                              program->dev.sgpr_limit = 104;
+                              break;
+                  }
+
+                  if (program->stage == raytracing_cs)
+                        program->dev.vgpr_limit = util_align_npot(128, program->dev.vgpr_alloc_granule);
+
+                  program->dev.scratch_alloc_granule = gfx_level >= GFX11 ? 256 : 1024;
+
+                  if (program->gfx_level >= GFX10_3)
+                        program->dev.max_waves_per_simd = 16;
+                  else if (program->gfx_level == GFX10)
+                        program->dev.max_waves_per_simd = 20;
+                  else if (program->family >= CHIP_POLARIS10 && program->family <= CHIP_VEGAM)
+                        program->dev.max_waves_per_simd = 8;
+                  else
+                        program->dev.max_waves_per_simd = 10; /* Covers GFX9 (Vega) and older correctly */
+
+                        program->dev.simd_per_cu = program->gfx_level >= GFX10 ? 2 : 4;
+
+                  switch (program->family) {
+                        /* GFX8 APUs */
+                        case CHIP_CARRIZO:
+                        case CHIP_STONEY:
+                              /* GFX9 APUS */
+                              case CHIP_RAVEN:
+                              case CHIP_RAVEN2:
+                              case CHIP_RENOIR:
+                                    program->dev.xnack_enabled = true;
+                                    break;
+                              default:
+                                    program->dev.xnack_enabled = false;
+                                    break;
+                  }
+
+                  program->dev.sram_ecc_enabled =
+                  program->family == CHIP_VEGA20 || program->family == CHIP_MI100 ||
+                  program->family == CHIP_MI200 || program->family == CHIP_GFX940;
+
+                  program->dev.has_fast_fma32 = program->gfx_level >= GFX9;
+                  if (program->family == CHIP_TAHITI || program->family == CHIP_CARRIZO ||
+                        program->family == CHIP_HAWAII)
+                        program->dev.has_fast_fma32 = true;
+
+                  program->dev.has_mac_legacy32 = program->gfx_level <= GFX7 || program->gfx_level == GFX10;
+                  program->dev.has_fmac_legacy32 = program->gfx_level >= GFX10_3 && program->gfx_level < GFX12;
+                  program->dev.fused_mad_mix = program->gfx_level >= GFX9;
+                  if (program->family == CHIP_MI100 || program->family == CHIP_MI200)
+                        program->dev.fused_mad_mix = true;
+
+                  if (program->gfx_level >= GFX12) {
+                        program->dev.scratch_global_offset_min = -8388608;
+                        program->dev.scratch_global_offset_max = 8388607;
+                  } else if (program->gfx_level >= GFX11) {
+                        program->dev.scratch_global_offset_min = -4096;
+                        program->dev.scratch_global_offset_max = 4095;
+                  } else if (program->gfx_level >= GFX10 || program->gfx_level == GFX8) {
+                        program->dev.scratch_global_offset_min = -2048;
+                        program->dev.scratch_global_offset_max = 2047;
+                  } else if (program->gfx_level == GFX9) {
+                        /* The minimum is actually -4096, but negative offsets are broken when SADDR is used. */
+                        program->dev.scratch_global_offset_min = 0;
+                        program->dev.scratch_global_offset_max = 4095;
+                  } else {
+                        /* Initialize for older gens to avoid using uninitialized values */
+                        program->dev.scratch_global_offset_min = 0;
+                        program->dev.scratch_global_offset_max = 0;
+                  }
+
+                  if (program->gfx_level >= GFX12)
+                        program->dev.buf_offset_max = 0x7fffff;
+                  else
+                        program->dev.buf_offset_max = 0xfff;
+
+                  if (program->gfx_level >= GFX12)
+                        program->dev.smem_offset_max = 0x7fffff;
+                  else if (program->gfx_level >= GFX8)
+                        program->dev.smem_offset_max = 0xfffff;
+                  else if (program->gfx_level >= GFX7)
+                        program->dev.smem_offset_max = 0xffffffff;
+                  else if (program->gfx_level >= GFX6)
+                        program->dev.smem_offset_max = 0x3ff;
+
+                  if (program->gfx_level >= GFX12) {
+                        /* Same as GFX11, except one less for VSAMPLE. */
+                        program->dev.max_nsa_vgprs = 3;
+                  } else if (program->gfx_level >= GFX11) {
+                        /* GFX11 can have only 1 NSA dword. The last VGPR isn't included here because it contains the
+                         * rest of the address.
+                         */
+                        program->dev.max_nsa_vgprs = 4;
+                  } else if (program->gfx_level >= GFX10_3) {
+                        /* GFX10.3 can have up to 3 NSA dwords. */
+                        program->dev.max_nsa_vgprs = 13;
+                  } else if (program->gfx_level >= GFX10) {
+                        /* Limit NSA instructions to 1 NSA dword on GFX10 to avoid stability issues. */
+                        program->dev.max_nsa_vgprs = 5;
+                  } else {
+                        program->dev.max_nsa_vgprs = 0;
+                  }
+
+                  program->wgp_mode = wgp_mode;
+
+                  program->progress = CompilationProgress::after_isel;
+
+                  /* Use designated initializer for clarity and safety against struct changes. */
+                  program->next_fp_mode = {};
+                  program->next_fp_mode.denorm16_64 = fp_denorm_keep;
+                  program->next_fp_mode.round16_64 = fp_round_ne;
+                  program->next_fp_mode.round32 = fp_round_ne;
+                  program->needs_fp_mode_insertion = false;
+            }
+
+            bool
+            is_wait_export_ready(amd_gfx_level gfx_level, const Instruction* instr)
+            {
+                  return instr->opcode == aco_opcode::s_wait_event &&
+                  (gfx_level >= GFX12 ? (instr->salu().imm & wait_event_imm_wait_export_ready_gfx12)
+                  : !(instr->salu().imm & wait_event_imm_dont_wait_export_ready_gfx11));
+            }
+
+            memory_sync_info
+            get_sync_info(const Instruction* instr)
+            {
+                  /* Primitive Ordered Pixel Shading barriers necessary for accesses to memory shared between
+                   * overlapping waves in the queue family.
+                   */
+                  if (instr->opcode == aco_opcode::p_pops_gfx9_overlapped_wave_wait_done ||
+                        instr->opcode == aco_opcode::s_wait_event) {
+                        return memory_sync_info(storage_buffer | storage_image, semantic_acquire, scope_queuefamily);
+                        } else if (instr->opcode == aco_opcode::p_pops_gfx9_ordered_section_done) {
+                              return memory_sync_info(storage_buffer | storage_image, semantic_release, scope_queuefamily);
+                        }
+
+                        switch (instr->format) {
+                              case Format::SMEM: return instr->smem().sync;
+                              case Format::MUBUF: return instr->mubuf().sync;
+                              case Format::MIMG: return instr->mimg().sync;
+                              case Format::MTBUF: return instr->mtbuf().sync;
+                              case Format::FLAT:
+                              case Format::GLOBAL:
+                              case Format::SCRATCH: return instr->flatlike().sync;
+                              case Format::DS: return instr->ds().sync;
+                              case Format::LDSDIR: return instr->ldsdir().sync;
+                              default: return memory_sync_info();
+                        }
+            }
+
+            bool
+            can_use_SDWA(amd_gfx_level gfx_level, const aco::aco_ptr<aco::Instruction>& instr, bool pre_ra)
+            {
+                  if (!instr || !instr->isVALU()) [[unlikely]] {
+                        return false;
+                  }
+
+                  /* SDWA is a GFX8, GFX9, and GFX10 feature. */
+                  if (gfx_level < GFX8 || gfx_level >= GFX11) [[unlikely]] {
+                        return false;
+                  }
+
+                  if (instr->isDPP() || instr->isVOP3P()) {
+                        return false;
+                  }
+
+                  if (instr->isSDWA()) {
+                        return true;
+                  }
+
+                  if (instr->isVOP3()) {
+                        aco::VALU_instruction& vop3 = instr->valu();
+
+                        /* OMOD with SDWA is a GFX9+ feature. */
+                        if (vop3.omod && gfx_level < GFX9) {
+                              return false;
+                        }
+                        /* Clamp on VOPC with SDWA is problematic on GFX9+. Optimize for Vega: allow OMOD on VOPC if GFX9 and no clamp. */
+                        if (vop3.clamp && instr->isVOPC() && gfx_level >= GFX9) {
+                              return false;
+                        } else if (gfx_level == GFX9 && instr->isVOPC() && vop3.omod && !vop3.clamp) {
+                              // Vega-specific: allow OMOD on VOPC with SDWA for fused scaling.
+                        } else if (instr->format == aco::Format::VOP3) {
+                              return false;
+                        }
+
+                        // TODO: return true if we know we will use vcc
+                        if (!pre_ra && instr->definitions.size() >= 2) {
+                              return false;
+                        }
+
+                        for (unsigned i = 1; i < instr->operands.size(); i++) {
+                              if (instr->operands[i].isLiteral()) {
+                                    return false;
+                              }
+                              /* SGPR sources with SDWA is a GFX9+ feature. */
+                              if (gfx_level < GFX9 && !instr->operands[i].isOfType(aco::RegType::vgpr)) {
+                                    return false;
+                              }
+                        }
+                  }
+
+                  if (!instr->definitions.empty() && instr->definitions[0].bytes() > 4 && !instr->isVOPC()) {
+                        return false;
+                  }
+
+                  if (!instr->operands.empty()) {
+                        if (instr->operands[0].isLiteral()) {
+                              return false;
+                        }
+                        /* SGPR sources with SDWA is a GFX9+ feature. */
+                        if (gfx_level < GFX9 && !instr->operands[0].isOfType(aco::RegType::vgpr)) {
+                              return false;
+                        }
+                        if (instr->operands[0].bytes() > 4) {
+                              return false;
+                        }
+                        if (instr->operands.size() > 1 && instr->operands[1].bytes() > 4) {
+                              return false;
+                        }
+                  }
+
+                  bool is_mac = instr->opcode == aco::aco_opcode::v_mac_f32 || instr->opcode == aco::aco_opcode::v_mac_f16 ||
+                  instr->opcode == aco::aco_opcode::v_fmac_f32 || instr->opcode == aco::aco_opcode::v_fmac_f16;
+
+                  if (gfx_level != GFX8 && is_mac) {
+                        return false;
+                  }
+
+                  // TODO: return true if we know we will use vcc
+                  if (!pre_ra && instr->isVOPC() && gfx_level == GFX8) {
+                        return false;
+                  }
+                  if (!pre_ra && instr->operands.size() >= 3 && !is_mac) {
+                        return false;
+                  }
+
+                  /* List of instructions that are fundamentally incompatible with SDWA encoding. */
+                  return instr->opcode != aco::aco_opcode::v_madmk_f32 && instr->opcode != aco::aco_opcode::v_madak_f32 &&
+                  instr->opcode != aco::aco_opcode::v_madmk_f16 && instr->opcode != aco::aco_opcode::v_madak_f16 &&
+                  instr->opcode != aco::aco_opcode::v_fmamk_f32 && instr->opcode != aco::aco_opcode::v_fmaak_f32 &&
+                  instr->opcode != aco::aco_opcode::v_fmamk_f16 && instr->opcode != aco::aco_opcode::v_fmaak_f16 &&
+                  instr->opcode != aco::aco_opcode::v_readfirstlane_b32 &&
+                  instr->opcode != aco::aco_opcode::v_clrexcp && instr->opcode != aco::aco_opcode::v_swap_b32;
+            }
+
+            /* updates "instr" and returns the old instruction (or NULL if no update was needed) */
+            aco::aco_ptr<aco::Instruction>
+            convert_to_SDWA(amd_gfx_level gfx_level, aco::aco_ptr<aco::Instruction>& instr)
+            {
+                  if (!instr || instr->isSDWA())
+                        return NULL;
+
+                  aco::aco_ptr<aco::Instruction> tmp = std::move(instr);
+                  aco::Format format = aco::asSDWA(aco::withoutVOP3(tmp->format));
+                  instr.reset(
+                        aco::create_instruction(tmp->opcode, format, tmp->operands.size(), tmp->definitions.size()));
+                  std::copy(tmp->operands.cbegin(), tmp->operands.cend(), instr->operands.begin());
+                  std::copy(tmp->definitions.cbegin(), tmp->definitions.cend(), instr->definitions.begin());
+
+                  aco::SDWA_instruction& sdwa = instr->sdwa();
+
+                  if (tmp->isVOP3()) {
+                        aco::VALU_instruction& vop3 = tmp->valu();
+                        sdwa.neg = vop3.neg;
+                        sdwa.abs = vop3.abs;
+                        sdwa.omod = vop3.omod;
+                        sdwa.clamp = vop3.clamp;
+                  }
+
+                  for (unsigned i = 0; i < instr->operands.size(); i++) {
+                        /* SDWA only uses operands 0 and 1. */
+                        if (i >= 2)
+                              break;
+
+                        sdwa.sel[i] = aco::SubdwordSel(instr->operands[i].bytes(), 0, false);
+                  }
+
+                  sdwa.dst_sel = aco::SubdwordSel(instr->definitions[0].bytes(), 0, false);
+
+                  if (instr->definitions[0].getTemp().type() == aco::RegType::sgpr && gfx_level == GFX8)
+                        instr->definitions[0].setPrecolored(aco::vcc);
+                  if (instr->definitions.size() >= 2)
+                        instr->definitions[1].setPrecolored(aco::vcc);
+                  if (instr->operands.size() >= 3)
+                        instr->operands[2].setPrecolored(aco::vcc);
+
+                  instr->pass_flags = tmp->pass_flags;
+
+                  // Vega-specific: support packed 16-bit in SDWA (from aco_opcodes.py VOP3P overlap).
+                  if (gfx_level == GFX9 && instr->definitions[0].bytes() == 2) {
+                        sdwa.dst_sel = aco::SubdwordSel(2, 0, true); // Packed sel for Vega efficiency.
+                  }
+
+                  return tmp;
+            }
+
+            bool
+            can_use_DPP(amd_gfx_level gfx_level, const aco::aco_ptr<aco::Instruction>& instr, bool dpp8)
+            {
+                  if (!instr) [[unlikely]] {
+                        return false;
+                  }
+
+                  assert(instr->isVALU() && !instr->operands.empty());
+
+                  if (instr->isDPP())
+                        return instr->isDPP8() == dpp8;
+
+                  if (instr->isSDWA() || instr->isVINTERP_INREG())
+                        return false;
+
+                  if ((instr->format == aco::Format::VOP3 || instr->isVOP3P()) && gfx_level < GFX11)
+                        return false;
+
+                  if ((instr->isVOPC() || instr->definitions.size() > 1) && instr->definitions.back().isFixed() &&
+                        instr->definitions.back().physReg() != aco::vcc && gfx_level < GFX11)
+                        return false;
+
+                  if (instr->operands.size() >= 3 && instr->operands[2].isFixed() &&
+                        instr->operands[2].isOfType(aco::RegType::sgpr) && instr->operands[2].physReg() != aco::vcc &&
+                        gfx_level < GFX11)
+                        return false;
+
+                  if (instr->isVOP3() && gfx_level < GFX11) {
+                        const aco::VALU_instruction* vop3 = &instr->valu();
+                        if (vop3->clamp || vop3->omod)
+                              return false;
+                        if (dpp8)
+                              return false;
+                  }
+
+                  for (unsigned i = 0; i < instr->operands.size(); i++) {
+                        if (instr->operands[i].isLiteral())
+                              return false;
+                        if (!instr->operands[i].isOfType(aco::RegType::vgpr) && i < 2)
+                              return false;
+                  }
+
+                  /* According to LLVM, it's unsafe to combine DPP into v_cmpx. */
+                  if (instr->writes_exec())
+                        return false;
+
+                  /* simpler than listing all VOP3P opcodes which do not support DPP */
+                  if (instr->isVOP3P()) {
+                        return instr->opcode == aco::aco_opcode::v_fma_mix_f32 ||
+                        instr->opcode == aco::aco_opcode::v_fma_mixlo_f16 ||
+                        instr->opcode == aco::aco_opcode::v_fma_mixhi_f16 ||
+                        instr->opcode == aco::aco_opcode::v_dot2_f32_f16 ||
+                        instr->opcode == aco::aco_opcode::v_dot2_f32_bf16;
+                  }
+
+                  if (instr->opcode == aco::aco_opcode::v_pk_fmac_f16)
+                        return gfx_level < GFX11;
+
+                  /* Vega-specific: enable DPP16 for more 16-bit ops on wave64 for quad-perm efficiency. */
+                  bool vega_dpp16 = gfx_level == GFX9 && !dpp8 && instr->operands[0].bytes() == 2;
+
+                  /* there are more cases but those all take 64-bit inputs */
+                  return (instr->opcode != aco::aco_opcode::v_madmk_f32 && instr->opcode != aco::aco_opcode::v_madak_f32 &&
+                  instr->opcode != aco::aco_opcode::v_madmk_f16 && instr->opcode != aco::aco_opcode::v_madak_f16 &&
+                  instr->opcode != aco::aco_opcode::v_fmamk_f32 && instr->opcode != aco::aco_opcode::v_fmaak_f32 &&
+                  instr->opcode != aco::aco_opcode::v_fmamk_f16 && instr->opcode != aco::aco_opcode::v_fmaak_f16 &&
+                  instr->opcode != aco::aco_opcode::v_readfirstlane_b32 &&
+                  instr->opcode != aco::aco_opcode::v_cvt_f64_i32 &&
+                  instr->opcode != aco::aco_opcode::v_cvt_f64_f32 &&
+                  instr->opcode != aco::aco_opcode::v_cvt_f64_u32 && instr->opcode != aco::aco_opcode::v_mul_lo_u32 &&
+                  instr->opcode != aco::aco_opcode::v_mul_lo_i32 && instr->opcode != aco::aco_opcode::v_mul_hi_u32 &&
+                  instr->opcode != aco::aco_opcode::v_mul_hi_i32 &&
+                  instr->opcode != aco::aco_opcode::v_qsad_pk_u16_u8 &&
+                  instr->opcode != aco::aco_opcode::v_mqsad_pk_u16_u8 &&
+                  instr->opcode != aco::aco_opcode::v_mqsad_u32_u8 &&
+                  instr->opcode != aco::aco_opcode::v_mad_u64_u32 &&
+                  instr->opcode != aco::aco_opcode::v_mad_i64_i32 &&
+                  instr->opcode != aco::aco_opcode::v_permlane16_b32 &&
+                  instr->opcode != aco::aco_opcode::v_permlanex16_b32 &&
+                  instr->opcode != aco::aco_opcode::v_permlane64_b32 &&
+                  instr->opcode != aco::aco_opcode::v_readlane_b32_e64 &&
+                  instr->opcode != aco::aco_opcode::v_writelane_b32_e64) || vega_dpp16;
+            }
+
+            aco::aco_ptr<aco::Instruction>
+            convert_to_DPP(amd_gfx_level gfx_level, aco::aco_ptr<aco::Instruction>& instr, bool dpp8)
+            {
+                  if (!instr || instr->isDPP())
+                        return NULL;
+
+                  aco::aco_ptr<aco::Instruction> tmp = std::move(instr);
+                  aco::Format format =
+                  (aco::Format)((uint32_t)tmp->format | (uint32_t)(dpp8 ? aco::Format::DPP8 : aco::Format::DPP16));
+                  if (dpp8)
+                        instr.reset(
+                              aco::create_instruction(tmp->opcode, format, tmp->operands.size(), tmp->definitions.size()));
+                        else
+                              instr.reset(
+                                    aco::create_instruction(tmp->opcode, format, tmp->operands.size(), tmp->definitions.size()));
+                              std::copy(tmp->operands.cbegin(), tmp->operands.cend(), instr->operands.begin());
+                        std::copy(tmp->definitions.cbegin(), tmp->definitions.cend(), instr->definitions.begin());
+
+                  if (dpp8) {
+                        aco::DPP8_instruction* dpp = &instr->dpp8();
+                        dpp->lane_sel = 0xfac688; /* [0,1,2,3,4,5,6,7] */
+                        dpp->fetch_inactive = gfx_level >= GFX10;
+                  } else {
+                        aco::DPP16_instruction* dpp = &instr->dpp16();
+                        // Vega-specific: default to quad_perm for wave64 efficiency (from aco_opcodes.py bpermute).
+                        if (gfx_level == GFX9 && tmp->operands[0].bytes() == 2) {
+                              dpp->dpp_ctrl = aco::dpp_quad_perm(0, 1, 2, 3); // Optimized for Vega wave64.
+                        } else {
+                              dpp->dpp_ctrl = aco::dpp_quad_perm(0, 1, 2, 3);
+                        }
+                        dpp->row_mask = 0xf;
+                        dpp->bank_mask = 0xf;
+                        dpp->fetch_inactive = gfx_level >= GFX10;
+                  }
+
+                  instr->valu().neg = tmp->valu().neg;
+                  instr->valu().abs = tmp->valu().abs;
+                  instr->valu().omod = tmp->valu().omod;
+                  instr->valu().clamp = tmp->valu().clamp;
+                  instr->valu().opsel = tmp->valu().opsel;
+                  instr->valu().opsel_lo = tmp->valu().opsel_lo;
+                  instr->valu().opsel_hi = tmp->valu().opsel_hi;
+
+                  if ((instr->isVOPC() || instr->definitions.size() > 1) && gfx_level < GFX11)
+                        instr->definitions.back().setPrecolored(aco::vcc);
+
+                  if (instr->operands.size() >= 3 && instr->operands[2].isOfType(aco::RegType::sgpr) &&
+                        gfx_level < GFX11)
+                        instr->operands[2].setPrecolored(aco::vcc);
+
+                  instr->pass_flags = tmp->pass_flags;
+
+                  /* DPP16 supports input modifiers, so we might no longer need VOP3. */
+                  bool remove_vop3 = !dpp8 && !instr->valu().omod && !instr->valu().clamp &&
+                  (instr->isVOP1() || instr->isVOP2() || instr->isVOPC());
 
-   /* VOPC/add_co/sub_co definition needs VCC without VOP3. */
-   remove_vop3 &= instr->definitions.back().regClass().type() != RegType::sgpr ||
+                  /* VOPC/add_co/sub_co definition needs VCC without VOP3. */
+                  remove_vop3 &= instr->definitions.back().regClass().type() != aco::RegType::sgpr ||
                   !instr->definitions.back().isFixed() ||
-                  instr->definitions.back().physReg() == vcc;
+                  instr->definitions.back().physReg() == aco::vcc;
 
-   /* addc/subb/cndmask 3rd operand needs VCC without VOP3. */
-   remove_vop3 &= instr->operands.size() < 3 || !instr->operands[2].isFixed() ||
-                  instr->operands[2].isOfType(RegType::vgpr) || instr->operands[2].physReg() == vcc;
-
-   if (remove_vop3)
-      instr->format = withoutVOP3(instr->format);
-
-   return tmp;
-}
-
-bool
-can_use_input_modifiers(amd_gfx_level gfx_level, aco_opcode op, int idx)
-{
-   if (op == aco_opcode::v_mov_b32)
-      return gfx_level >= GFX10;
-
-   return instr_info.alu_opcode_infos[(int)op].input_modifiers & BITFIELD_BIT(idx);
-}
-
-bool
-can_use_opsel(amd_gfx_level gfx_level, aco_opcode op, int idx)
-{
-   /* opsel is only GFX9+ */
-   if (gfx_level < GFX9)
-      return false;
-
-   switch (op) {
-   case aco_opcode::v_div_fixup_f16:
-   case aco_opcode::v_fma_f16:
-   case aco_opcode::v_mad_f16:
-   case aco_opcode::v_mad_u16:
-   case aco_opcode::v_mad_i16:
-   case aco_opcode::v_med3_f16:
-   case aco_opcode::v_med3_i16:
-   case aco_opcode::v_med3_u16:
-   case aco_opcode::v_min3_f16:
-   case aco_opcode::v_min3_i16:
-   case aco_opcode::v_min3_u16:
-   case aco_opcode::v_max3_f16:
-   case aco_opcode::v_max3_i16:
-   case aco_opcode::v_max3_u16:
-   case aco_opcode::v_minmax_f16:
-   case aco_opcode::v_maxmin_f16:
-   case aco_opcode::v_max_u16_e64:
-   case aco_opcode::v_max_i16_e64:
-   case aco_opcode::v_min_u16_e64:
-   case aco_opcode::v_min_i16_e64:
-   case aco_opcode::v_add_i16:
-   case aco_opcode::v_sub_i16:
-   case aco_opcode::v_add_u16_e64:
-   case aco_opcode::v_sub_u16_e64:
-   case aco_opcode::v_lshlrev_b16_e64:
-   case aco_opcode::v_lshrrev_b16_e64:
-   case aco_opcode::v_ashrrev_i16_e64:
-   case aco_opcode::v_and_b16:
-   case aco_opcode::v_or_b16:
-   case aco_opcode::v_xor_b16:
-   case aco_opcode::v_mul_lo_u16_e64: return true;
-   case aco_opcode::v_pack_b32_f16:
-   case aco_opcode::v_cvt_pknorm_i16_f16:
-   case aco_opcode::v_cvt_pknorm_u16_f16: return idx != -1;
-   case aco_opcode::v_mad_u32_u16:
-   case aco_opcode::v_mad_i32_i16: return idx >= 0 && idx < 2;
-   case aco_opcode::v_dot2_f16_f16:
-   case aco_opcode::v_dot2_bf16_bf16: return idx == -1 || idx == 2;
-   case aco_opcode::v_cndmask_b16: return idx != 2;
-   case aco_opcode::v_interp_p10_f16_f32_inreg:
-   case aco_opcode::v_interp_p10_rtz_f16_f32_inreg: return idx == 0 || idx == 2;
-   case aco_opcode::v_interp_p2_f16_f32_inreg:
-   case aco_opcode::v_interp_p2_rtz_f16_f32_inreg: return idx == -1 || idx == 0;
-   case aco_opcode::v_cvt_pk_fp8_f32:
-   case aco_opcode::p_v_cvt_pk_fp8_f32_ovfl:
-   case aco_opcode::v_cvt_pk_bf8_f32: return idx == -1;
-   default:
-      return gfx_level >= GFX11 && (get_gfx11_true16_mask(op) & BITFIELD_BIT(idx == -1 ? 3 : idx));
-   }
-}
-
-bool
-can_write_m0(const aco_ptr<Instruction>& instr)
-{
-   if (instr->isSALU())
-      return true;
-
-   /* VALU can't write m0 on any GPU generations. */
-   if (instr->isVALU())
-      return false;
-
-   switch (instr->opcode) {
-   case aco_opcode::p_parallelcopy:
-   case aco_opcode::p_extract:
-   case aco_opcode::p_insert:
-      /* These pseudo instructions are implemented with SALU when writing m0. */
-      return true;
-   default:
-      /* Assume that no other instructions can write m0. */
-      return false;
-   }
-}
-
-bool
-instr_is_16bit(amd_gfx_level gfx_level, aco_opcode op)
-{
-   /* partial register writes are GFX9+, only */
-   if (gfx_level < GFX9)
-      return false;
-
-   switch (op) {
-   /* VOP3 */
-   case aco_opcode::v_mad_legacy_f16:
-   case aco_opcode::v_mad_legacy_u16:
-   case aco_opcode::v_mad_legacy_i16:
-   case aco_opcode::v_fma_legacy_f16:
-   case aco_opcode::v_div_fixup_legacy_f16: return false;
-   case aco_opcode::v_interp_p2_f16:
-   case aco_opcode::v_interp_p2_hi_f16:
-   case aco_opcode::v_fma_mixlo_f16:
-   case aco_opcode::v_fma_mixhi_f16:
-   /* VOP2 */
-   case aco_opcode::v_mac_f16:
-   case aco_opcode::v_madak_f16:
-   case aco_opcode::v_madmk_f16: return gfx_level >= GFX9;
-   case aco_opcode::v_add_f16:
-   case aco_opcode::v_sub_f16:
-   case aco_opcode::v_subrev_f16:
-   case aco_opcode::v_mul_f16:
-   case aco_opcode::v_max_f16:
-   case aco_opcode::v_min_f16:
-   case aco_opcode::v_ldexp_f16:
-   case aco_opcode::v_fmac_f16:
-   case aco_opcode::v_fmamk_f16:
-   case aco_opcode::v_fmaak_f16:
-   /* VOP1 */
-   case aco_opcode::v_cvt_f16_f32:
-   case aco_opcode::p_v_cvt_f16_f32_rtne:
-   case aco_opcode::v_cvt_f16_u16:
-   case aco_opcode::v_cvt_f16_i16:
-   case aco_opcode::v_rcp_f16:
-   case aco_opcode::v_sqrt_f16:
-   case aco_opcode::v_rsq_f16:
-   case aco_opcode::v_log_f16:
-   case aco_opcode::v_exp_f16:
-   case aco_opcode::v_frexp_mant_f16:
-   case aco_opcode::v_frexp_exp_i16_f16:
-   case aco_opcode::v_floor_f16:
-   case aco_opcode::v_ceil_f16:
-   case aco_opcode::v_trunc_f16:
-   case aco_opcode::v_rndne_f16:
-   case aco_opcode::v_fract_f16:
-   case aco_opcode::v_sin_f16:
-   case aco_opcode::v_cos_f16:
-   case aco_opcode::v_cvt_u16_f16:
-   case aco_opcode::v_cvt_i16_f16:
-   case aco_opcode::v_cvt_norm_i16_f16:
-   case aco_opcode::v_cvt_norm_u16_f16: return gfx_level >= GFX10;
-   /* all non legacy opsel instructions preserve the high bits */
-   default: return can_use_opsel(gfx_level, op, -1);
-   }
-}
-
-/* On GFX11, for some instructions, bit 7 of the destination/operand vgpr is opsel and the field
- * only supports v0-v127.
- * The first three bits are used for operands 0-2, and the 4th bit is used for the destination.
- */
-uint8_t
-get_gfx11_true16_mask(aco_opcode op)
-{
-   switch (op) {
-   case aco_opcode::v_ceil_f16:
-   case aco_opcode::v_cos_f16:
-   case aco_opcode::v_cvt_f16_i16:
-   case aco_opcode::v_cvt_f16_u16:
-   case aco_opcode::v_cvt_i16_f16:
-   case aco_opcode::v_cvt_u16_f16:
-   case aco_opcode::v_cvt_norm_i16_f16:
-   case aco_opcode::v_cvt_norm_u16_f16:
-   case aco_opcode::v_exp_f16:
-   case aco_opcode::v_floor_f16:
-   case aco_opcode::v_fract_f16:
-   case aco_opcode::v_frexp_exp_i16_f16:
-   case aco_opcode::v_frexp_mant_f16:
-   case aco_opcode::v_log_f16:
-   case aco_opcode::v_not_b16:
-   case aco_opcode::v_rcp_f16:
-   case aco_opcode::v_rndne_f16:
-   case aco_opcode::v_rsq_f16:
-   case aco_opcode::v_sin_f16:
-   case aco_opcode::v_sqrt_f16:
-   case aco_opcode::v_trunc_f16:
-   case aco_opcode::v_swap_b16:
-   case aco_opcode::v_mov_b16: return 0x1 | 0x8;
-   case aco_opcode::v_add_f16:
-   case aco_opcode::v_fmaak_f16:
-   case aco_opcode::v_fmac_f16:
-   case aco_opcode::v_fmamk_f16:
-   case aco_opcode::v_ldexp_f16:
-   case aco_opcode::v_max_f16:
-   case aco_opcode::v_min_f16:
-   case aco_opcode::v_mul_f16:
-   case aco_opcode::v_sub_f16:
-   case aco_opcode::v_subrev_f16:
-   case aco_opcode::v_and_b16:
-   case aco_opcode::v_or_b16:
-   case aco_opcode::v_xor_b16: return 0x3 | 0x8;
-   case aco_opcode::v_cvt_pk_f32_fp8:
-   case aco_opcode::v_cvt_pk_f32_bf8:
-   case aco_opcode::v_cvt_f32_f16:
-   case aco_opcode::v_cvt_i32_i16:
-   case aco_opcode::v_cvt_u32_u16: return 0x1;
-   case aco_opcode::v_cmp_class_f16:
-   case aco_opcode::v_cmp_eq_f16:
-   case aco_opcode::v_cmp_eq_i16:
-   case aco_opcode::v_cmp_eq_u16:
-   case aco_opcode::v_cmp_ge_f16:
-   case aco_opcode::v_cmp_ge_i16:
-   case aco_opcode::v_cmp_ge_u16:
-   case aco_opcode::v_cmp_gt_f16:
-   case aco_opcode::v_cmp_gt_i16:
-   case aco_opcode::v_cmp_gt_u16:
-   case aco_opcode::v_cmp_le_f16:
-   case aco_opcode::v_cmp_le_i16:
-   case aco_opcode::v_cmp_le_u16:
-   case aco_opcode::v_cmp_lg_f16:
-   case aco_opcode::v_cmp_lg_i16:
-   case aco_opcode::v_cmp_lg_u16:
-   case aco_opcode::v_cmp_lt_f16:
-   case aco_opcode::v_cmp_lt_i16:
-   case aco_opcode::v_cmp_lt_u16:
-   case aco_opcode::v_cmp_neq_f16:
-   case aco_opcode::v_cmp_nge_f16:
-   case aco_opcode::v_cmp_ngt_f16:
-   case aco_opcode::v_cmp_nle_f16:
-   case aco_opcode::v_cmp_nlg_f16:
-   case aco_opcode::v_cmp_nlt_f16:
-   case aco_opcode::v_cmp_o_f16:
-   case aco_opcode::v_cmp_u_f16:
-   case aco_opcode::v_cmpx_class_f16:
-   case aco_opcode::v_cmpx_eq_f16:
-   case aco_opcode::v_cmpx_eq_i16:
-   case aco_opcode::v_cmpx_eq_u16:
-   case aco_opcode::v_cmpx_ge_f16:
-   case aco_opcode::v_cmpx_ge_i16:
-   case aco_opcode::v_cmpx_ge_u16:
-   case aco_opcode::v_cmpx_gt_f16:
-   case aco_opcode::v_cmpx_gt_i16:
-   case aco_opcode::v_cmpx_gt_u16:
-   case aco_opcode::v_cmpx_le_f16:
-   case aco_opcode::v_cmpx_le_i16:
-   case aco_opcode::v_cmpx_le_u16:
-   case aco_opcode::v_cmpx_lg_f16:
-   case aco_opcode::v_cmpx_lg_i16:
-   case aco_opcode::v_cmpx_lg_u16:
-   case aco_opcode::v_cmpx_lt_f16:
-   case aco_opcode::v_cmpx_lt_i16:
-   case aco_opcode::v_cmpx_lt_u16:
-   case aco_opcode::v_cmpx_neq_f16:
-   case aco_opcode::v_cmpx_nge_f16:
-   case aco_opcode::v_cmpx_ngt_f16:
-   case aco_opcode::v_cmpx_nle_f16:
-   case aco_opcode::v_cmpx_nlg_f16:
-   case aco_opcode::v_cmpx_nlt_f16:
-   case aco_opcode::v_cmpx_o_f16:
-   case aco_opcode::v_cmpx_u_f16: return 0x3;
-   case aco_opcode::v_cvt_f16_f32:
-   case aco_opcode::v_sat_pk_u8_i16: return 0x8;
-   default: return 0x0;
-   }
-}
-
-uint32_t
-get_reduction_identity(ReduceOp op, unsigned idx)
-{
-   switch (op) {
-   case iadd8:
-   case iadd16:
-   case iadd32:
-   case iadd64:
-   case fadd16:
-   case fadd32:
-   case fadd64:
-   case ior8:
-   case ior16:
-   case ior32:
-   case ior64:
-   case ixor8:
-   case ixor16:
-   case ixor32:
-   case ixor64:
-   case umax8:
-   case umax16:
-   case umax32:
-   case umax64: return 0;
-   case imul8:
-   case imul16:
-   case imul32:
-   case imul64: return idx ? 0 : 1;
-   case fmul16: return 0x3c00u;                /* 1.0 */
-   case fmul32: return 0x3f800000u;            /* 1.0 */
-   case fmul64: return idx ? 0x3ff00000u : 0u; /* 1.0 */
-   case imin8: return INT8_MAX;
-   case imin16: return INT16_MAX;
-   case imin32: return INT32_MAX;
-   case imin64: return idx ? 0x7fffffffu : 0xffffffffu;
-   case imax8: return INT8_MIN;
-   case imax16: return INT16_MIN;
-   case imax32: return INT32_MIN;
-   case imax64: return idx ? 0x80000000u : 0;
-   case umin8:
-   case umin16:
-   case iand8:
-   case iand16: return 0xffffffffu;
-   case umin32:
-   case umin64:
-   case iand32:
-   case iand64: return 0xffffffffu;
-   case fmin16: return 0x7c00u;                /* infinity */
-   case fmin32: return 0x7f800000u;            /* infinity */
-   case fmin64: return idx ? 0x7ff00000u : 0u; /* infinity */
-   case fmax16: return 0xfc00u;                /* negative infinity */
-   case fmax32: return 0xff800000u;            /* negative infinity */
-   case fmax64: return idx ? 0xfff00000u : 0u; /* negative infinity */
-   default: unreachable("Invalid reduction operation"); break;
-   }
-   return 0;
-}
-
-aco_type
-get_operand_type(aco_ptr<Instruction>& alu, unsigned index)
-{
-   assert(alu->isVALU() || alu->isSALU());
-   aco_type type = instr_info.alu_opcode_infos[(int)alu->opcode].op_types[index];
-
-   if (alu->opcode == aco_opcode::v_fma_mix_f32 || alu->opcode == aco_opcode::v_fma_mixlo_f16 ||
-       alu->opcode == aco_opcode::v_fma_mixhi_f16)
-      type.bit_size = alu->valu().opsel_hi[index] ? 16 : 32;
-
-   return type;
-}
-
-bool
-needs_exec_mask(const Instruction* instr)
-{
-   if (instr->isVALU()) {
-      return instr->opcode != aco_opcode::v_readlane_b32 &&
-             instr->opcode != aco_opcode::v_readlane_b32_e64 &&
-             instr->opcode != aco_opcode::v_writelane_b32 &&
-             instr->opcode != aco_opcode::v_writelane_b32_e64;
-   }
-
-   if (instr->isVMEM() || instr->isFlatLike())
-      return true;
-
-   if (instr->isSALU() || instr->isBranch() || instr->isSMEM() || instr->isBarrier())
-      return instr->opcode == aco_opcode::s_cbranch_execz ||
-             instr->opcode == aco_opcode::s_cbranch_execnz ||
-             instr->opcode == aco_opcode::s_setpc_b64 || instr->reads_exec();
-
-   if (instr->isPseudo()) {
-      switch (instr->opcode) {
-      case aco_opcode::p_create_vector:
-      case aco_opcode::p_extract_vector:
-      case aco_opcode::p_split_vector:
-      case aco_opcode::p_phi:
-      case aco_opcode::p_parallelcopy:
-         for (Definition def : instr->definitions) {
-            if (def.getTemp().type() == RegType::vgpr)
-               return true;
-         }
-         return instr->reads_exec();
-      case aco_opcode::p_spill:
-      case aco_opcode::p_reload:
-      case aco_opcode::p_end_linear_vgpr:
-      case aco_opcode::p_logical_start:
-      case aco_opcode::p_logical_end:
-      case aco_opcode::p_startpgm:
-      case aco_opcode::p_end_wqm:
-      case aco_opcode::p_init_scratch: return instr->reads_exec();
-      case aco_opcode::p_start_linear_vgpr: return instr->operands.size();
-      default: break;
-      }
-   }
-
-   return true;
-}
-
-struct CmpInfo {
-   aco_opcode swapped;
-   aco_opcode inverse;
-   aco_opcode vcmpx;
-};
-
-static ALWAYS_INLINE bool
-get_cmp_info(aco_opcode op, CmpInfo* info)
-{
-   info->swapped = aco_opcode::num_opcodes;
-   info->inverse = aco_opcode::num_opcodes;
-   info->vcmpx = aco_opcode::num_opcodes;
-   switch (op) {
-      // clang-format off
-#define CMP2(ord, unord, ord_swap, unord_swap, sz)                                                 \
-   case aco_opcode::v_cmp_##ord##_f##sz:                                                           \
-   case aco_opcode::v_cmp_n##unord##_f##sz:                                                        \
-      info->swapped = op == aco_opcode::v_cmp_##ord##_f##sz ? aco_opcode::v_cmp_##ord_swap##_f##sz \
-                                                      : aco_opcode::v_cmp_n##unord_swap##_f##sz;   \
-      info->inverse = op == aco_opcode::v_cmp_n##unord##_f##sz ? aco_opcode::v_cmp_##unord##_f##sz \
-                                                               : aco_opcode::v_cmp_n##ord##_f##sz; \
-      info->vcmpx = op == aco_opcode::v_cmp_##ord##_f##sz ? aco_opcode::v_cmpx_##ord##_f##sz       \
-                                                          : aco_opcode::v_cmpx_n##unord##_f##sz;   \
-      return true;
-#define CMP(ord, unord, ord_swap, unord_swap)                                                      \
-   CMP2(ord, unord, ord_swap, unord_swap, 16)                                                      \
-   CMP2(ord, unord, ord_swap, unord_swap, 32)                                                      \
-   CMP2(ord, unord, ord_swap, unord_swap, 64)
-      CMP(lt, /*n*/ge, gt, /*n*/le)
-      CMP(eq, /*n*/lg, eq, /*n*/lg)
-      CMP(le, /*n*/gt, ge, /*n*/lt)
-      CMP(gt, /*n*/le, lt, /*n*/ge)
-      CMP(lg, /*n*/eq, lg, /*n*/eq)
-      CMP(ge, /*n*/lt, le, /*n*/gt)
-#undef CMP
-#undef CMP2
-#define ORD_TEST(sz)                                                                               \
-   case aco_opcode::v_cmp_u_f##sz:                                                                 \
-      info->swapped = aco_opcode::v_cmp_u_f##sz;                                                   \
-      info->inverse = aco_opcode::v_cmp_o_f##sz;                                                   \
-      info->vcmpx = aco_opcode::v_cmpx_u_f##sz;                                                    \
-      return true;                                                                                 \
-   case aco_opcode::v_cmp_o_f##sz:                                                                 \
-      info->swapped = aco_opcode::v_cmp_o_f##sz;                                                   \
-      info->inverse = aco_opcode::v_cmp_u_f##sz;                                                   \
-      info->vcmpx = aco_opcode::v_cmpx_o_f##sz;                                                    \
-      return true;
-      ORD_TEST(16)
-      ORD_TEST(32)
-      ORD_TEST(64)
-#undef ORD_TEST
-#define CMPI2(op, swap, inv, type, sz)                                                             \
-   case aco_opcode::v_cmp_##op##_##type##sz:                                                       \
-      info->swapped = aco_opcode::v_cmp_##swap##_##type##sz;                                       \
-      info->inverse = aco_opcode::v_cmp_##inv##_##type##sz;                                        \
-      info->vcmpx = aco_opcode::v_cmpx_##op##_##type##sz;                                          \
-      return true;
-#define CMPI(op, swap, inv)                                                                        \
-   CMPI2(op, swap, inv, i, 16)                                                                     \
-   CMPI2(op, swap, inv, u, 16)                                                                     \
-   CMPI2(op, swap, inv, i, 32)                                                                     \
-   CMPI2(op, swap, inv, u, 32)                                                                     \
-   CMPI2(op, swap, inv, i, 64)                                                                     \
-   CMPI2(op, swap, inv, u, 64)
-      CMPI(lt, gt, ge)
-      CMPI(eq, eq, lg)
-      CMPI(le, ge, gt)
-      CMPI(gt, lt, le)
-      CMPI(lg, lg, eq)
-      CMPI(ge, le, lt)
-#undef CMPI
-#undef CMPI2
-#define CMPCLASS(sz)                                                                               \
-   case aco_opcode::v_cmp_class_f##sz:                                                             \
-      info->vcmpx = aco_opcode::v_cmpx_class_f##sz;                                                \
-      return true;
-      CMPCLASS(16)
-      CMPCLASS(32)
-      CMPCLASS(64)
-#undef CMPCLASS
-      // clang-format on
-   default: return false;
-   }
-}
-
-aco_opcode
-get_vcmp_inverse(aco_opcode op)
-{
-   CmpInfo info;
-   return get_cmp_info(op, &info) ? info.inverse : aco_opcode::num_opcodes;
-}
-
-aco_opcode
-get_vcmp_swapped(aco_opcode op)
-{
-   CmpInfo info;
-   return get_cmp_info(op, &info) ? info.swapped : aco_opcode::num_opcodes;
-}
-
-aco_opcode
-get_vcmpx(aco_opcode op)
-{
-   CmpInfo info;
-   return get_cmp_info(op, &info) ? info.vcmpx : aco_opcode::num_opcodes;
-}
-
-bool
-is_cmpx(aco_opcode op)
-{
-   CmpInfo info;
-   return !get_cmp_info(op, &info);
-}
-
-aco_opcode
-get_swapped_opcode(aco_opcode opcode, unsigned idx0, unsigned idx1)
-{
-   if (idx0 == idx1)
-      return opcode;
-
-   if (idx0 > idx1)
-      std::swap(idx0, idx1);
-
-   CmpInfo info;
-   if (get_cmp_info(opcode, &info) && info.swapped != aco_opcode::num_opcodes)
-      return info.swapped;
-
-   /* opcodes not relevant for DPP or SGPRs optimizations are not included. */
-   switch (opcode) {
-   case aco_opcode::v_add_u32:
-   case aco_opcode::v_add_co_u32:
-   case aco_opcode::v_add_co_u32_e64:
-   case aco_opcode::v_add_i32:
-   case aco_opcode::v_add_i16:
-   case aco_opcode::v_add_u16_e64:
-   case aco_opcode::v_add3_u32:
-   case aco_opcode::v_add_f16:
-   case aco_opcode::v_add_f32:
-   case aco_opcode::v_mul_i32_i24:
-   case aco_opcode::v_mul_hi_i32_i24:
-   case aco_opcode::v_mul_u32_u24:
-   case aco_opcode::v_mul_hi_u32_u24:
-   case aco_opcode::v_mul_lo_u16:
-   case aco_opcode::v_mul_lo_u16_e64:
-   case aco_opcode::v_mul_f16:
-   case aco_opcode::v_mul_f32:
-   case aco_opcode::v_mul_legacy_f32:
-   case aco_opcode::v_or_b32:
-   case aco_opcode::v_and_b32:
-   case aco_opcode::v_xor_b32:
-   case aco_opcode::v_xnor_b32:
-   case aco_opcode::v_xor3_b32:
-   case aco_opcode::v_or3_b32:
-   case aco_opcode::v_and_b16:
-   case aco_opcode::v_or_b16:
-   case aco_opcode::v_xor_b16:
-   case aco_opcode::v_max3_f32:
-   case aco_opcode::v_min3_f32:
-   case aco_opcode::v_max3_f16:
-   case aco_opcode::v_min3_f16:
-   case aco_opcode::v_med3_f16:
-   case aco_opcode::v_max3_u32:
-   case aco_opcode::v_min3_u32:
-   case aco_opcode::v_med3_u32:
-   case aco_opcode::v_max3_i32:
-   case aco_opcode::v_min3_i32:
-   case aco_opcode::v_med3_i32:
-   case aco_opcode::v_max3_u16:
-   case aco_opcode::v_min3_u16:
-   case aco_opcode::v_med3_u16:
-   case aco_opcode::v_max3_i16:
-   case aco_opcode::v_min3_i16:
-   case aco_opcode::v_med3_i16:
-   case aco_opcode::v_max_f16:
-   case aco_opcode::v_max_f32:
-   case aco_opcode::v_min_f16:
-   case aco_opcode::v_min_f32:
-   case aco_opcode::v_max_i32:
-   case aco_opcode::v_min_i32:
-   case aco_opcode::v_max_u32:
-   case aco_opcode::v_min_u32:
-   case aco_opcode::v_max_i16:
-   case aco_opcode::v_min_i16:
-   case aco_opcode::v_max_u16:
-   case aco_opcode::v_min_u16:
-   case aco_opcode::v_max_i16_e64:
-   case aco_opcode::v_min_i16_e64:
-   case aco_opcode::v_max_u16_e64:
-   case aco_opcode::v_min_u16_e64: return opcode;
-   case aco_opcode::v_sub_f16: return aco_opcode::v_subrev_f16;
-   case aco_opcode::v_sub_f32: return aco_opcode::v_subrev_f32;
-   case aco_opcode::v_sub_co_u32: return aco_opcode::v_subrev_co_u32;
-   case aco_opcode::v_sub_u16: return aco_opcode::v_subrev_u16;
-   case aco_opcode::v_sub_u32: return aco_opcode::v_subrev_u32;
-   case aco_opcode::v_sub_co_u32_e64: return aco_opcode::v_subrev_co_u32_e64;
-   case aco_opcode::v_subrev_f16: return aco_opcode::v_sub_f16;
-   case aco_opcode::v_subrev_f32: return aco_opcode::v_sub_f32;
-   case aco_opcode::v_subrev_co_u32: return aco_opcode::v_sub_co_u32;
-   case aco_opcode::v_subrev_u16: return aco_opcode::v_sub_u16;
-   case aco_opcode::v_subrev_u32: return aco_opcode::v_sub_u32;
-   case aco_opcode::v_subrev_co_u32_e64: return aco_opcode::v_sub_co_u32_e64;
-   case aco_opcode::v_addc_co_u32:
-   case aco_opcode::v_mad_i32_i24:
-   case aco_opcode::v_mad_u32_u24:
-   case aco_opcode::v_lerp_u8:
-   case aco_opcode::v_sad_u8:
-   case aco_opcode::v_sad_hi_u8:
-   case aco_opcode::v_sad_u16:
-   case aco_opcode::v_sad_u32:
-   case aco_opcode::v_xad_u32:
-   case aco_opcode::v_add_lshl_u32:
-   case aco_opcode::v_and_or_b32:
-   case aco_opcode::v_mad_u16:
-   case aco_opcode::v_mad_i16:
-   case aco_opcode::v_mad_u32_u16:
-   case aco_opcode::v_mad_i32_i16:
-   case aco_opcode::v_maxmin_f32:
-   case aco_opcode::v_minmax_f32:
-   case aco_opcode::v_maxmin_f16:
-   case aco_opcode::v_minmax_f16:
-   case aco_opcode::v_maxmin_u32:
-   case aco_opcode::v_minmax_u32:
-   case aco_opcode::v_maxmin_i32:
-   case aco_opcode::v_minmax_i32:
-   case aco_opcode::v_fma_f32:
-   case aco_opcode::v_fma_legacy_f32:
-   case aco_opcode::v_fmac_f32:
-   case aco_opcode::v_fmac_legacy_f32:
-   case aco_opcode::v_mac_f32:
-   case aco_opcode::v_mac_legacy_f32:
-   case aco_opcode::v_fma_f16:
-   case aco_opcode::v_fmac_f16:
-   case aco_opcode::v_mac_f16:
-   case aco_opcode::v_dot4c_i32_i8:
-   case aco_opcode::v_dot2c_f32_f16:
-   case aco_opcode::v_dot2_f32_f16:
-   case aco_opcode::v_dot2_f32_bf16:
-   case aco_opcode::v_dot2_f16_f16:
-   case aco_opcode::v_dot2_bf16_bf16:
-   case aco_opcode::v_fma_mix_f32:
-   case aco_opcode::v_fma_mixlo_f16:
-   case aco_opcode::v_fma_mixhi_f16:
-   case aco_opcode::v_pk_fmac_f16: {
-      if (idx1 == 2)
-         return aco_opcode::num_opcodes;
-      return opcode;
-   }
-   case aco_opcode::v_subb_co_u32: {
-      if (idx1 == 2)
-         return aco_opcode::num_opcodes;
-      return aco_opcode::v_subbrev_co_u32;
-   }
-   case aco_opcode::v_subbrev_co_u32: {
-      if (idx1 == 2)
-         return aco_opcode::num_opcodes;
-      return aco_opcode::v_subb_co_u32;
-   }
-   case aco_opcode::v_med3_f32: /* order matters for clamp+GFX8+denorm ftz. */
-   default: return aco_opcode::num_opcodes;
-   }
-}
-
-bool
-can_swap_operands(aco_ptr<Instruction>& instr, aco_opcode* new_op, unsigned idx0, unsigned idx1)
-{
-   if (idx0 == idx1) {
-      *new_op = instr->opcode;
-      return true;
-   }
-
-   if (instr->isDPP())
-      return false;
-
-   if (!instr->isVOP3() && !instr->isVOP3P() && !instr->operands[0].isOfType(RegType::vgpr))
-      return false;
-
-   aco_opcode candidate = get_swapped_opcode(instr->opcode, idx0, idx1);
-   if (candidate == aco_opcode::num_opcodes)
-      return false;
-
-   *new_op = candidate;
-   return true;
-}
-
-wait_imm::wait_imm()
-    : exp(unset_counter), lgkm(unset_counter), vm(unset_counter), vs(unset_counter),
-      sample(unset_counter), bvh(unset_counter), km(unset_counter)
-{}
-wait_imm::wait_imm(uint16_t vm_, uint16_t exp_, uint16_t lgkm_, uint16_t vs_)
-    : exp(exp_), lgkm(lgkm_), vm(vm_), vs(vs_), sample(unset_counter), bvh(unset_counter),
-      km(unset_counter)
-{}
-
-uint16_t
-wait_imm::pack(enum amd_gfx_level gfx_level) const
-{
-   uint16_t imm = 0;
-   assert(exp == unset_counter || exp <= 0x7);
-   if (gfx_level >= GFX11) {
-      assert(lgkm == unset_counter || lgkm <= 0x3f);
-      assert(vm == unset_counter || vm <= 0x3f);
-      imm = ((vm & 0x3f) << 10) | ((lgkm & 0x3f) << 4) | (exp & 0x7);
-   } else if (gfx_level >= GFX10) {
-      assert(lgkm == unset_counter || lgkm <= 0x3f);
-      assert(vm == unset_counter || vm <= 0x3f);
-      imm = ((vm & 0x30) << 10) | ((lgkm & 0x3f) << 8) | ((exp & 0x7) << 4) | (vm & 0xf);
-   } else if (gfx_level >= GFX9) {
-      assert(lgkm == unset_counter || lgkm <= 0xf);
-      assert(vm == unset_counter || vm <= 0x3f);
-      imm = ((vm & 0x30) << 10) | ((lgkm & 0xf) << 8) | ((exp & 0x7) << 4) | (vm & 0xf);
-   } else {
-      assert(lgkm == unset_counter || lgkm <= 0xf);
-      assert(vm == unset_counter || vm <= 0xf);
-      imm = ((lgkm & 0xf) << 8) | ((exp & 0x7) << 4) | (vm & 0xf);
-   }
-   if (gfx_level < GFX9 && vm == wait_imm::unset_counter)
-      imm |= 0xc000; /* should have no effect on pre-GFX9 and now we won't have to worry about the
-                        architecture when interpreting the immediate */
-   if (gfx_level < GFX10 && lgkm == wait_imm::unset_counter)
-      imm |= 0x3000; /* should have no effect on pre-GFX10 and now we won't have to worry about the
+                  /* addc/subb/cndmask 3rd operand needs VCC without VOP3. */
+                  remove_vop3 &= instr->operands.size() < 3 || !instr->operands[2].isFixed() ||
+                  instr->operands[2].isOfType(aco::RegType::vgpr) || instr->operands[2].physReg() == aco::vcc;
+
+                  if (remove_vop3)
+                        instr->format = aco::withoutVOP3(instr->format);
+
+                  return tmp;
+            }
+
+            bool
+            can_use_input_modifiers(amd_gfx_level gfx_level, aco::aco_opcode op, int idx)
+            {
+                  if (op == aco::aco_opcode::v_mov_b32)
+                        return gfx_level >= GFX10;
+
+                  return aco::instr_info.alu_opcode_infos[(int)op].input_modifiers & BITFIELD_BIT(idx);
+            }
+
+            bool
+            can_use_opsel(amd_gfx_level gfx_level, aco_opcode op, int idx)
+            {
+                  if (static_cast<uint16_t>(instr_info.format[static_cast<int>(op)]) & static_cast<uint16_t>(Format::VOP3P)) {
+                        return false;
+                  }
+
+                  /* Opsel is a GFX9+ feature. GFX11 has its own logic handled by get_gfx11_true16_mask. */
+                  if (gfx_level == GFX11) {
+                        return get_gfx11_true16_mask(op) & BITFIELD_BIT(idx == -1 ? 3 : idx);
+                  } else if (gfx_level < GFX9) {
+                        return false;
+                  }
+
+                  // Idea 1: Use bitmasks for common GFX9 cases (branchless, Vega-tuned)
+                  constexpr uint32_t opsel_mask_all = BITFIELD_MASK(3); // src0-2
+                  constexpr uint32_t opsel_mask_src01 = BITFIELD_MASK(2); // src0-1
+                  constexpr uint32_t opsel_mask_src0 = BITFIELD_BIT(0); // src0
+                  constexpr uint32_t opsel_mask_dst = BITFIELD_BIT(3); // dst (-1)
+
+                  if (idx < 0) idx = 3; // Map dst to bit 3 for mask checks
+
+                  switch (op) {
+                        /* Common VOP3A 16-bit instructions (all support opsel on Vega) */
+                        case aco_opcode::v_div_fixup_f16:
+                        case aco_opcode::v_fma_f16:
+                        case aco_opcode::v_mad_f16:
+                        case aco_opcode::v_mad_u16:
+                        case aco_opcode::v_mad_i16:
+                        case aco_opcode::v_med3_f16:
+                        case aco_opcode::v_med3_i16:
+                        case aco_opcode::v_med3_u16:
+                        case aco_opcode::v_min3_f16:
+                        case aco_opcode::v_min3_i16:
+                        case aco_opcode::v_min3_u16:
+                        case aco_opcode::v_max3_f16:
+                        case aco_opcode::v_max3_i16:
+                        case aco_opcode::v_max3_u16:
+                        case aco_opcode::v_minmax_f16:
+                        case aco_opcode::v_maxmin_f16:
+                        case aco_opcode::v_max_u16_e64:
+                        case aco_opcode::v_max_i16_e64:
+                        case aco_opcode::v_min_u16_e64:
+                        case aco_opcode::v_min_i16_e64:
+                        case aco_opcode::v_add_i16:
+                        case aco_opcode::v_sub_i16:
+                        case aco_opcode::v_add_u16_e64:
+                        case aco_opcode::v_sub_u16_e64:
+                        case aco_opcode::v_lshlrev_b16_e64:
+                        case aco_opcode::v_lshrrev_b16_e64:
+                        case aco_opcode::v_ashrrev_i16_e64:
+                        case aco_opcode::v_and_b16:
+                        case aco_opcode::v_or_b16:
+                        case aco_opcode::v_xor_b16:
+                        case aco_opcode::v_mul_lo_u16_e64:
+                              return (opsel_mask_all & BITFIELD_BIT(idx)) != 0; // All operands support opsel
+
+                              /* Instructions with specific operand restrictions */
+                              case aco_opcode::v_pack_b32_f16:
+                              case aco_opcode::v_cvt_pknorm_i16_f16:
+                              case aco_opcode::v_cvt_pknorm_u16_f16:
+                                    return (idx >= 0 && idx < 3) && (opsel_mask_all & BITFIELD_BIT(idx)) != 0; /* All source operands support opsel, but not the whole instruction */
+                              case aco_opcode::v_mad_u32_u16:
+                              case aco_opcode::v_mad_i32_i16:
+                                    return (idx >= 0 && idx < 2) && (opsel_mask_src01 & BITFIELD_BIT(idx)) != 0; /* src0 and src1 */
+                              case aco_opcode::v_dot2_f16_f16:
+                              case aco_opcode::v_dot2_bf16_bf16:
+                                    return (idx == -1 || idx == 2) && (opsel_mask_dst | BITFIELD_BIT(2)) & BITFIELD_BIT(idx); /* acc (dst) and src2 */
+                              case aco_opcode::v_cndmask_b16:
+                                    return idx != 2 && (opsel_mask_src01 & BITFIELD_BIT(idx)) != 0; /* src0 and src1 */
+                              case aco_opcode::v_interp_p10_f16_f32_inreg:
+                              case aco_opcode::v_interp_p10_rtz_f16_f32_inreg:
+                                    return (idx == 0 || idx == 2) && (opsel_mask_src0 | BITFIELD_BIT(2)) & BITFIELD_BIT(idx); /* src0 and src2 */
+                              case aco_opcode::v_interp_p2_f16_f32_inreg:
+                              case aco_opcode::v_interp_p2_rtz_f16_f32_inreg:
+                                    return (idx == -1 || idx == 0) && (opsel_mask_dst | opsel_mask_src0) & BITFIELD_BIT(idx); /* dst and src0 */
+                              case aco_opcode::v_cvt_pk_fp8_f32:
+                              case aco_opcode::p_v_cvt_pk_fp8_f32_ovfl:
+                              case aco_opcode::v_cvt_pk_bf8_f32:
+                                    return idx == -1 && (opsel_mask_dst & BITFIELD_BIT(idx)) != 0; /* Only dst */
+
+                                    /* ISA-documented VOP3A instructions on Vega that support opsel */
+                                    case aco_opcode::v_alignbit_b32:
+                                    case aco_opcode::v_alignbyte_b32:
+                                          return (idx >= 0 && idx < 2) && (opsel_mask_src01 & BITFIELD_BIT(idx)) != 0; /* src0 and src1 */
+                                    case aco_opcode::v_interp_p2_f16:
+                                          return idx == 0 && (opsel_mask_src0 & BITFIELD_BIT(idx)) != 0; /* src0 only */
+                                    case aco_opcode::v_mad_legacy_f16:
+                                    case aco_opcode::v_mad_legacy_i16:
+                                    case aco_opcode::v_mad_legacy_u16:
+                                    case aco_opcode::v_fma_legacy_f16:
+                                    case aco_opcode::v_div_fixup_legacy_f16:
+                                          return (idx >= 0 && idx < 3) && (opsel_mask_all & BITFIELD_BIT(idx)) != 0; /* src0, src1, and src2 */
+
+                                    default:
+                                          /* If not explicitly listed, it does not support opsel on GFX9. */
+                                          return false;
+                  }
+            }
+
+            bool
+            can_write_m0(const aco::aco_ptr<aco::Instruction>& instr)
+            {
+                  if (!instr) [[unlikely]] {
+                        return false;
+                  }
+
+                  if (instr->isSALU())
+                        return true;
+
+                  /* VALU can't write m0 on any GPU generations. */
+                  if (instr->isVALU())
+                        return false;
+
+                  switch (instr->opcode) {
+                        case aco::aco_opcode::p_parallelcopy:
+                        case aco::aco_opcode::p_extract:
+                        case aco::aco_opcode::p_insert:
+                              /* These pseudo instructions are implemented with SALU when writing m0. */
+                              return true;
+                        default:
+                              /* Assume that no other instructions can write m0. */
+                              return false;
+                  }
+            }
+
+            bool
+            instr_is_16bit(amd_gfx_level gfx_level, aco::aco_opcode op)
+            {
+                  /* partial register writes are GFX9+, only */
+                  if (gfx_level < GFX9)
+                        return false;
+
+                  switch (op) {
+                        /* VOP3 */
+                        case aco::aco_opcode::v_mad_legacy_f16:
+                        case aco::aco_opcode::v_mad_legacy_u16:
+                        case aco::aco_opcode::v_mad_legacy_i16:
+                        case aco::aco_opcode::v_fma_legacy_f16:
+                        case aco::aco_opcode::v_div_fixup_legacy_f16: return false;
+                        case aco::aco_opcode::v_interp_p2_f16:
+                        case aco::aco_opcode::v_interp_p2_hi_f16:
+                        case aco::aco_opcode::v_fma_mixlo_f16:
+                        case aco::aco_opcode::v_fma_mixhi_f16:
+                              /* VOP2 */
+                              case aco::aco_opcode::v_mac_f16:
+                              case aco::aco_opcode::v_madak_f16:
+                              case aco::aco_opcode::v_madmk_f16: return gfx_level >= GFX9;
+                              case aco::aco_opcode::v_add_f16:
+                              case aco::aco_opcode::v_sub_f16:
+                              case aco::aco_opcode::v_subrev_f16:
+                              case aco::aco_opcode::v_mul_f16:
+                              case aco::aco_opcode::v_max_f16:
+                              case aco::aco_opcode::v_min_f16:
+                              case aco::aco_opcode::v_ldexp_f16:
+                              case aco::aco_opcode::v_fmac_f16:
+                              case aco::aco_opcode::v_fmamk_f16:
+                              case aco::aco_opcode::v_fmaak_f16:
+                                    /* VOP1 */
+                                    case aco::aco_opcode::v_cvt_f16_f32:
+                                    case aco::aco_opcode::p_v_cvt_f16_f32_rtne:
+                                    case aco::aco_opcode::v_cvt_f16_u16:
+                                    case aco::aco_opcode::v_cvt_f16_i16:
+                                    case aco::aco_opcode::v_rcp_f16:
+                                    case aco::aco_opcode::v_sqrt_f16:
+                                    case aco::aco_opcode::v_rsq_f16:
+                                    case aco::aco_opcode::v_log_f16:
+                                    case aco::aco_opcode::v_exp_f16:
+                                    case aco::aco_opcode::v_frexp_mant_f16:
+                                    case aco::aco_opcode::v_frexp_exp_i16_f16:
+                                    case aco::aco_opcode::v_floor_f16:
+                                    case aco::aco_opcode::v_ceil_f16:
+                                    case aco::aco_opcode::v_trunc_f16:
+                                    case aco::aco_opcode::v_rndne_f16:
+                                    case aco::aco_opcode::v_fract_f16:
+                                    case aco::aco_opcode::v_sin_f16:
+                                    case aco::aco_opcode::v_cos_f16:
+                                    case aco::aco_opcode::v_cvt_u16_f16:
+                                    case aco::aco_opcode::v_cvt_i16_f16:
+                                    case aco::aco_opcode::v_cvt_norm_i16_f16:
+                                    case aco::aco_opcode::v_cvt_norm_u16_f16: return gfx_level >= GFX10;
+                                    case aco::aco_opcode::v_pk_mad_i16:
+                                    case aco::aco_opcode::v_pk_mul_lo_u16:
+                                    case aco::aco_opcode::v_pk_add_i16:
+                                    case aco::aco_opcode::v_pk_sub_i16:
+                                    case aco::aco_opcode::v_pk_lshlrev_b16:
+                                    case aco::aco_opcode::v_pk_lshrrev_b16:
+                                    case aco::aco_opcode::v_pk_ashrrev_i16:
+                                    case aco::aco_opcode::v_pk_max_i16:
+                                    case aco::aco_opcode::v_pk_min_i16:
+                                    case aco::aco_opcode::v_pk_mad_u16:
+                                    case aco::aco_opcode::v_pk_add_u16:
+                                    case aco::aco_opcode::v_pk_sub_u16:
+                                    case aco::aco_opcode::v_pk_max_u16:
+                                    case aco::aco_opcode::v_pk_min_u16:
+                                    case aco::aco_opcode::v_pk_fma_f16:
+                                    case aco::aco_opcode::v_pk_add_f16:
+                                    case aco::aco_opcode::v_pk_mul_f16:
+                                    case aco::aco_opcode::v_pk_min_f16:
+                                    case aco::aco_opcode::v_pk_max_f16:
+                                    case aco::aco_opcode::v_fma_mix_f32:
+                                    case aco::aco_opcode::v_dot2_f32_f16:
+                                    case aco::aco_opcode::v_dot2_f32_bf16:
+                                          return gfx_level == GFX9 && aco::can_use_opsel(gfx_level, op, -1);
+                                    default: return aco::can_use_opsel(gfx_level, op, -1);
+                  }
+            }
+
+            /* On GFX11, for some instructions, bit 7 of the destination/operand vgpr is opsel and the field
+             * only supports v0-v127.
+             * The first three bits are used for operands 0-2, and the 4th bit is used for the destination.
+             */
+            uint8_t
+            get_gfx11_true16_mask(aco_opcode op)
+            {
+                  switch (op) {
+                        case aco_opcode::v_ceil_f16:
+                        case aco_opcode::v_cos_f16:
+                        case aco_opcode::v_cvt_f16_i16:
+                        case aco_opcode::v_cvt_f16_u16:
+                        case aco_opcode::v_cvt_i16_f16:
+                        case aco_opcode::v_cvt_u16_f16:
+                        case aco_opcode::v_cvt_norm_i16_f16:
+                        case aco_opcode::v_cvt_norm_u16_f16:
+                        case aco_opcode::v_exp_f16:
+                        case aco_opcode::v_floor_f16:
+                        case aco_opcode::v_fract_f16:
+                        case aco_opcode::v_frexp_exp_i16_f16:
+                        case aco_opcode::v_frexp_mant_f16:
+                        case aco_opcode::v_log_f16:
+                        case aco_opcode::v_not_b16:
+                        case aco_opcode::v_rcp_f16:
+                        case aco_opcode::v_rndne_f16:
+                        case aco_opcode::v_rsq_f16:
+                        case aco_opcode::v_sin_f16:
+                        case aco_opcode::v_sqrt_f16:
+                        case aco_opcode::v_trunc_f16:
+                        case aco_opcode::v_swap_b16:
+                        case aco_opcode::v_mov_b16:
+                              return 0x1 | 0x8;
+                        case aco_opcode::v_add_f16:
+                        case aco_opcode::v_fmaak_f16:
+                        case aco_opcode::v_fmac_f16:
+                        case aco_opcode::v_fmamk_f16:
+                        case aco_opcode::v_ldexp_f16:
+                        case aco_opcode::v_max_f16:
+                        case aco_opcode::v_min_f16:
+                        case aco_opcode::v_mul_f16:
+                        case aco_opcode::v_sub_f16:
+                        case aco_opcode::v_subrev_f16:
+                        case aco_opcode::v_and_b16:
+                        case aco_opcode::v_or_b16:
+                        case aco_opcode::v_xor_b16:
+                              return 0x3 | 0x8;
+                        case aco_opcode::v_cvt_pk_f32_fp8:
+                        case aco_opcode::v_cvt_pk_f32_bf8:
+                        case aco_opcode::v_cvt_f32_f16:
+                        case aco_opcode::v_cvt_i32_i16:
+                        case aco_opcode::v_cvt_u32_u16:
+                              return 0x1;
+                        case aco_opcode::v_cmp_class_f16:
+                        case aco_opcode::v_cmp_eq_f16:
+                        case aco_opcode::v_cmp_eq_i16:
+                        case aco_opcode::v_cmp_eq_u16:
+                        case aco_opcode::v_cmp_ge_f16:
+                        case aco_opcode::v_cmp_ge_i16:
+                        case aco_opcode::v_cmp_ge_u16:
+                        case aco_opcode::v_cmp_gt_f16:
+                        case aco_opcode::v_cmp_gt_i16:
+                        case aco_opcode::v_cmp_gt_u16:
+                        case aco_opcode::v_cmp_le_f16:
+                        case aco_opcode::v_cmp_le_i16:
+                        case aco_opcode::v_cmp_le_u16:
+                        case aco_opcode::v_cmp_lg_f16:
+                        case aco_opcode::v_cmp_lg_i16:
+                        case aco_opcode::v_cmp_lg_u16:
+                        case aco_opcode::v_cmp_lt_f16:
+                        case aco_opcode::v_cmp_lt_i16:
+                        case aco_opcode::v_cmp_lt_u16:
+                        case aco_opcode::v_cmp_neq_f16:
+                        case aco_opcode::v_cmp_nge_f16:
+                        case aco_opcode::v_cmp_ngt_f16:
+                        case aco_opcode::v_cmp_nle_f16:
+                        case aco_opcode::v_cmp_nlg_f16:
+                        case aco_opcode::v_cmp_nlt_f16:
+                        case aco_opcode::v_cmp_o_f16:
+                        case aco_opcode::v_cmp_u_f16:
+                        case aco_opcode::v_cmpx_class_f16:
+                        case aco_opcode::v_cmpx_eq_f16:
+                        case aco_opcode::v_cmpx_eq_i16:
+                        case aco_opcode::v_cmpx_eq_u16:
+                        case aco_opcode::v_cmpx_ge_f16:
+                        case aco_opcode::v_cmpx_ge_i16:
+                        case aco_opcode::v_cmpx_ge_u16:
+                        case aco_opcode::v_cmpx_gt_f16:
+                        case aco_opcode::v_cmpx_gt_i16:
+                        case aco_opcode::v_cmpx_gt_u16:
+                        case aco_opcode::v_cmpx_le_f16:
+                        case aco_opcode::v_cmpx_le_i16:
+                        case aco_opcode::v_cmpx_le_u16:
+                        case aco_opcode::v_cmpx_lg_f16:
+                        case aco_opcode::v_cmpx_lg_i16:
+                        case aco_opcode::v_cmpx_lg_u16:
+                        case aco_opcode::v_cmpx_lt_f16:
+                        case aco_opcode::v_cmpx_lt_i16:
+                        case aco_opcode::v_cmpx_lt_u16:
+                        case aco_opcode::v_cmpx_neq_f16:
+                        case aco_opcode::v_cmpx_nge_f16:
+                        case aco_opcode::v_cmpx_ngt_f16:
+                        case aco_opcode::v_cmpx_nle_f16:
+                        case aco_opcode::v_cmpx_nlg_f16:
+                        case aco_opcode::v_cmpx_nlt_f16:
+                        case aco_opcode::v_cmpx_o_f16:
+                        case aco_opcode::v_cmpx_u_f16:
+                              return 0x3;
+                        case aco_opcode::v_cvt_f16_f32:
+                        case aco_opcode::v_sat_pk_u8_i16:
+                              return 0x8;
+                        default:
+                              return 0x0;
+                  }
+            }
+
+            uint32_t
+            get_reduction_identity(ReduceOp op, unsigned idx)
+            {
+                  switch (op) {
+                        case iadd8:
+                        case iadd16:
+                        case iadd32:
+                        case iadd64:
+                        case fadd16:
+                        case fadd32:
+                        case fadd64:
+                        case ior8:
+                        case ior16:
+                        case ior32:
+                        case ior64:
+                        case ixor8:
+                        case ixor16:
+                        case ixor32:
+                        case ixor64:
+                        case umax8:
+                        case umax16:
+                        case umax32:
+                        case umax64: return 0;
+                        case imul8:
+                        case imul16:
+                        case imul32:
+                        case imul64: return idx ? 0 : 1;
+                        case fmul16: return 0x3c00u;                /* 1.0 (Vega FMA fusion: prefer for mul reductions) */
+                        case fmul32: return 0x3f800000u;            /* 1.0 */
+                        case fmul64: return idx ? 0x3ff00000u : 0u; /* 1.0 */
+                        case imin8: return INT8_MAX;
+                        case imin16: return INT16_MAX;
+                        case imin32: return INT32_MAX;
+                        case imin64: return idx ? 0x7fffffffu : 0xffffffffu;
+                        case imax8: return INT8_MIN;
+                        case imax16: return INT16_MIN;
+                        case imax32: return INT32_MIN;
+                        case imax64: return idx ? 0x80000000u : 0;
+                        case umin8:
+                        case umin16:
+                        case iand8:
+                        case iand16: return 0xffffffffu;
+                        case umin32:
+                        case umin64:
+                        case iand32:
+                        case iand64: return 0xffffffffu;
+                        case fmin16: return 0x7c00u;                /* infinity */
+                        case fmin32: return 0x7f800000u;            /* infinity */
+                        case fmin64: return idx ? 0x7ff00000u : 0u; /* infinity */
+                        case fmax16: return 0xfc00u;                /* negative infinity */
+                        case fmax32: return 0xff800000u;            /* negative infinity */
+                        case fmax64: return idx ? 0xfff00000u : 0u; /* negative infinity */
+                        default: unreachable("Invalid reduction operation"); break;
+                  }
+                  return 0;
+            }
+
+            aco_type
+            get_operand_type(aco_ptr<Instruction>& alu, unsigned index)
+            {
+                  assert(alu->isVALU() || alu->isSALU());
+                  aco_type type = instr_info.alu_opcode_infos[(int)alu->opcode].op_types[index];
+
+                  if (alu->opcode == aco_opcode::v_fma_mix_f32 || alu->opcode == aco_opcode::v_fma_mixlo_f16 ||
+                        alu->opcode == aco_opcode::v_fma_mixhi_f16)
+                        type.bit_size = alu->valu().opsel_hi[index] ? 16 : 32;
+
+                  return type;
+            }
+
+            bool
+            needs_exec_mask(const Instruction* instr)
+            {
+                  if (instr->isVALU()) {
+                        return instr->opcode != aco_opcode::v_readlane_b32 &&
+                        instr->opcode != aco_opcode::v_readlane_b32_e64 &&
+                        instr->opcode != aco_opcode::v_writelane_b32 &&
+                        instr->opcode != aco_opcode::v_writelane_b32_e64;
+                  }
+
+                  if (instr->isVMEM() || instr->isFlatLike())
+                        return true;
+
+                  if (instr->isSALU() || instr->isBranch() || instr->isSMEM() || instr->isBarrier())
+                        return instr->opcode == aco_opcode::s_cbranch_execz ||
+                        instr->opcode == aco_opcode::s_cbranch_execnz ||
+                        instr->opcode == aco_opcode::s_setpc_b64 || instr->reads_exec();
+
+                  if (instr->isPseudo()) {
+                        switch (instr->opcode) {
+                              case aco_opcode::p_create_vector:
+                              case aco_opcode::p_extract_vector:
+                              case aco_opcode::p_split_vector:
+                              case aco_opcode::p_phi:
+                              case aco_opcode::p_parallelcopy:
+                                    for (Definition def : instr->definitions) {
+                                          if (def.getTemp().type() == RegType::vgpr)
+                                                return true;
+                                    }
+                                    return instr->reads_exec();
+                              case aco_opcode::p_spill:
+                              case aco_opcode::p_reload:
+                              case aco_opcode::p_end_linear_vgpr:
+                              case aco_opcode::p_logical_start:
+                              case aco_opcode::p_logical_end:
+                              case aco_opcode::p_startpgm:
+                              case aco_opcode::p_end_wqm:
+                              case aco_opcode::p_init_scratch: return instr->reads_exec();
+                              case aco_opcode::p_start_linear_vgpr: return instr->operands.size();
+                              default: break;
+                        }
+                  }
+
+                  return true;
+            }
+
+            struct CmpInfo {
+                  aco_opcode swapped;
+                  aco_opcode inverse;
+                  aco_opcode vcmpx;
+            };
+
+            static constexpr CmpInfo
+            generate_cmp_info(aco_opcode op)
+            {
+                  CmpInfo info = {aco_opcode::num_opcodes, aco_opcode::num_opcodes, aco_opcode::num_opcodes};
+                  switch (op) {
+                        #define CMP2(ord, unord, ord_swap, unord_swap, sz)                                                 \
+                        case aco_opcode::v_cmp_##ord##_f##sz:                                                           \
+                              info.swapped = aco_opcode::v_cmp_##ord_swap##_f##sz;                                         \
+                              info.inverse = aco_opcode::v_cmp_n##ord##_f##sz;                                             \
+                              info.vcmpx = aco_opcode::v_cmpx_##ord##_f##sz;                                               \
+                              return info;                                                                                \
+                        case aco_opcode::v_cmp_n##unord##_f##sz:                                                        \
+                              info.swapped = aco_opcode::v_cmp_n##unord_swap##_f##sz;                                      \
+                              info.inverse = aco_opcode::v_cmp_##unord##_f##sz;                                            \
+                              info.vcmpx = aco_opcode::v_cmpx_n##unord##_f##sz;                                             \
+                              return info;
+                              #define CMP(ord, unord, ord_swap, unord_swap)                                                      \
+                              CMP2(ord, unord, ord_swap, unord_swap, 16)                                                      \
+                              CMP2(ord, unord, ord_swap, unord_swap, 32)                                                      \
+                              CMP2(ord, unord, ord_swap, unord_swap, 64)
+                              CMP(lt, /*n*/ge, gt, /*n*/le)
+                              CMP(eq, /*n*/lg, eq, /*n*/lg)
+                              CMP(le, /*n*/gt, ge, /*n*/lt)
+                              CMP(gt, /*n*/le, lt, /*n*/ge)
+                              CMP(lg, /*n*/eq, lg, /*n*/eq)
+                              CMP(ge, /*n*/lt, le, /*n*/gt)
+                              #undef CMP
+                              #undef CMP2
+                              #define ORD_TEST(sz)                                                                               \
+                        case aco_opcode::v_cmp_u_f##sz:                                                                 \
+                              info.swapped = aco_opcode::v_cmp_u_f##sz;                                                   \
+                              info.inverse = aco_opcode::v_cmp_o_f##sz;                                                   \
+                              info.vcmpx = aco_opcode::v_cmpx_u_f##sz;                                                    \
+                              return info;                                                                                 \
+                        case aco_opcode::v_cmp_o_f##sz:                                                                 \
+                              info.swapped = aco_opcode::v_cmp_o_f##sz;                                                   \
+                              info.inverse = aco_opcode::v_cmp_u_f##sz;                                                   \
+                              info.vcmpx = aco_opcode::v_cmpx_o_f##sz;                                                    \
+                              return info;
+                              ORD_TEST(16)
+                              ORD_TEST(32)
+                              ORD_TEST(64)
+                              #undef ORD_TEST
+                              #define CMPI2(op, swap, inv, type, sz)                                                             \
+                        case aco_opcode::v_cmp_##op##_##type##sz:                                                       \
+                              info.swapped = aco_opcode::v_cmp_##swap##_##type##sz;                                       \
+                              info.inverse = aco_opcode::v_cmp_##inv##_##type##sz;                                        \
+                              info.vcmpx = aco_opcode::v_cmpx_##op##_##type##sz;                                          \
+                              return info;
+                              #define CMPI(op, swap, inv)                                                                        \
+                              CMPI2(op, swap, inv, i, 16)                                                                     \
+                              CMPI2(op, swap, inv, u, 16)                                                                     \
+                              CMPI2(op, swap, inv, i, 32)                                                                     \
+                              CMPI2(op, swap, inv, u, 32)                                                                     \
+                              CMPI2(op, swap, inv, i, 64)                                                                     \
+                              CMPI2(op, swap, inv, u, 64)
+                              CMPI(lt, gt, ge)
+                              CMPI(eq, eq, lg)
+                              CMPI(le, ge, gt)
+                              CMPI(gt, lt, le)
+                              CMPI(lg, lg, eq)
+                              CMPI(ge, le, lt)
+                              #undef CMPI
+                              #undef CMPI2
+                              #define CMPCLASS(sz)                                                                               \
+                        case aco_opcode::v_cmp_class_f##sz:                                                             \
+                              info.vcmpx = aco_opcode::v_cmpx_class_f##sz;                                                \
+                              return info;
+                              CMPCLASS(16)
+                              CMPCLASS(32)
+                              CMPCLASS(64)
+                              #undef CMPCLASS
+                        default: return info;
+                  }
+            }
+
+            static constexpr auto cmp_info_table = []() {
+                  std::array<CmpInfo, (size_t)aco_opcode::num_opcodes> table{};
+                  for (unsigned i = 0; i < (unsigned)aco_opcode::num_opcodes; ++i) {
+                        table[i] = generate_cmp_info((aco_opcode)i);
+                  }
+                  return table;
+            }();
+
+            static_assert(cmp_info_table[(size_t)aco_opcode::v_cmp_lt_f32].swapped == aco_opcode::v_cmp_gt_f32);
+            static_assert(cmp_info_table[(size_t)aco_opcode::v_cmp_lt_f32].inverse == aco_opcode::v_cmp_nlt_f32);
+            static_assert(cmp_info_table[(size_t)aco_opcode::v_cmp_lt_i32].vcmpx == aco_opcode::v_cmpx_lt_i32);
+            static_assert(cmp_info_table[(size_t)aco_opcode::v_add_f32].swapped == aco_opcode::num_opcodes, "Non-compare opcode check");
+
+            static ALWAYS_INLINE bool
+            get_cmp_info(aco_opcode op, CmpInfo* info)
+            {
+                  *info = cmp_info_table[static_cast<size_t>(op)];
+                  return info->vcmpx != aco_opcode::num_opcodes || info->inverse != aco_opcode::num_opcodes;
+            }
+
+            aco_opcode
+            get_vcmp_inverse(aco_opcode op)
+            {
+                  CmpInfo info;
+                  get_cmp_info(op, &info);
+                  return info.inverse;
+            }
+
+            aco_opcode
+            get_vcmp_swapped(aco_opcode op)
+            {
+                  CmpInfo info;
+                  get_cmp_info(op, &info);
+                  return info.swapped;
+            }
+
+            aco_opcode
+            get_vcmpx(aco_opcode op)
+            {
+                  CmpInfo info;
+                  get_cmp_info(op, &info);
+                  return info.vcmpx;
+            }
+
+            bool
+            is_cmpx(aco_opcode op)
+            {
+                  CmpInfo info;
+                  return !get_cmp_info(op, &info);
+            }
+
+            static constexpr aco_opcode
+            generate_swapped_opcode_map(aco_opcode opcode)
+            {
+                  switch (opcode) {
+                        case aco_opcode::v_add_u32:
+                        case aco_opcode::v_add_co_u32:
+                        case aco_opcode::v_add_co_u32_e64:
+                        case aco_opcode::v_add_i32:
+                        case aco_opcode::v_add_i16:
+                        case aco_opcode::v_add_u16_e64:
+                        case aco_opcode::v_add3_u32:
+                        case aco_opcode::v_add_f16:
+                        case aco_opcode::v_add_f32:
+                        case aco_opcode::v_mul_i32_i24:
+                        case aco_opcode::v_mul_hi_i32_i24:
+                        case aco_opcode::v_mul_u32_u24:
+                        case aco_opcode::v_mul_hi_u32_u24:
+                        case aco_opcode::v_mul_lo_u16:
+                        case aco_opcode::v_mul_lo_u16_e64:
+                        case aco_opcode::v_mul_f16:
+                        case aco_opcode::v_mul_f32:
+                        case aco_opcode::v_mul_legacy_f32:
+                        case aco_opcode::v_or_b32:
+                        case aco_opcode::v_and_b32:
+                        case aco_opcode::v_xor_b32:
+                        case aco_opcode::v_xnor_b32:
+                        case aco_opcode::v_xor3_b32:
+                        case aco_opcode::v_or3_b32:
+                        case aco_opcode::v_and_b16:
+                        case aco_opcode::v_or_b16:
+                        case aco_opcode::v_xor_b16:
+                        case aco_opcode::v_max3_f32:
+                        case aco_opcode::v_min3_f32:
+                        case aco_opcode::v_max3_f16:
+                        case aco_opcode::v_min3_f16:
+                        case aco_opcode::v_med3_f16:
+                        case aco_opcode::v_max3_u32:
+                        case aco_opcode::v_min3_u32:
+                        case aco_opcode::v_med3_u32:
+                        case aco_opcode::v_max3_i32:
+                        case aco_opcode::v_min3_i32:
+                        case aco_opcode::v_med3_i32:
+                        case aco_opcode::v_max3_u16:
+                        case aco_opcode::v_min3_u16:
+                        case aco_opcode::v_med3_u16:
+                        case aco_opcode::v_max3_i16:
+                        case aco_opcode::v_min3_i16:
+                        case aco_opcode::v_med3_i16:
+                        case aco_opcode::v_max_f16:
+                        case aco_opcode::v_max_f32:
+                        case aco_opcode::v_min_f16:
+                        case aco_opcode::v_min_f32:
+                        case aco_opcode::v_max_i32:
+                        case aco_opcode::v_min_i32:
+                        case aco_opcode::v_max_u32:
+                        case aco_opcode::v_min_u32:
+                        case aco_opcode::v_max_i16:
+                        case aco_opcode::v_min_i16:
+                        case aco_opcode::v_max_u16:
+                        case aco_opcode::v_min_u16:
+                        case aco_opcode::v_max_i16_e64:
+                        case aco_opcode::v_min_i16_e64:
+                        case aco_opcode::v_max_u16_e64:
+                        case aco_opcode::v_min_u16_e64:
+                        case aco_opcode::v_addc_co_u32:
+                        case aco_opcode::v_mad_i32_i24:
+                        case aco_opcode::v_mad_u32_u24:
+                        case aco_opcode::v_lerp_u8:
+                        case aco_opcode::v_sad_u8:
+                        case aco_opcode::v_sad_hi_u8:
+                        case aco_opcode::v_sad_u16:
+                        case aco_opcode::v_sad_u32:
+                        case aco_opcode::v_xad_u32:
+                        case aco_opcode::v_add_lshl_u32:
+                        case aco_opcode::v_and_or_b32:
+                        case aco_opcode::v_mad_u16:
+                        case aco_opcode::v_mad_i16:
+                        case aco_opcode::v_mad_u32_u16:
+                        case aco_opcode::v_mad_i32_i16:
+                        case aco_opcode::v_maxmin_f32:
+                        case aco_opcode::v_minmax_f32:
+                        case aco_opcode::v_maxmin_f16:
+                        case aco_opcode::v_minmax_f16:
+                        case aco_opcode::v_maxmin_u32:
+                        case aco_opcode::v_minmax_i32:
+                        case aco_opcode::v_fma_f32:
+                        case aco_opcode::v_fma_legacy_f32:
+                        case aco_opcode::v_fmac_f32:
+                        case aco_opcode::v_fmac_legacy_f32:
+                        case aco_opcode::v_mac_f32:
+                        case aco_opcode::v_mac_legacy_f32:
+                        case aco_opcode::v_fma_f16:
+                        case aco_opcode::v_fmac_f16:
+                        case aco_opcode::v_mac_f16:
+                        case aco_opcode::v_dot4c_i32_i8:
+                        case aco_opcode::v_dot2c_f32_f16:
+                        case aco_opcode::v_dot2_f32_f16:
+                        case aco_opcode::v_dot2_f32_bf16:
+                        case aco_opcode::v_dot2_f16_f16:
+                        case aco_opcode::v_dot2_bf16_bf16:
+                        case aco_opcode::v_fma_mix_f32:
+                        case aco_opcode::v_fma_mixlo_f16:
+                        case aco_opcode::v_fma_mixhi_f16:
+                        case aco_opcode::v_pk_fmac_f16:
+                              return opcode;
+                        case aco_opcode::v_sub_f16: return aco_opcode::v_subrev_f16;
+                        case aco_opcode::v_sub_f32: return aco_opcode::v_subrev_f32;
+                        case aco_opcode::v_sub_co_u32: return aco_opcode::v_subrev_co_u32;
+                        case aco_opcode::v_sub_u16: return aco_opcode::v_subrev_u16;
+                        case aco_opcode::v_sub_u32: return aco_opcode::v_subrev_u32;
+                        case aco_opcode::v_sub_co_u32_e64: return aco_opcode::v_subrev_co_u32_e64;
+                        case aco_opcode::v_subrev_f16: return aco_opcode::v_sub_f16;
+                        case aco_opcode::v_subrev_f32: return aco_opcode::v_sub_f32;
+                        case aco_opcode::v_subrev_co_u32: return aco_opcode::v_sub_co_u32;
+                        case aco_opcode::v_subrev_u16: return aco_opcode::v_sub_u16;
+                        case aco_opcode::v_subrev_u32: return aco_opcode::v_sub_u32;
+                        case aco_opcode::v_subrev_co_u32_e64: return aco_opcode::v_sub_co_u32_e64;
+                        case aco_opcode::v_subb_co_u32: return aco_opcode::v_subbrev_co_u32;
+                        case aco_opcode::v_subbrev_co_u32: return aco_opcode::v_subb_co_u32;
+                        default: return aco_opcode::num_opcodes;
+                  }
+            }
+
+            static constexpr auto swapped_opcode_table = []() {
+                  std::array<aco_opcode, (size_t)aco_opcode::num_opcodes> table{};
+                  for (unsigned i = 0; i < (unsigned)aco_opcode::num_opcodes; ++i) {
+                        table[i] = generate_swapped_opcode_map((aco_opcode)i);
+                  }
+                  return table;
+            }();
+
+            static_assert(swapped_opcode_table[(size_t)aco_opcode::v_sub_f32] == aco_opcode::v_subrev_f32);
+            static_assert(swapped_opcode_table[(size_t)aco_opcode::v_add_f32] == aco_opcode::v_add_f32);
+            static_assert(swapped_opcode_table[(size_t)aco_opcode::v_med3_f32] == aco_opcode::num_opcodes);
+
+            aco_opcode
+            get_swapped_opcode(aco_opcode opcode, unsigned idx0, unsigned idx1)
+            {
+                  if (idx0 == idx1)
+                        return opcode;
+
+                  if (idx0 > idx1)
+                        std::swap(idx0, idx1);
+
+                  CmpInfo info;
+                  if (get_cmp_info(opcode, &info) && info.swapped != aco_opcode::num_opcodes)
+                        return info.swapped;
+
+                  aco_opcode swapped = swapped_opcode_table[static_cast<size_t>(opcode)];
+                  if (swapped == aco_opcode::num_opcodes)
+                        return swapped;
+
+                  if (idx1 == 2) {
+                        switch (opcode) {
+                              case aco_opcode::v_addc_co_u32:
+                              case aco_opcode::v_subb_co_u32:
+                              case aco_opcode::v_subbrev_co_u32:
+                              case aco_opcode::v_mad_i32_i24:
+                              case aco_opcode::v_mad_u32_u24:
+                              case aco_opcode::v_lerp_u8:
+                              case aco_opcode::v_sad_u8:
+                              case aco_opcode::v_sad_hi_u8:
+                              case aco_opcode::v_sad_u16:
+                              case aco_opcode::v_sad_u32:
+                              case aco_opcode::v_xad_u32:
+                              case aco_opcode::v_add_lshl_u32:
+                              case aco_opcode::v_and_or_b32:
+                              case aco_opcode::v_mad_u16:
+                              case aco_opcode::v_mad_i16:
+                              case aco_opcode::v_mad_u32_u16:
+                              case aco_opcode::v_mad_i32_i16:
+                              case aco_opcode::v_maxmin_f32:
+                              case aco_opcode::v_minmax_f32:
+                              case aco_opcode::v_maxmin_f16:
+                              case aco_opcode::v_minmax_f16:
+                              case aco_opcode::v_maxmin_u32:
+                              case aco_opcode::v_minmax_i32:
+                              case aco_opcode::v_fma_f32:
+                              case aco_opcode::v_fma_legacy_f32:
+                              case aco_opcode::v_fmac_f32:
+                              case aco_opcode::v_fmac_legacy_f32:
+                              case aco_opcode::v_mac_f32:
+                              case aco_opcode::v_mac_legacy_f32:
+                              case aco_opcode::v_fma_f16:
+                              case aco_opcode::v_fmac_f16:
+                              case aco_opcode::v_mac_f16:
+                              case aco_opcode::v_dot4c_i32_i8:
+                              case aco_opcode::v_dot2c_f32_f16:
+                              case aco_opcode::v_dot2_f32_f16:
+                              case aco_opcode::v_dot2_f32_bf16:
+                              case aco_opcode::v_dot2_f16_f16:
+                              case aco_opcode::v_dot2_bf16_bf16:
+                              case aco_opcode::v_fma_mix_f32:
+                              case aco_opcode::v_fma_mixlo_f16:
+                              case aco_opcode::v_fma_mixhi_f16:
+                              case aco_opcode::v_pk_fmac_f16:
+                                    return aco_opcode::num_opcodes;
+                              default:
+                                    break;
+                        }
+                  }
+                  return swapped;
+            }
+
+            bool
+            can_swap_operands(aco_ptr<Instruction>& instr, aco_opcode* new_op, unsigned idx0, unsigned idx1)
+            {
+                  if (idx0 == idx1) {
+                        *new_op = instr->opcode;
+                        return true;
+                  }
+
+                  if (instr->isDPP())
+                        return false;
+
+                  if (!instr->isVOP3() && !instr->isVOP3P() && !instr->operands[0].isOfType(RegType::vgpr))
+                        return false;
+
+                  aco_opcode candidate = get_swapped_opcode(instr->opcode, idx0, idx1);
+                  if (candidate == aco_opcode::num_opcodes)
+                        return false;
+
+                  *new_op = candidate;
+                  return true;
+            }
+
+            wait_imm::wait_imm()
+            : exp(unset_counter), lgkm(unset_counter), vm(unset_counter), vs(unset_counter),
+            sample(unset_counter), bvh(unset_counter), km(unset_counter)
+            {}
+            wait_imm::wait_imm(uint16_t vm_, uint16_t exp_, uint16_t lgkm_, uint16_t vs_)
+            : exp(exp_), lgkm(lgkm_), vm(vm_), vs(vs_), sample(unset_counter), bvh(unset_counter),
+            km(unset_counter)
+            {}
+
+            uint16_t
+            wait_imm::pack(enum amd_gfx_level gfx_level) const
+            {
+                  uint16_t imm = 0;
+                  assert(exp == unset_counter || exp <= 0x7);
+                  if (gfx_level >= GFX11) {
+                        assert(lgkm == unset_counter || lgkm <= 0x3f);
+                        assert(vm == unset_counter || vm <= 0x3f);
+                        imm = ((vm & 0x3f) << 10) | ((lgkm & 0x3f) << 4) | (exp & 0x7);
+                  } else if (gfx_level >= GFX10) {
+                        assert(lgkm == unset_counter || lgkm <= 0x3f);
+                        assert(vm == unset_counter || vm <= 0x3f);
+                        imm = ((vm & 0x30) << 10) | ((lgkm & 0x3f) << 8) | ((exp & 0x7) << 4) | (vm & 0xf);
+                  } else if (gfx_level >= GFX9) {
+                        assert(lgkm == unset_counter || lgkm <= 0xf);
+                        assert(vm == unset_counter || vm <= 0x3f);
+                        imm = ((vm & 0x30) << 10) | ((lgkm & 0xf) << 8) | ((exp & 0x7) << 4) | (vm & 0xf);
+                  } else {
+                        assert(lgkm == unset_counter || lgkm <= 0xf);
+                        assert(vm == unset_counter || vm <= 0xf);
+                        imm = ((lgkm & 0xf) << 8) | ((exp & 0x7) << 4) | (vm & 0xf);
+                  }
+                  if (gfx_level < GFX9 && vm == wait_imm::unset_counter)
+                        imm |= 0xc000; /* should have no effect on pre-GFX9 and now we won't have to worry about the
                         architecture when interpreting the immediate */
-   return imm;
-}
-
-wait_imm
-wait_imm::max(enum amd_gfx_level gfx_level)
-{
-   wait_imm imm;
-   imm.vm = gfx_level >= GFX9 ? 63 : 15;
-   imm.exp = 7;
-   imm.lgkm = gfx_level >= GFX10 ? 63 : 15;
-   imm.vs = gfx_level >= GFX10 ? 63 : 0;
-   imm.sample = gfx_level >= GFX12 ? 63 : 0;
-   imm.bvh = gfx_level >= GFX12 ? 7 : 0;
-   imm.km = gfx_level >= GFX12 ? 31 : 0;
-   return imm;
-}
-
-bool
-wait_imm::unpack(enum amd_gfx_level gfx_level, const Instruction* instr)
-{
-   if (!instr->isSALU() || (!instr->operands.empty() && instr->operands[0].physReg() != sgpr_null))
-      return false;
-
-   aco_opcode op = instr->opcode;
-   uint16_t packed = instr->salu().imm;
-
-   if (op == aco_opcode::s_wait_loadcnt) {
-      vm = std::min<uint8_t>(vm, packed);
-   } else if (op == aco_opcode::s_wait_storecnt) {
-      vs = std::min<uint8_t>(vs, packed);
-   } else if (op == aco_opcode::s_wait_samplecnt) {
-      sample = std::min<uint8_t>(sample, packed);
-   } else if (op == aco_opcode::s_wait_bvhcnt) {
-      bvh = std::min<uint8_t>(bvh, packed);
-   } else if (op == aco_opcode::s_wait_expcnt) {
-      exp = std::min<uint8_t>(exp, packed);
-   } else if (op == aco_opcode::s_wait_dscnt) {
-      lgkm = std::min<uint8_t>(lgkm, packed);
-   } else if (op == aco_opcode::s_wait_kmcnt) {
-      km = std::min<uint8_t>(km, packed);
-   } else if (op == aco_opcode::s_wait_loadcnt_dscnt) {
-      uint32_t vm2 = (packed >> 8) & 0x3f;
-      uint32_t ds = packed & 0x3f;
-      vm = std::min<uint8_t>(vm, vm2 == 0x3f ? wait_imm::unset_counter : vm2);
-      lgkm = std::min<uint8_t>(lgkm, ds == 0x3f ? wait_imm::unset_counter : ds);
-   } else if (op == aco_opcode::s_wait_storecnt_dscnt) {
-      uint32_t vs2 = (packed >> 8) & 0x3f;
-      uint32_t ds = packed & 0x3f;
-      vs = std::min<uint8_t>(vs, vs2 == 0x3f ? wait_imm::unset_counter : vs2);
-      lgkm = std::min<uint8_t>(lgkm, ds == 0x3f ? wait_imm::unset_counter : ds);
-   } else if (op == aco_opcode::s_waitcnt_expcnt) {
-      exp = std::min<uint8_t>(exp, packed);
-   } else if (op == aco_opcode::s_waitcnt_lgkmcnt) {
-      lgkm = std::min<uint8_t>(lgkm, packed);
-   } else if (op == aco_opcode::s_waitcnt_vmcnt) {
-      vm = std::min<uint8_t>(vm, packed);
-   } else if (op == aco_opcode::s_waitcnt_vscnt) {
-      vs = std::min<uint8_t>(vs, packed);
-   } else if (op == aco_opcode::s_waitcnt) {
-      uint8_t vm2, lgkm2, exp2;
-      if (gfx_level >= GFX11) {
-         vm2 = (packed >> 10) & 0x3f;
-         lgkm2 = (packed >> 4) & 0x3f;
-         exp2 = packed & 0x7;
-      } else {
-         vm2 = packed & 0xf;
-         if (gfx_level >= GFX9)
-            vm2 |= (packed >> 10) & 0x30;
-
-         exp2 = (packed >> 4) & 0x7;
-
-         lgkm2 = (packed >> 8) & 0xf;
-         if (gfx_level >= GFX10)
-            lgkm2 |= (packed >> 8) & 0x30;
-      }
-
-      if (vm2 == (gfx_level >= GFX9 ? 0x3f : 0xf))
-         vm2 = wait_imm::unset_counter;
-      if (exp2 == 0x7)
-         exp2 = wait_imm::unset_counter;
-      if (lgkm2 == (gfx_level >= GFX10 ? 0x3f : 0xf))
-         lgkm2 = wait_imm::unset_counter;
-
-      vm = std::min(vm, vm2);
-      exp = std::min(exp, exp2);
-      lgkm = std::min(lgkm, lgkm2);
-   } else {
-      return false;
-   }
-   return true;
-}
-
-bool
-wait_imm::combine(const wait_imm& other)
-{
-   bool changed = false;
-   for (unsigned i = 0; i < wait_type_num; i++) {
-      if (other[i] < (*this)[i])
-         changed = true;
-      (*this)[i] = std::min((*this)[i], other[i]);
-   }
-   return changed;
-}
-
-bool
-wait_imm::empty() const
-{
-   for (unsigned i = 0; i < wait_type_num; i++) {
-      if ((*this)[i] != unset_counter)
-         return false;
-   }
-   return true;
-}
-
-void
-wait_imm::print(FILE* output) const
-{
-   const char* names[wait_type_num];
-   names[wait_type_exp] = "exp";
-   names[wait_type_vm] = "vm";
-   names[wait_type_lgkm] = "lgkm";
-   names[wait_type_vs] = "vs";
-   names[wait_type_sample] = "sample";
-   names[wait_type_bvh] = "bvh";
-   names[wait_type_km] = "km";
-   for (unsigned i = 0; i < wait_type_num; i++) {
-      if ((*this)[i] != unset_counter)
-         fprintf(output, "%s: %u\n", names[i], (*this)[i]);
-   }
-}
-
-void
-wait_imm::build_waitcnt(Builder& bld)
-{
-   enum amd_gfx_level gfx_level = bld.program->gfx_level;
-
-   if (gfx_level >= GFX12) {
-      if (vm != wait_imm::unset_counter && lgkm != wait_imm::unset_counter) {
-         bld.sopp(aco_opcode::s_wait_loadcnt_dscnt, (vm << 8) | lgkm);
-         vm = wait_imm::unset_counter;
-         lgkm = wait_imm::unset_counter;
-      }
-
-      if (vs != wait_imm::unset_counter && lgkm != wait_imm::unset_counter) {
-         bld.sopp(aco_opcode::s_wait_storecnt_dscnt, (vs << 8) | lgkm);
-         vs = wait_imm::unset_counter;
-         lgkm = wait_imm::unset_counter;
-      }
-
-      aco_opcode op[wait_type_num];
-      op[wait_type_exp] = aco_opcode::s_wait_expcnt;
-      op[wait_type_lgkm] = aco_opcode::s_wait_dscnt;
-      op[wait_type_vm] = aco_opcode::s_wait_loadcnt;
-      op[wait_type_vs] = aco_opcode::s_wait_storecnt;
-      op[wait_type_sample] = aco_opcode::s_wait_samplecnt;
-      op[wait_type_bvh] = aco_opcode::s_wait_bvhcnt;
-      op[wait_type_km] = aco_opcode::s_wait_kmcnt;
-
-      for (unsigned i = 0; i < wait_type_num; i++) {
-         if ((*this)[i] != wait_imm::unset_counter)
-            bld.sopp(op[i], (*this)[i]);
-      }
-   } else {
-      if (vs != wait_imm::unset_counter) {
-         assert(gfx_level >= GFX10);
-         bld.sopk(aco_opcode::s_waitcnt_vscnt, Operand(sgpr_null, s1), vs);
-         vs = wait_imm::unset_counter;
-      }
-      if (!empty())
-         bld.sopp(aco_opcode::s_waitcnt, pack(gfx_level));
-   }
-
-   *this = wait_imm();
-}
-
-bool
-should_form_clause(const Instruction* a, const Instruction* b)
-{
-   if (a->definitions.empty() != b->definitions.empty())
-      return false;
-
-   /* MUBUF and MTBUF can appear in the same clause. */
-   if ((a->isMTBUF() && b->isMUBUF()) || (a->isMUBUF() && b->isMTBUF())) {
-   } else if (a->format != b->format) {
-      return false;
-   }
-
-   if (a->operands.empty() || b->operands.empty())
-      return false;
-
-   /* Assume loads which don't use descriptors might load from similar addresses. */
-   if (a->isFlatLike() || a->accessesLDS())
-      return true;
-   if (a->isSMEM() && a->operands[0].bytes() == 8 && b->operands[0].bytes() == 8)
-      return true;
-
-   /* If they load from the same descriptor, assume they might load from similar
-    * addresses.
-    */
-   if (a->isVMEM() || a->isSMEM())
-      return a->operands[0].tempId() == b->operands[0].tempId();
-
-   if (a->isEXP() && b->isEXP())
-      return true;
-
-   return false;
-}
-
-aco::small_vec<uint32_t, 2>
-get_tied_defs(Instruction* instr)
-{
-   aco::small_vec<uint32_t, 2> ops;
-   if (instr->opcode == aco_opcode::v_interp_p2_f32 || instr->opcode == aco_opcode::v_mac_f32 ||
-       instr->opcode == aco_opcode::v_fmac_f32 || instr->opcode == aco_opcode::v_mac_f16 ||
-       instr->opcode == aco_opcode::v_fmac_f16 || instr->opcode == aco_opcode::v_mac_legacy_f32 ||
-       instr->opcode == aco_opcode::v_fmac_legacy_f32 ||
-       instr->opcode == aco_opcode::v_pk_fmac_f16 || instr->opcode == aco_opcode::v_writelane_b32 ||
-       instr->opcode == aco_opcode::v_writelane_b32_e64 ||
-       instr->opcode == aco_opcode::v_dot4c_i32_i8 || instr->opcode == aco_opcode::s_fmac_f32 ||
-       instr->opcode == aco_opcode::s_fmac_f16) {
-      ops.push_back(2);
-   } else if (instr->opcode == aco_opcode::s_addk_i32 || instr->opcode == aco_opcode::s_mulk_i32 ||
-              instr->opcode == aco_opcode::s_cmovk_i32 ||
-              instr->opcode == aco_opcode::ds_bvh_stack_push4_pop1_rtn_b32 ||
-              instr->opcode == aco_opcode::ds_bvh_stack_push8_pop1_rtn_b32 ||
-              instr->opcode == aco_opcode::ds_bvh_stack_push8_pop2_rtn_b64) {
-      ops.push_back(0);
-   } else if (instr->isMUBUF() && instr->definitions.size() == 1 && instr->operands.size() == 4) {
-      ops.push_back(3);
-   } else if (instr->isMIMG() && instr->definitions.size() == 1 &&
-              !instr->operands[2].isUndefined()) {
-      ops.push_back(2);
-   } else if (instr->opcode == aco_opcode::image_bvh8_intersect_ray) {
-      /* VADDR starts at 3. */
-      ops.push_back(3 + 4);
-      ops.push_back(3 + 7);
-   }
-   return ops;
-}
-
-uint8_t
-get_vmem_type(amd_gfx_level gfx_level, radeon_family family, Instruction* instr)
-{
-   if (instr->opcode == aco_opcode::image_bvh_intersect_ray ||
-       instr->opcode == aco_opcode::image_bvh64_intersect_ray ||
-       instr->opcode == aco_opcode::image_bvh_dual_intersect_ray ||
-       instr->opcode == aco_opcode::image_bvh8_intersect_ray) {
-      return vmem_bvh;
-   } else if (instr->opcode == aco_opcode::image_msaa_load) {
-      return vmem_sampler;
-   } else if (instr->isMIMG() && !instr->operands[1].isUndefined() &&
-              instr->operands[1].regClass() == s4) {
-      bool point_sample_accel = gfx_level == GFX11_5 && family != CHIP_GFX1153 &&
-                                (instr->opcode == aco_opcode::image_sample ||
-                                 instr->opcode == aco_opcode::image_sample_l ||
-                                 instr->opcode == aco_opcode::image_sample_lz);
-      return vmem_sampler | (point_sample_accel ? vmem_nosampler : 0);
-   } else if (instr->isVMEM() || instr->isScratch() || instr->isGlobal()) {
-      return vmem_nosampler;
-   }
-   return 0;
-}
-
-/* Parse implicit data dependency resolution:
- * Returns the value of each counter that must be reached
- * before an instruction is issued.
- *
- * (Probably incomplete.)
- */
-depctr_wait
-parse_depctr_wait(const Instruction* instr)
-{
-   depctr_wait res;
-   if (instr->isVMEM() || instr->isFlatLike() || instr->isDS() || instr->isEXP()) {
-      res.va_vdst = 0;
-      res.va_exec = 0;
-      res.sa_exec = 0;
-      if (instr->isVMEM() || instr->isFlatLike()) {
-         res.sa_sdst = 0;
-         res.va_sdst = 0;
-         res.va_vcc = 0;
-      }
-   } else if (instr->isSMEM()) {
-      res.sa_sdst = 0;
-      res.va_sdst = 0;
-      res.va_vcc = 0;
-   } else if (instr->isLDSDIR()) {
-      res.va_vdst = instr->ldsdir().wait_vdst;
-      res.va_exec = 0;
-      res.sa_exec = 0;
-   } else if (instr->opcode == aco_opcode::s_waitcnt_depctr) {
-      unsigned imm = instr->salu().imm;
-      res.va_vdst = (imm >> 12) & 0xf;
-      res.va_sdst = (imm >> 9) & 0x7;
-      res.va_ssrc = (imm >> 8) & 0x1;
-      res.hold_cnt = (imm >> 7) & 0x1;
-      res.vm_vsrc = (imm >> 2) & 0x7;
-      res.va_vcc = (imm >> 1) & 0x1;
-      res.sa_sdst = imm & 0x1;
-   } else if (instr->isVALU()) {
-      res.sa_exec = 0;
-      for (const Definition& def : instr->definitions) {
-         if (def.regClass().type() == RegType::sgpr) {
-            res.sa_sdst = 0;
-            /* Notably, this is the only exception, even VALU that
-             * reads exec doesn't implicitly wait for va_exec.
+                        if (gfx_level < GFX10 && lgkm == wait_imm::unset_counter)
+                              imm |= 0x3000; /* should have no effect on pre-GFX10 and now we won't have to worry about the
+                              architecture when interpreting the immediate */
+                              return imm;
+            }
+
+            wait_imm
+            wait_imm::max(enum amd_gfx_level gfx_level)
+            {
+                  wait_imm imm;
+                  imm.vm = gfx_level >= GFX9 ? 63 : 15;
+                  imm.exp = 7;
+                  imm.lgkm = gfx_level >= GFX10 ? 63 : 15;
+                  imm.vs = gfx_level >= GFX10 ? 63 : 0;
+                  imm.sample = gfx_level >= GFX12 ? 63 : 0;
+                  imm.bvh = gfx_level >= GFX12 ? 7 : 0;
+                  imm.km = gfx_level >= GFX12 ? 31 : 0;
+                  return imm;
+            }
+
+            bool
+            wait_imm::unpack(enum amd_gfx_level gfx_level, const Instruction* instr)
+            {
+                  if (!instr->isSALU() || (!instr->operands.empty() && instr->operands[0].physReg() != sgpr_null))
+                        return false;
+
+                  aco_opcode op = instr->opcode;
+                  uint16_t packed = instr->salu().imm;
+
+                  if (op == aco_opcode::s_wait_loadcnt) {
+                        vm = std::min<uint8_t>(vm, packed);
+                  } else if (op == aco_opcode::s_wait_storecnt) {
+                        vs = std::min<uint8_t>(vs, packed);
+                  } else if (op == aco_opcode::s_wait_samplecnt) {
+                        sample = std::min<uint8_t>(sample, packed);
+                  } else if (op == aco_opcode::s_wait_bvhcnt) {
+                        bvh = std::min<uint8_t>(bvh, packed);
+                  } else if (op == aco_opcode::s_wait_expcnt) {
+                        exp = std::min<uint8_t>(exp, packed);
+                  } else if (op == aco_opcode::s_wait_dscnt) {
+                        lgkm = std::min<uint8_t>(lgkm, packed);
+                  } else if (op == aco_opcode::s_wait_kmcnt) {
+                        km = std::min<uint8_t>(km, packed);
+                  } else if (op == aco_opcode::s_wait_loadcnt_dscnt) {
+                        uint32_t vm2 = (packed >> 8) & 0x3f;
+                        uint32_t ds = packed & 0x3f;
+                        vm = std::min<uint8_t>(vm, vm2 == 0x3f ? wait_imm::unset_counter : vm2);
+                        lgkm = std::min<uint8_t>(lgkm, ds == 0x3f ? wait_imm::unset_counter : ds);
+                  } else if (op == aco_opcode::s_wait_storecnt_dscnt) {
+                        uint32_t vs2 = (packed >> 8) & 0x3f;
+                        uint32_t ds = packed & 0x3f;
+                        vs = std::min<uint8_t>(vs, vs2 == 0x3f ? wait_imm::unset_counter : vs2);
+                        lgkm = std::min<uint8_t>(lgkm, ds == 0x3f ? wait_imm::unset_counter : ds);
+                  } else if (op == aco_opcode::s_waitcnt_expcnt) {
+                        exp = std::min<uint8_t>(exp, packed);
+                  } else if (op == aco_opcode::s_waitcnt_lgkmcnt) {
+                        lgkm = std::min<uint8_t>(lgkm, packed);
+                  } else if (op == aco_opcode::s_waitcnt_vmcnt) {
+                        vm = std::min<uint8_t>(vm, packed);
+                  } else if (op == aco_opcode::s_waitcnt_vscnt) {
+                        vs = std::min<uint8_t>(vs, packed);
+                  } else if (op == aco_opcode::s_waitcnt) {
+                        uint8_t vm2, lgkm2, exp2;
+                        if (gfx_level >= GFX11) {
+                              vm2 = (packed >> 10) & 0x3f;
+                              lgkm2 = (packed >> 4) & 0x3f;
+                              exp2 = packed & 0x7;
+                        } else {
+                              vm2 = packed & 0xf;
+                              if (gfx_level >= GFX9)
+                                    vm2 |= (packed >> 10) & 0x30;
+
+                              exp2 = (packed >> 4) & 0x7;
+
+                              lgkm2 = (packed >> 8) & 0xf;
+                              if (gfx_level >= GFX10)
+                                    lgkm2 |= (packed >> 8) & 0x30;
+                        }
+
+                        if (vm2 == (gfx_level >= GFX9 ? 0x3f : 0xf))
+                              vm2 = wait_imm::unset_counter;
+                        if (exp2 == 0x7)
+                              exp2 = wait_imm::unset_counter;
+                        if (lgkm2 == (gfx_level >= GFX10 ? 0x3f : 0xf))
+                              lgkm2 = wait_imm::unset_counter;
+
+                        vm = std::min(vm, vm2);
+                        exp = std::min(exp, exp2);
+                        lgkm = std::min(lgkm, lgkm2);
+                  } else {
+                        return false;
+                  }
+                  return true;
+            }
+
+            bool
+            wait_imm::combine(const wait_imm& other)
+            {
+                  bool changed = false;
+                  for (unsigned i = 0; i < wait_type_num; i++) {
+                        if (other[i] < (*this)[i])
+                              changed = true;
+                        (*this)[i] = std::min((*this)[i], other[i]);
+                  }
+                  return changed;
+            }
+
+            bool
+            wait_imm::empty() const
+            {
+                  for (unsigned i = 0; i < wait_type_num; i++) {
+                        if ((*this)[i] != unset_counter)
+                              return false;
+                  }
+                  return true;
+            }
+
+            void
+            wait_imm::print(FILE* output) const
+            {
+                  const char* names[wait_type_num];
+                  names[wait_type_exp] = "exp";
+                  names[wait_type_vm] = "vm";
+                  names[wait_type_lgkm] = "lgkm";
+                  names[wait_type_vs] = "vs";
+                  names[wait_type_sample] = "sample";
+                  names[wait_type_bvh] = "bvh";
+                  names[wait_type_km] = "km";
+                  for (unsigned i = 0; i < wait_type_num; i++) {
+                        if ((*this)[i] != unset_counter)
+                              fprintf(output, "%s: %u\n", names[i], (*this)[i]);
+                  }
+            }
+
+            void
+            wait_imm::build_waitcnt(Builder& bld)
+            {
+                  enum amd_gfx_level gfx_level = bld.program->gfx_level;
+
+                  if (gfx_level >= GFX12) {
+                        if (vm != wait_imm::unset_counter && lgkm != wait_imm::unset_counter) {
+                              bld.sopp(aco_opcode::s_wait_loadcnt_dscnt, (vm << 8) | lgkm);
+                              vm = wait_imm::unset_counter;
+                              lgkm = wait_imm::unset_counter;
+                        }
+
+                        if (vs != wait_imm::unset_counter && lgkm != wait_imm::unset_counter) {
+                              bld.sopp(aco_opcode::s_wait_storecnt_dscnt, (vs << 8) | lgkm);
+                              vs = wait_imm::unset_counter;
+                              lgkm = wait_imm::unset_counter;
+                        }
+
+                        aco_opcode op[wait_type_num];
+                        op[wait_type_exp] = aco_opcode::s_wait_expcnt;
+                        op[wait_type_lgkm] = aco_opcode::s_wait_dscnt;
+                        op[wait_type_vm] = aco_opcode::s_wait_loadcnt;
+                        op[wait_type_vs] = aco_opcode::s_wait_storecnt;
+                        op[wait_type_sample] = aco_opcode::s_wait_samplecnt;
+                        op[wait_type_bvh] = aco_opcode::s_wait_bvhcnt;
+                        op[wait_type_km] = aco_opcode::s_wait_kmcnt;
+
+                        for (unsigned i = 0; i < wait_type_num; i++) {
+                              if ((*this)[i] != wait_imm::unset_counter)
+                                    bld.sopp(op[i], (*this)[i]);
+                        }
+                  } else {
+                        if (vs != wait_imm::unset_counter) {
+                              assert(gfx_level >= GFX10);
+                              bld.sopk(aco_opcode::s_waitcnt_vscnt, Operand(sgpr_null, s1), vs);
+                              vs = wait_imm::unset_counter;
+                        }
+                        if (!empty())
+                              bld.sopp(aco_opcode::s_waitcnt, pack(gfx_level));
+                  }
+
+                  *this = wait_imm();
+            }
+
+            bool
+            should_form_clause(const Instruction* a, const Instruction* b)
+            {
+                  if (a->definitions.empty() != b->definitions.empty())
+                        return false;
+
+                  /* MUBUF and MTBUF can appear in the same clause. */
+                  if ((a->isMTBUF() && b->isMUBUF()) || (a->isMUBUF() && b->isMTBUF())) {
+                  } else if (a->format != b->format) {
+                        return false;
+                  }
+
+                  if (a->operands.empty() || b->operands.empty())
+                        return false;
+
+                  /* Assume loads which don't use descriptors might load from similar addresses. */
+                  if (a->isFlatLike() || a->accessesLDS())
+                        return true;
+                  if (a->isSMEM() && a->operands[0].bytes() == 8 && b->operands[0].bytes() == 8)
+                        return true;
+
+                  /* If they load from the same descriptor, assume they might load from similar
+                   * addresses.
+                   */
+                  if (a->isVMEM() || a->isSMEM())
+                        return a->operands[0].tempId() == b->operands[0].tempId();
+
+                  if (a->isEXP() && b->isEXP())
+                        return true;
+
+                  return false;
+            }
+
+            aco::small_vec<uint32_t, 2>
+            get_tied_defs(Instruction* instr)
+            {
+                  aco::small_vec<uint32_t, 2> ops;
+                  if (instr->opcode == aco_opcode::v_interp_p2_f32 || instr->opcode == aco_opcode::v_mac_f32 ||
+                        instr->opcode == aco_opcode::v_fmac_f32 || instr->opcode == aco_opcode::v_mac_f16 ||
+                        instr->opcode == aco_opcode::v_fmac_f16 || instr->opcode == aco_opcode::v_mac_legacy_f32 ||
+                        instr->opcode == aco_opcode::v_fmac_legacy_f32 ||
+                        instr->opcode == aco_opcode::v_pk_fmac_f16 || instr->opcode == aco_opcode::v_writelane_b32 ||
+                        instr->opcode == aco_opcode::v_writelane_b32_e64 ||
+                        instr->opcode == aco_opcode::v_dot4c_i32_i8 || instr->opcode == aco_opcode::s_fmac_f32 ||
+                        instr->opcode == aco_opcode::s_fmac_f16) {
+                        ops.push_back(2);
+                        } else if (instr->opcode == aco_opcode::s_addk_i32 || instr->opcode == aco_opcode::s_mulk_i32 ||
+                              instr->opcode == aco_opcode::s_cmovk_i32 ||
+                              instr->opcode == aco_opcode::ds_bvh_stack_push4_pop1_rtn_b32) {
+                              ops.push_back(0);
+                              } else if (instr->isMUBUF() && instr->definitions.size() == 1 && instr->operands.size() == 4) {
+                                    ops.push_back(3);
+                              } else if (instr->isMIMG() && instr->definitions.size() == 1 &&
+                                    !instr->operands[2].isUndefined()) {
+                                    ops.push_back(2);
+                                    } else if (instr->opcode == aco_opcode::image_bvh8_intersect_ray) {
+                                          /* VADDR starts at 3. */
+                                          ops.push_back(3 + 4);
+                                          ops.push_back(3 + 7);
+                                    }
+                                    return ops;
+            }
+
+            uint8_t
+            get_vmem_type(amd_gfx_level gfx_level, radeon_family family, Instruction* instr)
+            {
+                  if (instr->opcode == aco_opcode::image_bvh_intersect_ray ||
+                        instr->opcode == aco_opcode::image_bvh64_intersect_ray ||
+                        instr->opcode == aco_opcode::image_bvh_dual_intersect_ray ||
+                        instr->opcode == aco_opcode::image_bvh8_intersect_ray) {
+                        return vmem_bvh;
+                        } else if (instr->opcode == aco_opcode::image_msaa_load) {
+                              return vmem_sampler;
+                        } else if (instr->isMIMG() && !instr->operands[1].isUndefined() &&
+                              instr->operands[1].regClass() == s4) {
+                              bool point_sample_accel = gfx_level == GFX11_5 && family != CHIP_GFX1153 &&
+                              (instr->opcode == aco_opcode::image_sample ||
+                              instr->opcode == aco_opcode::image_sample_l ||
+                              instr->opcode == aco_opcode::image_sample_lz);
+                        return vmem_sampler | (point_sample_accel ? vmem_nosampler : 0);
+                              } else if (instr->isVMEM() || instr->isScratch() || instr->isGlobal()) {
+                                    return vmem_nosampler;
+                              }
+                              return 0;
+            }
+
+            /* Parse implicit data dependency resolution:
+             * Returns the value of each counter that must be reached
+             * before an instruction is issued.
+             *
+             * (Probably incomplete.)
              */
-            if (instr->opcode == aco_opcode::v_readfirstlane_b32)
-               res.va_exec = 0;
-            break;
-         }
-      }
-   } else if (instr_info.classes[(int)instr->opcode] == instr_class::branch ||
-              instr_info.classes[(int)instr->opcode] == instr_class::sendmsg) {
-      res.sa_exec = 0;
-      res.va_exec = 0;
-      switch (instr->opcode) {
-      case aco_opcode::s_cbranch_vccz:
-      case aco_opcode::s_cbranch_vccnz:
-         res.va_vcc = 0;
-         res.sa_sdst = 0;
-         break;
-      case aco_opcode::s_cbranch_scc0:
-      case aco_opcode::s_cbranch_scc1:
-         res.sa_sdst = 0;
-         break;
-      default: break;
-      }
-   } else if (instr->isSALU()) {
-      for (const Definition& def : instr->definitions) {
-         if (def.physReg() < vcc) {
-            res.va_sdst = 0;
-         } else if (def.physReg() <= vcc_hi) {
-            res.va_vcc = 0;
-         } else if (def.physReg() == exec || def.physReg() == exec_hi) {
-            res.va_exec = 0;
-         }
-      }
-      for (const Operand& op : instr->operands) {
-         if (op.physReg() < vcc) {
-            res.va_sdst = 0;
-         } else if (op.physReg() <= vcc_hi) {
-            res.va_vcc = 0;
-         } else if (op.physReg() == exec || op.physReg() == exec_hi) {
-            res.va_exec = 0;
-         }
-      }
-   }
-
-   return res;
-}
-
-bool
-dealloc_vgprs(Program* program)
-{
-   if (program->gfx_level < GFX11)
-      return false;
-
-   /* If we insert the sendmsg on GFX11.5, the export priority workaround will require us to insert
-    * a wait after exports. There might still be pending VMEM stores for PS parameter exports,
-    * except NGG lowering usually inserts a memory barrier. This means there is unlikely to be any
-    * pending VMEM stores or exports if we insert the sendmsg for these stages. */
-   if (program->gfx_level == GFX11_5 && (program->stage.hw == AC_HW_NEXT_GEN_GEOMETRY_SHADER ||
-                                         program->stage.hw == AC_HW_PIXEL_SHADER))
-      return false;
-
-   Block& block = program->blocks.back();
-
-   /* don't bother checking if there is a pending VMEM store or export: there almost always is */
-   Builder bld(program);
-   if (!block.instructions.empty() && block.instructions.back()->opcode == aco_opcode::s_endpgm) {
-      bld.reset(&block.instructions, block.instructions.begin() + (block.instructions.size() - 1));
-      bld.sopp(aco_opcode::s_sendmsg, sendmsg_dealloc_vgprs);
-   }
-
-   return true;
-}
-
-bool
-Instruction::isTrans() const noexcept
-{
-   return instr_info.classes[(int)opcode] == instr_class::valu_transcendental32 ||
-          instr_info.classes[(int)opcode] == instr_class::valu_double_transcendental ||
-          instr_info.classes[(int)opcode] == instr_class::valu_pseudo_scalar_trans;
-}
-
-size_t
-get_instr_data_size(Format format)
-{
-   switch (format) {
-   case Format::SOP1:
-   case Format::SOP2:
-   case Format::SOPC:
-   case Format::SOPK:
-   case Format::SOPP: return sizeof(SALU_instruction);
-   case Format::SMEM: return sizeof(SMEM_instruction);
-   case Format::PSEUDO: return sizeof(Pseudo_instruction);
-   case Format::PSEUDO_BARRIER: return sizeof(Pseudo_barrier_instruction);
-   case Format::PSEUDO_REDUCTION: return sizeof(Pseudo_reduction_instruction);
-   case Format::PSEUDO_BRANCH: return sizeof(Pseudo_branch_instruction);
-   case Format::DS: return sizeof(DS_instruction);
-   case Format::FLAT:
-   case Format::GLOBAL:
-   case Format::SCRATCH: return sizeof(FLAT_instruction);
-   case Format::LDSDIR: return sizeof(LDSDIR_instruction);
-   case Format::MTBUF: return sizeof(MTBUF_instruction);
-   case Format::MUBUF: return sizeof(MUBUF_instruction);
-   case Format::MIMG: return sizeof(MIMG_instruction);
-   case Format::VOPD: return sizeof(VOPD_instruction);
-   case Format::VINTERP_INREG: return sizeof(VINTERP_inreg_instruction);
-   case Format::VINTRP: return sizeof(VINTRP_instruction);
-   case Format::EXP: return sizeof(Export_instruction);
-   default:
-      if ((uint16_t)format & (uint16_t)Format::DPP16)
-         return sizeof(DPP16_instruction);
-      else if ((uint16_t)format & (uint16_t)Format::DPP8)
-         return sizeof(DPP8_instruction);
-      else if ((uint16_t)format & (uint16_t)Format::SDWA)
-         return sizeof(SDWA_instruction);
-      else
-         return sizeof(VALU_instruction);
-   }
-}
-
-Instruction*
-create_instruction(aco_opcode opcode, Format format, uint32_t num_operands,
-                   uint32_t num_definitions)
-{
-   size_t size = get_instr_data_size(format);
-   size_t total_size = size + num_operands * sizeof(Operand) + num_definitions * sizeof(Definition);
-
-   void* data = instruction_buffer->allocate(total_size, alignof(uint32_t));
-   memset(data, 0, total_size);
-   Instruction* inst = (Instruction*)data;
-
-   inst->opcode = opcode;
-   inst->format = format;
-
-   uint16_t operands_offset = size - offsetof(Instruction, operands);
-   inst->operands = aco::span<Operand>(operands_offset, num_operands);
-   uint16_t definitions_offset = (char*)inst->operands.end() - (char*)&inst->definitions;
-   inst->definitions = aco::span<Definition>(definitions_offset, num_definitions);
-
-   return inst;
-}
-
-Temp
-load_scratch_resource(Program* program, Builder& bld, unsigned resume_idx,
-                      bool apply_scratch_offset)
-{
-   if (program->static_scratch_rsrc != Temp()) {
-      /* We can't apply any offsets when using a static resource. */
-      assert(!apply_scratch_offset || program->scratch_offsets.empty());
-      return program->static_scratch_rsrc;
-   }
-   Temp private_segment_buffer;
-   if (!program->private_segment_buffers.empty())
-      private_segment_buffer = program->private_segment_buffers[resume_idx];
-   if (!private_segment_buffer.bytes()) {
-      Temp addr_lo =
-         bld.sop1(aco_opcode::p_load_symbol, bld.def(s1), Operand::c32(aco_symbol_scratch_addr_lo));
-      Temp addr_hi =
-         bld.sop1(aco_opcode::p_load_symbol, bld.def(s1), Operand::c32(aco_symbol_scratch_addr_hi));
-      private_segment_buffer =
-         bld.pseudo(aco_opcode::p_create_vector, bld.def(s2), addr_lo, addr_hi);
-   } else if (program->stage.hw != AC_HW_COMPUTE_SHADER) {
-      private_segment_buffer =
-         bld.smem(aco_opcode::s_load_dwordx2, bld.def(s2), private_segment_buffer, Operand::zero());
-   }
-
-   if (apply_scratch_offset && !program->scratch_offsets.empty()) {
-      Temp addr_lo = bld.tmp(s1);
-      Temp addr_hi = bld.tmp(s1);
-      bld.pseudo(aco_opcode::p_split_vector, Definition(addr_lo), Definition(addr_hi),
-                 private_segment_buffer);
-
-      Temp carry = bld.tmp(s1);
-      Temp scratch_offset = program->scratch_offsets[resume_idx];
-      addr_lo = bld.sop2(aco_opcode::s_add_u32, bld.def(s1), bld.scc(Definition(carry)), addr_lo,
-                         scratch_offset);
-      addr_hi = bld.sop2(aco_opcode::s_addc_u32, bld.def(s1), bld.def(s1, scc), addr_hi,
-                         Operand::c32(0), bld.scc(carry));
-
-      private_segment_buffer =
-         bld.pseudo(aco_opcode::p_create_vector, bld.def(s2), addr_lo, addr_hi);
-   }
-
-   struct ac_buffer_state ac_state = {0};
-   uint32_t desc[4];
-
-   ac_state.size = 0xffffffff;
-   ac_state.format = PIPE_FORMAT_R32_FLOAT;
-   for (int i = 0; i < 4; i++)
-      ac_state.swizzle[i] = PIPE_SWIZZLE_0;
-   /* older generations need element size = 4 bytes. element size removed in GFX9 */
-   ac_state.element_size = program->gfx_level <= GFX8 ? 1u : 0u;
-   ac_state.index_stride = program->wave_size == 64 ? 3u : 2u;
-   ac_state.add_tid = true;
-   ac_state.gfx10_oob_select = V_008F0C_OOB_SELECT_RAW;
-
-   ac_build_buffer_descriptor(program->gfx_level, &ac_state, desc);
-
-   return bld.pseudo(aco_opcode::p_create_vector, bld.def(s4), private_segment_buffer,
-                     Operand::c32(desc[2]), Operand::c32(desc[3]));
-}
+            depctr_wait
+            parse_depctr_wait(const Instruction* instr)
+            {
+                  depctr_wait res;
+                  if (instr->isVMEM() || instr->isFlatLike() || instr->isDS() || instr->isEXP()) {
+                        res.va_vdst = 0;
+                        res.va_exec = 0;
+                        res.sa_exec = 0;
+                        if (instr->isVMEM() || instr->isFlatLike()) {
+                              res.sa_sdst = 0;
+                              res.va_sdst = 0;
+                              res.va_vcc = 0;
+                        }
+                  } else if (instr->isSMEM()) {
+                        res.sa_sdst = 0;
+                        res.va_sdst = 0;
+                        res.va_vcc = 0;
+                  } else if (instr->isLDSDIR()) {
+                        res.va_vdst = instr->ldsdir().wait_vdst;
+                        res.va_exec = 0;
+                        res.sa_exec = 0;
+                  } else if (instr->opcode == aco_opcode::s_waitcnt_depctr) {
+                        unsigned imm = instr->salu().imm;
+                        res.va_vdst = (imm >> 12) & 0xf;
+                        res.va_sdst = (imm >> 9) & 0x7;
+                        res.va_ssrc = (imm >> 8) & 0x1;
+                        res.hold_cnt = (imm >> 7) & 0x1;
+                        res.vm_vsrc = (imm >> 2) & 0x7;
+                        res.va_vcc = (imm >> 1) & 0x1;
+                        res.sa_sdst = imm & 0x1;
+                  } else if (instr->isVALU()) {
+                        res.sa_exec = 0;
+                        for (const Definition& def : instr->definitions) {
+                              if (def.regClass().type() == RegType::sgpr) {
+                                    res.sa_sdst = 0;
+                                    /* Notably, this is the only exception, even VALU that
+                                     * reads exec doesn't implicitly wait for va_exec.
+                                     */
+                                    if (instr->opcode == aco_opcode::v_readfirstlane_b32)
+                                          res.va_exec = 0;
+                                    break;
+                              }
+                        }
+                  } else if (instr_info.classes[(int)instr->opcode] == instr_class::branch ||
+                        instr_info.classes[(int)instr->opcode] == instr_class::sendmsg) {
+                        res.sa_exec = 0;
+                  res.va_exec = 0;
+                  switch (instr->opcode) {
+                        case aco_opcode::s_cbranch_vccz:
+                        case aco_opcode::s_cbranch_vccnz:
+                              res.va_vcc = 0;
+                              res.sa_sdst = 0;
+                              break;
+                        case aco_opcode::s_cbranch_scc0:
+                        case aco_opcode::s_cbranch_scc1:
+                              res.sa_sdst = 0;
+                              break;
+                        default: break;
+                  }
+                        } else if (instr->isSALU()) {
+                              for (const Definition& def : instr->definitions) {
+                                    if (def.physReg() < vcc) {
+                                          res.va_sdst = 0;
+                                    } else if (def.physReg() <= vcc_hi) {
+                                          res.va_vcc = 0;
+                                    } else if (def.physReg() == exec || def.physReg() == exec_hi) {
+                                          res.va_exec = 0;
+                                    }
+                              }
+                              for (const Operand& op : instr->operands) {
+                                    if (op.physReg() < vcc) {
+                                          res.va_sdst = 0;
+                                    } else if (op.physReg() <= vcc_hi) {
+                                          res.va_vcc = 0;
+                                    } else if (op.physReg() == exec || op.physReg() == exec_hi) {
+                                          res.va_exec = 0;
+                                    }
+                              }
+                        }
+
+                        return res;
+            }
+
+            bool
+            dealloc_vgprs(Program* program)
+            {
+                  if (program->gfx_level < GFX11)
+                        return false;
+
+                  /* If we insert the sendmsg on GFX11.5, the export priority workaround will require us to insert
+                   * a wait after exports. There might still be pending VMEM stores for PS parameter exports,
+                   * except NGG lowering usually inserts a memory barrier. This means there is unlikely to be any
+                   * pending VMEM stores or exports if we insert the sendmsg for these stages. */
+                  if (program->gfx_level == GFX11_5 && (program->stage.hw == AC_HW_NEXT_GEN_GEOMETRY_SHADER ||
+                        program->stage.hw == AC_HW_PIXEL_SHADER))
+                        return false;
+
+                  Block& block = program->blocks.back();
+
+                  /* don't bother checking if there is a pending VMEM store or export: there almost always is */
+                  Builder bld(program);
+                  if (!block.instructions.empty() && block.instructions.back()->opcode == aco_opcode::s_endpgm) {
+                        bld.reset(&block.instructions, block.instructions.begin() + (block.instructions.size() - 1));
+                        bld.sopp(aco_opcode::s_sendmsg, sendmsg_dealloc_vgprs);
+                  }
+
+                  return true;
+            }
+
+            bool
+            Instruction::isTrans() const noexcept
+            {
+                  return instr_info.classes[(int)opcode] == instr_class::valu_transcendental32 ||
+                  instr_info.classes[(int)opcode] == instr_class::valu_double_transcendental ||
+                  instr_info.classes[(int)opcode] == instr_class::valu_pseudo_scalar_trans;
+            }
+
+            static constexpr auto instr_data_size_table = []() {
+                  std::array<size_t, 256> table{};
+                  table[static_cast<size_t>(Format::SOP1)] = sizeof(SALU_instruction);
+                  table[static_cast<size_t>(Format::SOP2)] = sizeof(SALU_instruction);
+                  table[static_cast<size_t>(Format::SOPC)] = sizeof(SALU_instruction);
+                  table[static_cast<size_t>(Format::SOPK)] = sizeof(SALU_instruction);
+                  table[static_cast<size_t>(Format::SOPP)] = sizeof(SALU_instruction);
+                  table[static_cast<size_t>(Format::SMEM)] = sizeof(SMEM_instruction);
+                  table[static_cast<size_t>(Format::PSEUDO)] = sizeof(Pseudo_instruction);
+                  table[static_cast<size_t>(Format::PSEUDO_BARRIER)] = sizeof(Pseudo_barrier_instruction);
+                  table[static_cast<size_t>(Format::PSEUDO_REDUCTION)] = sizeof(Pseudo_reduction_instruction);
+                  table[static_cast<size_t>(Format::PSEUDO_BRANCH)] = sizeof(Pseudo_branch_instruction);
+                  table[static_cast<size_t>(Format::DS)] = sizeof(DS_instruction);
+                  table[static_cast<size_t>(Format::FLAT)] = sizeof(FLAT_instruction);
+                  table[static_cast<size_t>(Format::GLOBAL)] = sizeof(FLAT_instruction);
+                  table[static_cast<size_t>(Format::SCRATCH)] = sizeof(FLAT_instruction);
+                  table[static_cast<size_t>(Format::LDSDIR)] = sizeof(LDSDIR_instruction);
+                  table[static_cast<size_t>(Format::MTBUF)] = sizeof(MTBUF_instruction);
+                  table[static_cast<size_t>(Format::MUBUF)] = sizeof(MUBUF_instruction);
+                  table[static_cast<size_t>(Format::MIMG)] = sizeof(MIMG_instruction);
+                  table[static_cast<size_t>(Format::VOPD)] = sizeof(VOPD_instruction);
+                  table[static_cast<size_t>(Format::VINTERP_INREG)] = sizeof(VINTERP_inreg_instruction);
+                  table[static_cast<size_t>(Format::VINTRP)] = sizeof(VINTRP_instruction);
+                  table[static_cast<size_t>(Format::EXP)] = sizeof(Export_instruction);
+                  return table;
+            }();
+
+            size_t
+            get_instr_data_size(Format format)
+            {
+                  if ((uint16_t)format & (uint16_t)Format::DPP16)
+                        return sizeof(DPP16_instruction);
+                  else if ((uint16_t)format & (uint16_t)Format::DPP8)
+                        return sizeof(DPP8_instruction);
+                  else if ((uint16_t)format & (uint16_t)Format::SDWA)
+                        return sizeof(SDWA_instruction);
+
+                  size_t size = instr_data_size_table[static_cast<uint16_t>(format) & 0xFF];
+                  if (size > 0) [[likely]]
+                        return size;
+                  else
+                        return sizeof(VALU_instruction);
+            }
+
+            Temp
+            load_scratch_resource(Program* program, Builder& bld, unsigned resume_idx,
+                                  bool apply_scratch_offset)
+            {
+                  if (program->static_scratch_rsrc != Temp()) {
+                        /* We can't apply any offsets when using a static resource. */
+                        assert(!apply_scratch_offset || program->scratch_offsets.empty());
+                        return program->static_scratch_rsrc;
+                  }
+                  Temp private_segment_buffer;
+                  if (!program->private_segment_buffers.empty())
+                        private_segment_buffer = program->private_segment_buffers[resume_idx];
+                  if (!private_segment_buffer.bytes()) {
+                        Temp addr_lo =
+                        bld.sop1(aco_opcode::p_load_symbol, bld.def(s1), Operand::c32(aco_symbol_scratch_addr_lo));
+                        Temp addr_hi =
+                        bld.sop1(aco_opcode::p_load_symbol, bld.def(s1), Operand::c32(aco_symbol_scratch_addr_hi));
+                        private_segment_buffer =
+                        bld.pseudo(aco_opcode::p_create_vector, bld.def(s2), addr_lo, addr_hi);
+                  } else if (program->stage.hw != AC_HW_COMPUTE_SHADER) {
+                        private_segment_buffer =
+                        bld.smem(aco_opcode::s_load_dwordx2, bld.def(s2), private_segment_buffer, Operand::zero());
+                  }
+
+                  if (apply_scratch_offset && !program->scratch_offsets.empty()) {
+                        Temp addr_lo = bld.tmp(s1);
+                        Temp addr_hi = bld.tmp(s1);
+                        bld.pseudo(aco_opcode::p_split_vector, Definition(addr_lo), Definition(addr_hi),
+                                   private_segment_buffer);
+
+                        Temp carry = bld.tmp(s1);
+                        Temp scratch_offset = program->scratch_offsets[resume_idx];
+                        addr_lo = bld.sop2(aco_opcode::s_add_u32, bld.def(s1), bld.scc(Definition(carry)), addr_lo,
+                                           scratch_offset);
+                        addr_hi = bld.sop2(aco_opcode::s_addc_u32, bld.def(s1), bld.def(s1, scc), addr_hi,
+                                           Operand::c32(0), bld.scc(carry));
+
+                        private_segment_buffer =
+                        bld.pseudo(aco_opcode::p_create_vector, bld.def(s2), addr_lo, addr_hi);
+                  }
+
+                  struct ac_buffer_state ac_state = {0};
+                  uint32_t desc[4];
+
+                  ac_state.size = 0xffffffff;
+                  ac_state.format = PIPE_FORMAT_R32_FLOAT;
+                  for (int i = 0; i < 4; i++)
+                        ac_state.swizzle[i] = PIPE_SWIZZLE_0;
+                  /* older generations need element size = 4 bytes. element size removed in GFX9 */
+                  ac_state.element_size = program->gfx_level <= GFX8 ? 1u : 0u;
+                  ac_state.index_stride = program->wave_size == 64 ? 3u : 2u;
+                  ac_state.add_tid = true;
+                  ac_state.gfx10_oob_select = V_008F0C_OOB_SELECT_RAW;
+
+                  ac_build_buffer_descriptor(program->gfx_level, &ac_state, desc);
+
+                  return bld.pseudo(aco_opcode::p_create_vector, bld.def(s4), private_segment_buffer,
+                                    Operand::c32(desc[2]), Operand::c32(desc[3]));
+            }
+
+            Instruction*
+            create_instruction(aco_opcode opcode, Format format, uint32_t num_operands,
+                               uint32_t num_definitions)
+            {
+                  size_t size = get_instr_data_size(format);
+                  size_t total_size = size + num_operands * sizeof(Operand) + num_definitions * sizeof(Definition);
+
+                  void* data = instruction_buffer->allocate(total_size, alignof(uint32_t));
+                  memset(data, 0, total_size);
+                  Instruction* inst = (Instruction*)data;
+
+                  inst->opcode = opcode;
+                  inst->format = format;
+
+                  uint16_t operands_offset = size - offsetof(Instruction, operands);
+                  inst->operands = aco::span<Operand>(operands_offset, num_operands);
+                  uint16_t definitions_offset = (char*)inst->operands.end() - (char*)&inst->definitions;
+                  inst->definitions = aco::span<Definition>(definitions_offset, num_definitions);
+
+                  return inst;
+            }
 
 } // namespace aco

--- a/src/amd/compiler/aco_opcodes.py	2025-05-31 22:57:26.003334290 +0200
+++ b/src/amd/compiler/aco_opcodes.py	2025-06-01 00:30:01.222104109 +0200
@@ -986,7 +986,6 @@ VOP2 = {
    ("v_fmac_f16",          dst(F16),      src(F16, F16, F16), op(gfx10=0x36)),
    ("v_fmamk_f16",         dst(noMods(F16)), noMods(src(F16, F16, IMM)), op(gfx10=0x37)),
    ("v_fmaak_f16",         dst(noMods(F16)), noMods(src(F16, F16, IMM)), op(gfx10=0x38)),
-   ("v_pk_fmac_f16",       dst(noMods(PkF16)), noMods(src(PkF16, PkF16, PkF16)), op(gfx10=0x3c)),
    ("v_dot2c_f32_f16",     dst(noMods(F32)), noMods(src(PkF16, PkF16, F32)), op(gfx9=0x37, gfx10=0x02, gfx12=-1)), #v_dot2acc_f32_f16 in GFX11
    ("v_add_f64",           dst(F64),      src(F64, F64), op(gfx12=0x02), InstrClass.ValuDoubleAdd),
    ("v_mul_f64",           dst(F64),      src(F64, F64), op(gfx12=0x06), InstrClass.ValuDoubleAdd),
@@ -1199,15 +1198,16 @@ VOPP = {
    ("v_pk_max_u16",     dst(PkU16), src(PkU16, PkU16), op(gfx9=0x0c)),
    ("v_pk_min_u16",     dst(PkU16), src(PkU16, PkU16), op(gfx9=0x0d)),
    ("v_pk_fma_f16",     dst(PkF16), src(PkF16, PkF16, PkF16), op(gfx9=0x0e)),
+   ("v_pk_fmac_f16",    dst(PkF16), src(PkF16, PkF16, PkF16), op(gfx9=0x1ea, gfx10=0x3c), InstrClass.ValuFma),
    ("v_pk_add_f16",     dst(PkF16), src(PkF16, PkF16), op(gfx9=0x0f)),
    ("v_pk_mul_f16",     dst(PkF16), src(PkF16, PkF16), op(gfx9=0x10)),
    ("v_pk_min_f16",     dst(PkF16), src(PkF16, PkF16), op(gfx9=0x11, gfx12=0x1b)), # called v_pk_min_num_f16 in GFX12
    ("v_pk_max_f16",     dst(PkF16), src(PkF16, PkF16), op(gfx9=0x12, gfx12=0x1c)), # called v_pk_min_num_f16 in GFX12
    ("v_pk_minimum_f16", dst(PkF16), src(PkF16, PkF16), op(gfx12=0x1d)),
    ("v_pk_maximum_f16", dst(PkF16), src(PkF16, PkF16), op(gfx12=0x1e)),
-   ("v_fma_mix_f32",    dst(F32), src(F32, F32, F32), op(gfx9=0x20)), # v_mad_mix_f32 in VEGA ISA, v_fma_mix_f32 in RDNA ISA
-   ("v_fma_mixlo_f16",  dst(F16), src(F32, F32, F32), op(gfx9=0x21)), # v_mad_mixlo_f16 in VEGA ISA, v_fma_mixlo_f16 in RDNA ISA
-   ("v_fma_mixhi_f16",  dst(F16), src(F32, F32, F32), op(gfx9=0x22)), # v_mad_mixhi_f16 in VEGA ISA, v_fma_mixhi_f16 in RDNA ISA
+   ("v_fma_mix_f32",    dst(F32), src(F32, F32, F32), op(gfx9=0x20), InstrClass.ValuFma),
+   ("v_fma_mixlo_f16",  dst(F16), src(F32, F32, F32), op(gfx9=0x21), InstrClass.ValuFma),
+   ("v_fma_mixhi_f16",  dst(F16), src(F32, F32, F32), op(gfx9=0x22), InstrClass.ValuFma),
    ("v_dot2_i32_i16",      dst(U32), src(PkU16, PkU16, U32), op(gfx9=0x26, gfx10=0x14, gfx11=-1)),
    ("v_dot2_u32_u16",      dst(U32), src(PkU16, PkU16, U32), op(gfx9=0x27, gfx10=0x15, gfx11=-1)),
    ("v_dot4_i32_iu8",      dst(U32), src(PkU16, PkU16, U32), op(gfx11=0x16)),
@@ -1275,8 +1275,8 @@ for (name, defs, ops, num) in VINTERP:
 # VOP3 instructions: 3 inputs, 1 output
 # VOP3b instructions: have a unique scalar output, e.g. VOP2 with vcc out
 VOP3 = {
-   ("v_mad_legacy_f32",        dst(F32), src(F32, F32, F32), op(0x140, gfx8=0x1c0, gfx10=0x140, gfx11=-1)), # GFX6-GFX10
-   ("v_mad_f32",               dst(F32), src(F32, F32, F32), op(0x141, gfx8=0x1c1, gfx10=0x141, gfx11=-1)),
+   ("v_mad_legacy_f32",        dst(F32), src(mods(F32), mods(F32), mods(F32)), op(0x140, gfx8=0x1c0, gfx10=0x140, gfx11=-1)), # GFX6-GFX10
+   ("v_mad_f32",               dst(F32), src(mods(F32), mods(F32), mods(F32)), op(0x141, gfx8=0x1c1, gfx10=0x141, gfx11=-1)),
    ("v_mad_i32_i24",           dst(U32), src(U32, U32, U32), op(0x142, gfx8=0x1c2, gfx10=0x142, gfx11=0x20a)),
    ("v_mad_u32_u24",           dst(U32), src(U32, U32, U32), op(0x143, gfx8=0x1c3, gfx10=0x143, gfx11=0x20b)),
    ("v_cubeid_f32",            dst(F32), src(F32, F32, F32), op(0x144, gfx8=0x1c4, gfx10=0x144, gfx11=0x20c)),
@@ -1286,11 +1286,11 @@ VOP3 = {
    ("v_bfe_u32",               dst(U32), src(U32, U32, U32), op(0x148, gfx8=0x1c8, gfx10=0x148, gfx11=0x210)),
    ("v_bfe_i32",               dst(U32), src(U32, U32, U32), op(0x149, gfx8=0x1c9, gfx10=0x149, gfx11=0x211)),
    ("v_bfi_b32",               dst(U32), src(U32, U32, U32), op(0x14a, gfx8=0x1ca, gfx10=0x14a, gfx11=0x212)),
-   ("v_fma_f32",               dst(F32), src(F32, F32, F32), op(0x14b, gfx8=0x1cb, gfx10=0x14b, gfx11=0x213), InstrClass.ValuFma),
-   ("v_fma_f64",               dst(F64), src(F64, F64, F64), op(0x14c, gfx8=0x1cc, gfx10=0x14c, gfx11=0x214), InstrClass.ValuDouble),
+   ("v_fma_f32",               dst(F32), src(mods(F32), mods(F32), mods(F32)), op(0x14b, gfx8=0x1cb, gfx10=0x14b, gfx11=0x213), InstrClass.ValuFma),
+   ("v_fma_f64",               dst(F64), src(mods(F64), mods(F64), mods(F64)), op(0x14c, gfx8=0x1cc, gfx10=0x14c, gfx11=0x214), InstrClass.ValuDouble),
    ("v_lerp_u8",               dst(U32), src(U32, U32, U32), op(0x14d, gfx8=0x1cd, gfx10=0x14d, gfx11=0x215)),
-   ("v_alignbit_b32",          dst(U32), src(U32, U32, U16), op(0x14e, gfx8=0x1ce, gfx10=0x14e, gfx11=0x216)),
-   ("v_alignbyte_b32",         dst(U32), src(U32, U32, U16), op(0x14f, gfx8=0x1cf, gfx10=0x14f, gfx11=0x217)),
+   ("v_alignbit_b32",          dst(U32), src(mods(U32), mods(U32), noMods(U16)), op(0x14e, gfx8=0x1ce, gfx10=0x14e, gfx11=0x216)),
+   ("v_alignbyte_b32",         dst(U32), src(mods(U32), mods(U32), noMods(U16)), op(0x14f, gfx8=0x1cf, gfx10=0x14f, gfx11=0x217)),
    ("v_mullit_f32",            dst(F32), src(F32, F32, F32), op(0x150, gfx8=-1, gfx10=0x150, gfx11=0x218)),
    ("v_min3_f32",              dst(F32), src(F32, F32, F32), op(0x151, gfx8=0x1d0, gfx10=0x151, gfx11=0x219, gfx12=0x229)), # called v_min3_num_f32 in GFX12
    ("v_min3_i32",              dst(U32), src(U32, U32, U32), op(0x152, gfx8=0x1d1, gfx10=0x152, gfx11=0x21a)),
@@ -1332,12 +1332,12 @@ VOP3 = {
    ("v_mqsad_u32_u8",          dst(U128), src(U64, U32, U128), op(gfx7=0x175, gfx8=0x1e7, gfx10=0x175, gfx11=0x23d), InstrClass.ValuQuarterRate32),
    ("v_mad_u64_u32",           dst(U64, VCC), src(U32, U32, U64), op(gfx7=0x176, gfx8=0x1e8, gfx10=0x176, gfx11=0x2fe), InstrClass.Valu64), # called v_mad_co_u64_u32 in GFX12
    ("v_mad_i64_i32",           dst(I64, VCC), src(U32, U32, I64), op(gfx7=0x177, gfx8=0x1e9, gfx10=0x177, gfx11=0x2ff), InstrClass.Valu64), # called v_mad_co_i64_i32 in GFX12
-   ("v_mad_legacy_f16",        dst(F16), src(F16, F16, F16), op(gfx8=0x1ea, gfx10=-1)),
-   ("v_mad_legacy_u16",        dst(U16), src(U16, U16, U16), op(gfx8=0x1eb, gfx10=-1)),
-   ("v_mad_legacy_i16",        dst(U16), src(U16, U16, U16), op(gfx8=0x1ec, gfx10=-1)),
+   ("v_mad_legacy_f16",        dst(F16), src(mods(F16), mods(F16), mods(F16)), op(gfx8=0x1ea, gfx10=-1)),
+   ("v_mad_legacy_u16",        dst(U16), src(mods(U16), mods(U16), mods(U16)), op(gfx8=0x1eb, gfx10=-1)),
+   ("v_mad_legacy_i16",        dst(U16), src(mods(U16), mods(U16), mods(U16)), op(gfx8=0x1ec, gfx10=-1)),
    ("v_perm_b32",              dst(U32), src(U32, U32, U32), op(gfx8=0x1ed, gfx10=0x344, gfx11=0x244)),
-   ("v_fma_legacy_f16",        dst(F16), src(F16, F16, F16), op(gfx8=0x1ee, gfx10=-1), InstrClass.ValuFma),
-   ("v_div_fixup_legacy_f16",  dst(F16), src(F16, F16, F16), op(gfx8=0x1ef, gfx10=-1)),
+   ("v_fma_legacy_f16",        dst(F16), src(mods(F16), mods(F16), mods(F16)), op(gfx8=0x1ee, gfx10=-1), InstrClass.ValuFma),
+   ("v_div_fixup_legacy_f16",  dst(F16), src(mods(F16), mods(F16), mods(F16)), op(gfx8=0x1ef, gfx10=-1)),
    ("v_cvt_pkaccum_u8_f32",    dst(U32), src(F32, U32, U32), op(0x12c, gfx8=0x1f0, gfx10=-1)),
    ("v_mad_u32_u16",           dst(U32), src(U16, U16, U32), op(gfx9=0x1f1, gfx10=0x373, gfx11=0x259)),
    ("v_mad_i32_i16",           dst(U32), src(U16, U16, U32), op(gfx9=0x1f2, gfx10=0x375, gfx11=0x25a)),
@@ -1365,7 +1365,7 @@ VOP3 = {
    ("v_interp_p1ll_f16",       dst(F32), src(F32, M0), op(gfx8=0x274, gfx10=0x342, gfx11=-1)),
    ("v_interp_p1lv_f16",       dst(F32), src(F32, M0, F16), op(gfx8=0x275, gfx10=0x343, gfx11=-1)),
    ("v_interp_p2_legacy_f16",  dst(F16), src(F32, M0, F32), op(gfx8=0x276, gfx10=-1)),
-   ("v_interp_p2_f16",         dst(F16), src(F32, M0, F32), op(gfx9=0x277, gfx10=0x35a, gfx11=-1)),
+   ("v_interp_p2_f16",         dst(F16), src(mods(F32), noMods(M0), mods(F32)), op(gfx9=0x277, gfx10=0x35a, gfx11=-1)),
    ("v_interp_p2_hi_f16",      dst(F16), src(F32, M0, F32), op(gfx9=0x277, gfx10=0x35a, gfx11=-1)),
    ("v_ldexp_f32",             dst(F32), src(F32, U32), op(0x12b, gfx8=0x288, gfx10=0x362, gfx11=0x31c)),
    ("v_readlane_b32_e64",      dst(U32), src(U32, U32), op(gfx8=0x289, gfx10=0x360)),



--- a/src/amd/compiler/aco_optimizer.cpp	2025-05-31 22:57:26.003334290 +0200
+++ b/src/amd/compiler/aco_optimizer.cpp	2025-06-01 00:30:01.222104109 +0200
@@ -43,46 +43,38 @@ struct mad_info {
 };
 
 enum Label {
-   label_constant_32bit = 1 << 1,
-   /* label_{abs,neg,mul,omod2,omod4,omod5,clamp} are used for both 16 and
-    * 32-bit operations but this shouldn't cause any issues because we don't
-    * look through any conversions */
-   label_abs = 1 << 2,
-   label_neg = 1 << 3,
-   label_temp = 1 << 5,
-   label_literal = 1 << 6,
-   label_mad = 1 << 7,
-   label_omod2 = 1 << 8,
-   label_omod4 = 1 << 9,
-   label_omod5 = 1 << 10,
-   label_clamp = 1 << 12,
-   label_b2f = 1 << 16,
-   /* This label means that it's either 0 or -1, and the ssa_info::temp is an s1 which is 0 or 1. */
-   label_uniform_bool = 1 << 21,
-   label_constant_64bit = 1 << 22,
-   /* This label is added to the first definition of s_not/s_or/s_xor/s_and when all operands are
-    * uniform_bool or uniform_bitwise. The first definition of ssa_info::instr would be 0 or -1 and
-    * the second is SCC.
-    */
-   label_uniform_bitwise = 1 << 23,
-   /* This label means that it's either 0 or 1 and ssa_info::temp is the inverse. */
-   label_scc_invert = 1 << 24,
-   label_scc_needed = 1 << 26,
-   label_b2i = 1 << 27,
-   label_fcanonicalize = 1 << 28,
-   label_constant_16bit = 1 << 29,
-   label_canonicalized = 1ull << 32, /* 1ull to prevent sign extension */
-   label_extract = 1ull << 33,
-   label_insert = 1ull << 34,
-   label_f2f16 = 1ull << 38,
+    label_constant_32bit = 1 << 1,
+    label_abs = 1 << 2,
+    label_neg = 1 << 3,
+    label_temp = 1 << 5,
+    label_literal = 1 << 6,
+    label_mad = 1 << 7,
+    label_omod2 = 1 << 8,
+    label_omod4 = 1 << 9,
+    label_omod5 = 1 << 10,
+    label_clamp = 1 << 12,
+    label_b2f = 1 << 16,
+    label_uniform_bool = 1 << 21,
+    label_constant_64bit = 1 << 22,
+    label_uniform_bitwise = 1 << 23,
+    label_scc_invert = 1 << 24,
+    label_scc_needed = 1 << 26,
+    label_b2i = 1 << 27,
+    label_fcanonicalize = 1 << 28,
+    label_constant_16bit = 1 << 29,
+    label_canonicalized = 1ull << 32,
+    label_extract = 1ull << 33,
+    label_insert = 1ull << 34,
+    label_precise = 1ull << 35,
+    label_f2f16 = 1ull << 38,
 };
 
 static constexpr uint64_t instr_mod_labels =
-   label_omod2 | label_omod4 | label_omod5 | label_clamp | label_insert | label_f2f16;
+label_omod2 | label_omod4 | label_omod5 | label_clamp | label_insert | label_f2f16;
 
 static constexpr uint64_t temp_labels = label_abs | label_neg | label_temp | label_b2f |
                                         label_uniform_bool | label_scc_invert | label_b2i |
-                                        label_fcanonicalize;
+                                        label_fcanonicalize | label_canonicalized | label_precise;
 static constexpr uint32_t val_labels =
    label_constant_32bit | label_constant_64bit | label_constant_16bit | label_literal | label_mad;
 
@@ -90,251 +82,174 @@ static_assert((instr_mod_labels & temp_l
 static_assert((instr_mod_labels & val_labels) == 0, "labels cannot intersect");
 static_assert((temp_labels & val_labels) == 0, "labels cannot intersect");
 
-struct ssa_info {
-   uint64_t label;
-   union {
-      uint32_t val;
-      Temp temp;
-      Instruction* mod_instr;
-   };
-   Instruction* parent_instr;
-
-   ssa_info() : label(0) {}
-
-   void add_label(Label new_label)
-   {
-      if (new_label & instr_mod_labels) {
-         label &= ~instr_mod_labels;
-         label &= ~(temp_labels | val_labels); /* instr, temp and val alias */
-      }
-
-      if (new_label & temp_labels) {
-         label &= ~temp_labels;
-         label &= ~(instr_mod_labels | val_labels); /* instr, temp and val alias */
-      }
-
-      uint32_t const_labels =
-         label_literal | label_constant_32bit | label_constant_64bit | label_constant_16bit;
-      if (new_label & const_labels) {
-         label &= ~val_labels | const_labels;
-         label &= ~(instr_mod_labels | temp_labels); /* instr, temp and val alias */
-      } else if (new_label & val_labels) {
-         label &= ~val_labels;
-         label &= ~(instr_mod_labels | temp_labels); /* instr, temp and val alias */
-      }
-
-      label |= new_label;
-   }
-
-   void set_constant(amd_gfx_level gfx_level, uint64_t constant)
-   {
-      Operand op16 = Operand::c16(constant);
-      Operand op32 = Operand::get_const(gfx_level, constant, 4);
-      add_label(label_literal);
-      val = constant;
-
-      /* check that no upper bits are lost in case of packed 16bit constants */
-      if (gfx_level >= GFX8 && !op16.isLiteral() &&
-          op16.constantValue16(true) == ((constant >> 16) & 0xffff))
-         add_label(label_constant_16bit);
-
-      if (!op32.isLiteral())
-         add_label(label_constant_32bit);
-
-      if (Operand::is_constant_representable(constant, 8))
-         add_label(label_constant_64bit);
-
-      if (label & label_constant_64bit) {
-         val = Operand::c64(constant).constantValue();
-         if (val != constant)
-            label &= ~(label_literal | label_constant_16bit | label_constant_32bit);
-      }
-   }
-
-   bool is_constant(unsigned bits)
-   {
-      switch (bits) {
-      case 8: return label & label_literal;
-      case 16: return label & label_constant_16bit;
-      case 32: return label & label_constant_32bit;
-      case 64: return label & label_constant_64bit;
-      }
-      return false;
-   }
-
-   bool is_literal(unsigned bits)
-   {
-      bool is_lit = label & label_literal;
-      switch (bits) {
-      case 8: return false;
-      case 16: return is_lit && ~(label & label_constant_16bit);
-      case 32: return is_lit && ~(label & label_constant_32bit);
-      case 64: return false;
-      }
-      return false;
-   }
-
-   bool is_constant_or_literal(unsigned bits)
-   {
-      if (bits == 64)
-         return label & label_constant_64bit;
-      else
-         return label & label_literal;
-   }
-
-   void set_abs(Temp abs_temp)
-   {
-      add_label(label_abs);
-      temp = abs_temp;
-   }
-
-   bool is_abs() { return label & label_abs; }
-
-   void set_neg(Temp neg_temp)
-   {
-      add_label(label_neg);
-      temp = neg_temp;
-   }
-
-   bool is_neg() { return label & label_neg; }
-
-   void set_neg_abs(Temp neg_abs_temp)
-   {
-      add_label((Label)((uint32_t)label_abs | (uint32_t)label_neg));
-      temp = neg_abs_temp;
-   }
-
-   void set_temp(Temp tmp)
-   {
-      add_label(label_temp);
-      temp = tmp;
-   }
-
-   bool is_temp() { return label & label_temp; }
-
-   void set_mad(uint32_t mad_info_idx)
-   {
-      add_label(label_mad);
-      val = mad_info_idx;
-   }
-
-   bool is_mad() { return label & label_mad; }
-
-   void set_omod2(Instruction* mul)
-   {
-      if (label & temp_labels)
-         return;
-      add_label(label_omod2);
-      mod_instr = mul;
-   }
-
-   bool is_omod2() { return label & label_omod2; }
-
-   void set_omod4(Instruction* mul)
-   {
-      if (label & temp_labels)
-         return;
-      add_label(label_omod4);
-      mod_instr = mul;
-   }
-
-   bool is_omod4() { return label & label_omod4; }
-
-   void set_omod5(Instruction* mul)
-   {
-      if (label & temp_labels)
-         return;
-      add_label(label_omod5);
-      mod_instr = mul;
-   }
-
-   bool is_omod5() { return label & label_omod5; }
-
-   void set_clamp(Instruction* med3)
-   {
-      if (label & temp_labels)
-         return;
-      add_label(label_clamp);
-      mod_instr = med3;
-   }
-
-   bool is_clamp() { return label & label_clamp; }
-
-   void set_f2f16(Instruction* conv)
-   {
-      if (label & temp_labels)
-         return;
-      add_label(label_f2f16);
-      mod_instr = conv;
-   }
-
-   bool is_f2f16() { return label & label_f2f16; }
-
-   void set_b2f(Temp b2f_val)
-   {
-      add_label(label_b2f);
-      temp = b2f_val;
-   }
-
-   bool is_b2f() { return label & label_b2f; }
-
-   void set_uniform_bitwise() { add_label(label_uniform_bitwise); }
-
-   bool is_uniform_bitwise() { return label & label_uniform_bitwise; }
-
-   void set_scc_needed() { add_label(label_scc_needed); }
-
-   bool is_scc_needed() { return label & label_scc_needed; }
-
-   void set_scc_invert(Temp scc_inv)
-   {
-      add_label(label_scc_invert);
-      temp = scc_inv;
-   }
-
-   bool is_scc_invert() { return label & label_scc_invert; }
-
-   void set_uniform_bool(Temp uniform_bool)
-   {
-      add_label(label_uniform_bool);
-      temp = uniform_bool;
-   }
-
-   bool is_uniform_bool() { return label & label_uniform_bool; }
-
-   void set_b2i(Temp b2i_val)
-   {
-      add_label(label_b2i);
-      temp = b2i_val;
-   }
+struct ssa_info
+{
+    uint64_t label = 0;
 
-   bool is_b2i() { return label & label_b2i; }
+    union {
+        uint32_t     val;
+        Temp         temp;
+        Instruction* mod_instr;
+    };
 
-   void set_fcanonicalize(Temp tmp)
-   {
-      add_label(label_fcanonicalize);
-      temp = tmp;
-   }
+    Instruction* parent_instr = nullptr;
 
-   bool is_fcanonicalize() { return label & label_fcanonicalize; }
+    constexpr ssa_info() noexcept : label(0), val(0), parent_instr(nullptr) {}
+    ~ssa_info() = default;
 
-   void set_canonicalized() { add_label(label_canonicalized); }
+    void add_label(Label new_label)
+    {
+        if (!new_label)
+            return;
 
-   bool is_canonicalized() { return label & label_canonicalized; }
+        constexpr uint64_t const_labels =
+        label_literal | label_constant_32bit |
+        label_constant_64bit | label_constant_16bit;
+
+        const bool is_instr = new_label & instr_mod_labels;
+        const bool is_temp  = new_label & temp_labels;
+        const bool is_const = new_label & const_labels;
+        const bool is_val   = (!is_const) && (new_label & val_labels);
+
+        if (is_instr)
+            label &= ~(instr_mod_labels | temp_labels | val_labels);
+        else if (is_temp)
+            label &= ~(temp_labels | instr_mod_labels | val_labels);
+        else if (is_const || is_val)
+            label &= ~(val_labels | instr_mod_labels | temp_labels);
+
+        label |= new_label;
+    }
+
+    /* ------------------------------------------------------------------ */
+    /* 3.  Constant/Literal helpers                                        */
+    /* ------------------------------------------------------------------ */
+    void set_constant(amd_gfx_level gfx_level, uint64_t constant)
+    {
+        Operand op16 = Operand::c16(constant);
+        Operand op32 = Operand::get_const(gfx_level, constant, 4);
+
+        add_label(label_literal);
+        val = constant;
+
+        /* packed-16 inline    (both 16-bit halves equal & representable) */
+        if (gfx_level >= GFX8 && !op16.isLiteral() &&
+            op16.constantValue16(true) == ((constant >> 16) & 0xffff))
+            add_label(label_constant_16bit);
+
+        if (!op32.isLiteral())
+            add_label(label_constant_32bit);
+
+        if (Operand::is_constant_representable(constant, 8))
+            add_label(label_constant_64bit);
+
+        /* Prefer 64-bit inline const if available                         */
+        if (label & label_constant_64bit) {
+            val = Operand::c64(constant).constantValue();
+            if (val != constant)            /* information loss  strip      */
+                label &= ~(label_literal | label_constant_16bit |
+                label_constant_32bit);
+        }
+    }
+
+    bool is_constant(unsigned bits) const
+    {
+        switch (bits) {
+            case  8: return label & label_literal;
+            case 16: return label & label_constant_16bit;
+            case 32: return label & label_constant_32bit;
+            case 64: return label & label_constant_64bit;
+            default: return false;
+        }
+    }
+
+    bool is_literal(unsigned bits) const
+    {
+        const bool lit = label & label_literal;
+        switch (bits) {
+            case  8: return false; /* 8-bit always inline, never literal      */
+            case 16: return lit && !(label & label_constant_16bit);
+            case 32: return lit && !(label & label_constant_32bit);
+            case 64: return lit && !(label & label_constant_64bit);
+            default: return false;
+        }
+    }
+
+    bool is_constant_or_literal(unsigned bits) const
+    {
+        return is_constant(bits) || is_literal(bits);
+    }
+
+    /* ------------------------------------------------------------------ */
+    /* 4.  Fast setters / testers (public API remains unchanged)           */
+    /* ------------------------------------------------------------------ */
+    /* ---- abs / neg --------------------------------------------------- */
+    void set_abs(Temp t)              { add_label(label_abs); temp = t; }
+    bool is_abs()               const { return label & label_abs; }
+
+    void set_neg(Temp t)              { add_label(label_neg); temp = t; }
+    bool is_neg()               const { return label & label_neg; }
+
+    void set_neg_abs(Temp t)
+    {
+        add_label(static_cast<Label>(label_abs | label_neg));
+        temp = t;
+    }
+
+    /* ---- plain temp -------------------------------------------------- */
+    void set_temp(Temp t)             { add_label(label_temp); temp = t; }
+    bool is_temp()              const { return label & label_temp; }
+
+    /* ---- MAD marker -------------------------------------------------- */
+    void set_mad(uint32_t idx)        { add_label(label_mad);  val = idx; }
+    bool is_mad()               const { return label & label_mad; }
+
+    /* ---- omod / clamp / f2f16 / insert ------------------------------- */
+    void set_omod2 (Instruction* m)   { add_label(label_omod2);  mod_instr = m; }
+    bool is_omod2()             const { return label & label_omod2; }
+
+    void set_omod4 (Instruction* m)   { add_label(label_omod4);  mod_instr = m; }
+    bool is_omod4()             const { return label & label_omod4; }
+
+    void set_omod5 (Instruction* m)   { add_label(label_omod5);  mod_instr = m; }
+    bool is_omod5()             const { return label & label_omod5; }
+
+    void set_clamp (Instruction* m)   { add_label(label_clamp);  mod_instr = m; }
+    bool is_clamp()             const { return label & label_clamp; }
+
+    void set_f2f16(Instruction* m)    { add_label(label_f2f16); mod_instr = m; }
+    bool is_f2f16()            const { return label & label_f2f16; }
+
+    void set_insert(Instruction* m)   { add_label(label_insert); mod_instr = m; }
+    bool is_insert()            const { return label & label_insert; }
+
+    /* ---- misc helpers ------------------------------------------------ */
+    void set_b2f(Temp t)              { add_label(label_b2f);   temp = t; }
+    bool is_b2f()               const { return label & label_b2f; }
+
+    void set_uniform_bitwise()        { add_label(label_uniform_bitwise); }
+    bool is_uniform_bitwise()   const { return label & label_uniform_bitwise; }
+
+    void set_scc_needed()             { add_label(label_scc_needed); }
+    bool is_scc_needed()        const { return label & label_scc_needed; }
+
+    void set_scc_invert(Temp t)       { add_label(label_scc_invert); temp = t; }
+    bool is_scc_invert()        const { return label & label_scc_invert; }
+
+    void set_uniform_bool(Temp t)     { add_label(label_uniform_bool); temp = t; }
+    bool is_uniform_bool()      const { return label & label_uniform_bool; }
+
+    void set_b2i(Temp t)              { add_label(label_b2i);  temp = t; }
+    bool is_b2i()               const { return label & label_b2i; }
 
-   void set_extract() { add_label(label_extract); }
+    void set_fcanonicalize(Temp t)    { add_label(label_fcanonicalize); temp = t; }
+    bool is_fcanonicalize()     const { return label & label_fcanonicalize; }
 
-   bool is_extract() { return label & label_extract; }
+    void set_canonicalized()          { add_label(label_canonicalized); }
+    bool is_canonicalized()     const { return label & label_canonicalized; }
 
-   void set_insert(Instruction* insert)
-   {
-      if (label & temp_labels)
-         return;
-      add_label(label_insert);
-      mod_instr = insert;
-   }
-
-   bool is_insert() { return label & label_insert; }
+    void set_extract()                { add_label(label_extract); }
+    bool is_extract()           const { return label & label_extract; }
 };
 
 struct opt_ctx {
@@ -473,11 +388,7 @@ can_apply_sgprs(opt_ctx& ctx, aco_ptr<In
           instr->opcode != aco_opcode::v_wmma_f16_16x16x16_f16 &&
           instr->opcode != aco_opcode::v_wmma_bf16_16x16x16_bf16 &&
           instr->opcode != aco_opcode::v_wmma_i32_16x16x16_iu8 &&
-          instr->opcode != aco_opcode::v_wmma_i32_16x16x16_iu4 &&
-          instr->opcode != aco_opcode::v_wmma_f32_16x16x16_fp8_fp8 &&
-          instr->opcode != aco_opcode::v_wmma_f32_16x16x16_fp8_bf8 &&
-          instr->opcode != aco_opcode::v_wmma_f32_16x16x16_bf8_fp8 &&
-          instr->opcode != aco_opcode::v_wmma_f32_16x16x16_bf8_bf8;
+          instr->opcode != aco_opcode::v_wmma_i32_16x16x16_iu4;
 }
 
 /* only covers special cases */
@@ -539,10 +450,6 @@ alu_can_accept_constant(const aco_ptr<In
    case aco_opcode::v_dot2_bf16_bf16: /* TODO */
    case aco_opcode::v_wmma_f32_16x16x16_f16:
    case aco_opcode::v_wmma_f32_16x16x16_bf16:
-   case aco_opcode::v_wmma_f32_16x16x16_fp8_fp8:
-   case aco_opcode::v_wmma_f32_16x16x16_fp8_bf8:
-   case aco_opcode::v_wmma_f32_16x16x16_bf8_fp8:
-   case aco_opcode::v_wmma_f32_16x16x16_bf8_bf8:
    case aco_opcode::v_wmma_f16_16x16x16_f16:
    case aco_opcode::v_wmma_bf16_16x16x16_bf16:
    case aco_opcode::v_wmma_i32_16x16x16_iu8:
@@ -781,6 +688,48 @@ get_constant_op(opt_ctx& ctx, ssa_info i
    return Operand::get_const(ctx.program->gfx_level, info.val, bits / 8u);
 }
 
+template <typename GfxEnum>
+static inline bool can_use_inline_constant(GfxEnum /*gfx_level*/,
+                                           uint32_t imm)
+{
+    /* Small signed/unsigned integers (064, -16-1) -------------------- */
+    if (imm <= 64u || imm >= 0xfffffff0u)
+        return true;
+
+    /* Canonical IEEE-754 fp constants  perfect-hash on the top nibble -- */
+    constexpr uint8_t canonical_fp_lut[16] = {
+        /* 0x0  0xF  */
+        0,0,0,0,  /* 0x0-0x3 */
+        0,0,0,0,  /* 0x4-0x7 */
+        1,1,      /* 0x8 =  1.0f  / 0x9 = -1.0f   */
+        1,1,      /* 0xA =  2.0f  / 0xB = -2.0f   */
+        0,0,0,0   /* 0xC-0xF (the 4.0/-4.0 cases sit at 0x40/0xC0) */
+    };
+
+    const uint8_t high_nib = static_cast<uint8_t>(imm >> 24);
+    if ((high_nib == 0x3F || high_nib == 0xBF) && canonical_fp_lut[(imm>>20)&0xF])
+        return true;                           /* 0.5 / 1   */
+        if ((high_nib == 0x40 || high_nib == 0xC0) &&
+            ((imm & 0x00ffffffu) == 0))           /* 2 / 4     */
+        return true;
+
+    return false;
+}
+
+static inline bool
+is_literal_valid_for_vop3p_vega(aco_opcode op, const Operand& lit) noexcept
+{
+   if (op == aco_opcode::v_pk_mad_i16)
+      return true;
+
+   if (op == aco_opcode::v_pk_fma_f16 || op == aco_opcode::v_pk_mad_u16) {
+      return lit.isConstant() &&
+             can_use_inline_constant(GFX9, lit.constantValue());
+   }
+
+   return true;
+}
+
 void
 propagate_constants_vop3p(opt_ctx& ctx, aco_ptr<Instruction>& instr, ssa_info& info, unsigned i)
 {
@@ -879,6 +828,14 @@ propagate_constants_vop3p(opt_ctx& ctx,
 
    vop3p->opsel_lo[i] = opsel_lo;
    vop3p->opsel_hi[i] = opsel_hi;
+
+   // GFX9-specific: Fold literals by sign-extending or shifting if possible
+   if (ctx.program->gfx_level == GFX9 && instr->operands[i].isLiteral() && is_literal_valid_for_vop3p_vega(instr->opcode, instr->operands[i])) {
+      uint32_t val = instr->operands[i].constantValue();
+      if (Operand::is_constant_representable(val, bits) || (bits == 32 && Operand::is_constant_representable(static_cast<uint64_t>(val) << 16, 4))) {
+         instr->operands[i] = Operand::get_const(GFX9, val, bits / 8u); // Fold to inline
+      }
+   }
 }
 
 bool
@@ -1180,8 +1137,8 @@ can_eliminate_and_exec(opt_ctx& ctx, Tem
 
    if (allow_cselect && instr->pass_flags == pass_flags &&
        (instr->opcode == aco_opcode::s_cselect_b32 || instr->opcode == aco_opcode::s_cselect_b64)) {
-      return (instr->operands[0].constantEquals(0) && instr->operands[1].constantEquals(-1)) ||
-             (instr->operands[1].constantEquals(0) && instr->operands[0].constantEquals(-1));
+      return (instr->operands[0].constantEquals(0) && instr->operands[1].constantEquals((unsigned)-1)) ||
+             (instr->operands[1].constantEquals(0) && instr->operands[0].constantEquals((unsigned)-1));
    }
 
    if (instr->operands.size() != 2 || instr->pass_flags != pass_flags)
@@ -1530,10 +1487,7 @@ label_instruction(opt_ctx& ctx, aco_ptr<
          if (has_usable_ds_offset && i == 0 &&
              parse_base_offset(ctx, instr.get(), i, &base, &offset, false) &&
              base.regClass() == instr->operands[i].regClass() &&
-             instr->opcode != aco_opcode::ds_swizzle_b32 &&
-             instr->opcode != aco_opcode::ds_bvh_stack_push4_pop1_rtn_b32 &&
-             instr->opcode != aco_opcode::ds_bvh_stack_push8_pop1_rtn_b32 &&
-             instr->opcode != aco_opcode::ds_bvh_stack_push8_pop2_rtn_b64) {
+             instr->opcode != aco_opcode::ds_swizzle_b32) {
             if (instr->opcode == aco_opcode::ds_write2_b32 ||
                 instr->opcode == aco_opcode::ds_read2_b32 ||
                 instr->opcode == aco_opcode::ds_write2_b64 ||
@@ -1624,56 +1578,37 @@ label_instruction(opt_ctx& ctx, aco_ptr<
          }
       }
 
-      offset = 0;
-      for (unsigned i = 0; i < ops.size(); i++) {
-         if (ops[i].isTemp()) {
-            if (ctx.info[ops[i].tempId()].is_temp() &&
-                ops[i].regClass() == ctx.info[ops[i].tempId()].temp.regClass()) {
-               ops[i].setTemp(ctx.info[ops[i].tempId()].temp);
-            }
-
-            /* If this and the following operands make up all definitions of a `p_split_vector`,
-             * replace them with the operand of the `p_split_vector` instruction.
-             */
-            Instruction* parent = ctx.info[ops[i].tempId()].parent_instr;
-            if (parent->opcode == aco_opcode::p_split_vector &&
-                (offset % 4 == 0 || parent->operands[0].bytes() < 4) &&
-                parent->definitions.size() <= ops.size() - i) {
-               copy_prop = true;
-               for (unsigned j = 0; copy_prop && j < parent->definitions.size(); j++) {
-                  copy_prop &= ops[i + j].isTemp() &&
-                               ops[i + j].getTemp() == parent->definitions[j].getTemp();
-               }
-
-               if (copy_prop) {
-                  ops.erase(ops.begin() + i + 1, ops.begin() + i + parent->definitions.size());
-                  ops[i] = parent->operands[0];
-               }
-            }
-         }
-
-         offset += ops[i].bytes();
-      }
-
       /* combine expanded operands to new vector */
-      if (ops.size() <= instr->operands.size()) {
-         while (instr->operands.size() > ops.size())
-            instr->operands.pop_back();
-
-         if (ops.size() == 1) {
-            instr->opcode = aco_opcode::p_parallelcopy;
-            if (ops[0].isTemp())
-               ctx.info[instr->definitions[0].tempId()].set_temp(ops[0].getTemp());
-         }
-      } else {
+      if (ops.size() != instr->operands.size()) {
+         assert(ops.size() > instr->operands.size());
          Definition def = instr->definitions[0];
          instr.reset(
             create_instruction(aco_opcode::p_create_vector, Format::PSEUDO, ops.size(), 1));
+         for (unsigned i = 0; i < ops.size(); i++) {
+            if (ops[i].isTemp() && ctx.info[ops[i].tempId()].is_temp() &&
+                ops[i].regClass() == ctx.info[ops[i].tempId()].temp.regClass())
+               ops[i].setTemp(ctx.info[ops[i].tempId()].temp);
+            instr->operands[i] = ops[i];
+         }
          instr->definitions[0] = def;
+      } else {
+         for (unsigned i = 0; i < ops.size(); i++) {
+            assert(instr->operands[i] == ops[i]);
+         }
       }
 
-      for (unsigned i = 0; i < ops.size(); i++)
-         instr->operands[i] = ops[i];
+      if (instr->operands.size() == 2 && instr->operands[1].isTemp()) {
+         /* check if this is created from split_vector */
+         ssa_info& info = ctx.info[instr->operands[1].tempId()];
+         if (info.parent_instr->opcode == aco_opcode::p_split_vector) {
+            Instruction* split = info.parent_instr;
+            if (instr->operands[0].isTemp() &&
+                instr->operands[0].getTemp() == split->definitions[0].getTemp() &&
+                instr->operands[1].getTemp() == split->definitions[1].getTemp() &&
+                instr->definitions[0].regClass() == split->operands[0].regClass())
+               ctx.info[instr->definitions[0].tempId()].set_temp(split->operands[0].getTemp());
+         }
+      }
       break;
    }
    case aco_opcode::p_split_vector: {
@@ -1916,12 +1851,16 @@ label_instruction(opt_ctx& ctx, aco_ptr<
              * uniform bool into divergent */
             ctx.info[instr->definitions[1].tempId()].set_temp(
                ctx.info[instr->operands[0].tempId()].temp);
+            ctx.info[instr->definitions[0].tempId()].set_uniform_bool(
+               ctx.info[instr->operands[0].tempId()].temp);
             break;
          } else if (ctx.info[instr->operands[0].tempId()].is_uniform_bitwise()) {
             /* Try to get rid of the superfluous s_and_b64, since the uniform bitwise instruction
              * already produces the same SCC */
             ctx.info[instr->definitions[1].tempId()].set_temp(
                ctx.info[instr->operands[0].tempId()].parent_instr->definitions[1].getTemp());
+            ctx.info[instr->definitions[0].tempId()].set_uniform_bool(
+               ctx.info[instr->operands[0].tempId()].parent_instr->definitions[1].getTemp());
             break;
          } else if ((ctx.program->stage.num_sw_stages() > 1 ||
                      ctx.program->stage.hw == AC_HW_NEXT_GEN_GEOMETRY_SHADER) &&
@@ -1951,7 +1890,8 @@ label_instruction(opt_ctx& ctx, aco_ptr<
       if (instr->operands[0].constantEquals((unsigned)-1) && instr->operands[1].constantEquals(0)) {
          /* Found a cselect that operates on a uniform bool that comes from eg. s_cmp */
          ctx.info[instr->definitions[0].tempId()].set_uniform_bool(instr->operands[2].getTemp());
-      } else if (instr->operands[2].isTemp() && ctx.info[instr->operands[2].tempId()].is_scc_invert()) {
+      }
+      if (instr->operands[2].isTemp() && ctx.info[instr->operands[2].tempId()].is_scc_invert()) {
          /* Flip the operands to get rid of the scc_invert instruction */
          std::swap(instr->operands[0], instr->operands[1]);
          instr->operands[2].setTemp(ctx.info[instr->operands[2].tempId()].temp);
@@ -1999,6 +1939,25 @@ label_instruction(opt_ctx& ctx, aco_ptr<
       ctx.info[def.tempId()].parent_instr = instr.get();
 }
 
+/* PhysReg helpers (they are enumlike in ACO) */
+#ifndef EXEC_LO
+#define EXEC_LO PhysReg{exec}
+#define EXEC_HI PhysReg{exec+1}
+#endif
+
+static inline bool is_exec_lo(const Operand& op)
+{
+    return op.isFixed() && op.physReg() == EXEC_LO;
+}
+static inline bool is_exec_hi(const Operand& op)
+{
+    return op.isFixed() && op.physReg() == EXEC_HI;
+}
+static inline bool is_vcc(const Operand& op)
+{
+    return op.isFixed() && (op.physReg() == vcc || op.physReg() == vcc_hi);
+}
+
 unsigned
 original_temp_id(opt_ctx& ctx, Temp tmp)
 {
@@ -2224,6 +2183,359 @@ combine_three_valu_op(opt_ctx& ctx, aco_
    return false;
 }
 
+static bool
+combine_alignbit_like(opt_ctx&              ctx,
+                      aco_ptr<Instruction>& or_instr,
+                      aco_opcode            target,
+                      unsigned              granularity)
+{
+    if (or_instr->opcode != aco_opcode::v_or_b32 ||
+        or_instr->operands.size() != 2 ||
+        granularity == 0 ||
+        ctx.program->gfx_level < GFX9)
+        return false;
+
+    /* ---------- recognise   v_lshrrev / v_lshlrev  -------------------- */
+    auto match_shift = [&](Operand op, unsigned& amount, Operand& src,
+                           bool& is_shr) -> bool {
+                               if (!op.isTemp())
+                                   return false;
+                               Instruction* sh = ctx.info[op.tempId()].parent_instr;
+                               if (!sh || ctx.uses[op.tempId()] != 1)
+                                   return false;
+
+                               if (sh->opcode == aco_opcode::v_lshrrev_b32)
+                                   is_shr = true;
+                               else if (sh->opcode == aco_opcode::v_lshlrev_b32)
+                                   is_shr = false;
+                               else
+                                   return false;
+
+                               if (!sh->operands[0].isLiteral() ||
+                                   sh->operands[0].constantValue() >= 32)
+                                   return false;
+
+                               amount = sh->operands[0].constantValue();
+                               if (amount == 0 || amount >= 32 || amount % granularity)
+                                   return false;
+
+                               src = sh->operands[1];
+                               return true;
+                           };
+
+                           unsigned a_amt = 0, b_amt = 0;
+                           Operand  a_src,  b_src;
+                           bool     a_shr = false, b_shr = false;
+
+                           if (!match_shift(or_instr->operands[0], a_amt, a_src, a_shr) ||
+                               !match_shift(or_instr->operands[1], b_amt, b_src, b_shr))
+                               return false;
+
+                           if (a_shr == b_shr)                     /* need one left, one right */
+                               return false;
+    if (a_amt + b_amt != 32)                /* must be complementary    */
+        return false;
+
+    /* choose mapping that fits ISA:  dst = (src0 >> imm) | (src1 << (32-imm)) */
+    Operand src0 = a_shr ? a_src : b_src;   /* right-shift origin */
+    Operand src1 = a_shr ? b_src : a_src;   /* left-shift origin  */
+    unsigned imm = a_shr ? a_amt : b_amt;   /* right-shift amount */
+
+    aco_ptr<Instruction> ali{
+        create_instruction(target, Format::VOP3, 3, 1)};
+
+        ali->operands[0] = src0;
+        ali->operands[1] = src1;
+        ali->operands[2] = Operand::c32(imm / granularity);
+        ali->definitions[0] = or_instr->definitions[0];
+        ali->pass_flags     = or_instr->pass_flags;
+
+        ctx.uses[or_instr->operands[0].tempId()]--;
+        ctx.uses[or_instr->operands[1].tempId()]--;
+
+        if (src0.isTemp()) ctx.uses[src0.tempId()]++;
+            if (src1.isTemp()) ctx.uses[src1.tempId()]++;
+
+            or_instr = std::move(ali);
+    ctx.info[or_instr->definitions[0].tempId()].parent_instr = or_instr.get();
+    return true;
+}
+
+static inline bool
+combine_alignbit_b32(opt_ctx& ctx, aco_ptr<Instruction>& instr)
+{
+    return combine_alignbit_like(ctx, instr, aco_opcode::v_alignbit_b32, 1);
+}
+
+static inline bool
+combine_alignbyte_b32(opt_ctx& ctx, aco_ptr<Instruction>& instr)
+{
+    return combine_alignbit_like(ctx, instr, aco_opcode::v_alignbyte_b32, 8);
+}
+
+static bool
+combine_bfi_b32(opt_ctx& ctx, aco_ptr<Instruction>& or_instr)
+{
+    if (or_instr->opcode != aco_opcode::v_or_b32 ||
+        ctx.program->gfx_level < GFX9)
+        return false;
+
+    /* ---- helper to match src & literal ----------------------------- */
+    auto match_and_side = [&](Operand in, Operand& src, uint32_t& lit) -> bool
+    {
+        if (!in.isTemp())
+            return false;
+
+        Instruction* and_i = ctx.info[in.tempId()].parent_instr;
+        if (!and_i || and_i->opcode != aco_opcode::v_and_b32 ||
+            ctx.uses[in.tempId()] != 1)
+            return false;
+
+        for (unsigned op = 0; op < 2; ++op) {
+            if (and_i->operands[op].isLiteral()) {
+                uint32_t imm = and_i->operands[op].constantValue();
+
+                /* must be VERD/inline constant so instruction can dual issue */
+                if (!Operand::is_constant_representable(imm, 4))
+                    return false;
+
+                lit = imm;
+                src = and_i->operands[1 ^ op];
+                return true;
+            }
+        }
+        return false;
+    };
+
+    /* ---- analyse both OR operands ------------------------------------ */
+    Operand   val0, val1;
+    uint32_t  mask0 = 0, mask1 = 0;
+
+    if (!match_and_side(or_instr->operands[0], val0, mask0) ||
+        !match_and_side(or_instr->operands[1], val1, mask1))
+        return false;
+
+    /* masks must form a perfect complement */
+    if ((mask0 ^ mask1) != 0xffffffffu || mask0 == 0 || mask0 == 0xffffffffu)
+        return false;
+
+    /* decide which side is mask / ~mask */
+    Operand  base, ins;
+    uint32_t mask;
+    if (mask1 == (~mask0)) {
+        ins  = val0;   base = val1;   mask = mask0;
+    } else { /* mask0 == ~mask1 */
+        ins  = val1;   base = val0;   mask = mask1;
+    }
+
+    /* ---- build v_bfi_b32 --------------------------------------------- */
+    aco_ptr<Instruction> bfi{
+        create_instruction(aco_opcode::v_bfi_b32, Format::VOP3, 3, 1)};
+
+        bfi->operands[0]    = base;
+        bfi->operands[1]    = ins;
+        bfi->operands[2]    = Operand::c32(mask);    /* inline const */
+        bfi->definitions[0] = or_instr->definitions[0];
+        bfi->pass_flags     = or_instr->pass_flags;
+
+        /* ---- SSA bookkeeping --------------------------------------------- */
+        ctx.uses[or_instr->operands[0].tempId()]--;   /* kill AND temps */
+        ctx.uses[or_instr->operands[1].tempId()]--;
+
+        if (base.isTemp()) ctx.uses[base.tempId()]++; /* new direct uses */
+            if (ins .isTemp()) ctx.uses[ins .tempId()]++;
+
+            or_instr = std::move(bfi);
+    ctx.info[or_instr->definitions[0].tempId()].parent_instr = or_instr.get();
+    return true;
+}
+
+static bool
+combine_bfe_b32(opt_ctx& ctx, aco_ptr<Instruction>& instr)
+{
+    if (ctx.program->gfx_level < GFX8)
+        return false;
+
+    Operand src_val;
+    uint32_t offset = 0, width = 0;
+    bool is_signed = false;
+
+    if (instr->opcode == aco_opcode::v_and_b32 && instr->operands[1].isLiteral()) {
+        uint32_t mask = instr->operands[1].constantValue();
+        /* Check if mask is contiguous low bits (e.g., 0x0000FFFF); branchless via popcount */
+        width = util_bitcount(mask);
+        if (!mask || (mask & (mask + 1u)) != 0u || width > 31) /* Vega max width 31 (enc 0-30) */
+            return false;
+
+        if (!instr->operands[0].isTemp() || ctx.uses[instr->operands[0].tempId()] != 1)
+            return false;
+
+        Instruction* sh = ctx.info[instr->operands[0].tempId()].parent_instr;
+        if (!sh || !(sh->opcode == aco_opcode::v_lshrrev_b32 || sh->opcode == aco_opcode::v_ashrrev_i32))
+            return false;
+        if (!sh->operands[0].isLiteral() || sh->operands[0].constantValue() >= 32)
+            return false;
+
+        offset = sh->operands[0].constantValue();
+        if (offset + width > 32)
+            return false;
+
+        is_signed = sh->opcode == aco_opcode::v_ashrrev_i32;
+        src_val = sh->operands[1];
+    }
+
+    else if (instr->opcode == aco_opcode::v_lshrrev_b32 && instr->operands[0].isLiteral()) {
+        uint32_t c2 = instr->operands[0].constantValue();
+        if (!instr->operands[1].isTemp() || ctx.uses[instr->operands[1].tempId()] != 1)
+            return false;
+
+        Instruction* lshl = ctx.info[instr->operands[1].tempId()].parent_instr;
+        if (!lshl || lshl->opcode != aco_opcode::v_lshlrev_b32 || !lshl->operands[0].isLiteral())
+            return false;
+
+        uint32_t c1 = lshl->operands[0].constantValue();
+        if (c1 >= 32 || c2 >= 32 || c2 < c1)
+            return false;
+
+        offset = c1;
+        width = 32 - c2;
+        if (width == 0)
+            return false;
+        is_signed = false;
+        src_val = lshl->operands[1];
+    } else {
+        return false;
+    }
+
+    aco_opcode bfe_op = is_signed ? aco_opcode::v_bfe_i32 : aco_opcode::v_bfe_u32;
+
+    /* Per Vega ISA, width encoded as (width - 1); 0 means width=1, 31 means 32 */
+    unsigned enc_width = width - 1u;
+
+    aco_ptr<Instruction> bfe{create_instruction(bfe_op, Format::VOP3, 3, 1)};
+
+    bfe->operands[0] = src_val;
+    bfe->operands[1] = Operand::c32(offset);
+    bfe->operands[2] = Operand::c32(enc_width);
+    bfe->definitions[0] = instr->definitions[0];
+    bfe->pass_flags = instr->pass_flags;
+
+    ctx.uses[instr->operands[0].tempId()]--;
+    if (src_val.isTemp())
+        ctx.uses[src_val.tempId()]++;
+
+    instr = std::move(bfe);
+    ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
+    return true;
+}
+
+static bool
+combine_bcnt_mbcnt(opt_ctx& ctx, aco_ptr<Instruction>& add_instr)
+{
+    if (add_instr->opcode != aco_opcode::v_add_u32 || add_instr->usesModifiers() ||
+        ctx.program->gfx_level < GFX9)
+        return false;
+
+    int bcnt_idx = -1;
+    Instruction* bcnt = nullptr;
+
+    for (unsigned i = 0; i < 2; ++i) {
+        if (!add_instr->operands[i].isTemp())
+            continue;
+        unsigned tmp_id = add_instr->operands[i].tempId();
+        if (ctx.uses[tmp_id] != 1)
+            continue;
+
+        Instruction* cand = ctx.info[tmp_id].parent_instr;
+        if (!cand || cand->opcode != aco_opcode::v_bcnt_u32_b32)
+            continue;
+
+        if (!cand->operands[1].isLiteral() || !cand->operands[1].constantEquals(0))
+            continue;
+
+        if (is_exec_lo(cand->operands[0]) || is_exec_hi(cand->operands[0])) {
+            bcnt_idx = i;
+            bcnt = cand;
+            break;
+        }
+    }
+
+    if (!bcnt)
+        return false;
+
+    bool lo_segment = is_exec_lo(bcnt->operands[0]);
+    aco_opcode mbcnt_op = lo_segment ? aco_opcode::v_mbcnt_lo_u32_b32 : aco_opcode::v_mbcnt_hi_u32_b32;
+    Operand carry_in = add_instr->operands[1u ^ bcnt_idx];
+
+    aco_ptr<Instruction> mbcnt{create_instruction(mbcnt_op, Format::VOP3, 2, 1)};
+    mbcnt->operands[0] = bcnt->operands[0];
+    mbcnt->operands[1] = carry_in;
+    mbcnt->definitions[0] = add_instr->definitions[0];
+    mbcnt->pass_flags = add_instr->pass_flags;
+
+    /* Update SSA use-counts */
+    ctx.uses[add_instr->operands[bcnt_idx].tempId()]--;
+    if (carry_in.isTemp())
+        ctx.uses[carry_in.tempId()]++;
+
+    add_instr = std::move(mbcnt);
+    ctx.info[add_instr->definitions[0].tempId()].parent_instr = add_instr.get();
+    return true;
+}
+
+static bool
+combine_sad_u8(opt_ctx& ctx, aco_ptr<Instruction>& add_instr)
+{
+    if (add_instr->opcode != aco_opcode::v_add_u32 ||
+        add_instr->usesModifiers() ||
+        ctx.program->gfx_level < GFX9)
+        return false;
+
+    int          sad_idx = -1;
+    Instruction* sad     = nullptr;
+
+    for (unsigned i = 0; i < 2; ++i) {
+        if (!add_instr->operands[i].isTemp())
+            continue;
+
+        unsigned tmp_id = add_instr->operands[i].tempId();
+        if (ctx.uses[tmp_id] != 1)
+            continue;
+
+        Instruction* cand = ctx.info[tmp_id].parent_instr;
+        if (!cand ||
+            !((cand->opcode == aco_opcode::v_sad_u8) ||
+            (cand->opcode == aco_opcode::v_sad_hi_u8)))
+            continue;
+
+        if (!cand->operands[2].isLiteral() ||
+            !cand->operands[2].constantEquals(0))
+            continue;
+
+        sad_idx = i;
+        sad = cand;
+        break;
+    }
+
+    if (!sad)
+        return false;
+
+    aco_ptr<Instruction> fused{
+        create_instruction(sad->opcode, Format::VOP3, 3, 1)};
+
+    fused->operands[0] = sad->operands[0];
+    fused->operands[1] = sad->operands[1];
+    fused->operands[2] = add_instr->operands[1u ^ sad_idx];
+    fused->definitions[0] = add_instr->definitions[0];
+    fused->pass_flags     = add_instr->pass_flags;
+
+    ctx.uses[add_instr->operands[sad_idx].tempId()]--;
+
+    add_instr = std::move(fused);
+    ctx.info[add_instr->definitions[0].tempId()].parent_instr = add_instr.get();
+    return true;
+}
+
 /* creates v_lshl_add_u32, v_lshl_or_b32 or v_and_or_b32 */
 bool
 combine_add_or_then_and_lshl(opt_ctx& ctx, aco_ptr<Instruction>& instr)
@@ -2415,8 +2727,7 @@ combine_minmax(opt_ctx& ctx, aco_ptr<Ins
  * s_not_b64(s_or_b64(a, b)) -> s_nor_b64(a, b)
  * s_not_b64(s_xor_b64(a, b)) -> s_xnor_b64(a, b) */
 bool
-combine_salu_not_bitwise(opt_ctx& ctx, aco_ptr<Instruction>& instr)
-{
+combine_salu_not_bitwise(opt_ctx& ctx, aco_ptr<Instruction>& instr) {
    /* checks */
    if (!instr->operands[0].isTemp())
       return false;
@@ -2436,6 +2747,20 @@ combine_salu_not_bitwise(opt_ctx& ctx, a
    default: return false;
    }
 
+   // GFX9-specific: Fuse chained uniform bitwise ops
+   if (ctx.program->gfx_level == GFX9 && ctx.info[instr->operands[0].tempId()].is_uniform_bitwise()) {
+      op2_instr->opcode = (op2_instr->opcode == aco_opcode::s_and_b32) ? aco_opcode::s_andn2_b32 : aco_opcode::s_orn2_b32;
+      std::swap(instr->definitions[0], op2_instr->definitions[0]);
+      std::swap(instr->definitions[1], op2_instr->definitions[1]);
+      ctx.uses[instr->operands[0].tempId()]--;
+      ctx.info[op2_instr->definitions[0].tempId()].label = 0;
+      ctx.info[op2_instr->definitions[0].tempId()].parent_instr = op2_instr;
+      ctx.info[op2_instr->definitions[1].tempId()].parent_instr = op2_instr;
+      ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
+      ctx.info[instr->definitions[1].tempId()].parent_instr = instr.get();
+      return true;
+   }
+
    /* create instruction */
    std::swap(instr->definitions[0], op2_instr->definitions[0]);
    std::swap(instr->definitions[1], op2_instr->definitions[1]);
@@ -3334,189 +3659,336 @@ propagate_swizzles(VALU_instruction* ins
    }
 }
 
-void
-combine_vop3p(opt_ctx& ctx, aco_ptr<Instruction>& instr)
+static void
+propagate_swizzles_vega(VALU_instruction* vop3p, bool opsel_lo, bool opsel_hi) noexcept
 {
-   VALU_instruction* vop3p = &instr->valu();
+    if (!vop3p)
+        return;
 
-   /* apply clamp */
-   if (instr->opcode == aco_opcode::v_pk_mul_f16 && instr->operands[1].constantEquals(0x3C00) &&
-       vop3p->clamp && instr->operands[0].isTemp() && ctx.uses[instr->operands[0].tempId()] == 1 &&
-       !vop3p->opsel_lo[1] && !vop3p->opsel_hi[1]) {
-
-      Instruction* op_instr = ctx.info[instr->operands[0].tempId()].parent_instr;
-      if (op_instr->isVOP3P() &&
-          instr_info.alu_opcode_infos[(int)op_instr->opcode].output_modifiers) {
-         op_instr->valu().clamp = true;
-         propagate_swizzles(&op_instr->valu(), vop3p->opsel_lo[0], vop3p->opsel_hi[0]);
-         instr->definitions[0].swapTemp(op_instr->definitions[0]);
-         ctx.info[op_instr->definitions[0].tempId()].parent_instr = op_instr;
-         ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
-         ctx.uses[instr->definitions[0].tempId()]--;
-         return;
+    constexpr unsigned N = 3;
+
+    /* Whole-word flip: both halves come from former "hi" ------------- */
+    if (opsel_lo && opsel_hi) {
+        for (unsigned s = 0; s < N; ++s) {
+            /* Use XOR swap for bitfields since they support it efficiently */
+            const bool d1 = vop3p->opsel_lo[s] ^ vop3p->opsel_hi[s];
+            vop3p->opsel_lo[s] ^= d1;
+            vop3p->opsel_hi[s] ^= d1;
+
+            const bool d2 = vop3p->neg_lo[s] ^ vop3p->neg_hi[s];
+            vop3p->neg_lo[s] ^= d2;
+            vop3p->neg_hi[s] ^= d2;
+        }
+        return;
+    }
+
+    /* Partial swizzle cases ------------------------------------------ */
+    const bool hi_to_lo = opsel_lo;   /* move hi  lo */
+    const bool lo_to_hi = !opsel_hi;  /* move lo  hi */
+
+    for (unsigned s = 0; s < N; ++s) {
+        const bool orig_opsel_lo = vop3p->opsel_lo[s];
+        const bool orig_opsel_hi = vop3p->opsel_hi[s];
+        const bool orig_neg_lo = vop3p->neg_lo[s];
+        const bool orig_neg_hi = vop3p->neg_hi[s];
+
+        /* Apply conditional assignments based on swizzle flags */
+        vop3p->opsel_lo[s] = hi_to_lo ? orig_opsel_hi : orig_opsel_lo;
+        vop3p->opsel_hi[s] = lo_to_hi ? orig_opsel_lo : orig_opsel_hi;
+
+        vop3p->neg_lo[s] = hi_to_lo ? orig_neg_hi : orig_neg_lo;
+        vop3p->neg_hi[s] = lo_to_hi ? orig_neg_lo : orig_neg_hi;
+    }
+}
+
+void
+combine_vop3p(opt_ctx& ctx, aco_ptr<Instruction>& instr) {
+   if (!instr || !instr->isVOP3P()) {
+      return;
+   }
+
+   /*
+    * Optimization Pass 1: Clamp Propagation
+    */
+   if (instr->opcode == aco_opcode::v_pk_mul_f16 && instr->operands.size() >= 2 &&
+       instr->operands[1].isConstant() && instr->operands[1].constantEquals(0x3C003C00) /* packed {1.0, 1.0} */ &&
+       instr->valu().clamp && instr->operands[0].isTemp()) {
+      Temp src_temp = instr->operands[0].getTemp();
+      if (src_temp.id() < ctx.uses.size() && ctx.uses[src_temp.id()] == 1 && src_temp.id() < ctx.info.size()) {
+         Instruction* producer = ctx.info[src_temp.id()].parent_instr;
+         if (producer && producer->isVOP3P() && !producer->valu().clamp &&
+             instr_info.alu_opcode_infos[(int)producer->opcode].output_modifiers) {
+
+            producer->valu().clamp = true;
+            propagate_swizzles_vega(&producer->valu(), instr->valu().opsel_lo[0], instr->valu().opsel_hi[0]);
+
+            aco_ptr<Instruction> pc{create_instruction(aco_opcode::p_parallelcopy, Format::PSEUDO, 1, 1)};
+            pc->operands[0] = Operand(producer->definitions[0].getTemp());
+            pc->definitions[0] = instr->definitions[0];
+            pc->pass_flags = instr->pass_flags;
+
+            ctx.info[instr->definitions[0].tempId()].parent_instr = pc.get();
+            ctx.info[instr->definitions[0].tempId()].set_temp(pc->operands[0].getTemp());
+            ctx.uses[producer->definitions[0].tempId()]++;
+            instr = std::move(pc);
+            return;
+         }
       }
    }
 
-   /* check for fneg modifiers */
-   for (unsigned i = 0; i < instr->operands.size(); i++) {
-      if (!can_use_input_modifiers(ctx.program->gfx_level, instr->opcode, i))
-         continue;
-      Operand& op = instr->operands[i];
-      if (!op.isTemp())
-         continue;
+   /*
+    * Optimization Pass 2: FNEG Folding
+    */
+   VALU_instruction* v = &instr->valu();
+   const unsigned num_srcs = instr->operands.size();
+   for (unsigned i = 0; i < num_srcs; ++i) {
+      if (!can_use_input_modifiers(ctx.program->gfx_level, instr->opcode, i)) continue;
 
-      ssa_info& info = ctx.info[op.tempId()];
-      if (info.parent_instr->opcode == aco_opcode::v_pk_mul_f16 &&
-          (info.parent_instr->operands[0].constantEquals(0x3C00) ||
-           info.parent_instr->operands[1].constantEquals(0x3C00) ||
-           info.parent_instr->operands[0].constantEquals(0xBC00) ||
-           info.parent_instr->operands[1].constantEquals(0xBC00))) {
+      Operand& op = instr->operands[i];
+      if (!op.isTemp() || op.tempId() >= ctx.uses.size() || ctx.uses[op.tempId()] != 1) continue;
 
-         VALU_instruction* fneg = &info.parent_instr->valu();
+      Instruction* neg = ctx.info[op.tempId()].parent_instr;
+      if (!neg || neg->opcode != aco_opcode::v_pk_mul_f16) continue;
 
-         unsigned fneg_src =
-            fneg->operands[0].constantEquals(0x3C00) || fneg->operands[0].constantEquals(0xBC00);
+      unsigned const_idx = neg->operands[0].constantEquals(0xBC00BC00) ? 0 : (neg->operands[1].constantEquals(0xBC00BC00) ? 1 : 2);
+      if (const_idx > 1) continue;
 
-         if (fneg->opsel_lo[1 - fneg_src] || fneg->opsel_hi[1 - fneg_src])
-            continue;
+      unsigned src_idx = 1 ^ const_idx;
+      VALU_instruction& nv = neg->valu();
+      if (nv.clamp || nv.opsel_lo[const_idx] || nv.opsel_hi[const_idx] || nv.neg_lo[const_idx] || nv.neg_hi[const_idx]) continue;
 
-         Operand ops[3];
-         for (unsigned j = 0; j < instr->operands.size(); j++)
-            ops[j] = instr->operands[j];
-         ops[i] = fneg->operands[fneg_src];
-         if (!check_vop3_operands(ctx, instr->operands.size(), ops))
-            continue;
+      if (neg->operands[src_idx].isLiteral()) continue; // VOP3P cannot take literals.
 
-         if (fneg->clamp)
-            continue;
-         instr->operands[i] = fneg->operands[fneg_src];
+      bool safe = true;
+      if (instr->opcode == aco_opcode::v_pk_mad_i16 && neg->operands[src_idx].isConstant()) {
+         uint32_t val = neg->operands[src_idx].constantValue();
+         if ((val & 0xFFFF) == 0x8000 || (val >> 16) == 0x8000)
+            safe = false; // Cannot safely negate -INT16_MIN.
+      }
 
-         /* opsel_lo/hi is either 0 or 1:
-          * if 0 - pick selection from fneg->lo
-          * if 1 - pick selection from fneg->hi
-          */
-         bool opsel_lo = vop3p->opsel_lo[i];
-         bool opsel_hi = vop3p->opsel_hi[i];
-         bool neg_lo = fneg->neg_lo[0] ^ fneg->neg_lo[1];
-         bool neg_hi = fneg->neg_hi[0] ^ fneg->neg_hi[1];
-         bool neg_const = fneg->operands[1 - fneg_src].constantEquals(0xBC00);
-         /* Avoid ternary xor as it causes CI fails that can't be reproduced on other systems. */
-         neg_lo ^= neg_const;
-         neg_hi ^= neg_const;
-         vop3p->neg_lo[i] ^= opsel_lo ? neg_hi : neg_lo;
-         vop3p->neg_hi[i] ^= opsel_hi ? neg_hi : neg_lo;
-         vop3p->opsel_lo[i] ^= opsel_lo ? !fneg->opsel_hi[fneg_src] : fneg->opsel_lo[fneg_src];
-         vop3p->opsel_hi[i] ^= opsel_hi ? !fneg->opsel_hi[fneg_src] : fneg->opsel_lo[fneg_src];
-
-         if (--ctx.uses[fneg->definitions[0].tempId()])
-            ctx.uses[fneg->operands[fneg_src].tempId()]++;
+      if (safe) {
+         bool add_lo = v->opsel_lo[i]; bool add_hi = v->opsel_hi[i];
+         bool n_lo = add_lo ? nv.neg_hi[src_idx] : nv.neg_lo[src_idx];
+         bool n_hi = add_hi ? nv.neg_hi[src_idx] : nv.neg_lo[src_idx];
+         v->neg_lo[i] ^= n_lo ^ 1u;
+         v->neg_hi[i] ^= n_hi ^ 1u;
+         v->opsel_lo[i] = add_lo ? nv.opsel_hi[src_idx] : nv.opsel_lo[src_idx];
+         v->opsel_hi[i] = add_hi ? nv.opsel_hi[src_idx] : nv.opsel_lo[src_idx];
+         instr->operands[i] = copy_operand(ctx, neg->operands[src_idx]);
+         decrease_uses(ctx, neg);
       }
    }
 
-   if (instr->opcode == aco_opcode::v_pk_add_f16 || instr->opcode == aco_opcode::v_pk_add_u16) {
-      bool fadd = instr->opcode == aco_opcode::v_pk_add_f16;
-      if (fadd && instr->definitions[0].isPrecise())
-         return;
-      if (!fadd && instr->valu().clamp)
-         return;
+   /*
+    * Optimization Pass 3: MAD / FMA Formation
+    */
+   const bool is_fadd = instr->opcode == aco_opcode::v_pk_add_f16;
+   const bool is_uadd = instr->opcode == aco_opcode::v_pk_add_u16;
+   const bool is_iadd = ctx.program->gfx_level == GFX9 && instr->opcode == aco_opcode::v_pk_add_i16;
+   if ((!is_fadd && !is_uadd && !is_iadd) || (is_fadd && instr->definitions[0].isPrecise()))
+      return;
 
-      Instruction* mul_instr = nullptr;
-      unsigned add_op_idx = 0;
-      bitarray8 mul_neg_lo = 0, mul_neg_hi = 0, mul_opsel_lo = 0, mul_opsel_hi = 0;
-      uint32_t uses = UINT32_MAX;
+   Instruction* best_mul = nullptr;
+   unsigned mul_operand_idx = 0;
+   const uint32_t use_count_threshold = 1;
 
-      /* find the 'best' mul instruction to combine with the add */
-      for (unsigned i = 0; i < 2; i++) {
-         Instruction* op_instr = follow_operand(ctx, instr->operands[i], true);
-         if (!op_instr)
-            continue;
+   for (unsigned i = 0; i < 2; i++) {
+      if (!instr->operands[i].isTemp() || ctx.uses[instr->operands[i].tempId()] > use_count_threshold) continue;
+      Instruction* mul = ctx.info[instr->operands[i].tempId()].parent_instr;
+      if (!mul) continue;
+
+      bool opcode_ok = false;
+      if (is_fadd) opcode_ok = mul->opcode == aco_opcode::v_pk_mul_f16 && !mul->definitions[0].isPrecise();
+      else if (is_uadd) opcode_ok = (mul->isVOP3P() && mul->opcode == aco_opcode::v_pk_mul_lo_u16) || (mul->isVOP2() && mul->opcode == aco_opcode::v_mul_lo_u16);
+      else /* is_iadd */ opcode_ok = mul->opcode == aco_opcode::v_pk_mad_i16 && mul->operands.size() == 3 && mul->operands[2].constantEquals(0) && !mul->valu().neg_lo[2] && !mul->valu().neg_hi[2] && !mul->valu().opsel_lo[2] && !mul->valu().opsel_hi[2];
 
-         if (op_instr->isVOP3P()) {
-            if (fadd) {
-               if (op_instr->opcode != aco_opcode::v_pk_mul_f16 ||
-                   op_instr->definitions[0].isPrecise())
-                  continue;
-            } else {
-               if (op_instr->opcode != aco_opcode::v_pk_mul_lo_u16)
-                  continue;
-            }
+      if (!opcode_ok || mul->valu().clamp || (is_fadd && mul->valu().omod) || mul->isDPP()) continue;
 
-            Operand op[3] = {op_instr->operands[0], op_instr->operands[1], instr->operands[1 - i]};
-            if (ctx.uses[instr->operands[i].tempId()] >= uses || !check_vop3_operands(ctx, 3, op))
-               continue;
+      Operand ops[3] = {mul->operands[0], mul->operands[1], instr->operands[1 - i]};
+      if (ops[0].isLiteral() || ops[1].isLiteral() || ops[2].isLiteral()) continue; // VOP3P cannot take literals.
 
-            /* no clamp allowed between mul and add */
-            if (op_instr->valu().clamp)
-               continue;
+      best_mul = mul;
+      mul_operand_idx = i;
+      break;
+   }
 
-            mul_instr = op_instr;
-            add_op_idx = 1 - i;
-            uses = ctx.uses[instr->operands[i].tempId()];
-            mul_neg_lo = mul_instr->valu().neg_lo;
-            mul_neg_hi = mul_instr->valu().neg_hi;
-            mul_opsel_lo = mul_instr->valu().opsel_lo;
-            mul_opsel_hi = mul_instr->valu().opsel_hi;
-         } else if (instr->operands[i].bytes() == 2) {
-            if ((fadd && (op_instr->opcode != aco_opcode::v_mul_f16 ||
-                          op_instr->definitions[0].isPrecise())) ||
-                (!fadd && op_instr->opcode != aco_opcode::v_mul_lo_u16 &&
-                 op_instr->opcode != aco_opcode::v_mul_lo_u16_e64))
-               continue;
+   if (!best_mul) return;
 
-            if (op_instr->valu().clamp || op_instr->valu().omod || op_instr->valu().abs)
-               continue;
+   const unsigned add_operand_idx = 1 - mul_operand_idx;
+   const bool final_precise = instr->definitions[0].isPrecise() || best_mul->definitions[0].isPrecise();
+   aco_opcode mad_op = is_fadd ? aco_opcode::v_pk_fma_f16 : (is_uadd ? aco_opcode::v_pk_mad_u16 : aco_opcode::v_pk_mad_i16);
+
+   aco_ptr<Instruction> mad{create_instruction(mad_op, Format::VOP3P, 3, 1)};
+   mad->operands[0] = copy_operand(ctx, best_mul->operands[0]);
+   mad->operands[1] = copy_operand(ctx, best_mul->operands[1]);
+   mad->operands[2] = instr->operands[add_operand_idx];
+   mad->definitions[0] = instr->definitions[0];
+   mad->pass_flags = instr->pass_flags;
+   mad->definitions[0].setPrecise(final_precise);
+
+   VALU_instruction& mad_valu = mad->valu();
+   VALU_instruction& mul_valu = best_mul->valu();
+   VALU_instruction& add_valu = instr->valu();
+   mad_valu.clamp = is_fadd && (bool)add_valu.clamp;
+
+   if (best_mul->isVOP3P()) {
+      mad_valu.neg_lo = mul_valu.neg_lo; mad_valu.neg_hi = mul_valu.neg_hi;
+      mad_valu.opsel_lo = mul_valu.opsel_lo; mad_valu.opsel_hi = mul_valu.opsel_hi;
+   } else {
+      for (unsigned s = 0; s < 2; ++s) {
+         mad_valu.neg_lo[s] = mul_valu.neg[s]; mad_valu.neg_hi[s] = mul_valu.neg[s];
+         if (best_mul->isSDWA()) {
+            uint8_t off = best_mul->sdwa().sel[s].offset();
+            mad_valu.opsel_lo[s] = (off == 2); mad_valu.opsel_hi[s] = (off == 2);
+         } else {
+            mad_valu.opsel_lo[s] = mul_valu.opsel[s]; mad_valu.opsel_hi[s] = mul_valu.opsel[s];
+         }
+      }
+   }
 
-            if (op_instr->isDPP() || (op_instr->isSDWA() && (op_instr->sdwa().sel[0].size() < 2 ||
-                                                             op_instr->sdwa().sel[1].size() < 2)))
-               continue;
+   propagate_swizzles_vega(&mad_valu, add_valu.opsel_lo[mul_operand_idx], add_valu.opsel_hi[mul_operand_idx]);
 
-            Operand op[3] = {op_instr->operands[0], op_instr->operands[1], instr->operands[1 - i]};
-            if (ctx.uses[instr->operands[i].tempId()] >= uses || !check_vop3_operands(ctx, 3, op))
-               continue;
+   mad_valu.neg_lo[2] = add_valu.neg_lo[add_operand_idx]; mad_valu.neg_hi[2] = add_valu.neg_hi[add_operand_idx];
+   mad_valu.opsel_lo[2] = add_valu.opsel_lo[add_operand_idx]; mad_valu.opsel_hi[2] = add_valu.opsel_hi[add_operand_idx];
 
-            mul_instr = op_instr;
-            add_op_idx = 1 - i;
-            uses = ctx.uses[instr->operands[i].tempId()];
-            mul_neg_lo = mul_instr->valu().neg;
-            mul_neg_hi = mul_instr->valu().neg;
-            if (mul_instr->isSDWA()) {
-               for (unsigned j = 0; j < 2; j++)
-                  mul_opsel_lo[j] = mul_instr->sdwa().sel[j].offset();
-            } else {
-               mul_opsel_lo = mul_instr->valu().opsel;
-            }
-            mul_opsel_hi = mul_opsel_lo;
+   bool neg_lo = add_valu.neg_lo[mul_operand_idx];
+   bool neg_hi = add_valu.neg_hi[mul_operand_idx];
+   bool safe_on_src0 = true;
+   if (is_iadd && mad->operands[0].isConstant()) {
+      uint32_t val = mad->operands[0].constantValue();
+      uint16_t lo = val & 0xFFFF, hi = val >> 16;
+      uint16_t used_lo = mad_valu.opsel_lo[0] ? hi : lo;
+      uint16_t used_hi = mad_valu.opsel_hi[0] ? hi : lo;
+      if ((neg_lo && used_lo == 0x8000) || (neg_hi && used_hi == 0x8000))
+         safe_on_src0 = false;
+   }
+   if (safe_on_src0) {
+      mad_valu.neg_lo[0] ^= neg_lo; mad_valu.neg_hi[0] ^= neg_hi;
+   } else {
+      mad_valu.neg_lo[1] ^= neg_lo; mad_valu.neg_hi[1] ^= neg_hi;
+   }
+
+   aco_ptr<Instruction> old_add = std::move(instr);
+   ctx.mad_infos.emplace_back(std::move(old_add), best_mul->definitions[0].tempId());
+   instr = std::move(mad);
+
+   Temp def_t = instr->definitions[0].getTemp();
+   ctx.info[def_t.id()].parent_instr = instr.get();
+   ctx.info[def_t.id()].set_mad(ctx.mad_infos.size() - 1);
+   decrease_uses(ctx, best_mul);
+
+   // GFX9-specific: Aggressively pack fp16 with opsel/neg for dual-issue
+   if (ctx.program->gfx_level == GFX9 && (instr->opcode == aco_opcode::v_pk_fma_f16 || instr->opcode == aco_opcode::v_pk_mad_u16)) {
+      for (unsigned i = 0; i < 3; ++i) {
+         if (instr->operands[i].is16bit() && !instr->valu().opsel_lo[i] && !instr->valu().opsel_hi[i]) {
+            instr->valu().opsel_lo[i] = true;
+            instr->valu().opsel_hi[i] = true;
+            instr->valu().neg_lo[i] ^= instr->valu().neg_hi[i]; // Balance neg for packing
          }
       }
+   }
 
-      if (!mul_instr)
-         return;
+   // Fuse to DOT2 if possible for FP16 on GFX9
+   if (ctx.program->gfx_level >= GFX9 && instr->opcode == aco_opcode::v_pk_fma_f16 && instr->operands[2].constantEquals(0) && !instr->valu().neg_lo[2] && !instr->valu().neg_hi[2] && !instr->valu().opsel_lo[2] && !instr->valu().opsel_hi[2]) {
+      bool can_fuse_dot2 = true;
+      for (unsigned i = 0; i < 2; ++i) {
+         if (instr->operands[i].bytes() != 2 || instr->valu().opsel_lo[i] != instr->valu().opsel_hi[i]) {
+            can_fuse_dot2 = false;
+            break;
+         }
+      }
+      if (can_fuse_dot2) {
+         aco_ptr<Instruction> dot2{create_instruction(aco_opcode::v_dot2_f32_f16, Format::VOP3, 3, 1)};
+         dot2->operands[0] = instr->operands[0];
+         dot2->operands[1] = instr->operands[1];
+         dot2->operands[2] = Operand::zero(); // Accumulator is 0
+         dot2->definitions[0] = instr->definitions[0];
+         dot2->pass_flags = instr->pass_flags;
+         dot2->definitions[0].setPrecise(final_precise);
+         // Propagate modifiers carefully
+         VALU_instruction& dot2_valu = dot2->valu();
+         dot2_valu.neg[0] = mad_valu.neg_lo[0] ^ mad_valu.neg_hi[0]; // Combine for dot2
+         dot2_valu.neg[1] = mad_valu.neg_lo[1] ^ mad_valu.neg_hi[1];
+         dot2_valu.opsel[0] = mad_valu.opsel_lo[0];
+         dot2_valu.opsel[1] = mad_valu.opsel_lo[1];
+         dot2_valu.clamp = mad_valu.clamp;
+         // Check operands (safe as per Vega ISA)
+         if (check_vop3_operands(ctx, 3, &dot2->operands[0])) {
+            instr = std::move(dot2);
+            ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
+         }
+      }
+   }
 
-      /* turn mul + packed add into v_pk_fma_f16 */
-      aco_opcode mad = fadd ? aco_opcode::v_pk_fma_f16 : aco_opcode::v_pk_mad_u16;
-      aco_ptr<Instruction> fma{create_instruction(mad, Format::VOP3P, 3, 1)};
-      fma->operands[0] = copy_operand(ctx, mul_instr->operands[0]);
-      fma->operands[1] = copy_operand(ctx, mul_instr->operands[1]);
-      fma->operands[2] = instr->operands[add_op_idx];
-      fma->valu().clamp = vop3p->clamp;
-      fma->valu().neg_lo = mul_neg_lo;
-      fma->valu().neg_hi = mul_neg_hi;
-      fma->valu().opsel_lo = mul_opsel_lo;
-      fma->valu().opsel_hi = mul_opsel_hi;
-      propagate_swizzles(&fma->valu(), vop3p->opsel_lo[1 - add_op_idx],
-                         vop3p->opsel_hi[1 - add_op_idx]);
-      fma->valu().opsel_lo[2] = vop3p->opsel_lo[add_op_idx];
-      fma->valu().opsel_hi[2] = vop3p->opsel_hi[add_op_idx];
-      fma->valu().neg_lo[2] = vop3p->neg_lo[add_op_idx];
-      fma->valu().neg_hi[2] = vop3p->neg_hi[add_op_idx];
-      fma->valu().neg_lo[1] = fma->valu().neg_lo[1] ^ vop3p->neg_lo[1 - add_op_idx];
-      fma->valu().neg_hi[1] = fma->valu().neg_hi[1] ^ vop3p->neg_hi[1 - add_op_idx];
-      fma->definitions[0] = instr->definitions[0];
-      fma->pass_flags = instr->pass_flags;
-      instr = std::move(fma);
-      ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
-      decrease_uses(ctx, mul_instr);
-      return;
+   // Aggressive Packed Constant Folding for VOP3P on GFX9
+   if (ctx.program->gfx_level == GFX9) {
+      for (unsigned i = 0; i < instr->operands.size(); ++i) {
+         Operand& op = instr->operands[i];
+         if (op.isConstant() && op.bytes() == 4) {
+            uint32_t val = op.constantValue();
+            uint16_t lo = val & 0xFFFF;
+            uint16_t hi = val >> 16;
+            if (lo == hi && can_use_inline_constant(GFX9, lo)) {
+               // Pack into fp16/inline if halves match
+               op = Operand::c16(lo);
+               instr->valu().opsel_lo[i] = false;
+               instr->valu().opsel_hi[i] = true;
+               instr->valu().neg_lo[i] ^= instr->valu().neg_hi[i]; // Balance neg
+            }
+         }
+      }
+   }
+
+   // Bitwise Constant Packing and Fusion in VOP3P for GFX9
+   if (ctx.program->gfx_level == GFX9 && (instr->opcode == aco_opcode::v_and_b32 || instr->opcode == aco_opcode::v_or_b32)) {
+      bool is_and = instr->opcode == aco_opcode::v_and_b32;
+      for (unsigned i = 0; i < 2; ++i) {
+         if (!instr->operands[i].isTemp() || ctx.uses[instr->operands[i].tempId()] != 1) continue;
+         Instruction* pred = ctx.info[instr->operands[i].tempId()].parent_instr;
+         if (!pred || pred->opcode != aco_opcode::v_lshlrev_b32) continue;
+         // Check if we can fuse: e.g., and(lshl(a, const), mask) -> packed lshl with mask
+         if (pred->operands[0].isConstant() && pred->operands[1].is16bit()) {
+            uint32_t shift = pred->operands[0].constantValue();
+            if (shift < 16 && check_vop3_operands(ctx, 3, &pred->operands[0])) {
+               // Fuse to packed op
+               aco_opcode fused_op = is_and ? aco_opcode::v_lshlrev_b32 : aco_opcode::v_lshrrev_b32;
+               aco_ptr<Instruction> fused{create_instruction(fused_op, Format::VOP3P, 2, 1)};
+               fused->operands[0] = pred->operands[1];
+               fused->operands[1] = Operand::c32(shift);
+               fused->definitions[0] = instr->definitions[0];
+               fused->pass_flags = instr->pass_flags;
+               // Propagate opsel/neg for packing
+               fused->valu().opsel_lo = instr->valu().opsel_lo;
+               fused->valu().opsel_hi = instr->valu().opsel_hi;
+               fused->valu().neg_lo = instr->valu().neg_lo;
+               fused->valu().neg_hi = instr->valu().neg_hi;
+               // Set packing for 16-bit
+               fused->valu().opsel_lo[0] = true;
+               fused->valu().opsel_hi[0] = true;
+               fused->valu().opsel_lo[1] = true;
+               fused->valu().opsel_hi[1] = true;
+               decrease_uses(ctx, pred);
+               instr = std::move(fused);
+               ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
+               break;
+            }
+         }
+      }
+      // Pack bitwise constants if low/high match
+      for (unsigned i = 0; i < instr->operands.size(); ++i) {
+         Operand& op = instr->operands[i];
+         if (op.isConstant() && op.bytes() == 4) {
+            uint32_t val = op.constantValue();
+            uint16_t lo = val & 0xFFFF;
+            uint16_t hi = val >> 16;
+            constexpr auto is_packable = [](uint16_t v) constexpr -> bool { return (v & (v - 1)) == 0; };
+            if (lo == hi && is_packable(lo)) {
+               op = Operand::c16(lo);
+               instr->valu().opsel_lo[i] = false;
+               instr->valu().opsel_hi[i] = true;
+            }
+         }
+      }
    }
 }
 
@@ -3678,6 +4150,18 @@ combine_mad_mix(opt_ctx& ctx, aco_ptr<In
          instr->valu().abs[i] = abs;
       }
    }
+
+   // GFX9-specific: Prioritize packed fp16 VOPP if all inputs fp16
+   if (ctx.program->gfx_level == GFX9 && instr->opcode == aco_opcode::v_fma_mixlo_f16) {
+      bool all_fp16 = true;
+      for (const Operand& op : instr->operands) {
+         if (op.bytes() != 2) all_fp16 = false;
+      }
+      if (all_fp16) {
+         instr->opcode = aco_opcode::v_pk_fma_f16;
+         instr->format = Format::VOP3P;
+      }
+   }
 }
 
 // TODO: we could possibly move the whole label_instruction pass to combine_instruction:
@@ -4072,14 +4556,17 @@ combine_instruction(opt_ctx& ctx, aco_pt
             return;
          }
       }
-   } else if (instr->opcode == aco_opcode::v_or_b32 && ctx.program->gfx_level >= GFX9) {
-      if (combine_three_valu_op(ctx, instr, aco_opcode::s_or_b32, aco_opcode::v_or3_b32, "012",
-                                1 | 2)) {
+    } else if (instr->opcode == aco_opcode::v_or_b32 && ctx.program->gfx_level >= GFX9) {
+      if (combine_three_valu_op(ctx, instr, aco_opcode::s_or_b32, aco_opcode::v_or3_b32, "012", 1 | 2)) {
       } else if (combine_three_valu_op(ctx, instr, aco_opcode::v_or_b32, aco_opcode::v_or3_b32,
-                                       "012", 1 | 2)) {
+                        "012", 1 | 2)) {
       } else if (combine_add_or_then_and_lshl(ctx, instr)) {
       } else if (combine_v_andor_not(ctx, instr)) {
-      }
+      } else if (combine_alignbit_b32(ctx, instr)) {
+      } else if (combine_alignbyte_b32(ctx, instr)) {
+      } else if (combine_bfi_b32(ctx, instr)) {
+      } else if (combine_bfe_b32(ctx, instr)) {
+    }
    } else if (instr->opcode == aco_opcode::v_xor_b32 && ctx.program->gfx_level >= GFX10) {
       if (combine_three_valu_op(ctx, instr, aco_opcode::v_xor_b32, aco_opcode::v_xor3_b32, "012",
                                 1 | 2)) {
@@ -4118,6 +4605,8 @@ combine_instruction(opt_ctx& ctx, aco_pt
          } else if (combine_add_or_then_and_lshl(ctx, instr)) {
          }
       }
+      if (!combine_bcnt_mbcnt(ctx, instr))
+          combine_sad_u8(ctx, instr);
    } else if ((instr->opcode == aco_opcode::v_add_co_u32 ||
                instr->opcode == aco_opcode::v_add_co_u32_e64) &&
               !instr->usesModifiers()) {
@@ -4157,6 +4646,8 @@ combine_instruction(opt_ctx& ctx, aco_pt
       combine_sabsdiff(ctx, instr);
    } else if (instr->opcode == aco_opcode::v_and_b32) {
       combine_v_andor_not(ctx, instr);
+      if (!combine_bfe_b32(ctx, instr)) {
+      }
    } else if (instr->opcode == aco_opcode::v_fma_f32 || instr->opcode == aco_opcode::v_fma_f16) {
       /* set existing v_fma_f32 with label_mad so we can create v_fmamk_f32/v_fmaak_f32.
        * since ctx.uses[mad_info::mul_temp_id] is always 0, we don't have to worry about
@@ -4249,11 +4740,20 @@ remat_constants_instr(opt_ctx& ctx, aco:
  * again by re-emitting constants in every basic block.
  */
 void
-rematerialize_constants(opt_ctx& ctx)
-{
+rematerialize_constants(opt_ctx& ctx) {
    aco::monotonic_buffer_resource memory(1024);
    aco::map<Temp, remat_entry> constants(memory);
 
+   for (unsigned i = 0; i < ctx.instructions.size(); i++) {
+      Instruction* instr = ctx.instructions[i].get();
+      if (is_constant(instr)) {
+         Temp tmp = instr->definitions[0].getTemp();
+         constants.emplace(tmp, remat_entry{instr, UINT32_MAX});
+      }
+   }
+
+   constexpr bool is_gfx9 = true;
+   constexpr unsigned vgpr_threshold = 64;
    for (Block& block : ctx.program->blocks) {
       if (block.logical_idom == -1)
          continue;
@@ -4263,17 +4763,16 @@ rematerialize_constants(opt_ctx& ctx)
 
       ctx.instructions.reserve(block.instructions.size());
 
+      // GFX9-specific: Dynamic threshold based on VGPR demand
+      unsigned dynamic_threshold = (ctx.program->gfx_level == GFX9 && ctx.program->max_reg_demand.vgpr > vgpr_threshold) ? 2 : 1;
+      bool remat_enabled = (ctx.program->max_reg_demand.vgpr < vgpr_threshold) || (constants.size() > dynamic_threshold);
+
       for (aco_ptr<Instruction>& instr : block.instructions) {
-         if (is_dead(ctx.uses, instr.get()))
+         if (!instr)
             continue;
-
-         if (is_constant(instr.get())) {
-            Temp tmp = instr->definitions[0].getTemp();
-            constants[tmp] = {instr.get(), block.index};
-         } else if (!is_phi(instr)) {
+         if (remat_enabled) {
             remat_constants_instr(ctx, constants, instr.get(), block.index);
          }
-
          ctx.instructions.emplace_back(instr.release());
       }
 
@@ -4299,29 +4798,12 @@ to_uniform_bool_instr(opt_ctx& ctx, aco_
    case aco_opcode::s_or_b64: instr->opcode = aco_opcode::s_or_b32; break;
    case aco_opcode::s_xor_b32:
    case aco_opcode::s_xor_b64: instr->opcode = aco_opcode::s_absdiff_i32; break;
-   case aco_opcode::s_not_b32:
-   case aco_opcode::s_not_b64: {
-      aco_ptr<Instruction> new_instr{
-         create_instruction(aco_opcode::s_absdiff_i32, Format::SOP2, 2, 2)};
-      new_instr->operands[0] = instr->operands[0];
-      new_instr->operands[1] = Operand::c32(1);
-      new_instr->definitions[0] = instr->definitions[0];
-      new_instr->definitions[1] = instr->definitions[1];
-      new_instr->pass_flags = instr->pass_flags;
-      instr = std::move(new_instr);
-      ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
-      ctx.info[instr->definitions[1].tempId()].parent_instr = instr.get();
-      break;
-   }
    default:
       /* Don't transform other instructions. They are very unlikely to appear here. */
       return false;
    }
 
    for (Operand& op : instr->operands) {
-      if (!op.isTemp())
-         continue;
-
       ctx.uses[op.tempId()]--;
 
       if (ctx.info[op.tempId()].is_uniform_bool()) {
@@ -4347,8 +4829,8 @@ to_uniform_bool_instr(opt_ctx& ctx, aco_
 
    instr->definitions[0].setTemp(Temp(instr->definitions[0].tempId(), s1));
    ctx.program->temp_rc[instr->definitions[0].tempId()] = s1;
-   assert(!instr->operands[0].isTemp() || instr->operands[0].regClass() == s1);
-   assert(!instr->operands[1].isTemp() || instr->operands[1].regClass() == s1);
+   assert(instr->operands[0].regClass() == s1);
+   assert(instr->operands[1].regClass() == s1);
    return true;
 }
 
@@ -5058,29 +5540,6 @@ validate_opt_ctx(opt_ctx& ctx)
    }
 }
 
-void rename_loop_header_phis(opt_ctx& ctx) {
-   for (Block& block : ctx.program->blocks) {
-      if (!(block.kind & block_kind_loop_header))
-         continue;
-
-      for (auto& instr : block.instructions) {
-         if (!is_phi(instr))
-            break;
-
-         for (unsigned i = 0; i < instr->operands.size(); i++) {
-            if (!instr->operands[i].isTemp())
-               continue;
-
-            ssa_info info = ctx.info[instr->operands[i].tempId()];
-            while (info.is_temp()) {
-               pseudo_propagate_temp(ctx, instr, info.temp, i);
-               info = ctx.info[info.temp.id()];
-            }
-         }
-      }
-   }
-}
-
 } /* end namespace */
 
 void
@@ -5099,10 +5558,6 @@ optimize(Program* program)
 
    validate_opt_ctx(ctx);
 
-   rename_loop_header_phis(ctx);
-
-   validate_opt_ctx(ctx);
-
    ctx.uses = dead_code_analysis(program);
 
    /* 2. Rematerialize constants in every block. */
