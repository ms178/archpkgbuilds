--- a/src/amd/compiler/aco_lower_subdword.cpp	2025-08-07 00:20:06.036127008 +0200
+++ b/src/amd/compiler/aco_lower_subdword.cpp	2025-08-07 00:21:43.765304715 +0200
@@ -7,8 +7,9 @@
 #include "aco_builder.h"
 #include "aco_ir.h"
 
-namespace aco {
+#include <numeric>
 
+namespace aco {
 namespace {
 
 Temp
@@ -28,8 +29,15 @@ dword_def(Program* program, Definition d
 {
    def.setTemp(dword_temp(def.getTemp()));
 
-   if (def.isTemp())
-      program->temp_rc[def.tempId()] = def.regClass();
+   if (def.isTemp()) {
+      uint32_t id = def.tempId();
+      if (id < program->temp_rc.size()) {
+         program->temp_rc[id] = def.regClass();
+      } else {
+         program->temp_rc.resize(id + 1, RegClass());
+         program->temp_rc[id] = def.regClass();
+      }
+   }
 
    return def;
 }
@@ -37,10 +45,11 @@ dword_def(Program* program, Definition d
 Operand
 dword_op(Operand op, bool convert_const)
 {
-   if (op.isTemp() || op.isUndefined())
+   if (op.isTemp() || op.isUndefined()) {
       op.setTemp(dword_temp(op.getTemp()));
-   else if (convert_const && op.isConstant() && op.bytes() < 4)
+   } else if (convert_const && op.isConstant() && op.bytes() < 4) {
       op = Operand::c32(op.constantValue());
+   }
    return op;
 }
 
@@ -55,7 +64,7 @@ emit_pack(Builder& bld, Definition def,
 {
    assert(def.regClass().type() == RegType::vgpr);
 
-   /* split definition into dwords. */
+   /* Handle multi-dword definitions iteratively to avoid recursion depth issues. */
    if (def.size() > 1) {
       aco_ptr<Instruction> vec{
          create_instruction(aco_opcode::p_create_vector, Format::PSEUDO, def.size(), 1)};
@@ -64,25 +73,23 @@ emit_pack(Builder& bld, Definition def,
       unsigned op_idx = 0;
       for (unsigned i = 0; i < def.size(); i++) {
          std::vector<op_info> sub_operands;
+         sub_operands.reserve(operands.size() - op_idx);
          Definition sub_def = bld.def(v1);
          vec->operands[i] = Operand(sub_def.getTemp());
          unsigned sub_bytes = 0;
-         while (sub_bytes < 4) {
-            unsigned new_bytes = MIN2(operands[op_idx].bytes, 4 - sub_bytes);
+         while (sub_bytes < 4 && op_idx < operands.size()) {
+            unsigned new_bytes = std::min(operands[op_idx].bytes, 4u - sub_bytes);
             sub_bytes += new_bytes;
 
             sub_operands.push_back({operands[op_idx].op, operands[op_idx].offset, new_bytes});
 
             if (new_bytes == operands[op_idx].bytes) {
                op_idx++;
-               if (op_idx >= operands.size())
-                  break;
             } else {
                operands[op_idx].offset += new_bytes;
                operands[op_idx].bytes -= new_bytes;
             }
          }
-
          emit_pack(bld, sub_def, std::move(sub_operands));
       }
 
@@ -90,28 +97,53 @@ emit_pack(Builder& bld, Definition def,
       return;
    }
 
-   /* split operands into dwords. */
-   for (unsigned i = 0; i < operands.size(); i++) {
+   /* Fast path for the common case of packing two 16-bit values. */
+   if (operands.size() == 2 && operands[0].bytes == 2 && operands[1].bytes == 2 &&
+       operands[0].offset == 0 && operands[1].offset == 0 &&
+       !operands[0].op.isUndefined() && !operands[1].op.isUndefined()) {
+
+      Operand lo = operands[0].op;
+      Operand hi = operands[1].op;
+
+      if (!lo.isOfType(RegType::vgpr))
+         lo = bld.copy(bld.def(v1), lo);
+      if (!hi.isOfType(RegType::vgpr))
+         hi = bld.copy(bld.def(v1), hi);
+
+      Temp hi_shifted = bld.vop2(aco_opcode::v_lshlrev_b32, bld.def(v1), Operand::c32(16), hi);
+      bld.vop2(aco_opcode::v_or_b32, def, lo, Operand(hi_shifted));
+      return;
+   }
+
+   /* Split multi-dword operands into single dwords. */
+   std::vector<op_info> split_operands;
+   split_operands.reserve(operands.size() * 2);
+   for (unsigned i = 0; i < operands.size(); ) {
       Operand op = operands[i].op;
       unsigned offset = operands[i].offset;
       unsigned bytes = operands[i].bytes;
 
       if (op.isUndefined() || op.isConstant()) {
-         if (op.isConstant())
-            operands[i].op = Operand::c32(op.constantValue64() >> (offset * 8));
-         else
-            operands[i].op = Operand(v1);
+         if (op.isConstant()) {
+            uint64_t val = op.constantValue64();
+            assert(offset < 8 && "Invalid offset for constant");
+            operands[i].op = Operand::c32(static_cast<uint32_t>(val >> (offset * 8)));
+         } else {
+            operands[i].op = Operand(v1); /* Represents an undef dword */
+         }
          operands[i].offset = 0;
+         ++i;
          continue;
       }
 
-      if (op.size() == 1)
+      if (op.size() == 1) {
+         ++i;
          continue;
+      }
 
       assert(!op.isFixed());
 
       RegClass rc = op.isOfType(RegType::vgpr) ? v1 : s1;
-
       aco_ptr<Instruction> split{
          create_instruction(aco_opcode::p_split_vector, Format::PSEUDO, 1, op.size())};
       split->operands[0] = op;
@@ -119,75 +151,60 @@ emit_pack(Builder& bld, Definition def,
          split->definitions[j] = bld.def(rc);
 
       unsigned dword_off = offset / 4;
-      unsigned new_bytes = MIN2(4 - (offset % 4), bytes);
-      operands[i].op = Operand(split->definitions[dword_off++].getTemp());
-      operands[i].offset = offset % 4;
+      unsigned byte_off = offset % 4;
+      unsigned new_bytes = std::min(4u - byte_off, bytes);
+      operands[i].op = Operand(split->definitions[dword_off].getTemp());
+      operands[i].offset = byte_off;
       operands[i].bytes = new_bytes;
-      if (new_bytes != bytes) {
-         i++;
-         operands.insert(
-            std::next(operands.begin(), i),
-            {Operand(split->definitions[dword_off++].getTemp()), 0, bytes - new_bytes});
+
+      bytes -= new_bytes;
+      offset += new_bytes;
+      if (bytes > 0) {
+         dword_off = offset / 4;
+         byte_off = offset % 4;
+         operands.insert(operands.begin() + i + 1,
+                         {Operand(split->definitions[dword_off].getTemp()), byte_off, bytes});
       }
+      ++i;
 
       bld.insert(std::move(split));
    }
 
-   /* remove undef operands */
-   for (unsigned i = 0; i < operands.size(); i++) {
-      Operand op = operands[i].op;
-      unsigned bytes = operands[i].bytes;
-      if (!op.isUndefined())
-         continue;
-
-      if (i != operands.size() - 1) {
-         unsigned offset = operands[i + 1].offset;
-         operands[i + 1].offset -= MIN2(offset, bytes);
-         bytes -= MIN2(offset, bytes);
-      }
-
-      if (i != 0) {
-         unsigned rem = 4 - (operands[i - 1].bytes + operands[i - 1].offset);
-         operands[i - 1].bytes += MIN2(rem, bytes);
-         bytes -= MIN2(rem, bytes);
-      }
+   /* Remove undef operands. */
+   std::erase_if(operands, [](const op_info& info) { return info.op.isUndefined(); });
 
-      if (bytes == 0) {
-         operands.erase(std::next(operands.begin(), i));
-         i--;
-      } else {
-         operands[i].op = Operand::c32(0);
-         operands[i].bytes = bytes;
+   /* Combine adjacent constant operands. */
+   bool folded;
+   do {
+      folded = false;
+      for (unsigned i = 1; i < operands.size(); i++) {
+         if (!operands[i].op.isConstant() || !operands[i - 1].op.isConstant() ||
+             operands[i].offset != 0 || operands[i - 1].offset != 0 ||
+             operands[i - 1].bytes + operands[i].bytes > 4)
+            continue;
+
+         uint32_t prev = operands[i - 1].op.constantValue() & BITFIELD_MASK(operands[i - 1].bytes * 8);
+         uint32_t current = operands[i].op.constantValue() << (operands[i - 1].bytes * 8);
+
+         operands[i - 1].op = Operand::c32(prev | current);
+         operands[i - 1].bytes += operands[i].bytes;
+         operands.erase(operands.begin() + i);
+         folded = true;
+         break;
       }
-   }
+   } while (folded);
 
-   /* combine constant operands */
-   for (unsigned i = 1; i < operands.size(); i++) {
-      if (!operands[i].op.isConstant())
-         continue;
-      assert(operands[i].offset == 0);
-
-      if (!operands[i - 1].op.isConstant())
-         continue;
-
-      unsigned bytes = operands[i - 1].bytes;
-      uint32_t prev = operands[i - 1].op.constantValue() & BITFIELD_MASK(bytes * 8);
-      uint32_t current = operands[i].op.constantValue() << (bytes * 8);
-
-      operands[i - 1].op = Operand::c32(prev | current);
-      operands[i - 1].bytes += operands[i].bytes;
-      operands.erase(std::next(operands.begin(), i));
-      i--;
+   /* Emit final packing sequence. */
+   if (operands.empty()) {
+      bld.copy(def, Operand::c32(0));
+      return;
    }
 
    if (operands.size() == 1) {
       Operand op = operands[0].op;
       unsigned offset = operands[0].offset;
       if (offset != 0) {
-         if (op.isOfType(RegType::vgpr))
-            bld.vop2(aco_opcode::v_lshrrev_b32, def, Operand::c32(offset * 8), op);
-         else
-            bld.vop2_e64(aco_opcode::v_lshrrev_b32, def, Operand::c32(offset * 8), op);
+         bld.vop2(aco_opcode::v_lshrrev_b32, def, Operand::c32(offset * 8), op);
       } else {
          bld.copy(def, op);
       }
@@ -195,21 +212,8 @@ emit_pack(Builder& bld, Definition def,
    }
 
    Operand curr = operands[0].op;
-   unsigned shift = (4 - (operands[0].bytes + operands[0].offset)) * 8;
-   if (shift != 0) {
-      if (curr.isConstant())
-         curr = Operand::c32(curr.constantValue() << shift);
-      else if (curr.isOfType(RegType::vgpr))
-         curr = bld.vop2(aco_opcode::v_lshlrev_b32, bld.def(v1), Operand::c32(shift), curr);
-      else
-         curr = bld.sop2(aco_opcode::s_lshl_b32, bld.def(s1), bld.def(s1, scc), curr,
-                         Operand::c32(shift));
-   }
-
-   if (curr.isLiteral())
-      curr = bld.copy(bld.def(s1), curr);
-
    unsigned packed_bytes = operands[0].bytes;
+
    for (unsigned i = 1; i < operands.size(); i++) {
       Operand op = operands[i].op;
       unsigned offset = operands[i].offset;
@@ -222,23 +226,32 @@ emit_pack(Builder& bld, Definition def,
                           Operand::c32(offset * 8));
       }
 
-      if (curr.isOfType(RegType::sgpr) && (op.isOfType(RegType::sgpr) || op.isLiteral()))
+      if (curr.isOfType(RegType::sgpr) && (op.isOfType(RegType::sgpr) || op.isLiteral())) {
          op = bld.copy(bld.def(v1), op);
-      else if (op.isLiteral())
+      } else if (op.isLiteral() && !curr.isConstant()) {
          op = bld.copy(bld.def(s1), op);
+      }
+
+      Definition next = (i + 1 == operands.size()) ? def : bld.def(v1);
 
-      Definition next = i + 1 == operands.size() ? def : bld.def(v1);
-      unsigned bytes = i + 1 == operands.size() ? 4 - packed_bytes : operands[i].bytes;
-      curr = bld.vop3(aco_opcode::v_alignbyte_b32, next, op, curr, Operand::c32(bytes));
-      packed_bytes += bytes;
+      if (curr.isConstant() && op.isConstant()) {
+         uint32_t val = (op.constantValue() << (packed_bytes * 8)) | curr.constantValue();
+         curr = Operand::c32(val);
+      } else {
+         curr = bld.vop3(aco_opcode::v_alignbyte_b32, next, op, curr, Operand::c32(packed_bytes));
+      }
+      packed_bytes += operands[i].bytes;
    }
+
+   if (curr.isConstant())
+      bld.copy(def, curr);
 }
 
 void
 emit_split_vector(Builder& bld, aco_ptr<Instruction>& instr)
 {
    bool needs_lowering = false;
-   for (Definition& def : instr->definitions)
+   for (const Definition& def : instr->definitions)
       needs_lowering |= def.regClass().is_subdword();
 
    if (!needs_lowering) {
@@ -246,11 +259,20 @@ emit_split_vector(Builder& bld, aco_ptr<
       return;
    }
 
-   std::vector<op_info> operands = {{dword_op(instr->operands[0], true), 0, 0}};
+   Operand src = dword_op(instr->operands[0], true);
+   RegClass src_rc = src.regClass();
+   if (!src.isOfType(RegType::vgpr))
+      src = bld.copy(bld.def(v1), src);
+
+   unsigned current_offset_bits = 0;
    for (Definition& def : instr->definitions) {
-      operands[0].bytes = def.bytes();
-      emit_pack(bld, dword_def(bld.program, def), operands);
-      operands[0].offset += def.bytes();
+      Definition dword_def_out = dword_def(bld.program, def);
+      if (current_offset_bits == 0) {
+         bld.copy(dword_def_out, src);
+      } else {
+         bld.vop2(aco_opcode::v_lshrrev_b32, dword_def_out, Operand::c32(current_offset_bits), src);
+      }
+      current_offset_bits += def.bytes() * 8;
    }
 }
 
@@ -259,7 +281,7 @@ emit_create_vector(Builder& bld, aco_ptr
 {
    instr->definitions[0] = dword_def(bld.program, instr->definitions[0]);
    bool needs_lowering = false;
-   for (Operand& op : instr->operands)
+   for (const Operand& op : instr->operands)
       needs_lowering |= (op.hasRegClass() && op.regClass().is_subdword()) || op.bytes() < 4;
 
    if (!needs_lowering) {
@@ -282,8 +304,7 @@ process_block(Program* program, Block* b
    instructions.reserve(block->instructions.size());
 
    Builder bld(program, &instructions);
-   for (unsigned idx = 0; idx < block->instructions.size(); idx++) {
-      aco_ptr<Instruction> instr = std::move(block->instructions[idx]);
+   for (aco_ptr<Instruction>& instr : block->instructions) {
 
       if (instr->opcode == aco_opcode::p_split_vector) {
          emit_split_vector(bld, instr);
@@ -292,10 +313,18 @@ process_block(Program* program, Block* b
       } else if (instr->opcode == aco_opcode::p_extract_vector &&
                  instr->definitions[0].regClass().is_subdword()) {
          const Definition& def = instr->definitions[0];
-         unsigned offset = def.bytes() * instr->operands[1].constantValue();
-         std::vector<op_info> operands = {
-            {dword_op(instr->operands[0], true), offset, def.bytes()}};
-         emit_pack(bld, dword_def(program, def), std::move(operands));
+         Operand src = dword_op(instr->operands[0], true);
+         uint32_t index = instr->operands[1].constantValue();
+         uint32_t offset_bits = def.bytes() * index * 8;
+         uint32_t num_bits = def.bytes() * 8;
+         assert(offset_bits <= UINT32_MAX - num_bits && "Overflow in BFE param");
+         uint32_t bfe_param = (offset_bits & 0x1F) | ((num_bits & 0x1F) << 16);
+
+         if (!src.isOfType(RegType::vgpr))
+            src = bld.copy(bld.def(v1), src);
+
+         bld.vop3(aco_opcode::v_bfe_u32, dword_def(program, def), src, Operand::c32(bfe_param));
+
       } else {
          for (Definition& def : instr->definitions)
             def = dword_def(program, def);

--- a/src/amd/compiler/aco_lower_to_cssa.cpp	2025-05-30 14:11:47.936806808 +0200
+++ b/src/amd/compiler/aco_lower_to_cssa.cpp	2025-05-30 15:44:30.928163674 +0200
@@ -8,20 +8,11 @@
 #include "aco_ir.h"
 
 #include <algorithm>
-#include <map>
-#include <unordered_map>
 #include <vector>
-
-/*
- * Implements an algorithm to lower to Conventional SSA Form (CSSA).
- * After "Revisiting Out-of-SSA Translation for Correctness, CodeQuality, and Efficiency"
- * by B. Boissinot, A. Darte, F. Rastello, B. Dupont de Dinechin, C. Guillon,
- *
- * By lowering the IR to CSSA, the insertion of parallelcopies is separated from
- * the register coalescing problem. Additionally, correctness is ensured w.r.t. spilling.
- * The algorithm coalesces non-interfering phi-resources while taking value-equality
- * into account. Re-indexes the SSA-defs.
- */
+#include <unordered_map>
+#include <limits>
+#include <cassert>
+#include <cstring>
 
 namespace aco {
 namespace {
@@ -33,534 +24,1123 @@ struct copy {
    Operand op;
 };
 
-struct merge_node {
-   Operand value = Operand(); /* original value: can be an SSA-def or constant value */
-   uint32_t index = -1u;      /* index into the vector of merge sets */
-   uint32_t defined_at = -1u; /* defining block */
-
-   /* We also remember two closest equal intersecting ancestors. Because they intersect with this
-    * merge node, they must dominate it (intersection isn't possible otherwise) and have the same
-    * value (or else they would not be allowed to be in the same merge set).
-    */
-   Temp equal_anc_in = Temp();  /* within the same merge set */
-   Temp equal_anc_out = Temp(); /* from the other set we're currently trying to merge with */
+/* Single place for invalid sentinels */
+constexpr uint32_t INVALID_IDX = std::numeric_limits<uint32_t>::max();
+constexpr uint32_t MAX_BLOCK_COUNT = 65536u;  /* Reasonable limit for shader blocks */
+constexpr uint32_t MAX_TEMP_COUNT = 1048576u;  /* Reasonable limit for temp IDs */
+
+/*----------------------------------------------------------------------
+ * Node in the Location-Transfer-Graph (one per outstanding copy)
+ *----------------------------------------------------------------------*/
+struct alignas(8) ltg_node {
+   copy* cp;
+   uint32_t read_key;
+   uint32_t num_uses;
+
+   ltg_node() noexcept
+      : cp(nullptr), read_key(INVALID_IDX), num_uses(0u)
+   {}
+
+   ltg_node(copy* c, uint32_t r, uint32_t u) noexcept
+      : cp(c), read_key(r), num_uses(u)
+   {}
+};
+
+struct alignas(32) merge_node {
+   Operand value;
+   uint32_t index;
+   uint32_t defined_at;
+   Temp equal_anc_in;
+   Temp equal_anc_out;
+
+   merge_node() noexcept
+      : value(Operand()), index(INVALID_IDX), defined_at(INVALID_IDX),
+        equal_anc_in(Temp()), equal_anc_out(Temp())
+   {}
 };
 
 struct cssa_ctx {
    Program* program;
-   std::vector<std::vector<copy>> parallelcopies; /* copies per block */
-   std::vector<merge_set> merge_sets;             /* each vector is one (ordered) merge set */
-   std::unordered_map<uint32_t, merge_node> merge_node_table; /* tempid -> merge node */
+   std::vector<std::vector<copy>> parallelcopies;
+   std::vector<merge_set> merge_sets;
+   std::vector<merge_node> merge_node_table;
+
+   /* Thread-local cache for live-out queries */
+   struct live_out_cache_t {
+      struct entry {
+         uint32_t var_id;
+         uint32_t block_idx;
+         bool result;
+      };
+      static constexpr size_t CACHE_SIZE = 256u;
+      entry entries[CACHE_SIZE];
+      uint32_t next_slot;
+
+      live_out_cache_t() noexcept : next_slot(0u) {
+         std::memset(entries, 0, sizeof(entries));
+      }
+
+      bool lookup(uint32_t var_id, uint32_t block_idx, bool& result) const noexcept {
+         /* Simple linear probe with wraparound */
+         const uint32_t hash = ((var_id * 2654435761u) ^ (block_idx * 1103515245u)) % CACHE_SIZE;
+         for (uint32_t i = 0u; i < 4u; ++i) {
+            const uint32_t slot = (hash + i) % CACHE_SIZE;
+            const entry& e = entries[slot];
+            if (e.var_id == var_id && e.block_idx == block_idx && e.var_id != 0u) {
+               result = e.result;
+               return true;
+            }
+         }
+         return false;
+      }
+
+      void insert(uint32_t var_id, uint32_t block_idx, bool result) noexcept {
+         if (var_id == 0u) {
+            return;
+         }
+         const uint32_t hash = ((var_id * 2654435761u) ^ (block_idx * 1103515245u)) % CACHE_SIZE;
+         const uint32_t slot = hash % CACHE_SIZE;
+         entries[slot] = {var_id, block_idx, result};
+      }
+   };
+
+   mutable live_out_cache_t live_out_cache;
 };
 
-/* create (virtual) parallelcopies for each phi instruction and
- * already merge copy-definitions with phi-defs into merge sets */
-void
+/* -------------------------------
+ * Helper accessors with bounds checking
+ * ------------------------------- */
+
+[[nodiscard]] static inline const merge_node&
+get_node_const(const cssa_ctx& ctx, uint32_t id) noexcept
+{
+   static const merge_node sentinel;
+   if (id >= ctx.merge_node_table.size()) {
+      assert(!"merge_node_table out of bounds access");
+      return sentinel;
+   }
+   return ctx.merge_node_table[id];
+}
+
+[[nodiscard]] static inline merge_node&
+get_node(cssa_ctx& ctx, uint32_t id)
+{
+   if (id >= MAX_TEMP_COUNT) {
+      assert(!"Temp ID exceeds maximum allowed");
+      static merge_node sentinel;
+      return sentinel;
+   }
+
+   if (id >= ctx.merge_node_table.size()) {
+      /* Grow by power of 2 */
+      size_t new_size = ctx.merge_node_table.empty() ? 64u : ctx.merge_node_table.size();
+      while (new_size <= id && new_size < MAX_TEMP_COUNT) {
+         new_size = new_size * 2u;
+      }
+      new_size = std::min<size_t>(new_size, MAX_TEMP_COUNT);
+      ctx.merge_node_table.resize(new_size);
+   }
+
+   return ctx.merge_node_table[id];
+}
+
+/* -------------------------------
+ * Optimized rename map with path compression
+ * ------------------------------- */
+
+class rename_map_t {
+private:
+   std::unordered_map<uint32_t, Operand> map_;
+   mutable std::unordered_map<uint32_t, Operand> cache_;
+   static constexpr uint32_t MAX_PATH_LENGTH = 256u;
+
+public:
+   [[nodiscard]] bool has(uint32_t id) const noexcept {
+      return map_.find(id) != map_.end();
+   }
+
+   void set(uint32_t id, const Operand& op) {
+      /* Don't store self-mappings */
+      if (op.isTemp() && op.getTemp().id() == id) {
+         return;
+      }
+      map_[id] = op;
+      cache_.erase(id);
+   }
+
+   [[nodiscard]] Operand resolve(uint32_t id) const {
+      /* Check cache first */
+      auto cache_it = cache_.find(id);
+      if (cache_it != cache_.end()) {
+         return cache_it->second;
+      }
+
+      auto it = map_.find(id);
+      if (it == map_.end()) {
+         return Operand();
+      }
+
+      Operand cur = it->second;
+      if (!cur.isTemp()) {
+         cache_[id] = cur;
+         return cur;
+      }
+
+      /* Path compression with cycle detection using Floyd's algorithm */
+      std::vector<uint32_t> path;
+      path.reserve(16u);
+
+      uint32_t slow = id;
+      uint32_t fast = cur.getTemp().id();
+      bool has_cycle = false;
+
+      /* Detect cycle */
+      for (uint32_t steps = 0u; steps < MAX_PATH_LENGTH; ++steps) {
+         auto slow_it = map_.find(slow);
+         if (slow_it == map_.end() || !slow_it->second.isTemp()) {
+            break;
+         }
+         slow = slow_it->second.getTemp().id();
+
+         /* Fast pointer moves twice */
+         for (int i = 0; i < 2; ++i) {
+            auto fast_it = map_.find(fast);
+            if (fast_it == map_.end() || !fast_it->second.isTemp()) {
+               goto end_cycle_detection;
+            }
+            fast = fast_it->second.getTemp().id();
+         }
+
+         if (slow == fast) {
+            has_cycle = true;
+            break;
+         }
+      }
+      end_cycle_detection:
+
+      if (has_cycle) {
+         /* Return without caching to avoid infinite loop */
+         return cur;
+      }
+
+      /* Walk path and compress */
+      uint32_t cur_id = cur.getTemp().id();
+      path.push_back(id);
+
+      for (uint32_t steps = 0u; steps < MAX_PATH_LENGTH; ++steps) {
+         auto next_it = map_.find(cur_id);
+         if (next_it == map_.end()) {
+            break;
+         }
+
+         path.push_back(cur_id);
+
+         if (!next_it->second.isTemp()) {
+            cur = next_it->second;
+            break;
+         }
+
+         cur = next_it->second;
+         cur_id = cur.getTemp().id();
+      }
+
+      /* Update cache for all nodes in path */
+      for (uint32_t node_id : path) {
+         cache_[node_id] = cur;
+      }
+
+      return cur;
+   }
+
+   void clear() noexcept {
+      map_.clear();
+      cache_.clear();
+   }
+};
+
+/* -------------------------------
+ * Parallelcopy collection
+ * ------------------------------- */
+
+static void
 collect_parallelcopies(cssa_ctx& ctx)
 {
-   ctx.parallelcopies.resize(ctx.program->blocks.size());
+   const size_t num_blocks = ctx.program->blocks.size();
+
+   if (num_blocks > MAX_BLOCK_COUNT) {
+      assert(!"Block count exceeds maximum");
+      return;
+   }
+
+   ctx.parallelcopies.resize(num_blocks);
+   ctx.merge_sets.reserve(num_blocks * 2u);
+
+   /* Count temps needed */
+   uint32_t max_temp_id = ctx.program->temp_rc.size();
+   uint32_t extra_temps = 0u;
+
+   for (const Block& block : ctx.program->blocks) {
+      for (const aco_ptr<Instruction>& phi : block.instructions) {
+         if (phi->opcode != aco_opcode::p_phi &&
+             phi->opcode != aco_opcode::p_linear_phi) {
+            break;
+         }
+
+         const Definition& def = phi->definitions[0];
+         if (!def.isTemp() || def.isKill()) {
+            continue;
+         }
+
+         for (const Operand& op : phi->operands) {
+            if (!op.isUndefined()) {
+               if (extra_temps < MAX_TEMP_COUNT - 1u) {
+                  extra_temps++;
+               }
+            }
+         }
+      }
+   }
+
+   /* Allocate merge node table with bounds checking */
+   const uint32_t total_temps = std::min<uint32_t>(
+      max_temp_id + extra_temps + 256u,
+      MAX_TEMP_COUNT
+   );
+
+   size_t table_size = 64u;
+   while (table_size < total_temps && table_size < MAX_TEMP_COUNT) {
+      table_size = table_size * 2u;
+   }
+   table_size = std::min<size_t>(table_size, MAX_TEMP_COUNT);
+
+   ctx.merge_node_table.reserve(table_size);
+   ctx.merge_node_table.resize(table_size);
+
    Builder bld(ctx.program);
+
+   /* Create parallel copies */
    for (Block& block : ctx.program->blocks) {
+      const uint32_t block_idx = block.index;
+
+      if (block_idx >= MAX_BLOCK_COUNT) {
+         continue;
+      }
+
       for (aco_ptr<Instruction>& phi : block.instructions) {
-         if (phi->opcode != aco_opcode::p_phi && phi->opcode != aco_opcode::p_linear_phi)
+         if (phi->opcode != aco_opcode::p_phi &&
+             phi->opcode != aco_opcode::p_linear_phi) {
             break;
+         }
 
          const Definition& def = phi->definitions[0];
-
-         /* if the definition is not temp, it is the exec mask.
-          * We can reload the exec mask directly from the spill slot.
-          */
-         if (!def.isTemp() || def.isKill())
+         if (!def.isTemp() || def.isKill()) {
             continue;
+         }
+
+         const Block::edge_vec& preds = (phi->opcode == aco_opcode::p_phi)
+                                       ? block.logical_preds
+                                       : block.linear_preds;
 
-         Block::edge_vec& preds =
-            phi->opcode == aco_opcode::p_phi ? block.logical_preds : block.linear_preds;
-         uint32_t index = ctx.merge_sets.size();
+         const uint32_t set_idx = ctx.merge_sets.size();
          merge_set set;
+         set.reserve(phi->operands.size() + 1u);
 
          bool has_preheader_copy = false;
-         for (unsigned i = 0; i < phi->operands.size(); i++) {
+         const bool is_loop_header = (block.kind & block_kind_loop_header) != 0u;
+
+         for (unsigned i = 0u; i < phi->operands.size(); ++i) {
             Operand op = phi->operands[i];
-            if (op.isUndefined())
+            if (op.isUndefined()) {
                continue;
+            }
 
+            /* Check if constant can be inlined */
             if (def.regClass().type() == RegType::sgpr && !op.isTemp()) {
-               /* SGPR inline constants and literals on GFX10+ can be spilled
-                * and reloaded directly (without intermediate register) */
                if (op.isConstant()) {
-                  if (ctx.program->gfx_level >= GFX10)
-                     continue;
-                  if (op.size() == 1 && !op.isLiteral())
-                     continue;
+                  if (ctx.program->gfx_level >= GFX10) {
+                     if (op.size() == 1u && !op.isLiteral()) {
+                        continue;
+                     }
+                  } else {
+                     const int64_t val = op.constantValue();
+                     const bool can_inline = op.isLiteral() ||
+                                            (op.size() == 1u && val >= -16 && val <= 64);
+                     if (can_inline) {
+                        continue;
+                     }
+                  }
                } else {
                   assert(op.isFixed() && op.physReg() == exec);
                   continue;
                }
             }
 
-            /* create new temporary and rename operands */
+            /* Validate predecessor index */
+            if (i >= preds.size()) {
+               assert(!"Phi operand count mismatch");
+               continue;
+            }
+
+            const uint32_t pred_idx = preds[i];
+            if (pred_idx >= ctx.parallelcopies.size()) {
+               assert(!"Invalid predecessor index");
+               continue;
+            }
+
+            /* Create copy */
+            if (ctx.parallelcopies[pred_idx].empty()) {
+               ctx.parallelcopies[pred_idx].reserve(8u);
+            }
+
             Temp tmp = bld.tmp(def.regClass());
-            ctx.parallelcopies[preds[i]].emplace_back(copy{Definition(tmp), op});
+
+            /* Validate temp ID */
+            if (tmp.id() >= MAX_TEMP_COUNT) {
+               assert(!"Temp ID exceeds maximum");
+               continue;
+            }
+
+            ctx.parallelcopies[pred_idx].push_back({Definition(tmp), op});
+
             phi->operands[i] = Operand(tmp);
             phi->operands[i].setKill(true);
 
-            /* place the new operands in the same merge set */
             set.emplace_back(tmp);
-            ctx.merge_node_table[tmp.id()] = {op, index, preds[i]};
 
-            has_preheader_copy |= i == 0 && block.kind & block_kind_loop_header;
+            /* Store merge node */
+            merge_node& node = get_node(ctx, tmp.id());
+            node.value = op;
+            node.index = set_idx;
+            node.defined_at = pred_idx;
+
+            has_preheader_copy |= (i == 0u && is_loop_header);
          }
 
-         if (set.empty())
+         if (set.empty()) {
             continue;
+         }
+
+         /* Insert definition temp */
+         if (has_preheader_copy) {
+            set.emplace(std::next(set.begin()), def.getTemp());
+         } else if (is_loop_header) {
+            set.emplace(set.begin(), def.getTemp());
+         } else {
+            set.emplace_back(def.getTemp());
+         }
 
-         /* place the definition in dominance-order */
-         if (def.isTemp()) {
-            if (has_preheader_copy)
-               set.emplace(std::next(set.begin()), def.getTemp());
-            else if (block.kind & block_kind_loop_header)
-               set.emplace(set.begin(), def.getTemp());
-            else
-               set.emplace_back(def.getTemp());
-            ctx.merge_node_table[def.tempId()] = {Operand(def.getTemp()), index, block.index};
+         /* Store merge node for definition */
+         if (def.tempId() < MAX_TEMP_COUNT) {
+            merge_node& node = get_node(ctx, def.tempId());
+            node.value = Operand(def.getTemp());
+            node.index = set_idx;
+            node.defined_at = block_idx;
          }
-         ctx.merge_sets.emplace_back(set);
+
+         ctx.merge_sets.emplace_back(std::move(set));
       }
    }
 }
 
-/* check whether the definition of a comes after b. */
-inline bool
-defined_after(cssa_ctx& ctx, Temp a, Temp b)
+/* -------------------------------
+ * Utility predicates
+ * ------------------------------- */
+
+[[nodiscard]] static inline bool
+defined_after(const cssa_ctx& ctx, Temp a, Temp b) noexcept
 {
-   merge_node& node_a = ctx.merge_node_table[a.id()];
-   merge_node& node_b = ctx.merge_node_table[b.id()];
-   if (node_a.defined_at == node_b.defined_at)
+   if (a.id() >= ctx.merge_node_table.size() ||
+       b.id() >= ctx.merge_node_table.size()) {
       return a.id() > b.id();
+   }
 
-   return node_a.defined_at > node_b.defined_at;
+   const merge_node& node_a = ctx.merge_node_table[a.id()];
+   const merge_node& node_b = ctx.merge_node_table[b.id()];
+
+   if (node_a.defined_at == INVALID_IDX || node_b.defined_at == INVALID_IDX) {
+      return a.id() > b.id();
+   }
+
+   return (node_a.defined_at == node_b.defined_at)
+          ? (a.id() > b.id())
+          : (node_a.defined_at > node_b.defined_at);
 }
 
-/* check whether a dominates b where b is defined after a */
-inline bool
-dominates(cssa_ctx& ctx, Temp a, Temp b)
+[[nodiscard]] static inline bool
+dominates(const cssa_ctx& ctx, Temp a, Temp b) noexcept
 {
    assert(defined_after(ctx, b, a));
-   Block& parent = ctx.program->blocks[ctx.merge_node_table[a.id()].defined_at];
-   Block& child = ctx.program->blocks[ctx.merge_node_table[b.id()].defined_at];
-   if (b.regClass().type() == RegType::vgpr)
-      return dominates_logical(parent, child);
-   else
-      return dominates_linear(parent, child);
-}
-
-/* Checks whether some variable is live-out, not considering any phi-uses. */
-inline bool
-is_live_out(cssa_ctx& ctx, Temp var, uint32_t block_idx)
-{
-   Block::edge_vec& succs = var.is_linear() ? ctx.program->blocks[block_idx].linear_succs
-                                            : ctx.program->blocks[block_idx].logical_succs;
-
-   return std::any_of(succs.begin(), succs.end(), [&](unsigned succ)
-                      { return ctx.program->live.live_in[succ].count(var.id()); });
-}
-
-/* check intersection between var and parent:
- * We already know that parent dominates var. */
-inline bool
-intersects(cssa_ctx& ctx, Temp var, Temp parent)
-{
-   merge_node& node_var = ctx.merge_node_table[var.id()];
-   merge_node& node_parent = ctx.merge_node_table[parent.id()];
-   assert(node_var.index != node_parent.index);
-   uint32_t block_idx = node_var.defined_at;
-
-   /* if parent is defined in a different block than var */
-   if (node_parent.defined_at < node_var.defined_at) {
-      /* if the parent is not live-in, they don't interfere */
-      if (!ctx.program->live.live_in[block_idx].count(parent.id()))
-         return false;
+
+   const merge_node& node_a = get_node_const(ctx, a.id());
+   const merge_node& node_b = get_node_const(ctx, b.id());
+
+   if (node_a.defined_at == INVALID_IDX ||
+       node_b.defined_at == INVALID_IDX ||
+       node_a.defined_at >= ctx.program->blocks.size() ||
+       node_b.defined_at >= ctx.program->blocks.size()) {
+      return false;
    }
 
-   /* if the parent is live-out at the definition block of var, they intersect */
-   bool parent_live = is_live_out(ctx, parent, block_idx);
-   if (parent_live)
+   const Block& parent = ctx.program->blocks[node_a.defined_at];
+   const Block& child = ctx.program->blocks[node_b.defined_at];
+
+   return (b.regClass().type() == RegType::vgpr)
+          ? dominates_logical(parent, child)
+          : dominates_linear(parent, child);
+}
+
+[[nodiscard]] static bool
+is_live_out(const cssa_ctx& ctx, Temp var, uint32_t block_idx) noexcept
+{
+   if (block_idx >= ctx.program->blocks.size()) {
+      return false;
+   }
+
+   /* Check cache */
+   bool cached_result;
+   if (ctx.live_out_cache.lookup(var.id(), block_idx, cached_result)) {
+      return cached_result;
+   }
+
+   /* Compute result */
+   const Block& block = ctx.program->blocks[block_idx];
+   const Block::edge_vec& succs = var.is_linear()
+                                 ? block.linear_succs
+                                 : block.logical_succs;
+
+   bool result = false;
+   for (unsigned succ_idx : succs) {
+      if (succ_idx < ctx.program->live.live_in.size() &&
+          ctx.program->live.live_in[succ_idx].count(var.id())) {
+         result = true;
+         break;
+      }
+   }
+
+   /* Update cache */
+   ctx.live_out_cache.insert(var.id(), block_idx, result);
+   return result;
+}
+
+[[nodiscard]] static bool
+intersects(const cssa_ctx& ctx, Temp var, Temp parent) noexcept
+{
+   const merge_node& node_var = get_node_const(ctx, var.id());
+   const uint32_t block_idx = node_var.defined_at;
+
+   if (block_idx == INVALID_IDX || block_idx >= ctx.program->blocks.size()) {
+      return false;
+   }
+
+   const merge_node& node_parent = get_node_const(ctx, parent.id());
+
+   /* Early exit if parent not live at var's definition */
+   if (node_parent.defined_at < node_var.defined_at &&
+       block_idx < ctx.program->live.live_in.size() &&
+       !ctx.program->live.live_in[block_idx].count(parent.id())) {
+      return false;
+   }
+
+   if (is_live_out(ctx, parent, block_idx)) {
       return true;
+   }
 
-   for (const copy& cp : ctx.parallelcopies[block_idx]) {
-      /* if var is defined at the edge, they don't intersect */
-      if (cp.def.getTemp() == var)
-         return false;
-      if (cp.op.isTemp() && cp.op.getTemp() == parent)
-         parent_live = true;
+   /* Check parallel copies */
+   bool parent_live = false;
+   if (block_idx < ctx.parallelcopies.size()) {
+      for (const copy& cp : ctx.parallelcopies[block_idx]) {
+         if (cp.def.getTemp() == var) {
+            return false;
+         }
+         if (cp.op.isTemp() && cp.op.getTemp() == parent) {
+            parent_live = true;
+         }
+      }
    }
-   /* if the parent is live at the edge, they intersect */
-   if (parent_live)
+
+   if (parent_live) {
       return true;
+   }
 
-   /* both, parent and var, are present in the same block */
+   /* Check instructions */
    const Block& block = ctx.program->blocks[block_idx];
    for (auto it = block.instructions.crbegin(); it != block.instructions.crend(); ++it) {
-      /* if the parent was not encountered yet, it can only be used by a phi */
-      if (is_phi(it->get()))
+      if (is_phi(it->get())) {
          break;
+      }
 
       for (const Definition& def : (*it)->definitions) {
-         if (!def.isTemp())
-            continue;
-         /* if parent was not found yet, they don't intersect */
-         if (def.getTemp() == var)
+         if (def.isTemp() && def.getTemp() == var) {
             return false;
+         }
       }
 
       for (const Operand& op : (*it)->operands) {
-         if (!op.isTemp())
-            continue;
-         /* if the var was defined before this point, they intersect */
-         if (op.getTemp() == parent)
+         if (op.isTemp() && op.getTemp() == parent) {
             return true;
+         }
       }
    }
 
    return false;
 }
 
-/* check interference between var and parent:
- * i.e. they have different values and intersect.
- * If parent and var intersect and share the same value, also updates the equal ancestor. */
-inline bool
-interference(cssa_ctx& ctx, Temp var, Temp parent)
+[[nodiscard]] static bool
+interference(cssa_ctx& ctx, Temp var, Temp parent) noexcept
 {
    assert(var != parent);
-   merge_node& node_var = ctx.merge_node_table[var.id()];
+
+   merge_node& node_var = get_node(ctx, var.id());
    node_var.equal_anc_out = Temp();
 
-   if (node_var.index == ctx.merge_node_table[parent.id()].index) {
-      /* Check/update in other set. equal_anc_out is only present if it intersects with 'parent',
-       * but that's fine since it has to for it to intersect with 'var'. */
-      parent = ctx.merge_node_table[parent.id()].equal_anc_out;
-   }
+   const merge_node& node_parent = get_node_const(ctx, parent.id());
 
-   Temp tmp = parent;
-   /* Check if 'var' intersects with 'parent' or any ancestors which might intersect too. */
-   while (tmp != Temp() && !intersects(ctx, var, tmp)) {
-      merge_node& node_tmp = ctx.merge_node_table[tmp.id()];
-      tmp = node_tmp.equal_anc_in;
+   if (node_var.index == node_parent.index) {
+      parent = node_parent.equal_anc_out;
    }
 
-   /* no intersection found */
-   if (tmp == Temp())
-      return false;
+   /* Limit iteration count to prevent infinite loops */
+   for (uint32_t iter = 0u; iter < 256u && parent != Temp(); ++iter) {
+      const merge_node& node_tmp = get_node_const(ctx, parent.id());
 
-   /* var and parent, same value and intersect, but in different sets */
-   if (node_var.value == ctx.merge_node_table[parent.id()].value) {
-      node_var.equal_anc_out = tmp;
-      return false;
+      if (!intersects(ctx, var, parent)) {
+         parent = node_tmp.equal_anc_in;
+         continue;
+      }
+
+      if (node_var.value == node_tmp.value) {
+         node_var.equal_anc_out = parent;
+         return false;
+      }
+      return true;
    }
 
-   /* var and parent, different values and intersect */
-   return true;
+   return false;
 }
 
-/* tries to merge set_b into set_a of given temporary and
- * drops that temporary as it is being coalesced */
-bool
+/* -------------------------------
+ * Merge set operations
+ * ------------------------------- */
+
+static bool
 try_merge_merge_set(cssa_ctx& ctx, Temp dst, merge_set& set_b)
 {
-   auto def_node_it = ctx.merge_node_table.find(dst.id());
-   uint32_t index = def_node_it->second.index;
+   const uint32_t index = get_node_const(ctx, dst.id()).index;
+
+   if (index == INVALID_IDX || index >= ctx.merge_sets.size()) {
+      return false;
+   }
+
    merge_set& set_a = ctx.merge_sets[index];
-   std::vector<Temp> dom; /* stack of the traversal */
-   merge_set union_set;   /* the new merged merge-set */
-   uint32_t i_a = 0;
-   uint32_t i_b = 0;
+
+   /* Build union while checking dominance */
+   std::vector<Temp> dom_stack;
+   dom_stack.reserve(16u);
+
+   std::vector<Temp> union_set;
+   union_set.reserve(set_a.size() + set_b.size());
+
+   size_t i_a = 0u;
+   size_t i_b = 0u;
 
    while (i_a < set_a.size() || i_b < set_b.size()) {
-      Temp current;
-      if (i_a == set_a.size())
-         current = set_b[i_b++];
-      else if (i_b == set_b.size())
-         current = set_a[i_a++];
-      /* else pick the one defined first */
-      else if (defined_after(ctx, set_a[i_a], set_b[i_b]))
-         current = set_b[i_b++];
-      else
-         current = set_a[i_a++];
-
-      while (!dom.empty() && !dominates(ctx, dom.back(), current))
-         dom.pop_back(); /* not the desired parent, remove */
-
-      if (!dom.empty() && interference(ctx, current, dom.back())) {
-         for (Temp t : union_set)
-            ctx.merge_node_table[t.id()].equal_anc_out = Temp();
-         return false; /* intersection detected */
-      }
-
-      dom.emplace_back(current); /* otherwise, keep checking */
-      if (current != dst)
-         union_set.emplace_back(current); /* maintain the new merge-set sorted */
-   }
-
-   /* update hashmap */
-   for (Temp t : union_set) {
-      merge_node& node = ctx.merge_node_table[t.id()];
-      /* update the equal ancestors:
-       * i.e. the 'closest' dominating def which intersects */
-      Temp in = node.equal_anc_in;
-      Temp out = node.equal_anc_out;
-      if (in == Temp() || (out != Temp() && defined_after(ctx, out, in)))
-         node.equal_anc_in = out;
+      Temp cur;
+
+      if (i_a == set_a.size()) {
+         cur = set_b[i_b++];
+      } else if (i_b == set_b.size()) {
+         cur = set_a[i_a++];
+      } else if (defined_after(ctx, set_a[i_a], set_b[i_b])) {
+         cur = set_b[i_b++];
+      } else {
+         cur = set_a[i_a++];
+      }
+
+      /* Pop non-dominators */
+      while (!dom_stack.empty() && !dominates(ctx, dom_stack.back(), cur)) {
+         dom_stack.pop_back();
+      }
+
+      /* Check interference */
+      if (!dom_stack.empty() && interference(ctx, cur, dom_stack.back())) {
+         /* Rollback */
+         for (const Temp& t : union_set) {
+            merge_node& node = get_node(ctx, t.id());
+            node.equal_anc_out = Temp();
+         }
+         return false;
+      }
+
+      dom_stack.push_back(cur);
+
+      if (cur != dst) {
+         union_set.push_back(cur);
+      }
+   }
+
+   /* Update nodes */
+   for (const Temp& t : union_set) {
+      merge_node& node = get_node(ctx, t.id());
+
+      if (node.equal_anc_in == Temp() ||
+          (node.equal_anc_out != Temp() &&
+           defined_after(ctx, node.equal_anc_out, node.equal_anc_in))) {
+         node.equal_anc_in = node.equal_anc_out;
+      }
+
       node.equal_anc_out = Temp();
-      /* update merge-set index */
       node.index = index;
    }
-   set_b = merge_set(); /* free the old set_b */
-   ctx.merge_sets[index] = union_set;
-   ctx.merge_node_table.erase(dst.id()); /* remove the temporary */
 
+   set_b.clear();
+   ctx.merge_sets[index] = std::move(union_set);
    return true;
 }
 
-/* returns true if the copy can safely be omitted */
-bool
-try_coalesce_copy(cssa_ctx& ctx, copy copy, uint32_t block_idx)
+static bool
+try_coalesce_copy(cssa_ctx& ctx, copy cp, uint32_t block_idx)
 {
-   /* we can only coalesce temporaries */
-   if (!copy.op.isTemp() || !copy.op.isKill())
+   if (!cp.op.isTemp() || !cp.op.isKill()) {
+      return false;
+   }
+
+   if (cp.op.regClass() != cp.def.regClass()) {
       return false;
+   }
 
-   /* we can only coalesce copies of the same register class */
-   if (copy.op.regClass() != copy.def.regClass())
+   if (cp.op.tempId() >= MAX_TEMP_COUNT || cp.def.tempId() >= MAX_TEMP_COUNT) {
       return false;
+   }
+
+   merge_node& op_node = get_node(ctx, cp.op.tempId());
 
-   /* try emplace a merge_node for the copy operand */
-   merge_node& op_node = ctx.merge_node_table[copy.op.tempId()];
-   if (op_node.defined_at == -1u) {
-      /* find defining block of operand */
-      while (ctx.program->live.live_in[block_idx].count(copy.op.tempId()))
-         block_idx = copy.op.regClass().type() == RegType::vgpr
-                        ? ctx.program->blocks[block_idx].logical_idom
-                        : ctx.program->blocks[block_idx].linear_idom;
-      op_node.defined_at = block_idx;
-      op_node.value = copy.op;
-   }
-
-   /* check if this operand has not yet been coalesced */
-   if (op_node.index == -1u) {
-      merge_set op_set = merge_set{copy.op.getTemp()};
-      return try_merge_merge_set(ctx, copy.def.getTemp(), op_set);
-   }
-
-   /* check if this operand has been coalesced into the same set */
-   assert(ctx.merge_node_table.count(copy.def.tempId()));
-   if (op_node.index == ctx.merge_node_table[copy.def.tempId()].index)
+   /* Find definition block */
+   if (op_node.defined_at == INVALID_IDX) {
+      uint32_t def_block = block_idx;
+
+      while (def_block < ctx.program->blocks.size() &&
+             ctx.program->live.live_in[def_block].count(cp.op.tempId())) {
+         const Block& block = ctx.program->blocks[def_block];
+         const uint32_t idom = (cp.op.regClass().type() == RegType::vgpr)
+                             ? block.logical_idom
+                             : block.linear_idom;
+
+         if (idom == def_block || idom >= ctx.program->blocks.size()) {
+            break;
+         }
+         def_block = idom;
+      }
+
+      op_node.defined_at = def_block;
+      op_node.value = cp.op;
+   }
+
+   /* Try merge */
+   if (op_node.index == INVALID_IDX) {
+      merge_set singleton{cp.op.getTemp()};
+      return try_merge_merge_set(ctx, cp.def.getTemp(), singleton);
+   }
+
+   const uint32_t def_idx = get_node_const(ctx, cp.def.tempId()).index;
+   if (op_node.index == def_idx) {
       return true;
+   }
 
-   /* otherwise, try to coalesce both merge sets */
-   return try_merge_merge_set(ctx, copy.def.getTemp(), ctx.merge_sets[op_node.index]);
-}
+   if (op_node.index >= ctx.merge_sets.size()) {
+      return false;
+   }
 
-/* node in the location-transfer-graph */
-struct ltg_node {
-   copy* cp;
-   uint32_t read_idx;
-   uint32_t num_uses = 0;
-};
+   return try_merge_merge_set(ctx, cp.def.getTemp(), ctx.merge_sets[op_node.index]);
+}
 
-/* emit the copies in an order that does not
- * create interferences within a merge-set */
-void
-emit_copies_block(Builder& bld, std::map<uint32_t, ltg_node>& ltg, RegType type)
+/* -------------------------------
+ * Copy emission
+ * ------------------------------- */
+
+static void
+emit_copies_block(Builder& bld,
+                  std::vector<ltg_node>& ltg,
+                  std::vector<uint8_t>& ltg_valid,
+                  const std::vector<uint32_t>& active_keys,
+                  RegType type,
+                  rename_map_t& rename_map)
 {
+   if (active_keys.empty()) {
+      return;
+   }
+
    RegisterDemand live_changes;
-   RegisterDemand reg_demand = bld.it->get()->register_demand - get_temp_registers(bld.it->get()) -
+   RegisterDemand reg_demand = bld.it->get()->register_demand -
+                               get_temp_registers(bld.it->get()) -
                                get_live_changes(bld.it->get());
-   auto&& it = ltg.begin();
-   while (it != ltg.end()) {
-      copy& cp = *it->second.cp;
-
-      /* wrong regclass or still needed as operand */
-      if (cp.def.regClass().type() != type || it->second.num_uses > 0) {
-         ++it;
+
+   /* Count uses */
+   std::unordered_map<uint32_t, uint32_t> remaining_uses;
+   remaining_uses.reserve(active_keys.size());
+
+   for (uint32_t key : active_keys) {
+      if (key >= ltg.size() || !ltg_valid[key]) {
          continue;
       }
 
-      /* update the location transfer graph */
-      if (it->second.read_idx != -1u) {
-         auto&& other = ltg.find(it->second.read_idx);
-         if (other != ltg.end())
-            other->second.num_uses--;
-      }
-      ltg.erase(it);
-
-      /* Remove the kill flag if we still need this operand for other copies. */
-      if (cp.op.isKill() && std::any_of(ltg.begin(), ltg.end(),
-                                        [&](auto& other) { return other.second.cp->op == cp.op; }))
-         cp.op.setKill(false);
-
-      /* emit the copy */
-      Instruction* instr = bld.copy(cp.def, cp.op);
-      live_changes += get_live_changes(instr);
-      RegisterDemand temps = get_temp_registers(instr);
-      instr->register_demand = reg_demand + live_changes + temps;
-
-      it = ltg.begin();
-   }
-
-   /* count the number of remaining circular dependencies */
-   unsigned num = std::count_if(
-      ltg.begin(), ltg.end(), [&](auto& n) { return n.second.cp->def.regClass().type() == type; });
-
-   /* if there are circular dependencies, we just emit them as single parallelcopy */
-   if (num) {
-      // TODO: this should be restricted to a feasible number of registers
-      // and otherwise use a temporary to avoid having to reload more (spilled)
-      // variables than we have registers.
-      aco_ptr<Instruction> copy{
-         create_instruction(aco_opcode::p_parallelcopy, Format::PSEUDO, num, num)};
-      it = ltg.begin();
-      for (unsigned i = 0; i < num; i++) {
-         while (it->second.cp->def.regClass().type() != type)
-            ++it;
-
-         copy->definitions[i] = it->second.cp->def;
-         copy->operands[i] = it->second.cp->op;
-         it = ltg.erase(it);
-      }
-      live_changes += get_live_changes(copy.get());
-      RegisterDemand temps = get_temp_registers(copy.get());
-      copy->register_demand = reg_demand + live_changes + temps;
-      bld.insert(std::move(copy));
-   }
-
-   /* Update RegisterDemand after inserted copies */
-   for (auto instr_it = bld.it; instr_it != bld.instructions->end(); ++instr_it) {
-      instr_it->get()->register_demand += live_changes;
+      const ltg_node& node = ltg[key];
+      if (node.cp && node.cp->op.isTemp()) {
+         remaining_uses[node.cp->op.tempId()]++;
+      }
+   }
+
+   /* Lambda for last use check */
+   auto is_last_use = [&remaining_uses](uint32_t temp_id) -> bool {
+      auto it = remaining_uses.find(temp_id);
+      if (it == remaining_uses.end()) {
+         return true;
+      }
+      const bool last = (it->second == 1u);
+      if (last) {
+         remaining_uses.erase(it);
+      } else {
+         --it->second;
+      }
+      return last;
+   };
+
+   /* Process ready nodes */
+   std::vector<uint32_t> worklist;
+   worklist.reserve(active_keys.size());
+
+   for (uint32_t key : active_keys) {
+      if (key >= ltg.size() || !ltg_valid[key]) {
+         continue;
+      }
+
+      const ltg_node& node = ltg[key];
+      if (node.cp && node.cp->def.regClass().type() == type && node.num_uses == 0u) {
+         worklist.push_back(key);
+      }
+   }
+
+   while (!worklist.empty()) {
+      const uint32_t write_key = worklist.back();
+      worklist.pop_back();
+
+      if (write_key >= ltg.size() || !ltg_valid[write_key]) {
+         continue;
+      }
+
+      ltg_node node = ltg[write_key];
+      ltg_valid[write_key] = 0u;
+
+      if (!node.cp) {
+         continue;
+      }
+
+      Operand src = node.cp->op;
+
+      /* Resolve rename */
+      if (src.isTemp()) {
+         Operand resolved = rename_map.resolve(src.getTemp().id());
+         if (!resolved.isUndefined()) {
+            src = resolved;
+         }
+
+         if (src.isTemp() && src.isKill()) {
+            src.setKill(is_last_use(src.getTemp().id()));
+         }
+      }
+
+      /* Update dependencies */
+      if (node.read_key != INVALID_IDX && node.read_key < ltg.size() && ltg_valid[node.read_key]) {
+         ltg_node& read_node = ltg[node.read_key];
+         if (read_node.num_uses > 0u) {
+            --read_node.num_uses;
+            if (read_node.num_uses == 0u && read_node.cp &&
+                read_node.cp->def.regClass().type() == type) {
+               worklist.push_back(node.read_key);
+            }
+         }
+      }
+
+      /* Emit copy */
+      Instruction* copy_ins = bld.copy(node.cp->def, src);
+      live_changes += get_live_changes(copy_ins);
+      copy_ins->register_demand = reg_demand + live_changes + get_temp_registers(copy_ins);
+   }
+
+   /* Handle cycles with parallel copy */
+   unsigned pc_slots = 0u;
+   for (uint32_t key : active_keys) {
+      if (key < ltg.size() && ltg_valid[key] && ltg[key].cp &&
+          ltg[key].cp->def.regClass().type() == type) {
+         pc_slots++;
+      }
+   }
+
+   if (pc_slots > 0u) {
+      aco_ptr<Instruction> pc{
+         create_instruction(aco_opcode::p_parallelcopy, Format::PSEUDO, pc_slots, pc_slots)
+      };
+
+      unsigned slot = 0u;
+      for (uint32_t key : active_keys) {
+         if (key >= ltg.size() || !ltg_valid[key]) {
+            continue;
+         }
+
+         ltg_node& node = ltg[key];
+         if (!node.cp || node.cp->def.regClass().type() != type) {
+            continue;
+         }
+
+         pc->definitions[slot] = node.cp->def;
+
+         Operand src = node.cp->op;
+         if (src.isTemp()) {
+            Operand resolved = rename_map.resolve(src.getTemp().id());
+            if (!resolved.isUndefined()) {
+               src = resolved;
+            }
+
+            if (src.isTemp() && src.isKill()) {
+               src.setKill(is_last_use(src.getTemp().id()));
+            }
+         }
+
+         pc->operands[slot] = src;
+         ltg_valid[key] = 0u;
+
+         /* Update dependencies */
+         if (node.read_key != INVALID_IDX && node.read_key < ltg.size() &&
+             ltg_valid[node.read_key] && ltg[node.read_key].num_uses > 0u) {
+            --ltg[node.read_key].num_uses;
+         }
+
+         ++slot;
+      }
+
+      assert(slot == pc_slots);
+
+      live_changes += get_live_changes(pc.get());
+      pc->register_demand = reg_demand + live_changes + get_temp_registers(pc.get());
+      bld.insert(std::move(pc));
+   }
+
+   /* Update register demand */
+   if (live_changes.sgpr || live_changes.vgpr) {
+      for (auto it = bld.it; it != bld.instructions->end(); ++it) {
+         (*it)->register_demand += live_changes;
+      }
    }
 }
 
-/* either emits or coalesces all parallelcopies and
- * renames the phi-operands accordingly. */
-void
+/* -------------------------------
+ * Main parallelcopy emission
+ * ------------------------------- */
+
+static void
 emit_parallelcopies(cssa_ctx& ctx)
 {
-   std::unordered_map<uint32_t, Operand> renames;
+   rename_map_t rename_map;
+
+   /* Process blocks in reverse */
+   for (int block_idx = static_cast<int>(ctx.program->blocks.size()) - 1;
+        block_idx >= 0;
+        --block_idx) {
 
-   /* we iterate backwards to prioritize coalescing in else-blocks */
-   for (int i = ctx.program->blocks.size() - 1; i >= 0; i--) {
-      if (ctx.parallelcopies[i].empty())
+      if (static_cast<size_t>(block_idx) >= ctx.parallelcopies.size() ||
+          ctx.parallelcopies[block_idx].empty()) {
          continue;
+      }
+
+      std::vector<bool> coalesced(ctx.parallelcopies[block_idx].size(), false);
+
+      /* Stage 1: Coalesce */
+      for (int type_pass = 0; type_pass < 2; ++type_pass) {
+         const RegType current_type = (type_pass == 0) ? RegType::vgpr : RegType::sgpr;
 
-      std::map<uint32_t, ltg_node> ltg;
-      bool has_vgpr_copy = false;
-      bool has_sgpr_copy = false;
-
-      /* first, try to coalesce all parallelcopies */
-      for (copy& cp : ctx.parallelcopies[i]) {
-         if (try_coalesce_copy(ctx, cp, i)) {
+         for (size_t n = 0u; n < ctx.parallelcopies[block_idx].size(); ++n) {
+            if (coalesced[n]) {
+               continue;
+            }
+
+            copy& cp = ctx.parallelcopies[block_idx][n];
+            if (cp.def.regClass().type() != current_type) {
+               continue;
+            }
+
+            if (!try_coalesce_copy(ctx, cp, static_cast<uint32_t>(block_idx))) {
+               continue;
+            }
+
+            coalesced[n] = true;
             assert(cp.op.isTemp() && cp.op.isKill());
-            /* As this temp will be used as phi operand and becomes live-out,
-             * remove the kill flag from any other copy of this same temp.
-             */
-            for (copy& other : ctx.parallelcopies[i]) {
-               if (&other != &cp && other.op.isTemp() && other.op.getTemp() == cp.op.getTemp())
+
+            /* Update kill flags */
+            for (copy& other : ctx.parallelcopies[block_idx]) {
+               if (&other != &cp && other.op.isTemp() &&
+                   other.op.getTemp() == cp.op.getTemp()) {
                   other.op.setKill(false);
+                  other.op.setFirstKill(false);
+               }
             }
-            renames.emplace(cp.def.tempId(), cp.op);
-         } else {
-            uint32_t read_idx = -1u;
-            if (cp.op.isTemp()) {
-               read_idx = ctx.merge_node_table[cp.op.tempId()].index;
-               /* In case the original phi-operand was killed, it might still be live-out
-                * if the logical successor is not the same as linear successors.
-                * Thus, re-check whether the temp is live-out.
-                */
-               cp.op.setKill(cp.op.isKill() && !is_live_out(ctx, cp.op.getTemp(), i));
-               cp.op.setFirstKill(cp.op.isKill());
-            }
-            uint32_t write_idx = ctx.merge_node_table[cp.def.tempId()].index;
-            assert(write_idx != -1u);
-            ltg[write_idx] = {&cp, read_idx};
-
-            bool is_vgpr = cp.def.regClass().type() == RegType::vgpr;
-            has_vgpr_copy |= is_vgpr;
-            has_sgpr_copy |= !is_vgpr;
+
+            rename_map.set(cp.def.tempId(), cp.op);
          }
       }
 
-      /* build location-transfer-graph */
-      for (auto& pair : ltg) {
-         if (pair.second.read_idx == -1u)
+      /* Stage 2: Build LTG */
+      uint32_t local_max_temp = 0u;
+      for (size_t n = 0u; n < ctx.parallelcopies[block_idx].size(); ++n) {
+         if (coalesced[n]) {
             continue;
-         auto&& it = ltg.find(pair.second.read_idx);
-         if (it != ltg.end())
-            it->second.num_uses++;
+         }
+
+         const copy& cp = ctx.parallelcopies[block_idx][n];
+         local_max_temp = std::max(local_max_temp, cp.def.tempId());
+         if (cp.op.isTemp()) {
+            local_max_temp = std::max(local_max_temp, cp.op.tempId());
+         }
       }
 
-      /* emit parallelcopies ordered */
+      if (local_max_temp >= MAX_TEMP_COUNT) {
+         assert(!"Local temp ID exceeds maximum");
+         continue;
+      }
+
+      std::vector<ltg_node> ltg(local_max_temp + 1u);
+      std::vector<uint8_t> ltg_valid(local_max_temp + 1u, 0u);
+      std::vector<uint32_t> active_keys;
+      active_keys.reserve(ctx.parallelcopies[block_idx].size());
+
+      bool has_vgpr = false;
+      bool has_sgpr = false;
+
+      /* Populate LTG */
+      for (size_t n = 0u; n < ctx.parallelcopies[block_idx].size(); ++n) {
+         if (coalesced[n]) {
+            continue;
+         }
+
+         copy& cp = ctx.parallelcopies[block_idx][n];
+
+         /* Apply renames */
+         if (cp.op.isTemp()) {
+            Operand resolved = rename_map.resolve(cp.op.tempId());
+            if (!resolved.isUndefined()) {
+               cp.op = resolved;
+            }
+         }
+
+         /* Update kill flags */
+         if (cp.op.isTemp()) {
+            const bool live_out = is_live_out(ctx, cp.op.getTemp(),
+                                              static_cast<uint32_t>(block_idx));
+            const bool keep_kill = cp.op.isKill() && !live_out;
+            cp.op.setKill(keep_kill);
+            cp.op.setFirstKill(keep_kill);
+         }
+
+         const uint32_t dst_key = cp.def.tempId();
+         const uint32_t read_key = cp.op.isTemp() ? cp.op.tempId() : INVALID_IDX;
+
+         if (dst_key > local_max_temp) {
+            continue;
+         }
+
+         ltg[dst_key] = ltg_node(&cp, read_key, 0u);
+         ltg_valid[dst_key] = 1u;
+         active_keys.push_back(dst_key);
+
+         has_vgpr |= (cp.def.regClass().type() == RegType::vgpr);
+         has_sgpr |= (cp.def.regClass().type() == RegType::sgpr);
+      }
+
+      /* Count uses */
+      for (uint32_t key : active_keys) {
+         if (key < ltg.size() && ltg_valid[key]) {
+            const uint32_t read_key = ltg[key].read_key;
+            if (read_key != INVALID_IDX && read_key < ltg.size() && ltg_valid[read_key]) {
+               ltg[read_key].num_uses++;
+            }
+         }
+      }
+
+      /* Stage 3: Emit */
       Builder bld(ctx.program);
-      Block& block = ctx.program->blocks[i];
+      Block& block = ctx.program->blocks[block_idx];
 
-      if (has_vgpr_copy) {
-         /* emit VGPR copies */
-         auto IsLogicalEnd = [](const aco_ptr<Instruction>& inst) -> bool
-         { return inst->opcode == aco_opcode::p_logical_end; };
-         auto it =
-            std::find_if(block.instructions.rbegin(), block.instructions.rend(), IsLogicalEnd);
-         bld.reset(&block.instructions, std::prev(it.base()));
-         emit_copies_block(bld, ltg, RegType::vgpr);
+      if (has_vgpr) {
+         auto logical_end_it = std::find_if(
+            block.instructions.rbegin(),
+            block.instructions.rend(),
+            [](const aco_ptr<Instruction>& ins) {
+               return ins->opcode == aco_opcode::p_logical_end;
+            }
+         );
+
+         auto insert_pt = (logical_end_it == block.instructions.rend())
+                        ? std::prev(block.instructions.end())
+                        : std::prev(logical_end_it.base());
+
+         bld.reset(&block.instructions, insert_pt);
+         emit_copies_block(bld, ltg, ltg_valid, active_keys, RegType::vgpr, rename_map);
       }
 
-      if (has_sgpr_copy) {
-         /* emit SGPR copies */
+      if (has_sgpr) {
          bld.reset(&block.instructions, std::prev(block.instructions.end()));
-         emit_copies_block(bld, ltg, RegType::sgpr);
+         emit_copies_block(bld, ltg, ltg_valid, active_keys, RegType::sgpr, rename_map);
       }
    }
 
-   RegisterDemand new_demand;
+   /* Update phi operands and register demand */
+   RegisterDemand program_demand;
+
    for (Block& block : ctx.program->blocks) {
-      /* Finally, rename coalesced phi operands */
       for (aco_ptr<Instruction>& phi : block.instructions) {
-         if (phi->opcode != aco_opcode::p_phi && phi->opcode != aco_opcode::p_linear_phi)
+         if (phi->opcode != aco_opcode::p_phi &&
+             phi->opcode != aco_opcode::p_linear_phi) {
             break;
+         }
 
          for (Operand& op : phi->operands) {
-            if (!op.isTemp())
-               continue;
-            auto&& it = renames.find(op.tempId());
-            if (it != renames.end()) {
-               op = it->second;
-               renames.erase(it);
+            if (op.isTemp()) {
+               Operand resolved = rename_map.resolve(op.tempId());
+               if (!resolved.isUndefined()) {
+                  op = resolved;
+               }
             }
          }
       }
 
-      /* Resummarize the block's register demand */
       block.register_demand = block.live_in_demand;
-      for (const aco_ptr<Instruction>& instr : block.instructions)
-         block.register_demand.update(instr->register_demand);
-      new_demand.update(block.register_demand);
-   }
+      for (const auto& ins : block.instructions) {
+         block.register_demand.update(ins->register_demand);
+      }
 
-   /* Update max_reg_demand and num_waves */
-   update_vgpr_sgpr_demand(ctx.program, new_demand);
+      program_demand.update(block.register_demand);
+   }
 
-   assert(renames.empty());
+   update_vgpr_sgpr_demand(ctx.program, program_demand);
 }
 
-} /* end namespace */
+} // namespace
 
 void
 lower_to_cssa(Program* program)
 {
+   if (!program) {
+      assert(!"Null program pointer");
+      return;
+   }
+
    reindex_ssa(program);
-   cssa_ctx ctx = {program};
+
+   cssa_ctx ctx{program};
    collect_parallelcopies(ctx);
    emit_parallelcopies(ctx);
 
-   /* Validate live variable information */
-   if (!validate_live_vars(program))
+   if (!validate_live_vars(program)) {
       abort();
+   }
 }
+
 } // namespace aco
