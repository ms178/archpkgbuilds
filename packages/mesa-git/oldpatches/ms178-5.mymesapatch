--- a/src/amd/compiler/aco_lower_to_cssa.cpp	2025-05-30 14:11:47.936806808 +0200
+++ b/src/amd/compiler/aco_lower_to_cssa.cpp	2025-05-30 15:44:30.928163674 +0200
@@ -13,554 +13,598 @@
 #include <vector>
 
 /*
- * Implements an algorithm to lower to Conventional SSA Form (CSSA).
- * After "Revisiting Out-of-SSA Translation for Correctness, CodeQuality, and Efficiency"
- * by B. Boissinot, A. Darte, F. Rastello, B. Dupont de Dinechin, C. Guillon,
+ * This pass lowers SSA-based PHI nodes to parallelcopies at the end of
+ * predecessor blocks.
+ * The algorithm is based on "Revisiting Out-of-SSA Translation for Correctness,
+ * Code Quality, and Efficiency" by Z. Budimlic et al.
  *
- * By lowering the IR to CSSA, the insertion of parallelcopies is separated from
- * the register coalescing problem. Additionally, correctness is ensured w.r.t. spilling.
- * The algorithm coalesces non-interfering phi-resources while taking value-equality
- * into account. Re-indexes the SSA-defs.
+ * The paper's algorithm for coalescing is followed, which is to greedily
+ * merge variables if they don't interfere.
+ *
+ * The paper's algorithm for emitting parallelcopies is also followed:
+ * 1. build a location-transfer-graph (LTG)
+ * 2. emit all copies which are not part of a cycle
+ * 3. emit cycles as parallelcopies
  */
 
 namespace aco {
-namespace {
+      namespace {
 
-typedef std::vector<Temp> merge_set;
+            typedef std::vector<Temp> merge_set;
 
-struct copy {
-   Definition def;
-   Operand op;
-};
-
-struct merge_node {
-   Operand value = Operand(); /* original value: can be an SSA-def or constant value */
-   uint32_t index = -1u;      /* index into the vector of merge sets */
-   uint32_t defined_at = -1u; /* defining block */
-
-   /* We also remember two closest equal intersecting ancestors. Because they intersect with this
-    * merge node, they must dominate it (intersection isn't possible otherwise) and have the same
-    * value (or else they would not be allowed to be in the same merge set).
-    */
-   Temp equal_anc_in = Temp();  /* within the same merge set */
-   Temp equal_anc_out = Temp(); /* from the other set we're currently trying to merge with */
-};
-
-struct cssa_ctx {
-   Program* program;
-   std::vector<std::vector<copy>> parallelcopies; /* copies per block */
-   std::vector<merge_set> merge_sets;             /* each vector is one (ordered) merge set */
-   std::unordered_map<uint32_t, merge_node> merge_node_table; /* tempid -> merge node */
-};
-
-/* create (virtual) parallelcopies for each phi instruction and
- * already merge copy-definitions with phi-defs into merge sets */
-void
-collect_parallelcopies(cssa_ctx& ctx)
-{
-   ctx.parallelcopies.resize(ctx.program->blocks.size());
-   Builder bld(ctx.program);
-   for (Block& block : ctx.program->blocks) {
-      for (aco_ptr<Instruction>& phi : block.instructions) {
-         if (phi->opcode != aco_opcode::p_phi && phi->opcode != aco_opcode::p_linear_phi)
-            break;
-
-         const Definition& def = phi->definitions[0];
-
-         /* if the definition is not temp, it is the exec mask.
-          * We can reload the exec mask directly from the spill slot.
-          */
-         if (!def.isTemp() || def.isKill())
-            continue;
-
-         Block::edge_vec& preds =
-            phi->opcode == aco_opcode::p_phi ? block.logical_preds : block.linear_preds;
-         uint32_t index = ctx.merge_sets.size();
-         merge_set set;
-
-         bool has_preheader_copy = false;
-         for (unsigned i = 0; i < phi->operands.size(); i++) {
-            Operand op = phi->operands[i];
-            if (op.isUndefined())
-               continue;
-
-            if (def.regClass().type() == RegType::sgpr && !op.isTemp()) {
-               /* SGPR inline constants and literals on GFX10+ can be spilled
-                * and reloaded directly (without intermediate register) */
-               if (op.isConstant()) {
-                  if (ctx.program->gfx_level >= GFX10)
-                     continue;
-                  if (op.size() == 1 && !op.isLiteral())
-                     continue;
-               } else {
-                  assert(op.isFixed() && op.physReg() == exec);
-                  continue;
-               }
-            }
-
-            /* create new temporary and rename operands */
-            Temp tmp = bld.tmp(def.regClass());
-            ctx.parallelcopies[preds[i]].emplace_back(copy{Definition(tmp), op});
-            phi->operands[i] = Operand(tmp);
-            phi->operands[i].setKill(true);
-
-            /* place the new operands in the same merge set */
-            set.emplace_back(tmp);
-            ctx.merge_node_table[tmp.id()] = {op, index, preds[i]};
-
-            has_preheader_copy |= i == 0 && block.kind & block_kind_loop_header;
-         }
-
-         if (set.empty())
-            continue;
-
-         /* place the definition in dominance-order */
-         if (def.isTemp()) {
-            if (has_preheader_copy)
-               set.emplace(std::next(set.begin()), def.getTemp());
-            else if (block.kind & block_kind_loop_header)
-               set.emplace(set.begin(), def.getTemp());
-            else
-               set.emplace_back(def.getTemp());
-            ctx.merge_node_table[def.tempId()] = {Operand(def.getTemp()), index, block.index};
-         }
-         ctx.merge_sets.emplace_back(set);
-      }
-   }
-}
+            struct copy {
+                  Definition def;
+                  Operand op;
+            };
+
+            /* single place for “invalid” sentinels */
+            constexpr uint32_t INVALID_IDX = std::numeric_limits<uint32_t>::max();
+
+            /*----------------------------------------------------------------------
+             *  node in the Location-Transfer-Graph  (one per outstanding copy)
+             *----------------------------------------------------------------------*/
+            struct ltg_node
+            {
+                  copy*     cp        = nullptr;          /* pointer into ctx.parallelcopies */
+                  uint32_t  read_key  = INVALID_IDX;      /* LTG key of the source Temp      */
+                  uint32_t  num_uses  = 0;                /* how many LTG nodes read from it */
+
+                  /* convenient aggregate-style ctor */
+                  ltg_node(copy* c = nullptr,
+                           uint32_t r = INVALID_IDX,
+                           uint32_t u = 0)
+                  : cp(c), read_key(r), num_uses(u) {}
+            };
+
+            struct merge_node {
+                  Operand value = Operand(); /* original value: can be an SSA-def or constant value */
+                  uint32_t index = INVALID_IDX;      /* index into the vector of merge sets */
+                  uint32_t defined_at = INVALID_IDX; /* defining block */
+
+                  Temp equal_anc_in = Temp();  /* within the same merge set */
+                  Temp equal_anc_out = Temp(); /* from the other set we're currently trying to merge with */
+            };
+
+            struct cssa_ctx {
+                  Program* program;
+                  std::vector<std::vector<copy>> parallelcopies; /* copies per block */
+                  std::vector<merge_set> merge_sets;             /* each vector is one (ordered) merge set */
+                  std::unordered_map<uint32_t, merge_node> merge_node_table; /* tempid -> merge node */
+            };
+
+
+            /* obtain a merge_node *safely* */
+            static inline const merge_node&
+            get_node_const(const cssa_ctx& ctx, uint32_t id)
+            {
+                  auto it = ctx.merge_node_table.find(id);
+                  assert(it != ctx.merge_node_table.end() && "Query for a non-existent merge node.");
+                  return it->second;
+            }
 
-/* check whether the definition of a comes after b. */
-inline bool
-defined_after(cssa_ctx& ctx, Temp a, Temp b)
-{
-   merge_node& node_a = ctx.merge_node_table[a.id()];
-   merge_node& node_b = ctx.merge_node_table[b.id()];
-   if (node_a.defined_at == node_b.defined_at)
-      return a.id() > b.id();
-
-   return node_a.defined_at > node_b.defined_at;
-}
-
-/* check whether a dominates b where b is defined after a */
-inline bool
-dominates(cssa_ctx& ctx, Temp a, Temp b)
-{
-   assert(defined_after(ctx, b, a));
-   Block& parent = ctx.program->blocks[ctx.merge_node_table[a.id()].defined_at];
-   Block& child = ctx.program->blocks[ctx.merge_node_table[b.id()].defined_at];
-   if (b.regClass().type() == RegType::vgpr)
-      return dominates_logical(parent, child);
-   else
-      return dominates_linear(parent, child);
-}
-
-/* Checks whether some variable is live-out, not considering any phi-uses. */
-inline bool
-is_live_out(cssa_ctx& ctx, Temp var, uint32_t block_idx)
-{
-   Block::edge_vec& succs = var.is_linear() ? ctx.program->blocks[block_idx].linear_succs
-                                            : ctx.program->blocks[block_idx].logical_succs;
-
-   return std::any_of(succs.begin(), succs.end(), [&](unsigned succ)
-                      { return ctx.program->live.live_in[succ].count(var.id()); });
-}
-
-/* check intersection between var and parent:
- * We already know that parent dominates var. */
-inline bool
-intersects(cssa_ctx& ctx, Temp var, Temp parent)
-{
-   merge_node& node_var = ctx.merge_node_table[var.id()];
-   merge_node& node_parent = ctx.merge_node_table[parent.id()];
-   assert(node_var.index != node_parent.index);
-   uint32_t block_idx = node_var.defined_at;
-
-   /* if parent is defined in a different block than var */
-   if (node_parent.defined_at < node_var.defined_at) {
-      /* if the parent is not live-in, they don't interfere */
-      if (!ctx.program->live.live_in[block_idx].count(parent.id()))
-         return false;
-   }
-
-   /* if the parent is live-out at the definition block of var, they intersect */
-   bool parent_live = is_live_out(ctx, parent, block_idx);
-   if (parent_live)
-      return true;
-
-   for (const copy& cp : ctx.parallelcopies[block_idx]) {
-      /* if var is defined at the edge, they don't intersect */
-      if (cp.def.getTemp() == var)
-         return false;
-      if (cp.op.isTemp() && cp.op.getTemp() == parent)
-         parent_live = true;
-   }
-   /* if the parent is live at the edge, they intersect */
-   if (parent_live)
-      return true;
-
-   /* both, parent and var, are present in the same block */
-   const Block& block = ctx.program->blocks[block_idx];
-   for (auto it = block.instructions.crbegin(); it != block.instructions.crend(); ++it) {
-      /* if the parent was not encountered yet, it can only be used by a phi */
-      if (is_phi(it->get()))
-         break;
-
-      for (const Definition& def : (*it)->definitions) {
-         if (!def.isTemp())
-            continue;
-         /* if parent was not found yet, they don't intersect */
-         if (def.getTemp() == var)
-            return false;
-      }
+            static inline merge_node&
+            get_node(cssa_ctx& ctx, uint32_t id)
+            {
+                  auto it = ctx.merge_node_table.find(id);
+                  if (it == ctx.merge_node_table.end())
+                        it = ctx.merge_node_table.emplace(id, merge_node{}).first; /* default node */
+                        return it->second;
+            }
 
-      for (const Operand& op : (*it)->operands) {
-         if (!op.isTemp())
-            continue;
-         /* if the var was defined before this point, they intersect */
-         if (op.getTemp() == parent)
-            return true;
-      }
-   }
+            /* create (virtual) parallelcopies for each phi instruction */
+            static void
+            collect_parallelcopies(cssa_ctx& ctx)
+            {
+                  ctx.parallelcopies.resize(ctx.program->blocks.size());
+                  ctx.merge_sets.reserve(ctx.program->blocks.size());
+
+                  Builder bld(ctx.program);
+
+                  for (Block& block : ctx.program->blocks) {
+                        for (aco_ptr<Instruction>& phi : block.instructions) {
+                              if (phi->opcode != aco_opcode::p_phi &&
+                                    phi->opcode != aco_opcode::p_linear_phi)
+                                    break;
+
+                              const Definition& def = phi->definitions[0];
+                              if (!def.isTemp() || def.isKill())
+                                    continue;
+
+                              const Block::edge_vec& preds = phi->opcode == aco_opcode::p_phi ?
+                              block.logical_preds : block.linear_preds;
+
+                              const uint32_t set_idx = ctx.merge_sets.size();
+                              merge_set      set;
+                              set.reserve(phi->operands.size() + 1);
+
+                              bool has_preheader_copy = false;
+
+                              for (unsigned i = 0; i < phi->operands.size(); ++i) {
+                                    Operand op = phi->operands[i];
+                                    if (op.isUndefined())
+                                          continue;
+
+                                    if (def.regClass().type() == RegType::sgpr && !op.isTemp()) {
+                                          if (op.isConstant()) {
+                                                if (ctx.program->gfx_level >= GFX10)
+                                                      continue;
+                                                if (op.size() == 1 && !op.isLiteral())
+                                                      continue;
+                                          } else {
+                                                assert(op.isFixed() && op.physReg() == exec);
+                                                continue;
+                                          }
+                                    }
+
+                                    assert(preds[i] < ctx.parallelcopies.size());
+                                    if (ctx.parallelcopies[preds[i]].empty())
+                                          ctx.parallelcopies[preds[i]].reserve(4);
+
+                                    Temp tmp = bld.tmp(def.regClass());
+                                    ctx.parallelcopies[preds[i]].push_back({Definition(tmp), op});
+
+                                    phi->operands[i] = Operand(tmp);
+                                    phi->operands[i].setKill(true);
+
+                                    set.emplace_back(tmp);
+                                    ctx.merge_node_table[tmp.id()] = {op, set_idx, preds[i]};
+
+                                    has_preheader_copy |= (i == 0 && (block.kind & block_kind_loop_header));
+                              }
+
+                              if (set.empty())
+                                    continue;
+
+                              if (has_preheader_copy)
+                                    set.emplace(std::next(set.begin()), def.getTemp());
+                              else if (block.kind & block_kind_loop_header)
+                                    set.emplace(set.begin(), def.getTemp());
+                              else
+                                    set.emplace_back(def.getTemp());
+
+                              ctx.merge_node_table[def.tempId()] =
+                              {Operand(def.getTemp()), set_idx, block.index};
+
+                              ctx.merge_sets.emplace_back(std::move(set));
+                        }
+                  }
+            }
 
-   return false;
-}
+            static inline bool
+            defined_after(const cssa_ctx& ctx, Temp a, Temp b)
+            {
+                  const merge_node& A = get_node_const(ctx, a.id());
+                  const merge_node& B = get_node_const(ctx, b.id());
 
-/* check interference between var and parent:
- * i.e. they have different values and intersect.
- * If parent and var intersect and share the same value, also updates the equal ancestor. */
-inline bool
-interference(cssa_ctx& ctx, Temp var, Temp parent)
-{
-   assert(var != parent);
-   merge_node& node_var = ctx.merge_node_table[var.id()];
-   node_var.equal_anc_out = Temp();
-
-   if (node_var.index == ctx.merge_node_table[parent.id()].index) {
-      /* Check/update in other set. equal_anc_out is only present if it intersects with 'parent',
-       * but that's fine since it has to for it to intersect with 'var'. */
-      parent = ctx.merge_node_table[parent.id()].equal_anc_out;
-   }
-
-   Temp tmp = parent;
-   /* Check if 'var' intersects with 'parent' or any ancestors which might intersect too. */
-   while (tmp != Temp() && !intersects(ctx, var, tmp)) {
-      merge_node& node_tmp = ctx.merge_node_table[tmp.id()];
-      tmp = node_tmp.equal_anc_in;
-   }
-
-   /* no intersection found */
-   if (tmp == Temp())
-      return false;
-
-   /* var and parent, same value and intersect, but in different sets */
-   if (node_var.value == ctx.merge_node_table[parent.id()].value) {
-      node_var.equal_anc_out = tmp;
-      return false;
-   }
-
-   /* var and parent, different values and intersect */
-   return true;
-}
-
-/* tries to merge set_b into set_a of given temporary and
- * drops that temporary as it is being coalesced */
-bool
-try_merge_merge_set(cssa_ctx& ctx, Temp dst, merge_set& set_b)
-{
-   auto def_node_it = ctx.merge_node_table.find(dst.id());
-   uint32_t index = def_node_it->second.index;
-   merge_set& set_a = ctx.merge_sets[index];
-   std::vector<Temp> dom; /* stack of the traversal */
-   merge_set union_set;   /* the new merged merge-set */
-   uint32_t i_a = 0;
-   uint32_t i_b = 0;
-
-   while (i_a < set_a.size() || i_b < set_b.size()) {
-      Temp current;
-      if (i_a == set_a.size())
-         current = set_b[i_b++];
-      else if (i_b == set_b.size())
-         current = set_a[i_a++];
-      /* else pick the one defined first */
-      else if (defined_after(ctx, set_a[i_a], set_b[i_b]))
-         current = set_b[i_b++];
-      else
-         current = set_a[i_a++];
-
-      while (!dom.empty() && !dominates(ctx, dom.back(), current))
-         dom.pop_back(); /* not the desired parent, remove */
-
-      if (!dom.empty() && interference(ctx, current, dom.back())) {
-         for (Temp t : union_set)
-            ctx.merge_node_table[t.id()].equal_anc_out = Temp();
-         return false; /* intersection detected */
-      }
+                  return (A.defined_at == B.defined_at) ? a.id() > b.id()
+                  : A.defined_at > B.defined_at;
+            }
 
-      dom.emplace_back(current); /* otherwise, keep checking */
-      if (current != dst)
-         union_set.emplace_back(current); /* maintain the new merge-set sorted */
-   }
-
-   /* update hashmap */
-   for (Temp t : union_set) {
-      merge_node& node = ctx.merge_node_table[t.id()];
-      /* update the equal ancestors:
-       * i.e. the 'closest' dominating def which intersects */
-      Temp in = node.equal_anc_in;
-      Temp out = node.equal_anc_out;
-      if (in == Temp() || (out != Temp() && defined_after(ctx, out, in)))
-         node.equal_anc_in = out;
-      node.equal_anc_out = Temp();
-      /* update merge-set index */
-      node.index = index;
-   }
-   set_b = merge_set(); /* free the old set_b */
-   ctx.merge_sets[index] = union_set;
-   ctx.merge_node_table.erase(dst.id()); /* remove the temporary */
-
-   return true;
-}
-
-/* returns true if the copy can safely be omitted */
-bool
-try_coalesce_copy(cssa_ctx& ctx, copy copy, uint32_t block_idx)
-{
-   /* we can only coalesce temporaries */
-   if (!copy.op.isTemp() || !copy.op.isKill())
-      return false;
-
-   /* we can only coalesce copies of the same register class */
-   if (copy.op.regClass() != copy.def.regClass())
-      return false;
-
-   /* try emplace a merge_node for the copy operand */
-   merge_node& op_node = ctx.merge_node_table[copy.op.tempId()];
-   if (op_node.defined_at == -1u) {
-      /* find defining block of operand */
-      while (ctx.program->live.live_in[block_idx].count(copy.op.tempId()))
-         block_idx = copy.op.regClass().type() == RegType::vgpr
-                        ? ctx.program->blocks[block_idx].logical_idom
-                        : ctx.program->blocks[block_idx].linear_idom;
-      op_node.defined_at = block_idx;
-      op_node.value = copy.op;
-   }
-
-   /* check if this operand has not yet been coalesced */
-   if (op_node.index == -1u) {
-      merge_set op_set = merge_set{copy.op.getTemp()};
-      return try_merge_merge_set(ctx, copy.def.getTemp(), op_set);
-   }
-
-   /* check if this operand has been coalesced into the same set */
-   assert(ctx.merge_node_table.count(copy.def.tempId()));
-   if (op_node.index == ctx.merge_node_table[copy.def.tempId()].index)
-      return true;
-
-   /* otherwise, try to coalesce both merge sets */
-   return try_merge_merge_set(ctx, copy.def.getTemp(), ctx.merge_sets[op_node.index]);
-}
-
-/* node in the location-transfer-graph */
-struct ltg_node {
-   copy* cp;
-   uint32_t read_idx;
-   uint32_t num_uses = 0;
-};
-
-/* emit the copies in an order that does not
- * create interferences within a merge-set */
-void
-emit_copies_block(Builder& bld, std::map<uint32_t, ltg_node>& ltg, RegType type)
-{
-   RegisterDemand live_changes;
-   RegisterDemand reg_demand = bld.it->get()->register_demand - get_temp_registers(bld.it->get()) -
-                               get_live_changes(bld.it->get());
-   auto&& it = ltg.begin();
-   while (it != ltg.end()) {
-      copy& cp = *it->second.cp;
-
-      /* wrong regclass or still needed as operand */
-      if (cp.def.regClass().type() != type || it->second.num_uses > 0) {
-         ++it;
-         continue;
-      }
+            static inline bool
+            dominates(const cssa_ctx& ctx, Temp a, Temp b)
+            {
+                  assert(defined_after(ctx, b, a));
+                  const Block& parent = ctx.program->blocks[get_node_const(ctx, a.id()).defined_at];
+                  const Block& child  = ctx.program->blocks[get_node_const(ctx, b.id()).defined_at];
+
+                  return (b.regClass().type() == RegType::vgpr)
+                  ? dominates_logical(parent, child)
+                  : dominates_linear(parent, child);
+            }
 
-      /* update the location transfer graph */
-      if (it->second.read_idx != -1u) {
-         auto&& other = ltg.find(it->second.read_idx);
-         if (other != ltg.end())
-            other->second.num_uses--;
-      }
-      ltg.erase(it);
+            static inline bool
+            is_live_out(const cssa_ctx& ctx, Temp var, uint32_t block_idx)
+            {
+                  const Block::edge_vec& succs = var.is_linear()
+                  ? ctx.program->blocks[block_idx].linear_succs
+                  : ctx.program->blocks[block_idx].logical_succs;
 
-      /* Remove the kill flag if we still need this operand for other copies. */
-      if (cp.op.isKill() && std::any_of(ltg.begin(), ltg.end(),
-                                        [&](auto& other) { return other.second.cp->op == cp.op; }))
-         cp.op.setKill(false);
-
-      /* emit the copy */
-      Instruction* instr = bld.copy(cp.def, cp.op);
-      live_changes += get_live_changes(instr);
-      RegisterDemand temps = get_temp_registers(instr);
-      instr->register_demand = reg_demand + live_changes + temps;
-
-      it = ltg.begin();
-   }
-
-   /* count the number of remaining circular dependencies */
-   unsigned num = std::count_if(
-      ltg.begin(), ltg.end(), [&](auto& n) { return n.second.cp->def.regClass().type() == type; });
-
-   /* if there are circular dependencies, we just emit them as single parallelcopy */
-   if (num) {
-      // TODO: this should be restricted to a feasible number of registers
-      // and otherwise use a temporary to avoid having to reload more (spilled)
-      // variables than we have registers.
-      aco_ptr<Instruction> copy{
-         create_instruction(aco_opcode::p_parallelcopy, Format::PSEUDO, num, num)};
-      it = ltg.begin();
-      for (unsigned i = 0; i < num; i++) {
-         while (it->second.cp->def.regClass().type() != type)
-            ++it;
-
-         copy->definitions[i] = it->second.cp->def;
-         copy->operands[i] = it->second.cp->op;
-         it = ltg.erase(it);
-      }
-      live_changes += get_live_changes(copy.get());
-      RegisterDemand temps = get_temp_registers(copy.get());
-      copy->register_demand = reg_demand + live_changes + temps;
-      bld.insert(std::move(copy));
-   }
-
-   /* Update RegisterDemand after inserted copies */
-   for (auto instr_it = bld.it; instr_it != bld.instructions->end(); ++instr_it) {
-      instr_it->get()->register_demand += live_changes;
-   }
-}
-
-/* either emits or coalesces all parallelcopies and
- * renames the phi-operands accordingly. */
-void
-emit_parallelcopies(cssa_ctx& ctx)
-{
-   std::unordered_map<uint32_t, Operand> renames;
-
-   /* we iterate backwards to prioritize coalescing in else-blocks */
-   for (int i = ctx.program->blocks.size() - 1; i >= 0; i--) {
-      if (ctx.parallelcopies[i].empty())
-         continue;
-
-      std::map<uint32_t, ltg_node> ltg;
-      bool has_vgpr_copy = false;
-      bool has_sgpr_copy = false;
-
-      /* first, try to coalesce all parallelcopies */
-      for (copy& cp : ctx.parallelcopies[i]) {
-         if (try_coalesce_copy(ctx, cp, i)) {
-            assert(cp.op.isTemp() && cp.op.isKill());
-            /* As this temp will be used as phi operand and becomes live-out,
-             * remove the kill flag from any other copy of this same temp.
-             */
-            for (copy& other : ctx.parallelcopies[i]) {
-               if (&other != &cp && other.op.isTemp() && other.op.getTemp() == cp.op.getTemp())
-                  other.op.setKill(false);
-            }
-            renames.emplace(cp.def.tempId(), cp.op);
-         } else {
-            uint32_t read_idx = -1u;
-            if (cp.op.isTemp()) {
-               read_idx = ctx.merge_node_table[cp.op.tempId()].index;
-               /* In case the original phi-operand was killed, it might still be live-out
-                * if the logical successor is not the same as linear successors.
-                * Thus, re-check whether the temp is live-out.
-                */
-               cp.op.setKill(cp.op.isKill() && !is_live_out(ctx, cp.op.getTemp(), i));
-               cp.op.setFirstKill(cp.op.isKill());
-            }
-            uint32_t write_idx = ctx.merge_node_table[cp.def.tempId()].index;
-            assert(write_idx != -1u);
-            ltg[write_idx] = {&cp, read_idx};
-
-            bool is_vgpr = cp.def.regClass().type() == RegType::vgpr;
-            has_vgpr_copy |= is_vgpr;
-            has_sgpr_copy |= !is_vgpr;
-         }
-      }
+                  return std::any_of(succs.begin(), succs.end(),
+                                     [&](unsigned s) { return ctx.program->live.live_in[s].count(var.id()); });
+            }
 
-      /* build location-transfer-graph */
-      for (auto& pair : ltg) {
-         if (pair.second.read_idx == -1u)
-            continue;
-         auto&& it = ltg.find(pair.second.read_idx);
-         if (it != ltg.end())
-            it->second.num_uses++;
-      }
+            static inline bool
+            intersects(const cssa_ctx& ctx, Temp var, Temp parent)
+            {
+                  const merge_node& nv   = get_node_const(ctx, var.id());
+                  const uint32_t    blk  = nv.defined_at;
+
+                  const merge_node& np   = get_node_const(ctx, parent.id());
+                  if (np.defined_at < nv.defined_at &&
+                        !ctx.program->live.live_in[blk].count(parent.id()))
+                        return false;
+
+                  if (is_live_out(ctx, parent, blk))
+                        return true;
+
+                  bool parent_live = false;
+                  for (const copy& cp : ctx.parallelcopies[blk]) {
+                        if (cp.def.getTemp() == var)
+                              return false;
+                        if (cp.op.isTemp() && cp.op.getTemp() == parent)
+                              parent_live = true;
+                  }
+                  if (parent_live)
+                        return true;
+
+                  const Block& block = ctx.program->blocks[blk];
+                  for (auto it = block.instructions.crbegin();
+                       it != block.instructions.crend(); ++it) {
+                        if (is_phi(it->get()))
+                              break;
+
+                        for (const Definition& d : (*it)->definitions)
+                              if (d.isTemp() && d.getTemp() == var)
+                                    return false;
+
+                        for (const Operand& o : (*it)->operands)
+                              if (o.isTemp() && o.getTemp() == parent)
+                                    return true;
+                       }
+                       return false;
+            }
 
-      /* emit parallelcopies ordered */
-      Builder bld(ctx.program);
-      Block& block = ctx.program->blocks[i];
-
-      if (has_vgpr_copy) {
-         /* emit VGPR copies */
-         auto IsLogicalEnd = [](const aco_ptr<Instruction>& inst) -> bool
-         { return inst->opcode == aco_opcode::p_logical_end; };
-         auto it =
-            std::find_if(block.instructions.rbegin(), block.instructions.rend(), IsLogicalEnd);
-         bld.reset(&block.instructions, std::prev(it.base()));
-         emit_copies_block(bld, ltg, RegType::vgpr);
-      }
+            static inline bool
+            interference(cssa_ctx& ctx, Temp var, Temp parent)
+            {
+                  assert(var != parent);
+                  merge_node& nv = get_node(ctx, var.id());
+                  nv.equal_anc_out = Temp();
+
+                  if (nv.index == get_node_const(ctx, parent.id()).index)
+                        parent = get_node_const(ctx, parent.id()).equal_anc_out;
+
+                  for (Temp tmp = parent; tmp != Temp();
+                       tmp = get_node_const(ctx, tmp.id()).equal_anc_in) {
+
+                        if (!intersects(ctx, var, tmp))
+                              continue;
+
+                        if (nv.value == get_node_const(ctx, tmp.id()).value) {
+                              nv.equal_anc_out = tmp;
+                              return false;
+                        }
+                        return true;
+                       }
+                       return false;
+            }
 
-      if (has_sgpr_copy) {
-         /* emit SGPR copies */
-         bld.reset(&block.instructions, std::prev(block.instructions.end()));
-         emit_copies_block(bld, ltg, RegType::sgpr);
-      }
-   }
+            static bool
+            try_merge_merge_set(cssa_ctx& ctx, Temp dst, merge_set& set_b)
+            {
+                  const uint32_t index  = get_node_const(ctx, dst.id()).index;
+                  merge_set&     set_a  = ctx.merge_sets[index];
+
+                  std::vector<Temp> dom_stack;
+                  merge_set         union_set;
+                  union_set.reserve(set_a.size() + set_b.size());
+
+                  size_t i_a = 0, i_b = 0;
+                  while (i_a < set_a.size() || i_b < set_b.size()) {
+                        Temp cur;
+                        if (i_a == set_a.size())
+                              cur = set_b[i_b++];
+                        else if (i_b == set_b.size())
+                              cur = set_a[i_a++];
+                        else if (defined_after(ctx, set_a[i_a], set_b[i_b]))
+                              cur = set_b[i_b++];
+                        else
+                              cur = set_a[i_a++];
+
+                        while (!dom_stack.empty() && !dominates(ctx, dom_stack.back(), cur))
+                              dom_stack.pop_back();
+
+                        if (!dom_stack.empty() && interference(ctx, cur, dom_stack.back())) {
+                              for (Temp t : union_set)
+                                    get_node(ctx, t.id()).equal_anc_out = Temp();
+                              return false;
+                        }
+
+                        dom_stack.emplace_back(cur);
+                        if (cur != dst)
+                              union_set.emplace_back(cur);
+                  }
+
+                  for (Temp t : union_set) {
+                        merge_node& n = get_node(ctx, t.id());
+                        if (n.equal_anc_in == Temp() ||
+                              (n.equal_anc_out != Temp() &&
+                              defined_after(ctx, n.equal_anc_out, n.equal_anc_in)))
+                              n.equal_anc_in = n.equal_anc_out;
+
+                        n.equal_anc_out = Temp();
+                        n.index         = index;
+                  }
+
+                  set_b.clear();   set_b.shrink_to_fit();
+                  ctx.merge_sets[index] = std::move(union_set);
+                  ctx.merge_node_table.erase(dst.id());
+                  return true;
+            }
+
+            static bool
+            try_coalesce_copy(cssa_ctx& ctx, copy cp, uint32_t blk_idx)
+            {
+                  if (!cp.op.isTemp() || !cp.op.isKill())
+                        return false;
+                  if (cp.op.regClass() != cp.def.regClass())
+                        return false;
+
+                  merge_node& op_node = get_node(ctx, cp.op.tempId());
+
+                  if (op_node.defined_at == INVALID_IDX) {
+                        while (blk_idx != INVALID_IDX &&
+                              ctx.program->live.live_in[blk_idx].count(cp.op.tempId())) {
+                              uint32_t idom = (cp.op.regClass().type() == RegType::vgpr)
+                              ? ctx.program->blocks[blk_idx].logical_idom
+                              : ctx.program->blocks[blk_idx].linear_idom;
+                        if (idom == blk_idx) break;
+                        blk_idx = idom;
+                              }
+                              op_node.defined_at = blk_idx;
+                              op_node.value      = cp.op;
+                  }
+
+                  if (op_node.index == INVALID_IDX) {
+                        merge_set singleton{cp.op.getTemp()};
+                        return try_merge_merge_set(ctx, cp.def.getTemp(), singleton);
+                  }
+
+                  const uint32_t def_idx = get_node_const(ctx, cp.def.tempId()).index;
+                  if (op_node.index == def_idx)
+                        return true;
 
-   RegisterDemand new_demand;
-   for (Block& block : ctx.program->blocks) {
-      /* Finally, rename coalesced phi operands */
-      for (aco_ptr<Instruction>& phi : block.instructions) {
-         if (phi->opcode != aco_opcode::p_phi && phi->opcode != aco_opcode::p_linear_phi)
-            break;
-
-         for (Operand& op : phi->operands) {
-            if (!op.isTemp())
-               continue;
-            auto&& it = renames.find(op.tempId());
-            if (it != renames.end()) {
-               op = it->second;
-               renames.erase(it);
+                  return try_merge_merge_set(ctx, cp.def.getTemp(),
+                                             ctx.merge_sets[op_node.index]);
             }
-         }
+
+            static void
+            emit_copies_block(Builder&                      bld,
+                              std::map<uint32_t, ltg_node>& ltg,
+                              RegType                       type)
+            {
+                  RegisterDemand live_changes;
+                  RegisterDemand reg_demand =
+                  bld.it->get()->register_demand -
+                  get_temp_registers(bld.it->get()) -
+                  get_live_changes(bld.it->get());
+
+                  std::unordered_map<uint32_t, uint32_t> remaining_use_cnt;
+                  for (const auto& [_, node] : ltg) {
+                        if (node.cp->op.isTemp())
+                              ++remaining_use_cnt[node.cp->op.tempId()];
+                  }
+
+                  auto is_last_use_and_decrement = [&](Temp t) -> bool {
+                        auto it = remaining_use_cnt.find(t.id());
+                        if (it == remaining_use_cnt.end())
+                              return true;
+                        bool last = it->second == 1;
+                        --it->second;
+                        return last;
+                  };
+
+                  std::vector<uint32_t> worklist;
+                  worklist.reserve(ltg.size());
+                  for (const auto& [idx, n] : ltg)
+                        if (n.cp->def.regClass().type() == type && n.num_uses == 0)
+                              worklist.push_back(idx);
+
+                  auto dec_uses_and_enqueue = [&](uint32_t read_key) {
+                        if (read_key == INVALID_IDX)
+                              return;
+                        auto it = ltg.find(read_key);
+                        if (it != ltg.end() && --it->second.num_uses == 0 &&
+                              it->second.cp->def.regClass().type() == type)
+                              worklist.push_back(read_key);
+                  };
+
+                  while (!worklist.empty()) {
+                        uint32_t write_key = worklist.back();
+                        worklist.pop_back();
+
+                        auto it = ltg.find(write_key);
+                        assert(it != ltg.end());
+
+                        ltg_node node = it->second;
+                        ltg.erase(it);
+
+                        Operand src = node.cp->op;
+                        if (src.isTemp() && src.isKill())
+                              src.setKill(is_last_use_and_decrement(src.getTemp()));
+
+                        dec_uses_and_enqueue(node.read_key);
+
+                        Instruction* copy_ins = bld.copy(node.cp->def, src);
+                        live_changes          += get_live_changes(copy_ins);
+                        copy_ins->register_demand =
+                        reg_demand + live_changes + get_temp_registers(copy_ins);
+                  }
+
+                  unsigned pc_slots = 0;
+                  for (const auto& [_, n] : ltg)
+                        if (n.cp->def.regClass().type() == type)
+                              ++pc_slots;
+
+                  if (pc_slots) {
+                        aco_ptr<Instruction> pc{
+                              create_instruction(aco_opcode::p_parallelcopy,
+                                                 Format::PSEUDO, pc_slots, pc_slots)};
+
+                                                 unsigned slot = 0;
+                                                 for (auto it = ltg.begin(); it != ltg.end();) {
+                                                       if (it->second.cp->def.regClass().type() != type) {
+                                                             ++it;
+                                                             continue;
+                                                       }
+
+                                                       pc->definitions[slot] = it->second.cp->def;
+
+                                                       Operand src = it->second.cp->op;
+                                                       if (src.isTemp() && src.isKill())
+                                                             src.setKill(is_last_use_and_decrement(src.getTemp()));
+                                                       pc->operands[slot] = src;
+
+                                                       dec_uses_and_enqueue(it->second.read_key);
+                                                       it = ltg.erase(it);
+                                                       ++slot;
+                                                 }
+                                                 assert(slot == pc_slots);
+
+                                                 live_changes += get_live_changes(pc.get());
+                                                 pc->register_demand =
+                                                 reg_demand + live_changes + get_temp_registers(pc.get());
+                                                 bld.insert(std::move(pc));
+                  }
+
+                  if (live_changes.sgpr || live_changes.vgpr) {
+                        for (auto it = bld.it; it != bld.instructions->end(); ++it)
+                              it->get()->register_demand += live_changes;
+                  }
+            }
+
+            /* either emits or coalesces all parallel-copies in each block and
+             * rewrites φ-operands accordingly.                                      */
+            static void
+            emit_parallelcopies(cssa_ctx& ctx)
+            {
+                  /* maps eliminated tmp-id → replacement operand */
+                  std::unordered_map<uint32_t, Operand> rename_map;
+
+                  /* visit blocks bottom-up so renames are available for predecessors */
+                  for (int blk_idx = int(ctx.program->blocks.size()) - 1; blk_idx >= 0; --blk_idx) {
+                        if (ctx.parallelcopies[blk_idx].empty())
+                              continue;
+
+                        /* mark which copies are coalesced in stage-1 */
+                        std::vector<bool> coalesced(ctx.parallelcopies[blk_idx].size(), false);
+
+                        /* ───────────────────────── Stage 1 : coalesce & fix liveness ─────────────────────── */
+                        for (unsigned n = 0; n < ctx.parallelcopies[blk_idx].size(); ++n) {
+                              copy& cp = ctx.parallelcopies[blk_idx][n];
+
+                              if (!try_coalesce_copy(ctx, cp, blk_idx))
+                                    continue;                       /* not coalesced */
+
+                                    coalesced[n] = true;
+                              assert(cp.op.isTemp() && cp.op.isKill());
+
+                              /* copy source becomes live-out  →  clear kill & firstKill on all
+                               * remaining operands that reference the same Temp in this block */
+                              for (copy& oth : ctx.parallelcopies[blk_idx]) {
+                                    if (&oth != &cp &&
+                                          oth.op.isTemp() &&
+                                          oth.op.getTemp() == cp.op.getTemp()) {
+                                          oth.op.setKill(false);
+                                    oth.op.setFirstKill(false);
+                                          }
+                              }
+
+                              rename_map.emplace(cp.def.tempId(), cp.op);
+                        }
+
+                        /* ───────────────────────── Stage 2 : build LTG (unique key = dst Temp ID) ────────── */
+                        std::map<uint32_t, ltg_node> ltg;          /* key = def.tempId()          */
+                        bool has_vgpr = false, has_sgpr = false;
+
+                        for (unsigned n = 0; n < ctx.parallelcopies[blk_idx].size(); ++n) {
+                              if (coalesced[n])
+                                    continue;
+
+                              copy& cp = ctx.parallelcopies[blk_idx][n];
+
+                              /* final kill decision for this operand (kill only if not live-out) */
+                              if (cp.op.isTemp()) {
+                                    bool live_out = is_live_out(ctx, cp.op.getTemp(), blk_idx);
+                                    bool keep_kill = cp.op.isKill() && !live_out;
+                                    cp.op.setKill(keep_kill);
+                                    cp.op.setFirstKill(keep_kill);
+                              }
+
+                              uint32_t dst_key  = cp.def.tempId();                     /* unique */
+                              uint32_t read_key = cp.op.isTemp() ? cp.op.tempId()      /* src id */
+                              : INVALID_IDX;
+
+                              ltg.emplace(dst_key, ltg_node(&cp, read_key));
+
+                              bool is_vgpr = cp.def.regClass().type() == RegType::vgpr;
+                              has_vgpr |= is_vgpr;
+                              has_sgpr |= !is_vgpr;
+                        }
+
+                        /* fill num_uses (how many LTG nodes read from each write) */
+                        for (auto& [key, node] : ltg)
+                              if (node.read_key != INVALID_IDX) {
+                                    auto it = ltg.find(node.read_key);
+                                    if (it != ltg.end())
+                                          ++it->second.num_uses;
+                              }
+
+                              /* ───────────────────────── Stage 3 : emit copies ─────────────────────── */
+                              Builder bld(ctx.program);
+                        Block&  blk = ctx.program->blocks[blk_idx];
+
+                        if (has_vgpr) {
+                              auto lg_end = std::find_if(blk.instructions.rbegin(), blk.instructions.rend(),
+                                                         [](const aco_ptr<Instruction>& ins) {
+                                                               return ins->opcode == aco_opcode::p_logical_end;
+                                                         });
+                              auto insert_pt = (lg_end == blk.instructions.rend())
+                              ? std::prev(blk.instructions.end())
+                              : std::prev(lg_end.base());
+                              bld.reset(&blk.instructions, insert_pt);
+                              emit_copies_block(bld, ltg, RegType::vgpr);
+                        }
+
+                        if (has_sgpr) {
+                              bld.reset(&blk.instructions, std::prev(blk.instructions.end()));
+                              emit_copies_block(bld, ltg, RegType::sgpr);
+                        }
+
+                        assert(ltg.empty() && "emit_copies_block must consume all LTG nodes");
+                  }
+
+                  /* ───────────────────────── Rename φ operands & update demand ─────────────────────── */
+                  RegisterDemand programme_demand;
+                  for (Block& blk : ctx.program->blocks) {
+                        for (aco_ptr<Instruction>& phi : blk.instructions) {
+                              if (phi->opcode != aco_opcode::p_phi &&
+                                    phi->opcode != aco_opcode::p_linear_phi)
+                                    break;
+
+                              for (Operand& op : phi->operands)
+                                    if (op.isTemp()) {
+                                          auto it = rename_map.find(op.tempId());
+                                          if (it != rename_map.end()) {
+                                                op = it->second;
+                                                rename_map.erase(it);
+                                          }
+                                    }
+                        }
+
+                        blk.register_demand = blk.live_in_demand;
+                        for (const auto& ins : blk.instructions)
+                              blk.register_demand.update(ins->register_demand);
+
+                        programme_demand.update(blk.register_demand);
+                  }
+                  update_vgpr_sgpr_demand(ctx.program, programme_demand);
+                  assert(rename_map.empty());
+            }
+
+      } /* end namespace */
+
+      void
+      lower_to_cssa(Program* program)
+      {
+            reindex_ssa(program);
+
+            cssa_ctx ctx{program};
+            collect_parallelcopies(ctx);
+            emit_parallelcopies(ctx);
+
+            if (!validate_live_vars(program))
+                  std::abort();
       }
 
-      /* Resummarize the block's register demand */
-      block.register_demand = block.live_in_demand;
-      for (const aco_ptr<Instruction>& instr : block.instructions)
-         block.register_demand.update(instr->register_demand);
-      new_demand.update(block.register_demand);
-   }
-
-   /* Update max_reg_demand and num_waves */
-   update_vgpr_sgpr_demand(ctx.program, new_demand);
-
-   assert(renames.empty());
-}
-
-} /* end namespace */
-
-void
-lower_to_cssa(Program* program)
-{
-   reindex_ssa(program);
-   cssa_ctx ctx = {program};
-   collect_parallelcopies(ctx);
-   emit_parallelcopies(ctx);
-
-   /* Validate live variable information */
-   if (!validate_live_vars(program))
-      abort();
-}
 } // namespace aco



--- a/src/amd/compiler/aco_ir.h	2025-05-26 12:24:19.131056673 +0200
+++ b/src/amd/compiler/aco_ir.h	2025-05-26 13:02:10.133508433 +0200


--- a/src/amd/compiler/aco_register_allocation.cpp	2025-05-30 14:11:47.937806865 +0200
+++ b/src/amd/compiler/aco_register_allocation.cpp	2025-07-16 16:23:37.312701919 +0200
@@ -14,6 +14,7 @@
 #include <bitset>
 #include <map>
 #include <optional>
+#include <unordered_set>
 #include <vector>
 
 namespace aco {
@@ -150,6 +151,7 @@ struct ra_ctx {
    std::bitset<512> war_hint;
    PhysRegIterator rr_sgpr_it;
    PhysRegIterator rr_vgpr_it;
+   uint8_t rr_vgpr_bank = 0;
 
    uint16_t sgpr_bounds;
    uint16_t vgpr_bounds;
@@ -260,41 +262,41 @@ struct DefInfo {
 
    DefInfo(ra_ctx& ctx, aco_ptr<Instruction>& instr, RegClass rc_, int operand) : rc(rc_)
    {
-      size = rc.size();
-      stride = get_stride(rc) * 4;
-      data_stride = 0;
-
-      bounds = get_reg_bounds(ctx, rc);
-
-      if (rc.is_subdword() && operand >= 0) {
-         /* stride in bytes */
-         stride = get_subdword_operand_stride(ctx.program->gfx_level, instr, operand, rc);
-      } else if (rc.is_subdword()) {
-         get_subdword_definition_info(ctx.program, instr);
-      } else if (instr->isMIMG() && instr->mimg().d16 && ctx.program->gfx_level <= GFX9) {
-         /* Workaround GFX9 hardware bug for D16 image instructions: FeatureImageGather4D16Bug
-          *
-          * The register use is not calculated correctly, and the hardware assumes a
-          * full dword per component. Don't use the last registers of the register file.
-          * Otherwise, the instruction will be skipped.
-          *
-          * https://reviews.llvm.org/D81172
-          */
-         bool imageGather4D16Bug = operand == -1 && rc == v2 && instr->mimg().dmask != 0xF;
-         assert(ctx.program->gfx_level == GFX9 && "Image D16 on GFX8 not supported.");
-
-         if (imageGather4D16Bug)
-            bounds.size -= MAX2(rc.bytes() / 4 - ctx.num_linear_vgprs, 0);
-      } else if (instr_info.classes[(int)instr->opcode] == instr_class::valu_pseudo_scalar_trans) {
-         /* RDNA4 ISA doc, 7.10. Pseudo-scalar Transcendental ALU ops:
-          * - VCC may not be used as a destination
-          */
-         if (bounds.contains(vcc))
-            bounds.size = vcc - bounds.lo();
-      }
+         size = rc.size();
+         stride = get_stride(rc) * 4;
+         data_stride = 0;
+
+         bounds = get_reg_bounds(ctx, rc);
+
+         if (rc.is_subdword() && operand >= 0) {
+               /* stride in bytes */
+               stride = get_subdword_operand_stride(ctx.program->gfx_level, instr, operand, rc);
+         } else if (rc.is_subdword()) {
+               get_subdword_definition_info(ctx.program, instr);
+         } else if (instr->isMIMG() && instr->mimg().d16 && ctx.program->gfx_level == GFX9) {
+               /* Workaround GFX9 hardware bug for D16 image instructions: FeatureImageGather4D16Bug
+                *
+                * The register use is not calculated correctly, and the hardware assumes a
+                * full dword per component. Don't use the last registers of the register file.
+                * Otherwise, the instruction will be skipped.
+                *
+                * https://reviews.llvm.org/D81172
+                */
+               bool imageGather4D16Bug = operand == -1 && rc == v2 && instr->mimg().dmask != 0xF;
+               assert(ctx.program->gfx_level == GFX9 && "Image D16 on GFX8 not supported.");
+
+               if (imageGather4D16Bug)
+                     bounds.size -= MAX2(rc.bytes() / 4 - ctx.num_linear_vgprs, 0);
+         } else if (instr_info.classes[(int)instr->opcode] == instr_class::valu_pseudo_scalar_trans) {
+               /* RDNA4 ISA doc, 7.10. Pseudo-scalar Transcendental ALU ops:
+                * - VCC may not be used as a destination
+                */
+               if (bounds.contains(vcc))
+                     bounds.size = vcc - bounds.lo();
+         }
 
-      if (!data_stride)
-         data_stride = stride;
+         if (!data_stride)
+               data_stride = stride;
    }
 
 private:
@@ -1010,6 +1012,38 @@ get_reg_simple(ra_ctx& ctx, const Regist
    }
 
    PhysRegIterator& rr_it = rc.type() == RegType::vgpr ? ctx.rr_vgpr_it : ctx.rr_sgpr_it;
+
+   /* GFX9 VGPR bank conflict mitigation using a stateful bank iterator */
+   if (rc.type() == RegType::vgpr && ctx.program->gfx_level == GFX9 && stride == 1 && size <= 4) {
+      auto is_free_win = [&](PhysRegInterval reg_win) {
+         for (PhysReg reg : reg_win) {
+            if (reg_file[reg] != 0 || ctx.war_hint[reg.reg()])
+               return false;
+         }
+         return true;
+      };
+
+      uint8_t start_bank = (ctx.rr_vgpr_bank + 2) & 3; /* Prefer opposite bank */
+      for (uint8_t bank_offset = 0; bank_offset < 4; ++bank_offset) {
+         uint8_t current_bank = (start_bank + bank_offset) & 3;
+         for (unsigned reg_base = bounds.lo().reg() & ~3u; reg_base + current_bank < bounds.hi().reg(); reg_base += 4) {
+            PhysReg start_reg(reg_base + current_bank);
+            if (start_reg < bounds.lo())
+               continue;
+            if (start_reg.reg() + size > bounds.hi().reg())
+               break;
+
+            PhysRegInterval reg_win = {start_reg, size};
+            if (is_free_win(reg_win)) {
+               ctx.rr_vgpr_bank = (uint8_t)((start_reg.reg() + size - 1) & 3);
+               adjust_max_used_regs(ctx, rc, reg_win.lo());
+               return reg_win.lo();
+            }
+         }
+      }
+   }
+
+
    if (stride == 1) {
       if (rr_it != bounds.begin() && bounds.contains(rr_it.reg)) {
          assert(bounds.begin() < rr_it);
@@ -1022,8 +1056,7 @@ get_reg_simple(ra_ctx& ctx, const Regist
       }
    }
 
-   auto is_free = [&](PhysReg reg_index)
-   { return reg_file[reg_index] == 0 && !ctx.war_hint[reg_index]; };
+   auto is_free = [&](PhysReg reg_index) { return reg_file[reg_index] == 0 && !ctx.war_hint[reg_index]; };
 
    for (PhysRegInterval reg_win = {bounds.lo(), size}; reg_win.hi() <= bounds.hi();
         reg_win += stride) {
@@ -1038,9 +1071,6 @@ get_reg_simple(ra_ctx& ctx, const Regist
       }
    }
 
-   /* do this late because using the upper bytes of a register can require
-    * larger instruction encodings or copies
-    * TODO: don't do this in situations where it doesn't benefit */
    if (rc.is_subdword()) {
       for (const std::pair<const uint32_t, std::array<uint32_t, 4>>& entry :
            reg_file.subdword_regs) {
@@ -1050,12 +1080,10 @@ get_reg_simple(ra_ctx& ctx, const Regist
 
          auto it = entry.second.begin();
          for (unsigned i = 0; i < 4; i += info.stride) {
-            /* check if there's a block of free bytes large enough to hold the register */
             bool reg_found =
                std::all_of(std::next(it, i), std::next(it, std::min(4u, i + rc.bytes())),
                            [](unsigned v) { return v == 0; });
 
-            /* check if also the neighboring reg is free if needed */
             if (reg_found && i + rc.bytes() > 4)
                reg_found = (reg_file[PhysReg{entry.first + 1}] == 0);
 
@@ -1103,19 +1131,46 @@ std::vector<unsigned>
 collect_vars(ra_ctx& ctx, RegisterFile& reg_file, const PhysRegInterval reg_interval)
 {
    std::vector<unsigned> ids = find_vars(ctx, reg_file, reg_interval);
-   std::sort(ids.begin(), ids.end(),
-             [&](unsigned a, unsigned b)
-             {
-                assignment& var_a = ctx.assignments[a];
-                assignment& var_b = ctx.assignments[b];
-                return var_a.rc.bytes() > var_b.rc.bytes() ||
-                       (var_a.rc.bytes() == var_b.rc.bytes() && var_a.reg < var_b.reg);
-             });
+
+   /* Fast path for small collections */
+   if (ids.size() <= 8) {
+      std::sort(ids.begin(), ids.end(),
+                [&](unsigned a, unsigned b) {
+                   assignment& var_a = ctx.assignments[a];
+                   assignment& var_b = ctx.assignments[b];
+                   return var_a.rc.bytes() > var_b.rc.bytes() ||
+                          (var_a.rc.bytes() == var_b.rc.bytes() && var_a.reg < var_b.reg);
+                });
+   } else {
+      /* Radix sort by size for cache-friendly access pattern */
+      std::array<std::vector<unsigned>, 17> buckets;  /* 0-16 dwords */
+
+      for (unsigned id : ids) {
+         unsigned bucket = std::min(16u, ctx.assignments[id].rc.size());
+         buckets[bucket].push_back(id);
+      }
+
+      ids.clear();
+      for (int b = 16; b >= 0; b--) {
+         if (buckets[b].empty()) {
+            continue;
+         }
+
+         /* Sort by register within bucket for sequential access */
+         std::sort(buckets[b].begin(), buckets[b].end(),
+                   [&](unsigned a, unsigned b) {
+                      return ctx.assignments[a].reg < ctx.assignments[b].reg;
+                   });
+
+         ids.insert(ids.end(), buckets[b].begin(), buckets[b].end());
+      }
+   }
 
    for (unsigned id : ids) {
       assignment& var = ctx.assignments[id];
       reg_file.clear(var.reg, var.rc);
    }
+
    return ids;
 }
 
@@ -2857,7 +2912,14 @@ vop3_can_use_vop2acc(ra_ctx& ctx, Instru
    switch (instr->opcode) {
    case aco_opcode::v_mad_f32:
    case aco_opcode::v_mad_f16:
-   case aco_opcode::v_mad_legacy_f16: break;
+   case aco_opcode::v_mad_legacy_f16:
+      break;
+   /* GFX9 v_fma_mix instructions are F16*F16+F32 -> F32, which can be encoded as v_fmac_f16. This is critical for Rapid Packed Math. */
+   case aco_opcode::v_fma_mixlo_f16:
+   case aco_opcode::v_fma_mixhi_f16:
+      if (ctx.program->gfx_level < GFX9)
+         return false;
+      break;
    case aco_opcode::v_fma_f32:
    case aco_opcode::v_pk_fma_f16:
    case aco_opcode::v_fma_f16:
@@ -2873,12 +2935,14 @@ vop3_can_use_vop2acc(ra_ctx& ctx, Instru
       if (!ctx.program->dev.has_fmac_legacy32)
          return false;
       break;
-   default: return false;
+   default:
+      return false;
    }
 
    if (!instr->operands[2].isOfType(RegType::vgpr) || !instr->operands[2].isKillBeforeDef() ||
-       (!instr->operands[0].isOfType(RegType::vgpr) && !instr->operands[1].isOfType(RegType::vgpr)))
+       (!instr->operands[0].isOfType(RegType::vgpr) && !instr->operands[1].isOfType(RegType::vgpr))) {
       return false;
+   }
 
    if (instr->isVOP3P()) {
       for (unsigned i = 0; i < 3; i++) {
@@ -2889,7 +2953,7 @@ vop3_can_use_vop2acc(ra_ctx& ctx, Instru
             return false;
 
          /* v_pk_fmac_f16 inline constants are replicated to hi bits starting with gfx11. */
-         if (instr->valu().opsel_hi[i] ==
+         if (instr->valu().opsel_hi[i] !=
              (instr->operands[i].isConstant() && ctx.program->gfx_level >= GFX11))
             return false;
       }
@@ -2913,30 +2977,42 @@ vop3_can_use_vop2acc(ra_ctx& ctx, Instru
 bool
 sop2_can_use_sopk(ra_ctx& ctx, Instruction* instr)
 {
-   if (instr->opcode != aco_opcode::s_add_i32 && instr->opcode != aco_opcode::s_add_u32 &&
-       instr->opcode != aco_opcode::s_mul_i32 && instr->opcode != aco_opcode::s_cselect_b32)
-      return false;
-
-   if (instr->opcode == aco_opcode::s_add_u32 && !instr->definitions[1].isKill())
-      return false;
+      if (instr->opcode != aco_opcode::s_add_i32 && instr->opcode != aco_opcode::s_add_u32 &&
+            instr->opcode != aco_opcode::s_mul_i32 && instr->opcode != aco_opcode::s_cselect_b32)
+            return false;
 
-   uint32_t literal_idx = 0;
+      if (instr->opcode == aco_opcode::s_add_u32 && !instr->definitions[1].isKill())
+            return false;
 
-   if (instr->opcode != aco_opcode::s_cselect_b32 && instr->operands[1].isLiteral())
-      literal_idx = 1;
+      uint32_t literal_idx = 0;
 
-   if (!instr->operands[!literal_idx].isTemp() || !instr->operands[!literal_idx].isKillBeforeDef())
-      return false;
+      /* s_cselect_b32 is D = SCC ? S0 : S1.
+       * s_cmovk_i32 is if(SCC) D = imm.
+       * This is only a valid transformation if S1 is the literal and S0 is the other operand.
+       * If S0 is the literal, the condition would need to be inverted, requiring another instruction. */
+      if (instr->opcode == aco_opcode::s_cselect_b32) {
+            if (!instr->operands[1].isLiteral())
+                  return false;
+            literal_idx = 1;
+      } else {
+            if (instr->operands[1].isLiteral())
+                  literal_idx = 1;
+            else if (instr->operands[0].isLiteral())
+                  literal_idx = 0;
+            else
+                  return false; /* Neither is a literal */
+      }
 
-   if (!instr->operands[literal_idx].isLiteral())
-      return false;
+      if (!instr->operands[!literal_idx].isTemp() || !instr->operands[!literal_idx].isKillBeforeDef())
+            return false;
 
-   const uint32_t i16_mask = 0xffff8000u;
-   uint32_t value = instr->operands[literal_idx].constantValue();
-   if ((value & i16_mask) && (value & i16_mask) != i16_mask)
-      return false;
+      /* Check if the literal value can fit into a 16-bit signed immediate */
+      const uint32_t i16_mask = 0xffff8000u;
+      uint32_t value = instr->operands[literal_idx].constantValue();
+      if ((value & i16_mask) != 0 && (value & i16_mask) != i16_mask)
+            return false;
 
-   return true;
+      return true;
 }
 
 void
@@ -3167,9 +3243,11 @@ optimize_encoding_vop2(ra_ctx& ctx, Regi
       assignment& affinity = ctx.assignments[ctx.assignments[def_id].affinity];
       if (affinity.assigned && affinity.reg != instr->operands[2].physReg() &&
           (!register_file.test(affinity.reg, instr->operands[2].bytes()) ||
-           std::any_of(instr->operands.begin(), instr->operands.end(), [&](Operand op)
-                       { return op.isKillBeforeDef() && op.physReg() == affinity.reg; })))
+           std::any_of(instr->operands.begin(), instr->operands.end(), [&](Operand op) {
+              return op.isKillBeforeDef() && op.physReg() == affinity.reg;
+           }))) {
          return;
+      }
    }
 
    if (!instr->operands[1].isOfType(RegType::vgpr))
@@ -3186,27 +3264,49 @@ optimize_encoding_vop2(ra_ctx& ctx, Regi
                             (unsigned)Format::VOP2);
    instr->valu().opsel_lo = 0;
    instr->valu().opsel_hi = 0;
+
    switch (instr->opcode) {
-   case aco_opcode::v_mad_f32: instr->opcode = aco_opcode::v_mac_f32; break;
-   case aco_opcode::v_fma_f32: instr->opcode = aco_opcode::v_fmac_f32; break;
+   case aco_opcode::v_mad_f32:
+      instr->opcode = aco_opcode::v_mac_f32;
+      break;
+   case aco_opcode::v_fma_f32:
+      instr->opcode = aco_opcode::v_fmac_f32;
+      break;
    case aco_opcode::v_mad_f16:
-   case aco_opcode::v_mad_legacy_f16: instr->opcode = aco_opcode::v_mac_f16; break;
-   case aco_opcode::v_fma_f16: instr->opcode = aco_opcode::v_fmac_f16; break;
-   case aco_opcode::v_pk_fma_f16: instr->opcode = aco_opcode::v_pk_fmac_f16; break;
-   case aco_opcode::v_dot4_i32_i8: instr->opcode = aco_opcode::v_dot4c_i32_i8; break;
-   case aco_opcode::v_mad_legacy_f32: instr->opcode = aco_opcode::v_mac_legacy_f32; break;
-   case aco_opcode::v_fma_legacy_f32: instr->opcode = aco_opcode::v_fmac_legacy_f32; break;
-   default: break;
+   case aco_opcode::v_mad_legacy_f16:
+      instr->opcode = aco_opcode::v_mac_f16;
+      break;
+   case aco_opcode::v_fma_f16:
+   case aco_opcode::v_fma_mixlo_f16:
+   case aco_opcode::v_fma_mixhi_f16:
+      instr->opcode = aco_opcode::v_fmac_f16;
+      break;
+   case aco_opcode::v_pk_fma_f16:
+      instr->opcode = aco_opcode::v_pk_fmac_f16;
+      break;
+   case aco_opcode::v_dot4_i32_i8:
+      instr->opcode = aco_opcode::v_dot4c_i32_i8;
+      break;
+   case aco_opcode::v_mad_legacy_f32:
+      instr->opcode = aco_opcode::v_mac_legacy_f32;
+      break;
+   case aco_opcode::v_fma_legacy_f32:
+      instr->opcode = aco_opcode::v_fmac_legacy_f32;
+      break;
+   default:
+      break;
    }
 }
 
 void
 optimize_encoding_sopk(ra_ctx& ctx, RegisterFile& register_file, aco_ptr<Instruction>& instr)
 {
-   /* try to optimize sop2 with literal source to sopk */
    if (!sop2_can_use_sopk(ctx, instr.get()))
       return;
-   unsigned literal_idx = instr->operands[1].isLiteral();
+
+   unsigned literal_idx = instr->operands[1].isLiteral() ? 1 : 0;
+   if (instr->opcode == aco_opcode::s_cselect_b32)
+      literal_idx = 1;
 
    PhysReg op_reg = instr->operands[!literal_idx].physReg();
    if (!is_sgpr_writable_without_side_effects(ctx.program->gfx_level, op_reg))
@@ -3383,41 +3483,55 @@ assign_tied_definitions(ra_ctx& ctx, aco
 }
 
 void
-emit_parallel_copy_internal(ra_ctx& ctx, std::vector<parallelcopy>& parallelcopy,
+emit_parallel_copy_internal(ra_ctx& ctx, std::vector<parallelcopy>& copies,
                             aco_ptr<Instruction>& instr,
                             std::vector<aco_ptr<Instruction>>& instructions, bool temp_in_scc,
                             RegisterFile& register_file)
 {
-   if (parallelcopy.empty())
+   if (copies.empty())
+      return;
+
+   /* Filter out redundant copies */
+   std::vector<struct parallelcopy> filtered_copies;
+   filtered_copies.reserve(copies.size());
+   for (const auto& copy : copies) {
+      /* Skip identity copies */
+      if (copy.op.isTemp() && copy.op.physReg() == copy.def.physReg() && copy.op.size() == copy.def.size())
+         continue;
+      filtered_copies.push_back(copy);
+   }
+
+   if (filtered_copies.empty()) {
+      copies.clear();
       return;
+   }
+
 
    aco_ptr<Instruction> pc;
-   pc.reset(create_instruction(aco_opcode::p_parallelcopy, Format::PSEUDO, parallelcopy.size(),
-                               parallelcopy.size()));
+   pc.reset(create_instruction(aco_opcode::p_parallelcopy, Format::PSEUDO, filtered_copies.size(),
+                               filtered_copies.size()));
    bool linear_vgpr = false;
    bool may_swap_sgprs = false;
    std::bitset<256> sgpr_operands;
-   for (unsigned i = 0; i < parallelcopy.size(); i++) {
-      linear_vgpr |= parallelcopy[i].op.regClass().is_linear_vgpr();
+   for (unsigned i = 0; i < filtered_copies.size(); i++) {
+      linear_vgpr |= filtered_copies[i].op.regClass().is_linear_vgpr();
 
-      if (!may_swap_sgprs && parallelcopy[i].op.isTemp() &&
-          parallelcopy[i].op.getTemp().type() == RegType::sgpr) {
-         unsigned op_reg = parallelcopy[i].op.physReg().reg();
-         unsigned def_reg = parallelcopy[i].def.physReg().reg();
-         for (unsigned j = 0; j < parallelcopy[i].op.size(); j++) {
+      if (!may_swap_sgprs && filtered_copies[i].op.isTemp() &&
+          filtered_copies[i].op.getTemp().type() == RegType::sgpr) {
+         unsigned op_reg = filtered_copies[i].op.physReg().reg();
+         unsigned def_reg = filtered_copies[i].def.physReg().reg();
+         for (unsigned j = 0; j < filtered_copies[i].op.size(); j++) {
             sgpr_operands.set(op_reg + j);
             if (sgpr_operands.test(def_reg + j))
                may_swap_sgprs = true;
          }
       }
 
-      pc->operands[i] = parallelcopy[i].op;
-      pc->definitions[i] = parallelcopy[i].def;
+      pc->operands[i] = filtered_copies[i].op;
+      pc->definitions[i] = filtered_copies[i].def;
       assert(pc->operands[i].size() == pc->definitions[i].size());
 
-      if (parallelcopy[i].copy_kill < 0) {
-         /* it might happen that the operand is already renamed. we have to restore the
-          * original name. */
+      if (filtered_copies[i].copy_kill < 0) {
          auto it =
             ctx.orig_names.find(pc->operands[i].tempId());
          Temp orig = it != ctx.orig_names.end() ? it->second : pc->operands[i].getTemp();
@@ -3427,7 +3541,6 @@ emit_parallel_copy_internal(ra_ctx& ctx,
    }
 
    if (temp_in_scc && (may_swap_sgprs || linear_vgpr)) {
-      /* disable definitions and re-enable operands */
       RegisterFile tmp_file(register_file);
       for (const Definition& def : instr->definitions) {
          if (def.isTemp() && !def.isKill())
@@ -3446,7 +3559,7 @@ emit_parallel_copy_internal(ra_ctx& ctx,
 
    instructions.emplace_back(std::move(pc));
 
-   parallelcopy.clear();
+   copies.clear();
 }
 
 void
