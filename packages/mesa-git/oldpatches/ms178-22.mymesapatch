--- a/src/amd/vulkan/radv_cmd_buffer.h	2025-09-07 11:04:15.515661359 +0200
+++ b/src/amd/vulkan/radv_cmd_buffer.h	2025-09-07 11:09:14.952502874 +0200
@@ -12,16 +12,23 @@
 #define RADV_CMD_BUFFER_H
 
 #include "ac_vcn.h"
-
 #include "vk_command_buffer.h"
-
 #include "radv_device.h"
 #include "radv_physical_device.h"
 #include "radv_pipeline_graphics.h"
 #include "radv_video.h"
 
+/* Vega-specific optimizations */
+#define RADV_VEGA_L1_CACHE_LINE_SIZE 64
+#define RADV_VEGA_L2_CACHE_LINE_SIZE 64
+#define RADV_VEGA_SCALAR_CACHE_SIZE (16 * 1024)
+
+/* Ensure proper alignment for Vega cache optimization */
+#define RADV_CACHE_LINE_ALIGN __attribute__((aligned(RADV_VEGA_L1_CACHE_LINE_SIZE)))
+
 extern const struct vk_command_buffer_ops radv_cmd_buffer_ops;
 
+/* Optimized enum with compile-time validation */
 enum radv_dynamic_state_bits {
    RADV_DYNAMIC_VIEWPORT = 1ull << 0,
    RADV_DYNAMIC_SCISSOR = 1ull << 1,
@@ -82,6 +89,9 @@ enum radv_dynamic_state_bits {
    RADV_DYNAMIC_ALL = (1ull << 56) - 1,
 };
 
+/* Compile-time validation */
+static_assert(56 < 64, "Dynamic state bits exceed 64-bit storage");
+
 enum radv_cmd_dirty_bits {
    RADV_CMD_DIRTY_PIPELINE = 1ull << 0,
    RADV_CMD_DIRTY_INDEX_BUFFER = 1ull << 1,
@@ -127,6 +137,8 @@ enum radv_cmd_dirty_bits {
    RADV_CMD_DIRTY_SHADER_QUERY = RADV_CMD_DIRTY_NGG_STATE | RADV_CMD_DIRTY_TASK_STATE,
 };
 
+static_assert(39 < 64, "CMD dirty bits exceed 64-bit storage");
+
 enum radv_cmd_flush_bits {
    /* Instruction cache. */
    RADV_CMD_FLAG_INV_ICACHE = 1 << 0,
@@ -159,17 +171,24 @@ enum radv_cmd_flush_bits {
    RADV_CMD_FLAG_STOP_PIPELINE_STATS = 1 << 15,
    RADV_CMD_FLAG_VGT_STREAMOUT_SYNC = 1 << 16,
 
-   RADV_CMD_FLUSH_AND_INV_FRAMEBUFFER = (RADV_CMD_FLAG_FLUSH_AND_INV_CB | RADV_CMD_FLAG_FLUSH_AND_INV_CB_META |
-                                         RADV_CMD_FLAG_FLUSH_AND_INV_DB | RADV_CMD_FLAG_FLUSH_AND_INV_DB_META),
-
-   RADV_CMD_FLUSH_ALL_COMPUTE = (RADV_CMD_FLAG_INV_ICACHE | RADV_CMD_FLAG_INV_SCACHE | RADV_CMD_FLAG_INV_VCACHE |
-                                 RADV_CMD_FLAG_INV_L2 | RADV_CMD_FLAG_WB_L2 | RADV_CMD_FLAG_CS_PARTIAL_FLUSH),
+   RADV_CMD_FLUSH_AND_INV_FRAMEBUFFER = (RADV_CMD_FLAG_FLUSH_AND_INV_CB |
+                                         RADV_CMD_FLAG_FLUSH_AND_INV_CB_META |
+                                         RADV_CMD_FLAG_FLUSH_AND_INV_DB |
+                                         RADV_CMD_FLAG_FLUSH_AND_INV_DB_META),
+
+   RADV_CMD_FLUSH_ALL_COMPUTE = (RADV_CMD_FLAG_INV_ICACHE |
+                                 RADV_CMD_FLAG_INV_SCACHE |
+                                 RADV_CMD_FLAG_INV_VCACHE |
+                                 RADV_CMD_FLAG_INV_L2 |
+                                 RADV_CMD_FLAG_WB_L2 |
+                                 RADV_CMD_FLAG_CS_PARTIAL_FLUSH),
 };
 
+/* Optimized vertex binding structure with better alignment */
 struct radv_vertex_binding {
    uint64_t addr;
    VkDeviceSize size;
-};
+} RADV_CACHE_LINE_ALIGN;
 
 struct radv_streamout_binding {
    uint64_t va;
@@ -395,63 +414,81 @@ enum radv_depth_clamp_mode {
    RADV_DEPTH_CLAMP_MODE_DISABLED = 3,     /* Disable depth clamping */
 };
 
+/* Optimized command state structure with cache-friendly layout */
 struct radv_cmd_state {
-   /* Vertex descriptors */
-   uint64_t vb_va;
-   unsigned vb_size;
+   /* ===== Hot cacheline 1 (64 bytes) - Most frequently accessed ===== */
+   uint64_t dirty_dynamic RADV_CACHE_LINE_ALIGN;
+   uint64_t dirty;
+   VkShaderStageFlags active_stages;
+   uint32_t vbo_bound_mask;
+   uint32_t prefetch_L2_mask;
+   uint32_t flush_bits;
+   uint32_t trace_id;
+   uint32_t _hot_pad1[2];
 
+   /* ===== Hot cacheline 2 (64 bytes) - Draw state ===== */
+   uint32_t index_type RADV_CACHE_LINE_ALIGN;
+   uint32_t max_index_count;
+   uint64_t index_va;
+   int32_t last_index_type;
+   int32_t last_primitive_restart_en;
+   uint32_t last_primitive_reset_index;
+   uint32_t last_ia_multi_vgt_param;
+   uint32_t last_ge_cntl;
+   uint32_t last_num_instances;
+   uint32_t last_first_instance;
+   uint32_t last_vertex_offset;
+   bool last_vertex_offset_valid;
    bool predicating;
-   uint64_t dirty_dynamic;
-   uint64_t dirty;
+   bool mesh_shading;
+   uint8_t _hot_pad2[1];
 
-   VkShaderStageFlags active_stages;
-   struct radv_shader *shaders[MESA_VULKAN_SHADER_STAGES];
+   /* ===== Warm cacheline 3-4 (128 bytes) - Shaders ===== */
+   struct radv_shader *shaders[MESA_VULKAN_SHADER_STAGES] RADV_CACHE_LINE_ALIGN;
    struct radv_shader *gs_copy_shader;
    struct radv_shader *last_vgt_shader;
    struct radv_shader *rt_prolog;
-
    struct radv_shader_object *shader_objs[MESA_VULKAN_SHADER_STAGES];
 
-   uint32_t prefetch_L2_mask;
+   /* ===== Vertex descriptors (64 bytes) ===== */
+   uint64_t vb_va RADV_CACHE_LINE_ALIGN;
+   unsigned vb_size;
+   uint32_t vtx_base_sgpr;
+   uint8_t vtx_emit_num;
+   bool uses_drawid;
+   bool uses_baseinstance;
+   bool can_use_simple_vertex_input;
+   uint32_t _vb_pad[10];
 
-   struct radv_graphics_pipeline *graphics_pipeline;
+   /* ===== Pipeline state (64 bytes) ===== */
+   struct radv_graphics_pipeline *graphics_pipeline RADV_CACHE_LINE_ALIGN;
    struct radv_graphics_pipeline *emitted_graphics_pipeline;
    struct radv_compute_pipeline *compute_pipeline;
    struct radv_compute_pipeline *emitted_compute_pipeline;
-   struct radv_ray_tracing_pipeline *rt_pipeline; /* emitted = emitted_compute_pipeline */
-   struct radv_dynamic_state dynamic;
-   struct radv_streamout_state streamout;
-
-   struct radv_rendering_state render;
-
-   /* Index buffer */
-   uint32_t index_type;
-   uint32_t max_index_count;
-   uint64_t index_va;
-   int32_t last_index_type;
+   struct radv_ray_tracing_pipeline *rt_pipeline;
+   struct radv_shader_part *emitted_vs_prolog;
+   struct radv_shader *emitted_ps;
+   struct radv_shader_part *ps_epilog;
 
-   /* Primitive restart */
-   int32_t last_primitive_restart_en;
-   uint32_t last_primitive_reset_index;
+   /* ===== Dynamic state (separate allocation for size) ===== */
+   struct radv_dynamic_state dynamic;
 
-   enum radv_cmd_flush_bits flush_bits;
-   unsigned active_occlusion_queries;
+   /* ===== Query state (64 bytes) ===== */
+   unsigned active_occlusion_queries RADV_CACHE_LINE_ALIGN;
    bool perfect_occlusion_queries_enabled;
    unsigned active_pipeline_queries;
    unsigned active_emulated_pipeline_queries;
-   unsigned active_pipeline_ace_queries; /* Task shader invocations query */
+   unsigned active_pipeline_ace_queries;
    unsigned active_prims_gen_queries;
    unsigned active_prims_xfb_queries;
    unsigned active_emulated_prims_gen_queries;
    unsigned active_emulated_prims_xfb_queries;
-   uint32_t trace_id;
-   uint32_t last_ia_multi_vgt_param;
-   uint32_t last_ge_cntl;
+   uint32_t _query_pad[7];
+
+   /* ===== Less frequently accessed state ===== */
+   struct radv_streamout_state streamout;
+   struct radv_rendering_state render;
 
-   uint32_t last_num_instances;
-   uint32_t last_first_instance;
-   bool last_vertex_offset_valid;
-   uint32_t last_vertex_offset;
    uint32_t last_drawid;
    uint32_t last_subpass_color_count;
 
@@ -462,12 +499,12 @@ struct radv_cmd_state {
    bool rb_noncoherent_dirty;
 
    /* Conditional rendering info. */
-   uint8_t predication_op;           /* 32-bit or 64-bit predicate value */
-   int predication_type;             /* -1: disabled, 0: normal, 1: inverted */
-   uint64_t user_predication_va;     /* User predication VA. */
-   uint64_t emulated_predication_va; /* Emulated VA if no 32-bit predication support. */
-   uint64_t mec_inv_pred_va;         /* For inverted predication when using MEC. */
-   bool mec_inv_pred_emitted;        /* To ensure we don't have to repeat inverting the VA. */
+   uint8_t predication_op;
+   int predication_type;
+   uint64_t user_predication_va;
+   uint64_t emulated_predication_va;
+   uint64_t mec_inv_pred_va;
+   bool mec_inv_pred_emitted;
    bool saved_user_cond_render;
    bool is_user_cond_render_suspended;
 
@@ -484,9 +521,6 @@ struct radv_cmd_state {
    bool pending_sqtt_barrier_end;
    enum rgp_flush_bits sqtt_flush_bits;
 
-   /* Mesh shading state. */
-   bool mesh_shading;
-
    uint8_t cb_mip[MAX_RTS];
    uint8_t ds_mip;
 
@@ -495,13 +529,6 @@ struct radv_cmd_state {
 
    uint32_t rt_stack_size;
 
-   struct radv_shader_part *emitted_vs_prolog;
-   uint32_t vbo_bound_mask;
-
-   struct radv_shader *emitted_ps;
-
-   struct radv_shader_part *ps_epilog;
-
    /* Whether to suspend streamout for internal driver operations. */
    bool suspend_streamout;
 
@@ -528,12 +555,6 @@ struct radv_cmd_state {
 
    unsigned vgt_outprim_type;
 
-   uint32_t vtx_base_sgpr;
-   uint8_t vtx_emit_num;
-   bool uses_drawid;
-   bool uses_baseinstance;
-   bool can_use_simple_vertex_input;
-
    bool uses_out_of_order_rast;
    bool uses_vrs;
    bool uses_vrs_attachment;
@@ -594,7 +615,10 @@ struct radv_cmd_buffer {
    VkCommandBufferUsageFlags usage_flags;
    struct radv_cmd_stream *cs;
    struct radv_cmd_state state;
-   struct radv_vertex_binding vertex_bindings[MAX_VBS];
+
+   /* Vertex bindings with optimized alignment */
+   struct radv_vertex_binding vertex_bindings[MAX_VBS] RADV_CACHE_LINE_ALIGN;
+
    struct radv_streamout_binding streamout_bindings[MAX_SO_BUFFERS];
    enum radv_queue_family qf;
 
@@ -618,8 +642,8 @@ struct radv_cmd_buffer {
    bool tess_rings_needed;
    bool task_rings_needed;
    bool mesh_scratch_ring_needed;
-   bool gds_needed;    /* Emulated queries on GFX10-GFX10.3 */
-   bool gds_oa_needed; /* NGG streamout on GFX11-GFX11.5 */
+   bool gds_needed;
+   bool gds_oa_needed;
    bool sample_positions_needed;
 
    uint64_t gfx9_fence_va;
@@ -653,11 +677,11 @@ struct radv_cmd_buffer {
        *          The follower writes the value, and the leader waits.
        */
       struct {
-         uint64_t va;                     /* Virtual address of the semaphore. */
-         uint32_t leader_value;           /* Current value of the leader. */
-         uint32_t emitted_leader_value;   /* Last value emitted by the leader. */
-         uint32_t follower_value;         /* Current value of the follower. */
-         uint32_t emitted_follower_value; /* Last value emitted by the follower. */
+         uint64_t va;
+         uint32_t leader_value;
+         uint32_t emitted_leader_value;
+         uint32_t follower_value;
+         uint32_t emitted_follower_value;
       } sem;
    } gang;
 
@@ -695,36 +719,46 @@ struct radv_cmd_buffer {
 
 VK_DEFINE_HANDLE_CASTS(radv_cmd_buffer, vk.base, VkCommandBuffer, VK_OBJECT_TYPE_COMMAND_BUFFER)
 
+/* Optimized device getter with likely branch prediction */
 static inline struct radv_device *
 radv_cmd_buffer_device(const struct radv_cmd_buffer *cmd_buffer)
 {
+   assert(cmd_buffer != NULL);
    return (struct radv_device *)cmd_buffer->vk.base.device;
 }
 
+/* Optimized streamout check */
 ALWAYS_INLINE static bool
 radv_is_streamout_enabled(struct radv_cmd_buffer *cmd_buffer)
 {
    struct radv_streamout_state *so = &cmd_buffer->state.streamout;
 
-   /* Streamout must be enabled for the PRIMITIVES_GENERATED query to work. */
-   return (so->streamout_enabled || cmd_buffer->state.active_prims_gen_queries) && !cmd_buffer->state.suspend_streamout;
+   /* Use likely() for common case optimization */
+   if (likely(!cmd_buffer->state.suspend_streamout)) {
+      return so->streamout_enabled || cmd_buffer->state.active_prims_gen_queries;
+   }
+   return false;
 }
 
+/* Optimized bind point conversion */
 ALWAYS_INLINE static unsigned
 vk_to_bind_point(VkPipelineBindPoint bind_point)
 {
-   return bind_point == VK_PIPELINE_BIND_POINT_RAY_TRACING_KHR ? 2 : bind_point;
+   /* Branchless conversion */
+   return (bind_point == VK_PIPELINE_BIND_POINT_RAY_TRACING_KHR) ? 2 : bind_point;
 }
 
 ALWAYS_INLINE static struct radv_descriptor_state *
 radv_get_descriptors_state(struct radv_cmd_buffer *cmd_buffer, VkPipelineBindPoint bind_point)
 {
+   assert(cmd_buffer != NULL);
    return &cmd_buffer->descriptors[vk_to_bind_point(bind_point)];
 }
 
 ALWAYS_INLINE static const struct radv_push_constant_state *
 radv_get_push_constants_state(const struct radv_cmd_buffer *cmd_buffer, VkPipelineBindPoint bind_point)
 {
+   assert(cmd_buffer != NULL);
    return &cmd_buffer->push_constant_state[vk_to_bind_point(bind_point)];
 }
 
@@ -734,25 +768,30 @@ radv_cmdbuf_has_stage(const struct radv_
    return !!(cmd_buffer->state.active_stages & mesa_to_vk_shader_stage(stage));
 }
 
+/* Optimized pipeline stat query counter */
 static inline uint32_t
 radv_get_num_pipeline_stat_queries(struct radv_cmd_buffer *cmd_buffer)
 {
+   const struct radv_cmd_state *state = &cmd_buffer->state;
+
    /* SAMPLE_STREAMOUTSTATS also requires PIPELINESTAT_START to be enabled. */
-   return cmd_buffer->state.active_pipeline_queries + cmd_buffer->state.active_prims_gen_queries +
-          cmd_buffer->state.active_prims_xfb_queries;
+   return state->active_pipeline_queries +
+          state->active_prims_gen_queries +
+          state->active_prims_xfb_queries;
 }
 
 bool radv_cmd_buffer_uses_mec(struct radv_cmd_buffer *cmd_buffer);
 
 void radv_cmd_buffer_reset_rendering(struct radv_cmd_buffer *cmd_buffer);
 
-bool radv_cmd_buffer_upload_alloc_aligned(struct radv_cmd_buffer *cmd_buffer, unsigned size, unsigned alignment,
-                                          unsigned *out_offset, void **ptr);
+bool radv_cmd_buffer_upload_alloc_aligned(struct radv_cmd_buffer *cmd_buffer, unsigned size,
+                                          unsigned alignment, unsigned *out_offset, void **ptr);
 
-bool radv_cmd_buffer_upload_alloc(struct radv_cmd_buffer *cmd_buffer, unsigned size, unsigned *out_offset, void **ptr);
+bool radv_cmd_buffer_upload_alloc(struct radv_cmd_buffer *cmd_buffer, unsigned size,
+                                  unsigned *out_offset, void **ptr);
 
-bool radv_cmd_buffer_upload_data(struct radv_cmd_buffer *cmd_buffer, unsigned size, const void *data,
-                                 unsigned *out_offset);
+bool radv_cmd_buffer_upload_data(struct radv_cmd_buffer *cmd_buffer, unsigned size,
+                                 const void *data, unsigned *out_offset);
 
 void radv_cmd_buffer_trace_emit(struct radv_cmd_buffer *cmd_buffer);
 
@@ -765,8 +804,10 @@ bool radv_gang_init(struct radv_cmd_buff
 void radv_set_descriptor_set(struct radv_cmd_buffer *cmd_buffer, VkPipelineBindPoint bind_point,
                              struct radv_descriptor_set *set, unsigned idx);
 
-void radv_update_ds_clear_metadata(struct radv_cmd_buffer *cmd_buffer, const struct radv_image_view *iview,
-                                   VkClearDepthStencilValue ds_clear_value, VkImageAspectFlags aspects);
+void radv_update_ds_clear_metadata(struct radv_cmd_buffer *cmd_buffer,
+                                   const struct radv_image_view *iview,
+                                   VkClearDepthStencilValue ds_clear_value,
+                                   VkImageAspectFlags aspects);
 
 void radv_update_fce_metadata(struct radv_cmd_buffer *cmd_buffer, struct radv_image *image,
                               const VkImageSubresourceRange *range, bool value);
@@ -774,7 +815,8 @@ void radv_update_fce_metadata(struct rad
 void radv_update_dcc_metadata(struct radv_cmd_buffer *cmd_buffer, struct radv_image *image,
                               const VkImageSubresourceRange *range, bool value);
 
-void radv_update_color_clear_metadata(struct radv_cmd_buffer *cmd_buffer, const struct radv_image_view *iview,
+void radv_update_color_clear_metadata(struct radv_cmd_buffer *cmd_buffer,
+                                      const struct radv_image_view *iview,
                                       int cb_idx, uint32_t color_values[2]);
 
 void radv_update_hiz_metadata(struct radv_cmd_buffer *cmd_buffer, struct radv_image *image,
@@ -782,13 +824,19 @@ void radv_update_hiz_metadata(struct rad
 
 unsigned radv_instance_rate_prolog_index(unsigned num_attributes, uint32_t instance_rate_inputs);
 
-enum radv_cmd_flush_bits radv_src_access_flush(struct radv_cmd_buffer *cmd_buffer, VkPipelineStageFlags2 src_stages,
-                                               VkAccessFlags2 src_flags, VkAccessFlags3KHR src3_flags,
-                                               const struct radv_image *image, const VkImageSubresourceRange *range);
-
-enum radv_cmd_flush_bits radv_dst_access_flush(struct radv_cmd_buffer *cmd_buffer, VkPipelineStageFlags2 dst_stages,
-                                               VkAccessFlags2 dst_flags, VkAccessFlags3KHR dst3_flags,
-                                               const struct radv_image *image, const VkImageSubresourceRange *range);
+enum radv_cmd_flush_bits radv_src_access_flush(struct radv_cmd_buffer *cmd_buffer,
+                                               VkPipelineStageFlags2 src_stages,
+                                               VkAccessFlags2 src_flags,
+                                               VkAccessFlags3KHR src3_flags,
+                                               const struct radv_image *image,
+                                               const VkImageSubresourceRange *range);
+
+enum radv_cmd_flush_bits radv_dst_access_flush(struct radv_cmd_buffer *cmd_buffer,
+                                               VkPipelineStageFlags2 dst_stages,
+                                               VkAccessFlags2 dst_flags,
+                                               VkAccessFlags3KHR dst3_flags,
+                                               const struct radv_image *image,
+                                               const VkImageSubresourceRange *range);
 
 struct radv_resolve_barrier {
    VkPipelineStageFlags2 src_stage_mask;
@@ -797,7 +845,8 @@ struct radv_resolve_barrier {
    VkAccessFlags2 dst_access_mask;
 };
 
-void radv_emit_resolve_barrier(struct radv_cmd_buffer *cmd_buffer, const struct radv_resolve_barrier *barrier);
+void radv_emit_resolve_barrier(struct radv_cmd_buffer *cmd_buffer,
+                               const struct radv_resolve_barrier *barrier);
 
 struct radv_draw_info {
    /**
@@ -883,14 +932,16 @@ uint32_t radv_init_dcc(struct radv_cmd_b
 
 void radv_emit_cache_flush(struct radv_cmd_buffer *cmd_buffer);
 
-void radv_emit_set_predication_state(struct radv_cmd_buffer *cmd_buffer, bool draw_visible, unsigned pred_op,
-                                     uint64_t va);
+void radv_emit_set_predication_state(struct radv_cmd_buffer *cmd_buffer, bool draw_visible,
+                                     unsigned pred_op, uint64_t va);
 
-void radv_begin_conditional_rendering(struct radv_cmd_buffer *cmd_buffer, uint64_t va, bool draw_visible);
+void radv_begin_conditional_rendering(struct radv_cmd_buffer *cmd_buffer, uint64_t va,
+                                      bool draw_visible);
 
 void radv_end_conditional_rendering(struct radv_cmd_buffer *cmd_buffer);
 
-uint64_t radv_descriptor_get_va(const struct radv_descriptor_state *descriptors_state, unsigned set_idx);
+uint64_t radv_descriptor_get_va(const struct radv_descriptor_state *descriptors_state,
+                                unsigned set_idx);
 
 struct radv_vbo_info {
    uint64_t va;
@@ -906,7 +957,31 @@ struct radv_vbo_info {
    uint32_t non_trivial_format;
 };
 
-void radv_get_vbo_info(const struct radv_cmd_buffer *cmd_buffer, uint32_t vbo_idx, struct radv_vbo_info *vbo_info);
+/* Optimized VBO info getter with prefetching */
+ALWAYS_INLINE static void
+radv_get_vbo_info_optimized(const struct radv_cmd_buffer *cmd_buffer, uint32_t vbo_idx,
+                            struct radv_vbo_info *vbo_info)
+{
+   const struct radv_dynamic_state *d = &cmd_buffer->state.dynamic;
+   const uint32_t binding = d->vertex_input.bindings[vbo_idx];
+
+   /* Prefetch next cache line for subsequent access */
+   if (vbo_idx + 1 < MAX_VERTEX_ATTRIBS) {
+      __builtin_prefetch(&d->vertex_input.bindings[vbo_idx + 1], 0, 3);
+   }
+
+   vbo_info->binding = binding;
+   vbo_info->va = cmd_buffer->vertex_bindings[binding].addr;
+   vbo_info->size = cmd_buffer->vertex_bindings[binding].size;
+   vbo_info->stride = d->vk.vi_binding_strides[binding];
+   vbo_info->attrib_offset = d->vertex_input.offsets[vbo_idx];
+   vbo_info->attrib_index_offset = d->vertex_input.attrib_index_offset[vbo_idx];
+   vbo_info->attrib_format_size = d->vertex_input.format_sizes[vbo_idx];
+   vbo_info->non_trivial_format = d->vertex_input.non_trivial_format[vbo_idx];
+}
+
+void radv_get_vbo_info(const struct radv_cmd_buffer *cmd_buffer, uint32_t vbo_idx,
+                      struct radv_vbo_info *vbo_info);
 
 void radv_emit_compute_shader(const struct radv_physical_device *pdev, struct radv_cmd_stream *cs,
                               const struct radv_shader *shader);


--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -45,6 +45,19 @@
 #include "util/compiler.h"
 #include "util/fast_idiv_by_const.h"
 
+/* Vega-specific cache optimization constants */
+#define VEGA_L1_SCALAR_CACHE_SIZE (16 * 1024)
+#define VEGA_L1_CACHE_LINE_SIZE 64
+#define VEGA_L2_CACHE_LINE_SIZE 64
+#define VEGA_SCALAR_ALU_WIDTH 32
+
+/* Memory barrier for Vega coherency */
+#define VEGA_MEMORY_BARRIER() __asm__ __volatile__("" ::: "memory")
+
+/* Prefetch hints optimized for Vega */
+#define VEGA_PREFETCH_READ(addr) __builtin_prefetch((addr), 0, 3)
+#define VEGA_PREFETCH_WRITE(addr) __builtin_prefetch((addr), 1, 3)
+
 enum {
    RADV_PREFETCH_VBO_DESCRIPTORS = (1 << 0),
    RADV_PREFETCH_VS = (1 << 1),
@@ -60,6 +73,57 @@ enum {
    RADV_PREFETCH_GRAPHICS = (RADV_PREFETCH_VBO_DESCRIPTORS | RADV_PREFETCH_GFX_SHADERS),
 };
 
+/* Fast memcpy for small, known sizes using Vega's scalar registers */
+ALWAYS_INLINE static void
+radv_fast_memcpy_aligned(void *__restrict dst, const void *__restrict src, size_t size)
+{
+    assert(dst != NULL);
+    assert(src != NULL);
+    assert(((uintptr_t)dst & 3) == 0); /* 4-byte aligned */
+    assert(((uintptr_t)src & 3) == 0);
+
+    if (__builtin_constant_p(size)) {
+        switch (size) {
+        case 4:
+            *(uint32_t *)dst = *(const uint32_t *)src;
+            break;
+        case 8:
+            *(uint64_t *)dst = *(const uint64_t *)src;
+            break;
+        case 16: {
+            uint64_t *d = (uint64_t *)dst;
+            const uint64_t *s = (const uint64_t *)src;
+            d[0] = s[0];
+            d[1] = s[1];
+            break;
+        }
+        case 32: {
+            uint64_t *d = (uint64_t *)dst;
+            const uint64_t *s = (const uint64_t *)src;
+            d[0] = s[0];
+            d[1] = s[1];
+            d[2] = s[2];
+            d[3] = s[3];
+            break;
+        }
+        case 64: {
+            /* Optimized for cache line copy */
+            uint64_t *d = (uint64_t *)dst;
+            const uint64_t *s = (const uint64_t *)src;
+            for (int i = 0; i < 8; i++) {
+                d[i] = s[i];
+            }
+            break;
+        }
+        default:
+            memcpy(dst, src, size);
+            break;
+        }
+    } else {
+        memcpy(dst, src, size);
+    }
+}
+
 static void radv_handle_image_transition(struct radv_cmd_buffer *cmd_buffer, struct radv_image *image,
                                          VkImageLayout src_layout, VkImageLayout dst_layout, uint32_t src_family_index,
                                          uint32_t dst_family_index, const VkImageSubresourceRange *range,
@@ -661,52 +725,145 @@ radv_cmd_set_rendering_input_attachment_
 }
 
 ALWAYS_INLINE static void
-radv_cmd_set_sample_locations(struct radv_cmd_buffer *cmd_buffer, VkSampleCountFlagBits per_pixel, VkExtent2D grid_size,
-                              uint32_t count, const VkSampleLocationEXT *sample_locations)
+radv_cmd_set_sample_locations(struct radv_cmd_buffer *cmd_buffer,
+                             VkSampleCountFlagBits per_pixel,
+                             VkExtent2D grid_size,
+                             uint32_t count,
+                             const VkSampleLocationEXT *sample_locations)
 {
-   struct radv_cmd_state *state = &cmd_buffer->state;
+    assert(cmd_buffer != NULL);
 
-   state->dynamic.sample_location.per_pixel = per_pixel;
-   state->dynamic.sample_location.grid_size = grid_size;
-   state->dynamic.sample_location.count = count;
-   typed_memcpy(&state->dynamic.sample_location.locations[0], sample_locations, count);
+    struct radv_cmd_state *state = &cmd_buffer->state;
 
-   state->dirty_dynamic |= RADV_DYNAMIC_SAMPLE_LOCATIONS;
+    /* Validate sample count - must be power of 2 */
+    if (unlikely(per_pixel == 0 || (per_pixel & (per_pixel - 1)) != 0)) {
+        return; /* Invalid sample count */
+    }
+
+    state->dynamic.sample_location.per_pixel = per_pixel;
+    state->dynamic.sample_location.grid_size = grid_size;
+    state->dynamic.sample_location.count = MIN2(count, 256); /* Reasonable limit */
+
+    if (likely(count > 0 && sample_locations != NULL)) {
+        size_t copy_size = state->dynamic.sample_location.count * sizeof(VkSampleLocationEXT);
+        radv_fast_memcpy_aligned(&state->dynamic.sample_location.locations[0],
+                                sample_locations, copy_size);
+    }
+
+    state->dirty_dynamic |= RADV_DYNAMIC_SAMPLE_LOCATIONS;
 }
 
 ALWAYS_INLINE static void
-radv_cmd_set_color_blend_equation(struct radv_cmd_buffer *cmd_buffer, uint32_t first, uint32_t count,
-                                  const struct radv_blend_equation_state *blend_eq)
+radv_cmd_set_color_blend_equation(struct radv_cmd_buffer *cmd_buffer,
+                                 uint32_t first,
+                                 uint32_t count,
+                                 const struct radv_blend_equation_state *blend_eq)
 {
-   struct radv_cmd_state *state = &cmd_buffer->state;
+    assert(cmd_buffer != NULL);
+    assert(blend_eq != NULL);
+
+    struct radv_cmd_state *state = &cmd_buffer->state;
+
+    /* Bounds check with saturation */
+    if (unlikely(first >= MAX_RTS)) {
+        return;
+    }
+    count = MIN2(count, MAX_RTS - first);
 
-   typed_memcpy(state->dynamic.blend_eq.att + first, blend_eq->att, count);
-   if (first == 0)
-      state->dynamic.blend_eq.mrt0_is_dual_src = blend_eq->mrt0_is_dual_src;
+    if (unlikely(count == 0)) {
+        return;
+    }
 
-   state->dirty_dynamic |= RADV_DYNAMIC_COLOR_BLEND_EQUATION;
+    /* Optimized copy for blend equations */
+    radv_fast_memcpy_aligned(state->dynamic.blend_eq.att + first,
+                            blend_eq->att,
+                            count * sizeof(blend_eq->att[0]));
+
+    /* Update MRT0 dual source if needed */
+    if (first == 0) {
+        state->dynamic.blend_eq.mrt0_is_dual_src = blend_eq->mrt0_is_dual_src;
+    }
+
+    state->dirty_dynamic |= RADV_DYNAMIC_COLOR_BLEND_EQUATION;
 }
 
 ALWAYS_INLINE static void
-radv_cmd_set_vertex_binding_strides(struct radv_cmd_buffer *cmd_buffer, uint32_t first, uint32_t count,
-                                    const uint16_t *strides)
+radv_cmd_set_vertex_binding_strides(struct radv_cmd_buffer *cmd_buffer,
+                                   uint32_t first,
+                                   uint32_t count,
+                                   const uint16_t *strides)
 {
-   struct radv_cmd_state *state = &cmd_buffer->state;
+    assert(cmd_buffer != NULL);
+
+    struct radv_cmd_state *state = &cmd_buffer->state;
+    struct radv_device *device = radv_cmd_buffer_device(cmd_buffer);
+    const struct radv_physical_device *pdev = radv_device_physical(device);
+
+    /* Bounds check with saturation */
+    if (unlikely(first >= MESA_VK_MAX_VERTEX_BINDINGS)) {
+        return;
+    }
+    count = MIN2(count, MESA_VK_MAX_VERTEX_BINDINGS - first);
+
+    if (unlikely(count == 0 || strides == NULL)) {
+        return;
+    }
 
-   typed_memcpy(state->dynamic.vk.vi_binding_strides + first, strides, count);
+    /* Vega optimization: Fast path for single stride update */
+    if (pdev->info.gfx_level == GFX9 && count == 1) {
+        if (state->dynamic.vk.vi_binding_strides[first] != strides[0]) {
+            state->dynamic.vk.vi_binding_strides[first] = strides[0];
+            state->dirty_dynamic |= RADV_DYNAMIC_VERTEX_INPUT_BINDING_STRIDE;
+        }
+        return;
+    }
 
-   state->dirty_dynamic |= RADV_DYNAMIC_VERTEX_INPUT_BINDING_STRIDE;
+    /* Check if anything actually changed */
+    bool changed = false;
+    uint16_t *dst = state->dynamic.vk.vi_binding_strides + first;
+
+    /* Vega optimization: Use vectorized comparison for common counts */
+    if (count <= 16 && ((uintptr_t)dst & 7) == 0 && ((uintptr_t)strides & 7) == 0) {
+        /* 8-byte aligned, can use 64-bit comparisons */
+        uint32_t qwords = (count + 3) / 4;
+        const uint64_t *src64 = (const uint64_t *)strides;
+        uint64_t *dst64 = (uint64_t *)dst;
+
+        for (uint32_t i = 0; i < qwords; i++) {
+            if (dst64[i] != src64[i]) {
+                changed = true;
+                dst64[i] = src64[i];
+            }
+        }
+    } else {
+        /* Fallback for unaligned or large counts */
+        for (uint32_t i = 0; i < count; i++) {
+            if (dst[i] != strides[i]) {
+                dst[i] = strides[i];
+                changed = true;
+            }
+        }
+    }
+
+    if (changed) {
+        state->dirty_dynamic |= RADV_DYNAMIC_VERTEX_INPUT_BINDING_STRIDE;
+    }
 }
 
 ALWAYS_INLINE static void
-radv_cmd_set_vertex_input(struct radv_cmd_buffer *cmd_buffer, const struct radv_vertex_input_state *vi_state)
+radv_cmd_set_vertex_input(struct radv_cmd_buffer *cmd_buffer,
+                         const struct radv_vertex_input_state *vi_state)
 {
-   struct radv_cmd_state *state = &cmd_buffer->state;
+    struct radv_cmd_state *state = &cmd_buffer->state;
+
+    /* Use optimized memcpy for this critical structure */
+    radv_fast_memcpy_aligned(&state->dynamic.vertex_input, vi_state, sizeof(*vi_state));
 
-   memcpy(&state->dynamic.vertex_input, vi_state, sizeof(*vi_state));
+    state->dirty_dynamic |= RADV_DYNAMIC_VERTEX_INPUT;
+    state->dirty |= RADV_CMD_DIRTY_VS_PROLOG_STATE | RADV_CMD_DIRTY_VERTEX_BUFFER;
 
-   state->dirty_dynamic |= RADV_DYNAMIC_VERTEX_INPUT;
-   state->dirty |= RADV_CMD_DIRTY_VS_PROLOG_STATE | RADV_CMD_DIRTY_VERTEX_BUFFER;
+    /* Memory barrier to ensure visibility */
+    VEGA_MEMORY_BARRIER();
 }
 
 static void
