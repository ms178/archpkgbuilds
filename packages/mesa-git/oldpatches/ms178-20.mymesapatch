--- a/src/amd/compiler/instruction_selection/aco_select_nir_alu.cpp	2025-08-01 12:17:31.531652664 +0200
+++ b/src/amd/compiler/instruction_selection/aco_select_nir_alu.cpp	2025-08-01 12:42:07.300288345 +0200
@@ -1909,6 +1909,45 @@ visit_alu_instr(isel_context* ctx, nir_a
       }
       break;
    }
+   case nir_op_fdiv: {
+      Temp src0 = get_alu_src(ctx, instr->src[0]);
+      Temp src1 = get_alu_src(ctx, instr->src[1]);
+      if (dst.regClass() == v2) {
+         if (ctx->program->gfx_level >= GFX10) {
+            bld.vop3(aco_opcode::v_div_f64, Definition(dst), as_vgpr(ctx, src0),
+                     as_vgpr(ctx, src1));
+         } else {
+            Temp rcp = bld.vop1(aco_opcode::v_rcp_f64, bld.def(v2), as_vgpr(ctx, src1));
+            bld.vop3(aco_opcode::v_mul_f64_e64, Definition(dst), as_vgpr(ctx, src0), rcp);
+         }
+      } else if (dst.regClass() == s1 && instr->def.bit_size == 32) {
+         Temp rcp = bld.tmp(s1);
+         emit_rcp(ctx, bld, Definition(rcp), src1);
+         bld.sop2(aco_opcode::s_mul_f32, Definition(dst), src0, rcp);
+      } else if (dst.regClass() == v1) { // f32
+         Temp rcp = bld.tmp(v1);
+         emit_rcp(ctx, bld, Definition(rcp), src1);
+         bld.vop2(aco_opcode::v_mul_f32, Definition(dst), as_vgpr(ctx, src0), as_vgpr(ctx, rcp));
+      } else if (dst.regClass() == v2b || (dst.regClass() == s1 && instr->def.bit_size == 16)) { // f16
+         Temp rcp = bld.tmp(dst.regClass());
+         if (dst.regClass() == s1) {
+            if (ctx->program->gfx_level >= GFX12) {
+               bld.vop3(aco_opcode::v_s_rcp_f16, Definition(rcp), as_vgpr(ctx, src1));
+            } else {
+               rcp = bld.as_uniform(
+                  bld.vop1(aco_opcode::v_rcp_f16, bld.def(v2b), as_vgpr(ctx, src1)));
+            }
+            bld.sop2(aco_opcode::s_mul_f16, Definition(dst), src0, rcp);
+         } else {
+            bld.vop1(aco_opcode::v_rcp_f16, Definition(rcp), as_vgpr(ctx, src1));
+            bld.vop2(aco_opcode::v_mul_f16, Definition(dst), as_vgpr(ctx, src0),
+                     as_vgpr(ctx, rcp));
+         }
+      } else {
+         isel_err(&instr->instr, "Unimplemented NIR fdiv bit size");
+      }
+      break;
+   }
    case nir_op_ffma: {
       if (dst.regClass() == v2b) {
          emit_vop3a_instruction(ctx, instr, aco_opcode::v_fma_f16, dst, false, 3);
@@ -2042,6 +2081,58 @@ visit_alu_instr(isel_context* ctx, nir_a
       emit_idot_instruction(ctx, instr, aco_opcode::v_dot2_u32_u16, dst, true);
       break;
    }
+   case nir_op_fdot2: {
+      /* fdot2(a, b) = a.x * b.x + a.y * b.y */
+      const unsigned bit_size = instr->src[0].src.ssa->bit_size;
+      assert(bit_size == instr->src[1].src.ssa->bit_size);
+      assert(instr->def.bit_size == 32);
+
+      if (dst.type() == RegType::vgpr && ctx->program->gfx_level >= GFX11 &&
+          (bit_size == 16 || bit_size == 32)) {
+         aco_opcode opcode =
+            bit_size == 16 ? aco_opcode::v_dot2_f32_f16 : aco_opcode::v_dot2_f32_f32;
+         Temp src0 = as_vgpr(ctx, get_alu_src(ctx, instr->src[0], 2));
+         Temp src1 = as_vgpr(ctx, get_alu_src(ctx, instr->src[1], 2));
+         Instruction* dot = bld.vop3(opcode, Definition(dst), src0, src1);
+         break;
+      }
+
+      /* Fallback for SGPR or older hardware. */
+      Temp src0 = get_alu_src(ctx, instr->src[0], 2);
+      Temp src1 = get_alu_src(ctx, instr->src[1], 2);
+      RegClass vec_rc = RegClass::get(src0.type(), bit_size / 8u);
+
+      Temp src0_x = emit_extract_vector(ctx, src0, 0, vec_rc);
+      Temp src0_y = emit_extract_vector(ctx, src0, 1, vec_rc);
+      Temp src1_x = emit_extract_vector(ctx, src1, 0, vec_rc);
+      Temp src1_y = emit_extract_vector(ctx, src1, 1, vec_rc);
+
+      Temp res;
+      if (dst.type() == RegType::sgpr) {
+         assert(bit_size == 32 && "16-bit fdot2 should not produce SGPR result on this path");
+         Temp mul = bld.sop2(aco_opcode::s_mul_f32, bld.def(s1), src0_x, src1_x);
+         res = bld.sop2(aco_opcode::s_fmac_f32, bld.def(s1), src0_y, src1_y, mul);
+      } else {
+         Temp x_prod = bld.tmp(v1), y_prod = bld.tmp(v1);
+         if (bit_size == 16) {
+            Temp mul_x = bld.vop2(aco_opcode::v_mul_f16, bld.def(v2b), as_vgpr(ctx, src0_x),
+                                  as_vgpr(ctx, src1_x));
+            Temp mul_y = bld.vop2(aco_opcode::v_mul_f16, bld.def(v2b), as_vgpr(ctx, src0_y),
+                                  as_vgpr(ctx, src1_y));
+            bld.vop1(aco_opcode::v_cvt_f32_f16, Definition(x_prod), mul_x);
+            bld.vop1(aco_opcode::v_cvt_f32_f16, Definition(y_prod), mul_y);
+         } else { /* 32-bit */
+            bld.vop2(aco_opcode::v_mul_f32, Definition(x_prod), as_vgpr(ctx, src0_x),
+                     as_vgpr(ctx, src1_x));
+            bld.vop2(aco_opcode::v_mul_f32, Definition(y_prod), as_vgpr(ctx, src0_y),
+                     as_vgpr(ctx, src1_y));
+         }
+         res = bld.vop2(aco_opcode::v_add_f32, bld.def(v1), x_prod, y_prod);
+      }
+
+      bld.copy(Definition(dst), res);
+      break;
+   }
    case nir_op_bfdot2_bfadd: {
       Temp src0 = as_vgpr(ctx, get_alu_src(ctx, instr->src[0], 2));
       Temp src1 = as_vgpr(ctx, get_alu_src(ctx, instr->src[1], 2));
