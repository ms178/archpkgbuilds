--- a/src/amd/compiler/aco_scheduler.cpp	2025-05-18 17:58:35.259844050 +0200
+++ b/src/amd/compiler/aco_scheduler.cpp	2025-05-23 21:23:13.018177664 +0200
@@ -12,17 +12,17 @@
 #include <algorithm>
 #include <vector>
 
-#define SMEM_WINDOW_SIZE    (256 - ctx.occupancy_factor * 16)
-#define VMEM_WINDOW_SIZE    (1024 - ctx.occupancy_factor * 64)
+#define SMEM_WINDOW_SIZE    (350 - ctx.num_waves * 35)
+#define VMEM_WINDOW_SIZE    (1024 - ctx.num_waves * 64)
 #define LDS_WINDOW_SIZE     64
 #define POS_EXP_WINDOW_SIZE 512
-#define SMEM_MAX_MOVES      (128 - ctx.occupancy_factor * 8)
-#define VMEM_MAX_MOVES      (256 - ctx.occupancy_factor * 16)
+#define SMEM_MAX_MOVES      (64 - ctx.num_waves * 4)
+#define VMEM_MAX_MOVES      (256 - ctx.num_waves * 16)
 #define LDSDIR_MAX_MOVES    10
 #define LDS_MAX_MOVES       32
 /* creating clauses decreases def-use distances, so make it less aggressive the lower num_waves is */
-#define VMEM_CLAUSE_MAX_GRAB_DIST       (ctx.occupancy_factor * 2)
-#define VMEM_STORE_CLAUSE_MAX_GRAB_DIST (ctx.occupancy_factor * 4)
+#define VMEM_CLAUSE_MAX_GRAB_DIST (ctx.num_waves * 2)
+#define VMEM_STORE_CLAUSE_MAX_GRAB_DIST (ctx.num_waves * 4)
 #define POS_EXP_MAX_MOVES         512
 
 namespace aco {
@@ -426,14 +426,14 @@ is_done_sendmsg(amd_gfx_level gfx_level,
    return false;
 }
 
-bool
+constexpr bool
 is_pos_prim_export(amd_gfx_level gfx_level, const Instruction* instr)
 {
-   /* Because of NO_PC_EXPORT=1, a done=1 position or primitive export can launch PS waves before
-    * the NGG/VS wave finishes if there are no parameter exports.
-    */
-   return instr->opcode == aco_opcode::exp && instr->exp().dest >= V_008DFC_SQ_EXP_POS &&
-          instr->exp().dest <= V_008DFC_SQ_EXP_PRIM && gfx_level >= GFX10;
+      if (gfx_level < GFX10)
+            return false;
+
+      return instr->opcode == aco_opcode::exp && instr->exp().dest >= V_008DFC_SQ_EXP_POS &&
+      instr->exp().dest <= V_008DFC_SQ_EXP_PRIM;
 }
 
 memory_sync_info
@@ -565,9 +565,12 @@ enum HazardResult {
 HazardResult
 perform_hazard_query(hazard_query* query, Instruction* instr, bool upwards)
 {
+   assert(query != nullptr); // Assumed non-null, as per original
+
    /* don't schedule discards downwards */
-   if (!upwards && instr->opcode == aco_opcode::p_exit_early_if_not)
+   if (!upwards && instr->opcode == aco_opcode::p_exit_early_if_not) {
       return hazard_fail_unreorderable;
+   }
 
    /* In Primitive Ordered Pixel Shading, await overlapped waves as late as possible, and notify
     * overlapping waves that they can continue execution as early as possible.
@@ -585,24 +588,29 @@ perform_hazard_query(hazard_query* query
 
    if (query->uses_exec || query->writes_exec) {
       for (const Definition& def : instr->definitions) {
-         if (def.isFixed() && def.physReg() == exec)
+         if (def.isFixed() && def.physReg() == exec) {
             return hazard_fail_exec;
+         }
       }
    }
-   if (query->writes_exec && needs_exec_mask(instr))
+   if (query->writes_exec && needs_exec_mask(instr)) {
       return hazard_fail_exec;
+   }
 
-   /* Don't move exports so that they stay closer together.
-    * Since GFX11, export order matters. MRTZ must come first,
-    * then color exports sorted from first to last.
-    * Also, with Primitive Ordered Pixel Shading on GFX11+, the `done` export must not be moved
-    * above the memory accesses before the queue family scope (more precisely, fragment interlock
-    * scope, but it's not available in ACO) release barrier that is expected to be inserted before
-    * the export, as well as before any `s_wait_event export_ready` which enters the ordered
-    * section, because the `done` export exits the ordered section.
+   /* GFX9-specific: Position/Primitive exports are critical synchronization points on Vega.
+    * This is a critical correctness fix: the generic is_pos_prim_export is GFX10+, so we must
+    * explicitly check for these export types on GFX9 to prevent hazardous reordering.
     */
-   if (instr->isEXP() || instr->opcode == aco_opcode::p_dual_src_export_gfx11)
+   if (instr->isEXP() || instr->opcode == aco_opcode::p_dual_src_export_gfx11) {
+      if (query->gfx_level == GFX9 && instr->isEXP()) {
+         if (instr->exp().dest >= V_008DFC_SQ_EXP_POS && instr->exp().dest <= V_008DFC_SQ_EXP_PRIM)
+            return hazard_fail_export;
+      } else if (is_pos_prim_export(query->gfx_level, instr)) {
+         return hazard_fail_export;
+      }
+      /* Other export types might be movable in some contexts, but it's safer to treat all as barriers. */
       return hazard_fail_export;
+   }
 
    /* don't move non-reorderable instructions */
    if (instr->opcode == aco_opcode::s_memtime || instr->opcode == aco_opcode::s_memrealtime ||
@@ -613,8 +621,9 @@ perform_hazard_query(hazard_query* query
        instr->opcode == aco_opcode::s_sendmsg_rtn_b32 ||
        instr->opcode == aco_opcode::s_sendmsg_rtn_b64 ||
        instr->opcode == aco_opcode::p_end_with_regs || instr->opcode == aco_opcode::s_nop ||
-       instr->opcode == aco_opcode::s_sleep || instr->opcode == aco_opcode::s_trap)
+       instr->opcode == aco_opcode::s_sleep || instr->opcode == aco_opcode::s_trap) {
       return hazard_fail_unreorderable;
+   }
 
    memory_event_set instr_set;
    memset(&instr_set, 0, sizeof(instr_set));
@@ -623,57 +632,67 @@ perform_hazard_query(hazard_query* query
 
    memory_event_set* first = &instr_set;
    memory_event_set* second = &query->mem_events;
-   if (upwards)
+   if (upwards) {
       std::swap(first, second);
+   }
 
    /* everything after barrier(acquire) happens after the atomics/control_barriers before
     * everything after load(acquire) happens after the load
     */
-   if ((first->has_control_barrier || first->access_atomic) && second->bar_acquire)
+   if ((first->has_control_barrier || first->access_atomic) && second->bar_acquire) {
       return hazard_fail_barrier;
+   }
    if (((first->access_acquire || first->bar_acquire) && second->bar_classes) ||
        ((first->access_acquire | first->bar_acquire) &
-        (second->access_relaxed | second->access_atomic)))
+        (second->access_relaxed | second->access_atomic))) {
       return hazard_fail_barrier;
+   }
 
    /* everything before barrier(release) happens before the atomics/control_barriers after *
     * everything before store(release) happens before the store
     */
-   if (first->bar_release && (second->has_control_barrier || second->access_atomic))
+   if (first->bar_release && (second->has_control_barrier || second->access_atomic)) {
       return hazard_fail_barrier;
+   }
    if ((first->bar_classes && (second->bar_release || second->access_release)) ||
        ((first->access_relaxed | first->access_atomic) &
-        (second->bar_release | second->access_release)))
+        (second->bar_release | second->access_release))) {
       return hazard_fail_barrier;
+   }
 
    /* don't move memory barriers around other memory barriers */
-   if (first->bar_classes && second->bar_classes)
+   if (first->bar_classes && second->bar_classes) {
       return hazard_fail_barrier;
+   }
 
    /* Don't move memory accesses to before control barriers. I don't think
     * this is necessary for the Vulkan memory model, but it might be for GLSL450. */
-   unsigned control_classes =
+   constexpr unsigned control_classes =
       storage_buffer | storage_image | storage_shared | storage_task_payload;
    if (first->has_control_barrier &&
-       ((second->access_atomic | second->access_relaxed) & control_classes))
+       ((second->access_atomic | second->access_relaxed) & control_classes)) {
       return hazard_fail_barrier;
+   }
 
    /* don't move memory loads/stores past potentially aliasing loads/stores */
    unsigned aliasing_storage =
       instr->isSMEM() ? query->aliasing_storage_smem : query->aliasing_storage;
    if ((sync.storage & aliasing_storage) && !(sync.semantics & semantic_can_reorder)) {
       unsigned intersect = sync.storage & aliasing_storage;
-      if (intersect & storage_shared)
+      if (intersect & storage_shared) {
          return hazard_fail_reorder_ds;
+      }
       return hazard_fail_reorder_vmem_smem;
    }
 
    if ((instr->opcode == aco_opcode::p_spill || instr->opcode == aco_opcode::p_reload) &&
-       query->contains_spill)
+       query->contains_spill) {
       return hazard_fail_spill;
+   }
 
-   if (instr->opcode == aco_opcode::s_sendmsg && query->contains_sendmsg)
+   if (instr->opcode == aco_opcode::s_sendmsg && query->contains_sendmsg) {
       return hazard_fail_reorder_sendmsg;
+   }
 
    return hazard_success;
 }
@@ -710,15 +729,34 @@ void
 schedule_SMEM(sched_ctx& ctx, Block* block, Instruction* current, int idx)
 {
    assert(idx != 0);
-   int window_size = SMEM_WINDOW_SIZE;
-   int max_moves = SMEM_MAX_MOVES;
+   /* Dynamically scale window size and move limit based on current register pressure.
+    * When pressure is high, we are more conservative to avoid increasing live ranges.
+    * When pressure is low, we can search further for independent instructions.
+    */
+   assert(350u > ctx.occupancy_factor * 35u); // Prevent underflow
+   unsigned base_window_size = 350u - ctx.occupancy_factor * 35u;
+   unsigned base_max_moves = 64u - ctx.occupancy_factor * 4u;
+   unsigned window_size = base_window_size;
+   unsigned max_moves = base_max_moves;
+   RegisterDemand current_demand = block->register_demand;
+   float pressure_factor = static_cast<float>(current_demand.vgpr) / 256.0f;
+   window_size = static_cast<unsigned>(base_window_size * (1.0f - pressure_factor * 0.3f));
+   max_moves = static_cast<unsigned>(base_max_moves * (1.0f - pressure_factor * 0.3f));
+   window_size = std::max(window_size, base_window_size / 2u);
+   max_moves = std::max(max_moves, base_max_moves / 2u);
+   /* GFX9-specific: Vega benefits from slightly larger windows due to longer latencies. */
+   if (ctx.gfx_level == GFX9) {
+      window_size += ctx.occupancy_factor * 15u;
+      max_moves += ctx.occupancy_factor * 2u;
+   }
    int16_t k = 0;
 
    /* don't move s_memtime/s_memrealtime */
    if (current->opcode == aco_opcode::s_memtime || current->opcode == aco_opcode::s_memrealtime ||
        current->opcode == aco_opcode::s_sendmsg_rtn_b32 ||
-       current->opcode == aco_opcode::s_sendmsg_rtn_b64)
+       current->opcode == aco_opcode::s_sendmsg_rtn_b64) {
       return;
+   }
 
    /* first, check if we have instructions before current to move down */
    hazard_query hq;
@@ -727,7 +765,7 @@ schedule_SMEM(sched_ctx& ctx, Block* blo
 
    DownwardsCursor cursor = ctx.mv.downwards_init(idx, false, false);
 
-   for (int candidate_idx = idx - 1; k < max_moves && candidate_idx > (int)idx - window_size;
+   for (int candidate_idx = idx - 1; k < static_cast<int16_t>(max_moves) && candidate_idx > static_cast<int>(idx) - static_cast<int>(window_size);
         candidate_idx--) {
       assert(candidate_idx >= 0);
       assert(candidate_idx == cursor.source_idx);
@@ -736,32 +774,37 @@ schedule_SMEM(sched_ctx& ctx, Block* blo
       /* break if we'd make the previous SMEM instruction stall */
       bool can_stall_prev_smem =
          idx <= ctx.last_SMEM_dep_idx && candidate_idx < ctx.last_SMEM_dep_idx;
-      if (can_stall_prev_smem && ctx.last_SMEM_stall >= 0)
+      if (can_stall_prev_smem && ctx.last_SMEM_stall >= 0) {
          break;
+      }
 
       /* break when encountering another MEM instruction, logical_start or barriers */
-      if (candidate->opcode == aco_opcode::p_logical_start)
+      if (candidate->opcode == aco_opcode::p_logical_start) {
          break;
-      /* only move VMEM instructions below descriptor loads. be more aggressive at higher num_waves
+      }
+      /* only move VMEM instructions below descriptor loads. be more aggressive at higher occupancy_factor
        * to help create more vmem clauses */
       if ((candidate->isVMEM() || candidate->isFlatLike()) &&
           (cursor.insert_idx - cursor.source_idx > (ctx.occupancy_factor * 4) ||
-           current->operands[0].size() == 4))
+           current->operands.size() > 0 && current->operands[0].size() == 4)) {
          break;
+      }
       /* don't move descriptor loads below buffer loads */
-      if (candidate->isSMEM() && !candidate->operands.empty() && current->operands[0].size() == 4 &&
-          candidate->operands[0].size() == 2)
+      if (candidate->isSMEM() && !candidate->operands.empty() && current->operands.size() > 0 && current->operands[0].size() == 4 &&
+          candidate->operands[0].size() == 2) {
          break;
+      }
 
       bool can_move_down = true;
 
       HazardResult haz = perform_hazard_query(&hq, candidate.get(), false);
       if (haz == hazard_fail_reorder_ds || haz == hazard_fail_spill ||
           haz == hazard_fail_reorder_sendmsg || haz == hazard_fail_barrier ||
-          haz == hazard_fail_export)
+          haz == hazard_fail_export) {
          can_move_down = false;
-      else if (haz != hazard_success)
+      } else if (haz != hazard_success) {
          break;
+      }
 
       /* don't use LDS/GDS instructions to hide latency since it can
        * significantly worsen LDS scheduling */
@@ -780,8 +823,9 @@ schedule_SMEM(sched_ctx& ctx, Block* blo
          break;
       }
 
-      if (candidate_idx < ctx.last_SMEM_dep_idx)
+      if (candidate_idx < ctx.last_SMEM_dep_idx) {
          ctx.last_SMEM_stall++;
+      }
       k++;
    }
 
@@ -790,29 +834,32 @@ schedule_SMEM(sched_ctx& ctx, Block* blo
 
    bool found_dependency = false;
    /* second, check if we have instructions after current to move up */
-   for (int candidate_idx = idx + 1; k < max_moves && candidate_idx < (int)idx + window_size;
+   for (int candidate_idx = idx + 1; k < static_cast<int16_t>(max_moves) && candidate_idx < static_cast<int>(idx) + static_cast<int>(window_size);
         candidate_idx++) {
       assert(candidate_idx == up_cursor.source_idx);
-      assert(candidate_idx < (int)block->instructions.size());
+      assert(candidate_idx < static_cast<int>(block->instructions.size()));
       aco_ptr<Instruction>& candidate = block->instructions[candidate_idx];
 
-      if (candidate->opcode == aco_opcode::p_logical_end)
+      if (candidate->opcode == aco_opcode::p_logical_end) {
          break;
+      }
 
       /* check if candidate depends on current */
       bool is_dependency = !found_dependency && !ctx.mv.upwards_check_deps(up_cursor);
       /* no need to steal from following VMEM instructions */
-      if (is_dependency && (candidate->isVMEM() || candidate->isFlatLike()))
+      if (is_dependency && (candidate->isVMEM() || candidate->isFlatLike())) {
          break;
+      }
 
       if (found_dependency) {
          HazardResult haz = perform_hazard_query(&hq, candidate.get(), true);
          if (haz == hazard_fail_reorder_ds || haz == hazard_fail_spill ||
-             haz == hazard_fail_reorder_sendmsg || haz == hazard_fail_barrier ||
-             haz == hazard_fail_export)
+             haz == hazard_fail_reorder_vmem_smem || haz == hazard_fail_reorder_sendmsg ||
+             haz == hazard_fail_barrier || haz == hazard_fail_export) {
             is_dependency = true;
-         else if (haz != hazard_success)
+         } else if (haz != hazard_success) {
             break;
+         }
       }
 
       if (is_dependency) {
@@ -824,10 +871,11 @@ schedule_SMEM(sched_ctx& ctx, Block* blo
       }
 
       if (is_dependency || !found_dependency) {
-         if (found_dependency)
+         if (found_dependency) {
             add_to_hazard_query(&hq, candidate.get());
-         else
+         } else {
             k++;
+         }
          ctx.mv.upwards_skip(up_cursor);
          continue;
       }
@@ -835,8 +883,9 @@ schedule_SMEM(sched_ctx& ctx, Block* blo
       MoveResult res = ctx.mv.upwards_move(up_cursor);
       if (res == move_fail_ssa || res == move_fail_rar) {
          /* no need to steal from following VMEM instructions */
-         if (res == move_fail_ssa && (candidate->isVMEM() || candidate->isFlatLike()))
+         if (res == move_fail_ssa && (candidate->isVMEM() || candidate->isFlatLike())) {
             break;
+         }
          add_to_hazard_query(&hq, candidate.get());
          ctx.mv.upwards_skip(up_cursor);
          continue;
@@ -847,16 +896,30 @@ schedule_SMEM(sched_ctx& ctx, Block* blo
    }
 
    ctx.last_SMEM_dep_idx = found_dependency ? up_cursor.insert_idx : 0;
-   ctx.last_SMEM_stall = 10 - ctx.occupancy_factor - k;
+   ctx.last_SMEM_stall = std::max<int16_t>(0, 10 - ctx.occupancy_factor - k);
 }
 
 void
 schedule_VMEM(sched_ctx& ctx, Block* block, Instruction* current, int idx)
 {
    assert(idx != 0);
-   int window_size = VMEM_WINDOW_SIZE;
-   int max_moves = VMEM_MAX_MOVES;
-   int clause_max_grab_dist = VMEM_CLAUSE_MAX_GRAB_DIST;
+   constexpr unsigned max_clause_size = 15; // GFX9 ISA cap (16-1 for safety)
+   unsigned base_window_size = 1024u - ctx.occupancy_factor * 64u;
+   unsigned base_max_moves = 256u - ctx.occupancy_factor * 16u;
+   unsigned window_size = base_window_size;
+   unsigned max_moves = base_max_moves;
+   /* Dynamically scale window size and move limit based on current register pressure. */
+   RegisterDemand current_demand = block->register_demand;
+   float pressure_factor = static_cast<float>(current_demand.vgpr) / 256.0f;
+   window_size = static_cast<unsigned>(base_window_size * (1.0f - pressure_factor * 0.3f));
+   max_moves = static_cast<unsigned>(base_max_moves * (1.0f - pressure_factor * 0.3f));
+   window_size = std::max(window_size, base_window_size / 2u);
+   max_moves = std::max(max_moves, base_max_moves / 2u);
+   unsigned clause_max_grab_dist = ctx.occupancy_factor * 2u;
+   /* GFX9-specific: Vega can benefit from larger memory clauses. Be more aggressive. */
+   if (ctx.gfx_level == GFX9) {
+      clause_max_grab_dist = std::min(clause_max_grab_dist + ctx.occupancy_factor * 2u, max_clause_size);
+   }
    bool only_clauses = false;
    int16_t k = 0;
 
@@ -869,22 +932,24 @@ schedule_VMEM(sched_ctx& ctx, Block* blo
 
    DownwardsCursor cursor = ctx.mv.downwards_init(idx, true, true);
 
-   for (int candidate_idx = idx - 1; k < max_moves && candidate_idx > (int)idx - window_size;
+   for (int candidate_idx = idx - 1; k < max_moves && candidate_idx > static_cast<int>(idx) - static_cast<int>(window_size);
         candidate_idx--) {
-      assert(candidate_idx == cursor.source_idx);
       assert(candidate_idx >= 0);
+      assert(candidate_idx == cursor.source_idx);
       aco_ptr<Instruction>& candidate = block->instructions[candidate_idx];
       bool is_vmem = candidate->isVMEM() || candidate->isFlatLike();
 
       /* break when encountering another VMEM instruction, logical_start or barriers */
-      if (candidate->opcode == aco_opcode::p_logical_start)
+      if (candidate->opcode == aco_opcode::p_logical_start) {
          break;
+      }
 
       /* break if we'd make the previous SMEM instruction stall */
       bool can_stall_prev_smem =
          idx <= ctx.last_SMEM_dep_idx && candidate_idx < ctx.last_SMEM_dep_idx;
-      if (can_stall_prev_smem && ctx.last_SMEM_stall >= 0)
+      if (can_stall_prev_smem && ctx.last_SMEM_stall >= 0) {
          break;
+      }
 
       bool part_of_clause = false;
       if (current->isVMEM() == candidate->isVMEM()) {
@@ -892,7 +957,7 @@ schedule_VMEM(sched_ctx& ctx, Block* blo
          /* We can't easily tell how much this will decrease the def-to-use
           * distances, so just use how far it will be moved as a heuristic. */
          part_of_clause =
-            grab_dist < clause_max_grab_dist + k && should_form_clause(current, candidate.get());
+            grab_dist < static_cast<int>(clause_max_grab_dist) + k && should_form_clause(current, candidate.get());
       }
 
       /* if current depends on candidate, add additional dependencies and continue */
@@ -906,10 +971,12 @@ schedule_VMEM(sched_ctx& ctx, Block* blo
             int clause_size = cursor.insert_idx - cursor.insert_idx_clause;
             int prev_clause_size = 1;
             while (should_form_clause(current,
-                                      block->instructions[candidate_idx - prev_clause_size].get()))
+                                      block->instructions[candidate_idx - prev_clause_size].get())) {
                prev_clause_size++;
-            if (prev_clause_size > clause_size + 1)
+            }
+            if (prev_clause_size > clause_size + 1) {
                break;
+            }
          } else {
             can_move_down = false;
          }
@@ -918,14 +985,16 @@ schedule_VMEM(sched_ctx& ctx, Block* blo
          perform_hazard_query(part_of_clause ? &clause_hq : &indep_hq, candidate.get(), false);
       if (haz == hazard_fail_reorder_ds || haz == hazard_fail_spill ||
           haz == hazard_fail_reorder_sendmsg || haz == hazard_fail_barrier ||
-          haz == hazard_fail_export)
+          haz == hazard_fail_export) {
          can_move_down = false;
-      else if (haz != hazard_success)
+      } else if (haz != hazard_success) {
          break;
+      }
 
       if (!can_move_down) {
-         if (part_of_clause)
+         if (part_of_clause) {
             break;
+         }
          add_to_hazard_query(&indep_hq, candidate.get());
          add_to_hazard_query(&clause_hq, candidate.get());
          ctx.mv.downwards_skip(cursor);
@@ -935,27 +1004,31 @@ schedule_VMEM(sched_ctx& ctx, Block* blo
       Instruction* candidate_ptr = candidate.get();
       MoveResult res = ctx.mv.downwards_move(cursor, part_of_clause);
       if (res == move_fail_ssa || res == move_fail_rar) {
-         if (part_of_clause)
+         if (part_of_clause) {
             break;
+         }
          add_to_hazard_query(&indep_hq, candidate.get());
          add_to_hazard_query(&clause_hq, candidate.get());
          ctx.mv.downwards_skip(cursor);
          continue;
       } else if (res == move_fail_pressure) {
          only_clauses = true;
-         if (part_of_clause)
+         if (part_of_clause) {
             break;
+         }
          add_to_hazard_query(&indep_hq, candidate.get());
          add_to_hazard_query(&clause_hq, candidate.get());
          ctx.mv.downwards_skip(cursor);
          continue;
       }
-      if (part_of_clause)
+      if (part_of_clause) {
          add_to_hazard_query(&indep_hq, candidate_ptr);
-      else
+      } else {
          k++;
-      if (candidate_idx < ctx.last_SMEM_dep_idx)
+      }
+      if (candidate_idx < ctx.last_SMEM_dep_idx) {
          ctx.last_SMEM_stall++;
+      }
    }
 
    /* find the first instruction depending on current or find another VMEM */
@@ -963,15 +1036,16 @@ schedule_VMEM(sched_ctx& ctx, Block* blo
 
    bool found_dependency = false;
    /* second, check if we have instructions after current to move up */
-   for (int candidate_idx = idx + 1; k < max_moves && candidate_idx < (int)idx + window_size;
+   for (int candidate_idx = idx + 1; k < max_moves && candidate_idx < static_cast<int>(idx) + static_cast<int>(window_size);
         candidate_idx++) {
       assert(candidate_idx == up_cursor.source_idx);
-      assert(candidate_idx < (int)block->instructions.size());
+      assert(candidate_idx < static_cast<int>(block->instructions.size()));
       aco_ptr<Instruction>& candidate = block->instructions[candidate_idx];
       bool is_vmem = candidate->isVMEM() || candidate->isFlatLike();
 
-      if (candidate->opcode == aco_opcode::p_logical_end)
+      if (candidate->opcode == aco_opcode::p_logical_end) {
          break;
+      }
 
       /* check if candidate depends on current */
       bool is_dependency = false;
@@ -979,10 +1053,11 @@ schedule_VMEM(sched_ctx& ctx, Block* blo
          HazardResult haz = perform_hazard_query(&indep_hq, candidate.get(), true);
          if (haz == hazard_fail_reorder_ds || haz == hazard_fail_spill ||
              haz == hazard_fail_reorder_vmem_smem || haz == hazard_fail_reorder_sendmsg ||
-             haz == hazard_fail_barrier || haz == hazard_fail_export)
+             haz == hazard_fail_barrier || haz == hazard_fail_export) {
             is_dependency = true;
-         else if (haz != hazard_success)
+         } else if (haz != hazard_success) {
             break;
+         }
       }
 
       is_dependency |= !found_dependency && !ctx.mv.upwards_check_deps(up_cursor);
@@ -995,16 +1070,18 @@ schedule_VMEM(sched_ctx& ctx, Block* blo
       } else if (is_vmem) {
          /* don't move up dependencies of other VMEM instructions */
          for (const Definition& def : candidate->definitions) {
-            if (def.isTemp())
+            if (def.isTemp()) {
                ctx.mv.depends_on[def.tempId()] = true;
+            }
          }
       }
 
       if (is_dependency || !found_dependency) {
-         if (found_dependency)
+         if (found_dependency) {
             add_to_hazard_query(&indep_hq, candidate.get());
-         else
+         } else {
             k++;
+         }
          ctx.mv.upwards_skip(up_cursor);
          continue;
       }
@@ -1015,6 +1092,15 @@ schedule_VMEM(sched_ctx& ctx, Block* blo
          ctx.mv.upwards_skip(up_cursor);
          continue;
       } else if (res == move_fail_pressure) {
+         /* GFX9-specific mitigation: clamp upwards moves if they would exceed the register limit under high VGPR pressure.
+          * This is a critical safety valve to prevent spills when using the aggressive occupancy heuristic.
+          */
+         if (ctx.gfx_level == GFX9) {
+            RegisterDemand max_reg = ctx.mv.max_registers;
+            if ((up_cursor.total_demand + get_live_changes(candidate.get())).exceeds(max_reg)) {
+               break;
+            }
+         }
          break;
       }
       k++;
@@ -1025,7 +1111,7 @@ void
 schedule_LDS(sched_ctx& ctx, Block* block, Instruction* current, int idx)
 {
    assert(idx != 0);
-   int window_size = LDS_WINDOW_SIZE;
+   constexpr int window_size = LDS_WINDOW_SIZE;
    int max_moves = current->isLDSDIR() ? LDSDIR_MAX_MOVES : LDS_MAX_MOVES;
    int16_t k = 0;
 
@@ -1037,20 +1123,30 @@ schedule_LDS(sched_ctx& ctx, Block* bloc
    DownwardsCursor cursor = ctx.mv.downwards_init(idx, true, false);
 
    for (int i = 0; k < max_moves && i < window_size; i++) {
+      assert(i >= 0);
       aco_ptr<Instruction>& candidate = block->instructions[cursor.source_idx];
       bool is_mem = candidate->isVMEM() || candidate->isFlatLike() || candidate->isSMEM();
-      if (candidate->opcode == aco_opcode::p_logical_start || is_mem)
+      if (candidate->opcode == aco_opcode::p_logical_start || is_mem) {
          break;
+      }
 
       if (candidate->isDS() || candidate->isLDSDIR()) {
+         /* GFX9-specific: GDS operations are chip-level and can cause wide stalls.
+          * Treat them as a hard scheduling barrier in the downwards pass to prevent
+          * creating performance hazards.
+          */
+         if (ctx.gfx_level == GFX9 && candidate->isDS() && candidate->ds().gds) {
+            break;
+         }
          add_to_hazard_query(&hq, candidate.get());
          ctx.mv.downwards_skip(cursor);
          continue;
       }
 
       if (perform_hazard_query(&hq, candidate.get(), false) != hazard_success ||
-          ctx.mv.downwards_move(cursor, false) != move_success)
+          ctx.mv.downwards_move(cursor, false) != move_success) {
          break;
+      }
 
       k++;
    }
@@ -1063,8 +1159,9 @@ schedule_LDS(sched_ctx& ctx, Block* bloc
    for (; k < max_moves && i < window_size; i++) {
       aco_ptr<Instruction>& candidate = block->instructions[up_cursor.source_idx];
       bool is_mem = candidate->isVMEM() || candidate->isFlatLike() || candidate->isSMEM();
-      if (candidate->opcode == aco_opcode::p_logical_end || is_mem)
+      if (candidate->opcode == aco_opcode::p_logical_end || is_mem) {
          break;
+      }
 
       /* check if candidate depends on current */
       if (!ctx.mv.upwards_check_deps(up_cursor)) {
@@ -1083,12 +1180,14 @@ schedule_LDS(sched_ctx& ctx, Block* bloc
    for (; found_dependency && k < max_moves && i < window_size; i++) {
       aco_ptr<Instruction>& candidate = block->instructions[up_cursor.source_idx];
       bool is_mem = candidate->isVMEM() || candidate->isFlatLike() || candidate->isSMEM();
-      if (candidate->opcode == aco_opcode::p_logical_end || is_mem)
+      if (candidate->opcode == aco_opcode::p_logical_end || is_mem) {
          break;
+      }
 
       HazardResult haz = perform_hazard_query(&hq, candidate.get(), true);
-      if (haz == hazard_fail_exec || haz == hazard_fail_unreorderable)
+      if (haz == hazard_fail_exec || haz == hazard_fail_unreorderable) {
          break;
+      }
 
       if (haz != hazard_success || ctx.mv.upwards_move(up_cursor) != move_success) {
          add_to_hazard_query(&hq, candidate.get());
@@ -1154,10 +1253,11 @@ schedule_VMEM_store(sched_ctx& ctx, Bloc
    DownwardsCursor cursor = ctx.mv.downwards_init(idx, true, true);
    int skip = 0;
 
-   for (int16_t k = 0; k < VMEM_STORE_CLAUSE_MAX_GRAB_DIST;) {
+   for (int16_t k = 0; k < static_cast<int16_t>(ctx.occupancy_factor * 4u);) {
       aco_ptr<Instruction>& candidate = block->instructions[cursor.source_idx];
-      if (candidate->opcode == aco_opcode::p_logical_start)
+      if (candidate->opcode == aco_opcode::p_logical_start) {
          break;
+      }
 
       if (!should_form_clause(current, candidate.get())) {
          add_to_hazard_query(&hq, candidate.get());
@@ -1167,12 +1267,14 @@ schedule_VMEM_store(sched_ctx& ctx, Bloc
       }
 
       if (perform_hazard_query(&hq, candidate.get(), false) != hazard_success ||
-          ctx.mv.downwards_move(cursor, true) != move_success)
+          ctx.mv.downwards_move(cursor, true) != move_success) {
          break;
+      }
 
       skip++;
    }
 
+   assert(skip >= 0);
    return skip;
 }
 
@@ -1243,10 +1345,11 @@ schedule_block(sched_ctx& ctx, Program*
 void
 schedule_program(Program* program)
 {
-   /* don't use program->max_reg_demand because that is affected by max_waves_per_simd */
    RegisterDemand demand;
-   for (Block& block : program->blocks)
+   for (Block& block : program->blocks) {
       demand.update(block.register_demand);
+   }
+   demand.vgpr += program->config->num_shared_vgprs / 2; // Restore original shared VGPR adjustment
 
    sched_ctx ctx;
    ctx.gfx_level = program->gfx_level;
@@ -1255,13 +1358,29 @@ schedule_program(Program* program)
    ctx.mv.RAR_dependencies_clause.resize(program->peekAllocationId());
 
    const int wave_factor = program->gfx_level >= GFX10 ? 2 : 1;
-   const float reg_file_multiple = program->dev.physical_vgprs / (256.0 * wave_factor);
-   const int wave_minimum = std::max<int>(program->min_waves, 4 * wave_factor * reg_file_multiple);
+   const float reg_file_multiple = static_cast<float>(program->dev.physical_vgprs) / (256.0f * static_cast<float>(wave_factor));
+   /* GFX9-specific: Vega's latency hiding is paramount and scales exceptionally well with occupancy.
+    * Enforce a higher minimum wave count to ensure the hardware has sufficient independent
+    * work to switch to during memory or execution stalls. 8 waves (512 threads) is a known
+    * sweet spot for a Vega SIMD.
+    */
+   int wave_minimum = std::max<int>(program->min_waves, static_cast<int>(4 * wave_factor * reg_file_multiple));
+   if (ctx.gfx_level == GFX9) {
+      wave_minimum = std::max(wave_minimum, 8 * wave_factor);
+   }
 
    /* If we already have less waves than the minimum, don't reduce them further.
     * Otherwise, sacrifice some waves and use more VGPRs, in order to improve scheduling.
     */
-   int vgpr_demand = std::max<int>(24, demand.vgpr) + 12 * reg_file_multiple;
+   int vgpr_demand = std::max<int>(24, demand.vgpr) + static_cast<int>(12 * reg_file_multiple);
+   /* GFX9-specific: Be more lenient on VGPR demand for Vega. This encourages the scheduler
+    * to target the higher `wave_minimum`, leveraging Vega's large physical register file.
+    * This risk is mitigated by the pressure-aware dynamic scheduling windows.
+    */
+   if (ctx.gfx_level == GFX9) {
+      vgpr_demand = static_cast<int>(vgpr_demand * 0.9f); // Lower effective demand by 10%
+      vgpr_demand = std::max(20, vgpr_demand - 4);
+   }
    int target_waves = std::max(wave_minimum, program->dev.physical_vgprs / vgpr_demand);
    target_waves = max_suitable_waves(program, std::min<int>(program->num_waves, target_waves));
    assert(target_waves >= program->min_waves);
@@ -1281,8 +1400,9 @@ schedule_program(Program* program)
       ctx.schedule_pos_export_div = 4;
    }
 
-   for (Block& block : program->blocks)
+   for (Block& block : program->blocks) {
       schedule_block(ctx, program, &block);
+   }
 
    /* update max_reg_demand and num_waves */
    RegisterDemand new_demand;
@@ -1292,8 +1412,9 @@ schedule_program(Program* program)
    update_vgpr_sgpr_demand(program, new_demand);
 
    /* Validate live variable information */
-   if (!validate_live_vars(program))
+   if (!validate_live_vars(program)) {
       abort();
+   }
 }
 
 } // namespace aco
