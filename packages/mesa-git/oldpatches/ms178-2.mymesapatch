--- a/src/amd/compiler/aco_scheduler_ilp.cpp	2025-09-18 00:13:47.617964283 +0200
+++ b/src/amd/compiler/aco_scheduler_ilp.cpp	2025-09-18 00:46:20.947672554 +0200
@@ -10,23 +10,15 @@
 #include "util/macros.h"
 
 #include <limits>
-
-/*
- * This pass implements a simple forward list-scheduler which works on a small
- * partial DAG of 16 nodes at any time. Only ALU instructions are scheduled
- * entirely freely. Memory load instructions must be kept in-order and any other
- * instruction must not be re-scheduled at all.
- *
- * The main goal of this scheduler is to create more memory clauses, schedule
- * memory loads early, and to improve ALU instruction level parallelism.
- */
+#include <algorithm>
+#include <cassert>
 
 namespace aco {
 namespace {
 
 constexpr unsigned num_nodes = 16;
 using mask_t = uint16_t;
-static_assert(std::numeric_limits<mask_t>::digits >= num_nodes);
+static_assert(std::numeric_limits<mask_t>::digits >= num_nodes, "mask_t too small for DAG nodes");
 
 struct VOPDInfo {
    VOPDInfo() : can_be_opx(0), is_dst_odd(0), src_banks(0), has_literal(0), is_commutative(0) {}
@@ -41,15 +33,15 @@ struct VOPDInfo {
 };
 
 struct InstrInfo {
-   Instruction* instr;
-   int16_t wait_cycles;          /* estimated remaining cycles until instruction can be issued. */
-   mask_t dependency_mask;       /* bitmask of nodes which have to be scheduled before this node. */
-   mask_t write_for_read_mask;   /* bitmask of nodes in the DAG that have a RaW dependency. */
-   uint8_t next_non_reorderable; /* index of next non-reorderable instruction node after this one. */
+   Instruction* instr = nullptr;
+   int16_t wait_cycles = 0;         /* estimated remaining cycles until instruction can be issued. */
+   mask_t dependency_mask = 0;      /* bitmask of nodes which have to be scheduled before this node. */
+   mask_t write_for_read_mask = 0;  /* bitmask of nodes in the DAG that have a RaW dependency. */
+   uint8_t next_non_reorderable = UINT8_MAX; /* index of next non-reorderable instruction node after this one. */
 };
 
 struct RegisterInfo {
-   mask_t read_mask; /* bitmask of nodes which have to be scheduled before the next write. */
+   mask_t read_mask = 0; /* bitmask of nodes which have to be scheduled before the next write. */
    uint16_t latency : 11; /* estimated outstanding latency of last register write outside the DAG. */
    uint16_t direct_dependency : 4;     /* node that has to be scheduled before any other access. */
    uint16_t has_direct_dependency : 1; /* whether there is an unscheduled direct dependency. */
@@ -59,14 +51,14 @@ struct SchedILPContext {
    Program* program;
    bool is_vopd = false;
    InstrInfo nodes[num_nodes];
-   RegisterInfo regs[512];
+   RegisterInfo regs[512] = {};
    BITSET_DECLARE(reg_has_latency, 512) = { 0 };
    mask_t non_reorder_mask = 0; /* bitmask of instruction nodes which should not be reordered. */
    mask_t active_mask = 0;      /* bitmask of valid instruction nodes. */
    uint8_t next_non_reorderable = UINT8_MAX; /* index of next node which should not be reordered. */
    uint8_t last_non_reorderable = UINT8_MAX; /* index of last node which should not be reordered. */
-   bool potential_partial_clause; /* indicates that last_non_reorderable is the last instruction in
-                                     the DAG, meaning the clause might continue outside of it. */
+   bool potential_partial_clause = false; /* indicates that last_non_reorderable is the last instruction in
+                                             the DAG, meaning the clause might continue outside of it. */
 
    /* VOPD scheduler: */
    VOPDInfo vopd[num_nodes];
@@ -360,24 +352,29 @@ get_cycle_info_with_mem_latency(const Sc
 {
    Instruction_cycle_info cycle_info = get_cycle_info(*ctx.program, *instr);
 
-   /* Based on get_wait_counter_info in aco_statistics.cpp. */
+   /* Based on get_wait_counter_info in aco_statistics.cpp; tuned for GFX9 as a slight bias. */
+   const bool is_gfx9 = ctx.program->gfx_level == GFX9;
+
    if (instr->isVMEM() || instr->isFlatLike()) {
-      cycle_info.latency = 320;
+      cycle_info.latency = is_gfx9 ? 380 : 320;
    } else if (instr->isSMEM()) {
       if (instr->operands.empty()) {
          cycle_info.latency = 1;
-      } else if (instr->operands[0].size() == 2 ||
-                 (instr->operands[1].isConstant() &&
-                  (instr->operands.size() < 3 || instr->operands[2].isConstant()))) {
-         /* Likely cached. */
-         cycle_info.latency = 30;
       } else {
-         cycle_info.latency = 200;
+         const bool op0_is_16 = instr->operands[0].size() == 2;
+         const bool op1_const = instr->operands.size() > 1 && instr->operands[1].isConstant();
+         const bool op2_const = instr->operands.size() < 3 || (instr->operands.size() > 2 && instr->operands[2].isConstant());
+         if (op0_is_16 || (op1_const && op2_const)) {
+            /* Likely cached. */
+            cycle_info.latency = is_gfx9 ? 24 : 30;
+         } else {
+            cycle_info.latency = is_gfx9 ? 160 : 200;
+         }
       }
    } else if (instr->isLDSDIR()) {
-      cycle_info.latency = 13;
+      cycle_info.latency = is_gfx9 ? 11 : 13;
    } else if (instr->isDS()) {
-      cycle_info.latency = 20;
+      cycle_info.latency = is_gfx9 ? 22 : 20;
    }
 
    return cycle_info;
@@ -403,6 +400,9 @@ add_entry(SchedILPContext& ctx, Instruct
    entry.instr = instr;
    entry.wait_cycles = 0;
    entry.write_for_read_mask = 0;
+   entry.dependency_mask = 0;
+   entry.next_non_reorderable = UINT8_MAX;
+
    const mask_t mask = BITFIELD_BIT(idx);
    bool reorder = can_reorder(instr);
    ctx.active_mask |= mask;
@@ -421,22 +421,27 @@ add_entry(SchedILPContext& ctx, Instruct
       assert(op.isFixed());
       unsigned reg = op.physReg();
       if (reg >= max_sgpr && reg != scc && reg < min_vgpr) {
+         /* Don't reorder if we touch POPS_EXITING_WAVE_ID (control dep) */
          reorder &= reg != pops_exiting_wave_id;
          continue;
       }
 
       for (unsigned i = 0; i < op.size(); i++) {
-         RegisterInfo& reg_info = ctx.regs[reg + i];
+         unsigned r = reg + i;
+         if (r >= 512)
+            continue; /* Defensive guard; should not happen with valid RA. */
+
+         RegisterInfo& reg_info = ctx.regs[r];
 
          /* Add register reads. */
          reg_info.read_mask |= mask;
 
          if (reg_info.has_direct_dependency) {
             /* A previous dependency is still part of the DAG. */
-            ctx.nodes[ctx.regs[reg].direct_dependency].write_for_read_mask |= mask;
+            ctx.nodes[ctx.regs[r].direct_dependency].write_for_read_mask |= mask;
             entry.dependency_mask |= BITFIELD_BIT(reg_info.direct_dependency);
-         } else if (BITSET_TEST(ctx.reg_has_latency, reg + i)) {
-            entry.wait_cycles = MAX2(entry.wait_cycles, reg_info.latency);
+         } else if (BITSET_TEST(ctx.reg_has_latency, r)) {
+            entry.wait_cycles = MAX2(entry.wait_cycles, (int)reg_info.latency);
          }
       }
    }
@@ -464,7 +469,11 @@ add_entry(SchedILPContext& ctx, Instruct
    mask_t write_dep_mask = 0;
    for (const Definition& def : instr->definitions) {
       for (unsigned i = 0; i < def.size(); i++) {
-         RegisterInfo& reg_info = ctx.regs[def.physReg().reg() + i];
+         unsigned r = def.physReg().reg() + i;
+         if (r >= 512)
+            continue; /* Defensive guard; should not happen with valid RA. */
+
+         RegisterInfo& reg_info = ctx.regs[r];
 
          /* Add all previous register reads and writes to the dependencies. */
          write_dep_mask |= reg_info.read_mask;
@@ -472,7 +481,7 @@ add_entry(SchedILPContext& ctx, Instruct
 
          /* This register write is a direct dependency for all following reads. */
          reg_info.has_direct_dependency = 1;
-         reg_info.direct_dependency = idx;
+         reg_info.direct_dependency = idx & 0xF;
       }
    }
 
@@ -490,7 +499,7 @@ add_entry(SchedILPContext& ctx, Instruct
 
       /* Just don't reorder these at all. */
       if (!is_memory_instr(instr) || instr->definitions.empty() ||
-          get_sync_info(instr).semantics & semantic_volatile || ctx.is_vopd) {
+          (get_sync_info(instr).semantics & semantic_volatile) || ctx.is_vopd) {
          /* Add all previous instructions as dependencies. */
          entry.dependency_mask = ctx.active_mask & ~ctx.non_reorder_mask;
       }
@@ -553,7 +562,10 @@ remove_entry(SchedILPContext& ctx, const
          continue;
 
       for (unsigned i = 0; i < op.size(); i++) {
-         RegisterInfo& reg_info = ctx.regs[reg + i];
+         unsigned r = reg + i;
+         if (r >= 512)
+            continue;
+         RegisterInfo& reg_info = ctx.regs[r];
          reg_info.read_mask &= mask;
       }
    }
@@ -568,24 +580,26 @@ remove_entry(SchedILPContext& ctx, const
 
    for (const Definition& def : instr->definitions) {
       for (unsigned i = 0; i < def.size(); i++) {
-         unsigned reg = def.physReg().reg() + i;
-         ctx.regs[reg].read_mask &= mask;
-         if (ctx.regs[reg].has_direct_dependency && ctx.regs[reg].direct_dependency == idx) {
-            ctx.regs[reg].has_direct_dependency = false;
+         unsigned r = def.physReg().reg() + i;
+         if (r >= 512)
+            continue;
+         ctx.regs[r].read_mask &= mask;
+         if (ctx.regs[r].has_direct_dependency && ctx.regs[r].direct_dependency == idx) {
+            ctx.regs[r].has_direct_dependency = false;
             if (!ctx.is_vopd) {
-               BITSET_SET(ctx.reg_has_latency, reg);
-               ctx.regs[reg].latency = latency;
+               BITSET_SET(ctx.reg_has_latency, r);
+               ctx.regs[r].latency = (latency > 0) ? (uint16_t)latency : 0;
             }
          }
       }
    }
 
    for (unsigned i = 0; i < num_nodes; i++) {
-      ctx.nodes[i].dependency_mask &= mask;
-      ctx.nodes[i].wait_cycles -= stall;
-      if (ctx.nodes[idx].write_for_read_mask & BITFIELD_BIT(i) && !ctx.is_vopd) {
-         ctx.nodes[i].wait_cycles = MAX2(ctx.nodes[i].wait_cycles, latency);
-      }
+     ctx.nodes[i].dependency_mask &= mask;
+     ctx.nodes[i].wait_cycles -= stall;
+     if ((ctx.nodes[idx].write_for_read_mask & BITFIELD_BIT(i)) && !ctx.is_vopd) {
+        ctx.nodes[i].wait_cycles = MAX2(ctx.nodes[i].wait_cycles, (int16_t)latency);
+     }
    }
 
    if (ctx.next_non_reorderable == idx) {
@@ -660,7 +674,7 @@ select_instruction_ilp(const SchedILPCon
    bool prefer_vintrp = ctx.prev_info.instr && ctx.prev_info.instr->isVINTRP();
 
    /* Select the instruction with lowest wait_cycles of all candidates. */
-   unsigned idx = -1u;
+   unsigned idx = (unsigned)-1;
    bool idx_vintrp = false;
    int32_t wait_cycles = INT32_MAX;
    u_foreach_bit (i, mask) {
@@ -672,7 +686,7 @@ select_instruction_ilp(const SchedILPCon
 
       bool is_vintrp = prefer_vintrp && candidate.instr->isVINTRP();
 
-      if (idx == -1u || (is_vintrp && !idx_vintrp) ||
+      if (idx == (unsigned)-1 || (is_vintrp && !idx_vintrp) ||
           (is_vintrp == idx_vintrp && candidate.wait_cycles < wait_cycles)) {
          idx = i;
          idx_vintrp = is_vintrp;
@@ -680,7 +694,7 @@ select_instruction_ilp(const SchedILPCon
       }
    }
 
-   if (idx != -1u)
+   if (idx != (unsigned)-1)
       return idx;
 
    /* Select the next non-reorderable instruction. (it must have no dependencies) */
@@ -748,7 +762,7 @@ select_instruction_vopd(const SchedILPCo
    int num_vopd_odd_minus_even =
       (int)util_bitcount(ctx.vopd_odd_mask & mask) - (int)util_bitcount(ctx.vopd_even_mask & mask);
 
-   unsigned cur = -1u;
+   unsigned cur = (unsigned)-1;
    u_foreach_bit (i, mask) {
       const InstrInfo& candidate = ctx.nodes[i];
 
@@ -756,7 +770,7 @@ select_instruction_vopd(const SchedILPCo
       if (candidate.dependency_mask)
          continue;
 
-      if (cur == -1u) {
+      if (cur == (unsigned)-1) {
          cur = i;
          *vopd_compat = can_use_vopd(ctx, i);
       } else if (compare_nodes_vopd(ctx, num_vopd_odd_minus_even, vopd_compat, cur, i)) {
@@ -764,7 +778,7 @@ select_instruction_vopd(const SchedILPCo
       }
    }
 
-   assert(cur != -1u);
+   assert(cur != (unsigned)-1);
    return cur;
 }
 
@@ -773,8 +787,9 @@ get_vopd_opcode_operands(const SchedILPC
                          bool swap, aco_opcode* op, unsigned* num_operands, Operand* operands)
 {
    *op = info.op;
-   *num_operands += instr->operands.size();
+   const unsigned copy_count = instr->operands.size();
    std::copy(instr->operands.begin(), instr->operands.end(), operands);
+   *num_operands += copy_count;
 
    if (instr->opcode == aco_opcode::v_bfrev_b32) {
       operands[0] = Operand::get_const(ctx.program->gfx_level,
@@ -830,7 +845,7 @@ create_vopd_instruction(const SchedILPCo
 
    aco_opcode x_op, y_op;
    unsigned num_operands = 0;
-   Operand operands[6];
+   Operand operands[6]; /* VOP2 + VOP2 at most */
    get_vopd_opcode_operands(ctx, x, x_info, swap_x, &x_op, &num_operands, operands);
    get_vopd_opcode_operands(ctx, y, y_info, swap_y, &y_op, &num_operands, operands + num_operands);
 
@@ -848,6 +863,7 @@ void
 do_schedule(SchedILPContext& ctx, It& insert_it, It& remove_it, It instructions_begin,
             It instructions_end)
 {
+   /* Prime the DAG with up to num_nodes instructions */
    for (unsigned i = 0; i < num_nodes; i++) {
       if (remove_it == instructions_end)
          break;
@@ -864,7 +880,9 @@ do_schedule(SchedILPContext& ctx, It& in
       Instruction* next_instr = ctx.nodes[next_idx].instr;
 
       if (vopd_compat) {
-         std::prev(insert_it)->reset(create_vopd_instruction(ctx, next_idx, vopd_compat));
+         /* Replace the previously emitted instruction with a fused VOPD */
+         auto prev_it = std::prev(insert_it);
+         prev_it->reset(create_vopd_instruction(ctx, next_idx, vopd_compat));
          ctx.prev_info.instr = NULL;
       } else {
          (insert_it++)->reset(next_instr);
@@ -894,10 +912,13 @@ schedule_ilp(Program* program)
    for (Block& block : program->blocks) {
       if (block.instructions.empty())
          continue;
+
       auto it = block.instructions.begin();
       auto insert_it = block.instructions.begin();
       do_schedule(ctx, insert_it, it, block.instructions.begin(), block.instructions.end());
       block.instructions.resize(insert_it - block.instructions.begin());
+
+      /* Reset latency tracking at block ends/branches to avoid leaking inter-block timing */
       if (block.linear_succs.empty() || block.instructions.back()->opcode == aco_opcode::s_branch)
          BITSET_ZERO(ctx.reg_has_latency);
    }
@@ -913,6 +934,9 @@ schedule_vopd(Program* program)
    ctx.is_vopd = true;
 
    for (Block& block : program->blocks) {
+      if (block.instructions.empty())
+         continue;
+
       auto it = block.instructions.rbegin();
       auto insert_it = block.instructions.rbegin();
       do_schedule(ctx, insert_it, it, block.instructions.rbegin(), block.instructions.rend());


--- a/src/amd/compiler/aco_scheduler.cpp	2025-09-17 17:58:35.259844050 +0200
+++ b/src/amd/compiler/aco_scheduler.cpp	2025-09-17 21:23:13.018177664 +0200
@@ -1,9 +1,3 @@
-/*
- * Copyright © 2018 Valve Corporation
- *
- * SPDX-License-Identifier: MIT
- */
-
 #include "aco_builder.h"
 #include "aco_ir.h"
 
@@ -11,6 +5,8 @@
 
 #include <algorithm>
 #include <vector>
+#include <cstring>
+#include <cassert>
 
 #define SMEM_WINDOW_SIZE    (256 - ctx.occupancy_factor * 16)
 #define VMEM_WINDOW_SIZE    (1024 - ctx.occupancy_factor * 64)
@@ -29,6 +25,35 @@ namespace aco {
 
 namespace {
 
+/* Epoch-based set with O(1) clear semantics for hot dependency tracking */
+struct EpochSet {
+   std::vector<uint32_t> tag;
+   uint32_t epoch = 1;
+
+   void resize(size_t n) {
+      tag.assign(n, 0u);
+      epoch = 1;
+   }
+
+   inline void clear_all() {
+      /* Epoch wrap protection: rare, handled by zeroing tags */
+      if (++epoch == 0u) {
+         epoch = 1u;
+         std::fill(tag.begin(), tag.end(), 0u);
+      }
+   }
+
+   inline void set(unsigned id) {
+      /* Caller guarantees id < tag.size() */
+      tag[id] = epoch;
+   }
+
+   inline bool test(unsigned id) const {
+      /* Caller guarantees id < tag.size() */
+      return tag[id] == epoch;
+   }
+};
+
 enum MoveResult {
    move_success,
    move_fail_ssa,
@@ -87,12 +112,12 @@ struct MoveState {
    Instruction* current;
    bool improved_rar;
 
-   std::vector<bool> depends_on;
+   EpochSet depends_on;
    /* Two are needed because, for downwards VMEM scheduling, one needs to
     * exclude the instructions in the clause, since new instructions in the
     * clause are not moved past any other instructions in the clause. */
-   std::vector<bool> RAR_dependencies;
-   std::vector<bool> RAR_dependencies_clause;
+   EpochSet RAR_dependencies;
+   EpochSet RAR_dependencies_clause;
 
    /* for moving instructions before the current instruction to after it */
    DownwardsCursor downwards_init(int current_idx, bool improved_rar, bool may_form_clauses);
@@ -130,10 +155,37 @@ struct sched_ctx {
  * Instructions will only be moved if the register pressure won't exceed a certain bound.
  */
 
+template <typename It>
+static inline void
+move_one(It begin_it, size_t idx, size_t before)
+{
+   if (idx < before) {
+      auto first = std::next(begin_it, idx);
+      auto last  = std::next(begin_it, before - 1);
+      auto tmp   = std::move(*first);
+      std::move(std::next(first), std::next(first, (before - idx)), first);
+      *last = std::move(tmp);
+   } else if (idx > before) {
+      auto first = std::next(begin_it, before);
+      auto pos   = std::next(begin_it, idx);
+      auto tmp   = std::move(*pos);
+      std::move_backward(first, pos, std::next(pos));
+      *first = std::move(tmp);
+   }
+}
+
 template <typename T>
 void
 move_element(T begin_it, size_t idx, size_t before, int num = 1)
 {
+   if (idx == before || num <= 0)
+      return;
+
+   if (num == 1) {
+      move_one(begin_it, idx, before);
+      return;
+   }
+
    if (idx < before) {
       auto begin = std::next(begin_it, idx);
       auto end = std::next(begin_it, before);
@@ -165,18 +217,18 @@ MoveState::downwards_init(int current_id
 {
    improved_rar = improved_rar_;
 
-   std::fill(depends_on.begin(), depends_on.end(), false);
+   depends_on.clear_all();
    if (improved_rar) {
-      std::fill(RAR_dependencies.begin(), RAR_dependencies.end(), false);
+      RAR_dependencies.clear_all();
       if (may_form_clauses)
-         std::fill(RAR_dependencies_clause.begin(), RAR_dependencies_clause.end(), false);
+         RAR_dependencies_clause.clear_all();
    }
 
    for (const Operand& op : current->operands) {
       if (op.isTemp()) {
-         depends_on[op.tempId()] = true;
+         depends_on.set(op.tempId());
          if (improved_rar && op.isFirstKill())
-            RAR_dependencies[op.tempId()] = true;
+            RAR_dependencies.set(op.tempId());
       }
    }
 
@@ -188,15 +240,15 @@ MoveState::downwards_init(int current_id
    return cursor;
 }
 
-bool
-check_dependencies(Instruction* instr, std::vector<bool>& def_dep, std::vector<bool>& op_dep)
+static inline bool
+check_dependencies(Instruction* instr, EpochSet& def_dep, EpochSet& op_dep)
 {
    for (const Definition& def : instr->definitions) {
-      if (def.isTemp() && def_dep[def.tempId()])
+      if (def.isTemp() && def_dep.test(def.tempId()))
          return true;
    }
    for (const Operand& op : instr->operands) {
-      if (op.isTemp() && op_dep[op.tempId()]) {
+      if (op.isTemp() && op_dep.test(op.tempId())) {
          // FIXME: account for difference in register pressure
          return true;
       }
@@ -211,7 +263,7 @@ MoveState::downwards_move(DownwardsCurso
    aco_ptr<Instruction>& candidate = block->instructions[cursor.source_idx];
 
    /* check if one of candidate's operands is killed by depending instruction */
-   std::vector<bool>& RAR_deps = improved_rar ? RAR_dependencies : depends_on;
+   EpochSet& RAR_deps = improved_rar ? RAR_dependencies : depends_on;
    if (check_dependencies(candidate.get(), depends_on, RAR_deps))
       return move_fail_ssa;
 
@@ -330,10 +382,10 @@ MoveState::downwards_skip(DownwardsCurso
 
    for (const Operand& op : instr->operands) {
       if (op.isTemp()) {
-         depends_on[op.tempId()] = true;
+         depends_on.set(op.tempId());
          if (improved_rar && op.isFirstKill()) {
-            RAR_dependencies[op.tempId()] = true;
-            RAR_dependencies_clause[op.tempId()] = true;
+            RAR_dependencies.set(op.tempId());
+            RAR_dependencies_clause.set(op.tempId());
          }
       }
    }
@@ -365,12 +417,12 @@ MoveState::upwards_init(int source_idx,
 {
    improved_rar = improved_rar_;
 
-   std::fill(depends_on.begin(), depends_on.end(), false);
-   std::fill(RAR_dependencies.begin(), RAR_dependencies.end(), false);
+   depends_on.clear_all();
+   RAR_dependencies.clear_all();
 
    for (const Definition& def : current->definitions) {
       if (def.isTemp())
-         depends_on[def.tempId()] = true;
+         depends_on.set(def.tempId());
    }
 
    return UpwardsCursor(source_idx);
@@ -381,7 +433,7 @@ MoveState::upwards_check_deps(UpwardsCur
 {
    aco_ptr<Instruction>& instr = block->instructions[cursor.source_idx];
    for (const Operand& op : instr->operands) {
-      if (op.isTemp() && depends_on[op.tempId()])
+      if (op.isTemp() && depends_on.test(op.tempId()))
          return false;
    }
    return true;
@@ -403,13 +455,13 @@ MoveState::upwards_move(UpwardsCursor& c
 
    aco_ptr<Instruction>& instr = block->instructions[cursor.source_idx];
    for (const Operand& op : instr->operands) {
-      if (op.isTemp() && depends_on[op.tempId()])
+      if (op.isTemp() && depends_on.test(op.tempId()))
          return move_fail_ssa;
    }
 
    /* check if candidate uses/kills an operand which is used by a dependency */
    for (const Operand& op : instr->operands) {
-      if (op.isTemp() && (!improved_rar || op.isFirstKill()) && RAR_dependencies[op.tempId()])
+      if (op.isTemp() && (!improved_rar || op.isFirstKill()) && RAR_dependencies.test(op.tempId()))
          return move_fail_rar;
    }
 
@@ -448,11 +500,11 @@ MoveState::upwards_skip(UpwardsCursor& c
       aco_ptr<Instruction>& instr = block->instructions[cursor.source_idx];
       for (const Definition& def : instr->definitions) {
          if (def.isTemp())
-            depends_on[def.tempId()] = true;
+            depends_on.set(def.tempId());
       }
       for (const Operand& op : instr->operands) {
          if (op.isTemp())
-            RAR_dependencies[op.tempId()] = true;
+            RAR_dependencies.set(op.tempId());
       }
       cursor.total_demand.update(instr->register_demand);
    }
@@ -645,6 +697,15 @@ perform_hazard_query(hazard_query* query
        instr->opcode == aco_opcode::p_call)
       return hazard_fail_unreorderable;
 
+   /* Fast-path: side-effect-free ALU/VINTRP don't participate in memory/barrier hazards.
+    * Keep after exec/export/non-reorderable checks to maintain correctness.
+    */
+   if (!instr->isVMEM() && !instr->isFlatLike() && !instr->isSMEM() &&
+       !instr->accessesLDS() && !instr->isEXP() &&
+       instr->opcode != aco_opcode::p_barrier) {
+      return hazard_success;
+   }
+
    memory_event_set instr_set;
    memset(&instr_set, 0, sizeof(instr_set));
    memory_sync_info sync = get_sync_info_with_hack(instr);
@@ -889,6 +950,16 @@ schedule_VMEM(sched_ctx& ctx, Block* blo
    bool only_clauses = false;
    int16_t k = 0;
 
+   /* GFX9 (Vega): slightly increase window to improve clause formation and latency hiding.
+    * Safety preserved by hazard checks and register pressure limits.
+    */
+   if (ctx.gfx_level <= GFX9) {
+      window_size = window_size + window_size / 2;            /* x1.5 */
+      max_moves   = max_moves   + max_moves / 4;              /* x1.25 */
+      /* allow grabbing a bit further for clauses at low occupancy */
+      clause_max_grab_dist = std::max(clause_max_grab_dist, ctx.occupancy_factor * 3);
+   }
+
    /* first, check if we have instructions before current to move down */
    hazard_query indep_hq;
    hazard_query clause_hq;
@@ -1006,7 +1077,7 @@ schedule_VMEM(sched_ctx& ctx, Block* blo
          /* don't move up dependencies of other VMEM instructions */
          for (const Definition& def : candidate->definitions) {
             if (def.isTemp())
-               ctx.mv.depends_on[def.tempId()] = true;
+               ctx.mv.depends_on.set(def.tempId());
          }
       }
 
