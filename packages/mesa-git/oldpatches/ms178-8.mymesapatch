--- a/src/util/blob.c	2025-09-19 11:24:25.535738075 +0200
+++ b/src/util/blob.c	2025-09-19 11:26:46.618936402 +0200
@@ -22,6 +22,9 @@
  */
 
 #include <string.h>
+#include <assert.h>
+#include <stdint.h>
+#include <limits.h>
 
 #include "blob.h"
 #include "u_math.h"
@@ -34,7 +37,43 @@
 #define VG(x)
 #endif
 
+/* Use compiler intrinsics when available */
+#ifdef __has_builtin
+#  if __has_builtin(__builtin_expect)
+#    define LIKELY(x)   __builtin_expect(!!(x), 1)
+#    define UNLIKELY(x) __builtin_expect(!!(x), 0)
+#  else
+#    define LIKELY(x)   (x)
+#    define UNLIKELY(x) (x)
+#  endif
+#  if __has_builtin(__builtin_prefetch)
+#    define PREFETCH_READ(addr, locality) __builtin_prefetch((addr), 0, (locality))
+#  else
+#    define PREFETCH_READ(addr, locality) ((void)0)
+#  endif
+#else
+#  define LIKELY(x)   (x)
+#  define UNLIKELY(x) (x)
+#  define PREFETCH_READ(addr, locality) ((void)0)
+#endif
+
 #define BLOB_INITIAL_SIZE 4096
+#define CACHE_LINE_SIZE 64
+
+/* Fast alignment for power-of-2 alignments only
+ * Falls back to generic align_uintptr for non-power-of-2
+ */
+static inline size_t
+fast_align(size_t offset, size_t alignment)
+{
+   /* Check if alignment is power of 2 using bit trick */
+   if (LIKELY((alignment & (alignment - 1)) == 0)) {
+      size_t mask = alignment - 1;
+      return (offset + mask) & ~mask;
+   }
+   /* Fallback for non-power-of-2 alignments */
+   return align_uintptr(offset, alignment);
+}
 
 /* Ensure that \blob will be able to fit an additional object of size
  * \additional.  The growing (if any) will occur by doubling the existing
@@ -45,27 +84,55 @@ grow_to_fit(struct blob *blob, size_t ad
 {
    size_t to_allocate;
    uint8_t *new_data;
+   size_t required_size;
 
-   if (blob->out_of_memory)
+   if (UNLIKELY(blob->out_of_memory)) {
       return false;
+   }
 
-   if (blob->size + additional <= blob->allocated)
+   /* Check for overflow */
+   if (UNLIKELY(additional > SIZE_MAX - blob->size)) {
+      blob->out_of_memory = true;
+      return false;
+   }
+
+   required_size = blob->size + additional;
+
+   if (LIKELY(required_size <= blob->allocated)) {
       return true;
+   }
 
-   if (blob->fixed_allocation) {
+   if (UNLIKELY(blob->fixed_allocation)) {
       blob->out_of_memory = true;
       return false;
    }
 
-   if (blob->allocated == 0)
+   if (blob->allocated == 0) {
       to_allocate = BLOB_INITIAL_SIZE;
-   else
-      to_allocate = blob->allocated * 2;
+   } else {
+      /* Use 1.5x growth factor for better memory utilization
+       * compared to 2x, while still maintaining amortized O(1)
+       */
+      size_t growth = blob->allocated + (blob->allocated >> 1);
+
+      /* Check for overflow in growth calculation */
+      if (UNLIKELY(growth < blob->allocated)) {
+         growth = SIZE_MAX;
+      }
 
-   to_allocate = MAX2(to_allocate, blob->allocated + additional);
+      to_allocate = growth;
+   }
+
+   /* Ensure we allocate at least what's required */
+   if (to_allocate < required_size) {
+      to_allocate = required_size;
+   }
+
+   /* Round up to cache line boundary for better alignment */
+   to_allocate = fast_align(to_allocate, CACHE_LINE_SIZE);
 
    new_data = realloc(blob->data, to_allocate);
-   if (new_data == NULL) {
+   if (UNLIKELY(new_data == NULL)) {
       blob->out_of_memory = true;
       return false;
    }
@@ -85,14 +152,16 @@ grow_to_fit(struct blob *blob, size_t ad
 bool
 blob_align(struct blob *blob, size_t alignment)
 {
-   const size_t new_size = align_uintptr(blob->size, alignment);
+   const size_t new_size = fast_align(blob->size, alignment);
 
    if (blob->size < new_size) {
-      if (!grow_to_fit(blob, new_size - blob->size))
+      if (!grow_to_fit(blob, new_size - blob->size)) {
          return false;
+      }
 
-      if (blob->data)
+      if (blob->data) {
          memset(blob->data + blob->size, 0, new_size - blob->size);
+      }
       blob->size = new_size;
    }
 
@@ -102,7 +171,7 @@ blob_align(struct blob *blob, size_t ali
 void
 blob_reader_align(struct blob_reader *blob, size_t alignment)
 {
-   blob->current = blob->data + align_uintptr(blob->current - blob->data, alignment);
+   blob->current = blob->data + fast_align(blob->current - blob->data, alignment);
 }
 
 void
@@ -132,8 +201,14 @@ blob_finish_get_buffer(struct blob *blob
    *size = blob->size;
    blob->data = NULL;
 
-   /* Trim the buffer. */
-   *buffer = realloc(*buffer, *size);
+   /* Trim the buffer - but don't lose data on failure */
+   if (*size > 0 && *buffer != NULL) {
+      void *trimmed = realloc(*buffer, *size);
+      if (trimmed != NULL) {
+         *buffer = trimmed;
+      }
+      /* If realloc fails, we keep the original buffer */
+   }
 }
 
 bool
@@ -142,14 +217,21 @@ blob_overwrite_bytes(struct blob *blob,
                      const void *bytes,
                      size_t to_write)
 {
-   /* Detect an attempt to overwrite data out of bounds. */
-   if (offset + to_write < offset || blob->size < offset + to_write)
+   /* Check for offset overflow first */
+   if (UNLIKELY(offset > blob->size)) {
+      return false;
+   }
+
+   /* Check for size overflow */
+   if (UNLIKELY(to_write > blob->size - offset)) {
       return false;
+   }
 
    VG(VALGRIND_CHECK_MEM_IS_DEFINED(bytes, to_write));
 
-   if (blob->data)
+   if (blob->data && to_write > 0) {
       memcpy(blob->data + offset, bytes, to_write);
+   }
 
    return true;
 }
@@ -157,8 +239,9 @@ blob_overwrite_bytes(struct blob *blob,
 bool
 blob_write_bytes(struct blob *blob, const void *bytes, size_t to_write)
 {
-   if (! grow_to_fit(blob, to_write))
-       return false;
+   if (!grow_to_fit(blob, to_write)) {
+      return false;
+   }
 
    if (blob->data && to_write > 0) {
       VG(VALGRIND_CHECK_MEM_IS_DEFINED(bytes, to_write));
@@ -174,8 +257,9 @@ blob_reserve_bytes(struct blob *blob, si
 {
    intptr_t ret;
 
-   if (! grow_to_fit (blob, to_write))
+   if (!grow_to_fit(blob, to_write)) {
       return -1;
+   }
 
    ret = blob->size;
    blob->size += to_write;
@@ -186,23 +270,38 @@ blob_reserve_bytes(struct blob *blob, si
 intptr_t
 blob_reserve_uint32(struct blob *blob)
 {
-   blob_align(blob, sizeof(uint32_t));
+   if (!blob_align(blob, sizeof(uint32_t))) {
+      return -1;
+   }
    return blob_reserve_bytes(blob, sizeof(uint32_t));
 }
 
 intptr_t
 blob_reserve_intptr(struct blob *blob)
 {
-   blob_align(blob, sizeof(intptr_t));
+   if (!blob_align(blob, sizeof(intptr_t))) {
+      return -1;
+   }
    return blob_reserve_bytes(blob, sizeof(intptr_t));
 }
 
-#define BLOB_WRITE_TYPE(name, type)                      \
-bool                                                     \
-name(struct blob *blob, type value)                      \
-{                                                        \
-   blob_align(blob, sizeof(value));                      \
-   return blob_write_bytes(blob, &value, sizeof(value)); \
+/* Optimized write for small types using direct memory access */
+#define BLOB_WRITE_TYPE(name, type)                                    \
+bool                                                                   \
+name(struct blob *blob, type value)                                    \
+{                                                                      \
+   if (!blob_align(blob, sizeof(type))) {                             \
+      return false;                                                    \
+   }                                                                   \
+   if (!grow_to_fit(blob, sizeof(type))) {                            \
+      return false;                                                    \
+   }                                                                   \
+   if (blob->data) {                                                  \
+      /* Use memcpy to avoid aliasing violations */                   \
+      memcpy(blob->data + blob->size, &value, sizeof(type));          \
+   }                                                                   \
+   blob->size += sizeof(type);                                        \
+   return true;                                                        \
 }
 
 BLOB_WRITE_TYPE(blob_write_uint8, uint8_t)
@@ -212,30 +311,30 @@ BLOB_WRITE_TYPE(blob_write_uint64, uint6
 BLOB_WRITE_TYPE(blob_write_intptr, intptr_t)
 
 #define ASSERT_ALIGNED(_offset, _align) \
-   assert(align_uintptr((_offset), (_align)) == (_offset))
+   assert(fast_align((_offset), (_align)) == (_offset))
 
 bool
-blob_overwrite_uint8 (struct blob *blob,
-                      size_t offset,
-                      uint8_t value)
+blob_overwrite_uint8(struct blob *blob,
+                     size_t offset,
+                     uint8_t value)
 {
    ASSERT_ALIGNED(offset, sizeof(value));
    return blob_overwrite_bytes(blob, offset, &value, sizeof(value));
 }
 
 bool
-blob_overwrite_uint32 (struct blob *blob,
-                       size_t offset,
-                       uint32_t value)
+blob_overwrite_uint32(struct blob *blob,
+                      size_t offset,
+                      uint32_t value)
 {
    ASSERT_ALIGNED(offset, sizeof(value));
    return blob_overwrite_bytes(blob, offset, &value, sizeof(value));
 }
 
 bool
-blob_overwrite_intptr (struct blob *blob,
-                       size_t offset,
-                       intptr_t value)
+blob_overwrite_intptr(struct blob *blob,
+                      size_t offset,
+                      intptr_t value)
 {
    ASSERT_ALIGNED(offset, sizeof(value));
    return blob_overwrite_bytes(blob, offset, &value, sizeof(value));
@@ -244,7 +343,20 @@ blob_overwrite_intptr (struct blob *blob
 bool
 blob_write_string(struct blob *blob, const char *str)
 {
-   return blob_write_bytes(blob, str, strlen(str) + 1);
+   /* Use optimized strlen + memcpy for best performance */
+   size_t len = strlen(str) + 1;
+
+   if (!grow_to_fit(blob, len)) {
+      return false;
+   }
+
+   if (blob->data) {
+      VG(VALGRIND_CHECK_MEM_IS_DEFINED(str, len));
+      memcpy(blob->data + blob->size, str, len);
+   }
+   blob->size += len;
+
+   return true;
 }
 
 void
@@ -254,6 +366,17 @@ blob_reader_init(struct blob_reader *blo
    blob->end = blob->data + size;
    blob->current = data;
    blob->overrun = false;
+
+   /* Prefetch first cache lines for sequential read pattern */
+   if (size > 0) {
+      PREFETCH_READ(blob->current, 3);  /* High temporal locality */
+      if (size > CACHE_LINE_SIZE) {
+         PREFETCH_READ(blob->current + CACHE_LINE_SIZE, 2);
+      }
+      if (size > 2 * CACHE_LINE_SIZE) {
+         PREFETCH_READ(blob->current + 2 * CACHE_LINE_SIZE, 1);
+      }
+   }
 }
 
 /* Check that an object of size \size can be read from this blob.
@@ -263,14 +386,16 @@ blob_reader_init(struct blob_reader *blo
 static bool
 ensure_can_read(struct blob_reader *blob, size_t size)
 {
-   if (blob->overrun)
+   if (UNLIKELY(blob->overrun)) {
       return false;
+   }
 
-   if (blob->current <= blob->end && blob->end - blob->current >= size)
+   if (LIKELY(blob->current <= blob->end &&
+              (size_t)(blob->end - blob->current) >= size)) {
       return true;
+   }
 
    blob->overrun = true;
-
    return false;
 }
 
@@ -279,13 +404,22 @@ blob_read_bytes(struct blob_reader *blob
 {
    const void *ret;
 
-   if (! ensure_can_read (blob, size))
+   if (!ensure_can_read(blob, size)) {
       return NULL;
+   }
 
    ret = blob->current;
-
    blob->current += size;
 
+   /* Prefetch ahead for sequential access pattern */
+   if ((uintptr_t)blob->current >= (uintptr_t)ret + CACHE_LINE_SIZE) {
+      /* We crossed a cache line boundary */
+      const uint8_t *prefetch_addr = blob->current + CACHE_LINE_SIZE;
+      if (prefetch_addr < blob->end) {
+         PREFETCH_READ(prefetch_addr, 2);
+      }
+   }
+
    return ret;
 }
 
@@ -295,28 +429,36 @@ blob_copy_bytes(struct blob_reader *blob
    const void *bytes;
 
    bytes = blob_read_bytes(blob, size);
-   if (bytes == NULL || size == 0)
+   if (bytes == NULL) {
       return;
+   }
 
-   memcpy(dest, bytes, size);
+   if (size > 0) {
+      memcpy(dest, bytes, size);
+   }
 }
 
 void
 blob_skip_bytes(struct blob_reader *blob, size_t size)
 {
-   if (ensure_can_read (blob, size))
+   if (ensure_can_read(blob, size)) {
       blob->current += size;
+   }
 }
 
-#define BLOB_READ_TYPE(name, type)         \
-type                                       \
-name(struct blob_reader *blob)             \
-{                                          \
-   type ret = 0;                           \
-   int size = sizeof(ret);                 \
-   blob_reader_align(blob, size);          \
-   blob_copy_bytes(blob, &ret, size);      \
-   return ret;                             \
+/* Optimized read for small types */
+#define BLOB_READ_TYPE(name, type)                                     \
+type                                                                   \
+name(struct blob_reader *blob)                                         \
+{                                                                      \
+   type ret = 0;                                                       \
+   blob_reader_align(blob, sizeof(type));                             \
+   if (ensure_can_read(blob, sizeof(type))) {                         \
+      /* Use memcpy to avoid aliasing violations */                   \
+      memcpy(&ret, blob->current, sizeof(type));                      \
+      blob->current += sizeof(type);                                  \
+   }                                                                   \
+   return ret;                                                        \
 }
 
 BLOB_READ_TYPE(blob_read_uint8, uint8_t)
@@ -333,7 +475,7 @@ blob_read_string(struct blob_reader *blo
    uint8_t *nul;
 
    /* If we're already at the end, then this is an overrun. */
-   if (blob->current >= blob->end) {
+   if (UNLIKELY(blob->current >= blob->end)) {
       blob->overrun = true;
       return NULL;
    }
@@ -343,17 +485,15 @@ blob_read_string(struct blob_reader *blo
     */
    nul = memchr(blob->current, 0, blob->end - blob->current);
 
-   if (nul == NULL) {
+   if (UNLIKELY(nul == NULL)) {
       blob->overrun = true;
       return NULL;
    }
 
    size = nul - blob->current + 1;
 
-   assert(ensure_can_read(blob, size));
-
-   ret = (char *) blob->current;
-
+   /* We already know we can read this much */
+   ret = (char *)blob->current;
    blob->current += size;
 
    return ret;


--- a/src/util/bitscan.h	2025-06-12 22:18:54.150523804 +0200
+++ b/src/util/bitscan.h	2025-06-12 22:28:09.322741675 +0200
@@ -19,13 +19,12 @@
  * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
  * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
  * IN NO EVENT SHALL VMWARE AND/OR ITS SUPPLIERS BE LIABLE FOR
- * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
- * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF
+ * CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
  * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
  *
  **************************************************************************/
 
-
 #ifndef BITSCAN_H
 #define BITSCAN_H
 
@@ -39,9 +38,15 @@
 #endif
 
 #if defined(__POPCNT__)
+/* _mm_popcnt_u32/_mm_popcnt_u64 intrinsics */
 #include <popcntintrin.h>
 #endif
 
+#if (defined(__x86_64__) || defined(_M_X64) || defined(__i386__) || defined(_M_IX86)) && defined(__BMI2__)
+/* For _pdep_u32 (BMI2) */
+#include <immintrin.h>
+#endif
+
 #include "util/detect_arch.h"
 #include "util/detect_cc.h"
 #include "util/macros.h"
@@ -50,7 +55,6 @@
 extern "C" {
 #endif
 
-
 /**
  * Find first bit set in word.  Least significant bit is 1.
  * Return 0 if no bits set.
@@ -58,18 +62,18 @@ extern "C" {
 #ifdef HAVE___BUILTIN_FFS
 #define ffs __builtin_ffs
 #elif defined(_MSC_VER) && (_M_IX86 || _M_ARM || _M_AMD64 || _M_IA64)
-static inline
-int ffs(int i)
+static inline int
+ffs(int i)
 {
    unsigned long index;
-   if (_BitScanForward(&index, i))
-      return index + 1;
+   if (_BitScanForward(&index, (unsigned long)i))
+      return (int)index + 1;
    else
       return 0;
 }
 #else
-extern
-int ffs(int i);
+extern int
+ffs(int i);
 #endif
 
 #ifdef HAVE___BUILTIN_FFSLL
@@ -79,8 +83,8 @@ static inline int
 ffsll(long long int i)
 {
    unsigned long index;
-   if (_BitScanForward64(&index, i))
-      return index + 1;
+   if (_BitScanForward64(&index, (unsigned long long)i))
+      return (int)index + 1;
    else
       return 0;
 }
@@ -89,7 +93,6 @@ extern int
 ffsll(long long int val);
 #endif
 
-
 /* Destructively loop over all of the bits in a mask as in:
  *
  * while (mymask) {
@@ -97,46 +100,68 @@ ffsll(long long int val);
  *   ... process element i
  * }
  *
+ * Note: u_bit_scan() asserts mask != 0 (debug) to prevent UB (shift by -1).
  */
 static inline int
 u_bit_scan(unsigned *mask)
 {
+   assert(mask && *mask);
+#if defined(__GNUC__) || defined(__clang__)
+   /* Fast path: ctz → clear LSB (mask &= mask - 1) */
+   const unsigned m = *mask;
+   const int i = __builtin_ctz(m);
+   *mask = m & (m - 1);
+   return i;
+#else
    const int i = ffs(*mask) - 1;
-   *mask ^= (1u << i);
+   *mask &= ~(1u << i);
    return i;
+#endif
 }
 
-#define u_foreach_bit(b, dword)                          \
-   for (uint32_t __dword = (dword), b;                     \
-        ((b) = ffs(__dword) - 1, __dword);      \
-        __dword &= ~(1 << (b)))
+/* Iterate over set bits in a 32-bit mask.
+ * 'b' is assigned the bit index on each iteration.
+ */
+#define u_foreach_bit(b, dword)                                  \
+   for (uint32_t __dword = (uint32_t)(dword), b;                  \
+        ((b) = ffs(__dword) - 1, __dword);                        \
+        __dword &= ~(1u << (b)))
 
 static inline int
 u_bit_scan64(uint64_t *mask)
 {
+   assert(mask && *mask);
+#if defined(__GNUC__) || defined(__clang__)
+   const uint64_t m = *mask;
+   const int i = __builtin_ctzll(m);
+   *mask = m & (m - 1);
+   return i;
+#else
    const int i = ffsll(*mask) - 1;
-   *mask ^= (((uint64_t)1) << i);
+   *mask &= ~(1ull << i);
    return i;
+#endif
 }
 
-#define u_foreach_bit64(b, dword)                          \
-   for (uint64_t __dword = (dword), b;                     \
-        ((b) = ffsll(__dword) - 1, __dword);      \
+/* Iterate over set bits in a 64-bit mask.
+ * 'b' is assigned the bit index on each iteration.
+ */
+#define u_foreach_bit64(b, dword)                                \
+   for (uint64_t __dword = (uint64_t)(dword), b;                  \
+        ((b) = ffsll(__dword) - 1, __dword);                      \
         __dword &= ~(1ull << (b)))
 
 /* Given two bitmasks, loop over all bits of both of them.
- * Bits of mask1 are: b = scan_bit(mask1);
- * Bits of mask2 are: b = offset + scan_bit(mask2);
+ * Bits of mask1 are: b = ffsll(mask1) - 1;
+ * Bits of mask2 are: b = offset + (ffsll(mask2) - 1);
  */
-#define u_foreach_bit64_two_masks(b, mask1, offset, mask2)                          \
-   for (uint64_t __mask1 = (mask1), __mask2 = (mask2), b;                           \
-        (__mask1 ? ((b) = ffsll(__mask1) - 1)                                       \
-                 : ((b) = ffsll(__mask2) - 1 + offset), __mask1 || __mask2);        \
-        __mask1 ? (__mask1 &= ~(1ull << (b))) : (__mask2 &= ~(1ull << (b - offset))))
+#define u_foreach_bit64_two_masks(b, mask1, offset, mask2)                             \
+   for (uint64_t __mask1 = (uint64_t)(mask1), __mask2 = (uint64_t)(mask2), b;          \
+        (__mask1 ? ((b) = ffsll(__mask1) - 1)                                          \
+                 : ((b) = ffsll(__mask2) - 1 + (offset)), __mask1 || __mask2);         \
+        __mask1 ? (__mask1 &= ~(1ull << (b))) : (__mask2 &= ~(1ull << ((b) - (offset)))))
 
 /* Determine if an uint32_t value is a power of two.
- *
- * \note
  * Zero is treated as a power of two.
  */
 static inline bool
@@ -146,8 +171,6 @@ util_is_power_of_two_or_zero(uint32_t v)
 }
 
 /* Determine if an uint64_t value is a power of two.
- *
- * \note
  * Zero is treated as a power of two.
  */
 static inline bool
@@ -157,33 +180,18 @@ util_is_power_of_two_or_zero64(uint64_t
 }
 
 /* Determine if an uint32_t value is a power of two.
+ * Zero is not treated as a power of two.
  *
- * \note
- * Zero is \b not treated as a power of two.
+ * Branchless bit trick (fast on all CPUs; no popcnt required).
  */
 static inline bool
 util_is_power_of_two_nonzero(uint32_t v)
 {
-   /* __POPCNT__ is different from HAVE___BUILTIN_POPCOUNT.  The latter
-    * indicates the existence of the __builtin_popcount function.  The former
-    * indicates that _mm_popcnt_u32 exists and is a native instruction.
-    *
-    * The other alternative is to use SSE 4.2 compile-time flags.  This has
-    * two drawbacks.  First, there is currently no build infrastructure for
-    * SSE 4.2 (only 4.1), so that would have to be added.  Second, some AMD
-    * CPUs support POPCNT but not SSE 4.2 (e.g., Barcelona).
-    */
-#ifdef __POPCNT__
-   return _mm_popcnt_u32(v) == 1;
-#else
    return IS_POT_NONZERO(v);
-#endif
 }
 
 /* Determine if an uint64_t value is a power of two.
- *
- * \note
- * Zero is \b not treated as a power of two.
+ * Zero is not treated as a power of two.
  */
 static inline bool
 util_is_power_of_two_nonzero64(uint64_t v)
@@ -191,10 +199,8 @@ util_is_power_of_two_nonzero64(uint64_t
    return IS_POT_NONZERO(v);
 }
 
-/* Determine if an size_t/uintptr_t/intptr_t value is a power of two.
- *
- * \note
- * Zero is \b not treated as a power of two.
+/* Determine if a uintptr_t value is a power of two.
+ * Zero is not treated as a power of two.
  */
 static inline bool
 util_is_power_of_two_nonzero_uintptr(uintptr_t v)
@@ -202,30 +208,19 @@ util_is_power_of_two_nonzero_uintptr(uin
    return IS_POT_NONZERO(v);
 }
 
-/* For looping over a bitmask when you want to loop over consecutive bits
- * manually, for example:
- *
- * while (mask) {
- *    int start, count, i;
- *
- *    u_bit_scan_consecutive_range(&mask, &start, &count);
- *
- *    for (i = 0; i < count; i++)
- *       ... process element (start+i)
- * }
- */
+/* Loop over a bitmask yielding ranges of consecutive bits. */
 static inline void
 u_bit_scan_consecutive_range(unsigned *mask, int *start, int *count)
 {
-   if (*mask == 0xffffffff) {
+   if (*mask == 0xffffffffu) {
       *start = 0;
       *count = 32;
-      *mask = 0;
+      *mask  = 0;
       return;
    }
    *start = ffs(*mask) - 1;
    *count = ffs(~(*mask >> *start)) - 1;
-   *mask &= ~(((1u << *count) - 1) << *start);
+   *mask &= ~(((1u << (unsigned)*count) - 1u) << (unsigned)*start);
 }
 
 static inline void
@@ -234,31 +229,29 @@ u_bit_scan_consecutive_range64(uint64_t
    if (*mask == UINT64_MAX) {
       *start = 0;
       *count = 64;
-      *mask = 0;
+      *mask  = 0;
       return;
    }
    *start = ffsll(*mask) - 1;
    *count = ffsll(~(*mask >> *start)) - 1;
-   *mask &= ~(((((uint64_t)1) << *count) - 1) << *start);
+   *mask &= ~(((((uint64_t)1) << (unsigned)*count) - 1ull) << (unsigned)*start);
 }
 
-
 /**
- * Find last bit set in a word.  The least significant bit is 1.
- * Return 0 if no bits are set.
- * Essentially ffs() in the reverse direction.
+ * Find last bit set in a word. Least significant bit is 1.
+ * Return 0 if no bits are set. (Essentially ffs() in reverse.)
  */
 static inline unsigned
 util_last_bit(unsigned u)
 {
 #if defined(HAVE___BUILTIN_CLZ)
-   return u == 0 ? 0 : 32 - __builtin_clz(u);
+   return u == 0 ? 0u : 32u - (unsigned)__builtin_clz(u);
 #elif defined(_MSC_VER) && (_M_IX86 || _M_ARM || _M_AMD64 || _M_IA64)
    unsigned long index;
-   if (_BitScanReverse(&index, u))
-      return index + 1;
+   if (_BitScanReverse(&index, (unsigned long)u))
+      return (unsigned)index + 1u;
    else
-      return 0;
+      return 0u;
 #else
    unsigned r = 0;
    while (u) {
@@ -270,21 +263,20 @@ util_last_bit(unsigned u)
 }
 
 /**
- * Find last bit set in a word.  The least significant bit is 1.
- * Return 0 if no bits are set.
- * Essentially ffsll() in the reverse direction.
+ * Find last bit set in a 64-bit word. Least significant bit is 1.
+ * Return 0 if no bits are set. (Essentially ffsll() in reverse.)
  */
 static inline unsigned
 util_last_bit64(uint64_t u)
 {
 #if defined(HAVE___BUILTIN_CLZLL)
-   return u == 0 ? 0 : 64 - __builtin_clzll(u);
+   return u == 0 ? 0u : 64u - (unsigned)__builtin_clzll(u);
 #elif defined(_MSC_VER) && (_M_AMD64 || _M_ARM64 || _M_IA64)
    unsigned long index;
-   if (_BitScanReverse64(&index, u))
-      return index + 1;
+   if (_BitScanReverse64(&index, (unsigned long long)u))
+      return (unsigned)index + 1u;
    else
-      return 0;
+      return 0u;
 #else
    unsigned r = 0;
    while (u) {
@@ -296,29 +288,26 @@ util_last_bit64(uint64_t u)
 }
 
 /**
- * Find last bit in a word that does not match the sign bit. The least
- * significant bit is 1.
- * Return 0 if no bits are set.
+ * Find last bit in a word that does not match the sign bit.
+ * The least significant bit is 1. Return 0 if no bits are set.
  */
 static inline unsigned
 util_last_bit_signed(int i)
 {
    if (i >= 0)
-      return util_last_bit(i);
+      return util_last_bit((unsigned)i);
    else
       return util_last_bit(~(unsigned)i);
 }
 
-/* Returns a bitfield in which the first count bits starting at start are
- * set.
- */
+/* Returns a bitfield in which the first count bits starting at start are set. */
 static inline unsigned
 u_bit_consecutive(unsigned start, unsigned count)
 {
    assert(start + count <= 32);
    if (count == 32)
-      return ~0;
-   return ((1u << count) - 1) << start;
+      return ~0u;
+   return ((1u << count) - 1u) << start;
 }
 
 static inline uint64_t
@@ -327,28 +316,24 @@ u_bit_consecutive64(unsigned start, unsi
    assert(start + count <= 64);
    if (count == 64)
       return ~(uint64_t)0;
-   return (((uint64_t)1 << count) - 1) << start;
+   return (((uint64_t)1 << count) - 1ull) << start;
 }
 
 /**
- * Return number of bits set in n.
+ * Return number of bits set in n (32-bit).
  */
 static inline unsigned
 util_bitcount(unsigned n)
 {
 #if defined(HAVE___BUILTIN_POPCOUNT)
-   return __builtin_popcount(n);
+   return (unsigned)__builtin_popcount(n);
 #elif __OPENCL_VERSION__
-   return popcount(n);
+   return (unsigned)popcount(n);
 #else
-   /* K&R classic bitcount.
-    *
-    * For each iteration, clear the LSB from the bitfield.
-    * Requires only one iteration per set bit, instead of
-    * one iteration per bit less than highest set bit.
-    */
-   unsigned bits;
-   for (bits = 0; n; bits++) {
+   /* K&R bitcount: clear lowest set bit per iteration. */
+   unsigned bits = 0;
+   while (n) {
+      bits++;
       n &= n - 1;
    }
    return bits;
@@ -358,9 +343,7 @@ util_bitcount(unsigned n)
 /**
  * Return the number of bits set in n using the native popcnt instruction.
  * The caller is responsible for ensuring that popcnt is supported by the CPU.
- *
- * gcc doesn't use it if -mpopcnt or -march= that has popcnt is missing.
- *
+ * gcc won't auto-emit popcnt unless -mpopcnt or suitable -march is used.
  */
 static inline unsigned
 util_popcnt_inline_asm(unsigned n)
@@ -370,47 +353,61 @@ util_popcnt_inline_asm(unsigned n)
    __asm volatile("popcnt %1, %0" : "=r"(out) : "r"(n));
    return out;
 #else
-   /* We should never get here by accident, but I'm sure it'll happen. */
+   /* Fallback safely to software popcount. */
    return util_bitcount(n);
 #endif
 }
 
+/**
+ * Return number of bits set in n (64-bit).
+ */
 static inline unsigned
 util_bitcount64(uint64_t n)
 {
 #ifdef HAVE___BUILTIN_POPCOUNTLL
-   return __builtin_popcountll(n);
+   return (unsigned)__builtin_popcountll(n);
 #elif __OPENCL_VERSION__
-   return popcount(n);
+   return (unsigned)popcount(n);
 #else
+   /* Portable fallback via two 32-bit popcounts. */
    return util_bitcount((unsigned)n) + util_bitcount((unsigned)(n >> 32));
 #endif
 }
 
 /**
- * Widens the given bit mask by a multiplier, meaning that it will
- * replicate each bit by that amount.
- *
- * For example:
- * 0b101 widened by 2 will become: 0b110011
- *
- * This is typically used in shader I/O to transform a 64-bit
- * writemask to a 32-bit writemask.
+ * Widens the given bit mask by a multiplier, replicating each bit by that count.
+ * Example: 0b101 widened by 2 -> 0b110011
  */
 static inline uint32_t
 util_widen_mask(uint32_t mask, unsigned multiplier)
 {
+   if (!mask || multiplier == 0)
+      return 0u;
+
+#if (defined(__x86_64__) || defined(_M_X64) || defined(__i386__) || defined(_M_IX86)) && defined(__BMI2__)
+   /* Fast path for the most common case: multiplier == 2.
+    * Use BMI2 PDEP to interleave zeros between bits, then OR with a shift to replicate.
+    */
+   if (multiplier == 2) {
+      /* Spread source bits into odd positions. */
+      uint32_t spread = _pdep_u32(mask, 0x55555555u);
+      return spread | (spread << 1);
+   }
+#endif
+
+   /* Generic path for any multiplier. */
    uint32_t new_mask = 0;
-   u_foreach_bit(i, mask)
+   u_foreach_bit(i, mask) {
       new_mask |= ((1u << multiplier) - 1u) << (i * multiplier);
+   }
    return new_mask;
 }
 
 #ifdef __cplusplus
-}
+} /* extern "C" */
 
-/* util_bitcount has large measurable overhead (~2%), so it's recommended to
- * use the POPCNT instruction via inline assembly if the CPU supports it.
+/* util_bitcount has measurable overhead (~2%) compared to popcnt,
+ * so prefer inline assembly if the CPU supports it.
  */
 enum util_popcnt {
    POPCNT_NO,
@@ -419,8 +416,7 @@ enum util_popcnt {
 };
 
 /* Convenient function to select popcnt through a C++ template argument.
- * This should be used as part of larger functions that are optimized
- * as a whole.
+ * This should be used as part of larger functions optimized as a whole.
  */
 template<util_popcnt POPCNT> inline unsigned
 util_bitcount_fast(unsigned n)
