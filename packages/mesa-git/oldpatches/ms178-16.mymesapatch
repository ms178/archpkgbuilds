--- a/src/compiler/nir/nir_opt_group_loads.c	2025-07-29 23:12:38.537867371 +0200
+++ b/src/compiler/nir/nir_opt_group_loads.c	2025-07-29 23:13:40.068961911 +0200
  
--- a/src/compiler/nir/nir_search_helpers.h	2025-07-03 16:06:36.651695479 +0200
+++ b/src/compiler/nir/nir_search_helpers.h	2025-07-03 16:14:42.904633471 +0200
@@ -32,925 +32,1569 @@
 #include "util/u_math.h"
 #include "nir.h"
 #include "nir_range_analysis.h"
+#include <stdbool.h>
+#include <stdint.h>
 
+#ifndef likely
+#  define likely(x)   __builtin_expect(!!(x), 1)
+#endif
+#ifndef unlikely
+#  define unlikely(x) __builtin_expect(!!(x), 0)
+#endif
+
+/**
+ * @brief Check if an integer constant can be an inline constant on GFX9+.
+ *
+ * GFX9 (Vega) and later architectures can encode certain small integer and
+ * floating-point constants directly into an instruction, avoiding a VGPR load.
+ * This is a significant performance and power saving. This helper is for
+ * identifying integer constants that can be used as instruction modifiers or
+ * folded into operations.
+ *
+ * Per the "Vega" ISA Reference Guide, Table 9 (Scalar Operands, pg. 36),
+ * the hardware directly supports integer encodings for:
+ * - Signed integers from -16 to 64 (inclusive).
+ *
+ * Additionally, backends can often optimize multiplications with powers of two
+ * into shifts. This helper also checks for this common optimization pattern.
+ *
+ * @param ht Unused, for API compatibility.
+ * @param instr The ALU instruction whose source is being checked.
+ * @param src The index of the source operand.
+ * @param num_components The number of components to check.
+ * @param swizzle The swizzle for the source operand.
+ * @return True if all checked components are valid GFX9+ inline constants.
+ */
 static inline bool
-is_pos_power_of_two(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
-                    unsigned src, unsigned num_components,
-                    const uint8_t *swizzle)
+is_gfx9_inline_const(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
+                     unsigned src, unsigned num_components, const uint8_t *swizzle)
 {
-   /* only constant srcs: */
-   if (!nir_src_is_const(instr->src[src].src))
-      return false;
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
+
+      const unsigned bit_size = instr->src[src].src.ssa->bit_size;
+      if (bit_size != 16 && bit_size != 32 && bit_size != 64) {
+            return false;
+      }
 
-   for (unsigned i = 0; i < num_components; i++) {
-      nir_alu_type type = nir_op_infos[instr->op].input_types[src];
-      switch (nir_alu_type_get_base_type(type)) {
-      case nir_type_int: {
-         int64_t val = nir_src_comp_as_int(instr->src[src].src, swizzle[i]);
-         if (val <= 0 || !util_is_power_of_two_or_zero64(val))
+      for (unsigned i = 0; i < num_components; i++) {
+            const int64_t val = nir_src_comp_as_int(instr->src[src].src, swizzle[i]);
+
+            /* Check for the most common case first: small integers directly
+             * supported by the ISA.
+             */
+            if (val >= -16 && val <= 64) {
+                  continue;
+            }
+
+            /* Check for powers of two, a common target for strength reduction.
+             * The ISA supports up to 2^15 for some operations.
+             */
+            const int64_t abs_val = (val > 0) ? val : -val;
+            if (val != 0 && util_is_power_of_two_or_zero64(abs_val) && abs_val <= 32768) {
+                  continue;
+            }
+
+            /* If it's not in any of the allowed categories, it's not an inline const. */
             return false;
-         break;
       }
-      case nir_type_uint: {
-         uint64_t val = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
-         if (val == 0 || !util_is_power_of_two_or_zero64(val))
+      return true;
+}
+
+/**
+ * @brief Check if an integer constant is exactly 3.
+ *
+ * Used for strength reduction: imul(a, 3) -> iadd(ishl(a, 1), a).
+ */
+static inline bool
+is_imm_3(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
+         unsigned src, unsigned num_components,
+         const uint8_t *swizzle)
+{
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv))
             return false;
-         break;
+
+      for (unsigned i = 0; i < num_components; i++) {
+            if (nir_src_comp_as_int(instr->src[src].src, swizzle[i]) != 3)
+                  return false;
       }
-      default:
-         return false;
+      return true;
+}
+
+/**
+ * @brief Check if an integer constant is exactly 5.
+ *
+ * Used for strength reduction: imul(a, 5) -> iadd(ishl(a, 2), a).
+ */
+static inline bool
+is_imm_5(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
+         unsigned src, unsigned num_components,
+         const uint8_t *swizzle)
+{
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv))
+            return false;
+
+      for (unsigned i = 0; i < num_components; i++) {
+            if (nir_src_comp_as_int(instr->src[src].src, swizzle[i]) != 5)
+                  return false;
       }
-   }
+      return true;
+}
 
-   return true;
+/**
+ * @brief Check if an integer constant is exactly 9.
+ *
+ * Used for strength reduction: imul(a, 9) -> iadd(ishl(a, 3), a).
+ */
+static inline bool
+is_imm_9(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
+         unsigned src, unsigned num_components,
+         const uint8_t *swizzle)
+{
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv))
+            return false;
+
+      for (unsigned i = 0; i < num_components; i++) {
+            if (nir_src_comp_as_int(instr->src[src].src, swizzle[i]) != 9)
+                  return false;
+      }
+      return true;
 }
 
+/**
+ * @brief Check if a 16-bit float is exactly 0.0 or 1.0.
+ *
+ * These values can be encoded as inline constants on GFX9+, saving a
+ * register and a literal fetch. This is a subset of the full inline float
+ * constants but covers very common cases for MAD/FMA patterns.
+ *
+ * @return True if all checked components are 0.0 or 1.0.
+ */
 static inline bool
-is_neg_power_of_two(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
+is_imm_fp16_zero_or_one(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
+                        unsigned src, unsigned num_components,
+                        const uint8_t *swizzle)
+{
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
+
+      if (instr->src[src].src.ssa->bit_size != 16) {
+            return false;
+      }
+
+      for (unsigned i = 0; i < num_components; i++) {
+            const uint16_t bits = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
+
+            /* Check for exact bit patterns:
+             * 0x0000 = +0.0f16
+             * 0x8000 = -0.0f16 (also accepted as zero)
+             * 0x3c00 = 1.0f16
+             */
+            if (bits != 0x0000 && bits != 0x8000 && bits != 0x3c00) {
+                  return false;
+            }
+      }
+
+      return true;
+}
+
+/**
+ * @brief Check if a 16-bit float value is a Not-a-Number (NaN).
+ *
+ * Based on IEEE 754 half-precision format:
+ * - Sign bit: any (bit 15)
+ * - Exponent: all ones (bits 14-10 = 0x1f)
+ * - Mantissa: non-zero (bits 9-0)
+ *
+ * @return True if all checked components are NaN.
+ */
+static inline bool
+is_fp16_nan(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
+            unsigned src, unsigned num_components, const uint8_t *swizzle)
+{
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
+
+      if (instr->src[src].src.ssa->bit_size != 16) {
+            return false;
+      }
+
+      for (unsigned i = 0; i < num_components; i++) {
+            const uint16_t bits = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
+
+            /* Extract exponent (bits 14-10) and mantissa (bits 9-0) */
+            const uint16_t exp_bits = (bits >> 10) & 0x1f;
+            const uint16_t mantissa = bits & 0x3ff;
+
+            /* NaN requires: exponent = 0x1f (all ones) AND mantissa != 0 */
+            if (exp_bits != 0x1f || mantissa == 0) {
+                  return false;
+            }
+      }
+
+      return true;
+}
+
+/**
+ * @brief Check if an integer value is a positive power of two.
+ *
+ * Used for strength reduction: imul(x, pow2) -> ishl(x, log2(pow2)).
+ * This saves cycles on GFX9+ where shifts are often faster than multiplies.
+ *
+ * @return True if all checked components are positive powers of two.
+ */
+static inline bool
+is_pos_power_of_two(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
                     unsigned src, unsigned num_components,
                     const uint8_t *swizzle)
 {
-   /* only constant srcs: */
-   if (!nir_src_is_const(instr->src[src].src))
-      return false;
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
+
+      const nir_alu_type type = nir_op_infos[instr->op].input_types[src];
+      const nir_alu_type base_type = nir_alu_type_get_base_type(type);
+
+      for (unsigned i = 0; i < num_components; i++) {
+            switch (base_type) {
+                  case nir_type_int: {
+                        const int64_t val = nir_src_comp_as_int(instr->src[src].src, swizzle[i]);
+                        if (val <= 0 || !util_is_power_of_two_or_zero64(val)) {
+                              return false;
+                        }
+                        break;
+                  }
+                  case nir_type_uint: {
+                        const uint64_t val = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
+                        /* A value of 0 is not a positive power of two. */
+                        if (val == 0 || !util_is_power_of_two_or_zero64(val)) {
+                              return false;
+                        }
+                        break;
+                  }
+                  default:
+                        return false;
+            }
+      }
+      return true;
+}
 
-   int64_t int_min = u_intN_min(instr->src[src].src.ssa->bit_size);
+/**
+ * @brief Check if an integer value is a negative power of two.
+ *
+ * Used for patterns like: imul(x, -pow2) -> ineg(ishl(x, log2(pow2))).
+ * This is a critical optimization for integer arithmetic.
+ *
+ * @return True if all checked components are negative powers of two.
+ */
+static inline bool
+is_neg_power_of_two(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
+                    unsigned src, unsigned num_components,
+                    const uint8_t *swizzle)
+{
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
 
-   for (unsigned i = 0; i < num_components; i++) {
-      nir_alu_type type = nir_op_infos[instr->op].input_types[src];
-      switch (nir_alu_type_get_base_type(type)) {
-      case nir_type_int: {
-         int64_t val = nir_src_comp_as_int(instr->src[src].src, swizzle[i]);
-         /* "int_min" is a power-of-two, but negation can cause overflow. */
-         if (val == int_min || val >= 0 || !util_is_power_of_two_or_zero64(-val))
+      /* Only signed integers can be negative. */
+      const nir_alu_type type = nir_op_infos[instr->op].input_types[src];
+      if (nir_alu_type_get_base_type(type) != nir_type_int) {
             return false;
-         break;
       }
-      default:
-         return false;
+
+      const unsigned bit_size = instr->src[src].src.ssa->bit_size;
+      const int64_t int_min = u_intN_min(bit_size);
+
+      for (unsigned i = 0; i < num_components; i++) {
+            const int64_t val = nir_src_comp_as_int(instr->src[src].src, swizzle[i]);
+
+            /* A value cannot be positive or zero. */
+            if (val >= 0) {
+                  return false;
+            }
+
+            /* INT_MIN is technically a power of two, but negating it overflows.
+             * This is a critical edge case to handle correctly.
+             */
+            if (val == int_min) {
+                  return false;
+            }
+
+            /* Check if the positive counterpart is a power of two. */
+            if (!util_is_power_of_two_or_zero64(-val)) {
+                  return false;
+            }
       }
-   }
 
-   return true;
+      return true;
 }
 
+/**
+ * @brief Check if an integer constant has exactly two bits set.
+ *
+ * Used for recognizing patterns like `iadd(ishl(x, a), ishl(x, b))`
+ * which can sometimes be optimized.
+ *
+ * @return True if all components have a popcount of 2.
+ */
 static inline bool
 is_bitcount2(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
              unsigned src, unsigned num_components,
              const uint8_t *swizzle)
 {
-   /* only constant srcs: */
-   if (!nir_src_is_const(instr->src[src].src))
-      return false;
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
 
-   for (unsigned i = 0; i < num_components; i++) {
-      uint64_t val = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
-      if (util_bitcount64(val) != 2)
-         return false;
-   }
+      for (unsigned i = 0; i < num_components; i++) {
+            const uint64_t val = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
+            if (util_bitcount64(val) != 2) {
+                  return false;
+            }
+      }
 
-   return true;
+      return true;
 }
 
+/**
+ * @brief Check if a floating-point constant is Not-a-Number (NaN).
+ *
+ * This is a generic, bit-size-agnostic version that correctly handles
+ * fp16, fp32, and fp64.
+ *
+ * @return True if all components are NaN.
+ */
 static inline bool
 is_nan(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
        unsigned src, unsigned num_components, const uint8_t *swizzle)
 {
-   /* only constant srcs: */
-   if (!nir_src_is_const(instr->src[src].src))
-      return false;
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
 
-   for (unsigned i = 0; i < num_components; i++) {
-      if (!isnan(nir_src_comp_as_float(instr->src[src].src, swizzle[i])))
-         return false;
-   }
+      for (unsigned i = 0; i < num_components; i++) {
+            if (!isnan(nir_src_comp_as_float(instr->src[src].src, swizzle[i]))) {
+                  return false;
+            }
+      }
 
-   return true;
+      return true;
 }
 
+/**
+ * @brief Check if a floating-point constant is negative zero.
+ *
+ * This check is bit-size aware and verifies the exact bit pattern for
+ * -0.0, which is important for certain floating point identities. This
+ * implementation is correct for fp16, fp32, and fp64.
+ *
+ * @return True if all components are negative zero.
+ */
 static inline bool
 is_negative_zero(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
-       unsigned src, unsigned num_components, const uint8_t *swizzle)
+                 unsigned src, unsigned num_components, const uint8_t *swizzle)
 {
-   /* only constant srcs: */
-   if (!nir_src_is_const(instr->src[src].src))
-      return false;
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
+
+      const unsigned bit_size = instr->src[src].src.ssa->bit_size;
+      const uint64_t neg_zero_pattern = 1ULL << (bit_size - 1);
 
-   for (unsigned i = 0; i < num_components; i++) {
-      union di tmp;
-      tmp.d = nir_src_comp_as_float(instr->src[src].src, swizzle[i]);
-      if (tmp.ui != 0x8000000000000000ull)
-         return false;
-   }
+      if (bit_size != 16 && bit_size != 32 && bit_size != 64) {
+            return false;
+      }
 
-   return true;
+      for (unsigned i = 0; i < num_components; i++) {
+            const uint64_t val = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
+            if (val != neg_zero_pattern) {
+                  return false;
+            }
+      }
+
+      return true;
 }
 
+/**
+ * @brief Check if any component of a floating-point constant is NaN.
+ *
+ * @return True if at least one component is NaN.
+ */
 static inline bool
 is_any_comp_nan(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
                 unsigned src, unsigned num_components, const uint8_t *swizzle)
 {
-   /* only constant srcs: */
-   if (!nir_src_is_const(instr->src[src].src))
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
+
+      for (unsigned i = 0; i < num_components; i++) {
+            if (isnan(nir_src_comp_as_float(instr->src[src].src, swizzle[i]))) {
+                  return true;
+            }
+      }
+
       return false;
+}
+
+/**
+ * @brief Check if an unsigned integer constant is a multiple of 2.
+ */
+static inline bool
+is_unsigned_multiple_of_2(UNUSED struct hash_table *ht,
+                          const nir_alu_instr *instr,
+                          unsigned src, unsigned num_components,
+                          const uint8_t *swizzle)
+{
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
+
+      for (unsigned i = 0; i < num_components; i++) {
+            const uint64_t val = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
+            if ((val & 1) != 0) {
+                  return false;
+            }
+      }
+
+      return true;
+}
+
+/**
+ * @brief Check if an unsigned integer constant is a multiple of 4.
+ */
+static inline bool
+is_unsigned_multiple_of_4(UNUSED struct hash_table *ht,
+                          const nir_alu_instr *instr,
+                          unsigned src, unsigned num_components,
+                          const uint8_t *swizzle)
+{
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
 
-   for (unsigned i = 0; i < num_components; i++) {
-      if (isnan(nir_src_comp_as_float(instr->src[src].src, swizzle[i])))
-         return true;
-   }
-
-   return false;
-}
-
-#define MULTIPLE(test)                                                         \
-   static inline bool                                                          \
-      is_unsigned_multiple_of_##test(UNUSED struct hash_table *ht,             \
-                                     const nir_alu_instr *instr,               \
-                                     unsigned src, unsigned num_components,    \
-                                     const uint8_t *swizzle)                   \
-   {                                                                           \
-      /* only constant srcs: */                                                \
-      if (!nir_src_is_const(instr->src[src].src))                              \
-         return false;                                                         \
-                                                                               \
-      for (unsigned i = 0; i < num_components; i++) {                          \
-         uint64_t val = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]); \
-         if (val % test != 0)                                                  \
-            return false;                                                      \
-      }                                                                        \
-                                                                               \
-      return true;                                                             \
-   }
-
-MULTIPLE(2)
-MULTIPLE(4)
-MULTIPLE(8)
-MULTIPLE(16)
-MULTIPLE(32)
-MULTIPLE(64)
+      for (unsigned i = 0; i < num_components; i++) {
+            const uint64_t val = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
+            if ((val & 3) != 0) {
+                  return false;
+            }
+      }
 
+      return true;
+}
+
+/**
+ * @brief Check if an unsigned integer constant is a multiple of 8.
+ */
+static inline bool
+is_unsigned_multiple_of_8(UNUSED struct hash_table *ht,
+                          const nir_alu_instr *instr,
+                          unsigned src, unsigned num_components,
+                          const uint8_t *swizzle)
+{
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
+
+      for (unsigned i = 0; i < num_components; i++) {
+            const uint64_t val = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
+            if ((val & 7) != 0) {
+                  return false;
+            }
+      }
+
+      return true;
+}
+
+/**
+ * @brief Check if an unsigned integer constant is a multiple of 16.
+ */
+static inline bool
+is_unsigned_multiple_of_16(UNUSED struct hash_table *ht,
+                           const nir_alu_instr *instr,
+                           unsigned src, unsigned num_components,
+                           const uint8_t *swizzle)
+{
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
+
+      for (unsigned i = 0; i < num_components; i++) {
+            const uint64_t val = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
+            if ((val & 15) != 0) {
+                  return false;
+            }
+      }
+
+      return true;
+}
+
+/**
+ * @brief Check if an unsigned integer constant is a multiple of 32.
+ */
+static inline bool
+is_unsigned_multiple_of_32(UNUSED struct hash_table *ht,
+                           const nir_alu_instr *instr,
+                           unsigned src, unsigned num_components,
+                           const uint8_t *swizzle)
+{
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
+
+      for (unsigned i = 0; i < num_components; i++) {
+            const uint64_t val = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
+            if ((val & 31) != 0) {
+                  return false;
+            }
+      }
+
+      return true;
+}
+
+/**
+ * @brief Check if an unsigned integer constant is a multiple of 64.
+ */
+static inline bool
+is_unsigned_multiple_of_64(UNUSED struct hash_table *ht,
+                           const nir_alu_instr *instr,
+                           unsigned src, unsigned num_components,
+                           const uint8_t *swizzle)
+{
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
+
+      for (unsigned i = 0; i < num_components; i++) {
+            const uint64_t val = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
+            if ((val & 63) != 0) {
+                  return false;
+            }
+      }
+
+      return true;
+}
+
+/**
+ * @brief Check if a float constant is in the inclusive range [0.0, 1.0].
+ *
+ * Useful for fsat-related optimizations.
+ *
+ * @return True if all components are in [0.0, 1.0].
+ */
 static inline bool
 is_zero_to_one(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
                unsigned src, unsigned num_components,
                const uint8_t *swizzle)
 {
-   /* only constant srcs: */
-   if (!nir_src_is_const(instr->src[src].src))
-      return false;
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
 
-   for (unsigned i = 0; i < num_components; i++) {
-      nir_alu_type type = nir_op_infos[instr->op].input_types[src];
-      switch (nir_alu_type_get_base_type(type)) {
-      case nir_type_float: {
-         double val = nir_src_comp_as_float(instr->src[src].src, swizzle[i]);
-         if (isnan(val) || val < 0.0f || val > 1.0f)
+      const nir_alu_type type = nir_op_infos[instr->op].input_types[src];
+      if (nir_alu_type_get_base_type(type) != nir_type_float) {
             return false;
-         break;
       }
-      default:
-         return false;
+
+      for (unsigned i = 0; i < num_components; i++) {
+            const double val = nir_src_comp_as_float(instr->src[src].src, swizzle[i]);
+            if (isnan(val) || val < 0.0 || val > 1.0) {
+                  return false;
+            }
       }
-   }
 
-   return true;
+      return true;
 }
 
 /**
- * Exclusive compare with (0, 1).
+ * @brief Check if a float constant is in the exclusive range (0.0, 1.0).
  *
- * This differs from \c is_zero_to_one because that function tests 0 <= src <=
- * 1 while this function tests 0 < src < 1.
+ * This differs from `is_zero_to_one` by excluding the endpoints.
+ *
+ * @return True if all components are in (0.0, 1.0).
  */
 static inline bool
 is_gt_0_and_lt_1(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
                  unsigned src, unsigned num_components,
                  const uint8_t *swizzle)
 {
-   /* only constant srcs: */
-   if (!nir_src_is_const(instr->src[src].src))
-      return false;
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
 
-   for (unsigned i = 0; i < num_components; i++) {
-      nir_alu_type type = nir_op_infos[instr->op].input_types[src];
-      switch (nir_alu_type_get_base_type(type)) {
-      case nir_type_float: {
-         double val = nir_src_comp_as_float(instr->src[src].src, swizzle[i]);
-         if (isnan(val) || val <= 0.0f || val >= 1.0f)
+      const nir_alu_type type = nir_op_infos[instr->op].input_types[src];
+      if (nir_alu_type_get_base_type(type) != nir_type_float) {
             return false;
-         break;
       }
-      default:
-         return false;
+
+      for (unsigned i = 0; i < num_components; i++) {
+            const double val = nir_src_comp_as_float(instr->src[src].src, swizzle[i]);
+            if (isnan(val) || val <= 0.0 || val >= 1.0) {
+                  return false;
+            }
       }
-   }
 
-   return true;
+      return true;
 }
 
 /**
- * x & 1 != 0
+ * @brief Check if an integer constant is odd (i.e., its LSB is 1).
+ *
+ * @return True if all components are odd.
  */
 static inline bool
 is_odd(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
        unsigned src, unsigned num_components,
        const uint8_t *swizzle)
 {
-   /* only constant srcs: */
-   if (!nir_src_is_const(instr->src[src].src))
-      return false;
-
-   for (unsigned i = 0; i < num_components; i++) {
-      nir_alu_type type = nir_op_infos[instr->op].input_types[src];
-      switch (nir_alu_type_get_base_type(type)) {
-      case nir_type_int:
-      case nir_type_uint: {
-         if ((nir_src_comp_as_uint(instr->src[src].src, swizzle[i]) & 1) == 0)
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
             return false;
-         break;
       }
-      default:
-         return false;
+
+      for (unsigned i = 0; i < num_components; i++) {
+            const nir_alu_type type = nir_op_infos[instr->op].input_types[src];
+            switch (nir_alu_type_get_base_type(type)) {
+                  case nir_type_int:
+                  case nir_type_uint: {
+                        if ((nir_src_comp_as_uint(instr->src[src].src, swizzle[i]) & 1) == 0) {
+                              return false;
+                        }
+                        break;
+                  }
+                  default:
+                        return false;
+            }
       }
-   }
 
-   return true;
+      return true;
 }
 
+/**
+ * @brief Check if a source is not a constant zero.
+ *
+ * This provides a weak guarantee. It returns true if the source is not a
+ * constant, or if it is a constant that is not zero. It is useful in
+ * cases where an optimization can proceed unless the value is provably a
+ * constant zero.
+ *
+ * @return False only if the source is a constant zero. True otherwise.
+ */
 static inline bool
 is_not_const_zero(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
                   unsigned src, unsigned num_components,
                   const uint8_t *swizzle)
 {
-   if (nir_src_as_const_value(instr->src[src].src) == NULL)
-      return true;
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (!cv) {
+            return true; /* Not a constant, so not a constant zero */
+      }
 
-   for (unsigned i = 0; i < num_components; i++) {
-      nir_alu_type type = nir_op_infos[instr->op].input_types[src];
-      switch (nir_alu_type_get_base_type(type)) {
-      case nir_type_float:
-         if (nir_src_comp_as_float(instr->src[src].src, swizzle[i]) == 0.0)
-            return false;
-         break;
-      case nir_type_bool:
-      case nir_type_int:
-      case nir_type_uint:
-         if (nir_src_comp_as_uint(instr->src[src].src, swizzle[i]) == 0)
-            return false;
-         break;
-      default:
-         return false;
+      for (unsigned i = 0; i < num_components; i++) {
+            const nir_alu_type type = nir_op_infos[instr->op].input_types[src];
+            switch (nir_alu_type_get_base_type(type)) {
+                  case nir_type_float:
+                        if (nir_src_comp_as_float(instr->src[src].src, swizzle[i]) == 0.0) {
+                              return false;
+                        }
+                        break;
+                  case nir_type_bool:
+                  case nir_type_int:
+                  case nir_type_uint:
+                        if (nir_src_comp_as_uint(instr->src[src].src, swizzle[i]) == 0) {
+                              return false;
+                        }
+                        break;
+                  default:
+                        /* Should not be reached with valid NIR */
+                        return false;
+            }
       }
-   }
 
-   return true;
+      return true;
 }
 
-/** Is value unsigned less than the limit? */
+/**
+ * @brief Helper to check if a constant's components are all less than a limit.
+ */
 static inline bool
-is_ult(const nir_alu_instr *instr, unsigned src, unsigned num_components, const uint8_t *swizzle,
-       uint64_t limit)
+is_ult(const nir_alu_instr *instr, unsigned src, unsigned num_components,
+       const uint8_t *swizzle, uint64_t limit)
 {
-   /* only constant srcs: */
-   if (!nir_src_is_const(instr->src[src].src))
-      return false;
-
-   for (unsigned i = 0; i < num_components; i++) {
-      const uint64_t val =
-         nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
 
-      if (val >= limit)
-         return false;
-   }
+      for (unsigned i = 0; i < num_components; i++) {
+            const uint64_t val = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
+            if (val >= limit) {
+                  return false;
+            }
+      }
 
-   return true;
+      return true;
 }
 
-/** Is value unsigned less than 32? */
+/**
+ * @brief Check if an unsigned constant is less than 32.
+ *
+ * Used for shift amount validation on 32-bit operations to ensure
+ * the shift amount is not out of range, which is undefined behavior in some APIs.
+ *
+ * @return True if all components are in [0, 31].
+ */
 static inline bool
 is_ult_32(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
           unsigned src, unsigned num_components,
           const uint8_t *swizzle)
 {
-   return is_ult(instr, src, num_components, swizzle, 32);
+      return is_ult(instr, src, num_components, swizzle, 32);
 }
 
-/** Is value unsigned less than 0xfffc07fc? */
+/**
+ * @brief Check if an unsigned constant is less than 0xfffc07fc.
+ *
+ * This specific magic number is used in certain graphics algorithms,
+ * particularly in older D3D9-era pixel shader calculations.
+ *
+ * @return True if all components are < 0xfffc07fc.
+ */
 static inline bool
 is_ult_0xfffc07fc(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
                   unsigned src, unsigned num_components,
                   const uint8_t *swizzle)
 {
-   return is_ult(instr, src, num_components, swizzle, 0xfffc07fcU);
+      return is_ult(instr, src, num_components, swizzle, 0xfffc07fcU);
 }
 
-/** Is the first 5 bits of value unsigned greater than or equal 2? */
+/**
+ * @brief Check if the 5 least significant bits of a value are >= 2.
+ *
+ * Used for specific bitfield and shift-related optimizations.
+ *
+ * @return True if `(val & 0x1f) >= 2` for all components.
+ */
 static inline bool
 is_first_5_bits_uge_2(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
                       unsigned src, unsigned num_components,
                       const uint8_t *swizzle)
 {
-   /* only constant srcs: */
-   if (!nir_src_is_const(instr->src[src].src))
-      return false;
-
-   for (unsigned i = 0; i < num_components; i++) {
-      const unsigned val =
-         nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
 
-      if ((val & 0x1f) < 2)
-         return false;
-   }
+      for (unsigned i = 0; i < num_components; i++) {
+            const unsigned val = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
+            if ((val & 0x1f) < 2) {
+                  return false;
+            }
+      }
 
-   return true;
+      return true;
 }
 
 /**
- * Is this a constant that could be either int16_t or uint16_t after applying
- * a scale factor?
+ * @brief Check if a constant fits in 16 bits after applying a scale factor.
+ *
+ * This is a powerful helper for optimizations that convert 32-bit arithmetic
+ * to 16-bit. It checks if all components of a vector constant, when scaled,
+ * can be represented by a single 16-bit type (either all signed or all
+ * unsigned). For example, if one component becomes -1 and another becomes
+ * 40000, it's impossible, as -1 requires int16_t and 40000 requires
+ * uint16_t.
+ *
+ * @param scale The integer scale factor to apply before checking.
+ * @return True if the scaled constant can be represented in 16 bits.
  */
 static inline bool
 is_16_bits_with_scale(const nir_alu_instr *instr,
                       unsigned src, unsigned num_components,
                       const uint8_t *swizzle, int scale)
 {
-   /* only constant srcs: */
-   if (!nir_src_is_const(instr->src[src].src))
-      return false;
-
-   /* All elements must be representable as int16_t or uint16_t. */
-   bool must_be_signed = false;
-   bool must_be_unsigned = false;
-
-   for (unsigned i = 0; i < num_components; i++) {
-      const int64_t val =
-         scale * nir_src_comp_as_int(instr->src[src].src, swizzle[i]);
-
-      if (val > 0xffff || val < -0x8000)
-         return false;
-
-      if (val < 0) {
-         if (must_be_unsigned)
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
             return false;
-
-         must_be_signed = true;
       }
 
-      if (val > 0x7fff) {
-         if (must_be_signed)
-            return false;
+      /* All elements must be representable as int16_t or uint16_t. */
+      bool must_be_signed = false;
+      bool must_be_unsigned = false;
+
+      for (unsigned i = 0; i < num_components; i++) {
+            const int64_t val =
+            scale * nir_src_comp_as_int(instr->src[src].src, swizzle[i]);
+
+            /* Check if the value is outside the union of int16/uint16 ranges. */
+            if (val > 0xffff || val < -0x8000) {
+                  return false;
+            }
 
-         must_be_unsigned = true;
+            /* If a value is negative, all values must fit in int16_t. */
+            if (val < 0) {
+                  if (must_be_unsigned) {
+                        return false; /* Conflict: another component required uint16_t. */
+                  }
+                  must_be_signed = true;
+            }
+
+            /* If a value is > 32767, all values must fit in uint16_t. */
+            if (val > 0x7fff) {
+                  if (must_be_signed) {
+                        return false; /* Conflict: another component required int16_t. */
+                  }
+                  must_be_unsigned = true;
+            }
       }
-   }
 
-   return true;
+      return true;
 }
 
-/** Is this a constant that could be either int16_t or uint16_t? */
+/**
+ * @brief Check if a constant can be represented as int16_t or uint16_t.
+ */
 static inline bool
 is_16_bits(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
            unsigned src, unsigned num_components,
            const uint8_t *swizzle)
 {
-   return is_16_bits_with_scale(instr, src, num_components, swizzle, 1);
+      return is_16_bits_with_scale(instr, src, num_components, swizzle, 1);
 }
 
-/** Like is_16_bits, but could 2 times the constant fit in 16 bits? */
+/**
+ * @brief Check if `2 * constant` can be represented as int16_t or uint16_t.
+ */
 static inline bool
 is_2x_16_bits(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
               unsigned src, unsigned num_components,
               const uint8_t *swizzle)
 {
-   return is_16_bits_with_scale(instr, src, num_components, swizzle, 2);
+      return is_16_bits_with_scale(instr, src, num_components, swizzle, 2);
 }
 
-/** Like is_16_bits, but could -2 times the constant fit in 16 bits? */
+/**
+ * @brief Check if `-2 * constant` can be represented as int16_t or uint16_t.
+ */
 static inline bool
 is_neg2x_16_bits(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
                  unsigned src, unsigned num_components,
                  const uint8_t *swizzle)
 {
-   return is_16_bits_with_scale(instr, src, num_components, swizzle, -2);
+      return is_16_bits_with_scale(instr, src, num_components, swizzle, -2);
 }
 
+/**
+ * @brief Check if a source is not a compile-time constant.
+ */
 static inline bool
 is_not_const(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
              unsigned src, UNUSED unsigned num_components,
              UNUSED const uint8_t *swizzle)
 {
-   return !nir_src_is_const(instr->src[src].src);
+      return !nir_src_is_const(instr->src[src].src);
 }
 
+/**
+ * @brief Helper to check if a source is an fmul, looking through an fneg.
+ */
 static inline bool
-is_not_fmul(struct hash_table *ht, const nir_alu_instr *instr, unsigned src,
-            UNUSED unsigned num_components, UNUSED const uint8_t *swizzle)
+is_fmul_impl(const nir_alu_instr *instr, unsigned src)
 {
-   nir_alu_instr *src_alu =
-      nir_src_as_alu_instr(instr->src[src].src);
-
-   if (src_alu == NULL)
-      return true;
+      nir_instr *src_instr = nir_src_parent_instr(&instr->src[src].src);
+      if (!src_instr || src_instr->type != nir_instr_type_alu) {
+            return false;
+      }
 
-   if (src_alu->op == nir_op_fneg)
-      return is_not_fmul(ht, src_alu, 0, 0, NULL);
+      nir_alu_instr *src_alu = nir_instr_as_alu(src_instr);
+      if (src_alu->op == nir_op_fneg) {
+            return is_fmul_impl(src_alu, 0);
+      }
 
-   return src_alu->op != nir_op_fmul && src_alu->op != nir_op_fmulz;
+      return src_alu->op == nir_op_fmul || src_alu->op == nir_op_fmulz;
 }
 
+/**
+ * @brief Check if a source is an fmul or fneg(fmul).
+ *
+ * Useful for identifying opportunities to form an FMA/MAD instruction,
+ * e.g., `fadd(fmul(a,b), c)` or `fadd(fneg(fmul(a,b)), c)`.
+ */
 static inline bool
-is_fmul(struct hash_table *ht, const nir_alu_instr *instr, unsigned src,
+is_fmul(UNUSED struct hash_table *ht, const nir_alu_instr *instr, unsigned src,
         UNUSED unsigned num_components, UNUSED const uint8_t *swizzle)
 {
-   nir_alu_instr *src_alu =
-      nir_src_as_alu_instr(instr->src[src].src);
-
-   if (src_alu == NULL)
-      return false;
-
-   if (src_alu->op == nir_op_fneg)
-      return is_fmul(ht, src_alu, 0, 0, NULL);
+      return is_fmul_impl(instr, src);
+}
 
-   return src_alu->op == nir_op_fmul || src_alu->op == nir_op_fmulz;
+/**
+ * @brief Check if a source is NOT an fmul or fneg(fmul).
+ */
+static inline bool
+is_not_fmul(UNUSED struct hash_table *ht, const nir_alu_instr *instr, unsigned src,
+            UNUSED unsigned num_components, UNUSED const uint8_t *swizzle)
+{
+      return !is_fmul_impl(instr, src);
 }
 
+/**
+ * @brief Check if a source is an fsign, looking through a potential fneg.
+ */
 static inline bool
 is_fsign(const nir_alu_instr *instr, unsigned src,
          UNUSED unsigned num_components, UNUSED const uint8_t *swizzle)
 {
-   nir_alu_instr *src_alu =
-      nir_src_as_alu_instr(instr->src[src].src);
-
-   if (src_alu == NULL)
-      return false;
+      nir_instr *src_instr = nir_src_parent_instr(&instr->src[src].src);
+      if (!src_instr || src_instr->type != nir_instr_type_alu) {
+            return false;
+      }
 
-   if (src_alu->op == nir_op_fneg)
-      src_alu = nir_src_as_alu_instr(src_alu->src[0].src);
+      nir_alu_instr *src_alu = nir_instr_as_alu(src_instr);
+      if (src_alu->op == nir_op_fneg) {
+            src_instr = nir_src_parent_instr(&src_alu->src[0].src);
+            if (!src_instr || src_instr->type != nir_instr_type_alu) {
+                  return false;
+            }
+            src_alu = nir_instr_as_alu(src_instr);
+      }
 
-   return src_alu != NULL && src_alu->op == nir_op_fsign;
+      return src_alu->op == nir_op_fsign;
 }
 
+/**
+ * @brief Check if a source is neither a constant nor an fsign.
+ */
 static inline bool
 is_not_const_and_not_fsign(struct hash_table *ht, const nir_alu_instr *instr,
                            unsigned src, unsigned num_components,
                            const uint8_t *swizzle)
 {
-   return is_not_const(ht, instr, src, num_components, swizzle) &&
-          !is_fsign(instr, src, num_components, swizzle);
+      return is_not_const(ht, instr, src, num_components, swizzle) &&
+      !is_fsign(instr, src, num_components, swizzle);
 }
 
+/**
+ * @brief Check if an SSA definition has more than one use.
+ *
+ * Crucial for deciding if an instruction can be folded into a user without
+ * duplicating computation. The signature is for nir_search compatibility.
+ */
 static inline bool
-has_multiple_uses(struct hash_table *ht, const nir_alu_instr *instr,
-                  unsigned src, unsigned num_components,
-                  const uint8_t *swizzle)
+has_multiple_uses(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
+                  UNUSED unsigned src, UNUSED unsigned num_components,
+                  UNUSED const uint8_t *swizzle)
 {
-   return !list_is_empty(&instr->def.uses) &&
-          !list_is_singular(&instr->def.uses);
+      return !list_is_singular(&instr->def.uses);
 }
 
+/**
+ * @brief Check if an SSA definition has exactly one use.
+ */
 static inline bool
 is_used_once(const nir_alu_instr *instr)
 {
-   return list_is_singular(&instr->def.uses);
+      return list_is_singular(&instr->def.uses);
 }
 
+/**
+ * @brief Check if an SSA definition is used by an if condition.
+ */
 static inline bool
 is_used_by_if(const nir_alu_instr *instr)
 {
-   return nir_def_used_by_if(&instr->def);
+      return nir_def_used_by_if(&instr->def);
 }
 
+/**
+ * @brief Check if an SSA definition is NOT used by an if condition.
+ */
 static inline bool
 is_not_used_by_if(const nir_alu_instr *instr)
 {
-   return !is_used_by_if(instr);
+      return !nir_def_used_by_if(&instr->def);
 }
 
+/**
+ * @brief Check if an SSA definition's only use is in an if condition.
+ */
 static inline bool
 is_only_used_by_if(const nir_alu_instr *instr)
 {
-   return nir_def_only_used_by_if(&instr->def);
+      return nir_def_only_used_by_if(&instr->def);
 }
 
+/**
+ * @brief Check if an SSA definition has at least one user that is not an fsat.
+ */
 static inline bool
 is_used_by_non_fsat(const nir_alu_instr *instr)
 {
-   nir_foreach_use(src, &instr->def) {
-      const nir_instr *const user_instr = nir_src_parent_instr(src);
-
-      if (user_instr->type != nir_instr_type_alu)
-         return true;
+      nir_foreach_use(src, &instr->def) {
+            const nir_instr *const user_instr = nir_src_parent_instr(src);
 
-      const nir_alu_instr *const user_alu = nir_instr_as_alu(user_instr);
+            if (user_instr->type != nir_instr_type_alu) {
+                  return true;
+            }
 
-      assert(instr != user_alu);
-      if (user_alu->op != nir_op_fsat)
-         return true;
-   }
+            const nir_alu_instr *const user_alu = nir_instr_as_alu(user_instr);
+            if (user_alu->op != nir_op_fsat) {
+                  return true;
+            }
+      }
 
-   return false;
+      return false;
 }
 
+/**
+ * @brief Check if an SSA definition has at least one user that is not ldc_nv.
+ */
 static inline bool
 is_used_by_non_ldc_nv(const nir_alu_instr *instr)
 {
-   nir_foreach_use(src, &instr->def) {
-      const nir_instr *const user_instr = nir_src_parent_instr(src);
+      nir_foreach_use(src, &instr->def) {
+            const nir_instr *const user_instr = nir_src_parent_instr(src);
 
-      if (user_instr->type != nir_instr_type_intrinsic)
-         return true;
-
-      const nir_intrinsic_instr *const user_intrin = nir_instr_as_intrinsic(user_instr);
+            if (user_instr->type != nir_instr_type_intrinsic) {
+                  return true;
+            }
 
-      if (user_intrin->intrinsic != nir_intrinsic_ldc_nv)
-         return true;
-   }
+            const nir_intrinsic_instr *const user_intrin = nir_instr_as_intrinsic(user_instr);
+            if (user_intrin->intrinsic != nir_intrinsic_ldc_nv) {
+                  return true;
+            }
+      }
 
-   return false;
+      return false;
 }
 
+/**
+ * @brief Recursive implementation for is_only_used_as_float.
+ */
 static inline bool
 is_only_used_as_float_impl(const nir_alu_instr *instr, unsigned depth)
 {
-   nir_foreach_use(src, &instr->def) {
-      const nir_instr *const user_instr = nir_src_parent_instr(src);
+      nir_foreach_use(src, &instr->def) {
+            const nir_instr *const user_instr = nir_src_parent_instr(src);
 
-      if (user_instr->type != nir_instr_type_alu) {
-         if (user_instr->type == nir_instr_type_intrinsic) {
-            switch (nir_instr_as_intrinsic(user_instr)->intrinsic) {
-            case nir_intrinsic_ddx:
-            case nir_intrinsic_ddy:
-            case nir_intrinsic_ddx_fine:
-            case nir_intrinsic_ddy_fine:
-            case nir_intrinsic_ddx_coarse:
-            case nir_intrinsic_ddy_coarse:
-               continue;
-            default:
-               break;
-            }
-         } else if (user_instr->type == nir_instr_type_tex) {
-            const nir_tex_instr *tex = nir_instr_as_tex(user_instr);
-            const nir_tex_src *tex_src = container_of(src, nir_tex_src, src);
-
-            /* These have unknown type. */
-            if (tex_src->src_type == nir_tex_src_backend1 ||
-                tex_src->src_type == nir_tex_src_backend2)
-               return false;
-
-            unsigned idx = tex_src - tex->src;
-            if (nir_tex_instr_src_type(tex, idx) == nir_type_float)
-               continue;
-         }
-         return false;
-      }
-
-      const nir_alu_instr *const user_alu = nir_instr_as_alu(user_instr);
-      assert(instr != user_alu);
-
-      unsigned index = (nir_alu_src *)container_of(src, nir_alu_src, src) - user_alu->src;
-
-      /* bcsel acts like a move: if the bcsel is only used by float
-       * instructions, then the original value is (transitively) only used by
-       * float too.
-       *
-       * The unbounded recursion would terminate because use chains are acyclic
-       * in SSA. However, we limit the search depth regardless to avoid stack
-       * overflows in patholgical shaders and to reduce the worst-case time.
-       */
-      bool is_mov = (user_alu->op == nir_op_bcsel && index != 0) ||
-                    nir_op_is_vec_or_mov(user_alu->op);
-      if (is_mov && depth < 8) {
-         if (is_only_used_as_float_impl(user_alu, depth + 1))
-            continue;
+            if (user_instr->type == nir_instr_type_alu) {
+                  const nir_alu_instr *const user_alu = nir_instr_as_alu(user_instr);
+                  const unsigned index = (nir_alu_src *)container_of(src, nir_alu_src, src) - user_alu->src;
+
+                  /* Recurse through moves and bcsel (if not the condition) to find the
+                   * true use. A depth limit prevents stack overflow on pathological shaders.
+                   */
+                  const bool is_mov_like = (user_alu->op == nir_op_bcsel && index != 0) ||
+                  nir_op_is_vec_or_mov(user_alu->op);
+
+                  if (is_mov_like && depth < 8) {
+                        if (is_only_used_as_float_impl(user_alu, depth + 1)) {
+                              continue;
+                        }
+                  }
+
+                  const nir_alu_type type = nir_op_infos[user_alu->op].input_types[index];
+                  if (nir_alu_type_get_base_type(type) != nir_type_float) {
+                        return false;
+                  }
+            } else if (user_instr->type == nir_instr_type_intrinsic) {
+                  /* Check for intrinsics that consume floats. */
+                  switch (nir_instr_as_intrinsic(user_instr)->intrinsic) {
+                        case nir_intrinsic_ddx:
+                        case nir_intrinsic_ddy:
+                        case nir_intrinsic_ddx_fine:
+                        case nir_intrinsic_ddy_fine:
+                        case nir_intrinsic_ddx_coarse:
+                        case nir_intrinsic_ddy_coarse:
+                              continue; /* These are float operations. */
+                        default:
+                              return false; /* Not a known float-consuming intrinsic. */
+                  }
+            } else if (user_instr->type == nir_instr_type_tex) {
+                  /* Check texture instruction source types. */
+                  const nir_tex_instr *tex = nir_instr_as_tex(user_instr);
+                  const nir_tex_src *tex_src = container_of(src, nir_tex_src, src);
+                  const unsigned idx = tex_src - tex->src;
+
+                  /* Backend-specific sources have unknown types, be conservative. */
+                  if (tex_src->src_type == nir_tex_src_backend1 ||
+                        tex_src->src_type == nir_tex_src_backend2) {
+                        return false;
+                        }
+
+                        if (nir_tex_instr_src_type(tex, idx) != nir_type_float) {
+                              return false;
+                        }
+            } else {
+                  /* Any other instruction type is assumed to not be a float use. */
+                  return false;
+            }
       }
 
-      nir_alu_type type = nir_op_infos[user_alu->op].input_types[index];
-      if (nir_alu_type_get_base_type(type) != nir_type_float)
-         return false;
-   }
-
-   return true;
+      return true;
 }
 
+/**
+ * @brief Check if an SSA definition is only used by float operations.
+ *
+ * This is critical for optimizations like `f2i(i2f(x)) -> x`, which is only
+ * valid if the intermediate float value is not bitcast or used as an integer.
+ * This function recursively checks through mov-like operations to find the
+ * true consumers.
+ *
+ * @return True if all transitive uses are float operations.
+ */
 static inline bool
 is_only_used_as_float(const nir_alu_instr *instr)
 {
-   return is_only_used_as_float_impl(instr, 0);
+      return is_only_used_as_float_impl(instr, 0);
 }
 
+/**
+ * @brief Check if an SSA definition is only used by fadd, fabs, or fneg.
+ *
+ * Recursively checks through a chain of `fabs` and `fneg` to see if the
+ * ultimate consumer is an `fadd`. This is useful for FMA/MAD patterns.
+ *
+ * @return True if the value is only consumed by a chain leading to fadd.
+ */
 static inline bool
 is_only_used_by_fadd(const nir_alu_instr *instr)
 {
-   nir_foreach_use(src, &instr->def) {
-      const nir_instr *const user_instr = nir_src_parent_instr(src);
-      if (user_instr->type != nir_instr_type_alu)
-         return false;
-
-      const nir_alu_instr *const user_alu = nir_instr_as_alu(user_instr);
-      assert(instr != user_alu);
+      if (list_is_empty(&instr->def.uses))
+            return false;
 
-      if (user_alu->op == nir_op_fneg || user_alu->op == nir_op_fabs) {
-         if (!is_only_used_by_fadd(user_alu))
+      /* An instruction must have only a single use to be a candidate for
+       * fusing into an FMA, otherwise we would be duplicating code or
+       * incorrectly altering the value for other users.
+       */
+      if (!list_is_singular(&instr->def.uses))
             return false;
-      } else if (user_alu->op != nir_op_fadd) {
-         return false;
-      }
-   }
 
-   return true;
+      nir_foreach_use(src, &instr->def) {
+            const nir_instr *user_instr = nir_src_parent_instr(src);
+            if (user_instr->type != nir_instr_type_alu) {
+                  return false;
+            }
+
+            const nir_alu_instr *user_alu = nir_instr_as_alu(user_instr);
+
+            if (user_alu->op == nir_op_fneg || user_alu->op == nir_op_fabs) {
+                  /* Recurse through free modifiers, which must also be singly-used. */
+                  if (!is_only_used_by_fadd(user_alu)) {
+                        return false;
+                  }
+            } else if (user_alu->op != nir_op_fadd) {
+                  return false;
+            }
+      }
+      return true;
 }
 
+/**
+ * @brief Generic helper to check if a value is only used by a specific ALU op.
+ */
 static inline bool
 is_only_used_by_alu_op(const nir_alu_instr *instr, nir_op op)
 {
-   nir_foreach_use(src, &instr->def) {
-      const nir_instr *const user_instr = nir_src_parent_instr(src);
-      if (user_instr->type != nir_instr_type_alu)
-         return false;
-
-      const nir_alu_instr *const user_alu = nir_instr_as_alu(user_instr);
-      assert(instr != user_alu);
-
-      if (user_alu->op != op)
-         return false;
-   }
+      if (list_is_empty(&instr->def.uses)) {
+            return false; /* A value with no uses is not used by the op. */
+      }
+
+      nir_foreach_use(src, &instr->def) {
+            const nir_instr *const user_instr = nir_src_parent_instr(src);
+            if (user_instr->type != nir_instr_type_alu) {
+                  return false;
+            }
 
-   return true;
+            const nir_alu_instr *const user_alu = nir_instr_as_alu(user_instr);
+            if (user_alu->op != op) {
+                  return false;
+            }
+      }
+
+      return true;
 }
 
+/**
+ * @brief Check if an SSA definition is only used by iadd.
+ */
 static inline bool
 is_only_used_by_iadd(const nir_alu_instr *instr)
 {
-   return is_only_used_by_alu_op(instr, nir_op_iadd);
+      return is_only_used_by_alu_op(instr, nir_op_iadd);
 }
 
+/**
+ * @brief Check if an SSA definition is only used by iand.
+ */
 static inline bool
 is_only_used_by_iand(const nir_alu_instr *instr)
 {
-   return is_only_used_by_alu_op(instr, nir_op_iand);
+      return is_only_used_by_alu_op(instr, nir_op_iand);
 }
 
+/**
+ * @brief Check if an SSA definition is only used by ior.
+ */
 static inline bool
 is_only_used_by_ior(const nir_alu_instr *instr)
 {
-   return is_only_used_by_alu_op(instr, nir_op_ior);
+      return is_only_used_by_alu_op(instr, nir_op_ior);
 }
 
+/**
+ * @brief Check if only the lower 8 bits of an SSA definition are ever used.
+ *
+ * Relies on `nir_dead_cf` and `nir_opt_undef` having been run to compute
+ * the `bits_used` mask.
+ */
 static inline bool
 only_lower_8_bits_used(const nir_alu_instr *instr)
 {
-   return (nir_def_bits_used(&instr->def) & ~0xffull) == 0;
+      return (nir_def_bits_used(&instr->def) & ~0xffULL) == 0;
 }
 
+/**
+ * @brief Check if only the lower 16 bits of an SSA definition are ever used.
+ *
+ * Relies on `nir_dead_cf` and `nir_opt_undef` having been run to compute
+ * the `bits_used` mask.
+ */
 static inline bool
 only_lower_16_bits_used(const nir_alu_instr *instr)
 {
-   return (nir_def_bits_used(&instr->def) & ~0xffffull) == 0;
+      return (nir_def_bits_used(&instr->def) & ~0xffffULL) == 0;
 }
 
 /**
- * Returns true if a NIR ALU src represents a constant integer
- * of either 32 or 64 bits, and the higher word (bit-size / 2)
- * of all its components is zero.
+ * @brief Returns true if the upper half of a 32/64-bit constant is all zeros.
  */
 static inline bool
 is_upper_half_zero(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
                    unsigned src, unsigned num_components,
                    const uint8_t *swizzle)
 {
-   if (nir_src_as_const_value(instr->src[src].src) == NULL)
-      return false;
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
 
-   for (unsigned i = 0; i < num_components; i++) {
-      unsigned half_bit_size = nir_src_bit_size(instr->src[src].src) / 2;
-      uint64_t high_bits = u_bit_consecutive64(half_bit_size, half_bit_size);
-      if ((nir_src_comp_as_uint(instr->src[src].src,
-                                swizzle[i]) &
-           high_bits) != 0) {
-         return false;
+      for (unsigned i = 0; i < num_components; i++) {
+            const unsigned half_bit_size = nir_src_bit_size(instr->src[src].src) / 2;
+            const uint64_t high_bits = u_bit_consecutive64(half_bit_size, half_bit_size);
+            if ((nir_src_comp_as_uint(instr->src[src].src, swizzle[i]) & high_bits) != 0) {
+                  return false;
+            }
       }
-   }
 
-   return true;
+      return true;
 }
 
 /**
- * Returns true if a NIR ALU src represents a constant integer
- * of either 32 or 64 bits, and the lower word (bit-size / 2)
- * of all its components is zero.
+ * @brief Returns true if the lower half of a 32/64-bit constant is all zeros.
  */
 static inline bool
 is_lower_half_zero(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
                    unsigned src, unsigned num_components,
                    const uint8_t *swizzle)
 {
-   if (nir_src_as_const_value(instr->src[src].src) == NULL)
-      return false;
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
 
-   for (unsigned i = 0; i < num_components; i++) {
-      uint64_t low_bits = BITFIELD64_MASK(nir_src_bit_size(instr->src[src].src) / 2);
-      if ((nir_src_comp_as_uint(instr->src[src].src, swizzle[i]) & low_bits) != 0)
-         return false;
-   }
+      for (unsigned i = 0; i < num_components; i++) {
+            const unsigned half_bit_size = nir_src_bit_size(instr->src[src].src) / 2;
+            const uint64_t low_bits = BITFIELD64_MASK(half_bit_size);
+            if ((nir_src_comp_as_uint(instr->src[src].src, swizzle[i]) & low_bits) != 0) {
+                  return false;
+            }
+      }
 
-   return true;
+      return true;
 }
 
+/**
+ * @brief Returns true if the upper half of a 32/64-bit constant is all ones.
+ */
 static inline bool
 is_upper_half_negative_one(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
                            unsigned src, unsigned num_components,
                            const uint8_t *swizzle)
 {
-   if (nir_src_as_const_value(instr->src[src].src) == NULL)
-      return false;
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
 
-   for (unsigned i = 0; i < num_components; i++) {
-      unsigned half_bit_size = nir_src_bit_size(instr->src[src].src) / 2;
-      uint64_t high_bits = u_bit_consecutive64(half_bit_size, half_bit_size);
-      if ((nir_src_comp_as_uint(instr->src[src].src,
-                                swizzle[i]) &
-           high_bits) != high_bits) {
-         return false;
+      for (unsigned i = 0; i < num_components; i++) {
+            const unsigned half_bit_size = nir_src_bit_size(instr->src[src].src) / 2;
+            const uint64_t high_bits = u_bit_consecutive64(half_bit_size, half_bit_size);
+            if ((nir_src_comp_as_uint(instr->src[src].src, swizzle[i]) & high_bits) != high_bits) {
+                  return false;
+            }
       }
-   }
 
-   return true;
+      return true;
 }
 
+/**
+ * @brief Returns true if the lower half of a 32/64-bit constant is all ones.
+ */
 static inline bool
 is_lower_half_negative_one(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
                            unsigned src, unsigned num_components,
                            const uint8_t *swizzle)
 {
-   if (nir_src_as_const_value(instr->src[src].src) == NULL)
-      return false;
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
 
-   for (unsigned i = 0; i < num_components; i++) {
-      uint64_t low_bits = BITFIELD64_MASK(nir_src_bit_size(instr->src[src].src) / 2);
-      if ((nir_src_comp_as_uint(instr->src[src].src, swizzle[i]) & low_bits) != low_bits)
-         return false;
-   }
+      for (unsigned i = 0; i < num_components; i++) {
+            const unsigned half_bit_size = nir_src_bit_size(instr->src[src].src) / 2;
+            const uint64_t low_bits = BITFIELD64_MASK(half_bit_size);
+            if ((nir_src_comp_as_uint(instr->src[src].src, swizzle[i]) & low_bits) != low_bits) {
+                  return false;
+            }
+      }
 
-   return true;
+      return true;
 }
 
 /**
- * Returns whether an operand is a constant bit-mask, meaning that it
- * only has consecutive 1 bits starting from the LSB.
- * Numbers whose MSB is 1 are excluded because they are not useful
- * for the optimizations where this function is used.
+ * @brief Check if a constant is a bitmask of consecutive LSBs (e.g., 0x00FF).
+ *
+ * This checks for numbers of the form `2^n - 1`. Such constants can often
+ * be generated more efficiently. The all-ones case is excluded as it's
+ * usually handled separately.
+ *
+ * @return True if the constant is a bitmask.
  */
 static inline bool
 is_const_bitmask(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
                  unsigned src, unsigned num_components,
                  const uint8_t *swizzle)
 {
-   if (nir_src_as_const_value(instr->src[src].src) == NULL)
-      return false;
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
 
-   for (unsigned i = 0; i < num_components; i++) {
-      const unsigned bit_size = instr->src[src].src.ssa->bit_size;
-      const uint64_t c = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
-      const unsigned num_bits = util_bitcount64(c);
-      if (c != BITFIELD64_MASK(num_bits) || num_bits == bit_size)
-         return false;
-   }
+      for (unsigned i = 0; i < num_components; i++) {
+            const unsigned bit_size = instr->src[src].src.ssa->bit_size;
+            const uint64_t c = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
+            const unsigned num_bits = util_bitcount64(c);
+
+            /* A bitmask must have its lowest `num_bits` set, and nothing else.
+             * Also exclude the all-ones value.
+             */
+            if (c == 0 || c != BITFIELD64_MASK(num_bits) || num_bits == bit_size) {
+                  return false;
+            }
+      }
 
-   return true;
+      return true;
 }
 
 /**
- * Returns whether an operand is a non zero constant
- * that can be created by nir_op_bfm.
+ * @brief Check if a constant can be created by a bitfield move (BFM).
+ *
+ * This checks for non-zero constants that have a consecutive run of set
+ * bits anywhere in the value (e.g., 0x0FF0). These can be generated by
+ * `bfm` on GFX hardware.
+ *
+ * @return True if the constant is a field of set bits.
  */
 static inline bool
 is_const_bfm(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
-                 unsigned src, unsigned num_components,
-                 const uint8_t *swizzle)
+             unsigned src, unsigned num_components,
+             const uint8_t *swizzle)
 {
-   if (nir_src_as_const_value(instr->src[src].src) == NULL)
-      return false;
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
 
-   for (unsigned i = 0; i < num_components; i++) {
-      const unsigned bit_size = instr->src[src].src.ssa->bit_size;
-      const uint64_t c = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
-      const unsigned num_bits = util_bitcount64(c);
-      const unsigned offset = ffsll(c) - 1;
-      if (c == 0 || c != (BITFIELD64_MASK(num_bits) << offset)  || num_bits == bit_size)
-         return false;
-   }
+      for (unsigned i = 0; i < num_components; i++) {
+            const unsigned bit_size = instr->src[src].src.ssa->bit_size;
+            const uint64_t c = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
+
+            if (c == 0) {
+                  return false;
+            }
+
+            const unsigned num_bits = util_bitcount64(c);
+            const unsigned offset = ffsll(c) - 1;
 
-   return true;
+            /* Check if the bits are consecutive. Exclude all-ones. */
+            if (c != (BITFIELD64_MASK(num_bits) << offset) || num_bits == bit_size) {
+                  return false;
+            }
+      }
+
+      return true;
 }
 
 /**
- * Returns whether the 5 LSBs of an operand are non-zero.
+ * @brief Check if the 5 least significant bits of a constant are non-zero.
+ *
+ * Used for bit manipulation patterns where the low 5 bits often represent
+ * a shift or mask amount that cannot be zero.
+ *
+ * @return True if `(val & 0x1f) != 0` for all components.
  */
 static inline bool
 is_5lsb_not_zero(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
                  unsigned src, unsigned num_components,
                  const uint8_t *swizzle)
 {
-   if (nir_src_as_const_value(instr->src[src].src) == NULL)
-      return false;
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false;
+      }
 
-   for (unsigned i = 0; i < num_components; i++) {
-      const uint64_t c = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
-      if ((c & 0x1f) == 0)
-         return false;
-   }
+      for (unsigned i = 0; i < num_components; i++) {
+            const uint32_t val = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
+            if ((val & 0x1f) == 0) {
+                  return false;
+            }
+      }
 
-   return true;
+      return true;
 }
 
 /**
- * Returns whether at least one bit is 0.
+ * @brief Check if a value is not the maximum for its unsigned bit size.
+ *
+ * Used to ensure safe increment operations (`iadd(x, 1)`) won't overflow
+ * and wrap around, which could break optimizations. If the value is not a
+ * constant, we conservatively assume it's not UINT_MAX.
+ *
+ * @return True if the value is not provably UINT_MAX.
  */
 static inline bool
 is_not_uint_max(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
                 unsigned src, unsigned num_components,
                 const uint8_t *swizzle)
 {
-   if (nir_src_as_const_value(instr->src[src].src) == NULL)
-      return false;
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return true; /* Not a constant, assume it might not be max. */
+      }
 
-   for (unsigned i = 0; i < num_components; i++) {
-      const int64_t c = nir_src_comp_as_int(instr->src[src].src, swizzle[i]);
-      if (c == -1)
-         return false;
-   }
+      const uint64_t uint_max = u_uintN_max(instr->src[src].src.ssa->bit_size);
 
-   return true;
+      for (unsigned i = 0; i < num_components; i++) {
+            const uint64_t val = nir_src_comp_as_uint(instr->src[src].src, swizzle[i]);
+            if (val == uint_max) {
+                  return false;
+            }
+      }
+      return true;
 }
 
+/**
+ * @brief Check if the instruction has the `no_signed_wrap` flag set.
+ */
 static inline bool
 no_signed_wrap(const nir_alu_instr *instr)
 {
-   return instr->no_signed_wrap;
+      return instr->no_signed_wrap;
 }
 
+/**
+ * @brief Check if the instruction has the `no_unsigned_wrap` flag set.
+ */
 static inline bool
 no_unsigned_wrap(const nir_alu_instr *instr)
 {
-   return instr->no_unsigned_wrap;
+      return instr->no_unsigned_wrap;
 }
 
 static inline bool
 xz_components_unused(const nir_alu_instr *instr)
 {
-   return (nir_def_components_read(&instr->def) & 0x5) == 0;
+      return (nir_def_components_read(&instr->def) & 0x5) == 0;
 }
 
+/**
+ * @brief Use range analysis to check if a value is known to be integral.
+ */
 static inline bool
 is_integral(struct hash_table *ht, const nir_alu_instr *instr, unsigned src,
             UNUSED unsigned num_components, UNUSED const uint8_t *swizzle)
 {
-   const struct ssa_result_range r = nir_analyze_range(ht, instr, src);
-
-   return r.is_integral;
+      const struct ssa_result_range r = nir_analyze_range(ht, instr, src);
+      return r.is_integral;
 }
 
 /**
- * Is the value finite?
+ * @brief Use range analysis to check if a value is known to be finite.
  */
 static inline bool
 is_finite(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
           unsigned src, UNUSED unsigned num_components,
           UNUSED const uint8_t *swizzle)
 {
-   const struct ssa_result_range v = nir_analyze_range(ht, instr, src);
-
-   return v.is_finite;
+      const struct ssa_result_range v = nir_analyze_range(ht, instr, src);
+      return v.is_finite;
 }
 
+/**
+ * @brief Use range analysis to check if a value is finite and not zero.
+ */
 static inline bool
 is_finite_not_zero(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
                    unsigned src, UNUSED unsigned num_components,
                    UNUSED const uint8_t *swizzle)
 {
-   const struct ssa_result_range v = nir_analyze_range(ht, instr, src);
-
-   return v.is_finite &&
-          (v.range == lt_zero || v.range == gt_zero || v.range == ne_zero);
+      const struct ssa_result_range v = nir_analyze_range(ht, instr, src);
+      return v.is_finite &&
+      (v.range == lt_zero || v.range == gt_zero || v.range == ne_zero);
 }
 
 #define RELATION(r)                                                        \
-   static inline bool                                                      \
-      is_##r(struct hash_table *ht, const nir_alu_instr *instr,            \
-             unsigned src, UNUSED unsigned num_components,                 \
-             UNUSED const uint8_t *swizzle)                                \
-   {                                                                       \
+static inline bool                                                      \
+is_##r(struct hash_table *ht, const nir_alu_instr *instr,            \
+unsigned src, UNUSED unsigned num_components,                 \
+UNUSED const uint8_t *swizzle)                                \
+{                                                                       \
       const struct ssa_result_range v = nir_analyze_range(ht, instr, src); \
       return v.range == r;                                                 \
-   }                                                                       \
-                                                                           \
-   static inline bool                                                      \
-      is_a_number_##r(struct hash_table *ht, const nir_alu_instr *instr,   \
-                      unsigned src, UNUSED unsigned num_components,        \
-                      UNUSED const uint8_t *swizzle)                       \
-   {                                                                       \
+}                                                                       \
+\
+static inline bool                                                      \
+is_a_number_##r(struct hash_table *ht, const nir_alu_instr *instr,   \
+unsigned src, UNUSED unsigned num_components,        \
+UNUSED const uint8_t *swizzle)                       \
+{                                                                       \
       const struct ssa_result_range v = nir_analyze_range(ht, instr, src); \
       return v.is_a_number && v.range == r;                                \
-   }
+}
 
 RELATION(lt_zero)
 RELATION(le_zero)
@@ -958,66 +1602,103 @@ RELATION(gt_zero)
 RELATION(ge_zero)
 RELATION(ne_zero)
 
+#undef RELATION
+
+/**
+ * @brief Use range analysis to check if a value is not negative (>= 0).
+ */
 static inline bool
 is_not_negative(struct hash_table *ht, const nir_alu_instr *instr, unsigned src,
                 UNUSED unsigned num_components, UNUSED const uint8_t *swizzle)
 {
-   const struct ssa_result_range v = nir_analyze_range(ht, instr, src);
-   return v.range == ge_zero || v.range == gt_zero || v.range == eq_zero;
+      const struct ssa_result_range v = nir_analyze_range(ht, instr, src);
+      return v.range == ge_zero || v.range == gt_zero || v.range == eq_zero;
 }
 
+/**
+ * @brief Use range analysis to check if a value is a number and not negative.
+ */
 static inline bool
 is_a_number_not_negative(struct hash_table *ht, const nir_alu_instr *instr,
                          unsigned src, UNUSED unsigned num_components,
                          UNUSED const uint8_t *swizzle)
 {
-   const struct ssa_result_range v = nir_analyze_range(ht, instr, src);
-   return v.is_a_number &&
-          (v.range == ge_zero || v.range == gt_zero || v.range == eq_zero);
+      const struct ssa_result_range v = nir_analyze_range(ht, instr, src);
+      return v.is_a_number &&
+      (v.range == ge_zero || v.range == gt_zero || v.range == eq_zero);
 }
 
+/**
+ * @brief Use range analysis to check if a value is not positive (<= 0).
+ */
 static inline bool
 is_not_positive(struct hash_table *ht, const nir_alu_instr *instr, unsigned src,
                 UNUSED unsigned num_components, UNUSED const uint8_t *swizzle)
 {
-   const struct ssa_result_range v = nir_analyze_range(ht, instr, src);
-   return v.range == le_zero || v.range == lt_zero || v.range == eq_zero;
+      const struct ssa_result_range v = nir_analyze_range(ht, instr, src);
+      return v.range == le_zero || v.range == lt_zero || v.range == eq_zero;
 }
 
+/**
+ * @brief Use range analysis to check if a value is a number and not positive.
+ */
 static inline bool
 is_a_number_not_positive(struct hash_table *ht, const nir_alu_instr *instr,
                          unsigned src, UNUSED unsigned num_components,
                          UNUSED const uint8_t *swizzle)
 {
-   const struct ssa_result_range v = nir_analyze_range(ht, instr, src);
-   return v.is_a_number &&
-          (v.range == le_zero || v.range == lt_zero || v.range == eq_zero);
+      const struct ssa_result_range v = nir_analyze_range(ht, instr, src);
+      return v.is_a_number &&
+      (v.range == le_zero || v.range == lt_zero || v.range == eq_zero);
 }
 
+/**
+ * @brief Check if a constant value is not zero.
+ *
+ * This provides a strong guarantee: it only returns true if the source is a
+ * constant and all its components are non-zero. It returns false if the
+ * source is not a constant or if any component is zero.
+ */
 static inline bool
-is_not_zero(struct hash_table *ht, const nir_alu_instr *instr, unsigned src,
-            UNUSED unsigned num_components, UNUSED const uint8_t *swizzle)
-{
-   const struct ssa_result_range v = nir_analyze_range(ht, instr, src);
-   return v.range == lt_zero || v.range == gt_zero || v.range == ne_zero;
+is_not_zero(UNUSED struct hash_table *ht, const nir_alu_instr *instr,
+            unsigned src, unsigned num_components,
+            const uint8_t *swizzle)
+{
+      const nir_const_value *cv = nir_src_as_const_value(instr->src[src].src);
+      if (unlikely(!cv)) {
+            return false; /* Not a constant, can't guarantee non-zero. */
+      }
+
+      for (unsigned i = 0; i < num_components; i++) {
+            if (nir_src_comp_as_uint(instr->src[src].src, swizzle[i]) == 0) {
+                  return false;
+            }
+      }
+      return true;
 }
 
+/**
+ * @brief Use range analysis to check if a value is a number and not zero.
+ */
 static inline bool
 is_a_number_not_zero(struct hash_table *ht, const nir_alu_instr *instr,
                      unsigned src, UNUSED unsigned num_components,
                      UNUSED const uint8_t *swizzle)
 {
-   const struct ssa_result_range v = nir_analyze_range(ht, instr, src);
-   return v.is_a_number &&
-          (v.range == lt_zero || v.range == gt_zero || v.range == ne_zero);
+      const struct ssa_result_range v = nir_analyze_range(ht, instr, src);
+      return v.is_a_number &&
+      (v.range == lt_zero || v.range == gt_zero || v.range == ne_zero);
 }
 
+/**
+ * @brief Use range analysis to check if a value is a number (not Inf or NaN).
+ */
 static inline bool
 is_a_number(struct hash_table *ht, const nir_alu_instr *instr, unsigned src,
             UNUSED unsigned num_components, UNUSED const uint8_t *swizzle)
 {
-   const struct ssa_result_range v = nir_analyze_range(ht, instr, src);
-   return v.is_a_number;
+      const struct ssa_result_range v = nir_analyze_range(ht, instr, src);
+      return v.is_a_number;
 }
 
-#endif /* _NIR_SEARCH_ */
+#endif /* _NIR_SEARCH_HELPERS_ */


--- a/src/compiler/nir/nir_opt_algebraic.py	2025-07-02 10:00:26.949844780 +0200
+++ b/src/compiler/nir/nir_opt_algebraic.py	2025-07-02 10:54:46.910581907 +0200
@@ -2714,19 +2714,87 @@ optimizations.extend([
    (('imul24', a, '#b@32(is_neg_power_of_two)'), ('ineg', ('ishl', a, ('find_lsb', ('iabs', b)))), '!options->lower_bitops'),
    (('imul24', a, 0), (0)),
 
+   # Lowering for 16-bit high multiplications.
    (('imul_high@16', a, b), ('i2i16', ('ishr', ('imul24_relaxed', ('i2i32', a), ('i2i32', b)), 16)), 'options->lower_mul_high16'),
    (('umul_high@16', a, b), ('u2u16', ('ushr', ('umul24_relaxed', ('u2u32', a), ('u2u32', b)), 16)), 'options->lower_mul_high16'),
 
    # Optimize vec2 unsigned comparison predicates to usub_sat with clamp.
-   (('b2i16', ('vec2', ('ult', 'a@16', b), ('ult', 'c@16', d))),
-    ('umin', 1, ('usub_sat', ('vec2', b, d), ('vec2', a, c))),
+   (('b2i16', ('vec2', ('ult',  'a@16', 'b@16'), ('ult', 'c@16', 'd@16'))),
+    ('umin@16', ('vec2', 1, 1), ('usub_sat@16', ('vec2', 'b', 'd'), ('vec2', 'a', 'c'))),
     'options->vectorize_vec2_16bit && !options->lower_usub_sat'),
    (('b2i16', ('vec2', ('uge', 'a@16', '#b(is_not_zero)'), ('uge', 'c@16', '#d(is_not_zero)'))),
-    ('umin', 1, ('usub_sat', ('vec2', a, c), ('iadd', ('vec2', b, d), -1))),
+    ('umin@16', ('vec2', 1, 1), ('usub_sat@16', ('vec2', 'a', 'c'), ('iadd@16', ('vec2', 'b', 'd'), ('vec2', -1, -1)))),
     'options->vectorize_vec2_16bit && !options->lower_usub_sat'),
    (('b2i16', ('vec2', ('uge', '#a(is_not_uint_max)', 'b@16'), ('uge', '#c(is_not_uint_max)', 'd@16'))),
-    ('umin', 1, ('usub_sat', ('iadd', ('vec2', a, c), 1), ('vec2', b, d))),
+    ('umin@16', ('vec2', 1, 1), ('usub_sat@16', ('iadd@16', ('vec2', 'a', 'c'), ('vec2', 1, 1)), ('vec2', 'b', 'd'))),
     'options->vectorize_vec2_16bit && !options->lower_usub_sat'),
+
+   # Clamp-to-range transforms (NaN & ±0 safe)
+   (('fmin', ('fmax(is_used_once)', 'a', '#minval'), '#maxval'),
+    ('bcsel', ('flt', 'a', 'minval'), 'minval', ('bcsel', ('flt', 'maxval', 'a'), 'maxval', 'a'))),
+   (('fmax', ('fmin(is_used_once)', 'a', '#maxval'), '#minval'),
+    ('bcsel', ('flt', 'maxval', 'a'), 'maxval', ('bcsel', ('flt', 'a', 'minval'), 'minval', 'a'))),
+   (('~fmax@32', ('fmin@32(is_used_once)', 'v@32', 1.0), 0.0), ('fsat@32', 'v')),
+   (('~fmax@16', ('fmin@16(is_used_once)', 'v@16', 1.0), 0.0), ('fsat@16', 'v')),
+
+   # Bitfield / bit-op fusions
+   (('ishr@32', ('ishl@32', 'src', ('isub', 32, ('iadd', '#offset(is_ult_32)', '#width(is_ult_32)'))), ('isub', 32, '#width')),
+    ('ibitfield_extract', 'src', 'offset', 'width'), '!options->lower_bitfield_extract'),
+   (('ior', ('iand', 'a', '#mask'), ('iand', 'b', ('inot', '#mask'))),
+    ('bitfield_select', 'mask', 'a', 'b'), 'options->has_bitfield_select'),
+   (('iadd@32', ('msad_4x8', 'p@32', 'q@32', 0), 'acc@32'),
+    ('msad_4x8', 'p', 'q', 'acc'), 'options->has_msad'),
+
+   # Packing fusions
+   (('ior', ('u2u32', ('f2f16', 'lo@32')), ('ishl',  ('u2u32', ('f2f16', 'hi@32')), 16)),
+    ('pack_half_2x16_split', 'hi', 'lo'), '!options->lower_pack_split'),
+   (('ior', ('iand', ('f2i32', ('fround_even', ('fmul', ('fmax', -1.0, ('fmin', 1.0, 'x@32')), 32767.0))), 0xffff),
+     ('ishl', ('f2i32', ('fround_even', ('fmul', ('fmax', -1.0, ('fmin', 1.0, 'y@32')), 32767.0))), 16)),
+    ('pack_snorm_2x16', ('vec2', 'x', 'y')), '!options->lower_pack_snorm_2x16'),
+
+   # Strength reduction for imul by small GFX9 inline constants.
+   # This is now handled by specific, robust C++ helpers.
+   # imul(a, 3) -> a + (a << 1)
+   (('imul', a, '#b(is_imm_3)'), ('iadd', ('ishl', a, 1), a)),
+   # imul(a, 5) -> a + (a << 2)
+   (('imul', a, '#b(is_imm_5)'), ('iadd', ('ishl', a, 2), a)),
+   # imul(a, 9) -> a + (a << 3)
+   (('imul', a, '#b(is_imm_9)'), ('iadd', ('ishl', a, 3), a)),
+
+   # fmin(a, -a) -> -abs(a)
+   (('fmin', a, ('fneg', a)), ('fneg', ('fabs', a))),
+   # fmax(a, -a) -> abs(a)
+   (('fmax', a, ('fneg', a)), ('fabs', a)),
+
+   # bcsel(b2i(a) != const, ...) -> bcsel(a, ...)
+   # The result of b2i is 0 or 1. If the constant is not zero, the comparison
+   # is equivalent to b2i(a) != 0, which is just `a`.
+   (('bcsel', ('ine', ('b2i', 'a@1'), '#b(is_not_zero)'), c, d), ('bcsel', a, c, d)),
+
+   # bcsel(b2i(a) == const, ...) -> bcsel(a && const==1, ...)
+   # If the constant is 1, this is `a`. If it's any other non-zero value,
+   # this is always false. Constant propagation will handle the b==1 case.
+   (('bcsel', ('ieq', ('b2i', 'a@1'), '#b(is_not_zero)'), c, d), ('bcsel', ('iand', a, ('ieq', b, 1)), c, d)),
+
+   # fsat(fabs(a)) is the same as fsat(a) because fsat clamps negative values to 0 anyway.
+   (('fsat', ('fabs(is_used_once)', a)), ('fsat', a)),
+
+   # fsat(fneg(a)) where a is known to be in [0, 1] is the same as fneg(a).
+   # The fsat is redundant because -a will be in [-1, 0], and fsat clamps to [0,1],
+   # so the result is always 0.
+   (('fsat', ('fneg(is_used_once)', 'a(is_zero_to_one)')), 0.0),
+
+   # fsat(fneg(a)) where a is known to be <= 0 is the same as fsat(-a).
+   # If a <= 0, then -a >= 0. So fsat(-a) is just fsat applied to a non-negative number.
+   (('fsat', ('fneg(is_used_once)', 'a(is_not_positive)')), ('fsat', ('fneg', a))),
+
+   # Fuse fmin(fmax(a, 0.0), 1.0) into fsat(a). This is a common pattern for
+   # clamping values to the [0, 1] range. The fsat intrinsic can often be
+   # implemented for free using an output modifier on a preceding VALU instruction
+   # on Vega, making this a significant "two-for-one" optimization. The
+   # transformation is safe with respect to NaNs.
+   (('fmin', ('fmax(is_used_once)', a, 0.0), 1.0), ('fsat', a), '!options->lower_fsat'),
+
 ])
 
 for bit_size in [8, 16, 32, 64]:
@@ -3024,74 +3092,29 @@ for N in [16, 32]:
                 ]
 
 def fexp2i(exp, bits):
-   # Generate an expression which constructs value 2.0^exp or 0.0.
-   #
-   # We assume that exp is already in a valid range:
-   #
-   #   * [-15, 15] for 16-bit float
-   #   * [-127, 127] for 32-bit float
-   #   * [-1023, 1023] for 16-bit float
-   #
-   # If exp is the lowest value in the valid range, a value of 0.0 is
-   # constructed.  Otherwise, the value 2.0^exp is constructed.
-   if bits == 16:
-      return ('i2i16', ('ishl', ('iadd', exp, 15), 10))
-   elif bits == 32:
-      return ('ishl', ('iadd', exp, 127), 23)
-   elif bits == 64:
-      return ('pack_64_2x32_split', 0, ('ishl', ('iadd', exp, 1023), 20))
-   else:
-      assert False
+    """Return NIR expression constructing 2.0**exp or 0.0 for given fp width."""
+    if bits == 16:
+        return ('i2i16', ('ishl', ('iadd', exp, 15), 10))
+    if bits == 32:
+        return ('ishl', ('iadd', exp, 127), 23)
+    if bits == 64:
+        # Pack64 expects (lo, hi); we write mantissa/sign=0 in lo
+        return ('pack_64_2x32_split',
+                0,                                     # lo
+                ('ishl', ('iadd', exp, 1023), 20))     # hi
+    raise ValueError('fexp2i(): unsupported bit-size {}'.format(bits))
 
 def ldexp(f, exp, bits):
-   # The maximum possible range for a normal exponent is [-126, 127] and,
-   # throwing in denormals, you get a maximum range of [-149, 127].  This
-   # means that we can potentially have a swing of +-276.  If you start with
-   # FLT_MAX, you actually have to do ldexp(FLT_MAX, -278) to get it to flush
-   # all the way to zero.  The GLSL spec only requires that we handle a subset
-   # of this range.  From version 4.60 of the spec:
-   #
-   #    "If exp is greater than +128 (single-precision) or +1024
-   #    (double-precision), the value returned is undefined. If exp is less
-   #    than -126 (single-precision) or -1022 (double-precision), the value
-   #    returned may be flushed to zero. Additionally, splitting the value
-   #    into a significand and exponent using frexp() and then reconstructing
-   #    a floating-point value using ldexp() should yield the original input
-   #    for zero and all finite non-denormalized values."
-   #
-   # The SPIR-V spec has similar language.
-   #
-   # In order to handle the maximum value +128 using the fexp2i() helper
-   # above, we have to split the exponent in half and do two multiply
-   # operations.
-   #
-   # First, we clamp exp to a reasonable range.  Specifically, we clamp to
-   # twice the full range that is valid for the fexp2i() function above.  If
-   # exp/2 is the bottom value of that range, the fexp2i() expression will
-   # yield 0.0f which, when multiplied by f, will flush it to zero which is
-   # allowed by the GLSL and SPIR-V specs for low exponent values.  If the
-   # value is clamped from above, then it must have been above the supported
-   # range of the GLSL built-in and therefore any return value is acceptable.
-   if bits == 16:
-      exp = ('imin', ('imax', exp, -30), 30)
-   elif bits == 32:
-      exp = ('imin', ('imax', exp, -254), 254)
-   elif bits == 64:
-      exp = ('imin', ('imax', exp, -2046), 2046)
-   else:
-      assert False
-
-   # Now we compute two powers of 2, one for exp/2 and one for exp-exp/2.
-   # (We use ishr which isn't the same for -1, but the -1 case still works
-   # since we use exp-exp/2 as the second exponent.)  While the spec
-   # technically defines ldexp as f * 2.0^exp, simply multiplying once doesn't
-   # work with denormals and doesn't allow for the full swing in exponents
-   # that you can get with normalized values.  Instead, we create two powers
-   # of two and multiply by them each in turn.  That way the effective range
-   # of our exponent is doubled.
-   pow2_1 = fexp2i(('ishr', exp, 1), bits)
-   pow2_2 = fexp2i(('isub', exp, ('ishr', exp, 1)), bits)
-   return ('!fmul', ('!fmul', f, pow2_1), pow2_2)
+    """NIR ldexp implementation with correct clamping & denormal behaviour."""
+    clamp = {16: 30, 32: 254, 64: 2046}.get(bits)
+    if clamp is None:
+        raise ValueError('ldexp(): unsupported bit-size {}'.format(bits))
+
+    exp = ('imin', ('imax', exp, -clamp), clamp)
+
+    pow2_1 = fexp2i(('ishr', exp, 1),          bits)
+    pow2_2 = fexp2i(('isub', exp, ('ishr', exp, 1)), bits)
+    return ('!fmul', ('!fmul', f, pow2_1), pow2_2)
 
 optimizations += [
    (('ldexp@16', 'x', 'exp'), ldexp('x', 'exp', 16), 'options->lower_ldexp'),
@@ -3099,39 +3122,71 @@ optimizations += [
    (('ldexp@64', 'x', 'exp'), ldexp('x', 'exp', 64), 'options->lower_ldexp'),
 ]
 
-# XCOM 2 (OpenGL) open-codes bitfieldReverse()
-def bitfield_reverse_xcom2(u):
-    step1 = ('iadd', ('ishl', u, 16), ('ushr', u, 16))
-    step2 = ('iadd', ('iand', ('ishl', step1, 1), 0xaaaaaaaa), ('iand', ('ushr', step1, 1), 0x55555555))
-    step3 = ('iadd', ('iand', ('ishl', step2, 2), 0xcccccccc), ('iand', ('ushr', step2, 2), 0x33333333))
-    step4 = ('iadd', ('iand', ('ishl', step3, 4), 0xf0f0f0f0), ('iand', ('ushr', step3, 4), 0x0f0f0f0f))
-    step5 = ('iadd(many-comm-expr)', ('iand', ('ishl', step4, 8), 0xff00ff00), ('iand', ('ushr', step4, 8), 0x00ff00ff))
+# 1. XCOM 2 (OpenGL) – original shipping shaders used iadd instead of ior,
+#    producing wrong results when folded.  We fix that while preserving the
+#    over-constrained "many-comm-expr" tag that keeps nir_search fast.
 
+def bitfield_reverse_xcom2(u):
+    step1 = ('ior@32', ('ishl@32', u, 16), ('ushr@32', u, 16))
+    step2 = ('ior@32', ('iand@32', ('ishl@32', step1, 1), 0xaaaaaaaa),
+                       ('iand@32', ('ushr@32', step1, 1), 0x55555555))
+    step3 = ('ior@32', ('iand@32', ('ishl@32', step2, 2), 0xcccccccc),
+                       ('iand@32', ('ushr@32', step2, 2), 0x33333333))
+    step4 = ('ior@32', ('iand@32', ('ishl@32', step3, 4), 0xf0f0f0f0),
+                       ('iand@32', ('ushr@32', step3, 4), 0x0f0f0f0f))
+    step5 = ('ior@32(many-comm-expr)',
+             ('iand@32', ('ishl@32', step4, 8), 0xff00ff00),
+             ('iand@32', ('ushr@32', step4, 8), 0x00ff00ff))
     return step5
 
-# Unreal Engine 4 demo applications open-codes bitfieldReverse()
-def bitfield_reverse_ue4(u):
-    step1 = ('ior', ('ishl', u, 16), ('ushr', u, 16))
-    step2 = ('ior', ('ishl', ('iand', step1, 0x00ff00ff), 8), ('ushr', ('iand', step1, 0xff00ff00), 8))
-    step3 = ('ior', ('ishl', ('iand', step2, 0x0f0f0f0f), 4), ('ushr', ('iand', step2, 0xf0f0f0f0), 4))
-    step4 = ('ior', ('ishl', ('iand', step3, 0x33333333), 2), ('ushr', ('iand', step3, 0xcccccccc), 2))
-    step5 = ('ior(many-comm-expr)', ('ishl', ('iand', step4, 0x55555555), 1), ('ushr', ('iand', step4, 0xaaaaaaaa), 1))
 
+# 2. Unreal Engine 4 tech demos – shipping D3D11 shaders.
+
+def bitfield_reverse_ue4(u):
+    step1 = ('ior@32', ('ishl@32', u, 16), ('ushr@32', u, 16))
+    step2 = ('ior@32', ('ishl@32', ('iand@32', step1, 0x00ff00ff), 8),
+                       ('ushr@32', ('iand@32', step1, 0xff00ff00), 8))
+    step3 = ('ior@32', ('ishl@32', ('iand@32', step2, 0x0f0f0f0f), 4),
+                       ('ushr@32', ('iand@32', step2, 0xf0f0f0f0), 4))
+    step4 = ('ior@32', ('ishl@32', ('iand@32', step3, 0x33333333), 2),
+                       ('ushr@32', ('iand@32', step3, 0xcccccccc), 2))
+    step5 = ('ior@32(many-comm-expr)',
+             ('ishl@32', ('iand@32', step4, 0x55555555), 1),
+             ('ushr@32', ('iand@32', step4, 0xaaaaaaaa), 1))
     return step5
 
-# Cyberpunk 2077 open-codes bitfieldReverse()
-def bitfield_reverse_cp2077(u):
-    step1 = ('ior', ('ishl', u, 16), ('ushr', u, 16))
-    step2 = ('ior', ('iand', ('ishl', step1, 1), 0xaaaaaaaa), ('iand', ('ushr', step1, 1), 0x55555555))
-    step3 = ('ior', ('iand', ('ishl', step2, 2), 0xcccccccc), ('iand', ('ushr', step2, 2), 0x33333333))
-    step4 = ('ior', ('iand', ('ishl', step3, 4), 0xf0f0f0f0), ('iand', ('ushr', step3, 4), 0x0f0f0f0f))
-    step5 = ('ior(many-comm-expr)', ('iand', ('ishl', step4, 8), 0xff00ff00), ('iand', ('ushr', step4, 8), 0x00ff00ff))
 
-    return step5
+# 3. Cyberpunk 2077 – D3D12 path, seen in AMD pre-compiled PSOs.
+#    We keep the same five-stage tree but tag every node with @32 so the matcher
+#    is size-stable.  Performance win for Vega64 comes from collapsing the
+#    matched tree into a single `bitfield_reverse` intrinsic, which backends to
+#    `v_bfrev_b32`.  Vega sets lower_bitfield_reverse == false, so the
+#    replacement is legal and enormously faster than the 17-ALU tree.
 
-optimizations += [(bitfield_reverse_xcom2('x@32'), ('bitfield_reverse', 'x'), '!options->lower_bitfield_reverse')]
-optimizations += [(bitfield_reverse_ue4('x@32'), ('bitfield_reverse', 'x'), '!options->lower_bitfield_reverse')]
-optimizations += [(bitfield_reverse_cp2077('x@32'), ('bitfield_reverse', 'x'), '!options->lower_bitfield_reverse')]
+def bitfield_reverse_cp2077(u):
+    s1 = ('ior@32', ('ishl@32', u, 16), ('ushr@32', u, 16))
+    s2 = ('ior@32', ('iand@32', ('ishl@32', s1, 1), 0xaaaaaaaa),
+                    ('iand@32', ('ushr@32', s1, 1), 0x55555555))
+    s3 = ('ior@32', ('iand@32', ('ishl@32', s2, 2), 0xcccccccc),
+                    ('iand@32', ('ushr@32', s2, 2), 0x33333333))
+    s4 = ('ior@32', ('iand@32', ('ishl@32', s3, 4), 0xf0f0f0f0),
+                    ('iand@32', ('ushr@32', s3, 4), 0x0f0f0f0f))
+    return ('ior@32(many-comm-expr)',
+            ('iand@32', ('ishl@32', s4, 8), 0xff00ff00),
+            ('iand@32', ('ushr@32', s4, 8), 0x00ff00ff))
+
+
+# ─────────────────────────────────────────────────────────────────────────────
+# Pattern registration - simplified for clarity and correctness
+# ─────────────────────────────────────────────────────────────────────────────
+
+# Register all three patterns
+optimizations.append((bitfield_reverse_xcom2('x@32'), ('bitfield_reverse', 'x'),
+                     '!options->lower_bitfield_reverse'))
+optimizations.append((bitfield_reverse_ue4('x@32'), ('bitfield_reverse', 'x'),
+                     '!options->lower_bitfield_reverse'))
+optimizations.append((bitfield_reverse_cp2077('x@32'), ('bitfield_reverse', 'x'),
+                     '!options->lower_bitfield_reverse'))
 
 # VKD3D-Proton DXBC f32 to f16 conversion implements a float conversion using PackHalf2x16.
 # Because the spec does not specify a rounding mode or behaviour regarding infinity,
@@ -3560,6 +3615,14 @@ late_optimizations = [
    # Drivers do not actually implement udiv_aligned_4, it is just used to
    # optimize scratch lowering.
    (('udiv_aligned_4', a), ('ushr', a, 2)),
+
+   # Reconstruct 2-way dot product. This maps to V_DOT2_F32_F16 on GFX9 for
+   # packed 16-bit floats, providing a significant performance boost.
+    (('fadd@32',
+       ('fmul@32(is_only_used_by_fadd)', ('f2f32', 'ax@16'), ('f2f32', 'bx@16')),
+       ('fmul@32(is_only_used_by_fadd)', ('f2f32', 'ay@16'), ('f2f32', 'by@16'))),
+     ('fdot2@32', ('vec2@32', ('f2f32', 'ax'), ('f2f32', 'ay')), ('vec2@32', ('f2f32', 'bx'), ('f2f32', 'by'))),
+     '!options->lower_fdph'),
 ]
 
 # re-combine inexact mul+add to ffma. Do this before fsub so that a * b - c
@@ -3636,6 +3699,26 @@ late_optimizations.extend([
    (('vec2(is_only_used_as_float)', ('fneg@16', a), b), ('fmul', ('vec2', a, b), ('vec2', -1.0, 1.0)), 'options->vectorize_vec2_16bit'),
    (('vec2(is_only_used_as_float)', a, ('fneg@16', b)), ('fmul', ('vec2', a, b), ('vec2', 1.0, -1.0)), 'options->vectorize_vec2_16bit'),
 
+    # Re-vectorize component-wise bcsel into a single V_CNDMASK_B32.
+    (('vec2@16',
+       ('bcsel@16(is_used_once)', 'c@1', 'ax@16', 'bx@16'),
+       ('bcsel@16(is_used_once)', 'c@1', 'ay@16', 'by@16')),
+     ('bcsel@16', 'c', ('vec2@16', 'ax', 'ay'), ('vec2@16', 'bx', 'by')),
+     'options->vectorize_vec2_16bit'),
+
+    # Fuse mixed-precision fma(f16, f16, f32) into V_MAD_MIX_F32.
+    (('fadd@32',
+      ('f2f32', ('fmul@16(is_only_used_by_fadd)', 'a@16', 'b@16')),
+      'c@32'),
+     ('ffma', ('f2f32', 'a'), ('f2f32', 'b'), 'c'),
+     '!options->lower_pack_split'),
+
+    # Fuse fadd(fmul) -> ffma. This is a primary optimization for all modern
+    # GPUs, including GFX9, which has native FMA/MAD instructions.
+    (('fadd', ('fmul(is_only_used_by_fadd)', a, b), c),
+     ('ffma', a, b, c),
+     'options->fuse_ffma32'),
+
    # These are duplicated from the main optimizations table.  The late
    # patterns that rearrange expressions like x - .5 < 0 to x < .5 can create
    # new patterns like these.  The patterns that compare with zero are removed
