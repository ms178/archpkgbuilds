--- sha1.c.orig	2025-07-06 14:49:54.254429212 +0200
+++ sha1.c	2025-07-06 16:55:09.150579865 +0200
@@ -1,170 +1,495 @@
-/*	$OpenBSD: sha1.c,v 1.26 2015/09/11 09:18:27 guenther Exp $	*/
-
 /*
  * SHA-1 in C
  * By Steve Reid <steve@edmweb.com>
  * 100% Public Domain
  *
- * Test Vectors (from FIPS PUB 180-1)
- * "abc"
- *   A9993E36 4706816A BA3E2571 7850C26C 9CD0D89D
- * "abcdbcdecdefdefgefghfghighijhijkijkljklmklmnlmnomnopnopq"
- *   84983E44 1C3BD26E BAAE4AA1 F95129E5 E54670F1
- * A million repetitions of "a"
- *   34AA973C D4C4DAA4 F61EEB2B DBAD2731 6534016F
+ * Optimized for Intel Raptor Lake and AMD Vega 64
  */
 
 #include <stdint.h>
 #include <string.h>
 #include "sha1.h"
 
-#define rol(value, bits) (((value) << (bits)) | ((value) >> (32 - (bits))))
-
-/*
- * blk0() and blk() perform the initial expand.
- * I got the idea of expanding during the round function from SSLeay
- */
-# define blk0(i) (block->l[i] = (rol(block->l[i],24)&0xFF00FF00) \
-    |(rol(block->l[i],8)&0x00FF00FF))
-    
-#define blk(i) (block->l[i&15] = rol(block->l[(i+13)&15]^block->l[(i+8)&15] \
-    ^block->l[(i+2)&15]^block->l[i&15],1))
+/* Detect endianness at compile time */
+#if defined(__BYTE_ORDER__) && defined(__ORDER_LITTLE_ENDIAN__)
+#define SHA1_LITTLE_ENDIAN (__BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__)
+#elif defined(_WIN32) || defined(__i386__) || defined(__x86_64__) || defined(__aarch64__)
+#define SHA1_LITTLE_ENDIAN 1
+#else
+#define SHA1_LITTLE_ENDIAN 0
+#endif
+
+/* Architecture detection */
+#if defined(__x86_64__) || defined(_M_X64) || defined(__i386__) || defined(_M_IX86)
+#define SHA1_ARCH_X86 1
+#if defined(__x86_64__) || defined(_M_X64)
+#define SHA1_ARCH_X86_64 1
+#endif
+#endif
+
+/* Compiler-specific optimizations */
+#if defined(__GNUC__) || defined(__clang__)
+#define SHA1_INLINE __attribute__((always_inline)) inline
+#elif defined(_MSC_VER)
+#define SHA1_INLINE __forceinline
+#else
+#define SHA1_INLINE inline
+#endif
+
+/* Include intrinsics */
+#if defined(SHA1_ARCH_X86)
+#if defined(_MSC_VER) || defined(__clang__)
+#include <intrin.h>
+#endif
+#if defined(__GNUC__) || defined(__clang__)
+#include <immintrin.h>
+#endif
+#endif
+
+/* Optimized rotate left */
+static SHA1_INLINE uint32_t
+rotl32(uint32_t v, unsigned bits)
+{
+    #if defined(_MSC_VER)
+    return _rotl(v, bits);
+    #else
+    return (v << bits) | (v >> (32u - bits));
+    #endif
+}
 
-/*
- * (R0+R1), R2, R3, R4 are the different operations (rounds) used in SHA1
- */
-#define R0(v,w,x,y,z,i) z+=((w&(x^y))^y)+blk0(i)+0x5A827999+rol(v,5);w=rol(w,30);
-#define R1(v,w,x,y,z,i) z+=((w&(x^y))^y)+blk(i)+0x5A827999+rol(v,5);w=rol(w,30);
-#define R2(v,w,x,y,z,i) z+=(w^x^y)+blk(i)+0x6ED9EBA1+rol(v,5);w=rol(w,30);
-#define R3(v,w,x,y,z,i) z+=(((w|x)&y)|(w&x))+blk(i)+0x8F1BBCDC+rol(v,5);w=rol(w,30);
-#define R4(v,w,x,y,z,i) z+=(w^x^y)+blk(i)+0xCA62C1D6+rol(v,5);w=rol(w,30);
-
-typedef union {
-	uint8_t c[64];
-	uint32_t l[16];
-} CHAR64LONG16;
+/* Optimized byte swap for little endian */
+static SHA1_INLINE uint32_t
+swap32(uint32_t v)
+{
+    #if defined(__GNUC__) || defined(__clang__)
+    return __builtin_bswap32(v);
+    #elif defined(_MSC_VER)
+    return _byteswap_ulong(v);
+    #else
+    return ((v & 0x000000FFU) << 24) |
+    ((v & 0x0000FF00U) << 8)  |
+    ((v & 0x00FF0000U) >> 8)  |
+    ((v & 0xFF000000U) >> 24);
+    #endif
+}
 
-/*
- * Hash a single 512-bit block. This is the core of the algorithm.
- */
-void
-SHA1Transform(uint32_t state[5], const uint8_t* buffer)
+/* SHA1 round macros */
+#define R0(v,w,x,y,z,i) do { \
+z += ((w&(x^y))^y) + blk0(i) + 0x5A827999 + rotl32(v,5); \
+w = rotl32(w,30); \
+} while(0)
+
+#define R1(v,w,x,y,z,i) do { \
+z += ((w&(x^y))^y) + blk(i) + 0x5A827999 + rotl32(v,5); \
+w = rotl32(w,30); \
+} while(0)
+
+#define R2(v,w,x,y,z,i) do { \
+z += (w^x^y) + blk(i) + 0x6ED9EBA1 + rotl32(v,5); \
+w = rotl32(w,30); \
+} while(0)
+
+#define R3(v,w,x,y,z,i) do { \
+z += (((w|x)&y)|(w&x)) + blk(i) + 0x8F1BBCDC + rotl32(v,5); \
+w = rotl32(w,30); \
+} while(0)
+
+#define R4(v,w,x,y,z,i) do { \
+z += (w^x^y) + blk(i) + 0xCA62C1D6 + rotl32(v,5); \
+w = rotl32(w,30); \
+} while(0)
+
+/* Core SHA1 transform - optimized for performance */
+static void
+SHA1Transform(uint32_t state[5], const uint8_t buffer[SHA1_BLOCK_LENGTH])
 {
-	uint32_t a, b, c, d, e;
-	uint8_t workspace[SHA1_BLOCK_LENGTH];
-	CHAR64LONG16 *block = (CHAR64LONG16 *)workspace;
-
-	(void)memcpy(block, buffer, SHA1_BLOCK_LENGTH);
-
-	/* Copy context->state[] to working vars */
-	a = state[0];
-	b = state[1];
-	c = state[2];
-	d = state[3];
-	e = state[4];
-
-	/* 4 rounds of 20 operations each. Loop unrolled. */
-	R0(a,b,c,d,e, 0); R0(e,a,b,c,d, 1); R0(d,e,a,b,c, 2); R0(c,d,e,a,b, 3);
-	R0(b,c,d,e,a, 4); R0(a,b,c,d,e, 5); R0(e,a,b,c,d, 6); R0(d,e,a,b,c, 7);
-	R0(c,d,e,a,b, 8); R0(b,c,d,e,a, 9); R0(a,b,c,d,e,10); R0(e,a,b,c,d,11);
-	R0(d,e,a,b,c,12); R0(c,d,e,a,b,13); R0(b,c,d,e,a,14); R0(a,b,c,d,e,15);
-	R1(e,a,b,c,d,16); R1(d,e,a,b,c,17); R1(c,d,e,a,b,18); R1(b,c,d,e,a,19);
-	R2(a,b,c,d,e,20); R2(e,a,b,c,d,21); R2(d,e,a,b,c,22); R2(c,d,e,a,b,23);
-	R2(b,c,d,e,a,24); R2(a,b,c,d,e,25); R2(e,a,b,c,d,26); R2(d,e,a,b,c,27);
-	R2(c,d,e,a,b,28); R2(b,c,d,e,a,29); R2(a,b,c,d,e,30); R2(e,a,b,c,d,31);
-	R2(d,e,a,b,c,32); R2(c,d,e,a,b,33); R2(b,c,d,e,a,34); R2(a,b,c,d,e,35);
-	R2(e,a,b,c,d,36); R2(d,e,a,b,c,37); R2(c,d,e,a,b,38); R2(b,c,d,e,a,39);
-	R3(a,b,c,d,e,40); R3(e,a,b,c,d,41); R3(d,e,a,b,c,42); R3(c,d,e,a,b,43);
-	R3(b,c,d,e,a,44); R3(a,b,c,d,e,45); R3(e,a,b,c,d,46); R3(d,e,a,b,c,47);
-	R3(c,d,e,a,b,48); R3(b,c,d,e,a,49); R3(a,b,c,d,e,50); R3(e,a,b,c,d,51);
-	R3(d,e,a,b,c,52); R3(c,d,e,a,b,53); R3(b,c,d,e,a,54); R3(a,b,c,d,e,55);
-	R3(e,a,b,c,d,56); R3(d,e,a,b,c,57); R3(c,d,e,a,b,58); R3(b,c,d,e,a,59);
-	R4(a,b,c,d,e,60); R4(e,a,b,c,d,61); R4(d,e,a,b,c,62); R4(c,d,e,a,b,63);
-	R4(b,c,d,e,a,64); R4(a,b,c,d,e,65); R4(e,a,b,c,d,66); R4(d,e,a,b,c,67);
-	R4(c,d,e,a,b,68); R4(b,c,d,e,a,69); R4(a,b,c,d,e,70); R4(e,a,b,c,d,71);
-	R4(d,e,a,b,c,72); R4(c,d,e,a,b,73); R4(b,c,d,e,a,74); R4(a,b,c,d,e,75);
-	R4(e,a,b,c,d,76); R4(d,e,a,b,c,77); R4(c,d,e,a,b,78); R4(b,c,d,e,a,79);
-
-	/* Add the working vars back into context.state[] */
-	state[0] += a;
-	state[1] += b;
-	state[2] += c;
-	state[3] += d;
-	state[4] += e;
+    uint32_t a, b, c, d, e;
+    uint32_t block[16];
 
-	/* Wipe variables */
-	a = b = c = d = e = 0;
+    /* Load and byte swap if needed */
+    #if SHA1_LITTLE_ENDIAN
+    const uint32_t *src = (const uint32_t*)buffer;
+    for (int i = 0; i < 16; i++) {
+        block[i] = swap32(src[i]);
+    }
+    #else
+    memcpy(block, buffer, SHA1_BLOCK_LENGTH);
+    #endif
+
+    /* Macros for message schedule */
+    #define blk0(i) block[i]
+    #define blk(i) (block[i & 15] = rotl32(block[(i+13)&15] ^ \
+    block[(i+8)&15] ^ block[(i+2)&15] ^ block[i&15], 1))
+
+    /* Initialize working variables */
+    a = state[0];
+    b = state[1];
+    c = state[2];
+    d = state[3];
+    e = state[4];
+
+    /* Round 0-19 */
+    R0(a,b,c,d,e, 0); R0(e,a,b,c,d, 1); R0(d,e,a,b,c, 2); R0(c,d,e,a,b, 3);
+    R0(b,c,d,e,a, 4); R0(a,b,c,d,e, 5); R0(e,a,b,c,d, 6); R0(d,e,a,b,c, 7);
+    R0(c,d,e,a,b, 8); R0(b,c,d,e,a, 9); R0(a,b,c,d,e,10); R0(e,a,b,c,d,11);
+    R0(d,e,a,b,c,12); R0(c,d,e,a,b,13); R0(b,c,d,e,a,14); R0(a,b,c,d,e,15);
+    R1(e,a,b,c,d,16); R1(d,e,a,b,c,17); R1(c,d,e,a,b,18); R1(b,c,d,e,a,19);
+
+    /* Round 20-39 */
+    R2(a,b,c,d,e,20); R2(e,a,b,c,d,21); R2(d,e,a,b,c,22); R2(c,d,e,a,b,23);
+    R2(b,c,d,e,a,24); R2(a,b,c,d,e,25); R2(e,a,b,c,d,26); R2(d,e,a,b,c,27);
+    R2(c,d,e,a,b,28); R2(b,c,d,e,a,29); R2(a,b,c,d,e,30); R2(e,a,b,c,d,31);
+    R2(d,e,a,b,c,32); R2(c,d,e,a,b,33); R2(b,c,d,e,a,34); R2(a,b,c,d,e,35);
+    R2(e,a,b,c,d,36); R2(d,e,a,b,c,37); R2(c,d,e,a,b,38); R2(b,c,d,e,a,39);
+
+    /* Round 40-59 */
+    R3(a,b,c,d,e,40); R3(e,a,b,c,d,41); R3(d,e,a,b,c,42); R3(c,d,e,a,b,43);
+    R3(b,c,d,e,a,44); R3(a,b,c,d,e,45); R3(e,a,b,c,d,46); R3(d,e,a,b,c,47);
+    R3(c,d,e,a,b,48); R3(b,c,d,e,a,49); R3(a,b,c,d,e,50); R3(e,a,b,c,d,51);
+    R3(d,e,a,b,c,52); R3(c,d,e,a,b,53); R3(b,c,d,e,a,54); R3(a,b,c,d,e,55);
+    R3(e,a,b,c,d,56); R3(d,e,a,b,c,57); R3(c,d,e,a,b,58); R3(b,c,d,e,a,59);
+
+    /* Round 60-79 */
+    R4(a,b,c,d,e,60); R4(e,a,b,c,d,61); R4(d,e,a,b,c,62); R4(c,d,e,a,b,63);
+    R4(b,c,d,e,a,64); R4(a,b,c,d,e,65); R4(e,a,b,c,d,66); R4(d,e,a,b,c,67);
+    R4(c,d,e,a,b,68); R4(b,c,d,e,a,69); R4(a,b,c,d,e,70); R4(e,a,b,c,d,71);
+    R4(d,e,a,b,c,72); R4(c,d,e,a,b,73); R4(b,c,d,e,a,74); R4(a,b,c,d,e,75);
+    R4(e,a,b,c,d,76); R4(d,e,a,b,c,77); R4(c,d,e,a,b,78); R4(b,c,d,e,a,79);
+
+    /* Update state */
+    state[0] += a;
+    state[1] += b;
+    state[2] += c;
+    state[3] += d;
+    state[4] += e;
+
+    /* Clean up */
+    #undef blk0
+    #undef blk
 }
 
+#if defined(__SHA__) && SHA1_LITTLE_ENDIAN && defined(SHA1_ARCH_X86)
+/* Intel SHA extensions implementation */
+static void
+SHA1Transform_x86(uint32_t state[5], const uint8_t data[])
+{
+    __m128i abcd, abcd_save, e0, e0_save, e1;
+    __m128i msg0, msg1, msg2, msg3;
 
-/*
- * SHA1Init - Initialize new context
- */
-void
-SHA1Init(SHA1_CTX *context)
+    /* Byte swap mask for little endian */
+    const __m128i MASK = _mm_set_epi64x(0x0001020304050607ULL,
+                                        0x08090a0b0c0d0e0fULL);
+
+    /* Load initial values */
+    abcd = _mm_loadu_si128((const __m128i*) state);
+    e0 = _mm_set_epi32(state[4], 0, 0, 0);
+    abcd = _mm_shuffle_epi32(abcd, 0x1B);
+
+    /* Save current state */
+    abcd_save = abcd;
+    e0_save = e0;
+
+    /* Rounds 0-3 */
+    msg0 = _mm_loadu_si128((const __m128i*)(data + 0));
+    msg0 = _mm_shuffle_epi8(msg0, MASK);
+    e0 = _mm_add_epi32(e0, msg0);
+    e1 = abcd;
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 0);
+
+    /* Rounds 4-7 */
+    msg1 = _mm_loadu_si128((const __m128i*)(data + 16));
+    msg1 = _mm_shuffle_epi8(msg1, MASK);
+    e1 = _mm_sha1nexte_epu32(e1, msg1);
+    e0 = abcd;
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 0);
+    msg0 = _mm_sha1msg1_epu32(msg0, msg1);
+
+    /* Rounds 8-11 */
+    msg2 = _mm_loadu_si128((const __m128i*)(data + 32));
+    msg2 = _mm_shuffle_epi8(msg2, MASK);
+    e0 = _mm_sha1nexte_epu32(e0, msg2);
+    e1 = abcd;
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 0);
+    msg1 = _mm_sha1msg1_epu32(msg1, msg2);
+    msg0 = _mm_xor_si128(msg0, msg2);
+
+    /* Rounds 12-15 */
+    msg3 = _mm_loadu_si128((const __m128i*)(data + 48));
+    msg3 = _mm_shuffle_epi8(msg3, MASK);
+    e1 = _mm_sha1nexte_epu32(e1, msg3);
+    e0 = abcd;
+    msg0 = _mm_sha1msg2_epu32(msg0, msg3);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 0);
+    msg2 = _mm_sha1msg1_epu32(msg2, msg3);
+    msg1 = _mm_xor_si128(msg1, msg3);
+
+    /* Rounds 16-19 */
+    e0 = _mm_sha1nexte_epu32(e0, msg0);
+    e1 = abcd;
+    msg1 = _mm_sha1msg2_epu32(msg1, msg0);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 0);
+    msg3 = _mm_sha1msg1_epu32(msg3, msg0);
+    msg2 = _mm_xor_si128(msg2, msg0);
+
+    /* Rounds 20-23 */
+    e1 = _mm_sha1nexte_epu32(e1, msg1);
+    e0 = abcd;
+    msg2 = _mm_sha1msg2_epu32(msg2, msg1);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 1);
+    msg0 = _mm_sha1msg1_epu32(msg0, msg1);
+    msg3 = _mm_xor_si128(msg3, msg1);
+
+    /* Rounds 24-27 */
+    e0 = _mm_sha1nexte_epu32(e0, msg2);
+    e1 = abcd;
+    msg3 = _mm_sha1msg2_epu32(msg3, msg2);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 1);
+    msg1 = _mm_sha1msg1_epu32(msg1, msg2);
+    msg0 = _mm_xor_si128(msg0, msg2);
+
+    /* Rounds 28-31 */
+    e1 = _mm_sha1nexte_epu32(e1, msg3);
+    e0 = abcd;
+    msg0 = _mm_sha1msg2_epu32(msg0, msg3);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 1);
+    msg2 = _mm_sha1msg1_epu32(msg2, msg3);
+    msg1 = _mm_xor_si128(msg1, msg3);
+
+    /* Rounds 32-35 */
+    e0 = _mm_sha1nexte_epu32(e0, msg0);
+    e1 = abcd;
+    msg1 = _mm_sha1msg2_epu32(msg1, msg0);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 1);
+    msg3 = _mm_sha1msg1_epu32(msg3, msg0);
+    msg2 = _mm_xor_si128(msg2, msg0);
+
+    /* Rounds 36-39 */
+    e1 = _mm_sha1nexte_epu32(e1, msg1);
+    e0 = abcd;
+    msg2 = _mm_sha1msg2_epu32(msg2, msg1);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 1);
+    msg0 = _mm_sha1msg1_epu32(msg0, msg1);
+    msg3 = _mm_xor_si128(msg3, msg1);
+
+    /* Rounds 40-43 */
+    e0 = _mm_sha1nexte_epu32(e0, msg2);
+    e1 = abcd;
+    msg3 = _mm_sha1msg2_epu32(msg3, msg2);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 2);
+    msg1 = _mm_sha1msg1_epu32(msg1, msg2);
+    msg0 = _mm_xor_si128(msg0, msg2);
+
+    /* Rounds 44-47 */
+    e1 = _mm_sha1nexte_epu32(e1, msg3);
+    e0 = abcd;
+    msg0 = _mm_sha1msg2_epu32(msg0, msg3);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 2);
+    msg2 = _mm_sha1msg1_epu32(msg2, msg3);
+    msg1 = _mm_xor_si128(msg1, msg3);
+
+    /* Rounds 48-51 */
+    e0 = _mm_sha1nexte_epu32(e0, msg0);
+    e1 = abcd;
+    msg1 = _mm_sha1msg2_epu32(msg1, msg0);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 2);
+    msg3 = _mm_sha1msg1_epu32(msg3, msg0);
+    msg2 = _mm_xor_si128(msg2, msg0);
+
+    /* Rounds 52-55 */
+    e1 = _mm_sha1nexte_epu32(e1, msg1);
+    e0 = abcd;
+    msg2 = _mm_sha1msg2_epu32(msg2, msg1);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 2);
+    msg0 = _mm_sha1msg1_epu32(msg0, msg1);
+    msg3 = _mm_xor_si128(msg3, msg1);
+
+    /* Rounds 56-59 */
+    e0 = _mm_sha1nexte_epu32(e0, msg2);
+    e1 = abcd;
+    msg3 = _mm_sha1msg2_epu32(msg3, msg2);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 2);
+    msg1 = _mm_sha1msg1_epu32(msg1, msg2);
+    msg0 = _mm_xor_si128(msg0, msg2);
+
+    /* Rounds 60-63 */
+    e1 = _mm_sha1nexte_epu32(e1, msg3);
+    e0 = abcd;
+    msg0 = _mm_sha1msg2_epu32(msg0, msg3);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 3);
+    msg2 = _mm_sha1msg1_epu32(msg2, msg3);
+    msg1 = _mm_xor_si128(msg1, msg3);
+
+    /* Rounds 64-67 */
+    e0 = _mm_sha1nexte_epu32(e0, msg0);
+    e1 = abcd;
+    msg1 = _mm_sha1msg2_epu32(msg1, msg0);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 3);
+    msg3 = _mm_sha1msg1_epu32(msg3, msg0);
+    msg2 = _mm_xor_si128(msg2, msg0);
+
+    /* Rounds 68-71 */
+    e1 = _mm_sha1nexte_epu32(e1, msg1);
+    e0 = abcd;
+    msg2 = _mm_sha1msg2_epu32(msg2, msg1);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 3);
+    msg3 = _mm_xor_si128(msg3, msg1);
+
+    /* Rounds 72-75 */
+    e0 = _mm_sha1nexte_epu32(e0, msg2);
+    e1 = abcd;
+    msg3 = _mm_sha1msg2_epu32(msg3, msg2);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 3);
+
+    /* Rounds 76-79 */
+    e1 = _mm_sha1nexte_epu32(e1, msg3);
+    e0 = abcd;
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 3);
+
+    /* Combine state */
+    e0 = _mm_sha1nexte_epu32(e0, e0_save);
+    abcd = _mm_add_epi32(abcd, abcd_save);
+
+    /* Save state */
+    abcd = _mm_shuffle_epi32(abcd, 0x1B);
+    _mm_storeu_si128((__m128i*) state, abcd);
+    state[4] = _mm_extract_epi32(e0, 3);
+}
+
+/* Runtime detection of SHA extensions - simplified */
+static int has_sha_extensions(void)
 {
+    static int detected = -1;
+
+    if (detected >= 0) {
+        return detected;
+    }
+
+    #if defined(_MSC_VER) || defined(__clang__)
+    int cpuinfo[4];
+    __cpuid(cpuinfo, 0);
+    if (cpuinfo[0] >= 7) {
+        __cpuidex(cpuinfo, 7, 0);
+        detected = (cpuinfo[1] & (1 << 29)) != 0;
+    } else {
+        detected = 0;
+    }
+    #else
+    /* If compiled with SHA support, assume it's available */
+    detected = 1;
+    #endif
 
-	/* SHA1 initialization constants */
-	context->count = 0;
-	context->state[0] = 0x67452301;
-	context->state[1] = 0xEFCDAB89;
-	context->state[2] = 0x98BADCFE;
-	context->state[3] = 0x10325476;
-	context->state[4] = 0xC3D2E1F0;
+    return detected;
 }
+#endif /* __SHA__ && SHA1_LITTLE_ENDIAN && SHA1_ARCH_X86 */
 
+void SHA1Init(SHA1_CTX *ctx)
+{
+    ctx->count = 0;
+    ctx->state[0] = 0x67452301;
+    ctx->state[1] = 0xEFCDAB89;
+    ctx->state[2] = 0x98BADCFE;
+    ctx->state[3] = 0x10325476;
+    ctx->state[4] = 0xC3D2E1F0;
+}
 
-/*
- * Run your data through this.
- */
-void
-SHA1Update(SHA1_CTX *context, const uint8_t *data, size_t len)
+void SHA1Update(SHA1_CTX *ctx, const uint8_t *data, size_t len)
 {
-	size_t i, j;
+    size_t j = (size_t)((ctx->count >> 3) & 63);
+    size_t i = 0;
 
-	j = (size_t)((context->count >> 3) & 63);
-	context->count += (len << 3);
-	if ((j + len) > 63) {
-		(void)memcpy(&context->buffer[j], data, (i = 64-j));
-		SHA1Transform(context->state, context->buffer);
-		for ( ; i + 63 < len; i += 64)
-			SHA1Transform(context->state, (uint8_t *)&data[i]);
-		j = 0;
-	} else {
-		i = 0;
-	}
-	(void)memcpy(&context->buffer[j], &data[i], len - i);
-}
+    /* Update bit count */
+    ctx->count += (uint64_t)len << 3;
 
+    /* Handle partial block */
+    if (j != 0) {
+        size_t copy = 64 - j;
+        if (len < copy) {
+            memcpy(&ctx->buffer[j], data, len);
+            return;
+        }
+        memcpy(&ctx->buffer[j], data, copy);
+
+        #if defined(__SHA__) && SHA1_LITTLE_ENDIAN && defined(SHA1_ARCH_X86)
+        if (has_sha_extensions()) {
+            SHA1Transform_x86(ctx->state, ctx->buffer);
+        } else
+            #endif
+        {
+            SHA1Transform(ctx->state, ctx->buffer);
+        }
+
+        i = copy;
+    }
+
+    /* Process complete blocks */
+    for (; i + 63 < len; i += 64) {
+        #if defined(__SHA__) && SHA1_LITTLE_ENDIAN && defined(SHA1_ARCH_X86)
+        if (has_sha_extensions()) {
+            SHA1Transform_x86(ctx->state, data + i);
+        } else
+            #endif
+        {
+            SHA1Transform(ctx->state, data + i);
+        }
+    }
+
+    /* Save remaining bytes */
+    if (i < len) {
+        memcpy(ctx->buffer, data + i, len - i);
+    }
+}
 
-/*
- * Add padding and return the message digest.
- */
-void
+/* Optimized padding */
+static void
 SHA1Pad(SHA1_CTX *context)
 {
-	uint8_t finalcount[8];
-	uint32_t i;
+    uint8_t finalcount[8];
+    size_t padlen;
+
+    /* Store length in big-endian format */
+    uint64_t bits = context->count;
+    finalcount[0] = (uint8_t)(bits >> 56);
+    finalcount[1] = (uint8_t)(bits >> 48);
+    finalcount[2] = (uint8_t)(bits >> 40);
+    finalcount[3] = (uint8_t)(bits >> 32);
+    finalcount[4] = (uint8_t)(bits >> 24);
+    finalcount[5] = (uint8_t)(bits >> 16);
+    finalcount[6] = (uint8_t)(bits >> 8);
+    finalcount[7] = (uint8_t)bits;
+
+    /* Calculate padding length */
+    size_t used = (size_t)((context->count >> 3) & 63);
+    padlen = (used < 56) ? (56 - used) : (120 - used);
+
+    /* Padding buffer - first byte is 0x80, rest are zeros */
+    static const uint8_t padding[64] = { 0x80 };
 
-	for (i = 0; i < 8; i++) {
-		finalcount[i] = (uint8_t)((context->count >>
-		    ((7 - (i & 7)) * 8)) & 255);	/* Endian independent */
-	}
-	SHA1Update(context, (uint8_t *)"\200", 1);
-	while ((context->count & 504) != 448)
-		SHA1Update(context, (uint8_t *)"\0", 1);
-	SHA1Update(context, finalcount, 8); /* Should cause a SHA1Transform() */
+    /* Add padding */
+    SHA1Update(context, padding, padlen);
+
+    /* Append length */
+    SHA1Update(context, finalcount, 8);
 }
 
 void
 SHA1Final(uint8_t digest[SHA1_DIGEST_LENGTH], SHA1_CTX *context)
 {
-	uint32_t i;
+    uint32_t i;
+
+    SHA1Pad(context);
 
-	SHA1Pad(context);
-	for (i = 0; i < SHA1_DIGEST_LENGTH; i++) {
-		digest[i] = (uint8_t)
-		   ((context->state[i>>2] >> ((3-(i & 3)) * 8) ) & 255);
-	}
-	memset(context, 0, sizeof(*context));
+    /* Extract hash in big-endian format */
+    for (i = 0; i < SHA1_DIGEST_LENGTH; i++) {
+        digest[i] = (uint8_t)((context->state[i>>2] >> ((3-(i & 3)) * 8)) & 255);
+    }
+
+    /* Clear sensitive data */
+    memset(context, 0, sizeof(*context));
 }
+
+/* Clean up macros */
+#undef R0
+#undef R1
+#undef R2
+#undef R3
+#undef R4
