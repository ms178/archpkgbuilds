--- a/src/util/sha1/sha1.h	2025-07-06 14:59:52.697157410 +0200
+++ b/src/util/sha1/sha1.h	2025-07-06 15:01:09.785818582 +0200
@@ -18,35 +18,19 @@
 
 #ifdef __cplusplus
 extern "C" {
-#endif
-
-typedef struct _SHA1_CTX {
-    uint32_t state[5];
-    uint64_t count;
-    uint8_t buffer[SHA1_BLOCK_LENGTH];
-} SHA1_CTX;
-
-void SHA1Init(SHA1_CTX *);
-void SHA1Pad(SHA1_CTX *);
-void SHA1Transform(uint32_t [5], const uint8_t*);
-void SHA1Update(SHA1_CTX *, const uint8_t *, size_t);
-void SHA1Final(uint8_t [SHA1_DIGEST_LENGTH], SHA1_CTX *);
+        #endif
 
-#define HTONDIGEST(x) do {                                              \
-        x[0] = htonl(x[0]);                                             \
-        x[1] = htonl(x[1]);                                             \
-        x[2] = htonl(x[2]);                                             \
-        x[3] = htonl(x[3]);                                             \
-        x[4] = htonl(x[4]); } while (0)
+        typedef struct _SHA1_CTX {
+                uint32_t state[5];
+                uint64_t count;
+                uint8_t buffer[SHA1_BLOCK_LENGTH];
+        } SHA1_CTX;
+
+        void SHA1Init(SHA1_CTX *);
+        void SHA1Update(SHA1_CTX *, const uint8_t *, size_t);
+        void SHA1Final(uint8_t [SHA1_DIGEST_LENGTH], SHA1_CTX *);
 
-#define NTOHDIGEST(x) do {                                              \
-        x[0] = ntohl(x[0]);                                             \
-        x[1] = ntohl(x[1]);                                             \
-        x[2] = ntohl(x[2]);                                             \
-        x[3] = ntohl(x[3]);                                             \
-        x[4] = ntohl(x[4]); } while (0)
-
-#ifdef __cplusplus
+        #ifdef __cplusplus
 }
 #endif
 

--- a/src/util/sha1/sha1.c	2025-07-06 14:49:54.254429212 +0200
+++ b/src/util/sha1/sha1.c	2025-06-04 18:51:59.398083736 +0200
@@ -1,170 +1,495 @@
-/*	$OpenBSD: sha1.c,v 1.26 2015/09/11 09:18:27 guenther Exp $	*/
-
 /*
  * SHA-1 in C
  * By Steve Reid <steve@edmweb.com>
  * 100% Public Domain
  *
- * Test Vectors (from FIPS PUB 180-1)
- * "abc"
- *   A9993E36 4706816A BA3E2571 7850C26C 9CD0D89D
- * "abcdbcdecdefdefgefghfghighijhijkijkljklmklmnlmnomnopnopq"
- *   84983E44 1C3BD26E BAAE4AA1 F95129E5 E54670F1
- * A million repetitions of "a"
- *   34AA973C D4C4DAA4 F61EEB2B DBAD2731 6534016F
+ * Optimized for Intel Raptor Lake and AMD Vega 64
  */
 
 #include <stdint.h>
 #include <string.h>
 #include "sha1.h"
 
-#define rol(value, bits) (((value) << (bits)) | ((value) >> (32 - (bits))))
-
-/*
- * blk0() and blk() perform the initial expand.
- * I got the idea of expanding during the round function from SSLeay
- */
-# define blk0(i) (block->l[i] = (rol(block->l[i],24)&0xFF00FF00) \
-    |(rol(block->l[i],8)&0x00FF00FF))
-    
-#define blk(i) (block->l[i&15] = rol(block->l[(i+13)&15]^block->l[(i+8)&15] \
-    ^block->l[(i+2)&15]^block->l[i&15],1))
+/* Detect endianness at compile time */
+#if defined(__BYTE_ORDER__) && defined(__ORDER_LITTLE_ENDIAN__)
+#define SHA1_LITTLE_ENDIAN (__BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__)
+#elif defined(_WIN32) || defined(__i386__) || defined(__x86_64__) || defined(__aarch64__)
+#define SHA1_LITTLE_ENDIAN 1
+#else
+#define SHA1_LITTLE_ENDIAN 0
+#endif
+
+/* Architecture detection */
+#if defined(__x86_64__) || defined(_M_X64) || defined(__i386__) || defined(_M_IX86)
+#define SHA1_ARCH_X86 1
+#if defined(__x86_64__) || defined(_M_X64)
+#define SHA1_ARCH_X86_64 1
+#endif
+#endif
+
+/* Compiler-specific optimizations */
+#if defined(__GNUC__) || defined(__clang__)
+#define SHA1_INLINE __attribute__((always_inline)) inline
+#elif defined(_MSC_VER)
+#define SHA1_INLINE __forceinline
+#else
+#define SHA1_INLINE inline
+#endif
+
+/* Include intrinsics */
+#if defined(SHA1_ARCH_X86)
+#if defined(_MSC_VER) || defined(__clang__)
+#include <intrin.h>
+#endif
+#if defined(__GNUC__) || defined(__clang__)
+#include <immintrin.h>
+#endif
+#endif
+
+/* Optimized rotate left */
+static SHA1_INLINE uint32_t
+rotl32(uint32_t v, unsigned bits)
+{
+    #if defined(_MSC_VER)
+    return _rotl(v, bits);
+    #else
+    return (v << bits) | (v >> (32u - bits));
+    #endif
+}
 
-/*
- * (R0+R1), R2, R3, R4 are the different operations (rounds) used in SHA1
- */
-#define R0(v,w,x,y,z,i) z+=((w&(x^y))^y)+blk0(i)+0x5A827999+rol(v,5);w=rol(w,30);
-#define R1(v,w,x,y,z,i) z+=((w&(x^y))^y)+blk(i)+0x5A827999+rol(v,5);w=rol(w,30);
-#define R2(v,w,x,y,z,i) z+=(w^x^y)+blk(i)+0x6ED9EBA1+rol(v,5);w=rol(w,30);
-#define R3(v,w,x,y,z,i) z+=(((w|x)&y)|(w&x))+blk(i)+0x8F1BBCDC+rol(v,5);w=rol(w,30);
-#define R4(v,w,x,y,z,i) z+=(w^x^y)+blk(i)+0xCA62C1D6+rol(v,5);w=rol(w,30);
-
-typedef union {
-	uint8_t c[64];
-	uint32_t l[16];
-} CHAR64LONG16;
+/* Optimized byte swap for little endian */
+static SHA1_INLINE uint32_t
+swap32(uint32_t v)
+{
+    #if defined(__GNUC__) || defined(__clang__)
+    return __builtin_bswap32(v);
+    #elif defined(_MSC_VER)
+    return _byteswap_ulong(v);
+    #else
+    return ((v & 0x000000FFU) << 24) |
+    ((v & 0x0000FF00U) << 8)  |
+    ((v & 0x00FF0000U) >> 8)  |
+    ((v & 0xFF000000U) >> 24);
+    #endif
+}
 
-/*
- * Hash a single 512-bit block. This is the core of the algorithm.
- */
-void
-SHA1Transform(uint32_t state[5], const uint8_t* buffer)
+/* SHA1 round macros */
+#define R0(v,w,x,y,z,i) do { \
+z += ((w&(x^y))^y) + blk0(i) + 0x5A827999 + rotl32(v,5); \
+w = rotl32(w,30); \
+} while(0)
+
+#define R1(v,w,x,y,z,i) do { \
+z += ((w&(x^y))^y) + blk(i) + 0x5A827999 + rotl32(v,5); \
+w = rotl32(w,30); \
+} while(0)
+
+#define R2(v,w,x,y,z,i) do { \
+z += (w^x^y) + blk(i) + 0x6ED9EBA1 + rotl32(v,5); \
+w = rotl32(w,30); \
+} while(0)
+
+#define R3(v,w,x,y,z,i) do { \
+z += (((w|x)&y)|(w&x)) + blk(i) + 0x8F1BBCDC + rotl32(v,5); \
+w = rotl32(w,30); \
+} while(0)
+
+#define R4(v,w,x,y,z,i) do { \
+z += (w^x^y) + blk(i) + 0xCA62C1D6 + rotl32(v,5); \
+w = rotl32(w,30); \
+} while(0)
+
+/* Core SHA1 transform - optimized for performance */
+static void
+SHA1Transform(uint32_t state[5], const uint8_t buffer[SHA1_BLOCK_LENGTH])
 {
-	uint32_t a, b, c, d, e;
-	uint8_t workspace[SHA1_BLOCK_LENGTH];
-	CHAR64LONG16 *block = (CHAR64LONG16 *)workspace;
-
-	(void)memcpy(block, buffer, SHA1_BLOCK_LENGTH);
-
-	/* Copy context->state[] to working vars */
-	a = state[0];
-	b = state[1];
-	c = state[2];
-	d = state[3];
-	e = state[4];
-
-	/* 4 rounds of 20 operations each. Loop unrolled. */
-	R0(a,b,c,d,e, 0); R0(e,a,b,c,d, 1); R0(d,e,a,b,c, 2); R0(c,d,e,a,b, 3);
-	R0(b,c,d,e,a, 4); R0(a,b,c,d,e, 5); R0(e,a,b,c,d, 6); R0(d,e,a,b,c, 7);
-	R0(c,d,e,a,b, 8); R0(b,c,d,e,a, 9); R0(a,b,c,d,e,10); R0(e,a,b,c,d,11);
-	R0(d,e,a,b,c,12); R0(c,d,e,a,b,13); R0(b,c,d,e,a,14); R0(a,b,c,d,e,15);
-	R1(e,a,b,c,d,16); R1(d,e,a,b,c,17); R1(c,d,e,a,b,18); R1(b,c,d,e,a,19);
-	R2(a,b,c,d,e,20); R2(e,a,b,c,d,21); R2(d,e,a,b,c,22); R2(c,d,e,a,b,23);
-	R2(b,c,d,e,a,24); R2(a,b,c,d,e,25); R2(e,a,b,c,d,26); R2(d,e,a,b,c,27);
-	R2(c,d,e,a,b,28); R2(b,c,d,e,a,29); R2(a,b,c,d,e,30); R2(e,a,b,c,d,31);
-	R2(d,e,a,b,c,32); R2(c,d,e,a,b,33); R2(b,c,d,e,a,34); R2(a,b,c,d,e,35);
-	R2(e,a,b,c,d,36); R2(d,e,a,b,c,37); R2(c,d,e,a,b,38); R2(b,c,d,e,a,39);
-	R3(a,b,c,d,e,40); R3(e,a,b,c,d,41); R3(d,e,a,b,c,42); R3(c,d,e,a,b,43);
-	R3(b,c,d,e,a,44); R3(a,b,c,d,e,45); R3(e,a,b,c,d,46); R3(d,e,a,b,c,47);
-	R3(c,d,e,a,b,48); R3(b,c,d,e,a,49); R3(a,b,c,d,e,50); R3(e,a,b,c,d,51);
-	R3(d,e,a,b,c,52); R3(c,d,e,a,b,53); R3(b,c,d,e,a,54); R3(a,b,c,d,e,55);
-	R3(e,a,b,c,d,56); R3(d,e,a,b,c,57); R3(c,d,e,a,b,58); R3(b,c,d,e,a,59);
-	R4(a,b,c,d,e,60); R4(e,a,b,c,d,61); R4(d,e,a,b,c,62); R4(c,d,e,a,b,63);
-	R4(b,c,d,e,a,64); R4(a,b,c,d,e,65); R4(e,a,b,c,d,66); R4(d,e,a,b,c,67);
-	R4(c,d,e,a,b,68); R4(b,c,d,e,a,69); R4(a,b,c,d,e,70); R4(e,a,b,c,d,71);
-	R4(d,e,a,b,c,72); R4(c,d,e,a,b,73); R4(b,c,d,e,a,74); R4(a,b,c,d,e,75);
-	R4(e,a,b,c,d,76); R4(d,e,a,b,c,77); R4(c,d,e,a,b,78); R4(b,c,d,e,a,79);
-
-	/* Add the working vars back into context.state[] */
-	state[0] += a;
-	state[1] += b;
-	state[2] += c;
-	state[3] += d;
-	state[4] += e;
+    uint32_t a, b, c, d, e;
+    uint32_t block[16];
 
-	/* Wipe variables */
-	a = b = c = d = e = 0;
+    /* Load and byte swap if needed */
+    #if SHA1_LITTLE_ENDIAN
+    const uint32_t *src = (const uint32_t*)buffer;
+    for (int i = 0; i < 16; i++) {
+        block[i] = swap32(src[i]);
+    }
+    #else
+    memcpy(block, buffer, SHA1_BLOCK_LENGTH);
+    #endif
+
+    /* Macros for message schedule */
+    #define blk0(i) block[i]
+    #define blk(i) (block[i & 15] = rotl32(block[(i+13)&15] ^ \
+    block[(i+8)&15] ^ block[(i+2)&15] ^ block[i&15], 1))
+
+    /* Initialize working variables */
+    a = state[0];
+    b = state[1];
+    c = state[2];
+    d = state[3];
+    e = state[4];
+
+    /* Round 0-19 */
+    R0(a,b,c,d,e, 0); R0(e,a,b,c,d, 1); R0(d,e,a,b,c, 2); R0(c,d,e,a,b, 3);
+    R0(b,c,d,e,a, 4); R0(a,b,c,d,e, 5); R0(e,a,b,c,d, 6); R0(d,e,a,b,c, 7);
+    R0(c,d,e,a,b, 8); R0(b,c,d,e,a, 9); R0(a,b,c,d,e,10); R0(e,a,b,c,d,11);
+    R0(d,e,a,b,c,12); R0(c,d,e,a,b,13); R0(b,c,d,e,a,14); R0(a,b,c,d,e,15);
+    R1(e,a,b,c,d,16); R1(d,e,a,b,c,17); R1(c,d,e,a,b,18); R1(b,c,d,e,a,19);
+
+    /* Round 20-39 */
+    R2(a,b,c,d,e,20); R2(e,a,b,c,d,21); R2(d,e,a,b,c,22); R2(c,d,e,a,b,23);
+    R2(b,c,d,e,a,24); R2(a,b,c,d,e,25); R2(e,a,b,c,d,26); R2(d,e,a,b,c,27);
+    R2(c,d,e,a,b,28); R2(b,c,d,e,a,29); R2(a,b,c,d,e,30); R2(e,a,b,c,d,31);
+    R2(d,e,a,b,c,32); R2(c,d,e,a,b,33); R2(b,c,d,e,a,34); R2(a,b,c,d,e,35);
+    R2(e,a,b,c,d,36); R2(d,e,a,b,c,37); R2(c,d,e,a,b,38); R2(b,c,d,e,a,39);
+
+    /* Round 40-59 */
+    R3(a,b,c,d,e,40); R3(e,a,b,c,d,41); R3(d,e,a,b,c,42); R3(c,d,e,a,b,43);
+    R3(b,c,d,e,a,44); R3(a,b,c,d,e,45); R3(e,a,b,c,d,46); R3(d,e,a,b,c,47);
+    R3(c,d,e,a,b,48); R3(b,c,d,e,a,49); R3(a,b,c,d,e,50); R3(e,a,b,c,d,51);
+    R3(d,e,a,b,c,52); R3(c,d,e,a,b,53); R3(b,c,d,e,a,54); R3(a,b,c,d,e,55);
+    R3(e,a,b,c,d,56); R3(d,e,a,b,c,57); R3(c,d,e,a,b,58); R3(b,c,d,e,a,59);
+
+    /* Round 60-79 */
+    R4(a,b,c,d,e,60); R4(e,a,b,c,d,61); R4(d,e,a,b,c,62); R4(c,d,e,a,b,63);
+    R4(b,c,d,e,a,64); R4(a,b,c,d,e,65); R4(e,a,b,c,d,66); R4(d,e,a,b,c,67);
+    R4(c,d,e,a,b,68); R4(b,c,d,e,a,69); R4(a,b,c,d,e,70); R4(e,a,b,c,d,71);
+    R4(d,e,a,b,c,72); R4(c,d,e,a,b,73); R4(b,c,d,e,a,74); R4(a,b,c,d,e,75);
+    R4(e,a,b,c,d,76); R4(d,e,a,b,c,77); R4(c,d,e,a,b,78); R4(b,c,d,e,a,79);
+
+    /* Update state */
+    state[0] += a;
+    state[1] += b;
+    state[2] += c;
+    state[3] += d;
+    state[4] += e;
+
+    /* Clean up */
+    #undef blk0
+    #undef blk
 }
 
+#if defined(__SHA__) && SHA1_LITTLE_ENDIAN && defined(SHA1_ARCH_X86)
+/* Intel SHA extensions implementation */
+static void
+SHA1Transform_x86(uint32_t state[5], const uint8_t data[])
+{
+    __m128i abcd, abcd_save, e0, e0_save, e1;
+    __m128i msg0, msg1, msg2, msg3;
 
-/*
- * SHA1Init - Initialize new context
- */
-void
-SHA1Init(SHA1_CTX *context)
+    /* Byte swap mask for little endian */
+    const __m128i MASK = _mm_set_epi64x(0x0001020304050607ULL,
+                                        0x08090a0b0c0d0e0fULL);
+
+    /* Load initial values */
+    abcd = _mm_loadu_si128((const __m128i*) state);
+    e0 = _mm_set_epi32(state[4], 0, 0, 0);
+    abcd = _mm_shuffle_epi32(abcd, 0x1B);
+
+    /* Save current state */
+    abcd_save = abcd;
+    e0_save = e0;
+
+    /* Rounds 0-3 */
+    msg0 = _mm_loadu_si128((const __m128i*)(data + 0));
+    msg0 = _mm_shuffle_epi8(msg0, MASK);
+    e0 = _mm_add_epi32(e0, msg0);
+    e1 = abcd;
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 0);
+
+    /* Rounds 4-7 */
+    msg1 = _mm_loadu_si128((const __m128i*)(data + 16));
+    msg1 = _mm_shuffle_epi8(msg1, MASK);
+    e1 = _mm_sha1nexte_epu32(e1, msg1);
+    e0 = abcd;
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 0);
+    msg0 = _mm_sha1msg1_epu32(msg0, msg1);
+
+    /* Rounds 8-11 */
+    msg2 = _mm_loadu_si128((const __m128i*)(data + 32));
+    msg2 = _mm_shuffle_epi8(msg2, MASK);
+    e0 = _mm_sha1nexte_epu32(e0, msg2);
+    e1 = abcd;
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 0);
+    msg1 = _mm_sha1msg1_epu32(msg1, msg2);
+    msg0 = _mm_xor_si128(msg0, msg2);
+
+    /* Rounds 12-15 */
+    msg3 = _mm_loadu_si128((const __m128i*)(data + 48));
+    msg3 = _mm_shuffle_epi8(msg3, MASK);
+    e1 = _mm_sha1nexte_epu32(e1, msg3);
+    e0 = abcd;
+    msg0 = _mm_sha1msg2_epu32(msg0, msg3);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 0);
+    msg2 = _mm_sha1msg1_epu32(msg2, msg3);
+    msg1 = _mm_xor_si128(msg1, msg3);
+
+    /* Rounds 16-19 */
+    e0 = _mm_sha1nexte_epu32(e0, msg0);
+    e1 = abcd;
+    msg1 = _mm_sha1msg2_epu32(msg1, msg0);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 0);
+    msg3 = _mm_sha1msg1_epu32(msg3, msg0);
+    msg2 = _mm_xor_si128(msg2, msg0);
+
+    /* Rounds 20-23 */
+    e1 = _mm_sha1nexte_epu32(e1, msg1);
+    e0 = abcd;
+    msg2 = _mm_sha1msg2_epu32(msg2, msg1);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 1);
+    msg0 = _mm_sha1msg1_epu32(msg0, msg1);
+    msg3 = _mm_xor_si128(msg3, msg1);
+
+    /* Rounds 24-27 */
+    e0 = _mm_sha1nexte_epu32(e0, msg2);
+    e1 = abcd;
+    msg3 = _mm_sha1msg2_epu32(msg3, msg2);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 1);
+    msg1 = _mm_sha1msg1_epu32(msg1, msg2);
+    msg0 = _mm_xor_si128(msg0, msg2);
+
+    /* Rounds 28-31 */
+    e1 = _mm_sha1nexte_epu32(e1, msg3);
+    e0 = abcd;
+    msg0 = _mm_sha1msg2_epu32(msg0, msg3);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 1);
+    msg2 = _mm_sha1msg1_epu32(msg2, msg3);
+    msg1 = _mm_xor_si128(msg1, msg3);
+
+    /* Rounds 32-35 */
+    e0 = _mm_sha1nexte_epu32(e0, msg0);
+    e1 = abcd;
+    msg1 = _mm_sha1msg2_epu32(msg1, msg0);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 1);
+    msg3 = _mm_sha1msg1_epu32(msg3, msg0);
+    msg2 = _mm_xor_si128(msg2, msg0);
+
+    /* Rounds 36-39 */
+    e1 = _mm_sha1nexte_epu32(e1, msg1);
+    e0 = abcd;
+    msg2 = _mm_sha1msg2_epu32(msg2, msg1);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 1);
+    msg0 = _mm_sha1msg1_epu32(msg0, msg1);
+    msg3 = _mm_xor_si128(msg3, msg1);
+
+    /* Rounds 40-43 */
+    e0 = _mm_sha1nexte_epu32(e0, msg2);
+    e1 = abcd;
+    msg3 = _mm_sha1msg2_epu32(msg3, msg2);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 2);
+    msg1 = _mm_sha1msg1_epu32(msg1, msg2);
+    msg0 = _mm_xor_si128(msg0, msg2);
+
+    /* Rounds 44-47 */
+    e1 = _mm_sha1nexte_epu32(e1, msg3);
+    e0 = abcd;
+    msg0 = _mm_sha1msg2_epu32(msg0, msg3);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 2);
+    msg2 = _mm_sha1msg1_epu32(msg2, msg3);
+    msg1 = _mm_xor_si128(msg1, msg3);
+
+    /* Rounds 48-51 */
+    e0 = _mm_sha1nexte_epu32(e0, msg0);
+    e1 = abcd;
+    msg1 = _mm_sha1msg2_epu32(msg1, msg0);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 2);
+    msg3 = _mm_sha1msg1_epu32(msg3, msg0);
+    msg2 = _mm_xor_si128(msg2, msg0);
+
+    /* Rounds 52-55 */
+    e1 = _mm_sha1nexte_epu32(e1, msg1);
+    e0 = abcd;
+    msg2 = _mm_sha1msg2_epu32(msg2, msg1);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 2);
+    msg0 = _mm_sha1msg1_epu32(msg0, msg1);
+    msg3 = _mm_xor_si128(msg3, msg1);
+
+    /* Rounds 56-59 */
+    e0 = _mm_sha1nexte_epu32(e0, msg2);
+    e1 = abcd;
+    msg3 = _mm_sha1msg2_epu32(msg3, msg2);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 2);
+    msg1 = _mm_sha1msg1_epu32(msg1, msg2);
+    msg0 = _mm_xor_si128(msg0, msg2);
+
+    /* Rounds 60-63 */
+    e1 = _mm_sha1nexte_epu32(e1, msg3);
+    e0 = abcd;
+    msg0 = _mm_sha1msg2_epu32(msg0, msg3);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 3);
+    msg2 = _mm_sha1msg1_epu32(msg2, msg3);
+    msg1 = _mm_xor_si128(msg1, msg3);
+
+    /* Rounds 64-67 */
+    e0 = _mm_sha1nexte_epu32(e0, msg0);
+    e1 = abcd;
+    msg1 = _mm_sha1msg2_epu32(msg1, msg0);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 3);
+    msg3 = _mm_sha1msg1_epu32(msg3, msg0);
+    msg2 = _mm_xor_si128(msg2, msg0);
+
+    /* Rounds 68-71 */
+    e1 = _mm_sha1nexte_epu32(e1, msg1);
+    e0 = abcd;
+    msg2 = _mm_sha1msg2_epu32(msg2, msg1);
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 3);
+    msg3 = _mm_xor_si128(msg3, msg1);
+
+    /* Rounds 72-75 */
+    e0 = _mm_sha1nexte_epu32(e0, msg2);
+    e1 = abcd;
+    msg3 = _mm_sha1msg2_epu32(msg3, msg2);
+    abcd = _mm_sha1rnds4_epu32(abcd, e0, 3);
+
+    /* Rounds 76-79 */
+    e1 = _mm_sha1nexte_epu32(e1, msg3);
+    e0 = abcd;
+    abcd = _mm_sha1rnds4_epu32(abcd, e1, 3);
+
+    /* Combine state */
+    e0 = _mm_sha1nexte_epu32(e0, e0_save);
+    abcd = _mm_add_epi32(abcd, abcd_save);
+
+    /* Save state */
+    abcd = _mm_shuffle_epi32(abcd, 0x1B);
+    _mm_storeu_si128((__m128i*) state, abcd);
+    state[4] = _mm_extract_epi32(e0, 3);
+}
+
+/* Runtime detection of SHA extensions - simplified */
+static int has_sha_extensions(void)
 {
+    static int detected = -1;
+
+    if (detected >= 0) {
+        return detected;
+    }
+
+    #if defined(_MSC_VER) || defined(__clang__)
+    int cpuinfo[4];
+    __cpuid(cpuinfo, 0);
+    if (cpuinfo[0] >= 7) {
+        __cpuidex(cpuinfo, 7, 0);
+        detected = (cpuinfo[1] & (1 << 29)) != 0;
+    } else {
+        detected = 0;
+    }
+    #else
+    /* If compiled with SHA support, assume it's available */
+    detected = 1;
+    #endif
 
-	/* SHA1 initialization constants */
-	context->count = 0;
-	context->state[0] = 0x67452301;
-	context->state[1] = 0xEFCDAB89;
-	context->state[2] = 0x98BADCFE;
-	context->state[3] = 0x10325476;
-	context->state[4] = 0xC3D2E1F0;
+    return detected;
 }
+#endif /* __SHA__ && SHA1_LITTLE_ENDIAN && SHA1_ARCH_X86 */
 
+void SHA1Init(SHA1_CTX *ctx)
+{
+    ctx->count = 0;
+    ctx->state[0] = 0x67452301;
+    ctx->state[1] = 0xEFCDAB89;
+    ctx->state[2] = 0x98BADCFE;
+    ctx->state[3] = 0x10325476;
+    ctx->state[4] = 0xC3D2E1F0;
+}
 
-/*
- * Run your data through this.
- */
-void
-SHA1Update(SHA1_CTX *context, const uint8_t *data, size_t len)
+void SHA1Update(SHA1_CTX *ctx, const uint8_t *data, size_t len)
 {
-	size_t i, j;
+    size_t j = (size_t)((ctx->count >> 3) & 63);
+    size_t i = 0;
 
-	j = (size_t)((context->count >> 3) & 63);
-	context->count += (len << 3);
-	if ((j + len) > 63) {
-		(void)memcpy(&context->buffer[j], data, (i = 64-j));
-		SHA1Transform(context->state, context->buffer);
-		for ( ; i + 63 < len; i += 64)
-			SHA1Transform(context->state, (uint8_t *)&data[i]);
-		j = 0;
-	} else {
-		i = 0;
-	}
-	(void)memcpy(&context->buffer[j], &data[i], len - i);
-}
+    /* Update bit count */
+    ctx->count += (uint64_t)len << 3;
 
+    /* Handle partial block */
+    if (j != 0) {
+        size_t copy = 64 - j;
+        if (len < copy) {
+            memcpy(&ctx->buffer[j], data, len);
+            return;
+        }
+        memcpy(&ctx->buffer[j], data, copy);
+
+        #if defined(__SHA__) && SHA1_LITTLE_ENDIAN && defined(SHA1_ARCH_X86)
+        if (has_sha_extensions()) {
+            SHA1Transform_x86(ctx->state, ctx->buffer);
+        } else
+            #endif
+        {
+            SHA1Transform(ctx->state, ctx->buffer);
+        }
+
+        i = copy;
+    }
+
+    /* Process complete blocks */
+    for (; i + 63 < len; i += 64) {
+        #if defined(__SHA__) && SHA1_LITTLE_ENDIAN && defined(SHA1_ARCH_X86)
+        if (has_sha_extensions()) {
+            SHA1Transform_x86(ctx->state, data + i);
+        } else
+            #endif
+        {
+            SHA1Transform(ctx->state, data + i);
+        }
+    }
+
+    /* Save remaining bytes */
+    if (i < len) {
+        memcpy(ctx->buffer, data + i, len - i);
+    }
+}
 
-/*
- * Add padding and return the message digest.
- */
-void
+/* Optimized padding */
+static void
 SHA1Pad(SHA1_CTX *context)
 {
-	uint8_t finalcount[8];
-	uint32_t i;
+    uint8_t finalcount[8];
+    size_t padlen;
+
+    /* Store length in big-endian format */
+    uint64_t bits = context->count;
+    finalcount[0] = (uint8_t)(bits >> 56);
+    finalcount[1] = (uint8_t)(bits >> 48);
+    finalcount[2] = (uint8_t)(bits >> 40);
+    finalcount[3] = (uint8_t)(bits >> 32);
+    finalcount[4] = (uint8_t)(bits >> 24);
+    finalcount[5] = (uint8_t)(bits >> 16);
+    finalcount[6] = (uint8_t)(bits >> 8);
+    finalcount[7] = (uint8_t)bits;
+
+    /* Calculate padding length */
+    size_t used = (size_t)((context->count >> 3) & 63);
+    padlen = (used < 56) ? (56 - used) : (120 - used);
+
+    /* Padding buffer - first byte is 0x80, rest are zeros */
+    static const uint8_t padding[64] = { 0x80 };
 
-	for (i = 0; i < 8; i++) {
-		finalcount[i] = (uint8_t)((context->count >>
-		    ((7 - (i & 7)) * 8)) & 255);	/* Endian independent */
-	}
-	SHA1Update(context, (uint8_t *)"\200", 1);
-	while ((context->count & 504) != 448)
-		SHA1Update(context, (uint8_t *)"\0", 1);
-	SHA1Update(context, finalcount, 8); /* Should cause a SHA1Transform() */
+    /* Add padding */
+    SHA1Update(context, padding, padlen);
+
+    /* Append length */
+    SHA1Update(context, finalcount, 8);
 }
 
 void
 SHA1Final(uint8_t digest[SHA1_DIGEST_LENGTH], SHA1_CTX *context)
 {
-	uint32_t i;
+    uint32_t i;
+
+    SHA1Pad(context);
 
-	SHA1Pad(context);
-	for (i = 0; i < SHA1_DIGEST_LENGTH; i++) {
-		digest[i] = (uint8_t)
-		   ((context->state[i>>2] >> ((3-(i & 3)) * 8) ) & 255);
-	}
-	memset(context, 0, sizeof(*context));
+    /* Extract hash in big-endian format */
+    for (i = 0; i < SHA1_DIGEST_LENGTH; i++) {
+        digest[i] = (uint8_t)((context->state[i>>2] >> ((3-(i & 3)) * 8)) & 255);
+    }
+
+    /* Clear sensitive data */
+    memset(context, 0, sizeof(*context));
 }
+
+/* Clean up macros */
+#undef R0
+#undef R1
+#undef R2
+#undef R3
+#undef R4

--- a/src/util/thread.h	2025-07-05 23:59:58.545397731 +0200
+++ b/src/util/thread.h	2025-07-06 00:01:11.635771822 +0200
@@ -14,6 +14,15 @@
 #include "./rc/util_rc.h"
 #include "./rc/util_rc_ptr.h"
 
+// Force inline macro for hot paths
+#if defined(_MSC_VER)
+#define DXVK_FORCE_INLINE __forceinline
+#elif defined(__GNUC__) || defined(__clang__)
+#define DXVK_FORCE_INLINE inline __attribute__((always_inline))
+#else
+#define DXVK_FORCE_INLINE inline
+#endif
+
 namespace dxvk {
 
   /**
@@ -24,7 +33,7 @@ namespace dxvk {
     Lowest,
   };
 
-#ifdef _WIN32
+  #ifdef _WIN32
 
   using ThreadProc = std::function<void()>;
 
@@ -37,8 +46,9 @@ namespace dxvk {
     : proc(std::move(proc_)) { }
 
     ~ThreadData() {
-      if (handle)
+      if (handle) {
         CloseHandle(handle);
+      }
     }
 
     HANDLE                handle = nullptr;
@@ -47,8 +57,10 @@ namespace dxvk {
     ThreadProc            proc;
 
     void decRef() {
-      if (refs.fetch_sub(1, std::memory_order_release) == 1)
+      if (refs.fetch_sub(1, std::memory_order_release) == 1) {
+        std::atomic_thread_fence(std::memory_order_acquire);
         delete this;
+      }
     }
   };
 
@@ -72,35 +84,44 @@ namespace dxvk {
 
     ~thread();
 
-    thread(thread&& other)
+    thread(thread&& other) noexcept
     : m_data(std::exchange(other.m_data, nullptr)) { }
 
-    thread& operator = (thread&& other) {
-      if (m_data)
+    thread& operator = (thread&& other) noexcept {
+      if (joinable()) {
+        std::terminate();
+      }
+
+      if (m_data) {
         m_data->decRef();
+      }
 
       m_data = std::exchange(other.m_data, nullptr);
       return *this;
     }
 
     void detach() {
+      if (!joinable()) {
+        throw std::system_error(std::make_error_code(std::errc::invalid_argument), "Thread not detachable");
+      }
+
       m_data->decRef();
       m_data = nullptr;
     }
 
-    bool joinable() const {
+    DXVK_FORCE_INLINE bool joinable() const {
       return m_data != nullptr;
     }
 
-    id get_id() const {
+    DXVK_FORCE_INLINE id get_id() const {
       return joinable() ? m_data->id : id();
     }
 
-    native_handle_type native_handle() const {
+    DXVK_FORCE_INLINE native_handle_type native_handle() const {
       return joinable() ? m_data->handle : native_handle_type();
     }
 
-    void swap(thread& other) {
+    void swap(thread& other) noexcept {
       std::swap(m_data, other.m_data);
     }
 
@@ -120,11 +141,11 @@ namespace dxvk {
 
 
   namespace this_thread {
-    inline void yield() {
+    DXVK_FORCE_INLINE void yield() {
       SwitchToThread();
     }
 
-    inline thread::id get_id() {
+    DXVK_FORCE_INLINE thread::id get_id() {
       return thread::id(GetCurrentThreadId());
     }
 
@@ -149,19 +170,19 @@ namespace dxvk {
     mutex(const mutex&) = delete;
     mutex& operator = (const mutex&) = delete;
 
-    void lock() {
+    DXVK_FORCE_INLINE void lock() noexcept {
       AcquireSRWLockExclusive(&m_lock);
     }
 
-    void unlock() {
+    DXVK_FORCE_INLINE void unlock() noexcept {
       ReleaseSRWLockExclusive(&m_lock);
     }
 
-    bool try_lock() {
+    DXVK_FORCE_INLINE bool try_lock() noexcept {
       return TryAcquireSRWLockExclusive(&m_lock);
     }
 
-    native_handle_type native_handle() {
+    DXVK_FORCE_INLINE native_handle_type native_handle() noexcept {
       return &m_lock;
     }
 
@@ -173,6 +194,72 @@ namespace dxvk {
 
 
   /**
+   * \brief Fast mutex with adaptive spinning
+   *
+   * Spins briefly before falling back to SRW lock to reduce
+   * context switch overhead for short critical sections.
+   */
+  class fast_mutex {
+  public:
+    using native_handle_type = PSRWLOCK;
+
+    fast_mutex() { }
+
+    fast_mutex(const fast_mutex&) = delete;
+    fast_mutex& operator = (const fast_mutex&) = delete;
+
+    DXVK_FORCE_INLINE void lock() noexcept {
+      if (likely(try_lock())) {
+        return;
+      }
+
+      // Phase 1: Aggressive spin for ultra-short contention.
+      for (uint32_t i = 0; i < 64; i++) {
+        #if defined(DXVK_ARCH_X86)
+        #if defined(_MSC_VER)
+        _mm_pause();
+        #elif defined(__GNUC__) || defined(__clang__)
+        __builtin_ia32_pause();
+        #endif
+        #endif
+        if (try_lock()) {
+          return;
+        }
+      }
+
+      // Phase 2: Yielding spin for short-term contention.
+      for (uint32_t i = 0; i < 16; i++) {
+        dxvk::this_thread::yield();
+        if (try_lock()) {
+          return;
+        }
+      }
+
+      // Phase 3: Fall back to a blocking OS call.
+      AcquireSRWLockExclusive(&m_lock);
+    }
+
+    DXVK_FORCE_INLINE void unlock() noexcept {
+      ReleaseSRWLockExclusive(&m_lock);
+    }
+
+    DXVK_FORCE_INLINE bool try_lock() noexcept {
+      return TryAcquireSRWLockExclusive(&m_lock);
+    }
+
+    DXVK_FORCE_INLINE native_handle_type native_handle() noexcept {
+      return &m_lock;
+    }
+
+  private:
+    SRWLOCK m_lock = SRWLOCK_INIT;
+  };
+
+  // Alias for compatibility
+  using spin_mutex = fast_mutex;
+
+
+  /**
    * \brief Recursive mutex implementation
    *
    * Drop-in replacement for \c std::recursive_mutex that
@@ -195,19 +282,19 @@ namespace dxvk {
     recursive_mutex(const recursive_mutex&) = delete;
     recursive_mutex& operator = (const recursive_mutex&) = delete;
 
-    void lock() {
+    DXVK_FORCE_INLINE void lock() noexcept {
       EnterCriticalSection(&m_lock);
     }
 
-    void unlock() {
+    DXVK_FORCE_INLINE void unlock() noexcept {
       LeaveCriticalSection(&m_lock);
     }
 
-    bool try_lock() {
+    DXVK_FORCE_INLINE bool try_lock() noexcept {
       return TryEnterCriticalSection(&m_lock);
     }
 
-    native_handle_type native_handle() {
+    DXVK_FORCE_INLINE native_handle_type native_handle() noexcept {
       return &m_lock;
     }
 
@@ -238,11 +325,11 @@ namespace dxvk {
 
     condition_variable& operator = (condition_variable&) = delete;
 
-    void notify_one() {
+    DXVK_FORCE_INLINE void notify_one() noexcept {
       WakeConditionVariable(&m_cond);
     }
 
-    void notify_all() {
+    DXVK_FORCE_INLINE void notify_all() noexcept {
       WakeAllConditionVariable(&m_cond);
     }
 
@@ -253,26 +340,27 @@ namespace dxvk {
 
     template<typename Predicate>
     void wait(std::unique_lock<dxvk::mutex>& lock, Predicate pred) {
-      while (!pred())
+      while (!pred()) {
         wait(lock);
+      }
     }
 
     template<typename Clock, typename Duration>
     std::cv_status wait_until(std::unique_lock<dxvk::mutex>& lock, const std::chrono::time_point<Clock, Duration>& time) {
       auto now = Clock::now();
-
       return (now < time)
-        ? wait_for(lock, now - time)
-        : std::cv_status::timeout;
+      ? wait_for(lock, time - now)
+      : std::cv_status::timeout;
     }
 
     template<typename Clock, typename Duration, typename Predicate>
     bool wait_until(std::unique_lock<dxvk::mutex>& lock, const std::chrono::time_point<Clock, Duration>& time, Predicate pred) {
-      if (pred())
-        return true;
-
-      auto now = Clock::now();
-      return now < time && wait_for(lock, now - time, pred);
+      while (!pred()) {
+        if (wait_until(lock, time) == std::cv_status::timeout) {
+          return pred();
+        }
+      }
+      return true;
     }
 
     template<typename Rep, typename Period>
@@ -280,22 +368,27 @@ namespace dxvk {
       auto ms = std::chrono::duration_cast<std::chrono::milliseconds>(timeout);
       auto srw = lock.mutex()->native_handle();
 
-      return SleepConditionVariableSRW(&m_cond, srw, ms.count(), 0)
-        ? std::cv_status::no_timeout
-        : std::cv_status::timeout;
+      if (ms.count() < 0) {
+        ms = std::chrono::milliseconds(0);
+      }
+
+      return SleepConditionVariableSRW(&m_cond, srw, DWORD(ms.count()), 0)
+      ? std::cv_status::no_timeout
+      : std::cv_status::timeout;
     }
 
     template<typename Rep, typename Period, typename Predicate>
     bool wait_for(std::unique_lock<dxvk::mutex>& lock, const std::chrono::duration<Rep, Period>& timeout, Predicate pred) {
-      bool result = pred();
-
-      if (!result && wait_for(lock, timeout) == std::cv_status::no_timeout)
-        result = pred();
-
-      return result;
+      auto end_time = std::chrono::steady_clock::now() + timeout;
+      while (!pred()) {
+        if (wait_for(lock, end_time - std::chrono::steady_clock::now()) == std::cv_status::timeout) {
+          return pred();
+        }
+      }
+      return true;
     }
 
-    native_handle_type native_handle() {
+    DXVK_FORCE_INLINE native_handle_type native_handle() noexcept {
       return &m_cond;
     }
 
@@ -305,33 +398,36 @@ namespace dxvk {
 
   };
 
-#else
+  #else
+  #include <sched.h>
   class thread : public std::thread {
   public:
     using std::thread::thread;
 
     void set_priority(ThreadPriority priority) {
-      ::sched_param param = {};
-      int32_t policy;
+      sched_param param = {};
+      int policy;
       switch (priority) {
         default:
         case ThreadPriority::Normal: policy = SCHED_OTHER; break;
-#ifndef __linux__
+        #ifndef __linux__
         case ThreadPriority::Lowest: policy = SCHED_OTHER; break;
-#else
+        #else
         case ThreadPriority::Lowest: policy = SCHED_IDLE;  break;
-#endif
+        #endif
       }
-      ::pthread_setschedparam(this->native_handle(), policy, &param);
+      pthread_setschedparam(this->native_handle(), policy, Â¶m);
     }
   };
 
   using mutex              = std::mutex;
   using recursive_mutex    = std::recursive_mutex;
   using condition_variable = std::condition_variable;
+  using fast_mutex         = std::mutex;
+  using spin_mutex         = std::mutex;
 
   namespace this_thread {
-    inline void yield() {
+    DXVK_FORCE_INLINE void yield() {
       std::this_thread::yield();
     }
 
@@ -341,6 +437,6 @@ namespace dxvk {
       return false;
     }
   }
-#endif
+  #endif
 
 }


--- a/src/util/thread.cpp	2025-07-05 23:59:55.224477951 +0200
+++ b/src/util/thread.cpp	2025-07-06 00:01:46.855182990 +0200
@@ -1,6 +1,8 @@
 #include <atomic>
+#include <vector>
 
 #include "thread.h"
+#include "util_bit.h"
 #include "util_likely.h"
 
 #ifdef _WIN32
@@ -9,38 +11,55 @@ namespace dxvk {
 
   thread::thread(ThreadProc&& proc)
   : m_data(new ThreadData(std::move(proc))) {
+    // Check if we're in DLL detachment
+    if (this_thread::isInModuleDetachment()) {
+      delete m_data;
+      m_data = nullptr;
+      throw std::system_error(std::make_error_code(std::errc::operation_canceled),
+                              "Cannot create thread during module detachment");
+    }
+
     m_data->handle = ::CreateThread(nullptr, 0x100000,
-      thread::threadProc, m_data, STACK_SIZE_PARAM_IS_A_RESERVATION,
-      &m_data->id);
+                                    thread::threadProc, m_data, STACK_SIZE_PARAM_IS_A_RESERVATION,
+                                    &m_data->id);
 
     if (!m_data->handle) {
       delete m_data;
+      m_data = nullptr;
       throw std::system_error(std::make_error_code(std::errc::resource_unavailable_try_again), "Failed to create thread");
     }
   }
 
 
   thread::~thread() {
-    if (joinable())
+    if (joinable()) {
       std::terminate();
+    }
   }
 
 
   void thread::join() {
-    if (!joinable())
+    if (!joinable()) {
       throw std::system_error(std::make_error_code(std::errc::invalid_argument), "Thread not joinable");
+    }
 
-    if (get_id() == this_thread::get_id())
+    if (get_id() == this_thread::get_id()) {
       throw std::system_error(std::make_error_code(std::errc::resource_deadlock_would_occur), "Cannot join current thread");
+    }
 
-    if(::WaitForSingleObjectEx(m_data->handle, INFINITE, FALSE) == WAIT_FAILED)
+    if(::WaitForSingleObjectEx(m_data->handle, INFINITE, FALSE) == WAIT_FAILED) {
       throw std::system_error(std::make_error_code(std::errc::invalid_argument), "Joining thread failed");
+    }
 
     detach();
   }
 
 
   void thread::set_priority(ThreadPriority priority) {
+    if (!joinable()) {
+      return;
+    }
+
     int32_t value;
     switch (priority) {
       default:
@@ -48,12 +67,49 @@ namespace dxvk {
       case ThreadPriority::Lowest: value = THREAD_PRIORITY_LOWEST; break;
     }
 
-    if (m_data)
-      ::SetThreadPriority(m_data->handle, int32_t(value));
+    if (m_data) {
+      ::SetThreadPriority(m_data->handle, value);
+    }
   }
 
 
   uint32_t thread::hardware_concurrency() {
+    using GetLogicalProcessorInformationEx_t = BOOL (WINAPI *)(LOGICAL_PROCESSOR_RELATIONSHIP, PSYSTEM_LOGICAL_PROCESSOR_INFORMATION_EX, PDWORD);
+    static auto pGetLogicalProcessorInformationEx = reinterpret_cast<GetLogicalProcessorInformationEx_t>(
+      ::GetProcAddress(::GetModuleHandleW(L"kernel32.dll"), "GetLogicalProcessorInformationEx"));
+
+    if (pGetLogicalProcessorInformationEx) {
+      DWORD length = 0;
+      pGetLogicalProcessorInformationEx(RelationProcessorCore, nullptr, &length);
+
+      if (length > 0) {
+        std::vector<char> buffer(length);
+        auto info = reinterpret_cast<PSYSTEM_LOGICAL_PROCESSOR_INFORMATION_EX>(buffer.data());
+
+        if (pGetLogicalProcessorInformationEx(RelationProcessorCore, info, &length)) {
+          uint32_t pCoreCount = 0;
+          uint32_t totalLogicalCoreCount = 0;
+          DWORD offset = 0;
+
+          while (offset < length) {
+            auto currentInfo = reinterpret_cast<PSYSTEM_LOGICAL_PROCESSOR_INFORMATION_EX>(buffer.data() + offset);
+
+            uint32_t logicalProcessorsInCore = dxvk::bit::popcnt(currentInfo->Processor.GroupMask[0].Mask);
+            totalLogicalCoreCount += logicalProcessorsInCore;
+
+            if (currentInfo->Processor.EfficiencyClass == 0) {
+              pCoreCount += logicalProcessorsInCore;
+            }
+            offset += currentInfo->Size;
+          }
+
+          if (pCoreCount > 0 && pCoreCount < totalLogicalCoreCount) {
+            return pCoreCount;
+          }
+        }
+      }
+    }
+
     SYSTEM_INFO info = { };
     ::GetSystemInfo(&info);
     return info.dwNumberOfProcessors;
@@ -85,28 +141,33 @@ namespace dxvk::this_thread {
     static auto RtlDllShutdownInProgress = reinterpret_cast<PFN_RtlDllShutdownInProgress>(
       ::GetProcAddress(::GetModuleHandleW(L"ntdll.dll"), "RtlDllShutdownInProgress"));
 
-    return RtlDllShutdownInProgress();
+    return RtlDllShutdownInProgress && RtlDllShutdownInProgress();
   }
 
 }
 
 #else
 
+#if defined(__linux__)
+#include <unistd.h>
+#endif
+
 namespace dxvk::this_thread {
-  
-  static std::atomic<uint32_t> g_threadCtr = { 0u };
-  static thread_local uint32_t g_threadId  = 0u;
-  
-  // This implementation returns thread ids unique to the current instance.
-  // ie. if you use this across multiple .so's then you might get conflicting ids.
-  //
-  // This isn't an issue for us, as it is only used by the spinlock implementation,
-  // but may be for you if you use this elsewhere.
+
   uint32_t get_id() {
-    if (unlikely(!g_threadId))
-      g_threadId = ++g_threadCtr;
+    static thread_local uint32_t t_id = 0u;
+
+    if (likely(t_id != 0u)) {
+      return t_id;
+    }
 
-    return g_threadId;
+    #if defined(__linux__)
+    t_id = static_cast<uint32_t>(::gettid());
+    #else
+    static std::atomic<uint32_t> g_threadCtr = { 0u };
+    t_id = ++g_threadCtr;
+    #endif
+    return t_id;
   }
 
 }

--- a/src/util/util_bit.h	2025-07-05 23:05:07.187210703 +0200
+++ b/src/util/util_bit.h	2025-07-05 23:16:19.036245223 +0200
@@ -1,31 +1,31 @@
 #pragma once
 
 #if (defined(__x86_64__) && !defined(__arm64ec__)) || (defined(_M_X64) && !defined(_M_ARM64EC)) \
-    || defined(__i386__) || defined(_M_IX86) || defined(__e2k__)
-  #define DXVK_ARCH_X86
-  #if defined(__x86_64__) || defined(_M_X64) || defined(__e2k__)
-    #define DXVK_ARCH_X86_64
-  #endif
+|| defined(__i386__) || defined(_M_IX86)
+#define DXVK_ARCH_X86
+#if defined(__x86_64__) || defined(_M_X64)
+#define DXVK_ARCH_X86_64
+#endif
 #elif defined(__aarch64__) || defined(_M_ARM64) || defined(_M_ARM64EC)
-  #define DXVK_ARCH_ARM64
+#define DXVK_ARCH_ARM64
+#elif defined(__e2k__)
+// E2K is a distinct architecture, not x86
+#define DXVK_ARCH_E2K
 #endif
 
 #ifdef DXVK_ARCH_X86
-  #ifndef _MSC_VER
-    #if defined(_WIN32) && (defined(__AVX__) || defined(__AVX2__))
-      #error "AVX-enabled builds not supported due to stack alignment issues."
-    #endif
-    #if defined(__WINE__) && defined(__clang__)
-      #pragma push_macro("_WIN32")
-      #undef _WIN32
-    #endif
-    #include <x86intrin.h>
-    #if defined(__WINE__) && defined(__clang__)
-      #pragma pop_macro("_WIN32")
-    #endif
-  #else
-    #include <intrin.h>
-  #endif
+#ifndef _MSC_VER
+#if defined(__WINE__) && defined(__clang__)
+#pragma push_macro("_WIN32")
+#undef _WIN32
+#endif
+#include <x86intrin.h>
+#if defined(__WINE__) && defined(__clang__)
+#pragma pop_macro("_WIN32")
+#endif
+#else
+#include <intrin.h>
+#endif
 #endif
 
 #include "util_likely.h"
@@ -38,25 +38,54 @@
 #include <type_traits>
 #include <vector>
 
+// Force inline macro
+#if defined(_MSC_VER)
+#define DXVK_FORCE_INLINE __forceinline
+#elif defined(__GNUC__) || defined(__clang__)
+#define DXVK_FORCE_INLINE inline __attribute__((always_inline))
+#else
+#define DXVK_FORCE_INLINE inline
+#endif
+
 namespace dxvk::bit {
 
   template<typename T, typename J>
-  T cast(const J& src) {
+  DXVK_FORCE_INLINE T cast(const J& src) {
     static_assert(sizeof(T) == sizeof(J));
-    static_assert(std::is_trivially_copyable<J>::value && std::is_trivial<T>::value);
+    static_assert(std::is_trivially_copyable_v<J> && std::is_trivial_v<T>);
 
     T dst;
     std::memcpy(&dst, &src, sizeof(T));
     return dst;
   }
-  
+
   template<typename T>
-  T extract(T value, uint32_t fst, uint32_t lst) {
+  DXVK_FORCE_INLINE T extract(T value, uint32_t fst, uint32_t lst) {
+    if (unlikely(lst < fst)) {
+      return 0;
+    }
     return (value >> fst) & ~(~T(0) << (lst - fst + 1));
   }
 
   template<typename T>
-  T popcnt(T n) {
+  DXVK_FORCE_INLINE T popcnt(T n) {
+    static_assert(std::is_unsigned<T>::value, "popcnt requires unsigned type");
+
+    #if defined(__POPCNT__) && (defined(__GNUC__) || defined(__clang__))
+    if constexpr (sizeof(T) <= 4) {
+      return static_cast<T>(__builtin_popcount(static_cast<uint32_t>(n)));
+    } else if constexpr (sizeof(T) == 8) {
+      return static_cast<T>(__builtin_popcountll(static_cast<uint64_t>(n)));
+    }
+    #elif defined(_MSC_VER) && defined(DXVK_ARCH_X86) && defined(__POPCNT__)
+    if constexpr (sizeof(T) <= 4) {
+      return static_cast<T>(__popcnt(static_cast<uint32_t>(n)));
+    } else if constexpr (sizeof(T) == 8 && defined(DXVK_ARCH_X86_64)) {
+      return static_cast<T>(__popcnt64(static_cast<uint64_t>(n)));
+    }
+    #endif
+
+    // Fallback implementation
     n -= ((n >> 1u) & T(0x5555555555555555ull));
     n = (n & T(0x3333333333333333ull)) + ((n >> 2u) & T(0x3333333333333333ull));
     n = (n + (n >> 4u)) & T(0x0f0f0f0f0f0f0f0full);
@@ -64,239 +93,169 @@ namespace dxvk::bit {
     return n >> (8u * (sizeof(T) - 1u));
   }
 
-  inline uint32_t tzcnt(uint32_t n) {
+  DXVK_FORCE_INLINE uint32_t tzcnt(uint32_t n) {
     #if defined(_MSC_VER) && !defined(__clang__)
-    if(n == 0)
-      return 32;
-    return _tzcnt_u32(n);
-    #elif defined(__BMI__)
-    return __tzcnt_u32(n);
-    #elif defined(DXVK_ARCH_X86) && (defined(__GNUC__) || defined(__clang__))
-    // tzcnt is encoded as rep bsf, so we can use it on all
-    // processors, but the behaviour of zero inputs differs:
-    // - bsf:   zf = 1, cf = ?, result = ?
-    // - tzcnt: zf = 0, cf = 1, result = 32
-    // We'll have to handle this case manually.
-    uint32_t res;
-    uint32_t tmp;
-    asm (
-      "tzcnt %2, %0;"
-      "mov  $32, %1;"
-      "test  %2, %2;"
-      "cmovz %1, %0;"
-      : "=&r" (res), "=&r" (tmp)
-      : "r" (n)
-      : "cc");
-    return res;
+    unsigned long idx;
+    return _BitScanForward(&idx, n) ? idx : 32;
     #elif defined(__GNUC__) || defined(__clang__)
     return n != 0 ? __builtin_ctz(n) : 32;
     #else
+    if (unlikely(n == 0)) {
+      return 32;
+    }
     uint32_t r = 31;
-    n &= -n;
-    r -= (n & 0x0000FFFF) ? 16 : 0;
-    r -= (n & 0x00FF00FF) ?  8 : 0;
-    r -= (n & 0x0F0F0F0F) ?  4 : 0;
-    r -= (n & 0x33333333) ?  2 : 0;
-    r -= (n & 0x55555555) ?  1 : 0;
-    return n != 0 ? r : 32;
+    n &= -int32_t(n);
+    if (n & 0x0000FFFF) r -= 16;
+    if (n & 0x00FF00FF) r -=  8;
+    if (n & 0x0F0F0F0F) r -=  4;
+    if (n & 0x33333333) r -=  2;
+    if (n & 0x55555555) r -=  1;
+    return r;
     #endif
   }
 
-  inline uint32_t tzcnt(uint64_t n) {
+  DXVK_FORCE_INLINE uint32_t tzcnt(uint64_t n) {
     #if defined(DXVK_ARCH_X86_64) && defined(_MSC_VER) && !defined(__clang__)
-    if(n == 0)
-      return 64;
-    return (uint32_t)_tzcnt_u64(n);
-    #elif defined(DXVK_ARCH_X86_64) && defined(__BMI__)
-    return __tzcnt_u64(n);
-    #elif defined(DXVK_ARCH_X86_64) && (defined(__GNUC__) || defined(__clang__))
-    uint64_t res;
-    uint64_t tmp;
-    asm (
-      "tzcnt %2, %0;"
-      "mov  $64, %1;"
-      "test  %2, %2;"
-      "cmovz %1, %0;"
-      : "=&r" (res), "=&r" (tmp)
-      : "r" (n)
-      : "cc");
-    return res;
+    unsigned long idx;
+    return _BitScanForward64(&idx, n) ? static_cast<uint32_t>(idx) : 64;
     #elif defined(__GNUC__) || defined(__clang__)
     return n != 0 ? __builtin_ctzll(n) : 64;
     #else
     uint32_t lo = uint32_t(n);
     if (lo) {
       return tzcnt(lo);
-    } else {
-      uint32_t hi = uint32_t(n >> 32);
-      return tzcnt(hi) + 32;
     }
+    uint32_t hi = uint32_t(n >> 32);
+    return hi ? tzcnt(hi) + 32 : 64;
     #endif
   }
 
-  inline uint32_t bsf(uint32_t n) {
-    #if (defined(__GNUC__) || defined(__clang__)) && !defined(__BMI__) && defined(DXVK_ARCH_X86)
-    uint32_t res;
-    asm ("tzcnt %1,%0"
-    : "=r" (res)
-    : "r" (n)
-    : "cc");
-    return res;
-    #else
+  DXVK_FORCE_INLINE uint32_t bsf(uint32_t n) {
     return tzcnt(n);
-    #endif
   }
 
-  inline uint32_t bsf(uint64_t n) {
-    #if (defined(__GNUC__) || defined(__clang__)) && !defined(__BMI__) && defined(DXVK_ARCH_X86_64)
-    uint64_t res;
-    asm ("tzcnt %1,%0"
-    : "=r" (res)
-    : "r" (n)
-    : "cc");
-    return res;
-    #else
+  DXVK_FORCE_INLINE uint32_t bsf(uint64_t n) {
     return tzcnt(n);
-    #endif
   }
 
-  inline uint32_t lzcnt(uint32_t n) {
-    #if defined(_MSC_VER) && !defined(__clang__) && !defined(__LZCNT__)
+  DXVK_FORCE_INLINE uint32_t lzcnt(uint32_t n) {
+    #if defined(_MSC_VER) && !defined(__clang__)
     unsigned long bsr;
-    if(n == 0)
-      return 32;
-    _BitScanReverse(&bsr, n);
-    return 31-bsr;
-    #elif (defined(_MSC_VER) && !defined(__clang__)) || defined(__LZCNT__)
-    return _lzcnt_u32(n);
+    return _BitScanReverse(&bsr, n) ? 31 - bsr : 32;
     #elif defined(__GNUC__) || defined(__clang__)
     return n != 0 ? __builtin_clz(n) : 32;
     #else
+    if (unlikely(n == 0)) {
+      return 32;
+    }
     uint32_t r = 0;
-
-    if (n == 0)	return 32;
-
     if (n <= 0x0000FFFF) { r += 16; n <<= 16; }
-    if (n <= 0x00FFFFFF) { r += 8;  n <<= 8; }
-    if (n <= 0x0FFFFFFF) { r += 4;  n <<= 4; }
-    if (n <= 0x3FFFFFFF) { r += 2;  n <<= 2; }
-    if (n <= 0x7FFFFFFF) { r += 1;  n <<= 1; }
-
+    if (n <= 0x00FFFFFF) { r +=  8; n <<=  8; }
+    if (n <= 0x0FFFFFFF) { r +=  4; n <<=  4; }
+    if (n <= 0x3FFFFFFF) { r +=  2; n <<=  2; }
+    if (n <= 0x7FFFFFFF) { r +=  1; }
     return r;
     #endif
   }
 
-  inline uint32_t lzcnt(uint64_t n) {
-    #if defined(_MSC_VER) && !defined(__clang__) && !defined(__LZCNT__) && defined(DXVK_ARCH_X86_64)
+  DXVK_FORCE_INLINE uint32_t lzcnt(uint64_t n) {
+    #if defined(DXVK_ARCH_X86_64) && defined(_MSC_VER) && !defined(__clang__)
     unsigned long bsr;
-    if(n == 0)
-      return 64;
-    _BitScanReverse64(&bsr, n);
-    return 63-bsr;
-    #elif defined(DXVK_ARCH_X86_64) && ((defined(_MSC_VER) && !defined(__clang__)) && defined(__LZCNT__))
-    return _lzcnt_u64(n);
-    #elif defined(DXVK_ARCH_X86_64) && (defined(__GNUC__) || defined(__clang__))
+    return _BitScanReverse64(&bsr, n) ? 63 - bsr : 64;
+    #elif defined(__GNUC__) || defined(__clang__)
     return n != 0 ? __builtin_clzll(n) : 64;
     #else
-    uint32_t lo = uint32_t(n);
     uint32_t hi = uint32_t(n >> 32u);
-    return hi ? lzcnt(hi) : lzcnt(lo) + 32u;
+    return hi ? lzcnt(hi) : lzcnt(uint32_t(n)) + 32u;
     #endif
   }
 
   template<typename T>
-  uint32_t pack(T& dst, uint32_t& shift, T src, uint32_t count) {
+  DXVK_FORCE_INLINE uint32_t pack(T& dst, uint32_t& shift, T src, uint32_t count) {
     constexpr uint32_t Bits = 8 * sizeof(T);
-    if (likely(shift < Bits))
+    if (likely(shift < Bits)) {
       dst |= src << shift;
+    }
     shift += count;
     return shift > Bits ? shift - Bits : 0;
   }
 
   template<typename T>
-  uint32_t unpack(T& dst, T src, uint32_t& shift, uint32_t count) {
+  DXVK_FORCE_INLINE uint32_t unpack(T& dst, T src, uint32_t& shift, uint32_t count) {
     constexpr uint32_t Bits = 8 * sizeof(T);
-    if (likely(shift < Bits))
+    if (likely(shift < Bits)) {
       dst = (src >> shift) & ((T(1) << count) - 1);
+    }
     shift += count;
     return shift > Bits ? shift - Bits : 0;
   }
 
-
-  /**
-   * \brief Clears cache lines of memory
-   *
-   * Uses non-temporal stores. The memory region offset
-   * and size are assumed to be aligned to 64 bytes.
-   * \param [in] mem Memory region to clear
-   * \param [in] size Number of bytes to clear
-   */
   inline void bclear(void* mem, size_t size) {
-    #if defined(DXVK_ARCH_X86) && (defined(__GNUC__) || defined(__clang__) || defined(_MSC_VER))
+    #if defined(DXVK_ARCH_X86) && defined(__AVX2__)
+    auto zero = _mm256_setzero_si256();
+    #pragma nounroll
+    for (size_t i = 0; i < size; i += 64u) {
+      auto* ptr = reinterpret_cast<__m256i*>(reinterpret_cast<char*>(mem) + i);
+      _mm256_stream_si256(ptr + 0u, zero);
+      _mm256_stream_si256(ptr + 1u, zero);
+    }
+    _mm_sfence();
+    #elif defined(DXVK_ARCH_X86) && (defined(__GNUC__) || defined(__clang__) || defined(_MSC_VER))
     auto zero = _mm_setzero_si128();
-
-    #if defined(__clang__)
     #pragma nounroll
-    #elif defined(__GNUC__)
-    #pragma GCC unroll 0
-    #endif
     for (size_t i = 0; i < size; i += 64u) {
-      auto* ptr = reinterpret_cast<__m128i*>(mem) + i / sizeof(zero);
+      auto* ptr = reinterpret_cast<__m128i*>(reinterpret_cast<char*>(mem) + i);
       _mm_stream_si128(ptr + 0u, zero);
       _mm_stream_si128(ptr + 1u, zero);
       _mm_stream_si128(ptr + 2u, zero);
       _mm_stream_si128(ptr + 3u, zero);
     }
+    _mm_sfence();
     #else
     std::memset(mem, 0, size);
     #endif
   }
 
-
-  /**
-   * \brief Compares two aligned structs bit by bit
-   *
-   * \param [in] a First struct
-   * \param [in] b Second struct
-   * \returns \c true if the structs are equal
-   */
   template<typename T>
   bool bcmpeq(const T* a, const T* b) {
     static_assert(alignof(T) >= 16);
     #if defined(DXVK_ARCH_X86) && (defined(__GNUC__) || defined(__clang__) || defined(_MSC_VER))
-    auto ai = reinterpret_cast<const __m128i*>(a);
-    auto bi = reinterpret_cast<const __m128i*>(b);
-
-    size_t i = 0;
-
-    #if defined(__clang__)
-    #pragma nounroll
-    #elif defined(__GNUC__)
-    #pragma GCC unroll 0
+    const char* ap = reinterpret_cast<const char*>(a);
+    const char* bp = reinterpret_cast<const char*>(b);
+    size_t offset = 0;
+
+    #if defined(__AVX2__)
+    if constexpr (alignof(T) >= 32) {
+      #pragma nounroll
+      for ( ; offset + 64 <= sizeof(T); offset += 64) {
+        _mm_prefetch(ap + offset + 256, _MM_HINT_T0);
+        __m256i eq0 = _mm256_cmpeq_epi8(_mm256_load_si256(reinterpret_cast<const __m256i*>(ap + offset)),
+                                        _mm256_load_si256(reinterpret_cast<const __m256i*>(bp + offset)));
+        __m256i eq1 = _mm256_cmpeq_epi8(_mm256_load_si256(reinterpret_cast<const __m256i*>(ap + offset + 32)),
+                                        _mm256_load_si256(reinterpret_cast<const __m256i*>(bp + offset + 32)));
+        __m256i eq = _mm256_and_si256(eq0, eq1);
+        if (_mm256_movemask_epi8(eq) != 0xFFFFFFFF) {
+          return false;
+        }
+      }
+    }
     #endif
 
-    for ( ; i < 2 * (sizeof(T) / 32); i += 2) {
-      __m128i eq0 = _mm_cmpeq_epi8(
-        _mm_load_si128(ai + i),
-        _mm_load_si128(bi + i));
-      __m128i eq1 = _mm_cmpeq_epi8(
-        _mm_load_si128(ai + i + 1),
-        _mm_load_si128(bi + i + 1));
+    #pragma nounroll
+    for ( ; offset + 32 <= sizeof(T); offset += 32) {
+      _mm_prefetch(ap + offset + 128, _MM_HINT_T0);
+      __m128i eq0 = _mm_cmpeq_epi8(_mm_load_si128(reinterpret_cast<const __m128i*>(ap + offset)),
+                                   _mm_load_si128(reinterpret_cast<const __m128i*>(bp + offset)));
+      __m128i eq1 = _mm_cmpeq_epi8(_mm_load_si128(reinterpret_cast<const __m128i*>(ap + offset + 16)),
+                                   _mm_load_si128(reinterpret_cast<const __m128i*>(bp + offset + 16)));
       __m128i eq = _mm_and_si128(eq0, eq1);
-
-      int mask = _mm_movemask_epi8(eq);
-      if (mask != 0xFFFF)
+      if (_mm_movemask_epi8(eq) != 0xFFFF) {
         return false;
+      }
     }
 
-    for ( ; i < sizeof(T) / 16; i++) {
-      __m128i eq = _mm_cmpeq_epi8(
-        _mm_load_si128(ai + i),
-        _mm_load_si128(bi + i));
-
-      int mask = _mm_movemask_epi8(eq);
-      if (mask != 0xFFFF)
-        return false;
+    if (offset < sizeof(T)) {
+      return !std::memcmp(ap + offset, bp + offset, sizeof(T) - offset);
     }
 
     return true;
@@ -311,82 +270,90 @@ namespace dxvk::bit {
   public:
 
     constexpr bitset()
-      : m_dwords() {
+    : m_dwords() {
 
     }
 
     constexpr bool get(uint32_t idx) const {
+      if (unlikely(idx >= Bits)) {
+        return false;
+      }
       uint32_t dword = 0;
       uint32_t bit   = idx;
-
-      // Compiler doesn't remove this otherwise.
       if constexpr (Dwords > 1) {
         dword = idx / 32;
         bit   = idx % 32;
       }
-
       return m_dwords[dword] & (1u << bit);
     }
 
     constexpr void set(uint32_t idx, bool value) {
+      if (unlikely(idx >= Bits)) {
+        return;
+      }
       uint32_t dword = 0;
       uint32_t bit   = idx;
-
-      // Compiler doesn't remove this otherwise.
       if constexpr (Dwords > 1) {
         dword = idx / 32;
         bit   = idx % 32;
       }
-
-      if (value)
+      if (value) {
         m_dwords[dword] |= 1u << bit;
-      else
+      } else {
         m_dwords[dword] &= ~(1u << bit);
+      }
     }
 
     constexpr bool exchange(uint32_t idx, bool value) {
+      if (unlikely(idx >= Bits)) {
+        return value;
+      }
       bool oldValue = get(idx);
       set(idx, value);
       return oldValue;
     }
 
     constexpr void flip(uint32_t idx) {
+      if (unlikely(idx >= Bits)) {
+        return;
+      }
       uint32_t dword = 0;
       uint32_t bit   = idx;
-
-      // Compiler doesn't remove this otherwise.
       if constexpr (Dwords > 1) {
         dword = idx / 32;
         bit   = idx % 32;
       }
-
       m_dwords[dword] ^= 1u << bit;
     }
 
     constexpr void setAll() {
       if constexpr (Bits % 32 == 0) {
-        for (size_t i = 0; i < Dwords; i++)
-          m_dwords[i] = std::numeric_limits<uint32_t>::max();
-      }
-      else {
-        for (size_t i = 0; i < Dwords - 1; i++)
-          m_dwords[i] = std::numeric_limits<uint32_t>::max();
-
-        m_dwords[Dwords - 1] = (1u << (Bits % 32)) - 1;
+        for (size_t i = 0; i < Dwords; i++) {
+          m_dwords[i] = 0xFFFFFFFFu;
+        }
+      } else {
+        size_t i = 0;
+        for ( ; i < Dwords - 1; i++) {
+          m_dwords[i] = 0xFFFFFFFFu;
+        }
+        if constexpr (Dwords > 0) {
+          m_dwords[Dwords - 1] = (1ull << (Bits % 32)) - 1;
+        }
       }
     }
 
     constexpr void clearAll() {
-      for (size_t i = 0; i < Dwords; i++)
+      for (size_t i = 0; i < Dwords; i++) {
         m_dwords[i] = 0;
+      }
     }
 
     constexpr bool any() const {
       for (size_t i = 0; i < Dwords; i++) {
-        if (m_dwords[i] != 0)
+        if (m_dwords[i] != 0) {
           return true;
+        }
       }
-
       return false;
     }
 
@@ -394,11 +361,11 @@ namespace dxvk::bit {
       return m_dwords[idx];
     }
 
-    constexpr size_t bitCount() {
+    constexpr size_t bitCount() const {
       return Bits;
     }
 
-    constexpr size_t dwordCount() {
+    constexpr size_t dwordCount() const {
       return Dwords;
     }
 
@@ -407,14 +374,21 @@ namespace dxvk::bit {
     }
 
     constexpr void setN(uint32_t bits) {
+      if (unlikely(bits > Bits)) {
+        bits = Bits;
+      }
       uint32_t fullDwords = bits / 32;
       uint32_t offset = bits % 32;
 
-      for (size_t i = 0; i < fullDwords; i++)
-        m_dwords[i] = std::numeric_limits<uint32_t>::max();
-     
-      if (offset > 0)
+      for (size_t i = 0; i < Dwords; i++) {
+        m_dwords[i] = 0;
+      }
+      for (size_t i = 0; i < fullDwords; i++) {
+        m_dwords[i] = 0xFFFFFFFFu;
+      }
+      if (offset > 0 && fullDwords < Dwords) {
         m_dwords[fullDwords] = (1u << offset) - 1;
+      }
     }
 
   private:
@@ -427,35 +401,38 @@ namespace dxvk::bit {
   public:
 
     bool get(uint32_t idx) const {
+      if (unlikely(idx >= m_bitCount)) {
+        return false;
+      }
       uint32_t dword = idx / 32;
       uint32_t bit   = idx % 32;
-
       return m_dwords[dword] & (1u << bit);
     }
 
     void ensureSize(uint32_t bitCount) {
-      uint32_t dword = bitCount / 32;
-      if (unlikely(dword >= m_dwords.size())) {
-        m_dwords.resize(dword + 1);
+      if (bitCount <= m_bitCount) {
+        return;
       }
-      m_bitCount = std::max(m_bitCount, bitCount);
+      uint32_t dwordCount = (bitCount - 1) / 32 + 1;
+      if (dwordCount > m_dwords.size()) {
+        m_dwords.resize(dwordCount, 0);
+      }
+      m_bitCount = bitCount;
     }
 
     void set(uint32_t idx, bool value) {
       ensureSize(idx + 1);
-
-      uint32_t dword = 0;
-      uint32_t bit   = idx;
-
-      if (value)
+      uint32_t dword = idx / 32;
+      uint32_t bit   = idx % 32;
+      if (value) {
         m_dwords[dword] |= 1u << bit;
-      else
+      } else {
         m_dwords[dword] &= ~(1u << bit);
+      }
     }
 
     bool exchange(uint32_t idx, bool value) {
       ensureSize(idx + 1);
-
       bool oldValue = get(idx);
       set(idx, value);
       return oldValue;
@@ -463,37 +440,37 @@ namespace dxvk::bit {
 
     void flip(uint32_t idx) {
       ensureSize(idx + 1);
-
       uint32_t dword = idx / 32;
       uint32_t bit   = idx % 32;
-
       m_dwords[dword] ^= 1u << bit;
     }
 
     void setAll() {
-      if (m_bitCount % 32 == 0) {
-        for (size_t i = 0; i < m_dwords.size(); i++)
-          m_dwords[i] = std::numeric_limits<uint32_t>::max();
-      }
-      else {
-        for (size_t i = 0; i < m_dwords.size() - 1; i++)
-          m_dwords[i] = std::numeric_limits<uint32_t>::max();
-
-        m_dwords[m_dwords.size() - 1] = (1u << (m_bitCount % 32)) - 1;
+      if (m_dwords.empty()) {
+        return;
+      }
+      size_t lastDwordIdx = m_dwords.size() - 1;
+      std::memset(m_dwords.data(), 0xFF, lastDwordIdx * sizeof(uint32_t));
+      uint32_t remainderBits = m_bitCount % 32;
+      if (remainderBits == 0) {
+        m_dwords[lastDwordIdx] = 0xFFFFFFFFu;
+      } else {
+        m_dwords[lastDwordIdx] = (1ull << remainderBits) - 1;
       }
     }
 
     void clearAll() {
-      for (size_t i = 0; i < m_dwords.size(); i++)
-        m_dwords[i] = 0;
+      if (!m_dwords.empty()) {
+        std::memset(m_dwords.data(), 0, m_dwords.size() * sizeof(uint32_t));
+      }
     }
 
     bool any() const {
-      for (size_t i = 0; i < m_dwords.size(); i++) {
-        if (m_dwords[i] != 0)
+      for (uint32_t dword : m_dwords) {
+        if (dword != 0) {
           return true;
+        }
       }
-
       return false;
     }
 
@@ -515,15 +492,22 @@ namespace dxvk::bit {
 
     void setN(uint32_t bits) {
       ensureSize(bits);
-
+      if (bits == 0) {
+        clearAll();
+        return;
+      }
       uint32_t fullDwords = bits / 32;
       uint32_t offset = bits % 32;
-
-      for (size_t i = 0; i < fullDwords; i++)
-        m_dwords[i] = std::numeric_limits<uint32_t>::max();
-
-      if (offset > 0)
+      if (fullDwords > 0) {
+        std::memset(m_dwords.data(), 0xFF, fullDwords * sizeof(uint32_t));
+      }
+      if (offset > 0 && fullDwords < m_dwords.size()) {
         m_dwords[fullDwords] = (1u << offset) - 1;
+      }
+      size_t clearStart = fullDwords + (offset > 0 ? 1 : 0);
+      if (clearStart < m_dwords.size()) {
+        std::memset(&m_dwords[clearStart], 0, (m_dwords.size() - clearStart) * sizeof(uint32_t));
+      }
     }
 
   private:
@@ -547,7 +531,7 @@ namespace dxvk::bit {
       using reference = T;
 
       explicit iterator(T flags)
-        : m_mask(flags) { }
+      : m_mask(flags) { }
 
       iterator& operator ++ () {
         m_mask &= m_mask - 1;
@@ -574,10 +558,10 @@ namespace dxvk::bit {
     };
 
     BitMask()
-      : m_mask(0) { }
+    : m_mask(0) { }
 
     explicit BitMask(T n)
-      : m_mask(n) { }
+    : m_mask(n) { }
 
     iterator begin() {
       return iterator(m_mask);
@@ -607,25 +591,30 @@ namespace dxvk::bit {
    */
   template<typename T, int32_t I, int32_t F>
   T encodeFixed(float n) {
-    if (n != n)
+    static_assert(I + F <= int32_t(sizeof(T) * 8), "Fixed point format exceeds type size");
+    static_assert(I >= 0 && F >= 0, "Fixed point format requires non-negative bit counts");
+
+    if (unlikely(n != n)) {
       return 0u;
+    }
 
     n *= float(1u << F);
 
     if constexpr (std::is_signed_v<T>) {
-      n = std::max(n, -float(1u << (I + F - 1u)));
-      n = std::min(n,  float(1u << (I + F - 1u)) - 1.0f);
+      n = std::max(n, -float(1ull << (I + F - 1u)));
+      n = std::min(n,  float((1ull << (I + F - 1u)) - 1u));
       n += n < 0.0f ? -0.5f : 0.5f;
     } else {
       n = std::max(n, 0.0f);
-      n = std::min(n, float(1u << (I + F)) - 1.0f);
+      n = std::min(n, float((1ull << (I + F)) - 1u));
       n += 0.5f;
     }
 
     T result = T(n);
 
-    if constexpr (std::is_signed_v<T>)
-      result &= ((T(1u) << (I + F)) - 1u);
+    if constexpr (std::is_signed_v<T> && I + F < int32_t(sizeof(T) * 8)) {
+      result &= ((T(1) << (I + F)) - 1u);
+    }
 
     return result;
   }
@@ -642,9 +631,14 @@ namespace dxvk::bit {
    */
   template<typename T, int32_t I, int32_t F>
   float decodeFixed(T n) {
-    // Sign-extend as necessary
-    if constexpr (std::is_signed_v<T>)
-      n -= (n & (T(1u) << (I + F - 1u))) << 1u;
+    static_assert(I + F <= int32_t(sizeof(T) * 8), "Fixed point format exceeds type size");
+    static_assert(I >= 0 && F >= 0, "Fixed point format requires non-negative bit counts");
+
+    if constexpr (std::is_signed_v<T> && I + F < int32_t(sizeof(T) * 8)) {
+      if (n & (T(1) << (I + F - 1))) {
+        n |= ~((T(1) << (I + F)) - 1u);
+      }
+    }
 
     return float(n) / float(1u << F);
   }
@@ -653,7 +647,7 @@ namespace dxvk::bit {
   /**
    * \brief Inserts one null bit after each bit
    */
-  inline uint32_t split2(uint32_t c) {
+  DXVK_FORCE_INLINE uint32_t split2(uint32_t c) {
     c = (c ^ (c << 8u)) & 0x00ff00ffu;
     c = (c ^ (c << 4u)) & 0x0f0f0f0fu;
     c = (c ^ (c << 2u)) & 0x33333333u;
@@ -665,7 +659,7 @@ namespace dxvk::bit {
   /**
    * \brief Inserts two null bits after each bit
    */
-  inline uint64_t split3(uint64_t c) {
+  DXVK_FORCE_INLINE uint64_t split3(uint64_t c) {
     c = (c | c << 32u) & 0x001f00000000ffffull;
     c = (c | c << 16u) & 0x001f0000ff0000ffull;
     c = (c | c <<  8u) & 0x100f00f00f00f00full;
@@ -683,8 +677,14 @@ namespace dxvk::bit {
    * \param [in] y Y coordinate
    * \returns Morton code of x and y
    */
-  inline uint32_t interleave(uint16_t x, uint16_t y) {
+  DXVK_FORCE_INLINE uint32_t interleave(uint16_t x, uint16_t y) {
+    #if defined(__BMI2__) && (defined(__GNUC__) || defined(__clang__))
+    uint32_t x_spread = _pdep_u32(x, 0x55555555u);
+    uint32_t y_spread = _pdep_u32(y, 0xAAAAAAAAu);
+    return x_spread | y_spread;
+    #else
     return split2(x) | (split2(y) << 1u);
+    #endif
   }
 
 
@@ -693,8 +693,15 @@ namespace dxvk::bit {
    *
    * All three numbers must fit into 16 bits.
    */
-  inline uint64_t interleave(uint16_t x, uint16_t y, uint16_t z) {
+  DXVK_FORCE_INLINE uint64_t interleave(uint16_t x, uint16_t y, uint16_t z) {
+    #if defined(__BMI2__) && defined(DXVK_ARCH_X86_64) && (defined(__GNUC__) || defined(__clang__))
+    uint64_t x_spread = _pdep_u64(x, 0x1249249249249249ull);
+    uint64_t y_spread = _pdep_u64(y, 0x2492492492492492ull);
+    uint64_t z_spread = _pdep_u64(z, 0x4924924924924924ull);
+    return x_spread | y_spread | z_spread;
+    #else
     return split3(x) | (split3(y) << 1u) | (split3(z) << 2u);
+    #endif
   }
 
 
@@ -710,7 +717,6 @@ namespace dxvk::bit {
     uint16_t c;
 
     explicit operator uint64_t () const {
-      // GCC generates worse code if we promote to uint64 directly
       uint32_t lo = uint32_t(a) | (uint32_t(b) << 16);
       return uint64_t(lo) | (uint64_t(c) << 32);
     }
