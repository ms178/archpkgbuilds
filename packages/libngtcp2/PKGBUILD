pkgname=libngtcp2
pkgver=1.11.0
pkgrel=2.1
pkgdesc='Implementation of IETF QUIC protocol'
url='https://github.com/ngtcp2/ngtcp2'
arch=('x86_64')
license=('MIT')
depends=(
  'glibc'
  'gnutls'
)
makedepends=(
  'brotli'
  'git'
  'llvm'
  'clang'
  'openssl'
  'libnghttp3'
)
provides=(
  'libngtcp2.so'
  'libngtcp2_crypto_gnutls.so'
)
source=("${pkgname}-${pkgver}.tar.gz::https://github.com/ngtcp2/ngtcp2/archive/refs/tags/v${pkgver}.tar.gz")
sha256sums=('053ada7e22c3735f4f0c7df48aeede11fdb7e64bf5f0db03e56cf215aeb6dc04')
b2sums=('594400d4ab7a796c4372519418823e215c832fd0f6ae9040e11d7d16e334fe6dbe7b0e049388b4baea50ade0e53e355fba62438a5bac6f2f2dc46eabb97b49df')

prepare() {
  cd ngtcp2-${pkgver}
  autoreconf -i

  # Create an advanced PGO workload script with comprehensive QUIC protocol coverage
  cat > "$srcdir/pgo-workload.sh" << 'EOF'
#!/bin/bash
set -e

WORKDIR="$1"
LIB_PATH="$2"
TARGET_BINARY="$3"
cd "$WORKDIR"

echo "===== Advanced QUIC Protocol PGO Workload ====="
echo "Working directory: $WORKDIR"
echo "Library path: $LIB_PATH"

# Verify the library exists
if [ ! -f "$LIB_PATH" ]; then
  echo "ERROR: Library $LIB_PATH does not exist!"
  exit 1
fi

# Create test environment
mkdir -p "$WORKDIR/test-data"
chmod -R u+rwX "$WORKDIR/test-data"
cd "$WORKDIR/test-data"

# Function to sanitize strings for use in filenames
sanitize_filename() {
  local input="$1"
  # Replace characters that are problematic in filenames or as command-line args
  echo "$input" | sed -e 's|https\?://||g' -e 's|[:/\.-]|_|g'
}

# Function to run curl with proper instrumentation and timeout
curl_h3() {
  local timeout_val="$1"
  local profraw_prefix="$2"
  shift 2

  # Sanitize the prefix to avoid any problematic characters in filenames
  profraw_prefix="profile_$(sanitize_filename "$profraw_prefix")"

  local profraw_file="${profraw_prefix}_$(date +%s)_$$.profraw"
  echo "➤ curl --http3 (profile: $profraw_file) $*"
  LLVM_PROFILE_FILE="$profraw_file" LD_PRELOAD="$LIB_PATH" timeout "$timeout_val" curl --http3-only --insecure "$@" 2>/dev/null ||
    echo "  [Exit code: $?]"
}

# Check if a site supports HTTP/3
check_h3_support() {
  local site="$1"
  local profraw_prefix="$2"
  echo "Checking HTTP/3 support for $site..."
  if curl_h3 5 "$profraw_prefix" -sI "$site" | grep -i "HTTP/3" > /dev/null; then
    echo "✓ $site supports HTTP/3"
    return 0
  else
    echo "✗ $site does not appear to support HTTP/3"
    return 1
  fi
}

# List of HTTP/3 endpoints to test with
declare -a h3_sites=(
  "https://nghttp2.org/"
  "https://cloudflare-quic.com/"
  "https://quic.rocks/"
  "https://http3.is/"
  "https://www.cloudflare.com/"
)

# Find working HTTP/3 endpoints
working_sites=()
for site in "${h3_sites[@]}"; do
  if check_h3_support "$site" "check-h3"; then
    working_sites+=("$site")
  fi
done

if [ ${#working_sites[@]} -eq 0 ]; then
  echo "WARNING: No working HTTP/3 sites found! Using nghttp2.org anyway."
  working_sites=("https://nghttp2.org/")
fi

echo "Using working HTTP/3 sites: ${working_sites[*]}"

echo -e "\n===== 1. Basic Connection Establishment ====="
# Exercise connection establishment multiple times to profile handshake code
for site in "${working_sites[@]}"; do
  site_name=$(sanitize_filename "$site")
  for i in {1..3}; do
    echo "Connection test $i to $site"
    curl_h3 5 "conn_${site_name}_${i}" -s "$site" -o /dev/null
  done
done

echo -e "\n===== 2. Stream Multiplexing Tests ====="
# Test with different numbers of concurrent streams
for streams in 2 4 8; do
  echo "Testing with $streams concurrent streams"
  urls=""
  for ((i=1; i<=streams; i++)); do
    urls+=" ${working_sites[0]}httpbin/get"
  done
  curl_h3 15 "stream_${streams}" -s -Z $urls -o /dev/null
done

echo -e "\n===== 3. Flow Control Tests ====="
# Test with different rate limits to exercise flow control
for rate in 50k 200k 1M; do
  echo "Testing with rate limit: $rate"
  curl_h3 15 "flow_rate_$(sanitize_filename "$rate")" -s --limit-rate $rate "${working_sites[0]}httpbin/bytes/200000" -o /dev/null
done

echo -e "\n===== 4. Different HTTP Methods ====="
# Test different HTTP methods to exercise various QUIC frames
curl_h3 5 "method_head" -s -I "${working_sites[0]}" -o /dev/null

# Test POST with different payload sizes
for size in 10 100 1000; do
  json_data=$(dd if=/dev/urandom bs=1 count=$size 2>/dev/null | base64)
  echo "POST with ~$size bytes payload"
  echo "{\"data\":\"$json_data\"}" |
    curl_h3 5 "method_post_${size}" -s -H "Content-Type: application/json" -d @- "${working_sites[0]}httpbin/post" -o /dev/null
done

echo -e "\n===== 5. Session Resumption and 0-RTT Tests ====="
# Test session resumption with cookies (simulates 0-RTT behavior)
for i in {1..3}; do
  echo "Session test $i"
  curl_h3 5 "session_init_${i}" -s --cookie-jar "$WORKDIR/test-data/session.txt" "${working_sites[0]}httpbin/get" -o /dev/null
  curl_h3 5 "session_resume_${i}" -s --cookie "$WORKDIR/test-data/session.txt" "${working_sites[0]}httpbin/get" -o /dev/null
done

echo -e "\n===== 6. Varied Content Types and Sizes ====="
# Test different content types and sizes
endpoints=(
  "httpbin/get"
  "httpbin/headers"
  "httpbin/ip"
  "httpbin/user-agent"
  "httpbin/bytes/1000"
  "httpbin/bytes/10000"
)

for endpoint in "${endpoints[@]}"; do
  endpoint_name=$(sanitize_filename "$endpoint")
  echo "Fetching ${working_sites[0]}$endpoint"
  curl_h3 5 "content_${endpoint_name}" -s "${working_sites[0]}$endpoint" -o /dev/null
done

echo -e "\n===== 7. Connection Migration Simulation ====="
# Simulate connection migration by creating and closing connections
for i in {1..3}; do
  echo "Migration test $i"
  # First connection
  curl_h3 5 "migration_first_${i}" -s "${working_sites[0]}httpbin/get" -o /dev/null
  # Force reconnection (simulates new network path)
  sleep 1
  curl_h3 5 "migration_second_${i}" -s "${working_sites[0]}httpbin/get" -o /dev/null
done

echo -e "\n===== 8. Error Recovery Tests ====="
# Test with retries to exercise connection recovery
for i in {1..3}; do
  echo "Retry test $i"
  curl_h3 10 "retry_${i}" -s --retry 2 --retry-delay 1 "${working_sites[0]}httpbin/get" -o /dev/null
done

# Test with non-existent paths (controlled error cases)
curl_h3 5 "error_nonexistent_1" -s "${working_sites[0]}nonexistent-path-123" -o /dev/null
curl_h3 5 "error_nonexistent_2" -s "${working_sites[0]}another-nonexistent-path" -o /dev/null

echo -e "\n===== 9. Long-lived Connection Tests ====="
# Test long-lived connections with intermittent activity
echo "Starting long-lived connection test with pauses..."
for i in {1..3}; do
  curl_h3 5 "long_lived_first_${i}" -s "${working_sites[0]}httpbin/get" -o /dev/null
  echo "  Pause $i..."
  sleep 2
  curl_h3 5 "long_lived_second_${i}" -s "${working_sites[0]}httpbin/get" -o /dev/null
done

echo -e "\n===== 10. Advanced Protocol Features ====="
# Test with special header conditions
curl_h3 5 "header_special" -s -H "X-Test-Header: value1, value2" "${working_sites[0]}httpbin/headers" -o /dev/null

# Test with large headers to exercise HPACK compression (reduced size to avoid crashes)
big_header_value=$(dd if=/dev/urandom bs=1 count=200 2>/dev/null | base64)
curl_h3 5 "header_large" -s -H "X-Large-Header: $big_header_value" "${working_sites[0]}httpbin/headers" -o /dev/null

echo -e "\n===== Stress Testing ====="
# Run rapid consecutive requests to stress connection handling
echo "Running stress test (10 rapid requests)..."
for i in {1..10}; do
  curl_h3 5 "stress_${i}" -s "${working_sites[0]}httpbin/get?id=$i" -o /dev/null &
  if [ $((i % 3)) -eq 0 ]; then
    # Give a short pause every 3 requests to avoid overloading
    sleep 0.5
  fi
done
wait # Wait for all background requests to complete

echo -e "\n===== Comprehensive Mixed Workload ====="
# Run a mix of different request types simultaneously
echo "Running mixed workload..."
curl_h3 10 "mixed_get" -s "${working_sites[0]}httpbin/get" -o /dev/null &
curl_h3 10 "mixed_head" -s -I "${working_sites[0]}" -o /dev/null &
echo '{"test":"data"}' | curl_h3 10 "mixed_post" -s -d @- "${working_sites[0]}httpbin/post" -o /dev/null &
curl_h3 10 "mixed_rate" -s --limit-rate 100k "${working_sites[0]}httpbin/bytes/50000" -o /dev/null &
wait

echo -e "\n===== Verifying Library Usage ====="
# Verify our instrumented library is being used
LD_PRELOAD="$LIB_PATH" ldd $(which curl) 2>/dev/null | grep -q "$LIB_PATH" &&
  echo "✓ Successfully verified instrumented library is being loaded" ||
  echo "✗ Could not verify library preloading"

echo -e "\n===== Comprehensive PGO Workload Complete ====="
# Print some stats
echo "Tests completed with $(find "$WORKDIR/test-data" -type f | wc -l) files generated"

# Ensure profile data is written with correct permissions
echo "Ensuring profile data permissions..."
chmod -R u+rwX "$WORKDIR/test-data"
EOF

  chmod +x "$srcdir/pgo-workload.sh"

  # Generate self-signed certificates for QUIC testing
  cd "$srcdir"

  # Create certificates with backward compatibility
  if openssl version | grep -q "OpenSSL 1.1.1\|OpenSSL 3"; then
    openssl req -new -x509 -days 365 -nodes \
      -out server.crt -keyout server.key \
      -subj "/CN=localhost" \
      -addext "subjectAltName = DNS:localhost"
  else
    cat > "$srcdir/openssl.cnf" << EOF
[req]
distinguished_name = req_distinguished_name
x509_extensions = v3_req
prompt = no

[req_distinguished_name]
CN = localhost

[v3_req]
subjectAltName = DNS:localhost
EOF

    openssl req -new -x509 -days 365 -nodes \
      -out server.crt -keyout server.key \
      -config "$srcdir/openssl.cnf"

    rm "$srcdir/openssl.cnf"
  fi

  # Ensure certificate files have correct permissions
  chmod -R u+rwX "$srcdir/server.crt" "$srcdir/server.key"

  # Return to the source directory
  cd "$srcdir/ngtcp2-${pkgver}"
}

build() {
  cd ngtcp2-${pkgver}

  # Define build directories and PGO paths
  build_dir="$srcdir/build"
  pgo_dir="$srcdir/pgo-data"

  # Ensure directories are created with proper permissions
  mkdir -p "$build_dir" "$pgo_dir/standard" "$pgo_dir/cs"
  chmod -R u+rwX "$build_dir" "$pgo_dir"

  # Store original flags
  orig_cflags="$CFLAGS"
  orig_cxxflags="$CXXFLAGS"
  orig_ldflags="$LDFLAGS"

  # Use clang for compilation
  export CC=clang
  export CXX=clang++

  # Common instrumentation flags
  local _common_instrument_flags=" -g3 -fno-omit-frame-pointer -mllvm -vp-counters-per-site=10 -mllvm -runtime-counter-relocation -mllvm -enable-value-profiling"

  ######################################################################
  # STAGE 1: Standard PGO - Build with instrumentation
  ######################################################################
  msg2 "STAGE 1: Building with standard PGO instrumentation"

  # Set up stage 1 build directory
  stage1_dir="$build_dir/stage1"
  mkdir -p "$stage1_dir"
  chmod -R u+rwX "$stage1_dir"
  cd "$stage1_dir"

  # Standard PGO instrumentation flags
  export CFLAGS="${orig_cflags} -fprofile-generate=${pgo_dir}/standard ${_common_instrument_flags}"
  export CXXFLAGS="${orig_cxxflags} -fprofile-generate=${pgo_dir}/standard ${_common_instrument_flags}"
  export LDFLAGS="${orig_ldflags} -fprofile-generate=${pgo_dir}/standard"

  # Configure and build stage 1
  "$srcdir/ngtcp2-${pkgver}/configure" \
    --prefix=/usr \
    --with-libbrotlienc \
    --with-libbrotlidec \
    --with-gnutls \
    --enable-debug \
    --enable-lib-only

  make clean
  make -j4

  # Find the library for workload testing, including hidden directories
  standard_lib=$(find "$stage1_dir" -type f -path "*/.libs/libngtcp2.so*" -not -name "*.la" | head -n 1)
  if [ -z "$standard_lib" ]; then
    error "Stage 1: Could not find libngtcp2.so library"
    exit 1
  fi

  msg2 "Stage 1: Found library at $standard_lib"

  # Verify standard PGO instrumentation
  msg2 "Verifying standard PGO instrumentation:"
  if file "$standard_lib" | grep -q "not stripped"; then
    echo "✓ Library contains debug info as expected"
  else
    echo "✗ WARNING: Library appears to be stripped!"
  fi

  # Run workload to generate standard PGO data
  msg2 "Stage 1: Running workload for standard PGO"
  mkdir -p "$srcdir/workload1"
  chmod -R u+rwX "$srcdir/workload1"
  cp "$srcdir/server.crt" "$srcdir/server.key" "$srcdir/workload1/"

  if ! "$srcdir/pgo-workload.sh" "$srcdir/workload1" "$standard_lib" ""; then
    error "Stage 1: Workload execution failed"
    exit 1
  fi

  # Move generated profraw files to standard directory
  msg2 "Stage 1: Moving generated profraw files"
  find "$srcdir/workload1/test-data" -type f -name "*.profraw" -exec mv {} "${pgo_dir}/standard/" \;

  # Verify profile data was generated
  profile_count=$(find "${pgo_dir}/standard" -type f -name "*.profraw" | wc -l)
  msg2 "Stage 1: Generated $profile_count profile files"

  if [ "$profile_count" -eq 0 ]; then
    error "Stage 1: No profile data was generated!"
    exit 1
  fi

  # Merge standard profile data
  msg2 "Stage 1: Merging standard profile data"
  if ! llvm-profdata merge -output="${pgo_dir}/standard.profdata" "${pgo_dir}/standard/"*.profraw; then
    error "Stage 1: Failed to merge standard profile data"
    exit 1
  fi

  ######################################################################
  # STAGE 2: Context-Sensitive PGO - Build with CS instrumentation
  ######################################################################
  msg2 "STAGE 2: Building with context-sensitive PGO instrumentation"

  # Set up stage 2 build directory
  stage2_dir="$build_dir/stage2"
  mkdir -p "$stage2_dir"
  chmod -R u+rwX "$stage2_dir"
  cd "$stage2_dir"

  # Context-sensitive PGO instrumentation flags
  export CFLAGS="${orig_cflags} -fprofile-use=${pgo_dir}/standard.profdata -fcs-profile-generate=${pgo_dir}/cs ${_common_instrument_flags}"
  export CXXFLAGS="${orig_cxxflags} -fprofile-use=${pgo_dir}/standard.profdata -fcs-profile-generate=${pgo_dir}/cs ${_common_instrument_flags}"
  export LDFLAGS="${orig_ldflags} -fprofile-use=${pgo_dir}/standard.profdata -fcs-profile-generate=${pgo_dir}/cs"

  # Configure and build stage 2
  "$srcdir/ngtcp2-${pkgver}/configure" \
    --prefix=/usr \
    --with-libbrotlienc \
    --with-libbrotlidec \
    --with-gnutls \
    --enable-debug \
    --enable-lib-only

  make clean
  make -j4

  # Find the library for workload testing, including hidden directories
  cs_lib=$(find "$stage2_dir" -type f -path "*/.libs/libngtcp2.so*" -not -name "*.la" | head -n 1)
  if [ -z "$cs_lib" ]; then
    error "Stage 2: Could not find libngtcp2.so library"
    exit 1
  fi

  msg2 "Stage 2: Found library at $cs_lib"

  # Verify CS-PGO instrumentation
  msg2 "Verifying CS-PGO instrumentation:"
  if file "$cs_lib" | grep -q "not stripped"; then
    echo "✓ CS-PGO Library contains debug info as expected"
  else
    echo "✗ WARNING: CS-PGO Library appears to be stripped!"
  fi

  # Run workload to generate context-sensitive PGO data
  msg2 "Stage 2: Running workload for context-sensitive PGO"
  mkdir -p "$srcdir/workload2"
  chmod -R u+rwX "$srcdir/workload2"
  cp "$srcdir/server.crt" "$srcdir/server.key" "$srcdir/workload2/"

  if ! "$srcdir/pgo-workload.sh" "$srcdir/workload2" "$cs_lib" ""; then
    error "Stage 2: Workload execution failed"
    exit 1
  fi

  # Move generated profraw files to cs directory
  msg2 "Stage 2: Moving generated profraw files"
  find "$srcdir/workload2/test-data" -type f -name "*.profraw" -exec mv {} "${pgo_dir}/cs/" \;

  # Verify CS profile data was generated
  cs_profile_count=$(find "${pgo_dir}/cs" -type f -name "*.profraw" | wc -l)
  msg2 "Stage 2: Generated $cs_profile_count context-sensitive profile files"

  # Merge profile data with strict error handling
  if [ "$cs_profile_count" -gt 0 ]; then
    msg2 "Stage 2: Merging context-sensitive profile data"
    if ! llvm-profdata merge -output="${pgo_dir}/cs.profdata" "${pgo_dir}/cs/"*.profraw; then
      error "Stage 2: Failed to merge context-sensitive profile data"
      exit 1
    fi

    msg2 "Stage 2: Merging standard and context-sensitive profile data"
    if ! llvm-profdata merge -output="${pgo_dir}/merged.profdata" "${pgo_dir}/cs.profdata" "${pgo_dir}/standard.profdata"; then
      error "Stage 2: Failed to merge standard and context-sensitive profile data"
      exit 1
    fi
  else
    msg2 "Stage 2: No context-sensitive profile data generated, using standard profile data"
    cp "${pgo_dir}/standard.profdata" "${pgo_dir}/merged.profdata"
  fi

  ######################################################################
  # STAGE 3: Final Build - Use merged profile data
  ######################################################################
  msg2 "STAGE 3: Building final optimized version with PGO data"

  # Set up stage 3 build directory
  stage3_dir="$build_dir/stage3"
  mkdir -p "$stage3_dir"
  chmod -R u+rwX "$stage3_dir"
  cd "$stage3_dir"

  # Final build using merged profile data
  export CFLAGS="${orig_cflags} -fprofile-use=${pgo_dir}/merged.profdata"
  export CXXFLAGS="${orig_cxxflags} -fprofile-use=${pgo_dir}/merged.profdata"
  export LDFLAGS="${orig_ldflags} -fprofile-use=${pgo_dir}/merged.profdata"

  # Configure and build final version
  "$srcdir/ngtcp2-${pkgver}/configure" \
    --prefix=/usr \
    --with-libbrotlienc \
    --with-libbrotlidec \
    --with-gnutls \
    --enable-lib-only

  make clean
  make -j4

  # Save the stage3 directory path for packaging
  echo "$stage3_dir" > "$srcdir/final_build_path"
}

package() {
  # Use the final optimized build
  final_build_path=$(cat "$srcdir/final_build_path")

  # Install from the final build directory
  cd "$final_build_path"
  make DESTDIR="${pkgdir}" install

  # Install license and verification script
  install -Dm644 "$srcdir/ngtcp2-${pkgver}/COPYING" -t "${pkgdir}/usr/share/licenses/${pkgname}"
}
