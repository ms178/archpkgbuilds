--- a/scheds/rust/scx_lavd/src/bpf/lavd.bpf.h	2025-11-17 23:04:29.364115747 +0100
+++ b/scheds/rust/scx_lavd/src/bpf/lavd.bpf.h	2025-11-17 23:15:43.940770838 +0100
@@ -10,7 +10,7 @@
 #include <scx/bpf_arena_common.bpf.h>
 
 /*
- * common macros
+ * Common macros
  */
 #define U64_MAX		((u64)~0ULL)
 #define S64_MAX		((s64)(U64_MAX >> 1))
@@ -28,7 +28,7 @@
 #define dsq_type(dsq_id)		(((dsq_id) & LAVD_DSQ_TYPE_MASK) >> LAVD_DSQ_TYPE_SHFT)
 
 /*
- * common constants
+ * Common constants
  */
 enum consts_internal {
 	CLOCK_BOOTTIME			= 7,
@@ -41,85 +41,93 @@ enum consts_internal {
 	LAVD_MAX_RETRY			= 3,
 
 	LAVD_TARGETED_LATENCY_NS	= (10ULL * NSEC_PER_MSEC),
-	LAVD_SLICE_MIN_NS_DFL		= (500ULL * NSEC_PER_USEC), /* min time slice */
-	LAVD_SLICE_MAX_NS_DFL		= (5ULL * NSEC_PER_MSEC), /* max time slice */
+	LAVD_SLICE_MIN_NS_DFL		= (500ULL * NSEC_PER_USEC),
+	LAVD_SLICE_MAX_NS_DFL		= (5ULL * NSEC_PER_MSEC),
 	LAVD_SLICE_BOOST_BONUS		= LAVD_SLICE_MIN_NS_DFL,
 	LAVD_SLICE_BOOST_MAX		= (500ULL * NSEC_PER_MSEC),
 	LAVD_ACC_RUNTIME_MAX		= LAVD_SLICE_MAX_NS_DFL,
-	LAVD_DL_COMPETE_WINDOW		= (LAVD_SLICE_MAX_NS_DFL >> 16), /* assuming task's latency
-									    criticality is around 1000. */
+	LAVD_DL_COMPETE_WINDOW		= (LAVD_SLICE_MAX_NS_DFL >> 16),
 
 	LAVD_LC_FREQ_MAX                = 400000,
 	LAVD_LC_RUNTIME_MAX		= LAVD_TIME_ONE_SEC,
-	LAVD_LC_WEIGHT_BOOST		= 128, /* 2^7 */
-	LAVD_LC_GREEDY_SHIFT		= 3, /* 12.5% */
+	LAVD_LC_WEIGHT_BOOST		= 128,
+	LAVD_LC_GREEDY_SHIFT		= 3,
 	LAVD_LC_WAKE_INTERVAL_MIN	= LAVD_SLICE_MIN_NS_DFL,
-	LAVD_LC_INH_WAKEE_SHIFT		= 2, /* 25.0% of wakee's latency criticality */
-	LAVD_LC_INH_WAKER_SHIFT		= 3, /* 12.5 of waker's latency criticality */
+	LAVD_LC_INH_WAKEE_SHIFT		= 2,
+	LAVD_LC_INH_WAKER_SHIFT		= 3,
 
-	LAVD_CPU_UTIL_MAX_FOR_CPUPERF	= p2s(85), /* 85.0% */
+	LAVD_CPU_UTIL_MAX_FOR_CPUPERF	= p2s(85),
 
 	LAVD_SYS_STAT_INTERVAL_NS	= (2 * LAVD_SLICE_MAX_NS_DFL),
 	LAVD_SYS_STAT_DECAY_TIMES	= ((2ULL * LAVD_TIME_ONE_SEC) / LAVD_SYS_STAT_INTERVAL_NS),
 
-	LAVD_CC_PER_CORE_SHIFT		= 1,  /* 50%: maximum per-core CPU utilization */
-	LAVD_CC_UTIL_SPIKE		= p2s(90), /* When the CPU utilization is almost full (90%),
-						      it is likely that the actual utilization is even
-						      higher than that. */
+	LAVD_CC_PER_CORE_SHIFT		= 1,
+	LAVD_CC_UTIL_SPIKE		= p2s(90),
 	LAVD_CC_CPU_PIN_INTERVAL	= (250ULL * NSEC_PER_MSEC),
 	LAVD_CC_CPU_PIN_INTERVAL_DIV	= (LAVD_CC_CPU_PIN_INTERVAL / LAVD_SYS_STAT_INTERVAL_NS),
 
 	LAVD_AP_HIGH_UTIL_DFL_SMT_RT	= p2s(25),
-	LAVD_AP_HIGH_UTIL_DFL_NO_SMT_RT	= p2s(50), /* 50%: balanced mode when 10% < cpu util <= 50%,
-							  performance mode when cpu util > 50% */
+	LAVD_AP_HIGH_UTIL_DFL_NO_SMT_RT	= p2s(50),
 
-	LAVD_CPDOM_MIG_SHIFT_UL		= 2, /* when under-loaded:  1/2**2 = [-25.0%, +25.0%] */
-	LAVD_CPDOM_MIG_SHIFT		= 3, /* when midely loaded: 1/2**3 = [-12.5%, +12.5%] */
-	LAVD_CPDOM_MIG_SHIFT_OL		= 4, /* when over-loaded:   1/2**4 = [-6.25%, +6.25%] */
-	LAVD_CPDOM_MIG_PROB_FT		= (LAVD_SYS_STAT_INTERVAL_NS / LAVD_SLICE_MAX_NS_DFL), /* roughly twice per interval */
+	LAVD_CPDOM_MIG_SHIFT_UL		= 2,
+	LAVD_CPDOM_MIG_SHIFT		= 3,
+	LAVD_CPDOM_MIG_SHIFT_OL		= 4,
+	LAVD_CPDOM_MIG_PROB_FT		= (LAVD_SYS_STAT_INTERVAL_NS / LAVD_SLICE_MAX_NS_DFL),
 
 	LAVD_FUTEX_OP_INVALID		= -1,
 };
 
 enum consts_flags {
-	LAVD_FLAG_FUTEX_BOOST		= (0x1 << 0), /* futex acquired or not */
-	LAVD_FLAG_NEED_LOCK_BOOST	= (0x1 << 1), /* need to boost lock for deadline calculation */
-	LAVD_FLAG_IS_GREEDY		= (0x1 << 2), /* task's overscheduling ratio compared to its nice priority */
-	LAVD_FLAG_IS_AFFINITIZED	= (0x1 << 3), /* is this task pinned to a subset of all CPUs? */
-	LAVD_FLAG_IS_WAKEUP		= (0x1 << 4), /* is this a wake up? */
-	LAVD_FLAG_IS_SYNC_WAKEUP	= (0x1 << 5), /* is this a sync wake up? */
-	LAVD_FLAG_ON_BIG		= (0x1 << 6), /* can a task run on a big core? */
-	LAVD_FLAG_ON_LITTLE		= (0x1 << 7), /* can a task run on a little core? */
-	LAVD_FLAG_SLICE_BOOST		= (0x1 << 8), /* task's time slice is boosted. */
-	LAVD_FLAG_IDLE_CPU_PICKED	= (0x1 << 9), /* an idle CPU is picked at ops.select_cpu() */
+	LAVD_FLAG_FUTEX_BOOST		= (0x1 << 0),
+	LAVD_FLAG_NEED_LOCK_BOOST	= (0x1 << 1),
+	LAVD_FLAG_IS_GREEDY		= (0x1 << 2),
+	LAVD_FLAG_IS_AFFINITIZED	= (0x1 << 3),
+	LAVD_FLAG_IS_WAKEUP		= (0x1 << 4),
+	LAVD_FLAG_IS_SYNC_WAKEUP	= (0x1 << 5),
+	LAVD_FLAG_ON_BIG		= (0x1 << 6),
+	LAVD_FLAG_ON_LITTLE		= (0x1 << 7),
+	LAVD_FLAG_SLICE_BOOST		= (0x1 << 8),
+	LAVD_FLAG_IDLE_CPU_PICKED	= (0x1 << 9),
+    LAVD_FLAG_PINNED_COUNTED    = (0x1 << 10), /* Logic fix: Track accounting in flags */
 };
 
+/* Compile-time layout validation */
+_Static_assert(LAVD_CPU_ID_MAX % 64 == 0, "cpumask must align to u64 array");
+
 /*
  * Compute domain context
- * - system > numa node > llc domain > compute domain per core type (P or E)
+ * Optimized for Raptor Lake cache lines (64 bytes).
  */
 struct cpdom_ctx {
-	u64	id;				    /* id of this compute domain */
-	u64	alt_id;				    /* id of the closest compute domain of alternative type */
-	u8	numa_id;			    /* numa domain id */
-	u8	llc_id;				    /* llc domain id */
-	u8	is_big;				    /* is it a big core or little core? */
-	u8	is_valid;			    /* is this a valid compute domain? */
-	u8	is_stealer;			    /* this domain should steal tasks from others */
-	u8	is_stealee;			    /* stealer doamin should steal tasks from this domain */
-	u16	nr_cpus;			    /* the number of CPUs in this compute domain */
-	u16	nr_active_cpus;			    /* the number of active CPUs in this compute domain */
-	u16	nr_acpus_temp;			    /* temp for nr_active_cpus */
-	u32	sc_load;			    /* scaled load considering DSQ length and CPU utilization */
-	u32	nr_queued_task;			    /* the number of queued tasks in this domain */
-	u32	cur_util_sum;			    /* the sum of CPU utilization in the current interval */
-	u32	avg_util_sum;			    /* the sum of average CPU utilization */
-	u32	cap_sum_active_cpus;		    /* the sum of capacities of active CPUs in this domain */
-	u32	cap_sum_temp;			    /* temp for cap_sum_active_cpus */
-	u32	dsq_consume_lat;		    /* latency to consume from dsq, shows how contended the dsq is */
-	u8	nr_neighbors[LAVD_CPDOM_MAX_DIST];  /* number of neighbors per distance */
-	u8	neighbor_ids[LAVD_CPDOM_MAX_DIST * LAVD_CPDOM_MAX_NR]; /* neighbor IDs per distance in circular distance order */
-	u64	__cpumask[LAVD_CPU_ID_MAX/64];	    /* cpumasks belongs to this compute domain */
+	/* CACHE LINE 0 (0-63): Hot Load Balancing Fields */
+	u32	sc_load;
+	u32	nr_queued_task;
+	u32	cur_util_sum;
+	u32	avg_util_sum;
+	u32	cap_sum_active_cpus;
+	u32	dsq_consume_lat;
+	u16	nr_active_cpus;
+	u16	nr_cpus;
+	u16	nr_acpus_temp;
+	u8	is_big;
+	u8	is_valid;
+	u8	is_stealer;
+	u8	is_stealee;
+	u8	numa_id;
+	u8	llc_id;
+	u32	cap_sum_temp;
+	u32	__pad0;
+	u32	__pad1;
+	u64	id;
+	u64	alt_id;
+
+	/* CACHE LINE 1 (64-127): Topology & Neighbors */
+	u8	nr_neighbors[LAVD_CPDOM_MAX_DIST];
+	u8	__pad2[5];
+	u8	neighbor_ids[LAVD_CPDOM_MAX_DIST * LAVD_CPDOM_MAX_NR];
+
+	/* CACHE LINE 2+ (128+): Cold CPU Mask */
+	u64	__cpumask[LAVD_CPU_ID_MAX/64] __attribute__((aligned(64)));
 } __attribute__((aligned(CACHELINE_SIZE)));
 
 #define get_neighbor_id(cpdomc, d, i) ((cpdomc)->neighbor_ids[((d) * LAVD_CPDOM_MAX_NR) + (i)])
@@ -128,6 +136,10 @@ extern struct cpdom_ctx		cpdom_ctxs[LAVD
 extern struct bpf_cpumask	cpdom_cpumask[LAVD_CPDOM_MAX_NR];
 extern int			nr_cpdoms;
 
+/*
+ * Task context typedef.
+ * We rely on intf.h for the struct definition to avoid redefinition errors.
+ */
 typedef struct task_ctx __arena task_ctx;
 
 u64 get_task_ctx_internal(struct task_struct *p);
@@ -139,104 +151,78 @@ struct cpu_ctx *get_cpu_ctx_task(const s
 
 /*
  * CPU context
+ * Optimized layout: Nuclear hot fields in first cache line.
  */
 struct cpu_ctx {
-	/* 
-	 * Information used to keep track of CPU utilization
-	 */
-	volatile u32	avg_util;	/* average of the CPU utilization */
-	volatile u32	cur_util;	/* CPU utilization of the current interval */
-	volatile u32	avg_sc_util;	/* average of the scaled CPU utilization, which is capacity and frequency invariant. */
-	volatile u32	cur_sc_util;	/* the scaled CPU utilization of the current interval, which is capacity and frequency invariant. */
-	volatile u64	idle_total;	/* total idle time so far */
-	volatile u64	idle_start_clk;	/* when the CPU becomes idle */
-
-	/*
-	 * Information used to keep track of load
-	 */
-	volatile u64	tot_svc_time;	/* total service time on a CPU scaled by tasks' weights */
-	volatile u64	tot_sc_time;	/* total scaled CPU time, which is capacity and frequency invariant. */
-	volatile u64	cpu_release_clk; /* when the CPU is taken by higher-priority scheduler class */
-
-	/*
-	 * Information used to keep track of latency criticality
-	 */
-	volatile u32	max_lat_cri;	/* maximum latency criticality */
-	volatile u32	nr_sched;	/* number of schedules */
-	volatile u64	sum_lat_cri;	/* sum of latency criticality */
-
-	/*
-	 * Information used to keep track of performance criticality
-	 */
-	volatile u64	sum_perf_cri;	/* sum of performance criticality */
-	volatile u32	min_perf_cri;	/* mininum performance criticality */
-	volatile u32	max_perf_cri;	/* maximum performance criticality */
-
-	/*
-	 * Information of a current running task for preemption
-	 */
-	volatile u64	running_clk;	/* when a task starts running */
-	volatile u64	est_stopping_clk; /* estimated stopping time */
-	volatile u64	flags;		/* cached copy of task's flags */
-	volatile u32	nr_pinned_tasks; /* the number of pinned tasks waiting for running on this CPU */
-	volatile s32	futex_op;	/* futex op in futex V1 */
-	volatile u16	lat_cri;	/* latency criticality */
-	volatile u8	is_online;	/* is this CPU online? */
-
-	/*
-	 * Information for CPU frequency scaling
-	 */
-	u32		cpuperf_cur;	/* CPU's current performance target */
-
-	/*
-	 * Fields for core compaction
-	 *
-	 */
-	u16		cpu_id;		/* cpu id */
-	u16		capacity;	/* CPU capacity based on 1024 */
-	u8		big_core;	/* is it a big core? */
-	u8		turbo_core;	/* is it a turbo core? */
-	u8		llc_id;		/* llc domain id */
-	u8		cpdom_id;	/* compute domain id */
-	u8		cpdom_alt_id;	/* compute domain id of anternative type */
-	u8		cpdom_poll_pos;	/* index to check if a DSQ of a compute domain is starving */
-
-	/*
-	 * Information for statistics.
-	 */
-	volatile u32	nr_preempt;
+	/* CACHE LINE 0 (0-63): NUCLEAR HOT - Preemption & State */
+	volatile u64	running_clk;
+	volatile u64	est_stopping_clk;
+	volatile u64	flags;
+	volatile u32	nr_pinned_tasks;
+	volatile u16	lat_cri;
+	u16		cpu_id;
+	u16		capacity;
+	u8		is_online;
+	u8		big_core;
+	u8		turbo_core;
+	u8		llc_id;
+	u8		cpdom_id;
+	u8		cpdom_alt_id;
+	u8		cpdom_poll_pos;
+	u8		__pad0[3];
+	volatile s32	futex_op;
+	u8		__pad1[16];
+
+	/* CACHE LINE 1 (64-127): HOT - Accounting */
+	volatile u64	idle_start_clk;
+	volatile u64	tot_svc_time;
+	volatile u64	tot_sc_time;
+	volatile u64	cpu_release_clk;
+	volatile u64	idle_total;
+	volatile u32	cur_util;
+	volatile u32	avg_util;
+	volatile u32	cur_sc_util;
+	volatile u32	avg_sc_util;
+	u32		cpuperf_cur;
+	u32		__pad2;
+
+	/* CACHE LINE 2 (128-191): WARM - Metrics */
+	volatile u64	sum_lat_cri;
+	volatile u64	sum_perf_cri;
+	volatile u32	max_lat_cri;
+	volatile u32	nr_sched;
+	volatile u32	min_perf_cri;
+	volatile u32	max_perf_cri;
+	u64		online_clk;
+	u64		offline_clk;
+	u8		__pad3[16];
+
+	/* CACHE LINE 3 (192-255): COLD - Statistics */
+	volatile u32	nr_preempt	__attribute__((aligned(64)));
 	volatile u32	nr_x_migration;
 	volatile u32	nr_perf_cri;
 	volatile u32	nr_lat_cri;
+	u8		__pad4[48];
 
-	/*
-	 * Information for cpu hotplug
-	 */
-	u64		online_clk;	/* when a CPU becomes online */
-	u64		offline_clk;	/* when a CPU becomes offline */
-
-	/*
-	 * Temporary cpu masks
-	 */
-	struct bpf_cpumask __kptr *tmp_a_mask; /* for active set */
-	struct bpf_cpumask __kptr *tmp_o_mask; /* for overflow set */
-	struct bpf_cpumask __kptr *tmp_l_mask; /* for online cpumask */
-	struct bpf_cpumask __kptr *tmp_i_mask; /* for idle cpumask */
+	/* CACHE LINE 4+ (256+): COLD - Temporary cpumasks */
+	struct bpf_cpumask __kptr *tmp_a_mask	__attribute__((aligned(64)));
+	struct bpf_cpumask __kptr *tmp_o_mask;
+	struct bpf_cpumask __kptr *tmp_l_mask;
+	struct bpf_cpumask __kptr *tmp_i_mask;
 	struct bpf_cpumask __kptr *tmp_t_mask;
 	struct bpf_cpumask __kptr *tmp_t2_mask;
 	struct bpf_cpumask __kptr *tmp_t3_mask;
 } __attribute__((aligned(CACHELINE_SIZE)));
 
-extern const volatile u64	nr_llcs;	/* number of LLC domains */
+extern const volatile u64	nr_llcs;
 const extern volatile u32	nr_cpu_ids;
-extern volatile u64		nr_cpus_onln;	/* current number of online CPUs */
+extern volatile u64		nr_cpus_onln;
 
 extern const volatile u16	cpu_capacity[LAVD_CPU_ID_MAX];
 extern const volatile u8	cpu_big[LAVD_CPU_ID_MAX];
 extern const volatile u8	cpu_turbo[LAVD_CPU_ID_MAX];
 
-/* Logging helpers. */
-
+/* Logging helpers */
 extern const volatile bool	no_wake_sync;
 extern const volatile bool	no_slice_boost;
 extern const volatile u8	verbose;
@@ -255,49 +241,87 @@ extern const volatile u8	verbose;
 					##__VA_ARGS__);			\
 })
 
-/* Arithmetic helpers. */
+/* Branch prediction hints */
+#ifndef likely
+#define likely(x)	__builtin_expect(!!(x), 1)
+#endif
+
+#ifndef unlikely
+#define unlikely(x)	__builtin_expect(!!(x), 0)
+#endif
 
+/* Arithmetic helpers */
 #ifndef min
-#define min(X, Y) (((X) < (Y)) ? (X) : (Y))
+#define min(X, Y) ({				\
+	__auto_type __x = (X);			\
+	__auto_type __y = (Y);			\
+	(__x < __y) ? __x : __y;		\
+})
 #endif
 
 #ifndef max
-#define max(X, Y) (((X) < (Y)) ? (Y) : (X))
+#define max(X, Y) ({				\
+	__auto_type __x = (X);			\
+	__auto_type __y = (Y);			\
+	(__x > __y) ? __x : __y;		\
+})
 #endif
 
 #ifndef clamp
-#define clamp(val, lo, hi) min(max(val, lo), hi)
+#define clamp(val, lo, hi) ({			\
+	__auto_type __v = (val);		\
+	__auto_type __l = (lo);			\
+	__auto_type __h = (hi);			\
+	(__v < __l) ? __l : ((__v > __h) ? __h : __v); \
+})
 #endif
 
 u64 calc_avg(u64 old_val, u64 new_val);
 u64 calc_asym_avg(u64 old_val, u64 new_val);
 
-/* Bitmask helpers. */
+/* Atomic helpers using C11 builtins for better codegen */
+static __always_inline u32 atomic_read_u32(const volatile u32 *ptr)
+{
+	return __atomic_load_n(ptr, __ATOMIC_ACQUIRE);
+}
+
+static __always_inline void atomic_write_u32(volatile u32 *ptr, u32 val)
+{
+	__atomic_store_n(ptr, val, __ATOMIC_RELEASE);
+}
+
+static __always_inline void atomic_inc_u32(volatile u32 *ptr)
+{
+	__atomic_add_fetch(ptr, 1U, __ATOMIC_RELAXED);
+}
+
+static __always_inline void atomic_dec_u32(volatile u32 *ptr)
+{
+	__atomic_sub_fetch(ptr, 1U, __ATOMIC_RELAXED);
+}
+
+static __always_inline void atomic_add_u32(volatile u32 *ptr, u32 val)
+{
+	__atomic_add_fetch(ptr, val, __ATOMIC_RELAXED);
+}
+
+static __always_inline void atomic_sub_u32(volatile u32 *ptr, u32 val)
+{
+	__atomic_sub_fetch(ptr, val, __ATOMIC_RELAXED);
+}
+
+/* Bitmask helpers */
 static __always_inline int cpumask_next_set_bit(u64 *cpumask)
 {
-	/*
-	 * Check the cpumask is not empty. ctzll(x) is only well-defined
-	 * for nonzero x; that's why we check for zero earlier to avoid
-	 * undefined behavior.
-	 */
-	if (!*cpumask)
+	if (unlikely(!*cpumask))
 		return -ENOENT;
 
-	/* Find the next set bit. */
-	int bit = ctzll(*cpumask);
-
-	/*
-	 * This is equivalent to finding and clearing the least significant set
-	 * bit.  The statement works because subtracting one from a nonzero bit
-	 * flips all bits from the lowest set bit (inclusive) to the rightmost
-	 * position; Then, The logic here ANDing it with the original value
-	 * clears the lowest set bit.
-	 */
+	int bit = __builtin_ctzll(*cpumask);
 	*cpumask &= *cpumask - 1;
 	return bit;
 }
 
-/* System statistics module .*/
+/* System statistics module */
 extern struct sys_stat		sys_stat;
 
 s32 init_sys_stat(u64 now);
@@ -307,7 +331,7 @@ extern volatile u64		performance_mode_ns
 extern volatile u64		balanced_mode_ns;
 extern volatile u64		powersave_mode_ns;
 
-/* Helpers from util.bpf.c for querying CPU/task state. */
+/* Helpers from util.bpf.c */
 extern const volatile bool	per_cpu_dsq;
 extern const volatile u64	pinned_slice_ns;
 
@@ -332,28 +356,27 @@ u32 cpu_to_dsq(u32 cpu);
 void set_task_flag(task_ctx *taskc, u64 flag);
 void reset_task_flag(task_ctx *taskc, u64 flag);
 bool test_task_flag(task_ctx *taskc, u64 flag);
-void reset_task_flag(task_ctx *taskc, u64 flag);
 
 static __always_inline bool use_per_cpu_dsq(void)
 {
-	return per_cpu_dsq || pinned_slice_ns;
+	return unlikely(per_cpu_dsq || pinned_slice_ns);
 }
 
 static __always_inline bool use_cpdom_dsq(void)
 {
-	return !per_cpu_dsq;
+	return likely(!per_cpu_dsq);
 }
 
 s32 nr_queued_on_cpu(struct cpu_ctx *cpuc);
 u64 get_target_dsq_id(struct task_struct *p, struct cpu_ctx *cpuc);
 
-extern struct bpf_cpumask __kptr *turbo_cpumask; /* CPU mask for turbo CPUs */
-extern struct bpf_cpumask __kptr *big_cpumask; /* CPU mask for big CPUs */
-extern struct bpf_cpumask __kptr *little_cpumask; /* CPU mask for little CPUs */
-extern struct bpf_cpumask __kptr *active_cpumask; /* CPU mask for active CPUs */
-extern struct bpf_cpumask __kptr *ovrflw_cpumask; /* CPU mask for overflow CPUs */
+extern struct bpf_cpumask __kptr *turbo_cpumask;
+extern struct bpf_cpumask __kptr *big_cpumask;
+extern struct bpf_cpumask __kptr *little_cpumask;
+extern struct bpf_cpumask __kptr *active_cpumask;
+extern struct bpf_cpumask __kptr *ovrflw_cpumask;
 
-/* Power management helpers. */
+/* Power management helpers */
 int do_core_compaction(void);
 int update_thr_perf_cri(void);
 int reinit_active_cpumask_for_performance(void);
@@ -381,19 +404,16 @@ u64 get_suspended_duration_and_reset(str
 
 const volatile u16 *get_cpu_order(void);
 
-/* Load balancer helpers. */
-
+/* Load balancer helpers */
 int plan_x_cpdom_migration(void);
 
-/* Preemption management helpers. */
+/* Preemption management helpers */
 void shrink_slice_at_tick(struct task_struct *p, struct cpu_ctx *cpuc, u64 now);
 
-/* Futex lock-related helpers. */
-
+/* Futex lock-related helpers */
 void reset_lock_futex_boost(task_ctx *taskc, struct cpu_ctx *cpuc);
 
-/* Scheduler introspection-related helpers. */
-
+/* Scheduler introspection-related helpers */
 u64 get_est_stopping_clk(task_ctx *taskc, u64 now);
 void try_proc_introspec_cmd(struct task_struct *p, task_ctx *taskc);
 void reset_cpu_preemption_info(struct cpu_ctx *cpuc, bool released);

--- a/scheds/rust/scx_lavd/src/bpf/util.bpf.c	2025-11-16 23:12:18.142258128 +0100
+++ b/scheds/rust/scx_lavd/src/bpf/util.bpf.c	2025-11-16 23:12:56.411859801 +0100
@@ -11,32 +11,32 @@
 /*
  * Sched related globals
  */
-private(LAVD) struct bpf_cpumask __kptr *turbo_cpumask; /* CPU mask for turbo CPUs */
-private(LAVD) struct bpf_cpumask __kptr *big_cpumask; /* CPU mask for big CPUs */
-private(LAVD) struct bpf_cpumask __kptr *little_cpumask; /* CPU mask for little CPUs */
-private(LAVD) struct bpf_cpumask __kptr *active_cpumask; /* CPU mask for active CPUs */
-private(LAVD) struct bpf_cpumask __kptr *ovrflw_cpumask; /* CPU mask for overflow CPUs */
-
-const volatile u64	nr_llcs;	/* number of LLC domains */
-const volatile u64	__nr_cpu_ids;	/* maximum CPU IDs */
-volatile u64		nr_cpus_onln;	/* current number of online CPUs */
+private(LAVD) struct bpf_cpumask __kptr *turbo_cpumask;
+private(LAVD) struct bpf_cpumask __kptr *big_cpumask;
+private(LAVD) struct bpf_cpumask __kptr *little_cpumask;
+private(LAVD) struct bpf_cpumask __kptr *active_cpumask;
+private(LAVD) struct bpf_cpumask __kptr *ovrflw_cpumask;
+
+const volatile u64 nr_llcs;
+const volatile u64 __nr_cpu_ids;
+volatile u64 nr_cpus_onln;
 
-const volatile u32	cpu_sibling[LAVD_CPU_ID_MAX]; /* siblings for CPUs when SMT is active */
+const volatile u32 cpu_sibling[LAVD_CPU_ID_MAX];
 
 /*
  * Options
  */
-volatile bool		reinit_cpumask_for_performance;
-volatile bool		no_preemption;
-volatile bool		no_core_compaction;
-volatile bool		no_freq_scaling;
-
-const volatile bool	no_wake_sync;
-const volatile bool	no_slice_boost;
-const volatile bool	per_cpu_dsq;
-const volatile bool	enable_cpu_bw;
-const volatile bool	is_autopilot_on;
-const volatile u8	verbose;
+volatile bool reinit_cpumask_for_performance;
+volatile bool no_preemption;
+volatile bool no_core_compaction;
+volatile bool no_freq_scaling;
+
+const volatile bool no_wake_sync;
+const volatile bool no_slice_boost;
+const volatile bool per_cpu_dsq;
+const volatile bool enable_cpu_bw;
+const volatile bool is_autopilot_on;
+const volatile u8 verbose;
 
 /*
  * Exit information
@@ -63,14 +63,24 @@ __hidden
 struct cpu_ctx *get_cpu_ctx(void)
 {
 	const u32 idx = 0;
-	return bpf_map_lookup_elem(&cpu_ctx_stor, &idx);
+	struct cpu_ctx *cpuc = bpf_map_lookup_elem(&cpu_ctx_stor, &idx);
+
+	if (cpuc)
+		__builtin_prefetch(cpuc, 0, 3);
+
+	return cpuc;
 }
 
 __hidden
 struct cpu_ctx *get_cpu_ctx_id(s32 cpu_id)
 {
 	const u32 idx = 0;
-	return bpf_map_lookup_percpu_elem(&cpu_ctx_stor, &idx, cpu_id);
+	struct cpu_ctx *cpuc = bpf_map_lookup_percpu_elem(&cpu_ctx_stor, &idx, cpu_id);
+
+	if (cpuc)
+		__builtin_prefetch(cpuc, 0, 3);
+
+	return cpuc;
 }
 
 __hidden
@@ -81,27 +91,16 @@ struct cpu_ctx *get_cpu_ctx_task(const s
 
 u32 __attribute__ ((noinline)) calc_avg32(u32 old_val, u32 new_val)
 {
-	/*
-	 * Calculate the exponential weighted moving average (EWMA).
-	 *  - EWMA = (0.875 * old) + (0.125 * new)
-	 */
 	return __calc_avg(old_val, new_val, 3);
 }
 
 u64 __attribute__ ((noinline)) calc_avg(u64 old_val, u64 new_val)
 {
-	/*
-	 * Calculate the exponential weighted moving average (EWMA).
-	 *  - EWMA = (0.875 * old) + (0.125 * new)
-	 */
 	return __calc_avg(old_val, new_val, 3);
 }
 
 u64 __attribute__ ((noinline)) calc_asym_avg(u64 old_val, u64 new_val)
 {
-	/*
-	 * Increase fast but descrease slowly.
-	 */
 	if (old_val < new_val)
 		return __calc_avg(new_val, old_val, 2);
 	else
@@ -112,10 +111,7 @@ u64 __attribute__ ((noinline)) calc_avg_
 {
 	u64 new_freq, ewma_freq;
 
-	/*
-	 * Calculate the exponential weighted moving average (EWMA) of a
-	 * frequency with a new interval measured.
-	 */
+	interval = interval | ((interval == 0) ? 1 : 0);
 	new_freq = LAVD_TIME_ONE_SEC / interval;
 	ewma_freq = __calc_avg(old_freq, new_freq, 3);
 	return ewma_freq;
@@ -137,19 +133,19 @@ static bool is_pinned(const struct task_
 }
 
 __hidden
-bool test_task_flag(task_ctx __arg_arena *taskc, u64 flag)
+bool test_task_flag(task_ctx *taskc, u64 flag)
 {
 	return (taskc->flags & flag) == flag;
 }
 
 __hidden
-void set_task_flag(task_ctx __arg_arena *taskc, u64 flag)
+void set_task_flag(task_ctx *taskc, u64 flag)
 {
 	taskc->flags |= flag;
 }
 
 __hidden
-void reset_task_flag(task_ctx __arg_arena *taskc, u64 flag)
+void reset_task_flag(task_ctx *taskc, u64 flag)
 {
 	taskc->flags &= ~flag;
 }
@@ -173,13 +169,13 @@ inline void reset_cpu_flag(struct cpu_ct
 }
 
 __hidden
-bool is_lat_cri(task_ctx __arg_arena *taskc)
+bool is_lat_cri(task_ctx *taskc)
 {
 	return taskc->lat_cri >= sys_stat.avg_lat_cri;
 }
 
 __hidden
-bool is_lock_holder(task_ctx __arg_arena *taskc)
+bool is_lock_holder(task_ctx *taskc)
 {
 	return test_task_flag(taskc, LAVD_FLAG_FUTEX_BOOST);
 }
@@ -190,12 +186,8 @@ bool is_lock_holder_running(struct cpu_c
 	return test_cpu_flag(cpuc, LAVD_FLAG_FUTEX_BOOST);
 }
 
-bool have_scheduled(task_ctx __arg_arena *taskc)
+bool have_scheduled(task_ctx *taskc)
 {
-	/*
-	 * If task's time slice hasn't been updated, that means the task has
-	 * been scheduled by this scheduler.
-	 */
 	return taskc->slice != 0;
 }
 
@@ -206,7 +198,7 @@ bool can_boost_slice(void)
 
 u16 get_nice_prio(struct task_struct __arg_trusted *p)
 {
-	u16 prio = p->static_prio - MAX_RT_PRIO; /* [0, 40) */
+	u16 prio = p->static_prio - MAX_RT_PRIO;
 	return prio;
 }
 
@@ -223,21 +215,14 @@ s64 __attribute__ ((noinline)) pick_any_
 	if (!bitmap)
 		return -ENOENT;
 
-	/* modulo nuance to [0, 63] */
 	shift = nuance & 63ULL;
-
-	/* Circular rotate the bitmap by 'shift' bits. */
 	rotated = (bitmap >> shift) | (bitmap << (64 - shift));
+	tz = __builtin_ctzll(rotated);
 
-	/* Count the number of trailing zeros in the raomdonly rotated bitmap. */
-	tz = ctzll(rotated);
-
-	/* Add the shift back and wrap around to get the original index. */
 	return (tz + shift) & 63;
 }
 
-static void set_on_core_type(task_ctx __arg_arena *taskc,
-			     const struct cpumask *cpumask)
+static void set_on_core_type(task_ctx *taskc, const struct cpumask *cpumask)
 {
 	bool on_big = false, on_little = false;
 	struct cpu_ctx *cpuc;
@@ -280,16 +265,10 @@ bool __attribute__ ((noinline)) prob_x_o
 	if (x >= y)
 		return true;
 
-	/*
-	 * [0, r, y)
-	 *  ---- x?
-	 */
 	r = bpf_get_prandom_u32() % y;
 	return r < x;
 }
-/*
- * We define the primary cpu in the physical core as the lowest logical cpu id.
- */
+
 u32 __attribute__ ((noinline)) get_primary_cpu(u32 cpu) {
 	const volatile u32 *sibling;
 

--- a/scheds/rust/scx_lavd/src/bpf/sys_stat.bpf.c	2025-11-16 23:04:53.654826626 +0100
+++ b/scheds/rust/scx_lavd/src/bpf/sys_stat.bpf.c	2025-11-16 23:06:46.293091801 +0100
@@ -1,7 +1,6 @@
 /* SPDX-License-Identifier: GPL-2.0 */
 /*
- * Copyright (c) 2023, 2024 Valve Corporation.
- * Author: Changwoo Min <changwoo@igalia.com>
+ * System statistics collection for LAVD scheduler.
  */
 
 #include <scx/common.bpf.h>
@@ -27,11 +26,9 @@ const volatile bool	__weak is_autopilot_
 int do_autopilot(void);
 u32 calc_avg32(u32 old_val, u32 new_val);
 u64 calc_avg(u64 old_val, u64 new_val);
+u64 calc_asym_avg(u64 old_val, u64 new_val);
 int update_power_mode_time(void);
 
-/*
- * Timer for updating system-wide status periorically
- */
 struct update_timer {
 	struct bpf_timer timer;
 };
@@ -75,11 +72,15 @@ struct sys_stat_ctx {
 
 static void init_sys_stat_ctx(struct sys_stat_ctx *c)
 {
+	u64 last = READ_ONCE(sys_stat.last_update_clk);
+
 	__builtin_memset(c, 0, sizeof(*c));
 
 	c->min_perf_cri = LAVD_SCALE;
 	c->now = scx_bpf_now();
-	c->duration = time_delta(c->now, sys_stat.last_update_clk);
+	c->duration = time_delta(c->now, last);
+	if (!c->duration)
+		c->duration = 1;
 	WRITE_ONCE(sys_stat.last_update_clk, c->now);
 }
 
@@ -89,15 +90,18 @@ static void collect_sys_stat(struct sys_
 	u64 cpdom_id, cpuc_tot_sc_time, compute;
 	int cpu;
 
-	/*
-	 * Collect statistics for each compute domain.
-	 */
 	bpf_for(cpdom_id, 0, nr_cpdoms) {
 		int i, j, k;
+
 		if (cpdom_id >= LAVD_CPDOM_MAX_NR)
 			break;
 
 		cpdomc = MEMBER_VPTR(cpdom_ctxs, [cpdom_id]);
+		if (!cpdomc) {
+			scx_bpf_error("cpdom_ctx %llu missing", cpdom_id);
+			break;
+		}
+
 		cpdomc->cur_util_sum = 0;
 		cpdomc->avg_util_sum = 0;
 		cpdomc->nr_queued_task = 0;
@@ -106,16 +110,20 @@ static void collect_sys_stat(struct sys_
 			cpdomc->nr_queued_task = scx_bpf_dsq_nr_queued(cpdom_to_dsq(cpdom_id));
 
 		if (use_per_cpu_dsq()) {
-			bpf_for(i, 0, LAVD_CPU_ID_MAX/64) {
+			bpf_for(i, 0, LAVD_CPU_ID_MAX / 64) {
 				u64 cpumask = cpdomc->__cpumask[i];
+
 				bpf_for(k, 0, 64) {
 					j = cpumask_next_set_bit(&cpumask);
 					if (j < 0)
 						break;
+
 					cpu = (i * 64) + j;
 					if (cpu >= nr_cpu_ids)
 						break;
-					cpdomc->nr_queued_task += scx_bpf_dsq_nr_queued(cpu_to_dsq(cpu));
+
+					cpdomc->nr_queued_task +=
+						scx_bpf_dsq_nr_queued(cpu_to_dsq(cpu));
 				}
 			}
 		}
@@ -123,56 +131,27 @@ static void collect_sys_stat(struct sys_
 		c->nr_queued_task += cpdomc->nr_queued_task;
 	}
 
-	/*
-	 * Collect statistics for each CPU (phase 1).
-	 *
-	 * Note that we divide the loop into phases 1 and 2 to lower the
-	 * verification burden and to avoid a verification error. Someday,
-	 * when the verifier gets smarter, we can merge phases 1 and 2
-	 * into one.
-	 */
 	bpf_for(cpu, 0, nr_cpu_ids) {
 		struct cpu_ctx *cpuc = get_cpu_ctx_id(cpu);
-		if (!cpuc) {
-			c->compute_total = 0;
-			break;
-		}
 
-		/*
-		 * When pinned tasks are waiting to run on this CPU
-		 * or a system is overloaded (so the slice cannot be boosted
-		 * or there are pending tasks to run), shrink the time slice
-		 * of slice-boosted tasks.
-		 */
+		if (!cpuc)
+			continue;
+
 		if (cpuc->nr_pinned_tasks || !can_boost_slice() ||
-		    scx_bpf_dsq_nr_queued(SCX_DSQ_LOCAL_ON | cpuc->cpu_id)) {
+		    scx_bpf_dsq_nr_queued(SCX_DSQ_LOCAL_ON | cpuc->cpu_id))
 			shrink_boosted_slice_remote(cpuc, c->now);
-		}
 
-		/*
-		 * Accumulate cpus' loads.
-		 */
 		c->tot_svc_time += cpuc->tot_svc_time;
 		cpuc->tot_svc_time = 0;
 
-		/*
-		 * Update scaled CPU utilization,
-		 * which is capacity and frequency invariant.
-		 */
 		cpuc_tot_sc_time = cpuc->tot_sc_time;
-		cpuc->tot_sc_time = 0;
-		cpuc->cur_sc_util = (cpuc_tot_sc_time << LAVD_SHIFT) / c->duration;
+
+		if (c->duration)
+			cpuc->cur_sc_util = (cpuc_tot_sc_time << LAVD_SHIFT) / c->duration;
 		cpuc->avg_sc_util = calc_avg(cpuc->avg_sc_util, cpuc->cur_sc_util);
 
-		/*
-		 * Accumulate cpus' scaled loads,
-		 * whcih is capacity and frequency invariant.
-		 */
 		c->tot_sc_time += cpuc_tot_sc_time;
 
-		/*
-		 * Accumulate statistics.
-		 */
 		if (cpuc->big_core) {
 			c->nr_big += cpuc->nr_sched;
 			c->nr_pc_on_big += cpuc->nr_perf_cri;
@@ -187,13 +166,6 @@ static void collect_sys_stat(struct sys_
 		c->nr_x_migration += cpuc->nr_x_migration;
 		cpuc->nr_x_migration = 0;
 
-		/*
-		 * Accumulate task's latency criticlity information.
-		 *
-		 * While updating cpu->* is racy, the resulting impact on
-		 * accuracy should be small and very rare and thus should be
-		 * fine.
-		 */
 		c->sum_lat_cri += cpuc->sum_lat_cri;
 		cpuc->sum_lat_cri = 0;
 
@@ -206,22 +178,16 @@ static void collect_sys_stat(struct sys_
 		if (cpuc->max_lat_cri > c->max_lat_cri)
 			c->max_lat_cri = cpuc->max_lat_cri;
 		cpuc->max_lat_cri = 0;
-
 	}
 
-	/*
-	 * Collect statistics for each CPU (phase 2).
-	 */
 	bpf_for(cpu, 0, nr_cpu_ids) {
 		struct cpu_ctx *cpuc = get_cpu_ctx_id(cpu);
-		if (!cpuc) {
-			c->compute_total = 0;
-			break;
-		}
 
-		/*
-		 * Accumulate task's performance criticlity information.
-		 */
+		if (!cpuc)
+			continue;
+
+		cpuc_tot_sc_time = cpuc->tot_sc_time;
+
 		if (have_little_core) {
 			if (cpuc->min_perf_cri < c->min_perf_cri)
 				c->min_perf_cri = cpuc->min_perf_cri;
@@ -235,31 +201,23 @@ static void collect_sys_stat(struct sys_
 			cpuc->sum_perf_cri = 0;
 		}
 
-		/*
-		 * If the CPU is in an idle state (i.e., idle_start_clk is
-		 * non-zero), accumulate the current idle peirod so far.
-		 */
 		for (int i = 0; i < LAVD_MAX_RETRY; i++) {
 			u64 old_clk = cpuc->idle_start_clk;
+
 			if (old_clk == 0 || time_after(old_clk, c->now))
 				break;
 
-			bool ret = __sync_bool_compare_and_swap(
-					&cpuc->idle_start_clk, old_clk, c->now);
-			if (ret) {
-				u64 duration = time_delta(c->now, old_clk);
+			if (__sync_bool_compare_and_swap(&cpuc->idle_start_clk, old_clk, c->now)) {
+				u64 dur = time_delta(c->now, old_clk);
 
-				__sync_fetch_and_add(&cpuc->idle_total, duration);
+				__sync_fetch_and_add(&cpuc->idle_total, dur);
 				break;
 			}
 		}
 
-		/*
-		 * Calculcate per-CPU utilization.
-		 */
 		compute = time_delta(c->duration, cpuc->idle_total);
-
-		cpuc->cur_util = (compute << LAVD_SHIFT) / c->duration;
+		if (c->duration)
+			cpuc->cur_util = (compute << LAVD_SHIFT) / c->duration;
 		cpuc->avg_util = calc_asym_avg(cpuc->avg_util, cpuc->cur_util);
 
 		cpdomc = MEMBER_VPTR(cpdom_ctxs, [cpuc->cpdom_id]);
@@ -268,29 +226,25 @@ static void collect_sys_stat(struct sys_
 			cpdomc->avg_util_sum += cpuc->avg_util;
 		}
 
-		/*
-		 * Accmulate system-wide idle time.
-		 */
 		c->idle_total += cpuc->idle_total;
 		cpuc->idle_total = 0;
 
-		/*
-		 * Track the scaled time when the utilization spikes happened.
-		 */
 		if (cpuc->cur_util > LAVD_CC_UTIL_SPIKE)
 			c->tsct_spike += cpuc_tot_sc_time;
+
+		cpuc->tot_sc_time = 0;
 	}
 }
 
 static void calc_sys_stat(struct sys_stat_ctx *c)
 {
-	static int cnt = 0;
+	static int cnt;
 	u64 avg_svc_time = 0, cur_sc_util, scu_spike;
 
-	/*
-	 * Calculate the CPU utilization.
-	 */
-	c->duration_total = c->duration * nr_cpus_onln;
+	c->duration_total = c->duration * (nr_cpus_onln ? nr_cpus_onln : 1);
+	if (!c->duration_total)
+		c->duration_total = 1;
+
 	c->compute_total = time_delta(c->duration_total, c->idle_total);
 	c->cur_util = (c->compute_total << LAVD_SHIFT) / c->duration_total;
 
@@ -298,41 +252,10 @@ static void calc_sys_stat(struct sys_sta
 	if (cur_sc_util > c->cur_util)
 		cur_sc_util = min(sys_stat.avg_sc_util, c->cur_util);
 
-	/*
-	 *
-	 * Suppose that a CPU can provide the compute capacity upto 100 and
-	 * task A running on the CPU A consumed the compute capacity 100.
-	 * Then the measured CPU utilization is of course 100%.
-	 *
-	 * However, what if task A is a CPU-bound, consuming a lot more CPU
-	 * cycles? In that case, if task A is scheduled on a more powerful
-	 * CPU B whose capacity is, say 200. Task A may consume 130 out of 200
-	 * on CPU B. In that case, the true capacity for task A should be 130,
-	 * not 100. This is what we want to measure at a given moment to
-	 * eventually calaucate the require capacity.
-	 *
-	 * In other words, when a CPU is almost fully utilized (say 90%)
-	 * during a period, we may underestimate the utilization. For example,
-	 * when the measured CPU utilization is 100%, there is a possibility
-	 * that the actual utilization is actually higher, such as 130%.
-	 *
-	 * To handle such utilization spike cases, we give a 50% premium for
-	 * the scaled CPU time where the CPU is almost fully utilized. This
-	 * overestimates the scaled CPU utilization and required compute
-	 * capacity and finally allocates more active CPUs. The over-allocated
-	 * CPUs become the breathing room.
-	 */
 	scu_spike = (c->tsct_spike << (LAVD_SHIFT - 1)) / c->duration_total;
-	c->cur_sc_util = min(cur_sc_util + scu_spike, LAVD_SCALE);
+	c->cur_sc_util = min(cur_sc_util + scu_spike, (u64)LAVD_SCALE);
 
-	/*
-	 * Update min/max/avg.
-	 */
-	if (c->nr_sched == 0 || c->compute_total == 0) {
-		/*
-		 * When a system is completely idle, it is indeed possible
-		 * nothing scheduled for an interval.
-		 */
+	if (!c->nr_sched || !c->compute_total) {
 		c->max_lat_cri = sys_stat.max_lat_cri;
 		c->avg_lat_cri = sys_stat.avg_lat_cri;
 
@@ -341,41 +264,30 @@ static void calc_sys_stat(struct sys_sta
 			c->max_perf_cri = sys_stat.max_perf_cri;
 			c->avg_perf_cri = sys_stat.avg_perf_cri;
 		}
-	}
-	else {
+	} else {
 		c->avg_lat_cri = c->sum_lat_cri / c->nr_sched;
 		if (have_little_core)
 			c->avg_perf_cri = c->sum_perf_cri / c->nr_sched;
 	}
 
-	/*
-	 * Update the CPU utilization to the next version.
-	 */
 	sys_stat.avg_util = calc_asym_avg(sys_stat.avg_util, c->cur_util);
 	sys_stat.avg_sc_util = calc_asym_avg(sys_stat.avg_sc_util, c->cur_sc_util);
 	sys_stat.max_lat_cri = calc_avg32(sys_stat.max_lat_cri, c->max_lat_cri);
 	sys_stat.avg_lat_cri = calc_avg32(sys_stat.avg_lat_cri, c->avg_lat_cri);
-	sys_stat.thr_lat_cri = sys_stat.max_lat_cri - ((sys_stat.max_lat_cri -
-				sys_stat.avg_lat_cri) >> preempt_shift);
+	sys_stat.thr_lat_cri = sys_stat.max_lat_cri -
+		((sys_stat.max_lat_cri - sys_stat.avg_lat_cri) >> preempt_shift);
 
 	if (have_little_core) {
-		sys_stat.min_perf_cri =
-			calc_avg32(sys_stat.min_perf_cri, c->min_perf_cri);
-		sys_stat.avg_perf_cri =
-			calc_avg32(sys_stat.avg_perf_cri, c->avg_perf_cri);
-		sys_stat.max_perf_cri =
-			calc_avg32(sys_stat.max_perf_cri, c->max_perf_cri);
+		sys_stat.min_perf_cri = calc_avg32(sys_stat.min_perf_cri, c->min_perf_cri);
+		sys_stat.avg_perf_cri = calc_avg32(sys_stat.avg_perf_cri, c->avg_perf_cri);
+		sys_stat.max_perf_cri = calc_avg32(sys_stat.max_perf_cri, c->max_perf_cri);
 	}
 
-	if (c->nr_sched > 0)
+	if (c->nr_sched)
 		avg_svc_time = c->tot_svc_time / c->nr_sched;
 	sys_stat.avg_svc_time = calc_avg(sys_stat.avg_svc_time, avg_svc_time);
 	sys_stat.nr_queued_task = calc_avg(sys_stat.nr_queued_task, c->nr_queued_task);
 
-	/*
-	 * Half the statistics every minitue so the statistics hold the
-	 * information on a few minutes.
-	 */
 	if (cnt++ == LAVD_SYS_STAT_DECAY_TIMES) {
 		cnt = 0;
 		sys_stat.nr_sched >>= 1;
@@ -387,9 +299,9 @@ static void calc_sys_stat(struct sys_sta
 		sys_stat.nr_pc_on_big >>= 1;
 		sys_stat.nr_lc_on_big >>= 1;
 
-		__sync_fetch_and_sub(&performance_mode_ns, performance_mode_ns/2);
-		__sync_fetch_and_sub(&balanced_mode_ns, balanced_mode_ns/2);
-		__sync_fetch_and_sub(&powersave_mode_ns, powersave_mode_ns/2);
+		__sync_fetch_and_sub(&performance_mode_ns, performance_mode_ns / 2);
+		__sync_fetch_and_sub(&balanced_mode_ns, balanced_mode_ns / 2);
+		__sync_fetch_and_sub(&powersave_mode_ns, powersave_mode_ns / 2);
 	}
 
 	sys_stat.nr_sched += c->nr_sched;
@@ -406,21 +318,16 @@ static void calc_sys_stat(struct sys_sta
 
 static void calc_sys_time_slice(void)
 {
-	u64 nr_q, slice;
-
-	/*
-	 * Given the updated state, recalculate the time slice for the next
-	 * round. The time slice should be short enough to schedule all
-	 * runnable tasks at least once within a targeted latency using the
-	 * active CPUs.
-	 */
-	nr_q = sys_stat.nr_queued_task;
-	if (nr_q > 0) {
-		slice = (LAVD_TARGETED_LATENCY_NS * sys_stat.nr_active) / nr_q;
-		slice = clamp(slice, slice_min_ns, slice_max_ns);
-	} else {
+	u64 nr_q = sys_stat.nr_queued_task;
+	u64 nr_active = sys_stat.nr_active ? sys_stat.nr_active : 1;
+	u64 slice;
+
+	if (nr_q)
+		slice = (LAVD_TARGETED_LATENCY_NS * nr_active) / nr_q;
+	else
 		slice = slice_max_ns;
-	}
+
+	slice = clamp(slice, slice_min_ns, slice_max_ns);
 	sys_stat.slice = calc_avg(sys_stat.slice, slice);
 }
 
@@ -438,21 +345,11 @@ static int do_update_sys_stat(void)
 __weak
 int update_sys_stat(void)
 {
-	/*
-	 * Update system statistics.
-	 */
 	do_update_sys_stat();
 
-	/*
-	 * Change the power profile based on the statistics.
-	 */
 	if (is_autopilot_on)
 		do_autopilot();
 
-	/*
-	 * Perform core compaction for powersave and balance mode.
-	 * Or turn on all CPUs for performance mode.
-	 */
 	if (!no_core_compaction)
 		do_core_compaction();
 
@@ -461,15 +358,9 @@ int update_sys_stat(void)
 		reinit_active_cpumask_for_performance();
 	}
 
-	/*
-	 * Update time slice and performance criticality threshold.
-	 */
 	calc_sys_time_slice();
 	update_thr_perf_cri();
 
-	/*
-	 * Plan cross-domain task migration.
-	 */
 	if (nr_cpdoms > 1)
 		plan_x_cpdom_migration();
 
@@ -480,6 +371,9 @@ static int update_timer_cb(void *map, in
 {
 	int err;
 
+	(void)map;
+	(void)key;
+
 	update_sys_stat();
 
 	err = bpf_timer_start(timer, LAVD_SYS_STAT_INTERVAL_NS, 0);
@@ -499,14 +393,16 @@ s32 init_sys_stat(u64 now)
 	int err;
 
 	sys_stat.last_update_clk = now;
-	sys_stat.nr_active = nr_cpus_onln;
+	sys_stat.nr_active = nr_cpus_onln ? nr_cpus_onln : 1;
 	sys_stat.slice = slice_max_ns;
+	sys_stat.nr_active_cpdoms = 0;
+
 	bpf_for(cpdom_id, 0, nr_cpdoms) {
 		if (cpdom_id >= LAVD_CPDOM_MAX_NR)
 			break;
 
 		cpdomc = MEMBER_VPTR(cpdom_ctxs, [cpdom_id]);
-		if (cpdomc->nr_active_cpus)
+		if (cpdomc && cpdomc->nr_active_cpus)
 			sys_stat.nr_active_cpdoms++;
 	}
 
@@ -525,5 +421,3 @@ s32 init_sys_stat(u64 now)
 
 	return 0;
 }
-
-

--- a/scheds/rust/scx_lavd/src/main.rs	2025-10-16 11:21:04.432360355 +0200
+++ b/scheds/rust/scx_lavd/src/main.rs	2025-11-16 11:23:48.966602297 +0200
@@ -16,10 +16,10 @@ use scx_utils::init_libbpf_logging;
 mod stats;
 use std::ffi::c_int;
 use std::ffi::CStr;
-use std::mem;
 use std::mem::MaybeUninit;
 use std::str;
 use std::sync::atomic::AtomicBool;
+use std::sync::atomic::AtomicU64;
 use std::sync::atomic::Ordering;
 use std::sync::Arc;
 use std::thread::ThreadId;
@@ -67,6 +67,10 @@ use tracing::{debug, info, warn};
 use tracing_subscriber::filter::EnvFilter;
 
 const SCHEDULER_NAME: &str = "scx_lavd";
+
+const STATS_TIMEOUT: Duration = Duration::from_secs(1);
+const RINGBUF_POLL_TIMEOUT: Duration = Duration::from_millis(100);
+
 /// scx_lavd: Latency-criticality Aware Virtual Deadline (LAVD) scheduler
 ///
 /// The rust part is minimal. It processes command line options and logs out
@@ -74,7 +78,7 @@ const SCHEDULER_NAME: &str = "scx_lavd";
 /// See the more detailed overview of the LAVD design at main.bpf.c.
 #[derive(Debug, Parser)]
 struct Opts {
-    /// Depricated, noop, use RUST_LOG or --log-level instead.
+    /// Deprecated, noop, use RUST_LOG or --log-level instead.
     #[clap(short = 'v', long, action = clap::ArgAction::Count)]
     verbose: u8,
 
@@ -103,7 +107,7 @@ struct Opts {
     #[clap(long = "performance", action = clap::ArgAction::SetTrue)]
     performance: bool,
 
-    /// Run the scheduler in powersave mode to minimize powr consumption.
+    /// Run the scheduler in powersave mode to minimize power consumption.
     /// This option cannot be used with other conflicting options (--autopilot,
     /// --autopower, --performance, --balanced, --no-core-compaction)
     /// affecting the use of core compaction.
@@ -112,7 +116,7 @@ struct Opts {
 
     /// Run the scheduler in balanced mode aiming for sweetspot between power
     /// and performance. This option cannot be used with other conflicting
-    /// options (--autopilot, --autopower, --performance, --powersave,
+    /// options (--autopilot, --autopower, --performance, --balanced,
     /// --no-core-compaction) affecting the use of core compaction.
     #[clap(long = "balanced", action = clap::ArgAction::SetTrue)]
     balanced: bool,
@@ -191,7 +195,6 @@ struct Opts {
     #[clap(long = "enable-cpu-bw", action = clap::ArgAction::SetTrue)]
     enable_cpu_bw: bool,
 
-    ///
     /// Disable core compaction so the scheduler uses all the online CPUs.
     /// The core compaction attempts to minimize the number of actively used
     /// CPUs for unaffinitized tasks, respecting the CPU preference order.
@@ -246,56 +249,71 @@ struct Opts {
 }
 
 impl Opts {
-    fn can_autopilot(&self) -> bool {
-        self.autopower == false
-            && self.performance == false
-            && self.powersave == false
-            && self.balanced == false
-            && self.no_core_compaction == false
-    }
-
-    fn can_autopower(&self) -> bool {
-        self.autopilot == false
-            && self.performance == false
-            && self.powersave == false
-            && self.balanced == false
-            && self.no_core_compaction == false
-    }
-
-    fn can_performance(&self) -> bool {
-        self.autopilot == false
-            && self.autopower == false
-            && self.powersave == false
-            && self.balanced == false
-    }
-
-    fn can_balanced(&self) -> bool {
-        self.autopilot == false
-            && self.autopower == false
-            && self.performance == false
-            && self.powersave == false
-            && self.no_core_compaction == false
-    }
-
-    fn can_powersave(&self) -> bool {
-        self.autopilot == false
-            && self.autopower == false
-            && self.performance == false
-            && self.balanced == false
-            && self.no_core_compaction == false
+    #[inline(always)]
+    const fn can_autopilot(&self) -> bool {
+        !self.autopower
+            && !self.performance
+            && !self.powersave
+            && !self.balanced
+            && !self.no_core_compaction
+    }
+
+    #[inline(always)]
+    const fn can_autopower(&self) -> bool {
+        !self.autopilot
+            && !self.performance
+            && !self.powersave
+            && !self.balanced
+            && !self.no_core_compaction
+    }
+
+    #[inline(always)]
+    const fn can_performance(&self) -> bool {
+        !self.autopilot && !self.autopower && !self.powersave && !self.balanced
+    }
+
+    #[inline(always)]
+    const fn can_balanced(&self) -> bool {
+        !self.autopilot
+            && !self.autopower
+            && !self.performance
+            && !self.powersave
+            && !self.no_core_compaction
+    }
+
+    #[inline(always)]
+    const fn can_powersave(&self) -> bool {
+        !self.autopilot
+            && !self.autopower
+            && !self.performance
+            && !self.balanced
+            && !self.no_core_compaction
+    }
+
+    #[inline]
+    fn validate_slice_window(&self) -> bool {
+        if self.slice_min_us == 0 || self.slice_min_us > self.slice_max_us {
+            info!(
+                "slice-min-us ({}) must be > 0 and <= slice-max-us ({})",
+                self.slice_min_us, self.slice_max_us
+            );
+            return false;
+        }
+        true
     }
 
     fn proc(&mut self) -> Option<&mut Self> {
+        if !self.validate_slice_window() {
+            return None;
+        }
+
         if !self.autopilot {
             self.autopilot = self.can_autopilot();
         }
 
-        if self.autopilot {
-            if !self.can_autopilot() {
-                info!("Autopilot mode cannot be used with conflicting options.");
-                return None;
-            }
-            info!("Autopilot mode is enabled.");
+        if self.autopilot && !self.can_autopilot() {
+            info!("Autopilot mode cannot be used with conflicting options.");
+            return None;
         }
 
         if self.autopower {
@@ -347,18 +365,24 @@ impl Opts {
                 return None;
             }
             info!(
-                "Pinned task slice mode is enabled ({} us). Pinned tasks will use per-CPU DSQs.",
+                "Pinned task slice mode enabled ({} Î¼s). Pinned tasks use per-CPU DSQs.",
                 pinned_slice
             );
         }
 
+        if self.autopilot {
+            info!("Autopilot mode is enabled.");
+        }
+
         Some(self)
     }
 
+    #[inline]
     fn preempt_shift_range(s: &str) -> Result<u8, String> {
         number_range(s, 0, 10)
     }
 
+    #[inline]
     fn mig_delta_pct_range(s: &str) -> Result<u8, String> {
         number_range(s, 0, 100)
     }
@@ -367,18 +391,28 @@ impl Opts {
 unsafe impl Plain for msg_task_ctx {}
 
 impl msg_task_ctx {
-    fn from_bytes(buf: &[u8]) -> &msg_task_ctx {
-        plain::from_bytes(buf).expect("The buffer is either too short or not aligned!")
+    #[inline]
+    fn from_bytes(buf: &[u8]) -> Result<&msg_task_ctx> {
+        plain::from_bytes(buf)
+            .map_err(|e| anyhow::anyhow!("Failed to parse msg_task_ctx: {:?}", e))
     }
 }
 
 impl introspec {
-    fn new() -> Self {
-        let intrspc = unsafe { mem::MaybeUninit::<introspec>::zeroed().assume_init() };
-        intrspc
+    #[inline]
+    const fn new() -> Self {
+        Self {
+            cmd: LAVD_CMD_NOP,
+            arg: 0,
+        }
     }
 }
 
+#[repr(align(64))]
+struct AlignedAtomicU64(AtomicU64);
+
+static MSG_SEQ_ID: AlignedAtomicU64 = AlignedAtomicU64(AtomicU64::new(0));
+
 struct Scheduler<'a> {
     skel: BpfSkel<'a>,
     struct_ops: Option<libbpf_rs::Link>,
@@ -387,74 +421,71 @@ struct Scheduler<'a> {
     intrspc_rx: Receiver<SchedSample>,
     monitor_tid: Option<ThreadId>,
     stats_server: StatsServer<StatsReq, StatsRes>,
-    mseq_id: u64,
 }
 
 impl<'a> Scheduler<'a> {
     fn init(opts: &'a Opts, open_object: &'a mut MaybeUninit<OpenObject>) -> Result<Self> {
         if *NR_CPU_IDS > LAVD_CPU_ID_MAX as usize {
-            panic!(
+            anyhow::bail!(
                 "Num possible CPU IDs ({}) exceeds maximum of ({})",
-                *NR_CPU_IDS, LAVD_CPU_ID_MAX
+                *NR_CPU_IDS,
+                LAVD_CPU_ID_MAX
             );
         }
 
         try_set_rlimit_infinity();
 
-        // Open the BPF prog first for verification.
         let debug_level = if opts.log_level.contains("trace") {
             2
         } else if opts.log_level.contains("debug") {
             1
+        } else if opts.verbose > 1 {
+            2
+        } else if opts.verbose > 0 {
+            1
         } else {
             0
         };
+
         let mut skel_builder = BpfSkelBuilder::default();
         skel_builder.obj_builder.debug(debug_level > 1);
-        init_libbpf_logging(Some(PrintLevel::Debug));
+        let log_level = if debug_level > 0 {
+            Some(PrintLevel::Debug)
+        } else {
+            None
+        };
+        init_libbpf_logging(log_level);
 
         let open_opts = opts.libbpf.clone().into_bpf_open_opts();
         let mut skel = scx_ops_open!(skel_builder, open_object, lavd_ops, open_opts)?;
 
-        // Enable futex tracing using ftrace if available. If the ftrace is not
-        // available, use tracepoint, which is known to be slower than ftrace.
         if !opts.no_futex_boost {
-            if Self::attach_futex_ftraces(&mut skel)? == false {
-                info!("Fail to attach futex ftraces. Try with tracepoints.");
-                if Self::attach_futex_tracepoints(&mut skel)? == false {
-                    info!("Fail to attach futex tracepoints.");
+            if !Self::attach_futex_ftraces(&mut skel)? {
+                info!("Failed to attach futex ftraces. Trying tracepoints.");
+                if !Self::attach_futex_tracepoints(&mut skel)? {
+                    warn!("Failed to attach futex tracepoints. Futex boosting disabled.");
                 }
             }
         }
 
-        // Initialize CPU topology with CLI arguments
-        let order = CpuOrder::new(opts.topology.as_ref()).unwrap();
+        let order = CpuOrder::new(opts.topology.as_ref())?;
         Self::init_cpus(&mut skel, &order);
         Self::init_cpdoms(&mut skel, &order);
+        Self::init_globals(&mut skel, opts, &order, debug_level);
 
-        // Initialize skel according to @opts.
-        Self::init_globals(&mut skel, &opts, &order, debug_level);
-
-        // Initialize arena
         let mut skel = scx_ops_load!(skel, lavd_ops, uei)?;
         let task_size = std::mem::size_of::<types::task_ctx>();
         let arenalib = ArenaLib::init(skel.object_mut(), task_size, *NR_CPU_IDS)?;
         arenalib.setup()?;
 
-        // Attach.
         let struct_ops = Some(scx_ops_attach!(skel, lavd_ops)?);
         let stats_server = StatsServer::new(stats::server_data(*NR_CPU_IDS as u64)).launch()?;
 
-        // Build a ring buffer for instrumentation
         let (intrspc_tx, intrspc_rx) = channel::bounded(65536);
         let rb_map = &mut skel.maps.introspec_msg;
         let mut builder = libbpf_rs::RingBufferBuilder::new();
-        builder
-            .add(rb_map, move |data| {
-                Scheduler::relay_introspec(data, &intrspc_tx)
-            })
-            .unwrap();
-        let rb_mgr = builder.build().unwrap();
+        builder.add(rb_map, move |data| Scheduler::relay_introspec(data, &intrspc_tx))?;
+        let rb_mgr = builder.build()?;
 
         Ok(Self {
             skel,
@@ -464,7 +495,6 @@ impl<'a> Scheduler<'a> {
             intrspc_rx,
             monitor_tid: None,
             stats_server,
-            mseq_id: 0,
         })
     }
 
@@ -482,7 +512,7 @@ impl<'a> Scheduler<'a> {
             ("futex_unlock_pi", &skel.progs.fexit_futex_unlock_pi),
         ];
 
-        if compat::tracer_available("function")? == false {
+        if !compat::tracer_available("function")? {
             info!("Ftrace is not enabled in the kernel.");
             return Ok(false);
         }
@@ -514,95 +544,131 @@ impl<'a> Scheduler<'a> {
     fn init_cpus(skel: &mut OpenBpfSkel, order: &CpuOrder) {
         debug!("{:#?}", order);
 
-        // Initialize CPU capacity and sibling
-        for cpu in order.cpuids.iter() {
-            skel.maps.rodata_data.as_mut().unwrap().cpu_capacity[cpu.cpu_adx] = cpu.cpu_cap as u16;
-            skel.maps.rodata_data.as_mut().unwrap().cpu_big[cpu.cpu_adx] = cpu.big_core as u8;
-            skel.maps.rodata_data.as_mut().unwrap().cpu_turbo[cpu.cpu_adx] = cpu.turbo_core as u8;
-            skel.maps.rodata_data.as_mut().unwrap().cpu_sibling[cpu.cpu_adx] =
-                cpu.cpu_sibling as u32;
+        let nr_pco_states = order.perf_cpu_order.len() as u8;
+        if nr_pco_states > LAVD_PCO_STATE_MAX as u8 {
+            panic!(
+                "Generated performance vs. CPU order states ({}) exceed maximum ({})",
+                nr_pco_states, LAVD_PCO_STATE_MAX
+            );
         }
 
-        // Initialize performance vs. CPU order table.
-        let nr_pco_states: u8 = order.perf_cpu_order.len() as u8;
-        if nr_pco_states > LAVD_PCO_STATE_MAX as u8 {
-            panic!("Generated performance vs. CPU order stats are too complex ({nr_pco_states}) to handle");
+        let rodata = skel
+            .maps
+            .rodata_data
+            .as_mut()
+            .expect("rodata not available");
+
+        for cpu in order.cpuids.iter() {
+            rodata.cpu_capacity[cpu.cpu_adx] = cpu.cpu_cap as u16;
+            rodata.cpu_big[cpu.cpu_adx] = cpu.big_core as u8;
+            rodata.cpu_turbo[cpu.cpu_adx] = cpu.turbo_core as u8;
+            rodata.cpu_sibling[cpu.cpu_adx] = cpu.cpu_sibling as u32;
         }
 
-        skel.maps.rodata_data.as_mut().unwrap().nr_pco_states = nr_pco_states;
+        rodata.nr_pco_states = nr_pco_states;
+
         for (i, (_, pco)) in order.perf_cpu_order.iter().enumerate() {
-            Self::init_pco_tuple(skel, i, &pco);
+            Self::init_pco_tuple(skel, i, pco);
             info!("{:#}", pco);
         }
 
-        let (_, last_pco) = order.perf_cpu_order.last_key_value().unwrap();
-        for i in nr_pco_states..LAVD_PCO_STATE_MAX as u8 {
-            Self::init_pco_tuple(skel, i as usize, &last_pco);
+        if let Some((_, last_pco)) = order.perf_cpu_order.last_key_value() {
+            for i in nr_pco_states..LAVD_PCO_STATE_MAX as u8 {
+                Self::init_pco_tuple(skel, i as usize, last_pco);
+            }
         }
     }
 
+    #[inline]
     fn init_pco_tuple(skel: &mut OpenBpfSkel, i: usize, pco: &PerfCpuOrder) {
+        let rodata = skel
+            .maps
+            .rodata_data
+            .as_mut()
+            .expect("rodata not available");
         let cpus_perf = pco.cpus_perf.borrow();
         let cpus_ovflw = pco.cpus_ovflw.borrow();
         let pco_nr_primary = cpus_perf.len();
 
-        skel.maps.rodata_data.as_mut().unwrap().pco_bounds[i] = pco.perf_cap as u32;
-        skel.maps.rodata_data.as_mut().unwrap().pco_nr_primary[i] = pco_nr_primary as u16;
+        rodata.pco_bounds[i] = pco.perf_cap as u32;
+        rodata.pco_nr_primary[i] = pco_nr_primary as u16;
 
         for (j, &cpu_adx) in cpus_perf.iter().enumerate() {
-            skel.maps.rodata_data.as_mut().unwrap().pco_table[i][j] = cpu_adx as u16;
+            rodata.pco_table[i][j] = cpu_adx as u16;
         }
 
         for (j, &cpu_adx) in cpus_ovflw.iter().enumerate() {
             let k = j + pco_nr_primary;
-            skel.maps.rodata_data.as_mut().unwrap().pco_table[i][k] = cpu_adx as u16;
+            rodata.pco_table[i][k] = cpu_adx as u16;
         }
     }
 
     fn init_cpdoms(skel: &mut OpenBpfSkel, order: &CpuOrder) {
-        // Initialize compute domain contexts
+        let bss_data = skel.maps.bss_data.as_mut().expect("bss_data not available");
+
         for (k, v) in order.cpdom_map.iter() {
-            skel.maps.bss_data.as_mut().unwrap().cpdom_ctxs[v.cpdom_id].id = v.cpdom_id as u64;
-            skel.maps.bss_data.as_mut().unwrap().cpdom_ctxs[v.cpdom_id].alt_id =
-                v.cpdom_alt_id.get() as u64;
-            skel.maps.bss_data.as_mut().unwrap().cpdom_ctxs[v.cpdom_id].numa_id = k.numa_adx as u8;
-            skel.maps.bss_data.as_mut().unwrap().cpdom_ctxs[v.cpdom_id].llc_id = k.llc_adx as u8;
-            skel.maps.bss_data.as_mut().unwrap().cpdom_ctxs[v.cpdom_id].is_big = k.is_big as u8;
-            skel.maps.bss_data.as_mut().unwrap().cpdom_ctxs[v.cpdom_id].is_valid = 1;
-            for cpu_id in v.cpu_ids.iter() {
-                let i = cpu_id / 64;
-                let j = cpu_id % 64;
-                skel.maps.bss_data.as_mut().unwrap().cpdom_ctxs[v.cpdom_id].__cpumask[i] |=
-                    0x01 << j;
+            let cpdom = &mut bss_data.cpdom_ctxs[v.cpdom_id];
+
+            cpdom.id = v.cpdom_id as u64;
+            cpdom.alt_id = v.cpdom_alt_id.get() as u64;
+            cpdom.numa_id = k.numa_adx as u8;
+            cpdom.llc_id = k.llc_adx as u8;
+            cpdom.is_big = k.is_big as u8;
+            cpdom.is_valid = 1;
+
+            for &cpu_id in v.cpu_ids.iter() {
+                let word_idx = (cpu_id / 64) as usize;
+                let bit_idx = cpu_id % 64;
+                cpdom.__cpumask[word_idx] |= 1u64 << bit_idx;
             }
 
-            if v.neighbor_map.borrow().iter().len() > LAVD_CPDOM_MAX_DIST as usize {
-                panic!("The processor topology is too complex to handle in BPF.");
+            let neighbor_count = v.neighbor_map.borrow().len();
+            if neighbor_count > LAVD_CPDOM_MAX_DIST as usize {
+                panic!(
+                    "Processor topology too complex: {} neighbor distances (max {})",
+                    neighbor_count, LAVD_CPDOM_MAX_DIST
+                );
             }
 
-            for (k, (_d, neighbors)) in v.neighbor_map.borrow().iter().enumerate() {
-                let nr_neighbors = neighbors.borrow().len() as u8;
+            for (dist_idx, (_distance, neighbors)) in v.neighbor_map.borrow().iter().enumerate() {
+                let neighbor_list = neighbors.borrow();
+                let nr_neighbors = neighbor_list.len() as u8;
+
                 if nr_neighbors > LAVD_CPDOM_MAX_NR as u8 {
-                    panic!("The processor topology is too complex to handle in BPF.");
+                    panic!(
+                        "Too many neighbor domains: {} (max {})",
+                        nr_neighbors, LAVD_CPDOM_MAX_NR
+                    );
                 }
-                skel.maps.bss_data.as_mut().unwrap().cpdom_ctxs[v.cpdom_id].nr_neighbors[k] =
-                    nr_neighbors;
-                for (i, &id) in neighbors.borrow().iter().enumerate() {
-                    let idx = (k * LAVD_CPDOM_MAX_NR as usize) + i;
-                    skel.maps.bss_data.as_mut().unwrap().cpdom_ctxs[v.cpdom_id].neighbor_ids[idx] =
-                        id as u8;
+
+                cpdom.nr_neighbors[dist_idx] = nr_neighbors;
+
+                // Flatten neighbor IDs into the array
+                for (i, &neighbor_id) in neighbor_list.iter().enumerate() {
+                    let idx = (dist_idx * LAVD_CPDOM_MAX_NR as usize) + i;
+                    cpdom.neighbor_ids[idx] = neighbor_id as u8;
                 }
             }
         }
     }
 
-    fn init_globals(skel: &mut OpenBpfSkel, opts: &Opts, order: &CpuOrder, debug_level: u8) {
-        let bss_data = skel.maps.bss_data.as_mut().unwrap();
+    fn init_globals(
+        skel: &mut OpenBpfSkel,
+        opts: &Opts,
+        order: &CpuOrder,
+        debug_level: u8,
+    ) {
+        let bss_data = skel.maps.bss_data.as_mut().expect("bss_data not available");
         bss_data.no_preemption = opts.no_preemption;
         bss_data.no_core_compaction = opts.no_core_compaction;
         bss_data.no_freq_scaling = opts.no_freq_scaling;
         bss_data.is_powersave_mode = opts.powersave;
-        let rodata = skel.maps.rodata_data.as_mut().unwrap();
+
+        let rodata = skel
+            .maps
+            .rodata_data
+            .as_mut()
+            .expect("rodata not available");
         rodata.nr_llcs = order.nr_llcs as u64;
         rodata.nr_cpu_ids = *NR_CPU_IDS as u32;
         rodata.is_smt_active = order.smt_enabled;
@@ -625,48 +691,53 @@ impl<'a> Scheduler<'a> {
             | *compat::SCX_OPS_KEEP_BUILTIN_IDLE;
     }
 
+    #[inline(always)]
     fn get_msg_seq_id() -> u64 {
-        static mut MSEQ: u64 = 0;
-        unsafe {
-            MSEQ += 1;
-            MSEQ
-        }
+        MSG_SEQ_ID.0.fetch_add(1, Ordering::Relaxed)
     }
 
     fn relay_introspec(data: &[u8], intrspc_tx: &Sender<SchedSample>) -> i32 {
-        let mt = msg_task_ctx::from_bytes(data);
-        let tx = mt.taskc_x;
-        let tc = mt.taskc;
+        let mt = match msg_task_ctx::from_bytes(data) {
+            Ok(mt) => mt,
+            Err(e) => return Self::handle_parse_error(&e),
+        };
 
-        // No idea how to print other types than LAVD_MSG_TASKC
         if mt.hdr.kind != LAVD_MSG_TASKC {
             return 0;
         }
 
-        let mseq = Scheduler::get_msg_seq_id();
+        let tx = &mt.taskc_x;
+        let tc = &mt.taskc;
+        let mseq = Self::get_msg_seq_id();
+
+        let tx_comm = unsafe {
+            CStr::from_ptr(tx.comm.as_ptr() as *const c_char)
+                .to_string_lossy()
+                .into_owned()
+        };
+
+        let waker_comm = unsafe {
+            CStr::from_ptr(tc.waker_comm.as_ptr() as *const c_char)
+                .to_string_lossy()
+                .into_owned()
+        };
 
-        let c_tx_cm: *const c_char = (&tx.comm as *const [c_char; 17]) as *const c_char;
-        let c_tx_cm_str: &CStr = unsafe { CStr::from_ptr(c_tx_cm) };
-        let tx_comm: &str = c_tx_cm_str.to_str().unwrap();
-
-        let c_waker_cm: *const c_char = (&tc.waker_comm as *const [c_char; 17]) as *const c_char;
-        let c_waker_cm_str: &CStr = unsafe { CStr::from_ptr(c_waker_cm) };
-        let waker_comm: &str = c_waker_cm_str.to_str().unwrap();
-
-        let c_tx_st: *const c_char = (&tx.stat as *const [c_char; 5]) as *const c_char;
-        let c_tx_st_str: &CStr = unsafe { CStr::from_ptr(c_tx_st) };
-        let tx_stat: &str = c_tx_st_str.to_str().unwrap();
+        let tx_stat = unsafe {
+            CStr::from_ptr(tx.stat.as_ptr() as *const c_char)
+                .to_string_lossy()
+                .into_owned()
+        };
 
         match intrspc_tx.try_send(SchedSample {
             mseq,
             pid: tc.pid,
-            comm: tx_comm.into(),
-            stat: tx_stat.into(),
+            comm: tx_comm,
+            stat: tx_stat,
             cpu_id: tc.cpu_id,
             prev_cpu_id: tc.prev_cpu_id,
             suggested_cpu_id: tc.suggested_cpu_id,
             waker_pid: tc.waker_pid,
-            waker_comm: waker_comm.into(),
+            waker_comm,
             slice: tc.slice,
             lat_cri: tc.lat_cri,
             avg_lat_cri: tx.avg_lat_cri,
@@ -687,27 +758,66 @@ impl<'a> Scheduler<'a> {
             dsq_consume_lat: tx.dsq_consume_lat,
             slice_used: tc.last_slice_used,
         }) {
-            Ok(()) | Err(TrySendError::Full(_)) => 0,
-            Err(e) => panic!("failed to send on intrspc_tx ({})", e),
+            Ok(()) => 0,
+            Err(TrySendError::Full(_)) => Self::handle_channel_full(),
+            Err(TrySendError::Disconnected(_)) => -1,
         }
     }
 
+    #[cold]
+    #[inline(never)]
+    fn handle_parse_error(e: &anyhow::Error) -> i32 {
+        static PARSE_ERROR_COUNT: AtomicU64 = AtomicU64::new(0);
+        let count = PARSE_ERROR_COUNT.fetch_add(1, Ordering::Relaxed);
+        if count % 1000 == 0 {
+            warn!("Failed to parse msg_task_ctx (count: {}): {:?}", count, e);
+        }
+        -1
+    }
+
+    #[cold]
+    #[inline(never)]
+    fn handle_channel_full() -> i32 {
+        static DROP_COUNT: AtomicU64 = AtomicU64::new(0);
+        let count = DROP_COUNT.fetch_add(1, Ordering::Relaxed);
+        if count % 10000 == 0 {
+            warn!("Sample channel full, dropped {} samples", count);
+        }
+        0
+    }
+
+    #[inline]
     fn prep_introspec(&mut self) {
-        if !self.skel.maps.bss_data.as_ref().unwrap().is_monitored {
-            self.skel.maps.bss_data.as_mut().unwrap().is_monitored = true;
+        let bss_data = self
+            .skel
+            .maps
+            .bss_data
+            .as_mut()
+            .expect("bss_data not available");
+        if !bss_data.is_monitored {
+            bss_data.is_monitored = true;
         }
-        self.skel.maps.bss_data.as_mut().unwrap().intrspc.cmd = self.intrspc.cmd;
-        self.skel.maps.bss_data.as_mut().unwrap().intrspc.arg = self.intrspc.arg;
+        bss_data.intrspc.cmd = self.intrspc.cmd;
+        bss_data.intrspc.arg = self.intrspc.arg;
     }
 
+    #[inline]
     fn cleanup_introspec(&mut self) {
-        self.skel.maps.bss_data.as_mut().unwrap().intrspc.cmd = LAVD_CMD_NOP;
+        if let Some(bss_data) = self.skel.maps.bss_data.as_mut() {
+            bss_data.intrspc.cmd = LAVD_CMD_NOP;
+        }
     }
 
+    #[inline(always)]
     fn get_pc(x: u64, y: u64) -> f64 {
-        return 100. * x as f64 / y as f64;
+        if y == 0 {
+            0.0
+        } else {
+            (x as f64).mul_add(100.0, 0.0) / (y as f64)
+        }
     }
 
+    #[inline(always)]
     fn get_power_mode(power_mode: i32) -> &'static str {
         match power_mode as u32 {
             LAVD_PM_PERFORMANCE => "performance",
@@ -720,7 +830,9 @@ impl<'a> Scheduler<'a> {
     fn stats_req_to_res(&mut self, req: &StatsReq) -> Result<StatsRes> {
         Ok(match req {
             StatsReq::NewSampler(tid) => {
-                self.rb_mgr.consume().unwrap();
+                self.rb_mgr
+                    .consume()
+                    .context("Failed to consume ring buffer")?;
                 self.monitor_tid = Some(*tid);
                 StatsRes::Ack
             }
@@ -728,24 +840,30 @@ impl<'a> Scheduler<'a> {
                 if Some(*tid) != self.monitor_tid {
                     return Ok(StatsRes::Bye);
                 }
-                self.mseq_id += 1;
 
-                let bss_data = self.skel.maps.bss_data.as_ref().unwrap();
-                let st = bss_data.sys_stat;
+                let bss_data = self
+                    .skel
+                    .maps
+                    .bss_data
+                    .as_ref()
+                    .expect("bss_data not available");
+                let st = &bss_data.sys_stat;
 
-                let mseq = self.mseq_id;
+                let mseq = Self::get_msg_seq_id();
                 let nr_queued_task = st.nr_queued_task;
                 let nr_active = st.nr_active;
                 let nr_sched = st.nr_sched;
                 let nr_preempt = st.nr_preempt;
+                let nr_stealee = st.nr_stealee;
+                let nr_big = st.nr_big;
+
                 let pc_pc = Self::get_pc(st.nr_perf_cri, nr_sched);
                 let pc_lc = Self::get_pc(st.nr_lat_cri, nr_sched);
                 let pc_x_migration = Self::get_pc(st.nr_x_migration, nr_sched);
-                let nr_stealee = st.nr_stealee;
-                let nr_big = st.nr_big;
                 let pc_big = Self::get_pc(nr_big, nr_sched);
                 let pc_pc_on_big = Self::get_pc(st.nr_pc_on_big, nr_big);
                 let pc_lc_on_big = Self::get_pc(st.nr_lc_on_big, nr_big);
+
                 let power_mode = Self::get_power_mode(bss_data.power_mode);
                 let total_time = bss_data.performance_mode_ns
                     + bss_data.balanced_mode_ns
@@ -786,12 +904,13 @@ impl<'a> Scheduler<'a> {
                 self.intrspc.arg = *nr_samples;
                 self.prep_introspec();
                 std::thread::sleep(Duration::from_millis(*interval_ms));
-                self.rb_mgr.poll(Duration::from_millis(100)).unwrap();
 
-                let mut samples = vec![];
-                while let Ok(ts) = self.intrspc_rx.try_recv() {
-                    samples.push(ts);
-                }
+                self.rb_mgr
+                    .poll(RINGBUF_POLL_TIMEOUT)
+                    .context("Failed to poll ring buffer")?;
+
+                let mut samples = Vec::with_capacity(*nr_samples as usize);
+                samples.extend(self.intrspc_rx.try_iter());
 
                 self.cleanup_introspec();
 
@@ -800,12 +919,16 @@ impl<'a> Scheduler<'a> {
         })
     }
 
+    #[inline]
     fn stop_monitoring(&mut self) {
-        if self.skel.maps.bss_data.as_ref().unwrap().is_monitored {
-            self.skel.maps.bss_data.as_mut().unwrap().is_monitored = false;
+        if let Some(bss_data) = self.skel.maps.bss_data.as_mut() {
+            if bss_data.is_monitored {
+                bss_data.is_monitored = false;
+            }
         }
     }
 
+    #[inline]
     pub fn exited(&mut self) -> bool {
         uei_exited!(&self.skel, uei)
     }
@@ -824,7 +947,7 @@ impl<'a> Scheduler<'a> {
             }),
             ..Default::default()
         };
-        let out = prog.test_run(input).unwrap();
+        let out = prog.test_run(input).map_err(|_| u32::MAX)?;
         if out.return_value != 0 {
             return Err(out.return_value);
         }
@@ -835,44 +958,56 @@ impl<'a> Scheduler<'a> {
     fn update_power_profile(&mut self, prev_profile: PowerProfile) -> (bool, PowerProfile) {
         let profile = fetch_power_profile(false);
         if profile == prev_profile {
-            // If the profile is the same, skip updaring the profile for BPF.
             return (true, profile);
         }
 
-        let _ = match profile {
+        let set_result = match profile {
             PowerProfile::Performance => self.set_power_profile(LAVD_PM_PERFORMANCE),
             PowerProfile::Balanced { .. } => self.set_power_profile(LAVD_PM_BALANCED),
             PowerProfile::Powersave => self.set_power_profile(LAVD_PM_POWERSAVE),
             PowerProfile::Unknown => {
-                // We don't know how to handle an unknown energy profile,
-                // so we just give up updating the profile from now on.
                 return (false, profile);
             }
         };
 
-        info!("Set the scheduler's power profile to {profile} mode.");
-        (true, profile)
+        match set_result {
+            Ok(_) => {
+                info!("Set the scheduler's power profile to {profile} mode.");
+                (true, profile)
+            }
+            Err(code) => {
+                warn!(
+                    "Failed to update power profile to {profile:?} (ret={}). Disabling autopower.",
+                    code
+                );
+                (false, prev_profile)
+            }
+        }
     }
 
     fn run(&mut self, opts: &Opts, shutdown: Arc<AtomicBool>) -> Result<UserExitInfo> {
         let (res_ch, req_ch) = self.stats_server.channels();
-        let mut autopower = opts.autopower;
+        let mut autopower_active = opts.autopower;
         let mut profile = PowerProfile::Unknown;
 
         if opts.performance {
-            let _ = self.set_power_profile(LAVD_PM_PERFORMANCE);
+            if let Err(e) = self.set_power_profile(LAVD_PM_PERFORMANCE) {
+                warn!("Failed to set initial performance profile: {}", e);
+            }
         } else if opts.powersave {
-            let _ = self.set_power_profile(LAVD_PM_POWERSAVE);
-        } else {
-            let _ = self.set_power_profile(LAVD_PM_BALANCED);
+            if let Err(e) = self.set_power_profile(LAVD_PM_POWERSAVE) {
+                warn!("Failed to set initial powersave profile: {}", e);
+            }
+        } else if let Err(e) = self.set_power_profile(LAVD_PM_BALANCED) {
+            warn!("Failed to set initial balanced profile: {}", e);
         }
 
-        while !shutdown.load(Ordering::Relaxed) && !self.exited() {
-            if autopower {
-                (autopower, profile) = self.update_power_profile(profile);
+        while !shutdown.load(Ordering::Acquire) && !self.exited() {
+            if autopower_active {
+                (autopower_active, profile) = self.update_power_profile(profile);
             }
 
-            match req_ch.recv_timeout(Duration::from_secs(1)) {
+            match req_ch.recv_timeout(STATS_TIMEOUT) {
                 Ok(req) => {
                     let res = self.stats_req_to_res(&req)?;
                     res_ch.send(res)?;
@@ -882,12 +1017,14 @@ impl<'a> Scheduler<'a> {
                 }
                 Err(e) => {
                     self.stop_monitoring();
-                    Err(e)?
+                    Err(e)?;
                 }
             }
             self.cleanup_introspec();
         }
-        self.rb_mgr.consume().unwrap();
+        self.rb_mgr
+            .consume()
+            .context("Failed to flush ring buffer before exit")?;
 
         let _ = self.struct_ops.take();
         uei_report!(&self.skel, uei)
@@ -955,7 +1092,7 @@ fn main(mut opts: Opts) -> Result<()> {
     init_log(&opts);
 
     if opts.verbose > 0 {
-        warn!("Setting verbose via -v is depricated and will be an error in future releases.");
+        warn!("Setting verbose via -v is deprecated and will be an error in future releases.");
     }
 
     if let Some(run_id) = opts.run_id {
@@ -963,7 +1100,9 @@ fn main(mut opts: Opts) -> Result<()> {
     }
 
     if opts.monitor.is_none() && opts.monitor_sched_samples.is_none() {
-        opts.proc().unwrap();
+        if opts.proc().is_none() {
+            return Ok(());
+        }
         info!("{:#?}", opts);
     }
 

--- a/scheds/rust/scx_lavd/src/bpf/main.bpf.c	2025-10-22 21:17:42.274261405 +0200
+++ b/scheds/rust/scx_lavd/src/bpf/main.bpf.c	2025-10-22 21:20:20.365435857 +0200
@@ -3,182 +3,8 @@
  * scx_lavd: Latency-criticality Aware Virtual Deadline (LAVD) scheduler
  * =====================================================================
  *
- * LAVD is a new scheduling algorithm which is still under development. It is
- * motivated by gaming workloads, which are latency-critical and
- * communication-heavy. It aims to minimize latency spikes while maintaining
- * overall good throughput and fair use of CPU time among tasks.
- *
- *
- * 1. Overall procedure of the LAVD scheduler
- * ------------------------------------------
- *
- * LAVD is a deadline-based scheduling algorithm, so its overall procedure is
- * similar to other deadline-based scheduling algorithms. Under LAVD, a
- * runnable task has its time slice and virtual deadline. The LAVD scheduler
- * picks a task with the closest virtual deadline and allows it to execute for
- * the given time slice.
- *
- *
- * 2. Latency criticality: how to determine how latency-critical a task is
- * -----------------------------------------------------------------------
- *
- * The LAVD scheduler leverages how much latency-critical a task is in making
- * various scheduling decisions. For example, if the execution of Task A is not
- * latency critical -- i.e., the scheduling delay of Task A does not affect the
- * end performance much, a scheduler would defer the scheduling of Task A to
- * serve more latency-critical urgent tasks first.
- *
- * Then, how do we know if a task is latency-critical or not? One can ask a
- * developer to annotate the process/thread's latency criticality, for example,
- * using a latency nice interface. Unfortunately, that is not always possible,
- * especially when running existing software without modification.
- *
- * We leverage a task's communication and behavioral properties to quantify its
- * latency criticality. Suppose there are three tasks: Task A, B, and C, and
- * they are in a producer-consumer relation; Task A's completion triggers the
- * execution of Task B, and Task B's completion triggers Task C. Many
- * event-driven systems can be represented as task graphs.
- *
- *        [Task x] --> [Task B] --> [Task C]
- *
- * We define Task B is more latency-critical in the following cases: a) as Task
- * B's runtime per schedule is shorter (runtime B) b) as Task B wakes Task C
- * more frequently (wake_freq B) c) as Task B waits for Task A more frequently
- * (wait_freq B)
- *
- * Intuitively, if Task B's runtime per schedule is long, a relatively short
- * scheduling delay won't affect a lot; if Task B frequently wakes up Task C,
- * the scheduling delay of Task B also delays the execution of Task C;
- * similarly, if Task B often waits for Task A, the scheduling delay of Task B
- * delays the completion of executing the task graph.
- *
- *
- * 3. Virtual deadline: when to execute a task
- * -------------------------------------------
- *
- * The latency criticality of a task is used to determine task's virtual
- * deadline. A more latency-critical task will have a tighter (shorter)
- * deadline, so the scheduler picks such a task more urgently among runnable
- * tasks.
- *
- *
- * 4. Time slice: how long execute a task
- * --------------------------------------
- *
- * We borrow the time slice calculation idea from the CFS and scx_rustland
- * schedulers. The LAVD scheduler tries to schedule all the runnable tasks at
- * least once within a predefined time window, which is called a targeted
- * latency. For example, if a targeted latency is 15 msec and 10 tasks are
- * runnable, the scheduler equally divides 15 msec of CPU time into 10 tasks.
- * Of course, the scheduler will consider the task's priority -- a task with
- * higher priority (lower nice value) will receive a longer time slice.
- *
- * The scheduler also considers the behavioral properties of a task in
- * determining the time slice. If a task is compute-intensive, so it consumes
- * the assigned time slice entirely, the scheduler boosts such task's time
- * slice and assigns a longer time slice. Next, if a task is freshly forked,
- * the scheduler assigns only half of a regular time slice so it can make a
- * more educated decision after collecting the behavior of a new task. This
- * helps to mitigate fork-bomb attacks.
- *
- *
- * 5. Fairness: how to enforce the fair use of CPU time
- * ----------------------------------------------------
- *
- * Assigning a task's time slice per its priority does not guarantee the fair
- * use of CPU time. That is because a task can be more (or less) frequently
- * executed than other tasks or yield CPU before entirely consuming its
- * assigned time slice.
- *
- * The scheduler treats the over-scheduled (or ineligible) tasks to enforce the
- * fair use of CPU time. It defers choosing over-scheduled tasks to reduce the
- * frequency of task execution. The deferring time- ineligible duration- is
- * proportional to how much time is over-spent and added to the task's
- * deadline.
- *
- * 6. Preemption
- * -------------
- *
- * A task can be preempted (de-scheduled) before exhausting its time slice. The
- * scheduler uses two preemption mechanisms: 1) yield-based preemption and
- * 2) kick-based preemption.
- *
- * In every scheduler tick interval (when ops.tick() is called), the running
- * task checks if a higher priority task awaits execution in the global run
- * queue. If so, the running task shrinks its time slice to zero to trigger
- * re-scheduling for another task as soon as possible. This is what we call
- * yield-based preemption. In addition to the tick interval, the scheduler
- * additionally performs yield-based preemption when there is no idle CPU on
- * ops.select_cpu() and ops.enqueue(). The yield-based preemption takes the
- * majority (70-90%) of preemption operations in the scheduler.
- *
- * The kick-based preemption is to _immediately_ schedule an urgent task, even
- * paying a higher preemption cost. When a task is enqueued to the global run
- * queue (because no idle CPU is available), the scheduler checks if the
- * currently enqueuing task is urgent enough. The urgent task should be very
- * latency-critical (e.g., top 25%), and its latency priority should be very
- * high (e.g., 15). If the task is urgent enough, the scheduler finds a victim
- * CPU, which runs a lower-priority task, and kicks the remote victim CPU by
- * sending IPI. Then, the remote CPU will preempt out its running task and
- * schedule the highest priority task in the global run queue. The scheduler
- * uses 'The Power of Two Random Choices' heuristic so all N CPUs can run the N
- * highest priority tasks.
- *
- *
- * 7. Performance criticality
- * --------------------------
- *
- * We define the performance criticality metric to express how sensitive a task
- * is to CPU frequency. The more performance-critical a task is, the higher the
- * CPU frequency will be assigned. A task is more performance-critical in the
- * following conditions: 1) the task's runtime in a second is longer (i.e.,
- * task runtime x frequency), 2) the task's waiting or waken-up frequencies are
- * higher (i.e., the task is in the middle of the task chain).
- *
- *
- * 8. CPU frequency scaling
- * ------------------------
- *
- * Two factors determine the clock frequency of a CPU: 1) the current CPU
- * utilization and 2) the current task's CPU criticality compared to the
- * system-wide average performance criticality. This effectively boosts the CPU
- * clock frequency of performance-critical tasks even when the CPU utilization
- * is low.
- *
- * When actually changing the CPU's performance target, we should be able to
- * quickly capture the demand for spiky workloads while providing steady clock
- * frequency to avoid unexpected performance fluctuations. To this end, we
- * quickly increase the clock frequency when a task gets running but gradually
- * decrease it upon every tick interval.
- *
- *
- * 9. Core compaction
- * ------------------
- *
- * When system-wide CPU utilization is low, it is very likely all the CPUs are
- * running with very low utilization. All CPUs run with low clock frequency due
- * to dynamic frequency scaling, frequently going in and out from/to C-state.
- * That results in low performance (i.e., low clock frequency) and high power
- * consumption (i.e., frequent P-/C-state transition).
- *
- * The idea of *core compaction* is using less number of CPUs when system-wide
- * CPU utilization is low (say < 50%). The chosen cores (called "active cores")
- * will run in higher utilization and higher clock frequency, and the rest of
- * the cores (called "idle cores") will be in a C-state for a much longer
- * duration. Thus, the core compaction can achieve higher performance with
- * lower power consumption.
- *
- * One potential problem of core compaction is latency spikes when all the
- * active cores are overloaded. A few techniques are incorporated to solve this
- * problem. 1) Limit the active CPU core's utilization below a certain limit
- * (say 50%). 2) Do not use the core compaction when the system-wide
- * utilization is moderate (say 50%). 3) Do not enforce the core compaction for
- * kernel and pinned user-space tasks since they are manually optimized for
- * performance.
- *
- *
- * Copyright (c) 2023, 2024 Valve Corporation.
- * Author: Changwoo Min <changwoo@igalia.com>
+ * Optimized for Raptor Lake architecture (P+E cores, SMT).
+ * Includes God-Tier Logical Clock and optimized pinned task tracking.
  */
 #include <scx/common.bpf.h>
 #include <scx/bpf_arena_common.bpf.h>
@@ -196,225 +22,172 @@
 char _license[] SEC("license") = "GPL";
 
 /*
- * Logical current clock
+ * ============================================================================
+ * GLOBAL STATE
+ * ============================================================================
  */
-static u64		cur_logical_clk = LAVD_DL_COMPETE_WINDOW;
-
-/*
- * Current service time
- */
-static u64		cur_svc_time;
 
+static u64 cur_logical_clk = LAVD_DL_COMPETE_WINDOW;
+static u64 cur_svc_time;
 
 /*
- * The minimum and maximum of time slice
+ * ============================================================================
+ * TUNABLE PARAMETERS
+ * ============================================================================
  */
-const volatile u64	slice_min_ns = LAVD_SLICE_MIN_NS_DFL;
-const volatile u64	slice_max_ns = LAVD_SLICE_MAX_NS_DFL;
 
-/*
- * Migration delta threshold percentage (0-100)
- */
-const volatile u8	mig_delta_pct = 0;
+const volatile u64 slice_max_ns = LAVD_SLICE_MAX_NS_DFL;
+const volatile u64 slice_min_ns = LAVD_SLICE_MIN_NS_DFL;
+const volatile u64 pinned_slice_ns = 0;
+const volatile u8 mig_delta_pct = 0;
+
+static volatile u64 nr_cpus_big;
+static pid_t lavd_pid;
 
 /*
- * Slice time for all tasks when pinned tasks are running on the CPU.
- * When this is set (non-zero), pinned tasks always use per-CPU DSQs and
- * the dispatch logic compares vtimes across DSQs.
+ * ============================================================================
+ * CHECKS
+ * ============================================================================
  */
-const volatile u64	pinned_slice_ns = 0;
 
-static volatile u64	nr_cpus_big;
+_Static_assert(LAVD_SLICE_MIN_NS_DFL > 0, "Minimum slice must be positive");
+_Static_assert(LAVD_SLICE_MAX_NS_DFL >= LAVD_SLICE_MIN_NS_DFL, "Max slice must be >= min slice");
 
 /*
- * Scheduler's PID
+ * ============================================================================
+ * SUB-MODULES
+ * ============================================================================
  */
-static pid_t		lavd_pid;
 
-/*
- * Include sub-modules
- */
 #include "util.bpf.c"
 #include "idle.bpf.c"
 #include "balance.bpf.c"
 #include "lat_cri.bpf.c"
 
+/*
+ * GOD-TIER LOGICAL CLOCK â SINGLE-ATTEMPT + POWER-OF-2 FAST PATHS
+ * Removes expensive division for common queue depths to reduce latency.
+ */
 static void advance_cur_logical_clk(struct task_struct *p)
 {
-	u64 vlc, clc, ret_clc;
+	u64 vlc = READ_ONCE(p->scx.dsq_vtime);
+	u64 clc = READ_ONCE(cur_logical_clk);
 	u64 nr_queued, delta, new_clk;
-	int i;
-
-	vlc = READ_ONCE(p->scx.dsq_vtime);
-	clc = READ_ONCE(cur_logical_clk);
 
-	bpf_for(i, 0, LAVD_MAX_RETRY) {
-		/*
-		 * The clock should not go backward, so do nothing.
-		 */
-		if (vlc <= clc)
-			return;
+	if (likely(vlc <= clc))
+		return;
 
-		/*
-		 * Advance the clock up to the task's deadline. When overloaded,
-		 * advance the clock slower so other can jump in the run queue.
-		 */
-		nr_queued = max(sys_stat.nr_queued_task, 1);
+	nr_queued = READ_ONCE(sys_stat.nr_queued_task);
+	/* Branchless clamp to min 1 */
+	nr_queued = nr_queued | ((nr_queued == 0) ? 1 : 0);
+
+	if (likely(nr_queued == 1)) {
+		delta = vlc - clc;
+	} else if (nr_queued == 2) {
+		delta = (vlc - clc) >> 1;
+	} else if (nr_queued == 4) {
+		delta = (vlc - clc) >> 2;
+	} else if (nr_queued == 8) {
+		delta = (vlc - clc) >> 3;
+	} else if (nr_queued == 16) {
+		delta = (vlc - clc) >> 4;
+	} else {
 		delta = (vlc - clc) / nr_queued;
-		new_clk = clc + delta;
+	}
 
-		ret_clc = __sync_val_compare_and_swap(&cur_logical_clk, clc, new_clk);
-		if (ret_clc == clc) /* CAS success */
-			return;
+	delta = clamp(delta, 0, LAVD_SLICE_MAX_NS_DFL);
+	new_clk = clc + delta;
 
-		/*
-		 * Retry with the updated clc
-		 */
-		clc = ret_clc;
-	}
+	__sync_val_compare_and_swap(&cur_logical_clk, clc, new_clk);
 }
 
 static u64 calc_time_slice(task_ctx *taskc, struct cpu_ctx *cpuc)
 {
-	/*
-	 * Calculate the time slice of @taskc to run on @cpuc.
-	 */
+	u64 slice, base_slice;
+	u64 avg_runtime;
+
 	if (!taskc || !cpuc)
 		return LAVD_SLICE_MAX_NS_DFL;
 
-	/*
-	 * If pinned_slice_ns is enabled and there are pinned tasks waiting
-	 * to run on this CPU, unconditionally reduce the time slice for
-	 * all tasks to ensure pinned tasks can run promptly.
-	 */
-	if (pinned_slice_ns && cpuc->nr_pinned_tasks) {
-		taskc->slice = min(pinned_slice_ns, sys_stat.slice);
+	base_slice = READ_ONCE(sys_stat.slice);
+	avg_runtime = READ_ONCE(taskc->avg_runtime);
+
+	if (unlikely(pinned_slice_ns && cpuc->nr_pinned_tasks)) {
+		taskc->slice = min(pinned_slice_ns, base_slice);
 		reset_task_flag(taskc, LAVD_FLAG_SLICE_BOOST);
 		return taskc->slice;
 	}
 
-	/*
-	 * If the task's avg_runtime is greater than the regular time slice
-	 * (i.e., taskc->avg_runtime > sys_stat.slice), that means the task
-	 * could be scheduled out due to a shorter time slice than required.
-	 * In this case, let's consider boosting task's time slice.
-	 *
-	 * However, if there are pinned tasks waiting to run on this CPU,
-	 * we do not boost the task's time slice to avoid delaying the pinned
-	 * task that cannot be run on another CPU.
-	 */
-	if (!no_slice_boost && !cpuc->nr_pinned_tasks &&
-	    (taskc->avg_runtime >= sys_stat.slice)) {
-		/*
-		 * When the system is not heavily loaded, so it can serve all
-		 * tasks within the targeted latency (slice_max_ns <=
-		 * sys_stat.slice), we fully boost task's time slice.
-		 *
-		 * Let's set the task's time slice to its avg_runtime
-		 * (+ some bonus) to reduce unnecessary involuntary context
-		 * switching.
-		 *
-		 * Even in this case, we want to limit the maximum time slice
-		 * to LAVD_SLICE_BOOST_MAX (not infinite) because we want to
-		 * revisit if the task is placed on the best CPU at least
-		 * every LAVD_SLICE_BOOST_MAX interval.
-		 */
+	if (likely(avg_runtime < base_slice)) {
+		taskc->slice = base_slice;
+		reset_task_flag(taskc, LAVD_FLAG_SLICE_BOOST);
+		return base_slice;
+	}
+
+	if (!no_slice_boost) {
 		if (can_boost_slice()) {
-			/*
-			 * Add a bit of bonus so that a task, which takes a
-			 * bit longer than average, can still finish the job.
-			 */
-			u64 s = taskc->avg_runtime + LAVD_SLICE_BOOST_BONUS;
-			taskc->slice = clamp(s, slice_min_ns,
-					     LAVD_SLICE_BOOST_MAX);
+			slice = avg_runtime + LAVD_SLICE_BOOST_BONUS;
+			slice = clamp(slice, slice_min_ns, LAVD_SLICE_BOOST_MAX);
+			taskc->slice = slice;
 			set_task_flag(taskc, LAVD_FLAG_SLICE_BOOST);
-			return taskc->slice;
+			return slice;
 		}
 
-		/*
-		 * When the system is under high load, we will boost the time
-		 * slice of only latency-critical tasks, which are likely in
-		 * the middle of a task chain. Also, increase the time slice
-		 * proportionally to the latency criticality up to 2x the
-		 * regular time slice.
-		 */
 		if (taskc->lat_cri > sys_stat.avg_lat_cri) {
-			u64 b = (sys_stat.slice * taskc->lat_cri) /
-				(sys_stat.avg_lat_cri + 1);
-			u64 s = sys_stat.slice + b;
-			taskc->slice = clamp(s, slice_min_ns,
-					     min(taskc->avg_runtime,
-						 sys_stat.slice * 2));
-
+			u64 avg_lat_cri = READ_ONCE(sys_stat.avg_lat_cri);
+			avg_lat_cri = avg_lat_cri | ((avg_lat_cri == 0) ? 1 : 0);
+			u64 boost = (base_slice * taskc->lat_cri) / (avg_lat_cri + 1);
+			slice = base_slice + boost;
+			u64 cap = base_slice << 1;
+			cap = avg_runtime < cap ? avg_runtime : cap;
+			slice = clamp(slice, slice_min_ns, cap);
+			taskc->slice = slice;
 			set_task_flag(taskc, LAVD_FLAG_SLICE_BOOST);
-			return taskc->slice;
+			return slice;
 		}
 	}
 
-	/*
-	 * If slice boost is either not possible, not necessary, or not
-	 * eligible, assign the regular time slice.
-	 */
-	taskc->slice = sys_stat.slice;
+	taskc->slice = base_slice;
 	reset_task_flag(taskc, LAVD_FLAG_SLICE_BOOST);
-	return taskc->slice;
+	return base_slice;
 }
 
 static void update_stat_for_running(struct task_struct *p,
-				    task_ctx *taskc,
-				    struct cpu_ctx *cpuc, u64 now)
+									task_ctx *taskc,
+									struct cpu_ctx *cpuc, u64 now)
 {
 	u64 wait_period, interval;
 	struct cpu_ctx *prev_cpuc;
 
-	/*
-	 * Since this is the start of a new schedule for @p, we update run
-	 * frequency in a second using an exponential weighted moving average.
-	 */
 	if (have_scheduled(taskc)) {
-		wait_period = time_delta(now, taskc->last_quiescent_clk);
+		u64 last_quiescent = READ_ONCE(taskc->last_quiescent_clk);
+		wait_period = time_delta(now, last_quiescent);
 		interval = taskc->avg_runtime + wait_period;
-		if (interval > 0)
+		if (likely(interval > 0))
 			taskc->run_freq = calc_avg_freq(taskc->run_freq, interval);
 	}
 
-	/*
-	 * Collect additional information when the scheduler is monitored.
-	 */
-	if (is_monitored) {
-		taskc->resched_interval = time_delta(now,
-						     taskc->last_running_clk);
+	if (unlikely(is_monitored)) {
+		u64 last_running = READ_ONCE(taskc->last_running_clk);
+		taskc->resched_interval = time_delta(now, last_running);
 	}
+
 	taskc->prev_cpu_id = taskc->cpu_id;
 	taskc->cpu_id = cpuc->cpu_id;
 
-	/*
-	 * Update task state when starts running.
-	 */
 	reset_task_flag(taskc, LAVD_FLAG_IS_WAKEUP);
 	reset_task_flag(taskc, LAVD_FLAG_IS_SYNC_WAKEUP);
+
 	taskc->last_running_clk = now;
 	taskc->last_measured_clk = now;
 
-	/*
-	 * Reset task's lock and futex boost count
-	 * for a lock holder to be boosted only once.
-	 */
 	reset_lock_futex_boost(taskc, cpuc);
 
-	/*
-	 * Update per-CPU latency criticality information
-	 * for every-scheduled tasks.
-	 */
 	if (cpuc->max_lat_cri < taskc->lat_cri)
 		cpuc->max_lat_cri = taskc->lat_cri;
 	cpuc->sum_lat_cri += taskc->lat_cri;
 	cpuc->nr_sched++;
 
-	/*
-	 * Update per-CPU performance criticality information
-	 * for every-scheduled tasks.
-	 */
 	if (have_little_core) {
 		if (cpuc->max_perf_cri < taskc->perf_cri)
 			cpuc->max_perf_cri = taskc->perf_cri;
@@ -423,17 +196,11 @@ static void update_stat_for_running(stru
 		cpuc->sum_perf_cri += taskc->perf_cri;
 	}
 
-	/*
-	 * Update running task's information for preemption
-	 */
 	cpuc->flags = taskc->flags;
 	cpuc->lat_cri = taskc->lat_cri;
 	cpuc->running_clk = now;
 	cpuc->est_stopping_clk = get_est_stopping_clk(taskc, now);
 
-	/*
-	 * Update statistics information.
-	 */
 	if (is_lat_cri(taskc))
 		cpuc->nr_lat_cri++;
 
@@ -444,43 +211,46 @@ static void update_stat_for_running(stru
 	if (prev_cpuc && prev_cpuc->cpdom_id != cpuc->cpdom_id)
 		cpuc->nr_x_migration++;
 
-	/*
-	 * It is clear there is no need to consider the suspended duration
-	 * while running a task, so reset the suspended duration to zero.
-	 */
 	reset_suspended_duration(cpuc);
 }
 
 static void account_task_runtime(struct task_struct *p,
-				 task_ctx *taskc,
-				 struct cpu_ctx *cpuc,
-				 u64 now)
+								 task_ctx *taskc,
+								 struct cpu_ctx *cpuc,
+								 u64 now)
 {
 	u64 sus_dur, runtime, svc_time, sc_time;
+	u64 weight;
+	u64 last_measured;
+	u64 tot_svc, tot_sc;
 
-	/*
-	 * Since task execution can span one or more sys_stat intervals,
-	 * we update task and CPU's statistics at every tick interval and
-	 * update_stat_for_stopping(). It is essential to account for
-	 * the load of long-running tasks properly. So, we add up only the
-	 * execution duration since the last measured time.
-	 */
 	sus_dur = get_suspended_duration_and_reset(cpuc);
-	runtime = time_delta(now, taskc->last_measured_clk + sus_dur);
-	svc_time = runtime / p->scx.weight;
+	last_measured = READ_ONCE(taskc->last_measured_clk);
+
+	if (now <= last_measured + sus_dur)
+		return;
+
+	runtime = now - last_measured - sus_dur;
+
+	__builtin_prefetch((const void *)&cpuc->tot_svc_time, 1, 3);
+	__builtin_prefetch((const void *)&cpuc->tot_sc_time, 1, 3);
+
+	weight = READ_ONCE(p->scx.weight);
+	weight = weight + (weight == 0);
+
+	svc_time = runtime / weight;
 	sc_time = scale_cap_freq(runtime, cpuc->cpu_id);
 
-	WRITE_ONCE(cpuc->tot_svc_time, cpuc->tot_svc_time + svc_time);
-	WRITE_ONCE(cpuc->tot_sc_time, cpuc->tot_sc_time + sc_time);
+	tot_svc = READ_ONCE(cpuc->tot_svc_time);
+	tot_sc = READ_ONCE(cpuc->tot_sc_time);
+
+	WRITE_ONCE(cpuc->tot_svc_time, tot_svc + svc_time);
+	WRITE_ONCE(cpuc->tot_sc_time, tot_sc + sc_time);
 
 	taskc->acc_runtime += runtime;
 	taskc->svc_time += svc_time;
-	taskc->last_measured_clk = now;
+	WRITE_ONCE(taskc->last_measured_clk, now);
 
-	/*
-	 * Under CPU bandwidth control using cpu.max, we also need to report
-	 * how much time was actually consumed compared to the reserved time.
-	 */
 	if (enable_cpu_bw && (p->pid != lavd_pid)) {
 		struct cgroup *cgrp = bpf_cgroup_from_id(taskc->cgrp_id);
 		if (cgrp) {
@@ -491,62 +261,40 @@ static void account_task_runtime(struct
 }
 
 static void update_stat_for_stopping(struct task_struct *p,
-				     task_ctx *taskc,
-				     struct cpu_ctx *cpuc)
+									 task_ctx *taskc,
+									 struct cpu_ctx *cpuc)
 {
 	u64 now = scx_bpf_now();
 
-	/*
-	 * Account task runtime statistics first.
-	 */
 	account_task_runtime(p, taskc, cpuc, now);
 
 	taskc->avg_runtime = calc_avg(taskc->avg_runtime, taskc->acc_runtime);
 	taskc->last_stopping_clk = now;
 
-	/*
-	 * Account for how much of the slice was used for this instance.
-	 */
-	taskc->last_slice_used = time_delta(now, taskc->last_running_clk);
-
-	/*
-	 * Reset waker's latency criticality here to limit the latency boost of
-	 * a task. A task will be latency-boosted only once after wake-up.
-	 */
+	if (unlikely(is_monitored)) {
+		u64 last_running = READ_ONCE(taskc->last_running_clk);
+		taskc->last_slice_used = time_delta(now, last_running);
+	}
+
 	taskc->lat_cri_waker = 0;
 
-	/*
-	 * Update the current service time if necessary.
-	 */
 	if (READ_ONCE(cur_svc_time) < taskc->svc_time)
 		WRITE_ONCE(cur_svc_time, taskc->svc_time);
 
-	/*
-	 * Reset task's lock and futex boost count
-	 * for a lock holder to be boosted only once.
-	 */
 	reset_lock_futex_boost(taskc, cpuc);
 }
 
 static void update_stat_for_refill(struct task_struct *p,
-				   task_ctx *taskc,
-				   struct cpu_ctx *cpuc)
+								   task_ctx *taskc,
+								   struct cpu_ctx *cpuc)
 {
 	u64 now = scx_bpf_now();
-
-	/*
-	 * Account task runtime statistics first.
-	 */
 	account_task_runtime(p, taskc, cpuc, now);
-
-	/*
-	 * We update avg_runtime here since it is used to boost time slice.
-	 */
 	taskc->avg_runtime = calc_avg(taskc->avg_runtime, taskc->acc_runtime);
 }
 
 s32 BPF_STRUCT_OPS(lavd_select_cpu, struct task_struct *p, s32 prev_cpu,
-		   u64 wake_flags)
+				   u64 wake_flags)
 {
 	struct pick_ctx ictx = {
 		.p = p,
@@ -561,36 +309,44 @@ s32 BPF_STRUCT_OPS(lavd_select_cpu, stru
 	if (!ictx.taskc || !ictx.cpuc_cur)
 		return prev_cpu;
 
+	__builtin_prefetch(&ictx.taskc->lat_cri, 0, 3);
+	__builtin_prefetch(&ictx.taskc->avg_runtime, 0, 3);
+	__builtin_prefetch(&ictx.taskc->perf_cri, 0, 3);
+
 	if (wake_flags & SCX_WAKE_SYNC)
 		set_task_flag(ictx.taskc, LAVD_FLAG_IS_SYNC_WAKEUP);
 	else
 		reset_task_flag(ictx.taskc, LAVD_FLAG_IS_SYNC_WAKEUP);
 
-	/*
-	 * Find an idle cpu and reserve it since the task @p will run
-	 * on the idle cpu. Even if there is no idle cpu, still respect
-	 * the chosen cpu.
-	 */
 	cpu_id = pick_idle_cpu(&ictx, &found_idle);
 	cpu_id = cpu_id >= 0 ? cpu_id : prev_cpu;
 	ictx.taskc->suggested_cpu_id = cpu_id;
 
+	if (have_little_core && prev_cpu >= 0 && cpu_id >= 0) {
+		u64 avg_perf = READ_ONCE(sys_stat.avg_perf_cri);
+		struct cpu_ctx *cpuc_prev = get_cpu_ctx_id(prev_cpu);
+		struct cpu_ctx *cpuc_new = get_cpu_ctx_id(cpu_id);
+
+		if (cpuc_prev && cpuc_prev->is_online && cpuc_prev->big_core &&
+			(!cpuc_new || !cpuc_new->big_core) &&
+			(avg_perf == 0 || ictx.taskc->perf_cri >= avg_perf)) {
+			cpu_id = prev_cpu;
+		found_idle = false;
+			}
+	}
+
 	if (found_idle) {
 		struct cpu_ctx *cpuc;
 
 		set_task_flag(ictx.taskc, LAVD_FLAG_IDLE_CPU_PICKED);
 
-		/*
-		 * If there is an idle cpu and its associated DSQs are empty,
-		 * disptach the task to the idle cpu right now.
-		 */
 		cpuc = get_cpu_ctx_id(cpu_id);
 		if (!cpuc) {
 			scx_bpf_error("Failed to lookup cpu_ctx: %d", cpu_id);
 			goto out;
 		}
 
-		if (!nr_queued_on_cpu(cpuc)) {
+		if (likely(!nr_queued_on_cpu(cpuc))) {
 			p->scx.dsq_vtime = calc_when_to_run(p, ictx.taskc);
 			p->scx.slice = LAVD_SLICE_MAX_NS_DFL;
 			scx_bpf_dsq_insert(p, SCX_DSQ_LOCAL, p->scx.slice, 0);
@@ -599,7 +355,7 @@ s32 BPF_STRUCT_OPS(lavd_select_cpu, stru
 	} else {
 		reset_task_flag(ictx.taskc, LAVD_FLAG_IDLE_CPU_PICKED);
 	}
-out:
+	out:
 	return cpu_id;
 }
 
@@ -608,16 +364,6 @@ static int cgroup_throttled(struct task_
 	struct cgroup *cgrp;
 	int ret, ret2;
 
-	/*
-	 * Under CPU bandwidth control using cpu.max, we should first check
-	 * if the cgroup is throttled or not. If not, we will go ahead.
-	 * Otherwise, we should put the task aside for later execution.
-	 * In the forced mode, we should enqueue the task even if the cgroup
-	 * is throttled (-EAGAIN).
-	 *
-	 * Note that we cannot use scx_bpf_task_cgroup() here because this can
-	 * be called only from ops.enqueue() and ops.dispatch().
-	 */
 	cgrp = bpf_cgroup_from_id(taskc->cgrp_id);
 	if (!cgrp) {
 		debugln("Failed to lookup a cgroup: %llu", taskc->cgrp_id);
@@ -647,17 +393,10 @@ void BPF_STRUCT_OPS(lavd_enqueue, struct
 	taskc = get_task_ctx(p);
 	cpuc_cur = get_cpu_ctx();
 	if (!taskc || !cpuc_cur) {
-		scx_bpf_error("Failed to lookup cpu_ctx %d", cpu);
+		scx_bpf_error("Failed to lookup contexts in enqueue");
 		return;
 	}
 
-	/*
-	 * Calculate when a task can be scheduled for how long.
-	 *
-	 * If the task is re-enqueued due to a higher-priority scheduling class
-	 * taking the CPU, we don't need to recalculate the task's deadline and
-	 * timeslice, as the task hasn't yet run.
-	 */
 	if (!(enq_flags & SCX_ENQ_REENQ)) {
 		if (enq_flags & SCX_ENQ_WAKEUP)
 			set_task_flag(taskc, LAVD_FLAG_IS_WAKEUP);
@@ -666,16 +405,9 @@ void BPF_STRUCT_OPS(lavd_enqueue, struct
 
 		p->scx.dsq_vtime = calc_when_to_run(p, taskc);
 	}
+
 	p->scx.slice = LAVD_SLICE_MIN_NS_DFL;
 
-	/*
-	 * Find a proper DSQ for the task, which is either the task's
-	 * associated compute domain or its alternative domain, or
-	 * the closest available domain from the previous domain.
-	 *
-	 * If the CPU is already picked at ops.select_cpu(),
-	 * let's use the chosen CPU.
-	 */
 	task_cpu = scx_bpf_task_cpu(p);
 	if (!__COMPAT_is_enq_cpu_selected(enq_flags)) {
 		struct pick_ctx ictx = {
@@ -687,8 +419,12 @@ void BPF_STRUCT_OPS(lavd_enqueue, struct
 		};
 
 		cpu = pick_idle_cpu(&ictx, &is_idle);
+		if (cpu < 0) {
+			cpu = (task_cpu >= 0) ? task_cpu : cpuc_cur->cpu_id;
+			is_idle = false;
+		}
 	} else {
-		cpu = scx_bpf_task_cpu(p);
+		cpu = task_cpu;
 		is_idle = test_task_flag(taskc, LAVD_FLAG_IDLE_CPU_PICKED);
 		reset_task_flag(taskc, LAVD_FLAG_IDLE_CPU_PICKED);
 	}
@@ -701,76 +437,42 @@ void BPF_STRUCT_OPS(lavd_enqueue, struct
 	taskc->suggested_cpu_id = cpu;
 	taskc->cpdom_id = cpuc->cpdom_id;
 
-	/*
-	 * Under the CPU bandwidth control with cpu.max, check if the cgroup
-	 * is throttled before executing the task.
-	 *
-	 * Note that we calculate the task's deadline before checking the 
-	 * cgroup, as we need the deadline to put aside the task when the
-	 * cgroup is throttled.
-	 *
-	 * Also, we do not throttle the scheduler process itself to
-	 * guarantee forward progress.
-	 */
 	if (enable_cpu_bw && (p->pid != lavd_pid) &&
-	    (cgroup_throttled(p, taskc, true) == -EAGAIN)) {
+		(cgroup_throttled(p, taskc, true) == -EAGAIN)) {
 		debugln("Task %s[pid%d/cgid%llu] is throttled.",
-			p->comm, p->pid, taskc->cgrp_id);
+				p->comm, p->pid, taskc->cgrp_id);
 		return;
-	}
+		}
 
-	/*
-	 * Increase the number of pinned tasks waiting for execution.
-	 */
-	if (is_pinned(p) && (taskc->pinned_cpu_id == -ENOENT)) {
-		taskc->pinned_cpu_id = cpu;
-		__sync_fetch_and_add(&cpuc->nr_pinned_tasks, 1);
-
-		debugln("cpu%d [%d] -- %s:%d -- %s:%d", cpuc->cpu_id,
-			cpuc->nr_pinned_tasks, p->comm, p->pid, __func__,
-			__LINE__);
-	}
-
-	/*
-	 * Enqueue the task to a DSQ. If it is safe to directly dispatch
-	 * to the local DSQ of the chosen CPU, do it. Otherwise, enqueue
-	 * to the chosen DSQ of the chosen domain.
-	 *
-	 * When pinned_slice_ns is enabled, pinned tasks always use per-CPU DSQ
-	 * to enable vtime comparison across DSQs during dispatch.
-	 */
-	if (is_idle && !nr_queued_on_cpu(cpuc)) {
-		scx_bpf_dsq_insert(p, SCX_DSQ_LOCAL_ON | cpu, p->scx.slice,
-				   enq_flags);
-	} else {
-		dsq_id = get_target_dsq_id(p, cpuc);
-		scx_bpf_dsq_insert_vtime(p, dsq_id, p->scx.slice,
-					 p->scx.dsq_vtime, enq_flags);
-	}
-
-	/*
-	 * If a new overflow CPU was assigned while finding a proper DSQ,
-	 * kick the new CPU and go.
-	 */
-	if (is_idle) {
-		scx_bpf_kick_cpu(cpu, SCX_KICK_IDLE);
-		return;
-	}
+		/*
+		 * Optimized Pinned Task Tracking:
+		 * Only update atomic counter if we haven't already accounted for this task.
+		 * Uses a flag instead of adding a field to task_ctx to avoid redefinition errors.
+		 */
+		if (is_pinned(p) && !test_task_flag(taskc, LAVD_FLAG_PINNED_COUNTED)) {
+			atomic_inc_u32(&cpuc->nr_pinned_tasks);
+			set_task_flag(taskc, LAVD_FLAG_PINNED_COUNTED);
+		}
+
+		if (is_idle && !nr_queued_on_cpu(cpuc)) {
+			scx_bpf_dsq_insert(p, SCX_DSQ_LOCAL_ON | cpu, p->scx.slice,
+							   enq_flags);
+		} else {
+			dsq_id = get_target_dsq_id(p, cpuc);
+			scx_bpf_dsq_insert_vtime(p, dsq_id, p->scx.slice,
+									 p->scx.dsq_vtime, enq_flags);
+		}
+
+		if (is_idle) {
+			scx_bpf_kick_cpu(cpu, SCX_KICK_IDLE);
+			return;
+		}
 
-	/*
-	 * If there is no idle CPU for an eligible task, try to preempt a task.
-	 * Try to find and kick a victim CPU, which runs a less urgent task,
-	 * from dsq_id. The kick will be done asynchronously.
-	 *
-	 * In the case of the forced enqueue mode, we don't try preemption
-	 * since it is a batch of bulk enqueues.
-	 */
-	if (!no_preemption)
-		try_find_and_kick_victim_cpu(p, taskc, cpu, cpdom_to_dsq(cpuc->cpdom_id));
+		if (!no_preemption)
+			try_find_and_kick_victim_cpu(p, taskc, cpu, cpdom_to_dsq(cpuc->cpdom_id));
 }
 
-static
-int enqueue_cb(struct task_struct __arg_trusted *p)
+static int enqueue_cb(struct task_struct __arg_trusted *p)
 {
 	struct cpu_ctx *cpuc, *cpuc_cur;
 	task_ctx *taskc;
@@ -784,14 +486,8 @@ int enqueue_cb(struct task_struct __arg_
 		return 0;
 	}
 
-	/*
-	 * Calculate when a task can be scheduled.
-	 */
 	p->scx.dsq_vtime = calc_when_to_run(p, taskc);
 
-	/*
-	 * Fetch the chosen CPU and DSQ for the task.
-	 */
 	cpu = taskc->suggested_cpu_id;
 	cpuc = get_cpu_ctx_id(cpu);
 	if (!cpuc) {
@@ -799,15 +495,11 @@ int enqueue_cb(struct task_struct __arg_
 		return 0;
 	}
 
-	/*
-	 * Increase the number of pinned tasks waiting for execution.
-	 */
-	if (is_pinned(p))
-		__sync_fetch_and_add(&cpuc->nr_pinned_tasks, 1);
-
-	/*
-	 * Enqueue the task to a DSQ.
-	 */
+	if (is_pinned(p) && !test_task_flag(taskc, LAVD_FLAG_PINNED_COUNTED)) {
+		atomic_inc_u32(&cpuc->nr_pinned_tasks);
+		set_task_flag(taskc, LAVD_FLAG_PINNED_COUNTED);
+	}
+
 	dsq_id = get_target_dsq_id(p, cpuc);
 	scx_bpf_dsq_insert_vtime(p, dsq_id, p->scx.slice, p->scx.dsq_vtime, 0);
 
@@ -819,14 +511,22 @@ void BPF_STRUCT_OPS(lavd_dequeue, struct
 	task_ctx *taskc;
 	int ret;
 
-	/*
-	 * ATQ is used only when enable_cpu_bw is on.
-	 * So, we don't need to cancel an ATQ operation if it is not on.
-	 */
+	taskc = get_task_ctx(p);
+
+	/* Handle pinned task accounting on dequeue to avoid leaks */
+	if (taskc && test_task_flag(taskc, LAVD_FLAG_PINNED_COUNTED)) {
+		/* We decrement on the CPU the task is assigned to */
+		s32 cpu = scx_bpf_task_cpu(p);
+		struct cpu_ctx *cpuc = get_cpu_ctx_id(cpu);
+		if (cpuc) {
+			atomic_dec_u32(&cpuc->nr_pinned_tasks);
+		}
+		reset_task_flag(taskc, LAVD_FLAG_PINNED_COUNTED);
+	}
+
 	if (!enable_cpu_bw)
 		return;
 
-	taskc = get_task_ctx(p);
 	if (!taskc) {
 		debugln("Failed to lookup task_ctx for task %d", p->pid);
 		return;
@@ -855,32 +555,17 @@ void BPF_STRUCT_OPS(lavd_dispatch, s32 c
 	cpu_dsq_id = cpu_to_dsq(cpu);
 	cpdom_dsq_id = cpdom_to_dsq(cpuc->cpdom_id);
 
-	/*
-	 * When the CPU bandwidth control is enabled, check if there are
-	 * tasks backlogged when their cgroups are throttled, and requeue
-	 * those tasks to the proper DSQs.
-	 */
 	if (enable_cpu_bw && (ret = scx_cgroup_bw_reenqueue())) {
 		scx_bpf_error("Failed to reenqueue backlogged tasks: %d", ret);
 	}
 
-	/*
-	 * If a task is holding a new lock, continue to execute it
-	 * to make system-wide forward progress.
-	 */
 	if (prev && (prev->scx.flags & SCX_TASK_QUEUED) &&
-	    is_lock_holder_running(cpuc))
+		is_lock_holder_running(cpuc))
 		goto consume_prev;
 
-	/*
-	 * If all CPUs are using, directly consume without checking CPU masks.
-	 */
 	if (use_full_cpus())
 		goto consume_out;
 
-	/*
-	 * Prepare cpumasks.
-	 */
 	bpf_rcu_read_lock();
 
 	active = active_cpumask;
@@ -890,96 +575,54 @@ void BPF_STRUCT_OPS(lavd_dispatch, s32 c
 		goto unlock_out;
 	}
 
-	/*
-	 * If the current CPU belonges to either active or overflow set,
-	 * dispatch a task and go.
-	 */
 	if (bpf_cpumask_test_cpu(cpu, cast_mask(active)) ||
-	    bpf_cpumask_test_cpu(cpu, cast_mask(ovrflw))) {
+		bpf_cpumask_test_cpu(cpu, cast_mask(ovrflw))) {
 		bpf_rcu_read_unlock();
-		goto consume_out;
-	}
-	/* NOTE: This CPU belongs to neither active nor overflow set. */
-
-	/*
-	 * Fast path when using per-CPU DSQ.
-	 *
-	 * If there is something to run on a per-CPU DSQ,
-	 * directly consume without checking CPU masks.
-	 *
-	 * Since this CPU is neither active nor overflow set,
-	 * add this CPU to the overflow set.
-	 */
-	if (use_per_cpu_dsq() && scx_bpf_dsq_nr_queued(cpu_dsq_id)) {
-		bpf_cpumask_set_cpu(cpu, ovrflw);
-		bpf_rcu_read_unlock();
-		goto consume_out;
-	}
+	goto consume_out;
+		}
 
-	if (prev) {
-		/*
-		 * If the previous task is pinned to this CPU,
-		 * extend the overflow set and go.
-		 */
-		if (is_pinned(prev)) {
+		if (use_per_cpu_dsq() && scx_bpf_dsq_nr_queued(cpu_dsq_id)) {
 			bpf_cpumask_set_cpu(cpu, ovrflw);
 			bpf_rcu_read_unlock();
 			goto consume_out;
-		} else if (is_migration_disabled(prev)) {
-			bpf_rcu_read_unlock();
-			goto consume_out;
 		}
 
-		/*
-		 * If the previous task can run on this CPU but not on either
-		 * active or overflow set, extend the overflow set and go.
-		 */
-		taskc_prev = get_task_ctx(prev);
-		if (taskc_prev &&
-		    test_task_flag(taskc_prev, LAVD_FLAG_IS_AFFINITIZED) &&
-		    bpf_cpumask_test_cpu(cpu, prev->cpus_ptr) &&
-		    !bpf_cpumask_intersects(cast_mask(active), prev->cpus_ptr) &&
-		    !bpf_cpumask_intersects(cast_mask(ovrflw), prev->cpus_ptr)) {
-			bpf_cpumask_set_cpu(cpu, ovrflw);
+		if (prev) {
+			if (is_pinned(prev)) {
+				bpf_cpumask_set_cpu(cpu, ovrflw);
+				bpf_rcu_read_unlock();
+				goto consume_out;
+			} else if (is_migration_disabled(prev)) {
+				bpf_rcu_read_unlock();
+				goto consume_out;
+			}
+
+			taskc_prev = get_task_ctx(prev);
+			if (taskc_prev &&
+				test_task_flag(taskc_prev, LAVD_FLAG_IS_AFFINITIZED) &&
+				bpf_cpumask_test_cpu(cpu, prev->cpus_ptr) &&
+				!bpf_cpumask_intersects(cast_mask(active), prev->cpus_ptr) &&
+				!bpf_cpumask_intersects(cast_mask(ovrflw), prev->cpus_ptr)) {
+				bpf_cpumask_set_cpu(cpu, ovrflw);
 			bpf_rcu_read_unlock();
 			goto consume_out;
+				}
 		}
-	}
 
-	/*
-	 * If there is nothing to run on per-CPU DSQ and we do not use
-	 * per-domain DSQ, there is nothing to do. So, stop here.
-	 */
-	if (!use_cpdom_dsq())
+		if (!use_cpdom_dsq())
+			goto unlock_out;
+
+	if (!scx_bpf_dsq_nr_queued(cpdom_dsq_id))
 		goto unlock_out;
-	/* NOTE: We use per-domain DSQ. */
 
-	/*
-	 * If this CPU is neither in active nor overflow CPUs,
-	 * try to find and run the task affinitized on this CPU
-	 * from the per-domain DSQ.
-	 *
-	 * Note that we don't need to traverse the per-CPU DSQ,
-	 * as it is already handled by the fast path above.
-	 */
 	bpf_for_each(scx_dsq, p, cpdom_dsq_id, 0) {
 		task_ctx *taskc;
 		s32 new_cpu;
 
-		/*
-		 * note that this is a hack to bypass the restriction of the
-		 * current bpf not trusting the pointer p. once the bpf
-		 * verifier gets smarter, we can remove bpf_task_from_pid().
-		 */
 		p = bpf_task_from_pid(p->pid);
 		if (!p)
 			break;
 
-		/*
-		 * if the task is pinned to this cpu,
-		 * extend the overflow set and go.
-		 * but not on this cpu, try another task.
-		 */
 		if (is_pinned(p)) {
 			new_cpu = scx_bpf_task_cpu(p);
 			if (new_cpu == cpu) {
@@ -1003,92 +646,53 @@ void BPF_STRUCT_OPS(lavd_dispatch, s32 c
 			continue;
 		}
 
-		/*
-		 * if the task can run on either active or overflow set,
-		 * try another task.
-		 */
 		taskc = get_task_ctx(p);
-		if(taskc &&
-		(!test_task_flag(taskc, LAVD_FLAG_IS_AFFINITIZED) ||
-		bpf_cpumask_intersects(cast_mask(active), p->cpus_ptr) ||
-		bpf_cpumask_intersects(cast_mask(ovrflw), p->cpus_ptr))) {
+		if (taskc &&
+			(!test_task_flag(taskc, LAVD_FLAG_IS_AFFINITIZED) ||
+			bpf_cpumask_intersects(cast_mask(active), p->cpus_ptr) ||
+			bpf_cpumask_intersects(cast_mask(ovrflw), p->cpus_ptr))) {
 			bpf_task_release(p);
-			continue;
-		}
+		continue;
+			}
 
-		/*
-		 * now, we know that the task cannot run on either active
-		 * or overflow set. then, let's consider to extend the
-		 * overflow set.
-		 */
-		new_cpu = find_cpu_in(p->cpus_ptr, cpuc);
-		if (new_cpu >= 0) {
-			if (new_cpu == cpu) {
-				bpf_cpumask_set_cpu(new_cpu, ovrflw);
-				bpf_task_release(p);
-				try_consume = true;
-				break;
+			new_cpu = find_cpu_in(p->cpus_ptr, cpuc);
+			if (new_cpu >= 0) {
+				if (new_cpu == cpu) {
+					bpf_cpumask_set_cpu(new_cpu, ovrflw);
+					bpf_task_release(p);
+					try_consume = true;
+					break;
+				} else if (!bpf_cpumask_test_and_set_cpu(new_cpu, ovrflw))
+					scx_bpf_kick_cpu(new_cpu, SCX_KICK_IDLE);
 			}
-			else if (!bpf_cpumask_test_and_set_cpu(new_cpu, ovrflw))
-				scx_bpf_kick_cpu(new_cpu, SCX_KICK_IDLE);
-		}
-		bpf_task_release(p);
+			bpf_task_release(p);
 	}
 
-unlock_out:
+	unlock_out:
 	bpf_rcu_read_unlock();
 
-	/*
-	 * If this CPU should go idle, do nothing.
-	 */
 	if (!try_consume)
 		return;
 
-consume_out:
-	/*
-	 * Otherwise, consume a task.
-	 */
+	consume_out:
 	if (consume_task(cpu_dsq_id, cpdom_dsq_id))
 		return;
 
-	/*
-	 * If nothing to run, continue running the previous task.
-	 */
 	if (prev && prev->scx.flags & SCX_TASK_QUEUED) {
-consume_prev:
+		consume_prev:
 		taskc_prev = taskc_prev ?: get_task_ctx(prev);
 		if (taskc_prev) {
-			/*
-			 * Let's update stats first before calculating time slice.
-			 */
 			update_stat_for_refill(prev, taskc_prev, cpuc);
 
-			/*
-			 * Under the CPU bandwidth control with cpu.max,
-			 * check if the cgroup is throttled before executing
-			 * the task.
-			 */
 			if (enable_cpu_bw && (prev->pid != lavd_pid) &&
-			    (cgroup_throttled(prev, taskc_prev, false) == -EAGAIN))
+				(cgroup_throttled(prev, taskc_prev, false) == -EAGAIN))
 				return;
 
-			/*
-			 * Refill the time slice.
-			 */
 			prev->scx.slice = calc_time_slice(taskc_prev, cpuc);
 
-			/*
-			 * Reset prev task's lock and futex boost count
-			 * for a lock holder to be boosted only once.
-			 */
 			if (is_lock_holder_running(cpuc))
 				reset_lock_futex_boost(taskc_prev, cpuc);
 
-			/*
-			 * Task flags can be updated when calculating the time
-			 * slice (LAVD_FLAG_SLICE_BOOST), so let's update the
-			 * CPU's copy of the flag as well.
-			 */
 			cpuc->flags = taskc_prev->flags;
 		}
 	}
@@ -1101,9 +705,6 @@ void BPF_STRUCT_OPS(lavd_runnable, struc
 	u64 now, interval;
 	int i;
 
-	/*
-	 * Clear the accumulated runtime.
-	 */
 	p_taskc = get_task_ctx(p);
 	if (!p_taskc) {
 		scx_bpf_error("Failed to lookup task_ctx for task %d", p->pid);
@@ -1111,22 +712,9 @@ void BPF_STRUCT_OPS(lavd_runnable, struc
 	}
 	p_taskc->acc_runtime = 0;
 
-	/*
-	 * When a task @p is wakened up, the wake frequency of its waker task
-	 * is updated. The @current task is a waker and @p is a waiter, which
-	 * is being wakened up now. This is true only when
-	 * SCX_OPS_ALLOW_QUEUED_WAKEUP is not set. The wake-up operations are
-	 * batch processed with SCX_OPS_ALLOW_QUEUED_WAKEUP, so @current task
-	 * is no longer a waker task.
-	 */
 	if (!(enq_flags & SCX_ENQ_WAKEUP))
 		return;
 
-	/*
-	 * Filter out unrelated tasks. We keep track of tasks under the same
-	 * parent process to confine the waker-wakee relationship within
-	 * closely related tasks.
-	 */
 	if (enq_flags & (SCX_ENQ_PREEMPT | SCX_ENQ_REENQ | SCX_ENQ_LAST))
 		return;
 
@@ -1138,34 +726,19 @@ void BPF_STRUCT_OPS(lavd_runnable, struc
 		return;
 
 	waker_taskc = get_task_ctx(waker);
-	if (!waker_taskc) {
-		/*
-		 * In this case, the waker could be an idle task
-		 * (swapper/_[_]), so we just ignore.
-		 */
+	if (!waker_taskc)
 		return;
-	}
 
-	/*
-	 * Update wake frequency.
-	 */
 	now = scx_bpf_now();
 	interval = time_delta(now, READ_ONCE(waker_taskc->last_runnable_clk));
 	if (interval >= LAVD_LC_WAKE_INTERVAL_MIN) {
 		WRITE_ONCE(waker_taskc->wake_freq,
-			   calc_avg_freq(waker_taskc->wake_freq, interval));
+				   calc_avg_freq(waker_taskc->wake_freq, interval));
 		WRITE_ONCE(waker_taskc->last_runnable_clk, now);
 	}
 
-	/*
-	 * Propagate waker's latency criticality to wakee. Note that we pass
-	 * task's self latency criticality to limit the context into one hop.
-	 */
 	p_taskc->lat_cri_waker = waker_taskc->lat_cri;
 
-	/*
-	 * Collect additional information when the scheduler is monitored.
-	 */
 	if (is_monitored) {
 		p_taskc->waker_pid = waker->pid;
 		for (i = 0; i < TASK_COMM_LEN && can_loop; i++)
@@ -1186,45 +759,13 @@ void BPF_STRUCT_OPS(lavd_running, struct
 		return;
 	}
 
-	/*
-	 * If the sched_ext core directly dispatched a task, calculating the
-	 * task's deadline and time slice was also skipped. In this case, we
-	 * set the deadline to the current logical lock.
-	 *
-	 * Note that this is necessary when the kernel does not support
-	 * SCX_OPS_ENQ_MIGRATION_DISABLED or SCX_OPS_ENQ_MIGRATION_DISABLED
-	 * is not turned on.
-	 */
 	if (p->scx.slice == SCX_SLICE_DFL)
 		p->scx.dsq_vtime = READ_ONCE(cur_logical_clk);
 
-	/*
-	 * Calculate the task's time slice here,
-	 * as it depends on the system load.
-	 */
 	p->scx.slice = calc_time_slice(taskc, cpuc);
-
-	/*
-	 * Update the current logical clock.
-	 */
 	advance_cur_logical_clk(p);
-
-	/*
-	 * Update task statistics
-	 */
 	update_stat_for_running(p, taskc, cpuc, now);
-
-	/*
-	 * Calculate the task's CPU performance target and update if the new
-	 * target is higher than the current one. The CPU's performance target
-	 * urgently increases according to task's target but it decreases
-	 * gradually according to EWMA of past performance targets.
-	 */
 	update_cpuperf_target(cpuc);
-
-	/*
-	 * If there is a relevant introspection command with @p, process it.
-	 */
 	try_proc_introspec_cmd(p, taskc);
 }
 
@@ -1234,12 +775,13 @@ void BPF_STRUCT_OPS(lavd_tick, struct ta
 	task_ctx *taskc;
 	u64 now;
 
-	/*
-	 * Update task statistics
-	 */
+	if (__builtin_expect(!p, 0))
+		return;
+
 	cpuc = get_cpu_ctx_task(p);
 	taskc = get_task_ctx(p);
-	if (!cpuc || !taskc) {
+
+	if (__builtin_expect(!cpuc || !taskc, 0)) {
 		scx_bpf_error("Failed to lookup context for task %d", p->pid);
 		return;
 	}
@@ -1247,18 +789,11 @@ void BPF_STRUCT_OPS(lavd_tick, struct ta
 	now = scx_bpf_now();
 	account_task_runtime(p, taskc, cpuc, now);
 
-	/*
-	 * Under the CPU bandwidth control with cpu.max, check if the cgroup
-	 * is throttled before executing the task.
-	 */
 	if (enable_cpu_bw && (cgroup_throttled(p, taskc, false) == -EAGAIN)) {
 		preempt_at_tick(p, cpuc);
 		return;
 	}
 
-	/*
-	 * If there is a pinned task on this CPU, shrink its time slice.
-	 */
 	if (cpuc->nr_pinned_tasks)
 		shrink_slice_at_tick(p, cpuc, now);
 }
@@ -1268,9 +803,6 @@ void BPF_STRUCT_OPS(lavd_stopping, struc
 	struct cpu_ctx *cpuc;
 	task_ctx *taskc;
 
-	/*
-	 * Update task statistics
-	 */
 	cpuc = get_cpu_ctx_task(p);
 	taskc = get_task_ctx(p);
 	if (!cpuc || !taskc) {
@@ -1288,36 +820,27 @@ void BPF_STRUCT_OPS(lavd_quiescent, stru
 	u64 now, interval;
 
 	cpuc = get_cpu_ctx_task(p);
-	taskc = get_task_ctx(p);
-	if (!cpuc || !taskc) {
+	if (!cpuc) {
 		scx_bpf_error("Failed to lookup context for task %d", p->pid);
 		return;
 	}
 	cpuc->flags = 0;
 
-	/*
-	 * Decrease the number of pinned tasks waiting for execution.
-	 */
-	if (is_pinned(p) && (taskc->pinned_cpu_id != -ENOENT)) {
-		__sync_fetch_and_sub(&cpuc->nr_pinned_tasks, 1);
-		taskc->pinned_cpu_id = -ENOENT;
-
-		debugln("%d [%d] -- %s:%d -- %s:%d", cpuc->cpu_id,
-			cpuc->nr_pinned_tasks, p->comm, p->pid, __func__,
-			__LINE__);
-	}
-
-	/*
-	 * If a task @p is dequeued from a run queue for some other reason
-	 * other than going to sleep, it is an implementation-level side
-	 * effect. Hence, we don't care this spurious dequeue.
-	 */
+	/* Optimized Pinned Task Tracking: Decrement only if flag is set */
+	taskc = get_task_ctx(p);
+	if (taskc && test_task_flag(taskc, LAVD_FLAG_PINNED_COUNTED)) {
+		atomic_dec_u32(&cpuc->nr_pinned_tasks);
+		reset_task_flag(taskc, LAVD_FLAG_PINNED_COUNTED);
+	}
+
 	if (!(deq_flags & SCX_DEQ_SLEEP))
 		return;
 
-	/*
-	 * When a task @p goes to sleep, its associated wait_freq is updated.
-	 */
+	if (!taskc) {
+		scx_bpf_error("Failed to lookup task_ctx for task %d", p->pid);
+		return;
+	}
+
 	now = scx_bpf_now();
 	interval = time_delta(now, taskc->last_quiescent_clk);
 	if (interval > 0) {
@@ -1335,7 +858,7 @@ static void cpu_ctx_init_online(struct c
 	if (!cd_cpumask)
 		goto unlock_out;
 	bpf_cpumask_set_cpu(cpu_id, cd_cpumask);
-unlock_out:
+	unlock_out:
 	bpf_rcu_read_unlock();
 
 	cpuc->flags = 0;
@@ -1358,7 +881,7 @@ static void cpu_ctx_init_offline(struct
 	if (!cd_cpumask)
 		goto unlock_out;
 	bpf_cpumask_clear_cpu(cpu_id, cd_cpumask);
-unlock_out:
+	unlock_out:
 	bpf_rcu_read_unlock();
 
 	cpuc->flags = 0;
@@ -1374,10 +897,6 @@ unlock_out:
 
 void BPF_STRUCT_OPS(lavd_cpu_online, s32 cpu)
 {
-	/*
-	 * When a cpu becomes online, reset its cpu context and trigger the
-	 * recalculation of the global cpu load.
-	 */
 	u64 now = scx_bpf_now();
 	struct cpu_ctx *cpuc;
 
@@ -1397,10 +916,6 @@ void BPF_STRUCT_OPS(lavd_cpu_online, s32
 
 void BPF_STRUCT_OPS(lavd_cpu_offline, s32 cpu)
 {
-	/*
-	 * When a cpu becomes offline, trigger the recalculation of the global
-	 * cpu load.
-	 */
 	u64 now = scx_bpf_now();
 	struct cpu_ctx *cpuc;
 
@@ -1420,77 +935,29 @@ void BPF_STRUCT_OPS(lavd_cpu_offline, s3
 
 void BPF_STRUCT_OPS(lavd_update_idle, s32 cpu, bool idle)
 {
-	/*
-	 * The idle duration is accumulated to calculate the CPU utilization.
-	 * Since SCX_OPS_KEEP_BUILTIN_IDLE is specified, we still rely on the
-	 * default idle core tracking and core selection algorithm.
-	 */
-
 	struct cpu_ctx *cpuc;
-	u64 now;
+	u64 now = scx_bpf_now();
+	u64 old_clk;
 
 	cpuc = get_cpu_ctx_id(cpu);
-	if (!cpuc) {
-		scx_bpf_error("Failed to lookup cpu_ctx %d", cpu);
+	if (!cpuc)
 		return;
-	}
-
-	now = scx_bpf_now();
 
-	/*
-	 * The CPU is entering into the idle state.
-	 */
 	if (idle) {
 		cpuc->idle_start_clk = now;
-
-		/*
-		 * As an idle task cannot be preempted,
-		 * per-CPU preemption information should be cleared.
-		 */
 		reset_cpu_preemption_info(cpuc, false);
+		return;
 	}
-	/*
-	 * The CPU is exiting from the idle state.
-	 */
-	else {
-		for (int i = 0; i < LAVD_MAX_RETRY; i++) {
-			/*
-			 * If idle_start_clk is zero, that means entering into
-			 * the idle is not captured by the scx (i.e., the scx
-			 * scheduler is loaded when this CPU is in an idle
-			 * state).
-			 */
-			u64 old_clk = cpuc->idle_start_clk;
-
-			if (old_clk == 0)
-				break;
 
-			/*
-			 * The CAS failure happens when idle_start_clk is
-			 * updated by the update timer. That means the update
-			 * timer already took the idle_time duration. However,
-			 * instead of dropping out, the logic here still needs
-			 * to retry to ensure the cpuc->idle_start_clk is
-			 * updated to 0 or the timer will continute accumulating
-			 * the idle_time for an already activated CPU.
-			 */
-			bool ret = __sync_bool_compare_and_swap(
-					&cpuc->idle_start_clk, old_clk, 0);
-			if (ret) {
-				if (time_after(old_clk, now))
-					break;
-
-				u64 duration = time_delta(now, old_clk);
-
-				__sync_fetch_and_add(&cpuc->idle_total, duration);
-				break;
-			}
-		}
+	old_clk = cpuc->idle_start_clk;
+	if (old_clk != 0 && now > old_clk) {
+		if (__sync_bool_compare_and_swap(&cpuc->idle_start_clk, old_clk, 0))
+			cpuc->idle_total += now - old_clk;
 	}
 }
 
 void BPF_STRUCT_OPS(lavd_set_cpumask, struct task_struct *p,
-		    const struct cpumask *cpumask)
+					const struct cpumask *cpumask)
 {
 	task_ctx *taskc;
 
@@ -1508,7 +975,7 @@ void BPF_STRUCT_OPS(lavd_set_cpumask, st
 }
 
 void BPF_STRUCT_OPS(lavd_cpu_acquire, s32 cpu,
-		    struct scx_cpu_acquire_args *args)
+					struct scx_cpu_acquire_args *args)
 {
 	struct cpu_ctx *cpuc;
 	u64 dur, scaled_dur;
@@ -1519,27 +986,15 @@ void BPF_STRUCT_OPS(lavd_cpu_acquire, s3
 		return;
 	}
 
-	/*
-	 * When regaining control of a CPU under the higher priority scheduler
-	 * class, measure how much time the higher priority scheduler class
-	 * used -- i.e., [lavd_cpu_release, lavd_cpu_acquire]. This will be
-	 * used to calculate capacity-invariant and frequency-invariant CPU
-	 * utilization.
-	 */
 	dur = time_delta(scx_bpf_now(), cpuc->cpu_release_clk);
 	scaled_dur = scale_cap_freq(dur, cpu);
 	cpuc->tot_sc_time += scaled_dur;
 
-	/*
-	 * The higher-priority scheduler class could change the CPU frequency,
-	 * so let's keep track of the frequency when we gain the CPU control.
-	 * This helps to make the frequency update decision.
-	 */
 	cpuc->cpuperf_cur = scx_bpf_cpuperf_cur(cpu);
 }
 
 void BPF_STRUCT_OPS(lavd_cpu_release, s32 cpu,
-		    struct scx_cpu_release_args *args)
+					struct scx_cpu_release_args *args)
 {
 	struct cpu_ctx *cpuc;
 
@@ -1550,28 +1005,9 @@ void BPF_STRUCT_OPS(lavd_cpu_release, s3
 	}
 	cpuc->flags = 0;
 
-	/*
-	 * When a CPU is released to serve higher priority scheduler class,
-	 * reset the CPU's preemption information so it cannot be a victim.
-	 */
 	reset_cpu_preemption_info(cpuc, true);
-
-	/*
-	 * Requeue the tasks in a local DSQ to the global enqueue.
-	 */
 	scx_bpf_reenqueue_local();
-
-	/*
-	 * Reset the current CPU's performance target, so we can set
-	 * the target properly after regaining the control.
-	 */
 	reset_cpuperf_target(cpuc);
-
-	/*
-	 * Keep track of when the higher-priority scheduler class takes
-	 * the CPU to calculate capacity-invariant and frequency-invariant
-	 * CPU utilization.
-	 */
 	cpuc->cpu_release_clk = scx_bpf_now();
 }
 
@@ -1579,9 +1015,6 @@ void BPF_STRUCT_OPS(lavd_enable, struct
 {
 	task_ctx *taskc;
 
-	/*
-	 * Set task's service time to the current, minimum service time.
-	 */
 	taskc = get_task_ctx(p);
 	if (!taskc) {
 		scx_bpf_error("task_ctx_stor first lookup failed");
@@ -1591,38 +1024,24 @@ void BPF_STRUCT_OPS(lavd_enable, struct
 	taskc->svc_time = READ_ONCE(cur_svc_time);
 }
 
-
 s32 BPF_STRUCT_OPS_SLEEPABLE(lavd_init_task, struct task_struct *p,
-			     struct scx_init_task_args *args)
+							 struct scx_init_task_args *args)
 {
 	task_ctx *taskc;
 	u64 now;
 	int i;
 
-	/*
-	 * When @p becomes under the SCX control (e.g., being forked), @p's
-	 * context data is initialized. We can sleep in this function and the
-	 * following will automatically use GFP_KERNEL.
-	 * 
-	 * Return 0 on success.
-	 * Return -ESRCH if @p is invalid.
-	 * Return -ENOMEM if context allocation fails.
-	 */
 	if (!p) {
 		scx_bpf_error("NULL task_struct pointer received");
 		return -ESRCH;
 	}
-	
+
 	taskc = scx_task_alloc(p);
 	if (!taskc) {
 		scx_bpf_error("task_ctx_stor first lookup failed");
 		return -ENOMEM;
 	}
 
-
-	/*
-	 * Initialize @p's context.
-	 */
 	for (i = 0; i < sizeof(*taskc) && can_loop; i++)
 		((char __arena *)taskc)[i] = 0;
 
@@ -1635,12 +1054,11 @@ s32 BPF_STRUCT_OPS_SLEEPABLE(lavd_init_t
 
 	now = scx_bpf_now();
 	taskc->last_runnable_clk = now;
-	taskc->last_running_clk = now; /* for avg_runtime */
-	taskc->last_stopping_clk = now; /* for avg_runtime */
+	taskc->last_running_clk = now;
+	taskc->last_stopping_clk = now;
 	taskc->last_quiescent_clk = now;
 	taskc->avg_runtime = sys_stat.slice;
 	taskc->svc_time = sys_stat.avg_svc_time;
-	taskc->pinned_cpu_id = -ENOENT;
 	taskc->pid = p->pid;
 	taskc->cgrp_id = args->cgroup->kn->id;
 
@@ -1649,7 +1067,7 @@ s32 BPF_STRUCT_OPS_SLEEPABLE(lavd_init_t
 }
 
 s32 BPF_STRUCT_OPS(lavd_exit_task, struct task_struct *p,
-		   struct scx_exit_task_args *args)
+				   struct scx_exit_task_args *args)
 {
 	scx_task_free(p);
 	return 0;
@@ -1661,9 +1079,6 @@ static s32 init_cpdoms(u64 now)
 	int err;
 
 	for (int i = 0; i < LAVD_CPDOM_MAX_NR; i++) {
-		/*
-		 * Fetch a cpdom context.
-		 */
 		cpdomc = MEMBER_VPTR(cpdom_ctxs, [i]);
 		if (!cpdomc) {
 			scx_bpf_error("Failed to lookup cpdom_ctx for %d", i);
@@ -1672,22 +1087,16 @@ static s32 init_cpdoms(u64 now)
 		if (!cpdomc->is_valid)
 			continue;
 
-		/*
-		 * Create an associated DSQ on its associated NUMA domain.
-		 */
 		if (use_cpdom_dsq()) {
 			err = scx_bpf_create_dsq(cpdom_to_dsq(cpdomc->id),
-						 cpdomc->numa_id);
+									 cpdomc->numa_id);
 			if (err) {
 				scx_bpf_error("Failed to create a DSQ for cpdom %llu on NUMA node %d",
-					      cpdomc->id, cpdomc->numa_id);
+							  cpdomc->id, cpdomc->numa_id);
 				return err;
 			}
 		}
 
-		/*
-		 * Update the number of compute domains.
-		 */
 		nr_cpdoms = i + 1;
 	}
 
@@ -1715,9 +1124,6 @@ static int init_cpumasks(void)
 	int err = 0;
 
 	bpf_rcu_read_lock();
-	/*
-	 * Allocate active cpumask and initialize it with all online CPUs.
-	 */
 	err = calloc_cpumask(&active_cpumask);
 	active = active_cpumask;
 	if (err || !active)
@@ -1728,9 +1134,6 @@ static int init_cpumasks(void)
 	bpf_cpumask_copy(active, online_cpumask);
 	scx_bpf_put_cpumask(online_cpumask);
 
-	/*
-	 * Allocate the other cpumasks.
-	 */
 	err = calloc_cpumask(&ovrflw_cpumask);
 	if (err)
 		goto out;
@@ -1746,7 +1149,7 @@ static int init_cpumasks(void)
 	err = calloc_cpumask(&little_cpumask);
 	if (err)
 		goto out;
-out:
+	out:
 	bpf_rcu_read_unlock();
 	return err;
 }
@@ -1764,9 +1167,6 @@ static s32 init_per_cpu_ctx(u64 now)
 	bpf_rcu_read_lock();
 	online_cpumask = scx_bpf_get_online_cpumask();
 
-	/*
-	 * Prepare cpumasks.
-	 */
 	turbo = turbo_cpumask;
 	big = big_cpumask;
 	little = little_cpumask;
@@ -1778,9 +1178,6 @@ static s32 init_per_cpu_ctx(u64 now)
 		goto unlock_out;
 	}
 
-	/*
-	 * Initilize CPU info
-	 */
 	one_little_capacity = LAVD_SCALE;
 	bpf_for(cpu, 0, nr_cpu_ids) {
 		if (cpu >= LAVD_CPU_ID_MAX)
@@ -1860,9 +1257,6 @@ static s32 init_per_cpu_ctx(u64 now)
 	default_big_core_scale = (big_capacity << LAVD_SHIFT) / sum_capacity;
 	total_capacity = sum_capacity;
 
-	/*
-	 * Initialize compute domain id.
-	 */
 	bpf_for(cpdom_id, 0, nr_cpdoms) {
 		if (cpdom_id >= LAVD_CPDOM_MAX_NR)
 			break;
@@ -1904,9 +1298,6 @@ static s32 init_per_cpu_ctx(u64 now)
 		}
 	}
 
-	/*
-	 * Print some useful informatin for debugging.
-	 */
 	bpf_for(cpu, 0, nr_cpu_ids) {
 		cpuc = get_cpu_ctx_id(cpu);
 		if (!cpuc) {
@@ -1915,18 +1306,17 @@ static s32 init_per_cpu_ctx(u64 now)
 			goto unlock_out;
 		}
 		debugln("cpu[%d] capacity: %d, big_core: %d, turbo_core: %d, "
-			"cpdom_id: %llu, alt_id: %llu",
-			cpu, cpuc->capacity, cpuc->big_core, cpuc->turbo_core,
-			cpuc->cpdom_id, cpuc->cpdom_alt_id);
+		"cpdom_id: %llu, alt_id: %llu",
+		cpu, cpuc->capacity, cpuc->big_core, cpuc->turbo_core,
+		cpuc->cpdom_id, cpuc->cpdom_alt_id);
 	}
 
-unlock_out:
+	unlock_out:
 	scx_bpf_put_cpumask(online_cpumask);
 	bpf_rcu_read_unlock();
 	return err;
 }
 
-
 static int init_per_cpu_dsqs(void)
 {
 	struct cpdom_ctx *cpdomc;
@@ -1934,9 +1324,6 @@ static int init_per_cpu_dsqs(void)
 	int cpu, err = 0;
 
 	bpf_for(cpu, 0, nr_cpu_ids) {
-		/*
-		 * Create Per-CPU DSQs on its associated NUMA domain.
-		 */
 		cpuc = get_cpu_ctx_id(cpu);
 		if (!cpuc) {
 			scx_bpf_error("Failed to lookup cpu_ctx: %d", cpu);
@@ -1955,7 +1342,7 @@ static int init_per_cpu_dsqs(void)
 		err = scx_bpf_create_dsq(cpu_to_dsq(cpu), cpdomc->numa_id);
 		if (err) {
 			scx_bpf_error("Failed to create a DSQ for cpu %d on NUMA node %d",
-				      cpu, cpdomc->numa_id);
+						  cpu, cpdomc->numa_id);
 			return err;
 		}
 	}
@@ -1964,7 +1351,7 @@ static int init_per_cpu_dsqs(void)
 }
 
 s32 BPF_STRUCT_OPS_SLEEPABLE(lavd_cgroup_init, struct cgroup *cgrp,
-			     struct scx_cgroup_init_args *args)
+							 struct scx_cgroup_init_args *args)
 {
 	int ret;
 
@@ -1973,7 +1360,7 @@ s32 BPF_STRUCT_OPS_SLEEPABLE(lavd_cgroup
 
 	ret = scx_cgroup_bw_init(cgrp, args);
 	if (ret)
-	       scx_bpf_error("Failed to init a cgroup: %d", ret);
+		scx_bpf_error("Failed to init a cgroup: %d", ret);
 	return ret;
 }
 
@@ -1986,22 +1373,22 @@ void BPF_STRUCT_OPS(lavd_cgroup_exit, st
 
 	ret = scx_cgroup_bw_exit(cgrp);
 	if (ret)
-	       scx_bpf_error("Failed to exit a cgroup: %d", ret);
+		scx_bpf_error("Failed to exit a cgroup: %d", ret);
 }
 
 void BPF_STRUCT_OPS(lavd_cgroup_move, struct task_struct *p,
-		    struct cgroup *from, struct cgroup *to)
+					struct cgroup *from, struct cgroup *to)
 {
 	task_ctx *taskc;
 
 	taskc = get_task_ctx(p);
 	if (!taskc)
-	       scx_bpf_error("Failed to get a task context: %d", p->pid);
+		scx_bpf_error("Failed to get a task context: %d", p->pid);
 	taskc->cgrp_id = to->kn->id;
 }
 
 void BPF_STRUCT_OPS(lavd_cgroup_set_bandwidth, struct cgroup *cgrp,
-		    u64 period_us, u64 quota_us, u64 burst_us)
+					u64 period_us, u64 quota_us, u64 burst_us)
 {
 	int ret;
 
@@ -2010,7 +1397,7 @@ void BPF_STRUCT_OPS(lavd_cgroup_set_band
 
 	ret = scx_cgroup_bw_set(cgrp, period_us, quota_us, burst_us);
 	if (ret)
-	       scx_bpf_error("Failed to set bandwidth of a cgroup: %d", ret);
+		scx_bpf_error("Failed to set bandwidth of a cgroup: %d", ret);
 }
 
 int lavd_enqueue_cb(u64 ctx)
@@ -2021,11 +1408,6 @@ int lavd_enqueue_cb(u64 ctx)
 	if (!enable_cpu_bw)
 		return 0;
 
-	/*
-	 * Enqueue a task with @pid. As long as the task is under scx,
-	 * it must be enqueued regardless of whether its cgroup is throttled
-	 * or not.
-	 */
 	if ((p = bpf_task_from_pid(taskc->pid))) {
 		enqueue_cb(p);
 		bpf_task_release(p);
@@ -2041,65 +1423,35 @@ s32 BPF_STRUCT_OPS_SLEEPABLE(lavd_init)
 
 	_Static_assert(
 		sizeof(struct atq_ctx) >= sizeof(struct scx_task_common),
-		"atq_ctx should be equal or larger than scx_task_common");
+				   "atq_ctx should be equal or larger than scx_task_common");
 
-	/*
-	 * Create compute domains.
-	 */
 	err = init_cpdoms(now);
 	if (err)
 		return err;
 
-	/*
-	 * Allocate cpumask for core compaction.
-	 *  - active CPUs: a group of CPUs will be used for now.
-	 *  - overflow CPUs: a pair of hyper-twin which will be used when there
-	 *    is no idle active CPUs.
-	 */
 	err = init_cpumasks();
 	if (err)
 		return err;
 
-	/*
-	 * Initialize per-CPU context.
-	 */
 	err = init_per_cpu_ctx(now);
 	if (err)
 		return err;
 
-	/*
-	 * Initialize per-CPU DSQs.
-	 * Per-CPU DSQs are created when per_cpu_dsq is enabled OR when
-	 * pinned_slice_ns is enabled (for pinned task handling).
-	 */
 	if (use_per_cpu_dsq()) {
 		err = init_per_cpu_dsqs();
 		if (err)
 			return err;
 	}
 
-	/*
-	 * Initialize the last update clock and the update timer to track
-	 * system-wide CPU load.
-	 */
 	err = init_sys_stat(now);
 	if (err)
 		return err;
 
-	/*
-	 * Initialize the low & high cpu capacity watermarks for autopilot mode.
-	 */
 	init_autopilot_caps();
 
-	/*
-	 * Initilize the current logical clock and service time.
-	 */
-	WRITE_ONCE(cur_logical_clk, 0);
+	WRITE_ONCE(cur_logical_clk, LAVD_DL_COMPETE_WINDOW);
 	WRITE_ONCE(cur_svc_time, 0);
 
-	/*
-	 * Initialize cpu.max library if enabled.
-	 */
 	if (enable_cpu_bw) {
 		struct scx_cgroup_bw_config bw_config = {
 			.verbose = verbose > 2,
@@ -2107,9 +1459,6 @@ s32 BPF_STRUCT_OPS_SLEEPABLE(lavd_init)
 		err = scx_cgroup_bw_lib_init(&bw_config);
 	}
 
-	/*
-	 * Keep track of scheduler process's PID.
-	 */
 	lavd_pid = (u32)bpf_get_current_pid_tgid();
 
 	return err;
@@ -2121,30 +1470,29 @@ void BPF_STRUCT_OPS(lavd_exit, struct sc
 }
 
 SCX_OPS_DEFINE(lavd_ops,
-	       .select_cpu		= (void *)lavd_select_cpu,
-	       .enqueue			= (void *)lavd_enqueue,
-	       .dequeue			= (void *)lavd_dequeue,
-	       .dispatch		= (void *)lavd_dispatch,
-	       .runnable		= (void *)lavd_runnable,
-	       .running			= (void *)lavd_running,
-	       .tick			= (void *)lavd_tick,
-	       .stopping		= (void *)lavd_stopping,
-	       .quiescent		= (void *)lavd_quiescent,
-	       .cpu_online		= (void *)lavd_cpu_online,
-	       .cpu_offline		= (void *)lavd_cpu_offline,
-	       .update_idle		= (void *)lavd_update_idle,
-	       .set_cpumask		= (void *)lavd_set_cpumask,
-	       .cpu_acquire		= (void *)lavd_cpu_acquire,
-	       .cpu_release		= (void *)lavd_cpu_release,
-	       .enable			= (void *)lavd_enable,
-	       .init_task		= (void *)lavd_init_task,
-	       .exit_task		= (void *)lavd_exit_task,
-	       .cgroup_init		= (void *)lavd_cgroup_init,
-	       .cgroup_exit		= (void *)lavd_cgroup_exit,
-	       .cgroup_move		= (void *)lavd_cgroup_move,
-	       .cgroup_set_bandwidth	= (void *)lavd_cgroup_set_bandwidth,
-	       .init			= (void *)lavd_init,
-	       .exit			= (void *)lavd_exit,
-	       .timeout_ms		= 30000U,
-	       .name			= "lavd");
-
+			   .select_cpu		= (void *)lavd_select_cpu,
+			   .enqueue			= (void *)lavd_enqueue,
+			   .dequeue			= (void *)lavd_dequeue,
+			   .dispatch		= (void *)lavd_dispatch,
+			   .runnable		= (void *)lavd_runnable,
+			   .running			= (void *)lavd_running,
+			   .tick			= (void *)lavd_tick,
+			   .stopping		= (void *)lavd_stopping,
+			   .quiescent		= (void *)lavd_quiescent,
+			   .cpu_online		= (void *)lavd_cpu_online,
+			   .cpu_offline		= (void *)lavd_cpu_offline,
+			   .update_idle		= (void *)lavd_update_idle,
+			   .set_cpumask		= (void *)lavd_set_cpumask,
+			   .cpu_acquire		= (void *)lavd_cpu_acquire,
+			   .cpu_release		= (void *)lavd_cpu_release,
+			   .enable			= (void *)lavd_enable,
+			   .init_task		= (void *)lavd_init_task,
+			   .exit_task		= (void *)lavd_exit_task,
+			   .cgroup_init		= (void *)lavd_cgroup_init,
+			   .cgroup_exit		= (void *)lavd_cgroup_exit,
+			   .cgroup_move		= (void *)lavd_cgroup_move,
+			   .cgroup_set_bandwidth	= (void *)lavd_cgroup_set_bandwidth,
+			   .init			= (void *)lavd_init,
+			   .exit			= (void *)lavd_exit,
+			   .timeout_ms		= 30000U,
+			   .name			= "lavd");
