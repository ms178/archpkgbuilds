pkgbase=ollama
pkgname=(ollama ollama-rocm)
pkgver=0.6.0
pkgrel=2.1
pkgdesc='Create, run and share large language models (LLMs)'
arch=(x86_64)
url='https://github.com/ollama/ollama'
license=(MIT)
options=('!lto') # Disable LTO to avoid conflicts with PGO
makedepends=(cmake ninja git go hipblas clblast netcat)
source=(git+https://github.com/ollama/ollama#tag=v$pkgver
        ollama-ld.conf
        ollama.service
        sysusers.conf
        tmpfiles.d
        workload.txt)
b2sums=('SKIP'
        'SKIP'
        'SKIP'
        'SKIP'
        'SKIP'
        'SKIP')

prepare() {
  cd ollama

  # Remove runtime dependencies from CMake installation to avoid unnecessary system dependencies
  sed -i 's/PRE_INCLUDE_REGEXES.*/PRE_INCLUDE_REGEXES = ""/' CMakeLists.txt
}

build() {
  # Create build directory
  cd ollama
  BUILD_DIR=$(pwd)

  echo "PGO: Stage 1 - Build with instrumentation"

  # Set flags for profile generation
  export CGO_CPPFLAGS="${CPPFLAGS}"
  export CGO_CFLAGS="${CFLAGS} -march=native -mtune=native -fprofile-generate=${BUILD_DIR}/pgo_data"
  export CGO_CXXFLAGS="${CXXFLAGS} -march=native -mtune=native -fprofile-generate=${BUILD_DIR}/pgo_data"
  export CGO_LDFLAGS="${LDFLAGS} -march=native -mtune=native -fprofile-generate=${BUILD_DIR}/pgo_data"
  export GOPATH="${srcdir}"
  export GOFLAGS="-buildmode=pie -mod=readonly -modcacherw '-ldflags=-linkmode=external -compressdwarf=false -X=github.com/ollama/ollama/version.Version=$pkgver -X=github.com/ollama/ollama/server.mode=release' -gcflags=all=-B -trimpath -buildvcs=false"

  # Stage 1a: Build CMake components with instrumentation
  echo "PGO: Building C/C++ components with instrumentation"
  cmake -B build -G Ninja \
    -DAMDGPU_TARGETS="gfx900" \
    -DGGML_CUDA=OFF \
    -DCMAKE_INSTALL_PREFIX=/usr
  cmake --build build

  # Stage 1b: Build Go components with instrumentation
  echo "PGO: Building Go components with instrumentation"
  go build -o ollama-instrumented .

  # Create directory for PGO data
  mkdir -p "${BUILD_DIR}/pgo_data"
  echo "PGO: Profile data will be stored in ${BUILD_DIR}/pgo_data"

  # Function to check if a port is in use
  is_port_in_use() {
    local port="$1"
    if nc -z 127.0.0.1 "$port" >/dev/null 2>&1; then
      return 0  # Port is in use
    fi
    if command -v ss >/dev/null 2>&1; then
      if ss -tln | grep -q ":$port "; then
        return 0  # Port is in use
      fi
    elif command -v netstat >/dev/null 2>&1; then
      if netstat -tln | grep -q ":$port "; then
        return 0  # Port is in use
      fi
    fi
    return 1  # Port is not in use
  }

  # Function to find an available port
  find_available_port() {
    local port=12000
    local max_port=13000
    while [ $port -le $max_port ]; do
      if ! is_port_in_use "$port"; then
        echo $port
        return 0
      fi
      port=$((port + 1))
    done
    echo "Error: No available ports found in range $port-$max_port"
    return 1
  }

  # Function to run a generation with token tracking
  generate_with_token_tracking() {
    local prompt="$1"
    local max_tokens="$2"
    local total_tokens=0

    # Create a unique file for this generation
    local tmp_file=$(mktemp)

    # Start curl in the background to capture all tokens
    curl -s --max-time 120 -X POST "http://$ollama_host/api/generate" \
         -H "Content-Type: application/json" \
         -d "{\"model\":\"dolphin3-r1\",\"prompt\":\"$prompt\",\"stream\":true,\"options\":{\"num_predict\":$max_tokens}}" > "$tmp_file" &
    local curl_pid=$!

    # Track progress
    local token_pattern='"response":'
    local done_pattern='"done":true'
    local progress_shown=0
    local max_progress=20  # Show up to 20 progress markers

    echo -n "  Generating ["

    # Monitor the output file for new tokens
    while ps -p $curl_pid > /dev/null 2>&1; do
      # Count new tokens that have arrived
      new_total=$(grep -o "$token_pattern" "$tmp_file" | wc -l)

      # Update progress bar if we have new tokens
      if [ "$new_total" -gt "$total_tokens" ]; then
        # Calculate how many new markers to show
        local new_progress=$(($new_total * $max_progress / $max_tokens))
        if [ "$new_progress" -gt "$progress_shown" ]; then
          # Show new markers
          for ((i=progress_shown; i<new_progress && i<max_progress; i++)); do
            echo -n "#"
          done
          progress_shown=$new_progress
        fi
        total_tokens=$new_total
      fi

      # Check if generation is done
      if grep -q "$done_pattern" "$tmp_file"; then
        break
      fi

      # Brief pause
      sleep 0.2
    done

    # Fill in the rest of the progress bar if needed
    while [ "$progress_shown" -lt "$max_progress" ]; do
      echo -n "#"
      progress_shown=$((progress_shown + 1))
    done
    echo "] Done."

    # Get final token count
    final_tokens=$(grep -o "$token_pattern" "$tmp_file" | wc -l)
    echo "  Generated $final_tokens tokens"

    # Clean up
    rm -f "$tmp_file"

    # If curl is still running, kill it
    if ps -p $curl_pid > /dev/null 2>&1; then
      kill $curl_pid
    fi
  }

  # Check for existing Ollama server on default port
  echo "PGO: Checking for existing Ollama server..."
  if is_port_in_use 11434; then
    echo "PGO: Found existing Ollama server on port 11434"
    if curl -s http://127.0.0.1:11434/api/tags | grep -q "dolphin3-r1"; then
      echo "PGO: Using existing Ollama server with dolphin3-r1 model"
      ollama_host="127.0.0.1:11434"
      using_existing_server=true
    else
      echo "PGO: Existing server doesn't have the required model. Will start our own server."
      using_existing_server=false
    fi
  else
    echo "PGO: No existing Ollama server detected on port 11434"
    using_existing_server=false
  fi

  # Start our own server only if needed
  if [ "$using_existing_server" = false ]; then
    # Find an available port for our server
    ollama_port=$(find_available_port)
    if [ $? -ne 0 ]; then
      echo "Error: Unable to find an available port"
      exit 1
    fi

    echo "PGO: Using port $ollama_port for Ollama server"
    ollama_host="127.0.0.1:$ollama_port"

    # Get user's home directory to locate their model directory
    USER_HOME="$HOME"
    if [ -z "$USER_HOME" ]; then
      USER_HOME="$(getent passwd $(whoami) | cut -d: -f6)"
    fi

    # Use system models directory
    SYSTEM_MODELS_DIR="$USER_HOME/.ollama/models"
    echo "PGO: Using system models directory: $SYSTEM_MODELS_DIR"

    # Create log directory
    LOG_DIR="${BUILD_DIR}/pgo_data/logs"
    mkdir -p "$LOG_DIR"
    SERVER_LOG="$LOG_DIR/server.log"

    # Start the Ollama server in the background
    echo "PGO: Starting Ollama server for PGO workload..."
    OLLAMA_KEEP_ALIVE=1 OLLAMA_HOST="$ollama_host" OLLAMA_MODELS="$SYSTEM_MODELS_DIR" \
    ./ollama-instrumented serve > "$SERVER_LOG" 2>&1 &
    ollama_pid=$!
    echo "PGO: Server started with PID: $ollama_pid"

    # Verify server startup with retries
    echo "PGO: Waiting for server to start..."
    server_ready=false
    for i in {1..30}; do
      # Check process is still running
      if ! ps -p $ollama_pid > /dev/null 2>&1; then
        echo "Error: Server process terminated during startup"
        echo "Server log:"
        cat "$SERVER_LOG"
        exit 1
      fi

      # Check if server is responding
      if curl -s "http://$ollama_host/api/version" > /dev/null 2>&1; then
        echo "PGO: Server started successfully on port $ollama_port"
        server_ready=true
        break
      fi

      echo "PGO: Waiting for server to start... ($i/30)"
      sleep 2
    done

    if [ "$server_ready" = false ]; then
      echo "Error: Server failed to start within the timeout period"
      echo "Server log:"
      cat "$SERVER_LOG"
      if ps -p $ollama_pid > /dev/null 2>&1; then
        kill $ollama_pid
      fi
      exit 1
    fi

    # Check if the model is available on our server
    if ! curl -s "http://$ollama_host/api/tags" | grep -q "dolphin3-r1"; then
      echo "Warning: The required model 'dolphin3-r1' is not available on the server"
      echo "PGO will continue with a text-only approach"
      use_text_only=true
    else
      echo "PGO: Found model 'dolphin3-r1' on the server"
      use_text_only=false
    fi
  else
    # Using existing server
    ollama_pid=""
    use_text_only=false
  fi

  # Function to check server health
  check_server_health() {
    # Only check process health if we started our own server
    if [ -n "$ollama_pid" ]; then
      if ! ps -p $ollama_pid > /dev/null 2>&1; then
        echo "Error: Server process (PID $ollama_pid) terminated unexpectedly"
        return 1
      fi
    fi

    # Always check server API responsiveness
    if ! curl -s --max-time 3 "http://$ollama_host/api/version" > /dev/null 2>&1; then
      echo "Error: Server not responding to health check on $ollama_host"
      return 1
    fi

    return 0
  }

  # Run the representative workload to generate profiling data
  echo "PGO: Stage 2 - Running workload for profile data collection"

  # Set a time limit for the PGO phase (20 minutes)
  PGO_TIME_LIMIT=1200
  start_time=$(date +%s)

  workload_success=true
  prompt_count=0

  # We'll do fewer prompts but generate more tokens for each
  max_prompts=15

  # Track sections to ensure coverage from all types
  declare -A processed_sections
  max_per_section=1  # Just one prompt per section, but generate more tokens
  current_section=""

  # First confirm server is healthy before starting workload
  if ! check_server_health; then
    echo "Error: Server is not healthy before starting workload"
    workload_success=false
  else
    # Process the workload file
    while IFS= read -r line; do
      # Check time limit
      current_time=$(date +%s)
      elapsed_time=$((current_time - start_time))
      if [ $elapsed_time -ge $PGO_TIME_LIMIT ]; then
        echo "PGO: Time limit reached after $elapsed_time seconds and $prompt_count prompts."
        break
      fi

      # Process section headers
      if [[ "$line" =~ ^#\ Section ]]; then
        current_section="$line"
        processed_sections["$current_section"]=0
        echo "PGO: Processing $current_section"
        continue
      fi

      # Skip comments and empty lines
      if [[ -z "$line" || "$line" =~ ^# ]]; then
        continue
      fi

      # Limit prompts per section for balanced coverage
      if [[ -n "$current_section" && ${processed_sections["$current_section"]} -ge $max_per_section ]]; then
        continue
      fi

      # Limit total prompts
      if [ $prompt_count -ge $max_prompts ]; then
        echo "PGO: Maximum prompt limit reached ($max_prompts)."
        break
      fi

      prompt_count=$((prompt_count + 1))
      if [[ -n "$current_section" ]]; then
        processed_sections["$current_section"]=$((processed_sections["$current_section"] + 1))
      fi

      # Extract the actual prompt from within quotes if present
      if [[ "$line" =~ ^\"(.*)\"$ ]]; then
        prompt="${BASH_REMATCH[1]}"
      else
        prompt="$line"
      fi

      echo "PGO: Processing prompt $prompt_count: ${prompt:0:50}..."

      # Check server health before each prompt
      if ! check_server_health; then
        echo "Error: Server health check failed before prompt $prompt_count"
        workload_success=false
        break
      fi

      # If using text-only approach, just hit the API without model generation
      if [ "$use_text_only" = true ]; then
        # Simulated API call to exercise the same code paths without needing model
        echo "  Using text-only simulation mode (no model available)"
        for i in {1..5}; do
          curl -s --max-time 5 "http://$ollama_host/api/version" > /dev/null
          sleep 1
          echo -n "."
        done
        echo " Done."
      else
        # Determine token count based on section type
        token_count=100  # Default

        # Adjust token counts based on section type for more realistic PGO
        if [[ "$current_section" == *"Long-Form Text Generation"* ]]; then
          token_count=300  # More tokens for long-form generation
        elif [[ "$current_section" == *"Short-Form Text Generation"* ]]; then
          token_count=50   # Fewer tokens for short-form
        elif [[ "$current_section" == *"Code Generation"* ]]; then
          token_count=200  # Medium for code
        elif [[ "$current_section" == *"Translation"* ]]; then
          token_count=150  # Medium for translation
        elif [[ "$current_section" == *"GPU-Stressing"* ]]; then
          token_count=400  # More for GPU stress testing
        fi

        # Properly escape the prompt for JSON
        escaped_prompt=$(echo "$prompt" | sed 's/"/\\"/g')

        # Generate with token tracking to see progress
        generate_with_token_tracking "$escaped_prompt" "$token_count"
      fi

      # Give server a short break between generations
      sleep 2
    done < ../workload.txt
  fi

  echo "PGO: Completed workload with $prompt_count prompts."

  # Only stop the server if we started it
  if [ -n "$ollama_pid" ]; then
    echo "PGO: Stopping Ollama server (PID $ollama_pid)..."
    if ps -p $ollama_pid > /dev/null 2>&1; then
      kill $ollama_pid
      # Wait for process to terminate
      for i in {1..10}; do
        if ! ps -p $ollama_pid > /dev/null 2>&1; then
          echo "PGO: Server process terminated successfully."
          break
        fi
        sleep 1
      done
      # Force kill if still running
      if ps -p $ollama_pid > /dev/null 2>&1; then
        echo "PGO: Server still running, force killing..."
        kill -9 $ollama_pid
        sleep 1
      fi
    else
      echo "PGO: Server process already terminated."
    fi
  else
    echo "PGO: Using existing Ollama server, not stopping it."
  fi

  # Check for profile data and report results
  echo "PGO: Checking for profile data collection..."

  # Find any .gcda files in the build directory or PGO data directory
  PGO_FILES=$(find "${BUILD_DIR}" -name "*.gcda" 2>/dev/null | wc -l)
  if [ "$PGO_FILES" -gt 0 ]; then
    echo "PGO: ✓ Found $PGO_FILES profile data files (*.gcda)"
    PGO_SUCCESS=true

    # List some of the profile data files for verification
    echo "PGO: Sample profile data files:"
    find "${BUILD_DIR}" -name "*.gcda" | head -n 5
  else
    echo "PGO: ✗ No profile data files found"
    echo "PGO: Will continue without PGO, performance may not be optimal"
    PGO_SUCCESS=false
  fi

  # Stage 3: Rebuild with PGO data
  echo "PGO: Stage 3 - Rebuilding with PGO optimizations"

  if [ "$PGO_SUCCESS" = true ]; then
    # Set flags for profile use
    export CGO_CFLAGS="${CFLAGS} -march=native -mtune=native -fprofile-use=${BUILD_DIR}/pgo_data -fprofile-correction -Wno-error -Wno-missing-profile"
    export CGO_CXXFLAGS="${CXXFLAGS} -march=native -mtune=native -fprofile-use=${BUILD_DIR}/pgo_data -fprofile-correction -Wno-error -Wno-missing-profile"
    export CGO_LDFLAGS="${LDFLAGS} -march=native -mtune=native -fprofile-use=${BUILD_DIR}/pgo_data -fprofile-correction -Wno-error -Wno-missing-profile"
  else
    # Fall back to regular optimization flags without PGO
    export CGO_CFLAGS="${CFLAGS} -march=native -mtune=native"
    export CGO_CXXFLAGS="${CXXFLAGS} -march=native -mtune=native"
    export CGO_LDFLAGS="${LDFLAGS} -march=native -mtune=native"
  fi

  # Rebuild C/C++ components
  cmake --build build --clean-first

  # Rebuild Go components
  go build -o ollama .

  echo "PGO: Build completed"
}

check() {
  cd ollama
  ./ollama --version > /dev/null
  go test .
}

package_ollama() {
  DESTDIR="$pkgdir" cmake --install ollama/build --component CPU

  install -Dm755 ollama/ollama "$pkgdir/usr/bin/ollama"
  install -dm755 "$pkgdir/var/lib/ollama"
  install -Dm644 ollama.service "$pkgdir/usr/lib/systemd/system/ollama.service"
  install -Dm644 sysusers.conf "$pkgdir/usr/lib/sysusers.d/ollama.conf"
  install -Dm644 tmpfiles.d "$pkgdir/usr/lib/tmpfiles.d/ollama.conf"
  install -Dm644 ollama/LICENSE "$pkgdir/usr/share/licenses/$pkgname/LICENSE"

  ln -s /var/lib/ollama "$pkgdir/usr/share/ollama"
}

package_ollama-rocm() {
  pkgdesc='Create, run and share large language models (LLMs) with ROCm'
  depends+=(ollama hipblas)

  DESTDIR="$pkgdir" cmake --install ollama/build --component HIP
  rm -rf "$pkgdir"/usr/lib/ollama/rocm/rocblas/library
}
