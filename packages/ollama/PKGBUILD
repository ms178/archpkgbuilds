pkgbase=ollama
pkgname=(ollama ollama-rocm ollama-vulkan)
pkgver=0.13.5
pkgrel=3.1
pkgdesc='Create, run and share large language models (LLMs) with PGO (gfx900 only)'
arch=(x86_64)
url='https://github.com/ollama/ollama'
license=(MIT)
options=('!lto')
makedepends=(cmake ninja git go rocm-hip-sdk hipblas clblast jq bc)
source=(git+https://github.com/ollama/ollama#tag=v$pkgver
        ollama-ld.conf
        ollama.service
        sysusers.conf
        tmpfiles.d
        workload.txt)
b2sums=('SKIP'
        'SKIP'
        'SKIP'
        'SKIP'
        'SKIP'
        'SKIP')

prepare() {
  cd ollama
  sed -i 's/PRE_INCLUDE_REGEXES.*/PRE_INCLUDE_REGEXES = ""/' CMakeLists.txt
}

build() {
  cd ollama
  local BUILD_DIR="$(pwd)/build"
  local PGO_DATA_DIR="$(pwd)/pgo_profiles"
  mkdir -p "$PGO_DATA_DIR"
  find . -name "*.profraw" -delete
  find . -name "*.profdata" -delete

  local ORIGINAL_CFLAGS="$CFLAGS"
  local ORIGINAL_CXXFLAGS="$CXXFLAGS"
  local ORIGINAL_LDFLAGS="$LDFLAGS"

  ##################################################
  msg "=== PHASE 1: Build INSTRUMENTED C++ Libraries ==="
  ##################################################

  export CFLAGS="${ORIGINAL_CFLAGS} -fprofile-generate"
  export CXXFLAGS="${ORIGINAL_CXXFLAGS} -fprofile-generate"
  export LDFLAGS="${ORIGINAL_LDFLAGS} -fprofile-generate"
  export LLVM_PROFILE_FILE="${PGO_DATA_DIR}/ollama-%p.profraw"

  msg "Configuring C++/HIP components with instrumentation..."
  rm -rf build

  # Use AMDGPU_TARGETS as it is explicitly required by this version of ollama's CMakeLists.txt
  cmake -B build -G Ninja \
    -DCMAKE_INSTALL_PREFIX=/usr \
    -DCMAKE_PREFIX_PATH=/opt/rocm \
    -DAMDGPU_TARGETS="gfx900" \
    -DGPU_TARGETS="gfx900"

  msg "Building instrumented C++/HIP libraries..."
  cmake --build build

  msg "Building Go binary against instrumented libraries..."
  unset CFLAGS CXXFLAGS LDFLAGS
  export CGO_CFLAGS="${ORIGINAL_CFLAGS}"
  export CGO_CXXFLAGS="${ORIGINAL_CXXFLAGS}"
  export CGO_LDFLAGS="${ORIGINAL_LDFLAGS} -L${BUILD_DIR}/lib/ollama"

  export GOPATH="${srcdir}"
  export GOFLAGS="-buildmode=pie -mod=readonly -modcacherw '-ldflags=-linkmode=external -compressdwarf=false -X=github.com/ollama/ollama/version.Version=$pkgver -X=github.com/ollama/ollama/server.mode=release' -trimpath -buildvcs=false"

  go build -o ollama-pgo .
  msg "Instrumented build completed."


  ##################################################
  msg "=== PHASE 2: PROFILE GENERATION RUN (GPU & ALL CORES) ==="
  ##################################################

  local ollama_port
  ollama_port=$(python -c 'import socket; s=socket.socket(); s.bind(("", 0)); print(s.getsockname()[1]); s.close()')
  local ollama_host="127.0.0.1:$ollama_port"
  local SERVER_LOG="${PGO_DATA_DIR}/server.log"

  msg "Starting instrumented server with GPU library path..."
  LD_LIBRARY_PATH="${BUILD_DIR}/lib/ollama" OLLAMA_HOST="$ollama_host" OLLAMA_DEBUG=1 ./ollama-pgo serve > "$SERVER_LOG" 2>&1 &
  local ollama_pid=$!

  for i in {1..20}; do if curl -s "http://$ollama_host/api/version" >/dev/null; then break; fi; sleep 0.5; done
  if ! ps -p $ollama_pid >/dev/null; then error "Server process terminated." && cat "$SERVER_LOG" && return 1; fi
  local MODEL_NAME
  MODEL_NAME=$(curl -s "http://$ollama_host/api/tags" | jq -r '.models[0].name')
  if [[ -z "$MODEL_NAME" || "$MODEL_NAME" == "null" ]]; then error "No Ollama models found." && kill -9 $ollama_pid && return 1; fi
  msg "Using model '$MODEL_NAME' for PGO workload."
  while IFS= read -r prompt || [[ -n "$prompt" ]]; do
    [[ "$prompt" =~ ^#.* || -z "$prompt" ]] && continue
    msg "--> Running prompt: ${prompt:0:80}..."
    local payload
    payload=$(jq -n --arg model "$MODEL_NAME" --arg prompt "$prompt" --argjson threads "$(nproc)" '{"model": $model, "prompt": $prompt, "options": {"num_predict": 128, "num_thread": $threads}, "stream": false}')
    local response
    response=$(curl -s -X POST "http://$ollama_host/api/generate" -d "$payload")
    local eval_count eval_duration_ns
    eval_count=$(echo "$response" | jq '.eval_count')
    eval_duration_ns=$(echo "$response" | jq '.eval_duration')
    if [ "$eval_count" != "null" ] && [ "$eval_duration_ns" -gt 0 ]; then
      local eval_duration_s tps
      eval_duration_s=$(echo "scale=2; $eval_duration_ns / 1000000000" | bc)
      tps=$(echo "scale=1; $eval_count / $eval_duration_s" | bc)
      msg "    OK (${eval_count} tokens, ${tps} t/s)"
    fi
  done < ../workload.txt
  msg "PGO workload finished."
  kill $ollama_pid
  wait $ollama_pid 2>/dev/null


  ##################################################
  msg "=== PHASE 3: MERGE PROFILE DATA ==="
  ##################################################

  local PGO_SUCCESS=false
  mapfile -t PROFRAW_FILES < <(find . -name "*.profraw")

  if [ ${#PROFRAW_FILES[@]} -gt 0 ]; then
      msg "Found ${#PROFRAW_FILES[@]} profile files. Merging..."
      llvm-profdata merge -output="${PGO_DATA_DIR}/ollama.profdata" "${PROFRAW_FILES[@]}" && PGO_SUCCESS=true
  fi
  if [ "$PGO_SUCCESS" = false ]; then
      warning "No profile data generated. Skipping PGO optimizations."
  fi


  ##################################################
  msg "=== PHASE 4: Build OPTIMIZED Libraries and Final Binary ==="
  ##################################################

  msg "Rebuilding C++/HIP libraries with PGO data..."
  if [ "$PGO_SUCCESS" = true ]; then
      export CFLAGS="${ORIGINAL_CFLAGS} -fprofile-use=${PGO_DATA_DIR}/ollama.profdata"
      export CXXFLAGS="${ORIGINAL_CXXFLAGS} -fprofile-use=${PGO_DATA_DIR}/ollama.profdata"
      export LDFLAGS="${ORIGINAL_LDFLAGS} -fprofile-use=${PGO_DATA_DIR}/ollama.profdata"
  else
      export CFLAGS="$ORIGINAL_CFLAGS"; export CXXFLAGS="$ORIGINAL_CXXFLAGS"; export LDFLAGS="$ORIGINAL_LDFLAGS"
      warning "Building C++/HIP libraries without PGO."
  fi

  cmake -B build -DAMDGPU_TARGETS="gfx900" -DGPU_TARGETS="gfx900" -DCMAKE_PREFIX_PATH=/opt/rocm
  cmake --build build

  msg "Building final Go binary against optimized libraries..."
  unset CFLAGS CXXFLAGS LDFLAGS
  export CGO_CFLAGS="${ORIGINAL_CFLAGS}"
  export CGO_CXXFLAGS="${ORIGINAL_CXXFLAGS}"
  export CGO_LDFLAGS="${ORIGINAL_LDFLAGS} -L${BUILD_DIR}/lib/ollama"

  go build .
  msg "âœ“ Optimized build complete."
}

package_ollama() {
  DESTDIR="$pkgdir" cmake --install ollama/build --component CPU

  install -Dm755 $pkgname/ollama "$pkgdir/usr/bin/ollama"
  install -dm755 "$pkgdir/var/lib/ollama"
  install -Dm644 ollama.service "$pkgdir/usr/lib/systemd/system/ollama.service"
  install -Dm644 sysusers.conf "$pkgdir/usr/lib/sysusers.d/ollama.conf"
  install -Dm644 tmpfiles.d "$pkgdir/usr/lib/tmpfiles.d/ollama.conf"
  install -Dm644 ollama/LICENSE "$pkgdir/usr/share/licenses/$pkgname/LICENSE"

  ln -s /var/lib/ollama "$pkgdir/usr/share/ollama"
}

package_ollama-rocm() {
  pkgdesc='Create, run and share large language models (LLMs) with ROCm'
  depends+=(ollama hipblas)

  DESTDIR="$pkgdir" cmake --install ollama/build --component HIP
  rm -rf "$pkgdir"/usr/lib/ollama/rocm/rocblas/library
}

package_ollama-vulkan() {
  pkgdesc='Create, run and share large language models (LLMs) with Vulkan'
  depends=(ollama vulkan-icd-loader)

  DESTDIR="$pkgdir" cmake --install ollama/build --component Vulkan
}
