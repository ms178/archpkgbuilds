pkgbase=ollama
pkgname=(ollama ollama-rocm)
pkgver=0.6.1
pkgrel=2.1
pkgdesc='Create, run and share large language models (LLMs)'
arch=(x86_64)
url='https://github.com/ollama/ollama'
license=(MIT)
options=('!lto') # Disable LTO to avoid conflicts with PGO
makedepends=(cmake ninja git go hipblas clblast netcat)
source=(git+https://github.com/ollama/ollama#tag=v$pkgver
        ollama-ld.conf
        ollama.service
        sysusers.conf
        tmpfiles.d
        workload.txt)
b2sums=('SKIP'
        'SKIP'
        'SKIP'
        'SKIP'
        'SKIP'
        'SKIP')

prepare() {
  cd ollama

  # Remove runtime dependencies from CMake installation to avoid unnecessary system dependencies
  sed -i 's/PRE_INCLUDE_REGEXES.*/PRE_INCLUDE_REGEXES = ""/' CMakeLists.txt
}

build() {
  # Create build directory
  cd ollama
  BUILD_DIR="$(pwd)"

  # Set up directories for build
  PGO_DATA_DIR_GCC="${BUILD_DIR}/pgo_profiles_gcc"
  PGO_DATA_DIR_CLANG="${BUILD_DIR}/pgo_profiles_clang"
  mkdir -p "$PGO_DATA_DIR_GCC"
  mkdir -p "$PGO_DATA_DIR_CLANG"

  # Clean any existing profile data
  find "$BUILD_DIR" -name "*.gcda" -delete
  find "$BUILD_DIR" -name "*.profraw" -delete
  find "$BUILD_DIR" -name "*.profdata" -delete

  ##################################################
  echo "=== PHASE 1: PGO INSTRUMENTED BUILD ==="
  ##################################################

  # --- GCC (CPU) Flags ---
  ORIGINAL_CFLAGS_GCC="$CFLAGS"
  ORIGINAL_CXXFLAGS_GCC="$CXXFLAGS"
  ORIGINAL_LDFLAGS_GCC="$LDFLAGS"

  export CFLAGS_GCC="${ORIGINAL_CFLAGS_GCC} -fprofile-generate=${PGO_DATA_DIR_GCC}"
  export CXXFLAGS_GCC="${ORIGINAL_CXXFLAGS_GCC} -fprofile-generate=${PGO_DATA_DIR_GCC}"
  export LDFLAGS_GCC="${ORIGINAL_LDFLAGS_GCC} -fprofile-generate=${PGO_DATA_DIR_GCC}"

  # --- Clang (HIP) Flags ---
  #   Use -fprofile-instr-generate with environment variable for unique profile files
  ORIGINAL_CFLAGS_CLANG="$CFLAGS"
  ORIGINAL_CXXFLAGS_CLANG="$CXXFLAGS"
  ORIGINAL_LDFLAGS_CLANG="$LDFLAGS"

  export CFLAGS_CLANG="${ORIGINAL_CFLAGS_CLANG} -fprofile-instr-generate"
  export CXXFLAGS_CLANG="${ORIGINAL_CXXFLAGS_CLANG} -fprofile-instr-generate"
  export LDFLAGS_CLANG="${ORIGINAL_LDFLAGS_CLANG} -fprofile-instr-generate"
  export LLVM_PROFILE_FILE="${PGO_DATA_DIR_CLANG}/ollama-%p.profraw"

  # Set CGO flags (primarily for Go, which uses GCC)
  export CGO_CPPFLAGS="${CPPFLAGS}"
  export CGO_CFLAGS="${CFLAGS_GCC}"
  export CGO_CXXFLAGS="${CXXFLAGS_GCC}"
  export CGO_LDFLAGS="${LDFLAGS_GCC}"

  # Standard Go flags
  export GOPATH="${srcdir}"
  export GOFLAGS="-buildmode=pie -mod=readonly -modcacherw '-ldflags=-linkmode=external -compressdwarf=false -X=github.com/ollama/ollama/version.Version=$pkgver -X=github.com/ollama/ollama/server.mode=release' '-gcflags=all=-N -l' -trimpath -buildvcs=false"

  echo "Building C/C++ components with PGO instrumentation (GCC and Clang)..."
  rm -rf build
  cmake -B build -G Ninja \
    -DAMDGPU_TARGETS="gfx900" \
    -DCMAKE_C_COMPILER=gcc \
    -DCMAKE_CXX_COMPILER=g++ \
    -DCMAKE_C_FLAGS="${CFLAGS_GCC}" \
    -DCMAKE_CXX_FLAGS="${CXXFLAGS_GCC}" \
    -DCMAKE_EXE_LINKER_FLAGS="${LDFLAGS_GCC}" \
    -DCMAKE_SHARED_LINKER_FLAGS="${LDFLAGS_GCC}" \
    -DCMAKE_HIP_ARCHITECTURES="gfx900" \
    -DCMAKE_HIP_COMPILER=clang \
    -DCMAKE_HIP_FLAGS="${CFLAGS_CLANG}" \
    -DCMAKE_HIP_LINK_FLAGS="${LDFLAGS_CLANG}" \
    -DCMAKE_INSTALL_PREFIX=/usr
  cmake --build build

  echo "Building Go components with instrumentation..."
  go build -o ollama-pgo .

  echo "Instrumented build completed: $(ls -lh ollama-pgo)"

  ##################################################
  echo "=== PHASE 2: VERIFY SERVER & MODEL ==="
  ##################################################

  # (Rest of the server start and model selection code is the same as before,
  #  using the `ollama-pgo` binary and the selected model.  No changes needed here.)

  # Find an available port for our server
  find_available_port() {
    local port=12000
    local max_port=13000
    while [ $port -le $max_port ]; do
      if ! nc -z 127.0.0.1 "$port" >/dev/null 2>&1; then
        echo $port
        return 0
      fi
      port=$((port + 1))
    done
    echo "Error: No available ports found"
    return 1
  }

  ollama_port=$(find_available_port)
  if [ $? -ne 0 ]; then
    echo "Error: Unable to find an available port"
    exit 1
  fi

  # Server settings
  ollama_host="127.0.0.1:$ollama_port"
  SERVER_LOG="${PGO_DATA_DIR_GCC}/server.log" # Log in GCC dir

  # Get the directory with models
  USER_HOME="$HOME"
  SYSTEM_MODELS_DIR="$USER_HOME/.ollama/models"

  # Start the server with the instrumented binary - using env var, not command line flag
  echo "Starting instrumented server on port $ollama_port..."
  OLLAMA_HOST="$ollama_host" OLLAMA_MODELS="$SYSTEM_MODELS_DIR" OLLAMA_DEBUG=1 LLVM_PROFILE_FILE="${PGO_DATA_DIR_CLANG}/ollama-%p.profraw" ./ollama-pgo serve > "$SERVER_LOG" 2>&1 &
  ollama_pid=$!

  # Wait for server to start (with proper error handling)
  echo "Waiting for server to start..."
  server_ready=false
  for i in {1..30}; do
    if ! ps -p $ollama_pid > /dev/null 2>&1; then
      echo "ERROR: Server process terminated during startup"
      cat "$SERVER_LOG"
      exit 1
    fi

    if curl -s "http://$ollama_host/api/version" > /dev/null 2>&1; then
      echo "Server started successfully"
      server_ready=true
      break
    fi

    echo "Waiting... ($i/30)"
    sleep 2
  done

  if [ "$server_ready" = false ]; then
    echo "ERROR: Server failed to start within timeout"
    cat "$SERVER_LOG"
    if ps -p $ollama_pid > /dev/null 2>&1; then
      kill $ollama_pid
    fi
    exit 1
  fi

  # Check model availability - verify dolphin3-r1
  MODEL_NAME="dolphin3-r1:latest"
  echo "Checking for model $MODEL_NAME..."

  model_available=false
  if curl -s "http://$ollama_host/api/tags" | jq -e ".models[] | select(.name == \"$MODEL_NAME\")" > /dev/null 2>&1; then
      echo "✓ Model $MODEL_NAME is available"
      model_available=true
  else
    echo "⚠ Model $MODEL_NAME not found. Available models:"
    curl -s "http://$ollama_host/api/tags" | jq -r '.models[].name'
    echo "Using first available model instead"

    # Find a model to use instead
    AVAILABLE_MODELS=$(curl -s "http://$ollama_host/api/tags" | jq -r '.models[].name')
    if [ -n "$AVAILABLE_MODELS" ]; then
      # Try to find a small model first
        for model in llama2:7b tiny phi:latest gemma:2b cerebras:2b; do
            if echo "$AVAILABLE_MODELS" | grep -q "^$model$"; then
                echo "Selected small model: $model"
                MODEL_NAME="$model"
                model_available=true
                break
            fi
        done

      # If no small model found, use the first available
      if [ "$model_available" = false ]; then
        ALTERNATIVE_MODEL=$(echo "$AVAILABLE_MODELS" | head -n1)
        echo "Selected first available model: $ALTERNATIVE_MODEL"
        MODEL_NAME="$ALTERNATIVE_MODEL"
        model_available=true
      fi
    else
      echo "ERROR: No models available"
      if ps -p $ollama_pid > /dev/null 2>&1; then
        kill $ollama_pid
      fi
      exit 1
    fi
  fi

  ##################################################
  echo "=== PHASE 3: PROFILE GENERATION ==="
  ##################################################

# Function to run a simple generation and verify it works
test_generation() {
  local prompt="$1"
  local output_file="${PGO_DATA_DIR_GCC}/test_generation.json"

  echo "Testing generation with prompt: $prompt"

  curl -s -X POST "http://$ollama_host/api/generate" \
       -H "Content-Type: application/json" \
       -d "{\"model\":\"$MODEL_NAME\",\"prompt\":\"$prompt\",\"options\":{\"num_predict\":50}, \"stream\": false}" > "$output_file"
  curl_exit_code=$?

  if [ $curl_exit_code -ne 0 ]; then
    echo "✗ CURL request failed with exit code $curl_exit_code"
    cat "$output_file"
    return 1
  fi

  # Check if the response is valid JSON
  if ! jq -e '.' < "$output_file" > /dev/null 2>&1; then
    echo "✗ Server returned invalid JSON"
    cat "$output_file"
    return 1
  fi

  # Check for error field in the response
  if jq -e '.error' < "$output_file" > /dev/null 2>&1; then
    echo "✗ Server returned an error: $(jq -r '.error' < "$output_file")"
    return 1
  fi

  # Check if we got a non-empty response
  response=$(jq -r '.response' < "$output_file" 2>/dev/null)
  if [ $? -ne 0 ] || [ -z "$response" ]; then
    echo "✗ Test generation produced no output (empty response)"
    cat "$output_file"
    return 1
  fi

  echo "✓ Generation successful (response length: ${#response} characters)"
  return 0
}

# Function to generate with detailed output
run_generation() {
  local prompt="$1"
  local length="$2"
  local tag="$3"
  local output_file="${PGO_DATA_DIR_GCC}/generation_${tag}.json"

  echo "Generating response for: ${prompt:0:50}..."

  # Run generation and show progress
  start_time=$(date +%s)
  curl -s -X POST "http://$ollama_host/api/generate" \
       -H "Content-Type: application/json" \
       -d "{\"model\":\"$MODEL_NAME\",\"prompt\":$prompt,\"options\":{\"num_predict\":$length}, \"stream\": false}" > "$output_file"
  curl_exit_code=$?
  end_time=$(date +%s)

  if [ $curl_exit_code -ne 0 ]; then
    echo "✗ CURL request failed with exit code $curl_exit_code"
    cat "$output_file"
    return 1
  fi

  # Check if the response is valid JSON and contains a response field
  if ! jq -e '.' < "$output_file" > /dev/null 2>&1; then
    echo "✗ Server returned invalid JSON"
    cat "$output_file"
    return 1
  fi

  # Check for error field in the response
  if jq -e '.error' < "$output_file" > /dev/null 2>&1; then
    echo "✗ Server returned an error: $(jq -r '.error' < "$output_file")"
    return 1
  fi

  # Extract and verify the response field
  response=$(jq -r '.response' < "$output_file" 2>/dev/null)
  if [ $? -ne 0 ] || [ -z "$response" ]; then
    echo "✗ Generation produced no output (empty response)"
    cat "$output_file"
    return 1
  fi

  # Count tokens and show stats
  token_count=$(printf '%s' "$response" | wc -c)  # Use character count as a proxy for tokens
  duration=$((end_time - start_time))

  echo "✓ Generated response of length ${#response} characters (${token_count} estimated tokens) in $duration seconds"
  return 0
}

  # Run workload using prompts from workload.txt file
  echo "Running PGO workload from workload.txt..."

  # Set parameters for PGO workload
  max_prompts=12  # Number of prompts to process
  prompt_count=0
  current_section=""

  # Process workload.txt file
  while IFS= read -r line; do
    # Skip empty lines
    if [[ -z "$line" ]]; then
      continue
    fi

    # Track section headers
    if [[ "$line" =~ ^#\ Section ]]; then
      current_section="$line"
      echo "Processing section: ${current_section#\# }"
      continue
    fi

    # Skip comment lines
    if [[ "$line" =~ ^# ]]; then
      continue
    fi

    # Extract prompt from quotes if present
    if [[ "$line" =~ ^\"(.*)\"$ ]]; then
      prompt="${BASH_REMATCH[1]}"
    else
      prompt="$line"
    fi

    # Only process if we haven't reached max_prompts
    if [ $prompt_count -ge $max_prompts ]; then
      echo "Maximum number of prompts ($max_prompts) processed."
      break
    fi

    # Determine token count based on section
    token_count=200  # Default
    if [[ "$current_section" == *"Long-Form Text Generation"* ]]; then
      token_count=300
    elif [[ "$current_section" == *"Short-Form Text Generation"* ]]; then
      token_count=100
    elif [[ "$current_section" == *"Code Generation"* ]]; then
      token_count=250
    elif [[ "$current_section" == *"Translation"* ]]; then
      token_count=250
    elif [[ "$current_section" == *"Summarization"* ]]; then
      token_count=150
    elif [[ "$current_section" == *"GPU-Stressing"* ]]; then
      token_count=400
    fi

    # Increment prompt counter
    prompt_count=$((prompt_count + 1))

    # Properly escape the prompt for JSON
    escaped_prompt=$(printf '%s' "$prompt" | jq -sRr @json)

    # Run generation
    section_name=$(echo "$current_section" | sed 's/# Section [0-9]*: //;s/ /_/g')
    run_generation "$escaped_prompt" $token_count "${section_name}_${prompt_count}"

    # Brief pause between generations
    sleep 1
  done < ../workload.txt

  echo "Completed PGO workload with $prompt_count prompts."

  # Stop the server
  echo "Stopping Ollama server..."
  if ps -p $ollama_pid > /dev/null 2>&1; then
    kill $ollama_pid
    sleep 3
    if ps -p $ollama_pid > /dev/null 2>&1; then
      kill -9 $ollama_pid
    fi
  fi

  ##################################################
  echo "=== PHASE 4: VERIFY PROFILE DATA ==="
  ##################################################

  # --- Check GCC Profile Data ---
  echo "Searching for GCC profile data files..."
  find "$BUILD_DIR" -name "*.gcda" > "${PGO_DATA_DIR_GCC}/gcda_files.txt"
  GCDA_COUNT=$(wc -l < "${PGO_DATA_DIR_GCC}/gcda_files.txt")

  if [ "$GCDA_COUNT" -gt 0 ]; then
    echo "✓ Found $GCDA_COUNT GCC profile data files"
    PGO_SUCCESS_GCC=true
  else
    echo "⚠ No GCC profile data files found."
    PGO_SUCCESS_GCC=false
  fi

  # --- Check and Merge Clang Profile Data ---
  echo "Searching for Clang profile data files..."
  find "$BUILD_DIR" -name "*.profraw" > "${PGO_DATA_DIR_CLANG}/profraw_files.txt"
  PROFRAW_COUNT=$(wc -l < "${PGO_DATA_DIR_CLANG}/profraw_files.txt")

  if [ "$PROFRAW_COUNT" -gt 0 ]; then
    echo "✓ Found $PROFRAW_COUNT Clang profile data files"
    echo "Merging Clang profile data..."
    llvm-profdata merge -output="${PGO_DATA_DIR_CLANG}/ollama.profdata" $(cat "${PGO_DATA_DIR_CLANG}/profraw_files.txt")
    if [ $? -eq 0 ] && [ -f "${PGO_DATA_DIR_CLANG}/ollama.profdata" ]; then
      echo "✓ Successfully merged Clang profile data into ollama.profdata"
      PGO_SUCCESS_CLANG=true
    else
      echo "⚠ Failed to merge Clang profile data"
      PGO_SUCCESS_CLANG=false
    fi
  else
    echo "⚠ No Clang profile data files found."
    PGO_SUCCESS_CLANG=false
  fi

  # --- Overall PGO Success ---
  #   Consider PGO successful if *either* GCC or Clang PGO succeeded.
  #   You might want to adjust this logic based on your priorities.
  if [ "$PGO_SUCCESS_GCC" = true ] || [ "$PGO_SUCCESS_CLANG" = true ]; then
    PGO_SUCCESS=true
  else
    PGO_SUCCESS=false
  fi

  ##################################################
  echo "=== PHASE 5: OPTIMIZED REBUILD ==="
  ##################################################

  if [ "$PGO_SUCCESS" = true ]; then
    echo "Rebuilding with profile data..."

    # --- GCC (CPU) Optimized Flags ---
    if [ "$PGO_SUCCESS_GCC" = true ]; then
      export CFLAGS_GCC="${ORIGINAL_CFLAGS_GCC} -fprofile-use=${PGO_DATA_DIR_GCC} -fprofile-correction -Wno-error -Wno-missing-profile"
      export CXXFLAGS_GCC="${ORIGINAL_CXXFLAGS_GCC} -fprofile-use=${PGO_DATA_DIR_GCC} -fprofile-correction -Wno-error -Wno-missing-profile"
      export LDFLAGS_GCC="${ORIGINAL_LDFLAGS_GCC} -fprofile-use=${PGO_DATA_DIR_GCC} -fprofile-correction -Wno-error -Wno-missing-profile"
    else
      export CFLAGS_GCC="${ORIGINAL_CFLAGS_GCC} -Wno-error -Wno-missing-profile"
      export CXXFLAGS_GCC="${ORIGINAL_CXXFLAGS_GCC} -Wno-error -Wno-missing-profile"
      export LDFLAGS_GCC="${ORIGINAL_LDFLAGS_GCC} -Wno-error -Wno-missing-profile"
    fi

    # --- Clang (HIP) Optimized Flags ---
    if [ "$PGO_SUCCESS_CLANG" = true ]; then
      export CFLAGS_CLANG="${ORIGINAL_CFLAGS_CLANG} -fprofile-instr-use=${PGO_DATA_DIR_CLANG}/ollama.profdata -Wno-error -Wno-missing-profile"
      export CXXFLAGS_CLANG="${ORIGINAL_CXXFLAGS_CLANG} -fprofile-instr-use=${PGO_DATA_DIR_CLANG}/ollama.profdata -Wno-error -Wno-missing-profile"
      export LDFLAGS_CLANG="${ORIGINAL_LDFLAGS_CLANG} -fprofile-instr-use=${PGO_DATA_DIR_CLANG}/ollama.profdata -Wno-error -Wno-missing-profile"
    else
      export CFLAGS_CLANG="${ORIGINAL_CFLAGS_CLANG} -Wno-error -Wno-missing-profile"
      export CXXFLAGS_CLANG="${ORIGINAL_CXXFLAGS_CLANG} -Wno-error -Wno-missing-profile"
      export LDFLAGS_CLANG="${ORIGINAL_LDFLAGS_CLANG} -Wno-error -Wno-missing-profile"
    fi

    # Update CGO flags (still using GCC flags)
    export CGO_CFLAGS="${CFLAGS_GCC}"
    export CGO_CXXFLAGS="${CXXFLAGS_GCC}"
    export CGO_LDFLAGS="${LDFLAGS_GCC}"

  else
    echo "Rebuilding with standard optimizations (no PGO)..."
    # Use standard optimization flags
    export CFLAGS_GCC="${ORIGINAL_CFLAGS_GCC} -Wno-error -Wno-missing-profile"
    export CXXFLAGS_GCC="${ORIGINAL_CXXFLAGS_GCC} -Wno-error -Wno-missing-profile"
    export LDFLAGS_GCC="${ORIGINAL_LDFLAGS_GCC} -Wno-error -Wno-missing-profile"

    export CFLAGS_CLANG="${ORIGINAL_CFLAGS_CLANG} -Wno-error -Wno-missing-profile"
    export CXXFLAGS_CLANG="${ORIGINAL_CXXFLAGS_CLANG} -Wno-error -Wno-missing-profile"
    export LDFLAGS_CLANG="${ORIGINAL_LDFLAGS_CLANG} -Wno-error -Wno-missing-profile"

    # Update CGO flags
    export CGO_CFLAGS="${CFLAGS_GCC}"
    export CGO_CXXFLAGS="${CXXFLAGS_GCC}"
    export CGO_LDFLAGS="${LDFLAGS_GCC}"
  fi

  # Rebuild C/C++ components
  echo "Rebuilding C/C++ components..."
  cmake --build build --clean-first

  # Rebuild Go components (final binary)
  echo "Rebuilding Go components..."
  go build -o ollama .

  echo "✓ Build complete."
}

check() {
  cd ollama
  ./ollama --version > /dev/null
  go test .
}

package_ollama() {
  DESTDIR="$pkgdir" cmake --install ollama/build --component CPU

  install -Dm755 ollama/ollama "$pkgdir/usr/bin/ollama"
  install -dm755 "$pkgdir/var/lib/ollama"
  install -Dm644 ollama.service "$pkgdir/usr/lib/systemd/system/ollama.service"
  install -Dm644 sysusers.conf "$pkgdir/usr/lib/sysusers.d/ollama.conf"
  install -Dm644 tmpfiles.d "$pkgdir/usr/lib/tmpfiles.d/ollama.conf"
  install -Dm644 ollama/LICENSE "$pkgdir/usr/share/licenses/$pkgname/LICENSE"

  ln -s /var/lib/ollama "$pkgdir/usr/share/ollama"
}

package_ollama-rocm() {
  pkgdesc='Create, run and share large language models (LLMs) with ROCm'
  depends+=(ollama hipblas)

  DESTDIR="$pkgdir" cmake --install ollama/build --component HIP
  rm -rf "$pkgdir"/usr/lib/ollama/rocm/rocblas/library
}
