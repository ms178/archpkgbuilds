Switch to gnu17, a GCC bug-fix version of the C11 standard.

What is C17 and what changes have been made to the language?
https://stackoverflow.com/questions/47529854/

Signed-off-by: Mario Roy <>

diff -uarp a/arch/x86/Makefile b/arch/x86/Makefile
--- a/arch/x86/Makefile
+++ b/arch/x86/Makefile
@@ -48,7 +48,7 @@ endif
 
 # How to compile the 16-bit code.  Note we always compile for -march=i386;
 # that way we can complain to the user if the CPU is insufficient.
-REALMODE_CFLAGS	:= -std=gnu11 -m16 -g -Os -DDISABLE_BRANCH_PROFILING -D__DISABLE_EXPORTS \
+REALMODE_CFLAGS	:= -std=gnu17 -m16 -Os -DDISABLE_BRANCH_PROFILING -D__DISABLE_EXPORTS \
 		   -Wall -Wstrict-prototypes -march=i386 -mregparm=3 \
 		   -fno-strict-aliasing -fomit-frame-pointer -fno-pic \
 		   -mno-mmx -mno-sse $(call cc-option,-fcf-protection=none)
diff -uarp a/arch/x86/boot/compressed/Makefile b/arch/x86/boot/compressed/Makefile
--- a/arch/x86/boot/compressed/Makefile
+++ b/arch/x86/boot/compressed/Makefile
@@ -25,7 +25,7 @@ targets := vmlinux vmlinux.bin vmlinux.bin.gz vmlinux.bin.bz2 vmlinux.bin.lzma \
 # avoid errors with '-march=i386', and future flags may depend on the target to
 # be valid.
 KBUILD_CFLAGS := -m$(BITS) -O2 $(CLANG_FLAGS)
-KBUILD_CFLAGS += -std=gnu11
+KBUILD_CFLAGS += -std=gnu17
 KBUILD_CFLAGS += -fno-strict-aliasing -fPIE
 KBUILD_CFLAGS += -Wundef
 KBUILD_CFLAGS += -DDISABLE_BRANCH_PROFILING
diff -uarp a/drivers/firmware/efi/libstub/Makefile b/drivers/firmware/efi/libstub/Makefile
--- a/drivers/firmware/efi/libstub/Makefile
+++ b/drivers/firmware/efi/libstub/Makefile
@@ -11,7 +11,7 @@ cflags-y			:= $(KBUILD_CFLAGS)
 
 cflags-$(CONFIG_X86_32)		:= -march=i386
 cflags-$(CONFIG_X86_64)		:= -mcmodel=small
-cflags-$(CONFIG_X86)		+= -m$(BITS) -D__KERNEL__ -std=gnu11 \
+cflags-$(CONFIG_X86)		+= -m$(BITS) -D__KERNEL__ -std=gnu17 \
 				   -fPIC -fno-strict-aliasing -mno-red-zone \
 				   -mno-mmx -mno-sse -fshort-wchar \
 				   -Wno-pointer-sign \
diff -uarp a/Makefile b/Makefile
--- a/Makefile
+++ b/Makefile
@@ -464,7 +464,7 @@ HOSTRUSTC = rustc
 export KERNELDOC
 
 KBUILD_USERHOSTCFLAGS := -Wall -Wmissing-prototypes -Wstrict-prototypes \
-			 -O2 -fomit-frame-pointer -std=gnu11
+			 -O2 -fomit-frame-pointer -std=gnu17
 KBUILD_USERCFLAGS  := $(KBUILD_USERHOSTCFLAGS) $(USERCFLAGS)
 KBUILD_USERLDFLAGS := $(USERLDFLAGS)
 
@@ -584,7 +584,7 @@ LINUXINCLUDE    := \
 KBUILD_AFLAGS   := -D__ASSEMBLY__ -fno-PIE
 
 KBUILD_CFLAGS :=
-KBUILD_CFLAGS += -std=gnu11
+KBUILD_CFLAGS += -std=gnu17
 KBUILD_CFLAGS += -fshort-wchar
 KBUILD_CFLAGS += -funsigned-char
 KBUILD_CFLAGS += -fno-common
-- 
2.30.1

Speed up compression

diff -uarp a/scripts/Makefile.lib b/scripts/Makefile.lib
--- a/scripts/Makefile.lib
+++ b/scripts/Makefile.lib
@@ -459,13 +459,13 @@ quiet_cmd_xzmisc = XZMISC  $@
 # be used because it would require zstd to allocate a 128 MB buffer.
 
 quiet_cmd_zstd = ZSTD    $@
-      cmd_zstd = cat $(real-prereqs) | $(ZSTD) -19 > $@
+      cmd_zstd = cat $(real-prereqs) | $(ZSTD) -6 > $@
 
 quiet_cmd_zstd22 = ZSTD22  $@
-      cmd_zstd22 = cat $(real-prereqs) | $(ZSTD) -22 --ultra > $@
+      cmd_zstd22 = cat $(real-prereqs) | $(ZSTD) -6 --ultra > $@
 
 quiet_cmd_zstd22_with_size = ZSTD22  $@
-      cmd_zstd22_with_size = { cat $(real-prereqs) | $(ZSTD) -22 --ultra; $(size_append); } > $@
+      cmd_zstd22_with_size = { cat $(real-prereqs) | $(ZSTD) -6 --ultra; $(size_append); } > $@
 
 # ASM offsets
 # ---------------------------------------------------------------------------
-- 
2.30.1

Curated patches from ZEN
https://github.com/zen-kernel/zen-kernel

ZEN: INTERACTIVE: Tune ondemand governor for interactivity

diff --git a/drivers/cpufreq/cpufreq_ondemand.c b/drivers/cpufreq/cpufreq_ondemand.c
index 0e65d37c923113..b403fe4790dafd 100644
--- a/drivers/cpufreq/cpufreq_ondemand.c
+++ b/drivers/cpufreq/cpufreq_ondemand.c
@@ -18,10 +18,10 @@
 #include "cpufreq_ondemand.h"
 
 /* On-demand governor macros */
-#define DEF_FREQUENCY_UP_THRESHOLD		(80)
-#define DEF_SAMPLING_DOWN_FACTOR		(1)
+#define DEF_FREQUENCY_UP_THRESHOLD		(55)
+#define DEF_SAMPLING_DOWN_FACTOR		(5)
 #define MAX_SAMPLING_DOWN_FACTOR		(100000)
-#define MICRO_FREQUENCY_UP_THRESHOLD		(95)
+#define MICRO_FREQUENCY_UP_THRESHOLD		(60)
 #define MIN_FREQUENCY_UP_THRESHOLD		(1)
 #define MAX_FREQUENCY_UP_THRESHOLD		(100)
 
-- 
2.30.1

Curated patches from XanMod Linux
https://gitlab.com/xanmod/linux-patches

# 0008-XANMOD-block-mq-deadline-Increase-write-priority-to-.patch

Subject: [PATCH 08/20] XANMOD: block/mq-deadline: Increase write priority to
 improve responsiveness

Signed-off-by: Alexandre Frade <kernel@xxxxxxxxxx>
---
 block/mq-deadline.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/block/mq-deadline.c b/block/mq-deadline.c
index 3e741d33142d..26fca23aa43a 100644
--- a/block/mq-deadline.c
+++ b/block/mq-deadline.c
@@ -28,7 +28,7 @@
  * See Documentation/block/deadline-iosched.rst
  */
 static const int read_expire = HZ / 2;  /* max time before a read is submitted. */
-static const int write_expire = 5 * HZ; /* ditto for writes, these limits are SOFT! */
+static const int write_expire = HZ;     /* ditto for writes, these limits are SOFT! */
 /*
  * Time after which to dispatch lower priority requests even if higher
  * priority requests are pending.
-- 
2.47.2

# 0009-XANMOD-block-mq-deadline-Disable-front_merges-by-def.patch

Subject: [PATCH 09/20] XANMOD: block/mq-deadline: Disable front_merges by
 default

Signed-off-by: Alexandre Frade <kernel@xxxxxxxxxx>
---
 block/mq-deadline.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/block/mq-deadline.c b/block/mq-deadline.c
index 26fca23aa43a..148c21f84729 100644
--- a/block/mq-deadline.c
+++ b/block/mq-deadline.c
@@ -566,7 +566,7 @@ static int dd_init_sched(struct request_queue *q, struct elevator_queue *eq)
 	dd->fifo_expire[DD_READ] = read_expire;
 	dd->fifo_expire[DD_WRITE] = write_expire;
 	dd->writes_starved = writes_starved;
-	dd->front_merges = 1;
+	dd->front_merges = 0;
 	dd->last_dir = DD_WRITE;
 	dd->fifo_batch = fifo_batch;
 	dd->prio_aging_expire = prio_aging_expire;
-- 
2.47.2

# 0010-XANMOD-block-Set-rq_affinity-to-force-complete-I-O-r.patch

Subject: [PATCH 10/20] XANMOD: block: Set rq_affinity to force complete I/O
 requests on same CPU

Signed-off-by: Alexandre Frade <kernel@xxxxxxxxxx>
---
 include/linux/blkdev.h | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 70b671a9a7f7..814b7a6fe783 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -661,7 +661,8 @@ enum {
 	QUEUE_FLAG_MAX
 };
 
-#define QUEUE_FLAG_MQ_DEFAULT	(1UL << QUEUE_FLAG_SAME_COMP)
+#define QUEUE_FLAG_MQ_DEFAULT	((1UL << QUEUE_FLAG_SAME_COMP) |		\
+				 (1UL << QUEUE_FLAG_SAME_FORCE))
 
 void blk_queue_flag_set(unsigned int flag, struct request_queue *q);
 void blk_queue_flag_clear(unsigned int flag, struct request_queue *q);
-- 
2.47.2

# 0011-XANMOD-blk-wbt-Set-wbt_default_latency_nsec-to-2msec.patch

Subject: [PATCH 11/20] XANMOD: blk-wbt: Set wbt_default_latency_nsec() to
 2msec

Signed-off-by: Alexandre Frade <kernel@xxxxxxxxxx>
---
 block/blk-wbt.c | 10 ++--------
 1 file changed, 2 insertions(+), 8 deletions(-)

diff --git a/block/blk-wbt.c b/block/blk-wbt.c
index eb8037bae0bd..fe2d29903ae2 100644
--- a/block/blk-wbt.c
+++ b/block/blk-wbt.c
@@ -743,14 +743,8 @@ EXPORT_SYMBOL_GPL(wbt_enable_default);
 
 u64 wbt_default_latency_nsec(struct request_queue *q)
 {
-	/*
-	 * We default to 2msec for non-rotational storage, and 75msec
-	 * for rotational storage.
-	 */
-	if (blk_queue_nonrot(q))
-		return 2000000ULL;
-	else
-		return 75000000ULL;
+	/* XanMod defaults to 2msec for any type of storage */
+	return 2000000ULL;
 }
 
 static int wbt_data_dir(const struct request *rq)
-- 
2.47.2

# 0013-XANMOD-vfs-Decrease-rate-at-which-vfs-caches-are-rec.patch

Subject: [PATCH 13/20] XANMOD: vfs: Decrease rate at which vfs caches are
 reclaimed

Signed-off-by: Alexandre Frade <kernel@xxxxxxxxxx>
---
 fs/dcache.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/fs/dcache.c b/fs/dcache.c
index 035cccbc9276..c56793f84122 100644
--- a/fs/dcache.c
+++ b/fs/dcache.c
@@ -73,7 +73,7 @@
  * If no ancestor relationship:
  * arbitrary, since it's serialized on rename_lock
  */
-static int sysctl_vfs_cache_pressure __read_mostly = 100;
+static int sysctl_vfs_cache_pressure __read_mostly = 50;
 static int sysctl_vfs_cache_pressure_denom __read_mostly = 100;
 
 unsigned long vfs_pressure_ratio(unsigned long val)
-- 
2.47.2

# 0018-XANMOD-lib-kconfig.debug-disable-default-SYMBOLIC_ER.patch

Subject: [PATCH 18/20] XANMOD: lib/kconfig.debug: disable default
 SYMBOLIC_ERRNAME and DEBUG_BUGVERBOSE

Signed-off-by: Alexandre Frade <kernel@xxxxxxxxxx>
---
 lib/Kconfig.debug | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index 713cc94caa02..5a78ef05dbd2 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -190,7 +190,7 @@ config DYNAMIC_DEBUG_CORE
 
 config SYMBOLIC_ERRNAME
 	bool "Support symbolic error names in printf"
-	default y if PRINTK
+	default n
 	help
 	  If you say Y here, the kernel's printf implementation will
 	  be able to print symbolic error names such as ENOSPC instead
@@ -200,7 +200,7 @@ config SYMBOLIC_ERRNAME
 config DEBUG_BUGVERBOSE
 	bool "Verbose BUG() reporting (adds 70K)" if DEBUG_KERNEL
 	depends on BUG && (GENERIC_BUG || HAVE_DEBUG_BUGVERBOSE)
-	default y
+	default n
 	help
 	  Say Y here to make BUG() panics output the file name and line number
 	  of the BUG call as well as the EIP and oops trace.  This aids
-- 
2.47.2

Subject: [PATCH 2/2] ZEN: dm-crypt: Disable workqueues for crypto ops

Queueing in dm-crypt for crypto operations reduces performance on modern
systems.  As discussed in an article from Cloudflare, they discovered
that queuing was introduced because the crypto subsystem used to be
synchronous.  Since it's now asynchronous, we get double queueing when
using the subsystem through dm-crypt.  This is obviously undesirable and
reduces throughput and increases latency.

Fixes: https://github.com/zen-kernel/zen-kernel/issues/282
Signed-off-by: Alexandre Frade <kernel@xxxxxxxxxx>
---
 drivers/md/dm-crypt.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/drivers/md/dm-crypt.c b/drivers/md/dm-crypt.c
index 5ef43231fe77..c6f9815830e1 100644
--- a/drivers/md/dm-crypt.c
+++ b/drivers/md/dm-crypt.c
@@ -3305,6 +3305,9 @@ static int crypt_ctr(struct dm_target *ti, unsigned int argc, char **argv)
 			goto bad;
 	}
 
+	set_bit(DM_CRYPT_NO_READ_WORKQUEUE, &cc->flags);
+	set_bit(DM_CRYPT_NO_WRITE_WORKQUEUE, &cc->flags);
+
 	ret = crypt_ctr_cipher(ti, argv[0], argv[1]);
 	if (ret < 0)
 		goto bad;
-- 
2.47.2

Curated patches from SUNLIGHT Linux
https://github.com/sunlightlinux/linux-sunlight/commits/6.18

SUNLIGHT: x86/tsc: Use rdtsc_ordered() when RDTSCP or LFENCE_RDTSC are supported
https://github.com/sunlightlinux/linux-sunlight/commit/929c1af3dc63

On AMD processors the TSC has been reported drifting on and off for
various platforms. This has been root caused to becaused by out of order
TSC and HPET counter values. When the SoC supports RDTSCP or LFENCE_RDTSC
use ordered tsc reads instead.

Signed-off-by: Mario Limonciello <mario.limonciello@xxxxxxxxxx>
Signed-off-by: Ionut Nechita <ionut_n2001@xxxxxxxxxx>
---
 arch/x86/include/asm/tsc.h | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -79,6 +79,9 @@ static inline cycles_t get_cycles(void)
 	if (!IS_ENABLED(CONFIG_X86_TSC) &&
 	    !cpu_feature_enabled(X86_FEATURE_TSC))
 		return 0;
+	if (cpu_feature_enabled(X86_FEATURE_LFENCE_RDTSC) ||
+	    cpu_feature_enabled(X86_FEATURE_RDTSCP))
+		return rdtsc_ordered();
 	return rdtsc();
 }
 #define get_cycles get_cycles
-- 
2.43.0

SUNLIGHT: Change default value for wq_cpu_intensive_thresh_us
https://github.com/sunlightlinux/linux-sunlight/commit/bc5af06c8ce6

Description:
 - 10ms -> 30ms (edit: change to 20ms)
 - By using new processors it helps to have
   a higher threshold for thresh

Signed-off-by: Ionut Nechita <ionut_n2001@xxxxxxxxxx>
Signed-off-by: Mario Roy <>
---
 kernel/workqueue.c | 8 ++++----
 1 file changed, 4 insertions(+), 4 deletions(-)

diff --git a/kernel/workqueue.c b/kernel/workqueue.c
--- a/kernel/workqueue.c
+++ b/kernel/workqueue.c
@@ -7864,20 +7864,20 @@ static void __init wq_cpu_intensive_thresh_init(void)
 		return;
 
 	/*
-	 * The default of 10ms is derived from the fact that most modern (as of
-	 * 2023) processors can do a lot in 10ms and that it's just below what
+	 * The default of 20ms is derived from the fact that most modern (as of
+	 * 2023) processors can do a lot in 20ms and that it's just below what
 	 * most consider human-perceivable. However, the kernel also runs on a
 	 * lot slower CPUs including microcontrollers where the threshold is way
 	 * too low.
 	 *
-	 * Let's scale up the threshold upto 1 second if BogoMips is below 4000.
+	 * Let's scale up the threshold upto 2 second if BogoMips is below 4000.
 	 * This is by no means accurate but it doesn't have to be. The mechanism
 	 * is still useful even when the threshold is fully scaled up. Also, as
 	 * the reports would usually be applicable to everyone, some machines
 	 * operating on longer thresholds won't significantly diminish their
 	 * usefulness.
 	 */
-	thresh = 10 * USEC_PER_MSEC;
+	thresh = 20 * USEC_PER_MSEC;
 
 	/* see init/calibrate.c for lpj -> BogoMIPS calculation */
 	bogo = max_t(unsigned long, loops_per_jiffy / 500000 * HZ, 1);
-- 
2.43.0

SUNLIGHT: sched/core: Further optimize sched_move_task by avoiding
  lock acquisition when autogroup is enabled
https://github.com/sunlightlinux/linux-sunlight/commit/cd9f17bf83e7

Description:
 - Some sched_move_task calls are useless because task_struct->sched_task_group
   might not change (remains equal to task_group of cpu_cgroup) when the system
   has autogroup enabled. This patch optimizes the process by:

   1. Splitting sched_change_group() into two functions: one to get the task_group
   and another to perform the actual group change
   2. Adding a new sched_needs_group_change() function to check if a change is needed
   3. Most importantly, performing this check *before* acquiring the runqueue lock
   to completely avoid lock acquisition when no change is needed

   This approach provides a significant performance improvement over the original
   patch, reducing the time spent in sched_move_task by 74-96% (compared to the
   57.4% improvement in the original patch) in test scenarios involving frequent
   task creation and exit.

Based on the original idea from:
Link: https://lkml.kernel.org/r/20230321064459.39421-1-wuchi.zero@gmail.com
Signed-off-by: Ionut Nechita <ionut_n2001@xxxxxxxxxx>
---
 kernel/sched/core.c | 27 ++++++++++++++++++++++++---
 1 file changed, 24 insertions(+), 3 deletions(-)

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 2e61c9707bea..fa7b0d1e8be6 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -9179,7 +9179,7 @@ void sched_release_group(struct task_group *tg)
 	spin_unlock_irqrestore(&task_group_lock, flags);
 }
 
-static void sched_change_group(struct task_struct *tsk)
+static struct task_group *sched_get_task_group(struct task_struct *tsk)
 {
 	struct task_group *tg;
 
@@ -9191,8 +9191,13 @@ static void sched_change_group(struct task_struct *tsk)
 	tg = container_of(task_css_check(tsk, cpu_cgrp_id, true),
 			  struct task_group, css);
 	tg = autogroup_task_group(tsk, tg);
-	tsk->sched_task_group = tg;
 
+	return tg;
+}
+
+static void sched_change_group(struct task_struct *tsk, struct task_group *group)
+{
+	tsk->sched_task_group = group;
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	if (tsk->sched_class->task_change_group)
 		tsk->sched_class->task_change_group(tsk);
@@ -9201,6 +9206,18 @@ static void sched_change_group(struct task_struct *tsk)
 		set_task_rq(tsk, task_cpu(tsk));
 }
 
+static struct task_group *sched_needs_group_change(struct task_struct *tsk)
+{
+	struct task_group *new_group;
+
+	new_group = sched_get_task_group(tsk);
+
+	if (likely(new_group == tsk->sched_task_group))
+		return NULL;
+
+	return new_group;
+}
+
 /*
  * Change task's runqueue when it moves between groups.
  *
@@ -9210,10 +9227,14 @@ static void sched_change_group(struct task_struct *tsk)
  */
 void sched_move_task(struct task_struct *tsk, bool for_autogroup)
 {
+	struct task_group *new_group;
 	int queued, running, queue_flags =
 		DEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;
 	struct rq *rq;
 
+	if (!(new_group = sched_needs_group_change(tsk)))
+		return;
+
 	CLASS(task_rq_lock, rq_guard)(tsk);
 	rq = rq_guard.rq;
 
@@ -9227,7 +9248,7 @@ void sched_move_task(struct task_struct *tsk, bool for_autogroup)
 	if (running)
 		put_prev_task(rq, tsk);
 
-	sched_change_group(tsk);
+	sched_change_group(tsk, new_group);
 	if (!for_autogroup)
 		scx_cgroup_move_task(tsk);
 
-- 
2.43.0

sched/fair: do not scan twice in detach_tasks()
https://lore.kernel.org/all/20250722102600.25976-1-shijie@os.amperecomputing.com/

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 7cc9d50e3e11..9c1f21d59b5c 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -11765,12 +11765,15 @@ static int sched_balance_rq(int this_cpu, struct rq *this_rq,
 		 * still unbalanced. ld_moved simply stays zero, so it is
 		 * correctly treated as an imbalance.
 		 */
-		env.loop_max  = min(sysctl_sched_nr_migrate, busiest->nr_running);
-
 more_balance:
 		rq_lock_irqsave(busiest, &rf);
 		update_rq_clock(busiest);
 
+		if (!env.loop_max)
+			env.loop_max  = min(sysctl_sched_nr_migrate, busiest->cfs.h_nr_queued);
+		else
+			env.loop_max  = min(env.loop_max, busiest->cfs.h_nr_queued);
+
 		/*
 		 * cur_ld_moved - load moved in current iteration
 		 * ld_moved     - cumulative load moved across iterations
-- 
2.40.1

Bluetooth adapters of many Acer laptops don't work without it
Add BT support for ASUS TUF Gaming A16 and TX Gaming FA608FM

diff -uarp a/drivers/bluetooth/btusb.c b/drivers/bluetooth/btusb.c
--- a/drivers/bluetooth/btusb.c
+++ b/drivers/bluetooth/btusb.c
@@ -579,6 +579,10 @@ static const struct usb_device_id quirks
 						     BTUSB_WIDEBAND_SPEECH },
 	{ USB_DEVICE(0x13d3, 0x3591), .driver_info = BTUSB_REALTEK |
 						     BTUSB_WIDEBAND_SPEECH },
+	{ USB_DEVICE(0x13d3, 0x3601), .driver_info = BTUSB_REALTEK |
+						     BTUSB_WIDEBAND_SPEECH },
+	{ USB_DEVICE(0x13d3, 0x3612), .driver_info = BTUSB_REALTEK |
+						     BTUSB_WIDEBAND_SPEECH },
 	{ USB_DEVICE(0x13d3, 0x3618), .driver_info = BTUSB_REALTEK |
 						     BTUSB_WIDEBAND_SPEECH },
 	{ USB_DEVICE(0x0489, 0xe123), .driver_info = BTUSB_REALTEK |
-- 
2.40.2

Add Logitech C310

diff --git a/drivers/usb/core/quirks.c b/drivers/usb/core/quirks.c
index 0cf94c7a2c9c..20450f7a0c92 100644
--- a/drivers/usb/core/quirks.c
+++ b/drivers/usb/core/quirks.c
@@ -268,6 +268,9 @@ static const struct usb_device_id usb_quirk_list[] = {
 	/* Logitech Harmony 700-series */
 	{ USB_DEVICE(0x046d, 0xc122), .driver_info = USB_QUIRK_DELAY_INIT },
 
+	/* Logitech Webcam C310 */
+	{ USB_DEVICE(0x046d, 0x081b), .driver_info = USB_QUIRK_RESET_RESUME },
+
 	/* Philips PSC805 audio device */
 	{ USB_DEVICE(0x0471, 0x0155), .driver_info = USB_QUIRK_RESET_RESUME },
 
-- 
2.40.2

processor_idle: Skip dummy wait for processors based on Zen microarchitecture
https://lore.kernel.org/all/20220921063638.2489-1-kprateek.nayak@amd.com/

diff --git a/drivers/acpi/processor_idle.c b/drivers/acpi/processor_idle.c
index 2c2dc559e0f8de..48d6b137e8f6ae 100644
--- a/drivers/acpi/processor_idle.c
+++ b/drivers/acpi/processor_idle.c
@@ -524,8 +524,11 @@ static __cpuidle void io_idle(unsigned long addr)
 	inb(addr);
 
 #ifdef	CONFIG_X86
-	/* No delay is needed if we are in guest */
-	if (boot_cpu_has(X86_FEATURE_HYPERVISOR))
+	/*
+	 * No delay is needed if we are in guest or on a processor
+	 * based on the Zen microarchitecture.
+	 */
+	if (boot_cpu_has(X86_FEATURE_HYPERVISOR) || boot_cpu_has(X86_FEATURE_ZEN))
 		return;
 	/*
 	 * Modern (>=Nehalem) Intel systems use ACPI via intel_idle,
-- 
2.40.2

rcu: Reduce synchronize_rcu() latency by reporting GP kthread's CPU QS early
https://lore.kernel.org/all/20251230004124.438070-1-joelagnelf@nvidia.com/

diff --git a/kernel/rcu/tree.c b/kernel/rcu/tree.c
index 78c045a5ef03..b7c818cabe44 100644
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@ -160,6 +160,7 @@ static void rcu_report_qs_rnp(unsigned long mask, struct rcu_node *rnp,
 			      unsigned long gps, unsigned long flags);
 static void invoke_rcu_core(void);
 static void rcu_report_exp_rdp(struct rcu_data *rdp);
+static void rcu_report_qs_rdp(struct rcu_data *rdp);
 static void check_cb_ovld_locked(struct rcu_data *rdp, struct rcu_node *rnp);
 static bool rcu_rdp_is_offloaded(struct rcu_data *rdp);
 static bool rcu_rdp_cpu_online(struct rcu_data *rdp);
@@ -1983,6 +1984,17 @@ static noinline_for_stack bool rcu_gp_init(void)
 	if (IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD))
 		on_each_cpu(rcu_strict_gp_boundary, NULL, 0);
 
+	/*
+	 * Immediately report QS for the GP kthread's CPU. The GP kthread
+	 * cannot be in an RCU read-side critical section while running
+	 * the FQS scan. This eliminates the need for a second FQS wait
+	 * when all CPUs are idle.
+	 */
+	preempt_disable();
+	rcu_qs();
+	rcu_report_qs_rdp(this_cpu_ptr(&rcu_data));
+	preempt_enable();
+
 	return true;
 }
 

-- 
2.34.1

drm/amd: Fix hang on amdgpu unload by using pci_dev_is_disconnected()
https://lore.kernel.org/all/20260205164254.4091912-1-mario.limonciello@amd.com/

The commit 28695ca09d32 ("drm/amd: Clean up kfd node on surprise
disconnect") introduced early KFD cleanup when drm_dev_is_unplugged()
returns true. However, this causes hangs during normal module unload
(rmmod amdgpu).

The issue occurs because drm_dev_unplug() is called in amdgpu_pci_remove()
for all removal scenarios, not just surprise disconnects. This was done
intentionally in commit 39934d3ed5725c ("Revert "drm/amdgpu: TA unload
messages are not actually sent to psp when amdgpu is uninstalled"") to
fix IGT PCI software unplug test failures. As a result,
drm_dev_is_unplugged() returns true even during normal module unload,
triggering the early KFD cleanup inappropriately.

The correct check should distinguish between:
 - Actual surprise disconnect (eGPU unplugged): pci_dev_is_disconnected()
   returns true
 - Normal module unload (rmmod): pci_dev_is_disconnected() returns false

Replace drm_dev_is_unplugged() with pci_dev_is_disconnected() to ensure
the early cleanup only happens during true hardware disconnect events.

Fixes: 28695ca09d32 ("drm/amd: Clean up kfd node on surprise disconnect")
Signed-off-by: Mario Limonciello <mario.limonciello@xxxxxxxxxx>
---
 drivers/gpu/drm/amd/amdgpu/amdgpu_device.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c
index d2c3885de711f..8900e0dc8a61d 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c
@@ -4989,7 +4989,7 @@ void amdgpu_device_fini_hw(struct amdgpu_device *adev)
 	 * before ip_fini_early to prevent kfd locking refcount issues by calling
 	 * amdgpu_amdkfd_suspend()
 	 */
-	if (drm_dev_is_unplugged(adev_to_drm(adev)))
+	if (pci_dev_is_disconnected(adev->pdev))
 		amdgpu_amdkfd_device_fini_sw(adev);
 
 	amdgpu_device_ip_fini_early(adev);
@@ -5001,7 +5001,7 @@ void amdgpu_device_fini_hw(struct amdgpu_device *adev)
 
 	amdgpu_gart_dummy_page_fini(adev);
 
-	if (drm_dev_is_unplugged(adev_to_drm(adev)))
+	if (pci_dev_is_disconnected(adev->pdev))
 		amdgpu_device_unmap_mmio(adev);
 
 }
-- 
2.52.0

