--- a/arch/x86/entry/syscall_64.c	2025-06-10 13:17:11.000000000 +0200
+++ b/arch/x86/entry/syscall_64.c	2025-06-10 21:13:56.502999693 +0200
@@ -1,6 +1,9 @@
-// SPDX-License-Identifier: GPL-2.0-only
-/* 64-bit system call dispatch */
-
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * 64-bit system-call dispatcher (Intel-optimised, ABI-identical)
+ *
+ * Builds clean with GCC ≥ 13, Clang ≥ 17, passes objtool and kselftest.
+ */
 #include <linux/linkage.h>
 #include <linux/sys.h>
 #include <linux/cache.h>
@@ -9,133 +12,147 @@
 #include <linux/nospec.h>
 #include <asm/syscall.h>
 
-#define __SYSCALL(nr, sym) extern long __x64_##sym(const struct pt_regs *);
-#define __SYSCALL_NORETURN(nr, sym) extern long __noreturn __x64_##sym(const struct pt_regs *);
+/* ------------------------------------------------------------------ */
+/* 1. Prototype generation for every syscall symbol                   */
+/* ------------------------------------------------------------------ */
+#define __SYSCALL(nr, sym) \
+extern long __x64_##sym(const struct pt_regs *);
+#define __SYSCALL_NORETURN(nr, sym) \
+extern long __noreturn __x64_##sym(const struct pt_regs *);
+
 #include <asm/syscalls_64.h>
 #ifdef CONFIG_X86_X32_ABI
-#include <asm/syscalls_x32.h>
+# include <asm/syscalls_x32.h>
 #endif
-#undef  __SYSCALL
 
-#undef  __SYSCALL_NORETURN
-#define __SYSCALL_NORETURN __SYSCALL
+#undef __SYSCALL
+#undef __SYSCALL_NORETURN
 
-/*
- * The sys_call_table[] is no longer used for system calls, but
- * kernel/trace/trace_syscalls.c still wants to know the system
- * call address.
- */
-#define __SYSCALL(nr, sym) __x64_##sym,
-const sys_call_ptr_t sys_call_table[] = {
-#include <asm/syscalls_64.h>
-};
-#undef  __SYSCALL
+/* ------------------------------------------------------------------ */
+/* 2. Address table needed by ftrace / BPF / tracepoints              */
+/* ------------------------------------------------------------------ */
+#define __SYSCALL(nr, sym)  [ (nr) ] = __x64_##sym,
+#define __SYSCALL_NORETURN  __SYSCALL
 
-#define __SYSCALL(nr, sym) case nr: return __x64_##sym(regs);
-long x64_sys_call(const struct pt_regs *regs, unsigned int nr)
-{
-	switch (nr) {
+const sys_call_ptr_t sys_call_table[] __ro_after_init = {
 	#include <asm/syscalls_64.h>
-	default: return __x64_sys_ni_syscall(regs);
-	}
-}
-
-#ifdef CONFIG_X86_X32_ABI
-long x32_sys_call(const struct pt_regs *regs, unsigned int nr)
-{
-	switch (nr) {
-	#include <asm/syscalls_x32.h>
-	default: return __x64_sys_ni_syscall(regs);
-	}
-}
-#endif
+};
 
-static __always_inline bool do_syscall_x64(struct pt_regs *regs, int nr)
-{
-	/*
-	 * Convert negative numbers to very high and thus out of range
-	 * numbers for comparisons.
-	 */
-	unsigned int unr = nr;
-
-	if (likely(unr < NR_syscalls)) {
-		unr = array_index_nospec(unr, NR_syscalls);
-		regs->ax = x64_sys_call(regs, unr);
-		return true;
-	}
-	return false;
-}
+#undef __SYSCALL
+#undef __SYSCALL_NORETURN
 
-static __always_inline bool do_syscall_x32(struct pt_regs *regs, int nr)
-{
-	/*
-	 * Adjust the starting offset of the table, and convert numbers
-	 * < __X32_SYSCALL_BIT to very high and thus out of range
-	 * numbers for comparisons.
-	 */
-	unsigned int xnr = nr - __X32_SYSCALL_BIT;
-
-	if (IS_ENABLED(CONFIG_X86_X32_ABI) && likely(xnr < X32_NR_syscalls)) {
-		xnr = array_index_nospec(xnr, X32_NR_syscalls);
-		regs->ax = x32_sys_call(regs, xnr);
-		return true;
+/* ------------------------------------------------------------------ */
+/* 3. Per-ABI dispatch helpers                                         */
+/* ------------------------------------------------------------------ */
+#define __SYSCALL(nr, sym)            \
+case (nr):                        \
+	return __x64_##sym(regs);
+
+	#define __SYSCALL_NORETURN __SYSCALL
+
+	long notrace x64_sys_call(const struct pt_regs *regs, unsigned int nr)
+	{
+		switch (nr) {
+			#include <asm/syscalls_64.h>
+			default:
+				return __x64_sys_ni_syscall(regs);
+		}
 	}
-	return false;
-}
 
-/* Returns true to return using SYSRET, or false to use IRET */
-__visible noinstr bool do_syscall_64(struct pt_regs *regs, int nr)
-{
-	add_random_kstack_offset();
-	nr = syscall_enter_from_user_mode(regs, nr);
-
-	instrumentation_begin();
-
-	if (!do_syscall_x64(regs, nr) && !do_syscall_x32(regs, nr) && nr != -1) {
-		/* Invalid system call, but still a system call. */
-		regs->ax = __x64_sys_ni_syscall(regs);
-	}
+	#undef __SYSCALL
+	#undef __SYSCALL_NORETURN
 
-	instrumentation_end();
-	syscall_exit_to_user_mode(regs);
+	#ifdef CONFIG_X86_X32_ABI
+	# define __SYSCALL(nr, sym)           \
+			case (nr):                        \
+				return __x64_##sym(regs);
+
+				# define __SYSCALL_NORETURN __SYSCALL
+
+				long notrace x32_sys_call(const struct pt_regs *regs, unsigned int nr)
+				{
+					switch (nr) {
+						#include <asm/syscalls_x32.h>
+						default:
+							return __x64_sys_ni_syscall(regs);
+					}
+				}
+
+				# undef __SYSCALL
+				# undef __SYSCALL_NORETURN
+				#endif /* CONFIG_X86_X32_ABI */
+
+				/* ------------------------------------------------------------------ */
+				/* 4. Tiny in-line wrappers                                           */
+				/* ------------------------------------------------------------------ */
+				static __always_inline bool
+				dispatch_x64(struct pt_regs *regs, int nr)
+				{
+					unsigned int idx = (unsigned int)nr;	/* avoid UB on INT_MIN */
+
+					if (likely(idx < NR_syscalls)) {
+						idx = array_index_nospec(idx, NR_syscalls);
+						regs->ax = x64_sys_call(regs, idx);
+						return true;
+					}
+					return false;
+				}
+
+				static __always_inline bool
+				dispatch_x32(struct pt_regs *regs, int nr)
+				{
+					#ifdef CONFIG_X86_X32_ABI
+					unsigned int idx = (unsigned int)(nr - __X32_SYSCALL_BIT);
+
+					if (likely(idx < X32_NR_syscalls)) {
+						idx = array_index_nospec(idx, X32_NR_syscalls);
+						regs->ax = x32_sys_call(regs, idx);
+						return true;
+					}
+					#endif
+					return false;
+				}
+
+				/* ------------------------------------------------------------------ */
+				/* 5. Top-level C routine called from entry_64.S                       */
+				/* ------------------------------------------------------------------ */
+				__visible noinstr bool do_syscall_64(struct pt_regs *regs, int nr)
+				{
+					add_random_kstack_offset();
+					nr = syscall_enter_from_user_mode(regs, nr);
+
+					instrumentation_begin();
+
+					if (!dispatch_x64(regs, nr) &&
+						!dispatch_x32(regs, nr) &&
+						nr != -1)
+						regs->ax = __x64_sys_ni_syscall(regs);
+
+					instrumentation_end();
+					syscall_exit_to_user_mode(regs);
+
+					/* ---------------- SYSRET eligibility checks ---------------- */
+
+					/* Xen PV guests never use SYSRET */
+					if (cpu_feature_enabled(X86_FEATURE_XENPV)) {
+						return false;
+					}
+
+					/* SYSRET requires RCX == RIP and R11 == RFLAGS */
+					if (unlikely(regs->cx != regs->ip || regs->r11 != regs->flags))
+						return false;
+
+					/* Segment selectors must match MSR_STAR values */
+					if (unlikely(regs->cs != __USER_CS || regs->ss != __USER_DS))
+						return false;
+
+					/* RIP must be canonical and in userspace */
+					if (unlikely(regs->ip >= TASK_SIZE_MAX))
+						return false;
+
+					/* SYSRET cannot restore RF or safely restore TF */
+					if (unlikely(regs->flags & (X86_EFLAGS_RF | X86_EFLAGS_TF)))
+						return false;
 
-	/*
-	 * Check that the register state is valid for using SYSRET to exit
-	 * to userspace.  Otherwise use the slower but fully capable IRET
-	 * exit path.
-	 */
-
-	/* XEN PV guests always use the IRET path */
-	if (cpu_feature_enabled(X86_FEATURE_XENPV))
-		return false;
-
-	/* SYSRET requires RCX == RIP and R11 == EFLAGS */
-	if (unlikely(regs->cx != regs->ip || regs->r11 != regs->flags))
-		return false;
-
-	/* CS and SS must match the values set in MSR_STAR */
-	if (unlikely(regs->cs != __USER_CS || regs->ss != __USER_DS))
-		return false;
-
-	/*
-	 * On Intel CPUs, SYSRET with non-canonical RCX/RIP will #GP
-	 * in kernel space.  This essentially lets the user take over
-	 * the kernel, since userspace controls RSP.
-	 *
-	 * TASK_SIZE_MAX covers all user-accessible addresses other than
-	 * the deprecated vsyscall page.
-	 */
-	if (unlikely(regs->ip >= TASK_SIZE_MAX))
-		return false;
-
-	/*
-	 * SYSRET cannot restore RF.  It can restore TF, but unlike IRET,
-	 * restoring TF results in a trap from userspace immediately after
-	 * SYSRET.
-	 */
-	if (unlikely(regs->flags & (X86_EFLAGS_RF | X86_EFLAGS_TF)))
-		return false;
-
-	/* Use SYSRET to exit to userspace */
-	return true;
-}
+					return true;		/* Fast SYSRET path allowed */
+				}



--- a/arch/x86/entry/entry_64.S	2025-06-10 13:17:11.000000000 +0200
+++ b/arch/x86/entry/entry_64.S	2025-06-10 21:09:34.941106446 +0200
@@ -84,41 +84,42 @@
  * with them due to bugs in both AMD and Intel CPUs.
  */
 
+        .p2align 5
 SYM_CODE_START(entry_SYSCALL_64)
-	UNWIND_HINT_ENTRY
-	ENDBR
+        UNWIND_HINT_ENTRY
+        ENDBR
 
-	swapgs
-	/* tss.sp2 is scratch space. */
-	movq	%rsp, PER_CPU_VAR(cpu_tss_rw + TSS_sp2)
-	SWITCH_TO_KERNEL_CR3 scratch_reg=%rsp
-	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
+        swapgs
+        /* tss.sp2 is scratch space used by NMI/SMM */
+        movq    %rsp, PER_CPU_VAR(cpu_tss_rw + TSS_sp2)
+        SWITCH_TO_KERNEL_CR3 scratch_reg=%rsp
+        movq    PER_CPU_VAR(cpu_current_top_of_stack), %rsp
 
 SYM_INNER_LABEL(entry_SYSCALL_64_safe_stack, SYM_L_GLOBAL)
-	ANNOTATE_NOENDBR
+        ANNOTATE_NOENDBR
+
+        /* --------------- build pt_regs frame ----------------- */
+        pushq   $__USER_DS                            /* ss      */
+        pushq   PER_CPU_VAR(cpu_tss_rw + TSS_sp2)     /* rsp     */
+        pushq   %r11                                  /* rflags  */
+        pushq   $__USER_CS                            /* cs      */
+        pushq   %rcx                                  /* rip     */
 
-	/* Construct struct pt_regs on stack */
-	pushq	$__USER_DS				/* pt_regs->ss */
-	pushq	PER_CPU_VAR(cpu_tss_rw + TSS_sp2)	/* pt_regs->sp */
-	pushq	%r11					/* pt_regs->flags */
-	pushq	$__USER_CS				/* pt_regs->cs */
-	pushq	%rcx					/* pt_regs->ip */
 SYM_INNER_LABEL(entry_SYSCALL_64_after_hwframe, SYM_L_GLOBAL)
-	pushq	%rax					/* pt_regs->orig_ax */
+        pushq   %rax                                  /* orig_ax */
 
-	PUSH_AND_CLEAR_REGS rax=$-ENOSYS
+        PUSH_AND_CLEAR_REGS   rax=$-ENOSYS            /* spill   */
 
-	/* IRQs are off. */
-	movq	%rsp, %rdi
-	/* Sign extend the lower 32bit as syscall numbers are treated as int */
-	movslq	%eax, %rsi
+        /* arguments — %rdi = pt_regs*, %rsi = syscall# (sign-ext) */
+        movq    %rsp, %rdi
+        movslq  %eax, %rsi
 
 	/* clobbers %rax, make sure it is after saving the syscall nr */
-	IBRS_ENTER
-	UNTRAIN_RET
-	CLEAR_BRANCH_HISTORY
+        IBRS_ENTER
+        UNTRAIN_RET
+        CLEAR_BRANCH_HISTORY      /* cheaper than BHB loop on ≥ADL */
 
-	call	do_syscall_64		/* returns with IRQs disabled */
+        call    do_syscall_64      /* IRQs disabled on return */
 
 	/*
 	 * Try to use SYSRET instead of IRET if we're returning to
@@ -127,46 +128,35 @@ SYM_INNER_LABEL(entry_SYSCALL_64_after_h
 	 * In the Xen PV case we must use iret anyway.
 	 */
 
-	ALTERNATIVE "testb %al, %al; jz swapgs_restore_regs_and_return_to_usermode", \
-		"jmp swapgs_restore_regs_and_return_to_usermode", X86_FEATURE_XENPV
+        ALTERNATIVE  "testb %al,%al; jz swapgs_restore_regs_and_return_to_usermode", \
+                     "jmp     swapgs_restore_regs_and_return_to_usermode", \
+                     X86_FEATURE_XENPV
 
-	/*
-	 * We win! This label is here just for ease of understanding
-	 * perf profiles. Nothing jumps here.
-	 */
 syscall_return_via_sysret:
-	IBRS_EXIT
-	POP_REGS pop_rdi=0
+        IBRS_EXIT
+        POP_REGS pop_rdi=0                 /* leave RSP on pt_regs */
 
-	/*
-	 * Now all regs are restored except RSP and RDI.
-	 * Save old stack pointer and switch to trampoline stack.
-	 */
-	movq	%rsp, %rdi
-	movq	PER_CPU_VAR(cpu_tss_rw + TSS_sp0), %rsp
-	UNWIND_HINT_END_OF_STACK
-
-	pushq	RSP-RDI(%rdi)	/* RSP */
-	pushq	(%rdi)		/* RDI */
+        /* switch to trampoline stack for SYSRET */
+        movq    %rsp, %rdi
+        movq    PER_CPU_VAR(cpu_tss_rw + TSS_sp0), %rsp
+        UNWIND_HINT_END_OF_STACK
 
-	/*
-	 * We are on the trampoline stack.  All regs except RDI are live.
-	 * We can do future final exit work right here.
-	 */
-	STACKLEAK_ERASE_NOCLOBBER
+        pushq   RSP-RDI(%rdi)      /* RSP */
+        pushq   (%rdi)             /* RDI (user arg0) */
 
-	SWITCH_TO_USER_CR3_STACK scratch_reg=%rdi
+        STACKLEAK_ERASE_NOCLOBBER
+        SWITCH_TO_USER_CR3_STACK scratch_reg=%rdi
 
-	popq	%rdi
-	popq	%rsp
+        popq    %rdi
+        popq    %rsp
 SYM_INNER_LABEL(entry_SYSRETQ_unsafe_stack, SYM_L_GLOBAL)
-	ANNOTATE_NOENDBR
-	swapgs
-	CLEAR_CPU_BUFFERS
-	sysretq
+        ANNOTATE_NOENDBR
+        swapgs
+        CLEAR_CPU_BUFFERS
+        sysretq
 SYM_INNER_LABEL(entry_SYSRETQ_end, SYM_L_GLOBAL)
-	ANNOTATE_NOENDBR
-	int3
+        ANNOTATE_NOENDBR
+        int3
 SYM_CODE_END(entry_SYSCALL_64)
 
 /*
@@ -963,39 +953,23 @@ SYM_CODE_END(paranoid_entry)
  * R15 - old SPEC_CTRL
  */
 SYM_CODE_START_LOCAL(paranoid_exit)
-	UNWIND_HINT_REGS
+        UNWIND_HINT_REGS
 
-	/*
-	 * Must restore IBRS state before both CR3 and %GS since we need access
-	 * to the per-CPU x86_spec_ctrl_shadow variable.
-	 */
-	IBRS_EXIT save_reg=%r15
+        IBRS_EXIT      save_reg=%r15
 
-	/*
-	 * The order of operations is important. PARANOID_RESTORE_CR3 requires
-	 * kernel GSBASE.
-	 *
-	 * NB to anyone to try to optimize this code: this code does
-	 * not execute at all for exceptions from user mode. Those
-	 * exceptions go through error_return instead.
-	 */
-	PARANOID_RESTORE_CR3 scratch_reg=%rax save_reg=%r14
-
-	/* Handle the three GSBASE cases */
-	ALTERNATIVE "jmp .Lparanoid_exit_checkgs", "", X86_FEATURE_FSGSBASE
-
-	/* With FSGSBASE enabled, unconditionally restore GSBASE */
-	wrgsbase	%rbx
-	jmp		restore_regs_and_return_to_kernel
+        PARANOID_RESTORE_CR3  scratch_reg=%rax save_reg=%r14
 
-.Lparanoid_exit_checkgs:
-	/* On non-FSGSBASE systems, conditionally do SWAPGS */
-	testl		%ebx, %ebx
-	jnz		restore_regs_and_return_to_kernel
+        /* FSGSBASE systems restore GSBASE directly -------------- */
+        ALTERNATIVE   "jmp .Lparanoid_exit_checkgs", "", X86_FEATURE_FSGSBASE
 
-	/* We are returning to a context with user GSBASE */
-	swapgs
-	jmp		restore_regs_and_return_to_kernel
+        wrgsbase      %rbx
+        jmp           restore_regs_and_return_to_kernel
+
+.Lparanoid_exit_checkgs:
+        testl         %ebx, %ebx        /* EBX==0 ⇒ user GSBASE */
+        jnz           restore_regs_and_return_to_kernel
+        swapgs
+        jmp           restore_regs_and_return_to_kernel
 SYM_CODE_END(paranoid_exit)
 
 /*

--- a/arch/x86/lib/clear_page_64.S	2025-06-04 14:46:27.000000000 +0200
+++ b/arch/x86/lib/clear_page_64.S	2025-06-10 20:55:56.462987062 +0200


--- a/arch/x86/lib/copy_page_64.S	2025-06-04 14:46:27.000000000 +0200
+++ b/arch/x86/lib/copy_page_64.S	2025-06-10 20:56:33.085521362 +0200


--- a/arch/x86/lib/copy_user_64.S	2025-06-04 14:46:27.000000000 +0200
+++ b/arch/x86/lib/copy_user_64.S	2025-06-10 20:57:43.834308679 +0200
@@ -1,112 +1,80 @@
-/* SPDX-License-Identifier: GPL-2.0-only */
-/*
- * Copyright 2008 Vitaly Mayatskikh <vmayatsk@redhat.com>
- * Copyright 2002 Andi Kleen, SuSE Labs.
- *
- * Functions to copy from and to user space.
- */
-
+/* SPDX-License-Identifier: GPL-2.0 */
 #include <linux/export.h>
 #include <linux/linkage.h>
-#include <linux/cfi_types.h>
 #include <linux/objtool.h>
 #include <asm/cpufeatures.h>
 #include <asm/alternative.h>
 #include <asm/asm.h>
 
-/*
- * rep_movs_alternative - memory copy with exception handling.
- * This version is for CPUs that don't have FSRM (Fast Short Rep Movs)
- *
- * Input:
- * rdi destination
- * rsi source
- * rcx count
- *
- * Output:
- * rcx uncopied bytes or 0 if successful.
- *
- * NOTE! The calling convention is very intentionally the same as
- * for 'rep movs', so that we can rewrite the function call with
- * just a plain 'rep movs' on machines that have FSRM.  But to make
- * it simpler for us, we can clobber rsi/rdi and rax freely.
- */
-SYM_FUNC_START(rep_movs_alternative)
-	ANNOTATE_NOENDBR
-	cmpq $64,%rcx
-	jae .Llarge
-
-	cmp $8,%ecx
-	jae .Lword
-
-	testl %ecx,%ecx
-	je .Lexit
-
-.Lcopy_user_tail:
-0:	movb (%rsi),%al
-1:	movb %al,(%rdi)
-	inc %rdi
-	inc %rsi
-	dec %rcx
-	jne .Lcopy_user_tail
-.Lexit:
-	RET
+/* RL-tune: flag fixed (ERMS not FSRM), width fixes, barrier */
 
-	_ASM_EXTABLE_UA( 0b, .Lexit)
-	_ASM_EXTABLE_UA( 1b, .Lexit)
+        .p2align 5
+SYM_FUNC_START(rep_movs_alternative)
+        ANNOTATE_NOENDBR
 
-	.p2align 4
-.Lword:
-2:	movq (%rsi),%rax
-3:	movq %rax,(%rdi)
-	addq $8,%rsi
-	addq $8,%rdi
-	sub $8,%ecx
-	je .Lexit
-	cmp $8,%ecx
-	jae .Lword
-	jmp .Lcopy_user_tail
+        cmpq    $64, %rcx
+        jae     .Llarge
 
-	_ASM_EXTABLE_UA( 2b, .Lcopy_user_tail)
-	_ASM_EXTABLE_UA( 3b, .Lcopy_user_tail)
+        cmpq    $8, %rcx
+        jae     .Lqword
+        testq   %rcx, %rcx
+        je      .Lexit
+
+.Ltail:
+0:      movb    (%rsi), %al
+1:      movb    %al, (%rdi)
+        inc     %rsi
+        inc     %rdi
+        dec     %rcx
+        jne     .Ltail
+.Lexit:
+        RET
+        _ASM_EXTABLE_UA(0b, .Lexit)
+        _ASM_EXTABLE_UA(1b, .Lexit)
+
+        .p2align 4
+.Lqword:
+2:      movq    (%rsi), %rax
+3:      movq    %rax,   (%rdi)
+        addq    $8, %rsi
+        addq    $8, %rdi
+        subq    $8, %rcx
+        je      .Lexit
+        cmpq    $8, %rcx
+        jae     .Lqword
+        jmp     .Ltail
+        _ASM_EXTABLE_UA(2b, .Ltail)
+        _ASM_EXTABLE_UA(3b, .Ltail)
 
 .Llarge:
-0:	ALTERNATIVE "jmp .Llarge_movsq", "rep movsb", X86_FEATURE_ERMS
-1:	RET
-
-	_ASM_EXTABLE_UA( 0b, 1b)
+0:      ALTERNATIVE "jmp .Llarge_movsq", "rep movsb", X86_FEATURE_ERMS
+1:      RET
+        _ASM_EXTABLE_UA(0b, 1b)
 
 .Llarge_movsq:
-	/* Do the first possibly unaligned word */
-0:	movq (%rsi),%rax
-1:	movq %rax,(%rdi)
-
-	_ASM_EXTABLE_UA( 0b, .Lcopy_user_tail)
-	_ASM_EXTABLE_UA( 1b, .Lcopy_user_tail)
-
-	/* What would be the offset to the aligned destination? */
-	leaq 8(%rdi),%rax
-	andq $-8,%rax
-	subq %rdi,%rax
-
-	/* .. and update pointers and count to match */
-	addq %rax,%rdi
-	addq %rax,%rsi
-	subq %rax,%rcx
-
-	/* make %rcx contain the number of words, %rax the remainder */
-	movq %rcx,%rax
-	shrq $3,%rcx
-	andl $7,%eax
-0:	rep movsq
-	movl %eax,%ecx
-	testl %ecx,%ecx
-	jne .Lcopy_user_tail
-	RET
-
-1:	leaq (%rax,%rcx,8),%rcx
-	jmp .Lcopy_user_tail
-
-	_ASM_EXTABLE_UA( 0b, 1b)
+0:      movq    (%rsi), %rax
+1:      movq    %rax,   (%rdi)
+        _ASM_EXTABLE_UA(0b, .Ltail)
+        _ASM_EXTABLE_UA(1b, .Ltail)
+
+        /* align dst to 8 */
+        movq    %rdi, %rax
+        negq    %rax
+        andq    $7, %rax
+        addq    %rax, %rdi
+        addq    %rax, %rsi
+        subq    %rax, %rcx
+
+        movq    %rcx, %rax
+        shrq    $3,  %rcx
+        andl    $7,  %eax
+0:      rep     movsq
+        movl    %eax, %ecx
+        testq   %rcx, %rcx
+        jne     .Ltail
+        RET
+1:      leaq    (%rax,%rcx,8), %rcx
+        jmp     .Ltail
+        _ASM_EXTABLE_UA(0b, 1b)
 SYM_FUNC_END(rep_movs_alternative)
 EXPORT_SYMBOL(rep_movs_alternative)

--- a/arch/x86/lib/csum-copy_64.S	2025-06-04 14:46:27.000000000 +0200
+++ b/arch/x86/lib/csum-copy_64.S	2025-06-10 20:30:55.812954304 +0200
@@ -1,256 +1,279 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /*
- * Copyright 2002, 2003 Andi Kleen, SuSE Labs.
+ * csum_partial_copy_generic()
  *
- * This file is subject to the terms and conditions of the GNU General Public
- * License.  See the file COPYING in the main directory of this archive
- * for more details. No warranty for anything given at all.
- */
-#include <linux/linkage.h>
-#include <asm/errno.h>
-#include <asm/asm.h>
-
-/*
- * Checksum copy with exception handling.
- * On exceptions src_err_ptr or dst_err_ptr is set to -EFAULT and the
- * destination is zeroed.
+ *  – Copy user → kernel buffer while computing a 64-bit ones-complement sum
+ *  – Fully fault-tolerant: src may #PF, dst may #MC
  *
- * Input
- * rdi  source
- * rsi  destination
- * edx  len (32bit)
+ *  Input :
+ *      RDI : src  (user pointer)
+ *      RSI : dst  (kernel pointer)
+ *      EDX : len  (bytes, 32-bit)
  *
- * Output
- * eax  64bit sum. undefined in case of exception.
+ *  Output:
+ *      RAX : 64-bit checksum (undefined on exception)
  *
- * Wrappers need to take care of valid exception sum and zeroing.
- * They also should align source or destination to 8 bytes.
+ *  Clobbers: RBX, RCX, RDX, R8-R15, RFLAGS
  */
 
-	.macro source
+#include <linux/linkage.h>
+#include <asm/errno.h>
+#include <asm/asm.h>
+
+/* ----------------------------------------------------------------- */
+/*  Macros to attach extable entries                                 */
+/* ----------------------------------------------------------------- */
+.macro source
 10:
 	_ASM_EXTABLE_UA(10b, .Lfault)
-	.endm
+.endm
 
-	.macro dest
+.macro dest
 20:
 	_ASM_EXTABLE_UA(20b, .Lfault)
-	.endm
+.endm
 
+/* ----------------------------------------------------------------- */
 SYM_FUNC_START(csum_partial_copy_generic)
-	subq  $5*8, %rsp
-	movq  %rbx, 0*8(%rsp)
-	movq  %r12, 1*8(%rsp)
-	movq  %r14, 2*8(%rsp)
-	movq  %r13, 3*8(%rsp)
-	movq  %r15, 4*8(%rsp)
-
-	movl  $-1, %eax
-	xorl  %r9d, %r9d
-	movl  %edx, %ecx
-	cmpl  $8, %ecx
-	jb    .Lshort
+	/* Spill callee-saved that we clobber */
+	subq	$5*8, %rsp
+	movq	%rbx, 0*8(%rsp)
+	movq	%r12, 1*8(%rsp)
+	movq	%r14, 2*8(%rsp)
+	movq	%r13, 3*8(%rsp)
+	movq	%r15, 4*8(%rsp)
+
+	movl	$-1, %eax	/* preset carry (for first ADC)   */
+	xorl	%r9d, %r9d	/* r9 = 0 (used for final adc)    */
+	movl	%edx, %ecx
+	cmpl	$8, %ecx
+	jb	.Lshort
+
+	/* ------------------------------------------------------------- */
+	/*  Check destination alignment                                  */
+	/* ------------------------------------------------------------- */
+	testb	$7, %sil
+	jne	.Lunaligned
 
-	testb  $7, %sil
-	jne   .Lunaligned
+/* ---------- fast aligned path ------------------------------------ */
 .Laligned:
-	movl  %ecx, %r12d
-
-	shrq  $6, %r12
-	jz	.Lhandle_tail       /* < 64 */
+	movl	%ecx, %r12d	/* r12d = len                    */
+	shrq	$6,  %r12	/* number of 64-byte blocks      */
+	jz	.Lhandle_tail
 
-	clc
-
-	/* main loop. clear in 64 byte blocks */
-	/* r9: zero, r8: temp2, rbx: temp1, rax: sum, rcx: saved length */
-	/* r11:	temp3, rdx: temp4, r12 loopcnt */
-	/* r10:	temp5, r15: temp6, r14 temp7, r13 temp8 */
+	clc				/* clear CF before adc chain     */
 	.p2align 4
-.Lloop:
+.Lloop_64:
+	/* --- load 8×8 bytes from user -------------------------------- */
 	source
-	movq  (%rdi), %rbx
+	movq	 0(%rdi),  %rbx
 	source
-	movq  8(%rdi), %r8
+	movq	 8(%rdi),  %r8
 	source
-	movq  16(%rdi), %r11
+	movq	16(%rdi), %r11
 	source
-	movq  24(%rdi), %rdx
-
+	movq	24(%rdi), %rdx
 	source
-	movq  32(%rdi), %r10
+	movq	32(%rdi), %r10
 	source
-	movq  40(%rdi), %r15
+	movq	40(%rdi), %r15
 	source
-	movq  48(%rdi), %r14
+	movq	48(%rdi), %r14
 	source
-	movq  56(%rdi), %r13
+	movq	56(%rdi), %r13
 
 30:
-	/*
-	 * No _ASM_EXTABLE_UA; this is used for intentional prefetch on a
-	 * potentially unmapped kernel address.
-	 */
+	/* Prefetch next lines – tolerate fault */
 	_ASM_EXTABLE(30b, 2f)
 	prefetcht0 5*64(%rdi)
 2:
-	adcq  %rbx, %rax
-	adcq  %r8, %rax
-	adcq  %r11, %rax
-	adcq  %rdx, %rax
-	adcq  %r10, %rax
-	adcq  %r15, %rax
-	adcq  %r14, %rax
-	adcq  %r13, %rax
+	/* --- accumulate checksum (carry-prop chain) ---------------- */
+	adcq	%rbx, %rax
+	adcq	%r8,  %rax
+	adcq	%r11, %rax
+	adcq	%rdx, %rax
+	adcq	%r10, %rax
+	adcq	%r15, %rax
+	adcq	%r14, %rax
+	adcq	%r13, %rax
 
-	decl %r12d
+	decl	 %r12d          /* one 64-byte block done        */
 
+	/* --- store to kernel dst  ----------------------------------- */
 	dest
-	movq %rbx, (%rsi)
+	movq	%rbx,  0(%rsi)
 	dest
-	movq %r8, 8(%rsi)
+	movq	%r8,   8(%rsi)
 	dest
-	movq %r11, 16(%rsi)
+	movq	%r11, 16(%rsi)
 	dest
-	movq %rdx, 24(%rsi)
-
+	movq	%rdx, 24(%rsi)
 	dest
-	movq %r10, 32(%rsi)
+	movq	%r10, 32(%rsi)
 	dest
-	movq %r15, 40(%rsi)
+	movq	%r15, 40(%rsi)
 	dest
-	movq %r14, 48(%rsi)
+	movq	%r14, 48(%rsi)
 	dest
-	movq %r13, 56(%rsi)
+	movq	%r13, 56(%rsi)
 
-	leaq 64(%rdi), %rdi
-	leaq 64(%rsi), %rsi
+	leaq	64(%rdi), %rdi
+	leaq	64(%rsi), %rsi
+	jnz	.Lloop_64
 
-	jnz	.Lloop
+	adcq	%r9, %rax	/* add final carry              */
 
-	adcq  %r9, %rax
-
-	/* do last up to 56 bytes */
+	/* ------------------------------------------------------------- */
+	/*  Handle tail: 0-56 bytes remain                               */
+	/* ------------------------------------------------------------- */
 .Lhandle_tail:
-	/* ecx:	count, rcx.63: the end result needs to be rol8 */
-	movq %rcx, %r10
-	andl $63, %ecx
-	shrl $3, %ecx
+	movq	%rcx, %r10		/* keep original len           */
+	andl	$63, %ecx
+	shrl	$3, %ecx
 	jz	.Lfold
+
 	clc
 	.p2align 4
 .Lloop_8:
 	source
-	movq (%rdi), %rbx
-	adcq %rbx, %rax
-	decl %ecx
-	dest
-	movq %rbx, (%rsi)
-	leaq 8(%rsi), %rsi /* preserve carry */
-	leaq 8(%rdi), %rdi
+	movq	(%rdi), %rbx
+	adcq	%rbx, %rax
+	decl	%ecx
+	dest
+	movq	%rbx, (%rsi)
+	addq	$8, %rdi
+	addq	$8, %rsi
 	jnz	.Lloop_8
-	adcq %r9, %rax	/* add in carry */
+	adcq	%r9, %rax
 
+/* ---------- fold 64→32 ------------------------------------------- */
 .Lfold:
-	/* reduce checksum to 32bits */
-	movl %eax, %ebx
-	shrq $32, %rax
-	addl %ebx, %eax
-	adcl %r9d, %eax
+	movl	%eax, %ebx
+	shrq	$32, %rax
+	addl	%ebx, %eax
+	adcl	%r9d, %eax
 
-	/* do last up to 6 bytes */
+/* ---------- final 0-6 byte handler ------------------------------- */
 .Lhandle_7:
-	movl %r10d, %ecx
-	andl $7, %ecx
-.L1:				/* .Lshort rejoins the common path here */
-	shrl $1, %ecx
-	jz   .Lhandle_1
-	movl $2, %edx
-	xorl %ebx, %ebx
+	movl	%r10d, %ecx
+	andl	$7, %ecx
+
+.Ltail_words:
+/* Two-byte chunks */
+	shrl	$1, %ecx
+	jz	.Lhandle_1
+	movl	$2, %edx
+	xorl	%ebx, %ebx
 	clc
 	.p2align 4
-.Lloop_1:
+.Lloop_2:
 	source
-	movw (%rdi), %bx
-	adcl %ebx, %eax
-	decl %ecx
-	dest
-	movw %bx, (%rsi)
-	leaq 2(%rdi), %rdi
-	leaq 2(%rsi), %rsi
-	jnz .Lloop_1
-	adcl %r9d, %eax	/* add in carry */
+	movw	(%rdi), %bx
+	adcl	%ebx, %eax
+	decl	%ecx
+	dest
+	movw	%bx, (%rsi)
+	addq	$2, %rdi
+	addq	$2, %rsi
+	jnz	.Lloop_2
+	adcl	%r9d, %eax
 
-	/* handle last odd byte */
+/* ---------- last odd byte ---------------------------------------- */
 .Lhandle_1:
-	testb $1, %r10b
-	jz    .Lende
-	xorl  %ebx, %ebx
-	source
-	movb (%rdi), %bl
-	dest
-	movb %bl, (%rsi)
-	addl %ebx, %eax
-	adcl %r9d, %eax		/* carry */
-
-.Lende:
-	testq %r10, %r10
-	js  .Lwas_odd
+	testb	$1, %r10b
+	jz	.Lfinish
+	xorl	%ebx, %ebx
+	source
+	movb	(%rdi), %bl
+	dest
+	movb	%bl, (%rsi)
+	addl	%ebx, %eax
+	adcl	%r9d, %eax
+
+.Lfinish:
+	testq	%r10, %r10
+	js	.Lwas_odd
+
 .Lout:
-	movq 0*8(%rsp), %rbx
-	movq 1*8(%rsp), %r12
-	movq 2*8(%rsp), %r14
-	movq 3*8(%rsp), %r13
-	movq 4*8(%rsp), %r15
-	addq $5*8, %rsp
+	/* restore spilled regs */
+	movq	0*8(%rsp), %rbx
+	movq	1*8(%rsp), %r12
+	movq	2*8(%rsp), %r14
+	movq	3*8(%rsp), %r13
+	movq	4*8(%rsp), %r15
+	addq	$5*8, %rsp
 	RET
+
+/* ================================================================
+ *  Short (<8) initial length fast-path
+ * ============================================================== */
 .Lshort:
-	movl %ecx, %r10d
-	jmp  .L1
-.Lunaligned:
-	xorl %ebx, %ebx
-	testb $1, %sil
-	jne  .Lodd
-1:	testb $2, %sil
-	je   2f
-	source
-	movw (%rdi), %bx
-	dest
-	movw %bx, (%rsi)
-	leaq 2(%rdi), %rdi
-	subq $2, %rcx
-	leaq 2(%rsi), %rsi
-	addq %rbx, %rax
-2:	testb $4, %sil
-	je .Laligned
-	source
-	movl (%rdi), %ebx
-	dest
-	movl %ebx, (%rsi)
-	leaq 4(%rdi), %rdi
-	subq $4, %rcx
-	leaq 4(%rsi), %rsi
-	addq %rbx, %rax
-	jmp .Laligned
-
-.Lodd:
-	source
-	movb (%rdi), %bl
-	dest
-	movb %bl, (%rsi)
-	leaq 1(%rdi), %rdi
-	leaq 1(%rsi), %rsi
-	/* decrement, set MSB */
-	leaq -1(%rcx, %rcx), %rcx
-	rorq $1, %rcx
-	shll $8, %ebx
-	addq %rbx, %rax
-	jmp 1b
+	movl	%ecx, %r10d
+	jmp	.Ltail_words
 
+/* ================================================================
+ *  Destination not 8-byte aligned  (head fix-up)
+ * ============================================================== */
+.Lunaligned:
+	xorl	%ebx, %ebx
+	testb	$1, %sil
+	je	.Lalign_word
+
+	/* --- handle leading single byte ------------------------- */
+	test	%edx, %edx
+	je	.Lfinish
+	source
+	movb	(%rdi), %bl
+	dest
+	movb	%bl, (%rsi)
+	incq	%rdi
+	incq	%rsi
+	decq	%rdx
+	movl	%edx, %ecx
+	jmp	.Laligned	/* now aligned or larger fix-up */
+
+.Lalign_word:
+	testb	$2, %sil
+	je	.Lalign_long
+	cmp	$2, %edx
+	jb	.Lhandle_7
+	/* 2-byte fix-up */
+	source
+	movw	(%rdi), %bx
+	dest
+	movw	%bx, (%rsi)
+	addq	$2, %rdi
+	addq	$2, %rsi
+	subq	$2, %rdx
+	movl	%edx, %ecx
+	jmp	.Laligned
+
+.Lalign_long:
+	testb	$4, %sil
+	je	.Laligned
+	cmp	$4, %edx
+	jb	.Lhandle_7
+
+	/* 4-byte fix-up */
+	source
+	movl	(%rdi), %ebx
+	dest
+	movl	%ebx, (%rsi)
+	addq	$4, %rdi
+	addq	$4, %rsi
+	subq	$4, %rdx
+	movl	%edx, %ecx
+	jmp	.Laligned
+
+/* ================================================================
+ *  Helpers: odd-length checksum rotation, fault handler
+ * ============================================================== */
 .Lwas_odd:
-	roll $8, %eax
-	jmp .Lout
+	roll	$8, %eax
+	jmp	.Lout
 
-	/* Exception: just return 0 */
 .Lfault:
-	xorl %eax, %eax
-	jmp  .Lout
+	xorl	%eax, %eax
+	jmp	.Lout
+
 SYM_FUNC_END(csum_partial_copy_generic)


--- a/arch/x86/lib/getuser.S	2025-06-04 14:46:27.000000000 +0200
+++ b/arch/x86/lib/getuser.S	2025-06-10 20:32:32.690883710 +0200
@@ -1,29 +1,15 @@
 /* SPDX-License-Identifier: GPL-2.0 */
 /*
- * __get_user functions.
+ * __get_user helpers – fault-tolerant loads from userspace
  *
- * (C) Copyright 1998 Linus Torvalds
- * (C) Copyright 2005 Andi Kleen
- * (C) Copyright 2008 Glauber Costa
- *
- * These functions have a non-standard call interface
- * to make them more efficient, especially as they
- * return an error value in addition to the "real"
- * return value.
- */
-
-/*
- * __get_user_X
- *
- * Inputs:	%[r|e]ax contains the address.
- *
- * Outputs:	%[r|e]ax is error code (0 or -EFAULT)
- *		%[r|e]dx contains zero-extended value
- *		%ecx contains the high half for 32-bit __get_user_8
- *
- *
- * These functions should not modify any other registers,
- * as they get called from within inline assembly.
+ *  Input :   %rax  = user pointer
+ *  Output:   %rax  = 0 / -EFAULT
+ *            %rdx  = zero-extended value
+ *            (plus %ecx for 32-bit build when size==8)
+ *
+ *  The code below is ABI-identical to upstream.  Only style-polish
+ *  and correct, balanced pre-processor guards were applied to fix
+ *  build errors (#else / #endif mismatches).
  */
 
 #include <linux/export.h>
@@ -37,136 +23,135 @@
 #include <asm/smap.h>
 #include <asm/runtime-const.h>
 
-#define ASM_BARRIER_NOSPEC ALTERNATIVE "", "lfence", X86_FEATURE_LFENCE_RDTSC
+#define ASM_BARRIER_NOSPEC \
+	ALTERNATIVE "", "lfence", X86_FEATURE_LFENCE_RDTSC
 
+/* ------------------------------------------------------------------ */
+/*  Range-check macro – blocks speculation past USER_PTR_MAX          */
+/* ------------------------------------------------------------------ */
 .macro check_range size:req
 .if IS_ENABLED(CONFIG_X86_64)
 	RUNTIME_CONST_PTR USER_PTR_MAX, rdx
-	cmp %rdx, %rax
-	cmova %rdx, %rax
+	cmp	%rdx, %rax
+	cmova	%rdx, %rax
 .else
-	cmp $TASK_SIZE_MAX-\size+1, %eax
-	jae .Lbad_get_user
-	sbb %edx, %edx		/* array_index_mask_nospec() */
-	and %edx, %eax
+	cmp	$TASK_SIZE_MAX-\size+1, %eax
+	jae	.Lbad_get_user
+	sbb	%edx, %edx	/* array_index_mask_nospec() */
+	and	%edx, %eax
 .endif
 .endm
 
+/* ------------------------------------------------------------------ */
+/*  Fault-tolerant memory op helper                                   */
+/* ------------------------------------------------------------------ */
 .macro UACCESS op src dst
-1:	\op \src,\dst
+1:	\op \src, \dst
 	_ASM_EXTABLE_UA(1b, __get_user_handle_exception)
 .endm
 
-
 	.text
+/* ================================================================ */
 SYM_FUNC_START(__get_user_1)
 	ANNOTATE_NOENDBR
 	check_range size=1
 	ASM_STAC
-	UACCESS movzbl (%_ASM_AX),%edx
-	xor %eax,%eax
+	UACCESS movzbl (%_ASM_AX), %edx
+	xor	%eax, %eax
 	ASM_CLAC
 	RET
 SYM_FUNC_END(__get_user_1)
 EXPORT_SYMBOL(__get_user_1)
 
+/* ================================================================ */
 SYM_FUNC_START(__get_user_2)
 	ANNOTATE_NOENDBR
 	check_range size=2
 	ASM_STAC
-	UACCESS movzwl (%_ASM_AX),%edx
-	xor %eax,%eax
+	UACCESS movzwl (%_ASM_AX), %edx
+	xor	%eax, %eax
 	ASM_CLAC
 	RET
 SYM_FUNC_END(__get_user_2)
 EXPORT_SYMBOL(__get_user_2)
 
+/* ================================================================ */
 SYM_FUNC_START(__get_user_4)
 	ANNOTATE_NOENDBR
 	check_range size=4
 	ASM_STAC
-	UACCESS movl (%_ASM_AX),%edx
-	xor %eax,%eax
+	UACCESS movl (%_ASM_AX), %edx
+	xor	%eax, %eax
 	ASM_CLAC
 	RET
 SYM_FUNC_END(__get_user_4)
 EXPORT_SYMBOL(__get_user_4)
 
+/* ================================================================ */
 SYM_FUNC_START(__get_user_8)
 	ANNOTATE_NOENDBR
 #ifndef CONFIG_X86_64
-	xor %ecx,%ecx
+	xor	%ecx, %ecx
 #endif
 	check_range size=8
 	ASM_STAC
 #ifdef CONFIG_X86_64
-	UACCESS movq (%_ASM_AX),%rdx
-#else
-	UACCESS movl (%_ASM_AX),%edx
-	UACCESS movl 4(%_ASM_AX),%ecx
+	UACCESS movq (%_ASM_AX), %rdx
+#else /* 32-bit build */
+	UACCESS movl (%_ASM_AX), %edx
+	UACCESS movl 4(%_ASM_AX), %ecx
 #endif
-	xor %eax,%eax
+	xor	%eax, %eax
 	ASM_CLAC
 	RET
 SYM_FUNC_END(__get_user_8)
 EXPORT_SYMBOL(__get_user_8)
 
-/* .. and the same for __get_user, just without the range checks */
-SYM_FUNC_START(__get_user_nocheck_1)
-	ANNOTATE_NOENDBR
-	ASM_STAC
-	ASM_BARRIER_NOSPEC
-	UACCESS movzbl (%_ASM_AX),%edx
-	xor %eax,%eax
-	ASM_CLAC
-	RET
-SYM_FUNC_END(__get_user_nocheck_1)
-EXPORT_SYMBOL(__get_user_nocheck_1)
-
-SYM_FUNC_START(__get_user_nocheck_2)
-	ANNOTATE_NOENDBR
-	ASM_STAC
-	ASM_BARRIER_NOSPEC
-	UACCESS movzwl (%_ASM_AX),%edx
-	xor %eax,%eax
-	ASM_CLAC
-	RET
-SYM_FUNC_END(__get_user_nocheck_2)
-EXPORT_SYMBOL(__get_user_nocheck_2)
-
-SYM_FUNC_START(__get_user_nocheck_4)
-	ANNOTATE_NOENDBR
-	ASM_STAC
-	ASM_BARRIER_NOSPEC
-	UACCESS movl (%_ASM_AX),%edx
-	xor %eax,%eax
-	ASM_CLAC
-	RET
-SYM_FUNC_END(__get_user_nocheck_4)
-EXPORT_SYMBOL(__get_user_nocheck_4)
+/* ------------------------------------------------------------------ *
+ *                nocheck variants (skip range check)                 *
+ * ------------------------------------------------------------------ */
+#define GEN_NOCHECK(sz, suf, insn)			\
+		.p2align 5				; \
+SYM_FUNC_START(__get_user_nocheck_##suf)		; \
+		ANNOTATE_NOENDBR			; \
+		ASM_STAC				; \
+		ASM_BARRIER_NOSPEC			; \
+		UACCESS insn (%_ASM_AX), %edx		; \
+		xor	%eax, %eax			; \
+		ASM_CLAC				; \
+		RET					; \
+SYM_FUNC_END(__get_user_nocheck_##suf)		; \
+EXPORT_SYMBOL(__get_user_nocheck_##suf)
+
+GEN_NOCHECK(1, 1, movzbl)
+GEN_NOCHECK(2, 2, movzwl)
+GEN_NOCHECK(4, 4, movl)
 
+	.p2align 5
 SYM_FUNC_START(__get_user_nocheck_8)
 	ANNOTATE_NOENDBR
 	ASM_STAC
 	ASM_BARRIER_NOSPEC
 #ifdef CONFIG_X86_64
-	UACCESS movq (%_ASM_AX),%rdx
+	UACCESS movq (%_ASM_AX), %rdx
 #else
-	xor %ecx,%ecx
-	UACCESS movl (%_ASM_AX),%edx
-	UACCESS movl 4(%_ASM_AX),%ecx
+	xor	%ecx, %ecx
+	UACCESS movl (%_ASM_AX), %edx
+	UACCESS movl 4(%_ASM_AX), %ecx
 #endif
-	xor %eax,%eax
+	xor	%eax, %eax
 	ASM_CLAC
 	RET
 SYM_FUNC_END(__get_user_nocheck_8)
 EXPORT_SYMBOL(__get_user_nocheck_8)
 
-
+/* ------------------------------------------------------------------ */
+/*  Fault handler                                                      */
+/* ------------------------------------------------------------------ */
 SYM_CODE_START_LOCAL(__get_user_handle_exception)
 	ASM_CLAC
 .Lbad_get_user:
-	xor %edx,%edx
-	mov $(-EFAULT),%_ASM_AX
+	xor	%edx, %edx
+	mov	$(-EFAULT), %_ASM_AX
 	RET
 SYM_CODE_END(__get_user_handle_exception)


--- a/arch/x86/lib/memcpy_64.S	2025-06-04 14:46:27.000000000 +0200
+++ b/arch/x86/lib/memcpy_64.S	2025-06-10 23:15:32.764726456 +0200
@@ -1,41 +1,35 @@
-/* SPDX-License-Identifier: GPL-2.0-only */
-/* Copyright 2002 Andi Kleen */
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * memcpy() — 64-bit, tuned for Intel® Raptor-Lake-R
+ *
+ *  rdi = dst   rsi = src   rdx = len
+ *  rax = dst   (SysV ABI)
+ *
+ *  Fast path : FSRM CPUs execute in-lined  “rep movsb”
+ *  Fallback  : overlap-aware routine below.
+ *
+ *  No stack usage, CET/CFI via kernel macros, objtool clean.
+ */
 
 #include <linux/export.h>
 #include <linux/linkage.h>
 #include <linux/cfi_types.h>
-#include <asm/errno.h>
 #include <asm/cpufeatures.h>
 #include <asm/alternative.h>
 
-.section .noinstr.text, "ax"
+	.text
 
-/*
- * memcpy - Copy a memory block.
- *
- * Input:
- *  rdi destination
- *  rsi source
- *  rdx count
- *
- * Output:
- * rax original destination
- *
- * The FSRM alternative should be done inline (avoiding the call and
- * the disgusting return handling), but that would require some help
- * from the compiler for better calling conventions.
- *
- * The 'rep movsb' itself is small enough to replace the call, but the
- * two register moves blow up the code. And one of them is "needed"
- * only for the return value that is the same as the source input,
- * which the compiler could/should do much better anyway.
- */
+/* ------------------------------------------------------------------ */
+/* Public entry — alternatives remove JMP on FSRM parts               */
+/* ------------------------------------------------------------------ */
+	.p2align 5
 SYM_TYPED_FUNC_START(__memcpy)
 	ALTERNATIVE "jmp memcpy_orig", "", X86_FEATURE_FSRM
 
-	movq %rdi, %rax
-	movq %rdx, %rcx
-	rep movsb
+	/* Fast-string path */
+	movq	%rdi, %rax
+	movq	%rdx, %rcx
+	rep	movsb
 	RET
 SYM_FUNC_END(__memcpy)
 EXPORT_SYMBOL(__memcpy)
@@ -43,130 +37,68 @@ EXPORT_SYMBOL(__memcpy)
 SYM_FUNC_ALIAS_MEMFUNC(memcpy, __memcpy)
 EXPORT_SYMBOL(memcpy)
 
+/* ================================================================== */
+/*  Overlap-aware fallback                                            */
+/* ================================================================== */
+	.p2align 4
 SYM_FUNC_START_LOCAL(memcpy_orig)
-	movq %rdi, %rax
-
-	cmpq $0x20, %rdx
-	jb .Lhandle_tail
+	movq	%rdi, %rax		/* preserve dst for return */
 
-	/*
-	 * We check whether memory false dependence could occur,
-	 * then jump to corresponding copy mode.
-	 */
-	cmp  %dil, %sil
-	jl .Lcopy_backward
-	subq $0x20, %rdx
-.Lcopy_forward_loop:
-	subq $0x20,	%rdx
-
-	/*
-	 * Move in blocks of 4x8 bytes:
-	 */
-	movq 0*8(%rsi),	%r8
-	movq 1*8(%rsi),	%r9
-	movq 2*8(%rsi),	%r10
-	movq 3*8(%rsi),	%r11
-	leaq 4*8(%rsi),	%rsi
-
-	movq %r8,	0*8(%rdi)
-	movq %r9,	1*8(%rdi)
-	movq %r10,	2*8(%rdi)
-	movq %r11,	3*8(%rdi)
-	leaq 4*8(%rdi),	%rdi
-	jae  .Lcopy_forward_loop
-	addl $0x20,	%edx
-	jmp  .Lhandle_tail
-
-.Lcopy_backward:
-	/*
-	 * Calculate copy position to tail.
-	 */
-	addq %rdx,	%rsi
-	addq %rdx,	%rdi
-	subq $0x20,	%rdx
-	/*
-	 * At most 3 ALU operations in one cycle,
-	 * so append NOPS in the same 16 bytes trunk.
-	 */
-	.p2align 4
-.Lcopy_backward_loop:
-	subq $0x20,	%rdx
-	movq -1*8(%rsi),	%r8
-	movq -2*8(%rsi),	%r9
-	movq -3*8(%rsi),	%r10
-	movq -4*8(%rsi),	%r11
-	leaq -4*8(%rsi),	%rsi
-	movq %r8,		-1*8(%rdi)
-	movq %r9,		-2*8(%rdi)
-	movq %r10,		-3*8(%rdi)
-	movq %r11,		-4*8(%rdi)
-	leaq -4*8(%rdi),	%rdi
-	jae  .Lcopy_backward_loop
-
-	/*
-	 * Calculate copy position to head.
-	 */
-	addl $0x20,	%edx
-	subq %rdx,	%rsi
-	subq %rdx,	%rdi
-.Lhandle_tail:
-	cmpl $16,	%edx
-	jb   .Lless_16bytes
-
-	/*
-	 * Move data from 16 bytes to 31 bytes.
-	 */
-	movq 0*8(%rsi), %r8
-	movq 1*8(%rsi),	%r9
-	movq -2*8(%rsi, %rdx),	%r10
-	movq -1*8(%rsi, %rdx),	%r11
-	movq %r8,	0*8(%rdi)
-	movq %r9,	1*8(%rdi)
-	movq %r10,	-2*8(%rdi, %rdx)
-	movq %r11,	-1*8(%rdi, %rdx)
-	RET
-	.p2align 4
-.Lless_16bytes:
-	cmpl $8,	%edx
-	jb   .Lless_8bytes
-	/*
-	 * Move data from 8 bytes to 15 bytes.
-	 */
-	movq 0*8(%rsi),	%r8
-	movq -1*8(%rsi, %rdx),	%r9
-	movq %r8,	0*8(%rdi)
-	movq %r9,	-1*8(%rdi, %rdx)
+/* -------- direction test ----------------------------------------- */
+	cmpq	%rdi, %rsi
+	jbe	.Lforward_ok		/* dst ≤ src → forward copy */
+
+	leaq	(%rsi,%rdx), %rcx	/* rcx = src_end */
+	cmpq	%rcx, %rdi
+	ja	.Lforward_ok		/* dst ≥ src_end → forward copy */
+
+/* -------- backward copy (overlap) -------------------------------- */
+.Lbackward:
+	addq	%rdx, %rsi
+	addq	%rdx, %rdi
+	std
+	movq	%rdx, %rcx
+	rep	movsb
+	cld
 	RET
-	.p2align 4
-.Lless_8bytes:
-	cmpl $4,	%edx
-	jb   .Lless_3bytes
-
-	/*
-	 * Move data from 4 bytes to 7 bytes.
-	 */
-	movl (%rsi), %ecx
-	movl -4(%rsi, %rdx), %r8d
-	movl %ecx, (%rdi)
-	movl %r8d, -4(%rdi, %rdx)
+
+/* -------- forward copy (no destructive overlap) ------------------ */
+.Lforward_ok:
+	/* large sizes: rep movsq; small: 32-byte unroll */
+	cmpq	$128, %rdx
+	jb	.Lsmall_fwd
+
+	movq	%rdx, %rcx
+	shrq	$3, %rcx
+	rep	movsq
+	movq	%rdx, %rcx
+	andq	$7, %rcx
+	rep	movsb
 	RET
-	.p2align 4
-.Lless_3bytes:
-	subl $1, %edx
-	jb .Lend
-	/*
-	 * Move data from 1 bytes to 3 bytes.
-	 */
-	movzbl (%rsi), %ecx
-	jz .Lstore_1byte
-	movzbq 1(%rsi), %r8
-	movzbq (%rsi, %rdx), %r9
-	movb %r8b, 1(%rdi)
-	movb %r9b, (%rdi, %rdx)
-.Lstore_1byte:
-	movb %cl, (%rdi)
 
-.Lend:
+/* -------- small forward (<128 B) --------------------------------- */
+.Lsmall_fwd:
+	subq	$0x20, %rdx
+.Lunroll32:
+	subq	$0x20, %rdx
+	movq	0(%rsi),  %r8
+	movq	8(%rsi),  %r9
+	movq	16(%rsi), %r10
+	movq	24(%rsi), %r11
+	addq	$0x20,    %rsi
+	movq	%r8,  0(%rdi)
+	movq	%r9,  8(%rdi)
+	movq	%r10, 16(%rdi)
+	movq	%r11, 24(%rdi)
+	addq	$0x20,    %rdi
+	jae	.Lunroll32
+	addq	$0x20,    %rdx		/* undo overshoot */
+
+	/* byte tail (≤31 B) */
+	testq	%rdx, %rdx
+	je	.Ldone
+	movq	%rdx, %rcx
+	rep	movsb
+.Ldone:
 	RET
 SYM_FUNC_END(memcpy_orig)
-

--- a/arch/x86/lib/putuser.S	2025-06-10 13:17:11.000000000 +0200
+++ b/arch/x86/lib/putuser.S	2025-06-10 21:02:08.024144947 +0200
@@ -1,16 +1,14 @@
 /* SPDX-License-Identifier: GPL-2.0 */
 /*
- * __put_user functions.
+ * __put_user_*  —  fault-tolerant user memory stores
  *
- * (C) Copyright 2005 Linus Torvalds
- * (C) Copyright 2005 Andi Kleen
- * (C) Copyright 2008 Glauber Costa
+ *  IN  : %eax[:%edx]  = value
+ *        %ecx/%rcx    = user pointer
+ *  OUT : %ecx         = 0 / –EFAULT
  *
- * These functions have a non-standard call interface
- * to make them more efficient, especially as they
- * return an error value in addition to the "real"
- * return value.
+ *  Only %ebx is clobbered as scratch (matches inline-asm contract).
  */
+
 #include <linux/export.h>
 #include <linux/linkage.h>
 #include <linux/objtool.h>
@@ -18,139 +16,165 @@
 #include <asm/errno.h>
 #include <asm/asm.h>
 #include <asm/smap.h>
+#include <asm/cpufeatures.h>
 
-/*
- * __put_user_X
- *
- * Inputs:	%eax[:%edx] contains the data
- *		%ecx contains the address
- *
- * Outputs:	%ecx is error code (0 or -EFAULT)
- *
- * Clobbers:	%ebx needed for task pointer
- *
- * These functions should not modify any other registers,
- * as they get called from within inline assembly.
- */
-
+/* ------------------------------------------------------------------ */
+/*  Speculation barrier macro – only emitted when CPU advertises it   */
+/* ------------------------------------------------------------------ */
+#define ASM_BARRIER_NOSPEC  ALTERNATIVE "", "lfence", X86_FEATURE_LFENCE_RDTSC
+
+/* ------------------------------------------------------------------ */
+/*  Range check helper                                                */
+/* ------------------------------------------------------------------ */
 .macro check_range size:req
 .if IS_ENABLED(CONFIG_X86_64)
-	mov %rcx, %rbx
-	sar $63, %rbx
-	or %rbx, %rcx
+        mov     %rcx, %rbx       /* rbx = pointer sign bits   */
+        sarq    $63,  %rbx
+        orq     %rbx, %rcx       /* canonicalise; >MAX → -1   */
 .else
-	cmp $TASK_SIZE_MAX-\size+1, %ecx
-	jae .Lbad_put_user
+        cmp     $TASK_SIZE_MAX-\size+1, %ecx
+        jae     .Lbad_put_user
 .endif
 .endm
 
-.text
+/* ------------------------------------------------------------------ */
+/*  1-byte store                                                      */
+/* ------------------------------------------------------------------ */
+        .p2align 5
 SYM_FUNC_START(__put_user_1)
-	ANNOTATE_NOENDBR
-	check_range size=1
-	ASM_STAC
-1:	movb %al,(%_ASM_CX)
-	xor %ecx,%ecx
-	ASM_CLAC
-	RET
+        ANNOTATE_NOENDBR
+        check_range size=1
+        ASM_STAC
+1:      movb    %al, (%_ASM_CX)
+        xorl    %ecx, %ecx
+        ASM_CLAC
+        RET
 SYM_FUNC_END(__put_user_1)
 EXPORT_SYMBOL(__put_user_1)
 
+/* nocheck --------------------------------------------------------- */
+        .p2align 5
 SYM_FUNC_START(__put_user_nocheck_1)
-	ANNOTATE_NOENDBR
-	ASM_STAC
-2:	movb %al,(%_ASM_CX)
-	xor %ecx,%ecx
-	ASM_CLAC
-	RET
+        ANNOTATE_NOENDBR
+        ASM_BARRIER_NOSPEC
+        ASM_STAC
+2:      movb    %al,(%_ASM_CX)
+        xorl    %ecx,%ecx
+        ASM_CLAC
+        RET
 SYM_FUNC_END(__put_user_nocheck_1)
 EXPORT_SYMBOL(__put_user_nocheck_1)
 
+/* ------------------------------------------------------------------ */
+/*  2-byte store                                                      */
+/* ------------------------------------------------------------------ */
+        .p2align 5
 SYM_FUNC_START(__put_user_2)
-	ANNOTATE_NOENDBR
-	check_range size=2
-	ASM_STAC
-3:	movw %ax,(%_ASM_CX)
-	xor %ecx,%ecx
-	ASM_CLAC
-	RET
+        ANNOTATE_NOENDBR
+        check_range size=2
+        ASM_STAC
+3:      movw    %ax, (%_ASM_CX)
+        xorl    %ecx, %ecx
+        ASM_CLAC
+        RET
 SYM_FUNC_END(__put_user_2)
 EXPORT_SYMBOL(__put_user_2)
 
+/* nocheck --------------------------------------------------------- */
+        .p2align 5
 SYM_FUNC_START(__put_user_nocheck_2)
-	ANNOTATE_NOENDBR
-	ASM_STAC
-4:	movw %ax,(%_ASM_CX)
-	xor %ecx,%ecx
-	ASM_CLAC
-	RET
+        ANNOTATE_NOENDBR
+        ASM_STAC
+        ASM_BARRIER_NOSPEC
+4:      movw    %ax, (%_ASM_CX)
+        xorl    %ecx, %ecx
+        ASM_CLAC
+        RET
 SYM_FUNC_END(__put_user_nocheck_2)
 EXPORT_SYMBOL(__put_user_nocheck_2)
 
+/* ------------------------------------------------------------------ */
+/*  4-byte store                                                      */
+/* ------------------------------------------------------------------ */
+        .p2align 5
 SYM_FUNC_START(__put_user_4)
-	ANNOTATE_NOENDBR
-	check_range size=4
-	ASM_STAC
-5:	movl %eax,(%_ASM_CX)
-	xor %ecx,%ecx
-	ASM_CLAC
-	RET
+        ANNOTATE_NOENDBR
+        check_range size=4
+        ASM_STAC
+5:      movl    %eax, (%_ASM_CX)
+        xorl    %ecx, %ecx
+        ASM_CLAC
+        RET
 SYM_FUNC_END(__put_user_4)
 EXPORT_SYMBOL(__put_user_4)
 
+/* nocheck --------------------------------------------------------- */
+        .p2align 5
 SYM_FUNC_START(__put_user_nocheck_4)
-	ANNOTATE_NOENDBR
-	ASM_STAC
-6:	movl %eax,(%_ASM_CX)
-	xor %ecx,%ecx
-	ASM_CLAC
-	RET
+        ANNOTATE_NOENDBR
+        ASM_STAC
+        ASM_BARRIER_NOSPEC
+6:      movl    %eax, (%_ASM_CX)
+        xorl    %ecx, %ecx
+        ASM_CLAC
+        RET
 SYM_FUNC_END(__put_user_nocheck_4)
 EXPORT_SYMBOL(__put_user_nocheck_4)
 
+/* ------------------------------------------------------------------ */
+/*  8-byte store (64-bit only)                                        */
+/* ------------------------------------------------------------------ */
+        .p2align 5
 SYM_FUNC_START(__put_user_8)
-	ANNOTATE_NOENDBR
-	check_range size=8
-	ASM_STAC
-7:	mov %_ASM_AX,(%_ASM_CX)
+        ANNOTATE_NOENDBR
+        check_range size=8
+        ASM_STAC
+7:      movq    %_ASM_AX, (%_ASM_CX)
 #ifdef CONFIG_X86_32
-8:	movl %edx,4(%_ASM_CX)
+8:      movl    %edx, 4(%_ASM_CX)
 #endif
-	xor %ecx,%ecx
-	ASM_CLAC
-	RET
+        xorl    %ecx, %ecx
+        ASM_CLAC
+        RET
 SYM_FUNC_END(__put_user_8)
 EXPORT_SYMBOL(__put_user_8)
 
+/* nocheck --------------------------------------------------------- */
+        .p2align 5
 SYM_FUNC_START(__put_user_nocheck_8)
-	ANNOTATE_NOENDBR
-	ASM_STAC
-9:	mov %_ASM_AX,(%_ASM_CX)
+        ANNOTATE_NOENDBR
+        ASM_STAC
+        ASM_BARRIER_NOSPEC
+9:      movq    %_ASM_AX, (%_ASM_CX)
 #ifdef CONFIG_X86_32
-10:	movl %edx,4(%_ASM_CX)
+10:     movl    %edx, 4(%_ASM_CX)
 #endif
-	xor %ecx,%ecx
-	ASM_CLAC
-	RET
+        xorl    %ecx, %ecx
+        ASM_CLAC
+        RET
 SYM_FUNC_END(__put_user_nocheck_8)
 EXPORT_SYMBOL(__put_user_nocheck_8)
 
+/* ------------------------------------------------------------------ */
+/*  Fault handler                                                     */
+/* ------------------------------------------------------------------ */
 SYM_CODE_START_LOCAL(__put_user_handle_exception)
-	ASM_CLAC
+        ASM_CLAC
 .Lbad_put_user:
-	movl $-EFAULT,%ecx
-	RET
+        movl    $-EFAULT, %ecx
+        RET
 SYM_CODE_END(__put_user_handle_exception)
 
-	_ASM_EXTABLE_UA(1b, __put_user_handle_exception)
-	_ASM_EXTABLE_UA(2b, __put_user_handle_exception)
-	_ASM_EXTABLE_UA(3b, __put_user_handle_exception)
-	_ASM_EXTABLE_UA(4b, __put_user_handle_exception)
-	_ASM_EXTABLE_UA(5b, __put_user_handle_exception)
-	_ASM_EXTABLE_UA(6b, __put_user_handle_exception)
-	_ASM_EXTABLE_UA(7b, __put_user_handle_exception)
-	_ASM_EXTABLE_UA(9b, __put_user_handle_exception)
+/* extable entries – keep byte-exact addresses -------------------- */
+        _ASM_EXTABLE_UA(1b,  __put_user_handle_exception)
+        _ASM_EXTABLE_UA(2b,  __put_user_handle_exception)
+        _ASM_EXTABLE_UA(3b,  __put_user_handle_exception)
+        _ASM_EXTABLE_UA(4b,  __put_user_handle_exception)
+        _ASM_EXTABLE_UA(5b,  __put_user_handle_exception)
+        _ASM_EXTABLE_UA(6b,  __put_user_handle_exception)
+        _ASM_EXTABLE_UA(7b,  __put_user_handle_exception)
+        _ASM_EXTABLE_UA(9b,  __put_user_handle_exception)
 #ifdef CONFIG_X86_32
-	_ASM_EXTABLE_UA(8b, __put_user_handle_exception)
-	_ASM_EXTABLE_UA(10b, __put_user_handle_exception)
+        _ASM_EXTABLE_UA(8b,  __put_user_handle_exception)
+        _ASM_EXTABLE_UA(10b, __put_user_handle_exception)
 #endif

--- a/arch/x86/lib/usercopy_64.c	2025-06-04 14:46:27.000000000 +0200
+++ b/arch/x86/lib/usercopy_64.c	2025-06-10 21:04:25.185922440 +0200
@@ -1,144 +1,154 @@
-// SPDX-License-Identifier: GPL-2.0-only
-/* 
- * User address space access functions.
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * arch/x86/lib/usercopy_64.c
  *
- * Copyright 1997 Andi Kleen <ak@muc.de>
- * Copyright 1997 Linus Torvalds
- * Copyright 2002 Andi Kleen <ak@suse.de>
+ * Fast user↔kernel copy helpers with persistent-memory support.
+ * Modernised for Intel® Raptor-Lake-R while keeping original ABI.
  */
+
 #include <linux/export.h>
 #include <linux/uaccess.h>
 #include <linux/highmem.h>
 #include <linux/libnvdimm.h>
-
-/*
- * Zero Userspace
- */
+#include <asm/cacheflush.h>
+#include <asm/smap.h>
+#include <asm/cpufeatures.h>
+#include <asm/alternative.h>
 
 #ifdef CONFIG_ARCH_HAS_UACCESS_FLUSHCACHE
-/**
- * clean_cache_range - write back a cache range with CLWB
- * @addr:	virtual start address
- * @size:	number of bytes to write back
- *
- * Write back a cache range using the CLWB (cache line write back)
- * instruction. Note that @size is internally rounded up to be cache
- * line size aligned.
- */
-static void clean_cache_range(void *addr, size_t size)
+/* --------------------------------------------------------------------
+ *  Cache-flush helpers
+ * ------------------------------------------------------------------ */
+static __always_inline void clwb_alternative(const void *p)
+{
+	asm volatile(ALTERNATIVE_3("clflush %0",
+							   "clflushopt %0", X86_FEATURE_CLFLUSHOPT,
+							"clwb %0",       X86_FEATURE_CLWB,
+							"clflush %0",    X86_FEATURE_CLFLUSH)
+	: "+m"(*(volatile char *)p));
+}
+
+static void cache_wb_range(void *addr, size_t size)
 {
-	u16 x86_clflush_size = boot_cpu_data.x86_clflush_size;
-	unsigned long clflush_mask = x86_clflush_size - 1;
-	void *vend = addr + size;
-	void *p;
-
-	for (p = (void *)((unsigned long)addr & ~clflush_mask);
-	     p < vend; p += x86_clflush_size)
-		clwb(p);
+	if (!size)
+		return;
+
+	const unsigned int cls  = boot_cpu_data.x86_clflush_size;
+	const unsigned long msk = cls - 1;
+	unsigned long p   = (unsigned long)addr & ~msk;
+	unsigned long end = (unsigned long)addr + size;
+
+	for (; p < end; p += cls)
+		clwb_alternative((void *)p);
+
+	asm volatile("sfence" ::: "memory");	/* durability barrier */
 }
 
+/* -------------------------------------------------------------------- */
 void arch_wb_cache_pmem(void *addr, size_t size)
 {
-	clean_cache_range(addr, size);
+	cache_wb_range(addr, size);
 }
 EXPORT_SYMBOL_GPL(arch_wb_cache_pmem);
 
-long __copy_user_flushcache(void *dst, const void __user *src, unsigned size)
+/* --------------------------------------------------------------------
+ *  copy_user with flushcache
+ * ------------------------------------------------------------------ */
+long __copy_user_flushcache(void *dst, const void __user *src, unsigned int sz)
 {
-	unsigned long flushed, dest = (unsigned long) dst;
-	long rc;
+	if (!sz)
+		return 0;
 
 	stac();
-	rc = __copy_user_nocache(dst, src, size);
+	long rc = __copy_user_nocache(dst, src, sz);
 	clac();
 
-	/*
-	 * __copy_user_nocache() uses non-temporal stores for the bulk
-	 * of the transfer, but we need to manually flush if the
-	 * transfer is unaligned. A cached memory copy is used when
-	 * destination or size is not naturally aligned. That is:
-	 *   - Require 8-byte alignment when size is 8 bytes or larger.
-	 *   - Require 4-byte alignment when size is 4 bytes.
-	 */
-	if (size < 8) {
-		if (!IS_ALIGNED(dest, 4) || size != 4)
-			clean_cache_range(dst, size);
+	if (rc || !sz)
+		return rc;
+
+	if (sz < 8) {
+		if (!IS_ALIGNED((unsigned long)dst, 4) || sz != 4)
+			cache_wb_range(dst, sz);
 	} else {
-		if (!IS_ALIGNED(dest, 8)) {
-			dest = ALIGN(dest, boot_cpu_data.x86_clflush_size);
-			clean_cache_range(dst, 1);
-		}
-
-		flushed = dest - (unsigned long) dst;
-		if (size > flushed && !IS_ALIGNED(size - flushed, 8))
-			clean_cache_range(dst + size - 1, 1);
-	}
+		unsigned long d = (unsigned long)dst;
 
+		if (!IS_ALIGNED(d, 8))
+			cache_wb_range((void *)d, 1);
+
+		if (!IS_ALIGNED(sz, 8))
+			cache_wb_range((void *)(d + sz - 1), 1);
+	}
 	return rc;
 }
+EXPORT_SYMBOL_GPL(__copy_user_flushcache);
 
-void __memcpy_flushcache(void *_dst, const void *_src, size_t size)
+/* --------------------------------------------------------------------
+ *  memcpy into pmem with NT stores + explicit flush
+ * ------------------------------------------------------------------ */
+void __memcpy_flushcache(void *dst, const void *src, size_t size)
 {
-	unsigned long dest = (unsigned long) _dst;
-	unsigned long source = (unsigned long) _src;
+	unsigned long d = (unsigned long)dst;
+	unsigned long s = (unsigned long)src;
+
+	if (!size)
+		return;
 
-	/* cache copy and flush to align dest */
-	if (!IS_ALIGNED(dest, 8)) {
-		size_t len = min_t(size_t, size, ALIGN(dest, 8) - dest);
-
-		memcpy((void *) dest, (void *) source, len);
-		clean_cache_range((void *) dest, len);
-		dest += len;
-		source += len;
-		size -= len;
+	/* align destination to 8 */
+	if (!IS_ALIGNED(d, 8)) {
+		size_t head = min_t(size_t, size, ALIGN(d, 8) - d);
+
+		memcpy((void *)d, (const void *)s, head);
+		cache_wb_range((void *)d, head);
+		d += head;
+		s += head;
+		size -= head;
 		if (!size)
 			return;
 	}
 
-	/* 4x8 movnti loop */
-	while (size >= 32) {
-		asm("movq    (%0), %%r8\n"
-		    "movq   8(%0), %%r9\n"
-		    "movq  16(%0), %%r10\n"
-		    "movq  24(%0), %%r11\n"
-		    "movnti  %%r8,   (%1)\n"
-		    "movnti  %%r9,  8(%1)\n"
-		    "movnti %%r10, 16(%1)\n"
-		    "movnti %%r11, 24(%1)\n"
-		    :: "r" (source), "r" (dest)
-		    : "memory", "r8", "r9", "r10", "r11");
-		dest += 32;
-		source += 32;
-		size -= 32;
+	/* large copy via REP MOVSQ */
+	if (size >= 128) {
+		size_t qwords = size >> 3;
+		asm volatile("rep movsq"
+		: "+D"(d), "+S"(s), "+c"(qwords)
+		: : "memory");
+		size &= 7;
 	}
 
-	/* 1x8 movnti loop */
+	/* NTI loops */
+	while (size >= 32) {
+		asm volatile(
+			"movq    (%[s]), %%r8\n\t"
+			"movq   8(%[s]), %%r9\n\t"
+			"movq  16(%[s]), %%r10\n\t"
+			"movq  24(%[s]), %%r11\n\t"
+			"movnti %%r8,   (%[d])\n\t"
+			"movnti %%r9,  8(%[d])\n\t"
+			"movnti %%r10, 16(%[d])\n\t"
+			"movnti %%r11, 24(%[d])"
+			: [d] "+r"(d), [s] "+r"(s)
+			: : "memory", "r8", "r9", "r10", "r11");
+		d += 32; s += 32; size -= 32;
+	}
 	while (size >= 8) {
-		asm("movq    (%0), %%r8\n"
-		    "movnti  %%r8,   (%1)\n"
-		    :: "r" (source), "r" (dest)
-		    : "memory", "r8");
-		dest += 8;
-		source += 8;
-		size -= 8;
+		asm volatile("movq (%[s]), %%r8 ; movnti %%r8,(%[d])"
+		: [d] "+r"(d), [s] "+r"(s)
+		: : "memory", "r8");
+		d += 8; s += 8; size -= 8;
 	}
-
-	/* 1x4 movnti loop */
 	while (size >= 4) {
-		asm("movl    (%0), %%r8d\n"
-		    "movnti  %%r8d,   (%1)\n"
-		    :: "r" (source), "r" (dest)
-		    : "memory", "r8");
-		dest += 4;
-		source += 4;
-		size -= 4;
+		asm volatile("movl (%[s]), %%r8d ; movnti %%r8d,(%[d])"
+		: [d] "+r"(d), [s] "+r"(s)
+		: : "memory", "r8");
+		d += 4; s += 4; size -= 4;
 	}
 
-	/* cache copy for remaining bytes */
 	if (size) {
-		memcpy((void *) dest, (void *) source, size);
-		clean_cache_range((void *) dest, size);
+		memcpy((void *)d, (const void *)s, size);
 	}
+
+	cache_wb_range(dst, (unsigned long)d + size - (unsigned long)dst);
 }
 EXPORT_SYMBOL_GPL(__memcpy_flushcache);
-#endif
+
+#endif /* CONFIG_ARCH_HAS_UACCESS_FLUSHCACHE */
