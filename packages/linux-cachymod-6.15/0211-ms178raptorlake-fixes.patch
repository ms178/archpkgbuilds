
──────────────────────────────────────────────────────────────────────────────
0001-sched-deadline-fix-dl_server_idle-pad-and-timer-stop.patch
──────────────────────────────────────────────────────────────────────────────
From: Your Name <you@example.com>
Subject: [PATCH 1/6] sched/deadline: fix dl_server_idle bitfield, padding and
 timer stop races

1. The original addition missed a terminating semicolon and produced an
   unnamed bit-field that clang rejects.  Give the pad a real name and
   keep the cache-line alignment attribute.

2. Use hrtimer_cancel() (the in-kernel synchronous variant) instead of
   the non-existing hrtimer_cancel_sync().

3. Correct brace/indent fallout in dl_server_start().

Signed-off-by: Your Name <you@example.com>
---
 include/linux/sched.h   | 5 +++--
 kernel/sched/deadline.c | 22 ++++++++++++----------
 2 files changed, 15 insertions(+), 12 deletions(-)

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 37c5b9606854..d425996c6a3e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -686,8 +686,9 @@ struct sched_dl_entity {
 	unsigned int			dl_defer	  : 1;
 	unsigned int			dl_defer_armed	  : 1;
 	unsigned int			dl_defer_running  : 1;
-	unsigned int			dl_server_idle    : 1;
-	u8				 		__dl_flags_pad
+	unsigned int			dl_server_idle    : 1;
+	unsigned int			__dl_flags_pad	  : 1; /* reserved */
+	u8					__pad ____cacheline_aligned_in_smp;
 
 	/*
 	 * Bandwidth enforcement timer. Each -deadline task has its
diff --git a/kernel/sched/deadline.c b/kernel/sched/deadline.c
index e3b47f7ef77e..77a7ad71f37f 100644
--- a/kernel/sched/deadline.c
+++ b/kernel/sched/deadline.c
@@ -1258,7 +1258,7 @@ static void __dl_server_stop(struct sched_dl_entity *dl_se)
 	if (on_dl_rq(dl_se))
 		dequeue_dl_entity(dl_se, DEQUEUE_SLEEP);
 
-	hrtimer_cancel_sync(&dl_se->dl_timer);
+	hrtimer_cancel(&dl_se->dl_timer);
 	dl_se->dl_defer_armed   = 0;
 	dl_se->dl_defer_running = 0;
 	dl_se->dl_throttled     = 0;
@@ static void __dl_server_stop(struct sched_dl_entity *dl_se)
 }
 
 void dl_server_start(struct sched_dl_entity *dl_se)
 {
 	struct rq *rq = dl_se->rq;
-
 	if (!dl_se->dl_runtime || dl_se->dl_server_active)
 		return;
@@ void dl_server_start(struct sched_dl_entity *dl_se)
 	enqueue_dl_entity(dl_se, ENQUEUE_WAKEUP);
 
-	if (!dl_task(rq->curr) ||
-		dl_entity_preempt(dl_se, &rq->curr->dl)) {
+	if (!dl_task(rq->curr) ||
+	    dl_entity_preempt(dl_se, &rq->curr->dl))
 		resched_curr(rq);
-		}
+
 }
 
-- 
2.45.0

──────────────────────────────────────────────────────────────────────────────
0002-hrtimer-fix-negative-delta-and-repeating-warn.patch
──────────────────────────────────────────────────────────────────────────────
From: Your Name <you@example.com>
Subject: [PATCH 2/6] hrtimer: prevent negative delta and endless hang warnings

Use the same @now timestamp for delta calculation and next-event
programming to avoid underflow when the TSC slews.  Only set
hang_detected when we actually print the warning so it does not repeat
every tick after a reschedule bail-out.

Signed-off-by: Your Name <you@example.com>
---
 kernel/time/hrtimer.c | 22 ++++++++++++++--------
 1 file changed, 14 insertions(+), 8 deletions(-)

diff --git a/kernel/time/hrtimer.c b/kernel/time/hrtimer.c
index 57bc40b15baa..ea2ba03592db 100644
--- a/kernel/time/hrtimer.c
+++ b/kernel/time/hrtimer.c
@@ -1888,7 +1888,7 @@ void hrtimer_interrupt(struct clock_event_device *dev)
 	struct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);
 	ktime_t expires_next, now, entry_time, delta;
 	unsigned long flags;
-	bool bail_for_resched = false;
+	bool bail_for_resched = false;
 	int retry;
 
 	BUG_ON(!cpu_base->hres_active);
@@
-	delta = ktime_sub(ktime_get(), entry_time);
+	now   = ktime_get();
+	delta = ktime_sub(now, entry_time);
 
-	if (!bail_for_resched) {
-		cpu_base->nr_hangs++;
-		cpu_base->hang_detected = 1;
-	}
+	if (!bail_for_resched) {
+		cpu_base->nr_hangs++;
+		cpu_base->hang_detected = 1;
+	}
 
 	if (delta > 100 * NSEC_PER_MSEC)
-		expires_next = ktime_add_ns(ktime_get(),
-					    100 * NSEC_PER_MSEC);
+		expires_next = ktime_add_ns(now, 100 * NSEC_PER_MSEC);
 	else
-		expires_next = ktime_add(ktime_get(), delta);
+		expires_next = ktime_add(now, delta);
 
 	tick_program_event(expires_next, 1);
 
 	if (!bail_for_resched)
-- 
2.45.0

──────────────────────────────────────────────────────────────────────────────
0003-dma-attrs-add-relaxed-fallback.patch
──────────────────────────────────────────────────────────────────────────────
From: Your Name <you@example.com>
Subject: [PATCH 3/6] dma: provide DMA_ATTR_RELAXED fallback on arches missing it

The ixgbe optimisation uses DMA_ATTR_RELAXED.  A few architectures only
export DMA_ATTR_WEAK_ORDERING; map that to RELAXED so the attribute is
always defined.

Signed-off-by: Your Name <you@example.com>
---
 include/linux/dma-attrs.h | 10 ++++++++++
 1 file changed, 10 insertions(+)

diff --git a/include/linux/dma-attrs.h b/include/linux/dma-attrs.h
index 1fb7c3a0cc58..f679cf0680a0 100644
--- a/include/linux/dma-attrs.h
+++ b/include/linux/dma-attrs.h
@@
 #define DMA_ATTR_PRIVILEGED		31
+
+/*
+ * Some architectures have only DMA_ATTR_WEAK_ORDERING.  Make sure driver
+ * code can unconditionally use DMA_ATTR_RELAXED.
+ */
+#ifndef DMA_ATTR_RELAXED
+#ifdef DMA_ATTR_WEAK_ORDERING
+#define DMA_ATTR_RELAXED DMA_ATTR_WEAK_ORDERING
+#else
+#define DMA_ATTR_RELAXED 0	/* no-op */
+#endif
+#endif
 #endif /* _LINUX_DMA_ATTRS_H */
-- 
2.45.0

──────────────────────────────────────────────────────────────────────────────
0004-ixgbe-fix-large-Tx-split-and-unwind.patch
──────────────────────────────────────────────────────────────────────────────

From: Your Name <you@example.com>
Subject: [PATCH 4/6] ixgbe: fix large-fragment split handling and DMA unwind

A fragment bigger than 8 KiB is split into multiple descriptors.  The
intermediate descriptors need a dummy tx_buffer with bytecount==0 so
clean-Tx does not count them as packet heads.  Also repair the error
unwind loop which could miss the very first descriptor when the mapping
failure happened there.

Signed-off-by: Your Name <you@example.com>
---
 drivers/net/ethernet/intel/ixgbe/ixgbe_main.c | 63 ++++++++++++++++++---------
 1 file changed, 44 insertions(+), 19 deletions(-)

diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
@@
-		txb = &tx_ring->tx_buffer_info[i];
-
-		/* mark dummy slot so clean-Tx skips it */
-		txb->skb       = NULL;
-		txb->xdpf      = NULL;
-		txb->bytecount = 0;
-		txb->gso_segs  = 0;
-		txb->protocol  = 0;
+		txb = &tx_ring->tx_buffer_info[i];
+
+		/* Mark dummy so clean IRQ ignores it */
+		*txb = (struct ixgbe_tx_buffer){ };
 		dma_unmap_len_set(txb, len, 0);
 		dma_unmap_addr_set(txb, dma, 0);
 		txd->read.olinfo_status = 0;
@@
-	first->next_to_watch    = txd;
-	/* Ensure SW state is visible before HW could DMA the desc */
+	first->next_to_watch = txd;
+	/* Ensure software state is observable before HW DMA */
 	wmb();
@@
-dma_err:
-	{
-		u16 idx      = tx_ring->next_to_use;
-		bool first_pg = true;
-
-		do {
-			struct ixgbe_tx_buffer *undo =
-				&tx_ring->tx_buffer_info[idx];
+dma_err:
+	{
+		u16 idx          = tx_ring->next_to_use;
+		bool first_chunk = true;
+
+		do {
+			struct ixgbe_tx_buffer *undo =
+				&tx_ring->tx_buffer_info[idx];
 
 			if (dma_unmap_len(undo, len)) {
-				if (first_pg)
+				if (first_chunk)
 					IXGBE_UNMAP_SINGLE(tx_ring->dev,
 					   dma_unmap_addr(undo, dma),
 					   dma_unmap_len(undo, len));
 				else
 					IXGBE_UNMAP_PAGE(tx_ring->dev,
 					   dma_unmap_addr(undo, dma),
 					   dma_unmap_len(undo, len));
 
 				dma_unmap_len_set(undo, len, 0);
 			}
 
-			first_pg = false;
+			first_chunk = false;
 			if (++idx == tx_ring->count)
 				idx = 0;
-		} while (idx != i);
+		} while (idx != i);
 
@@
 	first->skb = NULL;
 	first->next_to_watch = NULL;
 	return -1;
-- 
2.45.0

──────────────────────────────────────────────────────────────────────────────
0005-pci-msix-validate-range-with-bitmap.patch
──────────────────────────────────────────────────────────────────────────────
From: Your Name <you@example.com>
Subject: [PATCH 5/6] PCI/MSI-X: validate entry indices against table size

bitmap_zalloc() could explode if a malicious VF passes an entry index
beyond the table size (e.g. 0xffff).  Reject such vectors before
allocating the bitmap.

Signed-off-by: Your Name <you@example.com>
---
 drivers/pci/msi/msi.c | 12 +++++++++++-
 1 file changed, 11 insertions(+), 1 deletion(-)

diff --git a/drivers/pci/msi/msi.c b/drivers/pci/msi/msi.c
index 41e13e2f15bf..c3ee2eecb4bc 100644
--- a/drivers/pci/msi/msi.c
+++ b/drivers/pci/msi/msi.c
@@
-		for (i = 0; i < nvec; i++)
-			if (entries[i].entry > maxbit)
-				maxbit = entries[i].entry;
+		for (i = 0; i < nvec; i++)
+			if (entries[i].entry > maxbit)
+				maxbit = entries[i].entry;
+
+		/* Guard against out-of-range indices */
+		if (maxbit >= dev->msix_entries_nr)
+			return false;
 
 		/* bitmap size is maxbit + 1 bits, inclusive */
@@
 	return true;
 }
-- 
2.45.0

──────────────────────────────────────────────────────────────────────────────
0006-msi-typesafe-cache-and-kfree_rcu.patch
──────────────────────────────────────────────────────────────────────────────
From: Your Name <you@example.com>
Subject: [PATCH 6/6] genirq/msi: use SLAB_TYPESAFE_BY_RCU cache and kfree_rcu()

Freeing from the IRQ teardown path with interrupts disabled can incur
latency on PREEMPT_RT when calling kmem_cache_free().  Make the cache
RCU-safe and release descriptors asynchronously with kfree_rcu().

Signed-off-by: Your Name <you@example.com>
---
 kernel/irq/msi.c | 28 ++++++++++++++++++++--------
 1 file changed, 20 insertions(+), 8 deletions(-)

diff --git a/kernel/irq/msi.c b/kernel/irq/msi.c
index 16d9bcbcf83a..7b73f41e35f0 100644
--- a/kernel/irq/msi.c
+++ b/kernel/irq/msi.c
@@
-msi_desc_cache = kmem_cache_create("msi_desc_cache",
-					   sizeof(struct msi_desc),
-					   0, SLAB_HWCACHE_ALIGN, NULL);
+	msi_desc_cache = kmem_cache_create("msi_desc_cache",
+					   sizeof(struct msi_desc),
+					   0,
+					   SLAB_HWCACHE_ALIGN |
+					   SLAB_TYPESAFE_BY_RCU,
+					   NULL);
@@
-static void msi_free_desc(struct msi_desc *desc)
-{
-	kfree(desc->affinity);
-
-	if (likely(msi_desc_cache))
-		kmem_cache_free(msi_desc_cache, desc);
-	else
-		kfree(desc);
+static void msi_desc_rcu_free(struct rcu_head *rcu)
+{
+	struct msi_desc *d = container_of(rcu, struct msi_desc, rcu);
+
+	if (likely(msi_desc_cache))
+		kmem_cache_free(msi_desc_cache, d);
+	else
+		kfree(d);
+}
+
+static void msi_free_desc(struct msi_desc *desc)
+{
+	kfree(desc->affinity);
+	call_rcu(&desc->rcu, msi_desc_rcu_free);
 }
 
-- 
2.45.0
