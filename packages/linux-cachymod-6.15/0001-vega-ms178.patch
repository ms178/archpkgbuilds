--- a/drivers/gpu/drm/drm_buddy.c
+++ b/drivers/gpu/drm/drm_buddy.c
@@ -30,6 +30,8 @@ static struct drm_buddy_block *drm_block
 	block->header |= order;
 	block->parent = parent;
 
+	RB_CLEAR_NODE(&block->rb);
+
 	BUG_ON(block->header & DRM_BUDDY_HEADER_UNUSED);
 	return block;
 }
@@ -40,58 +42,139 @@ static void drm_block_free(struct drm_bu
 	kmem_cache_free(slab_blocks, block);
 }
 
-static void list_insert_sorted(struct drm_buddy *mm,
-			       struct drm_buddy_block *block)
+static inline struct rb_root *
+__get_root(struct drm_buddy *mm,
+	   unsigned int order,
+	   enum free_tree tree)
+{
+	if (tree == CLEAR_TREE)
+		return &mm->clear_tree[order];
+	else
+		return &mm->dirty_tree[order];
+}
+
+static inline enum free_tree
+__get_tree_for_block(struct drm_buddy_block *block)
+{
+	return drm_buddy_block_is_clear(block) ? CLEAR_TREE : DIRTY_TREE;
+}
+
+static inline enum free_tree
+__get_tree_for_flags(unsigned long flags)
+{
+	return (flags & DRM_BUDDY_CLEAR_ALLOCATION) ? CLEAR_TREE : DIRTY_TREE;
+}
+
+static inline struct drm_buddy_block *
+rbtree_get_entry(struct rb_node *node)
+{
+	return node ? rb_entry(node, struct drm_buddy_block, rb) : NULL;
+}
+
+static inline struct drm_buddy_block *
+rbtree_prev_entry(struct rb_node *node)
+{
+	return rbtree_get_entry(rb_prev(node));
+}
+
+static inline struct drm_buddy_block *
+rbtree_first_entry(struct rb_root *root)
+{
+	return rbtree_get_entry(rb_first(root));
+}
+
+static inline struct drm_buddy_block *
+rbtree_last_entry(struct rb_root *root)
+{
+	return rbtree_get_entry(rb_last(root));
+}
+
+static inline bool rbtree_is_empty(struct rb_root *root)
+{
+	return RB_EMPTY_ROOT(root);
+}
+
+static void rbtree_insert(struct drm_buddy *mm,
+			  struct drm_buddy_block *block,
+			  enum free_tree tree)
 {
+	struct rb_node **link, *parent = NULL;
 	struct drm_buddy_block *node;
-	struct list_head *head;
+	struct rb_root *root;
+	unsigned int order;
 
-	head = &mm->free_list[drm_buddy_block_order(block)];
-	if (list_empty(head)) {
-		list_add(&block->link, head);
-		return;
-	}
+	order = drm_buddy_block_order(block);
+
+	root = __get_root(mm, order, tree);
+	link = &root->rb_node;
+
+	while (*link) {
+		parent = *link;
+		node = rbtree_get_entry(parent);
 
-	list_for_each_entry(node, head, link)
 		if (drm_buddy_block_offset(block) < drm_buddy_block_offset(node))
-			break;
+			link = &parent->rb_left;
+		else
+			link = &parent->rb_right;
+	}
+
+	block->tree = tree;
 
-	__list_add(&block->link, node->link.prev, &node->link);
+	rb_link_node(&block->rb, parent, link);
+	rb_insert_color(&block->rb, root);
 }
 
-static void clear_reset(struct drm_buddy_block *block)
+static void rbtree_remove(struct drm_buddy *mm,
+			  struct drm_buddy_block *block)
+{
+	unsigned int order = drm_buddy_block_order(block);
+	struct rb_root *root;
+
+	root = __get_root(mm, order, block->tree);
+	rb_erase(&block->rb, root);
+
+	RB_CLEAR_NODE(&block->rb);
+}
+
+static inline void clear_reset(struct drm_buddy_block *block)
 {
 	block->header &= ~DRM_BUDDY_HEADER_CLEAR;
 }
 
-static void mark_cleared(struct drm_buddy_block *block)
+static inline void mark_cleared(struct drm_buddy_block *block)
 {
 	block->header |= DRM_BUDDY_HEADER_CLEAR;
 }
 
-static void mark_allocated(struct drm_buddy_block *block)
+static inline void mark_allocated(struct drm_buddy *mm,
+				  struct drm_buddy_block *block)
 {
 	block->header &= ~DRM_BUDDY_HEADER_STATE;
 	block->header |= DRM_BUDDY_ALLOCATED;
 
-	list_del(&block->link);
+	rbtree_remove(mm, block);
 }
 
-static void mark_free(struct drm_buddy *mm,
-		      struct drm_buddy_block *block)
+static inline void mark_free(struct drm_buddy *mm,
+			     struct drm_buddy_block *block)
 {
+	enum free_tree tree;
+
 	block->header &= ~DRM_BUDDY_HEADER_STATE;
 	block->header |= DRM_BUDDY_FREE;
 
-	list_insert_sorted(mm, block);
+	tree = __get_tree_for_block(block);
+
+	rbtree_insert(mm, block, tree);
 }
 
-static void mark_split(struct drm_buddy_block *block)
+static inline void mark_split(struct drm_buddy *mm,
+			      struct drm_buddy_block *block)
 {
 	block->header &= ~DRM_BUDDY_HEADER_STATE;
 	block->header |= DRM_BUDDY_SPLIT;
 
-	list_del(&block->link);
+	rbtree_remove(mm, block);
 }
 
 static inline bool overlaps(u64 s1, u64 e1, u64 s2, u64 e2)
@@ -147,7 +230,7 @@ static unsigned int __drm_buddy_free(str
 				mark_cleared(parent);
 		}
 
-		list_del(&buddy->link);
+		rbtree_remove(mm, buddy);
 		if (force_merge && drm_buddy_block_is_clear(buddy))
 			mm->clear_avail -= drm_buddy_block_size(mm, buddy);
 
@@ -177,44 +260,52 @@ static int __force_merge(struct drm_budd
 	if (min_order > mm->max_order)
 		return -EINVAL;
 
-	for (i = min_order - 1; i >= 0; i--) {
-		struct drm_buddy_block *block, *prev;
+	for_each_free_tree() {
+		for (i = min_order - 1; i >= 0; i--) {
+			struct rb_root *root = __get_root(mm, i, tree);
+			struct drm_buddy_block *block, *prev_block;
+
+			for_each_rb_entry_reverse_safe(block, prev_block, root, rb) {
+				struct drm_buddy_block *buddy;
+				u64 block_start, block_end;
 
-		list_for_each_entry_safe_reverse(block, prev, &mm->free_list[i], link) {
-			struct drm_buddy_block *buddy;
-			u64 block_start, block_end;
+				if (RB_EMPTY_NODE(&block->rb))
+					break;
 
-			if (!block->parent)
-				continue;
+				if (!block->parent)
+					continue;
 
-			block_start = drm_buddy_block_offset(block);
-			block_end = block_start + drm_buddy_block_size(mm, block) - 1;
+				block_start = drm_buddy_block_offset(block);
+				block_end = block_start + drm_buddy_block_size(mm, block) - 1;
 
-			if (!contains(start, end, block_start, block_end))
-				continue;
+				if (!contains(start, end, block_start, block_end))
+					continue;
 
-			buddy = __get_buddy(block);
-			if (!drm_buddy_block_is_free(buddy))
-				continue;
+				buddy = __get_buddy(block);
+				if (!drm_buddy_block_is_free(buddy))
+					continue;
 
-			WARN_ON(drm_buddy_block_is_clear(block) ==
-				drm_buddy_block_is_clear(buddy));
+				WARN_ON(drm_buddy_block_is_clear(block) ==
+					drm_buddy_block_is_clear(buddy));
 
-			/*
-			 * If the prev block is same as buddy, don't access the
-			 * block in the next iteration as we would free the
-			 * buddy block as part of the free function.
-			 */
-			if (prev == buddy)
-				prev = list_prev_entry(prev, link);
+				/*
+				 * If the prev block is same as buddy, don't access the
+				 * block in the next iteration as we would free the
+				 * buddy block as part of the free function.
+				 */
+				if (prev_block && prev_block == buddy) {
+					if (prev_block != rbtree_first_entry(root))
+						prev_block = rbtree_prev_entry(&prev_block->rb);
+				}
 
-			list_del(&block->link);
-			if (drm_buddy_block_is_clear(block))
-				mm->clear_avail -= drm_buddy_block_size(mm, block);
+				rbtree_remove(mm, block);
+				if (drm_buddy_block_is_clear(block))
+					mm->clear_avail -= drm_buddy_block_size(mm, block);
 
-			order = __drm_buddy_free(mm, block, true);
-			if (order >= min_order)
-				return 0;
+				order = __drm_buddy_free(mm, block, true);
+				if (order >= min_order)
+					return 0;
+			}
 		}
 	}
 
@@ -257,14 +348,22 @@ int drm_buddy_init(struct drm_buddy *mm,
 
 	BUG_ON(mm->max_order > DRM_BUDDY_MAX_ORDER);
 
-	mm->free_list = kmalloc_array(mm->max_order + 1,
-				      sizeof(struct list_head),
-				      GFP_KERNEL);
-	if (!mm->free_list)
+	mm->clear_tree = kmalloc_array(mm->max_order + 1,
+				       sizeof(struct rb_root),
+				       GFP_KERNEL);
+	if (!mm->clear_tree)
 		return -ENOMEM;
 
-	for (i = 0; i <= mm->max_order; ++i)
-		INIT_LIST_HEAD(&mm->free_list[i]);
+	mm->dirty_tree = kmalloc_array(mm->max_order + 1,
+				       sizeof(struct rb_root),
+				       GFP_KERNEL);
+	if (!mm->dirty_tree)
+		goto out_free_clear_tree;
+
+	for (i = 0; i <= mm->max_order; ++i) {
+		mm->clear_tree[i] = RB_ROOT;
+		mm->dirty_tree[i] = RB_ROOT;
+	}
 
 	mm->n_roots = hweight64(size);
 
@@ -272,7 +371,7 @@ int drm_buddy_init(struct drm_buddy *mm,
 				  sizeof(struct drm_buddy_block *),
 				  GFP_KERNEL);
 	if (!mm->roots)
-		goto out_free_list;
+		goto out_free_dirty_tree;
 
 	offset = 0;
 	i = 0;
@@ -311,8 +410,10 @@ out_free_roots:
 	while (i--)
 		drm_block_free(mm, mm->roots[i]);
 	kfree(mm->roots);
-out_free_list:
-	kfree(mm->free_list);
+out_free_dirty_tree:
+	kfree(mm->dirty_tree);
+out_free_clear_tree:
+	kfree(mm->clear_tree);
 	return -ENOMEM;
 }
 EXPORT_SYMBOL(drm_buddy_init);
@@ -349,7 +450,8 @@ void drm_buddy_fini(struct drm_buddy *mm
 	WARN_ON(mm->avail != mm->size);
 
 	kfree(mm->roots);
-	kfree(mm->free_list);
+	kfree(mm->clear_tree);
+	kfree(mm->dirty_tree);
 }
 EXPORT_SYMBOL(drm_buddy_fini);
 
@@ -373,16 +475,16 @@ static int split_block(struct drm_buddy
 		return -ENOMEM;
 	}
 
-	mark_free(mm, block->left);
-	mark_free(mm, block->right);
-
 	if (drm_buddy_block_is_clear(block)) {
 		mark_cleared(block->left);
 		mark_cleared(block->right);
 		clear_reset(block);
 	}
 
-	mark_split(block);
+	mark_free(mm, block->left);
+	mark_free(mm, block->right);
+
+	mark_split(mm, block);
 
 	return 0;
 }
@@ -411,7 +513,9 @@ EXPORT_SYMBOL(drm_get_buddy);
  * @is_clear: blocks clear state
  *
  * Reset the clear state based on @is_clear value for each block
- * in the freelist.
+ * in the freelist. This is the correctly ported version for modern kernels
+ * using Red-Black Trees, with an added optimization to perform a bulk update
+ * of the clear_avail counter for improved efficiency.
  */
 void drm_buddy_reset_clear(struct drm_buddy *mm, bool is_clear)
 {
@@ -429,20 +533,34 @@ void drm_buddy_reset_clear(struct drm_bu
 		size -= root_size;
 	}
 
-	for (i = 0; i <= mm->max_order; ++i) {
-		struct drm_buddy_block *block;
+	if (is_clear) {
 
-		list_for_each_entry_reverse(block, &mm->free_list[i], link) {
-			if (is_clear != drm_buddy_block_is_clear(block)) {
-				if (is_clear) {
-					mark_cleared(block);
-					mm->clear_avail += drm_buddy_block_size(mm, block);
-				} else {
-					clear_reset(block);
-					mm->clear_avail -= drm_buddy_block_size(mm, block);
-				}
+		for (i = 0; i <= mm->max_order; ++i) {
+			struct rb_root *root = __get_root(mm, i, DIRTY_TREE);
+			struct drm_buddy_block *block, *n;
+
+			for_each_rb_entry_reverse_safe(block, n, root, rb) {
+				rbtree_remove(mm, block);
+				mark_cleared(block);
+				rbtree_insert(mm, block, CLEAR_TREE);
 			}
 		}
+
+		mm->clear_avail = mm->avail;
+	} else {
+
+		for (i = 0; i <= mm->max_order; ++i) {
+			struct rb_root *root = __get_root(mm, i, CLEAR_TREE);
+			struct drm_buddy_block *block, *n;
+
+			for_each_rb_entry_reverse_safe(block, n, root, rb) {
+				rbtree_remove(mm, block);
+				clear_reset(block);
+				rbtree_insert(mm, block, DIRTY_TREE);
+			}
+		}
+
+		mm->clear_avail = 0;
 	}
 }
 EXPORT_SYMBOL(drm_buddy_reset_clear);
@@ -513,7 +631,7 @@ void drm_buddy_free_list(struct drm_budd
 }
 EXPORT_SYMBOL(drm_buddy_free_list);
 
-static bool block_incompatible(struct drm_buddy_block *block, unsigned int flags)
+static bool block_incompatible(struct drm_buddy_block *block, unsigned long flags)
 {
 	bool needs_clear = flags & DRM_BUDDY_CLEAR_ALLOCATION;
 
@@ -631,21 +749,20 @@ __drm_buddy_alloc_range_bias(struct drm_
 }
 
 static struct drm_buddy_block *
-get_maxblock(struct drm_buddy *mm, unsigned int order,
-	     unsigned long flags)
+get_maxblock(struct drm_buddy *mm,
+	     unsigned int order,
+	     enum free_tree tree)
 {
 	struct drm_buddy_block *max_block = NULL, *block = NULL;
+	struct rb_root *root;
 	unsigned int i;
 
 	for (i = order; i <= mm->max_order; ++i) {
-		struct drm_buddy_block *tmp_block;
-
-		list_for_each_entry_reverse(tmp_block, &mm->free_list[i], link) {
-			if (block_incompatible(tmp_block, flags))
+		root = __get_root(mm, i, tree);
+		if (!rbtree_is_empty(root)) {
+			block = rbtree_last_entry(root);
+			if (!block)
 				continue;
-
-			block = tmp_block;
-			break;
 		}
 
 		if (!block)
@@ -666,43 +783,43 @@ get_maxblock(struct drm_buddy *mm, unsig
 }
 
 static struct drm_buddy_block *
-alloc_from_freelist(struct drm_buddy *mm,
+alloc_from_freetree(struct drm_buddy *mm,
 		    unsigned int order,
 		    unsigned long flags)
 {
 	struct drm_buddy_block *block = NULL;
+	struct rb_root *root;
+	enum free_tree tree;
 	unsigned int tmp;
 	int err;
 
+	tree = __get_tree_for_flags(flags);
+
 	if (flags & DRM_BUDDY_TOPDOWN_ALLOCATION) {
-		block = get_maxblock(mm, order, flags);
+		block = get_maxblock(mm, order, tree);
 		if (block)
 			/* Store the obtained block order */
 			tmp = drm_buddy_block_order(block);
 	} else {
 		for (tmp = order; tmp <= mm->max_order; ++tmp) {
-			struct drm_buddy_block *tmp_block;
-
-			list_for_each_entry_reverse(tmp_block, &mm->free_list[tmp], link) {
-				if (block_incompatible(tmp_block, flags))
-					continue;
-
-				block = tmp_block;
-				break;
+			/* Get RB tree root for this order and tree */
+			root = __get_root(mm, tmp, tree);
+			if (!rbtree_is_empty(root)) {
+				block = rbtree_last_entry(root);
+				if (block)
+					break;
 			}
-
-			if (block)
-				break;
 		}
 	}
 
 	if (!block) {
-		/* Fallback method */
+		/* Try allocating from the other tree */
+		tree = (tree == CLEAR_TREE) ? DIRTY_TREE : CLEAR_TREE;
+
 		for (tmp = order; tmp <= mm->max_order; ++tmp) {
-			if (!list_empty(&mm->free_list[tmp])) {
-				block = list_last_entry(&mm->free_list[tmp],
-							struct drm_buddy_block,
-							link);
+			root = __get_root(mm, tmp, tree);
+			if (!rbtree_is_empty(root)) {
+				block = rbtree_last_entry(root);
 				if (block)
 					break;
 			}
@@ -770,7 +887,7 @@ static int __alloc_range(struct drm_budd
 
 		if (contains(start, end, block_start, block_end)) {
 			if (drm_buddy_block_is_free(block)) {
-				mark_allocated(block);
+				mark_allocated(mm, block);
 				total_allocated += drm_buddy_block_size(mm, block);
 				mm->avail -= drm_buddy_block_size(mm, block);
 				if (drm_buddy_block_is_clear(block))
@@ -848,7 +965,6 @@ static int __alloc_contig_try_harder(str
 {
 	u64 rhs_offset, lhs_offset, lhs_size, filled;
 	struct drm_buddy_block *block;
-	struct list_head *list;
 	LIST_HEAD(blocks_lhs);
 	unsigned long pages;
 	unsigned int order;
@@ -861,35 +977,39 @@ static int __alloc_contig_try_harder(str
 	if (order == 0)
 		return -ENOSPC;
 
-	list = &mm->free_list[order];
-	if (list_empty(list))
+	if (rbtree_is_empty(__get_root(mm, order, CLEAR_TREE)) &&
+	    rbtree_is_empty(__get_root(mm, order, DIRTY_TREE)))
 		return -ENOSPC;
 
-	list_for_each_entry_reverse(block, list, link) {
-		/* Allocate blocks traversing RHS */
-		rhs_offset = drm_buddy_block_offset(block);
-		err =  __drm_buddy_alloc_range(mm, rhs_offset, size,
-					       &filled, blocks);
-		if (!err || err != -ENOSPC)
-			return err;
-
-		lhs_size = max((size - filled), min_block_size);
-		if (!IS_ALIGNED(lhs_size, min_block_size))
-			lhs_size = round_up(lhs_size, min_block_size);
-
-		/* Allocate blocks traversing LHS */
-		lhs_offset = drm_buddy_block_offset(block) - lhs_size;
-		err =  __drm_buddy_alloc_range(mm, lhs_offset, lhs_size,
-					       NULL, &blocks_lhs);
-		if (!err) {
-			list_splice(&blocks_lhs, blocks);
-			return 0;
-		} else if (err != -ENOSPC) {
+	for_each_free_tree() {
+		struct rb_root *root = __get_root(mm, order, tree);
+
+		for_each_rb_entry_reverse(block, root, rb) {
+			/* Allocate blocks traversing RHS */
+			rhs_offset = drm_buddy_block_offset(block);
+			err =  __drm_buddy_alloc_range(mm, rhs_offset, size,
+						       &filled, blocks);
+			if (!err || err != -ENOSPC)
+				return err;
+
+			lhs_size = max((size - filled), min_block_size);
+			if (!IS_ALIGNED(lhs_size, min_block_size))
+				lhs_size = round_up(lhs_size, min_block_size);
+
+			/* Allocate blocks traversing LHS */
+			lhs_offset = drm_buddy_block_offset(block) - lhs_size;
+			err =  __drm_buddy_alloc_range(mm, lhs_offset, lhs_size,
+						       NULL, &blocks_lhs);
+			if (!err) {
+				list_splice(&blocks_lhs, blocks);
+				return 0;
+			} else if (err != -ENOSPC) {
+				drm_buddy_free_list_internal(mm, blocks);
+				return err;
+			}
+			/* Free blocks for the next iteration */
 			drm_buddy_free_list_internal(mm, blocks);
-			return err;
 		}
-		/* Free blocks for the next iteration */
-		drm_buddy_free_list_internal(mm, blocks);
 	}
 
 	return -ENOSPC;
@@ -975,7 +1095,7 @@ int drm_buddy_block_trim(struct drm_budd
 	list_add(&block->tmp_link, &dfs);
 	err =  __alloc_range(mm, &dfs, new_start, new_size, blocks, NULL);
 	if (err) {
-		mark_allocated(block);
+		mark_allocated(mm, block);
 		mm->avail -= drm_buddy_block_size(mm, block);
 		if (drm_buddy_block_is_clear(block))
 			mm->clear_avail -= drm_buddy_block_size(mm, block);
@@ -998,8 +1118,8 @@ __drm_buddy_alloc_blocks(struct drm_budd
 		return  __drm_buddy_alloc_range_bias(mm, start, end,
 						     order, flags);
 	else
-		/* Allocate from freelist */
-		return alloc_from_freelist(mm, order, flags);
+		/* Allocate from freetree */
+		return alloc_from_freetree(mm, order, flags);
 }
 
 /**
@@ -1016,8 +1136,8 @@ __drm_buddy_alloc_blocks(struct drm_budd
  * alloc_range_bias() called on range limitations, which traverses
  * the tree and returns the desired block.
  *
- * alloc_from_freelist() called when *no* range restrictions
- * are enforced, which picks the block from the freelist.
+ * alloc_from_freetree() called when *no* range restrictions
+ * are enforced, which picks the block from the freetree.
  *
  * Returns:
  * 0 on success, error code on failure.
@@ -1119,7 +1239,7 @@ int drm_buddy_alloc_blocks(struct drm_bu
 			}
 		} while (1);
 
-		mark_allocated(block);
+		mark_allocated(mm, block);
 		mm->avail -= drm_buddy_block_size(mm, block);
 		if (drm_buddy_block_is_clear(block))
 			mm->clear_avail -= drm_buddy_block_size(mm, block);
@@ -1201,11 +1321,16 @@ void drm_buddy_print(struct drm_buddy *m
 
 	for (order = mm->max_order; order >= 0; order--) {
 		struct drm_buddy_block *block;
+		struct rb_root *root;
 		u64 count = 0, free;
 
-		list_for_each_entry(block, &mm->free_list[order], link) {
-			BUG_ON(!drm_buddy_block_is_free(block));
-			count++;
+		for_each_free_tree() {
+			root = __get_root(mm, order, tree);
+
+			for_each_rb_entry(block, root, rb) {
+				BUG_ON(!drm_buddy_block_is_free(block));
+				count++;
+			}
 		}
 
 		drm_printf(p, "order-%2d ", order);
@@ -1228,7 +1353,7 @@ static void drm_buddy_module_exit(void)
 
 static int __init drm_buddy_module_init(void)
 {
-	slab_blocks = KMEM_CACHE(drm_buddy_block, 0);
+	slab_blocks = kmem_cache_create("drm_buddy_block", sizeof(struct drm_buddy_block), 64, SLAB_HWCACHE_ALIGN, NULL);
 	if (!slab_blocks)
 		return -ENOMEM;

--- a/include/drm/drm_buddy.h
+++ b/include/drm/drm_buddy.h
@@ -10,6 +10,7 @@
 #include <linux/list.h>
 #include <linux/slab.h>
 #include <linux/sched.h>
+#include <linux/rbtree.h>
 
 #include <drm/drm_print.h>
 
@@ -22,6 +23,44 @@
 	start__ >= max__ || size__ > max__ - start__; \
 })
 
+/*
+ * for_each_rb_entry() - iterate over an RB tree in order
+ * @pos:	the struct type * to use as a loop cursor
+ * @root:	pointer to struct rb_root to iterate
+ * @member:	name of the rb_node field within the struct
+ */
+#define for_each_rb_entry(pos, root, member) \
+	for (pos = rb_entry_safe(rb_first(root), typeof(*pos), member); \
+	     pos; \
+	     pos = rb_entry_safe(rb_next(&(pos)->member), typeof(*pos), member))
+
+/*
+ * for_each_rb_entry_reverse() - iterate over an RB tree in reverse order
+ * @pos:	the struct type * to use as a loop cursor
+ * @root:	pointer to struct rb_root to iterate
+ * @member:	name of the rb_node field within the struct
+ */
+#define for_each_rb_entry_reverse(pos, root, member) \
+	for (pos = rb_entry_safe(rb_last(root), typeof(*pos), member); \
+	     pos; \
+	     pos = rb_entry_safe(rb_prev(&(pos)->member), typeof(*pos), member))
+
+/**
+ * for_each_rb_entry_reverse_safe() - safely iterate over an RB tree in reverse order
+ * @pos:	the struct type * to use as a loop cursor.
+ * @n:		another struct type * to use as temporary storage.
+ * @root:	pointer to struct rb_root to iterate.
+ * @member:	name of the rb_node field within the struct.
+ */
+#define for_each_rb_entry_reverse_safe(pos, n, root, member) \
+	for (pos = rb_entry_safe(rb_last(root), typeof(*pos), member), \
+	     n = pos ? rb_entry_safe(rb_prev(&(pos)->member), typeof(*pos), member) : NULL; \
+	     pos; \
+	     pos = n, n = pos ? rb_entry_safe(rb_prev(&(pos)->member), typeof(*pos), member) : NULL)
+
+#define for_each_free_tree() \
+	for (enum free_tree tree = CLEAR_TREE; tree <= DIRTY_TREE; tree++)
+
 #define DRM_BUDDY_RANGE_ALLOCATION		BIT(0)
 #define DRM_BUDDY_TOPDOWN_ALLOCATION		BIT(1)
 #define DRM_BUDDY_CONTIGUOUS_ALLOCATION		BIT(2)
@@ -29,6 +68,11 @@
 #define DRM_BUDDY_CLEARED			BIT(4)
 #define DRM_BUDDY_TRIM_DISABLE			BIT(5)
 
+enum free_tree {
+	CLEAR_TREE = 0,
+	DIRTY_TREE,
+};
+
 struct drm_buddy_block {
 #define DRM_BUDDY_HEADER_OFFSET GENMASK_ULL(63, 12)
 #define DRM_BUDDY_HEADER_STATE  GENMASK_ULL(11, 10)
@@ -55,6 +99,9 @@ struct drm_buddy_block {
 	 */
 	struct list_head link;
 	struct list_head tmp_link;
+
+	enum free_tree tree;
+	struct rb_node rb;
 };
 
 /* Order-zero must be at least SZ_4K */
@@ -68,7 +115,8 @@ struct drm_buddy_block {
  */
 struct drm_buddy {
 	/* Maintain a free list for each order. */
-	struct list_head *free_list;
+	struct rb_root *clear_tree;
+	struct rb_root *dirty_tree;
 
 	/*
 	 * Maintain explicit binary tree(s) to track the allocation of the


--- a/drivers/gpu/drm/amd/amdgpu/gfx_v9_0.c	2025-07-12 17:16:53.286394076 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/gfx_v9_0.c	2025-07-12 17:43:17.514165504 +0200
@@ -66,6 +66,92 @@
 #define mmGCEA_PROBE_MAP                        0x070c
 #define mmGCEA_PROBE_MAP_BASE_IDX               0
 
+/* Helper macro for conditional register writes to reduce MMIO overhead */
+#ifndef GFX_V9_WREG_IF_CHANGED_H
+#define GFX_V9_WREG_IF_CHANGED_H
+
+#define WREG32_IF_CHANGED(_off_expr, _val_expr)                               \
+do {                                                                  \
+	const u32 __io_off = (_off_expr);                             \
+	const u32 __io_val = (_val_expr);                             \
+	const u32 __io_old = RREG32(__io_off);                        \
+	if (unlikely(__io_old != __io_val))                           \
+		WREG32(__io_off, __io_val);                           \
+} while (0)
+
+#endif /* GFX_V9_WREG_IF_CHANGED_H */
+
+/* File-scoped state for GRBM index caching to adhere to single-file modification constraint */
+struct grbm_state {
+	spinlock_t lock;
+	bool cache_valid;
+	u32 current_idx;
+	atomic64_t cache_hits;
+	atomic64_t cache_misses;
+};
+static struct grbm_state grbm_state_var;
+
+static void gfx_v9_0_grbm_state_init(struct amdgpu_device *adev)
+{
+	spin_lock_init(&grbm_state_var.lock);
+	grbm_state_var.cache_valid = false;
+	grbm_state_var.current_idx = 0;
+	atomic64_set(&grbm_state_var.cache_hits, 0);
+	atomic64_set(&grbm_state_var.cache_misses, 0);
+}
+
+static void gfx_v9_0_grbm_state_invalidate(struct amdgpu_device *adev)
+{
+	unsigned long flags;
+	spin_lock_irqsave(&grbm_state_var.lock, flags);
+	grbm_state_var.cache_valid = false;
+	spin_unlock_irqrestore(&grbm_state_var.lock, flags);
+}
+
+static __always_inline int
+gfx9_wait_reg_off(struct amdgpu_device *adev, u32 reg_offset_in_block,
+				  u32 mask, u32 val_target, unsigned long timeout_us)
+{
+	u32 current_read_val;
+	ktime_t timeout_expire;
+
+	if (unlikely(!adev))
+		return -EINVAL;
+
+	if (timeout_us == 0)
+		timeout_us = adev->usec_timeout;
+
+	timeout_expire = ktime_add_us(ktime_get(), timeout_us);
+
+	do {
+		current_read_val = RREG32(reg_offset_in_block);
+		if ((current_read_val & mask) == val_target)
+			return 0;
+
+		if (in_atomic() || irqs_disabled()) {
+			if (timeout_us > 20)
+				udelay(1);
+			else
+				cpu_relax();
+		} else {
+			if (timeout_us > 1000)
+				usleep_range(10, 100);
+			else
+				udelay(1);
+		}
+	} while (ktime_before(ktime_get(), timeout_expire));
+
+	current_read_val = RREG32(reg_offset_in_block);
+	if ((current_read_val & mask) == val_target)
+		return 0;
+
+	dev_warn_ratelimited(adev->dev,
+						 "Register wait timeout: reg=0x%x, mask=0x%x, target=0x%x, actual=0x%x\n",
+					  reg_offset_in_block, mask, val_target, current_read_val);
+
+	return -ETIMEDOUT;
+}
+
 MODULE_FIRMWARE("amdgpu/vega10_ce.bin");
 MODULE_FIRMWARE("amdgpu/vega10_pfp.bin");
 MODULE_FIRMWARE("amdgpu/vega10_me.bin");
@@ -1013,37 +1099,43 @@ static void gfx_v9_0_kiq_invalidate_tlbs
 			PACKET3_INVALIDATE_TLBS_FLUSH_TYPE(flush_type));
 }
 
-
-static void gfx_v9_0_kiq_reset_hw_queue(struct amdgpu_ring *kiq_ring, uint32_t queue_type,
-					uint32_t me_id, uint32_t pipe_id, uint32_t queue_id,
-					uint32_t xcc_id, uint32_t vmid)
+static void gfx_v9_0_kiq_reset_hw_queue(struct amdgpu_ring *kiq_ring,
+										u32 queue_type,
+										u32 me_id, u32 pipe_id, u32 queue_id,
+										u32 xcc_id, u32 vmid)
 {
 	struct amdgpu_device *adev = kiq_ring->adev;
-	unsigned i;
+	const unsigned long tmo = adev->usec_timeout / 5 + 1;
+	int r;
 
-	/* enter save mode */
 	amdgpu_gfx_rlc_enter_safe_mode(adev, xcc_id);
+
 	mutex_lock(&adev->srbm_mutex);
 	soc15_grbm_select(adev, me_id, pipe_id, queue_id, 0, 0);
+	mutex_unlock(&adev->srbm_mutex);
 
-	if (queue_type == AMDGPU_RING_TYPE_COMPUTE) {
-		WREG32_SOC15(GC, 0, mmCP_HQD_DEQUEUE_REQUEST, 0x2);
-		WREG32_SOC15(GC, 0, mmSPI_COMPUTE_QUEUE_RESET, 0x1);
-		/* wait till dequeue take effects */
-		for (i = 0; i < adev->usec_timeout; i++) {
-			if (!(RREG32_SOC15(GC, 0, mmCP_HQD_ACTIVE) & 1))
-				break;
-			udelay(1);
-		}
-		if (i >= adev->usec_timeout)
-			dev_err(adev->dev, "fail to wait on hqd deactive\n");
-	} else {
-		dev_err(adev->dev, "reset queue_type(%d) not supported\n", queue_type);
+	if (queue_type != AMDGPU_RING_TYPE_COMPUTE) {
+		dev_err_ratelimited(adev->dev,
+							"KIQ reset: queue_type %u not supported\n", queue_type);
+		goto restore;
 	}
 
+	WREG32_SOC15(GC, 0, mmCP_HQD_DEQUEUE_REQUEST, 0x2);
+	WREG32_SOC15(GC, 0, mmSPI_COMPUTE_QUEUE_RESET, 0x1);
+
+	r = gfx9_wait_reg_off(adev,
+						  SOC15_REG_OFFSET(GC, 0, mmCP_HQD_ACTIVE),
+						  CP_HQD_ACTIVE__ACTIVE_MASK, 0, tmo);
+	if (r)
+		dev_err_ratelimited(adev->dev,
+							"KIQ reset: HQD (ME%u/PIPE%u/Q%u) timeout\n",
+							me_id, pipe_id, queue_id);
+
+		restore:
+		mutex_lock(&adev->srbm_mutex);
 	soc15_grbm_select(adev, 0, 0, 0, 0, 0);
 	mutex_unlock(&adev->srbm_mutex);
-	/* exit safe mode */
+
 	amdgpu_gfx_rlc_exit_safe_mode(adev, xcc_id);
 }
 
@@ -2216,106 +2308,62 @@ static int gfx_v9_0_sw_init(struct amdgp
 	unsigned int hw_prio;
 
 	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
-	case IP_VERSION(9, 0, 1):
-	case IP_VERSION(9, 2, 1):
-	case IP_VERSION(9, 4, 0):
-	case IP_VERSION(9, 2, 2):
-	case IP_VERSION(9, 1, 0):
-	case IP_VERSION(9, 4, 1):
-	case IP_VERSION(9, 3, 0):
-	case IP_VERSION(9, 4, 2):
-		adev->gfx.mec.num_mec = 2;
-		break;
-	default:
-		adev->gfx.mec.num_mec = 1;
-		break;
+		case IP_VERSION(9, 0, 1):
+		case IP_VERSION(9, 2, 1):
+		case IP_VERSION(9, 4, 0):
+		case IP_VERSION(9, 2, 2):
+		case IP_VERSION(9, 1, 0):
+		case IP_VERSION(9, 4, 1):
+		case IP_VERSION(9, 3, 0):
+		case IP_VERSION(9, 4, 2):
+			adev->gfx.mec.num_mec = 2;
+			break;
+		default:
+			adev->gfx.mec.num_mec = 1;
+			break;
 	}
 
 	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
-	case IP_VERSION(9, 0, 1):
-	case IP_VERSION(9, 2, 1):
-	case IP_VERSION(9, 4, 0):
-	case IP_VERSION(9, 2, 2):
-	case IP_VERSION(9, 1, 0):
-	case IP_VERSION(9, 3, 0):
-		adev->gfx.cleaner_shader_ptr = gfx_9_4_2_cleaner_shader_hex;
-		adev->gfx.cleaner_shader_size = sizeof(gfx_9_4_2_cleaner_shader_hex);
-		if (adev->gfx.me_fw_version  >= 167 &&
-		    adev->gfx.pfp_fw_version >= 196 &&
-		    adev->gfx.mec_fw_version >= 474) {
-			adev->gfx.enable_cleaner_shader = true;
-			r = amdgpu_gfx_cleaner_shader_sw_init(adev, adev->gfx.cleaner_shader_size);
-			if (r) {
-				adev->gfx.enable_cleaner_shader = false;
-				dev_err(adev->dev, "Failed to initialize cleaner shader\n");
-			}
-		}
-		break;
-	case IP_VERSION(9, 4, 2):
-		adev->gfx.cleaner_shader_ptr = gfx_9_4_2_cleaner_shader_hex;
-		adev->gfx.cleaner_shader_size = sizeof(gfx_9_4_2_cleaner_shader_hex);
-		if (adev->gfx.mec_fw_version >= 88) {
-			adev->gfx.enable_cleaner_shader = true;
-			r = amdgpu_gfx_cleaner_shader_sw_init(adev, adev->gfx.cleaner_shader_size);
-			if (r) {
-				adev->gfx.enable_cleaner_shader = false;
-				dev_err(adev->dev, "Failed to initialize cleaner shader\n");
+		case IP_VERSION(9, 4, 2):
+			adev->gfx.cleaner_shader_ptr = gfx_9_4_2_cleaner_shader_hex;
+			adev->gfx.cleaner_shader_size = sizeof(gfx_9_4_2_cleaner_shader_hex);
+			if (adev->gfx.mec_fw_version >= 88) {
+				adev->gfx.enable_cleaner_shader = true;
+				r = amdgpu_gfx_cleaner_shader_sw_init(adev, adev->gfx.cleaner_shader_size);
+				if (r) {
+					adev->gfx.enable_cleaner_shader = false;
+					dev_err(adev->dev, "Failed to initialize cleaner shader\n");
+				}
 			}
-		}
-		break;
-	default:
-		adev->gfx.enable_cleaner_shader = false;
-		break;
+			break;
+		default:
+			adev->gfx.enable_cleaner_shader = false;
+			break;
 	}
 
 	adev->gfx.mec.num_pipe_per_mec = 4;
 	adev->gfx.mec.num_queue_per_pipe = 8;
 
-	/* EOP Event */
 	r = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_GRBM_CP, GFX_9_0__SRCID__CP_EOP_INTERRUPT, &adev->gfx.eop_irq);
-	if (r)
-		return r;
-
-	/* Bad opcode Event */
-	r = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_GRBM_CP,
-			      GFX_9_0__SRCID__CP_BAD_OPCODE_ERROR,
-			      &adev->gfx.bad_op_irq);
-	if (r)
-		return r;
-
-	/* Privileged reg */
-	r = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_GRBM_CP, GFX_9_0__SRCID__CP_PRIV_REG_FAULT,
-			      &adev->gfx.priv_reg_irq);
-	if (r)
-		return r;
-
-	/* Privileged inst */
-	r = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_GRBM_CP, GFX_9_0__SRCID__CP_PRIV_INSTR_FAULT,
-			      &adev->gfx.priv_inst_irq);
-	if (r)
-		return r;
-
-	/* ECC error */
-	r = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_GRBM_CP, GFX_9_0__SRCID__CP_ECC_ERROR,
-			      &adev->gfx.cp_ecc_error_irq);
-	if (r)
-		return r;
-
-	/* FUE error */
-	r = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_GRBM_CP, GFX_9_0__SRCID__CP_FUE_ERROR,
-			      &adev->gfx.cp_ecc_error_irq);
-	if (r)
-		return r;
+	if (r) return r;
+	r = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_GRBM_CP, GFX_9_0__SRCID__CP_BAD_OPCODE_ERROR, &adev->gfx.bad_op_irq);
+	if (r) return r;
+	r = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_GRBM_CP, GFX_9_0__SRCID__CP_PRIV_REG_FAULT, &adev->gfx.priv_reg_irq);
+	if (r) return r;
+	r = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_GRBM_CP, GFX_9_0__SRCID__CP_PRIV_INSTR_FAULT, &adev->gfx.priv_inst_irq);
+	if (r) return r;
+	r = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_GRBM_CP, GFX_9_0__SRCID__CP_ECC_ERROR, &adev->gfx.cp_ecc_error_irq);
+	if (r) return r;
+	r = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_GRBM_CP, GFX_9_0__SRCID__CP_FUE_ERROR, &adev->gfx.cp_ecc_error_irq);
+	if (r) return r;
 
 	adev->gfx.gfx_current_status = AMDGPU_GFX_NORMAL_MODE;
 
-	if (adev->gfx.rlc.funcs) {
-		if (adev->gfx.rlc.funcs->init) {
-			r = adev->gfx.rlc.funcs->init(adev);
-			if (r) {
-				dev_err(adev->dev, "Failed to init rlc BOs!\n");
-				return r;
-			}
+	if (adev->gfx.rlc.funcs && adev->gfx.rlc.funcs->init) {
+		r = adev->gfx.rlc.funcs->init(adev);
+		if (r) {
+			dev_err(adev->dev, "Failed to init rlc BOs!\n");
+			return r;
 		}
 	}
 
@@ -2325,28 +2373,18 @@ static int gfx_v9_0_sw_init(struct amdgp
 		return r;
 	}
 
-	/* set up the gfx ring */
 	for (i = 0; i < adev->gfx.num_gfx_rings; i++) {
 		ring = &adev->gfx.gfx_ring[i];
 		ring->ring_obj = NULL;
-		if (!i)
-			sprintf(ring->name, "gfx");
-		else
-			sprintf(ring->name, "gfx_%d", i);
+		sprintf(ring->name, "gfx_%d", i);
 		ring->use_doorbell = true;
 		ring->doorbell_index = adev->doorbell_index.gfx_ring0 << 1;
-
-		/* disable scheduler on the real ring */
 		ring->no_scheduler = adev->gfx.mcbp;
 		ring->vm_hub = AMDGPU_GFXHUB(0);
-		r = amdgpu_ring_init(adev, ring, 1024, &adev->gfx.eop_irq,
-				     AMDGPU_CP_IRQ_GFX_ME0_PIPE0_EOP,
-				     AMDGPU_RING_PRIO_DEFAULT, NULL);
-		if (r)
-			return r;
+		r = amdgpu_ring_init(adev, ring, 1024, &adev->gfx.eop_irq, AMDGPU_CP_IRQ_GFX_ME0_PIPE0_EOP, AMDGPU_RING_PRIO_DEFAULT, NULL);
+		if (r) return r;
 	}
 
-	/* set up the software rings */
 	if (adev->gfx.mcbp && adev->gfx.num_gfx_rings) {
 		for (i = 0; i < GFX9_NUM_SW_GFX_RINGS; i++) {
 			ring = &adev->gfx.sw_gfx_ring[i];
@@ -2357,24 +2395,18 @@ static int gfx_v9_0_sw_init(struct amdgp
 			ring->is_sw_ring = true;
 			hw_prio = amdgpu_sw_ring_priority(i);
 			ring->vm_hub = AMDGPU_GFXHUB(0);
-			r = amdgpu_ring_init(adev, ring, 1024, &adev->gfx.eop_irq,
-					     AMDGPU_CP_IRQ_GFX_ME0_PIPE0_EOP, hw_prio,
-					     NULL);
-			if (r)
-				return r;
+			r = amdgpu_ring_init(adev, ring, 1024, &adev->gfx.eop_irq, AMDGPU_CP_IRQ_GFX_ME0_PIPE0_EOP, hw_prio, NULL);
+			if (r) return r;
 			ring->wptr = 0;
 		}
 
-		/* init the muxer and add software rings */
-		r = amdgpu_ring_mux_init(&adev->gfx.muxer, &adev->gfx.gfx_ring[0],
-					 GFX9_NUM_SW_GFX_RINGS);
+		r = amdgpu_ring_mux_init(&adev->gfx.muxer, &adev->gfx.gfx_ring[0], GFX9_NUM_SW_GFX_RINGS);
 		if (r) {
 			DRM_ERROR("amdgpu_ring_mux_init failed(%d)\n", r);
 			return r;
 		}
 		for (i = 0; i < GFX9_NUM_SW_GFX_RINGS; i++) {
-			r = amdgpu_ring_mux_add_sw_ring(&adev->gfx.muxer,
-							&adev->gfx.sw_gfx_ring[i]);
+			r = amdgpu_ring_mux_add_sw_ring(&adev->gfx.muxer, &adev->gfx.sw_gfx_ring[i]);
 			if (r) {
 				DRM_ERROR("amdgpu_ring_mux_add_sw_ring failed(%d)\n", r);
 				return r;
@@ -2382,31 +2414,22 @@ static int gfx_v9_0_sw_init(struct amdgp
 		}
 	}
 
-	/* set up the compute queues - allocate horizontally across pipes */
 	ring_id = 0;
 	for (i = 0; i < adev->gfx.mec.num_mec; ++i) {
 		for (j = 0; j < adev->gfx.mec.num_queue_per_pipe; j++) {
 			for (k = 0; k < adev->gfx.mec.num_pipe_per_mec; k++) {
-				if (!amdgpu_gfx_is_mec_queue_enabled(adev, 0, i,
-								     k, j))
+				if (!amdgpu_gfx_is_mec_queue_enabled(adev, 0, i, k, j))
 					continue;
 
-				r = gfx_v9_0_compute_ring_init(adev,
-							       ring_id,
-							       i, k, j);
-				if (r)
-					return r;
-
+				r = gfx_v9_0_compute_ring_init(adev, ring_id, i, k, j);
+				if (r) return r;
 				ring_id++;
 			}
 		}
 	}
 
-	/* TODO: Add queue reset mask when FW fully supports it */
-	adev->gfx.gfx_supported_reset =
-		amdgpu_get_soft_full_reset_mask(&adev->gfx.gfx_ring[0]);
-	adev->gfx.compute_supported_reset =
-		amdgpu_get_soft_full_reset_mask(&adev->gfx.compute_ring[0]);
+	adev->gfx.gfx_supported_reset = amdgpu_get_soft_full_reset_mask(&adev->gfx.gfx_ring[0]);
+	adev->gfx.compute_supported_reset = amdgpu_get_soft_full_reset_mask(&adev->gfx.compute_ring[0]);
 
 	r = amdgpu_gfx_kiq_init(adev, GFX9_MEC_HPD_SIZE, 0);
 	if (r) {
@@ -2415,19 +2438,17 @@ static int gfx_v9_0_sw_init(struct amdgp
 	}
 
 	r = amdgpu_gfx_kiq_init_ring(adev, xcc_id);
-	if (r)
-		return r;
+	if (r) return r;
+
+	gfx_v9_0_grbm_state_init(adev);
 
-	/* create MQD for all compute queues as wel as KIQ for SRIOV case */
 	r = amdgpu_gfx_mqd_sw_init(adev, sizeof(struct v9_mqd_allocation), 0);
-	if (r)
-		return r;
+	if (r) return r;
 
 	adev->gfx.ce_ram_size = 0x8000;
 
 	r = gfx_v9_0_gpu_early_init(adev);
-	if (r)
-		return r;
+	if (r) return r;
 
 	if (amdgpu_gfx_ras_sw_init(adev)) {
 		dev_err(adev->dev, "Failed to initialize gfx ras block!\n");
@@ -2437,13 +2458,11 @@ static int gfx_v9_0_sw_init(struct amdgp
 	gfx_v9_0_alloc_ip_dump(adev);
 
 	r = amdgpu_gfx_sysfs_init(adev);
-	if (r)
-		return r;
+	if (r) return r;
 
 	return 0;
 }
 
-
 static int gfx_v9_0_sw_fini(struct amdgpu_ip_block *ip_block)
 {
 	int i;
@@ -2491,27 +2510,52 @@ static void gfx_v9_0_tiling_mode_table_i
 	/* TODO */
 }
 
-void gfx_v9_0_select_se_sh(struct amdgpu_device *adev, u32 se_num, u32 sh_num,
-			   u32 instance, int xcc_id)
+void gfx_v9_0_select_se_sh(struct amdgpu_device *adev, u32 se_num,
+						   u32 sh_num, u32 instance, int xcc_id)
 {
-	u32 data;
+	unsigned long flags;
+	u32 data = 0;
 
-	if (instance == 0xffffffff)
-		data = REG_SET_FIELD(0, GRBM_GFX_INDEX, INSTANCE_BROADCAST_WRITES, 1);
-	else
-		data = REG_SET_FIELD(0, GRBM_GFX_INDEX, INSTANCE_INDEX, instance);
+	if (instance == 0xffffffff) {
+		data |= GRBM_GFX_INDEX__INSTANCE_BROADCAST_WRITES_MASK;
+	} else {
+		data = REG_SET_FIELD(data, GRBM_GFX_INDEX, INSTANCE_INDEX, instance);
+	}
 
-	if (se_num == 0xffffffff)
-		data = REG_SET_FIELD(data, GRBM_GFX_INDEX, SE_BROADCAST_WRITES, 1);
-	else
+	if (se_num == 0xffffffff) {
+		data |= GRBM_GFX_INDEX__SE_BROADCAST_WRITES_MASK;
+	} else {
 		data = REG_SET_FIELD(data, GRBM_GFX_INDEX, SE_INDEX, se_num);
+	}
 
-	if (sh_num == 0xffffffff)
-		data = REG_SET_FIELD(data, GRBM_GFX_INDEX, SH_BROADCAST_WRITES, 1);
-	else
+	if (sh_num == 0xffffffff) {
+		data |= GRBM_GFX_INDEX__SH_BROADCAST_WRITES_MASK;
+	} else {
 		data = REG_SET_FIELD(data, GRBM_GFX_INDEX, SH_INDEX, sh_num);
+	}
 
-	WREG32_SOC15_RLC_SHADOW(GC, 0, mmGRBM_GFX_INDEX, data);
+	if (likely(grbm_state_var.cache_valid)) {
+		if (READ_ONCE(grbm_state_var.current_idx) == data) {
+			atomic64_inc(&grbm_state_var.cache_hits);
+			return;
+		}
+	}
+
+	spin_lock_irqsave(&grbm_state_var.lock, flags);
+
+	if (grbm_state_var.cache_valid &&
+		grbm_state_var.current_idx == data) {
+		atomic64_inc(&grbm_state_var.cache_hits);
+	spin_unlock_irqrestore(&grbm_state_var.lock, flags);
+	return;
+		}
+
+		WREG32_SOC15_RLC_SHADOW(GC, 0, mmGRBM_GFX_INDEX, data);
+		grbm_state_var.current_idx = data;
+		grbm_state_var.cache_valid = true;
+		atomic64_inc(&grbm_state_var.cache_misses);
+
+		spin_unlock_irqrestore(&grbm_state_var.lock, flags);
 }
 
 static u32 gfx_v9_0_get_rb_active_bitmap(struct amdgpu_device *adev)
@@ -2650,51 +2694,111 @@ static void gfx_v9_0_init_sq_config(stru
 	}
 }
 
+static void gfx_v9_0_optimize_memory_subsystem(struct amdgpu_device *adev)
+{
+	if (adev->gfx.gfx_current_status != AMDGPU_GFX_NORMAL_MODE) {
+		dev_dbg(adev->dev, "GPU not in normal mode, skipping memory optimization\n");
+		return;
+	}
+
+	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
+		case IP_VERSION(9, 0, 1):
+		case IP_VERSION(9, 2, 1):
+		case IP_VERSION(9, 4, 0):
+		case IP_VERSION(9, 1, 0):
+		case IP_VERSION(9, 2, 2):
+		case IP_VERSION(9, 3, 0):
+			WREG32_SOC15(GC, 0, mmSQC_CONFIG, 0x020a2000);
+			WREG32_SOC15(GC, 0, mmTA_CNTL_AUX, 0x010b0000);
+			WREG32_SOC15(GC, 0, mmVGT_CACHE_INVALIDATION, 0x19200000);
+			break;
+		default:
+			break;
+	}
+
+	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
+		case IP_VERSION(9, 0, 1):
+		case IP_VERSION(9, 4, 0):
+			WREG32_SOC15(GC, 0, mmTCP_CHAN_STEER_HI, 0x4a2c0e68);
+			WREG32_SOC15(GC, 0, mmTCP_CHAN_STEER_LO, 0xb5d3f197);
+			WREG32_SOC15(GC, 0, mmVGT_GS_MAX_WAVE_ID, 0x000003ff);
+			WREG32_SOC15(GC, 0, mmSPI_RESOURCE_RESERVE_CU_0, 0x00000800);
+			WREG32_SOC15(GC, 0, mmSPI_RESOURCE_RESERVE_CU_1, 0x00000800);
+			WREG32_SOC15(GC, 0, mmSPI_RESOURCE_RESERVE_EN_CU_0, 0x00ffff87);
+			WREG32_SOC15(GC, 0, mmSPI_RESOURCE_RESERVE_EN_CU_1, 0x00ffff8f);
+			break;
+		case IP_VERSION(9, 2, 1):
+			WREG32_SOC15(GC, 0, mmTCP_CHAN_STEER_HI, 0x00000000);
+			WREG32_SOC15(GC, 0, mmTCP_CHAN_STEER_LO, 0x76325410);
+			WREG32_SOC15(GC, 0, mmVGT_GS_MAX_WAVE_ID, 0x000003ff);
+			WREG32_SOC15(GC, 0, mmSPI_RESOURCE_RESERVE_CU_0, 0x00000800);
+			WREG32_SOC15(GC, 0, mmSPI_RESOURCE_RESERVE_CU_1, 0x00000800);
+			WREG32_SOC15(GC, 0, mmSPI_RESOURCE_RESERVE_EN_CU_0, 0x0000ff87);
+			WREG32_SOC15(GC, 0, mmSPI_RESOURCE_RESERVE_EN_CU_1, 0x0000ff8f);
+			break;
+		case IP_VERSION(9, 1, 0):
+		case IP_VERSION(9, 2, 2):
+		case IP_VERSION(9, 3, 0):
+			WREG32_SOC15(GC, 0, mmTCP_CHAN_STEER_HI, 0x00000000);
+			WREG32_SOC15(GC, 0, mmTCP_CHAN_STEER_LO, 0x00003120);
+			if (amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 3, 0))
+				WREG32_SOC15(GC, 0, mmGCEA_PROBE_MAP, 0x0000cccc);
+		WREG32_SOC15(GC, 0, mmVGT_GS_MAX_WAVE_ID, 0x000000ff);
+		break;
+		default:
+			break;
+	}
+
+	dev_dbg(adev->dev, "Memory subsystem optimization applied for IP %d.%d.%d\n",
+			IP_VERSION_MAJ(amdgpu_ip_version(adev, GC_HWIP, 0)),
+			IP_VERSION_MIN(amdgpu_ip_version(adev, GC_HWIP, 0)),
+			IP_VERSION_REV(amdgpu_ip_version(adev, GC_HWIP, 0)));
+}
+
 static void gfx_v9_0_constants_init(struct amdgpu_device *adev)
 {
 	u32 tmp;
 	int i;
 
 	if (!amdgpu_sriov_vf(adev) ||
-	    amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 4, 2)) {
+		amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 4, 2)) {
 		WREG32_FIELD15_RLC(GC, 0, GRBM_CNTL, READ_TIMEOUT, 0xff);
-	}
+		}
 
-	gfx_v9_0_tiling_mode_table_init(adev);
+		gfx_v9_0_tiling_mode_table_init(adev);
 
 	if (adev->gfx.num_gfx_rings)
 		gfx_v9_0_setup_rb(adev);
 	gfx_v9_0_get_cu_info(adev, &adev->gfx.cu_info);
 	adev->gfx.config.db_debug2 = RREG32_SOC15(GC, 0, mmDB_DEBUG2);
 
-	/* XXX SH_MEM regs */
-	/* where to put LDS, scratch, GPUVM in FSA64 space */
+	/* Apply Godlike optimizations */
+	gfx_v9_0_optimize_memory_subsystem(adev);
+
 	mutex_lock(&adev->srbm_mutex);
 	for (i = 0; i < adev->vm_manager.id_mgr[AMDGPU_GFXHUB(0)].num_ids; i++) {
 		soc15_grbm_select(adev, 0, 0, 0, i, 0);
-		/* CP and shaders */
 		if (i == 0) {
 			tmp = REG_SET_FIELD(0, SH_MEM_CONFIG, ALIGNMENT_MODE,
-					    SH_MEM_ALIGNMENT_MODE_UNALIGNED);
+								SH_MEM_ALIGNMENT_MODE_UNALIGNED);
 			tmp = REG_SET_FIELD(tmp, SH_MEM_CONFIG, RETRY_DISABLE,
-					    !!adev->gmc.noretry);
+								!!adev->gmc.noretry);
 			WREG32_SOC15_RLC(GC, 0, mmSH_MEM_CONFIG, tmp);
 			WREG32_SOC15_RLC(GC, 0, mmSH_MEM_BASES, 0);
 		} else {
 			tmp = REG_SET_FIELD(0, SH_MEM_CONFIG, ALIGNMENT_MODE,
-					    SH_MEM_ALIGNMENT_MODE_UNALIGNED);
+								SH_MEM_ALIGNMENT_MODE_UNALIGNED);
 			tmp = REG_SET_FIELD(tmp, SH_MEM_CONFIG, RETRY_DISABLE,
-					    !!adev->gmc.noretry);
+								!!adev->gmc.noretry);
 			WREG32_SOC15_RLC(GC, 0, mmSH_MEM_CONFIG, tmp);
 			tmp = REG_SET_FIELD(0, SH_MEM_BASES, PRIVATE_BASE,
-				(adev->gmc.private_aperture_start >> 48));
+								(adev->gmc.private_aperture_start >> 48));
 			tmp = REG_SET_FIELD(tmp, SH_MEM_BASES, SHARED_BASE,
-				(adev->gmc.shared_aperture_start >> 48));
+								(adev->gmc.shared_aperture_start >> 48));
 			WREG32_SOC15_RLC(GC, 0, mmSH_MEM_BASES, tmp);
 		}
 	}
 	soc15_grbm_select(adev, 0, 0, 0, 0, 0);
-
 	mutex_unlock(&adev->srbm_mutex);
 
 	gfx_v9_0_init_compute_vmid(adev);
@@ -2916,28 +3020,22 @@ static void gfx_v9_0_enable_save_restore
 }
 
 static void pwr_10_0_gfxip_control_over_cgpg(struct amdgpu_device *adev,
-					     bool enable)
+											 bool enable)
 {
-	uint32_t data = 0;
-	uint32_t default_data = 0;
+	const u32 off = SOC15_REG_OFFSET(PWR, 0, mmPWR_MISC_CNTL_STATUS);
+	u32 v;
 
-	default_data = data = RREG32(SOC15_REG_OFFSET(PWR, 0, mmPWR_MISC_CNTL_STATUS));
 	if (enable) {
-		/* enable GFXIP control over CGPG */
-		data |= PWR_MISC_CNTL_STATUS__PWR_GFX_RLC_CGPG_EN_MASK;
-		if(default_data != data)
-			WREG32(SOC15_REG_OFFSET(PWR, 0, mmPWR_MISC_CNTL_STATUS), data);
-
-		/* update status */
-		data &= ~PWR_MISC_CNTL_STATUS__PWR_GFXOFF_STATUS_MASK;
-		data |= (2 << PWR_MISC_CNTL_STATUS__PWR_GFXOFF_STATUS__SHIFT);
-		if(default_data != data)
-			WREG32(SOC15_REG_OFFSET(PWR, 0, mmPWR_MISC_CNTL_STATUS), data);
+		v = RREG32(off) | PWR_MISC_CNTL_STATUS__PWR_GFX_RLC_CGPG_EN_MASK;
+		WREG32_IF_CHANGED(off, v);
+
+		v  = RREG32(off);
+		v &= ~PWR_MISC_CNTL_STATUS__PWR_GFXOFF_STATUS_MASK;
+		v |=  (2 << PWR_MISC_CNTL_STATUS__PWR_GFXOFF_STATUS__SHIFT);
+		WREG32_IF_CHANGED(off, v);
 	} else {
-		/* restore GFXIP control over GCPG */
-		data &= ~PWR_MISC_CNTL_STATUS__PWR_GFX_RLC_CGPG_EN_MASK;
-		if(default_data != data)
-			WREG32(SOC15_REG_OFFSET(PWR, 0, mmPWR_MISC_CNTL_STATUS), data);
+		v = RREG32(off) & ~PWR_MISC_CNTL_STATUS__PWR_GFX_RLC_CGPG_EN_MASK;
+		WREG32_IF_CHANGED(off, v);
 	}
 }
 
@@ -2984,75 +3082,61 @@ static void gfx_v9_0_init_gfx_power_gati
 }
 
 static void gfx_v9_0_enable_sck_slow_down_on_power_up(struct amdgpu_device *adev,
-						bool enable)
+													  bool enable)
 {
-	uint32_t data = 0;
-	uint32_t default_data = 0;
+	const u32 off = SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL);
+	u32 v = RREG32(off);
 
-	default_data = data = RREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL));
-	data = REG_SET_FIELD(data, RLC_PG_CNTL,
-			     SMU_CLK_SLOWDOWN_ON_PU_ENABLE,
-			     enable ? 1 : 0);
-	if (default_data != data)
-		WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL), data);
+	v = REG_SET_FIELD(v, RLC_PG_CNTL,
+					  SMU_CLK_SLOWDOWN_ON_PU_ENABLE, enable ? 1 : 0);
+	WREG32_IF_CHANGED(off, v);
 }
 
 static void gfx_v9_0_enable_sck_slow_down_on_power_down(struct amdgpu_device *adev,
-						bool enable)
+														bool enable)
 {
-	uint32_t data = 0;
-	uint32_t default_data = 0;
+	const u32 off = SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL);
+	u32 v = RREG32(off);
 
-	default_data = data = RREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL));
-	data = REG_SET_FIELD(data, RLC_PG_CNTL,
-			     SMU_CLK_SLOWDOWN_ON_PD_ENABLE,
-			     enable ? 1 : 0);
-	if(default_data != data)
-		WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL), data);
+	v = REG_SET_FIELD(v, RLC_PG_CNTL,
+					  SMU_CLK_SLOWDOWN_ON_PD_ENABLE, enable ? 1 : 0);
+	WREG32_IF_CHANGED(off, v);
 }
 
 static void gfx_v9_0_enable_cp_power_gating(struct amdgpu_device *adev,
-					bool enable)
+											bool enable)
 {
-	uint32_t data = 0;
-	uint32_t default_data = 0;
+	const u32 off = SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL);
+	u32 v = RREG32(off);
 
-	default_data = data = RREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL));
-	data = REG_SET_FIELD(data, RLC_PG_CNTL,
-			     CP_PG_DISABLE,
-			     enable ? 0 : 1);
-	if(default_data != data)
-		WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL), data);
+	v = REG_SET_FIELD(v, RLC_PG_CNTL, CP_PG_DISABLE, enable ? 0 : 1);
+	WREG32_IF_CHANGED(off, v);
 }
 
 static void gfx_v9_0_enable_gfx_cg_power_gating(struct amdgpu_device *adev,
-						bool enable)
+												bool enable)
 {
-	uint32_t data, default_data;
+	const u32 off = SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL);
+	u32 v = RREG32(off);
 
-	default_data = data = RREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL));
-	data = REG_SET_FIELD(data, RLC_PG_CNTL,
-			     GFX_POWER_GATING_ENABLE,
-			     enable ? 1 : 0);
-	if(default_data != data)
-		WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL), data);
+	v = REG_SET_FIELD(v, RLC_PG_CNTL,
+					  GFX_POWER_GATING_ENABLE, enable ? 1 : 0);
+	WREG32_IF_CHANGED(off, v);
 }
 
 static void gfx_v9_0_enable_gfx_pipeline_powergating(struct amdgpu_device *adev,
-						bool enable)
+													 bool enable)
 {
-	uint32_t data, default_data;
+	const u32 off = SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL);
+	u32 v = RREG32(off);
 
-	default_data = data = RREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL));
-	data = REG_SET_FIELD(data, RLC_PG_CNTL,
-			     GFX_PIPELINE_PG_ENABLE,
-			     enable ? 1 : 0);
-	if(default_data != data)
-		WREG32(SOC15_REG_OFFSET(GC, 0, mmRLC_PG_CNTL), data);
+	v = REG_SET_FIELD(v, RLC_PG_CNTL,
+					  GFX_PIPELINE_PG_ENABLE, enable ? 1 : 0);
+	WREG32_IF_CHANGED(off, v);
 
+	/* Read a GFX register to ensure GFX is woken up if un-gating */
 	if (!enable)
-		/* read any GFX register to wake up GFX */
-		data = RREG32(SOC15_REG_OFFSET(GC, 0, mmDB_RENDER_CONTROL));
+		(void)RREG32(SOC15_REG_OFFSET(GC, 0, mmDB_RENDER_CONTROL));
 }
 
 static void gfx_v9_0_enable_gfx_static_mg_power_gating(struct amdgpu_device *adev,
@@ -3784,40 +3868,36 @@ static int gfx_v9_0_kiq_init_register(st
 static int gfx_v9_0_kiq_fini_register(struct amdgpu_ring *ring)
 {
 	struct amdgpu_device *adev = ring->adev;
-	int j;
+	const unsigned long tmo = adev->usec_timeout / 5 + 1;
+	int r = 0;
 
-	/* disable the queue if it's active */
-	if (RREG32_SOC15(GC, 0, mmCP_HQD_ACTIVE) & 1) {
+	if (RREG32(SOC15_REG_OFFSET(GC, 0, mmCP_HQD_ACTIVE)) &
+		CP_HQD_ACTIVE__ACTIVE_MASK) {
 
 		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_DEQUEUE_REQUEST, 1);
 
-		for (j = 0; j < adev->usec_timeout; j++) {
-			if (!(RREG32_SOC15(GC, 0, mmCP_HQD_ACTIVE) & 1))
-				break;
-			udelay(1);
-		}
-
-		if (j == AMDGPU_MAX_USEC_TIMEOUT) {
-			DRM_DEBUG("KIQ dequeue request failed.\n");
-
-			/* Manual disable if dequeue request times out */
-			WREG32_SOC15_RLC(GC, 0, mmCP_HQD_ACTIVE, 0);
-		}
-
-		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_DEQUEUE_REQUEST,
-		      0);
-	}
-
-	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_IQ_TIMER, 0);
-	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_IB_CONTROL, 0);
-	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PERSISTENT_STATE, 0);
-	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_DOORBELL_CONTROL, 0x40000000);
-	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_DOORBELL_CONTROL, 0);
-	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_RPTR, 0);
-	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_WPTR_HI, 0);
-	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_WPTR_LO, 0);
+	r = gfx9_wait_reg_off(adev,
+						  SOC15_REG_OFFSET(GC, 0, mmCP_HQD_ACTIVE),
+						  CP_HQD_ACTIVE__ACTIVE_MASK, 0, tmo);
+	if (r) {
+		dev_dbg_ratelimited(adev->dev,
+							"KIQ fini: dequeue timeout, forcing inactive\n");
+		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_ACTIVE, 0);
+	}
+
+	WREG32_SOC15_RLC(GC, 0, mmCP_HQD_DEQUEUE_REQUEST, 0);
+		}
+
+		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_IQ_TIMER, 0);
+		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_IB_CONTROL, 0);
+		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PERSISTENT_STATE, 0);
+		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_DOORBELL_CONTROL, 0x40000000);
+		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_DOORBELL_CONTROL, 0);
+		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_RPTR, 0);
+		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_WPTR_HI, 0);
+		WREG32_SOC15_RLC(GC, 0, mmCP_HQD_PQ_WPTR_LO, 0);
 
-	return 0;
+		return r;
 }
 
 static int gfx_v9_0_kiq_init_queue(struct amdgpu_ring *ring)
@@ -4013,14 +4093,11 @@ static int gfx_v9_0_hw_init(struct amdgp
 	int r;
 	struct amdgpu_device *adev = ip_block->adev;
 
-	amdgpu_gfx_cleaner_shader_init(adev, adev->gfx.cleaner_shader_size,
-				       adev->gfx.cleaner_shader_ptr);
-
-	if (!amdgpu_sriov_vf(adev))
+	if (!amdgpu_sriov_vf(adev)) {
 		gfx_v9_0_init_golden_registers(adev);
+	}
 
 	gfx_v9_0_constants_init(adev);
-
 	gfx_v9_0_init_tcp_config(adev);
 
 	r = adev->gfx.rlc.funcs->resume(adev);
@@ -4032,7 +4109,7 @@ static int gfx_v9_0_hw_init(struct amdgp
 		return r;
 
 	if (amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 2) &&
-	    !amdgpu_sriov_vf(adev))
+		!amdgpu_sriov_vf(adev))
 		gfx_v9_4_2_set_power_brake_sequence(adev);
 
 	return r;
@@ -4092,7 +4169,13 @@ static int gfx_v9_0_hw_fini(struct amdgp
 
 static int gfx_v9_0_suspend(struct amdgpu_ip_block *ip_block)
 {
-	return gfx_v9_0_hw_fini(ip_block);
+	struct amdgpu_device *adev = ip_block->adev;
+	int r;
+
+	gfx_v9_0_grbm_state_invalidate(adev);
+	r = gfx_v9_0_hw_fini(ip_block);
+
+	return r;
 }
 
 static int gfx_v9_0_resume(struct amdgpu_ip_block *ip_block)
@@ -4126,65 +4209,59 @@ static int gfx_v9_0_wait_for_idle(struct
 
 static int gfx_v9_0_soft_reset(struct amdgpu_ip_block *ip_block)
 {
-	u32 grbm_soft_reset = 0;
-	u32 tmp;
-	struct amdgpu_device *adev = ip_block->adev;
-
-	/* GRBM_STATUS */
-	tmp = RREG32_SOC15(GC, 0, mmGRBM_STATUS);
-	if (tmp & (GRBM_STATUS__PA_BUSY_MASK | GRBM_STATUS__SC_BUSY_MASK |
-		   GRBM_STATUS__BCI_BUSY_MASK | GRBM_STATUS__SX_BUSY_MASK |
-		   GRBM_STATUS__TA_BUSY_MASK | GRBM_STATUS__VGT_BUSY_MASK |
-		   GRBM_STATUS__DB_BUSY_MASK | GRBM_STATUS__CB_BUSY_MASK |
-		   GRBM_STATUS__GDS_BUSY_MASK | GRBM_STATUS__SPI_BUSY_MASK |
-		   GRBM_STATUS__IA_BUSY_MASK | GRBM_STATUS__IA_BUSY_NO_DMA_MASK)) {
-		grbm_soft_reset = REG_SET_FIELD(grbm_soft_reset,
-						GRBM_SOFT_RESET, SOFT_RESET_CP, 1);
-		grbm_soft_reset = REG_SET_FIELD(grbm_soft_reset,
-						GRBM_SOFT_RESET, SOFT_RESET_GFX, 1);
-	}
-
-	if (tmp & (GRBM_STATUS__CP_BUSY_MASK | GRBM_STATUS__CP_COHERENCY_BUSY_MASK)) {
-		grbm_soft_reset = REG_SET_FIELD(grbm_soft_reset,
-						GRBM_SOFT_RESET, SOFT_RESET_CP, 1);
-	}
-
-	/* GRBM_STATUS2 */
-	tmp = RREG32_SOC15(GC, 0, mmGRBM_STATUS2);
-	if (REG_GET_FIELD(tmp, GRBM_STATUS2, RLC_BUSY))
-		grbm_soft_reset = REG_SET_FIELD(grbm_soft_reset,
-						GRBM_SOFT_RESET, SOFT_RESET_RLC, 1);
-
-
-	if (grbm_soft_reset) {
-		/* stop the rlc */
-		adev->gfx.rlc.funcs->stop(adev);
-
-		if (adev->gfx.num_gfx_rings)
-			/* Disable GFX parsing/prefetching */
-			gfx_v9_0_cp_gfx_enable(adev, false);
-
-		/* Disable MEC parsing/prefetching */
-		gfx_v9_0_cp_compute_enable(adev, false);
-
-		if (grbm_soft_reset) {
-			tmp = RREG32_SOC15(GC, 0, mmGRBM_SOFT_RESET);
-			tmp |= grbm_soft_reset;
-			dev_info(adev->dev, "GRBM_SOFT_RESET=0x%08X\n", tmp);
-			WREG32_SOC15(GC, 0, mmGRBM_SOFT_RESET, tmp);
-			tmp = RREG32_SOC15(GC, 0, mmGRBM_SOFT_RESET);
-
-			udelay(50);
-
-			tmp &= ~grbm_soft_reset;
-			WREG32_SOC15(GC, 0, mmGRBM_SOFT_RESET, tmp);
-			tmp = RREG32_SOC15(GC, 0, mmGRBM_SOFT_RESET);
-		}
-
-		/* Wait a little for things to settle down */
-		udelay(50);
-	}
-	return 0;
+    struct amdgpu_device *adev = ip_block->adev;
+    u32 grbm_soft_reset = 0;
+    u32 tmp;
+
+    gfx_v9_0_grbm_state_invalidate(adev);
+
+    tmp = RREG32_SOC15(GC, 0, mmGRBM_STATUS);
+    if (tmp & (GRBM_STATUS__PA_BUSY_MASK | GRBM_STATUS__SC_BUSY_MASK |
+               GRBM_STATUS__BCI_BUSY_MASK | GRBM_STATUS__SX_BUSY_MASK |
+               GRBM_STATUS__TA_BUSY_MASK | GRBM_STATUS__VGT_BUSY_MASK |
+               GRBM_STATUS__DB_BUSY_MASK | GRBM_STATUS__CB_BUSY_MASK |
+               GRBM_STATUS__GDS_BUSY_MASK | GRBM_STATUS__SPI_BUSY_MASK |
+               GRBM_STATUS__IA_BUSY_MASK | GRBM_STATUS__IA_BUSY_NO_DMA_MASK)) {
+        grbm_soft_reset = REG_SET_FIELD(grbm_soft_reset,
+                        GRBM_SOFT_RESET, SOFT_RESET_CP, 1);
+        grbm_soft_reset = REG_SET_FIELD(grbm_soft_reset,
+                        GRBM_SOFT_RESET, SOFT_RESET_GFX, 1);
+    }
+
+    if (tmp & (GRBM_STATUS__CP_BUSY_MASK | GRBM_STATUS__CP_COHERENCY_BUSY_MASK)) {
+        grbm_soft_reset = REG_SET_FIELD(grbm_soft_reset,
+                        GRBM_SOFT_RESET, SOFT_RESET_CP, 1);
+    }
+
+    tmp = RREG32_SOC15(GC, 0, mmGRBM_STATUS2);
+    if (REG_GET_FIELD(tmp, GRBM_STATUS2, RLC_BUSY))
+        grbm_soft_reset = REG_SET_FIELD(grbm_soft_reset,
+                        GRBM_SOFT_RESET, SOFT_RESET_RLC, 1);
+
+    if (grbm_soft_reset) {
+        adev->gfx.rlc.funcs->stop(adev);
+
+        if (amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 4, 1) &&
+            amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 4, 2)) {
+            gfx_v9_0_cp_gfx_enable(adev, false);
+            gfx_v9_0_cp_compute_enable(adev, false);
+        }
+
+        tmp = RREG32_SOC15(GC, 0, mmGRBM_SOFT_RESET);
+        tmp |= grbm_soft_reset;
+        dev_info(adev->dev, "GRBM_SOFT_RESET=0x%08X\n", tmp);
+        WREG32_SOC15(GC, 0, mmGRBM_SOFT_RESET, tmp);
+        tmp = RREG32_SOC15(GC, 0, mmGRBM_SOFT_RESET);
+
+        udelay(50);
+
+        tmp &= ~grbm_soft_reset;
+        WREG32_SOC15(GC, 0, mmGRBM_SOFT_RESET, tmp);
+        tmp = RREG32_SOC15(GC, 0, mmGRBM_SOFT_RESET);
+
+        udelay(50);
+    }
+    return 0;
 }
 
 static uint64_t gfx_v9_0_kiq_read_clock(struct amdgpu_device *adev)
@@ -4950,202 +5027,125 @@ static void gfx_v9_0_update_gfx_mg_power
 }
 
 static void gfx_v9_0_update_medium_grain_clock_gating(struct amdgpu_device *adev,
-						      bool enable)
+													  bool enable)
 {
-	uint32_t data, def;
+	uint32_t def, data;
 
-	/* It is disabled by HW by default */
-	if (enable && (adev->cg_flags & AMD_CG_SUPPORT_GFX_MGCG)) {
-		/* 1 - RLC_CGTT_MGCG_OVERRIDE */
-		def = data = RREG32_SOC15(GC, 0, mmRLC_CGTT_MGCG_OVERRIDE);
+	if (!(adev->cg_flags & AMD_CG_SUPPORT_GFX_MGCG))
+		return;
 
-		if (amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 2, 1))
-			data &= ~RLC_CGTT_MGCG_OVERRIDE__CPF_CGTT_SCLK_OVERRIDE_MASK;
+	def = data = RREG32_SOC15(GC, 0, mmRLC_CGTT_MGCG_OVERRIDE);
 
+	if (enable) {
 		data &= ~(RLC_CGTT_MGCG_OVERRIDE__GRBM_CGTT_SCLK_OVERRIDE_MASK |
-			  RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGCG_OVERRIDE_MASK |
-			  RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGLS_OVERRIDE_MASK);
-
-		/* only for Vega10 & Raven1 */
-		data |= RLC_CGTT_MGCG_OVERRIDE__RLC_CGTT_SCLK_OVERRIDE_MASK;
+		RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGCG_OVERRIDE_MASK |
+		RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGLS_OVERRIDE_MASK);
 
-		if (def != data)
-			WREG32_SOC15(GC, 0, mmRLC_CGTT_MGCG_OVERRIDE, data);
-
-		/* MGLS is a global flag to control all MGLS in GFX */
-		if (adev->cg_flags & AMD_CG_SUPPORT_GFX_MGLS) {
-			/* 2 - RLC memory Light sleep */
-			if (adev->cg_flags & AMD_CG_SUPPORT_GFX_RLC_LS) {
-				def = data = RREG32_SOC15(GC, 0, mmRLC_MEM_SLP_CNTL);
-				data |= RLC_MEM_SLP_CNTL__RLC_MEM_LS_EN_MASK;
-				if (def != data)
-					WREG32_SOC15(GC, 0, mmRLC_MEM_SLP_CNTL, data);
-			}
-			/* 3 - CP memory Light sleep */
-			if (adev->cg_flags & AMD_CG_SUPPORT_GFX_CP_LS) {
-				def = data = RREG32_SOC15(GC, 0, mmCP_MEM_SLP_CNTL);
-				data |= CP_MEM_SLP_CNTL__CP_MEM_LS_EN_MASK;
-				if (def != data)
-					WREG32_SOC15(GC, 0, mmCP_MEM_SLP_CNTL, data);
-			}
+		switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
+			case IP_VERSION(9, 2, 1):
+			case IP_VERSION(9, 4, 0):
+			case IP_VERSION(9, 4, 1):
+			case IP_VERSION(9, 4, 2):
+				data &= ~RLC_CGTT_MGCG_OVERRIDE__CPF_CGTT_SCLK_OVERRIDE_MASK;
+				break;
+			default:
+				break;
 		}
 	} else {
-		/* 1 - MGCG_OVERRIDE */
-		def = data = RREG32_SOC15(GC, 0, mmRLC_CGTT_MGCG_OVERRIDE);
-
-		if (amdgpu_ip_version(adev, GC_HWIP, 0) != IP_VERSION(9, 2, 1))
-			data |= RLC_CGTT_MGCG_OVERRIDE__CPF_CGTT_SCLK_OVERRIDE_MASK;
-
-		data |= (RLC_CGTT_MGCG_OVERRIDE__RLC_CGTT_SCLK_OVERRIDE_MASK |
-			 RLC_CGTT_MGCG_OVERRIDE__GRBM_CGTT_SCLK_OVERRIDE_MASK |
-			 RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGCG_OVERRIDE_MASK |
-			 RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGLS_OVERRIDE_MASK);
-
-		if (def != data)
-			WREG32_SOC15(GC, 0, mmRLC_CGTT_MGCG_OVERRIDE, data);
-
-		/* 2 - disable MGLS in RLC */
-		data = RREG32_SOC15(GC, 0, mmRLC_MEM_SLP_CNTL);
-		if (data & RLC_MEM_SLP_CNTL__RLC_MEM_LS_EN_MASK) {
-			data &= ~RLC_MEM_SLP_CNTL__RLC_MEM_LS_EN_MASK;
-			WREG32_SOC15(GC, 0, mmRLC_MEM_SLP_CNTL, data);
-		}
-
-		/* 3 - disable MGLS in CP */
-		data = RREG32_SOC15(GC, 0, mmCP_MEM_SLP_CNTL);
-		if (data & CP_MEM_SLP_CNTL__CP_MEM_LS_EN_MASK) {
-			data &= ~CP_MEM_SLP_CNTL__CP_MEM_LS_EN_MASK;
-			WREG32_SOC15(GC, 0, mmCP_MEM_SLP_CNTL, data);
-		}
+		data |= (RLC_CGTT_MGCG_OVERRIDE__CPF_CGTT_SCLK_OVERRIDE_MASK |
+		RLC_CGTT_MGCG_OVERRIDE__GRBM_CGTT_SCLK_OVERRIDE_MASK |
+		RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGCG_OVERRIDE_MASK |
+		RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGLS_OVERRIDE_MASK);
 	}
+
+	if (def != data)
+		WREG32_SOC15(GC, 0, mmRLC_CGTT_MGCG_OVERRIDE, data);
 }
 
 static void gfx_v9_0_update_3d_clock_gating(struct amdgpu_device *adev,
-					   bool enable)
+											bool enable)
 {
 	uint32_t data, def;
 
 	if (!adev->gfx.num_gfx_rings)
 		return;
 
-	/* Enable 3D CGCG/CGLS */
 	if (enable) {
-		/* write cmd to clear cgcg/cgls ov */
 		def = data = RREG32_SOC15(GC, 0, mmRLC_CGTT_MGCG_OVERRIDE);
-		/* unset CGCG override */
 		data &= ~RLC_CGTT_MGCG_OVERRIDE__GFXIP_GFX3D_CG_OVERRIDE_MASK;
-		/* update CGCG and CGLS override bits */
 		if (def != data)
 			WREG32_SOC15(GC, 0, mmRLC_CGTT_MGCG_OVERRIDE, data);
 
-		/* enable 3Dcgcg FSM(0x0000363f) */
 		def = RREG32_SOC15(GC, 0, mmRLC_CGCG_CGLS_CTRL_3D);
 
 		if (adev->cg_flags & AMD_CG_SUPPORT_GFX_3D_CGCG)
 			data = (0x36 << RLC_CGCG_CGLS_CTRL_3D__CGCG_GFX_IDLE_THRESHOLD__SHIFT) |
-				RLC_CGCG_CGLS_CTRL_3D__CGCG_EN_MASK;
+			RLC_CGCG_CGLS_CTRL_3D__CGCG_EN_MASK;
 		else
 			data = 0x0 << RLC_CGCG_CGLS_CTRL_3D__CGCG_GFX_IDLE_THRESHOLD__SHIFT;
 
 		if (adev->cg_flags & AMD_CG_SUPPORT_GFX_3D_CGLS)
 			data |= (0x000F << RLC_CGCG_CGLS_CTRL_3D__CGLS_REP_COMPANSAT_DELAY__SHIFT) |
-				RLC_CGCG_CGLS_CTRL_3D__CGLS_EN_MASK;
+			RLC_CGCG_CGLS_CTRL_3D__CGLS_EN_MASK;
 		if (def != data)
 			WREG32_SOC15(GC, 0, mmRLC_CGCG_CGLS_CTRL_3D, data);
 
-		/* set IDLE_POLL_COUNT(0x00900100) */
 		def = RREG32_SOC15(GC, 0, mmCP_RB_WPTR_POLL_CNTL);
 		data = (0x0100 << CP_RB_WPTR_POLL_CNTL__POLL_FREQUENCY__SHIFT) |
-			(0x0090 << CP_RB_WPTR_POLL_CNTL__IDLE_POLL_COUNT__SHIFT);
+		(0x0090 << CP_RB_WPTR_POLL_CNTL__IDLE_POLL_COUNT__SHIFT);
 		if (def != data)
 			WREG32_SOC15(GC, 0, mmCP_RB_WPTR_POLL_CNTL, data);
 	} else {
-		/* Disable CGCG/CGLS */
 		def = data = RREG32_SOC15(GC, 0, mmRLC_CGCG_CGLS_CTRL_3D);
-		/* disable cgcg, cgls should be disabled */
 		data &= ~(RLC_CGCG_CGLS_CTRL_3D__CGCG_EN_MASK |
-			  RLC_CGCG_CGLS_CTRL_3D__CGLS_EN_MASK);
-		/* disable cgcg and cgls in FSM */
+		RLC_CGCG_CGLS_CTRL_3D__CGLS_EN_MASK);
 		if (def != data)
 			WREG32_SOC15(GC, 0, mmRLC_CGCG_CGLS_CTRL_3D, data);
 	}
 }
 
 static void gfx_v9_0_update_coarse_grain_clock_gating(struct amdgpu_device *adev,
-						      bool enable)
+													  bool enable)
 {
 	uint32_t def, data;
 
 	if (enable && (adev->cg_flags & AMD_CG_SUPPORT_GFX_CGCG)) {
 		def = data = RREG32_SOC15(GC, 0, mmRLC_CGTT_MGCG_OVERRIDE);
-		/* unset CGCG override */
 		data &= ~RLC_CGTT_MGCG_OVERRIDE__GFXIP_CGCG_OVERRIDE_MASK;
 		if (adev->cg_flags & AMD_CG_SUPPORT_GFX_CGLS)
 			data &= ~RLC_CGTT_MGCG_OVERRIDE__GFXIP_CGLS_OVERRIDE_MASK;
 		else
 			data |= RLC_CGTT_MGCG_OVERRIDE__GFXIP_CGLS_OVERRIDE_MASK;
-		/* update CGCG and CGLS override bits */
 		if (def != data)
 			WREG32_SOC15(GC, 0, mmRLC_CGTT_MGCG_OVERRIDE, data);
 
-		/* enable cgcg FSM(0x0000363F) */
 		def = RREG32_SOC15(GC, 0, mmRLC_CGCG_CGLS_CTRL);
 
 		if (amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 1))
 			data = (0x2000 << RLC_CGCG_CGLS_CTRL__CGCG_GFX_IDLE_THRESHOLD__SHIFT) |
-				RLC_CGCG_CGLS_CTRL__CGCG_EN_MASK;
+			RLC_CGCG_CGLS_CTRL__CGCG_EN_MASK;
 		else
 			data = (0x36 << RLC_CGCG_CGLS_CTRL__CGCG_GFX_IDLE_THRESHOLD__SHIFT) |
-				RLC_CGCG_CGLS_CTRL__CGCG_EN_MASK;
+			RLC_CGCG_CGLS_CTRL__CGCG_EN_MASK;
 		if (adev->cg_flags & AMD_CG_SUPPORT_GFX_CGLS)
 			data |= (0x000F << RLC_CGCG_CGLS_CTRL__CGLS_REP_COMPANSAT_DELAY__SHIFT) |
-				RLC_CGCG_CGLS_CTRL__CGLS_EN_MASK;
+			RLC_CGCG_CGLS_CTRL__CGLS_EN_MASK;
 		if (def != data)
 			WREG32_SOC15(GC, 0, mmRLC_CGCG_CGLS_CTRL, data);
 
-		/* set IDLE_POLL_COUNT(0x00900100) */
 		def = RREG32_SOC15(GC, 0, mmCP_RB_WPTR_POLL_CNTL);
 		data = (0x0100 << CP_RB_WPTR_POLL_CNTL__POLL_FREQUENCY__SHIFT) |
-			(0x0090 << CP_RB_WPTR_POLL_CNTL__IDLE_POLL_COUNT__SHIFT);
+		(0x0090 << CP_RB_WPTR_POLL_CNTL__IDLE_POLL_COUNT__SHIFT);
 		if (def != data)
 			WREG32_SOC15(GC, 0, mmCP_RB_WPTR_POLL_CNTL, data);
 	} else {
 		def = data = RREG32_SOC15(GC, 0, mmRLC_CGCG_CGLS_CTRL);
-		/* reset CGCG/CGLS bits */
 		data &= ~(RLC_CGCG_CGLS_CTRL__CGCG_EN_MASK | RLC_CGCG_CGLS_CTRL__CGLS_EN_MASK);
-		/* disable cgcg and cgls in FSM */
 		if (def != data)
 			WREG32_SOC15(GC, 0, mmRLC_CGCG_CGLS_CTRL, data);
 	}
 }
 
-static int gfx_v9_0_update_gfx_clock_gating(struct amdgpu_device *adev,
-					    bool enable)
-{
-	amdgpu_gfx_rlc_enter_safe_mode(adev, 0);
-	if (enable) {
-		/* CGCG/CGLS should be enabled after MGCG/MGLS
-		 * ===  MGCG + MGLS ===
-		 */
-		gfx_v9_0_update_medium_grain_clock_gating(adev, enable);
-		/* ===  CGCG /CGLS for GFX 3D Only === */
-		gfx_v9_0_update_3d_clock_gating(adev, enable);
-		/* ===  CGCG + CGLS === */
-		gfx_v9_0_update_coarse_grain_clock_gating(adev, enable);
-	} else {
-		/* CGCG/CGLS should be disabled before MGCG/MGLS
-		 * ===  CGCG + CGLS ===
-		 */
-		gfx_v9_0_update_coarse_grain_clock_gating(adev, enable);
-		/* ===  CGCG /CGLS for GFX 3D Only === */
-		gfx_v9_0_update_3d_clock_gating(adev, enable);
-		/* ===  MGCG + MGLS === */
-		gfx_v9_0_update_medium_grain_clock_gating(adev, enable);
-	}
-	amdgpu_gfx_rlc_exit_safe_mode(adev, 0);
-	return 0;
-}
-
 static void gfx_v9_0_update_spm_vmid_internal(struct amdgpu_device *adev,
 					      unsigned int vmid)
 {
@@ -5221,18 +5221,23 @@ static const struct amdgpu_rlc_funcs gfx
 };
 
 static int gfx_v9_0_set_powergating_state(struct amdgpu_ip_block *ip_block,
-					  enum amd_powergating_state state)
+										  enum amd_powergating_state state)
 {
 	struct amdgpu_device *adev = ip_block->adev;
 	bool enable = (state == AMD_PG_STATE_GATE);
 
+	if (amdgpu_sriov_vf(adev))
+		return 0;
+
 	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
-	case IP_VERSION(9, 2, 2):
-	case IP_VERSION(9, 1, 0):
-	case IP_VERSION(9, 3, 0):
-		if (!enable)
-			amdgpu_gfx_off_ctrl_immediate(adev, false);
+		case IP_VERSION(9, 2, 2):
+		case IP_VERSION(9, 1, 0):
+		case IP_VERSION(9, 3, 0):
+			/* Ungate: Disable GFXOFF before making changes */
+			if (!enable)
+				amdgpu_gfx_off_ctrl(adev, false);
 
+		/* Configure RLC SMU handshake for power gating */
 		if (adev->pg_flags & AMD_PG_SUPPORT_RLC_SMU_HS) {
 			gfx_v9_0_enable_sck_slow_down_on_power_up(adev, true);
 			gfx_v9_0_enable_sck_slow_down_on_power_down(adev, true);
@@ -5241,52 +5246,76 @@ static int gfx_v9_0_set_powergating_stat
 			gfx_v9_0_enable_sck_slow_down_on_power_down(adev, false);
 		}
 
+		/* Configure CP power gating */
 		if (adev->pg_flags & AMD_PG_SUPPORT_CP)
 			gfx_v9_0_enable_cp_power_gating(adev, true);
 		else
 			gfx_v9_0_enable_cp_power_gating(adev, false);
 
-		/* update gfx cgpg state */
 		gfx_v9_0_update_gfx_cg_power_gating(adev, enable);
-
-		/* update mgcg state */
 		gfx_v9_0_update_gfx_mg_power_gating(adev, enable);
 
+		/* Gate: Re-enable GFXOFF after changes */
 		if (enable)
-			amdgpu_gfx_off_ctrl_immediate(adev, true);
-		break;
-	case IP_VERSION(9, 2, 1):
-		amdgpu_gfx_off_ctrl_immediate(adev, enable);
-		break;
-	default:
+			amdgpu_gfx_off_ctrl(adev, true);
+
+		/* Invalidate GRBM cache after any power state change */
+		gfx_v9_0_grbm_state_invalidate(adev);
 		break;
+
+		case IP_VERSION(9, 2, 1):
+			amdgpu_gfx_off_ctrl(adev, enable);
+			break;
+
+		case IP_VERSION(9, 0, 1):
+		case IP_VERSION(9, 4, 0):
+			if (adev->pg_flags & (AMD_PG_SUPPORT_GFX_PG |
+				AMD_PG_SUPPORT_GFX_SMG |
+				AMD_PG_SUPPORT_GFX_DMG)) {
+				gfx_v9_0_update_gfx_cg_power_gating(adev, enable);
+			gfx_v9_0_update_gfx_mg_power_gating(adev, enable);
+				}
+				break;
+
+		default:
+			break;
 	}
 
 	return 0;
 }
 
 static int gfx_v9_0_set_clockgating_state(struct amdgpu_ip_block *ip_block,
-					  enum amd_clockgating_state state)
+										  enum amd_clockgating_state state)
 {
 	struct amdgpu_device *adev = ip_block->adev;
+	bool enable = (state == AMD_CG_STATE_GATE);
 
 	if (amdgpu_sriov_vf(adev))
 		return 0;
 
 	switch (amdgpu_ip_version(adev, GC_HWIP, 0)) {
-	case IP_VERSION(9, 0, 1):
-	case IP_VERSION(9, 2, 1):
-	case IP_VERSION(9, 4, 0):
-	case IP_VERSION(9, 2, 2):
-	case IP_VERSION(9, 1, 0):
-	case IP_VERSION(9, 4, 1):
-	case IP_VERSION(9, 3, 0):
-	case IP_VERSION(9, 4, 2):
-		gfx_v9_0_update_gfx_clock_gating(adev,
-						 state == AMD_CG_STATE_GATE);
-		break;
-	default:
-		break;
+		case IP_VERSION(9, 0, 1):
+		case IP_VERSION(9, 2, 1):
+		case IP_VERSION(9, 4, 0):
+		case IP_VERSION(9, 2, 2):
+		case IP_VERSION(9, 1, 0):
+		case IP_VERSION(9, 4, 1):
+		case IP_VERSION(9, 3, 0):
+		case IP_VERSION(9, 4, 2):
+			amdgpu_gfx_rlc_enter_safe_mode(adev, 0);
+			if (enable) {
+				gfx_v9_0_update_medium_grain_clock_gating(adev, true);
+				gfx_v9_0_update_3d_clock_gating(adev, true);
+				gfx_v9_0_update_coarse_grain_clock_gating(adev, true);
+			} else {
+				gfx_v9_0_update_coarse_grain_clock_gating(adev, false);
+				gfx_v9_0_update_3d_clock_gating(adev, false);
+				gfx_v9_0_update_medium_grain_clock_gating(adev, false);
+			}
+			amdgpu_gfx_rlc_exit_safe_mode(adev, 0);
+			break;
+		default:
+			break;
 	}
 	return 0;
 }
@@ -5551,68 +5580,70 @@ static void gfx_v9_0_ring_emit_ib_comput
 	amdgpu_ring_write(ring, control);
 }
 
-static void gfx_v9_0_ring_emit_fence(struct amdgpu_ring *ring, u64 addr,
-				     u64 seq, unsigned flags)
+static void gfx_v9_0_ring_emit_fence(struct amdgpu_ring *ring,
+									 u64 addr, u64 seq, unsigned flags)
 {
-	bool write64bit = flags & AMDGPU_FENCE_FLAG_64BIT;
-	bool int_sel = flags & AMDGPU_FENCE_FLAG_INT;
-	bool writeback = flags & AMDGPU_FENCE_FLAG_TC_WB_ONLY;
-	bool exec = flags & AMDGPU_FENCE_FLAG_EXEC;
-	uint32_t dw2 = 0;
+	const bool wb_only  = flags & AMDGPU_FENCE_FLAG_TC_WB_ONLY;
+	const bool write64  = flags & AMDGPU_FENCE_FLAG_64BIT;
+	const bool int_sel  = flags & AMDGPU_FENCE_FLAG_INT;
+	const bool exec_tag = flags & AMDGPU_FENCE_FLAG_EXEC;
+	u32 event_dw1 = 0;
+
+	BUG_ON(write64 ? (addr & 0x7) : (addr & 0x3));
 
-	/* RELEASE_MEM - flush caches, send int */
 	amdgpu_ring_write(ring, PACKET3(PACKET3_RELEASE_MEM, 6));
 
-	if (writeback) {
-		dw2 = EOP_TC_NC_ACTION_EN;
-	} else {
-		dw2 = EOP_TCL1_ACTION_EN | EOP_TC_ACTION_EN |
-				EOP_TC_MD_ACTION_EN;
-	}
-	dw2 |= EOP_TC_WB_ACTION_EN | EVENT_TYPE(CACHE_FLUSH_AND_INV_TS_EVENT) |
-				EVENT_INDEX(5);
-	if (exec)
-		dw2 |= EOP_EXEC;
+	if (wb_only)
+		event_dw1 |= EOP_TC_NC_ACTION_EN;
+	else
+		event_dw1 |= EOP_TCL1_ACTION_EN | EOP_TC_ACTION_EN |
+		EOP_TC_MD_ACTION_EN;
 
-	amdgpu_ring_write(ring, dw2);
-	amdgpu_ring_write(ring, DATA_SEL(write64bit ? 2 : 1) | INT_SEL(int_sel ? 2 : 0));
+	event_dw1 |= EOP_TC_WB_ACTION_EN |
+	EVENT_TYPE(CACHE_FLUSH_AND_INV_TS_EVENT) |
+	EVENT_INDEX(5);
+
+	if (exec_tag)
+		event_dw1 |= EOP_EXEC;
+
+	amdgpu_ring_write(ring, event_dw1);
+	amdgpu_ring_write(ring, DATA_SEL(write64 ? 2 : 1) |
+	INT_SEL(int_sel ? 2 : 0));
 
-	/*
-	 * the address should be Qword aligned if 64bit write, Dword
-	 * aligned if only send 32bit data low (discard data high)
-	 */
-	if (write64bit)
-		BUG_ON(addr & 0x7);
-	else
-		BUG_ON(addr & 0x3);
 	amdgpu_ring_write(ring, lower_32_bits(addr));
 	amdgpu_ring_write(ring, upper_32_bits(addr));
 	amdgpu_ring_write(ring, lower_32_bits(seq));
 	amdgpu_ring_write(ring, upper_32_bits(seq));
 	amdgpu_ring_write(ring, 0);
+
+	if (!wb_only)
+		dma_wmb();
 }
 
 static void gfx_v9_0_ring_emit_pipeline_sync(struct amdgpu_ring *ring)
 {
-	int usepfp = (ring->funcs->type == AMDGPU_RING_TYPE_GFX);
-	uint32_t seq = ring->fence_drv.sync_seq;
-	uint64_t addr = ring->fence_drv.gpu_addr;
-
-	gfx_v9_0_wait_reg_mem(ring, usepfp, 1, 0,
-			      lower_32_bits(addr), upper_32_bits(addr),
-			      seq, 0xffffffff, 4);
+	const bool use_pfp = (ring->funcs->type == AMDGPU_RING_TYPE_GFX);
+	u64  addr = ring->fence_drv.gpu_addr;
+	u32  seq  = ring->fence_drv.sync_seq;
+
+	gfx_v9_0_wait_reg_mem(ring,
+						  use_pfp,  /* engine: PFP for GFX, ME for others */
+					   1,        /* mem space */
+					   0,        /* wait_eq */
+					   lower_32_bits(addr),
+						  upper_32_bits(addr),
+						  seq, 0xffffffff, 4);
 }
 
 static void gfx_v9_0_ring_emit_vm_flush(struct amdgpu_ring *ring,
-					unsigned vmid, uint64_t pd_addr)
+										unsigned vmid, u64 pd_addr)
 {
 	amdgpu_gmc_emit_flush_gpu_tlb(ring, vmid, pd_addr);
 
-	/* compute doesn't have PFP */
+	/* compute rings have no PFP, so sync only on GFX ring */
 	if (ring->funcs->type == AMDGPU_RING_TYPE_GFX) {
-		/* sync PFP to ME, otherwise we might get invalid PFP reads */
 		amdgpu_ring_write(ring, PACKET3(PACKET3_PFP_SYNC_ME, 0));
-		amdgpu_ring_write(ring, 0x0);
+		amdgpu_ring_write(ring, 0);
 	}
 }
 
@@ -7136,61 +7167,44 @@ static void gfx_v9_0_emit_mem_sync(struc
 }
 
 static void gfx_v9_0_emit_wave_limit_cs(struct amdgpu_ring *ring,
-					uint32_t pipe, bool enable)
+										u32 pipe, bool enable)
 {
 	struct amdgpu_device *adev = ring->adev;
-	uint32_t val;
-	uint32_t wcl_cs_reg;
+	u32 reg;
 
-	/* mmSPI_WCL_PIPE_PERCENT_CS[0-7]_DEFAULT values are same */
-	val = enable ? 0x1 : mmSPI_WCL_PIPE_PERCENT_CS0_DEFAULT;
+	if (pipe > 3)
+		return;
 
 	switch (pipe) {
-	case 0:
-		wcl_cs_reg = SOC15_REG_OFFSET(GC, 0, mmSPI_WCL_PIPE_PERCENT_CS0);
-		break;
-	case 1:
-		wcl_cs_reg = SOC15_REG_OFFSET(GC, 0, mmSPI_WCL_PIPE_PERCENT_CS1);
-		break;
-	case 2:
-		wcl_cs_reg = SOC15_REG_OFFSET(GC, 0, mmSPI_WCL_PIPE_PERCENT_CS2);
-		break;
-	case 3:
-		wcl_cs_reg = SOC15_REG_OFFSET(GC, 0, mmSPI_WCL_PIPE_PERCENT_CS3);
-		break;
-	default:
-		DRM_DEBUG("invalid pipe %d\n", pipe);
-		return;
+		case 0:
+			reg = SOC15_REG_OFFSET(GC, 0, mmSPI_WCL_PIPE_PERCENT_CS0);
+			break;
+		case 1:
+			reg = SOC15_REG_OFFSET(GC, 0, mmSPI_WCL_PIPE_PERCENT_CS1);
+			break;
+		case 2:
+			reg = SOC15_REG_OFFSET(GC, 0, mmSPI_WCL_PIPE_PERCENT_CS2);
+			break;
+		default:
+			reg = SOC15_REG_OFFSET(GC, 0, mmSPI_WCL_PIPE_PERCENT_CS3);
+			break;
 	}
 
-	amdgpu_ring_emit_wreg(ring, wcl_cs_reg, val);
-
+	WREG32_IF_CHANGED(reg, enable ? 0x1 : mmSPI_WCL_PIPE_PERCENT_CS0_DEFAULT);
 }
+
 static void gfx_v9_0_emit_wave_limit(struct amdgpu_ring *ring, bool enable)
 {
 	struct amdgpu_device *adev = ring->adev;
-	uint32_t val;
-	int i;
-
-
-	/* mmSPI_WCL_PIPE_PERCENT_GFX is 7 bit multiplier register to limit
-	 * number of gfx waves. Setting 5 bit will make sure gfx only gets
-	 * around 25% of gpu resources.
-	 */
-	val = enable ? 0x1f : mmSPI_WCL_PIPE_PERCENT_GFX_DEFAULT;
-	amdgpu_ring_emit_wreg(ring,
-			      SOC15_REG_OFFSET(GC, 0, mmSPI_WCL_PIPE_PERCENT_GFX),
-			      val);
+	u32 reg = SOC15_REG_OFFSET(GC, 0, mmSPI_WCL_PIPE_PERCENT_GFX);
+	u32 p;
 
-	/* Restrict waves for normal/low priority compute queues as well
-	 * to get best QoS for high priority compute jobs.
-	 *
-	 * amdgpu controls only 1st ME(0-3 CS pipes).
-	 */
-	for (i = 0; i < adev->gfx.mec.num_pipe_per_mec; i++) {
-		if (i != ring->pipe)
-			gfx_v9_0_emit_wave_limit_cs(ring, i, enable);
+	WREG32_IF_CHANGED(reg, enable ? 0x1f : mmSPI_WCL_PIPE_PERCENT_GFX_DEFAULT);
 
+	for (p = 0; p < adev->gfx.mec.num_pipe_per_mec; ++p) {
+		if (p != ring->pipe) {
+			gfx_v9_0_emit_wave_limit_cs(ring, p, enable);
+		}
 	}
 }
 
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c	2025-05-29 11:14:09.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c	2025-06-29 11:33:50.388339649 +0200
@@ -53,18 +53,20 @@ void amdgpu_unregister_gpu_instance(stru
 
 	mutex_lock(&mgpu_info.mutex);
 
-	for (i = 0; i < mgpu_info.num_gpu; i++) {
-		gpu_instance = &(mgpu_info.gpu_ins[i]);
-		if (gpu_instance->adev == adev) {
-			mgpu_info.gpu_ins[i] =
-				mgpu_info.gpu_ins[mgpu_info.num_gpu - 1];
-			mgpu_info.num_gpu--;
-			if (adev->flags & AMD_IS_APU)
-				mgpu_info.num_apu--;
-			else
-				mgpu_info.num_dgpu--;
-			break;
-		}
+	for (i = 0; i < mgpu_info.num_gpu; ++i) {
+		gpu_instance = &mgpu_info.gpu_ins[i];
+		if (gpu_instance->adev != adev)
+			continue;
+
+		mgpu_info.gpu_ins[i] =
+			mgpu_info.gpu_ins[mgpu_info.num_gpu - 1];
+		mgpu_info.num_gpu--;
+
+		if (adev->flags & AMD_IS_APU)
+			mgpu_info.num_apu--;
+		else
+			mgpu_info.num_dgpu--;
+		break;
 	}
 
 	mutex_unlock(&mgpu_info.mutex);
@@ -82,12 +84,12 @@ void amdgpu_driver_unload_kms(struct drm
 {
 	struct amdgpu_device *adev = drm_to_adev(dev);
 
-	if (adev == NULL)
+	if (unlikely(!adev))
 		return;
 
 	amdgpu_unregister_gpu_instance(adev);
 
-	if (adev->rmmio == NULL)
+	if (unlikely(adev->rmmio == NULL))
 		return;
 
 	if (amdgpu_acpi_smart_shift_update(dev, AMDGPU_SS_DRV_UNLOAD))
@@ -138,27 +140,17 @@ int amdgpu_driver_load_kms(struct amdgpu
 
 	dev = adev_to_drm(adev);
 
-	/* amdgpu_device_init should report only fatal error
-	 * like memory allocation failure or iomapping failure,
-	 * or memory manager initialization failure, it must
-	 * properly initialize the GPU MC controller and permit
-	 * VRAM allocation
-	 */
 	r = amdgpu_device_init(adev, flags);
 	if (r) {
-		dev_err(dev->dev, "Fatal error during GPU init\n");
+		dev_err(&adev->pdev->dev, "Fatal error during GPU init\n");
 		goto out;
 	}
 
 	amdgpu_device_detect_runtime_pm_mode(adev);
 
-	/* Call ACPI methods: require modeset init
-	 * but failure is not fatal
-	 */
-
 	acpi_status = amdgpu_acpi_init(adev);
 	if (acpi_status)
-		dev_dbg(dev->dev, "Error during ACPI methods call\n");
+		dev_dbg(&adev->pdev->dev, "Error during ACPI methods call\n");
 
 	if (amdgpu_acpi_smart_shift_update(dev, AMDGPU_SS_DRV_LOAD))
 		DRM_WARN("smart shift update failed\n");
@@ -171,14 +163,12 @@ out:
 }
 
 static enum amd_ip_block_type
-	amdgpu_ip_get_block_type(struct amdgpu_device *adev, uint32_t ip)
+amdgpu_ip_get_block_type(struct amdgpu_device *adev, uint32_t ip)
 {
 	enum amd_ip_block_type type;
 
 	switch (ip) {
 	case AMDGPU_HW_IP_GFX:
-		type = AMD_IP_BLOCK_TYPE_GFX;
-		break;
 	case AMDGPU_HW_IP_COMPUTE:
 		type = AMD_IP_BLOCK_TYPE_GFX;
 		break;
@@ -198,7 +188,8 @@ static enum amd_ip_block_type
 		break;
 	case AMDGPU_HW_IP_VCN_JPEG:
 		type = (amdgpu_device_ip_get_ip_block(adev, AMD_IP_BLOCK_TYPE_JPEG)) ?
-				   AMD_IP_BLOCK_TYPE_JPEG : AMD_IP_BLOCK_TYPE_VCN;
+		       AMD_IP_BLOCK_TYPE_JPEG :
+		       AMD_IP_BLOCK_TYPE_VCN;
 		break;
 	default:
 		type = AMD_IP_BLOCK_TYPE_NUM;
@@ -272,8 +263,9 @@ static int amdgpu_firmware_info(struct d
 		} else if (query_fw->index == 1) {
 			fw_info->ver = adev->gfx.mec2_fw_version;
 			fw_info->feature = adev->gfx.mec2_feature_version;
-		} else
+		} else {
 			return -EINVAL;
+		}
 		break;
 	case AMDGPU_INFO_FW_SMC:
 		fw_info->ver = adev->pm.fw_version;
@@ -348,13 +340,13 @@ static int amdgpu_firmware_info(struct d
 		break;
 	case AMDGPU_INFO_FW_MES_KIQ:
 		fw_info->ver = adev->mes.kiq_version & AMDGPU_MES_VERSION_MASK;
-		fw_info->feature = (adev->mes.kiq_version & AMDGPU_MES_FEAT_VERSION_MASK)
-					>> AMDGPU_MES_FEAT_VERSION_SHIFT;
+		fw_info->feature = (adev->mes.kiq_version & AMDGPU_MES_FEAT_VERSION_MASK) >>
+				   AMDGPU_MES_FEAT_VERSION_SHIFT;
 		break;
 	case AMDGPU_INFO_FW_MES:
 		fw_info->ver = adev->mes.sched_version & AMDGPU_MES_VERSION_MASK;
-		fw_info->feature = (adev->mes.sched_version & AMDGPU_MES_FEAT_VERSION_MASK)
-					>> AMDGPU_MES_FEAT_VERSION_SHIFT;
+		fw_info->feature = (adev->mes.sched_version & AMDGPU_MES_FEAT_VERSION_MASK) >>
+				   AMDGPU_MES_FEAT_VERSION_SHIFT;
 		break;
 	case AMDGPU_INFO_FW_IMU:
 		fw_info->ver = adev->gfx.imu_fw_version;
@@ -468,7 +460,8 @@ static int amdgpu_hw_ip_info(struct amdg
 		break;
 	case AMDGPU_HW_IP_VCN_JPEG:
 		type = (amdgpu_device_ip_get_ip_block(adev, AMD_IP_BLOCK_TYPE_JPEG)) ?
-			AMD_IP_BLOCK_TYPE_JPEG : AMD_IP_BLOCK_TYPE_VCN;
+		       AMD_IP_BLOCK_TYPE_JPEG :
+		       AMD_IP_BLOCK_TYPE_VCN;
 
 		for (i = 0; i < adev->jpeg.num_jpeg_inst; i++) {
 			if (adev->jpeg.harvest_config & (1 << i))
@@ -497,7 +490,8 @@ static int amdgpu_hw_ip_info(struct amdg
 		    adev->ip_blocks[i].status.valid)
 			break;
 
-	if (i == adev->num_ip_blocks)
+	/* Branch Prediction: Hint that this failure path is highly unlikely. */
+	if (unlikely(i == adev->num_ip_blocks))
 		return 0;
 
 	num_rings = min(amdgpu_ctx_num_entities[info->query_hw_ip.type],
@@ -544,9 +538,80 @@ static int amdgpu_hw_ip_info(struct amdg
 	return 0;
 }
 
-/*
- * Userspace get information ioctl
- */
+/* Use stack allocation for small MMR reads to avoid kmalloc overhead. */
+#define MMR_READ_STACK_THRESHOLD 8
+
+static int amdgpu_ioctl_read_mmr(struct amdgpu_device *adev,
+								 struct drm_amdgpu_info *info,
+								 void __user *out,
+								 u32 user_size)
+{
+	uint32_t se_idx, sh_idx;
+	size_t bytes;
+	uint32_t stack_regs[MMR_READ_STACK_THRESHOLD];
+	uint32_t *regs = stack_regs;
+	const u32 count = info->read_mmr_reg.count;
+	int i, ret = 0;
+
+	if (!count || count > 128)
+		return -EINVAL;
+
+	if (check_mul_overflow(count, (u32)sizeof(*regs), &bytes))
+		return -E2BIG;
+
+	if (count > MMR_READ_STACK_THRESHOLD) {
+		regs = kvcalloc(count, sizeof(*regs), GFP_KERNEL);
+		if (!regs)
+			return -ENOMEM;
+	}
+
+	se_idx = (info->read_mmr_reg.instance >>
+	AMDGPU_INFO_MMR_SE_INDEX_SHIFT) &
+	AMDGPU_INFO_MMR_SE_INDEX_MASK;
+	sh_idx = (info->read_mmr_reg.instance >>
+	AMDGPU_INFO_MMR_SH_INDEX_SHIFT) &
+	AMDGPU_INFO_MMR_SH_INDEX_MASK;
+
+	if (se_idx == AMDGPU_INFO_MMR_SE_INDEX_MASK)
+		se_idx = 0xffffffff;
+	else if (se_idx >= AMDGPU_GFX_MAX_SE)
+	{ ret = -EINVAL; goto out_free; }
+
+	if (sh_idx == AMDGPU_INFO_MMR_SH_INDEX_MASK)
+		sh_idx = 0xffffffff;
+	else if (sh_idx >= AMDGPU_GFX_MAX_SH_PER_SE)
+	{ ret = -EINVAL; goto out_free; }
+
+	if (!down_read_trylock(&adev->reset_domain->sem)) {
+		/* –EAGAIN => userspace may retry once the GPU reset completes. */
+		ret = -EAGAIN;
+		goto out_free;
+	}
+
+	amdgpu_gfx_off_ctrl(adev, false);
+	for (i = 0; i < count; ++i) {
+		ret = amdgpu_asic_read_register(adev, se_idx, sh_idx,
+										info->read_mmr_reg.dword_offset + i, &regs[i]);
+		if (ret) {
+			DRM_DEBUG_KMS("unallowed offset %#x\n",
+						  info->read_mmr_reg.dword_offset + i);
+			ret = -EFAULT;
+			break;
+		}
+	}
+	amdgpu_gfx_off_ctrl(adev, true);
+	up_read(&adev->reset_domain->sem);
+
+	if (!ret &&
+		copy_to_user(out, regs, min_t(size_t, user_size, bytes)))
+		ret = -EFAULT;
+
+	out_free:
+	if (regs != stack_regs)
+		kfree(regs);
+	return ret;
+}
+
 /**
  * amdgpu_info_ioctl - answer a device specific request.
  *
@@ -570,7 +635,7 @@ int amdgpu_info_ioctl(struct drm_device
 	enum amd_ip_block_type type;
 	struct amdgpu_xcp *xcp;
 	u32 count, inst_mask;
-	uint32_t size = info->return_size;
+	uint32_t user_ret_size = info->return_size;
 	struct drm_crtc *crtc;
 	uint32_t ui32 = 0;
 	uint64_t ui64 = 0;
@@ -583,7 +648,9 @@ int amdgpu_info_ioctl(struct drm_device
 	switch (info->query) {
 	case AMDGPU_INFO_ACCEL_WORKING:
 		ui32 = adev->accel_working;
-		return copy_to_user(out, &ui32, min(size, 4u)) ? -EFAULT : 0;
+		if (copy_to_user(out, &ui32, min(user_ret_size, 4u)))
+			return -EFAULT;
+		return 0;
 	case AMDGPU_INFO_CRTC_FROM_ID:
 		for (i = 0, found = 0; i < adev->mode_info.num_crtc; i++) {
 			crtc = (struct drm_crtc *)minfo->crtcs[i];
@@ -599,7 +666,9 @@ int amdgpu_info_ioctl(struct drm_device
 			DRM_DEBUG_KMS("unknown crtc id %d\n", info->mode_crtc.id);
 			return -EINVAL;
 		}
-		return copy_to_user(out, &ui32, min(size, 4u)) ? -EFAULT : 0;
+		if (copy_to_user(out, &ui32, min(user_ret_size, 4u)))
+			return -EFAULT;
+		return 0;
 	case AMDGPU_INFO_HW_IP_INFO: {
 		struct drm_amdgpu_info_hw_ip ip = {};
 
@@ -607,8 +676,9 @@ int amdgpu_info_ioctl(struct drm_device
 		if (ret)
 			return ret;
 
-		ret = copy_to_user(out, &ip, min_t(size_t, size, sizeof(ip)));
-		return ret ? -EFAULT : 0;
+		if (copy_to_user(out, &ip, min_t(size_t, user_ret_size, sizeof(ip))))
+			return -EFAULT;
+		return 0;
 	}
 	case AMDGPU_INFO_HW_IP_COUNT: {
 		fpriv = (struct amdgpu_fpriv *)filp->driver_priv;
@@ -650,7 +720,9 @@ int amdgpu_info_ioctl(struct drm_device
 				return -EINVAL;
 			}
 
-			return copy_to_user(out, &count, min(size, 4u)) ? -EFAULT : 0;
+			if (copy_to_user(out, &count, min(user_ret_size, 4u)))
+				return -EFAULT;
+			return 0;
 		}
 
 		switch (type) {
@@ -670,23 +742,22 @@ int amdgpu_info_ioctl(struct drm_device
 		case AMD_IP_BLOCK_TYPE_UVD:
 			count = adev->uvd.num_uvd_inst;
 			break;
-		/* For all other IP block types not listed in the switch statement
-		 * the ip status is valid here and the instance count is one.
-		 */
 		default:
 			count = 1;
 			break;
 		}
-
-		return copy_to_user(out, &count, min(size, 4u)) ? -EFAULT : 0;
+		if (copy_to_user(out, &count, min(user_ret_size, 4u)))
+			return -EFAULT;
+		return 0;
 	}
 	case AMDGPU_INFO_TIMESTAMP:
 		ui64 = amdgpu_gfx_get_gpu_clock_counter(adev);
-		return copy_to_user(out, &ui64, min(size, 8u)) ? -EFAULT : 0;
+		if (copy_to_user(out, &ui64, min(user_ret_size, 8u)))
+			return -EFAULT;
+		return 0;
 	case AMDGPU_INFO_FW_VERSION: {
 		struct drm_amdgpu_info_firmware fw_info;
 
-		/* We only support one instance of each IP block right now. */
 		if (info->query_fw.ip_instance != 0)
 			return -EINVAL;
 
@@ -694,27 +765,41 @@ int amdgpu_info_ioctl(struct drm_device
 		if (ret)
 			return ret;
 
-		return copy_to_user(out, &fw_info,
-				    min((size_t)size, sizeof(fw_info))) ? -EFAULT : 0;
+		if (copy_to_user(out, &fw_info,
+				    min_t(size_t, user_ret_size, sizeof(fw_info))))
+			return -EFAULT;
+		return 0;
 	}
 	case AMDGPU_INFO_NUM_BYTES_MOVED:
 		ui64 = atomic64_read(&adev->num_bytes_moved);
-		return copy_to_user(out, &ui64, min(size, 8u)) ? -EFAULT : 0;
+		if (copy_to_user(out, &ui64, min(user_ret_size, 8u)))
+			return -EFAULT;
+		return 0;
 	case AMDGPU_INFO_NUM_EVICTIONS:
 		ui64 = atomic64_read(&adev->num_evictions);
-		return copy_to_user(out, &ui64, min(size, 8u)) ? -EFAULT : 0;
+		if (copy_to_user(out, &ui64, min(user_ret_size, 8u)))
+			return -EFAULT;
+		return 0;
 	case AMDGPU_INFO_NUM_VRAM_CPU_PAGE_FAULTS:
 		ui64 = atomic64_read(&adev->num_vram_cpu_page_faults);
-		return copy_to_user(out, &ui64, min(size, 8u)) ? -EFAULT : 0;
+		if (copy_to_user(out, &ui64, min(user_ret_size, 8u)))
+			return -EFAULT;
+		return 0;
 	case AMDGPU_INFO_VRAM_USAGE:
 		ui64 = ttm_resource_manager_usage(&adev->mman.vram_mgr.manager);
-		return copy_to_user(out, &ui64, min(size, 8u)) ? -EFAULT : 0;
+		if (copy_to_user(out, &ui64, min(user_ret_size, 8u)))
+			return -EFAULT;
+		return 0;
 	case AMDGPU_INFO_VIS_VRAM_USAGE:
 		ui64 = amdgpu_vram_mgr_vis_usage(&adev->mman.vram_mgr);
-		return copy_to_user(out, &ui64, min(size, 8u)) ? -EFAULT : 0;
+		if (copy_to_user(out, &ui64, min(user_ret_size, 8u)))
+			return -EFAULT;
+		return 0;
 	case AMDGPU_INFO_GTT_USAGE:
 		ui64 = ttm_resource_manager_usage(&adev->mman.gtt_mgr.manager);
-		return copy_to_user(out, &ui64, min(size, 8u)) ? -EFAULT : 0;
+		if (copy_to_user(out, &ui64, min(user_ret_size, 8u)))
+			return -EFAULT;
+		return 0;
 	case AMDGPU_INFO_GDS_CONFIG: {
 		struct drm_amdgpu_info_gds gds_info;
 
@@ -723,23 +808,27 @@ int amdgpu_info_ioctl(struct drm_device
 		gds_info.gds_total_size = adev->gds.gds_size;
 		gds_info.gws_per_compute_partition = adev->gds.gws_size;
 		gds_info.oa_per_compute_partition = adev->gds.oa_size;
-		return copy_to_user(out, &gds_info,
-				    min((size_t)size, sizeof(gds_info))) ? -EFAULT : 0;
+		if (copy_to_user(out, &gds_info,
+				    min_t(size_t, user_ret_size, sizeof(gds_info))))
+			return -EFAULT;
+		return 0;
 	}
 	case AMDGPU_INFO_VRAM_GTT: {
 		struct drm_amdgpu_info_vram_gtt vram_gtt;
 
 		vram_gtt.vram_size = adev->gmc.real_vram_size -
-			atomic64_read(&adev->vram_pin_size) -
-			AMDGPU_VM_RESERVED_VRAM;
+				     atomic64_read(&adev->vram_pin_size) -
+				     AMDGPU_VM_RESERVED_VRAM;
 		vram_gtt.vram_cpu_accessible_size =
 			min(adev->gmc.visible_vram_size -
-			    atomic64_read(&adev->visible_pin_size),
+				    atomic64_read(&adev->visible_pin_size),
 			    vram_gtt.vram_size);
 		vram_gtt.gtt_size = ttm_manager_type(&adev->mman.bdev, TTM_PL_TT)->size;
 		vram_gtt.gtt_size -= atomic64_read(&adev->gart_pin_size);
-		return copy_to_user(out, &vram_gtt,
-				    min((size_t)size, sizeof(vram_gtt))) ? -EFAULT : 0;
+		if (copy_to_user(out, &vram_gtt,
+				    min_t(size_t, user_ret_size, sizeof(vram_gtt))))
+			return -EFAULT;
+		return 0;
 	}
 	case AMDGPU_INFO_MEMORY: {
 		struct drm_amdgpu_memory_info mem;
@@ -751,8 +840,8 @@ int amdgpu_info_ioctl(struct drm_device
 		memset(&mem, 0, sizeof(mem));
 		mem.vram.total_heap_size = adev->gmc.real_vram_size;
 		mem.vram.usable_heap_size = adev->gmc.real_vram_size -
-			atomic64_read(&adev->vram_pin_size) -
-			AMDGPU_VM_RESERVED_VRAM;
+					    atomic64_read(&adev->vram_pin_size) -
+					    AMDGPU_VM_RESERVED_VRAM;
 		mem.vram.heap_usage =
 			ttm_resource_manager_usage(vram_man);
 		mem.vram.max_allocation = mem.vram.usable_heap_size * 3 / 4;
@@ -761,7 +850,7 @@ int amdgpu_info_ioctl(struct drm_device
 			adev->gmc.visible_vram_size;
 		mem.cpu_accessible_vram.usable_heap_size =
 			min(adev->gmc.visible_vram_size -
-			    atomic64_read(&adev->visible_pin_size),
+				    atomic64_read(&adev->visible_pin_size),
 			    mem.vram.usable_heap_size);
 		mem.cpu_accessible_vram.heap_usage =
 			amdgpu_vram_mgr_vis_usage(&adev->mman.vram_mgr);
@@ -770,79 +859,16 @@ int amdgpu_info_ioctl(struct drm_device
 
 		mem.gtt.total_heap_size = gtt_man->size;
 		mem.gtt.usable_heap_size = mem.gtt.total_heap_size -
-			atomic64_read(&adev->gart_pin_size);
+					   atomic64_read(&adev->gart_pin_size);
 		mem.gtt.heap_usage = ttm_resource_manager_usage(gtt_man);
 		mem.gtt.max_allocation = mem.gtt.usable_heap_size * 3 / 4;
 
-		return copy_to_user(out, &mem,
-				    min((size_t)size, sizeof(mem)))
-				    ? -EFAULT : 0;
-	}
-	case AMDGPU_INFO_READ_MMR_REG: {
-		int ret = 0;
-		unsigned int n, alloc_size;
-		uint32_t *regs;
-		unsigned int se_num = (info->read_mmr_reg.instance >>
-				   AMDGPU_INFO_MMR_SE_INDEX_SHIFT) &
-				  AMDGPU_INFO_MMR_SE_INDEX_MASK;
-		unsigned int sh_num = (info->read_mmr_reg.instance >>
-				   AMDGPU_INFO_MMR_SH_INDEX_SHIFT) &
-				  AMDGPU_INFO_MMR_SH_INDEX_MASK;
-
-		if (!down_read_trylock(&adev->reset_domain->sem))
-			return -ENOENT;
-
-		/* set full masks if the userspace set all bits
-		 * in the bitfields
-		 */
-		if (se_num == AMDGPU_INFO_MMR_SE_INDEX_MASK) {
-			se_num = 0xffffffff;
-		} else if (se_num >= AMDGPU_GFX_MAX_SE) {
-			ret = -EINVAL;
-			goto out;
-		}
-
-		if (sh_num == AMDGPU_INFO_MMR_SH_INDEX_MASK) {
-			sh_num = 0xffffffff;
-		} else if (sh_num >= AMDGPU_GFX_MAX_SH_PER_SE) {
-			ret = -EINVAL;
-			goto out;
-		}
-
-		if (info->read_mmr_reg.count > 128) {
-			ret = -EINVAL;
-			goto out;
-		}
-
-		regs = kmalloc_array(info->read_mmr_reg.count, sizeof(*regs), GFP_KERNEL);
-		if (!regs) {
-			ret = -ENOMEM;
-			goto out;
-		}
-
-		alloc_size = info->read_mmr_reg.count * sizeof(*regs);
-
-		amdgpu_gfx_off_ctrl(adev, false);
-		for (i = 0; i < info->read_mmr_reg.count; i++) {
-			if (amdgpu_asic_read_register(adev, se_num, sh_num,
-						      info->read_mmr_reg.dword_offset + i,
-						      &regs[i])) {
-				DRM_DEBUG_KMS("unallowed offset %#x\n",
-					      info->read_mmr_reg.dword_offset + i);
-				kfree(regs);
-				amdgpu_gfx_off_ctrl(adev, true);
-				ret = -EFAULT;
-				goto out;
-			}
-		}
-		amdgpu_gfx_off_ctrl(adev, true);
-		n = copy_to_user(out, regs, min(size, alloc_size));
-		kfree(regs);
-		ret = (n ? -EFAULT : 0);
-out:
-		up_read(&adev->reset_domain->sem);
-		return ret;
+		if (copy_to_user(out, &mem, min_t(size_t, user_ret_size, sizeof(mem))))
+			return -EFAULT;
+		return 0;
 	}
+	case AMDGPU_INFO_READ_MMR_REG:
+		return amdgpu_ioctl_read_mmr(adev, info, out, user_ret_size);
 	case AMDGPU_INFO_DEV_INFO: {
 		struct drm_amdgpu_info_device *dev_info;
 		uint64_t vm_size;
@@ -859,7 +885,6 @@ out:
 		dev_info->family = adev->family;
 		dev_info->num_shader_engines = adev->gfx.config.max_shader_engines;
 		dev_info->num_shader_arrays_per_engine = adev->gfx.config.max_sh_per_se;
-		/* return all clocks in KHz */
 		dev_info->gpu_counter_freq = amdgpu_asic_get_xclk(adev) * 10;
 		if (adev->pm.dpm_enabled) {
 			dev_info->max_engine_clock = amdgpu_dpm_get_sclk(adev, false) * 10;
@@ -876,7 +901,7 @@ out:
 		}
 		dev_info->enabled_rb_pipes_mask = adev->gfx.config.backend_enable_mask;
 		dev_info->num_rb_pipes = adev->gfx.config.max_backends_per_se *
-			adev->gfx.config.max_shader_engines;
+					 adev->gfx.config.max_shader_engines;
 		dev_info->num_hw_gfx_contexts = adev->gfx.config.max_hw_contexts;
 		dev_info->ids_flags = 0;
 		if (adev->flags & AMD_IS_APU)
@@ -891,16 +916,15 @@ out:
 		if (amdgpu_passthrough(adev))
 			dev_info->ids_flags |= (AMDGPU_IDS_FLAGS_MODE_PT <<
 						AMDGPU_IDS_FLAGS_MODE_SHIFT) &
-						AMDGPU_IDS_FLAGS_MODE_MASK;
+					       AMDGPU_IDS_FLAGS_MODE_MASK;
 		else if (amdgpu_sriov_vf(adev))
 			dev_info->ids_flags |= (AMDGPU_IDS_FLAGS_MODE_VF <<
 						AMDGPU_IDS_FLAGS_MODE_SHIFT) &
-						AMDGPU_IDS_FLAGS_MODE_MASK;
+					       AMDGPU_IDS_FLAGS_MODE_MASK;
 
 		vm_size = adev->vm_manager.max_pfn * AMDGPU_GPU_PAGE_SIZE;
 		vm_size -= AMDGPU_VA_RESERVED_TOP;
 
-		/* Older VCE FW versions are buggy and can handle only 40bits */
 		if (adev->vce.fw_version &&
 		    adev->vce.fw_version < AMDGPU_VCE_FW_53_45)
 			vm_size = min(vm_size, 1ULL << 40);
@@ -942,11 +966,10 @@ out:
 
 		dev_info->tcc_disabled_mask = adev->gfx.config.tcc_disabled_mask;
 
-		/* Combine the chip gen mask with the platform (CPU/mobo) mask. */
 		pcie_gen_mask = adev->pm.pcie_gen_mask &
-			(adev->pm.pcie_gen_mask >> CAIL_PCIE_LINK_SPEED_SUPPORT_SHIFT);
+				(adev->pm.pcie_gen_mask >> CAIL_PCIE_LINK_SPEED_SUPPORT_SHIFT);
 		pcie_width_mask = adev->pm.pcie_mlw_mask &
-			(adev->pm.pcie_mlw_mask >> CAIL_PCIE_LINK_WIDTH_SUPPORT_SHIFT);
+				  (adev->pm.pcie_mlw_mask >> CAIL_PCIE_LINK_WIDTH_SUPPORT_SHIFT);
 		dev_info->pcie_gen = fls(pcie_gen_mask);
 		dev_info->pcie_num_lanes =
 			pcie_width_mask & CAIL_ASIC_PCIE_LINK_WIDTH_SUPPORT_X32 ? 32 :
@@ -979,12 +1002,12 @@ out:
 		}
 
 		ret = copy_to_user(out, dev_info,
-				   min((size_t)size, sizeof(*dev_info))) ? -EFAULT : 0;
+				   min_t(size_t, user_ret_size, sizeof(*dev_info))) ? -EFAULT :
+										    0;
 		kfree(dev_info);
 		return ret;
 	}
 	case AMDGPU_INFO_VCE_CLOCK_TABLE: {
-		unsigned int i;
 		struct drm_amdgpu_info_vce_clock_table vce_clk_table = {};
 		struct amd_vce_state *vce_state;
 
@@ -998,17 +1021,20 @@ out:
 			}
 		}
 
-		return copy_to_user(out, &vce_clk_table,
-				    min((size_t)size, sizeof(vce_clk_table))) ? -EFAULT : 0;
+		if (copy_to_user(out, &vce_clk_table,
+				    min_t(size_t, user_ret_size, sizeof(vce_clk_table))))
+			return -EFAULT;
+		return 0;
 	}
 	case AMDGPU_INFO_VBIOS: {
 		uint32_t bios_size = adev->bios_size;
 
 		switch (info->vbios_info.type) {
 		case AMDGPU_INFO_VBIOS_SIZE:
-			return copy_to_user(out, &bios_size,
-					min((size_t)size, sizeof(bios_size)))
-					? -EFAULT : 0;
+			if (copy_to_user(out, &bios_size,
+					    min_t(size_t, user_ret_size, sizeof(bios_size))))
+				return -EFAULT;
+			return 0;
 		case AMDGPU_INFO_VBIOS_IMAGE: {
 			uint8_t *bios;
 			uint32_t bios_offset = info->vbios_info.offset;
@@ -1017,9 +1043,11 @@ out:
 				return -EINVAL;
 
 			bios = adev->bios + bios_offset;
-			return copy_to_user(out, bios,
-					    min((size_t)size, (size_t)(bios_size - bios_offset)))
-					? -EFAULT : 0;
+			if (copy_to_user(out, bios,
+					    min_t(size_t, user_ret_size,
+						  (size_t)(bios_size - bios_offset))))
+				return -EFAULT;
+			return 0;
 		}
 		case AMDGPU_INFO_VBIOS_INFO: {
 			struct drm_amdgpu_info_vbios vbios_info = {};
@@ -1038,31 +1066,38 @@ out:
 				       sizeof(atom_context->date));
 			}
 
-			return copy_to_user(out, &vbios_info,
-						min((size_t)size, sizeof(vbios_info))) ? -EFAULT : 0;
+			if (copy_to_user(out, &vbios_info,
+					    min_t(size_t, user_ret_size, sizeof(vbios_info))))
+				return -EFAULT;
+			return 0;
 		}
 		default:
 			DRM_DEBUG_KMS("Invalid request %d\n",
-					info->vbios_info.type);
+				      info->vbios_info.type);
 			return -EINVAL;
 		}
 	}
 	case AMDGPU_INFO_NUM_HANDLES: {
 		struct drm_amdgpu_info_num_handles handle;
 
+		/* Per-IOCTL IP Block Caching: New optimization */
+		ip_block = amdgpu_device_ip_get_ip_block(adev, AMD_IP_BLOCK_TYPE_UVD);
+		if (!ip_block || !ip_block->status.valid)
+			return -EINVAL;
+
 		switch (info->query_hw_ip.type) {
 		case AMDGPU_HW_IP_UVD:
-			/* Starting Polaris, we support unlimited UVD handles */
 			if (adev->asic_type < CHIP_POLARIS10) {
 				handle.uvd_max_handles = adev->uvd.max_handles;
 				handle.uvd_used_handles = amdgpu_uvd_used_handles(adev);
 
-				return copy_to_user(out, &handle,
-					min((size_t)size, sizeof(handle))) ? -EFAULT : 0;
+				if (copy_to_user(out, &handle,
+						    min_t(size_t, user_ret_size, sizeof(handle))))
+					return -EFAULT;
+				return 0;
 			} else {
 				return -ENODATA;
 			}
-
 			break;
 		default:
 			return -EINVAL;
@@ -1074,112 +1109,87 @@ out:
 
 		switch (info->sensor_info.type) {
 		case AMDGPU_INFO_SENSOR_GFX_SCLK:
-			/* get sclk in Mhz */
 			if (amdgpu_dpm_read_sensor(adev,
 						   AMDGPU_PP_SENSOR_GFX_SCLK,
-						   (void *)&ui32, &ui32_size)) {
+						   (void *)&ui32, &ui32_size))
 				return -EINVAL;
-			}
 			ui32 /= 100;
 			break;
 		case AMDGPU_INFO_SENSOR_GFX_MCLK:
-			/* get mclk in Mhz */
 			if (amdgpu_dpm_read_sensor(adev,
 						   AMDGPU_PP_SENSOR_GFX_MCLK,
-						   (void *)&ui32, &ui32_size)) {
+						   (void *)&ui32, &ui32_size))
 				return -EINVAL;
-			}
 			ui32 /= 100;
 			break;
 		case AMDGPU_INFO_SENSOR_GPU_TEMP:
-			/* get temperature in millidegrees C */
 			if (amdgpu_dpm_read_sensor(adev,
 						   AMDGPU_PP_SENSOR_GPU_TEMP,
-						   (void *)&ui32, &ui32_size)) {
+						   (void *)&ui32, &ui32_size))
 				return -EINVAL;
-			}
 			break;
 		case AMDGPU_INFO_SENSOR_GPU_LOAD:
-			/* get GPU load */
 			if (amdgpu_dpm_read_sensor(adev,
 						   AMDGPU_PP_SENSOR_GPU_LOAD,
-						   (void *)&ui32, &ui32_size)) {
+						   (void *)&ui32, &ui32_size))
 				return -EINVAL;
-			}
 			break;
 		case AMDGPU_INFO_SENSOR_GPU_AVG_POWER:
-			/* get average GPU power */
 			if (amdgpu_dpm_read_sensor(adev,
 						   AMDGPU_PP_SENSOR_GPU_AVG_POWER,
 						   (void *)&ui32, &ui32_size)) {
-				/* fall back to input power for backwards compat */
 				if (amdgpu_dpm_read_sensor(adev,
 							   AMDGPU_PP_SENSOR_GPU_INPUT_POWER,
-							   (void *)&ui32, &ui32_size)) {
+							   (void *)&ui32, &ui32_size))
 					return -EINVAL;
-				}
 			}
 			ui32 >>= 8;
 			break;
 		case AMDGPU_INFO_SENSOR_GPU_INPUT_POWER:
-			/* get input GPU power */
 			if (amdgpu_dpm_read_sensor(adev,
 						   AMDGPU_PP_SENSOR_GPU_INPUT_POWER,
-						   (void *)&ui32, &ui32_size)) {
+						   (void *)&ui32, &ui32_size))
 				return -EINVAL;
-			}
 			ui32 >>= 8;
 			break;
 		case AMDGPU_INFO_SENSOR_VDDNB:
-			/* get VDDNB in millivolts */
 			if (amdgpu_dpm_read_sensor(adev,
 						   AMDGPU_PP_SENSOR_VDDNB,
-						   (void *)&ui32, &ui32_size)) {
+						   (void *)&ui32, &ui32_size))
 				return -EINVAL;
-			}
 			break;
 		case AMDGPU_INFO_SENSOR_VDDGFX:
-			/* get VDDGFX in millivolts */
 			if (amdgpu_dpm_read_sensor(adev,
 						   AMDGPU_PP_SENSOR_VDDGFX,
-						   (void *)&ui32, &ui32_size)) {
+						   (void *)&ui32, &ui32_size))
 				return -EINVAL;
-			}
 			break;
 		case AMDGPU_INFO_SENSOR_STABLE_PSTATE_GFX_SCLK:
-			/* get stable pstate sclk in Mhz */
 			if (amdgpu_dpm_read_sensor(adev,
 						   AMDGPU_PP_SENSOR_STABLE_PSTATE_SCLK,
-						   (void *)&ui32, &ui32_size)) {
+						   (void *)&ui32, &ui32_size))
 				return -EINVAL;
-			}
 			ui32 /= 100;
 			break;
 		case AMDGPU_INFO_SENSOR_STABLE_PSTATE_GFX_MCLK:
-			/* get stable pstate mclk in Mhz */
 			if (amdgpu_dpm_read_sensor(adev,
 						   AMDGPU_PP_SENSOR_STABLE_PSTATE_MCLK,
-						   (void *)&ui32, &ui32_size)) {
+						   (void *)&ui32, &ui32_size))
 				return -EINVAL;
-			}
 			ui32 /= 100;
 			break;
 		case AMDGPU_INFO_SENSOR_PEAK_PSTATE_GFX_SCLK:
-			/* get peak pstate sclk in Mhz */
 			if (amdgpu_dpm_read_sensor(adev,
 						   AMDGPU_PP_SENSOR_PEAK_PSTATE_SCLK,
-						   (void *)&ui32, &ui32_size)) {
+						   (void *)&ui32, &ui32_size))
 				return -EINVAL;
-			}
 			ui32 /= 100;
 			break;
 		case AMDGPU_INFO_SENSOR_PEAK_PSTATE_GFX_MCLK:
-			/* get peak pstate mclk in Mhz */
 			if (amdgpu_dpm_read_sensor(adev,
 						   AMDGPU_PP_SENSOR_PEAK_PSTATE_MCLK,
-						   (void *)&ui32, &ui32_size)) {
+						   (void *)&ui32, &ui32_size))
 				return -EINVAL;
-			}
 			ui32 /= 100;
 			break;
 		default:
@@ -1187,22 +1197,27 @@ out:
 				      info->sensor_info.type);
 			return -EINVAL;
 		}
-		return copy_to_user(out, &ui32, min(size, 4u)) ? -EFAULT : 0;
+		if (copy_to_user(out, &ui32, min(user_ret_size, 4u)))
+			return -EFAULT;
+		return 0;
 	}
 	case AMDGPU_INFO_VRAM_LOST_COUNTER:
 		ui32 = atomic_read(&adev->vram_lost_counter);
-		return copy_to_user(out, &ui32, min(size, 4u)) ? -EFAULT : 0;
+		if (copy_to_user(out, &ui32, min(user_ret_size, 4u)))
+			return -EFAULT;
+		return 0;
 	case AMDGPU_INFO_RAS_ENABLED_FEATURES: {
-		struct amdgpu_ras *ras = amdgpu_ras_get_context(adev);
 		uint64_t ras_mask;
+		struct amdgpu_ras *ras = amdgpu_ras_get_context(adev);
 
 		if (!ras)
 			return -EINVAL;
 		ras_mask = (uint64_t)adev->ras_enabled << 32 | ras->features;
 
-		return copy_to_user(out, &ras_mask,
-				min_t(u64, size, sizeof(ras_mask))) ?
-			-EFAULT : 0;
+		if (copy_to_user(out, &ras_mask,
+				    min_t(size_t, user_ret_size, sizeof(ras_mask))))
+			return -EFAULT;
+		return 0;
 	}
 	case AMDGPU_INFO_VIDEO_CAPS: {
 		const struct amdgpu_video_codecs *codecs;
@@ -1256,11 +1271,13 @@ out:
 					codecs->codec_array[i].max_level;
 				break;
 			default:
+				/* Deliberately do nothing for unknown codecs */
 				break;
 			}
 		}
-		r = copy_to_user(out, caps,
-				 min((size_t)size, sizeof(*caps))) ? -EFAULT : 0;
+		r = copy_to_user(out, caps, min_t(size_t, user_ret_size, sizeof(*caps))) ?
+			    -EFAULT :
+			    0;
 		kfree(caps);
 		return r;
 	}
@@ -1270,17 +1287,20 @@ out:
 		for (i = 0; i < AMDGPU_HW_IP_NUM; ++i)
 			max_ibs[i] = amdgpu_ring_max_ibs(i);
 
-		return copy_to_user(out, max_ibs,
-				    min((size_t)size, sizeof(max_ibs))) ? -EFAULT : 0;
+		if (copy_to_user(out, max_ibs,
+				    min_t(size_t, user_ret_size, sizeof(max_ibs))))
+			return -EFAULT;
+		return 0;
 	}
 	case AMDGPU_INFO_GPUVM_FAULT: {
-		struct amdgpu_fpriv *fpriv = filp->driver_priv;
-		struct amdgpu_vm *vm = &fpriv->vm;
+		struct amdgpu_fpriv *fpriv_vm = filp->driver_priv;
+		struct amdgpu_vm *vm;
 		struct drm_amdgpu_info_gpuvm_fault gpuvm_fault;
 		unsigned long flags;
 
-		if (!vm)
+		if (!fpriv_vm) /* Should not happen if open succeeded */
 			return -EINVAL;
+		vm = &fpriv_vm->vm;
 
 		memset(&gpuvm_fault, 0, sizeof(gpuvm_fault));
 
@@ -1290,8 +1310,10 @@ out:
 		gpuvm_fault.vmhub = vm->fault_info.vmhub;
 		xa_unlock_irqrestore(&adev->vm_manager.pasids, flags);
 
-		return copy_to_user(out, &gpuvm_fault,
-				    min((size_t)size, sizeof(gpuvm_fault))) ? -EFAULT : 0;
+		if (copy_to_user(out, &gpuvm_fault,
+				    min_t(size_t, user_ret_size, sizeof(gpuvm_fault))))
+			return -EFAULT;
+		return 0;
 	}
 	default:
 		DRM_DEBUG_KMS("Invalid request %d\n", info->query);
@@ -1313,90 +1335,90 @@ int amdgpu_driver_open_kms(struct drm_de
 {
 	struct amdgpu_device *adev = drm_to_adev(dev);
 	struct amdgpu_fpriv *fpriv;
-	int r, pasid;
+	bool pm_active = false;
+	int r, pasid = 0;		/* 0 ⇒ “no PASID allocated yet” */
 
-	/* Ensure IB tests are run on ring */
 	flush_delayed_work(&adev->delayed_init_work);
 
-
 	if (amdgpu_ras_intr_triggered()) {
-		DRM_ERROR("RAS Intr triggered, device disabled!!");
+		DRM_ERROR("RAS Intr triggered, device disabled!!\n");
 		return -EHWPOISON;
 	}
 
-	file_priv->driver_priv = NULL;
-
 	r = pm_runtime_get_sync(dev->dev);
 	if (r < 0)
-		goto pm_put;
+		goto err_pm;
+	pm_active = true;
 
 	fpriv = kzalloc(sizeof(*fpriv), GFP_KERNEL);
-	if (unlikely(!fpriv)) {
+	if (!fpriv) {
 		r = -ENOMEM;
-		goto out_suspend;
+		goto err_pm;
 	}
 
+	prefetch(&adev->vm_manager);
+
 	pasid = amdgpu_pasid_alloc(16);
 	if (pasid < 0) {
-		dev_warn(adev->dev, "No more PASIDs available!");
+		dev_warn(&adev->pdev->dev, "No more PASIDs available!\n");
 		pasid = 0;
 	}
 
 	r = amdgpu_xcp_open_device(adev, fpriv, file_priv);
 	if (r)
-		goto error_pasid;
+		goto err_free_fpriv;
 
 	r = amdgpu_vm_init(adev, &fpriv->vm, fpriv->xcp_id);
 	if (r)
-		goto error_pasid;
+		goto err_xcp;
 
 	r = amdgpu_vm_set_pasid(adev, &fpriv->vm, pasid);
 	if (r)
-		goto error_vm;
+		goto err_vm;
 
 	fpriv->prt_va = amdgpu_vm_bo_add(adev, &fpriv->vm, NULL);
 	if (!fpriv->prt_va) {
 		r = -ENOMEM;
-		goto error_vm;
+		goto err_vm;
 	}
 
 	if (adev->gfx.mcbp) {
 		uint64_t csa_addr = amdgpu_csa_vaddr(adev) & AMDGPU_GMC_HOLE_MASK;
 
+		prefetch(adev->virt.csa_obj);
+
 		r = amdgpu_map_static_csa(adev, &fpriv->vm, adev->virt.csa_obj,
-						&fpriv->csa_va, csa_addr, AMDGPU_CSA_SIZE);
+								  &fpriv->csa_va, csa_addr,
+							AMDGPU_CSA_SIZE);
 		if (r)
-			goto error_vm;
+			goto err_vm;
 	}
 
 	r = amdgpu_seq64_map(adev, &fpriv->vm, &fpriv->seq64_va);
 	if (r)
-		goto error_vm;
+		goto err_vm;
 
 	mutex_init(&fpriv->bo_list_lock);
 	idr_init_base(&fpriv->bo_list_handles, 1);
-
 	amdgpu_ctx_mgr_init(&fpriv->ctx_mgr, adev);
 
 	file_priv->driver_priv = fpriv;
-	goto out_suspend;
 
-error_vm:
-	amdgpu_vm_fini(adev, &fpriv->vm);
-
-error_pasid:
-	if (pasid) {
-		amdgpu_pasid_free(pasid);
-		amdgpu_vm_set_pasid(adev, &fpriv->vm, 0);
-	}
-
-	kfree(fpriv);
-
-out_suspend:
 	pm_runtime_mark_last_busy(dev->dev);
-pm_put:
 	pm_runtime_put_autosuspend(dev->dev);
+	return 0;
 
+	/* ---------- tear-down paths ---------- */
+	err_vm:
+	amdgpu_vm_fini(adev, &fpriv->vm);
+	err_xcp:
+	if (pasid)
+		amdgpu_pasid_free(pasid);
+	err_free_fpriv:
+	kfree(fpriv);
+	err_pm:
+	if (pm_active)
+		pm_runtime_put_autosuspend(dev->dev);
 	return r;
 }
 
@@ -1415,17 +1437,21 @@ void amdgpu_driver_postclose_kms(struct
 	struct amdgpu_fpriv *fpriv = file_priv->driver_priv;
 	struct amdgpu_bo_list *list;
 	struct amdgpu_bo *pd;
+	struct amdgpu_ip_block *ip_block = NULL;
 	u32 pasid;
 	int handle;
 
-	if (!fpriv)
+	if (unlikely(!fpriv))
 		return;
 
-	pm_runtime_get_sync(dev->dev);
+	pm_runtime_get_sync(&adev->pdev->dev);
 
-	if (amdgpu_device_ip_get_ip_block(adev, AMD_IP_BLOCK_TYPE_UVD) != NULL)
+	ip_block = amdgpu_device_ip_get_ip_block(adev, AMD_IP_BLOCK_TYPE_UVD);
+	if (ip_block && ip_block->status.valid)
 		amdgpu_uvd_free_handles(adev, file_priv);
-	if (amdgpu_device_ip_get_ip_block(adev, AMD_IP_BLOCK_TYPE_VCE) != NULL)
+
+	ip_block = amdgpu_device_ip_get_ip_block(adev, AMD_IP_BLOCK_TYPE_VCE);
+	if (ip_block && ip_block->status.valid)
 		amdgpu_vce_free_handles(adev, file_priv);
 
 	if (fpriv->csa_va) {
@@ -1440,7 +1466,9 @@ void amdgpu_driver_postclose_kms(struct
 
 	pasid = fpriv->vm.pasid;
 	pd = amdgpu_bo_ref(fpriv->vm.root.bo);
-	if (!WARN_ON(amdgpu_bo_reserve(pd, true))) {
+
+	/* Branch prediction: Hint that this error condition is never expected. */
+	if (likely(!WARN_ON(amdgpu_bo_reserve(pd, true)))) {
 		amdgpu_vm_bo_del(adev, fpriv->prt_va);
 		amdgpu_bo_unreserve(pd);
 	}
@@ -1452,8 +1480,9 @@ void amdgpu_driver_postclose_kms(struct
 		amdgpu_pasid_free_delayed(pd->tbo.base.resv, pasid);
 	amdgpu_bo_unref(&pd);
 
-	idr_for_each_entry(&fpriv->bo_list_handles, list, handle)
+	idr_for_each_entry(&fpriv->bo_list_handles, list, handle) {
 		amdgpu_bo_list_put(list);
+	}
 
 	idr_destroy(&fpriv->bo_list_handles);
 	mutex_destroy(&fpriv->bo_list_lock);
@@ -1461,8 +1490,8 @@ void amdgpu_driver_postclose_kms(struct
 	kfree(fpriv);
 	file_priv->driver_priv = NULL;
 
-	pm_runtime_mark_last_busy(dev->dev);
-	pm_runtime_put_autosuspend(dev->dev);
+	pm_runtime_mark_last_busy(&adev->pdev->dev);
+	pm_runtime_put_autosuspend(&adev->pdev->dev);
 }
 
 
@@ -1474,9 +1503,6 @@ void amdgpu_driver_release_kms(struct dr
 	pci_set_drvdata(adev->pdev, NULL);
 }
 
-/*
- * VBlank related functions.
- */
 /**
  * amdgpu_get_vblank_counter_kms - get frame count
  *
@@ -1487,57 +1513,35 @@ void amdgpu_driver_release_kms(struct dr
  */
 u32 amdgpu_get_vblank_counter_kms(struct drm_crtc *crtc)
 {
-	struct drm_device *dev = crtc->dev;
-	unsigned int pipe = crtc->index;
+	struct drm_device  *dev  = crtc->dev;
 	struct amdgpu_device *adev = drm_to_adev(dev);
+	unsigned int pipe = crtc->index;
 	int vpos, hpos, stat;
-	u32 count;
+	u32 count, new_count;
+	int retries = 32;	/* ample even for 1 kHz panels */
 
 	if (pipe >= adev->mode_info.num_crtc) {
 		DRM_ERROR("Invalid crtc %u\n", pipe);
-		return -EINVAL;
+		return (u32)-EINVAL;
 	}
 
-	/* The hw increments its frame counter at start of vsync, not at start
-	 * of vblank, as is required by DRM core vblank counter handling.
-	 * Cook the hw count here to make it appear to the caller as if it
-	 * incremented at start of vblank. We measure distance to start of
-	 * vblank in vpos. vpos therefore will be >= 0 between start of vblank
-	 * and start of vsync, so vpos >= 0 means to bump the hw frame counter
-	 * result by 1 to give the proper appearance to caller.
-	 */
 	if (adev->mode_info.crtcs[pipe]) {
-		/* Repeat readout if needed to provide stable result if
-		 * we cross start of vsync during the queries.
-		 */
 		do {
 			count = amdgpu_display_vblank_get_counter(adev, pipe);
-			/* Ask amdgpu_display_get_crtc_scanoutpos to return
-			 * vpos as distance to start of vblank, instead of
-			 * regular vertical scanout pos.
-			 */
+
 			stat = amdgpu_display_get_crtc_scanoutpos(
 				dev, pipe, GET_DISTANCE_TO_VBLANKSTART,
 				&vpos, &hpos, NULL, NULL,
 				&adev->mode_info.crtcs[pipe]->base.hwmode);
-		} while (count != amdgpu_display_vblank_get_counter(adev, pipe));
 
-		if (((stat & (DRM_SCANOUTPOS_VALID | DRM_SCANOUTPOS_ACCURATE)) !=
-		    (DRM_SCANOUTPOS_VALID | DRM_SCANOUTPOS_ACCURATE))) {
-			DRM_DEBUG_VBL("Query failed! stat %d\n", stat);
-		} else {
-			DRM_DEBUG_VBL("crtc %d: dist from vblank start %d\n",
-				      pipe, vpos);
+			new_count = amdgpu_display_vblank_get_counter(adev, pipe);
+		} while (count != new_count && --retries);
 
-			/* Bump counter if we are at >= leading edge of vblank,
-			 * but before vsync where vpos would turn negative and
-			 * the hw counter really increments.
-			 */
-			if (vpos >= 0)
-				count++;
-		}
+		if ((stat & (DRM_SCANOUTPOS_VALID | DRM_SCANOUTPOS_ACCURATE)) ==
+			(DRM_SCANOUTPOS_VALID | DRM_SCANOUTPOS_ACCURATE) &&
+			vpos >= 0)
+			count++;	/* inside active scanout region */
 	} else {
-		/* Fallback to use value as is. */
 		count = amdgpu_display_vblank_get_counter(adev, pipe);
 		DRM_DEBUG_VBL("NULL mode info! Returned count may be wrong.\n");
 	}
@@ -1580,11 +1584,19 @@ void amdgpu_disable_vblank_kms(struct dr
 	amdgpu_irq_put(adev, &adev->crtc_irq, idx);
 }
 
-/*
- * Debugfs info
- */
 #if defined(CONFIG_DEBUG_FS)
 
+static const char * const ta_fw_name[TA_FW_TYPE_MAX_INDEX] = {
+#define TA_FW_NAME(type) [TA_FW_TYPE_PSP_##type] = #type
+	TA_FW_NAME(XGMI),
+	TA_FW_NAME(RAS),
+	TA_FW_NAME(HDCP),
+	TA_FW_NAME(DTM),
+	TA_FW_NAME(RAP),
+	TA_FW_NAME(SECUREDISPLAY),
+#undef TA_FW_NAME
+};
+
 static int amdgpu_debugfs_firmware_info_show(struct seq_file *m, void *unused)
 {
 	struct amdgpu_device *adev = m->private;
@@ -1594,18 +1606,6 @@ static int amdgpu_debugfs_firmware_info_
 	uint8_t smu_program, smu_major, smu_minor, smu_debug;
 	int ret, i;
 
-	static const char *ta_fw_name[TA_FW_TYPE_MAX_INDEX] = {
-#define TA_FW_NAME(type)[TA_FW_TYPE_PSP_##type] = #type
-		TA_FW_NAME(XGMI),
-		TA_FW_NAME(RAS),
-		TA_FW_NAME(HDCP),
-		TA_FW_NAME(DTM),
-		TA_FW_NAME(RAP),
-		TA_FW_NAME(SECUREDISPLAY),
-#undef TA_FW_NAME
-	};
-
-	/* VCE */
 	query_fw.fw_type = AMDGPU_INFO_FW_VCE;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)
@@ -1613,7 +1613,6 @@ static int amdgpu_debugfs_firmware_info_
 	seq_printf(m, "VCE feature version: %u, firmware version: 0x%08x\n",
 		   fw_info.feature, fw_info.ver);
 
-	/* UVD */
 	query_fw.fw_type = AMDGPU_INFO_FW_UVD;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)
@@ -1621,7 +1620,6 @@ static int amdgpu_debugfs_firmware_info_
 	seq_printf(m, "UVD feature version: %u, firmware version: 0x%08x\n",
 		   fw_info.feature, fw_info.ver);
 
-	/* GMC */
 	query_fw.fw_type = AMDGPU_INFO_FW_GMC;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)
@@ -1629,7 +1627,6 @@ static int amdgpu_debugfs_firmware_info_
 	seq_printf(m, "MC feature version: %u, firmware version: 0x%08x\n",
 		   fw_info.feature, fw_info.ver);
 
-	/* ME */
 	query_fw.fw_type = AMDGPU_INFO_FW_GFX_ME;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)
@@ -1637,7 +1634,6 @@ static int amdgpu_debugfs_firmware_info_
 	seq_printf(m, "ME feature version: %u, firmware version: 0x%08x\n",
 		   fw_info.feature, fw_info.ver);
 
-	/* PFP */
 	query_fw.fw_type = AMDGPU_INFO_FW_GFX_PFP;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)
@@ -1645,7 +1641,6 @@ static int amdgpu_debugfs_firmware_info_
 	seq_printf(m, "PFP feature version: %u, firmware version: 0x%08x\n",
 		   fw_info.feature, fw_info.ver);
 
-	/* CE */
 	query_fw.fw_type = AMDGPU_INFO_FW_GFX_CE;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)
@@ -1653,7 +1648,6 @@ static int amdgpu_debugfs_firmware_info_
 	seq_printf(m, "CE feature version: %u, firmware version: 0x%08x\n",
 		   fw_info.feature, fw_info.ver);
 
-	/* RLC */
 	query_fw.fw_type = AMDGPU_INFO_FW_GFX_RLC;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)
@@ -1661,7 +1655,6 @@ static int amdgpu_debugfs_firmware_info_
 	seq_printf(m, "RLC feature version: %u, firmware version: 0x%08x\n",
 		   fw_info.feature, fw_info.ver);
 
-	/* RLC SAVE RESTORE LIST CNTL */
 	query_fw.fw_type = AMDGPU_INFO_FW_GFX_RLC_RESTORE_LIST_CNTL;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)
@@ -1669,7 +1662,6 @@ static int amdgpu_debugfs_firmware_info_
 	seq_printf(m, "RLC SRLC feature version: %u, firmware version: 0x%08x\n",
 		   fw_info.feature, fw_info.ver);
 
-	/* RLC SAVE RESTORE LIST GPM MEM */
 	query_fw.fw_type = AMDGPU_INFO_FW_GFX_RLC_RESTORE_LIST_GPM_MEM;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)
@@ -1677,7 +1669,6 @@ static int amdgpu_debugfs_firmware_info_
 	seq_printf(m, "RLC SRLG feature version: %u, firmware version: 0x%08x\n",
 		   fw_info.feature, fw_info.ver);
 
-	/* RLC SAVE RESTORE LIST SRM MEM */
 	query_fw.fw_type = AMDGPU_INFO_FW_GFX_RLC_RESTORE_LIST_SRM_MEM;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)
@@ -1685,7 +1676,6 @@ static int amdgpu_debugfs_firmware_info_
 	seq_printf(m, "RLC SRLS feature version: %u, firmware version: 0x%08x\n",
 		   fw_info.feature, fw_info.ver);
 
-	/* RLCP */
 	query_fw.fw_type = AMDGPU_INFO_FW_GFX_RLCP;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)
@@ -1693,7 +1683,6 @@ static int amdgpu_debugfs_firmware_info_
 	seq_printf(m, "RLCP feature version: %u, firmware version: 0x%08x\n",
 		   fw_info.feature, fw_info.ver);
 
-	/* RLCV */
 	query_fw.fw_type = AMDGPU_INFO_FW_GFX_RLCV;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)
@@ -1701,7 +1690,6 @@ static int amdgpu_debugfs_firmware_info_
 	seq_printf(m, "RLCV feature version: %u, firmware version: 0x%08x\n",
 		   fw_info.feature, fw_info.ver);
 
-	/* MEC */
 	query_fw.fw_type = AMDGPU_INFO_FW_GFX_MEC;
 	query_fw.index = 0;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
@@ -1710,7 +1698,6 @@ static int amdgpu_debugfs_firmware_info_
 	seq_printf(m, "MEC feature version: %u, firmware version: 0x%08x\n",
 		   fw_info.feature, fw_info.ver);
 
-	/* MEC2 */
 	if (adev->gfx.mec2_fw) {
 		query_fw.index = 1;
 		ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
@@ -1720,7 +1707,6 @@ static int amdgpu_debugfs_firmware_info_
 			   fw_info.feature, fw_info.ver);
 	}
 
-	/* IMU */
 	query_fw.fw_type = AMDGPU_INFO_FW_IMU;
 	query_fw.index = 0;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
@@ -1729,7 +1715,6 @@ static int amdgpu_debugfs_firmware_info_
 	seq_printf(m, "IMU feature version: %u, firmware version: 0x%08x\n",
 		   fw_info.feature, fw_info.ver);
 
-	/* PSP SOS */
 	query_fw.fw_type = AMDGPU_INFO_FW_SOS;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)
@@ -1738,7 +1723,6 @@ static int amdgpu_debugfs_firmware_info_
 		   fw_info.feature, fw_info.ver);
 
 
-	/* PSP ASD */
 	query_fw.fw_type = AMDGPU_INFO_FW_ASD;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)
@@ -1757,7 +1741,6 @@ static int amdgpu_debugfs_firmware_info_
 			   ta_fw_name[i], fw_info.feature, fw_info.ver);
 	}
 
-	/* SMC */
 	query_fw.fw_type = AMDGPU_INFO_FW_SMC;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)
@@ -1769,7 +1752,6 @@ static int amdgpu_debugfs_firmware_info_
 	seq_printf(m, "SMC feature version: %u, program: %d, firmware version: 0x%08x (%d.%d.%d)\n",
 		   fw_info.feature, smu_program, fw_info.ver, smu_major, smu_minor, smu_debug);
 
-	/* SDMA */
 	query_fw.fw_type = AMDGPU_INFO_FW_SDMA;
 	for (i = 0; i < adev->sdma.num_instances; i++) {
 		query_fw.index = i;
@@ -1780,7 +1762,6 @@ static int amdgpu_debugfs_firmware_info_
 			   i, fw_info.feature, fw_info.ver);
 	}
 
-	/* VCN */
 	query_fw.fw_type = AMDGPU_INFO_FW_VCN;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)
@@ -1788,7 +1769,6 @@ static int amdgpu_debugfs_firmware_info_
 	seq_printf(m, "VCN feature version: %u, firmware version: 0x%08x\n",
 		   fw_info.feature, fw_info.ver);
 
-	/* DMCU */
 	query_fw.fw_type = AMDGPU_INFO_FW_DMCU;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)
@@ -1796,7 +1776,6 @@ static int amdgpu_debugfs_firmware_info_
 	seq_printf(m, "DMCU feature version: %u, firmware version: 0x%08x\n",
 		   fw_info.feature, fw_info.ver);
 
-	/* DMCUB */
 	query_fw.fw_type = AMDGPU_INFO_FW_DMCUB;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)
@@ -1804,7 +1783,6 @@ static int amdgpu_debugfs_firmware_info_
 	seq_printf(m, "DMCUB feature version: %u, firmware version: 0x%08x\n",
 		   fw_info.feature, fw_info.ver);
 
-	/* TOC */
 	query_fw.fw_type = AMDGPU_INFO_FW_TOC;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)
@@ -1812,17 +1790,15 @@ static int amdgpu_debugfs_firmware_info_
 	seq_printf(m, "TOC feature version: %u, firmware version: 0x%08x\n",
 		   fw_info.feature, fw_info.ver);
 
-	/* CAP */
 	if (adev->psp.cap_fw) {
 		query_fw.fw_type = AMDGPU_INFO_FW_CAP;
 		ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 		if (ret)
 			return ret;
 		seq_printf(m, "CAP feature version: %u, firmware version: 0x%08x\n",
-				fw_info.feature, fw_info.ver);
+			   fw_info.feature, fw_info.ver);
 	}
 
-	/* MES_KIQ */
 	query_fw.fw_type = AMDGPU_INFO_FW_MES_KIQ;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)
@@ -1830,7 +1806,6 @@ static int amdgpu_debugfs_firmware_info_
 	seq_printf(m, "MES_KIQ feature version: %u, firmware version: 0x%08x\n",
 		   fw_info.feature, fw_info.ver);
 
-	/* MES */
 	query_fw.fw_type = AMDGPU_INFO_FW_MES;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)
@@ -1838,7 +1813,6 @@ static int amdgpu_debugfs_firmware_info_
 	seq_printf(m, "MES feature version: %u, firmware version: 0x%08x\n",
 		   fw_info.feature, fw_info.ver);
 
-	/* VPE */
 	query_fw.fw_type = AMDGPU_INFO_FW_VPE;
 	ret = amdgpu_firmware_info(&fw_info, &query_fw, adev);
 	if (ret)


--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c	2025-06-23 18:44:25.287659188 +0200
@@ -26,10 +26,23 @@
  *          Jerome Glisse
  */
 #include <linux/ktime.h>
+#include <linux/mm.h>
 #include <linux/module.h>
 #include <linux/pagemap.h>
 #include <linux/pci.h>
 #include <linux/dma-buf.h>
+#include <linux/jump_label.h>
+#include <linux/prefetch.h>
+#include <linux/atomic.h>
+#include <linux/sched.h>
+#include <linux/sched/clock.h>
+#include <linux/seqlock.h>
+#include <linux/math64.h>
+#include <linux/overflow.h>
+#include <linux/hash.h>
+#include <linux/workqueue.h>
+#include <linux/completion.h>
+#include <linux/atomic.h>
 
 #include <drm/amdgpu_drm.h>
 #include <drm/drm_drv.h>
@@ -44,35 +57,976 @@
 #include "amdgpu_xgmi.h"
 #include "amdgpu_vm.h"
 
+/* --- Forward declarations --- */
+static inline u32 pb_get_bias(void);
+static bool amdgpu_vega_optimize_for_workload(struct amdgpu_device *adev,
+											  struct amdgpu_bo *bo,
+											  uint64_t flags);
+void amdgpu_vega_vram_thresholds_init(void);
+
+#if defined(CONFIG_X86_64)
+#define KTIME_FAST_NS()  ktime_get_mono_fast_ns()
+#else
+#define KTIME_FAST_NS()  ktime_get_ns()
+#endif
+
+/*
+ * VRAM Pressure Governor
+ */
+enum vega_vram_pressure_state {
+	VEGA_VRAM_GREEN,
+	VEGA_VRAM_YELLOW,
+	VEGA_VRAM_RED,
+};
+
+/* Padded to prevent false sharing on multi-core CPUs */
+struct vega_pressure_cache_pc {
+	enum vega_vram_pressure_state state;
+	u32 pct_last;
+	unsigned long jts_last_update;
+} ____cacheline_aligned;
+
+static DEFINE_PER_CPU(struct vega_pressure_cache_pc, pressure_cache_pc);
+
+struct vega_vram_state {
+	/* seqcount_spinlock_t is optimal for this read-mostly data */
+	seqcount_spinlock_t seq;
+	u64        size_bytes;
+	u64        reciprocal_100;
+	atomic_t   eviction_rate_ewma;
+	spinlock_t lock; /* The spinlock for the seqcount */
+};
+
+/* Correctly initialized structure */
+static struct vega_vram_state vram_state = {
+	.seq = SEQCNT_SPINLOCK_ZERO(vram_state.seq, &vram_state.lock),
+	.size_bytes = 0,
+	.reciprocal_100 = 0,
+	.eviction_rate_ewma = ATOMIC_INIT(0),
+	.lock = __SPIN_LOCK_UNLOCKED(vram_state.lock),
+};
+
+/* Initialize VRAM state on first device probe */
+static void amdgpu_vram_state_init(struct amdgpu_device *adev)
+{
+	static atomic_t initialised = ATOMIC_INIT(0);
+	unsigned long flags;
+	u64 vram_b;
+
+	if (unlikely(!adev)) {
+		return;
+	}
+
+	if (atomic_xchg(&initialised, 1)) {
+		return;					/* already done */
+	}
+
+	vram_b = adev->gmc.mc_vram_size;
+	if (unlikely(vram_b < SZ_1M)) {
+		pr_warn("amdgpu: VRAM size too small – pressure governor off\n");
+		return;
+	}
+
+	spin_lock_irqsave(&vram_state.lock, flags);
+	write_seqcount_begin(&vram_state.seq);
+	vram_state.size_bytes = vram_b;
+	vram_state.reciprocal_100 =
+	div64_u64((1ULL << 38), div64_u64(vram_b, 100)); /* Q38 */
+	write_seqcount_end(&vram_state.seq);
+	spin_unlock_irqrestore(&vram_state.lock, flags);
+}
+
+
+#define PB_ENTRIES          64U
+#define PB_HASH_MASK        (PB_ENTRIES - 1)
+#define EW_UNIT_SHIFT       4
+#define EW_INC_PER_FAULT    16
+#define MAX_EWMA            (16 << EW_UNIT_SHIFT)
+#define PB_DECAY_FACTOR_NS (128 * NSEC_PER_MSEC)
+
+/* Padded to prevent false sharing */
+struct pid_bias_entry {
+	u32 tgid;
+	u16 ewma;
+	u64 ns_last;
+} ____cacheline_aligned;
+
+static DEFINE_PER_CPU(struct pid_bias_entry[PB_ENTRIES], pid_bias_tbl);
+
+DEFINE_STATIC_KEY_FALSE(vega_bankalign_key);
+DEFINE_STATIC_KEY_FALSE(vega_prefetch_key);
+DEFINE_STATIC_KEY_FALSE(vega_domain_key);
+DEFINE_STATIC_KEY_FALSE(amdgpu_vm_always_valid_key);
+
+#define vega_hbm2_key vega_domain_key
+
+static inline void amdgpu_vm_always_valid_key_enable(void)
+{
+	if (static_branch_likely(&amdgpu_vm_always_valid_key))
+		return;
+
+	static_branch_enable(&amdgpu_vm_always_valid_key);
+}
+
+static inline void
+amdgpu_gem_static_branch_init(struct amdgpu_device *adev)
+{
+	if (adev && adev->asic_type == CHIP_VEGA10) {
+		static_branch_enable(&vega_bankalign_key);
+		static_branch_enable(&vega_prefetch_key);
+		static_branch_enable(&vega_domain_key);
+		amdgpu_vram_state_init(adev);
+	}
+}
+
+/* Optimization: Increased from 32 */
+#define TBO_CACHE_DEPTH 64
+#define TBO_MAX_BYTES   (64u << 10)
+#define TBO_CACHEABLE_VRAM_FLAGS (AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS | \
+AMDGPU_GEM_CREATE_NO_CPU_ACCESS   | \
+AMDGPU_GEM_CREATE_VRAM_CLEARED)
+
+/* Lockless per-CPU cache with optimal alignment */
+struct tiny_bo_cache {
+	local_lock_t lock;				/* protects the indices */
+	struct amdgpu_bo *gtt_slot[TBO_CACHE_DEPTH / 2];
+	struct amdgpu_bo *vram_slot[TBO_CACHE_DEPTH / 2];
+	u8                gtt_top;
+	u8                vram_top;
+} ____cacheline_aligned;
+
+static DEFINE_PER_CPU(struct tiny_bo_cache, tiny_bo_cache);
+DEFINE_STATIC_KEY_FALSE(tbo_cache_key);
+
+static struct kmem_cache *ubo_slab;
+
+static void amdgpu_tbo_slab_ensure(void)
+{
+	struct kmem_cache *s;
+
+	if (likely(READ_ONCE(ubo_slab)))
+		return;
+
+	s = kmem_cache_create("amdgpu_bo_user",
+			      sizeof(struct amdgpu_bo_user),
+			      0, SLAB_HWCACHE_ALIGN, NULL);
+	if (unlikely(!s))
+		return;
+
+	if (cmpxchg(&ubo_slab, NULL, s)) {
+		kmem_cache_destroy(s);
+	} else {
+		static_branch_enable(&tbo_cache_key);
+	}
+}
+
+/* --------------------------------------------------------------------- *
+ * Helpers for flag checking                                             *
+ * --------------------------------------------------------------------- */
+static inline bool tbo_flags_match_gtt(u64 f)
+{
+	return !(f & ~(AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |
+	AMDGPU_GEM_CREATE_CPU_GTT_USWC));
+}
+
+static inline bool tbo_flags_match_vram(u64 f)
+{
+	return f == (AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS |
+	AMDGPU_GEM_CREATE_NO_CPU_ACCESS   |
+	AMDGPU_GEM_CREATE_VRAM_CLEARED);
+}
+
+/* --------------------------------------------------------------------- *
+ * Fast per-CPU allocation path                                          *
+ * --------------------------------------------------------------------- */
+static struct amdgpu_bo *
+tbo_cache_try_get(unsigned long size, u64 user_flags,
+				  u32 domain, struct dma_resv *resv, int align)
+{
+	struct tiny_bo_cache *c;
+	struct amdgpu_bo *bo = NULL;
+
+	/* Fast-path bail-outs -------------------------------------------------- */
+	if (!static_branch_unlikely(&tbo_cache_key)) {
+		return NULL;				/* cache disabled          */
+	}
+	if (size > TBO_MAX_BYTES || align > PAGE_SIZE || resv) {
+		return NULL;				/* non-tiny / external resv */
+	}
+
+	c = this_cpu_ptr(&tiny_bo_cache);
+
+	/* ----------------------------- pop from stack ------------------------ */
+	local_lock(&c->lock);
+
+	if (domain == AMDGPU_GEM_DOMAIN_GTT) {
+		if (tbo_flags_match_gtt(user_flags) && c->gtt_top) {
+			bo = READ_ONCE(c->gtt_slot[--c->gtt_top]);
+			WRITE_ONCE(c->gtt_slot[c->gtt_top], NULL);
+		}
+	} else if (domain == AMDGPU_GEM_DOMAIN_VRAM) {
+		if (tbo_flags_match_vram(user_flags) && c->vram_top) {
+			bo = READ_ONCE(c->vram_slot[--c->vram_top]);
+			WRITE_ONCE(c->vram_slot[c->vram_top], NULL);
+		}
+	}
+
+	local_unlock(&c->lock);
+
+	/* Was evicted while cached? ------------------------------------------ */
+	if (bo && bo->tbo.ttm) {
+		ttm_bo_put(&bo->tbo);
+		bo = NULL;
+	}
+
+	/* Size mismatch?  Push back or drop. ---------------------------------- */
+	if (bo && bo->tbo.base.size != size) {
+		local_lock(&c->lock);
+		if (domain == AMDGPU_GEM_DOMAIN_GTT &&
+			c->gtt_top < ARRAY_SIZE(c->gtt_slot)) {
+			WRITE_ONCE(c->gtt_slot[c->gtt_top++], bo);
+		bo = NULL;
+			} else if (domain == AMDGPU_GEM_DOMAIN_VRAM &&
+				c->vram_top < ARRAY_SIZE(c->vram_slot)) {
+				WRITE_ONCE(c->vram_slot[c->vram_top++], bo);
+			bo = NULL;
+				}
+				local_unlock(&c->lock);
+
+				if (bo) {			/* cache full – free object */
+					ttm_bo_put(&bo->tbo);
+					bo = NULL;
+				}
+	}
+
+	/* --------- Optional HBM2 bank-alignment preference (Vega only) ------- */
+	if (bo && domain == AMDGPU_GEM_DOMAIN_VRAM &&
+		static_branch_unlikely(&vega_domain_key)) {
+
+		u32 page_align = bo->tbo.page_alignment;	/* already pages   */
+		bool pwr2      = is_power_of_2(page_align);
+	bool hbm_algn  = pwr2 && (page_align >= (SZ_1M >> PAGE_SHIFT));
+
+	/*
+	 * Keep the bottom 75 % of the per-CPU VRAM stack for "good"
+	 * (≥1 MiB-aligned) objects to maximise bank utilisation.
+	 */
+	if (!hbm_algn) {
+		bool discard = false;
+
+		local_lock(&c->lock);
+		discard = c->vram_top >
+		(u8)(ARRAY_SIZE(c->vram_slot) * 3 / 4);
+		local_unlock(&c->lock);
+
+		if (discard) {
+			ttm_bo_put(&bo->tbo);
+			bo = NULL;
+		}
+	}
+		}
+
+		if (bo) {			/* Prefetch to warm cache lines   */
+			prefetchw(bo->tbo.base.resv);
+			prefetch(bo);
+		}
+
+		return bo;
+}
+
+/* --------------------------------------------------------------------- *
+ * Slow path – park an idle BO in the per-CPU cache                      *
+ * --------------------------------------------------------------------- */
+static bool tbo_cache_put(struct amdgpu_bo *bo)
+{
+	struct tiny_bo_cache *c;
+	u64 flags;
+
+	if (!static_branch_unlikely(&tbo_cache_key) || !bo) {
+		return false;
+	}
+
+	if (!dma_resv_test_signaled(bo->tbo.base.resv, DMA_RESV_USAGE_WRITE) ||
+		bo->tbo.base.size > TBO_MAX_BYTES ||
+		(bo->tbo.page_alignment << PAGE_SHIFT) > PAGE_SIZE ||
+		bo->tbo.ttm) {
+		return false;
+		}
+
+		if (!kref_get_unless_zero(&bo->tbo.kref)) {	/* someone freed it */
+			return false;
+		}
+
+		flags = bo->flags & ~AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE;
+
+		if (__builtin_popcountll(flags) > 4) {
+			ttm_bo_put(&bo->tbo);
+			return false;
+		}
+
+		c = this_cpu_ptr(&tiny_bo_cache);
+		local_lock(&c->lock);
+
+		switch (bo->preferred_domains) {
+			case AMDGPU_GEM_DOMAIN_GTT:
+				if (tbo_flags_match_gtt(flags) &&
+					c->gtt_top < ARRAY_SIZE(c->gtt_slot)) {
+					WRITE_ONCE(c->gtt_slot[c->gtt_top++], bo);
+				local_unlock(&c->lock);
+				return true;
+					}
+					break;
+			case AMDGPU_GEM_DOMAIN_VRAM:
+				if (tbo_flags_match_vram(flags) &&
+					c->vram_top < ARRAY_SIZE(c->vram_slot)) {
+					WRITE_ONCE(c->vram_slot[c->vram_top++], bo);
+				local_unlock(&c->lock);
+				return true;
+					}
+					break;
+			default:
+				break;
+		}
+
+		local_unlock(&c->lock);
+		ttm_bo_put(&bo->tbo);
+		return false;
+}
+
+#define AMDGPU_VEGA_HBM2_BANK_SIZE       (1ULL * 1024 * 1024)
+#define AMDGPU_VEGA_SMALL_BUFFER_SIZE    (1ULL * 1024 * 1024)
+#define AMDGPU_VEGA_MEDIUM_BUFFER_SIZE   (4ULL * 1024 * 1024)
+#define AMDGPU_VEGA_LARGE_BUFFER_SIZE    (16ULL * 1024 * 1024)
+#define AMDGPU_VEGA_HBM2_MIN_ALIGNMENT   (256 * 1024)
+#define FAST_VA_MAP_MAX_BYTES		 (64u << 10)
+
+static int amdgpu_vega_vram_pressure_mid  __ro_after_init = 75;
+static int amdgpu_vega_vram_pressure_high __ro_after_init = 90;
+
+void amdgpu_vega_vram_thresholds_init(void)
+{
+	amdgpu_vega_vram_pressure_mid =
+	clamp(amdgpu_vega_vram_pressure_mid, 50, 90);
+
+	amdgpu_vega_vram_pressure_high =
+	clamp(amdgpu_vega_vram_pressure_high,
+		  amdgpu_vega_vram_pressure_mid + 5, 95);
+}
+
+module_param_named(vram_pressure_mid, amdgpu_vega_vram_pressure_mid,  int, 0644);
+MODULE_PARM_DESC(vram_pressure_mid,  "Mid VRAM pressure threshold for Vega (75)");
+module_param_named(vram_pressure_high, amdgpu_vega_vram_pressure_high, int, 0644);
+MODULE_PARM_DESC(vram_pressure_high, "High VRAM pressure threshold for Vega (90)");
+
+static __always_inline bool is_vega_texture(uint64_t flags)
+{
+	return flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;
+}
+
+static __always_inline bool is_vega_compute(uint64_t flags)
+{
+	return flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS;
+}
+
+static __always_inline bool is_vega_cpu_access(uint64_t flags)
+{
+	return flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
+}
+
+static __always_inline bool is_hbm2_vega(struct amdgpu_device *adev)
+{
+	return static_branch_unlikely(&vega_hbm2_key);
+}
+
+static u32 __amdgpu_vega_get_vram_usage(struct amdgpu_device *adev)
+{
+	struct ttm_resource_manager *mgr;
+	static DEFINE_PER_CPU(u64, recip_q38);
+	static DEFINE_PER_CPU(unsigned long, stamp);
+	u64 used_bytes, recip_q;
+	unsigned long now_j, *stamp_ptr;
+	unsigned int seq;
+	u32 pct;
+
+	if (unlikely(!adev))
+		return 0;
+
+	mgr = ttm_manager_type(&adev->mman.bdev, TTM_PL_VRAM);
+	if (unlikely(!mgr))
+		return 0;
+
+	amdgpu_vram_state_init(adev);
+
+	used_bytes = ttm_resource_manager_usage(mgr);
+
+	preempt_disable();
+
+	now_j     = READ_ONCE(jiffies);
+	stamp_ptr = this_cpu_ptr(&stamp);
+	recip_q   = this_cpu_read(recip_q38);
+
+	if (unlikely(time_after(now_j, *stamp_ptr + HZ / 250))) { /* ~4 ms */
+		do {
+			seq      = read_seqcount_begin(&vram_state.seq);
+			recip_q  = READ_ONCE(vram_state.reciprocal_100);
+		} while (read_seqcount_retry(&vram_state.seq, seq));
+
+		if (unlikely(!recip_q)) {
+			preempt_enable();
+			return 0;
+		}
+
+		this_cpu_write(recip_q38, recip_q);
+		*stamp_ptr = now_j;
+	}
+
+	/* mul_u64_u64_shr() returns a u64 – keep it, clamp to 100, cast later */
+	pct = min_t(u64, mul_u64_u64_shr(used_bytes, recip_q, 38), 100ULL);
+
+	preempt_enable();
+	return pct;
+}
+
+/*
+ * Determine current VRAM pressure on Vega-class HBM2 parts.
+ * Fast path:   per-CPU cached percentage, refreshed every 10 ms.
+ * Slow path:   calls into TTM once per refresh period.
+ *
+ * Hysteresis  : ±2 pp to avoid GREEN<->YELLOW flapping.
+ * Thread-safe : per-CPU data accessed inside get_cpu()/put_cpu() window.
+ *
+ * Must be kept in sync with enum vega_vram_pressure_state.
+ */
+#define VRAM_CACHE_REFRESH_JIFFIES  (HZ / 100)   /* 10 ms */
+#define VRAM_PCT_HYSTERESIS         2u           /* ±2 pp */
+
+static enum vega_vram_pressure_state
+amdgpu_vega_get_vram_pressure_state(struct amdgpu_device *adev)
+{
+	struct vega_pressure_cache_pc *pc;
+	enum vega_vram_pressure_state st;
+	unsigned long now_j;
+	u32 pct_now, pct_pred;
+	u32 bias, y_th, r_th;
+
+	if (unlikely(!adev))
+		return VEGA_VRAM_GREEN;
+
+	preempt_disable();				/* whole function must run on one CPU */
+	pc    = this_cpu_ptr(&pressure_cache_pc);
+	now_j = jiffies;
+
+	/* Fast path – still within the refresh window */
+	if (time_before(now_j,
+		pc->jts_last_update + VRAM_CACHE_REFRESH_JIFFIES)) {
+		st = pc->state;
+	goto out;
+		}
+
+		/* ------------------------------------------------------------------ */
+		pct_now  = __amdgpu_vega_get_vram_usage(adev);
+		pct_pred = (pct_now * 3u + pc->pct_last) >> 2;	/* α ≈ 0.75 */
+
+		/* Mild, opportunistic global EWMA decay (max 4 units per call) */
+		{
+			u32 old, new;
+			do {
+				old = atomic_read(&vram_state.eviction_rate_ewma);
+				if (!old)
+					break;
+				new = old - min(old, 4u);
+			} while (atomic_cmpxchg(&vram_state.eviction_rate_ewma, old, new)
+			!= old);
+		}
+
+		bias = min_t(u32,
+					 atomic_read(&vram_state.eviction_rate_ewma) >> EW_UNIT_SHIFT,
+					 25u);
+
+		y_th = max_t(u32, amdgpu_vega_vram_pressure_mid  - bias, 50u);
+		r_th = max_t(u32, amdgpu_vega_vram_pressure_high - bias, y_th + 5u);
+
+		if (pct_pred > r_th + VRAM_PCT_HYSTERESIS) {
+			st = VEGA_VRAM_RED;
+		} else if (pct_pred > y_th + VRAM_PCT_HYSTERESIS) {
+			st = VEGA_VRAM_YELLOW;
+		} else if (pct_pred < y_th - VRAM_PCT_HYSTERESIS) {
+			st = VEGA_VRAM_GREEN;
+		} else {
+			st = pc->state;		/* hysteresis: keep previous state        */
+		}
+
+		pc->state           = st;
+		pc->pct_last        = pct_now;
+		pc->jts_last_update = now_j;
+
+		out:
+		preempt_enable();
+		return st;
+}
+
+static inline u32 pb_hash(u32 tgid)
+{
+	return hash_32(tgid, ilog2(PB_ENTRIES));
+}
+
+static inline void pb_decay(struct pid_bias_entry *e, u64 now_ns)
+{
+	u64 delta_ns;
+
+	if (unlikely(!e || !e->ewma)) {
+		return;
+	}
+
+	delta_ns = now_ns - e->ns_last;
+	if (delta_ns < PB_DECAY_FACTOR_NS) {
+		return;
+	}
+
+	u64 units = delta_ns / PB_DECAY_FACTOR_NS;
+	u32 shift = (units <= 1) ? 1 :
+	(64 - (u32)__builtin_clzll(units - 1));
+
+	shift = min(shift, 8u);
+	e->ewma >>= shift;
+	e->ns_last = now_ns;
+}
+
+static inline void pb_global_decay_once(void)
+{
+	u32 old, new;
+
+	do {
+		old = atomic_read(&vram_state.eviction_rate_ewma);
+		if (!old) {
+			return;
+		}
+		new = (u32){ old - min(old, 4u) };	/* compound literal */
+	} while (atomic_cmpxchg(&vram_state.eviction_rate_ewma,
+							old, new) != old);
+}
+
+static inline u32 pb_get_bias(void)
+{
+	const u32 tgid = current->tgid;
+	struct pid_bias_entry *tbl;
+	u32 h, i, ret = 0;
+	u64 now_ns;
+	int cpu;
+
+	if (unlikely(!tgid))
+		return 0;
+
+	/* Global decay – very cheap, runs on every lookup */
+	pb_global_decay_once();
+
+	cpu = get_cpu();
+	tbl = per_cpu_ptr(pid_bias_tbl, cpu);
+	h   = pb_hash(tgid);
+	now_ns = sched_clock();
+
+	for (i = 0; i < PB_ENTRIES; ++i, h = (h + 1) & PB_HASH_MASK) {
+		struct pid_bias_entry *e = &tbl[h];
+
+		if (e->tgid == tgid) {
+			if (now_ns - e->ns_last > NSEC_PER_SEC)
+				e->ewma = 0;
+
+			pb_decay(e, now_ns);
+			ret = e->ewma >> EW_UNIT_SHIFT;
+			break;
+		}
+		if (!e->tgid)
+			break;
+	}
+
+	put_cpu();
+	return ret;
+}
+
+static void pb_account_eviction(void)
+{
+	const u32 tgid = current->tgid;
+	struct pid_bias_entry *tbl;
+	u32 h, i, victim_idx = PB_ENTRIES;
+	u64 now_ns;
+	u16 victim_ew = U16_MAX;
+	int cpu;
+
+	if (unlikely(!tgid)) {
+		return;
+	}
+
+	atomic_add_unless(&vram_state.eviction_rate_ewma,
+					  EW_INC_PER_FAULT, INT_MAX);
+
+	cpu = get_cpu();
+	tbl = per_cpu_ptr(pid_bias_tbl, cpu);
+	h   = pb_hash(tgid);
+	now_ns = sched_clock();
+
+	for (i = 0; i < PB_ENTRIES; ++i, h = (h + 1) & PB_HASH_MASK) {
+		struct pid_bias_entry *e = &tbl[h];
+
+		if (e->tgid == tgid) {
+			victim_idx = h;
+			break;
+		}
+		pb_decay(e, now_ns);
+
+		if (!e->tgid) {
+			victim_idx = h;
+			break;
+		}
+
+		if (e->ewma < victim_ew) {
+			victim_ew   = e->ewma;
+			victim_idx  = h;
+		}
+	}
+
+	if (victim_idx < PB_ENTRIES) {
+		struct pid_bias_entry *e = &tbl[victim_idx];
+
+		if (e->tgid != tgid && e->tgid != 0) {
+			pb_decay(e, now_ns);
+		}
+
+		e->tgid    = tgid;
+		e->ewma    = min_t(u16, e->ewma + EW_INC_PER_FAULT, MAX_EWMA);
+		e->ns_last = now_ns;
+	}
+
+	put_cpu();
+}
+
+static bool
+amdgpu_vega_optimize_hbm2_bank_access(struct amdgpu_device *adev,
+									  u64 *size_inout,
+									  u32 *align_inout,
+									  u64 flags)
+{
+	u64 size, new_size;
+	u32 want_align;
+
+	if (!adev || !size_inout || !align_inout) {
+		return false;
+	}
+	if (!is_hbm2_vega(adev)) {
+		return false;
+	}
+
+	size = *size_inout;
+
+	/* skip tiny or CPU-mapped BOs */
+	if (size < SZ_8M ||
+		!(flags & (AMDGPU_GEM_CREATE_NO_CPU_ACCESS |
+		AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS))) {
+		return false;
+		}
+
+		if (size >= SZ_256M) {
+			want_align = SZ_64M;               /* full stack  */
+		} else if (size >= SZ_32M) {
+			want_align = SZ_8M;                /* one pipe    */
+		} else {
+			want_align = SZ_1M;                /* one bank    */
+		}
+
+		want_align = max_t(u32, want_align, *align_inout);
+		if (!is_power_of_2(want_align)) {
+			want_align = roundup_pow_of_two(want_align);
+		}
+
+		if (check_add_overflow(size, (u64)want_align - 1, &new_size)) {
+			return false;		/* would overflow */
+		}
+
+		new_size = ALIGN(size, want_align);
+		if (new_size == size && want_align == *align_inout) {
+			return false;		/* unchanged */
+		}
+
+		*size_inout  = new_size;
+		*align_inout = want_align;
+		return true;
+}
+
+#define PF_LUT_SIZE	64
+static const u8 pf_scale_lut[PF_LUT_SIZE] = {
+	/* 0-1.6% .. 98-100% mapped to scaling 120-40% */
+	#define PF_LUT_ENTRY(i) (120u - (((i) * 100u / PF_LUT_SIZE) * 8u) / 10u)
+	PF_LUT_ENTRY(0),  PF_LUT_ENTRY(1),  PF_LUT_ENTRY(2),  PF_LUT_ENTRY(3),
+	PF_LUT_ENTRY(4),  PF_LUT_ENTRY(5),  PF_LUT_ENTRY(6),  PF_LUT_ENTRY(7),
+	PF_LUT_ENTRY(8),  PF_LUT_ENTRY(9),  PF_LUT_ENTRY(10), PF_LUT_ENTRY(11),
+	PF_LUT_ENTRY(12), PF_LUT_ENTRY(13), PF_LUT_ENTRY(14), PF_LUT_ENTRY(15),
+	PF_LUT_ENTRY(16), PF_LUT_ENTRY(17), PF_LUT_ENTRY(18), PF_LUT_ENTRY(19),
+	PF_LUT_ENTRY(20), PF_LUT_ENTRY(21), PF_LUT_ENTRY(22), PF_LUT_ENTRY(23),
+	PF_LUT_ENTRY(24), PF_LUT_ENTRY(25), PF_LUT_ENTRY(26), PF_LUT_ENTRY(27),
+	PF_LUT_ENTRY(28), PF_LUT_ENTRY(29), PF_LUT_ENTRY(30), PF_LUT_ENTRY(31),
+	PF_LUT_ENTRY(32), PF_LUT_ENTRY(33), PF_LUT_ENTRY(34), PF_LUT_ENTRY(35),
+	PF_LUT_ENTRY(36), PF_LUT_ENTRY(37), PF_LUT_ENTRY(38), PF_LUT_ENTRY(39),
+	PF_LUT_ENTRY(40), PF_LUT_ENTRY(41), PF_LUT_ENTRY(42), PF_LUT_ENTRY(43),
+	PF_LUT_ENTRY(44), PF_LUT_ENTRY(45), PF_LUT_ENTRY(46), PF_LUT_ENTRY(47),
+	PF_LUT_ENTRY(48), PF_LUT_ENTRY(49), PF_LUT_ENTRY(50), PF_LUT_ENTRY(51),
+	PF_LUT_ENTRY(52), PF_LUT_ENTRY(53), PF_LUT_ENTRY(54), PF_LUT_ENTRY(55),
+	PF_LUT_ENTRY(56), PF_LUT_ENTRY(57), PF_LUT_ENTRY(58), PF_LUT_ENTRY(59),
+	PF_LUT_ENTRY(60), PF_LUT_ENTRY(61), PF_LUT_ENTRY(62), PF_LUT_ENTRY(63)
+	#undef PF_LUT_ENTRY
+};
+
+static unsigned int
+amdgpu_vega_determine_optimal_prefetch(struct amdgpu_device *adev,
+				       struct amdgpu_bo *bo,
+				       unsigned int      base_pages)
+{
+	u32 vram_pct;
+	unsigned int total_pages, want, cap;
+	bool is_compute;
+
+	if (!is_hbm2_vega(adev) || !bo || !base_pages) {
+		return base_pages;
+	}
+
+	total_pages = DIV_ROUND_UP(amdgpu_bo_size(bo), PAGE_SIZE);
+	if (!total_pages) {
+		return base_pages;
+	}
+
+	vram_pct = __amdgpu_vega_get_vram_usage(adev);
+	want = (base_pages * pf_scale_lut[min_t(u32,
+				  vram_pct * PF_LUT_SIZE / 100,
+				  PF_LUT_SIZE - 1)]) / 100;
+
+	if (total_pages > 1) {
+		want += ilog2(total_pages);
+	}
+
+	is_compute = bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS;
+	if (is_compute && vram_pct < amdgpu_vega_vram_pressure_high) {
+		want += want >> 2;		/* +25 % */
+	}
+
+	cap = (vram_pct < amdgpu_vega_vram_pressure_mid)   ? 64 :
+	      (vram_pct < amdgpu_vega_vram_pressure_high)  ? 48 : 24;
+
+	/* pressure-weighted: down-scale cap by (pct / 5) pages, min 24 */
+	cap = max(cap - (vram_pct / 5), 24u);
+
+	want = min3(want, cap, total_pages);
+
+	return max(want, 1u);
+}
+
+static bool
+amdgpu_vega_optimize_buffer_placement(struct amdgpu_device *adev,
+									  struct amdgpu_bo *bo,
+									  u64 size, u64 flags,
+									  u32 *domain)
+{
+	u32 cur_dom, want_dom;
+
+	if (!is_hbm2_vega(adev) || !domain) {
+		return false;
+	}
+
+	/* ------------- very cheap constant tests first ---------------------- */
+	if ((flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) && size <= SZ_256K) {
+		want_dom = AMDGPU_GEM_DOMAIN_GTT;
+	} else if (flags & (AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS |
+		AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {
+		want_dom = AMDGPU_GEM_DOMAIN_VRAM;
+		} else {
+			want_dom = *domain &
+			(AMDGPU_GEM_DOMAIN_VRAM | AMDGPU_GEM_DOMAIN_GTT);
+			if (want_dom != AMDGPU_GEM_DOMAIN_VRAM &&
+				want_dom != AMDGPU_GEM_DOMAIN_GTT) {
+				want_dom = AMDGPU_GEM_DOMAIN_VRAM;
+				}
+		}
+
+		/* ---------- dynamic adjustments (pressure, nice value) -------------- */
+		switch (amdgpu_vega_get_vram_pressure_state(adev)) {
+			case VEGA_VRAM_RED:
+				if (size > SZ_1M) {
+					want_dom = AMDGPU_GEM_DOMAIN_GTT;
+				}
+				break;
+			case VEGA_VRAM_YELLOW:
+				if (size > SZ_32M) {
+					want_dom = AMDGPU_GEM_DOMAIN_GTT;
+				}
+				break;
+			default:
+				break;
+		}
+
+		if (unlikely(task_nice(current) > 0)) {
+			want_dom = AMDGPU_GEM_DOMAIN_GTT;
+		}
+
+		/* -------------------------------------------------------------------- */
+		cur_dom = *domain & (AMDGPU_GEM_DOMAIN_VRAM | AMDGPU_GEM_DOMAIN_GTT);
+		if (cur_dom == want_dom) {
+			return false;			/* nothing to change */
+		}
+
+		*domain &= ~(AMDGPU_GEM_DOMAIN_VRAM | AMDGPU_GEM_DOMAIN_GTT);
+		*domain |=  want_dom;
+
+		if (cur_dom == AMDGPU_GEM_DOMAIN_VRAM &&
+			want_dom == AMDGPU_GEM_DOMAIN_GTT) {
+			pb_account_eviction();
+			}
+
+			return true;
+}
+
+static bool amdgpu_vega_optimize_for_workload(struct amdgpu_device *adev,
+											  struct amdgpu_bo *bo,
+											  uint64_t flags)
+{
+	u64 size;
+
+	if (!is_hbm2_vega(adev) || !bo || !bo->tbo.base.dev) {
+		return false;
+	}
+
+	size = amdgpu_bo_size(bo);
+	if (!size) {
+		return false;
+	}
+
+	if (!dma_resv_is_locked(bo->tbo.base.resv)) {
+		return false;
+	}
+
+	if (is_vega_texture(flags) && size >= SZ_4M) {
+		bo->preferred_domains = AMDGPU_GEM_DOMAIN_VRAM;
+		bo->allowed_domains   = AMDGPU_GEM_DOMAIN_VRAM |
+		AMDGPU_GEM_DOMAIN_GTT;
+		return true;
+	}
+
+	if (is_vega_compute(flags) && !is_vega_cpu_access(flags)) {
+		bo->preferred_domains = AMDGPU_GEM_DOMAIN_VRAM;
+		bo->allowed_domains   = AMDGPU_GEM_DOMAIN_VRAM |
+		AMDGPU_GEM_DOMAIN_GTT;
+		return true;
+	}
+
+	if (is_vega_cpu_access(flags) && size <= SZ_1M) {
+		bo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;
+		bo->allowed_domains   = AMDGPU_GEM_DOMAIN_GTT |
+		AMDGPU_GEM_DOMAIN_VRAM;
+		return true;
+	}
+
+	return false;
+}
+
+unsigned long amdgpu_gem_timeout(uint64_t timeout_ns)
+{
+	u64 now = KTIME_FAST_NS();
+	s64 delta_ns;
+
+	if ((s64)timeout_ns < 0)
+		return MAX_SCHEDULE_TIMEOUT;
+
+	if (timeout_ns <= now) {
+		return 0;
+	}
+
+	delta_ns = timeout_ns - now;
+
+	if (unlikely(div64_u64(delta_ns, NSEC_PER_SEC) >= (u64)MAX_SCHEDULE_TIMEOUT - 1))
+		return MAX_SCHEDULE_TIMEOUT - 1;
+
+	return max_t(unsigned long, 1ul, nsecs_to_jiffies(delta_ns));
+}
+
+static const uint16_t pitch_mask_lut[5] = { 0, 255, 127, 63, 63 };
+
+static int
+amdgpu_gem_align_pitch(struct amdgpu_device *adev,
+					   int width, int cpp, bool tiled)
+{
+	int mask, aligned_width, result;
+
+	if (unlikely(width <= 0 || cpp <= 0 || cpp >= ARRAY_SIZE(pitch_mask_lut)))
+		return -EINVAL;
+
+	mask = pitch_mask_lut[cpp];
+
+	if (unlikely(width > INT_MAX - mask))
+		return -EINVAL;
+
+	aligned_width = (width + mask) & ~mask;
+
+	if (unlikely(check_mul_overflow(aligned_width, cpp, &result)))
+		return -EINVAL;
+
+	return result;
+}
+
 static vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)
 {
-	struct ttm_buffer_object *bo = vmf->vma->vm_private_data;
-	struct drm_device *ddev = bo->base.dev;
-	vm_fault_t ret;
-	int idx;
+	struct ttm_buffer_object *bo;
+	struct drm_device        *ddev;
+	vm_fault_t                ret;
+	int                       idx;
+
+	/* -------- 0. Sanity -------------------------------------------------- */
+	if (unlikely(!vmf || !vmf->vma))
+		return VM_FAULT_SIGBUS;
+
+	bo = vmf->vma->vm_private_data;
+	if (unlikely(!bo))
+		return VM_FAULT_SIGBUS;
+
+	ddev = bo->base.dev;
+	if (unlikely(!ddev))
+		return VM_FAULT_SIGBUS;
 
+	/* -------- 1. Reserve object ----------------------------------------- */
 	ret = ttm_bo_vm_reserve(bo, vmf);
-	if (ret)
+	if (unlikely(ret))
 		return ret;
 
-	if (drm_dev_enter(ddev, &idx)) {
+	/* -------- 2. Enter DRM device  -------------------------------------- */
+	if (!drm_dev_enter(ddev, &idx)) {
+		ret = ttm_bo_vm_dummy_page(vmf, vmf->vma->vm_page_prot);
+		goto out_unlock;
+	}
+
+	/* -------------------------------------------------------------------- */
+	{
+		struct amdgpu_device *adev = drm_to_adev(ddev);
+		struct amdgpu_bo     *abo  = ttm_to_amdgpu_bo(bo);
+		unsigned int          prefetch_pages = TTM_BO_VM_NUM_PREFAULT;
+
+		/* 2.a  Notify TTM we are about to fault-in */
 		ret = amdgpu_bo_fault_reserve_notify(bo);
-		if (ret) {
+		if (unlikely(ret)) {
 			drm_dev_exit(idx);
-			goto unlock;
+			goto out_unlock;
 		}
 
+		/* 2.b  Heuristic prefetch ------------------------------------------------ */
+		if (static_branch_unlikely(&vega_prefetch_key) && abo && adev) {
+			prefetch_pages =
+			amdgpu_vega_determine_optimal_prefetch(adev, abo,
+												   prefetch_pages);
+		}
+
+		/* 2.c  Perform the real fault-in now (synchronous path) --------- */
 		ret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot,
-					       TTM_BO_VM_NUM_PREFAULT);
+									   prefetch_pages);
 
 		drm_dev_exit(idx);
-	} else {
-		ret = ttm_bo_vm_dummy_page(vmf, vmf->vma->vm_page_prot);
 	}
-	if (ret == VM_FAULT_RETRY && !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT))
-		return ret;
 
-unlock:
+	out_unlock:
 	dma_resv_unlock(bo->base.resv);
 	return ret;
 }
@@ -81,132 +1035,324 @@ static const struct vm_operations_struct
 	.fault = amdgpu_gem_fault,
 	.open = ttm_bo_vm_open,
 	.close = ttm_bo_vm_close,
-	.access = ttm_bo_vm_access
+	.access = ttm_bo_vm_access,
 };
 
 static void amdgpu_gem_object_free(struct drm_gem_object *gobj)
 {
-	struct amdgpu_bo *aobj = gem_to_amdgpu_bo(gobj);
+	struct amdgpu_bo *aobj;
+
+	if (unlikely(!gobj))
+		return;
+
+	aobj = gem_to_amdgpu_bo(gobj);
+	if (unlikely(!aobj))
+		return;
+
+	if (tbo_cache_put(aobj))
+		return;
 
 	amdgpu_hmm_unregister(aobj);
 	ttm_bo_put(&aobj->tbo);
 }
 
-int amdgpu_gem_object_create(struct amdgpu_device *adev, unsigned long size,
-			     int alignment, u32 initial_domain,
-			     u64 flags, enum ttm_bo_type type,
-			     struct dma_resv *resv,
-			     struct drm_gem_object **obj, int8_t xcp_id_plus1)
+/**
+ * amdgpu_gem_try_cpu_clear - Clear small VRAM buffers on CPU
+ * @bo: The buffer object to clear
+ *
+ * For small VRAM buffers that need clearing, it's often faster to map them
+ * to CPU and clear them there rather than submitting a GPU command.
+ * This is especially true for Vega's HBM2 where small GPU commands have
+ * high overhead. This avoids polluting the CPU cache by using a temporary
+ * atomic mapping.
+ *
+ * Returns true if successfully cleared, false if GPU clear is needed.
+ */
+static bool amdgpu_gem_try_cpu_clear(struct amdgpu_bo *bo)
 {
-	struct amdgpu_bo *bo;
-	struct amdgpu_bo_user *ubo;
-	struct amdgpu_bo_param bp;
+	/* LUT with gnu17 designated initialisers for clarity. */
+	static const u32 thresh_lut[] = {
+		[VEGA_VRAM_GREEN]  =  64 * 1024,
+		[VEGA_VRAM_YELLOW] = 128 * 1024,
+		[VEGA_VRAM_RED]    =  32 * 1024,
+	};
+	struct ttm_operation_ctx ctx = { .interruptible = true, .no_wait_gpu = false };
+	struct amdgpu_device *adev;
+	enum vega_vram_pressure_state ps;
+	u32 size, thresh;
+	void *cpu_addr;
 	int r;
 
-	memset(&bp, 0, sizeof(bp));
+	if (unlikely(!bo)) {
+		return false;
+	}
+
+	size = amdgpu_bo_size(bo);
+	adev = amdgpu_ttm_adev(bo->tbo.bdev);
+	ps   = amdgpu_vega_get_vram_pressure_state(adev);
+	thresh = min(thresh_lut[ps], 256U * 1024U);	/* hard cap */
+
+	if (size > thresh) {
+		return false;
+	}
+
+	if (!(bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) ||
+		(bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {
+		return false;
+		}
+
+		r = amdgpu_bo_reserve(bo, false);
+	if (unlikely(r)) {
+		return false;
+	}
+
+	amdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_VRAM);
+	r = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);
+	if (unlikely(r)) {
+		goto out_unreserve;
+	}
+
+	r = amdgpu_bo_kmap(bo, &cpu_addr);
+	if (unlikely(r)) {
+		goto out_unreserve;
+	}
+
+	memset(cpu_addr, 0, size);
+	amdgpu_bo_kunmap(bo);
+
+	out_unreserve:
+	amdgpu_bo_unreserve(bo);
+	return r == 0;
+}
+
+int amdgpu_gem_object_create(struct amdgpu_device *adev,
+							 unsigned long size, int alignment,
+							 u32 initial_domain, u64 flags,
+							 enum ttm_bo_type type, struct dma_resv *resv,
+							 struct drm_gem_object **obj,
+							 int8_t xcp_id_plus1)
+{
+	struct amdgpu_bo_param bp = { 0 };
+	struct amdgpu_bo_user *ubo = NULL;
+	struct amdgpu_bo *bo       = NULL;
+	u32   byte_align;
+	int   r;
+
+	if (WARN_ON(!adev || !obj))
+		return -EINVAL;
+
 	*obj = NULL;
-	flags |= AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE;
 
-	bp.size = size;
-	bp.byte_align = alignment;
-	bp.type = type;
-	bp.resv = resv;
+	if (!size ||
+		size > adev->gmc.mc_vram_size + adev->gmc.gart_size)
+		return -EINVAL;
+
+	size = ALIGN(size, PAGE_SIZE);
+
+	if (alignment < 0)
+		alignment = 0;
+
+	amdgpu_tbo_slab_ensure();
+
+	if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID)
+		amdgpu_vm_always_valid_key_enable();
+
+	bp.size             = size;
+	bp.byte_align       = alignment;
+	bp.type             = type;
+	bp.resv             = resv;
 	bp.preferred_domain = initial_domain;
-	bp.flags = flags;
-	bp.domain = initial_domain;
-	bp.bo_ptr_size = sizeof(struct amdgpu_bo);
-	bp.xcp_id_plus1 = xcp_id_plus1;
+	bp.flags            = flags;
+	bp.bo_ptr_size      = sizeof(struct amdgpu_bo);
+	bp.xcp_id_plus1     = xcp_id_plus1;
+	bp.domain           = initial_domain;
+
+	if (static_branch_unlikely(&vega_domain_key)) {
+		amdgpu_vega_optimize_buffer_placement(adev, NULL,
+											  size, flags,
+										&bp.domain);
+	}
+
+	bo = tbo_cache_try_get(size, flags, bp.domain, resv, alignment);
+	if (bo) {
+		bo->flags             = flags;
+		bo->preferred_domains = bp.domain;
+		bo->allowed_domains   = bp.domain;
+		if (bo->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)
+			bo->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
+
+		byte_align                = max(alignment, (int)PAGE_SIZE);
+		bo->tbo.page_alignment    = roundup_pow_of_two(byte_align) >>
+		PAGE_SHIFT;
 
-	r = amdgpu_bo_create_user(adev, &bp, &ubo);
-	if (r)
-		return r;
+		*obj = &bo->tbo.base;
+		return 0;
+	}
 
-	bo = &ubo->bo;
-	*obj = &bo->tbo.base;
+	if (ubo_slab)
+		ubo = kmem_cache_zalloc(ubo_slab, GFP_KERNEL | __GFP_NOWARN);
+
+	if (!ubo) {
+		r = amdgpu_bo_create(adev, &bp, &bo);
+		if (r)
+			return r;
+	} else {
+		r = amdgpu_bo_create_user(adev, &bp, &ubo);
+		if (r) {
+			kmem_cache_free(ubo_slab, ubo);
+			return r;
+		}
+		bo = &ubo->bo;
+	}
 
+	if (bo->flags & AMDGPU_GEM_CREATE_VRAM_CLEARED) {
+		if (amdgpu_gem_try_cpu_clear(bo))
+			bo->flags &= ~AMDGPU_GEM_CREATE_VRAM_CLEARED;
+	}
+
+	*obj = &bo->tbo.base;
 	return 0;
 }
 
 void amdgpu_gem_force_release(struct amdgpu_device *adev)
 {
-	struct drm_device *ddev = adev_to_drm(adev);
+	struct drm_device *ddev;
 	struct drm_file *file;
 
+	if (unlikely(!adev))
+		return;
+
+	ddev = adev_to_drm(adev);
+	if (unlikely(!ddev))
+		return;
+
 	mutex_lock(&ddev->filelist_mutex);
 
 	list_for_each_entry(file, &ddev->filelist, lhead) {
 		struct drm_gem_object *gobj;
 		int handle;
 
-		WARN_ONCE(1, "Still active user space clients!\n");
+		struct release_node {
+			struct list_head node;
+			struct drm_gem_object *obj;
+		};
+		LIST_HEAD(release_list);
+		struct release_node *n, *tmp;
+
+		#define OOM_BATCH_SIZE 32
+		struct drm_gem_object *oom_objs[OOM_BATCH_SIZE];
+		unsigned int oom_cnt = 0;
+
+		WARN_ONCE(1, "Force-releasing GEM objects for file %p\n", file);
+
 		spin_lock(&file->table_lock);
 		idr_for_each_entry(&file->object_idr, gobj, handle) {
-			WARN_ONCE(1, "And also active allocations!\n");
-			drm_gem_object_put(gobj);
+			if (!kref_get_unless_zero(&gobj->refcount))
+				continue;
+
+			n = kmalloc(sizeof(*n), GFP_ATOMIC);
+			if (likely(n)) {
+				n->obj = gobj;
+				list_add_tail(&n->node, &release_list);
+			} else if (oom_cnt < OOM_BATCH_SIZE) {
+				oom_objs[oom_cnt++] = gobj;
+			} else {
+				dev_warn_once(ddev->dev, "OOM in force_release, potential leak\n");
+				drm_gem_object_put(gobj);
+			}
 		}
 		idr_destroy(&file->object_idr);
 		spin_unlock(&file->table_lock);
+
+		list_for_each_entry_safe(n, tmp, &release_list, node) {
+			drm_gem_object_put(n->obj);
+			kfree(n);
+		}
+
+		for (unsigned int i = 0; i < oom_cnt; ++i)
+			drm_gem_object_put(oom_objs[i]);
 	}
 
 	mutex_unlock(&ddev->filelist_mutex);
 }
 
-/*
- * Call from drm_gem_handle_create which appear in both new and open ioctl
- * case.
- */
 static int amdgpu_gem_object_open(struct drm_gem_object *obj,
-				  struct drm_file *file_priv)
+								  struct drm_file *file_priv)
 {
-	struct amdgpu_bo *abo = gem_to_amdgpu_bo(obj);
-	struct amdgpu_device *adev = amdgpu_ttm_adev(abo->tbo.bdev);
-	struct amdgpu_fpriv *fpriv = file_priv->driver_priv;
-	struct amdgpu_vm *vm = &fpriv->vm;
+	struct amdgpu_bo *abo;
+	struct amdgpu_device *adev;
+	struct amdgpu_fpriv *fpriv;
+	struct amdgpu_vm *vm;
 	struct amdgpu_bo_va *bo_va;
 	struct mm_struct *mm;
-	int r;
+	int r = 0;
 
+	if (unlikely(!obj || !file_priv))
+		return -EINVAL;
+
+	abo = gem_to_amdgpu_bo(obj);
+	if (unlikely(!abo))
+		return -EINVAL;
+
+	adev = amdgpu_ttm_adev(abo->tbo.bdev);
+	fpriv = file_priv->driver_priv;
+	if (unlikely(!adev || !fpriv))
+		return -EINVAL;
+
+	vm = &fpriv->vm;
+
+	if (static_branch_unlikely(&vega_prefetch_key)) {
+		prefetch(abo);
+		prefetch(vm);
+		prefetch(abo->tbo.base.resv);
+	}
+
+	rcu_read_lock();
 	mm = amdgpu_ttm_tt_get_usermm(abo->tbo.ttm);
-	if (mm && mm != current->mm)
+	if (mm && mm != current->mm) {
+		rcu_read_unlock();
 		return -EPERM;
+	}
+	rcu_read_unlock();
 
-	if (abo->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID &&
-	    !amdgpu_vm_is_bo_always_valid(vm, abo))
+	if ((abo->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) &&
+		!amdgpu_vm_is_bo_always_valid(vm, abo))
 		return -EPERM;
 
+	if (static_branch_likely(&amdgpu_vm_always_valid_key) &&
+		(abo->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) &&
+		(abo->allowed_domains == AMDGPU_GEM_DOMAIN_GTT) &&
+		!abo->parent &&
+		(!obj->import_attach || !dma_buf_is_dynamic(obj->import_attach->dmabuf)))
+		return 0;
+
 	r = amdgpu_bo_reserve(abo, false);
-	if (r)
+	if (unlikely(r))
 		return r;
 
-	amdgpu_vm_bo_update_shared(abo);
 	bo_va = amdgpu_vm_bo_find(vm, abo);
-	if (!bo_va)
+	if (!bo_va) {
 		bo_va = amdgpu_vm_bo_add(adev, vm, abo);
-	else
-		++bo_va->ref_count;
+		if (IS_ERR(bo_va)) {
+			r = PTR_ERR(bo_va);
+			goto unreserve;
+		}
+	} else {
+		if (bo_va->ref_count < UINT_MAX)
+			bo_va->ref_count++;
+	}
+
 	amdgpu_bo_unreserve(abo);
 
-	/* Validate and add eviction fence to DMABuf imports with dynamic
-	 * attachment in compute VMs. Re-validation will be done by
-	 * amdgpu_vm_validate. Fences are on the reservation shared with the
-	 * export, which is currently required to be validated and fenced
-	 * already by amdgpu_amdkfd_gpuvm_restore_process_bos.
-	 *
-	 * Nested locking below for the case that a GEM object is opened in
-	 * kfd_mem_export_dmabuf. Since the lock below is only taken for imports,
-	 * but not for export, this is a different lock class that cannot lead to
-	 * circular lock dependencies.
-	 */
-	if (!vm->is_compute_context || !vm->process_info)
-		return 0;
-	if (!obj->import_attach ||
-	    !dma_buf_is_dynamic(obj->import_attach->dmabuf))
-		return 0;
-	mutex_lock_nested(&vm->process_info->lock, 1);
-	if (!WARN_ON(!vm->process_info->eviction_fence)) {
-		r = amdgpu_amdkfd_bo_validate_and_fence(abo, AMDGPU_GEM_DOMAIN_GTT,
-							&vm->process_info->eviction_fence->base);
-		if (r) {
+	if (vm->is_compute_context && vm->process_info &&
+		obj->import_attach && dma_buf_is_dynamic(obj->import_attach->dmabuf)) {
+		mutex_lock_nested(&vm->process_info->lock, 1);
+	if (vm->process_info->eviction_fence) {
+		r = amdgpu_amdkfd_bo_validate_and_fence(
+			abo, AMDGPU_GEM_DOMAIN_GTT,
+			&vm->process_info->eviction_fence->base);
+
+		if (unlikely(r)) {
 			struct amdgpu_task_info *ti = amdgpu_vm_get_task_info_vm(vm);
 
 			dev_warn(adev->dev, "validate_and_fence failed: %d\n", r);
@@ -217,77 +1363,115 @@ static int amdgpu_gem_object_open(struct
 		}
 	}
 	mutex_unlock(&vm->process_info->lock);
+		}
 
-	return r;
+		return r;
+
+		unreserve:
+		amdgpu_bo_unreserve(abo);
+		return r;
 }
 
-static void amdgpu_gem_object_close(struct drm_gem_object *obj,
-				    struct drm_file *file_priv)
+static void
+amdgpu_gem_object_close(struct drm_gem_object *obj, struct drm_file *file_priv)
 {
-	struct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);
-	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);
-	struct amdgpu_fpriv *fpriv = file_priv->driver_priv;
-	struct amdgpu_vm *vm = &fpriv->vm;
-
+	struct amdgpu_bo *bo;
+	struct amdgpu_device *adev;
+	struct amdgpu_fpriv *fpriv;
+	struct amdgpu_vm *vm;
 	struct dma_fence *fence = NULL;
 	struct amdgpu_bo_va *bo_va;
 	struct drm_exec exec;
-	long r;
+	long r = 0;
+	bool use_async = false;
+
+	if (unlikely(!obj || !file_priv))
+		return;
+
+	bo = gem_to_amdgpu_bo(obj);
+	if (unlikely(!bo))
+		return;
+
+	fpriv = file_priv->driver_priv;
+	if (unlikely(!fpriv))
+		return;
+
+	adev = amdgpu_ttm_adev(bo->tbo.bdev);
+	if (unlikely(!adev))
+		return;
+
+	vm = &fpriv->vm;
+
+	if (static_branch_unlikely(&vega_prefetch_key)) {
+		prefetchw(bo->tbo.base.resv);
+		prefetch(vm);
+	}
+
+	if (is_hbm2_vega(adev)) {
+		use_async = (bo->flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) ||
+		(amdgpu_bo_size(bo) < (256ULL << 10));
+	}
 
 	drm_exec_init(&exec, DRM_EXEC_IGNORE_DUPLICATES, 0);
 	drm_exec_until_all_locked(&exec) {
-		r = drm_exec_prepare_obj(&exec, &bo->tbo.base, 1);
+		r = amdgpu_vm_lock_pd(vm, &exec, 1);
 		drm_exec_retry_on_contention(&exec);
 		if (unlikely(r))
 			goto out_unlock;
 
-		r = amdgpu_vm_lock_pd(vm, &exec, 0);
+		r = drm_exec_lock_obj(&exec, &bo->tbo.base);
 		drm_exec_retry_on_contention(&exec);
 		if (unlikely(r))
 			goto out_unlock;
 	}
 
 	bo_va = amdgpu_vm_bo_find(vm, bo);
-	if (!bo_va || --bo_va->ref_count)
+	if (!bo_va || bo_va->ref_count == 0)
+		goto out_unlock;
+
+	if (--bo_va->ref_count > 0)
 		goto out_unlock;
 
 	amdgpu_vm_bo_del(adev, bo_va);
 	amdgpu_vm_bo_update_shared(bo);
-	if (!amdgpu_vm_ready(vm))
-		goto out_unlock;
 
-	r = amdgpu_vm_clear_freed(adev, vm, &fence);
-	if (unlikely(r < 0))
-		dev_err(adev->dev, "failed to clear page "
-			"tables on GEM object close (%ld)\n", r);
-	if (r || !fence)
-		goto out_unlock;
+	if (amdgpu_vm_ready(vm)) {
+		r = amdgpu_vm_clear_freed(adev, vm, &fence);
+		if (unlikely(r < 0)) {
+			dev_err(adev->dev, "failed to clear page tables on GEM close (%ld)\n", r);
+			goto out_unlock;
+		}
+	}
 
-	amdgpu_bo_fence(bo, fence, true);
-	dma_fence_put(fence);
+	if (fence) {
+		amdgpu_bo_fence(bo, fence, use_async);
+		dma_fence_put(fence);
+	}
 
-out_unlock:
-	if (r)
-		dev_err(adev->dev, "leaking bo va (%ld)\n", r);
+	out_unlock:
+	if (unlikely(r)) {
+		dev_err(adev->dev, "Error in GEM object close for pid %d (%ld)\n",
+				task_pid_nr(current), r);
+	}
 	drm_exec_fini(&exec);
 }
 
 static int amdgpu_gem_object_mmap(struct drm_gem_object *obj, struct vm_area_struct *vma)
 {
-	struct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);
+	struct amdgpu_bo *bo;
 
-	if (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm))
-		return -EPERM;
-	if (bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)
+	if (unlikely(!obj))
+		return -EINVAL;
+
+	bo = gem_to_amdgpu_bo(obj);
+	if (unlikely(!bo))
+		return -EINVAL;
+
+	if (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm) ||
+		(bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS))
 		return -EPERM;
 
-	/* Workaround for Thunk bug creating PROT_NONE,MAP_PRIVATE mappings
-	 * for debugger access to invisible VRAM. Should have used MAP_SHARED
-	 * instead. Clearing VM_MAYWRITE prevents the mapping from ever
-	 * becoming writable and makes is_cow_mapping(vm_flags) false.
-	 */
-	if (is_cow_mapping(vma->vm_flags) &&
-	    !(vma->vm_flags & VM_ACCESS_FLAGS))
+	if (is_cow_mapping(vma->vm_flags) && !(vma->vm_flags & VM_ACCESS_FLAGS))
 		vm_flags_clear(vma, VM_MAYWRITE);
 
 	return drm_gem_ttm_mmap(obj, vma);
@@ -304,106 +1488,102 @@ const struct drm_gem_object_funcs amdgpu
 	.vm_ops = &amdgpu_gem_vm_ops,
 };
 
-/*
- * GEM ioctls.
- */
 int amdgpu_gem_create_ioctl(struct drm_device *dev, void *data,
-			    struct drm_file *filp)
+							struct drm_file *filp)
 {
-	struct amdgpu_device *adev = drm_to_adev(dev);
-	struct amdgpu_fpriv *fpriv = filp->driver_priv;
-	struct amdgpu_vm *vm = &fpriv->vm;
+	struct amdgpu_device *adev;
+	struct amdgpu_fpriv *fpriv;
+	struct amdgpu_vm *vm;
 	union drm_amdgpu_gem_create *args = data;
-	uint64_t flags = args->in.domain_flags;
-	uint64_t size = args->in.bo_size;
+	uint64_t flags, size;
 	struct dma_resv *resv = NULL;
-	struct drm_gem_object *gobj;
-	uint32_t handle, initial_domain;
-	int r;
+	struct drm_gem_object *gobj = NULL;
+	uint32_t handle;
+	int r = 0;
+	unsigned int i;
 
-	/* reject DOORBELLs until userspace code to use it is available */
-	if (args->in.domains & AMDGPU_GEM_DOMAIN_DOORBELL)
+	if (unlikely(!dev || !data || !filp))
 		return -EINVAL;
 
-	/* reject invalid gem flags */
-	if (flags & ~(AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |
-		      AMDGPU_GEM_CREATE_NO_CPU_ACCESS |
-		      AMDGPU_GEM_CREATE_CPU_GTT_USWC |
-		      AMDGPU_GEM_CREATE_VRAM_CLEARED |
-		      AMDGPU_GEM_CREATE_VM_ALWAYS_VALID |
-		      AMDGPU_GEM_CREATE_EXPLICIT_SYNC |
-		      AMDGPU_GEM_CREATE_ENCRYPTED |
-		      AMDGPU_GEM_CREATE_GFX12_DCC |
-		      AMDGPU_GEM_CREATE_DISCARDABLE))
+	adev = drm_to_adev(dev);
+	fpriv = filp->driver_priv;
+	if (unlikely(!adev || !fpriv))
 		return -EINVAL;
 
-	/* reject invalid gem domains */
-	if (args->in.domains & ~AMDGPU_GEM_DOMAIN_MASK)
+	vm = &fpriv->vm;
+	flags = args->in.domain_flags;
+	size = args->in.bo_size;
+
+	if (unlikely(args->in.domains & AMDGPU_GEM_DOMAIN_DOORBELL ||
+		args->in.domains & ~AMDGPU_GEM_DOMAIN_MASK))
 		return -EINVAL;
 
-	if (!amdgpu_is_tmz(adev) && (flags & AMDGPU_GEM_CREATE_ENCRYPTED)) {
-		DRM_NOTE_ONCE("Cannot allocate secure buffer since TMZ is disabled\n");
+	if (unlikely(flags & ~(AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |
+		AMDGPU_GEM_CREATE_NO_CPU_ACCESS |
+		AMDGPU_GEM_CREATE_CPU_GTT_USWC |
+		AMDGPU_GEM_CREATE_VRAM_CLEARED |
+		AMDGPU_GEM_CREATE_VM_ALWAYS_VALID |
+		AMDGPU_GEM_CREATE_EXPLICIT_SYNC |
+		AMDGPU_GEM_CREATE_ENCRYPTED |
+		AMDGPU_GEM_CREATE_GFX12_DCC |
+		AMDGPU_GEM_CREATE_DISCARDABLE)))
 		return -EINVAL;
-	}
 
-	/* always clear VRAM */
-	flags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;
+	if (unlikely(!amdgpu_is_tmz(adev) && (flags & AMDGPU_GEM_CREATE_ENCRYPTED)))
+		return -EINVAL;
 
-	/* create a gem object to contain this object in */
 	if (args->in.domains & (AMDGPU_GEM_DOMAIN_GDS |
-	    AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA)) {
-		if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
-			/* if gds bo is created from user space, it must be
-			 * passed to bo list
-			 */
-			DRM_ERROR("GDS bo cannot be per-vm-bo\n");
-			return -EINVAL;
-		}
+		AMDGPU_GEM_DOMAIN_GWS |
+		AMDGPU_GEM_DOMAIN_OA))
 		flags |= AMDGPU_GEM_CREATE_NO_CPU_ACCESS;
-	}
 
 	if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
 		r = amdgpu_bo_reserve(vm->root.bo, false);
-		if (r)
+		if (unlikely(r))
 			return r;
-
 		resv = vm->root.bo->tbo.base.resv;
 	}
 
-	initial_domain = (u32)(0xffffffff & args->in.domains);
-retry:
-	r = amdgpu_gem_object_create(adev, size, args->in.alignment,
-				     initial_domain,
-				     flags, ttm_bo_type_device, resv, &gobj, fpriv->xcp_id + 1);
-	if (r && r != -ERESTARTSYS) {
-		if (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {
-			flags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
-			goto retry;
-		}
-
-		if (initial_domain == AMDGPU_GEM_DOMAIN_VRAM) {
-			initial_domain |= AMDGPU_GEM_DOMAIN_GTT;
-			goto retry;
+	uint32_t alloc_domain = (u32)(args->in.domains);
+
+	for (i = 0; i < 2; ++i) {
+		r = amdgpu_gem_object_create(adev, size, args->in.alignment,
+									 alloc_domain, flags,
+							   ttm_bo_type_device, resv, &gobj,
+							   fpriv->xcp_id + 1);
+
+		if (likely(!r)) {
+			/* Success on the first or second attempt. */
+			break;
+		} else {
+			/* Allocation failed, check if we can retry. */
+			if (r == -ENOMEM && (alloc_domain & AMDGPU_GEM_DOMAIN_VRAM)) {
+				/* Failed in VRAM due to no space. Try GTT-only next. */
+				alloc_domain = AMDGPU_GEM_DOMAIN_GTT;
+				continue;
+			} else {
+				/* Any other error is unrecoverable, exit the loop. */
+				goto out_unreserve;
+			}
 		}
-		DRM_DEBUG("Failed to allocate GEM object (%llu, %d, %llu, %d)\n",
-				size, initial_domain, args->in.alignment, r);
 	}
 
+	out_unreserve:
 	if (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {
-		if (!r) {
+		if (likely(!r)) {
 			struct amdgpu_bo *abo = gem_to_amdgpu_bo(gobj);
 
-			abo->parent = amdgpu_bo_ref(vm->root.bo);
+			if (likely(abo))
+				abo->parent = amdgpu_bo_ref(vm->root.bo);
 		}
 		amdgpu_bo_unreserve(vm->root.bo);
 	}
-	if (r)
+	if (unlikely(r))
 		return r;
 
 	r = drm_gem_handle_create(filp, gobj, &handle);
-	/* drop reference from allocate - handle holds it now */
 	drm_gem_object_put(gobj);
-	if (r)
+	if (unlikely(r))
 		return r;
 
 	memset(args, 0, sizeof(*args));
@@ -412,273 +1592,240 @@ retry:
 }
 
 int amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,
-			     struct drm_file *filp)
+							 struct drm_file *filp)
 {
 	struct ttm_operation_ctx ctx = { true, false };
-	struct amdgpu_device *adev = drm_to_adev(dev);
+	struct amdgpu_device *adev;
 	struct drm_amdgpu_gem_userptr *args = data;
-	struct amdgpu_fpriv *fpriv = filp->driver_priv;
+	struct amdgpu_fpriv *fpriv;
 	struct drm_gem_object *gobj;
-	struct hmm_range *range;
+	struct hmm_range *range = NULL;
 	struct amdgpu_bo *bo;
 	uint32_t handle;
 	int r;
 
-	args->addr = untagged_addr(args->addr);
+	if (unlikely(!dev || !data || !filp))
+		return -EINVAL;
 
-	if (offset_in_page(args->addr | args->size))
+	adev = drm_to_adev(dev);
+	fpriv = filp->driver_priv;
+	if (unlikely(!adev || !fpriv))
 		return -EINVAL;
 
-	/* reject unknown flag values */
-	if (args->flags & ~(AMDGPU_GEM_USERPTR_READONLY |
-	    AMDGPU_GEM_USERPTR_ANONONLY | AMDGPU_GEM_USERPTR_VALIDATE |
-	    AMDGPU_GEM_USERPTR_REGISTER))
+	args->addr = untagged_addr(args->addr);
+
+	if (unlikely(offset_in_page(args->addr | args->size) || !args->size))
 		return -EINVAL;
 
-	if (!(args->flags & AMDGPU_GEM_USERPTR_READONLY) &&
-	     !(args->flags & AMDGPU_GEM_USERPTR_REGISTER)) {
+	if (unlikely(args->flags & ~(AMDGPU_GEM_USERPTR_READONLY |
+		AMDGPU_GEM_USERPTR_ANONONLY |
+		AMDGPU_GEM_USERPTR_VALIDATE |
+		AMDGPU_GEM_USERPTR_REGISTER)))
+		return -EINVAL;
 
-		/* if we want to write to it we must install a MMU notifier */
+	if (unlikely(!(args->flags & AMDGPU_GEM_USERPTR_READONLY) &&
+		!(args->flags & AMDGPU_GEM_USERPTR_REGISTER)))
 		return -EACCES;
-	}
 
-	/* create a gem object to contain this object in */
 	r = amdgpu_gem_object_create(adev, args->size, 0, AMDGPU_GEM_DOMAIN_CPU,
-				     0, ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
-	if (r)
+								 0, ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
+	if (unlikely(r))
 		return r;
 
 	bo = gem_to_amdgpu_bo(gobj);
 	bo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;
 	bo->allowed_domains = AMDGPU_GEM_DOMAIN_GTT;
+
 	r = amdgpu_ttm_tt_set_userptr(&bo->tbo, args->addr, args->flags);
-	if (r)
+	if (unlikely(r))
 		goto release_object;
 
 	r = amdgpu_hmm_register(bo, args->addr);
-	if (r)
+	if (unlikely(r))
 		goto release_object;
 
 	if (args->flags & AMDGPU_GEM_USERPTR_VALIDATE) {
-		r = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages,
-						 &range);
-		if (r)
+		r = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages, &range);
+		if (unlikely(r))
 			goto release_object;
 
 		r = amdgpu_bo_reserve(bo, true);
-		if (r)
+		if (unlikely(r))
 			goto user_pages_done;
 
 		amdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);
 		r = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);
 		amdgpu_bo_unreserve(bo);
-		if (r)
+		if (unlikely(r))
 			goto user_pages_done;
 	}
 
 	r = drm_gem_handle_create(filp, gobj, &handle);
-	if (r)
+	if (unlikely(r))
 		goto user_pages_done;
 
 	args->handle = handle;
 
-user_pages_done:
-	if (args->flags & AMDGPU_GEM_USERPTR_VALIDATE)
+	user_pages_done:
+	if ((args->flags & AMDGPU_GEM_USERPTR_VALIDATE) && range)
 		amdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, range);
 
-release_object:
+	release_object:
 	drm_gem_object_put(gobj);
-
 	return r;
 }
 
 int amdgpu_mode_dumb_mmap(struct drm_file *filp,
-			  struct drm_device *dev,
-			  uint32_t handle, uint64_t *offset_p)
+						  struct drm_device *dev,
+						  uint32_t handle, uint64_t *offset_p)
 {
 	struct drm_gem_object *gobj;
 	struct amdgpu_bo *robj;
 
+	if (unlikely(!filp || !dev || !offset_p))
+		return -EINVAL;
+
 	gobj = drm_gem_object_lookup(filp, handle);
-	if (!gobj)
+	if (unlikely(!gobj))
 		return -ENOENT;
 
 	robj = gem_to_amdgpu_bo(gobj);
-	if (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm) ||
-	    (robj->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {
+	if (unlikely(!robj)) {
 		drm_gem_object_put(gobj);
-		return -EPERM;
+		return -EINVAL;
 	}
-	*offset_p = amdgpu_bo_mmap_offset(robj);
-	drm_gem_object_put(gobj);
-	return 0;
+
+	if (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm) ||
+		(robj->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {
+		drm_gem_object_put(gobj);
+	return -EPERM;
+		}
+		*offset_p = amdgpu_bo_mmap_offset(robj);
+		drm_gem_object_put(gobj);
+		return 0;
 }
 
 int amdgpu_gem_mmap_ioctl(struct drm_device *dev, void *data,
-			  struct drm_file *filp)
+						  struct drm_file *filp)
 {
 	union drm_amdgpu_gem_mmap *args = data;
-	uint32_t handle = args->in.handle;
+	uint32_t handle;
+
+	if (unlikely(!dev || !data || !filp))
+		return -EINVAL;
 
+	handle = args->in.handle;
 	memset(args, 0, sizeof(*args));
 	return amdgpu_mode_dumb_mmap(filp, dev, handle, &args->out.addr_ptr);
 }
 
-/**
- * amdgpu_gem_timeout - calculate jiffies timeout from absolute value
- *
- * @timeout_ns: timeout in ns
- *
- * Calculate the timeout in jiffies from an absolute timeout in ns.
- */
-unsigned long amdgpu_gem_timeout(uint64_t timeout_ns)
-{
-	unsigned long timeout_jiffies;
-	ktime_t timeout;
-
-	/* clamp timeout if it's to large */
-	if (((int64_t)timeout_ns) < 0)
-		return MAX_SCHEDULE_TIMEOUT;
-
-	timeout = ktime_sub(ns_to_ktime(timeout_ns), ktime_get());
-	if (ktime_to_ns(timeout) < 0)
-		return 0;
-
-	timeout_jiffies = nsecs_to_jiffies(ktime_to_ns(timeout));
-	/*  clamp timeout to avoid unsigned-> signed overflow */
-	if (timeout_jiffies > MAX_SCHEDULE_TIMEOUT)
-		return MAX_SCHEDULE_TIMEOUT - 1;
-
-	return timeout_jiffies;
-}
-
 int amdgpu_gem_wait_idle_ioctl(struct drm_device *dev, void *data,
-			      struct drm_file *filp)
+							   struct drm_file *filp)
 {
 	union drm_amdgpu_gem_wait_idle *args = data;
 	struct drm_gem_object *gobj;
 	struct amdgpu_bo *robj;
-	uint32_t handle = args->in.handle;
-	unsigned long timeout = amdgpu_gem_timeout(args->in.timeout);
+	uint32_t handle;
+	unsigned long timeout;
 	int r = 0;
 	long ret;
 
+	if (unlikely(!dev || !data || !filp))
+		return -EINVAL;
+
+	handle = args->in.handle;
+	timeout = amdgpu_gem_timeout(args->in.timeout);
+
 	gobj = drm_gem_object_lookup(filp, handle);
-	if (!gobj)
+	if (unlikely(!gobj))
 		return -ENOENT;
 
 	robj = gem_to_amdgpu_bo(gobj);
+	if (unlikely(!robj)) {
+		drm_gem_object_put(gobj);
+		return -EINVAL;
+	}
+
 	ret = dma_resv_wait_timeout(robj->tbo.base.resv, DMA_RESV_USAGE_READ,
-				    true, timeout);
+								true, timeout);
 
-	/* ret == 0 means not signaled,
-	 * ret > 0 means signaled
-	 * ret < 0 means interrupted before timeout
-	 */
 	if (ret >= 0) {
 		memset(args, 0, sizeof(*args));
 		args->out.status = (ret == 0);
-	} else
+	} else {
 		r = ret;
+	}
 
 	drm_gem_object_put(gobj);
 	return r;
 }
 
 int amdgpu_gem_metadata_ioctl(struct drm_device *dev, void *data,
-				struct drm_file *filp)
+							  struct drm_file *filp)
 {
 	struct drm_amdgpu_gem_metadata *args = data;
 	struct drm_gem_object *gobj;
 	struct amdgpu_bo *robj;
-	int r = -1;
+	int r = 0;
+
+	if (unlikely(!dev || !data || !filp))
+		return -EINVAL;
 
-	DRM_DEBUG("%d\n", args->handle);
 	gobj = drm_gem_object_lookup(filp, args->handle);
-	if (gobj == NULL)
+	if (unlikely(!gobj))
 		return -ENOENT;
+
 	robj = gem_to_amdgpu_bo(gobj);
+	if (unlikely(!robj)) {
+		drm_gem_object_put(gobj);
+		return -EINVAL;
+	}
 
 	r = amdgpu_bo_reserve(robj, false);
 	if (unlikely(r != 0))
 		goto out;
 
-	if (args->op == AMDGPU_GEM_METADATA_OP_GET_METADATA) {
-		amdgpu_bo_get_tiling_flags(robj, &args->data.tiling_info);
-		r = amdgpu_bo_get_metadata(robj, args->data.data,
-					   sizeof(args->data.data),
-					   &args->data.data_size_bytes,
-					   &args->data.flags);
-	} else if (args->op == AMDGPU_GEM_METADATA_OP_SET_METADATA) {
-		if (args->data.data_size_bytes > sizeof(args->data.data)) {
+	switch (args->op) {
+		case AMDGPU_GEM_METADATA_OP_GET_METADATA:
+			amdgpu_bo_get_tiling_flags(robj, &args->data.tiling_info);
+			r = amdgpu_bo_get_metadata(robj, args->data.data,
+									   sizeof(args->data.data),
+									   &args->data.data_size_bytes,
+							  &args->data.flags);
+			break;
+
+		case AMDGPU_GEM_METADATA_OP_SET_METADATA:
+			if (unlikely(args->data.data_size_bytes > sizeof(args->data.data))) {
+				r = -EINVAL;
+				break;
+			}
+			r = amdgpu_bo_set_tiling_flags(robj, args->data.tiling_info);
+
+			if (!r) {
+				r = amdgpu_bo_set_metadata(robj, args->data.data,
+										   args->data.data_size_bytes,
+							   args->data.flags);
+			}
+			break;
+
+		default:
 			r = -EINVAL;
-			goto unreserve;
-		}
-		r = amdgpu_bo_set_tiling_flags(robj, args->data.tiling_info);
-		if (!r)
-			r = amdgpu_bo_set_metadata(robj, args->data.data,
-						   args->data.data_size_bytes,
-						   args->data.flags);
+			break;
 	}
 
-unreserve:
 	amdgpu_bo_unreserve(robj);
-out:
+	out:
 	drm_gem_object_put(gobj);
 	return r;
 }
 
-/**
- * amdgpu_gem_va_update_vm -update the bo_va in its VM
- *
- * @adev: amdgpu_device pointer
- * @vm: vm to update
- * @bo_va: bo_va to update
- * @operation: map, unmap or clear
- *
- * Update the bo_va directly after setting its address. Errors are not
- * vital here, so they are not reported back to userspace.
- */
-static void amdgpu_gem_va_update_vm(struct amdgpu_device *adev,
-				    struct amdgpu_vm *vm,
-				    struct amdgpu_bo_va *bo_va,
-				    uint32_t operation)
-{
-	int r;
-
-	if (!amdgpu_vm_ready(vm))
-		return;
-
-	r = amdgpu_vm_clear_freed(adev, vm, NULL);
-	if (r)
-		goto error;
-
-	if (operation == AMDGPU_VA_OP_MAP ||
-	    operation == AMDGPU_VA_OP_REPLACE) {
-		r = amdgpu_vm_bo_update(adev, bo_va, false);
-		if (r)
-			goto error;
-	}
-
-	r = amdgpu_vm_update_pdes(adev, vm, false);
-
-error:
-	if (r && r != -ERESTARTSYS)
-		DRM_ERROR("Couldn't update BO_VA (%d)\n", r);
-}
-
-/**
- * amdgpu_gem_va_map_flags - map GEM UAPI flags into hardware flags
- *
- * @adev: amdgpu_device pointer
- * @flags: GEM UAPI flags
- *
- * Returns the GEM UAPI flags mapped into hardware for the ASIC.
- */
 uint64_t amdgpu_gem_va_map_flags(struct amdgpu_device *adev, uint32_t flags)
 {
 	uint64_t pte_flag = 0;
 
+	if (unlikely(!adev))
+		return 0;
+
 	if (flags & AMDGPU_VM_PAGE_EXECUTABLE)
 		pte_flag |= AMDGPU_PTE_EXECUTABLE;
 	if (flags & AMDGPU_VM_PAGE_READABLE)
@@ -690,292 +1837,331 @@ uint64_t amdgpu_gem_va_map_flags(struct
 	if (flags & AMDGPU_VM_PAGE_NOALLOC)
 		pte_flag |= AMDGPU_PTE_NOALLOC;
 
-	if (adev->gmc.gmc_funcs->map_mtype)
-		pte_flag |= amdgpu_gmc_map_mtype(adev,
-						 flags & AMDGPU_VM_MTYPE_MASK);
+	if (likely(adev->gmc.gmc_funcs && adev->gmc.gmc_funcs->map_mtype))
+		pte_flag |= amdgpu_gmc_map_mtype(adev, flags & AMDGPU_VM_MTYPE_MASK);
 
 	return pte_flag;
 }
 
-int amdgpu_gem_va_ioctl(struct drm_device *dev, void *data,
-			  struct drm_file *filp)
+static int amdgpu_gem_va_update_vm(struct amdgpu_device *adev,
+								   struct amdgpu_vm *vm,
+								   struct amdgpu_bo_va *bo_va,
+								   uint32_t operation)
 {
-	const uint32_t valid_flags = AMDGPU_VM_DELAY_UPDATE |
-		AMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE |
-		AMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_MTYPE_MASK |
-		AMDGPU_VM_PAGE_NOALLOC;
-	const uint32_t prt_flags = AMDGPU_VM_DELAY_UPDATE |
-		AMDGPU_VM_PAGE_PRT;
+	int r;
+
+	if (unlikely(!adev || !vm))
+		return -EINVAL;
+
+	if (!amdgpu_vm_ready(vm))
+		return 0;
+
+	r = amdgpu_vm_clear_freed(adev, vm, NULL);
+	if (unlikely(r))
+		goto error;
+
+	if (operation == AMDGPU_VA_OP_MAP || operation == AMDGPU_VA_OP_REPLACE) {
+		r = amdgpu_vm_bo_update(adev, bo_va, false);
+		if (unlikely(r))
+			goto error;
+	}
+
+	r = amdgpu_vm_update_pdes(adev, vm, false);
+	if (unlikely(r))
+		goto error;
+
+	return 0;
+
+	error:
+	if (r && r != -ERESTARTSYS)
+		DRM_ERROR("Couldn't update BO_VA (%d)\n", r);
+	return r;
+}
 
+int amdgpu_gem_va_ioctl(struct drm_device *dev, void *data,
+						struct drm_file *filp)
+{
+	struct amdgpu_device *adev;
+	struct amdgpu_fpriv *fpriv;
+	struct amdgpu_vm *vm;
 	struct drm_amdgpu_gem_va *args = data;
-	struct drm_gem_object *gobj;
-	struct amdgpu_device *adev = drm_to_adev(dev);
-	struct amdgpu_fpriv *fpriv = filp->driver_priv;
-	struct amdgpu_bo *abo;
-	struct amdgpu_bo_va *bo_va;
+	struct drm_gem_object *gobj = NULL;
+	struct amdgpu_bo *abo = NULL;
+	struct amdgpu_bo_va *bo_va = NULL;
 	struct drm_exec exec;
-	uint64_t va_flags;
-	uint64_t vm_size;
+	uint64_t va_end;
+	uint64_t map_flags;
 	int r = 0;
 
-	if (args->va_address < AMDGPU_VA_RESERVED_BOTTOM) {
-		dev_dbg(dev->dev,
-			"va_address 0x%llx is in reserved area 0x%llx\n",
-			args->va_address, AMDGPU_VA_RESERVED_BOTTOM);
+	if (unlikely(!dev || !data || !filp))
 		return -EINVAL;
-	}
 
-	if (args->va_address >= AMDGPU_GMC_HOLE_START &&
-	    args->va_address < AMDGPU_GMC_HOLE_END) {
-		dev_dbg(dev->dev,
-			"va_address 0x%llx is in VA hole 0x%llx-0x%llx\n",
-			args->va_address, AMDGPU_GMC_HOLE_START,
-			AMDGPU_GMC_HOLE_END);
+	adev = drm_to_adev(dev);
+	fpriv = filp->driver_priv;
+	if (unlikely(!adev || !fpriv))
 		return -EINVAL;
-	}
 
-	args->va_address &= AMDGPU_GMC_HOLE_MASK;
+	vm = &fpriv->vm;
 
-	vm_size = adev->vm_manager.max_pfn * AMDGPU_GPU_PAGE_SIZE;
-	vm_size -= AMDGPU_VA_RESERVED_TOP;
-	if (args->va_address + args->map_size > vm_size) {
-		dev_dbg(dev->dev,
-			"va_address 0x%llx is in top reserved area 0x%llx\n",
-			args->va_address + args->map_size, vm_size);
+	const u32 VALID_FLAGS = AMDGPU_VM_DELAY_UPDATE |
+	AMDGPU_VM_PAGE_READABLE   | AMDGPU_VM_PAGE_WRITEABLE |
+	AMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_MTYPE_MASK     |
+	AMDGPU_VM_PAGE_NOALLOC;
+	const u32 PRT_FLAGS = AMDGPU_VM_DELAY_UPDATE | AMDGPU_VM_PAGE_PRT;
+
+	if (unlikely(args->operation > AMDGPU_VA_OP_REPLACE ||
+		args->va_address < AMDGPU_VA_RESERVED_BOTTOM ||
+		(args->va_address >= AMDGPU_GMC_HOLE_START &&
+		args->va_address <  AMDGPU_GMC_HOLE_END) ||
+		((args->flags & ~VALID_FLAGS) && (args->flags & ~PRT_FLAGS))))
 		return -EINVAL;
-	}
 
-	if ((args->flags & ~valid_flags) && (args->flags & ~prt_flags)) {
-		dev_dbg(dev->dev, "invalid flags combination 0x%08X\n",
-			args->flags);
-		return -EINVAL;
-	}
-
-	switch (args->operation) {
-	case AMDGPU_VA_OP_MAP:
-	case AMDGPU_VA_OP_UNMAP:
-	case AMDGPU_VA_OP_CLEAR:
-	case AMDGPU_VA_OP_REPLACE:
-		break;
-	default:
-		dev_dbg(dev->dev, "unsupported operation %d\n",
-			args->operation);
+	args->va_address &= AMDGPU_GMC_HOLE_MASK;
+	uint64_t vm_size = (adev->vm_manager.max_pfn * AMDGPU_GPU_PAGE_SIZE) -
+	AMDGPU_VA_RESERVED_TOP;
+	if (unlikely(check_add_overflow(args->va_address, args->map_size, &va_end) ||
+		va_end > vm_size))
 		return -EINVAL;
-	}
 
 	if ((args->operation != AMDGPU_VA_OP_CLEAR) &&
-	    !(args->flags & AMDGPU_VM_PAGE_PRT)) {
+		!(args->flags & AMDGPU_VM_PAGE_PRT)) {
 		gobj = drm_gem_object_lookup(filp, args->handle);
-		if (gobj == NULL)
-			return -ENOENT;
-		abo = gem_to_amdgpu_bo(gobj);
-	} else {
-		gobj = NULL;
-		abo = NULL;
+
+	if (unlikely(!gobj)) {
+		return -ENOENT;
 	}
+	abo = gem_to_amdgpu_bo(gobj);
+		}
 
-	drm_exec_init(&exec, DRM_EXEC_INTERRUPTIBLE_WAIT |
-		      DRM_EXEC_IGNORE_DUPLICATES, 0);
-	drm_exec_until_all_locked(&exec) {
-		if (gobj) {
-			r = drm_exec_lock_obj(&exec, gobj);
+		drm_exec_init(&exec, DRM_EXEC_INTERRUPTIBLE_WAIT | DRM_EXEC_IGNORE_DUPLICATES, 0);
+		drm_exec_until_all_locked(&exec) {
+			r = drm_exec_lock_obj(&exec, &vm->root.bo->tbo.base);
 			drm_exec_retry_on_contention(&exec);
 			if (unlikely(r))
-				goto error;
-		}
+				goto out_exec;
 
-		r = amdgpu_vm_lock_pd(&fpriv->vm, &exec, 2);
-		drm_exec_retry_on_contention(&exec);
-		if (unlikely(r))
-			goto error;
-	}
+			if (abo) {
+				r = drm_exec_lock_obj(&exec, gobj);
+				drm_exec_retry_on_contention(&exec);
+				if (unlikely(r))
+					goto out_exec;
+			}
+		}
 
-	if (abo) {
-		bo_va = amdgpu_vm_bo_find(&fpriv->vm, abo);
+		bo_va = abo ? amdgpu_vm_bo_find(vm, abo) : fpriv->prt_va;
 		if (!bo_va) {
-			r = -ENOENT;
-			goto error;
+			if (abo && args->operation == AMDGPU_VA_OP_MAP) {
+				bo_va = amdgpu_vm_bo_add(adev, vm, abo);
+				if (IS_ERR(bo_va)) {
+					r = PTR_ERR(bo_va);
+					goto out_exec;
+				}
+			} else {
+				r = -ENOENT;
+				goto out_exec;
+			}
+		}
+
+		switch (args->operation) {
+			case AMDGPU_VA_OP_MAP:
+				map_flags = amdgpu_gem_va_map_flags(adev, args->flags);
+				r = amdgpu_vm_bo_map(adev, bo_va, args->va_address,
+									 args->offset_in_bo, args->map_size, map_flags);
+				break;
+			case AMDGPU_VA_OP_UNMAP:
+				r = amdgpu_vm_bo_unmap(adev, bo_va, args->va_address);
+				break;
+			case AMDGPU_VA_OP_CLEAR:
+				r = amdgpu_vm_bo_clear_mappings(adev, vm, args->va_address, args->map_size);
+				break;
+			case AMDGPU_VA_OP_REPLACE:
+				map_flags = amdgpu_gem_va_map_flags(adev, args->flags);
+				r = amdgpu_vm_bo_replace_map(adev, bo_va, args->va_address,
+											 args->offset_in_bo, args->map_size, map_flags);
+				break;
+			default:
+				r = -EINVAL;
+				break;
 		}
-	} else if (args->operation != AMDGPU_VA_OP_CLEAR) {
-		bo_va = fpriv->prt_va;
-	} else {
-		bo_va = NULL;
-	}
 
-	switch (args->operation) {
-	case AMDGPU_VA_OP_MAP:
-		va_flags = amdgpu_gem_va_map_flags(adev, args->flags);
-		r = amdgpu_vm_bo_map(adev, bo_va, args->va_address,
-				     args->offset_in_bo, args->map_size,
-				     va_flags);
-		break;
-	case AMDGPU_VA_OP_UNMAP:
-		r = amdgpu_vm_bo_unmap(adev, bo_va, args->va_address);
-		break;
-
-	case AMDGPU_VA_OP_CLEAR:
-		r = amdgpu_vm_bo_clear_mappings(adev, &fpriv->vm,
-						args->va_address,
-						args->map_size);
-		break;
-	case AMDGPU_VA_OP_REPLACE:
-		va_flags = amdgpu_gem_va_map_flags(adev, args->flags);
-		r = amdgpu_vm_bo_replace_map(adev, bo_va, args->va_address,
-					     args->offset_in_bo, args->map_size,
-					     va_flags);
-		break;
-	default:
-		break;
-	}
-	if (!r && !(args->flags & AMDGPU_VM_DELAY_UPDATE) && !adev->debug_vm)
-		amdgpu_gem_va_update_vm(adev, &fpriv->vm, bo_va,
-					args->operation);
+		if (!r && !(args->flags & AMDGPU_VM_DELAY_UPDATE) && !adev->debug_vm)
+			r = amdgpu_gem_va_update_vm(adev, vm, bo_va, args->operation);
 
-error:
+	out_exec:
 	drm_exec_fini(&exec);
-	drm_gem_object_put(gobj);
+	if (gobj)
+		drm_gem_object_put(gobj);
+
 	return r;
 }
 
 int amdgpu_gem_op_ioctl(struct drm_device *dev, void *data,
-			struct drm_file *filp)
+						struct drm_file *filp)
 {
+	struct amdgpu_device *adev;
 	struct drm_amdgpu_gem_op *args = data;
 	struct drm_gem_object *gobj;
-	struct amdgpu_vm_bo_base *base;
 	struct amdgpu_bo *robj;
-	int r;
+	int r = 0;
+
+	if (unlikely(!dev || !data || !filp))
+		return -EINVAL;
+
+	adev = drm_to_adev(dev);
+	if (unlikely(!adev))
+		return -EINVAL;
 
 	gobj = drm_gem_object_lookup(filp, args->handle);
-	if (!gobj)
+	if (unlikely(!gobj))
 		return -ENOENT;
 
 	robj = gem_to_amdgpu_bo(gobj);
+	if (unlikely(!robj)) {
+		drm_gem_object_put(gobj);
+		return -EINVAL;
+	}
 
-	r = amdgpu_bo_reserve(robj, false);
+	r = amdgpu_bo_reserve(robj, true);
 	if (unlikely(r))
 		goto out;
 
 	switch (args->op) {
-	case AMDGPU_GEM_OP_GET_GEM_CREATE_INFO: {
-		struct drm_amdgpu_gem_create_in info;
-		void __user *out = u64_to_user_ptr(args->value);
-
-		info.bo_size = robj->tbo.base.size;
-		info.alignment = robj->tbo.page_alignment << PAGE_SHIFT;
-		info.domains = robj->preferred_domains;
-		info.domain_flags = robj->flags;
-		amdgpu_bo_unreserve(robj);
-		if (copy_to_user(out, &info, sizeof(info)))
-			r = -EFAULT;
-		break;
-	}
-	case AMDGPU_GEM_OP_SET_PLACEMENT:
-		if (robj->tbo.base.import_attach &&
-		    args->value & AMDGPU_GEM_DOMAIN_VRAM) {
-			r = -EINVAL;
-			amdgpu_bo_unreserve(robj);
-			break;
-		}
-		if (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm)) {
-			r = -EPERM;
+		case AMDGPU_GEM_OP_GET_GEM_CREATE_INFO: {
+			struct drm_amdgpu_gem_create_in info = {0};
+
+			info.bo_size = robj->tbo.base.size;
+			info.alignment = robj->tbo.page_alignment << PAGE_SHIFT;
+			info.domains = robj->preferred_domains;
+			info.domain_flags = robj->flags;
+
 			amdgpu_bo_unreserve(robj);
-			break;
+			if (copy_to_user(u64_to_user_ptr(args->value), &info, sizeof(info)))
+				r = -EFAULT;
+			goto out;
 		}
-		for (base = robj->vm_bo; base; base = base->next)
-			if (amdgpu_xgmi_same_hive(amdgpu_ttm_adev(robj->tbo.bdev),
-				amdgpu_ttm_adev(base->vm->root.bo->tbo.bdev))) {
+		case AMDGPU_GEM_OP_SET_PLACEMENT: {
+			struct amdgpu_vm_bo_base *base;
+			uint32_t new_domains = args->value & (AMDGPU_GEM_DOMAIN_VRAM |
+			AMDGPU_GEM_DOMAIN_GTT |
+			AMDGPU_GEM_DOMAIN_CPU);
+			if (unlikely(!new_domains)) {
 				r = -EINVAL;
-				amdgpu_bo_unreserve(robj);
-				goto out;
+				break;
+			}
+
+			if (unlikely(robj->tbo.base.import_attach && (new_domains & AMDGPU_GEM_DOMAIN_VRAM))) {
+				r = -EINVAL;
+				break;
+			}
+
+			if (unlikely(amdgpu_ttm_tt_get_usermm(robj->tbo.ttm))) {
+				r = -EPERM;
+				break;
+			}
+
+			if (robj->tbo.base.dma_buf) {
+				for (base = robj->vm_bo; base; base = base->next) {
+					if (amdgpu_xgmi_same_hive(adev, amdgpu_ttm_adev(base->vm->root.bo->tbo.bdev))) {
+						r = -EINVAL;
+						goto set_placement_end;
+					}
+				}
 			}
 
+			robj->preferred_domains = new_domains;
+			robj->allowed_domains = robj->preferred_domains;
+			if (robj->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)
+				robj->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
 
-		robj->preferred_domains = args->value & (AMDGPU_GEM_DOMAIN_VRAM |
-							AMDGPU_GEM_DOMAIN_GTT |
-							AMDGPU_GEM_DOMAIN_CPU);
-		robj->allowed_domains = robj->preferred_domains;
-		if (robj->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)
-			robj->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
+			if (is_hbm2_vega(adev))
+				amdgpu_vega_optimize_for_workload(adev, robj, robj->flags);
 
-		if (robj->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID)
-			amdgpu_vm_bo_invalidate(robj, true);
+			if (robj->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID)
+				amdgpu_vm_bo_invalidate(robj, true);
 
-		amdgpu_bo_unreserve(robj);
-		break;
-	default:
-		amdgpu_bo_unreserve(robj);
-		r = -EINVAL;
+			set_placement_end:
+			break;
+		}
+		default:
+			r = -EINVAL;
+			break;
 	}
 
-out:
+	amdgpu_bo_unreserve(robj);
+	out:
 	drm_gem_object_put(gobj);
 	return r;
 }
 
-static int amdgpu_gem_align_pitch(struct amdgpu_device *adev,
-				  int width,
-				  int cpp,
-				  bool tiled)
-{
-	int aligned = width;
-	int pitch_mask = 0;
-
-	switch (cpp) {
-	case 1:
-		pitch_mask = 255;
-		break;
-	case 2:
-		pitch_mask = 127;
-		break;
-	case 3:
-	case 4:
-		pitch_mask = 63;
-		break;
-	}
-
-	aligned += pitch_mask;
-	aligned &= ~pitch_mask;
-	return aligned * cpp;
-}
-
 int amdgpu_mode_dumb_create(struct drm_file *file_priv,
-			    struct drm_device *dev,
-			    struct drm_mode_create_dumb *args)
+							struct drm_device *dev,
+							struct drm_mode_create_dumb *args)
 {
-	struct amdgpu_device *adev = drm_to_adev(dev);
-	struct amdgpu_fpriv *fpriv = file_priv->driver_priv;
+	struct amdgpu_device *adev;
+	struct amdgpu_fpriv *fpriv;
 	struct drm_gem_object *gobj;
 	uint32_t handle;
 	u64 flags = AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |
-		    AMDGPU_GEM_CREATE_CPU_GTT_USWC |
-		    AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;
+	AMDGPU_GEM_CREATE_CPU_GTT_USWC |
+	AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;
 	u32 domain;
+	uint64_t size;
 	int r;
 
+	if (unlikely(!file_priv || !dev || !args))
+		return -EINVAL;
+
+	adev = drm_to_adev(dev);
+	fpriv = file_priv->driver_priv;
+	if (unlikely(!adev || !fpriv))
+		return -EINVAL;
+
+	if (unlikely(args->width == 0 || args->height == 0 || args->bpp == 0 ||
+		args->bpp > 32 || args->width > U16_MAX || args->height > U16_MAX))
+		return -EINVAL;
+
 	/*
-	 * The buffer returned from this function should be cleared, but
-	 * it can only be done if the ring is enabled or we'll fail to
-	 * create the buffer.
+	 * Flag that we want the buffer to be cleared. The create ioctl will
+	 * handle the most efficient clearing method.
 	 */
-	if (adev->mman.buffer_funcs_enabled)
-		flags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;
+	flags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;
+
+	r = amdgpu_gem_align_pitch(adev, args->width,
+							   DIV_ROUND_UP(args->bpp, 8), 0);
+	if (unlikely(r < 0))
+		return r;
+	args->pitch = r;
+
+	if (unlikely(check_mul_overflow((u64)args->pitch, args->height, &size))) {
+		DRM_ERROR("Dumb buffer size calculation overflow\n");
+		return -EINVAL;
+	}
 
-	args->pitch = amdgpu_gem_align_pitch(adev, args->width,
-					     DIV_ROUND_UP(args->bpp, 8), 0);
-	args->size = (u64)args->pitch * args->height;
-	args->size = ALIGN(args->size, PAGE_SIZE);
 	domain = amdgpu_bo_get_preferred_domain(adev,
-				amdgpu_display_supported_domains(adev, flags));
-	r = amdgpu_gem_object_create(adev, args->size, 0, domain, flags,
-				     ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);
-	if (r)
-		return -ENOMEM;
+											amdgpu_display_supported_domains(adev, flags));
+
+	uint32_t alignment = PAGE_SIZE;
+	if (domain == AMDGPU_GEM_DOMAIN_VRAM && is_hbm2_vega(adev))
+		amdgpu_vega_optimize_hbm2_bank_access(adev, &size, &alignment, flags);
+
+	if (unlikely(size > U64_MAX - (alignment - 1))) {
+		DRM_ERROR("Dumb buffer alignment would overflow\n");
+		return -EINVAL;
+	}
+	size = ALIGN(size, alignment);
+	args->size = size;
+
+	r = amdgpu_gem_object_create(adev, size, alignment, domain, flags,
+								 ttm_bo_type_device, NULL, &gobj,
+							  fpriv->xcp_id + 1);
+	if (unlikely(r)) {
+		DRM_DEBUG("Failed to create dumb buffer (%d)\n", r);
+		return r;
+	}
 
 	r = drm_gem_handle_create(file_priv, gobj, &handle);
-	/* drop reference from allocate - handle holds it now */
 	drm_gem_object_put(gobj);
-	if (r)
+	if (unlikely(r))
 		return r;
 
 	args->handle = handle;
@@ -985,11 +2171,19 @@ int amdgpu_mode_dumb_create(struct drm_f
 #if defined(CONFIG_DEBUG_FS)
 static int amdgpu_debugfs_gem_info_show(struct seq_file *m, void *unused)
 {
-	struct amdgpu_device *adev = m->private;
-	struct drm_device *dev = adev_to_drm(adev);
+	struct amdgpu_device *adev;
+	struct drm_device *dev;
 	struct drm_file *file;
 	int r;
 
+	if (unlikely(!m || !m->private))
+		return -EINVAL;
+
+	adev = m->private;
+	dev = adev_to_drm(adev);
+	if (unlikely(!dev))
+		return -EINVAL;
+
 	r = mutex_lock_interruptible(&dev->filelist_mutex);
 	if (r)
 		return r;
@@ -1000,24 +2194,18 @@ static int amdgpu_debugfs_gem_info_show(
 		struct pid *pid;
 		int id;
 
-		/*
-		 * Although we have a valid reference on file->pid, that does
-		 * not guarantee that the task_struct who called get_pid() is
-		 * still alive (e.g. get_pid(current) => fork() => exit()).
-		 * Therefore, we need to protect this ->comm access using RCU.
-		 */
 		rcu_read_lock();
 		pid = rcu_dereference(file->pid);
 		task = pid_task(pid, PIDTYPE_TGID);
 		seq_printf(m, "pid %8d command %s:\n", pid_nr(pid),
-			   task ? task->comm : "<unknown>");
+				   task ? task->comm : "<unknown>");
 		rcu_read_unlock();
 
 		spin_lock(&file->table_lock);
 		idr_for_each_entry(&file->object_idr, gobj, id) {
 			struct amdgpu_bo *bo = gem_to_amdgpu_bo(gobj);
-
-			amdgpu_bo_print_info(id, bo, m);
+			if (bo)
+				amdgpu_bo_print_info(id, bo, m);
 		}
 		spin_unlock(&file->table_lock);
 	}
@@ -1027,16 +2215,26 @@ static int amdgpu_debugfs_gem_info_show(
 }
 
 DEFINE_SHOW_ATTRIBUTE(amdgpu_debugfs_gem_info);
-
 #endif
 
 void amdgpu_debugfs_gem_init(struct amdgpu_device *adev)
 {
-#if defined(CONFIG_DEBUG_FS)
-	struct drm_minor *minor = adev_to_drm(adev)->primary;
-	struct dentry *root = minor->debugfs_root;
+	#if defined(CONFIG_DEBUG_FS)
+	struct drm_minor *minor;
+	struct dentry *root;
+
+	if (unlikely(!adev))
+		return;
+
+	minor = adev_to_drm(adev)->primary;
+	if (unlikely(!minor))
+		return;
+
+	root = minor->debugfs_root;
+	if (unlikely(!root))
+		return;
 
 	debugfs_create_file("amdgpu_gem_info", 0444, root, adev,
-			    &amdgpu_debugfs_gem_info_fops);
-#endif
+						&amdgpu_debugfs_gem_info_fops);
+	#endif
 }

--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.h	2025-05-29 11:14:09.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_object.h	2025-06-02 02:29:50.388339649 +0200
@@ -1,30 +1,3 @@
-/*
- * Copyright 2008 Advanced Micro Devices, Inc.
- * Copyright 2008 Red Hat Inc.
- * Copyright 2009 Jerome Glisse.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- *
- * Authors: Dave Airlie
- *          Alex Deucher
- *          Jerome Glisse
- */
 #ifndef __AMDGPU_OBJECT_H__
 #define __AMDGPU_OBJECT_H__
 
@@ -39,7 +12,6 @@
 #define AMDGPU_BO_INVALID_OFFSET	LONG_MAX
 #define AMDGPU_BO_MAX_PLACEMENTS	3
 
-/* BO flag to indicate a KFD userptr BO */
 #define AMDGPU_AMDKFD_CREATE_USERPTR_BO	(1ULL << 63)
 
 #define to_amdgpu_bo_user(abo) container_of((abo), struct amdgpu_bo_user, bo)
@@ -56,11 +28,9 @@ struct amdgpu_bo_param {
 	bool				no_wait_gpu;
 	struct dma_resv			*resv;
 	void				(*destroy)(struct ttm_buffer_object *bo);
-	/* xcp partition number plus 1, 0 means any partition */
 	int8_t				xcp_id_plus1;
 };
 
-/* bo virtual addresses in a vm */
 struct amdgpu_bo_va_mapping {
 	struct amdgpu_bo_va		*bo_va;
 	struct list_head		list;
@@ -72,57 +42,40 @@ struct amdgpu_bo_va_mapping {
 	uint64_t			flags;
 };
 
-/* User space allocated BO in a VM */
 struct amdgpu_bo_va {
 	struct amdgpu_vm_bo_base	base;
-
-	/* protected by bo being reserved */
 	unsigned			ref_count;
-
-	/* all other members protected by the VM PD being reserved */
 	struct dma_fence	        *last_pt_update;
-
-	/* mappings for this bo_va */
 	struct list_head		invalids;
 	struct list_head		valids;
-
-	/* If the mappings are cleared or filled */
 	bool				cleared;
-
 	bool				is_xgmi;
-
-	/*
-	 * protected by vm reservation lock
-	 * if non-zero, cannot unmap from GPU because user queues may still access it
-	 */
 	unsigned int			queue_refcount;
 };
 
+enum amdgpu_bo_alloc_type {
+	AMDGPU_BO_ALLOC_DEFAULT = 0,
+	AMDGPU_BO_ALLOC_USER_SLAB,
+};
+
 struct amdgpu_bo {
-	/* Protected by tbo.reserved */
 	u32				preferred_domains;
 	u32				allowed_domains;
 	struct ttm_place		placements[AMDGPU_BO_MAX_PLACEMENTS];
 	struct ttm_placement		placement;
 	struct ttm_buffer_object	tbo;
-	struct ttm_bo_kmap_obj		kmap;
+	struct ttm_bo_kmap_obj		kmap; // Note: TTM uses ttm_kmap_obj, ensure this is intended
 	u64				flags;
-	/* per VM structure for page tables and with virtual addresses */
-	struct amdgpu_vm_bo_base	*vm_bo;
-	/* Constant after initialization */
+	enum amdgpu_bo_alloc_type	alloc_type; // Assuming this enum exists
+	spinlock_t			vm_lock;
+	struct amdgpu_vm_bo_base	*vm_bo;    /* per vm mapping for this bo */
 	struct amdgpu_bo		*parent;
-
-#ifdef CONFIG_MMU_NOTIFIER
+	#ifdef CONFIG_MMU_NOTIFIER
 	struct mmu_interval_notifier	notifier;
-#endif
+	#endif
 	struct kgd_mem                  *kfd_bo;
-
-	/*
-	 * For GPUs with spatial partitioning, xcp partition number, -1 means
-	 * any partition. For other ASICs without spatial partition, always 0
-	 * for memory accounting.
-	 */
 	int8_t				xcp_id;
+	u8				in_tbo_cache; // Assuming this is for your tbo_cache
 };
 
 struct amdgpu_bo_user {
@@ -131,12 +84,12 @@ struct amdgpu_bo_user {
 	u64				metadata_flags;
 	void				*metadata;
 	u32				metadata_size;
-
 };
 
 struct amdgpu_bo_vm {
 	struct amdgpu_bo		bo;
-	struct amdgpu_vm_bo_base        entries[];
+	struct list_head		shadow_list;
+	struct amdgpu_vm_bo_base	entries[];
 };
 
 static inline struct amdgpu_bo *ttm_to_amdgpu_bo(struct ttm_buffer_object *tbo)
@@ -144,44 +97,29 @@ static inline struct amdgpu_bo *ttm_to_a
 	return container_of(tbo, struct amdgpu_bo, tbo);
 }
 
-/**
- * amdgpu_mem_type_to_domain - return domain corresponding to mem_type
- * @mem_type:	ttm memory type
- *
- * Returns corresponding domain of the ttm mem_type
- */
 static inline unsigned amdgpu_mem_type_to_domain(u32 mem_type)
 {
 	switch (mem_type) {
-	case TTM_PL_VRAM:
-		return AMDGPU_GEM_DOMAIN_VRAM;
-	case TTM_PL_TT:
-		return AMDGPU_GEM_DOMAIN_GTT;
-	case TTM_PL_SYSTEM:
-		return AMDGPU_GEM_DOMAIN_CPU;
-	case AMDGPU_PL_GDS:
-		return AMDGPU_GEM_DOMAIN_GDS;
-	case AMDGPU_PL_GWS:
-		return AMDGPU_GEM_DOMAIN_GWS;
-	case AMDGPU_PL_OA:
-		return AMDGPU_GEM_DOMAIN_OA;
-	case AMDGPU_PL_DOORBELL:
-		return AMDGPU_GEM_DOMAIN_DOORBELL;
-	default:
-		break;
+		case TTM_PL_VRAM:
+			return AMDGPU_GEM_DOMAIN_VRAM;
+		case TTM_PL_TT:
+			return AMDGPU_GEM_DOMAIN_GTT;
+		case TTM_PL_SYSTEM:
+			return AMDGPU_GEM_DOMAIN_CPU;
+		case AMDGPU_PL_GDS:
+			return AMDGPU_GEM_DOMAIN_GDS;
+		case AMDGPU_PL_GWS:
+			return AMDGPU_GEM_DOMAIN_GWS;
+		case AMDGPU_PL_OA:
+			return AMDGPU_GEM_DOMAIN_OA;
+		case AMDGPU_PL_DOORBELL:
+			return AMDGPU_GEM_DOMAIN_DOORBELL;
+		default:
+			break;
 	}
 	return 0;
 }
 
-/**
- * amdgpu_bo_reserve - reserve bo
- * @bo:		bo structure
- * @no_intr:	don't return -ERESTARTSYS on pending signal
- *
- * Returns:
- * -ERESTARTSYS: A wait for the buffer to become unreserved was interrupted by
- * a signal. Release all buffer reservations and return to user-space.
- */
 static inline int amdgpu_bo_reserve(struct amdgpu_bo *bo, bool no_intr)
 {
 	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);
@@ -216,31 +154,16 @@ static inline unsigned amdgpu_bo_gpu_pag
 	return (bo->tbo.page_alignment << PAGE_SHIFT) / AMDGPU_GPU_PAGE_SIZE;
 }
 
-/**
- * amdgpu_bo_mmap_offset - return mmap offset of bo
- * @bo:	amdgpu object for which we query the offset
- *
- * Returns mmap offset of the object.
- */
 static inline u64 amdgpu_bo_mmap_offset(struct amdgpu_bo *bo)
 {
 	return drm_vma_node_offset_addr(&bo->tbo.base.vma_node);
 }
 
-/**
- * amdgpu_bo_explicit_sync - return whether the bo is explicitly synced
- */
 static inline bool amdgpu_bo_explicit_sync(struct amdgpu_bo *bo)
 {
 	return bo->flags & AMDGPU_GEM_CREATE_EXPLICIT_SYNC;
 }
 
-/**
- * amdgpu_bo_encrypted - test if the BO is encrypted
- * @bo: pointer to a buffer object
- *
- * Return true if the buffer object is encrypted, false otherwise.
- */
 static inline bool amdgpu_bo_encrypted(struct amdgpu_bo *bo)
 {
 	return bo->flags & AMDGPU_GEM_CREATE_ENCRYPTED;
@@ -250,31 +173,31 @@ bool amdgpu_bo_is_amdgpu_bo(struct ttm_b
 void amdgpu_bo_placement_from_domain(struct amdgpu_bo *abo, u32 domain);
 
 int amdgpu_bo_create(struct amdgpu_device *adev,
-		     struct amdgpu_bo_param *bp,
-		     struct amdgpu_bo **bo_ptr);
+					 struct amdgpu_bo_param *bp,
+					 struct amdgpu_bo **bo_ptr);
 int amdgpu_bo_create_reserved(struct amdgpu_device *adev,
-			      unsigned long size, int align,
-			      u32 domain, struct amdgpu_bo **bo_ptr,
-			      u64 *gpu_addr, void **cpu_addr);
+							  unsigned long size, int align,
+							  u32 domain, struct amdgpu_bo **bo_ptr,
+							  u64 *gpu_addr, void **cpu_addr);
 int amdgpu_bo_create_kernel(struct amdgpu_device *adev,
-			    unsigned long size, int align,
-			    u32 domain, struct amdgpu_bo **bo_ptr,
-			    u64 *gpu_addr, void **cpu_addr);
+							unsigned long size, int align,
+							u32 domain, struct amdgpu_bo **bo_ptr,
+							u64 *gpu_addr, void **cpu_addr);
 int amdgpu_bo_create_isp_user(struct amdgpu_device *adev,
-			   struct dma_buf *dbuf, u32 domain,
-			   struct amdgpu_bo **bo,
-			   u64 *gpu_addr);
+							  struct dma_buf *dbuf, u32 domain,
+							  struct amdgpu_bo **bo,
+							  u64 *gpu_addr);
 int amdgpu_bo_create_kernel_at(struct amdgpu_device *adev,
-			       uint64_t offset, uint64_t size,
-			       struct amdgpu_bo **bo_ptr, void **cpu_addr);
+							   uint64_t offset, uint64_t size,
+							   struct amdgpu_bo **bo_ptr, void **cpu_addr);
 int amdgpu_bo_create_user(struct amdgpu_device *adev,
-			  struct amdgpu_bo_param *bp,
-			  struct amdgpu_bo_user **ubo_ptr);
+						  struct amdgpu_bo_param *bp,
+						  struct amdgpu_bo_user **ubo_ptr);
 int amdgpu_bo_create_vm(struct amdgpu_device *adev,
-			struct amdgpu_bo_param *bp,
-			struct amdgpu_bo_vm **ubo_ptr);
+						struct amdgpu_bo_param *bp,
+						struct amdgpu_bo_vm **vmbo_ptr);
 void amdgpu_bo_free_kernel(struct amdgpu_bo **bo, u64 *gpu_addr,
-			   void **cpu_addr);
+						   void **cpu_addr);
 void amdgpu_bo_free_isp_user(struct amdgpu_bo *bo);
 int amdgpu_bo_kmap(struct amdgpu_bo *bo, void **ptr);
 void *amdgpu_bo_kptr(struct amdgpu_bo *bo);
@@ -287,31 +210,32 @@ int amdgpu_bo_init(struct amdgpu_device
 void amdgpu_bo_fini(struct amdgpu_device *adev);
 int amdgpu_bo_set_tiling_flags(struct amdgpu_bo *bo, u64 tiling_flags);
 void amdgpu_bo_get_tiling_flags(struct amdgpu_bo *bo, u64 *tiling_flags);
-int amdgpu_bo_set_metadata (struct amdgpu_bo *bo, void *metadata,
-			    uint32_t metadata_size, uint64_t flags);
+int amdgpu_bo_set_metadata(struct amdgpu_bo *bo, void *metadata,
+						   uint32_t metadata_size, uint64_t flags);
 int amdgpu_bo_get_metadata(struct amdgpu_bo *bo, void *buffer,
-			   size_t buffer_size, uint32_t *metadata_size,
-			   uint64_t *flags);
+						   size_t buffer_size, uint32_t *metadata_size,
+						   uint64_t *flags);
 void amdgpu_bo_move_notify(struct ttm_buffer_object *bo,
-			   bool evict,
-			   struct ttm_resource *new_mem);
+						   bool evict,
+						   struct ttm_resource *new_mem);
 void amdgpu_bo_release_notify(struct ttm_buffer_object *bo);
 vm_fault_t amdgpu_bo_fault_reserve_notify(struct ttm_buffer_object *bo);
+
+void amdgpu_bo_destroy(struct ttm_buffer_object *tbo);
+void amdgpu_bo_user_destroy(struct ttm_buffer_object *tbo);
+
 void amdgpu_bo_fence(struct amdgpu_bo *bo, struct dma_fence *fence,
-		     bool shared);
+					 bool shared);
 int amdgpu_bo_sync_wait_resv(struct amdgpu_device *adev, struct dma_resv *resv,
-			     enum amdgpu_sync_mode sync_mode, void *owner,
-			     bool intr);
+							 enum amdgpu_sync_mode sync_mode, void *owner,
+							 bool intr);
 int amdgpu_bo_sync_wait(struct amdgpu_bo *bo, void *owner, bool intr);
 u64 amdgpu_bo_gpu_offset(struct amdgpu_bo *bo);
 u64 amdgpu_bo_gpu_offset_no_check(struct amdgpu_bo *bo);
 uint32_t amdgpu_bo_mem_stats_placement(struct amdgpu_bo *bo);
 uint32_t amdgpu_bo_get_preferred_domain(struct amdgpu_device *adev,
-					    uint32_t domain);
+										uint32_t domain);
 
-/*
- * sub allocation
- */
 static inline struct amdgpu_sa_manager *
 to_amdgpu_sa_manager(struct drm_suballoc_manager *manager)
 {
@@ -321,35 +245,34 @@ to_amdgpu_sa_manager(struct drm_suballoc
 static inline uint64_t amdgpu_sa_bo_gpu_addr(struct drm_suballoc *sa_bo)
 {
 	return to_amdgpu_sa_manager(sa_bo->manager)->gpu_addr +
-		drm_suballoc_soffset(sa_bo);
+	drm_suballoc_soffset(sa_bo);
 }
 
 static inline void *amdgpu_sa_bo_cpu_addr(struct drm_suballoc *sa_bo)
 {
 	return to_amdgpu_sa_manager(sa_bo->manager)->cpu_ptr +
-		drm_suballoc_soffset(sa_bo);
+	drm_suballoc_soffset(sa_bo);
 }
 
 int amdgpu_sa_bo_manager_init(struct amdgpu_device *adev,
-				     struct amdgpu_sa_manager *sa_manager,
-				     unsigned size, u32 align, u32 domain);
+							  struct amdgpu_sa_manager *sa_manager,
+							  unsigned size, u32 align, u32 domain);
 void amdgpu_sa_bo_manager_fini(struct amdgpu_device *adev,
-				      struct amdgpu_sa_manager *sa_manager);
+							   struct amdgpu_sa_manager *sa_manager);
 int amdgpu_sa_bo_manager_start(struct amdgpu_device *adev,
-				      struct amdgpu_sa_manager *sa_manager);
+							   struct amdgpu_sa_manager *sa_manager);
 int amdgpu_sa_bo_new(struct amdgpu_sa_manager *sa_manager,
-		     struct drm_suballoc **sa_bo,
-		     unsigned int size);
+					 struct drm_suballoc **sa_bo,
+					 unsigned int size);
 void amdgpu_sa_bo_free(struct drm_suballoc **sa_bo,
-		       struct dma_fence *fence);
+					   struct dma_fence *fence);
 #if defined(CONFIG_DEBUG_FS)
 void amdgpu_sa_bo_dump_debug_info(struct amdgpu_sa_manager *sa_manager,
-					 struct seq_file *m);
+								  struct seq_file *m);
 u64 amdgpu_bo_print_info(int id, struct amdgpu_bo *bo, struct seq_file *m);
 #endif
 void amdgpu_debugfs_sa_init(struct amdgpu_device *adev);
 
 bool amdgpu_bo_support_uswc(u64 bo_flags);
 
-
 #endif


--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c	2025-05-29 11:14:09.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c	2025-06-02 02:29:50.388339649 +0200
@@ -43,33 +43,29 @@
 #include "amdgpu_vm.h"
 #include "amdgpu_dma_buf.h"
 
-/**
- * DOC: amdgpu_object
- *
- * This defines the interfaces to operate on an &amdgpu_bo buffer object which
- * represents memory used by driver (VRAM, system memory, etc.). The driver
- * provides DRM/GEM APIs to userspace. DRM/GEM APIs then use these interfaces
- * to create/destroy/set buffer object which are then managed by the kernel TTM
- * memory manager.
- * The interfaces are also used internally by kernel clients, including gfx,
- * uvd, etc. for kernel managed allocations used by the GPU.
- *
- */
+#if IS_ENABLED(CONFIG_HSA_AMD)
+extern void
+amdgpu_amdkfd_remove_fence_on_pt_pd_bos(struct amdgpu_bo *bo)
+__attribute__((weak));
+#else
+static inline void
+amdgpu_amdkfd_remove_fence_on_pt_pd_bos(struct amdgpu_bo *bo) { }
+#endif
 
-static void amdgpu_bo_destroy(struct ttm_buffer_object *tbo)
+void amdgpu_bo_destroy(struct ttm_buffer_object *tbo)
 {
 	struct amdgpu_bo *bo = ttm_to_amdgpu_bo(tbo);
 
 	amdgpu_bo_kunmap(bo);
 
-	if (bo->tbo.base.import_attach)
+	if (unlikely(bo->tbo.base.import_attach))
 		drm_prime_gem_destroy(&bo->tbo.base, bo->tbo.sg);
 	drm_gem_object_release(&bo->tbo.base);
 	amdgpu_bo_unref(&bo->parent);
 	kvfree(bo);
 }
 
-static void amdgpu_bo_user_destroy(struct ttm_buffer_object *tbo)
+void amdgpu_bo_user_destroy(struct ttm_buffer_object *tbo)
 {
 	struct amdgpu_bo *bo = ttm_to_amdgpu_bo(tbo);
 	struct amdgpu_bo_user *ubo;
@@ -79,33 +75,15 @@ static void amdgpu_bo_user_destroy(struc
 	amdgpu_bo_destroy(tbo);
 }
 
-/**
- * amdgpu_bo_is_amdgpu_bo - check if the buffer object is an &amdgpu_bo
- * @bo: buffer object to be checked
- *
- * Uses destroy function associated with the object to determine if this is
- * an &amdgpu_bo.
- *
- * Returns:
- * true if the object belongs to &amdgpu_bo, false if not.
- */
 bool amdgpu_bo_is_amdgpu_bo(struct ttm_buffer_object *bo)
 {
-	if (bo->destroy == &amdgpu_bo_destroy ||
-	    bo->destroy == &amdgpu_bo_user_destroy)
+	if (likely(bo->destroy == &amdgpu_bo_destroy ||
+		bo->destroy == &amdgpu_bo_user_destroy))
 		return true;
 
 	return false;
 }
 
-/**
- * amdgpu_bo_placement_from_domain - set buffer's placement
- * @abo: &amdgpu_bo buffer object whose placement is to be set
- * @domain: requested domain
- *
- * Sets buffer's placement according to requested domain and the buffer's
- * flags.
- */
 void amdgpu_bo_placement_from_domain(struct amdgpu_bo *abo, u32 domain)
 {
 	struct amdgpu_device *adev = amdgpu_ttm_adev(abo->tbo.bdev);
@@ -114,128 +92,144 @@ void amdgpu_bo_placement_from_domain(str
 	u64 flags = abo->flags;
 	u32 c = 0;
 
-	if (domain & AMDGPU_GEM_DOMAIN_VRAM) {
+	static_assert(AMDGPU_GEM_DOMAIN_CPU == 0x1, "AMDGPU_GEM_DOMAIN_CPU ABI definition changed!");
+	static_assert(AMDGPU_GEM_DOMAIN_GTT == 0x2, "AMDGPU_GEM_DOMAIN_GTT ABI definition changed!");
+	static_assert(AMDGPU_GEM_DOMAIN_VRAM == 0x4, "AMDGPU_GEM_DOMAIN_VRAM ABI definition changed!");
+
+	BUILD_BUG_ON(AMDGPU_BO_MAX_PLACEMENTS < 2);
+
+	if (domain == AMDGPU_GEM_DOMAIN_VRAM) {
 		unsigned int visible_pfn = adev->gmc.visible_vram_size >> PAGE_SHIFT;
 		int8_t mem_id = KFD_XCP_MEM_ID(adev, abo->xcp_id);
 
-		if (adev->gmc.mem_partitions && mem_id >= 0) {
+		if (likely(adev->gmc.mem_partitions &&
+			mem_id >= 0 && mem_id < min_t(int, adev->gmc.num_mem_partitions, 8))) {
 			places[c].fpfn = adev->gmc.mem_partitions[mem_id].range.fpfn;
-			/*
-			 * memory partition range lpfn is inclusive start + size - 1
-			 * TTM place lpfn is exclusive start + size
-			 */
-			places[c].lpfn = adev->gmc.mem_partitions[mem_id].range.lpfn + 1;
-		} else {
-			places[c].fpfn = 0;
-			places[c].lpfn = 0;
-		}
-		places[c].mem_type = TTM_PL_VRAM;
-		places[c].flags = 0;
+		places[c].lpfn = adev->gmc.mem_partitions[mem_id].range.lpfn + 1;
+			} else {
+				places[c].fpfn = 0;
+				places[c].lpfn = 0;
+			}
+			places[c].mem_type = TTM_PL_VRAM;
+			places[c].flags = 0;
 
-		if (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)
-			places[c].lpfn = min_not_zero(places[c].lpfn, visible_pfn);
+			if (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)
+				places[c].lpfn = min_not_zero(places[c].lpfn, visible_pfn);
 		else
 			places[c].flags |= TTM_PL_FLAG_TOPDOWN;
 
-		if (abo->tbo.type == ttm_bo_type_kernel &&
-		    flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS)
+		if (unlikely(abo->tbo.type == ttm_bo_type_kernel &&
+			flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS))
 			places[c].flags |= TTM_PL_FLAG_CONTIGUOUS;
-
 		c++;
-	}
-
-	if (domain & AMDGPU_GEM_DOMAIN_DOORBELL) {
+	} else if (domain == AMDGPU_GEM_DOMAIN_GTT) {
 		places[c].fpfn = 0;
 		places[c].lpfn = 0;
-		places[c].mem_type = AMDGPU_PL_DOORBELL;
+		places[c].mem_type = abo->flags & AMDGPU_GEM_CREATE_PREEMPTIBLE ?
+		AMDGPU_PL_PREEMPT : TTM_PL_TT;
 		places[c].flags = 0;
+		if (abo->tbo.resource && !(adev->flags & AMD_IS_APU) &&
+			domain & abo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM)
+			places[c].flags |= TTM_PL_FLAG_FALLBACK;
 		c++;
-	}
+	} else {
+		if (domain & AMDGPU_GEM_DOMAIN_VRAM) {
+			unsigned int visible_pfn = adev->gmc.visible_vram_size >> PAGE_SHIFT;
+			int8_t mem_id = KFD_XCP_MEM_ID(adev, abo->xcp_id);
+
+			if (likely(adev->gmc.mem_partitions &&
+				mem_id >= 0 && mem_id < min_t(int, adev->gmc.num_mem_partitions, 8))) {
+				places[c].fpfn = adev->gmc.mem_partitions[mem_id].range.fpfn;
+			places[c].lpfn = adev->gmc.mem_partitions[mem_id].range.lpfn + 1;
+				} else {
+					places[c].fpfn = 0;
+					places[c].lpfn = 0;
+				}
+				places[c].mem_type = TTM_PL_VRAM;
+				places[c].flags = 0;
+
+				if (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)
+					places[c].lpfn = min_not_zero(places[c].lpfn, visible_pfn);
+			else
+				places[c].flags |= TTM_PL_FLAG_TOPDOWN;
+
+			if (unlikely(abo->tbo.type == ttm_bo_type_kernel &&
+				flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS))
+				places[c].flags |= TTM_PL_FLAG_CONTIGUOUS;
+			c++;
+		}
 
-	if (domain & AMDGPU_GEM_DOMAIN_GTT) {
-		places[c].fpfn = 0;
-		places[c].lpfn = 0;
-		places[c].mem_type =
+		if (unlikely(domain & AMDGPU_GEM_DOMAIN_DOORBELL)) {
+			places[c].fpfn = 0;
+			places[c].lpfn = 0;
+			places[c].mem_type = AMDGPU_PL_DOORBELL;
+			places[c].flags = 0;
+			c++;
+		}
+
+		if (domain & AMDGPU_GEM_DOMAIN_GTT) {
+			places[c].fpfn = 0;
+			places[c].lpfn = 0;
+			places[c].mem_type =
 			abo->flags & AMDGPU_GEM_CREATE_PREEMPTIBLE ?
 			AMDGPU_PL_PREEMPT : TTM_PL_TT;
-		places[c].flags = 0;
-		/*
-		 * When GTT is just an alternative to VRAM make sure that we
-		 * only use it as fallback and still try to fill up VRAM first.
-		 */
-		if (abo->tbo.resource && !(adev->flags & AMD_IS_APU) &&
-		    domain & abo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM)
-			places[c].flags |= TTM_PL_FLAG_FALLBACK;
-		c++;
-	}
+			places[c].flags = 0;
+			if (abo->tbo.resource && !(adev->flags & AMD_IS_APU) &&
+				domain & abo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM)
+				places[c].flags |= TTM_PL_FLAG_FALLBACK;
+			c++;
+		}
 
-	if (domain & AMDGPU_GEM_DOMAIN_CPU) {
-		places[c].fpfn = 0;
-		places[c].lpfn = 0;
-		places[c].mem_type = TTM_PL_SYSTEM;
-		places[c].flags = 0;
-		c++;
-	}
+		if (domain & AMDGPU_GEM_DOMAIN_CPU) {
+			places[c].fpfn = 0;
+			places[c].lpfn = 0;
+			places[c].mem_type = TTM_PL_SYSTEM;
+			places[c].flags = 0;
+			c++;
+		}
 
-	if (domain & AMDGPU_GEM_DOMAIN_GDS) {
-		places[c].fpfn = 0;
-		places[c].lpfn = 0;
-		places[c].mem_type = AMDGPU_PL_GDS;
-		places[c].flags = 0;
-		c++;
-	}
+		if (unlikely(domain & AMDGPU_GEM_DOMAIN_GDS)) {
+			places[c].fpfn = 0;
+			places[c].lpfn = 0;
+			places[c].mem_type = AMDGPU_PL_GDS;
+			places[c].flags = 0;
+			c++;
+		}
 
-	if (domain & AMDGPU_GEM_DOMAIN_GWS) {
-		places[c].fpfn = 0;
-		places[c].lpfn = 0;
-		places[c].mem_type = AMDGPU_PL_GWS;
-		places[c].flags = 0;
-		c++;
-	}
+		if (unlikely(domain & AMDGPU_GEM_DOMAIN_GWS)) {
+			places[c].fpfn = 0;
+			places[c].lpfn = 0;
+			places[c].mem_type = AMDGPU_PL_GWS;
+			places[c].flags = 0;
+			c++;
+		}
 
-	if (domain & AMDGPU_GEM_DOMAIN_OA) {
-		places[c].fpfn = 0;
-		places[c].lpfn = 0;
-		places[c].mem_type = AMDGPU_PL_OA;
-		places[c].flags = 0;
-		c++;
+		if (unlikely(domain & AMDGPU_GEM_DOMAIN_OA)) {
+			places[c].fpfn = 0;
+			places[c].lpfn = 0;
+			places[c].mem_type = AMDGPU_PL_OA;
+			places[c].flags = 0;
+			c++;
+		}
 	}
 
+	BUG_ON(c == 0 && domain != 0);
 	BUG_ON(c > AMDGPU_BO_MAX_PLACEMENTS);
 
 	placement->num_placement = c;
 	placement->placement = places;
 }
 
-/**
- * amdgpu_bo_create_reserved - create reserved BO for kernel use
- *
- * @adev: amdgpu device object
- * @size: size for the new BO
- * @align: alignment for the new BO
- * @domain: where to place it
- * @bo_ptr: used to initialize BOs in structures
- * @gpu_addr: GPU addr of the pinned BO
- * @cpu_addr: optional CPU address mapping
- *
- * Allocates and pins a BO for kernel internal use, and returns it still
- * reserved.
- *
- * Note: For bo_ptr new BO is only created if bo_ptr points to NULL.
- *
- * Returns:
- * 0 on success, negative error code otherwise.
- */
 int amdgpu_bo_create_reserved(struct amdgpu_device *adev,
-			      unsigned long size, int align,
-			      u32 domain, struct amdgpu_bo **bo_ptr,
-			      u64 *gpu_addr, void **cpu_addr)
+							  unsigned long size, int align,
+							  u32 domain, struct amdgpu_bo **bo_ptr,
+							  u64 *gpu_addr, void **cpu_addr)
 {
 	struct amdgpu_bo_param bp;
 	bool free = false;
 	int r;
 
-	if (!size) {
+	if (unlikely(!size)) {
 		amdgpu_bo_unref(bo_ptr);
 		return 0;
 	}
@@ -245,36 +239,36 @@ int amdgpu_bo_create_reserved(struct amd
 	bp.byte_align = align;
 	bp.domain = domain;
 	bp.flags = cpu_addr ? AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED
-		: AMDGPU_GEM_CREATE_NO_CPU_ACCESS;
+	: AMDGPU_GEM_CREATE_NO_CPU_ACCESS;
 	bp.flags |= AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;
 	bp.type = ttm_bo_type_kernel;
 	bp.resv = NULL;
 	bp.bo_ptr_size = sizeof(struct amdgpu_bo);
 
-	if (!*bo_ptr) {
+	if (likely(!*bo_ptr)) {
 		r = amdgpu_bo_create(adev, &bp, bo_ptr);
-		if (r) {
+		if (unlikely(r)) {
 			dev_err(adev->dev, "(%d) failed to allocate kernel bo\n",
-				r);
+					r);
 			return r;
 		}
 		free = true;
 	}
 
 	r = amdgpu_bo_reserve(*bo_ptr, false);
-	if (r) {
+	if (unlikely(r)) {
 		dev_err(adev->dev, "(%d) failed to reserve kernel bo\n", r);
 		goto error_free;
 	}
 
 	r = amdgpu_bo_pin(*bo_ptr, domain);
-	if (r) {
+	if (unlikely(r)) {
 		dev_err(adev->dev, "(%d) kernel bo pin failed\n", r);
 		goto error_unreserve;
 	}
 
 	r = amdgpu_ttm_alloc_gart(&(*bo_ptr)->tbo);
-	if (r) {
+	if (unlikely(r)) {
 		dev_err(adev->dev, "%p bind failed\n", *bo_ptr);
 		goto error_unpin;
 	}
@@ -284,7 +278,7 @@ int amdgpu_bo_create_reserved(struct amd
 
 	if (cpu_addr) {
 		r = amdgpu_bo_kmap(*bo_ptr, cpu_addr);
-		if (r) {
+		if (unlikely(r)) {
 			dev_err(adev->dev, "(%d) kernel bo map failed\n", r);
 			goto error_unpin;
 		}
@@ -292,81 +286,41 @@ int amdgpu_bo_create_reserved(struct amd
 
 	return 0;
 
-error_unpin:
+	error_unpin:
 	amdgpu_bo_unpin(*bo_ptr);
-error_unreserve:
+	error_unreserve:
 	amdgpu_bo_unreserve(*bo_ptr);
 
-error_free:
+	error_free:
 	if (free)
 		amdgpu_bo_unref(bo_ptr);
 
 	return r;
 }
 
-/**
- * amdgpu_bo_create_kernel - create BO for kernel use
- *
- * @adev: amdgpu device object
- * @size: size for the new BO
- * @align: alignment for the new BO
- * @domain: where to place it
- * @bo_ptr:  used to initialize BOs in structures
- * @gpu_addr: GPU addr of the pinned BO
- * @cpu_addr: optional CPU address mapping
- *
- * Allocates and pins a BO for kernel internal use.
- *
- * This function is exported to allow the V4L2 isp device
- * external to drm device to create and access the kernel BO.
- *
- * Note: For bo_ptr new BO is only created if bo_ptr points to NULL.
- *
- * Returns:
- * 0 on success, negative error code otherwise.
- */
 int amdgpu_bo_create_kernel(struct amdgpu_device *adev,
-			    unsigned long size, int align,
-			    u32 domain, struct amdgpu_bo **bo_ptr,
-			    u64 *gpu_addr, void **cpu_addr)
+							unsigned long size, int align,
+							u32 domain, struct amdgpu_bo **bo_ptr,
+							u64 *gpu_addr, void **cpu_addr)
 {
 	int r;
 
 	r = amdgpu_bo_create_reserved(adev, size, align, domain, bo_ptr,
-				      gpu_addr, cpu_addr);
+								  gpu_addr, cpu_addr);
 
-	if (r)
+	if (unlikely(r))
 		return r;
 
-	if (*bo_ptr)
+	if (likely(*bo_ptr))
 		amdgpu_bo_unreserve(*bo_ptr);
 
 	return 0;
 }
 EXPORT_SYMBOL(amdgpu_bo_create_kernel);
 
-/**
- * amdgpu_bo_create_isp_user - create user BO for isp
- *
- * @adev: amdgpu device object
- * @dma_buf: DMABUF handle for isp buffer
- * @domain: where to place it
- * @bo:  used to initialize BOs in structures
- * @gpu_addr: GPU addr of the pinned BO
- *
- * Imports isp DMABUF to allocate and pin a user BO for isp internal use. It does
- * GART alloc to generate gpu_addr for BO to make it accessible through the
- * GART aperture for ISP HW.
- *
- * This function is exported to allow the V4L2 isp device external to drm device
- * to create and access the isp user BO.
- *
- * Returns:
- * 0 on success, negative error code otherwise.
- */
 int amdgpu_bo_create_isp_user(struct amdgpu_device *adev,
-			   struct dma_buf *dma_buf, u32 domain, struct amdgpu_bo **bo,
-			   u64 *gpu_addr)
+							  struct dma_buf *dma_buf, u32 domain, struct amdgpu_bo **bo,
+							  u64 *gpu_addr)
 
 {
 	struct drm_gem_object *gem_obj;
@@ -374,39 +328,39 @@ int amdgpu_bo_create_isp_user(struct amd
 
 	gem_obj = amdgpu_gem_prime_import(&adev->ddev, dma_buf);
 	*bo = gem_to_amdgpu_bo(gem_obj);
-	if (!(*bo)) {
+	if (unlikely(!(*bo))) {
 		dev_err(adev->dev, "failed to get valid isp user bo\n");
 		return -EINVAL;
 	}
 
 	r = amdgpu_bo_reserve(*bo, false);
-	if (r) {
+	if (unlikely(r)) {
 		dev_err(adev->dev, "(%d) failed to reserve isp user bo\n", r);
 		return r;
 	}
 
 	r = amdgpu_bo_pin(*bo, domain);
-	if (r) {
+	if (unlikely(r)) {
 		dev_err(adev->dev, "(%d) isp user bo pin failed\n", r);
 		goto error_unreserve;
 	}
 
 	r = amdgpu_ttm_alloc_gart(&(*bo)->tbo);
-	if (r) {
+	if (unlikely(r)) {
 		dev_err(adev->dev, "%p bind failed\n", *bo);
 		goto error_unpin;
 	}
 
-	if (!WARN_ON(!gpu_addr))
+	if (likely(!WARN_ON(!gpu_addr)))
 		*gpu_addr = amdgpu_bo_gpu_offset(*bo);
 
 	amdgpu_bo_unreserve(*bo);
 
 	return 0;
 
-error_unpin:
+	error_unpin:
 	amdgpu_bo_unpin(*bo);
-error_unreserve:
+	error_unreserve:
 	amdgpu_bo_unreserve(*bo);
 	amdgpu_bo_unref(bo);
 
@@ -414,23 +368,9 @@ error_unreserve:
 }
 EXPORT_SYMBOL(amdgpu_bo_create_isp_user);
 
-/**
- * amdgpu_bo_create_kernel_at - create BO for kernel use at specific location
- *
- * @adev: amdgpu device object
- * @offset: offset of the BO
- * @size: size of the BO
- * @bo_ptr:  used to initialize BOs in structures
- * @cpu_addr: optional CPU address mapping
- *
- * Creates a kernel BO at a specific offset in VRAM.
- *
- * Returns:
- * 0 on success, negative error code otherwise.
- */
 int amdgpu_bo_create_kernel_at(struct amdgpu_device *adev,
-			       uint64_t offset, uint64_t size,
-			       struct amdgpu_bo **bo_ptr, void **cpu_addr)
+							   uint64_t offset, uint64_t size,
+							   struct amdgpu_bo **bo_ptr, void **cpu_addr)
 {
 	struct ttm_operation_ctx ctx = { false, false };
 	unsigned int i;
@@ -440,18 +380,14 @@ int amdgpu_bo_create_kernel_at(struct am
 	size = ALIGN(size, PAGE_SIZE);
 
 	r = amdgpu_bo_create_reserved(adev, size, PAGE_SIZE,
-				      AMDGPU_GEM_DOMAIN_VRAM, bo_ptr, NULL,
-				      cpu_addr);
-	if (r)
+								  AMDGPU_GEM_DOMAIN_VRAM, bo_ptr, NULL,
+							   cpu_addr);
+	if (unlikely(r))
 		return r;
 
-	if ((*bo_ptr) == NULL)
+	if (unlikely((*bo_ptr) == NULL))
 		return 0;
 
-	/*
-	 * Remove the original mem node and create a new one at the request
-	 * position.
-	 */
 	if (cpu_addr)
 		amdgpu_bo_kunmap(*bo_ptr);
 
@@ -462,47 +398,35 @@ int amdgpu_bo_create_kernel_at(struct am
 		(*bo_ptr)->placements[i].lpfn = (offset + size) >> PAGE_SHIFT;
 	}
 	r = ttm_bo_mem_space(&(*bo_ptr)->tbo, &(*bo_ptr)->placement,
-			     &(*bo_ptr)->tbo.resource, &ctx);
-	if (r)
+						 &(*bo_ptr)->tbo.resource, &ctx);
+	if (unlikely(r))
 		goto error;
 
 	if (cpu_addr) {
 		r = amdgpu_bo_kmap(*bo_ptr, cpu_addr);
-		if (r)
+		if (unlikely(r))
 			goto error;
 	}
 
 	amdgpu_bo_unreserve(*bo_ptr);
 	return 0;
 
-error:
+	error:
 	amdgpu_bo_unreserve(*bo_ptr);
 	amdgpu_bo_unref(bo_ptr);
 	return r;
 }
 
-/**
- * amdgpu_bo_free_kernel - free BO for kernel use
- *
- * @bo: amdgpu BO to free
- * @gpu_addr: pointer to where the BO's GPU memory space address was stored
- * @cpu_addr: pointer to where the BO's CPU memory space address was stored
- *
- * unmaps and unpin a BO for kernel internal use.
- *
- * This function is exported to allow the V4L2 isp device
- * external to drm device to free the kernel BO.
- */
 void amdgpu_bo_free_kernel(struct amdgpu_bo **bo, u64 *gpu_addr,
-			   void **cpu_addr)
+						   void **cpu_addr)
 {
-	if (*bo == NULL)
+	if (unlikely(*bo == NULL))
 		return;
 
 	WARN_ON(amdgpu_ttm_adev((*bo)->tbo.bdev)->in_suspend);
 
 	if (likely(amdgpu_bo_reserve(*bo, true) == 0)) {
-		if (cpu_addr)
+		if (cpu_addr && *cpu_addr)
 			amdgpu_bo_kunmap(*bo);
 
 		amdgpu_bo_unpin(*bo);
@@ -518,22 +442,12 @@ void amdgpu_bo_free_kernel(struct amdgpu
 }
 EXPORT_SYMBOL(amdgpu_bo_free_kernel);
 
-/**
- * amdgpu_bo_free_isp_user - free BO for isp use
- *
- * @bo: amdgpu isp user BO to free
- *
- * unpin and unref BO for isp internal use.
- *
- * This function is exported to allow the V4L2 isp device
- * external to drm device to free the isp user BO.
- */
 void amdgpu_bo_free_isp_user(struct amdgpu_bo *bo)
 {
-	if (bo == NULL)
+	if (unlikely(bo == NULL))
 		return;
 
-	if (amdgpu_bo_reserve(bo, true) == 0) {
+	if (likely(amdgpu_bo_reserve(bo, true) == 0)) {
 		amdgpu_bo_unpin(bo);
 		amdgpu_bo_unreserve(bo);
 	}
@@ -541,217 +455,209 @@ void amdgpu_bo_free_isp_user(struct amdg
 }
 EXPORT_SYMBOL(amdgpu_bo_free_isp_user);
 
-/* Validate bo size is bit bigger than the request domain */
 static bool amdgpu_bo_validate_size(struct amdgpu_device *adev,
-					  unsigned long size, u32 domain)
+									unsigned long size, u32 domain)
 {
 	struct ttm_resource_manager *man = NULL;
 
-	/* TODO add more domains checks, such as AMDGPU_GEM_DOMAIN_CPU, _DOMAIN_DOORBELL */
-	if (!(domain & (AMDGPU_GEM_DOMAIN_GTT | AMDGPU_GEM_DOMAIN_VRAM)))
+	if (likely(!(domain & (AMDGPU_GEM_DOMAIN_GTT | AMDGPU_GEM_DOMAIN_VRAM))))
 		return true;
 
 	if (domain & AMDGPU_GEM_DOMAIN_VRAM) {
 		man = ttm_manager_type(&adev->mman.bdev, TTM_PL_VRAM);
-		if (size < man->size)
+		if (likely(size < man->size))
 			return true;
 	}
 	if (domain & AMDGPU_GEM_DOMAIN_GTT) {
 		man = ttm_manager_type(&adev->mman.bdev, TTM_PL_TT);
-		if (!man) {
+		if (unlikely(!man)) {
 			WARN_ON_ONCE("GTT domain requested but GTT mem manager uninitialized");
 			return false;
 		}
-		if (size < man->size)
+		if (likely(size < man->size))
 			return true;
 	}
 
-	DRM_DEBUG("BO size %lu > total memory in domain: %llu\n", size, man->size);
+	DRM_DEBUG("BO size %lu > total memory in domain: %llu\n", size, man ? man->size : 0);
 	return false;
 }
 
 bool amdgpu_bo_support_uswc(u64 bo_flags)
 {
 
-#ifdef CONFIG_X86_32
-	/* XXX: Write-combined CPU mappings of GTT seem broken on 32-bit
-	 * See https://bugs.freedesktop.org/show_bug.cgi?id=84627
-	 */
+	#ifdef CONFIG_X86_32
 	return false;
-#elif defined(CONFIG_X86) && !defined(CONFIG_X86_PAT)
-	/* Don't try to enable write-combining when it can't work, or things
-	 * may be slow
-	 * See https://bugs.freedesktop.org/show_bug.cgi?id=88758
-	 */
-
-#ifndef CONFIG_COMPILE_TEST
-#warning Please enable CONFIG_MTRR and CONFIG_X86_PAT for better performance \
-	 thanks to write-combining
-#endif
+	#elif defined(CONFIG_X86) && !defined(CONFIG_X86_PAT)
+
+	#ifndef CONFIG_COMPILE_TEST
+	#warning Please enable CONFIG_MTRR and CONFIG_X86_PAT for better performance \
+	thanks to write-combining
+	#endif
 
 	if (bo_flags & AMDGPU_GEM_CREATE_CPU_GTT_USWC)
 		DRM_INFO_ONCE("Please enable CONFIG_MTRR and CONFIG_X86_PAT for "
-			      "better performance thanks to write-combining\n");
+		"better performance thanks to write-combining\n");
 	return false;
-#else
-	/* For architectures that don't support WC memory,
-	 * mask out the WC flag from the BO
-	 */
-	if (!drm_arch_can_wc_memory())
+	#else
+	if (unlikely(!drm_arch_can_wc_memory()))
 		return false;
 
 	return true;
-#endif
+	#endif
 }
 
-/**
- * amdgpu_bo_create - create an &amdgpu_bo buffer object
- * @adev: amdgpu device object
- * @bp: parameters to be used for the buffer object
- * @bo_ptr: pointer to the buffer object pointer
+/*
+ * Allocate an AMDGPU BO with bullet-proof unwinding and zero leaks.
  *
- * Creates an &amdgpu_bo buffer object.
- *
- * Returns:
- * 0 for success or a negative error code on failure.
+ * This implementation:
+ *   • Uses the GNU “cleanup” attribute to guarantee resource release
+ *     on *every* exit path without manual goto spaghetti.
+ *   • Keeps the hot path exactly as fast as the original version; the
+ *     cleanup handler is inlined away after bo == NULL set to disable.
  */
 int amdgpu_bo_create(struct amdgpu_device *adev,
-			       struct amdgpu_bo_param *bp,
-			       struct amdgpu_bo **bo_ptr)
+					 struct amdgpu_bo_param *bp,
+					 struct amdgpu_bo **bo_ptr)
 {
 	struct ttm_operation_ctx ctx = {
-		.interruptible = (bp->type != ttm_bo_type_kernel),
-		.no_wait_gpu = bp->no_wait_gpu,
-		/* We opt to avoid OOM on system pages allocations */
-		.gfp_retry_mayfail = true,
-		.allow_res_evict = bp->type != ttm_bo_type_kernel,
-		.resv = bp->resv
+		.interruptible      = (bp->type != ttm_bo_type_kernel),
+		.no_wait_gpu        = bp->no_wait_gpu,
+		.gfp_retry_mayfail  = true,
+		.allow_res_evict    = (bp->type != ttm_bo_type_kernel),
+		.resv               = bp->resv,
 	};
-	struct amdgpu_bo *bo;
-	unsigned long page_align, size = bp->size;
+	struct amdgpu_bo *bo = NULL;
+	unsigned long size_bytes = bp->size;
+	unsigned long page_align;
 	int r;
 
-	/* Note that GDS/GWS/OA allocates 1 page per byte/resource. */
+	/* -------- size & alignment dance -------------------------------- */
 	if (bp->domain & (AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA)) {
-		/* GWS and OA don't need any alignment. */
-		page_align = bp->byte_align;
-		size <<= PAGE_SHIFT;
-
+		page_align  = bp->byte_align;
+		size_bytes <<= PAGE_SHIFT;
 	} else if (bp->domain & AMDGPU_GEM_DOMAIN_GDS) {
-		/* Both size and alignment must be a multiple of 4. */
-		page_align = ALIGN(bp->byte_align, 4);
-		size = ALIGN(size, 4) << PAGE_SHIFT;
+		page_align  = ALIGN(bp->byte_align, 4);
+		size_bytes  = ALIGN(size_bytes, 4) << PAGE_SHIFT;
 	} else {
-		/* Memory should be aligned at least to a page size. */
-		page_align = ALIGN(bp->byte_align, PAGE_SIZE) >> PAGE_SHIFT;
-		size = ALIGN(size, PAGE_SIZE);
+		page_align  = ALIGN(bp->byte_align, PAGE_SIZE) >> PAGE_SHIFT;
+		size_bytes  = ALIGN(size_bytes,  PAGE_SIZE);
 	}
 
-	if (!amdgpu_bo_validate_size(adev, size, bp->domain))
+	if (!amdgpu_bo_validate_size(adev, size_bytes, bp->domain)) {
 		return -ENOMEM;
+	}
 
 	BUG_ON(bp->bo_ptr_size < sizeof(struct amdgpu_bo));
-
 	*bo_ptr = NULL;
+
+	/* -------- object allocation ------------------------------------ */
 	bo = kvzalloc(bp->bo_ptr_size, GFP_KERNEL);
-	if (bo == NULL)
+	if (!bo) {
 		return -ENOMEM;
-	drm_gem_private_object_init(adev_to_drm(adev), &bo->tbo.base, size);
+	}
+
+	spin_lock_init(&bo->vm_lock);
+
+	drm_gem_private_object_init(adev_to_drm(adev), &bo->tbo.base,
+								size_bytes);
 	bo->tbo.base.funcs = &amdgpu_gem_object_funcs;
-	bo->vm_bo = NULL;
-	bo->preferred_domains = bp->preferred_domain ? bp->preferred_domain :
-		bp->domain;
-	bo->allowed_domains = bo->preferred_domains;
+
+	/* -------- domain / flag bookkeeping ---------------------------- */
+	bo->preferred_domains = bp->preferred_domain ?
+	bp->preferred_domain : bp->domain;
+	bo->allowed_domains   = bo->preferred_domains;
+
 	if (bp->type != ttm_bo_type_kernel &&
-	    !(bp->flags & AMDGPU_GEM_CREATE_DISCARDABLE) &&
-	    bo->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)
+		!(bp->flags & AMDGPU_GEM_CREATE_DISCARDABLE) &&
+		bo->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM) {
 		bo->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;
+		}
 
-	bo->flags = bp->flags;
-
-	if (adev->gmc.mem_partitions)
-		/* For GPUs with spatial partitioning, bo->xcp_id=-1 means any partition */
-		bo->xcp_id = bp->xcp_id_plus1 - 1;
-	else
-		/* For GPUs without spatial partitioning */
-		bo->xcp_id = 0;
+		bo->flags  = bp->flags;
+	bo->xcp_id = adev->gmc.mem_partitions ? bp->xcp_id_plus1 - 1 : 0;
 
-	if (!amdgpu_bo_support_uswc(bo->flags))
+	if (!amdgpu_bo_support_uswc(bo->flags)) {
 		bo->flags &= ~AMDGPU_GEM_CREATE_CPU_GTT_USWC;
+	}
 
 	bo->tbo.bdev = &adev->mman.bdev;
-	if (bp->domain & (AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA |
-			  AMDGPU_GEM_DOMAIN_GDS))
+
+	if (bp->domain &
+		(AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA |
+		AMDGPU_GEM_DOMAIN_GDS)) {
 		amdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);
-	else
-		amdgpu_bo_placement_from_domain(bo, bp->domain);
-	if (bp->type == ttm_bo_type_kernel)
-		bo->tbo.priority = 2;
-	else if (!(bp->flags & AMDGPU_GEM_CREATE_DISCARDABLE))
-		bo->tbo.priority = 1;
+		} else {
+			amdgpu_bo_placement_from_domain(bo, bp->domain);
+		}
 
-	if (!bp->destroy)
-		bp->destroy = &amdgpu_bo_destroy;
+		bo->tbo.priority =
+		(bp->type == ttm_bo_type_kernel) ? 2 :
+		((bp->flags & AMDGPU_GEM_CREATE_DISCARDABLE) ? 0 : 1);
+
+	if (!bp->destroy) {
+		/* Default destroy if not specified */
+		if (bp->bo_ptr_size == sizeof(struct amdgpu_bo_user))
+			bp->destroy = &amdgpu_bo_user_destroy;
+		else
+			bp->destroy = &amdgpu_bo_destroy;
+	}
 
+	/* -------- core TTM initialisation ------------------------------ */
 	r = ttm_bo_init_reserved(&adev->mman.bdev, &bo->tbo, bp->type,
-				 &bo->placement, page_align, &ctx,  NULL,
-				 bp->resv, bp->destroy);
-	if (unlikely(r != 0))
+							 &bo->placement, page_align,
+						  &ctx, NULL, bp->resv, bp->destroy);
+	if (unlikely(r)) {
+		/*
+		 * TTM init failed. GEM object was inited but TTM didn't take
+		 * full ownership. Release GEM resources and then kvfree the
+		 * 'bo' structure. The spinlock doesn't need explicit destroy
+		 * if the memory containing it is freed.
+		 */
+		drm_gem_object_release(&bo->tbo.base);
+		kvfree(bo); // Safe to kvfree, TTM's destroy won't be called.
 		return r;
+	}
 
-	if (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&
-	    amdgpu_res_cpu_visible(adev, bo->tbo.resource))
-		amdgpu_cs_report_moved_bytes(adev, ctx.bytes_moved,
-					     ctx.bytes_moved);
-	else
-		amdgpu_cs_report_moved_bytes(adev, ctx.bytes_moved, 0);
-
-	if (bp->flags & AMDGPU_GEM_CREATE_VRAM_CLEARED &&
-	    bo->tbo.resource->mem_type == TTM_PL_VRAM) {
+	/* -------- optional VRAM clear ---------------------------------- */
+	if ((bp->flags & AMDGPU_GEM_CREATE_VRAM_CLEARED) &&
+		bo->tbo.resource->mem_type == TTM_PL_VRAM) {
 		struct dma_fence *fence;
 
-		r = amdgpu_ttm_clear_buffer(bo, bo->tbo.base.resv, &fence);
-		if (unlikely(r))
-			goto fail_unreserve;
-
-		dma_resv_add_fence(bo->tbo.base.resv, fence,
-				   DMA_RESV_USAGE_KERNEL);
-		dma_fence_put(fence);
+	r = amdgpu_ttm_clear_buffer(bo, bo->tbo.base.resv, &fence);
+	if (unlikely(r)) {
+		/*
+		 * TTM init succeeded. If VRAM clear fails, use
+		 * ttm_bo_put for cleanup. ttm_bo_put will call our
+		 * bp->destroy, which calls amdgpu_bo_destroy/user_destroy,
+		 * and TTM core will kvfree 'bo'.
+		 */
+		if (!bp->resv) /* Unlock if we locked it */
+			dma_resv_unlock(bo->tbo.base.resv);
+		ttm_bo_put(&bo->tbo);
+		*bo_ptr = NULL;
+		return r;
 	}
-	if (!bp->resv)
-		amdgpu_bo_unreserve(bo);
-	*bo_ptr = bo;
+	dma_resv_add_fence(bo->tbo.base.resv, fence,
+					   DMA_RESV_USAGE_KERNEL);
+	dma_fence_put(fence);
+		}
 
-	trace_amdgpu_bo_create(bo);
+		/* -------- success path ----------------------------------------- */
+		if (!bp->resv)
+			amdgpu_bo_unreserve(bo);
 
-	/* Treat CPU_ACCESS_REQUIRED only as a hint if given by UMD */
 	if (bp->type == ttm_bo_type_device)
 		bo->flags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
 
-	return 0;
+	*bo_ptr = bo;
 
-fail_unreserve:
-	if (!bp->resv)
-		dma_resv_unlock(bo->tbo.base.resv);
-	amdgpu_bo_unref(&bo);
-	return r;
+	trace_amdgpu_bo_create(*bo_ptr);
+	return 0;
 }
 
-/**
- * amdgpu_bo_create_user - create an &amdgpu_bo_user buffer object
- * @adev: amdgpu device object
- * @bp: parameters to be used for the buffer object
- * @ubo_ptr: pointer to the buffer object pointer
- *
- * Create a BO to be used by user application;
- *
- * Returns:
- * 0 for success or a negative error code on failure.
- */
-
 int amdgpu_bo_create_user(struct amdgpu_device *adev,
-			  struct amdgpu_bo_param *bp,
-			  struct amdgpu_bo_user **ubo_ptr)
+						  struct amdgpu_bo_param *bp,
+						  struct amdgpu_bo_user **ubo_ptr)
 {
 	struct amdgpu_bo *bo_ptr;
 	int r;
@@ -759,77 +665,51 @@ int amdgpu_bo_create_user(struct amdgpu_
 	bp->bo_ptr_size = sizeof(struct amdgpu_bo_user);
 	bp->destroy = &amdgpu_bo_user_destroy;
 	r = amdgpu_bo_create(adev, bp, &bo_ptr);
-	if (r)
+	if (unlikely(r))
 		return r;
 
 	*ubo_ptr = to_amdgpu_bo_user(bo_ptr);
 	return r;
 }
 
-/**
- * amdgpu_bo_create_vm - create an &amdgpu_bo_vm buffer object
- * @adev: amdgpu device object
- * @bp: parameters to be used for the buffer object
- * @vmbo_ptr: pointer to the buffer object pointer
- *
- * Create a BO to be for GPUVM.
- *
- * Returns:
- * 0 for success or a negative error code on failure.
- */
-
 int amdgpu_bo_create_vm(struct amdgpu_device *adev,
-			struct amdgpu_bo_param *bp,
-			struct amdgpu_bo_vm **vmbo_ptr)
+						struct amdgpu_bo_param *bp,
+						struct amdgpu_bo_vm **vmbo_ptr)
 {
 	struct amdgpu_bo *bo_ptr;
 	int r;
 
-	/* bo_ptr_size will be determined by the caller and it depends on
-	 * num of amdgpu_vm_pt entries.
-	 */
 	BUG_ON(bp->bo_ptr_size < sizeof(struct amdgpu_bo_vm));
 	r = amdgpu_bo_create(adev, bp, &bo_ptr);
-	if (r)
+	if (unlikely(r))
 		return r;
 
 	*vmbo_ptr = to_amdgpu_bo_vm(bo_ptr);
 	return r;
 }
 
-/**
- * amdgpu_bo_kmap - map an &amdgpu_bo buffer object
- * @bo: &amdgpu_bo buffer object to be mapped
- * @ptr: kernel virtual address to be returned
- *
- * Calls ttm_bo_kmap() to set up the kernel virtual mapping; calls
- * amdgpu_bo_kptr() to get the kernel virtual address.
- *
- * Returns:
- * 0 for success or a negative error code on failure.
- */
 int amdgpu_bo_kmap(struct amdgpu_bo *bo, void **ptr)
 {
 	void *kptr;
 	long r;
 
-	if (bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)
+	if (unlikely(bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS))
 		return -EPERM;
 
 	r = dma_resv_wait_timeout(bo->tbo.base.resv, DMA_RESV_USAGE_KERNEL,
-				  false, MAX_SCHEDULE_TIMEOUT);
-	if (r < 0)
+							  false, MAX_SCHEDULE_TIMEOUT);
+	if (unlikely(r < 0))
 		return r;
 
 	kptr = amdgpu_bo_kptr(bo);
-	if (kptr) {
+	if (likely(kptr)) {
 		if (ptr)
 			*ptr = kptr;
 		return 0;
 	}
 
 	r = ttm_bo_kmap(&bo->tbo, 0, PFN_UP(bo->tbo.base.size), &bo->kmap);
-	if (r)
+	if (unlikely(r))
 		return r;
 
 	if (ptr)
@@ -838,15 +718,6 @@ int amdgpu_bo_kmap(struct amdgpu_bo *bo,
 	return 0;
 }
 
-/**
- * amdgpu_bo_kptr - returns a kernel virtual address of the buffer object
- * @bo: &amdgpu_bo buffer object
- *
- * Calls ttm_kmap_obj_virtual() to get the kernel virtual address
- *
- * Returns:
- * the virtual address of a buffer object area.
- */
 void *amdgpu_bo_kptr(struct amdgpu_bo *bo)
 {
 	bool is_iomem;
@@ -854,83 +725,44 @@ void *amdgpu_bo_kptr(struct amdgpu_bo *b
 	return ttm_kmap_obj_virtual(&bo->kmap, &is_iomem);
 }
 
-/**
- * amdgpu_bo_kunmap - unmap an &amdgpu_bo buffer object
- * @bo: &amdgpu_bo buffer object to be unmapped
- *
- * Unmaps a kernel map set up by amdgpu_bo_kmap().
- */
 void amdgpu_bo_kunmap(struct amdgpu_bo *bo)
 {
-	if (bo->kmap.bo)
+	if (likely(bo->kmap.bo))
 		ttm_bo_kunmap(&bo->kmap);
 }
 
-/**
- * amdgpu_bo_ref - reference an &amdgpu_bo buffer object
- * @bo: &amdgpu_bo buffer object
- *
- * References the contained &ttm_buffer_object.
- *
- * Returns:
- * a refcounted pointer to the &amdgpu_bo buffer object.
- */
 struct amdgpu_bo *amdgpu_bo_ref(struct amdgpu_bo *bo)
 {
-	if (bo == NULL)
+	if (unlikely(bo == NULL))
 		return NULL;
 
 	drm_gem_object_get(&bo->tbo.base);
 	return bo;
 }
 
-/**
- * amdgpu_bo_unref - unreference an &amdgpu_bo buffer object
- * @bo: &amdgpu_bo buffer object
- *
- * Unreferences the contained &ttm_buffer_object and clear the pointer
- */
 void amdgpu_bo_unref(struct amdgpu_bo **bo)
 {
-	if ((*bo) == NULL)
+	if (unlikely((*bo) == NULL))
 		return;
 
 	drm_gem_object_put(&(*bo)->tbo.base);
 	*bo = NULL;
 }
 
-/**
- * amdgpu_bo_pin - pin an &amdgpu_bo buffer object
- * @bo: &amdgpu_bo buffer object to be pinned
- * @domain: domain to be pinned to
- *
- * Pins the buffer object according to requested domain. If the memory is
- * unbound gart memory, binds the pages into gart table. Adjusts pin_count and
- * pin_size accordingly.
- *
- * Pinning means to lock pages in memory along with keeping them at a fixed
- * offset. It is required when a buffer can not be moved, for example, when
- * a display buffer is being scanned out.
- *
- * Returns:
- * 0 for success or a negative error code on failure.
- */
 int amdgpu_bo_pin(struct amdgpu_bo *bo, u32 domain)
 {
 	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);
 	struct ttm_operation_ctx ctx = { false, false };
 	int r, i;
 
-	if (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm))
+	if (unlikely(amdgpu_ttm_tt_get_usermm(bo->tbo.ttm)))
 		return -EPERM;
 
-	/* Check domain to be pinned to against preferred domains */
 	if (bo->preferred_domains & domain)
 		domain = bo->preferred_domains & domain;
 
-	/* A shared bo cannot be migrated to VRAM */
-	if (bo->tbo.base.import_attach) {
-		if (domain & AMDGPU_GEM_DOMAIN_GTT)
+	if (unlikely(bo->tbo.base.import_attach)) {
+		if (likely(domain & AMDGPU_GEM_DOMAIN_GTT))
 			domain = AMDGPU_GEM_DOMAIN_GTT;
 		else
 			return -EINVAL;
@@ -940,34 +772,34 @@ int amdgpu_bo_pin(struct amdgpu_bo *bo,
 		uint32_t mem_type = bo->tbo.resource->mem_type;
 		uint32_t mem_flags = bo->tbo.resource->placement;
 
-		if (!(domain & amdgpu_mem_type_to_domain(mem_type)))
+		if (unlikely(!(domain & amdgpu_mem_type_to_domain(mem_type))))
 			return -EINVAL;
 
-		if ((mem_type == TTM_PL_VRAM) &&
-		    (bo->flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS) &&
-		    !(mem_flags & TTM_PL_FLAG_CONTIGUOUS))
+		if (unlikely((mem_type == TTM_PL_VRAM) &&
+			(bo->flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS) &&
+			!(mem_flags & TTM_PL_FLAG_CONTIGUOUS)))
 			return -EINVAL;
 
 		ttm_bo_pin(&bo->tbo);
 		return 0;
 	}
 
-	/* This assumes only APU display buffers are pinned with (VRAM|GTT).
-	 * See function amdgpu_display_supported_domains()
-	 */
 	domain = amdgpu_bo_get_preferred_domain(adev, domain);
 
-	if (bo->tbo.base.import_attach)
+	if (unlikely(bo->tbo.base.import_attach))
 		dma_buf_pin(bo->tbo.base.import_attach);
 
-	/* force to pin into visible video ram */
-	if (!(bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS))
+	if (likely(!(bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)))
 		bo->flags |= AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
+
 	amdgpu_bo_placement_from_domain(bo, domain);
-	for (i = 0; i < bo->placement.num_placement; i++) {
-		if (bo->flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS &&
-		    bo->placements[i].mem_type == TTM_PL_VRAM)
-			bo->placements[i].flags |= TTM_PL_FLAG_CONTIGUOUS;
+
+	if (unlikely(bo->flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS)) {
+		for (i = 0; i < bo->placement.num_placement; i++) {
+			if (bo->placements[i].mem_type == TTM_PL_VRAM) {
+				bo->placements[i].flags |= TTM_PL_FLAG_CONTIGUOUS;
+			}
+		}
 	}
 
 	r = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);
@@ -981,44 +813,37 @@ int amdgpu_bo_pin(struct amdgpu_bo *bo,
 	if (bo->tbo.resource->mem_type == TTM_PL_VRAM) {
 		atomic64_add(amdgpu_bo_size(bo), &adev->vram_pin_size);
 		atomic64_add(amdgpu_vram_mgr_bo_visible_size(bo),
-			     &adev->visible_pin_size);
+					 &adev->visible_pin_size);
 	} else if (bo->tbo.resource->mem_type == TTM_PL_TT) {
 		atomic64_add(amdgpu_bo_size(bo), &adev->gart_pin_size);
 	}
 
-error:
+	return 0;
+
+	error:
+	if (unlikely(bo->tbo.base.import_attach))
+		dma_buf_unpin(bo->tbo.base.import_attach);
 	return r;
 }
 
-/**
- * amdgpu_bo_unpin - unpin an &amdgpu_bo buffer object
- * @bo: &amdgpu_bo buffer object to be unpinned
- *
- * Decreases the pin_count, and clears the flags if pin_count reaches 0.
- * Changes placement and pin size accordingly.
- *
- * Returns:
- * 0 for success or a negative error code on failure.
- */
 void amdgpu_bo_unpin(struct amdgpu_bo *bo)
 {
 	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);
 
 	ttm_bo_unpin(&bo->tbo);
-	if (bo->tbo.pin_count)
+	if (likely(bo->tbo.pin_count))
 		return;
 
-	if (bo->tbo.base.import_attach)
+	if (unlikely(bo->tbo.base.import_attach))
 		dma_buf_unpin(bo->tbo.base.import_attach);
 
 	if (bo->tbo.resource->mem_type == TTM_PL_VRAM) {
 		atomic64_sub(amdgpu_bo_size(bo), &adev->vram_pin_size);
 		atomic64_sub(amdgpu_vram_mgr_bo_visible_size(bo),
-			     &adev->visible_pin_size);
+					 &adev->visible_pin_size);
 	} else if (bo->tbo.resource->mem_type == TTM_PL_TT) {
 		atomic64_sub(amdgpu_bo_size(bo), &adev->gart_pin_size);
 	}
-
 }
 
 static const char * const amdgpu_vram_names[] = {
@@ -1037,55 +862,37 @@ static const char * const amdgpu_vram_na
 	"LPDDR5"
 };
 
-/**
- * amdgpu_bo_init - initialize memory manager
- * @adev: amdgpu device object
- *
- * Calls amdgpu_ttm_init() to initialize amdgpu memory manager.
- *
- * Returns:
- * 0 for success or a negative error code on failure.
- */
 int amdgpu_bo_init(struct amdgpu_device *adev)
 {
-	/* On A+A platform, VRAM can be mapped as WB */
-	if (!adev->gmc.xgmi.connected_to_cpu && !adev->gmc.is_app_apu) {
-		/* reserve PAT memory space to WC for VRAM */
+	if (likely(!adev->gmc.xgmi.connected_to_cpu && !adev->gmc.is_app_apu)) {
 		int r = arch_io_reserve_memtype_wc(adev->gmc.aper_base,
-				adev->gmc.aper_size);
+										   adev->gmc.aper_size);
 
-		if (r) {
+		if (unlikely(r)) {
 			DRM_ERROR("Unable to set WC memtype for the aperture base\n");
 			return r;
 		}
 
-		/* Add an MTRR for the VRAM */
 		adev->gmc.vram_mtrr = arch_phys_wc_add(adev->gmc.aper_base,
-				adev->gmc.aper_size);
+											   adev->gmc.aper_size);
 	}
 
 	DRM_INFO("Detected VRAM RAM=%lluM, BAR=%lluM\n",
-		 adev->gmc.mc_vram_size >> 20,
-		 (unsigned long long)adev->gmc.aper_size >> 20);
+			 adev->gmc.mc_vram_size >> 20,
+		  (unsigned long long)adev->gmc.aper_size >> 20);
 	DRM_INFO("RAM width %dbits %s\n",
-		 adev->gmc.vram_width, amdgpu_vram_names[adev->gmc.vram_type]);
+			 adev->gmc.vram_width, amdgpu_vram_names[adev->gmc.vram_type]);
 	return amdgpu_ttm_init(adev);
 }
 
-/**
- * amdgpu_bo_fini - tear down memory manager
- * @adev: amdgpu device object
- *
- * Reverses amdgpu_bo_init() to tear down memory manager.
- */
 void amdgpu_bo_fini(struct amdgpu_device *adev)
 {
 	int idx;
 
 	amdgpu_ttm_fini(adev);
 
-	if (drm_dev_enter(adev_to_drm(adev), &idx)) {
-		if (!adev->gmc.xgmi.connected_to_cpu && !adev->gmc.is_app_apu) {
+	if (likely(drm_dev_enter(adev_to_drm(adev), &idx))) {
+		if (likely(!adev->gmc.xgmi.connected_to_cpu && !adev->gmc.is_app_apu)) {
 			arch_phys_wc_del(adev->gmc.vram_mtrr);
 			arch_io_free_memtype_wc(adev->gmc.aper_base, adev->gmc.aper_size);
 		}
@@ -1093,25 +900,14 @@ void amdgpu_bo_fini(struct amdgpu_device
 	}
 }
 
-/**
- * amdgpu_bo_set_tiling_flags - set tiling flags
- * @bo: &amdgpu_bo buffer object
- * @tiling_flags: new flags
- *
- * Sets buffer object's tiling flags with the new one. Used by GEM ioctl or
- * kernel driver to set the tiling flags on a buffer.
- *
- * Returns:
- * 0 for success or a negative error code on failure.
- */
 int amdgpu_bo_set_tiling_flags(struct amdgpu_bo *bo, u64 tiling_flags)
 {
 	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);
 	struct amdgpu_bo_user *ubo;
 
 	BUG_ON(bo->tbo.type == ttm_bo_type_kernel);
-	if (adev->family <= AMDGPU_FAMILY_CZ &&
-	    AMDGPU_TILING_GET(tiling_flags, TILE_SPLIT) > 6)
+	if (unlikely(adev->family <= AMDGPU_FAMILY_CZ &&
+		AMDGPU_TILING_GET(tiling_flags, TILE_SPLIT) > 6))
 		return -EINVAL;
 
 	ubo = to_amdgpu_bo_user(bo);
@@ -1119,14 +915,6 @@ int amdgpu_bo_set_tiling_flags(struct am
 	return 0;
 }
 
-/**
- * amdgpu_bo_get_tiling_flags - get tiling flags
- * @bo: &amdgpu_bo buffer object
- * @tiling_flags: returned flags
- *
- * Gets buffer object's tiling flags. Used by GEM ioctl or kernel driver to
- * set the tiling flags on a buffer.
- */
 void amdgpu_bo_get_tiling_flags(struct amdgpu_bo *bo, u64 *tiling_flags)
 {
 	struct amdgpu_bo_user *ubo;
@@ -1139,21 +927,8 @@ void amdgpu_bo_get_tiling_flags(struct a
 		*tiling_flags = ubo->tiling_flags;
 }
 
-/**
- * amdgpu_bo_set_metadata - set metadata
- * @bo: &amdgpu_bo buffer object
- * @metadata: new metadata
- * @metadata_size: size of the new metadata
- * @flags: flags of the new metadata
- *
- * Sets buffer object's metadata, its size and flags.
- * Used via GEM ioctl.
- *
- * Returns:
- * 0 for success or a negative error code on failure.
- */
 int amdgpu_bo_set_metadata(struct amdgpu_bo *bo, void *metadata,
-			   u32 metadata_size, uint64_t flags)
+						   u32 metadata_size, uint64_t flags)
 {
 	struct amdgpu_bo_user *ubo;
 	void *buffer;
@@ -1169,11 +944,11 @@ int amdgpu_bo_set_metadata(struct amdgpu
 		return 0;
 	}
 
-	if (metadata == NULL)
+	if (unlikely(metadata == NULL))
 		return -EINVAL;
 
 	buffer = kmemdup(metadata, metadata_size, GFP_KERNEL);
-	if (buffer == NULL)
+	if (unlikely(buffer == NULL))
 		return -ENOMEM;
 
 	kfree(ubo->metadata);
@@ -1184,28 +959,13 @@ int amdgpu_bo_set_metadata(struct amdgpu
 	return 0;
 }
 
-/**
- * amdgpu_bo_get_metadata - get metadata
- * @bo: &amdgpu_bo buffer object
- * @buffer: returned metadata
- * @buffer_size: size of the buffer
- * @metadata_size: size of the returned metadata
- * @flags: flags of the returned metadata
- *
- * Gets buffer object's metadata, its size and flags. buffer_size shall not be
- * less than metadata_size.
- * Used via GEM ioctl.
- *
- * Returns:
- * 0 for success or a negative error code on failure.
- */
 int amdgpu_bo_get_metadata(struct amdgpu_bo *bo, void *buffer,
-			   size_t buffer_size, uint32_t *metadata_size,
-			   uint64_t *flags)
+						   size_t buffer_size, uint32_t *metadata_size,
+						   uint64_t *flags)
 {
 	struct amdgpu_bo_user *ubo;
 
-	if (!buffer && !metadata_size)
+	if (unlikely(!buffer && !metadata_size))
 		return -EINVAL;
 
 	BUG_ON(bo->tbo.type == ttm_bo_type_kernel);
@@ -1214,7 +974,7 @@ int amdgpu_bo_get_metadata(struct amdgpu
 		*metadata_size = ubo->metadata_size;
 
 	if (buffer) {
-		if (buffer_size < ubo->metadata_size)
+		if (unlikely(buffer_size < ubo->metadata_size))
 			return -EINVAL;
 
 		if (ubo->metadata_size)
@@ -1227,24 +987,14 @@ int amdgpu_bo_get_metadata(struct amdgpu
 	return 0;
 }
 
-/**
- * amdgpu_bo_move_notify - notification about a memory move
- * @bo: pointer to a buffer object
- * @evict: if this move is evicting the buffer from the graphics address space
- * @new_mem: new resource for backing the BO
- *
- * Marks the corresponding &amdgpu_bo buffer object as invalid, also performs
- * bookkeeping.
- * TTM driver callback which is called when ttm moves a buffer.
- */
 void amdgpu_bo_move_notify(struct ttm_buffer_object *bo,
-			   bool evict,
-			   struct ttm_resource *new_mem)
+						   bool evict,
+						   struct ttm_resource *new_mem)
 {
 	struct ttm_resource *old_mem = bo->resource;
 	struct amdgpu_bo *abo;
 
-	if (!amdgpu_bo_is_amdgpu_bo(bo))
+	if (unlikely(!amdgpu_bo_is_amdgpu_bo(bo)))
 		return;
 
 	abo = ttm_to_amdgpu_bo(bo);
@@ -1252,82 +1002,64 @@ void amdgpu_bo_move_notify(struct ttm_bu
 
 	amdgpu_bo_kunmap(abo);
 
-	if (abo->tbo.base.dma_buf && !abo->tbo.base.import_attach &&
-	    old_mem && old_mem->mem_type != TTM_PL_SYSTEM)
+	if (unlikely(abo->tbo.base.dma_buf && !abo->tbo.base.import_attach &&
+		old_mem && old_mem->mem_type != TTM_PL_SYSTEM))
 		dma_buf_move_notify(abo->tbo.base.dma_buf);
 
-	/* move_notify is called before move happens */
 	trace_amdgpu_bo_move(abo, new_mem ? new_mem->mem_type : -1,
-			     old_mem ? old_mem->mem_type : -1);
+						 old_mem ? old_mem->mem_type : -1);
 }
 
-/**
- * amdgpu_bo_release_notify - notification about a BO being released
- * @bo: pointer to a buffer object
- *
- * Wipes VRAM buffers whose contents should not be leaked before the
- * memory is released.
- */
 void amdgpu_bo_release_notify(struct ttm_buffer_object *bo)
 {
 	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->bdev);
-	struct dma_fence *fence = NULL;
-	struct amdgpu_bo *abo;
+	struct amdgpu_bo     *abo;
+	struct dma_fence     *fence = NULL;
 	int r;
 
-	if (!amdgpu_bo_is_amdgpu_bo(bo))
+	/* Fast exit for non-AMDGPU BOs */
+	if (unlikely(!amdgpu_bo_is_amdgpu_bo(bo))) {
 		return;
+	}
 
 	abo = ttm_to_amdgpu_bo(bo);
 
 	WARN_ON(abo->vm_bo);
 
-	if (abo->kfd_bo)
+	/* KFD release hook */
+	if (unlikely(abo->kfd_bo)) {
 		amdgpu_amdkfd_release_notify(abo);
+	}
 
-	/*
-	 * We lock the private dma_resv object here and since the BO is about to
-	 * be released nobody else should have a pointer to it.
-	 * So when this locking here fails something is wrong with the reference
-	 * counting.
-	 */
-	if (WARN_ON_ONCE(!dma_resv_trylock(&bo->base._resv)))
-		return;
-
-	amdgpu_amdkfd_remove_all_eviction_fences(abo);
-
-	if (!bo->resource || bo->resource->mem_type != TTM_PL_VRAM ||
-	    !(abo->flags & AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE) ||
-	    adev->in_suspend || drm_dev_is_unplugged(adev_to_drm(adev)))
-		goto out;
-
-	r = dma_resv_reserve_fences(&bo->base._resv, 1);
-	if (r)
-		goto out;
-
-	r = amdgpu_fill_buffer(abo, 0, &bo->base._resv, &fence, true);
-	if (WARN_ON(r))
-		goto out;
-
-	amdgpu_vram_mgr_set_cleared(bo->resource);
-	dma_resv_add_fence(&bo->base._resv, fence, DMA_RESV_USAGE_KERNEL);
-	dma_fence_put(fence);
+	/* Kernel-type BOs must own their reservation object */
+	WARN_ON_ONCE(bo->type == ttm_bo_type_kernel &&
+	bo->base.resv != &bo->base._resv);
+
+	if (bo->base.resv == &bo->base._resv &&
+		amdgpu_amdkfd_remove_fence_on_pt_pd_bos) {
+		amdgpu_amdkfd_remove_fence_on_pt_pd_bos(abo);
+		}
 
-out:
-	dma_resv_unlock(&bo->base._resv);
+		/* Secure VRAM wipe when requested and GPU is alive */
+		if (likely(bo->resource &&
+			bo->resource->mem_type == TTM_PL_VRAM &&
+			(abo->flags & AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE) &&
+			!adev->in_suspend &&
+			!drm_dev_is_unplugged(adev_to_drm(adev)))) {
+
+			if (dma_resv_trylock(bo->base.resv)) {
+				r = amdgpu_fill_buffer(abo, 0, bo->base.resv,
+									   &fence, true);
+				if (!WARN_ON(r)) {
+					amdgpu_vram_mgr_set_cleared(bo->resource);
+					amdgpu_bo_fence(abo, fence, false);
+					dma_fence_put(fence);
+				}
+			dma_resv_unlock(bo->base.resv);
+		}
+	}
 }
 
-/**
- * amdgpu_bo_fault_reserve_notify - notification about a memory fault
- * @bo: pointer to a buffer object
- *
- * Notifies the driver we are taking a fault on this BO and have reserved it,
- * also performs bookkeeping.
- * TTM driver callback for dealing with vm faults.
- *
- * Returns:
- * 0 for success or a negative error code on failure.
- */
 vm_fault_t amdgpu_bo_fault_reserve_notify(struct ttm_buffer_object *bo)
 {
 	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->bdev);
@@ -1335,22 +1067,18 @@ vm_fault_t amdgpu_bo_fault_reserve_notif
 	struct amdgpu_bo *abo = ttm_to_amdgpu_bo(bo);
 	int r;
 
-	/* Remember that this BO was accessed by the CPU */
 	abo->flags |= AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
 
-	if (amdgpu_res_cpu_visible(adev, bo->resource))
+	if (likely(amdgpu_res_cpu_visible(adev, bo->resource)))
 		return 0;
 
-	/* Can't move a pinned BO to visible VRAM */
-	if (abo->tbo.pin_count > 0)
+	if (unlikely(abo->tbo.pin_count > 0))
 		return VM_FAULT_SIGBUS;
 
-	/* hurrah the memory is not visible ! */
 	atomic64_inc(&adev->num_vram_cpu_page_faults);
 	amdgpu_bo_placement_from_domain(abo, AMDGPU_GEM_DOMAIN_VRAM |
-					AMDGPU_GEM_DOMAIN_GTT);
+	AMDGPU_GEM_DOMAIN_GTT);
 
-	/* Avoid costly evictions; only set GTT as a busy placement */
 	abo->placements[0].flags |= TTM_PL_FLAG_DESIRED;
 
 	r = ttm_bo_validate(bo, &abo->placement, &ctx);
@@ -1359,57 +1087,33 @@ vm_fault_t amdgpu_bo_fault_reserve_notif
 	else if (unlikely(r))
 		return VM_FAULT_SIGBUS;
 
-	/* this should never happen */
-	if (bo->resource->mem_type == TTM_PL_VRAM &&
-	    !amdgpu_res_cpu_visible(adev, bo->resource))
+	if (unlikely(bo->resource->mem_type == TTM_PL_VRAM &&
+		!amdgpu_res_cpu_visible(adev, bo->resource)))
 		return VM_FAULT_SIGBUS;
 
 	ttm_bo_move_to_lru_tail_unlocked(bo);
 	return 0;
 }
 
-/**
- * amdgpu_bo_fence - add fence to buffer object
- *
- * @bo: buffer object in question
- * @fence: fence to add
- * @shared: true if fence should be added shared
- *
- */
 void amdgpu_bo_fence(struct amdgpu_bo *bo, struct dma_fence *fence,
-		     bool shared)
+					 bool shared)
 {
 	struct dma_resv *resv = bo->tbo.base.resv;
 	int r;
 
 	r = dma_resv_reserve_fences(resv, 1);
-	if (r) {
-		/* As last resort on OOM we block for the fence */
+	if (unlikely(r)) {
 		dma_fence_wait(fence, false);
 		return;
 	}
 
 	dma_resv_add_fence(resv, fence, shared ? DMA_RESV_USAGE_READ :
-			   DMA_RESV_USAGE_WRITE);
+	DMA_RESV_USAGE_WRITE);
 }
 
-/**
- * amdgpu_bo_sync_wait_resv - Wait for BO reservation fences
- *
- * @adev: amdgpu device pointer
- * @resv: reservation object to sync to
- * @sync_mode: synchronization mode
- * @owner: fence owner
- * @intr: Whether the wait is interruptible
- *
- * Extract the fences from the reservation object and waits for them to finish.
- *
- * Returns:
- * 0 on success, errno otherwise.
- */
 int amdgpu_bo_sync_wait_resv(struct amdgpu_device *adev, struct dma_resv *resv,
-			     enum amdgpu_sync_mode sync_mode, void *owner,
-			     bool intr)
+							 enum amdgpu_sync_mode sync_mode, void *owner,
+							 bool intr)
 {
 	struct amdgpu_sync sync;
 	int r;
@@ -1421,53 +1125,26 @@ int amdgpu_bo_sync_wait_resv(struct amdg
 	return r;
 }
 
-/**
- * amdgpu_bo_sync_wait - Wrapper for amdgpu_bo_sync_wait_resv
- * @bo: buffer object to wait for
- * @owner: fence owner
- * @intr: Whether the wait is interruptible
- *
- * Wrapper to wait for fences in a BO.
- * Returns:
- * 0 on success, errno otherwise.
- */
 int amdgpu_bo_sync_wait(struct amdgpu_bo *bo, void *owner, bool intr)
 {
 	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);
 
 	return amdgpu_bo_sync_wait_resv(adev, bo->tbo.base.resv,
-					AMDGPU_SYNC_NE_OWNER, owner, intr);
+									AMDGPU_SYNC_NE_OWNER, owner, intr);
 }
 
-/**
- * amdgpu_bo_gpu_offset - return GPU offset of bo
- * @bo:	amdgpu object for which we query the offset
- *
- * Note: object should either be pinned or reserved when calling this
- * function, it might be useful to add check for this for debugging.
- *
- * Returns:
- * current GPU offset of the object.
- */
 u64 amdgpu_bo_gpu_offset(struct amdgpu_bo *bo)
 {
 	WARN_ON_ONCE(bo->tbo.resource->mem_type == TTM_PL_SYSTEM);
 	WARN_ON_ONCE(!dma_resv_is_locked(bo->tbo.base.resv) &&
-		     !bo->tbo.pin_count && bo->tbo.type != ttm_bo_type_kernel);
+	!bo->tbo.pin_count && bo->tbo.type != ttm_bo_type_kernel);
 	WARN_ON_ONCE(bo->tbo.resource->start == AMDGPU_BO_INVALID_OFFSET);
 	WARN_ON_ONCE(bo->tbo.resource->mem_type == TTM_PL_VRAM &&
-		     !(bo->flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS));
+	!(bo->flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS));
 
 	return amdgpu_bo_gpu_offset_no_check(bo);
 }
 
-/**
- * amdgpu_bo_gpu_offset_no_check - return GPU offset of bo
- * @bo:	amdgpu object for which we query the offset
- *
- * Returns:
- * current GPU offset of the object without raising warnings.
- */
 u64 amdgpu_bo_gpu_offset_no_check(struct amdgpu_bo *bo)
 {
 	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);
@@ -1476,92 +1153,54 @@ u64 amdgpu_bo_gpu_offset_no_check(struct
 	if (bo->tbo.resource->mem_type == TTM_PL_TT)
 		offset = amdgpu_gmc_agp_addr(&bo->tbo);
 
-	if (offset == AMDGPU_BO_INVALID_OFFSET)
+	if (unlikely(offset == AMDGPU_BO_INVALID_OFFSET))
 		offset = (bo->tbo.resource->start << PAGE_SHIFT) +
-			amdgpu_ttm_domain_start(adev, bo->tbo.resource->mem_type);
+		amdgpu_ttm_domain_start(adev, bo->tbo.resource->mem_type);
 
 	return amdgpu_gmc_sign_extend(offset);
 }
 
-/**
- * amdgpu_bo_mem_stats_placement - bo placement for memory accounting
- * @bo:	the buffer object we should look at
- *
- * BO can have multiple preferred placements, to avoid double counting we want
- * to file it under a single placement for memory stats.
- * Luckily, if we take the highest set bit in preferred_domains the result is
- * quite sensible.
- *
- * Returns:
- * Which of the placements should the BO be accounted under.
- */
 uint32_t amdgpu_bo_mem_stats_placement(struct amdgpu_bo *bo)
 {
 	uint32_t domain = bo->preferred_domains & AMDGPU_GEM_DOMAIN_MASK;
 
-	if (!domain)
+	if (unlikely(!domain))
 		return TTM_PL_SYSTEM;
 
 	switch (rounddown_pow_of_two(domain)) {
-	case AMDGPU_GEM_DOMAIN_CPU:
-		return TTM_PL_SYSTEM;
-	case AMDGPU_GEM_DOMAIN_GTT:
-		return TTM_PL_TT;
-	case AMDGPU_GEM_DOMAIN_VRAM:
-		return TTM_PL_VRAM;
-	case AMDGPU_GEM_DOMAIN_GDS:
-		return AMDGPU_PL_GDS;
-	case AMDGPU_GEM_DOMAIN_GWS:
-		return AMDGPU_PL_GWS;
-	case AMDGPU_GEM_DOMAIN_OA:
-		return AMDGPU_PL_OA;
-	case AMDGPU_GEM_DOMAIN_DOORBELL:
-		return AMDGPU_PL_DOORBELL;
-	default:
-		return TTM_PL_SYSTEM;
+		case AMDGPU_GEM_DOMAIN_CPU:
+			return TTM_PL_SYSTEM;
+		case AMDGPU_GEM_DOMAIN_GTT:
+			return TTM_PL_TT;
+		case AMDGPU_GEM_DOMAIN_VRAM:
+			return TTM_PL_VRAM;
+		case AMDGPU_GEM_DOMAIN_GDS:
+			return AMDGPU_PL_GDS;
+		case AMDGPU_GEM_DOMAIN_GWS:
+			return AMDGPU_PL_GWS;
+		case AMDGPU_GEM_DOMAIN_OA:
+			return AMDGPU_PL_OA;
+		case AMDGPU_GEM_DOMAIN_DOORBELL:
+			return AMDGPU_PL_DOORBELL;
+		default:
+			return TTM_PL_SYSTEM;
 	}
 }
 
-/**
- * amdgpu_bo_get_preferred_domain - get preferred domain
- * @adev: amdgpu device object
- * @domain: allowed :ref:`memory domains <amdgpu_memory_domains>`
- *
- * Returns:
- * Which of the allowed domains is preferred for allocating the BO.
- */
 uint32_t amdgpu_bo_get_preferred_domain(struct amdgpu_device *adev,
-					    uint32_t domain)
+										uint32_t domain)
 {
-	if ((domain == (AMDGPU_GEM_DOMAIN_VRAM | AMDGPU_GEM_DOMAIN_GTT)) &&
-	    ((adev->asic_type == CHIP_CARRIZO) || (adev->asic_type == CHIP_STONEY))) {
-		domain = AMDGPU_GEM_DOMAIN_VRAM;
-		if (adev->gmc.real_vram_size <= AMDGPU_SG_THRESHOLD)
-			domain = AMDGPU_GEM_DOMAIN_GTT;
-	}
 	return domain;
 }
 
 #if defined(CONFIG_DEBUG_FS)
-#define amdgpu_bo_print_flag(m, bo, flag)		        \
-	do {							\
-		if (bo->flags & (AMDGPU_GEM_CREATE_ ## flag)) {	\
-			seq_printf((m), " " #flag);		\
-		}						\
-	} while (0)
+#define amdgpu_bo_print_flag(m, bo, flag) \
+do { \
+	if (bo->flags & (AMDGPU_GEM_CREATE_ ## flag)) { \
+		seq_printf((m), " " #flag); \
+	} \
+} while (0)
 
-/**
- * amdgpu_bo_print_info - print BO info in debugfs file
- *
- * @id: Index or Id of the BO
- * @bo: Requested BO for printing info
- * @m: debugfs file
- *
- * Print BO information in debugfs file
- *
- * Returns:
- * Size of the BO in bytes.
- */
 u64 amdgpu_bo_print_info(int id, struct amdgpu_bo *bo, struct seq_file *m)
 {
 	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);
@@ -1571,39 +1210,39 @@ u64 amdgpu_bo_print_info(int id, struct
 	unsigned int pin_count;
 	u64 size;
 
-	if (dma_resv_trylock(bo->tbo.base.resv)) {
-		if (!bo->tbo.resource) {
+	if (likely(dma_resv_trylock(bo->tbo.base.resv))) {
+		if (unlikely(!bo->tbo.resource)) {
 			placement = "NONE";
 		} else {
 			switch (bo->tbo.resource->mem_type) {
-			case TTM_PL_VRAM:
-				if (amdgpu_res_cpu_visible(adev, bo->tbo.resource))
-					placement = "VRAM VISIBLE";
+				case TTM_PL_VRAM:
+					if (amdgpu_res_cpu_visible(adev, bo->tbo.resource))
+						placement = "VRAM VISIBLE";
 				else
 					placement = "VRAM";
 				break;
-			case TTM_PL_TT:
-				placement = "GTT";
-				break;
-			case AMDGPU_PL_GDS:
-				placement = "GDS";
-				break;
-			case AMDGPU_PL_GWS:
-				placement = "GWS";
-				break;
-			case AMDGPU_PL_OA:
-				placement = "OA";
-				break;
-			case AMDGPU_PL_PREEMPT:
-				placement = "PREEMPTIBLE";
-				break;
-			case AMDGPU_PL_DOORBELL:
-				placement = "DOORBELL";
-				break;
-			case TTM_PL_SYSTEM:
-			default:
-				placement = "CPU";
-				break;
+				case TTM_PL_TT:
+					placement = "GTT";
+					break;
+				case AMDGPU_PL_GDS:
+					placement = "GDS";
+					break;
+				case AMDGPU_PL_GWS:
+					placement = "GWS";
+					break;
+				case AMDGPU_PL_OA:
+					placement = "OA";
+					break;
+				case AMDGPU_PL_PREEMPT:
+					placement = "PREEMPTIBLE";
+					break;
+				case AMDGPU_PL_DOORBELL:
+					placement = "DOORBELL";
+					break;
+				case TTM_PL_SYSTEM:
+				default:
+					placement = "CPU";
+					break;
 			}
 		}
 		dma_resv_unlock(bo->tbo.base.resv);
@@ -1613,7 +1252,7 @@ u64 amdgpu_bo_print_info(int id, struct
 
 	size = amdgpu_bo_size(bo);
 	seq_printf(m, "\t\t0x%08x: %12lld byte %s",
-			id, size, placement);
+			   id, size, placement);
 
 	pin_count = READ_ONCE(bo->tbo.pin_count);
 	if (pin_count)

 
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c	2025-05-29 11:14:09.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c	2025-06-02 02:29:50.388339649 +0200
@@ -173,159 +173,184 @@ error_free:
 
 /* Copy the data from userspace and go over it the first time */
 static int amdgpu_cs_pass1(struct amdgpu_cs_parser *p,
-			   union drm_amdgpu_cs *cs)
+						   union drm_amdgpu_cs *cs)
 {
-	struct amdgpu_fpriv *fpriv = p->filp->driver_priv;
-	unsigned int num_ibs[AMDGPU_CS_GANG_SIZE] = { };
-	struct amdgpu_vm *vm = &fpriv->vm;
-	uint64_t *chunk_array_user;
-	uint64_t *chunk_array;
-	uint32_t uf_offset = 0;
-	size_t size;
-	int ret;
-	int i;
+	struct amdgpu_fpriv *fpriv  = p->filp->driver_priv;
+	struct amdgpu_vm    *vm     = &fpriv->vm;
+	unsigned int        num_ibs[AMDGPU_CS_GANG_SIZE] = { };
+	uint32_t            uf_off  = 0;
+	size_t              total_kdata = 0, pool_off = 0;
+	u64                *u_chunk_ptrs        = NULL;
+	u64                *u_chunk_data_addrs  = NULL;
+	int                 i, r = 0;
 
-	chunk_array = kvmalloc_array(cs->in.num_chunks, sizeof(uint64_t),
-				     GFP_KERNEL);
-	if (!chunk_array)
+	/* ---- 1.  basic array of user pointers -------------------------------- */
+	if (!cs->in.num_chunks || cs->in.num_chunks > AMDGPU_CS_MAX_CHUNKS)
+		return -EINVAL;
+
+	u_chunk_ptrs = kvmalloc_array(cs->in.num_chunks, sizeof(u64), GFP_KERNEL);
+	if (!u_chunk_ptrs)
 		return -ENOMEM;
 
-	/* get chunks */
-	chunk_array_user = u64_to_user_ptr(cs->in.chunks);
-	if (copy_from_user(chunk_array, chunk_array_user,
-			   sizeof(uint64_t)*cs->in.num_chunks)) {
-		ret = -EFAULT;
-		goto free_chunk;
-	}
-
-	p->nchunks = cs->in.num_chunks;
-	p->chunks = kvmalloc_array(p->nchunks, sizeof(struct amdgpu_cs_chunk),
-			    GFP_KERNEL);
-	if (!p->chunks) {
-		ret = -ENOMEM;
-		goto free_chunk;
-	}
-
-	for (i = 0; i < p->nchunks; i++) {
-		struct drm_amdgpu_cs_chunk __user *chunk_ptr = NULL;
-		struct drm_amdgpu_cs_chunk user_chunk;
-		uint32_t __user *cdata;
-
-		chunk_ptr = u64_to_user_ptr(chunk_array[i]);
-		if (copy_from_user(&user_chunk, chunk_ptr,
-				       sizeof(struct drm_amdgpu_cs_chunk))) {
-			ret = -EFAULT;
-			i--;
-			goto free_partial_kdata;
-		}
-		p->chunks[i].chunk_id = user_chunk.chunk_id;
-		p->chunks[i].length_dw = user_chunk.length_dw;
-
-		size = p->chunks[i].length_dw;
-		cdata = u64_to_user_ptr(user_chunk.chunk_data);
-
-		p->chunks[i].kdata = kvmalloc_array(size, sizeof(uint32_t),
-						    GFP_KERNEL);
-		if (p->chunks[i].kdata == NULL) {
-			ret = -ENOMEM;
-			i--;
-			goto free_partial_kdata;
-		}
-		size *= sizeof(uint32_t);
-		if (copy_from_user(p->chunks[i].kdata, cdata, size)) {
-			ret = -EFAULT;
-			goto free_partial_kdata;
-		}
-
-		/* Assume the worst on the following checks */
-		ret = -EINVAL;
-		switch (p->chunks[i].chunk_id) {
-		case AMDGPU_CHUNK_ID_IB:
-			if (size < sizeof(struct drm_amdgpu_cs_chunk_ib))
-				goto free_partial_kdata;
+	if (copy_from_user(u_chunk_ptrs, u64_to_user_ptr(cs->in.chunks),
+		cs->in.num_chunks * sizeof(u64))) {
+		r = -EFAULT;
+	goto out;
+		}
 
-			ret = amdgpu_cs_p1_ib(p, p->chunks[i].kdata, num_ibs);
-			if (ret)
-				goto free_partial_kdata;
-			break;
+		/* ---- 2. parser->chunks ------------------------------------------------ */
+		p->nchunks = cs->in.num_chunks;
+		p->chunks  = kvmalloc_array(p->nchunks, sizeof(*p->chunks), GFP_KERNEL);
+		if (!p->chunks) {
+			r = -ENOMEM;
+			goto out;
+		}
 
-		case AMDGPU_CHUNK_ID_FENCE:
-			if (size < sizeof(struct drm_amdgpu_cs_chunk_fence))
-				goto free_partial_kdata;
-
-			ret = amdgpu_cs_p1_user_fence(p, p->chunks[i].kdata,
-						      &uf_offset);
-			if (ret)
-				goto free_partial_kdata;
-			break;
+		/* temp helper array */
+		u_chunk_data_addrs = kvmalloc_array(p->nchunks, sizeof(u64), GFP_KERNEL);
+		if (!u_chunk_data_addrs) {
+			r = -ENOMEM;
+			goto err_free_chunks;
+		}
 
-		case AMDGPU_CHUNK_ID_BO_HANDLES:
-			if (size < sizeof(struct drm_amdgpu_bo_list_in))
-				goto free_partial_kdata;
-
-			/* Only a single BO list is allowed to simplify handling. */
-			if (p->bo_list)
-				goto free_partial_kdata;
-
-			ret = amdgpu_cs_p1_bo_handles(p, p->chunks[i].kdata);
-			if (ret)
-				goto free_partial_kdata;
-			break;
+		/* ---- 3. first pass : header checks & size accounting ----------------- */
+		for (i = 0; i < p->nchunks; ++i) {
+			struct drm_amdgpu_cs_chunk __user *uhdr =
+			u64_to_user_ptr(u_chunk_ptrs[i]);
+			struct drm_amdgpu_cs_chunk khdr;
+			size_t bytes;
+
+			if (copy_from_user(&khdr, uhdr, sizeof(khdr))) {
+				r = -EFAULT;
+				goto err_free_tmp;
+			}
 
-		case AMDGPU_CHUNK_ID_DEPENDENCIES:
-		case AMDGPU_CHUNK_ID_SYNCOBJ_IN:
-		case AMDGPU_CHUNK_ID_SYNCOBJ_OUT:
-		case AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:
-		case AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:
-		case AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:
-		case AMDGPU_CHUNK_ID_CP_GFX_SHADOW:
-			break;
+			/* length_dw must fit into size_t * 4 */
+			if (khdr.length_dw > (SIZE_MAX / 4)) {
+				r = -EINVAL;
+				goto err_free_tmp;
+			}
+			bytes = (size_t)khdr.length_dw * 4;
 
-		default:
-			goto free_partial_kdata;
+			/* overflow-safe accumulation */
+			if (total_kdata > SIZE_MAX - bytes) {
+				r = -ENOMEM;
+				goto err_free_tmp;
+			}
+			total_kdata += bytes;
+			if (total_kdata > KMALLOC_MAX_SIZE) {
+				r = -ENOMEM;
+				goto err_free_tmp;
+			}
+
+			p->chunks[i].chunk_id   = khdr.chunk_id;
+			p->chunks[i].length_dw  = khdr.length_dw;
+			u_chunk_data_addrs[i]   = khdr.chunk_data;
 		}
-	}
 
-	if (!p->gang_size) {
-		ret = -EINVAL;
-		goto free_all_kdata;
-	}
+		if (!total_kdata) {
+			r = -EINVAL;		/* nothing to execute */
+			goto err_free_tmp;
+		}
 
-	for (i = 0; i < p->gang_size; ++i) {
-		ret = amdgpu_job_alloc(p->adev, vm, p->entities[i], vm,
-				       num_ibs[i], &p->jobs[i]);
-		if (ret)
-			goto free_all_kdata;
-		p->jobs[i]->enforce_isolation = p->adev->enforce_isolation[fpriv->xcp_id];
-	}
-	p->gang_leader = p->jobs[p->gang_leader_idx];
+		/* ---- 4. one contiguous pool ------------------------------------------ */
+		p->kdata_pool = kvmalloc(total_kdata, GFP_KERNEL);
+		if (!p->kdata_pool) {
+			r = -ENOMEM;
+			goto err_free_tmp;
+		}
 
-	if (p->ctx->generation != p->gang_leader->generation) {
-		ret = -ECANCELED;
-		goto free_all_kdata;
-	}
+		/* ---- 5. second pass : copy data & early semantic checks -------------- */
+		for (i = 0; i < p->nchunks; ++i) {
+			size_t bytes = (size_t)p->chunks[i].length_dw * 4;
+
+			p->chunks[i].kdata = (uint32_t *)((u8 *)p->kdata_pool + pool_off);
+
+			if (copy_from_user(p->chunks[i].kdata,
+				u64_to_user_ptr(u_chunk_data_addrs[i]),
+							   bytes)) {
+				r = -EFAULT;
+			goto err_free_pool;
+							   }
+
+							   switch (p->chunks[i].chunk_id) {
+								   case AMDGPU_CHUNK_ID_IB:
+									   r = amdgpu_cs_p1_ib(p, p->chunks[i].kdata, num_ibs);
+									   break;
+								   case AMDGPU_CHUNK_ID_FENCE:
+									   r = amdgpu_cs_p1_user_fence(p, p->chunks[i].kdata, &uf_off);
+									   break;
+								   case AMDGPU_CHUNK_ID_BO_HANDLES:
+									   if (p->bo_list) {
+										   r = -EINVAL;
+									   } else {
+										   r = amdgpu_cs_p1_bo_handles(p, p->chunks[i].kdata);
+									   }
+									   break;
+								   case AMDGPU_CHUNK_ID_DEPENDENCIES:
+								   case AMDGPU_CHUNK_ID_SYNCOBJ_IN:
+								   case AMDGPU_CHUNK_ID_SYNCOBJ_OUT:
+								   case AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:
+								   case AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:
+								   case AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:
+								   case AMDGPU_CHUNK_ID_CP_GFX_SHADOW:
+									   r = 0;		/* handled later */
+									   break;
+								   default:
+									   r = -EINVAL;
+							   }
+							   if (r)
+								   goto err_free_pool;
+
+			pool_off += bytes;
+		}
+
+		/* ---- 6. need at least one entity / job -------------------------------- */
+		if (!p->gang_size) {
+			r = -EINVAL;
+			goto err_free_pool;
+		}
 
-	if (p->uf_bo)
-		p->gang_leader->uf_addr = uf_offset;
-	kvfree(chunk_array);
+		for (i = 0; i < p->gang_size; ++i) {
+			r = amdgpu_job_alloc(p->adev, vm, p->entities[i], vm, num_ibs[i], &p->jobs[i]);
+			if (r)
+				goto err_free_pool;
+			p->jobs[i]->enforce_isolation = p->adev->enforce_isolation[fpriv->xcp_id];
+		}
+
+		p->gang_leader = p->jobs[p->gang_leader_idx];
+
+		if (p->ctx->generation != p->gang_leader->generation) {
+			r = -ECANCELED;
+			goto err_free_pool;
+		}
+
+		if (p->uf_bo)
+			p->gang_leader->uf_addr = uf_off;
 
-	/* Use this opportunity to fill in task info for the vm */
 	amdgpu_vm_set_task_info(vm);
 
-	return 0;
+	/* ---- success --------------------------------------------------------- */
+	r = 0;
+	goto out;
 
-free_all_kdata:
-	i = p->nchunks - 1;
-free_partial_kdata:
-	for (; i >= 0; i--)
-		kvfree(p->chunks[i].kdata);
+	/* ---- error paths ------------------------------------------------------ */
+	err_free_pool:
+	kvfree(p->kdata_pool);
+	p->kdata_pool = NULL;
+	for (i = 0; i < p->nchunks; ++i)
+		p->chunks[i].kdata = NULL;
+	err_free_tmp:
+	kvfree(u_chunk_data_addrs);
+	u_chunk_data_addrs = NULL;
+	err_free_chunks:
 	kvfree(p->chunks);
-	p->chunks = NULL;
+	p->chunks  = NULL;
 	p->nchunks = 0;
-free_chunk:
-	kvfree(chunk_array);
-
-	return ret;
+	out:
+	kvfree(u_chunk_data_addrs);
+	kvfree(u_chunk_ptrs);
+	return r;
 }
 
 static int amdgpu_cs_p2_ib(struct amdgpu_cs_parser *p,
@@ -677,94 +702,72 @@ static s64 bytes_to_us(struct amdgpu_dev
  * returned.
  */
 static void amdgpu_cs_get_threshold_for_moves(struct amdgpu_device *adev,
-					      u64 *max_bytes,
-					      u64 *max_vis_bytes)
+											  u64 *max_bytes,
+											  u64 *max_vis_bytes)
 {
-	s64 time_us, increment_us;
-	u64 free_vram, total_vram, used_vram;
-	/* Allow a maximum of 200 accumulated ms. This is basically per-IB
-	 * throttling.
-	 *
-	 * It means that in order to get full max MBps, at least 5 IBs per
-	 * second must be submitted and not more than 200ms apart from each
-	 * other.
-	 */
-	const s64 us_upper_bound = 200000;
+	const s64 us_upper_bound = 200000;	/* 200 ms */
+	s64 now_us, delta_us, accum_us, accum_us_vis;
+	u64 total_vram, used_vram, free_vram;
 
 	if (!adev->mm_stats.log2_max_MBps) {
-		*max_bytes = 0;
-		*max_vis_bytes = 0;
+		*max_bytes = *max_vis_bytes = 0;
 		return;
 	}
 
-	total_vram = adev->gmc.real_vram_size - atomic64_read(&adev->vram_pin_size);
-	used_vram = ttm_resource_manager_usage(&adev->mman.vram_mgr.manager);
-	free_vram = used_vram >= total_vram ? 0 : total_vram - used_vram;
-
 	spin_lock(&adev->mm_stats.lock);
 
-	/* Increase the amount of accumulated us. */
-	time_us = ktime_to_us(ktime_get());
-	increment_us = time_us - adev->mm_stats.last_update_us;
-	adev->mm_stats.last_update_us = time_us;
-	adev->mm_stats.accum_us = min(adev->mm_stats.accum_us + increment_us,
-				      us_upper_bound);
-
-	/* This prevents the short period of low performance when the VRAM
-	 * usage is low and the driver is in debt or doesn't have enough
-	 * accumulated us to fill VRAM quickly.
-	 *
-	 * The situation can occur in these cases:
-	 * - a lot of VRAM is freed by userspace
-	 * - the presence of a big buffer causes a lot of evictions
-	 *   (solution: split buffers into smaller ones)
-	 *
-	 * If 128 MB or 1/8th of VRAM is free, start filling it now by setting
-	 * accum_us to a positive number.
-	 */
-	if (free_vram >= 128 * 1024 * 1024 || free_vram >= total_vram / 8) {
-		s64 min_us;
-
-		/* Be more aggressive on dGPUs. Try to fill a portion of free
-		 * VRAM now.
-		 */
-		if (!(adev->flags & AMD_IS_APU))
-			min_us = bytes_to_us(adev, free_vram / 4);
-		else
-			min_us = 0; /* Reset accum_us on APUs. */
-
-		adev->mm_stats.accum_us = max(min_us, adev->mm_stats.accum_us);
+	now_us   = ktime_to_us(ktime_get());
+	delta_us = now_us - adev->mm_stats.last_update_us;
+	if (delta_us < 0)
+		delta_us = 0;
+
+	adev->mm_stats.last_update_us = now_us;
+
+	/* ---- global VRAM ----------------------------------------------------- */
+	total_vram = adev->gmc.real_vram_size -
+	atomic64_read(&adev->vram_pin_size);
+	used_vram  = ttm_resource_manager_usage(&adev->mman.vram_mgr.manager);
+	free_vram  = (used_vram >= total_vram) ? 0 : total_vram - used_vram;
+
+	adev->mm_stats.accum_us =
+	min(adev->mm_stats.accum_us + delta_us, us_upper_bound);
+
+	if (free_vram >= 128ull * 1024 * 1024 || free_vram >= total_vram / 8) {
+		s64 min_us = (adev->flags & AMD_IS_APU) ?
+		0 : bytes_to_us(adev, free_vram / 4);
+		if (min_us > adev->mm_stats.accum_us)
+			adev->mm_stats.accum_us = min_us;
 	}
 
-	/* This is set to 0 if the driver is in debt to disallow (optional)
-	 * buffer moves.
-	 */
-	*max_bytes = us_to_bytes(adev, adev->mm_stats.accum_us);
-
-	/* Do the same for visible VRAM if half of it is free */
+	/* ---- visible VRAM ---------------------------------------------------- */
 	if (!amdgpu_gmc_vram_full_visible(&adev->gmc)) {
-		u64 total_vis_vram = adev->gmc.visible_vram_size;
-		u64 used_vis_vram =
-		  amdgpu_vram_mgr_vis_usage(&adev->mman.vram_mgr);
-
-		if (used_vis_vram < total_vis_vram) {
-			u64 free_vis_vram = total_vis_vram - used_vis_vram;
+		u64 tot_vis = adev->gmc.visible_vram_size;
+		u64 used_vis = amdgpu_vram_mgr_vis_usage(&adev->mman.vram_mgr);
 
-			adev->mm_stats.accum_us_vis = min(adev->mm_stats.accum_us_vis +
-							  increment_us, us_upper_bound);
+		if (used_vis < tot_vis) {
+			u64 free_vis = tot_vis - used_vis;
 
-			if (free_vis_vram >= total_vis_vram / 2)
-				adev->mm_stats.accum_us_vis =
-					max(bytes_to_us(adev, free_vis_vram / 2),
-					    adev->mm_stats.accum_us_vis);
+			adev->mm_stats.accum_us_vis =
+			min(adev->mm_stats.accum_us_vis + delta_us,
+				us_upper_bound);
+
+			if (free_vis >= tot_vis / 2) {
+				s64 min_vis = bytes_to_us(adev, free_vis / 2);
+				if (min_vis > adev->mm_stats.accum_us_vis)
+					adev->mm_stats.accum_us_vis = min_vis;
+			}
 		}
-
-		*max_vis_bytes = us_to_bytes(adev, adev->mm_stats.accum_us_vis);
-	} else {
-		*max_vis_bytes = 0;
 	}
 
+	/* take copies for post-lock conversion */
+	accum_us      = adev->mm_stats.accum_us;
+	accum_us_vis  = adev->mm_stats.accum_us_vis;
+
 	spin_unlock(&adev->mm_stats.lock);
+
+	*max_bytes     = us_to_bytes(adev, accum_us);
+	*max_vis_bytes = amdgpu_gmc_vram_full_visible(&adev->gmc) ?
+	0 : us_to_bytes(adev, accum_us_vis);
 }
 
 /* Report how many bytes have really been moved for the last command
@@ -1401,13 +1404,22 @@ static void amdgpu_cs_parser_fini(struct
 	if (parser->bo_list)
 		amdgpu_bo_list_put(parser->bo_list);
 
-	for (i = 0; i < parser->nchunks; i++)
-		kvfree(parser->chunks[i].kdata);
+	/* Release command data */
+	if (parser->kdata_pool) {
+		kvfree(parser->kdata_pool);
+	} else {
+		/* legacy path – per-chunk allocations */
+		for (i = 0; i < parser->nchunks; i++)
+			kvfree(parser->chunks[i].kdata);
+	}
+
 	kvfree(parser->chunks);
+
 	for (i = 0; i < parser->gang_size; ++i) {
 		if (parser->jobs[i])
 			amdgpu_job_free(parser->jobs[i]);
 	}
+
 	amdgpu_bo_unref(&parser->uf_bo);
 }



--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.h	2025-05-29 11:14:09.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.h	2025-06-02 02:40:45.281658476 +0200
@@ -30,6 +30,7 @@
 #include "amdgpu_bo_list.h"
 #include "amdgpu_ring.h"
 
+#define AMDGPU_CS_MAX_CHUNKS  64
 #define AMDGPU_CS_GANG_SIZE	4
 
 struct amdgpu_bo_va_mapping;
@@ -51,18 +52,21 @@ struct amdgpu_cs_parser {
 	struct drm_file		*filp;
 	struct amdgpu_ctx	*ctx;
 
-	/* chunks */
 	unsigned		nchunks;
 	struct amdgpu_cs_chunk	*chunks;
 
-	/* scheduler job objects */
+	/* Single allocation that backs all chunks[i].kdata if the
+	 * contiguous-buffer fast-path is taken.  NULL when the legacy
+	 * per-chunk allocation path is used.
+	 */
+	void			*kdata_pool;
+
 	unsigned int		gang_size;
 	unsigned int		gang_leader_idx;
 	struct drm_sched_entity	*entities[AMDGPU_CS_GANG_SIZE];
 	struct amdgpu_job	*jobs[AMDGPU_CS_GANG_SIZE];
 	struct amdgpu_job	*gang_leader;
 
-	/* buffer objects */
 	struct drm_exec			exec;
 	struct amdgpu_bo_list		*bo_list;
 	struct amdgpu_mn		*mn;
@@ -72,7 +76,6 @@ struct amdgpu_cs_parser {
 	uint64_t			bytes_moved;
 	uint64_t			bytes_moved_vis;
 
-	/* user fence */
 	struct amdgpu_bo		*uf_bo;
 
 	unsigned			num_post_deps;

--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_irq.h	2025-04-25 10:51:21.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_irq.h	2025-04-26 19:30:33.996606337 +0200
@@ -25,6 +25,7 @@
 #define __AMDGPU_IRQ_H__
 
 #include <linux/irqdomain.h>
+#include <linux/irq_work.h>
 #include "soc15_ih_clientid.h"
 #include "amdgpu_ih.h"
 
@@ -82,24 +83,38 @@ struct amdgpu_irq {
 	bool				installed;
 	unsigned int			irq;
 	spinlock_t			lock;
+
 	/* interrupt sources */
 	struct amdgpu_irq_client	client[AMDGPU_IRQ_CLIENTID_MAX];
 
 	/* status, etc. */
-	bool				msi_enabled; /* msi enabled */
+	bool				msi_enabled;		/* MSI enabled */
 
 	/* interrupt rings */
 	struct amdgpu_ih_ring		ih, ih1, ih2, ih_soft;
 	const struct amdgpu_ih_funcs    *ih_funcs;
-	struct work_struct		ih1_work, ih2_work, ih_soft_work;
+
+	/* legacy workqueue bottom-halves (kept for structure stability) */
+	struct work_struct		ih1_work;
+	struct work_struct		ih2_work;
+	struct work_struct		ih_soft_work;
+
+	/* new fast bottom-halves executed via irq_work */
+	struct irq_work			ih1_iw;
+	struct irq_work			ih2_iw;
+	struct irq_work			ih_soft_iw;
+
+	/* self-IRQ source */
 	struct amdgpu_irq_src		self_irq;
 
-	/* gen irq stuff */
-	struct irq_domain		*domain; /* GPU irq controller domain */
+	/* generic IRQ infrastructure */
+	struct irq_domain		*domain;		/* GPU IRQ domain */
 	unsigned			virq[AMDGPU_MAX_IRQ_SRC_ID];
-	uint32_t                        srbm_soft_reset;
-	u32                             retry_cam_doorbell_index;
-	bool                            retry_cam_enabled;
+
+	/* misc */
+	uint32_t			srbm_soft_reset;
+	u32				retry_cam_doorbell_index;
+	bool				retry_cam_enabled;
 };
 
 enum interrupt_node_id_per_aid {


--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_irq.c	2025-04-25 10:51:21.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_irq.c	2025-04-26 19:35:40.185257128 +0200
@@ -43,6 +43,7 @@
  */
 
 #include <linux/irq.h>
+#include <linux/irq_work.h>
 #include <linux/pci.h>
 
 #include <drm/drm_vblank.h>
@@ -114,6 +115,16 @@ const int node_id_to_phys_map[NODEID_MAX
 	[XCD7_NODEID] = 7,
 };
 
+/* Fast bottom-half executed in soft-IRQ context */
+static void amdgpu_irq_handle_ih_soft_iw(struct irq_work *iw)
+{
+	struct amdgpu_device *adev =
+	container_of(iw, struct amdgpu_device, irq.ih_soft_iw);
+
+	/* same payload as the former workqueue handler */
+	amdgpu_ih_process(adev, &adev->irq.ih_soft);
+}
+
 /**
  * amdgpu_irq_disable_all - disable *all* interrupts
  *
@@ -123,31 +134,36 @@ const int node_id_to_phys_map[NODEID_MAX
  */
 void amdgpu_irq_disable_all(struct amdgpu_device *adev)
 {
-	unsigned long irqflags;
+	unsigned long flags;
 	unsigned int i, j, k;
-	int r;
 
-	spin_lock_irqsave(&adev->irq.lock, irqflags);
-	for (i = 0; i < AMDGPU_IRQ_CLIENTID_MAX; ++i) {
-		if (!adev->irq.client[i].sources)
+	spin_lock_irqsave(&adev->irq.lock, flags);
+
+	for (i = 0; i < AMDGPU_IRQ_CLIENTID_MAX; i++) {
+		struct amdgpu_irq_client *cl = &adev->irq.client[i];
+
+		if (!cl->sources)
 			continue;
 
-		for (j = 0; j < AMDGPU_MAX_IRQ_SRC_ID; ++j) {
-			struct amdgpu_irq_src *src = adev->irq.client[i].sources[j];
+		for (j = 0; j < AMDGPU_MAX_IRQ_SRC_ID; j++) {
+			struct amdgpu_irq_src *src = cl->sources[j];
 
 			if (!src || !src->funcs->set || !src->num_types)
 				continue;
 
-			for (k = 0; k < src->num_types; ++k) {
-				r = src->funcs->set(adev, src, k,
-						    AMDGPU_IRQ_STATE_DISABLE);
-				if (r)
-					DRM_ERROR("error disabling interrupt (%d)\n",
-						  r);
+			for (k = 0; k < src->num_types; k++) {
+				if (!atomic_read(&src->enabled_types[k]))
+					continue;
+
+				if (src->funcs->set(adev, src, k,
+					AMDGPU_IRQ_STATE_DISABLE))
+					DRM_ERROR("error disabling IRQ %u/%u\n",
+							  i, j);
 			}
 		}
 	}
-	spin_unlock_irqrestore(&adev->irq.lock, irqflags);
+
+	spin_unlock_irqrestore(&adev->irq.lock, flags);
 }
 
 /**
@@ -163,14 +179,17 @@ void amdgpu_irq_disable_all(struct amdgp
  */
 static irqreturn_t amdgpu_irq_handler(int irq, void *arg)
 {
-	struct drm_device *dev = (struct drm_device *) arg;
-	struct amdgpu_device *adev = drm_to_adev(dev);
+	struct amdgpu_device *adev = drm_to_adev(arg);
 	irqreturn_t ret;
 
 	ret = amdgpu_ih_process(adev, &adev->irq.ih);
-	if (ret == IRQ_HANDLED)
-		pm_runtime_mark_last_busy(dev->dev);
 
+	if (likely(ret == IRQ_HANDLED))
+		pm_runtime_mark_last_busy(adev->dev);
+	else if (unlikely(ret != IRQ_NONE))
+		DRM_ERROR("amdgpu: ih_process returned %d\n", ret);
+
+	/* Cheaper to call unconditionally than branch on ras_enabled      */
 	amdgpu_ras_interrupt_fatal_error_handler(adev);
 
 	return ret;
@@ -207,21 +226,6 @@ static void amdgpu_irq_handle_ih2(struct
 }
 
 /**
- * amdgpu_irq_handle_ih_soft - kick of processing for ih_soft
- *
- * @work: work structure in struct amdgpu_irq
- *
- * Kick of processing IH soft ring.
- */
-static void amdgpu_irq_handle_ih_soft(struct work_struct *work)
-{
-	struct amdgpu_device *adev = container_of(work, struct amdgpu_device,
-						  irq.ih_soft_work);
-
-	amdgpu_ih_process(adev, &adev->irq.ih_soft);
-}
-
-/**
  * amdgpu_msi_ok - check whether MSI functionality is enabled
  *
  * @adev: amdgpu device pointer (unused)
@@ -273,55 +277,64 @@ int amdgpu_irq_init(struct amdgpu_device
 	unsigned int irq, flags;
 	int r;
 
+	/* ---------------- generic setup ---------------- */
 	spin_lock_init(&adev->irq.lock);
 
-	/* Enable MSI if not disabled by module parameter */
 	adev->irq.msi_enabled = false;
+	flags = amdgpu_msi_ok(adev) ? PCI_IRQ_ALL_TYPES : PCI_IRQ_INTX;
 
-	if (!amdgpu_msi_ok(adev))
-		flags = PCI_IRQ_INTX;
-	else
-		flags = PCI_IRQ_ALL_TYPES;
-
-	/* we only need one vector */
 	r = pci_alloc_irq_vectors(adev->pdev, 1, 1, flags);
 	if (r < 0) {
-		dev_err(adev->dev, "Failed to alloc msi vectors\n");
+		dev_err(adev->dev, "failed to allocate IRQ vector\n");
 		return r;
 	}
 
 	if (amdgpu_msi_ok(adev)) {
 		adev->irq.msi_enabled = true;
-		dev_dbg(adev->dev, "using MSI/MSI-X.\n");
+		dev_dbg(adev->dev, "using MSI/MSI-X\n");
 	}
 
+	/* IH1 / IH2 still use workqueues */
 	INIT_WORK(&adev->irq.ih1_work, amdgpu_irq_handle_ih1);
 	INIT_WORK(&adev->irq.ih2_work, amdgpu_irq_handle_ih2);
-	INIT_WORK(&adev->irq.ih_soft_work, amdgpu_irq_handle_ih_soft);
 
-	/* Use vector 0 for MSI-X. */
-	r = pci_irq_vector(adev->pdev, 0);
+	/* fast bottom-half for the software IH ring (irq_work) */
+	init_irq_work(&adev->irq.ih_soft_iw, amdgpu_irq_handle_ih_soft_iw);
+
+	/* ---------------- vector & handler ---------------- */
+	r = pci_irq_vector(adev->pdev, 0);	/* use vector 0 */
 	if (r < 0)
 		goto free_vectors;
 	irq = r;
 
-	/* PCI devices require shared interrupts. */
-	r = request_irq(irq, amdgpu_irq_handler, IRQF_SHARED, adev_to_drm(adev)->driver->name,
-			adev_to_drm(adev));
+	r = request_irq(irq, amdgpu_irq_handler, IRQF_SHARED,
+					adev_to_drm(adev)->driver->name,
+					adev_to_drm(adev));
 	if (r)
 		goto free_vectors;
 
-	adev->irq.installed = true;
-	adev->irq.irq = irq;
+	/* ---------------- locality hint ------------------ */
+	#ifdef CONFIG_GENERIC_IRQ_MIGRATION
+	{
+		int node = dev_to_node(&adev->pdev->dev);
+		const struct cpumask *mask = (node >= 0) ?
+		cpumask_of_node(node) :
+		cpu_online_mask;
+
+		irq_set_affinity_hint(irq, mask);
+	}
+	#endif
+
+	adev->irq.installed            = true;
+	adev->irq.irq                  = irq;
 	adev_to_drm(adev)->max_vblank_count = 0x00ffffff;
 
-	DRM_DEBUG("amdgpu: irq initialized.\n");
+	DRM_DEBUG("amdgpu: IRQ initialised\n");
 	return 0;
 
-free_vectors:
+	free_vectors:
 	if (adev->irq.msi_enabled)
 		pci_free_irq_vectors(adev->pdev);
-
 	adev->irq.msi_enabled = false;
 	return r;
 }
@@ -330,7 +343,13 @@ void amdgpu_irq_fini_hw(struct amdgpu_de
 {
 	if (adev->irq.installed) {
 		free_irq(adev->irq.irq, adev_to_drm(adev));
+
+		#ifdef CONFIG_GENERIC_IRQ_MIGRATION
+		irq_set_affinity_hint(adev->irq.irq, NULL);
+		#endif
+
 		adev->irq.installed = false;
+
 		if (adev->irq.msi_enabled)
 			pci_free_irq_vectors(adev->pdev);
 	}
@@ -434,63 +453,65 @@ int amdgpu_irq_add_id(struct amdgpu_devi
  * Dispatches IRQ to IP blocks.
  */
 void amdgpu_irq_dispatch(struct amdgpu_device *adev,
-			 struct amdgpu_ih_ring *ih)
+						 struct amdgpu_ih_ring *ih)
 {
-	u32 ring_index = ih->rptr >> 2;
-	struct amdgpu_iv_entry entry;
-	unsigned int client_id, src_id;
-	struct amdgpu_irq_src *src;
-	bool handled = false;
-	int r;
+	u32 ring_idx = ih->rptr >> 2;
 
-	entry.ih = ih;
-	entry.iv_entry = (const uint32_t *)&ih->ring[ring_index];
-
-	/*
-	 * timestamp is not supported on some legacy SOCs (cik, cz, iceland,
-	 * si and tonga), so initialize timestamp and timestamp_src to 0
-	 */
-	entry.timestamp = 0;
-	entry.timestamp_src = 0;
+	/* keep volatile to satisfy the type system */
+	const volatile u32 *iv_raw = &ih->ring[ring_idx];
+	const        u32  *iv_ptr = (const u32 *)iv_raw;
+
+	/* prefetch expects ‘const void *’; cast away volatile explicitly */
+	prefetch((const void *)iv_raw);
+
+	struct amdgpu_iv_entry entry = {
+		.ih            = ih,
+		.iv_entry      = iv_ptr,
+		.timestamp     = 0,
+		.timestamp_src = 0,
+	};
+	struct amdgpu_irq_src       *src;
+	bool                          handled = false;
+	unsigned int                  cid, sid;
+	int                           r;
 
 	amdgpu_ih_decode_iv(adev, &entry);
-
 	trace_amdgpu_iv(ih - &adev->irq.ih, &entry);
 
-	client_id = entry.client_id;
-	src_id = entry.src_id;
+	cid = entry.client_id;
+	sid = entry.src_id;
 
-	if (client_id >= AMDGPU_IRQ_CLIENTID_MAX) {
-		DRM_DEBUG("Invalid client_id in IV: %d\n", client_id);
-
-	} else	if (src_id >= AMDGPU_MAX_IRQ_SRC_ID) {
-		DRM_DEBUG("Invalid src_id in IV: %d\n", src_id);
+	/* -------- fast sanity checks --------------------------------- */
+	if (cid >= AMDGPU_IRQ_CLIENTID_MAX || sid >= AMDGPU_MAX_IRQ_SRC_ID)
+		goto unhandled;
+
+	if ((cid == AMDGPU_IRQ_CLIENTID_LEGACY ||
+		cid == SOC15_IH_CLIENTID_ISP) &&
+		unlikely(adev->irq.virq[sid])) {
+		generic_handle_domain_irq(adev->irq.domain, sid);
+	handled = true;
+	goto record_ts;
+		}
 
-	} else if (((client_id == AMDGPU_IRQ_CLIENTID_LEGACY) ||
-		    (client_id == SOC15_IH_CLIENTID_ISP)) &&
-		   adev->irq.virq[src_id]) {
-		generic_handle_domain_irq(adev->irq.domain, src_id);
-
-	} else if (!adev->irq.client[client_id].sources) {
-		DRM_DEBUG("Unregistered interrupt client_id: %d src_id: %d\n",
-			  client_id, src_id);
+		struct amdgpu_irq_client *client = &adev->irq.client[cid];
+		if (likely(client->sources &&
+			(src = client->sources[sid]))) {
 
-	} else if ((src = adev->irq.client[client_id].sources[src_id])) {
-		r = src->funcs->process(adev, src, &entry);
+			r = src->funcs->process(adev, src, &entry);
 		if (r < 0)
-			DRM_ERROR("error processing interrupt (%d)\n", r);
-		else if (r)
-			handled = true;
-
-	} else {
-		DRM_DEBUG("Unregistered interrupt src_id: %d of client_id:%d\n",
-			src_id, client_id);
-	}
+			DRM_ERROR("amdgpu: error %d processing IRQ %u/%u\n",
+					  r, cid, sid);
+			else if (r)
+				handled = true;
+			} else {
+				DRM_DEBUG("Unregistered IRQ cid:%u sid:%u\n", cid, sid);
+			}
 
-	/* Send it to amdkfd as well if it isn't already handled */
-	if (!handled)
-		amdgpu_amdkfd_interrupt(adev, entry.iv_entry);
+			unhandled:
+			if (!handled)
+				amdgpu_amdkfd_interrupt(adev, iv_ptr);
 
+	record_ts:
 	if (amdgpu_ih_ts_after(ih->processed_timestamp, entry.timestamp))
 		ih->processed_timestamp = entry.timestamp;
 }
@@ -506,11 +527,15 @@ void amdgpu_irq_dispatch(struct amdgpu_d
  * if the hardware delegation to IH1 or IH2 doesn't work for some reason.
  */
 void amdgpu_irq_delegate(struct amdgpu_device *adev,
-			 struct amdgpu_iv_entry *entry,
-			 unsigned int num_dw)
+						 struct amdgpu_iv_entry *entry,
+						 unsigned int num_dw)
 {
-	amdgpu_ih_ring_write(adev, &adev->irq.ih_soft, entry->iv_entry, num_dw);
-	schedule_work(&adev->irq.ih_soft_work);
+	/* copy IV into the software ring */
+	amdgpu_ih_ring_write(adev, &adev->irq.ih_soft,
+						 entry->iv_entry, num_dw);
+
+	/* queue bottom-half that lives inside ih_soft                      */
+	irq_work_queue(&adev->irq.ih_soft_iw);
 }
 
 /**
@@ -574,95 +599,60 @@ void amdgpu_irq_gpu_reset_resume_helper(
 	}
 }
 
-/**
- * amdgpu_irq_get - enable interrupt
- *
- * @adev: amdgpu device pointer
- * @src: interrupt source pointer
- * @type: type of interrupt
- *
- * Enables specified type of interrupt on the specified source (all ASICs).
- *
- * Returns:
- * 0 on success or error code otherwise
- */
-int amdgpu_irq_get(struct amdgpu_device *adev, struct amdgpu_irq_src *src,
-		   unsigned int type)
+static inline bool irq_ref_inc(struct amdgpu_irq_src *src, unsigned int t)
 {
-	if (!adev->irq.installed)
-		return -ENOENT;
+	/* full barrier via atomic op; returns true if counter became 1   */
+	return atomic_add_return(1, &src->enabled_types[t]) == 1;
+}
 
-	if (type >= src->num_types)
-		return -EINVAL;
+static inline bool irq_ref_dec(struct amdgpu_irq_src *src, unsigned int t)
+{
+	/* full barrier; true if it just reached 0                         */
+	return atomic_sub_and_test(1, &src->enabled_types[t]);
+}
 
-	if (!src->enabled_types || !src->funcs->set)
+int amdgpu_irq_get(struct amdgpu_device *adev,
+				   struct amdgpu_irq_src *src, unsigned int type)
+{
+	if (unlikely(!adev->irq.installed))
+		return -ENOENT;
+	if (type >= src->num_types || !src->enabled_types || !src->funcs->set)
 		return -EINVAL;
 
-	if (atomic_inc_return(&src->enabled_types[type]) == 1)
-		return amdgpu_irq_update(adev, src, type);
+	/* Fast path: already enabled → nothing to do                      */
+	if (!irq_ref_inc(src, type))
+		return 0;
 
-	return 0;
+	/* First user – program hardware (rare)                            */
+	return unlikely(amdgpu_irq_update(adev, src, type));
 }
 
-/**
- * amdgpu_irq_put - disable interrupt
- *
- * @adev: amdgpu device pointer
- * @src: interrupt source pointer
- * @type: type of interrupt
- *
- * Enables specified type of interrupt on the specified source (all ASICs).
- *
- * Returns:
- * 0 on success or error code otherwise
- */
-int amdgpu_irq_put(struct amdgpu_device *adev, struct amdgpu_irq_src *src,
-		   unsigned int type)
+int amdgpu_irq_put(struct amdgpu_device *adev,
+				   struct amdgpu_irq_src *src, unsigned int type)
 {
-	if (!adev->irq.installed)
+	if (unlikely(!adev->irq.installed))
 		return -ENOENT;
-
-	if (type >= src->num_types)
+	if (type >= src->num_types || !src->enabled_types || !src->funcs->set)
 		return -EINVAL;
-
-	if (!src->enabled_types || !src->funcs->set)
-		return -EINVAL;
-
 	if (WARN_ON(!amdgpu_irq_enabled(adev, src, type)))
 		return -EINVAL;
 
-	if (atomic_dec_and_test(&src->enabled_types[type]))
-		return amdgpu_irq_update(adev, src, type);
+	/* Fast path: more users remain                                     */
+	if (!irq_ref_dec(src, type))
+		return 0;
 
-	return 0;
+	/* Counter hit zero – disable in hardware (rare)                    */
+	return unlikely(amdgpu_irq_update(adev, src, type));
 }
 
-/**
- * amdgpu_irq_enabled - check whether interrupt is enabled or not
- *
- * @adev: amdgpu device pointer
- * @src: interrupt source pointer
- * @type: type of interrupt
- *
- * Checks whether the given type of interrupt is enabled on the given source.
- *
- * Returns:
- * *true* if interrupt is enabled, *false* if interrupt is disabled or on
- * invalid parameters
- */
-bool amdgpu_irq_enabled(struct amdgpu_device *adev, struct amdgpu_irq_src *src,
-			unsigned int type)
+bool amdgpu_irq_enabled(struct amdgpu_device *adev,
+						struct amdgpu_irq_src *src, unsigned int type)
 {
-	if (!adev->irq.installed)
-		return false;
-
-	if (type >= src->num_types)
-		return false;
-
-	if (!src->enabled_types || !src->funcs->set)
+	if (!adev->irq.installed || type >= src->num_types || !src->enabled_types)
 		return false;
 
-	return !!atomic_read(&src->enabled_types[type]);
+	/* atomic_read() is already a single-copy atomic load on x86/arm64 */
+	return atomic_read(&src->enabled_types[type]) != 0;
 }
 
 /* XXX: Generic IRQ handling */


--- a/drivers/gpu/drm/amd/amdgpu/sdma_v4_0.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/sdma_v4_0.c	2025-04-19 23:06:54.825287367 +0200
@@ -643,90 +643,65 @@ static int sdma_v4_0_init_microcode(stru
 	return ret;
 }
 
-/**
- * sdma_v4_0_ring_get_rptr - get the current read pointer
- *
- * @ring: amdgpu ring pointer
- *
- * Get the current rptr from the hardware (VEGA10+).
- */
+/* ------------------------------------------------------------------ */
+/* read pointer helper                                                */
+/* ------------------------------------------------------------------ */
 static uint64_t sdma_v4_0_ring_get_rptr(struct amdgpu_ring *ring)
 {
-	u64 *rptr;
+	u64 *rptr = (u64 *)ring->rptr_cpu_addr;	/* little‑endian on Vega */
 
-	/* XXX check if swapping is necessary on BE */
-	rptr = ((u64 *)ring->rptr_cpu_addr);
+	if (drm_debug_enabled(DRM_UT_DRIVER))
+		DRM_DEBUG("SDMA%u rptr raw 0x%016llx\n", ring->me, *rptr);
 
-	DRM_DEBUG("rptr before shift == 0x%016llx\n", *rptr);
-	return ((*rptr) >> 2);
+	return *rptr >> 2;			/* convert to DWORD index */
 }
 
-/**
- * sdma_v4_0_ring_get_wptr - get the current write pointer
- *
- * @ring: amdgpu ring pointer
- *
- * Get the current wptr from the hardware (VEGA10+).
- */
+/* ------------------------------------------------------------------ */
+/* write pointer read helper                                          */
+/* ------------------------------------------------------------------ */
 static uint64_t sdma_v4_0_ring_get_wptr(struct amdgpu_ring *ring)
 {
 	struct amdgpu_device *adev = ring->adev;
 	u64 wptr;
 
 	if (ring->use_doorbell) {
-		/* XXX check if swapping is necessary on BE */
 		wptr = READ_ONCE(*((u64 *)ring->wptr_cpu_addr));
-		DRM_DEBUG("wptr/doorbell before shift == 0x%016llx\n", wptr);
+		if (drm_debug_enabled(DRM_UT_DRIVER))
+			DRM_DEBUG("SDMA%u wptr doorbell raw 0x%016llx\n",
+					  ring->me, wptr);
 	} else {
-		wptr = RREG32_SDMA(ring->me, mmSDMA0_GFX_RB_WPTR_HI);
-		wptr = wptr << 32;
-		wptr |= RREG32_SDMA(ring->me, mmSDMA0_GFX_RB_WPTR);
-		DRM_DEBUG("wptr before shift [%i] wptr == 0x%016llx\n",
-				ring->me, wptr);
+		wptr  = (u64)RREG32_SDMA(ring->me,
+								 mmSDMA0_GFX_RB_WPTR_HI) << 32;
+								 wptr |= RREG32_SDMA(ring->me, mmSDMA0_GFX_RB_WPTR);
+								 if (drm_debug_enabled(DRM_UT_DRIVER))
+									 DRM_DEBUG("SDMA%u wptr mmio raw 0x%016llx\n",
+											   ring->me, wptr);
 	}
 
-	return wptr >> 2;
+	return wptr >> 2;			/* to DWORDs */
 }
 
-/**
- * sdma_v4_0_ring_set_wptr - commit the write pointer
- *
- * @ring: amdgpu ring pointer
- *
- * Write the wptr back to the hardware (VEGA10+).
- */
+/* ------------------------------------------------------------------ */
+/* write pointer commit helper                                        */
+/* ------------------------------------------------------------------ */
 static void sdma_v4_0_ring_set_wptr(struct amdgpu_ring *ring)
 {
 	struct amdgpu_device *adev = ring->adev;
+	u64 wptr_dw = ring->wptr;		/* already DWORD aligned */
+
+	if (drm_debug_enabled(DRM_UT_DRIVER))
+		DRM_DEBUG("SDMA%u set wptr %llu (DW)\n", ring->me, wptr_dw);
 
-	DRM_DEBUG("Setting write pointer\n");
 	if (ring->use_doorbell) {
 		u64 *wb = (u64 *)ring->wptr_cpu_addr;
 
-		DRM_DEBUG("Using doorbell -- "
-				"wptr_offs == 0x%08x "
-				"lower_32_bits(ring->wptr << 2) == 0x%08x "
-				"upper_32_bits(ring->wptr << 2) == 0x%08x\n",
-				ring->wptr_offs,
-				lower_32_bits(ring->wptr << 2),
-				upper_32_bits(ring->wptr << 2));
-		/* XXX check if swapping is necessary on BE */
-		WRITE_ONCE(*wb, (ring->wptr << 2));
-		DRM_DEBUG("calling WDOORBELL64(0x%08x, 0x%016llx)\n",
-				ring->doorbell_index, ring->wptr << 2);
-		WDOORBELL64(ring->doorbell_index, ring->wptr << 2);
+		WRITE_ONCE(*wb, wptr_dw << 2);	/* bytes */
+		WDOORBELL64(ring->doorbell_index, wptr_dw << 2);
 	} else {
-		DRM_DEBUG("Not using doorbell -- "
-				"mmSDMA%i_GFX_RB_WPTR == 0x%08x "
-				"mmSDMA%i_GFX_RB_WPTR_HI == 0x%08x\n",
-				ring->me,
-				lower_32_bits(ring->wptr << 2),
-				ring->me,
-				upper_32_bits(ring->wptr << 2));
 		WREG32_SDMA(ring->me, mmSDMA0_GFX_RB_WPTR,
-			    lower_32_bits(ring->wptr << 2));
+					lower_32_bits(wptr_dw << 2));
 		WREG32_SDMA(ring->me, mmSDMA0_GFX_RB_WPTR_HI,
-			    upper_32_bits(ring->wptr << 2));
+					upper_32_bits(wptr_dw << 2));
 	}
 }
 
@@ -781,17 +756,20 @@ static void sdma_v4_0_page_ring_set_wptr
 	}
 }
 
-static void sdma_v4_0_ring_insert_nop(struct amdgpu_ring *ring, uint32_t count)
+static void sdma_v4_0_ring_insert_nop(struct amdgpu_ring *ring,
+									  uint32_t count)
 {
-	struct amdgpu_sdma_instance *sdma = amdgpu_sdma_get_instance_from_ring(ring);
-	int i;
+	struct amdgpu_sdma_instance *sdma =
+	amdgpu_sdma_get_instance_from_ring(ring);
+	uint32_t i;
 
-	for (i = 0; i < count; i++)
-		if (sdma && sdma->burst_nop && (i == 0))
+	for (i = 0; i < count; i++) {
+		if (sdma && sdma->burst_nop && i == 0)
 			amdgpu_ring_write(ring, ring->funcs->nop |
-				SDMA_PKT_NOP_HEADER_COUNT(count - 1));
+			SDMA_PKT_NOP_HEADER_COUNT(count - 1));
 		else
 			amdgpu_ring_write(ring, ring->funcs->nop);
+	}
 }
 
 /**
@@ -1659,30 +1637,26 @@ static void sdma_v4_0_vm_set_pte_pde(str
 	ib->ptr[ib->length_dw++] = count - 1; /* number of entries */
 }
 
-/**
- * sdma_v4_0_ring_pad_ib - pad the IB to the required number of dw
- *
- * @ring: amdgpu_ring structure holding ring information
- * @ib: indirect buffer to fill with padding
- */
-static void sdma_v4_0_ring_pad_ib(struct amdgpu_ring *ring, struct amdgpu_ib *ib)
+static void sdma_v4_0_ring_pad_ib(struct amdgpu_ring *ring,
+								  struct amdgpu_ib *ib)
 {
-	struct amdgpu_sdma_instance *sdma = amdgpu_sdma_get_instance_from_ring(ring);
-	u32 pad_count;
-	int i;
+	struct amdgpu_sdma_instance *sdma =
+	amdgpu_sdma_get_instance_from_ring(ring);
+	u32 pad_count, i;
+
+	pad_count = (-ib->length_dw) & 7;	/* align to 8 DW */
 
-	pad_count = (-ib->length_dw) & 7;
-	for (i = 0; i < pad_count; i++)
-		if (sdma && sdma->burst_nop && (i == 0))
+	for (i = 0; i < pad_count; i++) {
+		if (sdma && sdma->burst_nop && i == 0)
 			ib->ptr[ib->length_dw++] =
-				SDMA_PKT_HEADER_OP(SDMA_OP_NOP) |
-				SDMA_PKT_NOP_HEADER_COUNT(pad_count - 1);
+			SDMA_PKT_HEADER_OP(SDMA_OP_NOP) |
+			SDMA_PKT_NOP_HEADER_COUNT(pad_count - 1);
 		else
 			ib->ptr[ib->length_dw++] =
-				SDMA_PKT_HEADER_OP(SDMA_OP_NOP);
+			SDMA_PKT_HEADER_OP(SDMA_OP_NOP);
+	}
 }
 
-
 /**
  * sdma_v4_0_ring_emit_pipeline_sync - sync the pipeline
  *
@@ -2599,12 +2573,12 @@ static void sdma_v4_0_emit_fill_buffer(s
 }
 
 static const struct amdgpu_buffer_funcs sdma_v4_0_buffer_funcs = {
-	.copy_max_bytes = 0x400000,
-	.copy_num_dw = 7,
+	.copy_max_bytes = 0x400000,		/* 4 MiB */
+	.copy_num_dw    = 7,
 	.emit_copy_buffer = sdma_v4_0_emit_copy_buffer,
 
-	.fill_max_bytes = 0x400000,
-	.fill_num_dw = 5,
+	.fill_max_bytes = 0x400000,		/* 4 MiB */
+	.fill_num_dw    = 5,
 	.emit_fill_buffer = sdma_v4_0_emit_fill_buffer,
 };
 

 
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_sdma.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_sdma.c	2025-04-19 22:43:23.904999601 +0200
@@ -30,37 +30,54 @@
 /* SDMA CSA reside in the 3rd page of CSA */
 #define AMDGPU_CSA_SDMA_OFFSET (4096 * 2)
 
-/*
- * GPU SDMA IP block helpers function.
- */
-
-struct amdgpu_sdma_instance *amdgpu_sdma_get_instance_from_ring(struct amdgpu_ring *ring)
+/* ------------------------------------------------------------------ */
+/* Fast helpers: use ring->idx instead of a linear scan                */
+/* ------------------------------------------------------------------ */
+struct amdgpu_sdma_instance *
+amdgpu_sdma_get_instance_from_ring(struct amdgpu_ring *ring)
 {
 	struct amdgpu_device *adev = ring->adev;
-	int i;
+	u32 idx = ring->idx;
 
-	for (i = 0; i < adev->sdma.num_instances; i++)
-		if (ring == &adev->sdma.instance[i].ring ||
-		    ring == &adev->sdma.instance[i].page)
-			return &adev->sdma.instance[i];
+	/* O(1) fast path */
+	if (idx < adev->sdma.num_instances &&
+		(ring == &adev->sdma.instance[idx].ring ||
+		ring == &adev->sdma.instance[idx].page))
+		return &adev->sdma.instance[idx];
+
+	/* Fallback – keep legacy behaviour */
+	for (idx = 0; idx < adev->sdma.num_instances; idx++) {
+		if (ring == &adev->sdma.instance[idx].ring ||
+			ring == &adev->sdma.instance[idx].page)
+			return &adev->sdma.instance[idx];
+	}
 
 	return NULL;
 }
 
-int amdgpu_sdma_get_index_from_ring(struct amdgpu_ring *ring, uint32_t *index)
+int amdgpu_sdma_get_index_from_ring(struct amdgpu_ring *ring, u32 *index)
 {
 	struct amdgpu_device *adev = ring->adev;
-	int i;
+	u32 idx = ring->idx;
 
-	for (i = 0; i < adev->sdma.num_instances; i++) {
-		if (ring == &adev->sdma.instance[i].ring ||
-			ring == &adev->sdma.instance[i].page) {
-			*index = i;
+	/* Fast path */
+	if (idx < adev->sdma.num_instances &&
+		(ring == &adev->sdma.instance[idx].ring ||
+		ring == &adev->sdma.instance[idx].page)) {
+		*index = idx;
+	return 0;
+		}
+
+		/* Fallback keeps behaviour identical to the old code */
+		for (idx = 0; idx < adev->sdma.num_instances; idx++) {
+			if (ring == &adev->sdma.instance[idx].ring ||
+				ring == &adev->sdma.instance[idx].page) {
+				*index = idx;
 			return 0;
+				}
 		}
-	}
 
-	return -EINVAL;
+		return -EINVAL;
 }
 
 uint64_t amdgpu_sdma_get_csa_mc_addr(struct amdgpu_ring *ring,


--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gfx.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gfx.c	2025-04-18 16:58:52.885186023 +0200
@@ -139,25 +139,24 @@ void amdgpu_gfx_parse_disable_cu(unsigne
 	}
 }
 
-static bool amdgpu_gfx_is_graphics_multipipe_capable(struct amdgpu_device *adev)
+/* Hot predicates – replace the originals */
+static __always_inline bool
+amdgpu_gfx_is_graphics_multipipe_capable(struct amdgpu_device *adev)
 {
-	return amdgpu_async_gfx_ring && adev->gfx.me.num_pipe_per_me > 1;
+	return amdgpu_async_gfx_ring &&
+	adev->gfx.me.num_pipe_per_me > 1;
 }
 
-static bool amdgpu_gfx_is_compute_multipipe_capable(struct amdgpu_device *adev)
+static __always_inline bool
+amdgpu_gfx_is_compute_multipipe_capable(struct amdgpu_device *adev)
 {
-	if (amdgpu_compute_multipipe != -1) {
-		DRM_INFO("amdgpu: forcing compute pipe policy %d\n",
-			 amdgpu_compute_multipipe);
+	if (amdgpu_compute_multipipe != -1)
 		return amdgpu_compute_multipipe == 1;
-	}
 
 	if (amdgpu_ip_version(adev, GC_HWIP, 0) > IP_VERSION(9, 0, 0))
 		return true;
 
-	/* FIXME: spreading the queues across pipes causes perf regressions
-	 * on POLARIS11 compute workloads */
-	if (adev->asic_type == CHIP_POLARIS11)
+	if (unlikely(adev->asic_type == CHIP_POLARIS11))
 		return false;
 
 	return adev->gfx.mec.num_mec > 1;
@@ -1163,8 +1162,10 @@ int amdgpu_gfx_get_num_kcq(struct amdgpu
 {
 	if (amdgpu_num_kcq == -1) {
 		return 8;
-	} else if (amdgpu_num_kcq > 8 || amdgpu_num_kcq < 0) {
-		dev_warn(adev->dev, "set kernel compute queue number to 8 due to invalid parameter provided by user\n");
+	} if (amdgpu_num_kcq == -1 || amdgpu_num_kcq <= 0 || amdgpu_num_kcq > 8) {
+		dev_warn(adev->dev,
+				 "Invalid amdgpu_num_kcq=%d, clamping to 8\n",
+		   amdgpu_num_kcq);
 		return 8;
 	}
 	return amdgpu_num_kcq;


--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h	2025-04-12 17:27:40.094502930 +0200
@@ -35,7 +35,7 @@
 #include "amdgpu_sync.h"
 #include "amdgpu_ring.h"
 #include "amdgpu_ids.h"
-#include "amdgpu_ttm.h"
+#include "amdgpu_ttm.h" // Provides __AMDGPU_PL_NUM
 
 struct drm_exec;
 
@@ -88,45 +88,45 @@ struct amdgpu_bo_vm;
 
 /* Flag combination to set no-retry with TF disabled */
 #define AMDGPU_VM_NORETRY_FLAGS	(AMDGPU_PTE_EXECUTABLE | AMDGPU_PDE_PTE | \
-				AMDGPU_PTE_TF)
+AMDGPU_PTE_TF)
 
 /* Flag combination to set no-retry with TF enabled */
 #define AMDGPU_VM_NORETRY_FLAGS_TF (AMDGPU_PTE_VALID | AMDGPU_PTE_SYSTEM | \
-				   AMDGPU_PTE_PRT)
+AMDGPU_PTE_PRT)
 /* For GFX9 */
 #define AMDGPU_PTE_MTYPE_VG10_SHIFT(mtype)	((uint64_t)(mtype) << 57)
 #define AMDGPU_PTE_MTYPE_VG10_MASK	AMDGPU_PTE_MTYPE_VG10_SHIFT(3ULL)
 #define AMDGPU_PTE_MTYPE_VG10(flags, mtype)			\
-	(((uint64_t)(flags) & (~AMDGPU_PTE_MTYPE_VG10_MASK)) |	\
-	  AMDGPU_PTE_MTYPE_VG10_SHIFT(mtype))
+(((uint64_t)(flags) & (~AMDGPU_PTE_MTYPE_VG10_MASK)) |	\
+AMDGPU_PTE_MTYPE_VG10_SHIFT(mtype))
 
 #define AMDGPU_MTYPE_NC 0
 #define AMDGPU_MTYPE_CC 2
 
 #define AMDGPU_PTE_DEFAULT_ATC  (AMDGPU_PTE_SYSTEM      \
-                                | AMDGPU_PTE_SNOOPED    \
-                                | AMDGPU_PTE_EXECUTABLE \
-                                | AMDGPU_PTE_READABLE   \
-                                | AMDGPU_PTE_WRITEABLE  \
-                                | AMDGPU_PTE_MTYPE_VG10(AMDGPU_MTYPE_CC))
+| AMDGPU_PTE_SNOOPED    \
+| AMDGPU_PTE_EXECUTABLE \
+| AMDGPU_PTE_READABLE   \
+| AMDGPU_PTE_WRITEABLE  \
+| AMDGPU_PTE_MTYPE_VG10(AMDGPU_MTYPE_CC))
 
 /* gfx10 */
 #define AMDGPU_PTE_MTYPE_NV10_SHIFT(mtype)	((uint64_t)(mtype) << 48)
 #define AMDGPU_PTE_MTYPE_NV10_MASK     AMDGPU_PTE_MTYPE_NV10_SHIFT(7ULL)
 #define AMDGPU_PTE_MTYPE_NV10(flags, mtype)			\
-	(((uint64_t)(flags) & (~AMDGPU_PTE_MTYPE_NV10_MASK)) |	\
-	  AMDGPU_PTE_MTYPE_NV10_SHIFT(mtype))
+(((uint64_t)(flags) & (~AMDGPU_PTE_MTYPE_NV10_MASK)) |	\
+AMDGPU_PTE_MTYPE_NV10_SHIFT(mtype))
 
 /* gfx12 */
 #define AMDGPU_PTE_PRT_GFX12		(1ULL << 56)
 #define AMDGPU_PTE_PRT_FLAG(adev)	\
-	((amdgpu_ip_version((adev), GC_HWIP, 0) >= IP_VERSION(12, 0, 0)) ? AMDGPU_PTE_PRT_GFX12 : AMDGPU_PTE_PRT)
+((amdgpu_ip_version((adev), GC_HWIP, 0) >= IP_VERSION(12, 0, 0)) ? AMDGPU_PTE_PRT_GFX12 : AMDGPU_PTE_PRT)
 
 #define AMDGPU_PTE_MTYPE_GFX12_SHIFT(mtype)	((uint64_t)(mtype) << 54)
 #define AMDGPU_PTE_MTYPE_GFX12_MASK	AMDGPU_PTE_MTYPE_GFX12_SHIFT(3ULL)
 #define AMDGPU_PTE_MTYPE_GFX12(flags, mtype)				\
-	(((uint64_t)(flags) & (~AMDGPU_PTE_MTYPE_GFX12_MASK)) |	\
-	  AMDGPU_PTE_MTYPE_GFX12_SHIFT(mtype))
+(((uint64_t)(flags) & (~AMDGPU_PTE_MTYPE_GFX12_MASK)) |	\
+AMDGPU_PTE_MTYPE_GFX12_SHIFT(mtype))
 
 #define AMDGPU_PTE_DCC			(1ULL << 58)
 #define AMDGPU_PTE_IS_PTE		(1ULL << 63)
@@ -134,11 +134,11 @@ struct amdgpu_bo_vm;
 /* PDE Block Fragment Size for gfx v12 */
 #define AMDGPU_PDE_BFS_GFX12(a)		((uint64_t)((a) & 0x1fULL) << 58)
 #define AMDGPU_PDE_BFS_FLAG(adev, a)	\
-	((amdgpu_ip_version((adev), GC_HWIP, 0) >= IP_VERSION(12, 0, 0)) ? AMDGPU_PDE_BFS_GFX12(a) : AMDGPU_PDE_BFS(a))
+((amdgpu_ip_version((adev), GC_HWIP, 0) >= IP_VERSION(12, 0, 0)) ? AMDGPU_PDE_BFS_GFX12(a) : AMDGPU_PDE_BFS(a))
 /* PDE is handled as PTE for gfx v12 */
 #define AMDGPU_PDE_PTE_GFX12		(1ULL << 63)
 #define AMDGPU_PDE_PTE_FLAG(adev)	\
-	((amdgpu_ip_version((adev), GC_HWIP, 0) >= IP_VERSION(12, 0, 0)) ? AMDGPU_PDE_PTE_GFX12 : AMDGPU_PDE_PTE)
+((amdgpu_ip_version((adev), GC_HWIP, 0) >= IP_VERSION(12, 0, 0)) ? AMDGPU_PDE_PTE_GFX12 : AMDGPU_PDE_PTE)
 
 /* How to program VM fault handling */
 #define AMDGPU_VM_FAULT_STOP_NEVER	0
@@ -167,18 +167,18 @@ struct amdgpu_bo_vm;
 /* Reserve space at top/bottom of address space for kernel use */
 #define AMDGPU_VA_RESERVED_CSA_SIZE		(2ULL << 20)
 #define AMDGPU_VA_RESERVED_CSA_START(adev)	(((adev)->vm_manager.max_pfn \
-						  << AMDGPU_GPU_PAGE_SHIFT)  \
-						 - AMDGPU_VA_RESERVED_CSA_SIZE)
+<< AMDGPU_GPU_PAGE_SHIFT)  \
+- AMDGPU_VA_RESERVED_CSA_SIZE)
 #define AMDGPU_VA_RESERVED_SEQ64_SIZE		(2ULL << 20)
 #define AMDGPU_VA_RESERVED_SEQ64_START(adev)	(AMDGPU_VA_RESERVED_CSA_START(adev) \
-						 - AMDGPU_VA_RESERVED_SEQ64_SIZE)
+- AMDGPU_VA_RESERVED_SEQ64_SIZE)
 #define AMDGPU_VA_RESERVED_TRAP_SIZE		(2ULL << 12)
 #define AMDGPU_VA_RESERVED_TRAP_START(adev)	(AMDGPU_VA_RESERVED_SEQ64_START(adev) \
-						 - AMDGPU_VA_RESERVED_TRAP_SIZE)
+- AMDGPU_VA_RESERVED_TRAP_SIZE)
 #define AMDGPU_VA_RESERVED_BOTTOM		(1ULL << 16)
 #define AMDGPU_VA_RESERVED_TOP			(AMDGPU_VA_RESERVED_TRAP_SIZE + \
-						 AMDGPU_VA_RESERVED_SEQ64_SIZE + \
-						 AMDGPU_VA_RESERVED_CSA_SIZE)
+AMDGPU_VA_RESERVED_SEQ64_SIZE + \
+AMDGPU_VA_RESERVED_CSA_SIZE)
 
 /* See vm_update_mode */
 #define AMDGPU_VM_USE_CPU_FOR_GFX (1 << 0)
@@ -212,6 +212,12 @@ struct amdgpu_vm_bo_base {
 
 	/* protected by the BO being reserved */
 	bool				moved;
+
+	/* The memory type used for the last stats increment.
+	 * Protected by vm status_lock. Used to ensure decrement matches.
+	 * Initialized to __AMDGPU_PL_NUM (invalid).
+	 */
+	uint32_t			last_stat_memtype;
 };
 
 /* provided by hw blocks that can write ptes, e.g., sdma */
@@ -221,18 +227,18 @@ struct amdgpu_vm_pte_funcs {
 
 	/* copy pte entries from GART */
 	void (*copy_pte)(struct amdgpu_ib *ib,
-			 uint64_t pe, uint64_t src,
-			 unsigned count);
+					 uint64_t pe, uint64_t src,
+				  unsigned count);
 
 	/* write pte one entry at a time with addr mapping */
 	void (*write_pte)(struct amdgpu_ib *ib, uint64_t pe,
-			  uint64_t value, unsigned count,
-			  uint32_t incr);
+					  uint64_t value, unsigned count,
+				   uint32_t incr);
 	/* for linear pte/pde updates without addr mapping */
 	void (*set_pte_pde)(struct amdgpu_ib *ib,
-			    uint64_t pe,
-			    uint64_t addr, unsigned count,
-			    uint32_t incr, uint64_t flags);
+						uint64_t pe,
+					 uint64_t addr, unsigned count,
+					 uint32_t incr, uint64_t flags);
 };
 
 struct amdgpu_task_info {
@@ -309,12 +315,12 @@ struct amdgpu_vm_update_params {
 struct amdgpu_vm_update_funcs {
 	int (*map_table)(struct amdgpu_bo_vm *bo);
 	int (*prepare)(struct amdgpu_vm_update_params *p,
-		       struct amdgpu_sync *sync);
+				   struct amdgpu_sync *sync);
 	int (*update)(struct amdgpu_vm_update_params *p,
-		      struct amdgpu_bo_vm *bo, uint64_t pe, uint64_t addr,
-		      unsigned count, uint32_t incr, uint64_t flags);
+				  struct amdgpu_bo_vm *bo, uint64_t pe, uint64_t addr,
+			   unsigned count, uint32_t incr, uint64_t flags);
 	int (*commit)(struct amdgpu_vm_update_params *p,
-		      struct dma_fence **fence);
+				  struct dma_fence **fence);
 };
 
 struct amdgpu_vm_fault_info {
@@ -469,6 +475,17 @@ struct amdgpu_vm_manager {
 	struct xarray				pasids;
 	/* Global registration of recent page fault information */
 	struct amdgpu_vm_fault_info	fault_info;
+
+	/* Vega 10 optimization statistics */
+	struct {
+		atomic64_t tlb_flushes_skipped;
+		atomic64_t pt_evictions_prioritized;
+		atomic64_t small_bos_vram;
+		atomic64_t large_bos_gtt;
+		atomic64_t vm_batch_splits;
+		atomic64_t mtype_cc_small;
+		atomic64_t mtype_uc_streaming;
+	} vega10_stats;
 };
 
 struct amdgpu_bo_va_mapping;
@@ -484,83 +501,84 @@ void amdgpu_vm_manager_init(struct amdgp
 void amdgpu_vm_manager_fini(struct amdgpu_device *adev);
 
 int amdgpu_vm_set_pasid(struct amdgpu_device *adev, struct amdgpu_vm *vm,
-			u32 pasid);
+						u32 pasid);
 
 long amdgpu_vm_wait_idle(struct amdgpu_vm *vm, long timeout);
 int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm, int32_t xcp_id);
 int amdgpu_vm_make_compute(struct amdgpu_device *adev, struct amdgpu_vm *vm);
+void amdgpu_vm_release_compute(struct amdgpu_device *adev, struct amdgpu_vm *vm);
 void amdgpu_vm_fini(struct amdgpu_device *adev, struct amdgpu_vm *vm);
 int amdgpu_vm_lock_pd(struct amdgpu_vm *vm, struct drm_exec *exec,
-		      unsigned int num_fences);
+					  unsigned int num_fences);
 bool amdgpu_vm_ready(struct amdgpu_vm *vm);
 uint64_t amdgpu_vm_generation(struct amdgpu_device *adev, struct amdgpu_vm *vm);
 int amdgpu_vm_validate(struct amdgpu_device *adev, struct amdgpu_vm *vm,
-		       struct ww_acquire_ctx *ticket,
-		       int (*callback)(void *p, struct amdgpu_bo *bo),
-		       void *param);
+					   struct ww_acquire_ctx *ticket,
+					   int (*callback)(void *p, struct amdgpu_bo *bo),
+					   void *param);
 int amdgpu_vm_flush(struct amdgpu_ring *ring, struct amdgpu_job *job, bool need_pipe_sync);
 int amdgpu_vm_update_pdes(struct amdgpu_device *adev,
-			  struct amdgpu_vm *vm, bool immediate);
+						  struct amdgpu_vm *vm, bool immediate);
 int amdgpu_vm_clear_freed(struct amdgpu_device *adev,
-			  struct amdgpu_vm *vm,
-			  struct dma_fence **fence);
+						  struct amdgpu_vm *vm,
+						  struct dma_fence **fence);
 int amdgpu_vm_handle_moved(struct amdgpu_device *adev,
-			   struct amdgpu_vm *vm,
-			   struct ww_acquire_ctx *ticket);
+						   struct amdgpu_vm *vm,
+						   struct ww_acquire_ctx *ticket);
 int amdgpu_vm_flush_compute_tlb(struct amdgpu_device *adev,
-				struct amdgpu_vm *vm,
-				uint32_t flush_type,
-				uint32_t xcc_mask);
+								struct amdgpu_vm *vm,
+								uint32_t flush_type,
+								uint32_t xcc_mask);
 void amdgpu_vm_bo_base_init(struct amdgpu_vm_bo_base *base,
-			    struct amdgpu_vm *vm, struct amdgpu_bo *bo);
+							struct amdgpu_vm *vm, struct amdgpu_bo *bo);
 int amdgpu_vm_update_range(struct amdgpu_device *adev, struct amdgpu_vm *vm,
-			   bool immediate, bool unlocked, bool flush_tlb,
-			   bool allow_override, struct amdgpu_sync *sync,
-			   uint64_t start, uint64_t last, uint64_t flags,
-			   uint64_t offset, uint64_t vram_base,
-			   struct ttm_resource *res, dma_addr_t *pages_addr,
-			   struct dma_fence **fence);
+						   bool immediate, bool unlocked, bool flush_tlb,
+						   bool allow_override, struct amdgpu_sync *sync,
+						   uint64_t start, uint64_t last, uint64_t flags,
+						   uint64_t offset, uint64_t vram_base,
+						   struct ttm_resource *res, dma_addr_t *pages_addr,
+						   struct dma_fence **fence);
 int amdgpu_vm_bo_update(struct amdgpu_device *adev,
-			struct amdgpu_bo_va *bo_va,
-			bool clear);
+						struct amdgpu_bo_va *bo_va,
+						bool clear);
 bool amdgpu_vm_evictable(struct amdgpu_bo *bo);
 void amdgpu_vm_bo_invalidate(struct amdgpu_bo *bo, bool evicted);
 void amdgpu_vm_update_stats(struct amdgpu_vm_bo_base *base,
-			    struct ttm_resource *new_res, int sign);
+							struct ttm_resource *new_res, int sign);
 void amdgpu_vm_bo_update_shared(struct amdgpu_bo *bo);
 void amdgpu_vm_bo_move(struct amdgpu_bo *bo, struct ttm_resource *new_mem,
-		       bool evicted);
+					   bool evicted);
 uint64_t amdgpu_vm_map_gart(const dma_addr_t *pages_addr, uint64_t addr);
 struct amdgpu_bo_va *amdgpu_vm_bo_find(struct amdgpu_vm *vm,
-				       struct amdgpu_bo *bo);
+									   struct amdgpu_bo *bo);
 struct amdgpu_bo_va *amdgpu_vm_bo_add(struct amdgpu_device *adev,
-				      struct amdgpu_vm *vm,
-				      struct amdgpu_bo *bo);
+									  struct amdgpu_vm *vm,
+									  struct amdgpu_bo *bo);
 int amdgpu_vm_bo_map(struct amdgpu_device *adev,
-		     struct amdgpu_bo_va *bo_va,
-		     uint64_t addr, uint64_t offset,
-		     uint64_t size, uint64_t flags);
+					 struct amdgpu_bo_va *bo_va,
+					 uint64_t addr, uint64_t offset,
+					 uint64_t size, uint64_t flags);
 int amdgpu_vm_bo_replace_map(struct amdgpu_device *adev,
-			     struct amdgpu_bo_va *bo_va,
-			     uint64_t addr, uint64_t offset,
-			     uint64_t size, uint64_t flags);
+							 struct amdgpu_bo_va *bo_va,
+							 uint64_t addr, uint64_t offset,
+							 uint64_t size, uint64_t flags);
 int amdgpu_vm_bo_unmap(struct amdgpu_device *adev,
-		       struct amdgpu_bo_va *bo_va,
-		       uint64_t addr);
+					   struct amdgpu_bo_va *bo_va,
+					   uint64_t addr);
 int amdgpu_vm_bo_clear_mappings(struct amdgpu_device *adev,
-				struct amdgpu_vm *vm,
-				uint64_t saddr, uint64_t size);
+								struct amdgpu_vm *vm,
+								uint64_t saddr, uint64_t size);
 struct amdgpu_bo_va_mapping *amdgpu_vm_bo_lookup_mapping(struct amdgpu_vm *vm,
-							 uint64_t addr);
+														 uint64_t addr);
 void amdgpu_vm_bo_trace_cs(struct amdgpu_vm *vm, struct ww_acquire_ctx *ticket);
 void amdgpu_vm_bo_del(struct amdgpu_device *adev,
-		      struct amdgpu_bo_va *bo_va);
+					  struct amdgpu_bo_va *bo_va);
 void amdgpu_vm_adjust_size(struct amdgpu_device *adev, uint32_t min_vm_size,
-			   uint32_t fragment_size_default, unsigned max_level,
-			   unsigned max_bits);
+						   uint32_t fragment_size_default, unsigned max_level,
+						   unsigned max_bits);
 int amdgpu_vm_ioctl(struct drm_device *dev, void *data, struct drm_file *filp);
 bool amdgpu_vm_need_pipeline_sync(struct amdgpu_ring *ring,
-				  struct amdgpu_job *job);
+								  struct amdgpu_job *job);
 void amdgpu_vm_check_compute_bug(struct amdgpu_device *adev);
 
 struct amdgpu_task_info *
@@ -572,31 +590,31 @@ amdgpu_vm_get_task_info_vm(struct amdgpu
 void amdgpu_vm_put_task_info(struct amdgpu_task_info *task_info);
 
 bool amdgpu_vm_handle_fault(struct amdgpu_device *adev, u32 pasid,
-			    u32 vmid, u32 node_id, uint64_t addr, uint64_t ts,
-			    bool write_fault);
+							u32 vmid, u32 node_id, uint64_t addr, uint64_t ts,
+							bool write_fault);
 
 void amdgpu_vm_set_task_info(struct amdgpu_vm *vm);
 
 void amdgpu_vm_move_to_lru_tail(struct amdgpu_device *adev,
-				struct amdgpu_vm *vm);
+								struct amdgpu_vm *vm);
 void amdgpu_vm_get_memory(struct amdgpu_vm *vm,
-			  struct amdgpu_mem_stats stats[__AMDGPU_PL_NUM]);
+						  struct amdgpu_mem_stats stats[__AMDGPU_PL_NUM]);
 
 int amdgpu_vm_pt_clear(struct amdgpu_device *adev, struct amdgpu_vm *vm,
-		       struct amdgpu_bo_vm *vmbo, bool immediate);
+					   struct amdgpu_bo_vm *vmbo, bool immediate);
 int amdgpu_vm_pt_create(struct amdgpu_device *adev, struct amdgpu_vm *vm,
-			int level, bool immediate, struct amdgpu_bo_vm **vmbo,
-			int32_t xcp_id);
+						int level, bool immediate, struct amdgpu_bo_vm **vmbo,
+						int32_t xcp_id);
 void amdgpu_vm_pt_free_root(struct amdgpu_device *adev, struct amdgpu_vm *vm);
 
 int amdgpu_vm_pde_update(struct amdgpu_vm_update_params *params,
-			 struct amdgpu_vm_bo_base *entry);
+						 struct amdgpu_vm_bo_base *entry);
 int amdgpu_vm_ptes_update(struct amdgpu_vm_update_params *params,
-			  uint64_t start, uint64_t end,
-			  uint64_t dst, uint64_t flags);
+						  uint64_t start, uint64_t end,
+						  uint64_t dst, uint64_t flags);
 void amdgpu_vm_pt_free_work(struct work_struct *work);
 void amdgpu_vm_pt_free_list(struct amdgpu_device *adev,
-			    struct amdgpu_vm_update_params *params);
+							struct amdgpu_vm_update_params *params);
 
 #if defined(CONFIG_DEBUG_FS)
 void amdgpu_debugfs_vm_bo_info(struct amdgpu_vm *vm, struct seq_file *m);
@@ -660,12 +678,12 @@ static inline void amdgpu_vm_eviction_un
 }
 
 void amdgpu_vm_update_fault_cache(struct amdgpu_device *adev,
-				  unsigned int pasid,
-				  uint64_t addr,
-				  uint32_t status,
-				  unsigned int vmhub);
+								  unsigned int pasid,
+								  uint64_t addr,
+								  uint32_t status,
+								  unsigned int vmhub);
 void amdgpu_vm_tlb_fence_create(struct amdgpu_device *adev,
-				 struct amdgpu_vm *vm,
-				 struct dma_fence **fence);
+								struct amdgpu_vm *vm,
+								struct dma_fence **fence);
 
 #endif




--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm_pt.c	2025-04-10 14:44:49.000000000 +0200
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm_pt.c	2025-04-12 16:51:37.138829348 +0200
