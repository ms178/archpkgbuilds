--- a/libswscale/x86/yuv2yuvX.asm	2025-09-26 17:45:35.080694096 +0200
+++ b/libswscale/x86/yuv2yuvX.asm	2025-09-26 17:52:10.822145544 +0200

--- a/libavfilter/x86/vf_gradfun.asm	2025-09-26 17:24:07.932171857 +0200
+++ b/libavfilter/x86/vf_gradfun.asm	2025-09-26 17:40:20.379245450 +0200
@@ -20,91 +20,274 @@
 
 %include "libavutil/x86/x86util.asm"
 
-SECTION_RODATA
+SECTION_RODATA 32
 
-pw_7f: times 8 dw 0x7F
-pw_ff: times 8 dw 0xFF
+; 32-byte alignment ensures safe aligned YMM loads; SSE uses the low 16 bytes.
+align 32
+pw_7f: times 16 dw 0x007F
+
+align 32
+pw_ff: times 16 dw 0x00FF
 
 SECTION .text
 
+; -----------------------------------------------------------------------------
+; FILTER_LINE
+; Processes a "half-vector" worth of pixels based on current INIT_* width:
+; - MMXEXT: movh = 32-bit (4B) per invocation
+; - SSSE3 : movh = 64-bit (8B) per invocation
+; This macro is used by legacy MMXEXT and 8-byte SSSE3 implementations.
+; -----------------------------------------------------------------------------
 %macro FILTER_LINE 1
-    movh       m0, [r2+r0]
-    movh       m1, [r3+r0]
-    punpcklbw  m0, m7
-    punpcklwd  m1, m1
-    psllw      m0, 7
-    psubw      m1, m0
-    PABSW      m2, m1
-    pmulhuw    m2, m5
-    psubw      m2, m6
-    pminsw     m2, m7
-    pmullw     m2, m2
-    psllw      m1, 2
-    paddw      m0, %1
-    pmulhw     m1, m2
-    paddw      m0, m1
-    psraw      m0, 7
-    packuswb   m0, m0
-    movh  [r1+r0], m0
+    movh        m0, [r2+r0]      ; src half-vector (bytes)
+    movh        m1, [r3+r0]      ; blur/delta half-vector (words)
+    punpcklbw   m0, m7           ; src: bytes -> words
+    punpcklwd   m1, m1           ; duplicate words: 4->8 (MMXEXT), 4->8 (SSSE3 half)
+    psllw       m0, 7            ; src << 7
+    psubw       m1, m0           ; diff = blur - (src<<7)
+    PABSW       m2, m1           ; abs(diff)  (macro emulates on MMXEXT)
+    pmulhuw     m2, m5           ; (abs(diff) * thr) >> 16 (unsigned high)
+    psubw       m2, m6           ; t = scaled_abs - 127
+    pminsw      m2, m7           ; t = min(t, 0)
+    pmullw      m2, m2           ; t = t * t
+    psllw       m1, 2            ; diff <<= 2
+    paddw       m0, %1           ; + dither constant
+    pmulhw      m1, m2           ; (diff * t) >> 16 (signed high)
+    paddw       m0, m1           ; accumulate
+    psraw       m0, 7            ; >> 7 arithmetic
+    packuswb    m0, m0           ; pack to bytes
+    movh  [r1+r0], m0            ; store half-vector
 %endmacro
 
+; -----------------------------------------------------------------------------
+; MMXEXT version (legacy). Processes 8 bytes per loop (4+4).
+; Fix: movh m5, r4d -> movd m5, r4d (explicit size).
+; -----------------------------------------------------------------------------
 INIT_MMX mmxext
 cglobal gradfun_filter_line, 6, 6
-    movh      m5, r4d
-    pxor      m7, m7
-    pshufw    m5, m5,0
-    mova      m6, [pw_7f]
-    mova      m3, [r5]
-    mova      m4, [r5+8]
-.loop:
-    FILTER_LINE m3
-    add       r0, 4
-    jge .end
-    FILTER_LINE m4
-    add       r0, 4
-    jl .loop
+    movd        m5, r4d
+    pxor        m7, m7
+    pshufw      m5, m5, 0        ; broadcast thr to words
+    mova        m6, [pw_7f]
+    mova        m3, [r5]         ; dither low  (4 words)
+    mova        m4, [r5+8]       ; dither high (4 words)
+
+ALIGN 16
+.loop:
+    FILTER_LINE m3               ; 4 px
+    add         r0, 4
+    jge         .end
+    FILTER_LINE m4               ; 4 px
+    add         r0, 4
+    jl          .loop
 .end:
     RET
 
+; -----------------------------------------------------------------------------
+; SSSE3 version (widened). Processes 16 bytes per iteration.
+; Note: r3 holds words. We must duplicate per-word (punpcklwd/hwd), not bytes.
+; -----------------------------------------------------------------------------
 INIT_XMM ssse3
-cglobal gradfun_filter_line, 6, 6, 8
-    movd       m5, r4d
-    pxor       m7, m7
-    pshuflw    m5, m5, 0
-    mova       m6, [pw_7f]
-    punpcklqdq m5, m5
-    mova       m4, [r5]
-.loop:
-    FILTER_LINE m4
-    add        r0, 8
-    jl .loop
+cglobal gradfun_filter_line, 6, 6, 12
+    movd        m5, r4d
+    pxor        m7, m7
+    pshuflw     m5, m5, 0
+    punpcklqdq  m5, m5           ; broadcast thr
+    mova        m6, [pw_7f]
+    mova        m4, [r5]         ; 8-word dither
+
+ALIGN 16
+.loop:
+    ; Load 16 src bytes and 8 blur words (16B)
+    movu        m0, [r2+r0]      ; src bytes
+    mova        m1, [r3+r0]      ; blur words
+
+    ; Low half (8 pixels: bytes 0..7, words 0..3 duplicated)
+    mova        m2, m0           ; src copy
+    mova        m3, m1           ; blur copy
+    punpcklbw   m2, m7           ; bytes -> words
+    punpcklwd   m3, m3           ; duplicate words 0..3 -> 8 words
+    psllw       m2, 7
+    psubw       m3, m2
+    pabsw       m8, m3
+    pmulhuw     m8, m5
+    psubw       m8, m6
+    pminsw      m8, m7
+    pmullw      m8, m8
+    psllw       m3, 2
+    paddw       m2, m4
+    pmulhw      m3, m8
+    paddw       m2, m3
+    psraw       m2, 7
+
+    ; High half (8 pixels: bytes 8..15, words 4..7 duplicated)
+    punpckhbw   m0, m7           ; bytes -> words
+    punpckhwd   m1, m1           ; duplicate words 4..7 -> 8 words
+    psllw       m0, 7
+    psubw       m1, m0
+    pabsw       m8, m1
+    pmulhuw     m8, m5
+    psubw       m8, m6
+    pminsw      m8, m7
+    pmullw      m8, m8
+    psllw       m1, 2
+    paddw       m0, m4
+    pmulhw      m1, m8
+    paddw       m0, m1
+    psraw       m0, 7
+
+    ; Pack and store
+    packuswb    m2, m0
+    movu   [r1+r0], m2
+
+    add         r0, 16
+    jl          .loop
+    RET
+
+; -----------------------------------------------------------------------------
+; AVX2 version. Processes 32 bytes per iteration (full YMM).
+; Bit-exact: keeps pmulhuw/pmulhw semantics. r3 holds words; we duplicate with
+; vpunpcklwd/vpunpckhwd to match per-byte src expansion.
+; -----------------------------------------------------------------------------
+INIT_YMM avx2
+cglobal gradfun_filter_line, 6, 6, 16
+    vpxor           m7, m7, m7                           ; zero
+    vmovd           xmm5, r4d                             ; thr -> xmm5
+    vpbroadcastw    m5, xmm5                              ; thr -> all words (ymm5)
+    vmovdqa         m6, [pw_7f]                           ; 32B of 0x007F (ymm6)
+    vbroadcasti128  m4, [r5]                              ; 8-word dither -> both lanes
+
+ALIGN 32
+.loop:
+    ; Load two 16B chunks of src (bytes) and 32B of blur (words)
+    vmovdqu         xmm0, [r2+r0]                         ; src low 16B
+    vmovdqu         xmm1, [r2+r0+16]                      ; src high 16B
+    vpmovzxbw       m2, xmm0                              ; src_low -> 16 words
+    vpmovzxbw       m8, xmm1                              ; src_high -> 16 words
+
+    vmovdqu         m1, [r3+r0]                           ; blur words (16 words)
+    vpunpcklwd      m9,  m1, m1                           ; duplicate blur words 0..3/8..11 per lane
+    vpunpckhwd      m14, m1, m1                           ; duplicate blur words 4..7/12..15 per lane
+
+    ; ---- Low half ----
+    vpsllw          m10, m2, 7                            ; src<<7
+    vpsubw          m11, m9, m10                          ; diff
+    vpabsw          m12, m11                              ; abs(diff)
+    vpmulhuw        m12, m12, m5                          ; (abs*thr)>>16 (unsigned)
+    vpsubw          m12, m12, m6                          ; t = scaled_abs - 127
+    vpminsw         m12, m12, m7                          ; t = min(t, 0)
+    vpmullw         m12, m12, m12                         ; t = t*t
+    vpsllw          m11, m11, 2                           ; diff <<= 2
+    vpaddw          m10, m10, m4                          ; + dither
+    vpmulhw         m11, m11, m12                         ; signed high-half
+    vpaddw          m10, m10, m11
+    vpsraw          m10, m10, 7
+
+    ; ---- High half ----
+    vpsllw          m13, m8, 7
+    vpsubw          m15, m14, m13
+    vpabsw          m0,  m15
+    vpmulhuw        m0,  m0,  m5
+    vpsubw          m0,  m0,  m6
+    vpminsw         m0,  m0,  m7
+    vpmullw         m0,  m0,  m0
+    vpsllw          m15, m15, 2
+    vpaddw          m13, m13, m4
+    vpmulhw         m15, m15, m0
+    vpaddw          m13, m13, m15
+    vpsraw          m13, m13, 7
+
+    ; Pack and store 32B
+    vpackuswb       m10, m10, m13
+    vmovdqu     [r1+r0], m10
+
+    add             r0, 32
+    jl              .loop
+    vzeroupper
     RET
 
+; -----------------------------------------------------------------------------
+; BLUR_LINE macro: SSE2 functions (movdqa, movdqu).
+; -----------------------------------------------------------------------------
 %macro BLUR_LINE 1
 cglobal gradfun_blur_line_%1, 6, 6, 8
-    mova        m7, [pw_ff]
+    mova            m7, [pw_ff]
+ALIGN 16
 .loop:
-    %1          m0, [r4+r0]
-    %1          m1, [r5+r0]
-    mova        m2, m0
-    mova        m3, m1
-    psrlw       m0, 8
-    psrlw       m1, 8
-    pand        m2, m7
-    pand        m3, m7
-    paddw       m0, m1
-    paddw       m2, m3
-    paddw       m0, m2
-    paddw       m0, [r2+r0]
-    mova        m1, [r1+r0]
-    mova   [r1+r0], m0
-    psubw       m0, m1
-    mova   [r3+r0], m0
-    add         r0, 16
-    jl .loop
+    %1              m0, [r4+r0]       ; top/prev (16B of words)
+    %1              m1, [r5+r0]       ; bottom/next
+    mova            m2, m0
+    mova            m3, m1
+    psrlw           m0, 8
+    psrlw           m1, 8
+    pand            m2, m7
+    pand            m3, m7
+    paddw           m0, m1
+    paddw           m2, m3
+    paddw           m0, m2             ; sum high+low bytes
+    paddw           m0, [r2+r0]        ; add center/neighbor contrib
+    mova            m1, [r1+r0]
+    mova        [r1+r0], m0
+    psubw           m0, m1
+    mova        [r3+r0], m0
+    add             r0, 16
+    jl              .loop
     RET
 %endmacro
 
 INIT_XMM sse2
 BLUR_LINE movdqa
 BLUR_LINE movdqu
+
+; -----------------------------------------------------------------------------
+; AVX2 blur versions (32B per iteration) for higher throughput.
+; -----------------------------------------------------------------------------
+INIT_YMM avx2
+cglobal gradfun_blur_line_movdqa, 6, 6, 16
+    vmovdqa        m7, [pw_ff]
+ALIGN 32
+.loop:
+    vmovdqa        m0, [r4+r0]
+    vmovdqa        m1, [r5+r0]
+    vpsrlw         m2, m0, 8
+    vpand          m0, m0, m7
+    vpaddw         m0, m0, m2
+    vpsrlw         m2, m1, 8
+    vpand          m1, m1, m7
+    vpaddw         m1, m1, m2
+    vpaddw         m0, m0, m1
+    vmovdqu        m2, [r2+r0]
+    vpaddw         m0, m0, m2
+    vmovdqu        m1, [r1+r0]
+    vmovdqu    [r1+r0], m0
+    vpsubw         m0, m0, m1
+    vmovdqu    [r3+r0], m0
+    add            r0, 32
+    jl             .loop
+    vzeroupper
+    RET
+
+INIT_YMM avx2
+cglobal gradfun_blur_line_movdqu, 6, 6, 16
+    vmovdqa        m7, [pw_ff]
+ALIGN 32
+.loop:
+    vmovdqu        m0, [r4+r0]
+    vmovdqu        m1, [r5+r0]
+    vpsrlw         m2, m0, 8
+    vpand          m0, m0, m7
+    vpaddw         m0, m0, m2
+    vpsrlw         m2, m1, 8
+    vpand          m1, m1, m7
+    vpaddw         m1, m1, m2
+    vpaddw         m0, m0, m1
+    vmovdqu        m2, [r2+r0]
+    vpaddw         m0, m0, m2
+    vmovdqu        m1, [r1+r0]
+    vmovdqu    [r1+r0], m0
+    vpsubw         m0, m0, m1
+    vmovdqu    [r3+r0], m0
+    add            r0, 32
+    jl             .loop
+    vzeroupper
+    RET
