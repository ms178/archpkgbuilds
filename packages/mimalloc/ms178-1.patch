--- a/src/alloc.c	2026-01-23 17:30:48.341057855 +0100
+++ b/src/alloc.c	2026-01-31 21:51:54.456295310 +0100
@@ -1,125 +1,135 @@
-/* ----------------------------------------------------------------------------
-Copyright (c) 2018-2024, Microsoft Research, Daan Leijen
-This is free software; you can redistribute it and/or modify it under the
-terms of the MIT license. A copy of the license can be found in the file
-"LICENSE" at the root of this distribution.
------------------------------------------------------------------------------*/
 #ifndef _DEFAULT_SOURCE
-#define _DEFAULT_SOURCE   // for realpath() on Linux
+#define _DEFAULT_SOURCE
 #endif
 
 #include "mimalloc.h"
 #include "mimalloc/internal.h"
 #include "mimalloc/atomic.h"
-#include "mimalloc/prim.h"   // _mi_prim_thread_id()
+#include "mimalloc/prim.h"
 
-#include <string.h>      // memset, strlen (for mi_strdup)
-#include <stdlib.h>      // malloc, abort
+#include <string.h>
+#include <stdlib.h>
+
+#if defined(__GNUC__) || defined(__clang__)
+  #if defined(__x86_64__) || defined(__i386__)
+    #define mi_prefetch_block(addr) __builtin_prefetch((addr), 0, 3)
+  #elif defined(__aarch64__)
+    #define mi_prefetch_block(addr) __builtin_prefetch((addr), 0, 3)
+  #else
+    #define mi_prefetch_block(addr) ((void)(addr))
+  #endif
+#else
+  #define mi_prefetch_block(addr) ((void)(addr))
+#endif
 
 #define MI_IN_ALLOC_C
 #include "alloc-override.c"
 #include "free.c"
 #undef MI_IN_ALLOC_C
 
-// ------------------------------------------------------
-// Allocation
-// ------------------------------------------------------
-
-// Fast allocation in a page: just pop from the free list.
-// Fall back to generic allocation only if the list is empty.
-// Note: in release mode the (inlined) routine is about 7 instructions with a single test.
 extern inline void* _mi_page_malloc_zero(mi_heap_t* heap, mi_page_t* page, size_t size, bool zero, size_t* usable) mi_attr_noexcept
 {
   mi_assert_internal(size >= MI_PADDING_SIZE);
-  mi_assert_internal(page->block_size == 0 /* empty heap */ || mi_page_block_size(page) >= size);
+  mi_assert_internal(page->block_size == 0 || mi_page_block_size(page) >= size);
 
-  // check the free list
   mi_block_t* const block = page->free;
   if mi_unlikely(block == NULL) {
     return _mi_malloc_generic(heap, size, zero, 0, usable);
   }
+
   mi_assert_internal(block != NULL && _mi_ptr_page(block) == page);
-  if (usable != NULL) { *usable = mi_page_usable_block_size(page); };
-  // pop from the free list
-  page->free = mi_block_next(page, block);
+
+  const size_t bsize = mi_page_usable_block_size(page);
+
+  if (usable != NULL) {
+    *usable = bsize;
+  }
+
+  mi_block_t* const next = mi_block_next(page, block);
+  page->free = next;
   page->used++;
+
+  if mi_likely(next != NULL) {
+    mi_prefetch_block(next);
+  }
+
   mi_assert_internal(page->free == NULL || _mi_ptr_page(page->free) == page);
   mi_assert_internal(page->block_size < MI_MAX_ALIGN_SIZE || _mi_is_aligned(block, MI_MAX_ALIGN_SIZE));
 
-  #if MI_DEBUG>3
+#if MI_DEBUG>3
   if (page->free_is_zero && size > sizeof(*block)) {
-    mi_assert_expensive(mi_mem_is_zero(block+1,size - sizeof(*block)));
+    mi_assert_expensive(mi_mem_is_zero(block + 1, size - sizeof(*block)));
   }
-  #endif
+#endif
 
-  // allow use of the block internally
-  // note: when tracking we need to avoid ever touching the MI_PADDING since
-  // that is tracked by valgrind etc. as non-accessible (through the red-zone, see `mimalloc/track.h`)
-  mi_track_mem_undefined(block, mi_page_usable_block_size(page));
+  mi_track_mem_undefined(block, bsize);
 
-  // zero the block? note: we need to zero the full block size (issue #63)
   if mi_unlikely(zero) {
-    mi_assert_internal(page->block_size != 0); // do not call with zero'ing for huge blocks (see _mi_malloc_generic)
-    #if MI_PADDING
+    mi_assert_internal(page->block_size != 0);
+#if MI_PADDING
     mi_assert_internal(page->block_size >= MI_PADDING_SIZE);
-    #endif
+#endif
+    const size_t zsize = page->block_size - MI_PADDING_SIZE;
     if (page->free_is_zero) {
       block->next = 0;
-      mi_track_mem_defined(block, page->block_size - MI_PADDING_SIZE);
+      mi_track_mem_defined(block, zsize);
     }
     else {
-      _mi_memzero_aligned(block, page->block_size - MI_PADDING_SIZE);
+      _mi_memzero_aligned(block, zsize);
     }
   }
 
-  #if (MI_DEBUG>0) && !MI_TRACK_ENABLED && !MI_TSAN
+#if (MI_DEBUG>0) && !MI_TRACK_ENABLED && !MI_TSAN
   if (!zero && !mi_page_is_huge(page)) {
-    memset(block, MI_DEBUG_UNINIT, mi_page_usable_block_size(page));
+    memset(block, MI_DEBUG_UNINIT, bsize);
   }
-  #elif (MI_SECURE!=0)
-  if (!zero) { block->next = 0; } // don't leak internal data
-  #endif
+#elif (MI_SECURE!=0)
+  if (!zero) {
+    block->next = 0;
+  }
+#endif
 
-  #if (MI_STAT>0)
-  const size_t bsize = mi_page_usable_block_size(page);
+#if (MI_STAT>0)
   if (bsize <= MI_MEDIUM_OBJ_SIZE_MAX) {
     mi_heap_stat_increase(heap, malloc_normal, bsize);
     mi_heap_stat_counter_increase(heap, malloc_normal_count, 1);
-    #if (MI_STAT>1)
+#if (MI_STAT>1)
     const size_t bin = _mi_bin(bsize);
     mi_heap_stat_increase(heap, malloc_bins[bin], 1);
     mi_heap_stat_increase(heap, malloc_requested, size - MI_PADDING_SIZE);
-    #endif
+#endif
   }
-  #endif
+#endif
 
-  #if MI_PADDING // && !MI_TRACK_ENABLED
-    mi_padding_t* const padding = (mi_padding_t*)((uint8_t*)block + mi_page_usable_block_size(page));
-    ptrdiff_t delta = ((uint8_t*)padding - (uint8_t*)block - (size - MI_PADDING_SIZE));
-    #if (MI_DEBUG>=2)
-    mi_assert_internal(delta >= 0 && mi_page_usable_block_size(page) >= (size - MI_PADDING_SIZE + delta));
-    #endif
-    mi_track_mem_defined(padding,sizeof(mi_padding_t));  // note: re-enable since mi_page_usable_block_size may set noaccess
-    padding->canary = mi_ptr_encode_canary(page,block,page->keys);
-    padding->delta  = (uint32_t)(delta);
-    #if MI_PADDING_CHECK
-    if (!mi_page_is_huge(page)) {
-      uint8_t* fill = (uint8_t*)padding - delta;
-      const size_t maxpad = (delta > MI_MAX_ALIGN_SIZE ? MI_MAX_ALIGN_SIZE : delta); // set at most N initial padding bytes
-      for (size_t i = 0; i < maxpad; i++) { fill[i] = MI_DEBUG_PADDING; }
+#if MI_PADDING
+  mi_padding_t* const padding = (mi_padding_t*)((uint8_t*)block + bsize);
+  ptrdiff_t delta = ((uint8_t*)padding - (uint8_t*)block - (size - MI_PADDING_SIZE));
+#if (MI_DEBUG>=2)
+  mi_assert_internal(delta >= 0 && bsize >= (size - MI_PADDING_SIZE + delta));
+#endif
+  mi_track_mem_defined(padding, sizeof(mi_padding_t));
+  padding->canary = mi_ptr_encode_canary(page, block, page->keys);
+  padding->delta  = (uint32_t)(delta);
+#if MI_PADDING_CHECK
+  if (!mi_page_is_huge(page)) {
+    uint8_t* fill = (uint8_t*)padding - delta;
+    const size_t maxpad = (delta > MI_MAX_ALIGN_SIZE ? MI_MAX_ALIGN_SIZE : (size_t)delta);
+    for (size_t i = 0; i < maxpad; i++) {
+      fill[i] = MI_DEBUG_PADDING;
     }
-    #endif
-  #endif
+  }
+#endif
+#endif
 
   return block;
 }
 
-// extra entries for improved efficiency in `alloc-aligned.c`.
 extern void* _mi_page_malloc(mi_heap_t* heap, mi_page_t* page, size_t size) mi_attr_noexcept {
-  return _mi_page_malloc_zero(heap,page,size,false,NULL);
+  return _mi_page_malloc_zero(heap, page, size, false, NULL);
 }
+
 extern void* _mi_page_malloc_zeroed(mi_heap_t* heap, mi_page_t* page, size_t size) mi_attr_noexcept {
-  return _mi_page_malloc_zero(heap,page,size,true,NULL);
+  return _mi_page_malloc_zero(heap, page, size, true, NULL);
 }
 
 #if MI_GUARDED
@@ -129,33 +139,33 @@ mi_decl_restrict void* _mi_heap_malloc_g
 static inline mi_decl_restrict void* mi_heap_malloc_small_zero(mi_heap_t* heap, size_t size, bool zero, size_t* usable) mi_attr_noexcept {
   mi_assert(heap != NULL);
   mi_assert(size <= MI_SMALL_SIZE_MAX);
-  #if MI_DEBUG
+#if MI_DEBUG
   const uintptr_t tid = _mi_thread_id();
-  mi_assert(heap->thread_id == 0 || heap->thread_id == tid); // heaps are thread local
-  #endif
-  #if (MI_PADDING || MI_GUARDED)
-  if (size == 0) { size = sizeof(void*); }
-  #endif
-  #if MI_GUARDED
-  if (mi_heap_malloc_use_guarded(heap,size)) {
+  mi_assert(heap->thread_id == 0 || heap->thread_id == tid);
+#endif
+#if (MI_PADDING || MI_GUARDED)
+  if (size == 0) {
+    size = sizeof(void*);
+  }
+#endif
+#if MI_GUARDED
+  if (mi_heap_malloc_use_guarded(heap, size)) {
     return _mi_heap_malloc_guarded(heap, size, zero);
   }
-  #endif
+#endif
 
-  // get page in constant time, and allocate from it
   mi_page_t* page = _mi_heap_get_free_small_page(heap, size + MI_PADDING_SIZE);
   void* const p = _mi_page_malloc_zero(heap, page, size + MI_PADDING_SIZE, zero, usable);
-  mi_track_malloc(p,size,zero);
+  mi_track_malloc(p, size, zero);
 
-  #if MI_DEBUG>3
+#if MI_DEBUG>3
   if (p != NULL && zero) {
     mi_assert_expensive(mi_mem_is_zero(p, size));
   }
-  #endif
+#endif
   return p;
 }
 
-// allocate a small block
 mi_decl_nodiscard extern inline mi_decl_restrict void* mi_heap_malloc_small(mi_heap_t* heap, size_t size) mi_attr_noexcept {
   return mi_heap_malloc_small_zero(heap, size, false, NULL);
 }
@@ -164,30 +174,27 @@ mi_decl_nodiscard extern inline mi_decl_
   return mi_heap_malloc_small(mi_prim_get_default_heap(), size);
 }
 
-// The main allocation function
 extern inline void* _mi_heap_malloc_zero_ex(mi_heap_t* heap, size_t size, bool zero, size_t huge_alignment, size_t* usable) mi_attr_noexcept {
-  // fast path for small objects
   if mi_likely(size <= MI_SMALL_SIZE_MAX) {
     mi_assert_internal(huge_alignment == 0);
     return mi_heap_malloc_small_zero(heap, size, zero, usable);
   }
-  #if MI_GUARDED
-  else if (huge_alignment==0 && mi_heap_malloc_use_guarded(heap,size)) {
+#if MI_GUARDED
+  else if (huge_alignment == 0 && mi_heap_malloc_use_guarded(heap, size)) {
     return _mi_heap_malloc_guarded(heap, size, zero);
   }
-  #endif
+#endif
   else {
-    // regular allocation
-    mi_assert(heap!=NULL);
-    mi_assert(heap->thread_id == 0 || heap->thread_id == _mi_thread_id());   // heaps are thread local
-    void* const p = _mi_malloc_generic(heap, size + MI_PADDING_SIZE, zero, huge_alignment, usable);  // note: size can overflow but it is detected in malloc_generic
-    mi_track_malloc(p,size,zero);
+    mi_assert(heap != NULL);
+    mi_assert(heap->thread_id == 0 || heap->thread_id == _mi_thread_id());
+    void* const p = _mi_malloc_generic(heap, size + MI_PADDING_SIZE, zero, huge_alignment, usable);
+    mi_track_malloc(p, size, zero);
 
-    #if MI_DEBUG>3
+#if MI_DEBUG>3
     if (p != NULL && zero) {
       mi_assert_expensive(mi_mem_is_zero(p, size));
     }
-    #endif
+#endif
     return p;
   }
 }
@@ -204,7 +211,6 @@ mi_decl_nodiscard extern inline mi_decl_
   return mi_heap_malloc(mi_prim_get_default_heap(), size);
 }
 
-// zero initialized small block
 mi_decl_nodiscard mi_decl_restrict void* mi_zalloc_small(size_t size) mi_attr_noexcept {
   return mi_heap_malloc_small_zero(mi_prim_get_default_heap(), size, true, NULL);
 }
@@ -214,21 +220,19 @@ mi_decl_nodiscard extern inline mi_decl_
 }
 
 mi_decl_nodiscard mi_decl_restrict void* mi_zalloc(size_t size) mi_attr_noexcept {
-  return mi_heap_zalloc(mi_prim_get_default_heap(),size);
+  return mi_heap_zalloc(mi_prim_get_default_heap(), size);
 }
 
-
 mi_decl_nodiscard extern inline mi_decl_restrict void* mi_heap_calloc(mi_heap_t* heap, size_t count, size_t size) mi_attr_noexcept {
   size_t total;
-  if (mi_count_size_overflow(count,size,&total)) return NULL;
-  return mi_heap_zalloc(heap,total);
+  if mi_unlikely(mi_count_size_overflow(count, size, &total)) return NULL;
+  return mi_heap_zalloc(heap, total);
 }
 
 mi_decl_nodiscard mi_decl_restrict void* mi_calloc(size_t count, size_t size) mi_attr_noexcept {
-  return mi_heap_calloc(mi_prim_get_default_heap(),count,size);
+  return mi_heap_calloc(mi_prim_get_default_heap(), count, size);
 }
 
-// Return usable size
 mi_decl_nodiscard mi_decl_restrict void* mi_umalloc_small(size_t size, size_t* usable) mi_attr_noexcept {
   return mi_heap_malloc_small_zero(mi_prim_get_default_heap(), size, false, usable);
 }
@@ -247,75 +251,76 @@ mi_decl_nodiscard mi_decl_restrict void*
 
 mi_decl_nodiscard mi_decl_restrict void* mi_ucalloc(size_t count, size_t size, size_t* usable) mi_attr_noexcept {
   size_t total;
-  if (mi_count_size_overflow(count,size,&total)) return NULL;
+  if mi_unlikely(mi_count_size_overflow(count, size, &total)) return NULL;
   return mi_uzalloc(total, usable);
 }
 
-// Uninitialized `calloc`
 mi_decl_nodiscard extern mi_decl_restrict void* mi_heap_mallocn(mi_heap_t* heap, size_t count, size_t size) mi_attr_noexcept {
   size_t total;
-  if (mi_count_size_overflow(count, size, &total)) return NULL;
+  if mi_unlikely(mi_count_size_overflow(count, size, &total)) return NULL;
   return mi_heap_malloc(heap, total);
 }
 
 mi_decl_nodiscard mi_decl_restrict void* mi_mallocn(size_t count, size_t size) mi_attr_noexcept {
-  return mi_heap_mallocn(mi_prim_get_default_heap(),count,size);
+  return mi_heap_mallocn(mi_prim_get_default_heap(), count, size);
 }
 
-// Expand (or shrink) in place (or fail)
 void* mi_expand(void* p, size_t newsize) mi_attr_noexcept {
-  #if MI_PADDING
-  // we do not shrink/expand with padding enabled
-  MI_UNUSED(p); MI_UNUSED(newsize);
+#if MI_PADDING
+  MI_UNUSED(p);
+  MI_UNUSED(newsize);
   return NULL;
-  #else
-  if (p == NULL) return NULL;
-  const mi_page_t* const page = mi_validate_ptr_page(p,"mi_expand");  
-  const size_t size = _mi_usable_size(p,page);
-  if (newsize > size) return NULL;
-  return p; // it fits
-  #endif
+#else
+  if mi_unlikely(p == NULL) return NULL;
+  const mi_page_t* const page = mi_validate_ptr_page(p, "mi_expand");
+  const size_t size = _mi_usable_size(p, page);
+  if mi_unlikely(newsize > size) return NULL;
+  return p;
+#endif
 }
 
 void* _mi_heap_realloc_zero(mi_heap_t* heap, void* p, size_t newsize, bool zero, size_t* usable_pre, size_t* usable_post) mi_attr_noexcept {
-  // if p == NULL then behave as malloc.
-  // else if size == 0 then reallocate to a zero-sized block (and don't return NULL, just as mi_malloc(0)).
-  // (this means that returning NULL always indicates an error, and `p` will not have been freed in that case.)
   const mi_page_t* page;
   size_t size;
-  if (p==NULL) {
+
+  if mi_unlikely(p == NULL) {
     page = NULL;
     size = 0;
-    if (usable_pre!=NULL) { *usable_pre = 0; }
+    if (usable_pre != NULL) {
+      *usable_pre = 0;
+    }
+  }
+  else {
+    page = mi_validate_ptr_page(p, "mi_realloc");
+    size = _mi_usable_size(p, page);
+    if (usable_pre != NULL) {
+      *usable_pre = mi_page_usable_block_size(page);
+    }
+    mi_prefetch_block(p);
   }
-  else {    
-    page = mi_validate_ptr_page(p,"mi_realloc");  
-    size = _mi_usable_size(p,page);
-    if (usable_pre!=NULL) { *usable_pre = mi_page_usable_block_size(page); }    
-  }
-  if mi_unlikely(newsize <= size && newsize >= (size / 2) && newsize > 0) {  // note: newsize must be > 0 or otherwise we return NULL for realloc(NULL,0)
-    mi_assert_internal(p!=NULL);
-    // todo: do not track as the usable size is still the same in the free; adjust potential padding?
-    // mi_track_resize(p,size,newsize)
-    // if (newsize < size) { mi_track_mem_noaccess((uint8_t*)p + newsize, size - newsize); }
-    if (usable_post!=NULL) { *usable_post = mi_page_usable_block_size(page); }
-    return p;  // reallocation still fits and not more than 50% waste
+
+  if mi_unlikely(newsize <= size && newsize >= (size / 2) && newsize > 0) {
+    mi_assert_internal(p != NULL);
+    if (usable_post != NULL) {
+      *usable_post = mi_page_usable_block_size(page);
+    }
+    return p;
   }
-  void* newp = mi_heap_umalloc(heap,newsize,usable_post);
+
+  void* newp = mi_heap_umalloc(heap, newsize, usable_post);
   if mi_likely(newp != NULL) {
     if (zero && newsize > size) {
-      // also set last word in the previous allocation to zero to ensure any padding is zero-initialized
       const size_t start = (size >= sizeof(intptr_t) ? size - sizeof(intptr_t) : 0);
       _mi_memzero((uint8_t*)newp + start, newsize - start);
     }
-    else if (newsize == 0) {
-      ((uint8_t*)newp)[0] = 0; // work around for applications that expect zero-reallocation to be zero initialized (issue #725)
+    else if mi_unlikely(newsize == 0) {
+      ((uint8_t*)newp)[0] = 0;
     }
     if mi_likely(p != NULL) {
       const size_t copysize = (newsize > size ? size : newsize);
-      mi_track_mem_defined(p,copysize);  // _mi_useable_size may be too large for byte precise memory tracking..
+      mi_track_mem_defined(p, copysize);
       _mi_memcpy(newp, p, copysize);
-      mi_free(p); // only free the original pointer if successful
+      mi_free(p);
     }
   }
   return newp;
@@ -327,15 +332,13 @@ mi_decl_nodiscard void* mi_heap_realloc(
 
 mi_decl_nodiscard void* mi_heap_reallocn(mi_heap_t* heap, void* p, size_t count, size_t size) mi_attr_noexcept {
   size_t total;
-  if (mi_count_size_overflow(count, size, &total)) return NULL;
+  if mi_unlikely(mi_count_size_overflow(count, size, &total)) return NULL;
   return mi_heap_realloc(heap, p, total);
 }
 
-
-// Reallocate but free `p` on errors
 mi_decl_nodiscard void* mi_heap_reallocf(mi_heap_t* heap, void* p, size_t newsize) mi_attr_noexcept {
   void* newp = mi_heap_realloc(heap, p, newsize);
-  if (newp==NULL && p!=NULL) mi_free(p);
+  if (newp == NULL && p != NULL) mi_free(p);
   return newp;
 }
 
@@ -345,26 +348,24 @@ mi_decl_nodiscard void* mi_heap_rezalloc
 
 mi_decl_nodiscard void* mi_heap_recalloc(mi_heap_t* heap, void* p, size_t count, size_t size) mi_attr_noexcept {
   size_t total;
-  if (mi_count_size_overflow(count, size, &total)) return NULL;
+  if mi_unlikely(mi_count_size_overflow(count, size, &total)) return NULL;
   return mi_heap_rezalloc(heap, p, total);
 }
 
-
 mi_decl_nodiscard void* mi_realloc(void* p, size_t newsize) mi_attr_noexcept {
-  return mi_heap_realloc(mi_prim_get_default_heap(),p,newsize);
+  return mi_heap_realloc(mi_prim_get_default_heap(), p, newsize);
 }
 
 mi_decl_nodiscard void* mi_reallocn(void* p, size_t count, size_t size) mi_attr_noexcept {
-  return mi_heap_reallocn(mi_prim_get_default_heap(),p,count,size);
+  return mi_heap_reallocn(mi_prim_get_default_heap(), p, count, size);
 }
 
 mi_decl_nodiscard void* mi_urealloc(void* p, size_t newsize, size_t* usable_pre, size_t* usable_post) mi_attr_noexcept {
-  return _mi_heap_realloc_zero(mi_prim_get_default_heap(),p,newsize, false, usable_pre, usable_post);
+  return _mi_heap_realloc_zero(mi_prim_get_default_heap(), p, newsize, false, usable_pre, usable_post);
 }
 
-// Reallocate but free `p` on errors
 mi_decl_nodiscard void* mi_reallocf(void* p, size_t newsize) mi_attr_noexcept {
-  return mi_heap_reallocf(mi_prim_get_default_heap(),p,newsize);
+  return mi_heap_reallocf(mi_prim_get_default_heap(), p, newsize);
 }
 
 mi_decl_nodiscard void* mi_rezalloc(void* p, size_t newsize) mi_attr_noexcept {
@@ -375,18 +376,11 @@ mi_decl_nodiscard void* mi_recalloc(void
   return mi_heap_recalloc(mi_prim_get_default_heap(), p, count, size);
 }
 
-
-
-// ------------------------------------------------------
-// strdup, strndup, and realpath
-// ------------------------------------------------------
-
-// `strdup` using mi_malloc
 mi_decl_nodiscard mi_decl_restrict char* mi_heap_strdup(mi_heap_t* heap, const char* s) mi_attr_noexcept {
-  if (s == NULL) return NULL;
+  if mi_unlikely(s == NULL) return NULL;
   size_t len = _mi_strlen(s);
-  char* t = (char*)mi_heap_malloc(heap,len+1);
-  if (t == NULL) return NULL;
+  char* t = (char*)mi_heap_malloc(heap, len + 1);
+  if mi_unlikely(t == NULL) return NULL;
   _mi_memcpy(t, s, len);
   t[len] = 0;
   return t;
@@ -396,37 +390,36 @@ mi_decl_nodiscard mi_decl_restrict char*
   return mi_heap_strdup(mi_prim_get_default_heap(), s);
 }
 
-// `strndup` using mi_malloc
 mi_decl_nodiscard mi_decl_restrict char* mi_heap_strndup(mi_heap_t* heap, const char* s, size_t n) mi_attr_noexcept {
-  if (s == NULL) return NULL;
-  const size_t len = _mi_strnlen(s,n);  // len <= n
-  char* t = (char*)mi_heap_malloc(heap, len+1);
-  if (t == NULL) return NULL;
+  if mi_unlikely(s == NULL) return NULL;
+  const size_t len = _mi_strnlen(s, n);
+  char* t = (char*)mi_heap_malloc(heap, len + 1);
+  if mi_unlikely(t == NULL) return NULL;
   _mi_memcpy(t, s, len);
   t[len] = 0;
   return t;
 }
 
 mi_decl_nodiscard mi_decl_restrict char* mi_strndup(const char* s, size_t n) mi_attr_noexcept {
-  return mi_heap_strndup(mi_prim_get_default_heap(),s,n);
+  return mi_heap_strndup(mi_prim_get_default_heap(), s, n);
 }
 
 #ifndef __wasi__
-// `realpath` using mi_malloc
 #ifdef _WIN32
 #ifndef PATH_MAX
 #define PATH_MAX MAX_PATH
 #endif
 
 mi_decl_nodiscard mi_decl_restrict char* mi_heap_realpath(mi_heap_t* heap, const char* fname, char* resolved_name) mi_attr_noexcept {
-  // todo: use GetFullPathNameW to allow longer file names
   char buf[PATH_MAX];
   DWORD res = GetFullPathNameA(fname, PATH_MAX, (resolved_name == NULL ? buf : resolved_name), NULL);
   if (res == 0) {
-    errno = GetLastError(); return NULL;
+    errno = GetLastError();
+    return NULL;
   }
   else if (res > PATH_MAX) {
-    errno = EINVAL; return NULL;
+    errno = EINVAL;
+    return NULL;
   }
   else if (resolved_name != NULL) {
     return resolved_name;
@@ -436,79 +429,43 @@ mi_decl_nodiscard mi_decl_restrict char*
   }
 }
 #else
-/*
-#include <unistd.h>  // pathconf
-static size_t mi_path_max(void) {
-  static size_t path_max = 0;
-  if (path_max <= 0) {
-    long m = pathconf("/",_PC_PATH_MAX);
-    if (m <= 0) path_max = 4096;      // guess
-    else if (m < 256) path_max = 256; // at least 256
-    else path_max = m;
-  }
-  return path_max;
-}
-*/
 char* mi_heap_realpath(mi_heap_t* heap, const char* fname, char* resolved_name) mi_attr_noexcept {
   if (resolved_name != NULL) {
-    return realpath(fname,resolved_name);
+    return realpath(fname, resolved_name);
   }
   else {
     char* rname = realpath(fname, NULL);
     if (rname == NULL) return NULL;
     char* result = mi_heap_strdup(heap, rname);
-    mi_cfree(rname);  // use checked free (which may be redirected to our free but that's ok)
-    // note: with ASAN realpath is intercepted and mi_cfree may leak the returned pointer :-(
-    return result;
-  }
-  /*
-    const size_t n  = mi_path_max();
-    char* buf = (char*)mi_malloc(n+1);
-    if (buf == NULL) {
-      errno = ENOMEM;
-      return NULL;
-    }
-    char* rname  = realpath(fname,buf);
-    char* result = mi_heap_strndup(heap,rname,n); // ok if `rname==NULL`
-    mi_free(buf);
+    mi_cfree(rname);
     return result;
   }
-  */
 }
 #endif
 
 mi_decl_nodiscard mi_decl_restrict char* mi_realpath(const char* fname, char* resolved_name) mi_attr_noexcept {
-  return mi_heap_realpath(mi_prim_get_default_heap(),fname,resolved_name);
+  return mi_heap_realpath(mi_prim_get_default_heap(), fname, resolved_name);
 }
 #endif
 
-/*-------------------------------------------------------
-C++ new and new_aligned
-The standard requires calling into `get_new_handler` and
-throwing the bad_alloc exception on failure. If we compile
-with a C++ compiler we can implement this precisely. If we
-use a C compiler we cannot throw a `bad_alloc` exception
-but we call `exit` instead (i.e. not returning).
--------------------------------------------------------*/
-
 #ifdef __cplusplus
 #include <new>
 static bool mi_try_new_handler(bool nothrow) {
-  #if defined(_MSC_VER) || (__cplusplus >= 201103L)
-    std::new_handler h = std::get_new_handler();
-  #else
-    std::new_handler h = std::set_new_handler();
-    std::set_new_handler(h);
-  #endif
-  if (h==NULL) {
+#if defined(_MSC_VER) || (__cplusplus >= 201103L)
+  std::new_handler h = std::get_new_handler();
+#else
+  std::new_handler h = std::set_new_handler();
+  std::set_new_handler(h);
+#endif
+  if (h == NULL) {
     _mi_error_message(ENOMEM, "out of memory in 'new'");
-    #if defined(_CPPUNWIND) || defined(__cpp_exceptions)  // exceptions are not always enabled
+#if defined(_CPPUNWIND) || defined(__cpp_exceptions)
     if (!nothrow) {
       throw std::bad_alloc();
     }
-    #else
+#else
     MI_UNUSED(nothrow);
-    #endif
+#endif
     return false;
   }
   else {
@@ -519,7 +476,7 @@ static bool mi_try_new_handler(bool noth
 #else
 typedef void (*std_new_handler_t)(void);
 
-#if (defined(__GNUC__) || (defined(__clang__) && !defined(_MSC_VER)))  // exclude clang-cl, see issue #631
+#if (defined(__GNUC__) || (defined(__clang__) && !defined(_MSC_VER)))
 std_new_handler_t __attribute__((weak)) _ZSt15get_new_handlerv(void) {
   return NULL;
 }
@@ -527,7 +484,6 @@ static std_new_handler_t mi_get_new_hand
   return _ZSt15get_new_handlerv();
 }
 #else
-// note: on windows we could dynamically link to `?get_new_handler@std@@YAP6AXXZXZ`.
 static std_new_handler_t mi_get_new_handler(void) {
   return NULL;
 }
@@ -535,10 +491,10 @@ static std_new_handler_t mi_get_new_hand
 
 static bool mi_try_new_handler(bool nothrow) {
   std_new_handler_t h = mi_get_new_handler();
-  if (h==NULL) {
+  if (h == NULL) {
     _mi_error_message(ENOMEM, "out of memory in 'new'");
     if (!nothrow) {
-      abort();  // cannot throw in plain C, use abort
+      abort();
     }
     return false;
   }
@@ -549,10 +505,10 @@ static bool mi_try_new_handler(bool noth
 }
 #endif
 
-mi_decl_export mi_decl_noinline void* mi_heap_try_new(mi_heap_t* heap, size_t size, bool nothrow ) {
+mi_decl_export mi_decl_noinline void* mi_heap_try_new(mi_heap_t* heap, size_t size, bool nothrow) {
   void* p = NULL;
-  while(p == NULL && mi_try_new_handler(nothrow)) {
-    p = mi_heap_malloc(heap,size);
+  while (p == NULL && mi_try_new_handler(nothrow)) {
+    p = mi_heap_malloc(heap, size);
   }
   return p;
 }
@@ -561,9 +517,8 @@ static mi_decl_noinline void* mi_try_new
   return mi_heap_try_new(mi_prim_get_default_heap(), size, nothrow);
 }
 
-
 mi_decl_nodiscard mi_decl_restrict void* mi_heap_alloc_new(mi_heap_t* heap, size_t size) {
-  void* p = mi_heap_malloc(heap,size);
+  void* p = mi_heap_malloc(heap, size);
   if mi_unlikely(p == NULL) return mi_heap_try_new(heap, size, false);
   return p;
 }
@@ -572,15 +527,14 @@ mi_decl_nodiscard mi_decl_restrict void*
   return mi_heap_alloc_new(mi_prim_get_default_heap(), size);
 }
 
-
 mi_decl_nodiscard mi_decl_restrict void* mi_heap_alloc_new_n(mi_heap_t* heap, size_t count, size_t size) {
   size_t total;
   if mi_unlikely(mi_count_size_overflow(count, size, &total)) {
-    mi_try_new_handler(false);  // on overflow we invoke the try_new_handler once to potentially throw std::bad_alloc
+    mi_try_new_handler(false);
     return NULL;
   }
   else {
-    return mi_heap_alloc_new(heap,total);
+    return mi_heap_alloc_new(heap, total);
   }
 }
 
@@ -588,7 +542,6 @@ mi_decl_nodiscard mi_decl_restrict void*
   return mi_heap_alloc_new_n(mi_prim_get_default_heap(), count, size);
 }
 
-
 mi_decl_nodiscard mi_decl_restrict void* mi_new_nothrow(size_t size) mi_attr_noexcept {
   void* p = mi_malloc(size);
   if mi_unlikely(p == NULL) return mi_try_new(size, true);
@@ -599,8 +552,7 @@ mi_decl_nodiscard mi_decl_restrict void*
   void* p;
   do {
     p = mi_malloc_aligned(size, alignment);
-  }
-  while(p == NULL && mi_try_new_handler(false));
+  } while (p == NULL && mi_try_new_handler(false));
   return p;
 }
 
@@ -608,8 +560,7 @@ mi_decl_nodiscard mi_decl_restrict void*
   void* p;
   do {
     p = mi_malloc_aligned(size, alignment);
-  }
-  while(p == NULL && mi_try_new_handler(true));
+  } while (p == NULL && mi_try_new_handler(true));
   return p;
 }
 
@@ -624,7 +575,7 @@ mi_decl_nodiscard void* mi_new_realloc(v
 mi_decl_nodiscard void* mi_new_reallocn(void* p, size_t newcount, size_t size) {
   size_t total;
   if mi_unlikely(mi_count_size_overflow(newcount, size, &total)) {
-    mi_try_new_handler(false);  // on overflow we invoke the try_new_handler once to potentially throw std::bad_alloc
+    mi_try_new_handler(false);
     return NULL;
   }
   else {
@@ -633,28 +584,24 @@ mi_decl_nodiscard void* mi_new_reallocn(
 }
 
 #if MI_GUARDED
-// We always allocate a guarded allocation at an offset (`mi_page_has_aligned` will be true).
-// We then set the first word of the block to `0` for regular offset aligned allocations (in `alloc-aligned.c`)
-// and the first word to `~0` for guarded allocations to have a correct `mi_usable_size`
-
 static void* mi_block_ptr_set_guarded(mi_block_t* block, size_t obj_size) {
-  // TODO: we can still make padding work by moving it out of the guard page area
   mi_page_t* const page = _mi_ptr_page(block);
   mi_page_set_has_aligned(page, true);
   block->next = MI_BLOCK_TAG_GUARDED;
 
-  // set guard page at the end of the block
   mi_segment_t* const segment = _mi_page_segment(page);
-  const size_t block_size = mi_page_block_size(page);  // must use `block_size` to match `mi_free_local`
+  const size_t block_size = mi_page_block_size(page);
   const size_t os_page_size = _mi_os_page_size();
+
   mi_assert_internal(block_size >= obj_size + os_page_size + sizeof(mi_block_t));
-  if (block_size < obj_size + os_page_size + sizeof(mi_block_t)) {
-    // should never happen
+  if mi_unlikely(block_size < obj_size + os_page_size + sizeof(mi_block_t)) {
     mi_free(block);
     return NULL;
   }
+
   uint8_t* guard_page = (uint8_t*)block + block_size - os_page_size;
   mi_assert_internal(_mi_is_aligned(guard_page, os_page_size));
+
   if mi_likely(segment->allow_decommit && _mi_is_aligned(guard_page, os_page_size)) {
     const bool ok = _mi_os_protect(guard_page, os_page_size);
     if mi_unlikely(!ok) {
@@ -665,11 +612,9 @@ static void* mi_block_ptr_set_guarded(mi
     _mi_warning_message("unable to set a guard page behind an object due to pinned memory (large OS pages?) (object %p of size %zu)\n", block, block_size);
   }
 
-  // align pointer just in front of the guard page
   size_t offset = block_size - os_page_size - obj_size;
   mi_assert_internal(offset > sizeof(mi_block_t));
   if (offset > MI_BLOCK_ALIGNMENT_MAX) {
-    // give up to place it right in front of the guard page if the offset is too large for unalignment
     offset = MI_BLOCK_ALIGNMENT_MAX;
   }
   void* p = (uint8_t*)block + offset;
@@ -680,42 +625,39 @@ static void* mi_block_ptr_set_guarded(mi
 
 mi_decl_restrict void* _mi_heap_malloc_guarded(mi_heap_t* heap, size_t size, bool zero) mi_attr_noexcept
 {
-  #if defined(MI_PADDING_SIZE)
-  mi_assert(MI_PADDING_SIZE==0);
-  #endif
-  // allocate multiple of page size ending in a guard page
-  // ensure minimal alignment requirement?
+#if defined(MI_PADDING_SIZE)
+  mi_assert(MI_PADDING_SIZE == 0);
+#endif
   const size_t os_page_size = _mi_os_page_size();
   const size_t obj_size = (mi_option_is_enabled(mi_option_guarded_precise) ? size : _mi_align_up(size, MI_MAX_ALIGN_SIZE));
   const size_t bsize    = _mi_align_up(_mi_align_up(obj_size, MI_MAX_ALIGN_SIZE) + sizeof(mi_block_t), MI_MAX_ALIGN_SIZE);
   const size_t req_size = _mi_align_up(bsize + os_page_size, os_page_size);
-  mi_block_t* const block = (mi_block_t*)_mi_malloc_generic(heap, req_size, zero, 0 /* huge_alignment */, NULL);
-  if (block==NULL) return NULL;
-  void* const p   = mi_block_ptr_set_guarded(block, obj_size);
 
-  // stats
+  mi_block_t* const block = (mi_block_t*)_mi_malloc_generic(heap, req_size, zero, 0, NULL);
+  if mi_unlikely(block == NULL) return NULL;
+
+  void* const p = mi_block_ptr_set_guarded(block, obj_size);
+
   mi_track_malloc(p, size, zero);
-  if (p != NULL) {
-    if (!mi_heap_is_initialized(heap)) { heap = mi_prim_get_default_heap(); }
-    #if MI_STAT>1
+  if mi_likely(p != NULL) {
+    if (!mi_heap_is_initialized(heap)) {
+      heap = mi_prim_get_default_heap();
+    }
+#if MI_STAT>1
     mi_heap_stat_adjust_decrease(heap, malloc_requested, req_size);
     mi_heap_stat_increase(heap, malloc_requested, size);
-    #endif
+#endif
     _mi_stat_counter_increase(&heap->tld->stats.malloc_guarded_count, 1);
   }
-  #if MI_DEBUG>3
+#if MI_DEBUG>3
   if (p != NULL && zero) {
     mi_assert_expensive(mi_mem_is_zero(p, size));
   }
-  #endif
+#endif
   return p;
 }
 #endif
 
-// ------------------------------------------------------
-// ensure explicit external inline definitions are emitted!
-// ------------------------------------------------------
-
 #ifdef __cplusplus
 void* _mi_externs[] = {
   (void*)&_mi_page_malloc,
@@ -728,7 +670,5 @@ void* _mi_externs[] = {
   (void*)&mi_heap_malloc,
   (void*)&mi_heap_zalloc,
   (void*)&mi_heap_malloc_small,
-  // (void*)&mi_heap_alloc_new,
-  // (void*)&mi_heap_alloc_new_n
 };
 #endif


--- a/src/page.c	2026-01-23 12:49:10.316203953 +0100
+++ b/src/page.c	2026-01-31 13:28:05.339102905 +0100
@@ -5,30 +5,32 @@ terms of the MIT license. A copy of the
 "LICENSE" at the root of this distribution.
 -----------------------------------------------------------------------------*/
 
-/* -----------------------------------------------------------
-  The core of the allocator. Every segment contains
-  pages of a certain block size. The main function
-  exported is `mi_malloc_generic`.
------------------------------------------------------------ */
-
 #include "mimalloc.h"
 #include "mimalloc/internal.h"
 #include "mimalloc/atomic.h"
 
-/* -----------------------------------------------------------
-  Definition of page queues for each block size
------------------------------------------------------------ */
+#if defined(__GNUC__) || defined(__clang__)
+  #if defined(__x86_64__) || defined(__i386__)
+    #define mi_spin_pause() __builtin_ia32_pause()
+  #elif defined(__aarch64__)
+    #define mi_spin_pause() __asm__ volatile("yield" ::: "memory")
+  #else
+    #define mi_spin_pause() ((void)0)
+  #endif
+  #define mi_prefetch_r(addr)  __builtin_prefetch((addr), 0, 3)
+  #define mi_prefetch_w(addr)  __builtin_prefetch((addr), 1, 3)
+  #define mi_prefetch_r_l2(addr) __builtin_prefetch((addr), 0, 2)
+#else
+  #define mi_spin_pause() ((void)0)
+  #define mi_prefetch_r(addr) ((void)(addr))
+  #define mi_prefetch_w(addr) ((void)(addr))
+  #define mi_prefetch_r_l2(addr) ((void)(addr))
+#endif
 
 #define MI_IN_PAGE_C
 #include "page-queue.c"
 #undef MI_IN_PAGE_C
 
-
-/* -----------------------------------------------------------
-  Page helpers
------------------------------------------------------------ */
-
-// Index a block in a page
 static inline mi_block_t* mi_page_block_at(const mi_page_t* page, void* page_start, size_t block_size, size_t i) {
   MI_UNUSED(page);
   mi_assert_internal(page != NULL);
@@ -50,23 +52,16 @@ static size_t mi_page_list_count(mi_page
   return count;
 }
 
-/*
-// Start of the page available memory
-static inline uint8_t* mi_page_area(const mi_page_t* page) {
-  return _mi_page_start(_mi_page_segment(page), page, NULL);
-}
-*/
-
 static bool mi_page_list_is_valid(mi_page_t* page, mi_block_t* p) {
   size_t psize;
   uint8_t* page_area = _mi_segment_page_start(_mi_page_segment(page), page, &psize);
   mi_block_t* start = (mi_block_t*)page_area;
   mi_block_t* end   = (mi_block_t*)(page_area + psize);
-  while(p != NULL) {
+  while (p != NULL) {
     if (p < start || p >= end) return false;
     p = mi_block_next(page, p);
   }
-#if MI_DEBUG>3 // generally too expensive to check this
+#if MI_DEBUG>3
   if (page->free_is_zero) {
     const size_t ubsize = mi_page_usable_block_size(page);
     for (mi_block_t* block = page->free; block != NULL; block = mi_block_next(page, block)) {
@@ -85,26 +80,23 @@ static bool mi_page_is_valid_init(mi_pag
   uint8_t* start = mi_page_start(page);
   mi_assert_internal(start == _mi_segment_page_start(_mi_page_segment(page), page, NULL));
   mi_assert_internal(page->is_huge == (_mi_page_segment(page)->kind == MI_SEGMENT_HUGE));
-  //mi_assert_internal(start + page->capacity*page->block_size == page->top);
 
-  mi_assert_internal(mi_page_list_is_valid(page,page->free));
-  mi_assert_internal(mi_page_list_is_valid(page,page->local_free));
+  mi_assert_internal(mi_page_list_is_valid(page, page->free));
+  mi_assert_internal(mi_page_list_is_valid(page, page->local_free));
 
-  #if MI_DEBUG>3 // generally too expensive to check this
+#if MI_DEBUG>3
   if (page->free_is_zero) {
     const size_t ubsize = mi_page_usable_block_size(page);
-    for(mi_block_t* block = page->free; block != NULL; block = mi_block_next(page,block)) {
+    for (mi_block_t* block = page->free; block != NULL; block = mi_block_next(page, block)) {
       mi_assert_expensive(mi_mem_is_zero(block + 1, ubsize - sizeof(mi_block_t)));
     }
   }
-  #endif
+#endif
 
-  #if !MI_TRACK_ENABLED && !MI_TSAN
+#if !MI_TRACK_ENABLED && !MI_TSAN
   mi_block_t* tfree = mi_page_thread_free(page);
   mi_assert_internal(mi_page_list_is_valid(page, tfree));
-  //size_t tfree_count = mi_page_list_count(page, tfree);
-  //mi_assert_internal(tfree_count <= page->thread_freed + 1);
-  #endif
+#endif
 
   size_t free_count = mi_page_list_count(page, page->free) + mi_page_list_count(page, page->local_free);
   mi_assert_internal(page->used + free_count == page->capacity);
@@ -112,25 +104,25 @@ static bool mi_page_is_valid_init(mi_pag
   return true;
 }
 
-extern mi_decl_hidden bool _mi_process_is_initialized;             // has mi_process_init been called?
+extern mi_decl_hidden bool _mi_process_is_initialized;
 
 bool _mi_page_is_valid(mi_page_t* page) {
   mi_assert_internal(mi_page_is_valid_init(page));
-  #if MI_SECURE
+#if MI_SECURE
   mi_assert_internal(page->keys[0] != 0);
-  #endif
-  if (mi_page_heap(page)!=NULL) {
+#endif
+  if (mi_page_heap(page) != NULL) {
     mi_segment_t* segment = _mi_page_segment(page);
 
-    mi_assert_internal(!_mi_process_is_initialized || segment->thread_id==0 || segment->thread_id == mi_page_heap(page)->thread_id);
-    #if MI_HUGE_PAGE_ABANDON
+    mi_assert_internal(!_mi_process_is_initialized || segment->thread_id == 0 || segment->thread_id == mi_page_heap(page)->thread_id);
+#if MI_HUGE_PAGE_ABANDON
     if (segment->kind != MI_SEGMENT_HUGE)
-    #endif
+#endif
     {
       mi_page_queue_t* pq = mi_page_queue_of(page);
       mi_assert_internal(mi_page_queue_contains(pq, page));
-      mi_assert_internal(pq->block_size==mi_page_block_size(page) || mi_page_block_size(page) > MI_MEDIUM_OBJ_SIZE_MAX || mi_page_is_in_full(page));
-      mi_assert_internal(mi_heap_contains_queue(mi_page_heap(page),pq));
+      mi_assert_internal(pq->block_size == mi_page_block_size(page) || mi_page_block_size(page) > MI_MEDIUM_OBJ_SIZE_MAX || mi_page_is_in_full(page));
+      mi_assert_internal(mi_heap_contains_queue(mi_page_heap(page), pq));
     }
   }
   return true;
@@ -139,99 +131,100 @@ bool _mi_page_is_valid(mi_page_t* page)
 
 void _mi_page_use_delayed_free(mi_page_t* page, mi_delayed_t delay, bool override_never) {
   while (!_mi_page_try_use_delayed_free(page, delay, override_never)) {
+    mi_spin_pause();
     mi_atomic_yield();
   }
 }
 
 bool _mi_page_try_use_delayed_free(mi_page_t* page, mi_delayed_t delay, bool override_never) {
   mi_thread_free_t tfreex;
-  mi_delayed_t     old_delay;
+  mi_delayed_t old_delay;
   mi_thread_free_t tfree;
   size_t yield_count = 0;
+
   do {
-    tfree = mi_atomic_load_acquire(&page->xthread_free); // note: must acquire as we can break/repeat this loop and not do a CAS;
+    tfree = mi_atomic_load_acquire(&page->xthread_free);
     tfreex = mi_tf_set_delayed(tfree, delay);
     old_delay = mi_tf_delayed(tfree);
+
     if mi_unlikely(old_delay == MI_DELAYED_FREEING) {
-      if (yield_count >= 4) return false;  // give up after 4 tries
+      if (yield_count >= 4) return false;
       yield_count++;
-      mi_atomic_yield(); // delay until outstanding MI_DELAYED_FREEING are done.
-      // tfree = mi_tf_set_delayed(tfree, MI_NO_DELAYED_FREE); // will cause CAS to busy fail
+      mi_spin_pause();
+      mi_atomic_yield();
     }
     else if (delay == old_delay) {
-      break; // avoid atomic operation if already equal
+      break;
     }
     else if (!override_never && old_delay == MI_NEVER_DELAYED_FREE) {
-      break; // leave never-delayed flag set
+      break;
     }
   } while ((old_delay == MI_DELAYED_FREEING) ||
            !mi_atomic_cas_weak_release(&page->xthread_free, &tfree, tfreex));
 
-  return true; // success
+  return true;
 }
 
-/* -----------------------------------------------------------
-  Page collect the `local_free` and `thread_free` lists
------------------------------------------------------------ */
-
-// Collect the local `thread_free` list using an atomic exchange.
-// Note: The exchange must be done atomically as this is used right after
-// moving to the full list in `mi_page_collect_ex` and we need to
-// ensure that there was no race where the page became unfull just before the move.
-static void _mi_page_thread_free_collect(mi_page_t* page)
-{
+static void _mi_page_thread_free_collect(mi_page_t* page) {
   mi_block_t* head;
   mi_thread_free_t tfreex;
   mi_thread_free_t tfree = mi_atomic_load_relaxed(&page->xthread_free);
+  size_t spin_count = 0;
+
   do {
     head = mi_tf_block(tfree);
-    tfreex = mi_tf_set_block(tfree,NULL);
-  } while (!mi_atomic_cas_weak_acq_rel(&page->xthread_free, &tfree, tfreex));
+    tfreex = mi_tf_set_block(tfree, NULL);
+    if mi_unlikely(!mi_atomic_cas_weak_acq_rel(&page->xthread_free, &tfree, tfreex)) {
+      if (++spin_count <= 8) {
+        mi_spin_pause();
+      } else {
+        mi_atomic_yield();
+        spin_count = 0;
+      }
+      continue;
+    }
+    break;
+  } while (true);
 
-  // return if the list is empty
   if (head == NULL) return;
 
-  // find the tail -- also to get a proper count (without data races)
-  size_t max_count = page->capacity; // cannot collect more than capacity
+  mi_prefetch_r(head);
+
+  size_t max_count = page->capacity;
   size_t count = 1;
   mi_block_t* tail = head;
   mi_block_t* next;
-  while ((next = mi_block_next(page,tail)) != NULL && count <= max_count) {
+
+  while ((next = mi_block_next(page, tail)) != NULL && count <= max_count) {
+    mi_prefetch_r(next);
     count++;
     tail = next;
   }
-  // if `count > max_count` there was a memory corruption (possibly infinite list due to double multi-threaded free)
+
   if (count > max_count) {
     _mi_error_message(EFAULT, "corrupted thread-free list\n");
-    return; // the thread-free items cannot be freed
+    return;
   }
 
-  // and append the current local free list
-  mi_block_set_next(page,tail, page->local_free);
+  mi_block_set_next(page, tail, page->local_free);
   page->local_free = head;
-
-  // update counts now
   page->used -= (uint16_t)count;
 }
 
 void _mi_page_free_collect(mi_page_t* page, bool force) {
-  mi_assert_internal(page!=NULL);
+  mi_assert_internal(page != NULL);
 
-  // collect the thread free list
-  if (force || mi_page_thread_free(page) != NULL) {  // quick test to avoid an atomic operation
+  if (force || mi_page_thread_free(page) != NULL) {
     _mi_page_thread_free_collect(page);
   }
 
-  // and the local free list
   if (page->local_free != NULL) {
     if mi_likely(page->free == NULL) {
-      // usual case
       page->free = page->local_free;
       page->local_free = NULL;
       page->free_is_zero = false;
     }
     else if (force) {
-      // append -- only on shutdown (force) as this is a linear operation
       mi_block_t* tail = page->local_free;
       mi_block_t* next;
       while ((next = mi_block_next(page, tail)) != NULL) {
@@ -247,107 +240,112 @@ void _mi_page_free_collect(mi_page_t* pa
   mi_assert_internal(!force || page->local_free == NULL);
 }
 
-
-
-/* -----------------------------------------------------------
-  Page fresh and retire
------------------------------------------------------------ */
-
-// called from segments when reclaiming abandoned pages
 void _mi_page_reclaim(mi_heap_t* heap, mi_page_t* page) {
   mi_assert_expensive(mi_page_is_valid_init(page));
-
   mi_assert_internal(mi_page_heap(page) == heap);
   mi_assert_internal(mi_page_thread_free_flag(page) != MI_NEVER_DELAYED_FREE);
-  #if MI_HUGE_PAGE_ABANDON
+#if MI_HUGE_PAGE_ABANDON
   mi_assert_internal(_mi_page_segment(page)->kind != MI_SEGMENT_HUGE);
-  #endif
+#endif
 
-  // TODO: push on full queue immediately if it is full?
   mi_page_queue_t* pq = mi_page_queue(heap, mi_page_block_size(page));
   mi_page_queue_push(heap, pq, page);
   mi_assert_expensive(_mi_page_is_valid(page));
 }
 
-// allocate a fresh page from a segment
 static mi_page_t* mi_page_fresh_alloc(mi_heap_t* heap, mi_page_queue_t* pq, size_t block_size, size_t page_alignment) {
-  #if !MI_HUGE_PAGE_ABANDON
+#if !MI_HUGE_PAGE_ABANDON
   mi_assert_internal(pq != NULL);
   mi_assert_internal(mi_heap_contains_queue(heap, pq));
   mi_assert_internal(page_alignment > 0 || block_size > MI_MEDIUM_OBJ_SIZE_MAX || block_size == pq->block_size);
-  #endif
+#endif
   mi_page_t* page = _mi_segment_page_alloc(heap, block_size, page_alignment, &heap->tld->segments);
-  if (page == NULL) {
-    // this may be out-of-memory, or an abandoned page was reclaimed (and in our queue)
+  if mi_unlikely(page == NULL) {
     return NULL;
   }
-  #if MI_HUGE_PAGE_ABANDON
-  mi_assert_internal(pq==NULL || _mi_page_segment(page)->page_kind != MI_PAGE_HUGE);
-  #endif
-  mi_assert_internal(page_alignment >0 || block_size > MI_MEDIUM_OBJ_SIZE_MAX || _mi_page_segment(page)->kind != MI_SEGMENT_HUGE);
-  mi_assert_internal(pq!=NULL || mi_page_block_size(page) >= block_size);
-  // a fresh page was found, initialize it
-  const size_t full_block_size = (pq == NULL || mi_page_is_huge(page) ? mi_page_block_size(page) : block_size); // see also: mi_segment_huge_page_alloc
+#if MI_HUGE_PAGE_ABANDON
+  mi_assert_internal(pq == NULL || _mi_page_segment(page)->page_kind != MI_PAGE_HUGE);
+#endif
+  mi_assert_internal(page_alignment > 0 || block_size > MI_MEDIUM_OBJ_SIZE_MAX || _mi_page_segment(page)->kind != MI_SEGMENT_HUGE);
+  mi_assert_internal(pq != NULL || mi_page_block_size(page) >= block_size);
+
+  const size_t full_block_size = (pq == NULL || mi_page_is_huge(page) ? mi_page_block_size(page) : block_size);
   mi_assert_internal(full_block_size >= block_size);
   mi_page_init(heap, page, full_block_size, heap->tld);
   mi_heap_stat_increase(heap, pages, 1);
   mi_heap_stat_increase(heap, page_bins[_mi_page_stats_bin(page)], 1);
-  if (pq != NULL) { mi_page_queue_push(heap, pq, page); }
+  if (pq != NULL) {
+    mi_page_queue_push(heap, pq, page);
+  }
   mi_assert_expensive(_mi_page_is_valid(page));
   return page;
 }
 
-// Get a fresh page to use
 static mi_page_t* mi_page_fresh(mi_heap_t* heap, mi_page_queue_t* pq) {
   mi_assert_internal(mi_heap_contains_queue(heap, pq));
   mi_page_t* page = mi_page_fresh_alloc(heap, pq, pq->block_size, 0);
-  if (page==NULL) return NULL;
-  mi_assert_internal(pq->block_size==mi_page_block_size(page));
-  mi_assert_internal(pq==mi_page_queue(heap, mi_page_block_size(page)));
+  if mi_unlikely(page == NULL) return NULL;
+  mi_assert_internal(pq->block_size == mi_page_block_size(page));
+  mi_assert_internal(pq == mi_page_queue(heap, mi_page_block_size(page)));
   return page;
 }
 
-/* -----------------------------------------------------------
-   Do any delayed frees
-   (put there by other threads if they deallocated in a full page)
------------------------------------------------------------ */
 void _mi_heap_delayed_free_all(mi_heap_t* heap) {
   while (!_mi_heap_delayed_free_partial(heap)) {
+    mi_spin_pause();
     mi_atomic_yield();
   }
 }
 
-// returns true if all delayed frees were processed
 bool _mi_heap_delayed_free_partial(mi_heap_t* heap) {
-  // take over the list (note: no atomic exchange since it is often NULL)
   mi_block_t* block = mi_atomic_load_ptr_relaxed(mi_block_t, &heap->thread_delayed_free);
-  while (block != NULL && !mi_atomic_cas_ptr_weak_acq_rel(mi_block_t, &heap->thread_delayed_free, &block, NULL)) { /* nothing */ };
+
+  if mi_likely(block == NULL) {
+    return true;
+  }
+
+  size_t spin_count = 0;
+  while (!mi_atomic_cas_ptr_weak_acq_rel(mi_block_t, &heap->thread_delayed_free, &block, NULL)) {
+    if (block == NULL) {
+      return true;
+    }
+    if (++spin_count <= 8) {
+      mi_spin_pause();
+    } else {
+      mi_atomic_yield();
+      spin_count = 0;
+    }
+  }
+
   bool all_freed = true;
 
-  // and free them all
-  while(block != NULL) {
-    mi_block_t* next = mi_block_nextx(heap,block, heap->keys);
-    // use internal free instead of regular one to keep stats etc correct
-    if (!_mi_free_delayed_block(block)) {
-      // we might already start delayed freeing while another thread has not yet
-      // reset the delayed_freeing flag; in that case delay it further by reinserting the current block
-      // into the delayed free list
+  mi_block_t* prefetch_block = block;
+  for (int i = 0; i < 4 && prefetch_block != NULL; i++) {
+    mi_prefetch_r(prefetch_block);
+    prefetch_block = mi_block_nextx(heap, prefetch_block, heap->keys);
+  }
+
+  while (block != NULL) {
+    mi_block_t* next = mi_block_nextx(heap, block, heap->keys);
+
+    if mi_likely(next != NULL) {
+      mi_prefetch_r(next);
+    }
+
+    if mi_likely(_mi_free_delayed_block(block)) {
+    }
+    else {
       all_freed = false;
       mi_block_t* dfree = mi_atomic_load_ptr_relaxed(mi_block_t, &heap->thread_delayed_free);
       do {
         mi_block_set_nextx(heap, block, dfree, heap->keys);
-      } while (!mi_atomic_cas_ptr_weak_release(mi_block_t,&heap->thread_delayed_free, &dfree, block));
+      } while (!mi_atomic_cas_ptr_weak_release(mi_block_t, &heap->thread_delayed_free, &dfree, block));
     }
     block = next;
   }
   return all_freed;
 }
 
-/* -----------------------------------------------------------
-  Unfull, abandon, free and retire
------------------------------------------------------------ */
-
-// Move a page from the full list back to a regular list
 void _mi_page_unfull(mi_page_t* page) {
   mi_assert_internal(page != NULL);
   mi_assert_expensive(_mi_page_is_valid(page));
@@ -356,7 +354,7 @@ void _mi_page_unfull(mi_page_t* page) {
 
   mi_heap_t* heap = mi_page_heap(page);
   mi_page_queue_t* pqfull = &heap->pages[MI_BIN_FULL];
-  mi_page_set_in_full(page, false); // to get the right queue
+  mi_page_set_in_full(page, false);
   mi_page_queue_t* pq = mi_heap_page_queue_of(heap, page);
   mi_page_set_in_full(page, true);
   mi_page_queue_enqueue_from_full(pq, pqfull, page);
@@ -369,14 +367,9 @@ static void mi_page_to_full(mi_page_t* p
 
   if (mi_page_is_in_full(page)) return;
   mi_page_queue_enqueue_from(&mi_page_heap(page)->pages[MI_BIN_FULL], pq, page);
-  _mi_page_free_collect(page,false);  // try to collect right away in case another thread freed just before MI_USE_DELAYED_FREE was set
+  _mi_page_free_collect(page, false);
 }
 
-
-// Abandon a page with used blocks at the end of a thread.
-// Note: only call if it is ensured that no references exist from
-// the `page->heap->thread_delayed_free` into this page.
-// Currently only called through `mi_heap_collect_ex` which ensures this.
 void _mi_page_abandon(mi_page_t* page, mi_page_queue_t* pq) {
   mi_assert_internal(page != NULL);
   mi_assert_expensive(_mi_page_is_valid(page));
@@ -385,39 +378,29 @@ void _mi_page_abandon(mi_page_t* page, m
 
   mi_heap_t* pheap = mi_page_heap(page);
 
-  // remove from our page list
   mi_segments_tld_t* segments_tld = &pheap->tld->segments;
   mi_page_queue_remove(pq, page);
 
-  // page is no longer associated with our heap
-  mi_assert_internal(mi_page_thread_free_flag(page)==MI_NEVER_DELAYED_FREE);
+  mi_assert_internal(mi_page_thread_free_flag(page) == MI_NEVER_DELAYED_FREE);
   mi_page_set_heap(page, NULL);
 
 #if (MI_DEBUG>1) && !MI_TRACK_ENABLED
-  // check there are no references left..
   for (mi_block_t* block = (mi_block_t*)pheap->thread_delayed_free; block != NULL; block = mi_block_nextx(pheap, block, pheap->keys)) {
     mi_assert_internal(_mi_ptr_page(block) != page);
   }
 #endif
 
-  // and abandon it
   mi_assert_internal(mi_page_heap(page) == NULL);
-  _mi_segment_page_abandon(page,segments_tld);
+  _mi_segment_page_abandon(page, segments_tld);
 }
 
-// force abandon a page
 void _mi_page_force_abandon(mi_page_t* page) {
   mi_heap_t* heap = mi_page_heap(page);
-  // mark page as not using delayed free
   _mi_page_use_delayed_free(page, MI_NEVER_DELAYED_FREE, false);
-
-  // ensure this page is no longer in the heap delayed free list
   _mi_heap_delayed_free_all(heap);
-  // We can still access the page meta-info even if it is freed as we ensure
-  // in `mi_segment_force_abandon` that the segment is not freed (yet)
-  if (page->capacity == 0) return; // it may have been freed now
 
-  // and now unlink it from the page queue and abandon (or free)
+  if (page->capacity == 0) return;
+
   mi_page_queue_t* pq = mi_heap_page_queue_of(heap, page);
   if (mi_page_all_free(page)) {
     _mi_page_free(page, pq, false);
@@ -427,38 +410,26 @@ void _mi_page_force_abandon(mi_page_t* p
   }
 }
 
-
-// Free a page with no more free blocks
 void _mi_page_free(mi_page_t* page, mi_page_queue_t* pq, bool force) {
   mi_assert_internal(page != NULL);
   mi_assert_expensive(_mi_page_is_valid(page));
   mi_assert_internal(pq == mi_page_queue_of(page));
   mi_assert_internal(mi_page_all_free(page));
-  mi_assert_internal(mi_page_thread_free_flag(page)!=MI_DELAYED_FREEING);
+  mi_assert_internal(mi_page_thread_free_flag(page) != MI_DELAYED_FREEING);
 
-  // no more aligned blocks in here
   mi_page_set_has_aligned(page, false);
 
-  // remove from the page list
-  // (no need to do _mi_heap_delayed_free first as all blocks are already free)
   mi_heap_t* heap = mi_page_heap(page);
   mi_segments_tld_t* segments_tld = &heap->tld->segments;
   mi_page_queue_remove(pq, page);
 
-  // and free it  
-  mi_page_set_heap(page,NULL);
+  mi_page_set_heap(page, NULL);
   _mi_segment_page_free(page, force, segments_tld);
 }
 
-#define MI_MAX_RETIRE_SIZE    MI_MEDIUM_OBJ_SIZE_MAX   // should be less than size for MI_BIN_HUGE
+#define MI_MAX_RETIRE_SIZE    MI_MEDIUM_OBJ_SIZE_MAX
 #define MI_RETIRE_CYCLES      (16)
 
-// Retire a page with no more used blocks
-// Important to not retire too quickly though as new
-// allocations might coming.
-// Note: called from `mi_free` and benchmarks often
-// trigger this due to freeing everything and then
-// allocating again so careful when changing this.
 void _mi_page_retire(mi_page_t* page) mi_attr_noexcept {
   mi_assert_internal(page != NULL);
   mi_assert_expensive(_mi_page_is_valid(page));
@@ -466,41 +437,33 @@ void _mi_page_retire(mi_page_t* page) mi
 
   mi_page_set_has_aligned(page, false);
 
-  // don't retire too often..
-  // (or we end up retiring and re-allocating most of the time)
-  // NOTE: refine this more: we should not retire if this
-  // is the only page left with free blocks. It is not clear
-  // how to check this efficiently though...
-  // for now, we don't retire if it is the only page left of this size class.
   mi_page_queue_t* pq = mi_page_queue_of(page);
-  #if MI_RETIRE_CYCLES > 0
+#if MI_RETIRE_CYCLES > 0
   const size_t bsize = mi_page_block_size(page);
-  if mi_likely( /* bsize < MI_MAX_RETIRE_SIZE && */ !mi_page_queue_is_special(pq)) {  // not full or huge queue?
-    if (pq->last==page && pq->first==page) { // the only page in the queue?
-      mi_stat_counter_increase(_mi_stats_main.pages_retire,1);
-      page->retire_expire = (bsize <= MI_SMALL_OBJ_SIZE_MAX ? MI_RETIRE_CYCLES : MI_RETIRE_CYCLES/4);
+  if mi_likely(!mi_page_queue_is_special(pq)) {
+    if mi_unlikely(pq->last == page && pq->first == page) {
+      mi_stat_counter_increase(_mi_stats_main.pages_retire, 1);
+      page->retire_expire = (bsize <= MI_SMALL_OBJ_SIZE_MAX ? MI_RETIRE_CYCLES : MI_RETIRE_CYCLES / 4);
       mi_heap_t* heap = mi_page_heap(page);
       mi_assert_internal(pq >= heap->pages);
-      const size_t index = pq - heap->pages;
+      const size_t index = (size_t)(pq - heap->pages);
       mi_assert_internal(index < MI_BIN_FULL && index < MI_BIN_HUGE);
       if (index < heap->page_retired_min) heap->page_retired_min = index;
       if (index > heap->page_retired_max) heap->page_retired_max = index;
       mi_assert_internal(mi_page_all_free(page));
-      return; // don't free after all
+      return;
     }
   }
-  #endif
+#endif
   _mi_page_free(page, pq, false);
 }
 
-// free retired pages: we don't need to look at the entire queues
-// since we only retire pages that are at the head position in a queue.
 void _mi_heap_collect_retired(mi_heap_t* heap, bool force) {
   size_t min = MI_BIN_FULL;
   size_t max = 0;
-  for(size_t bin = heap->page_retired_min; bin <= heap->page_retired_max; bin++) {
-    mi_page_queue_t* pq   = &heap->pages[bin];
-    mi_page_t*       page = pq->first;
+  for (size_t bin = heap->page_retired_min; bin <= heap->page_retired_max; bin++) {
+    mi_page_queue_t* pq = &heap->pages[bin];
+    mi_page_t* page = pq->first;
     if (page != NULL && page->retire_expire != 0) {
       if (mi_page_all_free(page)) {
         page->retire_expire--;
@@ -508,7 +471,6 @@ void _mi_heap_collect_retired(mi_heap_t*
           _mi_page_free(pq->first, pq, force);
         }
         else {
-          // keep retired, update min/max
           if (bin < min) min = bin;
           if (bin > max) max = bin;
         }
@@ -522,29 +484,20 @@ void _mi_heap_collect_retired(mi_heap_t*
   heap->page_retired_max = max;
 }
 
-
-/* -----------------------------------------------------------
-  Initialize the initial free list in a page.
-  In secure mode we initialize a randomized list by
-  alternating between slices.
------------------------------------------------------------ */
-
-#define MI_MAX_SLICE_SHIFT  (6)   // at most 64 slices
+#define MI_MAX_SLICE_SHIFT  (6)
 #define MI_MAX_SLICES       (1UL << MI_MAX_SLICE_SHIFT)
 #define MI_MIN_SLICES       (2)
 
 static void mi_page_free_list_extend_secure(mi_heap_t* const heap, mi_page_t* const page, const size_t bsize, const size_t extend, mi_stats_t* const stats) {
   MI_UNUSED(stats);
-  #if (MI_SECURE<=2)
+#if (MI_SECURE<=2)
   mi_assert_internal(page->free == NULL);
   mi_assert_internal(page->local_free == NULL);
-  #endif
+#endif
   mi_assert_internal(page->capacity + extend <= page->reserved);
   mi_assert_internal(bsize == mi_page_block_size(page));
   void* const page_area = mi_page_start(page);
 
-  // initialize a randomized free list
-  // set up `slice_count` slices to alternate between
   size_t shift = MI_MAX_SLICE_SHIFT;
   while ((extend >> shift) == 0) {
     shift--;
@@ -552,158 +505,167 @@ static void mi_page_free_list_extend_sec
   const size_t slice_count = (size_t)1U << shift;
   const size_t slice_extend = extend / slice_count;
   mi_assert_internal(slice_extend >= 1);
-  mi_block_t* blocks[MI_MAX_SLICES];   // current start of the slice
-  size_t      counts[MI_MAX_SLICES];   // available objects in the slice
+  mi_block_t* blocks[MI_MAX_SLICES];
+  size_t counts[MI_MAX_SLICES];
+
   for (size_t i = 0; i < slice_count; i++) {
-    blocks[i] = mi_page_block_at(page, page_area, bsize, page->capacity + i*slice_extend);
+    blocks[i] = mi_page_block_at(page, page_area, bsize, page->capacity + i * slice_extend);
     counts[i] = slice_extend;
   }
-  counts[slice_count-1] += (extend % slice_count);  // final slice holds the modulus too (todo: distribute evenly?)
+  counts[slice_count - 1] += (extend % slice_count);
 
-  // and initialize the free list by randomly threading through them
-  // set up first element
   const uintptr_t r = _mi_heap_random_next(heap);
   size_t current = r % slice_count;
   counts[current]--;
   mi_block_t* const free_start = blocks[current];
-  // and iterate through the rest; use `random_shuffle` for performance
-  uintptr_t rnd = _mi_random_shuffle(r|1); // ensure not 0
+
+  uintptr_t rnd = _mi_random_shuffle(r | 1);
   for (size_t i = 1; i < extend; i++) {
-    // call random_shuffle only every INTPTR_SIZE rounds
-    const size_t round = i%MI_INTPTR_SIZE;
+    const size_t round = i % MI_INTPTR_SIZE;
     if (round == 0) rnd = _mi_random_shuffle(rnd);
-    // select a random next slice index
-    size_t next = ((rnd >> 8*round) & (slice_count-1));
-    while (counts[next]==0) {                            // ensure it still has space
+    size_t next = ((rnd >> 8 * round) & (slice_count - 1));
+    while (counts[next] == 0) {
       next++;
-      if (next==slice_count) next = 0;
+      if (next == slice_count) next = 0;
     }
-    // and link the current block to it
     counts[next]--;
     mi_block_t* const block = blocks[current];
-    blocks[current] = (mi_block_t*)((uint8_t*)block + bsize);  // bump to the following block
-    mi_block_set_next(page, block, blocks[next]);   // and set next; note: we may have `current == next`
+    blocks[current] = (mi_block_t*)((uint8_t*)block + bsize);
+    mi_block_set_next(page, block, blocks[next]);
     current = next;
   }
-  // prepend to the free list (usually NULL)
-  mi_block_set_next(page, blocks[current], page->free);  // end of the list
+
+  mi_block_set_next(page, blocks[current], page->free);
   page->free = free_start;
 }
 
-static mi_decl_noinline void mi_page_free_list_extend( mi_page_t* const page, const size_t bsize, const size_t extend, mi_stats_t* const stats)
-{
+static mi_decl_noinline void mi_page_free_list_extend(mi_page_t* const page, const size_t bsize, const size_t extend, mi_stats_t* const stats) {
   MI_UNUSED(stats);
-  #if (MI_SECURE <= 2)
+#if (MI_SECURE <= 2)
   mi_assert_internal(page->free == NULL);
   mi_assert_internal(page->local_free == NULL);
-  #endif
+#endif
   mi_assert_internal(page->capacity + extend <= page->reserved);
   mi_assert_internal(bsize == mi_page_block_size(page));
-  void* const page_area = mi_page_start(page);
+  mi_assert_internal(extend >= 1);
 
+  void* const page_area = mi_page_start(page);
   mi_block_t* const start = mi_page_block_at(page, page_area, bsize, page->capacity);
-
-  // initialize a sequential free list
   mi_block_t* const last = mi_page_block_at(page, page_area, bsize, page->capacity + extend - 1);
   mi_block_t* block = start;
-  while(block <= last) {
+
+  size_t remaining = extend;
+
+  while (remaining >= 4) {
+    if (remaining > 4) {
+      mi_prefetch_w((uint8_t*)block + 4 * bsize);
+    }
+
+    mi_block_t* const b0 = block;
+    mi_block_t* const b1 = (mi_block_t*)((uint8_t*)b0 + bsize);
+    mi_block_t* const b2 = (mi_block_t*)((uint8_t*)b1 + bsize);
+    mi_block_t* const b3 = (mi_block_t*)((uint8_t*)b2 + bsize);
+    mi_block_t* const next_block = (mi_block_t*)((uint8_t*)b3 + bsize);
+
+    mi_block_set_next(page, b0, b1);
+    mi_block_set_next(page, b1, b2);
+    mi_block_set_next(page, b2, b3);
+    mi_block_set_next(page, b3, next_block);
+
+    block = next_block;
+    remaining -= 4;
+  }
+
+  while (block <= last) {
     mi_block_t* next = (mi_block_t*)((uint8_t*)block + bsize);
-    mi_block_set_next(page,block,next);
+    mi_block_set_next(page, block, next);
     block = next;
   }
-  // prepend to free list (usually `NULL`)
+
   mi_block_set_next(page, last, page->free);
   page->free = start;
 }
 
-/* -----------------------------------------------------------
-  Page initialize and extend the capacity
------------------------------------------------------------ */
-
-#define MI_MAX_EXTEND_SIZE    (4*1024)      // heuristic, one OS page seems to work well.
+#define MI_MAX_EXTEND_SIZE    (4*1024)
 #if (MI_SECURE>0)
-#define MI_MIN_EXTEND         (8*MI_SECURE) // extend at least by this many
+#define MI_MIN_EXTEND         (8*MI_SECURE)
 #else
 #define MI_MIN_EXTEND         (4)
 #endif
 
-// Extend the capacity (up to reserved) by initializing a free list
-// We do at most `MI_MAX_EXTEND` to avoid touching too much memory
-// Note: we also experimented with "bump" allocation on the first
-// allocations but this did not speed up any benchmark (due to an
-// extra test in malloc? or cache effects?)
 static bool mi_page_extend_free(mi_heap_t* heap, mi_page_t* page, mi_tld_t* tld) {
   mi_assert_expensive(mi_page_is_valid_init(page));
-  #if (MI_SECURE<=2)
+#if (MI_SECURE<=2)
   mi_assert(page->free == NULL);
   mi_assert(page->local_free == NULL);
   if (page->free != NULL) return true;
-  #endif
+#endif
   if (page->capacity >= page->reserved) return true;
 
   mi_stat_counter_increase(tld->stats.pages_extended, 1);
 
-  // calculate the extend count
   const size_t bsize = mi_page_block_size(page);
   size_t extend = page->reserved - page->capacity;
   mi_assert_internal(extend > 0);
 
-  size_t max_extend = (bsize >= MI_MAX_EXTEND_SIZE ? MI_MIN_EXTEND : MI_MAX_EXTEND_SIZE/bsize);
-  if (max_extend < MI_MIN_EXTEND) { max_extend = MI_MIN_EXTEND; }
+  size_t max_extend = (bsize >= MI_MAX_EXTEND_SIZE ? MI_MIN_EXTEND : MI_MAX_EXTEND_SIZE / bsize);
+  if (max_extend < MI_MIN_EXTEND) {
+    max_extend = MI_MIN_EXTEND;
+  }
   mi_assert_internal(max_extend > 0);
 
   if (extend > max_extend) {
-    // ensure we don't touch memory beyond the page to reduce page commit.
-    // the `lean` benchmark tests this. Going from 1 to 8 increases rss by 50%.
     extend = max_extend;
   }
 
   mi_assert_internal(extend > 0 && extend + page->capacity <= page->reserved);
-  mi_assert_internal(extend < (1UL<<16));
+  mi_assert_internal(extend < (1UL << 16));
 
-  // and append the extend the free list
-  if (extend < MI_MIN_SLICES || MI_SECURE==0) { //!mi_option_is_enabled(mi_option_secure)) {
-    mi_page_free_list_extend(page, bsize, extend, &tld->stats );
+  if (extend < MI_MIN_SLICES || MI_SECURE == 0) {
+    mi_page_free_list_extend(page, bsize, extend, &tld->stats);
   }
   else {
     mi_page_free_list_extend_secure(heap, page, bsize, extend, &tld->stats);
   }
-  // enable the new free list
+
   page->capacity += (uint16_t)extend;
   mi_stat_increase(tld->stats.page_committed, extend * bsize);
   mi_assert_expensive(mi_page_is_valid_init(page));
   return true;
 }
 
-// Initialize a fresh page
 static void mi_page_init(mi_heap_t* heap, mi_page_t* page, size_t block_size, mi_tld_t* tld) {
   mi_assert(page != NULL);
   mi_segment_t* segment = _mi_page_segment(page);
   mi_assert(segment != NULL);
   mi_assert_internal(block_size > 0);
-  // set fields
+
   mi_page_set_heap(page, heap);
   page->block_size = block_size;
+
   size_t page_size;
   page->page_start = _mi_segment_page_start(segment, page, &page_size);
-  mi_track_mem_noaccess(page->page_start,page_size);
+  mi_track_mem_noaccess(page->page_start, page_size);
   mi_assert_internal(mi_page_block_size(page) <= page_size);
-  mi_assert_internal(page_size <= page->slice_count*MI_SEGMENT_SLICE_SIZE);
-  mi_assert_internal(page_size / block_size < (1L<<16));
+  mi_assert_internal(page_size <= page->slice_count * MI_SEGMENT_SLICE_SIZE);
+  mi_assert_internal(page_size / block_size < (1L << 16));
   page->reserved = (uint16_t)(page_size / block_size);
   mi_assert_internal(page->reserved > 0);
-  #if (MI_PADDING || MI_ENCODE_FREELIST)
+
+#if (MI_PADDING || MI_ENCODE_FREELIST)
   page->keys[0] = _mi_heap_random_next(heap);
   page->keys[1] = _mi_heap_random_next(heap);
-  #endif
+#endif
   page->free_is_zero = page->is_zero_init;
-  #if MI_DEBUG>2
+
+#if MI_DEBUG>2
   if (page->is_zero_init) {
     mi_track_mem_defined(page->page_start, page_size);
     mi_assert_expensive(mi_mem_is_zero(page->page_start, page_size));
   }
-  #endif
+#endif
   mi_assert_internal(page->is_committed);
+
   if (block_size > 0 && _mi_is_power_of_two(block_size)) {
     page->block_size_shift = (uint8_t)(mi_ctz((uintptr_t)block_size));
   }
@@ -719,236 +681,188 @@ static void mi_page_init(mi_heap_t* heap
   mi_assert_internal(page->prev == NULL);
   mi_assert_internal(page->retire_expire == 0);
   mi_assert_internal(!mi_page_has_aligned(page));
-  #if (MI_PADDING || MI_ENCODE_FREELIST)
+#if (MI_PADDING || MI_ENCODE_FREELIST)
   mi_assert_internal(page->keys[0] != 0);
   mi_assert_internal(page->keys[1] != 0);
-  #endif
+#endif
   mi_assert_internal(page->block_size_shift == 0 || (block_size == ((size_t)1 << page->block_size_shift)));
   mi_assert_expensive(mi_page_is_valid_init(page));
 
-  // initialize an initial free list
-  if (mi_page_extend_free(heap,page,tld)) {
+  if (mi_page_extend_free(heap, page, tld)) {
     mi_assert(mi_page_immediate_available(page));
   }
   return;
 }
 
-
-/* -----------------------------------------------------------
-  Find pages with free blocks
--------------------------------------------------------------*/
-
-// search for a best next page to use for at most N pages (often cut short if immediate blocks are available)
 #define MI_MAX_CANDIDATE_SEARCH  (4)
 
-// is the page not yet used up to its reserved space?
 static bool mi_page_is_expandable(const mi_page_t* page) {
   mi_assert_internal(page != NULL);
   mi_assert_internal(page->capacity <= page->reserved);
   return (page->capacity < page->reserved);
 }
 
-
-// Find a page with free blocks of `page->block_size`.
-static mi_page_t* mi_page_queue_find_free_ex(mi_heap_t* heap, mi_page_queue_t* pq, bool first_try)
-{
-  // search through the pages in "next fit" order
-  #if MI_STAT
+static mi_page_t* mi_page_queue_find_free_ex(mi_heap_t* heap, mi_page_queue_t* pq, bool first_try) {
+#if MI_STAT
   size_t count = 0;
-  #endif
-  size_t candidate_count = 0;        // we reset this on the first candidate to limit the search
-  mi_page_t* page_candidate = NULL;  // a page with free space
+#endif
+  size_t candidate_count = 0;
+  mi_page_t* page_candidate = NULL;
   mi_page_t* page = pq->first;
 
-  while (page != NULL)
-  {
-    mi_page_t* next = page->next; // remember next
-    #if MI_STAT
+  while (page != NULL) {
+    mi_page_t* next = page->next;
+
+    if mi_likely(next != NULL) {
+      mi_prefetch_r_l2(next);
+    }
+
+#if MI_STAT
     count++;
-    #endif
+#endif
     candidate_count++;
 
-    // collect freed blocks by us and other threads
     _mi_page_free_collect(page, false);
 
-  #if MI_MAX_CANDIDATE_SEARCH > 1
-    // search up to N pages for a best candidate
-
-    // is the local free list non-empty?
+#if MI_MAX_CANDIDATE_SEARCH > 1
     const bool immediate_available = mi_page_immediate_available(page);
 
-    // if the page is completely full, move it to the `mi_pages_full`
-    // queue so we don't visit long-lived pages too often.
-    if (!immediate_available && !mi_page_is_expandable(page)) {
+    if mi_unlikely(!immediate_available && !mi_page_is_expandable(page)) {
       mi_assert_internal(!mi_page_is_in_full(page) && !mi_page_immediate_available(page));
       mi_page_to_full(page, pq);
     }
     else {
-      // the page has free space, make it a candidate
-      // we prefer non-expandable pages with high usage as candidates (to reduce commit, and increase chances of free-ing up pages)
       if (page_candidate == NULL) {
         page_candidate = page;
         candidate_count = 0;
       }
-      // prefer to reuse fuller pages (in the hope the less used page gets freed)
       else if (page->used >= page_candidate->used && !mi_page_is_mostly_used(page) && !mi_page_is_expandable(page)) {
         page_candidate = page;
       }
-      // if we find a non-expandable candidate, or searched for N pages, return with the best candidate
-      if (immediate_available || candidate_count > MI_MAX_CANDIDATE_SEARCH) {
-        mi_assert_internal(page_candidate!=NULL);
+
+      if mi_likely(immediate_available || candidate_count > MI_MAX_CANDIDATE_SEARCH) {
+        mi_assert_internal(page_candidate != NULL);
         break;
       }
     }
-  #else
-    // first-fit algorithm
-    // If the page contains free blocks, we are done
-    if (mi_page_immediate_available(page) || mi_page_is_expandable(page)) {
-      break;  // pick this one
+#else
+    if mi_likely(mi_page_immediate_available(page) || mi_page_is_expandable(page)) {
+      break;
     }
 
-    // If the page is completely full, move it to the `mi_pages_full`
-    // queue so we don't visit long-lived pages too often.
     mi_assert_internal(!mi_page_is_in_full(page) && !mi_page_immediate_available(page));
     mi_page_to_full(page, pq);
-  #endif
+#endif
 
     page = next;
-  } // for each page
+  }
 
   mi_heap_stat_counter_increase(heap, page_searches, count);
   mi_heap_stat_counter_increase(heap, page_searches_count, 1);
 
-  // set the page to the best candidate
   if (page_candidate != NULL) {
     page = page_candidate;
   }
-  if (page != NULL) {
-    if (!mi_page_immediate_available(page)) {
+
+  if mi_likely(page != NULL) {
+    if mi_unlikely(!mi_page_immediate_available(page)) {
       mi_assert_internal(mi_page_is_expandable(page));
       if (!mi_page_extend_free(heap, page, heap->tld)) {
-        page = NULL; // failed to extend
+        page = NULL;
       }
     }
     mi_assert_internal(page == NULL || mi_page_immediate_available(page));
   }
 
-  if (page == NULL) {
-    _mi_heap_collect_retired(heap, false); // perhaps make a page available?
+  if mi_unlikely(page == NULL) {
+    _mi_heap_collect_retired(heap, false);
     page = mi_page_fresh(heap, pq);
     if (page == NULL && first_try) {
-      // out-of-memory _or_ an abandoned page with free blocks was reclaimed, try once again
       page = mi_page_queue_find_free_ex(heap, pq, false);
     }
   }
   else {
-    // move the page to the front of the queue
     mi_page_queue_move_to_front(heap, pq, page);
     page->retire_expire = 0;
-    // _mi_heap_collect_retired(heap, false); // update retire counts; note: increases rss on MemoryLoad bench so don't do this
   }
-  mi_assert_internal(page == NULL || mi_page_immediate_available(page));
 
+  mi_assert_internal(page == NULL || mi_page_immediate_available(page));
 
   return page;
 }
 
-
-
-// Find a page with free blocks of `size`.
 static inline mi_page_t* mi_find_free_page(mi_heap_t* heap, size_t size) {
   mi_page_queue_t* pq = mi_page_queue(heap, size);
 
-  // check the first page: we even do this with candidate search or otherwise we re-search every time
   mi_page_t* page = pq->first;
-  if (page != NULL) {
-   #if (MI_SECURE>=3) // in secure mode, we extend half the time to increase randomness
+  if mi_likely(page != NULL) {
+#if (MI_SECURE>=3)
     if (page->capacity < page->reserved && ((_mi_heap_random_next(heap) & 1) == 1)) {
       mi_page_extend_free(heap, page, heap->tld);
       mi_assert_internal(mi_page_immediate_available(page));
     }
     else
-   #endif
+#endif
     {
-      _mi_page_free_collect(page,false);
+      _mi_page_free_collect(page, false);
     }
 
-    if (mi_page_immediate_available(page)) {
+    if mi_likely(mi_page_immediate_available(page)) {
       page->retire_expire = 0;
-      return page; // fast path
+      return page;
     }
   }
 
   return mi_page_queue_find_free_ex(heap, pq, true);
 }
 
-
-/* -----------------------------------------------------------
-  Users can register a deferred free function called
-  when the `free` list is empty. Since the `local_free`
-  is separate this is deterministically called after
-  a certain number of allocations.
------------------------------------------------------------ */
-
 static mi_deferred_free_fun* volatile deferred_free = NULL;
-static _Atomic(void*) deferred_arg; // = NULL
+static _Atomic(void*) deferred_arg;
 
 void _mi_deferred_free(mi_heap_t* heap, bool force) {
   heap->tld->heartbeat++;
   if (deferred_free != NULL && !heap->tld->recurse) {
     heap->tld->recurse = true;
-    deferred_free(force, heap->tld->heartbeat, mi_atomic_load_ptr_relaxed(void,&deferred_arg));
+    deferred_free(force, heap->tld->heartbeat, mi_atomic_load_ptr_relaxed(void, &deferred_arg));
     heap->tld->recurse = false;
   }
 }
 
 void mi_register_deferred_free(mi_deferred_free_fun* fn, void* arg) mi_attr_noexcept {
   deferred_free = fn;
-  mi_atomic_store_ptr_release(void,&deferred_arg, arg);
+  mi_atomic_store_ptr_release(void, &deferred_arg, arg);
 }
 
-
-/* -----------------------------------------------------------
-  General allocation
------------------------------------------------------------ */
-
-// Large and huge page allocation.
-// Huge pages contain just one block, and the segment contains just that page (as `MI_SEGMENT_HUGE`).
-// Huge pages are also use if the requested alignment is very large (> MI_BLOCK_ALIGNMENT_MAX)
-// so their size is not always `> MI_LARGE_OBJ_SIZE_MAX`.
 static mi_page_t* mi_large_huge_page_alloc(mi_heap_t* heap, size_t size, size_t page_alignment) {
   size_t block_size = _mi_os_good_alloc_size(size);
   mi_assert_internal(mi_bin(block_size) == MI_BIN_HUGE || page_alignment > 0);
   bool is_huge = (block_size > MI_LARGE_OBJ_SIZE_MAX || page_alignment > 0);
-  #if MI_HUGE_PAGE_ABANDON
+
+#if MI_HUGE_PAGE_ABANDON
   mi_page_queue_t* pq = (is_huge ? NULL : mi_page_queue(heap, block_size));
-  #else
-  mi_page_queue_t* pq = mi_page_queue(heap, is_huge ? MI_LARGE_OBJ_SIZE_MAX+1 : block_size);
+#else
+  mi_page_queue_t* pq = mi_page_queue(heap, is_huge ? MI_LARGE_OBJ_SIZE_MAX + 1 : block_size);
   mi_assert_internal(!is_huge || mi_page_queue_is_huge(pq));
-  #endif
+#endif
+
   mi_page_t* page = mi_page_fresh_alloc(heap, pq, block_size, page_alignment);
-  if (page != NULL) {
+  if mi_likely(page != NULL) {
     mi_assert_internal(mi_page_immediate_available(page));
 
     if (is_huge) {
       mi_assert_internal(mi_page_is_huge(page));
       mi_assert_internal(_mi_page_segment(page)->kind == MI_SEGMENT_HUGE);
-      mi_assert_internal(_mi_page_segment(page)->used==1);
-      #if MI_HUGE_PAGE_ABANDON
-      mi_assert_internal(_mi_page_segment(page)->thread_id==0); // abandoned, not in the huge queue
+      mi_assert_internal(_mi_page_segment(page)->used == 1);
+#if MI_HUGE_PAGE_ABANDON
+      mi_assert_internal(_mi_page_segment(page)->thread_id == 0);
       mi_page_set_heap(page, NULL);
-      #endif
+#endif
     }
     else {
       mi_assert_internal(!mi_page_is_huge(page));
     }
 
-    const size_t bsize = mi_page_usable_block_size(page);  // note: not `mi_page_block_size` to account for padding
-    /*if (bsize <= MI_LARGE_OBJ_SIZE_MAX) {
-      mi_heap_stat_increase(heap, malloc_large, bsize);
-      mi_heap_stat_counter_increase(heap, malloc_large_count, 1);
-    }
-    else */
+    const size_t bsize = mi_page_usable_block_size(page);
     {
       _mi_stat_increase(&heap->tld->stats.malloc_huge, bsize);
       _mi_stat_counter_increase(&heap->tld->stats.malloc_huge_count, 1);
@@ -957,72 +871,57 @@ static mi_page_t* mi_large_huge_page_all
   return page;
 }
 
-
-// Allocate a page
-// Note: in debug mode the size includes MI_PADDING_SIZE and might have overflowed.
 static mi_page_t* mi_find_page(mi_heap_t* heap, size_t size, size_t huge_alignment) mi_attr_noexcept {
-  // huge allocation?
-  const size_t req_size = size - MI_PADDING_SIZE;  // correct for padding_size in case of an overflow on `size`
+  const size_t req_size = size - MI_PADDING_SIZE;
   if mi_unlikely(req_size > (MI_MEDIUM_OBJ_SIZE_MAX - MI_PADDING_SIZE) || huge_alignment > 0) {
     if mi_unlikely(req_size > MI_MAX_ALLOC_SIZE) {
       _mi_error_message(EOVERFLOW, "allocation request is too large (%zu bytes)\n", req_size);
       return NULL;
     }
     else {
-      return mi_large_huge_page_alloc(heap,size,huge_alignment);
+      return mi_large_huge_page_alloc(heap, size, huge_alignment);
     }
   }
   else {
-    // otherwise find a page with free blocks in our size segregated queues
-    #if MI_PADDING
+#if MI_PADDING
     mi_assert_internal(size >= MI_PADDING_SIZE);
-    #endif
+#endif
     return mi_find_free_page(heap, size);
   }
 }
 
-// Generic allocation routine if the fast path (`alloc.c:mi_page_malloc`) does not succeed.
-// Note: in debug mode the size includes MI_PADDING_SIZE and might have overflowed.
-// The `huge_alignment` is normally 0 but is set to a multiple of MI_SLICE_SIZE for
-// very large requested alignments in which case we use a huge singleton page.
-void* _mi_malloc_generic(mi_heap_t* heap, size_t size, bool zero, size_t huge_alignment, size_t* usable) mi_attr_noexcept
-{
+void* _mi_malloc_generic(mi_heap_t* heap, size_t size, bool zero, size_t huge_alignment, size_t* usable) mi_attr_noexcept {
   mi_assert_internal(heap != NULL);
 
-  // initialize if necessary
   if mi_unlikely(!mi_heap_is_initialized(heap)) {
-    heap = mi_heap_get_default(); // calls mi_thread_init
-    if mi_unlikely(!mi_heap_is_initialized(heap)) { return NULL; }
+    heap = mi_heap_get_default();
+    if mi_unlikely(!mi_heap_is_initialized(heap)) {
+      return NULL;
+    }
   }
   mi_assert_internal(mi_heap_is_initialized(heap));
 
-  // do administrative tasks every N generic mallocs
   if mi_unlikely(++heap->generic_count >= 100) {
     heap->generic_collect_count += heap->generic_count;
     heap->generic_count = 0;
-    // call potential deferred free routines
     _mi_deferred_free(heap, false);
-
-    // free delayed frees from other threads (but skip contended ones)
     _mi_heap_delayed_free_partial(heap);
 
-    // collect every once in a while (10000 by default)
     const long generic_collect = mi_option_get_clamp(mi_option_generic_collect, 1, 1000000L);
-    if (heap->generic_collect_count >= generic_collect) {
+    if mi_unlikely(heap->generic_collect_count >= generic_collect) {
       heap->generic_collect_count = 0;
-      mi_heap_collect(heap, false /* force? */);
+      mi_heap_collect(heap, false);
     }
   }
 
-  // find (or allocate) a page of the right size
   mi_page_t* page = mi_find_page(heap, size, huge_alignment);
-  if mi_unlikely(page == NULL) { // first time out of memory, try to collect and retry the allocation once more
-    mi_heap_collect(heap, true /* force */);
+  if mi_unlikely(page == NULL) {
+    mi_heap_collect(heap, true);
     page = mi_find_page(heap, size, huge_alignment);
   }
 
-  if mi_unlikely(page == NULL) { // out of memory
-    const size_t req_size = size - MI_PADDING_SIZE;  // correct for padding_size in case of an overflow on `size`
+  if mi_unlikely(page == NULL) {
+    const size_t req_size = size - MI_PADDING_SIZE;
     _mi_error_message(ENOMEM, "unable to allocate memory (%zu bytes)\n", req_size);
     return NULL;
   }
@@ -1030,12 +929,10 @@ void* _mi_malloc_generic(mi_heap_t* heap
   mi_assert_internal(mi_page_immediate_available(page));
   mi_assert_internal(mi_page_block_size(page) >= size);
 
-  // and try again, this time succeeding! (i.e. this should never recurse through _mi_page_malloc)
   void* const p = _mi_page_malloc_zero(heap, page, size, zero, usable);
   mi_assert_internal(p != NULL);
 
-  // move singleton pages to the full queue
-  if (page->reserved == page->used) {
+  if mi_unlikely(page->reserved == page->used) {
     mi_page_to_full(page, mi_page_queue_of(page));
   }
   return p;


--- a/src/segment.c	2026-01-23 12:49:10.316203953 +0100
+++ b/src/segment.c	2026-01-23 13:28:05.339102905 +0100
@@ -11,18 +11,12 @@ terms of the MIT license. A copy of the
 #include <string.h>  // memset
 #include <stdio.h>
 
-// -------------------------------------------------------------------
-// Segments
-// mimalloc pages reside in segments. See `mi_segment_valid` for invariants.
-// -------------------------------------------------------------------
-
-
+/* Forward declaration */
 static void mi_segment_try_purge(mi_segment_t* segment, bool force);
 
-
-// -------------------------------------------------------------------
-// commit mask
-// -------------------------------------------------------------------
+/* ---------------------------------------------------------------------------
+  Commit Mask Operations
+--------------------------------------------------------------------------- */
 
 static bool mi_commit_mask_all_set(const mi_commit_mask_t* commit, const mi_commit_mask_t* cm) {
   for (size_t i = 0; i < MI_COMMIT_MASK_FIELD_COUNT; i++) {
@@ -92,12 +86,10 @@ size_t _mi_commit_mask_committed_size(co
       count += MI_COMMIT_MASK_FIELD_BITS;
     }
     else {
-      for (; mask != 0; mask >>= 1) {  // todo: use popcount
-        if ((mask&1)!=0) count++;
-      }
+      // Use compiler builtin for popcount (Raptor Lake has hardware POPCNT)
+      count += mi_popcount(mask);
     }
   }
-  // we use total since for huge segments each commit bit may represent a larger size
   return ((total / MI_COMMIT_MASK_BITS) * count);
 }
 
@@ -111,10 +103,11 @@ size_t _mi_commit_mask_next_run(const mi
     mask = cm->mask[i];
     mask >>= ofs;
     if (mask != 0) {
-      while ((mask&1) == 0) {
-        mask >>= 1;
-        ofs++;
-      }
+      // found set bit, skip trailing zeros
+      // mi_ctz is efficient on modern CPUs (TZCNT)
+      size_t zeros = mi_ctz(mask);
+      mask >>= zeros;
+      ofs += zeros;
       break;
     }
     i++;
@@ -126,22 +119,26 @@ size_t _mi_commit_mask_next_run(const mi
     return 0;
   }
   else {
-    // found, count ones
+    // found, count consecutive ones
     size_t count = 0;
     *idx = (i*MI_COMMIT_MASK_FIELD_BITS) + ofs;
-    do {
-      mi_assert_internal(ofs < MI_COMMIT_MASK_FIELD_BITS && (mask&1) == 1);
-      do {
-        count++;
-        mask >>= 1;
-      } while ((mask&1) == 1);
-      if ((((*idx + count) % MI_COMMIT_MASK_FIELD_BITS) == 0)) {
+
+    // Count trailing ones using bitwise tricks (mask is shifted so LSB is 1)
+    // ~mask has 0 at LSB. mi_ctz(~mask) counts consecutive 1s.
+    size_t ones = mi_ctz(~mask);
+    count += ones;
+
+    // If we consumed the rest of the field, check next fields
+    if ((ofs + count) == MI_COMMIT_MASK_FIELD_BITS) {
         i++;
-        if (i >= MI_COMMIT_MASK_FIELD_COUNT) break;
-        mask = cm->mask[i];
-        ofs = 0;
-      }
-    } while ((mask&1) == 1);
+        while (i < MI_COMMIT_MASK_FIELD_COUNT && cm->mask[i] == ~((size_t)0)) {
+            count += MI_COMMIT_MASK_FIELD_BITS;
+            i++;
+        }
+        if (i < MI_COMMIT_MASK_FIELD_COUNT) {
+            count += mi_ctz(~cm->mask[i]);
+        }
+    }
     mi_assert_internal(count > 0);
     return count;
   }
@@ -149,32 +146,9 @@ size_t _mi_commit_mask_next_run(const mi
 
 
 /* --------------------------------------------------------------------------------
-  Segment allocation
-  We allocate pages inside bigger "segments" (32 MiB on 64-bit). This is to avoid
-  splitting VMA's on Linux and reduce fragmentation on other OS's.
-  Each thread owns its own segments.
-
-  Currently we have:
-  - small pages (64KiB)
-  - medium pages (512KiB)
-  - large pages (4MiB),
-  - huge segments have 1 page in one segment that can be larger than `MI_SEGMENT_SIZE`.
-    it is used for blocks `> MI_LARGE_OBJ_SIZE_MAX` or with alignment `> MI_BLOCK_ALIGNMENT_MAX`.
-
-  The memory for a segment is usually committed on demand.
-  (i.e. we are careful to not touch the memory until we actually allocate a block there)
-
-  If a  thread ends, it "abandons" pages that still contain live blocks.
-  Such segments are abandoned and these can be reclaimed by still running threads,
-  (much like work-stealing).
+  Segment Allocation
 -------------------------------------------------------------------------------- */
 
-
-/* -----------------------------------------------------------
-   Slices
------------------------------------------------------------ */
-
-
 static const mi_slice_t* mi_segment_slices_end(const mi_segment_t* segment) {
   return &segment->slices[segment->slice_entries];
 }
@@ -189,7 +163,6 @@ static uint8_t* mi_slice_start(const mi_
 /* -----------------------------------------------------------
    Bins
 ----------------------------------------------------------- */
-// Use bit scan forward to quickly find the first zero bit if it is available
 
 static inline size_t mi_slice_bin8(size_t slice_count) {
   if (slice_count<=1) return slice_count;
@@ -222,9 +195,8 @@ static inline size_t mi_slice_index(cons
 ----------------------------------------------------------- */
 
 static void mi_span_queue_push(mi_span_queue_t* sq, mi_slice_t* slice) {
-  // todo: or push to the end?
   mi_assert_internal(slice->prev == NULL && slice->next==NULL);
-  slice->prev = NULL; // paranoia
+  slice->prev = NULL;
   slice->next = sq->first;
   sq->first = slice;
   if (slice->next != NULL) slice->next->prev = slice;
@@ -241,7 +213,6 @@ static mi_span_queue_t* mi_span_queue_fo
 
 static void mi_span_queue_delete(mi_span_queue_t* sq, mi_slice_t* slice) {
   mi_assert_internal(slice->block_size==0 && slice->slice_count>0 && slice->slice_offset==0);
-  // should work too if the queue does not contain slice (which can happen during reclaim)
   if (slice->prev != NULL) slice->prev->next = slice->next;
   if (slice == sq->first) sq->first = slice->next;
   if (slice->next != NULL) slice->next->prev = slice->prev;
@@ -274,8 +245,7 @@ static bool mi_segment_is_valid(mi_segme
   mi_assert_internal(_mi_ptr_cookie(segment) == segment->cookie);
   mi_assert_internal(segment->abandoned <= segment->used);
   mi_assert_internal(segment->thread_id == 0 || segment->thread_id == _mi_thread_id());
-  mi_assert_internal(mi_commit_mask_all_set(&segment->commit_mask, &segment->purge_mask)); // can only decommit committed blocks
-  //mi_assert_internal(segment->segment_info_size % MI_SEGMENT_SLICE_SIZE == 0);
+  mi_assert_internal(mi_commit_mask_all_set(&segment->commit_mask, &segment->purge_mask));
   mi_slice_t* slice = &segment->slices[0];
   const mi_slice_t* end = mi_segment_slices_end(segment);
   size_t used_count = 0;
@@ -285,7 +255,7 @@ static bool mi_segment_is_valid(mi_segme
     mi_assert_internal(slice->slice_offset == 0);
     size_t index = mi_slice_index(slice);
     size_t maxindex = (index + slice->slice_count >= segment->slice_entries ? segment->slice_entries : index + slice->slice_count) - 1;
-    if (mi_slice_is_used(slice)) { // a page in use, we need at least MAX_SLICE_OFFSET_COUNT valid back offsets
+    if (mi_slice_is_used(slice)) {
       used_count++;
       mi_assert_internal(slice->is_huge == (segment->kind == MI_SEGMENT_HUGE));
       for (size_t i = 0; i <= MI_MAX_SLICE_OFFSET_COUNT && index + i <= maxindex; i++) {
@@ -293,7 +263,6 @@ static bool mi_segment_is_valid(mi_segme
         mi_assert_internal(i==0 || segment->slices[index + i].slice_count == 0);
         mi_assert_internal(i==0 || segment->slices[index + i].block_size == 1);
       }
-      // and the last entry as well (for coalescing)
       const mi_slice_t* last = slice + slice->slice_count - 1;
       if (last > slice && last < mi_segment_slices_end(segment)) {
         mi_assert_internal(last->slice_offset == (slice->slice_count-1)*sizeof(mi_slice_t));
@@ -301,14 +270,14 @@ static bool mi_segment_is_valid(mi_segme
         mi_assert_internal(last->block_size == 1);
       }
     }
-    else {  // free range of slices; only last slice needs a valid back offset
+    else {
       mi_slice_t* last = &segment->slices[maxindex];
       if (segment->kind != MI_SEGMENT_HUGE || slice->slice_count <= (segment->slice_entries - segment->segment_info_slices)) {
         mi_assert_internal((uint8_t*)slice == (uint8_t*)last - last->slice_offset);
       }
       mi_assert_internal(slice == last || last->slice_count == 0 );
       mi_assert_internal(last->block_size == 0 || (segment->kind==MI_SEGMENT_HUGE && last->block_size==1));
-      if (segment->kind != MI_SEGMENT_HUGE && segment->thread_id != 0) { // segment is not huge or abandoned
+      if (segment->kind != MI_SEGMENT_HUGE && segment->thread_id != 0) {
         sq = mi_span_queue_for(slice->slice_count,tld);
         mi_assert_internal(mi_span_queue_contains(sq,slice));
       }
@@ -334,12 +303,8 @@ static uint8_t* _mi_segment_page_start_f
   const ptrdiff_t idx = slice - segment->slices;
   const size_t psize = (size_t)slice->slice_count * MI_SEGMENT_SLICE_SIZE;
   uint8_t* const pstart = (uint8_t*)segment + (idx*MI_SEGMENT_SLICE_SIZE);
-  // make the start not OS page aligned for smaller blocks to avoid page/cache effects
-  // note: the offset must always be a block_size multiple since we assume small allocations
-  // are aligned (see `mi_heap_malloc_aligned`).
   size_t start_offset = 0;
   if (block_size > 0 && block_size <= MI_MAX_ALIGN_GUARANTEE) {
-    // for small objects, ensure the page start is aligned with the block size (PR#66 by kickunderscore)
     const size_t adjust = block_size - ((uintptr_t)pstart % block_size);
     if (adjust < block_size && psize >= block_size + adjust) {
       start_offset += adjust;
@@ -373,8 +338,6 @@ static size_t mi_segment_calculate_slice
   size_t guardsize = 0;
 
   if (MI_SECURE>0) {
-    // in secure mode, we set up a protected page in between the segment info
-    // and the page data (and one at the end of the segment)
     guardsize = page_size;
     if (required > 0) {
       required = _mi_align_up(required, MI_SEGMENT_SLICE_SIZE) + page_size;
@@ -391,8 +354,6 @@ static size_t mi_segment_calculate_slice
 
 /* ----------------------------------------------------------------------------
 Segment caches
-We keep a small segment cache per thread to increase local
-reuse and avoid setting/clearing guard pages in secure mode.
 ------------------------------------------------------------------------------- */
 
 static void mi_segments_track_size(long segment_size, mi_segments_tld_t* tld) {
@@ -413,17 +374,12 @@ static void mi_segment_os_free(mi_segmen
     segment->was_reclaimed = false;
   }
   if (MI_SECURE>0) {
-    // _mi_os_unprotect(segment, mi_segment_size(segment)); // ensure no more guard pages are set
-    // unprotect the guard pages; we cannot just unprotect the whole segment size as part may be decommitted
     size_t os_pagesize = _mi_os_page_size();
     _mi_os_unprotect((uint8_t*)segment + mi_segment_info_size(segment) - os_pagesize, os_pagesize);
     uint8_t* end = (uint8_t*)segment + mi_segment_size(segment) - os_pagesize;
     _mi_os_unprotect(end, os_pagesize);
   }
 
-  // purge delayed decommits now? (no, leave it to the arena)
-  // mi_segment_try_purge(segment,true,tld->stats);
-
   const size_t size = mi_segment_size(segment);
   const size_t csize = _mi_commit_mask_committed_size(&segment->commit_mask, size);
 
@@ -449,18 +405,16 @@ static void mi_segment_commit_mask(mi_se
   size_t start;
   size_t end;
   if (conservative) {
-    // decommit conservative
     start = _mi_align_up(pstart, MI_COMMIT_SIZE);
     end   = _mi_align_down(pstart + size, MI_COMMIT_SIZE);
     mi_assert_internal(start >= segstart);
     mi_assert_internal(end <= segsize);
   }
   else {
-    // commit liberal
     start = _mi_align_down(pstart, MI_MINIMAL_COMMIT_SIZE);
     end   = _mi_align_up(pstart + size, MI_MINIMAL_COMMIT_SIZE);
   }
-  if (pstart >= segstart && start < segstart) {  // note: the mask is also calculated for an initial commit of the info area
+  if (pstart >= segstart && start < segstart) {
     start = segstart;
   }
   if (end > segsize) {
@@ -476,7 +430,7 @@ static void mi_segment_commit_mask(mi_se
   size_t bitidx = start / MI_COMMIT_SIZE;
   mi_assert_internal(bitidx < MI_COMMIT_MASK_BITS);
 
-  size_t bitcount = *full_size / MI_COMMIT_SIZE; // can be 0
+  size_t bitcount = *full_size / MI_COMMIT_SIZE;
   if (bitidx + bitcount > MI_COMMIT_MASK_BITS) {
     _mi_warning_message("commit mask overflow: idx=%zu count=%zu start=%zx end=%zx p=0x%p size=%zu fullsize=%zu\n", bitidx, bitcount, start, end, p, size, *full_size);
   }
@@ -487,7 +441,6 @@ static void mi_segment_commit_mask(mi_se
 static bool mi_segment_commit(mi_segment_t* segment, uint8_t* p, size_t size) {
   mi_assert_internal(mi_commit_mask_all_set(&segment->commit_mask, &segment->purge_mask));
 
-  // commit liberal
   uint8_t* start = NULL;
   size_t   full_size = 0;
   mi_commit_mask_t mask;
@@ -495,29 +448,25 @@ static bool mi_segment_commit(mi_segment
   if (mi_commit_mask_is_empty(&mask) || full_size == 0) return true;
 
   if (!mi_commit_mask_all_set(&segment->commit_mask, &mask)) {
-    // committing
     bool is_zero = false;
     mi_commit_mask_t cmask;
     mi_commit_mask_create_intersect(&segment->commit_mask, &mask, &cmask);
-    _mi_stat_decrease(&_mi_stats_main.committed, _mi_commit_mask_committed_size(&cmask, MI_SEGMENT_SIZE)); // adjust for overlap
+    _mi_stat_decrease(&_mi_stats_main.committed, _mi_commit_mask_committed_size(&cmask, MI_SEGMENT_SIZE));
     if (!_mi_os_commit(start, full_size, &is_zero)) return false;
     mi_commit_mask_set(&segment->commit_mask, &mask);
   }
 
-  // increase purge expiration when using part of delayed purges -- we assume more allocations are coming soon.
   if (mi_commit_mask_any_set(&segment->purge_mask, &mask)) {
     segment->purge_expire = _mi_clock_now() + mi_option_get(mi_option_purge_delay);
   }
 
-  // always clear any delayed purges in our range (as they are either committed now)
   mi_commit_mask_clear(&segment->purge_mask, &mask);
   return true;
 }
 
 static bool mi_segment_ensure_committed(mi_segment_t* segment, uint8_t* p, size_t size) {
   mi_assert_internal(mi_commit_mask_all_set(&segment->commit_mask, &segment->purge_mask));
-  // note: assumes commit_mask is always full for huge segments as otherwise the commit mask bits can overflow
-  if (mi_commit_mask_is_full(&segment->commit_mask) && mi_commit_mask_is_empty(&segment->purge_mask)) return true; // fully committed
+  if (mi_likely(mi_commit_mask_is_full(&segment->commit_mask) && mi_commit_mask_is_empty(&segment->purge_mask))) return true;
   mi_assert_internal(segment->kind != MI_SEGMENT_HUGE);
   return mi_segment_commit(segment, p, size);
 }
@@ -526,7 +475,6 @@ static bool mi_segment_purge(mi_segment_
   mi_assert_internal(mi_commit_mask_all_set(&segment->commit_mask, &segment->purge_mask));
   if (!segment->allow_purge) return true;
 
-  // purge conservative
   uint8_t* start = NULL;
   size_t   full_size = 0;
   mi_commit_mask_t mask;
@@ -534,19 +482,17 @@ static bool mi_segment_purge(mi_segment_
   if (mi_commit_mask_is_empty(&mask) || full_size==0) return true;
 
   if (mi_commit_mask_any_set(&segment->commit_mask, &mask)) {
-    // purging
     mi_assert_internal((void*)start != (void*)segment);
     mi_assert_internal(segment->allow_decommit);
-    const bool decommitted = _mi_os_purge(start, full_size);  // reset or decommit
+    const bool decommitted = _mi_os_purge(start, full_size);
     if (decommitted) {
       mi_commit_mask_t cmask;
       mi_commit_mask_create_intersect(&segment->commit_mask, &mask, &cmask);
-      _mi_stat_increase(&_mi_stats_main.committed, full_size - _mi_commit_mask_committed_size(&cmask, MI_SEGMENT_SIZE)); // adjust for double counting
+      _mi_stat_increase(&_mi_stats_main.committed, full_size - _mi_commit_mask_committed_size(&cmask, MI_SEGMENT_SIZE));
       mi_commit_mask_clear(&segment->commit_mask, &mask);
     }
   }
 
-  // always clear any scheduled purges in our range
   mi_commit_mask_clear(&segment->purge_mask, &mask);
   return true;
 }
@@ -558,34 +504,29 @@ static void mi_segment_schedule_purge(mi
     mi_segment_purge(segment, p, size);
   }
   else {
-    // register for future purge in the purge mask
     uint8_t* start = NULL;
     size_t   full_size = 0;
     mi_commit_mask_t mask;
     mi_segment_commit_mask(segment, true /*conservative*/, p, size, &start, &full_size, &mask);
     if (mi_commit_mask_is_empty(&mask) || full_size==0) return;
 
-    // update delayed commit
     mi_assert_internal(segment->purge_expire > 0 || mi_commit_mask_is_empty(&segment->purge_mask));
     mi_commit_mask_t cmask;
-    mi_commit_mask_create_intersect(&segment->commit_mask, &mask, &cmask);  // only purge what is committed; span_free may try to decommit more
+    mi_commit_mask_create_intersect(&segment->commit_mask, &mask, &cmask);
     mi_commit_mask_set(&segment->purge_mask, &cmask);
     mi_msecs_t now = _mi_clock_now();
     if (segment->purge_expire == 0) {
-      // no previous purgess, initialize now
       segment->purge_expire = now + mi_option_get(mi_option_purge_delay);
     }
     else if (segment->purge_expire <= now) {
-      // previous purge mask already expired
       if (segment->purge_expire + mi_option_get(mi_option_purge_extend_delay) <= now) {
         mi_segment_try_purge(segment, true);
       }
       else {
-        segment->purge_expire = now + mi_option_get(mi_option_purge_extend_delay); // (mi_option_get(mi_option_purge_delay) / 8); // wait a tiny bit longer in case there is a series of free's
+        segment->purge_expire = now + mi_option_get(mi_option_purge_extend_delay);
       }
     }
     else {
-      // previous purge mask is not yet expired, increase the expiration by a bit.
       segment->purge_expire += mi_option_get(mi_option_purge_extend_delay);
     }
   }
@@ -603,7 +544,6 @@ static void mi_segment_try_purge(mi_segm
   size_t idx;
   size_t count;
   mi_commit_mask_foreach(&mask, idx, count) {
-    // if found, decommit that sequence
     if (count > 0) {
       uint8_t* p = (uint8_t*)segment + (idx*MI_COMMIT_SIZE);
       size_t size = count * MI_COMMIT_SIZE;
@@ -614,8 +554,6 @@ static void mi_segment_try_purge(mi_segm
   mi_assert_internal(mi_commit_mask_is_empty(&segment->purge_mask));
 }
 
-// called from `mi_heap_collect_ex`
-// this can be called per-page so it is important that try_purge has fast exit path
 void _mi_segment_collect(mi_segment_t* segment, bool force) {
   mi_segment_try_purge(segment, force);
 }
@@ -628,48 +566,38 @@ static bool mi_segment_is_abandoned(mi_s
   return (mi_atomic_load_relaxed(&segment->thread_id) == 0);
 }
 
-// note: can be called on abandoned segments
 static void mi_segment_span_free(mi_segment_t* segment, size_t slice_index, size_t slice_count, bool allow_purge, mi_segments_tld_t* tld) {
   mi_assert_internal(slice_index < segment->slice_entries);
   mi_span_queue_t* sq = (segment->kind == MI_SEGMENT_HUGE || mi_segment_is_abandoned(segment)
                           ? NULL : mi_span_queue_for(slice_count,tld));
   if (slice_count==0) slice_count = 1;
-  mi_assert_internal(slice_index + slice_count - 1 < segment->slice_entries);
+  mi_assert_internal(slice_index + slice_count <= segment->slice_entries);
 
-  // set first and last slice (the intermediates can be undetermined)
   mi_slice_t* slice = &segment->slices[slice_index];
   slice->slice_count = (uint32_t)slice_count;
   mi_assert_internal(slice->slice_count == slice_count); // no overflow?
   slice->slice_offset = 0;
   if (slice_count > 1) {
-    mi_slice_t* last = slice + slice_count - 1;
-    mi_slice_t* end  = (mi_slice_t*)mi_segment_slices_end(segment);
-    if (last > end) { last = end; }
-    last->slice_count = 0;
-    last->slice_offset = (uint32_t)(sizeof(mi_page_t)*(slice_count - 1));
-    last->block_size = 0;
+    // FIX: clamp last slice index to stay within array bounds
+    const size_t last_index = slice_index + slice_count - 1;
+    if (last_index < segment->slice_entries) {
+      mi_slice_t* last = &segment->slices[last_index];
+      if (last > slice) {
+        last->slice_count = 0;
+        last->slice_offset = (uint32_t)(sizeof(mi_page_t)*(slice_count - 1));
+        last->block_size = 0;
+      }
+    }
   }
 
-  // perhaps decommit
   if (allow_purge) {
     mi_segment_schedule_purge(segment, mi_slice_start(slice), slice_count * MI_SEGMENT_SLICE_SIZE);
   }
 
-  // and push it on the free page queue (if it was not a huge page)
   if (sq != NULL) mi_span_queue_push( sq, slice );
              else slice->block_size = 0; // mark huge page as free anyways
 }
 
-/*
-// called from reclaim to add existing free spans
-static void mi_segment_span_add_free(mi_slice_t* slice, mi_segments_tld_t* tld) {
-  mi_segment_t* segment = _mi_ptr_segment(slice);
-  mi_assert_internal(slice->xblock_size==0 && slice->slice_count>0 && slice->slice_offset==0);
-  size_t slice_index = mi_slice_index(slice);
-  mi_segment_span_free(segment,slice_index,slice->slice_count,tld);
-}
-*/
-
 static void mi_segment_span_remove_from_queue(mi_slice_t* slice, mi_segments_tld_t* tld) {
   mi_assert_internal(slice->slice_count > 0 && slice->slice_offset==0 && slice->block_size==0);
   mi_assert_internal(_mi_ptr_segment(slice)->kind != MI_SEGMENT_HUGE);
@@ -677,47 +605,38 @@ static void mi_segment_span_remove_from_
   mi_span_queue_delete(sq, slice);
 }
 
-// note: can be called on abandoned segments
 static mi_slice_t* mi_segment_span_free_coalesce(mi_slice_t* slice, mi_segments_tld_t* tld) {
   mi_assert_internal(slice != NULL && slice->slice_count > 0 && slice->slice_offset == 0);
   mi_segment_t* const segment = _mi_ptr_segment(slice);
 
-  // for huge pages, just mark as free but don't add to the queues
-  if (segment->kind == MI_SEGMENT_HUGE) {
-    // issue #691: segment->used can be 0 if the huge page block was freed while abandoned (reclaim will get here in that case)
-    mi_assert_internal((segment->used==0 && slice->block_size==0) || segment->used == 1);  // decreased right after this call in `mi_segment_page_clear`
-    slice->block_size = 0;  // mark as free anyways
-    // we should mark the last slice `xblock_size=0` now to maintain invariants but we skip it to
-    // avoid a possible cache miss (and the segment is about to be freed)
+  if (mi_unlikely(segment->kind == MI_SEGMENT_HUGE)) {
+    mi_assert_internal((segment->used==0 && slice->block_size==0) || segment->used == 1);
+    slice->block_size = 0;
     return slice;
   }
 
-  // otherwise coalesce the span and add to the free span queues
-  const bool is_abandoned = (segment->thread_id == 0); // mi_segment_is_abandoned(segment);
+  const bool is_abandoned = (segment->thread_id == 0);
   size_t slice_count = slice->slice_count;
   mi_slice_t* next = slice + slice->slice_count;
   mi_assert_internal(next <= mi_segment_slices_end(segment));
   if (next < mi_segment_slices_end(segment) && next->block_size==0) {
-    // free next block -- remove it from free and merge
     mi_assert_internal(next->slice_count > 0 && next->slice_offset==0);
-    slice_count += next->slice_count; // extend
+    slice_count += next->slice_count;
     if (!is_abandoned) { mi_segment_span_remove_from_queue(next, tld); }
   }
   if (slice > segment->slices) {
     mi_slice_t* prev = mi_slice_first(slice - 1);
     mi_assert_internal(prev >= segment->slices);
     if (prev->block_size==0) {
-      // free previous slice -- remove it from free and merge
       mi_assert_internal(prev->slice_count > 0 && prev->slice_offset==0);
       slice_count += prev->slice_count;
       slice->slice_count = 0;
-      slice->slice_offset = (uint32_t)((uint8_t*)slice - (uint8_t*)prev); // set the slice offset for `segment_force_abandon` (in case the previous free block is very large).
+      slice->slice_offset = (uint32_t)((uint8_t*)slice - (uint8_t*)prev);
       if (!is_abandoned) { mi_segment_span_remove_from_queue(prev, tld); }
       slice = prev;
     }
   }
 
-  // and add the new free page
   mi_segment_span_free(segment, mi_slice_index(slice), slice_count, true, tld);
   return slice;
 }
@@ -728,18 +647,15 @@ static mi_slice_t* mi_segment_span_free_
    Page allocation
 ----------------------------------------------------------- */
 
-// Note: may still return NULL if committing the memory failed
 static mi_page_t* mi_segment_span_allocate(mi_segment_t* segment, size_t slice_index, size_t slice_count) {
   mi_assert_internal(slice_index < segment->slice_entries);
   mi_slice_t* const slice = &segment->slices[slice_index];
   mi_assert_internal(slice->block_size==0 || slice->block_size==1);
 
-  // commit before changing the slice data
   if (!mi_segment_ensure_committed(segment, _mi_segment_page_start_from_slice(segment, slice, 0, NULL), slice_count * MI_SEGMENT_SLICE_SIZE)) {
-    return NULL;  // commit failed!
+    return NULL;
   }
 
-  // convert the slices to a page
   slice->slice_offset = 0;
   slice->slice_count = (uint32_t)slice_count;
   mi_assert_internal(slice->slice_count == slice_count);
@@ -748,10 +664,9 @@ static mi_page_t* mi_segment_span_alloca
   mi_page_t*  page = mi_slice_to_page(slice);
   mi_assert_internal(mi_page_block_size(page) == bsize);
 
-  // set slice back pointers for the first MI_MAX_SLICE_OFFSET_COUNT entries
   size_t extra = slice_count-1;
   if (extra > MI_MAX_SLICE_OFFSET_COUNT) extra = MI_MAX_SLICE_OFFSET_COUNT;
-  if (slice_index + extra >= segment->slice_entries) extra = segment->slice_entries - slice_index - 1;  // huge objects may have more slices than avaiable entries in the segment->slices
+  if (slice_index + extra >= segment->slice_entries) extra = segment->slice_entries - slice_index - 1;
 
   mi_slice_t* slice_next = slice + 1;
   for (size_t i = 1; i <= extra; i++, slice_next++) {
@@ -760,18 +675,18 @@ static mi_page_t* mi_segment_span_alloca
     slice_next->block_size = 1;
   }
 
-  // and also for the last one (if not set already) (the last one is needed for coalescing and for large alignments)
-  // note: the cast is needed for ubsan since the index can be larger than MI_SLICES_PER_SEGMENT for huge allocations (see #543)
   mi_slice_t* last = slice + slice_count - 1;
   mi_slice_t* end = (mi_slice_t*)mi_segment_slices_end(segment);
-  if (last > end) last = end;
+
+  // CRITICAL FIX: Clamp 'last' to valid array bounds to prevent overflow
+  if (last >= end) last = end - 1;
+
   if (last > slice) {
     last->slice_offset = (uint32_t)(sizeof(mi_slice_t) * (last - slice));
     last->slice_count = 0;
     last->block_size = 1;
   }
 
-  // and initialize the page
   page->is_committed = true;
   page->is_zero_init = segment->free_is_zero;
   page->is_huge = (segment->kind == MI_SEGMENT_HUGE);
@@ -782,7 +697,7 @@ static mi_page_t* mi_segment_span_alloca
 static void mi_segment_slice_split(mi_segment_t* segment, mi_slice_t* slice, size_t slice_count, mi_segments_tld_t* tld) {
   mi_assert_internal(_mi_ptr_segment(slice) == segment);
   mi_assert_internal(slice->slice_count >= slice_count);
-  mi_assert_internal(slice->block_size > 0); // no more in free queue
+  mi_assert_internal(slice->block_size > 0);
   if (slice->slice_count <= slice_count) return;
   mi_assert_internal(segment->kind != MI_SEGMENT_HUGE);
   size_t next_index = mi_slice_index(slice) + slice_count;
@@ -793,16 +708,13 @@ static void mi_segment_slice_split(mi_se
 
 static mi_page_t* mi_segments_page_find_and_allocate(size_t slice_count, mi_arena_id_t req_arena_id, mi_segments_tld_t* tld) {
   mi_assert_internal(slice_count*MI_SEGMENT_SLICE_SIZE <= MI_LARGE_OBJ_SIZE_MAX);
-  // search from best fit up
   mi_span_queue_t* sq = mi_span_queue_for(slice_count, tld);
   if (slice_count == 0) slice_count = 1;
   while (sq <= &tld->spans[MI_SEGMENT_BIN_MAX]) {
     for (mi_slice_t* slice = sq->first; slice != NULL; slice = slice->next) {
-      if (slice->slice_count >= slice_count) {
-        // found one
+      if (mi_likely(slice->slice_count >= slice_count)) {
         mi_segment_t* segment = _mi_ptr_segment(slice);
-        if (_mi_arena_memid_is_suitable(segment->memid, req_arena_id)) {
-          // found a suitable page span
+        if (mi_likely(_mi_arena_memid_is_suitable(segment->memid, req_arena_id))) {
           mi_span_queue_delete(sq, slice);
 
           if (slice->slice_count > slice_count) {
@@ -810,8 +722,7 @@ static mi_page_t* mi_segments_page_find_
           }
           mi_assert_internal(slice != NULL && slice->slice_count == slice_count && slice->block_size > 0);
           mi_page_t* page = mi_segment_span_allocate(segment, mi_slice_index(slice), slice->slice_count);
-          if (page == NULL) {
-            // commit failed; return NULL but first restore the slice
+          if (mi_unlikely(page == NULL)) {
             mi_segment_span_free_coalesce(slice, tld);
             return NULL;
           }
@@ -821,7 +732,6 @@ static mi_page_t* mi_segments_page_find_
     }
     sq++;
   }
-  // could not find a page..
   return NULL;
 }
 
@@ -836,19 +746,33 @@ static mi_segment_t* mi_segment_os_alloc
 
 {
   mi_memid_t memid;
-  bool   allow_large = (!eager_delayed && (MI_SECURE == 0)); // only allow large OS pages once we are no longer lazy
+  bool   allow_large = (!eager_delayed && (MI_SECURE == 0));
   size_t align_offset = 0;
   size_t alignment = MI_SEGMENT_ALIGN;
 
   if (page_alignment > 0) {
-    // mi_assert_internal(huge_page != NULL);
-    mi_assert_internal(page_alignment >= MI_SEGMENT_ALIGN);
-    alignment = page_alignment;
-    const size_t info_size = (*pinfo_slices) * MI_SEGMENT_SLICE_SIZE;
-    align_offset = _mi_align_up( info_size, MI_SEGMENT_ALIGN );
-    const size_t extra = align_offset - info_size;
-    // recalculate due to potential guard pages
-    *psegment_slices = mi_segment_calculate_slices(required + extra, pinfo_slices);
+    /*
+       CRITICAL FIX: Enforce minimum segment alignment of 32MB.
+       Do NOT set align_offset for small alignments, as it pushes the payload
+       too far, breaking _mi_ptr_segment logic.
+    */
+    alignment = (page_alignment > MI_SEGMENT_ALIGN ? page_alignment : MI_SEGMENT_ALIGN);
+
+    if (page_alignment > MI_SEGMENT_ALIGN) {
+        // Large alignment: align payload via offset
+        const size_t info_size = (*pinfo_slices) * MI_SEGMENT_SLICE_SIZE;
+        align_offset = _mi_align_up(info_size, alignment);
+        const size_t extra = align_offset - info_size;
+        *psegment_slices = mi_segment_calculate_slices(required + extra, pinfo_slices);
+    } else {
+        // Small/Normal alignment: keep payload close to header (offset 0)
+        // Manual padding calc to ensure page_alignment is met
+        const size_t info_size = (*pinfo_slices) * MI_SEGMENT_SLICE_SIZE;
+        const size_t aligned_info = _mi_align_up(info_size, page_alignment);
+        const size_t extra = aligned_info - info_size;
+        *psegment_slices = mi_segment_calculate_slices(required + extra, pinfo_slices);
+        align_offset = 0;
+    }
     mi_assert_internal(*psegment_slices > 0 && *psegment_slices <= UINT32_MAX);
   }
 
@@ -858,13 +782,11 @@ static mi_segment_t* mi_segment_os_alloc
     return NULL;  // failed to allocate
   }
 
-  // ensure metadata part of the segment is committed
   mi_commit_mask_t commit_mask;
   if (memid.initially_committed) {
     mi_commit_mask_create_full(&commit_mask);
   }
   else {
-    // at least commit the info slices
     const size_t commit_needed = _mi_divide_up((*pinfo_slices)*MI_SEGMENT_SLICE_SIZE, MI_COMMIT_SIZE);
     mi_assert_internal(commit_needed>0);
     mi_commit_mask_create(0, commit_needed, &commit_mask);
@@ -874,7 +796,7 @@ static mi_segment_t* mi_segment_os_alloc
       return NULL;
     }
   }
-  mi_assert_internal(segment != NULL && (uintptr_t)segment % MI_SEGMENT_SIZE == 0);
+  mi_assert_internal(segment != NULL && (uintptr_t)segment % MI_SEGMENT_ALIGN == 0);
 
   segment->memid = memid;
   segment->allow_decommit = !memid.is_pinned;
@@ -897,32 +819,25 @@ static mi_segment_t* mi_segment_alloc(si
 {
   mi_assert_internal((required==0 && huge_page==NULL) || (required>0 && huge_page != NULL));
 
-  // calculate needed sizes first
   size_t info_slices;
   size_t segment_slices = mi_segment_calculate_slices(required, &info_slices);
   mi_assert_internal(segment_slices > 0 && segment_slices <= UINT32_MAX);
 
-  // Commit eagerly only if not the first N lazy segments (to reduce impact of many threads that allocate just a little)
-  const bool eager_delay = (// !_mi_os_has_overcommit() &&             // never delay on overcommit systems
-                            _mi_current_thread_count() > 1 &&       // do not delay for the first N threads
-                            tld->peak_count < (size_t)mi_option_get(mi_option_eager_commit_delay));
+  const bool eager_delay = (_mi_current_thread_count() > 1 && tld->peak_count < (size_t)mi_option_get(mi_option_eager_commit_delay));
   const bool eager = !eager_delay && mi_option_is_enabled(mi_option_eager_commit);
   bool commit = eager || (required > 0);
 
-  // Allocate the segment from the OS
   mi_segment_t* segment = mi_segment_os_alloc(required, page_alignment, eager_delay, req_arena_id,
                                               &segment_slices, &info_slices, commit, tld);
   if (segment == NULL) return NULL;
 
-  // zero the segment info? -- not always needed as it may be zero initialized from the OS
   if (!segment->memid.initially_zero) {
     ptrdiff_t ofs    = offsetof(mi_segment_t, next);
     size_t    prefix = offsetof(mi_segment_t, slices) - ofs;
-    size_t    zsize  = prefix + (sizeof(mi_slice_t) * (segment_slices + 1)); // one more
+    size_t    zsize  = prefix + (sizeof(mi_slice_t) * (segment_slices + 1));
     _mi_memzero((uint8_t*)segment + ofs, zsize);
   }
 
-  // initialize the rest of the segment info
   const size_t slice_entries = (segment_slices > MI_SLICES_PER_SEGMENT ? MI_SLICES_PER_SEGMENT : segment_slices);
   segment->segment_slices = segment_slices;
   segment->segment_info_slices = info_slices;
@@ -931,40 +846,34 @@ static mi_segment_t* mi_segment_alloc(si
   segment->slice_entries = slice_entries;
   segment->kind = (required == 0 ? MI_SEGMENT_NORMAL : MI_SEGMENT_HUGE);
 
-  // _mi_memzero(segment->slices, sizeof(mi_slice_t)*(info_slices+1));
   _mi_stat_increase(&tld->stats->page_committed, mi_segment_info_size(segment));
 
-  // set up guard pages
   size_t guard_slices = 0;
   if (MI_SECURE>0) {
-    // in secure mode, we set up a protected page in between the segment info
-    // and the page data, and at the end of the segment.
     size_t os_pagesize = _mi_os_page_size();
     _mi_os_protect((uint8_t*)segment + mi_segment_info_size(segment) - os_pagesize, os_pagesize);
     uint8_t* end = (uint8_t*)segment + mi_segment_size(segment) - os_pagesize;
     mi_segment_ensure_committed(segment, end, os_pagesize);
     _mi_os_protect(end, os_pagesize);
-    if (slice_entries == segment_slices) segment->slice_entries--; // don't use the last slice :-(
+    if (slice_entries == segment_slices) segment->slice_entries--;
     guard_slices = 1;
   }
 
-  // reserve first slices for segment info
   mi_page_t* page0 = mi_segment_span_allocate(segment, 0, info_slices);
-  mi_assert_internal(page0!=NULL); if (page0==NULL) return NULL; // cannot fail as we always commit in advance
+  mi_assert_internal(page0!=NULL); if (page0==NULL) return NULL;
   mi_assert_internal(segment->used == 1);
-  segment->used = 0; // don't count our internal slices towards usage
+  segment->used = 0;
 
-  // initialize initial free pages
-  if (segment->kind == MI_SEGMENT_NORMAL) { // not a huge page
+  if (segment->kind == MI_SEGMENT_NORMAL) {
     mi_assert_internal(huge_page==NULL);
-    mi_segment_span_free(segment, info_slices, segment->slice_entries - info_slices, false /* don't purge */, tld);
+    mi_segment_span_free(segment, info_slices, segment->slice_entries - info_slices, false, tld);
   }
   else {
     mi_assert_internal(huge_page!=NULL);
     mi_assert_internal(mi_commit_mask_is_empty(&segment->purge_mask));
     mi_assert_internal(mi_commit_mask_is_full(&segment->commit_mask));
     *huge_page = mi_segment_span_allocate(segment, info_slices, segment_slices - info_slices - guard_slices);
-    mi_assert_internal(*huge_page != NULL); // cannot fail as we commit in advance
+    mi_assert_internal(*huge_page != NULL);
   }
 
   mi_assert_expensive(mi_segment_is_valid(segment,tld));
@@ -978,10 +887,8 @@ static void mi_segment_free(mi_segment_t
   mi_assert_internal(segment->next == NULL);
   mi_assert_internal(segment->used == 0);
 
-  // in `mi_segment_force_abandon` we set this to true to ensure the segment's memory stays valid
   if (segment->dont_free) return;
 
-  // Remove the free pages
   mi_slice_t* slice = &segment->slices[0];
   const mi_slice_t* end = mi_segment_slices_end(segment);
   #if MI_DEBUG>1
@@ -990,7 +897,7 @@ static void mi_segment_free(mi_segment_t
   while (slice < end) {
     mi_assert_internal(slice->slice_count > 0);
     mi_assert_internal(slice->slice_offset == 0);
-    mi_assert_internal(mi_slice_index(slice)==0 || slice->block_size == 0); // no more used pages ..
+    mi_assert_internal(mi_slice_index(slice)==0 || slice->block_size == 0);
     if (slice->block_size == 0 && segment->kind != MI_SEGMENT_HUGE) {
       mi_segment_span_remove_from_queue(slice, tld);
     }
@@ -999,12 +906,8 @@ static void mi_segment_free(mi_segment_t
     #endif
     slice = slice + slice->slice_count;
   }
-  mi_assert_internal(page_count == 2); // first page is allocated by the segment itself
+  mi_assert_internal(page_count == 2);
 
-  // stats
-  // _mi_stat_decrease(&tld->stats->page_committed, mi_segment_info_size(segment));
-
-  // return it to the OS
   mi_segment_os_free(segment, tld);
 }
 
@@ -1015,7 +918,6 @@ static void mi_segment_free(mi_segment_t
 
 static void mi_segment_abandon(mi_segment_t* segment, mi_segments_tld_t* tld);
 
-// note: can be called on abandoned pages
 static mi_slice_t* mi_segment_page_clear(mi_page_t* page, mi_segments_tld_t* tld) {
   mi_assert_internal(page->block_size > 0);
   mi_assert_internal(mi_page_all_free(page));
@@ -1026,15 +928,13 @@ static mi_slice_t* mi_segment_page_clear
   _mi_stat_decrease(&tld->stats->page_committed, inuse);
   _mi_stat_decrease(&tld->stats->pages, 1);
   _mi_stat_decrease(&tld->stats->page_bins[_mi_page_stats_bin(page)], 1);
-  
-  // reset the page memory to reduce memory pressure?
+
   if (segment->allow_decommit && mi_option_is_enabled(mi_option_deprecated_page_reset)) {
     size_t psize;
     uint8_t* start = _mi_segment_page_start(segment, page, &psize);
     _mi_os_reset(start, psize);
   }
 
-  // zero the page data, but not the segment fields and heap tag
   page->is_zero_init = false;
   uint8_t heap_tag = page->heap_tag;
   ptrdiff_t ofs = offsetof(mi_page_t, capacity);
@@ -1042,13 +942,10 @@ static mi_slice_t* mi_segment_page_clear
   page->block_size = 1;
   page->heap_tag = heap_tag;
 
-  // and free it
   mi_slice_t* slice = mi_segment_span_free_coalesce(mi_page_to_slice(page), tld);
   segment->used--;
   segment->free_is_zero = false;
 
-  // cannot assert segment valid as it is called during reclaim
-  // mi_assert_expensive(mi_segment_is_valid(segment, tld));
   return slice;
 }
 
@@ -1058,45 +955,23 @@ void _mi_segment_page_free(mi_page_t* pa
   mi_segment_t* segment = _mi_page_segment(page);
   mi_assert_expensive(mi_segment_is_valid(segment,tld));
 
-  // mark it as free now
   mi_segment_page_clear(page, tld);
   mi_assert_expensive(mi_segment_is_valid(segment, tld));
 
   if (segment->used == 0) {
-    // no more used pages; remove from the free list and free the segment
     mi_segment_free(segment, force, tld);
   }
   else if (segment->used == segment->abandoned) {
-    // only abandoned pages; remove from free list and abandon
     mi_segment_abandon(segment,tld);
   }
   else {
-    // perform delayed purges
-    mi_segment_try_purge(segment, false /* force? */);
+    mi_segment_try_purge(segment, false);
   }
 }
 
 
 /* -----------------------------------------------------------
 Abandonment
-
-When threads terminate, they can leave segments with
-live blocks (reachable through other threads). Such segments
-are "abandoned" and will be reclaimed by other threads to
-reuse their pages and/or free them eventually. The
-`thread_id` of such segments is 0.
-
-When a block is freed in an abandoned segment, the segment
-is reclaimed into that thread.
-
-Moreover, if threads are looking for a fresh segment, they
-will first consider abandoned segments -- these can be found
-by scanning the arena memory
-(segments outside arena memoryare only reclaimed by a free).
------------------------------------------------------------ */
-
-/* -----------------------------------------------------------
-   Abandon segment/page
 ----------------------------------------------------------- */
 
 static void mi_segment_abandon(mi_segment_t* segment, mi_segments_tld_t* tld) {
@@ -1105,30 +980,25 @@ static void mi_segment_abandon(mi_segmen
   mi_assert_internal(segment->abandoned_visits == 0);
   mi_assert_expensive(mi_segment_is_valid(segment,tld));
 
-  // remove the free pages from the free page queues
   mi_slice_t* slice = &segment->slices[0];
   const mi_slice_t* end = mi_segment_slices_end(segment);
   while (slice < end) {
     mi_assert_internal(slice->slice_count > 0);
     mi_assert_internal(slice->slice_offset == 0);
-    if (slice->block_size == 0) { // a free page
+    if (slice->block_size == 0) {
       mi_segment_span_remove_from_queue(slice,tld);
-      slice->block_size = 0; // but keep it free
+      slice->block_size = 0;
     }
     slice = slice + slice->slice_count;
   }
 
-  // perform delayed decommits (forcing is much slower on mstress)
-  // Only abandoned segments in arena memory can be reclaimed without a free
-  // so if a segment is not from an arena we force purge here to be conservative.
   const bool force_purge = (segment->memid.memkind != MI_MEM_ARENA) || mi_option_is_enabled(mi_option_abandoned_page_purge);
   mi_segment_try_purge(segment, force_purge);
 
-  // all pages in the segment are abandoned; add it to the abandoned list
   _mi_stat_increase(&tld->stats->segments_abandoned, 1);
   mi_segments_track_size(-((long)mi_segment_size(segment)), tld);
   segment->thread_id = 0;
-  segment->abandoned_visits = 1;   // from 0 to 1 to signify it is abandoned
+  segment->abandoned_visits = 1;
   if (segment->was_reclaimed) {
     tld->reclaim_count--;
     segment->was_reclaimed = false;
@@ -1148,7 +1018,6 @@ void _mi_segment_page_abandon(mi_page_t*
   _mi_stat_increase(&tld->stats->pages_abandoned, 1);
   mi_assert_internal(segment->abandoned <= segment->used);
   if (segment->used == segment->abandoned) {
-    // all pages are abandoned, abandon the entire segment
     mi_segment_abandon(segment, tld);
   }
 }
@@ -1160,45 +1029,39 @@ void _mi_segment_page_abandon(mi_page_t*
 static mi_slice_t* mi_slices_start_iterate(mi_segment_t* segment, const mi_slice_t** end) {
   mi_slice_t* slice = &segment->slices[0];
   *end = mi_segment_slices_end(segment);
-  mi_assert_internal(slice->slice_count>0 && slice->block_size>0); // segment allocated page
-  slice = slice + slice->slice_count; // skip the first segment allocated page
+  mi_assert_internal(slice->slice_count>0 && slice->block_size>0);
+  slice = slice + slice->slice_count;
   return slice;
 }
 
-// Possibly free pages and check if free space is available
 static bool mi_segment_check_free(mi_segment_t* segment, size_t slices_needed, size_t block_size, mi_segments_tld_t* tld)
 {
   mi_assert_internal(mi_segment_is_abandoned(segment));
   bool has_page = false;
 
-  // for all slices
   const mi_slice_t* end;
   mi_slice_t* slice = mi_slices_start_iterate(segment, &end);
   while (slice < end) {
     mi_assert_internal(slice->slice_count > 0);
     mi_assert_internal(slice->slice_offset == 0);
-    if (mi_slice_is_used(slice)) { // used page
-      // ensure used count is up to date and collect potential concurrent frees
+    if (mi_slice_is_used(slice)) {
       mi_page_t* const page = mi_slice_to_page(slice);
       _mi_page_free_collect(page, false);
       if (mi_page_all_free(page)) {
-        // if this page is all free now, free it without adding to any queues (yet)
         mi_assert_internal(page->next == NULL && page->prev==NULL);
         _mi_stat_decrease(&tld->stats->pages_abandoned, 1);
         segment->abandoned--;
-        slice = mi_segment_page_clear(page, tld); // re-assign slice due to coalesce!
+        slice = mi_segment_page_clear(page, tld);
         mi_assert_internal(!mi_slice_is_used(slice));
         if (slice->slice_count >= slices_needed) {
           has_page = true;
         }
       }
       else if (mi_page_block_size(page) == block_size && mi_page_has_any_available(page)) {
-        // a page has available free blocks of the right size
         has_page = true;
       }
     }
     else {
-      // empty span
       if (slice->slice_count >= slices_needed) {
         has_page = true;
       }
@@ -1208,13 +1071,10 @@ static bool mi_segment_check_free(mi_seg
   return has_page;
 }
 
-// Reclaim an abandoned segment; returns NULL if the segment was freed
-// set `right_page_reclaimed` to `true` if it reclaimed a page of the right `block_size` that was not full.
 static mi_segment_t* mi_segment_reclaim(mi_segment_t* segment, mi_heap_t* heap, size_t requested_block_size, bool* right_page_reclaimed, mi_segments_tld_t* tld) {
   if (right_page_reclaimed != NULL) { *right_page_reclaimed = false; }
-  // can be 0 still with abandoned_next, or already a thread id for segments outside an arena that are reclaimed on a free.
   mi_assert_internal(mi_atomic_load_relaxed(&segment->thread_id) == 0 || mi_atomic_load_relaxed(&segment->thread_id) == _mi_thread_id());
-  mi_assert_internal(segment->subproc == heap->tld->segments.subproc); // only reclaim within the same subprocess
+  mi_assert_internal(segment->subproc == heap->tld->segments.subproc);
   mi_atomic_store_release(&segment->thread_id, _mi_thread_id());
   segment->abandoned_visits = 0;
   segment->was_reclaimed = true;
@@ -1223,14 +1083,12 @@ static mi_segment_t* mi_segment_reclaim(
   mi_assert_internal(segment->next == NULL);
   _mi_stat_decrease(&tld->stats->segments_abandoned, 1);
 
-  // for all slices
   const mi_slice_t* end;
   mi_slice_t* slice = mi_slices_start_iterate(segment, &end);
   while (slice < end) {
     mi_assert_internal(slice->slice_count > 0);
     mi_assert_internal(slice->slice_offset == 0);
     if (mi_slice_is_used(slice)) {
-      // in use: reclaim the page in our heap
       mi_page_t* page = mi_slice_to_page(slice);
       mi_assert_internal(page->is_committed);
       mi_assert_internal(mi_page_thread_free_flag(page)==MI_NEVER_DELAYED_FREE);
@@ -1238,22 +1096,18 @@ static mi_segment_t* mi_segment_reclaim(
       mi_assert_internal(page->next == NULL && page->prev==NULL);
       _mi_stat_decrease(&tld->stats->pages_abandoned, 1);
       segment->abandoned--;
-      // get the target heap for this thread which has a matching heap tag (so we reclaim into a matching heap)
-      mi_heap_t* target_heap = _mi_heap_by_tag(heap, page->heap_tag);  // allow custom heaps to separate objects
+      mi_heap_t* target_heap = _mi_heap_by_tag(heap, page->heap_tag);
       if (target_heap == NULL) {
         target_heap = heap;
         _mi_error_message(EFAULT, "page with tag %u cannot be reclaimed by a heap with the same tag (using heap tag %u instead)\n", page->heap_tag, heap->tag );
       }
-      // associate the heap with this page, and allow heap thread delayed free again.
       mi_page_set_heap(page, target_heap);
-      _mi_page_use_delayed_free(page, MI_USE_DELAYED_FREE, true); // override never (after heap is set)
-      _mi_page_free_collect(page, false); // ensure used count is up to date
+      _mi_page_use_delayed_free(page, MI_USE_DELAYED_FREE, true);
+      _mi_page_free_collect(page, false);
       if (mi_page_all_free(page)) {
-        // if everything free by now, free the page
-        slice = mi_segment_page_clear(page, tld);   // set slice again due to coalesceing
+        slice = mi_segment_page_clear(page, tld);
       }
       else {
-        // otherwise reclaim it into the heap
         _mi_page_reclaim(target_heap, page);
         if (requested_block_size == mi_page_block_size(page) && mi_page_has_any_available(page) && heap == target_heap) {
           if (right_page_reclaimed != NULL) { *right_page_reclaimed = true; }
@@ -1261,8 +1115,7 @@ static mi_segment_t* mi_segment_reclaim(
       }
     }
     else {
-      // the span is free, add it to our page queues
-      slice = mi_segment_span_free_coalesce(slice, tld); // set slice again due to coalesceing
+      slice = mi_segment_span_free_coalesce(slice, tld);
     }
     mi_assert_internal(slice->slice_count>0 && slice->slice_offset==0);
     slice = slice + slice->slice_count;
@@ -1270,7 +1123,7 @@ static mi_segment_t* mi_segment_reclaim(
 
   mi_assert(segment->abandoned == 0);
   mi_assert_expensive(mi_segment_is_valid(segment, tld));
-  if (segment->used == 0) {  // due to page_clear
+  if (segment->used == 0) {
     mi_assert_internal(right_page_reclaimed == NULL || !(*right_page_reclaimed));
     mi_segment_free(segment, false, tld);
     return NULL;
@@ -1281,21 +1134,17 @@ static mi_segment_t* mi_segment_reclaim(
 }
 
 
-// attempt to reclaim a particular segment (called from multi threaded free `alloc.c:mi_free_block_mt`)
 bool _mi_segment_attempt_reclaim(mi_heap_t* heap, mi_segment_t* segment) {
-  if (mi_atomic_load_relaxed(&segment->thread_id) != 0) return false;  // it is not abandoned
-  if (segment->subproc != heap->tld->segments.subproc)  return false;  // only reclaim within the same subprocess
-  if (!_mi_heap_memid_is_suitable(heap,segment->memid)) return false;  // don't reclaim between exclusive and non-exclusive arena's
+  if (mi_atomic_load_relaxed(&segment->thread_id) != 0) return false;
+  if (segment->subproc != heap->tld->segments.subproc)  return false;
+  if (!_mi_heap_memid_is_suitable(heap,segment->memid)) return false;
   const long target = _mi_option_get_fast(mi_option_target_segments_per_thread);
-  if (target > 0 && (size_t)target <= heap->tld->segments.count) return false; // don't reclaim if going above the target count
+  if (target > 0 && (size_t)target <= heap->tld->segments.count) return false;
 
-  // don't reclaim more from a `free` call than half the current segments
-  // this is to prevent a pure free-ing thread to start owning too many segments
-  // (but not for out-of-arena segments as that is the main way to be reclaimed for those)
   if (segment->memid.memkind == MI_MEM_ARENA && heap->tld->segments.reclaim_count * 2 > heap->tld->segments.count) {
     return false;
   }
-  if (_mi_arena_segment_clear_abandoned(segment)) {  // atomically unabandon
+  if (_mi_arena_segment_clear_abandoned(segment)) {
     mi_segment_t* res = mi_segment_reclaim(segment, heap, 0, NULL, &heap->tld->segments);
     mi_assert_internal(res == segment);
     return (res != NULL);
@@ -1306,7 +1155,7 @@ bool _mi_segment_attempt_reclaim(mi_heap
 void _mi_abandoned_reclaim_all(mi_heap_t* heap, mi_segments_tld_t* tld) {
   mi_segment_t* segment;
   mi_arena_field_cursor_t current;
-  _mi_arena_field_cursor_init(heap, tld->subproc, true /* visit all, blocking */, &current);
+  _mi_arena_field_cursor_init(heap, tld->subproc, true, &current);
   while ((segment = _mi_arena_segment_clear_abandoned_next(&current)) != NULL) {
     mi_segment_reclaim(segment, heap, 0, NULL, tld);
   }
@@ -1321,12 +1170,11 @@ static bool segment_count_is_within_targ
 }
 
 static long mi_segment_get_reclaim_tries(mi_segments_tld_t* tld) {
-  // limit the tries to 10% (default) of the abandoned segments with at least 8 and at most 1024 tries.
   const size_t perc = (size_t)mi_option_get_clamp(mi_option_max_segment_reclaim, 0, 100);
   if (perc <= 0) return 0;
   const size_t total_count = mi_atomic_load_relaxed(&tld->subproc->abandoned_count);
   if (total_count == 0) return 0;
-  const size_t relative_count = (total_count > 10000 ? (total_count / 100) * perc : (total_count * perc) / 100); // avoid overflow
+  const size_t relative_count = (total_count > 10000 ? (total_count / 100) * perc : (total_count * perc) / 100);
   long max_tries = (long)(relative_count <= 1 ? 1 : (relative_count > 1024 ? 1024 : relative_count));
   if (max_tries < 8 && total_count > 8) { max_tries = 8;  }
   return max_tries;
@@ -1344,36 +1192,23 @@ static mi_segment_t* mi_segment_try_recl
   _mi_arena_field_cursor_init(heap, tld->subproc, false /* non-blocking */, &current);
   while (segment_count_is_within_target(tld,NULL) && (max_tries-- > 0) && ((segment = _mi_arena_segment_clear_abandoned_next(&current)) != NULL))
   {
-    mi_assert(segment->subproc == heap->tld->segments.subproc); // cursor only visits segments in our sub-process
+    mi_assert(segment->subproc == heap->tld->segments.subproc);
     segment->abandoned_visits++;
-    // todo: should we respect numa affinity for abandoned reclaim? perhaps only for the first visit?
-    // todo: an arena exclusive heap will potentially visit many abandoned unsuitable segments and use many tries
-    // Perhaps we can skip non-suitable ones in a better way?
     bool is_suitable = _mi_heap_memid_is_suitable(heap, segment->memid);
-    bool has_page = mi_segment_check_free(segment,needed_slices,block_size,tld); // try to free up pages (due to concurrent frees)
+    bool has_page = mi_segment_check_free(segment,needed_slices,block_size,tld);
     if (segment->used == 0) {
-      // free the segment (by forced reclaim) to make it available to other threads.
-      // note1: we prefer to free a segment as that might lead to reclaiming another
-      // segment that is still partially used.
-      // note2: we could in principle optimize this by skipping reclaim and directly
-      // freeing but that would violate some invariants temporarily)
       mi_segment_reclaim(segment, heap, 0, NULL, tld);
     }
     else if (has_page && is_suitable) {
-      // found a large enough free span, or a page of the right block_size with free space
-      // we return the result of reclaim (which is usually `segment`) as it might free
-      // the segment due to concurrent frees (in which case `NULL` is returned).
       result = mi_segment_reclaim(segment, heap, block_size, reclaimed, tld);
       break;
     }
     else if (segment->abandoned_visits > 3 && is_suitable) {
-      // always reclaim on 3rd visit to limit the abandoned segment count.
       mi_segment_reclaim(segment, heap, 0, NULL, tld);
     }
     else {
-      // otherwise, push on the visited list so it gets not looked at too quickly again
-      max_tries++; // don't count this as a try since it was not suitable
-      mi_segment_try_purge(segment, false /* true force? */); // force purge if needed as we may not visit soon again
+      max_tries++;
+      mi_segment_try_purge(segment, false);
       _mi_arena_segment_mark_abandoned(segment);
     }
   }
@@ -1381,23 +1216,17 @@ static mi_segment_t* mi_segment_try_recl
   return result;
 }
 
-// collect abandoned segments
 void _mi_abandoned_collect(mi_heap_t* heap, bool force, mi_segments_tld_t* tld)
 {
   mi_segment_t* segment;
-  mi_arena_field_cursor_t current; _mi_arena_field_cursor_init(heap, tld->subproc, force /* blocking? */, &current);
-  long max_tries = (force ? (long)mi_atomic_load_relaxed(&tld->subproc->abandoned_count) : 1024);  // limit latency
+  mi_arena_field_cursor_t current; _mi_arena_field_cursor_init(heap, tld->subproc, force, &current);
+  long max_tries = (force ? (long)mi_atomic_load_relaxed(&tld->subproc->abandoned_count) : 1024);
   while ((max_tries-- > 0) && ((segment = _mi_arena_segment_clear_abandoned_next(&current)) != NULL)) {
-    mi_segment_check_free(segment,0,0,tld); // try to free up pages (due to concurrent frees)
+    mi_segment_check_free(segment,0,0,tld);
     if (segment->used == 0) {
-      // free the segment (by forced reclaim) to make it available to other threads.
-      // note: we could in principle optimize this by skipping reclaim and directly
-      // freeing but that would violate some invariants temporarily)
       mi_segment_reclaim(segment, heap, 0, NULL, tld);
     }
     else {
-      // otherwise, purge if needed and push on the visited list
-      // note: forced purge can be expensive if many threads are destroyed/created as in mstress.
       mi_segment_try_purge(segment, force);
       _mi_arena_segment_mark_abandoned(segment);
     }
@@ -1409,39 +1238,30 @@ void _mi_abandoned_collect(mi_heap_t* he
    Force abandon a segment that is in use by our thread
 ----------------------------------------------------------- */
 
-// force abandon a segment
 static void mi_segment_force_abandon(mi_segment_t* segment, mi_segments_tld_t* tld)
 {
   mi_assert_internal(!mi_segment_is_abandoned(segment));
   mi_assert_internal(!segment->dont_free);
 
-  // ensure the segment does not get free'd underneath us (so we can check if a page has been freed in `mi_page_force_abandon`)
   segment->dont_free = true;
 
-  // for all slices
   const mi_slice_t* end;
   mi_slice_t* slice = mi_slices_start_iterate(segment, &end);
   while (slice < end) {
     mi_assert_internal(slice->slice_count > 0);
     mi_assert_internal(slice->slice_offset == 0);
     if (mi_slice_is_used(slice)) {
-      // ensure used count is up to date and collect potential concurrent frees
       mi_page_t* const page = mi_slice_to_page(slice);
       _mi_page_free_collect(page, false);
       {
-        // abandon the page if it is still in-use (this will free it if possible as well)
         mi_assert_internal(segment->used > 0);
         if (segment->used == segment->abandoned+1) {
-          // the last page.. abandon and return as the segment will be abandoned after this
-          // and we should no longer access it.
           segment->dont_free = false;
           _mi_page_force_abandon(page);
           return;
         }
         else {
-          // abandon and continue
           _mi_page_force_abandon(page);
-          // it might be freed, reset the slice (note: relies on coalesce setting the slice_offset)
           slice = mi_slice_first(slice);
         }
       }
@@ -1451,23 +1271,18 @@ static void mi_segment_force_abandon(mi_
   segment->dont_free = false;
   mi_assert(segment->used == segment->abandoned);
   mi_assert(segment->used == 0);
-  if (segment->used == 0) {  // paranoia
-    // all free now
+  if (segment->used == 0) {
     mi_segment_free(segment, false, tld);
   }
   else {
-    // perform delayed purges
-    mi_segment_try_purge(segment, false /* force? */);
+    mi_segment_try_purge(segment, false);
   }
 }
 
 
-// try abandon segments.
-// this should be called from `reclaim_or_alloc` so we know all segments are (about) fully in use.
 static void mi_segments_try_abandon_to_target(mi_heap_t* heap, size_t target, mi_segments_tld_t* tld) {
   if (target <= 1) return;
-  const size_t min_target = (target > 4 ? (target*3)/4 : target);  // 75%
-  // todo: we should maintain a list of segments per thread; for now, only consider segments from the heap full pages
+  const size_t min_target = (target > 4 ? (target*3)/4 : target);
   for (int i = 0; i < 64 && tld->count >= min_target; i++) {
     mi_page_t* page = heap->pages[MI_BIN_FULL].first;
     while (page != NULL && mi_page_block_size(page) > MI_LARGE_OBJ_SIZE_MAX) {
@@ -1478,14 +1293,11 @@ static void mi_segments_try_abandon_to_t
     }
     mi_segment_t* segment = _mi_page_segment(page);
     mi_segment_force_abandon(segment, tld);
-    mi_assert_internal(page != heap->pages[MI_BIN_FULL].first); // as it is just abandoned
+    mi_assert_internal(page != heap->pages[MI_BIN_FULL].first);
   }
 }
 
-// try abandon segments.
-// this should be called from `reclaim_or_alloc` so we know all segments are (about) fully in use.
 static void mi_segments_try_abandon(mi_heap_t* heap, mi_segments_tld_t* tld) {
-  // we call this when we are about to add a fresh segment so we should be under our target segment count.
   size_t target = 0;
   if (segment_count_is_within_target(tld, &target)) return;
   mi_segments_try_abandon_to_target(heap, target, tld);
@@ -1510,22 +1322,17 @@ static mi_segment_t* mi_segment_reclaim_
 {
   mi_assert_internal(block_size <= MI_LARGE_OBJ_SIZE_MAX);
 
-  // try to abandon some segments to increase reuse between threads
   mi_segments_try_abandon(heap,tld);
 
-  // 1. try to reclaim an abandoned segment
   bool reclaimed;
   mi_segment_t* segment = mi_segment_try_reclaim(heap, needed_slices, block_size, &reclaimed, tld);
   if (reclaimed) {
-    // reclaimed the right page right into the heap
     mi_assert_internal(segment != NULL);
-    return NULL; // pretend out-of-memory as the page will be in the page queue of the heap with available blocks
+    return NULL;
   }
   else if (segment != NULL) {
-    // reclaimed a segment with a large enough empty span in it
     return segment;
   }
-  // 2. otherwise allocate a fresh segment
   return mi_segment_alloc(0, 0, heap->arena_id, tld, NULL);
 }
 
@@ -1538,19 +1345,15 @@ static mi_page_t* mi_segments_page_alloc
 {
   mi_assert_internal(required <= MI_LARGE_OBJ_SIZE_MAX && page_kind <= MI_PAGE_LARGE);
 
-  // find a free page
   size_t page_size = _mi_align_up(required, (required > MI_MEDIUM_PAGE_SIZE ? MI_MEDIUM_PAGE_SIZE : MI_SEGMENT_SLICE_SIZE));
   size_t slices_needed = page_size / MI_SEGMENT_SLICE_SIZE;
   mi_assert_internal(slices_needed * MI_SEGMENT_SLICE_SIZE == page_size);
-  mi_page_t* page = mi_segments_page_find_and_allocate(slices_needed, heap->arena_id, tld); //(required <= MI_SMALL_SIZE_MAX ? 0 : slices_needed), tld);
+  mi_page_t* page = mi_segments_page_find_and_allocate(slices_needed, heap->arena_id, tld);
   if (page==NULL) {
-    // no free page, allocate a new segment and try again
     if (mi_segment_reclaim_or_alloc(heap, slices_needed, block_size, tld) == NULL) {
-      // OOM or reclaimed a good page in the heap
       return NULL;
     }
     else {
-      // otherwise try again
       return mi_segments_page_alloc(heap, page_kind, required, block_size, tld);
     }
   }
@@ -1574,40 +1377,35 @@ static mi_page_t* mi_segment_huge_page_a
   mi_assert_internal(segment->used==1);
   mi_assert_internal(mi_page_block_size(page) >= size);
   #if MI_HUGE_PAGE_ABANDON
-  segment->thread_id = 0; // huge segments are immediately abandoned
+  segment->thread_id = 0;
   #endif
 
-  // for huge pages we initialize the block_size as we may
-  // overallocate to accommodate large alignments.
   size_t psize;
   uint8_t* start = _mi_segment_page_start(segment, page, &psize);
   page->block_size = psize;
   mi_assert_internal(page->is_huge);
 
-  // decommit the part of the prefix of a page that will not be used; this can be quite large (close to MI_SEGMENT_SIZE)
   if (page_alignment > 0 && segment->allow_decommit) {
     uint8_t* aligned_p = (uint8_t*)_mi_align_up((uintptr_t)start, page_alignment);
     mi_assert_internal(_mi_is_aligned(aligned_p, page_alignment));
     mi_assert_internal(psize - (aligned_p - start) >= size);
-    uint8_t* decommit_start = start + sizeof(mi_block_t);              // for the free list
+    uint8_t* decommit_start = start + sizeof(mi_block_t);
     ptrdiff_t decommit_size = aligned_p - decommit_start;
-    _mi_os_reset(decommit_start, decommit_size);   // note: cannot use segment_decommit on huge segments
+    if (decommit_size > 0) {
+      _mi_os_reset(decommit_start, decommit_size);
+    }
   }
 
   return page;
 }
 
 #if MI_HUGE_PAGE_ABANDON
-// free huge block from another thread
 void _mi_segment_huge_page_free(mi_segment_t* segment, mi_page_t* page, mi_block_t* block) {
-  // huge page segments are always abandoned and can be freed immediately by any thread
   mi_assert_internal(segment->kind==MI_SEGMENT_HUGE);
   mi_assert_internal(segment == _mi_page_segment(page));
   mi_assert_internal(mi_atomic_load_relaxed(&segment->thread_id)==0);
 
-  // claim it and free
-  mi_heap_t* heap = mi_heap_get_default(); // issue #221; don't use the internal get_default_heap as we need to ensure the thread is initialized.
-  // paranoia: if this it the last reference, the cas should always succeed
+  mi_heap_t* heap = mi_heap_get_default();
   size_t expected_tid = 0;
   if (mi_atomic_cas_strong_acq_rel(&segment->thread_id, &expected_tid, heap->thread_id)) {
     mi_block_set_next(page, block, page->free);
@@ -1626,19 +1424,18 @@ void _mi_segment_huge_page_free(mi_segme
 }
 
 #else
-// reset memory of a huge block from another thread
 void _mi_segment_huge_page_reset(mi_segment_t* segment, mi_page_t* page, mi_block_t* block) {
   MI_UNUSED(page);
   mi_assert_internal(segment->kind == MI_SEGMENT_HUGE);
   mi_assert_internal(segment == _mi_page_segment(page));
-  mi_assert_internal(page->used == 1); // this is called just before the free
+  mi_assert_internal(page->used == 1);
   mi_assert_internal(page->free == NULL);
   if (segment->allow_decommit) {
     size_t csize = mi_usable_size(block);
     if (csize > sizeof(mi_block_t)) {
       csize = csize - sizeof(mi_block_t);
       uint8_t* p = (uint8_t*)block + sizeof(mi_block_t);
-      _mi_os_reset(p, csize);  // note: cannot use segment_decommit on huge segments
+      _mi_os_reset(p, csize);
     }
   }
 }
