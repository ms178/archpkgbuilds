--- a/src/page.c	2026-01-23 12:49:10.316203953 +0100
+++ b/src/page.c	2026-01-23 13:28:05.339102905 +0100


--- a/src/segment.c	2026-01-23 12:49:10.316203953 +0100
+++ b/src/segment.c	2026-01-23 13:28:05.339102905 +0100
@@ -11,18 +11,12 @@ terms of the MIT license. A copy of the
 #include <string.h>  // memset
 #include <stdio.h>
 
-// -------------------------------------------------------------------
-// Segments
-// mimalloc pages reside in segments. See `mi_segment_valid` for invariants.
-// -------------------------------------------------------------------
-
-
+/* Forward declaration */
 static void mi_segment_try_purge(mi_segment_t* segment, bool force);
 
-
-// -------------------------------------------------------------------
-// commit mask
-// -------------------------------------------------------------------
+/* ---------------------------------------------------------------------------
+  Commit Mask Operations
+--------------------------------------------------------------------------- */
 
 static bool mi_commit_mask_all_set(const mi_commit_mask_t* commit, const mi_commit_mask_t* cm) {
   for (size_t i = 0; i < MI_COMMIT_MASK_FIELD_COUNT; i++) {
@@ -92,12 +86,10 @@ size_t _mi_commit_mask_committed_size(co
       count += MI_COMMIT_MASK_FIELD_BITS;
     }
     else {
-      for (; mask != 0; mask >>= 1) {  // todo: use popcount
-        if ((mask&1)!=0) count++;
-      }
+      // Use compiler builtin for popcount (Raptor Lake has hardware POPCNT)
+      count += mi_popcount(mask);
     }
   }
-  // we use total since for huge segments each commit bit may represent a larger size
   return ((total / MI_COMMIT_MASK_BITS) * count);
 }
 
@@ -111,10 +103,11 @@ size_t _mi_commit_mask_next_run(const mi
     mask = cm->mask[i];
     mask >>= ofs;
     if (mask != 0) {
-      while ((mask&1) == 0) {
-        mask >>= 1;
-        ofs++;
-      }
+      // found set bit, skip trailing zeros
+      // mi_ctz is efficient on modern CPUs (TZCNT)
+      size_t zeros = mi_ctz(mask);
+      mask >>= zeros;
+      ofs += zeros;
       break;
     }
     i++;
@@ -126,22 +119,26 @@ size_t _mi_commit_mask_next_run(const mi
     return 0;
   }
   else {
-    // found, count ones
+    // found, count consecutive ones
     size_t count = 0;
     *idx = (i*MI_COMMIT_MASK_FIELD_BITS) + ofs;
-    do {
-      mi_assert_internal(ofs < MI_COMMIT_MASK_FIELD_BITS && (mask&1) == 1);
-      do {
-        count++;
-        mask >>= 1;
-      } while ((mask&1) == 1);
-      if ((((*idx + count) % MI_COMMIT_MASK_FIELD_BITS) == 0)) {
+
+    // Count trailing ones using bitwise tricks (mask is shifted so LSB is 1)
+    // ~mask has 0 at LSB. mi_ctz(~mask) counts consecutive 1s.
+    size_t ones = mi_ctz(~mask);
+    count += ones;
+
+    // If we consumed the rest of the field, check next fields
+    if ((ofs + count) == MI_COMMIT_MASK_FIELD_BITS) {
         i++;
-        if (i >= MI_COMMIT_MASK_FIELD_COUNT) break;
-        mask = cm->mask[i];
-        ofs = 0;
-      }
-    } while ((mask&1) == 1);
+        while (i < MI_COMMIT_MASK_FIELD_COUNT && cm->mask[i] == ~((size_t)0)) {
+            count += MI_COMMIT_MASK_FIELD_BITS;
+            i++;
+        }
+        if (i < MI_COMMIT_MASK_FIELD_COUNT) {
+            count += mi_ctz(~cm->mask[i]);
+        }
+    }
     mi_assert_internal(count > 0);
     return count;
   }
@@ -149,32 +146,9 @@ size_t _mi_commit_mask_next_run(const mi
 
 
 /* --------------------------------------------------------------------------------
-  Segment allocation
-  We allocate pages inside bigger "segments" (32 MiB on 64-bit). This is to avoid
-  splitting VMA's on Linux and reduce fragmentation on other OS's.
-  Each thread owns its own segments.
-
-  Currently we have:
-  - small pages (64KiB)
-  - medium pages (512KiB)
-  - large pages (4MiB),
-  - huge segments have 1 page in one segment that can be larger than `MI_SEGMENT_SIZE`.
-    it is used for blocks `> MI_LARGE_OBJ_SIZE_MAX` or with alignment `> MI_BLOCK_ALIGNMENT_MAX`.
-
-  The memory for a segment is usually committed on demand.
-  (i.e. we are careful to not touch the memory until we actually allocate a block there)
-
-  If a  thread ends, it "abandons" pages that still contain live blocks.
-  Such segments are abandoned and these can be reclaimed by still running threads,
-  (much like work-stealing).
+  Segment Allocation
 -------------------------------------------------------------------------------- */
 
-
-/* -----------------------------------------------------------
-   Slices
------------------------------------------------------------ */
-
-
 static const mi_slice_t* mi_segment_slices_end(const mi_segment_t* segment) {
   return &segment->slices[segment->slice_entries];
 }
@@ -189,7 +163,6 @@ static uint8_t* mi_slice_start(const mi_
 /* -----------------------------------------------------------
    Bins
 ----------------------------------------------------------- */
-// Use bit scan forward to quickly find the first zero bit if it is available
 
 static inline size_t mi_slice_bin8(size_t slice_count) {
   if (slice_count<=1) return slice_count;
@@ -222,9 +195,8 @@ static inline size_t mi_slice_index(cons
 ----------------------------------------------------------- */
 
 static void mi_span_queue_push(mi_span_queue_t* sq, mi_slice_t* slice) {
-  // todo: or push to the end?
   mi_assert_internal(slice->prev == NULL && slice->next==NULL);
-  slice->prev = NULL; // paranoia
+  slice->prev = NULL;
   slice->next = sq->first;
   sq->first = slice;
   if (slice->next != NULL) slice->next->prev = slice;
@@ -241,7 +213,6 @@ static mi_span_queue_t* mi_span_queue_fo
 
 static void mi_span_queue_delete(mi_span_queue_t* sq, mi_slice_t* slice) {
   mi_assert_internal(slice->block_size==0 && slice->slice_count>0 && slice->slice_offset==0);
-  // should work too if the queue does not contain slice (which can happen during reclaim)
   if (slice->prev != NULL) slice->prev->next = slice->next;
   if (slice == sq->first) sq->first = slice->next;
   if (slice->next != NULL) slice->next->prev = slice->prev;
@@ -274,8 +245,7 @@ static bool mi_segment_is_valid(mi_segme
   mi_assert_internal(_mi_ptr_cookie(segment) == segment->cookie);
   mi_assert_internal(segment->abandoned <= segment->used);
   mi_assert_internal(segment->thread_id == 0 || segment->thread_id == _mi_thread_id());
-  mi_assert_internal(mi_commit_mask_all_set(&segment->commit_mask, &segment->purge_mask)); // can only decommit committed blocks
-  //mi_assert_internal(segment->segment_info_size % MI_SEGMENT_SLICE_SIZE == 0);
+  mi_assert_internal(mi_commit_mask_all_set(&segment->commit_mask, &segment->purge_mask));
   mi_slice_t* slice = &segment->slices[0];
   const mi_slice_t* end = mi_segment_slices_end(segment);
   size_t used_count = 0;
@@ -285,7 +255,7 @@ static bool mi_segment_is_valid(mi_segme
     mi_assert_internal(slice->slice_offset == 0);
     size_t index = mi_slice_index(slice);
     size_t maxindex = (index + slice->slice_count >= segment->slice_entries ? segment->slice_entries : index + slice->slice_count) - 1;
-    if (mi_slice_is_used(slice)) { // a page in use, we need at least MAX_SLICE_OFFSET_COUNT valid back offsets
+    if (mi_slice_is_used(slice)) {
       used_count++;
       mi_assert_internal(slice->is_huge == (segment->kind == MI_SEGMENT_HUGE));
       for (size_t i = 0; i <= MI_MAX_SLICE_OFFSET_COUNT && index + i <= maxindex; i++) {
@@ -293,7 +263,6 @@ static bool mi_segment_is_valid(mi_segme
         mi_assert_internal(i==0 || segment->slices[index + i].slice_count == 0);
         mi_assert_internal(i==0 || segment->slices[index + i].block_size == 1);
       }
-      // and the last entry as well (for coalescing)
       const mi_slice_t* last = slice + slice->slice_count - 1;
       if (last > slice && last < mi_segment_slices_end(segment)) {
         mi_assert_internal(last->slice_offset == (slice->slice_count-1)*sizeof(mi_slice_t));
@@ -301,14 +270,14 @@ static bool mi_segment_is_valid(mi_segme
         mi_assert_internal(last->block_size == 1);
       }
     }
-    else {  // free range of slices; only last slice needs a valid back offset
+    else {
       mi_slice_t* last = &segment->slices[maxindex];
       if (segment->kind != MI_SEGMENT_HUGE || slice->slice_count <= (segment->slice_entries - segment->segment_info_slices)) {
         mi_assert_internal((uint8_t*)slice == (uint8_t*)last - last->slice_offset);
       }
       mi_assert_internal(slice == last || last->slice_count == 0 );
       mi_assert_internal(last->block_size == 0 || (segment->kind==MI_SEGMENT_HUGE && last->block_size==1));
-      if (segment->kind != MI_SEGMENT_HUGE && segment->thread_id != 0) { // segment is not huge or abandoned
+      if (segment->kind != MI_SEGMENT_HUGE && segment->thread_id != 0) {
         sq = mi_span_queue_for(slice->slice_count,tld);
         mi_assert_internal(mi_span_queue_contains(sq,slice));
       }
@@ -334,12 +303,8 @@ static uint8_t* _mi_segment_page_start_f
   const ptrdiff_t idx = slice - segment->slices;
   const size_t psize = (size_t)slice->slice_count * MI_SEGMENT_SLICE_SIZE;
   uint8_t* const pstart = (uint8_t*)segment + (idx*MI_SEGMENT_SLICE_SIZE);
-  // make the start not OS page aligned for smaller blocks to avoid page/cache effects
-  // note: the offset must always be a block_size multiple since we assume small allocations
-  // are aligned (see `mi_heap_malloc_aligned`).
   size_t start_offset = 0;
   if (block_size > 0 && block_size <= MI_MAX_ALIGN_GUARANTEE) {
-    // for small objects, ensure the page start is aligned with the block size (PR#66 by kickunderscore)
     const size_t adjust = block_size - ((uintptr_t)pstart % block_size);
     if (adjust < block_size && psize >= block_size + adjust) {
       start_offset += adjust;
@@ -373,8 +338,6 @@ static size_t mi_segment_calculate_slice
   size_t guardsize = 0;
 
   if (MI_SECURE>0) {
-    // in secure mode, we set up a protected page in between the segment info
-    // and the page data (and one at the end of the segment)
     guardsize = page_size;
     if (required > 0) {
       required = _mi_align_up(required, MI_SEGMENT_SLICE_SIZE) + page_size;
@@ -391,8 +354,6 @@ static size_t mi_segment_calculate_slice
 
 /* ----------------------------------------------------------------------------
 Segment caches
-We keep a small segment cache per thread to increase local
-reuse and avoid setting/clearing guard pages in secure mode.
 ------------------------------------------------------------------------------- */
 
 static void mi_segments_track_size(long segment_size, mi_segments_tld_t* tld) {
@@ -413,17 +374,12 @@ static void mi_segment_os_free(mi_segmen
     segment->was_reclaimed = false;
   }
   if (MI_SECURE>0) {
-    // _mi_os_unprotect(segment, mi_segment_size(segment)); // ensure no more guard pages are set
-    // unprotect the guard pages; we cannot just unprotect the whole segment size as part may be decommitted
     size_t os_pagesize = _mi_os_page_size();
     _mi_os_unprotect((uint8_t*)segment + mi_segment_info_size(segment) - os_pagesize, os_pagesize);
     uint8_t* end = (uint8_t*)segment + mi_segment_size(segment) - os_pagesize;
     _mi_os_unprotect(end, os_pagesize);
   }
 
-  // purge delayed decommits now? (no, leave it to the arena)
-  // mi_segment_try_purge(segment,true,tld->stats);
-
   const size_t size = mi_segment_size(segment);
   const size_t csize = _mi_commit_mask_committed_size(&segment->commit_mask, size);
 
@@ -449,18 +405,16 @@ static void mi_segment_commit_mask(mi_se
   size_t start;
   size_t end;
   if (conservative) {
-    // decommit conservative
     start = _mi_align_up(pstart, MI_COMMIT_SIZE);
     end   = _mi_align_down(pstart + size, MI_COMMIT_SIZE);
     mi_assert_internal(start >= segstart);
     mi_assert_internal(end <= segsize);
   }
   else {
-    // commit liberal
     start = _mi_align_down(pstart, MI_MINIMAL_COMMIT_SIZE);
     end   = _mi_align_up(pstart + size, MI_MINIMAL_COMMIT_SIZE);
   }
-  if (pstart >= segstart && start < segstart) {  // note: the mask is also calculated for an initial commit of the info area
+  if (pstart >= segstart && start < segstart) {
     start = segstart;
   }
   if (end > segsize) {
@@ -476,7 +430,7 @@ static void mi_segment_commit_mask(mi_se
   size_t bitidx = start / MI_COMMIT_SIZE;
   mi_assert_internal(bitidx < MI_COMMIT_MASK_BITS);
 
-  size_t bitcount = *full_size / MI_COMMIT_SIZE; // can be 0
+  size_t bitcount = *full_size / MI_COMMIT_SIZE;
   if (bitidx + bitcount > MI_COMMIT_MASK_BITS) {
     _mi_warning_message("commit mask overflow: idx=%zu count=%zu start=%zx end=%zx p=0x%p size=%zu fullsize=%zu\n", bitidx, bitcount, start, end, p, size, *full_size);
   }
@@ -487,7 +441,6 @@ static void mi_segment_commit_mask(mi_se
 static bool mi_segment_commit(mi_segment_t* segment, uint8_t* p, size_t size) {
   mi_assert_internal(mi_commit_mask_all_set(&segment->commit_mask, &segment->purge_mask));
 
-  // commit liberal
   uint8_t* start = NULL;
   size_t   full_size = 0;
   mi_commit_mask_t mask;
@@ -495,29 +448,25 @@ static bool mi_segment_commit(mi_segment
   if (mi_commit_mask_is_empty(&mask) || full_size == 0) return true;
 
   if (!mi_commit_mask_all_set(&segment->commit_mask, &mask)) {
-    // committing
     bool is_zero = false;
     mi_commit_mask_t cmask;
     mi_commit_mask_create_intersect(&segment->commit_mask, &mask, &cmask);
-    _mi_stat_decrease(&_mi_stats_main.committed, _mi_commit_mask_committed_size(&cmask, MI_SEGMENT_SIZE)); // adjust for overlap
+    _mi_stat_decrease(&_mi_stats_main.committed, _mi_commit_mask_committed_size(&cmask, MI_SEGMENT_SIZE));
     if (!_mi_os_commit(start, full_size, &is_zero)) return false;
     mi_commit_mask_set(&segment->commit_mask, &mask);
   }
 
-  // increase purge expiration when using part of delayed purges -- we assume more allocations are coming soon.
   if (mi_commit_mask_any_set(&segment->purge_mask, &mask)) {
     segment->purge_expire = _mi_clock_now() + mi_option_get(mi_option_purge_delay);
   }
 
-  // always clear any delayed purges in our range (as they are either committed now)
   mi_commit_mask_clear(&segment->purge_mask, &mask);
   return true;
 }
 
 static bool mi_segment_ensure_committed(mi_segment_t* segment, uint8_t* p, size_t size) {
   mi_assert_internal(mi_commit_mask_all_set(&segment->commit_mask, &segment->purge_mask));
-  // note: assumes commit_mask is always full for huge segments as otherwise the commit mask bits can overflow
-  if (mi_commit_mask_is_full(&segment->commit_mask) && mi_commit_mask_is_empty(&segment->purge_mask)) return true; // fully committed
+  if (mi_likely(mi_commit_mask_is_full(&segment->commit_mask) && mi_commit_mask_is_empty(&segment->purge_mask))) return true;
   mi_assert_internal(segment->kind != MI_SEGMENT_HUGE);
   return mi_segment_commit(segment, p, size);
 }
@@ -526,7 +475,6 @@ static bool mi_segment_purge(mi_segment_
   mi_assert_internal(mi_commit_mask_all_set(&segment->commit_mask, &segment->purge_mask));
   if (!segment->allow_purge) return true;
 
-  // purge conservative
   uint8_t* start = NULL;
   size_t   full_size = 0;
   mi_commit_mask_t mask;
@@ -534,19 +482,17 @@ static bool mi_segment_purge(mi_segment_
   if (mi_commit_mask_is_empty(&mask) || full_size==0) return true;
 
   if (mi_commit_mask_any_set(&segment->commit_mask, &mask)) {
-    // purging
     mi_assert_internal((void*)start != (void*)segment);
     mi_assert_internal(segment->allow_decommit);
-    const bool decommitted = _mi_os_purge(start, full_size);  // reset or decommit
+    const bool decommitted = _mi_os_purge(start, full_size);
     if (decommitted) {
       mi_commit_mask_t cmask;
       mi_commit_mask_create_intersect(&segment->commit_mask, &mask, &cmask);
-      _mi_stat_increase(&_mi_stats_main.committed, full_size - _mi_commit_mask_committed_size(&cmask, MI_SEGMENT_SIZE)); // adjust for double counting
+      _mi_stat_increase(&_mi_stats_main.committed, full_size - _mi_commit_mask_committed_size(&cmask, MI_SEGMENT_SIZE));
       mi_commit_mask_clear(&segment->commit_mask, &mask);
     }
   }
 
-  // always clear any scheduled purges in our range
   mi_commit_mask_clear(&segment->purge_mask, &mask);
   return true;
 }
@@ -558,34 +504,29 @@ static void mi_segment_schedule_purge(mi
     mi_segment_purge(segment, p, size);
   }
   else {
-    // register for future purge in the purge mask
     uint8_t* start = NULL;
     size_t   full_size = 0;
     mi_commit_mask_t mask;
     mi_segment_commit_mask(segment, true /*conservative*/, p, size, &start, &full_size, &mask);
     if (mi_commit_mask_is_empty(&mask) || full_size==0) return;
 
-    // update delayed commit
     mi_assert_internal(segment->purge_expire > 0 || mi_commit_mask_is_empty(&segment->purge_mask));
     mi_commit_mask_t cmask;
-    mi_commit_mask_create_intersect(&segment->commit_mask, &mask, &cmask);  // only purge what is committed; span_free may try to decommit more
+    mi_commit_mask_create_intersect(&segment->commit_mask, &mask, &cmask);
     mi_commit_mask_set(&segment->purge_mask, &cmask);
     mi_msecs_t now = _mi_clock_now();
     if (segment->purge_expire == 0) {
-      // no previous purgess, initialize now
       segment->purge_expire = now + mi_option_get(mi_option_purge_delay);
     }
     else if (segment->purge_expire <= now) {
-      // previous purge mask already expired
       if (segment->purge_expire + mi_option_get(mi_option_purge_extend_delay) <= now) {
         mi_segment_try_purge(segment, true);
       }
       else {
-        segment->purge_expire = now + mi_option_get(mi_option_purge_extend_delay); // (mi_option_get(mi_option_purge_delay) / 8); // wait a tiny bit longer in case there is a series of free's
+        segment->purge_expire = now + mi_option_get(mi_option_purge_extend_delay);
       }
     }
     else {
-      // previous purge mask is not yet expired, increase the expiration by a bit.
       segment->purge_expire += mi_option_get(mi_option_purge_extend_delay);
     }
   }
@@ -603,7 +544,6 @@ static void mi_segment_try_purge(mi_segm
   size_t idx;
   size_t count;
   mi_commit_mask_foreach(&mask, idx, count) {
-    // if found, decommit that sequence
     if (count > 0) {
       uint8_t* p = (uint8_t*)segment + (idx*MI_COMMIT_SIZE);
       size_t size = count * MI_COMMIT_SIZE;
@@ -614,8 +554,6 @@ static void mi_segment_try_purge(mi_segm
   mi_assert_internal(mi_commit_mask_is_empty(&segment->purge_mask));
 }
 
-// called from `mi_heap_collect_ex`
-// this can be called per-page so it is important that try_purge has fast exit path
 void _mi_segment_collect(mi_segment_t* segment, bool force) {
   mi_segment_try_purge(segment, force);
 }
@@ -628,48 +566,38 @@ static bool mi_segment_is_abandoned(mi_s
   return (mi_atomic_load_relaxed(&segment->thread_id) == 0);
 }
 
-// note: can be called on abandoned segments
 static void mi_segment_span_free(mi_segment_t* segment, size_t slice_index, size_t slice_count, bool allow_purge, mi_segments_tld_t* tld) {
   mi_assert_internal(slice_index < segment->slice_entries);
   mi_span_queue_t* sq = (segment->kind == MI_SEGMENT_HUGE || mi_segment_is_abandoned(segment)
                           ? NULL : mi_span_queue_for(slice_count,tld));
   if (slice_count==0) slice_count = 1;
-  mi_assert_internal(slice_index + slice_count - 1 < segment->slice_entries);
+  mi_assert_internal(slice_index + slice_count <= segment->slice_entries);
 
-  // set first and last slice (the intermediates can be undetermined)
   mi_slice_t* slice = &segment->slices[slice_index];
   slice->slice_count = (uint32_t)slice_count;
   mi_assert_internal(slice->slice_count == slice_count); // no overflow?
   slice->slice_offset = 0;
   if (slice_count > 1) {
-    mi_slice_t* last = slice + slice_count - 1;
-    mi_slice_t* end  = (mi_slice_t*)mi_segment_slices_end(segment);
-    if (last > end) { last = end; }
-    last->slice_count = 0;
-    last->slice_offset = (uint32_t)(sizeof(mi_page_t)*(slice_count - 1));
-    last->block_size = 0;
+    // FIX: clamp last slice index to stay within array bounds
+    const size_t last_index = slice_index + slice_count - 1;
+    if (last_index < segment->slice_entries) {
+      mi_slice_t* last = &segment->slices[last_index];
+      if (last > slice) {
+        last->slice_count = 0;
+        last->slice_offset = (uint32_t)(sizeof(mi_page_t)*(slice_count - 1));
+        last->block_size = 0;
+      }
+    }
   }
 
-  // perhaps decommit
   if (allow_purge) {
     mi_segment_schedule_purge(segment, mi_slice_start(slice), slice_count * MI_SEGMENT_SLICE_SIZE);
   }
 
-  // and push it on the free page queue (if it was not a huge page)
   if (sq != NULL) mi_span_queue_push( sq, slice );
              else slice->block_size = 0; // mark huge page as free anyways
 }
 
-/*
-// called from reclaim to add existing free spans
-static void mi_segment_span_add_free(mi_slice_t* slice, mi_segments_tld_t* tld) {
-  mi_segment_t* segment = _mi_ptr_segment(slice);
-  mi_assert_internal(slice->xblock_size==0 && slice->slice_count>0 && slice->slice_offset==0);
-  size_t slice_index = mi_slice_index(slice);
-  mi_segment_span_free(segment,slice_index,slice->slice_count,tld);
-}
-*/
-
 static void mi_segment_span_remove_from_queue(mi_slice_t* slice, mi_segments_tld_t* tld) {
   mi_assert_internal(slice->slice_count > 0 && slice->slice_offset==0 && slice->block_size==0);
   mi_assert_internal(_mi_ptr_segment(slice)->kind != MI_SEGMENT_HUGE);
@@ -677,47 +605,38 @@ static void mi_segment_span_remove_from_
   mi_span_queue_delete(sq, slice);
 }
 
-// note: can be called on abandoned segments
 static mi_slice_t* mi_segment_span_free_coalesce(mi_slice_t* slice, mi_segments_tld_t* tld) {
   mi_assert_internal(slice != NULL && slice->slice_count > 0 && slice->slice_offset == 0);
   mi_segment_t* const segment = _mi_ptr_segment(slice);
 
-  // for huge pages, just mark as free but don't add to the queues
-  if (segment->kind == MI_SEGMENT_HUGE) {
-    // issue #691: segment->used can be 0 if the huge page block was freed while abandoned (reclaim will get here in that case)
-    mi_assert_internal((segment->used==0 && slice->block_size==0) || segment->used == 1);  // decreased right after this call in `mi_segment_page_clear`
-    slice->block_size = 0;  // mark as free anyways
-    // we should mark the last slice `xblock_size=0` now to maintain invariants but we skip it to
-    // avoid a possible cache miss (and the segment is about to be freed)
+  if (mi_unlikely(segment->kind == MI_SEGMENT_HUGE)) {
+    mi_assert_internal((segment->used==0 && slice->block_size==0) || segment->used == 1);
+    slice->block_size = 0;
     return slice;
   }
 
-  // otherwise coalesce the span and add to the free span queues
-  const bool is_abandoned = (segment->thread_id == 0); // mi_segment_is_abandoned(segment);
+  const bool is_abandoned = (segment->thread_id == 0);
   size_t slice_count = slice->slice_count;
   mi_slice_t* next = slice + slice->slice_count;
   mi_assert_internal(next <= mi_segment_slices_end(segment));
   if (next < mi_segment_slices_end(segment) && next->block_size==0) {
-    // free next block -- remove it from free and merge
     mi_assert_internal(next->slice_count > 0 && next->slice_offset==0);
-    slice_count += next->slice_count; // extend
+    slice_count += next->slice_count;
     if (!is_abandoned) { mi_segment_span_remove_from_queue(next, tld); }
   }
   if (slice > segment->slices) {
     mi_slice_t* prev = mi_slice_first(slice - 1);
     mi_assert_internal(prev >= segment->slices);
     if (prev->block_size==0) {
-      // free previous slice -- remove it from free and merge
       mi_assert_internal(prev->slice_count > 0 && prev->slice_offset==0);
       slice_count += prev->slice_count;
       slice->slice_count = 0;
-      slice->slice_offset = (uint32_t)((uint8_t*)slice - (uint8_t*)prev); // set the slice offset for `segment_force_abandon` (in case the previous free block is very large).
+      slice->slice_offset = (uint32_t)((uint8_t*)slice - (uint8_t*)prev);
       if (!is_abandoned) { mi_segment_span_remove_from_queue(prev, tld); }
       slice = prev;
     }
   }
 
-  // and add the new free page
   mi_segment_span_free(segment, mi_slice_index(slice), slice_count, true, tld);
   return slice;
 }
@@ -728,18 +647,15 @@ static mi_slice_t* mi_segment_span_free_
    Page allocation
 ----------------------------------------------------------- */
 
-// Note: may still return NULL if committing the memory failed
 static mi_page_t* mi_segment_span_allocate(mi_segment_t* segment, size_t slice_index, size_t slice_count) {
   mi_assert_internal(slice_index < segment->slice_entries);
   mi_slice_t* const slice = &segment->slices[slice_index];
   mi_assert_internal(slice->block_size==0 || slice->block_size==1);
 
-  // commit before changing the slice data
   if (!mi_segment_ensure_committed(segment, _mi_segment_page_start_from_slice(segment, slice, 0, NULL), slice_count * MI_SEGMENT_SLICE_SIZE)) {
-    return NULL;  // commit failed!
+    return NULL;
   }
 
-  // convert the slices to a page
   slice->slice_offset = 0;
   slice->slice_count = (uint32_t)slice_count;
   mi_assert_internal(slice->slice_count == slice_count);
@@ -748,10 +664,9 @@ static mi_page_t* mi_segment_span_alloca
   mi_page_t*  page = mi_slice_to_page(slice);
   mi_assert_internal(mi_page_block_size(page) == bsize);
 
-  // set slice back pointers for the first MI_MAX_SLICE_OFFSET_COUNT entries
   size_t extra = slice_count-1;
   if (extra > MI_MAX_SLICE_OFFSET_COUNT) extra = MI_MAX_SLICE_OFFSET_COUNT;
-  if (slice_index + extra >= segment->slice_entries) extra = segment->slice_entries - slice_index - 1;  // huge objects may have more slices than avaiable entries in the segment->slices
+  if (slice_index + extra >= segment->slice_entries) extra = segment->slice_entries - slice_index - 1;
 
   mi_slice_t* slice_next = slice + 1;
   for (size_t i = 1; i <= extra; i++, slice_next++) {
@@ -760,18 +675,18 @@ static mi_page_t* mi_segment_span_alloca
     slice_next->block_size = 1;
   }
 
-  // and also for the last one (if not set already) (the last one is needed for coalescing and for large alignments)
-  // note: the cast is needed for ubsan since the index can be larger than MI_SLICES_PER_SEGMENT for huge allocations (see #543)
   mi_slice_t* last = slice + slice_count - 1;
   mi_slice_t* end = (mi_slice_t*)mi_segment_slices_end(segment);
-  if (last > end) last = end;
+
+  // CRITICAL FIX: Clamp 'last' to valid array bounds to prevent overflow
+  if (last >= end) last = end - 1;
+
   if (last > slice) {
     last->slice_offset = (uint32_t)(sizeof(mi_slice_t) * (last - slice));
     last->slice_count = 0;
     last->block_size = 1;
   }
 
-  // and initialize the page
   page->is_committed = true;
   page->is_zero_init = segment->free_is_zero;
   page->is_huge = (segment->kind == MI_SEGMENT_HUGE);
@@ -782,7 +697,7 @@ static mi_page_t* mi_segment_span_alloca
 static void mi_segment_slice_split(mi_segment_t* segment, mi_slice_t* slice, size_t slice_count, mi_segments_tld_t* tld) {
   mi_assert_internal(_mi_ptr_segment(slice) == segment);
   mi_assert_internal(slice->slice_count >= slice_count);
-  mi_assert_internal(slice->block_size > 0); // no more in free queue
+  mi_assert_internal(slice->block_size > 0);
   if (slice->slice_count <= slice_count) return;
   mi_assert_internal(segment->kind != MI_SEGMENT_HUGE);
   size_t next_index = mi_slice_index(slice) + slice_count;
@@ -793,16 +708,13 @@ static void mi_segment_slice_split(mi_se
 
 static mi_page_t* mi_segments_page_find_and_allocate(size_t slice_count, mi_arena_id_t req_arena_id, mi_segments_tld_t* tld) {
   mi_assert_internal(slice_count*MI_SEGMENT_SLICE_SIZE <= MI_LARGE_OBJ_SIZE_MAX);
-  // search from best fit up
   mi_span_queue_t* sq = mi_span_queue_for(slice_count, tld);
   if (slice_count == 0) slice_count = 1;
   while (sq <= &tld->spans[MI_SEGMENT_BIN_MAX]) {
     for (mi_slice_t* slice = sq->first; slice != NULL; slice = slice->next) {
-      if (slice->slice_count >= slice_count) {
-        // found one
+      if (mi_likely(slice->slice_count >= slice_count)) {
         mi_segment_t* segment = _mi_ptr_segment(slice);
-        if (_mi_arena_memid_is_suitable(segment->memid, req_arena_id)) {
-          // found a suitable page span
+        if (mi_likely(_mi_arena_memid_is_suitable(segment->memid, req_arena_id))) {
           mi_span_queue_delete(sq, slice);
 
           if (slice->slice_count > slice_count) {
@@ -810,8 +722,7 @@ static mi_page_t* mi_segments_page_find_
           }
           mi_assert_internal(slice != NULL && slice->slice_count == slice_count && slice->block_size > 0);
           mi_page_t* page = mi_segment_span_allocate(segment, mi_slice_index(slice), slice->slice_count);
-          if (page == NULL) {
-            // commit failed; return NULL but first restore the slice
+          if (mi_unlikely(page == NULL)) {
             mi_segment_span_free_coalesce(slice, tld);
             return NULL;
           }
@@ -821,7 +732,6 @@ static mi_page_t* mi_segments_page_find_
     }
     sq++;
   }
-  // could not find a page..
   return NULL;
 }
 
@@ -836,19 +746,33 @@ static mi_segment_t* mi_segment_os_alloc
 
 {
   mi_memid_t memid;
-  bool   allow_large = (!eager_delayed && (MI_SECURE == 0)); // only allow large OS pages once we are no longer lazy
+  bool   allow_large = (!eager_delayed && (MI_SECURE == 0));
   size_t align_offset = 0;
   size_t alignment = MI_SEGMENT_ALIGN;
 
   if (page_alignment > 0) {
-    // mi_assert_internal(huge_page != NULL);
-    mi_assert_internal(page_alignment >= MI_SEGMENT_ALIGN);
-    alignment = page_alignment;
-    const size_t info_size = (*pinfo_slices) * MI_SEGMENT_SLICE_SIZE;
-    align_offset = _mi_align_up( info_size, MI_SEGMENT_ALIGN );
-    const size_t extra = align_offset - info_size;
-    // recalculate due to potential guard pages
-    *psegment_slices = mi_segment_calculate_slices(required + extra, pinfo_slices);
+    /*
+       CRITICAL FIX: Enforce minimum segment alignment of 32MB.
+       Do NOT set align_offset for small alignments, as it pushes the payload
+       too far, breaking _mi_ptr_segment logic.
+    */
+    alignment = (page_alignment > MI_SEGMENT_ALIGN ? page_alignment : MI_SEGMENT_ALIGN);
+
+    if (page_alignment > MI_SEGMENT_ALIGN) {
+        // Large alignment: align payload via offset
+        const size_t info_size = (*pinfo_slices) * MI_SEGMENT_SLICE_SIZE;
+        align_offset = _mi_align_up(info_size, alignment);
+        const size_t extra = align_offset - info_size;
+        *psegment_slices = mi_segment_calculate_slices(required + extra, pinfo_slices);
+    } else {
+        // Small/Normal alignment: keep payload close to header (offset 0)
+        // Manual padding calc to ensure page_alignment is met
+        const size_t info_size = (*pinfo_slices) * MI_SEGMENT_SLICE_SIZE;
+        const size_t aligned_info = _mi_align_up(info_size, page_alignment);
+        const size_t extra = aligned_info - info_size;
+        *psegment_slices = mi_segment_calculate_slices(required + extra, pinfo_slices);
+        align_offset = 0;
+    }
     mi_assert_internal(*psegment_slices > 0 && *psegment_slices <= UINT32_MAX);
   }
 
@@ -858,13 +782,11 @@ static mi_segment_t* mi_segment_os_alloc
     return NULL;  // failed to allocate
   }
 
-  // ensure metadata part of the segment is committed
   mi_commit_mask_t commit_mask;
   if (memid.initially_committed) {
     mi_commit_mask_create_full(&commit_mask);
   }
   else {
-    // at least commit the info slices
     const size_t commit_needed = _mi_divide_up((*pinfo_slices)*MI_SEGMENT_SLICE_SIZE, MI_COMMIT_SIZE);
     mi_assert_internal(commit_needed>0);
     mi_commit_mask_create(0, commit_needed, &commit_mask);
@@ -874,7 +796,7 @@ static mi_segment_t* mi_segment_os_alloc
       return NULL;
     }
   }
-  mi_assert_internal(segment != NULL && (uintptr_t)segment % MI_SEGMENT_SIZE == 0);
+  mi_assert_internal(segment != NULL && (uintptr_t)segment % MI_SEGMENT_ALIGN == 0);
 
   segment->memid = memid;
   segment->allow_decommit = !memid.is_pinned;
@@ -897,32 +819,25 @@ static mi_segment_t* mi_segment_alloc(si
 {
   mi_assert_internal((required==0 && huge_page==NULL) || (required>0 && huge_page != NULL));
 
-  // calculate needed sizes first
   size_t info_slices;
   size_t segment_slices = mi_segment_calculate_slices(required, &info_slices);
   mi_assert_internal(segment_slices > 0 && segment_slices <= UINT32_MAX);
 
-  // Commit eagerly only if not the first N lazy segments (to reduce impact of many threads that allocate just a little)
-  const bool eager_delay = (// !_mi_os_has_overcommit() &&             // never delay on overcommit systems
-                            _mi_current_thread_count() > 1 &&       // do not delay for the first N threads
-                            tld->peak_count < (size_t)mi_option_get(mi_option_eager_commit_delay));
+  const bool eager_delay = (_mi_current_thread_count() > 1 && tld->peak_count < (size_t)mi_option_get(mi_option_eager_commit_delay));
   const bool eager = !eager_delay && mi_option_is_enabled(mi_option_eager_commit);
   bool commit = eager || (required > 0);
 
-  // Allocate the segment from the OS
   mi_segment_t* segment = mi_segment_os_alloc(required, page_alignment, eager_delay, req_arena_id,
                                               &segment_slices, &info_slices, commit, tld);
   if (segment == NULL) return NULL;
 
-  // zero the segment info? -- not always needed as it may be zero initialized from the OS
   if (!segment->memid.initially_zero) {
     ptrdiff_t ofs    = offsetof(mi_segment_t, next);
     size_t    prefix = offsetof(mi_segment_t, slices) - ofs;
-    size_t    zsize  = prefix + (sizeof(mi_slice_t) * (segment_slices + 1)); // one more
+    size_t    zsize  = prefix + (sizeof(mi_slice_t) * (segment_slices + 1));
     _mi_memzero((uint8_t*)segment + ofs, zsize);
   }
 
-  // initialize the rest of the segment info
   const size_t slice_entries = (segment_slices > MI_SLICES_PER_SEGMENT ? MI_SLICES_PER_SEGMENT : segment_slices);
   segment->segment_slices = segment_slices;
   segment->segment_info_slices = info_slices;
@@ -931,40 +846,34 @@ static mi_segment_t* mi_segment_alloc(si
   segment->slice_entries = slice_entries;
   segment->kind = (required == 0 ? MI_SEGMENT_NORMAL : MI_SEGMENT_HUGE);
 
-  // _mi_memzero(segment->slices, sizeof(mi_slice_t)*(info_slices+1));
   _mi_stat_increase(&tld->stats->page_committed, mi_segment_info_size(segment));
 
-  // set up guard pages
   size_t guard_slices = 0;
   if (MI_SECURE>0) {
-    // in secure mode, we set up a protected page in between the segment info
-    // and the page data, and at the end of the segment.
     size_t os_pagesize = _mi_os_page_size();
     _mi_os_protect((uint8_t*)segment + mi_segment_info_size(segment) - os_pagesize, os_pagesize);
     uint8_t* end = (uint8_t*)segment + mi_segment_size(segment) - os_pagesize;
     mi_segment_ensure_committed(segment, end, os_pagesize);
     _mi_os_protect(end, os_pagesize);
-    if (slice_entries == segment_slices) segment->slice_entries--; // don't use the last slice :-(
+    if (slice_entries == segment_slices) segment->slice_entries--;
     guard_slices = 1;
   }
 
-  // reserve first slices for segment info
   mi_page_t* page0 = mi_segment_span_allocate(segment, 0, info_slices);
-  mi_assert_internal(page0!=NULL); if (page0==NULL) return NULL; // cannot fail as we always commit in advance
+  mi_assert_internal(page0!=NULL); if (page0==NULL) return NULL;
   mi_assert_internal(segment->used == 1);
-  segment->used = 0; // don't count our internal slices towards usage
+  segment->used = 0;
 
-  // initialize initial free pages
-  if (segment->kind == MI_SEGMENT_NORMAL) { // not a huge page
+  if (segment->kind == MI_SEGMENT_NORMAL) {
     mi_assert_internal(huge_page==NULL);
-    mi_segment_span_free(segment, info_slices, segment->slice_entries - info_slices, false /* don't purge */, tld);
+    mi_segment_span_free(segment, info_slices, segment->slice_entries - info_slices, false, tld);
   }
   else {
     mi_assert_internal(huge_page!=NULL);
     mi_assert_internal(mi_commit_mask_is_empty(&segment->purge_mask));
     mi_assert_internal(mi_commit_mask_is_full(&segment->commit_mask));
     *huge_page = mi_segment_span_allocate(segment, info_slices, segment_slices - info_slices - guard_slices);
-    mi_assert_internal(*huge_page != NULL); // cannot fail as we commit in advance
+    mi_assert_internal(*huge_page != NULL);
   }
 
   mi_assert_expensive(mi_segment_is_valid(segment,tld));
@@ -978,10 +887,8 @@ static void mi_segment_free(mi_segment_t
   mi_assert_internal(segment->next == NULL);
   mi_assert_internal(segment->used == 0);
 
-  // in `mi_segment_force_abandon` we set this to true to ensure the segment's memory stays valid
   if (segment->dont_free) return;
 
-  // Remove the free pages
   mi_slice_t* slice = &segment->slices[0];
   const mi_slice_t* end = mi_segment_slices_end(segment);
   #if MI_DEBUG>1
@@ -990,7 +897,7 @@ static void mi_segment_free(mi_segment_t
   while (slice < end) {
     mi_assert_internal(slice->slice_count > 0);
     mi_assert_internal(slice->slice_offset == 0);
-    mi_assert_internal(mi_slice_index(slice)==0 || slice->block_size == 0); // no more used pages ..
+    mi_assert_internal(mi_slice_index(slice)==0 || slice->block_size == 0);
     if (slice->block_size == 0 && segment->kind != MI_SEGMENT_HUGE) {
       mi_segment_span_remove_from_queue(slice, tld);
     }
@@ -999,12 +906,8 @@ static void mi_segment_free(mi_segment_t
     #endif
     slice = slice + slice->slice_count;
   }
-  mi_assert_internal(page_count == 2); // first page is allocated by the segment itself
+  mi_assert_internal(page_count == 2);
 
-  // stats
-  // _mi_stat_decrease(&tld->stats->page_committed, mi_segment_info_size(segment));
-
-  // return it to the OS
   mi_segment_os_free(segment, tld);
 }
 
@@ -1015,7 +918,6 @@ static void mi_segment_free(mi_segment_t
 
 static void mi_segment_abandon(mi_segment_t* segment, mi_segments_tld_t* tld);
 
-// note: can be called on abandoned pages
 static mi_slice_t* mi_segment_page_clear(mi_page_t* page, mi_segments_tld_t* tld) {
   mi_assert_internal(page->block_size > 0);
   mi_assert_internal(mi_page_all_free(page));
@@ -1026,15 +928,13 @@ static mi_slice_t* mi_segment_page_clear
   _mi_stat_decrease(&tld->stats->page_committed, inuse);
   _mi_stat_decrease(&tld->stats->pages, 1);
   _mi_stat_decrease(&tld->stats->page_bins[_mi_page_stats_bin(page)], 1);
-  
-  // reset the page memory to reduce memory pressure?
+
   if (segment->allow_decommit && mi_option_is_enabled(mi_option_deprecated_page_reset)) {
     size_t psize;
     uint8_t* start = _mi_segment_page_start(segment, page, &psize);
     _mi_os_reset(start, psize);
   }
 
-  // zero the page data, but not the segment fields and heap tag
   page->is_zero_init = false;
   uint8_t heap_tag = page->heap_tag;
   ptrdiff_t ofs = offsetof(mi_page_t, capacity);
@@ -1042,13 +942,10 @@ static mi_slice_t* mi_segment_page_clear
   page->block_size = 1;
   page->heap_tag = heap_tag;
 
-  // and free it
   mi_slice_t* slice = mi_segment_span_free_coalesce(mi_page_to_slice(page), tld);
   segment->used--;
   segment->free_is_zero = false;
 
-  // cannot assert segment valid as it is called during reclaim
-  // mi_assert_expensive(mi_segment_is_valid(segment, tld));
   return slice;
 }
 
@@ -1058,45 +955,23 @@ void _mi_segment_page_free(mi_page_t* pa
   mi_segment_t* segment = _mi_page_segment(page);
   mi_assert_expensive(mi_segment_is_valid(segment,tld));
 
-  // mark it as free now
   mi_segment_page_clear(page, tld);
   mi_assert_expensive(mi_segment_is_valid(segment, tld));
 
   if (segment->used == 0) {
-    // no more used pages; remove from the free list and free the segment
     mi_segment_free(segment, force, tld);
   }
   else if (segment->used == segment->abandoned) {
-    // only abandoned pages; remove from free list and abandon
     mi_segment_abandon(segment,tld);
   }
   else {
-    // perform delayed purges
-    mi_segment_try_purge(segment, false /* force? */);
+    mi_segment_try_purge(segment, false);
   }
 }
 
 
 /* -----------------------------------------------------------
 Abandonment
-
-When threads terminate, they can leave segments with
-live blocks (reachable through other threads). Such segments
-are "abandoned" and will be reclaimed by other threads to
-reuse their pages and/or free them eventually. The
-`thread_id` of such segments is 0.
-
-When a block is freed in an abandoned segment, the segment
-is reclaimed into that thread.
-
-Moreover, if threads are looking for a fresh segment, they
-will first consider abandoned segments -- these can be found
-by scanning the arena memory
-(segments outside arena memoryare only reclaimed by a free).
------------------------------------------------------------ */
-
-/* -----------------------------------------------------------
-   Abandon segment/page
 ----------------------------------------------------------- */
 
 static void mi_segment_abandon(mi_segment_t* segment, mi_segments_tld_t* tld) {
@@ -1105,30 +980,25 @@ static void mi_segment_abandon(mi_segmen
   mi_assert_internal(segment->abandoned_visits == 0);
   mi_assert_expensive(mi_segment_is_valid(segment,tld));
 
-  // remove the free pages from the free page queues
   mi_slice_t* slice = &segment->slices[0];
   const mi_slice_t* end = mi_segment_slices_end(segment);
   while (slice < end) {
     mi_assert_internal(slice->slice_count > 0);
     mi_assert_internal(slice->slice_offset == 0);
-    if (slice->block_size == 0) { // a free page
+    if (slice->block_size == 0) {
       mi_segment_span_remove_from_queue(slice,tld);
-      slice->block_size = 0; // but keep it free
+      slice->block_size = 0;
     }
     slice = slice + slice->slice_count;
   }
 
-  // perform delayed decommits (forcing is much slower on mstress)
-  // Only abandoned segments in arena memory can be reclaimed without a free
-  // so if a segment is not from an arena we force purge here to be conservative.
   const bool force_purge = (segment->memid.memkind != MI_MEM_ARENA) || mi_option_is_enabled(mi_option_abandoned_page_purge);
   mi_segment_try_purge(segment, force_purge);
 
-  // all pages in the segment are abandoned; add it to the abandoned list
   _mi_stat_increase(&tld->stats->segments_abandoned, 1);
   mi_segments_track_size(-((long)mi_segment_size(segment)), tld);
   segment->thread_id = 0;
-  segment->abandoned_visits = 1;   // from 0 to 1 to signify it is abandoned
+  segment->abandoned_visits = 1;
   if (segment->was_reclaimed) {
     tld->reclaim_count--;
     segment->was_reclaimed = false;
@@ -1148,7 +1018,6 @@ void _mi_segment_page_abandon(mi_page_t*
   _mi_stat_increase(&tld->stats->pages_abandoned, 1);
   mi_assert_internal(segment->abandoned <= segment->used);
   if (segment->used == segment->abandoned) {
-    // all pages are abandoned, abandon the entire segment
     mi_segment_abandon(segment, tld);
   }
 }
@@ -1160,45 +1029,39 @@ void _mi_segment_page_abandon(mi_page_t*
 static mi_slice_t* mi_slices_start_iterate(mi_segment_t* segment, const mi_slice_t** end) {
   mi_slice_t* slice = &segment->slices[0];
   *end = mi_segment_slices_end(segment);
-  mi_assert_internal(slice->slice_count>0 && slice->block_size>0); // segment allocated page
-  slice = slice + slice->slice_count; // skip the first segment allocated page
+  mi_assert_internal(slice->slice_count>0 && slice->block_size>0);
+  slice = slice + slice->slice_count;
   return slice;
 }
 
-// Possibly free pages and check if free space is available
 static bool mi_segment_check_free(mi_segment_t* segment, size_t slices_needed, size_t block_size, mi_segments_tld_t* tld)
 {
   mi_assert_internal(mi_segment_is_abandoned(segment));
   bool has_page = false;
 
-  // for all slices
   const mi_slice_t* end;
   mi_slice_t* slice = mi_slices_start_iterate(segment, &end);
   while (slice < end) {
     mi_assert_internal(slice->slice_count > 0);
     mi_assert_internal(slice->slice_offset == 0);
-    if (mi_slice_is_used(slice)) { // used page
-      // ensure used count is up to date and collect potential concurrent frees
+    if (mi_slice_is_used(slice)) {
       mi_page_t* const page = mi_slice_to_page(slice);
       _mi_page_free_collect(page, false);
       if (mi_page_all_free(page)) {
-        // if this page is all free now, free it without adding to any queues (yet)
         mi_assert_internal(page->next == NULL && page->prev==NULL);
         _mi_stat_decrease(&tld->stats->pages_abandoned, 1);
         segment->abandoned--;
-        slice = mi_segment_page_clear(page, tld); // re-assign slice due to coalesce!
+        slice = mi_segment_page_clear(page, tld);
         mi_assert_internal(!mi_slice_is_used(slice));
         if (slice->slice_count >= slices_needed) {
           has_page = true;
         }
       }
       else if (mi_page_block_size(page) == block_size && mi_page_has_any_available(page)) {
-        // a page has available free blocks of the right size
         has_page = true;
       }
     }
     else {
-      // empty span
       if (slice->slice_count >= slices_needed) {
         has_page = true;
       }
@@ -1208,13 +1071,10 @@ static bool mi_segment_check_free(mi_seg
   return has_page;
 }
 
-// Reclaim an abandoned segment; returns NULL if the segment was freed
-// set `right_page_reclaimed` to `true` if it reclaimed a page of the right `block_size` that was not full.
 static mi_segment_t* mi_segment_reclaim(mi_segment_t* segment, mi_heap_t* heap, size_t requested_block_size, bool* right_page_reclaimed, mi_segments_tld_t* tld) {
   if (right_page_reclaimed != NULL) { *right_page_reclaimed = false; }
-  // can be 0 still with abandoned_next, or already a thread id for segments outside an arena that are reclaimed on a free.
   mi_assert_internal(mi_atomic_load_relaxed(&segment->thread_id) == 0 || mi_atomic_load_relaxed(&segment->thread_id) == _mi_thread_id());
-  mi_assert_internal(segment->subproc == heap->tld->segments.subproc); // only reclaim within the same subprocess
+  mi_assert_internal(segment->subproc == heap->tld->segments.subproc);
   mi_atomic_store_release(&segment->thread_id, _mi_thread_id());
   segment->abandoned_visits = 0;
   segment->was_reclaimed = true;
@@ -1223,14 +1083,12 @@ static mi_segment_t* mi_segment_reclaim(
   mi_assert_internal(segment->next == NULL);
   _mi_stat_decrease(&tld->stats->segments_abandoned, 1);
 
-  // for all slices
   const mi_slice_t* end;
   mi_slice_t* slice = mi_slices_start_iterate(segment, &end);
   while (slice < end) {
     mi_assert_internal(slice->slice_count > 0);
     mi_assert_internal(slice->slice_offset == 0);
     if (mi_slice_is_used(slice)) {
-      // in use: reclaim the page in our heap
       mi_page_t* page = mi_slice_to_page(slice);
       mi_assert_internal(page->is_committed);
       mi_assert_internal(mi_page_thread_free_flag(page)==MI_NEVER_DELAYED_FREE);
@@ -1238,22 +1096,18 @@ static mi_segment_t* mi_segment_reclaim(
       mi_assert_internal(page->next == NULL && page->prev==NULL);
       _mi_stat_decrease(&tld->stats->pages_abandoned, 1);
       segment->abandoned--;
-      // get the target heap for this thread which has a matching heap tag (so we reclaim into a matching heap)
-      mi_heap_t* target_heap = _mi_heap_by_tag(heap, page->heap_tag);  // allow custom heaps to separate objects
+      mi_heap_t* target_heap = _mi_heap_by_tag(heap, page->heap_tag);
       if (target_heap == NULL) {
         target_heap = heap;
         _mi_error_message(EFAULT, "page with tag %u cannot be reclaimed by a heap with the same tag (using heap tag %u instead)\n", page->heap_tag, heap->tag );
       }
-      // associate the heap with this page, and allow heap thread delayed free again.
       mi_page_set_heap(page, target_heap);
-      _mi_page_use_delayed_free(page, MI_USE_DELAYED_FREE, true); // override never (after heap is set)
-      _mi_page_free_collect(page, false); // ensure used count is up to date
+      _mi_page_use_delayed_free(page, MI_USE_DELAYED_FREE, true);
+      _mi_page_free_collect(page, false);
       if (mi_page_all_free(page)) {
-        // if everything free by now, free the page
-        slice = mi_segment_page_clear(page, tld);   // set slice again due to coalesceing
+        slice = mi_segment_page_clear(page, tld);
       }
       else {
-        // otherwise reclaim it into the heap
         _mi_page_reclaim(target_heap, page);
         if (requested_block_size == mi_page_block_size(page) && mi_page_has_any_available(page) && heap == target_heap) {
           if (right_page_reclaimed != NULL) { *right_page_reclaimed = true; }
@@ -1261,8 +1115,7 @@ static mi_segment_t* mi_segment_reclaim(
       }
     }
     else {
-      // the span is free, add it to our page queues
-      slice = mi_segment_span_free_coalesce(slice, tld); // set slice again due to coalesceing
+      slice = mi_segment_span_free_coalesce(slice, tld);
     }
     mi_assert_internal(slice->slice_count>0 && slice->slice_offset==0);
     slice = slice + slice->slice_count;
@@ -1270,7 +1123,7 @@ static mi_segment_t* mi_segment_reclaim(
 
   mi_assert(segment->abandoned == 0);
   mi_assert_expensive(mi_segment_is_valid(segment, tld));
-  if (segment->used == 0) {  // due to page_clear
+  if (segment->used == 0) {
     mi_assert_internal(right_page_reclaimed == NULL || !(*right_page_reclaimed));
     mi_segment_free(segment, false, tld);
     return NULL;
@@ -1281,21 +1134,17 @@ static mi_segment_t* mi_segment_reclaim(
 }
 
 
-// attempt to reclaim a particular segment (called from multi threaded free `alloc.c:mi_free_block_mt`)
 bool _mi_segment_attempt_reclaim(mi_heap_t* heap, mi_segment_t* segment) {
-  if (mi_atomic_load_relaxed(&segment->thread_id) != 0) return false;  // it is not abandoned
-  if (segment->subproc != heap->tld->segments.subproc)  return false;  // only reclaim within the same subprocess
-  if (!_mi_heap_memid_is_suitable(heap,segment->memid)) return false;  // don't reclaim between exclusive and non-exclusive arena's
+  if (mi_atomic_load_relaxed(&segment->thread_id) != 0) return false;
+  if (segment->subproc != heap->tld->segments.subproc)  return false;
+  if (!_mi_heap_memid_is_suitable(heap,segment->memid)) return false;
   const long target = _mi_option_get_fast(mi_option_target_segments_per_thread);
-  if (target > 0 && (size_t)target <= heap->tld->segments.count) return false; // don't reclaim if going above the target count
+  if (target > 0 && (size_t)target <= heap->tld->segments.count) return false;
 
-  // don't reclaim more from a `free` call than half the current segments
-  // this is to prevent a pure free-ing thread to start owning too many segments
-  // (but not for out-of-arena segments as that is the main way to be reclaimed for those)
   if (segment->memid.memkind == MI_MEM_ARENA && heap->tld->segments.reclaim_count * 2 > heap->tld->segments.count) {
     return false;
   }
-  if (_mi_arena_segment_clear_abandoned(segment)) {  // atomically unabandon
+  if (_mi_arena_segment_clear_abandoned(segment)) {
     mi_segment_t* res = mi_segment_reclaim(segment, heap, 0, NULL, &heap->tld->segments);
     mi_assert_internal(res == segment);
     return (res != NULL);
@@ -1306,7 +1155,7 @@ bool _mi_segment_attempt_reclaim(mi_heap
 void _mi_abandoned_reclaim_all(mi_heap_t* heap, mi_segments_tld_t* tld) {
   mi_segment_t* segment;
   mi_arena_field_cursor_t current;
-  _mi_arena_field_cursor_init(heap, tld->subproc, true /* visit all, blocking */, &current);
+  _mi_arena_field_cursor_init(heap, tld->subproc, true, &current);
   while ((segment = _mi_arena_segment_clear_abandoned_next(&current)) != NULL) {
     mi_segment_reclaim(segment, heap, 0, NULL, tld);
   }
@@ -1321,12 +1170,11 @@ static bool segment_count_is_within_targ
 }
 
 static long mi_segment_get_reclaim_tries(mi_segments_tld_t* tld) {
-  // limit the tries to 10% (default) of the abandoned segments with at least 8 and at most 1024 tries.
   const size_t perc = (size_t)mi_option_get_clamp(mi_option_max_segment_reclaim, 0, 100);
   if (perc <= 0) return 0;
   const size_t total_count = mi_atomic_load_relaxed(&tld->subproc->abandoned_count);
   if (total_count == 0) return 0;
-  const size_t relative_count = (total_count > 10000 ? (total_count / 100) * perc : (total_count * perc) / 100); // avoid overflow
+  const size_t relative_count = (total_count > 10000 ? (total_count / 100) * perc : (total_count * perc) / 100);
   long max_tries = (long)(relative_count <= 1 ? 1 : (relative_count > 1024 ? 1024 : relative_count));
   if (max_tries < 8 && total_count > 8) { max_tries = 8;  }
   return max_tries;
@@ -1344,36 +1192,23 @@ static mi_segment_t* mi_segment_try_recl
   _mi_arena_field_cursor_init(heap, tld->subproc, false /* non-blocking */, &current);
   while (segment_count_is_within_target(tld,NULL) && (max_tries-- > 0) && ((segment = _mi_arena_segment_clear_abandoned_next(&current)) != NULL))
   {
-    mi_assert(segment->subproc == heap->tld->segments.subproc); // cursor only visits segments in our sub-process
+    mi_assert(segment->subproc == heap->tld->segments.subproc);
     segment->abandoned_visits++;
-    // todo: should we respect numa affinity for abandoned reclaim? perhaps only for the first visit?
-    // todo: an arena exclusive heap will potentially visit many abandoned unsuitable segments and use many tries
-    // Perhaps we can skip non-suitable ones in a better way?
     bool is_suitable = _mi_heap_memid_is_suitable(heap, segment->memid);
-    bool has_page = mi_segment_check_free(segment,needed_slices,block_size,tld); // try to free up pages (due to concurrent frees)
+    bool has_page = mi_segment_check_free(segment,needed_slices,block_size,tld);
     if (segment->used == 0) {
-      // free the segment (by forced reclaim) to make it available to other threads.
-      // note1: we prefer to free a segment as that might lead to reclaiming another
-      // segment that is still partially used.
-      // note2: we could in principle optimize this by skipping reclaim and directly
-      // freeing but that would violate some invariants temporarily)
       mi_segment_reclaim(segment, heap, 0, NULL, tld);
     }
     else if (has_page && is_suitable) {
-      // found a large enough free span, or a page of the right block_size with free space
-      // we return the result of reclaim (which is usually `segment`) as it might free
-      // the segment due to concurrent frees (in which case `NULL` is returned).
       result = mi_segment_reclaim(segment, heap, block_size, reclaimed, tld);
       break;
     }
     else if (segment->abandoned_visits > 3 && is_suitable) {
-      // always reclaim on 3rd visit to limit the abandoned segment count.
       mi_segment_reclaim(segment, heap, 0, NULL, tld);
     }
     else {
-      // otherwise, push on the visited list so it gets not looked at too quickly again
-      max_tries++; // don't count this as a try since it was not suitable
-      mi_segment_try_purge(segment, false /* true force? */); // force purge if needed as we may not visit soon again
+      max_tries++;
+      mi_segment_try_purge(segment, false);
       _mi_arena_segment_mark_abandoned(segment);
     }
   }
@@ -1381,23 +1216,17 @@ static mi_segment_t* mi_segment_try_recl
   return result;
 }
 
-// collect abandoned segments
 void _mi_abandoned_collect(mi_heap_t* heap, bool force, mi_segments_tld_t* tld)
 {
   mi_segment_t* segment;
-  mi_arena_field_cursor_t current; _mi_arena_field_cursor_init(heap, tld->subproc, force /* blocking? */, &current);
-  long max_tries = (force ? (long)mi_atomic_load_relaxed(&tld->subproc->abandoned_count) : 1024);  // limit latency
+  mi_arena_field_cursor_t current; _mi_arena_field_cursor_init(heap, tld->subproc, force, &current);
+  long max_tries = (force ? (long)mi_atomic_load_relaxed(&tld->subproc->abandoned_count) : 1024);
   while ((max_tries-- > 0) && ((segment = _mi_arena_segment_clear_abandoned_next(&current)) != NULL)) {
-    mi_segment_check_free(segment,0,0,tld); // try to free up pages (due to concurrent frees)
+    mi_segment_check_free(segment,0,0,tld);
     if (segment->used == 0) {
-      // free the segment (by forced reclaim) to make it available to other threads.
-      // note: we could in principle optimize this by skipping reclaim and directly
-      // freeing but that would violate some invariants temporarily)
       mi_segment_reclaim(segment, heap, 0, NULL, tld);
     }
     else {
-      // otherwise, purge if needed and push on the visited list
-      // note: forced purge can be expensive if many threads are destroyed/created as in mstress.
       mi_segment_try_purge(segment, force);
       _mi_arena_segment_mark_abandoned(segment);
     }
@@ -1409,39 +1238,30 @@ void _mi_abandoned_collect(mi_heap_t* he
    Force abandon a segment that is in use by our thread
 ----------------------------------------------------------- */
 
-// force abandon a segment
 static void mi_segment_force_abandon(mi_segment_t* segment, mi_segments_tld_t* tld)
 {
   mi_assert_internal(!mi_segment_is_abandoned(segment));
   mi_assert_internal(!segment->dont_free);
 
-  // ensure the segment does not get free'd underneath us (so we can check if a page has been freed in `mi_page_force_abandon`)
   segment->dont_free = true;
 
-  // for all slices
   const mi_slice_t* end;
   mi_slice_t* slice = mi_slices_start_iterate(segment, &end);
   while (slice < end) {
     mi_assert_internal(slice->slice_count > 0);
     mi_assert_internal(slice->slice_offset == 0);
     if (mi_slice_is_used(slice)) {
-      // ensure used count is up to date and collect potential concurrent frees
       mi_page_t* const page = mi_slice_to_page(slice);
       _mi_page_free_collect(page, false);
       {
-        // abandon the page if it is still in-use (this will free it if possible as well)
         mi_assert_internal(segment->used > 0);
         if (segment->used == segment->abandoned+1) {
-          // the last page.. abandon and return as the segment will be abandoned after this
-          // and we should no longer access it.
           segment->dont_free = false;
           _mi_page_force_abandon(page);
           return;
         }
         else {
-          // abandon and continue
           _mi_page_force_abandon(page);
-          // it might be freed, reset the slice (note: relies on coalesce setting the slice_offset)
           slice = mi_slice_first(slice);
         }
       }
@@ -1451,23 +1271,18 @@ static void mi_segment_force_abandon(mi_
   segment->dont_free = false;
   mi_assert(segment->used == segment->abandoned);
   mi_assert(segment->used == 0);
-  if (segment->used == 0) {  // paranoia
-    // all free now
+  if (segment->used == 0) {
     mi_segment_free(segment, false, tld);
   }
   else {
-    // perform delayed purges
-    mi_segment_try_purge(segment, false /* force? */);
+    mi_segment_try_purge(segment, false);
   }
 }
 
 
-// try abandon segments.
-// this should be called from `reclaim_or_alloc` so we know all segments are (about) fully in use.
 static void mi_segments_try_abandon_to_target(mi_heap_t* heap, size_t target, mi_segments_tld_t* tld) {
   if (target <= 1) return;
-  const size_t min_target = (target > 4 ? (target*3)/4 : target);  // 75%
-  // todo: we should maintain a list of segments per thread; for now, only consider segments from the heap full pages
+  const size_t min_target = (target > 4 ? (target*3)/4 : target);
   for (int i = 0; i < 64 && tld->count >= min_target; i++) {
     mi_page_t* page = heap->pages[MI_BIN_FULL].first;
     while (page != NULL && mi_page_block_size(page) > MI_LARGE_OBJ_SIZE_MAX) {
@@ -1478,14 +1293,11 @@ static void mi_segments_try_abandon_to_t
     }
     mi_segment_t* segment = _mi_page_segment(page);
     mi_segment_force_abandon(segment, tld);
-    mi_assert_internal(page != heap->pages[MI_BIN_FULL].first); // as it is just abandoned
+    mi_assert_internal(page != heap->pages[MI_BIN_FULL].first);
   }
 }
 
-// try abandon segments.
-// this should be called from `reclaim_or_alloc` so we know all segments are (about) fully in use.
 static void mi_segments_try_abandon(mi_heap_t* heap, mi_segments_tld_t* tld) {
-  // we call this when we are about to add a fresh segment so we should be under our target segment count.
   size_t target = 0;
   if (segment_count_is_within_target(tld, &target)) return;
   mi_segments_try_abandon_to_target(heap, target, tld);
@@ -1510,22 +1322,17 @@ static mi_segment_t* mi_segment_reclaim_
 {
   mi_assert_internal(block_size <= MI_LARGE_OBJ_SIZE_MAX);
 
-  // try to abandon some segments to increase reuse between threads
   mi_segments_try_abandon(heap,tld);
 
-  // 1. try to reclaim an abandoned segment
   bool reclaimed;
   mi_segment_t* segment = mi_segment_try_reclaim(heap, needed_slices, block_size, &reclaimed, tld);
   if (reclaimed) {
-    // reclaimed the right page right into the heap
     mi_assert_internal(segment != NULL);
-    return NULL; // pretend out-of-memory as the page will be in the page queue of the heap with available blocks
+    return NULL;
   }
   else if (segment != NULL) {
-    // reclaimed a segment with a large enough empty span in it
     return segment;
   }
-  // 2. otherwise allocate a fresh segment
   return mi_segment_alloc(0, 0, heap->arena_id, tld, NULL);
 }
 
@@ -1538,19 +1345,15 @@ static mi_page_t* mi_segments_page_alloc
 {
   mi_assert_internal(required <= MI_LARGE_OBJ_SIZE_MAX && page_kind <= MI_PAGE_LARGE);
 
-  // find a free page
   size_t page_size = _mi_align_up(required, (required > MI_MEDIUM_PAGE_SIZE ? MI_MEDIUM_PAGE_SIZE : MI_SEGMENT_SLICE_SIZE));
   size_t slices_needed = page_size / MI_SEGMENT_SLICE_SIZE;
   mi_assert_internal(slices_needed * MI_SEGMENT_SLICE_SIZE == page_size);
-  mi_page_t* page = mi_segments_page_find_and_allocate(slices_needed, heap->arena_id, tld); //(required <= MI_SMALL_SIZE_MAX ? 0 : slices_needed), tld);
+  mi_page_t* page = mi_segments_page_find_and_allocate(slices_needed, heap->arena_id, tld);
   if (page==NULL) {
-    // no free page, allocate a new segment and try again
     if (mi_segment_reclaim_or_alloc(heap, slices_needed, block_size, tld) == NULL) {
-      // OOM or reclaimed a good page in the heap
       return NULL;
     }
     else {
-      // otherwise try again
       return mi_segments_page_alloc(heap, page_kind, required, block_size, tld);
     }
   }
@@ -1574,40 +1377,35 @@ static mi_page_t* mi_segment_huge_page_a
   mi_assert_internal(segment->used==1);
   mi_assert_internal(mi_page_block_size(page) >= size);
   #if MI_HUGE_PAGE_ABANDON
-  segment->thread_id = 0; // huge segments are immediately abandoned
+  segment->thread_id = 0;
   #endif
 
-  // for huge pages we initialize the block_size as we may
-  // overallocate to accommodate large alignments.
   size_t psize;
   uint8_t* start = _mi_segment_page_start(segment, page, &psize);
   page->block_size = psize;
   mi_assert_internal(page->is_huge);
 
-  // decommit the part of the prefix of a page that will not be used; this can be quite large (close to MI_SEGMENT_SIZE)
   if (page_alignment > 0 && segment->allow_decommit) {
     uint8_t* aligned_p = (uint8_t*)_mi_align_up((uintptr_t)start, page_alignment);
     mi_assert_internal(_mi_is_aligned(aligned_p, page_alignment));
     mi_assert_internal(psize - (aligned_p - start) >= size);
-    uint8_t* decommit_start = start + sizeof(mi_block_t);              // for the free list
+    uint8_t* decommit_start = start + sizeof(mi_block_t);
     ptrdiff_t decommit_size = aligned_p - decommit_start;
-    _mi_os_reset(decommit_start, decommit_size);   // note: cannot use segment_decommit on huge segments
+    if (decommit_size > 0) {
+      _mi_os_reset(decommit_start, decommit_size);
+    }
   }
 
   return page;
 }
 
 #if MI_HUGE_PAGE_ABANDON
-// free huge block from another thread
 void _mi_segment_huge_page_free(mi_segment_t* segment, mi_page_t* page, mi_block_t* block) {
-  // huge page segments are always abandoned and can be freed immediately by any thread
   mi_assert_internal(segment->kind==MI_SEGMENT_HUGE);
   mi_assert_internal(segment == _mi_page_segment(page));
   mi_assert_internal(mi_atomic_load_relaxed(&segment->thread_id)==0);
 
-  // claim it and free
-  mi_heap_t* heap = mi_heap_get_default(); // issue #221; don't use the internal get_default_heap as we need to ensure the thread is initialized.
-  // paranoia: if this it the last reference, the cas should always succeed
+  mi_heap_t* heap = mi_heap_get_default();
   size_t expected_tid = 0;
   if (mi_atomic_cas_strong_acq_rel(&segment->thread_id, &expected_tid, heap->thread_id)) {
     mi_block_set_next(page, block, page->free);
@@ -1626,19 +1424,18 @@ void _mi_segment_huge_page_free(mi_segme
 }
 
 #else
-// reset memory of a huge block from another thread
 void _mi_segment_huge_page_reset(mi_segment_t* segment, mi_page_t* page, mi_block_t* block) {
   MI_UNUSED(page);
   mi_assert_internal(segment->kind == MI_SEGMENT_HUGE);
   mi_assert_internal(segment == _mi_page_segment(page));
-  mi_assert_internal(page->used == 1); // this is called just before the free
+  mi_assert_internal(page->used == 1);
   mi_assert_internal(page->free == NULL);
   if (segment->allow_decommit) {
     size_t csize = mi_usable_size(block);
     if (csize > sizeof(mi_block_t)) {
       csize = csize - sizeof(mi_block_t);
       uint8_t* p = (uint8_t*)block + sizeof(mi_block_t);
-      _mi_os_reset(p, csize);  // note: cannot use segment_decommit on huge segments
+      _mi_os_reset(p, csize);
     }
   }
 }
