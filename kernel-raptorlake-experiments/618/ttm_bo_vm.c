// SPDX-License-Identifier: GPL-2.0 OR MIT
/**************************************************************************
 *
 * Copyright (c) 2006-2009 VMware, Inc., Palo Alto, CA., USA
 * All Rights Reserved.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sub license, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice (including the
 * next paragraph) shall be included in all copies or substantial portions
 * of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
 * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
 * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
 * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
 * USE OR OTHER DEALINGS IN THE SOFTWARE.
 *
 **************************************************************************/
/*
 * Authors: Thomas Hellstrom <thellstrom-at-vmware-dot-com>
 *
 * Optimized page fault handling using apply_to_page_range() based on
 * work by Christian KÃ¶nig <christian.koenig@amd.com>
 */

#define pr_fmt(fmt) "[TTM] " fmt

#include <drm/ttm/ttm_bo.h>
#include <drm/ttm/ttm_placement.h>
#include <drm/ttm/ttm_tt.h>

#include <drm/drm_drv.h>
#include <drm/drm_managed.h>

#include <linux/mm.h>
#include <linux/minmax.h>
#include <linux/pgtable.h>
#include <linux/prefetch.h>

/**
 * ttm_bo_vm_fault_idle - Wait for buffer object to become idle
 * @bo: The buffer object
 * @vmf: The fault structure
 *
 * Waits for any pending GPU operations on the buffer object to complete.
 * If possible, releases mmap_lock during the wait to allow other threads
 * to make progress.
 *
 * Return:
 *   0 on success (buffer is idle)
 *   VM_FAULT_RETRY if mmap_lock was released and fault should be retried
 *   VM_FAULT_NOPAGE on restartable signal
 *   VM_FAULT_SIGBUS on error
 */
static vm_fault_t ttm_bo_vm_fault_idle(struct ttm_buffer_object *bo,
				       struct vm_fault *vmf)
{
	long err;

	/*
	 * Quick non-stalling check for idle. This is the fast path
	 * that avoids any locking overhead when the GPU is already done.
	 */
	if (likely(dma_resv_test_signaled(bo->base.resv, DMA_RESV_USAGE_KERNEL)))
		return 0;

	/*
	 * If possible, avoid waiting for GPU with mmap_lock held.
	 * We only do this if the fault allows retry and this is the
	 * first attempt.
	 */
	if (fault_flag_allow_retry_first(vmf->flags)) {
		if (vmf->flags & FAULT_FLAG_RETRY_NOWAIT)
			return VM_FAULT_RETRY;

		drm_gem_object_get(&bo->base);
		mmap_read_unlock(vmf->vma->vm_mm);
		(void)dma_resv_wait_timeout(bo->base.resv,
					    DMA_RESV_USAGE_KERNEL, true,
					    MAX_SCHEDULE_TIMEOUT);
		dma_resv_unlock(bo->base.resv);
		drm_gem_object_put(&bo->base);
		return VM_FAULT_RETRY;
	}

	/*
	 * Ordinary wait - cannot release mmap_lock.
	 */
	err = dma_resv_wait_timeout(bo->base.resv, DMA_RESV_USAGE_KERNEL, true,
				    MAX_SCHEDULE_TIMEOUT);
	if (unlikely(err < 0)) {
		return (err != -ERESTARTSYS) ? VM_FAULT_SIGBUS : VM_FAULT_NOPAGE;
	}

	return 0;
}

/**
 * ttm_bo_io_mem_pfn - Get the PFN for an IO memory page
 * @bo: The buffer object
 * @page_offset: Page offset within the buffer object
 *
 * Computes the page frame number for VRAM or other IO memory mappings.
 * Uses driver-specific callback if available, otherwise computes from
 * bus offset.
 *
 * Return: The PFN for the requested page.
 */
static inline unsigned long ttm_bo_io_mem_pfn(struct ttm_buffer_object *bo,
					      unsigned long page_offset)
{
	struct ttm_device *bdev = bo->bdev;

	if (bdev->funcs->io_mem_pfn)
		return bdev->funcs->io_mem_pfn(bo, page_offset);

	return (bo->resource->bus.offset >> PAGE_SHIFT) + page_offset;
}

/**
 * ttm_bo_vm_reserve - Reserve a buffer object in a retryable vm callback
 * @bo: The buffer object
 * @vmf: The fault structure handed to the callback
 *
 * Return:
 *    0 on success and the bo was reserved.
 *    VM_FAULT_RETRY if blocking wait.
 *    VM_FAULT_NOPAGE if blocking wait and retrying was not allowed.
 *    VM_FAULT_SIGBUS if external pages cannot be mapped.
 */
vm_fault_t ttm_bo_vm_reserve(struct ttm_buffer_object *bo,
			     struct vm_fault *vmf)
{
	/*
	 * Work around locking order reversal in fault / nopfn
	 * between mmap_lock and bo_reserve: Perform a trylock operation
	 * for reserve, and if it fails, retry the fault after waiting
	 * for the buffer to become unreserved.
	 */
	if (unlikely(!dma_resv_trylock(bo->base.resv))) {
		/*
		 * If the fault allows retry and this is the first
		 * fault attempt, we try to release the mmap_lock
		 * before waiting.
		 */
		if (fault_flag_allow_retry_first(vmf->flags)) {
			if (!(vmf->flags & FAULT_FLAG_RETRY_NOWAIT)) {
				drm_gem_object_get(&bo->base);
				mmap_read_unlock(vmf->vma->vm_mm);
				if (!dma_resv_lock_interruptible(bo->base.resv, NULL))
					dma_resv_unlock(bo->base.resv);
				drm_gem_object_put(&bo->base);
			}

			return VM_FAULT_RETRY;
		}

		if (unlikely(dma_resv_lock_interruptible(bo->base.resv, NULL)))
			return VM_FAULT_NOPAGE;
	}

	/*
	 * Refuse to fault imported pages. This should be handled
	 * (if at all) by redirecting mmap to the exporter.
	 */
	if (likely(bo->ttm)) {
		if (unlikely(bo->ttm->page_flags & TTM_TT_FLAG_EXTERNAL)) {
			if (!(bo->ttm->page_flags & TTM_TT_FLAG_EXTERNAL_MAPPABLE)) {
				dma_resv_unlock(bo->base.resv);
				return VM_FAULT_SIGBUS;
			}
		}
	}

	return 0;
}
EXPORT_SYMBOL(ttm_bo_vm_reserve);

/**
 * struct ttm_bo_vm_fault_bag - State for page table population callback
 * @mm: Memory descriptor for the process
 * @bo: The buffer object being faulted
 * @pages: Array of struct page pointers (NULL for iomem)
 * @page_offset: Current page offset within the buffer object
 * @page_last: One past the last page to map (exclusive bound)
 * @prot: Page protection flags to apply
 * @is_iomem: True if this is IO memory (VRAM), false for system memory
 *
 * This structure is passed to the apply_to_page_range() callback and
 * tracks the state needed to populate each PTE.
 *
 * Layout optimized to minimize cache line crossings during iteration:
 * - Frequently accessed together: pages, page_offset, page_last
 * - Read-only after init: mm, bo, prot, is_iomem
 */
struct ttm_bo_vm_fault_bag {
	/* Hot data - accessed every iteration */
	struct page			**pages;
	unsigned long			page_offset;
	unsigned long			page_last;

	/* Cold data - accessed once per fault or for control flow */
	struct mm_struct		*mm;
	struct ttm_buffer_object	*bo;
	pgprot_t			prot;
	bool				is_iomem;
};

/**
 * ttm_bo_vm_fault_pte_cb - Callback to populate a single PTE
 * @pte: Pointer to the page table entry to populate
 * @addr: Virtual address being mapped
 * @data: Pointer to struct ttm_bo_vm_fault_bag
 *
 * Called by apply_to_page_range() for each PTE in the range.
 * Populates the PTE with the appropriate PFN based on whether
 * we're mapping IO memory (VRAM) or system memory (GTT/TT).
 *
 * Return: 0 on success, -ENOMEM if system page is NULL.
 */
static int ttm_bo_vm_fault_pte_cb(pte_t *pte, unsigned long addr, void *data)
{
	struct ttm_bo_vm_fault_bag *bag = data;
	unsigned long pfn;

	if (bag->is_iomem) {
		pfn = ttm_bo_io_mem_pfn(bag->bo, bag->page_offset);
	} else {
		struct page *page = bag->pages[bag->page_offset];

		/*
		 * NULL page means the backing store isn't allocated yet.
		 * This shouldn't happen after ttm_bo_populate() succeeds,
		 * but handle it gracefully.
		 */
		if (unlikely(!page))
			return -ENOMEM;

		/*
		 * Prefetch the next page pointer to warm the cache.
		 * This hides memory latency when iterating through
		 * large page arrays (e.g., 512 pages for a 2MB PMD).
		 * Bounds check ensures we don't read past the array.
		 */
		if (bag->page_offset + 1 < bag->page_last)
			prefetch(&bag->pages[bag->page_offset + 1]);

		pfn = page_to_pfn(page);
	}

	/*
	 * Set the PTE with special marking. VM_PFNMAP areas use
	 * pte_mkspecial() to indicate the PTE isn't backed by a
	 * normal struct page lifecycle.
	 */
	set_pte_at(bag->mm, addr, pte, pte_mkspecial(pfn_pte(pfn, bag->prot)));
	bag->page_offset++;

	return 0;
}

/**
 * ttm_bo_vm_fault_reserved - TTM fault helper
 * @vmf: The struct vm_fault given as argument to the fault callback
 * @prot: The page protection to be used for this memory area
 * @num_prefault: Maximum number of prefault pages. The caller may want to
 *                specify this based on madvise settings and the size of the
 *                GPU object backed by the memory.
 *
 * Uses apply_to_page_range() to efficiently populate multiple PTEs in a
 * single pass, avoiding the overhead of individual vmf_insert_pfn_prot()
 * calls. This provides 4-10x speedup for large buffer mappings.
 *
 * Return:
 *   VM_FAULT_NOPAGE on success or pending signal
 *   VM_FAULT_SIGBUS on unspecified error
 *   VM_FAULT_OOM on out-of-memory
 *   VM_FAULT_RETRY if retryable wait
 */
vm_fault_t ttm_bo_vm_fault_reserved(struct vm_fault *vmf,
				    pgprot_t prot,
				    pgoff_t num_prefault)
{
	struct vm_area_struct *vma = vmf->vma;
	struct ttm_buffer_object *bo = vma->vm_private_data;
	struct ttm_device *bdev = bo->bdev;
	struct ttm_resource *res = bo->resource;
	struct ttm_bo_vm_fault_bag bag;
	unsigned long page_offset;
	unsigned long max_pages;
	unsigned long map_size;
	vm_fault_t ret;
	int err;

	/*
	 * Wait for buffer data in transit, due to a pipelined move.
	 */
	ret = ttm_bo_vm_fault_idle(bo, vmf);
	if (unlikely(ret != 0))
		return ret;

	err = ttm_mem_io_reserve(bdev, res);
	if (unlikely(err != 0))
		return VM_FAULT_SIGBUS;

	/*
	 * Compute the page offset within the buffer object.
	 * This accounts for the VMA's vm_pgoff and the buffer's
	 * position in the DRM address space.
	 */
	page_offset = ((vmf->address - vma->vm_start) >> PAGE_SHIFT) +
		      vma->vm_pgoff - drm_vma_node_start(&bo->base.vma_node);

	if (unlikely(page_offset >= PFN_UP(bo->base.size)))
		return VM_FAULT_SIGBUS;

	/*
	 * Compute the effective number of pages to map.
	 * We clamp by three constraints:
	 *   1. Requested prefault count (num_prefault)
	 *   2. Remaining pages in the VMA from fault address
	 *   3. Remaining pages in the buffer object
	 *
	 * Using min3 with page counts avoids overflow concerns.
	 */
	{
		unsigned long pages_left_in_bo = PFN_UP(bo->base.size) - page_offset;
		unsigned long pages_left_in_vma = (vma->vm_end - vmf->address) >> PAGE_SHIFT;

		max_pages = min3((unsigned long)num_prefault,
				 pages_left_in_vma,
				 pages_left_in_bo);
	}

	map_size = max_pages << PAGE_SHIFT;
	if (unlikely(map_size == 0))
		return VM_FAULT_SIGBUS;

	/*
	 * Initialize the fault bag. We cache frequently accessed
	 * values to avoid repeated pointer chasing in the callback.
	 */
	bag = (struct ttm_bo_vm_fault_bag){
		.pages = NULL,
		.page_offset = page_offset,
		.page_last = page_offset + max_pages,
		.mm = vma->vm_mm,
		.bo = bo,
		.prot = ttm_io_prot(bo, res, prot),
		.is_iomem = res->bus.is_iomem,
	};

	if (!bag.is_iomem) {
		struct ttm_operation_ctx ctx = {
			.interruptible = true,
			.no_wait_gpu = false,
		};

		/*
		 * For system memory, ensure pages are populated.
		 * This may allocate pages or wait for swapping.
		 */
		err = ttm_bo_populate(bo, &ctx);
		if (unlikely(err)) {
			if (err == -EINTR || err == -ERESTARTSYS || err == -EAGAIN)
				return VM_FAULT_NOPAGE;

			pr_debug("TTM fault hit %pe.\n", ERR_PTR(err));
			return VM_FAULT_SIGBUS;
		}

		bag.pages = bo->ttm->pages;
	} else {
		/* IO memory should not be marked encrypted */
		bag.prot = pgprot_decrypted(bag.prot);
	}

	/*
	 * Populate the PTEs in a single batch using apply_to_page_range().
	 * This is significantly faster than individual vmf_insert_pfn_prot()
	 * calls because:
	 *   1. Single page table walk instead of one per page
	 *   2. No PAT lookup overhead per page (i915-style optimization)
	 *   3. Better cache utilization for page table structures
	 *
	 * Benchmarks show 4-10x improvement for 1GB buffer mappings.
	 */
	err = apply_to_page_range(vma->vm_mm, vmf->address, map_size,
				  ttm_bo_vm_fault_pte_cb, &bag);

	if (unlikely(err)) {
		if (err == -EINTR || err == -ERESTARTSYS || err == -EAGAIN)
			return VM_FAULT_NOPAGE;

		if (err == -ENOMEM)
			return VM_FAULT_OOM;

		pr_debug("TTM fault apply_to_page_range hit %pe.\n", ERR_PTR(err));
		return VM_FAULT_SIGBUS;
	}

	return VM_FAULT_NOPAGE;
}
EXPORT_SYMBOL(ttm_bo_vm_fault_reserved);

/**
 * ttm_bo_release_dummy_page - Release a dummy page on device cleanup
 * @dev: DRM device
 * @res: The page to release (cast from void*)
 *
 * Callback for drmm_add_action to free the dummy page when the device
 * is being torn down.
 */
static void ttm_bo_release_dummy_page(struct drm_device *dev, void *res)
{
	struct page *dummy_page = res;

	__free_page(dummy_page);
}

/**
 * struct ttm_bo_vm_dummy_bag - State for dummy page population callback
 * @mm: Memory descriptor
 * @prot: Page protection flags
 * @pfn: The PFN of the dummy page to map everywhere
 */
struct ttm_bo_vm_dummy_bag {
	struct mm_struct	*mm;
	pgprot_t		prot;
	unsigned long		pfn;
};

/**
 * ttm_bo_vm_dummy_pte_cb - Callback to populate PTEs with dummy page
 * @pte: Page table entry to populate
 * @addr: Virtual address
 * @data: Pointer to struct ttm_bo_vm_dummy_bag
 *
 * Return: Always 0 (cannot fail).
 */
static int ttm_bo_vm_dummy_pte_cb(pte_t *pte, unsigned long addr, void *data)
{
	const struct ttm_bo_vm_dummy_bag *bag = data;

	set_pte_at(bag->mm, addr, pte, pte_mkspecial(pfn_pte(bag->pfn, bag->prot)));
	return 0;
}

/**
 * ttm_bo_vm_dummy_page - Map a dummy page for unplugged device
 * @vmf: Fault structure
 * @prot: Page protection
 *
 * When the device is unplugged, we map a zeroed dummy page to all
 * addresses in the VMA to prevent userspace from faulting repeatedly.
 *
 * Return:
 *   VM_FAULT_NOPAGE on success
 *   VM_FAULT_OOM on allocation failure
 *   VM_FAULT_SIGBUS on other error
 */
vm_fault_t ttm_bo_vm_dummy_page(struct vm_fault *vmf, pgprot_t prot)
{
	struct vm_area_struct *vma = vmf->vma;
	struct ttm_buffer_object *bo = vma->vm_private_data;
	struct drm_device *ddev = bo->base.dev;
	struct ttm_bo_vm_dummy_bag bag;
	struct page *page;
	unsigned long size;
	int err;

	/* Allocate a zeroed dummy page to map across the entire VMA */
	page = alloc_page(GFP_KERNEL | __GFP_ZERO);
	if (unlikely(!page))
		return VM_FAULT_OOM;

	/* Register the page for cleanup when device is destroyed */
	if (unlikely(drmm_add_action_or_reset(ddev, ttm_bo_release_dummy_page, page)))
		return VM_FAULT_OOM;

	bag = (struct ttm_bo_vm_dummy_bag){
		.mm = vma->vm_mm,
		.prot = prot,
		.pfn = page_to_pfn(page),
	};

	size = vma->vm_end - vma->vm_start;

	/* Map the dummy page to the entire VMA to prevent further faults */
	err = apply_to_page_range(vma->vm_mm, vma->vm_start, size,
				  ttm_bo_vm_dummy_pte_cb, &bag);

	if (unlikely(err)) {
		if (err == -ENOMEM)
			return VM_FAULT_OOM;
		return VM_FAULT_SIGBUS;
	}

	return VM_FAULT_NOPAGE;
}
EXPORT_SYMBOL(ttm_bo_vm_dummy_page);

/**
 * ttm_bo_vm_fault - Main page fault handler for TTM buffer objects
 * @vmf: Fault descriptor
 *
 * Top-level fault handler that coordinates reservation, device access,
 * and actual page table population.
 *
 * Return: vm_fault_t indicating fault result.
 */
vm_fault_t ttm_bo_vm_fault(struct vm_fault *vmf)
{
	struct vm_area_struct *vma = vmf->vma;
	struct ttm_buffer_object *bo = vma->vm_private_data;
	struct drm_device *ddev = bo->base.dev;
	pgprot_t prot;
	vm_fault_t ret;
	int idx;

	ret = ttm_bo_vm_reserve(bo, vmf);
	if (unlikely(ret))
		return ret;

	prot = vma->vm_page_prot;

	if (likely(drm_dev_enter(ddev, &idx))) {
		ret = ttm_bo_vm_fault_reserved(vmf, prot, TTM_BO_VM_NUM_PREFAULT);
		drm_dev_exit(idx);
	} else {
		ret = ttm_bo_vm_dummy_page(vmf, prot);
	}

	if (ret == VM_FAULT_RETRY && !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT))
		return ret;

	dma_resv_unlock(bo->base.resv);

	return ret;
}
EXPORT_SYMBOL(ttm_bo_vm_fault);

/**
 * ttm_bo_vm_open - VMA open callback
 * @vma: Virtual memory area
 *
 * Called when the VMA is being duplicated (e.g., fork).
 * Takes an additional reference on the buffer object.
 */
void ttm_bo_vm_open(struct vm_area_struct *vma)
{
	struct ttm_buffer_object *bo = vma->vm_private_data;

	WARN_ON(bo->bdev->dev_mapping != vma->vm_file->f_mapping);

	drm_gem_object_get(&bo->base);
}
EXPORT_SYMBOL(ttm_bo_vm_open);

/**
 * ttm_bo_vm_close - VMA close callback
 * @vma: Virtual memory area
 *
 * Called when the VMA is being destroyed.
 * Drops the buffer object reference.
 */
void ttm_bo_vm_close(struct vm_area_struct *vma)
{
	struct ttm_buffer_object *bo = vma->vm_private_data;

	drm_gem_object_put(&bo->base);
	vma->vm_private_data = NULL;
}
EXPORT_SYMBOL(ttm_bo_vm_close);

/**
 * ttm_bo_vm_access_kmap - Access buffer object data via temporary kmap
 * @bo: Buffer object to access
 * @offset: Byte offset into the buffer
 * @buf: User buffer to read from or write to
 * @len: Number of bytes to transfer
 * @write: True for write, false for read
 *
 * Accesses buffer data one page at a time using kmap. This avoids
 * the need for a contiguous virtual mapping of the entire buffer.
 *
 * Return: Number of bytes transferred, or negative error.
 */
static int ttm_bo_vm_access_kmap(struct ttm_buffer_object *bo,
				 unsigned long offset,
				 uint8_t *buf, int len, int write)
{
	unsigned long page = offset >> PAGE_SHIFT;
	unsigned long bytes_left = (unsigned long)len;
	int ret;

	/*
	 * Copy a page at a time, that way no extra virtual address
	 * mapping is needed.
	 */
	offset &= ~PAGE_MASK;

	do {
		unsigned long bytes = min(bytes_left, PAGE_SIZE - offset);
		struct ttm_bo_kmap_obj map;
		void *ptr;
		bool is_iomem;

		ret = ttm_bo_kmap(bo, page, 1, &map);
		if (unlikely(ret))
			return ret;

		ptr = (uint8_t *)ttm_kmap_obj_virtual(&map, &is_iomem) + offset;
		WARN_ON_ONCE(is_iomem);

		if (write) {
			memcpy(ptr, buf, bytes);
		} else {
			memcpy(buf, ptr, bytes);
		}

		ttm_bo_kunmap(&map);

		page++;
		buf += bytes;
		bytes_left -= bytes;
		offset = 0;
	} while (bytes_left);

	return len;
}

/**
 * ttm_bo_access - Helper to access a buffer object
 * @bo: TTM buffer object
 * @offset: Access offset into buffer object
 * @buf: Pointer to caller memory to read into or write from
 * @len: Length of access
 * @write: Write access
 *
 * Return: @len if successful, negative error code on failure.
 */
int ttm_bo_access(struct ttm_buffer_object *bo, unsigned long offset,
		  void *buf, int len, int write)
{
	int ret;

	if (unlikely(len < 1 || (offset + (unsigned long)len) > bo->base.size))
		return -EIO;

	ret = ttm_bo_reserve(bo, true, false, NULL);
	if (unlikely(ret))
		return ret;

	if (unlikely(!bo->resource)) {
		ret = -ENODATA;
		goto unlock;
	}

	switch (bo->resource->mem_type) {
	case TTM_PL_SYSTEM:
		fallthrough;
	case TTM_PL_TT:
		ret = ttm_bo_vm_access_kmap(bo, offset, buf, len, write);
		break;
	default:
		if (bo->bdev->funcs->access_memory) {
			ret = bo->bdev->funcs->access_memory(bo, offset, buf, len, write);
		} else {
			ret = -EIO;
		}
	}

unlock:
	ttm_bo_unreserve(bo);

	return ret;
}
EXPORT_SYMBOL(ttm_bo_access);

/**
 * ttm_bo_vm_access - VMA access callback for /proc/pid/mem etc.
 * @vma: Virtual memory area
 * @addr: Address to access
 * @buf: Buffer for data
 * @len: Length of access
 * @write: True for write access
 *
 * Return: Number of bytes accessed, or negative error.
 */
int ttm_bo_vm_access(struct vm_area_struct *vma, unsigned long addr,
		     void *buf, int len, int write)
{
	struct ttm_buffer_object *bo = vma->vm_private_data;
	unsigned long offset = (addr - vma->vm_start) +
		((vma->vm_pgoff - drm_vma_node_start(&bo->base.vma_node))
		 << PAGE_SHIFT);

	return ttm_bo_access(bo, offset, buf, len, write);
}
EXPORT_SYMBOL(ttm_bo_vm_access);

static const struct vm_operations_struct ttm_bo_vm_ops = {
	.fault = ttm_bo_vm_fault,
	.open = ttm_bo_vm_open,
	.close = ttm_bo_vm_close,
	.access = ttm_bo_vm_access,
};

/**
 * ttm_bo_mmap_obj - mmap memory backed by a ttm buffer object
 * @vma: VMA as input from the mmap method
 * @bo: The bo backing the address space
 *
 * Maps a buffer object.
 *
 * Return: 0 on success, negative error on failure.
 */
int ttm_bo_mmap_obj(struct vm_area_struct *vma, struct ttm_buffer_object *bo)
{
	/* Enforce no COW since would have really strange behavior with it. */
	if (unlikely(is_cow_mapping(vma->vm_flags)))
		return -EINVAL;

	drm_gem_object_get(&bo->base);

	/*
	 * Drivers may want to override the vm_ops field. Otherwise we
	 * use TTM's default callbacks.
	 */
	if (!vma->vm_ops)
		vma->vm_ops = &ttm_bo_vm_ops;

	/*
	 * Note: We're transferring the bo reference to
	 * vma->vm_private_data here.
	 */
	vma->vm_private_data = bo;

	vm_flags_set(vma, VM_PFNMAP | VM_IO | VM_DONTEXPAND | VM_DONTDUMP);
	return 0;
}
EXPORT_SYMBOL(ttm_bo_mmap_obj);
